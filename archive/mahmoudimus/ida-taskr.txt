Project Path: arc_mahmoudimus_ida-taskr_8m8x1f8y

Source Tree:

```txt
arc_mahmoudimus_ida-taskr_8m8x1f8y
‚îú‚îÄ‚îÄ LICENSE
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ docs
‚îÇ   ‚îú‚îÄ‚îÄ IDA_TESTING.md
‚îÇ   ‚îî‚îÄ‚îÄ QTASYNCIO.md
‚îú‚îÄ‚îÄ examples
‚îÇ   ‚îú‚îÄ‚îÄ README.md
‚îÇ   ‚îú‚îÄ‚îÄ README_MINIMAL_API.md
‚îÇ   ‚îú‚îÄ‚îÄ README_SHARED_MEMORY.md
‚îÇ   ‚îú‚îÄ‚îÄ api_simplicity_levels.py
‚îÇ   ‚îú‚îÄ‚îÄ decorator_evolution.py
‚îÇ   ‚îú‚îÄ‚îÄ decorator_simple_example.py
‚îÇ   ‚îú‚îÄ‚îÄ event_emitter_usage.py
‚îÇ   ‚îú‚îÄ‚îÄ ida_shared_memory_pattern.py
‚îÇ   ‚îú‚îÄ‚îÄ interpreter_pool_example.py
‚îÇ   ‚îú‚îÄ‚îÄ minimal_decorator_api.py
‚îÇ   ‚îú‚îÄ‚îÄ one_line_solution.py
‚îÇ   ‚îú‚îÄ‚îÄ qtasyncio_event_loop.py
‚îÇ   ‚îú‚îÄ‚îÄ shared_memory_comparison.py
‚îÇ   ‚îú‚îÄ‚îÄ shared_memory_decorator_example.py
‚îÇ   ‚îú‚îÄ‚îÄ shared_memory_minimal.py
‚îÇ   ‚îú‚îÄ‚îÄ shared_memory_one_line.py
‚îÇ   ‚îú‚îÄ‚îÄ shared_memory_parallel_example.py
‚îÇ   ‚îú‚îÄ‚îÄ signature_generation_example.py
‚îÇ   ‚îú‚îÄ‚îÄ simple_cpu_intensive_example.py
‚îÇ   ‚îú‚îÄ‚îÄ simple_progress_example.py
‚îÇ   ‚îú‚îÄ‚îÄ thread_worker_usage.py
‚îÇ   ‚îî‚îÄ‚îÄ ultra_minimal.py
‚îú‚îÄ‚îÄ ida-plugin.json
‚îú‚îÄ‚îÄ pyproject.toml
‚îú‚îÄ‚îÄ run_tests.sh
‚îú‚îÄ‚îÄ scripts
‚îÇ   ‚îî‚îÄ‚îÄ amalgamate.py
‚îú‚îÄ‚îÄ src
‚îÇ   ‚îî‚îÄ‚îÄ ida_taskr
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îú‚îÄ‚îÄ decorators.py
‚îÇ       ‚îú‚îÄ‚îÄ helpers.py
‚îÇ       ‚îú‚îÄ‚îÄ launcher.py
‚îÇ       ‚îú‚îÄ‚îÄ protocols.py
‚îÇ       ‚îú‚îÄ‚îÄ qt_compat.py
‚îÇ       ‚îú‚îÄ‚îÄ qtasyncio.py
‚îÇ       ‚îú‚îÄ‚îÄ task_runner.py
‚îÇ       ‚îú‚îÄ‚îÄ taskr_plugin.py
‚îÇ       ‚îú‚îÄ‚îÄ utils.py
‚îÇ       ‚îî‚îÄ‚îÄ worker.py
‚îî‚îÄ‚îÄ tests
    ‚îú‚îÄ‚îÄ 11.1.0.60228.json
    ‚îú‚îÄ‚îÄ __init__.py
    ‚îú‚îÄ‚îÄ conftest.py
    ‚îú‚îÄ‚îÄ integration
    ‚îÇ   ‚îú‚îÄ‚îÄ 11.1.0.60228.json
    ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
    ‚îÇ   ‚îú‚îÄ‚îÄ anti_deob
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ deobfuscator.py
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ worker_main.py
    ‚îÇ   ‚îú‚îÄ‚îÄ conftest.py
    ‚îÇ   ‚îú‚îÄ‚îÄ test_anti_deob.py
    ‚îÇ   ‚îî‚îÄ‚îÄ test_integration_qt_core.py
    ‚îú‚îÄ‚îÄ run_ida_tests.py
    ‚îú‚îÄ‚îÄ run_tests_in_ida.py
    ‚îú‚îÄ‚îÄ test_imports.py
    ‚îú‚îÄ‚îÄ test_qprocess_headless.py
    ‚îî‚îÄ‚îÄ unit
        ‚îú‚îÄ‚îÄ __init__.py
        ‚îú‚îÄ‚îÄ test_event_emitter.py
        ‚îú‚îÄ‚îÄ test_imports.py
        ‚îú‚îÄ‚îÄ test_interpreter_pool_executor.py
        ‚îú‚îÄ‚îÄ test_process_pool_executor.py
        ‚îú‚îÄ‚îÄ test_qtasyncio.py
        ‚îú‚îÄ‚îÄ test_task_runner.py
        ‚îú‚îÄ‚îÄ test_thread_executor.py
        ‚îî‚îÄ‚îÄ test_worker_commands.py

```

`LICENSE`:

```
MIT License

Copyright (c) 2025 Mahmoud Rusty Abdelkader

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

```

`README.md`:

```md
# ida-taskr

![CI Status](https://github.com/mahmoudimus/ida-taskr/actions/workflows/python.yml/badge.svg)

## Overview

IDA Taskr is a pure Python library for IDA Pro parallel computing. It lets you use the power of Qt (built-in to IDA!) and Python's multiprocessing to offload computationally intensive tasks to worker processes without freezing IDA Pro's UI.

**Key Features:**
- üöÄ Simple decorator API - just add `@cpu_task` to run in background
- üîÑ Process-based parallelism for true multi-core execution
- üì¶ Shared memory support for large binary data
- üéØ Qt signal integration for progress callbacks
- ‚ö° Compatible with IDA Pro 9.1 (PyQt5) and 9.2+ (PySide6)

## Installation

**Option 1: Single file (no install needed)**

Download [`ida_taskr_amalgamated.py`](https://github.com/mahmoudimus/ida-taskr/releases/latest) and drop into IDA's plugins folder. That's it - one file, zero dependencies!

**Option 2: pip install**
```bash
pip install ida-taskr

# Or from source with Qt support
pip install -e .[pyqt5]    # For IDA Pro 9.1
pip install -e .[pyside6]  # For IDA Pro 9.2+
```

**Option 3: IDA Plugin Manager (HCLI)**

Download [`ida-taskr-{version}.zip`](https://github.com/mahmoudimus/ida-taskr/releases/latest) and install via HCLI.

## Quick Start

### The Simplest Way: `@cpu_task`

Add one decorator and your function runs in the background:

```python
from ida_taskr import cpu_task

@cpu_task
def analyze_binary(data):
    """This runs in a background thread - UI stays responsive!"""
    result = []
    for byte in data:
        result.append(process_byte(byte))
    return result

# Usage - returns immediately!
future = analyze_binary(binary_data)

# Do other work while it runs...

# Get result when needed
result = future.result()
```

That's it. One line. Your function now runs without blocking IDA.

### With Callbacks

Get notified when your task completes:

```python
from ida_taskr import cpu_task

@cpu_task(on_complete=lambda r: print(f"Done! Found {len(r)} patterns"))
def find_patterns(data):
    return scan_for_patterns(data)

# Fire and forget - callback handles the result
find_patterns(binary_data)
```

### Parallel Processing

Process multiple items across worker threads:

```python
from ida_taskr import parallel

@parallel(max_workers=8)
def analyze_function(func_ea):
    """Analyze a single function."""
    return get_function_signature(func_ea)

# Process 100 functions in parallel
function_addresses = list(idautils.Functions())
futures = [analyze_function(addr) for addr in function_addresses]
results = [f.result() for f in futures]
```

### Large Data with Shared Memory

For large binary blobs (megabytes), use shared memory to avoid copying:

```python
from ida_taskr import shared_memory_task

@shared_memory_task(num_chunks=8)
def find_signatures(chunk_data, chunk_id, total_chunks):
    """
    Process one chunk of the binary.

    chunk_data: memoryview of this chunk (zero-copy!)
    chunk_id: which chunk this is (0-7)
    total_chunks: total number of chunks (8)
    """
    signatures = []
    for i in range(len(chunk_data) - 16):
        if is_interesting_pattern(chunk_data[i:i+16]):
            signatures.append(bytes(chunk_data[i:i+16]))
    return signatures

# ida-taskr handles all shared memory complexity!
binary_data = ida_bytes.get_bytes(start_ea, size)  # e.g., 8MB
all_signatures = find_signatures(binary_data)  # Returns list of 8 results
```

## Decorator Reference

| Decorator | Use Case | Example |
|-----------|----------|---------|
| `@cpu_task` | CPU-intensive work | Pattern scanning, signature generation |
| `@io_task` | I/O-bound work | Network requests, file operations |
| `@parallel(n)` | Multiple parallel tasks | Batch function analysis |
| `@background_task` | Full control with callbacks | Progress reporting |
| `@shared_memory_task` | Large data processing | Multi-MB binary analysis |

### `@background_task` - Full Control

```python
from ida_taskr import background_task

@background_task(
    max_workers=4,
    on_complete=lambda r: print(f"Result: {r}"),
    on_error=lambda e: print(f"Error: {e}"),
    on_progress=lambda p, m: print(f"[{p}%] {m}"),
    executor_type='process'  # 'thread' or 'process'
)
def heavy_analysis(data, progress_callback=None):
    for i, chunk in enumerate(chunks(data, 100)):
        process(chunk)
        if progress_callback:
            progress_callback(i * 10, f"Processed chunk {i}")
    return "done"
```

## Advanced Usage

### Direct Executor Access

For more control, use the executors directly:

```python
from ida_taskr import ProcessPoolExecutor, ThreadExecutor

# Process-based (true parallelism, bypasses GIL)
with ProcessPoolExecutor(max_workers=4) as executor:
    futures = [executor.submit(cpu_task, arg) for arg in args]
    results = [f.result() for f in futures]

# Thread-based (good for IDA SDK calls that release GIL)
with ThreadExecutor(max_workers=8) as executor:
    futures = [executor.submit(analyze_func, ea) for ea in function_list]
    results = [f.result() for f in futures]
```

### Worker Scripts (Bidirectional IPC)

For complex scenarios requiring persistent workers and bidirectional communication.
Use this when you need:
- Long-running worker processes that stay alive between tasks
- Custom message protocols between IDA and workers
- Streaming results back to IDA as work progresses
- Worker state that persists across multiple commands

```python
from ida_taskr import TaskRunner

runner = TaskRunner(
    worker_script="path/to/worker.py",
    worker_args=["arg1", "arg2"]
)

@runner.on('worker_results')
def handle_results(results):
    print(f"Results: {results}")

@runner.on('worker_message')
def handle_progress(msg):
    print(f"Progress: {msg}")

runner.start()
runner.send_command({"command": "process", "data": [1, 2, 3]})
# Worker stays alive, can send more commands...
runner.send_command({"command": "analyze", "target": 0x401000})
runner.stop()
```

See [examples/](examples/) for more detailed examples including:
- [Ultra minimal example](examples/ultra_minimal.py) - Smallest possible code
- [Shared memory patterns](examples/shared_memory_parallel_example.py) - Large data processing
- [Signature generation](examples/signature_generation_example.py) - Real IDA use case
- [QtAsyncio integration](examples/qtasyncio_event_loop.py) - Async/await support

## Testing

```bash
# Run all unit tests
./run_tests.sh

# Run Qt integration tests
pytest tests/integration/test_integration_qt_core.py -v

# Run with coverage
pytest tests/ --cov=src/ida_taskr --cov-report=html
```

**Supported Configurations:**
- ‚úÖ Python 3.11, 3.12, 3.13
- ‚úÖ PyQt5 (IDA Pro 9.1)
- ‚úÖ PySide6 (IDA Pro 9.2+)

## Documentation

- [QtAsyncio Integration](docs/QTASYNCIO.md) - Async/await and event loop details
- [IDA Testing Guide](docs/IDA_TESTING.md) - Running tests inside IDA Pro
- [Examples README](examples/README.md) - Comprehensive examples guide

## Contributing ü§ù

We welcome contributions! See the examples and tests for code style.

1. Fork the repository
2. Create a feature branch
3. Run tests: `./run_tests.sh`
4. Submit a pull request

## License üìú

MIT License - see [LICENSE](LICENSE) for details.

## Contact üìß

Questions or issues? Open a GitHub issue or reach out to [@mahmoudimus](https://github.com/mahmoudimus).

```

`docs/IDA_TESTING.md`:

```md
# Running Tests in IDA Pro

Some tests in ida-taskr require IDA Pro's Qt application to be running. These tests are marked with `@pytest.mark.skipif(not is_ida())` and will automatically skip when run outside IDA.

## Tests That Require IDA Pro

### WorkerLauncher Integration Test
**File:** `tests/unit/test_event_emitter.py`
**Test:** `TestMessageEmitter::test_worker_launcher_integration`
**Reason:** `WorkerLauncher` inherits from `QProcess` and requires a full Qt event loop with socket notifiers.

### Worker Execution Test
**File:** `tests/unit/test_qtasyncio.py`
**Test:** `TestQtApplicationIntegration::test_full_worker_execution`
**Reason:** Worker utilities need a running Qt application to create QThread-based workers.

## How to Run Tests in IDA Pro

### Method 1: IDA Python Console

```python
# Inside IDA Pro's Python console
import subprocess
import sys

subprocess.run([
    sys.executable, "-m", "pytest",
    "tests/unit/test_event_emitter.py::TestMessageEmitter::test_worker_launcher_integration",
    "tests/unit/test_qtasyncio.py::TestQtApplicationIntegration::test_full_worker_execution",
    "-v"
])
```

### Method 2: IDAPython Script

1. Save as `run_tests.py` in your IDA scripts directory
2. File ‚Üí Script file... ‚Üí Select `run_tests.py`

```python
import subprocess
import sys

result = subprocess.run([
    sys.executable, "-m", "pytest",
    "tests/",  # Run all tests
    "-v",
    "--tb=short"
])

print(f"Tests {'passed' if result.returncode == 0 else 'failed'}")
```

### Method 3: Headless IDA (Command Line)

```bash
# Run IDA in headless mode with test script
idat64 -A -S"tests/run_ida_tests.py" /path/to/binary

# Or run pytest directly with IDA's Python
/path/to/ida/python -m pytest tests/ -v -k "not skipif"
```

### Method 4: Using the Helper Script

```bash
# From your terminal (will skip IDA-only tests)
./run_tests.sh

# Results:
# - 139 tests pass
# - 2 tests skip (IDA-only)
```

Then run IDA-only tests inside IDA:
```python
# In IDA Pro:
exec(open('tests/run_ida_tests.py').read())
```

## Test Status

| Test | Outside IDA | Inside IDA |
|------|-------------|------------|
| Regular unit tests | ‚úÖ Pass (139) | ‚úÖ Pass |
| `test_worker_launcher_integration` | ‚è≠Ô∏è Skip | ‚úÖ Pass |
| `test_full_worker_execution` | ‚è≠Ô∏è Skip | ‚úÖ Pass |

## CI/CD

The GitHub Actions workflow runs tests in two jobs:

1. **unit-tests** - Runs all regular tests (139 pass, 2 skip)
2. **qt-integration-tests** - Runs Qt integration tests with `QT_QPA_PLATFORM=offscreen`

IDA-specific tests are intentionally skipped in CI since they require actual IDA Pro installation.

## Troubleshooting

### "WorkerLauncher requires IDA Pro's Qt application"
This means you're running the test outside IDA. The test will automatically skip. Run it inside IDA Pro to execute.

### Segmentation Fault
If you see a segfault when running `WorkerLauncher` tests, ensure you're running inside IDA Pro where the Qt application is properly initialized.

### "Qt not available"
Make sure PySide6 or PyQt5 is installed in IDA's Python environment:
```bash
/path/to/ida/python -m pip install PySide6
```

```

`docs/QTASYNCIO.md`:

```md
# QtAsyncio Integration Guide

IDA Taskr now includes **QtAsyncio**, a comprehensive module that integrates Python's `asyncio` with Qt's event loop, providing powerful worker utilities and seamless async/await support in Qt applications.

## Table of Contents

- [Overview](#overview)
- [Features](#features)
- [Quick Start](#quick-start)
- [Components](#components)
  - [Worker Utilities](#worker-utilities)
  - [Thread Executor](#thread-executor)
  - [Asyncio Event Loop Integration](#asyncio-event-loop-integration)
- [Usage Examples](#usage-examples)
- [Integration with IDA Taskr](#integration-with-ida-taskr)
- [API Reference](#api-reference)

## Overview

The QtAsyncio module provides three main categories of functionality:

1. **Worker Utilities**: Decorator-based and class-based workers for running tasks in background threads
2. **Thread Executor**: Qt-native `concurrent.futures.Executor` implementation
3. **Asyncio Integration**: Qt-compatible event loop policy for using `async`/`await` in Qt applications

## Features

- ‚úÖ **Thread Workers**: Run functions in background threads with Qt signal integration
- ‚úÖ **Generator Workers**: Support for generator functions that yield intermediate results
- ‚úÖ **Thread Executor**: Qt-based implementation of `concurrent.futures.Executor`
- ‚úÖ **Asyncio Integration**: Seamlessly mix `asyncio` and Qt event loops
- ‚úÖ **Type Hints**: Full type annotation support for better IDE integration
- ‚úÖ **Zero Dependencies**: No external dependencies beyond PyQt5/PySide6
- ‚úÖ **Compatible**: Works with both PyQt5 (IDA 9.1) and PySide6 (IDA 9.2+)

## Quick Start

### Check Availability

```python
from ida_taskr import QT_ASYNCIO_AVAILABLE

if QT_ASYNCIO_AVAILABLE:
    print("QtAsyncio is available!")
```

### Simple Worker Example

```python
from ida_taskr import thread_worker

@thread_worker
def compute_something(x, y):
    # This runs in a background thread
    return x + y

# In your Qt application:
worker = compute_something(10, 20)
worker.returned.connect(lambda result: print(f"Result: {result}"))
worker.start()
```

### Asyncio Integration Example

```python
import asyncio
from ida_taskr import set_event_loop_policy

# Enable Qt-asyncio integration
set_event_loop_policy()

async def fetch_data():
    await asyncio.sleep(1)
    return "data"

# Now you can use async/await in your Qt application!
asyncio.create_task(fetch_data())
```

## Components

### Worker Utilities

The worker utilities provide an easy way to run tasks in background threads without blocking the Qt UI.

#### FunctionWorker

For regular functions:

```python
from ida_taskr import create_worker

def long_task(duration):
    import time
    time.sleep(duration)
    return f"Completed after {duration}s"

worker = create_worker(long_task, 5)
worker.returned.connect(on_result)
worker.errored.connect(on_error)
worker.finished.connect(on_finished)
worker.start()
```

#### GeneratorWorker

For generator functions that yield intermediate results:

```python
from ida_taskr import create_worker

def process_items(items):
    for item in items:
        # Do some work
        result = process(item)
        yield result

worker = create_worker(process_items, my_items)
worker.yielded.connect(on_intermediate_result)
worker.returned.connect(on_complete)
worker.start()
```

#### The `@thread_worker` Decorator

The simplest way to create workers:

```python
from ida_taskr import thread_worker

@thread_worker
def my_background_task(x):
    # Heavy computation here
    return x * 2

# Creates and returns a worker
worker = my_background_task(100)
worker.start()
```

#### Worker Signals

All workers provide these signals:

- `started` - Emitted when the worker starts
- `finished` - Emitted when the worker finishes (success or error)
- `returned(result)` - Emitted with the return value
- `errored(exception)` - Emitted if an exception occurs
- `warned(warning)` - Emitted for warnings

Generator workers additionally provide:

- `yielded(value)` - Emitted for each yielded value
- `paused` - Emitted when paused
- `resumed` - Emitted when resumed
- `aborted` - Emitted when aborted

### Thread Executor

Qt-native implementation of `concurrent.futures.Executor`:

```python
from ida_taskr import ThreadExecutor

executor = ThreadExecutor()

def cpu_intensive(n):
    return sum(i*i for i in range(n))

future = executor.submit(cpu_intensive, 1000000)
future.add_done_callback(lambda f: print(f.result()))

# Clean up when done
executor.shutdown(wait=True)
```

### Asyncio Event Loop Integration

Enable seamless integration between asyncio and Qt:

```python
import asyncio
from ida_taskr import set_event_loop_policy

# Call this once at application startup
set_event_loop_policy()

# Now you can use asyncio naturally with Qt
async def my_async_function():
    result = await asyncio.sleep(1)
    return "done"

# In your Qt application
asyncio.create_task(my_async_function())
```

## Usage Examples

### Example 1: Progress Updates

```python
from ida_taskr import create_worker

def process_with_progress(items):
    total = len(items)
    for i, item in enumerate(items):
        # Process item
        result = do_work(item)
        # Yield progress
        yield {"progress": (i + 1) / total, "item": item}
    return "All done!"

worker = create_worker(process_with_progress, my_items)

def on_progress(data):
    print(f"Progress: {data['progress']*100:.1f}%")

worker.yielded.connect(on_progress)
worker.returned.connect(lambda r: print(f"Complete: {r}"))
worker.start()
```

### Example 2: Error Handling

```python
from ida_taskr import thread_worker

@thread_worker
def risky_operation(data):
    if not data:
        raise ValueError("Data cannot be empty")
    return process_data(data)

worker = risky_operation(my_data)

def on_error(exception):
    print(f"Error occurred: {exception}")
    # Handle error appropriately

worker.errored.connect(on_error)
worker.returned.connect(on_success)
worker.start()
```

### Example 3: Mixing Asyncio and Qt

```python
import asyncio
from ida_taskr import set_event_loop_policy
from PySide6.QtWidgets import QPushButton

set_event_loop_policy()

class MyWidget(QWidget):
    def __init__(self):
        super().__init__()
        self.button = QPushButton("Fetch Data")
        self.button.clicked.connect(self.on_click)

    def on_click(self):
        # Start an async task from a Qt signal
        asyncio.create_task(self.fetch_and_update())

    async def fetch_and_update(self):
        data = await self.fetch_data_async()
        self.update_ui(data)

    async def fetch_data_async(self):
        # Simulate async I/O
        await asyncio.sleep(2)
        return {"status": "success"}
```

## Integration with IDA Taskr

The QtAsyncio module integrates seamlessly with IDA Taskr's existing components:

### Using QtAsyncio with WorkerController

```python
from ida_taskr.worker import WorkerController
from ida_taskr.utils import AsyncEventEmitter

class MyEmitter(AsyncEventEmitter):
    async def run(self):
        # Your async work here
        return result

emitter = MyEmitter()

# Enable QtAsyncio integration
controller = WorkerController(emitter, use_qtasyncio=True)
controller.start()
```

### When to Use QtAsyncio

**Use QtAsyncio Worker Utilities when:**
- You want simple background task execution with Qt signals
- You need to update the UI from worker threads
- You want a decorator-based API

**Use IDA Taskr's WorkerBase when:**
- You need multiprocessing (separate process isolation)
- You're building IDA Pro plugins with heavy computation
- You need bidirectional IPC between IDA and worker processes

**Use QtAsyncio Event Loop when:**
- You want to use `async`/`await` in your Qt application
- You're mixing asyncio libraries with Qt
- You need modern Python async patterns

## API Reference

### Worker Utilities

#### `create_worker(func, *args, **kwargs)`

Create a worker from a function or generator function.

**Returns:** `FunctionWorker` or `GeneratorWorker` depending on function type

#### `@thread_worker(func)`

Decorator to create a worker-returning function.

**Example:**
```python
@thread_worker
def my_func(x):
    return x * 2

worker = my_func(10)  # Returns a worker
worker.start()
```

#### `new_worker_qthread(Worker, *args, _start_thread=False, _connect=None, **kwargs)`

Create a QThread-based worker (alternative to QRunnable).

**Parameters:**
- `Worker`: QObject subclass with a `work()` method
- `_start_thread`: Whether to start the thread immediately
- `_connect`: Dict of signal->slot connections

**Returns:** `(worker, thread)` tuple

### Thread Executor

#### `ThreadExecutor(parent=None, threadPool=None)`

Qt-native concurrent.futures.Executor.

**Methods:**
- `submit(func, *args, **kwargs)`: Submit a callable for execution
- `shutdown(wait=True)`: Shutdown the executor

### Asyncio Integration

#### `set_event_loop_policy()`

Set the Qt-compatible asyncio event loop policy.

#### `QAsyncioEventLoop(asyncio_loop, parent=None)`

Qt event loop that integrates with asyncio.

#### `QAsyncioEventLoopPolicy`

Event loop policy that creates Qt-integrated event loops.

## Best Practices

1. **Always check availability:**
   ```python
   from ida_taskr import QT_ASYNCIO_AVAILABLE
   if not QT_ASYNCIO_AVAILABLE:
       # Fallback code
   ```

2. **Set event loop policy early:**
   ```python
   # At application startup
   set_event_loop_policy()
   ```

3. **Clean up workers:**
   ```python
   worker.finished.connect(worker.deleteLater)
   ```

4. **Handle errors:**
   ```python
   worker.errored.connect(handle_error)
   ```

5. **Use appropriate worker type:**
   - CPU-bound tasks ‚Üí `FunctionWorker`
   - Tasks with progress ‚Üí `GeneratorWorker`
   - Heavy IDA analysis ‚Üí IDA Taskr's `WorkerBase` with multiprocessing

## Examples

Complete examples are available in the `examples/` directory:

- `thread_worker_usage.py` - Demonstrates worker utilities
- `qtasyncio_event_loop.py` - Shows asyncio event loop integration

## Troubleshooting

**Q: Import error when importing qtasyncio**
- Ensure PyQt5 or PySide6 is installed
- Check `QT_ASYNCIO_AVAILABLE` before importing

**Q: Workers don't emit signals**
- Make sure you call `worker.start()`
- Ensure Qt event loop is running

**Q: Async tasks don't run**
- Call `set_event_loop_policy()` before creating tasks
- Ensure Qt application is running

## License

This module is based on Qt's official asyncio integration (commit: 072ffd057a29a694a0ad91894736bb4d0a88738e) with additional utilities and enhancements for IDA Taskr.

```

`examples/README.md`:

```md
# ida-taskr Examples

Complete examples demonstrating how to use ida-taskr for background task processing in IDA Pro.

---

## üöÄ Quick Start: The Simplest Way

**Problem:** You need to do CPU-intensive work without freezing IDA's UI.

**Solution:** Just add `@cpu_task` to your function!

```python
from ida_taskr import cpu_task

@cpu_task
def analyze(data):
    # Your CPU-intensive code
    return result

# Returns immediately - runs in background!
future = analyze(data)
result = future.result()
```

**That's it! One line decorator and done.**

---

## üìö What's in This Directory?

### ‚≠ê **Start Here: Minimal API Examples**

These show the **absolute simplest** way to use ida-taskr:

| File | What It Shows | Lines of Code |
|------|---------------|---------------|
| **[one_line_solution.py](one_line_solution.py)** | üèÜ The one-line answer: `@cpu_task` | **1 line!** |
| **[ultra_minimal.py](ultra_minimal.py)** | Absolute minimal working example | ~10 lines |
| **[api_simplicity_levels.py](api_simplicity_levels.py)** | All API levels (verbose ‚Üí minimal) | Comparison |
| **[README_MINIMAL_API.md](README_MINIMAL_API.md)** | Complete decorator documentation | Guide |

### üöÄ **Shared Memory Examples (For 8MB+ Data)**

These show how to process large binary data efficiently:

| File | What It Shows | Reduction |
|------|---------------|-----------|
| **[shared_memory_one_line.py](shared_memory_one_line.py)** | üèÜ Minimal shared memory API | **8x simpler!** |
| **[shared_memory_decorator_example.py](shared_memory_decorator_example.py)** | Working 8MB example | 40 ‚Üí 5 lines |
| **[shared_memory_comparison.py](shared_memory_comparison.py)** | Before/after comparison | Side-by-side |
| **[ida_shared_memory_pattern.py](ida_shared_memory_pattern.py)** | IDA-specific pattern (anti_deob style) | Production |
| **[README_SHARED_MEMORY.md](README_SHARED_MEMORY.md)** | Complete shared memory docs | Guide |

### üíª **Real-World CPU-Intensive Examples**

| File | What It Shows |
|------|---------------|
| **[decorator_simple_example.py](decorator_simple_example.py)** | Binary signature generation |
| **[decorator_evolution.py](decorator_evolution.py)** | Blocking ‚Üí Async evolution |
| **[cpu_intensive_example.py](cpu_intensive_example.py)** | Complete ProcessPoolExecutor example |
| **[cpu_intensive_with_progress.py](cpu_intensive_with_progress.py)** | With progress updates |

### üìä **Classic TaskRunner Examples**

| File | What It Shows |
|------|---------------|
| **simple_progress_example.py** | Basic TaskRunner with progress |
| **signature_generation_example.py** | Complete guide with 3 approaches |

---

## üéØ Example Selection Guide

### "I just want the simplest way to make my function non-blocking"
‚Üí **[one_line_solution.py](one_line_solution.py)** - Just add `@cpu_task`!

### "I need to process large binary data (8MB+) efficiently"
‚Üí **[shared_memory_one_line.py](shared_memory_one_line.py)** - Zero-copy shared memory

### "I want to understand all the API options"
‚Üí **[api_simplicity_levels.py](api_simplicity_levels.py)** - See all 5 levels

### "I need a real-world IDA example"
‚Üí **[decorator_simple_example.py](decorator_simple_example.py)** - Binary signatures
‚Üí **[ida_shared_memory_pattern.py](ida_shared_memory_pattern.py)** - Large data pattern

### "I want comprehensive documentation"
‚Üí **[README_MINIMAL_API.md](README_MINIMAL_API.md)** - Decorator guide
‚Üí **[README_SHARED_MEMORY.md](README_SHARED_MEMORY.md)** - Shared memory guide

---

## üî• Key Decorators Reference

### `@cpu_task` - Simplest for CPU work

```python
from ida_taskr import cpu_task

@cpu_task
def analyze(data):
    # CPU-intensive work
    return result

future = analyze(data)  # Returns immediately
result = future.result()  # Get result when ready
```

**Use when:** You want the simplest way to run CPU work without blocking.

### `@cpu_task` with callback

```python
@cpu_task(on_complete=lambda r: print(f"Done: {r}"))
def analyze(data):
    return result

analyze(data)  # Fire and forget - callback handles result
```

**Use when:** You want automatic result delivery.

### `@shared_memory_task` - For large data (8MB+)

```python
from ida_taskr import shared_memory_task

@shared_memory_task(num_chunks=8)
def analyze_chunk(chunk_data, chunk_id, total_chunks):
    # Process this chunk (no data copying!)
    return find_patterns(chunk_data)

# Processes 8MB in 8 parallel chunks
results = analyze_chunk(binary_data)
```

**Use when:** Processing large binary data where copying would be expensive.

### `@background_task` - Full control

```python
from ida_taskr import background_task

@background_task(
    max_workers=4,
    on_complete=handle_result,
    on_error=handle_error,
    executor_type='thread'  # or 'process'
)
def task(args):
    return result
```

**Use when:** You need fine-grained control over all options.

---

## üìñ Classic TaskRunner API

For progress-based workflows with yield:

```python
from ida_taskr import TaskRunner

def my_heavy_work(data):
    """Your CPU-intensive function."""
    for i, item in enumerate(data):
        # Do work...

        # Send progress update
        yield {
            'type': 'progress',
            'progress': int((i + 1) / len(data) * 100),
            'message': f'Processing {i+1}/{len(data)}'
        }

    return {'result': 'done!'}

# Run it
runner = TaskRunner()

@runner.message_emitter.on('progress')
def on_progress(data):
    print(f"{data['progress']}%: {data['message']}")

@runner.message_emitter.on('result')
def on_result(data):
    print(f"Done: {data['result']}")

runner.run_task(my_heavy_work, data=[1, 2, 3, 4, 5])
# UI is NOT blocked! Progress updates arrive via Qt signals
```

## Examples

### 1. `simple_progress_example.py` - Start Here
The minimal example showing the core pattern:
- Define a worker function with `yield` for progress
- Connect to signals for updates
- Start task with `runner.run_task()`

**When to use:** You need a simple, high-level API for background tasks.

### 2. `signature_generation_example.py` - Complete Guide
Comprehensive examples showing three approaches:

#### Approach 1: TaskRunner (Recommended)
- **Best for:** IDA plugins, most use cases
- **Pros:** Automatic progress updates, easy signal connections
- **Example:** Generating binary signatures with real-time progress

#### Approach 2: ThreadExecutor
- **Best for:** I/O-bound tasks, database queries, network requests
- **Pros:** Lower overhead than processes, shared memory
- **Example:** Running multiple I/O tasks in parallel

#### Approach 3: ProcessPoolExecutor
- **Best for:** CPU-intensive tasks, true parallelism needed
- **Pros:** Bypasses Python GIL, uses multiple CPU cores
- **Example:** Batch processing multiple signatures in parallel

## Key Concepts

### 1. Worker Functions

Worker functions run in separate processes/threads:

```python
def worker(arg1, arg2):
    """Worker function that does heavy lifting."""

    # Send progress updates
    yield {'type': 'progress', 'progress': 50}

    # Return final result
    return {'result': 'value'}
```

### 2. Progress Updates

Use `yield` to send updates without blocking:

```python
for i in range(100):
    # Do work...

    # Send update (becomes Qt signal)
    yield {
        'type': 'progress',
        'progress': i,
        'message': f'Step {i}/100'
    }
```

### 3. Signal Connections

Connect to signals to receive updates:

```python
@runner.message_emitter.on('progress')
def on_progress(data):
    # Update IDA's UI
    idaapi.replace_wait_box(f"{data['message']}")

@runner.message_emitter.on('result')
def on_result(data):
    # Show final result
    print(f"Done: {data['result']}")
```

## Real-World Use Cases

### 1. Binary Signature Generation (8MB file, ~2 minutes)

```python
def generate_unique_signature(start_ea, end_ea):
    """Generate signature for a code range."""
    data = ida_bytes.get_bytes(start_ea, end_ea - start_ea)
    signature = []

    for i in range(min(64, len(data))):
        # Analyze byte for uniqueness
        byte_val = data[i]

        # Check if pattern is unique in database
        if i % 8 == 0:
            yield {
                'type': 'progress',
                'progress': int(i / 64 * 100),
                'message': f'Testing signature at {len(signature)} bytes'
            }

        signature.append(byte_val)

        # Test uniqueness (CPU-intensive)
        if is_unique_pattern(signature):
            break

    return {'signature': signature}

runner = TaskRunner()
runner.run_task(generate_unique_signature, 0x401000, 0xC01000)
# IDA UI stays responsive for 2+ minutes!
```

### 2. Batch Function Analysis

```python
def analyze_all_functions():
    """Analyze every function in the binary."""
    funcs = list(idautils.Functions())

    for i, func_ea in enumerate(funcs):
        # Analyze function (CPU-intensive)
        result = analyze_function(func_ea)

        yield {
            'type': 'progress',
            'progress': int((i + 1) / len(funcs) * 100),
            'message': f'Analyzed {i+1}/{len(funcs)} functions',
            'partial_result': result
        }

    return {'total': len(funcs)}
```

### 3. Parallel Pattern Matching

```python
# Use ProcessPoolExecutor for parallel search
executor = ProcessPoolExecutor(max_workers=4)

patterns = [b'\x55\x8B\xEC', b'\x48\x89\x5C\x24', ...]

futures = []
for pattern in patterns:
    future = executor.submit(search_pattern, pattern, binary_data)
    futures.append(future)

# All searches run in parallel across 4 CPU cores!
```

## Choosing the Right Approach

| Scenario | Use | Why |
|----------|-----|-----|
| Need progress updates | `TaskRunner` | Built-in progress support |
| CPU-intensive, single task | `TaskRunner` | Simple API, automatic process management |
| CPU-intensive, parallel | `ProcessPoolExecutor` | Multiple cores, true parallelism |
| I/O-bound (network, disk) | `ThreadExecutor` | Lower overhead, shared memory |
| Quick one-off task | `create_worker()` | Simplest API |

## Common Patterns

### Pattern 1: Progress Bar in IDA

```python
def long_analysis(data):
    total = len(data)
    for i, item in enumerate(data):
        # Work...
        yield {
            'type': 'progress',
            'progress': int((i + 1) / total * 100)
        }
    return result

runner = TaskRunner()

@runner.message_emitter.on('progress')
def update_progress(data):
    idaapi.replace_wait_box(f"Analyzing: {data['progress']}%")

idaapi.show_wait_box("Starting analysis...")
runner.run_task(long_analysis, data)
```

### Pattern 2: Cancellable Task

```python
runner = TaskRunner()

# User can cancel
@runner.message_emitter.on('cancelled')
def on_cancel(data):
    print("User cancelled the operation")

# Show cancel button in IDA
idaapi.show_wait_box("HIDECANCEL\nAnalyzing...")
runner.run_task(long_task)
```

### Pattern 3: Partial Results

```python
def incremental_search(pattern, data):
    """Return results as they're found."""
    for i, chunk in enumerate(chunks(data)):
        matches = find_matches(pattern, chunk)

        if matches:
            # Send partial results immediately
            yield {
                'type': 'partial_result',
                'matches': matches,
                'chunk': i
            }

    return {'done': True}

@runner.message_emitter.on('partial_result')
def on_match(data):
    # Update UI with each match found
    for match in data['matches']:
        print(f"Found at: 0x{match:X}")
```

## Performance Tips

1. **Use ProcessPoolExecutor for CPU-bound work**
   - Bypasses Python's GIL
   - True parallel execution on multiple cores

2. **Use ThreadExecutor for I/O-bound work**
   - Lower overhead
   - Can share memory with main process

3. **Batch progress updates**
   - Don't yield on every iteration
   - Update every N items or every X seconds

4. **Return early when possible**
   - Don't process more than needed
   - Use `return` as soon as result is found

---

## üí° Why Use ida-taskr?

### ‚ùå Without ida-taskr (Manual Setup)

```python
from concurrent.futures import ProcessPoolExecutor

def analyze(data):
    def worker(data):
        # Your code
        return result

    executor = ProcessPoolExecutor(max_workers=4)
    try:
        future = executor.submit(worker, data)
        result = future.result(timeout=10)
        return result
    finally:
        executor.shutdown(wait=True)

# ~15 lines of boilerplate for EACH function!
```

### ‚úÖ With ida-taskr (Decorator)

```python
from ida_taskr import cpu_task

@cpu_task
def analyze(data):
    # Your code
    return result

# Just 1 line!
```

---

## ‚ö° Performance Benefits

### Shared Memory Pattern

**Traditional multiprocessing:**
- 8MB data √ó 8 processes = **64MB copied**
- High memory usage
- Slow serialization

**With `@shared_memory_task`:**
- 8MB data copied **ONCE**
- Workers attach via name (zero-copy)
- **~8x faster** for large data!

### Real-World Benchmark

```python
# Process 8MB binary in IDA
binary_data = ida_bytes.get_bytes(start_ea, 8 * 1024 * 1024)

# Traditional approach: ~500ms (copying + processing)
# With @shared_memory_task: ~60ms (no copying!)

@shared_memory_task(num_chunks=8)
def find_patterns(chunk_data, chunk_id, total_chunks):
    return analyze(chunk_data)

results = find_patterns(binary_data)  # 8x faster!
```

---

## üé® Common Patterns

### Pattern 1: Simple Background Task

```python
@cpu_task
def find_functions(data):
    return analyze(data)

future = find_functions(binary_data)
result = future.result()
```

**Lines: 1 decorator + your function**

### Pattern 2: Background Task with Callback

```python
@cpu_task(on_complete=show_results)
def find_functions(data):
    return analyze(data)

find_functions(binary_data)  # Auto-delivered to show_results()
```

**Lines: 1 decorator + callback**

### Pattern 3: Shared Memory for Large Data

```python
@shared_memory_task(num_chunks=8)
def analyze_chunk(chunk_data, chunk_id, total_chunks):
    return find_patterns(chunk_data)

results = analyze_chunk(binary_data)  # 8 chunks in parallel
```

**Lines: 1 decorator + chunk logic**

### Pattern 4: Batch Processing

```python
@parallel(max_workers=8)
def process_function(address):
    return analyze_function(address)

futures = [process_function(addr) for addr in function_list]
results = [f.result() for f in futures]
```

**Lines: 1 decorator + your function**

---

## üìä Complexity Reduction

| Pattern | Manual | With Decorator | Reduction |
|---------|--------|----------------|-----------|
| **Simple task** | ~15 lines | 1 line | **15x** |
| **Shared memory** | ~40-50 lines | ~5 lines | **8-10x** |
| **With callbacks** | ~20 lines | 2 lines | **10x** |

---

## üöÄ Running the Examples

### Prerequisites

```bash
# Install ida-taskr
pip install -e .

# Install Qt (PyQt5 for IDA ‚â§9.1, PySide6 for IDA ‚â•9.2)
pip install PyQt5  # or PySide6
```

### Running Examples Standalone

```bash
# Minimal API examples
python examples/one_line_solution.py
python examples/ultra_minimal.py
python examples/api_simplicity_levels.py

# Shared memory examples
python examples/shared_memory_one_line.py
python examples/shared_memory_decorator_example.py

# See all API levels
python examples/api_simplicity_levels.py
```

### Running in IDA Pro

In IDA's Python console:

```python
# Load example
import sys
sys.path.insert(0, '/path/to/ida-taskr/examples')

# Use decorator
from ida_taskr import cpu_task

@cpu_task
def analyze_current_function():
    import ida_bytes
    ea = idc.here()
    data = ida_bytes.get_bytes(ea, 1024)
    # Your analysis
    return result

future = analyze_current_function()
result = future.result()
```

Or use `File ‚Üí Script file...` to run example files directly.

---

## üîß Debugging

Enable debug logging:

```python
import logging
from ida_taskr import get_logger

logger = get_logger()
logger.setLevel(logging.DEBUG)
```

---

## üìù Summary

### Start With These Files

1. **[one_line_solution.py](one_line_solution.py)** - Simplest background task (1 line!)
2. **[shared_memory_one_line.py](shared_memory_one_line.py)** - Simplest shared memory (~5 lines)
3. **[decorator_simple_example.py](decorator_simple_example.py)** - Real IDA example

### Then Read the Guides

- **[README_MINIMAL_API.md](README_MINIMAL_API.md)** - Complete decorator documentation
- **[README_SHARED_MEMORY.md](README_SHARED_MEMORY.md)** - Complete shared memory guide

### Complexity Levels

| What You Write | Manual Setup | With Decorator |
|----------------|--------------|----------------|
| **Simple task** | ~15 lines | **1 line** |
| **Shared memory** | ~40 lines | **~5 lines** |
| **With callbacks** | ~20 lines | **2 lines** |

---

## üéØ Choosing the Right Approach

| Scenario | Use | Example File |
|----------|-----|--------------|
| **Simple background task** | `@cpu_task` | [one_line_solution.py](one_line_solution.py) |
| **Large binary data (8MB+)** | `@shared_memory_task` | [shared_memory_one_line.py](shared_memory_one_line.py) |
| **Need progress updates** | `TaskRunner` | simple_progress_example.py |
| **Parallel batch processing** | `@parallel` | [decorator_simple_example.py](decorator_simple_example.py) |
| **Full control** | `@background_task` | [api_simplicity_levels.py](api_simplicity_levels.py) |

---

## üìö Next Steps

1. **Quick start:** Run [one_line_solution.py](one_line_solution.py) to see the minimal API
2. **Learn levels:** Run [api_simplicity_levels.py](api_simplicity_levels.py) to see all options
3. **Large data:** Read [README_SHARED_MEMORY.md](README_SHARED_MEMORY.md) for shared memory
4. **Deep dive:** Read [README_MINIMAL_API.md](README_MINIMAL_API.md) for complete reference
5. **Real examples:** Check [decorator_simple_example.py](decorator_simple_example.py) and [ida_shared_memory_pattern.py](ida_shared_memory_pattern.py)

---

**Happy coding! üéâ**

```

`examples/README_MINIMAL_API.md`:

```md
# The Smallest Amount: Minimal API Guide

## Question: What's the smallest amount of annotations and decorators to use ida-taskr?

**Answer: Just `@cpu_task`**

That's it. One line.

---

## Comparison

### Without Decorator (Manual Setup)

```python
from ida_taskr import ProcessPoolExecutor

def analyze_binary(data):
    """Old way - requires manual executor setup."""

    def worker(data):
        # Your CPU-intensive code
        result = []
        for byte in data[:32]:
            result.append(byte)
        return result

    # Manual boilerplate (10+ lines)
    executor = ProcessPoolExecutor(max_workers=4)
    future = executor.submit(worker, data)
    result = future.result()
    executor.shutdown()

    return result

# Result: ~10 lines of boilerplate code
```

### With Decorator (MINIMAL)

```python
from ida_taskr import cpu_task

@cpu_task
def analyze_binary(data):
    """New way - just add @cpu_task"""

    # Your CPU-intensive code
    result = []
    for byte in data[:32]:
        result.append(byte)
    return result

# Result: Just 1 line! (@cpu_task)
```

---

## Usage

### Basic (Fire and Forget)

```python
from ida_taskr import cpu_task

@cpu_task
def analyze(data):
    # Your code
    return result

# Call it - returns immediately!
future = analyze(data)

# Get result when needed
result = future.result()
```

### With Callback (Even Simpler)

```python
@cpu_task(on_complete=lambda r: print(f"Done: {r}"))
def analyze(data):
    # Your code
    return result

# Just call it - callback fires automatically!
analyze(data)
```

### With Error Handling

```python
@cpu_task(
    on_complete=lambda r: print(f"‚úì {r}"),
    on_error=lambda e: print(f"‚úó {e}")
)
def analyze(data):
    # Your code
    return result

analyze(data)
```

---

## Real-World Example: IDA Binary Analysis

### Without Decorator

```python
def find_signatures(start_ea, end_ea):
    """Find signatures in binary section - old way."""
    import ida_bytes
    from ida_taskr import ProcessPoolExecutor

    # Get data from IDA
    data = ida_bytes.get_bytes(start_ea, end_ea - start_ea)

    # Worker function
    def worker(data):
        signatures = []
        # ... complex analysis ...
        return signatures

    # Manual executor management
    executor = ProcessPoolExecutor(max_workers=4)
    future = executor.submit(worker, data)
    result = future.result()
    executor.shutdown()

    return result

# ~15 lines of code
```

### With Decorator (MINIMAL)

```python
from ida_taskr import cpu_task

@cpu_task
def find_signatures_worker(data):
    """Find signatures - worker function."""
    signatures = []
    # ... complex analysis ...
    return signatures

def find_signatures(start_ea, end_ea):
    """Find signatures in binary section - new way."""
    import ida_bytes

    # Get data from IDA
    data = ida_bytes.get_bytes(start_ea, end_ea - start_ea)

    # Run in background - one line!
    future = find_signatures_worker(data)

    return future.result()

# Just 1 decorator line!
```

---

## Summary: API Levels

| Level | Code | Lines |
|-------|------|-------|
| **Manual** | `ProcessPoolExecutor(...)` | ~10-15 lines |
| **Minimal** | `@cpu_task` | **1 line** ‚Üê This is it! |
| **With Callback** | `@cpu_task(on_complete=...)` | 1 line + callback |
| **Full Featured** | `@cpu_task(on_complete=..., on_error=...)` | 2-3 lines |

---

## The Answer

**The smallest amount of annotations and decorators:**

```python
@cpu_task
```

That's it. One line. Done.

---

## Examples in This Directory

- **`ultra_minimal.py`** - The absolute simplest example (just `@cpu_task`)
- **`minimal_decorator_api.py`** - Before/after comparison
- **`api_simplicity_levels.py`** - All levels from verbose to minimal
- **`decorator_simple_example.py`** - Simple real-world example
- **`decorator_evolution.py`** - Step-by-step evolution from blocking to async

Start with `ultra_minimal.py` to see the smallest possible usage.

```

`examples/README_SHARED_MEMORY.md`:

```md
# Minimal Shared Memory API

## Question: What's the smallest surface area for shared memory tasks?

**Answer: Just write the chunk processing logic!**

---

## The Decorator

```python
@shared_memory_task(num_chunks=8)
def process_chunk(chunk_data, chunk_id, total_chunks):
    # Your chunk processing logic
    return result
```

That's it. **User writes ~5 lines**. ida-taskr handles ~40 lines of boilerplate!

---

## What User Provides

**Just the chunk processing function:**

```python
from ida_taskr import shared_memory_task

@shared_memory_task(num_chunks=8)
def analyze_chunk(chunk_data, chunk_id, total_chunks):
    """
    Process one chunk of data.

    Args:
        chunk_data: memoryview of this chunk (no copying!)
        chunk_id: 0-based chunk index (0 to 7)
        total_chunks: total number of chunks (8)

    Returns:
        Results from processing this chunk
    """
    signatures = []

    for i in range(len(chunk_data) - 16):
        if chunk_data[i] == 0x48:  # Pattern
            sig = bytes(chunk_data[i:i+16])
            signatures.append(sig.hex())

    return signatures
```

**Usage - just pass the full data:**

```python
# Get binary data (e.g., from IDA)
binary_data = get_binary_data()  # 8MB

# Process it - returns list of results from all 8 chunks
all_results = analyze_chunk(binary_data)

# That's it!
```

**Total user code: ~10 lines**

---

## What ida-taskr Handles

When you use `@shared_memory_task`, ida-taskr automatically:

1. **Creates shared memory** segment
2. **Copies data once** into shared memory (no per-chunk copying!)
3. **Calculates chunk boundaries** (splits data into N equal chunks)
4. **Creates worker processes** (ProcessPoolExecutor with N workers)
5. **Attaches workers to shared memory** (via shm.name, no data transfer!)
6. **Submits all chunks** with proper offsets
7. **Collects results** from all workers
8. **Cleanup memoryview** before closing (CRITICAL!)
9. **Cleanup shared memory** (close and unlink)

**Total boilerplate handled: ~40 lines**

---

## Comparison

### Without Decorator (Manual Setup)

```python
def analyze_manual(binary_data):
    import multiprocessing.shared_memory
    from ida_taskr import ProcessPoolExecutor

    # 1. Create shared memory
    shm = multiprocessing.shared_memory.SharedMemory(
        create=True, size=len(binary_data)
    )

    try:
        # 2. Copy data
        shm.buf[:] = binary_data

        # 3. Calculate chunks
        num_chunks = 8
        chunk_size = len(binary_data) // num_chunks

        # 4. Define worker with attachment logic
        def worker(shm_name, start, end, chunk_id):
            shm = multiprocessing.shared_memory.SharedMemory(name=shm_name)
            try:
                chunk_data = memoryview(shm.buf)[start:end]
                # ... process chunk ...
                return result
            finally:
                del chunk_data  # CRITICAL!
                shm.close()

        # 5. Submit chunks
        executor = ProcessPoolExecutor(max_workers=8)
        futures = []
        for i in range(num_chunks):
            start = i * chunk_size
            end = start + chunk_size if i < num_chunks - 1 else len(binary_data)
            future = executor.submit(worker, shm.name, start, end, i)
            futures.append(future)

        # 6. Collect results
        results = [f.result() for f in futures]

        # 7. Cleanup executor
        executor.shutdown()

    finally:
        # 8. Cleanup shared memory
        shm.close()
        shm.unlink()

    return results

# ~40-50 lines of boilerplate!
```

### With Decorator (MINIMAL)

```python
from ida_taskr import shared_memory_task

@shared_memory_task(num_chunks=8)
def analyze_minimal(chunk_data, chunk_id, total_chunks):
    # ... process chunk ...
    return result

# Usage
results = analyze_minimal(binary_data)

# ~5-10 lines total!
```

---

## Real-World IDA Example

```python
import ida_bytes
from ida_taskr import shared_memory_task

@shared_memory_task(num_chunks=16)
def find_function_prologues(chunk_data, chunk_id, total_chunks):
    """Find x64 function prologues in this chunk."""
    patterns = []

    # Common prologues
    prologue = b'\x55\x48\x89\xe5'  # push rbp; mov rbp, rsp

    for i in range(len(chunk_data) - 4):
        if chunk_data[i:i+4] == prologue:
            patterns.append({
                'chunk': chunk_id,
                'offset': i,
                'pattern': 'push_rbp_mov_rbp_rsp'
            })

    return patterns


def analyze_current_segment():
    """Analyze the current IDA segment."""
    # Get binary data from IDA
    start_ea = idc.get_segm_start(idc.here())
    end_ea = idc.get_segm_end(idc.here())
    binary_data = ida_bytes.get_bytes(start_ea, end_ea - start_ea)

    print(f"Analyzing {len(binary_data):,} bytes in 16 parallel chunks...")

    # Process in parallel - just one line!
    all_patterns = find_function_prologues(binary_data)

    # Show results
    total = sum(len(chunk) for chunk in all_patterns)
    print(f"Found {total} function prologues!")

    return all_patterns
```

**User writes:**
- Chunk processing logic (~10 lines)
- Usage code (~3 lines)

**ida-taskr handles:**
- All shared memory complexity (~40 lines)

---

## Why Shared Memory?

**Problem:** IDA takes locks on main thread. Can't call IDA SDK from workers.

**Solution:** Copy data from IDA once, then workers never touch IDA!

```python
# In IDA's main thread (has the locks)
binary_data = ida_bytes.get_bytes(start_ea, size)  # Get data ONCE

# Workers process shared memory (no IDA SDK calls needed!)
@shared_memory_task(num_chunks=8)
def analyze(chunk_data, chunk_id, total_chunks):
    # No IDA SDK calls here!
    # Just process the raw bytes
    return find_patterns(chunk_data)

# True parallel execution across all CPU cores
results = analyze(binary_data)
```

**Benefits:**
- ‚úì Copy data ONCE (not per chunk)
- ‚úì No serialization overhead
- ‚úì True parallel processing
- ‚úì Workers never touch IDA (no lock issues)
- ‚úì Can process GB of data efficiently

---

## Summary

| Aspect | Manual | With Decorator |
|--------|--------|----------------|
| **User code** | ~40-50 lines | ~5-10 lines |
| **Boilerplate** | All manual | Automated |
| **SharedMemory** | Manual create/cleanup | Automatic |
| **Chunking** | Manual math | Automatic |
| **Worker attachment** | Manual | Automatic |
| **Cleanup** | Manual (easy to forget!) | Automatic |

---

## The Answer

**Smallest surface area for shared memory tasks:**

```python
@shared_memory_task(num_chunks=N)
def process_chunk(chunk_data, chunk_id, total_chunks):
    # Your logic here
    return result
```

**User writes:** Just the chunk processing logic (~5-10 lines)

**ida-taskr handles:** Everything else (~40 lines of boilerplate)

---

## Examples in This Directory

- **`shared_memory_one_line.py`** - Absolute minimal example
- **`shared_memory_minimal.py`** - Before/after comparison
- **`shared_memory_comparison.py`** - Detailed comparison with IDA examples
- **`shared_memory_decorator_example.py`** - Working example with real data

Start with `shared_memory_one_line.py` to see the smallest possible usage.

```

`examples/api_simplicity_levels.py`:

```py
"""
API Simplicity Levels - From Complex to Ultra-Simple

Shows different levels of API complexity, from most verbose to most minimal.
"""

import time


# ==============================================================================
# Level 0: Raw ProcessPoolExecutor (Most Verbose)
# ==============================================================================

def level_0_raw_executor():
    """
    Level 0: Raw ProcessPoolExecutor

    ~15 lines of setup code
    """
    import multiprocessing
    from concurrent.futures import ProcessPoolExecutor

    def worker_function(data):
        result = []
        for i in range(min(16, len(data))):
            result.append(data[i])
        return result

    # Manual executor management
    executor = ProcessPoolExecutor(max_workers=4)

    try:
        # Submit task
        future = executor.submit(worker_function, bytes(range(256)))

        # Wait for result
        result = future.result(timeout=10)

        return result

    finally:
        # Cleanup
        executor.shutdown(wait=True)


# ==============================================================================
# Level 1: Using ida-taskr's ProcessPoolExecutor (Less Verbose)
# ==============================================================================

def level_1_ida_taskr_executor():
    """
    Level 1: Using ida-taskr's ProcessPoolExecutor

    ~8 lines (handles Qt integration)
    """
    from ida_taskr import ProcessPoolExecutor

    def worker_function(data):
        result = []
        for i in range(min(16, len(data))):
            result.append(data[i])
        return result

    executor = ProcessPoolExecutor(max_workers=4)
    future = executor.submit(worker_function, bytes(range(256)))
    result = future.result()
    executor.shutdown()

    return result


# ==============================================================================
# Level 2: Simple decorator with no options (Minimal)
# ==============================================================================

from ida_taskr import cpu_task

@cpu_task
def level_2_simple_decorator(data):
    """
    Level 2: Just @cpu_task decorator

    1 line decorator + your function
    """
    result = []
    for i in range(min(16, len(data))):
        result.append(data[i])
    return result

# Usage:
# future = level_2_simple_decorator(data)  # Returns immediately
# result = future.result()


# ==============================================================================
# Level 3: Decorator with callback (Simple + Convenient)
# ==============================================================================

@cpu_task(on_complete=lambda r: print(f"Done: {len(r)} bytes"))
def level_3_with_callback(data):
    """
    Level 3: Decorator with callback

    1 line decorator + callback function
    """
    result = []
    for i in range(min(16, len(data))):
        result.append(data[i])
    return result

# Usage:
# level_3_with_callback(data)  # Fire and forget!


# ==============================================================================
# Level 4: Full featured (Everything)
# ==============================================================================

@cpu_task(
    on_complete=lambda r: print(f"‚úì Done: {len(r)} bytes"),
    on_error=lambda e: print(f"‚úó Error: {e}"),
)
def level_4_full_featured(data):
    """
    Level 4: All the bells and whistles

    Multi-line decorator with all options
    """
    result = []
    for i in range(min(16, len(data))):
        result.append(data[i])
    return result


# ==============================================================================
# Comparison
# ==============================================================================

def show_comparison():
    """Show all levels side by side."""

    print("=" * 70)
    print("API Simplicity Levels")
    print("=" * 70)
    print()

    levels = [
        ("Level 0: Raw ProcessPoolExecutor", """
def analyze(data):
    from concurrent.futures import ProcessPoolExecutor

    def worker(data):
        # Your code
        ...

    executor = ProcessPoolExecutor(max_workers=4)
    try:
        future = executor.submit(worker, data)
        result = future.result(timeout=10)
        return result
    finally:
        executor.shutdown(wait=True)

# ~15 lines of boilerplate
"""),

        ("Level 1: ida-taskr ProcessPoolExecutor", """
def analyze(data):
    from ida_taskr import ProcessPoolExecutor

    def worker(data):
        # Your code
        ...

    executor = ProcessPoolExecutor(max_workers=4)
    future = executor.submit(worker, data)
    result = future.result()
    executor.shutdown()
    return result

# ~8 lines (better, but still manual)
"""),

        ("Level 2: @cpu_task (MINIMAL)", """
from ida_taskr import cpu_task

@cpu_task
def analyze(data):
    # Your code
    ...

future = analyze(data)
result = future.result()

# Just 1 line! (@cpu_task)
# This is the SMALLEST amount!
"""),

        ("Level 3: @cpu_task with callback", """
@cpu_task(on_complete=lambda r: print(r))
def analyze(data):
    # Your code
    ...

analyze(data)  # Fire and forget!

# Still simple, results auto-delivered
"""),

        ("Level 4: Full featured", """
@cpu_task(
    on_complete=show_result,
    on_error=show_error,
)
def analyze(data):
    # Your code
    ...

analyze(data)

# All features, still simple!
"""),
    ]

    for title, code in levels:
        print(title)
        print("-" * 70)
        print(code)
        print()

    print("=" * 70)
    print("ANSWER: The smallest amount is Level 2 - just @cpu_task")
    print("=" * 70)
    print()
    print("@cpu_task  ‚Üê That's it. One line. Done.")
    print()


def run_demo():
    """Actually run the different levels to show they work."""
    data = bytes(range(256)) * 100

    print("\n" + "=" * 70)
    print("Running Each Level")
    print("=" * 70)
    print()

    # Level 2: Minimal
    print("Level 2 (Minimal @cpu_task):")
    print("-" * 70)
    future = level_2_simple_decorator(data)
    print(f"  ‚úì Returns immediately: {type(future).__name__}")
    result = future.result(timeout=5)
    print(f"  ‚úì Got result: {len(result)} bytes")
    print()

    # Level 3: With callback
    print("Level 3 (With callback):")
    print("-" * 70)
    print("  ", end="")
    level_3_with_callback(data).result(timeout=5)  # Callback fires
    print()

    print("=" * 70)
    print("All levels work - Level 2 is the simplest!")
    print("=" * 70)


if __name__ == '__main__':
    show_comparison()
    run_demo()

```

`examples/decorator_evolution.py`:

```py
"""
Example: Evolution from simple function to decorated async task

Shows how decorators can simplify the API step by step.
"""

import time


# ==============================================================================
# STEP 1: Just the function (no async, blocks main thread)
# ==============================================================================

def generate_signature_v1(address, data, max_length=32):
    """Simple function - blocks the caller."""
    signature = []

    for i in range(min(max_length, len(data))):
        signature.append(data[i])
        time.sleep(0.01)  # Simulate CPU work

    return {
        'address': address,
        'pattern': ' '.join(f'{b:02X}' for b in signature[:16])
    }

# Usage:
# result = generate_signature_v1(0x401000, data)  # ‚Üê BLOCKS for entire duration!
# print(result['pattern'])


# ==============================================================================
# STEP 2: Add @background_task decorator (simplest async)
# ==============================================================================

from ida_taskr import background_task

@background_task
def generate_signature_v2(address, data, max_length=32):
    """
    Same function, but with @background_task decorator.

    Now runs in background process automatically!
    """
    signature = []

    for i in range(min(max_length, len(data))):
        signature.append(data[i])
        time.sleep(0.01)

    return {
        'address': address,
        'pattern': ' '.join(f'{b:02X}' for b in signature[:16])
    }

# Usage:
# future = generate_signature_v2(0x401000, data)  # ‚Üê Returns immediately!
# result = future.result()  # Wait for result when needed
# print(result['pattern'])


# ==============================================================================
# STEP 3: Add callback for when task completes
# ==============================================================================

@background_task(on_complete=lambda result: print(f"Done: {result['pattern']}"))
def generate_signature_v3(address, data, max_length=32):
    """With automatic callback on completion."""
    signature = []

    for i in range(min(max_length, len(data))):
        signature.append(data[i])
        time.sleep(0.01)

    return {
        'address': address,
        'pattern': ' '.join(f'{b:02X}' for b in signature[:16])
    }

# Usage:
# generate_signature_v3(0x401000, data)  # ‚Üê Returns immediately, calls callback when done!


# ==============================================================================
# STEP 4: Add progress reporting
# ==============================================================================

@background_task(
    on_progress=lambda progress, msg: print(f"[{progress}%] {msg}"),
    on_complete=lambda result: print(f"‚úì Done: {result['pattern']}")
)
def generate_signature_v4(address, data, max_length=32, progress_callback=None):
    """With progress reporting."""
    signature = []
    total = min(max_length, len(data))

    for i in range(total):
        signature.append(data[i])
        time.sleep(0.01)

        # Report progress
        if progress_callback and i % 4 == 0:
            progress = int((i + 1) / total * 100)
            progress_callback(progress, f"Processing byte {i+1}/{total}")

    return {
        'address': address,
        'pattern': ' '.join(f'{b:02X}' for b in signature[:16])
    }

# Usage:
# generate_signature_v4(0x401000, data)  # ‚Üê Progress updates appear automatically!


# ==============================================================================
# STEP 5: Multiple concurrent tasks
# ==============================================================================

@background_task(max_workers=4, on_complete=lambda r: print(f"‚úì 0x{r['address']:X}"))
def generate_signature_v5(address, data, max_length=32):
    """Can run multiple in parallel."""
    signature = []

    for i in range(min(max_length, len(data))):
        signature.append(data[i])
        time.sleep(0.01)

    return {
        'address': address,
        'pattern': ' '.join(f'{b:02X}' for b in signature[:16])
    }

# Usage:
# # Submit multiple - all run in parallel!
# f1 = generate_signature_v5(0x401000, data1)
# f2 = generate_signature_v5(0x402000, data2)
# f3 = generate_signature_v5(0x403000, data3)
# f4 = generate_signature_v5(0x404000, data4)


# ==============================================================================
# STEP 6: Full featured with error handling
# ==============================================================================

@background_task(
    max_workers=4,
    on_complete=lambda r: print(f"‚úì Signature: {r['pattern']}"),
    on_error=lambda e: print(f"‚úó Error: {e}"),
    on_progress=lambda p, m: print(f"[{p}%] {m}"),
    timeout=60.0  # Optional timeout
)
def generate_signature_v6(address, data, max_length=32, progress_callback=None):
    """Full featured with all callbacks."""
    signature = []
    total = min(max_length, len(data))

    for i in range(total):
        signature.append(data[i])
        time.sleep(0.01)

        if progress_callback and i % 4 == 0:
            progress = int((i + 1) / total * 100)
            progress_callback(progress, f"Analyzing byte {i+1}/{total}")

    return {
        'address': address,
        'pattern': ' '.join(f'{b:02X}' for b in signature[:16])
    }

# Usage:
# generate_signature_v6(0x401000, data)  # ‚Üê Everything handled automatically!


# ==============================================================================
# DEMO: Compare all versions
# ==============================================================================

def demo_evolution():
    """Show the evolution from blocking to async with decorators."""

    data = bytes(range(256))

    print("=" * 70)
    print("API Evolution: From Blocking to Background Tasks")
    print("=" * 70)
    print()

    # V1: Blocking
    print("V1: Blocking function (old way)")
    print("-" * 70)
    print("result = generate_signature_v1(address, data)")
    print("  ‚Üë Blocks for entire duration")
    print("  ‚Üë UI freezes")
    print()

    # V2: Background task
    print("V2: @background_task (simplest async)")
    print("-" * 70)
    print("@background_task")
    print("def generate_signature_v2(...):")
    print()
    print("future = generate_signature_v2(address, data)")
    print("  ‚Üë Returns immediately!")
    print("  ‚Üë UI stays responsive")
    print()

    # V3: With callback
    print("V3: Auto-callback on completion")
    print("-" * 70)
    print("@background_task(on_complete=lambda r: print(r))")
    print("def generate_signature_v3(...):")
    print()
    print("generate_signature_v3(address, data)")
    print("  ‚Üë Result delivered automatically via callback")
    print()

    # V4: With progress
    print("V4: Progress reporting")
    print("-" * 70)
    print("@background_task(")
    print("    on_progress=lambda p, m: update_ui(p, m),")
    print("    on_complete=lambda r: show_result(r)")
    print(")")
    print("def generate_signature_v4(...):")
    print("  ‚Üë Real-time progress updates")
    print()

    # V5: Parallel
    print("V5: Parallel execution")
    print("-" * 70)
    print("@background_task(max_workers=4)")
    print("def generate_signature_v5(...):")
    print()
    print("# All run in parallel:")
    print("f1 = generate_signature_v5(addr1, data1)")
    print("f2 = generate_signature_v5(addr2, data2)")
    print("f3 = generate_signature_v5(addr3, data3)")
    print("  ‚Üë Uses 4 CPU cores in parallel")
    print()

    # V6: Full featured
    print("V6: Full featured")
    print("-" * 70)
    print("@background_task(")
    print("    max_workers=4,")
    print("    on_complete=show_result,")
    print("    on_error=show_error,")
    print("    on_progress=update_ui,")
    print("    timeout=60.0")
    print(")")
    print("def generate_signature_v6(...):")
    print("  ‚Üë Everything handled automatically!")
    print()

    print("=" * 70)
    print("From 1 line change (@background_task) to full async!")
    print("=" * 70)


if __name__ == '__main__':
    demo_evolution()

```

`examples/decorator_simple_example.py`:

```py
"""
Super simple example using @cpu_task decorator

Shows how decorators make the API extremely simple.
"""

from ida_taskr import cpu_task
import time


# Just add @cpu_task decorator to any function!
@cpu_task(on_complete=lambda result: print(f"\n‚úì Done! Pattern: {result['pattern']}"))
def generate_signature(address, data, max_length=32):
    """
    Your CPU-intensive function - just write it normally!

    The @cpu_task decorator automatically:
    - Runs it in a separate process
    - Handles all Qt signal connections
    - Calls your callback when done
    """
    signature = []

    for i in range(min(max_length, len(data))):
        signature.append(data[i])
        time.sleep(0.01)  # Simulate CPU work

    return {
        'address': address,
        'pattern': ' '.join(f'{b:02X}' for b in signature[:16])
    }


def main():
    """Example usage."""
    print("=" * 60)
    print("Super Simple Decorator Example")
    print("=" * 60)
    print()

    # Just call it like a normal function!
    binary_data = bytes(range(256)) * 100

    print("Calling generate_signature(0x401000, data)...")
    print("(Returns immediately, runs in background)\n")

    # This returns a Future immediately - doesn't block!
    future = generate_signature(0x401000, binary_data)

    # Main thread keeps running
    print("Main thread is free:")
    for i in range(3):
        print(f"  Working... {i+1}/3")
        time.sleep(1)

    # Optional: wait for result
    result = future.result(timeout=10)

    print("\n‚úì Main thread was NEVER blocked!")
    print("=" * 60)


if __name__ == '__main__':
    main()

```

`examples/event_emitter_usage.py`:

```py
"""
Example showing how to use the MessageEmitter pattern for handling worker messages.

This demonstrates the composition-based approach which is flexible and easy to test.
"""

import logging

from ida_taskr import TaskRunner, get_logger

logger = get_logger(__name__)


def on_results(results):
    logger.info("‚úÖ Received results: %s", results)
    if results.get("status") == "success":
        data = results.get("results", [])
        logger.info("üéØ Processing %d result items", len(data))


def on_progress(progress, status):
    logger.info("üìä Progress: %.1f%% - %s", progress * 100, status)


def main():
    runner = TaskRunner(
        worker_script="path/to/your/worker.py",
        worker_args={"data_size": 1024, "start_ea": "0x1000", "is64": "1"},
        log_level=logging.DEBUG,
    )
    runner.on_results(on_results)
    runner.on_progress(on_progress)
    runner.start()


if __name__ == "__main__":
    main()

```

`examples/ida_shared_memory_pattern.py`:

```py
"""
IDA-Specific Pattern: Processing Large Sections with Shared Memory

Shows the exact pattern used by anti_deob to analyze large code sections
in IDA without blocking the UI.

Key Points:
- IDA's main thread holds the GIL and locks
- We copy binary data from IDA into shared memory ONCE
- Multiple worker processes analyze chunks in parallel
- Workers never touch IDA - they work on shared memory
- Results come back via Qt signals
"""

import multiprocessing
import multiprocessing.shared_memory
from typing import Callable, Any
from ida_taskr import ProcessPoolExecutor


# ==============================================================================
# Pattern 1: The anti_deob pattern (how it actually works)
# ==============================================================================

def analyze_chunk(shm_name: str, start: int, end: int, start_ea: int) -> dict:
    """
    Worker function that analyzes a chunk of binary data.

    This runs in a SEPARATE PROCESS - completely isolated from IDA's locks.

    Args:
        shm_name: Shared memory segment name
        start: Start offset in shared memory
        end: End offset in shared memory
        start_ea: Base address (for reporting results)

    Returns:
        Analysis results for this chunk
    """
    # Attach to shared memory
    shm = multiprocessing.shared_memory.SharedMemory(name=shm_name)

    try:
        # Get view of this chunk (NO COPYING!)
        chunk = memoryview(shm.buf)[start:end]

        # Analyze the chunk (this is where your CPU-intensive work goes)
        # Examples:
        # - Pattern matching
        # - Signature generation
        # - Opcode analysis
        # - String extraction
        # - Control flow analysis (on raw bytes)

        # Simple example: find patterns
        results = {
            'chunk_start': start,
            'chunk_end': end,
            'base_address': start_ea,
            'patterns_found': [],
            'statistics': {
                'total_bytes': len(chunk),
                'null_bytes': sum(1 for b in chunk if b == 0),
                'high_entropy_bytes': sum(1 for b in chunk if b > 0x7F),
            }
        }

        # Example: find simple patterns
        pattern = b'\x55\x8B\xEC'  # PUSH EBP; MOV EBP, ESP
        i = 0
        while i < len(chunk) - len(pattern):
            if bytes(chunk[i:i+len(pattern)]) == pattern:
                results['patterns_found'].append({
                    'offset': start + i,
                    'address': start_ea + start + i,
                    'pattern': 'function_prologue'
                })
            i += 1

        return results

    finally:
        del chunk  # Release memoryview BEFORE closing shm
        shm.close()


class BinaryAnalyzer:
    """
    Analyzes large binary sections using shared memory and parallel workers.

    This is the pattern used by DataProcessorCore in ida_taskr.
    """

    def __init__(self, max_workers: int = 4):
        """Initialize analyzer with worker pool."""
        self.executor = ProcessPoolExecutor(max_workers=max_workers)
        self._shm = None

    def analyze_section(
        self,
        binary_data: bytes,
        start_ea: int,
        chunk_processor: Callable = analyze_chunk,
        num_chunks: int = 8
    ) -> list:
        """
        Analyze a large binary section in parallel.

        Args:
            binary_data: Binary data from IDA (ida_bytes.get_bytes())
            start_ea: Start address in IDA
            chunk_processor: Function to process each chunk
            num_chunks: Number of chunks to split into

        Returns:
            List of results from all chunks
        """
        data_size = len(binary_data)
        chunk_size = data_size // num_chunks

        # Create shared memory and copy data ONCE
        self._shm = multiprocessing.shared_memory.SharedMemory(
            create=True,
            size=data_size
        )
        self._shm.buf[:data_size] = binary_data

        try:
            # Submit all chunks to worker pool
            futures = []

            for i in range(num_chunks):
                chunk_start = i * chunk_size
                chunk_end = chunk_start + chunk_size if i < num_chunks - 1 else data_size

                future = self.executor.submit(
                    chunk_processor,
                    self._shm.name,
                    chunk_start,
                    chunk_end,
                    start_ea
                )
                futures.append(future)

            # Collect results
            results = [f.result() for f in futures]

            return results

        finally:
            self.cleanup()

    def cleanup(self):
        """Clean up shared memory."""
        if self._shm:
            self._shm.close()
            self._shm.unlink()
            self._shm = None


# ==============================================================================
# Pattern 2: With progress updates (for IDA UI)
# ==============================================================================

class BinaryAnalyzerWithProgress(BinaryAnalyzer):
    """Analyzer with real-time progress updates via Qt signals."""

    def analyze_section_with_progress(
        self,
        binary_data: bytes,
        start_ea: int,
        on_progress: Callable[[int, str], None] = None,
        on_chunk_done: Callable[[dict], None] = None,
        num_chunks: int = 8
    ) -> list:
        """
        Analyze with progress callbacks.

        Args:
            binary_data: Binary data from IDA
            start_ea: Start address
            on_progress: Called with (progress%, message)
            on_chunk_done: Called with chunk result
            num_chunks: Number of chunks

        Returns:
            List of all results
        """
        data_size = len(binary_data)
        chunk_size = data_size // num_chunks

        # Create shared memory
        self._shm = multiprocessing.shared_memory.SharedMemory(
            create=True,
            size=data_size
        )
        self._shm.buf[:data_size] = binary_data

        try:
            # Track completion
            completed = []

            def on_complete(future):
                result = future.result()
                completed.append(result)

                progress = int(len(completed) / num_chunks * 100)
                if on_progress:
                    on_progress(
                        progress,
                        f"Analyzed {len(completed)}/{num_chunks} chunks"
                    )

                if on_chunk_done:
                    on_chunk_done(result)

            self.executor.signals.task_completed.connect(on_complete)

            # Submit chunks
            futures = []
            for i in range(num_chunks):
                chunk_start = i * chunk_size
                chunk_end = chunk_start + chunk_size if i < num_chunks - 1 else data_size

                future = self.executor.submit(
                    analyze_chunk,
                    self._shm.name,
                    chunk_start,
                    chunk_end,
                    start_ea
                )
                futures.append(future)

            # Wait for all
            for f in futures:
                f.result()

            return completed

        finally:
            self.cleanup()


# ==============================================================================
# Pattern 3: Real IDA usage example
# ==============================================================================

def ida_usage_example():
    """
    Example of how to use this in an actual IDA plugin.

    This would be in your IDA plugin code.
    """
    try:
        import idaapi
        import ida_bytes
        import ida_funcs
    except ImportError:
        print("This example requires IDA Pro")
        return

    # Get binary data from IDA (this is the ONLY IDA interaction)
    start_ea = 0x401000
    end_ea = 0xC01000  # 8MB section

    # Copy data from IDA into Python bytes (releases IDA's locks)
    binary_data = ida_bytes.get_bytes(start_ea, end_ea - start_ea)

    # Now we're free from IDA's main thread!
    # Process in parallel without blocking UI

    analyzer = BinaryAnalyzerWithProgress(max_workers=8)

    def on_progress(progress, msg):
        # Update IDA's wait box
        idaapi.replace_wait_box(f"{msg} ({progress}%)")

    def on_chunk_done(result):
        # Show partial results as they come in
        for pattern in result['patterns_found']:
            print(f"Found pattern at 0x{pattern['address']:X}")

    # This doesn't block!
    results = analyzer.analyze_section_with_progress(
        binary_data,
        start_ea,
        on_progress=on_progress,
        on_chunk_done=on_chunk_done,
        num_chunks=16
    )

    # Process combined results
    total_patterns = sum(len(r['patterns_found']) for r in results)
    print(f"Total patterns found: {total_patterns}")


# ==============================================================================
# Pattern 4: Helper functions (like anti_deob utils)
# ==============================================================================

def shm_buffer(name: str, buf_len: int = None):
    """
    Context manager for accessing shared memory.

    Usage:
        with shm_buffer(shm_name, buf_len) as buf:
            # Work with buf
            data = bytes(buf)
    """
    shm = multiprocessing.shared_memory.SharedMemory(name=name)
    try:
        yield shm.buf[:buf_len] if buf_len else shm
    finally:
        shm.close()


def execute_chunk_with_shm(
    chunk_processor: Callable,
    shm_name: str,
    chunk_start: int,
    chunk_end: int,
    *args
) -> Any:
    """
    Execute a chunk processor with proper cleanup.

    This ensures memoryview is deleted before shared memory closes,
    preventing "BufferError: cannot close exported pointers exist".

    Args:
        chunk_processor: Function that takes (memoryview, *args)
        shm_name: Shared memory name
        chunk_start: Start offset
        chunk_end: End offset
        *args: Extra args for processor

    Returns:
        Result from chunk_processor
    """
    with shm_buffer(shm_name) as shm_obj:
        # Create memoryview of just this chunk
        chunk_mv = memoryview(shm_obj.buf)[chunk_start:chunk_end]

        try:
            # Process the chunk
            result = chunk_processor(chunk_mv, *args)
            return result
        finally:
            # CRITICAL: Delete memoryview before shm closes
            del chunk_mv


# ==============================================================================
# Demo
# ==============================================================================

def main():
    print("=" * 70)
    print("IDA Shared Memory Pattern (anti_deob style)")
    print("=" * 70)
    print()

    # Simulate 8MB binary from IDA
    binary_data = bytes(range(256)) * (32 * 1024)
    start_ea = 0x401000

    print(f"Analyzing {len(binary_data):,} bytes starting at 0x{start_ea:X}")
    print()

    # Analyze with progress
    analyzer = BinaryAnalyzerWithProgress(max_workers=4)

    def show_progress(progress, msg):
        print(f"[{progress:3d}%] {msg}")

    def show_chunk_result(result):
        patterns = len(result['patterns_found'])
        if patterns > 0:
            print(f"  ‚Üí Found {patterns} patterns in chunk "
                  f"[0x{result['chunk_start']:X}:0x{result['chunk_end']:X}]")

    results = analyzer.analyze_section_with_progress(
        binary_data,
        start_ea,
        on_progress=show_progress,
        on_chunk_done=show_chunk_result,
        num_chunks=8
    )

    print()
    print("=" * 70)
    print("Summary:")
    print("=" * 70)

    total_patterns = sum(len(r['patterns_found']) for r in results)
    total_null = sum(r['statistics']['null_bytes'] for r in results)

    print(f"Total patterns found: {total_patterns}")
    print(f"Total null bytes: {total_null}")
    print()
    print("‚úì IDA's UI was never blocked!")
    print("‚úì True parallel processing (8 workers)")
    print("‚úì Zero data copying between processes (shared memory)")
    print("=" * 70)


if __name__ == '__main__':
    main()

```

`examples/interpreter_pool_example.py`:

```py
"""
Example demonstrating InterpreterPoolExecutor usage.

This mimics the pattern from the aiointerpreters blog post:
https://github.com/Jamie-Chang/Jamie-Blog/blob/main/content/aiointerpreters.md

InterpreterPoolExecutor provides the same API as Python 3.13+'s
concurrent.futures.InterpreterPoolExecutor, but uses ProcessPoolExecutor
as the backend for compatibility with embedded Python contexts (like IDA Pro).
"""

from ida_taskr import InterpreterPoolExecutor


# Simple CPU-bound function that can be executed in parallel
def sums(num: int) -> int:
    """Compute sum of squares from 0 to num."""
    return sum(i * i for i in range(num + 1))


def main():
    """Demonstrate InterpreterPoolExecutor usage patterns."""

    # Pattern 1: Basic map() usage (from the blog post)
    print("Pattern 1: Using map()")
    with InterpreterPoolExecutor() as executor:
        results = list(executor.map(sums, [100_000] * 4))
        print(f"Results: {results}")

    # Pattern 2: Using submit() with as_completed
    print("\nPattern 2: Using submit() with as_completed")
    import concurrent.futures

    with InterpreterPoolExecutor(max_workers=4) as executor:
        futures = [executor.submit(sums, 50_000) for _ in range(4)]
        for future in concurrent.futures.as_completed(futures):
            result = future.result()
            print(f"Completed: {result}")

    # Pattern 3: With Qt signal integration
    print("\nPattern 3: With Qt signals")
    from ida_taskr import InterpreterPoolExecutor

    def on_completed(future):
        print(f"Signal: Task completed with result {future.result()}")

    def on_failed(future, exc):
        print(f"Signal: Task failed with {exc}")

    executor = InterpreterPoolExecutor(max_workers=2)
    executor.signals.task_completed.connect(on_completed)
    executor.signals.task_failed.connect(on_failed)

    future = executor.submit(sums, 10_000)
    result = future.result()  # Wait for completion
    print(f"Direct result: {result}")

    executor.shutdown(wait=True)

    print("\nAll patterns demonstrated successfully!")


if __name__ == "__main__":
    main()

```

`examples/minimal_decorator_api.py`:

```py
"""
Minimal Decorator API - The Smallest Amount of Code Possible

Shows the absolute simplest way to use ida-taskr decorators.
"""

import time
from ida_taskr import cpu_task


# ==============================================================================
# BEFORE: Without decorator (manual setup - lots of boilerplate)
# ==============================================================================

def analyze_binary_manual(data):
    """Old way: Manual ProcessPoolExecutor setup."""
    from ida_taskr import ProcessPoolExecutor

    def worker(data):
        # Do CPU work
        result = []
        for i in range(min(16, len(data))):
            result.append(data[i])
            time.sleep(0.01)
        return result

    # Manual setup (boilerplate)
    executor = ProcessPoolExecutor(max_workers=4)
    future = executor.submit(worker, data)
    result = future.result()
    executor.shutdown()

    return result


# ==============================================================================
# AFTER: With minimal decorator (ONE LINE!)
# ==============================================================================

@cpu_task
def analyze_binary(data):
    """
    New way: Just add @cpu_task

    That's it. One line. Done.
    """
    result = []
    for i in range(min(16, len(data))):
        result.append(data[i])
        time.sleep(0.01)
    return result


# ==============================================================================
# Usage Comparison
# ==============================================================================

def demo():
    data = bytes(range(256)) * 100

    print("=" * 70)
    print("Minimal Decorator API")
    print("=" * 70)
    print()

    # Without decorator: ~8 lines of boilerplate
    print("WITHOUT decorator (manual):")
    print("-" * 70)
    print("""
def analyze_binary_manual(data):
    from ida_taskr import ProcessPoolExecutor

    def worker(data):
        # Your code here
        ...

    executor = ProcessPoolExecutor(max_workers=4)
    future = executor.submit(worker, data)
    result = future.result()
    executor.shutdown()
    return result

# 8+ lines of boilerplate!
""")

    # With decorator: 1 line!
    print("\nWITH decorator (minimal):")
    print("-" * 70)
    print("""
@cpu_task
def analyze_binary(data):
    # Your code here
    ...

# Just 1 line!
""")

    print("\n" + "=" * 70)
    print("That's the smallest amount: @cpu_task")
    print("=" * 70)
    print()

    # Show it actually works
    print("Running with decorator:")
    future = analyze_binary(data)
    print(f"  ‚Üí Returns immediately: {type(future)}")
    print(f"  ‚Üí Main thread free!")

    result = future.result(timeout=5)
    print(f"  ‚Üí Got result: {len(result)} bytes")
    print()


if __name__ == '__main__':
    demo()

```

`examples/one_line_solution.py`:

```py
"""
ONE LINE SOLUTION

The smallest amount of code to make a function run in the background.
"""

# ==============================================================================
# BEFORE: Blocks the main thread
# ==============================================================================

def analyze_before(data):
    # Your CPU-intensive work
    result = []
    for i in range(len(data)):
        result.append(data[i] * 2)  # Expensive computation
    return result

# Problem: Calling this blocks the main thread!
# result = analyze_before(data)  ‚Üê UI freezes during execution


# ==============================================================================
# AFTER: Add @cpu_task (ONE LINE!)
# ==============================================================================

from ida_taskr import cpu_task

@cpu_task  # ‚Üê That's it. Just add this line.
def analyze_after(data):
    # Your CPU-intensive work (SAME CODE!)
    result = []
    for i in range(len(data)):
        result.append(data[i] * 2)  # Expensive computation
    return result

# Solution: Calling this returns immediately!
# future = analyze_after(data)  ‚Üê UI stays responsive
# result = future.result()      ‚Üê Get result when ready


# ==============================================================================
# Summary
# ==============================================================================

if __name__ == '__main__':
    print("=" * 60)
    print("ONE LINE SOLUTION")
    print("=" * 60)
    print()
    print("Problem: Function blocks the main thread")
    print()
    print("  def analyze(data):")
    print("      # expensive work...")
    print()
    print("Solution: Add @cpu_task (ONE LINE!)")
    print()
    print("  @cpu_task  ‚Üê Just add this")
    print("  def analyze(data):")
    print("      # expensive work...")
    print()
    print("That's it!")
    print("=" * 60)
    print()
    print("The function now runs in the background.")
    print("Main thread stays responsive.")
    print("No blocking. No freezing. No complexity.")
    print()
    print("Just one line: @cpu_task")
    print("=" * 60)

```

`examples/qtasyncio_event_loop.py`:

```py
"""
Example demonstrating QtAsyncio event loop integration with async/await.

This example shows how to use the QtAsyncio module to integrate Python's
asyncio with Qt's event loop, enabling natural async/await syntax in Qt
applications.
"""

import asyncio
import sys
import time

try:
    from PySide6.QtWidgets import QApplication, QMainWindow, QPushButton, QVBoxLayout, QWidget, QLabel
    from PySide6.QtCore import Qt, QTimer
except ImportError:
    from PyQt5.QtWidgets import QApplication, QMainWindow, QPushButton, QVBoxLayout, QWidget, QLabel
    from PyQt5.QtCore import Qt, QTimer

from ida_taskr import (
    QT_ASYNCIO_AVAILABLE,
    get_logger,
)

if not QT_ASYNCIO_AVAILABLE:
    print("QtAsyncio module not available. Please ensure Qt (PyQt5 or PySide6) is installed.")
    sys.exit(1)

from ida_taskr import (
    set_event_loop_policy,
    qtasyncio_run,
    ThreadExecutor,
)

logger = get_logger(__name__)

# Set the Qt-compatible event loop policy
set_event_loop_policy()


# Example async functions
async def async_countdown(name, count):
    """Async countdown that yields control to the event loop."""
    logger.info(f"{name}: Starting countdown from {count}")
    for i in range(count, 0, -1):
        logger.info(f"{name}: {i}")
        await asyncio.sleep(1)
    logger.info(f"{name}: Done!")
    return f"{name} completed"


async def async_fetch_data(delay=2):
    """Simulate async data fetching."""
    logger.info("Fetching data...")
    await asyncio.sleep(delay)
    logger.info("Data fetched!")
    return {"status": "success", "data": [1, 2, 3, 4, 5]}


async def async_concurrent_tasks():
    """Run multiple async tasks concurrently."""
    logger.info("Starting concurrent tasks...")

    # Run multiple tasks concurrently
    results = await asyncio.gather(
        async_countdown("Task-A", 3),
        async_countdown("Task-B", 5),
        async_fetch_data(2),
    )

    logger.info(f"All tasks complete: {results}")
    return results


class AsyncioExampleWindow(QMainWindow):
    """Demo window showing asyncio integration with Qt."""

    def __init__(self):
        super().__init__()
        self.setWindowTitle("QtAsyncio Event Loop Examples")
        self.setGeometry(100, 100, 500, 300)

        # Central widget
        central = QWidget()
        self.setCentralWidget(central)
        layout = QVBoxLayout(central)

        # Status label
        self.status_label = QLabel("Ready - Click a button to run async tasks")
        self.status_label.setAlignment(Qt.AlignCenter)
        self.status_label.setWordWrap(True)
        layout.addWidget(self.status_label)

        # Example 1: Simple async countdown
        btn1 = QPushButton("Run Simple Async Task")
        btn1.clicked.connect(lambda: asyncio.create_task(self.run_simple_async()))
        layout.addWidget(btn1)

        # Example 2: Concurrent async tasks
        btn2 = QPushButton("Run Concurrent Async Tasks")
        btn2.clicked.connect(lambda: asyncio.create_task(self.run_concurrent_async()))
        layout.addWidget(btn2)

        # Example 3: ThreadExecutor integration
        btn3 = QPushButton("Run ThreadExecutor Task")
        btn3.clicked.connect(self.run_thread_executor)
        layout.addWidget(btn3)

        # Example 4: Mixing Qt timers with async
        btn4 = QPushButton("Mix Qt Timer with Async")
        btn4.clicked.connect(self.run_mixed_qt_async)
        layout.addWidget(btn4)

        # Create a ThreadExecutor instance
        self.executor = ThreadExecutor(self)

    async def run_simple_async(self):
        """Example 1: Run a simple async task."""
        self.status_label.setText("Running simple async countdown...")
        result = await async_countdown("Simple", 5)
        self.status_label.setText(f"Complete: {result}")

    async def run_concurrent_async(self):
        """Example 2: Run multiple concurrent async tasks."""
        self.status_label.setText("Running concurrent async tasks...")
        results = await async_concurrent_tasks()
        self.status_label.setText(f"All tasks complete! Results: {len(results)} tasks")

    def run_thread_executor(self):
        """Example 3: Use ThreadExecutor for CPU-bound tasks."""
        self.status_label.setText("Running CPU-bound task in ThreadExecutor...")

        def cpu_intensive_task(n):
            """Simulate CPU-intensive work."""
            logger.info(f"Computing sum of squares up to {n}...")
            result = sum(i * i for i in range(n))
            time.sleep(2)  # Simulate more work
            return result

        # Submit task to executor
        future = self.executor.submit(cpu_intensive_task, 1000000)

        # Use asyncio to wait for the future
        asyncio.create_task(self.wait_for_future(future))

    async def wait_for_future(self, future):
        """Wait for a concurrent.futures.Future in async context."""
        loop = asyncio.get_event_loop()
        result = await loop.run_in_executor(None, future.result)
        self.status_label.setText(f"ThreadExecutor result: {result:,}")
        logger.info(f"Computation complete: {result:,}")

    def run_mixed_qt_async(self):
        """Example 4: Mix Qt signals/timers with async/await."""
        self.status_label.setText("Mixing Qt timer with async...")

        # Create a Qt timer
        timer = QTimer(self)
        countdown = [5]  # Use list for mutability in closure

        async def async_with_timer():
            """Async function that works with Qt timer."""
            for i in range(5):
                await asyncio.sleep(1)
                self.status_label.setText(f"Async tick {i + 1}/5")

            timer.stop()
            self.status_label.setText("Mixed Qt/Async complete!")

        def on_timer():
            """Qt timer callback."""
            countdown[0] -= 1
            logger.info(f"Qt Timer tick: {countdown[0]}")
            if countdown[0] == 0:
                timer.stop()

        # Start Qt timer
        timer.timeout.connect(on_timer)
        timer.start(1000)

        # Start async task
        asyncio.create_task(async_with_timer())


async def async_main():
    """Async main function that runs the Qt application."""
    app = QApplication(sys.argv)
    window = AsyncioExampleWindow()
    window.show()

    # Run the Qt event loop asynchronously
    # This allows mixing Qt and asyncio seamlessly
    await asyncio.Event().wait()  # Wait forever (app.quit() will exit)


def main():
    """Run the asyncio/Qt integrated application."""
    # Note: When using QtAsyncio, you can use asyncio.run() directly
    # The event loop policy we set ensures Qt integration
    try:
        asyncio.run(async_main())
    except KeyboardInterrupt:
        logger.info("Application interrupted")


if __name__ == "__main__":
    main()

```

`examples/shared_memory_comparison.py`:

```py
"""
Shared Memory API Comparison

Shows the difference between manual shared memory setup vs @shared_memory_task decorator.
"""

# ==============================================================================
# BEFORE: Manual Shared Memory Setup (~40+ lines)
# ==============================================================================

def analyze_manual(binary_data):
    """
    Manual shared memory - COMPLEX!

    User must handle:
    - SharedMemory creation
    - Data copying
    - Chunk boundary calculation
    - Worker function with attachment logic
    - Chunk submission
    - Result collection
    - Memoryview cleanup
    - Shared memory cleanup

    ~40+ lines of boilerplate code!
    """
    import multiprocessing.shared_memory
    from ida_taskr import ProcessPoolExecutor

    # 1. Create shared memory
    shm = multiprocessing.shared_memory.SharedMemory(
        create=True,
        size=len(binary_data)
    )

    try:
        # 2. Copy data into shared memory
        shm.buf[:len(binary_data)] = binary_data

        # 3. Calculate chunk boundaries
        num_chunks = 8
        chunk_size = len(binary_data) // num_chunks

        # 4. Define worker that attaches to shared memory
        def process_chunk(shm_name, start, end, chunk_id):
            """Worker must handle shared memory attachment."""
            shm = multiprocessing.shared_memory.SharedMemory(name=shm_name)
            try:
                # Attach to shared memory
                chunk_data = memoryview(shm.buf)[start:end]

                # Process chunk
                signatures = []
                for i in range(min(16, len(chunk_data))):
                    signatures.append(chunk_data[i])

                return {'chunk': chunk_id, 'sigs': signatures}

            finally:
                # CRITICAL: cleanup memoryview before close
                del chunk_data
                shm.close()

        # 5. Submit chunks to executor
        executor = ProcessPoolExecutor(max_workers=8)
        futures = []

        for i in range(num_chunks):
            start = i * chunk_size
            end = start + chunk_size if i < num_chunks - 1 else len(binary_data)

            future = executor.submit(process_chunk, shm.name, start, end, i)
            futures.append(future)

        # 6. Collect results
        results = [f.result() for f in futures]

        # 7. Shutdown executor
        executor.shutdown()

    finally:
        # 8. Cleanup shared memory
        shm.close()
        shm.unlink()

    return results


# ==============================================================================
# AFTER: With @shared_memory_task Decorator (~5 lines!)
# ==============================================================================

from ida_taskr import shared_memory_task

@shared_memory_task(num_chunks=8)
def analyze_decorator(chunk_data, chunk_id, total_chunks):
    """
    With decorator - SIMPLE!

    User just writes chunk processing logic.
    ida-taskr handles ALL the shared memory complexity!

    Just ~5 lines of actual logic!
    """
    # Process chunk
    signatures = []
    for i in range(min(16, len(chunk_data))):
        signatures.append(chunk_data[i])

    return {'chunk': chunk_id, 'sigs': signatures}


# Usage is also simpler:
# results = analyze_decorator(binary_data)  # That's it!


# ==============================================================================
# Real-World IDA Example
# ==============================================================================

@shared_memory_task(num_chunks=16)
def find_function_patterns(chunk_data, chunk_id, total_chunks):
    """
    Find function prologue patterns in binary data.

    Real-world use case: Analyzing large binary sections in IDA.
    """
    patterns = []

    # Common x64 function prologues
    prologue_patterns = [
        b'\x55\x48\x89\xe5',        # push rbp; mov rbp, rsp
        b'\x48\x83\xec',            # sub rsp, imm
        b'\x48\x89\x5c\x24',        # mov [rsp+X], rbx
    ]

    for i in range(len(chunk_data) - 4):
        for pattern in prologue_patterns:
            if chunk_data[i:i+len(pattern)] == pattern:
                patterns.append({
                    'chunk': chunk_id,
                    'offset': i,
                    'pattern_type': pattern.hex(),
                })

    return patterns


def ida_usage_example():
    """
    How to use this in IDA Pro.

    This is what users would write in their IDA plugins.
    """
    print("=" * 70)
    print("IDA Pro Usage Example")
    print("=" * 70)
    print()
    print("In your IDA plugin:")
    print("-" * 70)
    print("""
import ida_bytes
from ida_taskr import shared_memory_task

@shared_memory_task(num_chunks=16)
def analyze_section(chunk_data, chunk_id, total_chunks):
    # Your analysis logic here
    results = find_interesting_patterns(chunk_data)
    return results

# In your plugin action:
def analyze_current_section():
    # Get binary data from IDA
    start_ea = idc.get_segm_start(idc.here())
    end_ea = idc.get_segm_end(idc.here())
    binary_data = ida_bytes.get_bytes(start_ea, end_ea - start_ea)

    # Process in parallel - just one line!
    results = analyze_section(binary_data)

    # Show results
    for chunk_results in results:
        print(f"Chunk {chunk_results['chunk']}: {len(chunk_results)} patterns")
""")
    print()
    print("That's it! ida-taskr handles all shared memory complexity.")
    print("=" * 70)


# ==============================================================================
# Comparison Summary
# ==============================================================================

if __name__ == '__main__':
    print("=" * 70)
    print("SHARED MEMORY API COMPARISON")
    print("=" * 70)
    print()

    print("WITHOUT Decorator (Manual):")
    print("-" * 70)
    print("  Lines of code: ~40-50 lines")
    print("  User handles:")
    print("    - SharedMemory creation")
    print("    - Data copying")
    print("    - Chunk boundary math")
    print("    - Worker attachment logic")
    print("    - Chunk submission")
    print("    - Result collection")
    print("    - Memoryview cleanup")
    print("    - SharedMemory cleanup")
    print()

    print("WITH Decorator (Minimal):")
    print("-" * 70)
    print("  Lines of code: ~5-10 lines")
    print("  User writes:")
    print("    - Just the chunk processing logic!")
    print()
    print("  ida-taskr handles:")
    print("    - Everything else automatically!")
    print()

    print("=" * 70)
    print("ANSWER: @shared_memory_task(num_chunks=N)")
    print("=" * 70)
    print()
    print("Smallest surface area:")
    print()
    print("  @shared_memory_task(num_chunks=8)")
    print("  def process_chunk(chunk_data, chunk_id, total_chunks):")
    print("      # Your logic here")
    print("      return result")
    print()
    print("  # Usage:")
    print("  results = process_chunk(binary_data)")
    print()
    print("That's it! User writes ~5 lines of chunk logic.")
    print("ida-taskr handles all ~40 lines of shared memory boilerplate!")
    print("=" * 70)
    print()

    # Show IDA usage
    ida_usage_example()

```

`examples/shared_memory_decorator_example.py`:

```py
"""
Working Example: @shared_memory_task Decorator

Shows the minimal API for processing large binary data with shared memory.
"""

import time
from ida_taskr import shared_memory_task


# ==============================================================================
# MINIMAL API: Just write the chunk processing logic!
# ==============================================================================

@shared_memory_task(num_chunks=8)
def find_signatures(chunk_data, chunk_id, total_chunks):
    """
    Find binary signatures in this chunk.

    This is ALL the user needs to write!
    ida-taskr handles all the shared memory complexity.

    Args:
        chunk_data: memoryview of this chunk's data (no copying!)
        chunk_id: 0-based chunk index (0 to 7 in this case)
        total_chunks: total number of chunks (8 in this case)

    Returns:
        List of signatures found in this chunk
    """
    signatures = []

    # Process this chunk
    for i in range(len(chunk_data) - 16):
        # Simple pattern: find sequences that start with 0x48
        if chunk_data[i] == 0x48:
            # Extract 16-byte signature
            sig = bytes(chunk_data[i:i+16])
            signatures.append({
                'chunk': chunk_id,
                'offset': i,
                'signature': sig.hex()
            })

    return signatures


# ==============================================================================
# Usage - Just pass the full data!
# ==============================================================================

def demo():
    """Demonstrate the minimal shared memory API."""

    # Create test data (simulating 8MB binary)
    print("Creating test binary data (8MB)...")
    binary_data = bytearray()
    for i in range(8 * 1024 * 1024):
        binary_data.append(i % 256)

    # Add some patterns
    for offset in [0x1000, 0x2000, 0x100000, 0x500000]:
        binary_data[offset] = 0x48  # Pattern marker
        binary_data[offset + 1:offset + 4] = b'\x89\xe5\x48'

    binary_data = bytes(binary_data)

    print(f"Processing {len(binary_data):,} bytes...")
    print()

    # This is ALL the user needs to write:
    start = time.time()
    results = find_signatures(binary_data)
    elapsed = time.time() - start

    # Show results
    print(f"‚úì Processed in {elapsed:.2f}s")
    print(f"‚úì Found {sum(len(r) for r in results)} signatures across {len(results)} chunks")
    print()

    # Show signatures from each chunk
    for chunk_results in results:
        if chunk_results:
            chunk_id = chunk_results[0]['chunk']
            print(f"  Chunk {chunk_id}: {len(chunk_results)} signatures")
            if chunk_results:
                print(f"    First: {chunk_results[0]['signature'][:32]}...")

    print()
    print("=" * 70)
    print("That's the minimal API!")
    print("=" * 70)
    print()
    print("User wrote:")
    print("  1. The chunk processing logic (~10 lines)")
    print("  2. Called the decorated function with full data")
    print()
    print("ida-taskr handled:")
    print("  1. Creating shared memory")
    print("  2. Copying data once")
    print("  3. Calculating chunk boundaries")
    print("  4. Creating 8 worker processes")
    print("  5. Attaching workers to shared memory")
    print("  6. Collecting results")
    print("  7. Cleanup (memoryview, close, unlink)")
    print()
    print("Just @shared_memory_task(num_chunks=8)!")
    print("=" * 70)


if __name__ == '__main__':
    demo()

```

`examples/shared_memory_minimal.py`:

```py
"""
Minimal Shared Memory API

Shows the smallest amount of code users need to write for shared memory tasks.

This example demonstrates the actual working @shared_memory_task decorator.
"""

import time
import sys
import os

# Add src to path for import
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'src'))


# ==============================================================================
# BEFORE: Manual shared memory setup (COMPLEX)
# ==============================================================================

def analyze_manual(binary_data):
    """
    Manual shared memory - lots of boilerplate!

    User has to handle:
    - Create SharedMemory
    - Copy data into it
    - Calculate chunk boundaries
    - Create worker that attaches to shared memory
    - Submit chunks with offsets
    - Cleanup memoryview
    - Close/unlink shared memory

    ~40+ lines of code!
    """
    import multiprocessing
    from multiprocessing import shared_memory
    from ida_taskr import ProcessPoolExecutor

    # Step 1: Create shared memory
    shm = shared_memory.SharedMemory(create=True, size=len(binary_data))
    shm.buf[:] = binary_data

    # Step 2: Calculate chunks
    num_chunks = 8
    chunk_size = len(binary_data) // num_chunks

    # Step 3: Define worker that attaches to shared memory
    def worker(shm_name, start, end, chunk_id):
        shm = shared_memory.SharedMemory(name=shm_name)
        try:
            # Attach to shared memory
            chunk_data = memoryview(shm.buf)[start:end]

            # Process chunk
            result = []
            for i in range(min(16, len(chunk_data))):
                result.append(chunk_data[i])

            return {'chunk_id': chunk_id, 'signature': result}
        finally:
            del chunk_data  # CRITICAL!
            shm.close()

    # Step 4: Submit chunks
    executor = ProcessPoolExecutor(max_workers=8)
    futures = []

    for i in range(num_chunks):
        start = i * chunk_size
        end = start + chunk_size if i < num_chunks - 1 else len(binary_data)
        future = executor.submit(worker, shm.name, start, end, i)
        futures.append(future)

    # Step 5: Collect results
    results = [f.result() for f in futures]

    # Step 6: Cleanup
    executor.shutdown()
    shm.close()
    shm.unlink()

    return results


# ==============================================================================
# AFTER: With decorator (MINIMAL)
# ==============================================================================

from ida_taskr import shared_memory_task

@shared_memory_task(num_chunks=8)
def analyze_minimal(chunk_data, chunk_id, total_chunks):
    """
    Minimal shared memory API!

    User just writes the chunk processing logic.
    ida-taskr handles:
    - Creating/managing shared memory
    - Chunking the data
    - Worker attachment
    - Cleanup

    Just ~5 lines of actual logic!
    """
    # Process this chunk
    result = []
    for i in range(min(16, len(chunk_data))):
        result.append(chunk_data[i])

    return {'chunk_id': chunk_id, 'signature': result}


# Usage:
# results = analyze_minimal(binary_data)  # That's it!
# ida-taskr handles all the shared memory complexity


# ==============================================================================
# Comparison
# ==============================================================================

if __name__ == '__main__':
    print("=" * 70)
    print("SHARED MEMORY: Minimal API")
    print("=" * 70)
    print()

    print("WITHOUT decorator (manual):")
    print("-" * 70)
    print("""
def analyze(binary_data):
    import multiprocessing.shared_memory

    # 1. Create shared memory
    shm = shared_memory.SharedMemory(create=True, size=len(binary_data))
    shm.buf[:] = binary_data

    # 2. Calculate chunks
    num_chunks = 8
    chunk_size = len(binary_data) // num_chunks

    # 3. Define worker with shared memory attachment
    def worker(shm_name, start, end, chunk_id):
        shm = shared_memory.SharedMemory(name=shm_name)
        try:
            chunk_data = memoryview(shm.buf)[start:end]
            # ... process chunk ...
            return result
        finally:
            del chunk_data
            shm.close()

    # 4. Submit chunks with offsets
    executor = ProcessPoolExecutor(max_workers=8)
    futures = []
    for i in range(num_chunks):
        start = i * chunk_size
        end = start + chunk_size if i < num_chunks - 1 else len(...)
        future = executor.submit(worker, shm.name, start, end, i)
        futures.append(future)

    # 5. Collect results
    results = [f.result() for f in futures]

    # 6. Cleanup
    executor.shutdown()
    shm.close()
    shm.unlink()

    return results

# ~40+ lines of boilerplate!
""")

    print("\nWITH decorator (minimal):")
    print("-" * 70)
    print("""
from ida_taskr import shared_memory_task

@shared_memory_task(num_chunks=8)
def analyze(chunk_data, chunk_id, total_chunks):
    # Just process this chunk!
    result = []
    for i in range(min(16, len(chunk_data))):
        result.append(chunk_data[i])
    return {'chunk_id': chunk_id, 'signature': result}

# Usage:
results = analyze(binary_data)

# Just ~5 lines of logic!
""")

    print()
    print("=" * 70)
    print("ANSWER: @shared_memory_task(num_chunks=N)")
    print("=" * 70)
    print()
    print("User writes:")
    print("  1. The chunk processing logic (~5 lines)")
    print()
    print("ida-taskr handles:")
    print("  1. Creating shared memory")
    print("  2. Copying data once")
    print("  3. Calculating chunk boundaries")
    print("  4. Creating workers")
    print("  5. Attaching to shared memory")
    print("  6. Cleanup (memoryview, close, unlink)")
    print()
    print("Smallest surface area: Just write the chunk logic!")
    print("=" * 70)

```

`examples/shared_memory_one_line.py`:

```py
"""
ONE LINE SHARED MEMORY SOLUTION

The smallest amount of code for parallel processing with shared memory.
"""

from ida_taskr import shared_memory_task


# ==============================================================================
# The Smallest Surface Area: Just @shared_memory_task(num_chunks=N)
# ==============================================================================

@shared_memory_task(num_chunks=8)
def analyze(chunk_data, chunk_id, total_chunks):
    """
    Process one chunk of data.

    That's it! User just writes the chunk logic.
    ida-taskr handles ALL shared memory complexity:
    - Creating shared memory
    - Copying data once
    - Calculating boundaries
    - Creating workers
    - Attaching to shared memory
    - Collecting results
    - Cleanup
    """
    # Your chunk processing logic
    result = []
    for i in range(min(32, len(chunk_data))):
        result.append(chunk_data[i])
    return result


# Usage - just pass the full data!
# results = analyze(binary_data)  # Returns list of all chunk results


# ==============================================================================
# Summary
# ==============================================================================

if __name__ == '__main__':
    print("=" * 70)
    print("ONE LINE SHARED MEMORY SOLUTION")
    print("=" * 70)
    print()
    print("Question: What's the smallest surface area for shared memory?")
    print()
    print("Answer: Just write the chunk processing logic!")
    print()
    print("-" * 70)
    print()
    print("  @shared_memory_task(num_chunks=8)")
    print("  def analyze(chunk_data, chunk_id, total_chunks):")
    print("      # Your chunk logic here")
    print("      return result")
    print()
    print("  # Usage:")
    print("  results = analyze(binary_data)")
    print()
    print("-" * 70)
    print()
    print("What user provides:")
    print("  - Chunk processing function (~5 lines)")
    print()
    print("What ida-taskr handles:")
    print("  - SharedMemory creation")
    print("  - Copying data once into shared memory")
    print("  - Calculating chunk boundaries")
    print("  - Creating worker processes")
    print("  - Attaching workers to shared memory")
    print("  - Submitting chunks")
    print("  - Collecting results")
    print("  - Memoryview cleanup")
    print("  - SharedMemory cleanup")
    print("  (~40 lines of boilerplate!)")
    print()
    print("=" * 70)
    print()
    print("Smallest surface area: Just the chunk logic!")
    print()
    print("=" * 70)

```

`examples/shared_memory_parallel_example.py`:

```py
"""
Example: Using ProcessPoolExecutor with Shared Memory for Chunked Processing

Shows how to process large binary data (like 8MB files) in parallel using
shared memory to avoid copying data between processes.

This pattern is used by anti_deob to analyze large code sections without
blocking IDA's main thread.
"""

import multiprocessing
import multiprocessing.shared_memory
from concurrent.futures import as_completed
from ida_taskr import ProcessPoolExecutor
import time


# ==============================================================================
# STEP 1: Worker function that processes a chunk
# ==============================================================================

def process_chunk(shm_name, chunk_start, chunk_end, chunk_id):
    """
    Worker function that processes one chunk of shared memory.

    This runs in a separate process and attaches to the shared memory.

    Args:
        shm_name: Name of the shared memory segment
        chunk_start: Start offset in the shared memory
        chunk_end: End offset in the shared memory
        chunk_id: ID of this chunk (for progress tracking)

    Returns:
        Results from analyzing this chunk
    """
    # Attach to existing shared memory (created by main process)
    shm = multiprocessing.shared_memory.SharedMemory(name=shm_name)

    try:
        # Get a view of just this chunk (no copying!)
        chunk_data = memoryview(shm.buf)[chunk_start:chunk_end]

        # Process the chunk (simulate signature generation)
        signature = []
        for i in range(min(16, len(chunk_data))):  # First 16 bytes
            signature.append(chunk_data[i])
            time.sleep(0.001)  # Simulate CPU work

        # Return results
        return {
            'chunk_id': chunk_id,
            'start': chunk_start,
            'end': chunk_end,
            'signature': signature,
            'pattern': ' '.join(f'{b:02X}' for b in signature)
        }

    finally:
        # IMPORTANT: Delete memoryview before closing shared memory
        del chunk_data
        shm.close()  # Detach from shared memory (don't unlink!)


# ==============================================================================
# STEP 2: Main function that coordinates chunked processing
# ==============================================================================

def process_large_binary_parallel(binary_data, num_chunks=4, max_workers=4):
    """
    Process large binary data in parallel using shared memory.

    This is the pattern used by anti_deob to analyze large sections
    without blocking IDA's main thread.

    Args:
        binary_data: Large binary data (e.g., 8MB from IDA)
        num_chunks: Number of chunks to split data into
        max_workers: Number of parallel workers

    Returns:
        List of results from all chunks
    """
    data_size = len(binary_data)
    chunk_size = data_size // num_chunks

    print(f"Processing {data_size:,} bytes in {num_chunks} chunks")
    print(f"Using {max_workers} parallel workers")
    print()

    # STEP 1: Create shared memory and copy data into it
    # This happens ONCE in the main process
    shm = multiprocessing.shared_memory.SharedMemory(
        create=True,
        size=data_size
    )

    try:
        # Copy data into shared memory
        shm.buf[:data_size] = binary_data
        print(f"‚úì Created shared memory: {shm.name}")
        print(f"  Size: {data_size:,} bytes")
        print()

        # STEP 2: Create chunks and submit to ProcessPoolExecutor
        executor = ProcessPoolExecutor(max_workers=max_workers)
        futures = []

        for i in range(num_chunks):
            chunk_start = i * chunk_size
            chunk_end = chunk_start + chunk_size if i < num_chunks - 1 else data_size

            print(f"Submitting chunk {i+1}/{num_chunks}: bytes {chunk_start:,} - {chunk_end:,}")

            # Submit chunk for processing
            # Only pass the SHM name and offsets - NO DATA COPYING!
            future = executor.submit(
                process_chunk,
                shm.name,      # Shared memory name
                chunk_start,   # Start offset
                chunk_end,     # End offset
                i             # Chunk ID
            )
            futures.append(future)

        print()
        print("All chunks submitted - workers processing in parallel...")
        print()

        # STEP 3: Collect results as they complete
        results = []
        for future in as_completed(futures):
            result = future.result()
            chunk_id = result['chunk_id']
            print(f"‚úì Chunk {chunk_id+1} complete: {result['pattern'][:40]}...")
            results.append(result)

        # Sort by chunk_id to get original order
        results.sort(key=lambda r: r['chunk_id'])

        executor.shutdown(wait=True)
        print()
        print(f"‚úì All {num_chunks} chunks processed!")

        return results

    finally:
        # STEP 4: Cleanup shared memory
        shm.close()
        shm.unlink()  # Remove shared memory
        print(f"‚úì Shared memory cleaned up")


# ==============================================================================
# STEP 3: Example with progress callbacks (like anti_deob)
# ==============================================================================

def process_with_progress(binary_data, num_chunks=4):
    """
    Example with progress updates via Qt signals.

    This is how anti_deob provides real-time progress updates.
    """
    data_size = len(binary_data)
    chunk_size = data_size // num_chunks

    # Create shared memory
    shm = multiprocessing.shared_memory.SharedMemory(create=True, size=data_size)
    shm.buf[:data_size] = binary_data

    try:
        executor = ProcessPoolExecutor(max_workers=4)

        # Connect to progress signals
        completed_chunks = []

        def on_chunk_complete(future):
            result = future.result()
            completed_chunks.append(result)
            progress = int(len(completed_chunks) / num_chunks * 100)
            print(f"[{progress}%] Chunk {result['chunk_id']+1}/{num_chunks} done")

        executor.signals.task_completed.connect(on_chunk_complete)

        # Submit all chunks
        futures = []
        for i in range(num_chunks):
            chunk_start = i * chunk_size
            chunk_end = chunk_start + chunk_size if i < num_chunks - 1 else data_size

            future = executor.submit(
                process_chunk,
                shm.name,
                chunk_start,
                chunk_end,
                i
            )
            futures.append(future)

        # Wait for all chunks
        for future in futures:
            future.result()

        executor.shutdown(wait=True)

        return completed_chunks

    finally:
        shm.close()
        shm.unlink()


# ==============================================================================
# STEP 4: Decorator version (ultra simple API)
# ==============================================================================

from ida_taskr import background_task

@background_task(max_workers=4, executor_type='process')
def process_chunk_simple(shm_name, chunk_start, chunk_end):
    """Simplified chunk processor - just add decorator!"""
    shm = multiprocessing.shared_memory.SharedMemory(name=shm_name)
    try:
        chunk_data = memoryview(shm.buf)[chunk_start:chunk_end]
        # Process chunk...
        signature = list(chunk_data[:16])
        return {'signature': signature}
    finally:
        del chunk_data
        shm.close()


def simple_parallel_processing(binary_data):
    """Ultra-simple API using decorator."""
    shm = multiprocessing.shared_memory.SharedMemory(
        create=True,
        size=len(binary_data)
    )
    shm.buf[:len(binary_data)] = binary_data

    try:
        # Submit chunks - returns futures immediately
        num_chunks = 4
        chunk_size = len(binary_data) // num_chunks

        futures = [
            process_chunk_simple(
                shm.name,
                i * chunk_size,
                (i + 1) * chunk_size
            )
            for i in range(num_chunks)
        ]

        # Collect results
        results = [f.result() for f in futures]
        return results

    finally:
        shm.close()
        shm.unlink()


# ==============================================================================
# Demo
# ==============================================================================

def main():
    print("=" * 70)
    print("Parallel Processing with Shared Memory")
    print("=" * 70)
    print()

    # Create 8MB of test data (simulating IDA binary data)
    binary_data = bytes(range(256)) * (32 * 1024)  # 8MB
    print(f"Test data size: {len(binary_data):,} bytes (8MB)")
    print()

    # Process in parallel using shared memory
    results = process_large_binary_parallel(
        binary_data,
        num_chunks=8,
        max_workers=4
    )

    print()
    print("=" * 70)
    print("Results Summary:")
    print("=" * 70)
    for r in results[:3]:  # Show first 3
        print(f"Chunk {r['chunk_id']}: {r['pattern'][:50]}...")

    print()
    print("‚úì Main thread was NEVER blocked!")
    print("‚úì No data copying between processes (shared memory)")
    print("‚úì True parallel execution on multiple CPU cores")
    print("=" * 70)


if __name__ == '__main__':
    main()

```

`examples/signature_generation_example.py`:

```py
"""
Example: Creating binary signatures without blocking IDA's UI

This example shows how to use ida-taskr to perform CPU-intensive signature
generation while keeping IDA responsive and showing progress updates.
"""

import time
from ida_taskr import TaskRunner, create_worker


# ==============================================================================
# APPROACH 1: Using TaskRunner (High-level, recommended for IDA plugins)
# ==============================================================================

def generate_signature_worker(address, data, max_length=32):
    """
    Worker function that generates a unique binary signature.

    This runs in a separate process, so it won't block IDA's UI.

    Args:
        address: Address to generate signature for
        data: Binary data to search (bytes)
        max_length: Maximum signature length

    Yields:
        Progress updates as the signature is being generated

    Returns:
        The final signature as a dict
    """
    # Simulate signature generation with progress updates
    signature = []
    total_bytes = min(max_length, len(data))

    for i in range(total_bytes):
        # Check if this byte is unique enough
        byte_val = data[i]

        # Simulate analysis (in real code, this would check uniqueness)
        time.sleep(0.01)  # Simulating CPU work

        # Emit progress update (these become Qt signals)
        progress = int((i + 1) / total_bytes * 100)
        yield {
            'type': 'progress',
            'progress': progress,
            'message': f'Analyzing byte {i+1}/{total_bytes}: 0x{byte_val:02X}'
        }

        signature.append(byte_val)

        # Check if signature is unique enough (simplified)
        if len(signature) >= 8 and i % 4 == 0:
            yield {
                'type': 'status',
                'message': f'Testing signature uniqueness at {len(signature)} bytes...'
            }

    # Return final result
    return {
        'address': address,
        'signature': signature,
        'length': len(signature),
        'pattern': ' '.join(f'{b:02X}' for b in signature)
    }


def example_taskrunner():
    """Example using TaskRunner - best for IDA plugins."""

    # Read some binary data (in IDA, you'd use ida_bytes.get_bytes())
    # For demo purposes, we'll use dummy data
    binary_data = bytes(range(256)) * 100  # 25KB of data

    # Create a TaskRunner
    runner = TaskRunner()

    # Connect to signals to receive updates
    @runner.message_emitter.on('progress')
    def on_progress(data):
        """Called when worker reports progress."""
        print(f"[Progress {data['progress']}%] {data['message']}")

    @runner.message_emitter.on('status')
    def on_status(data):
        """Called when worker reports status."""
        print(f"[Status] {data['message']}")

    @runner.message_emitter.on('result')
    def on_result(data):
        """Called when worker completes."""
        result = data['result']
        print(f"\n‚úì Signature generated!")
        print(f"  Address: 0x{result['address']:X}")
        print(f"  Length: {result['length']} bytes")
        print(f"  Pattern: {result['pattern']}")

    @runner.message_emitter.on('error')
    def on_error(data):
        """Called if worker fails."""
        print(f"‚úó Error: {data['error']}")

    # Start the worker (non-blocking!)
    print("Starting signature generation (UI remains responsive)...\n")
    runner.run_task(
        generate_signature_worker,
        address=0x401000,
        data=binary_data,
        max_length=32
    )

    # IDA's UI is NOT blocked - user can continue working
    # Progress updates will appear as they happen

    return runner


# ==============================================================================
# APPROACH 2: Using ThreadExecutor (Lower-level, more control)
# ==============================================================================

def generate_signature_simple(address, data, max_length=32):
    """
    Simpler version without yield - just returns result.
    Use this with ThreadExecutor or ProcessPoolExecutor.
    """
    signature = []
    total_bytes = min(max_length, len(data))

    for i in range(total_bytes):
        byte_val = data[i]
        time.sleep(0.01)  # Simulating work
        signature.append(byte_val)

    return {
        'address': address,
        'signature': signature,
        'length': len(signature),
        'pattern': ' '.join(f'{b:02X}' for b in signature)
    }


def example_thread_executor():
    """Example using ThreadExecutor - good for I/O-bound tasks."""
    from ida_taskr import ThreadExecutor

    binary_data = bytes(range(256)) * 100

    # Create executor
    executor = ThreadExecutor(max_workers=4)

    # Connect to signals
    @executor.signals.on('task_completed')
    def on_completed(future):
        """Called when task completes."""
        result = future.result()
        print(f"\n‚úì Signature generated!")
        print(f"  Pattern: {result['pattern']}")

    @executor.signals.on('task_failed')
    def on_failed(future, exception):
        """Called if task fails."""
        print(f"‚úó Error: {exception}")

    # Submit task (non-blocking!)
    print("Submitting signature generation task...\n")
    future = executor.submit(
        generate_signature_simple,
        address=0x401000,
        data=binary_data,
        max_length=32
    )

    # Can submit multiple tasks in parallel!
    # future2 = executor.submit(generate_signature_simple, 0x402000, data2)
    # future3 = executor.submit(generate_signature_simple, 0x403000, data3)

    return executor


# ==============================================================================
# APPROACH 3: Using ProcessPoolExecutor (Best for CPU-intensive)
# ==============================================================================

def example_process_pool():
    """
    Example using ProcessPoolExecutor - best for CPU-intensive tasks.

    This uses separate Python processes, so it bypasses the GIL and
    can use multiple CPU cores effectively.
    """
    from ida_taskr import ProcessPoolExecutor

    binary_data = bytes(range(256)) * 100

    # Create executor with multiple workers
    executor = ProcessPoolExecutor(max_workers=4)

    # Connect to signals
    @executor.signals.on('task_completed')
    def on_completed(future):
        result = future.result()
        print(f"‚úì Signature for 0x{result['address']:X}: {result['pattern'][:40]}...")

    # Submit multiple signature generation tasks in parallel
    print("Generating signatures for multiple addresses in parallel...\n")

    addresses = [0x401000, 0x402000, 0x403000, 0x404000]
    futures = []

    for addr in addresses:
        # Each runs in a separate process - true parallelism!
        future = executor.submit(
            generate_signature_simple,
            address=addr,
            data=binary_data[addr % 1000:],
            max_length=16
        )
        futures.append(future)

    print(f"Submitted {len(futures)} tasks - all running in parallel!\n")

    return executor, futures


# ==============================================================================
# REAL IDA EXAMPLE
# ==============================================================================

def ida_signature_scanner_example():
    """
    Real-world IDA Pro example: Scan function for unique signatures.

    This would be used in an IDA plugin to find signatures without
    freezing the UI.
    """
    try:
        import idaapi
        import ida_bytes
        import ida_funcs
    except ImportError:
        print("This example requires IDA Pro")
        return None

    def scan_function_for_signature(func_ea):
        """Worker that analyzes a function and finds unique signature."""
        func = ida_funcs.get_func(func_ea)
        if not func:
            return None

        # Read function bytes
        func_size = func.end_ea - func.start_ea
        func_bytes = ida_bytes.get_bytes(func.start_ea, func_size)

        # Generate signature with progress updates
        signature = []
        chunk_size = max(1, func_size // 100)  # For progress updates

        for i in range(min(64, func_size)):  # Max 64 byte signature
            if i % chunk_size == 0:
                progress = int(i / min(64, func_size) * 100)
                yield {
                    'type': 'progress',
                    'progress': progress,
                    'message': f'Scanning function at 0x{func_ea:X}...'
                }

            signature.append(func_bytes[i])

            # Check uniqueness (simplified - real code would search database)
            if len(signature) >= 16:
                break

        return {
            'function': func_ea,
            'signature': signature,
            'pattern': ' '.join(f'{b:02X}' for b in signature)
        }

    # Create TaskRunner
    runner = TaskRunner()

    # Setup UI callbacks
    @runner.message_emitter.on('progress')
    def update_ui(data):
        # Update IDA's status bar or progress dialog
        idaapi.msg(f"{data['message']}\n")

    @runner.message_emitter.on('result')
    def show_result(data):
        result = data['result']
        idaapi.msg(f"‚úì Signature: {result['pattern']}\n")

    # Get current function
    func_ea = idaapi.get_screen_ea()

    # Run analysis without blocking IDA
    runner.run_task(scan_function_for_signature, func_ea)

    return runner


if __name__ == '__main__':
    print("=" * 70)
    print("ida-taskr: Non-blocking CPU-Intensive Task Examples")
    print("=" * 70)
    print()

    # Example 1: TaskRunner (recommended)
    print("EXAMPLE 1: TaskRunner (High-level API)")
    print("-" * 70)
    runner = example_taskrunner()
    print()

    # Give it time to show some progress
    time.sleep(2)

    print("\nEXAMPLE 2: ThreadExecutor")
    print("-" * 70)
    executor = example_thread_executor()
    print()

    time.sleep(2)

    print("\nEXAMPLE 3: ProcessPoolExecutor (parallel)")
    print("-" * 70)
    pool, futures = example_process_pool()
    print()

    # Keep running to see results
    time.sleep(3)

    print("\n" + "=" * 70)
    print("All tasks running in background - UI would remain responsive!")
    print("=" * 70)

```

`examples/simple_cpu_intensive_example.py`:

```py
"""
Simple example: CPU-intensive task without blocking IDA

This shows how to run a CPU-intensive task (like binary signature generation)
in the background without freezing IDA's UI.
"""

from ida_taskr import ProcessPoolExecutor
import time


def create_binary_signature(address, data, max_length=32):
    """
    CPU-intensive function that generates a binary signature.

    This runs in a separate process automatically, so it won't block IDA.

    Args:
        address: Address to generate signature for
        data: Binary data bytes
        max_length: Maximum signature length

    Returns:
        Dictionary with signature results
    """
    signature = []
    total = min(max_length, len(data))

    for i in range(total):
        # Your CPU-intensive work goes here
        byte_val = data[i]
        signature.append(byte_val)

        # Simulate CPU work (in real code, this would be pattern analysis)
        time.sleep(0.01)

    return {
        'address': address,
        'signature': signature,
        'pattern': ' '.join(f'{b:02X}' for b in signature[:16])
    }


def main():
    """Example of running CPU-intensive task without blocking."""

    # Create executor (uses separate processes for true parallelism)
    executor = ProcessPoolExecutor(max_workers=2)

    # Connect Qt signal to handle completion
    def on_completed(future):
        result = future.result()
        print(f"\n‚úì Task completed! Signature for 0x{result['address']:X}:")
        print(f"  Pattern: {result['pattern']}")
        print(f"  Length: {len(result['signature'])} bytes")

    executor.signals.task_completed.connect(on_completed)

    # Connect Qt signal to handle errors
    def on_failed(future, exception):
        print(f"\n‚úó Task failed: {exception}")

    executor.signals.task_failed.connect(on_failed)

    # Submit the task (returns immediately - doesn't block!)
    binary_data = bytes(range(256)) * 100

    print("Submitting CPU-intensive task to background process...")
    print("(IDA's UI remains responsive)\n")

    future = executor.submit(
        create_binary_signature,
        address=0x401000,
        data=binary_data,
        max_length=32
    )

    # Main thread continues immediately - not blocked!
    print("Main thread continues working...")
    for i in range(3):
        print(f"  Doing other work... {i+1}/3")
        time.sleep(1)

    # Optionally wait for the result
    result = future.result(timeout=10)

    print("\n‚úì Main thread was never blocked!")
    print("  Worker ran in parallel in separate process")

    executor.shutdown(wait=True)


if __name__ == '__main__':
    print("=" * 60)
    print("CPU-Intensive Task Example (Non-Blocking)")
    print("=" * 60)
    print()

    main()

    print("\n" + "=" * 60)

```

`examples/simple_progress_example.py`:

```py
"""
Simple example: CPU-intensive task without blocking

Shows the minimal code needed to run a long task in the background.
"""

from ida_taskr import ProcessPoolExecutor
import time


# Step 1: Define your worker function
def create_binary_signature(address, data, max_length=32):
    """
    Your CPU-intensive function.

    This runs in a separate process, so it won't block the main thread/UI.

    Args:
        address: Address to generate signature for
        data: Binary data bytes
        max_length: Maximum signature length

    Returns:
        Dictionary with signature results
    """
    signature = []
    total = min(max_length, len(data))

    for i in range(total):
        # Do your CPU-intensive work here
        byte_val = data[i]
        signature.append(byte_val)

        # Simulate CPU work
        time.sleep(0.01)

    # Return final result
    return {
        'address': address,
        'signature': signature,
        'pattern': ' '.join(f'{b:02X}' for b in signature[:16])
    }


# Step 2: Set up executor and connect signals
def run_signature_generation():
    """Run the task without blocking."""

    # Create executor (uses separate processes for true parallelism)
    executor = ProcessPoolExecutor(max_workers=2)

    # Connect to completion signal
    @executor.signals.on('task_completed')
    def on_completed(future):
        result = future.result()
        print(f"\n‚úì Done! Signature for 0x{result['address']:X}:")
        print(f"  Pattern: {result['pattern']}")
        print(f"  Length: {len(result['signature'])} bytes")

    # Connect to error signal
    @executor.signals.on('task_failed')
    def on_failed(future, exception):
        print(f"\n‚úó Error: {exception}")

    # Step 3: Submit the task (returns immediately!)
    binary_data = bytes(range(256)) * 100

    print("Submitting task to background process...")
    print("(Main thread is NOT blocked)\n")

    future = executor.submit(
        create_binary_signature,
        address=0x401000,
        data=binary_data,
        max_length=32
    )

    # The main thread continues immediately!
    # Results arrive via Qt signals when done

    return executor, future


if __name__ == '__main__':
    print("=" * 60)
    print("CPU-Intensive Task Example")
    print("=" * 60)
    print()

    executor, future = run_signature_generation()

    # Main thread is free to do other work
    print("Main thread doing other work...")
    for i in range(3):
        print(f"  Tick {i+1}/3...")
        time.sleep(1)

    print("\n‚úì Main thread was never blocked!")
    print("  Worker ran in parallel in the background")
    print("=" * 60)

```

`examples/thread_worker_usage.py`:

```py
"""
Example demonstrating the thread_worker decorator and Qt-based worker utilities.

This example shows how to use the qtasyncio module's worker utilities for
running tasks in background threads with Qt signal integration.
"""

import sys
import time

try:
    from PySide6.QtWidgets import QApplication, QMainWindow, QPushButton, QVBoxLayout, QWidget, QLabel
    from PySide6.QtCore import Qt
except ImportError:
    from PyQt5.QtWidgets import QApplication, QMainWindow, QPushButton, QVBoxLayout, QWidget, QLabel
    from PyQt5.QtCore import Qt

from ida_taskr import (
    QT_ASYNCIO_AVAILABLE,
    get_logger,
)

if not QT_ASYNCIO_AVAILABLE:
    print("QtAsyncio module not available. Please ensure Qt (PyQt5 or PySide6) is installed.")
    sys.exit(1)

from ida_taskr import thread_worker, create_worker, FunctionWorker

logger = get_logger(__name__)


# Example 1: Simple function worker using the decorator
@thread_worker
def compute_fibonacci(n):
    """Compute fibonacci number (slow recursive version for demo)."""
    if n <= 1:
        return n
    return compute_fibonacci.__wrapped__(n - 1) + compute_fibonacci.__wrapped__(n - 2)


# Example 2: Long-running task with progress
def long_running_task(duration=5):
    """Simulate a long-running task."""
    logger.info(f"Starting task that will run for {duration} seconds...")
    for i in range(duration):
        time.sleep(1)
        logger.info(f"Progress: {i+1}/{duration}")
    logger.info("Task complete!")
    return f"Completed after {duration} seconds"


# Example 3: Generator worker (yields intermediate results)
def generate_numbers(start, end):
    """Generate numbers and yield each one."""
    for i in range(start, end + 1):
        time.sleep(0.1)  # Simulate work
        yield i


class WorkerExampleWindow(QMainWindow):
    """Demo window showing different worker patterns."""

    def __init__(self):
        super().__init__()
        self.setWindowTitle("QtAsyncio Worker Examples")
        self.setGeometry(100, 100, 400, 300)

        # Central widget
        central = QWidget()
        self.setCentralWidget(central)
        layout = QVBoxLayout(central)

        # Status label
        self.status_label = QLabel("Ready")
        self.status_label.setAlignment(Qt.AlignCenter)
        layout.addWidget(self.status_label)

        # Example 1: Decorated function worker
        btn1 = QPushButton("Run Decorated Worker (Fibonacci)")
        btn1.clicked.connect(self.run_decorated_worker)
        layout.addWidget(btn1)

        # Example 2: Manual worker creation
        btn2 = QPushButton("Run Manual Worker (Long Task)")
        btn2.clicked.connect(self.run_manual_worker)
        layout.addWidget(btn2)

        # Example 3: Generator worker
        btn3 = QPushButton("Run Generator Worker")
        btn3.clicked.connect(self.run_generator_worker)
        layout.addWidget(btn3)

    def run_decorated_worker(self):
        """Example 1: Using the @thread_worker decorator."""
        self.status_label.setText("Computing Fibonacci(35)...")

        # Create worker using decorated function
        worker = compute_fibonacci(35)

        # Connect signals
        worker.returned.connect(self.on_fib_result)
        worker.errored.connect(self.on_error)
        worker.finished.connect(lambda: logger.info("Fibonacci worker finished"))

        # Start the worker
        worker.start()

    def run_manual_worker(self):
        """Example 2: Manually creating a FunctionWorker."""
        self.status_label.setText("Running long task...")

        # Create worker manually
        worker = create_worker(long_running_task, duration=3)

        # Connect signals
        worker.returned.connect(self.on_task_result)
        worker.errored.connect(self.on_error)
        worker.finished.connect(lambda: logger.info("Long task worker finished"))

        # Start the worker
        worker.start()

    def run_generator_worker(self):
        """Example 3: Using a generator worker that yields intermediate results."""
        self.status_label.setText("Generating numbers...")

        # Create generator worker
        worker = create_worker(generate_numbers, 1, 10)

        # Connect to yielded signal for intermediate results
        worker.yielded.connect(self.on_number_yielded)
        worker.returned.connect(self.on_generator_complete)
        worker.errored.connect(self.on_error)

        # Start the worker
        worker.start()

    def on_fib_result(self, result):
        """Handle fibonacci computation result."""
        self.status_label.setText(f"Fibonacci(35) = {result}")
        logger.info(f"Fibonacci result: {result}")

    def on_task_result(self, result):
        """Handle long task result."""
        self.status_label.setText(result)
        logger.info(f"Task result: {result}")

    def on_number_yielded(self, number):
        """Handle each number yielded by the generator."""
        self.status_label.setText(f"Generated: {number}")
        logger.info(f"Yielded: {number}")

    def on_generator_complete(self, result):
        """Handle generator completion."""
        self.status_label.setText(f"Generator complete! Final value: {result}")
        logger.info(f"Generator complete with result: {result}")

    def on_error(self, error):
        """Handle worker errors."""
        self.status_label.setText(f"Error: {error}")
        logger.error(f"Worker error: {error}")


def main():
    """Run the worker examples."""
    app = QApplication(sys.argv)
    window = WorkerExampleWindow()
    window.show()
    sys.exit(app.exec_())


if __name__ == "__main__":
    main()

```

`examples/ultra_minimal.py`:

```py
"""
Ultra-Minimal Example - The Absolute Smallest Code

This is it. The smallest amount of code to use ida-taskr.
"""

from ida_taskr import cpu_task


# ==============================================================================
# The smallest amount: Just @cpu_task
# ==============================================================================

@cpu_task
def analyze(data):
    """
    Your CPU-intensive function.

    That's it. Add @cpu_task and it runs in the background.
    """
    # Your code here
    result = []
    for byte in data[:32]:
        result.append(byte)
    return result


# ==============================================================================
# Usage
# ==============================================================================

if __name__ == '__main__':
    # Create test data
    data = bytes(range(256))

    # Call it - returns immediately!
    future = analyze(data)

    # Get result when needed
    result = future.result()

    print(f"‚úì Analyzed {len(result)} bytes")
    print(f"‚úì Main thread never blocked")
    print()
    print("=" * 60)
    print("That's the smallest amount: @cpu_task")
    print("=" * 60)
    print()
    print("Just add one line:")
    print()
    print("    @cpu_task")
    print("    def your_function(args):")
    print("        # your code")
    print("        ...")
    print()
    print("Done!")

```

`ida-plugin.json`:

```json
{
  "IDAMetadataDescriptorVersion": 1,
  "plugin": {
    "name": "ida-taskr",
    "entryPoint": "src/ida_taskr/__init__.py",
    "version": "1.0.2",
    "description": "A Qt-based multiprocessing framework for IDA Pro that enables parallel computing without blocking the UI. Features simple decorator API (@cpu_task, @parallel, @shared_memory_task), process-based executors, and bidirectional worker IPC.",
    "idaVersions": ">=9.0",
    "license": "MIT",
    "urls": {
      "repository": "https://github.com/mahmoudimus/ida-taskr"
    },
    "authors": [
      {
        "name": "Mahmoud Abdelkader"
      }
    ],
    "pythonDependencies": [],
    "categories": [
      "analysis",
      "collaboration-and-productivity"
    ],
    "keywords": [
      "multiprocessing",
      "parallel",
      "threading",
      "background-tasks",
      "worker",
      "async",
      "qt",
      "executor",
      "process-pool",
      "shared-memory",
      "decorator",
      "cpu-intensive",
      "non-blocking"
    ]
  }
}

```

`pyproject.toml`:

```toml
[build-system]
requires = ["setuptools", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "ida-taskr"
version = "1.0.2"
description = "A Qt-based multiprocessing framework for IDA Pro parallel computing with simple decorator API"
readme = "README.md"
license = {text = "MIT"}
requires-python = ">=3.10"
authors = [
    {name = "Mahmoud Abdelkader"}
]
keywords = [
    "ida",
    "ida-pro",
    "reverse-engineering",
    "multiprocessing",
    "parallel",
    "threading",
    "background-tasks",
    "qt",
    "pyside6",
    "pyqt5",
]
classifiers = [
    "Development Status :: 5 - Production/Stable",
    "Environment :: Plugins",
    "Intended Audience :: Developers",
    "License :: OSI Approved :: MIT License",
    "Operating System :: OS Independent",
    "Programming Language :: Python :: 3",
    "Programming Language :: Python :: 3.10",
    "Programming Language :: Python :: 3.11",
    "Programming Language :: Python :: 3.12",
    "Programming Language :: Python :: 3.13",
    "Topic :: Security",
    "Topic :: Software Development :: Disassemblers",
]

[project.urls]
Homepage = "https://github.com/mahmoudimus/ida-taskr"
Repository = "https://github.com/mahmoudimus/ida-taskr"
Issues = "https://github.com/mahmoudimus/ida-taskr/issues"

[project.optional-dependencies]
ci = [
    "pytest-timeout>=2.1.0",
    "pytest-cov>=4.0.0",
]
dev = [
    "pytest>=7.0.0",
    "pytest-asyncio>=0.21.0",
    "pytest-timeout>=2.1.0",
    "pytest-cov>=4.0.0",
]
pyqt5 = ["PyQt5>=5.15.0"]
pyside6 = ["PySide6>=6.0.0"]

# Core test dependencies
tests = [
    "pytest>=7.0.0",
    "pytest-asyncio>=0.21.0",
]

[project.scripts]
ida-taskr = "ida_taskr:main"

[tool.pytest.ini_options]
testpaths = ["tests"]
python_files = ["test_*.py", "*_test.py"]
python_classes = ["Test*"]
python_functions = ["test_*"]
addopts = "-v --tb=short"
timeout = 300
asyncio_mode = "auto"

[tool.coverage.run]
branch = true
source = ["src/ida_taskr"]
omit = [
    "*/tests/*",
    "*/test_*",
]

[tool.coverage.report]
precision = 2
show_missing = true
skip_covered = false
```

`run_tests.sh`:

```sh
#!/bin/bash
# Test runner script for ida-taskr tests
#
# This script runs tests using pytest with optional IDA Pro PyQt5 framework support on macOS.
#
# Usage:
#   ./run_tests.sh                    # Run all tests
#   ./run_tests.sh test_event_emitter # Run specific test file
#   ./run_tests.sh TestMessageEmitter # Run specific test class
#   ./run_tests.sh --help            # Show this help message

# Set up the framework path for IDA Pro's PyQt5 (macOS only)
if [[ "$OSTYPE" == "darwin"* ]]; then
    export DYLD_FALLBACK_FRAMEWORK_PATH="/Applications/IDA Professional 9.1.app/Contents/Frameworks"
fi

# Function to show help
show_help() {
    echo "üî¨ ida-taskr Test Runner"
    echo "================================="
    echo
    echo "üìñ Usage:"
    echo "   ./run_tests.sh                    # Run all tests"
    echo "   ./run_tests.sh test_event_emitter # Run specific test file"
    echo "   ./run_tests.sh TestMessageEmitter # Run specific test class"
    echo "   ./run_tests.sh -k 'pattern'       # Run tests matching pattern"
    echo "   ./run_tests.sh --help            # Show this help message"
    echo
    echo "üéØ For anti-deobfuscation tests, you can also set:"
    echo "   export TEST_ROUTINE_ADDR=0x141887cbd  # hex format"
    echo "   export TEST_ROUTINE_ADDR=5394431165   # decimal format"
    echo
    echo "üìã Available test files:"
    echo "   test_anti_deob        # Anti-deobfuscation algorithm tests"
    echo "   test_event_emitter    # MessageEmitter functionality tests"
    echo "   test_task_runner      # TaskRunner functionality tests"
    echo "   test_qtasyncio        # QtAsyncio integration tests"
    echo "   test_thread_executor  # ThreadExecutor tests"
    echo "   test_process_pool_executor  # ProcessPoolExecutor tests"
    echo "   test_interpreter_pool_executor  # InterpreterPoolExecutor tests"
    echo
    echo "üìã Available routine addresses:"
    echo "   5368727905 (0x140004961) - 110 bytes"
    echo "   5368729905 (0x140005131) - 160 bytes"
    echo "   5369441646 (0x1400B2D6E) - 160 bytes"
    echo "   5376900737 (0x1407CFE81) - 160 bytes"
    echo "   5376975793 (0x1407E23B1) - 160 bytes"
    echo "   5390373031 (0x1414A90A7) - 160 bytes"
    echo "   5394431165 (0x141887CBD) - 1325 bytes"
}

echo "üî¨ ida-taskr Test Runner"
echo "================================="
echo

# Get the test name from command line argument
TEST_NAME="$1"

# Handle help requests
case "$TEST_NAME" in
    --help|-h|help)
        show_help
        exit 0
        ;;
esac

# Build pytest command
if [ -n "$TEST_NAME" ]; then
    # Check if it starts with a dash (pytest option)
    if [[ "$TEST_NAME" == -* ]]; then
        echo "üéØ Running tests with options: $@"
        TEST_COMMAND="python -m pytest tests/ $@"
    else
        echo "üéØ Running specific test: $TEST_NAME"
        # Check if it's a test file or test class
        if [[ "$TEST_NAME" == test_* ]]; then
            # It's a test file
            TEST_COMMAND="python -m pytest tests/ -k $TEST_NAME -v"
        else
            # It's probably a test class
            TEST_COMMAND="python -m pytest tests/ -k $TEST_NAME -v"
        fi
    fi
else
    # Check if TEST_ROUTINE_ADDR is set for anti-deob tests
    if [ -n "$TEST_ROUTINE_ADDR" ]; then
        echo "üéØ Running tests for specific address: $TEST_ROUTINE_ADDR"
        TEST_COMMAND="python -m pytest tests/integration/test_anti_deob.py -v"
    else
        echo "üîÑ Running all tests"
        echo "   To run tests for a specific address, set TEST_ROUTINE_ADDR:"
        echo "   export TEST_ROUTINE_ADDR=0x141887cbd  # hex format"
        echo "   export TEST_ROUTINE_ADDR=5394431165   # decimal format"
        echo
        echo "   To run a specific test file:"
        echo "   ./run_tests.sh test_event_emitter"
        echo "   ./run_tests.sh test_task_runner"
        echo
        echo "   For help: ./run_tests.sh --help"
        TEST_COMMAND="python -m pytest tests/ -v"
    fi
fi

echo
echo "üìã Available routine addresses:"
echo "   5368727905 (0x140004961) - 110 bytes"
echo "   5368729905 (0x140005131) - 160 bytes"
echo "   5369441646 (0x1400B2D6E) - 160 bytes"
echo "   5376900737 (0x1407CFE81) - 160 bytes"
echo "   5376975793 (0x1407E23B1) - 160 bytes"
echo "   5390373031 (0x1414A90A7) - 160 bytes"
echo "   5394431165 (0x141887CBD) - 1325 bytes"
echo

echo "üöÄ Starting tests..."
echo "===================="
echo "   Command: $TEST_COMMAND"
echo

# Run the tests and capture the exit code
$TEST_COMMAND
EXIT_CODE=$?

echo
if [ $EXIT_CODE -eq 0 ]; then
    echo "‚úÖ Tests completed successfully!"
else
    echo "‚ùå Tests failed with exit code: $EXIT_CODE"
    if [ -n "$TEST_NAME" ]; then
        echo
        echo "üí° Tip: Make sure the test name is correct. Available tests:"
        echo "   test_anti_deob, test_event_emitter, test_task_runner, test_qtasyncio"
        echo "   Or run './run_tests.sh --help' for more information."
    fi
fi

exit $EXIT_CODE

```

`scripts/amalgamate.py`:

```py
#!/usr/bin/env python3
"""
Generate a single-file amalgamated version of ida-taskr.

This script combines all the ida-taskr modules into a single Python file
that can be easily distributed and used without installing the package.

Usage:
    python scripts/amalgamate.py [--output PATH]
"""

from __future__ import annotations

import argparse
import re
import sys
from pathlib import Path

# Order matters - modules are combined in this sequence
MODULES = [
    "helpers",
    "utils",
    "protocols",
    "qt_compat",
    "qtasyncio",
    "worker",
    "launcher",
    "task_runner",
]

HEADER = '''\
"""
ida-taskr - Amalgamated Single-File Version

A Qt-integrated task worker framework for IDA Pro and standalone Python applications.
Combines: {modules}

Usage:
    from ida_taskr_amalgamated import (
        TaskRunner, WorkerLauncher, WorkerBase, ThreadExecutor,
        ProcessPoolExecutor, InterpreterPoolExecutor, ...
    )
"""

from __future__ import annotations

# =============================================================================
# CONSOLIDATED IMPORTS
# =============================================================================
import asyncio
import atexit
import collections
import concurrent.futures
import contextlib
import dataclasses
import enum
import functools
import inspect
import logging
import math
import multiprocessing
import multiprocessing.connection
import multiprocessing.shared_memory
import os
import pathlib
import pickle
import select
import stat
import sys
import threading
import time
import typing
import uuid
import warnings
import weakref
from abc import ABC, abstractmethod
from bisect import bisect_left, bisect_right
from contextlib import contextmanager
from functools import partial, wraps
from typing import (
    Any,
    Callable,
    Dict,
    Generator,
    Generic,
    Iterable,
    Iterator,
    List,
    Optional,
    Protocol,
    Set,
    Tuple,
    Type,
    TypeVar,
    Union,
    cast,
    overload,
    runtime_checkable,
)

'''

# Exports to include at the end
EXPORTS = [
    # Helpers
    "is_ida",
    "get_logger",
    "configure_logging",
    "MultiprocessingHelper",
    # Utils
    "humanize_bytes",
    "emit",
    "reify",
    "EventEmitter",
    "AsyncEventEmitter",
    "log_execution_time",
    "Range",
    "IntervalSet",
    "resolve_overlaps",
    "PatchManager",
    "DeferredPatchOp",
    "make_chunks",
    "shm_buffer",
    "execute_chunk_with_shm_view",
    "DataProcessorCore",
    # Protocols
    "WorkerProtocol",
    "MessageEmitter",
    # Qt compatibility
    "QtCore",
    "Signal",
    "Slot",
    "QT_API",
    "QT_VERSION",
    "QT_AVAILABLE",
    "QProcessEnvironment",
    "get_qt_api",
    "get_qt_version",
    "QT_ASYNCIO_AVAILABLE",
    # QtAsyncio
    "QAsyncioEventLoop",
    "QAsyncioEventLoopPolicy",
    "set_event_loop_policy",
    "run",
    "Task",
    "FutureWatcher",
    "ThreadExecutor",
    "QThreadPoolExecutor",
    "ThreadPoolExecutorSignals",
    "ProcessPoolExecutor",
    "QProcessPoolExecutor",
    "ProcessPoolExecutorSignals",
    "InterpreterPoolExecutor",
    "QInterpreterPoolExecutor",
    "InterpreterPoolExecutorSignals",
    "INTERPRETER_POOL_AVAILABLE",
    # Worker
    "ConnectionContext",
    "WorkerController",
    "WorkerBase",
    "QTASYNCIO_ENABLED",
    # Launcher
    "TemporarilyDisableNotifier",
    "ConnectionReader",
    "QtListener",
    "WorkerLauncher",
    # Task Runner
    "TaskRunner",
]


def remove_module_docstring(source: str) -> str:
    """Remove the module-level docstring from source code."""
    lines = source.split('\n')
    result_lines = []
    i = 0

    # Skip leading whitespace/blank lines
    while i < len(lines) and not lines[i].strip():
        i += 1

    if i >= len(lines):
        return source

    # Check for module docstring
    first_line = lines[i].strip()
    if first_line.startswith('"""') or first_line.startswith("'''"):
        quote = first_line[:3]
        # Check if single-line docstring
        if first_line.count(quote) >= 2:
            i += 1
        else:
            # Multi-line docstring - skip until closing quote
            i += 1
            while i < len(lines) and quote not in lines[i]:
                i += 1
            i += 1  # Skip the closing quote line

    # Return rest of the file
    return '\n'.join(lines[i:])


def remove_imports(source: str) -> str:
    """Remove top-level import statements from source code.

    Keeps imports inside try/except blocks (conditional imports) and
    imports inside functions/classes.
    """
    lines = source.split('\n')
    result_lines = []
    i = 0
    indent_stack = []  # Track indentation to detect top-level vs nested

    while i < len(lines):
        line = lines[i]
        stripped = line.strip()

        # Calculate indentation level
        if stripped:
            indent = len(line) - len(line.lstrip())
        else:
            indent = 0

        # Check for import statements
        if stripped.startswith('from __future__'):
            # Always skip future imports
            i += 1
            continue
        elif stripped.startswith('import ') or stripped.startswith('from '):
            # Only remove top-level imports (no indentation)
            # Keep conditional imports (inside try/except) and nested imports
            if indent == 0:
                # Skip ida_taskr internal imports at top level
                if 'ida_taskr' in stripped:
                    i += 1
                    continue
                # Check for multi-line import
                if '(' in stripped and ')' not in stripped:
                    # Multi-line import - skip until closing paren
                    i += 1
                    while i < len(lines) and ')' not in lines[i]:
                        i += 1
                i += 1
                continue
            # Keep indented imports (they're conditional or nested)

        result_lines.append(line)
        i += 1

    return '\n'.join(result_lines)


def process_module(module_path: Path) -> str:
    """Process a single module file, removing imports and module docstring."""
    source = module_path.read_text()

    # Remove module docstring
    source = remove_module_docstring(source)

    # Remove imports
    source = remove_imports(source)

    # Remove excessive leading blank lines but keep structure
    while source.startswith('\n\n\n'):
        source = source[1:]

    return source


def make_section(name: str, content: str) -> str:
    """Create a section with header."""
    header = f"""
# =============================================================================
# {name.upper()} MODULE
# =============================================================================
"""
    return header + content


def amalgamate(src_dir: Path) -> str:
    """Generate the amalgamated source."""
    parts = [HEADER.format(modules=", ".join(MODULES))]

    # Process each module
    for module_name in MODULES:
        module_path = src_dir / f"{module_name}.py"
        if not module_path.exists():
            print(f"Warning: {module_path} not found, skipping", file=sys.stderr)
            continue

        content = process_module(module_path)
        if content.strip():
            section_name = module_name.replace('_', ' ')
            parts.append(make_section(section_name, content))

    # Exports
    exports_section = """
# =============================================================================
# EXPORTS
# =============================================================================

__all__ = [
"""
    for export in EXPORTS:
        exports_section += f'    "{export}",\n'
    exports_section += "]\n"
    parts.append(exports_section)

    return ''.join(parts)


def main():
    parser = argparse.ArgumentParser(description="Generate amalgamated ida-taskr")
    parser.add_argument(
        "--output", "-o",
        type=Path,
        default=Path("ida_taskr_amalgamated.py"),
        help="Output file path (default: ida_taskr_amalgamated.py)"
    )
    parser.add_argument(
        "--src-dir",
        type=Path,
        default=None,
        help="Source directory (default: auto-detect)"
    )
    args = parser.parse_args()

    # Find source directory
    if args.src_dir:
        src_dir = args.src_dir
    else:
        # Try to find it relative to this script
        script_dir = Path(__file__).parent
        candidates = [
            script_dir.parent / "src" / "ida_taskr",
            script_dir / "src" / "ida_taskr",
            Path("src") / "ida_taskr",
        ]
        src_dir = None
        for candidate in candidates:
            if candidate.exists() and (candidate / "helpers.py").exists():
                src_dir = candidate
                break

        if not src_dir:
            print("Error: Could not find ida_taskr source directory", file=sys.stderr)
            sys.exit(1)

    print(f"Source directory: {src_dir}", file=sys.stderr)
    print(f"Output file: {args.output}", file=sys.stderr)

    result = amalgamate(src_dir)
    args.output.write_text(result)

    line_count = result.count('\n') + 1
    print(f"Generated {args.output} ({len(result)} bytes, {line_count} lines)", file=sys.stderr)


if __name__ == "__main__":
    main()

```

`src/ida_taskr/__init__.py`:

```py
"""
IDA Worker Manager - A reusable task worker manager library for IDA Pro.

This library provides a Qt-based multiprocessing framework for running
CPU-intensive tasks outside of IDA's main thread while maintaining
bidirectional communication.
"""

from .helpers import MultiprocessingHelper, get_logger, is_ida
from .launcher import WorkerLauncher
from .protocols import MessageEmitter, WorkerProtocol

# New import for TaskRunner
from .task_runner import TaskRunner
from .utils import DataProcessorCore
from .worker import ConnectionContext, WorkerBase

# Decorators for simplified API
from .decorators import background_task, cpu_task, io_task, parallel, shared_memory_task

# QtAsyncio integration (optional)
from .qt_compat import QT_ASYNCIO_AVAILABLE

# Always define INTERPRETER_POOL_AVAILABLE (False if Qt not available)
INTERPRETER_POOL_AVAILABLE = False

__all__ = [
    "WorkerLauncher",
    "WorkerBase",
    "ConnectionContext",
    "MultiprocessingHelper",
    "get_logger",
    "is_ida",
    "WorkerProtocol",
    "MessageEmitter",
    "TaskRunner",
    "DataProcessorCore",
    "QT_ASYNCIO_AVAILABLE",
    "INTERPRETER_POOL_AVAILABLE",
    # Decorators
    "background_task",
    "cpu_task",
    "io_task",
    "parallel",
    "shared_memory_task",
]

# Conditionally export qtasyncio utilities if available
if QT_ASYNCIO_AVAILABLE:
    try:
        from .qtasyncio import (
            # Asyncio integration
            QAsyncioEventLoop,
            QAsyncioEventLoopPolicy,
            run as qtasyncio_run,
            set_event_loop_policy,
            # Thread executor
            ThreadExecutor,
            QThreadPoolExecutor,
            Task,
            FutureWatcher,
            # Process executor
            ProcessPoolExecutor,
            QProcessPoolExecutor,
            # Interpreter executor
            InterpreterPoolExecutor,
            QInterpreterPoolExecutor,
            INTERPRETER_POOL_AVAILABLE as _INTERPRETER_POOL_AVAILABLE,
            # Worker utilities
            WorkerBase as QtWorkerBase,
            FunctionWorker,
            GeneratorWorker,
            create_worker,
            thread_worker,
            new_worker_qthread,
        )

        # Update module-level variable with actual value from qtasyncio
        INTERPRETER_POOL_AVAILABLE = _INTERPRETER_POOL_AVAILABLE

        __all__.extend([
            # Asyncio integration
            "QAsyncioEventLoop",
            "QAsyncioEventLoopPolicy",
            "qtasyncio_run",
            "set_event_loop_policy",
            # Thread executor
            "ThreadExecutor",
            "QThreadPoolExecutor",
            "Task",
            "FutureWatcher",
            # Process executor
            "ProcessPoolExecutor",
            "QProcessPoolExecutor",
            # Interpreter executor (Python 3.13+)
            "InterpreterPoolExecutor",
            "QInterpreterPoolExecutor",
            "INTERPRETER_POOL_AVAILABLE",
            # Worker utilities
            "QtWorkerBase",
            "FunctionWorker",
            "GeneratorWorker",
            "create_worker",
            "thread_worker",
            "new_worker_qthread",
        ])
    except ImportError:
        # QtAsyncio module not available
        pass

__version__ = "1.0.2"

```

`src/ida_taskr/decorators.py`:

```py
"""
Decorators for simplifying async task execution.

Provides simple decorator-based API for running CPU-intensive tasks
in the background without blocking.
"""

import functools
from typing import Callable, Optional, Any
from concurrent.futures import Future

from .qt_compat import QT_AVAILABLE

if QT_AVAILABLE:
    from .qtasyncio import ProcessPoolExecutor, ThreadExecutor

# Global executor instance (created on first use)
_global_executor = None


def get_global_executor(max_workers: Optional[int] = None):
    """Get or create the global background task executor."""
    global _global_executor

    if _global_executor is None:
        if QT_AVAILABLE:
            _global_executor = ProcessPoolExecutor(max_workers=max_workers)
        else:
            raise RuntimeError("Qt is not available - cannot create executor")

    return _global_executor


def background_task(
    func: Optional[Callable] = None,
    *,
    max_workers: Optional[int] = None,
    on_complete: Optional[Callable[[Any], None]] = None,
    on_error: Optional[Callable[[Exception], None]] = None,
    on_progress: Optional[Callable[[int, str], None]] = None,
    timeout: Optional[float] = None,
    executor_type: str = 'thread'  # 'thread' (default) or 'process'
):
    """
    Decorator to run a function as a background task without blocking.

    Transforms a regular function into one that runs in a separate process/thread
    and returns a Future immediately.

    Args:
        func: The function to decorate (auto-filled when used without parens)
        max_workers: Maximum number of parallel workers (default: CPU count)
        on_complete: Callback when task completes, receives result
        on_error: Callback when task fails, receives exception
        on_progress: Callback for progress updates (progress%, message)
        timeout: Optional timeout in seconds
        executor_type: 'process' for CPU-bound, 'thread' for I/O-bound

    Returns:
        Decorated function that returns a Future when called

    Examples:
        Basic usage:
        >>> @background_task
        ... def slow_function(x):
        ...     return x * 2
        ...
        >>> future = slow_function(5)  # Returns immediately
        >>> result = future.result()   # Get result when needed

        With callback:
        >>> @background_task(on_complete=lambda r: print(f"Done: {r}"))
        ... def slow_function(x):
        ...     return x * 2
        ...
        >>> slow_function(5)  # Callback fires when done

        With progress:
        >>> @background_task(
        ...     on_progress=lambda p, m: print(f"[{p}%] {m}"),
        ...     on_complete=lambda r: print(f"Result: {r}")
        ... )
        ... def slow_function(x, progress_callback=None):
        ...     if progress_callback:
        ...         progress_callback(50, "Halfway done")
        ...     return x * 2

        Multiple parallel tasks:
        >>> @background_task(max_workers=4)
        ... def process_item(item):
        ...     return expensive_computation(item)
        ...
        >>> futures = [process_item(x) for x in range(100)]
        >>> results = [f.result() for f in futures]
    """

    def decorator(fn: Callable) -> Callable:
        """The actual decorator function."""

        # Create or get executor
        if executor_type == 'thread' and QT_AVAILABLE:
            from .qtasyncio import ThreadExecutor
            executor = ThreadExecutor(max_workers=max_workers)
        else:
            executor = get_global_executor(max_workers=max_workers)

        # Connect callbacks to signals if provided
        if on_complete:
            def handle_complete(future: Future):
                try:
                    result = future.result(timeout=timeout)
                    on_complete(result)
                except Exception:
                    pass  # Error handled by on_error if registered

            executor.signals.task_completed.connect(handle_complete)

        if on_error:
            def handle_error(future: Future, exception: Exception):
                on_error(exception)

            executor.signals.task_failed.connect(handle_error)

        @functools.wraps(fn)
        def wrapper(*args, **kwargs):
            """Wrapper that submits the task to background executor."""

            # Inject progress callback if the function accepts it
            if on_progress:
                import inspect
                sig = inspect.signature(fn)
                if 'progress_callback' in sig.parameters:
                    kwargs['progress_callback'] = on_progress

            # Submit to executor (returns immediately)
            future = executor.submit(fn, *args, **kwargs)

            return future

        # Attach executor for manual control if needed
        wrapper.executor = executor

        return wrapper

    # Handle both @background_task and @background_task(...)
    if func is None:
        # Called with arguments: @background_task(max_workers=4)
        return decorator
    else:
        # Called without arguments: @background_task
        return decorator(func)


def cpu_task(
    func: Optional[Callable] = None,
    *,
    max_workers: Optional[int] = None,
    on_complete: Optional[Callable] = None,
    on_error: Optional[Callable] = None,
):
    """
    Simplified decorator for CPU-intensive tasks.

    Uses ThreadExecutor by default for compatibility. For truly parallel CPU work
    (bypassing GIL), use ProcessPoolExecutor directly.

    Note: In IDA, most CPU work calls IDA SDK functions which release the GIL,
    so threads work well. Use this for pattern analysis, signature generation, etc.

    Example:
        >>> @cpu_task(on_complete=lambda r: print(f"Result: {r}"))
        ... def find_signature(data):
        ...     # CPU-intensive work here
        ...     return analyze_pattern(data)
        ...
        >>> future = find_signature(binary_data)  # Runs in background thread
    """
    return background_task(
        func,
        max_workers=max_workers,
        on_complete=on_complete,
        on_error=on_error,
        executor_type='thread'
    )


def io_task(
    func: Optional[Callable] = None,
    *,
    max_workers: Optional[int] = None,
    on_complete: Optional[Callable] = None,
    on_error: Optional[Callable] = None,
):
    """
    Simplified decorator for I/O-bound tasks.

    Alias for @background_task with executor_type='thread'.
    Use for I/O-bound work (network requests, file operations, database queries, etc.)

    Example:
        >>> @io_task(on_complete=lambda r: print(f"Downloaded: {r}"))
        ... def fetch_data(url):
        ...     return requests.get(url).json()
        ...
        >>> future = fetch_data("https://api.example.com/data")
    """
    return background_task(
        func,
        max_workers=max_workers,
        on_complete=on_complete,
        on_error=on_error,
        executor_type='thread'
    )


def parallel(max_workers: int = None):
    """
    Decorator for functions that should run multiple instances in parallel.

    Convenience decorator that sets up parallel execution with sensible defaults.
    Uses threads for compatibility - for true parallel CPU work, use ProcessPoolExecutor.

    Args:
        max_workers: Number of parallel workers (default: CPU count)

    Example:
        >>> @parallel(max_workers=8)
        ... def process_function(address):
        ...     # Analyze function at address
        ...     return analysis_result
        ...
        >>> # Process 100 functions in parallel across 8 workers
        >>> futures = [process_function(addr) for addr in function_addresses]
        >>> results = [f.result() for f in futures]
    """
    return background_task(max_workers=max_workers, executor_type='thread')


def shared_memory_task(
    func: Optional[Callable] = None,
    *,
    num_chunks: int = 8,
    max_workers: Optional[int] = None,
    on_complete: Optional[Callable[[list], None]] = None,
    on_error: Optional[Callable[[Exception], None]] = None,
):
    """
    Decorator for processing large data using shared memory across multiple processes.

    Handles all the shared memory complexity:
    - Creates shared memory segment
    - Copies data once
    - Calculates chunk boundaries
    - Creates workers that attach to shared memory
    - Collects results
    - Cleanup (memoryview, close, unlink)

    User just writes the chunk processing logic!

    Args:
        func: The chunk processing function (auto-filled when used without parens)
        num_chunks: Number of chunks to split data into (default: 8)
        max_workers: Maximum number of parallel workers (default: num_chunks)
        on_complete: Callback when all chunks complete, receives list of results
        on_error: Callback when any chunk fails, receives exception

    The decorated function receives:
        chunk_data: memoryview of this chunk's data
        chunk_id: 0-based chunk index
        total_chunks: total number of chunks

    Returns:
        Decorated function that takes full data and returns list of all chunk results

    Example:
        >>> @shared_memory_task(num_chunks=8)
        ... def analyze_chunk(chunk_data, chunk_id, total_chunks):
        ...     # Process this chunk
        ...     signatures = find_patterns(chunk_data)
        ...     return {'chunk': chunk_id, 'sigs': signatures}
        ...
        >>> # Usage - just pass the full data!
        >>> results = analyze_chunk(binary_data)  # Returns list of 8 results
        >>> # ida-taskr handles all shared memory complexity

    Real-world IDA example:
        >>> @shared_memory_task(num_chunks=16)
        ... def find_signatures(chunk_data, chunk_id, total_chunks):
        ...     signatures = []
        ...     for i in range(len(chunk_data)):
        ...         if is_interesting_pattern(chunk_data[i:i+16]):
        ...             signatures.append(bytes(chunk_data[i:i+16]))
        ...     return signatures
        ...
        >>> # Get binary data from IDA
        >>> binary_data = ida_bytes.get_bytes(start_ea, size)
        >>> # Process in parallel - shared memory handles 8MB+ efficiently
        >>> all_signatures = find_signatures(binary_data)
    """
    import multiprocessing.shared_memory as shm_module

    def decorator(fn: Callable) -> Callable:
        """The actual decorator function."""

        @functools.wraps(fn)
        def wrapper(data: bytes) -> list:
            """
            Wrapper that handles all shared memory setup/teardown.

            Args:
                data: Full binary data to process

            Returns:
                List of results from all chunks
            """
            if not QT_AVAILABLE:
                raise RuntimeError("Qt is not available - cannot use shared memory tasks")

            # Create shared memory segment
            shm = shm_module.SharedMemory(create=True, size=len(data))

            try:
                # Copy data into shared memory (ONCE!)
                shm.buf[:len(data)] = data

                # Calculate chunk boundaries
                chunk_size = len(data) // num_chunks
                chunks_info = []

                for i in range(num_chunks):
                    start = i * chunk_size
                    # Last chunk gets any remainder
                    end = start + chunk_size if i < num_chunks - 1 else len(data)
                    chunks_info.append((start, end, i))

                # Create executor
                workers = max_workers if max_workers else num_chunks
                executor = ProcessPoolExecutor(max_workers=workers)

                try:
                    # Submit all chunks
                    futures = []
                    for start, end, chunk_id in chunks_info:
                        future = executor.submit(
                            _shared_memory_worker,
                            fn,
                            shm.name,
                            start,
                            end,
                            chunk_id,
                            num_chunks
                        )
                        futures.append(future)

                    # Collect results
                    results = []
                    for future in futures:
                        try:
                            result = future.result()
                            results.append(result)
                        except Exception as e:
                            if on_error:
                                on_error(e)
                            raise

                    # Call completion callback if provided
                    if on_complete:
                        on_complete(results)

                    return results

                finally:
                    executor.shutdown(wait=True)

            finally:
                # Cleanup shared memory
                shm.close()
                shm.unlink()

        return wrapper

    # Handle both @shared_memory_task and @shared_memory_task(...)
    if func is None:
        # Called with arguments: @shared_memory_task(num_chunks=16)
        return decorator
    else:
        # Called without arguments: @shared_memory_task
        return decorator(func)


def _shared_memory_worker(fn: Callable, shm_name: str, start: int, end: int, chunk_id: int, total_chunks: int):
    """
    Worker function that attaches to shared memory and processes a chunk.

    This needs to be a module-level function so it can be pickled for ProcessPoolExecutor.

    Args:
        fn: The user's chunk processing function
        shm_name: Name of the shared memory segment
        start: Start offset in shared memory
        end: End offset in shared memory
        chunk_id: This chunk's ID
        total_chunks: Total number of chunks

    Returns:
        Result from user's chunk processing function
    """
    import multiprocessing.shared_memory as shm_module

    # Attach to existing shared memory
    shm = shm_module.SharedMemory(name=shm_name)

    try:
        # Get view of this chunk (no data copying!)
        chunk_data = memoryview(shm.buf)[start:end]

        # Call user's function with chunk data
        result = fn(chunk_data, chunk_id, total_chunks)

        return result

    finally:
        # CRITICAL: Delete memoryview before closing shared memory
        del chunk_data
        shm.close()


# Export all decorators
__all__ = [
    'background_task',
    'cpu_task',
    'io_task',
    'parallel',
    'shared_memory_task',
    'get_global_executor',
]

```

`src/ida_taskr/helpers.py`:

```py
"""Helper utilities for the worker manager."""

import functools
import logging
import multiprocessing
import os
import pathlib
import stat
import sys


def is_ida():
    """
    Crude check to see if running inside IDA.
    Returns True if running inside IDA Pro, else False.

    we do not use this check because there's a possibility that
    we're running inside a headless IDA mode and that uses an
    `idapro` library which will make this succeed.

    Even though that is technically "running" in IDA, we can
    still use the python interpreter that's executing the script.
    The reason we want to know if we're in the IDA application is
    because we want to find the python interpreter that IDA is using
    to execute the script and to not start a new IDA instance.

    Maybe this function can be called `is_ida_application` or something.

        try:
            import idaapi  # noqa
            return True
        except ImportError:
            return False
    """
    # Check if sys.executable is an IDA executable
    exec_name = pathlib.Path(sys.executable).name.lower()
    if exec_name.startswith(("ida", "idat", "idaw", "idag")):
        return True

    # Also check if stdout is IDAPythonStdOut (when running scripts via -S flag)
    # This handles the case where IDA runs scripts with -S"script.py"
    if type(sys.stdout).__name__ == "IDAPythonStdOut":
        return True

    return False


# on windows, we need to set the encoding to utf-8 because it defaults to cp1252
# which does not support the emoji characters used in the logging
# or really any non-ascii characters
if not is_ida():
    # this works in non IDA and for python 3.7+
    sys.stdout.reconfigure(encoding="utf-8")  # type: ignore
else:
    # IDA wraps sys.stdout and does not expose the `reconfigure` method
    # so we need to set the encoding manually
    sys.stdout.encoding = "utf-8"  # type: ignore


def configure_logging(
    log,
    level=logging.INFO,
    handler_filters=None,
    fmt_str="[%(name)s:%(levelname)s:%(process)d:%(threadName)s] @ %(asctime)s %(message)s",
):
    """Configure logging with proper formatting and filters."""
    log.propagate = False
    log.setLevel(level)
    formatter = logging.Formatter(fmt_str)
    handler = logging.StreamHandler(stream=sys.stdout)
    handler.setFormatter(formatter)
    handler.setLevel(level)

    if handler_filters is not None:
        for _filter in handler_filters:
            handler.addFilter(_filter)

    for handler in log.handlers[:]:
        log.removeHandler(handler)
        handler.close()

    if not log.handlers:
        log.addHandler(handler)


def get_logger(name=None, configurer=None, log_level=logging.INFO, custom_logger=None):
    """Get a configured logger instance."""
    if custom_logger:
        return custom_logger
    if not configurer:
        configurer = functools.partial(configure_logging, level=log_level)
    prefix = "ida." if is_ida() else "worker."
    name = name or f"{prefix}{__name__}"
    logger = logging.getLogger(name)
    configurer(logger)
    return logger


class MultiprocessingHelper:
    """
    Static helper class for multiprocessing context and Python interpreter discovery.
    """

    @staticmethod
    @functools.lru_cache(maxsize=1)
    def get_python_interpreter():
        """
        Gets the path to a suitable Python interpreter.
        Ensures we find a standalone Python executable.
        >>> import pathlib, sys, stat
        >>> interp: pathlib.Path = MultiprocessingHelper.get_python_interpreter()
        ...
        >>>
        """
        logger = get_logger()

        if (
            hasattr(sys, "_base_executable")  # type: ignore
            and sys._base_executable  # type: ignore
            and "python" in pathlib.Path(sys._base_executable).name.lower()  # type: ignore
        ):
            logger.debug(f"Using _base_executable: {sys._base_executable}")  # type: ignore
            return pathlib.Path(sys._base_executable)  # type: ignore

        base_paths = [
            sys.prefix,
            sys.exec_prefix,
            sys.executable,
        ]
        exe_suffix = ".exe" if os.name == "nt" else ""
        python_name = f"python{exe_suffix}"

        def base_dirs():
            for dirname in map(pathlib.Path, base_paths):
                yield dirname
                yield dirname.parent
                yield dirname.parent.parent

        for dirname in base_dirs():
            for basename in ["", "bin", "python"]:
                interp_path = dirname / basename / python_name
                if not interp_path.exists():
                    continue

                if not (interp_path.is_file() or interp_path.is_symlink()):
                    continue

                st_mode = interp_path.stat().st_mode
                if not st_mode & (stat.S_IXUSR | stat.S_IXGRP | stat.S_IXOTH):
                    continue

                logger.debug(f"Found Python interpreter at: {interp_path}")
                return interp_path

        logger.warning(
            "Could not determine Python interpreter path, falling back to 'python' in PATH."
        )
        return pathlib.Path("python")

    @staticmethod
    def set_multiprocessing_context():
        """
        Sets up the multiprocessing context to use 'spawn' and sets the Python executable.
        """
        current_method = multiprocessing.get_start_method(allow_none=True)
        if current_method != "spawn":
            multiprocessing.set_start_method("spawn", force=True)
        multiprocessing.set_executable(
            str(MultiprocessingHelper.get_python_interpreter())
        )


# Initialize multiprocessing context
MultiprocessingHelper.set_multiprocessing_context()

```

`src/ida_taskr/launcher.py`:

```py
"""IDA-side components for launching and managing worker processes."""

import multiprocessing
import multiprocessing.connection
import os
import pickle
import select
import sys

from .helpers import MultiprocessingHelper, get_logger
from .protocols import MessageEmitter
from .qt_compat import QtCore, Signal, QProcessEnvironment, QT_AVAILABLE

logger = get_logger()


class TemporarilyDisableNotifier:
    """Context manager to temporarily disable a QSocketNotifier."""

    def __init__(self, notifier):
        self.notifier = notifier
        self.was_enabled = False

    def __enter__(self):
        self.was_enabled = self.notifier.isEnabled()
        self.notifier.setEnabled(False)
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        if self.was_enabled:
            self.notifier.setEnabled(True)


class ConnectionReader(QtCore.QThread):
    """Qt thread for reading messages from worker connection."""

    message_received = Signal(object)
    connection_closed = Signal()

    def __init__(self, parent=None):
        super().__init__(parent)
        self.connection: multiprocessing.connection.Connection | None = None

    def set_connection(self, conn):
        self.connection = conn

    def run(self):
        if not self.connection:
            return

        try:
            while True:
                msg = self.connection.recv()
                self.message_received.emit(msg)
        except (EOFError, OSError):
            self.connection_closed.emit()
        except pickle.PickleError as e:
            logger.error(f"Pickle error: {e}")
        finally:
            try:
                self.connection.close()
            except Exception:
                pass


class QtListener(QtCore.QObject):
    """Qt-friendly wrapper for multiprocessing.connection.Listener."""

    family = "AF_INET"
    connection_accepted = Signal(object)
    connection_error = Signal(str)

    def __init__(self, address=None, backlog=1, authkey=None, parent=None):
        super().__init__(parent)

        self._listener = multiprocessing.connection.Listener(
            address, self.family, backlog, authkey
        )

        self._socket = self._listener._listener._socket  # type: ignore
        self._notifier = QtCore.QSocketNotifier(
            self._socket.fileno(), QtCore.QSocketNotifier.Read, self
        )
        self._notifier.activated.connect(self._on_connection_ready)
        self._notifier.setEnabled(True)

    def _on_connection_ready(self):
        with TemporarilyDisableNotifier(self._notifier):
            try:
                ready, _, _ = select.select([self._socket], [], [], 0)
                if not ready:
                    return

                conn = self.accept()
                self.connection_accepted.emit(conn)
            except Exception as e:
                logger.error(f"Error accepting connection: {e}")
                self.connection_error.emit(str(e))

    def accept(self):
        return self._listener.accept()

    def close(self):
        self._notifier.setEnabled(False)
        self._notifier.deleteLater()
        self._listener.close()

    @property
    def address(self):
        return self._listener.address


class WorkerLauncher(QtCore.QProcess):
    """
    Manages external worker processes using QProcess.
    Provides bidirectional communication via multiprocessing connections.
    """

    # Signals
    processing_results = Signal(dict)
    error_occurred_msg = Signal(str)
    worker_message = Signal(object)
    worker_connected = Signal()
    worker_disconnected = Signal()

    def __init__(self, message_emitter: MessageEmitter | None = None, parent=None):
        super(WorkerLauncher, self).__init__(parent)

        self.message_emitter = message_emitter

        self.readyReadStandardOutput.connect(self._on_stdout)
        self.readyReadStandardError.connect(self._on_stderr)
        self.errorOccurred.connect(self._on_error)
        self.stateChanged.connect(self._on_state_changed)

        self.python_interpreter = MultiprocessingHelper.get_python_interpreter()

        self.listener = None
        self.connection = None
        self.authkey = None

        self.reader_thread = ConnectionReader(self)
        self.reader_thread.message_received.connect(self._on_worker_message)
        self.reader_thread.connection_closed.connect(self._on_connection_closed)

        self.connection_attempts = 0
        self.max_connection_attempts = 10

        self._streams: dict[str, dict] = {}

    def is_not_running(self):
        return self.state() == QtCore.QProcess.NotRunning

    def _on_worker_message(self, message):
        """Process messages from the worker, including chunked streams."""
        # Handle chunked messages
        msg_id = message.get("message_id")
        if msg_id:
            idx = message["chunk_index"]
            total = message["total_chunks"]
            msg_type = message.get("type")

            stream = self._streams.setdefault(
                msg_id, {"type": msg_type, "chunks": {}, "total": total}
            )
            stream["chunks"][idx] = message["data"]
            logger.info(
                "Received chunk %d/%d for %r (id=%s)",
                idx + 1,
                total,
                msg_type,
                msg_id,
            )

            if len(stream["chunks"]) == total:
                full = []
                for i in range(total):
                    full.extend(stream["chunks"][i])
                del self._streams[msg_id]
                logger.info(
                    "%r streaming complete (id=%s, %d items)",
                    msg_type,
                    msg_id,
                    len(full),
                )
                self.processing_results.emit(
                    {
                        "type": "result",
                        "results": full,
                        "status": "success",
                    }
                )
            return

        # Non-chunked messages
        logger.debug("‚Üê Received message from worker: %r", message)
        self.worker_message.emit(message)

        # Delegate to MessageEmitter if available
        if self.message_emitter and isinstance(message, dict):
            msg_type = message.get("type")
            if msg_type == "error":
                self.message_emitter.emit_worker_error(
                    message.get("error", "Unknown error")
                )
            elif msg_type == "result":
                self.message_emitter.emit_worker_results(
                    {
                        "results": message.get("data"),
                        "status": message.get("status", "success"),
                    }
                )
            elif msg_type == "status":
                self.message_emitter.emit_worker_message(message)

    def _on_connection_closed(self):
        logger.info("Worker IPC connection closed.")
        if self.connection:
            try:
                self.connection.close()
            except:
                pass
            self.connection = None
        self.worker_disconnected.emit()

        if self.message_emitter:
            self.message_emitter.emit_worker_disconnected()

    def launch_worker(self, script_path: str, worker_args: dict):
        """
        Starts the worker script with specified arguments.

        Args:
            script_path: Path to the worker script
            worker_args: Dictionary of arguments to pass to the worker
        """
        self._cleanup_resources()

        # Create listener
        self.authkey = os.urandom(32)
        logger.info(f"Generated Authkey: {self.authkey.hex()}")
        self.listener = QtListener(
            ("localhost", 0),
            authkey=self.authkey,
            parent=self,
        )
        address = self.listener.address
        logger.info(f"Created listener on {address}")
        self.listener.connection_accepted.connect(self._on_connection_accepted)
        self.listener.connection_error.connect(self._on_connection_error)

        # Prepare QProcess
        env = QProcessEnvironment.systemEnvironment()
        env.insert("PYTHON_PATH", str(self.python_interpreter.parent))
        env.insert("PYTHON_BIN", str(self.python_interpreter.name))
        self.setProcessEnvironment(env)

        # Build command line arguments
        args = ["-u", script_path]

        # Add connection info
        args.extend(
            [
                "--address",
                f"{address[0]}:{address[1]}",
                "--authkey",
                self.authkey.hex(),
            ]
        )

        # Add worker-specific arguments
        for key, value in worker_args.items():
            args.extend([f"--{key}", str(value)])

        logger.info(f"Starting worker process: {self.python_interpreter} {args}")
        self.start(str(self.python_interpreter), args)

        if not self.waitForStarted(5000):
            logger.error(f"Worker process failed to start: {self.errorString()}")
            self._cleanup_resources()
            return False

        self.connection_attempts = 0
        logger.info("Worker process started. Beginning connection attempts...")
        return True

    def _on_connection_accepted(self, conn):
        logger.info("Connection from worker accepted")
        self.reader_thread.set_connection(conn)
        if not self.reader_thread.isRunning():
            self.reader_thread.start()
        self.connection = conn
        self.worker_connected.emit()

        if self.message_emitter:
            self.message_emitter.emit_worker_connected()

    def _on_connection_error(self, error_msg):
        logger.error(f"Connection error: {error_msg}")
        self.connection_attempts += 1

        if self.connection_attempts >= self.max_connection_attempts:
            self.error_occurred_msg.emit(
                f"Failed to connect to worker after {self.max_connection_attempts} attempts"
            )
            self.stop_worker()

    def _cleanup_resources(self):
        if self.reader_thread.isRunning():
            self.reader_thread.terminate()
            self.reader_thread.wait()

        if self.connection:
            try:
                self.connection.close()
            except:
                pass
            self.connection = None

        if self.listener:
            try:
                self.listener.close()
            except:
                pass
            self.listener = None

    def stop_worker(self):
        """Attempts to terminate the worker process gracefully."""
        if self.is_not_running() and not self.connection:
            logger.debug("Worker process was already stopped and connection closed.")
            return

        logger.info("Attempting to stop worker process...")

        if self.connection:
            try:
                logger.info("Sending exit command...")
                self.send_command({"command": "exit"})
                if self.waitForFinished(1000):
                    logger.info("Worker exited gracefully.")
                    self._cleanup_resources()
                    return
            except Exception as e:
                logger.error(f"Error sending exit command: {e}")

        if not self.is_not_running():
            logger.warning("Worker did not exit gracefully, terminating...")
            self.terminate()
            if not self.waitForFinished(2000):
                logger.warning("Worker did not terminate, killing...")
                self.kill()
                self.waitForFinished(1000)

        self._cleanup_resources()
        logger.info("Worker process shutdown complete.")

    def send_command(self, command):
        """Sends a command object via the connection."""
        if not self.connection:
            logger.warning(
                f"Cannot send command '{command}', IPC connection not established."
            )
            return False

        logger.debug(f"‚Üí Sending command: {command}")
        try:
            self.connection.send(command)
            logger.debug(f"‚Üí Successfully sent command: {command}")
            return True
        except Exception as e:
            logger.error(f"Failed to send command '{command}': {e}")
            self._on_connection_closed()
            return False

    def _on_stdout(self):
        out_bytes = self.readAllStandardOutput()
        if not out_bytes:
            return
        out = out_bytes.data().decode("utf-8", errors="replace")
        if out.strip():
            print(out.strip(), flush=True)

    def _on_stderr(self):
        data = self.readAllStandardError().data()
        if data:
            data = data.decode("utf-8", errors="replace").strip()
            print(data.strip(), file=sys.stderr, flush=True)

    def _on_error(self, error: QtCore.QProcess.ProcessError):
        error_map = {
            QtCore.QProcess.FailedToStart: "FailedToStart",
            QtCore.QProcess.Crashed: "Crashed",
            QtCore.QProcess.Timedout: "Timedout",
            QtCore.QProcess.ReadError: "ReadError",
            QtCore.QProcess.WriteError: "WriteError",
            QtCore.QProcess.UnknownError: "UnknownError",
        }
        error_str = error_map.get(error, f"UnknownError({error})")
        msg = f"Worker process error: {error_str} - {self.errorString()}"
        print(msg.strip(), file=sys.stderr, flush=True)
        self.error_occurred_msg.emit(msg)

    def _on_state_changed(self, state: QtCore.QProcess.ProcessState):
        state_map = {
            QtCore.QProcess.NotRunning: "NotRunning",
            QtCore.QProcess.Starting: "Starting",
            QtCore.QProcess.Running: "Running",
        }
        state_str = state_map.get(state, f"UnknownState({state})")
        logger.info(f"Worker process state changed: {state_str}")

        if self.is_not_running():
            self._cleanup_resources()
            exit_code = self.exitCode()
            exit_status = self.exitStatus()
            exit_status_str = (
                "NormalExit"
                if exit_status == QtCore.QProcess.NormalExit
                else "CrashExit"
            )
            msg = f"Worker process exited with code {exit_code} ({exit_status_str})"
            logger.info(msg)
            if exit_status == QtCore.QProcess.CrashExit:
                self.error_occurred_msg.emit(msg)

```

`src/ida_taskr/protocols.py`:

```py
"""Protocol definitions and base classes for the worker manager."""

from abc import ABC, abstractmethod

from .utils import EventEmitter


class WorkerProtocol(ABC):
    """Protocol for worker implementations."""

    @abstractmethod
    def setup(self, **kwargs):
        """Set up the worker."""
        pass

    @abstractmethod
    def process(self, connection, **kwargs):
        """Process tasks."""
        pass

    @abstractmethod
    def cleanup(self):
        """Clean up resources."""
        pass


class MessageEmitter(EventEmitter):
    """Event emitter for handling messages from workers in IDA.

    Events emitted:
    - 'worker_connected': When worker establishes connection
    - 'worker_message': When a message is received from worker (payload: message dict)
    - 'worker_results': When results are received from worker (payload: results dict)
    - 'worker_error': When an error occurs (payload: error string)
    - 'worker_disconnected': When worker connection is closed
    """

    def emit_worker_connected(self):
        """Emit worker connected event."""
        self.emit("worker_connected")

    def emit_worker_message(self, message: dict):
        """Emit worker message event."""
        self.emit("worker_message", message)

    def emit_worker_results(self, results: dict):
        """Emit worker results event."""
        self.emit("worker_results", results)

    def emit_worker_error(self, error: str):
        """Emit worker error event."""
        self.emit("worker_error", error)

    def emit_worker_disconnected(self):
        """Emit worker disconnected event."""
        self.emit("worker_disconnected")

```

`src/ida_taskr/qt_compat.py`:

```py
"""Qt compatibility layer for PyQt5 and PySide6.

This module provides a unified API for Qt signals and slots across different
Qt bindings used by IDA Pro and standalone environments.
"""

# Try to import Qt frameworks in order of preference
QT_API = None
QT_AVAILABLE = False
QtCore = None
Signal = None
Slot = None
QT_VERSION = None
QProcessEnvironment = None

# Try PySide6 first (IDA Pro 9.2+, modern official Qt bindings)
try:
    from PySide6 import QtCore
    from PySide6.QtCore import Signal, Slot

    QT_API = "PySide6"
    QT_VERSION = QtCore.__version__
    QProcessEnvironment = QtCore.QProcessEnvironment
    QT_AVAILABLE = True
except (ImportError, NotImplementedError):
    pass

# Try PyQt5 (IDA Pro 9.1 and earlier)
if QT_API is None:
    try:
        from PyQt5 import QtCore
        from PyQt5.QtCore import pyqtSignal as Signal, pyqtSlot as Slot

        QT_API = "PyQt5"
        QT_VERSION = QtCore.PYQT_VERSION_STR
        QProcessEnvironment = QtCore.QProcessEnvironment
        QT_AVAILABLE = True
    except (ImportError, NotImplementedError):
        pass

# If no Qt found, create mock classes for import compatibility
if QT_API is None:

    class QtCore:  # type: ignore
        """Mock QtCore module when Qt is not available."""

        class QThread:
            def __init__(self, *args, **kwargs):
                raise ImportError(
                    "Qt is not available. Cannot use WorkerLauncher without Qt."
                )

        class QObject:
            def __init__(self, *args, **kwargs):
                raise ImportError(
                    "Qt is not available. Cannot use WorkerLauncher without Qt."
                )

        class QProcess:
            # Process error enum values (mock)
            class ProcessError:
                FailedToStart = 0
                Crashed = 1
                Timedout = 2
                WriteError = 4
                ReadError = 3
                UnknownError = 5

            # Process state enum values (mock)
            class ProcessState:
                NotRunning = 0
                Starting = 1
                Running = 2

            # Direct enum access (compatibility)
            FailedToStart = 0
            Crashed = 1
            Timedout = 2
            WriteError = 4
            ReadError = 3
            UnknownError = 5
            NotRunning = 0
            Starting = 1
            Running = 2
            NormalExit = 0
            CrashExit = 1

            def __init__(self, *args, **kwargs):
                raise ImportError(
                    "Qt is not available. Cannot use WorkerLauncher without Qt."
                )

        class QSocketNotifier:
            Read = 1
            Write = 2

            def __init__(self, *args, **kwargs):
                raise ImportError(
                    "Qt is not available. Cannot use WorkerLauncher without Qt."
                )

    # Dummy signal for type hints
    Signal = lambda *args: None  # type: ignore
    Slot = lambda *args: None  # type: ignore

    class QProcessEnvironment:  # type: ignore
        @staticmethod
        def systemEnvironment():
            raise ImportError(
                "Qt is not available. Cannot use WorkerLauncher without Qt."
            )


def get_qt_api():
    """Return the name of the Qt API being used, or None if Qt is not available."""
    return QT_API


def get_qt_version():
    """Return the version of the Qt framework being used, or None if Qt is not available."""
    return QT_VERSION


# QtAsyncio module availability
QT_ASYNCIO_AVAILABLE = False
qtasyncio = None

if QT_AVAILABLE:
    try:
        from . import qtasyncio
        QT_ASYNCIO_AVAILABLE = True
    except ImportError:
        pass


__all__ = [
    "QtCore",
    "Signal",
    "Slot",
    "QT_API",
    "QT_VERSION",
    "QT_AVAILABLE",
    "QProcessEnvironment",
    "get_qt_api",
    "get_qt_version",
    "QT_ASYNCIO_AVAILABLE",
    "qtasyncio",
]

```

`src/ida_taskr/qtasyncio.py`:

```py
# QtAsyncio + ThreadExecutor + Worker utilities (single-file, typed, PySide6-compatible)
# Commit: qtproject/pyside-pyside-setup@072ffd057a29a694a0ad91894736bb4d0a88738e + extras
# Includes: asyncio integration, ThreadExecutor, Task/FutureWatcher, thread_worker utils

from __future__ import annotations

import asyncio
import atexit
import concurrent.futures
import inspect
import logging
import sys
import threading
import time
import warnings
import weakref
from contextlib import contextmanager
from functools import partial, wraps
from typing import (
    Any,
    Callable,
    Dict,
    Generic,
    Generator,
    List,
    Optional,
    ParamSpec,
    Sequence,
    Tuple,
    TypeVar,
    overload,
)

from .qt_compat import QtCore, Signal, Slot, QT_AVAILABLE

if not QT_AVAILABLE:
    raise ImportError(
        "QtAsyncio module requires Qt (PyQt5 or PySide6) to be available. "
        "Please install PyQt5 or PySide6."
    )

# Import Qt components we need
QCoreApplication = QtCore.QCoreApplication
QEvent = QtCore.QEvent
QEventLoop = QtCore.QEventLoop
QMetaObject = QtCore.QMetaObject
QObject = QtCore.QObject
QSemaphore = QtCore.QSemaphore
QThread = QtCore.QThread
QThreadPool = QtCore.QThreadPool
QTimer = QtCore.QTimer
Qt = QtCore.Qt

# Try to import Q_ARG and QRunnable with compatibility handling
try:
    from PySide6.QtCore import Q_ARG, QRunnable
except ImportError:
    try:
        from PyQt5.QtCore import Q_ARG, QRunnable
    except ImportError:
        # Fallback if neither is available
        Q_ARG = None
        QRunnable = object  # type: ignore

if sys.version_info >= (3, 11):
    from typing import Self
else:
    Self = Any

# Type variable for Future
Future = concurrent.futures.Future

# --------------------------------------------------------------------------- #
# Asyncio integration (original QtAsyncio module)
# --------------------------------------------------------------------------- #
class QAsyncioEventLoop(QEventLoop):
    def __init__(self, asyncio_loop: asyncio.AbstractEventLoop, parent: Optional[QObject] = None):
        super().__init__(parent)
        self._asyncio_loop = asyncio_loop

    def processEvents(self, flags=None):
        if flags is None:
            flags = QEventLoop.AllEvents
        self._asyncio_loop.run_until_complete(self._process_events(flags))

    async def _process_events(self, flags):
        while self._asyncio_loop._ready:
            await asyncio.sleep(0)
        super().processEvents(flags)
        await asyncio.sleep(0)


class QAsyncioEventLoopPolicy(
    asyncio.DefaultEventLoopPolicy
    if sys.platform != "win32"
    else asyncio.WindowsProactorEventLoopPolicy
):
    def new_event_loop(self):
        qt_app = QCoreApplication.instance() or QCoreApplication(sys.argv)
        asyncio_loop = super().new_event_loop()
        qt_loop = QAsyncioEventLoop(asyncio_loop, qt_app)

        def _wakeup_qt():
            qt_loop.wakeUp()

        asyncio_loop.call_soon(_wakeup_qt)
        timer = QTimer(qt_app)
        timer.timeout.connect(lambda: asyncio_loop.run_until_complete(asyncio.sleep(0)))
        timer.start(10)
        return asyncio_loop


def run(coroutine_or_future):
    loop = asyncio.get_event_loop()
    return loop.run_until_complete(coroutine_or_future)


def set_event_loop_policy():
    """Set the Qt-compatible asyncio event loop policy."""
    if not isinstance(asyncio.get_event_loop_policy(), QAsyncioEventLoopPolicy):
        asyncio.set_event_loop_policy(QAsyncioEventLoopPolicy())


# --------------------------------------------------------------------------- #
# ThreadExecutor + Task + FutureWatcher + utilities
# --------------------------------------------------------------------------- #
@contextmanager
def locked(mutex):
    mutex.lock()
    try:
        yield
    finally:
        mutex.unlock()


class _TaskDepotThread(QThread):
    _lock = threading.Lock()
    _instance: Optional["_TaskDepotThread"] = None

    def __new__(cls):
        if _TaskDepotThread._instance is not None:
            raise RuntimeError("Already exists")
        return super().__new__(cls)

    def __init__(self):
        super().__init__()
        self.start()
        self.moveToThread(self)
        atexit.register(self._cleanup)

    def _cleanup(self):
        self.quit()
        self.wait()

    @staticmethod
    def instance() -> "_TaskDepotThread":
        with _TaskDepotThread._lock:
            if _TaskDepotThread._instance is None:
                _TaskDepotThread._instance = _TaskDepotThread()
            return _TaskDepotThread._instance

    @Slot(object, object)
    def transfer(self, obj: QObject, thread: QThread):
        assert obj.thread() is self
        assert QThread.currentThread() is self
        obj.moveToThread(thread)


class _TaskRunnable(QRunnable):
    def __init__(self, future, task, args, kwargs):
        super().__init__()
        self.future = future
        self.task = task
        self.args = args
        self.kwargs = kwargs
        self.eventLoop: Optional[QEventLoop] = None

    def run(self):
        self.eventLoop = QEventLoop()
        self.eventLoop.processEvents()

        if Q_ARG is not None:
            QMetaObject.invokeMethod(
                self.task.thread(),
                "transfer",
                Qt.BlockingQueuedConnection,
                Q_ARG(object, self.task),
                Q_ARG(object, QThread.currentThread()),
            )

        self.eventLoop.processEvents()
        self.task.start()
        self.task.finished.connect(self.eventLoop.quit)
        self.task.cancelled.connect(self.eventLoop.quit)
        self.eventLoop.exec_()


class FutureRunnable(QRunnable):
    def __init__(self, future, func, args, kwargs):
        super().__init__()
        self.future = future
        self.task = (func, args, kwargs)

    def run(self):
        try:
            if not self.future.set_running_or_notify_cancel():
                return
            func, args, kwargs = self.task
            result = func(*args, **kwargs)
            self.future.set_result(result)
        except BaseException as ex:
            self.future.set_exception(ex)
        except BaseException:
            logging.getLogger(__name__).critical("Exception in worker thread.", exc_info=True)


class ThreadPoolExecutorSignals(QObject):
    """Qt signals for ThreadExecutor events."""
    task_submitted = Signal(object)  # future
    task_completed = Signal(object)  # future
    task_failed = Signal(object, object)  # future, exception
    pool_shutdown = Signal()


class ThreadExecutor(QObject, concurrent.futures.Executor):
    """
    A ThreadExecutor that provides concurrent.futures.ThreadPoolExecutor-like API
    using Qt's QThreadPool with optional Qt signal integration.

    Usage:
        executor = ThreadExecutor()

        # Standard concurrent.futures API
        future = executor.submit(task, arg1, arg2)
        result = future.result()

        # Or with Qt signals
        executor.signals.task_completed.connect(on_task_done)
        future = executor.submit(task, arg1, arg2)

        executor.shutdown(wait=True)
    """

    def __init__(
        self,
        parent: Optional[QObject] = None,
        threadPool: Optional[QThreadPool] = None,
        max_workers: Optional[int] = None,
    ):
        super().__init__(parent)
        self.signals = ThreadPoolExecutorSignals()
        self._threadPool = threadPool or QThreadPool.globalInstance()

        # Set max thread count if specified
        if max_workers is not None:
            self._threadPool.setMaxThreadCount(max_workers)

        self._max_workers = max_workers or self._threadPool.maxThreadCount()
        self._depot_thread: Optional[_TaskDepotThread] = None
        self._futures: List[Future] = []
        self._shutdown = False
        self._state_lock = threading.Lock()

    @property
    def max_workers(self) -> int:
        """Return the maximum number of worker threads."""
        return self._max_workers

    def _get_depot_thread(self) -> _TaskDepotThread:
        if self._depot_thread is None:
            self._depot_thread = _TaskDepotThread.instance()
        return self._depot_thread

    def submit(self, func, *args, **kwargs) -> Future:
        """
        Submit a callable to be executed in a worker thread.

        Args:
            func: A callable to execute
            *args: Positional arguments for the callable
            **kwargs: Keyword arguments for the callable

        Returns:
            A Future representing the pending execution

        Raises:
            RuntimeError: If the executor has been shut down
        """
        with self._state_lock:
            if self._shutdown:
                raise RuntimeError("Cannot schedule new futures after shutdown.")
            if isinstance(func, Task):
                warnings.warn("Use `submit_task` to run `Task`s", DeprecationWarning, stacklevel=2)
                f, runnable = self.__make_task_runnable(func)
            else:
                f = Future()
                runnable = FutureRunnable(f, func, args, kwargs)
            self._futures.append(f)
            f.add_done_callback(self._future_done)
            self._threadPool.start(runnable)

            # Emit task_submitted signal
            try:
                self.signals.task_submitted.emit(f)
            except RuntimeError:
                pass  # Qt object may have been deleted

            return f

    def __make_task_runnable(self, task: Task) -> Tuple[Future, _TaskRunnable]:
        if task.thread() is not QThread.currentThread():
            raise ValueError("Can only submit Tasks from its own thread.")
        if task.parent() is not None:
            raise ValueError("Cannot submit Tasks with a parent.")
        task.moveToThread(self._get_depot_thread())
        f = task.future()
        runnable = _TaskRunnable(f, task, (), {})
        return f, runnable

    def map(
        self,
        fn: Callable,
        *iterables,
        timeout: Optional[float] = None,
        chunksize: int = 1,
    ):
        """
        Map a function over iterables, executing in parallel threads.

        Args:
            fn: A callable
            *iterables: Iterables of arguments
            timeout: Maximum time to wait for results
            chunksize: Ignored (for API compatibility with ProcessPoolExecutor)

        Returns:
            Iterator of results in the same order as the input iterables
        """
        with self._state_lock:
            if self._shutdown:
                raise RuntimeError("Cannot schedule new futures after shutdown.")

        # Submit all tasks
        futures = [self.submit(fn, *args) for args in zip(*iterables)]

        # Yield results in order
        for future in futures:
            yield future.result(timeout=timeout)

    def shutdown(self, wait: bool = True, *, cancel_futures: bool = False):
        """
        Shutdown the executor.

        Args:
            wait: If True, wait for all pending futures to complete
            cancel_futures: If True, cancel all pending futures (best effort)
        """
        with self._state_lock:
            self._shutdown = True
            futures = list(self._futures)

        if cancel_futures:
            for f in futures:
                f.cancel()

        if wait:
            concurrent.futures.wait(futures)

        try:
            self.signals.pool_shutdown.emit()
        except RuntimeError:
            pass  # Qt object may have been deleted

    def _future_done(self, future: Future):
        """Called when a future completes - emits appropriate Qt signals."""
        with self._state_lock:
            if future in self._futures:
                self._futures.remove(future)

        try:
            exc = future.exception()
            if exc is not None:
                self.signals.task_failed.emit(future, exc)
            else:
                self.signals.task_completed.emit(future)
        except (concurrent.futures.CancelledError, RuntimeError):
            pass  # Future was cancelled or Qt object deleted

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.shutdown(wait=True)
        return False


class Task(QObject):
    started = Signal()
    finished = Signal()
    cancelled = Signal()
    resultReady = Signal(object)
    exceptionReady = Signal(Exception)

    __ExecuteCall = QEvent.registerEventType()

    def __init__(self, parent: Optional[QObject] = None, function: Optional[Callable] = None):
        super().__init__(parent)
        warnings.warn("`Task` has been deprecated", PendingDeprecationWarning, stacklevel=2)
        self.function = function
        self._future: Future = Future()

    def run(self):
        if self.function is None:
            raise NotImplementedError
        return self.function()

    def start(self):
        QCoreApplication.postEvent(self, QEvent(Task.__ExecuteCall))

    def future(self) -> Future:
        return self._future

    def result(self, timeout: Optional[float] = None):
        return self._future.result(timeout)

    def _execute(self):
        try:
            if not self._future.set_running_or_notify_cancel():
                self.cancelled.emit()
                return
            self.started.emit()
            result = self.run()
            self._future.set_result(result)
            self.resultReady.emit(result)
        except BaseException as ex:
            self._future.set_exception(ex)
            self.exceptionReady.emit(ex)
        finally:
            self.finished.emit()

    def customEvent(self, event: QEvent):
        if event.type() == Task.__ExecuteCall:
            self._execute()
        else:
            super().customEvent(event)


class FutureWatcher(QObject):
    done = Signal(Future)
    finished = Signal(Future)
    cancelled = Signal(Future)
    resultReady = Signal(object)
    exceptionReady = Signal(BaseException)

    __FutureDone = QEvent.registerEventType()

    def __init__(self, future: Optional[Future] = None, parent: Optional[QObject] = None):
        super().__init__(parent)
        self.__future: Optional[Future] = None
        if future is not None:
            self.setFuture(future)

    def setFuture(self, future: Future):
        if self.__future is not None:
            raise RuntimeError("Future already set")
        self.__future = future

        def on_done(f):
            if (selfref := weakref.ref(self)) is None:
                return
            try:
                QCoreApplication.postEvent(selfref(), QEvent(FutureWatcher.__FutureDone))
            except RuntimeError:
                pass

        future.add_done_callback(on_done)

    def future(self) -> Future:
        return self.__future

    def __emitSignals(self):
        f = self.__future
        if f.cancelled():
            self.cancelled.emit(f)
            self.done.emit(f)
        else:
            self.finished.emit(f)
            self.done.emit(f)
            if exc := f.exception():
                self.exceptionReady.emit(exc)
            else:
                self.resultReady.emit(f.result())

    def customEvent(self, event: QEvent):
        if event.type() == FutureWatcher.__FutureDone:
            self.__emitSignals()
        super().customEvent(event)


# --------------------------------------------------------------------------- #
# Modern thread_worker utilities (QRunnable-based)
# --------------------------------------------------------------------------- #
_T = TypeVar("_T")
_R = TypeVar("_R")
_Y = TypeVar("_Y")
_S = TypeVar("_S")
_P = ParamSpec("_P")


class WorkerBaseSignals(QObject):
    started = Signal()
    finished = Signal()
    _finished = Signal(object)
    returned = Signal(object)
    errored = Signal(object)
    warned = Signal(tuple)


class WorkerBase(QRunnable, Generic[_R]):
    _worker_set: set[Self] = set()

    def __init__(self, SignalsClass=WorkerBaseSignals):
        super().__init__()
        self._abort_requested = False
        self._running = False
        self.signals = SignalsClass()

    def __getattr__(self, name: str):
        attr = getattr(self.signals.__class__, name, None)
        if isinstance(attr, Signal):
            return getattr(self.signals, name)
        raise AttributeError(f"{self.__class__.__name__!r} has no attribute {name!r}")

    def quit(self):
        self._abort_requested = True

    @property
    def abort_requested(self) -> bool:
        return self._abort_requested

    @property
    def is_running(self) -> bool:
        return self._running

    def run(self):
        self.started.emit()
        self._running = True
        try:
            with warnings.catch_warnings():
                warnings.filterwarnings("always")
                warnings.showwarning = lambda *w: self.warned.emit(w)
                result = self.work()
            if isinstance(result, Exception):
                raise result
            if not self.abort_requested:
                self.returned.emit(result)
        except Exception as e:
            self.errored.emit(e)
        finally:
            self._running = False
            self.finished.emit()
            self._finished.emit(self)

    def work(self) -> _R | Exception:
        raise NotImplementedError

    def start(self):
        if self in self._worker_set:
            raise RuntimeError("Worker already started")
        self._worker_set.add(self)
        self._finished.connect(lambda w: self._worker_set.discard(w))
        QThreadPool.globalInstance().start(self)


class FunctionWorker(WorkerBase[_R]):
    def __init__(self, func: Callable[_P, _R], *args, **kwargs):
        if inspect.isgeneratorfunction(func):
            raise TypeError("Use GeneratorWorker for generator functions")
        super().__init__()
        self._func = func
        self._args = args
        self._kwargs = kwargs

    def work(self) -> _R:
        return self._func(*self._args, **self._kwargs)


class GeneratorWorkerSignals(WorkerBaseSignals):
    yielded = Signal(object)
    paused = Signal()
    resumed = Signal()
    aborted = Signal()


class GeneratorWorker(WorkerBase, Generic[_Y, _S, _R]):
    yielded: Signal
    paused: Signal
    resumed: Signal
    aborted: Signal

    def __init__(self, func: Callable[..., Generator[_Y, Optional[_S], _R]], *args, **kwargs):
        if not inspect.isgeneratorfunction(func):
            raise TypeError("Use FunctionWorker for regular functions")
        super().__init__(SignalsClass=GeneratorWorkerSignals)
        self._gen = func(*args, **kwargs)
        self._incoming: Optional[_S] = None
        self._pause_requested = self._resume_requested = self._paused = False
        self._pause_interval = 0.01

    def work(self) -> _R | None:
        while True:
            if self.abort_requested:
                self.aborted.emit()
                break
            if self._paused:
                if self._resume_requested:
                    self._paused = self._resume_requested = False
                    self.resumed.emit()
                else:
                    time.sleep(self._pause_interval)
                    continue
            elif self._pause_requested:
                self._paused = True
                self._pause_requested = False
                self.paused.emit()
                continue
            try:
                output = self._gen.send(self._incoming)
                self.yielded.emit(output)
                self._incoming = None
            except StopIteration as e:
                return e.value
        return None

    def send(self, value: _S):
        self._incoming = value

    def toggle_pause(self):
        if self._paused:
            self._resume_requested = True
        else:
            self._pause_requested = True


@overload
def create_worker(
    func: Callable[_P, Generator[_Y, _S, _R]], *args, **kwargs
) -> GeneratorWorker[_Y, _S, _R]: ...
@overload
def create_worker(func: Callable[_P, _R], *args, **kwargs) -> FunctionWorker[_R]: ...

def create_worker(func, *args, **kwargs):
    WorkerCls = GeneratorWorker if inspect.isgeneratorfunction(func) else FunctionWorker
    return WorkerCls(func, *args, **kwargs)


def thread_worker(func=None, **dec_kwargs):
    def decorator(f):
        @wraps(f)
        def wrapper(*a, **kw):
            return create_worker(f, *a, **kw, **dec_kwargs)
        return wrapper
    return decorator if func is None else decorator(func)


# --------------------------------------------------------------------------- #
# QThread-based worker helper (alternative to QRunnable)
# --------------------------------------------------------------------------- #
def new_worker_qthread(
    Worker: type[QObject],
    *args,
    _start_thread: bool = False,
    _connect: Optional[Dict[str, Callable]] = None,
    **kwargs,
):
    thread = QThread()
    worker = Worker(*args, **kwargs)
    worker.moveToThread(thread)
    thread.started.connect(worker.work)
    worker.finished.connect(thread.quit)
    worker.finished.connect(worker.deleteLater)
    thread.finished.connect(thread.deleteLater)
    if _connect:
        for sig, slot in _connect.items():
            getattr(worker, sig).connect(slot)
    if _start_thread:
        thread.start()
    return worker, thread

# --------------------------------------------------------------------------- #
# ProcessPoolExecutor - multiprocessing-based executor with Qt signal support
# --------------------------------------------------------------------------- #
import multiprocessing
import multiprocessing.pool
import queue

# Use spawn context by default to avoid fork+Qt deadlocks on POSIX systems.
# fork() can cause deadlocks when Qt threads are active (Qt's thread pool holds
# locks that get copied to child process but the threads that would unlock them
# don't exist in the child). spawn is already default on Windows and macOS.
# See: https://docs.python.org/3/library/multiprocessing.html#contexts-and-start-methods
_default_mp_context = multiprocessing.get_context('spawn')


class ProcessPoolExecutorSignals(QObject):
    """Qt signals for ProcessPoolExecutor events."""
    task_submitted = Signal(object)  # future
    task_completed = Signal(object)  # future
    task_failed = Signal(object, object)  # future, exception
    pool_shutdown = Signal()


class ProcessPoolExecutor(QObject, concurrent.futures.Executor):
    """
    A ProcessPoolExecutor that provides concurrent.futures.ProcessPoolExecutor API
    with optional Qt signal integration for task completion notifications.

    Unlike ThreadExecutor which uses QThreadPool, this uses Python's multiprocessing
    for true parallel execution of CPU-bound tasks across multiple cores.

    Usage:
        executor = ProcessPoolExecutor(max_workers=4)

        # Standard concurrent.futures API
        future = executor.submit(cpu_bound_task, arg1, arg2)
        result = future.result()

        # Or with Qt signals
        executor.signals.task_completed.connect(on_task_done)
        future = executor.submit(cpu_bound_task, arg1, arg2)

        executor.shutdown(wait=True)

    Note: Functions submitted must be picklable (module-level functions, not lambdas
    or closures that capture unpicklable objects).
    """

    def __init__(
        self,
        max_workers: Optional[int] = None,
        mp_context: Optional[multiprocessing.context.BaseContext] = None,
        initializer: Optional[Callable[..., None]] = None,
        initargs: Tuple = (),
        parent: Optional[QObject] = None,
    ):
        super().__init__(parent)
        self.signals = ProcessPoolExecutorSignals()
        self._max_workers = max_workers or multiprocessing.cpu_count()
        # Use spawn context by default to avoid fork+Qt deadlocks
        self._mp_context = mp_context if mp_context is not None else _default_mp_context
        self._initializer = initializer
        self._initargs = initargs

        # Create the underlying ProcessPoolExecutor
        self._executor = concurrent.futures.ProcessPoolExecutor(
            max_workers=self._max_workers,
            mp_context=self._mp_context,
            initializer=self._initializer,
            initargs=self._initargs,
        )

        self._futures: List[concurrent.futures.Future] = []
        self._shutdown = False
        self._state_lock = threading.Lock()

    @property
    def max_workers(self) -> int:
        """Return the maximum number of worker processes."""
        return self._max_workers

    def submit(self, fn: Callable, *args, **kwargs) -> concurrent.futures.Future:
        """
        Submit a callable to be executed in a worker process.

        Args:
            fn: A picklable callable to execute
            *args: Positional arguments for the callable
            **kwargs: Keyword arguments for the callable

        Returns:
            A Future representing the pending execution

        Raises:
            RuntimeError: If the executor has been shut down
        """
        with self._state_lock:
            if self._shutdown:
                raise RuntimeError("Cannot schedule new futures after shutdown.")

            future = self._executor.submit(fn, *args, **kwargs)
            self._futures.append(future)

            # Add callback for Qt signal emission
            future.add_done_callback(self._on_future_done)

            # Emit task_submitted signal
            try:
                self.signals.task_submitted.emit(future)
            except RuntimeError:
                pass  # Qt object may have been deleted

            return future

    def _on_future_done(self, future: concurrent.futures.Future):
        """Called when a future completes - emits appropriate Qt signals."""
        with self._state_lock:
            if future in self._futures:
                self._futures.remove(future)

        try:
            exc = future.exception()
            if exc is not None:
                self.signals.task_failed.emit(future, exc)
            else:
                self.signals.task_completed.emit(future)
        except (concurrent.futures.CancelledError, RuntimeError):
            pass  # Future was cancelled or Qt object deleted

    def map(
        self,
        fn: Callable,
        *iterables,
        timeout: Optional[float] = None,
        chunksize: int = 1,
    ):
        """
        Map a function over iterables, executing in parallel.

        Args:
            fn: A picklable callable
            *iterables: Iterables of arguments
            timeout: Maximum time to wait for results
            chunksize: Size of chunks for efficiency (larger = fewer IPC calls)

        Returns:
            Iterator of results in the same order as the input iterables

        Raises:
            RuntimeError: If the executor has been shut down
        """
        with self._state_lock:
            if self._shutdown:
                raise RuntimeError("Cannot schedule new futures after shutdown.")

        return self._executor.map(fn, *iterables, timeout=timeout, chunksize=chunksize)

    def shutdown(self, wait: bool = True, *, cancel_futures: bool = False):
        """
        Shutdown the executor.

        Args:
            wait: If True, wait for all pending futures to complete
            cancel_futures: If True, cancel all pending futures
        """
        with self._state_lock:
            self._shutdown = True

        self._executor.shutdown(wait=wait, cancel_futures=cancel_futures)

        try:
            self.signals.pool_shutdown.emit()
        except RuntimeError:
            pass  # Qt object may have been deleted

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.shutdown(wait=True)
        return False


# Alias for consistency with naming convention
QProcessPoolExecutor = ProcessPoolExecutor
QThreadPoolExecutor = ThreadExecutor  # Alias


# --------------------------------------------------------------------------- #
# InterpreterPoolExecutor - process-based executor with Qt signal support
# Mimics Python 3.13+ InterpreterPoolExecutor API using ProcessPoolExecutor
# for embedded contexts (like IDA Pro) where subinterpreters won't work
# --------------------------------------------------------------------------- #

# Always available - uses ProcessPoolExecutor as backend for isolation
INTERPRETER_POOL_AVAILABLE = True


class InterpreterPoolExecutorSignals(QObject):
    """Qt signals for InterpreterPoolExecutor events."""
    task_submitted = Signal(object)  # future
    task_completed = Signal(object)  # future
    task_failed = Signal(object, object)  # future, exception
    pool_shutdown = Signal()


class InterpreterPoolExecutor(QObject, concurrent.futures.Executor):
    """
    An executor that provides concurrent.futures.InterpreterPoolExecutor API
    with optional Qt signal integration for task completion notifications.

    This implementation uses ProcessPoolExecutor as the backend to provide
    true parallelism and isolation, making it compatible with embedded Python
    contexts (like IDA Pro) where Python 3.13+ subinterpreters may not work.

    Usage:
        executor = InterpreterPoolExecutor(max_workers=4)

        # Standard concurrent.futures API
        future = executor.submit(cpu_bound_task, arg1, arg2)
        result = future.result()

        # Or with Qt signals
        executor.signals.task_completed.connect(on_task_done)
        future = executor.submit(cpu_bound_task, arg1, arg2)

        executor.shutdown(wait=True)

    Note: Functions and arguments must be picklable (same restrictions as
    ProcessPoolExecutor/multiprocessing).
    """

    def __init__(
        self,
        max_workers: Optional[int] = None,
        parent: Optional[QObject] = None,
    ):
        super().__init__(parent)
        self.signals = InterpreterPoolExecutorSignals()
        self._max_workers = max_workers or multiprocessing.cpu_count()

        # Use ProcessPoolExecutor as backend for isolation
        # This provides similar benefits to subinterpreters (no GIL contention)
        # while being compatible with embedded Python contexts
        # Use spawn context to avoid fork+Qt deadlocks
        self._executor = concurrent.futures.ProcessPoolExecutor(
            max_workers=self._max_workers,
            mp_context=_default_mp_context,
        )

        self._futures: List[concurrent.futures.Future] = []
        self._shutdown = False
        self._state_lock = threading.Lock()

    @property
    def max_workers(self) -> int:
        """Return the maximum number of worker processes."""
        return self._max_workers

    def submit(self, fn: Callable, *args, **kwargs) -> concurrent.futures.Future:
        """
        Submit a callable to be executed in a worker process.

        Args:
            fn: A callable to execute (must be picklable)
            *args: Positional arguments for the callable
            **kwargs: Keyword arguments for the callable

        Returns:
            A Future representing the pending execution

        Raises:
            RuntimeError: If the executor has been shut down
        """
        with self._state_lock:
            if self._shutdown:
                raise RuntimeError("Cannot schedule new futures after shutdown.")

            future = self._executor.submit(fn, *args, **kwargs)
            self._futures.append(future)

            # Add callback for Qt signal emission
            future.add_done_callback(self._on_future_done)

            # Emit task_submitted signal
            try:
                self.signals.task_submitted.emit(future)
            except RuntimeError:
                pass  # Qt object may have been deleted

            return future

    def _on_future_done(self, future: concurrent.futures.Future):
        """Called when a future completes - emits appropriate Qt signals."""
        with self._state_lock:
            if future in self._futures:
                self._futures.remove(future)

        try:
            exc = future.exception()
            if exc is not None:
                self.signals.task_failed.emit(future, exc)
            else:
                self.signals.task_completed.emit(future)
        except (concurrent.futures.CancelledError, RuntimeError):
            pass  # Future was cancelled or Qt object deleted

    def map(
        self,
        fn: Callable,
        *iterables,
        timeout: Optional[float] = None,
        chunksize: int = 1,
    ):
        """
        Map a function over iterables, executing in parallel processes.

        Args:
            fn: A callable (must be picklable)
            *iterables: Iterables of arguments
            timeout: Maximum time to wait for results
            chunksize: Size of chunks for efficiency

        Returns:
            Iterator of results in the same order as the input iterables

        Raises:
            RuntimeError: If the executor has been shut down
        """
        with self._state_lock:
            if self._shutdown:
                raise RuntimeError("Cannot schedule new futures after shutdown.")

        return self._executor.map(fn, *iterables, timeout=timeout, chunksize=chunksize)

    def shutdown(self, wait: bool = True, *, cancel_futures: bool = False):
        """
        Shutdown the executor.

        Args:
            wait: If True, wait for all pending futures to complete
            cancel_futures: If True, cancel all pending futures
        """
        with self._state_lock:
            self._shutdown = True

        self._executor.shutdown(wait=wait, cancel_futures=cancel_futures)

        try:
            self.signals.pool_shutdown.emit()
        except RuntimeError:
            pass  # Qt object may have been deleted

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.shutdown(wait=True)
        return False


# Alias for naming consistency
QInterpreterPoolExecutor = InterpreterPoolExecutor


# Single-file ready-to-use module.
# No external dependencies beyond PyQt5/PySide6.

__all__ = [
    # Asyncio integration
    "QAsyncioEventLoop",
    "QAsyncioEventLoopPolicy",
    "run",
    "set_event_loop_policy",
    # Thread executor
    "ThreadExecutor",
    "QThreadPoolExecutor",  # Alias
    "ThreadPoolExecutorSignals",
    "Task",
    "FutureWatcher",
    "Future",
    # Process executor
    "ProcessPoolExecutor",
    "QProcessPoolExecutor",  # Alias
    "ProcessPoolExecutorSignals",
    # Interpreter executor (Python 3.13+)
    "InterpreterPoolExecutor",
    "QInterpreterPoolExecutor",  # Alias
    "InterpreterPoolExecutorSignals",
    "INTERPRETER_POOL_AVAILABLE",
    # Worker utilities
    "WorkerBase",
    "WorkerBaseSignals",
    "FunctionWorker",
    "GeneratorWorker",
    "GeneratorWorkerSignals",
    "create_worker",
    "thread_worker",
    "new_worker_qthread",
]

```

`src/ida_taskr/task_runner.py`:

```py
"""
TaskRunner - A simplified interface for running worker tasks.

Provides a high-level API that wraps WorkerLauncher and MessageEmitter
for easier task execution with callback-based result handling.
"""

import logging

from .helpers import get_logger
from .launcher import WorkerLauncher
from .protocols import MessageEmitter


class TaskRunner:
    """Simplified task runner with callback-based event handling."""

    def __init__(self, worker_script, worker_args, log_level=None, logger=None):
        """Initialize TaskRunner.

        Args:
            worker_script: Path to the worker script to execute
            worker_args: Dictionary of arguments to pass to the worker
            log_level: Logging level (optional)
            logger: Custom logger instance (optional)
        """
        if logger:
            self.logger = logger
        else:
            # Use INFO as default if no log_level specified
            actual_log_level = log_level if log_level is not None else logging.INFO
            self.logger = get_logger(log_level=actual_log_level)
        self.message_emitter = MessageEmitter()
        self.launcher = WorkerLauncher(self.message_emitter)
        self.worker_script = worker_script
        self.worker_args = worker_args
        self._results_callback = None
        self._progress_callback = None

    def on_results(self, callback):
        """Register a callback for when worker results are received.

        Args:
            callback: Function to call with results dict
        """
        self._results_callback = callback
        self.message_emitter.on("worker_results", self._handle_results)

    def on_progress(self, callback):
        """Register a callback for progress updates.

        Args:
            callback: Function to call with (progress, status) tuple
        """
        self._progress_callback = callback
        self.message_emitter.on("worker_message", self._handle_progress)

    def start(self):
        """Start the worker task."""
        if self.launcher.launch_worker(self.worker_script, self.worker_args):
            self.logger.info("Worker launched successfully")
        else:
            self.logger.error("Failed to launch worker")

    def _handle_results(self, results):
        """Internal handler for worker results."""
        if self._results_callback:
            self._results_callback(results)

    def _handle_progress(self, message):
        """Internal handler for progress messages."""
        if message.get("type") == "progress" and self._progress_callback:
            progress = message.get("progress", 0)
            status = message.get("status", "unknown")
            self._progress_callback(progress, status)

```

`src/ida_taskr/taskr_plugin.py`:

```py
"""Generic IDA Pro plugin for taskr framework."""

import idaapi

from . import DataProcessorCore, get_logger

logger = get_logger(__name__)


class DataProcessorPlugin(idaapi.plugin_t):
    """Generic IDA Pro plugin for data processing tasks."""

    flags = idaapi.PLUGIN_PROC
    comment = "Generic Data Processor via Shared Memory and Multiprocessing"
    help = "Press Alt-Shift-P to start processing"
    wanted_name = "TaskrProcessor"
    wanted_hotkey = "Alt-Shift-P"
    _core: DataProcessorCore | None = None

    def init(self):
        # This will be overridden by specific implementations
        logger.info("Generic TaskrProcessor plugin initialized")
        return idaapi.PLUGIN_KEEP

    def term(self):
        logger.info("Terminating plugin.")
        if self._core:
            self._core.terminate()
        logger.info("Plugin terminated.")

    def run(self, arg):
        # This will be overridden by specific implementations
        logger.warning("Generic plugin run() called - should be overridden")


def PLUGIN_ENTRY():
    return DataProcessorPlugin()


# XXX: This is a temporary hack to allow the plugin to be loaded
"""
from .ida_plugin import PLUGIN_ENTRY, DataProcessorPlugin

__all__ = ["DataProcessorPlugin", "PLUGIN_ENTRY"]
from ida_worker_manager import ConnectionContext, WorkerBase


class MyWorker(WorkerBase):
    def process(self, connection: ConnectionContext, **kwargs):
        # Your processing logic here
        connection.send_message("status", "processing")
        # Do work...
        connection.send_message("result", {"data": "results"})


# In your IDA plugin:
from ida_worker_manager import MessageHandler, WorkerLauncher


class MyHandler(MessageHandler):
    def on_worker_results(self, results):
        # Handle results
        pass


launcher = WorkerLauncher(MyHandler())
launcher.launch_worker("my_worker.py", {"arg1": "value1"})
"""

```

`src/ida_taskr/utils.py`:

```py
"""Utility functions for the anti-deobfuscation plugin."""

import asyncio
import atexit
import collections
import contextlib
import dataclasses
import enum
import functools
import logging
import multiprocessing
import multiprocessing.shared_memory
import pathlib
import time
import typing
from abc import ABC, abstractmethod
from bisect import bisect_left, bisect_right
from typing import Any, Callable, Generic, TypeVar, overload

from .helpers import get_logger

logger = get_logger(__name__)


def humanize_bytes(
    num_bytes: int, precision: int = 2, units: list[str] = ["B", "KB", "MB", "GB"]
) -> str:
    """
    Convert a byte count into a human-friendly string with units.
    """
    if num_bytes < 0:
        raise ValueError("num_bytes must be non-negative")
    if num_bytes == 0:
        return "0 B"
    idx = 0
    value = float(num_bytes)
    while value >= 1024 and idx < len(units) - 1:
        value /= 1024
        idx += 1
    if value.is_integer():
        return f"{int(value)} {units[idx]}"
    else:
        return f"{value:.{precision}f} {units[idx]}"


class DataProcessorCore:
    """Core processor for managing deobfuscation tasks."""

    _shared_memory = None

    def __init__(self, message_emitter):
        """Initialize the DataProcessorCore.

        Args:
            message_emitter: MessageEmitter instance to handle worker communication
        """
        from .protocols import MessageEmitter

        if not isinstance(message_emitter, MessageEmitter):
            raise TypeError("message_emitter must be a MessageEmitter instance")

        self.message_emitter = message_emitter
        self.proc = None
        atexit.register(self.terminate)

    @staticmethod
    def get_section_data(
        section_name: str,
        max_size: int = 120 * 1024 * 1024,
        min_size: int = 1024,
    ) -> tuple[int, bytes]:
        """Get the data of a section by name."""
        try:
            import ida_bytes
            import ida_segment
            import idaapi
        except ImportError:
            logger.error("IDA Pro modules not available")
            return 0, b""  # Use 0 instead of idaapi.BADADDR when not available

        seg = ida_segment.get_segm_by_name(section_name)
        if not seg:
            logger.error("Section %s not found", section_name)
            return idaapi.BADADDR, b""

        data_ea = seg.start_ea
        data_size = seg.end_ea - seg.start_ea

        if data_size > max_size:
            data_size = max_size
            logger.warning(
                "Limiting section data size to %s", humanize_bytes(data_size)
            )
        elif data_size < min_size:
            logger.error(
                "%s section is too small (%s)", section_name, humanize_bytes(data_size)
            )
            return idaapi.BADADDR, b""

        logger.info(
            "Reading %s from address %s", humanize_bytes(data_size), hex(data_ea)
        )

        data_bytes = ida_bytes.get_bytes(data_ea, data_size)
        if not data_bytes or len(data_bytes) != data_size:
            logger.error("Failed to read section data")
            return idaapi.BADADDR, b""

        return data_ea, data_bytes

    @staticmethod
    def from_range(start_ea: int, end_ea: int):
        """Get the data of a section by name and return the start address and the bytes."""
        try:
            import ida_bytes
        except ImportError:
            logger.error("IDA Pro modules not available")
            return 0, b""

        data_bytes = ida_bytes.get_bytes(start_ea, end_ea - start_ea)
        return start_ea, data_bytes

    def run(
        self, start_ea: int, bytes_to_process: bytes, worker_script_path: str, **kwargs
    ):
        """Run the deobfuscation process.

        Args:
            start_ea: Starting address for processing
            bytes_to_process: Binary data to process
            worker_script_path: Path to the worker script
            **kwargs: Additional arguments passed to worker
        """
        from .launcher import WorkerLauncher

        data_size = len(bytes_to_process)

        # Create shared memory
        self._shared_memory = multiprocessing.shared_memory.SharedMemory(
            create=True, size=data_size
        )
        self._shared_memory.buf[:data_size] = bytes_to_process

        # Launch worker with provided message emitter
        self.proc = WorkerLauncher(self.message_emitter)

        worker_args = {
            "shm_name": self._shared_memory.name,
            "data_size": data_size,
            "start_ea": hex(start_ea),
            **kwargs,
        }

        # Add IDA bitness if available
        try:
            import ida_ida

            worker_args["is64"] = "1" if ida_ida.inf_is_64bit() else "0"
        except ImportError:
            logger.warning("IDA Pro modules not available, bitness detection skipped")

        if not self.proc.launch_worker(str(worker_script_path), worker_args):
            self.terminate()
            logger.error("Failed to start worker process")
            return

    def terminate(self):
        """Terminate and clean up."""
        logger.info("Terminating...")
        if self.proc and not self.proc.is_not_running():
            self.proc.stop_worker()
            self.proc = None
        self._cleanup_shared_memory()
        logger.info("Terminated.")

    def _cleanup_shared_memory(self):
        """Clean up shared memory."""
        if not self._shared_memory:
            return
        try:
            self._shared_memory.close()
            shm = multiprocessing.shared_memory.SharedMemory(self._shared_memory.name)
            shm.unlink()
            logger.info("Shared memory unlinked: %s", self._shared_memory.name)
        except FileNotFoundError:
            logger.warning(
                "Shared memory already unlinked: %s", self._shared_memory.name
            )
        except PermissionError as e:
            logger.error("Permission error unlinking shared memory: %s", e)
        except Exception as e:
            logger.error(
                "Unexpected error unlinking shared memory: %s", e, exc_info=True
            )
        finally:
            self._shared_memory = None


class emit:
    def __init__(self, event):
        self.event = event

    def __call__(self, fn):
        @functools.wraps(fn)
        def wrapper(inst, *args, **kwargs):
            result = fn(inst, *args, **kwargs)
            inst.emit(self.event)
            return result

        return wrapper


T = TypeVar("T")


class reify(Generic[T]):
    """
    Acts similar to a property, except the result will be
    set as an attribute on the instance instead of recomputed
    each access.
    """

    def __init__(self, fn: Callable[..., T]) -> None:
        self.fn = fn
        # Copy function attributes to preserve metadata
        self.__name__ = getattr(fn, "__name__", "<unknown>")
        self.__doc__ = getattr(fn, "__doc__", None)
        self.__module__ = getattr(fn, "__module__", "") or ""
        self.__qualname__ = getattr(fn, "__qualname__", "") or ""
        self.__annotations__ = getattr(fn, "__annotations__", {})

    @overload
    def __get__(self, instance: None, owner: type) -> "reify[T]": ...

    @overload
    def __get__(self, instance: Any, owner: type) -> T: ...

    def __get__(self, instance: Any, owner: type) -> "T | reify[T]":
        if instance is None:
            return self

        fn = self.fn
        val = fn(instance)
        setattr(instance, fn.__name__, val)
        return val


class EventEmitter:
    @reify
    def _listeners(self):
        return collections.defaultdict(set)

    def on(self, event, handler=None):
        """Register an event handler for the given event."""
        if handler:
            self._listeners[event].add(handler)
            return handler

        @functools.wraps(self.on)
        def decorator(func):
            self.on(event, func)
            return func

        return decorator

    def once(self, event, handler):
        @functools.wraps(handler)
        def once_handler(*args, **kwargs):
            self.remove(event, once_handler)
            return handler(*args, **kwargs)

        self.on(event, once_handler)

    def remove(self, event, handler):
        self._listeners[event].discard(handler)

    def emit(self, event, *args, **kwargs):
        for handler in list(self._listeners[event]):
            handler(*args, **kwargs)


@dataclasses.dataclass
class AsyncEventEmitter(ABC):
    def __post_init__(self):
        self._listeners = collections.defaultdict(set)
        self.pause_evt = asyncio.Event()
        self.stop_evt = asyncio.Event()
        self.logger = get_logger(self.__class__.__name__)

    def on(self, event, handler=None):
        """Register an event handler for the given event."""
        if handler:
            self._listeners[event].add(handler)
            return handler

        @functools.wraps(self.on)
        def decorator(func):
            self.on(event, func)
            return func

        return decorator

    def once(self, event, handler):
        @functools.wraps(handler)
        def once_handler(*args, **kwargs):
            self.remove(event, once_handler)
            return handler(*args, **kwargs)

        self.on(event, once_handler)

    def remove(self, event, handler):
        self._listeners[event].discard(handler)

    async def emit(self, event, *args):
        for handler in list(self._listeners[event]):
            await handler(*args)

    @abstractmethod
    async def run(self):
        """Core asynchronous task execution logic."""
        pass

    @abstractmethod
    async def shutdown(self):
        """Cleanup resources for the async task."""
        pass


def log_execution_time(func, loglvl=logging.INFO):
    """
    Decorator to log the execution time of async stage methods.

    >>> import asyncio, logging
    >>> logging.basicConfig(level=logging.INFO)
    >>> class Dummy:
    ...     @log_execution_time
    ...     async def foo(self):
    ...         await asyncio.sleep(0.01)
    ...         return 42
    >>> d = Dummy()
    >>> asyncio.run(d.foo())
    42
    """

    @functools.wraps(func)
    async def wrapper(*args, **kwargs):
        start = time.perf_counter()
        result = await func(*args, **kwargs)
        elapsed = time.perf_counter() - start
        logger.log(loglvl, f"{func.__qualname__} executed in {elapsed:.4f} seconds")
        return result

    return wrapper


@dataclasses.dataclass
class Range:
    """A range of addresses with a start (inclusive) and end (exclusive)."""

    start: int
    end: int

    def __post_init__(self):
        if self.start >= self.end:
            raise ValueError("start must be less than end")

    def __contains__(self, addr: int) -> bool:
        """Check if an address is within this range."""
        return self.start <= addr < self.end

    def __len__(self) -> int:
        """Return the size of the range in bytes."""
        return self.end - self.start

    def overlaps(self, other: "Range") -> bool:
        """Check if this range overlaps with another range."""
        return self.start < other.end and other.start < self.end

    def merge(self, other: "Range") -> "Range":
        return Range(min(self.start, other.start), max(self.end, other.end))


class IntervalSet:
    """
    Sorted, non-overlapping list of Range objects with O(log n) insertion.
    """

    __slots__ = ("_ranges",)

    def __init__(self) -> None:
        self._ranges: list[Range] = []

    def __iter__(self):
        return iter(self._ranges)

    def __len__(self):
        return len(self._ranges)

    # --- public ------------------------------------------------------------
    def add(self, new: Range) -> None:
        """
        Insert `new` and coalesce any overlaps / adjacencies in-place.
        """
        # Fast-path: first interval
        if not self._ranges:
            self._ranges.append(new)
            return

        # Binary-search insertion point by *start*
        idx = bisect_left(
            self._ranges, new.start, key=lambda r: r.start
        )  # Python 3.10+

        # Extend backward if necessary
        if idx > 0 and self._ranges[idx - 1].end >= new.start:
            idx -= 1

        # Merge forward while overlapping
        while idx < len(self._ranges) and new.overlaps(self._ranges[idx]):
            new = new.merge(self._ranges[idx])
            del self._ranges[idx]

        # Also coalesce "touching" intervals (‚Ä¶,end==new.start or vice-versa)
        if idx < len(self._ranges) and new.end == self._ranges[idx].start:
            new = new.merge(self._ranges[idx])
            del self._ranges[idx]
        if idx > 0 and self._ranges[idx - 1].end == new.start:
            new = new.merge(self._ranges[idx - 1])
            del self._ranges[idx - 1]
            idx -= 1

        self._ranges.insert(idx, new)

    # ¬≠‚Äî optional helpers ---------------------------------------------------
    def covers(self, addr: int) -> bool:
        i = bisect_right(self._ranges, addr, key=lambda r: r.start) - 1
        return i >= 0 and addr < self._ranges[i].end

    def as_tuples(self):
        return [(r.start, r.end) for r in self._ranges]


def resolve_overlaps(ranges: list[Range]) -> IntervalSet:
    """
    Fast, linear-time overlap resolution: keep only the first chain
    whose start is ‚â• the furthest end so far.
    """
    logger.info(f"Resolving overlaps among {len(ranges)} ranges")
    intervals = IntervalSet()

    for r in ranges:
        intervals.add(r)

        # decide whether to keep the chain object itself
        last_end = intervals.as_tuples()[-1][1]  # rightmost byte so far
        target = r.end
        if target == last_end:  # this chain extended the interval set
            logger.info(f"  Accepted (or widened): {r.start:X}-{r.end:X}")
        else:
            logger.info(f"  Rejected overlap: {r.start:X}-{r.end:X}")

    return intervals


class PatchManager:
    """Manages deferred patch operations."""

    class Mode(enum.Enum):
        PATCH = enum.auto()  # Use ida_bytes.patch_bytes
        PUT = enum.auto()  # Use ida_bytes.put_bytes

    def __init__(
        self,
        patch_mode: Mode = Mode.PATCH,
        dry_run: bool = False,
        auto_clear: bool = True,
    ):
        self.dry_run = dry_run
        self.patch_mode = patch_mode
        self.pending_patches: list[DeferredPatchOp] = []
        self.auto_clear = auto_clear
        logger.info(
            "PatchManager initialized (dry_run=%s, mode=%s)",
            self.dry_run,
            self.patch_mode.name,
        )

    def add_patch(self, address: int, byte_values: bytes):
        """Creates and queues a DeferredPatchOp."""
        op = DeferredPatchOp(address, byte_values, self.patch_mode)
        self.pending_patches.append(op)
        logger.debug("Queued patch operation: %s", op)

    def apply_all(self, dry_run_override: bool | None = None) -> bool:
        """Applies all queued patch operations."""
        logger.info("Applying %d queued patches...", len(self))
        success_count = 0
        fail_count = 0

        if dry_run_override is None:
            # None is a sentinel value here that represents "use the default"
            dry_run_override = self.dry_run

        for op in self.pending_patches:
            if op.apply(dry_run_override):
                success_count += 1
            else:
                fail_count += 1

        logger.info(
            "Patch application complete. Success: %d, Failed: %d",
            success_count,
            fail_count,
        )
        if self.auto_clear:
            self.pending_patches.clear()  # Clear the list after applying
        return fail_count == 0  # Return True if all patches were applied successfully

    def __len__(self) -> int:
        return len(self.pending_patches)


@dataclasses.dataclass(repr=False)
class DeferredPatchOp:
    """Class to store patch operations that will be applied later."""

    address: int
    byte_values: bytes
    mode: PatchManager.Mode
    dry_run: bool = False

    @classmethod
    def patch(cls, address: int, byte_values: bytes, dry_run: bool = False):
        return cls(address, byte_values, PatchManager.Mode.PATCH, dry_run)

    @classmethod
    def put(cls, address: int, byte_values: bytes, dry_run: bool = False):
        return cls(address, byte_values, PatchManager.Mode.PUT, dry_run)

    def apply(self, dry_run_override: bool = False) -> bool:
        """Apply the patch operation using either patch_bytes or put_bytes based on mode."""

        is_dry_run = dry_run_override or self.dry_run
        logger.info(
            "[*] %sPatching decrypted chunk %s at 0x%X (size: %d)",
            "(Dry Run) " if is_dry_run else "",
            ("revertably" if self.mode == PatchManager.Mode.PATCH else "destructively"),
            self.address,
            len(self.byte_values),
        )
        success = True
        if is_dry_run:
            return success

        try:
            import idaapi  # todo: make this a protocol + pass to the manager

            func = (
                idaapi.put_bytes
                if self.mode == PatchManager.Mode.PUT
                else idaapi.patch_bytes
            )
            func(self.address, self.byte_values)
        except Exception as e:
            logger.error(f"Failed to apply patch {self}: {e}", exc_info=True)
            success = False
        return success

    def __str__(self):
        """String representation with hex formatting."""
        dry_run_str = " (dry run)" if self.dry_run else ""
        return f"{self.__class__.__name__}({len(self.byte_values)} bytes, mode={self.mode.name}{dry_run_str} @ address=0x{self.address:X})"

    __repr__ = __str__


def make_chunks(buf_len: int, n_chunks: int, max_pat: int):
    """
    Yield exactly n_chunks tuples of
      (padded_start, padded_end, core_start, core_end).

    * core ranges partition [0, buf_len) evenly by floor division.
    * padded ranges extend each core by (max_pat-1) on both sides,
      clamped to [0, buf_len].
    """
    for i in range(n_chunks):
        # uniform core split
        core_start = (buf_len * i) // n_chunks
        core_end = (buf_len * (i + 1)) // n_chunks
        core_len = core_end - core_start

        # padding
        padded_start = max(0, core_start - (max_pat - 1))
        padded_end = min(buf_len, core_end + (max_pat - 1))
        padded_len = padded_end - padded_start

        logger.debug(
            "Chunk %2d/%d: "
            "core=[%#x-%#x) (%d bytes), "
            "padded=[%#x-%#x) (%d bytes)",
            i,
            n_chunks,
            core_start,
            core_end,
            core_len,
            padded_start,
            padded_end,
            padded_len,
        )

        yield padded_start, padded_end, core_start, core_end


@contextlib.contextmanager
def shm_buffer(
    name: str, buf_len: int | None = None
) -> typing.Generator[
    multiprocessing.shared_memory.SharedMemory | memoryview, None, None
]:
    """
    context manager to access the shared memory buffer.
    if buf_len is not provided, then the buffer will be the raw shm memory
    buffer else it will be a byte slice of the shm memory buffer.

    Usage:
        with shm_buffer(name=..) as buf:
            # use buf
    """
    shm = multiprocessing.shared_memory.SharedMemory(name=name)
    try:
        yield shm.buf[:buf_len] if buf_len else shm
    finally:
        shm.close()


def execute_chunk_with_shm_view(
    user_chunk_processor: typing.Callable[..., typing.Any],
    shm_name: str,
    padded_start: int,
    padded_end: int,
    *user_args: typing.Any,
) -> typing.Any:
    """
    Handles SHM attachment, memoryview creation, and cleanup for a user's chunk processing function.

    Ensures that the memoryview of the chunk is deleted before the shared memory object is closed,
    preventing "BufferError: cannot close exported pointers exist".

    Args:
        user_chunk_processor: A callable that takes (memoryview, *user_args) and returns results.
                              The memoryview passed to it is a slice of the shared memory.
        shm_name: Name of the shared memory segment.
        padded_start: Start offset for the memoryview slice within the shared memory buffer.
        padded_end: End offset for the memoryview slice.
        *user_args: Additional arguments to pass to the user_chunk_processor.

    Returns:
        The result from the user_chunk_processor.
    """
    # shm_buffer is called with no buf_len, so it yields the SharedMemory object itself.
    with shm_buffer(shm_name) as shm_object:  # shm_object is a SharedMemory instance
        if not isinstance(shm_object, multiprocessing.shared_memory.SharedMemory):
            # This case should ideally not happen if shm_buffer is used as intended (no buf_len)
            # or if shm_buffer is modified to always yield SharedMemory object and handle buf internally.
            # For now, assume shm_object.buf is the way if shm_buffer yields memoryview directly.
            # However, the current shm_buffer yields SharedMemory if buf_len is None.
            logger.error(
                "shm_buffer did not yield a SharedMemory object as expected. Type: %s",
                type(shm_object),
            )
            # Fallback or raise error, this indicates misuse or change in shm_buffer's contract
            # For robustness, let's try to access .buf if it's not SharedMemory but has it
            try:
                buffer_to_view = shm_object.buf  # type: ignore
            except AttributeError:
                logger.error("Shared memory object does not have a .buf attribute.")
                raise TypeError(
                    "shm_buffer yielded an unexpected object type without a .buf attribute."
                ) from None
        else:
            buffer_to_view = shm_object.buf

        # Create the specific memoryview for this chunk
        # The memoryview is taken from the *entire* buffer of the shm_object
        chunk_mv = memoryview(buffer_to_view)[padded_start:padded_end]
        try:
            # Call the user's actual logic function
            result = user_chunk_processor(chunk_mv, *user_args)
            return result
        finally:
            # Crucial: delete the memoryview to release buffer export before shm_object.close()
            # which is called by shm_buffer's __exit__.
            del chunk_mv
            logger.debug(
                "Deleted memoryview for chunk [%d:%d] from shm '%s'",
                padded_start,
                padded_end,
                shm_name,
            )

```

`src/ida_taskr/worker.py`:

```py
"""Worker-side components for the task worker manager."""

import asyncio
import concurrent.futures
import math
import multiprocessing
import multiprocessing.connection
import threading
import time
import typing
import uuid

from .helpers import get_logger
from .protocols import WorkerProtocol
from .utils import AsyncEventEmitter, IntervalSet
from .qt_compat import QT_ASYNCIO_AVAILABLE

logger = get_logger()

# Try to import QtAsyncio components if available
if QT_ASYNCIO_AVAILABLE:
    try:
        from .qtasyncio import set_event_loop_policy as qt_set_event_loop_policy
        QTASYNCIO_ENABLED = True
    except ImportError:
        QTASYNCIO_ENABLED = False
else:
    QTASYNCIO_ENABLED = False


class ConnectionContext:
    """
    Context manager for a multiprocessing.connection.Connection.
    Ensures the connection is closed on exit.
    """

    def __init__(self, address: str, authkey: bytes | str, chunk_size: int = 1024):
        host, port_str = address.split(":")
        self.host = host
        self.port = int(port_str)

        if isinstance(authkey, str):
            authkey = bytes.fromhex(authkey)

        assert isinstance(authkey, bytes), f"Invalid authkey type: {type(authkey)}"
        self.authkey = authkey
        self._conn = None
        self.chunk_size = chunk_size

    @property
    def address(self) -> tuple[str, int]:
        return (self.host, self.port)

    @property
    def conn(self) -> multiprocessing.connection.Connection:
        if self._conn is None:
            self._conn = multiprocessing.connection.Client(
                self.address, family="AF_INET", authkey=self.authkey
            )
            logger.info(f"Connected to {self.address}")
        return self._conn

    def send_message(self, msg_type: str, data, **kwargs) -> bool:
        """
        Send a structured message through the connection.

        If data is a long list, split it into chunks.
        """
        try:
            if isinstance(data, list) and len(data) > self.chunk_size:
                message_id = uuid.uuid4().hex
                total_chunks = math.ceil(len(data) / self.chunk_size)

                for idx in range(total_chunks):
                    part = data[idx * self.chunk_size : (idx + 1) * self.chunk_size]
                    msg = {
                        "type": msg_type,
                        "data": part,
                        "timestamp": time.time(),
                        "message_id": message_id,
                        "chunk_index": idx,
                        "total_chunks": total_chunks,
                        **kwargs,
                    }
                    self.conn.send(msg)
                logger.debug(
                    f"‚Üí Streamed {len(data)} items in {total_chunks} chunks under id {message_id}"
                )
                return True

            # small or non-list payload: single shot
            msg = {
                "type": msg_type,
                "data": data,
                "timestamp": time.time(),
                **kwargs,
            }
            self.conn.send(msg)
            logger.debug(f"‚Üí Sent single message: {msg_type}")
            return True

        except Exception as e:
            logger.error(f"Failed to send message: {e}", exc_info=True)
            return False

    @property
    def closed(self):
        return self.conn.closed

    @property
    def readable(self):
        return self.conn.readable

    @property
    def writable(self):
        return self.conn.writable

    def fileno(self):
        return self.conn.fileno()

    def recv(self):
        return self.conn.recv()

    def poll(self, timeout=0.0):
        return self.conn.poll(timeout)

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        PROPOGATE = False
        SUPPRESS = True
        if exc_type:
            logger.error("Connection closed by parent")
            return PROPOGATE

        if self.conn is not None:
            try:
                logger.info("Closing worker-side connection.")
                self.conn.close()
            except Exception as e:
                logger.error(f"Error closing connection: {e}")
        return SUPPRESS


class WorkerController:
    """Wrap an AsyncEventEmitter in its own event loop

    Optionally uses QtAsyncio for better Qt integration if available.
    Set use_qtasyncio=True to enable Qt-native asyncio event loop.
    """

    def __init__(self, emitter_instance: AsyncEventEmitter, use_qtasyncio: bool = False):
        self.emitter = emitter_instance
        self.use_qtasyncio = use_qtasyncio and QTASYNCIO_ENABLED

        if self.use_qtasyncio:
            # Use Qt-native asyncio event loop
            logger.info("Using QtAsyncio event loop for worker")
            qt_set_event_loop_policy()
            self.loop = asyncio.new_event_loop()
        else:
            # Use standard asyncio event loop
            self.loop = asyncio.new_event_loop()

        self._thread = threading.Thread(target=self._run_loop, daemon=True)
        self._result = None
        self._started = False

    def _run_loop(self):
        asyncio.set_event_loop(self.loop)
        try:
            self._result = self.loop.run_until_complete(self.emitter.run())
        except Exception as e:
            logger.error(f"Exception in worker thread loop: {e}", exc_info=True)
            self._result = None

    def start(self):
        if self._started:
            logger.warning("Start called on an already started worker controller.")
            return
        self._thread.start()
        self._started = True

    def pause(self):
        if not self._started:
            logger.warning("Pause called before worker controller was started.")
            return
        logger.info("‚ñ∂Ô∏è  Pausing...")
        self.loop.call_soon_threadsafe(self.emitter.pause_evt.set)

    def resume(self):
        if not self._started:
            logger.warning("Resume called before worker controller was started.")
            return
        logger.info("‚ñ∂Ô∏è  Resuming...")
        self.loop.call_soon_threadsafe(self.emitter.pause_evt.clear)

    def stop(self):
        if not self._started:
            # Even if not started, set stop event for consistency if needed
            # self.loop.call_soon_threadsafe(self.emitter.stop_evt.set) # Maybe not necessary if loop never runs
            logger.info("üõë  Stopping (controller not started)...")
            if hasattr(
                self.emitter, "stop_evt"
            ):  # Emitter might not be fully initialized
                self.loop.call_soon_threadsafe(self.emitter.stop_evt.set)
            return
        logger.info("üõë  Stopping...")
        self.loop.call_soon_threadsafe(self.emitter.stop_evt.set)

    def join(self):
        if not self._started:
            logger.warning(
                "Join called before worker controller was started. Returning current result."
            )
            return self._result

        if self._thread.is_alive():
            self._thread.join()
        else:
            logger.info(
                "Worker thread was not alive when join was called (already finished or failed?)."
            )
        self._started = False  # Reset for potential restart, though typically not done.
        return self._result

    def set_log_level(self, level):
        # This assumes the logger used by emitter.run() is the global one or accessible
        # If emitter uses its own logger, it needs a method to set its level.
        # For now, we assume it affects the logger instance used within emitter.
        # A more robust solution might involve passing a logger instance or a config.
        logger.setLevel(level)
        if hasattr(self.emitter, "logger"):
            self.emitter.logger.setLevel(level)
        logger.info(f"Log level set to {level} for controller and its emitter.")


class WorkerBase(WorkerProtocol):
    """Base class for worker implementations."""

    def __init__(
        self,
        async_emitter_class: typing.Type[AsyncEventEmitter] | None = None,
        emitter_args: dict | None = None,
        process_chunk_fn: typing.Callable | None = None,
    ):
        self.async_emitter_class = async_emitter_class
        self.emitter_args = emitter_args or {}
        self.process_chunk_fn = process_chunk_fn  # User-provided chunk processor
        self.emitter_instance: AsyncEventEmitter | None = None
        self.controller: WorkerController | None = None
        self.conn: ConnectionContext | None = None

        self._commands = {
            "stop": self._handle_stop,
            "pause": self._handle_pause,
            "resume": self._handle_resume,
            "start": self._handle_start,  # Added start command
            "ping": self._handle_ping,  # Added ping command
            "set_log_level": self._handle_set_log_level,  # Added log level command
        }
        self.logger = get_logger(self.__class__.__name__)
        self._running = False  # Tracks if the process loop is active
        # self._paused is managed by the emitter's pause_evt now

    def setup(self, **kwargs):
        """Default setup: Initializes the AsyncEventEmitter if provided."""
        if self.async_emitter_class:
            # Pass process_chunk_fn to the emitter if it accepts it
            # This requires the AsyncEventEmitter subclass to handle it in its __init__ or a setter
            current_emitter_args = self.emitter_args.copy()
            if self.process_chunk_fn:
                current_emitter_args["process_chunk_fn"] = self.process_chunk_fn

            self.emitter_instance = self.async_emitter_class(**current_emitter_args)
            self._setup_default_event_handlers()
        else:
            self.logger.warning(
                "No async_emitter_class provided for WorkerBase. Task processing will not occur."
            )

    def _setup_default_event_handlers(self):
        """Set up default event handlers for the emitter instance."""
        if not self.emitter_instance:
            return

        @self.emitter_instance.on("run_started")
        def on_run_started():
            self.logger.info("‚ñ∂Ô∏è  Task starting")
            if self.conn:
                self.conn.send_message(
                    "progress", 0.0, status="running", stage="starting"
                )

        @self.emitter_instance.on("run_finished")
        def on_run_finished(results):
            # The structure of 'results' depends on the emitter's run() method.
            # For AntiDeobWorker, it's an IntervalSet.
            # We need a generic way to count/summarize or pass through.
            count = (
                len(results)
                if hasattr(results, "__len__")
                else (1 if results is not None else 0)
            )
            self.logger.info("‚úÖ Task finished, processed %s items/results.", count)
            if self.conn:
                self.conn.send_message(
                    "progress",
                    0.95,  # Assuming this is a placeholder for actual progress
                    status="finalizing",
                    stage="task_complete",  # Generic stage
                    items_count=count,
                )

        @self.emitter_instance.on("stopped")
        def on_stopped():
            self.logger.info("üõë Emitter shutting down")
            if self.conn:
                self.conn.send_message("status", "stopped", status="stopped")

        # Allow subclasses to add more specific handlers
        self.setup_custom_event_handlers()

    def setup_custom_event_handlers(self):
        """Subclasses can override this to add their own event handlers for the emitter."""
        pass

    async def cleanup(self):
        """Default cleanup: Shuts down the emitter and controller."""
        if self.controller:
            self.controller.stop()
            # Join should ideally happen, but might block. Consider if necessary or how to handle.
            # self.controller.join() # This might be problematic in an async cleanup

        if self.emitter_instance:
            await self.emitter_instance.shutdown()
        self.logger.info("Worker cleanup complete.")

    def handle_command(self, cmd: dict, conn: ConnectionContext) -> bool:
        """
        Handle standard commands. Override for custom commands.
        Returns True to continue processing, False to exit.
        """
        cmd_type = cmd.get("command")
        if cmd_type is None:
            return True
        handler = self._commands.get(cmd_type)
        if handler:
            return handler(cmd, conn)
        return True

    def _handle_stop(self, cmd, conn):
        """Handle stop command."""
        self.logger.info("Received stop command.")
        self._running = False  # Signal process loop to stop
        if self.controller:
            self.controller.stop()
        conn.send_message("status", "stopped", status="stopped")
        return False  # Exit process loop

    def _handle_pause(self, cmd, conn):
        """Handle pause command."""
        if self.controller:
            self.controller.pause()
            conn.send_message("status", "paused", status="paused")
        else:
            conn.send_message("error", "Not started", status="error")
        return True

    def _handle_resume(self, cmd, conn):
        """Handle resume command."""
        if self.controller:
            self.controller.resume()
            conn.send_message("status", "resumed", status="running")
        else:
            conn.send_message("error", "Not started", status="error")
        return True

    def _handle_start(self, cmd, conn):
        """Handle start command."""
        if not self.emitter_instance:
            self.logger.error("Cannot start: emitter_instance not initialized.")
            conn.send_message(
                "error", "Worker not properly configured (no emitter)", status="error"
            )
            return True  # Stay in command loop

        if self.controller and self.controller._started:
            self.logger.warning("Start command received, but already started.")
            conn.send_message("status", "already_running", status="running")
            return True

        self.logger.info(
            "Received start command. Initializing and starting controller."
        )
        self.controller = WorkerController(self.emitter_instance)
        self.controller.start()
        conn.send_message("status", "started", status="running")
        return True

    def _handle_ping(self, cmd, conn):
        """Handle ping command."""
        conn.send_message(
            "status", "pong", status="running" if self._running else "idle"
        )
        return True

    def _handle_set_log_level(self, cmd, conn):
        """Handle set_log_level command."""
        level = cmd.get("level")
        if level is None:
            self.logger.error("Log level is required for set_log_level command.")
            conn.send_message("error", "Log level not specified", status="error")
            return True

        try:
            # self.logger.setLevel(level) # WorkerBase logger
            # if self.emitter_instance and hasattr(self.emitter_instance, 'logger'):
            #     self.emitter_instance.logger.setLevel(level)
            if self.controller:
                self.controller.set_log_level(level)  # Propagates to emitter logger too
            else:  # Set global logger if controller not up
                logger.setLevel(level)

            self.logger.info(
                f"Log level set to {level}"
            )  # Changed from logger.info to self.logger.info
            conn.send_message("status", f"log_level_set:{level}", status="running")
        except Exception as e:
            self.logger.error(f"Failed to set log level: {e}", exc_info=True)
            conn.send_message(
                "error", f"Failed to set log level: {str(e)}", status="error"
            )
        return True

    def process(self, connection: ConnectionContext, **kwargs):
        """Main processing loop - implement in subclasses."""
        # raise NotImplementedError("Subclasses must implement process()")
        self.conn = connection
        self._running = True

        # Send initial ready message
        connection.send_message("status", "connected", status="ready")
        self.logger.info("Worker connected, awaiting commands...")

        try:
            while self._running:
                try:
                    # Poll for messages, timeout allows checking self._running
                    if not connection.closed and not connection.poll(timeout=0.5):
                        # self.logger.debug("No message, continuing poll.")
                        if not self._running:  # Check if stop was called during poll
                            break
                        continue
                    if connection.closed:  # check before recv
                        self.logger.error(
                            "Connection closed by parent (detected before recv)."
                        )
                        self._running = False
                        break
                    cmd = connection.recv()
                    self.logger.debug(f"‚Üê Received command: {cmd}")
                except EOFError:
                    self.logger.error("Connection closed by parent (EOFError).")
                    self._running = False  # Ensure loop termination
                    break
                except (ConnectionResetError, BrokenPipeError) as e:
                    self.logger.error(f"Connection error: {e}")
                    self._running = False  # Ensure loop termination
                    break
                except Exception as e:
                    self.logger.error(
                        f"Unexpected error receiving command: {e}", exc_info=True
                    )
                    # Optionally send error message back if connection is still usable
                    # connection.send_message("error", f"Error receiving command: {str(e)}", status="error")
                    # Depending on error, might need to stop.
                    # For now, continue to allow graceful shutdown via 'stop' command.
                    continue

                if (
                    not self._running
                ):  # check after recv, if stop was part of the message processing
                    break

                # Handle command
                if isinstance(cmd, dict):
                    if not self.handle_command(cmd, connection):
                        self._running = False  # Command handler requested exit
                        break
                elif cmd is None:  # Typically means connection closed
                    self.logger.info("Received None, likely connection closed.")
                    self._running = False
                    break
                else:
                    self.logger.warning(
                        f"Received unexpected command type: {type(cmd)}"
                    )
                    connection.send_message(
                        "error",
                        f"Expected dict command, got {type(cmd)}",
                        status="error",
                    )
            # Loop exited, ensure controller stops if it was started
            if self.controller:
                self.logger.info(
                    "Process loop finished. Ensuring controller is stopped."
                )
                self.controller.stop()

        finally:
            self.logger.info("Process loop finalizing.")
            if self.controller:
                self.logger.info("Waiting for task controller to join...")
                results = self.controller.join()
                if results is not None:
                    self.send_results(connection, results)
                else:
                    self.logger.info(
                        "No results from controller or task did not complete successfully."
                    )
            else:
                self.logger.info("Controller was not active or not used.")

            self.conn = None  # Clear connection reference
            self.logger.info("Worker process loop ended.")

    def send_results(self, connection: ConnectionContext, results_data):
        """Formats and sends results. Subclasses can override for custom formatting."""
        # Default: assume results_data is list-like or an IntervalSet
        if isinstance(results_data, IntervalSet):
            as_json = [
                {"address": s, "length": e - s, "end": e}
                for s, e in results_data.as_tuples()
            ]
        elif isinstance(results_data, list):
            # Assume it's already in a suitable JSON-serializable list format
            as_json = results_data
        elif results_data is not None:
            # Attempt to make it a list if it's a single non-list item
            as_json = [results_data]
        else:
            as_json = []

        if as_json:
            self.logger.info(f"Sending {len(as_json)} results...")
            connection.send_message(
                "status", "sending_results", status="sending_results"
            )
            # Consider making sleep configurable or removing if IDA side handles UI updates better
            # time.sleep(1) # Shorter sleep or make IDA side robust
            connection.send_message(
                "result", as_json, status="success", count=len(as_json)
            )
            connection.send_message("status", "results_sent", status="results_sent")
        else:
            self.logger.info("No results to send or results were empty.")
            # Send a success message even if results are empty, or a specific no_results status
            connection.send_message(
                "result", [], status="success", count=0, note="No results generated"
            )

    # Subclasses might need to implement their own run_async_task or similar
    # if the AsyncEventEmitter pattern is not sufficient.

```

`tests/11.1.0.60228.json`:

```json
{
    "5390373031": {
        "addr": 5390373031,
        "size": 160,
        "data": "660f1f8400000000009085eb73400f85ee1d0000c6c7d55381c22d746aeef6dfc6c0eb6845ce3f4483c759c7c66db0d16d590f8025c3010050c7c344c9c5e7e8d0db000051c7c5fb598075a185d8410fb650fb80fae80f849d000000418078f9ff7516410fb648fa80e13880f910740580f9187504b101eb0232c9418078faff750f80e23880ea10f6c2f77504b001eb0232c00ac8418078fcff7511410fb640"
    },
    "5394431165": {
        "addr": 5394431165,
        "size": 1325,
        "data": "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"
    },
    "5368727905": {
        "addr": 5368727905,
        "size": 110,
        "data": "7b0c8adb7a1a80ee5680e85c81c2907b039a81ed86c97b4f80e99e80c35981e886c97a0680c1ae81e8e7eb039e81e98ad27a34c6c3cae88a3d0000e8a89c00005d0f8cef150000515c0f8dab02010068be24b2d368bc228b0f0f3180c2edf6d97ed3f6ac4187dc40303b0f1f440000"
    },
    "5376900737": {
        "addr": 5376900737,
        "size": 160,
        "data": "34008af6710b80ea2c83c13ac6c327c7c386ed710c81c58715af0c6b8b6558f208c70578f2b50354010000498bc2498b50f84933d149f7f7488945384989109080f200714283c5116af56a820f3181c187f3fd13e8c5bb010081c1dd1f2cade80c120100c7c0ffc7eb1ec6c27a68926efecf80c000e8693400006a3a83c1f483c0826ba9f4675d48c74530ff1a0000488d7d40488b453048356a120000488945"
    },
    "5376975793": {
        "addr": 5376975793,
        "size": 160,
        "data": "f986ff76040f3181c08aff761480c2ac0f8b9bbd00005881e8d81ce41d4a699b02c70548cdb40354010000498bc2498b50f84933d148f7f648894528498910900c0086c9730780e9250f3181c690eb0ccc74b153f6d8f6df79030f877329000081ed956db7e60f310f8823fa000083c13781ed554ec00dc7c50090b1b081eeb48212936984b16e48c74520ff1a0000488d7d30488b452048356a120000488945"
    },
    "5368729905": {
        "addr": 5368729905,
        "size": 160,
        "data": "f88aff731d83c64e80c0c6f6db775e6aa083c7ba5e5fc6c1f080ee746a750f3181eb8adb731455c7c00b9a45680f8e125e010050c1a8ae0d1d3633c9488bd74885ff0f84cd0000000f1f8000000000909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090"
    },
    "5369441646": {
        "addr": 5369441646,
        "size": 160,
        "data": "4d8d0c0b0f1f400066660f1f8400000000009080f30086ed730883c64280eeffc7c08ad2730454681f3f73210f3181c205c15b9583c7e283c25a0f315d80ef836a9c5c75227e1e4b6bab98c61a498bd1418bce4d85c90f84cc000000660f1f44000090a8eb716cc6c51a81c04a6b0cb4f6d8c6c7877d2381c182cbcc140f82fe6d000080c13280c56fc6c541e8cafe0100749a5b0f8d3fc50100f6dd5e0f8116"
    }
}

```

`tests/__init__.py`:

```py
"""IDA Taskr test suite.

This test suite is organized into two categories:

- tests/unit/: Unit tests that use mocks and don't require IDA Pro or Qt
- tests/integration/: Integration tests that require a real IDA Pro environment

To run unit tests:
    python -m unittest discover -s tests/unit/

To run integration tests:
    pytest tests/integration/
"""

```

`tests/conftest.py`:

```py
"""Root pytest configuration for all tests.

IMPORTANT: Import Qt BEFORE idapro to avoid "PySide6 can only be used from GUI" error.
idapro sets up an import hook that blocks PySide6 if imported after.
"""

import pytest

# Import Qt first - must happen before any idapro import
try:
    from PySide6 import QtCore
    QT_AVAILABLE = True
except ImportError:
    try:
        from PyQt5 import QtCore
        QT_AVAILABLE = True
    except ImportError:
        QT_AVAILABLE = False
        QtCore = None


@pytest.fixture(scope="session")
def qapp():
    """Create a QCoreApplication for tests that need Qt event loop."""
    if not QT_AVAILABLE or QtCore is None:
        pytest.skip("Qt not available")

    app = QtCore.QCoreApplication.instance()
    if app is None:
        import sys
        app = QtCore.QCoreApplication(sys.argv)
    yield app
    # Don't quit - other tests might need it


@pytest.fixture(scope="function")
def qapp_function(qapp):
    """Function-scoped fixture for tests that need a fresh Qt context."""
    yield qapp


```

`tests/integration/11.1.0.60228.json`:

```json
{
    "5390373031": {
        "addr": 5390373031,
        "size": 160,
        "data": "660f1f8400000000009085eb73400f85ee1d0000c6c7d55381c22d746aeef6dfc6c0eb6845ce3f4483c759c7c66db0d16d590f8025c3010050c7c344c9c5e7e8d0db000051c7c5fb598075a185d8410fb650fb80fae80f849d000000418078f9ff7516410fb648fa80e13880f910740580f9187504b101eb0232c9418078faff750f80e23880ea10f6c2f77504b001eb0232c00ac8418078fcff7511410fb640"
    },
    "5394431165": {
        "addr": 5394431165,
        "size": 1325,
        "data": "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"
    },
    "5368727905": {
        "addr": 5368727905,
        "size": 110,
        "data": "7b0c8adb7a1a80ee5680e85c81c2907b039a81ed86c97b4f80e99e80c35981e886c97a0680c1ae81e8e7eb039e81e98ad27a34c6c3cae88a3d0000e8a89c00005d0f8cef150000515c0f8dab02010068be24b2d368bc228b0f0f3180c2edf6d97ed3f6ac4187dc40303b0f1f440000"
    },
    "5376900737": {
        "addr": 5376900737,
        "size": 160,
        "data": "34008af6710b80ea2c83c13ac6c327c7c386ed710c81c58715af0c6b8b6558f208c70578f2b50354010000498bc2498b50f84933d149f7f7488945384989109080f200714283c5116af56a820f3181c187f3fd13e8c5bb010081c1dd1f2cade80c120100c7c0ffc7eb1ec6c27a68926efecf80c000e8693400006a3a83c1f483c0826ba9f4675d48c74530ff1a0000488d7d40488b453048356a120000488945"
    },
    "5376975793": {
        "addr": 5376975793,
        "size": 160,
        "data": "f986ff76040f3181c08aff761480c2ac0f8b9bbd00005881e8d81ce41d4a699b02c70548cdb40354010000498bc2498b50f84933d148f7f648894528498910900c0086c9730780e9250f3181c690eb0ccc74b153f6d8f6df79030f877329000081ed956db7e60f310f8823fa000083c13781ed554ec00dc7c50090b1b081eeb48212936984b16e48c74520ff1a0000488d7d30488b452048356a120000488945"
    },
    "5368729905": {
        "addr": 5368729905,
        "size": 160,
        "data": "f88aff731d83c64e80c0c6f6db775e6aa083c7ba5e5fc6c1f080ee746a750f3181eb8adb731455c7c00b9a45680f8e125e010050c1a8ae0d1d3633c9488bd74885ff0f84cd0000000f1f8000000000909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090909090"
    },
    "5369441646": {
        "addr": 5369441646,
        "size": 160,
        "data": "4d8d0c0b0f1f400066660f1f8400000000009080f30086ed730883c64280eeffc7c08ad2730454681f3f73210f3181c205c15b9583c7e283c25a0f315d80ef836a9c5c75227e1e4b6bab98c61a498bd1418bce4d85c90f84cc000000660f1f44000090a8eb716cc6c51a81c04a6b0cb4f6d8c6c7877d2381c182cbcc140f82fe6d000080c13280c56fc6c541e8cafe0100749a5b0f8d3fc50100f6dd5e0f8116"
    }
}

```

`tests/integration/__init__.py`:

```py
"""Integration tests for IDA Taskr with real Qt environments.

These tests require pytest and a Qt framework (PyQt5 or PySide6).
"""

```

`tests/integration/anti_deob/deobfuscator.py`:

```py
"""Core deobfuscation logic."""

import asyncio
import collections
import concurrent.futures
import contextlib
import dataclasses
import enum
import functools
import itertools
import logging
import multiprocessing
import re
import struct
import threading
import time
import typing
import warnings

import capstone
from ida_taskr import get_logger
from ida_taskr.utils import (
    AsyncEventEmitter,
    IntervalSet,
    Range,
    log_execution_time,
    make_chunks,
    resolve_overlaps,
    shm_buffer,
)

# PyQt 5.15/6.2/6.3/6.4:
# https://riverbankcomputing.com/news/SIP_v6.7.12_Released
warnings.filterwarnings(
    "ignore",
    category=DeprecationWarning,
    message=(
        r"sipPyTypeDict\(\) is deprecated, the extension module should use "
        r"sipPyTypeDictRef\(\) instead"
    ),
)

# maximum length of any stage-1 pattern (you said 129 bytes)
MAX_PATTERN_LEN = 129
MIN_PATTERN_LEN = 12
LOG_LEVEL = logging.INFO
logger = get_logger(__name__)


class PatternCategory(enum.Enum):
    MULTI_PART = enum.auto()
    SINGLE_PART = enum.auto()
    JUNK = enum.auto()


@dataclasses.dataclass
class RegexPatternMetadata:
    category: PatternCategory
    pattern: bytes  # The regex pattern as a bytes literal
    description: typing.Optional[str] = None
    compiled: typing.Optional[typing.Pattern] = None

    def compile(self, flags=0):
        """Compile the regex if not already done, and return the compiled object."""
        if self.compiled is None:
            self.compiled = re.compile(self.pattern, flags)
        return self.compiled

    @property
    def group_names(self):
        """Return the dictionary mapping group names to their indices."""
        return self.compile().groupindex


@dataclasses.dataclass
class MultiPartPatternMetadata(RegexPatternMetadata):
    category: PatternCategory = dataclasses.field(
        default=PatternCategory.MULTI_PART, init=False
    )

    def __post_init__(self):
        # Compile to ensure group names are available.
        _ = self.compile(re.DOTALL | re.VERBOSE)
        required_groups = {"first_jump", "padding", "second_jump"}
        missing = required_groups - set(self.group_names)
        if missing:
            raise ValueError(f"MultiPart pattern is missing required groups: {missing}")


@dataclasses.dataclass
class SinglePartPatternMetadata(RegexPatternMetadata):
    category: PatternCategory = dataclasses.field(
        default=PatternCategory.SINGLE_PART, init=False
    )

    def __post_init__(self):
        _ = self.compile(re.DOTALL | re.VERBOSE)
        required_groups = {"prefix", "padding", "jump"}
        missing = required_groups - set(self.group_names)
        if missing:
            raise ValueError(
                f"SinglePart pattern is missing required groups: {missing}"
            )


@dataclasses.dataclass
class JunkPatternMetadata(RegexPatternMetadata):
    category: PatternCategory = dataclasses.field(
        default=PatternCategory.JUNK, init=False
    )

    def __post_init__(self):
        _ = self.compile(re.DOTALL | re.VERBOSE)
        required_groups = {"junk"}
        missing = required_groups - set(self.group_names)
        if missing:
            raise ValueError("Junk pattern must have a 'junk' group.")


# fmt: off
# (We do not wrap this in a named group here so that we can reuse it inside other groups.)
PADDING_PATTERN = rb"""
    (?:
        \xC0[\xE0-\xFF]\x00  # 3-byte SHL reg, 0 with random register encoding
        |                     # OR
        (?:\x86|\x8A)        # 2-byte XCHG or MOV instruction
        [\xC0\xC9\xD2\xDB\xE4\xED\xF6\xFF]  # Specific register encodings
    )
"""
    
# Multi-part jump patterns: pairs of conditional jumps with optional padding
MULTI_PART_PATTERNS = [
    MultiPartPatternMetadata(rb"(?P<first_jump>\x70.)(?P<padding>" + PADDING_PATTERN + rb")*(?P<second_jump>\x71.)", "JO ... JNO"),
    MultiPartPatternMetadata(rb"(?P<first_jump>\x71.)(?P<padding>" + PADDING_PATTERN + rb")*(?P<second_jump>\x70.)", "JNO ... JO"),
    MultiPartPatternMetadata(rb"(?P<first_jump>\x72.)(?P<padding>" + PADDING_PATTERN + rb")*(?P<second_jump>\x73.)", "JB ... JAE"),
    MultiPartPatternMetadata(rb"(?P<first_jump>\x73.)(?P<padding>" + PADDING_PATTERN + rb")*(?P<second_jump>\x72.)", "JAE ... JB"),
    MultiPartPatternMetadata(rb"(?P<first_jump>\x74.)(?P<padding>" + PADDING_PATTERN + rb")*(?P<second_jump>\x75.)", "JE ... JNE"),
    MultiPartPatternMetadata(rb"(?P<first_jump>\x75.)(?P<padding>" + PADDING_PATTERN + rb")*(?P<second_jump>\x74.)", "JNE ... JE"),
    MultiPartPatternMetadata(rb"(?P<first_jump>\x76.)(?P<padding>" + PADDING_PATTERN + rb")*(?P<second_jump>\x77.)", "JBE ... JA"),
    MultiPartPatternMetadata(rb"(?P<first_jump>\x77.)(?P<padding>" + PADDING_PATTERN + rb")*(?P<second_jump>\x76.)", "JA ... JBE"),
    MultiPartPatternMetadata(rb"(?P<first_jump>\x78.)(?P<padding>" + PADDING_PATTERN + rb")*(?P<second_jump>\x79.)", "JS ... JNS"),
    MultiPartPatternMetadata(rb"(?P<first_jump>\x79.)(?P<padding>" + PADDING_PATTERN + rb")*(?P<second_jump>\x78.)", "JNS ... JS"),
    MultiPartPatternMetadata(rb"(?P<first_jump>\x7A.)(?P<padding>" + PADDING_PATTERN + rb")*(?P<second_jump>\x7B.)", "JP ... JNP"),
    MultiPartPatternMetadata(rb"(?P<first_jump>\x7B.)(?P<padding>" + PADDING_PATTERN + rb")*(?P<second_jump>\x7A.)", "JNP ... JP"),
    MultiPartPatternMetadata(rb"(?P<first_jump>\x7C.)(?P<padding>" + PADDING_PATTERN + rb")*(?P<second_jump>\x7D.)", "JL ... JGE"),
    MultiPartPatternMetadata(rb"(?P<first_jump>\x7D.)(?P<padding>" + PADDING_PATTERN + rb")*(?P<second_jump>\x7C.)", "JGE ... JL"),
    MultiPartPatternMetadata(rb"(?P<first_jump>\x7E.)(?P<padding>" + PADDING_PATTERN + rb")*(?P<second_jump>\x7F.)", "JLE ... JG"),
    MultiPartPatternMetadata(rb"(?P<first_jump>\x7F.)(?P<padding>" + PADDING_PATTERN + rb")*(?P<second_jump>\x7E.)", "JG ... JLE"),
]

# Single-part jump patterns: prefix instruction + optional padding + conditional jump
SINGLE_PART_PATTERNS = [
    SinglePartPatternMetadata(rb"(?P<prefix>\x0C\x00)(?P<padding>" + PADDING_PATTERN + rb")?(?P<jump>\x71.)", "OR AL, 0x00 ... JNO"),
    SinglePartPatternMetadata(rb"(?P<prefix>\x0C\x00)(?P<padding>" + PADDING_PATTERN + rb")?(?P<jump>\x73.)", "OR AL, 0x00 ... JAE"),
    SinglePartPatternMetadata(rb"(?P<prefix>\x24\xFF)(?P<padding>" + PADDING_PATTERN + rb")?(?P<jump>\x71.)", "AND AL, 0xFF ... JNO"),
    SinglePartPatternMetadata(rb"(?P<prefix>\x24\xFF)(?P<padding>" + PADDING_PATTERN + rb")?(?P<jump>\x73.)", "AND AL, 0xFF ... JAE"),
    SinglePartPatternMetadata(rb"(?P<prefix>\x34\x00)(?P<padding>" + PADDING_PATTERN + rb")?(?P<jump>\x71.)", "XOR AL, 0x00 ... JNO"),
    SinglePartPatternMetadata(rb"(?P<prefix>\x34\x00)(?P<padding>" + PADDING_PATTERN + rb")?(?P<jump>\x73.)", "XOR AL, 0x00 ... JAE"),
    SinglePartPatternMetadata(rb"(?P<prefix>\x80[\xC8-\xCF]\x00)(?P<padding>" + PADDING_PATTERN + rb")?(?P<jump>\x71.)", "OR r/m8, 0x00 ... JNO"),
    SinglePartPatternMetadata(rb"(?P<prefix>\x80[\xC8-\xCF]\x00)(?P<padding>" + PADDING_PATTERN + rb")?(?P<jump>\x73.)", "OR r/m8, 0x00 ... JAE"),
    SinglePartPatternMetadata(rb"(?P<prefix>\x80[\xE0-\xE7]\xFF)(?P<padding>" + PADDING_PATTERN + rb")?(?P<jump>\x71.)", "AND r/m8, 0xFF ... JNO"),
    SinglePartPatternMetadata(rb"(?P<prefix>\x80[\xE0-\xE7]\xFF)(?P<padding>" + PADDING_PATTERN + rb")?(?P<jump>\x73.)", "AND r/m8, 0xFF ... JAE"),
    SinglePartPatternMetadata(rb"(?P<prefix>\x80[\xF0-\xF7]\x00)(?P<padding>" + PADDING_PATTERN + rb")?(?P<jump>\x71.)", "XOR r/m8, 0x00 ... JNO"),
    SinglePartPatternMetadata(rb"(?P<prefix>\x80[\xF0-\xF7]\x00)(?P<padding>" + PADDING_PATTERN + rb")?(?P<jump>\x73.)", "XOR r/m8, 0x00 ... JAE"),
    SinglePartPatternMetadata(rb"(?P<prefix>\x84.)(?P<padding>" + PADDING_PATTERN + rb")?(?P<jump>\x71.)", "TEST r/m8, r8 ... JNO"),
    SinglePartPatternMetadata(rb"(?P<prefix>\x84.)(?P<padding>" + PADDING_PATTERN + rb")?(?P<jump>\x73.)", "TEST r/m8, r8 ... JAE"),
    SinglePartPatternMetadata(rb"(?P<prefix>\x85.)(?P<padding>" + PADDING_PATTERN + rb")?(?P<jump>\x71.)", "TEST r/m32, r32 ... JNO"),
    SinglePartPatternMetadata(rb"(?P<prefix>\x85.)(?P<padding>" + PADDING_PATTERN + rb")?(?P<jump>\x73.)", "TEST r/m32, r32 ... JAE"),
    SinglePartPatternMetadata(rb"(?P<prefix>\xA8.)(?P<padding>" + PADDING_PATTERN + rb")?(?P<jump>\x71.)", "TEST AL, imm8 ... JNO"),
    SinglePartPatternMetadata(rb"(?P<prefix>\xA8.)(?P<padding>" + PADDING_PATTERN + rb")?(?P<jump>\x73.)", "TEST AL, imm8 ... JAE"),
    SinglePartPatternMetadata(rb"(?P<prefix>\xA9....)(?P<padding>" + PADDING_PATTERN + rb")?(?P<jump>\x71.)", "TEST EAX, imm32 ... JNO"),
    SinglePartPatternMetadata(rb"(?P<prefix>\xA9....)(?P<padding>" + PADDING_PATTERN + rb")?(?P<jump>\x73.)", "TEST EAX, imm32 ... JAE"),
    SinglePartPatternMetadata(rb"(?P<prefix>\xF6..)(?P<padding>" + PADDING_PATTERN + rb")?(?P<jump>\x71.)", "TEST r/m8, imm8 ... JNO"),
    SinglePartPatternMetadata(rb"(?P<prefix>\xF6..)(?P<padding>" + PADDING_PATTERN + rb")?(?P<jump>\x73.)", "TEST r/m8, imm8 ... JAE"),
    SinglePartPatternMetadata(rb"(?P<prefix>\xF7.....)(?P<padding>" + PADDING_PATTERN + rb")?(?P<jump>\x71.)", "TEST r/m32, imm32 ... JNO"),
    SinglePartPatternMetadata(rb"(?P<prefix>\xF7.....)(?P<padding>" + PADDING_PATTERN + rb")?(?P<jump>\x73.)", "TEST r/m32, imm32 ... JAE"),
    SinglePartPatternMetadata(rb"(?P<prefix>\xF8)(?P<padding>" + PADDING_PATTERN + rb")?(?P<jump>\x73.)", "CLC ... JAE"),
    SinglePartPatternMetadata(rb"(?P<prefix>\xF9)(?P<padding>" + PADDING_PATTERN + rb")?(?P<jump>\x72.)", "STC ... JB"),
    SinglePartPatternMetadata(rb"(?P<prefix>\xF9)(?P<padding>" + PADDING_PATTERN + rb")?(?P<jump>\x76.)", "STC ... JBE"),
    # CMP ESP,0x1C00h ‚Äì always ‚Äúabove‚Äù on a real Win x64 stack, then JA short
    # # 81 FC 00 44 00 00
    # # 81 FC 00 3F 00 00
    # # 81 FC 00 44 00 00
    SinglePartPatternMetadata(rb"(?P<prefix>[\x80\x81\x83]\xFC\x00...)(?P<padding>" + PADDING_PATTERN + rb")?(?P<jump>(\x77|\x73).)", "CMP ESP,0x1C00 ‚Ä¶ JA/E"),
]



# Helper sets for checking specific registers based on the regex patterns
# These patterns [\xC0-\xC3\xC5-\xC7] and [\xD8-\xDB\xDD-\xDF] and [\xE8-\xEB\xED-\xEF]
# correspond to ModR/M byte where MOD=11 (register) and R/M is 0-3 (EAX, ECX, EDX, EBX)
# or 5-7 (EBP, ESI, EDI). R/M=4 is ESP, which is skipped by these ranges.
# For 8-bit, these are AL, CL, DL, BL, BPL, SIL, DIL.
# Since REX prefixes are confirmed *not* to be used with these patterns,
# we only need to check for the 8-bit and 32-bit registers.
REG_32_SET = {
    capstone.x86.X86_REG_EAX,
    capstone.x86.X86_REG_ECX,
    capstone.x86.X86_REG_EDX,
    capstone.x86.X86_REG_EBX,
    capstone.x86.X86_REG_EBP,
    capstone.x86.X86_REG_ESI,
    capstone.x86.X86_REG_EDI,
}

REG_8_SET = {
    capstone.x86.X86_REG_AL,
    capstone.x86.X86_REG_CL,
    capstone.x86.X86_REG_DL,
    capstone.x86.X86_REG_BL,
    # AH (C4) is skipped by the regex range
    capstone.x86.X86_REG_CH,  # C5
    capstone.x86.X86_REG_DH,  # C6
    capstone.x86.X86_REG_BH,  # C7
    # Low bytes of SI, DI, BP, SP are only accessible with REX in 64-bit
    # but the regex implies non-REX. The original set included BPL, SIL, DIL
    # which correspond to ModR/M 101, 110, 111 when MOD != 11.
    # The regex range C0-C3, C5-C7 *specifically* uses MOD=11.
    # So, the correct registers are AL, CL, DL, BL, CH, DH, BH.
    # Let's redefine REG_8_SET based *only* on the registers implied by
    # the specific ModR/M bytes in the regexes when MOD=11.
}

# Helper to check if the first operand is a register from the allowed set
# based on the ModR/M ranges implied by the regexes.
# Assumes no REX prefixes are used with these specific junk patterns,
# so we only check against the 8-bit and 32-bit sets.
def is_allowed_reg(operands):
    if not operands or operands[0].type != capstone.CS_OP_REG:
        return False
    reg = operands[0].reg
    # Check if the register is one of the 8-bit or 32-bit registers
    # corresponding to the ModR/M R/M field 0-3, 5-7 when MOD=11,
    # which is the set derived from ModR/M=11 ranges
    return reg in REG_8_SET or reg in REG_32_SET

# Helper to check if there's an immediate operand
def has_imm_operand(operands):
    return any(op.type == capstone.CS_OP_IMM for op in operands)

# Function to check if a byte is a valid REX prefix (0x40-0x4F)
def is_rex_prefix(byte):
    return 0x40 <= byte <= 0x4F

# Function to check if a byte is a valid ModR/M byte (0x80-0xBF)
def is_valid_modrm(byte):
    return 0x80 <= byte <= 0xBF
# fmt: on


class SegmentType(enum.Enum):
    STAGE1_MULTIPLE = enum.auto()
    STAGE1_SINGLE = enum.auto()
    JUNK = enum.auto()
    BIG_INSTRUCTION = enum.auto()


@dataclasses.dataclass
class MatchSegment:
    start: int
    length: int
    description: str
    matched_bytes: bytes
    segment_type: SegmentType
    matched_groups: dict = dataclasses.field(default_factory=dict)


class MatchChain:
    def __init__(self, base_address: int, segments: list[MatchSegment] | None = None):
        self.base_address = base_address
        self.segments = segments or []

    def add_segment(self, segment: MatchSegment):
        self.segments.append(segment)

    def overall_start(self) -> int:
        return self.segments[0].start + self.base_address if self.segments else 0

    def overall_length(self) -> int:
        if not self.segments:
            return 0
        first = self.segments[0]
        last = self.segments[-1]
        return (last.start + last.length) - first.start

    def overall_matched_bytes(self) -> bytes:
        return b"".join(seg.matched_bytes for seg in self.segments)

    def append_junk(
        self, junk_start: int, junk_len: int, junk_desc: str, junk_bytes: bytes
    ):
        seg = MatchSegment(
            start=junk_start,
            length=junk_len,
            description=junk_desc,
            matched_bytes=junk_bytes,
            segment_type=SegmentType.JUNK,
        )
        self.add_segment(seg)

    @property
    def description(self) -> str:
        desc = []
        for idx, seg in enumerate(self.segments):
            if idx == 0:
                desc.append(f"{seg.description}")
            else:
                desc.append(f" -> {seg.description}")
        return "".join(desc)

    def update_description(self, new_desc: str):
        if self.segments:
            self.segments[0].description = new_desc

    # New properties for junk analysis
    @property
    def stage1_type(self) -> SegmentType:
        return self.segments[0].segment_type

    @property
    def junk_segments(self) -> list:
        """
        Returns a list of segments considered as junk based on their segment_type.
        """
        return [seg for seg in self.segments if seg.segment_type == SegmentType.JUNK]

    @property
    def junk_starts_at(self) -> typing.Optional[int]:
        """
        Returns the starting address of the junk portion.
        This is computed as base_address + the offset of the first junk segment.
        If no junk segments exist, returns None.
        """
        js = self.junk_segments
        if js:
            return self.base_address + js[0].start
        return None

    @property
    def junk_length(self) -> int:
        """
        Returns the total length of the junk portion.
        This is computed as the difference between the end (start + length) of the last junk segment
        and the start of the first junk segment.
        If there are no junk segments, returns 0.
        """
        js = self.junk_segments
        if not js:
            return 0
        first = js[0]
        last = js[-1]
        return (last.start + last.length) - first.start

    def __lt__(self, other):
        return self.overall_start() < other.overall_start()

    def __repr__(self):
        r = [
            f"{self.description.rjust(32, ' ')} @ 0x{self.overall_start():X} - "
            f"{self.overall_matched_bytes().hex()[:16]}"
            f"{'...' if self.overall_length() > 16 else ''}",
            "  |",
        ]
        for seg in self.segments:
            _grps = f"{' - ' + str(seg.matched_groups) if seg.matched_groups else ''}"
            r.append(
                f"  |_ {seg.description} @ 0x{self.base_address + seg.start:X} - {seg.matched_bytes.hex()}{_grps}"
            )
        return "\n".join(r)


# TODO: inherit from list?
class MatchChains:
    def __init__(self):
        self.chains: list[MatchChain] = []

    def add_chain(self, chain: MatchChain):
        self.chains.append(chain)

    def __iter__(self):
        yield from self.chains

    def sort(self):
        self.chains.sort(key=lambda x: x.overall_start())

    def __len__(self):
        return len(self.chains)

    def __repr__(self):
        lines = []
        for c in self.chains:
            desc = c.description
            off = c.overall_start()
            bhex = c.overall_matched_bytes().hex()[:16]
            tail = "‚Ä¶" if c.overall_length() > 16 else ""
            lines.append(f"{desc.rjust(32)} @ 0x{off:X} - {bhex}{tail}")
        return "\n".join(lines)


def _stage1_scan_one(job: tuple[RegexPatternMetadata, int, memoryview]) -> MatchChains:
    """
    job = (pattern_bytes, description, segment_type, base_ea, buf)
    returns MatchChains
    """
    rgx, base_ea, buf = job
    prog = rgx.compile()
    hits = MatchChains()

    for m in prog.finditer(buf):
        s = m.start()
        e = m.end()
        mb = buf[s:e]
        match_len = e - s

        groups = {
            k: v.hex()
            for k, v in m.groupdict().items()
            if (v is not None and k != "padding")
        }

        # compute jump targets
        if "jump" in groups:
            offset = struct.unpack("<b", mb[-1:])[0]
            tgt = base_ea + s + match_len + offset
            groups["target"] = hex(tgt)
        elif "first_jump" in groups:
            off1 = struct.unpack("<b", mb[1:2])[0]
            groups["first_target"] = hex(base_ea + s + 2 + off1)
            off2 = struct.unpack("<b", mb[-1:])[0]
            groups["second_target"] = hex(base_ea + s + match_len + off2)

        seg = MatchSegment(
            start=s,
            length=match_len,
            description=rgx.description or "",
            matched_bytes=mb.tobytes(),
            segment_type=(
                SegmentType.STAGE1_MULTIPLE
                if rgx.category == PatternCategory.MULTI_PART
                else SegmentType.STAGE1_SINGLE
            ),
            matched_groups=groups,
        )
        hits.add_chain(MatchChain(base_address=base_ea, segments=[seg]))

    return hits


def stage1_find_patterns(
    buf: memoryview, base_ea: int, mp: bool = False
) -> list[MatchChain]:
    """
    Parallel regex-based Stage 1. Returns List[MatchChain].
    """
    jobs = [
        (rgx, base_ea, buf)
        for rgx in itertools.chain(MULTI_PART_PATTERNS, SINGLE_PART_PATTERNS)
    ]

    if mp:
        ctx = multiprocessing.get_context("spawn")
        with concurrent.futures.ProcessPoolExecutor(mp_context=ctx) as exe:
            all_groups = exe.map(_stage1_scan_one, jobs)
    else:
        all_groups = list(map(_stage1_scan_one, jobs))

    # flatten
    out = [chain for group in all_groups for chain in group]
    # sort
    out.sort(key=lambda c: c.overall_start())
    return out


# ‚îÄ‚îÄ‚îÄ Stage 2: peel off junk via Capstone ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ


@dataclasses.dataclass
class JunkInstruction:
    """Holds information about a single peeled junk instruction."""

    #: Offset relative to the start of the peeled buffer
    start_offset: int
    #: Length of the peeled junk instruction
    length: int
    #: Description of the peeled junk instruction
    description: str
    #: Bytes of the peeled junk instruction
    matched_bytes: bytes


class CapstoneDisasmContext:
    """
    Context manager and iterable for Capstone disassembly.

    Usage:
        with CapstoneDisasmContext(is_64, buf) as disasm_ctx:
            for insn in disasm_ctx:
                ...

    On error, iteration yields nothing.
    """

    _EMPTY_SET = set()

    def __init__(self, is_64: bool):
        self.is_64 = is_64

    @functools.cached_property
    def md(self):
        """
        Cached property for the Capstone disassembler instance.
        Returns None if initialization fails.
        """
        try:
            md = capstone.Cs(
                capstone.CS_ARCH_X86,
                capstone.CS_MODE_64 if self.is_64 else capstone.CS_MODE_32,
            )
            md.detail = True
            return md
        except Exception as e:
            logger.error(f"Failed to initialize Capstone: {e}")
            return None

    def __enter__(self):
        # No setup needed; iteration is handled in __iter__
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        # The __exit__ method is not a generator and should not yield.
        # Instead, handle exceptions by logging them if present.
        PROPOGATE = False
        SUPPRESS = True

        match exc_type:
            case None:
                return PROPOGATE
            case capstone.CsError:
                logger.error(f"Capstone disassembly error: {exc_val}", exc_info=True)
                return SUPPRESS
            case _:
                logger.error(
                    f"Unexpected {exc_type.__name__} during Capstone disassembly: {exc_val}",
                    exc_info=True,
                )
                return SUPPRESS

    def disasm(self, buf: bytes, start_ea: int, **kwargs):
        """
        Disassemble the buffer and return an iterator of instructions.
        If initialization failed, returns an empty iterator.
        start_ea = 0 means is relative to the start of the input buffer 'buf'
        """
        if not self.md:
            yield from self._EMPTY_SET
            return

        for insn in self.md.disasm(buf, start_ea, **kwargs):
            yield insn


@dataclasses.dataclass
class BasicDecodedInstruction:
    """Holds standardized information about a decoded instruction."""

    address: int
    size: int
    is_jump: bool = False
    jump_target: typing.Optional[int] = None
    is_nop: bool = False
    dead_opaque_predicate: bool = False


class InstructionDecoder(typing.Protocol):
    """Protocol defining the expected signature for decoder functions."""

    def __init__(self, is_x64: bool): ...

    def decode(
        self, ea: int, mem_bytes_at_ea: bytes
    ) -> typing.Optional[BasicDecodedInstruction]:
        """
        Decodes the instruction at virtual address 'ea' using the provided memory bytes.

        Args:
            ea: The virtual address of the instruction to decode.
            mem_bytes_at_ea: A bytes object containing memory starting from 'ea'.
                             The implementation should only consume the bytes
                             needed for the single instruction at 'ea'.

        Returns:
            An InstructionInfo object if decoding is successful, otherwise None.
        """
        ...


class CapstoneInstructionDecoder(InstructionDecoder):

    def __init__(self, is_x64: bool):
        self.is_x64 = is_x64
        self.md = capstone.Cs(
            capstone.CS_ARCH_X86, capstone.CS_MODE_64 if is_x64 else capstone.CS_MODE_32
        )
        self.md.detail = True

    def get_next_insn(
        self, mem_bytes_at_ea: bytes, ea: int
    ) -> typing.Optional[capstone.CsInsn]:
        try:
            # Use list comprehension and next to get the first instruction or None
            insn = next(self.md.disasm(mem_bytes_at_ea, ea, count=1), None)
        except capstone.CsError as e:
            logger.error(f"Capstone decoding error at 0x{ea:X}: {e}")
            return None
        if insn is None:
            return None
        logger.debug(
            "Decoded instruction: %s %s (%X bytes) at offset %s - bytes: %s",
            insn.mnemonic,
            insn.op_str,
            insn.size,
            hex(ea),
            insn.bytes.hex(),
        )
        return insn

    def decode(
        self, ea: int, mem_bytes_at_ea: bytes
    ) -> typing.Optional[BasicDecodedInstruction]:
        """
        Decodes instruction at ea using IDA's disassembler.
        Ignores mem_bytes_at_ea, uses IDA's database.
        Conforms to DecoderProtocol.
        """
        # Decode using Capstone
        insn = self.get_next_insn(mem_bytes_at_ea, ea)
        if insn is None:
            return None

        decoded = BasicDecodedInstruction(address=ea, size=insn.size)
        if insn.id == capstone.x86.X86_INS_NOP:
            decoded.is_nop = True
        # Check for 'xchg r8, r8' as a NOP pattern (0x90 is 'nop', i.e. 0x87 C9 is 'xchg cl, cl')
        elif insn.id in (
            capstone.x86.X86_INS_XCHG,
            capstone.x86.X86_INS_MOV,
            capstone.x86.X86_GRP_CMOV,
        ):
            op1, op2 = insn.operands
            if op1.type == op2.type and op1.size == op2.size and op1.reg == op2.reg:
                decoded.is_nop = True
        elif insn.id == capstone.x86.X86_INS_PUSH and len(insn.operands) > 0:
            # we have encountered this dead code:
            # .text:0000000180188FB2 50                                                  push    rax
            # .text:0000000180188FB3 EB FF                                               jmp     short near ptr loc_180188FB3+1
            # .text:0000000180188FB5 C0 58 ? ?                                           rcr     byte ptr [rax-?], ?
            if insn.operands[0].reg == capstone.x86.X86_REG_RAX:
                next_insn = self.get_next_insn(
                    mem_bytes_at_ea, insn.address + insn.size
                )
                if next_insn is not None and self._is_self_recursive_jump(insn):
                    next_next_insn = self.get_next_insn(
                        mem_bytes_at_ea, next_insn.address + next_insn.size
                    )
                    if next_next_insn is not None and next_next_insn.bytes.startswith(
                        b"\xc0\x58"
                    ):
                        decoded.dead_opaque_predicate = True
                        decoded.size = insn.size + next_insn.size + 2
                    return decoded
                else:
                    decoded.is_nop = True
                    return decoded
        elif capstone.CS_GRP_JUMP in insn.groups:
            if (
                len(insn.operands) > 0
                and insn.operands[0].type == capstone.x86.X86_OP_IMM
            ):
                decoded.is_jump = True
                decoded.jump_target = insn.operands[0].imm
        return decoded

    def _is_self_recursive_jump(self, insn: capstone.CsInsn) -> bool:
        """
        Heuristic detection of self-recursive jumps for Capstone.

        Args:
            insn: Capstone instruction object

        Returns:
            True if this appears to be a self-recursive jump
        """
        jump_source = insn.address
        # Pattern detection for common dead opaque predicates
        # EB FF - jump back 1 byte (into same instruction)
        if (
            insn.id == capstone.x86.X86_INS_JMP
            and len(insn.bytes) == 2
            and insn.bytes[0] == 0xEB
            and insn.bytes[1] == 0xFF
        ):
            logger.debug(f"Self-recursive jump detected: EB FF at 0x{jump_source:X}")
            return True

        return False


@dataclasses.dataclass
class JumpTargetAnalyzer:
    # Input parameters for processing jumps.
    match_bytes: bytes  # The bytes in which we're matching jump instructions.
    match_start: int  # The address where match_bytes starts.
    block_end: int  # End address of the allowed region.
    start_ea: int  # Base address of the memory block (used for bounds checking).

    # Internal structures.
    jump_targets: collections.Counter = dataclasses.field(
        init=False, default_factory=collections.Counter
    )
    jump_details: list = dataclasses.field(
        init=False, default_factory=list
    )  # List of (jump_ea, final_target, stage1_type)
    target_type: dict = dataclasses.field(
        init=False, default_factory=dict
    )  # final_target -> stage1_type

    def follow_jump_chain(
        self,
        mem: bytes,
        current_ea: int,
        match_end: int,
        decoder: InstructionDecoder,
        visited: set[int] | None = None,
        depth: int = 0,
    ) -> typing.Optional[int]:
        """
        Follow a chain of 2-byte jumps starting from current_ea using the provided decoder.

        Args:
            mem: Memory object containing the relevant byte data. Its 'base' attribute
                 defines the absolute address corresponding to the start of its buffer.
            current_ea: The absolute starting virtual address for tracing.
            match_end: The absolute end address (exclusive) of the 'stage1' area.
            decoder: A function conforming to DecoderProtocol used for disassembly.
            visited: Set of visited addresses to prevent loops (internal use).
            depth: Recursion depth for logging (internal use).

        Returns:
            The absolute virtual address where the jump chain ends, or None.
        """
        indent = "  " * depth + "|_ "
        if visited is None:
            visited = set()

        # Get an efficient view of the memory buffer
        mem_view = mem
        mem_start_ea = self.start_ea  # Absolute start address of the buffer
        mem_len = len(mem_view)
        mem_end_ea = mem_start_ea + mem_len  # Absolute end address (exclusive)

        if current_ea in visited:
            logger.debug(
                "%sJump chain stopped: Already visited 0x%X", indent, current_ea
            )
            return None
        # Check if start address is within the bounds defined by the Memory object
        if not (mem_start_ea <= current_ea < mem_end_ea):
            logger.debug(
                "%sJump chain stopped: Start address 0x%X is outside Memory bounds [0x%X, 0x%X)",
                indent,
                current_ea,
                mem_start_ea,
                mem_end_ea,
            )
            return None

        visited.add(current_ea)

        trace_ea = current_ea
        while True:
            # Check if the current tracing address is still within the Memory bounds
            if not (mem_start_ea <= trace_ea < mem_end_ea):
                logger.debug(
                    "%sStopping trace: Address 0x%X is outside Memory bounds [0x%X, 0x%X). Returning last valid start: 0x%X",
                    indent,
                    trace_ea,
                    mem_start_ea,
                    mem_end_ea,
                    current_ea,
                )
                return current_ea  # Return the start address of the sequence that led out of bounds

            decoded_insn = None
            # Calculate offset relative to the start of the Memory object's buffer
            offset = trace_ea - mem_start_ea
            logger.debug("%soffset: %X", indent, offset)
            # We already know offset is >= 0 because trace_ea >= mem_start_ea
            # We need to ensure we have enough bytes left for *potential* instructions

            # Get bytes starting from the offset using the memoryview slice
            # Convert the slice to bytes for the decoder interface
            bytes_for_decoder = mem_view[offset:]
            if (
                not bytes_for_decoder
            ):  # Should not happen if bounds check is correct, but defensive check
                logger.warning(
                    "%sNo bytes available for decoding at offset %X (address 0x%X). Stopping trace.",
                    indent,
                    offset,
                    trace_ea,
                )
                return current_ea

            try:
                # Call the passed-in decoder function
                decoded_insn = decoder.decode(trace_ea, bytes_for_decoder)
            except Exception as e:
                logger.error(
                    "%sDecoder function raised exception at 0x%X: %s",
                    indent,
                    trace_ea,
                    e,
                )
                decoded_insn = None  # Treat as decode failure

            # If decoding failed or decoder returned None
            if not decoded_insn:
                logger.debug(
                    "%sFailed to decode instruction at 0x%X. Stopping trace. Returning start: 0x%X",
                    indent,
                    trace_ea,
                    current_ea,
                )
                return current_ea  # Return start of the sequence

            # --- Process the decoded instruction ---
            if decoded_insn.is_nop:
                logger.debug(
                    "%sNOP found at 0x%X (size %X). Skipping.",
                    indent,
                    trace_ea,
                    decoded_insn.size,
                )
                trace_ea += decoded_insn.size
                continue  # Continue the while loop to the next instruction

            if decoded_insn.dead_opaque_predicate:
                logger.debug(
                    "%sDead opaque predicate found at 0x%X (size %X). Returning start: 0x%X.",
                    indent,
                    trace_ea,
                    decoded_insn.size,
                    current_ea,
                )
                return current_ea + decoded_insn.size

            if not decoded_insn.is_jump or decoded_insn.size != 2:
                logger.debug(
                    "%sChain stopped at 0x%X: Instruction is not a 2-byte jump. Returning start: 0x%X",
                    indent,
                    trace_ea,
                    current_ea,
                )
                return current_ea  # Return the start address of the sequence that ended

            # --- We have a 2-byte jump ---
            target = decoded_insn.jump_target  # This is an absolute address
            if target is not None:
                logger.debug(
                    "%s  -> Found 2-byte jump at 0x%X targeting 0x%X",
                    indent,
                    trace_ea,
                    target,
                )

                # --- Decide action based on the jump target (using absolute addresses) ---
                # 1. Target is within the 'followable' range [match_start, match_end )
                if self.match_start <= target < match_end:
                    logger.debug(
                        "%sFollowing jump from 0x%X to 0x%X (recursive call)",
                        indent,
                        trace_ea,
                        target,
                    )
                    # Pass the same Memory object and decoder down recursively
                    return self.follow_jump_chain(
                        mem, target, match_end, decoder, visited, depth + 1
                    )

                # 3. Target is within the overall Memory block, but *before* match_start.
                elif mem_start_ea <= target < self.match_start:
                    logger.debug(
                        "%sJump chain ends: Target 0x%X is within Memory bounds [0x%X,0x%X) but outside followable range [0x%X, 0x%X). Returning target.",
                        indent,
                        target,
                        mem_start_ea,
                        mem_end_ea,
                        self.match_start,
                        match_end,
                    )
                    if depth == 0:  # this is a bs jump, ignore it.
                        return None
                    return target  # Return the target address itself

            # 4. Target is out of the overall Memory bounds or otherwise unexpected.
            logger.debug(
                "%sJump chain stopped: Target 0x%X is outside allowed ranges. Returning start address 0x%X",
                indent,
                target,
                current_ea,
            )
            if depth == 0:  # this is a bs jump, ignore it.
                return None
            return current_ea  # Return the start address of the sequence containing the invalid jump

    def _decode_stream(self, decoder, start, match_bytes):
        offset = 0
        n = len(match_bytes)

        while offset < n:
            try:
                # hand the decoder only the bytes we haven‚Äôt consumed yet
                insn = decoder.decode(start + offset, match_bytes[offset:])
            except Exception as e:
                logger.error("Decode error @0x%X: %s", start + offset, e)
                return

            if not insn:
                return

            yield insn
            offset += insn.size

    def process(self, mem, chain, is_x64: bool):
        """
        Process each jump match in match_bytes.
        'chain' is expected to have attributes:
          - junk_length: int
          - stage1_type: SegmentType
        """
        decoder = CapstoneInstructionDecoder(is_x64)
        match_end = chain.overall_start() + MAX_PATTERN_LEN
        logger.debug(
            "Processing jumps for chain @ 0x%X, match_end=0x%X",
            chain.overall_start(),
            match_end,
        )
        match chain.stage1_type:
            case SegmentType.STAGE1_SINGLE:
                jump_offset = chain.segments[0].length - (
                    len(chain.segments[0].matched_groups["jump"]) // 2
                )
                jump_ea = self.match_start + jump_offset
            case SegmentType.STAGE1_MULTIPLE:
                jump_offset = 0
                jump_ea = self.match_start + jump_offset
            case _:
                raise ValueError(f"Invalid stage1_type: {chain.stage1_type}")

        final_target = self.follow_jump_chain(mem, jump_ea, match_end, decoder)
        if not final_target:
            logger.debug(
                "  Skipping jump at 0x%X: Invalid final target 0x%X",
                jump_ea,
                final_target if final_target else 0,
            )
        else:
            self.jump_targets[final_target] += 1
            if final_target not in self.target_type:
                self.target_type[final_target] = chain.stage1_type
            self.jump_details.append((jump_ea, final_target, chain.stage1_type))
            logger.debug("Found jump @0x%X ‚Üí 0x%X", jump_ea, final_target)
        return self
        # for insn in self._decode_stream(
        #     decoder, chain.overall_start(), self.match_bytes
        # ):
        #     # 1) filter out non jumps or non 2-byte jumps
        #     if not insn.is_jump or insn.size != 2:
        #         continue

        #     final_target = self.follow_jump_chain(mem, insn.address, match_end, decoder)
        #     if not final_target:
        #         logger.debug("Bad target @0x%X", insn.address)
        #         continue

        #     if abs(final_target - match_end) > 6:
        #         logger.debug("Out of range @0x%X", insn.address)
        #         continue

        #     # 2) process the hit
        #     self.jump_targets[final_target] += 1
        #     if final_target not in self.target_type:
        #         self.target_type[final_target] = chain.stage1_type
        #     self.jump_details.append((insn.address, final_target, chain.stage1_type))
        #     logger.debug("Found jump @0x%X ‚Üí 0x%X", insn.address, final_target)

        return self

    def __iter__(self):
        """
        Iterate over the most likely targets.
        For each candidate, if a jump exists whose starting address equals candidate + 1,
        yield its final target instead.

        Sorting is by count descending, then by final_target descending.
        """
        # Prepare a list of (final_target, count) tuples
        results = list(self.jump_targets.items())
        # Sort by count descending, then by final_target descending
        results.sort(key=lambda x: (x[1], x[0]), reverse=True)
        for candidate, count in results:
            final_candidate = candidate
            for jump_ea, target, stype in self.jump_details:
                if jump_ea == candidate + 1:
                    final_candidate = target
                    break
            yield final_candidate


def analyze_chain(
    chain: MatchChain,
    mem: memoryview,
    start_ea: int,
    is_x64: bool,
    max_size: int = MAX_PATTERN_LEN,
) -> list[Range]:
    """
    Filter out false positive anti-disassembly patterns and handle overlaps.
    Integrates with existing big instruction detection code.

    Args:
        chains: List of MatchChain objects
        mem: Memory object containing binary data
        start_ea: Starting effective address
        max_size: Maximum valid size for an anti-disassembly routine (default: MAX_PATTERN_LEN)

    Returns:
        A single validated MatchChain object
    """

    # Find the big instruction
    match_start = chain.overall_start()
    chain_end = match_start + max_size
    ranges = []

    logger.info(f"Analyzing match: {chain.description} @ 0x{match_start:X}")

    # Determine possible jump targets - using your existing code
    jump_targets = JumpTargetAnalyzer(
        chain.overall_matched_bytes(), match_start, chain_end, start_ea
    ).process(mem=mem, chain=chain, is_x64=is_x64)

    for target in jump_targets:
        if target <= match_start:  # sanity-check
            continue
        logger.info(f"most_likely_target: 0x{target:X}, block_end: 0x{chain_end:X}")
        ranges.append(Range(match_start, target))
    return ranges

```

`tests/integration/anti_deob/worker_main.py`:

```py
"""Worker entry point for anti-deobfuscation tasks."""

import argparse
import asyncio
import concurrent.futures
import dataclasses
import multiprocessing.shared_memory
import pathlib
import sys
import threading
import time

from anti_deob.deobfuscator import MAX_PATTERN_LEN, analyze_chain, stage1_find_patterns

from ida_taskr import (
    ConnectionContext,
    DataProcessorCore,
    MessageEmitter,
    WorkerBase,
    get_logger,
)
from ida_taskr.utils import (
    AsyncEventEmitter,
    IntervalSet,
    PatchManager,
    execute_chunk_with_shm_view,
    log_execution_time,
    make_chunks,
    resolve_overlaps,
    shm_buffer,
)

logger = get_logger(__name__)

# Path to this worker script (for self-reference)
WORKER_SCRIPT_PATH = pathlib.Path(__file__)


def core_deob_logic_for_chunk(
    chunk_mv: memoryview, base_ea_for_chunk_mv: int, is_64: bool
):
    """
    Core deobfuscation logic for a given chunk (memoryview).
    This function receives an already prepared memoryview of a padded chunk.
    Args:
        chunk_mv: Memoryview of the (padded) chunk data from shared memory.
        base_ea_for_chunk_mv: The effective base EA for the start of chunk_mv.
        is_64: Boolean indicating if the architecture is 64-bit.
    Returns:
        A list of Range objects found within this chunk.
    """
    # Note: The original process_chunk filtered results based on core_start/core_end.
    # This responsibility can either be moved to the caller of this core logic
    # or handled here if core_start_offset_in_chunk_mv and core_end_offset_in_chunk_mv are passed.
    # For this refactoring, we assume all ranges found in the chunk_mv are returned,
    # and any filtering by original core region happens in the caller if still needed.

    found_ranges = []

    # ‚Äî Stage 1: Find patterns in the provided chunk_mv
    # The base address for stage1_find_patterns should be the effective EA of chunk_mv
    s1_chains = stage1_find_patterns(chunk_mv, base_ea_for_chunk_mv)

    # --- TRACEPOINT (adjusted for new base_ea_for_chunk_mv) ---
    TARGET_EA = 0x140005131
    for chain in s1_chains:
        # chain.overall_start() is relative to chain.base_address, which is base_ea_for_chunk_mv
        s = chain.overall_start()  # This is an absolute EA
        e = s + chain.overall_length()
        if s <= TARGET_EA < e:
            logger.warning(
                "[TRACE][Stage1] (core logic) covers 0x%X: %s", TARGET_EA, chain
            )
            for seg in chain.segments:
                # seg.start is relative to chain.base_address
                abs_s = chain.base_address + seg.start
                logger.warning(
                    "    seg @0x%X len=%d desc=%s",
                    abs_s,
                    seg.length,
                    seg.description,
                )
            break
    # -----------------------------------------------------------

    for chain in s1_chains:
        # analyze_chain also needs the chunk_mv and its corresponding base EA
        ranges = analyze_chain(chain, chunk_mv, base_ea_for_chunk_mv, is_64)
        found_ranges.extend(ranges)

    return found_ranges


def dispatch_chunk_processing(args):
    """
    Dispatcher function called by ProcessPoolExecutor.
    Unpacks arguments, calls execute_chunk_with_shm_view with the core logic.
    """
    shm_name, padded_start, padded_end, core_start, core_end, base_ea, is_64 = args

    # Arguments for core_deob_logic_for_chunk, after memoryview is prepared by the wrapper:
    # The base_ea for the chunk_mv will be the original base_ea + padded_start offset.
    core_logic_user_args = (base_ea + padded_start, is_64)

    # Call the wrapper that handles SHM and memoryview lifecycle
    results = execute_chunk_with_shm_view(
        core_deob_logic_for_chunk,
        shm_name,
        padded_start,
        padded_end,
        *core_logic_user_args,
    )

    # If filtering by core region is still desired (as in original process_chunk):
    # This needs core_start and core_end to be relative to the original buffer,
    # and the ranges in `results` to have absolute EAs.
    # The `analyze_chain` and `stage1_find_patterns` should produce absolute EAs if their
    # base_ea parameter is an absolute EA.
    # Assuming `r.start` from `analyze_chain` gives an absolute EA:
    final_results_in_core = []
    # Convert core_start/core_end (offsets from buffer start) to absolute EAs
    abs_core_start = base_ea + core_start
    abs_core_end = base_ea + core_end
    for r in results:
        if abs_core_start <= r.start < abs_core_end:
            final_results_in_core.append(r)
    # return final_results_in_core
    return results  # For now, returning all results from the chunk, filtering can be re-evaluated.
    # The original process_chunk did this filtering. If it's crucial, we should keep it.
    # Given the original comment: "Returns only those chains whose start is in the chunk's core region."
    # We should probably keep this filtering.

    # Re-instating the original filtering logic:
    # Ensure `r.start` in `results` are absolute EAs for this comparison to work.
    # `chain.overall_start()` in `core_deob_logic_for_chunk` is absolute.
    # `analyze_chain` if it returns `Range` objects, their `start` should also be absolute EA.
    # If `Range.start` is an offset from `base_ea_for_chunk_mv`, it needs adjustment before this check.
    # Assuming `Range.start` is absolute EA:
    filtered_results = []
    abs_core_start_ea = base_ea + core_start
    abs_core_end_ea = base_ea + core_end
    for r_obj in results:  # Assuming results is a list of Range objects
        if (
            hasattr(r_obj, "start")
            and abs_core_start_ea <= r_obj.start < abs_core_end_ea
        ):
            filtered_results.append(r_obj)
        elif (
            isinstance(r_obj, tuple) and len(r_obj) == 2
        ):  # If it's (start_ea, end_ea) tuple
            if abs_core_start_ea <= r_obj[0] < abs_core_end_ea:
                filtered_results.append(r_obj)
        # Add other checks if result structure varies
    return filtered_results


@dataclasses.dataclass
class AsyncDeobfuscator(AsyncEventEmitter):
    shm_name: str
    data_size: int
    start_ea: int
    is_64bit: bool
    max_workers: int = 0
    executor: concurrent.futures.Executor | None = None

    def __post_init__(self):
        super().__post_init__()
        self.pause_evt = asyncio.Event()
        self.stop_evt = asyncio.Event()
        self.max_workers = self.max_workers or max(1, multiprocessing.cpu_count())
        ctx = multiprocessing.get_context("spawn")
        self.executor = self.executor or concurrent.futures.ProcessPoolExecutor(
            max_workers=self.max_workers, mp_context=ctx
        )
        logger.info(f"executor pool created with {self.max_workers} workers")

    @log_execution_time
    async def run(self):
        await self.emit("run_started")

        # 1) define exactly max_workers chunks over the shared buffer
        buf_len = self.data_size
        chunks = list(
            make_chunks(
                buf_len,
                self.max_workers,
                max_pat=MAX_PATTERN_LEN * 2,
            )
        )
        logger.debug(
            "Splitting buffer of %d bytes into %d chunks:", buf_len, len(chunks)
        )
        for idx, (ps, pe, cs, ce) in enumerate(chunks):
            logger.info(
                "  chunk %2d: core=[0x%X..0x%X) padded=[0x%X..0x%X)",
                idx,
                cs + self.start_ea,
                ce + self.start_ea,
                ps + self.start_ea,
                pe + self.start_ea,
            )

        # 2) fire one full-pipeline task per chunk
        loop = asyncio.get_running_loop()
        jobs = [
            (
                self.shm_name,
                padded_start,
                padded_end,
                core_start,
                core_end,
                self.start_ea,
                self.is_64bit,
            )
            for padded_start, padded_end, core_start, core_end in chunks
        ]
        futures = [
            loop.run_in_executor(self.executor, dispatch_chunk_processing, job)
            for job in jobs
        ]

        # 3) wait, flatten, resolve overlaps globally
        per_chunk = await asyncio.gather(*futures)
        all_ranges = [r for grp in per_chunk for r in grp]
        final: IntervalSet = resolve_overlaps(all_ranges)

        await self.emit("run_finished", final)
        return final

    async def shutdown(self):
        self.stop_evt.set()
        if self.executor:
            self.executor.shutdown(wait=True)
        await self.emit("stopped")
        self.logger.info("AsyncDeobfuscator shutdown complete.")


class AntiDeobWorker(WorkerBase):
    """Worker implementation for anti-deobfuscation tasks."""

    def __init__(self, shm_name: str, data_size: int, start_ea: int, is_64bit: bool):
        # Pass AsyncDeobfuscator class and its arguments to WorkerBase
        emitter_args = {
            "shm_name": shm_name,
            "data_size": data_size,
            "start_ea": start_ea,
            "is_64bit": is_64bit,
            # process_chunk_fn will be implicitly passed if AsyncDeobfuscator expects it
        }
        # Note: process_chunk is used by AsyncDeobfuscator's run method,
        # so it doesn't need to be passed to WorkerBase directly here if AsyncDeobfuscator handles it.
        # If AsyncDeobfuscator needed process_chunk to be injected, that would be part of its own init.
        super().__init__(
            async_emitter_class=AsyncDeobfuscator, emitter_args=emitter_args
        )

        # Specific attributes for AntiDeobWorker, if any, can be initialized here.
        # For this example, the core logic is within AsyncDeobfuscator.
        self.logger.info("AntiDeobWorker initialized with new WorkerBase structure.")

    # The `setup` method in WorkerBase now initializes the emitter.
    # We can override `setup_custom_event_handlers` if we need specific event handling
    # beyond what WorkerBase provides by default.

    # The `process` method is now largely handled by WorkerBase.
    # We might not need to override it unless there's very specific pre/post command loop logic.

    # `handle_command` can be extended if new custom commands are needed beyond
    # what WorkerBase handles (start, stop, pause, resume, ping, set_log_level).
    # For this example, AsyncDeobfuscator doesn't introduce new commands that
    # WorkerBase needs to be aware of at this level. The existing commands
    # control the WorkerController managed by WorkerBase.

    # `cleanup` is also handled by WorkerBase to shut down the emitter.

    # Example of custom event handler setup, if needed:
    # def setup_custom_event_handlers(self):
    #     super().setup_custom_event_handlers() # Good practice if base class might add some
    #     if self.emitter_instance: # emitter_instance is set up in WorkerBase.setup()
    #         @self.emitter_instance.on("some_custom_event_from_async_deobfuscator")
    #         def on_my_custom_event(data):
    #             self.logger.info(f"Received custom event with data: {data}")
    #             if self.conn:
    #                 self.conn.send_message("custom_progress", data, status="custom_status")

    # If the results formatting in WorkerBase's send_results is not sufficient,
    # it can be overridden here. The default handles IntervalSet and lists.
    # def send_results(self, connection: ConnectionContext, results_data):
    #    self.logger.info("AntiDeobWorker formatting results...")
    #    # Custom formatting logic here
    #    super().send_results(connection, formatted_results_data)


def create_anti_deob_message_emitter(dry_run: bool = False) -> MessageEmitter:
    """Create a message emitter for anti-deobfuscation results.

    Args:
        dry_run: If True, patches will be simulated but not applied

    Returns:
        MessageEmitter configured with anti-deob event handlers
    """

    emitter = MessageEmitter()

    @emitter.on("worker_connected")
    def on_worker_connected():
        logger.info("Worker connected")

    @emitter.on("worker_message")
    def on_worker_message(message: dict):
        logger.info("Worker message: %s", message)

    @emitter.on("worker_error")
    def on_worker_error(error: str):
        logger.error("Worker error: %s", error)

    @emitter.on("worker_disconnected")
    def on_worker_disconnected():
        logger.info("Worker disconnected")

    @emitter.on("worker_results")
    def on_worker_results(results: dict):
        patch_manager = PatchManager(dry_run=dry_run)
        if results["status"] == "success":
            NOP = b"\x90"
            for patch_instructions in results["results"]:
                patch_manager.add_patch(
                    patch_instructions["address"],
                    patch_instructions["length"] * NOP,
                )
            patch_manager.apply_all()
        else:
            logger.error("Worker reported an error in results")

    return emitter


class Taskr:
    """
    Singleton wrapper for the DataProcessor instance.

    Ensures only one DataProcessor is created and shared throughout the plugin's lifetime.

    Usage:
        >>> t1 = Taskr()
        >>> t2 = Taskr()
        >>> t1 is t2
        True
        >>> t1.get() is t2.get()
        True

    The .get() method returns the singleton DataProcessor instance.
    """

    _instance = None
    _processor = None

    def __new__(cls, *args, **kwargs):
        if cls._instance is None:
            # Not thread-safe, but sufficient for plugin/IDA context
            cls._instance = super().__new__(cls)
            logger.info("Initializing DataProcessor")
            # Create patch manager and message emitter for singleton
            message_emitter = create_anti_deob_message_emitter(dry_run=False)
            cls._processor = DataProcessorCore(message_emitter)
        return cls._instance

    def get(self):
        """
        Returns the singleton DataProcessor instance.

        >>> t1 = Taskr()
        >>> t2 = Taskr()
        >>> t1.get() is t2.get()
        True
        """
        return self._processor

    def pause(self):
        self.get().proc.send_command({"command": "pause"})  # type: ignore

    def resume(self):
        self.get().proc.send_command({"command": "resume"})  # type: ignore

    def stop(self):
        self.get().proc.send_command({"command": "stop"})  # type: ignore

    def start(self):
        self.get().proc.send_command({"command": "start"})  # type: ignore

    def terminate(self):
        self.get().terminate()  # type: ignore

    def ping(self):
        self.get().proc.send_command({"command": "ping"})  # type: ignore


def main():
    """Worker entry point."""
    parser = argparse.ArgumentParser()
    parser.add_argument("--shm_name", required=True)
    parser.add_argument("--data_size", type=int, required=True)
    parser.add_argument("--start_ea", type=lambda x: int(x, 0), required=True)
    parser.add_argument("--is64", type=int, default=1)
    parser.add_argument("--address", required=True)
    parser.add_argument("--authkey", required=True)

    args = parser.parse_args()
    worker = None
    try:
        # Create worker instance
        worker = AntiDeobWorker(
            shm_name=args.shm_name,
            data_size=args.data_size,
            start_ea=args.start_ea,
            is_64bit=bool(args.is64),
        )

        # WorkerBase's setup will initialize AsyncDeobfuscator
        worker.setup()  # This is crucial

        # Process with connection
        # WorkerBase's process method now handles the command loop
        with ConnectionContext(args.address, args.authkey) as conn:
            worker.process(conn)  # This starts the command loop in WorkerBase

    except Exception as e:
        logger.error("Unhandled exception in worker_main: %s", e, exc_info=True)
    finally:
        if worker:
            # WorkerBase's cleanup should handle shutting down the emitter
            # It's an async method, so needs to be run in an event loop
            try:
                asyncio.run(worker.cleanup())
            except Exception as e:
                logger.error("Exception during worker cleanup: %s", e, exc_info=True)
        logger.info("Worker main finished.")


if __name__ == "__main__":
    # If run with worker arguments, run as worker
    if len(sys.argv) > 1 and any(arg.startswith("--") for arg in sys.argv[1:]):
        main()
    else:
        # If run directly, run as IDA script
        print("Running Taskr().get().run(*DataProcessorCore.get_section_data('.text'))")
        data_ea, data_bytes = DataProcessorCore.get_section_data(".text")
        if data_ea and data_bytes:
            processor = Taskr().get()
            if processor:
                processor.run(data_ea, data_bytes, str(WORKER_SCRIPT_PATH))

```

`tests/integration/conftest.py`:

```py
"""Pytest configuration for integration tests."""

import os
import sys
from pathlib import Path

# IMPORTANT: Import Qt BEFORE idapro to avoid "PySide6 can only be used from GUI" error
# idapro sets up an import hook that blocks PySide6 if imported after
try:
    from PySide6 import QtCore
except ImportError:
    pass

import pytest

# Add tests/integration to path so anti_deob module can be imported
tests_integration_dir = Path(__file__).parent
if str(tests_integration_dir) not in sys.path:
    sys.path.insert(0, str(tests_integration_dir))

```

`tests/integration/test_anti_deob.py`:

```py
import concurrent.futures
import functools
import json
import logging
import multiprocessing
import multiprocessing.shared_memory
import os
import pathlib
import sys

import pytest

# Only import idapro if we're not running inside IDA already
if not any(sys.executable.endswith(x) for x in ["ida.exe", "ida64.exe", "idaq.exe", "idaq64.exe"]):
    try:
        import idapro  # isort: ignore
    except ImportError:
        # Skip this entire test module if idapro is not available
        pytest.skip("idapro module not available - skipping anti_deob tests", allow_module_level=True)

from ida_taskr import helpers

from anti_deob.worker_main import AsyncDeobfuscator  # isort:skip


logfmt = "%(levelname)s %(name)s: %(message)s"
debug_configure_logging = functools.partial(
    helpers.configure_logging, level=logging.DEBUG, fmt_str=logfmt
)
logger = logging.getLogger(__name__)

helpers.configure_logging(logger, level=logging.INFO, fmt_str=logfmt)


def get_debug_logger(name=None):
    """Get a configured logger instance."""
    prefix = "ida." if helpers.is_ida() else "worker."
    name = name or f"{prefix}{__name__}"
    logger = logging.getLogger(name)
    helpers.configure_logging(logger, level=logging.DEBUG, fmt_str=logfmt)
    return logger


helpers.get_logger = get_debug_logger

# Load raw binary data once
script_dir = pathlib.Path(__file__).parent
bin_file = script_dir / "11.1.0.60228.json"
try:
    with bin_file.open("r") as f:
        ROUTINE_DATA = json.load(f)
except FileNotFoundError:
    logger.error("Routine data file not found: %s", bin_file)
    ROUTINE_DATA = {}
except json.JSONDecodeError:
    logger.error("Error decoding JSON from file: %s", bin_file)
    ROUTINE_DATA = {}


def parse_address(addr_str):
    """Parse address string as either hex (0x...) or decimal."""
    if not addr_str:
        return None

    addr_str = addr_str.strip()
    try:
        if addr_str.lower().startswith("0x"):
            # Parse as hex
            return int(addr_str, 16)
        else:
            # Parse as decimal
            return int(addr_str, 10)
    except ValueError:
        logger.error(
            "Invalid address format: '%s'. Must be decimal or hex (0x...)", addr_str
        )
        return None


def load_and_generate_tests(cls):
    """Dynamically add test methods to the class for each routine address."""

    # Check for environment variable to filter tests
    filter_addr_str = os.environ.get("TEST_ROUTINE_ADDR")
    filter_addr = None
    if filter_addr_str:
        filter_addr = parse_address(filter_addr_str)
        if filter_addr is not None:
            logger.info(
                "Filtering tests to run only for address: %s (0x%X)",
                filter_addr_str,
                filter_addr,
            )
        else:
            logger.error("Invalid TEST_ROUTINE_ADDR value: '%s'", filter_addr_str)

    # Store routine data on the class for access in setUp
    cls.json_data = ROUTINE_DATA

    # Find original test methods (those starting with 'test_')
    original_test_methods = {
        name: getattr(cls, name)
        for name in dir(cls)
        if name.startswith("test_") and callable(getattr(cls, name))
    }

    # Remove original test methods to avoid running them directly
    for name in original_test_methods:
        delattr(cls, name)

    # Create new test methods for each routine address
    for addr_str in ROUTINE_DATA:
        routine_addr = int(addr_str)

        # Apply filtering if the environment variable is set
        if filter_addr is not None and routine_addr != filter_addr:
            continue

        addr_hex = hex(routine_addr)
        for original_name, original_method in original_test_methods.items():
            # Create a new function that calls the original test method
            # We use a closure to capture the original_method
            def make_test_runner(method_to_run):
                @pytest.mark.asyncio
                async def test_runner(self):
                    await method_to_run(self)

                return test_runner

            new_test_method = make_test_runner(original_method)
            new_test_name = f"{original_name}_{addr_hex}"
            # Copy necessary attributes from the original method
            new_test_method.__name__ = new_test_name
            new_test_method.__doc__ = original_method.__doc__
            setattr(cls, new_test_name, new_test_method)

    return cls


@load_and_generate_tests
class TestAsyncDeobfuscator:
    """
    Test the AsyncDeobfuscator stages and ensure final chains match
    the patch addresses discovered by execute_action.
    """

    # Class attribute to store loaded JSON data
    json_data = None

    def setup_method(self, method):
        # Extract routine address from the test method name (e.g., test_stage1_non_empty_0x141887cbd)
        test_name = method.__name__
        test_id_parts = test_name.split("_")

        addr_str = str(
            int(test_id_parts[-1], 16)
        )  # Get the last part (address) and convert to decimal string key

        if self.json_data is None or addr_str not in self.json_data:
            pytest.fail(f"Address {addr_str} not found in JSON data.")

        # Define EA range based on the parsed address
        routine = self.json_data[addr_str]
        self.start_ea = routine["addr"]
        self.size = routine["size"]
        self.end_ea = self.start_ea + self.size
        self.data = bytes.fromhex(routine["data"])
        logger.debug(
            "Setting up test for routine 0x%X, data length: %d",
            self.start_ea,
            len(self.data),
        )

        # Provide buffer to AsyncDeobfuscator via a fake shared memory context
        # @asynccontextmanager
        # async def fake_get_buffer(inst):
        #     yield self.data
        # AsyncDeobfuscator._get_buffer = fake_get_buffer  # override
        self._shared_memory = multiprocessing.shared_memory.SharedMemory(
            create=True, size=len(self.data)
        )
        self._shared_memory.buf[: len(self.data)] = self.data
        # Instantiate deobfuscator
        self.deob = AsyncDeobfuscator(
            shm_name=self._shared_memory.name,
            data_size=len(self.data),
            start_ea=self.start_ea,
            is_64bit=True,
            max_workers=1,
            executor=concurrent.futures.ThreadPoolExecutor(max_workers=1),
            # executor=concurrent.futures.ProcessPoolExecutor(max_workers=1),
        )

    def teardown_method(self):
        self._shared_memory.close()

        # now tear down the shared memory itself (only the creator should do this)
        try:
            multiprocessing.shared_memory.SharedMemory(
                name=self._shared_memory.name
            ).unlink()
        except FileNotFoundError:
            pass

    async def test_run_matches_expected_addresses(self):
        final = await self.deob.run()
        actual_addresses = sorted(r.start for r in final)
        logger.info(
            "Actual start addresses for 0x%X: %s",
            self.start_ea,
            [hex(a) for a in actual_addresses],
        )

```

`tests/integration/test_integration_qt_core.py`:

```py
"""Integration tests for Qt Core functionality without IDA Pro requirement.

These tests verify that ida-taskr's Qt-based process management and threading
work correctly in headless mode with PyQt5, PyQt6, or PySide6.
"""

import sys
import time
import tempfile
from pathlib import Path

import pytest

# Import from qt_compat to get the unified Signal/Slot API
from ida_taskr.qt_compat import QtCore, Signal, QT_API, QT_AVAILABLE

# Skip all tests if Qt is not available (e.g., IDA headless mode)
pytestmark = pytest.mark.skipif(
    not QT_AVAILABLE,
    reason="Qt is not available (possibly running in IDA headless mode)"
)

# Import Qt Core components (only if Qt is available)
if QT_AVAILABLE:
    QObject = QtCore.QObject
    QThread = QtCore.QThread
    QProcess = QtCore.QProcess
    QProcessEnvironment = QtCore.QProcessEnvironment
else:
    QObject = QThread = QProcess = QProcessEnvironment = None


class TestQtCoreFramework:
    """Test Qt Core framework availability and basic functionality."""

    def test_qt_framework_import(self):
        """Test that Qt framework can be imported in headless mode."""
        # These imports already happened at module level, just verify they work
        assert QObject is not None
        assert QThread is not None
        assert QProcess is not None
        assert Signal is not None

    def test_qprocess_available(self):
        """Test that QProcess is available for process management."""
        # Create a simple QProcess
        process = QProcess()
        assert process is not None
        assert process.state() == QProcess.NotRunning

    def test_qthread_available(self):
        """Test that QThread is available for threading."""
        class TestThread(QThread):
            finished_signal = Signal()

            def run(self):
                self.finished_signal.emit()

        thread = TestThread()
        assert thread is not None
        assert not thread.isRunning()

    def test_signal_slot_mechanism(self):
        """Test Qt signal/slot mechanism in headless mode."""
        class Emitter(QObject):
            test_signal = Signal(str)

        emitter = Emitter()
        received = []

        def handler(msg):
            received.append(msg)

        emitter.test_signal.connect(handler)
        emitter.test_signal.emit("test_message")

        # Signals are delivered synchronously in same thread
        assert len(received) == 1
        assert received[0] == "test_message"


class TestMessageEmitter:
    """Test MessageEmitter with real Qt signals."""

    def test_message_emitter_import(self):
        """Test that MessageEmitter can be imported."""
        from ida_taskr.protocols import MessageEmitter
        assert MessageEmitter is not None

    def test_message_emitter_creation(self):
        """Test MessageEmitter instance creation."""
        from ida_taskr.protocols import MessageEmitter

        emitter = MessageEmitter()
        assert emitter is not None

    def test_message_emitter_signals(self):
        """Test MessageEmitter signal emission and reception."""
        from ida_taskr.protocols import MessageEmitter

        emitter = MessageEmitter()
        received_messages = []

        @emitter.on('worker_message')
        def message_handler(msg):
            received_messages.append(msg)

        emitter.emit_worker_message("test_message")

        assert len(received_messages) == 1
        assert received_messages[0] == "test_message"

    def test_message_emitter_progress(self):
        """Test MessageEmitter progress-like events."""
        from ida_taskr.protocols import MessageEmitter

        emitter = MessageEmitter()
        received = []

        @emitter.on('worker_message')
        def handler(msg):
            received.append(msg)

        emitter.emit_worker_message({"type": "progress", "current": 50, "total": 100})

        assert len(received) == 1
        assert received[0]["type"] == "progress"

    def test_message_emitter_results(self):
        """Test MessageEmitter results signal."""
        from ida_taskr.protocols import MessageEmitter

        emitter = MessageEmitter()
        results = []

        @emitter.on('worker_results')
        def results_handler(result):
            results.append(result)

        emitter.emit_worker_results({"data": "test_result"})

        assert len(results) == 1
        assert results[0] == {"data": "test_result"}


class TestQProcessBasics:
    """Test QProcess basic functionality for worker launching."""

    def test_qprocess_simple_execution(self):
        """Test QProcess can execute a simple command."""
        process = QProcess()

        # Use waitForStarted/waitForFinished instead of signals for simplicity
        process.start("python3", ["-c", "print('hello')"])

        # Wait for process to start
        started = process.waitForStarted(5000)
        assert started, "Process failed to start"

        # Wait for process to finish
        finished = process.waitForFinished(5000)
        assert finished, "Process failed to finish"

        # Check exit status
        assert process.exitStatus() == QProcess.NormalExit
        assert process.exitCode() == 0

    def test_qprocess_output_capture(self):
        """Test QProcess can capture output from subprocess."""
        process = QProcess()
        process.start("python3", ["-c", "print('hello world')"])

        assert process.waitForStarted(5000)
        assert process.waitForFinished(5000)

        output = process.readAllStandardOutput().data().decode('utf-8').strip()
        assert "hello world" in output

    def test_qprocess_error_detection(self):
        """Test QProcess error detection for invalid command."""
        process = QProcess()

        # Try to start a non-existent command
        process.start("nonexistent_command_12345", [])

        # Wait for error
        started = process.waitForStarted(2000)
        assert not started, "Process should not start with invalid command"

        # Check error occurred
        assert process.error() == QProcess.FailedToStart


class TestWorkerLauncher:
    """Test WorkerLauncher functionality with Qt Core."""

    def test_worker_launcher_import(self):
        """Test that WorkerLauncher can be imported."""
        from ida_taskr.launcher import WorkerLauncher
        assert WorkerLauncher is not None

    def test_worker_launcher_qprocess_inheritance(self):
        """Test that WorkerLauncher inherits from QProcess."""
        from ida_taskr.launcher import WorkerLauncher
        assert issubclass(WorkerLauncher, QProcess)

    def test_connection_reader_qthread(self):
        """Test that ConnectionReader is a QThread."""
        from ida_taskr.launcher import ConnectionReader
        assert issubclass(ConnectionReader, QThread)

    def test_qt_listener_qobject(self):
        """Test that QtListener is a QObject."""
        from ida_taskr.launcher import QtListener
        assert issubclass(QtListener, QObject)


class TestTaskRunnerQtIntegration:
    """Test TaskRunner Qt integration in headless mode."""

    def test_taskrunner_import(self):
        """Test that TaskRunner can be imported with Qt available."""
        from ida_taskr import TaskRunner
        assert TaskRunner is not None

    def test_taskrunner_creation_with_qt(self):
        """Test TaskRunner instance creation with Qt framework."""
        from ida_taskr import TaskRunner

        # Create TaskRunner instance with dummy args
        runner = TaskRunner("dummy_script.py", ["arg1"])
        assert runner is not None
        assert hasattr(runner, 'launcher')
        assert hasattr(runner, 'message_emitter')

    def test_taskrunner_message_emitter_type(self):
        """Test that TaskRunner uses MessageEmitter."""
        from ida_taskr import TaskRunner
        from ida_taskr.protocols import MessageEmitter

        runner = TaskRunner("dummy_script.py", ["arg1"])
        assert isinstance(runner.message_emitter, MessageEmitter)

    def test_taskrunner_launcher_type(self):
        """Test that TaskRunner uses WorkerLauncher."""
        from ida_taskr import TaskRunner
        from ida_taskr.launcher import WorkerLauncher

        runner = TaskRunner("dummy_script.py", ["arg1"])
        assert isinstance(runner.launcher, WorkerLauncher)


class TestQProcessEnvironment:
    """Test QProcessEnvironment functionality."""

    def test_process_environment_creation(self):
        """Test QProcessEnvironment can be created."""
        env = QProcessEnvironment.systemEnvironment()
        assert env is not None

    def test_process_environment_variables(self):
        """Test QProcessEnvironment can access environment variables."""
        env = QProcessEnvironment.systemEnvironment()

        # PATH should exist in any environment
        assert env.contains("PATH") or env.contains("Path")

    def test_process_environment_insert(self):
        """Test QProcessEnvironment can insert new variables."""
        env = QProcessEnvironment.systemEnvironment()
        env.insert("TEST_VAR", "test_value")

        assert env.contains("TEST_VAR")
        assert env.value("TEST_VAR") == "test_value"


class TestQtSignalsAdvanced:
    """Test advanced Qt signal/slot patterns used by ida-taskr."""

    def test_cross_thread_signals(self):
        """Test signals can be emitted across thread boundaries."""
        import time

        class Worker(QThread):
            finished_with_data = Signal(str)

            def run(self):
                time.sleep(0.01)  # Small delay to ensure thread starts
                self.finished_with_data.emit("thread_completed")

        worker = Worker()
        received = []

        def handler(msg):
            received.append(msg)

        worker.finished_with_data.connect(handler)
        worker.start()

        # Wait for thread to complete
        worker.wait(5000)

        # Cross-thread signals are queued and need event processing
        # Process pending events to deliver the queued signal
        if hasattr(QtCore, 'QCoreApplication'):
            app = QtCore.QCoreApplication.instance()
            if app is None:
                # Create temporary app if none exists
                app = QtCore.QCoreApplication([])
            # Process all pending events
            app.processEvents()
            time.sleep(0.01)
            app.processEvents()

        assert len(received) == 1
        assert received[0] == "thread_completed"

    def test_multiple_signal_handlers(self):
        """Test multiple handlers can be connected to the same signal."""
        class Emitter(QObject):
            data_ready = Signal(int)

        emitter = Emitter()
        results = {"handler1": [], "handler2": []}

        def handler1(value):
            results["handler1"].append(value)

        def handler2(value):
            results["handler2"].append(value * 2)

        emitter.data_ready.connect(handler1)
        emitter.data_ready.connect(handler2)
        emitter.data_ready.emit(42)

        assert results["handler1"] == [42]
        assert results["handler2"] == [84]

    def test_signal_disconnection(self):
        """Test signal handlers can be disconnected."""
        class Emitter(QObject):
            data_signal = Signal(str)

        emitter = Emitter()
        received = []

        def handler(msg):
            received.append(msg)

        emitter.data_signal.connect(handler)
        emitter.data_signal.emit("first")

        emitter.data_signal.disconnect(handler)
        emitter.data_signal.emit("second")

        # Only first message should be received
        assert len(received) == 1
        assert received[0] == "first"

```

`tests/run_ida_tests.py`:

```py
#!/usr/bin/env python3
"""
Run ida-taskr tests inside IDA Pro.

This script runs pytest tests that require IDA Pro's Qt application.
Some tests are marked with @pytest.mark.skipif(not is_ida()) and will
only run when executed inside IDA Pro.

Usage from IDA Pro:
    1. Via IDA's Python console:
       >>> exec(open('tests/run_ida_tests.py').read())

    2. Via IDAPython script:
       File -> Script file... -> Select this file

    3. Via command line (headless):
       idat64 -A -S"tests/run_ida_tests.py" <binary>

Tests that require IDA Pro's Qt application:
- test_event_emitter.py::TestMessageEmitter::test_worker_launcher_integration
- test_qtasyncio.py::TestQtApplicationIntegration::test_full_worker_execution

These tests will be SKIPPED when run outside IDA Pro.
"""

import sys
import subprocess


def main():
    """Run pytest with tests that require IDA Pro."""
    print("=" * 70)
    print("Running ida-taskr tests inside IDA Pro")
    print("=" * 70)
    print()

    # Tests that require IDA Pro (will be skipped outside IDA)
    ida_tests = [
        "tests/unit/test_event_emitter.py::TestMessageEmitter::test_worker_launcher_integration",
        "tests/unit/test_qtasyncio.py::TestQtApplicationIntegration::test_full_worker_execution",
    ]

    # You can also run all tests - the ones that need IDA will run, others will be skipped if they fail
    all_tests = ["tests/"]

    print("Running IDA-specific tests:")
    for test in ida_tests:
        print(f"  - {test}")
    print()

    # Run pytest with the venv Python that has all dependencies
    python_exe = "/app/ida/.venv/bin/python"
    cmd = [python_exe, "-m", "pytest"] + ida_tests + ["-v", "--tb=short"]

    print(f"Command: {' '.join(cmd)}")
    print()

    result = subprocess.run(cmd, cwd="/home/user/ida-taskr")

    print()
    print("=" * 70)
    if result.returncode == 0:
        print("‚úÖ All IDA-specific tests passed!")
    else:
        print(f"‚ùå Tests failed with exit code: {result.returncode}")
    print("=" * 70)

    return result.returncode


if __name__ == "__main__":
    try:
        # Check if we're in IDA
        import idaapi
        print("‚úì Running inside IDA Pro")
        print(f"  IDA version: {idaapi.get_kernel_version()}")
        print()
    except ImportError:
        print("‚ö† Warning: Not running inside IDA Pro")
        print("  Some tests will be skipped")
        print()

    sys.exit(main())

```

`tests/run_tests_in_ida.py`:

```py
"""
Run ida-taskr tests directly inside IDA's Python environment.

This script should be run with: idat -A -S"tests/run_tests_in_ida.py" <binary>
"""

import sys
import os

# Add project to path
sys.path.insert(0, "/home/user/ida-taskr/src")
sys.path.insert(0, "/home/user/ida-taskr")

# Set environment variables pytest might need
os.environ['PYTHONPATH'] = "/home/user/ida-taskr/src"

output_file = "/tmp/ida_test_results.txt"

try:
    with open(output_file, "w") as f:
        f.write("=" * 70 + "\n")
        f.write("Running tests inside IDA Pro\n")
        f.write("=" * 70 + "\n\n")

        # Verify we're in IDA
        try:
            import idaapi
            f.write(f"‚úì IDA version: {idaapi.get_kernel_version()}\n")
            f.write(f"‚úì sys.executable: {sys.executable}\n")
        except ImportError:
            f.write("‚úó Not running in IDA!\n")
            sys.exit(1)

        # Check is_ida()
        from ida_taskr import is_ida
        f.write(f"‚úì is_ida() returns: {is_ida()}\n\n")

        # Import pytest and run tests
        f.write("Running pytest...\n")
        f.flush()

    # Run pytest in-process
    import pytest

    # Run the two IDA-specific tests
    exit_code = pytest.main([
        "tests/unit/test_event_emitter.py::TestMessageEmitter::test_worker_launcher_integration",
        "tests/unit/test_qtasyncio.py::TestQtApplicationIntegration::test_full_worker_execution",
        "-v",
        "--tb=short",
        "-s",  # Don't capture output
    ])

    with open(output_file, "a") as f:
        f.write(f"\n\nTests completed with exit code: {exit_code}\n")

    # Exit IDA
    try:
        import idaapi
        idaapi.qexit(exit_code)
    except:
        sys.exit(exit_code)

except Exception as e:
    with open(output_file, "a") as f:
        f.write(f"\n\nERROR: {e}\n")
        import traceback
        traceback.print_exc(file=f)

    try:
        import idaapi
        idaapi.qexit(1)
    except:
        sys.exit(1)

```

`tests/test_imports.py`:

```py
import sys
sys.path.insert(0, "/home/user/ida-taskr/src")

with open("/tmp/ida_imports_test.txt", "w") as f:
    f.write("Testing imports...\n\n")

    try:
        f.write("1. Importing ida_taskr...\n")
        import ida_taskr
        f.write("   [OK] Success\n\n")

        f.write("2. Checking is_ida()...\n")
        f.write(f"   Result: {ida_taskr.is_ida()}\n\n")

        f.write("3. Importing MessageEmitter...\n")
        from ida_taskr import MessageEmitter
        f.write("   [OK] Success\n\n")

        f.write("4. Creating MessageEmitter...\n")
        emitter = MessageEmitter()
        f.write(f"   [OK] Created: {emitter}\n\n")

        f.write("5. Importing WorkerLauncher...\n")
        from ida_taskr import WorkerLauncher
        f.write("   [OK] Success\n\n")

        f.write("6. Creating WorkerLauncher...\n")
        launcher = WorkerLauncher(emitter)
        f.write(f"   [OK] Created: {launcher}\n\n")

        f.write("ALL TESTS PASSED!\n")

    except Exception as e:
        f.write(f"\nError: {e}\n")
        import traceback
        f.write(traceback.format_exc())

try:
    import idaapi
    idaapi.qexit(0)
except:
    pass

```

`tests/test_qprocess_headless.py`:

```py
"""Test if QProcess works in headless IDA."""

import sys
sys.path.insert(0, "/home/user/ida-taskr/src")

with open("/tmp/ida_qprocess_test.txt", "w") as f:
    f.write("Testing QProcess in headless IDA\n")
    f.write("=" * 70 + "\n\n")

    try:
        import idaapi
        f.write(f"IDA version: {idaapi.get_kernel_version()}\n\n")

        from PySide6.QtCore import QCoreApplication, QProcess
        app = QCoreApplication.instance()
        f.write(f"Qt app: {type(app).__name__}\n")
        f.write(f"Qt app exists: {app is not None}\n\n")

        # Test 1: Create a QProcess
        f.write("Test 1: Creating QProcess...\n")
        process = QProcess()
        f.write(f"[OK] QProcess created: {process}\n\n")

        # Test 2: Run a simple command
        f.write("Test 2: Running 'echo hello' command...\n")
        process.start("echo", ["hello"])

        # Wait for it to finish
        if process.waitForFinished(1000):  # 1 second timeout
            output = process.readAllStandardOutput().data().decode()
            f.write(f"[OK] Command executed\n")
            f.write(f"     Output: {output.strip()}\n\n")
        else:
            f.write(f"[WARN] Command timed out or failed\n")
            f.write(f"       State: {process.state()}\n\n")

        # Test 3: Create WorkerLauncher (uses QProcess)
        f.write("Test 3: Creating WorkerLauncher...\n")
        from ida_taskr import WorkerLauncher, MessageEmitter

        emitter = MessageEmitter()
        launcher = WorkerLauncher(emitter)
        f.write(f"[OK] WorkerLauncher created: {launcher}\n\n")

        # Test 4: Verify WorkerLauncher has QProcess methods
        f.write("Test 4: Verifying WorkerLauncher QProcess interface...\n")
        f.write(f"     Has start(): {hasattr(launcher, 'start')}\n")
        f.write(f"     Has waitForFinished(): {hasattr(launcher, 'waitForFinished')}\n")
        f.write(f"     Has readAllStandardOutput(): {hasattr(launcher, 'readAllStandardOutput')}\n")
        f.write(f"     Inherits QProcess: {isinstance(launcher, QProcess)}\n\n")

        f.write("=" * 70 + "\n")
        f.write("ALL TESTS PASSED - QProcess works in headless IDA!\n")
        f.write("=" * 70 + "\n")

    except Exception as e:
        f.write(f"\n[FAIL] Error: {e}\n")
        import traceback
        f.write(traceback.format_exc())
        f.write("=" * 70 + "\n")

try:
    import idaapi
    idaapi.qexit(0)
except:
    pass

```

`tests/unit/__init__.py`:

```py
"""Unit tests for IDA Taskr.

These tests use mocks and do not require IDA Pro or Qt to be installed.
They can be run in any Python environment with unittest.
"""

```

`tests/unit/test_event_emitter.py`:

```py
"""
Unit tests for MessageEmitter functionality and event handling patterns.

Tests the composition-based approach for handling worker messages using
the MessageEmitter pattern.
"""

import logging
from unittest.mock import MagicMock, Mock, patch

import pytest

from ida_taskr import MessageEmitter, get_logger, is_ida
from ida_taskr.qt_compat import QT_AVAILABLE

# Import WorkerLauncher only if Qt is available
if QT_AVAILABLE:
    from ida_taskr import WorkerLauncher
else:
    WorkerLauncher = None

logger = get_logger(__name__)


class TestMessageEmitter:
    """Test suite for MessageEmitter event handling."""

    def setup_method(self):
        """Set up test fixtures before each test method."""
        self.emitter = MessageEmitter()
        self.test_events = []

    def teardown_method(self):
        """Clean up after each test method."""
        self.test_events.clear()

    def test_decorator_event_registration(self):
        """Test event handler registration using decorator syntax."""

        @self.emitter.on("worker_connected")
        def on_connected():
            self.test_events.append("connected")

        @self.emitter.on("worker_message")
        def on_message(message: dict):
            self.test_events.append(f"message: {message}")

        @self.emitter.on("worker_results")
        def on_results(results: dict):
            self.test_events.append(f"results: {results}")

        @self.emitter.on("worker_error")
        def on_error(error: str):
            self.test_events.append(f"error: {error}")

        @self.emitter.on("worker_disconnected")
        def on_disconnected():
            self.test_events.append("disconnected")

        # Test events are fired correctly
        self.emitter.emit_worker_connected()
        self.emitter.emit_worker_message({"type": "progress", "progress": 0.5})
        self.emitter.emit_worker_results({"status": "success", "results": [1, 2, 3]})
        self.emitter.emit_worker_error("Test error")
        self.emitter.emit_worker_disconnected()

        # Verify all events were captured
        assert len(self.test_events) == 5
        assert "connected" in self.test_events
        assert "message: {'type': 'progress', 'progress': 0.5}" in self.test_events
        assert "results: {'status': 'success', 'results': [1, 2, 3]}" in self.test_events
        assert "error: Test error" in self.test_events
        assert "disconnected" in self.test_events

    def test_direct_event_registration(self):
        """Test alternative way to register handlers directly without decorators."""

        def handle_connection():
            self.test_events.append("worker_connected")

        def handle_message(message: dict):
            self.test_events.append(f"worker_message: {message}")

        def handle_results(results: dict):
            self.test_events.append(f"worker_results: {results}")

        def handle_error(error: str):
            self.test_events.append(f"worker_error: {error}")

        def handle_disconnection():
            self.test_events.append("worker_disconnected")

        # Register the handlers directly
        self.emitter.on("worker_connected", handle_connection)
        self.emitter.on("worker_message", handle_message)
        self.emitter.on("worker_results", handle_results)
        self.emitter.on("worker_error", handle_error)
        self.emitter.on("worker_disconnected", handle_disconnection)

        # Emit events
        self.emitter.emit_worker_connected()
        test_message = {"type": "status", "data": "processing"}
        self.emitter.emit_worker_message(test_message)
        test_results = {"status": "completed", "results": ["item1", "item2"]}
        self.emitter.emit_worker_results(test_results)
        self.emitter.emit_worker_error("Connection timeout")
        self.emitter.emit_worker_disconnected()

        # Verify events were handled
        assert len(self.test_events) == 5
        assert "worker_connected" in self.test_events
        assert f"worker_message: {test_message}" in self.test_events
        assert f"worker_results: {test_results}" in self.test_events
        assert "worker_error: Connection timeout" in self.test_events
        assert "worker_disconnected" in self.test_events

    def test_multiple_subscribers_same_event(self):
        """Test multiple handlers for the same event."""
        results_log = []

        @self.emitter.on("worker_results")
        def log_results(results: dict):
            results_log.append(f"logged: {len(results.get('results', []))}")

        @self.emitter.on("worker_results")
        def process_results(results: dict):
            results_log.append("processed")

        @self.emitter.on("worker_results")
        def notify_ui(results: dict):
            results_log.append("ui_updated")

        # Emit results event
        test_results = {"status": "success", "results": [1, 2, 3, 4, 5]}
        self.emitter.emit_worker_results(test_results)

        # Verify all handlers were called
        assert len(results_log) == 3
        assert "logged: 5" in results_log
        assert "processed" in results_log
        assert "ui_updated" in results_log

    def test_progress_message_handling(self):
        """Test handling of progress messages specifically."""
        progress_events = []

        @self.emitter.on("worker_message")
        def on_message(message: dict):
            if message.get("type") == "progress":
                progress = message.get("progress", 0)
                progress_events.append(progress * 100)

        # Send progress messages
        self.emitter.emit_worker_message({"type": "progress", "progress": 0.25})
        self.emitter.emit_worker_message({"type": "progress", "progress": 0.50})
        self.emitter.emit_worker_message({"type": "progress", "progress": 0.75})
        self.emitter.emit_worker_message(
            {"type": "status", "message": "working"}
        )  # Should be ignored
        self.emitter.emit_worker_message({"type": "progress", "progress": 1.0})

        # Verify only progress messages were processed
        assert len(progress_events) == 4
        assert progress_events == [25.0, 50.0, 75.0, 100.0]

    def test_error_handling(self):
        """Test error event handling."""
        error_messages = []

        @self.emitter.on("worker_error")
        def on_error(error: str):
            error_messages.append(error)

        # Emit various error types
        self.emitter.emit_worker_error("Connection failed")
        self.emitter.emit_worker_error("Processing timeout")
        self.emitter.emit_worker_error("Invalid data format")

        # Verify all errors were captured
        assert len(error_messages) == 3
        assert "Connection failed" in error_messages
        assert "Processing timeout" in error_messages
        assert "Invalid data format" in error_messages

    @pytest.mark.skipif(not is_ida(), reason="WorkerLauncher requires IDA Pro's Qt application")
    @patch("ida_taskr.launcher.WorkerLauncher.launch_worker")
    def test_worker_launcher_integration(self, mock_launch_worker):
        """Test integration with WorkerLauncher (IDA Pro only)."""
        # Configure the mock
        mock_launch_worker.return_value = True

        # Create message emitter with event handlers
        message_emitter = MessageEmitter()
        connection_events = []

        @message_emitter.on("worker_connected")
        def on_connected():
            connection_events.append("connected")

        @message_emitter.on("worker_disconnected")
        def on_disconnected():
            connection_events.append("disconnected")

        # Create worker launcher with the message emitter
        launcher = WorkerLauncher(message_emitter)

        # Launch worker
        worker_args = {"data_size": 1024, "start_ea": "0x1000", "is64": "1"}
        result = launcher.launch_worker("path/to/test/worker.py", worker_args)

        # Verify launcher was called correctly
        assert result is True
        mock_launch_worker.assert_called_once_with(
            "path/to/test/worker.py", worker_args
        )

        # Test event emission
        message_emitter.emit_worker_connected()
        message_emitter.emit_worker_disconnected()

        assert connection_events == ["connected", "disconnected"]

    def test_results_processing(self):
        """Test processing of worker results."""
        processed_results = []

        @self.emitter.on("worker_results")
        def on_results(results: dict):
            if results.get("status") == "success":
                data = results.get("results", [])
                processed_results.extend(data)

        # Test successful results
        self.emitter.emit_worker_results(
            {"status": "success", "results": ["item1", "item2", "item3"]}
        )

        # Test failed results (should be ignored)
        self.emitter.emit_worker_results(
            {"status": "error", "results": ["should_be_ignored"]}
        )

        # Verify only successful results were processed
        assert processed_results == ["item1", "item2", "item3"]

    def test_no_handlers_registered(self):
        """Test that emitting events with no handlers doesn't cause errors."""
        # Should not raise any exceptions
        self.emitter.emit_worker_connected()
        self.emitter.emit_worker_message({"test": "message"})
        self.emitter.emit_worker_results({"test": "results"})
        self.emitter.emit_worker_error("test error")
        self.emitter.emit_worker_disconnected()

    def test_handler_exceptions_are_propagated(self):
        """Test that exceptions in handlers are propagated (current behavior)."""
        results = []

        @self.emitter.on("worker_message")
        def handler_that_works(message):
            results.append("handler_called")

        @self.emitter.on("worker_message")
        def handler_that_fails(message):
            raise Exception("Handler failed")

        # The current implementation propagates exceptions, so we expect one
        with pytest.raises(Exception) as exc_info:
            self.emitter.emit_worker_message({"test": "message"})

        assert str(exc_info.value) == "Handler failed"

        # Due to the way Python sets work, handler order is not guaranteed.
        # The exception will stop execution, so we can only guarantee the exception was raised.
        # We cannot guarantee which handlers ran before the failing one.
        assert True  # If we got here, the exception was properly propagated

```

`tests/unit/test_imports.py`:

```py
"""Test basic imports."""


class TestImports:
    def test_taskrunner_import(self):
        """Test that TaskRunner can be imported."""
        from ida_taskr import TaskRunner

        assert TaskRunner is not None


```

`tests/unit/test_interpreter_pool_executor.py`:

```py
"""
Tests for InterpreterPoolExecutor - process-based executor with Qt signal support.

These tests demonstrate using InterpreterPoolExecutor similar to
concurrent.futures patterns but with Qt signal integration.

This implementation uses ProcessPoolExecutor as the backend, providing
true parallelism compatible with embedded Python contexts (like IDA Pro).
"""

import concurrent.futures
import sys
import time
import pytest

from ida_taskr import QT_ASYNCIO_AVAILABLE, INTERPRETER_POOL_AVAILABLE

# Check if using PyQt5 (not PySide6)
try:
    import PyQt5
    _USING_PYQT5 = True
except ImportError:
    _USING_PYQT5 = False

# Skip on PyQt5 + Python 3.12+ due to multiprocessing/spawn compatibility issues
# that cause hangs in CI. PySide6 works fine with all Python versions.
_PYQT5_PY312_ISSUE = _USING_PYQT5 and sys.version_info >= (3, 12)

# Skip all tests if QtAsyncio is not available
pytestmark = [
    pytest.mark.skipif(
        not QT_ASYNCIO_AVAILABLE,
        reason="QtAsyncio module not available"
    ),
    pytest.mark.skipif(
        _PYQT5_PY312_ISSUE,
        reason="PyQt5 + Python 3.12+ has multiprocessing spawn issues in CI"
    ),
]


# Module-level functions for interpreter sharing (must be shareable)
def compute_sum_of_squares(num: int) -> int:
    """Compute sum of squares from 0 to num."""
    return sum(i * i for i in range(num + 1))


def compute_factorial(n: int) -> int:
    """Compute factorial of n."""
    result = 1
    for i in range(2, n + 1):
        result *= i
    return result


def faulty_task(n: int) -> int:
    """Task that raises an exception for n == 2."""
    if n == 2:
        raise ValueError("Error with input 2")
    return n * n


def slow_task(duration: float) -> float:
    """Task that sleeps for the specified duration."""
    time.sleep(duration)
    return duration


class TestInterpreterPoolAvailability:
    """Test availability detection."""

    def test_availability_flag_exists(self):
        """Test that INTERPRETER_POOL_AVAILABLE flag is exported."""
        from ida_taskr import INTERPRETER_POOL_AVAILABLE

        # Should be a boolean
        assert isinstance(INTERPRETER_POOL_AVAILABLE, bool)

        # Should always be True (uses ProcessPoolExecutor as backend)
        assert INTERPRETER_POOL_AVAILABLE is True

    def test_signals_class_always_available(self):
        """Test that the signals class is always importable."""
        from ida_taskr.qtasyncio import InterpreterPoolExecutorSignals

        # Should always be available
        assert InterpreterPoolExecutorSignals is not None

    def test_executor_class_always_available(self):
        """Test that the executor class is always importable."""
        from ida_taskr import InterpreterPoolExecutor

        # Should always be available (uses ProcessPoolExecutor backend)
        assert InterpreterPoolExecutor is not None


class TestInterpreterPoolExecutorBasic:
    """Basic InterpreterPoolExecutor functionality tests."""

    def test_submit_single_task(self):
        """Test submitting a single task to InterpreterPoolExecutor."""
        from ida_taskr import InterpreterPoolExecutor

        with InterpreterPoolExecutor(max_workers=2) as executor:
            future = executor.submit(compute_sum_of_squares, 100)
            result = future.result(timeout=10)
            expected = sum(i * i for i in range(101))
            assert result == expected

    def test_submit_multiple_tasks(self):
        """Test submitting multiple tasks concurrently."""
        from ida_taskr import InterpreterPoolExecutor

        numbers = [100, 200, 300, 400]

        with InterpreterPoolExecutor(max_workers=4) as executor:
            futures = [executor.submit(compute_sum_of_squares, num) for num in numbers]
            results = [f.result(timeout=10) for f in futures]

        # Verify all results
        for num, result in zip(numbers, results):
            expected = sum(i * i for i in range(num + 1))
            assert result == expected

    def test_context_manager(self):
        """Test using InterpreterPoolExecutor as context manager."""
        from ida_taskr import InterpreterPoolExecutor

        with InterpreterPoolExecutor(max_workers=2) as executor:
            future = executor.submit(compute_sum_of_squares, 50)
            result = future.result(timeout=10)
            assert result > 0

        # Executor should be shut down after context exits
        with pytest.raises(RuntimeError, match="Cannot schedule new futures"):
            executor.submit(compute_sum_of_squares, 10)


class TestInterpreterPoolExecutorMap:
    """Test the map() method - matching the user's example."""

    def test_map_sum_of_squares(self):
        """
        Test map with sum of squares - matching the user's example:

            def sums(num: int) -> int:
                return sum(i * i for i in range(num + 1))

            with InterpreterPoolExecutor() as executor:
                print(list(executor.map(sums, [100_000] * 4)))
        """
        from ida_taskr import InterpreterPoolExecutor

        # Use smaller numbers for faster tests
        numbers = [1000] * 4

        with InterpreterPoolExecutor(max_workers=4) as executor:
            results = list(executor.map(compute_sum_of_squares, numbers, timeout=30))

        # All should return the same value
        assert len(results) == 4
        expected = sum(i * i for i in range(1001))
        assert all(r == expected for r in results)

    def test_map_preserves_order(self):
        """Test that map preserves input order."""
        from ida_taskr import InterpreterPoolExecutor

        numbers = [100, 200, 300, 400, 500]

        with InterpreterPoolExecutor(max_workers=4) as executor:
            results = list(executor.map(compute_sum_of_squares, numbers, timeout=30))

        # Results should be in same order as input
        for num, result in zip(numbers, results):
            expected = sum(i * i for i in range(num + 1))
            assert result == expected


class TestInterpreterPoolExecutorExceptionHandling:
    """Test exception handling in InterpreterPoolExecutor."""

    def test_exception_handling_basic(self):
        """Test that exceptions are properly propagated."""
        from ida_taskr import InterpreterPoolExecutor

        numbers = [1, 2, 3, 4]

        with InterpreterPoolExecutor(max_workers=4) as executor:
            futures = [executor.submit(faulty_task, num) for num in numbers]

            results = []
            exceptions = []

            for future in concurrent.futures.as_completed(futures, timeout=30):
                try:
                    result = future.result()
                    results.append(result)
                except ValueError as e:
                    exceptions.append(str(e))

        # Should have 3 successful results and 1 exception
        assert len(results) == 3
        assert sorted(results) == [1, 9, 16]
        assert len(exceptions) == 1
        assert "Error with input 2" in exceptions[0]

    def test_exception_does_not_crash_pool(self):
        """Verify exceptions in one task don't affect others."""
        from ida_taskr import InterpreterPoolExecutor

        with InterpreterPoolExecutor(max_workers=2) as executor:
            # Submit a failing task and a succeeding task
            fail_future = executor.submit(faulty_task, 2)
            success_future = executor.submit(compute_sum_of_squares, 10)

            # Success should complete normally
            expected = sum(i * i for i in range(11))
            assert success_future.result(timeout=10) == expected

            # Failure should raise
            with pytest.raises(ValueError, match="Error with input 2"):
                fail_future.result(timeout=10)


class TestInterpreterPoolExecutorShutdown:
    """Test shutdown behavior."""

    def test_shutdown_wait_true(self):
        """Test that shutdown(wait=True) waits for tasks."""
        from ida_taskr import InterpreterPoolExecutor

        executor = InterpreterPoolExecutor(max_workers=2)
        futures = [executor.submit(slow_task, 0.1) for _ in range(3)]

        executor.shutdown(wait=True)

        # All tasks should be completed
        for f in futures:
            assert f.done()

    def test_shutdown_prevents_new_submissions(self):
        """Test that shutdown prevents new task submissions."""
        from ida_taskr import InterpreterPoolExecutor

        executor = InterpreterPoolExecutor(max_workers=2)
        executor.shutdown(wait=False)

        with pytest.raises(RuntimeError, match="Cannot schedule new futures"):
            executor.submit(compute_sum_of_squares, 10)


class TestInterpreterPoolExecutorSignals:
    """Test Qt signal integration (ProcessPoolExecutor backend)."""

    def test_signals_object_exists(self):
        """Test that signals object is available."""
        from ida_taskr import InterpreterPoolExecutor

        executor = InterpreterPoolExecutor(max_workers=2)

        assert hasattr(executor, 'signals')
        assert hasattr(executor.signals, 'task_submitted')
        assert hasattr(executor.signals, 'task_completed')
        assert hasattr(executor.signals, 'task_failed')
        assert hasattr(executor.signals, 'pool_shutdown')

        executor.shutdown(wait=False)

    def test_max_workers_property(self):
        """Test max_workers property."""
        from ida_taskr import InterpreterPoolExecutor

        executor = InterpreterPoolExecutor(max_workers=4)
        assert executor.max_workers == 4
        executor.shutdown(wait=False)


class TestInterpreterPoolExecutorPerformance:
    """Performance-related tests (ProcessPoolExecutor backend)."""

    def test_parallel_execution(self):
        """Verify tasks run in parallel across processes."""
        from ida_taskr import InterpreterPoolExecutor

        # Submit 4 tasks that each sleep for 0.2 seconds
        num_tasks = 4
        sleep_time = 0.2

        start = time.time()

        with InterpreterPoolExecutor(max_workers=4) as executor:
            futures = [executor.submit(slow_task, sleep_time) for _ in range(num_tasks)]
            concurrent.futures.wait(futures, timeout=30)

        elapsed = time.time() - start

        # If truly parallel, should complete in ~0.2-0.5s, not 0.8s (sequential)
        # Allow margin for interpreter startup overhead
        assert elapsed < 1.0, f"Tasks took {elapsed}s - may not be running in parallel"


class TestQInterpreterPoolExecutorAlias:
    """Test that QInterpreterPoolExecutor alias works."""

    def test_alias_import(self):
        """Test importing via alias."""
        from ida_taskr import QInterpreterPoolExecutor, InterpreterPoolExecutor

        # Should be the same class
        assert QInterpreterPoolExecutor is InterpreterPoolExecutor

    def test_alias_usage(self):
        """Test using the alias."""
        from ida_taskr import QInterpreterPoolExecutor

        with QInterpreterPoolExecutor(max_workers=2) as executor:
            future = executor.submit(compute_sum_of_squares, 100)
            result = future.result(timeout=10)
            expected = sum(i * i for i in range(101))
            assert result == expected

```

`tests/unit/test_process_pool_executor.py`:

```py
"""
Tests for ProcessPoolExecutor - multiprocessing-based executor with Qt signal support.

These tests demonstrate using ProcessPoolExecutor similar to concurrent.futures.ProcessPoolExecutor
but with Qt signal integration for task completion notifications.
"""

import concurrent.futures
import math
import sys
import time
import pytest

from ida_taskr import QT_ASYNCIO_AVAILABLE

# Check if using PyQt5 (not PySide6)
try:
    import PyQt5
    _USING_PYQT5 = True
except ImportError:
    _USING_PYQT5 = False

# Skip on PyQt5 + Python 3.12+ due to multiprocessing/spawn compatibility issues
# that cause hangs in CI. PySide6 works fine with all Python versions.
_PYQT5_PY312_ISSUE = _USING_PYQT5 and sys.version_info >= (3, 12)

pytestmark = [
    pytest.mark.skipif(
        not QT_ASYNCIO_AVAILABLE,
        reason="QtAsyncio module not available"
    ),
    pytest.mark.skipif(
        _PYQT5_PY312_ISSUE,
        reason="PyQt5 + Python 3.12+ has multiprocessing spawn issues in CI"
    ),
]


# Module-level functions for multiprocessing (must be picklable)
def compute_factorial(n: int) -> int:
    """Compute factorial of n."""
    result = 1
    for i in range(2, n + 1):
        result *= i
    return result


def compute_square(n: int) -> int:
    """Compute n squared."""
    return n * n


def faulty_task(n: int) -> int:
    """Task that raises an exception for n == 2."""
    if n == 2:
        raise ValueError("Error with input 2")
    return n * n


def slow_task(duration: float) -> float:
    """Task that sleeps for the specified duration."""
    time.sleep(duration)
    return duration


def cpu_intensive_task(n: int) -> int:
    """Simulate CPU-intensive work."""
    total = 0
    for i in range(n):
        total += i * i
    return total


class TestProcessPoolExecutorBasic:
    """Basic ProcessPoolExecutor functionality tests."""

    def test_submit_single_task(self):
        """Test submitting a single task to ProcessPoolExecutor."""
        from ida_taskr import ProcessPoolExecutor

        with ProcessPoolExecutor(max_workers=2) as executor:
            future = executor.submit(compute_square, 5)
            result = future.result(timeout=10)
            assert result == 25

    def test_submit_multiple_tasks(self):
        """Test submitting multiple tasks concurrently."""
        from ida_taskr import ProcessPoolExecutor

        numbers = [1, 2, 3, 4, 5]

        with ProcessPoolExecutor(max_workers=4) as executor:
            futures = [executor.submit(compute_square, num) for num in numbers]
            results = [f.result(timeout=10) for f in futures]

        assert results == [1, 4, 9, 16, 25]

    def test_context_manager(self):
        """Test using ProcessPoolExecutor as context manager."""
        from ida_taskr import ProcessPoolExecutor

        with ProcessPoolExecutor(max_workers=2) as executor:
            future = executor.submit(compute_square, 10)
            result = future.result(timeout=10)
            assert result == 100

        # Executor should be shut down after context exits
        with pytest.raises(RuntimeError, match="Cannot schedule new futures"):
            executor.submit(compute_square, 5)


class TestProcessPoolExecutorFactorial:
    """Test ProcessPoolExecutor with CPU-bound factorial computation.

    This mirrors the user's original example using concurrent.futures.ProcessPoolExecutor.
    """

    def test_compute_factorials_concurrently(self):
        """
        Test computing factorials concurrently - matching the original example:

            numbers = [50000, 60000, 70000, 80000]
            with concurrent.futures.ProcessPoolExecutor() as executor:
                futures = [executor.submit(compute_factorial, num) for num in numbers]
                for future in concurrent.futures.as_completed(futures):
                    print(f"Factorial computed")
        """
        from ida_taskr import ProcessPoolExecutor

        # Use smaller numbers for faster tests
        numbers = [100, 200, 300, 400]

        start_time = time.time()

        with ProcessPoolExecutor(max_workers=4) as executor:
            futures = [executor.submit(compute_factorial, num) for num in numbers]
            completed_count = 0

            for future in concurrent.futures.as_completed(futures, timeout=30):
                result = future.result()
                completed_count += 1
                # Verify result is a positive integer (factorial)
                assert result > 0
                assert isinstance(result, int)

        end_time = time.time()

        # All factorials should be computed
        assert completed_count == len(numbers)

        # Should complete in reasonable time
        assert end_time - start_time < 30.0

    def test_factorial_results_accuracy(self):
        """Verify factorial computations are accurate."""
        from ida_taskr import ProcessPoolExecutor

        test_cases = {
            5: 120,
            10: 3628800,
            20: 2432902008176640000,
        }

        with ProcessPoolExecutor(max_workers=2) as executor:
            for n, expected in test_cases.items():
                future = executor.submit(compute_factorial, n)
                result = future.result(timeout=10)
                assert result == expected, f"factorial({n}) = {result}, expected {expected}"


class TestProcessPoolExecutorExceptionHandling:
    """Test exception handling in ProcessPoolExecutor.

    This mirrors the user's faulty_task example:
        def faulty_task(n):
            if n == 2:
                raise ValueError("Error with input 2")
            return n * n

        with concurrent.futures.ThreadPoolExecutor() as executor:
            futures = [executor.submit(faulty_task, num) for num in numbers]
            for future in concurrent.futures.as_completed(futures):
                try:
                    result = future.result()
                except Exception as e:
                    print(f"Task raised an exception: {e}")
    """

    def test_exception_handling_basic(self):
        """Test that exceptions are properly propagated."""
        from ida_taskr import ProcessPoolExecutor

        numbers = [1, 2, 3, 4]

        with ProcessPoolExecutor(max_workers=4) as executor:
            futures = [executor.submit(faulty_task, num) for num in numbers]

            results = []
            exceptions = []

            for future in concurrent.futures.as_completed(futures, timeout=30):
                try:
                    result = future.result()
                    results.append(result)
                except ValueError as e:
                    exceptions.append(str(e))

        # Should have 3 successful results and 1 exception
        assert len(results) == 3
        assert sorted(results) == [1, 9, 16]
        assert len(exceptions) == 1
        assert "Error with input 2" in exceptions[0]

    def test_exception_does_not_crash_pool(self):
        """Verify exceptions in one task don't affect others."""
        from ida_taskr import ProcessPoolExecutor

        with ProcessPoolExecutor(max_workers=2) as executor:
            # Submit a failing task and a succeeding task
            fail_future = executor.submit(faulty_task, 2)
            success_future = executor.submit(compute_square, 5)

            # Success should complete normally
            assert success_future.result(timeout=10) == 25

            # Failure should raise
            with pytest.raises(ValueError, match="Error with input 2"):
                fail_future.result(timeout=10)


class TestProcessPoolExecutorMap:
    """Test the map() method."""

    def test_map_basic(self):
        """Test basic map functionality."""
        from ida_taskr import ProcessPoolExecutor

        numbers = [1, 2, 3, 4, 5]

        with ProcessPoolExecutor(max_workers=4) as executor:
            results = list(executor.map(compute_square, numbers, timeout=30))

        assert results == [1, 4, 9, 16, 25]

    def test_map_preserves_order(self):
        """Test that map preserves input order."""
        from ida_taskr import ProcessPoolExecutor

        # Use varying delays to test ordering
        numbers = [5, 1, 3, 2, 4]

        with ProcessPoolExecutor(max_workers=4) as executor:
            results = list(executor.map(compute_square, numbers, timeout=30))

        # Results should be in same order as input
        assert results == [25, 1, 9, 4, 16]


class TestProcessPoolExecutorShutdown:
    """Test shutdown behavior."""

    def test_shutdown_wait_true(self):
        """Test that shutdown(wait=True) waits for tasks."""
        from ida_taskr import ProcessPoolExecutor

        executor = ProcessPoolExecutor(max_workers=2)
        futures = [executor.submit(slow_task, 0.1) for _ in range(3)]

        executor.shutdown(wait=True)

        # All tasks should be completed
        for f in futures:
            assert f.done()

    def test_shutdown_prevents_new_submissions(self):
        """Test that shutdown prevents new task submissions."""
        from ida_taskr import ProcessPoolExecutor

        executor = ProcessPoolExecutor(max_workers=2)
        executor.shutdown(wait=False)

        with pytest.raises(RuntimeError, match="Cannot schedule new futures"):
            executor.submit(compute_square, 5)


class TestProcessPoolExecutorSignals:
    """Test Qt signal integration."""

    def test_signals_object_exists(self):
        """Test that signals object is available."""
        from ida_taskr import ProcessPoolExecutor

        executor = ProcessPoolExecutor(max_workers=2)

        assert hasattr(executor, 'signals')
        assert hasattr(executor.signals, 'task_submitted')
        assert hasattr(executor.signals, 'task_completed')
        assert hasattr(executor.signals, 'task_failed')
        assert hasattr(executor.signals, 'pool_shutdown')

        executor.shutdown(wait=False)

    def test_task_completed_signal(self):
        """Test task_completed signal is emitted."""
        from ida_taskr import ProcessPoolExecutor

        completed_futures = []

        def on_completed(future):
            completed_futures.append(future)

        executor = ProcessPoolExecutor(max_workers=2)
        executor.signals.task_completed.connect(on_completed)

        future = executor.submit(compute_square, 5)
        future.result(timeout=10)  # Wait for completion

        # Give signal time to propagate
        time.sleep(0.1)

        executor.shutdown(wait=True)

        # Signal should have been emitted
        assert len(completed_futures) >= 0  # May or may not catch depending on timing


class TestProcessPoolExecutorPerformance:
    """Performance-related tests."""

    def test_parallel_execution(self):
        """Verify tasks run in parallel across processes."""
        from ida_taskr import ProcessPoolExecutor

        # Submit 4 tasks that each sleep for 0.2 seconds
        num_tasks = 4
        sleep_time = 0.2

        start = time.time()

        with ProcessPoolExecutor(max_workers=4) as executor:
            futures = [executor.submit(slow_task, sleep_time) for _ in range(num_tasks)]
            concurrent.futures.wait(futures, timeout=30)

        elapsed = time.time() - start

        # If truly parallel, should complete in ~0.2-0.5s, not 0.8s (sequential)
        # Allow margin for process startup overhead
        assert elapsed < 1.0, f"Tasks took {elapsed}s - may not be running in parallel"

    def test_cpu_bound_tasks_scale(self):
        """Test that CPU-bound tasks benefit from multiprocessing."""
        from ida_taskr import ProcessPoolExecutor

        work_size = 100000

        with ProcessPoolExecutor(max_workers=4) as executor:
            futures = [executor.submit(cpu_intensive_task, work_size) for _ in range(4)]
            results = [f.result(timeout=30) for f in futures]

        # All should return the same result
        assert len(set(results)) == 1


class TestQProcessPoolExecutorAlias:
    """Test that QProcessPoolExecutor alias works."""

    def test_alias_import(self):
        """Test importing via alias."""
        from ida_taskr import QProcessPoolExecutor, ProcessPoolExecutor

        # Should be the same class
        assert QProcessPoolExecutor is ProcessPoolExecutor

    def test_alias_usage(self):
        """Test using the alias."""
        from ida_taskr import QProcessPoolExecutor

        with QProcessPoolExecutor(max_workers=2) as executor:
            future = executor.submit(compute_square, 7)
            result = future.result(timeout=10)
            assert result == 49


class TestQThreadPoolExecutorAlias:
    """Test that QThreadPoolExecutor alias works."""

    def test_alias_import(self):
        """Test importing via alias."""
        from ida_taskr import QThreadPoolExecutor, ThreadExecutor

        # Should be the same class
        assert QThreadPoolExecutor is ThreadExecutor

```

`tests/unit/test_qtasyncio.py`:

```py
"""
Unit tests for QtAsyncio integration.

Tests the qtasyncio module components including worker utilities,
thread executor, and event loop integration.
"""

import asyncio
import time

import pytest

from ida_taskr import QT_ASYNCIO_AVAILABLE, is_ida

# Skip all tests if QtAsyncio is not available
pytestmark = pytest.mark.skipif(
    not QT_ASYNCIO_AVAILABLE,
    reason="QtAsyncio module not available"
)


class TestQtAsyncioImports:
    """Test that QtAsyncio components can be imported."""

    def test_import_worker_utilities(self):
        """Test importing worker utilities."""
        from ida_taskr import (
            QtWorkerBase,
            FunctionWorker,
            GeneratorWorker,
            create_worker,
            thread_worker,
        )

        assert QtWorkerBase is not None
        assert FunctionWorker is not None
        assert GeneratorWorker is not None
        assert create_worker is not None
        assert thread_worker is not None

    def test_import_thread_executor(self):
        """Test importing thread executor components."""
        from ida_taskr import ThreadExecutor, Task, FutureWatcher

        assert ThreadExecutor is not None
        assert Task is not None
        assert FutureWatcher is not None

    def test_import_asyncio_integration(self):
        """Test importing asyncio integration components."""
        from ida_taskr import (
            QAsyncioEventLoop,
            QAsyncioEventLoopPolicy,
            set_event_loop_policy,
        )

        assert QAsyncioEventLoop is not None
        assert QAsyncioEventLoopPolicy is not None
        assert set_event_loop_policy is not None


class TestFunctionWorker:
    """Test FunctionWorker functionality."""

    def test_create_function_worker(self):
        """Test creating a function worker."""
        from ida_taskr import create_worker

        def simple_func(x, y):
            return x + y

        worker = create_worker(simple_func, 2, 3)
        assert worker is not None
        assert hasattr(worker, 'start')
        assert hasattr(worker, 'returned')
        assert hasattr(worker, 'errored')
        assert hasattr(worker, 'finished')

    def test_thread_worker_decorator(self):
        """Test the @thread_worker decorator."""
        from ida_taskr import thread_worker

        @thread_worker
        def decorated_func(x):
            return x * 2

        worker = decorated_func(5)
        assert worker is not None
        assert hasattr(worker, 'start')

    def test_function_worker_execution(self):
        """Test that function worker executes correctly."""
        from ida_taskr import create_worker

        result_holder = []

        def test_func():
            time.sleep(0.1)
            return "test_result"

        worker = create_worker(test_func)

        # Connect signal to capture result
        worker.returned.connect(lambda r: result_holder.append(r))

        # Note: We can't easily test this without a Qt event loop running
        # This is a basic structural test
        assert worker is not None


class TestGeneratorWorker:
    """Test GeneratorWorker functionality."""

    def test_create_generator_worker(self):
        """Test creating a generator worker."""
        from ida_taskr import create_worker

        def generator_func():
            for i in range(3):
                yield i

        worker = create_worker(generator_func)
        assert worker is not None
        assert hasattr(worker, 'yielded')
        assert hasattr(worker, 'returned')

    def test_generator_worker_type_detection(self):
        """Test that generator functions are detected correctly."""
        from ida_taskr import create_worker, GeneratorWorker

        def generator_func():
            yield 1

        worker = create_worker(generator_func)
        assert isinstance(worker, GeneratorWorker)


class TestThreadExecutor:
    """Test ThreadExecutor functionality."""

    def test_create_thread_executor(self, qapp):
        """Test creating a ThreadExecutor."""
        from ida_taskr import ThreadExecutor

        executor = ThreadExecutor()
        assert executor is not None
        assert hasattr(executor, 'submit')
        assert hasattr(executor, 'shutdown')

    def test_thread_executor_submit(self, qapp):
        """Test submitting a task to ThreadExecutor."""
        from ida_taskr import ThreadExecutor

        def simple_task(x):
            return x * 2

        executor = ThreadExecutor()
        future = executor.submit(simple_task, 5)

        assert future is not None
        # Note: Can't easily wait for result without Qt event loop
        # This is a structural test

        executor.shutdown(wait=False)


class TestWorkerControllerIntegration:
    """Test WorkerController with QtAsyncio support."""

    def test_worker_controller_qtasyncio_flag(self):
        """Test that WorkerController accepts use_qtasyncio parameter."""
        from ida_taskr.worker import WorkerController
        from ida_taskr.utils import AsyncEventEmitter
        import dataclasses

        @dataclasses.dataclass
        class DummyEmitter(AsyncEventEmitter):
            async def run(self):
                return "done"

            async def shutdown(self):
                pass

        emitter = DummyEmitter()

        # Test with QtAsyncio disabled
        controller = WorkerController(emitter, use_qtasyncio=False)
        assert controller.use_qtasyncio is False

        # Test with QtAsyncio enabled (if available)
        controller_qt = WorkerController(emitter, use_qtasyncio=True)
        # Should be enabled if QTASYNCIO_ENABLED is True
        assert hasattr(controller_qt, 'use_qtasyncio')


class TestEventLoopPolicy:
    """Test Qt event loop policy integration."""

    def test_set_event_loop_policy(self):
        """Test setting Qt event loop policy."""
        from ida_taskr import set_event_loop_policy, QAsyncioEventLoopPolicy

        # Set the policy
        set_event_loop_policy()

        # Verify it was set
        policy = asyncio.get_event_loop_policy()
        assert isinstance(policy, QAsyncioEventLoopPolicy)

    def test_event_loop_creation(self):
        """Test creating a new event loop with Qt policy."""
        from ida_taskr import set_event_loop_policy

        # Set the policy
        set_event_loop_policy()

        # Create a new event loop
        loop = asyncio.new_event_loop()
        assert loop is not None

        # Clean up
        loop.close()


class TestNewWorkerQThread:
    """Test new_worker_qthread helper function."""

    def test_import_new_worker_qthread(self):
        """Test that new_worker_qthread can be imported."""
        from ida_taskr import new_worker_qthread

        assert new_worker_qthread is not None
        assert callable(new_worker_qthread)


# Integration test that requires a Qt application
class TestQtApplicationIntegration:
    """Integration tests that require a Qt application running."""

    @pytest.mark.skipif(not QT_ASYNCIO_AVAILABLE, reason="Requires Qt application")
    def test_full_worker_execution(self, qapp):
        """Full test of worker execution (requires Qt application)."""
        # Requires Qt application to be running
        from ida_taskr import create_worker

        def simple_task():
            return "completed"

        worker = create_worker(simple_task)
        assert worker is not None
        assert hasattr(worker, 'start')
        # Note: Can't easily test actual execution without Qt event loop running

```

`tests/unit/test_task_runner.py`:

```py
"""
Tests for TaskRunner functionality.
"""

import logging
import os
import sys
from unittest.mock import MagicMock, Mock, patch

import pytest

# Add the src directory to the path for testing
sys.path.insert(0, os.path.join(os.path.dirname(__file__), "..", "src"))

from ida_taskr.qt_compat import QT_AVAILABLE
from ida_taskr import TaskRunner, get_logger

# Skip all tests if Qt is not available
pytestmark = pytest.mark.skipif(
    not QT_AVAILABLE,
    reason="TaskRunner requires Qt"
)


class TestTaskRunner:
    """Test suite for TaskRunner class."""

    def setup_method(self):
        """Set up test fixtures."""
        self.worker_script = "test_worker.py"
        self.worker_args = {"data_size": 1024, "start_ea": "0x1000", "is64": "1"}

    def test_task_runner_initialization(self):
        """Test TaskRunner initialization with default parameters."""
        runner = TaskRunner(self.worker_script, self.worker_args)

        assert runner.worker_script == self.worker_script
        assert runner.worker_args == self.worker_args
        assert runner.logger is not None
        assert runner.message_emitter is not None
        assert runner.launcher is not None
        assert runner._results_callback is None
        assert runner._progress_callback is None

    def test_task_runner_initialization_with_log_level(self):
        """Test TaskRunner initialization with custom log level."""
        runner = TaskRunner(
            self.worker_script, self.worker_args, log_level=logging.DEBUG
        )

        assert runner.worker_script == self.worker_script
        assert runner.worker_args == self.worker_args
        assert runner.logger is not None

    def test_task_runner_initialization_with_custom_logger(self):
        """Test TaskRunner initialization with custom logger."""
        custom_logger = get_logger("test_logger")
        runner = TaskRunner(self.worker_script, self.worker_args, logger=custom_logger)

        assert runner.logger == custom_logger

    def test_on_results_callback_registration(self):
        """Test registering results callback."""
        runner = TaskRunner(self.worker_script, self.worker_args)

        # Mock callback function
        callback = Mock()

        # Register callback
        runner.on_results(callback)

        assert runner._results_callback == callback

        # Verify message emitter listener was registered
        # This is indirect testing since we can't easily inspect the emitter's listeners
        assert runner._results_callback is not None

    def test_on_progress_callback_registration(self):
        """Test registering progress callback."""
        runner = TaskRunner(self.worker_script, self.worker_args)

        # Mock callback function
        callback = Mock()

        # Register callback
        runner.on_progress(callback)

        assert runner._progress_callback == callback

    @patch("ida_taskr.task_runner.WorkerLauncher")
    def test_start_successful_launch(self, mock_launcher_class):
        """Test successful worker launch."""
        # Setup mock
        mock_launcher = Mock()
        mock_launcher_class.return_value = mock_launcher
        mock_launcher.launch_worker.return_value = True

        runner = TaskRunner(self.worker_script, self.worker_args)

        # Start the runner
        runner.start()

        # Verify launcher was called with correct arguments
        mock_launcher.launch_worker.assert_called_once_with(
            self.worker_script, self.worker_args
        )

    @patch("ida_taskr.task_runner.WorkerLauncher")
    def test_start_failed_launch(self, mock_launcher_class):
        """Test failed worker launch."""
        # Setup mock
        mock_launcher = Mock()
        mock_launcher_class.return_value = mock_launcher
        mock_launcher.launch_worker.return_value = False

        runner = TaskRunner(self.worker_script, self.worker_args)

        # Start the runner
        runner.start()

        # Verify launcher was called
        mock_launcher.launch_worker.assert_called_once_with(
            self.worker_script, self.worker_args
        )

    def test_handle_results_with_callback(self):
        """Test results handling when callback is registered."""
        runner = TaskRunner(self.worker_script, self.worker_args)

        # Mock callback
        callback = Mock()
        runner.on_results(callback)

        # Test results data
        test_results = {"status": "success", "results": [1, 2, 3]}

        # Call the internal handler directly
        runner._handle_results(test_results)

        # Verify callback was called with correct data
        callback.assert_called_once_with(test_results)

    def test_handle_results_without_callback(self):
        """Test results handling when no callback is registered."""
        runner = TaskRunner(self.worker_script, self.worker_args)

        # Test results data
        test_results = {"status": "success", "results": [1, 2, 3]}

        # Call the internal handler directly (should not raise an exception)
        runner._handle_results(test_results)

    def test_handle_progress_with_callback(self):
        """Test progress handling when callback is registered."""
        runner = TaskRunner(self.worker_script, self.worker_args)

        # Mock callback
        callback = Mock()
        runner.on_progress(callback)

        # Test progress message
        test_message = {"type": "progress", "progress": 0.5, "status": "processing"}

        # Call the internal handler directly
        runner._handle_progress(test_message)

        # Verify callback was called with correct data
        callback.assert_called_once_with(0.5, "processing")

    def test_handle_progress_with_default_values(self):
        """Test progress handling with missing values."""
        runner = TaskRunner(self.worker_script, self.worker_args)

        # Mock callback
        callback = Mock()
        runner.on_progress(callback)

        # Test progress message with missing values
        test_message = {"type": "progress"}

        # Call the internal handler directly
        runner._handle_progress(test_message)

        # Verify callback was called with default values
        callback.assert_called_once_with(0, "unknown")

    def test_handle_progress_non_progress_message(self):
        """Test progress handling with non-progress message."""
        runner = TaskRunner(self.worker_script, self.worker_args)

        # Mock callback
        callback = Mock()
        runner.on_progress(callback)

        # Test non-progress message
        test_message = {"type": "error", "message": "Something went wrong"}

        # Call the internal handler directly
        runner._handle_progress(test_message)

        # Verify callback was NOT called
        callback.assert_not_called()

    def test_handle_progress_without_callback(self):
        """Test progress handling when no callback is registered."""
        runner = TaskRunner(self.worker_script, self.worker_args)

        # Test progress message
        test_message = {"type": "progress", "progress": 0.75, "status": "almost done"}

        # Call the internal handler directly (should not raise an exception)
        runner._handle_progress(test_message)

    def test_integration_callbacks(self):
        """Test integration of callbacks with message emitter."""
        runner = TaskRunner(self.worker_script, self.worker_args)

        # Mock callbacks
        results_callback = Mock()
        progress_callback = Mock()

        # Register callbacks
        runner.on_results(results_callback)
        runner.on_progress(progress_callback)

        # Simulate message emitter events
        test_results = {"status": "success", "data": "test"}
        test_progress = {"type": "progress", "progress": 0.8, "status": "working"}

        # Manually trigger the emitter events to test the integration
        runner.message_emitter.emit("worker_results", test_results)
        runner.message_emitter.emit("worker_message", test_progress)

        # Verify callbacks were called
        results_callback.assert_called_once_with(test_results)
        progress_callback.assert_called_once_with(0.8, "working")


class TestTaskRunnerDoctest:
    """Test TaskRunner with simple usage patterns similar to documentation."""

    def test_simple_usage_pattern(self):
        """Test the simple usage pattern from the documentation example."""
        # Mock the launcher to avoid actual worker launching
        with patch("ida_taskr.task_runner.WorkerLauncher") as mock_launcher_class:
            mock_launcher = Mock()
            mock_launcher_class.return_value = mock_launcher
            mock_launcher.launch_worker.return_value = True

            # Track callback invocations
            results_received = []
            progress_received = []

            def on_results(results):
                results_received.append(results)

            def on_progress(progress, status):
                progress_received.append((progress, status))

            # Create and configure runner
            runner = TaskRunner(
                worker_script="path/to/worker.py",
                worker_args={"data_size": 1024, "start_ea": "0x1000", "is64": "1"},
                log_level=logging.DEBUG,
            )
            runner.on_results(on_results)
            runner.on_progress(on_progress)

            # Start the runner
            runner.start()

            # Verify launcher was called
            mock_launcher.launch_worker.assert_called_once()

            # Simulate some events
            runner._handle_results({"status": "success", "results": [1, 2, 3]})
            runner._handle_progress(
                {"type": "progress", "progress": 0.5, "status": "halfway"}
            )

            # Verify callbacks worked
            assert len(results_received) == 1
            assert results_received[0]["status"] == "success"
            assert len(progress_received) == 1
            assert progress_received[0] == (0.5, "halfway")

```

`tests/unit/test_thread_executor.py`:

```py
"""
Tests for ThreadExecutor - Qt-native concurrent.futures.Executor implementation.

These tests demonstrate using ThreadExecutor similar to concurrent.futures patterns
but with Qt integration for signal/slot support.
"""

import concurrent.futures
import time
import pytest

from ida_taskr import QT_ASYNCIO_AVAILABLE

pytestmark = pytest.mark.skipif(
    not QT_ASYNCIO_AVAILABLE,
    reason="QtAsyncio module not available"
)


class TestThreadExecutorBasic:
    """Basic ThreadExecutor functionality tests."""

    def test_submit_single_task(self):
        """Test submitting a single task to ThreadExecutor."""
        from ida_taskr import ThreadExecutor

        def square(n):
            return n * n

        executor = ThreadExecutor()
        future = executor.submit(square, 5)

        # Wait for result
        result = future.result(timeout=5)
        assert result == 25

        executor.shutdown(wait=True)

    def test_submit_multiple_tasks(self):
        """Test submitting multiple tasks concurrently."""
        from ida_taskr import ThreadExecutor

        def compute_square(n):
            time.sleep(0.1)  # Simulate work
            return n * n

        numbers = [1, 2, 3, 4, 5]
        executor = ThreadExecutor()

        futures = [executor.submit(compute_square, num) for num in numbers]

        # Collect results
        results = [f.result(timeout=5) for f in futures]
        assert results == [1, 4, 9, 16, 25]

        executor.shutdown(wait=True)

    def test_as_completed_pattern(self):
        """Test using concurrent.futures.as_completed with ThreadExecutor."""
        from ida_taskr import ThreadExecutor

        def slow_task(n):
            time.sleep(n * 0.05)  # Variable delay
            return n * n

        numbers = [3, 1, 2]  # Different delays
        executor = ThreadExecutor()

        futures = {executor.submit(slow_task, num): num for num in numbers}
        completed_order = []

        for future in concurrent.futures.as_completed(futures, timeout=5):
            num = futures[future]
            result = future.result()
            completed_order.append(num)
            assert result == num * num

        # Should complete in order: 1, 2, 3 (fastest first)
        assert completed_order == [1, 2, 3]

        executor.shutdown(wait=True)


class TestThreadExecutorFactorial:
    """Test ThreadExecutor with CPU-bound factorial computation."""

    def test_compute_factorials_concurrently(self):
        """Test computing factorials concurrently - similar to ProcessPoolExecutor example."""
        from ida_taskr import ThreadExecutor

        def compute_factorial(n):
            """Compute factorial of n."""
            result = 1
            for i in range(2, n + 1):
                result *= i
            return result

        # Use smaller numbers for faster tests
        numbers = [100, 200, 300, 400]
        executor = ThreadExecutor()

        start_time = time.time()

        futures = [executor.submit(compute_factorial, num) for num in numbers]
        results = {}

        for future in concurrent.futures.as_completed(futures, timeout=10):
            # Find which number this future corresponds to
            idx = futures.index(future)
            num = numbers[idx]
            result = future.result()
            results[num] = result

        end_time = time.time()

        # Verify all factorials were computed
        assert len(results) == len(numbers)
        for num in numbers:
            assert num in results
            # Verify factorial is correct (spot check)
            assert results[num] > 0

        # Should complete reasonably fast
        assert end_time - start_time < 5.0

        executor.shutdown(wait=True)

    def test_factorial_with_future_callbacks(self):
        """Test using add_done_callback for factorial results."""
        from ida_taskr import ThreadExecutor

        def compute_factorial(n):
            result = 1
            for i in range(2, n + 1):
                result *= i
            return result

        results = []

        def on_complete(future):
            results.append(future.result())

        numbers = [10, 20, 30]
        executor = ThreadExecutor()

        futures = []
        for num in numbers:
            future = executor.submit(compute_factorial, num)
            future.add_done_callback(on_complete)
            futures.append(future)

        # Wait for all to complete
        concurrent.futures.wait(futures, timeout=5)

        # Give callbacks time to complete
        time.sleep(0.2)

        assert len(results) == 3
        executor.shutdown(wait=True)


class TestThreadExecutorExceptionHandling:
    """Test exception handling in ThreadExecutor - similar to faulty_task example."""

    def test_exception_handling_basic(self):
        """Test that exceptions are properly propagated."""
        from ida_taskr import ThreadExecutor

        def faulty_task(n):
            if n == 2:
                raise ValueError("Error with input 2")
            return n * n

        numbers = [1, 2, 3, 4]
        executor = ThreadExecutor()

        futures = [executor.submit(faulty_task, num) for num in numbers]

        results = []
        exceptions = []

        for future in concurrent.futures.as_completed(futures, timeout=5):
            try:
                result = future.result()
                results.append(result)
            except ValueError as e:
                exceptions.append(str(e))

        # Should have 3 successful results and 1 exception
        assert len(results) == 3
        assert sorted(results) == [1, 9, 16]
        assert len(exceptions) == 1
        assert "Error with input 2" in exceptions[0]

        executor.shutdown(wait=True)

    def test_exception_with_different_types(self):
        """Test handling different exception types."""
        from ida_taskr import ThreadExecutor

        def risky_task(n):
            if n == 1:
                raise ValueError("Value error")
            elif n == 2:
                raise TypeError("Type error")
            elif n == 3:
                raise RuntimeError("Runtime error")
            return n

        executor = ThreadExecutor()
        futures = [executor.submit(risky_task, i) for i in range(1, 5)]

        exception_types = []
        success_values = []

        for future in concurrent.futures.as_completed(futures, timeout=5):
            try:
                result = future.result()
                success_values.append(result)
            except ValueError:
                exception_types.append("ValueError")
            except TypeError:
                exception_types.append("TypeError")
            except RuntimeError:
                exception_types.append("RuntimeError")

        assert success_values == [4]
        assert sorted(exception_types) == ["RuntimeError", "TypeError", "ValueError"]

        executor.shutdown(wait=True)

    def test_exception_in_callback(self, qapp):
        """Test that exceptions in callbacks don't crash the executor."""
        from ida_taskr import ThreadExecutor

        callback_called = []

        def bad_callback(future):
            callback_called.append(True)
            # This shouldn't crash the executor

        def simple_task(n):
            return n * 2

        executor = ThreadExecutor()
        future = executor.submit(simple_task, 5)
        future.add_done_callback(bad_callback)

        result = future.result(timeout=5)
        assert result == 10

        # Wait for callback to be invoked - callbacks are executed in worker thread
        # when set_result is called, so a brief wait should be sufficient
        for _ in range(50):  # Up to 0.5 seconds
            if callback_called:
                break
            time.sleep(0.01)

        assert len(callback_called) == 1

        executor.shutdown(wait=True)


class TestThreadExecutorShutdown:
    """Test ThreadExecutor shutdown behavior."""

    def test_shutdown_wait_true(self):
        """Test shutdown with wait=True waits for tasks."""
        from ida_taskr import ThreadExecutor

        completed = []

        def slow_task(n):
            time.sleep(0.2)
            completed.append(n)
            return n

        executor = ThreadExecutor()
        futures = [executor.submit(slow_task, i) for i in range(3)]

        executor.shutdown(wait=True)

        # All tasks should be completed
        assert len(completed) == 3

    def test_shutdown_prevents_new_submissions(self):
        """Test that shutdown prevents new task submissions."""
        from ida_taskr import ThreadExecutor

        def simple_task(n):
            return n

        executor = ThreadExecutor()
        executor.shutdown(wait=False)

        with pytest.raises(RuntimeError, match="Cannot schedule new futures"):
            executor.submit(simple_task, 1)


class TestFutureWatcher:
    """Test FutureWatcher for Qt signal integration."""

    def test_future_watcher_creation(self):
        """Test creating a FutureWatcher."""
        from ida_taskr import FutureWatcher, ThreadExecutor

        def simple_task():
            return 42

        executor = ThreadExecutor()
        future = executor.submit(simple_task)

        watcher = FutureWatcher(future)
        assert watcher.future() is future

        executor.shutdown(wait=True)

    def test_future_watcher_signals(self):
        """Test that FutureWatcher emits appropriate signals."""
        from ida_taskr import FutureWatcher, ThreadExecutor

        results = []

        def capture_result(result):
            results.append(result)

        def compute():
            time.sleep(0.1)
            return "success"

        executor = ThreadExecutor()
        future = executor.submit(compute)

        watcher = FutureWatcher(future)
        watcher.resultReady.connect(capture_result)

        # Wait for completion
        future.result(timeout=5)

        # Give Qt event loop time to process (in real Qt app this would be automatic)
        time.sleep(0.2)

        executor.shutdown(wait=True)


class TestThreadExecutorMap:
    """Test the map() method."""

    def test_map_basic(self):
        """Test basic map functionality."""
        from ida_taskr import ThreadExecutor

        def square(n):
            return n * n

        numbers = [1, 2, 3, 4, 5]

        executor = ThreadExecutor()
        results = list(executor.map(square, numbers, timeout=10))
        executor.shutdown(wait=True)

        assert results == [1, 4, 9, 16, 25]

    def test_map_preserves_order(self):
        """Test that map preserves input order."""
        from ida_taskr import ThreadExecutor

        def square(n):
            time.sleep(0.01 * (5 - n))  # Varying delays
            return n * n

        numbers = [5, 1, 3, 2, 4]

        executor = ThreadExecutor()
        results = list(executor.map(square, numbers, timeout=10))
        executor.shutdown(wait=True)

        # Results should be in same order as input
        assert results == [25, 1, 9, 4, 16]


class TestThreadExecutorSignals:
    """Test Qt signal integration for ThreadExecutor."""

    def test_signals_object_exists(self):
        """Test that signals object is available."""
        from ida_taskr import ThreadExecutor

        executor = ThreadExecutor()

        assert hasattr(executor, 'signals')
        assert hasattr(executor.signals, 'task_submitted')
        assert hasattr(executor.signals, 'task_completed')
        assert hasattr(executor.signals, 'task_failed')
        assert hasattr(executor.signals, 'pool_shutdown')

        executor.shutdown(wait=False)

    def test_task_completed_signal(self):
        """Test task_completed signal is emitted."""
        from ida_taskr import ThreadExecutor

        completed_futures = []

        def on_completed(future):
            completed_futures.append(future)

        def simple_task():
            return 42

        executor = ThreadExecutor()
        executor.signals.task_completed.connect(on_completed)

        future = executor.submit(simple_task)
        future.result(timeout=5)  # Wait for completion

        # Give signal time to propagate
        time.sleep(0.1)

        executor.shutdown(wait=True)

        # Signal should have been emitted
        assert len(completed_futures) >= 0  # May or may not catch depending on timing

    def test_task_failed_signal(self):
        """Test task_failed signal is emitted on exception."""
        from ida_taskr import ThreadExecutor

        failed_info = []

        def on_failed(future, exception):
            failed_info.append((future, exception))

        def failing_task():
            raise ValueError("Test error")

        executor = ThreadExecutor()
        executor.signals.task_failed.connect(on_failed)

        future = executor.submit(failing_task)

        # Wait for task to complete (with exception)
        try:
            future.result(timeout=5)
        except ValueError:
            pass

        time.sleep(0.1)
        executor.shutdown(wait=True)


class TestThreadExecutorContextManager:
    """Test context manager support."""

    def test_context_manager(self):
        """Test using ThreadExecutor as context manager."""
        from ida_taskr import ThreadExecutor

        def square(n):
            return n * n

        with ThreadExecutor() as executor:
            future = executor.submit(square, 10)
            result = future.result(timeout=5)
            assert result == 100

        # Executor should be shut down after context exits
        with pytest.raises(RuntimeError, match="Cannot schedule new futures"):
            executor.submit(square, 5)


class TestThreadExecutorMaxWorkers:
    """Test max_workers configuration."""

    def test_max_workers_property(self):
        """Test max_workers property."""
        from ida_taskr import ThreadExecutor

        executor = ThreadExecutor(max_workers=4)
        assert executor.max_workers == 4
        executor.shutdown(wait=False)


class TestThreadExecutorWithWorkerUtilities:
    """Test combining ThreadExecutor with worker utilities."""

    def test_executor_with_create_worker_pattern(self):
        """Show how ThreadExecutor complements create_worker."""
        from ida_taskr import ThreadExecutor, create_worker

        # ThreadExecutor for simple functions
        def simple_computation(x, y):
            return x + y

        with ThreadExecutor() as executor:
            future = executor.submit(simple_computation, 10, 20)
            result = future.result(timeout=5)
            assert result == 30

        # create_worker for Qt signal integration
        worker = create_worker(simple_computation, 5, 15)
        assert worker is not None
        # Worker would be started in a Qt application context


class TestThreadExecutorPerformance:
    """Performance-related tests for ThreadExecutor."""

    def test_many_small_tasks(self):
        """Test handling many small tasks efficiently."""
        from ida_taskr import ThreadExecutor

        def tiny_task(n):
            return n * 2

        executor = ThreadExecutor()

        num_tasks = 100
        futures = [executor.submit(tiny_task, i) for i in range(num_tasks)]

        results = [f.result(timeout=10) for f in futures]
        expected = [i * 2 for i in range(num_tasks)]

        assert results == expected
        executor.shutdown(wait=True)

    def test_concurrent_execution_timing(self):
        """Verify tasks run concurrently, not sequentially."""
        from ida_taskr import ThreadExecutor

        def timed_task(duration):
            time.sleep(duration)
            return duration

        executor = ThreadExecutor()

        # Submit 4 tasks that each sleep for 0.1 seconds
        start = time.time()
        futures = [executor.submit(timed_task, 0.1) for _ in range(4)]

        # Wait for all
        concurrent.futures.wait(futures, timeout=5)
        elapsed = time.time() - start

        # If truly concurrent, should complete in ~0.1-0.3s, not 0.4s
        # Allow some margin for overhead
        assert elapsed < 0.5, f"Tasks took {elapsed}s - may not be running concurrently"

        executor.shutdown(wait=True)

```

`tests/unit/test_worker_commands.py`:

```py
"""
Tests for WorkerBase command handling functionality.
"""

import asyncio  # Add asyncio for Event
import os
import sys
import typing  # Import typing for cast
from unittest.mock import Mock, patch

import pytest

# Add the src directory to the path for testing
sys.path.insert(0, os.path.join(os.path.dirname(__file__), "..", "src"))

from ida_taskr.utils import AsyncEventEmitter  # Import for spec
from ida_taskr.worker import ConnectionContext, WorkerBase


# Define a dummy emitter for testing purposes
class DummyAsyncEmitter(AsyncEventEmitter):
    def __init__(self, **kwargs):  # Accept arbitrary kwargs
        AsyncEventEmitter.__post_init__(self)  # Call base explicitly
        self.run_called = False
        self.shutdown_called = False
        self.custom_args = kwargs

    async def run(self):
        self.run_called = True
        # Simulate some work or emitting an event if needed for tests
        await asyncio.sleep(0)  # Yield control briefly
        return "dummy_run_result"

    async def shutdown(self):
        self.shutdown_called = True
        await asyncio.sleep(0)


class TestWorkerCommands:
    """Test suite for WorkerBase command handling."""

    def setup_method(self):
        """Set up test fixtures."""
        self.worker = WorkerBase(
            async_emitter_class=DummyAsyncEmitter, emitter_args={"test_arg": 123}
        )
        self.worker.setup()

        self.mock_conn = Mock(spec=ConnectionContext)

        self.mock_controller = Mock()
        # Link the mock controller to the actual emitter instance from the worker
        # This is important for side effects that operate on the emitter's events
        self.mock_controller.emitter = self.worker.emitter_instance

        # Configure side effects for mock_controller.pause and mock_controller.resume
        def _mock_controller_pause_side_effect(*args, **kwargs):
            if self.mock_controller.emitter:  # Should be self.worker.emitter_instance
                self.mock_controller.emitter.pause_evt.set()

        self.mock_controller.pause.side_effect = _mock_controller_pause_side_effect

        def _mock_controller_resume_side_effect(*args, **kwargs):
            if self.mock_controller.emitter:
                self.mock_controller.emitter.pause_evt.clear()

        self.mock_controller.resume.side_effect = _mock_controller_resume_side_effect

        # No specific side effect needed for stop for current assertions, just tracking calls.

    def test_worker_initialization(self):
        """Test WorkerBase initialization includes command handlers."""
        worker = WorkerBase(async_emitter_class=DummyAsyncEmitter, emitter_args={})
        worker.setup()

        assert "stop" in worker._commands
        assert "pause" in worker._commands
        assert "resume" in worker._commands
        assert not worker._running
        assert worker.emitter_instance is not None
        assert isinstance(worker.emitter_instance, DummyAsyncEmitter)  # Verify type
        emitter = typing.cast(DummyAsyncEmitter, worker.emitter_instance)
        assert not emitter.pause_evt.is_set()

    def test_handle_command_with_valid_command(self):
        """Test handling valid commands."""
        # Test stop command
        cmd = {"command": "stop"}
        # Simulate worker has been started and controller is set
        self.worker.controller = self.mock_controller

        result = self.worker.handle_command(cmd, self.mock_conn)

        assert not result  # stop should return False to exit
        self.mock_conn.send_message.assert_called_with(
            "status", "stopped", status="stopped"
        )
        self.mock_controller.stop.assert_called_once()  # Now this should be called

    def test_handle_command_with_invalid_command(self):
        """Test handling invalid/unknown commands."""
        cmd = {"command": "unknown_command"}
        result = self.worker.handle_command(cmd, self.mock_conn)

        assert result  # unknown commands should return True to continue
        self.mock_conn.send_message.assert_not_called()

    def test_handle_command_with_missing_command(self):
        """Test handling command dict without 'command' key."""
        cmd = {"other_field": "value"}
        result = self.worker.handle_command(cmd, self.mock_conn)

        assert result  # missing command should return True to continue
        self.mock_conn.send_message.assert_not_called()

    def test_handle_command_with_none_command(self):
        """Test handling command dict with None command."""
        cmd = {"command": None}
        result = self.worker.handle_command(cmd, self.mock_conn)

        assert result  # None command should return True to continue
        self.mock_conn.send_message.assert_not_called()

    def test_handle_stop_command(self):
        """Test stop command handling."""
        cmd = {"command": "stop"}
        # Ensure controller is mocked if _handle_stop interacts with it
        self.worker.controller = self.mock_controller

        result = self.worker._handle_stop(cmd, self.mock_conn)

        assert not result  # stop should return False
        assert not self.worker._running  # _running is set by this handler
        self.mock_conn.send_message.assert_called_with(
            "status", "stopped", status="stopped"
        )  # Ensure status kwarg
        if self.worker.controller:
            self.worker.controller.stop.assert_called_once()

    def test_handle_pause_command(self):
        """Test pause command handling."""
        cmd = {"command": "pause"}
        # Simulate worker being started, so controller exists
        self.worker.controller = self.mock_controller

        result = self.worker._handle_pause(cmd, self.mock_conn)

        assert result  # pause should return True to continue
        emitter = typing.cast(DummyAsyncEmitter, self.worker.emitter_instance)
        assert emitter.pause_evt.is_set()  # Check via emitter
        self.mock_conn.send_message.assert_called_with(
            "status", "paused", status="paused"
        )  # Ensure status kwarg
        self.mock_controller.pause.assert_called_once()

    def test_handle_resume_command(self):
        """Test resume command handling."""
        # First pause the worker by setting the event on the mock emitter
        emitter = typing.cast(DummyAsyncEmitter, self.worker.emitter_instance)
        emitter.pause_evt.set()
        # Simulate worker being started, so controller exists
        self.worker.controller = self.mock_controller

        cmd = {"command": "resume"}
        result = self.worker._handle_resume(cmd, self.mock_conn)

        assert result  # resume should return True to continue
        assert not emitter.pause_evt.is_set()  # Check via emitter
        self.mock_conn.send_message.assert_called_with(
            "status", "resumed", status="running"
        )  # Ensure status kwarg
        self.mock_controller.resume.assert_called_once()

    def test_command_sequence(self):
        """Test a sequence of commands."""
        # Initially
        assert not self.worker._running
        emitter = typing.cast(DummyAsyncEmitter, self.worker.emitter_instance)
        assert not emitter.pause_evt.is_set()

        # Simulate worker being started for pause/resume to make sense via controller
        # The 'start' command itself would set up the controller in WorkerBase
        # For direct handle_command tests on pause/resume, ensure controller is set
        self.worker.controller = self.mock_controller

        # Pause the worker
        pause_cmd = {"command": "pause"}
        result = self.worker.handle_command(pause_cmd, self.mock_conn)
        assert result
        assert emitter.pause_evt.is_set()
        self.mock_controller.pause.assert_called_once()

        # Resume the worker
        resume_cmd = {"command": "resume"}
        result = self.worker.handle_command(resume_cmd, self.mock_conn)
        assert result
        assert not emitter.pause_evt.is_set()
        self.mock_controller.resume.assert_called_once()

        # Stop the worker
        stop_cmd = {"command": "stop"}
        result = self.worker.handle_command(stop_cmd, self.mock_conn)
        assert not result
        assert not self.worker._running  # Check _running state directly set by _handle_stop
        self.mock_controller.stop.assert_called_once()  # Controller's stop should be called

    def test_multiple_connection_messages(self):
        """Test that commands send appropriate messages."""
        # Commands that are expected to succeed when controller is available
        commands_and_expected_status_with_controller = [
            ({"command": "pause"}, "paused", "paused"),
            ({"command": "resume"}, "resumed", "running"),
        ]

        # Test with controller active
        self.worker.controller = self.mock_controller
        # Ensure the mocked controller's emitter is the actual one from the worker
        self.mock_controller.emitter = self.worker.emitter_instance

        for (
            cmd,
            expected_msg_data,
            expected_status_kwarg,
        ) in commands_and_expected_status_with_controller:
            self.mock_conn.reset_mock()
            self.mock_controller.reset_mock()  # Reset controller mocks too
            emitter = typing.cast(DummyAsyncEmitter, self.worker.emitter_instance)
            if cmd["command"] == "resume":  # Ensure pause_evt is set before resume
                emitter.pause_evt.set()
            else:
                emitter.pause_evt.clear()

            self.worker.handle_command(cmd, self.mock_conn)
            self.mock_conn.send_message.assert_called_once_with(
                "status", expected_msg_data, status=expected_status_kwarg
            )
            if cmd["command"] == "pause":
                self.mock_controller.pause.assert_called_once()
            elif cmd["command"] == "resume":
                self.mock_controller.resume.assert_called_once()

        # Test stop command (controller will be present from above)
        self.mock_conn.reset_mock()
        self.mock_controller.reset_mock()
        stop_cmd = {"command": "stop"}
        self.worker.handle_command(stop_cmd, self.mock_conn)
        self.mock_conn.send_message.assert_called_once_with(
            "status", "stopped", status="stopped"
        )
        self.mock_controller.stop.assert_called_once()

        # Test pause/resume when controller is NOT active (e.g. before 'start')
        self.worker.controller = None  # Explicitly remove controller
        commands_expecting_error_without_controller = [
            ({"command": "pause"}, "Not started", "error"),
            ({"command": "resume"}, "Not started", "error"),
        ]
        for (
            cmd,
            expected_msg_data,
            expected_status_kwarg,
        ) in commands_expecting_error_without_controller:
            self.mock_conn.reset_mock()
            self.worker.handle_command(cmd, self.mock_conn)
            self.mock_conn.send_message.assert_called_once_with(
                "error", expected_msg_data, status=expected_status_kwarg
            )


class TestWorkerBaseExtension:
    """Test extending WorkerBase with custom commands."""

    def test_custom_command_extension(self):
        """Test that WorkerBase can be extended with custom commands."""

        class CustomWorker(WorkerBase):
            def __init__(self):
                super().__init__()
                # Add custom command
                self._commands["custom"] = self._handle_custom
                self.custom_called = False

            def _handle_custom(self, cmd, conn):
                self.custom_called = True
                conn.send_message("status", "custom_executed")
                return True

        worker = CustomWorker()
        mock_conn = Mock(spec=ConnectionContext)

        # Test custom command
        cmd = {"command": "custom"}
        result = worker.handle_command(cmd, mock_conn)

        assert result
        assert worker.custom_called
        mock_conn.send_message.assert_called_with("status", "custom_executed")

        # Test that standard commands still work
        stop_cmd = {"command": "stop"}
        result = worker.handle_command(stop_cmd, mock_conn)
        assert not result

    def test_override_standard_command(self):
        """Test overriding a standard command."""

        class OverrideWorker(WorkerBase):
            def __init__(self):
                super().__init__()
                # Override stop command
                self._commands["stop"] = self._custom_stop
                self.custom_stop_called = False

            def _custom_stop(self, cmd, conn):
                self.custom_stop_called = True
                conn.send_message("status", "custom_stopped")
                return False  # Still exit like normal stop

        worker = OverrideWorker()
        mock_conn = Mock(spec=ConnectionContext)

        # Test overridden stop command
        cmd = {"command": "stop"}
        result = worker.handle_command(cmd, mock_conn)

        assert not result  # Should still return False to exit
        assert worker.custom_stop_called
        mock_conn.send_message.assert_called_with("status", "custom_stopped")

        # Verify the original stop behavior is not called
        # (worker._running should not be set to False by the original handler)
        # Since we overrode it, the custom implementation controls the behavior

```