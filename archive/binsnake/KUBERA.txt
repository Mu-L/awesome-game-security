Project Path: arc_binsnake_KUBERA_eppjykvo

Source Tree:

```txt
arc_binsnake_KUBERA_eppjykvo
├── CMakeLists.txt
├── CONTRIBUTING.md
├── KUBERA.hpp
├── LICENSE
├── README.md
├── configuration.hpp
├── deps
│   ├── boost_config
│   ├── boost_multiprecision
│   └── icedpp
├── emulator.hpp
├── example
│   └── main.cpp
├── iced.hpp
├── iced_internal.hpp
├── memory.hpp
├── references.txt
├── sign_extend.hpp
├── src
│   ├── handlers
│   │   ├── arithmetic.cpp
│   │   ├── bit.cpp
│   │   ├── bitwise_logical.cpp
│   │   ├── cmov.cpp
│   │   ├── compare.cpp
│   │   ├── control_flow.cpp
│   │   ├── data.cpp
│   │   ├── floating_80.cpp
│   │   ├── floating_point.cpp
│   │   ├── helpers.cpp
│   │   ├── helpers.hpp
│   │   ├── misc.cpp
│   │   ├── set.cpp
│   │   ├── simd.cpp
│   │   ├── stack_frame.cpp
│   │   ├── string.cpp
│   │   ├── syscall.cpp
│   │   └── system.cpp
│   ├── kubera.cpp
│   └── virtual_memory.cpp
├── types.hpp
└── x86.hpp

```

`CMakeLists.txt`:

```txt
cmake_minimum_required(VERSION 3.10)
project(KUBERA LANGUAGES CXX)

set(CMAKE_CXX_STANDARD 23)
set(CMAKE_CXX_STANDARD_REQUIRED ON)
set(CMAKE_CXX_EXTENSIONS OFF)

# Collect header files
file(GLOB HEADERS
    ${CMAKE_CURRENT_SOURCE_DIR}/*.hpp
    ${CMAKE_CURRENT_SOURCE_DIR}/*.h
)

# Source files
set(SOURCES
    ${CMAKE_CURRENT_SOURCE_DIR}/src/kubera.cpp
    ${CMAKE_CURRENT_SOURCE_DIR}/src/virtual_memory.cpp
    ${CMAKE_CURRENT_SOURCE_DIR}/src/handlers/arithmetic.cpp
    ${CMAKE_CURRENT_SOURCE_DIR}/src/handlers/bit.cpp
    ${CMAKE_CURRENT_SOURCE_DIR}/src/handlers/bitwise_logical.cpp
    ${CMAKE_CURRENT_SOURCE_DIR}/src/handlers/cmov.cpp
    ${CMAKE_CURRENT_SOURCE_DIR}/src/handlers/compare.cpp
    ${CMAKE_CURRENT_SOURCE_DIR}/src/handlers/control_flow.cpp
    ${CMAKE_CURRENT_SOURCE_DIR}/src/handlers/data.cpp
    ${CMAKE_CURRENT_SOURCE_DIR}/src/handlers/floating_80.cpp
    ${CMAKE_CURRENT_SOURCE_DIR}/src/handlers/floating_point.cpp
    ${CMAKE_CURRENT_SOURCE_DIR}/src/handlers/helpers.cpp
    ${CMAKE_CURRENT_SOURCE_DIR}/src/handlers/misc.cpp
    ${CMAKE_CURRENT_SOURCE_DIR}/src/handlers/set.cpp
    ${CMAKE_CURRENT_SOURCE_DIR}/src/handlers/simd.cpp
    ${CMAKE_CURRENT_SOURCE_DIR}/src/handlers/stack_frame.cpp
    ${CMAKE_CURRENT_SOURCE_DIR}/src/handlers/string.cpp
    ${CMAKE_CURRENT_SOURCE_DIR}/src/handlers/syscall.cpp
    ${CMAKE_CURRENT_SOURCE_DIR}/src/handlers/system.cpp
)

message(STATUS "Found sources: ${SOURCES}")

# Add the icedpp subdirectory
add_subdirectory(${CMAKE_CURRENT_SOURCE_DIR}/deps/icedpp)

# Define the KUBERA static library
add_library(${PROJECT_NAME} STATIC ${SOURCES} ${HEADERS})
target_include_directories(${PROJECT_NAME} PUBLIC
    ${CMAKE_CURRENT_SOURCE_DIR}
    ${CMAKE_CURRENT_SOURCE_DIR}/deps/boost_multiprecision/include
    ${CMAKE_CURRENT_SOURCE_DIR}/deps/boost_config/include
)

# Link icedpp to KUBERA
target_link_libraries(${PROJECT_NAME} PUBLIC icedpp)

# Compiler definitions
target_compile_definitions(${PROJECT_NAME} PUBLIC BOOST_MP_STANDALONE)

```

`CONTRIBUTING.md`:

```md
# Contributing to KUBERA

Thank you for your interest in contributing to KUBERA! We welcome contributions to improve this x86 software emulator. Please follow the guidelines below to ensure a smooth collaboration.

## How to Contribute

1. **Fork the Repository**: Create a fork of the [KUBERA repository](https://github.com/[your-repo]/kubera) on GitHub.
2. **Clone Your Fork**: Clone your fork to your local machine.
3. **Create a Branch**: Use a descriptive branch name (e.g., `feature/add-avx512-support` or `fix/memory-leak`).
4. **Make Changes**: Implement your feature, bug fix, or improvement.
5. **Test Your Changes**: Ensure your changes work as intended and do not introduce new issues. Test within an isolated VM environment.
6. **Commit Changes**: Write clear, concise commit messages following the [Conventional Commits](https://www.conventionalcommits.org/) format (e.g., `feat: add AVX512 emulation support`).
7. **Push to Your Fork**: Push your branch to your forked repository.
8. **Submit a Pull Request**: Open a pull request (PR) against the main repository’s `main` branch. Include a detailed description of your changes and reference any related issues.

## Code Style and Formatting

- Follow the formatting rules specified in the [`.editorconfig`](.editorconfig) file. Use an editor that supports EditorConfig to enforce these settings.
- Write clean, readable code with meaningful variable names and comments where necessary.
- Ensure consistency with the existing codebase (e.g., use spaces, not tabs, and follow naming conventions).

## Contribution Guidelines

- **Scope**: Contributions should align with KUBERA’s goals (x86 emulation, detailed analysis, isolation). Major feature proposals should be discussed in an issue first.
- **Testing**: Test your changes thoroughly, especially for edge cases involving different x86 architectures or operating systems. Include test cases if applicable.
- **Documentation**: Update relevant documentation (e.g., README, code comments) for new features or changes.
- **Licensing**: By contributing, you agree that your contributions are licensed under the [Creative Commons Attribution-NonCommercial 4.0 International License](https://creativecommons.org/licenses/by-nc/4.0/), as specified in the [LICENSE](LICENSE) file.

## Reporting Issues

- Use the GitHub Issues tab to report bugs or suggest features.
- Provide a clear title, detailed description, steps to reproduce (for bugs), and any relevant logs or screenshots.
- Check for existing issues to avoid duplicates.

## Code of Conduct

- Be respectful and inclusive in all interactions.
- Follow GitHub’s [Community Guidelines](https://docs.github.com/en/site-policy/github-terms/github-community-guidelines).
- Avoid submitting code that violates third-party intellectual property rights or introduces security risks.

## Contact

For questions or clarification, contact REAPS, s.r.o. at reapsgg@proton.me. For significant changes, open an issue to discuss before starting work.

Thank you for helping make KUBERA better!
```

`KUBERA.hpp`:

```hpp
#pragma once

#include <memory>
#include <array>
#include <functional>
#include <optional>
#include <cstdint>
#include "sign_extend.hpp"

#include "iced.hpp"
#include "configuration.hpp"
#include "types.hpp"
#include "memory.hpp"

#ifdef min
#undef min
#endif

#ifdef max
#undef max
#endif

#ifndef UNREACHABLE
#ifdef _MSC_VER
#define UNREACHABLE() __assume(0)
#else
#define UNREACHABLE() __builtin_unreachable()
#endif
#endif

#define GET_OPERAND_MASK(y) (~0ULL >> (64 - (y) * 8))
#define ALIGN_DOWN(var, align) var & ~(align - 1)

namespace kubera
{
	// Type alias for instruction handler function
	using InstructionHandler = void ( * ) ( const iced::Instruction&, class KUBERA& state );

	// Type alias for array of instruction handlers
	using InstructionHandlerList = std::array<InstructionHandler, static_cast< std::size_t > ( Mnemonic::COUNT )>;

	// Optional function to override target address
	inline bool ( *platform_target_override )( uint64_t ) = nullptr;

	// Pointer to the instruction dispatch table
	inline std::unique_ptr<InstructionHandlerList> instruction_dispatch_table = nullptr;
	// Memory Management Unit
	inline std::unique_ptr<VirtualMemory> memory = nullptr;

	class KUBERA {
	private:
		std::unique_ptr<CPU> cpu = nullptr;
		uint8_t instr_buffer [ 15 ] = { 0 };

	public:
		std::unique_ptr<iced::Decoder> decoder = nullptr;
		KUBERA ( );
		~KUBERA ( ) = default;

		uint64_t alloc_memory ( std::size_t size, uint8_t prot, std::size_t alignment = 0x1000 ) {
			return memory->alloc ( size, prot, alignment );
		}

		uint64_t load_memory ( const void* data, std::size_t size, uint8_t prot, std::size_t alignment = 0x1000 ) {
			return memory->load ( data, size, prot, alignment );
		}

		kubera::VirtualMemory* get_virtual_memory ( ) noexcept {
			return memory.get ( );
		}

		uint64_t stack_base ( ) const noexcept {
			return cpu->stack_base;
		}

		uint64_t stack_limit ( ) const noexcept {
			return cpu->stack_base + cpu->stack_size;
		}

		// Returns a mutable reference to cpu->rflags
		// Warning! This function can overwrite reserved bits!
		x86::Flags& get_flags ( ) noexcept {
			return cpu->rflags;
		}

		// Returns a mutable reference to cpu->mxcsr
		x86::Mxcsr& get_mxcsr ( ) noexcept {
			return cpu->mxcsr;
		}

		// Returns the current privilege level
		uint8_t& get_cpl ( ) const noexcept {
			return cpu->current_privilege_level;
		}

		// Increments the timestamp counter
		void increment_tsc ( size_t amount = 2 ) noexcept {
			cpu->increment_tsc ( amount );
		}

		// Returns a reference to the Floating Point Unit
		FPU& get_fpu ( ) noexcept {
			return cpu->fpu;
		}

		// Reads the timestamp counter
		uint64_t read_tsc ( ) const noexcept {
			return cpu->timestamp_counter;
		}

		// Returns a mutable reference to the RIP register
		uint64_t& rip ( ) noexcept {
			return cpu->registers [ KubRegister::RIP ];
		}

		// Returns the value of the RIP register
		uint64_t rip ( ) const noexcept {
			return cpu->registers [ KubRegister::RIP ];
		}

		std::size_t fetch_instruction_bytes ( uint64_t addr, uint8_t* buffer, std::size_t max_bytes ) {
			std::size_t fetched = 0;
			uint64_t current = addr;
			while ( fetched < max_bytes ) {
				void* src = memory->translate ( current, PageProtection::EXEC | PageProtection::READ );
				if ( !src ) {
					return 0;
				}
				std::size_t offset = current % memory->page_size;
				std::size_t to_copy = std::min ( max_bytes - fetched, memory->page_size - offset );
				std::memcpy ( buffer + fetched, src, to_copy );
				fetched += to_copy;
				current += to_copy;
			}
			return fetched;
		}

		// Emulates the instruction and updates the decoder
		void reconfigure ( uint64_t new_rip ) {
			rip ( ) = new_rip;
			std::size_t bytes_fetched = fetch_instruction_bytes ( new_rip, instr_buffer, 15 );
			if ( bytes_fetched == 0 ) {
				std::println ( "!!!!! FAILED TO FETCH INSTRUCTIONS" );
			}
			decoder->reconfigure ( instr_buffer, bytes_fetched, new_rip );
		}

		// Allocate Type on stack
		template <typename Type>
		Type* allocate_on_stack ( ) {
			uint64_t stack = get_reg_internal<KubRegister::RSP, Register::RSP, uint64_t> ( );
			stack = ALIGN_DOWN ( stack - sizeof ( Type ), std::max ( alignof( Type ), alignof( void* ) ) );
			set_reg_internal<KubRegister::RSP, Register::RSP, uint64_t> ( stack );
			return reinterpret_cast< Type* >( stack );
		}

		void unalign_stack ( ) {
			auto stack = get_reg_internal<KubRegister::RSP, Register::RSP, uint64_t> ( );
			stack = ALIGN_DOWN ( stack - 0x10, 0x10 ) + 0x8;
			set_reg_internal<KubRegister::RSP, Register::RSP, uint64_t> ( stack );
		}

		// Handles instruction pointer switch
		void handle_ip_switch ( uint64_t );

		// Returns the access mask for a register
		uint64_t get_access_mask ( Register reg, size_t size ) const noexcept;

		// Returns the access shift for a register
		uint8_t get_access_shift ( Register reg, size_t size ) const noexcept;

		// Returns the value of RFLAGS
		uint64_t get_rflags ( ) const noexcept;

		// Returns the value of a specified register
		uint64_t get_reg ( Register reg, size_t size = 8 ) const noexcept;

		// Executes an instruction using the dispatch table
		inline void execute ( const iced::Instruction& instr ) {
			( *kubera::instruction_dispatch_table ) [ static_cast< size_t >( instr.mnemonic ( ) ) ] ( instr, *this );
		}

		iced::Instruction& emulate ( ) {
			auto old_rip = rip ( );
			reconfigure ( rip ( ) );
			auto& instr = decoder->decode ( );
			execute ( instr );
			if ( rip ( ) == old_rip ) {
				rip ( ) += instr.length ( );
			}
			increment_tsc ( );
			return instr;
		}

		// Template function to read data of specified type from memory
		template <typename Type>
		Type read_type ( uint64_t address ) const {
			static_assert( !std::is_same_v<Type, float80_t>, "Use read_type_float80_t to read a float80_t" );
			return memory->read<Type> ( address );
		}

		// Reads 80-bit floating-point data from memory
		float80_t read_type_float80_t ( uint64_t address ) const;

		// Template function to write data of specified type to memory
		template <typename Type>
		void write_type ( uint64_t address, Type val ) {
			memory->write<Type> ( address, val );
		}

		// Template function to read data from stack with bounds checking
		template <typename Type>
		Type get_stack ( uint64_t address ) const {
			if ( !is_within_stack_bounds ( address, sizeof ( Type ) ) ) {
				// TODO: Implement exception handling
				return Type ( 0 );
			}

			return read_type<Type> ( address );
		}

		// Template function to read data from memory with permission checking
		template <typename Type>
		Type get_memory ( uint64_t address ) const {
			return read_type<Type> ( address );
		}

		// Sets the RFLAGS register
		void set_rflags ( uint64_t rflags ) noexcept;

		// Sets the value of a specified register
		void set_reg ( Register reg, uint64_t val, size_t size );

		// Template function to write data to stack with bounds checking
		template <typename Type>
		void set_stack ( uint64_t address, Type val ) {
			if ( !is_within_stack_bounds ( address, sizeof ( Type ) ) ) {
				return;
			}

			return write_type<Type> ( address, val );
		}

		// Template function to write data to memory with permission checking
		template <typename Type>
		void set_memory ( uint64_t address, Type val ) {
			return write_type<Type> ( address, val );
		}

		// Checks if an address is within stack bounds
		bool is_within_stack_bounds ( uint64_t address, size_t size ) const noexcept;

		// Retrieves raw XMM register value
		uint128_t get_xmm_raw ( Register reg ) const;

		// Sets raw XMM register value
		void set_xmm_raw ( Register reg, const uint128_t& value );

		// Retrieves raw YMM register value
		uint256_t get_ymm_raw ( Register reg ) const;

		// Sets raw YMM register value
		void set_ymm_raw ( Register reg, const uint256_t& value );

		// Retrieves raw ZMM register value
		uint512_t get_zmm_raw ( Register reg ) const;

		// Sets raw ZMM register value
		void set_zmm_raw ( Register reg, const uint512_t& value );

		// Retrieves XMM register value as float
		float get_xmm_float ( Register reg ) const;

		// Sets XMM register value as float
		void set_xmm_float ( Register reg, float value );

		// Retrieves XMM register value as double
		double get_xmm_double ( Register reg ) const;

		// Sets XMM register value as double
		void set_xmm_double ( Register reg, double value );

		template <Register reg, size_t size>
		static constexpr uint64_t get_access_mask_internal ( ) noexcept {
			switch ( size ) {
				case 8:
					return 0xFFFFFFFFFFFFFFFFULL;
				case 4:
					return 0x00000000FFFFFFFFULL;
				case 2:
					return 0x000000000000FFFFULL;
				case 1:
					if constexpr ( reg == Register::CH || reg == Register::DH || reg == Register::BH || reg == Register::AH ) {
						return 0x000000000000FF00ULL;
					}
					return 0x00000000000000FFULL;
				default:
					return 0x0000000000000000ULL;
			}
		}

		template <Register reg, size_t size>
		static constexpr uint8_t get_access_shift_internal ( ) noexcept {
			if constexpr ( reg == Register::AH || reg == Register::BH || reg == Register::CH || reg == Register::DH ) {
				return 8;
			}

			return 0;
		}

		// An internal helper to get a register which is known at compile-time with less overhead
		template <KubRegister reg, Register iced_reg, typename Type>
		Type get_reg_internal ( ) {
			if constexpr ( reg == KubRegister::RIP ) {
				return static_cast< Type >( rip ( ) );
			}

			constexpr auto access_mask = get_access_mask_internal<iced_reg, sizeof ( Type )> ( );
			constexpr auto shift = get_access_shift_internal<iced_reg, sizeof ( Type )> ( );
			const auto concrete_full = cpu->registers [ reg ];
			const Type extracted_value = ( concrete_full & access_mask ) >> shift;

			return extracted_value;
		}

		// An internal helper to set a register which is known at compile-time with less overhead
		template <KubRegister reg, Register iced_reg, typename Type>
		void set_reg_internal ( Type value ) {
			const auto old_full = cpu->registers [ reg ];
			constexpr auto access_mask = get_access_mask_internal<iced_reg, sizeof ( Type )> ( );
			constexpr auto shift = get_access_shift_internal<iced_reg, sizeof ( Type )> ( );
			constexpr auto mask = ( ~0ULL >> ( 64 - ( sizeof ( Type ) ) * 8 ) );

			if constexpr ( sizeof ( Type ) == 4 && ( reg >= KubRegister::RAX && reg <= KubRegister::R15 ) ) {
				value &= 0xFFFFFFFFULL;
			}
			else {
				value = ( value & ~access_mask ) | ( ( ( value & mask ) << shift ) & access_mask );
			}

			cpu->registers [ reg ] = value;
		}

		template <KubRegister reg, typename Type>
		Type get_reg_direct ( ) {
			if constexpr ( reg == KubRegister::RIP ) {
				return static_cast< Type >( rip ( ) );
			}

			const auto concrete_full = cpu->registers [ reg ];
			return concrete_full;
		}

		// An internal helper to set a register which is known at compile-time with less overhead
		template <KubRegister reg, typename Type>
		void set_reg_direct ( Type value ) {
			const auto old_full = cpu->registers [ reg ];
			cpu->registers [ reg ] = value;
		}

		static constexpr std::array<std::string_view, KubRegister::COUNT> register_names = {
			"RAX", "RBX", "RCX", "RDX", "RSI", "RDI", "RBP", "RSP",
			"R8", "R9", "R10", "R11", "R12", "R13", "R14", "R15",
			"RIP",
			"DR0", "DR1", "DR2", "DR3", "DR4", "DR5", "DR6", "DR7",
			"CR0", "CR2", "CR3", "CR4", "CR8",
			"ES", "CS", "SS", "DS", "FS", "GS"
		};

		std::array<std::uint64_t, KubRegister::COUNT> register_dump ( ) const noexcept {
			return cpu->registers;
		}

		x86::Flags rflags_dump ( ) const noexcept {
			return cpu->rflags;
		}

		x86::Mxcsr mxcsr_dump ( ) const noexcept {
			return cpu->mxcsr;
		}

		std::vector<std::string> get_register_changes ( const std::array<std::uint64_t, KubRegister::COUNT>& old_registers ) const;
		std::vector<std::string> get_rflags_changes ( const x86::Flags& old_rflags ) const;
		std::vector<std::string> get_mxcsr_changes ( const x86::Mxcsr& old_mxcsr ) const;
	};
}
```

`LICENSE`:

```
MIT License

Copyright (c) 2025 binsnake

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

```

`README.md`:

```md
# KUBERA
KUBERA is a concrete x86_64 software emulator focused on detailed analysis and control. You can also check out [winhorse](https://github.com/binsnake/winhorse), which utilizes KUBERA to emulate Windows applications.
It is aiming to be platform-independent and designed for research.


Disclaimer: This project is heavily work-in-progress. Development has been ongoing for several months.

## Purpose
KUBERA provides deterministic, reversible, and verbose emulation, offering maximum insight into execution flow, including stack, memory, and register changes. 
It includes a user-friendly SDK to intercept operations and aims to prevent emulation detection by software. Key use cases:

Analyze software interactions with operating systems.

Inspect functionality at the instruction level.

Emulate x86 software natively, including extensions (AVX2, AVX512, APX) on legacy hardware.

Examine edge cases across x86 CPU architectures (contribution-dependent).


## Features

Detailed execution tracing (stack, memory, registers).

Support for x86 extensions on unsupported hardware.

SDK for operation interception and analysis.

Focus on isolation (ongoing development).

## Usage Warnings
KUBERA is not a secure sandbox for malicious software. It may allow host data access or VM escapes, potentially enabling malicious code execution. 

Always run KUBERA and emulated applications in an isolated environment (e.g., VMWare, VirtualBox, QEMU). 

KUBERA is a research tool in early development, not sponsored, and not guaranteed to be safe or complete.

Third-Party Software

KUBERA references third-party software names for binary initialization. These names and binaries are the property of their respective legal entities or developers, and REAPS, s.r.o. claims no ownership. No binaries are distributed with this project.

A lot of the code is very experimental and prone to failure. KUBERA is mostly an ongoing learning project towards CPU & OS internals.

## Dependencies
KUBERA includes the following open-source libraries:

ImGui - Licensed under the MIT License.

Capstone - Licensed under the BSD 3-Clause License.

Boost libraries - Licensed under the BSL-1.0 License.

See the LICENSE file for full license texts and copyright notices.

## License
This project is licensed under the MIT license, effective from the commit including the LICENSE file. See the LICENSE file for details.

## Tests
It is recommended that new instructions are tested vs hardware, we use https://github.com/ZehMatt/x86Tester/ to generate instruction combinations that we test our framework against.

## Contributing
Contributions are welcome! Please follow the naming conventions and formatting specified in the .editorconfig file. For detailed guidelines, see CONTRIBUTING.md (if available) or contact REAPS, s.r.o.

## Similar projects

https://github.com/icicle-emu/icicle-emu - Icicle is an emulator written in Rust

https://github.com/unicorn-engine/unicorn

https://github.com/momo5502/sogen - User-space emulator using Icicle/Emulator. This has been a great inspiration for KUBERA's Windows emulator and some strategies were used as a reference. Make sure to check it out!

https://github.com/x86matthew/WinVisor

## Usage Disclaimer

KUBERA is provided "as is" for research purposes only. REAPS, s.r.o. is not liable for any damages, data loss, or security breaches resulting from its use. Users are responsible for ensuring compliance with all applicable laws and third-party intellectual property rights when using KUBERA or emulating third-party software.

## Artificially generated content disclaimer

Parts of KUBERA were made with AI-assistance, in the form of full-project context with Gemini 2.5 Pro.
Due to this reason, 'semantics' can't fall under the MIT license. For clarity, semantics will include the 'Unlicense'.

Gemini's large context window allows for importing the entire project into it.
While generally AI generated code leads to many inaccuracies, Gemini 2.5 Pro & Flash managed to pass tests against real hardware.
Gemini code can usually be identified by the comments surrounding it, as it's generally very verbose.

However, AI-generated code suffers from poor performance within the existing architecture, redundant & expensive operations.
While this is initially okay, because it accelerates development significantly, and allows to check for hard-to-spot issues, in the long run the instruction handlers need to be re-written completely to run faster.


```

`configuration.hpp`:

```hpp
#pragma once

#include <cstdint>

namespace kubera
{
	enum VendorType : uint8_t {
		INTEL = 0,
		AMD
	};
	struct ArchOptions {
		uint8_t x64 : 1;
		uint8_t verbose : 1;
		uint8_t exit_on_infinite_loop : 1;
		VendorType vendor : 1;
		uint8_t reserved : 4;
	};
};
```

`emulator.hpp`:

```hpp
#pragma once

#include "KUBERA.hpp"
namespace kubera
{
  namespace handlers
  {
    // Arithmetic Instructions
    void add ( const iced::Instruction& instr, KUBERA& context );
    void sub ( const iced::Instruction& instr, KUBERA& context );
    void inc ( const iced::Instruction& instr, KUBERA& context );
    void dec ( const iced::Instruction& instr, KUBERA& context );
    void mul ( const iced::Instruction& instr, KUBERA& context );
    void imul ( const iced::Instruction& instr, KUBERA& context );
    void div ( const iced::Instruction& instr, KUBERA& context );
    void idiv ( const iced::Instruction& instr, KUBERA& context );
    void adc ( const iced::Instruction& instr, KUBERA& context );
    void sbb ( const iced::Instruction& instr, KUBERA& context );
    void neg ( const iced::Instruction& instr, KUBERA& context );
    void xadd ( const iced::Instruction& instr, KUBERA& context );
    void cdq ( const iced::Instruction& instr, KUBERA& context );
    void cdqe ( const iced::Instruction& instr, KUBERA& context );
    void cwd ( const iced::Instruction& instr, KUBERA& context );
    void cqo ( const iced::Instruction& instr, KUBERA& context );
    void cwde ( const iced::Instruction& instr, KUBERA& context );
    void cbw ( const iced::Instruction& instr, KUBERA& context );

    // Bitwise and Logical Instructions
    void and_ ( const iced::Instruction& instr, KUBERA& context );
    void or_ ( const iced::Instruction& instr, KUBERA& context );
    void xor_ ( const iced::Instruction& instr, KUBERA& context );
    void not_ ( const iced::Instruction& instr, KUBERA& context );
    void shl ( const iced::Instruction& instr, KUBERA& context );
    void sal ( const iced::Instruction& instr, KUBERA& context );
    void sar ( const iced::Instruction& instr, KUBERA& context );
    void shr ( const iced::Instruction& instr, KUBERA& context );
    void shld ( const iced::Instruction& instr, KUBERA& context );
    void shrd ( const iced::Instruction& instr, KUBERA& context );
    void rol ( const iced::Instruction& instr, KUBERA& context );
    void ror ( const iced::Instruction& instr, KUBERA& context );
    void rcl ( const iced::Instruction& instr, KUBERA& context );
    void rcr ( const iced::Instruction& instr, KUBERA& context );

    // Conditional Move Instructions
    void cmovo ( const iced::Instruction& instr, KUBERA& context );
    void cmovb ( const iced::Instruction& instr, KUBERA& context );
    void cmovnl ( const iced::Instruction& instr, KUBERA& context );
    void cmovbe ( const iced::Instruction& instr, KUBERA& context );
    void cmovz ( const iced::Instruction& instr, KUBERA& context );
    void cmovle ( const iced::Instruction& instr, KUBERA& context );
    void cmovl ( const iced::Instruction& instr, KUBERA& context );
    void cmovnp ( const iced::Instruction& instr, KUBERA& context );
    void cmovns ( const iced::Instruction& instr, KUBERA& context );
    void cmovp ( const iced::Instruction& instr, KUBERA& context );
    void cmovnb ( const iced::Instruction& instr, KUBERA& context );
    void cmovno ( const iced::Instruction& instr, KUBERA& context );
    void cmovs ( const iced::Instruction& instr, KUBERA& context );
    void cmovnz ( const iced::Instruction& instr, KUBERA& context );
    void cmovnbe ( const iced::Instruction& instr, KUBERA& context );
    void cmovnle ( const iced::Instruction& instr, KUBERA& context );

    // Set Based on Condition Instructions
    void setb ( const iced::Instruction& instr, KUBERA& context );
    void setnp ( const iced::Instruction& instr, KUBERA& context );
    void sets ( const iced::Instruction& instr, KUBERA& context );
    void setnl ( const iced::Instruction& instr, KUBERA& context );
    void seto ( const iced::Instruction& instr, KUBERA& context );
    void setbe ( const iced::Instruction& instr, KUBERA& context );
    void setz ( const iced::Instruction& instr, KUBERA& context );
    void setnb ( const iced::Instruction& instr, KUBERA& context );
    void setno ( const iced::Instruction& instr, KUBERA& context );
    void setp ( const iced::Instruction& instr, KUBERA& context );
    void setle ( const iced::Instruction& instr, KUBERA& context );
    void setnle ( const iced::Instruction& instr, KUBERA& context );
    void setns ( const iced::Instruction& instr, KUBERA& context );
    void setl ( const iced::Instruction& instr, KUBERA& context );
    void setnbe ( const iced::Instruction& instr, KUBERA& context );
    void setnz ( const iced::Instruction& instr, KUBERA& context );

    // Bit Manipulation Instructions
    void bzhi ( const iced::Instruction& instr, KUBERA& context );
    void andn ( const iced::Instruction& instr, KUBERA& context );
    void bextr ( const iced::Instruction& instr, KUBERA& context );
    void popcnt ( const iced::Instruction& instr, KUBERA& context );
    void bswap ( const iced::Instruction& instr, KUBERA& context );
    void bt ( const iced::Instruction& instr, KUBERA& context );
    void bts ( const iced::Instruction& instr, KUBERA& context );
    void btr ( const iced::Instruction& instr, KUBERA& context );
    void btc ( const iced::Instruction& instr, KUBERA& context );
    void bsr ( const iced::Instruction& instr, KUBERA& context );
    void bsf ( const iced::Instruction& instr, KUBERA& context );
    void tzcnt ( const iced::Instruction& instr, KUBERA& context );

    // Comparison and Test Instructions
    void cmp ( const iced::Instruction& instr, KUBERA& context );
    void test ( const iced::Instruction& instr, KUBERA& context );
    void cmpxchg ( const iced::Instruction& instr, KUBERA& context );
    void cmpxchg16b ( const iced::Instruction& instr, KUBERA& context );

    // Control Flow Instructions
    void jmp ( const iced::Instruction& instr, KUBERA& context );
    void je ( const iced::Instruction& instr, KUBERA& context );
    void jne ( const iced::Instruction& instr, KUBERA& context );
    void jnbe ( const iced::Instruction& instr, KUBERA& context );
    void jg ( const iced::Instruction& instr, KUBERA& context );
    void jl ( const iced::Instruction& instr, KUBERA& context );
    void jnb ( const iced::Instruction& instr, KUBERA& context );
    void jb ( const iced::Instruction& instr, KUBERA& context );
    void jns ( const iced::Instruction& instr, KUBERA& context );
    void jnl ( const iced::Instruction& instr, KUBERA& context );
    void jo ( const iced::Instruction& instr, KUBERA& context );
    void jno ( const iced::Instruction& instr, KUBERA& context );
    void jbe ( const iced::Instruction& instr, KUBERA& context );
    void js ( const iced::Instruction& instr, KUBERA& context );
    void ja ( const iced::Instruction& instr, KUBERA& context );
    void jae ( const iced::Instruction& instr, KUBERA& context );
    void jge ( const iced::Instruction& instr, KUBERA& context );
    void jle ( const iced::Instruction& instr, KUBERA& context );
    void jp ( const iced::Instruction& instr, KUBERA& context );
    void jnp ( const iced::Instruction& instr, KUBERA& context );
    void jcxz ( const iced::Instruction& instr, KUBERA& context );
    void jecxz ( const iced::Instruction& instr, KUBERA& context );
    void jrcxz ( const iced::Instruction& instr, KUBERA& context );
    void call ( const iced::Instruction& instr, KUBERA& context );
    void ret ( const iced::Instruction& instr, KUBERA& context );
    void iret ( const iced::Instruction& instr, KUBERA& context );
    void iretd ( const iced::Instruction& instr, KUBERA& context );
    void iretq ( const iced::Instruction& instr, KUBERA& context );

    // Stack and Frame Instructions
    void enter ( const iced::Instruction& instr, KUBERA& context );
    void leave ( const iced::Instruction& instr, KUBERA& context );
    void push ( const iced::Instruction& instr, KUBERA& context );
    void pop ( const iced::Instruction& instr, KUBERA& context );
    void pushfq ( const iced::Instruction& instr, KUBERA& context );
    void popfq ( const iced::Instruction& instr, KUBERA& context );

    // System Instructions
    void cli ( const iced::Instruction& instr, KUBERA& context );
    void cld ( const iced::Instruction& instr, KUBERA& context );
    void clc ( const iced::Instruction& instr, KUBERA& context );
    void clui ( const iced::Instruction& instr, KUBERA& context );
    void cmc ( const iced::Instruction& instr, KUBERA& context );
    void stc ( const iced::Instruction& instr, KUBERA& context );
    void sti ( const iced::Instruction& instr, KUBERA& context );
    void std ( const iced::Instruction& instr, KUBERA& context );
    void rdtsc ( const iced::Instruction& instr, KUBERA& context );
    void cpuid ( const iced::Instruction& instr, KUBERA& context );
    void xgetbv ( const iced::Instruction& instr, KUBERA& context );
    void hlt ( const iced::Instruction& instr, KUBERA& context );
    void int1 ( const iced::Instruction& instr, KUBERA& context );
    void int3 ( const iced::Instruction& instr, KUBERA& context );
    void int_ ( const iced::Instruction& instr, KUBERA& context );
    void fxsave ( const iced::Instruction& instr, KUBERA& context );
    void fxrstor ( const iced::Instruction& instr, KUBERA& context );
    void stmxcsr ( const iced::Instruction& instr, KUBERA& context );
    void ldmxcsr ( const iced::Instruction& instr, KUBERA& context );
    void sahf ( const iced::Instruction& instr, KUBERA& context );
    void lahf ( const iced::Instruction& instr, KUBERA& context );
    void pushf ( const iced::Instruction& instr, KUBERA& context );
    void popf ( const iced::Instruction& instr, KUBERA& context );
    void syscall ( const iced::Instruction& instr, KUBERA& context );

    // Data Movement Instructions
    void mov ( const iced::Instruction& instr, KUBERA& context );
    void movd ( const iced::Instruction& instr, KUBERA& context );
    void movq ( const iced::Instruction& instr, KUBERA& context );
    void movabs ( const iced::Instruction& instr, KUBERA& context );
    void movsxd ( const iced::Instruction& instr, KUBERA& context );
    void movzx ( const iced::Instruction& instr, KUBERA& context );
    void movsx ( const iced::Instruction& instr, KUBERA& context );
    void movaps ( const iced::Instruction& instr, KUBERA& context );
    void movups ( const iced::Instruction& instr, KUBERA& context );
    void lea ( const iced::Instruction& instr, KUBERA& context );
    void xchg ( const iced::Instruction& instr, KUBERA& context );

    // String Operations
    void movs ( const iced::Instruction& instr, KUBERA& context );
    void movsw ( const iced::Instruction& instr, KUBERA& context );
    void movsb ( const iced::Instruction& instr, KUBERA& context );
    void movsd ( const iced::Instruction& instr, KUBERA& context );
    void movsq ( const iced::Instruction& instr, KUBERA& context );
    void stos ( const iced::Instruction& instr, KUBERA& context );

    // SIMD Instructions
    void vpxor ( const iced::Instruction& instr, KUBERA& context );
    void vpcmpeqw ( const iced::Instruction& instr, KUBERA& context );
    void vpmovmskb ( const iced::Instruction& instr, KUBERA& context );
    void vzeroupper ( const iced::Instruction& instr, KUBERA& context );
    void vinsertf128 ( const iced::Instruction& instr, KUBERA& context );
    void vmovups ( const iced::Instruction& instr, KUBERA& context );
    void vmovaps ( const iced::Instruction& instr, KUBERA& context );
    void vmovdqu ( const iced::Instruction& instr, KUBERA& context );
    void movdqu ( const iced::Instruction& instr, KUBERA& context );
    void movlhps ( const iced::Instruction& instr, KUBERA& context );
    void punpcklqdq ( const iced::Instruction& instr, KUBERA& context );
    void prefetchw ( const iced::Instruction& instr, KUBERA& context );
    void psrldq ( const iced::Instruction& instr, KUBERA& context );
    void movhlps ( const iced::Instruction& instr, KUBERA& context );
    void unpcklps ( const iced::Instruction& instr, KUBERA& context );
    void pinsrb ( const iced::Instruction& instr, KUBERA& context );
    void pinsrd ( const iced::Instruction& instr, KUBERA& context );
    void pinsrq ( const iced::Instruction& instr, KUBERA& context );

    void paddb(const iced::Instruction& instr, KUBERA& context);
    void paddw(const iced::Instruction& instr, KUBERA& context);
    void paddd(const iced::Instruction& instr, KUBERA& context);
    void paddq(const iced::Instruction& instr, KUBERA& context);

    // Floating-Point Instructions
    void addss ( const iced::Instruction& instr, KUBERA& context );
    void subss ( const iced::Instruction& instr, KUBERA& context );
    void mulss ( const iced::Instruction& instr, KUBERA& context );
    void divss ( const iced::Instruction& instr, KUBERA& context );
    void minss ( const iced::Instruction& instr, KUBERA& context );
    void maxss ( const iced::Instruction& instr, KUBERA& context );
    void andps ( const iced::Instruction& instr, KUBERA& context );
    void orps ( const iced::Instruction& instr, KUBERA& context );
    void xorps ( const iced::Instruction& instr, KUBERA& context );
    void sqrtss ( const iced::Instruction& instr, KUBERA& context );
    void sqrtsd ( const iced::Instruction& instr, KUBERA& context );
    void comiss ( const iced::Instruction& instr, KUBERA& context );
    void ucomiss ( const iced::Instruction& instr, KUBERA& context );
    void comisd ( const iced::Instruction& instr, KUBERA& context );
    void cmpss ( const iced::Instruction& instr, KUBERA& context );
    void cvtss2si ( const iced::Instruction& instr, KUBERA& context );
    void cvttss2si ( const iced::Instruction& instr, KUBERA& context );
    void cvtsi2ss ( const iced::Instruction& instr, KUBERA& context );
    void cvtsi2sd ( const iced::Instruction& instr, KUBERA& context );
    void cvtss2sd ( const iced::Instruction& instr, KUBERA& context );
    void cvtsd2ss ( const iced::Instruction& instr, KUBERA& context );
    void roundss ( const iced::Instruction& instr, KUBERA& context );
    void rcpss ( const iced::Instruction& instr, KUBERA& context );
    void rsqrtss ( const iced::Instruction& instr, KUBERA& context );
    void mulsd ( const iced::Instruction& instr, KUBERA& context );
    void movss ( const iced::Instruction& instr, KUBERA& context );

    // 80-bit floating point instructions
    void fld ( const iced::Instruction& instr, KUBERA& context );
    void fprem ( const iced::Instruction& instr, KUBERA& context );
    void fstp ( const iced::Instruction& instr, KUBERA& context );
    void ffree ( const iced::Instruction& instr, KUBERA& context );
    void fincstp ( const iced::Instruction& instr, KUBERA& context );
    void fmul ( const iced::Instruction& instr, KUBERA& context );
    void fnstcw ( const iced::Instruction& instr, KUBERA& context );

    // Miscellaneous Instructions
    void nop ( const iced::Instruction& instr, KUBERA& context );
  };
};
```

`example/main.cpp`:

```cpp
#include <KUBERA/KUBERA.hpp>
#include <print>
#include <sstream>
#include <chrono>
#include <Windows.h>

/*
0:  48 01 d1                add    rcx,rdx
3:  48 c1 c1 05             rol    rcx,0x5
7:  48 81 e1 aa aa 00 00    and    rcx,0xaaaa
e:  48 c1 c9 0b             ror    rcx,0xb
12: 48 81 f1 f0 f0 00 00    xor    rcx,0xf0f0
19: 48 c1 d1 0d             rcl    rcx,0xd
1d: 48 81 c9 cc cc 00 00    or     rcx,0xcccc
24: 48 c1 e1 08             shl    rcx,0x8
28: 48 81 e1 55 55 00 00    and    rcx,0x5555
2f: 48 c1 d1 03             rcl    rcx,0x3
33: 48 81 f1 a5 a5 00 00    xor    rcx,0xa5a5
3a: 48 f7 d1                not    rcx
3d: 48 81 c9 ff ff 00 00    or     rcx,0xffff
44: 48 83 f1 1f             xor    rcx,0x1f
48: 48 c1 e1 04             shl    rcx,0x4
4c: 48 81 e1 aa aa 00 00    and    rcx,0xaaaa
53: 48 c1 e9 07             shr    rcx,0x7
57: 48 c1 c1 09             rol    rcx,0x9
5b: 48 81 f1 3c 3c 3c 3c    xor    rcx,0x3c3c3c3c
62: 48 c1 e9 02             shr    rcx,0x2
66: 48 81 c9 ff ff 00 00    or     rcx,0xffff
6d: 48 c1 c9 06             ror    rcx,0x6
71: 48 81 e1 f5 f5 00 00    and    rcx,0xf5f5
78: 48 c1 e1 05             shl    rcx,0x5
7c: 48 81 f1 5a 5a 00 00    xor    rcx,0x5a5a
83: 48 89 c8                mov    rax,rcx
86: c3                      ret
*/
const uint8_t test_fn [ ] = { 0x48, 0x01, 0xD1, 0x48, 0xC1, 0xC1, 0x05, 0x48, 0x81, 0xE1, 0xAA, 0xAA, 0x00, 0x00, 0x48, 0xC1, 0xC9, 0x0B, 0x48, 0x81, 0xF1, 0xF0, 0xF0, 0x00, 0x00, 0x48, 0xC1, 0xD1, 0x0D, 0x48, 0x81, 0xC9, 0xCC, 0xCC, 0x00, 0x00, 0x48, 0xC1, 0xE1, 0x08, 0x48, 0x81, 0xE1, 0x55, 0x55, 0x00, 0x00, 0x48, 0xC1, 0xD1, 0x03, 0x48, 0x81, 0xF1, 0xA5, 0xA5, 0x00, 0x00, 0x48, 0xF7, 0xD1, 0x48, 0x81, 0xC9, 0xFF, 0xFF, 0x00, 0x00, 0x48, 0x83, 0xF1, 0x1F, 0x48, 0xC1, 0xE1, 0x04, 0x48, 0x81, 0xE1, 0xAA, 0xAA, 0x00, 0x00, 0x48, 0xC1, 0xE9, 0x07, 0x48, 0xC1, 0xC1, 0x09, 0x48, 0x81, 0xF1, 0x3C, 0x3C, 0x3C, 0x3C, 0x48, 0xC1, 0xE9, 0x02, 0x48, 0x81, 0xC9, 0xFF, 0xFF, 0x00, 0x00, 0x48, 0xC1, 0xC9, 0x06, 0x48, 0x81, 0xE1, 0xF5, 0xF5, 0x00, 0x00, 0x48, 0xC1, 0xE1, 0x05, 0x48, 0x81, 0xF1, 0x5A, 0x5A, 0x00, 0x00, 0x48, 0x89, 0xC8, 0xC3 };
const uint8_t benchmark_fn [ ] = {
	0x48, 0x01, 0xD1,				// add rcx, rdx
	0x48, 0xC1, 0xD1, 0x0D, // rcl rcx, 0xd
	0x48, 0xF7, 0xD1,				// not rcx
	0x48, 0x89, 0xC8,				// mov rax, rcx
};

// This function is a benchmark for the emulation speed.
void run_emulation_loop ( kubera::KUBERA& ctx, const uint8_t* test_fn, size_t test_fn_size, uint64_t flags_before_running ) {
	uint64_t instruction_count = 0;
	auto start_time = std::chrono::high_resolution_clock::now ( );
	while ( true ) {
		auto current_time = std::chrono::high_resolution_clock::now ( );
		auto elapsed_seconds = std::chrono::duration_cast< std::chrono::duration<double> > (
				current_time - start_time ).count ( );

		if ( elapsed_seconds >= 10.0 ) {
			break;
		}

		// We configure the decoder to the start of the shellcode.
		ctx.decoder->reconfigure ( test_fn, test_fn_size, 0 );
		// We use set_reg_internal here because we are aware of exactly which registers we want to set,
		// reducing overhead for getting the correct bits of a 64-bit register
		ctx.set_reg_internal<kubera::KubRegister::RCX, Register::RCX, uint32_t> ( 0xFFAA );
		ctx.set_reg_internal<kubera::KubRegister::RDX, Register::RDX, uint32_t> ( 0x0055 );
		// We reset the EFlags, so the state is reset properly for every iteration
		ctx.get_flags ( ).value = flags_before_running;
		// We are only emulating 4 instructions.
		for ( auto i = 0u; i < 4u; ++i ) {
			// We decode the instruction with Iced
			auto instr = ctx.decoder->decode ( );
			// We execute the instruction within out emulation context
			ctx.execute ( instr );
		}

		instruction_count += 4;
	}

	auto end_time = std::chrono::high_resolution_clock::now ( );
	auto total_seconds = std::chrono::duration_cast< std::chrono::duration<double> > (
			end_time - start_time ).count ( );
	double instructions_per_second = instruction_count / total_seconds;

	std::println ( "Total instructions executed: {}", instruction_count );
	std::println ( "Total time elapsed: {:.2f} seconds", total_seconds );
	std::println ( "Instructions per second: {:.2f}", instructions_per_second );
}


/// The following function executes a function on hardware, capturing the result and flags.
/// Then, it runs the same function with the same entry flags and parameters with the emulator and shows the results.
int main ( ) {
	std::println ( "[+] Initializing KUBERA - Windows example" );
	// Currently, the function is for running a benchmark
	kubera::KUBERA ctx {};
	run_emulation_loop ( ctx, benchmark_fn, sizeof ( benchmark_fn ), __readeflags ( ) );
	std::getchar ( );
	// Comment the emulation loop & getchar, along with the return underneath to test the emulator against hardware
	// on Windows.
	return 1;
	// Generic way to set a register's value
	ctx.set_reg ( Register::RCX, 0xFFAA, 4 );
	ctx.set_reg ( Register::RDX, 0x0055, 4 );

	DWORD old;
	// Allocate memory for the shellcode on host
	auto* alloc = VirtualAlloc ( nullptr, 0x1000, MEM_COMMIT | MEM_RESERVE, PAGE_EXECUTE_READWRITE );
	if ( !alloc ) {
		std::println ( "Failed to allocate memory" ); std::getchar ( );
		return -1;
	}
	// Copy the shellcode to the new memory allocation
	memcpy ( alloc, test_fn, sizeof ( test_fn ) );
	// Adjust permissions to be readable and executable, as RWX memory may cause problems.
	VirtualProtect ( alloc, 0x1000, PAGE_EXECUTE_READ, &old );
	// We record the flags before running the shellcode, to match in our emulation loop.
	const auto flags_before_running = __readeflags ( );
	// Run the shellcode natively to get expected output.
	const auto expected_value =
		reinterpret_cast< uint64_t ( * )( uint64_t, uint64_t ) >( alloc ) ( 0xFFAA, 0x0055 );
	// Resulting flags after running the shellcode natively.
	const auto native_flags = __readeflags ( );
	// Configure the decoder
	ctx.decoder->reconfigure ( test_fn, sizeof ( test_fn ), 0 );
	// Set the initial flags for the emulator to the same flags the host had prior to execution.
	ctx.get_flags ( ).value = flags_before_running;
	// Emulate in loop
	while ( ctx.decoder->can_decode ( ) ) {
		auto instr = ctx.decoder->decode ( );

		ctx.execute ( instr );
		std::println ( "{}", instr.to_string ( ) );
	}

	const auto emu_value = ctx.get_reg ( Register::RAX );
	const auto emu_flags = ctx.get_flags ( ).value;
	std::println ( "RAX    => [EMU: {:#x}] = [HW: {:#x}]", emu_value, expected_value );
	std::println ( "EFLAGS => [EMU: {:#x}] = [HW: {:#x}]", emu_flags, native_flags );
	// Compare the results
	assert ( emu_value == expected_value && "Native & Emulator value mismatch for test_fn" );
	assert ( emu_flags == native_flags && "Native & Emulator flag mismatch for test_fn" );
	std::getchar ( );
}
```

`iced.hpp`:

```hpp
#pragma once
#ifndef __ICED_DEF
#define __ICED_DEF

/* DEFINES */
#define ICED_USE_STD_STRING // Wrappers will include a std::string_view instead of a char*

/* INCLUDES */
#include <cstdint>
#include <cstddef>
#include <utility>
#include <vector>
#include <algorithm>
#include <cassert>

#ifdef ICED_USE_STD_STRING
#include <string>
#define ICED_STR std::string_view
#else
#define ICED_STR char*
#endif

#include "iced_internal.hpp"

/* MACROS */
#if __cplusplus >= 201703L || _MSVC_LANG >= 201703L
#define NODISCARD [[nodiscard]]
#else
#define NODISCARD
#endif

#if defined(_MSC_VER)
#define UNREACHABLE() __assume(false)
#define FORCE_INLINE __forceinline
#elif defined(__GNUC__) || defined(__clang__)
#define UNREACHABLE() __builtin_unreachable()
#define FORCE_INLINE __attribute__((always_inline)) inline
#else
#if __cplusplus >= 202302L || _MSVC_LANG >= 202302L
#define UNREACHABLE() std::unreachable()
#else
#define UNREACHABLE() do {} while (0) // Fallback for older standards
#endif
#define FORCE_INLINE inline
#endif

extern "C" {
	int disas ( void* obj, const void* code, std::size_t len );
	int disas2 ( void* obj, const void* code, std::size_t len );
}

NODISCARD constexpr OpKindSimple opkind_map_to_simple ( OpKind rawType ) {
	static constexpr OpKindSimple lookup [ ] = {
			OpKindSimple::Invalid,    // Invalid
			OpKindSimple::Register,   // Register8-512
			OpKindSimple::Register,
			OpKindSimple::Register,
			OpKindSimple::Register,
			OpKindSimple::Register,
			OpKindSimple::Register,
			OpKindSimple::Register,
			OpKindSimple::Memory,     // Memory8-512
			OpKindSimple::Memory,
			OpKindSimple::Memory,
			OpKindSimple::Memory,
			OpKindSimple::Memory,
			OpKindSimple::Memory,
			OpKindSimple::Memory,
			OpKindSimple::Immediate,  // Immediate8-64
			OpKindSimple::Immediate,
			OpKindSimple::Immediate,
			OpKindSimple::Immediate,
			OpKindSimple::Immediate,
			OpKindSimple::NearBranch, // NearBranch
			OpKindSimple::FarBranch   // FarBranch
	};

	return lookup [ static_cast< uint8_t >( rawType ) ];
}

constexpr bool operator==( OpKind lhs, OpKindSimple rhs ) noexcept {
	return opkind_map_to_simple ( lhs ) == rhs;
}

constexpr bool operator==( OpKindSimple lhs, OpKind rhs ) noexcept {
	return rhs == lhs;
}


/* CLASSES */
namespace iced
{
	class Instruction {
	public:
		Instruction ( ) = default;
		Instruction ( const __iced_internal::IcedInstruction& instruction, std::uint64_t ip_ ) : icedInstr ( instruction ), ip ( ip_ ) { }
		~Instruction ( ) { }

		NODISCARD FlowControl flow_control ( ) const noexcept {
			if ( jcc ( ) ) {
				return FlowControl::ConditionalBranch;
			}
			else if ( jmp ( ) ) {
				if ( op0_kind ( ) == OpKindSimple::Register || op0_kind ( ) == OpKindSimple::Memory ) {
					return FlowControl::IndirectBranch;
				}
				return FlowControl::UnconditionalBranch;
			}
			else if ( ret ( ) ) {
				return FlowControl::Return;
			}
			else if ( call ( ) ) {
				if ( op0_kind ( ) == OpKindSimple::Register || op0_kind ( ) == OpKindSimple::Memory ) {
					return FlowControl::IndirectCall;
				}
				return FlowControl::Call;
			}

			switch ( icedInstr.mnemonic ) {
				case Mnemonic::Syscall:
				case Mnemonic::Sysenter:
				case Mnemonic::Vmlaunch:
				case Mnemonic::Vmresume:
				case Mnemonic::Vmcall:
				case Mnemonic::Vmmcall:
				case Mnemonic::Vmgexit:
				case Mnemonic::Vmrun:
				case Mnemonic::Tdcall:
				case Mnemonic::Seamcall:
				case Mnemonic::Seamret:
					return FlowControl::Call;
				case Mnemonic::Xbegin:
				case Mnemonic::Xabort:
				case Mnemonic::Xend:
					return FlowControl::XbeginXabortXend;

				case Mnemonic::Loop:
				case Mnemonic::Loopne:
				case Mnemonic::Loope:
					return FlowControl::ConditionalBranch;
				case Mnemonic::Int:
				case Mnemonic::Int1:
				case Mnemonic::Int3:
				case Mnemonic::Into:
				case Mnemonic::Smint:
				case Mnemonic::Dmint:
					return FlowControl::Interrupt;
				case Mnemonic::INVALID:
				case Mnemonic::Ud0:
				case Mnemonic::Ud1:
				case Mnemonic::Ud2:
					return FlowControl::Exception;
				default:
					break;
			}

			return FlowControl::Next;
		}

		/// Returns operand size in bytes
		NODISCARD FORCE_INLINE std::size_t op_size ( std::size_t index ) const noexcept {
			static constexpr std::size_t lookup [ ] = {
					0,   // Invalid
					1,   // Register8
					2,   // Register16
					4,   // Register32
					8,   // Register64
					16,  // Register128
					32,  // Register256
					64,  // Register512
					1,   // Memory8
					2,   // Memory16
					4,   // Memory32
					8,   // Memory64
					16,  // Memory128
					32,  // Memory256
					64,  // Memory512
					1,   // Immediate8
					1,   // Immediate8_2nd
					2,   // Immediate16
					4,   // Immediate32
					8,   // Immediate64
					8,   // NearBranch
					4    // FarBranch
			};

			return lookup [ static_cast< uint8_t >( icedInstr.types [ index ] ) ];
		}
		/// <summary>
		///  Calculates size of operand in bytes
		/// </summary>
		/// <param name="index">index of operand</param>
		/// <returns>Width in bytes</returns>
		NODISCARD FORCE_INLINE std::size_t op0_size ( ) const noexcept { return op_size ( 0 ); }
		/// <summary>
		///  Calculates size of first operand in bytes
		/// </summary>
		/// <returns>Width in bytes</returns>
		NODISCARD FORCE_INLINE std::size_t op1_size ( ) const noexcept { return op_size ( 1 ); }
		/// <summary>
		///  Calculates size of second operand in bytes
		/// </summary>
		/// <returns>Width in bytes</returns>
		NODISCARD FORCE_INLINE std::size_t op2_size ( ) const noexcept { return op_size ( 2 ); }
		/// <summary>
		///  Calculates size of third operand in bytes
		/// </summary>
		/// <returns>Width in bytes</returns>
		NODISCARD FORCE_INLINE std::size_t op3_size ( ) const noexcept { return op_size ( 3 ); }

		/// <summary>
		///  Calculates width of operand in bits
		/// </summary>
		/// <param name="index">index of operand</param>
		/// <returns>Width in bits</returns>
		NODISCARD FORCE_INLINE std::size_t op_bit_width ( std::size_t index ) const noexcept { return op_size ( index ) * 8ULL; }
		/// <summary>
		///		Calculates width of the first operand in bits
		/// </summary>
		/// <returns>Width in bits</returns>
		NODISCARD FORCE_INLINE std::size_t op0_bit_width ( ) const noexcept { return op_bit_width ( 0 ); }
		/// <summary>
		///		Calculates width of the second operand in bits
		/// </summary>
		/// <returns>Width in bits</returns>
		NODISCARD FORCE_INLINE std::size_t op1_bit_width ( ) const noexcept { return op_bit_width ( 1 ); }
		/// <summary>
		///		Calculates width of the third operand in bits
		/// </summary>
		/// <returns>Width in bits</returns>
		NODISCARD FORCE_INLINE std::size_t op2_bit_width ( ) const noexcept { return op_bit_width ( 2 ); }
		/// <summary>
		///		Calculates width of the fourth operand in bits
		/// </summary>
		/// <returns>Width in bits</returns>
		NODISCARD FORCE_INLINE std::size_t op3_bit_width ( ) const noexcept { return op_bit_width ( 3 ); }

		NODISCARD FORCE_INLINE OpKind op_kind ( std::size_t index ) const noexcept { return static_cast< OpKind >( icedInstr.types [ index ] ); }
		NODISCARD FORCE_INLINE OpKind op0_kind ( ) const noexcept { return op_kind ( 0 ); }
		NODISCARD FORCE_INLINE OpKind op1_kind ( ) const noexcept { return op_kind ( 1 ); }
		NODISCARD FORCE_INLINE OpKind op2_kind ( ) const noexcept { return op_kind ( 2 ); }
		NODISCARD FORCE_INLINE OpKind op3_kind ( ) const noexcept { return op_kind ( 3 ); }

		NODISCARD FORCE_INLINE OpKindSimple op_kind_simple ( std::size_t index ) const noexcept { return opkind_map_to_simple ( icedInstr.types [ index ] ); }

		NODISCARD ICED_STR op_kind_simple_str ( std::size_t operandIndex ) const noexcept {
			switch ( op_kind_simple ( operandIndex ) ) {
				case OpKindSimple::Register:
					return "Register";
				case OpKindSimple::Memory:
					return "Memory";
				case OpKindSimple::Immediate:
					return "Immediate";
				case OpKindSimple::NearBranch:
					return "NearBranch";
				case OpKindSimple::FarBranch:
					return "FarBranch";
				default:
					break;
			}

			return "Invalid";
		}

		NODISCARD FORCE_INLINE ICED_STR op0_kind_simple_str ( ) const noexcept { return op_kind_simple_str ( 0 ); }
		NODISCARD FORCE_INLINE ICED_STR op1_kind_simple_str ( ) const noexcept { return op_kind_simple_str ( 1 ); }
		NODISCARD FORCE_INLINE ICED_STR op2_kind_simple_str ( ) const noexcept { return op_kind_simple_str ( 2 ); }
		NODISCARD FORCE_INLINE ICED_STR op3_kind_simple_str ( ) const noexcept { return op_kind_simple_str ( 3 ); }

		NODISCARD FORCE_INLINE Register op_reg ( std::size_t index ) const noexcept { return icedInstr.regs [ index ]; }
		NODISCARD FORCE_INLINE Register op0_reg ( ) const noexcept { return op_reg ( 0 ); }
		NODISCARD FORCE_INLINE Register op1_reg ( ) const noexcept { return op_reg ( 1 ); }
		NODISCARD FORCE_INLINE Register op2_reg ( ) const noexcept { return op_reg ( 2 ); }
		NODISCARD FORCE_INLINE Register op3_reg ( ) const noexcept { return op_reg ( 3 ); }

		NODISCARD FORCE_INLINE std::uint64_t immediate ( ) const noexcept { return icedInstr.immediate; }
		NODISCARD FORCE_INLINE std::uint64_t immediate2 ( ) const noexcept { return icedInstr.immediate2; }
		NODISCARD FORCE_INLINE std::uint64_t displacement ( ) const noexcept { return icedInstr.mem_disp; }
		NODISCARD FORCE_INLINE Register mem_index ( ) const noexcept { return icedInstr.mem_index; }
		NODISCARD FORCE_INLINE Register mem_base ( ) const noexcept { return icedInstr.mem_base; }
		NODISCARD FORCE_INLINE uint32_t mem_scale ( ) const noexcept { return icedInstr.mem_scale; }
		NODISCARD FORCE_INLINE Register segment_prefix ( ) const noexcept { return icedInstr.segment_prefix; }

		NODISCARD FORCE_INLINE __iced_internal::IcedInstruction& get_internal ( ) noexcept { return icedInstr; }
		NODISCARD FORCE_INLINE std::uint8_t op_count ( ) const noexcept { return icedInstr.operand_count_visible; }
		NODISCARD FORCE_INLINE std::uint8_t length ( ) const noexcept { return icedInstr.length; }
		NODISCARD FORCE_INLINE bool rep_prefix ( ) const noexcept { return icedInstr.attributes.rep; }
		NODISCARD FORCE_INLINE bool repne_prefix ( ) const noexcept { return icedInstr.attributes.repne; }
		NODISCARD FORCE_INLINE bool lock_prefix ( ) const noexcept { return icedInstr.attributes.lock; }
		NODISCARD FORCE_INLINE bool is_broadcast ( ) const noexcept { return icedInstr.is_broadcast; }
		NODISCARD FORCE_INLINE Mnemonic mnemonic ( ) const noexcept { return static_cast< Mnemonic >( icedInstr.mnemonic ); }
		NODISCARD FORCE_INLINE bool valid ( ) const noexcept { return icedInstr.mnemonic != Mnemonic::INVALID; }
		NODISCARD FORCE_INLINE std::uint8_t stack_growth ( ) const noexcept { return icedInstr.stack_growth; }
		NODISCARD FORCE_INLINE bool lea ( ) const noexcept { return match_mnemonic ( Mnemonic::Lea ); }
		NODISCARD FORCE_INLINE bool mov ( ) const noexcept { return match_mnemonic ( Mnemonic::Mov ); }
		NODISCARD FORCE_INLINE bool bp ( ) const noexcept { return match_mnemonic ( Mnemonic::Int3 ); }
		NODISCARD FORCE_INLINE bool nop ( ) const noexcept { return match_mnemonic ( Mnemonic::Nop ); }
		NODISCARD FORCE_INLINE bool call ( ) const noexcept { return match_mnemonic ( Mnemonic::Call ); }
		NODISCARD FORCE_INLINE bool jmp ( ) const noexcept { return match_mnemonic ( Mnemonic::Jmp ); }
		NODISCARD FORCE_INLINE bool jcc ( ) const noexcept {
			const auto& mnemonic = icedInstr.mnemonic;
			return mnemonic >= Mnemonic::Ja && mnemonic <= Mnemonic::Js;
		}
		NODISCARD FORCE_INLINE bool jump ( ) const noexcept { return jmp ( ) || jcc ( ); }
		NODISCARD FORCE_INLINE bool branching ( ) const noexcept { return call ( ) || jump ( ); }
		NODISCARD FORCE_INLINE bool conditional_branch ( ) const noexcept { return jcc ( ); }
		NODISCARD FORCE_INLINE bool unconditional_branch ( ) const noexcept { return call ( ) || jmp ( ); }
		NODISCARD FORCE_INLINE bool indirect_call ( ) const noexcept {
			if ( !call ( ) ) {
				return false;
			}

			return op0_kind ( ) == OpKindSimple::Register || op0_kind ( ) == OpKindSimple::Memory;
		}
		NODISCARD bool modifies_register ( Register reg ) const noexcept {
			return op_kind_simple ( 0 ) == OpKindSimple::Register && op0_reg ( ) == reg;
		}
		NODISCARD bool ret ( ) const noexcept {
			switch ( icedInstr.mnemonic ) {
				case Mnemonic::Ret:
				case Mnemonic::Iret:
				case Mnemonic::Uiret:
					return true;
				default:
					break;
			}

			return false;
		}

		NODISCARD FORCE_INLINE std::uint64_t compute_memory_address ( ) const noexcept {
			if ( icedInstr.mem_base == Register::RIP ) {
				return ip + length ( ) + icedInstr.mem_disp;
			}

			if ( icedInstr.mem_base == Register::None || icedInstr.mem_index == Register::None ) { // Displacement holds absolute address
				return icedInstr.mem_disp;
			}

			return icedInstr.immediate;
		}
		NODISCARD FORCE_INLINE std::uint64_t resolve_memory ( ) const noexcept { return compute_memory_address ( ); }
		NODISCARD std::uint64_t branch_target ( ) const noexcept {
			switch ( op_kind_simple ( 0 ) ) {
				case OpKindSimple::Immediate:
					return icedInstr.immediate2 ? icedInstr.immediate2 : icedInstr.immediate;
				case OpKindSimple::Memory:
					return resolve_memory ( );
				case OpKindSimple::NearBranch:
				case OpKindSimple::FarBranch:
					return ip + length ( ) + icedInstr.mem_disp;
				default:
					return 0ULL;
			}

			UNREACHABLE ( );
		}

		NODISCARD FORCE_INLINE ICED_STR to_string ( ) const noexcept {
			if ( !valid ( ) ) {
				return "Invalid instruction";
			}

			return icedInstr.text;
		}
		std::uint64_t ip;
	private:
		NODISCARD FORCE_INLINE bool match_mnemonic ( Mnemonic mnemonic ) const noexcept { return icedInstr.mnemonic == mnemonic; }
		__iced_internal::IcedInstruction icedInstr;
	};

	class DecoderBase {
	public:
		DecoderBase ( ) = delete;
		DecoderBase ( const std::uint8_t* buffer, std::size_t size, std::uint64_t baseAddress )
			: data_ ( buffer ), ip_ ( baseAddress ), baseAddr_ ( baseAddress ), size_ ( size ), offset_ ( 0 ),
			lastSuccessfulIp_ ( 0 ), lastSuccessfulLength_ ( 0 ) {
			//assert ( buffer != nullptr && "Buffer cannot be null" );
			//assert ( size > 0 && "Buffer size must be greater than 0" );
		}

		DecoderBase ( const DecoderBase& ) = delete;
		DecoderBase& operator=( const DecoderBase& ) = delete;

		DecoderBase ( DecoderBase&& other ) noexcept
			: data_ ( other.data_ ), ip_ ( other.ip_ ), baseAddr_ ( other.baseAddr_ ),
			size_ ( other.size_ ), offset_ ( other.offset_ ),
			lastSuccessfulIp_ ( other.lastSuccessfulIp_ ),
			lastSuccessfulLength_ ( other.lastSuccessfulLength_ ),
			currentInstruction_ ( std::move ( other.currentInstruction_ ) ) {
			other.data_ = nullptr;
			other.size_ = 0;
		}

		DecoderBase& operator=( DecoderBase&& other ) noexcept {
			if ( this != &other ) {
				data_ = other.data_;
				ip_ = other.ip_;
				baseAddr_ = other.baseAddr_;
				size_ = other.size_;
				offset_ = other.offset_;
				lastSuccessfulIp_ = other.lastSuccessfulIp_;
				lastSuccessfulLength_ = other.lastSuccessfulLength_;
				currentInstruction_ = std::move ( other.currentInstruction_ );
				other.data_ = nullptr;
				other.size_ = 0;
			}
			return *this;
		}

		virtual ~DecoderBase ( ) = default;

		NODISCARD FORCE_INLINE std::uint64_t ip ( ) const noexcept { return ip_; }
		NODISCARD FORCE_INLINE const Instruction& current_instruction ( ) const noexcept { return currentInstruction_; }
		NODISCARD FORCE_INLINE Instruction& current_instruction ( ) noexcept { return currentInstruction_; }
		NODISCARD FORCE_INLINE bool can_decode ( ) const noexcept { return offset_ < size_; }
		NODISCARD FORCE_INLINE std::uint64_t last_successful_ip ( ) const noexcept { return lastSuccessfulIp_; }
		NODISCARD FORCE_INLINE std::uint16_t last_successful_length ( ) const noexcept { return lastSuccessfulLength_; }
		NODISCARD FORCE_INLINE std::size_t remaining_size ( ) const noexcept { return size_ - offset_; }

		bool set_ip ( std::uint64_t ip ) noexcept {
			if ( ip < baseAddr_ || ip >= baseAddr_ + size_ ) {
				return false;
			}
			ip_ = ip;
			offset_ = ip - baseAddr_;
			return true;
		}

		bool set_ip ( std::uint8_t* _ip ) noexcept {
			auto ip = reinterpret_cast< std::uint64_t > ( _ip );
			if ( ip < baseAddr_ || ip >= baseAddr_ + size_ ) {
				return false;
			}
			ip_ = ip;
			offset_ = ip - baseAddr_;
			return true;
		}

		void reconfigure ( const std::uint8_t* buffer, std::size_t size, std::uint64_t baseAddress ) noexcept {
			//assert ( buffer != nullptr && "Buffer cannot be null" );
			//assert ( size > 0 && "Buffer size must be greater than 0" );

			data_ = buffer;
			size_ = size;
			baseAddr_ = baseAddress;
			ip_ = baseAddress;
			offset_ = 0;
			lastSuccessfulIp_ = 0;
			lastSuccessfulLength_ = 0;
			currentInstruction_ = Instruction {};
		}

		void reset ( ) noexcept {
			ip_ = baseAddr_;
			offset_ = 0;
			lastSuccessfulIp_ = 0;
			lastSuccessfulLength_ = 0;
			currentInstruction_ = Instruction {};
		}

	protected:
		FORCE_INLINE void update_state ( const __iced_internal::IcedInstruction& icedInstruction ) noexcept {
			const auto len = icedInstruction.length;
			currentInstruction_ = Instruction { icedInstruction, ip_ };
			lastSuccessfulIp_ = ip_;
			lastSuccessfulLength_ = len;
			ip_ += len;
			offset_ += len;
		}

		const std::uint8_t* data_;
		std::uint64_t ip_;
		std::size_t offset_;

		std::uint64_t baseAddr_;
		std::size_t size_;
		std::uint64_t lastSuccessfulIp_;
		std::uint16_t lastSuccessfulLength_;

		Instruction currentInstruction_;
	};

	class Decoder : public DecoderBase {
	private:
		using DisasmFunc = int( * )( void*, const void*, std::size_t );
		DisasmFunc disasmFunction_;

	public:
		explicit Decoder ( const std::uint8_t* buffer = nullptr, std::size_t size = 15ULL,
						std::uint64_t baseAddress = 0ULL, bool debug = true )
			: DecoderBase ( buffer, size, baseAddress ),
			disasmFunction_ ( debug ? disas2 : disas ) { }

		NODISCARD Instruction& decode ( ) noexcept {
			const auto* current_ptr = data_ + offset_;
			const auto code_size = remaining_size ( );

			__iced_internal::IcedInstruction icedInstruction {};

			constexpr auto decode_size = 16ULL;
			disasmFunction_ ( &icedInstruction, current_ptr, decode_size );

			update_state ( icedInstruction );
			return currentInstruction_;
		}

		void set_debug_mode ( bool debug ) noexcept {
			disasmFunction_ = debug ? disas2 : disas;
		}
	};

	class DebugDecoder : public DecoderBase {
	public:
		explicit DebugDecoder ( const std::uint8_t* buffer = nullptr, std::size_t size = 15ULL,
							 std::uint64_t baseAddress = 0ULL )
			: DecoderBase ( buffer, size, baseAddress ) { }

		NODISCARD Instruction& decode ( ) noexcept {
			const auto* current_ptr = data_ + offset_;
			const auto code_size = remaining_size ( );

			__iced_internal::IcedInstruction icedInstruction {};
			const auto decode_size = std::min ( static_cast< std::size_t >( 16 ), code_size );
			disas2 ( &icedInstruction, current_ptr, decode_size );

			update_state ( icedInstruction );
			return currentInstruction_;
		}

		NODISCARD Instruction peek ( ) noexcept {
			const auto* current_ptr = data_ + offset_;
			const auto code_size = remaining_size ( );

			__iced_internal::IcedInstruction icedInstruction {};
			const auto decode_size = std::min ( static_cast< std::size_t >( 16 ), code_size );
			disas2 ( &icedInstruction, current_ptr, decode_size );

			//updateState ( icedInstruction );
			return Instruction ( icedInstruction, ip ( ) );
		}
	};

	class ReleaseDecoder : public DecoderBase {
	public:
		explicit ReleaseDecoder ( const std::uint8_t* buffer = nullptr, std::size_t size = 15ULL,
								 std::uint64_t baseAddress = 0ULL )
			: DecoderBase ( buffer, size, baseAddress ) { }

		NODISCARD Instruction& decode ( ) noexcept {
			const auto* current_ptr = data_ + offset_;
			const auto code_size = remaining_size ( );

			__iced_internal::IcedInstruction icedInstruction {};
			const auto decode_size = std::min ( static_cast< std::size_t >( 16 ), code_size );
			disas ( &icedInstruction, current_ptr, decode_size );

			update_state ( icedInstruction );
			return currentInstruction_;
		}

		NODISCARD Instruction peek ( ) noexcept {
			const auto* current_ptr = data_ + offset_;
			const auto code_size = remaining_size ( );

			__iced_internal::IcedInstruction icedInstruction {};
			const auto decode_size = std::min ( static_cast< std::size_t >( 16 ), code_size );
			disas ( &icedInstruction, current_ptr, decode_size );

			//updateState ( icedInstruction );
			return Instruction ( icedInstruction, ip ( ) );
		}
	};

	template<bool Debug = true>
	NODISCARD auto make_decoder ( const std::uint8_t* buffer, std::size_t size, std::uint64_t baseAddress = 0ULL ) {
		if constexpr ( Debug ) {
			return DebugDecoder ( buffer, size, baseAddress );
		}
		else {
			return ReleaseDecoder ( buffer, size, baseAddress );
		}
	}
};
#endif

```

`iced_internal.hpp`:

```hpp
#ifndef __ICEDINT_DEF
#define __ICEDINT_DEF
#include <cstdint>

enum class Mnemonic : uint16_t {
  INVALID = 0,
  Aaa = 1,
  Aad = 2,
  Aam = 3,
  Aas = 4,
  Adc = 5,
  Adcx = 6,
  Add = 7,
  Addpd = 8,
  Addps = 9,
  Addsd = 10,
  Addss = 11,
  Addsubpd = 12,
  Addsubps = 13,
  Adox = 14,
  Aesdec = 15,
  Aesdeclast = 16,
  Aesenc = 17,
  Aesenclast = 18,
  Aesimc = 19,
  Aeskeygenassist = 20,
  And = 21,
  Andn = 22,
  Andnpd = 23,
  Andnps = 24,
  Andpd = 25,
  Andps = 26,
  Arpl = 27,
  Bextr = 28,
  Blcfill = 29,
  Blci = 30,
  Blcic = 31,
  Blcmsk = 32,
  Blcs = 33,
  Blendpd = 34,
  Blendps = 35,
  Blendvpd = 36,
  Blendvps = 37,
  Blsfill = 38,
  Blsi = 39,
  Blsic = 40,
  Blsmsk = 41,
  Blsr = 42,
  Bndcl = 43,
  Bndcn = 44,
  Bndcu = 45,
  Bndldx = 46,
  Bndmk = 47,
  Bndmov = 48,
  Bndstx = 49,
  Bound = 50,
  Bsf = 51,
  Bsr = 52,
  Bswap = 53,
  Bt = 54,
  Btc = 55,
  Btr = 56,
  Bts = 57,
  Bzhi = 58,
  Call = 59,
  Cbw = 60,
  Cdq = 61,
  Cdqe = 62,
  Cl1invmb = 63,
  Clac = 64,
  Clc = 65,
  Cld = 66,
  Cldemote = 67,
  Clflush = 68,
  Clflushopt = 69,
  Clgi = 70,
  Cli = 71,
  Clrssbsy = 72,
  Clts = 73,
  Clwb = 74,
  Clzero = 75,
  Cmc = 76,
  Cmova = 77,
  Cmovae = 78,
  Cmovb = 79,
  Cmovbe = 80,
  Cmove = 81,
  Cmovg = 82,
  Cmovge = 83,
  Cmovl = 84,
  Cmovle = 85,
  Cmovne = 86,
  Cmovno = 87,
  Cmovnp = 88,
  Cmovns = 89,
  Cmovo = 90,
  Cmovp = 91,
  Cmovs = 92,
  Cmp = 93,
  Cmppd = 94,
  Cmpps = 95,
  Cmpsb = 96,
  Cmpsd = 97,
  Cmpsq = 98,
  Cmpss = 99,
  Cmpsw = 100,
  Cmpxchg = 101,
  Cmpxchg16b = 102,
  Cmpxchg8b = 103,
  Comisd = 104,
  Comiss = 105,
  Cpuid = 106,
  Cqo = 107,
  Crc32 = 108,
  Cvtdq2pd = 109,
  Cvtdq2ps = 110,
  Cvtpd2dq = 111,
  Cvtpd2pi = 112,
  Cvtpd2ps = 113,
  Cvtpi2pd = 114,
  Cvtpi2ps = 115,
  Cvtps2dq = 116,
  Cvtps2pd = 117,
  Cvtps2pi = 118,
  Cvtsd2si = 119,
  Cvtsd2ss = 120,
  Cvtsi2sd = 121,
  Cvtsi2ss = 122,
  Cvtss2sd = 123,
  Cvtss2si = 124,
  Cvttpd2dq = 125,
  Cvttpd2pi = 126,
  Cvttps2dq = 127,
  Cvttps2pi = 128,
  Cvttsd2si = 129,
  Cvttss2si = 130,
  Cwd = 131,
  Cwde = 132,
  Daa = 133,
  Das = 134,
  Db = 135,
  Dd = 136,
  Dec = 137,
  Div = 138,
  Divpd = 139,
  Divps = 140,
  Divsd = 141,
  Divss = 142,
  Dppd = 143,
  Dpps = 144,
  Dq = 145,
  Dw = 146,
  Emms = 147,
  Encls = 148,
  Enclu = 149,
  Enclv = 150,
  Endbr32 = 151,
  Endbr64 = 152,
  Enqcmd = 153,
  Enqcmds = 154,
  Enter = 155,
  Extractps = 156,
  Extrq = 157,
  F2xm1 = 158,
  Fabs = 159,
  Fadd = 160,
  Faddp = 161,
  Fbld = 162,
  Fbstp = 163,
  Fchs = 164,
  Fclex = 165,
  Fcmovb = 166,
  Fcmovbe = 167,
  Fcmove = 168,
  Fcmovnb = 169,
  Fcmovnbe = 170,
  Fcmovne = 171,
  Fcmovnu = 172,
  Fcmovu = 173,
  Fcom = 174,
  Fcomi = 175,
  Fcomip = 176,
  Fcomp = 177,
  Fcompp = 178,
  Fcos = 179,
  Fdecstp = 180,
  Fdisi = 181,
  Fdiv = 182,
  Fdivp = 183,
  Fdivr = 184,
  Fdivrp = 185,
  Femms = 186,
  Feni = 187,
  Ffree = 188,
  Ffreep = 189,
  Fiadd = 190,
  Ficom = 191,
  Ficomp = 192,
  Fidiv = 193,
  Fidivr = 194,
  Fild = 195,
  Fimul = 196,
  Fincstp = 197,
  Finit = 198,
  Fist = 199,
  Fistp = 200,
  Fisttp = 201,
  Fisub = 202,
  Fisubr = 203,
  Fld = 204,
  Fld1 = 205,
  Fldcw = 206,
  Fldenv = 207,
  Fldl2e = 208,
  Fldl2t = 209,
  Fldlg2 = 210,
  Fldln2 = 211,
  Fldpi = 212,
  Fldz = 213,
  Fmul = 214,
  Fmulp = 215,
  Fnclex = 216,
  Fndisi = 217,
  Fneni = 218,
  Fninit = 219,
  Fnop = 220,
  Fnsave = 221,
  Fnsetpm = 222,
  Fnstcw = 223,
  Fnstenv = 224,
  Fnstsw = 225,
  Fpatan = 226,
  Fprem = 227,
  Fprem1 = 228,
  Fptan = 229,
  Frndint = 230,
  Frstor = 231,
  Frstpm = 232,
  Fsave = 233,
  Fscale = 234,
  Fsetpm = 235,
  Fsin = 236,
  Fsincos = 237,
  Fsqrt = 238,
  Fst = 239,
  Fstcw = 240,
  Fstdw = 241,
  Fstenv = 242,
  Fstp = 243,
  Fstpnce = 244,
  Fstsg = 245,
  Fstsw = 246,
  Fsub = 247,
  Fsubp = 248,
  Fsubr = 249,
  Fsubrp = 250,
  Ftst = 251,
  Fucom = 252,
  Fucomi = 253,
  Fucomip = 254,
  Fucomp = 255,
  Fucompp = 256,
  Fxam = 257,
  Fxch = 258,
  Fxrstor = 259,
  Fxrstor64 = 260,
  Fxsave = 261,
  Fxsave64 = 262,
  Fxtract = 263,
  Fyl2x = 264,
  Fyl2xp1 = 265,
  Getsec = 266,
  Gf2p8affineinvqb = 267,
  Gf2p8affineqb = 268,
  Gf2p8mulb = 269,
  Haddpd = 270,
  Haddps = 271,
  Hlt = 272,
  Hsubpd = 273,
  Hsubps = 274,
  Ibts = 275,
  Idiv = 276,
  Imul = 277,
  In = 278,
  Inc = 279,
  Incsspd = 280,
  Incsspq = 281,
  Insb = 282,
  Insd = 283,
  Insertps = 284,
  Insertq = 285,
  Insw = 286,
  Int = 287,
  Int1 = 288,
  Into = 289,
  Invd = 290,
  Invept = 291,
  Invlpg = 292,
  Invlpga = 293,
  Invpcid = 294,
  Invvpid = 295,
  Iret = 296,
  Ja = 297,
  Jae = 298,
  Jb = 299,
  Jbe = 300,
  Jcxz = 301,
  Je = 302,
  Jecxz = 303,
  Jg = 304,
  Jge = 305,
  Jl = 306,
  Jle = 307,
  Jmp = 308,
  Jmpe = 309,
  Jne = 310,
  Jno = 311,
  Jnp = 312,
  Jns = 313,
  Jo = 314,
  Jp = 315,
  Jrcxz = 316,
  Js = 317,
  Kaddb = 318,
  Kaddd = 319,
  Kaddq = 320,
  Kaddw = 321,
  Kandb = 322,
  Kandd = 323,
  Kandnb = 324,
  Kandnd = 325,
  Kandnq = 326,
  Kandnw = 327,
  Kandq = 328,
  Kandw = 329,
  Kmovb = 330,
  Kmovd = 331,
  Kmovq = 332,
  Kmovw = 333,
  Knotb = 334,
  Knotd = 335,
  Knotq = 336,
  Knotw = 337,
  Korb = 338,
  Kord = 339,
  Korq = 340,
  Kortestb = 341,
  Kortestd = 342,
  Kortestq = 343,
  Kortestw = 344,
  Korw = 345,
  Kshiftlb = 346,
  Kshiftld = 347,
  Kshiftlq = 348,
  Kshiftlw = 349,
  Kshiftrb = 350,
  Kshiftrd = 351,
  Kshiftrq = 352,
  Kshiftrw = 353,
  Ktestb = 354,
  Ktestd = 355,
  Ktestq = 356,
  Ktestw = 357,
  Kunpckbw = 358,
  Kunpckdq = 359,
  Kunpckwd = 360,
  Kxnorb = 361,
  Kxnord = 362,
  Kxnorq = 363,
  Kxnorw = 364,
  Kxorb = 365,
  Kxord = 366,
  Kxorq = 367,
  Kxorw = 368,
  Lahf = 369,
  Lar = 370,
  Lddqu = 371,
  Ldmxcsr = 372,
  Lds = 373,
  Lea = 374,
  Leave = 375,
  Les = 376,
  Lfence = 377,
  Lfs = 378,
  Lgdt = 379,
  Lgs = 380,
  Lidt = 381,
  Lldt = 382,
  Llwpcb = 383,
  Lmsw = 384,
  Loadall = 385,
  Lodsb = 386,
  Lodsd = 387,
  Lodsq = 388,
  Lodsw = 389,
  Loop = 390,
  Loope = 391,
  Loopne = 392,
  Lsl = 393,
  Lss = 394,
  Ltr = 395,
  Lwpins = 396,
  Lwpval = 397,
  Lzcnt = 398,
  Maskmovdqu = 399,
  Maskmovq = 400,
  Maxpd = 401,
  Maxps = 402,
  Maxsd = 403,
  Maxss = 404,
  Mcommit = 405,
  Mfence = 406,
  Minpd = 407,
  Minps = 408,
  Minsd = 409,
  Minss = 410,
  Monitor = 411,
  Monitorx = 412,
  Montmul = 413,
  Mov = 414,
  Movapd = 415,
  Movaps = 416,
  Movbe = 417,
  Movd = 418,
  Movddup = 419,
  Movdir64b = 420,
  Movdiri = 421,
  Movdq2q = 422,
  Movdqa = 423,
  Movdqu = 424,
  Movhlps = 425,
  Movhpd = 426,
  Movhps = 427,
  Movlhps = 428,
  Movlpd = 429,
  Movlps = 430,
  Movmskpd = 431,
  Movmskps = 432,
  Movntdq = 433,
  Movntdqa = 434,
  Movnti = 435,
  Movntpd = 436,
  Movntps = 437,
  Movntq = 438,
  Movntsd = 439,
  Movntss = 440,
  Movq = 441,
  Movq2dq = 442,
  Movsb = 443,
  Movsd = 444,
  Movshdup = 445,
  Movsldup = 446,
  Movsq = 447,
  Movss = 448,
  Movsw = 449,
  Movsx = 450,
  Movsxd = 451,
  Movupd = 452,
  Movups = 453,
  Movzx = 454,
  Mpsadbw = 455,
  Mul = 456,
  Mulpd = 457,
  Mulps = 458,
  Mulsd = 459,
  Mulss = 460,
  Mulx = 461,
  Mwait = 462,
  Mwaitx = 463,
  Neg = 464,
  Nop = 465,
  Not = 466,
  Or = 467,
  Orpd = 468,
  Orps = 469,
  Out = 470,
  Outsb = 471,
  Outsd = 472,
  Outsw = 473,
  Pabsb = 474,
  Pabsd = 475,
  Pabsw = 476,
  Packssdw = 477,
  Packsswb = 478,
  Packusdw = 479,
  Packuswb = 480,
  Paddb = 481,
  Paddd = 482,
  Paddq = 483,
  Paddsb = 484,
  Paddsw = 485,
  Paddusb = 486,
  Paddusw = 487,
  Paddw = 488,
  Palignr = 489,
  Pand = 490,
  Pandn = 491,
  Pause = 492,
  Pavgb = 493,
  Pavgusb = 494,
  Pavgw = 495,
  Pblendvb = 496,
  Pblendw = 497,
  Pclmulqdq = 498,
  Pcmpeqb = 499,
  Pcmpeqd = 500,
  Pcmpeqq = 501,
  Pcmpeqw = 502,
  Pcmpestri = 503,
  Pcmpestri64 = 504,
  Pcmpestrm = 505,
  Pcmpestrm64 = 506,
  Pcmpgtb = 507,
  Pcmpgtd = 508,
  Pcmpgtq = 509,
  Pcmpgtw = 510,
  Pcmpistri = 511,
  Pcmpistrm = 512,
  Pcommit = 513,
  Pconfig = 514,
  Pdep = 515,
  Pext = 516,
  Pextrb = 517,
  Pextrd = 518,
  Pextrq = 519,
  Pextrw = 520,
  Pf2id = 521,
  Pf2iw = 522,
  Pfacc = 523,
  Pfadd = 524,
  Pfcmpeq = 525,
  Pfcmpge = 526,
  Pfcmpgt = 527,
  Pfmax = 528,
  Pfmin = 529,
  Pfmul = 530,
  Pfnacc = 531,
  Pfpnacc = 532,
  Pfrcp = 533,
  Pfrcpit1 = 534,
  Pfrcpit2 = 535,
  Pfrcpv = 536,
  Pfrsqit1 = 537,
  Pfrsqrt = 538,
  Pfrsqrtv = 539,
  Pfsub = 540,
  Pfsubr = 541,
  Phaddd = 542,
  Phaddsw = 543,
  Phaddw = 544,
  Phminposuw = 545,
  Phsubd = 546,
  Phsubsw = 547,
  Phsubw = 548,
  Pi2fd = 549,
  Pi2fw = 550,
  Pinsrb = 551,
  Pinsrd = 552,
  Pinsrq = 553,
  Pinsrw = 554,
  Pmaddubsw = 555,
  Pmaddwd = 556,
  Pmaxsb = 557,
  Pmaxsd = 558,
  Pmaxsw = 559,
  Pmaxub = 560,
  Pmaxud = 561,
  Pmaxuw = 562,
  Pminsb = 563,
  Pminsd = 564,
  Pminsw = 565,
  Pminub = 566,
  Pminud = 567,
  Pminuw = 568,
  Pmovmskb = 569,
  Pmovsxbd = 570,
  Pmovsxbq = 571,
  Pmovsxbw = 572,
  Pmovsxdq = 573,
  Pmovsxwd = 574,
  Pmovsxwq = 575,
  Pmovzxbd = 576,
  Pmovzxbq = 577,
  Pmovzxbw = 578,
  Pmovzxdq = 579,
  Pmovzxwd = 580,
  Pmovzxwq = 581,
  Pmuldq = 582,
  Pmulhrsw = 583,
  Pmulhrw = 584,
  Pmulhuw = 585,
  Pmulhw = 586,
  Pmulld = 587,
  Pmullw = 588,
  Pmuludq = 589,
  Pop = 590,
  Popa = 591,
  Popcnt = 592,
  Popf = 593,
  Por = 594,
  Prefetch = 595,
  Prefetchnta = 596,
  Prefetcht0 = 597,
  Prefetcht1 = 598,
  Prefetcht2 = 599,
  Prefetchw = 600,
  Prefetchwt1 = 601,
  Psadbw = 602,
  Pshufb = 603,
  Pshufd = 604,
  Pshufhw = 605,
  Pshuflw = 606,
  Pshufw = 607,
  Psignb = 608,
  Psignd = 609,
  Psignw = 610,
  Pslld = 611,
  Pslldq = 612,
  Psllq = 613,
  Psllw = 614,
  Psrad = 615,
  Psraw = 616,
  Psrld = 617,
  Psrldq = 618,
  Psrlq = 619,
  Psrlw = 620,
  Psubb = 621,
  Psubd = 622,
  Psubq = 623,
  Psubsb = 624,
  Psubsw = 625,
  Psubusb = 626,
  Psubusw = 627,
  Psubw = 628,
  Pswapd = 629,
  Ptest = 630,
  Ptwrite = 631,
  Punpckhbw = 632,
  Punpckhdq = 633,
  Punpckhqdq = 634,
  Punpckhwd = 635,
  Punpcklbw = 636,
  Punpckldq = 637,
  Punpcklqdq = 638,
  Punpcklwd = 639,
  Push = 640,
  Pusha = 641,
  Pushf = 642,
  Pxor = 643,
  Rcl = 644,
  Rcpps = 645,
  Rcpss = 646,
  Rcr = 647,
  Rdfsbase = 648,
  Rdgsbase = 649,
  Rdmsr = 650,
  Rdpid = 651,
  Rdpkru = 652,
  Rdpmc = 653,
  Rdpru = 654,
  Rdrand = 655,
  Rdseed = 656,
  Rdsspd = 657,
  Rdsspq = 658,
  Rdtsc = 659,
  Rdtscp = 660,
  Reservednop = 661,
  Ret = 662,
  Retf = 663,
  Rol = 664,
  Ror = 665,
  Rorx = 666,
  Roundpd = 667,
  Roundps = 668,
  Roundsd = 669,
  Roundss = 670,
  Rsm = 671,
  Rsqrtps = 672,
  Rsqrtss = 673,
  Rstorssp = 674,
  Sahf = 675,
  Sal = 676,
  Salc = 677,
  Sar = 678,
  Sarx = 679,
  Saveprevssp = 680,
  Sbb = 681,
  Scasb = 682,
  Scasd = 683,
  Scasq = 684,
  Scasw = 685,
  Seta = 686,
  Setae = 687,
  Setb = 688,
  Setbe = 689,
  Sete = 690,
  Setg = 691,
  Setge = 692,
  Setl = 693,
  Setle = 694,
  Setne = 695,
  Setno = 696,
  Setnp = 697,
  Setns = 698,
  Seto = 699,
  Setp = 700,
  Sets = 701,
  Setssbsy = 702,
  Sfence = 703,
  Sgdt = 704,
  Sha1msg1 = 705,
  Sha1msg2 = 706,
  Sha1nexte = 707,
  Sha1rnds4 = 708,
  Sha256msg1 = 709,
  Sha256msg2 = 710,
  Sha256rnds2 = 711,
  Shl = 712,
  Shld = 713,
  Shlx = 714,
  Shr = 715,
  Shrd = 716,
  Shrx = 717,
  Shufpd = 718,
  Shufps = 719,
  Sidt = 720,
  Skinit = 721,
  Sldt = 722,
  Slwpcb = 723,
  Smsw = 724,
  Sqrtpd = 725,
  Sqrtps = 726,
  Sqrtsd = 727,
  Sqrtss = 728,
  Stac = 729,
  Stc = 730,
  Std = 731,
  Stgi = 732,
  Sti = 733,
  Stmxcsr = 734,
  Stosb = 735,
  Stosd = 736,
  Stosq = 737,
  Stosw = 738,
  Str = 739,
  Sub = 740,
  Subpd = 741,
  Subps = 742,
  Subsd = 743,
  Subss = 744,
  Swapgs = 745,
  Syscall = 746,
  Sysenter = 747,
  Sysexit = 748,
  Sysret = 749,
  T1mskc = 750,
  Test = 751,
  Tpause = 752,
  Tzcnt = 753,
  Tzmsk = 754,
  Ucomisd = 755,
  Ucomiss = 756,
  Ud0 = 757,
  Ud1 = 758,
  Ud2 = 759,
  Umonitor = 760,
  Umov = 761,
  Umwait = 762,
  Unpckhpd = 763,
  Unpckhps = 764,
  Unpcklpd = 765,
  Unpcklps = 766,
  V4fmaddps = 767,
  V4fmaddss = 768,
  V4fnmaddps = 769,
  V4fnmaddss = 770,
  Vaddpd = 771,
  Vaddps = 772,
  Vaddsd = 773,
  Vaddss = 774,
  Vaddsubpd = 775,
  Vaddsubps = 776,
  Vaesdec = 777,
  Vaesdeclast = 778,
  Vaesenc = 779,
  Vaesenclast = 780,
  Vaesimc = 781,
  Vaeskeygenassist = 782,
  Valignd = 783,
  Valignq = 784,
  Vandnpd = 785,
  Vandnps = 786,
  Vandpd = 787,
  Vandps = 788,
  Vblendmpd = 789,
  Vblendmps = 790,
  Vblendpd = 791,
  Vblendps = 792,
  Vblendvpd = 793,
  Vblendvps = 794,
  Vbroadcastf128 = 795,
  Vbroadcastf32x2 = 796,
  Vbroadcastf32x4 = 797,
  Vbroadcastf32x8 = 798,
  Vbroadcastf64x2 = 799,
  Vbroadcastf64x4 = 800,
  Vbroadcasti128 = 801,
  Vbroadcasti32x2 = 802,
  Vbroadcasti32x4 = 803,
  Vbroadcasti32x8 = 804,
  Vbroadcasti64x2 = 805,
  Vbroadcasti64x4 = 806,
  Vbroadcastsd = 807,
  Vbroadcastss = 808,
  Vcmppd = 809,
  Vcmpps = 810,
  Vcmpsd = 811,
  Vcmpss = 812,
  Vcomisd = 813,
  Vcomiss = 814,
  Vcompresspd = 815,
  Vcompressps = 816,
  Vcvtdq2pd = 817,
  Vcvtdq2ps = 818,
  Vcvtne2ps2bf16 = 819,
  Vcvtneps2bf16 = 820,
  Vcvtpd2dq = 821,
  Vcvtpd2ps = 822,
  Vcvtpd2qq = 823,
  Vcvtpd2udq = 824,
  Vcvtpd2uqq = 825,
  Vcvtph2ps = 826,
  Vcvtps2dq = 827,
  Vcvtps2pd = 828,
  Vcvtps2ph = 829,
  Vcvtps2qq = 830,
  Vcvtps2udq = 831,
  Vcvtps2uqq = 832,
  Vcvtqq2pd = 833,
  Vcvtqq2ps = 834,
  Vcvtsd2si = 835,
  Vcvtsd2ss = 836,
  Vcvtsd2usi = 837,
  Vcvtsi2sd = 838,
  Vcvtsi2ss = 839,
  Vcvtss2sd = 840,
  Vcvtss2si = 841,
  Vcvtss2usi = 842,
  Vcvttpd2dq = 843,
  Vcvttpd2qq = 844,
  Vcvttpd2udq = 845,
  Vcvttpd2uqq = 846,
  Vcvttps2dq = 847,
  Vcvttps2qq = 848,
  Vcvttps2udq = 849,
  Vcvttps2uqq = 850,
  Vcvttsd2si = 851,
  Vcvttsd2usi = 852,
  Vcvttss2si = 853,
  Vcvttss2usi = 854,
  Vcvtudq2pd = 855,
  Vcvtudq2ps = 856,
  Vcvtuqq2pd = 857,
  Vcvtuqq2ps = 858,
  Vcvtusi2sd = 859,
  Vcvtusi2ss = 860,
  Vdbpsadbw = 861,
  Vdivpd = 862,
  Vdivps = 863,
  Vdivsd = 864,
  Vdivss = 865,
  Vdpbf16ps = 866,
  Vdppd = 867,
  Vdpps = 868,
  Verr = 869,
  Verw = 870,
  Vexp2pd = 871,
  Vexp2ps = 872,
  Vexpandpd = 873,
  Vexpandps = 874,
  Vextractf128 = 875,
  Vextractf32x4 = 876,
  Vextractf32x8 = 877,
  Vextractf64x2 = 878,
  Vextractf64x4 = 879,
  Vextracti128 = 880,
  Vextracti32x4 = 881,
  Vextracti32x8 = 882,
  Vextracti64x2 = 883,
  Vextracti64x4 = 884,
  Vextractps = 885,
  Vfixupimmpd = 886,
  Vfixupimmps = 887,
  Vfixupimmsd = 888,
  Vfixupimmss = 889,
  Vfmadd132pd = 890,
  Vfmadd132ps = 891,
  Vfmadd132sd = 892,
  Vfmadd132ss = 893,
  Vfmadd213pd = 894,
  Vfmadd213ps = 895,
  Vfmadd213sd = 896,
  Vfmadd213ss = 897,
  Vfmadd231pd = 898,
  Vfmadd231ps = 899,
  Vfmadd231sd = 900,
  Vfmadd231ss = 901,
  Vfmaddpd = 902,
  Vfmaddps = 903,
  Vfmaddsd = 904,
  Vfmaddss = 905,
  Vfmaddsub132pd = 906,
  Vfmaddsub132ps = 907,
  Vfmaddsub213pd = 908,
  Vfmaddsub213ps = 909,
  Vfmaddsub231pd = 910,
  Vfmaddsub231ps = 911,
  Vfmaddsubpd = 912,
  Vfmaddsubps = 913,
  Vfmsub132pd = 914,
  Vfmsub132ps = 915,
  Vfmsub132sd = 916,
  Vfmsub132ss = 917,
  Vfmsub213pd = 918,
  Vfmsub213ps = 919,
  Vfmsub213sd = 920,
  Vfmsub213ss = 921,
  Vfmsub231pd = 922,
  Vfmsub231ps = 923,
  Vfmsub231sd = 924,
  Vfmsub231ss = 925,
  Vfmsubadd132pd = 926,
  Vfmsubadd132ps = 927,
  Vfmsubadd213pd = 928,
  Vfmsubadd213ps = 929,
  Vfmsubadd231pd = 930,
  Vfmsubadd231ps = 931,
  Vfmsubaddpd = 932,
  Vfmsubaddps = 933,
  Vfmsubpd = 934,
  Vfmsubps = 935,
  Vfmsubsd = 936,
  Vfmsubss = 937,
  Vfnmadd132pd = 938,
  Vfnmadd132ps = 939,
  Vfnmadd132sd = 940,
  Vfnmadd132ss = 941,
  Vfnmadd213pd = 942,
  Vfnmadd213ps = 943,
  Vfnmadd213sd = 944,
  Vfnmadd213ss = 945,
  Vfnmadd231pd = 946,
  Vfnmadd231ps = 947,
  Vfnmadd231sd = 948,
  Vfnmadd231ss = 949,
  Vfnmaddpd = 950,
  Vfnmaddps = 951,
  Vfnmaddsd = 952,
  Vfnmaddss = 953,
  Vfnmsub132pd = 954,
  Vfnmsub132ps = 955,
  Vfnmsub132sd = 956,
  Vfnmsub132ss = 957,
  Vfnmsub213pd = 958,
  Vfnmsub213ps = 959,
  Vfnmsub213sd = 960,
  Vfnmsub213ss = 961,
  Vfnmsub231pd = 962,
  Vfnmsub231ps = 963,
  Vfnmsub231sd = 964,
  Vfnmsub231ss = 965,
  Vfnmsubpd = 966,
  Vfnmsubps = 967,
  Vfnmsubsd = 968,
  Vfnmsubss = 969,
  Vfpclasspd = 970,
  Vfpclassps = 971,
  Vfpclasssd = 972,
  Vfpclassss = 973,
  Vfrczpd = 974,
  Vfrczps = 975,
  Vfrczsd = 976,
  Vfrczss = 977,
  Vgatherdpd = 978,
  Vgatherdps = 979,
  Vgatherpf0dpd = 980,
  Vgatherpf0dps = 981,
  Vgatherpf0qpd = 982,
  Vgatherpf0qps = 983,
  Vgatherpf1dpd = 984,
  Vgatherpf1dps = 985,
  Vgatherpf1qpd = 986,
  Vgatherpf1qps = 987,
  Vgatherqpd = 988,
  Vgatherqps = 989,
  Vgetexppd = 990,
  Vgetexpps = 991,
  Vgetexpsd = 992,
  Vgetexpss = 993,
  Vgetmantpd = 994,
  Vgetmantps = 995,
  Vgetmantsd = 996,
  Vgetmantss = 997,
  Vgf2p8affineinvqb = 998,
  Vgf2p8affineqb = 999,
  Vgf2p8mulb = 1000,
  Vhaddpd = 1001,
  Vhaddps = 1002,
  Vhsubpd = 1003,
  Vhsubps = 1004,
  Vinsertf128 = 1005,
  Vinsertf32x4 = 1006,
  Vinsertf32x8 = 1007,
  Vinsertf64x2 = 1008,
  Vinsertf64x4 = 1009,
  Vinserti128 = 1010,
  Vinserti32x4 = 1011,
  Vinserti32x8 = 1012,
  Vinserti64x2 = 1013,
  Vinserti64x4 = 1014,
  Vinsertps = 1015,
  Vlddqu = 1016,
  Vldmxcsr = 1017,
  Vmaskmovdqu = 1018,
  Vmaskmovpd = 1019,
  Vmaskmovps = 1020,
  Vmaxpd = 1021,
  Vmaxps = 1022,
  Vmaxsd = 1023,
  Vmaxss = 1024,
  Vmcall = 1025,
  Vmclear = 1026,
  Vmfunc = 1027,
  Vminpd = 1028,
  Vminps = 1029,
  Vminsd = 1030,
  Vminss = 1031,
  Vmlaunch = 1032,
  Vmload = 1033,
  Vmmcall = 1034,
  Vmovapd = 1035,
  Vmovaps = 1036,
  Vmovd = 1037,
  Vmovddup = 1038,
  Vmovdqa = 1039,
  Vmovdqa32 = 1040,
  Vmovdqa64 = 1041,
  Vmovdqu = 1042,
  Vmovdqu16 = 1043,
  Vmovdqu32 = 1044,
  Vmovdqu64 = 1045,
  Vmovdqu8 = 1046,
  Vmovhlps = 1047,
  Vmovhpd = 1048,
  Vmovhps = 1049,
  Vmovlhps = 1050,
  Vmovlpd = 1051,
  Vmovlps = 1052,
  Vmovmskpd = 1053,
  Vmovmskps = 1054,
  Vmovntdq = 1055,
  Vmovntdqa = 1056,
  Vmovntpd = 1057,
  Vmovntps = 1058,
  Vmovq = 1059,
  Vmovsd = 1060,
  Vmovshdup = 1061,
  Vmovsldup = 1062,
  Vmovss = 1063,
  Vmovupd = 1064,
  Vmovups = 1065,
  Vmpsadbw = 1066,
  Vmptrld = 1067,
  Vmptrst = 1068,
  Vmread = 1069,
  Vmresume = 1070,
  Vmrun = 1071,
  Vmsave = 1072,
  Vmulpd = 1073,
  Vmulps = 1074,
  Vmulsd = 1075,
  Vmulss = 1076,
  Vmwrite = 1077,
  Vmxoff = 1078,
  Vmxon = 1079,
  Vorpd = 1080,
  Vorps = 1081,
  Vp2intersectd = 1082,
  Vp2intersectq = 1083,
  Vp4dpwssd = 1084,
  Vp4dpwssds = 1085,
  Vpabsb = 1086,
  Vpabsd = 1087,
  Vpabsq = 1088,
  Vpabsw = 1089,
  Vpackssdw = 1090,
  Vpacksswb = 1091,
  Vpackusdw = 1092,
  Vpackuswb = 1093,
  Vpaddb = 1094,
  Vpaddd = 1095,
  Vpaddq = 1096,
  Vpaddsb = 1097,
  Vpaddsw = 1098,
  Vpaddusb = 1099,
  Vpaddusw = 1100,
  Vpaddw = 1101,
  Vpalignr = 1102,
  Vpand = 1103,
  Vpandd = 1104,
  Vpandn = 1105,
  Vpandnd = 1106,
  Vpandnq = 1107,
  Vpandq = 1108,
  Vpavgb = 1109,
  Vpavgw = 1110,
  Vpblendd = 1111,
  Vpblendmb = 1112,
  Vpblendmd = 1113,
  Vpblendmq = 1114,
  Vpblendmw = 1115,
  Vpblendvb = 1116,
  Vpblendw = 1117,
  Vpbroadcastb = 1118,
  Vpbroadcastd = 1119,
  Vpbroadcastmb2q = 1120,
  Vpbroadcastmw2d = 1121,
  Vpbroadcastq = 1122,
  Vpbroadcastw = 1123,
  Vpclmulqdq = 1124,
  Vpcmov = 1125,
  Vpcmpb = 1126,
  Vpcmpd = 1127,
  Vpcmpeqb = 1128,
  Vpcmpeqd = 1129,
  Vpcmpeqq = 1130,
  Vpcmpeqw = 1131,
  Vpcmpestri = 1132,
  Vpcmpestri64 = 1133,
  Vpcmpestrm = 1134,
  Vpcmpestrm64 = 1135,
  Vpcmpgtb = 1136,
  Vpcmpgtd = 1137,
  Vpcmpgtq = 1138,
  Vpcmpgtw = 1139,
  Vpcmpistri = 1140,
  Vpcmpistrm = 1141,
  Vpcmpq = 1142,
  Vpcmpub = 1143,
  Vpcmpud = 1144,
  Vpcmpuq = 1145,
  Vpcmpuw = 1146,
  Vpcmpw = 1147,
  Vpcomb = 1148,
  Vpcomd = 1149,
  Vpcompressb = 1150,
  Vpcompressd = 1151,
  Vpcompressq = 1152,
  Vpcompressw = 1153,
  Vpcomq = 1154,
  Vpcomub = 1155,
  Vpcomud = 1156,
  Vpcomuq = 1157,
  Vpcomuw = 1158,
  Vpcomw = 1159,
  Vpconflictd = 1160,
  Vpconflictq = 1161,
  Vpdpbusd = 1162,
  Vpdpbusds = 1163,
  Vpdpwssd = 1164,
  Vpdpwssds = 1165,
  Vperm2f128 = 1166,
  Vperm2i128 = 1167,
  Vpermb = 1168,
  Vpermd = 1169,
  Vpermi2b = 1170,
  Vpermi2d = 1171,
  Vpermi2pd = 1172,
  Vpermi2ps = 1173,
  Vpermi2q = 1174,
  Vpermi2w = 1175,
  Vpermil2pd = 1176,
  Vpermil2ps = 1177,
  Vpermilpd = 1178,
  Vpermilps = 1179,
  Vpermpd = 1180,
  Vpermps = 1181,
  Vpermq = 1182,
  Vpermt2b = 1183,
  Vpermt2d = 1184,
  Vpermt2pd = 1185,
  Vpermt2ps = 1186,
  Vpermt2q = 1187,
  Vpermt2w = 1188,
  Vpermw = 1189,
  Vpexpandb = 1190,
  Vpexpandd = 1191,
  Vpexpandq = 1192,
  Vpexpandw = 1193,
  Vpextrb = 1194,
  Vpextrd = 1195,
  Vpextrq = 1196,
  Vpextrw = 1197,
  Vpgatherdd = 1198,
  Vpgatherdq = 1199,
  Vpgatherqd = 1200,
  Vpgatherqq = 1201,
  Vphaddbd = 1202,
  Vphaddbq = 1203,
  Vphaddbw = 1204,
  Vphaddd = 1205,
  Vphadddq = 1206,
  Vphaddsw = 1207,
  Vphaddubd = 1208,
  Vphaddubq = 1209,
  Vphaddubw = 1210,
  Vphaddudq = 1211,
  Vphadduwd = 1212,
  Vphadduwq = 1213,
  Vphaddw = 1214,
  Vphaddwd = 1215,
  Vphaddwq = 1216,
  Vphminposuw = 1217,
  Vphsubbw = 1218,
  Vphsubd = 1219,
  Vphsubdq = 1220,
  Vphsubsw = 1221,
  Vphsubw = 1222,
  Vphsubwd = 1223,
  Vpinsrb = 1224,
  Vpinsrd = 1225,
  Vpinsrq = 1226,
  Vpinsrw = 1227,
  Vplzcntd = 1228,
  Vplzcntq = 1229,
  Vpmacsdd = 1230,
  Vpmacsdqh = 1231,
  Vpmacsdql = 1232,
  Vpmacssdd = 1233,
  Vpmacssdqh = 1234,
  Vpmacssdql = 1235,
  Vpmacsswd = 1236,
  Vpmacssww = 1237,
  Vpmacswd = 1238,
  Vpmacsww = 1239,
  Vpmadcsswd = 1240,
  Vpmadcswd = 1241,
  Vpmadd52huq = 1242,
  Vpmadd52luq = 1243,
  Vpmaddubsw = 1244,
  Vpmaddwd = 1245,
  Vpmaskmovd = 1246,
  Vpmaskmovq = 1247,
  Vpmaxsb = 1248,
  Vpmaxsd = 1249,
  Vpmaxsq = 1250,
  Vpmaxsw = 1251,
  Vpmaxub = 1252,
  Vpmaxud = 1253,
  Vpmaxuq = 1254,
  Vpmaxuw = 1255,
  Vpminsb = 1256,
  Vpminsd = 1257,
  Vpminsq = 1258,
  Vpminsw = 1259,
  Vpminub = 1260,
  Vpminud = 1261,
  Vpminuq = 1262,
  Vpminuw = 1263,
  Vpmovb2m = 1264,
  Vpmovd2m = 1265,
  Vpmovdb = 1266,
  Vpmovdw = 1267,
  Vpmovm2b = 1268,
  Vpmovm2d = 1269,
  Vpmovm2q = 1270,
  Vpmovm2w = 1271,
  Vpmovmskb = 1272,
  Vpmovq2m = 1273,
  Vpmovqb = 1274,
  Vpmovqd = 1275,
  Vpmovqw = 1276,
  Vpmovsdb = 1277,
  Vpmovsdw = 1278,
  Vpmovsqb = 1279,
  Vpmovsqd = 1280,
  Vpmovsqw = 1281,
  Vpmovswb = 1282,
  Vpmovsxbd = 1283,
  Vpmovsxbq = 1284,
  Vpmovsxbw = 1285,
  Vpmovsxdq = 1286,
  Vpmovsxwd = 1287,
  Vpmovsxwq = 1288,
  Vpmovusdb = 1289,
  Vpmovusdw = 1290,
  Vpmovusqb = 1291,
  Vpmovusqd = 1292,
  Vpmovusqw = 1293,
  Vpmovuswb = 1294,
  Vpmovw2m = 1295,
  Vpmovwb = 1296,
  Vpmovzxbd = 1297,
  Vpmovzxbq = 1298,
  Vpmovzxbw = 1299,
  Vpmovzxdq = 1300,
  Vpmovzxwd = 1301,
  Vpmovzxwq = 1302,
  Vpmuldq = 1303,
  Vpmulhrsw = 1304,
  Vpmulhuw = 1305,
  Vpmulhw = 1306,
  Vpmulld = 1307,
  Vpmullq = 1308,
  Vpmullw = 1309,
  Vpmultishiftqb = 1310,
  Vpmuludq = 1311,
  Vpopcntb = 1312,
  Vpopcntd = 1313,
  Vpopcntq = 1314,
  Vpopcntw = 1315,
  Vpor = 1316,
  Vpord = 1317,
  Vporq = 1318,
  Vpperm = 1319,
  Vprold = 1320,
  Vprolq = 1321,
  Vprolvd = 1322,
  Vprolvq = 1323,
  Vprord = 1324,
  Vprorq = 1325,
  Vprorvd = 1326,
  Vprorvq = 1327,
  Vprotb = 1328,
  Vprotd = 1329,
  Vprotq = 1330,
  Vprotw = 1331,
  Vpsadbw = 1332,
  Vpscatterdd = 1333,
  Vpscatterdq = 1334,
  Vpscatterqd = 1335,
  Vpscatterqq = 1336,
  Vpshab = 1337,
  Vpshad = 1338,
  Vpshaq = 1339,
  Vpshaw = 1340,
  Vpshlb = 1341,
  Vpshld = 1342,
  Vpshldd = 1343,
  Vpshldq = 1344,
  Vpshldvd = 1345,
  Vpshldvq = 1346,
  Vpshldvw = 1347,
  Vpshldw = 1348,
  Vpshlq = 1349,
  Vpshlw = 1350,
  Vpshrdd = 1351,
  Vpshrdq = 1352,
  Vpshrdvd = 1353,
  Vpshrdvq = 1354,
  Vpshrdvw = 1355,
  Vpshrdw = 1356,
  Vpshufb = 1357,
  Vpshufbitqmb = 1358,
  Vpshufd = 1359,
  Vpshufhw = 1360,
  Vpshuflw = 1361,
  Vpsignb = 1362,
  Vpsignd = 1363,
  Vpsignw = 1364,
  Vpslld = 1365,
  Vpslldq = 1366,
  Vpsllq = 1367,
  Vpsllvd = 1368,
  Vpsllvq = 1369,
  Vpsllvw = 1370,
  Vpsllw = 1371,
  Vpsrad = 1372,
  Vpsraq = 1373,
  Vpsravd = 1374,
  Vpsravq = 1375,
  Vpsravw = 1376,
  Vpsraw = 1377,
  Vpsrld = 1378,
  Vpsrldq = 1379,
  Vpsrlq = 1380,
  Vpsrlvd = 1381,
  Vpsrlvq = 1382,
  Vpsrlvw = 1383,
  Vpsrlw = 1384,
  Vpsubb = 1385,
  Vpsubd = 1386,
  Vpsubq = 1387,
  Vpsubsb = 1388,
  Vpsubsw = 1389,
  Vpsubusb = 1390,
  Vpsubusw = 1391,
  Vpsubw = 1392,
  Vpternlogd = 1393,
  Vpternlogq = 1394,
  Vptest = 1395,
  Vptestmb = 1396,
  Vptestmd = 1397,
  Vptestmq = 1398,
  Vptestmw = 1399,
  Vptestnmb = 1400,
  Vptestnmd = 1401,
  Vptestnmq = 1402,
  Vptestnmw = 1403,
  Vpunpckhbw = 1404,
  Vpunpckhdq = 1405,
  Vpunpckhqdq = 1406,
  Vpunpckhwd = 1407,
  Vpunpcklbw = 1408,
  Vpunpckldq = 1409,
  Vpunpcklqdq = 1410,
  Vpunpcklwd = 1411,
  Vpxor = 1412,
  Vpxord = 1413,
  Vpxorq = 1414,
  Vrangepd = 1415,
  Vrangeps = 1416,
  Vrangesd = 1417,
  Vrangess = 1418,
  Vrcp14pd = 1419,
  Vrcp14ps = 1420,
  Vrcp14sd = 1421,
  Vrcp14ss = 1422,
  Vrcp28pd = 1423,
  Vrcp28ps = 1424,
  Vrcp28sd = 1425,
  Vrcp28ss = 1426,
  Vrcpps = 1427,
  Vrcpss = 1428,
  Vreducepd = 1429,
  Vreduceps = 1430,
  Vreducesd = 1431,
  Vreducess = 1432,
  Vrndscalepd = 1433,
  Vrndscaleps = 1434,
  Vrndscalesd = 1435,
  Vrndscaless = 1436,
  Vroundpd = 1437,
  Vroundps = 1438,
  Vroundsd = 1439,
  Vroundss = 1440,
  Vrsqrt14pd = 1441,
  Vrsqrt14ps = 1442,
  Vrsqrt14sd = 1443,
  Vrsqrt14ss = 1444,
  Vrsqrt28pd = 1445,
  Vrsqrt28ps = 1446,
  Vrsqrt28sd = 1447,
  Vrsqrt28ss = 1448,
  Vrsqrtps = 1449,
  Vrsqrtss = 1450,
  Vscalefpd = 1451,
  Vscalefps = 1452,
  Vscalefsd = 1453,
  Vscalefss = 1454,
  Vscatterdpd = 1455,
  Vscatterdps = 1456,
  Vscatterpf0dpd = 1457,
  Vscatterpf0dps = 1458,
  Vscatterpf0qpd = 1459,
  Vscatterpf0qps = 1460,
  Vscatterpf1dpd = 1461,
  Vscatterpf1dps = 1462,
  Vscatterpf1qpd = 1463,
  Vscatterpf1qps = 1464,
  Vscatterqpd = 1465,
  Vscatterqps = 1466,
  Vshuff32x4 = 1467,
  Vshuff64x2 = 1468,
  Vshufi32x4 = 1469,
  Vshufi64x2 = 1470,
  Vshufpd = 1471,
  Vshufps = 1472,
  Vsqrtpd = 1473,
  Vsqrtps = 1474,
  Vsqrtsd = 1475,
  Vsqrtss = 1476,
  Vstmxcsr = 1477,
  Vsubpd = 1478,
  Vsubps = 1479,
  Vsubsd = 1480,
  Vsubss = 1481,
  Vtestpd = 1482,
  Vtestps = 1483,
  Vucomisd = 1484,
  Vucomiss = 1485,
  Vunpckhpd = 1486,
  Vunpckhps = 1487,
  Vunpcklpd = 1488,
  Vunpcklps = 1489,
  Vxorpd = 1490,
  Vxorps = 1491,
  Vzeroall = 1492,
  Vzeroupper = 1493,
  Wait = 1494,
  Wbinvd = 1495,
  Wbnoinvd = 1496,
  Wrfsbase = 1497,
  Wrgsbase = 1498,
  Wrmsr = 1499,
  Wrpkru = 1500,
  Wrssd = 1501,
  Wrssq = 1502,
  Wrussd = 1503,
  Wrussq = 1504,
  Xabort = 1505,
  Xadd = 1506,
  Xbegin = 1507,
  Xbts = 1508,
  Xchg = 1509,
  Xcryptcbc = 1510,
  Xcryptcfb = 1511,
  Xcryptctr = 1512,
  Xcryptecb = 1513,
  Xcryptofb = 1514,
  Xend = 1515,
  Xgetbv = 1516,
  Xlatb = 1517,
  Xor = 1518,
  Xorpd = 1519,
  Xorps = 1520,
  Xrstor = 1521,
  Xrstor64 = 1522,
  Xrstors = 1523,
  Xrstors64 = 1524,
  Xsave = 1525,
  Xsave64 = 1526,
  Xsavec = 1527,
  Xsavec64 = 1528,
  Xsaveopt = 1529,
  Xsaveopt64 = 1530,
  Xsaves = 1531,
  Xsaves64 = 1532,
  Xsetbv = 1533,
  Xsha1 = 1534,
  Xsha256 = 1535,
  Xstore = 1536,
  Xtest = 1537,
  Rmpadjust = 1538,
  Rmpupdate = 1539,
  Psmash = 1540,
  Pvalidate = 1541,
  Serialize = 1542,
  Xsusldtrk = 1543,
  Xresldtrk = 1544,
  Invlpgb = 1545,
  Tlbsync = 1546,
  Vmgexit = 1547,
  Getsecq = 1548,
  Sysexitq = 1549,
  Ldtilecfg = 1550,
  Tilerelease = 1551,
  Sttilecfg = 1552,
  Tilezero = 1553,
  Tileloaddt1 = 1554,
  Tilestored = 1555,
  Tileloadd = 1556,
  Tdpbf16ps = 1557,
  Tdpbuud = 1558,
  Tdpbusd = 1559,
  Tdpbsud = 1560,
  Tdpbssd = 1561,
  Sysretq = 1562,
  Fnstdw = 1563,
  Fnstsg = 1564,
  Rdshr = 1565,
  Wrshr = 1566,
  Smint = 1567,
  Dmint = 1568,
  Rdm = 1569,
  Svdc = 1570,
  Rsdc = 1571,
  Svldt = 1572,
  Rsldt = 1573,
  Svts = 1574,
  Rsts = 1575,
  Bb0_reset = 1576,
  Bb1_reset = 1577,
  Cpu_write = 1578,
  Cpu_read = 1579,
  Altinst = 1580,
  Paveb = 1581,
  Paddsiw = 1582,
  Pmagw = 1583,
  Pdistib = 1584,
  Psubsiw = 1585,
  Pmvzb = 1586,
  Pmvnzb = 1587,
  Pmvlzb = 1588,
  Pmvgezb = 1589,
  Pmulhriw = 1590,
  Pmachriw = 1591,
  Ftstp = 1592,
  Frint2 = 1593,
  Frichop = 1594,
  Frinear = 1595,
  Undoc = 1596,
  Tdcall = 1597,
  Seamret = 1598,
  Seamops = 1599,
  Seamcall = 1600,
  Aesencwide128kl = 1601,
  Aesdecwide128kl = 1602,
  Aesencwide256kl = 1603,
  Aesdecwide256kl = 1604,
  Loadiwkey = 1605,
  Aesenc128kl = 1606,
  Aesdec128kl = 1607,
  Aesenc256kl = 1608,
  Aesdec256kl = 1609,
  Encodekey128 = 1610,
  Encodekey256 = 1611,
  Pushad = 1612,
  Popad = 1613,
  Pushfd = 1614,
  Pushfq = 1615,
  Popfd = 1616,
  Popfq = 1617,
  Iretd = 1618,
  Iretq = 1619,
  Int3 = 1620,
  Uiret = 1621,
  Testui = 1622,
  Clui = 1623,
  Stui = 1624,
  Senduipi = 1625,
  Hreset = 1626,
  Ccs_hash = 1627,
  Ccs_encrypt = 1628,
  Lkgs = 1629,
  Eretu = 1630,
  Erets = 1631,
  Storeall = 1632,
  Vaddph = 1633,
  Vaddsh = 1634,
  Vcmpph = 1635,
  Vcmpsh = 1636,
  Vcomish = 1637,
  Vcvtdq2ph = 1638,
  Vcvtpd2ph = 1639,
  Vcvtph2dq = 1640,
  Vcvtph2pd = 1641,
  Vcvtph2psx = 1642,
  Vcvtph2qq = 1643,
  Vcvtph2udq = 1644,
  Vcvtph2uqq = 1645,
  Vcvtph2uw = 1646,
  Vcvtph2w = 1647,
  Vcvtps2phx = 1648,
  Vcvtqq2ph = 1649,
  Vcvtsd2sh = 1650,
  Vcvtsh2sd = 1651,
  Vcvtsh2si = 1652,
  Vcvtsh2ss = 1653,
  Vcvtsh2usi = 1654,
  Vcvtsi2sh = 1655,
  Vcvtss2sh = 1656,
  Vcvttph2dq = 1657,
  Vcvttph2qq = 1658,
  Vcvttph2udq = 1659,
  Vcvttph2uqq = 1660,
  Vcvttph2uw = 1661,
  Vcvttph2w = 1662,
  Vcvttsh2si = 1663,
  Vcvttsh2usi = 1664,
  Vcvtudq2ph = 1665,
  Vcvtuqq2ph = 1666,
  Vcvtusi2sh = 1667,
  Vcvtuw2ph = 1668,
  Vcvtw2ph = 1669,
  Vdivph = 1670,
  Vdivsh = 1671,
  Vfcmaddcph = 1672,
  Vfmaddcph = 1673,
  Vfcmaddcsh = 1674,
  Vfmaddcsh = 1675,
  Vfcmulcph = 1676,
  Vfmulcph = 1677,
  Vfcmulcsh = 1678,
  Vfmulcsh = 1679,
  Vfmaddsub132ph = 1680,
  Vfmaddsub213ph = 1681,
  Vfmaddsub231ph = 1682,
  Vfmsubadd132ph = 1683,
  Vfmsubadd213ph = 1684,
  Vfmsubadd231ph = 1685,
  Vfmadd132ph = 1686,
  Vfmadd213ph = 1687,
  Vfmadd231ph = 1688,
  Vfnmadd132ph = 1689,
  Vfnmadd213ph = 1690,
  Vfnmadd231ph = 1691,
  Vfmadd132sh = 1692,
  Vfmadd213sh = 1693,
  Vfmadd231sh = 1694,
  Vfnmadd132sh = 1695,
  Vfnmadd213sh = 1696,
  Vfnmadd231sh = 1697,
  Vfmsub132ph = 1698,
  Vfmsub213ph = 1699,
  Vfmsub231ph = 1700,
  Vfnmsub132ph = 1701,
  Vfnmsub213ph = 1702,
  Vfnmsub231ph = 1703,
  Vfmsub132sh = 1704,
  Vfmsub213sh = 1705,
  Vfmsub231sh = 1706,
  Vfnmsub132sh = 1707,
  Vfnmsub213sh = 1708,
  Vfnmsub231sh = 1709,
  Vfpclassph = 1710,
  Vfpclasssh = 1711,
  Vgetexpph = 1712,
  Vgetexpsh = 1713,
  Vgetmantph = 1714,
  Vgetmantsh = 1715,
  Vmaxph = 1716,
  Vmaxsh = 1717,
  Vminph = 1718,
  Vminsh = 1719,
  Vmovsh = 1720,
  Vmovw = 1721,
  Vmulph = 1722,
  Vmulsh = 1723,
  Vrcpph = 1724,
  Vrcpsh = 1725,
  Vreduceph = 1726,
  Vreducesh = 1727,
  Vrndscaleph = 1728,
  Vrndscalesh = 1729,
  Vrsqrtph = 1730,
  Vrsqrtsh = 1731,
  Vscalefph = 1732,
  Vscalefsh = 1733,
  Vsqrtph = 1734,
  Vsqrtsh = 1735,
  Vsubph = 1736,
  Vsubsh = 1737,
  Vucomish = 1738,
  Rdudbg = 1739,
  Wrudbg = 1740,
  Clevict0 = 1741,
  Clevict1 = 1742,
  Delay = 1743,
  Jknzd = 1744,
  Jkzd = 1745,
  Kand = 1746,
  Kandn = 1747,
  Kandnr = 1748,
  Kconcath = 1749,
  Kconcatl = 1750,
  Kextract = 1751,
  Kmerge2l1h = 1752,
  Kmerge2l1l = 1753,
  Kmov = 1754,
  Knot = 1755,
  Kor = 1756,
  Kortest = 1757,
  Kxnor = 1758,
  Kxor = 1759,
  Spflt = 1760,
  Tzcnti = 1761,
  Vaddnpd = 1762,
  Vaddnps = 1763,
  Vaddsetsps = 1764,
  Vcvtfxpntdq2ps = 1765,
  Vcvtfxpntpd2dq = 1766,
  Vcvtfxpntpd2udq = 1767,
  Vcvtfxpntps2dq = 1768,
  Vcvtfxpntps2udq = 1769,
  Vcvtfxpntudq2ps = 1770,
  Vexp223ps = 1771,
  Vfixupnanpd = 1772,
  Vfixupnanps = 1773,
  Vfmadd233ps = 1774,
  Vgatherpf0hintdpd = 1775,
  Vgatherpf0hintdps = 1776,
  Vgmaxabsps = 1777,
  Vgmaxpd = 1778,
  Vgmaxps = 1779,
  Vgminpd = 1780,
  Vgminps = 1781,
  Vloadunpackhd = 1782,
  Vloadunpackhpd = 1783,
  Vloadunpackhps = 1784,
  Vloadunpackhq = 1785,
  Vloadunpackld = 1786,
  Vloadunpacklpd = 1787,
  Vloadunpacklps = 1788,
  Vloadunpacklq = 1789,
  Vlog2ps = 1790,
  Vmovnrapd = 1791,
  Vmovnraps = 1792,
  Vmovnrngoapd = 1793,
  Vmovnrngoaps = 1794,
  Vpackstorehd = 1795,
  Vpackstorehpd = 1796,
  Vpackstorehps = 1797,
  Vpackstorehq = 1798,
  Vpackstoreld = 1799,
  Vpackstorelpd = 1800,
  Vpackstorelps = 1801,
  Vpackstorelq = 1802,
  Vpadcd = 1803,
  Vpaddsetcd = 1804,
  Vpaddsetsd = 1805,
  Vpcmpltd = 1806,
  Vpermf32x4 = 1807,
  Vpmadd231d = 1808,
  Vpmadd233d = 1809,
  Vpmulhd = 1810,
  Vpmulhud = 1811,
  Vprefetch0 = 1812,
  Vprefetch1 = 1813,
  Vprefetch2 = 1814,
  Vprefetche0 = 1815,
  Vprefetche1 = 1816,
  Vprefetche2 = 1817,
  Vprefetchenta = 1818,
  Vprefetchnta = 1819,
  Vpsbbd = 1820,
  Vpsbbrd = 1821,
  Vpsubrd = 1822,
  Vpsubrsetbd = 1823,
  Vpsubsetbd = 1824,
  Vrcp23ps = 1825,
  Vrndfxpntpd = 1826,
  Vrndfxpntps = 1827,
  Vrsqrt23ps = 1828,
  Vscaleps = 1829,
  Vscatterpf0hintdpd = 1830,
  Vscatterpf0hintdps = 1831,
  Vsubrpd = 1832,
  Vsubrps = 1833,
  Xsha512 = 1834,
  Xstore_alt = 1835,
  Xsha512_alt = 1836,
  Zero_bytes = 1837,
  Aadd = 1838,
  Aand = 1839,
  Aor = 1840,
  Axor = 1841,
  Cmpbexadd = 1842,
  Cmpbxadd = 1843,
  Cmplexadd = 1844,
  Cmplxadd = 1845,
  Cmpnbexadd = 1846,
  Cmpnbxadd = 1847,
  Cmpnlexadd = 1848,
  Cmpnlxadd = 1849,
  Cmpnoxadd = 1850,
  Cmpnpxadd = 1851,
  Cmpnsxadd = 1852,
  Cmpnzxadd = 1853,
  Cmpoxadd = 1854,
  Cmppxadd = 1855,
  Cmpsxadd = 1856,
  Cmpzxadd = 1857,
  Prefetchit0 = 1858,
  Prefetchit1 = 1859,
  Rdmsrlist = 1860,
  Rmpquery = 1861,
  Tdpfp16ps = 1862,
  Vbcstnebf162ps = 1863,
  Vbcstnesh2ps = 1864,
  Vcvtneebf162ps = 1865,
  Vcvtneeph2ps = 1866,
  Vcvtneobf162ps = 1867,
  Vcvtneoph2ps = 1868,
  Vpdpbssd = 1869,
  Vpdpbssds = 1870,
  Vpdpbsud = 1871,
  Vpdpbsuds = 1872,
  Vpdpbuud = 1873,
  Vpdpbuuds = 1874,
  Wrmsrlist = 1875,
  Wrmsrns = 1876,
  Tcmmrlfp16ps = 1877,
  Tcmmimfp16ps = 1878,
  Pbndkb = 1879,
  Vpdpwsud = 1880,
  Vpdpwsuds = 1881,
  Vpdpwusd = 1882,
  Vpdpwusds = 1883,
  Vpdpwuud = 1884,
  Vpdpwuuds = 1885,
  Vsha512msg1 = 1886,
  Vsha512msg2 = 1887,
  Vsha512rnds2 = 1888,
  Vsm3msg1 = 1889,
  Vsm3msg2 = 1890,
  Vsm3rnds2 = 1891,
  Vsm4key4 = 1892,
  Vsm4rnds4 = 1893,
  COUNT
};

enum class Register : uint8_t {
  None = 0,
  AL = 1,
  CL = 2,
  DL = 3,
  BL = 4,
  AH = 5,
  CH = 6,
  DH = 7,
  BH = 8,
  SPL = 9,
  BPL = 10,
  SIL = 11,
  DIL = 12,
  R8L = 13,
  R9L = 14,
  R10L = 15,
  R11L = 16,
  R12L = 17,
  R13L = 18,
  R14L = 19,
  R15L = 20,
  AX = 21,
  CX = 22,
  DX = 23,
  BX = 24,
  SP = 25,
  BP = 26,
  SI = 27,
  DI = 28,
  R8W = 29,
  R9W = 30,
  R10W = 31,
  R11W = 32,
  R12W = 33,
  R13W = 34,
  R14W = 35,
  R15W = 36,
  EAX = 37,
  ECX = 38,
  EDX = 39,
  EBX = 40,
  ESP = 41,
  EBP = 42,
  ESI = 43,
  EDI = 44,
  R8D = 45,
  R9D = 46,
  R10D = 47,
  R11D = 48,
  R12D = 49,
  R13D = 50,
  R14D = 51,
  R15D = 52,
  RAX = 53,
  RCX = 54,
  RDX = 55,
  RBX = 56,
  RSP = 57,
  RBP = 58,
  RSI = 59,
  RDI = 60,
  R8 = 61,
  R9 = 62,
  R10 = 63,
  R11 = 64,
  R12 = 65,
  R13 = 66,
  R14 = 67,
  R15 = 68,
  EIP = 69,
  RIP = 70,
  ES = 71,
  CS = 72,
  SS = 73,
  DS = 74,
  FS = 75,
  GS = 76,
  XMM0 = 77,
  XMM1 = 78,
  XMM2 = 79,
  XMM3 = 80,
  XMM4 = 81,
  XMM5 = 82,
  XMM6 = 83,
  XMM7 = 84,
  XMM8 = 85,
  XMM9 = 86,
  XMM10 = 87,
  XMM11 = 88,
  XMM12 = 89,
  XMM13 = 90,
  XMM14 = 91,
  XMM15 = 92,
  XMM16 = 93,
  XMM17 = 94,
  XMM18 = 95,
  XMM19 = 96,
  XMM20 = 97,
  XMM21 = 98,
  XMM22 = 99,
  XMM23 = 100,
  XMM24 = 101,
  XMM25 = 102,
  XMM26 = 103,
  XMM27 = 104,
  XMM28 = 105,
  XMM29 = 106,
  XMM30 = 107,
  XMM31 = 108,
  YMM0 = 109,
  YMM1 = 110,
  YMM2 = 111,
  YMM3 = 112,
  YMM4 = 113,
  YMM5 = 114,
  YMM6 = 115,
  YMM7 = 116,
  YMM8 = 117,
  YMM9 = 118,
  YMM10 = 119,
  YMM11 = 120,
  YMM12 = 121,
  YMM13 = 122,
  YMM14 = 123,
  YMM15 = 124,
  YMM16 = 125,
  YMM17 = 126,
  YMM18 = 127,
  YMM19 = 128,
  YMM20 = 129,
  YMM21 = 130,
  YMM22 = 131,
  YMM23 = 132,
  YMM24 = 133,
  YMM25 = 134,
  YMM26 = 135,
  YMM27 = 136,
  YMM28 = 137,
  YMM29 = 138,
  YMM30 = 139,
  YMM31 = 140,
  ZMM0 = 141,
  ZMM1 = 142,
  ZMM2 = 143,
  ZMM3 = 144,
  ZMM4 = 145,
  ZMM5 = 146,
  ZMM6 = 147,
  ZMM7 = 148,
  ZMM8 = 149,
  ZMM9 = 150,
  ZMM10 = 151,
  ZMM11 = 152,
  ZMM12 = 153,
  ZMM13 = 154,
  ZMM14 = 155,
  ZMM15 = 156,
  ZMM16 = 157,
  ZMM17 = 158,
  ZMM18 = 159,
  ZMM19 = 160,
  ZMM20 = 161,
  ZMM21 = 162,
  ZMM22 = 163,
  ZMM23 = 164,
  ZMM24 = 165,
  ZMM25 = 166,
  ZMM26 = 167,
  ZMM27 = 168,
  ZMM28 = 169,
  ZMM29 = 170,
  ZMM30 = 171,
  ZMM31 = 172,
  K0 = 173,
  K1 = 174,
  K2 = 175,
  K3 = 176,
  K4 = 177,
  K5 = 178,
  K6 = 179,
  K7 = 180,
  BND0 = 181,
  BND1 = 182,
  BND2 = 183,
  BND3 = 184,
  CR0 = 185,
  CR1 = 186,
  CR2 = 187,
  CR3 = 188,
  CR4 = 189,
  CR5 = 190,
  CR6 = 191,
  CR7 = 192,
  CR8 = 193,
  CR9 = 194,
  CR10 = 195,
  CR11 = 196,
  CR12 = 197,
  CR13 = 198,
  CR14 = 199,
  CR15 = 200,
  DR0 = 201,
  DR1 = 202,
  DR2 = 203,
  DR3 = 204,
  DR4 = 205,
  DR5 = 206,
  DR6 = 207,
  DR7 = 208,
  DR8 = 209,
  DR9 = 210,
  DR10 = 211,
  DR11 = 212,
  DR12 = 213,
  DR13 = 214,
  DR14 = 215,
  DR15 = 216,
  ST0 = 217,
  ST1 = 218,
  ST2 = 219,
  ST3 = 220,
  ST4 = 221,
  ST5 = 222,
  ST6 = 223,
  ST7 = 224,
  MM0 = 225,
  MM1 = 226,
  MM2 = 227,
  MM3 = 228,
  MM4 = 229,
  MM5 = 230,
  MM6 = 231,
  MM7 = 232,
  TR0 = 233,
  TR1 = 234,
  TR2 = 235,
  TR3 = 236,
  TR4 = 237,
  TR5 = 238,
  TR6 = 239,
  TR7 = 240,
  TMM0 = 241,
  TMM1 = 242,
  TMM2 = 243,
  TMM3 = 244,
  TMM4 = 245,
  TMM5 = 246,
  TMM6 = 247,
  TMM7 = 248,

  /// Don't use it!
  DontUse0 = 249,
  /// Don't use it!
  DontUseFA = 250,
  /// Don't use it!
  DontUseFB = 251,
  /// Don't use it!
  DontUseFC = 252,
  /// Don't use it!
  DontUseFD = 253,
  /// Don't use it!
  DontUseFE = 254,
  /// Don't use it!
  DontUseFF = 255,
};

enum class OpKind : uint8_t {
  Invalid,
  Register8,
  Register16,
  Register32,
  Register64,
  Register128,
  Register256,
  Register512,
  Memory8,
  Memory16,
  Memory32,
  Memory64,
  Memory128,
  Memory256,
  Memory512,
  Immediate8,
  Immediate8_2nd,
  Immediate16,
  Immediate32,
  Immediate64,
  NearBranch,
  FarBranch,
  End = FarBranch
};

enum class OpKindSimple : uint8_t {
  Invalid,
  Register,
  Memory,
  Immediate,
  NearBranch,
  FarBranch,
  End = FarBranch
};

enum class FlowControl {
  Next = 0,
  UnconditionalBranch = 1,
  IndirectBranch = 2,
  ConditionalBranch = 3,
  Return = 4,
  Call = 5,
  IndirectCall = 6,
  Interrupt = 7,
  XbeginXabortXend = 8,
  Exception = 9,
};

struct IcedAttribute {
  uint8_t rep : 1;
  uint8_t repne : 1; 
  uint8_t lock : 1;
  uint8_t reserved : 5;
};

namespace __iced_internal
{
  struct IcedInstruction {
    Mnemonic mnemonic;
    Register mem_base;
    Register mem_index;
    uint8_t mem_scale;
    uint8_t stack_growth;
    Register regs [ 4 ];
    OpKind types [ 4 ];
    IcedAttribute attributes;
    uint8_t length;
    uint8_t operand_count_visible;
    uint64_t immediate;
    union {
      uint64_t mem_disp;
      uint64_t immediate2;
    };
    Register segment_prefix;
    bool is_broadcast;
    char text[64];
  };

  static_assert( offsetof ( IcedInstruction, mnemonic ) == 0, "invalid offset" );
  static_assert( offsetof ( IcedInstruction, mem_base ) == 2, "invalid offset" );
  static_assert( offsetof ( IcedInstruction, mem_index ) == 3, "invalid offset" );
  static_assert( offsetof ( IcedInstruction, mem_scale ) == 4, "invalid offset" );
  static_assert( offsetof ( IcedInstruction, stack_growth ) == 5, "invalid offset" );
  static_assert( offsetof ( IcedInstruction, regs ) == 6, "invalid offset" );
  static_assert( offsetof ( IcedInstruction, types ) == 10, "invalid offset" );
  static_assert( offsetof ( IcedInstruction, attributes ) == 14, "invalid offset" );
  static_assert( offsetof ( IcedInstruction, length ) == 15, "invalid offset" );
  static_assert( offsetof ( IcedInstruction, operand_count_visible ) == 16, "invalid offset" );
  static_assert( offsetof ( IcedInstruction, immediate ) == 24, "invalid offset" );
  static_assert( offsetof ( IcedInstruction, immediate2 ) == 32, "invalid offset" );
}
#endif
```

`memory.hpp`:

```hpp
#ifndef KUBERA_MEMORY_HPP
#define KUBERA_MEMORY_HPP

#include <cstdint>
#include <unordered_map>
#include <map>
#include <array>
#include <cstring>
#include <memory>
#include <print>
#include "types.hpp"

namespace kubera
{
	constexpr auto verbose_memory = true;
	class VirtualMemory {
	public:
		explicit VirtualMemory ( std::size_t page_sz = 0x1000 );
		~VirtualMemory ( );

		[[nodiscard]] uint64_t alloc ( std::size_t size, uint8_t prot, std::size_t alignment = 0x1000, bool commit_immediately = false );
		[[nodiscard]] uint64_t alloc_at ( uint64_t base_addr, std::size_t size, uint8_t prot, std::size_t alignment = 0x1000, bool commit_immediately = false );
		[[nodiscard]] uint8_t* commit ( std::size_t size );
		void uncommit ( uint8_t* data );
		[[nodiscard]] uint64_t load ( const void* data, std::size_t size, uint8_t prot, std::size_t alignment = 0x1000 );
		void free ( uint64_t addr, std::size_t size );
		bool protect ( uint64_t addr, std::size_t size, uint8_t prot );
		[[nodiscard]] uint32_t map_to_win_protect ( uint64_t addr );
		[[nodiscard]] WinMemoryBasicInformation get_memory_basic_information ( uint64_t addr );
		[[nodiscard]] Page* get_page ( uint64_t addr );
		void set_read_hook ( uint64_t addr, std::function<void ( VirtualMemory*, uint64_t addr, std::size_t size )> hook );

		template<typename T> [[nodiscard]] T read ( uint64_t addr );
		template<typename T> void write ( uint64_t addr, T val );
		void read_bytes ( uint64_t addr, void* dest, std::size_t size, uint8_t access = PageProtection::READ );
		void write_bytes ( uint64_t addr, const void* src, std::size_t size, uint8_t access = PageProtection::WRITE );
		[[nodiscard]] void* translate ( uint64_t addr, uint8_t access, bool silent = false );
		[[nodiscard]] void* translate_bypass ( uint64_t addr, bool silent = false );
		[[nodiscard]] bool check ( uint64_t addr, std::size_t size, uint8_t access );

		std::size_t page_size;

	private:
		std::unordered_map<uint64_t, std::unique_ptr<Page>> pages;
		std::map<uint64_t, Region> regions;
		uint64_t next_alloc { 0x100000000ULL };

		struct CacheEntry { uint64_t virt; Page* page; };
		std::array<CacheEntry, 16> cache;
		std::size_t cache_pos { 0 };

		const Region* find_region ( uint64_t addr ) const;
		void split_region ( uint64_t base, uint64_t split_start, uint64_t split_end, uint8_t new_protect );
	};

	constexpr auto PAGE_ALIGN = 4096;

	template<typename T>
	inline T VirtualMemory::read ( uint64_t addr ) {
		T val {0};
		uint8_t* dest = reinterpret_cast< uint8_t* >( &val );
		std::size_t remaining = sizeof ( T );
		uint64_t current = addr;
		while ( remaining > 0 ) {
			void* src = translate ( current, PageProtection::READ );
			if ( !src ) return T {};
			std::size_t offset = current % page_size;
			std::size_t to_copy = std::min ( remaining, page_size - offset );
			std::memcpy ( dest, src, to_copy );
			dest += to_copy;
			current += to_copy;
			remaining -= to_copy;
		}
		return val;
	}

	template<>
	inline uint128_t VirtualMemory::read ( uint64_t addr ) {
		uint64_t low = read<uint64_t> ( addr );
		uint64_t high = read<uint64_t> ( addr + 8 );
		uint128_t result = uint128_t ( high );
		result <<= 64;
		result |= low;
		return result;
	}

	template<>
	inline uint256_t VirtualMemory::read ( uint64_t addr ) {
		uint256_t result = 0;
		for ( int i = 0; i < 4; i++ ) {
			uint64_t part = read<uint64_t> ( addr + i * 8 );
			result |= uint256_t ( part ) << ( i * 64 );
		}
		return result;
	}

	template<>
	inline uint512_t VirtualMemory::read ( uint64_t addr ) {
		uint512_t result = 0;
		for ( int i = 0; i < 8; i++ ) {
			uint64_t part = read<uint64_t> ( addr + i * 8 );
			result |= uint512_t ( part ) << ( i * 64 );
		}
		return result;
	}

	template<typename T>
	inline void VirtualMemory::write ( uint64_t addr, T val ) {
		const uint8_t* src = reinterpret_cast< const uint8_t* > ( &val );
		std::size_t remaining = sizeof ( T );
		uint64_t current = addr;
		while ( remaining > 0 ) {
			void* dest = translate ( current, PageProtection::WRITE );
			if ( !dest ) return;
			std::size_t offset = current % page_size;
			std::size_t to_copy = std::min ( remaining, page_size - offset );
			std::memcpy ( dest, src, to_copy );
			src += to_copy;
			current += to_copy;
			remaining -= to_copy;
		}
	}

	template<>
	inline void VirtualMemory::write ( uint64_t addr, uint128_t val ) {
		uint64_t low = static_cast< uint64_t >( val & uint128_t ( 0xFFFFFFFFFFFFFFFFULL ) );
		uint64_t high = static_cast< uint64_t >( val >> 64 );
		write<uint64_t> ( addr, low );
		write<uint64_t> ( addr + 8, high );
	}

	template<>
	inline void VirtualMemory::write ( uint64_t addr, uint256_t val ) {
		for ( int i = 0; i < 4; i++ ) {
			uint64_t part = static_cast< uint64_t > ( ( val >> ( i * 64 ) ) & uint256_t ( 0xFFFFFFFFFFFFFFFFULL ) );
			write<uint64_t> ( addr + i * 8, part );
		}
	}

	template<>
	inline void VirtualMemory::write ( uint64_t addr, uint512_t val ) {
		for ( int i = 0; i < 8; i++ ) {
			uint64_t part = static_cast< uint64_t > ( ( val >> ( i * 64 ) ) & uint512_t ( 0xFFFFFFFFFFFFFFFFULL ) );
			write<uint64_t> ( addr + i * 8, part );
		}
	}

	inline void VirtualMemory::read_bytes ( uint64_t addr, void* dest, std::size_t size, uint8_t access ) {
		uint8_t* d = static_cast< uint8_t* > ( dest );
		std::size_t remaining = size;
		uint64_t current = addr;
		while ( remaining > 0 ) {
			void* src = translate ( current, access );
			if ( !src ) {
				std::memset ( d, 0, remaining );
				return;
			}
			std::size_t offset = current % page_size;
			std::size_t to_copy = std::min ( remaining, page_size - offset );
			std::memcpy ( d, src, to_copy );
			d += to_copy;
			current += to_copy;
			remaining -= to_copy;
		}
	}

	inline void VirtualMemory::write_bytes ( uint64_t addr, const void* src, std::size_t size, uint8_t access ) {
		const uint8_t* s = static_cast< const uint8_t* >( src );
		std::size_t remaining = size;
		uint64_t current = addr;
		while ( remaining > 0 ) {
			void* dest = translate ( current, access );
			if ( !dest ) return;
			std::size_t offset = current % page_size;
			std::size_t to_copy = std::min ( remaining, page_size - offset );
			std::memcpy ( dest, s, to_copy );
			s += to_copy;
			current += to_copy;
			remaining -= to_copy;
		}
	}
} // namespace kubera

#endif
```

`references.txt`:

```txt
Changwei Zou, Yaoqing Gao, Jingling Xue - Practical Software-Based Shadow Stacks on x86-64 https://dl.acm.org/doi/10.1145/3556977
```

`sign_extend.hpp`:

```hpp
#pragma once

#include <type_traits>
#include <cstdint>
#include "types.hpp"

template <typename T>
struct BitWidth {
  static constexpr size_t value = 0;
};

template <> struct BitWidth<uint64_t> { static constexpr size_t value = 64; };
template <> struct BitWidth<uint128_t> { static constexpr size_t value = 128; };
template <> struct BitWidth<uint256_t> { static constexpr size_t value = 256; };
template <> struct BitWidth<uint512_t> { static constexpr size_t value = 512; };

template <typename T>
struct SignedType {
  using type = void;
};

template <> struct SignedType<uint64_t> { using type = int64_t; };
template <> struct SignedType<uint128_t> { using type = int128_t; };
template <> struct SignedType<uint256_t> { using type = int256_t; };
template <> struct SignedType<uint512_t> { using type = int512_t; };

template <typename T>
inline typename SignedType<T>::type sign_extend ( T value, size_t op_size ) {
  if ( op_size == 0 || op_size > BitWidth<T>::value / 8 ) {
    throw std::runtime_error ( "Invalid operand size for sign extension" );
  }
  using SignedT = typename SignedType<T>::type;
  constexpr size_t bit_width = BitWidth<T>::value;
  return static_cast< SignedT >( value << ( bit_width - op_size * 8 ) ) >> ( bit_width - op_size * 8 );
}

#define SIGN_EXTEND(value, op_size) sign_extend(value, op_size)
```

`src/handlers/arithmetic.cpp`:

```cpp
#include "../../emulator.hpp"
#include <bit>
#include "helpers.hpp"

using namespace kubera;
/// ADD - Add
void handlers::add ( const iced::Instruction& instr, KUBERA& context ) {
	const size_t op_size = instr.op0_size ( );
	const uint64_t a = helpers::get_operand_value<uint64_t> ( instr, 0u, context );
	const uint64_t b = helpers::get_operand_value<uint64_t> ( instr, 1u, context );
	const uint64_t mask = GET_OPERAND_MASK ( op_size );

	const uint64_t ua = a & mask;
	const uint64_t ub = b & mask;
	const uint64_t res = ( ua + ub ) & mask;

	const int64_t sa = SIGN_EXTEND ( ua, op_size );
	const int64_t sb = SIGN_EXTEND ( ub, op_size );
	const int64_t sres = SIGN_EXTEND ( res, op_size );

	auto& flags = context.get_flags ( );
	flags.CF = res < ua;
	flags.ZF = res == 0;
	flags.PF = std::popcount ( res & 0xFF ) % 2 == 0;
	flags.AF = ( ( ua & 0xF ) + ( ub & 0xF ) ) > 0xF; // Carry from bit 3 to 4
	flags.SF = ( res >> ( op_size * 8 - 1 ) ) & 1;
	flags.OF = ( sa > 0 && sb > 0 && sres < 0 ) || ( sa < 0 && sb < 0 && sres > 0 );

	helpers::set_operand_value<uint64_t> ( instr, 0u, res, context );
}
/// SUB - Sub
void handlers::sub ( const iced::Instruction& instr, KUBERA& context ) {
	const size_t op_size = instr.op0_size ( );
	const uint64_t a = helpers::get_operand_value<uint64_t> ( instr, 0u, context );
	const uint64_t b = helpers::get_operand_value<uint64_t> ( instr, 1u, context );
	const uint64_t mask = GET_OPERAND_MASK ( op_size );

	const uint64_t ua = a & mask;
	const uint64_t ub = b & mask;
	const uint64_t res = ( ua - ub ) & mask;

	const int64_t sa = SIGN_EXTEND ( ua, op_size );
	const int64_t sb = SIGN_EXTEND ( ub, op_size );
	const int64_t sres = SIGN_EXTEND ( res, op_size );

	auto& flags = context.get_flags ( );
	flags.CF = ua < ub;
	flags.ZF = res == 0;
	flags.PF = std::popcount ( res & 0xFF ) % 2 == 0; // Even parity for low 8 bits
	flags.AF = ( ( ua ^ ub ^ res ) & 0x10 ) != 0; // Borrow from bit 3 to 4
	flags.SF = sres < 0;
	flags.OF = ( sa >= 0 && sb < 0 && sres < 0 ) || ( sa < 0 && sb >= 0 && sres >= 0 );

	helpers::set_operand_value<uint64_t> ( instr, 0u, res, context );
}

/// INC - Increment
void handlers::inc ( const iced::Instruction& instr, KUBERA& context ) {
	const size_t op_size = instr.op0_size ( );
	const uint64_t a = helpers::get_operand_value<uint64_t> ( instr, 0u, context );
	const uint64_t mask = GET_OPERAND_MASK ( op_size );
	const uint64_t ua = a & mask;
	const uint64_t res = ( ua + 1 ) & mask;

	const int64_t sa = SIGN_EXTEND ( ua, op_size );
	const int64_t sres = SIGN_EXTEND ( res, op_size );

	auto& flags = context.get_flags ( );
	flags.OF = ( sa >= 0 && sres < 0 );
	flags.SF = ( res >> ( op_size * 8 - 1 ) ) & 1;
	flags.ZF = ( res == 0 );
	flags.PF = std::popcount ( res & 0xFF ) % 2 == 0;
	flags.AF = ( ( ua & 0xF ) + 1 ) > 0xF;

	helpers::set_operand_value<uint64_t> ( instr, 0u, res, context );
}

/// DEC - Decrement
void handlers::dec ( const iced::Instruction& instr, KUBERA& context ) {
	const size_t op_size = instr.op0_size ( );
	const uint64_t a = helpers::get_operand_value<uint64_t> ( instr, 0u, context );
	const uint64_t mask = GET_OPERAND_MASK ( op_size );
	const uint64_t ua = a & mask;
	const uint64_t res = ( ua - 1 ) & mask;

	const int64_t sa = SIGN_EXTEND ( ua, op_size );
	const int64_t sres = SIGN_EXTEND ( res, op_size );

	auto& flags = context.get_flags ( );
	flags.OF = ( sa < 0 && sres >= 0 );
	flags.SF = ( res >> ( op_size * 8 - 1 ) ) & 1;
	flags.ZF = ( res == 0 );
	flags.PF = std::popcount ( res & 0xFF ) % 2 == 0;
	flags.AF = ( ua & 0xF ) == 0;

	helpers::set_operand_value<uint64_t> ( instr, 0u, res, context );
}

template <typename OpType, typename Func>
void multiply ( const iced::Instruction& instr, KUBERA& context, Func set_flags ) {
	const size_t op_size = instr.op0_size ( );
	const uint64_t src_val = helpers::get_operand_value<uint64_t> ( instr, 0u, context );
	const uint64_t mask = GET_OPERAND_MASK ( op_size );

	Register low_reg, high_reg;
	switch ( op_size ) {
		case 1: low_reg = Register::AL; high_reg = Register::AH; break;
		case 2: low_reg = Register::AX; high_reg = Register::DX; break;
		case 4: low_reg = Register::EAX; high_reg = Register::EDX; break;
		case 8: low_reg = Register::RAX; high_reg = Register::RDX; break;
		default: UNREACHABLE ( );
	}

	const uint64_t acc_val = context.get_reg ( low_reg, op_size );
	const uint128_t full_res =
		uint128_t ( static_cast< OpType >( acc_val ) ) * uint128_t ( static_cast< OpType >( src_val ) );

	const uint64_t low_res = static_cast< uint64_t >( full_res & mask );
	const uint64_t high_res = static_cast< uint64_t >( full_res >> ( op_size * 8 ) );

	auto& flags = context.get_flags ( );
	set_flags ( flags, low_res, high_res, op_size );

	context.set_reg ( low_reg, low_res, op_size );
	context.set_reg ( high_reg, high_res, op_size );
}

/// MUL - Multiply
void handlers::mul ( const iced::Instruction& instr, KUBERA& context ) {
	multiply<uint64_t> ( instr, context, [ ] ( auto& flags, uint64_t low_res, uint64_t high_res, size_t op_size )
	{
		flags.CF = flags.OF = ( high_res != 0 );
	} );
}

template <size_t OpCount>
void imul_handler ( const iced::Instruction& instr, KUBERA& context );

template <>
void imul_handler<1> ( const iced::Instruction& instr, KUBERA& context ) {
	const size_t op_size = instr.op0_size ( );
	const int64_t src_val = static_cast< int64_t >( helpers::get_operand_value<uint64_t> ( instr, 0u, context ) );
	const uint64_t mask = GET_OPERAND_MASK ( op_size );
	Register low_reg, high_reg;
	switch ( op_size ) {
		case 1: low_reg = Register::AL; high_reg = Register::AH; break;
		case 2: low_reg = Register::AX; high_reg = Register::DX; break;
		case 4: low_reg = Register::EAX; high_reg = Register::EDX; break;
		case 8: low_reg = Register::RAX; high_reg = Register::RDX; break;
		default: UNREACHABLE ( );
	}
	const int64_t acc_val = static_cast< int64_t >( context.get_reg ( low_reg, op_size ) );
	const int128_t full_res = static_cast< int128_t >( acc_val ) * static_cast< int128_t >( src_val );
	const uint64_t low_res = static_cast< uint64_t >( full_res );
	const uint64_t high_res = static_cast< uint64_t >( full_res >> ( op_size * 8 ) );
	context.set_reg ( low_reg, low_res, op_size );
	context.set_reg ( high_reg, high_res, op_size );
	auto& flags = context.get_flags ( );
	const int64_t sext_low = SIGN_EXTEND ( low_res & mask, op_size );
	flags.CF = flags.OF = !( sext_low == static_cast< int64_t >( full_res ) );
	// unofficial but well-defined
	flags.SF = ( low_res & ( 1ULL << ( ( op_size * 8 ) - 1 ) ) ) != 0;
	flags.ZF = ( low_res & mask ) == 0;
	flags.PF = std::popcount ( low_res & 0xFF ) == 0;
	flags.AF = 0;
}

template <>
void imul_handler<2> ( const iced::Instruction& instr, KUBERA& context ) {
	const size_t op_size = instr.op0_size ( );
	const int64_t src1_val = static_cast< int64_t >( helpers::get_operand_value<uint64_t> ( instr, 0u, context ) );
	const int64_t src2_val = static_cast< int64_t >( helpers::get_operand_value<uint64_t> ( instr, 1u, context ) );
	const uint64_t mask = GET_OPERAND_MASK ( op_size );
	const int128_t res128 = static_cast< int128_t >( src1_val ) * static_cast< int128_t >( src2_val );
	const uint64_t res64 = static_cast< uint64_t >( static_cast< int64_t >( res128 ) );
	helpers::set_operand_value<uint64_t> ( instr, 0u, res64, context );
	auto& flags = context.get_flags ( );
	const int64_t sext_res = SIGN_EXTEND ( res64 & mask, op_size );
	flags.CF = flags.OF = !( sext_res == static_cast< int64_t >( res128 ) );
	// unofficial but well-defined
	flags.SF = ( res64 & ( 1ULL << ( ( op_size * 8 ) - 1 ) ) ) != 0;
	flags.ZF = ( res64 & mask ) == 0;
	flags.PF = std::popcount ( res64 & 0xFF ) == 0;
	flags.AF = 0;
}

template <>
void imul_handler<3> ( const iced::Instruction& instr, KUBERA& context ) {
	const size_t op_size = instr.op0_size ( );
	const int64_t src1_val = static_cast< int64_t >( helpers::get_operand_value<uint64_t> ( instr, 1u, context ) );
	const int64_t src2_val = static_cast< int64_t >( helpers::get_operand_value<uint64_t> ( instr, 2u, context ) );
	const uint64_t mask = GET_OPERAND_MASK ( op_size );
	const int128_t res128 = static_cast< int128_t >( src1_val ) * static_cast< int128_t >( src2_val );
	const uint64_t res64 = static_cast< uint64_t >( static_cast< int64_t >( res128 ) );
	helpers::set_operand_value<uint64_t> ( instr, 0u, res64, context );
	auto& flags = context.get_flags ( );
	const int64_t sext_res = SIGN_EXTEND ( res64 & mask, op_size );
	flags.CF = flags.OF = !( sext_res == static_cast< int64_t >( res128 ) );
	// unofficial but well-defined
	flags.SF = ( res64 & ( 1ULL << ( ( op_size * 8 ) - 1 ) ) ) != 0;
	flags.ZF = ( res64 & mask ) == 0;
	flags.PF = std::popcount ( res64 & 0xFF ) == 0;
	flags.AF = 0;
}

/// IMUL - Signed multiply
void handlers::imul ( const iced::Instruction& instr, KUBERA& context ) {
	switch ( instr.op_count ( ) ) {
		case 1: return imul_handler<1> ( instr, context );
		case 2: return imul_handler<2> ( instr, context );
		case 3: return imul_handler<3> ( instr, context );
		default:
			UNREACHABLE ( );
	}
}

template <typename DivType, typename QuotType, typename Func>
void divide ( const iced::Instruction& instr, KUBERA& context, Func build_dividend ) {
	const size_t op_size = instr.op0_size ( );
	const DivType divisor = static_cast< DivType >( helpers::get_operand_value<uint64_t> ( instr, 0u, context ) );

	Register quotient_reg, remainder_reg;
	switch ( op_size ) {
		case 1: quotient_reg = Register::AL; remainder_reg = Register::AH; break;
		case 2: quotient_reg = Register::AX; remainder_reg = Register::DX; break;
		case 4: quotient_reg = Register::EAX; remainder_reg = Register::EDX; break;
		case 8: quotient_reg = Register::RAX; remainder_reg = Register::RDX; break;
		default: UNREACHABLE ( );
	}

	auto dividend = build_dividend ( context, op_size, quotient_reg, remainder_reg );

	QuotType quotient_res = 0, remainder_res = 0;
	bool overflow = false;
	if constexpr ( std::is_same_v<DivType, uint64_t> ) {
		overflow = helpers::divide_unsigned_boost ( dividend, divisor, op_size, quotient_res, remainder_res );
	}
	else {
		overflow = helpers::divide_signed_boost ( dividend, divisor, op_size, quotient_res, remainder_res );
	}

	if ( overflow ) {
		// !TODO(exception)
	}

	context.set_reg ( quotient_reg, static_cast< uint64_t >( quotient_res ), op_size );
	context.set_reg ( remainder_reg, static_cast< uint64_t >( remainder_res ), op_size );
}

/// DIV - Divide
void handlers::div ( const iced::Instruction& instr, KUBERA& context ) {
	divide<uint64_t, uint64_t> ( instr, context, [ ] ( KUBERA& ctx, size_t op_size, Register quot_reg, Register rem_reg ) -> uint128_t
	{
		if ( op_size == 1 ) {
			return ctx.get_reg ( Register::AX, 2 );
		}
		return ( uint128_t ( ctx.get_reg ( rem_reg, op_size ) ) << ( op_size * 8 ) ) | ctx.get_reg ( quot_reg, op_size );
	} );
}

/// IDIV - Signed divide
void handlers::idiv ( const iced::Instruction& instr, KUBERA& context ) {
	divide<int64_t, int64_t> ( instr, context, [ ] ( KUBERA& ctx, size_t op_size, Register quot_reg, Register rem_reg ) -> int128_t
	{
		if ( op_size == 1 ) {
			return static_cast< int16_t >( ctx.get_reg ( Register::AX, 2 ) );
		}
		if ( op_size == 2 ) {
			uint16_t high = static_cast< uint16_t >( ctx.get_reg ( rem_reg, op_size ) );
			uint16_t low = static_cast< uint16_t >( ctx.get_reg ( quot_reg, op_size ) );
			return ( static_cast< int64_t >( high ) << 16 ) | low;
		}
		if ( op_size == 4 ) {
			uint32_t high = static_cast< uint32_t >( ctx.get_reg ( rem_reg, op_size ) );
			uint32_t low = static_cast< uint32_t >( ctx.get_reg ( quot_reg, op_size ) );
			return ( static_cast< int64_t >( high ) << 32 ) | low;
		}
		return ( int128_t ( ctx.get_reg ( rem_reg, 8 ) ) << 64 ) | ctx.get_reg ( quot_reg, 8 );
	} );
}

template <typename Func>
void add_sub_with_carry ( const iced::Instruction& instr, KUBERA& context, Func operation ) {
	const size_t op_size = instr.op0_size ( );
	const uint64_t a = helpers::get_operand_value<uint64_t> ( instr, 0u, context );
	const uint64_t b = helpers::get_operand_value<uint64_t> ( instr, 1u, context );
	const uint64_t mask = GET_OPERAND_MASK ( op_size );

	const uint64_t ua = a & mask;
	const uint64_t ub = b & mask;
	const uint64_t carry = context.get_flags ( ).CF ? 1 : 0;
	const uint64_t res = operation ( ua, ub, carry ) & mask;

	const int64_t sa = SIGN_EXTEND ( ua, op_size );
	const int64_t sb = SIGN_EXTEND ( ub, op_size );
	const int64_t sres = SIGN_EXTEND ( res, op_size );

	auto& flags = context.get_flags ( );
	flags.CF = operation ( ua, ub, carry ) > mask;
	flags.ZF = ( res == 0 );
	flags.PF = std::popcount ( res & 0xFF ) % 2 == 0;
	flags.AF = ( operation ( ua & 0xF, ub & 0xF, carry ) & 0x10 ) != 0;
	flags.SF = ( res >> ( op_size * 8 - 1 ) ) & 1;
	flags.OF = ( sa > 0 && sb > 0 && sres < 0 ) || ( sa < 0 && sb < 0 && sres > 0 );

	helpers::set_operand_value<uint64_t> ( instr, 0u, res, context );
}

/// ADC-Add With Carry
/// Adds the source operand, destination operand, and carry flag, storing the result in the destination and updating flags.
void handlers::adc ( const iced::Instruction& instr, KUBERA& context ) {
	add_sub_with_carry ( instr, context, [ ] ( uint64_t ua, uint64_t ub, uint64_t carry )
	{
		return ua + ub + carry;
	} );
}

/// SBB-Integer Subtraction With Borrow
/// Subtracts the source operand and carry flag from the destination operand, storing the result in the destination and updating flags.
void handlers::sbb ( const iced::Instruction& instr, KUBERA& context ) {
	add_sub_with_carry ( instr, context, [ ] ( uint64_t ua, uint64_t ub, uint64_t carry )
	{
		return ua - ub - carry;
	} );
}

/// NEG-Two's Complement Negation
/// Negates the destination operand by computing its two's complement, storing the result and updating flags.
void handlers::neg ( const iced::Instruction& instr, KUBERA& context ) {
	const size_t op_size = instr.op0_size ( );
	const uint64_t a = helpers::get_operand_value<uint64_t> ( instr, 0u, context );
	const uint64_t mask = GET_OPERAND_MASK ( op_size );
	const uint64_t ua = a & mask;
	const uint64_t res = ( ~ua + 1 ) & mask;

	const int64_t sa = SIGN_EXTEND ( ua, op_size );
	const int64_t sres = SIGN_EXTEND ( res, op_size );

	auto& flags = context.get_flags ( );
	flags.CF = ( ua != 0 );
	flags.ZF = ( res == 0 );
	flags.PF = std::popcount ( res & 0xFF ) % 2 == 0;
	flags.AF = ( ua & 0xF ) != 0;
	flags.SF = ( res >> ( op_size * 8 - 1 ) ) & 1;
	const uint64_t min_signed = static_cast< uint64_t >( 1ULL << ( op_size * 8 - 1 ) );
	flags.OF = ( ua == min_signed );

	helpers::set_operand_value<uint64_t> ( instr, 0u, res, context );
}

/// XADD-Exchange and Add
/// Exchanges the destination and source operands, adds them, stores the sum in the destination, and updates flags.
void handlers::xadd ( const iced::Instruction& instr, KUBERA& context ) {
	const size_t op_size = instr.op0_size ( );
	const uint64_t a = helpers::get_operand_value<uint64_t> ( instr, 0u, context );
	const uint64_t b = helpers::get_operand_value<uint64_t> ( instr, 1u, context );
	const uint64_t mask = GET_OPERAND_MASK ( op_size );

	const uint64_t ua = a & mask;
	const uint64_t ub = b & mask;
	const uint64_t res = ( ua + ub ) & mask;

	const int64_t sa = SIGN_EXTEND ( ua, op_size );
	const int64_t sb = SIGN_EXTEND ( ub, op_size );
	const int64_t sres = SIGN_EXTEND ( res, op_size );

	auto& flags = context.get_flags ( );
	flags.CF = ( res < ua );
	flags.ZF = ( res == 0 );
	flags.PF = std::popcount ( res & 0xFF ) % 2 == 0;
	flags.AF = ( ( ua & 0xF ) + ( ub & 0xF ) ) > 0xF;
	flags.SF = ( res >> ( op_size * 8 - 1 ) ) & 1;
	flags.OF = ( sa > 0 && sb > 0 && sres < 0 ) || ( sa < 0 && sb < 0 && sres > 0 );

	helpers::set_operand_value<uint64_t> ( instr, 0u, res, context );
	helpers::set_operand_value<uint64_t> ( instr, 1u, ua, context );
}

/// CDQ-Convert Doubleword to Quadword
/// Sign-extends EAX into EDX:EAX, setting EDX to 0xFFFFFFFF if EAX is negative, or 0 otherwise.
void handlers::cdq ( const iced::Instruction& instr, KUBERA& context ) {
	const auto eax = context.get_reg ( Register::EAX, 4 );
	int32_t eax_val = static_cast< int32_t > ( eax );
	uint32_t edx_val = ( eax_val < 0 ) ? 0xFFFFFFFF : 0;
	context.set_reg ( Register::EDX, edx_val, 4 );
}

/// CDQE-Convert Doubleword to Quadword Extended
/// Sign-extends EAX into RAX.
void handlers::cdqe ( const iced::Instruction& instr, KUBERA& context ) {
	const auto eax = context.get_reg ( Register::EAX, 4 );
	int32_t eax_val = static_cast< int32_t > ( eax );
	int64_t sign_extended = static_cast< int64_t > ( eax_val );
	context.set_reg ( Register::RAX, sign_extended, 8 );
}

/// CWD-Convert Word to Doubleword
/// Sign-extends AX into EAX and DX, setting DX to 0xFFFF if AX is negative, or 0 otherwise, and EAX to sign-extended AX.
void handlers::cwd ( const iced::Instruction& instr, KUBERA& context ) {
	const auto ax = context.get_reg ( Register::AX, 2 );
	int16_t ax_val = static_cast< int16_t >( ax );
	int32_t eax_val = static_cast< int32_t >( ax_val );
	uint16_t dx_val = ( ax_val < 0 ) ? 0xFFFF : 0;
	context.set_reg ( Register::EAX, eax_val, 4 );
	context.set_reg ( Register::DX, dx_val, 2 );
}

/// CQO-Convert Quadword to Octoword
/// Sign-extends RAX into RDX:RAX, setting RDX to 0xFFFFFFFFFFFFFFFF if RAX is negative, or 0 otherwise.
void handlers::cqo ( const iced::Instruction& instr, KUBERA& context ) {
	auto rax = context.get_reg ( Register::RAX, 8 );
	uint64_t rax_val = rax;
	uint64_t rdx_val = ( rax_val >> 63 ) ? 0xFFFFFFFFFFFFFFFFULL : 0;
	context.set_reg ( Register::RAX, rax_val, 8 );
	context.set_reg ( Register::RDX, rdx_val, 8 );
}

/// CWDE-Convert Word to Doubleword Extended
/// Sign-extends AX into EAX.
void handlers::cwde ( const iced::Instruction& instr, KUBERA& context ) {
	auto ax = context.get_reg ( Register::AX, 2 );
	int16_t ax_val = static_cast< int16_t >( ax );
	int32_t eax_val = static_cast< int32_t >( ax_val );
	context.set_reg ( Register::EAX, eax_val, 4 );
}

/// CBW-Convert Byte to Word
/// Sign-extends AL into AX.
void handlers::cbw ( const iced::Instruction& instr, KUBERA& context ) {
	auto al = context.get_reg ( Register::AL, 1 );
	int8_t al_val = static_cast< int8_t >( al );
	int16_t ax_val = static_cast< int16_t >( al_val );
	context.set_reg ( Register::AX, ax_val, 2 );
}

```

`src/handlers/bit.cpp`:

```cpp
#include "../../emulator.hpp"
#include <bit>
#include "helpers.hpp"

#include <stdint.h>

#ifdef _MSC_VER
#include <stdlib.h>
#define bswap32(x) _byteswap_ulong(x)
#define bswap64(x) _byteswap_uint64(x)
#elif defined(__GNUC__) || defined(__clang__)
#define bswap32(x) __builtin_bswap32(x)
#define bswap64(x) __builtin_bswap64(x)
#else
// Fallback implementation
static inline uint32_t bswap32 ( uint32_t x ) {
	return ( ( x & 0xFF000000u ) >> 24 ) |
		( ( x & 0x00FF0000u ) >> 8 ) |
		( ( x & 0x0000FF00u ) << 8 ) |
		( ( x & 0x000000FFu ) << 24 );
}

static inline uint64_t bswap64 ( uint64_t x ) {
	return ( ( x & 0xFF00000000000000ull ) >> 56 ) |
		( ( x & 0x00FF000000000000ull ) >> 40 ) |
		( ( x & 0x0000FF0000000000ull ) >> 24 ) |
		( ( x & 0x000000FF00000000ull ) >> 8 ) |
		( ( x & 0x00000000FF000000ull ) << 8 ) |
		( ( x & 0x0000000000FF0000ull ) << 24 ) |
		( ( x & 0x000000000000FF00ull ) << 40 ) |
		( ( x & 0x00000000000000FFull ) << 56 );
}
#endif


using namespace kubera;

inline int find_highest_bit ( uint64_t value ) {
	if ( value == 0 ) return -1; // or handle zero case as needed

#ifdef _MSC_VER
	unsigned long index;
	_BitScanReverse64 ( &index, value );
	return ( int ) index;
#else
	return 63 - __builtin_clzll ( value );
#endif
}

/// BZHI-Bit Zero High
/// Zeros the high bits of the source operand starting from the index in the second operand, stores the result in the destination, and updates flags.
void handlers::bzhi ( const iced::Instruction& instr, KUBERA& context ) {
	const size_t op_size = instr.op0_size ( );
	const uint64_t src = helpers::get_operand_value<uint64_t> ( instr, 1u, context );
	const uint64_t index_reg_val = helpers::get_operand_value<uint64_t> ( instr, 2u, context );
	const uint64_t mask = GET_OPERAND_MASK ( op_size );

	const uint64_t src_val = src & mask;
	const uint8_t index_pos = index_reg_val & 0xFF;
	const size_t size_in_bits = instr.op0_bit_width ( );

	uint64_t temp_mask = 0;
	if ( index_pos != 0 && index_pos < size_in_bits ) {
		temp_mask = ( 1ULL << index_pos ) - 1;
	}
	else if ( index_pos >= size_in_bits ) {
		temp_mask = mask;
	}

	const uint64_t res = src_val & temp_mask;
	helpers::set_operand_value<uint64_t> ( instr, 0u, res, context );

	auto& flags = context.get_flags ( );
	flags.CF = ( index_pos >= size_in_bits );
	flags.ZF = ( res == 0 );
	flags.OF = 0;
	flags.SF = 0;
	flags.AF = 0;
	flags.PF = 0;
}

/// ANDN-Logical AND NOT
/// Performs a bitwise AND of the inverted first source operand with the second source operand, stores the result in the destination, and updates flags.
void handlers::andn ( const iced::Instruction& instr, KUBERA& context ) {
	const size_t op_size = instr.op0_size ( );
	const uint64_t src1 = helpers::get_operand_value<uint64_t> ( instr, 1u, context );
	const uint64_t src2 = helpers::get_operand_value<uint64_t> ( instr, 2u, context );
	const uint64_t mask = GET_OPERAND_MASK ( op_size );

	const uint64_t res = ( ~src1 & src2 ) & mask;

	auto& flags = context.get_flags ( );
	flags.ZF = ( res == 0 );
	flags.SF = ( res >> ( op_size * 8 - 1 ) ) & 1;
	flags.OF = 0;
	flags.CF = 0;
	flags.PF = std::popcount ( res & 0xFF ) % 2 == 0;
	flags.AF = 0;

	helpers::set_operand_value<uint64_t> ( instr, 0u, res, context );
}

/// BEXTR-Bit Field Extract
/// Extracts a bit field from the first source operand based on the start and length in the second source operand, stores the result in the destination, and updates flags.
void handlers::bextr ( const iced::Instruction& instr, KUBERA& context ) {
	const size_t op_size = instr.op0_size ( );
	const uint64_t src = helpers::get_operand_value<uint64_t> ( instr, 1u, context );
	const uint64_t control = helpers::get_operand_value<uint64_t> ( instr, 2u, context );
	const uint64_t mask = GET_OPERAND_MASK ( op_size );

	const uint64_t src_val = src & mask;
	const uint8_t start = control & 0xFF;
	const uint8_t len = ( control >> 8 ) & 0xFF;
	const size_t bits_in_operand = instr.op0_bit_width ( );

	uint64_t res = 0;
	if ( len != 0 ) {
		uint64_t result_mask = ( len >= bits_in_operand ) ? mask : ( ( 1ULL << len ) - 1 );
		if ( start < bits_in_operand ) {
			res = ( src_val >> start ) & result_mask;
		}
	}

	helpers::set_operand_value<uint64_t> ( instr, 0u, res, context );

	auto& flags = context.get_flags ( );
	flags.ZF = ( res == 0 );
	flags.CF = 0;
	flags.OF = 0;
	flags.SF = 0;
	flags.PF = 0;
	flags.AF = 0;
}

/// POPCNT-Population Count
/// Counts the number of 1 bits in the source operand, stores the count in the destination, and updates flags.
void handlers::popcnt ( const iced::Instruction& instr, KUBERA& context ) {
	const size_t op_size = instr.op0_size ( );
	const uint64_t src = helpers::get_operand_value<uint64_t> ( instr, 1u, context );
	const uint64_t mask = GET_OPERAND_MASK ( op_size );
	const uint64_t val = src & mask;
	const uint64_t result = std::popcount ( val );

	helpers::set_operand_value<uint64_t> ( instr, 0u, result, context );

	auto& flags = context.get_flags ( );
	flags.ZF = ( result == 0 );
	flags.CF = 0;
	flags.OF = 0;
	flags.SF = 0;
	flags.PF = 0;
	flags.AF = 0;
}

/// BSWAP-Byte Swap
/// Reverses the byte order of the destination operand and stores the result.
void handlers::bswap ( const iced::Instruction& instr, KUBERA& context ) {
	const size_t op_size = instr.op0_size ( );
	uint64_t val = helpers::get_operand_value<uint64_t> ( instr, 0u, context );
	if ( op_size == 4 ) val = bswap32 ( static_cast< uint32_t >( val ) );
	else if ( op_size == 8 ) val = bswap64 ( val );

	helpers::set_operand_value<uint64_t> ( instr, 0u, val, context );
}

/// BT-Bit Test
/// Tests the bit at the index specified in the second operand in the first operand and sets the CF flag to the bit value.
void handlers::bt ( const iced::Instruction& instr, KUBERA& context ) {
	const size_t op_size = instr.op0_size ( );
	const uint64_t src1 = helpers::get_operand_value<uint64_t> ( instr, 0u, context );
	const uint64_t src2 = helpers::get_operand_value<uint64_t> ( instr, 1u, context );
	const uint64_t mask = GET_OPERAND_MASK ( op_size );

	const uint64_t val = src1 & mask;
	const uint64_t bit_idx = src2 & ( op_size * 8 - 1 );

	auto& flags = context.get_flags ( );
	flags.CF = ( val >> bit_idx ) & 1;
}

template <typename Func>
void bit_test_and_modify ( const iced::Instruction& instr, KUBERA& context, Func modify_op ) {
	const size_t op_size = instr.op0_size ( );
	const uint64_t src1 = helpers::get_operand_value<uint64_t> ( instr, 0u, context );
	const uint64_t src2 = helpers::get_operand_value<uint64_t> ( instr, 1u, context );
	const uint64_t mask = GET_OPERAND_MASK ( op_size );

	const uint64_t val = src1 & mask;
	const uint64_t bit_idx = src2 & ( op_size * 8 - 1 );
	const uint64_t original_bit = ( val >> bit_idx ) & 1;
	const uint64_t result = modify_op ( val, bit_idx );

	auto& flags = context.get_flags ( );
	flags.CF = original_bit;

	helpers::set_operand_value<uint64_t> ( instr, 0u, result, context );
}

/// BTS-Bit Test and Set
/// Tests the bit at the index specified in the second operand in the first operand, sets the CF flag to the bit value, and sets the bit to 1 in the first operand.
void handlers::bts ( const iced::Instruction& instr, KUBERA& context ) {
	bit_test_and_modify ( instr, context, [ ] ( uint64_t val, uint64_t bit_idx )
	{
		return val | ( 1ULL << bit_idx );
	} );
}

/// BTR-Bit Test and Reset
/// Tests the bit at the index specified in the second operand in the first operand, sets the CF flag to the bit value, and clears the bit to 0 in the first operand.
void handlers::btr ( const iced::Instruction& instr, KUBERA& context ) {
	bit_test_and_modify ( instr, context, [ ] ( uint64_t val, uint64_t bit_idx )
	{
		return val & ~( 1ULL << bit_idx );
	} );
}

/// BTC-Bit Test and Complement
/// Tests the bit at the index specified in the second operand in the first operand, sets the CF flag to the bit value, and complements the bit in the first operand.
void handlers::btc ( const iced::Instruction& instr, KUBERA& context ) {
	const size_t op_size = instr.op0_size ( );
	const uint64_t src1 = helpers::get_operand_value<uint64_t> ( instr, 0u, context );
	const uint64_t src2 = helpers::get_operand_value<uint64_t> ( instr, 1u, context );
	const uint64_t mask = GET_OPERAND_MASK ( op_size );

	const uint64_t val = src1 & mask;
	const uint64_t bit_idx = src2 & ( op_size * 8 - 1 );
	const uint64_t original_bit = ( val >> bit_idx ) & 1;
	const uint64_t result = val ^ ( 1ULL << bit_idx );

	auto& flags = context.get_flags ( );
	flags.CF = original_bit;

	helpers::set_operand_value<uint64_t> ( instr, 0u, result, context );
}

/// BSR-Bit Scan Reverse
/// Scans the source operand for the most significant set bit, stores its index in the destination, and sets ZF if the source is zero.
void handlers::bsr ( const iced::Instruction& instr, KUBERA& context ) {
	const size_t op_size = instr.op0_size ( );
	const uint64_t src_val = helpers::get_operand_value<uint64_t> ( instr, 1u, context );
	const uint64_t mask = GET_OPERAND_MASK ( op_size );
	const uint64_t val = src_val & mask;

	auto& flags = context.get_flags ( );
	if ( val == 0 ) {
		flags.ZF = 1;
	}
	else {
		flags.ZF = 0;
		unsigned long index = 0;
		if ( op_size == 8 ) index = find_highest_bit ( val );
		else if ( op_size == 4 ) index = find_highest_bit ( static_cast< uint32_t >( val ) );
		else if ( op_size == 2 ) index = find_highest_bit ( static_cast< uint16_t >( val ) );

		helpers::set_operand_value<uint64_t> ( instr, 0u, static_cast< uint64_t >( index ), context );
	}
}

/// TZCNT-Count Trailing Zeros
/// Counts the number of trailing zero bits in the source operand, stores the count in the destination, and updates CF and ZF flags.
void handlers::tzcnt ( const iced::Instruction& instr, KUBERA& context ) {
	const size_t op_size = instr.op0_size ( );
	const uint64_t src_val = helpers::get_operand_value<uint64_t> ( instr, 1u, context );
	const uint64_t mask = GET_OPERAND_MASK ( op_size );
	const uint64_t val = src_val & mask;
	const uint8_t size_in_bits = static_cast<uint8_t>(op_size * 8);

	uint64_t result_count = ( val == 0 ) ? size_in_bits : std::countr_zero ( val );

	auto& flags = context.get_flags ( );
	flags.CF = ( val == 0 );
	flags.ZF = ( result_count == 0 );

	helpers::set_operand_value<uint64_t> ( instr, 0u, result_count, context );
}

/// BSF-Bit Scan Forward
/// Scans the source operand for the least significant set bit, stores its index in the destination, and sets ZF if the source is zero.
void handlers::bsf ( const iced::Instruction& instr, KUBERA& context ) {
	const size_t op_size = instr.op0_size ( );
	const uint64_t src_val = helpers::get_operand_value<uint64_t> ( instr, 1u, context );
	const uint64_t mask = GET_OPERAND_MASK ( op_size );
	const uint64_t val = src_val & mask;

	auto& flags = context.get_flags ( );
	if ( val == 0 ) {
		flags.ZF = 1; // Source is zero, destination unchanged
	}
	else {
		flags.ZF = 0;
		unsigned long index = 0;
		if ( op_size == 8 ) index = find_highest_bit ( val );
		else if ( op_size == 4 ) index = find_highest_bit ( static_cast< uint32_t >( val ) );
		else if ( op_size == 2 ) index = find_highest_bit ( static_cast< uint16_t >( val ) );
		helpers::set_operand_value<uint64_t> ( instr, 0u, static_cast< uint64_t >( index ), context );
	}
}

```

`src/handlers/bitwise_logical.cpp`:

```cpp
#include "../../emulator.hpp"
#include <bit>
#include "helpers.hpp"

using namespace kubera;

enum class RotateSide : bool {
	RIGHT = 0,
	LEFT = 1,
};

enum class RotateCarry : bool {
	WITHOUT = 0,
	WITH = 1,
};

template <RotateSide side, RotateCarry rot_carry, typename Func>
void rotate ( const iced::Instruction& instr, KUBERA& context, Func rotate_op ) {
	const size_t op_size = instr.op0_size ( );
	const uint64_t src_count = helpers::get_operand_value<uint64_t> ( instr, 1u, context );
	const uint64_t val = helpers::get_operand_value<uint64_t> ( instr, 0u, context );
	const uint64_t mask = GET_OPERAND_MASK ( op_size );

	const uint64_t current_val = val & mask;
	const uint8_t size_in_bits = static_cast< uint8_t >( op_size * 8 );
	const uint8_t count_mask = ( op_size == 8 ) ? 0x3F : 0x1F;
	uint8_t rot = src_count & count_mask;
	if constexpr ( side == RotateSide::LEFT ) {
		rot = rot % size_in_bits;
	}
	else if constexpr ( rot_carry == RotateCarry::WITH ) {
		rot = ( op_size == 1 ) ? rot % 9 : ( op_size == 2 ) ? rot % 17 : rot % size_in_bits;
	}

	uint64_t final_result = current_val;
	uint64_t final_cf = context.get_flags ( ).CF;

	if ( rot != 0 ) {
		auto [result, cf] = rotate_op ( current_val, rot, size_in_bits, final_cf );
		final_result = result & mask;
		final_cf = cf;
	}

	helpers::set_operand_value<uint64_t> ( instr, 0u, final_result, context );

	auto& flags = context.get_flags ( );
	if ( rot != 0 ) {
		flags.CF = final_cf;
		if ( rot == 1 ) {
			flags.OF = rotate_op ( final_result, rot, size_in_bits, final_cf ).second;
		}
	}
}

/// ROL-Rotate Left
/// Rotates the destination operand left by the number of bits specified in the second operand, updating flags.
void handlers::rol ( const iced::Instruction& instr, KUBERA& context ) {
	rotate<RotateSide::LEFT, RotateCarry::WITHOUT> ( instr, context, [ ] ( uint64_t val, uint8_t rot, uint8_t size_in_bits, uint64_t cf )
	{
		uint64_t temp_val = val;
		uint64_t temp_cf = 0;
		for ( uint8_t i = 0; i < rot; ++i ) {
			temp_cf = ( temp_val >> ( size_in_bits - 1 ) ) & 1;
			temp_val = ( ( temp_val << 1 ) | temp_cf );
		}
		return std::make_pair ( temp_val, temp_val & 1 );
	} );
}

/// ROR-Rotate Right
/// Rotates the destination operand right by the number of bits specified in the second operand, updating flags.
void handlers::ror ( const iced::Instruction& instr, KUBERA& context ) {
	rotate<RotateSide::RIGHT, RotateCarry::WITHOUT> ( instr, context, [ ] ( uint64_t val, uint8_t rot, uint8_t size_in_bits, uint64_t cf )
	{
		uint64_t temp_val = val;
		uint64_t temp_cf = 0;
		for ( uint8_t i = 0; i < rot; ++i ) {
			temp_cf = temp_val & 1;
			temp_val = ( temp_val >> 1 ) | ( temp_cf << ( size_in_bits - 1 ) );
		}
		uint64_t msb = ( temp_val >> ( size_in_bits - 1 ) ) & 1;
		uint64_t msb_minus_1 = ( size_in_bits > 1 ) ? ( temp_val >> ( size_in_bits - 2 ) ) & 1 : temp_cf;
		return std::make_pair ( temp_val, msb ^ msb_minus_1 );
	} );
}

/// RCL-Rotate Left Through Carry
/// Rotates the destination operand left through the carry flag by the number of bits specified in the second operand, updating flags.
void handlers::rcl ( const iced::Instruction& instr, KUBERA& context ) {
	rotate<RotateSide::LEFT, RotateCarry::WITH> ( instr, context, [ ] ( uint64_t val, uint8_t rot, uint8_t size_in_bits, uint64_t cf )
	{
		uint64_t temp_val = val;
		uint64_t temp_cf = cf;
		for ( uint8_t i = 0; i < rot; ++i ) {
			uint64_t msb = ( temp_val >> ( size_in_bits - 1 ) ) & 1;
			temp_val = ( ( temp_val << 1 ) | temp_cf );
			temp_cf = msb;
		}
		return std::make_pair ( temp_val, temp_val >> ( size_in_bits - 1 ) );
	} );
}

/// RCR-Rotate Right Through Carry
/// Rotates the destination operand right through the carry flag by the number of bits specified in the second operand, updating flags.
void handlers::rcr ( const iced::Instruction& instr, KUBERA& context ) {
	rotate<RotateSide::RIGHT, RotateCarry::WITH> ( instr, context, [ ] ( uint64_t val, uint8_t rot, uint8_t size_in_bits, uint64_t cf )
	{
		uint64_t temp_val = val;
		uint64_t temp_cf = cf;
		for ( uint8_t i = 0; i < rot; ++i ) {
			uint64_t lsb = temp_val & 1;
			temp_val = ( temp_val >> 1 ) | ( temp_cf << ( size_in_bits - 1 ) );
			temp_cf = lsb;
		}
		uint64_t msb = ( temp_val >> ( size_in_bits - 1 ) ) & 1;
		uint64_t msb_minus_1 = ( size_in_bits > 1 ) ? ( temp_val >> ( size_in_bits - 2 ) ) & 1 : temp_cf;
		return std::make_pair ( temp_val, msb ^ msb_minus_1 );
	} );
}

/// AND-Logical AND
/// Performs a bitwise AND of the first and second operands, stores the result in the first operand, and updates flags.
void handlers::and_ ( const iced::Instruction& instr, KUBERA& context ) {
	const size_t op_size = instr.op0_size ( );
	const uint64_t a = helpers::get_operand_value<uint64_t> ( instr, 0u, context );
	const uint64_t b = helpers::get_operand_value<uint64_t> ( instr, 1u, context );
	const uint64_t mask = GET_OPERAND_MASK ( op_size );

	const uint64_t res = ( a & b ) & mask;
	const int64_t sres = static_cast< int64_t >( res << ( 64 - op_size * 8 ) ) >> ( 64 - op_size * 8 );

	auto& flags = context.get_flags ( );
	flags.SF = sres < 0;
	flags.ZF = ( res == 0 );
	flags.PF = std::popcount ( res & 0xFF ) % 2 == 0;
	flags.CF = 0;
	flags.OF = 0;
	flags.AF = 0;

	helpers::set_operand_value<uint64_t> ( instr, 0u, res, context );
}

/// OR-Logical OR
/// Performs a bitwise OR of the first and second operands, stores the result in the first operand, and updates flags.
void handlers::or_ ( const iced::Instruction& instr, KUBERA& context ) {
	const size_t op_size = instr.op0_size ( );
	const uint64_t a = helpers::get_operand_value<uint64_t> ( instr, 0u, context );
	const uint64_t b = helpers::get_operand_value<uint64_t> ( instr, 1u, context );
	const uint64_t mask = GET_OPERAND_MASK ( op_size );

	const uint64_t res = ( a | b ) & mask;
	const int64_t sres = static_cast< int64_t >( res << ( 64 - op_size * 8 ) ) >> ( 64 - op_size * 8 );

	auto& flags = context.get_flags ( );
	flags.SF = sres < 0;
	flags.ZF = ( res == 0 );
	flags.PF = std::popcount ( res & 0xFF ) % 2 == 0;
	flags.CF = 0;
	flags.OF = 0;
	flags.AF = 0;

	helpers::set_operand_value<uint64_t> ( instr, 0u, res, context );
}

/// XOR-Logical Exclusive OR
/// Performs a bitwise XOR of the first and second operands, stores the result in the first operand, and updates flags.
void handlers::xor_ ( const iced::Instruction& instr, KUBERA& context ) {
	const size_t op_size = instr.op0_size ( );
	const uint64_t a = helpers::get_operand_value<uint64_t> ( instr, 0u, context );
	const uint64_t b = helpers::get_operand_value<uint64_t> ( instr, 1u, context );
	const uint64_t mask = GET_OPERAND_MASK ( op_size );

	const uint64_t res = ( a ^ b ) & mask;

	auto& flags = context.get_flags ( );
	flags.SF = ( res >> ( op_size * 8 - 1 ) ) & 1;
	flags.ZF = ( res == 0 );
	flags.PF = std::popcount ( res & 0xFF ) % 2 == 0;
	flags.CF = 0;
	flags.OF = 0;
	flags.AF = 0;

	helpers::set_operand_value<uint64_t> ( instr, 0u, res, context );
}

/// NOT-Logical NOT
/// Performs a bitwise NOT on the destination operand and stores the result without affecting flags.
void handlers::not_ ( const iced::Instruction& instr, KUBERA& context ) {
	const size_t op_size = instr.op0_size ( );
	const uint64_t a = helpers::get_operand_value<uint64_t> ( instr, 0u, context );
	const uint64_t mask = GET_OPERAND_MASK ( op_size );

	const uint64_t res = ( ~a ) & mask;

	helpers::set_operand_value<uint64_t> ( instr, 0u, res, context );
}

/// SHL/SAL-Shift Left
/// Shifts the destination operand left by the number of bits specified in the second operand, stores the result, and updates flags (SHL and SAL are identical).
void handlers::shl ( const iced::Instruction& instr, KUBERA& context ) {
	const size_t op_size = instr.op0_size ( );
	const uint64_t val_operand = helpers::get_operand_value<uint64_t> ( instr, 0u, context );
	const uint64_t count_operand = helpers::get_operand_value<uint64_t> ( instr, 1u, context );
	const uint64_t mask = GET_OPERAND_MASK ( op_size );

	const uint64_t uval = val_operand & mask;
	const uint8_t size_in_bits = static_cast< uint8_t >( op_size * 8 );
	const uint8_t count_limit_mask = ( op_size == 8 ) ? 0x3F : 0x1F;
	const uint8_t count_raw = count_operand & 0xFF;
	const uint8_t effective_count = count_raw & count_limit_mask;

	uint64_t res = uval;
	if ( effective_count > 0 && effective_count < size_in_bits ) {
		res = ( uval << effective_count ) & mask;
	}
	else if ( effective_count >= size_in_bits ) {
		res = 0;
	}

	auto& flags = context.get_flags ( );
	if ( count_raw != 0 ) {
		flags.CF = ( effective_count <= size_in_bits ) ? ( uval >> ( size_in_bits - effective_count ) ) & 1 : 0;
		flags.SF = ( res >> ( size_in_bits - 1 ) ) & 1;
		flags.ZF = ( res == 0 );
		flags.PF = std::popcount ( res & 0xFF ) % 2 == 0;
		flags.AF = 0;
		flags.OF = ( count_raw == 1 ) ? ( ( uval >> ( size_in_bits - 1 ) ) & 1 ) ^ flags.CF : 0;
	}

	helpers::set_operand_value<uint64_t> ( instr, 0u, res, context );
}

/// SHL/SAL-Shift Left
/// Shifts the destination operand left by the number of bits specified in the second operand, stores the result, and updates flags (SHL and SAL are identical).
void handlers::sal ( const iced::Instruction& instr, KUBERA& context ) {
	shl ( instr, context );
}

/// SHR-Shift Right
/// Shifts the destination operand right by the number of bits specified in the second operand, stores the result, and updates flags.
void handlers::shr ( const iced::Instruction& instr, KUBERA& context ) {
	const size_t op_size = instr.op0_size ( );
	const uint64_t val_operand = helpers::get_operand_value<uint64_t> ( instr, 0u, context );
	const uint64_t count_operand = helpers::get_operand_value<uint64_t> ( instr, 1u, context );
	const uint64_t mask = GET_OPERAND_MASK ( op_size );

	const uint64_t uval = val_operand & mask;
	const uint8_t size_in_bits = static_cast< uint8_t >( op_size * 8 );
	const uint8_t count_limit_mask = ( op_size == 8 ) ? 0x3F : 0x1F;
	const uint8_t count_raw = count_operand & 0xFF;
	const uint8_t effective_count = count_raw & count_limit_mask;

	uint64_t res = uval;
	if ( effective_count > 0 && effective_count < size_in_bits ) {
		res = uval >> effective_count;
	}
	else if ( effective_count >= size_in_bits ) {
		res = 0;
	}

	auto& flags = context.get_flags ( );
	if ( count_raw != 0 ) {
		flags.CF = ( effective_count <= size_in_bits ) ? ( uval >> ( effective_count - 1 ) ) & 1 : 0;
		flags.SF = ( res >> ( size_in_bits - 1 ) ) & 1;
		flags.ZF = ( res == 0 );
		flags.PF = std::popcount ( res & 0xFF ) % 2 == 0;
		flags.AF = 0;
		flags.OF = ( count_raw == 1 ) ? ( uval >> ( size_in_bits - 1 ) ) & 1 : 0;
	}

	helpers::set_operand_value<uint64_t> ( instr, 0u, res, context );
}

template <typename Func>
void double_shift ( const iced::Instruction& instr, KUBERA& context, Func shift_op ) {
	const size_t op_size = instr.op0_size ( );
	const uint64_t dest = helpers::get_operand_value<uint64_t> ( instr, 0u, context );
	const uint64_t src = helpers::get_operand_value<uint64_t> ( instr, 1u, context );
	const uint64_t count = helpers::get_operand_value<uint64_t> ( instr, 2u, context );
	const uint64_t mask = GET_OPERAND_MASK ( op_size );

	const uint64_t value0 = dest & mask;
	const uint64_t value1 = src & mask;
	const uint8_t size_in_bits = static_cast< uint8_t >( op_size * 8 );
	const uint8_t count_raw = count & 0xFF;
	const uint8_t counter = ( op_size == 8 ) ? ( count % 64 ) : ( count % size_in_bits );

	uint64_t final_result = value0;
	uint64_t final_cf = context.get_flags ( ).CF;

	if ( counter != 0 && counter <= size_in_bits ) {
		auto [result, cf] = shift_op ( value0, value1, counter, size_in_bits );
		final_result = result & mask;
		final_cf = cf;
	}

	auto& flags = context.get_flags ( );
	if ( count_raw != 0 ) {
		flags.CF = final_cf;
		flags.SF = ( final_result >> ( size_in_bits - 1 ) ) & 1;
		flags.ZF = ( final_result == 0 );
		flags.PF = std::popcount ( final_result & 0xFF ) % 2 == 0;
		flags.AF = 0;
		if ( count_raw == 1 ) {
			flags.OF = shift_op ( value0, value0, 1, size_in_bits ).second;
		}
		else {
			flags.OF = 0;
		}
	}

	helpers::set_operand_value<uint64_t> ( instr, 0u, final_result, context );
}

/// SHLD-Double Precision Shift Left
/// Shifts the destination operand left by the number of bits specified, filling low bits with the source operand, and updates flags.
void handlers::shld ( const iced::Instruction& instr, KUBERA& context ) {
	double_shift ( instr, context, [ ] ( uint64_t value0, uint64_t value1, uint8_t counter, uint8_t size_in_bits )
	{
		uint64_t temp_result = value0;
		uint64_t final_cf = ( temp_result >> ( size_in_bits - counter ) ) & 1;
		temp_result = ( temp_result << counter ) | ( value1 >> ( size_in_bits - counter ) );
		uint64_t original_msb = ( value0 >> ( size_in_bits - 1 ) ) & 1;
		uint64_t result_msb = ( temp_result >> ( size_in_bits - 1 ) ) & 1;
		return std::make_pair ( temp_result, original_msb ^ result_msb );
	} );
}

/// SHRD-Double Precision Shift Right
/// Shifts the destination operand right by the number of bits specified, filling high bits with the source operand, and updates flags.
void handlers::shrd ( const iced::Instruction& instr, KUBERA& context ) {
	double_shift ( instr, context, [ ] ( uint64_t value0, uint64_t value1, uint8_t counter, uint8_t size_in_bits )
	{
		uint64_t temp_result = value0;
		uint64_t final_cf = ( temp_result >> ( counter - 1 ) ) & 1;
		temp_result = ( temp_result >> counter ) | ( value1 << ( size_in_bits - counter ) );
		uint64_t original_msb = ( value0 >> ( size_in_bits - 1 ) ) & 1;
		uint64_t result_msb = ( temp_result >> ( size_in_bits - 1 ) ) & 1;
		return std::make_pair ( temp_result, original_msb ^ result_msb );
	} );
}

/// SAR-Shift Arithmetic Right
/// Shifts the destination operand right arithmetically by the number of bits specified, preserving the sign bit, stores the result, and updates flags.
void handlers::sar ( const iced::Instruction& instr, KUBERA& context ) {
	const size_t op_size = instr.op0_size ( );
	const uint64_t val_operand = helpers::get_operand_value<uint64_t> ( instr, 0u, context );
	const uint64_t count_operand = helpers::get_operand_value<uint64_t> ( instr, 1u, context );
	const uint64_t mask = GET_OPERAND_MASK ( op_size );

	const uint64_t uval = val_operand & mask;
	const int64_t signed_val = static_cast< int64_t >( uval << ( 64 - op_size * 8 ) ) >> ( 64 - op_size * 8 );
	const uint8_t size_in_bits = static_cast< uint8_t >( op_size * 8 );
	const uint8_t count_limit_mask = ( op_size == 8 ) ? 0x3F : 0x1F;
	const uint8_t count_raw = count_operand & 0xFF;
	const uint8_t effective_count = count_raw & count_limit_mask;

	uint64_t res = uval;
	if ( effective_count > 0 ) {
		if ( effective_count < size_in_bits ) {
			res = static_cast< uint64_t > ( signed_val >> effective_count ) & mask;
		}
		else {
			res = ( signed_val < 0 ) ? mask : 0;
		}
	}

	auto& flags = context.get_flags ( );
	if ( count_raw != 0 ) {
		flags.CF = ( effective_count <= size_in_bits ) ? ( uval >> ( effective_count - 1 ) ) & 1 : 0;
		flags.SF = ( res >> ( size_in_bits - 1 ) ) & 1;
		flags.ZF = ( res == 0 );
		flags.PF = std::popcount ( res & 0xFF ) % 2 == 0;
		flags.AF = 0;
		flags.OF = ( count_raw == 1 ) ? 0 : 0;
	}

	helpers::set_operand_value<uint64_t> ( instr, 0u, res, context );
}

```

`src/handlers/cmov.cpp`:

```cpp
#include "../../emulator.hpp"
#include <bit>
#include "helpers.hpp"

using namespace kubera;

template <typename Func>
void cmovcc ( const iced::Instruction& instr, KUBERA& context, Func condition ) {
  if ( condition ( context ) ) {
    const uint64_t src_val = helpers::get_operand_value<uint64_t> ( instr, 1u, context );
    helpers::set_operand_value<uint64_t> ( instr, 0u, src_val, context );
  }
}

/// CMOVO-Conditional Move if Overflow
/// Moves the source operand to the destination if OF is set.
void handlers::cmovo ( const iced::Instruction& instr, KUBERA& context ) {
  cmovcc ( instr, context, [ ] ( KUBERA& context ) { return context.get_flags ( ).OF; } );
}

/// CMOVNL-Conditional Move if Not Less
/// Moves the source operand to the destination if SF equals OF.
void handlers::cmovnl ( const iced::Instruction& instr, KUBERA& context ) {
  cmovcc ( instr, context, [ ] ( KUBERA& context ) { return context.get_flags ( ).SF == context.get_flags ( ).OF; } );
}

/// CMOVBE-Conditional Move if Below or Equal
/// Moves the source operand to the destination if CF or ZF is set.
void handlers::cmovbe ( const iced::Instruction& instr, KUBERA& context ) {
  cmovcc ( instr, context, [ ] ( KUBERA& context ) { return context.get_flags ( ).CF || context.get_flags ( ).ZF; } );
}

/// CMOVZ-Conditional Move if Zero
/// Moves the source operand to the destination if ZF is set.
void handlers::cmovz ( const iced::Instruction& instr, KUBERA& context ) {
  cmovcc ( instr, context, [ ] ( KUBERA& context ) { return context.get_flags ( ).ZF; } );
}

/// CMOVLE-Conditional Move if Less or Equal
/// Moves the source operand to the destination if ZF is set or SF differs from OF.
void handlers::cmovle ( const iced::Instruction& instr, KUBERA& context ) {
  cmovcc ( instr, context, [ ] ( KUBERA& context ) { return context.get_flags ( ).ZF || ( context.get_flags ( ).SF != context.get_flags ( ).OF ); } );
}

/// CMOVL-Conditional Move if Less
/// Moves the source operand to the destination if SF differs from OF.
void handlers::cmovl ( const iced::Instruction& instr, KUBERA& context ) {
  cmovcc ( instr, context, [ ] ( KUBERA& context ) { return context.get_flags ( ).SF != context.get_flags ( ).OF; } );
}

/// CMOVNP-Conditional Move if Not Parity
/// Moves the source operand to the destination if PF is clear.
void handlers::cmovnp ( const iced::Instruction& instr, KUBERA& context ) {
  cmovcc ( instr, context, [ ] ( KUBERA& context ) { return !context.get_flags ( ).PF; } );
}

/// CMOVNS-Conditional Move if Not Sign
/// Moves the source operand to the destination if SF is clear.
void handlers::cmovns ( const iced::Instruction& instr, KUBERA& context ) {
  cmovcc ( instr, context, [ ] ( KUBERA& context ) { return !context.get_flags ( ).SF; } );
}

/// CMOVP-Conditional Move if Parity
/// Moves the source operand to the destination if PF is set.
void handlers::cmovp ( const iced::Instruction& instr, KUBERA& context ) {
  cmovcc ( instr, context, [ ] ( KUBERA& context ) { return context.get_flags ( ).PF; } );
}

/// CMOVNB-Conditional Move if Not Below
/// Moves the source operand to the destination if CF is clear.
void handlers::cmovnb ( const iced::Instruction& instr, KUBERA& context ) {
  cmovcc ( instr, context, [ ] ( KUBERA& context ) { return !context.get_flags ( ).CF; } );
}

/// CMOVNO-Conditional Move if Not Overflow
/// Moves the source operand to the destination if OF is clear.
void handlers::cmovno ( const iced::Instruction& instr, KUBERA& context ) {
  cmovcc ( instr, context, [ ] ( KUBERA& context ) { return !context.get_flags ( ).OF; } );
}

/// CMOVS-Conditional Move if Sign
/// Moves the source operand to the destination if SF is set.
void handlers::cmovs ( const iced::Instruction& instr, KUBERA& context ) {
  cmovcc ( instr, context, [ ] ( KUBERA& context ) { return context.get_flags ( ).SF; } );
}

/// CMOVNZ-Conditional Move if Not Zero
/// Moves the source operand to the destination if ZF is clear.
void handlers::cmovnz ( const iced::Instruction& instr, KUBERA& context ) {
  cmovcc ( instr, context, [ ] ( KUBERA& context ) { return !context.get_flags ( ).ZF; } );
}

/// CMOVNBE-Conditional Move if Not Below or Equal
/// Moves the source operand to the destination if CF and ZF are clear.
void handlers::cmovnbe ( const iced::Instruction& instr, KUBERA& context ) {
  cmovcc ( instr, context, [ ] ( KUBERA& context ) { return !context.get_flags ( ).ZF && !context.get_flags ( ).CF; } );
}

/// CMOVB-Conditional Move if Below
/// Moves the source operand to the destination if CF is set.
void handlers::cmovb ( const iced::Instruction& instr, KUBERA& context ) {
  cmovcc ( instr, context, [ ] ( KUBERA& context ) { return context.get_flags ( ).CF; } );
}

/// CMOVNLE-Conditional Move if Not Less or Equal
/// Moves the source operand to the destination if ZF is clear and SF equals OF.
void handlers::cmovnle ( const iced::Instruction& instr, KUBERA& context ) {
  cmovcc ( instr, context, [ ] ( KUBERA& context ) { return !context.get_flags ( ).ZF && ( context.get_flags ( ).SF == context.get_flags ( ).OF ); } );
}

```

`src/handlers/compare.cpp`:

```cpp
#include "../../emulator.hpp"
#include <bit>
#include "helpers.hpp"

using namespace kubera;

/// CMP-Compare
/// Compares the first operand with the second by subtracting the second from the first and updating flags without modifying operands.
void handlers::cmp ( const iced::Instruction& instr, KUBERA& context ) {
  const size_t op_size = instr.op0_size ( );
  const uint64_t src1 = helpers::get_operand_value<uint64_t> ( instr, 0u, context );
  const uint64_t src2 = helpers::get_operand_value<uint64_t> ( instr, 1u, context );
  const uint64_t mask = GET_OPERAND_MASK ( op_size );

  const uint64_t ua = src1 & mask;
  const uint64_t ub = src2 & mask;
  const uint64_t res = ( ua - ub ) & mask;

  const int64_t sa = SIGN_EXTEND ( ua, op_size );
  const int64_t sb = SIGN_EXTEND ( ub, op_size );
  const int64_t sres = SIGN_EXTEND ( res, op_size );

  auto& flags = context.get_flags ( );
  flags.CF = ( ua < ub );
  flags.ZF = ( res == 0 );
  flags.PF = std::popcount ( res & 0xFF ) % 2 == 0;
  flags.AF = ( ( ua ^ ub ^ res ) & 0x10 ) != 0;
  flags.SF = ( sres < 0 );
  flags.OF = ( sa >= 0 && sb < 0 && sres < 0 ) || ( sa < 0 && sb >= 0 && sres >= 0 );
}

/// TEST-Logical Compare
/// Performs a bitwise AND of the first and second operands and updates flags without modifying operands.
void handlers::test ( const iced::Instruction& instr, KUBERA& context ) {
  const size_t op_size = instr.op0_size ( );
  const uint64_t src1 = helpers::get_operand_value<uint64_t> ( instr, 0u, context );
  const uint64_t src2 = helpers::get_operand_value<uint64_t> ( instr, 1u, context );
  const uint64_t mask = GET_OPERAND_MASK ( op_size );

  const uint64_t res = ( src1 & src2 ) & mask;

  auto& flags = context.get_flags ( );
  flags.SF = ( res >> ( op_size * 8 - 1 ) ) & 1;
  flags.ZF = ( res == 0 );
  flags.PF = std::popcount ( res & 0xFF ) % 2 == 0;
  flags.CF = 0;
  flags.OF = 0;
  flags.AF = 0;
}

/// CMPXCHG-Compare and Exchange
/// Compares the destination with RAX (or portion thereof), sets ZF based on equality, and either stores the source in the destination (if equal) or loads the destination into RAX (if not equal), updating flags.
void handlers::cmpxchg ( const iced::Instruction& instr, KUBERA& context ) {
  const size_t op_size = instr.op0_size ( ) ? instr.op0_size ( ) : 8;
  const uint64_t mask = GET_OPERAND_MASK ( op_size );

  const uint64_t dst_val = helpers::get_operand_value<uint64_t> ( instr, 0u, context ) & mask;
  const uint64_t src_bits = helpers::get_operand_value<uint64_t> ( instr, 1u, context ) & mask;
  const uint64_t rax_full = context.get_reg ( Register::RAX, 8 );
  const uint64_t acc_bits = rax_full & mask;

  bool success = ( acc_bits == dst_val );
  if ( success ) {
    helpers::set_operand_value<uint64_t> ( instr, 0u, src_bits, context );
  }
  else {
    const uint64_t new_rax = ( rax_full & ~mask ) | dst_val;
    context.set_reg ( Register::RAX, new_rax, 8 );
  }

  const uint64_t res = ( acc_bits - dst_val ) & mask;
  const int64_t sa = SIGN_EXTEND ( acc_bits, op_size );
  const int64_t sb = SIGN_EXTEND ( dst_val, op_size );
  const int64_t sres = SIGN_EXTEND ( res, op_size );

  auto& flags = context.get_flags ( );
  flags.CF = ( acc_bits < dst_val );
  flags.ZF = success ? 1 : 0;
  flags.PF = std::popcount ( res & 0xFF ) % 2 == 0;
  flags.AF = ( ( acc_bits ^ dst_val ^ res ) & 0x10 ) != 0;
  flags.SF = ( sres < 0 );
  flags.OF = ( sa >= 0 && sb < 0 && sres < 0 ) || ( sa < 0 && sb >= 0 && sres >= 0 );
}

/// CMPXCHG16B-Compare and Exchange 16 Bytes
/// Compares the 128-bit memory destination with RDX:RAX, sets ZF based on equality, and either stores RCX:RBX in the destination (if equal) or loads the destination into RDX:RAX (if not equal).
void handlers::cmpxchg16b ( const iced::Instruction& instr, KUBERA& context ) {
  const uint64_t addr = helpers::calculate_mem_addr( instr, context );
  if ( addr % 16 != 0 ) {
    // !TODO(exception)
    return;
  }
  const uint128_t mem_val = context.get_memory<uint128_t> ( addr );
  const uint64_t rax = context.get_reg ( Register::RAX, 8 );
  const uint64_t rdx = context.get_reg ( Register::RDX, 8 );
  const uint128_t rdx_rax = ( uint128_t ( rdx ) << 64 ) | rax;

  const uint64_t rbx = context.get_reg ( Register::RBX, 8 );
  const uint64_t rcx = context.get_reg ( Register::RCX, 8 );
  const uint128_t rcx_rbx = ( uint128_t ( rcx ) << 64 ) | rbx;

  bool equal = ( mem_val == rdx_rax );
  if ( equal ) {
    context.set_memory<uint128_t> ( addr, rcx_rbx );
  }
  else {
    context.set_reg ( Register::RAX, static_cast< uint64_t >( mem_val ), 8 );
    context.set_reg ( Register::RDX, static_cast< uint64_t >( mem_val >> 64 ), 8 );
  }

  auto& flags = context.get_flags ( );
  flags.ZF = equal ? 1 : 0;
}

```

`src/handlers/control_flow.cpp`:

```cpp
#include "../../emulator.hpp"
#include <bit>
#include "helpers.hpp"

using namespace kubera;
/// JMP - Jump
/// Jumps to the target address unconditionally
void handlers::jmp ( const iced::Instruction& instr, KUBERA& context ) {
	const auto target = helpers::get_operand_value<uint64_t> ( instr, 0u, context );
	context.handle_ip_switch ( target );
}

/// JNE - Jump if Not Equal
/// Jumps to the target address if the zero flag (ZF) is 0, without affecting flags.
void handlers::jne ( const iced::Instruction& instr, KUBERA& context ) {
	if ( !context.get_flags ( ).ZF ) {
		handlers::jmp ( instr, context );
	}
}

/// JE - Jump if Equal
/// Jumps to the target address if the zero flag (ZF) is 1, without affecting flags.
void handlers::je ( const iced::Instruction& instr, KUBERA& context ) {
	if ( context.get_flags ( ).ZF ) {
		handlers::jmp ( instr, context );
	}
}

/// JNBE - Jump if Not Below or Equal
/// Jumps to the target address if the carry flag (CF) is 0 and the zero flag (ZF) is 0, without affecting flags.
void handlers::jnbe ( const iced::Instruction& instr, KUBERA& context ) {
	const auto& flags = context.get_flags ( );
	if ( !flags.CF && !flags.ZF ) {
		handlers::jmp ( instr, context );
	}
}

/// JG - Jump if Greater
/// Jumps to the target address if the zero flag (ZF) is 0 and the sign flag (SF) equals the overflow flag (OF), without affecting flags.
void handlers::jg ( const iced::Instruction& instr, KUBERA& context ) {
	const auto& flags = context.get_flags ( );
	if ( !flags.ZF && flags.SF == flags.OF ) {
		handlers::jmp ( instr, context );
	}
}

/// JL - Jump if Less
/// Jumps to the target address if the sign flag (SF) does not equal the overflow flag (OF), without affecting flags.
void handlers::jl ( const iced::Instruction& instr, KUBERA& context ) {
	const auto& flags = context.get_flags ( );
	if ( flags.SF != flags.OF ) {
		handlers::jmp ( instr, context );
	}
}

/// JNB - Jump if Not Below
/// Jumps to the target address if the carry flag (CF) is 0, without affecting flags.
void handlers::jnb ( const iced::Instruction& instr, KUBERA& context ) {
	if ( !context.get_flags ( ).CF ) {
		handlers::jmp ( instr, context );
	}
}

/// JB - Jump if Below
/// Jumps to the target address if the carry flag (CF) is 1, without affecting flags.
void handlers::jb ( const iced::Instruction& instr, KUBERA& context ) {
	if ( context.get_flags ( ).CF ) {
		handlers::jmp ( instr, context );
	}
}

/// JNS - Jump if Not Sign
/// Jumps to the target address if the sign flag (SF) is 0, without affecting flags.
void handlers::jns ( const iced::Instruction& instr, KUBERA& context ) {
	if ( !context.get_flags ( ).SF ) {
		handlers::jmp ( instr, context );
	}
}

/// JNL - Jump if Not Less
/// Jumps to the target address if the sign flag (SF) equals the overflow flag (OF), without affecting flags.
void handlers::jnl ( const iced::Instruction& instr, KUBERA& context ) {
	const auto& flags = context.get_flags ( );
	if ( flags.SF == flags.OF ) {
		handlers::jmp ( instr, context );
	}
}

/// JO - Jump if Overflow
/// Jumps to the target address if the overflow flag (OF) is 1, without affecting flags.
void handlers::jo ( const iced::Instruction& instr, KUBERA& context ) {
	if ( context.get_flags ( ).OF ) {
		handlers::jmp ( instr, context );
	}
}

/// JNO - Jump if Not Overflow
/// Jumps to the target address if the overflow flag (OF) is 0, without affecting flags.
void handlers::jno ( const iced::Instruction& instr, KUBERA& context ) {
	if ( !context.get_flags ( ).OF ) {
		handlers::jmp ( instr, context );
	}
}

/// JBE - Jump if Below or Equal
/// Jumps to the target address if the carry flag (CF) is 1 or the zero flag (ZF) is 1, without affecting flags.
void handlers::jbe ( const iced::Instruction& instr, KUBERA& context ) {
	const auto& flags = context.get_flags ( );
	if ( flags.CF || flags.ZF ) {
		handlers::jmp ( instr, context );
	}
}

/// JS - Jump if Sign
/// Jumps to the target address if the sign flag (SF) is 1, without affecting flags.
void handlers::js ( const iced::Instruction& instr, KUBERA& context ) {
	if ( context.get_flags ( ).SF ) {
		handlers::jmp ( instr, context );
	}
}

/// JA - Jump if Above
/// Jumps to the target address if the carry flag (CF) is 0 and the zero flag (ZF) is 0, without affecting flags.
void handlers::ja ( const iced::Instruction& instr, KUBERA& context ) {
	const auto& flags = context.get_flags ( );
	if ( !flags.CF && !flags.ZF ) {
		handlers::jmp ( instr, context );
	}
}

/// JAE - Jump if Above or Equal
/// Jumps to the target address if the carry flag (CF) is 0, without affecting flags.
void handlers::jae ( const iced::Instruction& instr, KUBERA& context ) {
	if ( !context.get_flags ( ).CF ) {
		handlers::jmp ( instr, context );
	}
}

/// JGE - Jump if Greater or Equal
/// Jumps to the target address if the sign flag (SF) equals the overflow flag (OF), without affecting flags.
void handlers::jge ( const iced::Instruction& instr, KUBERA& context ) {
	const auto& flags = context.get_flags ( );
	if ( flags.SF == flags.OF ) {
		handlers::jmp ( instr, context );
	}
}

/// JLE - Jump if Less or Equal
/// Jumps to the target address if the zero flag (ZF) is 1 or the sign flag (SF) does not equal the overflow flag (OF), without affecting flags.
void handlers::jle ( const iced::Instruction& instr, KUBERA& context ) {
	const auto& flags = context.get_flags ( );
	if ( flags.ZF || flags.SF != flags.OF ) {
		handlers::jmp ( instr, context );
	}
}

/// JP - Jump if Parity
/// Jumps to the target address if the parity flag (PF) is 1, without affecting flags.
void handlers::jp ( const iced::Instruction& instr, KUBERA& context ) {
	if ( context.get_flags ( ).PF ) {
		handlers::jmp ( instr, context );
	}
}

/// JNP - Jump if Not Parity
/// Jumps to the target address if the parity flag (PF) is 0, without affecting flags.
void handlers::jnp ( const iced::Instruction& instr, KUBERA& context ) {
	if ( !context.get_flags ( ).PF ) {
		handlers::jmp ( instr, context );
	}
}

/// JCXZ - Jump if CX Zero
/// Jumps to the target address if the CX register is 0 (16-bit address size), without affecting flags.
void handlers::jcxz ( const iced::Instruction& instr, KUBERA& context ) {
	const uint64_t cx = context.get_reg ( Register::CX, 2 );
	if ( cx == 0 ) {
		handlers::jmp ( instr, context );
	}
}

/// JECXZ - Jump if ECX Zero
/// Jumps to the target address if the ECX register is 0 (32-bit address size), without affecting flags.
void handlers::jecxz ( const iced::Instruction& instr, KUBERA& context ) {
	const uint64_t ecx = context.get_reg ( Register::ECX, 4 );
	if ( ecx == 0 ) {
		handlers::jmp ( instr, context );
	}
}

/// JRCXZ - Jump if RCX Zero
/// Jumps to the target address if the RCX register is 0 (64-bit address size), without affecting flags.
void handlers::jrcxz ( const iced::Instruction& instr, KUBERA& context ) {
	const uint64_t rcx = context.get_reg ( Register::RCX, 8 );
	if ( rcx == 0 ) {
		handlers::jmp ( instr, context );
	}
}

/// RET - Return from Procedure
/// Pops the return address from the stack, adjusts RSP (optionally by an immediate value), and jumps to the return address, without affecting flags.
void handlers::ret ( const iced::Instruction& instr, KUBERA& context ) {
	const size_t op_size = 8;
	const uint64_t imm = ( instr.op_count ( ) > 0 && instr.op0_kind ( ) == OpKindSimple::Immediate ) ?
		instr.immediate ( ) : 0;
	const uint64_t pop_size = op_size + imm;
	const uint64_t old_rsp = context.get_reg ( Register::RSP, op_size );

	const uint64_t return_ip = context.get_stack<uint64_t> ( old_rsp );
	context.set_reg ( Register::RSP, old_rsp + pop_size, op_size );
	context.handle_ip_switch ( return_ip );
}

/// CALL - Call Procedure
/// Pushes the return address (RIP + instruction length) onto the stack, adjusts RSP, and jumps to the target address, without affecting flags.
void handlers::call ( const iced::Instruction& instr, KUBERA& context ) {
	const size_t op_size = 8;
	const uint64_t return_ip = context.get_reg ( Register::RIP, op_size ) + instr.length ( );
	const uint64_t old_rsp = context.get_reg ( Register::RSP, op_size );
	const uint64_t new_rsp = old_rsp - op_size;

	context.set_stack<uint64_t> ( new_rsp, return_ip );
	context.set_reg ( Register::RSP, new_rsp, op_size );
	handlers::jmp ( instr, context );
}

void iret ( size_t op_size, const iced::Instruction& instr, KUBERA& context ) {
	uint64_t rsp = context.get_reg ( Register::RSP, 8 );

	uint64_t ip = context.get_stack<uint64_t> ( rsp );
	rsp += op_size;
	uint64_t cs = context.get_stack<uint64_t> ( rsp );
	rsp += op_size;
	uint64_t flags_val = context.get_stack<uint64_t> ( rsp );
	rsp += op_size;

	uint8_t new_cpl = cs & 0x3;
	uint8_t current_cpl = context.get_cpl ( );
	if ( new_cpl > current_cpl ) {
		uint64_t new_rsp = context.get_stack<uint64_t> ( rsp );
		rsp += op_size;
		uint64_t new_ss = context.get_stack<uint64_t> ( rsp );
		rsp += op_size;
		context.set_reg ( Register::RSP, new_rsp, op_size );
		context.set_reg ( Register::SS, new_ss, op_size );
	}

	context.set_reg ( Register::RIP, ip, op_size );
	context.set_reg ( Register::CS, cs, op_size );
	context.set_rflags ( flags_val );
	context.set_reg ( Register::RSP, rsp, op_size );
	context.get_cpl ( ) = new_cpl;
}

/// IRET - Interrupt Return 16-bit
/// Returns from an interrupt or exception handler by popping the instruction pointer, code segment, and flags from the stack, optionally popping the stack pointer and stack segment if changing privilege levels, updating CPL, without affecting other flags.
void handlers::iret ( const iced::Instruction& instr, KUBERA& context ) {
	::iret ( 2, instr, context );
}

/// IRETD - Interrupt Return 32-bit
/// Returns from an interrupt or exception handler by popping the instruction pointer, code segment, and flags from the stack, optionally popping the stack pointer and stack segment if changing privilege levels, updating CPL, without affecting other flags.
void handlers::iretd ( const iced::Instruction& instr, KUBERA& context ) {
	::iret ( 4, instr, context );
}

/// IRETQ - Interrupt Return 64-bit
/// Returns from an interrupt or exception handler by popping the instruction pointer, code segment, and flags from the stack, optionally popping the stack pointer and stack segment if changing privilege levels, updating CPL, without affecting other flags.
void handlers::iretq ( const iced::Instruction& instr, KUBERA& context ) {
	::iret ( 8, instr, context );
}

```

`src/handlers/data.cpp`:

```cpp
#include "../../emulator.hpp"
#include <bit>
#include "helpers.hpp"

using namespace kubera;
/// MOV - Move
/// Copies the value from the source operand to the destination operand without affecting flags.
void handlers::mov ( const iced::Instruction& instr, KUBERA& context ) {
	const size_t op_size = instr.op0_size ( );
	const uint64_t src_val = helpers::get_operand_value<uint64_t> ( instr, 1u, context );
	const uint64_t mask = GET_OPERAND_MASK ( op_size );
	const uint64_t value = src_val & mask;

	if ( instr.op0_kind ( ) == OpKindSimple::Register ) {
		helpers::set_operand_value<uint64_t> ( instr, 0u, value, context );
		return;
	}

	if ( instr.op0_kind ( ) == OpKindSimple::Memory ) {
		const uint64_t addr = helpers::calculate_mem_addr( instr, context );
		context.set_memory<uint64_t> ( addr, value );
		return;
	}

	// !TODO(exception)
}

/// MOVABS - Move Absolute
/// Moves a 64-bit immediate value to a 64-bit general-purpose register without affecting flags.
void handlers::movabs ( const iced::Instruction& instr, KUBERA& context ) {
	const uint64_t imm_val = instr.immediate ( );
	const size_t op_size = 8; // MOVABS is always 64-bit
	const uint64_t mask = GET_OPERAND_MASK ( op_size );
	const uint64_t value = imm_val & mask;

	if ( instr.op0_kind ( ) == OpKindSimple::Register ) {
		helpers::set_operand_value<uint64_t> ( instr, 0u, value, context );
		return;
	}

	// !TODO(exception)
}

/// MOVAPS - Move Aligned Packed Single-Precision Floating-Point
/// Moves 128 bits of packed single-precision floating-point values from the source to the destination, requiring 16-byte alignment for memory operands, without affecting flags.
void handlers::movaps ( const iced::Instruction& instr, KUBERA& context ) {
	if ( instr.op0_kind ( ) == OpKindSimple::Register && instr.op1_kind ( ) == OpKindSimple::Register ) {
		const uint128_t src_val = context.get_xmm_raw ( instr.op1_reg ( ) );
		context.set_xmm_raw ( instr.op0_reg ( ), src_val );
		return;
	}

	if ( instr.op0_kind ( ) == OpKindSimple::Register && instr.op1_kind ( ) == OpKindSimple::Memory ) {
		const uint64_t addr = helpers::calculate_mem_addr( instr, context );
		if ( addr % 16 != 0 ) {
			// !TODO(exception)
			return;
		}
		const uint128_t src_val = context.get_memory<uint128_t> ( addr );
		context.set_xmm_raw ( instr.op0_reg ( ), src_val );
		return;
	}

	if ( instr.op0_kind ( ) == OpKindSimple::Memory && instr.op1_kind ( ) == OpKindSimple::Register ) {
		const uint64_t addr = helpers::calculate_mem_addr( instr, context );
		if ( addr % 16 != 0 ) {
			// !TODO(exception)
			return;
		}
		const uint128_t src_val = context.get_xmm_raw ( instr.op1_reg ( )  );
		context.set_memory<uint128_t> ( addr, src_val );
		return;
	}

	// !TODO(exception)
}

/// MOVZX - Move with Zero-Extend
/// Moves the source operand (byte or word) to the destination operand (word, doubleword, or quadword), zero-extending the value to fill the destination, without affecting flags.
void handlers::movzx ( const iced::Instruction& instr, KUBERA& context ) {
	const size_t dst_size = instr.op0_size ( );
	const size_t src_size = instr.op1_size ( );
	const uint64_t src_val = helpers::get_operand_value<uint64_t> ( instr, 1u, context );
	const uint64_t src_mask = GET_OPERAND_MASK ( src_size );
	const uint64_t value = src_val & src_mask;

	// Validate operand sizes
	if ( src_size == 1 && ( dst_size == 2 || dst_size == 4 || dst_size == 8 ) ||
		src_size == 2 && ( dst_size == 4 || dst_size == 8 ) ||
		src_size == 4 && dst_size == 8 ) {
		if ( instr.op0_kind ( ) == OpKindSimple::Register ) {
			helpers::set_operand_value<uint64_t> ( instr, 0u, value, context );
			return;
		}
	}

	// !TODO(exception)
}

/// MOVD - Move Doubleword
/// Moves a 32-bit value between a general-purpose register and an XMM register, or between memory and an XMM register, without affecting flags.
void handlers::movd ( const iced::Instruction& instr, KUBERA& context ) {
	const size_t op_size = instr.op0_size ( );
	if ( op_size != 4 ) {
		// !TODO(exception)
		return;
	}

	if ( instr.op0_kind ( ) == OpKindSimple::Register && instr.op1_kind ( ) == OpKindSimple::Register ) {
		if ( instr.op0_reg ( ) >= Register::XMM0 && instr.op0_reg ( ) <= Register::XMM31 &&
				instr.op1_reg ( ) <= Register::R15 ) {
			const uint64_t src_val = helpers::get_operand_value<uint64_t> ( instr, 1u, context );
			const uint128_t xmm_val = static_cast< uint128_t >( src_val & 0xFFFFFFFF );
			context.set_xmm_raw ( instr.op0_reg ( ), xmm_val );
			return;
		}
		if ( instr.op0_reg ( ) <= Register::R15 &&
				instr.op1_reg ( ) >= Register::XMM0 && instr.op1_reg ( ) <= Register::XMM31 ) {
			const uint128_t src_val = context.get_xmm_raw ( instr.op1_reg ( ) );
			const uint64_t val = static_cast< uint64_t >( src_val & 0xFFFFFFFF );
			helpers::set_operand_value<uint64_t> ( instr, 0u, val, context );
			return;
		}
	}

	if ( instr.op0_kind ( ) == OpKindSimple::Register && instr.op1_kind ( ) == OpKindSimple::Memory ) {
		if ( instr.op0_reg ( ) >= Register::XMM0 && instr.op0_reg ( ) <= Register::XMM31 ) {
			const uint64_t addr = helpers::calculate_mem_addr( instr, context );
			if ( addr % 4 != 0 ) {
				// !TODO(exception)
				return;
			}
			const uint64_t src_val = context.get_memory<uint32_t> ( addr );
			const uint128_t xmm_val = static_cast< uint128_t >( src_val );
			context.set_xmm_raw ( instr.op0_reg ( ), xmm_val );
			return;
		}
	}

	if ( instr.op0_kind ( ) == OpKindSimple::Memory && instr.op1_kind ( ) == OpKindSimple::Register ) {
		if ( instr.op1_reg ( ) >= Register::XMM0 && instr.op1_reg ( ) <= Register::XMM31 ) {
			const uint64_t addr = helpers::calculate_mem_addr( instr, context );
			if ( addr % 4 != 0 ) {
				// !TODO(exception)
				return;
			}
			const uint128_t src_val = context.get_xmm_raw ( instr.op1_reg ( ) );
			const uint32_t val = static_cast< uint32_t >( src_val & 0xFFFFFFFF );
			context.set_memory<uint32_t> ( addr, val );
			return;
		}
	}

	// !TODO(exception)
}

/// LEA - Load Effective Address
/// Computes the effective address of the source memory operand and stores it in the destination register, without affecting flags.
void handlers::lea ( const iced::Instruction& instr, KUBERA& context ) {
	const size_t dst_size = instr.op0_size ( );
	if ( instr.op0_kind ( ) != OpKindSimple::Register ) {
		// !TODO(exception)
		return;
	}

	uint64_t effective_address = helpers::calculate_mem_addr ( instr, context );

	helpers::set_operand_value<uint64_t> ( instr, 0u, effective_address, context );
}

/// MOVSX - Move with Sign-Extend
/// Moves the source operand (byte or word) to the destination register (word, doubleword, or quadword), sign-extending the value, without affecting flags.
void handlers::movsx ( const iced::Instruction& instr, KUBERA& context ) {
	const size_t dst_size = instr.op0_size ( );
	const size_t src_size = instr.op1_size ( );
	if ( instr.op0_kind ( ) != OpKindSimple::Register ) {
		// !TODO(exception)
		return;
	}

	const uint64_t src_val = helpers::get_operand_value<uint64_t> ( instr, 1u, context );
	const uint64_t src_mask = GET_OPERAND_MASK ( src_size );
	const uint64_t masked_val = src_val & src_mask;
	const int64_t signed_val = SIGN_EXTEND ( masked_val, src_size );

	helpers::set_operand_value<uint64_t> ( instr, 0u, static_cast< uint64_t >( signed_val ), context );
}

/// MOVSXD - Move with Sign-Extend Doubleword
/// Moves a doubleword source operand to a quadword destination register, sign-extending the value, without affecting flags.
void handlers::movsxd ( const iced::Instruction& instr, KUBERA& context ) {
	const size_t dst_size = instr.op0_size ( );
	const size_t src_size = instr.op1_size ( );
	if ( instr.op0_kind ( ) != OpKindSimple::Register || dst_size != 8 || src_size != 4 ) {
		// !TODO(exception)
		return;
	}

	const uint64_t src_val = helpers::get_operand_value<uint64_t> ( instr, 1u, context );
	const uint64_t src_mask = GET_OPERAND_MASK ( src_size );
	const uint64_t masked_val = src_val & src_mask;
	const int64_t signed_val = SIGN_EXTEND ( masked_val, src_size );

	helpers::set_operand_value<uint64_t> ( instr, 0u, static_cast< uint64_t >( signed_val ), context );
}

/// XCHG - Exchange
/// Exchanges the contents of the source and destination operands, which may be registers or memory, without affecting flags.
void handlers::xchg ( const iced::Instruction& instr, KUBERA& context ) {
	const size_t op_size = instr.op0_size ( );
	const uint64_t val1 = helpers::get_operand_value<uint64_t> ( instr, 0u, context );
	const uint64_t val2 = helpers::get_operand_value<uint64_t> ( instr, 1u, context );
	const uint64_t mask = GET_OPERAND_MASK ( op_size );
	const uint64_t masked_val1 = val1 & mask;
	const uint64_t masked_val2 = val2 & mask;

	if ( instr.op0_kind ( ) == OpKindSimple::Register && instr.op1_kind ( ) == OpKindSimple::Register ) {
		helpers::set_operand_value<uint64_t> ( instr, 0u, masked_val2, context );
		helpers::set_operand_value<uint64_t> ( instr, 1u, masked_val1, context );
		return;
	}

	if ( instr.op0_kind ( ) == OpKindSimple::Memory && instr.op1_kind ( ) == OpKindSimple::Register ) {
		const uint64_t addr = helpers::calculate_mem_addr( instr, context );
		context.set_memory<uint64_t> ( addr, masked_val2 );
		helpers::set_operand_value<uint64_t> ( instr, 1u, masked_val1, context );
		return;
	}

	if ( instr.op0_kind ( ) == OpKindSimple::Register && instr.op1_kind ( ) == OpKindSimple::Memory ) {
		const uint64_t addr = helpers::calculate_mem_addr( instr, context );
		context.set_memory<uint64_t> ( addr, masked_val1 );
		helpers::set_operand_value<uint64_t> ( instr, 0u, masked_val2, context );
		return;
	}

	// !TODO(exception)
}

/// MOVUPS - Move Unaligned Packed Single-Precision Floating-Point
/// Moves 128 bits of packed single-precision floating-point values from the source to the destination, without alignment requirements, and without affecting flags.
void handlers::movups ( const iced::Instruction& instr, KUBERA& context ) {
	if ( instr.op0_kind ( ) == OpKindSimple::Register && instr.op1_kind ( ) == OpKindSimple::Register ) {
		const uint128_t src_val = context.get_xmm_raw ( instr.op1_reg ( ) );
		context.set_xmm_raw ( instr.op0_reg ( ), src_val );
		return;
	}

	if ( instr.op0_kind ( ) == OpKindSimple::Register && instr.op1_kind ( ) == OpKindSimple::Memory ) {
		const uint64_t addr = helpers::calculate_mem_addr( instr, context );
		const uint128_t src_val = context.get_memory<uint128_t> ( addr );
		context.set_xmm_raw ( instr.op0_reg ( ), src_val );
		return;
	}

	if ( instr.op0_kind ( ) == OpKindSimple::Memory && instr.op1_kind ( ) == OpKindSimple::Register ) {
		const uint64_t addr = helpers::calculate_mem_addr( instr, context );
		const uint128_t src_val = context.get_xmm_raw ( instr.op1_reg ( ) );
		context.set_memory<uint128_t> ( addr, src_val );
		return;
	}

	// !TODO(exception)
}

/// MOVQ - Move Quadword
/// Moves a 64-bit value between XMM registers, general-purpose registers, or memory, without affecting flags.
void handlers::movq ( const iced::Instruction& instr, KUBERA& context ) {
	const size_t op_size = instr.op0_size ( );
	if ( op_size != 8 ) {
		// !TODO(exception)
		return;
	}

	if ( instr.op0_kind ( ) == OpKindSimple::Register && instr.op1_kind ( ) == OpKindSimple::Register ) {
		if ( instr.op0_reg ( ) >= Register::XMM0 && instr.op0_reg ( ) <= Register::XMM31 &&
				instr.op1_reg ( ) >= Register::XMM0 && instr.op1_reg ( ) <= Register::XMM31 ) {
			const uint128_t src_val = context.get_xmm_raw ( instr.op1_reg ( ) );
			const uint64_t lo64 = static_cast< uint64_t >( src_val & 0xFFFFFFFFFFFFFFFF );
			const uint128_t dst_val = static_cast< uint128_t >( lo64 );
			context.set_xmm_raw ( instr.op0_reg ( ), dst_val );
			return;
		}
		if ( instr.op0_reg ( ) <= Register::R15 &&
				instr.op1_reg ( ) >= Register::XMM0 && instr.op1_reg ( ) <= Register::XMM31 ) {
			const uint128_t src_val = context.get_xmm_raw ( instr.op1_reg ( ) );
			const uint64_t lo64 = static_cast< uint64_t >( src_val & 0xFFFFFFFFFFFFFFFF );
			helpers::set_operand_value<uint64_t> ( instr, 0u, lo64, context );
			return;
		}
		if ( instr.op0_reg ( ) >= Register::XMM0 && instr.op0_reg ( ) <= Register::XMM31 &&
				instr.op1_reg ( ) <= Register::R15 ) {
			const uint64_t src_val = helpers::get_operand_value<uint64_t> ( instr, 1u, context );
			const uint128_t dst_val = static_cast< uint128_t >( src_val );
			context.set_xmm_raw ( instr.op0_reg ( ), dst_val );
			return;
		}
	}

	if ( instr.op0_kind ( ) == OpKindSimple::Register && instr.op1_kind ( ) == OpKindSimple::Memory ) {
		if ( instr.op0_reg ( ) >= Register::XMM0 && instr.op0_reg ( ) <= Register::XMM31 ) {
			const uint64_t addr = helpers::calculate_mem_addr( instr, context );
			const uint64_t src_val = context.get_memory<uint64_t> ( addr );
			const uint128_t dst_val = static_cast< uint128_t >( src_val );
			context.set_xmm_raw ( instr.op0_reg ( ), dst_val );
			return;
		}
	}

	if ( instr.op0_kind ( ) == OpKindSimple::Memory && instr.op1_kind ( ) == OpKindSimple::Register ) {
		if ( instr.op1_reg ( ) >= Register::XMM0 && instr.op1_reg ( ) <= Register::XMM31 ) {
			const uint64_t addr = helpers::calculate_mem_addr( instr, context );
			const uint128_t src_val = context.get_xmm_raw ( instr.op1_reg ( ) );
			const uint64_t lo64 = static_cast< uint64_t >( src_val & 0xFFFFFFFFFFFFFFFF );
			context.set_memory<uint64_t> ( addr, lo64 );
			return;
		}
	}

	// !TODO(exception)
}

```

`src/handlers/floating_80.cpp`:

```cpp
#include "../../emulator.hpp"
#include <bit>
#include "helpers.hpp"

using namespace kubera;

// Helper function to check FPU exceptions and set status flags
void check_fpu_exception ( x86::FPUStatusWord& fsw, const x86::FPUControlWord& fcw, uint16_t determined_fsw_flags ) {
	fsw.value |= determined_fsw_flags; // Set determined flags in FSW

	// Check for unmasked exceptions
	bool unmasked_occurred = false;
	if ( ( determined_fsw_flags & x86::FSW_IE ) && !fcw.IM ) unmasked_occurred = true;
	if ( ( determined_fsw_flags & x86::FSW_DE ) && !fcw.DM ) unmasked_occurred = true;
	if ( ( determined_fsw_flags & x86::FSW_ZE ) && !fcw.ZM ) unmasked_occurred = true;
	if ( ( determined_fsw_flags & x86::FSW_OE ) && !fcw.OM ) unmasked_occurred = true;
	if ( ( determined_fsw_flags & x86::FSW_UE ) && !fcw.UM ) unmasked_occurred = true;
	if ( ( determined_fsw_flags & x86::FSW_PE ) && !fcw.PM ) unmasked_occurred = true;
	if ( determined_fsw_flags & x86::FSW_SF ) unmasked_occurred = true; // Stack fault is unmaskable

	if ( unmasked_occurred ) {
		fsw.ES = 1; // Set Error Summary
		// !TODO(exception)
	}
}

/// FLD - Load Floating-Point Value
/// Loads a floating-point value from memory (32-bit, 64-bit, or 80-bit) or an FPU register (ST(i)) onto the FPU stack, modifying C1 (set on stack overflow, cleared otherwise), TOP (decremented), and possibly IE, SF, or DE.
void handlers::fld ( const iced::Instruction& instr, KUBERA& context ) {
	const size_t op_size = instr.op0_size ( );
	auto& fpu = context.get_fpu ( );
	const int next_top = ( fpu.fpu_top - 1 + 8 ) % 8;
	const int st7_phys_idx = ( next_top + 7 ) % 8;
	uint16_t fsw_flags = 0;

	if ( fpu.get_fpu_tag ( st7_phys_idx ) != x86::FPU_TAG_EMPTY ) {
		fsw_flags |= ( x86::FSW_IE | x86::FSW_SF | x86::FSW_C1 );
		check_fpu_exception ( fpu.fpu_status_word, fpu.fpu_control_word, fsw_flags );
		return;
	}

	float80_t value;
	if ( instr.op0_kind ( ) == OpKindSimple::Memory ) {
		const uint64_t addr = helpers::calculate_mem_addr ( instr, context );
		if ( op_size == 4 ) {
			value = helpers::get_operand_value<float> ( instr, 0u, context );
		}
		else if ( op_size == 8 ) {
			value = helpers::get_operand_value<double> ( instr, 0u, context );
		}
		else if ( op_size == 10 ) {
			value = context.read_type_float80_t ( addr );
		}
		else {
			// !TODO(exception)
			return;
		}
		if ( fpu.classify_fpu_operand ( value ) == x86::FPU_TAG_SPECIAL ) {
			if ( boost::multiprecision::fpclassify ( value ) == FP_SUBNORMAL ) {
				fsw_flags |= x86::FSW_DE;
			}
			else if ( boost::multiprecision::isnan ( value ) ) {
				fsw_flags |= x86::FSW_IE;
			}
		}
	}
	else if ( instr.op0_kind ( ) == OpKindSimple::Register &&
					instr.op0_reg ( ) >= Register::ST0 && instr.op0_reg ( ) <= Register::ST7 ) {
		const int src_sti = static_cast< int >( instr.op0_reg ( ) ) - static_cast< int >( Register::ST0 );
		const int src_phys_idx = fpu.get_fpu_phys_idx ( src_sti );
		if ( fpu.get_fpu_tag ( src_phys_idx ) == x86::FPU_TAG_EMPTY ) {
			fsw_flags |= ( x86::FSW_IE | x86::FSW_SF );
			check_fpu_exception ( fpu.fpu_status_word, fpu.fpu_control_word, fsw_flags );
			return;
		}
		value = fpu.fpu_stack [ src_phys_idx ];
	}
	else {
		// !TODO(exception)
		return;
	}

	fpu.fpu_top = next_top;
	fpu.fpu_stack [ fpu.fpu_top ] = value;
	fpu.fpu_status_word.C1 = 0;
	fpu.update_fsw_top ( );
	fpu.set_fpu_tag ( fpu.fpu_top, fpu.classify_fpu_operand ( value ) );
	check_fpu_exception ( fpu.fpu_status_word, fpu.fpu_control_word, fsw_flags );
}

/// FPREM - Partial Remainder
/// Computes the partial remainder of ST(0) divided by ST(1), storing the result in ST(0), modifying C2 (set if reduction incomplete), C0, C1, C3 (cleared), IE, DE, UE, and TOP indirectly.
void handlers::fprem ( const iced::Instruction& instr, KUBERA& context ) {
	auto& fpu = context.get_fpu ( );
	const int st0_phys_idx = fpu.fpu_top;
	const int st1_phys_idx = ( fpu.fpu_top + 1 ) % 8;
	uint16_t fsw_flags = 0;

	if ( fpu.get_fpu_tag ( st0_phys_idx ) == x86::FPU_TAG_EMPTY ||
			fpu.get_fpu_tag ( st1_phys_idx ) == x86::FPU_TAG_EMPTY ) {
		fsw_flags |= ( x86::FSW_IE | x86::FSW_SF | x86::FSW_C2 );
		check_fpu_exception ( fpu.fpu_status_word, fpu.fpu_control_word, fsw_flags );
		return;
	}

	const float80_t st0_val = fpu.fpu_stack [ st0_phys_idx ];
	const float80_t st1_val = fpu.fpu_stack [ st1_phys_idx ];

	if ( boost::multiprecision::isnan ( st0_val ) || boost::multiprecision::isnan ( st1_val ) ||
			boost::multiprecision::isinf ( st0_val ) || boost::multiprecision::isinf ( st1_val ) ||
			st1_val == 0 ) {
		fsw_flags |= x86::FSW_IE;
		fpu.fpu_status_word.C2 = 1;
		check_fpu_exception ( fpu.fpu_status_word, fpu.fpu_control_word, fsw_flags );
		return;
	}

	if ( boost::multiprecision::fpclassify ( st0_val ) == FP_SUBNORMAL ||
			boost::multiprecision::fpclassify ( st1_val ) == FP_SUBNORMAL ) {
		fsw_flags |= x86::FSW_DE;
	}

	float80_t result = boost::multiprecision::remainder ( st0_val, st1_val );
	if ( result == 0 && st0_val != 0 ) {
		result = st0_val;
	}

	if ( boost::multiprecision::fpclassify ( result ) == FP_SUBNORMAL && result != 0 ) {
		fsw_flags |= ( x86::FSW_UE | x86::FSW_DE );
	}

	const int exp0 = st0_val.backend ( ).exponent ( );
	const int exp1 = st1_val.backend ( ).exponent ( );
	if ( exp0 - exp1 >= 64 ) {
		fpu.fpu_status_word.C2 = 1;
		fpu.fpu_status_word.C0 = 0;
		fpu.fpu_status_word.C1 = 0;
		fpu.fpu_status_word.C3 = 0;
	}
	else {
		fpu.fpu_status_word.C2 = 0;
		fpu.fpu_status_word.C0 = 0;
		fpu.fpu_status_word.C1 = 0;
		fpu.fpu_status_word.C3 = 0;
	}

	fpu.fpu_stack [ st0_phys_idx ] = result;
	fpu.set_fpu_tag ( st0_phys_idx, fpu.classify_fpu_operand ( result ) );
	check_fpu_exception ( fpu.fpu_status_word, fpu.fpu_control_word, fsw_flags );
}

/// FSTP - Store Floating-Point Value and Pop
/// Stores ST(0) to memory (32-bit, 64-bit, or 80-bit) or an FPU register (ST(i)) and pops the FPU stack, modifying C1 (set on underflow/overflow, cleared otherwise), IE, DE, UE, OE, PE, and TOP (incremented).
void handlers::fstp ( const iced::Instruction& instr, KUBERA& context ) {
	const size_t op_size = instr.op0_size ( );
	auto& fpu = context.get_fpu ( );
	const int st0_phys_idx = fpu.fpu_top;
	uint16_t fsw_flags = 0;

	if ( fpu.get_fpu_tag ( st0_phys_idx ) == x86::FPU_TAG_EMPTY ) {
		fsw_flags |= ( x86::FSW_IE | x86::FSW_SF | x86::FSW_C1 );
		check_fpu_exception ( fpu.fpu_status_word, fpu.fpu_control_word, fsw_flags );
		return;
	}

	const float80_t value = fpu.fpu_stack [ st0_phys_idx ];
	if ( boost::multiprecision::isnan ( value ) ) {
		fsw_flags |= x86::FSW_IE;
	}
	if ( boost::multiprecision::fpclassify ( value ) == FP_SUBNORMAL ) {
		fsw_flags |= x86::FSW_DE;
	}

	if ( instr.op0_kind ( ) == OpKindSimple::Memory ) {
		const uint64_t addr = helpers::get_operand_value<uint64_t> ( instr, 0u, context );
		if ( op_size == 4 ) {
			const float f_val = value.convert_to<float> ( );
			const uint32_t f_bits = std::bit_cast< uint32_t >( f_val );
			helpers::set_operand_value<uint32_t> ( instr, 0u, f_bits, context );
			const float80_t check_back = f_val;
			if ( f_val != 0 && value != 0 && check_back == 0 ) fsw_flags |= x86::FSW_UE;
			if ( !boost::multiprecision::isinf ( check_back ) && boost::multiprecision::isinf ( value ) ) fsw_flags |= x86::FSW_OE;
			if ( check_back != value ) fsw_flags |= x86::FSW_PE;
		}
		else if ( op_size == 8 ) {
			const double d_val = value.convert_to<double> ( );
			const uint64_t d_bits = std::bit_cast< uint64_t >( d_val );
			helpers::set_operand_value<uint64_t> ( instr, 0u, d_bits, context );
			const float80_t check_back = d_val;
			if ( d_val != 0 && value != 0 && check_back == 0 ) fsw_flags |= x86::FSW_UE;
			if ( !boost::multiprecision::isinf ( check_back ) && boost::multiprecision::isinf ( value ) ) fsw_flags |= x86::FSW_OE;
			if ( check_back != value ) fsw_flags |= x86::FSW_PE;
		}
		else if ( op_size == 10 ) {
			context.write_type ( addr, value );
		}
		else {
			// !TODO(exception)
			return;
		}
	}
	else if ( instr.op0_kind ( ) == OpKindSimple::Register &&
					instr.op0_reg ( ) >= Register::ST0 && instr.op0_reg ( ) <= Register::ST7 ) {
		const int dst_sti = static_cast< int >( instr.op0_reg ( ) ) - static_cast< int >( Register::ST0 );
		const int dst_phys_idx = fpu.get_fpu_phys_idx ( dst_sti );
		fpu.fpu_stack [ dst_phys_idx ] = value;
		fpu.set_fpu_tag ( dst_phys_idx, fpu.classify_fpu_operand ( value ) );
	}
	else {
		// !TODO(exception)
		return;
	}

	fpu.fpu_status_word.C1 = ( fsw_flags & ( x86::FSW_UE | x86::FSW_OE ) ) ? 1 : 0;
	fpu.set_fpu_tag ( st0_phys_idx, x86::FPU_TAG_EMPTY );
	fpu.fpu_top = ( fpu.fpu_top + 1 ) % 8;
	fpu.update_fsw_top ( );
	check_fpu_exception ( fpu.fpu_status_word, fpu.fpu_control_word, fsw_flags );
}

/// FFREE - Free FPU Register
/// Marks the specified FPU register (ST(i)) as empty, without modifying FPU status flags directly, but affecting TOP indirectly via tag updates.
void handlers::ffree ( const iced::Instruction& instr, KUBERA& context ) {
	if ( instr.op0_kind ( ) == OpKindSimple::Register &&
			instr.op0_reg ( ) >= Register::ST0 && instr.op0_reg ( ) <= Register::ST7 ) {
		const int sti = static_cast< int >( instr.op0_reg ( ) ) - static_cast< int >( Register::ST0 );
		const int phys_idx = context.get_fpu ( ).get_fpu_phys_idx ( sti );
		context.get_fpu ( ).set_fpu_tag ( phys_idx, x86::FPU_TAG_EMPTY );
	}
	else {
		// !TODO(exception)
	}
}

/// FINCSTP - Increment FPU Stack Pointer
/// Increments the FPU stack top pointer, clearing C1 and modifying TOP.
void handlers::fincstp ( const iced::Instruction& instr, KUBERA& context ) {
	auto& fpu = context.get_fpu ( );
	fpu.fpu_top = ( fpu.fpu_top + 1 ) % 8;
	fpu.fpu_status_word.C1 = 0;
	fpu.update_fsw_top ( );
}

/// FMUL - Floating-Point Multiply
/// Multiplies ST(0) or ST(i) with another operand (ST(i) or memory), storing the result in ST(0) or ST(i), modifying IE, DE, OE, UE, PE, and C1 (cleared). FMULP pops the stack, incrementing TOP.
void handlers::fmul ( const iced::Instruction& instr, KUBERA& context ) {
	const bool pop_stack = ( instr.mnemonic ( ) == Mnemonic::Fmulp );
	const size_t op_count = instr.op_count ( );
	auto& fpu = context.get_fpu ( );
	const int st0_phys_idx = fpu.fpu_top;
	uint16_t fsw_flags = 0;
	float80_t operand1, operand2, result;
	int dst_phys_idx = st0_phys_idx;
	int dst_sti = 0;
	const int sti = static_cast< int >( instr.op0_reg ( ) ) - static_cast< int >( Register::ST0 );
	if ( op_count == 0 ) {
		const int st1_phys_idx = fpu.get_fpu_phys_idx ( 1 );
		dst_phys_idx = st1_phys_idx;
		dst_sti = 1;
		if ( fpu.get_fpu_tag ( st0_phys_idx ) == x86::FPU_TAG_EMPTY ||
				fpu.get_fpu_tag ( st1_phys_idx ) == x86::FPU_TAG_EMPTY ) {
			fsw_flags |= ( x86::FSW_IE | x86::FSW_SF );
			check_fpu_exception ( fpu.fpu_status_word, fpu.fpu_control_word, fsw_flags );
			return;
		}
		operand1 = fpu.fpu_stack [ st1_phys_idx ];
		operand2 = fpu.fpu_stack [ st0_phys_idx ];
	}
	else if ( op_count == 1 ) {
		if ( fpu.get_fpu_tag ( st0_phys_idx ) == x86::FPU_TAG_EMPTY ) {
			fsw_flags |= ( x86::FSW_IE | x86::FSW_SF );
			check_fpu_exception ( fpu.fpu_status_word, fpu.fpu_control_word, fsw_flags );
			return;
		}
		operand1 = fpu.fpu_stack [ st0_phys_idx ];
		if ( instr.op0_kind ( ) == OpKindSimple::Memory ) {
			const size_t op_size = instr.op0_size ( );
			if ( op_size == 4 ) {
				operand2 = helpers::get_operand_value<float> ( instr, 0u, context );
			}
			else if ( op_size == 8 ) {
				operand2 = helpers::get_operand_value<double> ( instr, 0u, context );
			}
			else {
				// !TODO(exception)
				return;
			}
		}
		else if ( instr.op0_kind ( ) == OpKindSimple::Register &&
						instr.op0_reg ( ) >= Register::ST0 && instr.op0_reg ( ) <= Register::ST7 ) {
			const int sti_phys_idx = fpu.get_fpu_phys_idx ( sti );
			if ( fpu.get_fpu_tag ( sti_phys_idx ) == x86::FPU_TAG_EMPTY ) {
				fsw_flags |= ( x86::FSW_IE | x86::FSW_SF );
				check_fpu_exception ( fpu.fpu_status_word, fpu.fpu_control_word, fsw_flags );
				return;
			}
			operand2 = fpu.fpu_stack [ sti_phys_idx ];
		}
		else {
			// !TODO(exception)
			return;
		}
	}
	else if ( op_count == 2 ) {
		if ( instr.op0_kind ( ) == OpKindSimple::Register &&
				instr.op0_reg ( ) >= Register::ST0 && instr.op0_reg ( ) <= Register::ST7 &&
				instr.op1_kind ( ) == OpKindSimple::Register && instr.op1_reg ( ) == Register::ST0 ) {
			dst_phys_idx = fpu.get_fpu_phys_idx ( sti );
			dst_sti = sti;
			if ( fpu.get_fpu_tag ( st0_phys_idx ) == x86::FPU_TAG_EMPTY ||
					fpu.get_fpu_tag ( dst_phys_idx ) == x86::FPU_TAG_EMPTY ) {
				fsw_flags |= ( x86::FSW_IE | x86::FSW_SF );
				check_fpu_exception ( fpu.fpu_status_word, fpu.fpu_control_word, fsw_flags );
				return;
			}
			operand1 = fpu.fpu_stack [ dst_phys_idx ];
			operand2 = fpu.fpu_stack [ st0_phys_idx ];
		}
		else if ( instr.op0_kind ( ) == OpKindSimple::Register && instr.op0_reg ( ) == Register::ST0 &&
						instr.op1_kind ( ) == OpKindSimple::Register &&
						instr.op1_reg ( ) >= Register::ST0 && instr.op1_reg ( ) <= Register::ST7 ) {
			const int sti_phys_idx = fpu.get_fpu_phys_idx ( sti );
			if ( fpu.get_fpu_tag ( st0_phys_idx ) == x86::FPU_TAG_EMPTY ||
					fpu.get_fpu_tag ( sti_phys_idx ) == x86::FPU_TAG_EMPTY ) {
				fsw_flags |= ( x86::FSW_IE | x86::FSW_SF );
				check_fpu_exception ( fpu.fpu_status_word, fpu.fpu_control_word, fsw_flags );
				return;
			}
			operand1 = fpu.fpu_stack [ st0_phys_idx ];
			operand2 = fpu.fpu_stack [ sti_phys_idx ];
		}
		else {
			// !TODO(exception)
			return;
		}
	}
	else {
		// !TODO(exception)
		return;
	}

	if ( boost::multiprecision::fpclassify ( operand1 ) == FP_SUBNORMAL ||
			boost::multiprecision::fpclassify ( operand2 ) == FP_SUBNORMAL ) {
		fsw_flags |= x86::FSW_DE;
	}
	if ( boost::multiprecision::isnan ( operand1 ) || boost::multiprecision::isnan ( operand2 ) ||
			( boost::multiprecision::fpclassify ( operand1 ) == FP_ZERO && boost::multiprecision::fpclassify ( operand2 ) == FP_INFINITE ) ||
			( boost::multiprecision::fpclassify ( operand1 ) == FP_INFINITE && boost::multiprecision::fpclassify ( operand2 ) == FP_ZERO ) ) {
		fsw_flags |= x86::FSW_IE;
		check_fpu_exception ( fpu.fpu_status_word, fpu.fpu_control_word, fsw_flags );
		return;
	}

	result = operand1 * operand2;
	if ( boost::multiprecision::fpclassify ( result ) == FP_INFINITE ) {
		fsw_flags |= x86::FSW_OE;
	}
	if ( boost::multiprecision::fpclassify ( result ) == FP_SUBNORMAL ) {
		fsw_flags |= x86::FSW_DE;
	}
	if ( ( boost::multiprecision::fpclassify ( result ) == FP_SUBNORMAL || boost::multiprecision::fpclassify ( result ) == FP_ZERO ) &&
			boost::multiprecision::fpclassify ( operand1 ) == FP_NORMAL && boost::multiprecision::fpclassify ( operand2 ) == FP_NORMAL ) {
		fsw_flags |= x86::FSW_UE;
	}
	if ( !( fsw_flags & ( x86::FSW_OE | x86::FSW_UE ) ) && boost::multiprecision::fpclassify ( result ) != FP_ZERO ) {
		fsw_flags |= x86::FSW_PE;
	}

	fpu.fpu_status_word.C1 = 0;
	if ( fsw_flags & x86::FSW_IE && !( fsw_flags & x86::FSW_SF ) && fpu.fpu_control_word.IM ) {
		result = std::numeric_limits<float80_t>::quiet_NaN ( );
	}

	fpu.fpu_stack [ dst_phys_idx ] = result;
	fpu.set_fpu_tag ( dst_phys_idx, fpu.classify_fpu_operand ( result ) );
	if ( pop_stack ) {
		fpu.set_fpu_tag ( st0_phys_idx, x86::FPU_TAG_EMPTY );
		fpu.fpu_top = ( fpu.fpu_top + 1 ) % 8;
		fpu.update_fsw_top ( );
	}
	check_fpu_exception ( fpu.fpu_status_word, fpu.fpu_control_word, fsw_flags );
}

/// FNSTCW - Store FPU Control Word
/// Stores the 16-bit FPU control word to a memory location, without affecting flags.
void handlers::fnstcw ( const iced::Instruction& instr, KUBERA& context ) {
	const size_t op_size = instr.op0_size ( );
	if ( instr.op0_kind ( ) != OpKindSimple::Memory || op_size != 2 ) {
		// !TODO(exception)
		return;
	}

	const uint64_t addr = helpers::calculate_mem_addr ( instr, context );
	const uint16_t control_word = context.get_fpu ( ).fpu_control_word.value;
	context.set_memory<uint16_t> ( addr, control_word );
}

```

`src/handlers/floating_point.cpp`:

```cpp
#include "../../emulator.hpp"
#include <bit>
#include <numeric>
#include <cfenv>
#include "helpers.hpp"

using namespace kubera;

/// ADDSS - Add Scalar Single-Precision
/// Adds a single-precision float from the source (XMM or memory) to the destination XMM register (bits 31:0), storing the result in the destination, modifying MXCSR flags (IE, DE, ZE, OE, UE, PE).
void handlers::addss ( const iced::Instruction& instr, KUBERA& context ) {
	if ( instr.op0_kind ( ) != OpKindSimple::Register ||
			instr.op0_size ( ) != 4 ||
			instr.op0_reg ( ) < Register::XMM0 || instr.op0_reg ( ) > Register::XMM31 ) {
		// !TODO(exception)
		return;
	}

	const float dst_val = context.get_xmm_float ( instr.op0_reg ( ) );
	const float src_val = helpers::get_operand_value<float> ( instr, 1u, context );
	const float result = dst_val + src_val;
	auto& mxcsr = context.get_mxcsr ( );

	mxcsr.IE = ( std::isnan ( dst_val ) || std::isnan ( src_val ) || std::isinf ( dst_val ) || std::isinf ( src_val ) ) ? 1 : 0;
	mxcsr.DE = ( std::fpclassify ( dst_val ) == FP_SUBNORMAL || std::fpclassify ( src_val ) == FP_SUBNORMAL ) ? 1 : 0;
	mxcsr.ZE = 0;
	mxcsr.OE = ( std::isinf ( result ) && !std::isinf ( dst_val ) && !std::isinf ( src_val ) ) ? 1 : 0;
	mxcsr.UE = ( std::fpclassify ( result ) == FP_SUBNORMAL && std::fpclassify ( dst_val ) != FP_SUBNORMAL &&
							std::fpclassify ( src_val ) != FP_SUBNORMAL ) ? 1 : 0;
	mxcsr.PE = ( !std::isinf ( result ) && !std::isnan ( result ) && std::fpclassify ( result ) != FP_ZERO ) ? 1 : 0;

	if ( mxcsr.IE || mxcsr.OE || mxcsr.UE ) {
		// !TODO(exception)
		return;
	}

	context.set_xmm_float ( instr.op0_reg ( ), result );
}

/// SUBSS - Subtract Scalar Single-Precision
/// Subtracts a single-precision float from the source (XMM or memory) from the destination XMM register (bits 31:0), storing the result in the destination, modifying MXCSR flags (IE, DE, OE, UE, PE).
void handlers::subss ( const iced::Instruction& instr, KUBERA& context ) {
	if ( instr.op0_kind ( ) != OpKindSimple::Register ||
			instr.op0_size ( ) != 4 ||
			instr.op0_reg ( ) < Register::XMM0 || instr.op0_reg ( ) > Register::XMM31 ) {
		// !TODO(exception)
		return;
	}

	const float dst_val = context.get_xmm_float ( instr.op0_reg ( ) );
	const float src_val = helpers::get_operand_value<float> ( instr, 1u, context );
	const float result = dst_val - src_val;
	auto& mxcsr = context.get_mxcsr ( );

	mxcsr.IE = ( std::isnan ( dst_val ) || std::isnan ( src_val ) || std::isinf ( dst_val ) || std::isinf ( src_val ) ) ? 1 : 0;
	mxcsr.DE = ( std::fpclassify ( dst_val ) == FP_SUBNORMAL || std::fpclassify ( src_val ) == FP_SUBNORMAL ) ? 1 : 0;
	mxcsr.ZE = 0;
	mxcsr.OE = ( std::isinf ( result ) && !std::isinf ( dst_val ) && !std::isinf ( src_val ) ) ? 1 : 0;
	mxcsr.UE = ( std::fpclassify ( result ) == FP_SUBNORMAL && std::fpclassify ( dst_val ) != FP_SUBNORMAL &&
							std::fpclassify ( src_val ) != FP_SUBNORMAL ) ? 1 : 0;
	mxcsr.PE = ( !std::isinf ( result ) && !std::isnan ( result ) && std::fpclassify ( result ) != FP_ZERO ) ? 1 : 0;

	if ( mxcsr.IE || mxcsr.OE || mxcsr.UE ) {
		// !TODO(exception)
		return;
	}

	context.set_xmm_float ( instr.op0_reg ( ), result );
}

/// MULSS - Multiply Scalar Single-Precision
/// Multiplies a single-precision float from the source (XMM or memory) with the destination XMM register (bits 31:0), storing the result in the destination, modifying MXCSR flags (IE, DE, OE, UE, PE).
void handlers::mulss ( const iced::Instruction& instr, KUBERA& context ) {
	if ( instr.op0_kind ( ) != OpKindSimple::Register ||
			instr.op0_size ( ) != 4 ||
			instr.op0_reg ( ) < Register::XMM0 || instr.op0_reg ( ) > Register::XMM31 ) {
		// !TODO(exception)
		return;
	}

	const float dst_val = context.get_xmm_float ( instr.op0_reg ( ) );
	const float src_val = helpers::get_operand_value<float> ( instr, 1u, context );
	const float result = dst_val * src_val;
	auto& mxcsr = context.get_mxcsr ( );

	mxcsr.IE = ( std::isnan ( dst_val ) || std::isnan ( src_val ) ||
							( std::fpclassify ( dst_val ) == FP_ZERO && std::isinf ( src_val ) ) ||
							( std::isinf ( dst_val ) && std::fpclassify ( src_val ) == FP_ZERO ) ) ? 1 : 0;
	mxcsr.DE = ( std::fpclassify ( dst_val ) == FP_SUBNORMAL || std::fpclassify ( src_val ) == FP_SUBNORMAL ) ? 1 : 0;
	mxcsr.ZE = 0;
	mxcsr.OE = ( std::isinf ( result ) && !std::isinf ( dst_val ) && !std::isinf ( src_val ) ) ? 1 : 0;
	mxcsr.UE = ( std::fpclassify ( result ) == FP_SUBNORMAL && std::fpclassify ( dst_val ) != FP_SUBNORMAL &&
							std::fpclassify ( src_val ) != FP_SUBNORMAL ) ? 1 : 0;
	mxcsr.PE = ( !std::isinf ( result ) && !std::isnan ( result ) && std::fpclassify ( result ) != FP_ZERO ) ? 1 : 0;

	if ( mxcsr.IE || mxcsr.OE || mxcsr.UE ) {
		// !TODO(exception)
		return;
	}

	context.set_xmm_float ( instr.op0_reg ( ), result );
}

/// DIVSS - Divide Scalar Single-Precision
/// Divides the destination XMM register (bits 31:0) by a single-precision float from the source (XMM or memory), storing the result in the destination, modifying MXCSR flags (IE, DE, ZE, OE, UE, PE).
void handlers::divss ( const iced::Instruction& instr, KUBERA& context ) {
	if ( instr.op0_kind ( ) != OpKindSimple::Register ||
			instr.op0_size ( ) != 4 ||
			instr.op0_reg ( ) < Register::XMM0 || instr.op0_reg ( ) > Register::XMM31 ) {
		// !TODO(exception)
		return;
	}

	const float dst_val = context.get_xmm_float ( instr.op0_reg ( ) );
	const float src_val = helpers::get_operand_value<float> ( instr, 1u, context );
	const float result = dst_val / src_val;
	auto& mxcsr = context.get_mxcsr ( );

	mxcsr.IE = ( std::isnan ( dst_val ) || std::isnan ( src_val ) || std::isinf ( dst_val ) || std::isinf ( src_val ) ) ? 1 : 0;
	mxcsr.DE = ( std::fpclassify ( dst_val ) == FP_SUBNORMAL || std::fpclassify ( src_val ) == FP_SUBNORMAL ) ? 1 : 0;
	mxcsr.ZE = ( src_val == 0 ) ? 1 : 0;
	mxcsr.OE = ( std::isinf ( result ) && !std::isinf ( dst_val ) && !std::isinf ( src_val ) ) ? 1 : 0;
	mxcsr.UE = ( std::fpclassify ( result ) == FP_SUBNORMAL && std::fpclassify ( dst_val ) != FP_SUBNORMAL &&
							std::fpclassify ( src_val ) != FP_SUBNORMAL ) ? 1 : 0;
	mxcsr.PE = ( !std::isinf ( result ) && !std::isnan ( result ) && std::fpclassify ( result ) != FP_ZERO ) ? 1 : 0;

	if ( mxcsr.IE || mxcsr.ZE || mxcsr.OE || mxcsr.UE ) {
		// !TODO(exception)
		return;
	}

	context.set_xmm_float ( instr.op0_reg ( ), result );
}

/// SQRTSS - Square Root Scalar Single-Precision
/// Computes the square root of a single-precision float from the source (XMM or memory), storing the result in the destination XMM register (bits 31:0), modifying MXCSR flags (IE, DE, PE).
void handlers::sqrtss ( const iced::Instruction& instr, KUBERA& context ) {
	if ( instr.op0_kind ( ) != OpKindSimple::Register ||
			instr.op0_size ( ) != 4 ||
			instr.op0_reg ( ) < Register::XMM0 || instr.op0_reg ( ) > Register::XMM31 ) {
		// !TODO(exception)
		return;
	}

	const float src_val = helpers::get_operand_value<float> ( instr, 1u, context );
	auto& mxcsr = context.get_mxcsr ( );

	if ( std::signbit ( src_val ) || std::isnan ( src_val ) ) {
		mxcsr.IE = 1;
		// !TODO(exception)
		return;
	}

	const float result = std::sqrt ( src_val );
	mxcsr.IE = 0;
	mxcsr.DE = ( std::fpclassify ( src_val ) == FP_SUBNORMAL ) ? 1 : 0;
	mxcsr.ZE = 0;
	mxcsr.OE = 0;
	mxcsr.UE = ( std::fpclassify ( result ) == FP_SUBNORMAL && std::fpclassify ( src_val ) != FP_SUBNORMAL ) ? 1 : 0;
	mxcsr.PE = ( !std::isinf ( result ) && !std::isnan ( result ) && std::fpclassify ( result ) != FP_ZERO ) ? 1 : 0;

	if ( mxcsr.IE || mxcsr.UE ) {
		// !TODO(exception)
		return;
	}

	context.set_xmm_float ( instr.op0_reg ( ), result );
}

/// SQRTSD - Square Root Scalar Double-Precision
/// Computes the square root of a double-precision float from the source (XMM or memory), storing the result in the destination XMM register (bits 63:0), modifying MXCSR flags (IE, DE, PE).
void handlers::sqrtsd ( const iced::Instruction& instr, KUBERA& context ) {
	if ( instr.op0_kind ( ) != OpKindSimple::Register ||
			instr.op0_size ( ) != 8 ||
			instr.op0_reg ( ) < Register::XMM0 || instr.op0_reg ( ) > Register::XMM31 ) {
		// !TODO(exception)
		return;
	}

	const double src_val = helpers::get_operand_value<double> ( instr, 1u, context );
	auto& mxcsr = context.get_mxcsr ( );

	if ( std::signbit ( src_val ) || std::isnan ( src_val ) ) {
		mxcsr.IE = 1;
		// !TODO(exception)
		return;
	}

	const double result = std::sqrt ( src_val );
	mxcsr.IE = 0;
	mxcsr.DE = ( std::fpclassify ( src_val ) == FP_SUBNORMAL ) ? 1 : 0;
	mxcsr.ZE = 0;
	mxcsr.OE = 0;
	mxcsr.UE = ( std::fpclassify ( result ) == FP_SUBNORMAL && std::fpclassify ( src_val ) != FP_SUBNORMAL ) ? 1 : 0;
	mxcsr.PE = ( !std::isinf ( result ) && !std::isnan ( result ) && std::fpclassify ( result ) != FP_ZERO ) ? 1 : 0;

	if ( mxcsr.IE || mxcsr.UE ) {
		// !TODO(exception)
		return;
	}

	context.set_xmm_double ( instr.op0_reg ( ), result );
}

/// COMISS - Compare Scalar Single-Precision Ordered
/// Compares two single-precision floats, setting EFLAGS (ZF, PF, CF based on comparison; OF, SF, AF cleared), signaling an invalid operation for signaling NaNs.
void handlers::comiss ( const iced::Instruction& instr, KUBERA& context ) {
	if ( instr.op0_kind ( ) != OpKindSimple::Register ||
			instr.op0_size ( ) != 4 ||
			instr.op0_reg ( ) < Register::XMM0 || instr.op0_reg ( ) > Register::XMM31 ) {
		// !TODO(exception)
		return;
	}

	const float src1 = context.get_xmm_float ( instr.op0_reg ( ) );
	const float src2 = helpers::get_operand_value<float> ( instr, 1u, context );
	auto& flags = context.get_flags ( );
	auto& mxcsr = context.get_mxcsr ( );

	flags.ZF = 0;
	flags.PF = 0;
	flags.CF = 0;
	flags.OF = 0;
	flags.SF = 0;
	flags.AF = 0;

	if ( std::isnan ( src1 ) || std::isnan ( src2 ) ) {
		mxcsr.IE = 1;
		flags.ZF = 1;
		flags.PF = 1;
		flags.CF = 1;
		// !TODO(exception)
		return;
	}

	if ( src1 == src2 ) {
		flags.ZF = 1;
	}
	else if ( src1 < src2 ) {
		flags.CF = 1;
	}
}

/// UCOMISS - Compare Scalar Single-Precision Unordered
/// Compares two single-precision floats, setting EFLAGS (ZF, PF, CF based on comparison; OF, SF, AF cleared), without signaling for NaNs.
void handlers::ucomiss ( const iced::Instruction& instr, KUBERA& context ) {
	if ( instr.op0_kind ( ) != OpKindSimple::Register ||
			instr.op0_size ( ) != 4 ||
			instr.op0_reg ( ) < Register::XMM0 || instr.op0_reg ( ) > Register::XMM31 ) {
		// !TODO(exception)
		return;
	}

	const float src1 = context.get_xmm_float ( instr.op0_reg ( ) );
	const float src2 = helpers::get_operand_value<float> ( instr, 1u, context );
	auto& flags = context.get_flags ( );

	flags.ZF = 0;
	flags.PF = 0;
	flags.CF = 0;
	flags.OF = 0;
	flags.SF = 0;
	flags.AF = 0;

	if ( std::isnan ( src1 ) || std::isnan ( src2 ) ) {
		flags.ZF = 1;
		flags.PF = 1;
		flags.CF = 1;
	}
	else if ( src1 == src2 ) {
		flags.ZF = 1;
	}
	else if ( src1 < src2 ) {
		flags.CF = 1;
	}
}

/// CMPSS - Compare Scalar Single-Precision
/// Compares two single-precision floats using a predicate (immediate), storing 0xFFFFFFFF or 0 in the destination XMM register (bits 31:0), modifying MXCSR IE for signaling NaNs.
void handlers::cmpss ( const iced::Instruction& instr, KUBERA& context ) {
	if ( instr.op0_kind ( ) != OpKindSimple::Register ||
			instr.op0_size ( ) != 4 ||
			instr.op0_reg ( ) < Register::XMM0 || instr.op0_reg ( ) > Register::XMM31 ||
			instr.op2_kind ( ) != OpKindSimple::Immediate ) {
		// !TODO(exception)
		return;
	}

	const float op1_val = context.get_xmm_float ( instr.op0_reg ( ) );
	const float op2_val = helpers::get_operand_value<float> ( instr, 1u, context );
	const uint8_t predicate = static_cast< uint8_t >( instr.immediate ( ) ) & 0x07;
	auto& mxcsr = context.get_mxcsr ( );

	bool result = false;
	if ( std::isnan ( op1_val ) || std::isnan ( op2_val ) ) {
		mxcsr.IE = 1;
		switch ( predicate ) {
			case 3: result = true; break; // UNORDERED
			case 4: case 5: case 6: result = true; break; // NEQ, NLT, NLE
			case 7: result = false; break; // ORDERED
			default: result = false; break; // EQ, LT, LE
		}
	}
	else {
		switch ( predicate ) {
			case 0: result = ( op1_val == op2_val ); break; // EQ
			case 1: result = ( op1_val < op2_val ); break; // LT
			case 2: result = ( op1_val <= op2_val ); break; // LE
			case 3: result = false; break; // UNORDERED
			case 4: result = ( op1_val != op2_val ); break; // NEQ
			case 5: result = !( op1_val < op2_val ); break; // NLT
			case 6: result = !( op1_val <= op2_val ); break; // NLE
			case 7: result = true; break; // ORDERED
		}
	}

	if ( mxcsr.IE ) {
		// !TODO(exception)
		return;
	}

	const uint32_t mask = result ? 0xFFFFFFFF : 0x00000000;
	context.set_xmm_float ( instr.op0_reg ( ), std::bit_cast< float >( mask ) );
}

/// CVTSS2SI - Convert Scalar Single-Precision to Signed Integer
/// Converts a single-precision float to a 32/64-bit signed integer with rounding, storing in a general-purpose register, modifying MXCSR flags (IE, PE).
void handlers::cvtss2si ( const iced::Instruction& instr, KUBERA& context ) {
	if ( instr.op0_kind ( ) != OpKindSimple::Register ||
			( instr.op0_size ( ) != 4 && instr.op0_size ( ) != 8 ) ) {
		// !TODO(exception)
		return;
	}

	const float src_val = helpers::get_operand_value<float> ( instr, 1u, context );
	auto& mxcsr = context.get_mxcsr ( );
	const int64_t indefinite_int = ( instr.op0_size ( ) == 8 ) ? INT64_MIN : INT32_MIN;
	int64_t result;

	std::fesetround ( mxcsr.RC );
	std::feclearexcept ( FE_ALL_EXCEPT );
	if ( instr.op0_size ( ) == 4 ) {
		result = static_cast< int64_t >( std::lrintf ( src_val ) );
	}
	else {
		result = std::lrint ( src_val );
	}
	const int fenv_excepts = std::fetestexcept ( FE_INVALID | FE_INEXACT );

	if ( fenv_excepts & FE_INVALID || std::isnan ( src_val ) || std::isinf ( src_val ) ||
			( instr.op0_size ( ) == 4 && ( result > INT32_MAX || result < INT32_MIN ) ) ||
			( instr.op0_size ( ) == 8 && ( src_val > static_cast< float >( INT64_MAX ) || src_val < static_cast< float > ( INT64_MIN ) ) ) ) {
		mxcsr.IE = 1;
		result = indefinite_int;
		// !TODO(exception)
	}
	else {
		mxcsr.IE = 0;
		mxcsr.PE = ( fenv_excepts & FE_INEXACT ) ? 1 : 0;
	}

	helpers::set_operand_value<int64_t> ( instr, 0u, result, context );
}

/// CVTTSS2SI - Convert Scalar Single-Precision to Signed Integer with Truncation
/// Converts a single-precision float to a 32/64-bit signed integer with truncation, storing in a general-purpose register, modifying MXCSR flags (IE, PE).
void handlers::cvttss2si ( const iced::Instruction& instr, KUBERA& context ) {
	if ( instr.op0_kind ( ) != OpKindSimple::Register ||
			( instr.op0_size ( ) != 4 && instr.op0_size ( ) != 8 ) ) {
		// !TODO(exception)
		return;
	}

	const float src_val = helpers::get_operand_value<float> ( instr, 1u, context );
	auto& mxcsr = context.get_mxcsr ( );
	const int64_t indefinite_int = ( instr.op0_size ( ) == 8 ) ? INT64_MIN : INT32_MIN;
	int64_t result;

	std::feclearexcept ( FE_ALL_EXCEPT );
	if ( instr.op0_size ( ) == 4 ) {
		result = static_cast< int32_t >( src_val );
	}
	else {
		result = static_cast< int64_t >( src_val );
	}
	const int fenv_excepts = std::fetestexcept ( FE_INVALID );

	if ( fenv_excepts & FE_INVALID || std::isnan ( src_val ) || std::isinf ( src_val ) ||
			( instr.op0_size ( ) == 4 && ( src_val >= 2147483648.0f || src_val < -2147483648.0f ) ) ||
			( instr.op0_size ( ) == 8 && ( src_val >= static_cast< float > ( INT64_MAX ) || src_val <= static_cast< float > ( INT64_MIN ) ) ) ) {
		mxcsr.IE = 1;
		result = indefinite_int;
		// !TODO(exception)
	}
	else {
		mxcsr.IE = 0;
		mxcsr.PE = 0; // Truncation does not set PE
	}

	helpers::set_operand_value<int64_t> ( instr, 0u, result, context );
}

/// CVTSI2SS - Convert Signed Integer to Scalar Single-Precision
/// Converts a 32/64-bit signed integer to a single-precision float, storing in the destination XMM register (bits 31:0), modifying MXCSR flag (PE).
void handlers::cvtsi2ss ( const iced::Instruction& instr, KUBERA& context ) {
	if ( instr.op0_kind ( ) != OpKindSimple::Register ||
			instr.op0_size ( ) != 4 ||
			instr.op0_reg ( ) < Register::XMM0 || instr.op0_reg ( ) > Register::XMM31 ||
			( instr.op1_size ( ) != 4 && instr.op1_size ( ) != 8 ) ) {
		// !TODO(exception)
		return;
	}

	const int64_t src_val = helpers::get_operand_value<int64_t> ( instr, 1u, context );
	auto& mxcsr = context.get_mxcsr ( );

	std::fesetround ( mxcsr.RC );
	std::feclearexcept ( FE_ALL_EXCEPT );
	const float result = static_cast< float >( instr.op1_size ( ) == 4 ? static_cast< int32_t >( src_val ) : src_val );
	const int fenv_excepts = std::fetestexcept ( FE_INEXACT );

	mxcsr.IE = 0;
	mxcsr.DE = 0;
	mxcsr.ZE = 0;
	mxcsr.OE = 0;
	mxcsr.UE = 0;
	mxcsr.PE = ( fenv_excepts & FE_INEXACT ) ? 1 : 0;

	context.set_xmm_float ( instr.op0_reg ( ), result );
}

/// CVTSI2SD - Convert Signed Integer to Scalar Double-Precision
/// Converts a 32/64-bit signed integer to a double-precision float, storing in the destination XMM register (bits 63:0), modifying MXCSR flag (PE).
void handlers::cvtsi2sd ( const iced::Instruction& instr, KUBERA& context ) {
	if ( instr.op0_kind ( ) != OpKindSimple::Register ||
			instr.op0_size ( ) != 8 ||
			instr.op0_reg ( ) < Register::XMM0 || instr.op0_reg ( ) > Register::XMM31 ||
			( instr.op1_size ( ) != 4 && instr.op1_size ( ) != 8 ) ) {
		// !TODO(exception)
		return;
	}

	const int64_t src_val = helpers::get_operand_value<int64_t> ( instr, 1u, context );
	auto& mxcsr = context.get_mxcsr ( );

	std::fesetround ( mxcsr.RC );
	std::feclearexcept ( FE_ALL_EXCEPT );
	const double result = static_cast< double >( instr.op1_size ( ) == 4 ? static_cast< int32_t >( src_val ) : src_val );
	const int fenv_excepts = std::fetestexcept ( FE_INEXACT );

	mxcsr.IE = 0;
	mxcsr.DE = 0;
	mxcsr.ZE = 0;
	mxcsr.OE = 0;
	mxcsr.UE = 0;
	mxcsr.PE = ( fenv_excepts & FE_INEXACT ) ? 1 : 0;

	context.set_xmm_double ( instr.op0_reg ( ), result );
}

/// CVTSS2SD - Convert Scalar Single-Precision to Double-Precision
/// Converts a single-precision float to a double-precision float, storing in the destination XMM register (bits 63:0), modifying MXCSR flags (IE, OE, UE, PE).
void handlers::cvtss2sd ( const iced::Instruction& instr, KUBERA& context ) {
	if ( instr.op0_kind ( ) != OpKindSimple::Register ||
			instr.op0_size ( ) != 8 ||
			instr.op0_reg ( ) < Register::XMM0 || instr.op0_reg ( ) > Register::XMM31 ) {
		// !TODO(exception)
		return;
	}

	const float src_val = helpers::get_operand_value<float> ( instr, 1u, context );
	auto& mxcsr = context.get_mxcsr ( );

	const double result = static_cast< double >( src_val );
	mxcsr.IE = ( std::isnan ( src_val ) || std::isinf ( src_val ) ) ? 1 : 0;
	mxcsr.DE = ( std::fpclassify ( src_val ) == FP_SUBNORMAL ) ? 1 : 0;
	mxcsr.ZE = 0;
	mxcsr.OE = 0;
	mxcsr.UE = ( std::fpclassify ( result ) == FP_SUBNORMAL && std::fpclassify ( src_val ) != FP_SUBNORMAL ) ? 1 : 0;
	mxcsr.PE = ( !std::isinf ( result ) && !std::isnan ( result ) && std::fpclassify ( result ) != FP_ZERO ) ? 1 : 0;

	if ( mxcsr.IE || mxcsr.UE ) {
		// !TODO(exception)
		return;
	}

	context.set_xmm_double ( instr.op0_reg ( ), result );
}

/// CVTSD2SS - Convert Scalar Double-Precision to Single-Precision
/// Converts a double-precision float to a single-precision float, storing in the destination XMM register (bits 31:0), modifying MXCSR flags (IE, OE, UE, PE).
void handlers::cvtsd2ss ( const iced::Instruction& instr, KUBERA& context ) {
	if ( instr.op0_kind ( ) != OpKindSimple::Register ||
			instr.op0_size ( ) != 4 ||
			instr.op0_reg ( ) < Register::XMM0 || instr.op0_reg ( ) > Register::XMM31 ) {
		// !TODO(exception)
		return;
	}

	const double src_val = helpers::get_operand_value<double> ( instr, 1u, context );
	auto& mxcsr = context.get_mxcsr ( );

	std::fesetround ( mxcsr.RC );
	std::feclearexcept ( FE_ALL_EXCEPT );
	const float result = static_cast< float >( src_val );
	const int fenv_excepts = std::fetestexcept ( FE_INVALID | FE_OVERFLOW | FE_UNDERFLOW | FE_INEXACT );

	mxcsr.IE = ( fenv_excepts & FE_INVALID || std::isnan ( src_val ) || std::isinf ( src_val ) ) ? 1 : 0;
	mxcsr.DE = ( std::fpclassify ( src_val ) == FP_SUBNORMAL ) ? 1 : 0;
	mxcsr.ZE = 0;
	mxcsr.OE = ( fenv_excepts & FE_OVERFLOW ) ? 1 : 0;
	mxcsr.UE = ( fenv_excepts & FE_UNDERFLOW ) ? 1 : 0;
	mxcsr.PE = ( fenv_excepts & FE_INEXACT ) ? 1 : 0;

	if ( mxcsr.IE || mxcsr.OE || mxcsr.UE ) {
		// !TODO(exception)
		return;
	}

	context.set_xmm_float ( instr.op0_reg ( ), result );
}

```

`src/handlers/helpers.cpp`:

```cpp
#include "helpers.hpp"

bool helpers::divide_unsigned_boost ( uint128_t dividend, uint64_t divisor, size_t op_size, uint64_t& quotient, uint64_t& remainder ) {
	if ( divisor == 0 ) {
		return true;
	}

	uint128_t q = dividend / divisor;
	uint128_t r = dividend % divisor;

	uint128_t max_quotient_val = GET_OPERAND_MASK ( op_size );

	if ( q > max_quotient_val ) {
		return true;
	}

	quotient = static_cast< uint64_t >( q );
	remainder = static_cast< uint64_t >( r );
	return false;
}

bool helpers::divide_signed_boost ( int128_t dividend, int64_t divisor, size_t op_size, int64_t& quotient, int64_t& remainder ) {
	int bits = static_cast<int>(op_size * 8);

	if ( divisor == 0 ) {
		return true;
	}

	int bits_dividend = bits * 2;
	if ( bits_dividend > 128 ) bits_dividend = 128;

	int128_t min_dividend = -( int128_t ( 1 ) << ( bits_dividend - 1 ) );
	if ( dividend == min_dividend && divisor == -1 ) {
		return true;
	}

	int128_t q = dividend / divisor;
	int128_t r = dividend % divisor;

	int128_t min_quotient = -( int128_t ( 1 ) << ( bits - 1 ) );
	int128_t max_quotient = ( int128_t ( 1 ) << ( bits - 1 ) ) - 1;
	if ( q < min_quotient || q > max_quotient ) {
		return true;
	}

	quotient = static_cast< int64_t >( q );
	remainder = static_cast< int64_t >( r );
	return false;
}

```

`src/handlers/helpers.hpp`:

```hpp
#pragma once

#include "../../KUBERA.hpp"

namespace helpers
{
	using namespace kubera;
	FORCE_INLINE uint64_t calculate_mem_addr ( const iced::Instruction& instr, KUBERA& state ) {
		uint64_t address = 0;
		if ( instr.mem_base ( ) == Register::RIP ) {
			address += instr.ip + instr.length ( );
		}
		else if ( instr.mem_base ( ) != Register::None ) {
			address += state.get_reg ( instr.mem_base ( ), 8 );
		}

		if ( instr.mem_index ( ) != Register::None ) {
			address += state.get_reg ( instr.mem_index ( ), 8 ) * instr.mem_scale ( );
		}
		if ( instr.segment_prefix ( ) != Register::None ) {
			address += state.get_reg ( instr.segment_prefix ( ), 8 );
		}

		address += instr.displacement ( );
		return address;
	}

	bool divide_unsigned_boost ( uint128_t dividend, uint64_t divisor, size_t op_size, uint64_t& quotient, uint64_t& remainder );
	bool divide_signed_boost ( int128_t dividend, int64_t divisor, size_t op_size, int64_t& quotient, int64_t& remainder );

	template <typename Type>
	Type get_operand_value ( const iced::Instruction& instr, size_t operand_index, KUBERA& state ) {
		switch ( instr.op_kind_simple ( operand_index ) ) {
			case OpKindSimple::Immediate:
				return static_cast< Type >( instr.immediate ( ) );
			case OpKindSimple::Register:
			{
				const auto reg = instr.op_reg ( operand_index );
				if constexpr ( std::is_same_v<Type, float> ) {
					if ( reg >= Register::XMM0 && reg <= Register::XMM31 ) {
						return state.get_xmm_float ( reg );
					}
				}
				else if constexpr ( std::is_same_v<Type, double> ) {
					if ( reg >= Register::XMM0 && reg <= Register::XMM31 ) {
						return state.get_xmm_double ( reg );
					}
				}
				else if constexpr ( std::is_same_v<Type, uint128_t> ) {
					if ( reg >= Register::XMM0 && reg <= Register::XMM31 ) {
						return state.get_xmm_raw ( reg );
					}
				}
				else if constexpr ( std::is_same_v<Type, uint256_t> ) {
					if ( reg >= Register::YMM0 && reg <= Register::YMM31 ) {
						return state.get_ymm_raw ( reg );
					}
				}
				else if constexpr ( std::is_same_v<Type, uint512_t> ) {
					if ( reg >= Register::ZMM0 && reg <= Register::ZMM31 ) {
						return state.get_zmm_raw ( reg );
					}
				}
				else {
					return static_cast< Type >( state.get_reg ( reg, sizeof ( Type ) ) );
				}
			}
			case OpKindSimple::Memory:
			{
				uint64_t address = calculate_mem_addr ( instr, state );
				return state.get_memory<Type> ( address );
			}
			case OpKindSimple::Invalid:
				break;
			default:
				return Type ( instr.branch_target ( ) );
		}
		return Type {};
	}
	template <typename Type>
	void set_operand_value ( const iced::Instruction& instr, size_t operand_index, Type value, KUBERA& state ) {
		switch ( instr.op_kind_simple ( operand_index ) ) {
			case OpKindSimple::Register:
			{
				const auto reg = instr.op_reg ( operand_index );
				if constexpr ( std::is_same_v<Type, float> ) {
					if ( reg >= Register::XMM0 && reg <= Register::XMM31 ) {
						return state.set_xmm_float ( reg, value );
					}
				}
				else if constexpr ( std::is_same_v<Type, double> ) {
					if ( reg >= Register::XMM0 && reg <= Register::XMM31 ) {
						return state.set_xmm_double ( reg, value );
					}
				}
				else if constexpr ( std::is_same_v<Type, uint128_t> ) {
					if ( reg >= Register::XMM0 && reg <= Register::XMM31 ) {
						return state.set_xmm_raw ( reg, value );
					}
				}
				else if constexpr ( std::is_same_v<Type, uint256_t> ) {
					if ( reg >= Register::YMM0 && reg <= Register::YMM31 ) {
						return state.set_ymm_raw ( reg, value );
					}
				}
				else if constexpr ( std::is_same_v<Type, uint512_t> ) {
					if ( reg >= Register::ZMM0 && reg <= Register::ZMM31 ) {
						return state.set_zmm_raw ( reg, value );
					}
				}
				else {
					return state.set_reg ( reg, value, sizeof ( Type ) );
				}
			}
			case OpKindSimple::Memory:
			{
				uint64_t address = calculate_mem_addr ( instr, state );
				return state.set_memory<Type> ( address, value );
			}
			default:
				break;
		}
	}
};

```

`src/handlers/misc.cpp`:

```cpp
#include "../../emulator.hpp"
#include <bit>
#include "helpers.hpp"

using namespace kubera;
/// NOP - No operation
void handlers::nop ( const iced::Instruction& instr, KUBERA& context ) {
	// No-op in emulation
}

/// PREFETCHW - Prefetch Write
/// Provides a hint to prefetch a memory location for writing, with no effect on registers or memory, without affecting flags.
void handlers::prefetchw ( const iced::Instruction& instr, KUBERA& context ) {
	if ( instr.op0_kind ( ) != OpKindSimple::Memory ) {
		// !TODO(exception)
		return;
	}
	// No-op in emulation
}

```

`src/handlers/set.cpp`:

```cpp
#include "../../emulator.hpp"
#include <bit>
#include "helpers.hpp"

using namespace kubera;

template <typename Func>
void setcc ( const iced::Instruction& instr, KUBERA& context, Func condition ) {
  const size_t op_size = instr.op0_size ( );
  const uint64_t result = condition ( context ) ? 1 : 0;
  helpers::set_operand_value<uint64_t> ( instr, 0u, result, context );
}

/// SETB-Set Byte if Below
/// Sets the destination byte to 1 if CF is set, otherwise to 0.
void handlers::setb ( const iced::Instruction& instr, KUBERA& context ) {
  setcc ( instr, context, [ ] ( KUBERA& context ) { return context.get_flags ( ).CF; } );
}

/// SETNP-Set Byte if Not Parity
/// Sets the destination byte to 1 if PF is clear, otherwise to 0.
void handlers::setnp ( const iced::Instruction& instr, KUBERA& context ) {
  setcc ( instr, context, [ ] ( KUBERA& context ) { return !context.get_flags ( ).PF; } );
}

/// SETS-Set Byte if Sign
/// Sets the destination byte to 1 if SF is set, otherwise to 0.
void handlers::sets ( const iced::Instruction& instr, KUBERA& context ) {
  setcc ( instr, context, [ ] ( KUBERA& context ) { return context.get_flags ( ).SF; } );
}

/// SETNL-Set Byte if Not Less
/// Sets the destination byte to 1 if SF equals OF, otherwise to 0.
void handlers::setnl ( const iced::Instruction& instr, KUBERA& context ) {
  setcc ( instr, context, [ ] ( KUBERA& context ) { return context.get_flags ( ).SF == context.get_flags ( ).OF; } );
}

/// SETO-Set Byte if Overflow
/// Sets the destination byte to 1 if OF is set, otherwise to 0.
void handlers::seto ( const iced::Instruction& instr, KUBERA& context ) {
  setcc ( instr, context, [ ] ( KUBERA& context ) { return context.get_flags ( ).OF; } );
}

/// SETBE-Set Byte if Below or Equal
/// Sets the destination byte to 1 if CF or ZF is set, otherwise to 0.
void handlers::setbe ( const iced::Instruction& instr, KUBERA& context ) {
  setcc ( instr, context, [ ] ( KUBERA& context ) { return context.get_flags ( ).CF | context.get_flags ( ).ZF; } );
}

/// SETZ-Set Byte if Zero
/// Sets the destination byte to 1 if ZF is set, otherwise to 0.
void handlers::setz ( const iced::Instruction& instr, KUBERA& context ) {
  setcc ( instr, context, [ ] ( KUBERA& context ) { return context.get_flags ( ).ZF; } );
}

/// SETNB-Set Byte if Not Below
/// Sets the destination byte to 1 if CF is clear, otherwise to 0.
void handlers::setnb ( const iced::Instruction& instr, KUBERA& context ) {
  setcc ( instr, context, [ ] ( KUBERA& context ) { return !context.get_flags ( ).CF; } );
}

/// SETNO-Set Byte if Not Overflow
/// Sets the destination byte to 1 if OF is clear, otherwise to 0.
void handlers::setno ( const iced::Instruction& instr, KUBERA& context ) {
  setcc ( instr, context, [ ] ( KUBERA& context ) { return !context.get_flags ( ).OF; } );
}

/// SETP-Set Byte if Parity
/// Sets the destination byte to 1 if PF is set, otherwise to 0.
void handlers::setp ( const iced::Instruction& instr, KUBERA& context ) {
  setcc ( instr, context, [ ] ( KUBERA& context ) { return context.get_flags ( ).PF; } );
}

/// SETLE-Set Byte if Less or Equal
/// Sets the destination byte to 1 if ZF is set or SF differs from OF, otherwise to 0.
void handlers::setle ( const iced::Instruction& instr, KUBERA& context ) {
  setcc ( instr, context, [ ] ( KUBERA& context ) { return context.get_flags ( ).ZF | ( context.get_flags ( ).SF ^ context.get_flags ( ).OF ); } );
}

/// SETNLE-Set Byte if Not Less or Equal
/// Sets the destination byte to 1 if ZF is clear and SF equals OF, otherwise to 0.
void handlers::setnle ( const iced::Instruction& instr, KUBERA& context ) {
  setcc ( instr, context, [ ] ( KUBERA& context ) { return !context.get_flags ( ).ZF && ( context.get_flags ( ).SF == context.get_flags ( ).OF ); } );
}

/// SETNS-Set Byte if Not Sign
/// Sets the destination byte to 1 if SF is clear, otherwise to 0.
void handlers::setns ( const iced::Instruction& instr, KUBERA& context ) {
  setcc ( instr, context, [ ] ( KUBERA& context ) { return !context.get_flags ( ).SF; } );
}

/// SETL-Set Byte if Less
/// Sets the destination byte to 1 if SF differs from OF, otherwise to 0.
void handlers::setl ( const iced::Instruction& instr, KUBERA& context ) {
  setcc ( instr, context, [ ] ( KUBERA& context ) { return context.get_flags ( ).SF != context.get_flags ( ).OF; } );
}

/// SETNBE-Set Byte if Not Below or Equal
/// Sets the destination byte to 1 if CF and ZF are clear, otherwise to 0.
void handlers::setnbe ( const iced::Instruction& instr, KUBERA& context ) {
  setcc ( instr, context, [ ] ( KUBERA& context ) { return !context.get_flags ( ).CF && !context.get_flags ( ).ZF; } );
}

/// SETNZ-Set Byte if Not Zero
/// Sets the destination byte to 1 if ZF is clear, otherwise to 0.
void handlers::setnz ( const iced::Instruction& instr, KUBERA& context ) {
  setcc ( instr, context, [ ] ( KUBERA& context ) { return !context.get_flags ( ).ZF; } );
}

```

`src/handlers/simd.cpp`:

```cpp
#include "../../emulator.hpp"
#include <bit>
#include <cfenv>
#include "helpers.hpp"

using namespace kubera;

/// VPXOR - Vector Packed XOR
/// Performs a bitwise XOR of two source XMM/YMM/ZMM registers or a register and memory, storing the result in the destination register, without affecting flags.
void handlers::vpxor ( const iced::Instruction& instr, KUBERA& context ) {
	const size_t op_size = instr.op0_size ( );
	if ( op_size != 16 && op_size != 32 && op_size != 64 ) {
		// !TODO(exception)
		return;
	}

	const uint512_t src1_val = helpers::get_operand_value<uint512_t> ( instr, 1u, context );
	const uint512_t src2_val = helpers::get_operand_value<uint512_t> ( instr, 2u, context );
	const uint512_t result = src1_val ^ src2_val;

	helpers::set_operand_value<uint512_t> ( instr, 0u, result, context );
}

/// VPCMPEQW - Vector Packed Compare Equal Word
/// Compares 16-bit words in two source XMM/YMM/ZMM registers or a register and memory, setting each word in the destination to 0xFFFF if equal or 0 if not equal, without affecting flags.
void handlers::vpcmpeqw ( const iced::Instruction& instr, KUBERA& context ) {
	const size_t op_size = instr.op0_size ( );
	if ( op_size != 16 && op_size != 32 && op_size != 64 ) {
		// !TODO(exception)
		return;
	}

	const int num_elements = static_cast< int >( op_size / sizeof ( uint16_t ) );
	const uint512_t src1_val = helpers::get_operand_value<uint512_t> ( instr, 1u, context );
	const uint512_t src2_val = helpers::get_operand_value<uint512_t> ( instr, 2u, context );
	uint512_t result = 0;

	for ( int i = 0; i < num_elements; ++i ) {
		const uint16_t element1 = static_cast< uint16_t > ( ( src1_val >> ( i * 16 ) ) & 0xFFFF );
		const uint16_t element2 = static_cast< uint16_t > ( ( src2_val >> ( i * 16 ) ) & 0xFFFF );
		if ( element1 == element2 ) {
			result |= ( uint512_t ( 0xFFFF ) << ( i * 16 ) );
		}
	}

	helpers::set_operand_value<uint512_t> ( instr, 0u, result, context );
}

/// VPMOVMSKB - Vector Move Mask Byte
/// Extracts the most significant bit of each byte in the source XMM/YMM/ZMM register, storing the resulting mask in a general-purpose register, without affecting flags.
void handlers::vpmovmskb ( const iced::Instruction& instr, KUBERA& context ) {
	const size_t src_size = instr.op1_size ( );
	const size_t dst_size = instr.op0_size ( );
	if ( ( src_size != 16 && src_size != 32 && src_size != 64 ) ||
			( dst_size != 4 && dst_size != 8 ) ||
			instr.op0_kind ( ) != OpKindSimple::Register ||
			instr.op1_kind ( ) != OpKindSimple::Register ) {
		// !TODO(exception)
		return;
	}

	const uint512_t src_val = helpers::get_operand_value<uint512_t> ( instr, 1u, context );
	const int num_bytes = static_cast< int >( src_size );
	uint64_t result = 0;

	for ( int i = 0; i < num_bytes; ++i ) {
		const uint8_t byte_val = static_cast< uint8_t > ( ( src_val >> ( i * 8 ) ) & 0xFF );
		if ( byte_val >> 7 ) {
			result |= ( 1ULL << i );
		}
	}

	helpers::set_operand_value<uint64_t> ( instr, 0u, result, context );
}

/// VZEROUPPER - Zero Upper Bits of YMM Registers
/// Zeroes the upper 128 bits of all YMM registers (YMM0-YMM15), preserving the lower 128 bits, without affecting flags.
void handlers::vzeroupper ( const iced::Instruction& instr, KUBERA& context ) {
	for ( int i = 0; i < 16; ++i ) {
		const Register ymm_reg = static_cast< Register > ( static_cast< int > ( Register::YMM0 ) + i );
		const uint256_t ymm_val = context.get_ymm_raw ( ymm_reg );
		const uint128_t lower_val = static_cast< uint128_t > ( ymm_val & uint256_t ( "0xFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF" ) );
		context.set_ymm_raw ( ymm_reg, lower_val );
	}
}

/// VINSERTF128 - Vector Insert Float 128
/// Inserts a 128-bit value from an XMM register or memory into the lower or upper 128 bits of a YMM register, based on an immediate, without affecting flags.
void handlers::vinsertf128 ( const iced::Instruction& instr, KUBERA& context ) {
	const size_t dst_size = instr.op0_size ( );
	if ( dst_size != 32 ||
			instr.op0_kind ( ) != OpKindSimple::Register ||
			instr.op1_kind ( ) != OpKindSimple::Register ||
			( instr.op2_kind ( ) != OpKindSimple::Register && instr.op2_kind ( ) != OpKindSimple::Memory ) ||
			instr.op3_kind ( ) != OpKindSimple::Immediate ) {
		// !TODO(exception)
		return;
	}

	const uint256_t src1_val = helpers::get_operand_value<uint256_t> ( instr, 1u, context );
	const uint128_t src2_val = helpers::get_operand_value<uint128_t> ( instr, 2u, context );
	const uint8_t imm = static_cast< uint8_t >( instr.immediate ( ) );
	uint256_t result;

	if ( ( imm & 0x01 ) == 0 ) { // Insert into lower 128 bits
		result = ( src1_val & uint256_t ( "0xFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF" ) ) | static_cast< uint256_t >( src2_val );
	}
	else { // Insert into upper 128 bits
		result = ( static_cast< uint256_t >( src2_val ) << 128 ) | ( src1_val & uint256_t ( "0xFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF" ) );
	}

	helpers::set_operand_value<uint256_t> ( instr, 0u, result, context );
}

/// VMOVUPS - Vector Move Unaligned Packed Single-Precision
/// Moves unaligned 128-bit (XMM), 256-bit (YMM), or 512-bit (ZMM) data between registers or memory, without affecting flags.
void handlers::vmovups ( const iced::Instruction& instr, KUBERA& context ) {
	const size_t op_size = instr.op0_kind ( ) == OpKindSimple::Memory ? instr.op1_size ( ) : instr.op0_size ( );
	if ( op_size != 16 && op_size != 32 && op_size != 64 ) {
		// !TODO(exception)
		return;
	}

	const uint512_t val = helpers::get_operand_value<uint512_t> ( instr, 1u, context );
	helpers::set_operand_value<uint512_t> ( instr, 0u, val, context );
}

/// VMOVAPS - Vector Move Aligned Packed Single-Precision
/// Moves aligned 128-bit (XMM), 256-bit (YMM), or 512-bit (ZMM) data between registers or memory, requiring alignment, without affecting flags.
void handlers::vmovaps ( const iced::Instruction& instr, KUBERA& context ) {
	const size_t op_size = instr.op0_kind ( ) == OpKindSimple::Memory ? instr.op1_size ( ) : instr.op0_size ( );
	if ( op_size != 16 && op_size != 32 && op_size != 64 ) {
		// !TODO(exception)
		return;
	}

	if ( instr.op1_kind ( ) == OpKindSimple::Memory ) {
		const uint64_t addr = helpers::calculate_mem_addr ( instr, context );
		if ( addr % op_size != 0 ) {
			// !TODO(exception)
			return;
		}
	}
	if ( instr.op0_kind ( ) == OpKindSimple::Memory ) {
		const uint64_t addr = helpers::calculate_mem_addr ( instr, context );
		if ( addr % op_size != 0 ) {
			// !TODO(exception)
			return;
		}
	}

	const uint512_t val = helpers::get_operand_value<uint512_t> ( instr, 1u, context );
	helpers::set_operand_value<uint512_t> ( instr, 0u, val, context );
}

/// VMOVDQU - Vector Move Unaligned Double Quadword
/// Moves unaligned 128-bit (XMM), 256-bit (YMM), or 512-bit (ZMM) data between registers or memory, without affecting flags.
void handlers::vmovdqu ( const iced::Instruction& instr, KUBERA& context ) {
	const size_t op_size = instr.op0_kind ( ) == OpKindSimple::Memory ? instr.op1_size ( ) : instr.op0_size ( );
	if ( op_size != 16 && op_size != 32 && op_size != 64 ) {
		// !TODO(exception)
		return;
	}

	const uint512_t val = helpers::get_operand_value<uint512_t> ( instr, 1u, context );
	helpers::set_operand_value<uint512_t> ( instr, 0u, val, context );
}

/// MOVDQU - Move Unaligned Double Quadword
/// Moves unaligned 128-bit (XMM) data between registers or memory, without affecting flags.
void handlers::movdqu ( const iced::Instruction& instr, KUBERA& context ) {
	const size_t op_size = instr.op0_kind ( ) == OpKindSimple::Memory ? instr.op1_size ( ) : instr.op0_size ( );
	if ( op_size != 16 ) {
		// !TODO(exception)
		return;
	}

	const uint512_t val = helpers::get_operand_value<uint512_t> ( instr, 1u, context );
	helpers::set_operand_value<uint512_t> ( instr, 0u, val, context );
}

/// PUNPCKLQDQ - Unpack Low Quadwords
/// Interleaves the low 64-bit quadwords of the source and destination XMM registers, storing the result in the destination, without affecting flags.
void handlers::punpcklqdq ( const iced::Instruction& instr, KUBERA& context ) {
	if ( instr.op0_kind ( ) != OpKindSimple::Register ||
			instr.op1_kind ( ) != OpKindSimple::Register ||
			instr.op0_size ( ) != 16 || instr.op1_size ( ) != 16 ||
			instr.op0_reg ( ) < Register::XMM0 || instr.op0_reg ( ) > Register::XMM31 ||
			instr.op1_reg ( ) < Register::XMM0 || instr.op1_reg ( ) > Register::XMM31 ) {
		// !TODO(exception)
		return;
	}

	const uint128_t v1 = context.get_xmm_raw ( instr.op0_reg ( ) );
	const uint128_t v2 = context.get_xmm_raw ( instr.op1_reg ( ) );
	const uint64_t lo1 = static_cast< uint64_t >( v1 );
	const uint64_t lo2 = static_cast< uint64_t >( v2 );
	const uint128_t result = ( static_cast< uint128_t >( lo2 ) << 64 ) | lo1;

	context.set_xmm_raw ( instr.op0_reg ( ), result );
}

/// MOVLHPS - Move Low to High Packed Single-Precision
/// Moves the low 64 bits of the source XMM register to the high 64 bits of the destination XMM register, preserving the low 64 bits, without affecting flags.
void handlers::movlhps ( const iced::Instruction& instr, KUBERA& context ) {
	if ( instr.op0_kind ( ) != OpKindSimple::Register ||
			instr.op1_kind ( ) != OpKindSimple::Register ||
			instr.op0_size ( ) != 16 || instr.op1_size ( ) != 16 ||
			instr.op0_reg ( ) < Register::XMM0 || instr.op0_reg ( ) > Register::XMM31 ||
			instr.op1_reg ( ) < Register::XMM0 || instr.op1_reg ( ) > Register::XMM31 ) {
		// !TODO(exception)
		return;
	}

	const uint128_t dst_val = context.get_xmm_raw ( instr.op0_reg ( ) );
	const uint128_t src_val = context.get_xmm_raw ( instr.op1_reg ( ) );
	const uint64_t dst_low = static_cast< uint64_t >( dst_val );
	const uint64_t src_low = static_cast< uint64_t >( src_val );
	const uint128_t result = ( static_cast< uint128_t >( src_low ) << 64 ) | ( dst_low & 0xFFFFFFFFFFFFFFFF );

	context.set_xmm_raw ( instr.op0_reg ( ), result );
}

/// PSRLDQ - Shift Right Logical Double Quadword
/// Shifts the destination XMM register right by the specified number of bytes (immediate), filling with zeros, without affecting flags.
void handlers::psrldq ( const iced::Instruction& instr, KUBERA& context ) {
	if ( instr.op0_kind ( ) != OpKindSimple::Register ||
			instr.op1_kind ( ) != OpKindSimple::Immediate ||
			instr.op0_size ( ) != 16 ||
			instr.op0_reg ( ) < Register::XMM0 || instr.op0_reg ( ) > Register::XMM31 ) {
		// !TODO(exception)
		return;
	}

	const uint8_t shift_amount = static_cast< uint8_t >( instr.immediate ( ) );
	const uint128_t val = context.get_xmm_raw ( instr.op0_reg ( ) );
	const uint128_t result = ( shift_amount > 15 ) ? 0 : ( val >> ( shift_amount * 8 ) );

	context.set_xmm_raw ( instr.op0_reg ( ), result );
}

/// MOVHLPS - Move High to Low Packed Single-Precision
/// Moves the high 64 bits of the source XMM register to the low 64 bits of the destination XMM register, preserving the high 64 bits of the destination, without affecting flags.
void handlers::movhlps ( const iced::Instruction& instr, KUBERA& context ) {
	if ( instr.op0_kind ( ) != OpKindSimple::Register ||
			instr.op1_kind ( ) != OpKindSimple::Register ||
			instr.op0_size ( ) != 16 || instr.op1_size ( ) != 16 ||
			instr.op0_reg ( ) < Register::XMM0 || instr.op0_reg ( ) > Register::XMM31 ||
			instr.op1_reg ( ) < Register::XMM0 || instr.op1_reg ( ) > Register::XMM31 ) {
		// !TODO(exception)
		return;
	}

	const uint128_t dst_val = context.get_xmm_raw ( instr.op0_reg ( ) );
	const uint128_t src_val = context.get_xmm_raw ( instr.op1_reg ( ) );
	const uint128_t result = ( dst_val & ( uint128_t ( 0xFFFFFFFFFFFFFFFF ) << 64 ) ) | static_cast< uint128_t >( src_val >> 64 );

	context.set_xmm_raw ( instr.op0_reg ( ), result );
}

/// UNPCKLPS - Unpack Low Packed Single-Precision
/// Interleaves the low 32-bit single-precision floats from the source and destination XMM registers or memory, storing the result in the destination XMM register, without affecting flags.
void handlers::unpcklps ( const iced::Instruction& instr, KUBERA& context ) {
	if ( instr.op0_kind ( ) != OpKindSimple::Register ||
			( instr.op1_kind ( ) != OpKindSimple::Register && instr.op1_kind ( ) != OpKindSimple::Memory ) ||
			instr.op0_size ( ) != 16 || instr.op1_size ( ) != 16 ||
			instr.op0_reg ( ) < Register::XMM0 || instr.op0_reg ( ) > Register::XMM31 ) {
		// !TODO(exception)
		return;
	}

	const uint128_t dst_val = context.get_xmm_raw ( instr.op0_reg ( ) );
	const uint128_t src_val = helpers::get_operand_value<uint128_t> ( instr, 1u, context );
	const uint32_t dst0 = static_cast< uint32_t >( dst_val );
	const uint32_t dst1 = static_cast< uint32_t >( dst_val >> 32 );
	const uint32_t src0 = static_cast< uint32_t >( src_val );
	const uint32_t src1 = static_cast< uint32_t >( src_val >> 32 );
	const uint128_t result = static_cast< uint128_t >( dst0 ) |
		( static_cast< uint128_t >( src0 ) << 32 ) |
		( static_cast< uint128_t >( dst1 ) << 64 ) |
		( static_cast< uint128_t >( src1 ) << 96 );

	context.set_xmm_raw ( instr.op0_reg ( ), result );
}

/// MINSS - Minimum Scalar Single-Precision
/// Stores the minimum of two single-precision floats from the destination XMM register (bits 31:0) and source (XMM or memory) in the destination, modifying MXCSR flag IE for signaling NaNs.
void handlers::minss ( const iced::Instruction& instr, KUBERA& context ) {
	if ( instr.op0_kind ( ) != OpKindSimple::Register ||
			instr.op0_size ( ) != 4 ||
			instr.op0_reg ( ) < Register::XMM0 || instr.op0_reg ( ) > Register::XMM31 ) {
		// !TODO(exception)
		return;
	}

	const float dst_val = context.get_xmm_float ( instr.op0_reg ( ) );
	const float src_val = helpers::get_operand_value<float> ( instr, 1u, context );
	auto& mxcsr = context.get_mxcsr ( );
	float result;

	if ( std::isnan ( dst_val ) || std::isnan ( src_val ) ) {
		mxcsr.IE = 1;
		result = src_val;
		// !TODO(exception)
	}
	else if ( dst_val == 0.0f && src_val == 0.0f ) {
		mxcsr.IE = 0;
		result = std::signbit ( dst_val ) ? dst_val : src_val;
	}
	else {
		mxcsr.IE = 0;
		result = ( dst_val < src_val ) ? dst_val : src_val;
	}

	context.set_xmm_float ( instr.op0_reg ( ), result );
}

/// MAXSS - Maximum Scalar Single-Precision
/// Stores the maximum of two single-precision floats from the destination XMM register (bits 31:0) and source (XMM or memory) in the destination, modifying MXCSR flag IE for signaling NaNs.
void handlers::maxss ( const iced::Instruction& instr, KUBERA& context ) {
	if ( instr.op0_kind ( ) != OpKindSimple::Register ||
			instr.op0_size ( ) != 4 ||
			instr.op0_reg ( ) < Register::XMM0 || instr.op0_reg ( ) > Register::XMM31 ) {
		// !TODO(exception)
		return;
	}

	const float dst_val = context.get_xmm_float ( instr.op0_reg ( ) );
	const float src_val = helpers::get_operand_value<float> ( instr, 1u, context );
	auto& mxcsr = context.get_mxcsr ( );
	float result;

	if ( std::isnan ( dst_val ) || std::isnan ( src_val ) ) {
		mxcsr.IE = 1;
		result = src_val;
		// !TODO(exception)
	}
	else if ( dst_val == 0.0f && src_val == 0.0f ) {
		mxcsr.IE = 0;
		result = std::signbit ( src_val ) ? dst_val : src_val;
	}
	else {
		mxcsr.IE = 0;
		result = ( dst_val > src_val ) ? dst_val : src_val;
	}

	context.set_xmm_float ( instr.op0_reg ( ), result );
}

/// ANDPS - Bitwise AND Packed Single-Precision
/// Performs a bitwise AND on 128-bit XMM registers or a register and memory, storing the result in the destination XMM register, without affecting flags.
void handlers::andps ( const iced::Instruction& instr, KUBERA& context ) {
	if ( instr.op0_kind ( ) != OpKindSimple::Register ||
			instr.op0_size ( ) != 16 ||
			instr.op0_reg ( ) < Register::XMM0 || instr.op0_reg ( ) > Register::XMM31 ||
			( instr.op1_kind ( ) != OpKindSimple::Register && instr.op1_kind ( ) != OpKindSimple::Memory ) ) {
		// !TODO(exception)
		return;
	}

	const uint128_t dst_val = context.get_xmm_raw ( instr.op0_reg ( ) );
	const uint128_t src_val = helpers::get_operand_value<uint128_t> ( instr, 1u, context );
	const uint128_t result = dst_val & src_val;

	context.set_xmm_raw ( instr.op0_reg ( ), result );
}

/// ORPS - Bitwise OR Packed Single-Precision
/// Performs a bitwise OR on 128-bit XMM registers or a register and memory, storing the result in the destination XMM register, without affecting flags.
void handlers::orps ( const iced::Instruction& instr, KUBERA& context ) {
	if ( instr.op0_kind ( ) != OpKindSimple::Register ||
			instr.op0_size ( ) != 16 ||
			instr.op0_reg ( ) < Register::XMM0 || instr.op0_reg ( ) > Register::XMM31 ||
			( instr.op1_kind ( ) != OpKindSimple::Register && instr.op1_kind ( ) != OpKindSimple::Memory ) ) {
		// !TODO(exception)
		return;
	}

	const uint128_t dst_val = context.get_xmm_raw ( instr.op0_reg ( ) );
	const uint128_t src_val = helpers::get_operand_value<uint128_t> ( instr, 1u, context );
	const uint128_t result = dst_val | src_val;

	context.set_xmm_raw ( instr.op0_reg ( ), result );
}

/// XORPS - Bitwise XOR Packed Single-Precision
/// Performs a bitwise XOR on 128-bit XMM registers or a register and memory, storing the result in the destination XMM register, zeroing if source equals destination, without affecting flags.
void handlers::xorps ( const iced::Instruction& instr, KUBERA& context ) {
	if ( instr.op0_kind ( ) != OpKindSimple::Register ||
			instr.op0_size ( ) != 16 ||
			instr.op0_reg ( ) < Register::XMM0 || instr.op0_reg ( ) > Register::XMM31 ||
			( instr.op1_kind ( ) != OpKindSimple::Register && instr.op1_kind ( ) != OpKindSimple::Memory ) ) {
		// !TODO(exception)
		return;
	}

	const uint128_t dst_val = context.get_xmm_raw ( instr.op0_reg ( ) );
	const uint128_t src_val = helpers::get_operand_value<uint128_t> ( instr, 1u, context );
	const uint128_t result = ( instr.op1_kind ( ) == OpKindSimple::Register &&
													 instr.op1_reg ( ) == instr.op0_reg ( ) ) ? 0 : ( dst_val ^ src_val );

	context.set_xmm_raw ( instr.op0_reg ( ), result );
}

/// COMISD - Compare Scalar Double-Precision Ordered
/// Compares two double-precision floats from the first XMM register (bits 63:0) and source (XMM or memory), setting EFLAGS (ZF, PF, CF based on comparison; OF, SF, AF cleared) and MXCSR IE for signaling NaNs.
void handlers::comisd ( const iced::Instruction& instr, KUBERA& context ) {
	if ( instr.op0_kind ( ) != OpKindSimple::Register ||
			instr.op0_size ( ) != 8 ||
			instr.op0_reg ( ) < Register::XMM0 || instr.op0_reg ( ) > Register::XMM31 ) {
		// !TODO(exception)
		return;
	}

	const double src1 = context.get_xmm_double ( instr.op0_reg ( ) );
	const double src2 = helpers::get_operand_value<double> ( instr, 1u, context );
	auto& flags = context.get_flags ( );
	auto& mxcsr = context.get_mxcsr ( );

	flags.ZF = 0;
	flags.PF = 0;
	flags.CF = 0;
	flags.OF = 0;
	flags.SF = 0;
	flags.AF = 0;

	if ( std::isnan ( src1 ) || std::isnan ( src2 ) ) {
		mxcsr.IE = 1;
		flags.ZF = 1;
		flags.PF = 1;
		flags.CF = 1;
		// !TODO(exception)
		return;
	}

	if ( src1 == src2 ) {
		flags.ZF = 1;
	}
	else if ( src1 < src2 ) {
		flags.CF = 1;
	}
}

/// MULSD - Multiply Scalar Double-Precision
/// Multiplies a double-precision float from the destination XMM register (bits 63:0) with a source (XMM or memory), storing the result in the destination, modifying MXCSR flags (IE, DE, OE, UE, PE).
void handlers::mulsd ( const iced::Instruction& instr, KUBERA& context ) {
	if ( instr.op0_kind ( ) != OpKindSimple::Register ||
			instr.op0_size ( ) != 8 ||
			instr.op0_reg ( ) < Register::XMM0 || instr.op0_reg ( ) > Register::XMM31 ) {
		// !TODO(exception)
		return;
	}

	const double dst_val = context.get_xmm_double ( instr.op0_reg ( ) );
	const double src_val = helpers::get_operand_value<double> ( instr, 1u, context );
	const double result = dst_val * src_val;
	auto& mxcsr = context.get_mxcsr ( );

	mxcsr.IE = ( std::isnan ( dst_val ) || std::isnan ( src_val ) ||
							( std::fpclassify ( dst_val ) == FP_ZERO && std::isinf ( src_val ) ) ||
							( std::isinf ( dst_val ) && std::fpclassify ( src_val ) == FP_ZERO ) ) ? 1 : 0;
	mxcsr.DE = ( std::fpclassify ( dst_val ) == FP_SUBNORMAL || std::fpclassify ( src_val ) == FP_SUBNORMAL ) ? 1 : 0;
	mxcsr.ZE = 0;
	mxcsr.OE = ( std::isinf ( result ) && !std::isinf ( dst_val ) && !std::isinf ( src_val ) ) ? 1 : 0;
	mxcsr.UE = ( std::fpclassify ( result ) == FP_SUBNORMAL && std::fpclassify ( dst_val ) != FP_SUBNORMAL &&
							std::fpclassify ( src_val ) != FP_SUBNORMAL ) ? 1 : 0;
	mxcsr.PE = ( !std::isinf ( result ) && !std::isnan ( result ) && std::fpclassify ( result ) != FP_ZERO ) ? 1 : 0;

	if ( mxcsr.IE || mxcsr.OE || mxcsr.UE ) {
		// !TODO(exception)
		return;
	}

	context.set_xmm_double ( instr.op0_reg ( ), result );
}

/// MOVSS - Move Scalar Single-Precision
/// Moves a single-precision float between XMM registers, from memory to XMM, or from XMM to memory, without affecting flags.
void handlers::movss ( const iced::Instruction& instr, KUBERA& context ) {
	if ( ( instr.op0_kind ( ) != OpKindSimple::Register && instr.op0_kind ( ) != OpKindSimple::Memory ) ||
			instr.op0_size ( ) != 4 ||
			( instr.op0_kind ( ) == OpKindSimple::Register &&
			 ( instr.op0_reg ( ) < Register::XMM0 || instr.op0_reg ( ) > Register::XMM31 ) ) ||
			( instr.op1_kind ( ) != OpKindSimple::Register && instr.op1_kind ( ) != OpKindSimple::Memory ) ||
			( instr.op1_kind ( ) == OpKindSimple::Register &&
			 ( instr.op1_reg ( ) < Register::XMM0 || instr.op1_reg ( ) > Register::XMM31 ) ) ) {
		// !TODO(exception)
		return;
	}

	const float src_val = helpers::get_operand_value<float> ( instr, 1u, context );

	if ( instr.op0_kind ( ) == OpKindSimple::Register ) {
		context.set_xmm_float ( instr.op0_reg ( ), src_val );
	}
	else if ( instr.op0_kind ( ) == OpKindSimple::Memory ) {
		const uint64_t addr = helpers::calculate_mem_addr( instr, context );
		context.set_memory<uint32_t> ( addr, std::bit_cast< uint32_t >( src_val ) );
	}
}

/// ROUNDSS - Round Scalar Single-Precision
/// Rounds a single-precision float from the source (XMM or memory) to a single-precision float using the specified rounding mode, storing in the destination XMM register (bits 31:0), modifying MXCSR flags (IE, DE, OE, UE, PE).
void handlers::roundss ( const iced::Instruction& instr, KUBERA& context ) {
	if ( instr.op0_kind ( ) != OpKindSimple::Register ||
			instr.op0_size ( ) != 4 ||
			instr.op0_reg ( ) < Register::XMM0 || instr.op0_reg ( ) > Register::XMM31 ||
			instr.op2_kind ( ) != OpKindSimple::Immediate ) {
		// !TODO(exception)
		return;
	}

	const float src_val = helpers::get_operand_value<float> ( instr, 1u, context );
	const uint8_t imm = static_cast< uint8_t >( instr.immediate ( ) );
	auto& mxcsr = context.get_mxcsr ( );

	int round_mode;
	switch ( imm & 0x07 ) {
		case 0: round_mode = FE_TONEAREST; break; // Round to nearest, ties to even
		case 1: round_mode = FE_DOWNWARD; break; // Round down
		case 2: round_mode = FE_UPWARD; break; // Round up
		case 3: round_mode = FE_TOWARDZERO; break; // Round toward zero
		default: round_mode = mxcsr.RC; break; // Use MXCSR rounding control
	}

	std::fesetround ( round_mode );
	std::feclearexcept ( FE_ALL_EXCEPT );
	const float result = std::nearbyintf ( src_val );
	const int fenv_excepts = std::fetestexcept ( FE_INVALID | FE_OVERFLOW | FE_UNDERFLOW | FE_INEXACT );

	mxcsr.IE = ( fenv_excepts & FE_INVALID || std::isnan ( src_val ) ) ? 1 : 0;
	mxcsr.DE = ( std::fpclassify ( src_val ) == FP_SUBNORMAL ) ? 1 : 0;
	mxcsr.ZE = 0;
	mxcsr.OE = ( fenv_excepts & FE_OVERFLOW ) ? 1 : 0;
	mxcsr.UE = ( fenv_excepts & FE_UNDERFLOW ) ? 1 : 0;
	mxcsr.PE = ( fenv_excepts & FE_INEXACT ) ? 1 : 0;

	if ( mxcsr.IE || mxcsr.OE || mxcsr.UE ) {
		// !TODO(exception)
		return;
	}

	context.set_xmm_float ( instr.op0_reg ( ), result );
}

/// RCPSS - Reciprocal Scalar Single-Precision
/// Computes an approximate reciprocal of a single-precision float from the source (XMM or memory), storing in the destination XMM register (bits 31:0), modifying MXCSR flags (IE, DE, PE).
void handlers::rcpss ( const iced::Instruction& instr, KUBERA& context ) {
	if ( instr.op0_kind ( ) != OpKindSimple::Register ||
			instr.op0_size ( ) != 4 ||
			instr.op0_reg ( ) < Register::XMM0 || instr.op0_reg ( ) > Register::XMM31 ) {
		// !TODO(exception)
		return;
	}

	const float src_val = helpers::get_operand_value<float> ( instr, 1u, context );
	auto& mxcsr = context.get_mxcsr ( );

	if ( src_val == 0.0f || std::isnan ( src_val ) || std::isinf ( src_val ) ) {
		mxcsr.IE = 1;
		// !TODO(exception)
		return;
	}

	const float result = 1.0f / src_val;
	mxcsr.IE = 0;
	mxcsr.DE = ( std::fpclassify ( src_val ) == FP_SUBNORMAL ) ? 1 : 0;
	mxcsr.ZE = 0;
	mxcsr.OE = 0;
	mxcsr.UE = ( std::fpclassify ( result ) == FP_SUBNORMAL && std::fpclassify ( src_val ) != FP_SUBNORMAL ) ? 1 : 0;
	mxcsr.PE = 1; // RCPSS is always approximate

	if ( mxcsr.IE || mxcsr.UE ) {
		// !TODO(exception)
		return;
	}

	context.set_xmm_float ( instr.op0_reg ( ), result );
}

/// RSQRTSS - Reciprocal Square Root Scalar Single-Precision
/// Computes an approximate reciprocal square root of a single-precision float from the source (XMM or memory), storing in the destination XMM register (bits 31:0), modifying MXCSR flags (IE, DE, PE).
void handlers::rsqrtss ( const iced::Instruction& instr, KUBERA& context ) {
	const float src_val = helpers::get_operand_value<float> ( instr, 1u, context );
	auto& mxcsr = context.get_mxcsr ( );

	if ( src_val < 0.0f || std::isnan ( src_val ) ) {
		mxcsr.IE = 1;
		// !TODO(exception)
		return;
	}

	const float result = src_val == 0.0f ? std::numeric_limits<float>::infinity ( ) : 1.0f / std::sqrt ( src_val );
	mxcsr.IE = 0;
	mxcsr.DE = ( std::fpclassify ( src_val ) == FP_SUBNORMAL ) ? 1 : 0;
	mxcsr.ZE = 0;
	mxcsr.OE = 0;
	mxcsr.UE = ( std::fpclassify ( result ) == FP_SUBNORMAL && std::fpclassify ( src_val ) != FP_SUBNORMAL ) ? 1 : 0;
	mxcsr.PE = 1; // RSQRTSS is always approximate

	if ( mxcsr.IE || mxcsr.UE ) {
		// !TODO(exception)
		return;
	}

	context.set_xmm_float ( instr.op0_reg ( ), result );
}

/// PINSRB - Insert Byte
/// Inserts a byte from a 32-bit general-purpose register or memory into a specified byte position in the destination XMM register, selected by an immediate, without affecting flags.
void handlers::pinsrb ( const iced::Instruction& instr, KUBERA& context ) {
	const uint8_t imm = static_cast< uint8_t >( instr.immediate ( ) );
	if ( imm > 15 ) {
		// !TODO(exception)
		return;
	}

	const uint32_t src_val = helpers::get_operand_value<uint32_t> ( instr, 1u, context );
	const uint8_t byte_val = static_cast< uint8_t >( src_val );
	const uint128_t dst_val = context.get_xmm_raw ( instr.op0_reg ( ) );
	const uint128_t mask = ~( uint128_t ( 0xFF ) << ( imm * 8 ) );
	const uint128_t result = ( dst_val & mask ) | ( static_cast< uint128_t >( byte_val ) << ( imm * 8 ) );

	context.set_xmm_raw ( instr.op0_reg ( ), result );
}

/// PINSRD - Insert Doubleword
/// Inserts a 32-bit doubleword from a 32-bit general-purpose register or memory into a specified doubleword position in the destination XMM register, selected by an immediate, without affecting flags.
void handlers::pinsrd ( const iced::Instruction& instr, KUBERA& context ) {
	const uint8_t imm = static_cast< uint8_t >( instr.immediate ( ) );
	if ( imm > 3 ) {
		// !TODO(exception)
		return;
	}

	const uint32_t src_val = helpers::get_operand_value<uint32_t> ( instr, 1u, context );
	const uint128_t dst_val = context.get_xmm_raw ( instr.op0_reg ( ) );
	const uint128_t mask = ~( uint128_t ( 0xFFFFFFFF ) << ( imm * 32 ) );
	const uint128_t result = ( dst_val & mask ) | ( static_cast< uint128_t >( src_val ) << ( imm * 32 ) );

	context.set_xmm_raw ( instr.op0_reg ( ), result );
}

/// PINSRQ - Insert Quadword
/// Inserts a 64-bit quadword from a 64-bit general-purpose register or memory into a specified quadword position in the destination XMM register, selected by an immediate, without affecting flags.
void handlers::pinsrq ( const iced::Instruction& instr, KUBERA& context ) {
	const uint8_t imm = static_cast< uint8_t >( instr.immediate ( ) );
	if ( imm > 1 ) {
		// !TODO(exception)
		return;
	}

	const uint64_t src_val = helpers::get_operand_value<uint64_t> ( instr, 1u, context );
	const uint128_t dst_val = context.get_xmm_raw ( instr.op0_reg ( ) );
	const uint128_t mask = ~( uint128_t ( 0xFFFFFFFFFFFFFFFF ) << ( imm * 64 ) );
	const uint128_t result = ( dst_val & mask ) | ( static_cast< uint128_t >( src_val ) << ( imm * 64 ) );

	context.set_xmm_raw ( instr.op0_reg ( ), result );
}

// Helper function for packed addition
template<typename T>
static void padd ( const iced::Instruction& instr, KUBERA& context, size_t elem_size ) {
	const uint128_t dst_val = context.get_xmm_raw ( instr.op0_reg ( ) );
	const uint128_t src_val = helpers::get_operand_value<uint128_t> ( instr, 1u, context );
	const int num_elements = static_cast< int >( 16 / elem_size );
	uint128_t result = 0;

	for ( int i = 0; i < num_elements; ++i ) {
		const T dst_elem = static_cast< T > ( ( dst_val >> ( i * elem_size * 8 ) ) & ( ( 1ULL << ( elem_size * 8 ) ) - 1 ) );
		const T src_elem = static_cast< T > ( ( src_val >> ( i * elem_size * 8 ) ) & ( ( 1ULL << ( elem_size * 8 ) ) - 1 ) );
		const T sum = dst_elem + src_elem;
		result |= static_cast< uint128_t > ( sum ) << ( i * elem_size * 8 );
	}

	context.set_xmm_raw ( instr.op0_reg ( ), result );
}

/// PADDB - Packed Add Bytes
/// Adds 16 packed 8-bit integers from the source (XMM or memory) to the destination XMM register, storing the result in the destination, without affecting flags.
void handlers::paddb ( const iced::Instruction& instr, KUBERA& context ) {
	padd<uint8_t> ( instr, context, 1 );
}

/// PADDW - Packed Add Words
/// Adds 8 packed 16-bit integers from the source (XMM or memory) to the destination XMM register, storing the result in the destination, without affecting flags.
void handlers::paddw ( const iced::Instruction& instr, KUBERA& context ) {
	padd<uint16_t> ( instr, context, 2 );
}

/// PADDD - Packed Add Doublewords
/// Adds 4 packed 32-bit integers from the source (XMM or memory) to the destination XMM register, storing the result in the destination, without affecting flags.
void handlers::paddd ( const iced::Instruction& instr, KUBERA& context ) {
	padd<uint32_t> ( instr, context, 4 );
}

/// PADDQ - Packed Add Quadwords
/// Adds 2 packed 64-bit integers from the source (XMM or memory) to the destination XMM register, storing the result in the destination, without affecting flags.
void handlers::paddq ( const iced::Instruction& instr, KUBERA& context ) {
	padd<uint64_t> ( instr, context, 8 );
}

```

`src/handlers/stack_frame.cpp`:

```cpp
#include "../../emulator.hpp"
#include <bit>
#include "helpers.hpp"

using namespace kubera;

/// ENTER - Enter Procedure
/// Allocates a stack frame by pushing the current RBP, setting RBP to the new RSP, and reserving space for local variables and optional nesting levels, without affecting flags.
void handlers::enter ( const iced::Instruction& instr, KUBERA& context ) {
	const uint64_t size = instr.immediate ( );
	const uint64_t nesting = instr.immediate2 ( );
	const size_t op_size = 8;

	if ( nesting != 0 ) {
		// !TODO(exception)
		return;
	}

	uint64_t current_rsp = context.get_reg ( Register::RSP, 8 );
	const uint64_t current_rbp = context.get_reg ( Register::RBP, 8 );

	current_rsp -= op_size;
	if ( !context.is_within_stack_bounds ( current_rsp, op_size ) ) {
		// !TODO(exception)
		return;
	}
	context.set_stack<uint64_t> ( current_rsp, current_rbp );
	context.set_reg ( Register::RBP, current_rsp, op_size );

	current_rsp -= size;
	if ( !context.is_within_stack_bounds ( current_rsp, size ) ) {
		// !TODO(exception)
		return;
	}
	context.set_reg ( Register::RSP, current_rsp, op_size );
}

/// LEAVE - Leave Procedure
/// Restores the stack frame by setting RSP to RBP, popping RBP from the stack, and adjusting RSP, without affecting flags.
void handlers::leave ( const iced::Instruction& instr, KUBERA& context ) {
	const size_t op_size = 8; // LEAVE uses 64-bit stack operations
	const uint64_t current_rbp = context.get_reg ( Register::RBP, op_size );
	const uint64_t saved_rbp = context.get_stack<uint64_t> ( current_rbp );

	if ( !context.is_within_stack_bounds ( current_rbp, op_size ) ) {
		// !TODO(exception)
		return;
	}

	context.set_reg ( Register::RSP, current_rbp, op_size );
	context.set_reg ( Register::RBP, saved_rbp, op_size );
	context.set_reg ( Register::RSP, current_rbp + op_size, op_size );
}

/// PUSHFQ - Push Flags (Quadword)
/// Pushes the 64-bit RFLAGS register onto the stack, without affecting flags.
void handlers::pushfq ( const iced::Instruction& instr, KUBERA& context ) {
	const size_t op_size = 8; // PUSHFQ pushes 64-bit RFLAGS
	uint64_t current_rsp = context.get_reg ( Register::RSP, op_size );
	current_rsp -= op_size;

	if ( !context.is_within_stack_bounds ( current_rsp, op_size ) ) {
		// !TODO(exception)
		return;
	}

	const uint64_t rflags = context.get_rflags ( );
	context.set_stack<uint64_t> ( current_rsp, rflags );
	context.set_reg ( Register::RSP, current_rsp, op_size );
}

/// POPFQ - Pop Flags (Quadword)
/// Pops a 64-bit value from the stack into the RFLAGS register, modifying CF, PF, AF, ZF, SF, TF, IF, DF, OF, IOPL, NT, RF, VM, AC, VIF, VIP, and ID flags.
void handlers::popfq ( const iced::Instruction& instr, KUBERA& context ) {
	const size_t op_size = 8; // POPFQ pops 64-bit RFLAGS
	const uint64_t current_rsp = context.get_reg ( Register::RSP, op_size );

	if ( !context.is_within_stack_bounds ( current_rsp, op_size ) ) {
		// !TODO(exception)
		return;
	}

	const uint64_t rflags = context.get_stack<uint64_t> ( current_rsp );
	context.set_rflags ( rflags );
	context.set_reg ( Register::RSP, current_rsp + op_size, op_size );
}

/// PUSH - Push onto Stack
/// Pushes a register, memory, or immediate value onto the stack, without affecting flags.
void handlers::push ( const iced::Instruction& instr, KUBERA& context ) {
	const size_t op_size = instr.op0_size ( );
	const uint64_t src_val = helpers::get_operand_value<uint64_t> ( instr, 0u, context );
	const uint64_t mask = GET_OPERAND_MASK ( op_size );
	const uint64_t value = src_val & mask;

	uint64_t current_rsp = context.get_reg ( Register::RSP, 8 );
	current_rsp -= 8; // Stack operations are 8 bytes

	if ( !context.is_within_stack_bounds ( current_rsp, 8 ) ) {
		// !TODO(exception)
		return;
	}

	context.set_stack<uint64_t> ( current_rsp, value );
	context.set_reg ( Register::RSP, current_rsp, 8 );
}

/// POP - Pop from Stack
/// Pops a value from the stack into a register or memory, without affecting flags.
void handlers::pop ( const iced::Instruction& instr, KUBERA& context ) {
	const size_t op_size = instr.op0_size ( );
	const uint64_t current_rsp = context.get_reg ( Register::RSP, 8 );

	if ( !context.is_within_stack_bounds ( current_rsp, 8 ) ) {
		// !TODO(exception)
		return;
	}

	const uint64_t value = context.get_stack<uint64_t> ( current_rsp );
	const uint64_t mask = GET_OPERAND_MASK ( op_size );
	const uint64_t masked_value = value & mask;

	if ( instr.op0_kind ( ) == OpKindSimple::Register ) {
		helpers::set_operand_value<uint64_t> ( instr, 0u, masked_value, context );
	}
	else if ( instr.op0_kind ( ) == OpKindSimple::Memory ) {
		const uint64_t addr = helpers::calculate_mem_addr ( instr, context );
		context.set_memory<uint64_t> ( addr, masked_value );
	}
	else {
		// !TODO(exception)
		return;
	}

	context.set_reg ( Register::RSP, current_rsp + 8, 8 );
}

```

`src/handlers/string.cpp`:

```cpp
#include "../../emulator.hpp"
#include <bit>
#include "helpers.hpp"

using namespace kubera;

void movs_handler ( const iced::Instruction& instr, KUBERA& context, uint8_t elem_size ) {
	const bool rep = instr.rep_prefix ( );
	const uint64_t count = rep ? context.get_reg ( Register::RCX, 8 ) : 1;
	if ( count == 0 ) {
		return;
	}

	const bool df = context.get_flags ( ).DF;
	const int64_t step = df ? -static_cast< int64_t >( elem_size ) : static_cast< int64_t >( elem_size );
	uint64_t rsi = context.get_reg ( Register::RSI, 8 );
	uint64_t rdi = context.get_reg ( Register::RDI, 8 );

	for ( uint64_t i = 0; i < count; ++i ) {
		const uint64_t src_addr = rsi + i * step;
		const uint64_t dst_addr = rdi + i * step;

		if ( elem_size == 1 ) {
			const uint8_t val = context.get_memory<uint8_t> ( src_addr );
			context.set_memory<uint8_t> ( dst_addr, val );
		}
		else if ( elem_size == 2 ) {
			const uint16_t val = context.get_memory<uint16_t> ( src_addr );
			context.set_memory<uint16_t> ( dst_addr, val );
		}
		else if ( elem_size == 4 ) {
			const uint32_t val = context.get_memory<uint32_t> ( src_addr );
			context.set_memory<uint32_t> ( dst_addr, val );
		}
		else if ( elem_size == 8 ) {
			const uint64_t val = context.get_memory<uint64_t> ( src_addr );
			context.set_memory<uint64_t> ( dst_addr, val );
		}
		else {
			// !TODO(exception)
			return;
		}
	}

	context.set_reg ( Register::RSI, rsi + count * step, 8 );
	context.set_reg ( Register::RDI, rdi + count * step, 8 );
	if ( rep ) {
		context.set_reg ( Register::RCX, 0, 8 );
	}
}

/// MOVS - Move String
/// Copies data of the specified size (byte, word, doubleword, or quadword) from the source address (RSI) to the destination address (RDI), updating RSI and RDI based on DF, and RCX if REP prefix is used, without affecting flags.
void handlers::movs ( const iced::Instruction& instr, KUBERA& context ) {
	const uint8_t elem_size = static_cast< uint8_t >( instr.op0_size ( ) );
	movs_handler ( instr, context, elem_size );
}

/// MOVSB - Move String Byte
/// Copies a byte from the source address (RSI) to the destination address (RDI), updating RSI and RDI based on DF, and RCX if REP prefix is used, without affecting flags.
void handlers::movsb ( const iced::Instruction& instr, KUBERA& context ) {
	movs_handler ( instr, context, 1 );
}

/// MOVSW - Move String Word
/// Copies a word (2 bytes) from the source address (RSI) to the destination address (RDI), updating RSI and RDI based on DF, and RCX if REP prefix is used, without affecting flags.
void handlers::movsw ( const iced::Instruction& instr, KUBERA& context ) {
	movs_handler ( instr, context, 2 );
}

/// MOVSD - Move String Doubleword
/// Copies a doubleword (4 bytes) from the source address (RSI) to the destination address (RDI), updating RSI and RDI based on DF, and RCX if REP prefix is used, without affecting flags.
void handlers::movsd ( const iced::Instruction& instr, KUBERA& context ) {
	movs_handler ( instr, context, 4 );
}

/// MOVSQ - Move String Quadword
/// Copies a quadword (8 bytes) from the source address (RSI) to the destination address (RDI), updating RSI and RDI based on DF, and RCX if REP prefix is used, without affecting flags.
void handlers::movsq ( const iced::Instruction& instr, KUBERA& context ) {
	movs_handler ( instr, context, 8 );
}

/// STOS - Store String
/// Stores a value from RAX (byte, word, doubleword, or quadword) to the destination address (RDI), updating RDI based on DF, and RCX if REP prefix is used, without affecting flags.
void handlers::stos ( const iced::Instruction& instr, KUBERA& context ) {
	const uint8_t elem_size = static_cast< uint8_t >( instr.op0_size ( ) );
	const bool rep = instr.rep_prefix ( );
	const uint64_t count = rep ? context.get_reg ( Register::RCX, 8 ) : 1;
	if ( count == 0 ) {
		if ( rep ) {
			context.set_reg ( Register::RCX, 0, 8 );
		}
		return;
	}

	const bool df = context.get_flags ( ).DF;
	const int64_t step = df ? -static_cast< int64_t >( elem_size ) : static_cast< int64_t >( elem_size );
	const uint64_t rdi = context.get_reg ( Register::RDI, 8 );
	const uint64_t rax = context.get_reg ( Register::RAX, elem_size );
	const uint64_t mask = GET_OPERAND_MASK ( elem_size );
	const uint64_t value = rax & mask;

	for ( uint64_t i = 0; i < count; ++i ) {
		const uint64_t dst_addr = rdi + i * step;

		if ( elem_size == 1 ) {
			context.set_memory<uint8_t> ( dst_addr, static_cast< uint8_t > ( value ) );
		}
		else if ( elem_size == 2 ) {
			context.set_memory<uint16_t> ( dst_addr, static_cast< uint16_t >( value ) );
		}
		else if ( elem_size == 4 ) {
			context.set_memory<uint32_t> ( dst_addr, static_cast< uint32_t >( value ) );
		}
		else if ( elem_size == 8 ) {
			context.set_memory<uint64_t> ( dst_addr, value );
		}
		else {
			// !TODO(exception)
			return;
		}
	}

	context.set_reg ( Register::RDI, rdi + count * step, 8 );
	if ( rep ) {
		context.set_reg ( Register::RCX, 0, 8 );
	}
}

```

`src/handlers/syscall.cpp`:

```cpp
#include "../../emulator.hpp"
#include <bit>
#include "helpers.hpp"

using namespace kubera;

/// SYSCALL-Fast System Call
void handlers::syscall ( const iced::Instruction& instr, KUBERA& context ) {
	std::printf ( "[!!!] Syscall instruction executed without a platform hook!\n" );
}
```

`src/handlers/system.cpp`:

```cpp
#include "../../emulator.hpp"
#include <bit>
#include "helpers.hpp"

using namespace kubera;

/// CLI-Clear Interrupt Flag
/// Clears the interrupt flag (IF) to disable maskable interrupts.
void handlers::cli ( const iced::Instruction& instr, KUBERA& context ) {
	if ( context.get_cpl ( ) == 0 ) {
		context.get_flags ( ).IF = 0;
	}
	else {
		// !TODO(exception)
	}
}

/// CLD-Clear Direction Flag
/// Clears the direction flag (DF), causing string instructions to increment the index registers.
void handlers::cld ( const iced::Instruction& instr, KUBERA& context ) {
	context.get_flags ( ).DF = 0;
}
/// CLC-Clear Carry Flag
/// Clears the carry flag (CF)
void handlers::clc ( const iced::Instruction& instr, KUBERA& context ) {
	context.get_flags ( ).CF = 0;
}

/// CLUI-Clear User Interrupt Flag
/// Clears the user interrupt flag (IF), disabling user-level interrupts (alias for CLI in some contexts).
void handlers::clui ( const iced::Instruction& instr, KUBERA& context ) {
	if ( context.get_cpl ( ) == 0 ) {
		context.get_flags ( ).IF = 0;
	}
	else {
		// !TODO(cr4)
		// !TODO(exception)
	}
}

/// CMC-Complement Carry Flag
/// Complements (toggles) the carry flag (CF).
void handlers::cmc ( const iced::Instruction& instr, KUBERA& context ) {
	context.get_flags ( ).CF ^= 1;
}

/// STC-Set Carry Flag
/// Sets the carry flag (CF) to 1.
void handlers::stc ( const iced::Instruction& instr, KUBERA& context ) {
	context.get_flags ( ).CF = 1;
}

/// STI-Set Interrupt Flag
/// Sets the interrupt flag (IF) to enable maskable interrupts.
void handlers::sti ( const iced::Instruction& instr, KUBERA& context ) {
	if ( context.get_cpl ( ) == 0 ) {
		context.get_flags ( ).IF = 1;
	}
	else {
		// !TODO(exception)
	}
}

/// STD-Set Direction Flag
/// Sets the direction flag (DF), causing string instructions to decrement the index registers.
void handlers::std ( const iced::Instruction& instr, KUBERA& context ) {
	context.get_flags ( ).DF = 1;
}

/// LAHF-Load Status Flags into AH
/// Loads the SF, ZF, AF, PF, and CF flags into the AH register (bits 7, 6, 4, 2, and 0, respectively).
void handlers::lahf ( const iced::Instruction& instr, KUBERA& context ) {
	auto& flags = context.get_flags ( );
	uint8_t ah_val = static_cast< uint8_t >( ( flags.SF << 7 ) | ( flags.ZF << 6 ) | ( flags.AF << 4 ) | ( flags.PF << 2 ) | flags.CF );
	context.set_reg ( Register::AH, ah_val, 1 );
}

/// SAHF-Store AH into Status Flags
/// Stores the contents of the AH register into the SF, ZF, AF, PF, and CF flags (bits 7, 6, 4, 2, and 0, respectively).
void handlers::sahf ( const iced::Instruction& instr, KUBERA& context ) {
	const uint64_t ah_val = context.get_reg ( Register::AH, 1 );
	auto& flags = context.get_flags ( );
	flags.SF = ( ah_val >> 7 ) & 1;
	flags.ZF = ( ah_val >> 6 ) & 1;
	flags.AF = ( ah_val >> 4 ) & 1;
	flags.PF = ( ah_val >> 2 ) & 1;
	flags.CF = ah_val & 1;
}

/// PUSHF-Push Flags
/// Pushes the lower 16 bits of the EFLAGS register onto the stack.
void handlers::pushf ( const iced::Instruction& instr, KUBERA& context ) {
	auto& flags = context.get_flags ( );
	uint16_t eflags = static_cast< uint16_t >( ( flags.SF << 7 ) | ( flags.ZF << 6 ) | ( flags.AF << 4 ) |
		( flags.PF << 2 ) | ( flags.CF ) | ( flags.IF << 9 ) | ( flags.DF << 10 ) );
	const uint64_t rsp = context.get_reg ( Register::RSP, 8 );
	const uint64_t addr = rsp - 2;
	context.set_memory<uint16_t> ( addr, eflags );
	context.set_reg ( Register::RSP, addr, 8 );
}

/// POPF-Pop Flags
/// Pops the lower 16 bits from the stack into the EFLAGS register.
void handlers::popf ( const iced::Instruction& instr, KUBERA& context ) {
	const uint64_t rsp = context.get_reg ( Register::RSP, 8 );
	const uint16_t eflags = context.get_memory<uint16_t> ( rsp );
	auto& flags = context.get_flags ( );
	flags.SF = ( eflags >> 7 ) & 1;
	flags.ZF = ( eflags >> 6 ) & 1;
	flags.AF = ( eflags >> 4 ) & 1;
	flags.PF = ( eflags >> 2 ) & 1;
	flags.CF = eflags & 1;
	flags.IF = ( eflags >> 9 ) & 1;
	flags.DF = ( eflags >> 10 ) & 1;
	context.set_reg ( Register::RSP, rsp + 2, 8 );
}

/// RDTSC-Read Time-Stamp Counter
/// Reads the processor's time-stamp counter into EDX:EAX (high:low 32 bits).
void handlers::rdtsc ( const iced::Instruction& instr, KUBERA& context ) {
	// !TODO(cr4)
	const auto tsc = context.read_tsc ( );
	const auto low = static_cast< uint32_t >( tsc & 0xFFFFFFFF );
	const auto high = static_cast< uint32_t >( tsc >> 32 );
	context.set_reg ( Register::RAX, low, 4 );
	context.set_reg ( Register::RDX, high, 4 );
}

/// INT1-Debug Trap
/// Triggers a debug exception (#DB) for single-step debugging.
void handlers::int1 ( const iced::Instruction& instr, KUBERA& context ) {
	if ( context.get_cpl ( ) == 0 ) {
		// !TODO(exception)
	}
	else {
		// !TODO(exception)
	}
}

/// INT3-Breakpoint
/// Triggers a breakpoint exception (#BP) for debugging.
void handlers::int3 ( const iced::Instruction& instr, KUBERA& context ) {
	// !TODO(exception)
}

/// INT-Software Interrupt
/// Triggers a software interrupt with the specified vector number (generic handler for all interrupts).
void handlers::int_ ( const iced::Instruction& instr, KUBERA& context ) {
	if ( context.get_cpl ( ) == 0 ) {
		// !TODO(exception)
	}
	else {
		// !TODO(exception)
	}
}

/// FXSAVE-Save x87 FPU, MMX, and SSE State
/// Saves the x87 FPU (FCW, FSW, FTW, FOP, FIP, FDP), MMX, and SSE (MXCSR, XMM0-XMM15) state to a 512-byte memory region specified by the destination operand.
void handlers::fxsave ( const iced::Instruction& instr, KUBERA& context ) {
	const uint64_t base_addr = helpers::calculate_mem_addr( instr, context );
	if ( base_addr % 16 != 0 ) {
		// !TODO(exception)
		return;
	}
	auto& fpu = context.get_fpu ( );
	context.set_memory<uint16_t> ( base_addr + 0, fpu.fpu_control_word.value );
	context.set_memory<uint16_t> ( base_addr + 2, fpu.fpu_status_word.value );

	uint8_t ftag = 0;
	for ( int i = 0; i < 8; ++i ) {
		int phys_idx = fpu.get_fpu_phys_idx ( i );
		if ( fpu.get_fpu_tag ( phys_idx ) == x86::FPU_TAG_EMPTY ) {
			ftag |= ( 1 << i );
		}
	}
	context.set_memory<uint8_t> ( base_addr + 4, ftag );
	context.set_memory<uint8_t> ( base_addr + 5, 0 );

	context.set_memory<uint16_t> ( base_addr + 6, 0 ); // FOP
	context.set_memory<uint64_t> ( base_addr + 8, 0 ); // FIP
	context.set_memory<uint64_t> ( base_addr + 16, 0 ); // FDP

	context.set_memory<uint32_t> ( base_addr + 24, context.get_mxcsr ( ).value );
	context.set_memory<uint32_t> ( base_addr + 28, 0xFFFF ); // MXCSR_MASK

	for ( int i = 0; i < 8; ++i ) {
		int phys_idx = fpu.get_fpu_phys_idx ( i );
		const float80_t& st_val = fpu.fpu_stack [ phys_idx ];
		uint64_t current_reg_addr = base_addr + 32 + ( i * 16 );
		context.write_type<float80_t> ( current_reg_addr, st_val );
		for ( int j = 10; j < 16; ++j ) {
			context.set_memory<uint8_t> ( current_reg_addr + j, 0 );
		}
	}

	for ( int i = 0; i < 31; ++i ) {
		uint128_t xmm_val = context.get_xmm_raw ( static_cast< Register > ( static_cast< int > ( Register::XMM0 ) + i ) );
		context.set_memory<uint128_t> ( base_addr + 160 + ( i * 16 ), xmm_val );
	}

	for ( int i = 0; i < 96; i += 8 ) {
		context.set_memory<uint64_t> ( base_addr + 416 + i, 0 );
	}
}

/// FXRSTOR-Restore x87 FPU, MMX, and SSE State
/// Restores the x87 FPU (FCW, FSW, FTW, FOP, FIP, FDP), MMX, and SSE (MXCSR, XMM0-XMM15) state from a 512-byte memory region specified by the source operand.
void handlers::fxrstor ( const iced::Instruction& instr, KUBERA& context ) {
	const uint64_t base_addr = helpers::get_operand_value<uint64_t> ( instr, 0u, context );
	if ( base_addr % 16 != 0 ) {
		// !TODO(exception)
		return;
	}

	uint16_t fcw = context.get_memory<uint16_t> ( base_addr + 0 );
	if ( ( fcw & 0xE0C0 ) != 0 ) {
		// !TODO(exception)
		return;
	}
	auto& fpu = context.get_fpu ( );
	fpu.fpu_control_word.value = fcw;

	uint16_t fsw = context.get_memory<uint16_t> ( base_addr + 2 );
	fpu.fpu_status_word.value = fsw;
	fpu.fpu_top = ( fsw & x86::FSW_TOP_MASK ) >> x86::FSW_TOP_SHIFT;

	uint8_t ftag = context.get_memory<uint8_t> ( base_addr + 4 );
	fpu.fpu_tag_word.value = 0;

	uint32_t mxcsr_val = context.get_memory<uint32_t> ( base_addr + 24 );
	if ( ( mxcsr_val >> 16 ) != 0 ) {
		// !TODO(exception)
		return;
	}
	context.get_mxcsr ( ) = static_cast< x86::Mxcsr >( mxcsr_val );

	for ( int i = 0; i < 8; ++i ) {
		int phys_idx = fpu.get_fpu_phys_idx ( i );
		uint64_t current_reg_addr = base_addr + 32 + ( i * 16 );
		fpu.fpu_stack [ phys_idx ] = context.read_type_float80_t ( current_reg_addr );
		fpu.set_fpu_tag ( phys_idx, ( ftag >> i ) & 1 ? x86::FPU_TAG_EMPTY :
																fpu.classify_fpu_operand ( fpu.fpu_stack [ phys_idx ] ) );
	}

	for ( auto i = 0u; i < 31u; ++i ) {
		uint128_t xmm_val = context.get_memory<uint128_t> ( base_addr + 160 + ( i * 16 ) );
		context.set_xmm_raw ( static_cast< Register > ( static_cast< int > ( Register::XMM0 ) + i ), xmm_val );
	}
}

/// HLT-Halt
/// Halts the processor until the next interrupt or reset.
void handlers::hlt ( const iced::Instruction& instr, KUBERA& context ) {
	if ( context.get_cpl ( ) == 0 ) {
		// !TODO(halt)
	}
	else {
		// !TODO(exception)
	}
}

/// STMXCSR-Store MXCSR Register
/// Stores the MXCSR register to a 32-bit memory location specified by the destination operand.
void handlers::stmxcsr ( const iced::Instruction& instr, KUBERA& context ) {
	const uint64_t dest_addr = helpers::calculate_mem_addr( instr, context );
	const size_t op_size = instr.op0_size ( );
	if ( op_size != 4 ) {
		// !TODO(exception)
		return;
	}
	context.set_memory<uint32_t> ( dest_addr, context.get_mxcsr ( ).value );
}

/// LDMXCSR-Load MXCSR Register
/// Loads the MXCSR register from a 32-bit memory location specified by the source operand.
void handlers::ldmxcsr ( const iced::Instruction& instr, KUBERA& context ) {
	const uint64_t src_addr = helpers::calculate_mem_addr( instr, context );
	const size_t op_size = instr.op0_size ( );
	if ( op_size != 4 ) {
		// !TODO(exception)
		return;
	}
	const uint32_t mxcsr_val = context.get_memory<uint32_t> ( src_addr );
	if ( ( mxcsr_val >> 16 ) != 0 ) {
		// !TODO(exception)
		return;
	}
	context.get_mxcsr ( ) = x86::Mxcsr { .value = mxcsr_val };
}

/// XGETBV-Get Value of Extended Control Register
/// Reads the specified extended control register (XCR) into EDX:EAX (high:low 32 bits).
void handlers::xgetbv ( const iced::Instruction& instr, KUBERA& context ) {
	// !TODO(implement)
	throw std::runtime_error ( "xgetbv not implemented!" );
	//const uint32_t ecx_in = context.get_reg_internal<KubRegister::RCX, Register::ECX, uint32_t> ( );
	//const uint64_t xcr_val = _xgetbv ( ecx_in );
	//const uint32_t eax_out = static_cast< uint32_t >( xcr_val & 0xFFFFFFFF );
	//const uint32_t edx_out = static_cast< uint32_t >( xcr_val >> 32 );
	//context.set_reg_internal<KubRegister::RAX, Register::EAX> ( eax_out );
	//context.set_reg_internal<KubRegister::RDX, Register::EDX> ( edx_out );
}

/// CPUID-CPU Identification
/// Returns processor identification and feature information in RAX, RBX, RCX, and RDX based on the input in RAX and RCX.
void handlers::cpuid ( const iced::Instruction& instr, KUBERA& context ) {
	const uint32_t eax_in = context.get_reg_internal<KubRegister::RAX, Register::RAX, uint32_t> ( );
	const uint32_t ecx_in = context.get_reg_internal<KubRegister::RCX, Register::RCX, uint32_t> ( );

	// !TODO(isolation)
	// !TODO(PRIORITY)
	std::array<int, 4> cpu_info;
#if defined(__x86_64__) || defined(__i386__) || defined(_M_X64)
#ifdef _MSC_VER
	__cpuidex ( cpu_info.data ( ), eax_in, ecx_in );
#else
	__asm__ __volatile__ (
			"cpuid"
			: "=a"( cpu_info [ 0 ] ), "=b"( cpu_info [ 1 ] ), "=c"( cpu_info [ 2 ] ), "=d"( cpu_info [ 3 ] )
			: "a"( eax_in ), "c"( ecx_in )
	);
#endif
#else
	throw std::runtime_error ( "CPUID emulation not available for non-x86 platforms" );
#endif

	context.set_reg_internal<KubRegister::RAX, Register::RAX> ( cpu_info [ 0 ] );
	context.set_reg_internal<KubRegister::RBX, Register::RBX> ( cpu_info [ 1 ] );
	context.set_reg_internal<KubRegister::RCX, Register::RCX> ( cpu_info [ 2 ] );
	context.set_reg_internal<KubRegister::RDX, Register::RDX> ( cpu_info [ 3 ] );
}

```

`src/kubera.cpp`:

```cpp
#include "../KUBERA.hpp"
#include "../emulator.hpp"
#include <print>

using namespace kubera;
std::array<KubRegister, static_cast< std::size_t > ( Register::DontUse0 )> reg_map;
std::array<int, static_cast< std::size_t > ( Register::DontUse0 )> avx_map {};

KubRegister map_register ( Register reg ) {
	return reg_map [ static_cast< size_t >( reg ) ];
}

void kubera::KUBERA::handle_ip_switch ( uint64_t target ) {
	if ( !memory->check ( target, 1, PageProtection::EXEC ) ) {
		return;
	}

	this->rip ( ) = target;
}

uint64_t KUBERA::get_access_mask ( Register reg, size_t size ) const noexcept {
	switch ( size ) {
		case 8:
		{
			return 0xFFFFFFFFFFFFFFFFULL;
		}
		case 4:
		{
			return 0x00000000FFFFFFFFULL;
		}
		case 2:
		{
			return 0x000000000000FFFFULL;
		}
		case 1:
		{
			if ( reg == Register::CH || reg == Register::DH || reg == Register::BH || reg == Register::AH ) {
				return 0x000000000000FF00ULL;
			}
			return 0x00000000000000FFULL;
		}
		default: return 0x0000000000000000ULL;
	}
}

uint8_t KUBERA::get_access_shift ( Register reg, size_t size ) const noexcept {
	if ( size != 1 ) {
		return 0;
	}
	if ( reg == Register::AH || reg == Register::BH || reg == Register::CH || reg == Register::DH ) {
		return 8;
	}

	return 0;
}

uint64_t KUBERA::get_rflags ( ) const noexcept {
	return cpu->rflags.value;
}

uint64_t KUBERA::get_reg ( Register reg, size_t size ) const noexcept {
	if ( reg == Register::RIP ) {
		return rip ( );
	}

	const auto full_reg = map_register ( reg );
	const auto concrete_full = cpu->registers [ full_reg ];
	const auto access_mask = get_access_mask ( reg, size );
	const auto shift = get_access_shift ( reg, size );
	const auto extracted_value = ( concrete_full & access_mask ) >> shift;

	return extracted_value;
}

void KUBERA::set_rflags ( uint64_t rflags ) noexcept {
	auto& flags = cpu->rflags;
	auto old_flags = flags.value;

	flags.CF = ( rflags >> 0 ) & 1;
	flags.PF = ( rflags >> 2 ) & 1;
	flags.AF = ( rflags >> 4 ) & 1;
	flags.ZF = ( rflags >> 6 ) & 1;
	flags.SF = ( rflags >> 7 ) & 1;
	flags.TF = ( rflags >> 8 ) & 1;
	flags.DF = ( rflags >> 10 ) & 1;
	flags.OF = ( rflags >> 11 ) & 1;
	flags.AC = ( rflags >> 18 ) & 1;

	if ( cpu->current_privilege_level == 0 ) {
		if ( cpu->current_privilege_level <= flags.IOPL ) {
			uint64_t old_IF = flags.IF;
			flags.IF = ( rflags >> 9 ) & 1;
		}

		flags.IOPL = ( rflags >> 12 ) & 3;
		flags.NT = ( rflags >> 14 ) & 1;
		flags.RF = ( rflags >> 16 ) & 1;
		flags.VM = ( rflags >> 17 ) & 1;
		flags.VIF = ( rflags >> 19 ) & 1;
		flags.VIP = ( rflags >> 20 ) & 1;
	}
}

void KUBERA::set_reg(Register reg, uint64_t value_to_set, size_t size) {
    const auto full_reg = map_register(reg);

    if (size == 4 && (full_reg >= KubRegister::RAX && full_reg <= KubRegister::R15)) {
        // 32-bit operations zero the upper 32 bits of the destination register.
        cpu->registers[full_reg] = value_to_set & 0xFFFFFFFFULL;
    } else if (size == 8) {
        // 64-bit operations overwrite the entire register.
        cpu->registers[full_reg] = value_to_set;
    }
    else {
        // 8-bit and 16-bit operations merge with the destination.
        const auto old_full_concrete = cpu->registers[full_reg];
        const auto access_mask = get_access_mask(reg, size);
        const auto shift = get_access_shift(reg, size);
        const uint64_t size_mask = GET_OPERAND_MASK(size);

        const uint64_t shifted_value = (value_to_set & size_mask) << shift;
        const uint64_t new_full_concrete = (old_full_concrete & ~access_mask) | (shifted_value & access_mask);

        cpu->registers[full_reg] = new_full_concrete;
    }
}

bool KUBERA::is_within_stack_bounds ( uint64_t address, size_t size ) const noexcept {
	const auto stack_base_addr = cpu->stack_base;
	const auto stack_top = stack_base_addr + cpu->stack_size;
	const auto stack_bot = stack_base_addr;
	return address >= stack_bot && address <= stack_top - size;
}

void map_gpr ( ) {
	reg_map [ ( size_t ) Register::RAX ] = KubRegister::RAX;
	reg_map [ ( size_t ) Register::EAX ] = KubRegister::RAX;
	reg_map [ ( size_t ) Register::AX ] = KubRegister::RAX;
	reg_map [ ( size_t ) Register::AH ] = KubRegister::RAX;
	reg_map [ ( size_t ) Register::AL ] = KubRegister::RAX;
	reg_map [ ( size_t ) Register::RBX ] = KubRegister::RBX;
	reg_map [ ( size_t ) Register::EBX ] = KubRegister::RBX;
	reg_map [ ( size_t ) Register::BX ] = KubRegister::RBX;
	reg_map [ ( size_t ) Register::BH ] = KubRegister::RBX;
	reg_map [ ( size_t ) Register::BL ] = KubRegister::RBX;
	reg_map [ ( size_t ) Register::RCX ] = KubRegister::RCX;
	reg_map [ ( size_t ) Register::ECX ] = KubRegister::RCX;
	reg_map [ ( size_t ) Register::CX ] = KubRegister::RCX;
	reg_map [ ( size_t ) Register::CH ] = KubRegister::RCX;
	reg_map [ ( size_t ) Register::CL ] = KubRegister::RCX;
	reg_map [ ( size_t ) Register::RDX ] = KubRegister::RDX;
	reg_map [ ( size_t ) Register::EDX ] = KubRegister::RDX;
	reg_map [ ( size_t ) Register::DX ] = KubRegister::RDX;
	reg_map [ ( size_t ) Register::DH ] = KubRegister::RDX;
	reg_map [ ( size_t ) Register::DL ] = KubRegister::RDX;
	reg_map [ ( size_t ) Register::RSI ] = KubRegister::RSI;
	reg_map [ ( size_t ) Register::ESI ] = KubRegister::RSI;
	reg_map [ ( size_t ) Register::SI ] = KubRegister::RSI;
	reg_map [ ( size_t ) Register::SIL ] = KubRegister::RSI;
	reg_map [ ( size_t ) Register::RDI ] = KubRegister::RDI;
	reg_map [ ( size_t ) Register::EDI ] = KubRegister::RDI;
	reg_map [ ( size_t ) Register::DI ] = KubRegister::RDI;
	reg_map [ ( size_t ) Register::DIL ] = KubRegister::RDI;
	reg_map [ ( size_t ) Register::RBP ] = KubRegister::RBP;
	reg_map [ ( size_t ) Register::EBP ] = KubRegister::RBP;
	reg_map [ ( size_t ) Register::BP ] = KubRegister::RBP;
	reg_map [ ( size_t ) Register::BPL ] = KubRegister::RBP;
	reg_map [ ( size_t ) Register::RSP ] = KubRegister::RSP;
	reg_map [ ( size_t ) Register::ESP ] = KubRegister::RSP;
	reg_map [ ( size_t ) Register::SP ] = KubRegister::RSP;
	reg_map [ ( size_t ) Register::SPL ] = KubRegister::RSP;
	reg_map [ ( size_t ) Register::R8 ] = KubRegister::R8;
	reg_map [ ( size_t ) Register::R8D ] = KubRegister::R8;
	reg_map [ ( size_t ) Register::R8W ] = KubRegister::R8;
	reg_map [ ( size_t ) Register::R8L ] = KubRegister::R8;
	reg_map [ ( size_t ) Register::R9 ] = KubRegister::R9;
	reg_map [ ( size_t ) Register::R9D ] = KubRegister::R9;
	reg_map [ ( size_t ) Register::R9W ] = KubRegister::R9;
	reg_map [ ( size_t ) Register::R9L ] = KubRegister::R9;
	reg_map [ ( size_t ) Register::R10 ] = KubRegister::R10;
	reg_map [ ( size_t ) Register::R10D ] = KubRegister::R10;
	reg_map [ ( size_t ) Register::R10W ] = KubRegister::R10;
	reg_map [ ( size_t ) Register::R10L ] = KubRegister::R10;
	reg_map [ ( size_t ) Register::R11 ] = KubRegister::R11;
	reg_map [ ( size_t ) Register::R11D ] = KubRegister::R11;
	reg_map [ ( size_t ) Register::R11W ] = KubRegister::R11;
	reg_map [ ( size_t ) Register::R11L ] = KubRegister::R11;
	reg_map [ ( size_t ) Register::R12 ] = KubRegister::R12;
	reg_map [ ( size_t ) Register::R12D ] = KubRegister::R12;
	reg_map [ ( size_t ) Register::R12W ] = KubRegister::R12;
	reg_map [ ( size_t ) Register::R12L ] = KubRegister::R12;
	reg_map [ ( size_t ) Register::R13 ] = KubRegister::R13;
	reg_map [ ( size_t ) Register::R13D ] = KubRegister::R13;
	reg_map [ ( size_t ) Register::R13W ] = KubRegister::R13;
	reg_map [ ( size_t ) Register::R13L ] = KubRegister::R13;
	reg_map [ ( size_t ) Register::R14 ] = KubRegister::R14;
	reg_map [ ( size_t ) Register::R14D ] = KubRegister::R14;
	reg_map [ ( size_t ) Register::R14W ] = KubRegister::R14;
	reg_map [ ( size_t ) Register::R14L ] = KubRegister::R14;
	reg_map [ ( size_t ) Register::R15 ] = KubRegister::R15;
	reg_map [ ( size_t ) Register::R15D ] = KubRegister::R15;
	reg_map [ ( size_t ) Register::R15W ] = KubRegister::R15;
	reg_map [ ( size_t ) Register::R15L ] = KubRegister::R15;
	reg_map [ ( size_t ) Register::RIP ] = KubRegister::RIP;
	reg_map [ ( size_t ) Register::EIP ] = KubRegister::RIP;
	reg_map [ ( size_t ) Register::DR0 ] = KubRegister::DR0;
	reg_map [ ( size_t ) Register::DR1 ] = KubRegister::DR1;
	reg_map [ ( size_t ) Register::DR2 ] = KubRegister::DR2;
	reg_map [ ( size_t ) Register::DR3 ] = KubRegister::DR3;
	reg_map [ ( size_t ) Register::DR4 ] = KubRegister::DR4;
	reg_map [ ( size_t ) Register::DR5 ] = KubRegister::DR5;
	reg_map [ ( size_t ) Register::DR6 ] = KubRegister::DR6;
	reg_map [ ( size_t ) Register::DR7 ] = KubRegister::DR7;
	reg_map [ ( size_t ) Register::CR0 ] = KubRegister::CR0;
	reg_map [ ( size_t ) Register::CR2 ] = KubRegister::CR2;
	reg_map [ ( size_t ) Register::CR3 ] = KubRegister::CR3;
	reg_map [ ( size_t ) Register::CR4 ] = KubRegister::CR4;
	reg_map [ ( size_t ) Register::CR8 ] = KubRegister::CR8;
	reg_map [ ( size_t ) Register::CS ] = KubRegister::CS;
	reg_map [ ( size_t ) Register::DS ] = KubRegister::DS;
	reg_map [ ( size_t ) Register::ES ] = KubRegister::ES;
	reg_map [ ( size_t ) Register::FS ] = KubRegister::FS;
	reg_map [ ( size_t ) Register::GS ] = KubRegister::GS;
	reg_map [ ( size_t ) Register::SS ] = KubRegister::SS;
}

void map_avx ( ) {
	std::array<Register, 3> bases { Register::XMM0, Register::YMM0, Register::ZMM0 };
	for ( const auto base : bases ) {
		for ( auto i = 0u; i < 31; ++i ) {
			avx_map [ static_cast< size_t > ( base ) + i ] = i;
		}
	}
};

inline int countl_zero_u64 ( uint64_t val ) {
	unsigned long leading_zero;
#ifdef _MSC_VER
	if ( _BitScanReverse64 ( &leading_zero, val ) )
		return 63 - leading_zero;
	else
		return 64;
#else
	return __builtin_clzll ( val );
#endif
}

float80_t from_ieee754_80 ( std::pair<uint64_t, uint16_t> bits ) {
	const uint64_t mantissa = bits.first;
	const uint16_t sign_exp = bits.second;

	const bool sign = ( sign_exp >> 15 ) & 1;
	const uint16_t biased_exp = sign_exp & 0x7FFF;
	const int16_t exponent_bias = 16383;

	float80_t result;
	// Get a reference to the backend to manipulate its internal state.
	auto& backend = result.backend ( );

	// Case 1: NaN or Infinity (exponent is all 1s)
	if ( biased_exp == 0x7FFF ) {
		// According to Intel's spec, I=1, F=0 is Infinity. Other combinations are NaNs.
		if ( mantissa == 0x8000000000000000ULL ) { // Infinity
			backend.exponent ( ) = float80_t::backend_type::exponent_infinity;
			backend.sign ( ) = sign;
		}
		else { // NaN
			backend.exponent ( ) = float80_t::backend_type::exponent_nan;
			backend.sign ( ) = false; // NaNs are unsigned in cpp_bin_float
		}
		// For special values, the 'bits' member is 0.
		return result;
	}

	// Case 2: Zero
	if ( biased_exp == 0 && mantissa == 0 ) {
		backend.exponent ( ) = float80_t::backend_type::exponent_zero;
		backend.sign ( ) = sign;
		return result;
	}

	// Case 3: Finite non-zero numbers (Normal, Denormal, Pseudo-denormal)
	backend.sign ( ) = sign;

	const bool is_normal = ( mantissa & 0x8000000000000000ULL ) != 0;

	// Optimization for standard normal numbers, which are already normalized.
	if ( biased_exp > 0 && is_normal ) {
		backend.exponent ( ) = biased_exp - exponent_bias;
		backend.bits ( ) = mantissa;
	}
	else {
		// Denormals (E=0, I=0, F!=0) and Pseudo-denormals (E>0, I=0)
		// require normalization for cpp_bin_float.

		// Determine the effective exponent before normalization.
		// For denormals (E=0), the exponent is fixed at 1 - bias.
		const int16_t effective_exp_base = ( biased_exp == 0 )
			? static_cast< int16_t >( 1 - exponent_bias )
			: static_cast< int16_t >( biased_exp - exponent_bias );

		if ( mantissa == 0 ) { // Should not happen if we already checked for zero
			throw std::logic_error ( "Invalid finite number with zero mantissa." );
		}

		// Find how many bits to shift left to normalize the mantissa.
		const int shift = countl_zero_u64 ( mantissa );

		// Normalize the mantissa by shifting its MSB to bit 63.
		backend.bits ( ) = mantissa << shift;

		// Adjust the exponent to compensate for the shift.
		backend.exponent ( ) = effective_exp_base - shift;
	}

	return result;
}

float80_t from_ieee754_80_bytes ( const uint8_t bytes [ 10 ] ) {
	// Safely construct integers from bytes (assumes little-endian source)
	uint64_t mantissa = 0;
	for ( int i = 7; i >= 0; --i ) {
		mantissa = ( mantissa << 8 ) | bytes [ i ];
	}
	uint16_t sign_exp = ( static_cast< uint16_t >( bytes [ 9 ] ) << 8 ) | bytes [ 8 ];

	return from_ieee754_80 ( { mantissa, sign_exp } );
}

float80_t KUBERA::read_type_float80_t ( uint64_t address ) const {
	uint64_t p0 = memory->read<uint64_t> ( address + 0 );
	uint16_t p1 = memory->read<uint16_t> ( address + 8 );

	alignas( 16 ) uint8_t temp [ 10 ];
	std::memcpy ( temp, &p0, 8 );
	std::memcpy ( temp + 8, &p1, 2 );

	return from_ieee754_80_bytes ( temp );
}

uint128_t KUBERA::get_xmm_raw ( Register _reg ) const {
	auto reg = avx_map [ static_cast< int >( _reg ) ];
	uint512_t value = ( *cpu->sse_registers ) [ reg ];
	return value.convert_to<uint128_t> ( );
}

void KUBERA::set_xmm_raw ( Register _reg, const uint128_t& value ) {
	auto reg = avx_map [ static_cast< int >( _reg ) ];
	( *cpu->sse_registers ) [ reg ] = value;
}

uint256_t KUBERA::get_ymm_raw ( Register _reg ) const {
	auto reg = avx_map [ static_cast< int >( _reg ) ];
	uint512_t value = ( *cpu->sse_registers ) [ reg ];
	return value.convert_to<uint256_t> ( );
}

void KUBERA::set_ymm_raw ( Register _reg, const uint256_t& value ) {
	auto reg = avx_map [ static_cast< int >( _reg ) ];
	( *cpu->sse_registers ) [ reg ] = value;
}

uint512_t KUBERA::get_zmm_raw ( Register _reg ) const {
	auto reg = avx_map [ static_cast< int >( _reg ) ];
	uint512_t value = ( *cpu->sse_registers ) [ reg ];
	return value;
}

void KUBERA::set_zmm_raw ( Register _reg, const uint512_t& value ) {
	auto reg = avx_map [ static_cast< int >( _reg ) ];
	( *cpu->sse_registers ) [ reg ] = value;
}

float KUBERA::get_xmm_float ( Register reg ) const {
	uint128_t raw = get_xmm_raw ( reg );
	uint32_t low_bits = static_cast< uint32_t >( raw & 0xFFFFFFFF );
	return std::bit_cast< float >( low_bits );
}

void KUBERA::set_xmm_float ( Register reg, float value ) {
	uint128_t current_raw = get_xmm_raw ( reg );
	uint32_t new_low_bits = std::bit_cast< uint32_t >( value );
	current_raw = ( current_raw & ~uint128_t ( 0xFFFFFFFF ) ) | uint128_t ( new_low_bits );
	set_xmm_raw ( reg, current_raw );
}

double KUBERA::get_xmm_double ( Register reg ) const {
	uint128_t raw = get_xmm_raw ( reg );
	uint64_t low_bits = static_cast< uint64_t >( raw & 0xFFFFFFFFFFFFFFFF );
	return std::bit_cast< double >( low_bits );
}

void KUBERA::set_xmm_double ( Register reg, double value ) {
	uint128_t current_raw = get_xmm_raw ( reg );
	uint64_t new_low_bits = std::bit_cast< uint64_t >( value );
	current_raw = ( current_raw & ~uint128_t ( 0xFFFFFFFFFFFFFFFF ) ) | uint128_t ( new_low_bits );
	set_xmm_raw ( reg, current_raw );
}

void unsupported_instruction ( const iced::Instruction& instr, KUBERA& context ) {
	std::println ( "[KUBERA] Unsupported instruction {}, skipping.", instr.to_string ( ) );
}

#define SET_HANDLER(x, y) instruction_dispatch_table->at ( static_cast< size_t >( x) ) = y
void map_handlers ( ) {
	SET_HANDLER ( Mnemonic::Add, handlers::add );
	SET_HANDLER ( Mnemonic::Sub, handlers::sub );
	SET_HANDLER ( Mnemonic::Inc, handlers::inc );
	SET_HANDLER ( Mnemonic::Dec, handlers::dec );
	SET_HANDLER ( Mnemonic::Mul, handlers::mul );
	SET_HANDLER ( Mnemonic::Imul, handlers::imul );
	SET_HANDLER ( Mnemonic::Div, handlers::div );
	SET_HANDLER ( Mnemonic::Idiv, handlers::idiv );
	SET_HANDLER ( Mnemonic::Adc, handlers::adc );
	SET_HANDLER ( Mnemonic::Sbb, handlers::sbb );
	SET_HANDLER ( Mnemonic::Neg, handlers::neg );
	SET_HANDLER ( Mnemonic::Xadd, handlers::xadd );
	SET_HANDLER ( Mnemonic::Cdq, handlers::cdq );
	SET_HANDLER ( Mnemonic::Cdqe, handlers::cdqe );
	SET_HANDLER ( Mnemonic::Cwd, handlers::cwd );
	SET_HANDLER ( Mnemonic::Cqo, handlers::cqo );
	SET_HANDLER ( Mnemonic::Cwde, handlers::cwde );
	SET_HANDLER ( Mnemonic::Cbw, handlers::cbw );
	SET_HANDLER ( Mnemonic::And, handlers::and_ );
	SET_HANDLER ( Mnemonic::Or, handlers::or_ );
	SET_HANDLER ( Mnemonic::Xor, handlers::xor_ );
	SET_HANDLER ( Mnemonic::Not, handlers::not_ );
	SET_HANDLER ( Mnemonic::Shl, handlers::shl );
	SET_HANDLER ( Mnemonic::Sal, handlers::sal );
	SET_HANDLER ( Mnemonic::Sar, handlers::sar );
	SET_HANDLER ( Mnemonic::Shr, handlers::shr );
	SET_HANDLER ( Mnemonic::Shld, handlers::shld );
	SET_HANDLER ( Mnemonic::Shrd, handlers::shrd );
	SET_HANDLER ( Mnemonic::Rol, handlers::rol );
	SET_HANDLER ( Mnemonic::Ror, handlers::ror );
	SET_HANDLER ( Mnemonic::Rcl, handlers::rcl );
	SET_HANDLER ( Mnemonic::Rcr, handlers::rcr );
	SET_HANDLER ( Mnemonic::Cmovo, handlers::cmovo );
	SET_HANDLER ( Mnemonic::Cmovb, handlers::cmovb );
	SET_HANDLER ( Mnemonic::Cmovge, handlers::cmovnl );
	SET_HANDLER ( Mnemonic::Cmovbe, handlers::cmovbe );
	SET_HANDLER ( Mnemonic::Cmove, handlers::cmovz );
	SET_HANDLER ( Mnemonic::Cmovle, handlers::cmovle );
	SET_HANDLER ( Mnemonic::Cmovl, handlers::cmovl );
	SET_HANDLER ( Mnemonic::Cmovnp, handlers::cmovnp );
	SET_HANDLER ( Mnemonic::Cmovns, handlers::cmovns );
	SET_HANDLER ( Mnemonic::Cmovp, handlers::cmovp );
	SET_HANDLER ( Mnemonic::Cmovae, handlers::cmovnb );
	SET_HANDLER ( Mnemonic::Cmovno, handlers::cmovno );
	SET_HANDLER ( Mnemonic::Cmovs, handlers::cmovs );
	SET_HANDLER ( Mnemonic::Cmovne, handlers::cmovnz );
	SET_HANDLER ( Mnemonic::Cmova, handlers::cmovnbe );
	SET_HANDLER ( Mnemonic::Cmovg, handlers::cmovnle );

	// Set Based on Condition Instructions
	SET_HANDLER ( Mnemonic::Setb, handlers::setb );
	SET_HANDLER ( Mnemonic::Setnp, handlers::setnp );
	SET_HANDLER ( Mnemonic::Sets, handlers::sets );
	SET_HANDLER ( Mnemonic::Setge, handlers::setnl );
	SET_HANDLER ( Mnemonic::Seto, handlers::seto );
	SET_HANDLER ( Mnemonic::Setbe, handlers::setbe );
	SET_HANDLER ( Mnemonic::Sete, handlers::setz );
	SET_HANDLER ( Mnemonic::Setae, handlers::setnb );
	SET_HANDLER ( Mnemonic::Setno, handlers::setno );
	SET_HANDLER ( Mnemonic::Setp, handlers::setp );
	SET_HANDLER ( Mnemonic::Setle, handlers::setle );
	SET_HANDLER ( Mnemonic::Setg, handlers::setnle );
	SET_HANDLER ( Mnemonic::Setns, handlers::setns );
	SET_HANDLER ( Mnemonic::Setl, handlers::setl );
	SET_HANDLER ( Mnemonic::Seta, handlers::setnbe );
	SET_HANDLER ( Mnemonic::Setne, handlers::setnz );

	// Bit Manipulation Instructions
	SET_HANDLER ( Mnemonic::Bzhi, handlers::bzhi );
	SET_HANDLER ( Mnemonic::Andn, handlers::andn );
	SET_HANDLER ( Mnemonic::Bextr, handlers::bextr );
	SET_HANDLER ( Mnemonic::Popcnt, handlers::popcnt );
	SET_HANDLER ( Mnemonic::Bswap, handlers::bswap );
	SET_HANDLER ( Mnemonic::Bt, handlers::bt );
	SET_HANDLER ( Mnemonic::Bts, handlers::bts );
	SET_HANDLER ( Mnemonic::Btr, handlers::btr );
	SET_HANDLER ( Mnemonic::Btc, handlers::btc );
	SET_HANDLER ( Mnemonic::Bsr, handlers::bsr );
	SET_HANDLER ( Mnemonic::Bsf, handlers::bsf );
	SET_HANDLER ( Mnemonic::Tzcnt, handlers::tzcnt );

	// Comparison and Test Instructions
	SET_HANDLER ( Mnemonic::Cmp, handlers::cmp );
	SET_HANDLER ( Mnemonic::Test, handlers::test );
	SET_HANDLER ( Mnemonic::Cmpxchg, handlers::cmpxchg );
	SET_HANDLER ( Mnemonic::Cmpxchg16b, handlers::cmpxchg16b );

	// Control Flow Instructions
	SET_HANDLER ( Mnemonic::Jmp, handlers::jmp );
	SET_HANDLER ( Mnemonic::Je, handlers::je );
	SET_HANDLER ( Mnemonic::Jne, handlers::jne );
	SET_HANDLER ( Mnemonic::Ja, handlers::jnbe );
	SET_HANDLER ( Mnemonic::Jg, handlers::jg );
	SET_HANDLER ( Mnemonic::Jl, handlers::jl );
	SET_HANDLER ( Mnemonic::Jae, handlers::jnb );
	SET_HANDLER ( Mnemonic::Jb, handlers::jb );
	SET_HANDLER ( Mnemonic::Jns, handlers::jns );
	SET_HANDLER ( Mnemonic::Jo, handlers::jo );
	SET_HANDLER ( Mnemonic::Jno, handlers::jno );
	SET_HANDLER ( Mnemonic::Jbe, handlers::jbe );
	SET_HANDLER ( Mnemonic::Js, handlers::js );
	SET_HANDLER ( Mnemonic::Ja, handlers::ja );
	SET_HANDLER ( Mnemonic::Jae, handlers::jae );
	SET_HANDLER ( Mnemonic::Jge, handlers::jge );
	SET_HANDLER ( Mnemonic::Jle, handlers::jle );
	SET_HANDLER ( Mnemonic::Jp, handlers::jp );
	SET_HANDLER ( Mnemonic::Jnp, handlers::jnp );
	SET_HANDLER ( Mnemonic::Jcxz, handlers::jcxz );
	SET_HANDLER ( Mnemonic::Jecxz, handlers::jecxz );
	SET_HANDLER ( Mnemonic::Jrcxz, handlers::jrcxz );
	SET_HANDLER ( Mnemonic::Call, handlers::call );
	SET_HANDLER ( Mnemonic::Ret, handlers::ret );
	SET_HANDLER ( Mnemonic::Iret, handlers::iret );
	SET_HANDLER ( Mnemonic::Iretd, handlers::iretd );
	SET_HANDLER ( Mnemonic::Iretq, handlers::iretq );

	// Stack and Frame Instructions
	SET_HANDLER ( Mnemonic::Enter, handlers::enter );
	SET_HANDLER ( Mnemonic::Leave, handlers::leave );
	SET_HANDLER ( Mnemonic::Push, handlers::push );
	SET_HANDLER ( Mnemonic::Pop, handlers::pop );
	SET_HANDLER ( Mnemonic::Pushfq, handlers::pushfq );
	SET_HANDLER ( Mnemonic::Popfq, handlers::popfq );

	// System Instructions
	SET_HANDLER ( Mnemonic::Cli, handlers::cli );
	SET_HANDLER ( Mnemonic::Cld, handlers::cld );
	SET_HANDLER ( Mnemonic::Clc, handlers::clc );
	SET_HANDLER ( Mnemonic::Clui, handlers::clui );
	SET_HANDLER ( Mnemonic::Cmc, handlers::cmc );
	SET_HANDLER ( Mnemonic::Stc, handlers::stc );
	SET_HANDLER ( Mnemonic::Sti, handlers::sti );
	SET_HANDLER ( Mnemonic::Std, handlers::std );
	SET_HANDLER ( Mnemonic::Rdtsc, handlers::rdtsc );
	SET_HANDLER ( Mnemonic::Cpuid, handlers::cpuid );
	SET_HANDLER ( Mnemonic::Xgetbv, handlers::xgetbv );
	SET_HANDLER ( Mnemonic::Hlt, handlers::hlt );
	SET_HANDLER ( Mnemonic::Int1, handlers::int1 );
	SET_HANDLER ( Mnemonic::Int3, handlers::int3 );
	SET_HANDLER ( Mnemonic::Int, handlers::int_ );
	SET_HANDLER ( Mnemonic::Fxsave, handlers::fxsave );
	SET_HANDLER ( Mnemonic::Fxrstor, handlers::fxrstor );
	SET_HANDLER ( Mnemonic::Stmxcsr, handlers::stmxcsr );
	SET_HANDLER ( Mnemonic::Ldmxcsr, handlers::ldmxcsr );
	SET_HANDLER ( Mnemonic::Sahf, handlers::sahf );
	SET_HANDLER ( Mnemonic::Lahf, handlers::lahf );
	SET_HANDLER ( Mnemonic::Pushf, handlers::pushf );
	SET_HANDLER ( Mnemonic::Popf, handlers::popf );
	SET_HANDLER ( Mnemonic::Syscall, handlers::syscall );

	// Data Movement Instructions
	SET_HANDLER ( Mnemonic::Mov, handlers::mov );
	SET_HANDLER ( Mnemonic::Movd, handlers::movd );
	SET_HANDLER ( Mnemonic::Movq, handlers::movq );
	SET_HANDLER ( Mnemonic::Movsxd, handlers::movsxd );
	SET_HANDLER ( Mnemonic::Movzx, handlers::movzx );
	SET_HANDLER ( Mnemonic::Movsx, handlers::movsx );
	SET_HANDLER ( Mnemonic::Movaps, handlers::movaps );
	SET_HANDLER ( Mnemonic::Movups, handlers::movups );
	SET_HANDLER ( Mnemonic::Lea, handlers::lea );
	SET_HANDLER ( Mnemonic::Xchg, handlers::xchg );

	// String Operations
	SET_HANDLER ( Mnemonic::Movsw, handlers::movsw );
	SET_HANDLER ( Mnemonic::Movsb, handlers::movsb );
	SET_HANDLER ( Mnemonic::Movsd, handlers::movsd );
	SET_HANDLER ( Mnemonic::Movsq, handlers::movsq );
	SET_HANDLER ( Mnemonic::Stosq, handlers::stos );
	SET_HANDLER ( Mnemonic::Stosd, handlers::stos );
	SET_HANDLER ( Mnemonic::Stosb, handlers::stos );
	SET_HANDLER ( Mnemonic::Stosw, handlers::stos );

	// SIMD Instructions
	SET_HANDLER ( Mnemonic::Vpxor, handlers::vpxor );
	SET_HANDLER ( Mnemonic::Vpcmpeqw, handlers::vpcmpeqw );
	SET_HANDLER ( Mnemonic::Vpmovmskb, handlers::vpmovmskb );
	SET_HANDLER ( Mnemonic::Vzeroupper, handlers::vzeroupper );
	SET_HANDLER ( Mnemonic::Vinsertf128, handlers::vinsertf128 );
	SET_HANDLER ( Mnemonic::Vmovups, handlers::vmovups );
	SET_HANDLER ( Mnemonic::Vmovaps, handlers::vmovaps );
	SET_HANDLER ( Mnemonic::Vmovdqu, handlers::vmovdqu );
	SET_HANDLER ( Mnemonic::Movdqu, handlers::movdqu );
	SET_HANDLER ( Mnemonic::Movlhps, handlers::movlhps );
	SET_HANDLER ( Mnemonic::Punpcklqdq, handlers::punpcklqdq );
	SET_HANDLER ( Mnemonic::Prefetchw, handlers::prefetchw );
	SET_HANDLER ( Mnemonic::Psrldq, handlers::psrldq );
	SET_HANDLER ( Mnemonic::Movhlps, handlers::movhlps );
	SET_HANDLER ( Mnemonic::Unpcklps, handlers::unpcklps );
	SET_HANDLER ( Mnemonic::Pinsrb, handlers::pinsrb );
	SET_HANDLER ( Mnemonic::Pinsrd, handlers::pinsrd );
	SET_HANDLER ( Mnemonic::Pinsrq, handlers::pinsrq );
	SET_HANDLER ( Mnemonic::Paddb, handlers::paddb );
	SET_HANDLER ( Mnemonic::Paddw, handlers::paddw );
	SET_HANDLER ( Mnemonic::Paddd, handlers::paddd );
	SET_HANDLER ( Mnemonic::Paddq, handlers::paddq );

	// Floating-Point Instructions
	SET_HANDLER ( Mnemonic::Addss, handlers::addss );
	SET_HANDLER ( Mnemonic::Subss, handlers::subss );
	SET_HANDLER ( Mnemonic::Mulss, handlers::mulss );
	SET_HANDLER ( Mnemonic::Divss, handlers::divss );
	SET_HANDLER ( Mnemonic::Minss, handlers::minss );
	SET_HANDLER ( Mnemonic::Maxss, handlers::maxss );
	SET_HANDLER ( Mnemonic::Andps, handlers::andps );
	SET_HANDLER ( Mnemonic::Orps, handlers::orps );
	SET_HANDLER ( Mnemonic::Xorps, handlers::xorps );
	SET_HANDLER ( Mnemonic::Sqrtss, handlers::sqrtss );
	SET_HANDLER ( Mnemonic::Sqrtsd, handlers::sqrtsd );
	SET_HANDLER ( Mnemonic::Comiss, handlers::comiss );
	SET_HANDLER ( Mnemonic::Ucomiss, handlers::ucomiss );
	SET_HANDLER ( Mnemonic::Comisd, handlers::comisd );
	SET_HANDLER ( Mnemonic::Cmpss, handlers::cmpss );
	SET_HANDLER ( Mnemonic::Cvtss2si, handlers::cvtss2si );
	SET_HANDLER ( Mnemonic::Cvttss2si, handlers::cvttss2si );
	SET_HANDLER ( Mnemonic::Cvtsi2ss, handlers::cvtsi2ss );
	SET_HANDLER ( Mnemonic::Cvtsi2sd, handlers::cvtsi2sd );
	SET_HANDLER ( Mnemonic::Cvtss2sd, handlers::cvtss2sd );
	SET_HANDLER ( Mnemonic::Cvtsd2ss, handlers::cvtsd2ss );
	SET_HANDLER ( Mnemonic::Roundss, handlers::roundss );
	SET_HANDLER ( Mnemonic::Rcpss, handlers::rcpss );
	SET_HANDLER ( Mnemonic::Rsqrtss, handlers::rsqrtss );
	SET_HANDLER ( Mnemonic::Mulsd, handlers::mulsd );
	SET_HANDLER ( Mnemonic::Movss, handlers::movss );

	// 80-bit Floating-Point Instructions
	SET_HANDLER ( Mnemonic::Fld, handlers::fld );
	SET_HANDLER ( Mnemonic::Fprem, handlers::fprem );
	SET_HANDLER ( Mnemonic::Fstp, handlers::fstp );
	SET_HANDLER ( Mnemonic::Ffree, handlers::ffree );
	SET_HANDLER ( Mnemonic::Fincstp, handlers::fincstp );
	SET_HANDLER ( Mnemonic::Fmul, handlers::fmul );
	SET_HANDLER ( Mnemonic::Fnstcw, handlers::fnstcw );

	// Miscellaneous Instructions
	SET_HANDLER ( Mnemonic::Nop, handlers::nop );
}

KUBERA::KUBERA ( ) {
	if ( !memory ) {
		memory = std::make_unique<VirtualMemory> ( );
	}
	const uint64_t stack_addr = memory->alloc_at ( 0xDEADBEEF00000000, 0x200000, PageProtection::READ | PageProtection::WRITE );
	cpu = std::make_unique<CPU> ( stack_addr, 0x200000 );
	decoder = std::make_unique<iced::Decoder> ( );
	instruction_dispatch_table = std::make_unique<InstructionHandlerList> ();
	map_avx ( );
	map_gpr ( );
	instruction_dispatch_table->fill ( unsupported_instruction );
	map_handlers ( );

	set_reg_internal<KubRegister::RSP, Register::RSP> ( stack_addr + cpu->stack_size );
}


std::vector<std::string> KUBERA::get_register_changes ( const std::array<std::uint64_t, KubRegister::COUNT>& old_registers ) const {
	std::vector<std::string> changes;
	for ( size_t i = 0; i < KubRegister::COUNT; ++i ) {
		if ( cpu->registers [ i ] != old_registers [ i ] && static_cast< KubRegister > ( i ) != KubRegister::RIP ) {
			std::stringstream ss;
			ss << register_names [ i ] << " 0x" << std::hex << old_registers [ i ] << ";0x" << std::hex << cpu->registers [ i ];
			changes.push_back ( ss.str ( ) );
		}
	}
	return changes;
}

std::vector<std::string> KUBERA::get_rflags_changes ( const x86::Flags& old_rflags ) const {
	std::vector<std::string> changes;
	auto add_change = [ &changes ] ( const std::string& name, uint64_t old_val, uint64_t new_val )
	{
		if ( old_val != new_val ) {
			changes.push_back ( name + " " + std::to_string ( old_val ) + ";" + std::to_string ( new_val ) );
		}
	};
	add_change ( "CF", old_rflags.CF, cpu->rflags.CF );
	add_change ( "PF", old_rflags.PF, cpu->rflags.PF );
	add_change ( "AF", old_rflags.AF, cpu->rflags.AF );
	add_change ( "ZF", old_rflags.ZF, cpu->rflags.ZF );
	add_change ( "SF", old_rflags.SF, cpu->rflags.SF );
	add_change ( "TF", old_rflags.TF, cpu->rflags.TF );
	add_change ( "IF", old_rflags.IF, cpu->rflags.IF );
	add_change ( "DF", old_rflags.DF, cpu->rflags.DF );
	add_change ( "OF", old_rflags.OF, cpu->rflags.OF );
	add_change ( "IOPL", old_rflags.IOPL, cpu->rflags.IOPL );
	add_change ( "NT", old_rflags.NT, cpu->rflags.NT );
	add_change ( "RF", old_rflags.RF, cpu->rflags.RF );
	add_change ( "VM", old_rflags.VM, cpu->rflags.VM );
	add_change ( "AC", old_rflags.AC, cpu->rflags.AC );
	add_change ( "VIF", old_rflags.VIF, cpu->rflags.VIF );
	add_change ( "VIP", old_rflags.VIP, cpu->rflags.VIP );
	add_change ( "ID", old_rflags.ID, cpu->rflags.ID );
	return changes;
}

std::vector<std::string> KUBERA::get_mxcsr_changes ( const x86::Mxcsr& old_mxcsr ) const {
	std::vector<std::string> changes;
	auto add_change = [ &changes ] ( const std::string& name, unsigned int old_val, unsigned int new_val )
	{
		if ( old_val != new_val ) {
			changes.push_back ( name + " " + std::to_string ( old_val ) + ";" + std::to_string ( new_val ) );
		}
	};
	add_change ( "IE", old_mxcsr.IE, cpu->mxcsr.IE );
	add_change ( "DE", old_mxcsr.DE, cpu->mxcsr.DE );
	add_change ( "ZE", old_mxcsr.ZE, cpu->mxcsr.ZE );
	add_change ( "OE", old_mxcsr.OE, cpu->mxcsr.OE );
	add_change ( "UE", old_mxcsr.UE, cpu->mxcsr.UE );
	add_change ( "PE", old_mxcsr.PE, cpu->mxcsr.PE );
	add_change ( "DAZ", old_mxcsr.DAZ, cpu->mxcsr.DAZ );
	add_change ( "IM", old_mxcsr.IM, cpu->mxcsr.IM );
	add_change ( "DM", old_mxcsr.DM, cpu->mxcsr.DM );
	add_change ( "ZM", old_mxcsr.ZM, cpu->mxcsr.ZM );
	add_change ( "OM", old_mxcsr.OM, cpu->mxcsr.OM );
	add_change ( "UM", old_mxcsr.UM, cpu->mxcsr.UM );
	add_change ( "PM", old_mxcsr.PM, cpu->mxcsr.PM );
	add_change ( "RC", old_mxcsr.RC, cpu->mxcsr.RC );
	add_change ( "FTZ", old_mxcsr.FTZ, cpu->mxcsr.FTZ );
	return changes;
}

template void KUBERA::write_type<uint512_t> ( uint64_t, uint512_t );
template void KUBERA::write_type<uint256_t> ( uint64_t, uint256_t );
template void KUBERA::write_type<uint128_t> ( uint64_t, uint128_t );
template void KUBERA::write_type<float80_t> ( uint64_t, float80_t );
template void KUBERA::write_type<uint64_t> ( uint64_t, uint64_t );
template void KUBERA::write_type<uint32_t> ( uint64_t, uint32_t );
template void KUBERA::write_type<uint16_t> ( uint64_t, uint16_t );
template void KUBERA::write_type<uint8_t> ( uint64_t, uint8_t );

template uint512_t KUBERA::read_type<uint512_t> ( uint64_t ) const;
template uint256_t KUBERA::read_type<uint256_t> ( uint64_t ) const;
template uint128_t KUBERA::read_type<uint128_t> ( uint64_t ) const;
template uint64_t KUBERA::read_type<uint64_t> ( uint64_t ) const;
template uint32_t KUBERA::read_type<uint32_t> ( uint64_t ) const;
template uint16_t KUBERA::read_type<uint16_t> ( uint64_t ) const;
template uint8_t KUBERA::read_type<uint8_t> ( uint64_t ) const;

```

`src/virtual_memory.cpp`:

```cpp
#include "../memory.hpp"
#include <memory>
namespace kubera
{
	VirtualMemory::VirtualMemory ( std::size_t ps ) : page_size ( ps ) {
		for ( auto& c : cache ) {
			c.virt = UINT64_MAX;
		}
	}

	VirtualMemory::~VirtualMemory ( ) {
		for ( auto& [v, p] : pages ) {
			if ( p->data ) {
			#ifdef _MSC_VER
				_aligned_free ( p->data );
			#else
				std::free ( p->data );
			#endif
			}
		}
	}

	void VirtualMemory::set_read_hook ( uint64_t addr, std::function<void ( VirtualMemory* vm, uint64_t addr, std::size_t size )> hook ) {
		auto* region = const_cast< Region* >( find_region ( addr ) );
		if ( region ) {
			region->read_hook = std::move ( hook );
		}
	}

	uint8_t* VirtualMemory::commit ( std::size_t size ) {
	#ifdef _MSC_VER
		return reinterpret_cast< uint8_t* >( _aligned_malloc ( size, PAGE_ALIGN ) );
	#else
		return reinterpret_cast< uint8_t* >( std::aligned_alloc ( size, PAGE_ALIGN ) );
	#endif
	}

	void VirtualMemory::uncommit ( uint8_t* data ) {
		if ( data ) {
		#ifdef _MSC_VER
			_aligned_free ( data );
		#else
			std::free ( data );
		#endif
		}
	}

	uint64_t VirtualMemory::alloc ( std::size_t size, uint8_t prot, std::size_t alignment, bool commit_immediately ) {
		uint64_t base = ( next_alloc + alignment - 1 ) & ~( alignment - 1 );
		std::size_t pages_needed = ( size + page_size - 1 ) / page_size;
		Region region { base, pages_needed * page_size, prot, prot };
		regions [ base ] = region;
		for ( std::size_t i = 0; i < pages_needed; i++ ) {
			uint64_t virt = base + i * page_size;
			pages [ virt ] = std::make_unique<Page> ( );
			pages [ virt ]->prot = prot;
			pages [ virt ]->region_base = base;
			if ( commit_immediately ) {
				pages [ virt ]->data = commit ( page_size );
				if ( !pages [ virt ]->data ) {
					if constexpr ( verbose_memory ) {
						std::println ( "Failed to commit memory for page at address {:#x}", virt );
					}

					for ( std::size_t j = 0; j <= i; j++ ) {
						pages.erase ( base + j * page_size );
					}
					regions.erase ( base );
					return 0;
				}
				std::memset ( pages [ virt ]->data, 0, page_size );
				pages [ virt ]->present = true;
			}
		}
		next_alloc = base + pages_needed * page_size;
		return base;
	}

	uint64_t VirtualMemory::alloc_at ( uint64_t base_addr, std::size_t size, uint8_t prot, std::size_t alignment, bool commit_immediately ) {
		uint64_t base = ( base_addr + alignment - 1 ) & ~( alignment - 1 );
		std::size_t pages_needed = ( size + page_size - 1 ) / page_size;
		uint64_t region_end = base + pages_needed * page_size;

		while ( true ) {
			auto it = regions.lower_bound ( base );
			if ( it != regions.begin ( ) ) --it;
			bool overlap = false;
			while ( it != regions.end ( ) && it->first < region_end ) {
				if ( it->first + it->second.size > base ) {
					if constexpr ( verbose_memory ) {
						std::println ( "Warning: Requested region at {:#x} overlaps with existing region at {:#x} (size {:#x}). Trying next available address.",
												 base, it->first, it->second.size );
					}
					overlap = true;
					base = ( it->first + it->second.size + alignment - 1 ) & ~( alignment - 1 );
					region_end = base + pages_needed * page_size;
					it = regions.lower_bound ( base );
					if ( it != regions.begin ( ) ) --it;
					continue;
				}
				++it;
			}

			if ( !overlap ) {
				break;
			}
		}

		Region region { base, pages_needed * page_size, prot, prot };
		regions [ base ] = region;

		for ( std::size_t i = 0; i < pages_needed; i++ ) {
			uint64_t virt = base + i * page_size;
			if ( pages.find ( virt ) != pages.end ( ) ) {
				if constexpr ( verbose_memory ) {
					std::println ( "Warning: Page at address {:#x} already exists. Allocation failed.", virt );
				}
				regions.erase ( base );
				for ( std::size_t j = 0; j < i; j++ ) {
					pages.erase ( base + j * page_size );
				}
				return 0;
			}
			pages [ virt ] = std::make_unique<Page> ( );
			pages [ virt ]->prot = prot;
			pages [ virt ]->region_base = base;
			if ( commit_immediately ) {
				pages [ virt ]->data = commit ( page_size );
				if ( !pages [ virt ]->data ) {
					if constexpr ( verbose_memory ) {
						std::println ( "Failed to commit memory for page at address {:#x}", virt );
					}
					for ( std::size_t j = 0; j <= i; j++ ) {
						pages.erase ( base + j * page_size );
					}
					regions.erase ( base );
					return 0;
				}
				std::memset ( pages [ virt ]->data, 0, page_size );
				pages [ virt ]->present = true;
			}
		}

		return base;
	}

	uint64_t VirtualMemory::load ( const void* data, std::size_t size, uint8_t prot, std::size_t alignment ) {
		uint64_t addr = alloc ( size, prot, alignment );
		write_bytes ( addr, data, size, PageProtection::WRITE );
		return addr;
	}

	void VirtualMemory::free ( uint64_t addr, std::size_t size ) {
		uint64_t base = addr & ~( page_size - 1 );
		std::size_t pages_needed = ( size + page_size - 1 ) / page_size;
		uint64_t region_end = base + pages_needed * page_size;

		auto it = regions.lower_bound ( base );
		if ( it != regions.begin ( ) ) --it;
		while ( it != regions.end ( ) && it->first < region_end ) {
			if ( it->first + it->second.size > base ) {
				auto next = std::next ( it );
				regions.erase ( it );
				it = next;
			}
			else {
				++it;
			}
		}

		for ( std::size_t i = 0; i < pages_needed; i++ ) {
			uint64_t virt = base + i * page_size;
			auto page_it = pages.find ( virt );
			if ( page_it != pages.end ( ) ) {
				uncommit ( page_it->second->data );
				pages.erase ( page_it );
			}
		}
	}

	bool VirtualMemory::protect ( uint64_t addr, std::size_t size, uint8_t prot ) {
		uint64_t start = addr & ~( page_size - 1 );
		std::size_t pages_needed = ( size + page_size - 1 ) / page_size;
		uint64_t end = start + pages_needed * page_size;

		auto* region = find_region ( addr );
		if ( !region ) {
			return false;
		}

		split_region ( region->base_address, start, end, prot );
		for ( std::size_t i = 0; i < pages_needed; i++ ) {
			uint64_t virt = start + i * page_size;
			auto it = pages.find ( virt );
			if ( it == pages.end ( ) ) return false;
			it->second->prot = prot;
		}
		return true;
	}

	const Region* VirtualMemory::find_region ( uint64_t addr ) const {
		auto it = regions.upper_bound ( addr );
		if ( it == regions.begin ( ) ) {
			return nullptr;
		}
		--it;
		if ( it->first <= addr && addr < it->first + it->second.size ) {
			return &it->second;
		}
		return nullptr;
	}

	void VirtualMemory::split_region ( uint64_t base, uint64_t split_start, uint64_t split_end, uint8_t new_protect ) {
		auto it = regions.find ( base );
		if ( it == regions.end ( ) ) return;

		Region old_region = it->second;
		regions.erase ( it );

		if ( split_start > base ) {
			Region before { base, static_cast< std::size_t >( split_start - base ), old_region.allocation_protect, old_region.current_protect };
			regions [ base ] = before;
		}

		Region modified { split_start, static_cast< std::size_t >( split_end - split_start ), old_region.allocation_protect, new_protect };
		regions [ split_start ] = modified;

		if ( split_end < base + old_region.size ) {
			Region after { split_end, static_cast< std::size_t > ( base + old_region.size - split_end ), old_region.allocation_protect, old_region.current_protect };
			regions [ split_end ] = after;
		}

		for ( auto& [virt, page] : pages ) {
			if ( virt >= base && virt < base + old_region.size ) {
				auto* new_region = find_region ( virt );
				if ( new_region ) {
					page->region_base = new_region->base_address;
				}
			}
		}
	}

	void* VirtualMemory::translate ( uint64_t addr, uint8_t access, bool silent ) {
		uint64_t virt_page = addr & ~( page_size - 1 );
		for ( auto& e : cache ) {
			if ( e.virt == virt_page ) {
				if ( ( e.page->prot & access ) != access ) {
					if constexpr ( verbose_memory ) {
						if ( !silent )
							std::println ( "Access violation at address {:#x} with access {:#x} (protection)", addr, access );
					}
					return nullptr;
				}
				if ( !e.page->present ) {
					e.page->data = commit ( page_size );
					if ( !e.page->data ) {
						if constexpr ( verbose_memory ) {
							if ( !silent )
								std::println ( "Access violation at address {:#x} with access {:#x} (memory not committed)", addr, access );
						}
						return nullptr;
					}
					std::memset ( e.page->data, 0, page_size );
					e.page->present = true;
				}
				if ( ( access & PageProtection::READ ) != 0 ) {
					auto* region = find_region ( addr );
					if ( region && region->read_hook ) {
						( *region->read_hook )( this, addr, page_size - ( addr - virt_page ) );
					}
				}
				return e.page->data + ( addr - virt_page );
			}
		}
		auto it = pages.find ( virt_page );
		if ( it == pages.end ( ) ) {
			if constexpr ( verbose_memory ) {
				if ( !silent )
					std::println ( "Access violation at address {:#x} with access {:#x} (invalid address)", addr, access );
			}
			return nullptr;
		}
		Page* pg = it->second.get ( );
		cache [ cache_pos ] = { virt_page, pg };
		cache_pos = ( cache_pos + 1 ) % cache.size ( );
		if ( ( pg->prot & access ) != access ) {
			if constexpr ( verbose_memory ) {
				if ( !silent )
					std::println ( "Access violation at address {:#x} with access {:#x}", addr, access );
			}
			return nullptr;
		}
		if ( !pg->present ) {
			pg->data = commit ( page_size );
			if ( !pg->data ) {
				if constexpr ( verbose_memory ) {
					if ( !silent )
						std::println ( "Access violation at address {:#x} with access %u (insufficient memory)", addr, access );
				}
				return nullptr;
			}
			std::memset ( pg->data, 0, page_size );
			pg->present = true;
		}

		auto* region = find_region ( addr );
		if ( region && region->read_hook ) {
			( *region->read_hook )( this, addr, page_size - ( addr - virt_page ) );
		}

		return pg->data + ( addr - virt_page );
	}

	void* VirtualMemory::translate_bypass ( uint64_t addr, bool silent ) {
		uint64_t virt_page = addr & ~( page_size - 1 );
		for ( auto& e : cache ) {
			if ( e.virt == virt_page ) {
				if ( !e.page->present ) {
					e.page->data = commit ( page_size );
					if ( !e.page->data ) {
						if constexpr ( verbose_memory ) {
							if ( !silent )
								std::println ( "Access violation at address {:#x} (insufficient memory)", addr );
						}
						return nullptr;
					}
					std::memset ( e.page->data, 0, page_size );
					e.page->present = true;
				}
				return e.page->data + ( addr - virt_page );
			}
		}

		auto it = pages.find ( virt_page );
		if ( it == pages.end ( ) ) {
			if constexpr ( verbose_memory ) {
				if ( !silent )
					std::println ( "Access violation at address {:#x} (invalid address)", addr );
			}
			return nullptr;
		}
		Page* pg = it->second.get ( );
		cache [ cache_pos ] = { virt_page, pg };
		cache_pos = ( cache_pos + 1 ) % cache.size ( );

		if ( !pg->present ) {
			pg->data = commit ( page_size );
			if ( !pg->data ) {
				if constexpr ( verbose_memory ) {
					if ( !silent )
						std::println ( "Access violation at address {:#x} (insufficient memory)", addr );
				}
				return nullptr;
			}
			std::memset ( pg->data, 0, page_size );
			pg->present = true;
		}

		return pg->data + ( addr - virt_page );
	}

	bool VirtualMemory::check ( uint64_t addr, std::size_t size, uint8_t access ) {
		for ( std::size_t offset = 0; offset < size; offset += page_size ) {
			void* p = translate ( addr + offset, access );
			if ( !p ) return false;
		}
		return true;
	}

	WinMemoryBasicInformation VirtualMemory::get_memory_basic_information ( uint64_t addr ) {
		WinMemoryBasicInformation mbi { 0 };
		auto* region = find_region ( addr );
		if ( region ) {
			mbi.base_address = region->base_address;
			mbi.allocation_base = region->base_address;
			mbi.allocation_protect = map_to_win_protect ( region->base_address );
			mbi.region_size = region->size;
			mbi.protect = map_to_win_protect ( addr );
			mbi.state = 0x1000;
			mbi.type = 0x20000;
		}
		return mbi;
	}

	Page* VirtualMemory::get_page ( uint64_t addr ) {
		auto it = pages.find ( addr & ~( page_size - 1 ) );
		if ( it == pages.end ( ) ) return nullptr;
		return it->second.get ( );
	}

	uint32_t VirtualMemory::map_to_win_protect ( uint64_t addr ) {
		auto* region = find_region ( addr );
		if ( !region ) {
			return 0x01; // PAGE_NOACCESS
		}
		const auto protection = region->current_protect;
		const auto executable = ( protection & PageProtection::EXEC ) != PageProtection::NONE;
		const auto readable = ( protection & PageProtection::READ ) != PageProtection::NONE;
		const auto writable = ( protection & PageProtection::WRITE ) != PageProtection::NONE;

		if ( !readable ) {
			return 0x01; // PAGE_NOACCESS
		}

		if ( executable && writable ) {
			return 0x40; // PAGE_EXECUTE_READWRITE
		}

		if ( writable ) {
			return 0x04; // PAGE_READWRITE
		}

		if ( executable ) {
			return 0x20; // PAGE_EXECUTE_READ
		}

		return 0x02; // PAGE_READONLY
	}
}

```

`types.hpp`:

```hpp
#pragma once

#include <boost/multiprecision/cpp_bin_float.hpp>
#include <boost/multiprecision/cpp_int.hpp>
#include <array>
#include <cstdint>
#include <unordered_map>
#include <optional>
#include <functional>
#include <vector>
#include "x86.hpp"

namespace mp = boost::multiprecision;
using int128_t = mp::int128_t;
using uint128_t = mp::uint128_t;
using uint256_t = mp::uint256_t;
using uint512_t = mp::uint512_t;
using int256_t = mp::int256_t;
using int512_t = mp::int512_t;
using float80_t =
mp::number<mp::cpp_bin_float<
	64,                 // Number of significand bits (including explicit leading bit when non-zero)
	mp::digit_base_2,   // Binary representation
	void, std::int16_t, // Use 16-bit exponent type
	-16382, 16383       // Min/Max exponent values
>, mp::et_off>;       // Disable expression templates for simplicity
#if defined(_MSC_VER)
#include <intrin.h>
#include <iced.hpp>
#define READ_TSC() __rdtsc()
#elif defined(__GNUC__) || defined(__clang__)
#if defined(__x86_64__) || defined(__i386__)
static inline uint64_t READ_TSC ( ) {
	uint32_t hi, lo;
	__asm__ __volatile__ ( "rdtsc" : "=a"( lo ), "=d"( hi ) );
	return ( ( uint64_t ) hi << 32 ) | lo;
}
#elif defined(__aarch64__)
static inline uint64_t READ_TSC ( ) {
	uint64_t val;
	__asm__ __volatile__ ( "mrs %0, cntvct_el0" : "=r"( val ) );
	return val;
}
#else
#define READ_TSC() get_timestamp() // fallback to time-based
#endif
#endif

#ifndef UNREACHABLE
#ifdef _MSC_VER
#define UNREACHABLE() __assume(0)
#else
#define UNREACHABLE() __builtin_unreachable()
#endif
#endif
namespace kubera
{
	enum KubRegister {
		// General-purpose registers (64-bit)
		RAX,
		RBX,
		RCX,
		RDX,
		RSI,
		RDI,
		RBP,
		RSP,
		R8,
		R9,
		R10,
		R11,
		R12,
		R13,
		R14,
		R15,
		RIP,

		// Debug registers
		DR0,
		DR1,
		DR2,
		DR3,
		DR4,
		DR5,
		DR6,
		DR7,

		// Control registers
		CR0, // Flags
		CR2, // Page fault linear address
		CR3, // Page table base
		CR4, // CPU features/extensions
		CR8, // CPU priority, interrupts

		ES, // Extra Segment
		CS, // Code Segment
		SS, // Stack Segment
		DS, // Data Segment
		FS, // FS Segment
		GS, // GS Segment

		COUNT // Total count for array sizing
	};

	struct alignas( 64 ) FPU {
		std::array<float80_t, 8> fpu_stack = { 0 };
		x86::FPUTagWord fpu_tag_word = { .value = 0xFFFF };
		x86::FPUStatusWord fpu_status_word = { .value = 0x0000 };
		x86::FPUControlWord fpu_control_word = { .value = 0x027F };
		uint8_t fpu_top = 0;

		int get_fpu_phys_idx ( int sti ) const {
			return ( fpu_top + sti ) % 8;
		}

		void set_fpu_tag ( int phys_idx, uint8_t tag ) {
			switch ( phys_idx ) {
				case 0: fpu_tag_word.TAG0 = tag; break;
				case 1: fpu_tag_word.TAG1 = tag; break;
				case 2: fpu_tag_word.TAG2 = tag; break;
				case 3: fpu_tag_word.TAG3 = tag; break;
				case 4: fpu_tag_word.TAG4 = tag; break;
				case 5: fpu_tag_word.TAG5 = tag; break;
				case 6: fpu_tag_word.TAG6 = tag; break;
				case 7: fpu_tag_word.TAG7 = tag; break;
				default: UNREACHABLE ( );
			}
		}

		uint8_t get_fpu_tag ( int phys_idx ) const {
			switch ( phys_idx ) {
				case 0: return fpu_tag_word.TAG0;
				case 1: return fpu_tag_word.TAG1;
				case 2: return fpu_tag_word.TAG2;
				case 3: return fpu_tag_word.TAG3;
				case 4: return fpu_tag_word.TAG4;
				case 5: return fpu_tag_word.TAG5;
				case 6: return fpu_tag_word.TAG6;
				case 7: return fpu_tag_word.TAG7;
				default: UNREACHABLE ( );
			}
		}

		void update_fsw_top ( ) {
			fpu_status_word.TOP = fpu_top & 0b111;
		}

		uint8_t classify_fpu_operand ( const float80_t& val ) const {
			using namespace boost::multiprecision;
			int c = fpclassify ( val );
			switch ( c ) {
				case FP_NAN:       return x86::FPU_TAG_SPECIAL;
				case FP_INFINITE:  return x86::FPU_TAG_SPECIAL;
				case FP_ZERO:      return x86::FPU_TAG_ZERO;
				case FP_SUBNORMAL: return x86::FPU_TAG_SPECIAL;
				case FP_NORMAL:    return x86::FPU_TAG_VALID;
				default:           return x86::FPU_TAG_SPECIAL;
			}
		}
	};

	struct alignas( 64 ) CPU {
		std::array<std::uint64_t, KubRegister::COUNT> registers = { 0 };
		std::uint64_t stack_base = 0ULL;
		std::size_t stack_size = 0x200000;
		std::vector<std::uint64_t> shadow_stack { };
		std::uint64_t ssp = 0ULL;
		x86::Flags rflags = static_cast< x86::Flags >( 0x0000000000000202ULL );
		x86::Mxcsr mxcsr = static_cast< x86::Mxcsr >( 0x1F80U );
		std::uint64_t timestamp_counter = 0ULL;
		std::uint8_t current_privilege_level = 3;
		FPU fpu { };
		std::unique_ptr<std::array<uint512_t, 32>> sse_registers = nullptr;

		CPU ( std::uint64_t stack_base_addr, std::size_t _stack_size ) : stack_base ( stack_base_addr ), stack_size ( _stack_size ) {
			sse_registers = std::make_unique<std::array<uint512_t, 32>> ( );
			sse_registers->fill ( uint512_t ( 0 ) );
			timestamp_counter = READ_TSC ( );
		}

		void increment_tsc ( std::size_t amount = 1 ) {
			timestamp_counter += amount;
		}
	};

	enum PageProtection : uint8_t {
		NONE = 0,
		READ = 1 << 0,
		WRITE = 1 << 1,
		EXEC = 1 << 2
	};


	struct Page {
		uint8_t* data { nullptr };
		uint8_t prot { PageProtection::NONE };
		bool present { false };
		uint64_t region_base { 0 };
	};

	struct Region {
		uint64_t base_address { 0 };
		std::size_t size { 0 };
		uint8_t allocation_protect { PageProtection::NONE };
		uint8_t current_protect { PageProtection::NONE };
		std::optional<std::function<void ( class VirtualMemory*, uint64_t addr, std::size_t size )>> read_hook;
	};

	struct WinMemoryBasicInformation {
		uint64_t base_address { 0 };
		uint64_t allocation_base { 0 };
		uint32_t allocation_protect { 0 };
		std::size_t region_size { 0 };
		uint32_t protect { 0 };
		uint32_t state { 0 };
		uint32_t type { 0 };
	};

	struct WinMemoryImageInformation {
		uint64_t ImageBase;
		int64_t SizeOfImage;

		union {
			uint32_t ImageFlags;

			struct {
				uint32_t ImagePartialMap : 1;
				uint32_t ImageNotExecutable : 1;
				uint32_t ImageSigningLevel : 4;     // REDSTONE3
				uint32_t ImageExtensionPresent : 1; // since 24H2
				uint32_t Reserved : 25;
			};
		};
	};
};
```

`x86.hpp`:

```hpp
#pragma once

#include <cstdint>

namespace x86
{
	// RFLAGS Register (64 bits)
	union Flags {
		std::uint64_t value;
		struct {
			std::uint64_t CF : 1;					// Carry Flag
			std::uint64_t reserved1 : 1;		// Reserved (always 1)
			std::uint64_t PF : 1;					// Parity Flag
			std::uint64_t reserved3 : 1;		// Reserved (0)
			std::uint64_t AF : 1;					// Auxiliary Carry Flag
			std::uint64_t reserved5 : 1;		// Reserved (0)
			std::uint64_t ZF : 1;					// Zero Flag
			std::uint64_t SF : 1;					// Sign Flag
			std::uint64_t TF : 1;					// Trap Flag
			std::uint64_t IF : 1;					// Interrupt Enable Flag
			std::uint64_t DF : 1;					// Direction Flag
			std::uint64_t OF : 1;					// Overflow Flag
			std::uint64_t IOPL : 2;				// I/O Privilege Level
			std::uint64_t NT : 1;					// Nested Task Flag
			std::uint64_t reserved15 : 1;  // Reserved (0)
			std::uint64_t RF : 1;					// Resume Flag
			std::uint64_t VM : 1;					// Virtual-8086 Mode
			std::uint64_t AC : 1;					// Alignment Check / Access Control
			std::uint64_t VIF : 1;					// Virtual Interrupt Flag
			std::uint64_t VIP : 1;					// Virtual Interrupt Pending
			std::uint64_t ID : 1;					// ID Flag
			std::uint64_t reserved22_63 : 41;  // Reserved (0)
		};
	};

	// MXCSR Register (32 bits)
	union Mxcsr {
		std::uint32_t value;
		struct {
			unsigned int IE : 1;   // Invalid Operation Flag
			unsigned int DE : 1;   // Denormal Flag
			unsigned int ZE : 1;   // Divide-by-Zero Flag
			unsigned int OE : 1;   // Overflow Flag
			unsigned int UE : 1;   // Underflow Flag
			unsigned int PE : 1;   // Precision Flag
			unsigned int DAZ : 1;  // Denormals Are Zeros Flag
			unsigned int IM : 1;   // Invalid Operation Mask
			unsigned int DM : 1;   // Denormal Mask
			unsigned int ZM : 1;   // Divide-by-Zero Mask
			unsigned int OM : 1;   // Overflow Mask
			unsigned int UM : 1;   // Underflow Mask
			unsigned int PM : 1;   // Precision Mask
			unsigned int RC : 2;   // Rounding Control
			unsigned int FTZ : 1;  // Flush To Zero Flag
			unsigned int reserved : 16;  // Reserved
		};
	};

	// x87 FPU Control Word (16 bits)
	union FPUControlWord {
		std::uint16_t value;
		struct {
			unsigned int IM : 1;  // Invalid Operation Mask
			unsigned int DM : 1;  // Denormal Operand Mask
			unsigned int ZM : 1;  // Divide-by-Zero Mask
			unsigned int OM : 1;  // Overflow Mask
			unsigned int UM : 1;  // Underflow Mask
			unsigned int PM : 1;  // Precision Mask
			unsigned int reserved6 : 1;  // Reserved
			unsigned int reserved7 : 1;  // Reserved
			unsigned int PC : 2;  // Precision Control
			unsigned int RC : 2;  // Rounding Control
			unsigned int IC : 1;  // Infinity Control (legacy)
			unsigned int reserved13 : 3;  // Reserved
		};
	};

	// x87 FPU Status Word (16 bits)
	union FPUStatusWord {
		std::uint16_t value;
		struct {
			unsigned int IE : 1;  // Invalid Operation Flag
			unsigned int DE : 1;  // Denormal Operand Flag
			unsigned int ZE : 1;  // Divide-by-Zero Flag
			unsigned int OE : 1;  // Overflow Flag
			unsigned int UE : 1;  // Underflow Flag
			unsigned int PE : 1;  // Precision Flag
			unsigned int SF : 1;  // Stack Fault Flag
			unsigned int ES : 1;  // Exception Summary Status Flag
			unsigned int C0 : 1;  // Condition Code 0
			unsigned int C1 : 1;  // Condition Code 1
			unsigned int C2 : 1;  // Condition Code 2
			unsigned int TOP : 3; // Top of Stack Pointer
			unsigned int C3 : 1;  // Condition Code 3
			unsigned int B : 1;   // Busy Flag
		};
	};

	// x87 FPU Tag Word (16 bits)
	union FPUTagWord {
		std::uint16_t value;
		struct {
			unsigned int TAG0 : 2;  // Tag for ST(0): 00=Valid, 01=Zero, 10=Special, 11=Empty
			unsigned int TAG1 : 2;  // Tag for ST(1): 00=Valid, 01=Zero, 10=Special, 11=Empty
			unsigned int TAG2 : 2;  // Tag for ST(2): 00=Valid, 01=Zero, 10=Special, 11=Empty
			unsigned int TAG3 : 2;  // Tag for ST(3): 00=Valid, 01=Zero, 10=Special, 11=Empty
			unsigned int TAG4 : 2;  // Tag for ST(4): 00=Valid, 01=Zero, 10=Special, 11=Empty
			unsigned int TAG5 : 2;  // Tag for ST(5): 00=Valid, 01=Zero, 10=Special, 11=Empty
			unsigned int TAG6 : 2;  // Tag for ST(6): 00=Valid, 01=Zero, 10=Special, 11=Empty
			unsigned int TAG7 : 2;  // Tag for ST(7): 00=Valid, 01=Zero, 10=Special, 11=Empty
		};
	};

	static constexpr uint8_t FPU_TAG_VALID = 0b00;
	static constexpr uint8_t FPU_TAG_ZERO = 0b01;
	static constexpr uint8_t FPU_TAG_SPECIAL = 0b10;
	static constexpr uint8_t FPU_TAG_EMPTY = 0b11;
	static constexpr uint16_t FSW_IE = ( 1 << 0 );
	static constexpr uint16_t FSW_DE = ( 1 << 1 );
	static constexpr uint16_t FSW_ZE = ( 1 << 2 );
	static constexpr uint16_t FSW_OE = ( 1 << 3 );
	static constexpr uint16_t FSW_UE = ( 1 << 4 );
	static constexpr uint16_t FSW_PE = ( 1 << 5 );
	static constexpr uint16_t FSW_SF = ( 1 << 6 );
	static constexpr uint16_t FSW_ES = ( 1 << 7 );
	static constexpr uint16_t FSW_C0 = ( 1 << 8 );
	static constexpr uint16_t FSW_C1 = ( 1 << 9 );
	static constexpr uint16_t FSW_C2 = ( 1 << 10 );
	static constexpr uint16_t FSW_TOP_SHIFT = 11;
	static constexpr uint16_t FSW_TOP_MASK = ( 0b111 << FSW_TOP_SHIFT );
	static constexpr uint16_t FSW_C3 = ( 1 << 14 );
	static constexpr uint16_t FSW_B = ( 1 << 15 );
	static constexpr uint16_t FCW_IM = ( 1 << 0 );
	static constexpr uint16_t FCW_DM = ( 1 << 1 );
	static constexpr uint16_t FCW_ZM = ( 1 << 2 );
	static constexpr uint16_t FCW_OM = ( 1 << 3 );
	static constexpr uint16_t FCW_UM = ( 1 << 4 );
	static constexpr uint16_t FCW_PM = ( 1 << 5 );
	static constexpr uint16_t FCW_PC_SHIFT = 8;
	static constexpr uint16_t FCW_PC_MASK = ( 0b11 << FCW_PC_SHIFT );
	static constexpr uint16_t FCW_RC_SHIFT = 10;
	static constexpr uint16_t FCW_RC_MASK = ( 0b11 << FCW_RC_SHIFT );
};
```