Project Path: arc_waldo-vision_waldo_2ceelcqc

Source Tree:

```txt
arc_waldo-vision_waldo_2ceelcqc
â”œâ”€â”€ README.md
â”œâ”€â”€ deepcheat
â”‚   â”œâ”€â”€ LICENSE
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ Setup.md
â”‚   â”œâ”€â”€ VideoMAEv2
â”‚   â”‚   â”œâ”€â”€ dataset
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ build.py
â”‚   â”‚   â”‚   â”œâ”€â”€ datasets.py
â”‚   â”‚   â”‚   â”œâ”€â”€ functional.py
â”‚   â”‚   â”‚   â”œâ”€â”€ loader.py
â”‚   â”‚   â”‚   â”œâ”€â”€ masking_generator.py
â”‚   â”‚   â”‚   â”œâ”€â”€ pretrain_datasets.py
â”‚   â”‚   â”‚   â”œâ”€â”€ rand_augment.py
â”‚   â”‚   â”‚   â”œâ”€â”€ random_erasing.py
â”‚   â”‚   â”‚   â”œâ”€â”€ transforms.py
â”‚   â”‚   â”‚   â”œâ”€â”€ video_transforms.py
â”‚   â”‚   â”‚   â””â”€â”€ volume_transforms.py
â”‚   â”‚   â”œâ”€â”€ do_cheater.sh
â”‚   â”‚   â”œâ”€â”€ engine_for_finetuning.py
â”‚   â”‚   â”œâ”€â”€ engine_for_pretraining.py
â”‚   â”‚   â”œâ”€â”€ eval_cheater.sh
â”‚   â”‚   â”œâ”€â”€ eval_killshot.sh
â”‚   â”‚   â”œâ”€â”€ eval_killshot_pred.py
â”‚   â”‚   â”œâ”€â”€ eval_mae.py
â”‚   â”‚   â”œâ”€â”€ extract_tad_feature.py
â”‚   â”‚   â”œâ”€â”€ models
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ modeling_finetune.py
â”‚   â”‚   â”‚   â””â”€â”€ modeling_pretrain.py
â”‚   â”‚   â”œâ”€â”€ optim_factory.py
â”‚   â”‚   â”œâ”€â”€ train_cheater_pred.py
â”‚   â”‚   â”œâ”€â”€ train_cheater_pred.py.backup
â”‚   â”‚   â”œâ”€â”€ train_cheater_pred_patch.py
â”‚   â”‚   â””â”€â”€ utils.py
â”‚   â”œâ”€â”€ dl_classify_video.py
â”‚   â”œâ”€â”€ environment.yml
â”‚   â”œâ”€â”€ finetuning
â”‚   â”‚   â””â”€â”€ README.md
â”‚   â”œâ”€â”€ pretraining
â”‚   â”‚   â””â”€â”€ README.md
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ utils
â”‚       â”œâ”€â”€ README.md
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ autoedit
â”‚       â”‚   â”œâ”€â”€ autoedit_improved.py
â”‚       â”‚   â”œâ”€â”€ sample
â”‚       â”‚   â””â”€â”€ temp
â”‚       â”œâ”€â”€ bb_killshot.py
â”‚       â”œâ”€â”€ combo_cheater_detection.py
â”‚       â”œâ”€â”€ common.py
â”‚       â”œâ”€â”€ crop_videos.py
â”‚       â”œâ”€â”€ default_video_cropper.py
â”‚       â”œâ”€â”€ download_links.py
â”‚       â”œâ”€â”€ generate-requirements.sh
â”‚       â”œâ”€â”€ install-environment.sh
â”‚       â”œâ”€â”€ killshot_labels
â”‚       â”‚   â”œâ”€â”€ -0iKXgFdlwc
â”‚       â”‚   â”‚   â””â”€â”€ annotations
â”‚       â”‚   â”‚       â””â”€â”€ bbox_labels_600_hierarchy.json
â”‚       â”‚   â”œâ”€â”€ -6c8yKPnts0
â”‚       â”‚   â”‚   â””â”€â”€ annotations
â”‚       â”‚   â”‚       â””â”€â”€ bbox_labels_600_hierarchy.json
â”‚       â”‚   â”œâ”€â”€ -GQSo52RO_o
â”‚       â”‚   â”‚   â””â”€â”€ annotations
â”‚       â”‚   â”‚       â””â”€â”€ bbox_labels_600_hierarchy.json
â”‚       â”‚   â”œâ”€â”€ -IyUS9onDDg
â”‚       â”‚   â”‚   â””â”€â”€ annotations
â”‚       â”‚   â”‚       â””â”€â”€ bbox_labels_600_hierarchy.json
â”‚       â”‚   â”œâ”€â”€ -Om8-wo3vTU
â”‚       â”‚   â”‚   â””â”€â”€ annotations
â”‚       â”‚   â”‚       â””â”€â”€ bbox_labels_600_hierarchy.json
â”‚       â”‚   â”œâ”€â”€ -ZVIdzrqpK0
â”‚       â”‚   â”‚   â””â”€â”€ annotations
â”‚       â”‚   â”‚       â””â”€â”€ bbox_labels_600_hierarchy.json
â”‚       â”‚   â”œâ”€â”€ -bjvfYf64tM
â”‚       â”‚   â”‚   â””â”€â”€ annotations
â”‚       â”‚   â”‚       â””â”€â”€ bbox_labels_600_hierarchy.json
â”‚       â”‚   â”œâ”€â”€ -r8hvrBP6TI
â”‚       â”‚   â”‚   â””â”€â”€ annotations
â”‚       â”‚   â”‚       â””â”€â”€ bbox_labels_600_hierarchy.json
â”‚       â”‚   â”œâ”€â”€ 02Wh2gO4Q58
â”‚       â”‚   â”‚   â””â”€â”€ annotations
â”‚       â”‚   â”‚       â””â”€â”€ bbox_labels_600_hierarchy.json
â”‚       â”‚   â””â”€â”€ 02y9VdPY8q8
â”‚       â”‚       â””â”€â”€ annotations
â”‚       â”‚           â””â”€â”€ bbox_labels_600_hierarchy.json
â”‚       â”œâ”€â”€ link_retrieval.py
â”‚       â”œâ”€â”€ make_traintest.py
â”‚       â”œâ”€â”€ manual_cheater_dataset.py
â”‚       â”œâ”€â”€ new_crop.py
â”‚       â”œâ”€â”€ nick_crop.py
â”‚       â”œâ”€â”€ parse_killshot_labels.py
â”‚       â”œâ”€â”€ raw2_traintest.py
â”‚       â”œâ”€â”€ segmentation.py
â”‚       â””â”€â”€ simple_cheater_dataset.py
â”œâ”€â”€ environment.yml
â”œâ”€â”€ evaluation_results
â”‚   â””â”€â”€ 2d865ce3-af90-4639-b7f7-7609ba4d35da
â”‚       â”œâ”€â”€ frames
â”‚       â”‚   â”œâ”€â”€ 2025-09-17_14-01-12_clip_1
â”‚       â”‚   â”œâ”€â”€ 2025-09-17_14-01-12_clip_10
â”‚       â”‚   â”œâ”€â”€ 2025-09-17_14-01-12_clip_11
â”‚       â”‚   â”œâ”€â”€ 2025-09-17_14-01-12_clip_12
â”‚       â”‚   â”œâ”€â”€ 2025-09-17_14-01-12_clip_13
â”‚       â”‚   â”œâ”€â”€ 2025-09-17_14-01-12_clip_14
â”‚       â”‚   â”œâ”€â”€ 2025-09-17_14-01-12_clip_15
â”‚       â”‚   â”œâ”€â”€ 2025-09-17_14-01-12_clip_16
â”‚       â”‚   â”œâ”€â”€ 2025-09-17_14-01-12_clip_17
â”‚       â”‚   â”œâ”€â”€ 2025-09-17_14-01-12_clip_18
â”‚       â”‚   â”œâ”€â”€ 2025-09-17_14-01-12_clip_19
â”‚       â”‚   â”œâ”€â”€ 2025-09-17_14-01-12_clip_2
â”‚       â”‚   â”œâ”€â”€ 2025-09-17_14-01-12_clip_20
â”‚       â”‚   â”œâ”€â”€ 2025-09-17_14-01-12_clip_21
â”‚       â”‚   â”œâ”€â”€ 2025-09-17_14-01-12_clip_22
â”‚       â”‚   â”œâ”€â”€ 2025-09-17_14-01-12_clip_23
â”‚       â”‚   â”œâ”€â”€ 2025-09-17_14-01-12_clip_24
â”‚       â”‚   â”œâ”€â”€ 2025-09-17_14-01-12_clip_25
â”‚       â”‚   â”œâ”€â”€ 2025-09-17_14-01-12_clip_26
â”‚       â”‚   â”œâ”€â”€ 2025-09-17_14-01-12_clip_27
â”‚       â”‚   â”œâ”€â”€ 2025-09-17_14-01-12_clip_28
â”‚       â”‚   â”œâ”€â”€ 2025-09-17_14-01-12_clip_29
â”‚       â”‚   â”œâ”€â”€ 2025-09-17_14-01-12_clip_3
â”‚       â”‚   â”œâ”€â”€ 2025-09-17_14-01-12_clip_30
â”‚       â”‚   â”œâ”€â”€ 2025-09-17_14-01-12_clip_31
â”‚       â”‚   â”œâ”€â”€ 2025-09-17_14-01-12_clip_32
â”‚       â”‚   â”œâ”€â”€ 2025-09-17_14-01-12_clip_33
â”‚       â”‚   â”œâ”€â”€ 2025-09-17_14-01-12_clip_34
â”‚       â”‚   â”œâ”€â”€ 2025-09-17_14-01-12_clip_35
â”‚       â”‚   â”œâ”€â”€ 2025-09-17_14-01-12_clip_36
â”‚       â”‚   â”œâ”€â”€ 2025-09-17_14-01-12_clip_37
â”‚       â”‚   â”œâ”€â”€ 2025-09-17_14-01-12_clip_38
â”‚       â”‚   â”œâ”€â”€ 2025-09-17_14-01-12_clip_39
â”‚       â”‚   â”œâ”€â”€ 2025-09-17_14-01-12_clip_4
â”‚       â”‚   â”œâ”€â”€ 2025-09-17_14-01-12_clip_40
â”‚       â”‚   â”œâ”€â”€ 2025-09-17_14-01-12_clip_41
â”‚       â”‚   â”œâ”€â”€ 2025-09-17_14-01-12_clip_42
â”‚       â”‚   â”œâ”€â”€ 2025-09-17_14-01-12_clip_43
â”‚       â”‚   â”œâ”€â”€ 2025-09-17_14-01-12_clip_44
â”‚       â”‚   â”œâ”€â”€ 2025-09-17_14-01-12_clip_45
â”‚       â”‚   â”œâ”€â”€ 2025-09-17_14-01-12_clip_46
â”‚       â”‚   â”œâ”€â”€ 2025-09-17_14-01-12_clip_47
â”‚       â”‚   â”œâ”€â”€ 2025-09-17_14-01-12_clip_48
â”‚       â”‚   â”œâ”€â”€ 2025-09-17_14-01-12_clip_49
â”‚       â”‚   â”œâ”€â”€ 2025-09-17_14-01-12_clip_5
â”‚       â”‚   â”œâ”€â”€ 2025-09-17_14-01-12_clip_50
â”‚       â”‚   â”œâ”€â”€ 2025-09-17_14-01-12_clip_6
â”‚       â”‚   â”œâ”€â”€ 2025-09-17_14-01-12_clip_7
â”‚       â”‚   â”œâ”€â”€ 2025-09-17_14-01-12_clip_8
â”‚       â”‚   â”œâ”€â”€ 2025-09-17_14-01-12_clip_9
â”‚       â”‚   â””â”€â”€ cheater_preds.txt
â”‚       â””â”€â”€ results.json
â”œâ”€â”€ install.sh
â”œâ”€â”€ main.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ run.sh
â”œâ”€â”€ temp_processing
â”‚   â””â”€â”€ evaluation_40379e6e
â”‚       â”œâ”€â”€ 2025-09-17_14-01-12_clip_1
â”‚       â”œâ”€â”€ 2025-09-17_14-01-12_clip_10
â”‚       â”œâ”€â”€ 2025-09-17_14-01-12_clip_11
â”‚       â”œâ”€â”€ 2025-09-17_14-01-12_clip_12
â”‚       â”œâ”€â”€ 2025-09-17_14-01-12_clip_13
â”‚       â”œâ”€â”€ 2025-09-17_14-01-12_clip_14
â”‚       â”œâ”€â”€ 2025-09-17_14-01-12_clip_15
â”‚       â”œâ”€â”€ 2025-09-17_14-01-12_clip_16
â”‚       â”œâ”€â”€ 2025-09-17_14-01-12_clip_17
â”‚       â”œâ”€â”€ 2025-09-17_14-01-12_clip_18
â”‚       â”œâ”€â”€ 2025-09-17_14-01-12_clip_19
â”‚       â”œâ”€â”€ 2025-09-17_14-01-12_clip_2
â”‚       â”œâ”€â”€ 2025-09-17_14-01-12_clip_20
â”‚       â”œâ”€â”€ 2025-09-17_14-01-12_clip_21
â”‚       â”œâ”€â”€ 2025-09-17_14-01-12_clip_22
â”‚       â”œâ”€â”€ 2025-09-17_14-01-12_clip_23
â”‚       â”œâ”€â”€ 2025-09-17_14-01-12_clip_24
â”‚       â”œâ”€â”€ 2025-09-17_14-01-12_clip_25
â”‚       â”œâ”€â”€ 2025-09-17_14-01-12_clip_26
â”‚       â”œâ”€â”€ 2025-09-17_14-01-12_clip_27
â”‚       â”œâ”€â”€ 2025-09-17_14-01-12_clip_28
â”‚       â”œâ”€â”€ 2025-09-17_14-01-12_clip_29
â”‚       â”œâ”€â”€ 2025-09-17_14-01-12_clip_3
â”‚       â”œâ”€â”€ 2025-09-17_14-01-12_clip_30
â”‚       â”œâ”€â”€ 2025-09-17_14-01-12_clip_31
â”‚       â”œâ”€â”€ 2025-09-17_14-01-12_clip_32
â”‚       â”œâ”€â”€ 2025-09-17_14-01-12_clip_33
â”‚       â”œâ”€â”€ 2025-09-17_14-01-12_clip_34
â”‚       â”œâ”€â”€ 2025-09-17_14-01-12_clip_35
â”‚       â”œâ”€â”€ 2025-09-17_14-01-12_clip_36
â”‚       â”œâ”€â”€ 2025-09-17_14-01-12_clip_37
â”‚       â”œâ”€â”€ 2025-09-17_14-01-12_clip_38
â”‚       â”œâ”€â”€ 2025-09-17_14-01-12_clip_39
â”‚       â”œâ”€â”€ 2025-09-17_14-01-12_clip_4
â”‚       â”œâ”€â”€ 2025-09-17_14-01-12_clip_40
â”‚       â”œâ”€â”€ 2025-09-17_14-01-12_clip_41
â”‚       â”œâ”€â”€ 2025-09-17_14-01-12_clip_42
â”‚       â”œâ”€â”€ 2025-09-17_14-01-12_clip_43
â”‚       â”œâ”€â”€ 2025-09-17_14-01-12_clip_44
â”‚       â”œâ”€â”€ 2025-09-17_14-01-12_clip_45
â”‚       â”œâ”€â”€ 2025-09-17_14-01-12_clip_46
â”‚       â”œâ”€â”€ 2025-09-17_14-01-12_clip_47
â”‚       â”œâ”€â”€ 2025-09-17_14-01-12_clip_48
â”‚       â”œâ”€â”€ 2025-09-17_14-01-12_clip_49
â”‚       â”œâ”€â”€ 2025-09-17_14-01-12_clip_5
â”‚       â”œâ”€â”€ 2025-09-17_14-01-12_clip_50
â”‚       â”œâ”€â”€ 2025-09-17_14-01-12_clip_6
â”‚       â”œâ”€â”€ 2025-09-17_14-01-12_clip_7
â”‚       â”œâ”€â”€ 2025-09-17_14-01-12_clip_8
â”‚       â”œâ”€â”€ 2025-09-17_14-01-12_clip_9
â”‚       â””â”€â”€ cheater_preds.txt
â”œâ”€â”€ templates
â”‚   â”œâ”€â”€ clip_analysis.html
â”‚   â”œâ”€â”€ evaluation.html
â”‚   â”œâ”€â”€ evaluation_results.html
â”‚   â”œâ”€â”€ index.html
â”‚   â”œâ”€â”€ processing.html
â”‚   â”œâ”€â”€ saved_evaluations.html
â”‚   â””â”€â”€ training.html
â”œâ”€â”€ wsl-install.bat
â”œâ”€â”€ wsl-run.bat
â””â”€â”€ wsl-setup.bat

```

`README.md`:

```md
# Visual Cheat Detection System (Waldo_alpha_01)

**Advanced AI-powered cheat detection for Counter-Strike 2 gameplay footage**

This system uses cutting-edge Vision Transformer deep learning technology to analyze gameplay clips and detect potential cheating behavior from the footage. The current version is specific to CS2 and must use a user-trained model. 

---

## What This Does

- **Analyzes CS2 gameplay videos** - Upload your raw gameplay footage or individual clips
- **Focuses on killshots** - Automatically extracts 2-second clips by detecting headshots from gameplay audio
- **Trains locally from your footage** - Select your processed clips folder and train or fine-tune a model with labels (cheating) or (not cheating) 
- **Analyzes processed clips** - Provides detailed confidence scores from 0.0 (likely legitimate) to 1.0 (likely cheating) on clips 
- **Frame-by-frame analysis** - See exactly which 16 frames the model analyzed for each clip
- **Persistent results** - All analysis results are automatically saved and can be revisited later
- **Export capabilities** - Export results as JSON for further analysis

## ğŸš€ Quick Start Guide (Windows 11)

### Step 1: Download and Setup

1. **Download this project** to your computer (extract the ZIP)

2. **Run wsl-install.bat as admin** (not a virus trust me bro)
    Right click wsl-install.bat and click run as administrator. This will take a few minutes and will look frozen, but it's working in the background to install WSL. 

    Once the terminal auto-closes, reboot your PC and run wsl-install.bat as admin again and follow the prompts to set up a username and password. 

3. **Run wsl-setup.bat NOT as admin** 
    Double click wsl-setup.bat to run it without elevated privilages. This will auto-install the conda environment and all the requirements within the new WSL you just made. 


### Step 2: Prepare model for training/inferencing

1. **Download the model**
    Go to https://huggingface.co/jinggu/jing-model/blob/main/vit_g_ps14_ak_ft_ckpt_7_clean.pth and download the .pth file

2. **Place the model**

    Place this .pth file in the "deepcheat/VideoMAEv2" folder found in the extracted project files. Paste any other downloaded models in "deepcheat/VideoMAEv2/output" for inferencing or fine tuning. 

3. **You're ready to train a model and analyze CS2 clips!**


### Step 3: Run the Application

1. **Start the server**:
    Double click wsl-run.bat

2. **Open your web browser** and go to:
    http://localhost:5000

3. **Follow the steps** to train a model and analyze footage


## ğŸš€ Quick Start Guide (Linux)


1. **Paste this** in your terminal - Script isn't currently tested. If it doesn't work, make install.sh executable and run it. Then do the same for run.sh

```bash
echo "ğŸš€ Starting Waldo installation..."

# Clone the repository
echo "ğŸ“¥ Downloading project from GitHub..."
git clone https://github.com/Mr-Homeless/waldo.git
cd waldo

# Make install.sh executable and run it
echo "ğŸ”§ Installing dependencies..."
chmod +x install.sh
./install.sh

# Download the model file
echo "ğŸ¤– Downloading AI model (1.9GB - this will take a few minutes)..."
wget --show-progress -O deepcheat/vit_g_ps14_ak_ft_ckpt_7_clean.pth \
  https://huggingface.co/jinggu/jing-model/resolve/main/vit_g_ps14_ak_ft_ckpt_7_clean.pth

# Ask user if they want to run the server
echo ""
echo "âœ… Installation complete!"
echo ""
read -p "Would you like to start the server now? (y/n): " -n 1 -r
echo ""
if [[ $REPLY =~ ^[Yy]$ ]]; then
    echo "ğŸŒ Starting server on http://localhost:5000"
    chmod +x run.sh
    ./run.sh
else
    echo "To start the server later, run: cd waldo&& ./run.sh"
fi
```




---

## ğŸ“– How to Use

### 1. Process Raw Footage
- **Upload CS2 gameplay videos** (any format: MP4, AVI, etc.)
- The system automatically detects kill moments using audio analysis
- Extracts 2-second clips around each detected kill
- Any resolution should work, but 59.94 or 60p footage is recommended
- Currently you must remove spectator kills and proximity headshot kills manually

### 2. Train or Fine-Tune Models on Processed Clips
- **Label your clips** as "Cheater" or "Not Cheater" with the drop down selector
- **Train new models** or fine-tune existing ones with your clips (currently you need at least 48 processed clips to start training)
- **Real-time progress tracking** with detailed json logs
- As Mr. Homeless trains more accurate models, they will be made public here for free download https://www.patreon.com/basicallyhomeless and can be placed in the deepcheat/output/ folder for use

### 3. Test Clips for Cheating
- **Select clips to analyze** (from footage processed in step 1.)
- **Choose a trained model** for analysis
- **Get detailed results** including:
  - Confidence scores
  - Probability estimates using sigmoid transformation
  - Frame-by-frame analysis showing exactly what frames and ROI the model examined
  - Color-coded confidence categories

### 4. View Detailed Results
- **Comprehensive dashboard** with statistics and visualizations
- **Individual clip analysis** with video playback
- **Frame viewer** showing the exact 16 frames analyzed
- **Export capabilities** for sharing or further analysis

---

## ğŸ”§ Technical Features

### Advanced Analysis
- **Vision Transformer architecture** with 1 billion parameters
- **Adaptive resolution scaling** - works on any video resolution (1080p, 1440p, 4K, etc.)
- **Smart frame selection** - analyzes frames 85-100 where killshots occur
- **Center crop focus** - concentrates on the crosshair area 
- **Clip generator functionality** - Makes two second clips based on a sample sound -- change the sample sound to any "kill" sound effect for different games

### Smart Video Processing
- **Automatic kill detection** using audio pattern recognition
- **Resolution-adaptive cropping** maintains consistent field of view across different video qualities
- **Optimal frame extraction** targets the exact moments where cheating behavior is most visible

### Professional Results
- **Statistical analysis** with mean, median, standard deviation
- **Distribution visualization** showing score patterns
- **Confidence categorization** with clear color coding
- **Persistent storage** - results saved automatically for future reference

---

## ğŸ“Š Understanding Results

### Confidence Scores
- **0.8 - 1.0**: ğŸ”´ **Very High Confidence - Likely Cheating**
- **0.6 - 0.8**: ğŸŸ  **High Confidence - Possible Cheating**
- **0.4 - 0.6**: ğŸŸ¡ **Medium Confidence - Uncertain**
- **0.2 - 0.4**: ğŸŸ¢ **Low Confidence - Likely Legitimate**
- **0.0 - 0.2**: âœ… **Very Low Confidence - Likely Legitimate**

### What the AI Analyzes
The system examines:
- **Crosshair movement patterns** during engagements
- **Reaction timing** to enemy appearances
- **Tracking smoothness** and micro-corrections
- **Pre-aim positioning** before enemies are visible
- **Flick accuracy** and consistency patterns

---

## ğŸ¥ Supported Video Formats

- **Resolution**: Any (1080p, 1440p, 4K, ultrawide, etc.)
- **Formats**: MP4, AVI, MOV, MKV, and most common video formats
- **Frame rates**: 59.94, 60fps, constant frame rate recommended 
- **Best quality**: Higher resolution and frame rate = better analysis

---

## âš¡ Performance Tips

### For Best Results:
1. **Use high-quality footage** (1440p+ recommended)
2. **Include multiple kills** in your uploaded videos
3. **Clear audio** helps with automatic kill detection
4. **Consistent crosshair placement** in center of screen

### System Requirements:
- **CPU**: 12th gen intel or Ryzen 5000 or newer
- **RAM**: 16GB+ recommended (32GB+ for large videos)
- **GPU**: NVIDIA GPU with CUDA support (other GPUs may work in compatibility mode but have not been tested)
- **Storage**: ~10GB free space for models and temporary files

---

## ğŸ”’ Privacy & Data

- **All processing is local** - your footage never leaves your computer
- **No internet required** for analysis (only for initial setup)
- **Results stored locally** in the `evaluation_results` folder
- **Your data remains private** and under your control - no cloud connection needed

---

## ğŸ› ï¸ Troubleshooting

- After training a new model or fine tuning a model, it may give an error code, but if it ran through the Epocs and trained, it did complete and the errors are likely not crucial to functioning. 

- If running this on Windows, it will run slightly slower and sometimes look frozen/won't have output. This is normal right now.

- Auto-clipping function Works for multiple audio streams, so if its not giving you 2 second clips, double check that the game audio is in the file and in-sync with the footage.



---

## ğŸ“œ License & Disclaimer

**This tool is for educational and analytical purposes.**

- Results should be considered **guidance, not definitive proof**
- Respect privacy and competitive integrity guidelines
- Use responsibly within gaming community standards
- This **is not the final, trained and tuned version of Waldo that will definitively tell you who is cheating**


---

## ğŸŒŸ Version Information

**Current Version**: Alpha 1.0
**Model Version**: VideoMAE v2 with CS2-specific training
**Last Updated**: September 19th, 2025

### TO-DO LIST:
- â˜ Create an OBS plugin that uses replay recording to record two second clips at the detected sound byte
- â˜ The wsl-setup.bat can be modified to put temp files in a better location and potentially speed up the training for Windows users
- â˜ Update pytorch without breaking everything - this should make training on newer GPUs faster
- â˜ Create a filter that deletes/removes clips from spectator view kills and proximity kills
- â˜ Have a whale lan with 500 locked down PCs to get massive amounts of clean labeled data 
- â˜ Train the terminator waldo model in year 2027 on RTX 6090Tis 

---

## ğŸ¤ Support

For questions, issues, or feedback about this beta version, please provide:
- Your system specifications
- Error messages (if any)
- Screenshots of issues
- Description of what you were trying to do

**Remember**: This is not finalized software. While thoroughly tested, you may encounter issues. Your feedback helps improve the system for everyone!

---

**Happy training and analyzing! ğŸ¯**

```

`deepcheat/LICENSE`:

```
Mozilla Public License Version 2.0
==================================

1. Definitions
--------------

1.1. "Contributor"
    means each individual or legal entity that creates, contributes to
    the creation of, or owns Covered Software.

1.2. "Contributor Version"
    means the combination of the Contributions of others (if any) used
    by a Contributor and that particular Contributor's Contribution.

1.3. "Contribution"
    means Covered Software of a particular Contributor.

1.4. "Covered Software"
    means Source Code Form to which the initial Contributor has attached
    the notice in Exhibit A, the Executable Form of such Source Code
    Form, and Modifications of such Source Code Form, in each case
    including portions thereof.

1.5. "Incompatible With Secondary Licenses"
    means

    (a) that the initial Contributor has attached the notice described
        in Exhibit B to the Covered Software; or

    (b) that the Covered Software was made available under the terms of
        version 1.1 or earlier of the License, but not also under the
        terms of a Secondary License.

1.6. "Executable Form"
    means any form of the work other than Source Code Form.

1.7. "Larger Work"
    means a work that combines Covered Software with other material, in
    a separate file or files, that is not Covered Software.

1.8. "License"
    means this document.

1.9. "Licensable"
    means having the right to grant, to the maximum extent possible,
    whether at the time of the initial grant or subsequently, any and
    all of the rights conveyed by this License.

1.10. "Modifications"
    means any of the following:

    (a) any file in Source Code Form that results from an addition to,
        deletion from, or modification of the contents of Covered
        Software; or

    (b) any new file in Source Code Form that contains any Covered
        Software.

1.11. "Patent Claims" of a Contributor
    means any patent claim(s), including without limitation, method,
    process, and apparatus claims, in any patent Licensable by such
    Contributor that would be infringed, but for the grant of the
    License, by the making, using, selling, offering for sale, having
    made, import, or transfer of either its Contributions or its
    Contributor Version.

1.12. "Secondary License"
    means either the GNU General Public License, Version 2.0, the GNU
    Lesser General Public License, Version 2.1, the GNU Affero General
    Public License, Version 3.0, or any later versions of those
    licenses.

1.13. "Source Code Form"
    means the form of the work preferred for making modifications.

1.14. "You" (or "Your")
    means an individual or a legal entity exercising rights under this
    License. For legal entities, "You" includes any entity that
    controls, is controlled by, or is under common control with You. For
    purposes of this definition, "control" means (a) the power, direct
    or indirect, to cause the direction or management of such entity,
    whether by contract or otherwise, or (b) ownership of more than
    fifty percent (50%) of the outstanding shares or beneficial
    ownership of such entity.

2. License Grants and Conditions
--------------------------------

2.1. Grants

Each Contributor hereby grants You a world-wide, royalty-free,
non-exclusive license:

(a) under intellectual property rights (other than patent or trademark)
    Licensable by such Contributor to use, reproduce, make available,
    modify, display, perform, distribute, and otherwise exploit its
    Contributions, either on an unmodified basis, with Modifications, or
    as part of a Larger Work; and

(b) under Patent Claims of such Contributor to make, use, sell, offer
    for sale, have made, import, and otherwise transfer either its
    Contributions or its Contributor Version.

2.2. Effective Date

The licenses granted in Section 2.1 with respect to any Contribution
become effective for each Contribution on the date the Contributor first
distributes such Contribution.

2.3. Limitations on Grant Scope

The licenses granted in this Section 2 are the only rights granted under
this License. No additional rights or licenses will be implied from the
distribution or licensing of Covered Software under this License.
Notwithstanding Section 2.1(b) above, no patent license is granted by a
Contributor:

(a) for any code that a Contributor has removed from Covered Software;
    or

(b) for infringements caused by: (i) Your and any other third party's
    modifications of Covered Software, or (ii) the combination of its
    Contributions with other software (except as part of its Contributor
    Version); or

(c) under Patent Claims infringed by Covered Software in the absence of
    its Contributions.

This License does not grant any rights in the trademarks, service marks,
or logos of any Contributor (except as may be necessary to comply with
the notice requirements in Section 3.4).

2.4. Subsequent Licenses

No Contributor makes additional grants as a result of Your choice to
distribute the Covered Software under a subsequent version of this
License (see Section 10.2) or under the terms of a Secondary License (if
permitted under the terms of Section 3.3).

2.5. Representation

Each Contributor represents that the Contributor believes its
Contributions are its original creation(s) or it has sufficient rights
to grant the rights to its Contributions conveyed by this License.

2.6. Fair Use

This License is not intended to limit any rights You have under
applicable copyright doctrines of fair use, fair dealing, or other
equivalents.

2.7. Conditions

Sections 3.1, 3.2, 3.3, and 3.4 are conditions of the licenses granted
in Section 2.1.

3. Responsibilities
-------------------

3.1. Distribution of Source Form

All distribution of Covered Software in Source Code Form, including any
Modifications that You create or to which You contribute, must be under
the terms of this License. You must inform recipients that the Source
Code Form of the Covered Software is governed by the terms of this
License, and how they can obtain a copy of this License. You may not
attempt to alter or restrict the recipients' rights in the Source Code
Form.

3.2. Distribution of Executable Form

If You distribute Covered Software in Executable Form then:

(a) such Covered Software must also be made available in Source Code
    Form, as described in Section 3.1, and You must inform recipients of
    the Executable Form how they can obtain a copy of such Source Code
    Form by reasonable means in a timely manner, at a charge no more
    than the cost of distribution to the recipient; and

(b) You may distribute such Executable Form under the terms of this
    License, or sublicense it under different terms, provided that the
    license for the Executable Form does not attempt to limit or alter
    the recipients' rights in the Source Code Form under this License.

3.3. Distribution of a Larger Work

You may create and distribute a Larger Work under terms of Your choice,
provided that You also comply with the requirements of this License for
the Covered Software. If the Larger Work is a combination of Covered
Software with a work governed by one or more Secondary Licenses, and the
Covered Software is not Incompatible With Secondary Licenses, this
License permits You to additionally distribute such Covered Software
under the terms of such Secondary License(s), so that the recipient of
the Larger Work may, at their option, further distribute the Covered
Software under the terms of either this License or such Secondary
License(s).

3.4. Notices

You may not remove or alter the substance of any license notices
(including copyright notices, patent notices, disclaimers of warranty,
or limitations of liability) contained within the Source Code Form of
the Covered Software, except that You may alter any license notices to
the extent required to remedy known factual inaccuracies.

3.5. Application of Additional Terms

You may choose to offer, and to charge a fee for, warranty, support,
indemnity or liability obligations to one or more recipients of Covered
Software. However, You may do so only on Your own behalf, and not on
behalf of any Contributor. You must make it absolutely clear that any
such warranty, support, indemnity, or liability obligation is offered by
You alone, and You hereby agree to indemnify every Contributor for any
liability incurred by such Contributor as a result of warranty, support,
indemnity or liability terms You offer. You may include additional
disclaimers of warranty and limitations of liability specific to any
jurisdiction.

4. Inability to Comply Due to Statute or Regulation
---------------------------------------------------

If it is impossible for You to comply with any of the terms of this
License with respect to some or all of the Covered Software due to
statute, judicial order, or regulation then You must: (a) comply with
the terms of this License to the maximum extent possible; and (b)
describe the limitations and the code they affect. Such description must
be placed in a text file included with all distributions of the Covered
Software under this License. Except to the extent prohibited by statute
or regulation, such description must be sufficiently detailed for a
recipient of ordinary skill to be able to understand it.

5. Termination
--------------

5.1. The rights granted under this License will terminate automatically
if You fail to comply with any of its terms. However, if You become
compliant, then the rights granted under this License from a particular
Contributor are reinstated (a) provisionally, unless and until such
Contributor explicitly and finally terminates Your grants, and (b) on an
ongoing basis, if such Contributor fails to notify You of the
non-compliance by some reasonable means prior to 60 days after You have
come back into compliance. Moreover, Your grants from a particular
Contributor are reinstated on an ongoing basis if such Contributor
notifies You of the non-compliance by some reasonable means, this is the
first time You have received notice of non-compliance with this License
from such Contributor, and You become compliant prior to 30 days after
Your receipt of the notice.

5.2. If You initiate litigation against any entity by asserting a patent
infringement claim (excluding declaratory judgment actions,
counter-claims, and cross-claims) alleging that a Contributor Version
directly or indirectly infringes any patent, then the rights granted to
You by any and all Contributors for the Covered Software under Section
2.1 of this License shall terminate.

5.3. In the event of termination under Sections 5.1 or 5.2 above, all
end user license agreements (excluding distributors and resellers) which
have been validly granted by You or Your distributors under this License
prior to termination shall survive termination.

************************************************************************
*                                                                      *
*  6. Disclaimer of Warranty                                           *
*  -------------------------                                           *
*                                                                      *
*  Covered Software is provided under this License on an "as is"       *
*  basis, without warranty of any kind, either expressed, implied, or  *
*  statutory, including, without limitation, warranties that the       *
*  Covered Software is free of defects, merchantable, fit for a        *
*  particular purpose or non-infringing. The entire risk as to the     *
*  quality and performance of the Covered Software is with You.        *
*  Should any Covered Software prove defective in any respect, You     *
*  (not any Contributor) assume the cost of any necessary servicing,   *
*  repair, or correction. This disclaimer of warranty constitutes an   *
*  essential part of this License. No use of any Covered Software is   *
*  authorized under this License except under this disclaimer.         *
*                                                                      *
************************************************************************

************************************************************************
*                                                                      *
*  7. Limitation of Liability                                          *
*  --------------------------                                          *
*                                                                      *
*  Under no circumstances and under no legal theory, whether tort      *
*  (including negligence), contract, or otherwise, shall any           *
*  Contributor, or anyone who distributes Covered Software as          *
*  permitted above, be liable to You for any direct, indirect,         *
*  special, incidental, or consequential damages of any character      *
*  including, without limitation, damages for lost profits, loss of    *
*  goodwill, work stoppage, computer failure or malfunction, or any    *
*  and all other commercial damages or losses, even if such party      *
*  shall have been informed of the possibility of such damages. This   *
*  limitation of liability shall not apply to liability for death or   *
*  personal injury resulting from such party's negligence to the       *
*  extent applicable law prohibits such limitation. Some               *
*  jurisdictions do not allow the exclusion or limitation of           *
*  incidental or consequential damages, so this exclusion and          *
*  limitation may not apply to You.                                    *
*                                                                      *
************************************************************************

8. Litigation
-------------

Any litigation relating to this License may be brought only in the
courts of a jurisdiction where the defendant maintains its principal
place of business and such litigation shall be governed by laws of that
jurisdiction, without reference to its conflict-of-law provisions.
Nothing in this Section shall prevent a party's ability to bring
cross-claims or counter-claims.

9. Miscellaneous
----------------

This License represents the complete agreement concerning the subject
matter hereof. If any provision of this License is held to be
unenforceable, such provision shall be reformed only to the extent
necessary to make it enforceable. Any law or regulation which provides
that the language of a contract shall be construed against the drafter
shall not be used to construe this License against a Contributor.

10. Versions of the License
---------------------------

10.1. New Versions

Mozilla Foundation is the license steward. Except as provided in Section
10.3, no one other than the license steward has the right to modify or
publish new versions of this License. Each version will be given a
distinguishing version number.

10.2. Effect of New Versions

You may distribute the Covered Software under the terms of the version
of the License under which You originally received the Covered Software,
or under the terms of any subsequent version published by the license
steward.

10.3. Modified Versions

If you create software not governed by this License, and you want to
create a new license for such software, you may create and use a
modified version of this License if you rename the license and remove
any references to the name of the license steward (except to note that
such modified license differs from this License).

10.4. Distributing Source Code Form that is Incompatible With Secondary
Licenses

If You choose to distribute Source Code Form that is Incompatible With
Secondary Licenses under the terms of this version of the License, the
notice described in Exhibit B of this License must be attached.

Exhibit A - Source Code Form License Notice
-------------------------------------------

  This Source Code Form is subject to the terms of the Mozilla Public
  License, v. 2.0. If a copy of the MPL was not distributed with this
  file, You can obtain one at http://mozilla.org/MPL/2.0/.

If it is not possible or desirable to put the notice in a particular
file, then You may include the notice in a location (such as a LICENSE
file in a relevant directory) where a recipient would be likely to look
for such a notice.

You may add additional accurate notices of copyright ownership.

Exhibit B - "Incompatible With Secondary Licenses" Notice
---------------------------------------------------------

  This Source Code Form is "Incompatible With Secondary Licenses", as
  defined by the Mozilla Public License, v. 2.0.

```

`deepcheat/README.md`:

```md
# models
Repository for model development and training for the Waldo Vision project

## Code Standards
We use *pylint* as our Python code linter of choice.
Please ensure that all problems detected by *pylint* are resolved before opening a pull request.
We have configured an automated *pylint* github action upon pull request that will scan code for issues (currently broken).

The easiest way to run *pylint* is to download an extension for your IDE of choice.
For example, VSCode provides Python and Pylint extensions that will automatically show you issues detected by Pylint in the 'problems' section of the integrated terminal.

You can also install *pylint* manually with your package manager of choice and run from the command line using the instructions here: https://docs.pylint.org/run.html

## Getting Started
1. Setup your environment by following the steps listed in Setup.md
2. Check out the Github Issues page, where we're keeping track of project progress
3. Hop in the Waldo Vision discord and chat with our devs to find out what the highest priorities are
```

`deepcheat/Setup.md`:

```md
# Development Environment Setup
- Install Anaconda on server
- Install 
  - pandas
  - requests
  - validators
  - pytube
- The above list will most likely be outdated over time, or entirely removed.
  - Consider using the `install-environment.sh` script instead.
```

`deepcheat/VideoMAEv2/dataset/__init__.py`:

```py
from .build import build_dataset, build_pretraining_dataset

__all__ = ['build_dataset', 'build_pretraining_dataset']

```

`deepcheat/VideoMAEv2/dataset/build.py`:

```py
# --------------------------------------------------------
# Based on BEiT, timm, DINO and DeiT code bases
# https://github.com/microsoft/unilm/tree/master/beit
# https://github.com/rwightman/pytorch-image-models/tree/master/timm
# https://github.com/facebookresearch/deit
# https://github.com/facebookresearch/dino
# --------------------------------------------------------'
import os

from .datasets import RawFrameClsDataset, VideoClsDataset, KillshotDataset
from .pretrain_datasets import (  # noqa: F401
    DataAugmentationForVideoMAEv2, HybridVideoMAE, VideoMAE,
)


def build_pretraining_dataset(args, test_mode=False):
    transform = DataAugmentationForVideoMAEv2(args)
    dataset = VideoMAE(
        root=args.data_root,
        setting=args.data_path,
        train=True,
        test_mode=test_mode,
        name_pattern=args.fname_tmpl,
        video_ext='mp4',
        is_color=True,
        modality='rgb',
        num_segments=1,
        num_crop=1,
        new_length=args.num_frames,
        new_step=args.sampling_rate,
        transform=transform,
        temporal_jitter=False,
        lazy_init=False,
        num_sample=args.num_sample)
    print("Data Aug = %s" % str(transform))
    return dataset


def build_dataset(is_train, test_mode, args):
    if is_train:
        mode = 'train'
        anno_path = os.path.join(args.data_path, 'train.csv')
    elif test_mode:
        mode = 'test'
        anno_path = os.path.join(args.data_path, 'val.csv')
    else:
        mode = 'validation'
        anno_path = os.path.join(args.data_path, 'val.csv')

    if args.data_set == 'Kinetics-400':
        if not args.sparse_sample:
            dataset = VideoClsDataset(
                anno_path=anno_path,
                data_root=args.data_root,
                mode=mode,
                clip_len=args.num_frames,
                frame_sample_rate=args.sampling_rate,
                num_segment=1,
                test_num_segment=args.test_num_segment,
                test_num_crop=args.test_num_crop,
                num_crop=1 if not test_mode else 3,
                keep_aspect_ratio=True,
                crop_size=args.input_size,
                short_side_size=args.short_side_size,
                new_height=256,
                new_width=320,
                sparse_sample=False,
                args=args)
        else:
            dataset = VideoClsDataset(
                anno_path=anno_path,
                data_root=args.data_root,
                mode=mode,
                clip_len=1,
                frame_sample_rate=1,
                num_segment=args.num_frames,
                test_num_segment=args.test_num_segment,
                test_num_crop=args.test_num_crop,
                num_crop=1 if not test_mode else 3,
                keep_aspect_ratio=True,
                crop_size=args.input_size,
                short_side_size=args.short_side_size,
                new_height=256,
                new_width=320,
                sparse_sample=True,
                args=args)
        nb_classes = 400

    elif args.data_set == 'Kinetics-600':
        dataset = VideoClsDataset(
            anno_path=anno_path,
            data_root=args.data_root,
            mode=mode,
            clip_len=args.num_frames,
            frame_sample_rate=args.sampling_rate,
            num_segment=1,
            test_num_segment=args.test_num_segment,
            test_num_crop=args.test_num_crop,
            num_crop=1 if not test_mode else 3,
            keep_aspect_ratio=True,
            crop_size=args.input_size,
            short_side_size=args.short_side_size,
            new_height=256,
            new_width=320,
            args=args)
        nb_classes = 600

    elif args.data_set == 'Kinetics-700':
        dataset = VideoClsDataset(
            anno_path=anno_path,
            data_root=args.data_root,
            mode=mode,
            clip_len=args.num_frames,
            frame_sample_rate=args.sampling_rate,
            num_segment=1,
            test_num_segment=args.test_num_segment,
            test_num_crop=args.test_num_crop,
            num_crop=1 if not test_mode else 3,
            keep_aspect_ratio=True,
            crop_size=args.input_size,
            short_side_size=args.short_side_size,
            new_height=256,
            new_width=320,
            args=args)
        nb_classes = 700

    elif args.data_set == 'Kinetics-710':
        dataset = VideoClsDataset(
            anno_path=anno_path,
            data_root=args.data_root,
            mode=mode,
            clip_len=args.num_frames,
            frame_sample_rate=args.sampling_rate,
            num_segment=1,
            test_num_segment=args.test_num_segment,
            test_num_crop=args.test_num_crop,
            num_crop=1 if not test_mode else 3,
            keep_aspect_ratio=True,
            crop_size=args.input_size,
            short_side_size=args.short_side_size,
            new_height=256,
            new_width=320,
            args=args)
        nb_classes = 710

    elif args.data_set == 'SSV2':
        dataset = RawFrameClsDataset(
            anno_path=anno_path,
            data_root=args.data_root,
            mode=mode,
            clip_len=1,
            num_segment=args.num_frames,
            test_num_segment=args.test_num_segment,
            test_num_crop=args.test_num_crop,
            num_crop=1 if not test_mode else 3,
            keep_aspect_ratio=True,
            crop_size=args.input_size,
            short_side_size=args.short_side_size,
            new_height=256,
            new_width=320,
            filename_tmpl=args.fname_tmpl,
            start_idx=args.start_idx,
            args=args)

        nb_classes = 174

    elif args.data_set == 'UCF101':
        dataset = VideoClsDataset(
            anno_path=anno_path,
            data_root=args.data_root,
            mode=mode,
            clip_len=args.num_frames,
            frame_sample_rate=args.sampling_rate,
            num_segment=1,
            test_num_segment=args.test_num_segment,
            test_num_crop=args.test_num_crop,
            num_crop=1 if not test_mode else 3,
            keep_aspect_ratio=True,
            crop_size=args.input_size,
            short_side_size=args.short_side_size,
            new_height=256,
            new_width=320,
            args=args)
        nb_classes = 101

    elif args.data_set == 'HMDB51':
        dataset = VideoClsDataset(
            anno_path=anno_path,
            data_root=args.data_root,
            mode=mode,
            clip_len=args.num_frames,
            frame_sample_rate=args.sampling_rate,
            num_segment=1,
            test_num_segment=args.test_num_segment,
            test_num_crop=args.test_num_crop,
            num_crop=1 if not test_mode else 3,
            keep_aspect_ratio=True,
            crop_size=args.input_size,
            short_side_size=args.short_side_size,
            new_height=256,
            new_width=320,
            args=args)
        nb_classes = 51

    elif args.data_set == 'Diving48':
        dataset = VideoClsDataset(
            anno_path=anno_path,
            data_root=args.data_root,
            mode=mode,
            clip_len=args.num_frames,
            frame_sample_rate=args.sampling_rate,
            num_segment=1,
            test_num_segment=args.test_num_segment,
            test_num_crop=args.test_num_crop,
            num_crop=1 if not test_mode else 3,
            keep_aspect_ratio=True,
            crop_size=args.input_size,
            short_side_size=args.short_side_size,
            new_height=256,
            new_width=320,
            args=args)
        nb_classes = 48
    elif args.data_set == 'MIT':
        if not args.sparse_sample:
            dataset = VideoClsDataset(
                anno_path=anno_path,
                data_root=args.data_root,
                mode=mode,
                clip_len=args.num_frames,
                frame_sample_rate=args.sampling_rate,
                num_segment=1,
                test_num_segment=args.test_num_segment,
                test_num_crop=args.test_num_crop,
                num_crop=1 if not test_mode else 3,
                keep_aspect_ratio=True,
                crop_size=args.input_size,
                short_side_size=args.short_side_size,
                new_height=256,
                new_width=320,
                sparse_sample=False,
                args=args)
        else:
            dataset = VideoClsDataset(
                anno_path=anno_path,
                data_root=args.data_root,
                mode=mode,
                clip_len=1,
                frame_sample_rate=1,
                num_segment=args.num_frames,
                test_num_segment=args.test_num_segment,
                test_num_crop=args.test_num_crop,
                num_crop=1 if not test_mode else 3,
                keep_aspect_ratio=True,
                crop_size=args.input_size,
                short_side_size=args.short_side_size,
                new_height=256,
                new_width=320,
                sparse_sample=True,
                args=args)
        nb_classes = 339
    elif args.data_set == "killshot":
        dataset = KillshotDataset(
                anno_path=os.path.join(args.data_root,"killshot",mode+".csv"),
                data_root=args.data_root,
                mode=mode,
                clip_len=args.num_frames,
                frame_sample_rate=1,
                num_segment=args.num_frames,
                test_num_segment=args.test_num_segment,
                test_num_crop=args.test_num_crop,
                num_crop=1 if not test_mode else 3,
                keep_aspect_ratio=True,
                crop_size=args.input_size,
                short_side_size=args.short_side_size,
                new_height=256,
                new_width=256,
                sparse_sample=True,
                args=args)
        nb_classes = 1
    elif args.data_set == "cheater":
        dataset = RawFrameClsDataset(
                anno_path=anno_path,
                data_root=args.data_root,
                mode=mode,
                clip_len=args.num_frames,
                num_segment=args.num_frames,
                test_num_segment=args.test_num_segment,
                test_num_crop=args.test_num_crop,
                num_crop=1 if not test_mode else 3,
                keep_aspect_ratio=True,
                crop_size=args.input_size,
                short_side_size=args.short_side_size,
                new_height=256,
                new_width=256,
                filename_tmpl=args.fname_tmpl,
                args=args)
        nb_classes = 1
        
    else:
        raise NotImplementedError('Unsupported Dataset')

    assert nb_classes == args.nb_classes
    print("Number of the class = %d" % args.nb_classes)

    return dataset, nb_classes

```

`deepcheat/VideoMAEv2/dataset/datasets.py`:

```py
# pylint: disable=line-too-long,too-many-lines,missing-docstring
import os
import warnings

import numpy as np
import pandas as pd
import torch
from torch.utils.data import Dataset
from torchvision import transforms

from . import video_transforms, volume_transforms
from .loader import get_image_loader, get_video_loader
from .random_erasing import RandomErasing
import random

class VideoClsDataset(Dataset):
    """Load your own video classification dataset."""

    def __init__(self,
                 anno_path,
                 data_root='',
                 mode='train',
                 clip_len=8,
                 frame_sample_rate=2,
                 crop_size=224,
                 short_side_size=256,
                 new_height=256,
                 new_width=340,
                 keep_aspect_ratio=True,
                 num_segment=1,
                 num_crop=1,
                 test_num_segment=10,
                 test_num_crop=3,
                 sparse_sample=False,
                 args=None):
        self.anno_path = anno_path
        self.data_root = data_root
        self.mode = mode
        self.clip_len = clip_len
        self.frame_sample_rate = frame_sample_rate
        self.crop_size = crop_size
        self.short_side_size = short_side_size
        self.new_height = new_height
        self.new_width = new_width
        self.keep_aspect_ratio = keep_aspect_ratio
        self.num_segment = num_segment
        self.test_num_segment = test_num_segment
        self.num_crop = num_crop
        self.test_num_crop = test_num_crop
        self.sparse_sample = sparse_sample
        self.args = args
        self.aug = False
        self.rand_erase = False

        if self.mode in ['train']:
            self.aug = True
            if self.args.reprob > 0:
                self.rand_erase = True

        self.video_loader = get_video_loader()

        cleaned = pd.read_csv(self.anno_path, header=None, delimiter=' ')
        self.dataset_samples = list(
            cleaned[0].apply(lambda row: os.path.join(self.data_root, row)))
        self.label_array = list(cleaned.values[:, 1])

        if (mode == 'train'):
            pass

        elif (mode == 'validation'):
            self.data_transform = video_transforms.Compose([
                video_transforms.Resize(
                    self.short_side_size, interpolation='bilinear'),
                video_transforms.CenterCrop(
                    size=(self.crop_size, self.crop_size)),
                volume_transforms.ClipToTensor(),
                video_transforms.Normalize(
                    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
            ])
        elif mode == 'test':
            self.data_resize = video_transforms.Compose([
                video_transforms.Resize(
                    size=(short_side_size), interpolation='bilinear')
            ])
            self.data_transform = video_transforms.Compose([
                volume_transforms.ClipToTensor(),
                video_transforms.Normalize(
                    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
            ])
            self.test_seg = []
            self.test_dataset = []
            self.test_label_array = []
            for ck in range(self.test_num_segment):
                for cp in range(self.test_num_crop):
                    for idx in range(len(self.label_array)):
                        sample_label = self.label_array[idx]
                        self.test_label_array.append(sample_label)
                        self.test_dataset.append(self.dataset_samples[idx])
                        self.test_seg.append((ck, cp))

    def __getitem__(self, index):
        if self.mode == 'train':
            args = self.args
            scale_t = 1

            sample = self.dataset_samples[index]
            # T H W C
            buffer = self.load_video(sample, sample_rate_scale=scale_t)
            if len(buffer) == 0:
                while len(buffer) == 0:
                    warnings.warn(
                        "video {} not correctly loaded during training".format(
                            sample))
                    index = np.random.randint(self.__len__())
                    sample = self.dataset_samples[index]
                    buffer = self.load_video(sample, sample_rate_scale=scale_t)

            if args.num_sample > 1:
                frame_list = []
                label_list = []
                index_list = []
                for _ in range(args.num_sample):
                    new_frames = self._aug_frame(buffer, args)
                    label = self.label_array[index]
                    frame_list.append(new_frames)
                    label_list.append(label)
                    index_list.append(index)
                return frame_list, label_list, index_list, {}
            else:
                buffer = self._aug_frame(buffer, args)

            return buffer, self.label_array[index], index, {}

        elif self.mode == 'validation':
            sample = self.dataset_samples[index]
            buffer = self.load_video(sample)
            if len(buffer) == 0:
                while len(buffer) == 0:
                    warnings.warn(
                        "video {} not correctly loaded during validation".
                        format(sample))
                    index = np.random.randint(self.__len__())
                    sample = self.dataset_samples[index]
                    buffer = self.load_video(sample)
            buffer = self.data_transform(buffer)
            return buffer, self.label_array[index], sample.split(
                "/")[-1].split(".")[0]

        elif self.mode == 'test':
            sample = self.test_dataset[index]
            chunk_nb, split_nb = self.test_seg[index]
            buffer = self.load_video(sample)

            while len(buffer) == 0:
                warnings.warn(
                    "video {}, temporal {}, spatial {} not found during testing"
                    .format(str(self.test_dataset[index]), chunk_nb, split_nb))
                index = np.random.randint(self.__len__())
                sample = self.test_dataset[index]
                chunk_nb, split_nb = self.test_seg[index]
                buffer = self.load_video(sample)

            buffer = self.data_resize(buffer)
            if isinstance(buffer, list):
                buffer = np.stack(buffer, 0)

            if self.sparse_sample:
                spatial_step = 1.0 * (max(buffer.shape[1], buffer.shape[2]) -
                                      self.short_side_size) / (
                                          self.test_num_crop - 1)
                temporal_start = chunk_nb
                spatial_start = int(split_nb * spatial_step)
                if buffer.shape[1] >= buffer.shape[2]:
                    buffer = buffer[temporal_start::self.test_num_segment,
                                    spatial_start:spatial_start +
                                    self.short_side_size, :, :]
                else:
                    buffer = buffer[temporal_start::self.test_num_segment, :,
                                    spatial_start:spatial_start +
                                    self.short_side_size, :]
            else:
                spatial_step = 1.0 * (max(buffer.shape[1], buffer.shape[2]) -
                                      self.short_side_size) / (
                                          self.test_num_crop - 1)
                temporal_step = max(
                    1.0 * (buffer.shape[0] - self.clip_len) /
                    (self.test_num_segment - 1), 0)
                temporal_start = int(chunk_nb * temporal_step)
                spatial_start = int(split_nb * spatial_step)
                if buffer.shape[1] >= buffer.shape[2]:
                    buffer = buffer[temporal_start:temporal_start +
                                    self.clip_len,
                                    spatial_start:spatial_start +
                                    self.short_side_size, :, :]
                else:
                    buffer = buffer[temporal_start:temporal_start +
                                    self.clip_len, :,
                                    spatial_start:spatial_start +
                                    self.short_side_size, :]

            buffer = self.data_transform(buffer)
            return buffer, self.test_label_array[index], sample.split(
                "/")[-1].split(".")[0], chunk_nb, split_nb
        else:
            raise NameError('mode {} unkown'.format(self.mode))

    def _aug_frame(self, buffer, args):
        aug_transform = video_transforms.create_random_augment(
            input_size=(self.crop_size, self.crop_size),
            auto_augment=args.aa,
            interpolation=args.train_interpolation,
        )

        buffer = [transforms.ToPILImage()(frame) for frame in buffer]

        buffer = aug_transform(buffer)

        buffer = [transforms.ToTensor()(img) for img in buffer]
        buffer = torch.stack(buffer)  # T C H W
        buffer = buffer.permute(0, 2, 3, 1)  # T H W C

        # T H W C
        buffer = tensor_normalize(buffer, [0.485, 0.456, 0.406],
                                  [0.229, 0.224, 0.225])
        # T H W C -> C T H W.
        buffer = buffer.permute(3, 0, 1, 2)
        # Perform data augmentation.
        scl, asp = (
            [0.08, 1.0],
            [0.75, 1.3333],
        )

        buffer = spatial_sampling(
            buffer,
            spatial_idx=-1,
            min_scale=256,
            max_scale=320,
            # crop_size=224,
            crop_size=args.input_size,
            random_horizontal_flip=False if args.data_set == 'SSV2' else True,
            inverse_uniform_sampling=False,
            aspect_ratio=asp,
            scale=scl,
            motion_shift=False)

        if self.rand_erase:
            erase_transform = RandomErasing(
                args.reprob,
                mode=args.remode,
                max_count=args.recount,
                num_splits=args.recount,
                device="cpu",
            )
            buffer = buffer.permute(1, 0, 2, 3)  # C T H W -> T C H W
            buffer = erase_transform(buffer)
            buffer = buffer.permute(1, 0, 2, 3)  # T C H W -> C T H W

        return buffer

    def load_video(self, sample, sample_rate_scale=1):
        fname = sample

        try:
            vr = self.video_loader(fname)
        except Exception as e:
            print(f"Failed to load video from {fname} with error {e}!")
            return []

        length = len(vr)

        if self.mode == 'test':
            if self.sparse_sample:
                tick = length / float(self.num_segment)
                all_index = []
                for t_seg in range(self.test_num_segment):
                    tmp_index = [
                        int(t_seg * tick / self.test_num_segment + tick * x)
                        for x in range(self.num_segment)
                    ]
                    all_index.extend(tmp_index)
                all_index = list(np.sort(np.array(all_index)))
            else:
                all_index = [
                    x for x in range(0, length, self.frame_sample_rate)
                ]
                while len(all_index) < self.clip_len:
                    all_index.append(all_index[-1])

            vr.seek(0)
            buffer = vr.get_batch(all_index).asnumpy()
            return buffer

        # handle temporal segments
        converted_len = int(self.clip_len * self.frame_sample_rate)
        seg_len = length // self.num_segment

        all_index = []
        for i in range(self.num_segment):
            if seg_len <= converted_len:
                index = np.linspace(
                    0, seg_len, num=seg_len // self.frame_sample_rate)
                index = np.concatenate(
                    (index,
                     np.ones(self.clip_len - seg_len // self.frame_sample_rate)
                     * seg_len))
                index = np.clip(index, 0, seg_len - 1).astype(np.int64)
            else:
                if self.mode == 'validation':
                    end_idx = (converted_len + seg_len) // 2
                else:
                    end_idx = np.random.randint(converted_len, seg_len)
                str_idx = end_idx - converted_len
                index = np.linspace(str_idx, end_idx, num=self.clip_len)
                index = np.clip(index, str_idx, end_idx - 1).astype(np.int64)
            index = index + i * seg_len
            all_index.extend(list(index))

        all_index = all_index[::int(sample_rate_scale)]
        vr.seek(0)
        buffer = vr.get_batch(all_index).asnumpy()
        return buffer

    def __len__(self):
        if self.mode != 'test':
            return len(self.dataset_samples)
        else:
            return len(self.test_dataset)


class RawFrameClsDataset(Dataset):
    """Load your own raw frame classification dataset."""

    def __init__(self,
                 anno_path,
                 data_root,
                 mode='train',
                 clip_len=8,
                 crop_size=224,
                 short_side_size=256,
                 new_height=256,
                 new_width=340,
                 keep_aspect_ratio=True,
                 num_segment=1,
                 num_crop=1,
                 test_num_segment=1,
                 test_num_crop=1,
                 filename_tmpl='img_{:05}.jpg',
                 start_idx=1,
                 args=None):
        self.anno_path = anno_path
        self.data_root = data_root
        self.mode = mode
        self.clip_len = clip_len
        self.crop_size = crop_size
        self.short_side_size = short_side_size
        self.new_height = new_height
        self.new_width = new_width
        self.keep_aspect_ratio = keep_aspect_ratio
        self.num_segment = num_segment
        self.test_num_segment = test_num_segment
        self.num_crop = num_crop
        self.test_num_crop = test_num_crop
        self.filename_tmpl = filename_tmpl
        self.start_idx = start_idx
        self.args = args
        self.aug = False
        self.rand_erase = False

        if self.mode in ['train']:
            self.aug = True
            if self.args.reprob > 0:
                self.rand_erase = True

        self.image_loader = get_image_loader()
        cleaned = pd.read_csv(self.anno_path, header=None, delimiter=' ')
        self.dataset_samples = list(
            cleaned[0].apply(lambda row: os.path.join(self.data_root, row)))
        self.total_frames = list(cleaned.values[:, 1])
        self.label_array = list(cleaned.values[:, -1])

        if (mode == 'train'):
            pass

        elif (mode == 'validation'):
            self.data_transform = video_transforms.Compose([
                video_transforms.Resize(
                    self.short_side_size, interpolation='bilinear'),
                video_transforms.CenterCrop(
                    size=(self.crop_size, self.crop_size)),
                volume_transforms.ClipToTensor(),
                video_transforms.Normalize(
                    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
            ])
        elif mode == 'test':
            self.data_resize = video_transforms.Compose([
                video_transforms.Resize(
                    size=(short_side_size), interpolation='bilinear')
            ])
            self.data_transform = video_transforms.Compose([
                volume_transforms.ClipToTensor(),
                video_transforms.Normalize(
                    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
            ])
            self.test_seg = []
            self.test_dataset = []
            self.test_total_frames = []
            self.test_label_array = []
            for ck in range(self.test_num_segment):
                for cp in range(self.test_num_crop):
                    for idx in range(len(self.label_array)):
                        self.test_seg.append((ck, cp))
                        self.test_dataset.append(self.dataset_samples[idx])
                        self.test_total_frames.append(self.total_frames[idx])
                        self.test_label_array.append(self.label_array[idx])

    def __getitem__(self, index):
        if self.mode == 'train':
            args = self.args
            scale_t = 1

            sample = self.dataset_samples[index]
            total_frame = self.total_frames[index]
            buffer = self.load_frame(
                sample, total_frame, sample_rate_scale=scale_t)  # T H W C
            if len(buffer) == 0:
                while len(buffer) == 0:
                    warnings.warn(
                        "video {} not correctly loaded during training".format(
                            sample))
                    index = np.random.randint(self.__len__())
                    sample = self.dataset_samples[index]
                    total_frame = self.total_frames[index]
                    buffer = self.load_frame(
                        sample, total_frame, sample_rate_scale=scale_t)

            if args.num_sample > 1:
                frame_list = []
                label_list = []
                index_list = []
                for _ in range(args.num_sample):
                    new_frames = self._aug_frame(buffer, args)
                    label = self.label_array[index]
                    frame_list.append(new_frames)
                    label_list.append(label)
                    index_list.append(index)
                return frame_list, label_list, index_list, {}
            else:
                buffer = self._aug_frame(buffer, args)

            return buffer, self.label_array[index], index, {}

        elif self.mode == 'validation':
            sample = self.dataset_samples[index]
            total_frame = self.total_frames[index]
            buffer = self.load_frame(sample, total_frame)
            if len(buffer) == 0:
                while len(buffer) == 0:
                    warnings.warn(
                        "video {} not correctly loaded during validation".
                        format(sample))
                    index = np.random.randint(self.__len__())
                    sample = self.dataset_samples[index]
                    buffer = self.load_frame(sample, total_frame)
            buffer = self.data_transform(buffer)
            return buffer, self.label_array[index], sample.split(
                "/")[-1].split(".")[0]

        elif self.mode == 'test':
            sample = self.test_dataset[index]
            total_frame = self.test_total_frames[index]
            chunk_nb, split_nb = self.test_seg[index]
            buffer = self.load_frame(sample, total_frame)

            while len(buffer) == 0:
                warnings.warn(
                    "video {}, temporal {}, spatial {} not found during testing"
                    .format(str(self.test_dataset[index]), chunk_nb, split_nb))
                index = np.random.randint(self.__len__())
                sample = self.test_dataset[index]
                total_frame = self.test_total_frames[index]
                chunk_nb, split_nb = self.test_seg[index]
                buffer = self.load_frame(sample, total_frame)

            buffer = self.data_resize(buffer)
            if isinstance(buffer, list):
                buffer = np.stack(buffer, 0)

            spatial_step = 1.0 * (max(buffer.shape[1], buffer.shape[2]) -
                                  self.short_side_size) / (
                                      self.test_num_crop )
            temporal_start = chunk_nb
            spatial_start = int(split_nb * spatial_step)
            if buffer.shape[1] >= buffer.shape[2]:
                buffer = buffer[temporal_start::self.test_num_segment,
                                spatial_start:spatial_start +
                                self.short_side_size, :, :]
            else:
                buffer = buffer[temporal_start::self.test_num_segment, :,
                                spatial_start:spatial_start +
                                self.short_side_size, :]

            buffer = self.data_transform(buffer)
            return buffer, self.test_label_array[index], sample.split(
                "/")[-1].split(".")[0], chunk_nb, split_nb
        else:
            raise NameError('mode {} unkown'.format(self.mode))

    def _aug_frame(self, buffer, args):
        aug_transform = video_transforms.create_random_augment(
            input_size=(self.crop_size, self.crop_size),
            auto_augment=args.aa,
            interpolation=args.train_interpolation,
        )

        buffer = [transforms.ToPILImage()(frame) for frame in buffer]

        buffer = aug_transform(buffer)

        buffer = [transforms.ToTensor()(img) for img in buffer]
        buffer = torch.stack(buffer)  # T C H W
        buffer = buffer.permute(0, 2, 3, 1)  # T H W C

        # T H W C
        buffer = tensor_normalize(buffer, [0.485, 0.456, 0.406],
                                  [0.229, 0.224, 0.225])
        # T H W C -> C T H W.
        buffer = buffer.permute(3, 0, 1, 2)
        # Perform data augmentation.
        scl, asp = (
            [0.08, 1.0],
            [0.75, 1.3333],
        )

        buffer = spatial_sampling(
            buffer,
            spatial_idx=-1,
            min_scale=256,
            max_scale=320,
            crop_size=self.crop_size,
            random_horizontal_flip=False if args.data_set == 'SSV2' else True,
            inverse_uniform_sampling=False,
            aspect_ratio=asp,
            scale=scl,
            motion_shift=False)

        if self.rand_erase:
            erase_transform = RandomErasing(
                args.reprob,
                mode=args.remode,
                max_count=args.recount,
                num_splits=args.recount,
                device="cpu",
            )
            buffer = buffer.permute(1, 0, 2, 3)
            buffer = erase_transform(buffer)
            buffer = buffer.permute(1, 0, 2, 3)

        return buffer

    def load_frame(self, sample, num_frames, sample_rate_scale=1):
        """Load video content using Decord"""
        fname = sample

        if self.mode == 'test':
            tick = num_frames / float(self.num_segment)
            all_index = []
            for t_seg in range(self.test_num_segment):
                tmp_index = [
                    int(t_seg * tick / self.test_num_segment + tick * x)
                    for x in range(self.num_segment)
                ]
                all_index.extend(tmp_index)
            all_index = list(np.sort(np.array(all_index) + self.start_idx))
            imgs = []
            for idx in all_index:
                frame_fname = os.path.join(fname,
                                           self.filename_tmpl.format(idx))
                img = self.image_loader(frame_fname)
                imgs.append(img)
            buffer = np.array(imgs)
            return buffer

        # handle temporal segments
        average_duration = num_frames // self.num_segment
        all_index = []
        if average_duration > 0:
            if self.mode == 'validation':
                all_index = list(
                    np.multiply(
                        list(range(self.num_segment)), average_duration) +
                    np.ones(self.num_segment, dtype=int) *
                    (average_duration // 2))
            else:
                all_index = list(
                    np.multiply(
                        list(range(self.num_segment)), average_duration) +
                    np.random.randint(average_duration, size=self.num_segment))
        elif num_frames > self.num_segment:
            if self.mode == 'validation':
                all_index = list(range(self.num_segment))
            else:
                all_index = list(
                    np.sort(
                        np.random.randint(num_frames, size=self.num_segment)))
        else:
            all_index = [0] * (self.num_segment - num_frames) + list(
                range(num_frames))
        all_index = list(np.array(all_index) + self.start_idx)
        imgs = []
        for idx in all_index:
            frame_fname = os.path.join(fname, self.filename_tmpl.format(idx))
            img = self.image_loader(frame_fname)
            imgs.append(img)
        buffer = np.array(imgs)
        return buffer

    def __len__(self):
        if self.mode != 'test':
            return len(self.dataset_samples)
        else:
            return len(self.test_dataset)


class KillshotDataset(Dataset):

    def __init__(self,
                 anno_path,
                 data_root,
                 mode='train',
                 clip_len=8,
                 crop_size=224,
                 short_side_size=256,
                 new_height=256,
                 new_width=340,
                 keep_aspect_ratio=True,
                 frame_sample_rate=1,
                 sparse_sample=1,
                 num_segment=1,
                 num_crop=1,
                 test_num_segment=10,
                 test_num_crop=3,
                 filename_tmpl='img_{:010}.jpg',
                 start_idx=1,
                 args=None):
        self.anno_path = anno_path
        self.data_root = data_root
        self.mode = mode
        self.clip_len = clip_len
        self.crop_size = crop_size
        self.short_side_size = short_side_size
        self.new_height = new_height
        self.new_width = new_width
        self.keep_aspect_ratio = keep_aspect_ratio
        self.num_segment = num_segment
        self.test_num_segment = test_num_segment
        self.num_crop = num_crop
        self.test_num_crop = test_num_crop
        self.filename_tmpl = filename_tmpl
        self.start_idx = start_idx
        self.args = args
        self.aug = False
        self.rand_erase = False
        self.nokill_prob = .25

        if self.mode in ['train']:
            self.aug = True
            if self.args.reprob > 0:
                self.rand_erase = True

        self.image_loader = get_image_loader()

        cleaned = pd.read_csv(self.anno_path, delimiter=',')

        self.frame_paths = []
        self.label_array = []

        for _, row in cleaned.iterrows():
            video_dir, _, frame_idx = row['path_to_frame_dir'], row['frame_of_kill'], row['index_of_frame']
            
            # Calculate start and end frame indices
            start_frame = max(0, frame_idx - clip_len + 1)
            end_frame = start_frame + (2*clip_len - 1) 

            frames = []
            for i in range(start_frame, end_frame):
                cur_path = os.path.join(video_dir, f"img_{i:010d}.jpg")
                if not os.path.exists(cur_path): break
                
                frames.append(cur_path)
            
            if len(frames) < clip_len: 
                raise ValueError(f"bad video clip length frame {frame_idx} dir {video_dir}")
            

            label = np.zeros(len(frames))
            
            # Adjust label position based on actual start frame
            label_position = frame_idx - start_frame
            label[label_position] = 1

            self.frame_paths.append(frames)
            self.label_array.append(label)

        kill_frames_set = set()
        video_dirs = set()

        for _, row in cleaned.iterrows():
            kill_frames_set.add((row['path_to_frame_dir'], row['index_of_frame']))
            video_dirs.add(row['path_to_frame_dir'])

        # Compute max frame for each video directory
        max_frame_dict = {}
        for video_dir, frame_idx in kill_frames_set:
            if video_dir not in max_frame_dict:
                max_frame_dict[video_dir] = frame_idx
            else:
                max_frame_dict[video_dir] = max(max_frame_dict[video_dir], frame_idx)

       


        if (mode == 'train'):
             #breakpoint()
            non_kill_clip_count = 0
            while non_kill_clip_count < int(self.nokill_prob * len(self.frame_paths)):
                video_dir = random.choice(list(video_dirs))
                random_idx = random.randint(self.clip_len, max_frame_dict[video_dir] - self.clip_len)
                
                overlap_with_kill = any((video_dir, i) in kill_frames_set for i in range(random_idx - self.clip_len + 1, random_idx + self.clip_len))
                
                if overlap_with_kill: continue

                frames = [os.path.join(video_dir, f"img_{i:010d}.jpg") for i in range(random_idx - self.clip_len + 1, random_idx + self.clip_len)]
                
                label = np.zeros(len(frames))
                self.frame_paths.append(frames)
                self.label_array.append(label)
                non_kill_clip_count += 1

        elif (mode == 'validation'):
            self.data_transform = video_transforms.Compose([
                video_transforms.Resize(
                    self.short_side_size, interpolation='bilinear'),
                video_transforms.CenterCrop(
                    size=(self.crop_size, self.crop_size)),
                volume_transforms.ClipToTensor(),
                video_transforms.Normalize(
                    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
            ])
        elif mode == 'test':
            self.data_resize = video_transforms.Compose([
                video_transforms.Resize(
                    size=(short_side_size), interpolation='bilinear')
            ])
            self.data_transform = video_transforms.Compose([
                video_transforms.CenterCrop(
                    size=(self.crop_size, self.crop_size)),
                volume_transforms.ClipToTensor(),
                video_transforms.Normalize(
                    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
            ])
            self.test_seg = []
            self.test_dataset = []
            self.test_total_frames = []
            self.test_label_array = []
            for ck in range(self.test_num_segment):
                for cp in range(self.test_num_crop):
                    for idx in range(len(self.label_array)):
                        self.test_seg.append((ck, cp))
                        self.test_dataset.append(self.frame_paths[idx])
                        #self.test_total_frames.append(self.total_frames[idx])
                        self.test_label_array.append(self.label_array[idx])

    def __getitem__(self, index):
        if self.mode == 'train':
            args = self.args
            scale_t = 1


            start_idx = random.randint(0, len(self.frame_paths[index]) - self.clip_len)
            labels = self.label_array[index][start_idx:start_idx + self.clip_len]
            
            imgs = []
            for frame_fname in self.frame_paths[index][start_idx:start_idx + self.clip_len]:
                img = self.image_loader(frame_fname)
                imgs.append(img)
            buffer = np.array(imgs)

            if args.num_sample > 1:
                frame_list = []
                label_list = []
                index_list = []
                for _ in range(args.num_sample):
                    new_frames = self._aug_frame(buffer, args)
                    label = labels
                    frame_list.append(new_frames)
                    label_list.append(label)
                    index_list.append(index)
                return frame_list, label_list, index_list, {}
            else:
                buffer = self._aug_frame(buffer, args)

            return buffer, labels, index, {}

        elif self.mode == 'validation' or self.mode =='test':
            #if index > len(self.frame_paths):
            #    #breakpoint()
            

            imgs = []
            for frame_fname in self.frame_paths[index]:
                img = self.image_loader(frame_fname)
                imgs.append(img)
            buffer = np.array(imgs)
            #if buffer.shape[0] != 31:
            #    breakpoint()
            #    buffer = buffer
            buffer = self.data_transform(buffer)
            labels = self.label_array[index]

            return buffer, labels, index, {}
        else:
            raise NameError('mode {} unkown'.format(self.mode))

    def _aug_frame(self, buffer, args):
        aug_transform = video_transforms.create_random_augment(
            input_size=(self.crop_size, self.crop_size),
            auto_augment=args.aa,
            interpolation=args.train_interpolation,
        )

        buffer = [transforms.ToPILImage()(frame) for frame in buffer]

        buffer = aug_transform(buffer)

        buffer = [transforms.ToTensor()(img) for img in buffer]
        buffer = torch.stack(buffer)  # T C H W
        buffer = buffer.permute(0, 2, 3, 1)  # T H W C

        # T H W C
        buffer = tensor_normalize(buffer, [0.485, 0.456, 0.406],
                                  [0.229, 0.224, 0.225])
        # T H W C -> C T H W.
        buffer = buffer.permute(3, 0, 1, 2)
        # Perform data augmentation.
        scl, asp = (
            [0.08, 1.0],
            [0.75, 1.3333],
        )

        buffer = spatial_sampling(
            buffer,
            spatial_idx=-1,
            min_scale=256,
            max_scale=320,
            crop_size=self.crop_size,
            random_horizontal_flip=False if args.data_set == 'SSV2' else True,
            inverse_uniform_sampling=False,
            aspect_ratio=asp,
            scale=scl,
            motion_shift=False)

        if self.rand_erase:
            erase_transform = RandomErasing(
                args.reprob,
                mode=args.remode,
                max_count=args.recount,
                num_splits=args.recount,
                device="cpu",
            )
            buffer = buffer.permute(1, 0, 2, 3)
            buffer = erase_transform(buffer)
            buffer = buffer.permute(1, 0, 2, 3)

        return buffer

    def load_frame(self, sample, num_frames, sample_rate_scale=1):
        """Load video content using Decord"""
        fname = sample

        if self.mode == 'test':
            tick = num_frames / float(self.num_segment)
            all_index = []
            for t_seg in range(self.test_num_segment):
                tmp_index = [
                    int(t_seg * tick / self.test_num_segment + tick * x)
                    for x in range(self.num_segment)
                ]
                all_index.extend(tmp_index)
            all_index = list(np.sort(np.array(all_index) + self.start_idx))
            imgs = []
            for idx in all_index:
                frame_fname = os.path.join(fname,
                                           self.filename_tmpl.format(idx))
                img = self.image_loader(frame_fname)
                imgs.append(img)
            buffer = np.array(imgs)
            return buffer

        # handle temporal segments
        average_duration = num_frames // self.num_segment
        all_index = []
        if average_duration > 0:
            if self.mode == 'validation':
                all_index = list(
                    np.multiply(
                        list(range(self.num_segment)), average_duration) +
                    np.ones(self.num_segment, dtype=int) *
                    (average_duration // 2))
            else:
                all_index = list(
                    np.multiply(
                        list(range(self.num_segment)), average_duration) +
                    np.random.randint(average_duration, size=self.num_segment))
        elif num_frames > self.num_segment:
            if self.mode == 'validation':
                all_index = list(range(self.num_segment))
            else:
                all_index = list(
                    np.sort(
                        np.random.randint(num_frames, size=self.num_segment)))
        else:
            all_index = [0] * (self.num_segment - num_frames) + list(
                range(num_frames))
        all_index = list(np.array(all_index) + self.start_idx)
        imgs = []
        for idx in all_index:
            frame_fname = os.path.join(fname, self.filename_tmpl.format(idx))
            img = self.image_loader(frame_fname)
            imgs.append(img)
        buffer = np.array(imgs)
        return buffer

    def __len__(self):
        return len(self.frame_paths)


def spatial_sampling(
    frames,
    spatial_idx=-1,
    min_scale=256,
    max_scale=320,
    crop_size=224,
    random_horizontal_flip=True,
    inverse_uniform_sampling=False,
    aspect_ratio=None,
    scale=None,
    motion_shift=False,
):
    """
    Perform spatial sampling on the given video frames. If spatial_idx is
    -1, perform random scale, random crop, and random flip on the given
    frames. If spatial_idx is 0, 1, or 2, perform spatial uniform sampling
    with the given spatial_idx.
    Args:
        frames (tensor): frames of images sampled from the video. The
            dimension is `num frames` x `height` x `width` x `channel`.
        spatial_idx (int): if -1, perform random spatial sampling. If 0, 1,
            or 2, perform left, center, right crop if width is larger than
            height, and perform top, center, buttom crop if height is larger
            than width.
        min_scale (int): the minimal size of scaling.
        max_scale (int): the maximal size of scaling.
        crop_size (int): the size of height and width used to crop the
            frames.
        inverse_uniform_sampling (bool): if True, sample uniformly in
            [1 / max_scale, 1 / min_scale] and take a reciprocal to get the
            scale. If False, take a uniform sample from [min_scale,
            max_scale].
        aspect_ratio (list): Aspect ratio range for resizing.
        scale (list): Scale range for resizing.
        motion_shift (bool): Whether to apply motion shift for resizing.
    Returns:
        frames (tensor): spatially sampled frames.
    """
    assert spatial_idx in [-1, 0, 1, 2]
    if spatial_idx == -1:
        if aspect_ratio is None and scale is None:
            frames, _ = video_transforms.random_short_side_scale_jitter(
                images=frames,
                min_size=min_scale,
                max_size=max_scale,
                inverse_uniform_sampling=inverse_uniform_sampling,
            )
            frames, _ = video_transforms.random_crop(frames, crop_size)
        else:
            transform_func = (
                video_transforms.random_resized_crop_with_shift
                if motion_shift else video_transforms.random_resized_crop)
            frames = transform_func(
                images=frames,
                target_height=crop_size,
                target_width=crop_size,
                scale=scale,
                ratio=aspect_ratio,
            )
        if random_horizontal_flip:
            frames, _ = video_transforms.horizontal_flip(0.5, frames)
    else:
        # The testing is deterministic and no jitter should be performed.
        # min_scale, max_scale, and crop_size are expect to be the same.
        assert len({min_scale, max_scale, crop_size}) == 1
        frames, _ = video_transforms.random_short_side_scale_jitter(
            frames, min_scale, max_scale)
        frames, _ = video_transforms.uniform_crop(frames, crop_size,
                                                  spatial_idx)
    return frames


def tensor_normalize(tensor, mean, std):
    """
    Normalize a given tensor by subtracting the mean and dividing the std.
    Args:
        tensor (tensor): tensor to normalize.
        mean (tensor or list): mean value to subtract.
        std (tensor or list): std to divide.
    """
    if tensor.dtype == torch.uint8:
        tensor = tensor.float()
        tensor = tensor / 255.0
    if type(mean) == list:
        mean = torch.tensor(mean)
    if type(std) == list:
        std = torch.tensor(std)
    tensor = tensor - mean
    tensor = tensor / std
    return tensor

```

`deepcheat/VideoMAEv2/dataset/functional.py`:

```py
import numbers

import cv2
import numpy as np
import PIL
import torch


def _is_tensor_clip(clip):
    return torch.is_tensor(clip) and clip.ndimension() == 4


def crop_clip(clip, min_h, min_w, h, w):
    if isinstance(clip[0], np.ndarray):
        cropped = [img[min_h:min_h + h, min_w:min_w + w, :] for img in clip]

    elif isinstance(clip[0], PIL.Image.Image):
        cropped = [
            img.crop((min_w, min_h, min_w + w, min_h + h)) for img in clip
        ]
    else:
        raise TypeError('Expected numpy.ndarray or PIL.Image' +
                        'but got list of {0}'.format(type(clip[0])))
    return cropped


def resize_clip(clip, size, interpolation='bilinear'):
    if isinstance(clip[0], np.ndarray):
        if isinstance(size, numbers.Number):
            im_h, im_w, im_c = clip[0].shape
            # Min spatial dim already matches minimal size
            if (im_w <= im_h and im_w == size) or (im_h <= im_w
                                                   and im_h == size):
                return clip
            new_h, new_w = get_resize_sizes(im_h, im_w, size)
            size = (new_w, new_h)
        else:
            size = size[0], size[1]
        if interpolation == 'bilinear':
            np_inter = cv2.INTER_LINEAR
        else:
            np_inter = cv2.INTER_NEAREST
        scaled = [
            cv2.resize(img, size, interpolation=np_inter) for img in clip
        ]
    elif isinstance(clip[0], PIL.Image.Image):
        if isinstance(size, numbers.Number):
            im_w, im_h = clip[0].size
            # Min spatial dim already matches minimal size
            if (im_w <= im_h and im_w == size) or (im_h <= im_w
                                                   and im_h == size):
                return clip
            new_h, new_w = get_resize_sizes(im_h, im_w, size)
            size = (new_w, new_h)
        else:
            size = size[1], size[0]
        if interpolation == 'bilinear':
            pil_inter = PIL.Image.BILINEAR
        else:
            pil_inter = PIL.Image.NEAREST
        scaled = [img.resize(size, pil_inter) for img in clip]
    else:
        raise TypeError('Expected numpy.ndarray or PIL.Image' +
                        'but got list of {0}'.format(type(clip[0])))
    return scaled


def get_resize_sizes(im_h, im_w, size):
    if im_w < im_h:
        ow = size
        oh = int(size * im_h / im_w)
    else:
        oh = size
        ow = int(size * im_w / im_h)
    return oh, ow


def normalize(clip, mean, std, inplace=False):
    if not _is_tensor_clip(clip):
        raise TypeError('tensor is not a torch clip.')

    if not inplace:
        clip = clip.clone()

    dtype = clip.dtype
    mean = torch.as_tensor(mean, dtype=dtype, device=clip.device)
    std = torch.as_tensor(std, dtype=dtype, device=clip.device)
    clip.sub_(mean[:, None, None, None]).div_(std[:, None, None, None])

    return clip

```

`deepcheat/VideoMAEv2/dataset/loader.py`:

```py
import io

import cv2
import numpy as np
from decord import VideoReader, cpu

try:
    from petrel_client.client import Client
    petrel_backend_imported = True
except (ImportError, ModuleNotFoundError):
    petrel_backend_imported = False


def get_video_loader(use_petrel_backend: bool = True,
                     enable_mc: bool = True,
                     conf_path: str = None):
    if petrel_backend_imported and use_petrel_backend:
        _client = Client(conf_path=conf_path, enable_mc=enable_mc)
    else:
        _client = None

    def _loader(video_path):
        if _client is not None and 's3:' in video_path:
            video_path = io.BytesIO(_client.get(video_path))

        vr = VideoReader(video_path, num_threads=1, ctx=cpu(0))
        return vr

    return _loader


def get_image_loader(use_petrel_backend: bool = True,
                     enable_mc: bool = True,
                     conf_path: str = None):
    if petrel_backend_imported and use_petrel_backend:
        _client = Client(conf_path=conf_path, enable_mc=enable_mc)
    else:
        _client = None

    def _loader(frame_path):
        if _client is not None and 's3:' in frame_path:
            img_bytes = _client.get(frame_path)
        else:
            with open(frame_path, 'rb') as f:
                img_bytes = f.read()

        img_np = np.frombuffer(img_bytes, np.uint8)
        img = cv2.imdecode(img_np, cv2.IMREAD_COLOR)
        cv2.cvtColor(img, cv2.COLOR_BGR2RGB, img)
        return img

    return _loader

```

`deepcheat/VideoMAEv2/dataset/masking_generator.py`:

```py
# --------------------------------------------------------
# Based on BEiT, timm, DINO and DeiT code bases
# https://github.com/microsoft/unilm/tree/master/beit
# https://github.com/rwightman/pytorch-image-models/tree/master/timm
# https://github.com/facebookresearch/deit
# https://github.com/facebookresearch/dino
# --------------------------------------------------------'
import numpy as np


class Cell():

    def __init__(self, num_masks, num_patches):
        self.num_masks = num_masks
        self.num_patches = num_patches
        self.size = num_masks + num_patches
        self.queue = np.hstack([np.ones(num_masks), np.zeros(num_patches)])
        self.queue_ptr = 0

    def set_ptr(self, pos=-1):
        self.queue_ptr = np.random.randint(self.size) if pos < 0 else pos

    def get_cell(self):
        cell_idx = (np.arange(self.size) + self.queue_ptr) % self.size
        return self.queue[cell_idx]

    def run_cell(self):
        self.queue_ptr += 1


class RandomMaskingGenerator:

    def __init__(self, input_size, mask_ratio):
        if not isinstance(input_size, tuple):
            input_size = (input_size, ) * 3

        self.frames, self.height, self.width = input_size

        self.num_patches = self.frames * self.height * self.width  # 8x14x14
        self.num_mask = int(mask_ratio * self.num_patches)

    def __repr__(self):
        repr_str = "Mask: total patches {}, mask patches {}".format(
            self.num_patches, self.num_mask)
        return repr_str

    def __call__(self):
        mask = np.hstack([
            np.zeros(self.num_patches - self.num_mask),
            np.ones(self.num_mask),
        ])
        np.random.shuffle(mask)
        return mask  # [196*8]


class TubeMaskingGenerator:

    def __init__(self, input_size, mask_ratio):
        self.frames, self.height, self.width = input_size
        self.num_patches_per_frame = self.height * self.width  # 14x14
        self.total_patches = self.frames * self.num_patches_per_frame
        self.num_masks_per_frame = int(mask_ratio * self.num_patches_per_frame)
        self.total_masks = self.frames * self.num_masks_per_frame

    def __repr__(self):
        repr_str = "Tube Masking: total patches {}, mask patches {}".format(
            self.total_patches, self.total_masks)
        return repr_str

    def __call__(self):
        mask_per_frame = np.hstack([
            np.zeros(self.num_patches_per_frame - self.num_masks_per_frame),
            np.ones(self.num_masks_per_frame),
        ])
        np.random.shuffle(mask_per_frame)
        mask = np.tile(mask_per_frame, (self.frames, 1))
        return mask  # [196*8]


class RunningCellMaskingGenerator:

    def __init__(self, input_size, mask_ratio=0.5):
        self.frames, self.height, self.width = input_size
        self.mask_ratio = mask_ratio

        num_masks_per_cell = int(4 * self.mask_ratio)
        assert 0 < num_masks_per_cell < 4
        num_patches_per_cell = 4 - num_masks_per_cell

        self.cell = Cell(num_masks_per_cell, num_patches_per_cell)
        self.cell_size = self.cell.size

        mask_list = []
        for ptr_pos in range(self.cell_size):
            self.cell.set_ptr(ptr_pos)
            mask = []
            for _ in range(self.frames):
                self.cell.run_cell()
                mask_unit = self.cell.get_cell().reshape(2, 2)
                mask_map = np.tile(mask_unit,
                                   [self.height // 2, self.width // 2])
                mask.append(mask_map.flatten())
            mask = np.stack(mask, axis=0)
            mask_list.append(mask)
        self.all_mask_maps = np.stack(mask_list, axis=0)

    def __repr__(self):
        repr_str = f"Running Cell Masking with mask ratio {self.mask_ratio}"
        return repr_str

    def __call__(self):
        mask = self.all_mask_maps[np.random.randint(self.cell_size)]
        return np.copy(mask)

```

`deepcheat/VideoMAEv2/dataset/pretrain_datasets.py`:

```py
import os
import random

import numpy as np
import torch
from PIL import Image
from torchvision import transforms

from .loader import get_image_loader, get_video_loader
from .masking_generator import (
    RunningCellMaskingGenerator,
    TubeMaskingGenerator,
)
from .transforms import (
    GroupMultiScaleCrop,
    GroupNormalize,
    Stack,
    ToTorchFormatTensor,
)


class DataAugmentationForVideoMAEv2(object):

    def __init__(self, args):
        self.input_mean = [0.485, 0.456, 0.406]
        self.input_std = [0.229, 0.224, 0.225]
        div = True
        roll = False
        normalize = GroupNormalize(self.input_mean, self.input_std)
        self.train_augmentation = GroupMultiScaleCrop(args.input_size,
                                                      [1, .875, .75, .66])
        self.transform = transforms.Compose([
            self.train_augmentation,
            Stack(roll=roll),
            ToTorchFormatTensor(div=div),
            normalize,
        ])
        if args.mask_type == 'tube':
            self.encoder_mask_map_generator = TubeMaskingGenerator(
                args.window_size, args.mask_ratio)
        else:
            raise NotImplementedError(
                'Unsupported encoder masking strategy type.')
        if args.decoder_mask_ratio > 0.:
            if args.decoder_mask_type == 'run_cell':
                self.decoder_mask_map_generator = RunningCellMaskingGenerator(
                    args.window_size, args.decoder_mask_ratio)
            else:
                raise NotImplementedError(
                    'Unsupported decoder masking strategy type.')

    def __call__(self, images):
        process_data, _ = self.transform(images)
        encoder_mask_map = self.encoder_mask_map_generator()
        if hasattr(self, 'decoder_mask_map_generator'):
            decoder_mask_map = self.decoder_mask_map_generator()
        else:
            decoder_mask_map = 1 - encoder_mask_map
        return process_data, encoder_mask_map, decoder_mask_map

    def __repr__(self):
        repr = "(DataAugmentationForVideoMAEv2,\n"
        repr += "  transform = %s,\n" % str(self.transform)
        repr += "  Encoder Masking Generator = %s,\n" % str(
            self.encoder_mask_map_generator)
        if hasattr(self, 'decoder_mask_map_generator'):
            repr += "  Decoder Masking Generator = %s,\n" % str(
                self.decoder_mask_map_generator)
        else:
            repr += "  Do not use decoder masking,\n"
        repr += ")"
        return repr


class HybridVideoMAE(torch.utils.data.Dataset):
    """Load your own videomae pretraining dataset.
    Parameters
    ----------
    root : str, required.
        Path to the root folder storing the dataset.
    setting : str, required.
        A text file describing the dataset, each line per video sample.
        There are four items in each line:
        (1) video path; (2) start_idx, (3) total frames and (4) video label.
        for pre-train video data
            total frames < 0, start_idx and video label meaningless
        for pre-train rawframe data
            video label meaningless
    train : bool, default True.
        Whether to load the training or validation set.
    test_mode : bool, default False.
        Whether to perform evaluation on the test set.
        Usually there is three-crop or ten-crop evaluation strategy involved.
    name_pattern : str, default 'img_{:05}.jpg'.
        The naming pattern of the decoded video frames.
        For example, img_00012.jpg.
    video_ext : str, default 'mp4'.
        If video_loader is set to True, please specify the video format accordinly.
    is_color : bool, default True.
        Whether the loaded image is color or grayscale.
    modality : str, default 'rgb'.
        Input modalities, we support only rgb video frames for now.
        Will add support for rgb difference image and optical flow image later.
    num_segments : int, default 1.
        Number of segments to evenly divide the video into clips.
        A useful technique to obtain global video-level information.
        Limin Wang, etal, Temporal Segment Networks: Towards Good Practices for Deep Action Recognition, ECCV 2016.
    num_crop : int, default 1.
        Number of crops for each image. default is 1.
        Common choices are three crops and ten crops during evaluation.
    new_length : int, default 1.
        The length of input video clip. Default is a single image, but it can be multiple video frames.
        For example, new_length=16 means we will extract a video clip of consecutive 16 frames.
    new_step : int, default 1.
        Temporal sampling rate. For example, new_step=1 means we will extract a video clip of consecutive frames.
        new_step=2 means we will extract a video clip of every other frame.
    transform : function, default None.
        A function that takes data and label and transforms them.
    temporal_jitter : bool, default False.
        Whether to temporally jitter if new_step > 1.
    lazy_init : bool, default False.
        If set to True, build a dataset instance without loading any dataset.
    num_sample : int, default 1.
        Number of sampled views for Repeated Augmentation.
    """

    def __init__(self,
                 root,
                 setting,
                 train=True,
                 test_mode=False,
                 name_pattern='img_{:05}.jpg',
                 video_ext='mp4',
                 is_color=True,
                 modality='rgb',
                 num_segments=1,
                 num_crop=1,
                 new_length=1,
                 new_step=1,
                 transform=None,
                 temporal_jitter=False,
                 lazy_init=False,
                 num_sample=1):

        super(HybridVideoMAE, self).__init__()
        self.root = root
        self.setting = setting
        self.train = train
        self.test_mode = test_mode
        self.is_color = is_color
        self.modality = modality
        self.num_segments = num_segments
        self.num_crop = num_crop
        self.new_length = new_length
        self.new_step = new_step
        self.skip_length = self.new_length * self.new_step
        self.temporal_jitter = temporal_jitter
        self.name_pattern = name_pattern
        self.video_ext = video_ext
        self.transform = transform
        self.lazy_init = lazy_init
        self.num_sample = num_sample

        # NOTE:
        # for hybrid train
        # different frame naming formats are used for different datasets
        # should MODIFY the fname_tmpl to your own situation
        self.ava_fname_tmpl = 'image_{:06}.jpg'
        self.ssv2_fname_tmpl = 'img_{:05}.jpg'

        # NOTE:
        # we set sampling_rate = 2 for ssv2
        # thus being consistent with the fine-tuning stage
        # Note that the ssv2 we use is decoded to frames at 12 fps;
        # if decoded at 24 fps, the sample interval should be 4.
        self.ssv2_skip_length = self.new_length * 2
        self.orig_skip_length = self.skip_length

        self.video_loader = get_video_loader()
        self.image_loader = get_image_loader()

        if not self.lazy_init:
            self.clips = self._make_dataset(root, setting)
            if len(self.clips) == 0:
                raise (
                    RuntimeError("Found 0 video clips in subfolders of: " +
                                 root + "\n"
                                 "Check your data directory (opt.data-dir)."))

    def __getitem__(self, index):
        try:
            video_name, start_idx, total_frame = self.clips[index]
            self.skip_length = self.orig_skip_length

            if total_frame < 0:
                decord_vr = self.video_loader(video_name)
                duration = len(decord_vr)

                segment_indices, skip_offsets = self._sample_train_indices(
                    duration)
                frame_id_list = self.get_frame_id_list(duration,
                                                       segment_indices,
                                                       skip_offsets)
                video_data = decord_vr.get_batch(frame_id_list).asnumpy()
                images = [
                    Image.fromarray(video_data[vid, :, :, :]).convert('RGB')
                    for vid, _ in enumerate(frame_id_list)
                ]

            else:
                # ssv2 & ava & other rawframe dataset
                if 'SomethingV2' in video_name:
                    self.skip_length = self.ssv2_skip_length
                    fname_tmpl = self.ssv2_fname_tmpl
                elif 'AVA2.2' in video_name:
                    fname_tmpl = self.ava_fname_tmpl
                else:
                    fname_tmpl = self.name_pattern

                segment_indices, skip_offsets = self._sample_train_indices(
                    total_frame)
                frame_id_list = self.get_frame_id_list(total_frame,
                                                       segment_indices,
                                                       skip_offsets)

                images = []
                for idx in frame_id_list:
                    frame_fname = os.path.join(
                        video_name, fname_tmpl.format(idx + start_idx))
                    img = self.image_loader(frame_fname)
                    img = Image.fromarray(img)
                    images.append(img)

        except Exception as e:
            print("Failed to load video from {} with error {}".format(
                video_name, e))
            index = random.randint(0, len(self.clips) - 1)
            return self.__getitem__(index)

        if self.num_sample > 1:
            process_data_list = []
            encoder_mask_list = []
            decoder_mask_list = []
            for _ in range(self.num_sample):
                process_data, encoder_mask, decoder_mask = self.transform(
                    (images, None))
                process_data = process_data.view(
                    (self.new_length, 3) + process_data.size()[-2:]).transpose(
                        0, 1)
                process_data_list.append(process_data)
                encoder_mask_list.append(encoder_mask)
                decoder_mask_list.append(decoder_mask)
            return process_data_list, encoder_mask_list, decoder_mask_list
        else:
            process_data, encoder_mask, decoder_mask = self.transform(
                (images, None))
            # T*C,H,W -> T,C,H,W -> C,T,H,W
            process_data = process_data.view(
                (self.new_length, 3) + process_data.size()[-2:]).transpose(
                    0, 1)
            return process_data, encoder_mask, decoder_mask

    def __len__(self):
        return len(self.clips)

    def _make_dataset(self, root, setting):
        if not os.path.exists(setting):
            raise (RuntimeError(
                "Setting file %s doesn't exist. Check opt.train-list and opt.val-list. "
                % (setting)))
        clips = []
        with open(setting) as split_f:
            data = split_f.readlines()
            for line in data:
                line_info = line.split(' ')
                # line format: video_path, video_duration, video_label
                if len(line_info) < 2:
                    raise (RuntimeError(
                        'Video input format is not correct, missing one or more element. %s'
                        % line))
                clip_path = os.path.join(root, line_info[0])
                start_idx = int(line_info[1])
                total_frame = int(line_info[2])
                item = (clip_path, start_idx, total_frame)
                clips.append(item)
        return clips

    def _sample_train_indices(self, num_frames):
        average_duration = (num_frames - self.skip_length +
                            1) // self.num_segments
        if average_duration > 0:
            offsets = np.multiply(
                list(range(self.num_segments)), average_duration)
            offsets = offsets + np.random.randint(
                average_duration, size=self.num_segments)
        elif num_frames > max(self.num_segments, self.skip_length):
            offsets = np.sort(
                np.random.randint(
                    num_frames - self.skip_length + 1, size=self.num_segments))
        else:
            offsets = np.zeros((self.num_segments, ))

        if self.temporal_jitter:
            skip_offsets = np.random.randint(
                self.new_step, size=self.skip_length // self.new_step)
        else:
            skip_offsets = np.zeros(
                self.skip_length // self.new_step, dtype=int)
        return offsets + 1, skip_offsets

    def get_frame_id_list(self, duration, indices, skip_offsets):
        frame_id_list = []
        for seg_ind in indices:
            offset = int(seg_ind)
            for i, _ in enumerate(range(0, self.skip_length, self.new_step)):
                if offset + skip_offsets[i] <= duration:
                    frame_id = offset + skip_offsets[i] - 1
                else:
                    frame_id = offset - 1
                frame_id_list.append(frame_id)
                if offset + self.new_step < duration:
                    offset += self.new_step
        return frame_id_list


class VideoMAE(torch.utils.data.Dataset):
    """Load your own videomae pretraining dataset.
    Parameters
    ----------
    root : str, required.
        Path to the root folder storing the dataset.
    setting : str, required.
        A text file describing the dataset, each line per video sample.
        There are four items in each line:
        (1) video path; (2) start_idx, (3) total frames and (4) video label.
        for pre-train video data
            total frames < 0, start_idx and video label meaningless
        for pre-train rawframe data
            video label meaningless
    train : bool, default True.
        Whether to load the training or validation set.
    test_mode : bool, default False.
        Whether to perform evaluation on the test set.
        Usually there is three-crop or ten-crop evaluation strategy involved.
    name_pattern : str, default 'img_{:05}.jpg'.
        The naming pattern of the decoded video frames.
        For example, img_00012.jpg.
    video_ext : str, default 'mp4'.
        If video_loader is set to True, please specify the video format accordinly.
    is_color : bool, default True.
        Whether the loaded image is color or grayscale.
    modality : str, default 'rgb'.
        Input modalities, we support only rgb video frames for now.
        Will add support for rgb difference image and optical flow image later.
    num_segments : int, default 1.
        Number of segments to evenly divide the video into clips.
        A useful technique to obtain global video-level information.
        Limin Wang, etal, Temporal Segment Networks: Towards Good Practices for Deep Action Recognition, ECCV 2016.
    num_crop : int, default 1.
        Number of crops for each image. default is 1.
        Common choices are three crops and ten crops during evaluation.
    new_length : int, default 1.
        The length of input video clip. Default is a single image, but it can be multiple video frames.
        For example, new_length=16 means we will extract a video clip of consecutive 16 frames.
    new_step : int, default 1.
        Temporal sampling rate. For example, new_step=1 means we will extract a video clip of consecutive frames.
        new_step=2 means we will extract a video clip of every other frame.
    transform : function, default None.
        A function that takes data and label and transforms them.
    temporal_jitter : bool, default False.
        Whether to temporally jitter if new_step > 1.
    lazy_init : bool, default False.
        If set to True, build a dataset instance without loading any dataset.
    num_sample : int, default 1.
        Number of sampled views for Repeated Augmentation.
    """

    def __init__(self,
                 root,
                 setting,
                 train=True,
                 test_mode=False,
                 name_pattern='img_{:05}.jpg',
                 video_ext='mp4',
                 is_color=True,
                 modality='rgb',
                 num_segments=1,
                 num_crop=1,
                 new_length=1,
                 new_step=1,
                 transform=None,
                 temporal_jitter=False,
                 lazy_init=False,
                 num_sample=1):

        super(VideoMAE, self).__init__()
        self.root = root
        self.setting = setting
        self.train = train
        self.test_mode = test_mode
        self.is_color = is_color
        self.modality = modality
        self.num_segments = num_segments
        self.num_crop = num_crop
        self.new_length = new_length
        self.new_step = new_step
        self.skip_length = self.new_length * self.new_step
        self.temporal_jitter = temporal_jitter
        self.name_pattern = name_pattern
        self.video_ext = video_ext
        self.transform = transform
        self.lazy_init = lazy_init
        self.num_sample = num_sample

        self.video_loader = get_video_loader()
        self.image_loader = get_image_loader()

        if not self.lazy_init:
            self.clips = self._make_dataset(root, setting)
            if len(self.clips) == 0:
                raise (
                    RuntimeError("Found 0 video clips in subfolders of: " +
                                 root + "\n"
                                 "Check your data directory (opt.data-dir)."))

    def __getitem__(self, index):
        #breakpoint()
        try:
            video_name, start_idx, total_frame = self.clips[index]
            if total_frame < 0:  # load video
                decord_vr = self.video_loader(video_name)
                duration = len(decord_vr)

                segment_indices, skip_offsets = self._sample_train_indices(
                    duration)
                frame_id_list = self.get_frame_id_list(duration,
                                                       segment_indices,
                                                       skip_offsets)
                video_data = decord_vr.get_batch(frame_id_list).asnumpy()
                images = [
                    Image.fromarray(video_data[vid, :, :, :]).convert('RGB')
                    for vid, _ in enumerate(frame_id_list)
                ]
            else:  # load frames
                segment_indices, skip_offsets = self._sample_train_indices(
                    total_frame)
                frame_id_list = self.get_frame_id_list(total_frame,
                                                       segment_indices,
                                                       skip_offsets)
                #print(frame_id_list)
                images = []
                for idx in frame_id_list:
                    frame_fname = os.path.join(
                        video_name, self.name_pattern.format(idx + start_idx))
                    img = self.image_loader(frame_fname)
                    img = Image.fromarray(img)
                    images.append(img)

        except Exception as e:
            print("Failed to load video from {} with error {}".format(
                video_name, e))
            index = random.randint(0, len(self.clips) - 1)
            return self.__getitem__(index)

        if self.num_sample > 1:
            process_data_list = []
            encoder_mask_list = []
            decoder_mask_list = []
            for _ in range(self.num_sample):
                process_data, encoder_mask, decoder_mask = self.transform(
                    (images, None))
                process_data = process_data.view(
                    (self.new_length, 3) + process_data.size()[-2:]).transpose(
                        0, 1)
                process_data_list.append(process_data)
                encoder_mask_list.append(encoder_mask)
                decoder_mask_list.append(decoder_mask)
            return process_data_list, encoder_mask_list, decoder_mask_list
        else:
            process_data, encoder_mask, decoder_mask = self.transform(
                (images, None))
            # T*C,H,W -> T,C,H,W -> C,T,H,W
            process_data = process_data.view(
                (self.new_length, 3) + process_data.size()[-2:]).transpose(
                    0, 1)
            return process_data, encoder_mask, decoder_mask

    def __len__(self):
        return len(self.clips)

    def _make_dataset(self, root, setting):
        if not os.path.exists(setting):
            raise (RuntimeError(
                "Setting file %s doesn't exist. Check opt.train-list and opt.val-list. "
                % (setting)))
        clips = []
        with open(setting) as split_f:
            data = split_f.readlines()
            for line in data:
                line_info = line.split(' ')
                # line format: video_path, start_idx, total_frames
                if len(line_info) < 3:
                    raise (RuntimeError(
                        'Video input format is not correct, missing one or more element. %s'
                        % line))

                clip_path = os.path.join(root, line_info[0])
                start_idx = int(line_info[1])
                total_frame = int(line_info[2])
                item = (clip_path, start_idx, total_frame)
                clips.append(item)
        return clips

    def _sample_train_indices(self, num_frames):
        average_duration = (num_frames - self.skip_length +
                            1) // self.num_segments
        if average_duration > 0:
            offsets = np.multiply(
                list(range(self.num_segments)), average_duration)
            offsets = offsets + np.random.randint(
                average_duration, size=self.num_segments)
        elif num_frames > max(self.num_segments, self.skip_length):
            offsets = np.sort(
                np.random.randint(
                    num_frames - self.skip_length + 1, size=self.num_segments))
        else:
            offsets = np.zeros((self.num_segments, ))

        if self.temporal_jitter:
            skip_offsets = np.random.randint(
                self.new_step, size=self.skip_length // self.new_step)
        else:
            skip_offsets = np.zeros(
                self.skip_length // self.new_step, dtype=int)
        return offsets + 1, skip_offsets

    def get_frame_id_list(self, duration, indices, skip_offsets):
        frame_id_list = []
        for seg_ind in indices:
            offset = int(seg_ind)
            for i, _ in enumerate(range(0, self.skip_length, self.new_step)):
                if offset + skip_offsets[i] <= duration:
                    frame_id = offset + skip_offsets[i] - 1
                else:
                    frame_id = offset - 1
                frame_id_list.append(frame_id)
                if offset + self.new_step < duration:
                    offset += self.new_step
        return frame_id_list

```

`deepcheat/VideoMAEv2/dataset/rand_augment.py`:

```py
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
"""
This implementation is based on
https://github.com/rwightman/pytorch-image-models/blob/master/timm/data/auto_augment.py
pulished under an Apache License 2.0.

COMMENT FROM ORIGINAL:
AutoAugment, RandAugment, and AugMix for PyTorch
This code implements the searched ImageNet policies with various tweaks and
improvements and does not include any of the search code. AA and RA
Implementation adapted from:
    https://github.com/tensorflow/tpu/blob/master/models/official/efficientnet/autoaugment.py
AugMix adapted from:
    https://github.com/google-research/augmix
Papers:
    AutoAugment: Learning Augmentation Policies from Data
    https://arxiv.org/abs/1805.09501
    Learning Data Augmentation Strategies for Object Detection
    https://arxiv.org/abs/1906.11172
    RandAugment: Practical automated data augmentation...
    https://arxiv.org/abs/1909.13719
    AugMix: A Simple Data Processing Method to Improve Robustness and
    Uncertainty https://arxiv.org/abs/1912.02781

Hacked together by / Copyright 2020 Ross Wightman
"""

import math
import random
import re

import numpy as np
import PIL
from PIL import Image, ImageEnhance, ImageOps

_PIL_VER = tuple([int(x) for x in PIL.__version__.split(".")[:2]])

_FILL = (128, 128, 128)

# This signifies the max integer that the controller RNN could predict for the
# augmentation scheme.
_MAX_LEVEL = 10.0

_HPARAMS_DEFAULT = {
    "translate_const": 250,
    "img_mean": _FILL,
}

_RANDOM_INTERPOLATION = (Image.BILINEAR, Image.BICUBIC)


def _interpolation(kwargs):
    interpolation = kwargs.pop("resample", Image.BILINEAR)
    if isinstance(interpolation, (list, tuple)):
        return random.choice(interpolation)
    else:
        return interpolation


def _check_args_tf(kwargs):
    if "fillcolor" in kwargs and _PIL_VER < (5, 0):
        kwargs.pop("fillcolor")
    kwargs["resample"] = _interpolation(kwargs)


def shear_x(img, factor, **kwargs):
    _check_args_tf(kwargs)
    return img.transform(img.size, Image.AFFINE, (1, factor, 0, 0, 1, 0),
                         **kwargs)


def shear_y(img, factor, **kwargs):
    _check_args_tf(kwargs)
    return img.transform(img.size, Image.AFFINE, (1, 0, 0, factor, 1, 0),
                         **kwargs)


def translate_x_rel(img, pct, **kwargs):
    pixels = pct * img.size[0]
    _check_args_tf(kwargs)
    return img.transform(img.size, Image.AFFINE, (1, 0, pixels, 0, 1, 0),
                         **kwargs)


def translate_y_rel(img, pct, **kwargs):
    pixels = pct * img.size[1]
    _check_args_tf(kwargs)
    return img.transform(img.size, Image.AFFINE, (1, 0, 0, 0, 1, pixels),
                         **kwargs)


def translate_x_abs(img, pixels, **kwargs):
    _check_args_tf(kwargs)
    return img.transform(img.size, Image.AFFINE, (1, 0, pixels, 0, 1, 0),
                         **kwargs)


def translate_y_abs(img, pixels, **kwargs):
    _check_args_tf(kwargs)
    return img.transform(img.size, Image.AFFINE, (1, 0, 0, 0, 1, pixels),
                         **kwargs)


def rotate(img, degrees, **kwargs):
    _check_args_tf(kwargs)
    if _PIL_VER >= (5, 2):
        return img.rotate(degrees, **kwargs)
    elif _PIL_VER >= (5, 0):
        w, h = img.size
        post_trans = (0, 0)
        rotn_center = (w / 2.0, h / 2.0)
        angle = -math.radians(degrees)
        matrix = [
            round(math.cos(angle), 15),
            round(math.sin(angle), 15),
            0.0,
            round(-math.sin(angle), 15),
            round(math.cos(angle), 15),
            0.0,
        ]

        def transform(x, y, matrix):
            (a, b, c, d, e, f) = matrix
            return a * x + b * y + c, d * x + e * y + f

        matrix[2], matrix[5] = transform(
            -rotn_center[0] - post_trans[0],
            -rotn_center[1] - post_trans[1],
            matrix,
        )
        matrix[2] += rotn_center[0]
        matrix[5] += rotn_center[1]
        return img.transform(img.size, Image.AFFINE, matrix, **kwargs)
    else:
        return img.rotate(degrees, resample=kwargs["resample"])


def auto_contrast(img, **__):
    return ImageOps.autocontrast(img)


def invert(img, **__):
    return ImageOps.invert(img)


def equalize(img, **__):
    return ImageOps.equalize(img)


def solarize(img, thresh, **__):
    return ImageOps.solarize(img, thresh)


def solarize_add(img, add, thresh=128, **__):
    lut = []
    for i in range(256):
        if i < thresh:
            lut.append(min(255, i + add))
        else:
            lut.append(i)
    if img.mode in ("L", "RGB"):
        if img.mode == "RGB" and len(lut) == 256:
            lut = lut + lut + lut
        return img.point(lut)
    else:
        return img


def posterize(img, bits_to_keep, **__):
    if bits_to_keep >= 8:
        return img
    return ImageOps.posterize(img, bits_to_keep)


def contrast(img, factor, **__):
    return ImageEnhance.Contrast(img).enhance(factor)


def color(img, factor, **__):
    return ImageEnhance.Color(img).enhance(factor)


def brightness(img, factor, **__):
    return ImageEnhance.Brightness(img).enhance(factor)


def sharpness(img, factor, **__):
    return ImageEnhance.Sharpness(img).enhance(factor)


def _randomly_negate(v):
    """With 50% prob, negate the value"""
    return -v if random.random() > 0.5 else v


def _rotate_level_to_arg(level, _hparams):
    # range [-30, 30]
    level = (level / _MAX_LEVEL) * 30.0
    level = _randomly_negate(level)
    return (level, )


def _enhance_level_to_arg(level, _hparams):
    # range [0.1, 1.9]
    return ((level / _MAX_LEVEL) * 1.8 + 0.1, )


def _enhance_increasing_level_to_arg(level, _hparams):
    # the 'no change' level is 1.0, moving away from that towards 0. or 2.0 increases the enhancement blend
    # range [0.1, 1.9]
    level = (level / _MAX_LEVEL) * 0.9
    level = 1.0 + _randomly_negate(level)
    return (level, )


def _shear_level_to_arg(level, _hparams):
    # range [-0.3, 0.3]
    level = (level / _MAX_LEVEL) * 0.3
    level = _randomly_negate(level)
    return (level, )


def _translate_abs_level_to_arg(level, hparams):
    translate_const = hparams["translate_const"]
    level = (level / _MAX_LEVEL) * float(translate_const)
    level = _randomly_negate(level)
    return (level, )


def _translate_rel_level_to_arg(level, hparams):
    # default range [-0.45, 0.45]
    translate_pct = hparams.get("translate_pct", 0.45)
    level = (level / _MAX_LEVEL) * translate_pct
    level = _randomly_negate(level)
    return (level, )


def _posterize_level_to_arg(level, _hparams):
    # As per Tensorflow TPU EfficientNet impl
    # range [0, 4], 'keep 0 up to 4 MSB of original image'
    # intensity/severity of augmentation decreases with level
    return (int((level / _MAX_LEVEL) * 4), )


def _posterize_increasing_level_to_arg(level, hparams):
    # As per Tensorflow models research and UDA impl
    # range [4, 0], 'keep 4 down to 0 MSB of original image',
    # intensity/severity of augmentation increases with level
    return (4 - _posterize_level_to_arg(level, hparams)[0], )


def _posterize_original_level_to_arg(level, _hparams):
    # As per original AutoAugment paper description
    # range [4, 8], 'keep 4 up to 8 MSB of image'
    # intensity/severity of augmentation decreases with level
    return (int((level / _MAX_LEVEL) * 4) + 4, )


def _solarize_level_to_arg(level, _hparams):
    # range [0, 256]
    # intensity/severity of augmentation decreases with level
    return (int((level / _MAX_LEVEL) * 256), )


def _solarize_increasing_level_to_arg(level, _hparams):
    # range [0, 256]
    # intensity/severity of augmentation increases with level
    return (256 - _solarize_level_to_arg(level, _hparams)[0], )


def _solarize_add_level_to_arg(level, _hparams):
    # range [0, 110]
    return (int((level / _MAX_LEVEL) * 110), )


LEVEL_TO_ARG = {
    "AutoContrast": None,
    "Equalize": None,
    "Invert": None,
    "Rotate": _rotate_level_to_arg,
    # There are several variations of the posterize level scaling in various Tensorflow/Google repositories/papers
    "Posterize": _posterize_level_to_arg,
    "PosterizeIncreasing": _posterize_increasing_level_to_arg,
    "PosterizeOriginal": _posterize_original_level_to_arg,
    "Solarize": _solarize_level_to_arg,
    "SolarizeIncreasing": _solarize_increasing_level_to_arg,
    "SolarizeAdd": _solarize_add_level_to_arg,
    "Color": _enhance_level_to_arg,
    "ColorIncreasing": _enhance_increasing_level_to_arg,
    "Contrast": _enhance_level_to_arg,
    "ContrastIncreasing": _enhance_increasing_level_to_arg,
    "Brightness": _enhance_level_to_arg,
    "BrightnessIncreasing": _enhance_increasing_level_to_arg,
    "Sharpness": _enhance_level_to_arg,
    "SharpnessIncreasing": _enhance_increasing_level_to_arg,
    "ShearX": _shear_level_to_arg,
    "ShearY": _shear_level_to_arg,
    "TranslateX": _translate_abs_level_to_arg,
    "TranslateY": _translate_abs_level_to_arg,
    "TranslateXRel": _translate_rel_level_to_arg,
    "TranslateYRel": _translate_rel_level_to_arg,
}

NAME_TO_OP = {
    "AutoContrast": auto_contrast,
    "Equalize": equalize,
    "Invert": invert,
    "Rotate": rotate,
    "Posterize": posterize,
    "PosterizeIncreasing": posterize,
    "PosterizeOriginal": posterize,
    "Solarize": solarize,
    "SolarizeIncreasing": solarize,
    "SolarizeAdd": solarize_add,
    "Color": color,
    "ColorIncreasing": color,
    "Contrast": contrast,
    "ContrastIncreasing": contrast,
    "Brightness": brightness,
    "BrightnessIncreasing": brightness,
    "Sharpness": sharpness,
    "SharpnessIncreasing": sharpness,
    "ShearX": shear_x,
    "ShearY": shear_y,
    "TranslateX": translate_x_abs,
    "TranslateY": translate_y_abs,
    "TranslateXRel": translate_x_rel,
    "TranslateYRel": translate_y_rel,
}


class AugmentOp:
    """
    Apply for video.
    """

    def __init__(self, name, prob=0.5, magnitude=10, hparams=None):
        hparams = hparams or _HPARAMS_DEFAULT
        self.aug_fn = NAME_TO_OP[name]
        self.level_fn = LEVEL_TO_ARG[name]
        self.prob = prob
        self.magnitude = magnitude
        self.hparams = hparams.copy()
        self.kwargs = {
            "fillcolor":
            hparams["img_mean"] if "img_mean" in hparams else _FILL,
            "resample":
            hparams["interpolation"]
            if "interpolation" in hparams else _RANDOM_INTERPOLATION,
        }

        # If magnitude_std is > 0, we introduce some randomness
        # in the usually fixed policy and sample magnitude from a normal distribution
        # with mean `magnitude` and std-dev of `magnitude_std`.
        # NOTE This is my own hack, being tested, not in papers or reference impls.
        self.magnitude_std = self.hparams.get("magnitude_std", 0)

    def __call__(self, img_list):
        if self.prob < 1.0 and random.random() > self.prob:
            return img_list
        magnitude = self.magnitude
        if self.magnitude_std and self.magnitude_std > 0:
            magnitude = random.gauss(magnitude, self.magnitude_std)
        magnitude = min(_MAX_LEVEL, max(0, magnitude))  # clip to valid range
        level_args = (
            self.level_fn(magnitude, self.hparams)
            if self.level_fn is not None else ())

        if isinstance(img_list, list):
            return [
                self.aug_fn(img, *level_args, **self.kwargs)
                for img in img_list
            ]
        else:
            return self.aug_fn(img_list, *level_args, **self.kwargs)


_RAND_TRANSFORMS = [
    "AutoContrast",
    "Equalize",
    "Invert",
    "Rotate",
    "Posterize",
    "Solarize",
    "SolarizeAdd",
    "Color",
    "Contrast",
    "Brightness",
    "Sharpness",
    "ShearX",
    "ShearY",
    "TranslateXRel",
    "TranslateYRel",
]

_RAND_INCREASING_TRANSFORMS = [
    "AutoContrast",
    "Equalize",
    "Invert",
    "Rotate",
    "PosterizeIncreasing",
    "SolarizeIncreasing",
    "SolarizeAdd",
    "ColorIncreasing",
    "ContrastIncreasing",
    "BrightnessIncreasing",
    "SharpnessIncreasing",
    "ShearX",
    "ShearY",
    "TranslateXRel",
    "TranslateYRel",
]

# These experimental weights are based loosely on the relative improvements mentioned in paper.
# They may not result in increased performance, but could likely be tuned to so.
_RAND_CHOICE_WEIGHTS_0 = {
    "Rotate": 0.3,
    "ShearX": 0.2,
    "ShearY": 0.2,
    "TranslateXRel": 0.1,
    "TranslateYRel": 0.1,
    "Color": 0.025,
    "Sharpness": 0.025,
    "AutoContrast": 0.025,
    "Solarize": 0.005,
    "SolarizeAdd": 0.005,
    "Contrast": 0.005,
    "Brightness": 0.005,
    "Equalize": 0.005,
    "Posterize": 0,
    "Invert": 0,
}


def _select_rand_weights(weight_idx=0, transforms=None):
    transforms = transforms or _RAND_TRANSFORMS
    assert weight_idx == 0  # only one set of weights currently
    rand_weights = _RAND_CHOICE_WEIGHTS_0
    probs = [rand_weights[k] for k in transforms]
    probs /= np.sum(probs)
    return probs


def rand_augment_ops(magnitude=10, hparams=None, transforms=None):
    hparams = hparams or _HPARAMS_DEFAULT
    transforms = transforms or _RAND_TRANSFORMS
    return [
        AugmentOp(name, prob=0.5, magnitude=magnitude, hparams=hparams)
        for name in transforms
    ]


class RandAugment:

    def __init__(self, ops, num_layers=2, choice_weights=None):
        self.ops = ops
        self.num_layers = num_layers
        self.choice_weights = choice_weights

    def __call__(self, img):
        # no replacement when using weighted choice
        ops = np.random.choice(
            self.ops,
            self.num_layers,
            replace=self.choice_weights is None,
            p=self.choice_weights,
        )
        for op in ops:
            img = op(img)
        return img


def rand_augment_transform(config_str, hparams):
    """
    RandAugment: Practical automated data augmentation... - https://arxiv.org/abs/1909.13719

    Create a RandAugment transform
    :param config_str: String defining configuration of random augmentation. Consists of multiple sections separated by
    dashes ('-'). The first section defines the specific variant of rand augment (currently only 'rand'). The remaining
    sections, not order sepecific determine
        'm' - integer magnitude of rand augment
        'n' - integer num layers (number of transform ops selected per image)
        'w' - integer probabiliy weight index (index of a set of weights to influence choice of op)
        'mstd' -  float std deviation of magnitude noise applied
        'inc' - integer (bool), use augmentations that increase in severity with magnitude (default: 0)
    Ex 'rand-m9-n3-mstd0.5' results in RandAugment with magnitude 9, num_layers 3, magnitude_std 0.5
    'rand-mstd1-w0' results in magnitude_std 1.0, weights 0, default magnitude of 10 and num_layers 2
    :param hparams: Other hparams (kwargs) for the RandAugmentation scheme
    :return: A PyTorch compatible Transform
    """
    magnitude = _MAX_LEVEL  # default to _MAX_LEVEL for magnitude (currently 10)
    num_layers = 2  # default to 2 ops per image
    weight_idx = None  # default to no probability weights for op choice
    transforms = _RAND_TRANSFORMS
    config = config_str.split("-")
    assert config[0] == "rand"
    config = config[1:]
    for c in config:
        cs = re.split(r"(\d.*)", c)
        if len(cs) < 2:
            continue
        key, val = cs[:2]
        if key == "mstd":
            # noise param injected via hparams for now
            hparams.setdefault("magnitude_std", float(val))
        elif key == "inc":
            if bool(val):
                transforms = _RAND_INCREASING_TRANSFORMS
        elif key == "m":
            magnitude = int(val)
        elif key == "n":
            num_layers = int(val)
        elif key == "w":
            weight_idx = int(val)
        else:
            assert NotImplementedError
    ra_ops = rand_augment_ops(
        magnitude=magnitude, hparams=hparams, transforms=transforms)
    choice_weights = (None if weight_idx is None else
                      _select_rand_weights(weight_idx))
    return RandAugment(ra_ops, num_layers, choice_weights=choice_weights)

```

`deepcheat/VideoMAEv2/dataset/random_erasing.py`:

```py
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
"""
This implementation is based on
https://github.com/rwightman/pytorch-image-models/blob/master/timm/data/random_erasing.py
pulished under an Apache License 2.0.

COMMENT FROM ORIGINAL:
Originally inspired by impl at https://github.com/zhunzhong07/Random-Erasing, Apache 2.0
Copyright Zhun Zhong & Liang Zheng
Hacked together by / Copyright 2020 Ross Wightman
"""
import math
import random

import torch


def _get_pixels(per_pixel,
                rand_color,
                patch_size,
                dtype=torch.float32,
                device="cuda"):
    # NOTE I've seen CUDA illegal memory access errors being caused by the normal_()
    # paths, flip the order so normal is run on CPU if this becomes a problem
    # Issue has been fixed in master https://github.com/pytorch/pytorch/issues/19508
    if per_pixel:
        return torch.empty(patch_size, dtype=dtype, device=device).normal_()
    elif rand_color:
        return torch.empty((patch_size[0], 1, 1), dtype=dtype,
                           device=device).normal_()
    else:
        return torch.zeros((patch_size[0], 1, 1), dtype=dtype, device=device)


class RandomErasing:
    """Randomly selects a rectangle region in an image and erases its pixels.
        'Random Erasing Data Augmentation' by Zhong et al.
        See https://arxiv.org/pdf/1708.04896.pdf
        This variant of RandomErasing is intended to be applied to either a batch
        or single image tensor after it has been normalized by dataset mean and std.
    Args:
         probability: Probability that the Random Erasing operation will be performed.
         min_area: Minimum percentage of erased area wrt input image area.
         max_area: Maximum percentage of erased area wrt input image area.
         min_aspect: Minimum aspect ratio of erased area.
         mode: pixel color mode, one of 'const', 'rand', or 'pixel'
            'const' - erase block is constant color of 0 for all channels
            'rand'  - erase block is same per-channel random (normal) color
            'pixel' - erase block is per-pixel random (normal) color
        max_count: maximum number of erasing blocks per image, area per box is scaled by count.
            per-image count is randomly chosen between 1 and this value.
    """

    def __init__(
        self,
        probability=0.5,
        min_area=0.02,
        max_area=1 / 3,
        min_aspect=0.3,
        max_aspect=None,
        mode="const",
        min_count=1,
        max_count=None,
        num_splits=0,
        device="cuda",
        cube=True,
    ):
        self.probability = probability
        self.min_area = min_area
        self.max_area = max_area
        max_aspect = max_aspect or 1 / min_aspect
        self.log_aspect_ratio = (math.log(min_aspect), math.log(max_aspect))
        self.min_count = min_count
        self.max_count = max_count or min_count
        self.num_splits = num_splits
        mode = mode.lower()
        self.rand_color = False
        self.per_pixel = False
        self.cube = cube
        if mode == "rand":
            self.rand_color = True  # per block random normal
        elif mode == "pixel":
            self.per_pixel = True  # per pixel random normal
        else:
            assert not mode or mode == "const"
        self.device = device

    def _erase(self, img, chan, img_h, img_w, dtype):
        if random.random() > self.probability:
            return
        area = img_h * img_w
        count = (
            self.min_count if self.min_count == self.max_count else
            random.randint(self.min_count, self.max_count))
        for _ in range(count):
            for _ in range(10):
                target_area = (
                    random.uniform(self.min_area, self.max_area) * area /
                    count)
                aspect_ratio = math.exp(random.uniform(*self.log_aspect_ratio))
                h = int(round(math.sqrt(target_area * aspect_ratio)))
                w = int(round(math.sqrt(target_area / aspect_ratio)))
                if w < img_w and h < img_h:
                    top = random.randint(0, img_h - h)
                    left = random.randint(0, img_w - w)
                    img[:, top:top + h, left:left + w] = _get_pixels(
                        self.per_pixel,
                        self.rand_color,
                        (chan, h, w),
                        dtype=dtype,
                        device=self.device,
                    )
                    break

    def _erase_cube(
        self,
        img,
        batch_start,
        batch_size,
        chan,
        img_h,
        img_w,
        dtype,
    ):
        if random.random() > self.probability:
            return
        area = img_h * img_w
        count = (
            self.min_count if self.min_count == self.max_count else
            random.randint(self.min_count, self.max_count))
        for _ in range(count):
            for _ in range(100):
                target_area = (
                    random.uniform(self.min_area, self.max_area) * area /
                    count)
                aspect_ratio = math.exp(random.uniform(*self.log_aspect_ratio))
                h = int(round(math.sqrt(target_area * aspect_ratio)))
                w = int(round(math.sqrt(target_area / aspect_ratio)))
                if w < img_w and h < img_h:
                    top = random.randint(0, img_h - h)
                    left = random.randint(0, img_w - w)
                    for i in range(batch_start, batch_size):
                        img_instance = img[i]
                        img_instance[:, top:top + h,
                                     left:left + w] = _get_pixels(
                                         self.per_pixel,
                                         self.rand_color,
                                         (chan, h, w),
                                         dtype=dtype,
                                         device=self.device,
                                     )
                    break

    def __call__(self, input):
        if len(input.size()) == 3:
            self._erase(input, *input.size(), input.dtype)
        else:
            batch_size, chan, img_h, img_w = input.size()
            # skip first slice of batch if num_splits is set (for clean portion of samples)
            batch_start = (
                batch_size // self.num_splits if self.num_splits > 1 else 0)
            if self.cube:
                self._erase_cube(
                    input,
                    batch_start,
                    batch_size,
                    chan,
                    img_h,
                    img_w,
                    input.dtype,
                )
            else:
                for i in range(batch_start, batch_size):
                    self._erase(input[i], chan, img_h, img_w, input.dtype)
        return input

```

`deepcheat/VideoMAEv2/dataset/transforms.py`:

```py
# --------------------------------------------------------
# Based on BEiT, timm, DINO and DeiT code bases
# https://github.com/microsoft/unilm/tree/master/beit
# https://github.com/rwightman/pytorch-image-models/tree/master/timm
# https://github.com/facebookresearch/deit
# https://github.com/facebookresearch/dino
# --------------------------------------------------------'
import math
import numbers
import random
import warnings

import numpy as np
import torch
import torchvision
import torchvision.transforms.functional as F
from PIL import Image, ImageOps


class ToNumpy:

    def __call__(self, pil_img):
        np_img = np.array(pil_img, dtype=np.uint8)
        if np_img.ndim < 3:
            np_img = np.expand_dims(np_img, axis=-1)
        np_img = np.rollaxis(np_img, 2)  # HWC to CHW
        return np_img


class ToTensor:

    def __init__(self, dtype=torch.float32):
        self.dtype = dtype

    def __call__(self, pil_img):
        np_img = np.array(pil_img, dtype=np.uint8)
        if np_img.ndim < 3:
            np_img = np.expand_dims(np_img, axis=-1)
        np_img = np.rollaxis(np_img, 2)  # HWC to CHW
        return torch.from_numpy(np_img).to(dtype=self.dtype)


_pil_interpolation_to_str = {
    Image.NEAREST: 'PIL.Image.NEAREST',
    Image.BILINEAR: 'PIL.Image.BILINEAR',
    Image.BICUBIC: 'PIL.Image.BICUBIC',
    Image.LANCZOS: 'PIL.Image.LANCZOS',
    Image.HAMMING: 'PIL.Image.HAMMING',
    Image.BOX: 'PIL.Image.BOX',
}


def _pil_interp(method):
    if method == 'bicubic':
        return Image.BICUBIC
    elif method == 'lanczos':
        return Image.LANCZOS
    elif method == 'hamming':
        return Image.HAMMING
    else:
        # default bilinear, do we want to allow nearest?
        return Image.BILINEAR


_RANDOM_INTERPOLATION = (Image.BILINEAR, Image.BICUBIC)


class RandomResizedCropAndInterpolationWithTwoPic:
    """Crop the given PIL Image to random size and aspect ratio with random interpolation.

    A crop of random size (default: of 0.08 to 1.0) of the original size and a random
    aspect ratio (default: of 3/4 to 4/3) of the original aspect ratio is made. This crop
    is finally resized to given size.
    This is popularly used to train the Inception networks.

    Args:
        size: expected output size of each edge
        scale: range of size of the origin size cropped
        ratio: range of aspect ratio of the origin aspect ratio cropped
        interpolation: Default: PIL.Image.BILINEAR
    """

    def __init__(self,
                 size,
                 second_size=None,
                 scale=(0.08, 1.0),
                 ratio=(3. / 4., 4. / 3.),
                 interpolation='bilinear',
                 second_interpolation='lanczos'):
        if isinstance(size, tuple):
            self.size = size
        else:
            self.size = (size, size)
        if second_size is not None:
            if isinstance(second_size, tuple):
                self.second_size = second_size
            else:
                self.second_size = (second_size, second_size)
        else:
            self.second_size = None
        if (scale[0] > scale[1]) or (ratio[0] > ratio[1]):
            warnings.warn("range should be of kind (min, max)")

        if interpolation == 'random':
            self.interpolation = _RANDOM_INTERPOLATION
        else:
            self.interpolation = _pil_interp(interpolation)
        self.second_interpolation = _pil_interp(second_interpolation)
        self.scale = scale
        self.ratio = ratio

    @staticmethod
    def get_params(img, scale, ratio):
        """Get parameters for ``crop`` for a random sized crop.

        Args:
            img (PIL Image): Image to be cropped.
            scale (tuple): range of size of the origin size cropped
            ratio (tuple): range of aspect ratio of the origin aspect ratio cropped

        Returns:
            tuple: params (i, j, h, w) to be passed to ``crop`` for a random
                sized crop.
        """
        area = img.size[0] * img.size[1]

        for attempt in range(10):
            target_area = random.uniform(*scale) * area
            log_ratio = (math.log(ratio[0]), math.log(ratio[1]))
            aspect_ratio = math.exp(random.uniform(*log_ratio))

            w = int(round(math.sqrt(target_area * aspect_ratio)))
            h = int(round(math.sqrt(target_area / aspect_ratio)))

            if w <= img.size[0] and h <= img.size[1]:
                i = random.randint(0, img.size[1] - h)
                j = random.randint(0, img.size[0] - w)
                return i, j, h, w

        # Fallback to central crop
        in_ratio = img.size[0] / img.size[1]
        if in_ratio < min(ratio):
            w = img.size[0]
            h = int(round(w / min(ratio)))
        elif in_ratio > max(ratio):
            h = img.size[1]
            w = int(round(h * max(ratio)))
        else:  # whole image
            w = img.size[0]
            h = img.size[1]
        i = (img.size[1] - h) // 2
        j = (img.size[0] - w) // 2
        return i, j, h, w

    def __call__(self, img):
        """
        Args:
            img (PIL Image): Image to be cropped and resized.

        Returns:
            PIL Image: Randomly cropped and resized image.
        """
        i, j, h, w = self.get_params(img, self.scale, self.ratio)
        if isinstance(self.interpolation, (tuple, list)):
            interpolation = random.choice(self.interpolation)
        else:
            interpolation = self.interpolation
        if self.second_size is None:
            return F.resized_crop(img, i, j, h, w, self.size, interpolation)
        else:
            return F.resized_crop(img, i, j, h, w, self.size,
                                  interpolation), F.resized_crop(
                                      img, i, j, h, w, self.second_size,
                                      self.second_interpolation)

    def __repr__(self):
        if isinstance(self.interpolation, (tuple, list)):
            interpolate_str = ' '.join(
                [_pil_interpolation_to_str[x] for x in self.interpolation])
        else:
            interpolate_str = _pil_interpolation_to_str[self.interpolation]
        format_string = self.__class__.__name__ + '(size={0}'.format(self.size)
        format_string += ', scale={0}'.format(
            tuple(round(s, 4) for s in self.scale))
        format_string += ', ratio={0}'.format(
            tuple(round(r, 4) for r in self.ratio))
        format_string += ', interpolation={0}'.format(interpolate_str)
        if self.second_size is not None:
            format_string += ', second_size={0}'.format(self.second_size)
            format_string += ', second_interpolation={0}'.format(
                _pil_interpolation_to_str[self.second_interpolation])
        format_string += ')'
        return format_string


class GroupRandomCrop(object):

    def __init__(self, size):
        if isinstance(size, numbers.Number):
            self.size = (int(size), int(size))
        else:
            self.size = size

    def __call__(self, img_tuple):
        img_group, label = img_tuple

        w, h = img_group[0].size
        th, tw = self.size

        out_images = list()

        x1 = random.randint(0, w - tw)
        y1 = random.randint(0, h - th)

        for img in img_group:
            assert (img.size[0] == w and img.size[1] == h)
            if w == tw and h == th:
                out_images.append(img)
            else:
                out_images.append(img.crop((x1, y1, x1 + tw, y1 + th)))

        return (out_images, label)


class GroupCenterCrop(object):

    def __init__(self, size):
        self.worker = torchvision.transforms.CenterCrop(size)

    def __call__(self, img_tuple):
        img_group, label = img_tuple
        return ([self.worker(img) for img in img_group], label)


class GroupRandomHorizontalFlip(object):
    """Randomly horizontally flips the given PIL.Image with a probability of 0.5
    """

    def __init__(self, selective_flip=True, is_flow=False):
        self.is_flow = is_flow
        self.class_LeftRight = [86, 87, 93, 94, 166, 167
                                ] if selective_flip else []

    def __call__(self, img_tuple, is_flow=False):
        img_group, label = img_tuple
        v = random.random()
        if (label not in self.class_LeftRight) and v < 0.5:
            ret = [img.transpose(Image.FLIP_LEFT_RIGHT) for img in img_group]
            if self.is_flow:
                for i in range(0, len(ret), 2):
                    ret[i] = ImageOps.invert(
                        ret[i])  # invert flow pixel values when flipping
            return (ret, label)
        else:
            return img_tuple


class GroupNormalize(object):

    def __init__(self, mean, std):
        self.mean = mean
        self.std = std

    def __call__(self, tensor_tuple):
        tensor, label = tensor_tuple
        rep_mean = self.mean * (tensor.size()[0] // len(self.mean))
        rep_std = self.std * (tensor.size()[0] // len(self.std))

        # TODO: make efficient
        for t, m, s in zip(tensor, rep_mean, rep_std):
            t.sub_(m).div_(s)

        return (tensor, label)


class GroupGrayScale(object):

    def __init__(self, size):
        self.worker = torchvision.transforms.Grayscale(size)

    def __call__(self, img_tuple):
        img_group, label = img_tuple
        return ([self.worker(img) for img in img_group], label)


class GroupScale(object):
    """ Rescales the input PIL.Image to the given 'size'.
    'size' will be the size of the smaller edge.
    For example, if height > width, then image will be
    rescaled to (size * height / width, size)
    size: size of the smaller edge
    interpolation: Default: PIL.Image.BILINEAR
    """

    def __init__(self, size, interpolation=Image.BILINEAR):
        self.worker = torchvision.transforms.Resize(size, interpolation)

    def __call__(self, img_tuple):
        img_group, label = img_tuple
        return ([self.worker(img) for img in img_group], label)


class GroupOverSample(object):

    def __init__(self, crop_size, scale_size=None):
        self.crop_size = crop_size if not isinstance(crop_size, int) else (
            crop_size, crop_size)

        if scale_size is not None:
            self.scale_worker = GroupScale(scale_size)
        else:
            self.scale_worker = None

    def __call__(self, img_tuple):
        if self.scale_worker is not None:
            img_tuple = self.scale_worker(img_tuple)

        img_group, label = img_tuple

        image_w, image_h = img_group[0].size
        crop_w, crop_h = self.crop_size

        offsets = GroupMultiScaleCrop.fill_fix_offset(False, image_w, image_h,
                                                      crop_w, crop_h)
        oversample_group = list()
        for o_w, o_h in offsets:
            normal_group = list()
            flip_group = list()
            for i, img in enumerate(img_group):
                crop = img.crop((o_w, o_h, o_w + crop_w, o_h + crop_h))
                normal_group.append(crop)
                flip_crop = crop.copy().transpose(Image.FLIP_LEFT_RIGHT)

                if img.mode == 'L' and i % 2 == 0:
                    flip_group.append(ImageOps.invert(flip_crop))
                else:
                    flip_group.append(flip_crop)

            oversample_group.extend(normal_group)
            oversample_group.extend(flip_group)
        return (oversample_group, label)


class GroupFullResSample(object):

    def __init__(self, crop_size, scale_size=None, flip=True):
        self.crop_size = crop_size if not isinstance(crop_size, int) else (
            crop_size, crop_size)

        if scale_size is not None:
            self.scale_worker = GroupScale(scale_size)
        else:
            self.scale_worker = None
        self.flip = flip

    def __call__(self, img_tuple):

        if self.scale_worker is not None:
            img_tuple = self.scale_worker(img_tuple)

        img_group, label = img_tuple
        image_w, image_h = img_group[0].size
        crop_w, crop_h = self.crop_size

        w_step = (image_w - crop_w) // 4
        h_step = (image_h - crop_h) // 4

        offsets = list()
        offsets.append((0 * w_step, 2 * h_step))  # left
        offsets.append((4 * w_step, 2 * h_step))  # right
        offsets.append((2 * w_step, 2 * h_step))  # center

        oversample_group = list()
        for o_w, o_h in offsets:
            normal_group = list()
            flip_group = list()
            for i, img in enumerate(img_group):
                crop = img.crop((o_w, o_h, o_w + crop_w, o_h + crop_h))
                normal_group.append(crop)
                if self.flip:
                    flip_crop = crop.copy().transpose(Image.FLIP_LEFT_RIGHT)

                    if img.mode == 'L' and i % 2 == 0:
                        flip_group.append(ImageOps.invert(flip_crop))
                    else:
                        flip_group.append(flip_crop)

            oversample_group.extend(normal_group)
            oversample_group.extend(flip_group)
        return (oversample_group, label)


class GroupMultiScaleCrop(object):

    def __init__(self,
                 input_size,
                 scales=None,
                 max_distort=1,
                 fix_crop=True,
                 more_fix_crop=True):
        self.scales = scales if scales is not None else [1, .875, .75, .66]
        self.max_distort = max_distort
        self.fix_crop = fix_crop
        self.more_fix_crop = more_fix_crop
        self.input_size = input_size if not isinstance(input_size, int) else [
            input_size, input_size
        ]
        self.interpolation = Image.BILINEAR

    def __call__(self, img_tuple):
        img_group, label = img_tuple

        im_size = img_group[0].size

        crop_w, crop_h, offset_w, offset_h = self._sample_crop_size(im_size)
        crop_img_group = [
            img.crop(
                (offset_w, offset_h, offset_w + crop_w, offset_h + crop_h))
            for img in img_group
        ]
        ret_img_group = [
            img.resize((self.input_size[0], self.input_size[1]),
                       self.interpolation) for img in crop_img_group
        ]
        return (ret_img_group, label)

    def _sample_crop_size(self, im_size):
        image_w, image_h = im_size[0], im_size[1]

        # find a crop size
        base_size = min(image_w, image_h)
        crop_sizes = [int(base_size * x) for x in self.scales]
        crop_h = [
            self.input_size[1] if abs(x - self.input_size[1]) < 3 else x
            for x in crop_sizes
        ]
        crop_w = [
            self.input_size[0] if abs(x - self.input_size[0]) < 3 else x
            for x in crop_sizes
        ]

        pairs = []
        for i, h in enumerate(crop_h):
            for j, w in enumerate(crop_w):
                if abs(i - j) <= self.max_distort:
                    pairs.append((w, h))

        crop_pair = random.choice(pairs)
        if not self.fix_crop:
            w_offset = random.randint(0, image_w - crop_pair[0])
            h_offset = random.randint(0, image_h - crop_pair[1])
        else:
            w_offset, h_offset = self._sample_fix_offset(
                image_w, image_h, crop_pair[0], crop_pair[1])

        return crop_pair[0], crop_pair[1], w_offset, h_offset

    def _sample_fix_offset(self, image_w, image_h, crop_w, crop_h):
        offsets = self.fill_fix_offset(self.more_fix_crop, image_w, image_h,
                                       crop_w, crop_h)
        return random.choice(offsets)

    @staticmethod
    def fill_fix_offset(more_fix_crop, image_w, image_h, crop_w, crop_h):
        w_step = (image_w - crop_w) // 4
        h_step = (image_h - crop_h) // 4

        ret = list()
        ret.append((0, 0))  # upper left
        ret.append((4 * w_step, 0))  # upper right
        ret.append((0, 4 * h_step))  # lower left
        ret.append((4 * w_step, 4 * h_step))  # lower right
        ret.append((2 * w_step, 2 * h_step))  # center

        if more_fix_crop:
            ret.append((0, 2 * h_step))  # center left
            ret.append((4 * w_step, 2 * h_step))  # center right
            ret.append((2 * w_step, 4 * h_step))  # lower center
            ret.append((2 * w_step, 0 * h_step))  # upper center

            ret.append((1 * w_step, 1 * h_step))  # upper left quarter
            ret.append((3 * w_step, 1 * h_step))  # upper right quarter
            ret.append((1 * w_step, 3 * h_step))  # lower left quarter
            ret.append((3 * w_step, 3 * h_step))  # lower righ quarter

        return ret


class GroupRandomSizedCrop(object):
    """Random crop the given PIL.Image to a random size of (0.08 to 1.0) of the original size
    and and a random aspect ratio of 3/4 to 4/3 of the original aspect ratio
    This is popularly used to train the Inception networks
    size: size of the smaller edge
    interpolation: Default: PIL.Image.BILINEAR
    """

    def __init__(self, size, interpolation=Image.BILINEAR):
        self.size = size
        self.interpolation = interpolation

    def __call__(self, img_tuple):
        img_group, label = img_tuple

        for attempt in range(10):
            area = img_group[0].size[0] * img_group[0].size[1]
            target_area = random.uniform(0.08, 1.0) * area
            aspect_ratio = random.uniform(3. / 4, 4. / 3)

            w = int(round(math.sqrt(target_area * aspect_ratio)))
            h = int(round(math.sqrt(target_area / aspect_ratio)))

            if random.random() < 0.5:
                w, h = h, w

            if w <= img_group[0].size[0] and h <= img_group[0].size[1]:
                x1 = random.randint(0, img_group[0].size[0] - w)
                y1 = random.randint(0, img_group[0].size[1] - h)
                found = True
                break
        else:
            found = False
            x1 = 0
            y1 = 0

        if found:
            out_group = list()
            for img in img_group:
                img = img.crop((x1, y1, x1 + w, y1 + h))
                assert (img.size == (w, h))
                out_group.append(
                    img.resize((self.size, self.size), self.interpolation))
            return out_group
        else:
            # Fallback
            scale = GroupScale(self.size, interpolation=self.interpolation)
            crop = GroupRandomCrop(self.size)
            return crop(scale(img_group))


class Stack(object):

    def __init__(self, roll=False):
        self.roll = roll

    def __call__(self, img_tuple):
        img_group, label = img_tuple

        if img_group[0].mode == 'L':
            return (np.concatenate([np.expand_dims(x, 2) for x in img_group],
                                   axis=2), label)
        elif img_group[0].mode == 'RGB':
            if self.roll:
                return (np.concatenate(
                    [np.array(x)[:, :, ::-1] for x in img_group],
                    axis=2), label)
            else:
                return (np.concatenate(img_group, axis=2), label)


class ToTorchFormatTensor(object):
    """ Converts a PIL.Image (RGB) or numpy.ndarray (H x W x C) in the range [0, 255]
    to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0] """

    def __init__(self, div=True):
        self.div = div

    def __call__(self, pic_tuple):
        pic, label = pic_tuple

        if isinstance(pic, np.ndarray):
            # handle numpy array
            img = torch.from_numpy(pic).permute(2, 0, 1).contiguous()
        else:
            # handle PIL Image
            img = torch.as_tensor(pic.tobytes(), dtype=torch.uint8)
            img = img.view(pic.size[1], pic.size[0], len(pic.mode))
            # put it from HWC to CHW format
            # yikes, this transpose takes 80% of the loading time/CPU
            img = img.transpose(0, 1).transpose(0, 2).contiguous()
        return (img.float().div(255.) if self.div else img.float(), label)


class IdentityTransform(object):

    def __call__(self, data):
        return data

```

`deepcheat/VideoMAEv2/dataset/video_transforms.py`:

```py
#!/usr/bin/env python3
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.

import math
import numbers
import random

import numpy as np
import PIL
import torch
import torchvision
import torchvision.transforms.functional as F
from PIL import Image
from torchvision import transforms

from . import functional as FF
from .rand_augment import rand_augment_transform
from .random_erasing import RandomErasing

_pil_interpolation_to_str = {
    Image.NEAREST: "PIL.Image.NEAREST",
    Image.BILINEAR: "PIL.Image.BILINEAR",
    Image.BICUBIC: "PIL.Image.BICUBIC",
    Image.LANCZOS: "PIL.Image.LANCZOS",
    Image.HAMMING: "PIL.Image.HAMMING",
    Image.BOX: "PIL.Image.BOX",
}

_RANDOM_INTERPOLATION = (Image.BILINEAR, Image.BICUBIC)


def _pil_interp(method):
    if method == "bicubic":
        return Image.BICUBIC
    elif method == "lanczos":
        return Image.LANCZOS
    elif method == "hamming":
        return Image.HAMMING
    else:
        return Image.BILINEAR


def random_short_side_scale_jitter(images,
                                   min_size,
                                   max_size,
                                   boxes=None,
                                   inverse_uniform_sampling=False):
    """
    Perform a spatial short scale jittering on the given images and
    corresponding boxes.
    Args:
        images (tensor): images to perform scale jitter. Dimension is
            `num frames` x `channel` x `height` x `width`.
        min_size (int): the minimal size to scale the frames.
        max_size (int): the maximal size to scale the frames.
        boxes (ndarray): optional. Corresponding boxes to images.
            Dimension is `num boxes` x 4.
        inverse_uniform_sampling (bool): if True, sample uniformly in
            [1 / max_scale, 1 / min_scale] and take a reciprocal to get the
            scale. If False, take a uniform sample from [min_scale, max_scale].
    Returns:
        (tensor): the scaled images with dimension of
            `num frames` x `channel` x `new height` x `new width`.
        (ndarray or None): the scaled boxes with dimension of
            `num boxes` x 4.
    """
    if inverse_uniform_sampling:
        size = int(
            round(1.0 / np.random.uniform(1.0 / max_size, 1.0 / min_size)))
    else:
        size = int(round(np.random.uniform(min_size, max_size)))

    height = images.shape[2]
    width = images.shape[3]
    if (width <= height and width == size) or (height <= width
                                               and height == size):
        return images, boxes
    new_width = size
    new_height = size
    if width < height:
        new_height = int(math.floor((float(height) / width) * size))
        if boxes is not None:
            boxes = boxes * float(new_height) / height
    else:
        new_width = int(math.floor((float(width) / height) * size))
        if boxes is not None:
            boxes = boxes * float(new_width) / width

    return (
        torch.nn.functional.interpolate(
            images,
            size=(new_height, new_width),
            mode="bilinear",
            align_corners=False,
        ),
        boxes,
    )


def crop_boxes(boxes, x_offset, y_offset):
    """
    Peform crop on the bounding boxes given the offsets.
    Args:
        boxes (ndarray or None): bounding boxes to peform crop. The dimension
            is `num boxes` x 4.
        x_offset (int): cropping offset in the x axis.
        y_offset (int): cropping offset in the y axis.
    Returns:
        cropped_boxes (ndarray or None): the cropped boxes with dimension of
            `num boxes` x 4.
    """
    cropped_boxes = boxes.copy()
    cropped_boxes[:, [0, 2]] = boxes[:, [0, 2]] - x_offset
    cropped_boxes[:, [1, 3]] = boxes[:, [1, 3]] - y_offset

    return cropped_boxes


def random_crop(images, size, boxes=None):
    """
    Perform random spatial crop on the given images and corresponding boxes.
    Args:
        images (tensor): images to perform random crop. The dimension is
            `num frames` x `channel` x `height` x `width`.
        size (int): the size of height and width to crop on the image.
        boxes (ndarray or None): optional. Corresponding boxes to images.
            Dimension is `num boxes` x 4.
    Returns:
        cropped (tensor): cropped images with dimension of
            `num frames` x `channel` x `size` x `size`.
        cropped_boxes (ndarray or None): the cropped boxes with dimension of
            `num boxes` x 4.
    """
    if images.shape[2] == size and images.shape[3] == size:
        return images
    height = images.shape[2]
    width = images.shape[3]
    y_offset = 0
    if height > size:
        y_offset = int(np.random.randint(0, height - size))
    x_offset = 0
    if width > size:
        x_offset = int(np.random.randint(0, width - size))
    cropped = images[:, :, y_offset:y_offset + size, x_offset:x_offset + size]

    cropped_boxes = (
        crop_boxes(boxes, x_offset, y_offset) if boxes is not None else None)

    return cropped, cropped_boxes


def horizontal_flip(prob, images, boxes=None):
    """
    Perform horizontal flip on the given images and corresponding boxes.
    Args:
        prob (float): probility to flip the images.
        images (tensor): images to perform horizontal flip, the dimension is
            `num frames` x `channel` x `height` x `width`.
        boxes (ndarray or None): optional. Corresponding boxes to images.
            Dimension is `num boxes` x 4.
    Returns:
        images (tensor): images with dimension of
            `num frames` x `channel` x `height` x `width`.
        flipped_boxes (ndarray or None): the flipped boxes with dimension of
            `num boxes` x 4.
    """
    if boxes is None:
        flipped_boxes = None
    else:
        flipped_boxes = boxes.copy()

    if np.random.uniform() < prob:
        images = images.flip((-1))

        if len(images.shape) == 3:
            width = images.shape[2]
        elif len(images.shape) == 4:
            width = images.shape[3]
        else:
            raise NotImplementedError("Dimension does not supported")
        if boxes is not None:
            flipped_boxes[:, [0, 2]] = width - boxes[:, [2, 0]] - 1

    return images, flipped_boxes


def uniform_crop(images, size, spatial_idx, boxes=None, scale_size=None):
    """
    Perform uniform spatial sampling on the images and corresponding boxes.
    Args:
        images (tensor): images to perform uniform crop. The dimension is
            `num frames` x `channel` x `height` x `width`.
        size (int): size of height and weight to crop the images.
        spatial_idx (int): 0, 1, or 2 for left, center, and right crop if width
            is larger than height. Or 0, 1, or 2 for top, center, and bottom
            crop if height is larger than width.
        boxes (ndarray or None): optional. Corresponding boxes to images.
            Dimension is `num boxes` x 4.
        scale_size (int): optinal. If not None, resize the images to scale_size before
            performing any crop.
    Returns:
        cropped (tensor): images with dimension of
            `num frames` x `channel` x `size` x `size`.
        cropped_boxes (ndarray or None): the cropped boxes with dimension of
            `num boxes` x 4.
    """
    assert spatial_idx in [0, 1, 2]
    ndim = len(images.shape)
    if ndim == 3:
        images = images.unsqueeze(0)
    height = images.shape[2]
    width = images.shape[3]

    if scale_size is not None:
        if width <= height:
            width, height = scale_size, int(height / width * scale_size)
        else:
            width, height = int(width / height * scale_size), scale_size
        images = torch.nn.functional.interpolate(
            images,
            size=(height, width),
            mode="bilinear",
            align_corners=False,
        )

    y_offset = int(math.ceil((height - size) / 2))
    x_offset = int(math.ceil((width - size) / 2))

    if height > width:
        if spatial_idx == 0:
            y_offset = 0
        elif spatial_idx == 2:
            y_offset = height - size
    else:
        if spatial_idx == 0:
            x_offset = 0
        elif spatial_idx == 2:
            x_offset = width - size
    cropped = images[:, :, y_offset:y_offset + size, x_offset:x_offset + size]
    cropped_boxes = (
        crop_boxes(boxes, x_offset, y_offset) if boxes is not None else None)
    if ndim == 3:
        cropped = cropped.squeeze(0)
    return cropped, cropped_boxes


def clip_boxes_to_image(boxes, height, width):
    """
    Clip an array of boxes to an image with the given height and width.
    Args:
        boxes (ndarray): bounding boxes to perform clipping.
            Dimension is `num boxes` x 4.
        height (int): given image height.
        width (int): given image width.
    Returns:
        clipped_boxes (ndarray): the clipped boxes with dimension of
            `num boxes` x 4.
    """
    clipped_boxes = boxes.copy()
    clipped_boxes[:, [0, 2]] = np.minimum(width - 1.0,
                                          np.maximum(0.0, boxes[:, [0, 2]]))
    clipped_boxes[:, [1, 3]] = np.minimum(height - 1.0,
                                          np.maximum(0.0, boxes[:, [1, 3]]))
    return clipped_boxes


def blend(images1, images2, alpha):
    """
    Blend two images with a given weight alpha.
    Args:
        images1 (tensor): the first images to be blended, the dimension is
            `num frames` x `channel` x `height` x `width`.
        images2 (tensor): the second images to be blended, the dimension is
            `num frames` x `channel` x `height` x `width`.
        alpha (float): the blending weight.
    Returns:
        (tensor): blended images, the dimension is
            `num frames` x `channel` x `height` x `width`.
    """
    return images1 * alpha + images2 * (1 - alpha)


def grayscale(images):
    """
    Get the grayscale for the input images. The channels of images should be
    in order BGR.
    Args:
        images (tensor): the input images for getting grayscale. Dimension is
            `num frames` x `channel` x `height` x `width`.
    Returns:
        img_gray (tensor): blended images, the dimension is
            `num frames` x `channel` x `height` x `width`.
    """
    # R -> 0.299, G -> 0.587, B -> 0.114.
    img_gray = torch.tensor(images)
    gray_channel = (0.299 * images[:, 2] + 0.587 * images[:, 1] +
                    0.114 * images[:, 0])
    img_gray[:, 0] = gray_channel
    img_gray[:, 1] = gray_channel
    img_gray[:, 2] = gray_channel
    return img_gray


def color_jitter(images, img_brightness=0, img_contrast=0, img_saturation=0):
    """
    Perfrom a color jittering on the input images. The channels of images
    should be in order BGR.
    Args:
        images (tensor): images to perform color jitter. Dimension is
            `num frames` x `channel` x `height` x `width`.
        img_brightness (float): jitter ratio for brightness.
        img_contrast (float): jitter ratio for contrast.
        img_saturation (float): jitter ratio for saturation.
    Returns:
        images (tensor): the jittered images, the dimension is
            `num frames` x `channel` x `height` x `width`.
    """

    jitter = []
    if img_brightness != 0:
        jitter.append("brightness")
    if img_contrast != 0:
        jitter.append("contrast")
    if img_saturation != 0:
        jitter.append("saturation")

    if len(jitter) > 0:
        order = np.random.permutation(np.arange(len(jitter)))
        for idx in range(0, len(jitter)):
            if jitter[order[idx]] == "brightness":
                images = brightness_jitter(img_brightness, images)
            elif jitter[order[idx]] == "contrast":
                images = contrast_jitter(img_contrast, images)
            elif jitter[order[idx]] == "saturation":
                images = saturation_jitter(img_saturation, images)
    return images


def brightness_jitter(var, images):
    """
    Perfrom brightness jittering on the input images. The channels of images
    should be in order BGR.
    Args:
        var (float): jitter ratio for brightness.
        images (tensor): images to perform color jitter. Dimension is
            `num frames` x `channel` x `height` x `width`.
    Returns:
        images (tensor): the jittered images, the dimension is
            `num frames` x `channel` x `height` x `width`.
    """
    alpha = 1.0 + np.random.uniform(-var, var)

    img_bright = torch.zeros(images.shape)
    images = blend(images, img_bright, alpha)
    return images


def contrast_jitter(var, images):
    """
    Perfrom contrast jittering on the input images. The channels of images
    should be in order BGR.
    Args:
        var (float): jitter ratio for contrast.
        images (tensor): images to perform color jitter. Dimension is
            `num frames` x `channel` x `height` x `width`.
    Returns:
        images (tensor): the jittered images, the dimension is
            `num frames` x `channel` x `height` x `width`.
    """
    alpha = 1.0 + np.random.uniform(-var, var)

    img_gray = grayscale(images)
    img_gray[:] = torch.mean(img_gray, dim=(1, 2, 3), keepdim=True)
    images = blend(images, img_gray, alpha)
    return images


def saturation_jitter(var, images):
    """
    Perfrom saturation jittering on the input images. The channels of images
    should be in order BGR.
    Args:
        var (float): jitter ratio for saturation.
        images (tensor): images to perform color jitter. Dimension is
            `num frames` x `channel` x `height` x `width`.
    Returns:
        images (tensor): the jittered images, the dimension is
            `num frames` x `channel` x `height` x `width`.
    """
    alpha = 1.0 + np.random.uniform(-var, var)
    img_gray = grayscale(images)
    images = blend(images, img_gray, alpha)

    return images


def lighting_jitter(images, alphastd, eigval, eigvec):
    """
    Perform AlexNet-style PCA jitter on the given images.
    Args:
        images (tensor): images to perform lighting jitter. Dimension is
            `num frames` x `channel` x `height` x `width`.
        alphastd (float): jitter ratio for PCA jitter.
        eigval (list): eigenvalues for PCA jitter.
        eigvec (list[list]): eigenvectors for PCA jitter.
    Returns:
        out_images (tensor): the jittered images, the dimension is
            `num frames` x `channel` x `height` x `width`.
    """
    if alphastd == 0:
        return images
    # generate alpha1, alpha2, alpha3.
    alpha = np.random.normal(0, alphastd, size=(1, 3))
    eig_vec = np.array(eigvec)
    eig_val = np.reshape(eigval, (1, 3))
    rgb = np.sum(
        eig_vec * np.repeat(alpha, 3, axis=0) * np.repeat(eig_val, 3, axis=0),
        axis=1,
    )
    out_images = torch.zeros_like(images)
    if len(images.shape) == 3:
        # C H W
        channel_dim = 0
    elif len(images.shape) == 4:
        # T C H W
        channel_dim = 1
    else:
        raise NotImplementedError(f"Unsupported dimension {len(images.shape)}")

    for idx in range(images.shape[channel_dim]):
        # C H W
        if len(images.shape) == 3:
            out_images[idx] = images[idx] + rgb[2 - idx]
        # T C H W
        elif len(images.shape) == 4:
            out_images[:, idx] = images[:, idx] + rgb[2 - idx]
        else:
            raise NotImplementedError(
                f"Unsupported dimension {len(images.shape)}")

    return out_images


def color_normalization(images, mean, stddev):
    """
    Perform color nomration on the given images.
    Args:
        images (tensor): images to perform color normalization. Dimension is
            `num frames` x `channel` x `height` x `width`.
        mean (list): mean values for normalization.
        stddev (list): standard deviations for normalization.

    Returns:
        out_images (tensor): the noramlized images, the dimension is
            `num frames` x `channel` x `height` x `width`.
    """
    if len(images.shape) == 3:
        assert (
            len(mean) == images.shape[0]), "channel mean not computed properly"
        assert (len(stddev) == images.shape[0]
                ), "channel stddev not computed properly"
    elif len(images.shape) == 4:
        assert (
            len(mean) == images.shape[1]), "channel mean not computed properly"
        assert (len(stddev) == images.shape[1]
                ), "channel stddev not computed properly"
    else:
        raise NotImplementedError(f"Unsupported dimension {len(images.shape)}")

    out_images = torch.zeros_like(images)
    for idx in range(len(mean)):
        # C H W
        if len(images.shape) == 3:
            out_images[idx] = (images[idx] - mean[idx]) / stddev[idx]
        elif len(images.shape) == 4:
            out_images[:, idx] = (images[:, idx] - mean[idx]) / stddev[idx]
        else:
            raise NotImplementedError(
                f"Unsupported dimension {len(images.shape)}")
    return out_images


def _get_param_spatial_crop(scale,
                            ratio,
                            height,
                            width,
                            num_repeat=10,
                            log_scale=True,
                            switch_hw=False):
    """
    Given scale, ratio, height and width, return sampled coordinates of the videos.
    """
    for _ in range(num_repeat):
        area = height * width
        target_area = random.uniform(*scale) * area
        if log_scale:
            log_ratio = (math.log(ratio[0]), math.log(ratio[1]))
            aspect_ratio = math.exp(random.uniform(*log_ratio))
        else:
            aspect_ratio = random.uniform(*ratio)

        w = int(round(math.sqrt(target_area * aspect_ratio)))
        h = int(round(math.sqrt(target_area / aspect_ratio)))

        if np.random.uniform() < 0.5 and switch_hw:
            w, h = h, w

        if 0 < w <= width and 0 < h <= height:
            i = random.randint(0, height - h)
            j = random.randint(0, width - w)
            return i, j, h, w

    # Fallback to central crop
    in_ratio = float(width) / float(height)
    if in_ratio < min(ratio):
        w = width
        h = int(round(w / min(ratio)))
    elif in_ratio > max(ratio):
        h = height
        w = int(round(h * max(ratio)))
    else:  # whole image
        w = width
        h = height
    i = (height - h) // 2
    j = (width - w) // 2
    return i, j, h, w


def random_resized_crop(
        images,
        target_height,
        target_width,
        scale=(0.8, 1.0),
        ratio=(3.0 / 4.0, 4.0 / 3.0),
):
    """
    Crop the given images to random size and aspect ratio. A crop of random
    size (default: of 0.08 to 1.0) of the original size and a random aspect
    ratio (default: of 3/4 to 4/3) of the original aspect ratio is made. This
    crop is finally resized to given size. This is popularly used to train the
    Inception networks.

    Args:
        images: Images to perform resizing and cropping.
        target_height: Desired height after cropping.
        target_width: Desired width after cropping.
        scale: Scale range of Inception-style area based random resizing.
        ratio: Aspect ratio range of Inception-style area based random resizing.
    """

    height = images.shape[2]
    width = images.shape[3]

    i, j, h, w = _get_param_spatial_crop(scale, ratio, height, width)
    cropped = images[:, :, i:i + h, j:j + w]
    return torch.nn.functional.interpolate(
        cropped,
        size=(target_height, target_width),
        mode="bilinear",
        align_corners=False,
    )


def random_resized_crop_with_shift(
        images,
        target_height,
        target_width,
        scale=(0.8, 1.0),
        ratio=(3.0 / 4.0, 4.0 / 3.0),
):
    """
    This is similar to random_resized_crop. However, it samples two different
    boxes (for cropping) for the first and last frame. It then linearly
    interpolates the two boxes for other frames.

    Args:
        images: Images to perform resizing and cropping.
        target_height: Desired height after cropping.
        target_width: Desired width after cropping.
        scale: Scale range of Inception-style area based random resizing.
        ratio: Aspect ratio range of Inception-style area based random resizing.
    """
    t = images.shape[1]
    height = images.shape[2]
    width = images.shape[3]

    i, j, h, w = _get_param_spatial_crop(scale, ratio, height, width)
    i_, j_, h_, w_ = _get_param_spatial_crop(scale, ratio, height, width)
    i_s = [int(i) for i in torch.linspace(i, i_, steps=t).tolist()]
    j_s = [int(i) for i in torch.linspace(j, j_, steps=t).tolist()]
    h_s = [int(i) for i in torch.linspace(h, h_, steps=t).tolist()]
    w_s = [int(i) for i in torch.linspace(w, w_, steps=t).tolist()]
    out = torch.zeros((3, t, target_height, target_width))
    for ind in range(t):
        out[:, ind:ind + 1, :, :] = torch.nn.functional.interpolate(
            images[:, ind:ind + 1, i_s[ind]:i_s[ind] + h_s[ind],
                   j_s[ind]:j_s[ind] + w_s[ind], ],
            size=(target_height, target_width),
            mode="bilinear",
            align_corners=False,
        )
    return out


def create_random_augment(
    input_size,
    auto_augment=None,
    interpolation="bilinear",
):
    """
    Get video randaug transform.

    Args:
        input_size: The size of the input video in tuple.
        auto_augment: Parameters for randaug. An example:
            "rand-m7-n4-mstd0.5-inc1" (m is the magnitude and n is the number
            of operations to apply).
        interpolation: Interpolation method.
    """
    if isinstance(input_size, tuple):
        img_size = input_size[-2:]
    else:
        img_size = input_size

    if auto_augment:
        assert isinstance(auto_augment, str)
        if isinstance(img_size, tuple):
            img_size_min = min(img_size)
        else:
            img_size_min = img_size
        aa_params = {"translate_const": int(img_size_min * 0.45)}
        if interpolation and interpolation != "random":
            aa_params["interpolation"] = _pil_interp(interpolation)
        if auto_augment.startswith("rand"):
            return transforms.Compose(
                [rand_augment_transform(auto_augment, aa_params)])
    raise NotImplementedError


def random_sized_crop_img(
        im,
        size,
        jitter_scale=(0.08, 1.0),
        jitter_aspect=(3.0 / 4.0, 4.0 / 3.0),
        max_iter=10,
):
    """
    Performs Inception-style cropping (used for training).
    """
    assert (len(
        im.shape) == 3), "Currently only support image for random_sized_crop"
    h, w = im.shape[1:3]
    i, j, h, w = _get_param_spatial_crop(
        scale=jitter_scale,
        ratio=jitter_aspect,
        height=h,
        width=w,
        num_repeat=max_iter,
        log_scale=False,
        switch_hw=True,
    )
    cropped = im[:, i:i + h, j:j + w]
    return torch.nn.functional.interpolate(
        cropped.unsqueeze(0),
        size=(size, size),
        mode="bilinear",
        align_corners=False,
    ).squeeze(0)


# The following code are modified based on timm lib, we will replace the following
# contents with dependency from PyTorchVideo.
# https://github.com/facebookresearch/pytorchvideo
class RandomResizedCropAndInterpolation:
    """Crop the given PIL Image to random size and aspect ratio with random interpolation.
    A crop of random size (default: of 0.08 to 1.0) of the original size and a random
    aspect ratio (default: of 3/4 to 4/3) of the original aspect ratio is made. This crop
    is finally resized to given size.
    This is popularly used to train the Inception networks.
    Args:
        size: expected output size of each edge
        scale: range of size of the origin size cropped
        ratio: range of aspect ratio of the origin aspect ratio cropped
        interpolation: Default: PIL.Image.BILINEAR
    """

    def __init__(
            self,
            size,
            scale=(0.08, 1.0),
            ratio=(3.0 / 4.0, 4.0 / 3.0),
            interpolation="bilinear",
    ):
        if isinstance(size, tuple):
            self.size = size
        else:
            self.size = (size, size)
        if (scale[0] > scale[1]) or (ratio[0] > ratio[1]):
            print("range should be of kind (min, max)")

        if interpolation == "random":
            self.interpolation = _RANDOM_INTERPOLATION
        else:
            self.interpolation = _pil_interp(interpolation)
        self.scale = scale
        self.ratio = ratio

    @staticmethod
    def get_params(img, scale, ratio):
        """Get parameters for ``crop`` for a random sized crop.
        Args:
            img (PIL Image): Image to be cropped.
            scale (tuple): range of size of the origin size cropped
            ratio (tuple): range of aspect ratio of the origin aspect ratio cropped
        Returns:
            tuple: params (i, j, h, w) to be passed to ``crop`` for a random
                sized crop.
        """
        area = img.size[0] * img.size[1]

        for _ in range(10):
            target_area = random.uniform(*scale) * area
            log_ratio = (math.log(ratio[0]), math.log(ratio[1]))
            aspect_ratio = math.exp(random.uniform(*log_ratio))

            w = int(round(math.sqrt(target_area * aspect_ratio)))
            h = int(round(math.sqrt(target_area / aspect_ratio)))

            if w <= img.size[0] and h <= img.size[1]:
                i = random.randint(0, img.size[1] - h)
                j = random.randint(0, img.size[0] - w)
                return i, j, h, w

        # Fallback to central crop
        in_ratio = img.size[0] / img.size[1]
        if in_ratio < min(ratio):
            w = img.size[0]
            h = int(round(w / min(ratio)))
        elif in_ratio > max(ratio):
            h = img.size[1]
            w = int(round(h * max(ratio)))
        else:  # whole image
            w = img.size[0]
            h = img.size[1]
        i = (img.size[1] - h) // 2
        j = (img.size[0] - w) // 2
        return i, j, h, w

    def __call__(self, img):
        """
        Args:
            img (PIL Image): Image to be cropped and resized.
        Returns:
            PIL Image: Randomly cropped and resized image.
        """
        i, j, h, w = self.get_params(img, self.scale, self.ratio)
        if isinstance(self.interpolation, (tuple, list)):
            interpolation = random.choice(self.interpolation)
        else:
            interpolation = self.interpolation
        return F.resized_crop(img, i, j, h, w, self.size, interpolation)

    def __repr__(self):
        if isinstance(self.interpolation, (tuple, list)):
            interpolate_str = " ".join(
                [_pil_interpolation_to_str[x] for x in self.interpolation])
        else:
            interpolate_str = _pil_interpolation_to_str[self.interpolation]
        format_string = self.__class__.__name__ + "(size={0}".format(self.size)
        format_string += ", scale={0}".format(
            tuple(round(s, 4) for s in self.scale))
        format_string += ", ratio={0}".format(
            tuple(round(r, 4) for r in self.ratio))
        format_string += ", interpolation={0})".format(interpolate_str)
        return format_string


def transforms_imagenet_train(
    img_size=224,
    scale=None,
    ratio=None,
    hflip=0.5,
    vflip=0.0,
    color_jitter=0.4,
    auto_augment=None,
    interpolation="random",
    use_prefetcher=False,
    mean=(0.485, 0.456, 0.406),
    std=(0.229, 0.224, 0.225),
    re_prob=0.0,
    re_mode="const",
    re_count=1,
    re_num_splits=0,
    separate=False,
):
    """
    If separate==True, the transforms are returned as a tuple of 3 separate transforms
    for use in a mixing dataset that passes
     * all data through the first (primary) transform, called the 'clean' data
     * a portion of the data through the secondary transform
     * normalizes and converts the branches above with the third, final transform
    """
    if isinstance(img_size, tuple):
        img_size = img_size[-2:]
    else:
        img_size = img_size

    scale = tuple(scale or (0.08, 1.0))  # default imagenet scale range
    ratio = tuple(ratio
                  or (3.0 / 4.0, 4.0 / 3.0))  # default imagenet ratio range
    primary_tfl = [
        RandomResizedCropAndInterpolation(
            img_size, scale=scale, ratio=ratio, interpolation=interpolation)
    ]
    if hflip > 0.0:
        primary_tfl += [transforms.RandomHorizontalFlip(p=hflip)]
    if vflip > 0.0:
        primary_tfl += [transforms.RandomVerticalFlip(p=vflip)]

    secondary_tfl = []
    if auto_augment:
        assert isinstance(auto_augment, str)
        if isinstance(img_size, tuple):
            img_size_min = min(img_size)
        else:
            img_size_min = img_size
        aa_params = dict(
            translate_const=int(img_size_min * 0.45),
            img_mean=tuple([min(255, round(255 * x)) for x in mean]),
        )
        if interpolation and interpolation != "random":
            aa_params["interpolation"] = _pil_interp(interpolation)
        if auto_augment.startswith("rand"):
            secondary_tfl += [rand_augment_transform(auto_augment, aa_params)]
        elif auto_augment.startswith("augmix"):
            raise NotImplementedError("Augmix not implemented")
        else:
            raise NotImplementedError("Auto aug not implemented")
    elif color_jitter is not None:
        # color jitter is enabled when not using AA
        if isinstance(color_jitter, (list, tuple)):
            # color jitter should be a 3-tuple/list if spec brightness/contrast/saturation
            # or 4 if also augmenting hue
            assert len(color_jitter) in (3, 4)
        else:
            # if it's a scalar, duplicate for brightness, contrast, and saturation, no hue
            color_jitter = (float(color_jitter), ) * 3
        secondary_tfl += [transforms.ColorJitter(*color_jitter)]

    final_tfl = []
    final_tfl += [
        transforms.ToTensor(),
        transforms.Normalize(mean=torch.tensor(mean), std=torch.tensor(std)),
    ]
    if re_prob > 0.0:
        final_tfl.append(
            RandomErasing(
                re_prob,
                mode=re_mode,
                max_count=re_count,
                num_splits=re_num_splits,
                device="cpu",
                cube=False,
            ))

    if separate:
        return (
            transforms.Compose(primary_tfl),
            transforms.Compose(secondary_tfl),
            transforms.Compose(final_tfl),
        )
    else:
        return transforms.Compose(primary_tfl + secondary_tfl + final_tfl)


############################################################################################################
############################################################################################################


class Compose(object):
    """Composes several transforms
    Args:
    transforms (list of ``Transform`` objects): list of transforms
    to compose
    """

    def __init__(self, transforms):
        self.transforms = transforms

    def __call__(self, clip):
        for t in self.transforms:
            clip = t(clip)
        return clip


class RandomHorizontalFlip(object):
    """Horizontally flip the list of given images randomly
    with a probability 0.5
    """

    def __call__(self, clip):
        """
        Args:
        img (PIL.Image or numpy.ndarray): List of images to be cropped
        in format (h, w, c) in numpy.ndarray
        Returns:
        PIL.Image or numpy.ndarray: Randomly flipped clip
        """
        if random.random() < 0.5:
            if isinstance(clip[0], np.ndarray):
                return [np.fliplr(img) for img in clip]
            elif isinstance(clip[0], PIL.Image.Image):
                return [
                    img.transpose(PIL.Image.FLIP_LEFT_RIGHT) for img in clip
                ]
            else:
                raise TypeError('Expected numpy.ndarray or PIL.Image' +
                                ' but got list of {0}'.format(type(clip[0])))
        return clip


class RandomResize(object):
    """Resizes a list of (H x W x C) numpy.ndarray to the final size
    The larger the original image is, the more times it takes to
    interpolate
    Args:
    interpolation (str): Can be one of 'nearest', 'bilinear'
    defaults to nearest
    size (tuple): (widht, height)
    """

    def __init__(self, ratio=(3. / 4., 4. / 3.), interpolation='nearest'):
        self.ratio = ratio
        self.interpolation = interpolation

    def __call__(self, clip):
        scaling_factor = random.uniform(self.ratio[0], self.ratio[1])

        if isinstance(clip[0], np.ndarray):
            im_h, im_w, im_c = clip[0].shape
        elif isinstance(clip[0], PIL.Image.Image):
            im_w, im_h = clip[0].size

        new_w = int(im_w * scaling_factor)
        new_h = int(im_h * scaling_factor)
        new_size = (new_w, new_h)
        resized = FF.resize_clip(
            clip, new_size, interpolation=self.interpolation)
        return resized


class Resize(object):
    """Resizes a list of (H x W x C) numpy.ndarray to the final size
    The larger the original image is, the more times it takes to
    interpolate
    Args:
    interpolation (str): Can be one of 'nearest', 'bilinear'
    defaults to nearest
    size (tuple): (widht, height)
    """

    def __init__(self, size, interpolation='nearest'):
        self.size = size
        self.interpolation = interpolation

    def __call__(self, clip):
        resized = FF.resize_clip(
            clip, self.size, interpolation=self.interpolation)
        return resized


class RandomCrop(object):
    """Extract random crop at the same location for a list of images
    Args:
    size (sequence or int): Desired output size for the
    crop in format (h, w)
    """

    def __init__(self, size):
        if isinstance(size, numbers.Number):
            size = (size, size)

        self.size = size

    def __call__(self, clip):
        """
        Args:
        img (PIL.Image or numpy.ndarray): List of images to be cropped
        in format (h, w, c) in numpy.ndarray
        Returns:
        PIL.Image or numpy.ndarray: Cropped list of images
        """
        h, w = self.size
        if isinstance(clip[0], np.ndarray):
            im_h, im_w, im_c = clip[0].shape
        elif isinstance(clip[0], PIL.Image.Image):
            im_w, im_h = clip[0].size
        else:
            raise TypeError('Expected numpy.ndarray or PIL.Image' +
                            'but got list of {0}'.format(type(clip[0])))
        if w > im_w or h > im_h:
            error_msg = (
                'Initial image size should be larger then '
                'cropped size but got cropped sizes : ({w}, {h}) while '
                'initial image is ({im_w}, {im_h})'.format(
                    im_w=im_w, im_h=im_h, w=w, h=h))
            raise ValueError(error_msg)

        x1 = random.randint(0, im_w - w)
        y1 = random.randint(0, im_h - h)
        cropped = FF.crop_clip(clip, y1, x1, h, w)

        return cropped


class ThreeCrop(object):
    """Extract random crop at the same location for a list of images
    Args:
    size (sequence or int): Desired output size for the
    crop in format (h, w)
    """

    def __init__(self, size):
        if isinstance(size, numbers.Number):
            size = (size, size)

        self.size = size

    def __call__(self, clip):
        """
        Args:
        img (PIL.Image or numpy.ndarray): List of images to be cropped
        in format (h, w, c) in numpy.ndarray
        Returns:
        PIL.Image or numpy.ndarray: Cropped list of images
        """
        h, w = self.size
        if isinstance(clip[0], np.ndarray):
            im_h, im_w, im_c = clip[0].shape
        elif isinstance(clip[0], PIL.Image.Image):
            im_w, im_h = clip[0].size
        else:
            raise TypeError('Expected numpy.ndarray or PIL.Image' +
                            'but got list of {0}'.format(type(clip[0])))
        if w != im_w and h != im_h:
            clip = FF.resize_clip(clip, self.size, interpolation="bilinear")
            im_h, im_w, im_c = clip[0].shape

        step = np.max((np.max((im_w, im_h)) - self.size[0]) // 2, 0)
        cropped = []
        for i in range(3):
            if (im_h > self.size[0]):
                x1 = 0
                y1 = i * step
                cropped.extend(FF.crop_clip(clip, y1, x1, h, w))
            else:
                x1 = i * step
                y1 = 0
                cropped.extend(FF.crop_clip(clip, y1, x1, h, w))
        return cropped


class RandomRotation(object):
    """Rotate entire clip randomly by a random angle within
    given bounds
    Args:
    degrees (sequence or int): Range of degrees to select from
    If degrees is a number instead of sequence like (min, max),
    the range of degrees, will be (-degrees, +degrees).
    """

    def __init__(self, degrees):
        if isinstance(degrees, numbers.Number):
            if degrees < 0:
                raise ValueError('If degrees is a single number,'
                                 'must be positive')
            degrees = (-degrees, degrees)
        else:
            if len(degrees) != 2:
                raise ValueError('If degrees is a sequence,'
                                 'it must be of len 2.')

        self.degrees = degrees

    def __call__(self, clip):
        """
        Args:
        img (PIL.Image or numpy.ndarray): List of images to be cropped
        in format (h, w, c) in numpy.ndarray
        Returns:
        PIL.Image or numpy.ndarray: Cropped list of images
        """
        import skimage
        angle = random.uniform(self.degrees[0], self.degrees[1])
        if isinstance(clip[0], np.ndarray):
            rotated = [skimage.transform.rotate(img, angle) for img in clip]
        elif isinstance(clip[0], PIL.Image.Image):
            rotated = [img.rotate(angle) for img in clip]
        else:
            raise TypeError('Expected numpy.ndarray or PIL.Image' +
                            'but got list of {0}'.format(type(clip[0])))

        return rotated


class CenterCrop(object):
    """Extract center crop at the same location for a list of images
    Args:
    size (sequence or int): Desired output size for the
    crop in format (h, w)
    """

    def __init__(self, size):
        if isinstance(size, numbers.Number):
            size = (size, size)

        self.size = size

    def __call__(self, clip):
        """
        Args:
        img (PIL.Image or numpy.ndarray): List of images to be cropped
        in format (h, w, c) in numpy.ndarray
        Returns:
        PIL.Image or numpy.ndarray: Cropped list of images
        """
        h, w = self.size
        if isinstance(clip[0], np.ndarray):
            im_h, im_w, im_c = clip[0].shape
        elif isinstance(clip[0], PIL.Image.Image):
            im_w, im_h = clip[0].size
        else:
            raise TypeError('Expected numpy.ndarray or PIL.Image' +
                            'but got list of {0}'.format(type(clip[0])))
        if w > im_w or h > im_h:
            error_msg = (
                'Initial image size should be larger then '
                'cropped size but got cropped sizes : ({w}, {h}) while '
                'initial image is ({im_w}, {im_h})'.format(
                    im_w=im_w, im_h=im_h, w=w, h=h))
            raise ValueError(error_msg)

        x1 = int(round((im_w - w) / 2.))
        y1 = int(round((im_h - h) / 2.))
        cropped = FF.crop_clip(clip, y1, x1, h, w)

        return cropped


class ColorJitter(object):
    """Randomly change the brightness, contrast and saturation and hue of the clip
    Args:
    brightness (float): How much to jitter brightness. brightness_factor
    is chosen uniformly from [max(0, 1 - brightness), 1 + brightness].
    contrast (float): How much to jitter contrast. contrast_factor
    is chosen uniformly from [max(0, 1 - contrast), 1 + contrast].
    saturation (float): How much to jitter saturation. saturation_factor
    is chosen uniformly from [max(0, 1 - saturation), 1 + saturation].
    hue(float): How much to jitter hue. hue_factor is chosen uniformly from
    [-hue, hue]. Should be >=0 and <= 0.5.
    """

    def __init__(self, brightness=0, contrast=0, saturation=0, hue=0):
        self.brightness = brightness
        self.contrast = contrast
        self.saturation = saturation
        self.hue = hue

    def get_params(self, brightness, contrast, saturation, hue):
        if brightness > 0:
            brightness_factor = random.uniform(
                max(0, 1 - brightness), 1 + brightness)
        else:
            brightness_factor = None

        if contrast > 0:
            contrast_factor = random.uniform(
                max(0, 1 - contrast), 1 + contrast)
        else:
            contrast_factor = None

        if saturation > 0:
            saturation_factor = random.uniform(
                max(0, 1 - saturation), 1 + saturation)
        else:
            saturation_factor = None

        if hue > 0:
            hue_factor = random.uniform(-hue, hue)
        else:
            hue_factor = None
        return brightness_factor, contrast_factor, saturation_factor, hue_factor

    def __call__(self, clip):
        """
        Args:
        clip (list): list of PIL.Image
        Returns:
        list PIL.Image : list of transformed PIL.Image
        """
        if isinstance(clip[0], np.ndarray):
            raise TypeError(
                'Color jitter not yet implemented for numpy arrays')
        elif isinstance(clip[0], PIL.Image.Image):
            brightness, contrast, saturation, hue = self.get_params(
                self.brightness, self.contrast, self.saturation, self.hue)

            # Create img transform function sequence
            img_transforms = []
            if brightness is not None:
                img_transforms.append(
                    lambda img: torchvision.transforms.functional.
                    adjust_brightness(img, brightness))
            if saturation is not None:
                img_transforms.append(
                    lambda img: torchvision.transforms.functional.
                    adjust_saturation(img, saturation))
            if hue is not None:
                img_transforms.append(lambda img: torchvision.transforms.
                                      functional.adjust_hue(img, hue))
            if contrast is not None:
                img_transforms.append(
                    lambda img: torchvision.transforms.functional.
                    adjust_contrast(img, contrast))
            random.shuffle(img_transforms)

            # Apply to all images
            jittered_clip = []
            for img in clip:
                for func in img_transforms:
                    jittered_img = func(img)
                jittered_clip.append(jittered_img)

        else:
            raise TypeError('Expected numpy.ndarray or PIL.Image' +
                            'but got list of {0}'.format(type(clip[0])))
        return jittered_clip


class Normalize(object):
    """Normalize a clip with mean and standard deviation.
    Given mean: ``(M1,...,Mn)`` and std: ``(S1,..,Sn)`` for ``n`` channels, this transform
    will normalize each channel of the input ``torch.*Tensor`` i.e.
    ``input[channel] = (input[channel] - mean[channel]) / std[channel]``
    .. note::
        This transform acts out of place, i.e., it does not mutates the input tensor.
    Args:
        mean (sequence): Sequence of means for each channel.
        std (sequence): Sequence of standard deviations for each channel.
    """

    def __init__(self, mean, std):
        self.mean = mean
        self.std = std

    def __call__(self, clip):
        """
        Args:
            clip (Tensor): Tensor clip of size (T, C, H, W) to be normalized.
        Returns:
            Tensor: Normalized Tensor clip.
        """
        return FF.normalize(clip, self.mean, self.std)

    def __repr__(self):
        return self.__class__.__name__ + '(mean={0}, std={1})'.format(
            self.mean, self.std)

```

`deepcheat/VideoMAEv2/dataset/volume_transforms.py`:

```py
import numpy as np
import torch
from PIL import Image


def convert_img(img):
    """Converts (H, W, C) numpy.ndarray to (C, W, H) format
    """
    if len(img.shape) == 3:
        img = img.transpose(2, 0, 1)
    if len(img.shape) == 2:
        img = np.expand_dims(img, 0)
    return img


class ClipToTensor(object):
    """Convert a list of m (H x W x C) numpy.ndarrays in the range [0, 255]
    to a torch.FloatTensor of shape (C x m x H x W) in the range [0, 1.0]
    """

    def __init__(self, channel_nb=3, div_255=True, numpy=False):
        self.channel_nb = channel_nb
        self.div_255 = div_255
        self.numpy = numpy

    def __call__(self, clip):
        """
        Args: clip (list of numpy.ndarray): clip (list of images)
        to be converted to tensor.
        """
        # Retrieve shape
        if isinstance(clip[0], np.ndarray):
            h, w, ch = clip[0].shape
            assert ch == self.channel_nb, 'Got {0} instead of 3 channels'.format(
                ch)
        elif isinstance(clip[0], Image.Image):
            w, h = clip[0].size
        else:
            raise TypeError('Expected numpy.ndarray or PIL.Image\
            but got list of {0}'.format(type(clip[0])))

        np_clip = np.zeros([self.channel_nb, len(clip), int(h), int(w)])

        # Convert
        for img_idx, img in enumerate(clip):
            if isinstance(img, np.ndarray):
                pass
            elif isinstance(img, Image.Image):
                img = np.array(img, copy=False)
            else:
                raise TypeError('Expected numpy.ndarray or PIL.Image\
                but got list of {0}'.format(type(clip[0])))
            img = convert_img(img)
            np_clip[:, img_idx, :, :] = img
        if self.numpy:
            if self.div_255:
                np_clip = np_clip / 255.0
            return np_clip

        else:
            tensor_clip = torch.from_numpy(np_clip)

            if not isinstance(tensor_clip, torch.FloatTensor):
                tensor_clip = tensor_clip.float()
            if self.div_255:
                tensor_clip = torch.div(tensor_clip, 255)
            return tensor_clip


# Note this norms data to -1/1
class ClipToTensor_K(object):
    """Convert a list of m (H x W x C) numpy.ndarrays in the range [0, 255]
    to a torch.FloatTensor of shape (C x m x H x W) in the range [0, 1.0]
    """

    def __init__(self, channel_nb=3, div_255=True, numpy=False):
        self.channel_nb = channel_nb
        self.div_255 = div_255
        self.numpy = numpy

    def __call__(self, clip):
        """
        Args: clip (list of numpy.ndarray): clip (list of images)
        to be converted to tensor.
        """
        # Retrieve shape
        if isinstance(clip[0], np.ndarray):
            h, w, ch = clip[0].shape
            assert ch == self.channel_nb, 'Got {0} instead of 3 channels'.format(
                ch)
        elif isinstance(clip[0], Image.Image):
            w, h = clip[0].size
        else:
            raise TypeError('Expected numpy.ndarray or PIL.Image\
            but got list of {0}'.format(type(clip[0])))

        np_clip = np.zeros([self.channel_nb, len(clip), int(h), int(w)])

        # Convert
        for img_idx, img in enumerate(clip):
            if isinstance(img, np.ndarray):
                pass
            elif isinstance(img, Image.Image):
                img = np.array(img, copy=False)
            else:
                raise TypeError('Expected numpy.ndarray or PIL.Image\
                but got list of {0}'.format(type(clip[0])))
            img = convert_img(img)
            np_clip[:, img_idx, :, :] = img
        if self.numpy:
            if self.div_255:
                np_clip = (np_clip - 127.5) / 127.5
            return np_clip

        else:
            tensor_clip = torch.from_numpy(np_clip)

            if not isinstance(tensor_clip, torch.FloatTensor):
                tensor_clip = tensor_clip.float()
            if self.div_255:
                tensor_clip = torch.div(torch.sub(tensor_clip, 127.5), 127.5)
            return tensor_clip


class ToTensor(object):
    """Converts numpy array to tensor
    """

    def __call__(self, array):
        tensor = torch.from_numpy(array)
        return tensor

```

`deepcheat/VideoMAEv2/do_cheater.sh`:

```sh
#!/bin/bash

#python -m debugpy --listen 5678 --wait-for-client \
python  \
    train_cheater_pred.py \
        --model vit_giant_patch14_224 \
        --data_set cheater \
        --nb_classes 1 \
        --data_path ~/Desktop/processed_vids \
		--data_root ~/Desktop/processed_vids \
        --finetune ../vit_g_ps14_ak_ft_ckpt_7_clean.pth \
        --log_dir output \
        --output_dir output \
        --batch_size 6 \
        --update_freq 12 \
        --input_size 224 \
        --short_side_size 224 \
        --save_ckpt_freq 20 \
        --num_frames 16 \
        --sampling_rate 1 \
        --num_sample 1 \
        --num_workers 10 \
        --opt adamw \
        --lr 1e-3 \
        --drop_path 0.1 \
        --clip_grad 1.0 \
        --layer_decay 0.9 \
        --opt_betas 0.9 0.999 \
        --weight_decay 0.000 \
        --warmup_epochs 10 \
        --epochs 100 \
        --nb_classes 1 \
        --test_num_segment 5 \
        --test_num_crop 3 $1
		
		
	

```

`deepcheat/VideoMAEv2/engine_for_finetuning.py`:

```py
# --------------------------------------------------------
# Based on BEiT, timm, DINO and DeiT code bases
# https://github.com/microsoft/unilm/tree/master/beit
# https://github.com/rwightman/pytorch-image-models/tree/master/timm
# https://github.com/facebookresearch/deit
# https://github.com/facebookresearch/dino
# --------------------------------------------------------'
import math
import os
import sys
from multiprocessing import Pool
from typing import Iterable, Optional

import numpy as np
import torch
from scipy.special import softmax
from timm.data import Mixup
from timm.utils import ModelEma, accuracy

import matplotlib.pyplot as plt
import utils

from sklearn.metrics import precision_score, recall_score, f1_score
from sklearn.metrics import precision_recall_curve, auc

def train_class_batch(model, samples, target, criterion):
    outputs = model(samples)#.squeeze()
    loss = criterion(outputs, target)
    return loss, outputs


def get_loss_scale_for_deepspeed(model):
    optimizer = model.optimizer
    return optimizer.loss_scale if hasattr(
        optimizer, "loss_scale") else optimizer.cur_scale


def train_one_epoch(model: torch.nn.Module,
                    criterion: torch.nn.Module,
                    data_loader: Iterable,
                    optimizer: torch.optim.Optimizer,
                    device: torch.device,
                    epoch: int,
                    loss_scaler,
                    max_norm: float = 0,
                    model_ema: Optional[ModelEma] = None,
                    mixup_fn: Optional[Mixup] = None,
                    log_writer=None,
                    start_steps=None,
                    lr_schedule_values=None,
                    wd_schedule_values=None,
                    num_training_steps_per_epoch=None,
                    update_freq=None):
    model.train(True)
    metric_logger = utils.MetricLogger(delimiter="  ")
    metric_logger.add_meter(
        'lr', utils.SmoothedValue(window_size=1, fmt='{value:.6f}'))
    metric_logger.add_meter(
        'min_lr', utils.SmoothedValue(window_size=1, fmt='{value:.6f}'))
    header = 'Epoch: [{}]'.format(epoch)
    print_freq = 5

    if loss_scaler is None:
        model.zero_grad()
        model.micro_steps = 0
    else:
        optimizer.zero_grad()

    #breakpoint()
    for data_iter_step, (samples, targets, _, _) in enumerate(
            metric_logger.log_every(data_loader, print_freq, header)):
        step = data_iter_step // update_freq
        if step >= num_training_steps_per_epoch:
            continue
        it = start_steps + step  # global training iteration
        # Update LR & WD for the first acc
        #if lr_schedule_values is not None or wd_schedule_values is not None and data_iter_step % update_freq == 0:
        #    for i, param_group in enumerate(optimizer.param_groups):
        #        if lr_schedule_values is not None:
        #            param_group["lr"] = lr_schedule_values[it] * param_group[
        #                "lr_scale"]
        #        if wd_schedule_values is not None and param_group[
        #                "weight_decay"] > 0:
        #            param_group["weight_decay"] = wd_schedule_values[it]

        samples = samples.to(device, non_blocking=True)
        targets = targets.to(device, non_blocking=True)

        if mixup_fn is not None:
            # mixup handle 3th & 4th dimension
            B, C, T, H, W = samples.shape
            samples = samples.view(B, C * T, H, W)
            samples, targets = mixup_fn(samples, targets)
            samples = samples.view(B, C, T, H, W)
            targets = targets[:,1].view(B,-1)

        if loss_scaler is None:
            samples = samples.half()
            loss, output = train_class_batch(model, samples, targets,
                                             criterion)
        else:
            with torch.amp.autocast('cuda', dtype=torch.bfloat16):
                loss, output = train_class_batch(model, samples, targets,
                                                 criterion)

        loss_value = loss.item()



        if not math.isfinite(loss_value):
            print("Loss is {}, stopping training".format(loss_value))
            sys.exit(1)

        if loss_scaler is None:
            loss /= update_freq
            model.backward(loss)
            grad_norm = model.get_global_grad_norm()
            model.step()

            if (data_iter_step + 1) % update_freq == 0:
                # Deepspeed will call step() & model.zero_grad() automatic
                if model_ema is not None:
                    model_ema.update(model)
            loss_scale_value = get_loss_scale_for_deepspeed(model)
        else:
            # this attribute is added by timm on one optimizer (adahessian)
            is_second_order = hasattr(
                optimizer, 'is_second_order') and optimizer.is_second_order
            loss /= update_freq

            grad_norm = loss_scaler(
                loss,
                optimizer,
                clip_grad=max_norm,
                parameters=model.parameters(),
                create_graph=is_second_order,
                update_grad=(data_iter_step + 1) % update_freq == 0)
            if (data_iter_step + 1) % update_freq == 0:
                # Removed debug print that was causing slowdowns
                optimizer.zero_grad()
                if model_ema is not None:
                    model_ema.update(model)
            loss_scale_value = loss_scaler.state_dict()["scale"]

        torch.cuda.synchronize()

        if mixup_fn is None:
            class_acc = (output.max(-1)[-1] == targets).float().mean()
        else:
            class_acc = None
        metric_logger.update(loss=loss_value)
        metric_logger.update(class_acc=class_acc)
        metric_logger.update(loss_scale=loss_scale_value)
        min_lr = 10.
        max_lr = 0.
        for group in optimizer.param_groups:
            min_lr = min(min_lr, group["lr"])
            max_lr = max(max_lr, group["lr"])

        metric_logger.update(lr=max_lr)
        metric_logger.update(min_lr=min_lr)
        weight_decay_value = None
        for group in optimizer.param_groups:
            if group["weight_decay"] > 0:
                weight_decay_value = group["weight_decay"]
        metric_logger.update(weight_decay=weight_decay_value)
        metric_logger.update(grad_norm=grad_norm)

        if log_writer is not None:
            log_writer.update(loss=loss_value, head="loss")
            log_writer.update(class_acc=class_acc, head="loss")
            log_writer.update(loss_scale=loss_scale_value, head="opt")
            log_writer.update(lr=max_lr, head="opt")
            log_writer.update(min_lr=min_lr, head="opt")
            log_writer.update(weight_decay=weight_decay_value, head="opt")
            log_writer.update(grad_norm=grad_norm, head="opt")

            log_writer.set_step()

    # gather the stats from all processes
    metric_logger.synchronize_between_processes()
    print("Averaged stats:", metric_logger)
    return {k: meter.global_avg for k, meter in metric_logger.meters.items()}


@torch.no_grad()
def validation_one_epoch(data_loader, model, device, min_eval_score=None, max_eval_score=None, make_fig_path=''):
    criterion = torch.nn.BCEWithLogitsLoss()

    metric_logger = utils.MetricLogger(delimiter="  ")
    header = 'Val:'

    # switch to evaluation mode
    model.eval()
    all_outputs = []
    all_targets = []

    # First loop: Normalize all outputs to range 0-1
    for batch in metric_logger.log_every(data_loader, 10, header):
        images, target = batch[0].to(device, non_blocking=True), batch[1].to(device, non_blocking=True)

        output = model(images).reshape(-1)
        loss = criterion(output, target.float())

        # Normalize output using sigmoid for values between 0 and 1

        all_outputs.extend(output.detach().cpu().numpy())
        all_targets.extend(target.cpu().numpy())

        batch_size = images.shape[0]
        metric_logger.update(loss=loss.item())

    if min_eval_score is not None:
        min_val = min_eval_score
        max_val = max_eval_score
        all_outputs = np.array([(x - min_val) / (max_val - min_val) for x in all_outputs])
        return all_outputs


    min_val, max_val = min(all_outputs), max(all_outputs)
    
    print("min ", min_val   , " max ", max_val)
    all_outputs = np.array([(x - min_val) / (max_val - min_val) for x in all_outputs])

    # Second loop: Compute precision, recall, and F1 score
    preds = (all_outputs > 0.5).astype(float)  # Thresholding at 0.5

    precision = precision_score(all_targets, preds)
    recall = recall_score(all_targets, preds)
    f1 = f1_score(all_targets, preds)

    metric_logger.meters['precision'].update(precision, n=1)
    metric_logger.meters['recall'].update(recall, n=1)
    metric_logger.meters['f1'].update(f1, n=1)

    # gather the stats from all processes
    metric_logger.synchronize_between_processes()

    # After loop
    #breakpoint()
    all_probs = torch.sigmoid(torch.tensor(all_outputs).float()).cpu().numpy()
    precision, recall, thresholds = precision_recall_curve(all_targets, all_probs)
    auc_pr = auc(recall, precision)
    

    if make_fig_path != '':
        # Plotting
        plt.figure(figsize=(10, 7))
        plt.plot(recall, precision, label=f'AUC-PR = {auc_pr:.4f}')
        plt.xlabel('Recall')
        plt.ylabel('Precision')
        plt.title('Precision-Recall Curve with Thresholds')

        # Choose a set of probability thresholds
        threshold_values = np.linspace(0.1, 0.9, 9)
        for thresh in threshold_values:
            idx = (np.abs(thresholds - thresh)).argmin()
            plt.scatter(recall[idx], precision[idx], label=f'Prob={thresh:.1f}')
            plt.annotate(f'{thresh:.1f}', (recall[idx], precision[idx]), textcoords="offset points", xytext=(0,5), ha='center')

        plt.legend()
        plt.grid(True)
        plt.tight_layout()
        plt.savefig('cheater_precision_recall_curve.png')
        # plt.show()


    ret = {k: meter.global_avg for k, meter in metric_logger.meters.items()}
    ret["auc-pr"] = auc_pr
    return ret , all_probs, all_targets


@torch.no_grad()
def final_test_killshot(data_loader, model, device, file):
    criterion = torch.nn.CrossEntropyLoss()

    metric_logger = utils.MetricLogger(delimiter="  ")
    header = 'Test:'

    # switch to evaluation mode
    model.eval()
    final_result = []
    correct_count = 0
    total =0
    distance_from_correct = []
    kth_guess = []
    print(len(data_loader))
    for batch in data_loader:
        print(total)
        images = batch[0]
        target = batch[1]
        images = images.to(device, non_blocking=True)
        #target = target.to(device, non_blocking=True)
        B = images.shape[0]
        # compute output
        with torch.amp.autocast('cuda'):
            #breakpoint()
            votes = np.zeros((images.shape[0],images.shape[2]))
            counts = np.zeros((images.shape[0],images.shape[2]))

            for i in range(16):

                output = model(images[:,:,i:i+16])
                votes[:,i:i+16] += output.cpu().numpy()
                counts[:,i:i+16] += np.ones((B,16))
                #loss = criterion(output, target)d
        out = votes/counts

        correct_count += (out.argmax(1) == target.argmax(1).numpy()).sum()
        total += B
        distance_from_correct.extend(np.abs(out.argmax(1) - target.argmax(1).numpy()))
        kth_guess.extend(np.sum(out > torch.from_numpy(out[:,15]).unsqueeze(-1).numpy(), axis=1))
        #breakpoint()
    print(images.shape)

    acc = correct_count / total


    return acc, distance_from_correct, kth_guess


@torch.no_grad()
def predict_killshot_scores(data_loader, model, device, file):
    criterion = torch.nn.CrossEntropyLoss()

    metric_logger = utils.MetricLogger(delimiter="  ")
    header = 'Test:'

    # switch to evaluation mode
    model.eval()
    final_result = []
    correct_count = 0
    total =0
    distance_from_correct = []
    kth_guess = []

    print(len(data_loader))
    #breakpoint()
    T = len(data_loader)
    votes = np.zeros(len(data_loader))
    counts = np.zeros(len(data_loader))
    B = 48
    i = 0
    V_L = 16
    #breakpoint()
    generator = data_loader.sequential_load(V_L)
    batched = []
    final = False
    while True:
        try:
            if not final:
                batch = next(generator)
                batched.append(batch)
            elif len(batched) == 0: break


            if len(batched) == B or final: 
                images = torch.stack(batched, dim=0).to(device, non_blocking=True)

                # compute output
                with torch.amp.autocast('cuda'):
                    outputs = model(images).cpu().numpy()
                for pred in outputs: 
                    votes[i:i+V_L]+=pred
                    counts[i:i+V_L]+=np.ones(len(pred))
                    i+=1
                batched = []

                if i % (5 * B) == 0:
                    print(f"frame {i} out of {T}")

            # Process batch
        except StopIteration:
            print("All batches processed!")
            final = True

    out = votes/counts

    return out


def merge(eval_path, num_tasks, method='prob'):
    assert method in ['prob', 'score']
    dict_feats = {}
    dict_label = {}
    dict_pos = {}
    print("Reading individual output files")

    for x in range(num_tasks):
        file = os.path.join(eval_path, str(x) + '.txt')
        lines = open(file, 'r').readlines()[1:]
        for line in lines:
            line = line.strip()
            name = line.split('[')[0]
            label = line.split(']')[1].split(' ')[1]
            chunk_nb = line.split(']')[1].split(' ')[2]
            split_nb = line.split(']')[1].split(' ')[3]
            data = np.fromstring(
                line.split('[')[1].split(']')[0], dtype=float, sep=',')
            if name not in dict_feats:
                dict_feats[name] = []
                dict_label[name] = 0
                dict_pos[name] = []
            if chunk_nb + split_nb in dict_pos[name]:
                continue
            if method == 'prob':
                dict_feats[name].append(softmax(data))
            else:
                dict_feats[name].append(data)
            dict_pos[name].append(chunk_nb + split_nb)
            dict_label[name] = label
    print("Computing final results")

    input_lst = []
    for i, item in enumerate(dict_feats):
        input_lst.append([i, item, dict_feats[item], dict_label[item]])
    p = Pool(64)
    # [pred, top1, top5, label]
    ans = p.map(compute_video, input_lst)
    top1 = [x[1] for x in ans]
    top5 = [x[2] for x in ans]
    label = [x[3] for x in ans]
    final_top1, final_top5 = np.mean(top1), np.mean(top5)

    return final_top1 * 100, final_top5 * 100


def compute_video(lst):
    i, video_id, data, label = lst
    feat = [x for x in data]
    feat = np.mean(feat, axis=0)
    pred = np.argmax(feat)
    top1 = (int(pred) == int(label)) * 1.0
    top5 = (int(label) in np.argsort(-feat)[:5]) * 1.0
    return [pred, top1, top5, int(label)]

```

`deepcheat/VideoMAEv2/engine_for_pretraining.py`:

```py
# --------------------------------------------------------
# Based on BEiT, timm, DINO and DeiT code bases
# https://github.com/microsoft/unilm/tree/master/beit
# https://github.com/rwightman/pytorch-image-models/tree/master/timm
# https://github.com/facebookresearch/deit
# https://github.com/facebookresearch/dino
# --------------------------------------------------------'
import math
import sys
from typing import Iterable

import torch
from einops import rearrange
from timm.data.constants import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD

import utils


def train_one_epoch(model: torch.nn.Module,
                    data_loader: Iterable,
                    optimizer: torch.optim.Optimizer,
                    device: torch.device,
                    epoch: int,
                    loss_scaler,
                    max_norm: float = 0,
                    patch_size: int = 16,
                    normlize_target: bool = True,
                    log_writer=None,
                    lr_scheduler=None,
                    start_steps=None,
                    lr_schedule_values=None,
                    wd_schedule_values=None):
    model.train()
    #breakpoint()
    metric_logger = utils.MetricLogger(delimiter="  ")
    metric_logger.add_meter(
        'lr', utils.SmoothedValue(window_size=1, fmt='{value:.6f}'))
    metric_logger.add_meter(
        'min_lr', utils.SmoothedValue(window_size=1, fmt='{value:.6f}'))
    header = 'Epoch: [{}]'.format(epoch)
    print_freq = 10

    for step, batch in enumerate(
            metric_logger.log_every(data_loader, print_freq, header)):
        # assign learning rate & weight decay for each step
        it = start_steps + step  # global training iteration
        if lr_schedule_values is not None or wd_schedule_values is not None:
            for i, param_group in enumerate(optimizer.param_groups):
                if lr_schedule_values is not None:
                    param_group["lr"] = lr_schedule_values[it] * param_group[
                        "lr_scale"]
                if wd_schedule_values is not None and param_group[
                        "weight_decay"] > 0:
                    param_group["weight_decay"] = wd_schedule_values[it]

        # NOTE: When the decoder mask ratio is 0,
        # in other words, when decoder masking is not used,
        # decode_masked_pos = ~bool_masked_pos
        images, bool_masked_pos, decode_masked_pos = batch

        images = images.to(device, non_blocking=True)
        bool_masked_pos = bool_masked_pos.to(
            device, non_blocking=True).flatten(1).to(torch.bool)
        decode_masked_pos = decode_masked_pos.to(
            device, non_blocking=True).flatten(1).to(torch.bool)

        with torch.no_grad():
            # calculate the predict label
            mean = torch.as_tensor(IMAGENET_DEFAULT_MEAN).to(device)[None, :,
                                                                     None,
                                                                     None,
                                                                     None]
            std = torch.as_tensor(IMAGENET_DEFAULT_STD).to(device)[None, :,
                                                                   None, None,
                                                                   None]
            unnorm_images = images * std + mean  # in [0, 1]

            if normlize_target:
                images_squeeze = rearrange(
                    unnorm_images,
                    'b c (t p0) (h p1) (w p2) -> b (t h w) (p0 p1 p2) c',
                    p0=2,
                    p1=patch_size,
                    p2=patch_size)
                images_norm = (images_squeeze - images_squeeze.mean(
                    dim=-2, keepdim=True)) / (
                        images_squeeze.var(
                            dim=-2, unbiased=True, keepdim=True).sqrt() + 1e-6)
                images_patch = rearrange(images_norm, 'b n p c -> b n (p c)')
            else:
                images_patch = rearrange(
                    unnorm_images,
                    'b c (t p0) (h p1) (w p2) -> b (t h w) (p0 p1 p2 c)',
                    p0=2,
                    p1=patch_size,
                    p2=patch_size)

            B, N, C = images_patch.shape
            labels = images_patch[~decode_masked_pos].reshape(B, -1, C)

        if loss_scaler is None:
            outputs = model(images, bool_masked_pos, decode_masked_pos)
            loss = (outputs - labels)**2
            loss = loss.mean(dim=-1)
            cal_loss_mask = bool_masked_pos[~decode_masked_pos].reshape(B, -1)
            loss = (loss * cal_loss_mask).sum() / cal_loss_mask.sum()
        else:
            with torch.cuda.amp.autocast():
                outputs = model(images, bool_masked_pos, decode_masked_pos)
                loss = (outputs - labels)**2
                loss = loss.mean(dim=-1)
                cal_loss_mask = bool_masked_pos[~decode_masked_pos].reshape(
                    B, -1)
                loss = (loss * cal_loss_mask).sum() / cal_loss_mask.sum()

        loss_value = loss.item()

        if not math.isfinite(loss_value):
            print("Loss is {}, stopping training".format(loss_value))
            sys.exit(2)

        optimizer.zero_grad()

        if loss_scaler is None:
            loss.backward()
            if max_norm is None:
                grad_norm = utils.get_grad_norm_(model.parameters())
            else:
                grad_norm = torch.nn.utils.clip_grad_norm_(
                    model.parameters(), max_norm)
            optimizer.step()
            loss_scale_value = 0
        else:
            # this attribute is added by timm on one optimizer (adahessian)
            is_second_order = hasattr(
                optimizer, 'is_second_order') and optimizer.is_second_order
            grad_norm = loss_scaler(
                loss,
                optimizer,
                clip_grad=max_norm,
                parameters=model.parameters(),
                create_graph=is_second_order)
            loss_scale_value = loss_scaler.state_dict()["scale"]

        torch.cuda.synchronize()

        metric_logger.update(loss=loss_value)
        metric_logger.update(loss_scale=loss_scale_value)
        min_lr = 10.
        max_lr = 0.
        for group in optimizer.param_groups:
            min_lr = min(min_lr, group["lr"])
            max_lr = max(max_lr, group["lr"])

        metric_logger.update(lr=max_lr)
        metric_logger.update(min_lr=min_lr)
        weight_decay_value = None
        for group in optimizer.param_groups:
            if group["weight_decay"] > 0:
                weight_decay_value = group["weight_decay"]
        metric_logger.update(weight_decay=weight_decay_value)
        metric_logger.update(grad_norm=grad_norm)

        if log_writer is not None:
            log_writer.update(loss=loss_value, head="loss")
            log_writer.update(loss_scale=loss_scale_value, head="opt")
            log_writer.update(lr=max_lr, head="opt")
            log_writer.update(min_lr=min_lr, head="opt")
            log_writer.update(weight_decay=weight_decay_value, head="opt")
            log_writer.update(grad_norm=grad_norm, head="opt")

            log_writer.set_step()

        if lr_scheduler is not None:
            lr_scheduler.step_update(start_steps + step)
    # gather the stats from all processes
    metric_logger.synchronize_between_processes()
    print("Averaged stats:", metric_logger)
    return {k: meter.global_avg for k, meter in metric_logger.meters.items()}

```

`deepcheat/VideoMAEv2/eval_cheater.sh`:

```sh
#!/bin/bash

python train_cheater_pred.py \
        --model vit_giant_patch14_224 \
        --data_set cheater \
        --nb_classes 1 \
        --finetune ./output/checkpoint-99.pth \
        --batch_size 8 \
        --input_size 224 \
        --short_side_size 224 \
        --save_ckpt_freq 200 \
        --num_frames 16 \
        --sampling_rate 1 \
        --num_sample 1 \
        --num_workers 10 \
        --opt adamw \
        --lr 1e-3 \
        --drop_path 0.3 \
        --clip_grad 5.0 \
        --layer_decay 0.9 \
        --opt_betas 0.9 0.999 \
        --weight_decay 0.1 \
        --warmup_epochs 40 \
        --epochs 1000 \
        --test_num_segment 1 \
        --test_num_crop 1 \
		--eval \
        --min_eval_score -1.9871155 \
        --max_eval_score 2.4927201 \
        --output_dir $1 \
        --data_path $1 \
		--data_root $1 
        
		
		
	
```

`deepcheat/VideoMAEv2/eval_killshot.sh`:

```sh
#!/bin/bash

python eval_killshot_pred.py \
        --model vit_base_patch16_224 \
        --data_set killshot \
        --nb_classes 1 \
        --data_path /home/waldo/code/models/VideoMAEv2/csg_processed2 \
		--data_root /home/waldo/code/models/VideoMAEv2/csg_processed2 \
        --finetune ./killshot2/checkpoint-999.pth \
        --log_dir killshot_test \
        --output_dir killshot_test \
        --batch_size 8 \
        --input_size 224 \
        --short_side_size 224 \
        --save_ckpt_freq 200 \
        --num_frames 16 \
        --sampling_rate 1 \
        --num_sample 1 \
        --num_workers 10 \
        --opt adamw \
        --lr 1e-3 \
        --drop_path 0.3 \
        --clip_grad 5.0 \
        --layer_decay 0.9 \
        --opt_betas 0.9 0.999 \
        --weight_decay 0.1 \
        --warmup_epochs 50 \
        --epochs 1000 \
        --test_num_segment 5 \
        --test_num_crop 3 \
		--eval
		
		
	
```

`deepcheat/VideoMAEv2/eval_killshot_pred.py`:

```py
# --------------------------------------------------------
# Based on BEiT, timm, DINO and DeiT code bases
# https://github.com/microsoft/unilm/tree/master/beit
# https://github.com/rwightman/pytorch-image-models/tree/master/timm
# https://github.com/facebookresearch/deit
# https://github.com/facebookresearch/dino
# --------------------------------------------------------'

import argparse
import datetime
import json
import os
import random
import time
from collections import OrderedDict
from functools import partial
from pathlib import Path

import deepspeed
import numpy as np
import torch
import torch.backends.cudnn as cudnn
from timm.data.mixup import Mixup
from timm.models import create_model
from timm.utils import ModelEma

# NOTE: Do not comment `import models`, it is used to register models
import models  # noqa: F401
import utils
from dataset import build_dataset
from engine_for_finetuning import (
    final_test_killshot,
    predict_killshot_scores,
    merge,
    train_one_epoch,
    validation_one_epoch,
)
from optim_factory import (
    LayerDecayValueAssigner,
    create_optimizer,
    get_parameter_groups,
)
from utils import NativeScalerWithGradNormCount as NativeScaler
from utils import multiple_samples_collate


import torch
import torch.nn as nn
import torch.nn.functional as F


import os
import torch
from torchvision import transforms
from PIL import Image
from collections import deque

class VideoLabelingDataset:
    def __init__(self, directory_path):
        if not os.path.exists(directory_path):
            raise ValueError(f"{directory_path} does not exist.")

        self.image_paths = sorted([os.path.join(directory_path, filename) 
                                   for filename in os.listdir(directory_path) 
                                   if filename.endswith(".jpg")])
        
        self.transform = transforms.Compose([
            transforms.Resize((224, 224)),  
            transforms.ToTensor(),  
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])

    def image_generator(self):
        """
        A generator that yields transformed images one by one.
        """
        for image_path in self.image_paths:
            image = Image.open(image_path)
            image = self.transform(image)
            yield image

    def load_images(self, index_start, index_end):
        if index_end > len(self.image_paths) or index_start < 0:
            raise ValueError("Invalid index range provided.")

        gen = self.image_generator()
        
        # Initialize deque with maximum length as difference between indices
        images = deque(maxlen=index_end - index_start)
        
        # Move generator to start index
        for _ in range(index_start):
            next(gen)
        
        # Populate the deque till the end index
        for _ in range(index_end - index_start):
            images.append(next(gen))

        # Convert deque of tensors to a single tensor
        return torch.stack(list(images),dim=1)

    def sequential_load(self, length):
        """
        Yield batches of images sequentially, shifting by one.
        Args:
            length (int): Length of each sequence.
        """
        gen = self.image_generator()
        
        # Initialize deque with maximum length
        images = deque(maxlen=length)
        
        # Load initial batch
        for _ in range(length):
            images.append(next(gen))
        
        yield torch.stack(list(images),dim=1)

        # For each subsequent image, pop the oldest and append the new one
        for image in gen:
            images.append(image)
            yield torch.stack(list(images),dim=1)

    def __len__(self):
        return len(self.image_paths)

class MultiLabelSmoothingCrossEntropy(nn.Module):
    def __init__(self, smoothing=0.1):
        super(MultiLabelSmoothingCrossEntropy, self).__init__()
        assert smoothing < 1.0
        self.smoothing = smoothing
        self.confidence = 1. - smoothing

    def forward(self, x: torch.Tensor, target: torch.Tensor) -> torch.Tensor:
        # Step 1: Apply sigmoid activation to the logits (x)
        logprobs = torch.sigmoid(x)

        # Step 2: Compute the NLL loss (Negative log-likelihood)
        # We'll use binary log loss as it's multi-label classification
        nll_loss = -(target * torch.log(logprobs) + (1 - target) * torch.log(1 - logprobs))
        
        # Step 3: Compute the smooth loss
        # We take the mean of the logprobs, then negate it (as we did with nll_loss)
        smooth_loss = -logprobs.mean(dim=-1)
        
        # Step 4: Combine the NLL loss and the smooth loss
        loss = self.confidence * nll_loss.mean(dim=-1) + self.smoothing * smooth_loss

        return loss.mean()

class MultiLabelSoftTargetCrossEntropy(nn.Module):
    def __init__(self):
        super(MultiLabelSoftTargetCrossEntropy, self).__init__()

    def forward(self, x: torch.Tensor, target: torch.Tensor) -> torch.Tensor:
        # Apply sigmoid activation to logits
        logprobs = torch.sigmoid(x)
        #breakpoint()

        # Compute the loss for soft targets in a multi-label setting
        # This uses binary cross-entropy loss for each label
        loss = -(target * torch.log(logprobs) + (1 - target) * torch.log(1 - logprobs))

        # Sum across classes and then take the mean across the batch
        loss = loss.sum(dim=-1).mean()

        return loss


def get_args():
    parser = argparse.ArgumentParser(
        'VideoMAE fine-tuning and evaluation script for action classification',
        add_help=False)
    parser.add_argument('--batch_size', default=64, type=int)
    parser.add_argument('--epochs', default=30, type=int)
    parser.add_argument('--update_freq', default=1, type=int)
    parser.add_argument('--save_ckpt_freq', default=100, type=int)

    # Model parameters
    parser.add_argument(
        '--model',
        default='vit_base_patch16_224',
        type=str,
        metavar='MODEL',
        help='Name of model to train')
    parser.add_argument('--tubelet_size', type=int, default=2)
    parser.add_argument(
        '--input_size', default=224, type=int, help='images input size')

    parser.add_argument(
        '--with_checkpoint', action='store_true', default=False)

    parser.add_argument(
        '--drop',
        type=float,
        default=0.0,
        metavar='PCT',
        help='Dropout rate (default: 0.)')
    parser.add_argument(
        '--attn_drop_rate',
        type=float,
        default=0.0,
        metavar='PCT',
        help='Attention dropout rate (default: 0.)')
    parser.add_argument(
        '--drop_path',
        type=float,
        default=0.1,
        metavar='PCT',
        help='Drop path rate (default: 0.1)')
    parser.add_argument(
        '--head_drop_rate',
        type=float,
        default=0.0,
        metavar='PCT',
        help='cls head dropout rate (default: 0.)')

    parser.add_argument(
        '--disable_eval_during_finetuning', action='store_true', default=False)

    parser.add_argument('--model_ema', action='store_true', default=False)
    parser.add_argument(
        '--model_ema_decay', type=float, default=0.9999, help='')
    parser.add_argument(
        '--model_ema_force_cpu', action='store_true', default=False, help='')

    # Optimizer parameters
    parser.add_argument(
        '--opt',
        default='adamw',
        type=str,
        metavar='OPTIMIZER',
        help='Optimizer (default: "adamw"')
    parser.add_argument(
        '--opt_eps',
        default=1e-8,
        type=float,
        metavar='EPSILON',
        help='Optimizer Epsilon (default: 1e-8)')
    parser.add_argument(
        '--opt_betas',
        default=None,
        type=float,
        nargs='+',
        metavar='BETA',
        help='Optimizer Betas (default: None, use opt default)')
    parser.add_argument(
        '--clip_grad',
        type=float,
        default=None,
        metavar='NORM',
        help='Clip gradient norm (default: None, no clipping)')
    parser.add_argument(
        '--momentum',
        type=float,
        default=0.9,
        metavar='M',
        help='SGD momentum (default: 0.9)')
    parser.add_argument(
        '--weight_decay',
        type=float,
        default=0.05,
        help='weight decay (default: 0.05)')
    parser.add_argument(
        '--weight_decay_end',
        type=float,
        default=None,
        help="""Final value of the
        weight decay. We use a cosine schedule for WD and using a larger decay by
        the end of training improves performance for ViTs.""")

    parser.add_argument(
        '--lr',
        type=float,
        default=1e-3,
        metavar='LR',
        help='learning rate (default: 1e-3)')
    parser.add_argument('--layer_decay', type=float, default=0.75)

    parser.add_argument(
        '--warmup_lr',
        type=float,
        default=1e-8,
        metavar='LR',
        help='warmup learning rate (default: 1e-6)')
    parser.add_argument(
        '--min_lr',
        type=float,
        default=1e-6,
        metavar='LR',
        help='lower lr bound for cyclic schedulers that hit 0 (1e-5)')

    parser.add_argument(
        '--warmup_epochs',
        type=int,
        default=5,
        metavar='N',
        help='epochs to warmup LR, if scheduler supports')
    parser.add_argument(
        '--warmup_steps',
        type=int,
        default=-1,
        metavar='N',
        help='num of steps to warmup LR, will overload warmup_epochs if set > 0'
    )

    # Augmentation parameters
    parser.add_argument(
        '--color_jitter',
        type=float,
        default=0.4,
        metavar='PCT',
        help='Color jitter factor (default: 0.4)')
    parser.add_argument(
        '--num_sample', type=int, default=2, help='Repeated_aug (default: 2)')
    parser.add_argument(
        '--aa',
        type=str,
        default='rand-m7-n4-mstd0.5-inc1',
        metavar='NAME',
        help=
        'Use AutoAugment policy. "v0" or "original". " + "(default: rand-m7-n4-mstd0.5-inc1)'
    ),
    parser.add_argument(
        '--smoothing',
        type=float,
        default=0.1,
        help='Label smoothing (default: 0.1)')
    parser.add_argument(
        '--train_interpolation',
        type=str,
        default='bicubic',
        help=
        'Training interpolation (random, bilinear, bicubic default: "bicubic")'
    )

    # Evaluation parameters
    parser.add_argument('--crop_pct', type=float, default=None)
    parser.add_argument('--short_side_size', type=int, default=224)
    parser.add_argument('--test_num_segment', type=int, default=10)
    parser.add_argument('--test_num_crop', type=int, default=3)

    # * Random Erase params
    parser.add_argument(
        '--reprob',
        type=float,
        default=0.25,
        metavar='PCT',
        help='Random erase prob (default: 0.25)')
    parser.add_argument(
        '--remode',
        type=str,
        default='pixel',
        help='Random erase mode (default: "pixel")')
    parser.add_argument(
        '--recount',
        type=int,
        default=1,
        help='Random erase count (default: 1)')
    parser.add_argument(
        '--resplit',
        action='store_true',
        default=False,
        help='Do not random erase first (clean) augmentation split')

    # * Mixup params
    parser.add_argument(
        '--mixup',
        type=float,
        default=0.8,
        help='mixup alpha, mixup enabled if > 0.')
    parser.add_argument(
        '--cutmix',
        type=float,
        default=1.0,
        help='cutmix alpha, cutmix enabled if > 0.')
    parser.add_argument(
        '--cutmix_minmax',
        type=float,
        nargs='+',
        default=None,
        help='cutmix min/max ratio, overrides alpha and enables cutmix if set')
    parser.add_argument(
        '--mixup_prob',
        type=float,
        default=1.0,
        help=
        'Probability of performing mixup or cutmix when either/both is enabled'
    )
    parser.add_argument(
        '--mixup_switch_prob',
        type=float,
        default=0.5,
        help=
        'Probability of switching to cutmix when both mixup and cutmix enabled'
    )
    parser.add_argument(
        '--mixup_mode',
        type=str,
        default='batch',
        help='How to apply mixup/cutmix params. Per "batch", "pair", or "elem"'
    )

    # * Finetuning params
    parser.add_argument(
        '--finetune', default='', help='finetune from checkpoint')
    parser.add_argument('--model_key', default='model|module', type=str)
    parser.add_argument('--model_prefix', default='', type=str)
    parser.add_argument('--init_scale', default=0.001, type=float)
    parser.add_argument('--use_mean_pooling', action='store_true')
    parser.set_defaults(use_mean_pooling=True)
    parser.add_argument(
        '--use_cls', action='store_false', dest='use_mean_pooling')

    # Dataset parameters
    parser.add_argument(
        '--data_path',
        default='/your/data/path/',
        type=str,
        help='dataset path')
    parser.add_argument(
        '--data_root', default='', type=str, help='dataset path root')
    parser.add_argument(
        '--eval_data_path',
        default=None,
        type=str,
        help='dataset path for evaluation')
    parser.add_argument(
        '--nb_classes',
        default=400,
        type=int,
        help='number of the classification types')
    parser.add_argument(
        '--imagenet_default_mean_and_std', default=True, action='store_true')
    parser.add_argument('--num_segments', type=int, default=1)
    parser.add_argument('--num_frames', type=int, default=16)
    parser.add_argument('--sampling_rate', type=int, default=4)
    parser.add_argument('--sparse_sample', default=False, action='store_true')
    parser.add_argument(
        '--data_set',
        default='killshot',
        type=str,
        help='dataset')
    parser.add_argument(
        '--pred_video',
        default='',
        type=str,
        help='which video to load and pred')
    parser.add_argument(
        '--fname_tmpl',
        default='img_{:05}.jpg',
        type=str,
        help='filename_tmpl for rawframe dataset')
    parser.add_argument(
        '--start_idx',
        default=1,
        type=int,
        help='start_idx for rwaframe dataset')

    parser.add_argument(
        '--output_dir',
        default='',
        help='path where to save, empty for no saving')
    parser.add_argument(
        '--log_dir', default=None, help='path where to tensorboard log')
    parser.add_argument(
        '--device',
        default='cuda',
        help='device to use for training / testing')
    parser.add_argument('--seed', default=0, type=int)
    parser.add_argument('--resume', default='', help='resume from checkpoint')
    parser.add_argument('--auto_resume', action='store_true')
    parser.add_argument(
        '--no_auto_resume', action='store_false', dest='auto_resume')
    parser.set_defaults(auto_resume=True)

    parser.add_argument('--save_ckpt', action='store_true')
    parser.add_argument(
        '--no_save_ckpt', action='store_false', dest='save_ckpt')
    parser.set_defaults(save_ckpt=True)

    parser.add_argument(
        '--start_epoch', default=0, type=int, metavar='N', help='start epoch')
    parser.add_argument(
        '--eval', action='store_true', help='Perform evaluation only')
    parser.add_argument(
        '--validation', action='store_true', help='Perform validation only')
    parser.add_argument(
        '--dist_eval',
        action='store_true',
        default=False,
        help='Enabling distributed evaluation')
    parser.add_argument('--num_workers', default=10, type=int)
    parser.add_argument(
        '--pin_mem',
        action='store_true',
        help=
        'Pin CPU memory in DataLoader for more efficient (sometimes) transfer to GPU.'
    )
    parser.add_argument('--no_pin_mem', action='store_false', dest='pin_mem')
    parser.set_defaults(pin_mem=True)

    # distributed training parameters
    parser.add_argument(
        '--world_size',
        default=1,
        type=int,
        help='number of distributed processes')
    parser.add_argument('--local_rank', default=-1, type=int)
    parser.add_argument('--dist_on_itp', action='store_true')
    parser.add_argument(
        '--dist_url',
        default='env://',
        help='url used to set up distributed training')

    parser.add_argument(
        '--enable_deepspeed', action='store_true', default=False)

    known_args, _ = parser.parse_known_args()

    if known_args.enable_deepspeed:
        parser = deepspeed.add_config_arguments(parser)
        ds_init = deepspeed.initialize
    else:
        ds_init = None

    return parser.parse_args(), ds_init


def main(args, ds_init):
    if args.eval == False and args.pred_video == False:
        raise ValueError("Need to set args.eval or args.pred_video")

    utils.init_distributed_mode(args)

    if ds_init is not None:
        utils.create_ds_config(args)

    print(args)

    device = torch.device(args.device)

    # fix the seed for reproducibility
    seed = args.seed + utils.get_rank()
    torch.manual_seed(seed)
    np.random.seed(seed)
    random.seed(seed)
    cudnn.benchmark = True

    args.nb_classes = 1

    if not args.pred_video:
        dataset_test, _ = build_dataset(is_train=False, test_mode=True, args=args)


    model = create_model(
        args.model,
        pred_each_frame=True,
        img_size=args.input_size,
        pretrained=False,
        num_classes=args.nb_classes,
        all_frames=args.num_frames * args.num_segments,
        tubelet_size=args.tubelet_size,
        drop_rate=args.drop,
        drop_path_rate=args.drop_path,
        attn_drop_rate=args.attn_drop_rate,
        head_drop_rate=args.head_drop_rate,
        drop_block_rate=None,
        use_mean_pooling=args.use_mean_pooling,
        init_scale=args.init_scale,
        with_cp=args.with_checkpoint,
    )
   

    patch_size = model.patch_embed.patch_size
    print("Patch size = %s" % str(patch_size))

    args.window_size = (args.num_frames // args.tubelet_size,
                        args.input_size // patch_size[0],
                        args.input_size // patch_size[1])

    args.patch_size = patch_size

    if args.finetune:
        if args.finetune.startswith('https'):
            checkpoint = torch.hub.load_state_dict_from_url(
                args.finetune, map_location='cpu', check_hash=True)
        else:
            checkpoint = torch.load(args.finetune, map_location='cpu')

        print("Load ckpt from %s" % args.finetune)
        checkpoint_model = None
        for model_key in args.model_key.split('|'):
            if model_key in checkpoint:
                checkpoint_model = checkpoint[model_key]
                print("Load state_dict by model_key = %s" % model_key)
                break
        if checkpoint_model is None:
            checkpoint_model = checkpoint
        for old_key in list(checkpoint_model.keys()):
            if old_key.startswith('_orig_mod.'):
                new_key = old_key[10:]
                checkpoint_model[new_key] = checkpoint_model.pop(old_key)

        state_dict = model.state_dict()
        for k in ['head.weight', 'head.bias']:
            if k in checkpoint_model and checkpoint_model[
                    k].shape != state_dict[k].shape:
                if checkpoint_model[k].shape[
                        0] == 710 and args.data_set.startswith('Kinetics'):
                    print(f'Convert K710 head to {args.data_set} head')
                    if args.data_set == 'Kinetics-400':
                        label_map_path = 'misc/label_710to400.json'
                    elif args.data_set == 'Kinetics-600':
                        label_map_path = 'misc/label_710to600.json'
                    elif args.data_set == 'Kinetics-700':
                        label_map_path = 'misc/label_710to700.json'

                    label_map = json.load(open(label_map_path))
                    checkpoint_model[k] = checkpoint_model[k][label_map]
                else:
                    print(f"Removing key {k} from pretrained checkpoint")
                    del checkpoint_model[k]

        all_keys = list(checkpoint_model.keys())
        new_dict = OrderedDict()
        for key in all_keys:
            if key.startswith('backbone.'):
                new_dict[key[9:]] = checkpoint_model[key]
            elif key.startswith('encoder.'):
                new_dict[key[8:]] = checkpoint_model[key]
            else:
                new_dict[key] = checkpoint_model[key]
        checkpoint_model = new_dict

        # interpolate position embedding
        if 'pos_embed' in checkpoint_model:
            pos_embed_checkpoint = checkpoint_model['pos_embed']
            embedding_size = pos_embed_checkpoint.shape[-1]  # channel dim
            num_patches = model.patch_embed.num_patches  #
            num_extra_tokens = model.pos_embed.shape[-2] - num_patches  # 0/1

            # height (== width) for the checkpoint position embedding
            orig_size = int(
                ((pos_embed_checkpoint.shape[-2] - num_extra_tokens) //
                 (args.num_frames // model.patch_embed.tubelet_size))**0.5)
            # height (== width) for the new position embedding
            new_size = int(
                (num_patches //
                 (args.num_frames // model.patch_embed.tubelet_size))**0.5)
            # class_token and dist_token are kept unchanged
            if orig_size != new_size:
                print("Position interpolate from %dx%d to %dx%d" %
                      (orig_size, orig_size, new_size, new_size))
                extra_tokens = pos_embed_checkpoint[:, :num_extra_tokens]
                # only the position tokens are interpolated
                pos_tokens = pos_embed_checkpoint[:, num_extra_tokens:]
                # B, L, C -> BT, H, W, C -> BT, C, H, W
                pos_tokens = pos_tokens.reshape(
                    -1, args.num_frames // model.patch_embed.tubelet_size,
                    orig_size, orig_size, embedding_size)
                pos_tokens = pos_tokens.reshape(-1, orig_size, orig_size,
                                                embedding_size).permute(
                                                    0, 3, 1, 2)
                pos_tokens = torch.nn.functional.interpolate(
                    pos_tokens,
                    size=(new_size, new_size),
                    mode='bicubic',
                    align_corners=False)
                # BT, C, H, W -> BT, H, W, C ->  B, T, H, W, C
                pos_tokens = pos_tokens.permute(0, 2, 3, 1).reshape(
                    -1, args.num_frames // model.patch_embed.tubelet_size,
                    new_size, new_size, embedding_size)
                pos_tokens = pos_tokens.flatten(1, 3)  # B, L, C
                new_pos_embed = torch.cat((extra_tokens, pos_tokens), dim=1)
                checkpoint_model['pos_embed'] = new_pos_embed
        elif args.input_size != 224:
            pos_tokens = model.pos_embed
            org_num_frames = 16
            T = org_num_frames // args.tubelet_size
            P = int((pos_tokens.shape[1] // T)**0.5)
            C = pos_tokens.shape[2]
            new_P = args.input_size // patch_size[0]
            # B, L, C -> BT, H, W, C -> BT, C, H, W
            pos_tokens = pos_tokens.reshape(-1, T, P, P, C)
            pos_tokens = pos_tokens.reshape(-1, P, P, C).permute(0, 3, 1, 2)
            pos_tokens = torch.nn.functional.interpolate(
                pos_tokens,
                size=(new_P, new_P),
                mode='bicubic',
                align_corners=False)
            # BT, C, H, W -> BT, H, W, C ->  B, T, H, W, C
            pos_tokens = pos_tokens.permute(0, 2, 3,
                                            1).reshape(-1, T, new_P, new_P, C)
            pos_tokens = pos_tokens.flatten(1, 3)  # B, L, C
            model.pos_embed = pos_tokens  # update
        if args.num_frames != 16:
            org_num_frames = 16
            T = org_num_frames // args.tubelet_size
            pos_tokens = model.pos_embed
            new_T = args.num_frames // args.tubelet_size
            P = int((pos_tokens.shape[1] // T)**0.5)
            C = pos_tokens.shape[2]
            pos_tokens = pos_tokens.reshape(-1, T, P, P, C)
            pos_tokens = pos_tokens.permute(0, 2, 3, 4,
                                            1).reshape(-1, C, T)  # BHW,C,T
            pos_tokens = torch.nn.functional.interpolate(
                pos_tokens, size=new_T, mode='linear')
            pos_tokens = pos_tokens.reshape(1, P, P, C,
                                            new_T).permute(0, 4, 1, 2, 3)
            pos_tokens = pos_tokens.flatten(1, 3)
            model.pos_embed = pos_tokens  # update

        utils.load_state_dict(
            model, checkpoint_model, prefix=args.model_prefix)

    #breakpoint()
    model.to(device)

    preds_file = os.path.join(args.output_dir, '0.txt')
    
    if args.eval:
        acc, distance_from_correct, kth_guess = final_test_killshot(data_loader_test, model, device, preds_file)

        log_stats = {
            "accuracy": acc,
            "distance_from_correct_avg": np.mean(distance_from_correct),
            "guesses_behind_correct_avg": np.mean(kth_guess),
        }
        for k,v in log_stats.items():
            print(k,v)
        with open(
            os.path.join(args.output_dir, "test_results.txt"),
            mode="a",
            encoding="utf-8") as f:
            f.write(json.dumps(log_stats) + "\n")
        exit(0)


    def top_k_indices(arr, k): return arr.argsort()[-k:][::-1]

    if args.pred_video:
        video_loaded = VideoLabelingDataset(args.pred_video)
        out_preds = predict_killshot_scores(video_loaded, model, device, preds_file)
        #breakpoint()
        print(top_k_indices(out_preds, 20))
        np.savetxt(os.path.join(args.pred_video, "preds.txt"), out_preds, delimiter=',')

        exit(0)


if __name__ == '__main__':
    opts, ds_init = get_args()
    if opts.output_dir:
        Path(opts.output_dir).mkdir(parents=True, exist_ok=True)
    main(opts, ds_init)

```

`deepcheat/VideoMAEv2/eval_mae.py`:

```py
# --------------------------------------------------------
# Based on BEiT, timm, DINO and DeiT code bases
# https://github.com/microsoft/unilm/tree/master/beit
# https://github.com/rwightman/pytorch-image-models/tree/master/timm
# https://github.com/facebookresearch/deit
# https://github.com/facebookresearch/dino
# --------------------------------------------------------'

import argparse
import datetime
import json
import os
import random
import time
from functools import partial
from pathlib import Path
from timm.data.constants import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
from torchvision.transforms import ToPILImage

import numpy as np
import torch
import torch.backends.cudnn as cudnn
from packaging import version
from timm.models import create_model

# NOTE: Do not comment `import models`, it is used to register models
import models  # noqa: F401
import utils
from dataset import build_pretraining_dataset
from engine_for_pretraining import train_one_epoch
from optim_factory import create_optimizer
from utils import NativeScalerWithGradNormCount as NativeScaler
from utils import multiple_pretrain_samples_collate
from einops import rearrange
import cv2


def get_args():
    parser = argparse.ArgumentParser(
        'VideoMAE v2 pre-training script', add_help=False)
    parser.add_argument('--batch_size', default=64, type=int)
    parser.add_argument('--epochs', default=300, type=int)
    parser.add_argument('--save_ckpt_freq', default=50, type=int)

    # Model parameters
    parser.add_argument(
        '--model',
        default='pretrain_videomae_base_patch16_224',
        type=str,
        metavar='MODEL',
        help='Name of model to train')
    parser.add_argument('--tubelet_size', type=int, default=2)
    parser.add_argument(
        '--with_checkpoint', action='store_true', default=False)

    parser.add_argument(
        '--decoder_depth', default=4, type=int, help='depth of decoder')

    parser.add_argument(
        '--mask_type',
        default='tube',
        choices=['random', 'tube'],
        type=str,
        help='encoder masked strategy')
    parser.add_argument(
        '--decoder_mask_type',
        default='run_cell',
        choices=['random', 'run_cell'],
        type=str,
        help='decoder masked strategy')

    parser.add_argument(
        '--mask_ratio', default=0.9, type=float, help='mask ratio of encoder')
    parser.add_argument(
        '--decoder_mask_ratio',
        default=0.0,
        type=float,
        help='mask ratio of decoder')

    parser.add_argument(
        '--input_size',
        default=224,
        type=int,
        help='images input size for backbone')

    parser.add_argument(
        '--drop_path',
        type=float,
        default=0.0,
        metavar='PCT',
        help='Drop path rate (default: 0.1)')

    parser.add_argument(
        '--normlize_target',
        default=True,
        type=bool,
        help='normalized the target patch pixels')

    # Optimizer parameters
    parser.add_argument(
        '--opt',
        default='adamw',
        type=str,
        metavar='OPTIMIZER',
        help='Optimizer (default: "adamw"')
    parser.add_argument(
        '--opt_eps',
        default=1e-8,
        type=float,
        metavar='EPSILON',
        help='Optimizer Epsilon (default: 1e-8)')
    parser.add_argument(
        '--opt_betas',
        default=None,
        type=float,
        nargs='+',
        metavar='BETA',
        help='Optimizer Betas (default: None, use opt default)')
    parser.add_argument(
        '--clip_grad',
        type=float,
        default=None,
        metavar='NORM',
        help='Clip gradient norm (default: None, no clipping)')
    parser.add_argument(
        '--momentum',
        type=float,
        default=0.9,
        metavar='M',
        help='SGD momentum (default: 0.9)')
    parser.add_argument(
        '--weight_decay',
        type=float,
        default=0.05,
        help='weight decay (default: 0.05)')
    parser.add_argument(
        '--weight_decay_end',
        type=float,
        default=None,
        help="""Final value of the
        weight decay. We use a cosine schedule for WD. 
        (Set the same value with args.weight_decay to keep weight decay no change)"""
    )

    parser.add_argument(
        '--lr',
        type=float,
        default=1.5e-4,
        metavar='LR',
        help='learning rate (default: 1.5e-4)')
    parser.add_argument(
        '--warmup_lr',
        type=float,
        default=1e-6,
        metavar='LR',
        help='warmup learning rate (default: 1e-6)')
    parser.add_argument(
        '--min_lr',
        type=float,
        default=1e-5,
        metavar='LR',
        help='lower lr bound for cyclic schedulers that hit 0 (1e-5)')

    parser.add_argument(
        '--warmup_epochs',
        type=int,
        default=40,
        metavar='N',
        help='epochs to warmup LR, if scheduler supports')
    parser.add_argument(
        '--warmup_steps',
        type=int,
        default=-1,
        metavar='N',
        help='epochs to warmup LR, if scheduler supports')

    # Augmentation parameters
    parser.add_argument(
        '--color_jitter',
        type=float,
        default=0.0,
        metavar='PCT',
        help='Color jitter factor (default: 0.4)')
    parser.add_argument(
        '--train_interpolation',
        type=str,
        default='bicubic',
        choices=['random', 'bilinear', 'bicubic'],
        help='Training interpolation')

    # * Finetuning params
    parser.add_argument(
        '--finetune', default='', help='finetune from checkpoint')

    # Dataset parameters
    parser.add_argument(
        '--data_path',
        default='/your/data/annotation/path',
        type=str,
        help='dataset path')
    parser.add_argument(
        '--data_root', default='', type=str, help='dataset path root')
    parser.add_argument(
        '--fname_tmpl',
        default='img_{:010}.jpg',
        type=str,
        help='filename_tmpl for rawframe data')
    parser.add_argument(
        '--imagenet_default_mean_and_std', default=True, action='store_true')
    parser.add_argument('--num_frames', type=int, default=32)
    parser.add_argument('--sampling_rate', type=int, default=1)
    parser.add_argument('--num_sample', type=int, default=1)
    parser.add_argument(
        '--output_dir',
        default='',
        help='path where to save, empty for no saving')
    parser.add_argument(
        '--log_dir', default=None, help='path where to tensorboard log')
    parser.add_argument(
        '--device',
        default='cuda',
        help='device to use for training / testing')
    parser.add_argument('--seed', default=0, type=int)
    parser.add_argument('--resume', default='', help='resume from checkpoint')
    parser.add_argument('--auto_resume', action='store_true')
    parser.add_argument(
        '--no_auto_resume', action='store_false', dest='auto_resume')
    parser.set_defaults(auto_resume=True)

    parser.add_argument(
        '--start_epoch', default=0, type=int, metavar='N', help='start epoch')
    parser.add_argument('--num_workers', default=10, type=int)
    parser.add_argument(
        '--pin_mem',
        action='store_true',
        help=
        'Pin CPU memory in DataLoader for more efficient (sometimes) transfer to GPU.'
    )
    parser.add_argument(
        '--no_pin_mem', action='store_false', dest='pin_mem', help='')
    parser.set_defaults(pin_mem=True)

    # distributed training parameters
    parser.add_argument(
        '--world_size',
        default=1,
        type=int,
        help='number of distributed processes')
    parser.add_argument('--local_rank', default=-1, type=int)
    parser.add_argument('--dist_on_itp', action='store_true')
    parser.add_argument(
        '--dist_url',
        default='env://',
        help='url used to set up distributed training')

    return parser.parse_args()


def get_model(args):
    print(f"Creating model: {args.model}")
    model = create_model(
        args.model,
        pretrained=False,
        drop_path_rate=args.drop_path,
        drop_block_rate=None,
        all_frames=args.num_frames,
        tubelet_size=args.tubelet_size,
        decoder_depth=args.decoder_depth,
        with_cp=args.with_checkpoint)

    if version.parse(torch.__version__) > version.parse('1.13.1'):
        torch.set_float32_matmul_precision('high')
        #model = torch.compile(model)

    return model

@torch.no_grad()
def main(args):

    print(args)
    device = torch.device(args.device)
    seed = args.seed
    # fix the seed for reproducibility
    torch.manual_seed(seed)
    np.random.seed(seed)
    random.seed(seed)
    cudnn.benchmark = True
    model = get_model(args)
    patch_size = model.encoder.patch_embed.patch_size
    print("Patch size = %s" % str(patch_size))
    args.window_size = (args.num_frames // args.tubelet_size,
                        args.input_size // patch_size[0],
                        args.input_size // patch_size[1])
    args.patch_size = patch_size

    # get dataset
    dataset_train = build_pretraining_dataset(args,test_mode=True)

    total_batch_size = args.batch_size 

    log_writer = None

    data_loader_test = torch.utils.data.DataLoader(
        dataset_train,
        batch_size=args.batch_size,
        num_workers=args.num_workers,
        pin_memory=args.pin_mem,
        drop_last=False,
        persistent_workers=True)

    if not args.finetune: raise ValueError("must provide args.finetune filepath")
    checkpoint = torch.load(args.finetune, map_location='cpu')

    print("Load ckpt from %s" % args.finetune)
    #breakpoint()
    checkpoint_model = None
    for model_key in ['model', 'module']:
        if model_key in checkpoint:
            checkpoint_model = checkpoint[model_key]
            print("Load state_dict by model_key = %s" % model_key)
            break
    if checkpoint_model is None:
        checkpoint_model = checkpoint
    if "/" not in args.finetune:
        modified_dict = {f'encoder.{key}': value for key, value in checkpoint_model.items()}
        modified_dict['encoder.norm.bias'] = checkpoint_model['fc_norm.bias']
        modified_dict['encoder.norm.weight'] = checkpoint_model['fc_norm.weight']
        checkpoint_model = modified_dict

    utils.load_state_dict(model, checkpoint_model)
    model.to(device)


    start_time = time.time()
    mean = torch.as_tensor(IMAGENET_DEFAULT_MEAN).to(device)[None, :, None, None, None]
    std = torch.as_tensor(IMAGENET_DEFAULT_STD).to(device)[None, :, None, None, None]

    get_test_loss=False
    total_loss = 0
    for step, batch in enumerate(data_loader_test):
        

        # NOTE: When the decoder mask ratio is 0,
        # in other words, when decoder masking is not used,
        # decode_masked_pos = ~bool_masked_pos
        images, bool_masked_pos, decode_masked_pos = batch

        images = images.to(device, non_blocking=True)
        bool_masked_pos = bool_masked_pos.to(
            device, non_blocking=True).flatten(1).to(torch.bool)
        decode_masked_pos = decode_masked_pos.to(
            device, non_blocking=True).flatten(1).to(torch.bool)

        if get_test_loss:
            unnorm_images = images * std + mean  # in [0, 1]

            if args.normlize_target:
                images_squeeze = rearrange(
                    unnorm_images,
                    'b c (t p0) (h p1) (w p2) -> b (t h w) (p0 p1 p2) c',
                    p0=2,
                    p1=patch_size[0],
                    p2=patch_size[0])
                images_norm = (images_squeeze - images_squeeze.mean(
                    dim=-2, keepdim=True)) / (
                        images_squeeze.var(
                            dim=-2, unbiased=True, keepdim=True).sqrt() + 1e-6)
                images_patch = rearrange(images_norm, 'b n p c -> b n (p c)')
            else:
                images_patch = rearrange(
                    unnorm_images,
                    'b c (t p0) (h p1) (w p2) -> b (t h w) (p0 p1 p2 c)',
                    p0=2,
                    p1=patch_size,
                    p2=patch_size)

            B, N, C = images_patch.shape
            labels = images_patch[~decode_masked_pos].reshape(B, -1, C)
            #breakpoint()
            #if loss_scaler is None:
            outputs = model(images, bool_masked_pos, decode_masked_pos)

            loss = (outputs - labels)**2
            loss = loss.mean(dim=-1)
            cal_loss_mask = bool_masked_pos[~decode_masked_pos].reshape(B, -1)
            loss = (loss * cal_loss_mask).sum()
            total_loss+=loss.item()
        else:
            break

    if get_test_loss:
        print("total_loss ", total_loss)
        exit()


    #video_data = vr.get_batch(frame_id_list).asnumpy()
    #print(video_data.shape)
    i = 1
    img = images[i].unsqueeze(0)
    bool_masked_pos =  bool_masked_pos[i].unsqueeze(0)
    decode_masked_pos = decode_masked_pos[i].unsqueeze(0)
    #transforms = DataAugmentationForVideoMAE(args)
    #img, bool_masked_pos = transforms((img, None)) # T*C,H,W
    # print(img.shape)
    #img = img.view((16 , 3) + img.size()[-2:]).transpose(0,1) # T*C,H,W -> T,C,H,W -> C,T,H,W
    # img = img.view(( -1 , args.num_frames) + img.size()[-2:]) 
    #bool_masked_pos = torch.from_numpy(bool_masked_pos)

    # img = img[None, :]
    # bool_masked_pos = bool_masked_pos[None, :]
    img = img
    print(img.shape)
    #bool_masked_pos = bool_masked_pos[0].unsqueeze(0)
    args.save_path = "out_imgs"
    
    #img = img.to(device, non_blocking=True)
    #bool_masked_pos = bool_masked_pos.to(device, non_blocking=True).flatten(1).to(torch.bool)
    #outputs = model(img, bool_masked_pos)
    outputs = model(img, bool_masked_pos, decode_masked_pos)


    tmp = np.arange(0,32, 2) + 60
    frame_id_list = tmp.tolist()
    #save original video
    
    ori_img = img * std + mean  # in [0, 1]
    #breakpoint()
    imgs = [ToPILImage()(ori_img[0,:,vid,:,:].cpu()) for vid, _ in enumerate(frame_id_list)  ]
    # Initialize video writer
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # codec for video
    frame_size = (imgs[0].size[0], imgs[0].size[1])  # assuming all images have same size
    fps = 3  # frames per second
    out = cv2.VideoWriter(f"{args.save_path}/ori.mp4", fourcc, fps, frame_size)

    # Save each image and write to video
    for id, im in enumerate(imgs):
        im.save(f"{args.save_path}/ori_img{id}.jpg")
        
        # Convert PIL image to NumPy array (OpenCV uses BGR instead of RGB)
        frame = cv2.cvtColor(np.array(im), cv2.COLOR_RGB2BGR)
        
        # Write the frame to the video
        out.write(frame)

    # Release the video writer
    out.release()

    img_squeeze = rearrange(ori_img, 'b c (t p0) (h p1) (w p2) -> b (t h w) (p0 p1 p2) c', p0=2, p1=patch_size[0], p2=patch_size[0])
    img_norm = (img_squeeze - img_squeeze.mean(dim=-2, keepdim=True)) / (img_squeeze.var(dim=-2, unbiased=True, keepdim=True).sqrt() + 1e-6)
    img_patch = rearrange(img_norm, 'b n p c -> b n (p c)')
    img_patch[bool_masked_pos] = outputs

    #make mask
    mask = torch.ones_like(img_patch)
    mask[bool_masked_pos] = 0
    mask = rearrange(mask, 'b n (p c) -> b n p c', c=3)
    mask = rearrange(mask, 'b (t h w) (p0 p1 p2) c -> b c (t p0) (h p1) (w p2) ', p0=2, p1=patch_size[0], p2=patch_size[1], h=14, w=14)

    #save reconstruction video
    rec_img = rearrange(img_patch, 'b n (p c) -> b n p c', c=3)
    # Notice: To visualize the reconstruction video, we add the predict and the original mean and var of each patch.
    rec_img = rec_img * (img_squeeze.var(dim=-2, unbiased=True, keepdim=True).sqrt() + 1e-6) + img_squeeze.mean(dim=-2, keepdim=True)
    rec_img = rearrange(rec_img, 'b (t h w) (p0 p1 p2) c -> b c (t p0) (h p1) (w p2)', p0=2, p1=patch_size[0], p2=patch_size[1], h=14, w=14)
    imgsr = [ ToPILImage()(rec_img[0, :, vid, :, :].cpu().clamp(0,0.996)) for vid, _ in enumerate(frame_id_list)  ]

    out = cv2.VideoWriter(f"{args.save_path}/rec.mp4", fourcc, fps, frame_size)

    # Save each image and write to video
    for id, im in enumerate(imgsr):
        im.save(f"{args.save_path}/rec_img{id}.jpg")
        
        # Convert PIL image to NumPy array (OpenCV uses BGR instead of RGB)
        frame = cv2.cvtColor(np.array(im), cv2.COLOR_RGB2BGR)
        
        # Write the frame to the video
        out.write(frame)

    # Release the video writer
    out.release()

    #save masked video 
    img_mask = rec_img * mask
    imgsm = [ToPILImage()(img_mask[0, :, vid, :, :].cpu()) for vid, _ in enumerate(frame_id_list)]
    out = cv2.VideoWriter(f"{args.save_path}/mask.mp4", fourcc, fps, frame_size)

    # Save each image and write to video
    for id, im in enumerate(imgsm):
        im.save(f"{args.save_path}/mask_img{id}.jpg")
        
        # Convert PIL image to NumPy array (OpenCV uses BGR instead of RGB)
        frame = cv2.cvtColor(np.array(im), cv2.COLOR_RGB2BGR)
        
        # Write the frame to the video
        out.write(frame)

    # Release the video writer
    out.release()
    single_frame_size = (imgs[0].size[0], imgs[0].size[1]) 
    frame_size = (single_frame_size[0] * 3, single_frame_size[1])

    out = cv2.VideoWriter(f"{args.save_path}/results.mp4", fourcc, fps, frame_size)

    # Save each image and write to video
    for id, (im, im_left, im_right) in enumerate(zip( imgsr,imgs, imgsm)):
        
        # Convert PIL image to NumPy array (OpenCV uses BGR instead of RGB)
        frame = cv2.cvtColor(np.hstack((np.array(im_left), np.array(im), np.array(im_right))), cv2.COLOR_RGB2BGR)
        
        # Write the frame to the video
        out.write(frame)

    # Release the video writer
    out.release()



if __name__ == '__main__':
    opts = get_args()
    if opts.output_dir:
        Path(opts.output_dir).mkdir(parents=True, exist_ok=True)
    main(opts)

```

`deepcheat/VideoMAEv2/extract_tad_feature.py`:

```py
"""Extract features for temporal action detection datasets"""
import argparse
import os
import random

import numpy as np
import torch
from timm.models import create_model
from torchvision import transforms

# NOTE: Do not comment `import models`, it is used to register models
import models  # noqa: F401
from dataset.loader import get_video_loader


def to_normalized_float_tensor(vid):
    return vid.permute(3, 0, 1, 2).to(torch.float32) / 255


# NOTE: for those functions, which generally expect mini-batches, we keep them
# as non-minibatch so that they are applied as if they were 4d (thus image).
# this way, we only apply the transformation in the spatial domain
def resize(vid, size, interpolation='bilinear'):
    # NOTE: using bilinear interpolation because we don't work on minibatches
    # at this level
    scale = None
    if isinstance(size, int):
        scale = float(size) / min(vid.shape[-2:])
        size = None
    return torch.nn.functional.interpolate(
        vid,
        size=size,
        scale_factor=scale,
        mode=interpolation,
        align_corners=False)


class ToFloatTensorInZeroOne(object):

    def __call__(self, vid):
        return to_normalized_float_tensor(vid)


class Resize(object):

    def __init__(self, size):
        self.size = size

    def __call__(self, vid):
        return resize(vid, self.size)


def get_args():
    parser = argparse.ArgumentParser(
        'Extract TAD features using the videomae model', add_help=False)

    parser.add_argument(
        '--data_set',
        default='THUMOS14',
        choices=['THUMOS14', 'FINEACTION'],
        type=str,
        help='dataset')

    parser.add_argument(
        '--data_path',
        default='YOUR_PATH/thumos14_video',
        type=str,
        help='dataset path')
    parser.add_argument(
        '--save_path',
        default='YOUR_PATH/thumos14_video/th14_vit_g_16_4',
        type=str,
        help='path for saving features')

    parser.add_argument(
        '--model',
        default='vit_giant_patch14_224',
        type=str,
        metavar='MODEL',
        help='Name of model')
    parser.add_argument(
        '--ckpt_path',
        default='YOUR_PATH/vit_g_hyrbid_pt_1200e_k710_ft.pth',
        help='load from checkpoint')

    return parser.parse_args()


def get_start_idx_range(data_set):

    def thumos14_range(num_frames):
        return range(0, num_frames - 15, 4)

    def fineaction_range(num_frames):
        return range(0, num_frames - 15, 16)

    if data_set == 'THUMOS14':
        return thumos14_range
    elif data_set == 'FINEACTION':
        return fineaction_range
    else:
        raise NotImplementedError()


def extract_feature(args):
    # preparation
    if not os.path.exists(args.save_path):
        os.makedirs(args.save_path)
    video_loader = get_video_loader()
    start_idx_range = get_start_idx_range(args.data_set)
    transform = transforms.Compose(
        [ToFloatTensorInZeroOne(),
         Resize((224, 224))])

    # get video path
    vid_list = os.listdir(args.data_path)
    random.shuffle(vid_list)

    # get model & load ckpt
    model = create_model(
        args.model,
        img_size=224,
        pretrained=False,
        num_classes=710,
        all_frames=16,
        tubelet_size=2,
        drop_path_rate=0.3,
        use_mean_pooling=True)
    ckpt = torch.load(args.ckpt_path, map_location='cpu')
    for model_key in ['model', 'module']:
        if model_key in ckpt:
            ckpt = ckpt[model_key]
            break
    model.load_state_dict(ckpt)
    model.eval()
    model.cuda()

    # extract feature
    num_videos = len(vid_list)
    for idx, vid_name in enumerate(vid_list):
        url = os.path.join(args.save_path, vid_name.split('.')[0] + '.npy')
        if os.path.exists(url):
            continue

        video_path = os.path.join(args.data_path, vid_name)
        vr = video_loader(video_path)

        feature_list = []
        for start_idx in start_idx_range(len(vr)):
            data = vr.get_batch(np.arange(start_idx, start_idx + 16)).asnumpy()
            frame = torch.from_numpy(data)  # torch.Size([16, 566, 320, 3])
            frame_q = transform(frame)  # torch.Size([3, 16, 224, 224])
            input_data = frame_q.unsqueeze(0).cuda()

            with torch.no_grad():
                feature = model.forward_features(input_data)
                feature_list.append(feature.cpu().numpy())

        # [N, C]
        np.save(url, np.vstack(feature_list))
        print(f'[{idx} / {num_videos}]: save feature on {url}')


if __name__ == '__main__':
    args = get_args()
    extract_feature(args)

```

`deepcheat/VideoMAEv2/models/__init__.py`:

```py
from .modeling_finetune import (
    vit_base_patch16_224,
    vit_giant_patch14_224,
    vit_huge_patch16_224,
    vit_large_patch16_224,
    vit_small_patch16_224,
)
from .modeling_pretrain import (
    pretrain_videomae_base_patch16_224,
    pretrain_videomae_giant_patch14_224,
    pretrain_videomae_huge_patch16_224,
    pretrain_videomae_large_patch16_224,
    pretrain_videomae_small_patch16_224,
)

__all__ = [
    'pretrain_videomae_small_patch16_224',
    'pretrain_videomae_base_patch16_224',
    'pretrain_videomae_large_patch16_224',
    'pretrain_videomae_huge_patch16_224',
    'pretrain_videomae_giant_patch14_224',
    'vit_small_patch16_224',
    'vit_base_patch16_224',
    'vit_large_patch16_224',
    'vit_huge_patch16_224',
    'vit_giant_patch14_224',
]
```

`deepcheat/VideoMAEv2/models/modeling_finetune.py`:

```py
# --------------------------------------------------------
# Based on BEiT, timm, DINO and DeiT code bases
# https://github.com/microsoft/unilm/tree/master/beit
# https://github.com/rwightman/pytorch-image-models/tree/master/timm
# https://github.com/facebookresearch/deit
# https://github.com/facebookresearch/dino
# --------------------------------------------------------'
from functools import partial

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.utils.checkpoint as cp
from timm.models.layers import drop_path, to_2tuple, trunc_normal_
from timm.models.registry import register_model


def _cfg(url='', **kwargs):
    return {
        'url': url,
        'num_classes': 400,
        'input_size': (3, 224, 224),
        'pool_size': None,
        'crop_pct': .9,
        'interpolation': 'bicubic',
        'mean': (0.5, 0.5, 0.5),
        'std': (0.5, 0.5, 0.5),
        **kwargs
    }


class DropPath(nn.Module):
    """Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).
    """

    def __init__(self, drop_prob=None):
        super(DropPath, self).__init__()
        self.drop_prob = drop_prob

    def forward(self, x):
        return drop_path(x, self.drop_prob, self.training)

    def extra_repr(self) -> str:
        return 'p={}'.format(self.drop_prob)


class Mlp(nn.Module):

    def __init__(self,
                 in_features,
                 hidden_features=None,
                 out_features=None,
                 act_layer=nn.GELU,
                 drop=0.):
        super().__init__()
        out_features = out_features or in_features
        hidden_features = hidden_features or in_features
        self.fc1 = nn.Linear(in_features, hidden_features)
        self.act = act_layer()
        self.fc2 = nn.Linear(hidden_features, out_features)
        self.drop = nn.Dropout(drop)

    def forward(self, x):
        x = self.fc1(x)
        x = self.act(x)
        # x = self.drop(x)
        # commit this for the orignal BERT implement
        x = self.fc2(x)
        x = self.drop(x)
        return x


class CosAttention(nn.Module):

    def __init__(self,
                 dim,
                 num_heads=8,
                 qkv_bias=False,
                 qk_scale=None,
                 attn_drop=0.,
                 proj_drop=0.,
                 attn_head_dim=None):
        super().__init__()
        self.num_heads = num_heads
        head_dim = dim // num_heads
        if attn_head_dim is not None:
            head_dim = attn_head_dim
        all_head_dim = head_dim * self.num_heads
        # self.scale = qk_scale or head_dim**-0.5
        # DO NOT RENAME [self.scale] (for no weight decay)
        if qk_scale is None:
            self.scale = nn.Parameter(
                torch.log(10 * torch.ones((num_heads, 1, 1))),
                requires_grad=True)
        else:
            self.scale = qk_scale

        self.qkv = nn.Linear(dim, all_head_dim * 3, bias=False)
        if qkv_bias:
            self.q_bias = nn.Parameter(torch.zeros(all_head_dim))
            self.v_bias = nn.Parameter(torch.zeros(all_head_dim))
        else:
            self.q_bias = None
            self.v_bias = None

        self.attn_drop = nn.Dropout(attn_drop)
        self.proj = nn.Linear(all_head_dim, dim)
        self.proj_drop = nn.Dropout(proj_drop)

    def forward(self, x):
        B, N, C = x.shape
        qkv_bias = None
        if self.q_bias is not None:
            qkv_bias = torch.cat(
                (self.q_bias,
                 torch.zeros_like(self.v_bias,
                                  requires_grad=False), self.v_bias))
        qkv = F.linear(input=x, weight=self.qkv.weight, bias=qkv_bias)
        qkv = qkv.reshape(B, N, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)
        q, k, v = qkv[0], qkv[1], qkv[
            2]  # make torchscript happy (cannot use tensor as tuple)

        attn = (
            F.normalize(q, dim=-1) @ F.normalize(k, dim=-1).transpose(-2, -1))

        # torch.log(torch.tensor(1. / 0.01)) = 4.6052
        logit_scale = torch.clamp(self.scale, max=4.6052).exp()

        attn = attn * logit_scale

        attn = attn.softmax(dim=-1)
        attn = self.attn_drop(attn)

        x = (attn @ v).transpose(1, 2).reshape(B, N, -1)

        x = self.proj(x)
        x = self.proj_drop(x)
        return x


class Attention(nn.Module):

    def __init__(self,
                 dim,
                 num_heads=8,
                 qkv_bias=False,
                 qk_scale=None,
                 attn_drop=0.,
                 proj_drop=0.,
                 attn_head_dim=None):
        super().__init__()
        self.num_heads = num_heads
        head_dim = dim // num_heads
        if attn_head_dim is not None:
            head_dim = attn_head_dim
        all_head_dim = head_dim * self.num_heads
        self.scale = qk_scale or head_dim**-0.5

        self.qkv = nn.Linear(dim, all_head_dim * 3, bias=False)
        if qkv_bias:
            self.q_bias = nn.Parameter(torch.zeros(all_head_dim))
            self.v_bias = nn.Parameter(torch.zeros(all_head_dim))
        else:
            self.q_bias = None
            self.v_bias = None

        self.attn_drop = nn.Dropout(attn_drop)
        self.proj = nn.Linear(all_head_dim, dim)
        self.proj_drop = nn.Dropout(proj_drop)

    def forward(self, x):
        B, N, C = x.shape
        qkv_bias = None
        if self.q_bias is not None:
            qkv_bias = torch.cat(
                (self.q_bias,
                 torch.zeros_like(self.v_bias,
                                  requires_grad=False), self.v_bias))
        qkv = F.linear(input=x, weight=self.qkv.weight, bias=qkv_bias)
        qkv = qkv.reshape(B, N, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)
        q, k, v = qkv[0], qkv[1], qkv[
            2]  # make torchscript happy (cannot use tensor as tuple)

        q = q * self.scale
        attn = (q @ k.transpose(-2, -1))

        attn = attn.softmax(dim=-1)
        attn = self.attn_drop(attn)

        x = (attn @ v).transpose(1, 2).reshape(B, N, -1)

        x = self.proj(x)
        x = self.proj_drop(x)
        return x


class Block(nn.Module):

    def __init__(self,
                 dim,
                 num_heads,
                 mlp_ratio=4.,
                 qkv_bias=False,
                 qk_scale=None,
                 drop=0.,
                 attn_drop=0.,
                 drop_path=0.,
                 init_values=None,
                 act_layer=nn.GELU,
                 norm_layer=nn.LayerNorm,
                 attn_head_dim=None,
                 cos_attn=False):
        super().__init__()
        self.norm1 = norm_layer(dim)
        if cos_attn:
            self.attn = CosAttention(
                dim,
                num_heads=num_heads,
                qkv_bias=qkv_bias,
                qk_scale=qk_scale,
                attn_drop=attn_drop,
                proj_drop=drop,
                attn_head_dim=attn_head_dim)
        else:
            self.attn = Attention(
                dim,
                num_heads=num_heads,
                qkv_bias=qkv_bias,
                qk_scale=qk_scale,
                attn_drop=attn_drop,
                proj_drop=drop,
                attn_head_dim=attn_head_dim)
        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here
        self.drop_path = DropPath(
            drop_path) if drop_path > 0. else nn.Identity()
        self.norm2 = norm_layer(dim)
        mlp_hidden_dim = int(dim * mlp_ratio)
        self.mlp = Mlp(
            in_features=dim,
            hidden_features=mlp_hidden_dim,
            act_layer=act_layer,
            drop=drop)

        if init_values > 0:
            self.gamma_1 = nn.Parameter(
                init_values * torch.ones((dim)), requires_grad=True)
            self.gamma_2 = nn.Parameter(
                init_values * torch.ones((dim)), requires_grad=True)
        else:
            self.gamma_1, self.gamma_2 = None, None

    def forward(self, x):
        if self.gamma_1 is None:
            x = x + self.drop_path(self.attn(self.norm1(x)))
            x = x + self.drop_path(self.mlp(self.norm2(x)))
        else:
            x = x + self.drop_path(self.gamma_1 * self.attn(self.norm1(x)))
            x = x + self.drop_path(self.gamma_2 * self.mlp(self.norm2(x)))
        return x


class PatchEmbed(nn.Module):
    """ Image to Patch Embedding
    """

    def __init__(self,
                 img_size=224,
                 patch_size=16,
                 in_chans=3,
                 embed_dim=768,
                 num_frames=16,
                 tubelet_size=2):
        super().__init__()
        img_size = to_2tuple(img_size)
        patch_size = to_2tuple(patch_size)
        num_spatial_patches = (img_size[0] // patch_size[0]) * (
            img_size[1] // patch_size[1])
        num_patches = num_spatial_patches * (num_frames // tubelet_size)

        self.img_size = img_size
        self.tubelet_size = tubelet_size
        self.patch_size = patch_size
        self.num_patches = num_patches
        self.proj = nn.Conv3d(
            in_channels=in_chans,
            out_channels=embed_dim,
            kernel_size=(self.tubelet_size, patch_size[0], patch_size[1]),
            stride=(self.tubelet_size, patch_size[0], patch_size[1]))

    def forward(self, x, **kwargs):
        B, C, T, H, W = x.shape
        assert H == self.img_size[0] and W == self.img_size[
            1], f"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]})."
        # b, c, l -> b, l, c
        x = self.proj(x).flatten(2).transpose(1, 2)
        return x


# sin-cos position encoding
# https://github.com/jadore801120/attention-is-all-you-need-pytorch/blob/master/transformer/Models.py#L31
def get_sinusoid_encoding_table(n_position, d_hid):
    ''' Sinusoid position encoding table '''

    # TODO: make it with torch instead of numpy
    def get_position_angle_vec(position):
        return [
            position / np.power(10000, 2 * (hid_j // 2) / d_hid)
            for hid_j in range(d_hid)
        ]

    sinusoid_table = np.array(
        [get_position_angle_vec(pos_i) for pos_i in range(n_position)])
    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # dim 2i
    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # dim 2i+1

    return torch.tensor(
        sinusoid_table, dtype=torch.float, requires_grad=False).unsqueeze(0)


class VisionTransformer(nn.Module):
    """ Vision Transformer with support for patch or hybrid CNN input stage
    """

    def __init__(self,
                 img_size=224,
                 patch_size=16,
                 in_chans=3,
                 num_classes=1000,
                 embed_dim=768,
                 depth=12,
                 num_heads=12,
                 mlp_ratio=4.,
                 qkv_bias=False,
                 qk_scale=None,
                 drop_rate=0.,
                 attn_drop_rate=0.,
                 drop_path_rate=0.,
                 head_drop_rate=0.,
                 norm_layer=nn.LayerNorm,
                 init_values=0.,
                 use_learnable_pos_emb=False,
                 init_scale=0.,
                 all_frames=16,
                 tubelet_size=2,
                 use_mean_pooling=True,
                 with_cp=False,
                 pred_each_frame=False,
                 cos_attn=False):
        super().__init__()
        self.num_classes = num_classes
        # num_features for consistency with other models
        self.num_features = self.embed_dim = embed_dim
        self.tubelet_size = tubelet_size
        self.patch_embed = PatchEmbed(
            img_size=img_size,
            patch_size=patch_size,
            in_chans=in_chans,
            embed_dim=embed_dim,
            num_frames=all_frames,
            tubelet_size=tubelet_size)
        num_patches = self.patch_embed.num_patches
        self.with_cp = with_cp
        self.pred_each_frame = pred_each_frame

        if use_learnable_pos_emb:
            self.pos_embed = nn.Parameter(
                torch.zeros(1, num_patches, embed_dim))
        else:
            # sine-cosine positional embeddings is on the way
            self.pos_embed = get_sinusoid_encoding_table(
                num_patches, embed_dim)

        self.pos_drop = nn.Dropout(p=drop_rate)

        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)
               ]  # stochastic depth decay rule
        self.blocks = nn.ModuleList([
            Block(
                dim=embed_dim,
                num_heads=num_heads,
                mlp_ratio=mlp_ratio,
                qkv_bias=qkv_bias,
                qk_scale=qk_scale,
                drop=drop_rate,
                attn_drop=attn_drop_rate,
                drop_path=dpr[i],
                norm_layer=norm_layer,
                init_values=init_values,
                cos_attn=cos_attn) for i in range(depth)
        ])
        self.norm = nn.Identity() if use_mean_pooling else norm_layer(
            embed_dim)
        self.fc_norm = norm_layer(embed_dim) if use_mean_pooling else None
        self.head_dropout = nn.Dropout(head_drop_rate)
        self.head = nn.Linear(
            embed_dim, num_classes) if num_classes > 0 else nn.Identity()
        self.head2 = nn.Linear(embed_dim, num_classes)

        if use_learnable_pos_emb:
            trunc_normal_(self.pos_embed, std=.02)

        self.apply(self._init_weights)

        self.head.weight.data.mul_(init_scale)
        self.head2.weight.data.mul_(init_scale)
        self.head.bias.data.mul_(init_scale)
        self.head2.bias.data.mul_(init_scale)

    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            trunc_normal_(m.weight, std=.02)
            if isinstance(m, nn.Linear) and m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.LayerNorm):
            nn.init.constant_(m.bias, 0)
            nn.init.constant_(m.weight, 1.0)

    def get_num_layers(self):
        return len(self.blocks)

    @torch.jit.ignore
    def no_weight_decay(self):
        return {'pos_embed', 'cls_token'}

    def get_classifier(self):
        return self.head

    def reset_classifier(self, num_classes, global_pool=''):
        self.num_classes = num_classes
        self.head = nn.Linear(  self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()
        self.head2 = nn.Linear(  self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()

    def forward_features(self, x):
        B = x.size(0)

        x = self.patch_embed(x)

        if self.pos_embed is not None:
            x = x + self.pos_embed.expand(B, -1, -1).type_as(x).to(
                x.device).clone().detach()
        x = self.pos_drop(x)

        for blk in self.blocks:
            if self.with_cp:
                x = cp.checkpoint(blk, x)
            else:
                x = blk(x)

        if self.pred_each_frame:
            return x.transpose(1, 2).view(-1, 768, 8, 14, 14).mean(-1).mean(-1)


        if self.fc_norm is not None:
            return self.fc_norm(x.mean(1))
        else:
            return self.norm(x[:, 0])

    def forward(self, x):
        x = self.forward_features(x)
        x = self.head_dropout(x)
        #breakpoint()
        # Reshape x for batched matrix multiplication
        #x = x.transpose(1, 2)  # Shape [B, 8, 768]
        x = self.head(x)

        # Multiply using the linear layer's weights (and add bias if it exists)
        #x1 = torch.einsum('bjk,kj->bk', x, self.head.weight) + self.head.bias
        #x2 = torch.einsum('bjk,kj->bk', x, self.head2.weight) + self.head2.bias

        #x = torch.stack((x1, x2), dim=2).view(x.size(0), -1)
       

        return x


@register_model
def vit_small_patch16_224(pretrained=False, **kwargs):
    model = VisionTransformer(
        patch_size=16,
        embed_dim=384,
        depth=12,
        num_heads=6,
        mlp_ratio=4,
        qkv_bias=True,
        norm_layer=partial(nn.LayerNorm, eps=1e-6),
        **kwargs)
    model.default_cfg = _cfg()
    return model


@register_model
def vit_base_patch16_224(pretrained=False, **kwargs):
    model = VisionTransformer(
        patch_size=16,
        embed_dim=768,
        depth=12,
        num_heads=12,
        mlp_ratio=4,
        qkv_bias=True,
        norm_layer=partial(nn.LayerNorm, eps=1e-6),
        **kwargs)
    model.default_cfg = _cfg()
    return model


@register_model
def vit_large_patch16_224(pretrained=False, **kwargs):
    model = VisionTransformer(
        patch_size=16,
        embed_dim=1024,
        depth=24,
        num_heads=16,
        mlp_ratio=4,
        qkv_bias=True,
        norm_layer=partial(nn.LayerNorm, eps=1e-6),
        **kwargs)
    model.default_cfg = _cfg()
    return model


@register_model
def vit_huge_patch16_224(pretrained=False, **kwargs):
    model = VisionTransformer(
        patch_size=16,
        embed_dim=1280,
        depth=32,
        num_heads=16,
        mlp_ratio=4,
        qkv_bias=True,
        norm_layer=partial(nn.LayerNorm, eps=1e-6),
        **kwargs)
    model.default_cfg = _cfg()
    return model


@register_model
def vit_giant_patch14_224(pretrained=False, **kwargs):
    model = VisionTransformer(
        patch_size=14,
        embed_dim=1408,
        depth=40,
        num_heads=16,
        mlp_ratio=48 / 11,
        qkv_bias=True,
        norm_layer=partial(nn.LayerNorm, eps=1e-6),
        **kwargs)
    model.default_cfg = _cfg()
    return model

```

`deepcheat/VideoMAEv2/models/modeling_pretrain.py`:

```py
# --------------------------------------------------------
# Based on BEiT, timm, DINO and DeiT code bases
# https://github.com/microsoft/unilm/tree/master/beit
# https://github.com/rwightman/pytorch-image-models/tree/master/timm
# https://github.com/facebookresearch/deit
# https://github.com/facebookresearch/dino
# --------------------------------------------------------'
from functools import partial

import torch
import torch.nn as nn
import torch.utils.checkpoint as cp
from timm.models.layers import trunc_normal_ as __call_trunc_normal_
from timm.models.registry import register_model

from .modeling_finetune import (
    Block,
    PatchEmbed,
    _cfg,
    get_sinusoid_encoding_table,
)


def trunc_normal_(tensor, mean=0., std=1.):
    __call_trunc_normal_(tensor, mean=mean, std=std, a=-std, b=std)


class PretrainVisionTransformerEncoder(nn.Module):
    """ Vision Transformer with support for patch or hybrid CNN input stage
    """

    def __init__(self,
                 img_size=224,
                 patch_size=16,
                 in_chans=3,
                 num_classes=0,
                 embed_dim=768,
                 depth=12,
                 num_heads=12,
                 mlp_ratio=4.,
                 qkv_bias=False,
                 qk_scale=None,
                 drop_rate=0.,
                 attn_drop_rate=0.,
                 drop_path_rate=0.,
                 norm_layer=nn.LayerNorm,
                 init_values=None,
                 tubelet_size=2,
                 use_learnable_pos_emb=False,
                 with_cp=False,
                 all_frames=16,
                 cos_attn=False):
        super().__init__()
        self.num_classes = num_classes
        # num_features for consistency with other models
        self.num_features = self.embed_dim = embed_dim
        self.patch_embed = PatchEmbed(
            img_size=img_size,
            patch_size=patch_size,
            in_chans=in_chans,
            embed_dim=embed_dim,
            num_frames=all_frames,
            tubelet_size=tubelet_size)
        num_patches = self.patch_embed.num_patches
        self.with_cp = with_cp

        if use_learnable_pos_emb:
            self.pos_embed = nn.Parameter(
                torch.zeros(1, num_patches + 1, embed_dim))
        else:
            # sine-cosine positional embeddings
            self.pos_embed = get_sinusoid_encoding_table(
                num_patches, embed_dim)

        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)
               ]  # stochastic depth decay rule
        self.blocks = nn.ModuleList([
            Block(
                dim=embed_dim,
                num_heads=num_heads,
                mlp_ratio=mlp_ratio,
                qkv_bias=qkv_bias,
                qk_scale=qk_scale,
                drop=drop_rate,
                attn_drop=attn_drop_rate,
                drop_path=dpr[i],
                norm_layer=norm_layer,
                init_values=init_values,
                cos_attn=cos_attn) for i in range(depth)
        ])
        self.norm = norm_layer(embed_dim)
        self.head = nn.Linear(
            embed_dim, num_classes) if num_classes > 0 else nn.Identity()

        if use_learnable_pos_emb:
            trunc_normal_(self.pos_embed, std=.02)

        self.apply(self._init_weights)

    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            nn.init.xavier_uniform_(m.weight)
            if isinstance(m, nn.Linear) and m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.LayerNorm):
            nn.init.constant_(m.bias, 0)
            nn.init.constant_(m.weight, 1.0)

    def get_num_layers(self):
        return len(self.blocks)

    @torch.jit.ignore
    def no_weight_decay(self):
        return {'pos_embed', 'cls_token'}

    def get_classifier(self):
        return self.head

    def reset_classifier(self, num_classes, global_pool=''):
        self.num_classes = num_classes
        self.head = nn.Linear(
            self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()

    def forward_features(self, x, mask):
        x = self.patch_embed(x)

        x = x + self.pos_embed.type_as(x).to(x.device).clone().detach()

        B, _, C = x.shape
        x_vis = x[~mask].reshape(B, -1, C)  # ~mask means visible

        for blk in self.blocks:
            if self.with_cp:
                x_vis = cp.checkpoint(blk, x_vis)
            else:
                x_vis = blk(x_vis)

        x_vis = self.norm(x_vis)
        return x_vis

    def forward(self, x, mask):
        x = self.forward_features(x, mask)
        x = self.head(x)
        return x


class PretrainVisionTransformerDecoder(nn.Module):
    """ Vision Transformer with support for patch or hybrid CNN input stage
    """

    def __init__(self,
                 patch_size=16,
                 num_classes=768,
                 embed_dim=768,
                 depth=12,
                 num_heads=12,
                 mlp_ratio=4.,
                 qkv_bias=False,
                 qk_scale=None,
                 drop_rate=0.,
                 attn_drop_rate=0.,
                 drop_path_rate=0.,
                 norm_layer=nn.LayerNorm,
                 init_values=None,
                 num_patches=196,
                 tubelet_size=2,
                 with_cp=False,
                 cos_attn=False):
        super().__init__()
        self.num_classes = num_classes
        assert num_classes == 3 * tubelet_size * patch_size**2
        # num_features for consistency with other models
        self.num_features = self.embed_dim = embed_dim
        self.patch_size = patch_size
        self.with_cp = with_cp

        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)
               ]  # stochastic depth decay rule
        self.blocks = nn.ModuleList([
            Block(
                dim=embed_dim,
                num_heads=num_heads,
                mlp_ratio=mlp_ratio,
                qkv_bias=qkv_bias,
                qk_scale=qk_scale,
                drop=drop_rate,
                attn_drop=attn_drop_rate,
                drop_path=dpr[i],
                norm_layer=norm_layer,
                init_values=init_values,
                cos_attn=cos_attn) for i in range(depth)
        ])
        self.norm = norm_layer(embed_dim)
        self.head = nn.Linear(
            embed_dim, num_classes) if num_classes > 0 else nn.Identity()

        self.apply(self._init_weights)

    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            nn.init.xavier_uniform_(m.weight)
            if isinstance(m, nn.Linear) and m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.LayerNorm):
            nn.init.constant_(m.bias, 0)
            nn.init.constant_(m.weight, 1.0)

    def get_num_layers(self):
        return len(self.blocks)

    @torch.jit.ignore
    def no_weight_decay(self):
        return {'pos_embed', 'cls_token'}

    def get_classifier(self):
        return self.head

    def reset_classifier(self, num_classes, global_pool=''):
        self.num_classes = num_classes
        self.head = nn.Linear(
            self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()

    def forward(self, x, return_token_num):
        for blk in self.blocks:
            if self.with_cp:
                x = cp.checkpoint(blk, x)
            else:
                x = blk(x)

        if return_token_num > 0:
            # only return the mask tokens predict pixels
            x = self.head(self.norm(x[:, -return_token_num:]))
        else:
            # [B, N, 3*16^2]
            x = self.head(self.norm(x))
        return x


class PretrainVisionTransformer(nn.Module):
    """ Vision Transformer with support for patch or hybrid CNN input stage
    """

    def __init__(
        self,
        img_size=224,
        patch_size=16,
        encoder_in_chans=3,
        encoder_num_classes=0,
        encoder_embed_dim=768,
        encoder_depth=12,
        encoder_num_heads=12,
        decoder_num_classes=1536,  # decoder_num_classes=768
        decoder_embed_dim=512,
        decoder_depth=8,
        decoder_num_heads=8,
        mlp_ratio=4.,
        qkv_bias=False,
        qk_scale=None,
        drop_rate=0.,
        attn_drop_rate=0.,
        drop_path_rate=0.,
        norm_layer=nn.LayerNorm,
        init_values=0.,
        use_learnable_pos_emb=False,
        tubelet_size=2,
        num_classes=0,  # avoid the error from create_fn in timm
        in_chans=0,  # avoid the error from create_fn in timm
        with_cp=False,
        all_frames=16,
        cos_attn=False,
    ):
        super().__init__()
        self.encoder = PretrainVisionTransformerEncoder(
            img_size=img_size,
            patch_size=patch_size,
            in_chans=encoder_in_chans,
            num_classes=encoder_num_classes,
            embed_dim=encoder_embed_dim,
            depth=encoder_depth,
            num_heads=encoder_num_heads,
            mlp_ratio=mlp_ratio,
            qkv_bias=qkv_bias,
            qk_scale=qk_scale,
            drop_rate=drop_rate,
            attn_drop_rate=attn_drop_rate,
            drop_path_rate=drop_path_rate,
            norm_layer=norm_layer,
            init_values=init_values,
            tubelet_size=tubelet_size,
            use_learnable_pos_emb=use_learnable_pos_emb,
            with_cp=with_cp,
            all_frames=all_frames,
            cos_attn=cos_attn)

        self.decoder = PretrainVisionTransformerDecoder(
            patch_size=patch_size,
            num_patches=self.encoder.patch_embed.num_patches,
            num_classes=decoder_num_classes,
            embed_dim=decoder_embed_dim,
            depth=decoder_depth,
            num_heads=decoder_num_heads,
            mlp_ratio=mlp_ratio,
            qkv_bias=qkv_bias,
            qk_scale=qk_scale,
            drop_rate=drop_rate,
            attn_drop_rate=attn_drop_rate,
            drop_path_rate=drop_path_rate,
            norm_layer=norm_layer,
            init_values=init_values,
            tubelet_size=tubelet_size,
            with_cp=with_cp,
            cos_attn=cos_attn)

        self.encoder_to_decoder = nn.Linear(
            encoder_embed_dim, decoder_embed_dim, bias=False)

        self.mask_token = nn.Parameter(torch.zeros(1, 1, decoder_embed_dim))

        self.pos_embed = get_sinusoid_encoding_table(
            self.encoder.patch_embed.num_patches, decoder_embed_dim)

        trunc_normal_(self.mask_token, std=.02)

    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            nn.init.xavier_uniform_(m.weight)
            if isinstance(m, nn.Linear) and m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.LayerNorm):
            nn.init.constant_(m.bias, 0)
            nn.init.constant_(m.weight, 1.0)

    def get_num_layers(self):
        return len(self.blocks)

    @torch.jit.ignore
    def no_weight_decay(self):
        return {'pos_embed', 'cls_token', 'mask_token'}

    def forward(self, x, mask, decode_mask=None):
        decode_vis = mask if decode_mask is None else ~decode_mask

        x_vis = self.encoder(x, mask)  # [B, N_vis, C_e]
        x_vis = self.encoder_to_decoder(x_vis)  # [B, N_vis, C_d]
        B, N_vis, C = x_vis.shape

        # we don't unshuffle the correct visible token order,
        # but shuffle the pos embedding accorddingly.
        expand_pos_embed = self.pos_embed.expand(B, -1, -1).type_as(x).to(
            x.device).clone().detach()
        pos_emd_vis = expand_pos_embed[~mask].reshape(B, -1, C)
        pos_emd_mask = expand_pos_embed[decode_vis].reshape(B, -1, C)

        # [B, N, C_d]
        x_full = torch.cat(
            [x_vis + pos_emd_vis, self.mask_token + pos_emd_mask], dim=1)
        # NOTE: if N_mask==0, the shape of x is [B, N_mask, 3 * 16 * 16]
        x = self.decoder(x_full, pos_emd_mask.shape[1])

        return x


@register_model
def pretrain_videomae_small_patch16_224(pretrained=False, **kwargs):
    model = PretrainVisionTransformer(
        img_size=224,
        patch_size=16,
        encoder_embed_dim=384,
        encoder_depth=12,
        encoder_num_heads=6,
        encoder_num_classes=0,
        decoder_num_classes=1536,  # 16 * 16 * 3 * 2
        decoder_embed_dim=192,
        decoder_num_heads=3,
        mlp_ratio=4,
        qkv_bias=True,
        norm_layer=partial(nn.LayerNorm, eps=1e-6),
        **kwargs)
    model.default_cfg = _cfg()
    if pretrained:
        checkpoint = torch.load(kwargs["init_ckpt"], map_location="cpu")
        model.load_state_dict(checkpoint["model"])
    return model


@register_model
def pretrain_videomae_base_patch16_224(pretrained=False, **kwargs):
    model = PretrainVisionTransformer(
        img_size=224,
        patch_size=16,
        encoder_embed_dim=768,
        encoder_depth=12,
        encoder_num_heads=12,
        encoder_num_classes=0,
        decoder_num_classes=1536,  # 16 * 16 * 3 * 2
        decoder_embed_dim=384,
        decoder_num_heads=6,
        mlp_ratio=4,
        qkv_bias=True,
        norm_layer=partial(nn.LayerNorm, eps=1e-6),
        **kwargs)
    model.default_cfg = _cfg()
    if pretrained:
        checkpoint = torch.load(kwargs["init_ckpt"], map_location="cpu")
        model.load_state_dict(checkpoint["model"])
    return model


@register_model
def pretrain_videomae_large_patch16_224(pretrained=False, **kwargs):
    model = PretrainVisionTransformer(
        img_size=224,
        patch_size=16,
        encoder_embed_dim=1024,
        encoder_depth=24,
        encoder_num_heads=16,
        encoder_num_classes=0,
        decoder_num_classes=1536,  # 16 * 16 * 3 * 2
        decoder_embed_dim=512,
        decoder_num_heads=8,
        mlp_ratio=4,
        qkv_bias=True,
        norm_layer=partial(nn.LayerNorm, eps=1e-6),
        **kwargs)
    model.default_cfg = _cfg()
    if pretrained:
        checkpoint = torch.load(kwargs["init_ckpt"], map_location="cpu")
        model.load_state_dict(checkpoint["model"])
    return model


@register_model
def pretrain_videomae_huge_patch16_224(pretrained=False, **kwargs):
    model = PretrainVisionTransformer(
        img_size=224,
        patch_size=16,
        encoder_embed_dim=1280,
        encoder_depth=32,
        encoder_num_heads=16,
        encoder_num_classes=0,
        decoder_num_classes=1536,  # 16 * 16 * 3 * 2
        decoder_embed_dim=512,
        decoder_num_heads=8,
        mlp_ratio=4,
        qkv_bias=True,
        norm_layer=partial(nn.LayerNorm, eps=1e-6),
        **kwargs)
    model.default_cfg = _cfg()
    if pretrained:
        checkpoint = torch.load(kwargs["init_ckpt"], map_location="cpu")
        model.load_state_dict(checkpoint["model"])
    return model


@register_model
def pretrain_videomae_giant_patch14_224(pretrained=False, **kwargs):
    model = PretrainVisionTransformer(
        img_size=224,
        patch_size=14,
        encoder_embed_dim=1408,
        encoder_depth=40,
        encoder_num_heads=16,
        encoder_num_classes=0,
        decoder_num_classes=1176,  # 14 * 14 * 3 * 2,
        decoder_embed_dim=512,
        decoder_num_heads=8,
        mlp_ratio=48 / 11,
        qkv_bias=True,
        norm_layer=partial(nn.LayerNorm, eps=1e-6),
        **kwargs)
    model.default_cfg = _cfg()
    if pretrained:
        checkpoint = torch.load(kwargs["init_ckpt"], map_location="cpu")
        model.load_state_dict(checkpoint["model"])
    return model

```

`deepcheat/VideoMAEv2/optim_factory.py`:

```py
# --------------------------------------------------------
# Based on BEiT, timm, DINO and DeiT code bases
# https://github.com/microsoft/unilm/tree/master/beit
# https://github.com/rwightman/pytorch-image-models/tree/master/timm
# https://github.com/facebookresearch/deit
# https://github.com/facebookresearch/dino
# --------------------------------------------------------'
import json

import torch
from timm.optim.adafactor import Adafactor
from timm.optim.adahessian import Adahessian
from timm.optim.adamp import AdamP
from timm.optim.lookahead import Lookahead
from timm.optim.nadam import Nadam
from timm.optim.novograd import NovoGrad
from timm.optim.nvnovograd import NvNovoGrad
from timm.optim.radam import RAdam
from timm.optim.rmsprop_tf import RMSpropTF
from timm.optim.sgdp import SGDP
from torch import optim as optim

try:
    from apex.optimizers import FusedAdam, FusedLAMB, FusedNovoGrad, FusedSGD
    has_apex = True
except ImportError:
    has_apex = False


def get_num_layer_for_vit(var_name, num_max_layer):
    if var_name in ("cls_token", "mask_token", "pos_embed"):
        return 0
    elif var_name.startswith("patch_embed"):
        return 0
    elif var_name.startswith("rel_pos_bias"):
        return num_max_layer - 1
    elif var_name.startswith("blocks"):
        layer_id = int(var_name.split('.')[1])
        return layer_id + 1
    else:
        return num_max_layer - 1


class LayerDecayValueAssigner(object):

    def __init__(self, values):
        self.values = values

    def get_scale(self, layer_id):
        return self.values[layer_id]

    def get_layer_id(self, var_name):
        return get_num_layer_for_vit(var_name, len(self.values))


def get_parameter_groups(model,
                         weight_decay=1e-5,
                         skip_list=(),
                         get_num_layer=None,
                         get_layer_scale=None):
    parameter_group_names = {}
    parameter_group_vars = {}

    for name, param in model.named_parameters():
        if not param.requires_grad:
            continue  # frozen weights
        if len(param.shape) == 1 or name.endswith(".bias") or name.endswith(
                ".scale") or name in skip_list:
            group_name = "no_decay"
            this_weight_decay = 0.
        else:
            group_name = "decay"
            this_weight_decay = weight_decay
        if get_num_layer is not None:
            layer_id = get_num_layer(name)
            group_name = "layer_%d_%s" % (layer_id, group_name)
        else:
            layer_id = None

        if group_name not in parameter_group_names:
            if get_layer_scale is not None:
                scale = get_layer_scale(layer_id)
            else:
                scale = 1.

            parameter_group_names[group_name] = {
                "weight_decay": this_weight_decay,
                "params": [],
                "lr_scale": scale
            }
            parameter_group_vars[group_name] = {
                "weight_decay": this_weight_decay,
                "params": [],
                "lr_scale": scale
            }

        parameter_group_vars[group_name]["params"].append(param)
        parameter_group_names[group_name]["params"].append(name)
    print("Param groups = %s" % json.dumps(parameter_group_names, indent=2))
    return list(parameter_group_vars.values())

def freeze_layers(model, layers_to_freeze):
    """
    Freeze specified layers of the model by setting their requires_grad property to False.
    """
    for name, param in model.named_parameters():
        if any(layer_name in name for layer_name in layers_to_freeze):
            param.requires_grad = False

def unfreeze_specific_layer(model, block_idx):
    """
    Unfreeze a specific block in the model.
    """
    for param in model.blocks[block_idx].parameters():
        param.requires_grad = True


def create_optimizer(args,
                     model,
                     get_num_layer=None,
                     get_layer_scale=None,
                     filter_bias_and_bn=True,
                     skip_list=None):

    freeze_layers(model, ['blocks'])
    #unfreeze_specific_layer(model, 8)
    #unfreeze_specific_layer(model, 9)
    #unfreeze_specific_layer(model, 10)
    

    for name, param in model.named_parameters():
        param.requires_grad = False
        if "head.weight" in name or "head.bias" in name:
            param.requires_grad = True
        if "head2.weight" in name or "head2.bias" in name:
            param.requires_grad = True
        if "fc_norm" in name:
            param.requires_grad = True


    unfreeze_specific_layer(model, -1)
    unfreeze_specific_layer(model, -2)
    unfreeze_specific_layer(model, -3)


    opt_lower = args.opt.lower()
    weight_decay = args.weight_decay
    if weight_decay and filter_bias_and_bn:
        skip = {}
        if skip_list is not None:
            skip = skip_list
        elif hasattr(model, 'no_weight_decay'):
            skip = model.no_weight_decay()
        parameters = get_parameter_groups(model, weight_decay, skip,
                                          get_num_layer, get_layer_scale)
        weight_decay = 0.
    else:
        parameters = model.parameters()

    #breakpoint()
    #parameters = (param for param in [
    #    model.head.weight,
    #    model.head.bias,
    #    *model.blocks[-1].parameters()
    #])


    if 'fused' in opt_lower:
        assert has_apex and torch.cuda.is_available(
        ), 'APEX and CUDA required for fused optimizers'

    opt_args = dict(lr=args.lr, weight_decay=weight_decay)
    if hasattr(args, 'opt_eps') and args.opt_eps is not None:
        opt_args['eps'] = args.opt_eps
    if hasattr(args, 'opt_betas') and args.opt_betas is not None:
        opt_args['betas'] = args.opt_betas

    print("optimizer settings:", opt_args)

    opt_split = opt_lower.split('_')
    opt_lower = opt_split[-1]
    if opt_lower == 'sgd' or opt_lower == 'nesterov':
        opt_args.pop('eps', None)
        optimizer = optim.SGD(
            parameters, momentum=args.momentum, nesterov=True, **opt_args)
    elif opt_lower == 'momentum':
        opt_args.pop('eps', None)
        optimizer = optim.SGD(
            parameters, momentum=args.momentum, nesterov=False, **opt_args)
    elif opt_lower == 'adam':
        optimizer = optim.Adam(parameters, **opt_args)
    elif opt_lower == 'adamw':
        optimizer = optim.AdamW(parameters, **opt_args)
    elif opt_lower == 'nadam':
        optimizer = Nadam(parameters, **opt_args)
    elif opt_lower == 'radam':
        optimizer = RAdam(parameters, **opt_args)
    elif opt_lower == 'adamp':
        optimizer = AdamP(parameters, wd_ratio=0.01, nesterov=True, **opt_args)
    elif opt_lower == 'sgdp':
        optimizer = SGDP(
            parameters, momentum=args.momentum, nesterov=True, **opt_args)
    elif opt_lower == 'adadelta':
        optimizer = optim.Adadelta(parameters, **opt_args)
    elif opt_lower == 'adafactor':
        if not args.lr:
            opt_args['lr'] = None
        optimizer = Adafactor(parameters, **opt_args)
    elif opt_lower == 'adahessian':
        optimizer = Adahessian(parameters, **opt_args)
    elif opt_lower == 'rmsprop':
        optimizer = optim.RMSprop(
            parameters, alpha=0.9, momentum=args.momentum, **opt_args)
    elif opt_lower == 'rmsproptf':
        optimizer = RMSpropTF(
            parameters, alpha=0.9, momentum=args.momentum, **opt_args)
    elif opt_lower == 'novograd':
        optimizer = NovoGrad(parameters, **opt_args)
    elif opt_lower == 'nvnovograd':
        optimizer = NvNovoGrad(parameters, **opt_args)
    elif opt_lower == 'fusedsgd':
        opt_args.pop('eps', None)
        optimizer = FusedSGD(
            parameters, momentum=args.momentum, nesterov=True, **opt_args)
    elif opt_lower == 'fusedmomentum':
        opt_args.pop('eps', None)
        optimizer = FusedSGD(
            parameters, momentum=args.momentum, nesterov=False, **opt_args)
    elif opt_lower == 'fusedadam':
        optimizer = FusedAdam(parameters, adam_w_mode=False, **opt_args)
    elif opt_lower == 'fusedadamw':
        optimizer = FusedAdam(parameters, adam_w_mode=True, **opt_args)
    elif opt_lower == 'fusedlamb':
        optimizer = FusedLAMB(parameters, **opt_args)
    elif opt_lower == 'fusednovograd':
        opt_args.setdefault('betas', (0.95, 0.98))
        optimizer = FusedNovoGrad(parameters, **opt_args)
    else:
        assert False and "Invalid optimizer"
        raise ValueError

    if len(opt_split) > 1:
        if opt_split[0] == 'lookahead':
            optimizer = Lookahead(optimizer)

    return optimizer

```

`deepcheat/VideoMAEv2/train_cheater_pred.py`:

```py
# --------------------------------------------------------
# Based on BEiT, timm, DINO and DeiT code bases
# https://github.com/microsoft/unilm/tree/master/beit
# https://github.com/rwightman/pytorch-image-models/tree/master/timm
# https://github.com/facebookresearch/deit
# https://github.com/facebookresearch/dino
# --------------------------------------------------------'

import argparse
import datetime
import json
import os
import random
import time
from collections import OrderedDict
from functools import partial
from pathlib import Path

import deepspeed
import numpy as np
import torch
import torch.backends.cudnn as cudnn
from timm.data.mixup import Mixup
from timm.models import create_model
from timm.utils import ModelEma

# NOTE: Do not comment `import models`, it is used to register models
import models  # noqa: F401
import utils
from dataset import build_dataset
from engine_for_finetuning import (
    final_test_killshot,
    predict_killshot_scores,
    merge,
    train_one_epoch,
    validation_one_epoch,
)
from optim_factory import (
    LayerDecayValueAssigner,
    create_optimizer,
    get_parameter_groups,
)
from utils import NativeScalerWithGradNormCount as NativeScaler
from utils import multiple_samples_collate


import torch
import torch.nn as nn
import torch.nn.functional as F


import os
import torch
from torchvision import transforms
from PIL import Image
from collections import deque


class BinaryLabelSmoothingCrossEntropy(nn.Module):
    def __init__(self, smoothing=0.1):
        super(BinaryLabelSmoothingCrossEntropy, self).__init__()
        assert 0 < smoothing < 1.0
        self.smoothing = smoothing
        self.confidence = 1. - smoothing

    def forward(self, x: torch.Tensor, target: torch.Tensor) -> torch.Tensor:
        logprobs = torch.sigmoid(x)

        # Modified negative log likelihood computation
        nll_loss = -(target * torch.log(logprobs) + (1 - target) * torch.log(1 - logprobs))
        
        # Modified smooth loss
        smooth_loss = -logprobs.mul(self.smoothing) - (1 - logprobs).mul(1 - self.smoothing)
        
        loss = self.confidence * nll_loss + self.smoothing * smooth_loss

        return loss.mean()

class BinarySoftTargetCrossEntropy(nn.Module):
    def __init__(self):
        super(BinarySoftTargetCrossEntropy, self).__init__()

    def forward(self, x: torch.Tensor, target: torch.Tensor) -> torch.Tensor:
        logprobs = torch.sigmoid(x)
        loss = -(target * torch.log(logprobs) + (1 - target) * torch.log(1 - logprobs))
        return loss.mean()



def get_args():
    parser = argparse.ArgumentParser(
        'VideoMAE fine-tuning and evaluation script for action classification',
        add_help=False)
    parser.add_argument('--batch_size', default=64, type=int)
    parser.add_argument('--epochs', default=30, type=int)
    parser.add_argument('--update_freq', default=1, type=int)
    parser.add_argument('--save_ckpt_freq', default=100, type=int)

    # Model parameters
    parser.add_argument(
        '--model',
        default='vit_giant_patch14_224 ',
        type=str,
        metavar='MODEL',
        help='Name of model to train')
    parser.add_argument('--tubelet_size', type=int, default=2)
    parser.add_argument(
        '--input_size', default=224, type=int, help='images input size')

    parser.add_argument(
        '--with_checkpoint', action='store_true', default=False)

    parser.add_argument(
        '--drop',
        type=float,
        default=0.0,
        metavar='PCT',
        help='Dropout rate (default: 0.)')
    parser.add_argument(
        '--attn_drop_rate',
        type=float,
        default=0.0,
        metavar='PCT',
        help='Attention dropout rate (default: 0.)')
    parser.add_argument(
        '--drop_path',
        type=float,
        default=0.1,
        metavar='PCT',
        help='Drop path rate (default: 0.1)')
    parser.add_argument(
        '--head_drop_rate',
        type=float,
        default=0.0,
        metavar='PCT',
        help='cls head dropout rate (default: 0.)')

    parser.add_argument(
        '--disable_eval_during_finetuning', action='store_true', default=False)

    parser.add_argument('--model_ema', action='store_true', default=False)
    parser.add_argument(
        '--model_ema_decay', type=float, default=0.9999, help='')
    parser.add_argument(
        '--model_ema_force_cpu', action='store_true', default=False, help='')

    # Optimizer parameters
    parser.add_argument(
        '--opt',
        default='adamw',
        type=str,
        metavar='OPTIMIZER',
        help='Optimizer (default: "adamw"')
    parser.add_argument(
        '--opt_eps',
        default=1e-8,
        type=float,
        metavar='EPSILON',
        help='Optimizer Epsilon (default: 1e-8)')
    parser.add_argument(
        '--opt_betas',
        default=None,
        type=float,
        nargs='+',
        metavar='BETA',
        help='Optimizer Betas (default: None, use opt default)')
    parser.add_argument(
        '--clip_grad',
        type=float,
        default=None,
        metavar='NORM',
        help='Clip gradient norm (default: None, no clipping)')
    parser.add_argument(
        '--momentum',
        type=float,
        default=0.9,
        metavar='M',
        help='SGD momentum (default: 0.9)')
    parser.add_argument(
        '--weight_decay',
        type=float,
        default=0.05,
        help='weight decay (default: 0.05)')
    parser.add_argument(
        '--weight_decay_end',
        type=float,
        default=None,
        help="""Final value of the
        weight decay. We use a cosine schedule for WD and using a larger decay by
        the end of training improves performance for ViTs.""")

    parser.add_argument(
        '--lr',
        type=float,
        default=1e-3,
        metavar='LR',
        help='learning rate (default: 1e-3)')
    parser.add_argument('--layer_decay', type=float, default=0.75)
    parser.add_argument('--min_eval_score', type=float, default=None)
    parser.add_argument('--max_eval_score', type=float, default=None)

    parser.add_argument(
        '--warmup_lr',
        type=float,
        default=1e-8,
        metavar='LR',
        help='warmup learning rate (default: 1e-6)')
    parser.add_argument(
        '--min_lr',
        type=float,
        default=1e-6,
        metavar='LR',
        help='lower lr bound for cyclic schedulers that hit 0 (1e-5)')

    parser.add_argument(
        '--warmup_epochs',
        type=int,
        default=5,
        metavar='N',
        help='epochs to warmup LR, if scheduler supports')
    parser.add_argument(
        '--warmup_steps',
        type=int,
        default=-1,
        metavar='N',
        help='num of steps to warmup LR, will overload warmup_epochs if set > 0'
    )

    # Augmentation parameters
    parser.add_argument(
        '--color_jitter',
        type=float,
        default=0.4,
        metavar='PCT',
        help='Color jitter factor (default: 0.4)')
    parser.add_argument(
        '--num_sample', type=int, default=2, help='Repeated_aug (default: 2)')
    parser.add_argument(
        '--aa',
        type=str,
        default='rand-m7-n4-mstd0.5-inc1',
        metavar='NAME',
        help=
        'Use AutoAugment policy. "v0" or "original". " + "(default: rand-m7-n4-mstd0.5-inc1)'
    ),
    parser.add_argument(
        '--smoothing',
        type=float,
        default=0.1,
        help='Label smoothing (default: 0.1)')
    parser.add_argument(
        '--train_interpolation',
        type=str,
        default='bicubic',
        help=
        'Training interpolation (random, bilinear, bicubic default: "bicubic")'
    )

    # Evaluation parameters
    parser.add_argument('--crop_pct', type=float, default=None)
    parser.add_argument('--short_side_size', type=int, default=224)
    parser.add_argument('--test_num_segment', type=int, default=1)
    parser.add_argument('--test_num_crop', type=int, default=1)

    # * Random Erase params
    parser.add_argument(
        '--reprob',
        type=float,
        default=0.25,
        metavar='PCT',
        help='Random erase prob (default: 0.25)')
    parser.add_argument(
        '--remode',
        type=str,
        default='pixel',
        help='Random erase mode (default: "pixel")')
    parser.add_argument(
        '--recount',
        type=int,
        default=1,
        help='Random erase count (default: 1)')
    parser.add_argument(
        '--resplit',
        action='store_true',
        default=False,
        help='Do not random erase first (clean) augmentation split')

    # * Mixup params
    parser.add_argument(
        '--mixup',
        type=float,
        default=0.0,
        help='mixup alpha, mixup enabled if > 0.')
    parser.add_argument(
        '--cutmix',
        type=float,
        default=1.0,
        help='cutmix alpha, cutmix enabled if > 0.')
    parser.add_argument(
        '--cutmix_minmax',
        type=float,
        nargs='+',
        default=None,
        help='cutmix min/max ratio, overrides alpha and enables cutmix if set')
    parser.add_argument(
        '--mixup_prob',
        type=float,
        default=1.0,
        help=
        'Probability of performing mixup or cutmix when either/both is enabled'
    )
    parser.add_argument(
        '--mixup_switch_prob',
        type=float,
        default=0.5,
        help=
        'Probability of switching to cutmix when both mixup and cutmix enabled'
    )
    parser.add_argument(
        '--mixup_mode',
        type=str,
        default='batch',
        help='How to apply mixup/cutmix params. Per "batch", "pair", or "elem"'
    )

    # * Finetuning params
    parser.add_argument(
        '--finetune', default='', help='finetune from checkpoint')
    parser.add_argument('--model_key', default='model|module', type=str)
    parser.add_argument('--model_prefix', default='', type=str)
    parser.add_argument('--init_scale', default=0.001, type=float)
    parser.add_argument('--use_mean_pooling', action='store_true')
    parser.set_defaults(use_mean_pooling=True)
    parser.add_argument(
        '--use_cls', action='store_false', dest='use_mean_pooling')

    # Dataset parameters
    parser.add_argument(
        '--data_path',
        default='/your/data/path/',
        type=str,
        help='dataset path')
    parser.add_argument(
        '--data_root', default='', type=str, help='dataset path root')
    parser.add_argument(
        '--eval_data_path',
        default=None,
        type=str,
        help='dataset path for evaluation')
    parser.add_argument(
        '--nb_classes',
        default=400,
        type=int,
        help='number of the classification types')
    parser.add_argument(
        '--imagenet_default_mean_and_std', default=True, action='store_true')
    parser.add_argument('--num_segments', type=int, default=1)
    parser.add_argument('--num_frames', type=int, default=16)
    parser.add_argument('--sampling_rate', type=int, default=4)
    parser.add_argument('--sparse_sample', default=False, action='store_true')
    parser.add_argument(
        '--data_set',
        default='killshot',
        type=str,
        help='dataset')
    parser.add_argument(
        '--pred_video',
        default='',
        type=str,
        help='which video to load and pred')
    parser.add_argument(
        '--fname_tmpl',
        default='img_{:010}.jpg',
        type=str,
        help='filename_tmpl for rawframe dataset')
    parser.add_argument(
        '--start_idx',
        default=1,
        type=int,
        help='start_idx for rwaframe dataset')

    parser.add_argument(
        '--output_dir',
        default='',
        help='path where to save, empty for no saving')
    parser.add_argument(
        '--log_dir', default=None, help='path where to tensorboard log')
    parser.add_argument(
        '--device',
        default='cuda',
        help='device to use for training / testing')
    parser.add_argument('--seed', default=0, type=int)
    parser.add_argument('--resume', default='', help='resume from checkpoint')
    parser.add_argument('--auto_resume', action='store_true')
    parser.add_argument(
        '--no_auto_resume', action='store_false', dest='auto_resume')
    parser.set_defaults(auto_resume=True)

    parser.add_argument('--save_ckpt', action='store_true')
    parser.add_argument(
        '--no_save_ckpt', action='store_false', dest='save_ckpt')
    parser.set_defaults(save_ckpt=True)

    parser.add_argument(
        '--start_epoch', default=0, type=int, metavar='N', help='start epoch')
    parser.add_argument(
        '--eval', action='store_true', help='Perform evaluation only')
    parser.add_argument(
        '--validation', action='store_true', help='Perform validation only')
    parser.add_argument(
        '--dist_eval',
        action='store_true',
        default=False,
        help='Enabling distributed evaluation')
    parser.add_argument('--num_workers', default=10, type=int)
    parser.add_argument(
        '--pin_mem',
        action='store_true',
        help=
        'Pin CPU memory in DataLoader for more efficient (sometimes) transfer to GPU.'
    )
    parser.add_argument('--no_pin_mem', action='store_false', dest='pin_mem')
    parser.set_defaults(pin_mem=True)

    # distributed training parameters
    parser.add_argument(
        '--world_size',
        default=1,
        type=int,
        help='number of distributed processes')
    parser.add_argument('--local_rank', default=-1, type=int)
    parser.add_argument('--dist_on_itp', action='store_true')
    parser.add_argument(
        '--dist_url',
        default='env://',
        help='url used to set up distributed training')

    parser.add_argument(
        '--enable_deepspeed', action='store_true', default=False)

    known_args, _ = parser.parse_known_args()

    if known_args.enable_deepspeed:
        parser = deepspeed.add_config_arguments(parser)
        ds_init = deepspeed.initialize
    else:
        ds_init = None

    return parser.parse_args(), ds_init


def main(args, ds_init):
    utils.init_distributed_mode(args)

    if ds_init is not None:
        utils.create_ds_config(args)

    print(args)

    device = torch.device(args.device)

    # fix the seed for reproducibility
    seed = args.seed + utils.get_rank()
    torch.manual_seed(seed)
    np.random.seed(seed)
    random.seed(seed)
    cudnn.benchmark = True

    #args.nb_classes = 2
    if not args.eval:
        dataset_train, args.nb_classes = build_dataset( is_train=True, test_mode=False, args=args)
    else:
        dataset_train = [0]*100
    dataset_val = None


    dataset_test, _ = build_dataset(is_train=False, test_mode=True, args=args)

    num_tasks = utils.get_world_size()
    global_rank = utils.get_rank()
    if not args.eval:
        sampler_train = torch.utils.data.DistributedSampler( dataset_train, num_replicas=num_tasks, rank=global_rank, shuffle=True)
        print("Sampler_train = %s" % str(sampler_train))
    if args.dist_eval:
        if len(dataset_val) % num_tasks != 0:
            print(
                'Warning: Enabling distributed evaluation with an eval dataset not divisible by process number. '
                'This will slightly alter validation results as extra duplicate entries are added to achieve '
                'equal num of samples per-process.')
        sampler_val = torch.utils.data.DistributedSampler(
            dataset_val,
            num_replicas=num_tasks,
            rank=global_rank,
            shuffle=False)
        sampler_test = torch.utils.data.DistributedSampler(
            dataset_test,
            num_replicas=num_tasks,
            rank=global_rank,
            shuffle=False)
    else:
        sampler_val = torch.utils.data.SequentialSampler(dataset_val)
        sampler_test = torch.utils.data.SequentialSampler(dataset_test)

    if global_rank == 0 and args.log_dir is not None:
        os.makedirs(args.log_dir, exist_ok=True)
        log_writer = utils.TensorboardLogger(log_dir=args.log_dir)
    else:
        log_writer = None

    if args.num_sample > 1:
        collate_func = partial(multiple_samples_collate, fold=False)
    else:
        collate_func = None

    if not args.eval:
        data_loader_train = torch.utils.data.DataLoader(
            dataset_train,
            sampler=sampler_train,
            batch_size=args.batch_size,
            num_workers=args.num_workers,
            pin_memory=args.pin_mem,
            drop_last=True,
            collate_fn=collate_func,
            persistent_workers=True)

    if dataset_val is not None:
        data_loader_val = torch.utils.data.DataLoader(
            dataset_val,
            sampler=sampler_val,
            batch_size=int(1.5 * args.batch_size),
            num_workers=args.num_workers,
            pin_memory=args.pin_mem,
            drop_last=False,
            persistent_workers=True)
    else:
        data_loader_val = None

    if dataset_test is not None:
        data_loader_test = torch.utils.data.DataLoader(
            dataset_test,
            sampler=sampler_test,
            batch_size=args.batch_size,
            num_workers=args.num_workers,
            pin_memory=args.pin_mem,
            drop_last=False,
            persistent_workers=True)
    else:
        data_loader_test = None

    mixup_fn = None
    mixup_active = args.mixup > 0 or args.cutmix > 0. or args.cutmix_minmax is not None
    if mixup_active:
        print("Mixup is activated!")
        mixup_fn = Mixup(
            mixup_alpha=args.mixup,
            cutmix_alpha=args.cutmix,
            cutmix_minmax=args.cutmix_minmax,
            prob=args.mixup_prob,
            switch_prob=args.mixup_switch_prob,
            mode=args.mixup_mode,
            label_smoothing=args.smoothing,
            num_classes=2)

    model = create_model(
        args.model,
        pred_each_frame=False,
        img_size=args.input_size,
        pretrained=False,
        num_classes=args.nb_classes,
        all_frames=args.num_frames * args.num_segments,
        tubelet_size=args.tubelet_size,
        drop_rate=args.drop,
        drop_path_rate=args.drop_path,
        attn_drop_rate=args.attn_drop_rate,
        head_drop_rate=args.head_drop_rate,
        drop_block_rate=None,
        use_mean_pooling=args.use_mean_pooling,
        init_scale=args.init_scale,
        with_cp=args.with_checkpoint,
    )
    #breakpoint()

    patch_size = model.patch_embed.patch_size
    print("Patch size = %s" % str(patch_size))

    args.window_size = (args.num_frames // args.tubelet_size,
                        args.input_size // patch_size[0],
                        args.input_size // patch_size[1])

    args.patch_size = patch_size

    if args.finetune:
        if args.finetune.startswith('https'):
            checkpoint = torch.hub.load_state_dict_from_url(
                args.finetune, map_location='cpu', check_hash=True)
        else:
            checkpoint = torch.load(args.finetune, map_location='cpu')

        print("Load ckpt from %s" % args.finetune)
        checkpoint_model = None
        for model_key in args.model_key.split('|'):
            if model_key in checkpoint:
                checkpoint_model = checkpoint[model_key]
                print("Load state_dict by model_key = %s" % model_key)
                break
        if checkpoint_model is None:
            checkpoint_model = checkpoint
        for old_key in list(checkpoint_model.keys()):
            if old_key.startswith('_orig_mod.'):
                new_key = old_key[10:]
                checkpoint_model[new_key] = checkpoint_model.pop(old_key)

        state_dict = model.state_dict()
        for k in ['head.weight', 'head.bias']:
            if k in checkpoint_model and checkpoint_model[
                    k].shape != state_dict[k].shape:
                if checkpoint_model[k].shape[
                        0] == 710 and args.data_set.startswith('Kinetics'):
                    print(f'Convert K710 head to {args.data_set} head')
                    if args.data_set == 'Kinetics-400':
                        label_map_path = 'misc/label_710to400.json'
                    elif args.data_set == 'Kinetics-600':
                        label_map_path = 'misc/label_710to600.json'
                    elif args.data_set == 'Kinetics-700':
                        label_map_path = 'misc/label_710to700.json'

                    label_map = json.load(open(label_map_path))
                    checkpoint_model[k] = checkpoint_model[k][label_map]
                else:
                    print(f"Removing key {k} from pretrained checkpoint")
                    del checkpoint_model[k]

        all_keys = list(checkpoint_model.keys())
        new_dict = OrderedDict()
        for key in all_keys:
            if key.startswith('backbone.'):
                new_dict[key[9:]] = checkpoint_model[key]
            elif key.startswith('encoder.'):
                new_dict[key[8:]] = checkpoint_model[key]
            else:
                new_dict[key] = checkpoint_model[key]
        checkpoint_model = new_dict

        # interpolate position embedding
        if 'pos_embed' in checkpoint_model:
            pos_embed_checkpoint = checkpoint_model['pos_embed']
            embedding_size = pos_embed_checkpoint.shape[-1]  # channel dim
            num_patches = model.patch_embed.num_patches  #
            num_extra_tokens = model.pos_embed.shape[-2] - num_patches  # 0/1

            # height (== width) for the checkpoint position embedding
            orig_size = int(
                ((pos_embed_checkpoint.shape[-2] - num_extra_tokens) //
                 (args.num_frames // model.patch_embed.tubelet_size))**0.5)
            # height (== width) for the new position embedding
            new_size = int(
                (num_patches //
                 (args.num_frames // model.patch_embed.tubelet_size))**0.5)
            # class_token and dist_token are kept unchanged
            if orig_size != new_size:
                print("Position interpolate from %dx%d to %dx%d" %
                      (orig_size, orig_size, new_size, new_size))
                extra_tokens = pos_embed_checkpoint[:, :num_extra_tokens]
                # only the position tokens are interpolated
                pos_tokens = pos_embed_checkpoint[:, num_extra_tokens:]
                # B, L, C -> BT, H, W, C -> BT, C, H, W
                pos_tokens = pos_tokens.reshape(
                    -1, args.num_frames // model.patch_embed.tubelet_size,
                    orig_size, orig_size, embedding_size)
                pos_tokens = pos_tokens.reshape(-1, orig_size, orig_size,
                                                embedding_size).permute(
                                                    0, 3, 1, 2)
                pos_tokens = torch.nn.functional.interpolate(
                    pos_tokens,
                    size=(new_size, new_size),
                    mode='bicubic',
                    align_corners=False)
                # BT, C, H, W -> BT, H, W, C ->  B, T, H, W, C
                pos_tokens = pos_tokens.permute(0, 2, 3, 1).reshape(
                    -1, args.num_frames // model.patch_embed.tubelet_size,
                    new_size, new_size, embedding_size)
                pos_tokens = pos_tokens.flatten(1, 3)  # B, L, C
                new_pos_embed = torch.cat((extra_tokens, pos_tokens), dim=1)
                checkpoint_model['pos_embed'] = new_pos_embed
        elif args.input_size != 224:
            pos_tokens = model.pos_embed
            org_num_frames = 16
            T = org_num_frames // args.tubelet_size
            P = int((pos_tokens.shape[1] // T)**0.5)
            C = pos_tokens.shape[2]
            new_P = args.input_size // patch_size[0]
            # B, L, C -> BT, H, W, C -> BT, C, H, W
            pos_tokens = pos_tokens.reshape(-1, T, P, P, C)
            pos_tokens = pos_tokens.reshape(-1, P, P, C).permute(0, 3, 1, 2)
            pos_tokens = torch.nn.functional.interpolate(
                pos_tokens,
                size=(new_P, new_P),
                mode='bicubic',
                align_corners=False)
            # BT, C, H, W -> BT, H, W, C ->  B, T, H, W, C
            pos_tokens = pos_tokens.permute(0, 2, 3,
                                            1).reshape(-1, T, new_P, new_P, C)
            pos_tokens = pos_tokens.flatten(1, 3)  # B, L, C
            model.pos_embed = pos_tokens  # update
        if args.num_frames != 16:
            org_num_frames = 16
            T = org_num_frames // args.tubelet_size
            pos_tokens = model.pos_embed
            new_T = args.num_frames // args.tubelet_size
            P = int((pos_tokens.shape[1] // T)**0.5)
            C = pos_tokens.shape[2]
            pos_tokens = pos_tokens.reshape(-1, T, P, P, C)
            pos_tokens = pos_tokens.permute(0, 2, 3, 4,
                                            1).reshape(-1, C, T)  # BHW,C,T
            pos_tokens = torch.nn.functional.interpolate(
                pos_tokens, size=new_T, mode='linear')
            pos_tokens = pos_tokens.reshape(1, P, P, C,
                                            new_T).permute(0, 4, 1, 2, 3)
            pos_tokens = pos_tokens.flatten(1, 3)
            model.pos_embed = pos_tokens  # update

        utils.load_state_dict(
            model, checkpoint_model, prefix=args.model_prefix)

    #breakpoint()
    model.to(device)

    model_ema = None
    if args.model_ema:
        # Important to create EMA model after cuda(), DP wrapper, and AMP but before SyncBN and DDP wrapper
        model_ema = ModelEma(
            model,
            decay=args.model_ema_decay,
            device='cpu' if args.model_ema_force_cpu else '',
            resume='')
        print("Using EMA with decay = %.8f" % args.model_ema_decay)

    model_without_ddp = model
    n_parameters = sum(p.numel() for p in model.parameters()
                       if p.requires_grad)

    print("Model = %s" % str(model_without_ddp))
    print('number of params:', n_parameters)

    total_batch_size = args.batch_size * args.update_freq * num_tasks
    num_training_steps_per_epoch = len(dataset_train) // total_batch_size
    args.lr = args.lr * total_batch_size / 8
    #########scale the lr#############
    args.min_lr = args.min_lr * total_batch_size / 8
    args.warmup_lr = args.warmup_lr * total_batch_size / 8
    #########scale the lr#############
    print("LR = %.8f" % args.lr)
    print("Batch size = %d" % total_batch_size)
    print("Update frequent = %d" % args.update_freq)
    print("Number of training examples = %d" % len(dataset_train))
    print("Number of training training per epoch = %d" %
          num_training_steps_per_epoch)

    num_layers = model_without_ddp.get_num_layers()
    if args.layer_decay < 1.0:
        assigner = LayerDecayValueAssigner(
            list(args.layer_decay**(num_layers + 1 - i)
                 for i in range(num_layers + 2)))
    else:
        assigner = None

    if assigner is not None:
        print("Assigned values = %s" % str(assigner.values))

    skip_weight_decay_list = model.no_weight_decay()
    print("Skip weight decay list: ", skip_weight_decay_list)

    if args.enable_deepspeed:
        loss_scaler = None
        optimizer_params = get_parameter_groups(
            model, args.weight_decay, skip_weight_decay_list,
            assigner.get_layer_id if assigner is not None else None,
            assigner.get_scale if assigner is not None else None)
        model, optimizer, _, _ = ds_init(
            args=args,
            model=model,
            model_parameters=optimizer_params,
            dist_init_required=not args.distributed,
        )

        print("model.gradient_accumulation_steps() = %d" %
              model.gradient_accumulation_steps())
        assert model.gradient_accumulation_steps() == args.update_freq
    else:
        if args.distributed:
            model = torch.nn.parallel.DistributedDataParallel(
                model, device_ids=[args.gpu], find_unused_parameters=False)
            model_without_ddp = model.module

        optimizer = create_optimizer(
            args,
            model_without_ddp,
            skip_list=skip_weight_decay_list,
            get_num_layer=assigner.get_layer_id
            if assigner is not None else None,
            get_layer_scale=assigner.get_scale
            if assigner is not None else None)
        loss_scaler = NativeScaler()

    print("Use step level LR scheduler!")
    lr_schedule_values = utils.cosine_scheduler(
        args.lr,
        args.min_lr,
        args.epochs,
        num_training_steps_per_epoch,
        warmup_epochs=args.warmup_epochs,
        warmup_steps=args.warmup_steps,
    )
    if args.weight_decay_end is None:
        args.weight_decay_end = args.weight_decay
    wd_schedule_values = utils.cosine_scheduler(args.weight_decay,
                                                args.weight_decay_end,
                                                args.epochs,
                                                num_training_steps_per_epoch)
    print("Max WD = %.7f, Min WD = %.7f" %
          (max(wd_schedule_values), min(wd_schedule_values)))

    if mixup_fn is not None:
        # smoothing is handled with mixup label transform
        criterion = BinarySoftTargetCrossEntropy()
    elif args.smoothing > 0.:
        criterion = BinarySmoothingCrossEntropy(smoothing=args.smoothing)
    else:
        criterion = torch.nn.BCEWithLogitsLoss()

    criterion = torch.nn.BCEWithLogitsLoss()
    

    print("criterion = %s" % str(criterion))

    utils.auto_load_model(
        args=args,
        model=model,
        model_without_ddp=model_without_ddp,
        optimizer=optimizer,
        loss_scaler=loss_scaler,
        model_ema=model_ema)
    if args.validation:
        test_stats = validation_one_epoch(data_loader_val, model, device)
        print(
            f"{len(dataset_val)} val images: Top-1 {test_stats['acc1']:.2f}%, Top-5 {test_stats['acc5']:.2f}%, loss {test_stats['loss']:.4f}"
        )
        exit(0)

    if args.eval:
        #breakpoint()
        preds_file = os.path.join(args.output_dir, str(global_rank) + '.txt')
        ret = validation_one_epoch(data_loader_test, model, device, min_eval_score=args.min_eval_score, max_eval_score=args.max_eval_score, make_fig_path="yes")
        if args.max_eval_score is not None:
            np.savetxt(os.path.join(args.output_dir, "cheater_preds.txt"), ret, fmt='%.4f')
            exit(0)


        test_stats, all_probs, all_targets = ret

        for k,v in test_stats.items():
            print(k,v)

        np.savetxt(os.path.join(args.output_dir, "cheater_preds.txt"), all_probs, fmt='%.4f')
        np.savetxt(os.path.join(args.output_dir, "cheater_labels.txt"), all_targets, fmt='%d')
        with open(
            os.path.join(args.output_dir, "test_results.txt"),
            mode="a",
            encoding="utf-8") as f:
            f.write(json.dumps(test_stats) + "\n")

        exit(0)




    print(f"Start training for {args.epochs} epochs")
    start_time = time.time()
    max_accuracy = 0.0
    for epoch in range(args.start_epoch, args.epochs):
        if args.distributed:
            data_loader_train.sampler.set_epoch(epoch)
        if log_writer is not None:
            log_writer.set_step(epoch * num_training_steps_per_epoch *
                                args.update_freq)
        train_stats = train_one_epoch(
            model,
            criterion,
            data_loader_train,
            optimizer,
            device,
            epoch,
            loss_scaler,
            args.clip_grad,
            model_ema,
            mixup_fn,
            log_writer=log_writer,
            start_steps=epoch * num_training_steps_per_epoch,
            lr_schedule_values=lr_schedule_values,
            wd_schedule_values=wd_schedule_values,
            num_training_steps_per_epoch=num_training_steps_per_epoch,
            update_freq=args.update_freq,
        )
        if args.output_dir and args.save_ckpt:
            _epoch = epoch + 1
            if _epoch % args.save_ckpt_freq == 0 or _epoch == args.epochs:
                utils.save_model(
                    args=args,
                    model=model,
                    model_without_ddp=model_without_ddp,
                    optimizer=optimizer,
                    loss_scaler=loss_scaler,
                    epoch=epoch,
                    model_ema=model_ema)
        if data_loader_val is not None:
            test_stats = validation_one_epoch(data_loader_val, model, device)
            print(
                f"Accuracy of the network on the {len(dataset_val)} val images: {test_stats['acc1']:.2f}%"
            )
            if max_accuracy < test_stats["acc1"]:
                max_accuracy = test_stats["acc1"]
                if args.output_dir and args.save_ckpt:
                    utils.save_model(
                        args=args,
                        model=model,
                        model_without_ddp=model_without_ddp,
                        optimizer=optimizer,
                        loss_scaler=loss_scaler,
                        epoch="best",
                        model_ema=model_ema)

            print(f'Max accuracy: {max_accuracy:.2f}%')
            if log_writer is not None:
                log_writer.update(
                    val_acc1=test_stats['acc1'], head="perf", step=epoch)
                log_writer.update(
                    val_acc5=test_stats['acc5'], head="perf", step=epoch)
                log_writer.update(
                    val_loss=test_stats['loss'], head="perf", step=epoch)

            log_stats = {
                **{f'train_{k}': v
                   for k, v in train_stats.items()},
                **{f'val_{k}': v
                   for k, v in test_stats.items()}, 'epoch': epoch,
                'n_parameters': n_parameters
            }
        else:
            log_stats = {
                **{f'train_{k}': v
                   for k, v in train_stats.items()}, 'epoch': epoch,
                'n_parameters': n_parameters
            }
        if args.output_dir and utils.is_main_process():
            if log_writer is not None:
                log_writer.flush()
            with open(
                    os.path.join(args.output_dir, "log.txt"),
                    mode="a",
                    encoding="utf-8") as f:
                f.write(json.dumps(log_stats) + "\n")

    preds_file = os.path.join(args.output_dir, str(global_rank) + '.txt')
    #test_stats = final_test(data_loader_test, model, device, preds_file)

    # Only use barrier if in distributed mode
    if args.distributed:
        torch.distributed.barrier()

    if global_rank == 0:
        print("Start merging results...")
        final_top1, final_top5 = merge(args.output_dir, num_tasks)
        print(
            f"Accuracy of the network on the {len(dataset_test)} test videos: Top-1: {final_top1:.2f}%, Top-5: {final_top5:.2f}%"
        )
        log_stats = {'Final top-1': final_top1, 'Final Top-5': final_top5}
        if args.output_dir and utils.is_main_process():
            with open(
                    os.path.join(args.output_dir, "log.txt"),
                    mode="a",
                    encoding="utf-8") as f:
                f.write(json.dumps(log_stats) + "\n")

    total_time = time.time() - start_time
    total_time_str = str(datetime.timedelta(seconds=int(total_time)))
    print('Training time {}'.format(total_time_str))


if __name__ == '__main__':
    opts, ds_init = get_args()
    if opts.output_dir:
        Path(opts.output_dir).mkdir(parents=True, exist_ok=True)
    main(opts, ds_init)

```

`deepcheat/VideoMAEv2/train_cheater_pred.py.backup`:

```backup
# --------------------------------------------------------
# Based on BEiT, timm, DINO and DeiT code bases
# https://github.com/microsoft/unilm/tree/master/beit
# https://github.com/rwightman/pytorch-image-models/tree/master/timm
# https://github.com/facebookresearch/deit
# https://github.com/facebookresearch/dino
# --------------------------------------------------------'

import argparse
import datetime
import json
import os
import random
import time
from collections import OrderedDict
from functools import partial
from pathlib import Path

import deepspeed
import numpy as np
import torch
import torch.backends.cudnn as cudnn
from timm.data.mixup import Mixup
from timm.models import create_model
from timm.utils import ModelEma

# NOTE: Do not comment `import models`, it is used to register models
import models  # noqa: F401
import utils
from dataset import build_dataset
from engine_for_finetuning import (
    final_test_killshot,
    predict_killshot_scores,
    merge,
    train_one_epoch,
    validation_one_epoch,
)
from optim_factory import (
    LayerDecayValueAssigner,
    create_optimizer,
    get_parameter_groups,
)
from utils import NativeScalerWithGradNormCount as NativeScaler
from utils import multiple_samples_collate


import torch
import torch.nn as nn
import torch.nn.functional as F


import os
import torch
from torchvision import transforms
from PIL import Image
from collections import deque


class BinaryLabelSmoothingCrossEntropy(nn.Module):
    def __init__(self, smoothing=0.1):
        super(BinaryLabelSmoothingCrossEntropy, self).__init__()
        assert 0 < smoothing < 1.0
        self.smoothing = smoothing
        self.confidence = 1. - smoothing

    def forward(self, x: torch.Tensor, target: torch.Tensor) -> torch.Tensor:
        logprobs = torch.sigmoid(x)

        # Modified negative log likelihood computation
        nll_loss = -(target * torch.log(logprobs) + (1 - target) * torch.log(1 - logprobs))
        
        # Modified smooth loss
        smooth_loss = -logprobs.mul(self.smoothing) - (1 - logprobs).mul(1 - self.smoothing)
        
        loss = self.confidence * nll_loss + self.smoothing * smooth_loss

        return loss.mean()

class BinarySoftTargetCrossEntropy(nn.Module):
    def __init__(self):
        super(BinarySoftTargetCrossEntropy, self).__init__()

    def forward(self, x: torch.Tensor, target: torch.Tensor) -> torch.Tensor:
        logprobs = torch.sigmoid(x)
        loss = -(target * torch.log(logprobs) + (1 - target) * torch.log(1 - logprobs))
        return loss.mean()



def get_args():
    parser = argparse.ArgumentParser(
        'VideoMAE fine-tuning and evaluation script for action classification',
        add_help=False)
    parser.add_argument('--batch_size', default=64, type=int)
    parser.add_argument('--epochs', default=30, type=int)
    parser.add_argument('--update_freq', default=1, type=int)
    parser.add_argument('--save_ckpt_freq', default=100, type=int)

    # Model parameters
    parser.add_argument(
        '--model',
        default='vit_giant_patch14_224 ',
        type=str,
        metavar='MODEL',
        help='Name of model to train')
    parser.add_argument('--tubelet_size', type=int, default=2)
    parser.add_argument(
        '--input_size', default=224, type=int, help='images input size')

    parser.add_argument(
        '--with_checkpoint', action='store_true', default=False)

    parser.add_argument(
        '--drop',
        type=float,
        default=0.0,
        metavar='PCT',
        help='Dropout rate (default: 0.)')
    parser.add_argument(
        '--attn_drop_rate',
        type=float,
        default=0.0,
        metavar='PCT',
        help='Attention dropout rate (default: 0.)')
    parser.add_argument(
        '--drop_path',
        type=float,
        default=0.1,
        metavar='PCT',
        help='Drop path rate (default: 0.1)')
    parser.add_argument(
        '--head_drop_rate',
        type=float,
        default=0.0,
        metavar='PCT',
        help='cls head dropout rate (default: 0.)')

    parser.add_argument(
        '--disable_eval_during_finetuning', action='store_true', default=False)

    parser.add_argument('--model_ema', action='store_true', default=False)
    parser.add_argument(
        '--model_ema_decay', type=float, default=0.9999, help='')
    parser.add_argument(
        '--model_ema_force_cpu', action='store_true', default=False, help='')

    # Optimizer parameters
    parser.add_argument(
        '--opt',
        default='adamw',
        type=str,
        metavar='OPTIMIZER',
        help='Optimizer (default: "adamw"')
    parser.add_argument(
        '--opt_eps',
        default=1e-8,
        type=float,
        metavar='EPSILON',
        help='Optimizer Epsilon (default: 1e-8)')
    parser.add_argument(
        '--opt_betas',
        default=None,
        type=float,
        nargs='+',
        metavar='BETA',
        help='Optimizer Betas (default: None, use opt default)')
    parser.add_argument(
        '--clip_grad',
        type=float,
        default=None,
        metavar='NORM',
        help='Clip gradient norm (default: None, no clipping)')
    parser.add_argument(
        '--momentum',
        type=float,
        default=0.9,
        metavar='M',
        help='SGD momentum (default: 0.9)')
    parser.add_argument(
        '--weight_decay',
        type=float,
        default=0.05,
        help='weight decay (default: 0.05)')
    parser.add_argument(
        '--weight_decay_end',
        type=float,
        default=None,
        help="""Final value of the
        weight decay. We use a cosine schedule for WD and using a larger decay by
        the end of training improves performance for ViTs.""")

    parser.add_argument(
        '--lr',
        type=float,
        default=1e-3,
        metavar='LR',
        help='learning rate (default: 1e-3)')
    parser.add_argument('--layer_decay', type=float, default=0.75)
    parser.add_argument('--min_eval_score', type=float, default=None)
    parser.add_argument('--max_eval_score', type=float, default=None)

    parser.add_argument(
        '--warmup_lr',
        type=float,
        default=1e-8,
        metavar='LR',
        help='warmup learning rate (default: 1e-6)')
    parser.add_argument(
        '--min_lr',
        type=float,
        default=1e-6,
        metavar='LR',
        help='lower lr bound for cyclic schedulers that hit 0 (1e-5)')

    parser.add_argument(
        '--warmup_epochs',
        type=int,
        default=5,
        metavar='N',
        help='epochs to warmup LR, if scheduler supports')
    parser.add_argument(
        '--warmup_steps',
        type=int,
        default=-1,
        metavar='N',
        help='num of steps to warmup LR, will overload warmup_epochs if set > 0'
    )

    # Augmentation parameters
    parser.add_argument(
        '--color_jitter',
        type=float,
        default=0.4,
        metavar='PCT',
        help='Color jitter factor (default: 0.4)')
    parser.add_argument(
        '--num_sample', type=int, default=2, help='Repeated_aug (default: 2)')
    parser.add_argument(
        '--aa',
        type=str,
        default='rand-m7-n4-mstd0.5-inc1',
        metavar='NAME',
        help=
        'Use AutoAugment policy. "v0" or "original". " + "(default: rand-m7-n4-mstd0.5-inc1)'
    ),
    parser.add_argument(
        '--smoothing',
        type=float,
        default=0.1,
        help='Label smoothing (default: 0.1)')
    parser.add_argument(
        '--train_interpolation',
        type=str,
        default='bicubic',
        help=
        'Training interpolation (random, bilinear, bicubic default: "bicubic")'
    )

    # Evaluation parameters
    parser.add_argument('--crop_pct', type=float, default=None)
    parser.add_argument('--short_side_size', type=int, default=224)
    parser.add_argument('--test_num_segment', type=int, default=1)
    parser.add_argument('--test_num_crop', type=int, default=1)

    # * Random Erase params
    parser.add_argument(
        '--reprob',
        type=float,
        default=0.25,
        metavar='PCT',
        help='Random erase prob (default: 0.25)')
    parser.add_argument(
        '--remode',
        type=str,
        default='pixel',
        help='Random erase mode (default: "pixel")')
    parser.add_argument(
        '--recount',
        type=int,
        default=1,
        help='Random erase count (default: 1)')
    parser.add_argument(
        '--resplit',
        action='store_true',
        default=False,
        help='Do not random erase first (clean) augmentation split')

    # * Mixup params
    parser.add_argument(
        '--mixup',
        type=float,
        default=0.0,
        help='mixup alpha, mixup enabled if > 0.')
    parser.add_argument(
        '--cutmix',
        type=float,
        default=1.0,
        help='cutmix alpha, cutmix enabled if > 0.')
    parser.add_argument(
        '--cutmix_minmax',
        type=float,
        nargs='+',
        default=None,
        help='cutmix min/max ratio, overrides alpha and enables cutmix if set')
    parser.add_argument(
        '--mixup_prob',
        type=float,
        default=1.0,
        help=
        'Probability of performing mixup or cutmix when either/both is enabled'
    )
    parser.add_argument(
        '--mixup_switch_prob',
        type=float,
        default=0.5,
        help=
        'Probability of switching to cutmix when both mixup and cutmix enabled'
    )
    parser.add_argument(
        '--mixup_mode',
        type=str,
        default='batch',
        help='How to apply mixup/cutmix params. Per "batch", "pair", or "elem"'
    )

    # * Finetuning params
    parser.add_argument(
        '--finetune', default='', help='finetune from checkpoint')
    parser.add_argument('--model_key', default='model|module', type=str)
    parser.add_argument('--model_prefix', default='', type=str)
    parser.add_argument('--init_scale', default=0.001, type=float)
    parser.add_argument('--use_mean_pooling', action='store_true')
    parser.set_defaults(use_mean_pooling=True)
    parser.add_argument(
        '--use_cls', action='store_false', dest='use_mean_pooling')

    # Dataset parameters
    parser.add_argument(
        '--data_path',
        default='/your/data/path/',
        type=str,
        help='dataset path')
    parser.add_argument(
        '--data_root', default='', type=str, help='dataset path root')
    parser.add_argument(
        '--eval_data_path',
        default=None,
        type=str,
        help='dataset path for evaluation')
    parser.add_argument(
        '--nb_classes',
        default=400,
        type=int,
        help='number of the classification types')
    parser.add_argument(
        '--imagenet_default_mean_and_std', default=True, action='store_true')
    parser.add_argument('--num_segments', type=int, default=1)
    parser.add_argument('--num_frames', type=int, default=16)
    parser.add_argument('--sampling_rate', type=int, default=4)
    parser.add_argument('--sparse_sample', default=False, action='store_true')
    parser.add_argument(
        '--data_set',
        default='killshot',
        type=str,
        help='dataset')
    parser.add_argument(
        '--pred_video',
        default='',
        type=str,
        help='which video to load and pred')
    parser.add_argument(
        '--fname_tmpl',
        default='img_{:010}.jpg',
        type=str,
        help='filename_tmpl for rawframe dataset')
    parser.add_argument(
        '--start_idx',
        default=1,
        type=int,
        help='start_idx for rwaframe dataset')

    parser.add_argument(
        '--output_dir',
        default='',
        help='path where to save, empty for no saving')
    parser.add_argument(
        '--log_dir', default=None, help='path where to tensorboard log')
    parser.add_argument(
        '--device',
        default='cuda',
        help='device to use for training / testing')
    parser.add_argument('--seed', default=0, type=int)
    parser.add_argument('--resume', default='', help='resume from checkpoint')
    parser.add_argument('--auto_resume', action='store_true')
    parser.add_argument(
        '--no_auto_resume', action='store_false', dest='auto_resume')
    parser.set_defaults(auto_resume=True)

    parser.add_argument('--save_ckpt', action='store_true')
    parser.add_argument(
        '--no_save_ckpt', action='store_false', dest='save_ckpt')
    parser.set_defaults(save_ckpt=True)

    parser.add_argument(
        '--start_epoch', default=0, type=int, metavar='N', help='start epoch')
    parser.add_argument(
        '--eval', action='store_true', help='Perform evaluation only')
    parser.add_argument(
        '--validation', action='store_true', help='Perform validation only')
    parser.add_argument(
        '--dist_eval',
        action='store_true',
        default=False,
        help='Enabling distributed evaluation')
    parser.add_argument('--num_workers', default=10, type=int)
    parser.add_argument(
        '--pin_mem',
        action='store_true',
        help=
        'Pin CPU memory in DataLoader for more efficient (sometimes) transfer to GPU.'
    )
    parser.add_argument('--no_pin_mem', action='store_false', dest='pin_mem')
    parser.set_defaults(pin_mem=True)

    # distributed training parameters
    parser.add_argument(
        '--world_size',
        default=1,
        type=int,
        help='number of distributed processes')
    parser.add_argument('--local_rank', default=-1, type=int)
    parser.add_argument('--dist_on_itp', action='store_true')
    parser.add_argument(
        '--dist_url',
        default='env://',
        help='url used to set up distributed training')

    parser.add_argument(
        '--enable_deepspeed', action='store_true', default=False)

    known_args, _ = parser.parse_known_args()

    if known_args.enable_deepspeed:
        parser = deepspeed.add_config_arguments(parser)
        ds_init = deepspeed.initialize
    else:
        ds_init = None

    return parser.parse_args(), ds_init


def main(args, ds_init):
    utils.init_distributed_mode(args)

    if ds_init is not None:
        utils.create_ds_config(args)

    print(args)

    device = torch.device(args.device)

    # fix the seed for reproducibility
    seed = args.seed + utils.get_rank()
    torch.manual_seed(seed)
    np.random.seed(seed)
    random.seed(seed)
    cudnn.benchmark = True

    #args.nb_classes = 2
    if not args.eval:
        dataset_train, args.nb_classes = build_dataset( is_train=True, test_mode=False, args=args)
    else:
        dataset_train = [0]*100
    dataset_val = None


    dataset_test, _ = build_dataset(is_train=False, test_mode=True, args=args)

    num_tasks = utils.get_world_size()
    global_rank = utils.get_rank()
    if not args.eval:
        sampler_train = torch.utils.data.DistributedSampler( dataset_train, num_replicas=num_tasks, rank=global_rank, shuffle=True)
        print("Sampler_train = %s" % str(sampler_train))
    if args.dist_eval:
        if len(dataset_val) % num_tasks != 0:
            print(
                'Warning: Enabling distributed evaluation with an eval dataset not divisible by process number. '
                'This will slightly alter validation results as extra duplicate entries are added to achieve '
                'equal num of samples per-process.')
        sampler_val = torch.utils.data.DistributedSampler(
            dataset_val,
            num_replicas=num_tasks,
            rank=global_rank,
            shuffle=False)
        sampler_test = torch.utils.data.DistributedSampler(
            dataset_test,
            num_replicas=num_tasks,
            rank=global_rank,
            shuffle=False)
    else:
        sampler_val = torch.utils.data.SequentialSampler(dataset_val)
        sampler_test = torch.utils.data.SequentialSampler(dataset_test)

    if global_rank == 0 and args.log_dir is not None:
        os.makedirs(args.log_dir, exist_ok=True)
        log_writer = utils.TensorboardLogger(log_dir=args.log_dir)
    else:
        log_writer = None

    if args.num_sample > 1:
        collate_func = partial(multiple_samples_collate, fold=False)
    else:
        collate_func = None

    if not args.eval:
        data_loader_train = torch.utils.data.DataLoader(
            dataset_train,
            sampler=sampler_train,
            batch_size=args.batch_size,
            num_workers=args.num_workers,
            pin_memory=args.pin_mem,
            drop_last=True,
            collate_fn=collate_func,
            persistent_workers=True)

    if dataset_val is not None:
        data_loader_val = torch.utils.data.DataLoader(
            dataset_val,
            sampler=sampler_val,
            batch_size=int(1.5 * args.batch_size),
            num_workers=args.num_workers,
            pin_memory=args.pin_mem,
            drop_last=False,
            persistent_workers=True)
    else:
        data_loader_val = None

    if dataset_test is not None:
        data_loader_test = torch.utils.data.DataLoader(
            dataset_test,
            sampler=sampler_test,
            batch_size=args.batch_size,
            num_workers=args.num_workers,
            pin_memory=args.pin_mem,
            drop_last=False,
            persistent_workers=True)
    else:
        data_loader_test = None

    mixup_fn = None
    mixup_active = args.mixup > 0 or args.cutmix > 0. or args.cutmix_minmax is not None
    if mixup_active:
        print("Mixup is activated!")
        mixup_fn = Mixup(
            mixup_alpha=args.mixup,
            cutmix_alpha=args.cutmix,
            cutmix_minmax=args.cutmix_minmax,
            prob=args.mixup_prob,
            switch_prob=args.mixup_switch_prob,
            mode=args.mixup_mode,
            label_smoothing=args.smoothing,
            num_classes=2)

    model = create_model(
        args.model,
        pred_each_frame=False,
        img_size=args.input_size,
        pretrained=False,
        num_classes=args.nb_classes,
        all_frames=args.num_frames * args.num_segments,
        tubelet_size=args.tubelet_size,
        drop_rate=args.drop,
        drop_path_rate=args.drop_path,
        attn_drop_rate=args.attn_drop_rate,
        head_drop_rate=args.head_drop_rate,
        drop_block_rate=None,
        use_mean_pooling=args.use_mean_pooling,
        init_scale=args.init_scale,
        with_cp=args.with_checkpoint,
    )
    #breakpoint()

    patch_size = model.patch_embed.patch_size
    print("Patch size = %s" % str(patch_size))

    args.window_size = (args.num_frames // args.tubelet_size,
                        args.input_size // patch_size[0],
                        args.input_size // patch_size[1])

    args.patch_size = patch_size

    if args.finetune:
        if args.finetune.startswith('https'):
            checkpoint = torch.hub.load_state_dict_from_url(
                args.finetune, map_location='cpu', check_hash=True)
        else:
            checkpoint = torch.load(args.finetune, map_location='cpu')

        print("Load ckpt from %s" % args.finetune)
        checkpoint_model = None
        for model_key in args.model_key.split('|'):
            if model_key in checkpoint:
                checkpoint_model = checkpoint[model_key]
                print("Load state_dict by model_key = %s" % model_key)
                break
        if checkpoint_model is None:
            checkpoint_model = checkpoint
        for old_key in list(checkpoint_model.keys()):
            if old_key.startswith('_orig_mod.'):
                new_key = old_key[10:]
                checkpoint_model[new_key] = checkpoint_model.pop(old_key)

        state_dict = model.state_dict()
        for k in ['head.weight', 'head.bias']:
            if k in checkpoint_model and checkpoint_model[
                    k].shape != state_dict[k].shape:
                if checkpoint_model[k].shape[
                        0] == 710 and args.data_set.startswith('Kinetics'):
                    print(f'Convert K710 head to {args.data_set} head')
                    if args.data_set == 'Kinetics-400':
                        label_map_path = 'misc/label_710to400.json'
                    elif args.data_set == 'Kinetics-600':
                        label_map_path = 'misc/label_710to600.json'
                    elif args.data_set == 'Kinetics-700':
                        label_map_path = 'misc/label_710to700.json'

                    label_map = json.load(open(label_map_path))
                    checkpoint_model[k] = checkpoint_model[k][label_map]
                else:
                    print(f"Removing key {k} from pretrained checkpoint")
                    del checkpoint_model[k]

        all_keys = list(checkpoint_model.keys())
        new_dict = OrderedDict()
        for key in all_keys:
            if key.startswith('backbone.'):
                new_dict[key[9:]] = checkpoint_model[key]
            elif key.startswith('encoder.'):
                new_dict[key[8:]] = checkpoint_model[key]
            else:
                new_dict[key] = checkpoint_model[key]
        checkpoint_model = new_dict

        # interpolate position embedding
        if 'pos_embed' in checkpoint_model:
            pos_embed_checkpoint = checkpoint_model['pos_embed']
            embedding_size = pos_embed_checkpoint.shape[-1]  # channel dim
            num_patches = model.patch_embed.num_patches  #
            num_extra_tokens = model.pos_embed.shape[-2] - num_patches  # 0/1

            # height (== width) for the checkpoint position embedding
            orig_size = int(
                ((pos_embed_checkpoint.shape[-2] - num_extra_tokens) //
                 (args.num_frames // model.patch_embed.tubelet_size))**0.5)
            # height (== width) for the new position embedding
            new_size = int(
                (num_patches //
                 (args.num_frames // model.patch_embed.tubelet_size))**0.5)
            # class_token and dist_token are kept unchanged
            if orig_size != new_size:
                print("Position interpolate from %dx%d to %dx%d" %
                      (orig_size, orig_size, new_size, new_size))
                extra_tokens = pos_embed_checkpoint[:, :num_extra_tokens]
                # only the position tokens are interpolated
                pos_tokens = pos_embed_checkpoint[:, num_extra_tokens:]
                # B, L, C -> BT, H, W, C -> BT, C, H, W
                pos_tokens = pos_tokens.reshape(
                    -1, args.num_frames // model.patch_embed.tubelet_size,
                    orig_size, orig_size, embedding_size)
                pos_tokens = pos_tokens.reshape(-1, orig_size, orig_size,
                                                embedding_size).permute(
                                                    0, 3, 1, 2)
                pos_tokens = torch.nn.functional.interpolate(
                    pos_tokens,
                    size=(new_size, new_size),
                    mode='bicubic',
                    align_corners=False)
                # BT, C, H, W -> BT, H, W, C ->  B, T, H, W, C
                pos_tokens = pos_tokens.permute(0, 2, 3, 1).reshape(
                    -1, args.num_frames // model.patch_embed.tubelet_size,
                    new_size, new_size, embedding_size)
                pos_tokens = pos_tokens.flatten(1, 3)  # B, L, C
                new_pos_embed = torch.cat((extra_tokens, pos_tokens), dim=1)
                checkpoint_model['pos_embed'] = new_pos_embed
        elif args.input_size != 224:
            pos_tokens = model.pos_embed
            org_num_frames = 16
            T = org_num_frames // args.tubelet_size
            P = int((pos_tokens.shape[1] // T)**0.5)
            C = pos_tokens.shape[2]
            new_P = args.input_size // patch_size[0]
            # B, L, C -> BT, H, W, C -> BT, C, H, W
            pos_tokens = pos_tokens.reshape(-1, T, P, P, C)
            pos_tokens = pos_tokens.reshape(-1, P, P, C).permute(0, 3, 1, 2)
            pos_tokens = torch.nn.functional.interpolate(
                pos_tokens,
                size=(new_P, new_P),
                mode='bicubic',
                align_corners=False)
            # BT, C, H, W -> BT, H, W, C ->  B, T, H, W, C
            pos_tokens = pos_tokens.permute(0, 2, 3,
                                            1).reshape(-1, T, new_P, new_P, C)
            pos_tokens = pos_tokens.flatten(1, 3)  # B, L, C
            model.pos_embed = pos_tokens  # update
        if args.num_frames != 16:
            org_num_frames = 16
            T = org_num_frames // args.tubelet_size
            pos_tokens = model.pos_embed
            new_T = args.num_frames // args.tubelet_size
            P = int((pos_tokens.shape[1] // T)**0.5)
            C = pos_tokens.shape[2]
            pos_tokens = pos_tokens.reshape(-1, T, P, P, C)
            pos_tokens = pos_tokens.permute(0, 2, 3, 4,
                                            1).reshape(-1, C, T)  # BHW,C,T
            pos_tokens = torch.nn.functional.interpolate(
                pos_tokens, size=new_T, mode='linear')
            pos_tokens = pos_tokens.reshape(1, P, P, C,
                                            new_T).permute(0, 4, 1, 2, 3)
            pos_tokens = pos_tokens.flatten(1, 3)
            model.pos_embed = pos_tokens  # update

        utils.load_state_dict(
            model, checkpoint_model, prefix=args.model_prefix)

    #breakpoint()
    model.to(device)

    model_ema = None
    if args.model_ema:
        # Important to create EMA model after cuda(), DP wrapper, and AMP but before SyncBN and DDP wrapper
        model_ema = ModelEma(
            model,
            decay=args.model_ema_decay,
            device='cpu' if args.model_ema_force_cpu else '',
            resume='')
        print("Using EMA with decay = %.8f" % args.model_ema_decay)

    model_without_ddp = model
    n_parameters = sum(p.numel() for p in model.parameters()
                       if p.requires_grad)

    print("Model = %s" % str(model_without_ddp))
    print('number of params:', n_parameters)

    total_batch_size = args.batch_size * args.update_freq * num_tasks
    num_training_steps_per_epoch = len(dataset_train) // total_batch_size
    args.lr = args.lr * total_batch_size / 8
    #########scale the lr#############
    args.min_lr = args.min_lr * total_batch_size / 8
    args.warmup_lr = args.warmup_lr * total_batch_size / 8
    #########scale the lr#############
    print("LR = %.8f" % args.lr)
    print("Batch size = %d" % total_batch_size)
    print("Update frequent = %d" % args.update_freq)
    print("Number of training examples = %d" % len(dataset_train))
    print("Number of training training per epoch = %d" %
          num_training_steps_per_epoch)

    num_layers = model_without_ddp.get_num_layers()
    if args.layer_decay < 1.0:
        assigner = LayerDecayValueAssigner(
            list(args.layer_decay**(num_layers + 1 - i)
                 for i in range(num_layers + 2)))
    else:
        assigner = None

    if assigner is not None:
        print("Assigned values = %s" % str(assigner.values))

    skip_weight_decay_list = model.no_weight_decay()
    print("Skip weight decay list: ", skip_weight_decay_list)

    if args.enable_deepspeed:
        loss_scaler = None
        optimizer_params = get_parameter_groups(
            model, args.weight_decay, skip_weight_decay_list,
            assigner.get_layer_id if assigner is not None else None,
            assigner.get_scale if assigner is not None else None)
        model, optimizer, _, _ = ds_init(
            args=args,
            model=model,
            model_parameters=optimizer_params,
            dist_init_required=not args.distributed,
        )

        print("model.gradient_accumulation_steps() = %d" %
              model.gradient_accumulation_steps())
        assert model.gradient_accumulation_steps() == args.update_freq
    else:
        if args.distributed:
            model = torch.nn.parallel.DistributedDataParallel(
                model, device_ids=[args.gpu], find_unused_parameters=False)
            model_without_ddp = model.module

        optimizer = create_optimizer(
            args,
            model_without_ddp,
            skip_list=skip_weight_decay_list,
            get_num_layer=assigner.get_layer_id
            if assigner is not None else None,
            get_layer_scale=assigner.get_scale
            if assigner is not None else None)
        loss_scaler = NativeScaler()

    print("Use step level LR scheduler!")
    lr_schedule_values = utils.cosine_scheduler(
        args.lr,
        args.min_lr,
        args.epochs,
        num_training_steps_per_epoch,
        warmup_epochs=args.warmup_epochs,
        warmup_steps=args.warmup_steps,
    )
    if args.weight_decay_end is None:
        args.weight_decay_end = args.weight_decay
    wd_schedule_values = utils.cosine_scheduler(args.weight_decay,
                                                args.weight_decay_end,
                                                args.epochs,
                                                num_training_steps_per_epoch)
    print("Max WD = %.7f, Min WD = %.7f" %
          (max(wd_schedule_values), min(wd_schedule_values)))

    if mixup_fn is not None:
        # smoothing is handled with mixup label transform
        criterion = BinarySoftTargetCrossEntropy()
    elif args.smoothing > 0.:
        criterion = BinarySmoothingCrossEntropy(smoothing=args.smoothing)
    else:
        criterion = torch.nn.BCEWithLogitsLoss()

    criterion = torch.nn.BCEWithLogitsLoss()
    

    print("criterion = %s" % str(criterion))

    utils.auto_load_model(
        args=args,
        model=model,
        model_without_ddp=model_without_ddp,
        optimizer=optimizer,
        loss_scaler=loss_scaler,
        model_ema=model_ema)
    if args.validation:
        test_stats = validation_one_epoch(data_loader_val, model, device)
        print(
            f"{len(dataset_val)} val images: Top-1 {test_stats['acc1']:.2f}%, Top-5 {test_stats['acc5']:.2f}%, loss {test_stats['loss']:.4f}"
        )
        exit(0)

    if args.eval:
        #breakpoint()
        preds_file = os.path.join(args.output_dir, str(global_rank) + '.txt')
        ret = validation_one_epoch(data_loader_test, model, device, min_eval_score=args.min_eval_score, max_eval_score=args.max_eval_score, make_fig_path="yes")
        if args.max_eval_score is not None:
            np.savetxt(os.path.join(args.output_dir, "cheater_preds.txt"), ret, fmt='%.4f')
            exit(0)


        test_stats, all_probs, all_targets = ret

        for k,v in test_stats.items():
            print(k,v)

        np.savetxt(os.path.join(args.output_dir, "cheater_preds.txt"), all_probs, fmt='%.4f')
        np.savetxt(os.path.join(args.output_dir, "cheater_labels.txt"), all_targets, fmt='%d')
        with open(
            os.path.join(args.output_dir, "test_results.txt"),
            mode="a",
            encoding="utf-8") as f:
            f.write(json.dumps(test_stats) + "\n")

        exit(0)




    print(f"Start training for {args.epochs} epochs")
    start_time = time.time()
    max_accuracy = 0.0
    for epoch in range(args.start_epoch, args.epochs):
        if args.distributed:
            data_loader_train.sampler.set_epoch(epoch)
        if log_writer is not None:
            log_writer.set_step(epoch * num_training_steps_per_epoch *
                                args.update_freq)
        train_stats = train_one_epoch(
            model,
            criterion,
            data_loader_train,
            optimizer,
            device,
            epoch,
            loss_scaler,
            args.clip_grad,
            model_ema,
            mixup_fn,
            log_writer=log_writer,
            start_steps=epoch * num_training_steps_per_epoch,
            lr_schedule_values=lr_schedule_values,
            wd_schedule_values=wd_schedule_values,
            num_training_steps_per_epoch=num_training_steps_per_epoch,
            update_freq=args.update_freq,
        )
        if args.output_dir and args.save_ckpt:
            _epoch = epoch + 1
            if _epoch % args.save_ckpt_freq == 0 or _epoch == args.epochs:
                utils.save_model(
                    args=args,
                    model=model,
                    model_without_ddp=model_without_ddp,
                    optimizer=optimizer,
                    loss_scaler=loss_scaler,
                    epoch=epoch,
                    model_ema=model_ema)
        if data_loader_val is not None:
            test_stats = validation_one_epoch(data_loader_val, model, device)
            print(
                f"Accuracy of the network on the {len(dataset_val)} val images: {test_stats['acc1']:.2f}%"
            )
            if max_accuracy < test_stats["acc1"]:
                max_accuracy = test_stats["acc1"]
                if args.output_dir and args.save_ckpt:
                    utils.save_model(
                        args=args,
                        model=model,
                        model_without_ddp=model_without_ddp,
                        optimizer=optimizer,
                        loss_scaler=loss_scaler,
                        epoch="best",
                        model_ema=model_ema)

            print(f'Max accuracy: {max_accuracy:.2f}%')
            if log_writer is not None:
                log_writer.update(
                    val_acc1=test_stats['acc1'], head="perf", step=epoch)
                log_writer.update(
                    val_acc5=test_stats['acc5'], head="perf", step=epoch)
                log_writer.update(
                    val_loss=test_stats['loss'], head="perf", step=epoch)

            log_stats = {
                **{f'train_{k}': v
                   for k, v in train_stats.items()},
                **{f'val_{k}': v
                   for k, v in test_stats.items()}, 'epoch': epoch,
                'n_parameters': n_parameters
            }
        else:
            log_stats = {
                **{f'train_{k}': v
                   for k, v in train_stats.items()}, 'epoch': epoch,
                'n_parameters': n_parameters
            }
        if args.output_dir and utils.is_main_process():
            if log_writer is not None:
                log_writer.flush()
            with open(
                    os.path.join(args.output_dir, "log.txt"),
                    mode="a",
                    encoding="utf-8") as f:
                f.write(json.dumps(log_stats) + "\n")

    preds_file = os.path.join(args.output_dir, str(global_rank) + '.txt')
    #test_stats = final_test(data_loader_test, model, device, preds_file)
    torch.distributed.barrier()

    if global_rank == 0:
        print("Start merging results...")
        final_top1, final_top5 = merge(args.output_dir, num_tasks)
        print(
            f"Accuracy of the network on the {len(dataset_test)} test videos: Top-1: {final_top1:.2f}%, Top-5: {final_top5:.2f}%"
        )
        log_stats = {'Final top-1': final_top1, 'Final Top-5': final_top5}
        if args.output_dir and utils.is_main_process():
            with open(
                    os.path.join(args.output_dir, "log.txt"),
                    mode="a",
                    encoding="utf-8") as f:
                f.write(json.dumps(log_stats) + "\n")

    total_time = time.time() - start_time
    total_time_str = str(datetime.timedelta(seconds=int(total_time)))
    print('Training time {}'.format(total_time_str))


if __name__ == '__main__':
    opts, ds_init = get_args()
    if opts.output_dir:
        Path(opts.output_dir).mkdir(parents=True, exist_ok=True)
    main(opts, ds_init)

```

`deepcheat/VideoMAEv2/train_cheater_pred_patch.py`:

```py
#!/usr/bin/env python3
"""
Patch for train_cheater_pred.py to handle single/multi GPU training gracefully
This script patches the merge function call to check if evaluation files exist
"""

import os
import sys

def patch_training_script():
    """Patch the training script to handle missing evaluation files"""
    script_path = os.path.join(os.path.dirname(__file__), 'train_cheater_pred.py')

    # Read the original file
    with open(script_path, 'r') as f:
        content = f.read()

    # Create backup
    backup_path = script_path + '.backup'
    if not os.path.exists(backup_path):
        with open(backup_path, 'w') as f:
            f.write(content)

    # Patch the merge call to handle missing files
    old_merge_block = '''    if global_rank == 0:
        print("Start merging results...")
        final_top1, final_top5 = merge(args.output_dir, num_tasks)
        print(
            f"Accuracy of the network on the {len(dataset_test)} test videos: Top-1: {final_top1:.2f}%, Top-5: {final_top5:.2f}%"
        )
        log_stats = {'Final top-1': final_top1, 'Final Top-5': final_top5}
        if args.output_dir and utils.is_main_process():
            with open(
                    os.path.join(args.output_dir, "log.txt"),
                    mode="a",
                    encoding="utf-8") as f:
                f.write(json.dumps(log_stats) + "\n")'''

    new_merge_block = '''    if global_rank == 0:
        # Check if evaluation files exist before attempting to merge
        eval_file_0 = os.path.join(args.output_dir, '0.txt')
        if os.path.exists(eval_file_0):
            print("Start merging results...")
            try:
                final_top1, final_top5 = merge(args.output_dir, num_tasks)
                print(
                    f"Accuracy of the network on the {len(dataset_test)} test videos: Top-1: {final_top1:.2f}%, Top-5: {final_top5:.2f}%"
                )
                log_stats = {'Final top-1': final_top1, 'Final Top-5': final_top5}
                if args.output_dir and utils.is_main_process():
                    with open(
                            os.path.join(args.output_dir, "log.txt"),
                            mode="a",
                            encoding="utf-8") as f:
                        f.write(json.dumps(log_stats) + "\n")
            except Exception as e:
                print(f"Warning: Could not merge evaluation results: {e}")
                print("This is normal for single-GPU training without evaluation.")
        else:
            print("No evaluation files found - skipping merge step.")
            print("This is normal for training-only mode or single-GPU setup.")'''

    # Apply the patch
    if old_merge_block in content:
        patched_content = content.replace(old_merge_block, new_merge_block)

        # Write the patched file
        with open(script_path, 'w') as f:
            f.write(patched_content)

        print(f"Successfully patched {script_path}")
        print("Training script now handles single/multi GPU setups automatically")
        return True
    else:
        print("Merge block not found - file may already be patched or different version")
        return False

if __name__ == '__main__':
    patch_training_script()
```

`deepcheat/VideoMAEv2/utils.py`:

```py
# --------------------------------------------------------
# --------------------------------------------------------
# Based on BEiT, timm, DINO and DeiT code bases
# https://github.com/microsoft/unilm/tree/master/beit
# https://github.com/rwightman/pytorch-image-models/tree/master/timm
# https://github.com/facebookresearch/deit
# https://github.com/facebookresearch/dino
# --------------------------------------------------------'
import datetime
import io
import json
import math
import os
import random
import subprocess
import time
from collections import defaultdict, deque
from pathlib import Path

import numpy as np
import torch
import torch.distributed as dist
from tensorboardX import SummaryWriter
from timm.utils import get_state_dict
from torch import inf
from torch.utils.data._utils.collate import default_collate


class SmoothedValue(object):
    """Track a series of values and provide access to smoothed values over a
    window or the global series average.
    """

    def __init__(self, window_size=20, fmt=None):
        if fmt is None:
            fmt = "{median:.4f} ({global_avg:.4f})"
        self.deque = deque(maxlen=window_size)
        self.total = 0.0
        self.count = 0
        self.fmt = fmt

    def update(self, value, n=1):
        self.deque.append(value)
        self.count += n
        self.total += value * n

    def synchronize_between_processes(self):
        """
        Warning: does not synchronize the deque!
        """
        if not is_dist_avail_and_initialized():
            return
        t = torch.tensor([self.count, self.total],
                         dtype=torch.float64,
                         device='cuda')
        dist.barrier()
        dist.all_reduce(t)
        t = t.tolist()
        self.count = int(t[0])
        self.total = t[1]

    @property
    def median(self):
        d = torch.tensor(list(self.deque))
        return d.median().item()

    @property
    def avg(self):
        d = torch.tensor(list(self.deque), dtype=torch.float32)
        return d.mean().item()

    @property
    def global_avg(self):
        return self.total / self.count

    @property
    def max(self):
        return max(self.deque)

    @property
    def min(self):
        return min(self.deque)

    @property
    def value(self):
        return self.deque[-1]

    def __str__(self):
        return self.fmt.format(
            median=self.median,
            avg=self.avg,
            global_avg=self.global_avg,
            max=self.max,
            min=self.min,
            value=self.value)


class MetricLogger(object):

    def __init__(self, delimiter="\t"):
        self.meters = defaultdict(SmoothedValue)
        self.delimiter = delimiter

    def update(self, **kwargs):
        for k, v in kwargs.items():
            if v is None:
                continue
            if isinstance(v, torch.Tensor):
                v = v.item()
            assert isinstance(v, (float, int))
            self.meters[k].update(v)

    def __getattr__(self, attr):
        if attr in self.meters:
            return self.meters[attr]
        if attr in self.__dict__:
            return self.__dict__[attr]
        raise AttributeError("'{}' object has no attribute '{}'".format(
            type(self).__name__, attr))

    def __str__(self):
        loss_str = []
        for name, meter in self.meters.items():
            loss_str.append("{}: {}".format(name, str(meter)))
        return self.delimiter.join(loss_str)

    def synchronize_between_processes(self):
        for meter in self.meters.values():
            meter.synchronize_between_processes()

    def add_meter(self, name, meter):
        self.meters[name] = meter

    def log_every(self, iterable, print_freq, header=None):
        i = 0
        if not header:
            header = ''
        start_time = time.time()
        end = time.time()
        iter_time = SmoothedValue(fmt='{avg:.4f} ({min:.4f} -- {max:.4f})')
        data_time = SmoothedValue(fmt='{avg:.4f} ({min:.4f} -- {max:.4f})')
        space_fmt = ':' + str(len(str(len(iterable)))) + 'd'
        log_msg = [
            header, '[{0' + space_fmt + '}/{1}]', 'eta: {eta}', '{meters}',
            'time: {time}', 'data: {data}'
        ]
        if torch.cuda.is_available():
            log_msg.append('max mem: {memory:.0f}')
        log_msg = self.delimiter.join(log_msg)
        MB = 1024.0 * 1024.0
        for obj in iterable:
            data_time.update(time.time() - end)
            yield obj
            iter_time.update(time.time() - end)
            if i % print_freq == 0 or i == len(iterable) - 1:
                eta_seconds = iter_time.global_avg * (len(iterable) - i)
                eta_string = str(datetime.timedelta(seconds=int(eta_seconds)))
                if torch.cuda.is_available():
                    print(
                        log_msg.format(
                            i,
                            len(iterable),
                            eta=eta_string,
                            meters=str(self),
                            time=str(iter_time),
                            data=str(data_time),
                            memory=torch.cuda.max_memory_allocated() / MB))
                else:
                    print(
                        log_msg.format(
                            i,
                            len(iterable),
                            eta=eta_string,
                            meters=str(self),
                            time=str(iter_time),
                            data=str(data_time)))
            i += 1
            end = time.time()
        total_time = time.time() - start_time
        total_time_str = str(datetime.timedelta(seconds=int(total_time)))
        print('{} Total time: {} ({:.4f} s / it)'.format(
            header, total_time_str, total_time / len(iterable)))


class TensorboardLogger(object):

    def __init__(self, log_dir):
        self.writer = SummaryWriter(logdir=log_dir)
        self.step = 0

    def set_step(self, step=None):
        if step is not None:
            self.step = step
        else:
            self.step += 1

    def update(self, head='scalar', step=None, **kwargs):
        for k, v in kwargs.items():
            if v is None:
                continue
            if isinstance(v, torch.Tensor):
                v = v.item()
            assert isinstance(v, (float, int))
            self.writer.add_scalar(head + "/" + k, v,
                                   self.step if step is None else step)

    def flush(self):
        self.writer.flush()


def seed_worker(worker_id):
    worker_seed = torch.initial_seed() % 2**32
    np.random.seed(worker_seed)
    random.seed(worker_seed)


def _load_checkpoint_for_ema(model_ema, checkpoint):
    """
    Workaround for ModelEma._load_checkpoint to accept an already-loaded object
    """
    mem_file = io.BytesIO()
    torch.save(checkpoint, mem_file)
    mem_file.seek(0)
    model_ema._load_checkpoint(mem_file)


def setup_for_distributed(is_master):
    """
    This function disables printing when not in master process
    """
    import builtins as __builtin__
    builtin_print = __builtin__.print

    def print(*args, **kwargs):
        force = kwargs.pop('force', False)
        if is_master or force:
            builtin_print(*args, **kwargs)

    __builtin__.print = print


def is_dist_avail_and_initialized():
    if not dist.is_available():
        return False
    if not dist.is_initialized():
        return False
    return True


def get_world_size():
    if not is_dist_avail_and_initialized():
        return 1
    return dist.get_world_size()


def get_rank():
    if not is_dist_avail_and_initialized():
        return 0
    return dist.get_rank()


def is_main_process():
    return get_rank() == 0


def save_on_master(*args, **kwargs):
    if is_main_process():
        torch.save(*args, **kwargs)


def init_distributed_mode(args):
    if args.dist_on_itp:
        args.rank = int(os.environ['OMPI_COMM_WORLD_RANK'])
        args.world_size = int(os.environ['OMPI_COMM_WORLD_SIZE'])
        args.gpu = int(os.environ['OMPI_COMM_WORLD_LOCAL_RANK'])
        args.dist_url = "tcp://%s:%s" % (os.environ['MASTER_ADDR'],
                                         os.environ['MASTER_PORT'])
        os.environ['LOCAL_RANK'] = str(args.gpu)
        os.environ['RANK'] = str(args.rank)
        os.environ['WORLD_SIZE'] = str(args.world_size)
        # ["RANK", "WORLD_SIZE", "MASTER_ADDR", "MASTER_PORT", "LOCAL_RANK"]
    elif 'SLURM_PROCID' in os.environ:
        args.rank = int(os.environ['SLURM_PROCID'])
        args.gpu = int(os.environ['SLURM_LOCALID'])
        args.world_size = int(os.environ['SLURM_NTASKS'])
        os.environ['RANK'] = str(args.rank)
        os.environ['LOCAL_RANK'] = str(args.gpu)
        os.environ['WORLD_SIZE'] = str(args.world_size)

        node_list = os.environ['SLURM_NODELIST']
        addr = subprocess.getoutput(
            f'scontrol show hostname {node_list} | head -n1')
        if 'MASTER_ADDR' not in os.environ:
            os.environ['MASTER_ADDR'] = addr
    elif 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:
        args.rank = int(os.environ["RANK"])
        args.world_size = int(os.environ['WORLD_SIZE'])
        args.gpu = int(os.environ['LOCAL_RANK'])
    else:
        print('Not using distributed mode')
        args.distributed = False
        return

    args.distributed = True

    torch.cuda.set_device(args.gpu)
    args.dist_backend = 'nccl'
    print(
        '| distributed init (rank {}): {}, gpu {}'.format(
            args.rank, args.dist_url, args.gpu),
        flush=True)
    torch.distributed.init_process_group(
        backend=args.dist_backend,
        init_method=args.dist_url,
        world_size=args.world_size,
        rank=args.rank)
    torch.cuda.empty_cache()
    torch.distributed.barrier()
    assert torch.distributed.is_initialized()
    setup_for_distributed(args.rank == 0)


def load_state_dict(model,
                    state_dict,
                    prefix='',
                    ignore_missing="relative_position_index"):
    missing_keys = []
    unexpected_keys = []
    error_msgs = []
    # copy state_dict so _load_from_state_dict can modify it
    metadata = getattr(state_dict, '_metadata', None)
    state_dict = state_dict.copy()
    if metadata is not None:
        state_dict._metadata = metadata

    def load(module, prefix=''):
        local_metadata = {} if metadata is None else metadata.get(
            prefix[:-1], {})
        module._load_from_state_dict(state_dict, prefix, local_metadata, True,
                                     missing_keys, unexpected_keys, error_msgs)
        for name, child in module._modules.items():
            if child is not None:
                load(child, prefix + name + '.')

    load(model, prefix=prefix)

    warn_missing_keys = []
    ignore_missing_keys = []
    for key in missing_keys:
        keep_flag = True
        for ignore_key in ignore_missing.split('|'):
            if ignore_key in key:
                keep_flag = False
                break
        if keep_flag:
            warn_missing_keys.append(key)
        else:
            ignore_missing_keys.append(key)

    missing_keys = warn_missing_keys

    if len(missing_keys) > 0:
        print("Weights of {} not initialized from pretrained model: {}".format(
            model.__class__.__name__, missing_keys))
    if len(unexpected_keys) > 0:
        print("Weights from pretrained model not used in {}: {}".format(
            model.__class__.__name__, unexpected_keys))
    if len(ignore_missing_keys) > 0:
        print(
            "Ignored weights of {} not initialized from pretrained model: {}".
            format(model.__class__.__name__, ignore_missing_keys))
    if len(error_msgs) > 0:
        print('\n'.join(error_msgs))


class NativeScalerWithGradNormCount:
    state_dict_key = "amp_scaler"

    def __init__(self):
        self._scaler = torch.cuda.amp.GradScaler()

    def __call__(self,
                 loss,
                 optimizer,
                 clip_grad=None,
                 parameters=None,
                 create_graph=False,
                 update_grad=True):
        self._scaler.scale(loss).backward(create_graph=create_graph)
        if update_grad:
            if clip_grad is not None:
                assert parameters is not None
                self._scaler.unscale_(
                    optimizer
                )  # unscale the gradients of optimizer's assigned params in-place
                norm = torch.nn.utils.clip_grad_norm_(parameters, clip_grad)
            else:
                self._scaler.unscale_(optimizer)
                norm = get_grad_norm_(parameters)
            self._scaler.step(optimizer)
            self._scaler.update()
        else:
            norm = None
        return norm

    def state_dict(self):
        return self._scaler.state_dict()

    def load_state_dict(self, state_dict):
        self._scaler.load_state_dict(state_dict)


def get_grad_norm_(parameters, norm_type: float = 2.0) -> torch.Tensor:
    if isinstance(parameters, torch.Tensor):
        parameters = [parameters]
    parameters = [p for p in parameters if p.grad is not None]
    norm_type = float(norm_type)
    if len(parameters) == 0:
        return torch.tensor(0.)
    device = parameters[0].grad.device
    if norm_type == inf:
        total_norm = max(p.grad.detach().abs().max().to(device)
                         for p in parameters)
    else:
        total_norm = torch.norm(
            torch.stack([
                torch.norm(p.grad.detach(), norm_type).to(device)
                for p in parameters
            ]), norm_type)
    return total_norm


def cosine_scheduler(base_value,
                     final_value,
                     epochs,
                     niter_per_ep,
                     warmup_epochs=0,
                     start_warmup_value=0,
                     warmup_steps=-1):
    warmup_schedule = np.array([])
    warmup_iters = warmup_epochs * niter_per_ep
    if warmup_steps > 0:
        warmup_iters = warmup_steps
    print("Set warmup steps = %d" % warmup_iters)
    if warmup_epochs > 0:
        warmup_schedule = np.linspace(start_warmup_value, base_value,
                                      warmup_iters)

    iters = np.arange(epochs * niter_per_ep - warmup_iters)
    schedule = np.array([
        final_value + 0.5 * (base_value - final_value) *
        (1 + math.cos(math.pi * i / (len(iters)))) for i in iters
    ])

    schedule = np.concatenate((warmup_schedule, schedule))

    assert len(schedule) == epochs * niter_per_ep
    return schedule


def save_model(args,
               epoch,
               model,
               model_without_ddp,
               optimizer,
               loss_scaler,
               model_ema=None):
    output_dir = Path(args.output_dir)
    epoch_name = str(epoch)
    if loss_scaler is not None:
        checkpoint_paths = [output_dir / ('checkpoint-%s.pth' % epoch_name)]
        for checkpoint_path in checkpoint_paths:
            to_save = {
                'model': model_without_ddp.state_dict(),
                'optimizer': optimizer.state_dict(),
                'epoch': epoch,
                'scaler': loss_scaler.state_dict(),
                'args': args,
            }

            if model_ema is not None:
                to_save['model_ema'] = get_state_dict(model_ema)

            save_on_master(to_save, checkpoint_path)
    else:
        client_state = {'epoch': epoch}
        if model_ema is not None:
            client_state['model_ema'] = get_state_dict(model_ema)
        model.save_checkpoint(
            save_dir=args.output_dir,
            tag="checkpoint-%s" % epoch_name,
            client_state=client_state)


def auto_load_model(args,
                    model,
                    model_without_ddp,
                    optimizer,
                    loss_scaler,
                    model_ema=None):
    output_dir = Path(args.output_dir)
    if loss_scaler is not None:
        # torch.amp
        if args.auto_resume and len(args.resume) == 0:
            import glob
            all_checkpoints = glob.glob(
                os.path.join(output_dir, 'checkpoint-*.pth'))
            latest_ckpt = -1
            for ckpt in all_checkpoints:
                t = ckpt.split('-')[-1].split('.')[0]
                if t.isdigit():
                    latest_ckpt = max(int(t), latest_ckpt)
            if latest_ckpt >= 0:
                args.resume = os.path.join(output_dir,
                                           'checkpoint-%d.pth' % latest_ckpt)
            print("Auto resume checkpoint: %s" % args.resume)

        if args.resume:
            if args.resume.startswith('https'):
                checkpoint = torch.hub.load_state_dict_from_url(
                    args.resume, map_location='cpu', check_hash=True)
            else:
                checkpoint = torch.load(args.resume, map_location='cpu')
            model_without_ddp.load_state_dict(checkpoint['model'])
            print("Resume checkpoint %s" % args.resume)
            if 'optimizer' in checkpoint and 'epoch' in checkpoint:
                optimizer.load_state_dict(checkpoint['optimizer'])
                args.start_epoch = checkpoint['epoch'] + 1
                if hasattr(args, 'model_ema') and args.model_ema:
                    _load_checkpoint_for_ema(model_ema,
                                             checkpoint['model_ema'])
                if 'scaler' in checkpoint:
                    loss_scaler.load_state_dict(checkpoint['scaler'])
                print("With optim & sched!")
    else:
        # deepspeed, only support '--auto_resume'.
        if args.auto_resume:
            import glob
            all_checkpoints = glob.glob(
                os.path.join(output_dir, 'checkpoint-*'))
            latest_ckpt = -1
            for ckpt in all_checkpoints:
                t = ckpt.split('-')[-1].split('.')[0]
                if t.isdigit():
                    latest_ckpt = max(int(t), latest_ckpt)
            if latest_ckpt >= 0:
                args.resume = os.path.join(output_dir,
                                           'checkpoint-%d' % latest_ckpt)
                print("Auto resume checkpoint: %d" % latest_ckpt)
                _, client_states = model.load_checkpoint(
                    args.output_dir, tag='checkpoint-%d' % latest_ckpt)
                if 'epoch' in client_states:
                    args.start_epoch = client_states['epoch'] + 1
                if model_ema is not None:
                    if args.model_ema:
                        _load_checkpoint_for_ema(model_ema,
                                                 client_states['model_ema'])


def create_ds_config(args):
    args.deepspeed_config = os.path.join(args.output_dir,
                                         "deepspeed_config.json")
    with open(args.deepspeed_config, mode="w") as writer:
        ds_config = {
            "train_batch_size":
            args.batch_size * args.update_freq * get_world_size(),
            "train_micro_batch_size_per_gpu":
            args.batch_size,
            "steps_per_print":
            1000,
            "gradient_clipping":
            0.0 if args.clip_grad is None else args.clip_grad,
            "optimizer": {
                "type": "Adam",
                "adam_w_mode": True,
                "params": {
                    "lr": args.lr,
                    "weight_decay": args.weight_decay,
                    "bias_correction": True,
                    "betas": [0.9, 0.999],
                    "eps": 1e-8
                }
            },
            "fp16": {
                "enabled": True,
                "loss_scale": 0,
                "initial_scale_power": 7,
                "loss_scale_window": 128
            }
        }

        writer.write(json.dumps(ds_config, indent=2))


def multiple_samples_collate(batch, fold=False):
    """
    Collate function for repeated augmentation. Each instance in the batch has
    more than one sample.
    Args:
        batch (tuple or list): data batch to collate.
    Returns:
        (tuple): collated data batch.
    """
    inputs, labels, video_idx, extra_data = zip(*batch)
    inputs = [item for sublist in inputs for item in sublist]
    labels = [item for sublist in labels for item in sublist]
    video_idx = [item for sublist in video_idx for item in sublist]
    inputs, labels, video_idx, extra_data = (
        default_collate(inputs),
        default_collate(labels),
        default_collate(video_idx),
        default_collate(extra_data),
    )
    if fold:
        return [inputs], labels, video_idx, extra_data
    else:
        return inputs, labels, video_idx, extra_data


def multiple_pretrain_samples_collate(batch, fold=False):
    """
    Collate function for repeated augmentation. Each instance in the batch has
    more than one sample.
    Args:
        batch (tuple or list): data batch to collate.
    Returns:
        (tuple): collated data batch.
    """
    process_data, encoder_mask, decoder_mask = zip(*batch)

    process_data = [item for sublist in process_data for item in sublist]
    encoder_mask = [item for sublist in encoder_mask for item in sublist]
    decoder_mask = [item for sublist in decoder_mask for item in sublist]
    process_data, encoder_mask, decoder_mask = (
        default_collate(process_data),
        default_collate(encoder_mask),
        default_collate(decoder_mask),
    )
    if fold:
        return [process_data], encoder_mask, decoder_mask
    else:
        return process_data, encoder_mask, decoder_mask

```

`deepcheat/dl_classify_video.py`:

```py
import os

import csv
import argparse
import uuid
import os
import subprocess
import numpy as np



from utils.download_links import download_videos_from_csv
from utils.bb_killshot import write_bb_kills
from utils.combo_cheater_detection import make_test_set
from utils.new_crop import process_videos


# Step 1: Parse command-line argument to identify the type of input
parser = argparse.ArgumentParser(description='Process a YouTube URL or a CSV of YouTube URLs.')
parser.add_argument('--url', type=str, help='A single YouTube URL')
parser.add_argument('--csv', type=str, help='Path to the CSV file containing YouTube URLs')
parser.add_argument('--dl_dir', type=str, help='Path to download the directory')
parser.add_argument('--ischeater', type=int, choices=[0,1])

args = parser.parse_args()

# Step 2: Function to generate temporary CSV from a single URL
def generate_csv_from_url(url, filename):
    with open(filename, "w", newline="") as f:
        writer = csv.writer(f)
        writer.writerow(["", "id", "url", "game"])
        writer.writerow([0, str(uuid.uuid4()), url, "CSG"])



# Step 4: Identify the type of input and call the respective function

if args.url:
    filename = "temp.csv"
    generate_csv_from_url(args.url, filename)
elif args.csv: 
    filename = args.url
else:
    print("Please provide either a URL or a CSV file.")


if not os.path.exists(args.dl_dir):
    os.makedirs(args.dl_dir)

download_videos_from_csv(filename, args.dl_dir)

# List all the .mp4 filenames in args.dl_dir
mp4_files = [] 
for f in os.listdir(args.dl_dir):
	if args.url:
		video_id = os.path.splitext(f)[0]
		if video_id not in args.url:
			continue
		else:
			mp4_files = [os.path.abspath(os.path.join(args.dl_dir,f))]
			break
	elif f.endswith('.mp4'):
		mp4_files.append(os.path.abspath(os.path.join(args.dl_dir,f)))


process_videos(args.dl_dir, args.dl_dir, 240, make_train_file=False)

for mp4_file in mp4_files:
	video_dir = os.path.splitext(mp4_file)[0]

	#killshot finder
	bb_file = os.path.join(video_dir, "bb_labels.txt")
	write_bb_kills(mp4_file, bb_file)


	os.chdir('./VideoMAEv2')
	subprocess.run(['./run_killshot.sh', '--pred_video', video_dir])

	# combine both killshots into 1 file
	make_test_set(bb_file, video_dir, args.ischeater, os.path.join(video_dir,"test.csv"))

	subprocess.run(['./eval_cheater.sh', video_dir])
	preds = np.loadtxt(os.path.join(video_dir,"cheater_preds.txt"))
	print("cheating suspicion: ", (preds > .5).sum() / len(preds) )

	os.chdir('..')


```

`deepcheat/environment.yml`:

```yml
name: waldo-dev-env
channels:
  - defaults
  - conda-forge
dependencies:
  - pandas
  - requests
  - validators
  - pylint
  - opencv
  - pytube

```

`deepcheat/finetuning/README.md`:

```md
# finetuning
This directory will contain all of the code used for fine-tuning the final classifier model.
```

`deepcheat/pretraining/README.md`:

```md
# pretraining
This directory will contain all the code used for the pretraining stage of the model
```

`deepcheat/requirements.txt`:

```txt
av
decord
deepspeed
einops
matplotlib
numpy
opencv-python
pandas
Pillow
scipy
tensorboard==2.9.0
tensorboardX==1.8
timm==0.4.12
torch>=1.2
torchaudio
torchvision
triton

```

`deepcheat/utils/README.md`:

```md
# utils
This directory contains all the preprocessing and utility scripts used across the models repo.

### install-environment.sh
---
This shell script can be run to automatically create an anaconda environment with the dependencies installed.
The full list of dependencies can be found in models/environment.yml

### generate-requirements.sh
---
After activating a conda environment this shell script can be run to export environment.yml

### link-retrieval.py
---
Get URL's from API and stores them locally
- Options
  - -e or --endpoint is the target url on the server
    - Default: `https://waldo.vision/api/trpc/urls`
    - Required: `False`
  - -k or --key is the API Key
    - If left unspecified the default key will be pulled from the environment variable `WALDO_API_KEY`
    - Required: `False`
  - -o or --output is the folder where the file will be stored
    - Required: `True`
    - Example: `/home/usr/website/files/` the output would then be `/home/usr/website/files/links.csv`
  - -r or --requirements is the specifications for the links you want to download
    - Default: `{"minReviews": 25, "rating": 90}`
    - Example: `{"minReviews": 20, "rating": 95, "otherspecfication": "gamename"}`

### download_links.py
---
Download YouTube videos using the CSV output of link-retrieval.py
- Options
  - -i or --input is the path to the CSV of YouTube URLs to be downloaded
    - Required: `True`
  - -o or --output is the path to the folder where downloaded videos will be placed
    - Required: `True`
  - -r or --max_resolution is the maximum resolution of video that the script will try to download
    - Default: `1080`
    - Required: `False`
```

`deepcheat/utils/autoedit/autoedit_improved.py`:

```py
import librosa
import numpy as np
import os
import subprocess
import soundfile as sf
import argparse
import sys
import shutil
import uuid
import time

def get_ffmpeg_path():
    """Finds the absolute path to the ffmpeg executable."""
    ffmpeg_path = shutil.which("ffmpeg")
    if ffmpeg_path:
        return ffmpeg_path
    conda_prefix = os.environ.get("CONDA_PREFIX")
    if conda_prefix:
        ffmpeg_path = os.path.join(conda_prefix, "bin", "ffmpeg")
        if os.path.exists(ffmpeg_path):
            return ffmpeg_path
    raise FileNotFoundError("Could not find ffmpeg executable.")

def get_video_duration(video_path):
    """Get video duration in seconds using ffprobe."""
    try:
        cmd = [
            "ffprobe", "-v", "quiet", "-show_entries", "format=duration", 
            "-of", "default=noprint_wrappers=1:nokey=1", video_path
        ]
        result = subprocess.run(cmd, capture_output=True, text=True, check=True)
        return float(result.stdout.strip())
    except (subprocess.CalledProcessError, ValueError):
        return None

def get_file_size_mb(file_path):
    """Get file size in MB."""
    return os.path.getsize(file_path) / (1024 * 1024)

def extract_audio(video_path, audio_output_path, resample_rate, max_duration=None):
    """Extracts and resamples the first audio stream from a video."""
    ffmpeg_path = get_ffmpeg_path()
    
    # First get video duration for progress calculation
    duration = get_video_duration(video_path)
    
    command = [
        ffmpeg_path,
        '-i', video_path,
        '-map', '0:a:0',
        '-acodec', 'pcm_s16le',  # Use faster PCM codec instead of default
        '-ac', '1',
        '-ar', str(resample_rate),
        '-progress', 'pipe:1',  # Output progress to stdout
        '-stats_period', '1',    # Update every 1 second for faster feedback
        '-threads', '0'          # Use all available CPU threads
    ]
    
    # Add duration limit if specified
    if max_duration:
        command.extend(['-t', str(max_duration)])
        duration = min(duration, max_duration) if duration else max_duration
    
    command.extend(['-y', audio_output_path])
    
    print(f"PROGRESS:0:Starting audio extraction from {os.path.basename(video_path)}...", flush=True)
    sys.stdout.flush()  # Force flush
    
    start_time = time.time()
    
    try:
        # Use Popen for real-time output
        process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, 
                                 text=True, bufsize=1, universal_newlines=True)
        
        last_progress = 0
        while True:
            line = process.stdout.readline()
            if not line:
                break
                
            # Parse ffmpeg progress output
            if 'out_time_ms=' in line:
                try:
                    # Extract time in microseconds
                    time_ms = int(line.split('out_time_ms=')[1].strip()) / 1000000
                    if duration and duration > 0:
                        # Calculate progress percentage (0-10% of total progress)
                        progress = min(int((time_ms / duration) * 10), 10)
                        if progress > last_progress:
                            print(f"PROGRESS:{progress}:Extracting audio... {int((time_ms/duration)*100)}% complete", flush=True)
                            sys.stdout.flush()
                            last_progress = progress
                except (ValueError, IndexError):
                    pass
        
        # Wait for process to complete
        process.wait()
        if process.returncode != 0:
            stderr_output = process.stderr.read()
            raise subprocess.CalledProcessError(process.returncode, command, stderr=stderr_output)
            
    except subprocess.CalledProcessError as e:
        print(f"Error extracting audio: {e.stderr}", file=sys.stderr)
        raise
    
    elapsed = time.time() - start_time
    print(f"PROGRESS:10:Audio extraction completed in {elapsed:.1f} seconds.", flush=True)
    sys.stdout.flush()
    return None

def detect_kill_sounds(audio_path, sample_sound_path, resample_rate, cooldown_period):
    """Detects kill sounds in an audio file."""
    print(f"PROGRESS:15:Loading sample kill sound...", flush=True)
    sys.stdout.flush()
    kill_sound, _ = librosa.load(sample_sound_path, sr=resample_rate)
    
    print(f"PROGRESS:20:Loading audio file for analysis...", flush=True)
    sys.stdout.flush()
    
    # Get file size to estimate loading time
    file_size_mb = os.path.getsize(audio_path) / (1024 * 1024)
    print(f"PROGRESS:22:Audio file size: {file_size_mb:.1f}MB - this may take a moment...", flush=True)
    sys.stdout.flush()
    
    start_time = time.time()
    audio, _ = librosa.load(audio_path, sr=resample_rate)
    load_time = time.time() - start_time
    print(f"PROGRESS:40:Audio loaded in {load_time:.1f} seconds. Duration: {len(audio)/resample_rate:.1f} seconds", flush=True)
    sys.stdout.flush()

    print("PROGRESS:45:Performing correlation analysis...", flush=True)
    sys.stdout.flush()
    start_time = time.time()
    correlation = np.correlate(audio, kill_sound, mode='valid')
    correlation /= np.max(np.abs(correlation))
    correlation_time = time.time() - start_time
    print(f"PROGRESS:60:Correlation analysis completed in {correlation_time:.1f} seconds.", flush=True)
    sys.stdout.flush()
    
    threshold = 0.8
    detections = np.where(correlation >= threshold)[0]
    timestamps = [(det / resample_rate) for det in detections]

    filtered_timestamps = []
    last_time = -cooldown_period
    for t in timestamps:
        if t - last_time >= cooldown_period:
            filtered_timestamps.append(t)
            last_time = t
    
    if not filtered_timestamps:
        print(f"No unique kill sounds detected in {os.path.basename(audio_path)}.", flush=True)
    else:
        print(f"Detected {len(filtered_timestamps)} unique kill sounds in {os.path.basename(audio_path)}.", flush=True)
    sys.stdout.flush()
    return filtered_timestamps

def extract_clips(video_path, timestamps, output_dir, video_basename):
    """Extracts clips from a video based on timestamps."""
    ffmpeg_path = get_ffmpeg_path()
    os.makedirs(output_dir, exist_ok=True)
    
    total_clips = len(timestamps)
    print(f"PROGRESS:65:Extracting {total_clips} clips...")
    for i, timestamp in enumerate(timestamps):
        # Calculate progress from 65% to 95%
        progress = 65 + int((i / total_clips) * 30)
        start_time = max(0, timestamp - 1.7)
        clip_output_path = os.path.join(output_dir, f'{video_basename}_clip_{i + 1}.mp4')
        command = [
            ffmpeg_path,
            '-ss', str(start_time),
            '-i', video_path,
            '-t', '2',
            '-c', 'copy',
            '-y',
            clip_output_path
        ]
        try:
            subprocess.run(command, shell=False, check=True, capture_output=True, text=True)
            print(f"PROGRESS:{progress}:Extracted clip {i+1}/{total_clips}: {os.path.basename(clip_output_path)}")
        except subprocess.CalledProcessError as e:
            print(f"  Error extracting clip {i+1}: {e}", file=sys.stderr)

def validate_input_files(input_dir, max_file_size_mb=None, max_duration_minutes=None):
    """Validate input files before processing."""
    issues = []
    
    for filename in os.listdir(input_dir):
        if filename.endswith((".mp4", ".mov", ".avi")):
            file_path = os.path.join(input_dir, filename)
            
            # Check file size only if limit is specified
            if max_file_size_mb is not None:
                size_mb = get_file_size_mb(file_path)
                if size_mb > max_file_size_mb:
                    issues.append(f"{filename}: File too large ({size_mb:.1f}MB > {max_file_size_mb}MB)")
            
            # Check duration only if limit is specified
            if max_duration_minutes is not None:
                duration = get_video_duration(file_path)
                if duration and duration > (max_duration_minutes * 60):
                    duration_mins = duration / 60
                    issues.append(f"{filename}: Video too long ({duration_mins:.1f}min > {max_duration_minutes}min)")
    
    return issues

def main():
    parser = argparse.ArgumentParser(description="Process video files to extract clips based on sound detection.")
    parser.add_argument('--input-dir', required=True, help="Directory containing input video files.")
    parser.add_argument('--output-dir', required=True, help="Directory to save the output clips.")
    parser.add_argument('--sample-sound', required=True, help="Path to the sample kill sound.")
    parser.add_argument('--temp-dir', required=True, help="Directory for temporary files.")
    parser.add_argument('--max-file-size-mb', type=int, default=None, help="Maximum file size in MB (default: no limit)")
    parser.add_argument('--max-duration-minutes', type=int, default=None, help="Maximum video duration in minutes (default: no limit)")
    parser.add_argument('--skip-validation', action='store_true', help="Skip file validation (use with caution)")
    args = parser.parse_args()

    resample_rate = 8000
    cooldown_period = 1

    print("=== WALD01 Auto-Edit Video Processor ===")
    print(f"Input directory: {args.input_dir}")
    print(f"Output directory: {args.output_dir}")
    print(f"Sample sound: {args.sample_sound}")
    print(f"Max file size: {'No limit' if args.max_file_size_mb is None else f'{args.max_file_size_mb}MB'}")
    print(f"Max duration: {'No limit' if args.max_duration_minutes is None else f'{args.max_duration_minutes} minutes'}")
    print()

    # Validate inputs
    if not args.skip_validation:
        print("Validating input files...")
        validation_issues = validate_input_files(
            args.input_dir, 
            args.max_file_size_mb, 
            args.max_duration_minutes
        )
        
        if validation_issues:
            print("âŒ Validation failed:")
            for issue in validation_issues:
                print(f"  - {issue}")
            print("\nOptions:")
            print("  1. Use smaller/shorter video files")
            print("  2. Increase limits with --max-duration-minutes or remove file size limit")
            print("  3. Skip validation with --skip-validation (not recommended)")
            return 1
        else:
            print("âœ… All files passed validation.")
    
    os.makedirs(args.output_dir, exist_ok=True)
    os.makedirs(args.temp_dir, exist_ok=True)

    video_files = [f for f in os.listdir(args.input_dir) if f.endswith((".mp4", ".mov", ".avi"))]
    
    if not video_files:
        print("No video files found in input directory.")
        return 1
    
    print(f"\nProcessing {len(video_files)} video file(s):")
    
    total_clips_extracted = 0
    
    for i, filename in enumerate(video_files):
        print(f"\n[{i+1}/{len(video_files)}] Processing: {filename}", flush=True)
        video_path = os.path.join(args.input_dir, filename)
        video_basename = os.path.splitext(filename)[0]
        temp_audio_path = os.path.join(args.temp_dir, f"{uuid.uuid4()}.wav")
        
        # Show file info
        size_mb = get_file_size_mb(video_path)
        duration = get_video_duration(video_path)
        print(f"  File size: {size_mb:.1f}MB", flush=True)
        if duration:
            print(f"  Duration: {duration/60:.1f} minutes", flush=True)
        print(f"PROGRESS:0:Starting processing of {filename}...", flush=True)
        sys.stdout.flush()  # Force immediate output

        try:
            # Extract audio
            extract_audio(video_path, temp_audio_path, resample_rate)
            
            # Detect kill sounds
            timestamps = detect_kill_sounds(temp_audio_path, args.sample_sound, resample_rate, cooldown_period)
            
            # Extract clips
            if timestamps:
                extract_clips(video_path, timestamps, args.output_dir, video_basename)
                total_clips_extracted += len(timestamps)
            else:
                print(f"PROGRESS:95:No clips extracted from {filename}")
                
        except subprocess.CalledProcessError as e:
            print(f"âŒ Error processing {filename}: {e.stderr}", file=sys.stderr)
        except Exception as e:
            print(f"âŒ Unexpected error with {filename}: {e}", file=sys.stderr)
        finally:
            # Clean up temp file
            if os.path.exists(temp_audio_path):
                os.remove(temp_audio_path)
    
    print(f"PROGRESS:100:Processing Complete")
    print(f"\n=== Processing Complete ===")
    print(f"Total clips extracted: {total_clips_extracted}")
    print(f"Output directory: {args.output_dir}")
    
    return 0

if __name__ == "__main__":
    sys.exit(main())

```

`deepcheat/utils/bb_killshot.py`:

```py
import cv2
import numpy as np
import sys
import os
import csv


def _get_bb_kills(filename):
    cap = cv2.VideoCapture(filename)
    DEATH_FRAME_COOLDOWN = 0
    DEATH_KILLFEED_PIXEL_AREA = 4700
    death = False
    death_counter = 0

    last_fifteen = [0]

    last_mov_med = 0
    frames_since_last_kill = 0
    total = 0
    cur_frame_count = 0
    kill_detection_frames = []
    while True:
        cur_frame_count+=1
        ret, frame = cap.read()
        if not ret:
            break

        height, width, _ = frame.shape
        frame = frame[0:height // 3, int(width / 1.3):width]
        red = cv2.inRange(frame, (0, 0, 100), (45, 45, 255))
        #cv2.imshow('red', red)
        contours, _ = cv2.findContours(red, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)
        cv2.drawContours(frame, contours, -1, (0, 255, 0), 3)
        total_contour_area = 0
        for contour in contours:
            total_contour_area += cv2.contourArea(contour)
        mask = np.zeros((frame.shape[0], frame.shape[1]), dtype=np.uint8)

        for contour in contours:
            contour = cv2.convexHull(contour)
            if cv2.contourArea(contour) > 100:
                cv2.drawContours(mask, [contour], 0, (255, 255, 255), thickness=cv2.FILLED)

        #cv2.imshow('mask', mask)
        final_contours, _ = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)
        #cv2.imshow('frame', frame)

        if death:
            death_counter += 1
        if death_counter > DEATH_FRAME_COOLDOWN and DEATH_FRAME_COOLDOWN!=0:
            print('Resuming detection')
            death = False
            death_counter = 0

        if not death:
            # Detect death
            red_pixels = np.count_nonzero(red)
            masked_pixels = np.count_nonzero(mask)

            if red_pixels > DEATH_KILLFEED_PIXEL_AREA and masked_pixels > DEATH_KILLFEED_PIXEL_AREA and DEATH_FRAME_COOLDOWN!=0:
                death = True
                print('Pausing detection because of potential death')
            last_fifteen.append(len(final_contours))
            if len(last_fifteen) > 15:
                last_fifteen.pop(0)
            moving_median = np.median(last_fifteen)
            if moving_median > last_mov_med:
                if frames_since_last_kill > 10:
                    print('Kill detected! on frame ', cur_frame_count)
                    kill_detection_frames.append(cur_frame_count)
                    total+=1
                    frames_since_last_kill = 0
            last_mov_med = moving_median
            frames_since_last_kill += 1

    print("total kills: ", total)
    return kill_detection_frames        

def write_bb_kills(infile, outfile):
    kill_detection_frames = _get_bb_kills(infile)
    with open(outfile, 'w') as csv_file:
        writer = csv.writer(csv_file)
        writer.writerow(kill_detection_frames)  # Writes the entire list in one row

if __name__ =="__main__":
    outfilename= os.path.join("csg_videos_bb_labels", sys.argv[1].split("/")[-1] +".txt")
    write_bb_kills(sys.argv[1], outfilename)




```

`deepcheat/utils/combo_cheater_detection.py`:

```py
import os
import csv
import argparse
import numpy as np


def make_test_set(bb_label_file, raw_frame_dir, aim_assist, output_csv, idx_offset=-15, frame_range=4):
    # List to store rows for csv
    dataset = []
   
    # Load the numpy predictions from preds.txt
    deep_model_preds = np.loadtxt(os.path.join(raw_frame_dir, 'preds.txt'))
    
    # Calculate the mean and standard deviation
    mean_pred = np.mean(deep_model_preds)
    std_pred = np.std(deep_model_preds)
    
    with open(bb_label_file, 'r') as f: bb_kill_pred_frames = f.readline().strip().split(',')
        
    for idx in bb_kill_pred_frames:
        idx_value = int(idx) + idx_offset
        
        # Check if index is within range for the specified criterion
        for i in range(idx_value - frame_range, idx_value + frame_range + 1):

            likely_dl_kill = abs(deep_model_preds[i] - mean_pred) >= 2 * std_pred
            not_firstlast_frame = i >= 0 and i < len(deep_model_preds)

            if (not_firstlast_frame and likely_dl_kill ):
                
                img_name = f"img_{i:010}.jpg"
                dataset.append([raw_frame_dir, img_name, idx_value, aim_assist])
                print(f"Appended: {raw_frame_dir}, {img_name}, {idx_value}, {aim_assist}")
                break # go to next bb_killframe
    
    # Write the dataset to the CSV file
    with open(output_csv, 'w', newline='') as csvfile:
        csvwriter = csv.writer(csvfile)
        csvwriter.writerow(['Directory Path', 'Filename', 'Index', 'Aim-Assist ON'])  # Writing the headers
        csvwriter.writerows(dataset)


def create_dataset_csv(txt_dir, img_dir, output_csv, idx_offset=-15, frame_range=4):
    # List to store rows for csv
    dataset = []
    
    # 1. Iterate over the directory containing the .mp4.txt files.
    for txt_file in os.listdir(txt_dir):
        if txt_file.endswith('.txt'):
            file_base_name = txt_file.replace('.mp4', '').replace('.txt', '')
            
            # Check for "Aim-Assist ON" in the filename
            aim_assist = 1 if "Aim-Assist ON" in txt_file else 0
            
            # Get the corresponding directory for this txt_file in img_dir
            corresponding_dir = os.path.join(img_dir, file_base_name)
            
            # Check if the directory exists
            if os.path.exists(corresponding_dir):
                
                # Load the numpy predictions from preds.txt
                preds = np.loadtxt(os.path.join(corresponding_dir, 'preds.txt'))
                
                # Calculate the mean and standard deviation
                mean_pred = np.mean(preds)
                std_pred = np.std(preds)
                
                # Read the indices from the .mp4.txt file
                with open(os.path.join(txt_dir, txt_file), 'r') as f:
                    indices = f.readline().strip().split(',')
                    
                    for idx in indices:
                        idx_value = int(idx) + idx_offset
                        
                        # Check if index is within range for the specified criterion
                        for i in range(idx_value - frame_range, idx_value + frame_range + 1):
                            if (i >= 0 and i < len(preds) and 
                                abs(preds[i] - mean_pred) >= 2 * std_pred):
                                
                                img_name = f"img_{i:010}.jpg"
                                dataset.append([corresponding_dir, img_name, idx_value, aim_assist])
                                print(f"Appended: {corresponding_dir}, {img_name}, {idx_value}, {aim_assist}")
                                break
    
    # Write the dataset to the CSV file
    with open(output_csv, 'w', newline='') as csvfile:
        csvwriter = csv.writer(csvfile)
        csvwriter.writerow(['Directory Path', 'Filename', 'Index', 'Aim-Assist ON'])  # Writing the headers
        csvwriter.writerows(dataset)


# python combo_cheater_detection.py --txt_dir csg_videos_bb_labels/rican/ --img_dir /home/waldo/code/models/utils/cheater_data/rican_processed/
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Center crop videos in a directory.")
    parser.add_argument("--txt_dir", required=True, help="Source directory containing the videos.")
    parser.add_argument("--img_dir", required=True, help="Destination directory to save the cropped videos.")
    parser.add_argument("--output_csv_file", type=str, default="combocheater.csv", help="Size of the center crop.")

    args = parser.parse_args()
    create_dataset_csv(args.txt_dir, args.img_dir, args.output_csv_file)

```

`deepcheat/utils/common.py`:

```py
"""Common functions that are used across files"""
import os

def ensure_dir_exists(path:str) -> None:
    '''Checks if directory exists. Creates it if it doesn't'''
    if not os.path.exists(path):
        os.makedirs(path)

```

`deepcheat/utils/crop_videos.py`:

```py
"""
Takes a given video and outputs cropped frames of the video.
"""
import argparse
import pathlib
from utils.default_video_cropper import DefaultVideoCropper

parser = argparse.ArgumentParser(
    description="Crop a video and store frames locally", add_help=False
)
parser.add_argument("-i", "--input", help="Input Video", required=True, type=str)
parser.add_argument("-w", "--width", help="Cropping width", required=True, type=int)
parser.add_argument("-h", "--height", help="Cropping height", required=True, type=int)
parser.add_argument(
    "-o", "--output", help="Folder To Store Output", required=True, type=str
)
parser.add_argument(
    "-x", "--x_position", help="Crop x position", required=False, default=(0), type=int
)
parser.add_argument(
    "-y", "--y_position", help="Crop y position", required=False, default=(0), type=int
)

args = vars(parser.parse_args())


def extract_vid_uuid(vid_path: str) -> str:
    """Extracts uuid from video path"""
    vid_path = pathlib.Path(vid_path)
    uuid = vid_path.with_suffix("").name  # Remove file extension (.mp4,.webm)
    return uuid


def main() -> None:
    """Crops an input video and saves its frames"""
    cropper = DefaultVideoCropper(
        output_dir=args["output"],
        cropping_dimensions=(args["width"], args["height"]),
        crop_position=(args["x_position"], args["y_position"]),
    )
    uuid = extract_vid_uuid(args["input"])
    cropper.process_video(args["input"], uuid)


if __name__ == "__main__":
    main()

```

`deepcheat/utils/default_video_cropper.py`:

```py
"""
Contains the default video cropper class.
This class can be used to convert a video into cropped frames.
"""
from typing import Any, Tuple
import cv2
from utils.common import ensure_dir_exists


class DefaultVideoCropper:
    # pylint: disable=too-few-public-methods
    # pylint: disable=no-member
    """
    Creates a video cropper object which can crop and save
    the frames of a video to a specified output directory
    """
    def __init__(
        self,
        output_dir: str,
        cropping_dimensions: Tuple[int, int],
        crop_position: Tuple[int, int] = (0, 0),
    ) -> None:
        self._output_dir = output_dir
        ensure_dir_exists(self._output_dir)
        self._crop_width, self._crop_height = cropping_dimensions
        self._crop_position_x, self._crop_position_y = crop_position

    def _crop_frame(self, video_frame: Any) -> Any:
        """Crops a given frame"""
        video_frame = video_frame[
            self._crop_position_y : self._crop_position_y + self._crop_height,
            self._crop_position_x : self._crop_position_x + self._crop_width,
        ]
        return video_frame

    def process_video(self, vid_path: str, video_uuid: str) -> None:
        """Crops an input video and saves its frames"""
        vidcap = cv2.VideoCapture(vid_path)
        count = 0
        was_read, frame = vidcap.read()
        while was_read:
            frame = self._crop_frame(frame)
            cv2.imwrite(f"{self._output_dir}/vid_{video_uuid}_frame_{count}.jpg", frame)
            was_read, frame = vidcap.read()
            count += 1

```

`deepcheat/utils/download_links.py`:

```py
"""
Downloads youtube videos from a csv of URLs
"""
import argparse

import pandas as pd
from pytube import YouTube
from pytube.exceptions import VideoUnavailable, AgeRestrictedError, VideoRegionBlocked, MembersOnly
from pytube.exceptions import LiveStreamError, RecordingUnavailable, VideoPrivate, RegexMatchError
from urllib.parse import urlparse, parse_qs



def get_youtube_uuid(url):
    parsed_url = urlparse(url)
    if 'youtube.com' in parsed_url.netloc:
        query = parse_qs(parsed_url.query)
        return query.get("v", [None])[0]
    elif 'youtu.be' in parsed_url.netloc:
        return parsed_url.path.lstrip('/')
    else:
        return None

def download_best_video(row, min_res, max_res, output_path, max_retries=2):
    """Downloads the Highest resolution, Highest FPS stream of a given video URL from YouTube"""

    retry = 0
    while retry <= max_retries:
        print(f"Downloading URL: {row['url']}")

        # create YT object
        try:
            yt_obj = YouTube(row['url'])
        except RegexMatchError:
            print(f"Bad Video URL: {row['url']}")
            break

        # get list of all mp4 video streams
        try:
            all_streams = yt_obj.streams.filter(file_extension='mp4',type='video')
        except (VideoUnavailable, AgeRestrictedError, VideoRegionBlocked,
                LiveStreamError, RecordingUnavailable, MembersOnly, VideoPrivate) as error:
            print(f"{row['url']} is not available for download: {error}")
            break
        except KeyError:
            retry += 1
            print(f"Error Getting Video Streams, retrying... ({retry}/{max_retries})")
            continue

        # get set of all available resolutions
        available_resolutions = []
        for stream in all_streams:
            available_resolutions.append(stream.resolution.split('p')[0])
        available_resolutions = [*set(available_resolutions)]

        # get streams with resolution that meets criteria
        good_resolutions = [int(res) for res in available_resolutions if int(res) >= int(min_res)]
        good_resolutions = [res for res in good_resolutions if res <= int(max_res)]
        if not good_resolutions:
            print(f"No streams available matching resolution criteria for {row['url']}!")
            break

        # get highest resolution
        best_res = str(max(good_resolutions)) + 'p'

        # get highest fps stream at best res
        best_stream = yt_obj.streams \
                            .filter(file_extension='mp4', type='video', res=best_res) \
                            .order_by('fps') \
                            .last()

        # download video
        out_filename = get_youtube_uuid(row['url']+".mp4")
        best_stream.download(output_path=output_path,
                            filename=out_filename,
                            skip_existing=True,
                            max_retries=max_retries)

        return True

    print(f"Could not download {row['url']}!")
    return False

def download_videos_from_csv(input_path, output_path, min_res=360, max_res=1080):
    """Reads URLs into dataframe and downloads each one"""

    # read csv into dataframe
    print(f'Reading Input CSV: {input_path}')
    urls_df = pd.read_csv(input_path)
    urls_df.dropna(inplace=True)

    # download all videos
    print('Beginning Downloads...')
    urls_df.apply(download_best_video,
                  min_res=min_res,
                  max_res=max_res,
                  output_path=output_path,
                  axis=1)

    print('Downloads Complete')

def main():

    # Setup command line arguments
    parser = argparse.ArgumentParser(description="Get URL's from API and store them locally")
    parser.add_argument("-i", "--input", help="Path to CSV of Youtube URLs to download",
                        required=True, type=str)
    parser.add_argument("-o", "--output", help='Folder To Store Downloaded Videos',
                        required=True, type=str)
    parser.add_argument("-n", "--min_resolution", help='The minimum resolution allowed',
                        required=False, type=str, default='360')
    parser.add_argument("-x", "--max_resolution", help='The maximum resolution allowed',
                        required=False, type=str, default='1080')
    args = vars(parser.parse_args())


    """Download all YouTube URLs from an input CSV"""
    # read csv into dataframe
    download_videos_from_csv(args['input'],
                             args['output'],
                             args['min_resolution'],
                             args['max_resolution'])

if __name__ == "__main__":
    main()

```

`deepcheat/utils/generate-requirements.sh`:

```sh
#!/bin/bash
cd ..
conda env export --from-history > environment.yml
sed -i '$ d' environment.yml
```

`deepcheat/utils/install-environment.sh`:

```sh
#Assuming conda is already installed, in the future we can run some check and install if needed
conda env create --file ../environment.yml
```

`deepcheat/utils/killshot_labels/-0iKXgFdlwc/annotations/bbox_labels_600_hierarchy.json`:

```json
{
    "LabelName": "/m/0",
    "Subcategory": []
}

```

`deepcheat/utils/killshot_labels/-6c8yKPnts0/annotations/bbox_labels_600_hierarchy.json`:

```json
{
    "LabelName": "/m/0",
    "Subcategory": []
}

```

`deepcheat/utils/killshot_labels/-GQSo52RO_o/annotations/bbox_labels_600_hierarchy.json`:

```json
{
    "LabelName": "/m/0",
    "Subcategory": []
}

```

`deepcheat/utils/killshot_labels/-IyUS9onDDg/annotations/bbox_labels_600_hierarchy.json`:

```json
{
    "LabelName": "/m/0",
    "Subcategory": []
}

```

`deepcheat/utils/killshot_labels/-Om8-wo3vTU/annotations/bbox_labels_600_hierarchy.json`:

```json
{
    "LabelName": "/m/0",
    "Subcategory": []
}

```

`deepcheat/utils/killshot_labels/-ZVIdzrqpK0/annotations/bbox_labels_600_hierarchy.json`:

```json
{
    "LabelName": "/m/0",
    "Subcategory": []
}

```

`deepcheat/utils/killshot_labels/-bjvfYf64tM/annotations/bbox_labels_600_hierarchy.json`:

```json
{
    "LabelName": "/m/0",
    "Subcategory": []
}

```

`deepcheat/utils/killshot_labels/-r8hvrBP6TI/annotations/bbox_labels_600_hierarchy.json`:

```json
{
    "LabelName": "/m/0",
    "Subcategory": []
}

```

`deepcheat/utils/killshot_labels/02Wh2gO4Q58/annotations/bbox_labels_600_hierarchy.json`:

```json
{
    "LabelName": "/m/0",
    "Subcategory": []
}

```

`deepcheat/utils/killshot_labels/02y9VdPY8q8/annotations/bbox_labels_600_hierarchy.json`:

```json
{
    "LabelName": "/m/0",
    "Subcategory": []
}

```

`deepcheat/utils/link_retrieval.py`:

```py
import argparse
import os
import pandas as pd
import requests
import validators
from common import ensure_dir_exists
from pathlib import Path

# Set up command line arguments
parser = argparse.ArgumentParser(description="Get URL's from API and store them locally")
parser.add_argument("-e", "--endpoint", help='Target URL on server', required=False,
                    default="https://waldo.vision/api/analysis/urls", type=str)
parser.add_argument("-k", "--key", help='API Key', required=False,
                    default=os.environ.get("WALDO_API_KEY"), type=str)
parser.add_argument("-i", "--id", help='API Key ID', required=False, type=str,
                    default=os.environ.get("WALDO_API_ID"))
parser.add_argument("-o", "--output", help='Folder to store output', required=True, type=str)
parser.add_argument('--minreviews', help='Minimum number of reviews', required=True, type=int)
parser.add_argument('--rating', help='Minimum rating', required=True, type=int)

args = vars(parser.parse_args())

def parse_data(data):
    """
    Convert the data to a pandas DataFrame and validate the URLs.

    :param data: A dictionary containing the API response data
    :return: A pandas DataFrame containing the parsed data
    """
    try:
        response_dataframe = pd.DataFrame(data)
        response_list = response_dataframe['gameplay'].tolist()
        response_dataframe = pd.DataFrame(columns=['id', 'url', 'game'])

        for obj in response_list:
            obj_dataframe = pd.DataFrame(obj, index=[0])
            obj_dataframe.rename(columns={"id": "id", "ytUrl": "url", "game": "game"}, inplace=True)
            response_dataframe = pd.concat([response_dataframe, obj_dataframe], ignore_index=True)

        # Validate the URLs
        for row in response_dataframe['url']:
            if not validators.url(row):
                return print("Invalid URL: " + row['url'])

        return response_dataframe
    except Exception as e:
        print(f"Error while parsing data: {e}")
        return pd.DataFrame(columns=['id', 'url', 'game'])

def main():
    """
    Pull URLs from the API that meet the criteria specified in the requirements argument.

    :param requirements: A dictionary containing the requirements for the URLs
    """
    endpoint = args['endpoint']
    params = {
        'rating': args['rating'],
        'minReviews': args['minreviews'],
        "page": 0
    }
    headers =  {'authorization': args['key'], 'authorization_id': args['id']}


    # Make the API request and retrieve the data
    try:
        response = requests.get(endpoint, params=params, headers=headers, timeout=10)
        data = response.json()
        print(data)
        total_pages = data["totalPages"]

        valid_urls = pd.DataFrame(columns=['id', 'url', 'game'])
        for page in range(0, total_pages + 1):  # Query all pages sequentially
            params["page"] = page  # Update page number
            print(f"Requesting page {page}")
            response = requests.get(endpoint, params=params, headers=headers, timeout=10)
            data = response.json()
            valid_urls = pd.concat([valid_urls, parse_data(data)], ignore_index=True)

        # Filter out duplicate links
        valid_urls.drop_duplicates(subset=["url"], inplace=True)
        download_dir = args['output']

        # Ensure output directory exists
        ensure_dir_exists(Path(download_dir))

        # Save the downloaded links to a file
        valid_urls_df = pd.DataFrame(valid_urls)
        valid_urls_df.to_csv(os.path.join(Path(download_dir), "links.csv"), index=True, columns=["id", "url", "game"])
    except requests.exceptions.Timeout as timeout_error:
        print(f"Request timed out: {timeout_error}")
    except requests.exceptions.TooManyRedirects as redirect_error:
        print(f"Too many redirects: {redirect_error}")
    except requests.exceptions.RequestException as request_error:
        print(f"Request failed: {request_error}")
    except Exception as e:
        print(f"An error occurred: {e}")
if __name__ == "__main__":
    main()

```

`deepcheat/utils/make_traintest.py`:

```py
import os
import argparse
import random

def main(directory):
    train_csv_lines = []
    test_csv_lines = []

    for subdir, _, files in os.walk(directory):
        if subdir == directory:  # Skip the root directory
            continue

        num_files = len(files)
        
        if num_files == 0:
            continue

        line_format = f"{subdir} 0 {num_files - 1}"

        # Randomly decide whether to include in train or test set
        if random.random() <= 0.9:
            train_csv_lines.append(line_format)
        else:
            test_csv_lines.append(line_format)

    with open("train.csv", "w") as train_csv:
        train_csv.write("\n".join(train_csv_lines))

    with open("test.csv", "w") as test_csv:
        test_csv.write("\n".join(test_csv_lines))

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Generate train and test CSV files.")
    parser.add_argument("directory", help="Path to the directory containing subdirectories.")

    args = parser.parse_args()
    main(args.directory)

```

`deepcheat/utils/manual_cheater_dataset.py`:

```py
import os
import csv
import argparse

def create_dataset_csv(txt_dir, img_dir, output_csv):
    # List to store rows for csv
    dataset = []
    
    # 1. Iterate over the directory containing the .mp4.txt files.
    for txt_file in os.listdir(txt_dir):
        if txt_file.endswith('.mp4.txt'):
            file_base_name = txt_file.replace('.mp4.txt', '')  # e.g., 'filename' for 'filename.mp4.txt'
            
            # Check for "Aim-Assist ON" in the filename
            cheater = 1 if "Aim-Assist ON" in txt_file else 0

            # Get the corresponding directory for this txt_file in img_dir
            corresponding_dir = os.path.join(img_dir, file_base_name)
            
            # Check if the directory exists
            if os.path.exists(corresponding_dir):
                # Read the indices from the txt file
                with open(os.path.join(txt_dir, txt_file), 'r') as f:
                    indices = f.readline().strip().split(',')
                    # 2. For each index, create a corresponding entry in the dataset
                    for idx in indices:
                        img_name = f"img_{int(idx):010}.jpg"  # Format the index to 10 digits
                        dataset.append([corresponding_dir, img_name, idx, cheater])
    
    # 3. Write the dataset to the CSV file
    with open(output_csv, 'w', newline='') as csvfile:
        csvwriter = csv.writer(csvfile)
        csvwriter.writerow(['Directory Path', 'Filename', 'Index'])  # Writing the headers
        csvwriter.writerows(dataset)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Center crop videos in a directory.")
    parser.add_argument("--txt_dir", required=True, help="Source directory containing the videos.")
    parser.add_argument("--img_dir", required=True, help="Destination directory to save the cropped videos.")
    parser.add_argument("--output_csv_file", type=str, default="simplecheater.csv", help="Size of the center crop.")

    args = parser.parse_args()
    create_dataset_csv(args.txt_dir, args.img_dir, args.output_csv_file)

```

`deepcheat/utils/new_crop.py`:

```py
import cv2
import os
import argparse
from pathlib import Path
from tqdm import tqdm

def center_crop(frame, crop_size):
    y, x, c = frame.shape
    start_x = x // 2 - (crop_size // 2)
    start_y = y // 2 - (crop_size // 2)
    return frame[start_y:start_y + crop_size, start_x:start_x + crop_size]

def process_videos(src_dir, dest_dir, crop_size, make_train_file=True):
    # Create destination directory if it doesn't exist
    Path(dest_dir).mkdir(parents=True, exist_ok=True)
    if crop_size > 240:
        raise ValueError("Crop size cannot be greater than 240.")
    #max_frames = max([ cv2.VideoCapture(os.path.join(src_dir, filename)).get(cv2.CAP_PROP_FRAME_COUNT) for filename in os.listdir(src_dir)])
    # Create a file to hold the annotations
    with open(os.path.join(dest_dir,"train.csv"), "w") as train_csv:
        for filename in tqdm(os.listdir(src_dir)):
            video_path = os.path.join(src_dir, filename)
            cap = cv2.VideoCapture(video_path)
            if not cap.isOpened():
                print(f"Skipped {filename}: Could not open the file.")
                continue

            # Create a directory for each video's frames
            frame_dir = os.path.join(dest_dir, filename.split('.')[0])
            Path(frame_dir).mkdir(parents=True, exist_ok=True)

            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            annotation = f"{frame_dir} {total_frames-1} 0\n"

            # Write annotation to train.csv
            if make_train_file: train_csv.write(annotation)

            frame_number = 0
            pbar = tqdm(total=total_frames)
            while True:
                ret, frame = cap.read()
                if not ret:
                    break

                # Center crop
                cropped_frame = center_crop(frame, crop_size)

                # Save frame as image
                zz=10
                img_filename = f"img_{frame_number:0{zz}d}.jpg"
                #print(img_filename)
                #exit()
                img_path = os.path.join(frame_dir, img_filename)
                cv2.imwrite(img_path, cropped_frame)

                frame_number += 1
                pbar.update(1)

            cap.release()

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Center crop videos in a directory.")
    parser.add_argument("--src_dir", required=True, help="Source directory containing the videos.")
    parser.add_argument("--dest_dir", required=True, help="Destination directory to save the cropped videos.")
    parser.add_argument("--crop_size", type=int, required=True, help="Size of the center crop.")

    args = parser.parse_args()
    process_videos(args.src_dir, args.dest_dir, args.crop_size)

```

`deepcheat/utils/nick_crop.py`:

```py
import cv2
import os
import argparse
from pathlib import Path
from tqdm import tqdm

def center_crop(frame, crop_size):
    y, x, c = frame.shape
    start_x = x // 2 - (crop_size // 2)
    start_y = y // 2 - (crop_size // 2)
    return frame[start_y:start_y + crop_size, start_x:start_x + crop_size]

def process_videos(src_dir, dest_dir, crop_size, make_train_file=True):
    # Create destination directory if it doesn't exist
    Path(dest_dir).mkdir(parents=True, exist_ok=True)
    if crop_size > 240:
        raise ValueError("Crop size cannot be greater than 240.")
    #max_frames = max([ cv2.VideoCapture(os.path.join(src_dir, filename)).get(cv2.CAP_PROP_FRAME_COUNT) for filename in os.listdir(src_dir)])
    # Create a file to hold the annotations
    with open(os.path.join(dest_dir,"train.csv"), "w") as train_csv:
        for filename in tqdm(os.listdir(src_dir)):
            video_path = os.path.join(src_dir, filename)
            cap = cv2.VideoCapture(video_path)
            if not cap.isOpened():
                print(f"Skipped {filename}: Could not open the file.")
                continue

            # Create a directory for each video's frames
            frame_dir = os.path.join(dest_dir, filename.split('.')[0])
            frame_dir = os.path.abspath(frame_dir)
            Path(frame_dir).mkdir(parents=True, exist_ok=True)

            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            annotation = f"{frame_dir} {15} 1\n"

            # Write annotation to train.csv
            if make_train_file: train_csv.write(annotation)

            frame_number = 0
            save_frame_number = 0  # This will be used to label frames as 0-15
            pbar = tqdm(total=total_frames)

            while True:
                ret, frame = cap.read()
                if not ret:
                    break

                if 85 <= frame_number <= 100:
                    # Center crop
                    cropped_frame = center_crop(frame, crop_size)

                    # Save frame as image
                    zz = 10
                    img_filename = f"img_{save_frame_number:0{zz}d}.jpg"
                    img_path = os.path.join(frame_dir, img_filename)
                    cv2.imwrite(img_path, cropped_frame)

                    save_frame_number += 1  # Increment the save frame number only for frames in the desired range

                frame_number += 1  # Increment the overall frame number regardless
                pbar.update(1)

            cap.release()

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Center crop videos in a directory.")
    parser.add_argument("--src_dir", required=True, help="Source directory containing the videos.")
    parser.add_argument("--dest_dir", required=True, help="Destination directory to save the cropped videos.")
    parser.add_argument("--crop_size", type=int, required=True, help="Size of the center crop.")

    args = parser.parse_args()
    process_videos(args.src_dir, args.dest_dir, args.crop_size)

```

`deepcheat/utils/parse_killshot_labels.py`:

```py
import os
import csv
from sklearn.model_selection import train_test_split

def extract_frame_index(frame_id):
    """
    Extract the integer part from the frame ID.
    Example: "img_0000002450" -> 2450
    """
    return int(frame_id.split('_')[-1])

def parse_labels(label_dir, video_dir):
    """
    Parse the annotation CSV files and returns the required dataset.
    """
    all_data = []

    # List all directories under the killshot_labels directory
    for dir_name in os.listdir(label_dir):
        if "train.csv" in dir_name: continue
        if "test.csv" in dir_name: continue
        #if not os.path.isdir(dir_name): continue
        annotation_file = os.path.join(label_dir, dir_name, "annotations", "default-annotations-human-imagelabels.csv")

        # Parse the CSV
        with open(annotation_file, 'r') as f:
            reader = csv.reader(f)
            next(reader)  # skip the header
            for row in reader:
                frame_id = row[0]
                frame_index = extract_frame_index(frame_id)
                frame_of_kill = frame_id + ".jpg"
                path_to_frame_dir = os.path.join(video_dir, dir_name)
                all_data.append([path_to_frame_dir, frame_of_kill, frame_index])

    return all_data

def save_to_csv(data, filename):
    """
    Save the data to a CSV file.
    """
    with open(filename, 'w') as f:
        writer = csv.writer(f)
        writer.writerow(["path_to_frame_dir", "frame_of_kill", "index_of_frame"])
        writer.writerows(data)

def main(label_dir, video_dir):
    # Parse the labels
    data = parse_labels(label_dir, video_dir)

    # Split the data into train and test
    train_data, test_data = train_test_split(data, test_size=0.1, random_state=42)

    # Save the datasets to CSV files
    save_to_csv(train_data, os.path.join(label_dir, 'train.csv'))
    save_to_csv(test_data, os.path.join(label_dir, 'test.csv'))

if __name__ == '__main__':
    import argparse

    parser = argparse.ArgumentParser()
    parser.add_argument('label_dir', help='Path to the killshot_labels directory')
    parser.add_argument('video_dir', help='Path to the video frames directory')
    
    args = parser.parse_args()

    main(args.label_dir, args.video_dir)


```

`deepcheat/utils/raw2_traintest.py`:

```py
import pandas as pd
import argparse
from sklearn.model_selection import train_test_split
parser = argparse.ArgumentParser(description='Split a CSV file into train and test sets.')
parser.add_argument('input_csv', type=str, help='Path to the input CSV file.')
parser.add_argument('output_train', type=str, help='Path to save the train CSV.')
parser.add_argument('output_test', type=str, help='Path to save the test CSV.')
parser.add_argument('--test_size', type=float, default=0.1, help='Proportion of the dataset to include in the test split (default: 0.1).')
args = parser.parse_args()
data = pd.read_csv(args.input_csv)
train, test = train_test_split(data, test_size=args.test_size, random_state=42)
train.to_csv(args.output_train, index=False)
test.to_csv(args.output_test, index=False)
print(f"Split {args.input_csv} into {args.output_train} (train) and {args.output_test} (test).")

#python split_csv.py input.csv train.csv test.csv



exit()
'''

# Import the necessary libraries
import torch
import torch.nn as nn
import torch.optim as optim

# Explanation:
# - torch: The main PyTorch library
# - nn: Neural Network layers and loss functions
# - optim: Optimization algorithms like SGD, Adam, etc.

# Define a simplified Transformer-based Masked AutoEncoder (MAE)
class SimpleMAE(nn.Module):
    def __init__(self, input_dim, hidden_dim):
        super(SimpleMAE, self).__init__()
        
        # Encoder layer
        self.encoder = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(d_model=input_dim, nhead=2),
            num_layers=1
        )
        
        # Decoder layer
        self.decoder = nn.Linear(hidden_dim, input_dim)

    def forward(self, x):
        x = self.encoder(x)
        x = self.decoder(x)
        return x

# Explanation:
# - Encoder: Transformer layer that compresses the input
# - Decoder: Linear layer to reconstruct the original data



# Define the custom dataset class
class TimeSeriesDataset(Dataset):
    def __init__(self, json_file_path, k=5):
        with open(json_file_path, 'r') as f:
            self.data = json.load(f)
        
        self.timestamps = sorted(list(self.data.keys()))
        self.k = k  # Number of sequential timestamps to return

    def __len__(self):
        return len(self.timestamps) - self.k + 1  # Adjust length to accommodate k sequential samples

    def __getitem__(self, idx):
        selected_timestamps = self.timestamps[idx: idx+self.k]
        sequential_data = [self.data[t] for t in selected_timestamps]
        return torch.tensor(sequential_data)

# Explanation:
# - __init__: Reads the JSON and sorts the timestamps
# - __len__: Ensures the dataset length reflects the sequence length (k)
# - __getitem__: Returns k sequential timestamps and their associated data

# Initialize the dataset and data loader
dataset = TimeSeriesDataset(json_file_path='your_data.json', k=5)
dataloader = DataLoader(dataset, batch_size=32, shuffle=True)

# Explanation:
# - Initialize our TimeSeriesDataset
# - DataLoader with batch size of 32 and shuffling enabled


# Model definition
class MaskedAutoEncoder(nn.Module):
    def __init__(self, input_dim, hidden_dim):
        super(MaskedAutoEncoder, self).__init__()

        # MLP
        self.mlp = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
        )
        
        # Transformer Encoder
        self.transformer_encoder = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=4),
            num_layers=3
        )
        
        # Transformer Decoder
        self.transformer_decoder = nn.TransformerDecoder(
            nn.TransformerDecoderLayer(d_model=hidden_dim, nhead=4),
            num_layers=3
        )

    # Explanation:
	# - MLP is for feature transformation
	# - Transformer Encoder and Decoder for sequence encoding and decoding
	# - Masking is used before Decoder


    def forward(self, x, mask):
        # MLP layer
        x = self.mlp(x)
        
        # Positional encoding
        # Assuming x shape: [batch_size, seq_len, hidden_dim]
        pos_encoding = torch.arange(0, x.size(1)).unsqueeze(0).float()
        x += pos_encoding
        
        # Add CLS token
        cls_token = torch.zeros(x.size(0), 1, x.size(2))  # Shape: [batch_size, 1, hidden_dim]
        x = torch.cat([cls_token, x], dim=1)  # New shape: [batch_size, seq_len + 1, hidden_dim]
        
        # Encoder
        x = self.transformer_encoder(x)
        
        # Masking
        masked_x = x * mask.unsqueeze(-1).float()
        
        # Decoder
        output = self.transformer_decoder(masked_x, x)
        
        return output



# Initialize model, optimizer, and loss function
model = MaskedAutoEncoder(input_dim=128, hidden_dim=64)
optimizer = optim.Adam(model.parameters(), lr=0.001)
loss_fn = nn.BCEWithLogitsLoss()

# Create a mask
for epoch in range(epochs)?:
	for x, y in dataloader:  # Assuming shape: [batch_size, k, input_dim]
	    optimizer.zero_grad()
	    
	    # Forward pass for cheater detection
	    output = model(x)
	    loss = loss_fn(output, y)
	    
	    # Backpropagation
	    loss.backward()
	    optimizer.step()

# Explanation:
# - random_mask is used for 75% masking; adjust accordingly
# - The loss is calculated only for the masked elements

'''
```

`deepcheat/utils/segmentation.py`:

```py
import argparse
import os
from pathlib import Path
from common import ensure_dir_exists
from paddleocr import PaddleOCR
# from default_video_cropper import DefaultVideoCropper
import cv2

parser = argparse.ArgumentParser(
    description="Segment video frames for using timeframes", add_help=False
)
args = vars(parser.parse_args())

# set directory path
frames_path = Path('./')
ensure_dir_exists(frames_path)


ocr = PaddleOCR(use_angle_cls=True)

# loop through each file in the directory
for file in os.listdir(frames_path):
    # check if the file extension is .jpg or .jpeg
    if file.endswith('.jpg'):
        # print the filename if it is a JPG file
        image = cv2.imread(file)
        height, width = image.shape[:2]
        crop_width = int(width * 0.2)
        crop_height = int(height * 0.2)
        left = crop_width
        top = 0
        right = width
        bottom = crop_height
        cropped_image = image[top:crop_height, crop_width:width]
        #cv2.imwrite(f"{output_path}/{os.path.basename(file).split('/')[-1]}", cropped_image)
        result = ocr.ocr(cropped_image)
        for line in result:
            for word in line:
                if word[1][1] >= 0.85:
                    print(word[1])
```

`deepcheat/utils/simple_cheater_dataset.py`:

```py
import os
import csv
import argparse

def create_dataset_csv(txt_dir, img_dir, output_csv):
    # List to store rows for csv
    dataset = []
    
    # 1. Iterate over the directory containing the .mp4.txt files.
    for txt_file in os.listdir(txt_dir):
        if txt_file.endswith('.mp4.txt'):
            file_base_name = txt_file.replace('.mp4.txt', '')  # e.g., 'filename' for 'filename.mp4.txt'
            
            # Check for "Aim-Assist ON" in the filename
            cheater = 1 if "Aim-Assist ON" in txt_file else 0

            # Get the corresponding directory for this txt_file in img_dir
            corresponding_dir = os.path.join(img_dir, file_base_name)
            
            # Check if the directory exists
            if os.path.exists(corresponding_dir):
                # Read the indices from the txt file
                with open(os.path.join(txt_dir, txt_file), 'r') as f:
                    indices = f.readline().strip().split(',')
                    # 2. For each index, create a corresponding entry in the dataset
                    for idx in indices:
                        img_name = f"img_{int(idx):010}.jpg"  # Format the index to 10 digits
                        dataset.append([corresponding_dir, img_name, idx, cheater])
    
    # 3. Write the dataset to the CSV file
    with open(output_csv, 'w', newline='') as csvfile:
        csvwriter = csv.writer(csvfile)
        csvwriter.writerow(['Directory Path', 'Filename', 'Index'])  # Writing the headers
        csvwriter.writerows(dataset)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Center crop videos in a directory.")
    parser.add_argument("--txt_dir", required=True, help="Source directory containing the videos.")
    parser.add_argument("--img_dir", required=True, help="Destination directory to save the cropped videos.")
    parser.add_argument("--output_csv_file", type=str, default="simplecheater.csv", help="Size of the center crop.")

    args = parser.parse_args()
    create_dataset_csv(args.txt_dir, args.img_dir, args.output_csv_file)

```

`environment.yml`:

```yml
name: cs2-detect-env
channels:
  - conda-forge
  - defaults
  - pytorch
dependencies:
  - python=3.9
  # Core ML packages with exact versions from working system
  - pytorch=2.5.1=*cuda*
  - torchvision=0.20.1=*cuda*
  - torchaudio=2.5.1=*cuda*
  - timm=0.4.12
  - einops=0.8.1
  - triton=3.1.0

  # Video/Audio processing - critical versions
  - ffmpeg=6.1.1
  - opencv=4.10.0
  - av=12.3.0
  - librosa=0.11.0

  # Web framework
  - flask=3.1.0
  - werkzeug

  # Data science core
  - numpy=1.26.4
  - pandas=2.3.1
  - scipy=1.13.1
  - matplotlib=3.9.2
  - pillow=11.3.0
  - scikit-learn

  # ML training tools
  - deepspeed=0.17.4

  # Utilities
  - requests
  - validators
  - pylint
  - pytube
  - absl-py=2.1.0
  - grpcio=1.71.0
  - defaults::libabseil=20250127.0=cxx17_h6a678d5_0
  - pip

  - pip:
    - decord
    - soundfile
    - tensorboard==2.9.0
    - tensorboardX==2.6.2
    - protobuf==3.20.3
```

`evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/cheater_preds.txt`:

```txt
-0.5149
-0.5157
-0.5157
-0.5114
-0.5170
-0.5147
-0.5122
-0.5127
-0.5124
-0.5184
-0.5180
-0.5129
-0.5110
-0.5135
-0.5093
-0.5103
-0.5146
-0.5120
-0.5152
-0.5163
-0.5153
-0.5070
-0.5143
-0.5129
-0.5160
-0.5119
-0.5102
-0.5149
-0.5098
-0.5140
-0.5097
-0.5159
-0.5144
-0.5153
-0.5143
-0.5147
-0.5116
-0.5171
-0.5086
-0.5100
-0.5136
-0.5178
-0.5150
-0.5113
-0.5145
-0.5143
-0.5124
-0.5169
-0.5111
-0.5123

```

`evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/results.json`:

```json
{
  "clip_results": [
    {
      "id": 0,
      "filename": "2025-09-17_14-01-12_clip_1.mp4",
      "clip_name": "2025-09-17_14-01-12_clip_1",
      "normalized_score": -0.5149,
      "probability": 0.013469281037187955,
      "confidence": {
        "category": "Very Low Confidence",
        "label": "Likely Legitimate",
        "color": "#28a745",
        "level": 1
      },
      "video_path": "/home/nicholas/Desktop/Waldo_alpha_01/processed_clips/clips_2025-09-17_14-01-12_ffa2/2025-09-17_14-01-12_clip_1.mp4",
      "frame_paths": [
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_1/img_0000000000.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_1/img_0000000001.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_1/img_0000000002.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_1/img_0000000003.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_1/img_0000000004.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_1/img_0000000005.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_1/img_0000000006.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_1/img_0000000007.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_1/img_0000000008.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_1/img_0000000009.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_1/img_0000000010.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_1/img_0000000011.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_1/img_0000000012.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_1/img_0000000013.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_1/img_0000000014.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_1/img_0000000015.jpg"
      ]
    },
    {
      "id": 1,
      "filename": "2025-09-17_14-01-12_clip_10.mp4",
      "clip_name": "2025-09-17_14-01-12_clip_10",
      "normalized_score": -0.5157,
      "probability": 0.01342174203917194,
      "confidence": {
        "category": "Very Low Confidence",
        "label": "Likely Legitimate",
        "color": "#28a745",
        "level": 1
      },
      "video_path": "/home/nicholas/Desktop/Waldo_alpha_01/processed_clips/clips_2025-09-17_14-01-12_ffa2/2025-09-17_14-01-12_clip_10.mp4",
      "frame_paths": [
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_10/img_0000000000.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_10/img_0000000001.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_10/img_0000000002.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_10/img_0000000003.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_10/img_0000000004.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_10/img_0000000005.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_10/img_0000000006.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_10/img_0000000007.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_10/img_0000000008.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_10/img_0000000009.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_10/img_0000000010.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_10/img_0000000011.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_10/img_0000000012.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_10/img_0000000013.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_10/img_0000000014.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_10/img_0000000015.jpg"
      ]
    },
    {
      "id": 2,
      "filename": "2025-09-17_14-01-12_clip_11.mp4",
      "clip_name": "2025-09-17_14-01-12_clip_11",
      "normalized_score": -0.5157,
      "probability": 0.01342174203917194,
      "confidence": {
        "category": "Very Low Confidence",
        "label": "Likely Legitimate",
        "color": "#28a745",
        "level": 1
      },
      "video_path": "/home/nicholas/Desktop/Waldo_alpha_01/processed_clips/clips_2025-09-17_14-01-12_ffa2/2025-09-17_14-01-12_clip_11.mp4",
      "frame_paths": [
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_11/img_0000000000.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_11/img_0000000001.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_11/img_0000000002.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_11/img_0000000003.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_11/img_0000000004.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_11/img_0000000005.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_11/img_0000000006.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_11/img_0000000007.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_11/img_0000000008.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_11/img_0000000009.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_11/img_0000000010.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_11/img_0000000011.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_11/img_0000000012.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_11/img_0000000013.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_11/img_0000000014.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_11/img_0000000015.jpg"
      ]
    },
    {
      "id": 3,
      "filename": "2025-09-17_14-01-12_clip_12.mp4",
      "clip_name": "2025-09-17_14-01-12_clip_12",
      "normalized_score": -0.5114,
      "probability": 0.01367922428392283,
      "confidence": {
        "category": "Very Low Confidence",
        "label": "Likely Legitimate",
        "color": "#28a745",
        "level": 1
      },
      "video_path": "/home/nicholas/Desktop/Waldo_alpha_01/processed_clips/clips_2025-09-17_14-01-12_ffa2/2025-09-17_14-01-12_clip_12.mp4",
      "frame_paths": [
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_12/img_0000000000.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_12/img_0000000001.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_12/img_0000000002.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_12/img_0000000003.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_12/img_0000000004.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_12/img_0000000005.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_12/img_0000000006.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_12/img_0000000007.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_12/img_0000000008.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_12/img_0000000009.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_12/img_0000000010.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_12/img_0000000011.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_12/img_0000000012.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_12/img_0000000013.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_12/img_0000000014.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_12/img_0000000015.jpg"
      ]
    },
    {
      "id": 4,
      "filename": "2025-09-17_14-01-12_clip_13.mp4",
      "clip_name": "2025-09-17_14-01-12_clip_13",
      "normalized_score": -0.517,
      "probability": 0.013344843922961529,
      "confidence": {
        "category": "Very Low Confidence",
        "label": "Likely Legitimate",
        "color": "#28a745",
        "level": 1
      },
      "video_path": "/home/nicholas/Desktop/Waldo_alpha_01/processed_clips/clips_2025-09-17_14-01-12_ffa2/2025-09-17_14-01-12_clip_13.mp4",
      "frame_paths": [
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_13/img_0000000000.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_13/img_0000000001.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_13/img_0000000002.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_13/img_0000000003.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_13/img_0000000004.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_13/img_0000000005.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_13/img_0000000006.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_13/img_0000000007.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_13/img_0000000008.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_13/img_0000000009.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_13/img_0000000010.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_13/img_0000000011.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_13/img_0000000012.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_13/img_0000000013.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_13/img_0000000014.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_13/img_0000000015.jpg"
      ]
    },
    {
      "id": 5,
      "filename": "2025-09-17_14-01-12_clip_14.mp4",
      "clip_name": "2025-09-17_14-01-12_clip_14",
      "normalized_score": -0.5147,
      "probability": 0.013481191713652115,
      "confidence": {
        "category": "Very Low Confidence",
        "label": "Likely Legitimate",
        "color": "#28a745",
        "level": 1
      },
      "video_path": "/home/nicholas/Desktop/Waldo_alpha_01/processed_clips/clips_2025-09-17_14-01-12_ffa2/2025-09-17_14-01-12_clip_14.mp4",
      "frame_paths": [
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_14/img_0000000000.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_14/img_0000000001.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_14/img_0000000002.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_14/img_0000000003.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_14/img_0000000004.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_14/img_0000000005.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_14/img_0000000006.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_14/img_0000000007.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_14/img_0000000008.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_14/img_0000000009.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_14/img_0000000010.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_14/img_0000000011.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_14/img_0000000012.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_14/img_0000000013.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_14/img_0000000014.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_14/img_0000000015.jpg"
      ]
    },
    {
      "id": 6,
      "filename": "2025-09-17_14-01-12_clip_15.mp4",
      "clip_name": "2025-09-17_14-01-12_clip_15",
      "normalized_score": -0.5122,
      "probability": 0.013630954542335011,
      "confidence": {
        "category": "Very Low Confidence",
        "label": "Likely Legitimate",
        "color": "#28a745",
        "level": 1
      },
      "video_path": "/home/nicholas/Desktop/Waldo_alpha_01/processed_clips/clips_2025-09-17_14-01-12_ffa2/2025-09-17_14-01-12_clip_15.mp4",
      "frame_paths": [
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_15/img_0000000000.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_15/img_0000000001.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_15/img_0000000002.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_15/img_0000000003.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_15/img_0000000004.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_15/img_0000000005.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_15/img_0000000006.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_15/img_0000000007.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_15/img_0000000008.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_15/img_0000000009.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_15/img_0000000010.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_15/img_0000000011.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_15/img_0000000012.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_15/img_0000000013.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_15/img_0000000014.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_15/img_0000000015.jpg"
      ]
    },
    {
      "id": 7,
      "filename": "2025-09-17_14-01-12_clip_16.mp4",
      "clip_name": "2025-09-17_14-01-12_clip_16",
      "normalized_score": -0.5127,
      "probability": 0.013600871293967677,
      "confidence": {
        "category": "Very Low Confidence",
        "label": "Likely Legitimate",
        "color": "#28a745",
        "level": 1
      },
      "video_path": "/home/nicholas/Desktop/Waldo_alpha_01/processed_clips/clips_2025-09-17_14-01-12_ffa2/2025-09-17_14-01-12_clip_16.mp4",
      "frame_paths": [
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_16/img_0000000000.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_16/img_0000000001.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_16/img_0000000002.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_16/img_0000000003.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_16/img_0000000004.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_16/img_0000000005.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_16/img_0000000006.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_16/img_0000000007.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_16/img_0000000008.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_16/img_0000000009.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_16/img_0000000010.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_16/img_0000000011.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_16/img_0000000012.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_16/img_0000000013.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_16/img_0000000014.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_16/img_0000000015.jpg"
      ]
    },
    {
      "id": 8,
      "filename": "2025-09-17_14-01-12_clip_17.mp4",
      "clip_name": "2025-09-17_14-01-12_clip_17",
      "normalized_score": -0.5124,
      "probability": 0.013618913376551965,
      "confidence": {
        "category": "Very Low Confidence",
        "label": "Likely Legitimate",
        "color": "#28a745",
        "level": 1
      },
      "video_path": "/home/nicholas/Desktop/Waldo_alpha_01/processed_clips/clips_2025-09-17_14-01-12_ffa2/2025-09-17_14-01-12_clip_17.mp4",
      "frame_paths": [
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_17/img_0000000000.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_17/img_0000000001.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_17/img_0000000002.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_17/img_0000000003.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_17/img_0000000004.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_17/img_0000000005.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_17/img_0000000006.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_17/img_0000000007.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_17/img_0000000008.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_17/img_0000000009.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_17/img_0000000010.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_17/img_0000000011.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_17/img_0000000012.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_17/img_0000000013.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_17/img_0000000014.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_17/img_0000000015.jpg"
      ]
    },
    {
      "id": 9,
      "filename": "2025-09-17_14-01-12_clip_18.mp4",
      "clip_name": "2025-09-17_14-01-12_clip_18",
      "normalized_score": -0.5184,
      "probability": 0.013262516589032056,
      "confidence": {
        "category": "Very Low Confidence",
        "label": "Likely Legitimate",
        "color": "#28a745",
        "level": 1
      },
      "video_path": "/home/nicholas/Desktop/Waldo_alpha_01/processed_clips/clips_2025-09-17_14-01-12_ffa2/2025-09-17_14-01-12_clip_18.mp4",
      "frame_paths": [
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_18/img_0000000000.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_18/img_0000000001.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_18/img_0000000002.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_18/img_0000000003.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_18/img_0000000004.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_18/img_0000000005.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_18/img_0000000006.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_18/img_0000000007.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_18/img_0000000008.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_18/img_0000000009.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_18/img_0000000010.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_18/img_0000000011.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_18/img_0000000012.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_18/img_0000000013.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_18/img_0000000014.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_18/img_0000000015.jpg"
      ]
    },
    {
      "id": 10,
      "filename": "2025-09-17_14-01-12_clip_19.mp4",
      "clip_name": "2025-09-17_14-01-12_clip_19",
      "normalized_score": -0.518,
      "probability": 0.013285987420530243,
      "confidence": {
        "category": "Very Low Confidence",
        "label": "Likely Legitimate",
        "color": "#28a745",
        "level": 1
      },
      "video_path": "/home/nicholas/Desktop/Waldo_alpha_01/processed_clips/clips_2025-09-17_14-01-12_ffa2/2025-09-17_14-01-12_clip_19.mp4",
      "frame_paths": [
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_19/img_0000000000.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_19/img_0000000001.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_19/img_0000000002.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_19/img_0000000003.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_19/img_0000000004.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_19/img_0000000005.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_19/img_0000000006.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_19/img_0000000007.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_19/img_0000000008.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_19/img_0000000009.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_19/img_0000000010.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_19/img_0000000011.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_19/img_0000000012.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_19/img_0000000013.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_19/img_0000000014.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_19/img_0000000015.jpg"
      ]
    },
    {
      "id": 11,
      "filename": "2025-09-17_14-01-12_clip_2.mp4",
      "clip_name": "2025-09-17_14-01-12_clip_2",
      "normalized_score": -0.5129,
      "probability": 0.013588856336691447,
      "confidence": {
        "category": "Very Low Confidence",
        "label": "Likely Legitimate",
        "color": "#28a745",
        "level": 1
      },
      "video_path": "/home/nicholas/Desktop/Waldo_alpha_01/processed_clips/clips_2025-09-17_14-01-12_ffa2/2025-09-17_14-01-12_clip_2.mp4",
      "frame_paths": [
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_2/img_0000000000.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_2/img_0000000001.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_2/img_0000000002.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_2/img_0000000003.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_2/img_0000000004.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_2/img_0000000005.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_2/img_0000000006.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_2/img_0000000007.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_2/img_0000000008.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_2/img_0000000009.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_2/img_0000000010.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_2/img_0000000011.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_2/img_0000000012.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_2/img_0000000013.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_2/img_0000000014.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_2/img_0000000015.jpg"
      ]
    },
    {
      "id": 12,
      "filename": "2025-09-17_14-01-12_clip_20.mp4",
      "clip_name": "2025-09-17_14-01-12_clip_20",
      "normalized_score": -0.511,
      "probability": 0.0137034223264762,
      "confidence": {
        "category": "Very Low Confidence",
        "label": "Likely Legitimate",
        "color": "#28a745",
        "level": 1
      },
      "video_path": "/home/nicholas/Desktop/Waldo_alpha_01/processed_clips/clips_2025-09-17_14-01-12_ffa2/2025-09-17_14-01-12_clip_20.mp4",
      "frame_paths": [
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_20/img_0000000000.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_20/img_0000000001.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_20/img_0000000002.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_20/img_0000000003.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_20/img_0000000004.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_20/img_0000000005.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_20/img_0000000006.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_20/img_0000000007.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_20/img_0000000008.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_20/img_0000000009.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_20/img_0000000010.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_20/img_0000000011.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_20/img_0000000012.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_20/img_0000000013.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_20/img_0000000014.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_20/img_0000000015.jpg"
      ]
    },
    {
      "id": 13,
      "filename": "2025-09-17_14-01-12_clip_21.mp4",
      "clip_name": "2025-09-17_14-01-12_clip_21",
      "normalized_score": -0.5135,
      "probability": 0.013552874236676157,
      "confidence": {
        "category": "Very Low Confidence",
        "label": "Likely Legitimate",
        "color": "#28a745",
        "level": 1
      },
      "video_path": "/home/nicholas/Desktop/Waldo_alpha_01/processed_clips/clips_2025-09-17_14-01-12_ffa2/2025-09-17_14-01-12_clip_21.mp4",
      "frame_paths": [
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_21/img_0000000000.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_21/img_0000000001.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_21/img_0000000002.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_21/img_0000000003.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_21/img_0000000004.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_21/img_0000000005.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_21/img_0000000006.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_21/img_0000000007.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_21/img_0000000008.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_21/img_0000000009.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_21/img_0000000010.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_21/img_0000000011.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_21/img_0000000012.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_21/img_0000000013.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_21/img_0000000014.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_21/img_0000000015.jpg"
      ]
    },
    {
      "id": 14,
      "filename": "2025-09-17_14-01-12_clip_22.mp4",
      "clip_name": "2025-09-17_14-01-12_clip_22",
      "normalized_score": -0.5093,
      "probability": 0.01380673577416037,
      "confidence": {
        "category": "Very Low Confidence",
        "label": "Likely Legitimate",
        "color": "#28a745",
        "level": 1
      },
      "video_path": "/home/nicholas/Desktop/Waldo_alpha_01/processed_clips/clips_2025-09-17_14-01-12_ffa2/2025-09-17_14-01-12_clip_22.mp4",
      "frame_paths": [
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_22/img_0000000000.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_22/img_0000000001.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_22/img_0000000002.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_22/img_0000000003.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_22/img_0000000004.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_22/img_0000000005.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_22/img_0000000006.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_22/img_0000000007.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_22/img_0000000008.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_22/img_0000000009.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_22/img_0000000010.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_22/img_0000000011.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_22/img_0000000012.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_22/img_0000000013.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_22/img_0000000014.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_22/img_0000000015.jpg"
      ]
    },
    {
      "id": 15,
      "filename": "2025-09-17_14-01-12_clip_23.mp4",
      "clip_name": "2025-09-17_14-01-12_clip_23",
      "normalized_score": -0.5103,
      "probability": 0.013745870510994334,
      "confidence": {
        "category": "Very Low Confidence",
        "label": "Likely Legitimate",
        "color": "#28a745",
        "level": 1
      },
      "video_path": "/home/nicholas/Desktop/Waldo_alpha_01/processed_clips/clips_2025-09-17_14-01-12_ffa2/2025-09-17_14-01-12_clip_23.mp4",
      "frame_paths": [
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_23/img_0000000000.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_23/img_0000000001.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_23/img_0000000002.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_23/img_0000000003.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_23/img_0000000004.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_23/img_0000000005.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_23/img_0000000006.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_23/img_0000000007.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_23/img_0000000008.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_23/img_0000000009.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_23/img_0000000010.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_23/img_0000000011.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_23/img_0000000012.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_23/img_0000000013.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_23/img_0000000014.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_23/img_0000000015.jpg"
      ]
    },
    {
      "id": 16,
      "filename": "2025-09-17_14-01-12_clip_24.mp4",
      "clip_name": "2025-09-17_14-01-12_clip_24",
      "normalized_score": -0.5146,
      "probability": 0.013487150946973158,
      "confidence": {
        "category": "Very Low Confidence",
        "label": "Likely Legitimate",
        "color": "#28a745",
        "level": 1
      },
      "video_path": "/home/nicholas/Desktop/Waldo_alpha_01/processed_clips/clips_2025-09-17_14-01-12_ffa2/2025-09-17_14-01-12_clip_24.mp4",
      "frame_paths": [
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_24/img_0000000000.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_24/img_0000000001.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_24/img_0000000002.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_24/img_0000000003.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_24/img_0000000004.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_24/img_0000000005.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_24/img_0000000006.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_24/img_0000000007.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_24/img_0000000008.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_24/img_0000000009.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_24/img_0000000010.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_24/img_0000000011.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_24/img_0000000012.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_24/img_0000000013.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_24/img_0000000014.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_24/img_0000000015.jpg"
      ]
    },
    {
      "id": 17,
      "filename": "2025-09-17_14-01-12_clip_25.mp4",
      "clip_name": "2025-09-17_14-01-12_clip_25",
      "normalized_score": -0.512,
      "probability": 0.013643006207066318,
      "confidence": {
        "category": "Very Low Confidence",
        "label": "Likely Legitimate",
        "color": "#28a745",
        "level": 1
      },
      "video_path": "/home/nicholas/Desktop/Waldo_alpha_01/processed_clips/clips_2025-09-17_14-01-12_ffa2/2025-09-17_14-01-12_clip_25.mp4",
      "frame_paths": [
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_25/img_0000000000.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_25/img_0000000001.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_25/img_0000000002.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_25/img_0000000003.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_25/img_0000000004.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_25/img_0000000005.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_25/img_0000000006.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_25/img_0000000007.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_25/img_0000000008.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_25/img_0000000009.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_25/img_0000000010.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_25/img_0000000011.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_25/img_0000000012.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_25/img_0000000013.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_25/img_0000000014.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_25/img_0000000015.jpg"
      ]
    },
    {
      "id": 18,
      "filename": "2025-09-17_14-01-12_clip_26.mp4",
      "clip_name": "2025-09-17_14-01-12_clip_26",
      "normalized_score": -0.5152,
      "probability": 0.013451434481442098,
      "confidence": {
        "category": "Very Low Confidence",
        "label": "Likely Legitimate",
        "color": "#28a745",
        "level": 1
      },
      "video_path": "/home/nicholas/Desktop/Waldo_alpha_01/processed_clips/clips_2025-09-17_14-01-12_ffa2/2025-09-17_14-01-12_clip_26.mp4",
      "frame_paths": [
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_26/img_0000000000.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_26/img_0000000001.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_26/img_0000000002.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_26/img_0000000003.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_26/img_0000000004.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_26/img_0000000005.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_26/img_0000000006.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_26/img_0000000007.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_26/img_0000000008.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_26/img_0000000009.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_26/img_0000000010.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_26/img_0000000011.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_26/img_0000000012.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_26/img_0000000013.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_26/img_0000000014.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_26/img_0000000015.jpg"
      ]
    },
    {
      "id": 19,
      "filename": "2025-09-17_14-01-12_clip_27.mp4",
      "clip_name": "2025-09-17_14-01-12_clip_27",
      "normalized_score": -0.5163,
      "probability": 0.013386196438115476,
      "confidence": {
        "category": "Very Low Confidence",
        "label": "Likely Legitimate",
        "color": "#28a745",
        "level": 1
      },
      "video_path": "/home/nicholas/Desktop/Waldo_alpha_01/processed_clips/clips_2025-09-17_14-01-12_ffa2/2025-09-17_14-01-12_clip_27.mp4",
      "frame_paths": [
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_27/img_0000000000.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_27/img_0000000001.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_27/img_0000000002.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_27/img_0000000003.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_27/img_0000000004.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_27/img_0000000005.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_27/img_0000000006.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_27/img_0000000007.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_27/img_0000000008.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_27/img_0000000009.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_27/img_0000000010.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_27/img_0000000011.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_27/img_0000000012.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_27/img_0000000013.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_27/img_0000000014.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_27/img_0000000015.jpg"
      ]
    },
    {
      "id": 20,
      "filename": "2025-09-17_14-01-12_clip_28.mp4",
      "clip_name": "2025-09-17_14-01-12_clip_28",
      "normalized_score": -0.5153,
      "probability": 0.013445490814184812,
      "confidence": {
        "category": "Very Low Confidence",
        "label": "Likely Legitimate",
        "color": "#28a745",
        "level": 1
      },
      "video_path": "/home/nicholas/Desktop/Waldo_alpha_01/processed_clips/clips_2025-09-17_14-01-12_ffa2/2025-09-17_14-01-12_clip_28.mp4",
      "frame_paths": [
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_28/img_0000000000.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_28/img_0000000001.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_28/img_0000000002.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_28/img_0000000003.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_28/img_0000000004.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_28/img_0000000005.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_28/img_0000000006.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_28/img_0000000007.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_28/img_0000000008.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_28/img_0000000009.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_28/img_0000000010.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_28/img_0000000011.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_28/img_0000000012.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_28/img_0000000013.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_28/img_0000000014.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_28/img_0000000015.jpg"
      ]
    },
    {
      "id": 21,
      "filename": "2025-09-17_14-01-12_clip_29.mp4",
      "clip_name": "2025-09-17_14-01-12_clip_29",
      "normalized_score": -0.507,
      "probability": 0.013947736122315035,
      "confidence": {
        "category": "Very Low Confidence",
        "label": "Likely Legitimate",
        "color": "#28a745",
        "level": 1
      },
      "video_path": "/home/nicholas/Desktop/Waldo_alpha_01/processed_clips/clips_2025-09-17_14-01-12_ffa2/2025-09-17_14-01-12_clip_29.mp4",
      "frame_paths": [
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_29/img_0000000000.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_29/img_0000000001.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_29/img_0000000002.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_29/img_0000000003.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_29/img_0000000004.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_29/img_0000000005.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_29/img_0000000006.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_29/img_0000000007.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_29/img_0000000008.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_29/img_0000000009.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_29/img_0000000010.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_29/img_0000000011.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_29/img_0000000012.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_29/img_0000000013.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_29/img_0000000014.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_29/img_0000000015.jpg"
      ]
    },
    {
      "id": 22,
      "filename": "2025-09-17_14-01-12_clip_3.mp4",
      "clip_name": "2025-09-17_14-01-12_clip_3",
      "normalized_score": -0.5143,
      "probability": 0.013505044240499347,
      "confidence": {
        "category": "Very Low Confidence",
        "label": "Likely Legitimate",
        "color": "#28a745",
        "level": 1
      },
      "video_path": "/home/nicholas/Desktop/Waldo_alpha_01/processed_clips/clips_2025-09-17_14-01-12_ffa2/2025-09-17_14-01-12_clip_3.mp4",
      "frame_paths": [
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_3/img_0000000000.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_3/img_0000000001.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_3/img_0000000002.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_3/img_0000000003.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_3/img_0000000004.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_3/img_0000000005.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_3/img_0000000006.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_3/img_0000000007.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_3/img_0000000008.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_3/img_0000000009.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_3/img_0000000010.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_3/img_0000000011.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_3/img_0000000012.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_3/img_0000000013.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_3/img_0000000014.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_3/img_0000000015.jpg"
      ]
    },
    {
      "id": 23,
      "filename": "2025-09-17_14-01-12_clip_30.mp4",
      "clip_name": "2025-09-17_14-01-12_clip_30",
      "normalized_score": -0.5129,
      "probability": 0.013588856336691447,
      "confidence": {
        "category": "Very Low Confidence",
        "label": "Likely Legitimate",
        "color": "#28a745",
        "level": 1
      },
      "video_path": "/home/nicholas/Desktop/Waldo_alpha_01/processed_clips/clips_2025-09-17_14-01-12_ffa2/2025-09-17_14-01-12_clip_30.mp4",
      "frame_paths": [
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_30/img_0000000000.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_30/img_0000000001.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_30/img_0000000002.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_30/img_0000000003.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_30/img_0000000004.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_30/img_0000000005.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_30/img_0000000006.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_30/img_0000000007.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_30/img_0000000008.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_30/img_0000000009.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_30/img_0000000010.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_30/img_0000000011.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_30/img_0000000012.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_30/img_0000000013.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_30/img_0000000014.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_30/img_0000000015.jpg"
      ]
    },
    {
      "id": 24,
      "filename": "2025-09-17_14-01-12_clip_31.mp4",
      "clip_name": "2025-09-17_14-01-12_clip_31",
      "normalized_score": -0.516,
      "probability": 0.013403957615925595,
      "confidence": {
        "category": "Very Low Confidence",
        "label": "Likely Legitimate",
        "color": "#28a745",
        "level": 1
      },
      "video_path": "/home/nicholas/Desktop/Waldo_alpha_01/processed_clips/clips_2025-09-17_14-01-12_ffa2/2025-09-17_14-01-12_clip_31.mp4",
      "frame_paths": [
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_31/img_0000000000.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_31/img_0000000001.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_31/img_0000000002.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_31/img_0000000003.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_31/img_0000000004.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_31/img_0000000005.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_31/img_0000000006.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_31/img_0000000007.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_31/img_0000000008.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_31/img_0000000009.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_31/img_0000000010.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_31/img_0000000011.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_31/img_0000000012.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_31/img_0000000013.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_31/img_0000000014.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_31/img_0000000015.jpg"
      ]
    },
    {
      "id": 25,
      "filename": "2025-09-17_14-01-12_clip_32.mp4",
      "clip_name": "2025-09-17_14-01-12_clip_32",
      "normalized_score": -0.5119,
      "probability": 0.013649035979316595,
      "confidence": {
        "category": "Very Low Confidence",
        "label": "Likely Legitimate",
        "color": "#28a745",
        "level": 1
      },
      "video_path": "/home/nicholas/Desktop/Waldo_alpha_01/processed_clips/clips_2025-09-17_14-01-12_ffa2/2025-09-17_14-01-12_clip_32.mp4",
      "frame_paths": [
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_32/img_0000000000.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_32/img_0000000001.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_32/img_0000000002.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_32/img_0000000003.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_32/img_0000000004.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_32/img_0000000005.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_32/img_0000000006.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_32/img_0000000007.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_32/img_0000000008.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_32/img_0000000009.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_32/img_0000000010.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_32/img_0000000011.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_32/img_0000000012.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_32/img_0000000013.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_32/img_0000000014.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_32/img_0000000015.jpg"
      ]
    },
    {
      "id": 26,
      "filename": "2025-09-17_14-01-12_clip_33.mp4",
      "clip_name": "2025-09-17_14-01-12_clip_33",
      "normalized_score": -0.5102,
      "probability": 0.01375194511212733,
      "confidence": {
        "category": "Very Low Confidence",
        "label": "Likely Legitimate",
        "color": "#28a745",
        "level": 1
      },
      "video_path": "/home/nicholas/Desktop/Waldo_alpha_01/processed_clips/clips_2025-09-17_14-01-12_ffa2/2025-09-17_14-01-12_clip_33.mp4",
      "frame_paths": [
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_33/img_0000000000.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_33/img_0000000001.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_33/img_0000000002.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_33/img_0000000003.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_33/img_0000000004.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_33/img_0000000005.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_33/img_0000000006.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_33/img_0000000007.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_33/img_0000000008.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_33/img_0000000009.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_33/img_0000000010.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_33/img_0000000011.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_33/img_0000000012.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_33/img_0000000013.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_33/img_0000000014.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_33/img_0000000015.jpg"
      ]
    },
    {
      "id": 27,
      "filename": "2025-09-17_14-01-12_clip_34.mp4",
      "clip_name": "2025-09-17_14-01-12_clip_34",
      "normalized_score": -0.5149,
      "probability": 0.013469281037187955,
      "confidence": {
        "category": "Very Low Confidence",
        "label": "Likely Legitimate",
        "color": "#28a745",
        "level": 1
      },
      "video_path": "/home/nicholas/Desktop/Waldo_alpha_01/processed_clips/clips_2025-09-17_14-01-12_ffa2/2025-09-17_14-01-12_clip_34.mp4",
      "frame_paths": [
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_34/img_0000000000.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_34/img_0000000001.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_34/img_0000000002.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_34/img_0000000003.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_34/img_0000000004.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_34/img_0000000005.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_34/img_0000000006.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_34/img_0000000007.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_34/img_0000000008.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_34/img_0000000009.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_34/img_0000000010.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_34/img_0000000011.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_34/img_0000000012.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_34/img_0000000013.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_34/img_0000000014.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_34/img_0000000015.jpg"
      ]
    },
    {
      "id": 28,
      "filename": "2025-09-17_14-01-12_clip_35.mp4",
      "clip_name": "2025-09-17_14-01-12_clip_35",
      "normalized_score": -0.5098,
      "probability": 0.013776269998375999,
      "confidence": {
        "category": "Very Low Confidence",
        "label": "Likely Legitimate",
        "color": "#28a745",
        "level": 1
      },
      "video_path": "/home/nicholas/Desktop/Waldo_alpha_01/processed_clips/clips_2025-09-17_14-01-12_ffa2/2025-09-17_14-01-12_clip_35.mp4",
      "frame_paths": [
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_35/img_0000000000.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_35/img_0000000001.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_35/img_0000000002.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_35/img_0000000003.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_35/img_0000000004.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_35/img_0000000005.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_35/img_0000000006.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_35/img_0000000007.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_35/img_0000000008.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_35/img_0000000009.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_35/img_0000000010.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_35/img_0000000011.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_35/img_0000000012.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_35/img_0000000013.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_35/img_0000000014.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_35/img_0000000015.jpg"
      ]
    },
    {
      "id": 29,
      "filename": "2025-09-17_14-01-12_clip_36.mp4",
      "clip_name": "2025-09-17_14-01-12_clip_36",
      "normalized_score": -0.514,
      "probability": 0.013522960947503736,
      "confidence": {
        "category": "Very Low Confidence",
        "label": "Likely Legitimate",
        "color": "#28a745",
        "level": 1
      },
      "video_path": "/home/nicholas/Desktop/Waldo_alpha_01/processed_clips/clips_2025-09-17_14-01-12_ffa2/2025-09-17_14-01-12_clip_36.mp4",
      "frame_paths": [
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_36/img_0000000000.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_36/img_0000000001.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_36/img_0000000002.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_36/img_0000000003.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_36/img_0000000004.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_36/img_0000000005.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_36/img_0000000006.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_36/img_0000000007.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_36/img_0000000008.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_36/img_0000000009.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_36/img_0000000010.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_36/img_0000000011.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_36/img_0000000012.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_36/img_0000000013.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_36/img_0000000014.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_36/img_0000000015.jpg"
      ]
    },
    {
      "id": 30,
      "filename": "2025-09-17_14-01-12_clip_37.mp4",
      "clip_name": "2025-09-17_14-01-12_clip_37",
      "normalized_score": -0.5097,
      "probability": 0.013782357845972513,
      "confidence": {
        "category": "Very Low Confidence",
        "label": "Likely Legitimate",
        "color": "#28a745",
        "level": 1
      },
      "video_path": "/home/nicholas/Desktop/Waldo_alpha_01/processed_clips/clips_2025-09-17_14-01-12_ffa2/2025-09-17_14-01-12_clip_37.mp4",
      "frame_paths": [
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_37/img_0000000000.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_37/img_0000000001.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_37/img_0000000002.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_37/img_0000000003.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_37/img_0000000004.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_37/img_0000000005.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_37/img_0000000006.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_37/img_0000000007.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_37/img_0000000008.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_37/img_0000000009.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_37/img_0000000010.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_37/img_0000000011.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_37/img_0000000012.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_37/img_0000000013.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_37/img_0000000014.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_37/img_0000000015.jpg"
      ]
    },
    {
      "id": 31,
      "filename": "2025-09-17_14-01-12_clip_38.mp4",
      "clip_name": "2025-09-17_14-01-12_clip_38",
      "normalized_score": -0.5159,
      "probability": 0.01340988317272185,
      "confidence": {
        "category": "Very Low Confidence",
        "label": "Likely Legitimate",
        "color": "#28a745",
        "level": 1
      },
      "video_path": "/home/nicholas/Desktop/Waldo_alpha_01/processed_clips/clips_2025-09-17_14-01-12_ffa2/2025-09-17_14-01-12_clip_38.mp4",
      "frame_paths": [
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_38/img_0000000000.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_38/img_0000000001.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_38/img_0000000002.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_38/img_0000000003.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_38/img_0000000004.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_38/img_0000000005.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_38/img_0000000006.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_38/img_0000000007.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_38/img_0000000008.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_38/img_0000000009.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_38/img_0000000010.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_38/img_0000000011.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_38/img_0000000012.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_38/img_0000000013.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_38/img_0000000014.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_38/img_0000000015.jpg"
      ]
    },
    {
      "id": 32,
      "filename": "2025-09-17_14-01-12_clip_39.mp4",
      "clip_name": "2025-09-17_14-01-12_clip_39",
      "normalized_score": -0.5144,
      "probability": 0.013499077209295616,
      "confidence": {
        "category": "Very Low Confidence",
        "label": "Likely Legitimate",
        "color": "#28a745",
        "level": 1
      },
      "video_path": "/home/nicholas/Desktop/Waldo_alpha_01/processed_clips/clips_2025-09-17_14-01-12_ffa2/2025-09-17_14-01-12_clip_39.mp4",
      "frame_paths": [
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_39/img_0000000000.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_39/img_0000000001.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_39/img_0000000002.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_39/img_0000000003.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_39/img_0000000004.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_39/img_0000000005.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_39/img_0000000006.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_39/img_0000000007.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_39/img_0000000008.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_39/img_0000000009.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_39/img_0000000010.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_39/img_0000000011.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_39/img_0000000012.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_39/img_0000000013.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_39/img_0000000014.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_39/img_0000000015.jpg"
      ]
    },
    {
      "id": 33,
      "filename": "2025-09-17_14-01-12_clip_4.mp4",
      "clip_name": "2025-09-17_14-01-12_clip_4",
      "normalized_score": -0.5153,
      "probability": 0.013445490814184812,
      "confidence": {
        "category": "Very Low Confidence",
        "label": "Likely Legitimate",
        "color": "#28a745",
        "level": 1
      },
      "video_path": "/home/nicholas/Desktop/Waldo_alpha_01/processed_clips/clips_2025-09-17_14-01-12_ffa2/2025-09-17_14-01-12_clip_4.mp4",
      "frame_paths": [
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_4/img_0000000000.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_4/img_0000000001.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_4/img_0000000002.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_4/img_0000000003.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_4/img_0000000004.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_4/img_0000000005.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_4/img_0000000006.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_4/img_0000000007.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_4/img_0000000008.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_4/img_0000000009.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_4/img_0000000010.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_4/img_0000000011.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_4/img_0000000012.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_4/img_0000000013.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_4/img_0000000014.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_4/img_0000000015.jpg"
      ]
    },
    {
      "id": 34,
      "filename": "2025-09-17_14-01-12_clip_40.mp4",
      "clip_name": "2025-09-17_14-01-12_clip_40",
      "normalized_score": -0.5143,
      "probability": 0.013505044240499347,
      "confidence": {
        "category": "Very Low Confidence",
        "label": "Likely Legitimate",
        "color": "#28a745",
        "level": 1
      },
      "video_path": "/home/nicholas/Desktop/Waldo_alpha_01/processed_clips/clips_2025-09-17_14-01-12_ffa2/2025-09-17_14-01-12_clip_40.mp4",
      "frame_paths": [
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_40/img_0000000000.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_40/img_0000000001.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_40/img_0000000002.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_40/img_0000000003.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_40/img_0000000004.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_40/img_0000000005.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_40/img_0000000006.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_40/img_0000000007.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_40/img_0000000008.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_40/img_0000000009.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_40/img_0000000010.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_40/img_0000000011.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_40/img_0000000012.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_40/img_0000000013.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_40/img_0000000014.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_40/img_0000000015.jpg"
      ]
    },
    {
      "id": 35,
      "filename": "2025-09-17_14-01-12_clip_41.mp4",
      "clip_name": "2025-09-17_14-01-12_clip_41",
      "normalized_score": -0.5147,
      "probability": 0.013481191713652115,
      "confidence": {
        "category": "Very Low Confidence",
        "label": "Likely Legitimate",
        "color": "#28a745",
        "level": 1
      },
      "video_path": "/home/nicholas/Desktop/Waldo_alpha_01/processed_clips/clips_2025-09-17_14-01-12_ffa2/2025-09-17_14-01-12_clip_41.mp4",
      "frame_paths": [
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_41/img_0000000000.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_41/img_0000000001.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_41/img_0000000002.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_41/img_0000000003.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_41/img_0000000004.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_41/img_0000000005.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_41/img_0000000006.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_41/img_0000000007.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_41/img_0000000008.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_41/img_0000000009.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_41/img_0000000010.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_41/img_0000000011.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_41/img_0000000012.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_41/img_0000000013.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_41/img_0000000014.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_41/img_0000000015.jpg"
      ]
    },
    {
      "id": 36,
      "filename": "2025-09-17_14-01-12_clip_42.mp4",
      "clip_name": "2025-09-17_14-01-12_clip_42",
      "normalized_score": -0.5116,
      "probability": 0.013667141068955852,
      "confidence": {
        "category": "Very Low Confidence",
        "label": "Likely Legitimate",
        "color": "#28a745",
        "level": 1
      },
      "video_path": "/home/nicholas/Desktop/Waldo_alpha_01/processed_clips/clips_2025-09-17_14-01-12_ffa2/2025-09-17_14-01-12_clip_42.mp4",
      "frame_paths": [
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_42/img_0000000000.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_42/img_0000000001.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_42/img_0000000002.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_42/img_0000000003.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_42/img_0000000004.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_42/img_0000000005.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_42/img_0000000006.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_42/img_0000000007.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_42/img_0000000008.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_42/img_0000000009.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_42/img_0000000010.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_42/img_0000000011.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_42/img_0000000012.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_42/img_0000000013.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_42/img_0000000014.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_42/img_0000000015.jpg"
      ]
    },
    {
      "id": 37,
      "filename": "2025-09-17_14-01-12_clip_43.mp4",
      "clip_name": "2025-09-17_14-01-12_clip_43",
      "normalized_score": -0.5171,
      "probability": 0.013338946717131704,
      "confidence": {
        "category": "Very Low Confidence",
        "label": "Likely Legitimate",
        "color": "#28a745",
        "level": 1
      },
      "video_path": "/home/nicholas/Desktop/Waldo_alpha_01/processed_clips/clips_2025-09-17_14-01-12_ffa2/2025-09-17_14-01-12_clip_43.mp4",
      "frame_paths": [
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_43/img_0000000000.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_43/img_0000000001.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_43/img_0000000002.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_43/img_0000000003.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_43/img_0000000004.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_43/img_0000000005.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_43/img_0000000006.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_43/img_0000000007.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_43/img_0000000008.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_43/img_0000000009.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_43/img_0000000010.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_43/img_0000000011.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_43/img_0000000012.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_43/img_0000000013.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_43/img_0000000014.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_43/img_0000000015.jpg"
      ]
    },
    {
      "id": 38,
      "filename": "2025-09-17_14-01-12_clip_44.mp4",
      "clip_name": "2025-09-17_14-01-12_clip_44",
      "normalized_score": -0.5086,
      "probability": 0.013849499492111261,
      "confidence": {
        "category": "Very Low Confidence",
        "label": "Likely Legitimate",
        "color": "#28a745",
        "level": 1
      },
      "video_path": "/home/nicholas/Desktop/Waldo_alpha_01/processed_clips/clips_2025-09-17_14-01-12_ffa2/2025-09-17_14-01-12_clip_44.mp4",
      "frame_paths": [
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_44/img_0000000000.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_44/img_0000000001.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_44/img_0000000002.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_44/img_0000000003.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_44/img_0000000004.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_44/img_0000000005.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_44/img_0000000006.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_44/img_0000000007.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_44/img_0000000008.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_44/img_0000000009.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_44/img_0000000010.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_44/img_0000000011.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_44/img_0000000012.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_44/img_0000000013.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_44/img_0000000014.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_44/img_0000000015.jpg"
      ]
    },
    {
      "id": 39,
      "filename": "2025-09-17_14-01-12_clip_45.mp4",
      "clip_name": "2025-09-17_14-01-12_clip_45",
      "normalized_score": -0.51,
      "probability": 0.013764102256666919,
      "confidence": {
        "category": "Very Low Confidence",
        "label": "Likely Legitimate",
        "color": "#28a745",
        "level": 1
      },
      "video_path": "/home/nicholas/Desktop/Waldo_alpha_01/processed_clips/clips_2025-09-17_14-01-12_ffa2/2025-09-17_14-01-12_clip_45.mp4",
      "frame_paths": [
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_45/img_0000000000.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_45/img_0000000001.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_45/img_0000000002.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_45/img_0000000003.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_45/img_0000000004.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_45/img_0000000005.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_45/img_0000000006.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_45/img_0000000007.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_45/img_0000000008.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_45/img_0000000009.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_45/img_0000000010.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_45/img_0000000011.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_45/img_0000000012.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_45/img_0000000013.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_45/img_0000000014.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_45/img_0000000015.jpg"
      ]
    },
    {
      "id": 40,
      "filename": "2025-09-17_14-01-12_clip_46.mp4",
      "clip_name": "2025-09-17_14-01-12_clip_46",
      "normalized_score": -0.5136,
      "probability": 0.013546886362606453,
      "confidence": {
        "category": "Very Low Confidence",
        "label": "Likely Legitimate",
        "color": "#28a745",
        "level": 1
      },
      "video_path": "/home/nicholas/Desktop/Waldo_alpha_01/processed_clips/clips_2025-09-17_14-01-12_ffa2/2025-09-17_14-01-12_clip_46.mp4",
      "frame_paths": [
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_46/img_0000000000.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_46/img_0000000001.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_46/img_0000000002.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_46/img_0000000003.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_46/img_0000000004.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_46/img_0000000005.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_46/img_0000000006.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_46/img_0000000007.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_46/img_0000000008.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_46/img_0000000009.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_46/img_0000000010.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_46/img_0000000011.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_46/img_0000000012.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_46/img_0000000013.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_46/img_0000000014.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_46/img_0000000015.jpg"
      ]
    },
    {
      "id": 41,
      "filename": "2025-09-17_14-01-12_clip_47.mp4",
      "clip_name": "2025-09-17_14-01-12_clip_47",
      "normalized_score": -0.5178,
      "probability": 0.013297738198064018,
      "confidence": {
        "category": "Very Low Confidence",
        "label": "Likely Legitimate",
        "color": "#28a745",
        "level": 1
      },
      "video_path": "/home/nicholas/Desktop/Waldo_alpha_01/processed_clips/clips_2025-09-17_14-01-12_ffa2/2025-09-17_14-01-12_clip_47.mp4",
      "frame_paths": [
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_47/img_0000000000.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_47/img_0000000001.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_47/img_0000000002.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_47/img_0000000003.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_47/img_0000000004.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_47/img_0000000005.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_47/img_0000000006.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_47/img_0000000007.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_47/img_0000000008.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_47/img_0000000009.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_47/img_0000000010.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_47/img_0000000011.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_47/img_0000000012.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_47/img_0000000013.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_47/img_0000000014.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_47/img_0000000015.jpg"
      ]
    },
    {
      "id": 42,
      "filename": "2025-09-17_14-01-12_clip_48.mp4",
      "clip_name": "2025-09-17_14-01-12_clip_48",
      "normalized_score": -0.515,
      "probability": 0.013463329591845138,
      "confidence": {
        "category": "Very Low Confidence",
        "label": "Likely Legitimate",
        "color": "#28a745",
        "level": 1
      },
      "video_path": "/home/nicholas/Desktop/Waldo_alpha_01/processed_clips/clips_2025-09-17_14-01-12_ffa2/2025-09-17_14-01-12_clip_48.mp4",
      "frame_paths": [
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_48/img_0000000000.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_48/img_0000000001.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_48/img_0000000002.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_48/img_0000000003.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_48/img_0000000004.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_48/img_0000000005.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_48/img_0000000006.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_48/img_0000000007.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_48/img_0000000008.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_48/img_0000000009.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_48/img_0000000010.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_48/img_0000000011.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_48/img_0000000012.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_48/img_0000000013.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_48/img_0000000014.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_48/img_0000000015.jpg"
      ]
    },
    {
      "id": 43,
      "filename": "2025-09-17_14-01-12_clip_49.mp4",
      "clip_name": "2025-09-17_14-01-12_clip_49",
      "normalized_score": -0.5113,
      "probability": 0.01368526984131102,
      "confidence": {
        "category": "Very Low Confidence",
        "label": "Likely Legitimate",
        "color": "#28a745",
        "level": 1
      },
      "video_path": "/home/nicholas/Desktop/Waldo_alpha_01/processed_clips/clips_2025-09-17_14-01-12_ffa2/2025-09-17_14-01-12_clip_49.mp4",
      "frame_paths": [
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_49/img_0000000000.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_49/img_0000000001.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_49/img_0000000002.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_49/img_0000000003.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_49/img_0000000004.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_49/img_0000000005.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_49/img_0000000006.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_49/img_0000000007.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_49/img_0000000008.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_49/img_0000000009.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_49/img_0000000010.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_49/img_0000000011.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_49/img_0000000012.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_49/img_0000000013.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_49/img_0000000014.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_49/img_0000000015.jpg"
      ]
    },
    {
      "id": 44,
      "filename": "2025-09-17_14-01-12_clip_5.mp4",
      "clip_name": "2025-09-17_14-01-12_clip_5",
      "normalized_score": -0.5145,
      "probability": 0.01349311277848734,
      "confidence": {
        "category": "Very Low Confidence",
        "label": "Likely Legitimate",
        "color": "#28a745",
        "level": 1
      },
      "video_path": "/home/nicholas/Desktop/Waldo_alpha_01/processed_clips/clips_2025-09-17_14-01-12_ffa2/2025-09-17_14-01-12_clip_5.mp4",
      "frame_paths": [
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_5/img_0000000000.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_5/img_0000000001.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_5/img_0000000002.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_5/img_0000000003.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_5/img_0000000004.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_5/img_0000000005.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_5/img_0000000006.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_5/img_0000000007.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_5/img_0000000008.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_5/img_0000000009.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_5/img_0000000010.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_5/img_0000000011.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_5/img_0000000012.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_5/img_0000000013.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_5/img_0000000014.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_5/img_0000000015.jpg"
      ]
    },
    {
      "id": 45,
      "filename": "2025-09-17_14-01-12_clip_50.mp4",
      "clip_name": "2025-09-17_14-01-12_clip_50",
      "normalized_score": -0.5143,
      "probability": 0.013505044240499347,
      "confidence": {
        "category": "Very Low Confidence",
        "label": "Likely Legitimate",
        "color": "#28a745",
        "level": 1
      },
      "video_path": "/home/nicholas/Desktop/Waldo_alpha_01/processed_clips/clips_2025-09-17_14-01-12_ffa2/2025-09-17_14-01-12_clip_50.mp4",
      "frame_paths": [
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_50/img_0000000000.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_50/img_0000000001.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_50/img_0000000002.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_50/img_0000000003.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_50/img_0000000004.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_50/img_0000000005.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_50/img_0000000006.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_50/img_0000000007.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_50/img_0000000008.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_50/img_0000000009.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_50/img_0000000010.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_50/img_0000000011.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_50/img_0000000012.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_50/img_0000000013.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_50/img_0000000014.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_50/img_0000000015.jpg"
      ]
    },
    {
      "id": 46,
      "filename": "2025-09-17_14-01-12_clip_6.mp4",
      "clip_name": "2025-09-17_14-01-12_clip_6",
      "normalized_score": -0.5124,
      "probability": 0.013618913376551965,
      "confidence": {
        "category": "Very Low Confidence",
        "label": "Likely Legitimate",
        "color": "#28a745",
        "level": 1
      },
      "video_path": "/home/nicholas/Desktop/Waldo_alpha_01/processed_clips/clips_2025-09-17_14-01-12_ffa2/2025-09-17_14-01-12_clip_6.mp4",
      "frame_paths": [
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_6/img_0000000000.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_6/img_0000000001.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_6/img_0000000002.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_6/img_0000000003.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_6/img_0000000004.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_6/img_0000000005.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_6/img_0000000006.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_6/img_0000000007.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_6/img_0000000008.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_6/img_0000000009.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_6/img_0000000010.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_6/img_0000000011.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_6/img_0000000012.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_6/img_0000000013.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_6/img_0000000014.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_6/img_0000000015.jpg"
      ]
    },
    {
      "id": 47,
      "filename": "2025-09-17_14-01-12_clip_7.mp4",
      "clip_name": "2025-09-17_14-01-12_clip_7",
      "normalized_score": -0.5169,
      "probability": 0.013350743700693096,
      "confidence": {
        "category": "Very Low Confidence",
        "label": "Likely Legitimate",
        "color": "#28a745",
        "level": 1
      },
      "video_path": "/home/nicholas/Desktop/Waldo_alpha_01/processed_clips/clips_2025-09-17_14-01-12_ffa2/2025-09-17_14-01-12_clip_7.mp4",
      "frame_paths": [
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_7/img_0000000000.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_7/img_0000000001.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_7/img_0000000002.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_7/img_0000000003.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_7/img_0000000004.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_7/img_0000000005.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_7/img_0000000006.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_7/img_0000000007.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_7/img_0000000008.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_7/img_0000000009.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_7/img_0000000010.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_7/img_0000000011.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_7/img_0000000012.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_7/img_0000000013.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_7/img_0000000014.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_7/img_0000000015.jpg"
      ]
    },
    {
      "id": 48,
      "filename": "2025-09-17_14-01-12_clip_8.mp4",
      "clip_name": "2025-09-17_14-01-12_clip_8",
      "normalized_score": -0.5111,
      "probability": 0.013697368861472003,
      "confidence": {
        "category": "Very Low Confidence",
        "label": "Likely Legitimate",
        "color": "#28a745",
        "level": 1
      },
      "video_path": "/home/nicholas/Desktop/Waldo_alpha_01/processed_clips/clips_2025-09-17_14-01-12_ffa2/2025-09-17_14-01-12_clip_8.mp4",
      "frame_paths": [
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_8/img_0000000000.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_8/img_0000000001.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_8/img_0000000002.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_8/img_0000000003.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_8/img_0000000004.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_8/img_0000000005.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_8/img_0000000006.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_8/img_0000000007.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_8/img_0000000008.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_8/img_0000000009.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_8/img_0000000010.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_8/img_0000000011.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_8/img_0000000012.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_8/img_0000000013.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_8/img_0000000014.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_8/img_0000000015.jpg"
      ]
    },
    {
      "id": 49,
      "filename": "2025-09-17_14-01-12_clip_9.mp4",
      "clip_name": "2025-09-17_14-01-12_clip_9",
      "normalized_score": -0.5123,
      "probability": 0.013624932647630537,
      "confidence": {
        "category": "Very Low Confidence",
        "label": "Likely Legitimate",
        "color": "#28a745",
        "level": 1
      },
      "video_path": "/home/nicholas/Desktop/Waldo_alpha_01/processed_clips/clips_2025-09-17_14-01-12_ffa2/2025-09-17_14-01-12_clip_9.mp4",
      "frame_paths": [
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_9/img_0000000000.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_9/img_0000000001.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_9/img_0000000002.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_9/img_0000000003.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_9/img_0000000004.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_9/img_0000000005.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_9/img_0000000006.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_9/img_0000000007.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_9/img_0000000008.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_9/img_0000000009.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_9/img_0000000010.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_9/img_0000000011.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_9/img_0000000012.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_9/img_0000000013.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_9/img_0000000014.jpg",
        "/home/nicholas/Desktop/Waldo_alpha_01/evaluation_results/2d865ce3-af90-4639-b7f7-7609ba4d35da/frames/2025-09-17_14-01-12_clip_9/img_0000000015.jpg"
      ]
    }
  ],
  "summary_stats": {
    "total_clips": 50,
    "mean_score": -0.5135059999999999,
    "median_score": -0.51415,
    "std_score": 0.0025871536483170077,
    "min_score": -0.5184,
    "max_score": -0.507,
    "high_confidence_count": 0,
    "medium_confidence_count": 0,
    "low_confidence_count": 50
  },
  "distribution": {
    "bins": [
      "0.0-0.1",
      "0.1-0.2",
      "0.2-0.3",
      "0.3-0.4",
      "0.4-0.5",
      "0.5-0.6",
      "0.6-0.7",
      "0.7-0.8",
      "0.8-0.9",
      "0.9-1.0"
    ],
    "counts": [
      0,
      0,
      0,
      0,
      0,
      0,
      0,
      0,
      0,
      0
    ]
  },
  "evaluation_metadata": {
    "model_path": "not_cheater_model_50clips_20250917_143356/checkpoint-19.pth",
    "clips_path": "/home/nicholas/Desktop/Waldo_alpha_01/processed_clips/clips_2025-09-17_14-01-12_ffa2",
    "total_clips_processed": 50,
    "evaluation_timestamp": "2025-09-17T14:37:45.390818"
  }
}
```

`install.sh`:

```sh
#!/bin/bash
echo "=================================================="
echo "CS2 Cheat Detection System - Linux/Mac Setup"
echo "=================================================="
echo

# First check if conda exists in the typical location
if [ -f "$HOME/miniconda3/bin/conda" ]; then
    echo "Found conda at $HOME/miniconda3"
    # Add to PATH for this session
    export PATH="$HOME/miniconda3/bin:$PATH"
    # Source conda for this session
    source "$HOME/miniconda3/etc/profile.d/conda.sh"
elif [ -f "$HOME/anaconda3/bin/conda" ]; then
    echo "Found conda at $HOME/anaconda3"
    # Add to PATH for this session
    export PATH="$HOME/anaconda3/bin:$PATH"
    # Source conda for this session
    source "$HOME/anaconda3/etc/profile.d/conda.sh"
elif command -v conda &> /dev/null; then
    echo "Conda found in PATH"
else
    echo "Conda is not installed. Installing Miniconda..."
    echo

    # Detect OS and architecture
    if [[ "$OSTYPE" == "linux-gnu"* ]]; then
        MINICONDA_URL="https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh"
    elif [[ "$OSTYPE" == "darwin"* ]]; then
        # Check if Apple Silicon
        if [[ $(uname -m) == 'arm64' ]]; then
            MINICONDA_URL="https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-arm64.sh"
        else
            MINICONDA_URL="https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-x86_64.sh"
        fi
    else
        echo "ERROR: Unsupported OS type: $OSTYPE"
        echo "Please install Miniconda manually from https://docs.conda.io/en/latest/miniconda.html"
        echo "Press Enter to exit..."
        read
        exit 1
    fi

    # Download and install Miniconda
    echo "Downloading Miniconda..."
    curl -o miniconda.sh $MINICONDA_URL

    echo "Installing Miniconda..."
    bash miniconda.sh -b -p $HOME/miniconda3
    rm miniconda.sh

    # Add conda to PATH for this session
    export PATH="$HOME/miniconda3/bin:$PATH"
    source "$HOME/miniconda3/etc/profile.d/conda.sh"

    # Initialize conda for bash
    conda init bash

    echo
    echo "Miniconda installed successfully!"
    echo "Continuing with environment setup..."
    echo
fi

echo "Conda is available! Setting up environment..."
echo

# Check for NVIDIA GPU
if command -v nvidia-smi &> /dev/null; then
    echo "NVIDIA GPU detected:"
    nvidia-smi | grep "NVIDIA GeForce\|NVIDIA RTX\|NVIDIA Quadro" | head -1
    echo "CUDA-enabled PyTorch will be installed for GPU acceleration."
else
    echo "WARNING: No NVIDIA GPU detected. Installing CPU-only PyTorch."
    echo "For GPU acceleration, please install NVIDIA drivers first."
fi
echo

# Check if cs2-detect-env already exists
if conda env list | grep -q "cs2-detect-env"; then
    echo "Environment 'cs2-detect-env' already exists."
    echo "Do you want to:"
    echo "1) Use existing environment (recommended if it's working)"
    echo "2) Update the existing environment"
    echo "3) Remove and recreate the environment"
    echo "4) Cancel installation"
    echo
    read -p "Enter choice (1/2/3/4): " choice

    case $choice in
        1)
            echo "Using existing environment..."
            ;;
        2)
            echo "Updating existing environment..."
            conda env update -f environment.yml
            ;;
        3)
            echo "Removing existing environment..."
            conda env remove -n cs2-detect-env -y
            echo "Creating new environment..."
            conda env create -f environment.yml
            ;;
        4)
            echo "Installation cancelled."
            echo "Press Enter to exit..."
            read
            exit 0
            ;;
        *)
            echo "Invalid choice. Installation cancelled."
            echo "Press Enter to exit..."
            read
            exit 1
            ;;
    esac
else
    echo "Creating conda environment 'cs2-detect-env'..."
    echo "This may take 10-15 minutes depending on your internet connection..."
    echo
    conda env create -f environment.yml
fi

if [ $? -ne 0 ]; then
    # Only show error if we actually tried to create/update
    if [[ "$choice" != "1" ]]; then
        echo
        echo "ERROR: Environment setup failed"
        echo "Please check the error messages above"
        echo "Press Enter to exit..."
        read
        exit 1
    fi
fi

# Update main.py to use the correct Python path
echo
echo "Updating configuration..."

# Get the conda environment Python path
CONDA_PREFIX="${CONDA_PREFIX:-$HOME/miniconda3}"
ENV_PYTHON="$CONDA_PREFIX/envs/cs2-detect-env/bin/python"

# Check if we need to update main.py (handle any hardcoded Python paths)
if [ -f main.py ]; then
    # Look for any hardcoded Python paths in main.py and replace them
    if grep -q "/home/.*/miniconda3/envs/cs2-detect-env/bin/python\|/home/.*/anaconda3/envs/cs2-detect-env/bin/python" main.py; then
        # Replace any hardcoded conda environment path with the actual one
        sed -i "s|/home/.*/miniconda3/envs/cs2-detect-env/bin/python|$ENV_PYTHON|g" main.py
        sed -i "s|/home/.*/anaconda3/envs/cs2-detect-env/bin/python|$ENV_PYTHON|g" main.py
        echo "Updated hardcoded Python paths in main.py to: $ENV_PYTHON"
    else
        echo "No hardcoded Python paths found in main.py (using dynamic detection)"
    fi
fi

echo
echo "=================================================="
echo "Installation Complete!"
echo "=================================================="
echo
echo "To start the CS2 Cheat Detection System:"
echo "1. Run: ./run.sh"
echo "   (The run script will automatically activate the environment)"
echo "2. Open your browser to http://localhost:5000"
echo
echo "Note: The model weights (.pth files) need to be placed in:"
echo "  deepcheat/VideoMAEv2/output/"
echo
echo "Press Enter to exit..."
read

# Make run script executable
chmod +x run.sh
```

`main.py`:

```py
from flask import Flask, render_template, request, redirect, url_for, flash, Response
import os
import subprocess
import uuid
import shutil
import json
import tempfile
import cv2
from datetime import datetime
from werkzeug.utils import secure_filename

def get_cs2_detect_python_path():
    """
    Dynamically detect the Python executable path for the cs2-detect-env conda environment.
    Works across different systems and usernames.
    """
    import os
    import subprocess

    # First try to get the conda environment path using conda itself
    try:
        # Try to find conda and get the environment path
        result = subprocess.run(['conda', 'info', '--envs'], capture_output=True, text=True, timeout=10)
        if result.returncode == 0:
            for line in result.stdout.split('\n'):
                if 'cs2-detect-env' in line and '*' not in line:  # Skip current env marker
                    parts = line.split()
                    if len(parts) >= 2:
                        env_path = parts[-1]  # Last part is the path
                        python_path = os.path.join(env_path, 'bin', 'python')
                        if os.path.exists(python_path):
                            return python_path
    except (subprocess.TimeoutExpired, FileNotFoundError, subprocess.SubprocessError):
        pass

    # Fallback: try common conda installation locations
    import getpass
    username = getpass.getuser()

    # Common conda locations to check
    conda_bases = [
        f'/home/{username}/miniconda3',
        f'/home/{username}/anaconda3',
        '/opt/conda',
        '/opt/miniconda3',
        '/opt/anaconda3'
    ]

    for conda_base in conda_bases:
        python_path = os.path.join(conda_base, 'envs', 'cs2-detect-env', 'bin', 'python')
        if os.path.exists(python_path):
            return python_path

    # If all else fails, try using the current Python and hope it's in the right environment
    import sys
    if 'cs2-detect-env' in sys.executable:
        return sys.executable

    # Last resort: return the hardcoded path and let it fail with a clear error
    raise FileNotFoundError(
        "Could not find Python executable for cs2-detect-env conda environment. "
        "Please ensure the environment is properly installed and activated."
    )

app = Flask(__name__)
app.secret_key = 'supersecretkey'

# --- Configuration ---
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
app.config['UPLOAD_FOLDER'] = os.path.join(BASE_DIR, 'uploads')
app.config['PROCESSED_FOLDER'] = os.path.join(BASE_DIR, 'processed_clips')
app.config['TRAINING_DATA_FOLDER'] = os.path.join(BASE_DIR, 'processed_vids')
app.config['MODELS_OUTPUT_FOLDER'] = os.path.join(BASE_DIR, 'deepcheat', 'VideoMAEv2', 'output')

# In-memory dictionary to store process commands and PIDs
PROCESS_STORE = {}

# Directory for persistent evaluation results
EVALUATIONS_DIR = os.path.join(BASE_DIR, 'evaluation_results')

def save_evaluation_results(process_id, results_data, temp_eval_dir):
    """Save evaluation results and frames to persistent storage"""
    os.makedirs(EVALUATIONS_DIR, exist_ok=True)

    eval_dir = os.path.join(EVALUATIONS_DIR, process_id)
    os.makedirs(eval_dir, exist_ok=True)

    # Save results data as JSON
    results_file = os.path.join(eval_dir, 'results.json')
    with open(results_file, 'w') as f:
        json.dump(results_data, f, indent=2)

    # Copy frames directory if it exists
    if os.path.exists(temp_eval_dir):
        frames_dir = os.path.join(eval_dir, 'frames')
        if os.path.exists(frames_dir):
            shutil.rmtree(frames_dir)
        shutil.copytree(temp_eval_dir, frames_dir)

    # Update frame paths to new location
    for clip in results_data['clip_results']:
        if clip['frame_paths']:
            clip['frame_paths'] = [
                p.replace(temp_eval_dir, os.path.join(eval_dir, 'frames'))
                for p in clip['frame_paths']
            ]

    # Save updated results with new paths
    with open(results_file, 'w') as f:
        json.dump(results_data, f, indent=2)

    return eval_dir

def load_evaluation_results(process_id):
    """Load saved evaluation results"""
    eval_dir = os.path.join(EVALUATIONS_DIR, process_id)
    results_file = os.path.join(eval_dir, 'results.json')

    if not os.path.exists(results_file):
        return None

    with open(results_file, 'r') as f:
        return json.load(f)

def get_saved_evaluations():
    """Get list of saved evaluation results"""
    if not os.path.exists(EVALUATIONS_DIR):
        return []

    evaluations = []
    for dirname in os.listdir(EVALUATIONS_DIR):
        eval_dir = os.path.join(EVALUATIONS_DIR, dirname)
        results_file = os.path.join(eval_dir, 'results.json')

        if os.path.exists(results_file):
            with open(results_file, 'r') as f:
                data = json.load(f)
                evaluations.append({
                    'process_id': dirname,
                    'timestamp': data['evaluation_metadata']['evaluation_timestamp'],
                    'clips_count': data['summary_stats']['total_clips'],
                    'model': os.path.basename(data['evaluation_metadata']['model_path']),
                    'dataset': os.path.basename(data['evaluation_metadata']['clips_path'])
                })

    return sorted(evaluations, key=lambda x: x['timestamp'], reverse=True)

def get_adaptive_crop_size(video_width, video_height, base_resolution=(3840, 2160), base_crop_size=240):
    """Calculate adaptive crop size based on video resolution to maintain same field of view"""
    base_width, base_height = base_resolution

    # Calculate the crop size as a percentage of the original training resolution
    crop_percentage = base_crop_size / base_width

    # Apply the same percentage to the current video width
    adaptive_crop_size = int(video_width * crop_percentage)

    # Ensure crop size is even and reasonable
    adaptive_crop_size = max(200, min(adaptive_crop_size, min(video_width, video_height)))

    return adaptive_crop_size

def generate_evaluation_results_data(predictions_file, clips_path, temp_eval_dir, process_info):
    """Generate comprehensive evaluation results data including statistics and clip analysis"""
    import numpy as np
    import math

    # Read prediction scores
    with open(predictions_file, 'r') as f:
        scores = [float(line.strip()) for line in f.readlines()]

    # Get clip names (in order they were processed)
    clips = sorted([f for f in os.listdir(clips_path) if f.endswith('.mp4')])

    # Ensure we have matching clips and scores
    if len(clips) != len(scores):
        clips = clips[:len(scores)]  # Trim to match scores

    # Calculate statistics
    scores_array = np.array(scores)

    def get_confidence_category(score):
        """Convert normalized score to confidence category"""
        if score >= 0.8:
            return {"category": "Very High Confidence", "label": "Likely Cheating", "color": "#dc3545", "level": 5}
        elif score >= 0.6:
            return {"category": "High Confidence", "label": "Possible Cheating", "color": "#fd7e14", "level": 4}
        elif score >= 0.4:
            return {"category": "Medium Confidence", "label": "Uncertain", "color": "#ffc107", "level": 3}
        elif score >= 0.2:
            return {"category": "Low Confidence", "label": "Likely Legitimate", "color": "#20c997", "level": 2}
        else:
            return {"category": "Very Low Confidence", "label": "Likely Legitimate", "color": "#28a745", "level": 1}

    def sigmoid_to_probability(normalized_score, min_logit=-1.9871155, max_logit=2.4927201):
        """Convert normalized score back to approximate probability using sigmoid"""
        # Reverse min-max normalization to get approximate logit
        estimated_logit = normalized_score * (max_logit - min_logit) + min_logit
        # Apply sigmoid to get probability
        probability = 1 / (1 + math.exp(-estimated_logit))
        return probability

    # Generate clip results
    clip_results = []
    for i, (clip, score) in enumerate(zip(clips, scores)):
        confidence = get_confidence_category(score)
        probability = sigmoid_to_probability(score)

        # Get frame paths if they exist
        frames_dir = os.path.join(temp_eval_dir, os.path.splitext(clip)[0])
        frame_paths = []
        if os.path.exists(frames_dir):
            frame_files = sorted([f for f in os.listdir(frames_dir) if f.endswith('.jpg')])
            frame_paths = [os.path.join(frames_dir, f) for f in frame_files]

        clip_data = {
            'id': i,
            'filename': clip,
            'clip_name': os.path.splitext(clip)[0],
            'normalized_score': score,
            'probability': probability,
            'confidence': confidence,
            'video_path': os.path.join(clips_path, clip),
            'frame_paths': frame_paths
        }
        clip_results.append(clip_data)

    # Calculate summary statistics
    summary_stats = {
        'total_clips': len(scores),
        'mean_score': float(np.mean(scores_array)),
        'median_score': float(np.median(scores_array)),
        'std_score': float(np.std(scores_array)),
        'min_score': float(np.min(scores_array)),
        'max_score': float(np.max(scores_array)),
        'high_confidence_count': len([s for s in scores if s >= 0.6]),
        'medium_confidence_count': len([s for s in scores if 0.4 <= s < 0.6]),
        'low_confidence_count': len([s for s in scores if s < 0.4])
    }

    # Calculate distribution for histogram
    bins = np.linspace(0, 1, 11)  # 10 bins from 0 to 1
    hist, _ = np.histogram(scores_array, bins=bins)

    distribution = {
        'bins': [f"{bins[i]:.1f}-{bins[i+1]:.1f}" for i in range(len(bins)-1)],
        'counts': hist.tolist()
    }

    return {
        'clip_results': clip_results,
        'summary_stats': summary_stats,
        'distribution': distribution,
        'evaluation_metadata': {
            'model_path': process_info['model_path'],
            'clips_path': clips_path,
            'total_clips_processed': len(scores),
            'evaluation_timestamp': datetime.now().isoformat()
        }
    }

# --- Helper Functions ---
def get_processed_clips_dirs():
    if not os.path.exists(app.config['PROCESSED_FOLDER']):
        return []
    return sorted([d for d in os.listdir(app.config['PROCESSED_FOLDER']) if os.path.isdir(os.path.join(app.config['PROCESSED_FOLDER'], d))])

def get_existing_models():
    models = []
    if not os.path.exists(app.config['MODELS_OUTPUT_FOLDER']):
        return []

    # Check for models in the main output directory
    for item in os.listdir(app.config['MODELS_OUTPUT_FOLDER']):
        item_path = os.path.join(app.config['MODELS_OUTPUT_FOLDER'], item)

        if item.endswith('.pth'):
            # Direct .pth file in main directory (legacy models)
            models.append({'path': item, 'display_name': item, 'full_path': item_path})
        elif os.path.isdir(item_path):
            # Check subdirectories for models
            model_info_path = os.path.join(item_path, 'model_info.json')
            checkpoint_files = [f for f in os.listdir(item_path) if f.endswith('.pth')]

            if checkpoint_files:
                # Get the latest checkpoint
                latest_checkpoint = max(checkpoint_files, key=lambda x: int(x.split('-')[1].split('.')[0]) if '-' in x and x.split('-')[1].split('.')[0].isdigit() else 0)
                full_checkpoint_path = os.path.join(item_path, latest_checkpoint)

                # Try to get display name from metadata
                display_name = item
                if os.path.exists(model_info_path):
                    try:
                        with open(model_info_path, 'r') as f:
                            metadata = json.load(f)

                        # Build display name with training history
                        total_clips = metadata.get('total_clips_trained', metadata.get('clips_count', '?'))
                        training_type = metadata.get('training_type', 'unknown')
                        label_type = metadata.get('label_type', 'unknown')
                        timestamp = metadata.get('timestamp', item)

                        # Create summary of training history
                        if 'training_history' in metadata and len(metadata['training_history']) > 1:
                            history = metadata['training_history']
                            steps = len(history)
                            # Show label types from training history
                            label_types = list(set(step.get('label_type', 'unknown') for step in history))
                            label_summary = '+'.join(label_types) if len(label_types) > 1 else label_types[0]
                            display_name = f"{label_summary} ({total_clips} total clips, {steps} training steps) - {timestamp}"
                        else:
                            display_name = f"{label_type} ({total_clips} clips) - {timestamp}"

                    except:
                        # Fallback to directory name
                        display_name = item

                models.append({
                    'path': os.path.join(item, latest_checkpoint),
                    'display_name': display_name,
                    'full_path': full_checkpoint_path
                })

    return sorted(models, key=lambda x: x['display_name'])

# --- Routes ---
@app.route('/')
def index():
    processed_clips = get_processed_clips_dirs()
    existing_models = get_existing_models()
    return render_template('index.html', processed_clips=processed_clips, existing_models=existing_models)

@app.route('/process', methods=['POST'])
def process_video():
    if 'videoFile' not in request.files:
        flash('No file part')
        return redirect(request.url)
    file = request.files['videoFile']
    if file.filename == '':
        flash('No selected file')
        return redirect(request.url)
    
    if file:
        filename = secure_filename(file.filename)
        temp_input_dir = os.path.join(app.config['UPLOAD_FOLDER'], f"temp_{uuid.uuid4().hex[:8]}")
        os.makedirs(temp_input_dir)
        uploaded_video_path = os.path.join(temp_input_dir, filename)
        file.save(uploaded_video_path)

        output_dir_name = f"clips_{os.path.splitext(filename)[0]}_{uuid.uuid4().hex[:4]}"
        output_dir_path = os.path.join(app.config['PROCESSED_FOLDER'], output_dir_name)
        
        sample_sound_path = os.path.join(BASE_DIR, 'deepcheat', 'utils', 'autoedit', 'sample', 'csheadshot.wav')
        autoedit_script_path = os.path.join(BASE_DIR, 'deepcheat', 'utils', 'autoedit', 'autoedit_improved.py')

        command = [
            'python', autoedit_script_path,
            '--input-dir', temp_input_dir,
            '--output-dir', output_dir_path,
            '--sample-sound', sample_sound_path,
            '--temp-dir', app.config['UPLOAD_FOLDER']
        ]
        
        process_id = str(uuid.uuid4())
        PROCESS_STORE[process_id] = {'command': command, 'cleanup_dir': temp_input_dir}
        
        return redirect(url_for('show_processing', process_id=process_id))

    return redirect(url_for('index'))

@app.route('/processing/<process_id>')
def show_processing(process_id):
    return render_template('processing.html', process_id=process_id)

@app.route('/training/<process_id>')
def show_training(process_id):
    return render_template('training.html', process_id=process_id)

@app.route('/stream-training/<process_id>')
def stream_training(process_id):
    process_info = PROCESS_STORE.get(process_id)
    if not process_info:
        return Response("Process not found.", mimetype='text/plain')

    command = process_info['command']
    clips_path = process_info['clips_path']
    temp_training_dir = process_info['temp_training_dir']
    label = process_info['label']

    def generate():
        try:
            # Step 1: Preprocess clips from MP4 to frames
            yield f"data: Starting preprocessing of clips...<br>\n\n"

            # Get list of clips
            clips = [f for f in os.listdir(clips_path) if f.endswith('.mp4')]
            total_clips = len(clips)

            if total_clips == 0:
                yield f"data: <b>Error:</b> No MP4 clips found in {clips_path}<br>\n\n"
                return

            # Check minimum clip requirement
            min_clips_required = 48  # batch_size * update_freq
            if total_clips < min_clips_required:
                yield f"data: <br><b>Error:</b> Training requires at least {min_clips_required} clips.<br>\n\n"
                yield f"data: You currently have {total_clips} clips.<br>\n\n"
                yield f"data: Please process more video footage or add more clips before training.<br>\n\n"
                return

            yield f"data: Found {total_clips} clips to process<br>\n\n"

            # Create CSV file for training
            csv_lines = []

            for idx, clip_file in enumerate(clips, 1):
                clip_name = os.path.splitext(clip_file)[0]
                clip_output_dir = os.path.join(temp_training_dir, clip_name)
                os.makedirs(clip_output_dir, exist_ok=True)

                yield f"data: Processing clip {idx}/{total_clips}: {clip_name}<br>\n\n"

                # Extract frames from video using the same method as the working version
                video_path = os.path.join(clips_path, clip_file)

                cap = cv2.VideoCapture(video_path)
                if not cap.isOpened():
                    yield f"data: Warning: Could not open {clip_name}<br>\n\n"
                    continue

                # Get video resolution for adaptive cropping
                video_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
                video_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
                crop_size = get_adaptive_crop_size(video_width, video_height)

                yield f"data: Video resolution: {video_width}x{video_height}, using {crop_size}px crop<br>\n\n"

                total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
                frame_number = 0
                save_frame_number = 0

                # Process frames similar to nick_crop.py
                while True:
                    ret, frame = cap.read()
                    if not ret:
                        break

                    # Extract frames 85-100 (where the killshot happens)
                    if 85 <= frame_number <= 100:
                        # Center crop with adaptive size
                        y, x, c = frame.shape
                        start_x = x // 2 - (crop_size // 2)
                        start_y = y // 2 - (crop_size // 2)
                        cropped_frame = frame[start_y:start_y + crop_size, start_x:start_x + crop_size]

                        # Save frame as image
                        img_filename = f"img_{save_frame_number:010d}.jpg"
                        img_path = os.path.join(clip_output_dir, img_filename)
                        cv2.imwrite(img_path, cropped_frame)

                        save_frame_number += 1

                        if save_frame_number >= 16:  # We only need 16 frames
                            break

                    frame_number += 1
                    if frame_number > 100:  # No need to read beyond frame 100
                        break

                cap.release()

                if save_frame_number < 16:
                    yield f"data: Warning: Only extracted {save_frame_number} frames from {clip_name}<br>\n\n"

                # Add to CSV
                csv_lines.append(f"{clip_output_dir} 15 {label}\n")

            # Write CSV files
            for csv_name in ['train.csv', 'val.csv', 'test.csv']:
                csv_path = os.path.join(temp_training_dir, csv_name)
                with open(csv_path, 'w') as f:
                    f.writelines(csv_lines)

            yield f"data: <br><b>Preprocessing complete!</b> Starting model training...<br><br>\n\n"

            # Step 2: Run training
            env = os.environ.copy()
            env['PYTHONUNBUFFERED'] = '1'
            env['CUDA_VISIBLE_DEVICES'] = '0'  # Use first GPU if available

            # Run with the same Python that's running Flask (should have correct environment)
            process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT,
                                     text=True, bufsize=1, universal_newlines=True, env=env,
                                     cwd=os.path.join(BASE_DIR, 'deepcheat', 'VideoMAEv2'))

            while True:
                line = process.stdout.readline()
                if not line:
                    break
                # Clean and format the line
                line = line.strip()
                if line:
                    # Highlight important training metrics
                    if 'epoch' in line.lower() or 'loss' in line.lower() or 'accuracy' in line.lower():
                        yield f"data: <b>{line}</b><br>\n\n"
                    else:
                        yield f"data: {line}<br>\n\n"

            process.stdout.close()
            return_code = process.wait()

            if return_code != 0:
                yield f"data: <br><b>Error:</b> Training exited with code {return_code}<br>\n\n"
            else:
                yield f"data: <br><b>Training completed successfully!</b><br>\n\n"
                # Get the actual output directory from the command
                output_dir = app.config['MODELS_OUTPUT_FOLDER']
                try:
                    output_idx = command.index('--output_dir')
                    if output_idx + 1 < len(command):
                        output_dir = command[output_idx + 1]
                except ValueError:
                    pass

                # Check for the latest checkpoint in the output directory
                try:
                    checkpoint_files = [f for f in os.listdir(output_dir) if f.startswith('checkpoint-') and f.endswith('.pth')]
                    if checkpoint_files:
                        latest_checkpoint = max(checkpoint_files, key=lambda x: int(x.split('-')[1].split('.')[0]))
                        checkpoint_path = os.path.join(output_dir, latest_checkpoint)

                        # Get model name from directory
                        model_name = os.path.basename(output_dir)
                        yield f"data: <br><b>New model '{model_name}' saved successfully!</b><br>\n\n"
                        yield f"data: Checkpoint: {checkpoint_path}<br>\n\n"
                        yield f"data: This model is now available for fine-tuning and evaluation.<br>\n\n"
                    else:
                        yield f"data: Model checkpoints saved to: {output_dir}<br>\n\n"
                except OSError:
                    yield f"data: Model checkpoints saved to: {output_dir}<br>\n\n"

        except Exception as e:
            yield f"data: <br><b>An unexpected error occurred:</b> {e}<br>\n\n"
        finally:
            # Cleanup temporary directory
            if os.path.exists(temp_training_dir):
                shutil.rmtree(temp_training_dir)
            yield "data: PROCESS_COMPLETE\n\n"
            if process_id in PROCESS_STORE:
                del PROCESS_STORE[process_id]

    return Response(generate(), mimetype='text/event-stream')

@app.route('/stream-logs/<process_id>')
def stream_logs(process_id):
    process_info = PROCESS_STORE.get(process_id)
    if not process_info:
        return Response("Process not found.", mimetype='text/plain')

    command = process_info['command']
    cleanup_dir = process_info.get('cleanup_dir')

    def generate():
        try:
            # Start the process immediately and notify client
            yield f"data: Starting video processing...\n\n"
            
            # Set PYTHONUNBUFFERED to ensure immediate output
            env = os.environ.copy()
            env['PYTHONUNBUFFERED'] = '1'
            
            process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, 
                                     text=True, bufsize=1, universal_newlines=True, env=env)
            
            # Read output line by line with no buffering
            while True:
                line = process.stdout.readline()
                if not line:
                    break
                # Send each line immediately
                yield f"data: {line.strip()}\n\n"
                
            process.stdout.close()
            return_code = process.wait()
            if return_code != 0:
                yield f"data: \n--- \n**Error:** Process exited with code {return_code}.\n\n"
        except Exception as e:
            yield f"data: \n--- \n**An unexpected error occurred:** {e}\n\n"
        finally:
            if cleanup_dir and os.path.exists(cleanup_dir):
                shutil.rmtree(cleanup_dir)
            yield "data: PROCESS_COMPLETE\n\n"
            # Clean up the process from the store
            if process_id in PROCESS_STORE:
                del PROCESS_STORE[process_id]

    return Response(generate(), mimetype='text/event-stream')

@app.route('/train', methods=['POST'])
def train_model():
    clips_directory = request.form.get('clipsDirectory')
    training_type = request.form.get('trainingType')
    model_type = request.form.get('modelType')
    existing_model = request.form.get('existingModel')

    if not clips_directory:
        flash('Please select a clips directory')
        return redirect(url_for('index'))

    # Full path to clips directory
    clips_path = os.path.join(app.config['PROCESSED_FOLDER'], clips_directory)

    # Create a temporary training data directory
    temp_training_dir = os.path.join(BASE_DIR, 'temp_processing', f"training_{uuid.uuid4().hex[:8]}")
    os.makedirs(temp_training_dir, exist_ok=True)

    # Prepare the command for training
    training_script_path = os.path.join(BASE_DIR, 'deepcheat', 'VideoMAEv2', 'train_cheater_pred.py')

    # Determine the label (0 for not cheater, 1 for cheater)
    label = 1 if model_type == 'cheater' else 0

    # Use the Python from cs2-detect-env specifically (dynamically detected)
    python_executable = get_cs2_detect_python_path()

    # Base command
    command = [
        python_executable, training_script_path,
        '--model', 'vit_giant_patch14_224',
        '--data_set', 'cheater',
        '--nb_classes', '1',
        '--data_path', temp_training_dir,
        '--data_root', temp_training_dir,
        '--log_dir', app.config['MODELS_OUTPUT_FOLDER'],
        '--output_dir', app.config['MODELS_OUTPUT_FOLDER'],
        '--batch_size', '4',
        '--update_freq', '12',
        '--input_size', '224',
        '--short_side_size', '224',
        '--save_ckpt_freq', '20',
        '--num_frames', '16',
        '--sampling_rate', '1',
        '--num_sample', '1',
        '--num_workers', '4',
        '--opt', 'adamw',
        '--lr', '1e-3',
        '--drop_path', '0.1',
        '--clip_grad', '1.0',
        '--layer_decay', '0.9',
        '--opt_betas', '0.9', '0.999',
        '--weight_decay', '0.000',
        '--warmup_epochs', '10',
        '--epochs', '100',
        '--test_num_segment', '5',
        '--test_num_crop', '3'
    ]

    # Add finetune flag if fine-tuning
    if training_type == 'finetune':
        if existing_model:
            # The model path is now relative from get_existing_models()
            model_path = os.path.join(app.config['MODELS_OUTPUT_FOLDER'], existing_model)
        else:
            # Default to checkpoint-99.pth if it exists
            model_path = os.path.join(app.config['MODELS_OUTPUT_FOLDER'], 'checkpoint-99.pth')
        command.extend(['--finetune', model_path])

        # For fine-tuning, also create a unique output directory to avoid auto-resume conflicts
        from datetime import datetime
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        model_label = model_type.replace('-', '_')
        clips_count = len([f for f in os.listdir(clips_path) if f.endswith('.mp4')])

        finetune_dir_name = f"finetune_{model_label}_{clips_count}clips_{timestamp}"
        unique_output_dir = os.path.join(app.config['MODELS_OUTPUT_FOLDER'], finetune_dir_name)
        os.makedirs(unique_output_dir, exist_ok=True)

        # Get training history from base model
        training_history = []
        total_clips_trained = clips_count

        print(f"DEBUG START: existing_model = '{existing_model}'")
        print(f"DEBUG START: clips_count = {clips_count}")
        print(f"DEBUG START: training_type = '{training_type}'")

        # Try to load base model metadata to get its history
        if existing_model:
            # Extract the model directory from the existing_model path (remove checkpoint filename)
            model_dir_from_existing = os.path.dirname(existing_model) if '/' in existing_model else existing_model.replace('.pth', '')
            base_model_dir = os.path.join(app.config['MODELS_OUTPUT_FOLDER'], model_dir_from_existing)
            base_model_info_path = os.path.join(base_model_dir, 'model_info.json')

            print(f"DEBUG: existing_model = {existing_model}")
            print(f"DEBUG: model_dir_from_existing = {model_dir_from_existing}")
            print(f"DEBUG: base_model_dir = {base_model_dir}")
            print(f"DEBUG: base_model_info_path = {base_model_info_path}")
            print(f"DEBUG: Path exists? = {os.path.exists(base_model_info_path)}")

            if os.path.exists(base_model_info_path):
                try:
                    with open(base_model_info_path, 'r') as f:
                        base_metadata = json.load(f)

                    print(f"DEBUG: Successfully loaded base metadata from {base_model_info_path}")
                    print(f"DEBUG: Base metadata keys: {list(base_metadata.keys())}")

                    # Add base model's complete history to our history
                    if 'training_history' in base_metadata:
                        training_history = base_metadata['training_history'].copy()
                        print(f"DEBUG: Copied {len(training_history)} entries from base training history")
                    else:
                        # If no history in base model, create it from the base model's metadata
                        training_history = [{
                            'step': 1,
                            'model_name': base_metadata.get('model_name', existing_model),
                            'training_type': base_metadata.get('training_type', 'unknown'),
                            'label_type': base_metadata.get('label_type', 'unknown'),
                            'clips_count': base_metadata.get('clips_count', 0),
                            'clips_directory': base_metadata.get('clips_directory', 'unknown'),
                            'timestamp': base_metadata.get('timestamp', 'unknown')
                        }]
                        print(f"DEBUG: Created training history from base model metadata")

                    # Calculate total clips: base model's total + current training clips
                    base_total_clips = base_metadata.get('total_clips_trained', base_metadata.get('clips_count', 0))
                    total_clips_trained = base_total_clips + clips_count
                    print(f"DEBUG: base_total_clips = {base_total_clips}, current clips = {clips_count}, total = {total_clips_trained}")

                except Exception as e:
                    print(f"Warning: Could not read base model metadata: {e}")

        # Add current training step to history
        current_step = len(training_history) + 1
        training_history.append({
            'step': current_step,
            'model_name': finetune_dir_name,
            'training_type': training_type,
            'label_type': model_type,
            'clips_count': clips_count,
            'clips_directory': clips_directory,
            'timestamp': timestamp
        })

        print(f"DEBUG: Final training_history has {len(training_history)} steps")
        print(f"DEBUG: Final total_clips_trained = {total_clips_trained}")

        # Save model metadata with full history
        metadata = {
            'model_name': finetune_dir_name,
            'training_type': training_type,
            'label_type': model_type,
            'clips_count': clips_count,  # This training session's clips
            'total_clips_trained': total_clips_trained,  # Cumulative across all training
            'clips_directory': clips_directory,
            'timestamp': timestamp,
            'created': datetime.now().isoformat(),
            'base_model': existing_model or 'checkpoint-99.pth',
            'training_history': training_history
        }

        with open(os.path.join(unique_output_dir, 'model_info.json'), 'w') as f:
            json.dump(metadata, f, indent=2)

        # Update output directories in command
        for i, arg in enumerate(command):
            if arg == '--log_dir':
                command[i+1] = unique_output_dir
            elif arg == '--output_dir':
                command[i+1] = unique_output_dir

        # Disable auto-resume for fine-tuning to start fresh
        command.extend(['--no_auto_resume'])
    else:
        # For new training, use the base pre-trained model and disable auto-resume
        pretrained_model_path = os.path.join(BASE_DIR, 'deepcheat', 'vit_g_ps14_ak_ft_ckpt_7_clean.pth')
        if os.path.exists(pretrained_model_path):
            command.extend(['--finetune', pretrained_model_path])
        # Disable auto-resume for new model training
        command.extend(['--no_auto_resume'])
        # Create a descriptive model directory name
        from datetime import datetime
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        model_label = model_type.replace('-', '_')
        clips_count = len([f for f in os.listdir(clips_path) if f.endswith('.mp4')])

        model_dir_name = f"{model_label}_model_{clips_count}clips_{timestamp}"
        unique_output_dir = os.path.join(app.config['MODELS_OUTPUT_FOLDER'], model_dir_name)
        os.makedirs(unique_output_dir, exist_ok=True)

        # Save model metadata with initial training history
        training_history = [{
            'step': 1,
            'model_name': model_dir_name,
            'training_type': training_type,
            'label_type': model_type,
            'clips_count': clips_count,
            'clips_directory': clips_directory,
            'timestamp': timestamp
        }]

        metadata = {
            'model_name': model_dir_name,
            'training_type': training_type,
            'label_type': model_type,
            'clips_count': clips_count,
            'total_clips_trained': clips_count,  # For new models, same as clips_count
            'clips_directory': clips_directory,
            'timestamp': timestamp,
            'created': datetime.now().isoformat(),
            'training_history': training_history
        }

        with open(os.path.join(unique_output_dir, 'model_info.json'), 'w') as f:
            json.dump(metadata, f, indent=2)
        # Update output directories in command
        for i, arg in enumerate(command):
            if arg == '--log_dir':
                command[i+1] = unique_output_dir
            elif arg == '--output_dir':
                command[i+1] = unique_output_dir

    process_id = str(uuid.uuid4())
    PROCESS_STORE[process_id] = {
        'command': command,
        'clips_path': clips_path,
        'temp_training_dir': temp_training_dir,
        'label': label,
        'preprocessing_needed': True
    }

    return redirect(url_for('show_training', process_id=process_id))

@app.route('/evaluate', methods=['POST'])
def evaluate_clips():
    clips_directory = request.form.get('clipsDirectory')
    model_path = request.form.get('model')

    if not clips_directory:
        flash('Please select a clips directory')
        return redirect(url_for('index'))

    if not model_path:
        flash('Please select a model for evaluation')
        return redirect(url_for('index'))

    # Full path to clips directory
    clips_path = os.path.join(app.config['PROCESSED_FOLDER'], clips_directory)

    # Create a temporary evaluation data directory
    temp_eval_dir = os.path.join(BASE_DIR, 'temp_processing', f"evaluation_{uuid.uuid4().hex[:8]}")
    os.makedirs(temp_eval_dir, exist_ok=True)

    # Prepare the command for evaluation
    training_script_path = os.path.join(BASE_DIR, 'deepcheat', 'VideoMAEv2', 'train_cheater_pred.py')

    # Use the Python from cs2-detect-env specifically (dynamically detected)
    python_executable = get_cs2_detect_python_path()

    # Full path to the selected model
    model_full_path = os.path.join(app.config['MODELS_OUTPUT_FOLDER'], model_path)

    # Base evaluation command (similar to eval_cheater.sh)
    command = [
        python_executable, training_script_path,
        '--model', 'vit_giant_patch14_224',
        '--data_set', 'cheater',
        '--nb_classes', '1',
        '--finetune', model_full_path,
        '--batch_size', '8',
        '--input_size', '224',
        '--short_side_size', '224',
        '--num_frames', '16',
        '--sampling_rate', '1',
        '--num_sample', '1',
        '--num_workers', '4',
        '--opt', 'adamw',
        '--lr', '1e-3',
        '--drop_path', '0.3',
        '--clip_grad', '5.0',
        '--layer_decay', '0.9',
        '--opt_betas', '0.9', '0.999',
        '--weight_decay', '0.1',
        '--test_num_segment', '1',
        '--test_num_crop', '1',
        '--eval',  # This enables evaluation mode
        '--min_eval_score', '-1.9871155',
        '--max_eval_score', '2.4927201',
        '--output_dir', temp_eval_dir,
        '--data_path', temp_eval_dir,
        '--data_root', temp_eval_dir
    ]

    process_id = str(uuid.uuid4())
    PROCESS_STORE[process_id] = {
        'command': command,
        'clips_path': clips_path,
        'temp_eval_dir': temp_eval_dir,
        'model_path': model_path,
        'evaluation_mode': True
    }

    return redirect(url_for('show_evaluation', process_id=process_id))

@app.route('/evaluation/<process_id>')
def show_evaluation(process_id):
    process_info = PROCESS_STORE.get(process_id)
    if not process_info:
        return "Process not found.", 404

    model_name = os.path.basename(process_info['model_path'])
    data_path = process_info['clips_path']
    start_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

    return render_template('evaluation.html',
                         process_id=process_id,
                         model_name=model_name,
                         data_path=data_path,
                         start_time=start_time)

@app.route('/stream-evaluation/<process_id>')
def stream_evaluation(process_id):
    process_info = PROCESS_STORE.get(process_id)
    if not process_info:
        return Response("Process not found.", mimetype='text/plain')

    command = process_info['command']
    clips_path = process_info['clips_path']
    temp_eval_dir = process_info['temp_eval_dir']
    model_path = process_info['model_path']

    def generate():
        try:
            # Step 1: Preprocess clips from MP4 to frames for evaluation
            yield f"data: {json.dumps({'type': 'log', 'content': 'Starting evaluation preprocessing...'})}\n\n"

            # Get list of clips
            clips = [f for f in os.listdir(clips_path) if f.endswith('.mp4')]
            total_clips = len(clips)

            if total_clips == 0:
                yield f"data: {json.dumps({'type': 'log', 'content': '<b>Error:</b> No MP4 clips found in ' + clips_path})}\n\n"
                yield f"data: {json.dumps({'type': 'status', 'status': 'error'})}\n\n"
                return

            yield f"data: {json.dumps({'type': 'log', 'content': f'Found {total_clips} clips to evaluate'})}\n\n"
            yield f"data: {json.dumps({'type': 'log', 'content': f'Using model: {os.path.basename(model_path)}'})}\n\n"

            # Create CSV file for evaluation (all clips get label 0 for evaluation)
            csv_lines = []

            for idx, clip_file in enumerate(clips, 1):
                clip_name = os.path.splitext(clip_file)[0]
                clip_output_dir = os.path.join(temp_eval_dir, clip_name)
                os.makedirs(clip_output_dir, exist_ok=True)

                yield f"data: {json.dumps({'type': 'log', 'content': f'Processing clip {idx}/{total_clips}: {clip_name}'})}\n\n"

                # Extract frames from video using the same method as the working version
                video_path = os.path.join(clips_path, clip_file)

                cap = cv2.VideoCapture(video_path)
                if not cap.isOpened():
                    yield f"data: {json.dumps({'type': 'log', 'content': f'Warning: Could not open {clip_name}'})}\n\n"
                    continue

                # Get video resolution for adaptive cropping
                video_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
                video_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
                crop_size = get_adaptive_crop_size(video_width, video_height)

                yield f"data: {json.dumps({'type': 'log', 'content': f'Video resolution: {video_width}x{video_height}, using {crop_size}px crop'})}\n\n"

                total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
                frame_number = 0
                save_frame_number = 0

                # Process frames similar to nick_crop.py
                while True:
                    ret, frame = cap.read()
                    if not ret:
                        break

                    # Extract frames 85-100 (where the killshot happens)
                    if 85 <= frame_number <= 100:
                        # Center crop with adaptive size
                        y, x, c = frame.shape
                        start_x = x // 2 - (crop_size // 2)
                        start_y = y // 2 - (crop_size // 2)
                        cropped_frame = frame[start_y:start_y + crop_size, start_x:start_x + crop_size]

                        # Save frame as image
                        img_filename = f"img_{save_frame_number:010d}.jpg"
                        img_path = os.path.join(clip_output_dir, img_filename)
                        cv2.imwrite(img_path, cropped_frame)

                        save_frame_number += 1

                        if save_frame_number >= 16:  # We only need 16 frames
                            break

                    frame_number += 1
                    if frame_number > 100:  # No need to read beyond frame 100
                        break

                cap.release()

                if save_frame_number < 16:
                    yield f"data: {json.dumps({'type': 'log', 'content': f'Warning: Only extracted {save_frame_number} frames from {clip_name}'})}\n\n"

                # Add to CSV (label doesn't matter for evaluation, just use 0)
                csv_lines.append(f"{clip_output_dir} 15 0\n")

            # Write CSV files
            for csv_name in ['test.csv', 'val.csv']:
                csv_path = os.path.join(temp_eval_dir, csv_name)
                with open(csv_path, 'w') as f:
                    f.writelines(csv_lines)

            yield f"data: {json.dumps({'type': 'log', 'content': '<b>Preprocessing complete!</b> Starting evaluation...'})}\n\n"

            # Step 2: Run evaluation
            env = os.environ.copy()
            env['PYTHONUNBUFFERED'] = '1'
            env['CUDA_VISIBLE_DEVICES'] = '0'  # Use first GPU if available

            # Run evaluation
            process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT,
                                     text=True, bufsize=1, universal_newlines=True, env=env,
                                     cwd=os.path.join(BASE_DIR, 'deepcheat', 'VideoMAEv2'))

            while True:
                line = process.stdout.readline()
                if not line:
                    break
                # Clean and format the line
                line = line.strip()
                if line:
                    # Highlight important evaluation metrics
                    if any(keyword in line.lower() for keyword in ['test', 'accuracy', 'evaluation', 'score']):
                        yield f"data: {json.dumps({'type': 'log', 'content': f'<b>{line}</b>'})}\n\n"
                    else:
                        yield f"data: {json.dumps({'type': 'log', 'content': line})}\n\n"

            process.stdout.close()
            return_code = process.wait()

            if return_code != 0:
                yield f"data: {json.dumps({'type': 'log', 'content': f'<b>Error:</b> Evaluation exited with code {return_code}'})}\n\n"
                yield f"data: {json.dumps({'type': 'status', 'status': 'error'})}\n\n"
            else:
                yield f"data: {json.dumps({'type': 'log', 'content': '<b>Evaluation completed successfully!</b>'})}\n\n"

                # Check for prediction results and generate comprehensive analysis
                predictions_file = os.path.join(temp_eval_dir, 'cheater_preds.txt')
                if os.path.exists(predictions_file):
                    yield f"data: {json.dumps({'type': 'log', 'content': '<b>Generating comprehensive results analysis...</b>'})}\n\n"

                    # Generate detailed results analysis
                    results_data = generate_evaluation_results_data(predictions_file, clips_path, temp_eval_dir, process_info)

                    # Store results data for the results page
                    PROCESS_STORE[process_id]['results_data'] = results_data

                    total_clips = len(results_data['clip_results'])
                    yield f"data: {json.dumps({'type': 'results_ready', 'process_id': process_id, 'total_clips': total_clips})}\n\n"
                    yield f"data: {json.dumps({'type': 'log', 'content': f'<b>Analysis complete!</b> Processed {total_clips} clips.'})}\n\n"

                else:
                    yield f"data: {json.dumps({'type': 'log', 'content': f'Results saved to: {temp_eval_dir}'})}\n\n"

                yield f"data: {json.dumps({'type': 'status', 'status': 'completed'})}\n\n"

        except Exception as e:
            yield f"data: {json.dumps({'type': 'log', 'content': f'<b>An unexpected error occurred:</b> {str(e)}'})}\n\n"
            yield f"data: {json.dumps({'type': 'status', 'status': 'error'})}\n\n"
        finally:
            # Save evaluation results to persistent storage
            if process_id in PROCESS_STORE and 'results_data' in PROCESS_STORE[process_id]:
                save_evaluation_results(process_id, PROCESS_STORE[process_id]['results_data'], temp_eval_dir)
            # Don't delete temp_eval_dir anymore - we need the frames
            # Don't delete process from store yet - let it be cleaned up later

    return Response(generate(), mimetype='text/event-stream')

@app.route('/evaluation-results/<process_id>')
def show_evaluation_results(process_id):
    """Display comprehensive evaluation results"""
    # First check if results are in memory
    process_info = PROCESS_STORE.get(process_id)
    if process_info and 'results_data' in process_info:
        results_data = process_info['results_data']
    else:
        # Try to load from saved results
        results_data = load_evaluation_results(process_id)
        if not results_data:
            return "Results not found.", 404

    return render_template('evaluation_results.html',
                         process_id=process_id,
                         results=results_data)

@app.route('/clip-analysis/<process_id>/<int:clip_id>')
def show_clip_analysis(process_id, clip_id):
    """Display individual clip analysis"""
    # First check if results are in memory
    process_info = PROCESS_STORE.get(process_id)
    if process_info and 'results_data' in process_info:
        results_data = process_info['results_data']
    else:
        # Try to load from saved results
        results_data = load_evaluation_results(process_id)
        if not results_data:
            return "Results not found.", 404

    if clip_id >= len(results_data['clip_results']):
        return "Clip not found.", 404

    clip_data = results_data['clip_results'][clip_id]
    return render_template('clip_analysis.html',
                         process_id=process_id,
                         clip=clip_data,
                         metadata=results_data['evaluation_metadata'])

@app.route('/serve-video/<process_id>/<int:clip_id>')
def serve_video(process_id, clip_id):
    """Serve video files for playback"""
    # First check if results are in memory
    process_info = PROCESS_STORE.get(process_id)
    if process_info and 'results_data' in process_info:
        results_data = process_info['results_data']
    else:
        # Try to load from saved results
        results_data = load_evaluation_results(process_id)
        if not results_data:
            return "Video not found.", 404

    if clip_id >= len(results_data['clip_results']):
        return "Video not found.", 404

    clip_data = results_data['clip_results'][clip_id]
    video_path = clip_data['video_path']

    if not os.path.exists(video_path):
        return "Video file not found.", 404

    return Response(
        open(video_path, 'rb').read(),
        mimetype='video/mp4',
        headers={'Content-Disposition': f'inline; filename="{clip_data["filename"]}"'}
    )

@app.route('/serve-frame/<process_id>/<int:clip_id>/<int:frame_idx>')
def serve_frame(process_id, clip_id, frame_idx):
    """Serve individual frame images"""
    # First check if results are in memory
    process_info = PROCESS_STORE.get(process_id)
    if process_info and 'results_data' in process_info:
        results_data = process_info['results_data']
    else:
        # Try to load from saved results
        results_data = load_evaluation_results(process_id)
        if not results_data:
            return "Frame not found.", 404

    if clip_id >= len(results_data['clip_results']):
        return "Clip not found.", 404

    clip_data = results_data['clip_results'][clip_id]
    if not clip_data.get('frame_paths') or frame_idx >= len(clip_data['frame_paths']):
        return "Frame not found.", 404

    frame_path = clip_data['frame_paths'][frame_idx]
    if not os.path.exists(frame_path):
        return "Frame file not found.", 404

    return Response(
        open(frame_path, 'rb').read(),
        mimetype='image/jpeg',
        headers={'Content-Disposition': f'inline; filename="frame_{frame_idx}.jpg"'}
    )

@app.route('/saved-evaluations')
def show_saved_evaluations():
    """Show list of saved evaluation results"""
    evaluations = get_saved_evaluations()
    return render_template('saved_evaluations.html', evaluations=evaluations)

@app.route('/export-results/<process_id>')
def export_results(process_id):
    """Export evaluation results as JSON"""
    # First check if results are in memory
    process_info = PROCESS_STORE.get(process_id)
    if process_info and 'results_data' in process_info:
        results_data = process_info['results_data']
    else:
        # Try to load from saved results
        results_data = load_evaluation_results(process_id)
        if not results_data:
            return "Results not found.", 404

    # Prepare export data without frame paths (too large)
    export_data = {
        'evaluation_metadata': results_data['evaluation_metadata'],
        'summary_stats': results_data['summary_stats'],
        'distribution': results_data['distribution'],
        'clip_results': [{
            'id': c['id'],
            'filename': c['filename'],
            'clip_name': c['clip_name'],
            'normalized_score': c['normalized_score'],
            'probability': c['probability'],
            'confidence': c['confidence']
        } for c in results_data['clip_results']]
    }

    return Response(
        json.dumps(export_data, indent=2),
        mimetype='application/json',
        headers={'Content-Disposition': f'attachment; filename="evaluation_{process_id}.json"'}
    )

if __name__ == '__main__':
    os.makedirs(app.config['UPLOAD_FOLDER'], exist_ok=True)
    os.makedirs(app.config['PROCESSED_FOLDER'], exist_ok=True)
    os.makedirs(app.config['TRAINING_DATA_FOLDER'], exist_ok=True)
    os.makedirs(app.config['MODELS_OUTPUT_FOLDER'], exist_ok=True)
    os.makedirs(os.path.join(BASE_DIR, 'final_footage'), exist_ok=True)
    
    app.run(debug=True, host='0.0.0.0', port=5000)

```

`requirements.txt`:

```txt
# CS2 Cheat Detection System Requirements
# Web Framework
Flask==2.3.3
Werkzeug==2.3.7

# Computer Vision and Video Processing
opencv-python==4.9.0.80
av==12.3.0

# Machine Learning and Data Science
torch>=1.8.0
torchvision>=0.9.0
torchaudio>=0.8.0
numpy>=1.21.0
pandas>=1.3.0
scipy>=1.7.0
Pillow>=8.3.0
matplotlib>=3.4.0

# Deep Learning Utilities
timm==0.4.12
einops>=0.4.0

# Logging and Monitoring
tensorboard==2.9.0
tensorboardX==2.6.2

# Data Processing
decord>=0.6.0

# CUDA acceleration (optional but recommended for GPU)
# Uncomment the next line if you have NVIDIA GPU with CUDA
# triton>=2.0.0

# System utilities
protobuf==3.20.3

# Additional utilities
tqdm>=4.62.0
scikit-learn>=1.0.0
```

`run.sh`:

```sh
#!/bin/bash
echo "=================================================="
echo "CS2 Cheat Detection System - Starting Server"
echo "=================================================="
echo

# First check if conda exists in the typical location
if [ -f "$HOME/miniconda3/bin/conda" ]; then
    echo "Found conda at $HOME/miniconda3"
    export PATH="$HOME/miniconda3/bin:$PATH"
    CONDA_BASE="$HOME/miniconda3"
elif [ -f "$HOME/anaconda3/bin/conda" ]; then
    echo "Found conda at $HOME/anaconda3"
    export PATH="$HOME/anaconda3/bin:$PATH"
    CONDA_BASE="$HOME/anaconda3"
elif command -v conda &> /dev/null; then
    echo "Conda found in PATH"
    CONDA_BASE=$(conda info --base)
else
    echo "ERROR: Conda not found"
    echo "Please run ./install.sh first to set up the environment"
    echo "Press Enter to exit..."
    read
    exit 1
fi

# Source conda.sh for activation
if [ -f "$CONDA_BASE/etc/profile.d/conda.sh" ]; then
    source "$CONDA_BASE/etc/profile.d/conda.sh"
else
    echo "ERROR: Could not find conda.sh"
    echo "Your conda installation may be incomplete"
    echo "Press Enter to exit..."
    read
    exit 1
fi

# Check if cs2-detect-env exists
if ! conda env list | grep -q "cs2-detect-env"; then
    echo "ERROR: cs2-detect-env environment not found"
    echo "Please run ./install.sh first to set up the environment"
    echo "Press Enter to exit..."
    read
    exit 1
fi

echo "Activating cs2-detect-env environment..."

# Activate the environment
conda activate cs2-detect-env

if [ $? -ne 0 ]; then
    echo "ERROR: Failed to activate conda environment"
    echo "Press Enter to exit..."
    read
    exit 1
fi

echo "Starting the CS2 Cheat Detection web interface..."
echo
echo "Once started, open your browser and go to:"
echo "http://localhost:5000"
echo
echo "Press Ctrl+C to stop the server when done."
echo

# Run the application
python main.py

echo
echo "Server stopped."
echo "Press Enter to exit..."
read
```

`temp_processing/evaluation_40379e6e/cheater_preds.txt`:

```txt
-0.5149
-0.5157
-0.5157
-0.5114
-0.5170
-0.5147
-0.5122
-0.5127
-0.5124
-0.5184
-0.5180
-0.5129
-0.5110
-0.5135
-0.5093
-0.5103
-0.5146
-0.5120
-0.5152
-0.5163
-0.5153
-0.5070
-0.5143
-0.5129
-0.5160
-0.5119
-0.5102
-0.5149
-0.5098
-0.5140
-0.5097
-0.5159
-0.5144
-0.5153
-0.5143
-0.5147
-0.5116
-0.5171
-0.5086
-0.5100
-0.5136
-0.5178
-0.5150
-0.5113
-0.5145
-0.5143
-0.5124
-0.5169
-0.5111
-0.5123

```

`templates/clip_analysis.html`:

```html
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Clip Analysis - {{ clip.clip_name }}</title>
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            background-color: #f5f5f5;
            line-height: 1.6;
        }
        .header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 30px;
            border-radius: 10px;
            margin-bottom: 30px;
            text-align: center;
        }
        .back-button {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 12px 24px;
            border: none;
            border-radius: 5px;
            font-size: 16px;
            cursor: pointer;
            text-decoration: none;
            display: inline-block;
            margin-bottom: 20px;
        }
        .analysis-grid {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 30px;
            margin-bottom: 30px;
        }
        .video-section {
            background: white;
            padding: 25px;
            border-radius: 10px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }
        .analysis-section {
            background: white;
            padding: 25px;
            border-radius: 10px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }
        .video-container {
            text-align: center;
            margin: 20px 0;
        }
        video {
            width: 100%;
            max-width: 500px;
            border-radius: 8px;
            box-shadow: 0 4px 12px rgba(0,0,0,0.2);
        }
        .confidence-display {
            text-align: center;
            padding: 20px;
            margin: 20px 0;
            border-radius: 10px;
            color: white;
            background-color: {{ clip.confidence.color }};
        }
        .confidence-score {
            font-size: 48px;
            font-weight: bold;
            margin: 0;
        }
        .confidence-label {
            font-size: 18px;
            margin: 5px 0;
        }
        .confidence-category {
            font-size: 14px;
            opacity: 0.9;
        }
        .detail-item {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin: 12px 0;
            padding: 12px;
            background: #f8f9fa;
            border-radius: 5px;
        }
        .detail-label {
            font-weight: 500;
            color: #495057;
        }
        .detail-value {
            font-weight: bold;
            color: #212529;
        }
        .progress-bar {
            width: 100%;
            height: 20px;
            background-color: #e9ecef;
            border-radius: 10px;
            overflow: hidden;
            margin: 10px 0;
        }
        .progress-fill {
            height: 100%;
            background-color: {{ clip.confidence.color }};
            width: {{ (clip.normalized_score * 100)|round }}%;
            transition: width 0.3s ease;
            display: flex;
            align-items: center;
            justify-content: center;
            color: white;
            font-weight: bold;
            font-size: 12px;
        }
        .technical-details {
            background: white;
            padding: 25px;
            border-radius: 10px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            margin-bottom: 30px;
        }
        .warning-box {
            background: #fff3cd;
            border: 1px solid #ffeaa7;
            border-radius: 8px;
            padding: 15px;
            margin: 20px 0;
            color: #856404;
        }
        .info-box {
            background: #e7f3ff;
            border: 1px solid #bee5eb;
            border-radius: 8px;
            padding: 15px;
            margin: 20px 0;
            color: #0c5460;
        }
        .section-title {
            color: #333;
            border-bottom: 2px solid #e9ecef;
            padding-bottom: 10px;
            margin-bottom: 20px;
        }
        .probability-explanation {
            background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
            padding: 15px;
            border-radius: 8px;
            margin: 15px 0;
        }
        .formula {
            font-family: monospace;
            background: #2d3748;
            color: #e2e8f0;
            padding: 8px 12px;
            border-radius: 4px;
            margin: 5px 0;
            font-size: 12px;
        }
        /* Modal styles */
        .modal {
            display: none;
            position: fixed;
            z-index: 1000;
            left: 0;
            top: 0;
            width: 100%;
            height: 100%;
            background-color: rgba(0, 0, 0, 0.9);
            overflow: auto;
        }
        .modal-content {
            margin: 50px auto;
            display: block;
            max-width: 90%;
            max-height: 90vh;
        }
        .modal-caption {
            text-align: center;
            color: #ccc;
            padding: 10px 0;
            font-size: 18px;
        }
        .modal-close {
            position: absolute;
            top: 15px;
            right: 35px;
            color: #f1f1f1;
            font-size: 40px;
            font-weight: bold;
            cursor: pointer;
        }
        .modal-close:hover {
            color: #bbb;
        }
    </style>
</head>
<body>
    <!-- Frame Modal -->
    <div id="frameModal" class="modal">
        <span class="modal-close" onclick="closeFrameModal()">&times;</span>
        <img class="modal-content" id="modalImage">
        <div class="modal-caption" id="modalCaption"></div>
    </div>

    <script>
        function showFrameModal(src, frameNumber) {
            const modal = document.getElementById('frameModal');
            const modalImg = document.getElementById('modalImage');
            const captionText = document.getElementById('modalCaption');
            modal.style.display = "block";
            modalImg.src = src;
            captionText.innerHTML = `Frame ${frameNumber} of 16 (adaptive center crop from frames 85-100)`;
        }

        function closeFrameModal() {
            document.getElementById('frameModal').style.display = "none";
        }

        // Close modal when clicking outside the image
        window.onclick = function(event) {
            const modal = document.getElementById('frameModal');
            if (event.target == modal) {
                modal.style.display = "none";
            }
        }
    </script>
    <div class="header">
        <h1>ğŸ” Clip Analysis</h1>
        <h2>{{ clip.clip_name }}</h2>
    </div>

    <a href="/evaluation-results/{{ process_id }}" class="back-button">â† Back to Results</a>

    <div class="analysis-grid">
        <div class="video-section">
            <h3 class="section-title">ğŸ¬ Video Playback</h3>
            <div class="video-container">
                <video controls>
                    <source src="/serve-video/{{ process_id }}/{{ clip.id }}" type="video/mp4">
                    Your browser does not support the video tag.
                </video>
            </div>
            <div class="info-box">
                <strong>ğŸ“¹ Video Information:</strong><br>
                â€¢ Original file: {{ clip.filename }}<br>
                â€¢ This is the exact clip analyzed by the model<br>
                â€¢ Model processes 16 frames at 224x224 resolution
            </div>
        </div>

        <div class="analysis-section">
            <h3 class="section-title">ğŸ¯ Model Prediction</h3>

            <div class="confidence-display">
                <div class="confidence-score">{{ "%.3f"|format(clip.normalized_score) }}</div>
                <div class="confidence-label">{{ clip.confidence.category }}</div>
                <div class="confidence-category">{{ clip.confidence.label }}</div>
            </div>

            <div class="progress-bar">
                <div class="progress-fill">
                    {{ (clip.normalized_score * 100)|round }}%
                </div>
            </div>

            <div class="probability-explanation">
                <strong>ğŸ§® Probability Estimate:</strong><br>
                Using sigmoid transformation: <strong>{{ "%.1f"|format(clip.probability * 100) }}%</strong><br>
                <div class="formula">P = 1 / (1 + e^(-logit))</div>
                <small>* Experimental feature based on estimated logit reconstruction</small>
            </div>
        </div>
    </div>

    <div class="technical-details">
        <h3 class="section-title">ğŸ”§ Technical Analysis</h3>

        <div class="detail-item">
            <span class="detail-label">Clip ID:</span>
            <span class="detail-value">#{{ clip.id + 1 }}</span>
        </div>

        <div class="detail-item">
            <span class="detail-label">Normalized Score:</span>
            <span class="detail-value">{{ "%.6f"|format(clip.normalized_score) }}</span>
        </div>

        <div class="detail-item">
            <span class="detail-label">Estimated Probability:</span>
            <span class="detail-value">{{ "%.4f"|format(clip.probability) }} ({{ "%.1f"|format(clip.probability * 100) }}%)</span>
        </div>

        <div class="detail-item">
            <span class="detail-label">Confidence Level:</span>
            <span class="detail-value">{{ clip.confidence.level }}/5</span>
        </div>

        <div class="detail-item">
            <span class="detail-label">Category:</span>
            <span class="detail-value" style="color: {{ clip.confidence.color }}">{{ clip.confidence.category }}</span>
        </div>

        <div class="detail-item">
            <span class="detail-label">File Path:</span>
            <span class="detail-value" style="font-family: monospace; font-size: 12px;">{{ clip.video_path }}</span>
        </div>
    </div>

    {% if clip.frame_paths and clip.frame_paths|length > 0 %}
    <div class="technical-details">
        <h3 class="section-title">ğŸ–¼ï¸ Analyzed Frames (Adaptive Center Crop)</h3>
        <p style="margin-bottom: 15px; color: #6c757d;">
            These are the exact 16 frames that were analyzed by the model, extracted from frames 85-100 (killshot moment) with an adaptive center crop.
            Crop size is scaled based on video resolution to maintain the same field of view as the training data (6.25% of video width).
            The model internally resizes these to 224x224 during processing.
        </p>
        <div class="frames-grid" style="display: grid; grid-template-columns: repeat(auto-fill, minmax(150px, 1fr)); gap: 10px;">
            {% for i in range(clip.frame_paths|length) %}
            <div class="frame-container" style="text-align: center; border: 1px solid #dee2e6; border-radius: 8px; padding: 8px; background: white;">
                <img src="/serve-frame/{{ process_id }}/{{ clip.id }}/{{ i }}"
                     alt="Frame {{ i + 1 }}"
                     style="width: 100%; height: auto; border-radius: 4px; cursor: pointer;"
                     onclick="showFrameModal('/serve-frame/{{ process_id }}/{{ clip.id }}/{{ i }}', {{ i + 1 }})">
                <div style="font-size: 12px; color: #6c757d; margin-top: 5px;">Frame {{ i + 1 }}/16</div>
                <div style="font-size: 10px; color: #adb5bd;">{{ "%.3f"|format((i / 15.0)) }}s</div>
            </div>
            {% endfor %}
        </div>
        <div style="margin-top: 15px; padding: 10px; background: #f8f9fa; border-radius: 8px;">
            <small style="color: #6c757d;">
                ğŸ’¡ <strong>Tip:</strong> Click on any frame to view it in full size.
                These frames were extracted at 16 FPS and represent approximately 1 second of gameplay.
            </small>
        </div>
    </div>
    {% endif %}

    <div class="technical-details">
        <h3 class="section-title">ğŸ“Š Score Interpretation Guide</h3>

        <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 15px;">
            <div style="padding: 15px; border-radius: 8px; background: #dc3545; color: white; text-align: center;">
                <strong>0.8 - 1.0</strong><br>
                Very High Confidence<br>
                <small>Likely Cheating</small>
            </div>
            <div style="padding: 15px; border-radius: 8px; background: #fd7e14; color: white; text-align: center;">
                <strong>0.6 - 0.8</strong><br>
                High Confidence<br>
                <small>Possible Cheating</small>
            </div>
            <div style="padding: 15px; border-radius: 8px; background: #ffc107; color: #212529; text-align: center;">
                <strong>0.4 - 0.6</strong><br>
                Medium Confidence<br>
                <small>Uncertain</small>
            </div>
            <div style="padding: 15px; border-radius: 8px; background: #20c997; color: white; text-align: center;">
                <strong>0.2 - 0.4</strong><br>
                Low Confidence<br>
                <small>Likely Legitimate</small>
            </div>
            <div style="padding: 15px; border-radius: 8px; background: #28a745; color: white; text-align: center;">
                <strong>0.0 - 0.2</strong><br>
                Very Low Confidence<br>
                <small>Likely Legitimate</small>
            </div>
        </div>
    </div>

    <div class="warning-box">
        <strong>âš ï¸ Important Notes:</strong><br>
        â€¢ These are model predictions, not definitive proof<br>
        â€¢ Always consider multiple factors when evaluating gameplay<br>
        â€¢ Model trained on specific patterns and may not catch all cheating methods<br>
        â€¢ Use results as guidance, not absolute truth
    </div>

    <div class="technical-details">
        <h3 class="section-title">ğŸ§  Model Information</h3>

        <div class="detail-item">
            <span class="detail-label">Architecture:</span>
            <span class="detail-value">Vision Transformer (ViT) with VideoMAE</span>
        </div>

        <div class="detail-item">
            <span class="detail-label">Model Size:</span>
            <span class="detail-value">vit_giant_patch14_224 (~1B parameters)</span>
        </div>

        <div class="detail-item">
            <span class="detail-label">Input Processing:</span>
            <span class="detail-value">16 frames @ 224x224 resolution</span>
        </div>

        <div class="detail-item">
            <span class="detail-label">Training Data:</span>
            <span class="detail-value">CS2 gameplay clips (cheater vs legitimate)</span>
        </div>

        <div class="detail-item">
            <span class="detail-label">Model Path:</span>
            <span class="detail-value" style="font-family: monospace; font-size: 12px;">{{ metadata.model_path }}</span>
        </div>

        <div class="detail-item">
            <span class="detail-label">Analysis Time:</span>
            <span class="detail-value">{{ metadata.evaluation_timestamp[:19].replace('T', ' ') }}</span>
        </div>
    </div>
</body>
</html>
```

`templates/evaluation.html`:

```html
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Evaluation Progress</title>
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            background-color: #f5f5f5;
        }
        .header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 30px;
            border-radius: 10px;
            margin-bottom: 30px;
            text-align: center;
        }
        .progress-container {
            background: white;
            padding: 30px;
            border-radius: 10px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            margin-bottom: 20px;
        }
        .progress-bar {
            width: 100%;
            height: 20px;
            background-color: #e0e0e0;
            border-radius: 10px;
            overflow: hidden;
            margin: 10px 0;
        }
        .progress-fill {
            height: 100%;
            background: linear-gradient(90deg, #4CAF50, #45a049);
            width: 0%;
            transition: width 0.3s ease;
        }
        .log-container {
            background: #1e1e1e;
            color: #00ff00;
            padding: 20px;
            border-radius: 10px;
            font-family: 'Courier New', monospace;
            font-size: 14px;
            max-height: 500px;
            overflow-y: auto;
            margin-top: 20px;
            border: 2px solid #333;
        }
        .status {
            font-size: 18px;
            margin: 10px 0;
            padding: 10px;
            border-radius: 5px;
        }
        .status.running {
            background-color: #fff3cd;
            color: #856404;
            border: 1px solid #ffeaa7;
        }
        .status.completed {
            background-color: #d4edda;
            color: #155724;
            border: 1px solid #c3e6cb;
        }
        .status.error {
            background-color: #f8d7da;
            color: #721c24;
            border: 1px solid #f5c6cb;
        }
        .back-button {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 12px 24px;
            border: none;
            border-radius: 5px;
            font-size: 16px;
            cursor: pointer;
            text-decoration: none;
            display: inline-block;
            margin-bottom: 20px;
        }
        .back-button:hover {
            background: linear-gradient(135deg, #5a67d8 0%, #6b46c1 100%);
        }
        .evaluation-details {
            background: white;
            padding: 20px;
            border-radius: 10px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            margin-bottom: 20px;
        }
        .detail-item {
            margin: 8px 0;
            padding: 8px;
            background: #f8f9fa;
            border-radius: 5px;
        }
        .detail-label {
            font-weight: bold;
            color: #495057;
        }
    </style>
</head>
<body>
    <div class="header">
        <h1>ğŸ” Evaluation Progress</h1>
        <p>Analyzing clips for cheating detection</p>
    </div>

    <a href="/" class="back-button">â† Back to Main</a>

    <div class="evaluation-details">
        <h3>Evaluation Configuration</h3>
        <div class="detail-item">
            <span class="detail-label">Model:</span> {{ model_name }}
        </div>
        <div class="detail-item">
            <span class="detail-label">Data Path:</span> {{ data_path }}
        </div>
        <div class="detail-item">
            <span class="detail-label">Process ID:</span> {{ process_id }}
        </div>
        <div class="detail-item">
            <span class="detail-label">Started:</span> <span id="start-time">{{ start_time }}</span>
        </div>
    </div>

    <div class="progress-container">
        <div id="status" class="status running">ğŸ”„ Evaluation in progress...</div>
        <div class="progress-bar">
            <div id="progress-fill" class="progress-fill"></div>
        </div>
        <div id="progress-text">Starting evaluation...</div>
    </div>

    <div class="log-container" id="log-container">
        <div id="logs">Connecting to evaluation stream...</div>
    </div>

    <script>
        const processId = '{{ process_id }}';
        const eventSource = new EventSource(`/stream-evaluation/${processId}`);
        const logsDiv = document.getElementById('logs');
        const statusDiv = document.getElementById('status');
        const progressFill = document.getElementById('progress-fill');
        const progressText = document.getElementById('progress-text');
        const logContainer = document.getElementById('log-container');

        let logLines = [];

        eventSource.onmessage = function(event) {
            const data = JSON.parse(event.data);

            if (data.type === 'log') {
                logLines.push(data.content);
                logsDiv.innerHTML = logLines.join('<br>');
                logContainer.scrollTop = logContainer.scrollHeight;

                // Update progress text
                progressText.textContent = data.content;

                // Simple progress estimation based on log content
                if (data.content.includes('Loading model')) {
                    progressFill.style.width = '20%';
                } else if (data.content.includes('DataLoader')) {
                    progressFill.style.width = '40%';
                } else if (data.content.includes('Start evaluation')) {
                    progressFill.style.width = '60%';
                } else if (data.content.includes('evaluating')) {
                    progressFill.style.width = '80%';
                }

            } else if (data.type === 'results_ready') {
                // Show results are ready with button to view detailed analysis
                const resultsButton = document.createElement('div');
                resultsButton.innerHTML = `
                    <div style="margin: 20px 0; padding: 20px; background: #e7f3ff; border: 2px solid #0066cc; border-radius: 10px; text-align: center;">
                        <h3 style="color: #0066cc; margin: 0 0 10px 0;">ğŸ“Š Detailed Results Ready!</h3>
                        <p style="margin: 0 0 15px 0;">Comprehensive analysis of ${data.total_clips} clips is complete.</p>
                        <a href="/evaluation-results/${data.process_id}"
                           style="background: #0066cc; color: white; padding: 12px 24px; text-decoration: none; border-radius: 5px; font-weight: bold;">
                           View Detailed Results â†’
                        </a>
                    </div>
                `;
                document.getElementById('log-container').appendChild(resultsButton);
                progressFill.style.width = '95%';

            } else if (data.type === 'status') {
                if (data.status === 'completed') {
                    statusDiv.className = 'status completed';
                    statusDiv.innerHTML = 'âœ… Evaluation completed successfully!';
                    progressFill.style.width = '100%';
                    progressText.textContent = 'Evaluation finished';
                    eventSource.close();
                } else if (data.status === 'error') {
                    statusDiv.className = 'status error';
                    statusDiv.innerHTML = 'âŒ Evaluation failed';
                    progressText.textContent = 'Evaluation failed';
                    eventSource.close();
                }
            }
        };

        eventSource.onerror = function(event) {
            console.log('EventSource failed:', event);
            statusDiv.className = 'status error';
            statusDiv.innerHTML = 'âŒ Connection lost';
            progressText.textContent = 'Connection error';
        };

        // Auto-scroll logs
        const observer = new MutationObserver(function(mutations) {
            logContainer.scrollTop = logContainer.scrollHeight;
        });

        observer.observe(logsDiv, {
            childList: true,
            subtree: true,
            characterData: true
        });
    </script>
</body>
</html>
```

`templates/evaluation_results.html`:

```html
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Evaluation Results - CS2 Cheat Detection</title>
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            max-width: 1400px;
            margin: 0 auto;
            padding: 20px;
            background-color: #f5f5f5;
            line-height: 1.6;
        }
        .header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 30px;
            border-radius: 10px;
            margin-bottom: 30px;
            text-align: center;
        }
        .back-button {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 12px 24px;
            border: none;
            border-radius: 5px;
            font-size: 16px;
            cursor: pointer;
            text-decoration: none;
            display: inline-block;
            margin-bottom: 20px;
        }
        .summary-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 20px;
            margin-bottom: 30px;
        }
        .summary-card {
            background: white;
            padding: 25px;
            border-radius: 10px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }
        .summary-card h3 {
            margin: 0 0 15px 0;
            color: #333;
            border-bottom: 2px solid #e9ecef;
            padding-bottom: 10px;
        }
        .stat-item {
            display: flex;
            justify-content: space-between;
            margin: 8px 0;
            padding: 8px;
            background: #f8f9fa;
            border-radius: 5px;
        }
        .stat-label {
            font-weight: 500;
            color: #495057;
        }
        .stat-value {
            font-weight: bold;
            color: #212529;
        }
        .confidence-distribution {
            display: flex;
            gap: 10px;
            margin: 15px 0;
        }
        .confidence-bar {
            flex: 1;
            text-align: center;
            padding: 8px;
            border-radius: 5px;
            color: white;
            font-weight: bold;
            font-size: 12px;
        }
        .high-conf { background-color: #dc3545; }
        .med-conf { background-color: #ffc107; color: #212529; }
        .low-conf { background-color: #28a745; }

        .results-table {
            background: white;
            border-radius: 10px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            overflow: hidden;
            margin-bottom: 30px;
        }
        .table-header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 20px;
        }
        .table-container {
            max-height: 600px;
            overflow-y: auto;
        }
        .clip-row {
            display: grid;
            grid-template-columns: 60px 2fr 120px 150px 120px 100px;
            align-items: center;
            padding: 15px 20px;
            border-bottom: 1px solid #e9ecef;
            transition: background-color 0.2s;
        }
        .clip-row:hover {
            background-color: #f8f9fa;
        }
        .clip-id {
            font-weight: bold;
            color: #495057;
        }
        .clip-name {
            font-family: monospace;
            color: #212529;
            word-break: break-all;
        }
        .confidence-indicator {
            display: flex;
            align-items: center;
            gap: 8px;
        }
        .confidence-badge {
            padding: 4px 8px;
            border-radius: 12px;
            font-size: 11px;
            font-weight: bold;
            color: white;
            white-space: nowrap;
        }
        .score-display {
            text-align: center;
        }
        .normalized-score {
            font-size: 18px;
            font-weight: bold;
            color: #212529;
        }
        .probability {
            font-size: 12px;
            color: #6c757d;
        }
        .progress-bar {
            width: 100%;
            height: 8px;
            background-color: #e9ecef;
            border-radius: 4px;
            overflow: hidden;
            margin: 4px 0;
        }
        .progress-fill {
            height: 100%;
            transition: width 0.3s ease;
        }
        .action-buttons {
            display: flex;
            gap: 5px;
        }
        .btn-small {
            padding: 6px 12px;
            border: none;
            border-radius: 4px;
            cursor: pointer;
            text-decoration: none;
            font-size: 12px;
            font-weight: 500;
        }
        .btn-primary {
            background: #007bff;
            color: white;
        }
        .btn-secondary {
            background: #6c757d;
            color: white;
        }
        .histogram {
            background: white;
            padding: 25px;
            border-radius: 10px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            margin-bottom: 30px;
        }
        .histogram-bars {
            display: flex;
            align-items: end;
            gap: 4px;
            height: 200px;
            margin: 20px 0;
        }
        .histogram-bar {
            flex: 1;
            background: linear-gradient(to top, #667eea, #764ba2);
            border-radius: 2px 2px 0 0;
            position: relative;
            transition: opacity 0.2s;
        }
        .histogram-bar:hover {
            opacity: 0.8;
        }
        .bar-label {
            position: absolute;
            bottom: -25px;
            left: 50%;
            transform: translateX(-50%);
            font-size: 10px;
            white-space: nowrap;
        }
        .bar-value {
            position: absolute;
            top: -20px;
            left: 50%;
            transform: translateX(-50%);
            font-size: 10px;
            font-weight: bold;
        }
        .explanation-box {
            background: #e7f3ff;
            border: 1px solid #bee5eb;
            border-radius: 10px;
            padding: 20px;
            margin-bottom: 30px;
        }
        .explanation-box h4 {
            color: #0c5460;
            margin: 0 0 10px 0;
        }
        .explanation-box p {
            margin: 5px 0;
            color: #0c5460;
        }
    </style>
</head>
<body>
    <div class="header">
        <h1>ğŸ¯ CS2 Cheat Detection Results</h1>
        <p>Comprehensive analysis of {{ results.summary_stats.total_clips }} gameplay clips</p>
    </div>

    <div style="display: flex; gap: 10px; margin-bottom: 20px;">
        <a href="/" class="back-button">â† Back to Main</a>
        <a href="/saved-evaluations" class="back-button" style="background: #6c757d;">ğŸ“‚ View Saved Results</a>
        <a href="/export-results/{{ process_id }}" class="back-button" style="background: #28a745;">ğŸ’¾ Export as JSON</a>
    </div>

    <div class="explanation-box">
        <h4>ğŸ“Š Understanding Your Results</h4>
        <p><strong>Confidence Scores (0.0 - 1.0):</strong> Normalized scores where higher values indicate stronger evidence of cheating behavior.</p>
        <p><strong>Probability Estimates:</strong> Converted using sigmoid transformation to approximate the likelihood of cheating (experimental).</p>
        <p><strong>Model Analysis:</strong> Based on {{ results.evaluation_metadata.total_clips_processed }} clips analyzed using Vision Transformer architecture.</p>
    </div>

    <div class="summary-grid">
        <div class="summary-card">
            <h3>ğŸ“ˆ Score Statistics</h3>
            <div class="stat-item">
                <span class="stat-label">Total Clips:</span>
                <span class="stat-value">{{ results.summary_stats.total_clips }}</span>
            </div>
            <div class="stat-item">
                <span class="stat-label">Average Score:</span>
                <span class="stat-value">{{ "%.3f"|format(results.summary_stats.mean_score) }}</span>
            </div>
            <div class="stat-item">
                <span class="stat-label">Median Score:</span>
                <span class="stat-value">{{ "%.3f"|format(results.summary_stats.median_score) }}</span>
            </div>
            <div class="stat-item">
                <span class="stat-label">Score Range:</span>
                <span class="stat-value">{{ "%.3f"|format(results.summary_stats.min_score) }} - {{ "%.3f"|format(results.summary_stats.max_score) }}</span>
            </div>
            <div class="stat-item">
                <span class="stat-label">Standard Deviation:</span>
                <span class="stat-value">{{ "%.3f"|format(results.summary_stats.std_score) }}</span>
            </div>
        </div>

        <div class="summary-card">
            <h3>ğŸ¯ Confidence Distribution</h3>
            <div class="confidence-distribution">
                <div class="confidence-bar high-conf">
                    High<br>{{ results.summary_stats.high_confidence_count }}
                </div>
                <div class="confidence-bar med-conf">
                    Medium<br>{{ results.summary_stats.medium_confidence_count }}
                </div>
                <div class="confidence-bar low-conf">
                    Low<br>{{ results.summary_stats.low_confidence_count }}
                </div>
            </div>
            <div class="stat-item">
                <span class="stat-label">High Confidence (â‰¥0.6):</span>
                <span class="stat-value">{{ results.summary_stats.high_confidence_count }} clips</span>
            </div>
            <div class="stat-item">
                <span class="stat-label">Medium Confidence (0.4-0.6):</span>
                <span class="stat-value">{{ results.summary_stats.medium_confidence_count }} clips</span>
            </div>
            <div class="stat-item">
                <span class="stat-label">Low Confidence (<0.4):</span>
                <span class="stat-value">{{ results.summary_stats.low_confidence_count }} clips</span>
            </div>
        </div>
    </div>

    <div class="histogram">
        <h3>ğŸ“Š Score Distribution</h3>
        <div class="histogram-bars">
            {% for i in range(results.distribution.bins|length) %}
            <div class="histogram-bar"
                 style="height: {{ (results.distribution.counts[i] / (results.distribution.counts|max if results.distribution.counts|max > 0 else 1) * 100)|round }}%">
                <div class="bar-value">{{ results.distribution.counts[i] }}</div>
                <div class="bar-label">{{ results.distribution.bins[i] }}</div>
            </div>
            {% endfor %}
        </div>
        <p style="text-align: center; color: #6c757d; font-size: 14px;">
            Distribution of confidence scores across {{ results.summary_stats.total_clips }} clips
        </p>
    </div>

    <div class="results-table">
        <div class="table-header">
            <h3 style="margin: 0;">ğŸ¬ Individual Clip Analysis</h3>
        </div>
        <div class="table-container">
            <div class="clip-row" style="background: #f8f9fa; font-weight: bold;">
                <div>ID</div>
                <div>Clip Name</div>
                <div>Confidence</div>
                <div>Score & Probability</div>
                <div>Category</div>
                <div>Actions</div>
            </div>
            {% for clip in results.clip_results %}
            <div class="clip-row">
                <div class="clip-id">#{{ clip.id + 1 }}</div>
                <div class="clip-name">{{ clip.clip_name }}</div>
                <div class="confidence-indicator">
                    <div class="progress-bar">
                        <div class="progress-fill"
                             style="width: {{ (clip.normalized_score * 100)|round }}%; background-color: {{ clip.confidence.color }}"></div>
                    </div>
                </div>
                <div class="score-display">
                    <div class="normalized-score">{{ "%.3f"|format(clip.normalized_score) }}</div>
                    <div class="probability">{{ "%.1f"|format(clip.probability * 100) }}% prob</div>
                </div>
                <div>
                    <div class="confidence-badge" style="background-color: {{ clip.confidence.color }}">
                        {{ clip.confidence.category }}
                    </div>
                    <div style="font-size: 11px; color: #6c757d; margin-top: 2px;">
                        {{ clip.confidence.label }}
                    </div>
                </div>
                <div class="action-buttons">
                    <a href="/clip-analysis/{{ process_id }}/{{ clip.id }}" class="btn-small btn-primary">Analyze</a>
                    <a href="/serve-video/{{ process_id }}/{{ clip.id }}" class="btn-small btn-secondary" target="_blank">Video</a>
                </div>
            </div>
            {% endfor %}
        </div>
    </div>

    <div class="summary-card">
        <h3>ğŸ”§ Technical Details</h3>
        <div class="stat-item">
            <span class="stat-label">Model Used:</span>
            <span class="stat-value">{{ results.evaluation_metadata.model_path.split('/')[-1] }}</span>
        </div>
        <div class="stat-item">
            <span class="stat-label">Data Source:</span>
            <span class="stat-value">{{ results.evaluation_metadata.clips_path.split('/')[-1] }}</span>
        </div>
        <div class="stat-item">
            <span class="stat-label">Analysis Time:</span>
            <span class="stat-value">{{ results.evaluation_metadata.evaluation_timestamp[:19].replace('T', ' ') }}</span>
        </div>
        <div class="stat-item">
            <span class="stat-label">Clips Processed:</span>
            <span class="stat-value">{{ results.evaluation_metadata.total_clips_processed }}</span>
        </div>
    </div>
</body>
</html>
```

`templates/index.html`:

```html
<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>WALD01 - CS2 Cheat Detection</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
  </head>
  <body>
    <div class="container">
      <h1 class="mt-5">WALD01 - CS2 Cheat Detection</h1>
      <p class="lead">A simple interface for training and using the CS2 cheat detection model.</p>

      <div class="text-center mt-3 mb-4">
        <a href="/saved-evaluations" class="btn btn-secondary">
          ğŸ“‚ View Saved Evaluation Results
        </a>
      </div>

      {% with messages = get_flashed_messages() %}
        {% if messages %}
          <div class="alert alert-info" role="alert">
            {% for message in messages %}
              {{ message }}<br>
            {% endfor %}
          </div>
        {% endif %}
      {% endwith %}

      <!-- Auto-Edit Section -->
      <div id="autoedit-section" class="mt-5 card">
        <div class="card-body">
          <h2 class="card-title">1. Process Raw Footage</h2>
          <p class="text-muted">Upload a CS2 gameplay video to automatically extract 2-second clips around detected kills.</p>
          
          
          <form id="process-form" action="/process" method="post" enctype="multipart/form-data">
            <div class="mb-3">
              <label for="videoFile" class="form-label">Select a raw video file to process:</label>
              <input type="file" class="form-control" id="videoFile" name="videoFile" accept=".mp4,.mov,.avi" required>
              <div class="form-text">Choose your CS2 gameplay recording with clear audio.</div>
            </div>
            <button id="process-button" type="submit" class="btn btn-primary">
              <span id="process-spinner" class="spinner-border spinner-border-sm" role="status" aria-hidden="true" style="display: none;"></span>
              <span id="process-button-text">Process Video</span>
            </button>
            <span id="processing-message" class="ms-3" style="display: none;">Processing video... This may take several minutes for large files.</span>
          </form>
        </div>
      </div>

      <!-- Training Section -->
      <div id="training-section" class="mt-5 card">
        <div class="card-body">
            <h2 class="card-title">2. Train or Fine-Tune a Model</h2>
            <form action="/train" method="post">
                <div class="mb-3">
                    <label for="clipsDirectory" class="form-label">Select directory of processed clips:</label>
                    <select class="form-select" id="clipsDirectory" name="clipsDirectory" required>
                        {% for clip_dir in processed_clips %}
                            <option value="{{ clip_dir }}">{{ clip_dir }}</option>
                        {% endfor %}
                    </select>
                </div>
                <div class="mb-3">
                    <label class="form-label">Training Type:</label>
                    <div class="form-check">
                        <input class="form-check-input" type="radio" name="trainingType" id="trainNew" value="new" checked>
                        <label class="form-check-label" for="trainNew">
                            Train a new model
                        </label>
                    </div>
                    <div class="form-check">
                        <input class="form-check-input" type="radio" name="trainingType" id="fineTune" value="finetune">
                        <label class="form-check-label" for="fineTune">
                            Fine-tune an existing model
                        </label>
                    </div>
                </div>
                <div class="mb-3">
                    <label for="modelType" class="form-label">Label clips as:</label>
                    <select class="form-select" id="modelType" name="modelType" required>
                        <option value="cheater">Cheater</option>
                        <option value="not-cheater">Not Cheater</option>
                    </select>
                </div>
                <div class="mb-3" id="existing-model-selector" style="display: none;">
                    <label for="existingModel" class="form-label">Select existing model to fine-tune:</label>
                    <select class="form-select" id="existingModel" name="existingModel">
                        {% for model in existing_models %}
                            <option value="{{ model.path }}">{{ model.display_name }}</option>
                        {% endfor %}
                    </select>
                </div>
                <button type="submit" class="btn btn-success">Start Training</button>
            </form>
        </div>
      </div>
      
      <!-- Evaluation Section -->
      <div id="evaluation-section" class="mt-5 card">
        <div class="card-body">
            <h2 class="card-title">3. Test Clips for Cheating</h2>
            <form action="/evaluate" method="post">
                <div class="mb-3">
                    <label for="evalClipsDirectory" class="form-label">Select directory of clips to evaluate:</label>
                    <select class="form-select" id="evalClipsDirectory" name="clipsDirectory" required>
                        {% for clip_dir in processed_clips %}
                            <option value="{{ clip_dir }}">{{ clip_dir }}</option>
                        {% endfor %}
                    </select>
                </div>
                <div class="mb-3">
                    <label for="evalModel" class="form-label">Select model to use for evaluation:</label>
                    <select class="form-select" id="evalModel" name="model" required>
                        {% for model in existing_models %}
                            <option value="{{ model.path }}">{{ model.display_name }}</option>
                        {% endfor %}
                    </select>
                </div>
                <button type="submit" class="btn btn-danger">Evaluate Clips</button>
            </form>
            <div id="results-section" class="mt-4">
                {% if results_image %}
                    <h3>Evaluation Results:</h3>
                    <img src="{{ url_for('static', filename=results_image) }}" class="img-fluid" alt="Evaluation Results">
                {% endif %}
            </div>
        </div>
      </div>

    </div>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
    <script>
        // Show/hide the existing model selector based on the training type
        const trainingTypeRadios = document.querySelectorAll('input[name="trainingType"]');
        const existingModelSelector = document.getElementById('existing-model-selector');

        trainingTypeRadios.forEach(radio => {
            radio.addEventListener('change', function() {
                if (this.value === 'finetune') {
                    existingModelSelector.style.display = 'block';
                } else {
                    existingModelSelector.style.display = 'none';
                }
            });
        });

        // --- Loading indicator for video processing ---
        const processForm = document.getElementById('process-form');
        const processButton = document.getElementById('process-button');
        const processSpinner = document.getElementById('process-spinner');
        const processButtonText = document.getElementById('process-button-text');
        const processingMessage = document.getElementById('processing-message');
        const videoFileInput = document.getElementById('videoFile');

        // File size display (no validation)
        videoFileInput.addEventListener('change', function(e) {
            const file = e.target.files[0];
            if (file) {
                const fileSizeMB = (file.size / (1024 * 1024)).toFixed(1);
                const fileSizeGB = (file.size / (1024 * 1024 * 1024)).toFixed(2);
                
                // Show file size feedback
                const feedback = document.querySelector('.form-text');
                if (fileSizeGB >= 1) {
                    feedback.textContent = `Selected: ${file.name} (${fileSizeGB}GB)`;
                } else {
                    feedback.textContent = `Selected: ${file.name} (${fileSizeMB}MB)`;
                }
            }
        });

        processForm.addEventListener('submit', (e) => {
            const file = videoFileInput.files[0];
            if (!file) {
                e.preventDefault();
                alert('Please select a video file first.');
                return;
            }
            
            // Disable button and show spinner
            processButton.disabled = true;
            processSpinner.style.display = 'inline-block';
            processButtonText.textContent = 'Processing...';
            processingMessage.style.display = 'inline';
        });
    </script>
  </body>
</html>

```

`templates/processing.html`:

```html
<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Processing... - WALD01</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
    <style>
        #log-container {
            background-color: #212529;
            color: #f8f9fa;
            font-family: monospace;
            white-space: pre-wrap;
            word-wrap: break-word;
            height: 50vh;
            overflow-y: scroll;
            border-radius: 5px;
            padding: 15px;
        }
        
        .progress-container {
            margin: 20px 0;
        }
        
        .progress {
            height: 30px;
            font-size: 14px;
        }
        
        .progress-bar {
            transition: width 0.5s ease-in-out;
        }
        
        .status-text {
            margin-top: 10px;
            font-weight: 500;
            color: #495057;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1 class="mt-5">Processing Video...</h1>
        <p class="lead">Please wait while the video is being processed. You can see the real-time progress below.</p>
        
        <!-- Progress Bar -->
        <div class="progress-container">
            <div class="progress">
                <div id="progress-bar" class="progress-bar progress-bar-striped progress-bar-animated bg-primary" 
                     role="progressbar" style="width: 0%" aria-valuenow="0" aria-valuemin="0" aria-valuemax="100">
                    0%
                </div>
            </div>
            <div id="status-text" class="status-text">Initializing...</div>
        </div>
        
        <div id="log-container"></div>

        <div class="mt-3">
            <a href="/" id="back-button" class="btn btn-secondary" style="display: none;">Back to Main Page</a>
            <button id="cancel-button" class="btn btn-danger">Cancel Process</button>
        </div>
    </div>

    <script>
        const logContainer = document.getElementById('log-container');
        const backButton = document.getElementById('back-button');
        const cancelButton = document.getElementById('cancel-button');
        const progressBar = document.getElementById('progress-bar');
        const statusText = document.getElementById('status-text');
        const processId = "{{ process_id }}";

        function updateProgress(percentage, status) {
            progressBar.style.width = percentage + '%';
            progressBar.setAttribute('aria-valuenow', percentage);
            progressBar.innerText = percentage + '%';
            
            if (status) {
                statusText.innerText = status;
            }
            
            // Change color based on progress
            if (percentage < 30) {
                progressBar.className = 'progress-bar progress-bar-striped progress-bar-animated bg-danger';
            } else if (percentage < 60) {
                progressBar.className = 'progress-bar progress-bar-striped progress-bar-animated bg-warning';
            } else if (percentage < 100) {
                progressBar.className = 'progress-bar progress-bar-striped progress-bar-animated bg-info';
            } else {
                progressBar.className = 'progress-bar bg-success';
            }
        }

        function appendLog(message) {
            // Check if message contains progress information
            const progressMatch = message.match(/^PROGRESS:(\d+):(.*)$/);
            
            if (progressMatch) {
                const percentage = parseInt(progressMatch[1]);
                const statusMsg = progressMatch[2];
                updateProgress(percentage, statusMsg);
                
                // Still show the status message in the log (without PROGRESS prefix)
                logContainer.innerHTML += statusMsg + '<br>';
            } else {
                logContainer.innerHTML += message + '<br>';
            }
            
            logContainer.scrollTop = logContainer.scrollHeight; // Auto-scroll
        }

        appendLog('Connecting to server for logs...');

        const eventSource = new EventSource(`/stream-logs/${processId}`);

        eventSource.onmessage = function(event) {
            if (event.data === "PROCESS_COMPLETE") {
                updateProgress(100, 'Process Complete!');
                appendLog('\n\n--- \n**Process finished!** You can now return to the main page.');
                eventSource.close();
                backButton.style.display = 'block';
                cancelButton.style.display = 'none';
            } else {
                appendLog(event.data);
            }
        };

        eventSource.onerror = function(err) {
            appendLog('\n\n--- \n**Error connecting to the server.** Please check the console and try again.');
            console.error("EventSource failed:", err);
            eventSource.close();
            backButton.style.display = 'block';
            cancelButton.style.display = 'none';
            updateProgress(0, 'Error occurred');
        };

        cancelButton.addEventListener('click', () => {
            appendLog('\n\n--- \n**Cancelling process...**');
            fetch(`/cancel/${processId}`, { method: 'POST' })
                .then(response => response.text())
                .then(data => {
                    appendLog(data);
                    cancelButton.disabled = true;
                    updateProgress(0, 'Process cancelled');
                })
                .catch(error => {
                    appendLog('Error cancelling process: ' + error);
                });
        });
    </script>
</body>
</html>
```

`templates/saved_evaluations.html`:

```html
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Saved Evaluation Results</title>
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            background-color: #f5f5f5;
        }
        .header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 30px;
            border-radius: 10px;
            margin-bottom: 30px;
            text-align: center;
        }
        .back-button {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 12px 24px;
            border: none;
            border-radius: 5px;
            font-size: 16px;
            cursor: pointer;
            text-decoration: none;
            display: inline-block;
            margin-bottom: 20px;
        }
        .evaluations-container {
            background: white;
            border-radius: 10px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            overflow: hidden;
        }
        .evaluations-header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 20px;
        }
        .evaluation-item {
            display: grid;
            grid-template-columns: 40px 1fr 150px 100px 200px 150px;
            align-items: center;
            padding: 15px 20px;
            border-bottom: 1px solid #e9ecef;
            transition: background-color 0.2s;
        }
        .evaluation-item:hover {
            background-color: #f8f9fa;
        }
        .evaluation-header-row {
            display: grid;
            grid-template-columns: 40px 1fr 150px 100px 200px 150px;
            padding: 15px 20px;
            background: #f8f9fa;
            font-weight: bold;
            border-bottom: 2px solid #dee2e6;
        }
        .evaluation-id {
            font-weight: bold;
            color: #495057;
        }
        .evaluation-dataset {
            font-family: monospace;
            color: #212529;
            font-size: 14px;
        }
        .evaluation-model {
            font-size: 13px;
            color: #6c757d;
        }
        .evaluation-clips {
            text-align: center;
            font-weight: bold;
        }
        .evaluation-time {
            font-size: 13px;
            color: #6c757d;
        }
        .action-buttons {
            display: flex;
            gap: 5px;
        }
        .btn-small {
            padding: 6px 12px;
            border: none;
            border-radius: 4px;
            cursor: pointer;
            text-decoration: none;
            font-size: 12px;
            font-weight: 500;
        }
        .btn-primary {
            background: #007bff;
            color: white;
        }
        .btn-success {
            background: #28a745;
            color: white;
        }
        .empty-state {
            padding: 60px;
            text-align: center;
            color: #6c757d;
        }
        .empty-icon {
            font-size: 48px;
            margin-bottom: 20px;
        }
        .info-box {
            background: #e7f3ff;
            border: 1px solid #bee5eb;
            border-radius: 10px;
            padding: 20px;
            margin-bottom: 20px;
            color: #0c5460;
        }
    </style>
</head>
<body>
    <div class="header">
        <h1>ğŸ“‚ Saved Evaluation Results</h1>
        <p>Access previously analyzed gameplay evaluations</p>
    </div>

    <a href="/" class="back-button">â† Back to Main</a>

    <div class="info-box">
        <strong>ğŸ’¾ About Saved Results:</strong> All evaluation results are automatically saved and can be accessed later.
        This includes the analysis scores, statistics, and the 16 frames analyzed for each clip.
        Results persist across sessions and can be exported for further analysis.
    </div>

    {% if evaluations %}
    <div class="evaluations-container">
        <div class="evaluations-header">
            <h3 style="margin: 0;">ğŸ“Š Available Evaluations</h3>
        </div>

        <div class="evaluation-header-row">
            <div>#</div>
            <div>Dataset</div>
            <div>Model</div>
            <div>Clips</div>
            <div>Timestamp</div>
            <div>Actions</div>
        </div>

        {% for eval in evaluations %}
        <div class="evaluation-item">
            <div class="evaluation-id">{{ loop.index }}</div>
            <div>
                <div class="evaluation-dataset">{{ eval.dataset }}</div>
                <div style="font-size: 10px; color: #adb5bd; margin-top: 2px;">ID: {{ eval.process_id[:8] }}...</div>
            </div>
            <div class="evaluation-model">{{ eval.model }}</div>
            <div class="evaluation-clips">{{ eval.clips_count }}</div>
            <div class="evaluation-time">{{ eval.timestamp[:19].replace('T', ' ') }}</div>
            <div class="action-buttons">
                <a href="/evaluation-results/{{ eval.process_id }}" class="btn-small btn-primary">View</a>
                <a href="/export-results/{{ eval.process_id }}" class="btn-small btn-success">Export</a>
            </div>
        </div>
        {% endfor %}
    </div>
    {% else %}
    <div class="evaluations-container">
        <div class="empty-state">
            <div class="empty-icon">ğŸ“­</div>
            <h3>No Saved Evaluations</h3>
            <p>Run an evaluation to see results here.</p>
            <a href="/" class="btn-small btn-primary" style="padding: 12px 24px; font-size: 14px; margin-top: 20px; display: inline-block;">
                Go to Main Page
            </a>
        </div>
    </div>
    {% endif %}
</body>
</html>
```

`templates/training.html`:

```html
<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Training Model - WALD01</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
    <style>
      #log-output {
        height: 500px;
        overflow-y: auto;
        background-color: #1e1e1e;
        color: #d4d4d4;
        padding: 15px;
        border-radius: 5px;
        font-family: 'Courier New', monospace;
        font-size: 14px;
        white-space: pre-wrap;
        word-wrap: break-word;
      }
      .progress {
        height: 30px;
      }
      .spinner-border {
        width: 1rem;
        height: 1rem;
      }
      #status-message {
        font-weight: bold;
        margin-bottom: 10px;
      }
    </style>
  </head>
  <body>
    <div class="container">
      <h1 class="mt-5">Training Model</h1>

      <div class="card mt-4">
        <div class="card-body">
          <div id="status-message">
            <span class="spinner-border text-primary me-2" role="status" aria-hidden="true"></span>
            Initializing training process...
          </div>

          <div class="progress mb-3" style="display: none;" id="progress-container">
            <div class="progress-bar progress-bar-striped progress-bar-animated"
                 id="progress-bar"
                 role="progressbar"
                 style="width: 0%">0%</div>
          </div>

          <div id="log-output">Waiting for training to start...</div>

          <div class="mt-3">
            <button class="btn btn-secondary" onclick="window.location.href='/'">
              Back to Home
            </button>
            <button class="btn btn-danger ms-2" id="stop-button" style="display: none;">
              Stop Training
            </button>
          </div>
        </div>
      </div>
    </div>

    <script>
      const processId = '{{ process_id }}';
      let eventSource = null;
      let isComplete = false;

      function formatLog(message) {
        // Convert HTML breaks to newlines for better display
        return message.replace(/<br>/gi, '\n').replace(/<\/?b>/gi, '');
      }

      function appendLog(message) {
        const logOutput = document.getElementById('log-output');
        if (logOutput.textContent === 'Waiting for training to start...') {
          logOutput.innerHTML = '';
        }

        // Parse HTML tags for formatting
        const formattedMessage = message
          .replace(/<b>/g, '<span style="color: #4CAF50; font-weight: bold;">')
          .replace(/<\/b>/g, '</span>')
          .replace(/<br>/g, '\n');

        const span = document.createElement('span');
        span.innerHTML = formattedMessage;
        logOutput.appendChild(span);

        // Auto-scroll to bottom
        logOutput.scrollTop = logOutput.scrollHeight;
      }

      function updateStatus(message) {
        const statusDiv = document.getElementById('status-message');
        statusDiv.innerHTML = `<span class="spinner-border text-primary me-2" role="status" aria-hidden="true"></span> ${message}`;
      }

      function completeTraining(success = true) {
        isComplete = true;
        const statusDiv = document.getElementById('status-message');
        if (success) {
          statusDiv.innerHTML = '<span class="text-success">âœ“ Training Complete!</span>';
        } else {
          statusDiv.innerHTML = '<span class="text-danger">âœ— Training Failed</span>';
        }

        // Hide progress bar
        document.getElementById('progress-container').style.display = 'none';
        document.getElementById('stop-button').style.display = 'none';
      }

      function startStreaming() {
        eventSource = new EventSource('/stream-training/' + processId);

        eventSource.onmessage = function(event) {
          const message = event.data;

          if (message === 'PROCESS_COMPLETE') {
            completeTraining(true);
            eventSource.close();
            return;
          }

          // Update status based on content
          if (message.includes('Starting preprocessing')) {
            updateStatus('Preprocessing clips for training...');
            document.getElementById('progress-container').style.display = 'block';
          } else if (message.includes('Processing clip')) {
            // Extract progress from "Processing clip X/Y"
            const match = message.match(/Processing clip (\d+)\/(\d+)/);
            if (match) {
              const current = parseInt(match[1]);
              const total = parseInt(match[2]);
              const percent = Math.round((current / total) * 100);
              const progressBar = document.getElementById('progress-bar');
              progressBar.style.width = percent + '%';
              progressBar.textContent = percent + '%';
            }
          } else if (message.includes('Starting model training')) {
            updateStatus('Training model... This may take a while...');
            document.getElementById('progress-container').style.display = 'none';
            document.getElementById('stop-button').style.display = 'inline-block';
          } else if (message.includes('epoch')) {
            // Try to extract epoch progress
            const match = message.match(/epoch[:\s]+(\d+)/i);
            if (match) {
              updateStatus(`Training model... Epoch ${match[1]}`);
            }
          }

          appendLog(message);
        };

        eventSource.onerror = function(error) {
          if (!isComplete) {
            appendLog('\n<b>Error: Connection to server lost.</b>\n');
            completeTraining(false);
          }
          if (eventSource) {
            eventSource.close();
          }
        };
      }

      // Start streaming when page loads
      document.addEventListener('DOMContentLoaded', function() {
        startStreaming();
      });

      // Stop button functionality
      document.getElementById('stop-button').addEventListener('click', function() {
        if (confirm('Are you sure you want to stop training?')) {
          if (eventSource) {
            eventSource.close();
          }
          appendLog('\n<b>Training stopped by user.</b>\n');
          completeTraining(false);
        }
      });
    </script>
  </body>
</html>
```

`wsl-install.bat`:

```bat
@echo off
setlocal EnableDelayedExpansion

cls
echo ======================================================================
echo  Waldo Alpha - WSL Installation Script
echo ======================================================================
echo.
echo This script will install Windows Subsystem for Linux (WSL) with Ubuntu
echo and prepare your system to run the Waldo Alpha application.
echo.
echo NOTE: This process requires administrator privileges and will need
echo a system restart after WSL installation.
echo.
echo ======================================================================
echo.
pause

REM Check for administrator privileges
net session >nul 2>&1
if %errorlevel% neq 0 (
    echo.
    echo ERROR: This script requires administrator privileges.
    echo.
    echo Please right-click on this file and select "Run as administrator"
    echo or use one of these methods:
    echo   1. Right-click Start Menu, select "Windows Terminal (Admin)"
    echo   2. Type: cd "%~dp0"
    echo   3. Type: wsl-install.bat
    echo.
    pause
    exit /b 1
)

echo.
echo STEP 1: Checking Windows version...
echo ----------------------------------------------------------------------
for /f "tokens=4-5 delims=. " %%i in ('ver') do set VERSION=%%i.%%j
echo Windows version detected: %VERSION%

REM Check if Windows version supports WSL
if "%VERSION%" LSS "10.0" (
    echo ERROR: WSL requires Windows 10 or later.
    echo Your version: %VERSION%
    pause
    exit /b 1
)

echo Windows version is compatible with WSL.
echo.

echo STEP 2: Checking WSL and Ubuntu installation status...
echo ----------------------------------------------------------------------

REM Check if any distributions are installed
wsl --list --quiet >nul 2>&1
if %errorlevel% neq 0 (
    echo WSL is not installed or no distributions found.
    echo Proceeding with full WSL installation...
    goto :INSTALL_WSL
)

REM If we get here, WSL has distributions - check for Ubuntu specifically
echo WSL is installed. Checking for Ubuntu...
wsl --list --quiet | findstr /i "Ubuntu" >nul 2>&1
if %errorlevel% equ 0 (
    echo Ubuntu is already installed.
    echo.
    echo Testing Ubuntu connection...
    wsl -d Ubuntu echo "Test successful" >nul 2>&1
    if %errorlevel% equ 0 (
        echo Ubuntu is working properly.
        echo You can proceed directly to wsl-setup.bat
        echo.
        pause
        exit /b 0
    ) else (
        echo Ubuntu is installed but not working properly.
        echo Reinstalling Ubuntu...
        goto :INSTALL_UBUNTU
    )
) else (
    echo Ubuntu is not installed. Installing Ubuntu...
    goto :INSTALL_UBUNTU
)

:INSTALL_WSL
echo.
echo STEP 3: Enabling required Windows features...
echo ----------------------------------------------------------------------
echo This may take several minutes...
echo.

REM Enable Windows Subsystem for Linux
echo Enabling Windows Subsystem for Linux...
dism.exe /online /enable-feature /featurename:Microsoft-Windows-Subsystem-Linux /all /norestart
if %errorlevel% neq 0 (
    echo ERROR: Failed to enable WSL feature.
    pause
    exit /b 1
)

REM Enable Virtual Machine Platform for WSL 2
echo.
echo Enabling Virtual Machine Platform...
dism.exe /online /enable-feature /featurename:VirtualMachinePlatform /all /norestart
if %errorlevel% neq 0 (
    echo WARNING: Could not enable Virtual Machine Platform.
    echo WSL 1 will be used instead of WSL 2.
)

echo.
echo STEP 4: Installing WSL and Ubuntu...
echo ----------------------------------------------------------------------
echo This will download and install Ubuntu. This may take 10-20 minutes...
echo.

REM Install WSL with Ubuntu
wsl --install -d Ubuntu
if %errorlevel% neq 0 (
    echo.
    echo WARNING: WSL installation may have encountered issues.
    echo Attempting alternative installation method...
    echo.

    REM Try installing just WSL first
    wsl --install --no-distribution

    REM Then install Ubuntu separately
    :INSTALL_UBUNTU
    echo.
    echo Installing Ubuntu distribution...
    wsl --install -d Ubuntu

    if %errorlevel% neq 0 (
        echo.
        echo ERROR: Failed to install Ubuntu.
        echo.
        echo Please try the following:
        echo 1. Restart your computer
        echo 2. Run this script again
        echo 3. If it still fails, install Ubuntu manually from Microsoft Store
        echo.
        pause
        exit /b 1
    )
)

echo.
echo STEP 5: Setting WSL 2 as default version...
echo ----------------------------------------------------------------------
wsl --set-default-version 2 >nul 2>&1
if %errorlevel% equ 0 (
    echo WSL 2 set as default version.
) else (
    echo WSL 1 will be used (WSL 2 not available on this system).
)

echo.
echo ======================================================================
echo  INSTALLATION COMPLETE - RESTART REQUIRED
echo ======================================================================
echo.
echo WSL and Ubuntu have been installed successfully!
echo.
echo IMPORTANT: You must restart your computer before proceeding.
echo.
echo After restarting:
echo 1. Ubuntu will prompt you to create a username and password
echo 2. Remember these credentials - you'll need them
echo 3. Run wsl-setup.bat to install the Waldo Alpha application
echo.
echo Would you like to restart now? (Y/N)
choice /c YN /m "Restart computer"
if errorlevel 2 (
    echo.
    echo Please restart manually before running wsl-setup.bat
    echo.
    pause
    exit /b 0
) else (
    echo.
    echo Restarting in 10 seconds...
    echo Save any open work now!
    timeout /t 10
    shutdown /r /t 0
)
```

`wsl-run.bat`:

```bat
@echo off
setlocal EnableDelayedExpansion

cls
echo ======================================================================
echo  Waldo Alpha - Starting Application in WSL
echo ======================================================================
echo.

REM Check if any WSL distributions are installed
echo Checking for WSL distributions...
wsl --list --verbose 2>nul | findstr /v "NAME\|----\|Windows Subsystem" | findstr /r "." >nul
if %errorlevel% neq 0 (
    echo ERROR: WSL is not installed or no distributions found.
    echo Please run wsl-install.bat first.
    echo.
    pause
    exit /b 1
)

REM Check if Ubuntu is working
echo Testing Ubuntu connection...
wsl -d Ubuntu echo "SUCCESS: Ubuntu is working"
if errorlevel 1 goto :UBUNTU_ERROR

echo.
echo Ubuntu connection successful!
echo.

echo STEP 2: Converting Windows path to WSL path...
echo ----------------------------------------------------------------------
REM Get the current directory in Windows format
set "WIN_PATH=%CD%"
echo Windows path: %WIN_PATH%

REM Convert Windows path to WSL path
set "WSL_PATH=%WIN_PATH:\=/%"
set "WSL_PATH=%WSL_PATH:C:=/mnt/c%"
set "WSL_PATH=%WSL_PATH:D:=/mnt/d%"
set "WSL_PATH=%WSL_PATH:E:=/mnt/e%"
set "WSL_PATH=%WSL_PATH:F:=/mnt/f%"

echo WSL path: %WSL_PATH%
echo.

echo STEP 3: Checking if run.sh exists...
echo ----------------------------------------------------------------------
wsl -d Ubuntu test -f "%WSL_PATH%/run.sh"
if %errorlevel% neq 0 (
    echo ERROR: run.sh not found in the current directory.
    echo Windows path: %WIN_PATH%
    echo WSL path: %WSL_PATH%
    echo.
    echo Please ensure you're running this script from the Waldo Alpha
    echo project directory that contains run.sh
    echo.
    echo Files in current directory:
    wsl -d Ubuntu ls -la "%WSL_PATH%/" 2>nul || echo "Could not list directory contents"
    echo.
    pause
    exit /b 1
)

echo run.sh found!

echo.
echo STEP 4: Checking conda environment...
echo ----------------------------------------------------------------------
echo Initializing conda...
wsl -d Ubuntu bash -c "source ~/miniconda3/etc/profile.d/conda.sh 2>/dev/null || source ~/anaconda3/etc/profile.d/conda.sh 2>/dev/null; conda env list | grep cs2-detect-env"
if errorlevel 1 goto :CONDA_ERROR

echo.
echo SUCCESS: cs2-detect-env environment found!
echo.
echo ======================================================================
echo  Starting CS2 Cheat Detection System
echo ======================================================================
echo.
echo The web interface will be available at: http://localhost:5000
echo.
echo Press Ctrl+C in this window to stop the server.
echo.
echo Starting server...
echo ----------------------------------------------------------------------
echo.

REM Make run.sh executable (in case it's not)
echo Making run.sh executable...
wsl -d Ubuntu chmod +x "%WSL_PATH%/run.sh"

echo.
echo STEP 5: Starting the CS2 Cheat Detection System...
echo ----------------------------------------------------------------------
echo The web interface will be available at: http://localhost:5000
echo Press Ctrl+C in this window to stop the server.
echo.

REM Run the application with proper conda initialization
echo Launching application with conda environment...
wsl -d Ubuntu bash -c "cd '%WSL_PATH%' && source ~/miniconda3/etc/profile.d/conda.sh 2>/dev/null || source ~/anaconda3/etc/profile.d/conda.sh 2>/dev/null; ./run.sh"

if %errorlevel% neq 0 (
    echo.
    echo ======================================================================
    echo  Application stopped or encountered an error
    echo ======================================================================
    echo.
    echo If you see errors above, common solutions include:
    echo.
    echo 1. Port conflict - Another application may be using port 5000
    echo    Solution: Close other applications or change the port in main.py
    echo.
    echo 2. Missing model files - The .pth model files may not be present
    echo    Solution: Place model files in deepcheat/VideoMAEv2/output/
    echo.
    echo 3. Environment issues - The conda environment may need updating
    echo    Solution: Run wsl-setup.bat again with option to update
    echo.
    echo You can also run manually in WSL:
    echo   1. Open WSL: wsl
    echo   2. Navigate: cd "%WSL_PATH%"
    echo   3. Run: ./run.sh
    echo.
) else (
    echo.
    echo Server stopped successfully.
    echo.
)

pause
exit /b 0

:UBUNTU_ERROR
echo.
echo ======================================================================
echo ERROR: Cannot connect to Ubuntu in WSL
echo ======================================================================
echo.
echo Available distributions:
wsl --list --verbose
echo.
echo Please ensure:
echo 1. Ubuntu is properly installed (run wsl-install.bat if needed)
echo 2. Ubuntu setup is complete (username/password configured)
echo 3. Try running 'wsl -d Ubuntu' manually to test
echo.
pause
exit /b 1

:CONDA_ERROR
echo.
echo ======================================================================
echo ERROR: cs2-detect-env conda environment not found
echo ======================================================================
echo.
echo Debugging conda installation...
echo Checking for miniconda:
wsl -d Ubuntu test -d ~/miniconda3 && echo "miniconda3 directory exists" || echo "miniconda3 directory not found"
echo Checking for anaconda:
wsl -d Ubuntu test -d ~/anaconda3 && echo "anaconda3 directory exists" || echo "anaconda3 directory not found"
echo.
echo Please run wsl-setup.bat first to install the application.
echo.
echo Available conda environments:
wsl -d Ubuntu bash -c "source ~/miniconda3/etc/profile.d/conda.sh 2>/dev/null || source ~/anaconda3/etc/profile.d/conda.sh 2>/dev/null; conda env list 2>/dev/null || echo 'Conda not properly initialized'"
echo.
pause
exit /b 1
```

`wsl-setup.bat`:

```bat
@echo off
setlocal EnableDelayedExpansion

cls
echo ======================================================================
echo  Waldo Alpha - WSL Application Setup
echo ======================================================================
echo.
echo This script will install the Waldo Alpha application inside WSL Ubuntu.
echo It will run the Linux install.sh script within the WSL environment.
echo.
echo Prerequisites:
echo - WSL with Ubuntu must be installed (run wsl-install.bat first)
echo - Ubuntu username and password must be configured
echo.
echo ======================================================================
echo.
pause

REM Check if any WSL distributions are installed
echo Checking for WSL distributions...
wsl --list --verbose 2>nul | findstr /v "NAME\|----\|Windows Subsystem" | findstr /r "." >nul
if %errorlevel% neq 0 (
    echo ERROR: WSL is not installed or no distributions found.
    echo Please run wsl-install.bat first.
    echo.
    pause
    exit /b 1
)

REM Check if Ubuntu is installed by trying to run a simple command
echo Checking for Ubuntu distribution...
echo Testing direct connection to Ubuntu...
wsl -d Ubuntu echo "Ubuntu test successful" >nul 2>&1
if %errorlevel% neq 0 (
    echo ERROR: Cannot connect to Ubuntu in WSL.
    echo Available distributions:
    wsl --list --verbose
    echo.
    echo Ubuntu appears to be installed but not accessible.
    echo This might be because:
    echo 1. Ubuntu needs to be started for the first time
    echo 2. User account setup is incomplete
    echo.
    echo Try running 'wsl -d Ubuntu' manually to complete setup.
    echo.
    pause
    exit /b 1
)

echo Ubuntu is accessible and working!

echo.
echo STEP 1: Checking WSL Ubuntu status...
echo ----------------------------------------------------------------------

REM Give Ubuntu a moment to fully register (especially right after installation)
timeout /t 3 /nobreak >nul 2>&1

REM Test if we can run commands in WSL using the default distribution
echo Testing WSL connection...
wsl echo "WSL Ubuntu is working!" >nul 2>&1
if %errorlevel% neq 0 (
    echo Cannot connect to default WSL distribution. Trying Ubuntu specifically...
    wsl -d Ubuntu echo "Ubuntu is working!" >nul 2>&1
    if %errorlevel% neq 0 (
        echo ERROR: Cannot connect to WSL Ubuntu.
        echo.
        echo Current WSL distributions:
        wsl --list --verbose
        echo.
        echo Please ensure:
        echo 1. Ubuntu installation completed successfully
        echo 2. You have set up your Ubuntu username and password
        echo 3. WSL service is running
        echo.
        echo Try running: wsl
        echo If Ubuntu doesn't start, you may need to set it up first.
        echo.
        pause
        exit /b 1
    )
)

echo WSL Ubuntu is ready.

echo.
echo STEP 2: Converting Windows path to WSL path...
echo ----------------------------------------------------------------------

REM Get the current directory in Windows format
set "WIN_PATH=%CD%"
echo Windows path: %WIN_PATH%

REM Convert Windows path to WSL path
REM Replace backslashes with forward slashes and C: with /mnt/c
set "WSL_PATH=%WIN_PATH:\=/%"
set "WSL_PATH=%WSL_PATH:C:=/mnt/c%"
set "WSL_PATH=%WSL_PATH:D:=/mnt/d%"
set "WSL_PATH=%WSL_PATH:E:=/mnt/e%"
set "WSL_PATH=%WSL_PATH:F:=/mnt/f%"

echo WSL path: %WSL_PATH%

echo.
echo STEP 3: Checking if install.sh exists...
echo ----------------------------------------------------------------------

wsl test -f "%WSL_PATH%/install.sh"
if %errorlevel% neq 0 (
    echo ERROR: install.sh not found in the current directory.
    echo.
    echo Please ensure you're running this script from the Waldo Alpha
    echo project directory that contains install.sh
    echo.
    pause
    exit /b 1
)

echo install.sh found.

echo.
echo STEP 4: Making install.sh executable...
echo ----------------------------------------------------------------------

wsl chmod +x "%WSL_PATH%/install.sh"
if %errorlevel% neq 0 (
    echo WARNING: Could not set execute permission on install.sh
    echo Attempting to continue anyway...
)

echo.
echo STEP 5: Installing required packages in Ubuntu...
echo ----------------------------------------------------------------------
echo This step ensures Ubuntu has necessary tools like curl...
echo.

REM Update package list and install essential tools
echo Updating Ubuntu package list...
wsl -e bash -c "sudo apt-get update -y"

echo.
echo Installing essential packages...
wsl -e bash -c "sudo apt-get install -y curl wget git build-essential"

echo.
echo STEP 6: Running install.sh in WSL...
echo ----------------------------------------------------------------------
echo This will install Miniconda and set up the Python environment.
echo This process may take 15-30 minutes depending on your internet speed.
echo.
echo Starting installation...
echo ======================================================================
echo.

REM Change to the project directory and run install.sh
wsl -e bash -c "cd '%WSL_PATH%' && ./install.sh"

set INSTALL_RESULT=%errorlevel%

if %INSTALL_RESULT% equ 0 (
    echo.
    echo ======================================================================
    echo  SETUP COMPLETE!
    echo ======================================================================
    echo.
    echo The Waldo Alpha application has been successfully installed in WSL!
    echo.
    echo To run the application:
    echo   1. Use wsl-run.bat (recommended)
    echo   2. Or manually: wsl -e bash -c "cd '%WSL_PATH%' && ./run.sh"
    echo.
    echo The web interface will be available at: http://localhost:5000
    echo.
) else (
    echo.
    echo ======================================================================
    echo  SETUP ENCOUNTERED ISSUES
    echo ======================================================================
    echo.
    echo The installation script reported an error.
    echo.
    echo Common issues and solutions:
    echo 1. Network issues - Check your internet connection and try again
    echo 2. Permission issues - Make sure you set up your Ubuntu user properly
    echo 3. Missing dependencies - Try running: wsl sudo apt-get update
    echo.
    echo You can also try running the installation manually:
    echo   1. Open WSL by typing: wsl
    echo   2. Navigate to: cd "%WSL_PATH%"
    echo   3. Run: ./install.sh
    echo.
)

pause
```