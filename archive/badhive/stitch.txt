Project Path: arc_badhive_stitch_tbab4vcr

Source Tree:

```txt
arc_badhive_stitch_tbab4vcr
├── CMakeLists.txt
├── LICENSE.txt
├── README.md
├── assets
│   ├── obf_after.png
│   └── obf_before.png
├── deps
│   └── CMakeLists.txt
├── examples
│   ├── CMakeLists.txt
│   ├── x86_complex_obf.cc
│   ├── x86_global_ref.cc
│   ├── x86_move_fn.cc
│   ├── x86_move_fn_branching.cc
│   ├── x86_opaque_pred.cc
│   └── x86_shellcode.cc
├── include
│   └── stitch
│       ├── binary
│       │   ├── binary.h
│       │   ├── pe.h
│       │   └── shellcode.h
│       ├── misc
│       │   ├── errors.h
│       │   └── utils.h
│       ├── stitch.h
│       └── target
│           ├── target.h
│           └── x86.h
├── src
│   ├── pe.cc
│   ├── shellcode.cc
│   └── x86.cc
└── unittests
    ├── CMakeLists.txt
    ├── binary
    │   ├── CMakeLists.txt
    │   ├── pe_parse_test.cc
    │   └── pe_rebuild_test.cc
    └── testdata
        ├── pe_branching.bin
        └── pe_simple.bin

```

`CMakeLists.txt`:

```txt
cmake_minimum_required(VERSION 3.31)
project(stitch)

set(CMAKE_CXX_STANDARD 23)
set(LIBSTITCH stitch)

option(STITCH_BUILD_SHARED "Build shared library" OFF)
option(STITCH_BUILD_TESTS "Build tests" ON)
option(STITCH_BUILD_EXAMPLES "Build examples" ON)

set(STITCH_SRC src/pe.cc
        src/x86.cc
        src/shellcode.cc
)

if (STITCH_BUILD_SHARED)
    add_library(${LIBSTITCH} SHARED ${STITCH_SRC})
else ()
    add_library(${LIBSTITCH} STATIC ${STITCH_SRC})
endif ()

target_include_directories(${LIBSTITCH} PUBLIC include)

# dependencies
add_subdirectory(deps)

include(CTest)

set(STITCH_TEST_DIR ${CMAKE_SOURCE_DIR}/unittests/testdata)

macro(stitch_add_test TEST_NAME)
    add_executable(${TEST_NAME} ${TEST_NAME}.cc)
    target_link_libraries(${TEST_NAME} PRIVATE ${LIBSTITCH})
    add_test(NAME ${TEST_NAME} COMMAND ${TEST_NAME} WORKING_DIRECTORY ${STITCH_TEST_DIR})
endmacro()

macro(stitch_add_test_with_sources TEST_NAME)
    add_executable(${TEST_NAME} ${ARGN})
    target_link_libraries(${TEST_NAME} PRIVATE ${LIBSTITCH})
    add_test(NAME ${TEST_NAME} COMMAND ${TEST_NAME} WORKING_DIRECTORY ${STITCH_TEST_DIR})
endmacro()

add_subdirectory(unittests)
add_subdirectory(examples)

```

`LICENSE.txt`:

```txt
                             Apache License
                       Version 2.0, January 2004
                    http://www.apache.org/licenses/

TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION

1. Definitions.

  "License" shall mean the terms and conditions for use, reproduction,
  and distribution as defined by Sections 1 through 9 of this document.

  "Licensor" shall mean the copyright owner or entity authorized by
  the copyright owner that is granting the License.

  "Legal Entity" shall mean the union of the acting entity and all
  other entities that control, are controlled by, or are under common
  control with that entity. For the purposes of this definition,
  "control" means (i) the power, direct or indirect, to cause the
  direction or management of such entity, whether by contract or
  otherwise, or (ii) ownership of fifty percent (50%) or more of the
  outstanding shares, or (iii) beneficial ownership of such entity.

  "You" (or "Your") shall mean an individual or Legal Entity
  exercising permissions granted by this License.

  "Source" form shall mean the preferred form for making modifications,
  including but not limited to software source code, documentation
  source, and configuration files.

  "Object" form shall mean any form resulting from mechanical
  transformation or translation of a Source form, including but
  not limited to compiled object code, generated documentation,
  and conversions to other media types.

  "Work" shall mean the work of authorship, whether in Source or
  Object form, made available under the License, as indicated by a
  copyright notice that is included in or attached to the work
  (an example is provided in the Appendix below).

  "Derivative Works" shall mean any work, whether in Source or Object
  form, that is based on (or derived from) the Work and for which the
  editorial revisions, annotations, elaborations, or other modifications
  represent, as a whole, an original work of authorship. For the purposes
  of this License, Derivative Works shall not include works that remain
  separable from, or merely link (or bind by name) to the interfaces of,
  the Work and Derivative Works thereof.

  "Contribution" shall mean any work of authorship, including
  the original version of the Work and any modifications or additions
  to that Work or Derivative Works thereof, that is intentionally
  submitted to Licensor for inclusion in the Work by the copyright owner
  or by an individual or Legal Entity authorized to submit on behalf of
  the copyright owner. For the purposes of this definition, "submitted"
  means any form of electronic, verbal, or written communication sent
  to the Licensor or its representatives, including but not limited to
  communication on electronic mailing lists, source code control systems,
  and issue tracking systems that are managed by, or on behalf of, the
  Licensor for the purpose of discussing and improving the Work, but
  excluding communication that is conspicuously marked or otherwise
  designated in writing by the copyright owner as "Not a Contribution."

  "Contributor" shall mean Licensor and any individual or Legal Entity
  on behalf of whom a Contribution has been received by Licensor and
  subsequently incorporated within the Work.

2. Grant of Copyright License. Subject to the terms and conditions of
  this License, each Contributor hereby grants to You a perpetual,
  worldwide, non-exclusive, no-charge, royalty-free, irrevocable
  copyright license to reproduce, prepare Derivative Works of,
  publicly display, publicly perform, sublicense, and distribute the
  Work and such Derivative Works in Source or Object form.

3. Grant of Patent License. Subject to the terms and conditions of
  this License, each Contributor hereby grants to You a perpetual,
  worldwide, non-exclusive, no-charge, royalty-free, irrevocable
  (except as stated in this section) patent license to make, have made,
  use, offer to sell, sell, import, and otherwise transfer the Work,
  where such license applies only to those patent claims licensable
  by such Contributor that are necessarily infringed by their
  Contribution(s) alone or by combination of their Contribution(s)
  with the Work to which such Contribution(s) was submitted. If You
  institute patent litigation against any entity (including a
  cross-claim or counterclaim in a lawsuit) alleging that the Work
  or a Contribution incorporated within the Work constitutes direct
  or contributory patent infringement, then any patent licenses
  granted to You under this License for that Work shall terminate
  as of the date such litigation is filed.

4. Redistribution. You may reproduce and distribute copies of the
  Work or Derivative Works thereof in any medium, with or without
  modifications, and in Source or Object form, provided that You
  meet the following conditions:

  (a) You must give any other recipients of the Work or
      Derivative Works a copy of this License; and

  (b) You must cause any modified files to carry prominent notices
      stating that You changed the files; and

  (c) You must retain, in the Source form of any Derivative Works
      that You distribute, all copyright, patent, trademark, and
      attribution notices from the Source form of the Work,
      excluding those notices that do not pertain to any part of
      the Derivative Works; and

  (d) If the Work includes a "NOTICE" text file as part of its
      distribution, then any Derivative Works that You distribute must
      include a readable copy of the attribution notices contained
      within such NOTICE file, excluding those notices that do not
      pertain to any part of the Derivative Works, in at least one
      of the following places: within a NOTICE text file distributed
      as part of the Derivative Works; within the Source form or
      documentation, if provided along with the Derivative Works; or,
      within a display generated by the Derivative Works, if and
      wherever such third-party notices normally appear. The contents
      of the NOTICE file are for informational purposes only and
      do not modify the License. You may add Your own attribution
      notices within Derivative Works that You distribute, alongside
      or as an addendum to the NOTICE text from the Work, provided
      that such additional attribution notices cannot be construed
      as modifying the License.

  You may add Your own copyright statement to Your modifications and
  may provide additional or different license terms and conditions
  for use, reproduction, or distribution of Your modifications, or
  for any such Derivative Works as a whole, provided Your use,
  reproduction, and distribution of the Work otherwise complies with
  the conditions stated in this License.

5. Submission of Contributions. Unless You explicitly state otherwise,
  any Contribution intentionally submitted for inclusion in the Work
  by You to the Licensor shall be under the terms and conditions of
  this License, without any additional terms or conditions.
  Notwithstanding the above, nothing herein shall supersede or modify
  the terms of any separate license agreement you may have executed
  with Licensor regarding such Contributions.

6. Trademarks. This License does not grant permission to use the trade
  names, trademarks, service marks, or product names of the Licensor,
  except as required for reasonable and customary use in describing the
  origin of the Work and reproducing the content of the NOTICE file.

7. Disclaimer of Warranty. Unless required by applicable law or
  agreed to in writing, Licensor provides the Work (and each
  Contributor provides its Contributions) on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
  implied, including, without limitation, any warranties or conditions
  of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
  PARTICULAR PURPOSE. You are solely responsible for determining the
  appropriateness of using or redistributing the Work and assume any
  risks associated with Your exercise of permissions under this License.

8. Limitation of Liability. In no event and under no legal theory,
  whether in tort (including negligence), contract, or otherwise,
  unless required by applicable law (such as deliberate and grossly
  negligent acts) or agreed to in writing, shall any Contributor be
  liable to You for damages, including any direct, indirect, special,
  incidental, or consequential damages of any character arising as a
  result of this License or out of the use or inability to use the
  Work (including but not limited to damages for loss of goodwill,
  work stoppage, computer failure or malfunction, or any and all
  other commercial damages or losses), even if such Contributor
  has been advised of the possibility of such damages.

9. Accepting Warranty or Additional Liability. While redistributing
  the Work or Derivative Works thereof, You may choose to offer,
  and charge a fee for, acceptance of support, warranty, indemnity,
  or other liability obligations and/or rights consistent with this
  License. However, in accepting such obligations, You may act only
  on Your own behalf and on Your sole responsibility, not on behalf
  of any other Contributor, and only if You agree to indemnify,
  defend, and hold each Contributor harmless for any liability
  incurred by, or claims asserted against, such Contributor by reason
  of your accepting any such warranty or additional liability.

END OF TERMS AND CONDITIONS

APPENDIX: How to apply the Apache License to your work.

  To apply the Apache License to your work, attach the following
  boilerplate notice, with the fields enclosed by brackets "[]"
  replaced with your own identifying information. (Don't include
  the brackets!)  The text should be enclosed in the appropriate
  comment syntax for the file format. We also recommend that a
  file or class name and description of purpose be included on the
  same "printed page" as the copyright notice for easier
  identification within third-party archives.

Copyright [yyyy] [name of copyright owner]

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

   http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.

```

`README.md`:

```md
## Stitch

**NOTE: Stitch is an experimental project and stability is not guaranteed. It has been
used for its intended purpose (targeted rewriting / obfuscation) with success but may not be reliable for
more complex tasks.**

Cross-platform C++ library for patching and obfuscating code in compiled binaries.
Code and binary parsing are logically separated in order to support as many
binary format + architecture combinations as possible.

Stitch currently works with the following binary formats:
- PE
- Shellcode

### Supported architectures

#### x86
Stitch's x86 capability is heavily built on top of [zasm](https://github.com/zyantific/zasm).
Each `X86Function` provides a `zasm::Assembler` instance that is pre-populated
with the original instructions as they were on disk. The original instructions can
be accessed with `X86Function::GetOriginalCode`, and their positions within the assembler
can be accessed with `X86Inst::GetPos`.

### Build

```
git clone https://github.com/badhive/stitch
cd stitch
cmake -DCMAKE_BUILD_TYPE=Release -B build .
cmake --build build/ --target stitch
```

### Examples + Use Cases

#### Binary manipulation

Stitch allows for functions to be safely edited post-compilation. Operands for 
position-relative instructions, such as `jmp`s, are replaced with labels so that
the code can be easily modified and serialised without worry.

#### Obfuscation

The main and intended use for Stitch is code obfuscation on a binary level. It handles
the tedious task of injecting new data into files, so that operators can focus on more
complex obfuscation techniques, including but not limited to VM-based obfuscation.
Here's an example program that applies basic obfuscation (in the form of opaque predicates) 
to a function (specified by its absolute address).

```c++
#include "stitch/binary/pe.h"
#include "stitch/target/x86.h"

const std::vector regs = {
    zasm::x86::rdi,
    zasm::x86::rsi,
    zasm::x86::rcx,
    zasm::x86::rdx,
    zasm::x86::r8,
    zasm::x86::r9,
    zasm::x86::r10,
};

auto& getRandomReg() {
  auto& reg = regs[rand() % regs.size()];
  return reg;
}

int main() {
  srand(time(nullptr));
  stitch::PE pe("pe_branching.bin");
  const auto code = pe.OpenCode();
  constexpr stitch::RVA fn_main = 0x00000001400015A1;
  const auto fn = dynamic_cast<stitch::X86Function*>(code->EditFunction(
      fn_main, ""));
  fn->Instrument([](stitch::X86Function* fo, zasm::x86::Assembler& as) {
    for (const stitch::X86Inst& inst : fo->GetOriginalCode()) {
      const bool to_insert = rand() % 2;
      const zasm::InstructionDetail& detail = inst.RawInst();
      if (detail.getMnemonic() != zasm::x86::Mnemonic::Ret && to_insert) {
        zasm::Label last_label = as.createLabel();
        const auto& reg = getRandomReg();
        as.setCursor(inst.GetPos());
        as.pushf();
        as.push(reg);
        as.xor_(reg, zasm::Imm(rand()));
        as.js(last_label);
        as.jns(last_label);
        as.bind(last_label);
        as.pop(reg);
        as.popf();
      }
    }
  });
  pe.SaveAs("target/pe_opaque_predicates.bin");
  pe.Close();
}
```
Here's the function before the obfuscation is applied:

![before](assets/obf_before.png)

...and after:

![after](assets/obf_after.png)

#### What happened?

1. When the function address is supplied to `Code::EditFunction`, Stitch
begins parsing the code at that address, while also doing basic control
flow tracking by splitting it up into basic blocks
2. The code is parsed into a `zasm::x86::Assembler` instance and intra-function
references are replaced with labels
3. `X86Function::Instrument` allows us to modify the code, making use of
an assembler that has been populated with the original code
4. `X86Function::Finish` updates PE relocation info, assembles and writes the
code to a new section with a 16-byte alignment. The memory ranges previously 
occupied by the function are patched out and a jmp to the new code is inserted
in its place.

```

`deps/CMakeLists.txt`:

```txt
include(FetchContent)

message(STATUS "Fetching zyantific/zasm @ c239a78...")
FetchContent_Declare(zasm
        GIT_REPOSITORY "https://github.com/zyantific/zasm"
        GIT_TAG c239a78
)
option(ZASM_BUILD_TESTS "" OFF)
option(ZASM_BUILD_BENCHMARKS "" OFF)
option(ZASM_BUILD_EXAMPLES "" OFF)
FetchContent_MakeAvailable(zasm)

target_link_libraries(${LIBSTITCH} PUBLIC zasm)

```

`examples/CMakeLists.txt`:

```txt
if(STITCH_BUILD_EXAMPLES)
    file(MAKE_DIRECTORY ${CMAKE_BINARY_DIR}/unittests/testdata/target)
    stitch_add_test(x86_move_fn)
    stitch_add_test(x86_move_fn_branching)
    stitch_add_test(x86_opaque_pred)
    stitch_add_test(x86_global_ref)
    stitch_add_test(x86_complex_obf)
    stitch_add_test(x86_shellcode)
    # stitch_add_test(x86_test)
endif ()
```

`examples/x86_complex_obf.cc`:

```cc
/*
 * Licensed to BadHive under one or more contributor license
 * agreements.  See the NOTICE file distributed with this work
 * for additional information regarding copyright ownership.
 * BadHive licenses this file to you under the Apache License,
 * Version 2.0 (the "License"); you may not use this file
 * except in compliance with the License.  You may obtain a
 * copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 */

#include <stitch/binary/pe.h>
#include <stitch/target/x86.h>

#include <algorithm>
#include <iostream>
#include <map>

const std::vector regs = {
    zasm::x86::rdi, zasm::x86::rsi, zasm::x86::rcx, zasm::x86::rdx,
    zasm::x86::r8,  zasm::x86::r9,  zasm::x86::r10,
};

const std::vector<zasm::InstrMnemonic> mnemonics = {
    zasm::x86::Mnemonic::Mov, zasm::x86::Mnemonic::Sub,
    zasm::x86::Mnemonic::Xor, zasm::x86::Mnemonic::Add};

const std::map<zasm::InstrMnemonic, zasm::InstrMnemonic> branches = {
    {zasm::x86::Mnemonic::Jz, zasm::x86::Mnemonic::Jnz},
    {zasm::x86::Mnemonic::Jb, zasm::x86::Mnemonic::Jnb},
    {zasm::x86::Mnemonic::Jo, zasm::x86::Mnemonic::Jno},
    {zasm::x86::Mnemonic::Jl, zasm::x86::Mnemonic::Jnl},
    {zasm::x86::Mnemonic::Jp, zasm::x86::Mnemonic::Jnp},
    {zasm::x86::Mnemonic::Jle, zasm::x86::Mnemonic::Jnle}};

const auto& getRandomReg() {
  auto& reg = regs[rand() % regs.size()];
  return reg;
}

const auto& getRandomMnemonic() {
  auto& reg = mnemonics[rand() % mnemonics.size()];
  return reg;
}

const auto& getRandomBranch() {
  auto it = branches.begin();
  std::advance(it, rand() % mnemonics.size());
  return *it;
}

int getRandomInt() { return rand() % (0x1000 + 1); }

int main() {
  srand(time(nullptr));
  stitch::PE pe("pe_branching.bin");
  stitch::Code* code = pe.OpenCode();
  constexpr stitch::RVA fn_main = 0x00000001400015A1;
  auto* fn =
      dynamic_cast<stitch::X86Function*>(code->EditFunction(fn_main, ""));
  fn->Instrument([](stitch::X86Function* fo, zasm::x86::Assembler& as) {
    for (const stitch::X86Inst& inst : fo->GetOriginalCode()) {
      bool to_insert = rand() & 2;
      if (inst.RawInst().getCategory() != zasm::x86::Category::Ret &&
          to_insert) {
        const auto cursor = as.getCursor();
        as.setCursor(inst.GetPos());
        if (!inst.CommonFlagsAvailable())
          continue;  // we won't use pushf and popf at all
        // if no regs available, we'll use any random register
        auto reg = inst.GetAvailableRegister();
        auto dummy = getRandomReg();
        as.push(dummy);
        bool reg_pushed = false;
        if (!reg.has_value()) {
          reg = zasm::x86::rax;
          as.push(*reg);
          reg_pushed = true;
        }

        auto label = as.createLabel();
        const auto pair1 = getRandomBranch();
        const auto pair2 = getRandomBranch();

        as.emit(getRandomMnemonic(), *reg, zasm::Imm(getRandomInt()));
        as.emit(pair1.first, label);
        for (int i = 0; i < getRandomInt() % 10; i++) {
          as.emit(getRandomMnemonic(), *reg, zasm::Imm(getRandomInt()));
        }
        as.emit(pair2.first, label);
        as.emit(pair2.second, label);
        as.bind(label);
        if (reg_pushed) as.pop(zasm::x86::rax);
        as.pop(dummy);
        as.setCursor(cursor);
      }
    }
  });
  pe.SaveAs("target/pe_complex_obf.bin");
  pe.Close();
}
```

`examples/x86_global_ref.cc`:

```cc
/*
 * Licensed to BadHive under one or more contributor license
 * agreements.  See the NOTICE file distributed with this work
 * for additional information regarding copyright ownership.
 * BadHive licenses this file to you under the Apache License,
 * Version 2.0 (the "License"); you may not use this file
 * except in compliance with the License.  You may obtain a
 * copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 */

#include "stitch/binary/pe.h"
#include "stitch/target/x86.h"

int main() {
  stitch::PE pe("pe_branching.bin");
  auto* code = pe.OpenCode<stitch::X86Code>();
  constexpr stitch::RVA fn_main = 0x00000001400015A1;
  stitch::Section* scn = pe.AddSection(".st0", stitch::SectionType::ROData);
  const stitch::GlobalRef* str_ref = scn->WriteWithRef("Hello, world!\n");
  auto* fn =
      dynamic_cast<stitch::X86Function*>(code->EditFunction(fn_main, ".st1"));
  fn->Instrument([&](stitch::X86Function* fo, zasm::x86::Assembler& as) {
    for (const stitch::X86Inst& inst : fo->GetOriginalCode()) {
      const zasm::InstructionDetail& detail = inst.RawInst();
      const zasm::Mem* target_op = nullptr;
      int target_op_pos = -1;
      for (int i = 0; i < detail.getOperandCount(); i++) {
        try {
          const auto& op = detail.getOperand<zasm::Mem>(i);
          // replace known string addr with GlobalRef to our new string
          if (op.getDisplacement() == 0x0000000140009000) {
            target_op = &op;
            target_op_pos = i;
            break;
          }
        } catch (const std::exception& _) {
        }
      }
      if (target_op) {
        auto new_inst = detail;
        zasm::Node* after = inst.GetPos()->getPrev();
        fo->GetProgram().destroy(inst.GetPos());
        zasm::Node* end = as.getCursor();
        as.setCursor(after);
        new_inst.setOperand(target_op_pos,
                            code->AddressOperand(str_ref->GetValue()));
        as.emit(new_inst);
        as.setCursor(end);
        break;
      }
    }
  });
  pe.SaveAs("target/pe_global_ref.bin");
  pe.Close();
}
```

`examples/x86_move_fn.cc`:

```cc
/*
 * Licensed to BadHive under one or more contributor license
 * agreements.  See the NOTICE file distributed with this work
 * for additional information regarding copyright ownership.
 * BadHive licenses this file to you under the Apache License,
 * Version 2.0 (the "License"); you may not use this file
 * except in compliance with the License.  You may obtain a
 * copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 */
#include "stitch/binary/pe.h"

int main() {
  stitch::PE pe("pe_simple.bin");
  stitch::Code* code = pe.OpenCode();
  constexpr stitch::RVA fn_main = 0x00000001400015A1;
  stitch::Function* fn = code->EditFunction(fn_main, "");
  fn->Finish();
  pe.SaveAs("target/pe_moved_fn.bin");
  pe.Close();
}

```

`examples/x86_move_fn_branching.cc`:

```cc
/*
 * Licensed to BadHive under one or more contributor license
 * agreements.  See the NOTICE file distributed with this work
 * for additional information regarding copyright ownership.
 * BadHive licenses this file to you under the Apache License,
 * Version 2.0 (the "License"); you may not use this file
 * except in compliance with the License.  You may obtain a
 * copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 */
#include "stitch/binary/pe.h"

int main() {
  stitch::PE pe("pe_branching.bin");
  stitch::Code* code = pe.OpenCode();
  constexpr stitch::RVA fn_main = 0x00000001400015A1;
  stitch::Function* fn = code->EditFunction(fn_main, "");
  fn->Finish();
  pe.SaveAs("target/pe_moved_fn_branching.bin");
  pe.Close();
}

```

`examples/x86_opaque_pred.cc`:

```cc
/*
 * Licensed to BadHive under one or more contributor license
 * agreements.  See the NOTICE file distributed with this work
 * for additional information regarding copyright ownership.
 * BadHive licenses this file to you under the Apache License,
 * Version 2.0 (the "License"); you may not use this file
 * except in compliance with the License.  You may obtain a
 * copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 */

#include "stitch/binary/pe.h"
#include "stitch/target/x86.h"

const std::vector regs = {
    zasm::x86::rdi, zasm::x86::rsi, zasm::x86::rcx, zasm::x86::rdx,
    zasm::x86::r8,  zasm::x86::r9,  zasm::x86::r10,
};

auto& getRandomReg() {
  auto& reg = regs[rand() % regs.size()];
  return reg;
}

int main() {
  srand(time(nullptr));
  stitch::PE pe("pe_branching.bin");
  const auto code = pe.OpenCode();
  constexpr stitch::RVA fn_main = 0x00000001400015A1;
  const auto fn =
      dynamic_cast<stitch::X86Function*>(code->EditFunction(fn_main, ""));
  fn->Instrument([](stitch::X86Function* fo, zasm::x86::Assembler& as) {
    for (const stitch::X86Inst& inst : fo->GetOriginalCode()) {
      const bool to_insert = rand() % 2;
      const zasm::InstructionDetail& detail = inst.RawInst();
      if (detail.getMnemonic() != zasm::x86::Mnemonic::Ret && to_insert) {
        zasm::Label last_label = as.createLabel();

        bool auto_reg = true;
        auto reg = inst.GetAvailableRegister<zasm::x86::Gp64>();
        if (!reg.has_value()) {
          auto_reg = false;
          reg = getRandomReg();
          as.push(*reg);
        }
        as.setCursor(inst.GetPos());

        if (!inst.CommonFlagsAvailable()) as.pushf();

        as.xor_(*reg, zasm::Imm(rand()));
        as.js(last_label);
        as.jns(last_label);
        as.bind(last_label);

        if (!inst.CommonFlagsAvailable()) as.popf();

        if (!auto_reg) as.pop(*reg);
      }
    }
  });
  pe.SaveAs("target/pe_opaque_predicates.bin");
  pe.Close();
}
```

`examples/x86_shellcode.cc`:

```cc
/*
 * Licensed to BadHive under one or more contributor license
 * agreements.  See the NOTICE file distributed with this work
 * for additional information regarding copyright ownership.
 * BadHive licenses this file to you under the Apache License,
 * Version 2.0 (the "License"); you may not use this file
 * except in compliance with the License.  You may obtain a
 * copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 */

#include "stitch/binary/shellcode.h"
#include "stitch/target/x86.h"

int main() {
  // shellcode will set off av so won't include it
  try {
    stitch::Shellcode shc("shellcode_simple.bin",
                          stitch::TargetArchitecture::AMD64,
                          stitch::Platform::Windows);
    auto* code = shc.OpenCode<stitch::X86Code>();
    auto* fn = dynamic_cast<stitch::X86Function*>(code->EditFunction(0x46, ""));
    fn->Finish();
    shc.SaveAs("target/obf.shellcode.bin");
    shc.Close();
  } catch (const std::exception& _) {
    return 0;
  }
}

```

`include/stitch/binary/binary.h`:

```h
/*
 * Licensed to BadHive under one or more contributor license
 * agreements.  See the NOTICE file distributed with this work
 * for additional information regarding copyright ownership.
 * BadHive licenses this file to you under the Apache License,
 * Version 2.0 (the "License"); you may not use this file
 * except in compliance with the License.  You may obtain a
 * copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 */

#ifndef STITCH_BINARY_BINARY_H_
#define STITCH_BINARY_BINARY_H_

#include <fstream>
#include <vector>

#include "stitch/misc/errors.h"
#include "stitch/misc/utils.h"

namespace stitch {
class Binary;
class Code;
class Section;

enum class SectionType {
  Invalid = 0,
  Code,
  Data,
  ROData,
  BSS,
};

enum class Platform {
  Invalid = 0,
  Windows,
};

class GlobalRef {
  VA value_;

 public:
  GlobalRef() : value_(0) {}

  explicit GlobalRef(const VA value) : value_(value) {}

  void SetValue(const VA value) { value_ = value; }

  void AdjustValue(const VA delta) { value_ += delta; }

  VA GetValue() const { return value_; }
};

class Binary {
  bool opened_;
  std::unique_ptr<Code> code_;

 protected:
  std::string file_name_;
  std::fstream file_stream_;
  const Platform platform_;
  bool open_;

  void setCode(std::unique_ptr<Code> code) { code_ = std::move(code); }

 public:
  explicit Binary(const Platform platform)
      : opened_(false), code_(nullptr), platform_(platform), open_(false) {}

  Binary(const std::string& file_name, const Platform platform)
      : opened_(false), code_(nullptr), platform_(platform), open_(false) {
    Binary::Open(file_name);
  }

  virtual ~Binary() = default;

  Platform GetPlatform() const { return platform_; }

  virtual VA GetImageBase() const = 0;

  virtual VA GetEntrypoint() const = 0;

  virtual void Open(const std::string& file_name) {
    if (opened_) return;
    file_stream_ = std::fstream(
        file_name, std::ios::in | std::ios::out | std::ios::binary);
    if (!file_stream_.good()) {
      throw std::runtime_error("failed to open file '" + file_name + "'");
    }
    file_name_ = file_name;
    open_ = opened_ = true;
  }

  virtual const uint8_t* ReadDataAt(VA address) const = 0;

  virtual Section* OpenSection(const std::string& name) const = 0;

  virtual Section* OpenSectionAt(VA address) const = 0;

  virtual Section* AddSection(const std::string& name, SectionType type) = 0;

  virtual char GetBitSize() const = 0;

  virtual std::string GetImportForAddress(VA address) const = 0;

  virtual VA GetAddressForImport(const std::string& import) const = 0;

  template <typename T = Code>
  T* OpenCode() const {
    return dynamic_cast<T*>(code_.get());
  }

  virtual void Save() = 0;

  virtual void SaveAs(const std::string& file_name) = 0;

  void Close() {
    if (!open_) return;
    file_stream_.close();
    open_ = false;
  }

  void SaveAndClose() {
    Save();
    Close();
  }
};

class Section {
  std::string name_;
  Binary* parent_;
  std::vector<uint8_t> data_;
  const bool existed_;
  SectionType type_;
  std::vector<std::unique_ptr<GlobalRef>> refs_;

 protected:
  std::vector<uint8_t>& getData() { return data_; }

 public:
  Section(const std::string& name, const SectionType type,
          const std::vector<uint8_t>& data, Binary* parent, const bool existed)
      : name_(name),
        parent_(parent),
        data_(data),
        existed_(existed),
        type_(type) {}

  virtual ~Section() = default;

  Section(Section&&) = default;

  const std::string& GetName() const { return name_; }

  SectionType GetType() const { return type_; }

  template <typename T = Binary>
  T* GetParent() const {
    return dynamic_cast<T*>(parent_);
  }

  const std::vector<uint8_t>& GetData() const { return data_; }

  virtual RVA GetAddress() const = 0;

  virtual void Relocate(const int64_t delta) {
    for (const auto& ref : refs_) {
      ref->AdjustValue(delta);
    }
  }

  int64_t GetSize() const { return static_cast<int64_t>(data_.size()); }

  bool OnDisk() const { return existed_; }

  void Write(const std::vector<char>& data) {
    const std::vector<uint8_t> v(data.begin(), data.end());
    Write(v);
  }

  void Write(const std::vector<int8_t>& data) {
    const std::vector<uint8_t> v(data.begin(), data.end());
    Write(v);
  }

  void Write(const std::string& str) {
    const std::vector v(
        reinterpret_cast<const uint8_t*>(str.c_str()),
        reinterpret_cast<const uint8_t*>(str.c_str()) + str.length() + 1);
    Write(v);
  }

  void Write(const std::wstring& str) {
    const std::vector v(reinterpret_cast<const uint8_t*>(str.c_str()),
                        reinterpret_cast<const uint8_t*>(str.c_str()) +
                            (str.length() + 1) * sizeof(wchar_t));
    Write(v);
  }

  void Write(const uint8_t* data, const uint64_t size) {
    const std::vector v(data, data + size);
    Write(v);
  }

  void Write(const char* str, const uint64_t size) {
    const std::vector<uint8_t> v(str, str + size);
    Write(v);
  }

  /// Writes data to section and returns a dynamic reference to that
  /// data. Classes implementing Section are responsible for
  /// calling Section::Relocate in their own Relocate function to update these
  /// refs if the section is moved
  /// @param args Write() args
  /// @return pointer to GlobalRef
  template <typename... Args>
  const GlobalRef* WriteWithRef(Args&&... args) {
    auto ref = std::make_unique<GlobalRef>(GetParent()->GetImageBase() +
                                           GetAddress() + GetSize());
    Write(std::forward<Args>(args)...);
    refs_.push_back(std::move(ref));
    return refs_.back().get();
  }

  virtual void Write(const std::vector<uint8_t>& data) = 0;

  void Memset(const RVA address, const uint8_t val, const size_t count) {
    if (address + count > GetSize())
      throw section_error("writing out of range");
    for (RVA i = address; i < address + count; i++) {
      data_[i] = val;
    }
  }

  void WriteAt(const RVA address, const uint8_t* data, const size_t size) {
    const std::vector buf(data, data + size);
    WriteAt(address, buf);
  }

  void WriteAt(const RVA address, const std::vector<uint8_t>& data) {
    if (address > GetSize()) throw section_error("address out of range");
    if (address + data.size() > GetSize())
      throw section_error("writing out of range");
    for (RVA i = address; i < address + data.size(); i++) {
      data_[i] = data[i - address];
    }
  }

 protected:
  virtual void setData(const std::vector<uint8_t>& data) {
    if (data.size() != data_.size())
      throw std::runtime_error("new data must be the same size as old data");
    data_ = data;
  }
};
}  // namespace stitch
#endif  // STITCH_BINARY_BINARY_H_
```

`include/stitch/binary/pe.h`:

```h
/*
 * Licensed to BadHive under one or more contributor license
 * agreements.  See the NOTICE file distributed with this work
 * for additional information regarding copyright ownership.
 * BadHive licenses this file to you under the Apache License,
 * Version 2.0 (the "License"); you may not use this file
 * except in compliance with the License.  You may obtain a
 * copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 */

#ifndef STITCH_BINARY_PE_H_
#define STITCH_BINARY_PE_H_

#include <map>
#include <memory>

#include "stitch/binary/binary.h"
#include "stitch/misc/utils.h"
#include "stitch/target/target.h"

namespace stitch {
namespace pe {
using BYTE = std::uint8_t;
using WORD = std::uint16_t;
using DWORD = std::uint32_t;
using QWORD = std::uint64_t;

using CHAR = char;
using SHORT = short;
using LONG = long;
using LONGLONG = std::int64_t;

using UCHAR = std::uint8_t;
using USHORT = std::uint16_t;
using ULONG = std::uint32_t;
using ULONGLONG = std::uint64_t;

constexpr DWORD IMAGE_DOS_SIGNATURE = 0x5A4D;
constexpr DWORD IMAGE_NT_SIGNATURE = 0x00004550;
constexpr DWORD IMAGE_NUMBEROF_DIRECTORY_ENTRIES = 16;
constexpr DWORD IMAGE_SIZEOF_SHORT_NAME = 8;
constexpr DWORD IMAGE_NT_OPTIONAL_HDR32_MAGIC = 0x10b;
constexpr DWORD IMAGE_NT_OPTIONAL_HDR64_MAGIC = 0x20b;
constexpr DWORD IMAGE_SCN_CNT_CODE = 0x00000020;
constexpr DWORD IMAGE_SCN_CNT_INITIALIZED_DATA = 0x00000040;
constexpr DWORD IMAGE_SCN_CNT_UNINITIALIZED_DATA = 0x00000080;
constexpr DWORD IMAGE_SCN_MEM_EXECUTE = 0x20000000;
constexpr DWORD IMAGE_SCN_MEM_READ = 0x40000000;
constexpr DWORD IMAGE_SCN_MEM_WRITE = 0x80000000;

// architectures
constexpr DWORD IMAGE_FILE_MACHINE_I386 = 0x0000014c;
constexpr DWORD IMAGE_FILE_MACHINE_AMD64 = 0x00008664;
constexpr DWORD IMAGE_FILE_MACHINE_ARM = 0x000001c0;
constexpr DWORD IMAGE_FILE_MACHINE_ARM64 = 0x0000aa64;

// dir entries
constexpr char IMAGE_DIRECTORY_ENTRY_EXPORT = 0;
constexpr char IMAGE_DIRECTORY_ENTRY_IMPORT = 1;
constexpr char IMAGE_DIRECTORY_ENTRY_SECURITY = 4;
constexpr char IMAGE_DIRECTORY_ENTRY_BASERELOC = 5;

constexpr ULONGLONG IMAGE_ORDINAL_FLAG64 = 0x8000000000000000ULL;
constexpr ULONGLONG IMAGE_ORDINAL_FLAG32 = 0x80000000;

// section flags
constexpr unsigned code_flags =
    IMAGE_SCN_CNT_CODE | IMAGE_SCN_MEM_EXECUTE | IMAGE_SCN_MEM_READ;
constexpr unsigned data_flags =
    IMAGE_SCN_CNT_INITIALIZED_DATA | IMAGE_SCN_MEM_READ | IMAGE_SCN_MEM_WRITE;
constexpr unsigned bss_flags =
    IMAGE_SCN_CNT_UNINITIALIZED_DATA | IMAGE_SCN_MEM_READ | IMAGE_SCN_MEM_WRITE;

struct DosHeader {
  WORD e_magic;
  WORD e_cblp;
  WORD e_cp;
  WORD e_crlc;
  WORD e_cparhdr;
  WORD e_minalloc;
  WORD e_maxalloc;
  WORD e_ss;
  WORD e_sp;
  WORD e_csum;
  WORD e_ip;
  WORD e_cs;
  WORD e_lfarlc;
  WORD e_ovno;
  WORD e_res[4];
  WORD e_oemid;
  WORD e_oeminfo;
  WORD e_res2[10];
  DWORD e_lfanew;
};

struct NtFileHeader {
  WORD Machine;
  WORD NumberOfSections;
  DWORD TimeDateStamp;
  DWORD PointerToSymbolTable;
  DWORD NumberOfSymbols;
  WORD SizeOfOptionalHeader;
  WORD Characteristics;
};

struct NtDataDirectory {
  DWORD VirtualAddress;
  DWORD Size;
};

struct NtOptionalHeader32 {
  WORD Magic;
  BYTE MajorLinkerVersion;
  BYTE MinorLinkerVersion;
  DWORD SizeOfCode;
  DWORD SizeOfInitializedData;
  DWORD SizeOfUninitializedData;
  DWORD AddressOfEntryPoint;
  DWORD BaseOfCode;
  DWORD BaseOfData;
  DWORD ImageBase;
  DWORD SectionAlignment;
  DWORD FileAlignment;
  WORD MajorOperatingSystemVersion;
  WORD MinorOperatingSystemVersion;
  WORD MajorImageVersion;
  WORD MinorImageVersion;
  WORD MajorSubsystemVersion;
  WORD MinorSubsystemVersion;
  DWORD Win32VersionValue;
  DWORD SizeOfImage;
  DWORD SizeOfHeaders;
  DWORD CheckSum;
  WORD Subsystem;
  WORD DllCharacteristics;
  DWORD SizeOfStackReserve;
  DWORD SizeOfStackCommit;
  DWORD SizeOfHeapReserve;
  DWORD SizeOfHeapCommit;
  DWORD LoaderFlags;
  DWORD NumberOfRvaAndSizes;
  NtDataDirectory DataDirectory[IMAGE_NUMBEROF_DIRECTORY_ENTRIES];
};

struct NtOptionalHeader64 {
  WORD Magic;
  BYTE MajorLinkerVersion;
  BYTE MinorLinkerVersion;
  DWORD SizeOfCode;
  DWORD SizeOfInitializedData;
  DWORD SizeOfUninitializedData;
  DWORD AddressOfEntryPoint;
  DWORD BaseOfCode;
  LONGLONG ImageBase;
  DWORD SectionAlignment;
  DWORD FileAlignment;
  WORD MajorOperatingSystemVersion;
  WORD MinorOperatingSystemVersion;
  WORD MajorImageVersion;
  WORD MinorImageVersion;
  WORD MajorSubsystemVersion;
  WORD MinorSubsystemVersion;
  DWORD Win32VersionValue;
  DWORD SizeOfImage;
  DWORD SizeOfHeaders;
  DWORD CheckSum;
  WORD Subsystem;
  WORD DllCharacteristics;
  ULONGLONG SizeOfStackReserve;
  ULONGLONG SizeOfStackCommit;
  ULONGLONG SizeOfHeapReserve;
  ULONGLONG SizeOfHeapCommit;
  DWORD LoaderFlags;
  DWORD NumberOfRvaAndSizes;
  NtDataDirectory DataDirectory[IMAGE_NUMBEROF_DIRECTORY_ENTRIES];
};

struct NtHeaders32 {
  DWORD Signature;
  NtFileHeader FileHeader;
  NtOptionalHeader32 OptionalHeader;
};

struct NtHeaders64 {
  DWORD Signature;
  NtFileHeader FileHeader;
  NtOptionalHeader64 OptionalHeader;
};

struct SectionHeader {
  CHAR Name[IMAGE_SIZEOF_SHORT_NAME];

  union {
    DWORD PhysicalAddress;
    DWORD VirtualSize;
  } Misc;

  DWORD VirtualAddress;
  DWORD SizeOfRawData;
  DWORD PointerToRawData;
  DWORD PointerToRelocations;
  DWORD PointerToLineNumbers;
  WORD NumberOfRelocations;
  WORD NumberOfLineNumbers;
  DWORD Characteristics;
};

struct ExportDataDirectory {
  DWORD Characteristics;
  DWORD TimeDateStamp;
  WORD MajorVersion;
  WORD MinorVersion;
  DWORD Name;
  DWORD Base;
  DWORD NumberOfFunctions;
  DWORD NumberOfNames;
  DWORD AddressOfFunctions;     // RVA from base of image
  DWORD AddressOfNames;         // RVA from base of image
  DWORD AddressOfNameOrdinals;  // RVA from base of image
};

struct ExportInfo {
  std::string name;
  DWORD address;
};

struct ImageImportDescriptor {
  union {
    DWORD Characteristics;    /* 0 for terminating null import descriptor  */
    DWORD OriginalFirstThunk; /* RVA to original unbound IAT */
  };
  DWORD TimeDateStamp;  /* 0 if not bound,
                         * -1 if bound, and real date\time stamp
                         *    in IMAGE_DIRECTORY_ENTRY_BOUND_IMPORT
                         * (new BIND)
                         * otherwise date/time stamp of DLL bound to
                         * (Old BIND)
                         */
  DWORD ForwarderChain; /* -1 if no forwarders */
  DWORD Name;
  /* RVA to IAT (if bound this IAT has actual addresses) */
  DWORD FirstThunk;
};

struct ImageImportByName {
  USHORT Hint;
  CHAR Name[1];
};

struct ImageThunkData64 {
  union {
    ULONGLONG ForwarderString;  // PBYTE
    ULONGLONG Function;         // PDWORD
    ULONGLONG Ordinal;
    ULONGLONG AddressOfData;  // PIMAGE_IMPORT_BY_NAME
  };
};

struct ImageThunkData32 {
  union {
    DWORD ForwarderString;  // PBYTE
    DWORD Function;         // PDWORD
    DWORD Ordinal;
    DWORD AddressOfData;  // PIMAGE_IMPORT_BY_NAME
  };
};

struct ImportInfo {
  std::string name;
  std::string module;
  VA iat_entry;  // absolute address of IAT entrypoint
};

struct BaseRelocationEntry {
  WORD Offset : 12;
  WORD Type : 4;
};

struct BaseRelocation {
  DWORD VirtualAddress;
  DWORD SizeOfBlock;
};

struct FullBaseRelocation {
  BaseRelocation base;
  std::vector<BaseRelocationEntry> entries;
};
}  // namespace pe

struct PESectionInfo {
  pe::SectionHeader header;
  std::vector<uint8_t> data;
};

struct PEFormat {
  pe::DWORD size_of_raw_headers;
  TargetArchitecture architecture;

  pe::DosHeader dos_header;
  std::vector<char> dos_stub;

  union {
    pe::NtHeaders32 nt_headers32;
    pe::NtHeaders64 nt_headers64;
  };

  // Map of section names to basic section information
  std::vector<PESectionInfo> sections;

  pe::ExportDataDirectory export_info;
  std::vector<pe::DWORD> exports;

  std::map<VA, pe::ImportInfo> imports;

  std::vector<pe::FullBaseRelocation> relocations;

  // certificate table is usually the last
  std::vector<char> cert_table;

  /// Parse a PE file into the structure. The method may throw an I/O error in
  /// the case of a malformed PE file.
  /// @param stream file stream of the open PE file
  /// @param format empty PE mapping object
  /// @throw invalid_binary_format_error the open file is not a Windows PE
  static void Parse(std::fstream& stream, PEFormat& format);

  bool Is64Bit() const;
  bool Is32Bit() const;

  /// Get basic information about the given section
  /// @param name Name of section
  /// @return a PESectionInfo structure containing details about the chosen
  /// section
  /// @throw section_not_found_error the section doesn't exist
  const PESectionInfo& GetSectionInfo(const std::string& name);

  pe::DWORD Entrypoint() const;
  intptr_t ImageBase() const;
  pe::DWORD FileAlignment() const;
  pe::DWORD SectionAlignment() const;
  pe::DWORD& SizeOfHeaders();
  pe::DWORD& SizeOfImage();
  pe::DWORD& SizeOfInitializedData();
  pe::DWORD& SizeOfUninitializedData();
  pe::DWORD& SizeOfCode();
  pe::NtDataDirectory& DataDirectory(char id);
};

class PESection final : public Section {
  friend class PE;

  PESectionInfo si_;
  std::vector<std::unique_ptr<GlobalRef>> refs;

  void growRaw(int64_t old, int64_t amount) const;
  void growVirtual(int64_t old, int64_t amount) const;

  // expose setData to PE
  void setData(const std::vector<uint8_t>& data) override {
    Section::setData(data);
  }

 public:
  using Section::Write;

  PESection(const PESectionInfo& si, const SectionType type,
            const std::vector<uint8_t>& data, Binary* parent,
            const bool existed = false)
      : Section(si.header.Name, type, data, parent, existed), si_(si) {}

  void Write(const std::vector<uint8_t>& data) override;

  PESectionInfo& GetSectionInfo() { return si_; }

  unsigned Characteristics() const;

  void SetCharacteristics(unsigned ch);

  RVA GetAddress() const override { return si_.header.VirtualAddress; }

  void Relocate(const int64_t delta) override {
    Section::Relocate(delta);
    si_.header.VirtualAddress += delta;
  }
};

class PE final : public Binary {
  friend class PESection;

  bool parsed_;
  PEFormat file_mapping_ = {};
  std::vector<std::unique_ptr<PESection>> sections_;
  char bit_size_;
  static constexpr uint16_t kMaxPESections = 96;

  void parse();
  void parseRelocations();
  template <typename T = pe::ImageThunkData64>
  void parseImports();
  void parseExports();
  void rebuild(std::vector<char>& data);
  void addSectionHeader();
  RVA getNewSectionRVA();
  RVA getNewSectionRawPointer();
  void growSectionRawSize(const std::string& section_name, int64_t amount);
  void growSectionVirtualSize(const std::string& section_name, int64_t old,
                              int64_t amount);
  pe::NtDataDirectory* getCertTable();
  void* getContentAt(RVA rva) const;

 public:
  explicit PE() : Binary(Platform::Windows), parsed_(false), bit_size_(0) {}

  explicit PE(const std::string& file_name, const bool no_analyze = false)
      : Binary(file_name, Platform::Windows), parsed_(false), bit_size_(0) {
    PE::Open(file_name);
    if (!no_analyze) OpenCode()->AnalyzeFrom(GetEntrypoint());
  }

  void Open(const std::string& file_name) override {
    Binary::Open(file_name);
    parse();
  }

  /// Reads data at a specified virtual address
  /// @param address virtual address to read from
  /// @return pointer to data
  const uint8_t* ReadDataAt(VA address) const override;

  /// Opens a section
  /// @param name name of section
  /// @return section object
  /// @throw section_not_found_error
  Section* OpenSection(const std::string& name) const override;

  /// Opens the section that the specified virtual address falls into
  /// @param address virtual address
  /// @return section or nullptr
  Section* OpenSectionAt(VA address) const override;

  /// Creates a new section in the PE.
  /// This results in the PointerToRawData for each section being updated
  /// to make space for the new section's header.
  /// @param name name of new section
  /// @param type type of new section
  /// @throw section_error bad name provided
  Section* AddSection(const std::string& name, SectionType type) override;

  TargetArchitecture GetArchitecture() const {
    return file_mapping_.architecture;
  }

  VA GetImageBase() const override;

  /// Returns the VA of the entrypoint. This is used as the start point
  /// for code analysis.
  VA GetEntrypoint() const override;

  void Save() override;

  void SaveAs(const std::string& file_name) override;

  char GetBitSize() const override { return bit_size_; }

  const PEFormat& GetFileMapping() { return file_mapping_; }

  /// Returns the name of the import for the provided IAT entry address, which
  /// is dereferenced in code when called.
  /// @param address address of IAT entry
  /// @return name of the import
  std::string GetImportForAddress(const VA address) const override {
    if (file_mapping_.imports.contains(address))
      return file_mapping_.imports.at(address).name;
    return "";
  }

  /// Returns the iat entry address of an imported function, or 0 if the
  /// function isn't imported
  /// @param import name of imported function
  /// @return address of iat entry of imported function
  VA GetAddressForImport(const std::string& import) const override {
    for (const auto& [_, import_info] : file_mapping_.imports) {
      if (utils::tolower(import_info.name) == utils::tolower(import))
        return import_info.iat_entry;
    }
    return 0;
  }
};
}  // namespace stitch

#endif  // STITCH_BINARY_PE_H_
```

`include/stitch/binary/shellcode.h`:

```h
/*
 * Licensed to BadHive under one or more contributor license
 * agreements.  See the NOTICE file distributed with this work
 * for additional information regarding copyright ownership.
 * BadHive licenses this file to you under the Apache License,
 * Version 2.0 (the "License"); you may not use this file
 * except in compliance with the License.  You may obtain a
 * copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 */

#ifndef STITCH_BINARY_SHELLCODE_H_
#define STITCH_BINARY_SHELLCODE_H_

#include <memory>

#include "stitch/binary/binary.h"
#include "stitch/target/target.h"

namespace stitch {
class SCSection final : public Section {
  RVA address_;

 public:
  explicit SCSection(const RVA address, const std::vector<uint8_t>& data,
                     Binary* parent, const bool existed)
      : Section("", SectionType::Code, data, parent, existed),
        address_(address) {}

  RVA GetAddress() const override { return address_; }

  void Write(const std::vector<uint8_t>& data) override {
    getData().insert(GetData().end(), data.begin(), data.end());
  }
};

class Shellcode final : public Binary {
  const TargetArchitecture architecture_;
  std::unique_ptr<SCSection> old_section_;
  std::unique_ptr<SCSection> new_section_;
  bool parsed_;

  void parse();

 public:
  Shellcode(const TargetArchitecture arch, const Platform platform)
      : Binary(platform), architecture_(arch), parsed_(false) {}

  Shellcode(const std::string& file_name, const TargetArchitecture arch,
            const Platform platform, const bool no_analyze = false)
      : Binary(file_name, platform), architecture_(arch), parsed_(false) {
    Shellcode::Open(file_name);
    if (!no_analyze) OpenCode()->AnalyzeFrom(0);
  }

  void Open(const std::string& file_name) override {
    Binary::Open(file_name);
    parse();
  }

  const uint8_t* ReadDataAt(VA address) const override;

  Section* AddSection(const std::string& name, SectionType type) override;

  Section* OpenSection(const std::string& name) const override;

  Section* OpenSectionAt(VA address) const override;

  VA GetImageBase() const override { return 0; }

  VA GetEntrypoint() const override { return 0; }

  void Save() override;

  void SaveAs(const std::string& file_name) override;

  char GetBitSize() const override {
    return architecture_ == TargetArchitecture::I386 ? 32 : 64;
  }

  std::string GetImportForAddress(VA address) const override { return ""; }

  VA GetAddressForImport(const std::string& import) const override { return 0; }
};
}  // namespace stitch
#endif  // STITCH_BINARY_SHELLCODE_H_
```

`include/stitch/misc/errors.h`:

```h
/*
 * Licensed to BadHive under one or more contributor license
 * agreements.  See the NOTICE file distributed with this work
 * for additional information regarding copyright ownership.
 * BadHive licenses this file to you under the Apache License,
 * Version 2.0 (the "License"); you may not use this file
 * except in compliance with the License.  You may obtain a
 * copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 */

#ifndef STITCH_ERRORS_H_
#define STITCH_ERRORS_H_

#include <stdexcept>

namespace stitch {
//----------------------------------------------------------------------------//
//------------------           top-level errors           --------------------//
//----------------------------------------------------------------------------//
class binary_error : public std::runtime_error {
 public:
  explicit binary_error(const std::string& msg) : std::runtime_error(msg) {}
};

class section_error : public std::runtime_error {
 public:
  explicit section_error(const std::string& msg) : runtime_error(msg) {}
};

class code_error : public std::runtime_error {
 public:
  explicit code_error(const std::string& msg) : runtime_error(msg) {}
};

//----------------------------------------------------------------------------//
//------------------              sub-errors              --------------------//
//----------------------------------------------------------------------------//
class invalid_binary_format_error : public binary_error {
 public:
  invalid_binary_format_error() : binary_error("invalid binary format") {}
};

class unsupported_section_type_error : public section_error {
 public:
  explicit unsupported_section_type_error(const std::string& name)
      : section_error("section '" + name +
                      "' stores data of an unsupported type") {}
};

class section_not_found_error : public section_error {
 public:
  explicit section_not_found_error(const std::string& name)
      : section_error("section '" + name + "' not found") {}
};

class invalid_section_name_error : public section_error {
 public:
  invalid_section_name_error() : section_error("section name too long") {}
};

class section_exists_error : public section_error {
 public:
  section_exists_error() : section_error("section already exists") {}
};

class arch_mismatch_error : public code_error {
 public:
  arch_mismatch_error()
      : code_error("architecture mismatch between code components") {}
};

class import_not_found_error : public code_error {
 public:
  import_not_found_error() : code_error("import not found") {}
};
}  // namespace stitch

#endif  // STITCH_ERRORS_H_
```

`include/stitch/misc/utils.h`:

```h
/*
 * Licensed to BadHive under one or more contributor license
 * agreements.  See the NOTICE file distributed with this work
 * for additional information regarding copyright ownership.
 * BadHive licenses this file to you under the Apache License,
 * Version 2.0 (the "License"); you may not use this file
 * except in compliance with the License.  You may obtain a
 * copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 */

#ifndef STITCH_UTILS_H_
#define STITCH_UTILS_H_

#include <string>

namespace stitch {
using VA = std::intptr_t;
using RVA = std::intptr_t;

namespace utils {
template <typename T>
inline constexpr bool dependent_false = false;

template <typename V, typename A>
V RoundToBoundary(V value, A alignment) {
  return value ? ((value + alignment - 1) / alignment) * alignment : 0;
}

inline std::string tolower(const std::string& str) {
  std::string ret(str);
  for (int i = 0; i < str.length(); i++) {
    ret[i] = std::tolower(str[i]);
  }
  return ret;
}

// stupidly simple solving that is useless outside this project
namespace sym {
class Reg {
  bool defined_;
  const std::string name_;
  uint64_t value_;

 public:
  explicit Reg(const std::string& name)
      : defined_(false), name_(name), value_(~0) {}

  explicit Reg(const std::string& name, const uint64_t value)
      : defined_(true), name_(name), value_(value) {}

  operator uint64_t() const { return value_; }

  bool Defined() const { return defined_; }

  void Undefine() {
    defined_ = false;
    value_ = ~0;
  }

  Reg& operator=(const uint64_t value) {
    value_ = value;
    defined_ = true;
    return *this;
  }

  Reg& operator=(const Reg& other) {
    defined_ = other.defined_;
    value_ = other.value_;
    return *this;
  }

  Reg& operator+(const Reg& other) {
    if (!other.defined_) {
      defined_ = false;
    } else {
      value_ += other.value_;
    }
    return *this;
  }

  Reg& operator-(const Reg& other) {
    if (!other.defined_) {
      if (*this == other) {
        defined_ = true;
        value_ = 0;
      } else
        defined_ = false;
    } else {
      value_ -= other.value_;
    }
    return *this;
  }

  Reg& operator*(const Reg& other) {
    if (!other.defined_) {
      defined_ = false;
    } else {
      value_ *= other.value_;
    }
    return *this;
  }

  Reg& operator/(const Reg& other) {
    if (!other.defined_) {
      if (*this == other) {
        defined_ = true;
        value_ = 1;
      } else
        defined_ = false;
    } else {
      value_ /= other.value_;
    }
    return *this;
  }

  Reg& operator&(const Reg& other) {
    if (!other.defined_) {
      defined_ = false;
    } else {
      value_ &= other.value_;
    }
    return *this;
  }

  Reg& operator|(const Reg& other) {
    if (!other.defined_) {
      defined_ = false;
    } else {
      value_ |= other.value_;
    }
    return *this;
  }

  Reg& operator^(const Reg& other) {
    if (!other.defined_) {
      if (*this == other) {
        defined_ = true;
        value_ = 0;
      } else
        defined_ = false;
    } else {
      value_ ^= other.value_;
    }
    return *this;
  }

  Reg operator+(const uint64_t other) {
    auto r = Reg(*this);
    r.value_ += other;
    return r;
  }

  Reg operator-(const uint64_t other) {
    auto r = Reg(*this);
    r.value_ -= other;
    return r;
  }

  Reg operator*(const uint64_t other) {
    auto r = Reg(*this);
    r.value_ *= other;
    return r;
  }

  Reg operator/(const uint64_t other) {
    auto r = Reg(*this);
    r.value_ /= other;
    return r;
  }

  Reg operator&(const uint64_t other) {
    auto r = Reg(*this);
    r.value_ &= other;
    return r;
  }

  Reg operator|(const uint64_t other) {
    auto r = Reg(*this);
    r.value_ |= other;
    return r;
  }

  Reg operator^(const uint64_t other) {
    auto r = Reg(*this);
    r.value_ ^= other;
    return r;
  }

  bool operator==(const Reg& other) const {
    return name_ == other.name_ && defined_ == other.defined_ &&
           value_ == other.value_;
  }

  bool operator==(const uint64_t other) const {
    return defined_ && value_ == other;
  }
};
}  // namespace sym
}  // namespace utils
}  // namespace stitch

#endif  // STITCH_UTILS_H_
```

`include/stitch/stitch.h`:

```h
/*
 * Licensed to BadHive under one or more contributor license
 * agreements.  See the NOTICE file distributed with this work
 * for additional information regarding copyright ownership.
 * BadHive licenses this file to you under the Apache License,
 * Version 2.0 (the "License"); you may not use this file
 * except in compliance with the License.  You may obtain a
 * copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 */

#ifndef STITCH_STITCH_H_
#define STITCH_STITCH_H_

// misc
#include <stitch/misc/errors.h>

// binary formats
#include <stitch/binary/pe.h>
#include <stitch/binary/shellcode.h>

// target architectures
#include <stitch/target/x86.h>

#endif //STITCH_STITCH_H_
```

`include/stitch/target/target.h`:

```h
/*
 * Licensed to BadHive under one or more contributor license
 * agreements.  See the NOTICE file distributed with this work
 * for additional information regarding copyright ownership.
 * BadHive licenses this file to you under the Apache License,
 * Version 2.0 (the "License"); you may not use this file
 * except in compliance with the License.  You may obtain a
 * copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 */

#ifndef STITCH_TARGET_TARGET_H_
#define STITCH_TARGET_TARGET_H_

#include "stitch/binary/binary.h"
#include "stitch/misc/utils.h"

namespace stitch {

class Function;
class Inst;
class Operand;

constexpr bool is_little_endian = std::endian::native == std::endian::little;

enum class TargetArchitecture {
  Invalid = 0,
  I386,
  AMD64,
  ARM64,
};

class Code {
  Binary* binary_;
  const TargetArchitecture kArch;

 public:
  explicit Code(Binary* binary, const TargetArchitecture arch)
      : binary_(binary), kArch(arch) {}

  virtual ~Code() = default;

  TargetArchitecture GetArchitecture() const { return kArch; }

  template <typename T = Binary>
  T* GetParent() const {
    return dynamic_cast<T*>(binary_);
  }

  virtual void AnalyzeFrom(VA address) = 0;

  virtual Function* CreateFunction(const std::string& in) = 0;

  virtual Function* CreateFunction(const Section& new_scn) = 0;

  virtual Function* EditFunction(VA address, const std::string& in) = 0;

  virtual Function* EditFunction(VA address, const Section& new_scn) = 0;

  virtual Function* RebuildFunction(VA address, const std::string& in) = 0;

  virtual Function* RebuildFunction(VA address, const Section& new_scn) = 0;
};

class Function {
  VA address_;
  int64_t size_;
  Code* code_;
  Inst* entry_point_;

 protected:
  VA startAddress() const { return address_; }

  void setAddress(const VA address) { address_ = address; }

  void setSize(const VA size) { size_ = size; }

 public:
  explicit Function(const VA address, Code* code)
      : address_(address), size_(0), code_(code), entry_point_(nullptr) {}

  virtual ~Function() = default;

  VA GetAddress() const { return address_; }

  VA GetSize() const { return size_; }

  template <typename T = Code>
  T* GetParent() const {
    return dynamic_cast<T*>(code_);
  }

  virtual const GlobalRef* Finish() = 0;

  void StartEdit(const std::string& in) const {
    code_->EditFunction(address_, in);
  }

  void StartEdit(const Section& in) const { code_->EditFunction(address_, in); }
};

class BasicBlock {
  VA address_;
  int64_t size_;
  const BasicBlock* fallthrough_;

 protected:
  void setFallthrough(const BasicBlock* bb) { fallthrough_ = bb; }

 public:
  BasicBlock(const VA address, const int64_t size,
             const BasicBlock* fallthrough = nullptr)
      : address_(address), size_(size), fallthrough_(fallthrough) {}

  VA GetAddress() const { return address_; }

  int64_t GetSize() const { return size_; }

  void SetSize(const int64_t size) { size_ = size; }

  template <typename T = BasicBlock>
  const T* GetFallthroughParent() const {
    return dynamic_cast<const T*>(fallthrough_);
  }
};

class Inst {
  VA address_;
  Function* function_;
  Binary* binary_;

 protected:
  void setAddress(const VA address) { address_ = address; }

 public:
  explicit Inst(const VA address, Function* function)
      : address_(address),
        function_(function),
        binary_(function->GetParent()->GetParent()) {}

  virtual ~Inst() = default;

  template <typename T = Function>
  T* GetParent() const {
    return dynamic_cast<T*>(function_);
  }

  VA GetAddress() const { return address_; }
};

class JumpTable32 {
  const bool le_;
  std::vector<uint32_t> handlers_;

  static uint32_t byteSwap(const uint32_t v) {
    uint32_t out{};
    auto* dst = reinterpret_cast<unsigned char*>(&out);
    auto* src = reinterpret_cast<const unsigned char*>(&v);
    for (size_t i = 0; i < sizeof(uint32_t); ++i)
      dst[i] = src[sizeof(uint32_t) - 1 - i];
    return out;
  }

  void normalize() {
    if (le_ == is_little_endian) return;
    for (auto& h : handlers_) h = byteSwap(h);
  }

 public:
  explicit JumpTable32(const bool le = true) : le_(le) {}

  uint64_t RegisterHandler(const uint32_t address) {
    handlers_.push_back(address);
    return handlers_.size() - 1;
  }

  uint64_t GetSize() const { return handlers_.size() * sizeof(uint32_t); }

  template <typename T = uint32_t*>
  T Get() {
    normalize();
    return reinterpret_cast<T>(handlers_.data());
  }
};

class JumpTable64 {
  const bool le_;
  std::vector<uint64_t> handlers_;

  static uint64_t byteSwap(const uint64_t v) {
    uint64_t out{};
    auto* dst = reinterpret_cast<unsigned char*>(&out);
    auto* src = reinterpret_cast<const unsigned char*>(&v);
    for (size_t i = 0; i < sizeof(uint64_t); ++i)
      dst[i] = src[sizeof(uint64_t) - 1 - i];
    return out;
  }

  void normalize() {
    std::sort(handlers_.begin(), handlers_.end());
    if (le_ == is_little_endian) return;
    for (auto& h : handlers_) h = byteSwap(h);
  }

 public:
  explicit JumpTable64(const bool le = true) : le_(le) {}

  uint64_t RegisterHandler(const uint64_t address) {
    handlers_.push_back(address);
    return handlers_.size() - 1;
  }

  void UpdateHandler(const uint64_t id, const uint64_t new_value) {
    if (handlers_.size() <= id) throw code_error("invalid handler id");
    handlers_[id] = new_value;
  }

  uint64_t GetSize() const { return handlers_.size() * sizeof(uint64_t); }

  template <typename T = uint64_t>
  T* Get() {
    normalize();
    return reinterpret_cast<T*>(handlers_.data());
  }
};
}  // namespace stitch

#endif  // STITCH_TARGET_TARGET_H_
```

`include/stitch/target/x86.h`:

```h
/*
 * Licensed to BadHive under one or more contributor license
 * agreements.  See the NOTICE file distributed with this work
 * for additional information regarding copyright ownership.
 * BadHive licenses this file to you under the Apache License,
 * Version 2.0 (the "License"); you may not use this file
 * except in compliance with the License.  You may obtain a
 * copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 */

#ifndef STITCH_TARGET_X86_H_
#define STITCH_TARGET_X86_H_

#include <algorithm>
#include <functional>
#include <map>
#include <memory>
#include <optional>
#include <set>
#include <zasm/zasm.hpp>

#include "stitch/binary/binary.h"
#include "stitch/target/target.h"

namespace stitch {
class X86Function;
class X86BasicBlock;
class X86Inst;
class X86InstBase;

namespace x86 {
static constexpr zasm::Reg nopReg(
    static_cast<zasm::Reg::Id>(ZYDIS_REGISTER_NONE));

static std::vector gp64 = {
    zasm::x86::rax, zasm::x86::rbx, zasm::x86::rcx, zasm::x86::rdx,
    zasm::x86::rbp, zasm::x86::rsp, zasm::x86::rdi, zasm::x86::rsi,
    zasm::x86::r8,  zasm::x86::r9,  zasm::x86::r10, zasm::x86::r11,
    zasm::x86::r12, zasm::x86::r13, zasm::x86::r14, zasm::x86::r15};

static std::vector gp32 = {zasm::x86::eax, zasm::x86::ebx, zasm::x86::ecx,
                           zasm::x86::edx, zasm::x86::ebp, zasm::x86::esp,
                           zasm::x86::edi, zasm::x86::esi};

static std::vector gpVolWin32 = {zasm::x86::eax, zasm::x86::ecx,
                                 zasm::x86::edx};

static std::vector gpVolWin64 = {zasm::x86::rax, zasm::x86::rcx, zasm::x86::rdx,
                                 zasm::x86::r8,  zasm::x86::r9,  zasm::x86::r10,
                                 zasm::x86::r11};
}  // namespace x86

using PatchPolicy =
    std::function<void(zasm::x86::Assembler& as, VA old_loc, VA new_loc)>;
using Instrumentor = std::function<void(zasm::x86::Assembler& as)>;
// deprecated
using ProgramInstrumentor =
    std::function<void(zasm::Program& pr, zasm::x86::Assembler& as)>;
using FunctionInstrumentor =
    std::function<void(X86Function* fn, zasm::x86::Assembler& as)>;

inline void DefaultPatchPolicy(zasm::x86::Assembler& as, const VA _,
                               const VA new_loc) {
  as.jmp(zasm::Imm(new_loc));
}

class X86Code final : public Code {
  friend class X86Function;

  bool analyzed_;
  std::vector<std::unique_ptr<X86Function>> functions_;
  PatchPolicy patch_policy_;
  zasm::MachineMode mm_;

  X86Function* analyzeFunction(VA address);
  void analyzeTailCalls();
  X86Function* editFunction(VA address, const std::string& in);
  X86Function* buildFunction(VA fn_address, const Section* scn, int reopen_idx);
  void patchOriginalLocation(const X86Function& fn, VA new_loc) const;

 public:
  static constexpr uint8_t kFunctionAlignment = 16;

  explicit X86Code(Binary* binary, const TargetArchitecture arch)
      : Code(binary, arch),
        analyzed_(false),
        patch_policy_(DefaultPatchPolicy) {
    if (arch != TargetArchitecture::I386 && arch != TargetArchitecture::AMD64) {
      throw std::runtime_error("unexpected architecture");
    }
    mm_ = GetArchitecture() == TargetArchitecture::I386
              ? zasm::MachineMode::I386
              : zasm::MachineMode::AMD64;
  }

  /// Changes the default method used to patch moved functions, which is
  /// to emit a jump call to the function's new VA.
  /// @param policy function that ultimately emits a jump call to the new
  /// function address
  void SetPatchPolicy(const PatchPolicy& policy) { patch_policy_ = policy; }

  /// Creates a new function object for writing new code.
  /// @param in name of section that function will exist in
  /// @return reference to new Function object
  Function* CreateFunction(const std::string& in) override;

  Function* CreateFunction(const Section& new_scn) override;

  /// Creates a new function object from the code at the provided address, to
  /// be edited in the specified section. If an empty name is specified,
  /// ".stitch" is used.
  /// @param address VA of function
  /// @param in name of section that code is moved to
  /// @return reference to new Function object.
  Function* EditFunction(VA address, const std::string& in) override;

  Function* EditFunction(VA address, const Section& scn) override;

  /// Creates a new function to replace the function at the provided address.
  /// The resulting object will have an empty assembler instance.
  /// @param address VA of function
  /// @param in name of section that code is moved to
  /// @return reference to new Function object
  Function* RebuildFunction(VA address, const std::string& in) override;

  Function* RebuildFunction(VA address, const Section& scn) override;

  std::vector<X86Function*> GetFunctions() const {
    std::vector<X86Function*> ret(functions_.size());
    int i = 0;
    for (const auto& fn : functions_) {
      ret[i] = fn.get();
      i++;
    }
    return ret;
  }

  /// Analyse all code in an executable file
  /// @param address address to start code analysis (entrypoint)
  void AnalyzeFrom(VA address) override;

  /// Returns the address of an imported function
  /// @param name name of imported symbol
  /// @return address of import
  /// @throw import_not_found_error if symbol not imported
  VA GetImport(const std::string& name) const {
    const VA address = GetParent()->GetAddressForImport(name);
    if (!address) throw import_not_found_error();
    return address;
  }

  /// Creates an assembler operand referencing an imported function
  /// @param name name of imported function
  /// @return zasm::Mem operand for use with call / jmp
  /// @throw import_not_found_error if symbol not imported
  zasm::Mem ImportOperand(const std::string& name) const {
    const VA address = GetImport(name);
    const auto bit_size = GetParent()->GetBitSize() == 64 ? zasm::BitSize::_64
                                                          : zasm::BitSize::_32;
    // absolute indirect addressing for 32-bit import callslast_label
    if (bit_size == zasm::BitSize::_32) {
      return zasm::Mem(bit_size, x86::nopReg, x86::nopReg, x86::nopReg, 0,
                       address);
    }
    return zasm::Mem(bit_size, x86::nopReg, zasm::x86::rip, x86::nopReg, 0,
                     address);
  }

  /// Converts an address to a zasm::Operand
  /// @param address global address of instruction or data
  /// @return rip-relative address (64-bit) or immediate (32-bit)
  zasm::Operand AddressOperand(const VA address) const {
    const auto bit_size = GetParent()->GetBitSize() == 64 ? zasm::BitSize::_64
                                                          : zasm::BitSize::_32;
    if (bit_size == zasm::BitSize::_32) {
      if (address > std::numeric_limits<std::uint32_t>::max())
        throw code_error("value exceeds 32-bit limit");
      return zasm::Imm32(address);
    }
    return zasm::Mem(bit_size, x86::nopReg, zasm::x86::rip, x86::nopReg, 0,
                     address);
  }
};

class X86Function final : public Function {
  friend class X86Code;

  bool finished_;

  // jump table database
  std::map<uint64_t, std::variant<JumpTable32, JumpTable64>> jump_tables_;
  std::map<uint64_t, std::vector<const X86BasicBlock*>> jump_tables_handlers_;

  std::string error_;
  zasm::Program program_;
  std::optional<zasm::x86::Assembler> assembler_;
  zasm::Node* start_pos_;

  std::vector<X86Inst> instructions_;
  std::vector<X86InstBase> new_instructions_;

  // only used for initial copy of function to new section
  std::vector<std::unique_ptr<X86BasicBlock>> basic_blocks_;
  std::set<X86BasicBlock*> exit_blocks_;
  std::set<X86BasicBlock*> dispatch_blocks_;

  Section* old_section_;
  Section* new_section_;

  std::set<VA> visited_insts_;
  std::set<VA> analyzed_insts_;

  zasm::Node* entry_point_;

  void setError(const std::string& error) { error_ = error; }

  zasm::MachineMode getMachineMode() const;
  std::vector<X86Inst*> getBlockInstructions(const X86BasicBlock* block);
  std::vector<const X86Inst*> getBlockInstructions(
      const X86BasicBlock* block) const;
  void disassemble(zasm::Decoder& decoder, const uint8_t* code,
                   size_t code_size, VA runtime_address, VA offset,
                   std::set<VA>& visited_insts);
  X86BasicBlock* analyzeControlFlow(std::vector<X86Inst>::iterator curr,
                                    std::set<VA>& analyzed_insts,
                                    X86BasicBlock* parent_block);

  // X86Function analysis passes
  void genBlockLivenessInfo();
  void genInstructionLivenessInfo();
  void genStackInfo(uint64_t initial_sp = 0);
  void genStackOffsets(std::vector<X86Inst>::iterator it,
                       std::map<int8_t, utils::sym::Reg>& reg_map,
                       std::set<VA>& visited_insts);

  void findAndSplitBasicBlock(VA address, X86BasicBlock* new_parent);
  X86BasicBlock* splitAfter(X86BasicBlock* block, VA address);
  void removeBasicBlocksAfter(const X86BasicBlock* final_block);
  X86BasicBlock* addBasicBlock(VA loc, uint64_t size, X86BasicBlock* parent,
                               X86BasicBlock* fallthrough = nullptr);
  bool isWithinFunction(VA address) const;
  Section* getOldSection() const { return old_section_; }
  void setOldSection(Section* section) { old_section_ = section; }
  Section* getNewSection() const { return new_section_; }
  void setNewSection(Section* section) { new_section_ = section; }
  int getInstructionAtAddress(VA address) const;

  const std::vector<std::unique_ptr<X86BasicBlock>>& getBasicBlocks() const {
    return basic_blocks_;
  }

  const std::set<X86BasicBlock*>& getExitBlocks() const { return exit_blocks_; }

  void runAnalyses(unsigned int entry = -1,
                   X86BasicBlock* parent_block = nullptr) {
    std::sort(instructions_.begin(), instructions_.end());
    if (entry == -1) entry = getInstructionAtAddress(GetAddress());
    analyzeControlFlow(instructions_.begin() + entry, analyzed_insts_,
                       parent_block);
    smartSortInstructions();
    genBlockLivenessInfo();
    genInstructionLivenessInfo();
    genStackInfo();
  }

  void smartSortInstructions();
  void refreshCode();
  void finalize();

  template <typename I>
  void callInstrumentor(const I& instrumentor) {
    if constexpr (std::is_invocable_v<I, zasm::x86::Assembler&>)
      instrumentor(assembler_.value());
    else if constexpr (std::is_invocable_v<I, zasm::Program&,
                                           zasm::x86::Assembler&>)
      instrumentor(program_, assembler_.value());
    else if constexpr (std::is_invocable_v<I, X86Function*,
                                           zasm::x86::Assembler&>)
      instrumentor(this, assembler_.value());
    else
      // neat trick, makes the constexpr false dependent on the template being
      // instantiated
      static_assert(
          utils::dependent_false<I>,
          "expected Instrumentor, ProgramInstrumentor or FunctionInstrumentor");
    refreshCode();
  }

 public:
  explicit X86Function(const VA address, zasm::Program&& program, X86Code* code)
      : Function(address, code),
        finished_(false),
        program_(std::move(program)),
        assembler_(program_),
        start_pos_(nullptr),
        old_section_(nullptr),
        new_section_(nullptr),
        entry_point_(nullptr) {}

  zasm::Node* GetStartPos() const { return start_pos_; }

  const std::string& GetError() const { return error_; }

  std::vector<const X86Inst*> GetBlockInstructions(
      const X86BasicBlock* block) const {
    return getBlockInstructions(block);
  }

  const std::vector<X86Inst>& GetOriginalCode() const { return instructions_; }

  const std::vector<X86InstBase>& GetCode() const { return new_instructions_; }

  X86BasicBlock* GetBasicBlockAt(VA address) const;

  std::vector<X86BasicBlock*> GetBlocks() const {
    std::vector<X86BasicBlock*> blocks;
    for (auto& bb : getBasicBlocks()) {
      blocks.push_back(bb.get());
    }
    return blocks;
  }

  /// Imports a jump table from disk and registers its lifted handlers
  /// @param address virtual address of the jump table
  /// @param num_entries number of entries in jump table
  /// @param dispatcher_block jump table dispatcher
  /// @param le binary endianness
  /// @return imported jump table ID
  uint64_t ImportJumpTable(VA address, uint32_t num_entries,
                           X86BasicBlock* dispatcher_block, bool le = true);

  /// Creates a new jump table
  /// @return jump table id
  uint64_t CreateJumpTable();

  /// Mark a basic block as a jump table handler. It will be used to generate
  /// a new jump table when the function is serialized.
  /// @param jt_id id of the jump table, received from CreateJumpTable
  /// @param bb basic block
  void MarkJumpTableHandler(uint64_t jt_id, const X86BasicBlock* bb);

  zasm::Program& GetProgram() { return program_; }

  /// Pass a list of functions to instrument this X86Function.
  /// @param instrumentors list of Instrumentor, ProgramInstrumentor or
  /// FunctionInstrumentor
  template <typename... Args>
  const GlobalRef* Instrument(Args... instrumentors) {
    (callInstrumentor(instrumentors), ...);
    return Finish();
  }

  /// Returns a generated jump table, which can be inserted at an appropriate
  /// location in the binary
  /// @tparam T type of jump table (JumpTable{32,64})
  /// @param id id of jump table
  /// @return reference to the jump table
  template <typename T>
  const T& GetJumpTable(const uint64_t id) const {
    if (!finished_ || jump_tables_.empty())
      throw code_error("jump table has not been generated");
    if (jump_tables_.size() <= id) throw code_error("jump table not found");
    return std::get<T>(jump_tables_.at(id));
  }

  void SetEntryPoint(zasm::Node* entry) { entry_point_ = entry; }

  zasm::Node* GetEntryPoint() const { return entry_point_; }

  /// Saves new code to file
  const GlobalRef* Finish() override;
};

enum class X86BlockTermReason {
  Invalid = 0,
  Fallthrough,
  CondBr,
  Jmp,
  TailCall,
  Ret,
  Error
};

class X86BasicBlock : public BasicBlock {
  friend class X86Function;

  zasm::Node* block_label_;
  bool is_exit_;
  X86BlockTermReason term_reason_;
  std::set<X86BasicBlock*> predecessors_;
  std::set<X86BasicBlock*> successors_;

  uint32_t regs_gen_;
  uint32_t regs_kill_;
  zasm::InstrCPUFlags flags_gen_;
  zasm::InstrCPUFlags flags_kill_;

  uint32_t regs_live_in_;
  uint32_t regs_live_out_;
  zasm::InstrCPUFlags flags_live_in_;
  zasm::InstrCPUFlags flags_live_out_;

  std::set<X86BasicBlock*>& getParents() { return predecessors_; }

  std::set<X86BasicBlock*>& getChildren() { return successors_; }

 public:
  X86BasicBlock(const VA address, const int64_t size, X86BasicBlock* parent,
                const X86BasicBlock* fallthrough)
      : BasicBlock(address, size, fallthrough),
        is_exit_(false),
        term_reason_(X86BlockTermReason::Invalid),
        regs_gen_(0),
        regs_kill_(0),
        flags_gen_(0),
        flags_kill_(0),
        regs_live_in_(0),
        regs_live_out_(0),
        flags_live_in_(0),
        flags_live_out_(0) {
    AddParent(parent);
  }

  const std::set<X86BasicBlock*>& GetParents() const { return predecessors_; }

  const std::set<X86BasicBlock*>& GetChildren() const { return successors_; }

  const X86BasicBlock* GetChild() const {
    if (successors_.empty()) return nullptr;
    return *successors_.begin();
  }

  void AddParent(X86BasicBlock* parent) {
    if (parent) predecessors_.insert(parent);
  }

  void AddChild(X86BasicBlock* child) {
    if (child) successors_.insert(child);
  }

  void SetExit(const bool is_exit) { is_exit_ = is_exit; }

  void SetTermReason(const X86BlockTermReason reason) { term_reason_ = reason; }

  X86BlockTermReason GetTermReason() const { return term_reason_; }
};

class X86Inst final : public Inst {
  friend class X86Function;

  zasm::Node* pos_;
  zasm::InstructionDetail instruction_;
  zasm::MachineMode mm_;
  X86BasicBlock* basic_block_;

  uint32_t regs_read_;
  uint32_t regs_written_;
  zasm::InstrCPUFlags flags_modified_;
  zasm::InstrCPUFlags flags_tested_;

  uint32_t regs_live_;
  zasm::InstrCPUFlags flags_live_;

  uint64_t stack_offset_;

  bool is_br_;

  VA br_location_;
  RVA br_distance_;

  bool br_is_local_;

  bool analyzed_;

  static uint32_t regMask(const zasm::Reg reg) { return 1u << reg.getIndex(); }

  void setPos(zasm::Node* pos) { pos_ = pos; }

  void setBasicBlock(X86BasicBlock* basic_block) { basic_block_ = basic_block; }

  // make positive to differentiate between valid and invalid stack offset
  void setStackOffset(const uint64_t offset) { stack_offset_ = -offset; }

  void addInstructionContext();
  void addInstructionSpecificContext(TargetArchitecture arch,
                                     Platform platform);

  void setIsBranching(const bool branching) { is_br_ = branching; }
  void setBranchLocation(const VA address) { br_location_ = address; }
  void setBranchDistance(const RVA distance) { br_distance_ = distance; }
  void setIsLocalBranch(const bool local) { br_is_local_ = local; }

 public:
  X86Inst(const zasm::MachineMode mm,
          const zasm::InstructionDetail& instruction, X86Function* function)
      : Inst(0, function),
        pos_(nullptr),
        instruction_(instruction),
        mm_(mm),
        basic_block_(nullptr),
        regs_read_(0),
        regs_written_(0),
        flags_modified_(0),
        flags_tested_(0),
        regs_live_(0),
        flags_live_(0),
        stack_offset_(-1),
        is_br_(false),
        br_location_(0),
        br_distance_(0),
        br_is_local_(false),
        analyzed_(false) {
    const TargetArchitecture arch = function->GetParent()->GetArchitecture();
    const Platform platform = function->GetParent()->GetParent()->GetPlatform();
    addInstructionContext();
    addInstructionSpecificContext(arch, platform);
    is_br_ = zasm::x86::isBranching(instruction_) &&
             instruction_.getCategory() != zasm::x86::Category::Ret;
    if (is_br_) {
      if (const auto jmp_imm = instruction.getOperandIf<zasm::Imm>(0)) {
        const VA jmp_addr = jmp_imm->value<VA>();
        setBranchLocation(jmp_addr);
        setBranchDistance(jmp_addr - (GetAddress() + instruction.getLength()));
      }
    }
  }

  const zasm::InstructionDetail& RawInst() const { return instruction_; }

  /// Gets the position of the instruction within the assembler. This
  /// position can only be used with the assembler that this instruction
  /// comes from
  /// @return position of instruction
  zasm::Node* GetPos() const { return pos_; }

  X86BasicBlock* GetBasicBlock() const { return basic_block_; }

  template <typename T = zasm::x86::Gp64>
  std::optional<T> GetAvailableRegister() const {
    auto available = GetAvailableRegisters<T>();
    if (available.size() == 0) return std::nullopt;
    return available.front();
  }

  template <typename T = zasm::x86::Gp64>
  std::vector<T> GetAvailableRegisters() const {
    std::vector<T> available;
    if constexpr (std::is_base_of_v<zasm::x86::Gp32, T>) {
      for (const auto& reg : x86::gp32) {
        if ((regs_live_ & regMask(reg)) == 0) {
          available.push_back(reg);  // reg is Gp32, matches T
        }
      }
    } else if constexpr (std::is_base_of_v<zasm::x86::Gp64, T>) {
      for (const auto& reg : x86::gp64) {
        if ((regs_live_ & regMask(reg)) == 0) {
          available.push_back(reg);  // reg is Gp64, matches T
        }
      }
    }
    return available;
  }

  RVA GetBranchLocation() const { return br_location_; }

  RVA GetBranchDistance() const { return br_distance_; }

  bool IsLocalBranch() const { return br_is_local_; }

  bool IsBranching() const { return is_br_; }

  uint64_t GetStackOffset() const { return stack_offset_; }

  uint32_t GetLiveFlags() const { return flags_live_; }

  bool CFLive() const { return flags_live_ & zasm::x86::CPUFlags::CF; }

  bool PFLive() const { return flags_live_ & zasm::x86::CPUFlags::PF; }

  bool AFLive() const { return flags_live_ & zasm::x86::CPUFlags::AF; }

  bool ZFLive() const { return flags_live_ & zasm::x86::CPUFlags::ZF; }

  bool SFLive() const { return flags_live_ & zasm::x86::CPUFlags::SF; }

  bool OFLive() const { return flags_live_ & zasm::x86::CPUFlags::OF; }

  /// Returns true if CF, PF, AF, ZF, SF, and OF flags are available to be
  /// overwritten
  /// @return true if available
  bool CommonFlagsAvailable() const {
    return !(CFLive() || PFLive() || AFLive() || ZFLive() || SFLive() ||
             OFLive());
  }

  bool operator<(const X86Inst& other) const {
    return GetAddress() < other.GetAddress();
  }

  X86Inst& operator=(const X86Inst& other) = default;
};

class X86InstBase final : public Inst {
  zasm::Instruction instruction_;
  zasm::Node* pos_;
  bool is_br_;

 public:
  X86InstBase(const zasm::Instruction& inst, zasm::Node* pos,
              X86Function* function)
      : Inst(0, function), instruction_(inst), pos_(pos) {
    is_br_ = zasm::x86::isBranching(instruction_) &&
             instruction_.getMnemonic() != zasm::x86::Mnemonic::Ret;
  }

  const zasm::Instruction& RawInst() { return instruction_; }

  zasm::Node* GetPos() const { return pos_; }

  X86InstBase& operator=(const X86InstBase& other) = default;

  X86InstBase& operator=(const X86Inst& other) {
    instruction_ = other.RawInst().getInstruction();
    pos_ = other.GetPos();
    is_br_ = other.IsBranching();
    return *this;
  }
};
}  // namespace stitch

#endif  // STITCH_TARGET_X86_H_

```

`src/pe.cc`:

```cc
/*
 * Licensed to BadHive under one or more contributor license
 * agreements.  See the NOTICE file distributed with this work
 * for additional information regarding copyright ownership.
 * BadHive licenses this file to you under the Apache License,
 * Version 2.0 (the "License"); you may not use this file
 * except in compliance with the License.  You may obtain a
 * copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 */

#include "stitch/binary/pe.h"

#include <iostream>

#include "stitch/misc/errors.h"
#include "stitch/target/x86.h"

namespace stitch {
void PEFormat::Parse(std::fstream& stream, PEFormat& format) {
  using namespace pe;
  stream.exceptions(std::ifstream::failbit | std::ifstream::badbit);

  stream.read(reinterpret_cast<char*>(&format.dos_header),
              sizeof(format.dos_header));
  if (format.dos_header.e_magic != IMAGE_DOS_SIGNATURE) {
    throw invalid_binary_format_error();
  }

  const auto curr = stream.tellg();
  const auto stub_size = format.dos_header.e_lfanew - curr;
  format.dos_stub.resize(stub_size);
  stream.read(format.dos_stub.data(), stub_size);

  // check signature
  stream.seekg(format.dos_header.e_lfanew);
  stream.read(reinterpret_cast<char*>(&format.nt_headers32.Signature),
              sizeof(format.nt_headers32.Signature));
  if (format.nt_headers32.Signature != IMAGE_NT_SIGNATURE) {
    throw invalid_binary_format_error();
  }

  // read file header and optional header magic
  stream.read(reinterpret_cast<char*>(&format.nt_headers32.FileHeader),
              sizeof(format.nt_headers32.FileHeader));

  switch (format.nt_headers32.FileHeader.Machine) {
    case IMAGE_FILE_MACHINE_AMD64:
      format.architecture = TargetArchitecture::AMD64;
      break;
    case IMAGE_FILE_MACHINE_I386:
      format.architecture = TargetArchitecture::I386;
      break;
    case IMAGE_FILE_MACHINE_ARM64:
      format.architecture = TargetArchitecture::ARM64;
      break;
    default:
      break;
  }

  stream.read(
      reinterpret_cast<char*>(&format.nt_headers32.OptionalHeader.Magic),
      sizeof(format.nt_headers32.OptionalHeader.Magic));

  // read the rest of the optional header depending on architecture
  if (format.Is64Bit()) {
    stream.read(reinterpret_cast<char*>(
                    &format.nt_headers64.OptionalHeader.MajorLinkerVersion),
                sizeof(format.nt_headers64.OptionalHeader) -
                    sizeof(format.nt_headers64.OptionalHeader.Magic));
  } else if (format.Is32Bit()) {
    stream.read(reinterpret_cast<char*>(
                    &format.nt_headers32.OptionalHeader.MajorLinkerVersion),
                sizeof(format.nt_headers32.OptionalHeader) -
                    sizeof(format.nt_headers32.OptionalHeader.Magic));
  } else {
    throw invalid_binary_format_error();
  }

  format.size_of_raw_headers = stream.tellg();
  for (WORD i = 0; i < format.nt_headers32.FileHeader.NumberOfSections; ++i) {
    // sections come right after NT header
    SectionHeader section_header = {};
    stream.read(reinterpret_cast<char*>(&section_header),
                sizeof(section_header));
    const auto nextHeader = stream.tellg();

    std::vector<uint8_t> section_data(section_header.SizeOfRawData);

    stream.seekg(section_header.PointerToRawData);
    stream.read(reinterpret_cast<char*>(section_data.data()),
                section_header.SizeOfRawData);

    format.sections.emplace_back(section_header, section_data);
    stream.seekg(nextHeader);
  }

  const NtDataDirectory* cert_table = nullptr;
  if (format.Is32Bit())
    cert_table = &format.nt_headers32.OptionalHeader
                      .DataDirectory[IMAGE_DIRECTORY_ENTRY_SECURITY];
  else if (format.Is64Bit())
    cert_table = &format.nt_headers64.OptionalHeader
                      .DataDirectory[IMAGE_DIRECTORY_ENTRY_SECURITY];
  if (cert_table && cert_table->VirtualAddress) {
    stream.seekg(cert_table->VirtualAddress);
    stream.read(format.cert_table.data(), cert_table->Size);
  }
}

bool PEFormat::Is64Bit() const {
  return nt_headers32.OptionalHeader.Magic == pe::IMAGE_NT_OPTIONAL_HDR64_MAGIC;
}

bool PEFormat::Is32Bit() const {
  return nt_headers32.OptionalHeader.Magic == pe::IMAGE_NT_OPTIONAL_HDR32_MAGIC;
}

pe::DWORD PEFormat::Entrypoint() const {
  return Is32Bit() ? nt_headers32.OptionalHeader.AddressOfEntryPoint
                   : nt_headers64.OptionalHeader.AddressOfEntryPoint;
}

intptr_t PEFormat::ImageBase() const {
  return Is32Bit() ? nt_headers32.OptionalHeader.ImageBase
                   : nt_headers64.OptionalHeader.ImageBase;
}

pe::DWORD PEFormat::FileAlignment() const {
  return Is32Bit() ? nt_headers32.OptionalHeader.FileAlignment
                   : nt_headers64.OptionalHeader.FileAlignment;
}

pe::DWORD PEFormat::SectionAlignment() const {
  return Is32Bit() ? nt_headers32.OptionalHeader.SectionAlignment
                   : nt_headers64.OptionalHeader.SectionAlignment;
}

pe::DWORD& PEFormat::SizeOfHeaders() {
  return Is64Bit() ? nt_headers64.OptionalHeader.SizeOfHeaders
                   : nt_headers32.OptionalHeader.SizeOfHeaders;
}

pe::DWORD& PEFormat::SizeOfImage() {
  return Is64Bit() ? nt_headers64.OptionalHeader.SizeOfImage
                   : nt_headers32.OptionalHeader.SizeOfImage;
}

pe::DWORD& PEFormat::SizeOfInitializedData() {
  return Is64Bit() ? nt_headers64.OptionalHeader.SizeOfInitializedData
                   : nt_headers32.OptionalHeader.SizeOfInitializedData;
}

pe::DWORD& PEFormat::SizeOfUninitializedData() {
  return Is64Bit() ? nt_headers64.OptionalHeader.SizeOfUninitializedData
                   : nt_headers32.OptionalHeader.SizeOfUninitializedData;
}

pe::DWORD& PEFormat::SizeOfCode() {
  return Is64Bit() ? nt_headers64.OptionalHeader.SizeOfCode
                   : nt_headers32.OptionalHeader.SizeOfCode;
}

pe::NtDataDirectory& PEFormat::DataDirectory(const char id) {
  return Is64Bit() ? nt_headers64.OptionalHeader.DataDirectory[id]
                   : nt_headers32.OptionalHeader.DataDirectory[id];
}

const PESectionInfo& PEFormat::GetSectionInfo(const std::string& name) {
  for (auto& scn : sections) {
    if (name == scn.header.Name) return scn;
  }
  throw section_not_found_error(name);
}

unsigned PESection::Characteristics() const {
  return si_.header.Characteristics;
}

void PESection::SetCharacteristics(const unsigned ch) {
  si_.header.Characteristics = ch;
}

void PESection::Write(const std::vector<uint8_t>& data) {
  std::vector<uint8_t>& m_data = getData();

  growRaw(static_cast<int64_t>(m_data.size()),
          static_cast<int64_t>(data.size()));
  growVirtual(static_cast<int64_t>(m_data.size()),
              static_cast<int64_t>(data.size()));

  m_data.insert(m_data.end(), data.begin(), data.end());
}

void PESection::growRaw(const int64_t old, const int64_t amount) const {
  if (auto* pe = GetParent<PE>()) {
    const int64_t free_space =
        si_.header.SizeOfRawData - static_cast<int64_t>(old);
    if (free_space < old + amount) {
      pe->growSectionRawSize(GetName(), amount - free_space);
    }
  }
}

void PESection::growVirtual(const int64_t old, const int64_t amount) const {
  if (auto* pe = GetParent<PE>()) {
    pe->growSectionVirtualSize(GetName(), old, amount);
  }
}

void PE::parse() {
  using namespace pe;
  if (!open_ || parsed_) return;
  PEFormat::Parse(file_stream_, file_mapping_);
  for (const auto& si : file_mapping_.sections) {
    SectionType type = (si.header.Characteristics & code_flags) == code_flags
                           ? SectionType::Code
                       : (si.header.Characteristics & data_flags) == data_flags
                           ? SectionType::Data
                       : (si.header.Characteristics & bss_flags) == bss_flags
                           ? SectionType::BSS
                           : SectionType::ROData;
    if (sections_.size() >= kMaxPESections)
      throw section_error("max number of sections has been reached");
    auto scn = std::make_unique<PESection>(si, type, si.data, this, true);
    sections_.push_back(std::move(scn));
  }
  const TargetArchitecture arch = file_mapping_.architecture;
  if (arch == TargetArchitecture::I386 || arch == TargetArchitecture::AMD64) {
    setCode(std::make_unique<X86Code>(this, arch));
  }
  bit_size_ = arch == TargetArchitecture::I386 ? 32 : 64;
  bit_size_ == 32 ? parseImports<ImageThunkData32>()
                  : parseImports<ImageThunkData64>();
  parseExports();
  parseRelocations();
  parsed_ = true;
}

void* PE::getContentAt(const RVA rva) const {
  using namespace pe;
  if (rva == 0) return nullptr;
  for (const auto& section : sections_) {
    const uint64_t scn_size = section->GetSize();
    if (rva >= section->GetAddress() &&
        rva < section->GetAddress() + scn_size) {
      const uint64_t disp = rva - section->GetAddress();
      return const_cast<unsigned char*>(section->GetData().data() + disp);
    }
  }
  return nullptr;
}

void PE::parseRelocations() {
  using namespace pe;
  const NtDataDirectory& reloc_dir =
      file_mapping_.DataDirectory(IMAGE_DIRECTORY_ENTRY_BASERELOC);
  if (reloc_dir.Size == 0) return;
  void* data = getContentAt(reloc_dir.VirtualAddress);
  if (data == nullptr) return;
  FullBaseRelocation full_reloc;
  auto* base_reloc = static_cast<BaseRelocation*>(data);
  // make sure we're still parsing within .reloc
  while (base_reloc->VirtualAddress != 0 && base_reloc->SizeOfBlock != 0) {
    full_reloc.base = *base_reloc;
    auto* entry = reinterpret_cast<BaseRelocationEntry*>(base_reloc + 1);
    // parse until entry points to end of block
    while (entry !=
           reinterpret_cast<BaseRelocationEntry*>(
               reinterpret_cast<char*>(base_reloc) + base_reloc->SizeOfBlock)) {
      full_reloc.entries.push_back(*entry);
      entry++;
    }
    file_mapping_.relocations.push_back(std::move(full_reloc));
    base_reloc = reinterpret_cast<BaseRelocation*>(entry);
  }
}

template <typename T = pe::ImageThunkData64>
void PE::parseImports() {
  using namespace pe;
  const NtDataDirectory& import_dir =
      file_mapping_.DataDirectory(IMAGE_DIRECTORY_ENTRY_IMPORT);
  if (import_dir.Size == 0) return;
  void* data = getContentAt(import_dir.VirtualAddress);
  if (data == nullptr) return;

  const auto* import_descriptor = static_cast<ImageImportDescriptor*>(data);
  for (; import_descriptor->Characteristics; import_descriptor++) {
    const auto* module_name =
        static_cast<char*>(getContentAt(import_descriptor->Name));
    if (!module_name) continue;
    auto* thunk_data =
        static_cast<T*>(getContentAt(import_descriptor->FirstThunk));
    if (!thunk_data) return;
    VA idx = import_descriptor->FirstThunk;
    for (; thunk_data->AddressOfData; ++thunk_data, idx += bit_size_ / 8) {
      const VA import_entry_addr = idx + GetImageBase();
      ImportInfo& import_info = file_mapping_.imports[import_entry_addr];
      import_info.module = module_name;
      // this address is dereferenced by external function calls
      import_info.iat_entry = import_entry_addr;
      // check with IMAGE_ORDINAL_FLAGXX (bitwise op resolves to either 64- or
      // 32-bit flag)
      const bool is_ordinal = (thunk_data->Ordinal & 1 << (bit_size_ - 1)) != 0;
      if (!is_ordinal) {
        const auto* name = static_cast<ImageImportByName*>(
            getContentAt(thunk_data->AddressOfData));
        import_info.name = name->Name;
      }
    }
  }
}

void PE::parseExports() {
  using namespace pe;
  const NtDataDirectory& export_dir =
      file_mapping_.DataDirectory(IMAGE_DIRECTORY_ENTRY_EXPORT);
  if (export_dir.Size == 0) return;
  void* data = getContentAt(export_dir.VirtualAddress);
  if (data == nullptr) return;

  file_mapping_.export_info = *static_cast<ExportDataDirectory*>(data);
  file_mapping_.exports.reserve(file_mapping_.export_info.NumberOfFunctions);
  for (DWORD i = 0; i < file_mapping_.export_info.NumberOfFunctions; i++) {
    void* export_entry = getContentAt(
        file_mapping_.export_info.AddressOfFunctions + i * sizeof(DWORD));
    if (export_entry)
      file_mapping_.exports.emplace_back(*static_cast<DWORD*>(export_entry));
  }
}

VA PE::GetImageBase() const { return file_mapping_.ImageBase(); }

VA PE::GetEntrypoint() const {
  return GetImageBase() + file_mapping_.Entrypoint();
}

const uint8_t* PE::ReadDataAt(const VA address) const {
  return static_cast<const uint8_t*>(getContentAt(address - GetImageBase()));
}

Section* PE::OpenSection(const std::string& name) const {
  for (const auto& scn : sections_) {
    if (scn->GetName() == name) return scn.get();
  }
  throw section_not_found_error(name);
}

Section* PE::OpenSectionAt(const VA address) const {
  for (const auto& scn : sections_) {
    const auto lb = GetImageBase() + scn->GetAddress();
    const auto ub = lb + scn->GetSize();
    if (address >= lb && address < ub) return scn.get();
  }
  return nullptr;
}

Section* PE::AddSection(const std::string& name, const SectionType type) {
  using namespace pe;
  if (name.length() > 7) throw invalid_section_name_error();
  for (const auto& scn : sections_) {
    if (scn->GetName() == name) {
      throw section_exists_error();
    }
  }
  PESectionInfo si{};
  name.copy(si.header.Name, sizeof(si.header.Name) - 1);
  if (type == SectionType::Code) {
    si.header.Characteristics =
        IMAGE_SCN_CNT_CODE | IMAGE_SCN_MEM_READ | IMAGE_SCN_MEM_EXECUTE;
  } else if (type == SectionType::Data) {
    si.header.Characteristics = IMAGE_SCN_CNT_INITIALIZED_DATA |
                                IMAGE_SCN_MEM_READ | IMAGE_SCN_MEM_WRITE;
  } else if (type == SectionType::ROData) {
    si.header.Characteristics =
        IMAGE_SCN_CNT_INITIALIZED_DATA | IMAGE_SCN_MEM_READ;
  } else if (type == SectionType::BSS) {
    throw section_error("multiple bss-like sections are not supported");
  }
  addSectionHeader();
  si.header.VirtualAddress = getNewSectionRVA();
  si.header.PointerToRawData = getNewSectionRawPointer();
  file_mapping_.nt_headers32.FileHeader.NumberOfSections++;
  auto scn =
      std::make_unique<PESection>(si, type, std::vector<uint8_t>{}, this);

  sections_.push_back(std::move(scn));
  return sections_.back().get();
}

/// PointerToRawData for each header field must be updated to fit the new
/// space within the header. This field must be a multiple of FileAlignment
/// as per the docs. Align this and update each PointerToRawData accordingly.
/// https://learn.microsoft.com/en-us/windows/win32/api/winnt/ns-winnt-image_section_header
void PE::addSectionHeader() {
  using namespace pe;
  constexpr unsigned size = sizeof(SectionHeader);
  const DWORD file_alignment = file_mapping_.FileAlignment();
  // we need to know the UNALIGNED raw header size (not including section
  // headers) to check if the sections' raw addresses need updating. We won't
  // know when raw size exceeds current SizeOfHeaders otherwise.
  const DWORD total_header_size = file_mapping_.size_of_raw_headers;

  const DWORD size_old_sec_headers = sections_.size() * size;
  const DWORD size_old_sec_headers_align = utils::RoundToBoundary(
      size_old_sec_headers + total_header_size, file_alignment);

  const DWORD size_new_sec_headers = (sections_.size() + 1) * size;
  const DWORD size_new_sec_headers_align = utils::RoundToBoundary(
      size_new_sec_headers + total_header_size, file_alignment);

  const DWORD new_offset =
      size_new_sec_headers_align - size_old_sec_headers_align;
  if (new_offset != 0) {
    for (const auto& section : sections_) {
      section->GetSectionInfo().header.PointerToRawData += new_offset;
    }
  }
  if (NtDataDirectory* ct = getCertTable()) {
    ct->VirtualAddress += new_offset;
  }
  const uint64_t old_v_size_headers = utils::RoundToBoundary(
      file_mapping_.SizeOfHeaders(), file_mapping_.SectionAlignment());

  file_mapping_.SizeOfHeaders() = file_mapping_.SizeOfHeaders() -
                                  size_old_sec_headers_align +
                                  size_new_sec_headers_align;

  const DWORD v_size_headers = utils::RoundToBoundary(
      file_mapping_.SizeOfHeaders(), file_mapping_.SectionAlignment());
  file_mapping_.SizeOfImage() =
      file_mapping_.SizeOfImage() - old_v_size_headers + v_size_headers;
}

RVA PE::getNewSectionRVA() {
  RVA largest_rva = 0;
  pe::DWORD largest_vsize = 0;
  for (const auto& section : sections_) {
    const PESectionInfo& si = section->GetSectionInfo();
    if (si.header.VirtualAddress > largest_rva) {
      largest_rva = si.header.VirtualAddress;
      largest_vsize = si.header.Misc.VirtualSize;
    }
  }
  const RVA section_alignment = file_mapping_.SectionAlignment();
  const RVA size_of_headers = file_mapping_.SizeOfHeaders();

  // if no sections present
  if (largest_rva == 0)
    return utils::RoundToBoundary(size_of_headers, section_alignment);

  // new section comes right after section loaded at the highest memory address
  return utils::RoundToBoundary(largest_rva + largest_vsize, section_alignment);
}

RVA PE::getNewSectionRawPointer() {
  RVA largest_offset = 0;
  pe::DWORD largest_size = 0;
  for (const auto& section : sections_) {
    const PESectionInfo& si = section->GetSectionInfo();
    if (si.header.PointerToRawData > largest_offset) {
      largest_offset = si.header.PointerToRawData;
      largest_size = si.header.SizeOfRawData;
    }
  }
  const RVA file_alignment = file_mapping_.FileAlignment();
  const RVA size_of_headers = file_mapping_.SizeOfHeaders();

  if (largest_offset == 0) {
    return utils::RoundToBoundary(size_of_headers, file_alignment);
  }
  if (largest_size == 0) {
    return largest_offset;  // always aligned
  }
  return utils::RoundToBoundary(largest_offset + largest_size, file_alignment);
}

// Adjusts the raw data pointer sections following the resized section.
void PE::growSectionRawSize(const std::string& section_name,
                            const int64_t amount) {
  // Round up growth size to FileAlignment
  const int64_t new_amount = utils::RoundToBoundary(
      amount, static_cast<int64_t>(file_mapping_.FileAlignment()));
  if (new_amount == 0) return;
  bool resize = false;
  SectionType ty = {};
  for (const auto& section : sections_) {
    if (section->GetName() == section_name) {
      section->GetSectionInfo().header.SizeOfRawData += new_amount;
      ty = section->GetType();
      resize = true;
      continue;
    }
    PESectionInfo& si = section->GetSectionInfo();
    // only adjust pointers for sections after adjusted section
    if (resize && si.header.PointerToRawData > 0) {
      si.header.PointerToRawData += new_amount;
    }
  }
  if (ty == SectionType::Code) {
    file_mapping_.SizeOfCode() += amount;
  } else if (ty == SectionType::Data || ty == SectionType::ROData) {
    file_mapping_.SizeOfInitializedData() += amount;
  }
  // adjust cert table position if present
  if (pe::NtDataDirectory* cert_table = getCertTable())
    cert_table->VirtualAddress += new_amount;
}

void PE::growSectionVirtualSize(const std::string& section_name,
                                const int64_t old, const int64_t amount) {
  int pos = 0;
  bool found = false;
  const int64_t new_size = old + amount;
  PESection* s = nullptr;
  for (const auto& section : sections_) {
    pos++;
    if (section->GetName() == section_name) {
      s = section.get();
      found = true;
      break;
    }
  }
  if (!found) return;

  // round new section size to SectionAlignment and use for SizeOfImage.
  // if section didn't grow up to SectionAlignment, SizeOfImage doesn't change
  const int64_t old_v_size = utils::RoundToBoundary(
      old, static_cast<uint64_t>(file_mapping_.SectionAlignment()));

  const int64_t new_v_size = utils::RoundToBoundary(
      new_size, static_cast<uint64_t>(file_mapping_.SectionAlignment()));
  file_mapping_.SizeOfImage() =
      file_mapping_.SizeOfImage() - old_v_size + new_v_size;

  // update virtual addresses for following sections
  if (new_v_size > old_v_size) {
    for (auto i = sections_.begin() + pos; i != sections_.end(); ++i) {
      (*i)->Relocate(new_v_size - old_v_size);
    }
  }
  s->GetSectionInfo().header.Misc.VirtualSize = new_size;
  if (s->GetType() == SectionType::BSS) {
    file_mapping_.SizeOfUninitializedData() += amount;
  }
}

pe::NtDataDirectory* PE::getCertTable() {
  using namespace stitch::pe;
  if (file_mapping_.cert_table.empty()) return nullptr;
  NtDataDirectory* cert_table =
      file_mapping_.Is32Bit()
          ? &file_mapping_.nt_headers32.OptionalHeader
                 .DataDirectory[IMAGE_DIRECTORY_ENTRY_SECURITY]
          : &file_mapping_.nt_headers64.OptionalHeader
                 .DataDirectory[IMAGE_DIRECTORY_ENTRY_SECURITY];
  return cert_table;
}

void PE::Save() {
  if (!open_ || !parsed_) return;
  file_stream_.close();
  // clear, as we are rebuilding the PE
  file_stream_ =
      std::fstream(file_name_, std::ios::in | std::ios::out | std::ios::trunc |
                                   std::ios::binary);
  std::vector<char> pe_file;
  rebuild(pe_file);
  file_stream_.write(pe_file.data(), static_cast<uint32_t>(pe_file.size()));
}

void PE::SaveAs(const std::string& file_name) {
  std::vector<char> pe_file;
  rebuild(pe_file);

  std::ofstream ofs(file_name, std::ios::binary);
  ofs.write(pe_file.data(), static_cast<uint32_t>(pe_file.size()));
  ofs.close();
}

void PE::rebuild(std::vector<char>& data) {
  using namespace stitch::pe;
  const auto p_dos_header = reinterpret_cast<char*>(&file_mapping_.dos_header);
  const auto p_nt_headers =
      reinterpret_cast<char*>(&file_mapping_.nt_headers32);
  // DOS header
  data.insert(data.end(), p_dos_header, p_dos_header + sizeof(DosHeader));
  // DOS stub
  data.insert(data.end(), file_mapping_.dos_stub.begin(),
              file_mapping_.dos_stub.end());
  // NT headers
  file_mapping_.SizeOfImage() = utils::RoundToBoundary(
      file_mapping_.SizeOfImage(), file_mapping_.SectionAlignment());

  DWORD& cbHeaders = file_mapping_.SizeOfHeaders();
  cbHeaders = utils::RoundToBoundary(cbHeaders, file_mapping_.FileAlignment());

  if (file_mapping_.Is32Bit()) {
    data.insert(data.end(), p_nt_headers, p_nt_headers + sizeof(NtHeaders32));
  } else if (file_mapping_.Is64Bit()) {
    data.insert(data.end(), p_nt_headers, p_nt_headers + sizeof(NtHeaders64));
  }
  // binary size is by default the end of the last section
  DWORD binary_size = 0;
  for (const auto& section : sections_) {
    PESectionInfo& si = section->GetSectionInfo();
    if (si.header.SizeOfRawData == 0) {
      if (section->GetType() != SectionType::BSS)
        throw section_error("section " + section->GetName() + " is empty");
      // don't allow useless bss sections
      if (si.header.Misc.VirtualSize == 0)
        throw section_error("section " + section->GetName() + " is empty");
    }
    const DWORD section_end =
        si.header.PointerToRawData + si.header.SizeOfRawData;
    if (section_end > binary_size) {
      binary_size = section_end;
    }
    const auto p_section_header = reinterpret_cast<char*>(&si.header);
    data.insert(data.end(), p_section_header,
                p_section_header + sizeof(SectionHeader));
  }
  // ... unless certificate table is present
  const NtDataDirectory* cert_table = getCertTable();
  if (cert_table) {
    binary_size = cert_table->VirtualAddress + cert_table->Size;
  }
  // grow to fit entire binary, in case raw pointers don't increase sequentially
  data.resize(binary_size);
  // section data
  for (const auto& section : sections_) {
    const PESectionInfo& si = section->GetSectionInfo();
    if (si.header.PointerToRawData)
      // insert raw section data in
      std::ranges::copy(section->GetData(),
                        data.begin() + si.header.PointerToRawData);
  }
  if (cert_table) {
    std::ranges::copy(file_mapping_.cert_table,
                      data.begin() + cert_table->VirtualAddress);
  }
}
}  // namespace stitch
```

`src/shellcode.cc`:

```cc
/*
 * Licensed to BadHive under one or more contributor license
 * agreements.  See the NOTICE file distributed with this work
 * for additional information regarding copyright ownership.
 * BadHive licenses this file to you under the Apache License,
 * Version 2.0 (the "License"); you may not use this file
 * except in compliance with the License.  You may obtain a
 * copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 */

#include "stitch/binary/shellcode.h"

#include <memory>

#include "stitch/target/x86.h"

namespace stitch {
void Shellcode::parse() {
  if (parsed_) return;
  switch (architecture_) {
    case TargetArchitecture::I386:
    case TargetArchitecture::AMD64:
      setCode(std::make_unique<X86Code>(this, architecture_));
      break;
    default:
      throw code_error("invalid architecture");
  }

  file_stream_.seekg(0, std::ios::end);
  const std::streamsize size = file_stream_.tellg();
  file_stream_.seekg(0, std::ios::beg);

  std::vector<uint8_t> data(size);
  if (!file_stream_.read(reinterpret_cast<char*>(data.data()), size)) {
    throw binary_error("could not read shellcode file");
  }
  old_section_ = std::make_unique<SCSection>(0, data, this, true);
  new_section_ = std::make_unique<SCSection>(
      data.size(), std::vector<uint8_t>{}, this, false);
  parsed_ = true;
}

const uint8_t* Shellcode::ReadDataAt(const VA address) const {
  const Section* scn = OpenSectionAt(address);
  if (!scn) return nullptr;
  return scn->GetData().data() + (address - GetImageBase());
}

Section* Shellcode::AddSection(const std::string& name, SectionType type) {
  return new_section_.get();
}

Section* Shellcode::OpenSection(const std::string& name) const {
  return new_section_.get();
}

Section* Shellcode::OpenSectionAt(const VA address) const {
  if (address >= 0 && address < old_section_->GetSize())
    return old_section_.get();
  if (address >= new_section_->GetAddress() &&
      address < new_section_->GetAddress() + new_section_->GetSize())
    return new_section_.get();
  return nullptr;
}

void Shellcode::Save() {
  if (!open_ || !parsed_) return;
  file_stream_.close();
  file_stream_ =
      std::fstream(file_name_, std::ios::in | std::ios::out | std::ios::trunc |
                                   std::ios::binary);
  file_stream_.write(
      reinterpret_cast<const char*>(old_section_->GetData().data()),
      static_cast<uint32_t>(old_section_->GetSize()));
  file_stream_.write(
      reinterpret_cast<const char*>(new_section_->GetData().data()),
      static_cast<uint32_t>(new_section_->GetSize()));
}

void Shellcode::SaveAs(const std::string& file_name) {
  std::ofstream ofs(file_name, std::ios::binary);
  ofs.write(reinterpret_cast<const char*>(old_section_->GetData().data()),
            static_cast<uint32_t>(old_section_->GetSize()));
  ofs.write(reinterpret_cast<const char*>(new_section_->GetData().data()),
            static_cast<uint32_t>(new_section_->GetSize()));
  ofs.close();
}
}  // namespace stitch
```

`src/x86.cc`:

```cc
/*
 * Licensed to BadHive under one or more contributor license
 * agreements.  See the NOTICE file distributed with this work
 * for additional information regarding copyright ownership.
 * BadHive licenses this file to you under the Apache License,
 * Version 2.0 (the "License"); you may not use this file
 * except in compliance with the License.  You may obtain a
 * copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 */

#include "stitch/target/x86.h"

#include <iostream>
#include <map>
#include <queue>
#include <set>

namespace stitch {
constexpr VA INVALID_ADDRESS = -1;

/*
 * Called only once. Performs the following analyses:
 * - Disassembly
 * - Control flow analysis
 * - Liveness analysis
 * - Tail call analysis
 */
void X86Code::AnalyzeFrom(const VA address) {
  if (analyzed_) return;
  analyzeFunction(address);
  analyzeTailCalls();
  analyzed_ = true;
}

X86Function* X86Code::analyzeFunction(const VA address) {
  int reopen_idx = -1;
  for (int i = 0; i < functions_.size(); i++) {
    X86Function* fn = functions_[i].get();
    if (fn->GetAddress() == address) {
      if (!fn->finished_) return fn;
      reopen_idx = i;
      break;
    }
  }
  Section* scn = GetParent()->OpenSectionAt(address);
  if (!scn) throw code_error("address out of range");
  X86Function* fn = buildFunction(address, scn, reopen_idx);
  fn->setOldSection(scn);
  return fn;
}

void X86Code::analyzeTailCalls() {
  // returns true if the jump was to the start of a known function
  auto check_jmp_to_fn = [&](const VA address) -> bool {
    for (const auto& fn : functions_) {
      if (fn->GetAddress() == address) return true;
    }
    return false;
  };
  // returns basic block that was the jump destination
  auto get_local_jmp_dst = [&](const X86Function* fn,
                               const VA address) -> X86BasicBlock* {
    for (const auto& bb : fn->getBasicBlocks()) {
      if (bb->GetAddress() == address && fn->GetAddress() != address)
        return bb.get();
    }
    return nullptr;
  };

  std::queue<X86Function*> worklist;
  for (const auto& fn : functions_) worklist.push(fn.get());
  while (!worklist.empty()) {
    X86Function* fn = worklist.front();
    worklist.pop();
    std::set<VA> tail_callers;
    for (auto& bb : fn->getBasicBlocks()) {
      // analyse BB if terminated due to unconditional jump
      if (bb->GetTermReason() == X86BlockTermReason::Jmp) {
        const auto inst = fn->getBlockInstructions(bb.get()).back();
        const auto dst = inst->RawInst().getOperandIf<zasm::Imm>(0);
        if (!dst) continue;
        const VA jmp_dst = dst->value<VA>();
        // in case it's a tail call to the current function (odd) ignore
        if (jmp_dst == fn->GetAddress()) continue;
        // check if jump goes to start of another function
        if (check_jmp_to_fn(jmp_dst) && inst->GetStackOffset() == 0) {
          tail_callers.insert(bb->GetAddress());
          bb->SetTermReason(X86BlockTermReason::TailCall);
        }
        /* if jump is (seemingly) internal, check stack offsets. multiple
         * functions tail calling to the same function will have their own
         * copies of the destination CFG. Get rid of those and create a single
         * function object instead
         *
         * this should also handle tail calls to functions with only 1 call site
         */
        else if (const auto dst_block = get_local_jmp_dst(fn, jmp_dst)) {
          const auto dst_inst = fn->getBlockInstructions(dst_block).front();
          if (inst->GetStackOffset() == 0 && dst_inst->GetStackOffset() == 0) {
            tail_callers.insert(bb->GetAddress());
            bb->SetTermReason(X86BlockTermReason::TailCall);
            worklist.push(analyzeFunction(jmp_dst));
          }
        }
      }
    }
    for (const auto tail_caller : tail_callers)
      fn->removeBasicBlocksAfter(fn->GetBasicBlockAt(tail_caller));
  }
}

X86Function* X86Code::editFunction(const VA address, const std::string& in) {
  X86Function* fn = analyzeFunction(address);
  std::string new_scn_name = in;
  if (in.empty()) new_scn_name = ".stitch";
  Binary* bin = GetParent();
  Section* new_scn = nullptr;
  try {
    new_scn = bin->OpenSection(new_scn_name);
  } catch (const std::exception& _) {
    new_scn = bin->AddSection(new_scn_name, SectionType::Code);
  }
  fn->setNewSection(new_scn);
  return fn;
}

Function* X86Code::CreateFunction(const std::string& in) {
  Section* new_scn = nullptr;
  std::string new_scn_name = in;
  if (in.empty()) new_scn_name = ".stitch";
  auto* bin = GetParent();

  try {
    new_scn = bin->OpenSection(new_scn_name);
  } catch (const std::exception& _) {
    new_scn = bin->AddSection(new_scn_name, SectionType::Code);
  }

  auto fn =
      std::make_unique<X86Function>(INVALID_ADDRESS, zasm::Program(mm_), this);
  fn->setNewSection(new_scn);
  fn->finalize();
  functions_.push_back(std::move(fn));
  return functions_.back().get();
}

Function* X86Code::CreateFunction(const Section& new_scn) {
  return CreateFunction(new_scn.GetName());
}

Function* X86Code::EditFunction(const VA address, const std::string& in) {
  X86Function* fn = editFunction(address, in);
  fn->finalize();
  return fn;
}

Function* X86Code::EditFunction(const VA address, const Section& scn) {
  return EditFunction(address, scn.GetName());
}

Function* X86Code::RebuildFunction(const VA address, const std::string& in) {
  X86Function* fn = editFunction(address, in);
  return fn;
}

Function* X86Code::RebuildFunction(const VA address, const Section& scn) {
  X86Function* fn = editFunction(address, scn.GetName());
  return fn;
}

void X86Code::patchOriginalLocation(const X86Function& fn,
                                    const VA new_loc) const {
  if (fn.GetAddress() == INVALID_ADDRESS) return;
  const Binary* binary = GetParent();
  const VA image_base = binary->GetImageBase();
  Section* scn = fn.getOldSection();
  if (!scn) return;

  for (const auto& block : fn.getBasicBlocks()) {
    const VA block_addr = block->GetAddress();
    const int64_t block_size = block->GetSize();
    // basic block address relative to the section's start address
    const RVA block_rel_addr = block_addr - scn->GetAddress() - image_base;
    // double check that basic block fits inside section
    if (block_addr - image_base + block_size >
        scn->GetAddress() + scn->GetSize())
      throw code_error("basic block outside of section range");
    scn->Memset(block_rel_addr, 0xcc, block_size);  // patch with int3
  }

  zasm::Program program(fn.getMachineMode());
  zasm::x86::Assembler as(program);
  patch_policy_(as, fn.GetAddress(), new_loc);
  zasm::Serializer serializer;
  const zasm::Error err = serializer.serialize(program, fn.GetAddress());

  if (err.getCode() != zasm::ErrorCode::None)
    throw code_error(std::string("failed to move function: ") +
                     err.getErrorMessage());

  // make sure that the patch code fits within the first basic block so that we
  // aren't overwriting code of another function
  const X86BasicBlock* first_block = fn.GetBasicBlockAt(fn.GetAddress());
  if (serializer.getCodeSize() > first_block->GetSize())
    throw code_error("patch stub too large");
  // now replace first basic block's address with the patch
  const RVA block_rel_addr =
      first_block->GetAddress() - scn->GetAddress() - image_base;
  scn->WriteAt(block_rel_addr, serializer.getCode(), serializer.getCodeSize());
}

X86Function* X86Code::buildFunction(const VA fn_address, const Section* scn,
                                    const int reopen_idx) {
  std::set<VA> visited_insts;
  X86Function* fn = nullptr;
  auto uf = std::make_unique<X86Function>(fn_address, zasm::Program(mm_), this);
  if (reopen_idx != -1) {
    functions_[reopen_idx] = std::move(uf);
    fn = functions_[reopen_idx].get();
  } else {
    functions_.emplace_back(std::move(uf));
    fn = functions_.back().get();
  }
  zasm::Decoder decoder(mm_);
  const RVA build_offset =
      fn_address - GetParent()->GetImageBase() - scn->GetAddress();
  fn->disassemble(decoder, scn->GetData().data(), scn->GetSize(), fn_address,
                  build_offset, fn->visited_insts_);
  fn->runAnalyses();
  const X86Inst& last_inst = fn->instructions_.back();
  fn->setSize(last_inst.GetAddress() + last_inst.RawInst().getLength() -
              fn->GetAddress());
  return fn;
}

zasm::MachineMode X86Function::getMachineMode() const {
  return GetParent()->GetArchitecture() == TargetArchitecture::I386
             ? zasm::MachineMode::I386
             : zasm::MachineMode::AMD64;
}

void X86Function::disassemble(zasm::Decoder& decoder, const uint8_t* code,
                              const size_t code_size, VA runtime_address,
                              VA offset, std::set<VA>& visited_insts) {
  while (offset < code_size) {
    // don't disassemble twice
    if (visited_insts.contains(runtime_address)) break;
    auto result =
        decoder.decode(code + offset, code_size - offset, runtime_address);
    if (!result) {
      setError(result.error().getErrorMessage());
      return;
    }
    const auto& inst = result.value();
    const uint8_t inst_length = inst.getLength();
    visited_insts.insert(runtime_address);

    X86Inst& x86inst =
        instructions_.emplace_back(decoder.getMode(), inst, this);
    x86inst.setAddress(runtime_address);

    // move 'cursor' forward
    offset += inst_length;
    runtime_address += inst_length;
    // any branching instruction other than call terminates a basic block
    if (zasm::x86::isBranching(inst)) {
      if (inst.getCategory() == zasm::x86::Category::Ret) break;
      if (inst.getCategory() == zasm::x86::Category::Call) {
        const auto* address = inst.getOperandIf<zasm::Imm>(0);
        // recursive disassembly, won't reanalyze if already in database
        if (address != nullptr)
          GetParent<X86Code>()->analyzeFunction(address->value<VA>());
        continue;
      }
      int64_t cf_dst = 0;
      try {
        cf_dst = inst.getOperand<zasm::Imm>(0).value<int64_t>();
      } catch (const std::exception& _) {
        // continue on conditional branch but exit on unconditional.
        // still need to follow basic control flow rules to disassemble
        // everything.
        if (inst.getCategory() == zasm::x86::Category::CondBr) continue;
        break;
      }
      const int64_t jump_distance = cf_dst - runtime_address;
      disassemble(decoder, code, code_size, cf_dst, offset + jump_distance,
                  visited_insts);
      if (inst.getMnemonic() == zasm::x86::Mnemonic::Jmp) {
        break;
      }
    }
  }
}

X86BasicBlock* X86Function::analyzeControlFlow(
    std::vector<X86Inst>::iterator curr, std::set<VA>& analyzed_insts,
    X86BasicBlock* parent_block) {
  X86BasicBlock* basic_block = nullptr;
  for (; curr != instructions_.end(); ++curr) {
    const auto& inst = curr->RawInst();
    const VA runtime_address = curr->GetAddress();

    if (analyzed_insts.contains(runtime_address)) {
      /*
       * This will either:
       * 1. Add a new parent for the basic block that we've reached, or
       * 2. If we jumped to the middle of an already-created basic block, then
       * we split it at that point, and set the new block's parents to where we
       *    jumped from (parent_block) and the block that used to own the
       * instruction at that address
       */
      findAndSplitBasicBlock(runtime_address, parent_block);
      break;
    }
    if (basic_block == nullptr) {
      basic_block = addBasicBlock(runtime_address, 0, parent_block);
    }

    curr->setBasicBlock(basic_block);
    const uint8_t inst_length = inst.getLength();
    analyzed_insts.insert(runtime_address);

    basic_block->SetSize(basic_block->GetSize() + inst_length);
    // any branching instruction other than call terminates a basic block
    if (zasm::x86::isBranching(inst)) {
      if (inst.getCategory() == zasm::x86::Category::Call) continue;
      if (inst.getCategory() == zasm::x86::Category::Ret) {
        basic_block->SetTermReason(X86BlockTermReason::Ret);
        exit_blocks_.insert(basic_block);
        break;
      }
      int64_t cf_dst = 0;
      try {
        cf_dst = inst.getOperand<zasm::Imm>(0).value<int64_t>();
      } catch (const std::exception& _) {
        // continue on conditional branch but exit on unconditional.
        // we don't need to create a child block as we don't know the other
        // branch location.
        // add as exit block; we can't recover CF following this block
        if (inst.getCategory() == zasm::x86::Category::CondBr) continue;
        basic_block->SetTermReason(X86BlockTermReason::Jmp);
        exit_blocks_.insert(basic_block);
        break;
      }
      const auto next_idx = getInstructionAtAddress(cf_dst);
      // shouldn't happen since we haven't done any function-level analysis at
      // this point
      if (next_idx < 0)
        throw code_error("got jump to non-existent instruction");
      const auto next = instructions_.begin() + next_idx;

      analyzeControlFlow(next, analyzed_insts, basic_block);

      // unconditional jump terminates a BB
      if (inst.getMnemonic() == zasm::x86::Mnemonic::Jmp) {
        basic_block->SetTermReason(X86BlockTermReason::Jmp);
        break;
      }
      basic_block->SetTermReason(X86BlockTermReason::CondBr);
      const auto inst_len = curr->RawInst().getLength();
      X86BasicBlock* old_block = basic_block;
      basic_block =
          addBasicBlock(runtime_address + inst_len, 0, old_block, old_block);
    }
  }
  if (!error_.empty() && basic_block != nullptr)
    basic_block->SetTermReason(X86BlockTermReason::Error);
  return basic_block;
}

void X86Function::findAndSplitBasicBlock(const VA address,
                                         X86BasicBlock* new_parent) {
  for (const auto& block : basic_blocks_) {
    // if we fall at the start of the basic block then no need to split,
    // just add our own block as a parent
    const VA block_addr = block->GetAddress();
    if (address == block_addr && new_parent) {
      block->AddParent(new_parent);
      new_parent->AddChild(block.get());
      return;
    }
    // if address is within basic block, then split it
    if (address > block_addr && address < block_addr + block->GetSize()) {
      X86BasicBlock* new_block = splitAfter(block.get(), address);
      if (new_parent) {
        new_block->AddParent(new_parent);
        new_parent->AddChild(new_block);
      }
      // if old block was an exit block, new block will become an exit block
      for (auto it = exit_blocks_.begin(); it != exit_blocks_.end(); ++it) {
        if (block_addr == (*it)->GetAddress()) {
          exit_blocks_.erase(it);
          exit_blocks_.insert(new_block);
          return;
        }
      }
      return;
    }
  }
}

X86BasicBlock* X86Function::splitAfter(X86BasicBlock* block, const VA address) {
  std::vector<X86Inst*> insts;
  // new block is child of old block
  X86BasicBlock* new_block = addBasicBlock(address, 0, block, block);
  // new block is effectively transferred ownership of old block's children
  auto& block_children = block->getChildren();
  for (auto it = block_children.begin(); it != block_children.end();) {
    const auto child = *it;
    if (child == new_block) {
      ++it;
      continue;
    }
    child->getParents().erase(block);
    child->AddParent(new_block);
    it = block_children.erase(it);
  }
  new_block->SetTermReason(block->GetTermReason());
  block->SetTermReason(X86BlockTermReason::Fallthrough);
  // move insts that are within the old block lower range to the new block
  for (const auto inst : getBlockInstructions(block)) {
    if (inst->GetAddress() >= address) {
      inst->setBasicBlock(new_block);
      const auto inst_size = inst->RawInst().getLength();
      new_block->SetSize(new_block->GetSize() + inst_size);
      block->SetSize(block->GetSize() - inst_size);
    }
  }
  return new_block;
}

// remove basic blocks after specified block if it is a tail call
void X86Function::removeBasicBlocksAfter(const X86BasicBlock* final_block) {
  if (!final_block) return;

  std::queue<X86BasicBlock*> to_delete;
  std::set<VA> seen;

  for (auto child : final_block->GetChildren()) to_delete.push(child);

  while (!to_delete.empty()) {
    auto block = to_delete.front();
    to_delete.pop();

    if (!seen.insert(block->GetAddress()).second) continue;

    // queue children for deletion
    for (auto child : block->GetChildren()) {
      to_delete.push(child);
    }
    // erase reference to block from parents
    for (const auto parent : block->getParents())
      parent->getChildren().erase(block);

    // erase instructions associated with block
    auto insts = GetBlockInstructions(block);
    for (auto inst : insts) {
      std::erase_if(instructions_, [inst](auto& i) {
        return i.GetAddress() == inst->GetAddress();
      });
    }
  }
  // erase blocks that we have visited
  for (auto block_address : seen) {
    std::erase_if(basic_blocks_, [block_address](auto& b) {
      return b->GetAddress() == block_address;
    });
  }
}

X86BasicBlock* X86Function::addBasicBlock(VA loc, uint64_t size,
                                          X86BasicBlock* parent,
                                          X86BasicBlock* fallthrough) {
  const auto bb = basic_blocks_
                      .emplace_back(std::make_unique<X86BasicBlock>(
                          loc, size, parent, fallthrough))
                      .get();
  if (parent) parent->AddChild(bb);
  return bb;
}

std::vector<X86Inst*> X86Function::getBlockInstructions(
    const X86BasicBlock* block) {
  std::vector<X86Inst*> insts;
  for (X86Inst& inst : instructions_) {
    // skip over instructions that haven't gone through control flow analysis
    // yet
    if (const auto* bb = inst.GetBasicBlock();
        bb && bb->GetAddress() == block->GetAddress()) {
      insts.push_back(&inst);
    }
  }
  return insts;
}

std::vector<const X86Inst*> X86Function::getBlockInstructions(
    const X86BasicBlock* block) const {
  std::vector<const X86Inst*> insts;
  for (const X86Inst& inst : instructions_) {
    if (const auto* bb = inst.GetBasicBlock();
        bb && bb->GetAddress() == block->GetAddress()) {
      insts.push_back(&inst);
    }
  }
  std::sort(insts.begin(), insts.end());
  return insts;
}

// Refs:
// https://en.wikipedia.org/wiki/Live-variable_analysis
// https://github.com/thesecretclub/riscy-business/blob/zasm-obfuscator/obfuscator/src/obfuscator/analyze.cpp#L186
void X86Function::genBlockLivenessInfo() {
  // 1: create GEN and KILL sets for each BB
  for (const auto& block : basic_blocks_) {
    for (const X86Inst* inst : getBlockInstructions(block.get())) {
      // kills regs that are written to before being read
      block->regs_gen_ |= inst->regs_read_ & ~block->regs_kill_;
      block->regs_kill_ |= inst->regs_written_;
      // kills flags that are modified before being tested
      block->flags_gen_ =
          block->flags_gen_ | (inst->flags_tested_ & ~block->flags_kill_);
      block->flags_kill_ = block->flags_kill_ | inst->flags_modified_;
    }
  }
  // 2: create block LIVEin and LIVEout sets through backwards iteration
  std::queue<X86BasicBlock*> blocks;
  std::set<RVA> visited_blocks;
  for (X86BasicBlock* block : exit_blocks_) {
    blocks.push(block);
    visited_blocks.insert(block->GetAddress());
  }
  while (!blocks.empty()) {
    auto* block = blocks.front();
    blocks.pop();
    visited_blocks.insert(block->GetAddress());
    // solve LIVEin and LIVEout equations
    block->regs_live_in_ =
        block->regs_gen_ | (block->regs_live_out_ & ~block->regs_kill_);
    block->flags_live_in_ =
        block->flags_gen_ | (block->flags_live_out_ & ~block->flags_kill_);

    for (X86BasicBlock* parent : block->GetParents()) {
      const auto old_regs_live_out = parent->regs_live_out_;
      const auto old_flags_live_out = parent->flags_live_out_;

      parent->regs_live_out_ |= block->regs_live_in_;
      parent->flags_live_out_ = parent->flags_live_out_ | block->flags_live_in_;

      // recompute parent LIVEin if its LIVEout as changed
      if (old_regs_live_out != parent->regs_live_out_ ||
          old_flags_live_out != parent->flags_live_out_)
        blocks.push(parent);
    }
  }
}

// Compute liveness for each individual instruction
void X86Function::genInstructionLivenessInfo() {
  for (const auto& block : basic_blocks_) {
    std::vector<X86Inst*> insts = getBlockInstructions(block.get());
    uint32_t regs_live = block->regs_live_out_;
    uint32_t flags_live = block->flags_live_out_;
    for (auto it = insts.rbegin(); it != insts.rend(); ++it) {
      X86Inst* inst = *it;
      /*
       * if var has been read from, it's considered live.
       * if var has been written to without being read from, it's now considered
       * dead.
       */
      regs_live |= inst->regs_read_;
      inst->regs_live_ = regs_live;

      const uint32_t regs_overwritten = inst->regs_written_ & ~inst->regs_read_;
      regs_live &= ~regs_overwritten;

      flags_live |= inst->flags_tested_;
      inst->flags_live_ = flags_live;

      const uint32_t flags_overwritten =
          inst->flags_modified_ & ~inst->flags_tested_;
      flags_live &= ~flags_overwritten;
    }
  }
}

bool X86Function::isWithinFunction(const VA address) const {
  bool within = false;
  for (const auto& block : basic_blocks_) {
    if (address >= block->GetAddress() &&
        address < block->GetAddress() + block->GetSize()) {
      within = true;
      break;
    }
  }
  return within;
}

X86BasicBlock* X86Function::GetBasicBlockAt(const VA address) const {
  for (const auto& block : basic_blocks_) {
    if (block->GetAddress() == address) {
      return block.get();
    }
  }
  return nullptr;
}

int X86Function::getInstructionAtAddress(const VA address) const {
  for (int i = 0; i < instructions_.size(); i++) {
    if (instructions_[i].GetAddress() == address) {
      return i;
    }
  }
  return -1;
}

void X86Function::genStackInfo(const uint64_t initial_sp) {
  using namespace utils;

  std::map<int8_t, sym::Reg> reg_map = {
      {zasm::x86::rsp.getIndex(), sym::Reg("sp", initial_sp)},  // initialised
      {zasm::x86::rbp.getIndex(), sym::Reg("bp")},
      {zasm::x86::rdi.getIndex(), sym::Reg("di")},
      {zasm::x86::rsi.getIndex(), sym::Reg("si")},
      {zasm::x86::rax.getIndex(), sym::Reg("ax")},
      {zasm::x86::rbx.getIndex(), sym::Reg("bx")},
      {zasm::x86::rcx.getIndex(), sym::Reg("cx")},
      {zasm::x86::rdx.getIndex(), sym::Reg("dx")},
      {zasm::x86::r8.getIndex(), sym::Reg("8")},
      {zasm::x86::r9.getIndex(), sym::Reg("9")},
      {zasm::x86::r10.getIndex(), sym::Reg("10")},
      {zasm::x86::r11.getIndex(), sym::Reg("11")},
      {zasm::x86::r12.getIndex(), sym::Reg("12")},
      {zasm::x86::r13.getIndex(), sym::Reg("13")},
      {zasm::x86::r14.getIndex(), sym::Reg("14")},
      {zasm::x86::r15.getIndex(), sym::Reg("15")}};

  // initialise volatile regs to zero
  if (GetParent()->GetParent()->GetPlatform() == Platform::Windows) {
    for (auto reg : x86::gpVolWin64) reg_map.at(reg.getIndex()) = 0;
  }

  std::set<VA> visited_insts;
  // start analysis from the entry; we haven't detected tail calls yet and
  // genStackOffsets follows the control flow so it works out
  const auto entry_inst =
      instructions_.begin() + getInstructionAtAddress(GetAddress());
  genStackOffsets(entry_inst, reg_map, visited_insts);
}

/// Sets a stack offset property for each instruction in the function.
/// This only handles common instructions that may modify the stack pointer.
void X86Function::genStackOffsets(std::vector<X86Inst>::iterator it,
                                  std::map<int8_t, utils::sym::Reg>& reg_map,
                                  std::set<VA>& visited_insts) {
  using namespace utils;

  for (; it != instructions_.end(); ++it) {
    X86Inst& inst = *it;
    if (visited_insts.contains(inst.GetAddress())) return;
    visited_insts.insert(inst.GetAddress());
    const zasm::InstructionDetail& ri = inst.RawInst();
    sym::Reg& rsp = reg_map.at(zasm::x86::rsp.getIndex());

    if (it == instructions_.begin()) inst.setStackOffset(0);

    // ret
    if (ri.getCategory() == zasm::x86::Category::Ret) return;

    if (ri.getOperandCount() == 0) {
      if (it + 1 != instructions_.end() && rsp.Defined())
        (it + 1)->setStackOffset(rsp);
      continue;
    }
    const zasm::Operand& op0 = ri.getOperand(0);

    // jmp
    if (ri.getCategory() == zasm::x86::Category::UncondBR) {
      const auto jmp_dst = op0.getIf<zasm::Imm>();
      if (jmp_dst) {
        const int inst_pos = getInstructionAtAddress(jmp_dst->value<VA>());
        if (inst_pos != -1) {
          const auto inst_it = instructions_.begin() + inst_pos;
          if (rsp.Defined()) inst_it->setStackOffset(rsp);
          auto reg_map_br = reg_map;
          genStackOffsets(inst_it, reg_map_br, visited_insts);
        }
      }
      return;
    }
    // jcc
    if (ri.getCategory() == zasm::x86::Category::CondBr) {
      const auto jmp_dst = op0.getIf<zasm::Imm>();
      if (jmp_dst) {
        const int inst_pos = getInstructionAtAddress(jmp_dst->value<VA>());
        if (inst_pos != -1) {
          const auto inst_it = instructions_.begin() + inst_pos;
          if (rsp.Defined()) inst_it->setStackOffset(rsp);
          auto reg_map_br = reg_map;
          genStackOffsets(inst_it, reg_map_br, visited_insts);
        }
      }
      // don't return, still need to set next instr's stack offset
    }

    try {
      switch (ri.getMnemonic()) {
        case zasm::x86::Mnemonic::Sub: {
          if (const auto reg_op0 = op0.getIf<zasm::x86::Reg>()) {
            sym::Reg& sym = reg_map.at(reg_op0->getIndex());
            if (const auto reg = ri.getOperandIf<zasm::x86::Reg>(1)) {
              sym = sym - reg_map.at(reg->getIndex());
            } else if (const auto imm = ri.getOperandIf<zasm::Imm>(1)) {
              sym = sym - imm->value<uint64_t>();
            } else
              sym.Undefine();
          }
        } break;
        case zasm::x86::Mnemonic::Add: {
          if (const auto reg_op0 = op0.getIf<zasm::x86::Reg>()) {
            sym::Reg& sym = reg_map.at(reg_op0->getIndex());
            if (const auto reg = ri.getOperandIf<zasm::x86::Reg>(1))
              sym = sym + reg_map.at(reg->getIndex());
            else if (const auto imm = ri.getOperandIf<zasm::Imm>(1))
              sym = sym + imm->value<uint64_t>();
            else
              sym.Undefine();
          }
        } break;
        case zasm::x86::Mnemonic::And: {
          const auto r_op0 = op0.getIf<zasm::x86::Reg>();
          if (!r_op0) break;
          sym::Reg& sym = reg_map.at(r_op0->getIndex());
          uint64_t mask = ~0;
          if (r_op0->isGp8())
            mask = 0xff;
          else if (r_op0->isGp16())
            mask = 0xffff;
          // set upper FULLSIZE - N bits if N != FULLSIZE to retain them
          if (const auto reg = ri.getOperandIf<zasm::x86::Reg>(1))
            sym = sym & (reg_map.at(reg->getIndex()) | ~mask);
          else if (const auto imm = ri.getOperandIf<zasm::Imm>(1))
            sym = sym & (imm->value<uint64_t>() | ~mask);
          else
            sym.Undefine();
        } break;
        case zasm::x86::Mnemonic::Or: {
          const auto r_op0 = op0.getIf<zasm::x86::Reg>();
          if (!r_op0) break;
          sym::Reg& sym = reg_map.at(r_op0->getIndex());
          uint64_t mask = ~0;
          if (r_op0->isGp8())
            mask = 0xff;
          else if (r_op0->isGp16())
            mask = 0xffff;
          // only OR with lower N bits of operand if not full size
          if (const auto reg = ri.getOperandIf<zasm::x86::Reg>(1))
            sym = sym | (reg_map.at(reg->getIndex()) & mask);
          else if (const auto imm = ri.getOperandIf<zasm::Imm>(1))
            sym = sym | (imm->value<uint64_t>() & mask);
          else
            sym.Undefine();
        } break;
        case zasm::x86::Mnemonic::Xor: {
          const auto r_op0 = op0.getIf<zasm::x86::Reg>();
          if (!r_op0) break;
          sym::Reg& sym = reg_map.at(r_op0->getIndex());
          uint64_t mask = ~0;
          if (r_op0->isGp8())
            mask = 0xff;
          else if (r_op0->isGp16())
            mask = 0xffff;
          // only XOR with lower N bits of operand if not full size
          if (const auto reg = ri.getOperandIf<zasm::x86::Reg>(1))
            sym = sym ^ (reg_map.at(reg->getIndex()) & mask);
          else if (const auto imm = ri.getOperandIf<zasm::Imm>(1))
            sym = sym ^ (imm->value<uint64_t>() & mask);
          else
            sym.Undefine();
        } break;
        case zasm::x86::Mnemonic::Lea: {
          if (const auto reg_op0 = op0.getIf<zasm::x86::Reg>()) {
            sym::Reg& sym = reg_map.at(reg_op0->getIndex());
            if (const auto mem = ri.getOperandIf<zasm::x86::Mem>(1)) {
              const int8_t reg_idx = mem->getBase().getIndex();
              // if base register cannot be fetched (i.e. is the ip) we break
              if (reg_idx < 0) break;
              const sym::Reg& base = reg_map.at(reg_idx);
              const int8_t index = mem->getIndex().getIndex();
              const uint8_t scale = mem->getScale();
              const int64_t disp = mem->getDisplacement();
              sym = base;
              if (index != -1)
                sym = sym + (reg_map.at(index) * static_cast<uint64_t>(scale));
              sym = sym + static_cast<uint64_t>(disp);
            } else
              sym.Undefine();
          }
        } break;
        case zasm::x86::Mnemonic::Xchg: {
          if (const auto reg_op0 = op0.getIf<zasm::x86::Reg>()) {
            sym::Reg& sym = reg_map.at(reg_op0->getIndex());
            if (const auto reg = ri.getOperandIf<zasm::x86::Reg>(1)) {
              const sym::Reg saved_sym = sym;
              sym = reg_map.at(reg->getIndex());
              reg_map.at(reg->getIndex()) = saved_sym;
            }
          }
        } break;
        case zasm::x86::Mnemonic::Mov: {
          if (const auto reg_op0 = op0.getIf<zasm::x86::Reg>()) {
            sym::Reg& sym = reg_map.at(reg_op0->getIndex());
            if (const auto reg = ri.getOperandIf<zasm::x86::Reg>(1))
              sym = reg_map.at(reg->getIndex());
            else if (const auto imm = ri.getOperandIf<zasm::Imm>(1))
              sym = imm->value<uint64_t>();
            else
              sym.Undefine();
          }
        } break;
          // subtract
        case zasm::x86::Mnemonic::Push: {
          sym::Reg& sym = reg_map.at(zasm::x86::rsp.getIndex());
          const uint64_t val =
              getMachineMode() == zasm::MachineMode::I386 ? 4 : 8;
          sym = sym - val;
        } break;
          // add
        case zasm::x86::Mnemonic::Pop: {
          sym::Reg& sym = reg_map.at(zasm::x86::rsp.getIndex());
          const uint64_t val =
              getMachineMode() == zasm::MachineMode::I386 ? 4 : 8;
          sym = sym + val;
        } break;
        default:;
      }
    } catch (const std::out_of_range& _) {
      continue;
    }
    // each instruction stores stack offset before it is run
    // technically sp should always be defined unless silliness is involved
    if (it + 1 != instructions_.end() && rsp.Defined())
      (it + 1)->setStackOffset(rsp);
  }
}

void X86Function::refreshCode() {
  new_instructions_.clear();
  for (zasm::Node* node = program_.getHead(); node != nullptr;
       node = node->getNext()) {
    if (const auto* inst = node->getIf<zasm::Instruction>()) {
      new_instructions_.emplace_back(*inst, node, this);
    }
  }
}

uint64_t X86Function::ImportJumpTable(const VA address,
                                      const uint32_t num_entries,
                                      X86BasicBlock* dispatcher_block,
                                      const bool le) {
  constexpr bool is_le = std::endian::native == std::endian::little;

  auto flip_int = [le](auto v) -> int64_t {
    constexpr size_t size = sizeof(v);
    if (is_le != le) {
      int64_t out = 0;
      const auto* src = reinterpret_cast<uint8_t*>(&v);
      auto* dst = reinterpret_cast<uint8_t*>(&out);
      for (size_t i = 0; i < size; ++i) dst[i] = src[size - 1 - i];
      return out;
    } else {
      return static_cast<int64_t>(v);
    }
  };

  const uint8_t* jt = GetParent<X86Code>()->GetParent()->ReadDataAt(address);
  std::vector<int64_t> addresses;
  zasm::Decoder decoder(getMachineMode());

  // Disassemble and run analyses on registered handlers. This can be done
  // even after the initial function analysis has been performed since we
  // end up calling finalize() again
  auto analyze_handler = [&](const int64_t hnd_address) {
    const Binary* bin = GetParent()->GetParent();
    const Section* scn = bin->OpenSectionAt(hnd_address);
    if (!scn) {
      throw code_error("invalid handler address");
    }
    const VA offset = hnd_address - bin->GetImageBase() - scn->GetAddress();
    disassemble(decoder, scn->GetData().data(), scn->GetSize(), hnd_address,
                offset, visited_insts_);

    // run analyses manually
    const int inst_pos = getInstructionAtAddress(hnd_address);
    runAnalyses(inst_pos, dispatcher_block);
  };

  if (getMachineMode() == zasm::MachineMode::I386) {
    const auto* ptr = reinterpret_cast<const int32_t*>(jt);
    for (uint32_t i = 0; i < num_entries; ++i) {
      addresses.push_back(flip_int(ptr[i]));
    }
  } else {  // amd64
    const auto* ptr = reinterpret_cast<const int64_t*>(jt);
    for (uint32_t i = 0; i < num_entries; ++i) {
      addresses.push_back(flip_int(ptr[i]));
    }
  }

  for (uint32_t i = 0; i < num_entries; ++i) {
    analyze_handler(addresses[i]);
  }
  finalize();

  const auto id = CreateJumpTable();
  // Mark handlers if found
  for (const auto addr : addresses) {
    X86BasicBlock* bb = GetBasicBlockAt(addr);
    if (!bb) {
      throw code_error("no jump table handler found at " +
                       std::to_string(addr));
    }
    const auto cur = assembler_->getCursor();
    if (auto first = GetBlockInstructions(bb).front(); first) {
      assembler_->setCursor(first->GetPos()->getPrev());
      auto label = assembler_->createLabel();
      assembler_->bind(label);
      bb->block_label_ = assembler_->getCursor();
    }
    assembler_->setCursor(cur);
    MarkJumpTableHandler(id, bb);
  }
  return id;
}

uint64_t X86Function::CreateJumpTable() {
  jump_tables_handlers_[jump_tables_handlers_.size()] = {};
  return jump_tables_handlers_.size() - 1;
}

void X86Function::MarkJumpTableHandler(const uint64_t jt_id,
                                       const X86BasicBlock* bb) {
  if (!jump_tables_handlers_.contains(jt_id))
    throw code_error("jump table not found");
  jump_tables_handlers_[jt_id].push_back(bb);
}

void X86Function::finalize() {
  // can't use program object again after calling clear()
  program_.clear();
  program_ = zasm::Program(getMachineMode());
  assembler_.reset();
  assembler_.emplace(program_);

  std::map<VA, zasm::Label> labels;
  start_pos_ = assembler_->getCursor();
  // first iteration - get all relN instructions and create labels for them
  for (X86Inst& inst : instructions_) {
    // auto v = inst.RawInst().getInstruction();
    // std::cout << "" << std::hex << inst.GetAddress() << std::dec << ": "
    //           << zasm::formatter::toString(
    //                  program_, &v,
    //                  zasm::formatter::detail::FormatOptions::HexImmediates)
    //           << " ; stack offset = "
    //           << static_cast<int64_t>(inst.GetStackOffset()) << std::endl;
    const zasm::InstructionDetail& raw_inst = inst.RawInst();
    if (zasm::x86::isBranching(raw_inst) &&
        raw_inst.getCategory() != zasm::x86::Category::Ret) {
      if (const auto jmp_imm = raw_inst.getOperandIf<zasm::Imm>(0)) {
        VA jmp_addr = jmp_imm->value<VA>();
        if (!isWithinFunction(jmp_addr)) {
          assembler_->emit(raw_inst);
          inst.setPos(assembler_->getCursor());
          continue;
        }
        inst.setIsLocalBranch(true);
        // don't create more labels if one exists for that address
        zasm::Label jmp_label;
        if (labels.contains(jmp_addr))
          jmp_label = labels.at(jmp_addr);
        else
          jmp_label = assembler_->createLabel();
        labels.emplace(jmp_addr, jmp_label);
        assembler_->emit(raw_inst.getMnemonic(), jmp_label);
      } else
        assembler_->emit(raw_inst);
    } else
      assembler_->emit(raw_inst);
    inst.setPos(assembler_->getCursor());
  }
  zasm::Node* end = assembler_->getCursor();
  // second iteration - bind labels to
  for (X86Inst& inst : instructions_) {
    // if we reach instruction that is destination of a jmp label, then
    // bind assembler cursor to the label (since we are going to emit this
    // instruction next)
    if (labels.contains(inst.GetAddress())) {
      const zasm::Label label = labels[inst.GetAddress()];
      labels.erase(inst.GetAddress());
      assembler_->setCursor(inst.GetPos()->getPrev());
      assembler_->bind(label);
    }
  }
  assembler_->setCursor(end);
  const auto& first = instructions_.at(getInstructionAtAddress(GetAddress()));
  entry_point_ = first.GetPos();
  refreshCode();
}

template <typename T>
void move_element(std::vector<T>& v, const T& elem, size_t new_index) {
  // infer old index from the reference
  size_t old_index = &elem - &v[0];

  if (old_index == new_index) {
    return;
  }

  if (old_index < new_index) {
    // move forward
    std::rotate(v.begin() + old_index, v.begin() + old_index + 1,
                v.begin() + new_index + 1);
  } else {
    // move backward
    std::rotate(v.begin() + new_index, v.begin() + old_index,
                v.begin() + old_index + 1);
  }
}

// sort instructions by basic block appearances
void X86Function::smartSortInstructions() {
  std::queue<X86BasicBlock*> worklist;
  std::set<VA> visited;
  worklist.push(GetBasicBlockAt(GetAddress()));

  int bb_start = 0;
  while (!worklist.empty()) {
    const auto block = worklist.front();
    worklist.pop();

    if (visited.contains(block->GetAddress())) continue;
    visited.insert(block->GetAddress());

    int idx = 0;
    auto instructions = GetBlockInstructions(block);
    for (int i = 0; i < instructions.size(); ++i) {
      // move to the front
      move_element(instructions_, *instructions[i], bb_start + i);
      idx++;
    }
    const X86BasicBlock* found_fallthrough = nullptr;
    bb_start += idx;
    // prioritize the block that is directly after this one in code
    for (auto child : block->GetChildren()) {
      if (child->GetFallthroughParent() == block) {
        worklist.push(child);
        found_fallthrough = child;
        break;
      }
    }
    // push all other blocks after
    for (auto child : block->GetChildren()) {
      if (child != found_fallthrough) worklist.push(child);
    }
  }
}

const GlobalRef* X86Function::Finish() {
  if (finished_)
    throw std::runtime_error("function already marked as finished");
  if (!new_section_)
    throw std::runtime_error("function is not in editing mode");
  // pointer to end of section
  VA new_write_address = new_section_->GetParent()->GetImageBase() +
                         new_section_->GetAddress() + new_section_->GetSize();

  const zasm::Label entry_label = assembler_->createLabel();
  const auto* entry = GetEntryPoint();

  zasm::Node* cur = assembler_->getCursor();

  assembler_->setCursor(nullptr);
  assembler_->align(zasm::Align::Type::Code, X86Code::kFunctionAlignment);

  if (entry == nullptr) {
    // set entry to the next node after alignment
    assembler_->setCursor(program_.getHead()->getNext());
  } else {
    assembler_->setCursor(entry->getPrev());
  }
  assembler_->bind(entry_label);

  std::map<uint64_t, std::vector<zasm::Label>> labels_map;

  // create label for each handler
  for (const auto& [id, handlers] : jump_tables_handlers_) {
    auto& labels = labels_map[id];
    for (const auto handler : handlers) {
      zasm::Node* bb_head = handler->block_label_;
      auto label = program_.createLabel();
      labels.push_back(label);
      assembler_->setCursor(bb_head->getPrev());
      assembler_->bind(label);
    }
  }

  assembler_->setCursor(cur);

  zasm::Serializer serializer;
  const zasm::Error code = serializer.serialize(program_, new_write_address);
  if (code.getCode() != zasm::ErrorCode::None)
    throw code_error(code.getErrorMessage());

  // now patch with entrypoint instruction address
  const VA entry_point_offset = serializer.getLabelOffset(entry_label.getId());
  const VA write_address_final = new_write_address + entry_point_offset;

  GetParent<X86Code>()->patchOriginalLocation(*this, write_address_final);

  // serialize jump tables
  for (auto& [jt_id, labels] : labels_map) {
    if (getMachineMode() == zasm::MachineMode::I386) {
      jump_tables_.emplace(jt_id, JumpTable32());
      auto& jt32 = std::get<JumpTable32>(jump_tables_.at(jt_id));
      for (auto l : labels)
        jt32.RegisterHandler(serializer.getLabelAddress(l.getId()));
    } else {
      jump_tables_.emplace(jt_id, JumpTable64());
      auto& jt64 = std::get<JumpTable64>(jump_tables_.at(jt_id));
      for (auto l : labels)
        jt64.RegisterHandler(serializer.getLabelAddress(l.getId()));
    }
  }

  finished_ = true;
  return new_section_->WriteWithRef(serializer.getCode(),
                                    serializer.getCodeSize());
}

// add liveness info on construction of X86Inst. Refs:
// https://github.com/thesecretclub/riscy-business/blob/zasm-obfuscator/obfuscator/src/obfuscator/program.cpp#L139
void X86Inst::addInstructionContext() {
  for (size_t i = 0; i < instruction_.getOperandCount(); i++) {
    const auto& operand = instruction_.getOperand(i);
    const auto access = instruction_.getOperandAccess(i);
    if (const auto reg = operand.getIf<zasm::Reg>()) {
      if (static_cast<uint32_t>(access & zasm::Operand::Access::MaskRead)) {
        regs_read_ |= regMask(reg->getRoot(mm_));
      } else if (static_cast<uint32_t>(access &
                                       zasm::Operand::Access::MaskWrite)) {
        regs_written_ |= regMask(reg->getRoot(mm_));
      }
    } else if (const auto mem = operand.getIf<zasm::Mem>()) {
      // index and base regs get read for mem ops
      regs_read_ |= regMask(mem->getIndex().getRoot(mm_));
      regs_read_ |= regMask(mem->getBase().getRoot(mm_));
    }
  }
  const auto& flags = instruction_.getCPUFlags();
  flags_modified_ = flags.set0 | flags.set1 | flags.modified | flags.undefined;
  flags_tested_ = flags.tested;
}

void X86Inst::addInstructionSpecificContext(const TargetArchitecture arch,
                                            const Platform platform) {
  // add instruction-specific context based on calling conventions across
  // platforms and architectures
  if (instruction_.getCategory() == zasm::x86::Category::Call) {
    //
    if (arch == TargetArchitecture::I386) {
      if (platform == Platform::Windows) {
        // __fastcall, esp read by push eip
        regs_read_ |= regMask(zasm::x86::ecx) | regMask(zasm::x86::edx) |
                      regMask(zasm::x86::esp);
        // volatile and return regs are stomped
        for (const auto reg : x86::gpVolWin32) regs_written_ |= regMask(reg);
      }
      regs_written_ |= regMask(zasm::x86::eax);
    } else if (arch == TargetArchitecture::AMD64) {
      if (platform == Platform::Windows) {
        // __fastcall, rsp read by push rip
        regs_read_ |= regMask(zasm::x86::rcx) | regMask(zasm::x86::rdx) |
                      regMask(zasm::x86::r8) | regMask(zasm::x86::r9) |
                      regMask(zasm::x86::rsp);
        // volatile and return regs are stomped
        for (const auto reg : x86::gpVolWin64) regs_written_ |= regMask(reg);
      }
      regs_written_ |= regMask(zasm::x86::rax);
    }
  } else if (instruction_.getCategory() == zasm::x86::Category::Ret) {
    if (arch == TargetArchitecture::I386) {
      if (platform == Platform::Windows) {
        regs_read_ |= regMask(zasm::x86::eax) | regMask(zasm::x86::ebx) |
                      regMask(zasm::x86::esi) | regMask(zasm::x86::edi) |
                      regMask(zasm::x86::ebp) | regMask(zasm::x86::esp);
      }
    } else if (arch == TargetArchitecture::AMD64) {
      if (platform == Platform::Windows) {
        regs_read_ |= regMask(zasm::x86::rax) | regMask(zasm::x86::rbx) |
                      regMask(zasm::x86::rbp) | regMask(zasm::x86::rsp) |
                      regMask(zasm::x86::rdi) | regMask(zasm::x86::rsi) |
                      regMask(zasm::x86::r12) | regMask(zasm::x86::r13) |
                      regMask(zasm::x86::r14) | regMask(zasm::x86::r15);
      }
    }
  }
}
}  // namespace stitch

```

`unittests/CMakeLists.txt`:

```txt
if (STITCH_BUILD_TESTS)
    add_subdirectory(binary)
endif ()

```

`unittests/binary/CMakeLists.txt`:

```txt
stitch_add_test(pe_parse_test)
stitch_add_test(pe_rebuild_test)

```

`unittests/binary/pe_parse_test.cc`:

```cc
/*
 * Licensed to BadHive under one or more contributor license
 * agreements.  See the NOTICE file distributed with this work
 * for additional information regarding copyright ownership.
 * BadHive licenses this file to you under the Apache License,
 * Version 2.0 (the "License"); you may not use this file
 * except in compliance with the License.  You may obtain a
 * copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 */

#include <fstream>

#include "stitch/binary/pe.h"

using namespace stitch;

int main() {
  std::fstream fs("pe_simple.bin");
  PEFormat format{};
  PEFormat::Parse(fs, format);

  format.GetSectionInfo(".CRT");
  format.GetSectionInfo(".bss");
  format.GetSectionInfo(".data");
  format.GetSectionInfo(".idata");
  format.GetSectionInfo(".pdata");
  format.GetSectionInfo(".rdata");
  format.GetSectionInfo(".reloc");
  format.GetSectionInfo(".reloc");
  format.GetSectionInfo(".text");
  format.GetSectionInfo(".tls");
  format.GetSectionInfo(".xdata");
  format.GetSectionInfo(".xdata");

  fs.close();
}

```

`unittests/binary/pe_rebuild_test.cc`:

```cc
/*
 * Licensed to BadHive under one or more contributor license
 * agreements.  See the NOTICE file distributed with this work
 * for additional information regarding copyright ownership.
 * BadHive licenses this file to you under the Apache License,
 * Version 2.0 (the "License"); you may not use this file
 * except in compliance with the License.  You may obtain a
 * copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 */
#include <cassert>
#include <cstring>

#include "stitch/binary/pe.h"

using namespace stitch;

int main() {
  PE pe("pe_simple.bin");
  Section* rdata = pe.AddSection(".vmp1", SectionType::ROData);
  rdata->Write("Hello, world!");
  Section* text = pe.AddSection(".vmp2", SectionType::Code);
  text->Write(std::vector<uint8_t>{0xc3});

  pe.SaveAs("new_pe_test.bin");
  pe.Close();

  std::fstream fs("new_pe_test.bin");
  PEFormat format;
  PEFormat::Parse(fs, format);
  fs.close();
  std::remove("new_pe_test.bin");

  const PESectionInfo& dsi = format.GetSectionInfo(".vmp1");
  assert(strcmp(reinterpret_cast<const char*>(dsi.data.data()),
                "Hello, world!") == 0);
  assert(dsi.header.Misc.VirtualSize == sizeof("Hello, world!"));
  assert(dsi.header.Characteristics ==
         (pe::IMAGE_SCN_MEM_READ | pe::IMAGE_SCN_CNT_INITIALIZED_DATA));

  const PESectionInfo& csi = format.GetSectionInfo(".vmp2");
  assert(csi.data.front() == 0xc3);
  assert(csi.header.Characteristics ==
         (pe::IMAGE_SCN_MEM_READ | pe::IMAGE_SCN_MEM_EXECUTE |
          pe::IMAGE_SCN_CNT_CODE));
}
```