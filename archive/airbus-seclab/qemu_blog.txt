Project Path: arc_airbus-seclab_qemu_blog_q6hu_3rt

Source Tree:

```txt
arc_airbus-seclab_qemu_blog_q6hu_3rt
├── LICENSE
├── README.md
├── _config.yml
├── brk.md
├── devices.md
├── docs
│   └── assets
│       └── images
│           ├── 34A45ACD-6EBF-4C3F-93E4-DF4EE3009469.png
│           └── 6d54ba55-f68f-43d1-a020-aca2e3a9468b.png
├── exec.md
├── interrupts.md
├── machine.md
├── options.md
├── pci.md
├── pci_slave.md
├── regions.md
├── runstate.md
├── snapshot.md
├── tcg_p1.md
├── tcg_p2.md
├── tcg_p3.md
└── timers.md

```

`LICENSE`:

```
                    GNU GENERAL PUBLIC LICENSE
                       Version 2, June 1991

 Copyright (C) 1989, 1991 Free Software Foundation, Inc.,
 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
 Everyone is permitted to copy and distribute verbatim copies
 of this license document, but changing it is not allowed.

                            Preamble

  The licenses for most software are designed to take away your
freedom to share and change it.  By contrast, the GNU General Public
License is intended to guarantee your freedom to share and change free
software--to make sure the software is free for all its users.  This
General Public License applies to most of the Free Software
Foundation's software and to any other program whose authors commit to
using it.  (Some other Free Software Foundation software is covered by
the GNU Lesser General Public License instead.)  You can apply it to
your programs, too.

  When we speak of free software, we are referring to freedom, not
price.  Our General Public Licenses are designed to make sure that you
have the freedom to distribute copies of free software (and charge for
this service if you wish), that you receive source code or can get it
if you want it, that you can change the software or use pieces of it
in new free programs; and that you know you can do these things.

  To protect your rights, we need to make restrictions that forbid
anyone to deny you these rights or to ask you to surrender the rights.
These restrictions translate to certain responsibilities for you if you
distribute copies of the software, or if you modify it.

  For example, if you distribute copies of such a program, whether
gratis or for a fee, you must give the recipients all the rights that
you have.  You must make sure that they, too, receive or can get the
source code.  And you must show them these terms so they know their
rights.

  We protect your rights with two steps: (1) copyright the software, and
(2) offer you this license which gives you legal permission to copy,
distribute and/or modify the software.

  Also, for each author's protection and ours, we want to make certain
that everyone understands that there is no warranty for this free
software.  If the software is modified by someone else and passed on, we
want its recipients to know that what they have is not the original, so
that any problems introduced by others will not reflect on the original
authors' reputations.

  Finally, any free program is threatened constantly by software
patents.  We wish to avoid the danger that redistributors of a free
program will individually obtain patent licenses, in effect making the
program proprietary.  To prevent this, we have made it clear that any
patent must be licensed for everyone's free use or not licensed at all.

  The precise terms and conditions for copying, distribution and
modification follow.

                    GNU GENERAL PUBLIC LICENSE
   TERMS AND CONDITIONS FOR COPYING, DISTRIBUTION AND MODIFICATION

  0. This License applies to any program or other work which contains
a notice placed by the copyright holder saying it may be distributed
under the terms of this General Public License.  The "Program", below,
refers to any such program or work, and a "work based on the Program"
means either the Program or any derivative work under copyright law:
that is to say, a work containing the Program or a portion of it,
either verbatim or with modifications and/or translated into another
language.  (Hereinafter, translation is included without limitation in
the term "modification".)  Each licensee is addressed as "you".

Activities other than copying, distribution and modification are not
covered by this License; they are outside its scope.  The act of
running the Program is not restricted, and the output from the Program
is covered only if its contents constitute a work based on the
Program (independent of having been made by running the Program).
Whether that is true depends on what the Program does.

  1. You may copy and distribute verbatim copies of the Program's
source code as you receive it, in any medium, provided that you
conspicuously and appropriately publish on each copy an appropriate
copyright notice and disclaimer of warranty; keep intact all the
notices that refer to this License and to the absence of any warranty;
and give any other recipients of the Program a copy of this License
along with the Program.

You may charge a fee for the physical act of transferring a copy, and
you may at your option offer warranty protection in exchange for a fee.

  2. You may modify your copy or copies of the Program or any portion
of it, thus forming a work based on the Program, and copy and
distribute such modifications or work under the terms of Section 1
above, provided that you also meet all of these conditions:

    a) You must cause the modified files to carry prominent notices
    stating that you changed the files and the date of any change.

    b) You must cause any work that you distribute or publish, that in
    whole or in part contains or is derived from the Program or any
    part thereof, to be licensed as a whole at no charge to all third
    parties under the terms of this License.

    c) If the modified program normally reads commands interactively
    when run, you must cause it, when started running for such
    interactive use in the most ordinary way, to print or display an
    announcement including an appropriate copyright notice and a
    notice that there is no warranty (or else, saying that you provide
    a warranty) and that users may redistribute the program under
    these conditions, and telling the user how to view a copy of this
    License.  (Exception: if the Program itself is interactive but
    does not normally print such an announcement, your work based on
    the Program is not required to print an announcement.)

These requirements apply to the modified work as a whole.  If
identifiable sections of that work are not derived from the Program,
and can be reasonably considered independent and separate works in
themselves, then this License, and its terms, do not apply to those
sections when you distribute them as separate works.  But when you
distribute the same sections as part of a whole which is a work based
on the Program, the distribution of the whole must be on the terms of
this License, whose permissions for other licensees extend to the
entire whole, and thus to each and every part regardless of who wrote it.

Thus, it is not the intent of this section to claim rights or contest
your rights to work written entirely by you; rather, the intent is to
exercise the right to control the distribution of derivative or
collective works based on the Program.

In addition, mere aggregation of another work not based on the Program
with the Program (or with a work based on the Program) on a volume of
a storage or distribution medium does not bring the other work under
the scope of this License.

  3. You may copy and distribute the Program (or a work based on it,
under Section 2) in object code or executable form under the terms of
Sections 1 and 2 above provided that you also do one of the following:

    a) Accompany it with the complete corresponding machine-readable
    source code, which must be distributed under the terms of Sections
    1 and 2 above on a medium customarily used for software interchange; or,

    b) Accompany it with a written offer, valid for at least three
    years, to give any third party, for a charge no more than your
    cost of physically performing source distribution, a complete
    machine-readable copy of the corresponding source code, to be
    distributed under the terms of Sections 1 and 2 above on a medium
    customarily used for software interchange; or,

    c) Accompany it with the information you received as to the offer
    to distribute corresponding source code.  (This alternative is
    allowed only for noncommercial distribution and only if you
    received the program in object code or executable form with such
    an offer, in accord with Subsection b above.)

The source code for a work means the preferred form of the work for
making modifications to it.  For an executable work, complete source
code means all the source code for all modules it contains, plus any
associated interface definition files, plus the scripts used to
control compilation and installation of the executable.  However, as a
special exception, the source code distributed need not include
anything that is normally distributed (in either source or binary
form) with the major components (compiler, kernel, and so on) of the
operating system on which the executable runs, unless that component
itself accompanies the executable.

If distribution of executable or object code is made by offering
access to copy from a designated place, then offering equivalent
access to copy the source code from the same place counts as
distribution of the source code, even though third parties are not
compelled to copy the source along with the object code.

  4. You may not copy, modify, sublicense, or distribute the Program
except as expressly provided under this License.  Any attempt
otherwise to copy, modify, sublicense or distribute the Program is
void, and will automatically terminate your rights under this License.
However, parties who have received copies, or rights, from you under
this License will not have their licenses terminated so long as such
parties remain in full compliance.

  5. You are not required to accept this License, since you have not
signed it.  However, nothing else grants you permission to modify or
distribute the Program or its derivative works.  These actions are
prohibited by law if you do not accept this License.  Therefore, by
modifying or distributing the Program (or any work based on the
Program), you indicate your acceptance of this License to do so, and
all its terms and conditions for copying, distributing or modifying
the Program or works based on it.

  6. Each time you redistribute the Program (or any work based on the
Program), the recipient automatically receives a license from the
original licensor to copy, distribute or modify the Program subject to
these terms and conditions.  You may not impose any further
restrictions on the recipients' exercise of the rights granted herein.
You are not responsible for enforcing compliance by third parties to
this License.

  7. If, as a consequence of a court judgment or allegation of patent
infringement or for any other reason (not limited to patent issues),
conditions are imposed on you (whether by court order, agreement or
otherwise) that contradict the conditions of this License, they do not
excuse you from the conditions of this License.  If you cannot
distribute so as to satisfy simultaneously your obligations under this
License and any other pertinent obligations, then as a consequence you
may not distribute the Program at all.  For example, if a patent
license would not permit royalty-free redistribution of the Program by
all those who receive copies directly or indirectly through you, then
the only way you could satisfy both it and this License would be to
refrain entirely from distribution of the Program.

If any portion of this section is held invalid or unenforceable under
any particular circumstance, the balance of the section is intended to
apply and the section as a whole is intended to apply in other
circumstances.

It is not the purpose of this section to induce you to infringe any
patents or other property right claims or to contest validity of any
such claims; this section has the sole purpose of protecting the
integrity of the free software distribution system, which is
implemented by public license practices.  Many people have made
generous contributions to the wide range of software distributed
through that system in reliance on consistent application of that
system; it is up to the author/donor to decide if he or she is willing
to distribute software through any other system and a licensee cannot
impose that choice.

This section is intended to make thoroughly clear what is believed to
be a consequence of the rest of this License.

  8. If the distribution and/or use of the Program is restricted in
certain countries either by patents or by copyrighted interfaces, the
original copyright holder who places the Program under this License
may add an explicit geographical distribution limitation excluding
those countries, so that distribution is permitted only in or among
countries not thus excluded.  In such case, this License incorporates
the limitation as if written in the body of this License.

  9. The Free Software Foundation may publish revised and/or new versions
of the General Public License from time to time.  Such new versions will
be similar in spirit to the present version, but may differ in detail to
address new problems or concerns.

Each version is given a distinguishing version number.  If the Program
specifies a version number of this License which applies to it and "any
later version", you have the option of following the terms and conditions
either of that version or of any later version published by the Free
Software Foundation.  If the Program does not specify a version number of
this License, you may choose any version ever published by the Free Software
Foundation.

  10. If you wish to incorporate parts of the Program into other free
programs whose distribution conditions are different, write to the author
to ask for permission.  For software which is copyrighted by the Free
Software Foundation, write to the Free Software Foundation; we sometimes
make exceptions for this.  Our decision will be guided by the two goals
of preserving the free status of all derivatives of our free software and
of promoting the sharing and reuse of software generally.

                            NO WARRANTY

  11. BECAUSE THE PROGRAM IS LICENSED FREE OF CHARGE, THERE IS NO WARRANTY
FOR THE PROGRAM, TO THE EXTENT PERMITTED BY APPLICABLE LAW.  EXCEPT WHEN
OTHERWISE STATED IN WRITING THE COPYRIGHT HOLDERS AND/OR OTHER PARTIES
PROVIDE THE PROGRAM "AS IS" WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESSED
OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF
MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE.  THE ENTIRE RISK AS
TO THE QUALITY AND PERFORMANCE OF THE PROGRAM IS WITH YOU.  SHOULD THE
PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF ALL NECESSARY SERVICING,
REPAIR OR CORRECTION.

  12. IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING
WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MAY MODIFY AND/OR
REDISTRIBUTE THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES,
INCLUDING ANY GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING
OUT OF THE USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED
TO LOSS OF DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY
YOU OR THIRD PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER
PROGRAMS), EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE
POSSIBILITY OF SUCH DAMAGES.

                     END OF TERMS AND CONDITIONS

            How to Apply These Terms to Your New Programs

  If you develop a new program, and you want it to be of the greatest
possible use to the public, the best way to achieve this is to make it
free software which everyone can redistribute and change under these terms.

  To do so, attach the following notices to the program.  It is safest
to attach them to the start of each source file to most effectively
convey the exclusion of warranty; and each file should have at least
the "copyright" line and a pointer to where the full notice is found.

    <one line to give the program's name and a brief idea of what it does.>
    Copyright (C) <year>  <name of author>

    This program is free software; you can redistribute it and/or modify
    it under the terms of the GNU General Public License as published by
    the Free Software Foundation; either version 2 of the License, or
    (at your option) any later version.

    This program is distributed in the hope that it will be useful,
    but WITHOUT ANY WARRANTY; without even the implied warranty of
    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
    GNU General Public License for more details.

    You should have received a copy of the GNU General Public License along
    with this program; if not, write to the Free Software Foundation, Inc.,
    51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.

Also add information on how to contact you by electronic and paper mail.

If the program is interactive, make it output a short notice like this
when it starts in an interactive mode:

    Gnomovision version 69, Copyright (C) year name of author
    Gnomovision comes with ABSOLUTELY NO WARRANTY; for details type `show w'.
    This is free software, and you are welcome to redistribute it
    under certain conditions; type `show c' for details.

The hypothetical commands `show w' and `show c' should show the appropriate
parts of the General Public License.  Of course, the commands you use may
be called something other than `show w' and `show c'; they could even be
mouse-clicks or menu items--whatever suits your program.

You should also get your employer (if you work as a programmer) or your
school, if any, to sign a "copyright disclaimer" for the program, if
necessary.  Here is a sample; alter the names:

  Yoyodyne, Inc., hereby disclaims all copyright interest in the program
  `Gnomovision' (which makes passes at compilers) written by James Hacker.

  <signature of Ty Coon>, 1 April 1989
  Ty Coon, President of Vice

This General Public License does not permit incorporating your program into
proprietary programs.  If your program is a subroutine library, you may
consider it more useful to permit linking proprietary applications with the
library.  If this is what you want to do, use the GNU Lesser General
Public License instead of this License.

```

`README.md`:

```md
# Introduction

This is a series of posts about **QEMU internals**. It won't cover
everything about QEMU, but should help you understand how it works and
foremost how to hack into it for fun and profit.

We won't explain usage and other things that can be found in the
official documentation. The following topics will be addressed:

- [Creating a new machine](machine.md)
- [Controlling memory regions](regions.md)
- [Creating a new device](devices.md)
- [Interrupts controller](interrupts.md)
- [Timers](timers.md)
- [PCI controller](pci.md)
- [PCI devices](pci_slave.md)
- [Options](options.md)
- [Execution loop](exec.md)
- [Breakpoints handling](brk.md)
- [VM running states](runstate.md)
- TCG internals [part 1](tcg_p1.md), [part 2](tcg_p2.md) and [part 3](tcg_p3.md)
- [Snapshots](snapshot.md)

The official code and documentation can be found here:

- https://github.com/qemu/qemu
- https://www.qemu.org/documentation/

# Terminology

## Host and target

The host is the plaform and architecture which QEMU is running
on. Usually an x86 machine.

The target is the architecture which is emulated by QEMU. You can
choose at build time which one you want:

```
./configure --target-list=ppc-softmmu ...
```

As such, in the source code organisation you will find all supported
architectures in the `target/` directory:

```
(qemu-git) ll target
drwxrwxr-x 2 xxx xxx 4.0K alpha
drwxrwxr-x 2 xxx xxx 4.0K arm
drwxrwxr-x 2 xxx xxx 4.0K cris
drwxrwxr-x 2 xxx xxx 4.0K hppa
drwxrwxr-x 3 xxx xxx 4.0K i386
drwxrwxr-x 2 xxx xxx 4.0K lm32
drwxrwxr-x 2 xxx xxx 4.0K m68k
drwxrwxr-x 2 xxx xxx 4.0K microblaze
drwxrwxr-x 2 xxx xxx 4.0K mips
drwxrwxr-x 2 xxx xxx 4.0K moxie
drwxrwxr-x 2 xxx xxx 4.0K nios2
drwxrwxr-x 2 xxx xxx 4.0K openrisc
drwxrwxr-x 3 xxx xxx 4.0K ppc
drwxr-xr-x 3 xxx xxx 4.0K riscv
drwxrwxr-x 2 xxx xxx 4.0K s390x
drwxrwxr-x 2 xxx xxx 4.0K sh4
drwxrwxr-x 2 xxx xxx 4.0K sparc
drwxrwxr-x 2 xxx xxx 4.0K tilegx
drwxrwxr-x 2 xxx xxx 4.0K tricore
drwxrwxr-x 2 xxx xxx 4.0K unicore32
drwxrwxr-x 9 xxx xxx 4.0K xtensa
```

The `qemu-system-<target>` binaries are built into their respective `<target>-softmmu` directory:

```
(qemu-git) ls -ld *-softmmu
drwxr-xr-x  9 xxx xxx 4096 i386-softmmu
drwxrwxr-x 11 xxx xxx 4096 ppc-softmmu
drwxr-xr-x  9 xxx xxx 4096 x86_64-softmmu
```


## System and user modes

QEMU is a system emulator. It offers emulation of a lot of
architectures and can be run on a lot of architectures.

It is able to emulate a full system (cpu, devices, kernel and apps)
through the `qemu-system-<target>` command line tool. This is the mode we
will dive into.

It also provides a *userland* emulation mode through the `qemu-<target>`
command line tool.

This allows to directly run `<target>` architecture Linux binaries on
a Linux host. It mainly emulates `<target>` instructions set and
forward system calls to the host Linux kernel. The emulation is only
related to user level cpu instructions, not system ones, no device
nore low level memory handling.

We won't cover qemu user mode in this blog post series.


## Emulation, JIT and virtualization

Initially QEMU was an emulation engine, with a Just-In-Time compiler
(TCG). The TCG is here to dynamically translate `target` instruction
set architecture (ISA) to `host` ISA.

We will later see that in the context of the TCG, the `tcg-target`
becomes the architecture to which the TCG has to generate final
assembly code to run on (which is host ISA). Obvious !

There exists scenario where `target` and `host` architectures are the
same. This is typically the case in classical virtualization
environment (VMware, VirtualBox, ...) when a user wants to run Windows
on Linux for instance. The terminology is usually Host and Guest
(*target*).

Nowadays, QEMU offers virtualization through different
**accelerators**. Virtualization is considered an accelerator because
it prevents unneeded emulation of instructions when host and target
share the same architecture. Only system level (aka
*supervisor/ring0*) instructions might be emulated/intercepted.

Of course, the QEMU virtualization capabilities are tied to the host
OS and architecture. The x86 architecture offers hardware
virtualization extensions (Intel VMX/AMD SVM). But the host operating
system must allow QEMU to take benefit of them.

Under an x86-64 Linux host, we found the following accelerators:

```
$ qemu-system-x86_64 -accel ?
Possible accelerators: kvm, xen, hax, tcg
```

While on an x86-64 MacOS host:

```
$ qemu-system-x86_64 -accel ?
Possible accelerators: tcg, hax, hvf
```

The supported accelerators can be found in
[`qemu_init_vcpu()`](https://github.com/qemu/qemu/tree/v4.2.0/cpus.c#L2134):

```c
void qemu_init_vcpu(CPUState *cpu)
{
...
    if (kvm_enabled()) {
        qemu_kvm_start_vcpu(cpu);
    } else if (hax_enabled()) {
        qemu_hax_start_vcpu(cpu);
    } else if (hvf_enabled()) {
        qemu_hvf_start_vcpu(cpu);
    } else if (tcg_enabled()) {
        qemu_tcg_init_vcpu(cpu);
    } else if (whpx_enabled()) {
        qemu_whpx_start_vcpu(cpu);
    } else {
        qemu_dummy_start_vcpu(cpu);
    }
...
}
```

To make it short:

- `kvm` is the *Linux Kernel-based Virtual Machine* accelerator;
- `hvf` is the MacOS *Hypervisor.framework* accelerator;
- `hax` is the cross-platform Intel HAXM accelerator;
- `whp` is the *Windows Hypervisor Platform* accelerator.

You can take benefit of the speed of x86 hardware virtualization under
the three major operating systems. Notice that the TCG is also
considered an accelerator. We can enter a long debate about
terminology here ...


## QEMU APIs

There exists a lot of APIs in QEMU, some are obsolete and not well
documented. Reading the source code still remains your best
option. There is a good overview
[available](https://habkost.net/posts/2016/11/incomplete-list-of-qemu-apis.html).

The posts series will mainly address QOM, qdev and VMState. The QOM is
the more abstract one. While QEMU is developped in C language, the
developpers chose to implement the QEMU Object Model to provide a
framework for registering user creatable types and instantiating
objects from those types: device, machine, cpu, ... People used to
[OOP](https://en.wikipedia.org/wiki/Object-oriented_programming)
concepts will find their mark in the QOM.

We will briefly illustrate how to make use of it, but won't detail its
underlying implementation. Stay pragmatic !

The interested reader can have a look at
[include/qom/object.h](https://github.com/qemu/qemu/tree/v4.2.0/include/qom/object.h).

# Disclaimer

It shall be noted that Airbus does not commit itself on the
exhaustiveness and completeness regarding this blog post series. The
information presented here results from the author knowledge and
understandings as of [QEMU
v4.2.0](https://github.com/qemu/qemu/tree/v4.2.0).

```

`_config.yml`:

```yml
theme: jekyll-theme-minimal
title: "QEMU internals"

```

`brk.md`:

```md
# A deep dive into QEMU: Breakpoints handling

In this post we will learn how breakpoints are checked and raised
during translation, and processed inside the vCPU main execution
loop. We assume an i386 target as most readers are familiar with this
architecture.

## From target to IR breakpoints

Remember QEMU translates target instructions with
[`tb_gen_code`](https://github.com/qemu/qemu/blob/v4.2.0/accel/tcg/translate-all.c#L1664)
which calls
[`gen_intermediate_code`](https://github.com/qemu/qemu/blob/v4.2.0/target/i386/translate.c#L8615). Obviously
this function is target specific because it translates from *target
assembly* to IR:

```c
void gen_intermediate_code(CPUState *cpu, TranslationBlock *tb, int max_insns)
{
    DisasContext dc;
    translator_loop(&i386_tr_ops, &dc.base, cpu, tb, max_insns);
}

static const TranslatorOps i386_tr_ops = {
    .init_disas_context = i386_tr_init_disas_context,
    .tb_start           = i386_tr_tb_start,
    .insn_start         = i386_tr_insn_start,
    .breakpoint_check   = i386_tr_breakpoint_check,
    .translate_insn     = i386_tr_translate_insn,
    .tb_stop            = i386_tr_tb_stop,
    .disas_log          = i386_tr_disas_log,
};
```

However the
[`translator_loop`](https://github.com/qemu/qemu/blob/v4.2.0/accel/tcg/translator.c#L34)
function is generic but called with target specific [translator
operators](https://github.com/qemu/qemu/blob/v4.2.0/include/exec/translator.h#L80)
(ie. `i386_tr_ops`).

During the translation process, QEMU checks for breakpoints hit:

```c
void translator_loop(const TranslatorOps *ops, DisasContextBase *db,
                     CPUState *cpu, TranslationBlock *tb, int max_insns)
{
    while (true) {
...
    /* Pass breakpoint hits to target for further processing */
    QTAILQ_FOREACH(bp, &cpu->breakpoints, entry) {
       if (bp->pc == db->pc_next) {
          if (ops->breakpoint_check(db, cpu, bp)) {
              break;
          }
        }
    }
...
}
```

The check makes use of target specific operator
`ops->breakpoint_check` which is set to
[`i386_tr_breakpoint_check`](https://github.com/qemu/qemu/blob/v4.2.0/target/i386/translate.c#L8534):

```c
static bool i386_tr_breakpoint_check(DisasContextBase *dcbase, CPUState *cpu,
                                     const CPUBreakpoint *bp)
{
    DisasContext *dc = container_of(dcbase, DisasContext, base);
    /* If RF is set, suppress an internally generated breakpoint.  */
    int flags = dc->base.tb->flags & HF_RF_MASK ? BP_GDB : BP_ANY;
    if (bp->flags & flags) {
        gen_debug(dc, dc->base.pc_next - dc->cs_base);
        dc->base.is_jmp = DISAS_NORETURN;
        dc->base.pc_next += 1;
        return true;
    } else {
        return false;
    }
}
```

## QEMU breakpoints

There exists different [breakpoint
types](https://github.com/qemu/qemu/blob/v4.2.0/include/hw/core/cpu.h#L1032)
inside QEMU. Some are installed from inside the VM, others might be
installed through the GDB server stub when debugging a VM. In such a
situation they are of type `BP_GDB` and are never ignored even if
`EFLAGS.RF` is set.

When there is a breakpoint hit, the
[`gen_debug`](https://github.com/qemu/qemu/blob/v4.2.0/target/i386/translate.c#L2528)
function is called which in turn calls `gen_helper_debug`, a *macro
generated* debug helper
[`helper_debug`](https://github.com/qemu/qemu/blob/v4.2.0/target/i386/misc_helper.c#L610):

```c
static void gen_debug(DisasContext *s, target_ulong cur_eip)
{
    gen_update_cc_op(s);
    gen_jmp_im(cur_eip);
    gen_helper_debug(cpu_env);
    s->base.is_jmp = DISAS_NORETURN;
}

void helper_debug(CPUX86State *env)
{
    CPUState *cs = CPU(x86_env_get_cpu(env));

    cs->exception_index = EXCP_DEBUG;
    cpu_loop_exit(cs);
}
```

The expected behavior should look familiar to you now. The
`exception_index` is set to `EXCP_DEBUG` which has a special meaning
and we go back to the main cpu execution loop, where
[`cpu_handle_exception`](https://github.com/qemu/qemu/blob/v4.2.0/accel/tcg/cpu-exec.c#L481)
is called and does:

```c
static inline bool cpu_handle_exception(CPUState *cpu, int *ret)
{
...
        *ret = cpu->exception_index;
        if (*ret == EXCP_DEBUG) {
            cpu_handle_debug_exception(cpu);
        }
...
}
```

Here we are with our debug event handler
[`cpu_handler_debug_exception`](https://github.com/qemu/qemu/blob/v4.2.0/accel/tcg/cpu-exec.c#L450):

```c
static inline void cpu_handle_debug_exception(CPUState *cpu)
{
    CPUClass *cc = CPU_GET_CLASS(cpu);
    CPUWatchpoint *wp;

    if (!cpu->watchpoint_hit) {
        QTAILQ_FOREACH(wp, &cpu->watchpoints, entry) {
            wp->flags &= ~BP_WATCHPOINT_HIT;
        }
    }

    cc->debug_excp_handler(cpu);
}
```

There is a special treatment for *watchpoints* that will be explained
later. But remember the `cc` pointer initialized in
[x86_cpu_common_class_init](https://github.com/qemu/qemu/blob/v4.2.0/target/i386/cpu.c#L7089). Its
`debug_excp_handler` field points to
[`breakpoint_handler`](https://github.com/qemu/qemu/blob/v4.2.0/target/i386/bpt_helper.c#L209):

```c
void breakpoint_handler(CPUState *cs)
{
    X86CPU *cpu = X86_CPU(cs);
    CPUX86State *env = &cpu->env;
    CPUBreakpoint *bp;

    if (cs->watchpoint_hit) {
        if (cs->watchpoint_hit->flags & BP_CPU) {
            cs->watchpoint_hit = NULL;
            if (check_hw_breakpoints(env, false)) {
                raise_exception(env, EXCP01_DB);
            } else {
                cpu_loop_exit_noexc(cs);
            }
        }
    } else {
        QTAILQ_FOREACH(bp, &cs->breakpoints, entry) {
            if (bp->pc == env->eip) {
                if (bp->flags & BP_CPU) {
                    check_hw_breakpoints(env, true);
                    raise_exception(env, EXCP01_DB);
                }
                break;
            }
        }
    }
}
```

This function is quite interesting. It decides if QEMU should inject
or not, the debug exception inside the target. Let's say for instance,
that the generated breakpoint is due to GDB from a host client. In
this case, no `raise_exception` happens and we return from
`breakpoint_handler`. But return where ? Out of
`cpu_handle_debug_exception`, then `cpu_handle_exception`, then
[`cpu_exec`](https://github.com/qemu/qemu/blob/v4.2.0/accel/tcg/cpu-exec.c#L711)
and even out of
[`tcg_cpu_exec`](https://github.com/qemu/qemu/tree/v4.2.0/cpus.c#L1461)
to land back in
[`qemu_tcg_rr_cpu_thread_fn`](https://github.com/qemu/qemu/tree/v4.2.0/cpus.c#L1580):

```c
static void *qemu_tcg_rr_cpu_thread_fn(void *arg)
{

...
    r = tcg_cpu_exec(cpu);

    if (r == EXCP_DEBUG) {
        cpu_handle_guest_debug(cpu);
        break;
    }
...
```

Where QEMU deals with `EXCP_DEBUG` and calls
[`cpu_handle_guest_debug`](https://github.com/qemu/qemu/tree/v4.2.0/cpus.c#L1141) which has nothing to do anymore with low level target breakpoint handling:

```c
static void cpu_handle_guest_debug(CPUState *cpu)
{
    gdb_set_stop_cpu(cpu);
    qemu_system_debug_request();
    cpu->stopped = true;
}
```

At this stage, this is pure QEMU internals about event requests and VM
state changes. We will have a blog post on this too. What you should
keep in mind is that the QEMU
[`main_loop_should_exit`](https://github.com/qemu/qemu/tree/v4.2.0/vl.c#L1743)
function will check the debug request and all associated handlers will
be notified.


## QEMU watchpoints

The logic is pretty much the same for *execution watchpoints*. However
watchpoints can also be installed for read/write memory operations. To
that extent, the QEMU memory access path should check for possible
watchpoint hit.

This is happening in QEMU [virtual
TLB](https://github.com/qemu/qemu/tree/v4.2.0/accel/tcg/cputlb.c)
management code for the TCG execution mode. The implied function is
[`cpu_check_watchpoint`](https://github.com/qemu/qemu/tree/v4.2.0/exec.c#L2685).

As you are getting used to, we will cover this in a QEMU low level
memory management dedicated blog post :)

```

`devices.md`:

```md
# A deep dive into QEMU: adding devices

In this post, we will see how to create a simple new device. Other
posts will be dedicated to more complex devices such as PCI and
interrupt controllers.

## The QEMU device tree (abbreviated)

The QEMU monitor offers you different commands to inspect devices for a running instance:

```
$ qemu-system-ppc -M 40p -s -S -monitor stdio
QEMU 4.2.0 monitor - type 'help' for more information

(qemu) info qom-tree
/machine (40p-machine)
  /unattached (container)
    /device[15] (pc87312)
      /pc87312[0] (qemu:memory-region)
    /device[0] (604-powerpc-cpu)
    /io[0] (qemu:memory-region)
    /sysbus (System)
    /device[10] (i8257)
      /dma-page[1] (qemu:memory-region)
    /vga.mmio[0] (qemu:memory-region)
    /device[25] (pcnet)
      /pcnet-mmio[0] (qemu:memory-region)
      /pcnet.rom[0] (qemu:memory-region)
    /device[6] (isa-pit)
      /pit[0] (qemu:memory-region)
      /unnamed-gpio-in[0] (irq)
    /device[5] (isa-i8259)
      /unnamed-gpio-in[5] (irq)
      /unnamed-gpio-in[0] (irq)
      /pic[0] (qemu:memory-region)
  /raven (raven-pcihost)
    /pci-conf-idx[0] (qemu:memory-region)
    /pci-conf-data[0] (qemu:memory-region)
    /pci.0 (PCI)
    /unnamed-gpio-in[0] (irq)
    /pci-memory[0] (qemu:memory-region)
```

A lot of things there. From the machine itself, to the CPU objects,
DMA engine, PCI controller, PCI bus, system bus, network (pcnet),
timer (pit) and interrupt controllers (pic).

All of them are QEMU Objects. You can also use the `info qtree`
command to have a more detailled view.

## QEMU Monitor commands

Notice that the monitor commands are implemented through the QMP API
and are referenced as `hmp commands` in the QEMU source code. The
commands subset is specific to a target. When building a PowerPC QEMU,
have a look at `ppc-softmmu/hmp-commands-info.h`. All the available
commands are located at
[`hmp-commands-info.hx`](https://github.com/qemu/qemu/blob/v4.2.0/hmp-commands-info.hx)

They look like the following:

```c
{
.name       = "mtree",
.args_type  = "flatview:-f,dispatch_tree:-d,owner:-o",
.params     = "[-f][-d][-o]",
.help       = "show memory tree (-f: dump flat view for address spaces;"
"-d: dump dispatch tree, valid with -f only);"
"-o: dump region owners/parents",
.cmd        = hmp_info_mtree,
},
```

Where
[`hmp_info_mtree()`](https://github.com/qemu/qemu/blob/v4.2.0/monitor/misc.c#L1076)
is the handler.

## A device is a QObject

As for the machine, we need to create the right
[`TypeInfo`](https://github.com/qemu/qemu/tree/v4.2.0/include/qom/object.h#L426),
[`DeviceClass`](https://github.com/qemu/qemu/tree/v4.2.0/include/hw/qdev-core.h#L37)
and
[`DeviceState`](https://github.com/qemu/qemu/tree/v4.2.0/include/hw/qdev-core.h#L141)
init functions.

Let's implement the minimum code for the CPIOM EDC device. We don't
care about its internals, we just need to know that :
- it has several IO memory mapped registers
- it can raise an interrupt
- it is attached to the system bus

```c
static void cpiom_edc_init(Object *obj)
{
    cpiom_edc_state_t *s = CPIOM_EDC(obj);
    SysBusDevice      *d = SYS_BUS_DEVICE(obj);

    memory_region_init_io(&s->reg1, obj, &edc_reg1_ops, s,
                          CPIOM_EDC_NAME"-reg1", CPIOM_MMAP_EDC_REG_SIZE);
    memory_region_init_io(&s->reg2, obj, &edc_reg2_ops, s,
                          CPIOM_EDC_NAME"-reg2", 6*4);
    memory_region_init_io(&s->err, obj, &edc_err_ops, s,
                          CPIOM_EDC_NAME"-err", CPIOM_MMAP_PPCERR_SIZE);

    sysbus_init_mmio(d, &s->reg1);
    memory_region_add_subregion(get_system_memory(), CPIOM_MMAP_PPCERR, &s->err);

    sysbus_init_irq(d, &s->irq);
}

static void cpiom_edc_class_init(ObjectClass *klass, void *data)
{
    DeviceClass *dc = DEVICE_CLASS(klass);
    dc->desc = CPIOM_EDC_NAME;
}

static const TypeInfo cpiom_edc_info = {
    .name = CPIOM_EDC_NAME,
    .parent = TYPE_SYS_BUS_DEVICE,
    .instance_size = sizeof(cpiom_edc_state_t),
    .instance_init = cpiom_edc_init,
    .class_init = cpiom_edc_class_init,
};

static void cpiom_edc_register_types(void)
{
    type_register_static(&cpiom_edc_info);
}

type_init(cpiom_edc_register_types)
```

Look familiar now. To be QOM compliant, we need a header file for our
device that may have the following content:

```c
#define CPIOM_MMAP_EDC_REG        0x20001000
#define CPIOM_MMAP_EDC_REG_SIZE   0x1000

#define CPIOM_MMAP_PPCERR         0x20002000
#define CPIOM_MMAP_PPCERR_SIZE    0x2000

#define CPIOM_EDC_NAME  "cpiom-edc"
#define CPIOM_EDC(obj)  OBJECT_CHECK(cpiom_edc_state_t,(obj),CPIOM_EDC_NAME)

typedef struct cpiom_edc_state
{
    /*< private >*/
    SysBusDevice     parent_obj;

    /*< public >*/
    MemoryRegion     reg1, reg2, err;
    qemu_irq         irq;

} cpiom_edc_state_t;
```

Everything here is essentialy device init boilerplate. Give name,
specific type, memory addresses ranges ...

No need to explain again that you should implement the associated
`MemoryRegionOps` for each IO memory region.

One important thing is that our EDC device is a
[`SysBusDevice`](https://github.com/qemu/qemu/tree/v4.2.0/include/hw/sysbus.h#L61)
thanks to its `TypeInfo` (`.parent = TYPE_SYS_BUS_DEVICE`). We will be
able to take benefit of the SysBus API.

## Instantiating our device

Back to the machine initialization code:

```c
cpiom_t* cpiom_init_mcs(MachineState *mcs)
{
...
    cpiom_init_dev(cpiom);
}

static void cpiom_init_dev(cpiom_t *cpiom)
{
...
    create_edc(cpiom);
....
}

static void create_edc(cpiom_t *cpiom)
{
    cpiom->edc = sysbus_create_varargs(
        "cpiom-edc", CPIOM_MMAP_EDC_REG,
        qdev_get_gpio_in_named(cpiom->intc, "ITN", INT_N_IT_EDC_ERR),
        NULL);

    cpiom_edc_state_t *s = CPIOM_EDC(cpiom->edc);
    memory_region_add_subregion(cpiom->config, 0xc, &s->reg2);
}
```

We have a specific EDC device init function that does complex
things. In short, it will :
- instantiate our device thanks to the system bus generic service
- attach its IRQ line to our CPIOM interrupt controller
  (`cpiom->intc`, more on this later)

The last memory region `reg2` is attached as a subregion to the CPIOM
board `cpiom->config` memory region. This is a CPIOM specific area
where several device configuration registers are located. Some of our
EDC device registers are thus mapped at `offset 0xc` into this
region. The `config` region itself is an IO memory subregion of the
`system memory`. You can segment and overlap already existing memory
regions with new subregions.


## Creating system bus devices

From the very low level, device creation is done through
[`qdev_create()`](https://github.com/qemu/qemu/blob/v4.2.0/hw/core/qdev.c#L117)
and
[`qdev_init_nofail()`](https://github.com/qemu/qemu/blob/v4.2.0/hw/core/qdev.c#L343). See
the documentation at
[`qdev-core.h`](https://github.com/qemu/qemu/blob/v4.2.0/include/hw/qdev-core.h#L50). They
essentialy find the correct device `TypeInfo` and instantiate the
`DeviceClass` accordingly, leading to the execution of our
`cpiom_edc_class_init/cpiom_edc_init` functions.

The[`sysbus_create_varargs`](https://github.com/qemu/qemu/blob/v4.2.0/hw/core/sysbus.c#L224)
function is a wrapper around the `qdev_xxx()` functions plus automatic
IO mappings and IRQ registration:

```c
DeviceState *sysbus_create_varargs(const char *name, hwaddr addr, ...)
{
    DeviceState *dev;
    SysBusDevice *s;
    va_list va;
    qemu_irq irq;
    int n;

    dev = qdev_create(NULL, name);
    s = SYS_BUS_DEVICE(dev);
    qdev_init_nofail(dev);
    if (addr != (hwaddr)-1) {
        sysbus_mmio_map(s, 0, addr);
    }
    va_start(va, addr);
    n = 0;
    while (1) {
        irq = va_arg(va, qemu_irq);
        if (!irq) {
            break;
        }
        sysbus_connect_irq(s, n, irq);
        n++;
    }
    va_end(va);
    return dev;
}
```

### Mapping device IO memory region

The `addr` argument of `sysbus_create_varargs` is the IO memory region
address `CPIOM_MMAP_EDC_REG` allocated in `cpiom_edc_init`. Remember
we didn't directly attached it as a subregion to the `system memory`
region, but rather did:

```c
static void cpiom_edc_init(Object *obj)
{
...
    memory_region_init_io(&s->reg1, obj, &edc_reg1_ops, ...);
    sysbus_init_mmio(d, &s->reg1);
...
}
```

Every `SysBusDevice` object has an internal `mmio` array of
`QDEV_MAX_MMIO` entries. The
[`sysbus_init_mmio()`](https://github.com/qemu/qemu/blob/v4.2.0/hw/core/sysbus.c#L190)
function will install such an entry. And then `sysbus_create_varargs`
function will call
[`sysbus_mmio_map()`](https://github.com/qemu/qemu/blob/v4.2.0/hw/core/sysbus.c#L130)
which will internally register the given memory region as a subregion
of `system memory`.


### Connecting IRQ lines

The remaining arguments of the function are variable length, NULL
terminated, `qemu_irq`. We will learn more about IRQ in the post on
interrupt controller. Just assume for now that a `qemu_irq` is a GPIO
whose `out` part is connected to the `in` part of another GPIO.

And this is exactly the case for our EDC device. First during EDC device init:

```c
static void cpiom_edc_init(Object *obj)
{
...
    sysbus_init_irq(d, &s->irq);
...
}
```

The
[`sysbus_init_irq()`](https://github.com/qemu/qemu/blob/v4.2.0/hw/core/sysbus.c#L179)
function will register the `out` part of our EDC irq object. Next,
during EDC device creation (`create_edc()`) the argument given to
`sysbus_create_varargs` is:

```c
qdev_get_gpio_in_named(cpiom->intc, "ITN", INT_N_IT_EDC_ERR)
```

Which in short is the `in` part of the `qemu_irq` number
`INT_N_IT_EDC_ERR` that belongs to the `cpiom->intc` device, which
happens to be an interrupt controller (a special device with a lot of
irq lines as you may guess).

The `sysbus_create_varargs` function will
[`sysbus_connect_irq()`](https://github.com/qemu/qemu/blob/v4.2.0/hw/core/sysbus.c#L113)
this irq with our EDC device `qemu_irq out` part. The GPIO connection
code looks like :


```c
qdev_connect_gpio_out_named( DEVICE(dev),
                             SYSBUS_DEVICE_GPIO_IRQ,
			     0,
                             qdev_get_gpio_in_named(cpiom->intc, "ITN", INT_N_IT_EDC_ERR)
			   );
```

Logically, when our device wants to raise an IRQ, it will
[`qemu_set_irq(irq,1)`](https://github.com/qemu/qemu/blob/v4.2.0/hw/core/irq.c#L39)
its own `qemu_irq` object so that the connected `qemu_irq` will
receive the signal.


## Building the device

Do not forget to fix `/hw/cpiom/Makefile.objs` to add our device
object file `edc.o` to be built.

```

`exec.md`:

```md
# A deep dive into QEMU: The execution loop

This is the beginning of the second part of the blog post series. We
will go deeper into QEMU internals this time to give insights to hack
into core components. Let's look at the virtual CPU execution loop.


## The Big picture

In the very [first blog post](README.md) we explained how
accelerators were started, through
[`qemu_init_vcpu()`](https://github.com/qemu/qemu/tree/v4.2.0/cpus.c#L2134). Suppose
we run QEMU with a single threaded TCG and no hardware assisted
virtualization backend, we will end up running our virtual CPU in a
dedicated thread:

```c
static void qemu_tcg_init_vcpu(CPUState *cpu)
{
...
    qemu_thread_create(cpu->thread, thread_name,
                       qemu_tcg_rr_cpu_thread_fn,
                       cpu, QEMU_THREAD_JOINABLE);
...
}

static void *qemu_tcg_rr_cpu_thread_fn(void *arg)
{
...
    while (1) {
        while (cpu && !cpu->queued_work_first && !cpu->exit_request) {

            qemu_clock_enable(QEMU_CLOCK_VIRTUAL, ...);
            
            if (cpu_can_run(cpu)) {
                r = tcg_cpu_exec(cpu);

                if (r == EXCP_DEBUG) {
                    cpu_handle_guest_debug(cpu);
                    break;
                }
            }
            cpu = CPU_NEXT(cpu);
        }
    }
}
```

This is a very simplified view but we can see the big picture. If the
vCPU is in a *runnable* state then we execute instructions via the
TCG. We will detail how it handles asynchronous events such as
interrupts and exceptions, but we can already see there is a special
handling for `EXCP_DEBUG` in the previous code excerpt.

There is nothing architecture dependent at this level, we are still in
a generic part of the QEMU engine. The debug exception special
treament here is usually triggered by underlying architecture
dependent events (ie. *breakpoints*) and require particular attention
from QEMU to be forwarded to other subsystems such as a GDB server
stub out of the context of the VM. We will also cover breakpoints
handling in a dedicated post.

## Entering the TCG execution loop

The interesting function to start with is
[`tcg_cpu_exec`](https://github.com/qemu/qemu/tree/v4.2.0/cpus.c#L1461)
and more specifically the
[`cpu_exec`](https://github.com/qemu/qemu/blob/v4.2.0/accel/tcg/cpu-exec.c#L661)
one. We will cover (definitely) in a future blog post the internals of
the TCG engine, but for now we only give an overview of the VM
execution. Simplified, it looks like:

```c
int cpu_exec(CPUState *cpu)
{
    cc->cpu_exec_enter(cpu);

    /* prepare setjmp context for exception handling */
    sigsetjmp(cpu->jmp_env, 0);

    /* if an exception is pending, we execute it here */
    while (!cpu_handle_exception(cpu, &ret)) {
        while (!cpu_handle_interrupt(cpu, &last_tb)) {
            tb = tb_find(cpu, last_tb, tb_exit, cflags);
            cpu_loop_exec_tb(cpu, tb, &last_tb, &tb_exit);
        }
    }

    cc->cpu_exec_exit(cpu);
}
```

QEMU makes use of `setjmp/longjmp` C library feature to implement
exception handling. This allows to get out of deep and complex TCG
translation functions whenever an event has been triggered, such as a
CPU interrupt or exception. The corresponding functions to exit the
CPU execution loop are
[`cpu_loop_exit_xxx`](https://github.com/qemu/qemu/blob/v4.2.0/accel/tcg/cpu-exec-common.c#L65):

```c
void cpu_loop_exit(CPUState *cpu)
{
    /* Undo the setting in cpu_tb_exec.  */
    cpu->can_do_io = 1;
    siglongjmp(cpu->jmp_env, 1);
}
```

The vCPU thread code execution goes back to the point it called
`sigsetjmp`. Then QEMU tries to deal with the event as soon as
possible. But if there is no pending one, it executes the so-called
*Translated Blocks* (TB).


## A primer on Translated Blocks

The TCG engine is a JIT compiler, this means it dynamically translates
the target architecture instructions set to the host architecture
instruction set. For those not familiar with the concept please refer
to [this](https://en.wikipedia.org/wiki/Just-in-time_compilation) and
have a look at an introduction to the QEMU TCG engine
[here](https://wiki.qemu.org/Documentation/TCG). The translation is
done in two steps:
- from target ISA to intermediate representation (IR)
- from IR to host ISA

QEMU first tries to look for existing TBs, with
[`tb_find`](https://github.com/qemu/qemu/blob/v4.2.0/accel/tcg/cpu-exec.c#L395). If
no one exists for the current location, it generates a new one with
[`tb_gen_code`](https://github.com/qemu/qemu/blob/v4.2.0/accel/tcg/translate-all.c#L1664):

```c
static inline TranslationBlock *tb_find(CPUState *cpu,
                                        TranslationBlock *last_tb,
                                        int tb_exit, uint32_t cf_mask)
{
...
    tb = tb_lookup__cpu_state(cpu, &pc, &cs_base, &flags, cf_mask);
    if (tb == NULL) {
        tb = tb_gen_code(cpu, pc, cs_base, flags, cf_mask);
...
}
```

When a TB is available, QEMU runs it with
[`cpu_loop_exec_tb`](https://github.com/qemu/qemu/blob/v4.2.0/accel/tcg/cpu-exec.c#L611)
which in short calls
[`cpu_tb_exec`](https://github.com/qemu/qemu/blob/v4.2.0/accel/tcg/cpu-exec.c#L141)
and then
[`tcg_qemu_tb_exec`](https://github.com/qemu/qemu/blob/v4.2.0/tcg/tcg.h#L1160). At
this point the target (VM) code has been translated to host code, QEMU
can run it directly on the host CPU. If we look at the definition of
this last function:

```c
#define tcg_qemu_tb_exec(env, tb_ptr) \
    ((uintptr_t (*)(void *, void *))tcg_ctx->code_gen_prologue)(env, tb_ptr)
```

The translation buffer receiving generated opcodes is *casted* to a
function pointer and called with arguments.

In the TCG dedicated blog post, we will see the TCG strategy in detail
and present various *helpers* for system instructions, memory access
and things which can't be translated from an architecture to the
other.

## Back to events handling

When an hardware interrupt (IRQ) or exception is raised, QEMU *helps*
the vCPU redirects execution to the appropriate handler. These
mechanisms are very specific to the target architecture, consequently
hardly translatable. The answer comes from *helpers* which are tiny
wrappers written in C, built with QEMU for a target architecture and
natively callable on the host architecture directly from the
translated blocks. Again, we will cover them in details later.

For instance for the PPC target (VM), the *helpers* backend to inform
QEMU that an exception is being *raised* is located into
[excp_helper.c](https://github.com/qemu/qemu/blob/v4.2.0/target/ppc/excp_helper.c#L972):

```c
void raise_exception(CPUPPCState *env, uint32_t exception)
{
    raise_exception_err_ra(env, exception, 0, 0);
}

void raise_exception_err_ra(CPUPPCState *env, uint32_t exception,
                            uint32_t error_code, uintptr_t raddr)
{
    CPUState *cs = env_cpu(env);

    cs->exception_index = exception;
    env->error_code = error_code;
    cpu_loop_exit_restore(cs, raddr);
}
```

Notice the call to `cpu_loop_exit_restore` to get back to the main cpu
loop execution context and enter
[`cpu_handle_exception`](https://github.com/qemu/qemu/blob/v4.2.0/accel/tcg/cpu-exec.c#L464):

```c
static inline bool cpu_handle_exception(CPUState *cpu, int *ret)
{
    if (cpu->exception_index >= EXCP_INTERRUPT) {
        /* exit request from the cpu execution loop */
        *ret = cpu->exception_index;
        if (*ret == EXCP_DEBUG) {
            cpu_handle_debug_exception(cpu);
        }
        cpu->exception_index = -1;
        return true;
    } else {
...
        /* deal with exception/interrupt */
        CPUClass *cc = CPU_GET_CLASS(cpu);
        cc->do_interrupt(cpu);
...
    }
}
```

There is once again a specific handling on *debug exceptions*, but in
essence if there is a pending exception in `cpu->exception_index` it
will be managed by `cc->do_interrupt` which is architecture dependent.

The `exception_index` field can hold the real hardware exception but
is also used for meta information (QEMU debug event, halt instruction,
VMEXIT for nested virtualization on x86).

The `CPUClass` type has several function pointers to be initialized by
specific target code. For an i386 target we may find it at
[x86_cpu_common_class_init](https://github.com/qemu/qemu/blob/v4.2.0/target/i386/cpu.c#L7040):

```c
static void x86_cpu_common_class_init(ObjectClass *oc, void *data)
{
    X86CPUClass *xcc = X86_CPU_CLASS(oc);
    CPUClass *cc = CPU_CLASS(oc);
    DeviceClass *dc = DEVICE_CLASS(oc);
...
#ifdef CONFIG_TCG
    cc->do_interrupt = x86_cpu_do_interrupt;
    cc->cpu_exec_interrupt = x86_cpu_exec_interrupt;
#endif
    cc->dump_state = x86_cpu_dump_state;
    cc->get_crash_info = x86_cpu_get_crash_info;
    cc->set_pc = x86_cpu_set_pc;
    cc->synchronize_from_tb = x86_cpu_synchronize_from_tb;
    cc->gdb_read_register = x86_cpu_gdb_read_register;
    cc->gdb_write_register = x86_cpu_gdb_write_register;
    cc->get_arch_id = x86_cpu_get_arch_id;
    cc->get_paging_enabled = x86_cpu_get_paging_enabled;
...
}
```

The underlying `x86_cpu_do_interrupt` is a place holder for various
cases (userland, system emulation or nested virtualization). In basic
system emulation mode it will call
[`do_interrupt_all`](https://github.com/qemu/qemu/blob/v4.2.0/target/i386/seg_helper.c#L1206)
which implements low level x86 specific interrupt handling.

```

`interrupts.md`:

```md
# A deep dive into QEMU: interrupt controller

In this post, we will see how to create an interrupt controller.


## Yet another SysBusDevice

Now that you understand how to create a SysBusDevice. Let's assume
that our controller is one of those.

It's association to the system bus is done at board level
(`hw/cpiom/board.c`) with :

```c
static void create_intr(cpiom_t *cpiom)
{
    cpiom->intc = sysbus_create_varargs(
        "cpiom-intc", CPIOM_MMAP_INTR_CTL,
        cpiom->cpu->env.irq_inputs[PPC6xx_INPUT_SRESET],
        cpiom->cpu->env.irq_inputs[PPC6xx_INPUT_MCP],
        cpiom->cpu->env.irq_inputs[PPC6xx_INPUT_INT],
        NULL);
}
```

Remember, the `sysbus_create_varargs` function takes a variable number
of `qemu_irq` as arguments. These irqs are the one to be connected
with the device being created.

We are creating an interrupt controller, whose main purpose is to
receive interrupt requests from other devices. But it also has to be
connected to the CPU IRQ pins. That way, the CPU will be able to raise
interrupts to the running operating system kernel and call its
`interrupt service routines` or `vector handlers` whatever you call
them.

In our case, the MPC 755 PowerPC CPU has three available IRQ pins for
:
- System Reset Exception
- Machine check Exception
- External Interrupt Exception

If you are not familiar whith these concepts, please
[RTFM](https://www.nxp.com/docs/en/reference-manual/MPC750UM.pdf) !


## Preparing the receiving IRQ lines

Looking at the device specific code in `/hw/cpiom/intr.c`:

```c
static void cpiom_intc_init(Object *obj)
{
    DeviceState         *dev = DEVICE(obj);
    cpiom_intc_state_t *intc = CPIOM_INTC(obj);
    SysBusDevice        *sbd = SYS_BUS_DEVICE(obj);

    memory_region_init_io(&intc->iomem, obj, &cpiom_intc_io_ops, intc,
                          CPIOM_INTC_NAME, CPIOM_MMAP_INTR_CTL_SIZE);

    sysbus_init_mmio(sbd, &intc->iomem);

    intc->act[INTC_RST].name = "RST";
...
    qdev_init_gpio_in_named_with_opaque(dev, rst_handler, dev,
                                        intc->act[INTC_RST].name, 32);

    intc->act[INTC_MCP].name = "MCP";
...
    qdev_init_gpio_in_named_with_opaque(dev, mcp_handler, dev,
                                        intc->act[INTC_MCP].name, 32);

    intc->act[INTC_ITN].name = "ITN";
...
    qdev_init_gpio_in_named_with_opaque(dev, itn_handler, dev,
                                        intc->act[INTC_ITN].name, 32);

    sysbus_init_irq(sbd, &intc->irq[CPIOM_IRQ_RST]);
    sysbus_init_irq(sbd, &intc->irq[CPIOM_IRQ_MCP]);
    sysbus_init_irq(sbd, &intc->irq[CPIOM_IRQ_ITN]);

    cpiom_intc_reset(dev);
}
```

The CPIOM interrupt controller is a simple device with an IO memory
region and three identical *inner* interrupt controllers, orchestrated
by a global freeze register. Each *inner* interrupt controller has a
simple logic of `enable, mask, priority` registers. We won't enter the
details of its logic, it's out of scope.

The fundamental element in the previous code, is that we are able to
register 32 IRQ lines per interrupt controller, for each of the three
CPU IRQ pins: RST, MCP and ITN. We do so thanks to:

```c
static void itn_handler(void *opaque, int irq, int level);

qdev_init_gpio_in_named_with_opaque(dev, itn_handler, dev, intc->act[INTC_ITN].name, 32);
```

We associate the `itn_handler` callback to the ITN interrupt
controller for its 32 available lines. GPIO `in/out` registration is
string based. And if you remember the previous post, the EDC device
chose to attach its IRQ line to the ITN interrupt controller this way:

```c
qdev_get_gpio_in_named(cpiom->intc, "ITN", INT_N_IT_EDC_ERR),
```

Upon IRQ events (*raised or lowered*) on this controller, the
`itn_handler` will be called with the related IRQ number and level as
argument.

## A bit more about IRQs

If you want to learn more about the internal representation of
[`qemu_irq`](https://github.com/qemu/qemu/blob/v4.2.0/hw/core/irq.c#L31)
in the QEMU code, they are defined as :

```c
typedef struct IRQState *qemu_irq

struct IRQState {
    Object parent_obj;

    qemu_irq_handler handler;
    void *opaque;
    int n;
};
```

It's a simple object with a number, a callback and its avaible generic
argument (usually the *interrupt controller* device whose handler is
being called). And if you look at
[`qdev_init_gpio_in_named_with_opaque`](https://github.com/qemu/qemu/blob/v4.2.0/hw/core/qdev.c#L412):

```c
void qdev_init_gpio_in_named_with_opaque(DeviceState *dev,
                                         qemu_irq_handler handler,
                                         void *opaque,
                                         const char *name, int n)
{
...
    gpio_list->in = qemu_extend_irqs(gpio_list->in, gpio_list->num_in, handler, opaque, n);
...
}

qemu_irq *qemu_extend_irqs(qemu_irq *old, int n_old, qemu_irq_handler handler,
                           void *opaque, int n)
{
    qemu_irq *s;
    int i;
...
    for (i = n_old; i < n + n_old; i++) {
        s[i] = qemu_allocate_irq(handler, opaque, i);
    }
...
}

qemu_irq qemu_allocate_irq(qemu_irq_handler handler, void *opaque, int n)
{
    struct IRQState *irq;

    irq = IRQ(object_new(TYPE_IRQ));
    irq->handler = handler;
    irq->opaque = opaque;
    irq->n = n;

    return irq;
}

```

And whenever an IRQ is raised or lowered:

```c
static inline void qemu_irq_raise(qemu_irq irq) { qemu_set_irq(irq, 1); }
static inline void qemu_irq_lower(qemu_irq irq) { qemu_set_irq(irq, 0); }

void qemu_set_irq(qemu_irq irq, int level)
{
    if (!irq)
        return;

    irq->handler(irq->opaque, irq->n, level);
}
```

```

`machine.md`:

```md
# A deep dive into QEMU: a new machine

In this post, we will see how to create a new machine. Opportunity is
given to introduce the so called QOM.

## Available Machines and CPUs

The available machines for a given architecture can be listed with the
`-M ?` command line option.

For the PowerPC target architecture we have:

```
$ qemu-system-ppc -M ?
Supported machines are:
40p                  IBM RS/6000 7020 (40p)
bamboo               bamboo
g3beige              Heathrow based PowerMAC (default)
mac99                Mac99 based PowerMAC
mpc8544ds            mpc8544ds
none                 empty machine
ppce500              generic paravirt e500 platform
prep                 PowerPC PREP platform (deprecated)
ref405ep             ref405ep
sam460ex             aCube Sam460ex
taihu                taihu
virtex-ml507         Xilinx Virtex ML507 reference design
```

Once you picked-up your favorite machine, you can also choose between
available CPUs for the related machine.

```
$ qemu-system-ppc -M 40p -cpu ?
PowerPC 601_v0           PVR 00010001
PowerPC 601_v1           PVR 00010001
...
PowerPC 755_v2.8         PVR 00083208
PowerPC 755              (alias for 755_v2.8)
PowerPC goldfinger       (alias for 755_v2.8)
...
```

## The CPIOM machine

Let's create a new machine to support
[CPIOM](http://www.artist-embedded.org/docs/Events/2007/IMA/Slides/ARTIST2_IMA_Itier.pdf)
in QEMU.

In Integrated Modular Avionics (IMA), the Core Processing Input Output
Modules (CPIOMs) are PowerPC MPC-755 CPU based embeded equipements
with RAM, Flash, NVM, Timers, Interrupt controller, DMA engine, PCI
controller and others components we won't cover (AFDX, DSP).

![CPIOM architecture](./docs/assets/images/34A45ACD-6EBF-4C3F-93E4-DF4EE3009469.png)

## Code organisation

First let's prepare our environment by isolating our cpiom code in a
dedicated directory. Usually, every single object you add to QEMU
should be placed in the right directory. They are organized by nature,
not target usage or association.

- a new PowerPC based machine should land in `hw/ppc/`
- a new serial device in `hw/char/`
- a new PCI host controller in `hw/pci-host/`
- a new network controller in `hw/net/`
- ...

So let's do the opposite and put everything at the same place in
`hw/cpiom`. The first thing we have to do is tell QEMU how to build
our new machine and futur devices:

```
$ cat hw/cpiom/Makefile.objs
obj-y += board.o dev1.o dev2.o

$ cat hw/Makefile.objs
...
devices-dirs-y += cpiom/
```

## Creating a new machine: `hw/cpiom/board.c`

To implement a new machine in QEMU we will need to create QOM
[`TypeInfo`](https://github.com/qemu/qemu/tree/v4.2.0/include/qom/object.h#L426)
and its associated
[`MachineClass`](https://github.com/qemu/qemu/tree/v4.2.0/include/hw/boards.h#L107)
and
[`MachineState`](https://github.com/qemu/qemu/tree/v4.2.0/include/hw/boards.h#268)
initialization functions.

You can find documentation about [QOM
conventions](https://wiki.qemu.org/Documentation/QOMConventions).

### The TypeInfo

We can use the predefined macro:

```c
DEFINE_MACHINE("cpiom", cpiom_machine_init)
```

Which generates the following code:

```c
static void cpiom_machine_init_class_init(ObjectClass *oc, void *data)
{
    MachineClass *mc = MACHINE_CLASS(oc);
    cpiom_machine_init(mc);
}

static const TypeInfo cpiom_machine_init_typeinfo = {
    .name       = MACHINE_TYPE_NAME("cpiom"),
    .parent     = TYPE_MACHINE,
    .class_init = cpiom_machine_init_class_init,
};

static void cpiom_machine_init_register_types(void)
{
    type_register_static(&cpiom_machine_init_typeinfo);
}
type_init(cpiom_machine_init_register_types)
```

This will register a new machine class type for our CPIOM. This is the
highest level definition.

### The MachineClass

As the API states, our `class_init` function will be called after all
parent class init methods have been called. The class is initialized
before any object instance of the class.

Let's write it :

```c
void cpiom_machine_init(MachineClass *mc)
{
    mc->desc = "CPIOM board";
    mc->init = cpiom_init;
    mc->default_cpu_type = POWERPC_CPU_TYPE_NAME("755_v2.8");
    mc->default_ram_size = CPIOM_MMAP_SDRAM_SIZE;
}
```

At this point we are able to define specific properties of the
machine, such as the CPU model, the RAM size, and most importantly the
instance initialization method `cpiom_init`.


### The MachineState

Once the class is ready, an object instance will be created. We
previously provided an `mc->init = cpiom_init` function.

```c
static void cpiom_init(MachineState *mcs)
{
    cpiom_t *cpiom = g_new0(cpiom_t, 1);

    cpiom_init_cpu(cpiom, mcs);
    cpiom_init_dev(cpiom);
    cpiom_init_boot(cpiom, mcs);
}
```
The core features of our board implementation will lie here.


## Initializing a CPU for the CPIOM: PowerPC 755

The cpu instantiation will look like the following:

```c
static void cpiom_init_cpu(cpiom_t *cpiom, MachineState *mcs)
{
    cpiom->cpu = POWERPC_CPU(cpu_create(mcs->cpu_type));

    /* Set time-base frequency to 16.6 Mhz */
    cpu_ppc_tb_init(&cpiom->cpu->env, CPIOM_TBFREQ);

    cpiom->cpu->env.spr[SPR_HID2] = 0x00040000;

    CPUClass *cc = CPU_GET_CLASS( CPU(cpiom->cpu) );
    PowerPCCPUClass *pcc = POWERPC_CPU_CLASS(cc);
    CPUPPCState *env = &cpiom->cpu->env;

    /* Default mmu_model for MPC755 is POWERPC_MMU_SOFT_6xx (no softmmu) */
    pcc->mmu_model = POWERPC_MMU_32B;
    env->mmu_model = POWERPC_MMU_32B;
    pcc->handle_mmu_fault = ppc_hash32_handle_mmu_fault;
}
```

### Obtaining a PowerPC CPU state object

The generic
[`cpu_create()`](https://github.com/qemu/qemu/tree/v4.2.0/hw/core/cpu.c#L58)
function will create a CPU object of the proper type. We want to
instantiate a PowerPC CPU whose specific type has been defined during
the class init (`POWERPC_CPU_TYPE_NAME("755_v2.8")`).

The code is equivalent to:

```c
CPUState *cpu = CPU( object_new( POWERPC_CPU_TYPE_NAME("755_v2.8") ));
```

The QOM type instantiation is complex and not that much
interesting. We should just remember that we want a PowerPC cpu from
the 755 family. The requested CPU model and family are defined in
[target/ppc/cpu-models.c](https://github.com/qemu/qemu/tree/v4.2.0/target/ppc/cpu-models.c#L642)
and
[target/ppc/translate_init.inc.c](https://github.com/qemu/qemu/tree/v4.2.0/target/ppc/translate_init.inc.c#L6462)
:


```c
POWERPC_DEF("755_v2.8", CPU_POWERPC_7x5_v28, 755, "PowerPC 755 v2.8")

POWERPC_FAMILY(755)(ObjectClass *oc, void *data)
{
    DeviceClass *dc = DEVICE_CLASS(oc);
    PowerPCCPUClass *pcc = POWERPC_CPU_CLASS(oc);

    dc->desc = "PowerPC 755";
    pcc->init_proc = init_proc_755;
...
    pcc->mmu_model = POWERPC_MMU_SOFT_6xx;
    pcc->excp_model = POWERPC_EXCP_7x5;
    pcc->bus_model = PPC_FLAGS_INPUT_6xx;
    pcc->bfd_mach = bfd_mach_ppc_750;
    pcc->flags = POWERPC_FLAG_SE | POWERPC_FLAG_BE |
                 POWERPC_FLAG_PMM | POWERPC_FLAG_BUS_CLK;
}

static void init_proc_755(CPUPPCState *env)
{
...
    init_excp_7x5(env);
    ppc6xx_irq_init(ppc_env_get_cpu(env));
....
}
```

This is the very low level and detailled CPU init code. We can see MMU
type, interrupts and exceptions initialization code.


### Accessing your PowerPC CPU

Along the QEMU code, you will usually find dynamic type cast macro
such as `DEVICE_CLASS, CPU_CLASS, CPU or POWERPC_CPU`. They navigate
with safe type checking through the inherited classes the device
belongs to. They help you retrieve the correct definition for a given
runtime object.

```c
#define POWERPC_CPU(obj)  OBJECT_CHECK(PowerPCCPU, (obj), TYPE_POWERPC_CPU)
```

Below are some use cases:

```c
CPUState        *cs     = CPU(cpiom->cpu);
CPUClass        *cc     = CPU_GET_CLASS(cs);
PowerPCCPU      *ppc    = POWERPC_CPU(cs);
PowerPCCPUClass *ppc_cc = POWERPC_CPU_CLASS(cc);
CPUPPCState     *ppc_cs = &ppc->env;
```

In the
[`CPUClass`](https://github.com/qemu/qemu/tree/v4.2.0/include/hw/core/cpu.h#L77)
you will find function pointers related to the CPU family such as
execution, interrupt, mmu fault handling. While the
[`CPUState`](https://github.com/qemu/qemu/tree/v4.2.0/include/hw/core/cpu.h#L297)
will hold generic state of the CPU such as its running state,
configured breakpoints, last exception raised, ...

They are both related to overall QEMU CPU design. The part you might
be interested in is the *environment pointer* which keeps track of the
CPU registers. In our case, look for
[`CPUPPCState`](https://github.com/qemu/qemu/tree/v4.2.0/target/ppc/cpu.h#L962).


```c
struct PowerPCCPU {
...
    CPUPPCState env;
...
}

struct CPUPPCState {
...
    /* general purpose registers */
    target_ulong gpr[32];
    /* Storage for GPR MSB, used by the SPE extension */
    target_ulong gprh[32];
...
}
```

So if you want to access `GPR[14]` of your CPIOM CPU, you will end
doing something like:

```c
cpiom->cpu->env.gpr[14] = xxxx;
```

### Forcing software MMU for the CPIOM

Once the CPU object is created, we can proceed with some tuning for
instance on its core frequency or model specific register default
values. Our init function is also responsible for enabling software
MMU support for the MPC755 family which is not the case by default:

```c
static void cpiom_init_cpu(cpiom_t *cpiom, MachineState *mcs)
{
...
    CPUClass *cc = CPU_GET_CLASS( CPU(cpiom->cpu) );
    PowerPCCPUClass *pcc = POWERPC_CPU_CLASS(cc);
    CPUPPCState *env = &cpiom->cpu->env;

    /* Default mmu_model for MPC755 is POWERPC_MMU_SOFT_6xx (no softmmu) */
    pcc->mmu_model = POWERPC_MMU_32B;
    env->mmu_model = POWERPC_MMU_32B;
    pcc->handle_mmu_fault = ppc_hash32_handle_mmu_fault;
}
```

Under the QEMU terminology, `softmmu` stands for software memory
management unit. This is the software implementation of CPU hardware
MMU. Several implementations are available in the QEMU source code,
for different architectures.

In the post dedicated to QEMU memory internals, we will explain how
address translation is operated and why we had to do this.


### Conclusion

This is not exhaustive, but you got the point. If you need more
control over your CPU init, have a look at its
[definition](https://github.com/qemu/qemu/tree/v4.2.0/target/ppc/cpu.h#L1180).

```

`options.md`:

```md
# A deep dive into QEMU: Giving and adding options

In this blog post we will see how we can give QEMU access to a file in
the host that represents the code flash backend through the `-drive`
option. We will also detail how to add new options to the QEMU command
line.

## A block backend for the code flash

The CPIOM board we are simulating has a code flash memory that holds
the bootloader and the operating system firmware. It is the first
software components executed by the CPU and must be available from the
boot.

Obviously, the strategy is to have bootloader and OS firmware stored
in a file representing the code flash memory layout, and expose it to
QEMU as a block device backend. We can use the `-drive` option:

```shell
-drive file=/path/to/flash.code,format=raw,id=mxfc-flash-code
```

The QEMU API will connect the host file `/path/to/flash.code` to a QOM
device whose `id` is `mxfc-flash-code`, not to be confused with a QOM
device type we use to define when implementing new devices.

The flash component emulation code will make use of the QEMU block
device API to look for this given `id`:

```c
#define MXFC_FLASH_BLK_NAME "mxfc-flash-code"

blk = blk_by_name(MXFC_FLASH_BLK_NAME);
if (blk == NULL) {
    cpiom_error("no flash code block backend found\n");
}

/* We look into block backend API to access "-drive=" option. We
 * should use blk_pread(), blk_pwrite() to access data.  Here we
 * directly mmap() the file into memory for faster access.
 */
BlockDriverState *bs = blk_bs(blk);

fd = open(bs->filename, O_RDONLY);
if (fd < 0) {
    cpiom_error("can't open block backend: %s\n", bs->filename);
}
```

And that's it ! We got our file access. The last step is to expose the
file content thanks to the QEMU memory region API. QEMU is able to
populate RAM memory regions from file descriptors:

```c
memory_region_init_ram_from_fd(&mem, owner, MXFC_FLASH_BLK_NAME"-mmap",
                               size, false, fd, &error_fatal);
```


## Adding a new option


### Option definition

At some point, you may want to add specific option to the QEMU command
line. The options definition is located at
[`qemu-options.hx`](https://github.com/qemu/qemu/blob/v4.2.0/qemu-options.hx). Below
is an example `-gustave <jsonfile>` option:

```shell
DEF("gustave", HAS_ARG, QEMU_OPTION_gustave, \
    "-gustave file   GUSTAVE fuzzer JSON configuration filename\n", QEMU_ARCH_ALL)
STEXI
@item -gustave @var{file}
@findex -gustave
GUSTAVE fuzzer JSON configuration filename.
ETEXI
```

This file is used by the build system to generate `qemu-options.def`
before building the qemu image. The previous `gustave` option looks
like:

```shell
DEF("gustave", HAS_ARG, QEMU_OPTION_gustave, \
"-gustave file   GUSTAVE fuzzer JSON configuration filename\n", QEMU_ARCH_ALL)
```

### Option registering

The options parser is located at
[vl.c](https://github.com/qemu/qemu/blob/v4.2.0/vl.c) which is QEMU
main C file. Options are regrouped in `QemuOptsList`:

```c
static QemuOptsList qemu_gustave_opts = {
    .name = "gustave",
    .implied_opt_name = "gustave",
    .head = QTAILQ_HEAD_INITIALIZER(qemu_gustave_opts.head),
    .merge_lists = true,
    .desc = {
        { /* end of list */ }
    },
};
```

And you add these new options with: `qemu_add_opts(&qemu_gustave_opts);` in QEMU [`main`](https://github.com/qemu/qemu/blob/v4.2.0/vl.c#L2880).


## Option parsing

As you might imagine, QEMU provides you with a complete API to handle
your options. You can navigate through the linked list of a specific
`QemuOptsList`, but you can also look for option by string such as:

```c
const char *fname;
fname = qemu_opt_get(qemu_find_opts_singleton("gustave"), "gustave");
```

It's just that simple. By the way our option takes a JSON file name
and QEMU comes with a
[`JSON`](https://github.com/qemu/qemu/blob/v4.2.0/qobject/qjson.c)
parser easily backed by
[`QDict`](https://github.com/qemu/qemu/blob/v4.2.0/include/qapi/qmp/qdict.h)
objects if you want to have a look at. Just give file content to:

```c
obj  = qobject_from_json(content, NULL);
dict = qobject_to(QDict, obj);

obj   = qdict_get(dict, "key");
value = qnum_get_uint(qobject_to(QNum, obj));
```

```

`pci.md`:

```md
# A deep dive into QEMU: PCI host bridge controller

The QEMU PCI subsystem is really interesting to understand. Because
PCI is a [specification](https://pcisig.com/specifications), QEMU
implements everything to simplify your life of *new PCI device*
developper.

## Some basics about PCI

A lot of ressources are available on
[PCI](https://wiki.osdev.org/PCI). Please take time to get familiar
wih the concepts involved.

Now that you are a PCI expert, you know what we have to do to expose a
PCI host bridge. Fortunately, the PCI config space is a standard
and device specific IO memory regions are located through config space
`BAR` registers. Seems we just have to setup config space values for
each simulated PCI device.

Our example PCI host bridge implementation is largely inspired by the
[RAVEN](https://github.com/qemu/qemu/blob/v4.2.0/hw/pci-host/prep.c)
PCI host bridge available in QEMU source code.

Implementing a PCI host bridge is not a tremendous task. An
interesting part is related to QEMU internal PCI API, where PCI
devices dynamically expose their IO memory regions through PCI config
space BAR. Apart from that, a PCI host bridge is just another device.

We will see in the implementation that a PCI host bridge is also a PCI
device.

## CPIOM PCI subsystem

Below is an overview of an imaginary PCI organisation in our CPIOM. We
will mainly focus on the PCI host bridge `master` and PCI slave device
`mxfc` which exposes through BAR registers the different flash
memories and a serial controller.

![CPIOM PCI subsystem](./docs/assets/images/6d54ba55-f68f-43d1-a020-aca2e3a9468b.png)

The code flash holds the firmware which is used to bootstrap the CPIOM
board. It must be available to the very first instruction fetched by
the CPU. We will detail flash simulation in another blog post.

The CPIOM machine initialization code is responsible for PCI devices
instantiation:

```c
static void cpiom_init_dev(cpiom_t *cpiom)
{
...
    pci_init(cpiom);
}

static void pci_init(cpiom_t *cpiom)
{
    DeviceState        *d;
    MasterPCIHostState *h;

    /* Master PCI Host controller */
    d = qdev_create(NULL, "master-pcihost");
    object_property_add_child(qdev_get_machine(), "master", OBJECT(d), NULL);

    h = MASTER_PCI_HOST_BRIDGE(d);
    memory_region_add_subregion(cpiom->config, 0x24, &h->pci_reg);

    qdev_init_nofail(d);

    PCIBus *pci_bus = (PCIBus *)qdev_get_child_bus(d, "pci.0");
    if (pci_bus == NULL) {
        cpiom_error("Couldn't create PCI host controller");
    }

    /* Slave PCI Device */
    pci_create_simple(pci_bus, PCI_DEVFN(1, 0), "mxfc");
}
```

As you can see, we first create the PCI host bridge `master` and then
attach our slave device `mxfc` to its bus. We are using the low-level
`qdev` API to create the host controller. Also notice the
`cpiom->config` subregion mapping some of the PCI host bridge IO
registers.

Now let's have a look at the master implementations.


## PCI Master

This component is the PCI host bridge. The device type declaration is
equivalent to other QOM devices:

```c
static void master_pcihost_class_init(ObjectClass *klass, void *data)
{
    DeviceClass *dc = DEVICE_CLASS(klass);

    set_bit(DEVICE_CATEGORY_BRIDGE, dc->categories);
    dc->realize = master_pcihost_realizefn;
    dc->props = master_pcihost_properties;
    dc->fw_name = "pci";
}

static const TypeInfo master_pcihost_info = {
    .name = TYPE_MASTER_PCI_HOST_BRIDGE,
    .parent = TYPE_PCI_HOST_BRIDGE,
    .instance_size = sizeof(MasterPCIHostState),
    .instance_init = master_pcihost_initfn,
    .class_init = master_pcihost_class_init,
};
```

We declare ourself as a child to `TYPE_PCI_HOST_BRIDGE` and
`DEVICE_CATEGORY_BRIDGE` category. The following
[categories](https://github.com/qemu/qemu/blob/v4.2.0/include/hw/qdev-core.h#L18)
are available inside QEMU:

```c
typedef enum DeviceCategory {
    DEVICE_CATEGORY_BRIDGE,
    DEVICE_CATEGORY_USB,
    DEVICE_CATEGORY_STORAGE,
    DEVICE_CATEGORY_NETWORK,
    DEVICE_CATEGORY_INPUT,
    DEVICE_CATEGORY_DISPLAY,
    DEVICE_CATEGORY_SOUND,
    DEVICE_CATEGORY_MISC,
    DEVICE_CATEGORY_CPU,
    DEVICE_CATEGORY_MAX
} DeviceCategory;
```

### The host bridge controller part

The most important part of host init code is located in
`master_pcihost_initfn` and `master_pcihost_realizefn`
functions. `Realized` is a property of QOM device state, usually set
after device instantiation. Have a look at
[DeviceClass](https://github.com/qemu/qemu/blob/v4.2.0/include/hw/qdev-core.h#L47)
for more details.

But let's have a quick overview of our host bridge type in the first
place. It's a little bit more complex than a simple QOM device:

```c
typedef struct MasterPCIDevState {
    PCIDevice    dev;
    MemoryRegion pci_cfg;
} MasterPCIDevState;

typedef struct MasterPCIHostState {
    PCIHostState         parent_obj;

    PCIBus               pci_bus;
    MemoryRegion         pci_reg;
    MemoryRegion         pci_mem;
    MasterPCIDevState    pci_dev;

} MasterPCIHostState;
```

It has an associated `PCIBus`, memory mapped IO registers (`pci_reg`),
a memory region overlapping system memory (`pci_mem`) and a dedicated
device `pci_dev`. We will also see that its parent type
(`PCIHostState`) holds important things.

The `pci_mem` is a full-span (4GB for a 32 bits system) memory overlap
of the system memory region with a lower access priority. It's used
merely by the QEMU PCI subsystem to dynamically configure BAR devices
(more on this later).

The `pci_reg` is specific to our host bridge implementation and allows
to intercept register accesses as for any device. It also helps to
intercept standard PCI config space IO operations through `CONF_ADDR`
and `CONF_DATA` registers.


### Accessing the PCI config space

When you want to access the PCI config space you usually make use of
`in/out` instructions targetting ports `0xcf8/0xcfc`. However, for the
PowerPC architecture there is no such `in/out` instructions, the PCI
config space is directly memory mapped (ala `PCI-express`).

As this mechanism is part of the PCI specification, QEMU provides
generic handlers to ease config space access:
[`pci_host_{data/conf}_{le/be}_ops`](https://github.com/qemu/qemu/blob/v4.2.0/hw/pci/pci_host.c#L190).

If you dig into the QEMU code, you will discover that these handlers
end up accessing PCI device `config_read/config_write`
callbacks. Hence the need for our controller to hold a `pci_dev`
object to expose its own PCI config space.

We will see in the init functions below how we connect these standard
handlers to the right IO memory area of our host brigde controller.


### The host bridge device part

This device is present to expose the PCI config space part of our host
bridge controller: vendor id, device id, revision and so on.

You usually don't need to reimplement the `config_read/config_write`
handlers. QEMU provides you with default ones
[`pci_default_read_config`](https://github.com/qemu/qemu/blob/v4.2.0/hw/pci/pci.c#L1380).

During PCI device initialization, you might setup its config space
values as the following:

```c
static void master_realize(PCIDevice *d, Error **errp)
{
    MasterPCIDevState *s = MASTER_PCI_DEVICE(d);

    pci_set_byte(d->config + PCI_LATENCY_TIMER, 0x80);
    pci_set_word(d->config + PCI_COMMAND, 0x1d6);
    pci_set_long(d->config + PCI_BASE_ADDRESS_0, 0x80000000);
}
```


### Putting it all together


The association of these components to the appropriate handlers and
mappings is done in the aforementioned initialization functions:

```c
static void master_pcihost_realizefn(DeviceState *d, Error **errp)
{
    PCIHostState       *h = PCI_HOST_BRIDGE(d);
    MasterPCIHostState *s = MASTER_PCI_HOST_BRIDGE(d);

    /* CONFIG_ADDR & CONFIG_DATA registers */
    memory_region_init_io(&h->conf_mem, OBJECT(h), &pci_host_conf_be_ops, s,
                          "pci-conf-addr", 4);

    memory_region_init_io(&h->data_mem, OBJECT(h), &pci_host_data_le_ops, s,
                          "pci-conf-data", 4);

    memory_region_add_subregion(&s->pci_reg, 0, &h->conf_mem);
    memory_region_add_subregion(&s->pci_reg, 4, &h->data_mem);

    object_property_set_bool(OBJECT(&s->pci_bus), true, "realized", errp);
    object_property_set_bool(OBJECT(&s->pci_dev), true, "realized", errp);
}

static void master_pcihost_initfn(Object *obj)
{
    PCIHostState       *phs = PCI_HOST_BRIDGE(obj);
    MasterPCIHostState *hhs = MASTER_PCI_HOST_BRIDGE(obj);
    DeviceState        *dev;

    memory_region_init(&hhs->pci_mem, obj, "pci", UINT32_MAX);
    memory_region_add_subregion_overlap(get_system_memory(),
                                        0, &hhs->pci_mem, -1);

    memory_region_init_io(&hhs->pci_reg, obj, &master_reg_ops, hhs,
                          "master-reg", 4*4);

    pci_root_bus_new_inplace(&hhs->pci_bus, sizeof(hhs->pci_bus),
                             DEVICE(obj), NULL,
                             &hhs->pci_mem,
                             &hhs->pci_reg,
                             0, TYPE_PCI_BUS);

    /* Create Master pci device 0.0 */
    phs->bus = &hhs->pci_bus;
    object_initialize(&hhs->pci_dev, sizeof(hhs->pci_dev),
                      TYPE_MASTER_PCI_DEVICE);

    dev = DEVICE(&hhs->pci_dev);
    qdev_set_parent_bus(dev, BUS(&hhs->pci_bus));
    object_property_set_int(OBJECT(&hhs->pci_dev), PCI_DEVFN(0,0), "addr", NULL);
    qdev_prop_set_bit(dev, "multifunction", false);
}
```

Notice how we attached the PCI device to the host bridge controller.

```

`pci_slave.md`:

```md
# A deep dive into QEMU: PCI slave devices

In the first [PCI article](pci.md) we covered the host bridge
part. Now that we have a clear overview of the QEMU PCI subsystem,
let's have a look at how PCI devices really work.

We will focus on the slave part of the CPIOM PCI components:

![CPIOM PCI subsystem](./docs/assets/images/6d54ba55-f68f-43d1-a020-aca2e3a9468b.png)


## PCI slave device: MXFC

This component is a multi-purpose PCI device. It provides flash and
serial support. For illustration purpose, we choose to call this PCI
device `MXFC`.

```c
typedef struct MXFCState {
    PCIDevice     parent_obj;
    uint8_t       _reg[MXFC_BAR3_SIZE];
    FILE          *serial;

    struct _nvm_regions {
        cpiom_nvm_code_t code;
        cpiom_nvm_data_t data;
        cpiom_nvm_data_t eeprom;
    } nvm;

    struct _memory_regions {
        MemoryRegion reg;
        MemoryRegion dgo;
    } mm;

} MXFCState;
```

We have dedicated objects for flash representation (uncovered here)
and a `FILE` handler for the serial port. We log received serial
characters to a file.

The device specification has several
[BAR](https://wiki.osdev.org/PCI#Base_Address_Registers)s which map
the following:
- BAR0, code flash
- BAR1, data flash
- BAR2, eeprom
- BAR3, various configuration registers

The BARs are exposed through our device PCI config space, but there
value might be changed by an OS driver at runtime. As they refer to
the location of memory mapped device registers, there should exist a
QEMU internal *moulinette* to inform the related emulated devices of
their possible relocation. We will see how in this post.

### Device initialization

As for usual QOM devices:

```c
static void mxfc_class_init(ObjectClass *klass, void *data)
{
    PCIDeviceClass *k  = PCI_DEVICE_CLASS(klass);
    DeviceClass    *dc = DEVICE_CLASS(klass);

    k->realize   = mxfc_realize;
    k->vendor_id = PCI_VENDOR_ID_MXFC;
    k->device_id = PCI_DEVICE_ID_MXFC;
    k->revision  = PCI_DEVICE_REV_ID_MXFC;
    k->class_id  = PCI_CLASS_OTHERS;
    k->config_write = mxfc_config_space_write;
}

static const TypeInfo mxfc_type_info = {
    .name          = TYPE_MXFC_PCI_DEVICE,
    .parent        = TYPE_PCI_DEVICE,
    .instance_size = sizeof(MXFCState),
    .instance_init = mxfc_init,
    .class_init    = mxfc_class_init,
    .interfaces    = (InterfaceInfo[]) {
        { INTERFACE_CONVENTIONAL_PCI_DEVICE },
        { },
    },
};
```

We have the standard type declaration followed by default values to
appear in the device PCI config space at runtime (device, vendor,
revision, ...). Notice that we overload the default `config_write`
callback for PCI config space access.

Additional setup is done in `mxfc_realize`, because at this step when
device is *realized*, the PCI device config space buffers are
allocated and we can access them safely:

```c
static void mxfc_realize(PCIDevice *pci, Error **errp)
{
    DeviceState *dev = DEVICE(pci);
    MXFCState   *t   = MXFC_PCI_DEVICE(dev);
    uint8_t     *conf;

    conf = pci->wmask;
    pci_set_word(conf + PCI_STATUS,  0xf800);
    pci_set_word(conf + PCI_COMMAND, 0x141);

    conf = pci->config;
    pci_set_word(conf + PCI_STATUS, 0x480);
    pci_set_word(conf + PCI_COMMAND, 0x1c2);
    pci_set_long(conf + PCI_BASE_ADDRESS_0, MXFC_BAR0_DFT);
...
}
```

We can define bitmasks for words stored in that space. I let you have
a look at
[`PCIDevice`](https://github.com/qemu/qemu/blob/v4.2.0/include/hw/pci/pci.h#L266)
type definition.

### Data flash and EEPROM

We then init data and eeprom flash components. We won't detail their
specific device implementation. We here use
[`pci_register_bar`](https://github.com/qemu/qemu/blob/v4.2.0/hw/pci/pci.c#L1138)
to tell QEMU that their respective `mmio` region is linked to a
BAR. Whenever BAR1 or BAR2 will be updated in the MXFC PCI config
space, the underlying `mmio` regions of `nvm_data` and `nvm_eeprom`
will be remapped to their new location ... *much appreciated*.

```c
/* BAR1 FLASH data */
cpiom_nvm_data_init(&t->nvm.data, "mxfc-flash-data",
                    OBJECT(t), MXFC_BAR1_SIZE);

pci_register_bar(pci, 1,
                 PCI_BASE_ADDRESS_SPACE_MEMORY |
                 PCI_BASE_ADDRESS_MEM_TYPE_32,
                 &t->nvm.data.mem);

/* BAR2 EEPROM */
cpiom_nvm_eeprom_init(&t->nvm.eeprom, "mxfc-eeprom",
                      OBJECT(t), MXFC_BAR2_SIZE);

pci_register_bar(pci, 2,
                 PCI_BASE_ADDRESS_SPACE_MEMORY |
                 PCI_BASE_ADDRESS_MEM_TYPE_32,
                 &t->nvm.eeprom.mem);
```

### Code flash

The code flash BAR0 setup has some extra configuration steps. The
device specification gives a default location and size:

```c
#define MXFC_BAR0_DFT    (0xfc000000)
#define MXFC_BAR0_SIZE   (64<<20)
```

This is the area where the PowerPC CPU fetches initial instructions at
bootup. Unfortunately, we cannot use `pci_register_bar` this time for
several reasons. One of them is that the `mmio` mappings are not
effective until an explicit access to the PCI config space that
usually happens in driver code. As previously said, the very first
instructions are fetched in this area so it must be available
immediatly after power on.

We thus directly map the region in the PCI address space:

```c
/* BAR0 FLASH code */
cpiom_nvm_code_init(&t->nvm.code, MXFC_FLASH_BLK_NAME,
                    OBJECT(t), MXFC_BAR0_SIZE);

memory_region_add_subregion_overlap(pci_get_bus(pci)->address_space_mem,
                                    MXFC_BAR0_DFT,
                                    &t->nvm.code.mem, 1);
```

If you remember, during device initialization we overloaded the
`config_write` callback with our own implementation. It looks like :

```c
static void mxfc_config_space_write(PCIDevice *pci_dev, uint32_t addr,
                                    uint32_t val, int len)
{
    if (addr == PCI_BASE_ADDRESS_0)
        return;

    pci_default_write_config(pci_dev, addr, val, len);
}
```

We ignore access to BAR0 because it is already in place and has no
reason to be modified (in our CPIOM environment). For other BARs, we
just call the original default handler.

## BAR access and updated mappings

When a target instruction writes to the PCI config space, usually the
[`pci_default_write_config`](https://github.com/qemu/qemu/blob/v4.2.0/hw/pci/pci.c#L1393)
`mmio` handler is called:

```c
void pci_default_write_config(PCIDevice *d, uint32_t addr, uint32_t val_in, int l)
{
...
    if (ranges_overlap(addr, l, PCI_BASE_ADDRESS_0, 24) ||
        ranges_overlap(addr, l, PCI_ROM_ADDRESS, 4) ||
        ranges_overlap(addr, l, PCI_ROM_ADDRESS1, 4) ||
        range_covers_byte(addr, l, PCI_COMMAND))
        pci_update_mappings(d);
...
}
```

As we can see, if the write operation hits a BAR, the
[`pci_update_mappings`](https://github.com/qemu/qemu/blob/v4.2.0/hw/pci/pci.c#L1320)
function is called and will update the corresponding memory subregion:

```c
static void pci_update_mappings(PCIDevice *d)
{
...
    r = &d->io_regions[i];
    new_addr = pci_bar_address(d, i, r->type, r->size);
    if (r->addr != PCI_BAR_UNMAPPED) {
        memory_region_add_subregion_overlap(r->address_space,
                                            r->addr, r->memory, 1);
...
}
```

Everything here is directly supported by QEMU as part of the PCI
subsystem and let the developpers focus only on device specificities.

```

`regions.md`:

```md
# A deep dive into QEMU: memory regions

In this post we'll have a glance at high level memory organisation in
QEMU: memory regions (MR).

We won't cover address spaces, because we usually manage memory
regions directly. However, have a look at
[docs/devel/memory](https://github.com/qemu/qemu/tree/v4.2.0/docs/devel/memory.rst)
and the code
([include/exec/memory.h](https://github.com/qemu/qemu/blob/v4.2.0/include/exec/memory.h)) for more details.

An high level external presentation of memory organisation is
available
[there](http://blog.vmsplice.net/2016/01/qemu-internals-how-guest-physical-ram.html). You
will also find a very interesting internal documentation at
[docs/devel/loads-stores](https://github.com/qemu/qemu/blob/v4.2.0/docs/devel/loads-stores.rst). This
is an enumeration of the available QEMU APIs for accessing memory.

When you want to play with memory regions in QEMU, you can either:

- get a direct pointer to the host buffer backing your VM memory
  region
- implement read/write callback functions to intercept every access (usually IO
  memory)
- use QEMU
  [`cpu_physical_memory_rw()`](https://github.com/qemu/qemu/tree/v4.2.0/exec.c#L3275) to safely access the region

In the blog post dedicated to the TCG, we will exactly see how
translated instructions access VM memory and how we can intercept at
this level.


## Looking at the memory tree (abbreviated)

Below is the tree of available memory regions once a `PowerPC 40p`
board is ready. As you can see, memory regions can contain other
memory regions (called `subregions`). This a clean way to organize
memory. Each memory region has its own properties and is attached to a
kind of view called the `address space`.

```
$ qemu-system-ppc -M 40p -s -S -monitor stdio
QEMU 4.2.0 monitor - type 'help' for more information

(qemu) info mtree

address-space: memory
  0000000000000000-ffffffffffffffff (prio 0, i/o): system
[...]
    0000000080000000-00000000807fffff (prio 1, i/o): pci-io-non-contiguous
    0000000080000000-00000000bf7fffff (prio 0, i/o): pci-io
      0000000080000000-0000000080000007 (prio 0, i/o): dma-chan
      0000000080000008-000000008000000f (prio 0, i/o): dma-cont
      0000000080000020-0000000080000021 (prio 0, i/o): pic
      0000000080000040-0000000080000043 (prio 0, i/o): pit
      0000000080000061-0000000080000061 (prio 0, i/o): pcspk
      0000000080000064-0000000080000064 (prio 0, i/o): i8042-cmd
      00000000800002f8-00000000800002ff (prio 0, i/o): serial
[...]
      0000000080000cf8-0000000080000cfb (prio 0, i/o): pci-conf-idx
      0000000080000cfc-0000000080000cff (prio 0, i/o): pci-conf-data
    0000000080800000-0000000080bfffff (prio 0, i/o): pciio
    00000000c0000000-00000000feffffff (prio 0, i/o): pci-memory
      00000000c00a0000-00000000c00bffff (prio 1, i/o): vga-lowmem
    00000000fff00000-00000000ffffffff (prio 0, rom): bios


address-space: cpu-memory-0
  0000000000000000-ffffffffffffffff (prio 0, i/o): system
[...]
    00000000fff00000-00000000ffffffff (prio 0, rom): bios


memory-region: pci-memory
  00000000c0000000-00000000feffffff (prio 0, i/o): pci-memory
    00000000c00a0000-00000000c00bffff (prio 1, i/o): vga-lowmem


memory-region: system
  0000000000000000-ffffffffffffffff (prio 0, i/o): system
[...]
    00000000fff00000-00000000ffffffff (prio 0, rom): bios

```

Default memory regions and address spaces are created by QEMU. The
most important is the `system memory region` which is created by
[`memory_map_init()`](https://github.com/qemu/qemu/blob/v4.2.0/exec.c#L2957)
from
[`cpu_exec_init_all()`](https://github.com/qemu/qemu/blob/v4.2.0/exec.c#L3402).

It can be seen as the top level one, and usually `subregions` are
added to the `system memory region`.


## Allocating system memory

This might be one of the most desired things when creating a new
machine : get RAM and load a firmware. The correct function to invoke
is
[`memory_region_allocate_system_memory()`](https://github.com/qemu/qemu/blob/v4.2.0/include/hw/boards.h#L14).

If we look at some other board implementations, for instance the
[`MIPS
R4K`](https://github.com/qemu/qemu/blob/v4.2.0/hw/mips/mips_r4k.c#L167):

```c
void mips_r4k_init(MachineState *machine)
{
    MemoryRegion *sys_mem = get_system_memory();
    MemoryRegion *ram = g_new(MemoryRegion, 1);

    memory_region_allocate_system_memory(ram, NULL, "mips_r4k.ram", ram_size);
    memory_region_add_subregion(sys_mem, 0, ram);
...
}
```

A new memory region for the RAM is created and directly added as a
`subregion` of the `system memory region`. From that point, accessing
physical addresses `0` to `ram_size` will hit the VM memory.

The `memory_region_allocate_system_memory` function actually calls
[memory_region_init_ram_shared_nomigrate()](https://github.com/qemu/qemu/blob/v4.2.0/memory.c#L1512)
from the QEMU memory API. It encapsulates RAM specific memory region
initialization.

The QEMU memory API allows you to create memory regions backed by file
descriptors, already allocated host buffers and callbacks as we will
see for IOs.

## IO memory regions

Getting back to our simple MIPS board example:

```c
void mips_r4k_init(MachineState *machine)
{
...
    MemoryRegion *iomem = g_new(MemoryRegion, 1);

    memory_region_init_io(iomem, NULL, &mips_qemu_ops, NULL, "mips-qemu", 0x10000);
    memory_region_add_subregion(sys_mem, 0x1fbf0000, iomem);
}
```

A new memory region `iomem` is created with
[`memory_region_init_io()`](https://github.com/qemu/qemu/blob/v4.2.0/memory.c#L1490)
and also added as a `subregion` of the `system memory`. This region is
not of RAM but IO type and has a special
[`MemoryRegionOps`](https://github.com/qemu/qemu/blob/v4.2.0/include/exec/memory.h#L144)
argument.

```c
static const MemoryRegionOps mips_qemu_ops = {
    .read = mips_qemu_read,
    .write = mips_qemu_write,
    .endianness = DEVICE_NATIVE_ENDIAN,
};

static void mips_qemu_write (void *opaque, hwaddr addr,
                             uint64_t val, unsigned size)
{
    if ((addr & 0xffff) == 0 && val == 42)
        qemu_system_reset_request(SHUTDOWN_CAUSE_GUEST_RESET);
...
}

static uint64_t mips_qemu_read (void *opaque, hwaddr addr,
                                unsigned size)
{
    return 0;
}
```

IO memory regions expose devices memory. They usually need special
interpretation during read/write accesses to simulate the expected
device behavior. Using `MemoryRegionOps` callback helps you implement
device operations.

In the previous example, the `iomem` region is mapped at `0x1fbf0000 -
0x1fc00000`. Whenever the VM accesses this memory range, the
read/write callbacks will be called. The `addr` argument is an offset
from the beginning of the related memory region.

So doing something like `*(u8*)0x1fbf0000 = 42` on this MIPS board
triggers a system reset.


## What about the CPIOM SDRAM

Sometimes you may like to combine both for a specific device. Part of
the device memory has no special meaning and is used for transactions,
while a small range might be interpreted as device registers.

```c
static void cpiom_sdram_init(Object *obj)
{
    cpiom_sdram_state_t *s = CPIOM_SDRAM(obj);
    SysBusDevice        *d = SYS_BUS_DEVICE(obj);
    MemoryRegion        *sysmem = get_system_memory();

    memory_region_allocate_system_memory(&s->ram, obj,
                                         CPIOM_SDRAM_NAME,
                                         CPIOM_MMAP_SDRAM_SIZE);
    memory_region_init_io(&s->reg, obj, &sdram_reg_ops, s,
                          CPIOM_SDRAM_NAME"-reg",
                          CPIOM_MMAP_SDRAM_REG_SIZE);
    memory_region_init_io(&s->pcode.mmio, obj, &sdram_pcode_ops, s,
                          CPIOM_SDRAM_NAME"-pcode",
                          CPIOM_MMAP_SDRAM_PCODE_SIZE);
    memory_region_init_io(&s->pdata.mmio, obj, &sdram_pdata_ops, s,
                          CPIOM_SDRAM_NAME"-pdata",
                          CPIOM_MMAP_SDRAM_PDATA_SIZE);
    memory_region_init_io(&s->raf, obj, &sdram_raf_ops, s,
                          CPIOM_SDRAM_NAME"-raf", 1*4);

    memory_region_add_subregion(sysmem, 0, &s->ram);
    sysbus_init_mmio(d, &s->reg);
    memory_region_add_subregion(sysmem,
                                CPIOM_MMAP_SDRAM_PCODE, &s->pcode.mmio);
    memory_region_add_subregion(sysmem,
                                CPIOM_MMAP_SDRAM_PDATA, &s->pdata.mmio);

    sysbus_init_irq(d, &s->irq_viol);
    sysbus_init_irq(d, &s->irq_err);
}
```

In this more complex example, the CPIOM SDRAM is a fully fledged QEMU
device object with several memory regions. Some memory regions (`reg`
and `raf`) are not directly added as subregions to the system
memory. We will explain why in the next post dedicated to device
creation.

The CPIOM SDRAM controller offers protection for code and data, as
well as refresh rate tunning. It is also able to raise interrupt
requests.

```

`runstate.md`:

```md
# A deep dive into QEMU: VM running states

This blog post details how the different running states of the virtual
machine (guest) are handled internally.

As you may imagine, a virtual machine and especially its virtual CPU
goes through different running states during its lifetime:

```c
/* from QAPI types */
typedef enum RunState {
    RUN_STATE_DEBUG,
    RUN_STATE_INMIGRATE,
    RUN_STATE_INTERNAL_ERROR,
    RUN_STATE_IO_ERROR,
    RUN_STATE_PAUSED,
    RUN_STATE_POSTMIGRATE,
    RUN_STATE_PRELAUNCH,
    RUN_STATE_FINISH_MIGRATE,
    RUN_STATE_RESTORE_VM,
    RUN_STATE_RUNNING,
    RUN_STATE_SAVE_VM,
    RUN_STATE_SHUTDOWN,
    RUN_STATE_SUSPENDED,
    RUN_STATE_WATCHDOG,
    RUN_STATE_GUEST_PANICKED,
    RUN_STATE_COLO,
    RUN_STATE_PRECONFIG,
    RUN_STATE__MAX,
} RunState;
```

These
[`RunState`](https://github.com/qemu/qemu/blob/v4.2.0/qapi/run-state.json)
are handled in the QEMU
[`main_loop`](https://github.com/qemu/qemu/tree/v4.2.0/vl.c#L1801)
which executes in the QEMU startup thread, not the ones dedicated to
[virtual
CPUs](https://github.com/qemu/qemu/tree/v4.2.0/cpus.c#L2134). The
function just wait for event requests to be processed in
[`main_loop_should_exit`](https://github.com/qemu/qemu/tree/v4.2.0/vl.c#L1743). They
are generally raised from the virtual CPU threads:

```c
static void main_loop(void)
{
...
    while (!main_loop_should_exit()) {
        main_loop_wait(false);
    }
}

static bool main_loop_should_exit(void)
{
    RunState r;
...
    if (qemu_debug_requested()) {
        vm_stop(RUN_STATE_DEBUG);
    }
    if (qemu_suspend_requested()) {
        qemu_system_suspend();
    }
...
    if (qemu_powerdown_requested()) {
        qemu_system_powerdown();
    }
    if (qemu_vmstop_requested(&r)) {
        vm_stop(r);
    }
...
}
```

If you remember the [breakpoints handling post](brk.md), while
handling the debug exception being raised, QEMU prepared a **debug
request** to the main loop from
[`cpu_handle_guest_debug`](https://github.com/qemu/qemu/tree/v4.2.0/cpus.c#L1141):

```c
static void cpu_handle_guest_debug(CPUState *cpu)
{
    gdb_set_stop_cpu(cpu);
    qemu_system_debug_request();
    cpu->stopped = true;
}
```

The
[`qemu_system_debug_request`](https://github.com/qemu/qemu/tree/v4.2.0/vl.c#L1737)
actually triggers an event notification to the main loop:

```c
void qemu_system_debug_request(void)
{
    debug_requested = 1;
    qemu_notify_event();
}
```

Back to the main loop, QEMU checks for a debug event with
[`qemu_debug_requested`](https://github.com/qemu/qemu/tree/v4.2.0/vl.c#L1527)
and in that case changes the virtual machine running state to that of
the related event with
[`vm_stop`](https://github.com/qemu/qemu/tree/v4.2.0/cpus.c#L2161):

```c
static bool main_loop_should_exit(void)
{
...
    if (qemu_debug_requested()) {
        vm_stop(RUN_STATE_DEBUG);
...
}

int vm_stop(RunState state)
{
    if (qemu_in_vcpu_thread()) {
        qemu_system_vmstop_request_prepare();
        qemu_system_vmstop_request(state);
        /*
         * FIXME: should not return to device code in case
         * vm_stop() has been requested.
         */
        cpu_stop_current();
        return 0;
    }

    return do_vm_stop(state, true);
}

```

The [`vm_stop`](https://github.com/qemu/qemu/tree/v4.2.0/cpus.c#L2161)
function checks where it is called from. If for instance a virtual CPU
calls it during the emulation of an instruction, a **stop request** is
raised instead of handling the run state transition directly. Because
state transitions **only happen** in the QEMU main loop thread.

Obviously, there exists the opposite service to start/resume a VM:
[`vm_start`](https://github.com/qemu/qemu/tree/v4.2.0/cpus.c#L2211).


## VMState change handlers

The real state transition service is
[`do_vm_stop`](https://github.com/qemu/qemu/tree/v4.2.0/cpus.c#L1096)
and we can see it implements all the low level mechanics:

```c
static int do_vm_stop(RunState state, bool send_stop)
{
    int ret = 0;

    if (runstate_is_running()) {
        cpu_disable_ticks();
        pause_all_vcpus();
        runstate_set(state);
        vm_state_notify(0, state);
        if (send_stop) {
            qapi_event_send_stop();
        }
    }

    bdrv_drain_all();
    ret = bdrv_flush_all();

    return ret;
}
```

QEMU stops the vCPU, tick counting and associated virtual clocks. It
also calls a special service:
[`vm_state_notify`](https://github.com/qemu/qemu/tree/v4.2.0/vl.c#L1423)
with the new running state of the VM as argument. Interestingly, we
are able to register callbacks to get notified of every VM running
state change thanks to
[`qemu_add_vm_change_state_handler`](https://github.com/qemu/qemu/tree/v4.2.0/vl.c#L1411):

```c
VMChangeStateEntry *qemu_add_vm_change_state_handler_prio(
        VMChangeStateHandler *cb, void *opaque, int priority)
{
    VMChangeStateEntry *e;
    VMChangeStateEntry *other;

    e = g_malloc0(sizeof(*e));
    e->cb = cb;
    e->opaque = opaque;
    e->priority = priority;

    /* Keep list sorted in ascending priority order */
    QTAILQ_FOREACH(other, &vm_change_state_head, entries) {
        if (priority < other->priority) {
            QTAILQ_INSERT_BEFORE(other, e, entries);
            return e;
        }
    }

    QTAILQ_INSERT_TAIL(&vm_change_state_head, e, entries);
    return e;
}

void vm_state_notify(int running, RunState state)
{
    VMChangeStateEntry *e, *next;

    trace_vm_state_notify(running, state, RunState_str(state));

    if (running) {
        QTAILQ_FOREACH_SAFE(e, &vm_change_state_head, entries, next) {
            e->cb(e->opaque, running, state);
        }
    } else {
        QTAILQ_FOREACH_REVERSE_SAFE(e, &vm_change_state_head, entries, next) {
            e->cb(e->opaque, running, state);
        }
    }
}
```

This is extremely convenient. As an example, the [GDB server
stub](https://github.com/qemu/qemu/tree/v4.2.0//gdbstub.c#L3365) does
register a callback to intercept debug events and check for gdb client
breakpoints:

```c
int gdbserver_start(const char *device)
{
...
    qemu_add_vm_change_state_handler(gdb_vm_state_change, NULL);
...
}

static void gdb_vm_state_change(void *opaque, int running, RunState state)
{
...
    switch (state) {
    case RUN_STATE_DEBUG:
        ...
        ret = GDB_SIGNAL_TRAP;
        break;
...
}
```

## Asynchronous handling of running states

We have seen that we can't change running state from every where. We
should rather request for a change.

This is especially true for some events. Consider you implement a
clock device which uses QEMU internal timers. The virtual clock timers
expiration is processed in the QEMU main loop and the associated
callbacks are called from that thread.

In that context, we **should not** try to stop the VM because we have
seen that the
[`do_vm_stop`](https://github.com/qemu/qemu/tree/v4.2.0/cpus.c#L1096)
service will try to disable all vCPU associated virtual clocks. And
*disabling the clocks will wait for related timerlists to stop* as
stated in the [~~documentation~~
code](https://github.com/qemu/qemu/tree/v4.2.0/util/qemu-timer.c#L148). This
will lead to a **dead-lock**.

The correct way to proceed is to request a state transition from the
timer callback itself:

```c
void my_user_timeout_cb(void *opaque)
{
   debug("--> vm timeout()\n");
   qemu_system_vmstop_request_prepare();
   qemu_system_vmstop_request(RUN_STATE_PAUSED);
}
```

And in your vm change state handler, **aysnchronously** deal with that
state thanks to
[`async_run_on_cpu`](https://github.com/qemu/qemu/tree/v4.2.0/cpus-common.c#L149):

```c
void my_vm_state_change(void *opaque, int running, RunState state)
{
   debug("vm state %s\n", RunState_str(state));

   if (state == RUN_STATE_PAUSED) {
      async_run_on_cpu(cpu, my_async_timeout_vm, RUN_ON_CPU_HOST_PTR(arg));
      return;
   }
```

This way, the `my_async_timeout_vm` function is added into the given
`cpu` work queue as a new
[`qemu_work_item`](https://github.com/qemu/qemu/tree/v4.2.0/cpus-common.c#L101)
and will be called out of the main loop context. It is safe to
consider your VM in the requested state (PAUSED) now and try to resume
it with
[`vm_start`](https://github.com/qemu/qemu/tree/v4.2.0/cpus.c#L2211)
for instance.

```

`snapshot.md`:

```md
# A deep dive into QEMU: snapshot API

This blog post gives some insights on the QEMU snapshot API.

## Overview

The QEMU monitor exposes commands to create and restore snapshots of
your running VM: `savevm` and `loadvm`. QEMU offers advanced features
such as live migration that we won't deal with in this article.

However, the principle is based on the ability to save a complete and
restorable state of your virtual machine including its vCPU, RAM and
devices.

If we look at the service involved internally when invoking the
[`savevm`](https://github.com/qemu/qemu/tree/v4.2.0/monitor/hmp-cmds.c)
command we will land in
[`save_snapshot`](https://github.com/qemu/qemu/tree/v4.2.0/migration/savevm.c#L2619):

```c
void hmp_savevm(Monitor *mon, const QDict *qdict)
{
    Error *err = NULL;

    save_snapshot(qdict_get_try_str(qdict, "name"), &err);
    hmp_handle_error(mon, &err);
}


int save_snapshot(const char *name, Error **errp)
{
...
    if (!bdrv_all_can_snapshot(&bs)) {
        error_setg(errp, "Device '%s' is writable but does not support "
                   "snapshots", bdrv_get_device_name(bs));
        return ret;
    }

...
    ret = global_state_store();
    if (ret) {
        error_setg(errp, "Error saving global state");
        return ret;
    }
    vm_stop(RUN_STATE_SAVE_VM);

...
    f = qemu_fopen_bdrv(bs, 1);
    if (!f) {
        error_setg(errp, "Could not open VM state file");
        goto the_end;
    }
    ret = qemu_savevm_state(f, errp);
    vm_state_size = qemu_ftell(f);
    qemu_fclose(f);
    if (ret < 0) {
        goto the_end;
    }

...
    if (saved_vm_running) {
        vm_start();
    }
}
```

A lot of interesting things there. First the snapshot API is a file
based API. Second, your board' block devices (if any) must be
*snapshotable*. Third, there exists special [running
states](runstate.md) related to snapshoting.

The block device *snapshot-ability* is specific and mainly related to
device read-only mode. However, the most important part of the
[`save_snapshot`](https://github.com/qemu/qemu/tree/v4.2.0/migration/savevm.c#L2619)
function is tied to
[`qemu_savevm_state`](https://github.com/qemu/qemu/tree/v4.2.0/migration/savevm.c#L1501)
which will proceed through every device
[`VMStateDescription`](https://github.com/qemu/qemu/tree/v4.2.0/include/migration/vmstate.h#L176)
thanks to
[`vmstate_save_state_v`](https://github.com/qemu/qemu/tree/v4.2.0/migration/vmstate.c#L323)


## Preparing your devices

To be snapshotable, a device must expose through a
[`VMStateDescription`](https://github.com/qemu/qemu/tree/v4.2.0/include/migration/vmstate.h#L176),
all of its internal state fields that must be saved and restored
during snapshot handling.

Obviously, it's a device based implementation. Usually, configured IO
registers are saved to preserve what drivers may have done during
initialisation.

Let's take an example based on [our timer device](timers.md) implementation:

```c
static const VMStateDescription vmstate_cpiom_timer = {
    .name = CPIOM_TIMERS_NAME,
    .version_id = 1,
    .minimum_version_id = 1,
    .post_load = cpiom_timer_post_load,
    .fields = (VMStateField[]) {
        VMSTATE_UINT32(reg.base.ctrl, cpiom_timer_state_t),
        VMSTATE_UINT32(reg.base.prescal, cpiom_timer_state_t),
        VMSTATE_UINT32(reg.base.period, cpiom_timer_state_t),
        VMSTATE_UINT32(reg.base.counter, cpiom_timer_state_t),
        VMSTATE_UINT32(reg.base.cycle, cpiom_timer_state_t),
        VMSTATE_UINT32(reg.base.slice_end, cpiom_timer_state_t),
        VMSTATE_UINT32(reg.base.slice_out, cpiom_timer_state_t),
        VMSTATE_UINT32(reg.base.tick, cpiom_timer_state_t),
        VMSTATE_UINT32(reg.base.win_begin, cpiom_timer_state_t),
        VMSTATE_UINT32(reg.base.win_end, cpiom_timer_state_t),
        VMSTATE_UINT32(reg.base.win_sts, cpiom_timer_state_t),
        VMSTATE_UINT32(reg.base.win_sav_sts, cpiom_timer_state_t),
        VMSTATE_UINT32(reg.conf.rtc_period, cpiom_timer_state_t),
        VMSTATE_UINT32(reg.conf.cpt_rtc, cpiom_timer_state_t),
        VMSTATE_END_OF_LIST()
    },
};

static void cpiom_timer_class_init(ObjectClass *klass, void *data)
{
    DeviceClass *dc = DEVICE_CLASS(klass);

    dc->vmsd = &vmstate_cpiom_timer;
    dc->reset = cpiom_timer_reset;
    dc->desc = CPIOM_TIMERS_NAME;
}
```

As you can see, the `VMStateDescription` exposes all the internal
registers of the timer so that they get automatically saved and
restored during snapshots.

The `vmsd` field of the `DeviceClass` is used during device
realization by QEMU
[`qdev`](https://github.com/qemu/qemu/tree/v4.2.0/hw/core/qdev.c#L893)
API with
[`vmstate_register`](https://github.com/qemu/qemu/tree/v4.2.0/migration/savevm.c#L787).

If you look at QEMU git tree existing devices implementation, you will
find more complex definitions with
[`VMSTATE_PCI_DEVICE`](https://github.com/qemu/qemu/tree/v4.2.0/include/hw/pci/pci.h#L847)
or
[`VMSTATE_STRUCT`](https://github.com/qemu/qemu/tree/v4.2.0/include/migration/vmstate.h#L816)
declarations. Devices may also inherit higher-level/generic
`VMStateDescription` such as
[`serial-isa`](https://github.com/qemu/qemu/tree/v4.2.0/hw/char/serial-isa.c#L85):

```c
const VMStateDescription vmstate_serial = {
    .name = "serial",
    .version_id = 3,
    .minimum_version_id = 2,
    .pre_save = serial_pre_save,
    .pre_load = serial_pre_load,
    .post_load = serial_post_load,
    .fields = (VMStateField[]) {
        VMSTATE_UINT16_V(divider, SerialState, 2),
        VMSTATE_UINT8(rbr, SerialState),
        VMSTATE_UINT8(ier, SerialState),
        VMSTATE_UINT8(iir, SerialState),
        VMSTATE_UINT8(lcr, SerialState),
        VMSTATE_UINT8(mcr, SerialState),
        VMSTATE_UINT8(lsr, SerialState),
        VMSTATE_UINT8(msr, SerialState),
        VMSTATE_UINT8(scr, SerialState),
        VMSTATE_UINT8_V(fcr_vmstate, SerialState, 3),
        VMSTATE_END_OF_LIST()
    },
    .subsections = (const VMStateDescription*[]) {
        &vmstate_serial_thr_ipending,
        &vmstate_serial_tsr,
        &vmstate_serial_recv_fifo,
        &vmstate_serial_xmit_fifo,
        &vmstate_serial_fifo_timeout_timer,
        &vmstate_serial_timeout_ipending,
        &vmstate_serial_poll,
        NULL
    }
};

static const VMStateDescription vmstate_isa_serial = {
    .name = "serial",
    .version_id = 3,
    .minimum_version_id = 2,
    .fields = (VMStateField[]) {
        VMSTATE_STRUCT(state, ISASerialState, 0, vmstate_serial, SerialState),
        VMSTATE_END_OF_LIST()
    }
};
```

## Lower level handlers

The internals of the VMState save/load API is backed by
[`SaveStateEntry`](https://github.com/qemu/qemu/tree/v4.2.0/migration/savevm.c#L233)
fields. When you register a `vmsd` with
[`vmstate_register`](https://github.com/qemu/qemu/tree/v4.2.0/migration/savevm.c#L787),
a new [`SaveStateEntry` is
created](https://github.com/qemu/qemu/tree/v4.2.0/migration/savevm.c#L798):

```c
int vmstate_register_with_alias_id(DeviceState *dev, int instance_id,
                                   const VMStateDescription *vmsd,
                                   void *opaque, int alias_id,
                                   int required_for_version,
                                   Error **errp)
{
    SaveStateEntry *se;
...
    se = g_new0(SaveStateEntry, 1);
    se->version_id = vmsd->version_id;
    se->section_id = savevm_state.global_section_id++;
    se->opaque = opaque;
    se->vmsd = vmsd;
...
    savevm_state_handler_insert(se);
    return 0;
}
```

The
[`qemu_savevm_state`](https://github.com/qemu/qemu/tree/v4.2.0/migration/savevm.c#L1525)
function will iterate through the list of `SaveStateEntries` and call
their associated [`SaveVMHandlers
*ops`](https://github.com/qemu/qemu/tree/v4.2.0/include/migration/register.h#L17)
respectively from
[`qemu_savevm_state_setup`](https://github.com/qemu/qemu/tree/v4.2.0/migration/savevm.c#L1148)
and
[`qemu_savevm_state_iterate`](https://github.com/qemu/qemu/tree/v4.2.0/migration/savevm.c#L1210).

```c
typedef struct SaveVMHandlers {
...
    void (*save_cleanup)(void *opaque);
    int (*save_live_complete_postcopy)(QEMUFile *f, void *opaque);
    int (*save_live_complete_precopy)(QEMUFile *f, void *opaque);
...
} SaveVMHandlers;
```

Depending on the fact that a device has a `vmsd` or not when
registering to the `vmstate` API or loading a snapshot file, QEMU
might call either the `SaveVMHandlers` from the `SaveStateEntry` or
lower level functions such as
[`vmstate_save_state_v`](https://github.com/qemu/qemu/tree/v4.2.0/migration/vmstate.c#L323)
or
[`vmstate_load_state`](https://github.com/qemu/qemu/tree/v4.2.0/migration/vmstate.c#L78)
as we can see for instance in
[`vmstate_load`](https://github.com/qemu/qemu/tree/v4.2.0/migration//savevm.c#L851):

```c
static int vmstate_load(QEMUFile *f, SaveStateEntry *se)
{
    trace_vmstate_load(se->idstr, se->vmsd ? se->vmsd->name : "(old)");
    if (!se->vmsd) {         /* Old style */
        return se->ops->load_state(f, se->opaque, se->load_version_id);
    }
    return vmstate_load_state(f, se->vmsd, se->opaque, se->load_version_id);
}
```

## How the RAM is snapshoted ?

Sometimes it's hard to find your way into the QEMU code, with all that
function pointers initialized you don't know where depending on some
other fields value :)

Using a debugger might speed-up the proces ! Let's use it to
understand how RAM is snapshoted.

First,
[`memory_region_allocate_system_memory`](https://github.com/qemu/qemu/tree/v4.2.0/numa.c#L557)
registers something related to a vmstate with
[`vmstate_register_ram`]():

```c
void vmstate_register_ram(MemoryRegion *mr, DeviceState *dev)
{
    qemu_ram_set_idstr(mr->ram_block,
                       memory_region_name(mr), dev);
    qemu_ram_set_migratable(mr->ram_block);
}
```

And ... *cool story bro' !*


We should better try to break into
[`qemu_savevm_state_setup`](https://github.com/qemu/qemu/tree/v4.2.0/migration//savevm.c#L1501):

```shell
Breakpoint 2, qemu_savevm_state_setup (f=0x555556bde230)

(gdb) print se->idstr 
$6 = "ram", '\000' <repeats 252 times>

(gdb) print se->vmsd
$7 = (const VMStateDescription *) 0x0

(gdb) print se->opaque
$8 = (void *) 0x5555565fabd8 <ram_state>

(gdb) print se->is_ram 
$9 = 1

(gdb) print se->ops 
$10 = (SaveVMHandlers *) 0x55555648eb20 <savevm_ram_handlers>
(gdb) print se->ops->save_cleanup
$11 = (void (*)(void *)) 0x55555580fc9e <ram_save_cleanup>
(gdb) print se->ops->has_postcopy 
$12 = (_Bool (*)(void *)) 0x5555558125a5 <ram_has_postcopy>
(gdb) print se->ops->save_setup 
$13 = (int (*)(QEMUFile *, void *)) 0x555555810b61 <ram_save_setup>
(gdb) print se->ops->save_state 
$14 = (SaveStateHandler *) 0x0
(gdb) print se->ops->load_state 
$15 = (LoadStateHandler *) 0x555555811f3f <ram_load>
```

The RAM `SaveStateEntry` does not have any `vmsd` and its `opaque`
field is initialized to a
[`RAMState`](https://github.com/qemu/qemu/tree/v4.2.0/migration/ram.c#L306)
object that will be used by the specific [`RAM
SaveVMHandlers`](https://github.com/qemu/qemu/tree/v4.2.0/migration/ram.c#L4577).

Now we have function names to look at `migrate/ram.c`. We won't detail
the code from here. The interested reader might deep-dive into it and
discover the QEMU strategy looking for dirty memory pages to save to
optimize footprint.

```

`tcg_p1.md`:

```md
# A deep dive into QEMU: The Tiny Code Generator (TCG), part 1

This blog post details some internals of the QEMU TCG engine, the
machinery responsible for executing target instructions on the host.

You should have already read [Execution loop](exec.md) and
[Breakpoints handling](brk.md) blog posts to have some pointers.


## Be kind rewind

The vCPU thread executes instructions through
[`tcg_cpu_exec`](https://github.com/qemu/qemu/tree/v4.2.0/cpus.c#L1461)
which finds and/or generates translated blocks.

As previously explained,
[`tb_gen_code`](https://github.com/qemu/qemu/blob/v4.2.0/accel/tcg/translate-all.c#L1734)
will generate *intermediate representation* (IR) code thanks to
[`gen_intermediate_code`](https://github.com/qemu/qemu/blob/v4.2.0/target/i386/translate.c#L8615)
and then host architecture assembly instructions with
[`tcg_gen_code`](https://github.com/qemu/qemu/blob/v4.2.0/tcg/tcg.c#L4013):

```c
TranslationBlock *tb_gen_code(CPUState *cpu,
                              target_ulong pc, target_ulong cs_base,
                              uint32_t flags, int cflags)
{
    tb = tb_alloc(pc);
...
    /* generate IR code */
    gen_intermediate_code(cpu, tb, max_insns);
...
    /* generate machine code */
    gen_code_size = tcg_gen_code(tcg_ctx, tb);
...
}
```

The QEMU TCG has a notion of
[frontend](https://wiki.qemu.org/Documentation/TCG/frontend-ops) and
[backend](https://wiki.qemu.org/Documentation/TCG/backend-ops)
operations. The *frontend-ops* are the generated intermediate code
which is **what** QEMU will execute. The *backend-ops* are the
operations implemented on the host CPU, which is **where** the code is
executed.


## Generating Intermediate Representation (IR)

The QEMU git tree has a
[README](https://github.com/qemu/qemu/blob/v4.2.0/tcg/README)
introduction about the TCG which details the IR language. It's an
interesting read for sure.

### Overview

The
[`gen_intermediate_code`](https://github.com/qemu/qemu/blob/v4.2.0/target/i386/translate.c#L8615)
function is a VM architecture dependent wrapper to the
[`translator_loop`](https://github.com/qemu/qemu/blob/v4.2.0/accel/tcg/translator.c#L34)
generic function. Let's imagine we were emulating PowerPC code on an
Intel x86 host.

The
[`gen_intermediate_code`](https://github.com/qemu/qemu/blob/v4.2.0/target/ppc/translate.c#L7963)
would look like:

```c
static const TranslatorOps ppc_tr_ops = {
    .init_disas_context = ppc_tr_init_disas_context,
    .tb_start           = ppc_tr_tb_start,
    .insn_start         = ppc_tr_insn_start,
    .breakpoint_check   = ppc_tr_breakpoint_check,
    .translate_insn     = ppc_tr_translate_insn,
    .tb_stop            = ppc_tr_tb_stop,
    .disas_log          = ppc_tr_disas_log,
};

void gen_intermediate_code(CPUState *cs, TranslationBlock *tb, int max_insns)
{
    DisasContext ctx;

    translator_loop(&ppc_tr_ops, &ctx.base, cs, tb, max_insns);
}
```

While the
[`translator_loop`](https://github.com/qemu/qemu/blob/v4.2.0/accel/tcg/translator.c#L34)
remains the same for any architecture, it relies on target specific
[translator
operators](https://github.com/qemu/qemu/blob/v4.2.0/include/exec/translator.h#L80). In
our case, the PowerPC ones (`ppc_tr_ops`).

```c
void translator_loop(const TranslatorOps *ops, DisasContextBase *db,
                     CPUState *cpu, TranslationBlock *tb, int max_insns)
{
    ops->init_disas_context(db, cpu);
...
    gen_tb_start(db->tb);
    ops->tb_start(db, cpu);

    while (true) {
        ops->translate_insn(db, cpu);
    }
...
    ops->tb_stop(db, cpu);
    gen_tb_end(db->tb, db->num_insns - bp_insn);
}
```

Each TB has a prologue (`tb_start`), and an epilogue (`tb_end`) with a
generic and target specific part (if needed). Usually epilogues are
placeholders for block chaining, which is an optimization feature of
the TCG that enables TBs to be called successively after execution,
without the need to get back to the QEMU code and look for the next TB
to execute.

### Disassembly context

The architecture dependent
[`DisasContext`](https://github.com/qemu/qemu/blob/v4.2.0/target/ppc/translate.c#L155)
is created alongside a generic
[`DisasContextBase`](https://github.com/qemu/qemu/blob/v4.2.0/include/exec/translator.h#L56).

For the PPC target, the
[`ppc_tr_init_disas_context`](https://github.com/qemu/qemu/blob/v4.2.0/target/ppc/translate.c#L7735)
handler will record current cpu state information. This means that TBs
are highly contextual and might not be reused in any situation where
we execute at a previously translated location.

```c
static void ppc_tr_init_disas_context(DisasContextBase *dcbase, CPUState *cs)
{
    DisasContext *ctx = container_of(dcbase, DisasContext, base);
    CPUPPCState *env = cs->env_ptr;
    int bound;

    ctx->exception = POWERPC_EXCP_NONE;
    ctx->spr_cb = env->spr_cb;
    ctx->pr = msr_pr;
    ctx->mem_idx = env->dmmu_idx;
    ctx->dr = msr_dr;
...
    ctx->insns_flags = env->insns_flags;
    ctx->insns_flags2 = env->insns_flags2;
    ctx->access_type = -1;
    ctx->need_access_type = !(env->mmu_model & POWERPC_MMU_64B);
    ctx->le_mode = !!(env->hflags & (1 << MSR_LE));
    ctx->default_tcg_memop_mask = ctx->le_mode ? MO_LE : MO_BE;
    ctx->flags = env->flags;
...
```

### Translated Block prologue/epilogue

The generic TB prologue is generated from
[`gen_tb_start`](https://github.com/qemu/qemu/blob/v4.2.0/include/exec/gen-icount.h#L35)

```c
static inline void gen_tb_start(TranslationBlock *tb)
{
    TCGv_i32 count, imm;

    tcg_ctx->exitreq_label = gen_new_label();
    if (tb_cflags(tb) & CF_USE_ICOUNT) {
        count = tcg_temp_local_new_i32();
    } else {
        count = tcg_temp_new_i32();
    }

    tcg_gen_ld_i32(count, cpu_env,
                   -ENV_OFFSET + offsetof(CPUState, icount_decr.u32));

    if (tb_cflags(tb) & CF_USE_ICOUNT) {
        imm = tcg_temp_new_i32();
        /* We emit a movi with a dummy immediate argument. Keep the insn index
         * of the movi so that we later (when we know the actual insn count)
         * can update the immediate argument with the actual insn count.  */
        tcg_gen_movi_i32(imm, 0xdeadbeef);
        icount_start_insn = tcg_last_op();

        tcg_gen_sub_i32(count, count, imm);
        tcg_temp_free_i32(imm);
    }

    tcg_gen_brcondi_i32(TCG_COND_LT, count, 0, tcg_ctx->exitreq_label);

    if (tb_cflags(tb) & CF_USE_ICOUNT) {
        tcg_gen_st16_i32(count, cpu_env,
                         -ENV_OFFSET + offsetof(CPUState, icount_decr.u16.low));
    }

    tcg_temp_free_i32(count);
}
```

It injects instructions to check for instruction count and an exit
condition.

The epilogue is generated from
[`gen_tb_end`](https://github.com/qemu/qemu/blob/v4.2.0/include/exec/gen-icount.h#L74). It
injects instructions to exit from the TB (`tcg_gen_exit_tb`).

```c
static inline void gen_tb_end(TranslationBlock *tb, int num_insns)
{
    if (tb_cflags(tb) & CF_USE_ICOUNT) {
        /* Update the num_insn immediate parameter now that we know
         * the actual insn count.  */
        tcg_set_insn_param(icount_start_insn, 1, num_insns);
    }

    gen_set_label(tcg_ctx->exitreq_label);
    tcg_gen_exit_tb(tb, TB_EXIT_REQUESTED);
}
```

The `TB_EXIT_REQUESTED` is a special value that tells QEMU to use the
`exitreq_label` created during the prologue. This label will
eventually be updated with the address of the next TB to be executed.


### Translating instructions

The operator used to translate target instructions to IR is
`translate_insn`
([`ppc_tr_translate_insn`](https://github.com/qemu/qemu/blob/v4.2.0/target/ppc/translate.c#L7845)
for PowerPC). This function makes use of the target CPU
[`opcodes`](https://github.com/qemu/qemu/blob/v4.2.0/target/ppc/translate.c#L6883)
handlers table which implements IR generation for every target native
instructions.

```c
static void ppc_tr_translate_insn(DisasContextBase *dcbase, CPUState *cs)
{
    opc_handler_t **table, *handler;

    table = cpu->opcodes;
    handler = table[opc1(ctx->opcode)];
...
    (*(handler->handler))(ctx);
}

struct PowerPCCPU {
...
    /* Those resources are used only during code translation */
    /* opcode handlers */
    opc_handler_t *opcodes[PPC_CPU_OPCODES_LEN];
...
}

static opcode_t opcodes[] = {
GEN_HANDLER(cmp, 0x1F, 0x00, 0x00, 0x00400000, PPC_INTEGER),
GEN_HANDLER(cmpi, 0x0B, 0xFF, 0xFF, 0x00400000, PPC_INTEGER),
GEN_HANDLER(cmpl, 0x1F, 0x00, 0x01, 0x00400001, PPC_INTEGER),
...
GEN_LDS(lbz, ld8u, 0x02, PPC_INTEGER)
GEN_LDS(lha, ld16s, 0x0A, PPC_INTEGER)
...
};
```

For the PowerPC `cmp` instruction, the associated handler will be
[`gen_cmp`](https://github.com/qemu/qemu/blob/v4.2.0/target/ppc/translate.c#L666)
which will emit the following IR instructions:

```c
static void gen_cmp(DisasContext *ctx)
{
    if ((ctx->opcode & 0x00200000) && (ctx->insns_flags & PPC_64B)) {
        gen_op_cmp(cpu_gpr[rA(ctx->opcode)], cpu_gpr[rB(ctx->opcode)],
                   1, crfD(ctx->opcode));
    } else {
        gen_op_cmp32(cpu_gpr[rA(ctx->opcode)], cpu_gpr[rB(ctx->opcode)],
                     1, crfD(ctx->opcode));
    }
}

static inline void gen_op_cmp(TCGv arg0, TCGv arg1, int s, int crf)
{
    TCGv t0 = tcg_temp_new();
    TCGv t1 = tcg_temp_new();
    TCGv_i32 t = tcg_temp_new_i32();

    tcg_gen_movi_tl(t0, CRF_EQ);
    tcg_gen_movi_tl(t1, CRF_LT);
    tcg_gen_movcond_tl((s ? TCG_COND_LT : TCG_COND_LTU),
                       t0, arg0, arg1, t1, t0);
    tcg_gen_movi_tl(t1, CRF_GT);
    tcg_gen_movcond_tl((s ? TCG_COND_GT : TCG_COND_GTU),
                       t0, arg0, arg1, t1, t0);

    tcg_gen_trunc_tl_i32(t, t0);
    tcg_gen_trunc_tl_i32(cpu_crf[crf], cpu_so);
    tcg_gen_or_i32(cpu_crf[crf], cpu_crf[crf], t);

    tcg_temp_free(t0);
    tcg_temp_free(t1);
    tcg_temp_free_i32(t);
}
```

#### Example PowerPC basic block translation

The PowerPC code here after shows 3 types of operations:
- simple instructions: arithmetic, immediate operands (`lis, ori, xor`)
- a memory write (`stw`)
- a system register write (`mtmsr`)

```asm
0xfff00100:  lis    r1,1

0xfff00104:  ori    r1,r1,0x409c

0xfff00108:  xor    r0,r0,r0

0xfff0010c:  stw    r0,4(r1)

0xfff00110:  mtmsr  r0
```

The following code gives the TCG IR equivalent:

```asm
0xfff00100:  movi_i32    r1,$0x10000

0xfff00104:  movi_i32    tmp0,$0x409c
             or_i32      r1,r1,tmp0

0xfff00108:  movi_i32    r0,$0x0

0xfff0010c:  movi_i32    tmp1,$0x4
             add_i32     tmp0,r1,tmp1
             qemu_st_i32 r0,tmp0,beul,3

0xfff00110:  movi_i32    nip,$0xfff00114
             mov_i32     tmp0,r0
             call        store_msr,$0,tmp0
             movi_i32    nip,$0xfff00114
             exit_tb     $0x0
             set_label   $L0
             exit_tb     $0x7f5a0caf8043
```

We notice that the memory write operation as well as the `MSR` access
are translated into *strange* IR opcodes:
- qemu_st_i32
- call store_msr

We will detail them in a blog post dedicated to TCG helpers.

Also notice the operands of the `exit_tb` opcode. The first one is
`$0x0` and might eventually be fixed with the **absolute host
address** of the next TB to be executed.

The last one is also a **host address** (`$0x7f5a0caf8043`) where code
is directly executed by the physical CPU. In this case, it goes back
to the QEMU translator.

At this point however, no *target translated* code is still
executed. We only end up with IR code which needs a final translation
step to the **host** architecture.

```

`tcg_p2.md`:

```md
# A deep dive into QEMU: The Tiny Code Generator (TCG), part 2

This blog post is the second [part](tcg_p1.md) dedicated to the QEMU
TCG engine. It covers TCG IR to host code translation and TBs
execution.

## Generating host code

In the context of the TCG, the terminology changes. The target isn't
the VM anymore but the host from the TCG point of view. Indeed, the
TCG generates assembly code from IR to be run on the host
architecture. So it's natural that its *target* is the host
architecture. It's referred as *tcg-target*.

As such, the *tcg-target* specific code is located at
`tcg/ARCH/tcg-target.inc.c`. For our Intel x86 host, it would be
[`tcg/i386/tcg-target.inc.c`](https://github.com/qemu/qemu/blob/v4.2.0/tcg/i386/tcg-target.inc.c)

The *tcg-target* code is generated thanks to
[`tcg_gen_code`](https://github.com/qemu/qemu/blob/v4.2.0/tcg/tcg.c#L4013):

```c
int tcg_gen_code(TCGContext *s, TranslationBlock *tb)
{
    int i, num_insns;
    TCGOp *op;
...
#ifdef TCG_TARGET_NEED_LDST_LABELS
    QSIMPLEQ_INIT(&s->ldst_labels);
#endif

    tcg_reg_alloc_start(s);

    QTAILQ_FOREACH(op, &s->ops, link) {
        TCGOpcode opc = op->opc;

        switch (opc) {
        case INDEX_op_mov_i32:
        case INDEX_op_mov_i64:
            tcg_reg_alloc_mov(s, op);
...
        case INDEX_op_call:
            tcg_reg_alloc_call(s, op);
            break;
        default:
            tcg_reg_alloc_op(s, op);
            break;
        }
    }
#ifdef TCG_TARGET_NEED_LDST_LABELS
    i = tcg_out_ldst_finalize(s);
    if (i < 0) {
        return i;
    }
#endif
...
}
```

Beside several optimizations and debug code not shown here, the
translation process generates corresponding host opcodes and registers
allocation from IR instructions. We will cover the `ldst` labels in a
blogpost dedicated to TCG memory accesses implementation
(ie. `qemu_ld`, `qemu_st`).

Remember the previous article TCG IR code extract:

```asm
0xfff00100:  movi_i32    r1,$0x10000
             movi_i32    tmp0,$0x409c
             or_i32      r1,r1,tmp0

0xfff00108:  movi_i32    r0,$0x0

0xfff0010c:  movi_i32    tmp1,$0x4
             add_i32     tmp0,r1,tmp1
             qemu_st_i32 r0,tmp0,beul,3

0xfff00110:  movi_i32    nip,$0xfff00114
             mov_i32     tmp0,r0
             call        store_msr,$0,tmp0

             movi_i32    nip,$0xfff00114
             exit_tb     $0x0
             set_label   $L0
             exit_tb     $0x7f5a0caf8043
```

It is translated into the following x86 assembly code:

```asm
0x7f5a0caf810b:  movl     $0x1409c, 4(%rbp)

0x7f5a0caf8112:  xorl     %ebx, %ebx

0x7f5a0caf8114:  movl     %ebx, (%rbp)
0x7f5a0caf8117:  movl     $0x140a0, %r12d
0x7f5a0caf811d:  movl     %r12d, %edi
0x7f5a0caf8129:  addq     0x398(%rbp), %rdi
...
0x7f5a0caf8159:  movq     %rbp, %rdi
0x7f5a0caf815c:  movl     %ebx, %esi
0x7f5a0caf815e:  callq    *0x34(%rip)

0x7f5a0caf8164:  movl     $0xfff00114, 0x16c(%rbp)
0x7f5a0caf8182:  movl     %ebx, %edx
0x7f5a0caf8184:  movl     $0xa3, %ecx
0x7f5a0caf8189:  leaq     -0x41(%rip), %r8
0x7f5a0caf8190:  pushq    %r8
0x7f5a0caf8192:  jmpq     *8(%rip)
0x7f5a0caf8198:  .quad  0x000055d62e46eba0
0x7f5a0caf81a0:  .quad  0x000055d62e3895a0
```

The x86 instruction set allows optimizations toward IR RISC-like
opcodes (ie. load immediate 32 bits value in a single
instruction). The immediate value memory store operation `qemu_st_i32`
has been directly translated at address `0x7f5a0caf8129`.

However, the `call store_msr` is implemented at address
`0x7f5a0caf815e` thanks to `RIP relative` addressing. The call site
locations are mixed within the generated code (`.quad xxxxx`). The
target function address is `0x000055d62e46eba0` which when you
disassemble the QEMU engine binary gives us:

```asm
(gdb) x/i 0x000055d62e46eba0
   0x000055d62e46eba0 <helper_store_msr>: push %r12
```

This is a QEMU *TCG helper* which we'll have a closer look at ... right now !


## Executing translated blocks

Now that we have host assembly code, we can directly run it on our
physical CPU. How does QEMU do that ?

The execution flow of a virtual CPU has been depicted in the
[execution loop blog post](exec.md). The
[`cpu_exec`](https://github.com/qemu/qemu/blob/v4.2.0/accel/tcg/cpu-exec.c#L661)
function calls
[`cpu_loop_exec_tb`](https://github.com/qemu/qemu/blob/v4.2.0/accel/tcg/cpu-exec.c#L611)
which then calls
[`cpu_tb_exec`](https://github.com/qemu/qemu/blob/v4.2.0/accel/tcg/cpu-exec.c#L141)
and finally
[`tcg_qemu_tb_exec`](https://github.com/qemu/qemu/blob/v4.2.0/tcg/tcg.h#L1160)
which is nothing more than a *function pointer cast* to the TB
buffer. Literally, QEMU *calls* the generated code.

However, some parts of the generated code redirect to *special
handlers* for IR operations that couldn't be translated into host
assembly code. For instance the PowerPC write to MSR (`mtmsr`) has no
equivalent Intel x86 instruction. It is a system specific instruction,
not a general purpose one.

The `mtmstr` opcode has been translated into IR `call store_msr` which
results in Intel x86 `callq *0x34(%rip)` redirecting to the QEMU
engine function `helper_store_msr`.


## TCG Helpers

The
[`helper_store_msr`](https://github.com/qemu/qemu/blob/v4.2.0/target/ppc/excp_helper.c#L1013)
function is implemented inside the PowerPC emulation part of
QEMU. That's the trade-off between emulation and virtualization. The
QEMU TCG is a *JIT-compiler* for general purpose instructions found in
any architecture. But once we reach system specific instructions, they
remain *emulated* and emulation requires a lot more *host*
instructions to be executed for a single *guest* instruction.

```c
void helper_store_msr(CPUPPCState *env, target_ulong val)
{
    uint32_t excp = hreg_store_msr(env, val, 0);

    if (excp != 0) {
        CPUState *cs = env_cpu(env);
        cpu_interrupt_exittb(cs);
        raise_exception(env, excp);
    }
}
```

The
[`hreg_store_msr`](https://github.com/qemu/qemu/blob/v4.2.0/target/ppc/helper_regs.h#L114)
implements the behavior of a write to the PowerPC `MSR` register. If
the emulation detects a `CPU exception` condition, QEMU will be able
to generate the event at the virtual CPU level thanks to
[`raise_exception`](https://github.com/qemu/qemu/blob/v4.2.0/target/ppc/excp_helper.c#L990)
which redirects execution back to the [main cpu loop events
handling](exec.md#back-to-events-handling).

Whilst `helper_store_msr` is the last step of the emulation process,
how does QEMU knows that it should translate the PowerPC `mtmsr`
instruction into a call to the `helper_store_msr` TCG helper ?

We already explained how guest instructions are [first
translated](tcg_p1.md#translating-instructions) into
IR. The `opcodes` table has also an entry for our PowerPC system
instruction `mtmsr`:

```c
static opcode_t opcodes[] = {
...
GEN_HANDLER(mtmsr, 0x1F, 0x12, 0x04, 0x001EF801, PPC_MISC),
...
};

#define GEN_HANDLER(name, opc1, opc2, opc3, inval, type)                      \
GEN_OPCODE(name, opc1, opc2, opc3, inval, type, PPC_NONE)

#define GEN_OPCODE(name, op1, op2, op3, invl, _typ, _typ2)                    \
{                                                                             \
    .opc1 = op1,                                                              \
    .opc2 = op2,                                                              \
    .opc3 = op3,                                                              \
    .opc4 = 0xff,                                                             \
    .handler = {                                                              \
        .inval1  = invl,                                                      \
        .type = _typ,                                                         \
        .type2 = _typ2,                                                       \
        .handler = &gen_##name,                                               \
    },                                                                        \
    .oname = stringify(name),                                                 \
}
```

This tells the QEMU translation engine that it should call
[`gen_mtmsr`](https://github.com/qemu/qemu/blob/v4.2.0/target/ppc/translate.c#L4390)
whenever it encounters the `mtmsr` instruction:

```c
static void gen_mtmsr(DisasContext *ctx)
{
    CHK_SV;
...
    gen_update_nip(ctx, ctx->base.pc_next);
    tcg_gen_mov_tl(msr, cpu_gpr[rS(ctx->opcode)]);
    gen_helper_store_msr(cpu_env, msr);
...
}
```

If you remember our [previous
articles](tcg_p1.md#example-powerpc-basic-block-translation) example
PowerPC basic block, you will find the IR opcodes related to
`gen_update_nip`, `tcg_gen_mov_tl` and `gen_helper_store_msr`:

```asm
0xfff00110:  movi_i32    nip,$0xfff00114
             mov_i32     tmp0,r0
             call        store_msr,$0,tmp0
```

There is no definition of `gen_helper_store_msr` in the QEMU code,
because it is *generated* during compilation. The programmer provides
the final implementation into `helper_store_msr` and QEMU generates
the glue to reach that function.

The starting point for the PowerPC is
[target/ppc/helper.h](https://github.com/qemu/qemu/blob/v4.2.0/target/ppc/helper.h#L8):

```c
DEF_HELPER_2(store_msr, void, env, tl)
```

And subsequent macro definitions in
[helper-head.h](https://github.com/qemu/qemu/blob/v4.2.0/include/exec/helper-head.h#L141)
and
[helper-gen.h](https://github.com/qemu/qemu/blob/v4.2.0/include/exec/helper-gen.h):

```c
#define HELPER(name) glue(helper_, name)

#define DEF_HELPER_2(name, ret, t1, t2) \
    DEF_HELPER_FLAGS_2(name, 0, ret, t1, t2)

#define DEF_HELPER_FLAGS_2(name, flags, ret, t1, t2)                    \
static inline void glue(gen_helper_, name)(dh_retvar_decl(ret)          \
    dh_arg_decl(t1, 1), dh_arg_decl(t2, 2))                             \
{                                                                       \
  TCGTemp *args[2] = { dh_arg(t1, 1), dh_arg(t2, 2) };                  \
  tcg_gen_callN(HELPER(name), dh_retvar(ret), 2, args);                 \
}
```

Without exposing too many details, the several levels of macro
expansion transform `gen_helper_store_msr()` into
`tcg_gen_callN(helper_store_msr)`. And
[tcg_gen_callN](https://github.com/qemu/qemu/blob/v4.2.0/tcg/tcg.c#L1688)
is simply responsible for the generation of a *function call* to our
helper function `helper_store_msr`.


## Last words

To be able to simulate *system* instructions, QEMU introduced the
*helper* concept which allows programmers to implement their emulation
inside the QEMU engine.

We will see in the [next blog post](tcg_p3.md) that the QEMU helpers
are largely used to implement guest memory accesses with the support
of virtual TLBs. They are also used for dealing with exceptions. The
interested reader can have a look at
[target/ppc/excp_helper.c](https://github.com/qemu/qemu/blob/v4.2.0/target/ppc/excp_helper.c)
and
[target/ppc/mem_helper.c](https://github.com/qemu/qemu/blob/v4.2.0/target/ppc/mem_helper.c).

```

`tcg_p3.md`:

```md
# A deep dive into QEMU: TCG memory accesses

This blog post details how the QEMU TCG engine manages guest memory
accesses.

As as JIT-compiler enabling guest code to be translated into host
code, a lot of questions arise:
- How does a virtual PowerPC userland address can be translated and
accessed into the host ?
- How the host cpu instructions are restricted to only access the
available QEMU virtual machine memory ?
- Is there an address translation cache ?
- What about the performances ?

Don't hesitate to refresh your QEMU memory model knowledge by reading
[this blog post](regions.md).


## Environment

We assume the reader to have previous knowledge of memory management
in modern architectures and operating systems. A lot of ressources are
publicly available:
- [Memory Management Unit](https://wiki.osdev.org/Memory_Management_Unit)
- [Memory Paging](https://wiki.osdev.org/Paging)
- [TLBs](https://wiki.osdev.org/TLB)

As usual in the blog series, we will assume the guest being a PowerPC
32 bits machine and the host an Intel x86_64 one.

We won't detail anything related to address translation in the host,
because QEMU is a simple user process of your hosting operating
system. The guest physical memory (RAM) is just a buffer allocated
into that QEMU process. The QEMU TCG job is thus to redirect any guest
memory access to address X (whether virtual or physical) to that
memory buffer.

Obviously, the guest vCPU type and running mode will have an impact on
how addresses are translated. This is where the so called
*QEMU-softmmu* enters the game. In system-mode emulation, as opposed
to user-mode, and for some architectures the QEMU engine supports a
*Software Memory Managment Unit* (soft-MMU). This component is able to
translate guest virtual addresses into guest physical ones. QEMU also
supports *Virtual Translation Lookaside Buffers* (vTLBs) to speed-up
further accesses to previously translated addresses.

The guest physical address is always translated into an offset to the
QEMU maintained representation of the guest RAM, which is usually a
memory mapped area in the host (a buffer obtained via `malloc()` or
`mmap()`).


## Analysing a QEMU TCG memory store operation

What happens in QEMU when it has to translate the following PowerPC
instruction:

```asm
0xfff0017c:  90010004  stw      r0, 4(r1)
```

### Translating PowerPC `stw` into IR

Like any other instructions, the
[`opcodes`](https://github.com/qemu/qemu/blob/v4.2.0/target/ppc/translate.c#L7381)
table has a specific entry for `stw`:

```c
static opcode_t opcodes[] = {
...
GEN_STS(stw, st32, 0x04, PPC_INTEGER)
...
};

#define GEN_ST(name, stop, opc, type)                        \
  GEN_HANDLER(name, opc, 0xFF, 0xFF, 0x00000000, type),

#define GEN_HANDLER(name, opc1, opc2, opc3, inval, type)     \
  GEN_OPCODE(name, opc1, opc2, opc3, inval, type, PPC_NONE)
```

We purposely omitted the expansion of `GEN_STS` to `GEN_HANDLER`
because it breaks down into all signed, unsigned and extended variant
of the store operation which is of no interest to us.

The `GEN_OPCODE` macro will expand to the declaration, if you remember
our [TCG article](tcg_p2.md#tcg-helpers), of the
[`gen_stw`](https://github.com/qemu/qemu/blob/v4.2.0/target/ppc/translate.c#L2717)
handler. We can't find its direct definition in the QEMU source code,
as for TCG *helpers*, it is partly generated at compilation time:

```c
#define GEN_ST(name, stop, opc, type)                                         \
static void glue(gen_, name)(DisasContext *ctx)                               \
{                                                                             \
    TCGv EA;                                                                  \
    gen_set_access_type(ctx, ACCESS_INT);                                     \
    EA = tcg_temp_new();                                                      \
    gen_addr_imm_index(ctx, EA, 0);                                           \
    gen_qemu_##stop(ctx, cpu_gpr[rS(ctx->opcode)], EA);                       \
    tcg_temp_free(EA);                                                        \
}
```

Beside some effective address computation, the interesting line of
code is the expansion of `gen_qemu_##stop` into
[`gen_qemu_st32`](https://github.com/qemu/qemu/blob/v4.2.0/target/ppc/translate.c#L2489)
which again has no direct definition but is generated thanks to
several macro expansions:

```c
#define GEN_QEMU_STORE_TL(stop, op)                                     \
static void glue(gen_qemu_, stop)(DisasContext *ctx,                    \
                                  TCGv val,                             \
                                  TCGv addr)                            \
{                                                                       \
    tcg_gen_qemu_st_tl(val, addr, ctx->mem_idx, op);                    \
}

GEN_QEMU_STORE_TL(st32, DEF_MEMOP(MO_UL))

/* from tcg/tcg-op.h */
#if TARGET_LONG_BITS == 32
...
#define tcg_gen_qemu_st_tl tcg_gen_qemu_st_i32
...
```

For a 32 bits PowerPC guest, the initial `stw` guest instruction gets
translated into QEMU TCG *frontend-op*
[`tcg_gen_qemu_st_i32`](https://github.com/qemu/qemu/blob/v4.2.0/tcg/tcg-op.c#L2845):

```c
void tcg_gen_qemu_st_i32(TCGv_i32 val, TCGv addr, TCGArg idx, TCGMemOp memop)
{
...
    gen_ldst_i32(INDEX_op_qemu_st_i32, val, addr, memop, idx);
...
}
```

From that point, we have an IR qemu_st_i32 opcode which is emitted.



### Translating IR `qemu_st_i32` into Intel x86_64 for execution

We won't explain host code generation again, read the [dedicated blog
post](tcg_p2.md). Once the execution loop reaches
[`tcg_gen_code`](https://github.com/qemu/qemu/blob/v4.2.0/tcg/tcg.c#L4013)
and more specifically
[`tcg_reg_alloc_op`](https://github.com/qemu/qemu/blob/v4.2.0/tcg/tcg.c#L3575),
QEMU generates the TCG *backend-op* for `qemu_st_i32`.

```c
static void tcg_reg_alloc_op(TCGContext *s, const TCGOp *op)
{
...
     tcg_out_op(s, op->opc, new_args, const_args);
...

}
```

In our situation, the *tcg-target* is an Intel x86 machine, so we will
find suitable `tcg_out_op` definition in
[`tcg/i386/tcg-target.inc.c`](https://github.com/qemu/qemu/blob/v4.2.0/tcg/i386/tcg-target.inc.c#L2255)

```c
static inline void tcg_out_op(TCGContext *s, TCGOpcode opc,
                              const TCGArg *args, const int *const_args)
{
...
    case INDEX_op_qemu_st_i32:
        tcg_out_qemu_st(s, args, 0);
...
}
```

Here we are. The
[`tcg_out_qemu_st`](https://github.com/qemu/qemu/blob/v4.2.0/tcg/i386/tcg-target.inc.c#L2219)
is a very interesting function to study. It holds the internals of
QEMU guest memory addressing.


## Resolving host address

The blog series does not intend to be a complete guide to the TCG
internals. Keep in mind, that at this level, functions are developped
with some conventions related to TCG arguments (ie. `TCGArg *args`).

```c
static void tcg_out_qemu_st(TCGContext *s, const TCGArg *args, bool is64)
{
    TCGReg datalo, datahi, addrlo;
    TCGReg addrhi __attribute__((unused));
    TCGMemOpIdx oi;
    MemOp opc;
#if defined(CONFIG_SOFTMMU)
    int mem_index;
    tcg_insn_unit *label_ptr[2];
#endif

    datalo = *args++;
    datahi = (TCG_TARGET_REG_BITS == 32 && is64 ? *args++ : 0);
    addrlo = *args++;
    addrhi = (TARGET_LONG_BITS > TCG_TARGET_REG_BITS ? *args++ : 0);
    oi = *args++;
    opc = get_memop(oi);

#if defined(CONFIG_SOFTMMU)
    mem_index = get_mmuidx(oi);

    tcg_out_tlb_load(s, addrlo, addrhi, mem_index, opc,
                     label_ptr, offsetof(CPUTLBEntry, addr_write));

    /* TLB Hit.  */
    tcg_out_qemu_st_direct(s, datalo, datahi, TCG_REG_L1, -1, 0, 0, opc);

    /* Record the current context of a store into ldst label */
    add_qemu_ldst_label(s, false, is64, oi, datalo, datahi, addrlo, addrhi,
                        s->code_ptr, label_ptr);
#else
    tcg_out_qemu_st_direct(s, datalo, datahi, addrlo, x86_guest_base_index,
                           x86_guest_base_offset, x86_guest_base_seg, opc);
#endif
}
```

Thanks to the *soft-mmu* and support of virtual TLBs, QEMU offers a
slow path and a fast path when accessing guest memory. The slow path
can be seen as a *TLB-miss* and implies a subsequent call to the
PowerPC software MMU implemented inside QEMU to translate a guest
virtual address into a guest physical address.

If there is a *TLB-hit*, QEMU already holds the guest physical address
in its vCPU maintained TLBs and is able to directly generate the final
memory access into guest RAM with an Intel x86 instruction. Have a
look at
[`tcg_out_qemu_st_direct`](https://github.com/qemu/qemu/blob/v4.2.0/tcg/i386/tcg-target.inc.c#L2138).

The mechanic behind that is tied to the following 3 lines:

```c
/* try to find a filled TLB entry */
tcg_out_tlb_load(s, addrlo, addrhi, mem_index, opc,
                 label_ptr, offsetof(CPUTLBEntry, addr_write));

/* TLB Hit. So generate a physical guest memory access */
tcg_out_qemu_st_direct(s, datalo, datahi, TCG_REG_L1, -1, 0, 0, opc);

/* TLB Miss. Filled during tlb_load and redirect to soft-MMU */
add_qemu_ldst_label(s, false, is64, oi, datalo, datahi, addrlo, addrhi,
                    s->code_ptr, label_ptr);
```

### QEMU vCPU TLBs

First,
[`tcg_out_tlb_load`](https://github.com/qemu/qemu/blob/v4.2.0/tcg/i386/tcg-target.inc.c#L1699)
will generate *host instructions* to check for a TLB entry. The QEMU
TLBs are generic to the architecture and defined at
[cpu-defs.h](https://github.com/qemu/qemu/blob/v4.2.0/include/exec/cpu-defs.h#L109):

```c
typedef struct CPUTLBEntry {
    /* bit TARGET_LONG_BITS to TARGET_PAGE_BITS : virtual address
       bit TARGET_PAGE_BITS-1..4  : Nonzero for accesses that should not
                                    go directly to ram.
       bit 3                      : indicates that the entry is invalid
       bit 2..0                   : zero
    */
    union {
        struct {
            target_ulong addr_read;
            target_ulong addr_write;
            target_ulong addr_code;
            /* Addend to virtual address to get host address.  IO accesses
               use the corresponding iotlb value.  */
            uintptr_t addend;
        };
        /* padding to get a power of two size */
        uint8_t dummy[1 << CPU_TLB_ENTRY_BITS];
    };
} CPUTLBEntry;
```

As translated blocks are generated once and executed several times
(potentially), it is convenient to generate host code that will
dynamically check for QEMU maintained vCPU TLBs. During the life of a
translated block, a given TLB entry might be invalidated then filled
again.

The
[`tcg_out_tlb_load`](https://github.com/qemu/qemu/blob/v4.2.0/tcg/i386/tcg-target.inc.c#L1699)
code is quite annoying to read, full of *tcg-target* opcodes
generators. A resulting Intel x86_64 translated block containing the
`tlb_load` algorithm looks like the following:

```asm
tcg_out_tlb_load:
0x7ffff41888e9 <code_gen_buffer+22716>:	mov    %esp,%edi
0x7ffff41888eb <code_gen_buffer+22718>:	shr    $0x7,%edi
0x7ffff41888ee <code_gen_buffer+22721>:	and    0x338(%rbp),%edi
0x7ffff41888f4 <code_gen_buffer+22727>:	add    0x388(%rbp),%rdi
0x7ffff41888fb <code_gen_buffer+22734>:	lea    0x3(%r12),%esi
0x7ffff4188900 <code_gen_buffer+22739>:	and    $0xfffff000,%esi
0x7ffff4188906 <code_gen_buffer+22745>:	cmp    0x4(%rdi),%esi
0x7ffff4188909 <code_gen_buffer+22748>:	mov    %r12d,%esi
0x7ffff418890c <code_gen_buffer+22751>:	jne    0x7ffff418897f  ---> back to LDST labels
0x7ffff4188912 <code_gen_buffer+22757>:	add    0x10(%rdi),%rsi
```

QEMU tries to read its `CPUTLBEntries` for the given guest virtual
address. For a store operation, the `addr_write` field is used for
comparaison at `0x7ffff4188906` in the extract.  The `RDI` register
points to the `CPUTLBEntry` and `ESI` holds the guest address.

If the comparaison fails, it's a *TLB-miss* and we jump to a *LDST
label* that we will explain later. Else, the `RSI` register is
adjusted to the final host address thanks to the `CPUTLBEntry.addend`
and the memory access can be done:

```asm
tcg_out_qemu_st_direct:
0x7ffff4188925 <code_gen_buffer+22776>:	movbe  %ebx,(%rsi)
```

The TLB verification implementation can also be found in the QEMU
`cpu_ld/st_xxx` API functions. As defined in the
[documentation](https://github.com/qemu/qemu/blob/v4.2.0/docs/devel/loads-stores.rst),
they operate on guest virtual addresses and **may cause guest CPU
exception**. Thus they do check TLBs and might redirect to the
software MMU. They are implemented through macros in
[cpu_ldst_template.h](https://github.com/qemu/qemu/blob/v4.2.0/include/exec/cpu_ldst_template.h):

```c
/* generic store macro */

static inline void
glue(glue(glue(cpu_st, SUFFIX), MEMSUFFIX), _ra)(CPUArchState *env,
                                                 target_ulong ptr,
                                                 RES_TYPE v, uintptr_t retaddr)
{
...
    addr = ptr;
    mmu_idx = CPU_MMU_INDEX;
    entry = tlb_entry(env, mmu_idx, addr);
    if (unlikely(tlb_addr_write(entry) !=
                 (addr & (TARGET_PAGE_MASK | (DATA_SIZE - 1))))) {
        oi = make_memop_idx(SHIFT, mmu_idx);
        glue(glue(helper_ret_st, SUFFIX), MMUSUFFIX)(env, addr, v, oi,
                                                     retaddr);
    } else {
        uintptr_t hostaddr = addr + entry->addend;
        glue(glue(st, SUFFIX), _p)((uint8_t *)hostaddr, v);
    }
...
}
```

Looks familiar to you, no :) ?


### QEMU LDST labels

The [`LDST
labels`](https://github.com/qemu/qemu/blob/v4.2.0/tcg/tcg-ldst.inc.c#L23)
stand for *load/store labels*. It is the mechanism used by QEMU to
redirect a *TLB-miss* to a call to the software MMU via a TCG helper.

The
[`TCGContext`](https://github.com/qemu/qemu/blob/v4.2.0/tcg/tcg.h#L583)
object holds the output buffer that receive generated *host assembly*
opcodes. Any time an instruction is added, the `code_ptr` pointer to
that buffer is obviously incremented.

During
[`tcg_out_tlb_load`](https://github.com/qemu/qemu/blob/v4.2.0/tcg/i386/tcg-target.inc.c#L1699),
when QEMU generates the comparaison and jump instructions for a
*TLB-miss*, it also records the location into the output buffer that
will hold the `JNE` offset to redirect execution to.

```c
static inline void tcg_out_tlb_load(TCGContext *s, TCGReg addrlo, TCGReg addrhi,
                                    int mem_index, MemOp opc,
                                    tcg_insn_unit **label_ptr, int which)
{
...
    /* jne slow_path */
    tcg_out_opc(s, OPC_JCC_long + JCC_JNE, 0, 0, 0);
    label_ptr[0] = s->code_ptr;
    s->code_ptr += 4;
...
}
```

Additionally, when we get back to
[`tcg_out_qemu_st`](https://github.com/qemu/qemu/blob/v4.2.0/tcg/i386/tcg-target.inc.c#L2247),
the call to
[`add_qemu_ldst_label`](https://github.com/qemu/qemu/blob/v4.2.0/tcg/i386/tcg-target.inc.c#L1784)
creates a new LDST label and records context details to later prepare
a call to the softmmu *slow path* TCG helper:

```c
static void add_qemu_ldst_label(TCGContext *s, bool is_ld, bool is_64,
                                TCGMemOpIdx oi,
                                TCGReg datalo, TCGReg datahi,
                                TCGReg addrlo, TCGReg addrhi,
                                tcg_insn_unit *raddr,
                                tcg_insn_unit **label_ptr)
{
    TCGLabelQemuLdst *label = new_ldst_label(s);

    label->is_ld = is_ld;
    label->oi = oi;
    label->type = is_64 ? TCG_TYPE_I64 : TCG_TYPE_I32;
    label->datalo_reg = datalo;
    label->datahi_reg = datahi;
    label->addrlo_reg = addrlo;
    label->addrhi_reg = addrhi;
    label->raddr = raddr;
    label->label_ptr[0] = label_ptr[0];
    if (TARGET_LONG_BITS > TCG_TARGET_REG_BITS) {
        label->label_ptr[1] = label_ptr[1];
    }
}
```

The call to the *slow path* helper is inserted by
[`tcg_gen_code`](https://github.com/qemu/qemu/blob/v4.2.0/tcg/tcg.c#L4013)
during translated block epilogue generation with a call to
[`tcg_out_ldst_finalize`](https://github.com/qemu/qemu/blob/v4.2.0/tcg/tcg-ldst.inc.c#L44). QEMU
checks if there exists *LDST labels* and generates the corresponding
TCG helper calls. For our store operation,
[`tcg_out_ldst_finalize`](https://github.com/qemu/qemu/blob/v4.2.0/tcg/tcg-ldst.inc.c#L44)
will emit a
[`tcg_out_qemu_st_slow_path`](https://github.com/qemu/qemu/blob/v4.2.0/tcg/i386/tcg-target.inc.c#L1895):


```c
int tcg_gen_code(TCGContext *s, TranslationBlock *tb)
{
...
#ifdef TCG_TARGET_NEED_LDST_LABELS
    i = tcg_out_ldst_finalize(s);
    if (i < 0) {
        return i;
    }
#endif
...
}

static int tcg_out_ldst_finalize(TCGContext *s)
{
...
    /* qemu_ld/st slow paths */
    QSIMPLEQ_FOREACH(lb, &s->ldst_labels, next) {
        if (lb->is_ld
            ? !tcg_out_qemu_ld_slow_path(s, lb)
            : !tcg_out_qemu_st_slow_path(s, lb)) {
            return -2;
        }
...
}

static bool tcg_out_qemu_st_slow_path(TCGContext *s, TCGLabelQemuLdst *l)
{
...
    /* "Tail call" to the helper, with the return address back inline.  */
    tcg_out_push(s, retaddr);
    tcg_out_jmp(s, qemu_st_helpers[opc & (MO_BSWAP | MO_SIZE)]);
    return true;
}
```

The preparation of the *slow path* helper call implies:
- resolving the label offset, previously recorded during `JNE` generation
- arguments/registers setup
- call to the appropriate
[`qemu_st_helper`](https://github.com/qemu/qemu/blob/v4.2.0/tcg/i386/tcg-target.inc.c#L1668)

Like others memory access functions, there exists a lot of variants
for store and load operations: *signed, unsigned, byte, word, long,
big or little endian*. Each one has a specific TCG helper which is a
wrapper to
[`store_helper`](https://github.com/qemu/qemu/blob/v4.2.0/accel/tcg/cputlb.c#L1663):


```c
/* helper signature: helper_ret_st_mmu(CPUState *env, target_ulong addr,
 *                                     uintxx_t val, int mmu_idx, uintptr_t ra)
 */
static void * const qemu_st_helpers[16] = {
    [MO_UB]   = helper_ret_stb_mmu,
    [MO_LEUW] = helper_le_stw_mmu,
    [MO_LEUL] = helper_le_stl_mmu,
    [MO_LEQ]  = helper_le_stq_mmu,
    [MO_BEUW] = helper_be_stw_mmu,
    [MO_BEUL] = helper_be_stl_mmu,
    [MO_BEQ]  = helper_be_stq_mmu,
};

void helper_be_stl_mmu(CPUArchState *env, target_ulong addr, uint32_t val,
                       TCGMemOpIdx oi, uintptr_t retaddr)
{
    store_helper(env, addr, val, oi, retaddr, MO_BEUL);
}

static inline void QEMU_ALWAYS_INLINE
store_helper(CPUArchState *env, target_ulong addr, uint64_t val,
             TCGMemOpIdx oi, uintptr_t retaddr, MemOp op)
{
...
        if (!tlb_hit_page(tlb_addr2, page2)) {
            if (!victim_tlb_hit(env, mmu_idx, index2, tlb_off, page2)) {
                tlb_fill(env_cpu(env), page2, size2, MMU_DATA_STORE,
                         mmu_idx, retaddr);
                index2 = tlb_index(env, mmu_idx, page2);
                entry2 = tlb_entry(env, mmu_idx, page2);
            }
            tlb_addr2 = tlb_addr_write(entry2);
        }
...
    haddr = (void *)((uintptr_t)addr + entry->addend);
    store_memop(haddr, val, op);
}
```

The helper checks TLBs, if there is a miss it calls
[`tlb_fill`](https://github.com/qemu/qemu/blob/v4.2.0/accel/tcg/cputlb.c#L900). In
any case, once the address is resolved it does the final memory access
in the host memory with
[`store_memop`](https://github.com/qemu/qemu/blob/v4.2.0/accel/tcg/cputlb.c#L1633).

```c
static void tlb_fill(CPUState *cpu, target_ulong addr, int size,
                     MMUAccessType access_type, int mmu_idx, uintptr_t retaddr)
{
    CPUClass *cc = CPU_GET_CLASS(cpu);
    bool ok;

    ok = cc->tlb_fill(cpu, addr, size, access_type, mmu_idx, false, retaddr);
    assert(ok);
}
```

As you may guess, the process of *filling a TLB* is done by the
software MMU whose implementation depends on the emulated
architecture. During the [PowerPC CPU
initialisation](https://github.com/qemu/qemu/blob/v4.2.0/target/ppc/translate_init.inc.c#L10662)
,
[`cc->tlb_fill`](https://github.com/qemu/qemu/blob/v4.2.0/accel/tcg/cputlb.c#L900)
is set to
[`ppc_cpu_tlb_fill`](https://github.com/qemu/qemu/blob/v4.2.0/target/ppc/mmu_helper.c#L3041).

```c
bool ppc_cpu_tlb_fill(CPUState *cs, vaddr addr, int size,
                      MMUAccessType access_type, int mmu_idx,
                      bool probe, uintptr_t retaddr)
{
    PowerPCCPU *cpu = POWERPC_CPU(cs);
    PowerPCCPUClass *pcc = POWERPC_CPU_GET_CLASS(cs);
    CPUPPCState *env = &cpu->env;
    int ret;

    if (pcc->handle_mmu_fault) {
        ret = pcc->handle_mmu_fault(cpu, addr, access_type, mmu_idx);
    } else {
        ret = cpu_ppc_handle_mmu_fault(env, addr, access_type, mmu_idx);
    }
    if (unlikely(ret != 0)) {
        if (probe) {
            return false;
        }
        raise_exception_err_ra(env, cs->exception_index, env->error_code,
                               retaddr);
    }
    return true;
}
```

And
[`ppc->handle_mmu_fault`](https://github.com/qemu/qemu/blob/v4.2.0/target/ppc/translate_init.inc.c#L5319)
might eventually be set to
[`ppc_hash32_handle_mmu_fault`](https://github.com/qemu/qemu/blob/v4.2.0/target/ppc/mmu-hash32.c#L415),
depending on your PowerPC CPU family. This is where the software MMU
implementation lies for the PowerPC.

```

`timers.md`:

```md
# A deep dive into QEMU: a Brief History of Time

Ever wanted to play with general relativity ? QEMU is a simulation
environment, guess what ? We can control time as seen by the VM !

Some architectures directly provide a clock register inside the
CPU. However, a board usually needs extended time control through
dedicated devices. How would you implement such a device inside QEMU ?

## Time in QEMU

### QEMU Clocks

QEMU implements several `clocks` to get informed about time. Obviously
you can still directly use host OS interface to get time information.

Looking at
[`timer.h`](https://github.com/qemu/qemu/blob/v4.2.0/include/qemu/timer.h#L16)
we learn that there exists 4 clock types:
- realtime
- host
- virtual
- virtual_rt

The one that will be of interest for simulating basic timers is
`virtual`. It only runs alongside the VM, so it reflects time reality
in the context of the VM.

QEMU provides a `qemu_clock_xxx` API to control time for related clocks.

```c
int64_t now = qemu_clock_get_ms(QEMU_CLOCK_VIRTUAL);
```

This returns the current time in millisecond for the `virtual`
clock.

### QEMU Timers

QEMU provides a `timer_xxx` API to create, modify, reset, delete
`timers`, for different clocks and granularity (ms, ns). You can
attach timers to specific clocks. The main QEMU execution loop
controls the `virtual` clock and can disable timers when the VM vCPU
is stopped.

The following piece of code creates a `timer` with milliseconds
granularity, that runs only when the VM vCPU runs:

```c
QEMUTimer *user_timer = timer_new_ms(QEMU_CLOCK_VIRTUAL, user_timeout_cb, obj);
int64_t    now        = qemu_clock_get_ms(QEMU_CLOCK_VIRTUAL);

timer_mod(timer, now + duration);

static void user_timeout_cb(void *opaque)
{
  obj_t *obj = (obj_t*)opaque;
...
}
```

When `duration` milliseconds have elapsed in the `virtual clock` time,
the callback function `user_timeout_cb` is called.


## Creating a timer device

As any other device, and following the datasheet of the timer you
would like to simulate, you will have to expose IO memory regions to
reflect device register configuration to QEMU timers setup and raise
IRQs on those timers expiration.

So you will need both device specific hardware representation and QEMU
internal clock model.

### CPIOM tick timer

Under our CPIOM example implementation this may look like the following:

```c
typedef struct cpiom_clock
{
    QEMUTimer *qemu_timer;
    uint32_t  *trigger;
    int64_t    restart;
    double     duration;

} cpiom_clock_t;

typedef struct cpiom_timer_device_state
{
    /*< private >*/
    SysBusDevice      parent_obj;

    /*< public >*/
    MemoryRegion      iomem;
    cpiom_timer_reg_t reg;
    qemu_irq          irq;

    /* internal clock management */
    cpiom_clock_t     tick;

} cpiom_timer_state_t;
```

We have a standard `SysBusDevice` with `iomem` IO memory region and
underlying device registers `reg`. It also declares a `cpiom_clock`
called `tick`. The real CPIOM timers are actually more complex, but
for the sake of simplicity here we will only consider a `tick` timer.

```c
static void cpiom_timer_init(Object *obj)
{
    cpiom_timer_state_t *tm  = CPIOM_TIMERS(obj);
    SysBusDevice        *dev = SYS_BUS_DEVICE(obj);

    memory_region_init_io(&tm->iomem, obj, &cpiom_timer_reg_ops, tm,
                          CPIOM_TIMERS_NAME"-reg", CPIOM_MMAP_TIMERS_SIZE);
    sysbus_init_mmio(dev, tm->iomem);
    sysbus_init_irq(dev, &tm->irq);

    tm->tick.qemu_timer = timer_new_ns(QEMU_CLOCK_VIRTUAL, tick_expired, tm);
    tm->tick.trigger    = &tm->reg.base.tick;
...
}
```

We actually setup a device whose any access to `tm->iomem` will update
`tm->reg` thanks to the `cpiom_timer_reg_ops`
[`MemoryRegionOps`](https://github.com/qemu/qemu/blob/v4.2.0/include/exec/memory.h#L144). In
the meantime, a nano second `virtual clock` timer is created to call
`tick_expired`.


### Accessing the CPIOM tick timer

Let's say offset `0x0c` is a `R/W 32 bits TIME_COUNTER` register for
our imaginary timer device. The counter is decremented at a given
frequency (usually adjustable via a scale register). When it reaches
0, it raises an IRQ.

Eventually an OS driver running on our CPIOM board, and trying to
setup the timer device, will happen to write to this register.

A candidate implementation would be:

```c
static const MemoryRegionOps cpiom_timer_reg_ops = {
    .read  = cpiom_timer_reg_read,
    .write = cpiom_timer_reg_write,
    .endianness = DEVICE_NATIVE_ENDIAN,
};

static void cpiom_timer_reg_write(void *opaque, hwaddr addr, uint64_t data, unsigned size)
{
....
    cpiom_timer_state_t *tm = (cpiom_timer_state_t*)opaque;

    if (addr == 0x0c)
        write_counter(tm, data);
....
}

static void write_counter(cpiom_timer_state_t *tm, uint32_t new)
{
    if (!timer_is_active(tm))
        return;

    if (new == 0)
        tick_expired((void*)tm);
    else
        clock_setup(tm, &tm->tick, new);
}

static void tick_expired(void *opaque)
{
    cpiom_timer_state_t *tm = (cpiom_timer_state_t*)opaque;
    qemu_irq_raise(tm->irq);
}
```

If the driver modifies the device `counter`, we should check for
possible immediate expiration and raise an IRQ. Else we must update
our QEMU internal timer to trigger a call to `tick_expired` at the
expected `virtual` clock time.


### Time dilatation

Interestingly, the `clock_setup` might look like:

```c
static void clock_setup(cpiom_timer_state_t *tm, cpiom_clock_t *clk, uint32_t count)
{
    clk->duration = nsperiod * count;
    clk->restart  = qemu_clock_get_ns(QEMU_CLOCK_VIRTUAL);

    uint64_t expire = clk->restart + (int64_t)floor(clk->duration);
    timer_mod(clk->qemu_timer, expire /* +/- speed factor */);
}
```

We compute the next expiration date in nano seconds based on the new
counter value and the timer frequency (expressed as `nsperiod`). This
period might be computed as follows:

```c
    nsperiod = (1/TIMER_FREQ_MHZ) * 1000 * scale;
```

Notice that we can also induce a *speed factor* effect to the
`virtual` clock.


### Elapsed time

Conversely, whenever a driver reads the device `counter` register,
your code must reflect the elapsed time in the VM and give back an
appropriate value. Something like:

```c
    now          = qemu_clock_get_ns(QEMU_CLOCK_VIRTUAL);
    count        = (now - clk->restart)/nsperiod;
    clk->restart = now;
```

```