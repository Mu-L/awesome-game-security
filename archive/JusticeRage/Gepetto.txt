Project Path: arc_JusticeRage_Gepetto_758ao8n3

Source Tree:

```txt
arc_JusticeRage_Gepetto_758ao8n3
├── LICENSE
├── README.md
├── babel.cfg
├── gepetto
│   ├── __init__.py
│   ├── config.ini
│   ├── config.py
│   ├── ida
│   │   ├── __init__.py
│   │   ├── cli.py
│   │   ├── comment_handler.py
│   │   ├── handlers.py
│   │   ├── status_panel
│   │   │   ├── __init__.py
│   │   │   ├── no_panel.py
│   │   │   ├── panel_interface.py
│   │   │   ├── qt_compat.py
│   │   │   ├── qt_panel.py
│   │   │   └── status_panel_factory.py
│   │   ├── tools
│   │   │   ├── __init__.py
│   │   │   ├── declare_c_type.py
│   │   │   ├── decompile_function.py
│   │   │   ├── disasm_function.py
│   │   │   ├── get_bytes.py
│   │   │   ├── get_current_function.py
│   │   │   ├── get_disasm.py
│   │   │   ├── get_ea.py
│   │   │   ├── get_screen_ea.py
│   │   │   ├── get_struct.py
│   │   │   ├── get_xrefs.py
│   │   │   ├── list_functions.py
│   │   │   ├── list_imports.py
│   │   │   ├── list_symbols.py
│   │   │   ├── refresh_view.py
│   │   │   ├── rename_function.py
│   │   │   ├── rename_global.py
│   │   │   ├── rename_lvar.py
│   │   │   ├── run_python.py
│   │   │   ├── search.py
│   │   │   ├── set_comment.py
│   │   │   ├── to_hex.py
│   │   │   └── tools.py
│   │   ├── ui.py
│   │   └── utils
│   │       ├── __init__.py
│   │       ├── function_helpers.py
│   │       ├── hooks.py
│   │       └── thread_helpers.py
│   ├── locales
│   │   ├── ca_ES
│   │   │   └── LC_MESSAGES
│   │   │       ├── gepetto.mo
│   │   │       └── gepetto.po
│   │   ├── es_ES
│   │   │   └── LC_MESSAGES
│   │   │       ├── gepetto.mo
│   │   │       └── gepetto.po
│   │   ├── fr_FR
│   │   │   └── LC_MESSAGES
│   │   │       ├── gepetto.mo
│   │   │       └── gepetto.po
│   │   ├── generate_mo_files.sh
│   │   ├── gepetto.pot
│   │   ├── it_IT
│   │   │   └── LC_MESSAGES
│   │   │       ├── gepetto.mo
│   │   │       └── gepetto.po
│   │   ├── ko_KR
│   │   │   └── LC_MESSAGES
│   │   │       ├── gepetto.mo
│   │   │       └── gepetto.po
│   │   ├── ru
│   │   │   └── LC_MESSAGES
│   │   │       ├── gepetto.mo
│   │   │       └── gepetto.po
│   │   ├── tr
│   │   │   └── LC_MESSAGES
│   │   │       ├── gepetto.mo
│   │   │       └── gepetto.po
│   │   ├── zh_CN
│   │   │   └── LC_MESSAGES
│   │   │       ├── gepetto.mo
│   │   │       └── gepetto.po
│   │   └── zh_TW
│   │       └── LC_MESSAGES
│   │           ├── gepetto.mo
│   │           └── gepetto.po
│   └── models
│       ├── __init__.py
│       ├── aliyun.py
│       ├── azure_openai.py
│       ├── base.py
│       ├── deepseek.py
│       ├── gemini.py
│       ├── groq.py
│       ├── kluster.py
│       ├── local_lmstudio.py
│       ├── local_ollama.py
│       ├── model_manager.py
│       ├── novita_ai.py
│       ├── openai.py
│       ├── openai_compatible.py
│       ├── openrouter.py
│       ├── siliconflow.py
│       └── together.py
├── gepetto.py
├── ida-plugin.json
├── pyproject.toml
├── readme
│   ├── cli.png
│   ├── comparison.png
│   ├── gepetto_logo.png
│   ├── select_model.png
│   └── usage.png
├── requirements.txt
└── tests
    ├── __init__.py
    ├── conftest.py
    ├── test_config_parser.py
    ├── test_gemini_convert_messages.py
    ├── test_tools.py
    └── testfiles
        └── manalyze.exe

```

`LICENSE`:

```
                    GNU GENERAL PUBLIC LICENSE
                       Version 3, 29 June 2007

 Copyright (C) 2007 Free Software Foundation, Inc. <https://fsf.org/>
 Everyone is permitted to copy and distribute verbatim copies
 of this license document, but changing it is not allowed.

                            Preamble

  The GNU General Public License is a free, copyleft license for
software and other kinds of works.

  The licenses for most software and other practical works are designed
to take away your freedom to share and change the works.  By contrast,
the GNU General Public License is intended to guarantee your freedom to
share and change all versions of a program--to make sure it remains free
software for all its users.  We, the Free Software Foundation, use the
GNU General Public License for most of our software; it applies also to
any other work released this way by its authors.  You can apply it to
your programs, too.

  When we speak of free software, we are referring to freedom, not
price.  Our General Public Licenses are designed to make sure that you
have the freedom to distribute copies of free software (and charge for
them if you wish), that you receive source code or can get it if you
want it, that you can change the software or use pieces of it in new
free programs, and that you know you can do these things.

  To protect your rights, we need to prevent others from denying you
these rights or asking you to surrender the rights.  Therefore, you have
certain responsibilities if you distribute copies of the software, or if
you modify it: responsibilities to respect the freedom of others.

  For example, if you distribute copies of such a program, whether
gratis or for a fee, you must pass on to the recipients the same
freedoms that you received.  You must make sure that they, too, receive
or can get the source code.  And you must show them these terms so they
know their rights.

  Developers that use the GNU GPL protect your rights with two steps:
(1) assert copyright on the software, and (2) offer you this License
giving you legal permission to copy, distribute and/or modify it.

  For the developers' and authors' protection, the GPL clearly explains
that there is no warranty for this free software.  For both users' and
authors' sake, the GPL requires that modified versions be marked as
changed, so that their problems will not be attributed erroneously to
authors of previous versions.

  Some devices are designed to deny users access to install or run
modified versions of the software inside them, although the manufacturer
can do so.  This is fundamentally incompatible with the aim of
protecting users' freedom to change the software.  The systematic
pattern of such abuse occurs in the area of products for individuals to
use, which is precisely where it is most unacceptable.  Therefore, we
have designed this version of the GPL to prohibit the practice for those
products.  If such problems arise substantially in other domains, we
stand ready to extend this provision to those domains in future versions
of the GPL, as needed to protect the freedom of users.

  Finally, every program is threatened constantly by software patents.
States should not allow patents to restrict development and use of
software on general-purpose computers, but in those that do, we wish to
avoid the special danger that patents applied to a free program could
make it effectively proprietary.  To prevent this, the GPL assures that
patents cannot be used to render the program non-free.

  The precise terms and conditions for copying, distribution and
modification follow.

                       TERMS AND CONDITIONS

  0. Definitions.

  "This License" refers to version 3 of the GNU General Public License.

  "Copyright" also means copyright-like laws that apply to other kinds of
works, such as semiconductor masks.

  "The Program" refers to any copyrightable work licensed under this
License.  Each licensee is addressed as "you".  "Licensees" and
"recipients" may be individuals or organizations.

  To "modify" a work means to copy from or adapt all or part of the work
in a fashion requiring copyright permission, other than the making of an
exact copy.  The resulting work is called a "modified version" of the
earlier work or a work "based on" the earlier work.

  A "covered work" means either the unmodified Program or a work based
on the Program.

  To "propagate" a work means to do anything with it that, without
permission, would make you directly or secondarily liable for
infringement under applicable copyright law, except executing it on a
computer or modifying a private copy.  Propagation includes copying,
distribution (with or without modification), making available to the
public, and in some countries other activities as well.

  To "convey" a work means any kind of propagation that enables other
parties to make or receive copies.  Mere interaction with a user through
a computer network, with no transfer of a copy, is not conveying.

  An interactive user interface displays "Appropriate Legal Notices"
to the extent that it includes a convenient and prominently visible
feature that (1) displays an appropriate copyright notice, and (2)
tells the user that there is no warranty for the work (except to the
extent that warranties are provided), that licensees may convey the
work under this License, and how to view a copy of this License.  If
the interface presents a list of user commands or options, such as a
menu, a prominent item in the list meets this criterion.

  1. Source Code.

  The "source code" for a work means the preferred form of the work
for making modifications to it.  "Object code" means any non-source
form of a work.

  A "Standard Interface" means an interface that either is an official
standard defined by a recognized standards body, or, in the case of
interfaces specified for a particular programming language, one that
is widely used among developers working in that language.

  The "System Libraries" of an executable work include anything, other
than the work as a whole, that (a) is included in the normal form of
packaging a Major Component, but which is not part of that Major
Component, and (b) serves only to enable use of the work with that
Major Component, or to implement a Standard Interface for which an
implementation is available to the public in source code form.  A
"Major Component", in this context, means a major essential component
(kernel, window system, and so on) of the specific operating system
(if any) on which the executable work runs, or a compiler used to
produce the work, or an object code interpreter used to run it.

  The "Corresponding Source" for a work in object code form means all
the source code needed to generate, install, and (for an executable
work) run the object code and to modify the work, including scripts to
control those activities.  However, it does not include the work's
System Libraries, or general-purpose tools or generally available free
programs which are used unmodified in performing those activities but
which are not part of the work.  For example, Corresponding Source
includes interface definition files associated with source files for
the work, and the source code for shared libraries and dynamically
linked subprograms that the work is specifically designed to require,
such as by intimate data communication or control flow between those
subprograms and other parts of the work.

  The Corresponding Source need not include anything that users
can regenerate automatically from other parts of the Corresponding
Source.

  The Corresponding Source for a work in source code form is that
same work.

  2. Basic Permissions.

  All rights granted under this License are granted for the term of
copyright on the Program, and are irrevocable provided the stated
conditions are met.  This License explicitly affirms your unlimited
permission to run the unmodified Program.  The output from running a
covered work is covered by this License only if the output, given its
content, constitutes a covered work.  This License acknowledges your
rights of fair use or other equivalent, as provided by copyright law.

  You may make, run and propagate covered works that you do not
convey, without conditions so long as your license otherwise remains
in force.  You may convey covered works to others for the sole purpose
of having them make modifications exclusively for you, or provide you
with facilities for running those works, provided that you comply with
the terms of this License in conveying all material for which you do
not control copyright.  Those thus making or running the covered works
for you must do so exclusively on your behalf, under your direction
and control, on terms that prohibit them from making any copies of
your copyrighted material outside their relationship with you.

  Conveying under any other circumstances is permitted solely under
the conditions stated below.  Sublicensing is not allowed; section 10
makes it unnecessary.

  3. Protecting Users' Legal Rights From Anti-Circumvention Law.

  No covered work shall be deemed part of an effective technological
measure under any applicable law fulfilling obligations under article
11 of the WIPO copyright treaty adopted on 20 December 1996, or
similar laws prohibiting or restricting circumvention of such
measures.

  When you convey a covered work, you waive any legal power to forbid
circumvention of technological measures to the extent such circumvention
is effected by exercising rights under this License with respect to
the covered work, and you disclaim any intention to limit operation or
modification of the work as a means of enforcing, against the work's
users, your or third parties' legal rights to forbid circumvention of
technological measures.

  4. Conveying Verbatim Copies.

  You may convey verbatim copies of the Program's source code as you
receive it, in any medium, provided that you conspicuously and
appropriately publish on each copy an appropriate copyright notice;
keep intact all notices stating that this License and any
non-permissive terms added in accord with section 7 apply to the code;
keep intact all notices of the absence of any warranty; and give all
recipients a copy of this License along with the Program.

  You may charge any price or no price for each copy that you convey,
and you may offer support or warranty protection for a fee.

  5. Conveying Modified Source Versions.

  You may convey a work based on the Program, or the modifications to
produce it from the Program, in the form of source code under the
terms of section 4, provided that you also meet all of these conditions:

    a) The work must carry prominent notices stating that you modified
    it, and giving a relevant date.

    b) The work must carry prominent notices stating that it is
    released under this License and any conditions added under section
    7.  This requirement modifies the requirement in section 4 to
    "keep intact all notices".

    c) You must license the entire work, as a whole, under this
    License to anyone who comes into possession of a copy.  This
    License will therefore apply, along with any applicable section 7
    additional terms, to the whole of the work, and all its parts,
    regardless of how they are packaged.  This License gives no
    permission to license the work in any other way, but it does not
    invalidate such permission if you have separately received it.

    d) If the work has interactive user interfaces, each must display
    Appropriate Legal Notices; however, if the Program has interactive
    interfaces that do not display Appropriate Legal Notices, your
    work need not make them do so.

  A compilation of a covered work with other separate and independent
works, which are not by their nature extensions of the covered work,
and which are not combined with it such as to form a larger program,
in or on a volume of a storage or distribution medium, is called an
"aggregate" if the compilation and its resulting copyright are not
used to limit the access or legal rights of the compilation's users
beyond what the individual works permit.  Inclusion of a covered work
in an aggregate does not cause this License to apply to the other
parts of the aggregate.

  6. Conveying Non-Source Forms.

  You may convey a covered work in object code form under the terms
of sections 4 and 5, provided that you also convey the
machine-readable Corresponding Source under the terms of this License,
in one of these ways:

    a) Convey the object code in, or embodied in, a physical product
    (including a physical distribution medium), accompanied by the
    Corresponding Source fixed on a durable physical medium
    customarily used for software interchange.

    b) Convey the object code in, or embodied in, a physical product
    (including a physical distribution medium), accompanied by a
    written offer, valid for at least three years and valid for as
    long as you offer spare parts or customer support for that product
    model, to give anyone who possesses the object code either (1) a
    copy of the Corresponding Source for all the software in the
    product that is covered by this License, on a durable physical
    medium customarily used for software interchange, for a price no
    more than your reasonable cost of physically performing this
    conveying of source, or (2) access to copy the
    Corresponding Source from a network server at no charge.

    c) Convey individual copies of the object code with a copy of the
    written offer to provide the Corresponding Source.  This
    alternative is allowed only occasionally and noncommercially, and
    only if you received the object code with such an offer, in accord
    with subsection 6b.

    d) Convey the object code by offering access from a designated
    place (gratis or for a charge), and offer equivalent access to the
    Corresponding Source in the same way through the same place at no
    further charge.  You need not require recipients to copy the
    Corresponding Source along with the object code.  If the place to
    copy the object code is a network server, the Corresponding Source
    may be on a different server (operated by you or a third party)
    that supports equivalent copying facilities, provided you maintain
    clear directions next to the object code saying where to find the
    Corresponding Source.  Regardless of what server hosts the
    Corresponding Source, you remain obligated to ensure that it is
    available for as long as needed to satisfy these requirements.

    e) Convey the object code using peer-to-peer transmission, provided
    you inform other peers where the object code and Corresponding
    Source of the work are being offered to the general public at no
    charge under subsection 6d.

  A separable portion of the object code, whose source code is excluded
from the Corresponding Source as a System Library, need not be
included in conveying the object code work.

  A "User Product" is either (1) a "consumer product", which means any
tangible personal property which is normally used for personal, family,
or household purposes, or (2) anything designed or sold for incorporation
into a dwelling.  In determining whether a product is a consumer product,
doubtful cases shall be resolved in favor of coverage.  For a particular
product received by a particular user, "normally used" refers to a
typical or common use of that class of product, regardless of the status
of the particular user or of the way in which the particular user
actually uses, or expects or is expected to use, the product.  A product
is a consumer product regardless of whether the product has substantial
commercial, industrial or non-consumer uses, unless such uses represent
the only significant mode of use of the product.

  "Installation Information" for a User Product means any methods,
procedures, authorization keys, or other information required to install
and execute modified versions of a covered work in that User Product from
a modified version of its Corresponding Source.  The information must
suffice to ensure that the continued functioning of the modified object
code is in no case prevented or interfered with solely because
modification has been made.

  If you convey an object code work under this section in, or with, or
specifically for use in, a User Product, and the conveying occurs as
part of a transaction in which the right of possession and use of the
User Product is transferred to the recipient in perpetuity or for a
fixed term (regardless of how the transaction is characterized), the
Corresponding Source conveyed under this section must be accompanied
by the Installation Information.  But this requirement does not apply
if neither you nor any third party retains the ability to install
modified object code on the User Product (for example, the work has
been installed in ROM).

  The requirement to provide Installation Information does not include a
requirement to continue to provide support service, warranty, or updates
for a work that has been modified or installed by the recipient, or for
the User Product in which it has been modified or installed.  Access to a
network may be denied when the modification itself materially and
adversely affects the operation of the network or violates the rules and
protocols for communication across the network.

  Corresponding Source conveyed, and Installation Information provided,
in accord with this section must be in a format that is publicly
documented (and with an implementation available to the public in
source code form), and must require no special password or key for
unpacking, reading or copying.

  7. Additional Terms.

  "Additional permissions" are terms that supplement the terms of this
License by making exceptions from one or more of its conditions.
Additional permissions that are applicable to the entire Program shall
be treated as though they were included in this License, to the extent
that they are valid under applicable law.  If additional permissions
apply only to part of the Program, that part may be used separately
under those permissions, but the entire Program remains governed by
this License without regard to the additional permissions.

  When you convey a copy of a covered work, you may at your option
remove any additional permissions from that copy, or from any part of
it.  (Additional permissions may be written to require their own
removal in certain cases when you modify the work.)  You may place
additional permissions on material, added by you to a covered work,
for which you have or can give appropriate copyright permission.

  Notwithstanding any other provision of this License, for material you
add to a covered work, you may (if authorized by the copyright holders of
that material) supplement the terms of this License with terms:

    a) Disclaiming warranty or limiting liability differently from the
    terms of sections 15 and 16 of this License; or

    b) Requiring preservation of specified reasonable legal notices or
    author attributions in that material or in the Appropriate Legal
    Notices displayed by works containing it; or

    c) Prohibiting misrepresentation of the origin of that material, or
    requiring that modified versions of such material be marked in
    reasonable ways as different from the original version; or

    d) Limiting the use for publicity purposes of names of licensors or
    authors of the material; or

    e) Declining to grant rights under trademark law for use of some
    trade names, trademarks, or service marks; or

    f) Requiring indemnification of licensors and authors of that
    material by anyone who conveys the material (or modified versions of
    it) with contractual assumptions of liability to the recipient, for
    any liability that these contractual assumptions directly impose on
    those licensors and authors.

  All other non-permissive additional terms are considered "further
restrictions" within the meaning of section 10.  If the Program as you
received it, or any part of it, contains a notice stating that it is
governed by this License along with a term that is a further
restriction, you may remove that term.  If a license document contains
a further restriction but permits relicensing or conveying under this
License, you may add to a covered work material governed by the terms
of that license document, provided that the further restriction does
not survive such relicensing or conveying.

  If you add terms to a covered work in accord with this section, you
must place, in the relevant source files, a statement of the
additional terms that apply to those files, or a notice indicating
where to find the applicable terms.

  Additional terms, permissive or non-permissive, may be stated in the
form of a separately written license, or stated as exceptions;
the above requirements apply either way.

  8. Termination.

  You may not propagate or modify a covered work except as expressly
provided under this License.  Any attempt otherwise to propagate or
modify it is void, and will automatically terminate your rights under
this License (including any patent licenses granted under the third
paragraph of section 11).

  However, if you cease all violation of this License, then your
license from a particular copyright holder is reinstated (a)
provisionally, unless and until the copyright holder explicitly and
finally terminates your license, and (b) permanently, if the copyright
holder fails to notify you of the violation by some reasonable means
prior to 60 days after the cessation.

  Moreover, your license from a particular copyright holder is
reinstated permanently if the copyright holder notifies you of the
violation by some reasonable means, this is the first time you have
received notice of violation of this License (for any work) from that
copyright holder, and you cure the violation prior to 30 days after
your receipt of the notice.

  Termination of your rights under this section does not terminate the
licenses of parties who have received copies or rights from you under
this License.  If your rights have been terminated and not permanently
reinstated, you do not qualify to receive new licenses for the same
material under section 10.

  9. Acceptance Not Required for Having Copies.

  You are not required to accept this License in order to receive or
run a copy of the Program.  Ancillary propagation of a covered work
occurring solely as a consequence of using peer-to-peer transmission
to receive a copy likewise does not require acceptance.  However,
nothing other than this License grants you permission to propagate or
modify any covered work.  These actions infringe copyright if you do
not accept this License.  Therefore, by modifying or propagating a
covered work, you indicate your acceptance of this License to do so.

  10. Automatic Licensing of Downstream Recipients.

  Each time you convey a covered work, the recipient automatically
receives a license from the original licensors, to run, modify and
propagate that work, subject to this License.  You are not responsible
for enforcing compliance by third parties with this License.

  An "entity transaction" is a transaction transferring control of an
organization, or substantially all assets of one, or subdividing an
organization, or merging organizations.  If propagation of a covered
work results from an entity transaction, each party to that
transaction who receives a copy of the work also receives whatever
licenses to the work the party's predecessor in interest had or could
give under the previous paragraph, plus a right to possession of the
Corresponding Source of the work from the predecessor in interest, if
the predecessor has it or can get it with reasonable efforts.

  You may not impose any further restrictions on the exercise of the
rights granted or affirmed under this License.  For example, you may
not impose a license fee, royalty, or other charge for exercise of
rights granted under this License, and you may not initiate litigation
(including a cross-claim or counterclaim in a lawsuit) alleging that
any patent claim is infringed by making, using, selling, offering for
sale, or importing the Program or any portion of it.

  11. Patents.

  A "contributor" is a copyright holder who authorizes use under this
License of the Program or a work on which the Program is based.  The
work thus licensed is called the contributor's "contributor version".

  A contributor's "essential patent claims" are all patent claims
owned or controlled by the contributor, whether already acquired or
hereafter acquired, that would be infringed by some manner, permitted
by this License, of making, using, or selling its contributor version,
but do not include claims that would be infringed only as a
consequence of further modification of the contributor version.  For
purposes of this definition, "control" includes the right to grant
patent sublicenses in a manner consistent with the requirements of
this License.

  Each contributor grants you a non-exclusive, worldwide, royalty-free
patent license under the contributor's essential patent claims, to
make, use, sell, offer for sale, import and otherwise run, modify and
propagate the contents of its contributor version.

  In the following three paragraphs, a "patent license" is any express
agreement or commitment, however denominated, not to enforce a patent
(such as an express permission to practice a patent or covenant not to
sue for patent infringement).  To "grant" such a patent license to a
party means to make such an agreement or commitment not to enforce a
patent against the party.

  If you convey a covered work, knowingly relying on a patent license,
and the Corresponding Source of the work is not available for anyone
to copy, free of charge and under the terms of this License, through a
publicly available network server or other readily accessible means,
then you must either (1) cause the Corresponding Source to be so
available, or (2) arrange to deprive yourself of the benefit of the
patent license for this particular work, or (3) arrange, in a manner
consistent with the requirements of this License, to extend the patent
license to downstream recipients.  "Knowingly relying" means you have
actual knowledge that, but for the patent license, your conveying the
covered work in a country, or your recipient's use of the covered work
in a country, would infringe one or more identifiable patents in that
country that you have reason to believe are valid.

  If, pursuant to or in connection with a single transaction or
arrangement, you convey, or propagate by procuring conveyance of, a
covered work, and grant a patent license to some of the parties
receiving the covered work authorizing them to use, propagate, modify
or convey a specific copy of the covered work, then the patent license
you grant is automatically extended to all recipients of the covered
work and works based on it.

  A patent license is "discriminatory" if it does not include within
the scope of its coverage, prohibits the exercise of, or is
conditioned on the non-exercise of one or more of the rights that are
specifically granted under this License.  You may not convey a covered
work if you are a party to an arrangement with a third party that is
in the business of distributing software, under which you make payment
to the third party based on the extent of your activity of conveying
the work, and under which the third party grants, to any of the
parties who would receive the covered work from you, a discriminatory
patent license (a) in connection with copies of the covered work
conveyed by you (or copies made from those copies), or (b) primarily
for and in connection with specific products or compilations that
contain the covered work, unless you entered into that arrangement,
or that patent license was granted, prior to 28 March 2007.

  Nothing in this License shall be construed as excluding or limiting
any implied license or other defenses to infringement that may
otherwise be available to you under applicable patent law.

  12. No Surrender of Others' Freedom.

  If conditions are imposed on you (whether by court order, agreement or
otherwise) that contradict the conditions of this License, they do not
excuse you from the conditions of this License.  If you cannot convey a
covered work so as to satisfy simultaneously your obligations under this
License and any other pertinent obligations, then as a consequence you may
not convey it at all.  For example, if you agree to terms that obligate you
to collect a royalty for further conveying from those to whom you convey
the Program, the only way you could satisfy both those terms and this
License would be to refrain entirely from conveying the Program.

  13. Use with the GNU Affero General Public License.

  Notwithstanding any other provision of this License, you have
permission to link or combine any covered work with a work licensed
under version 3 of the GNU Affero General Public License into a single
combined work, and to convey the resulting work.  The terms of this
License will continue to apply to the part which is the covered work,
but the special requirements of the GNU Affero General Public License,
section 13, concerning interaction through a network will apply to the
combination as such.

  14. Revised Versions of this License.

  The Free Software Foundation may publish revised and/or new versions of
the GNU General Public License from time to time.  Such new versions will
be similar in spirit to the present version, but may differ in detail to
address new problems or concerns.

  Each version is given a distinguishing version number.  If the
Program specifies that a certain numbered version of the GNU General
Public License "or any later version" applies to it, you have the
option of following the terms and conditions either of that numbered
version or of any later version published by the Free Software
Foundation.  If the Program does not specify a version number of the
GNU General Public License, you may choose any version ever published
by the Free Software Foundation.

  If the Program specifies that a proxy can decide which future
versions of the GNU General Public License can be used, that proxy's
public statement of acceptance of a version permanently authorizes you
to choose that version for the Program.

  Later license versions may give you additional or different
permissions.  However, no additional obligations are imposed on any
author or copyright holder as a result of your choosing to follow a
later version.

  15. Disclaimer of Warranty.

  THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY
APPLICABLE LAW.  EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT
HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM "AS IS" WITHOUT WARRANTY
OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO,
THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
PURPOSE.  THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM
IS WITH YOU.  SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF
ALL NECESSARY SERVICING, REPAIR OR CORRECTION.

  16. Limitation of Liability.

  IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING
WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS
THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY
GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE
USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF
DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD
PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS),
EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF
SUCH DAMAGES.

  17. Interpretation of Sections 15 and 16.

  If the disclaimer of warranty and limitation of liability provided
above cannot be given local legal effect according to their terms,
reviewing courts shall apply local law that most closely approximates
an absolute waiver of all civil liability in connection with the
Program, unless a warranty or assumption of liability accompanies a
copy of the Program in return for a fee.

                     END OF TERMS AND CONDITIONS

            How to Apply These Terms to Your New Programs

  If you develop a new program, and you want it to be of the greatest
possible use to the public, the best way to achieve this is to make it
free software which everyone can redistribute and change under these terms.

  To do so, attach the following notices to the program.  It is safest
to attach them to the start of each source file to most effectively
state the exclusion of warranty; and each file should have at least
the "copyright" line and a pointer to where the full notice is found.

    <one line to give the program's name and a brief idea of what it does.>
    Copyright (C) <year>  <name of author>

    This program is free software: you can redistribute it and/or modify
    it under the terms of the GNU General Public License as published by
    the Free Software Foundation, either version 3 of the License, or
    (at your option) any later version.

    This program is distributed in the hope that it will be useful,
    but WITHOUT ANY WARRANTY; without even the implied warranty of
    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
    GNU General Public License for more details.

    You should have received a copy of the GNU General Public License
    along with this program.  If not, see <https://www.gnu.org/licenses/>.

Also add information on how to contact you by electronic and paper mail.

  If the program does terminal interaction, make it output a short
notice like this when it starts in an interactive mode:

    <program>  Copyright (C) <year>  <name of author>
    This program comes with ABSOLUTELY NO WARRANTY; for details type `show w'.
    This is free software, and you are welcome to redistribute it
    under certain conditions; type `show c' for details.

The hypothetical commands `show w' and `show c' should show the appropriate
parts of the General Public License.  Of course, your program's commands
might be different; for a GUI interface, you would use an "about box".

  You should also get your employer (if you work as a programmer) or school,
if any, to sign a "copyright disclaimer" for the program, if necessary.
For more information on this, and how to apply and follow the GNU GPL, see
<https://www.gnu.org/licenses/>.

  The GNU General Public License does not permit incorporating your program
into proprietary programs.  If your program is a subroutine library, you
may consider it more useful to permit linking proprietary applications with
the library.  If this is what you want to do, use the GNU Lesser General
Public License instead of this License.  But first, please read
<https://www.gnu.org/licenses/why-not-lgpl.html>.

```

`README.md`:

```md
# Gepetto

Gepetto is a Python plugin which uses various large language models to provide meaning to functions 
decompiled by IDA Pro (≥ 7.6). It can leverage them to explain what a function does, and to automatically 
rename its variables. Here is a simple example of what results it can provide in mere seconds:

![](https://github.com/JusticeRage/Gepetto/blob/main/readme/comparison.png?raw=true)

## Setup

### Using hcli (Recommended)
The easiest way to install Gepetto is using the [Hex-Rays CLI tool (hcli)](https://github.com/HexRaysSA/ida-hcli):
```bash
pip install ida-hcli
hcli plugin install gepetto
```

This will automatically install the plugin to your IDA user directory.

### Manual Installation
Alternatively, you can manually install the plugin:
1. Drop this script (`gepetto.py`, as well as the `gepetto/` folder) into your IDA plugins folder (`$IDAUSR/plugins`).
2. The plugins directory location depends on your system:
   - **Windows**: `%APPDATA%\Hex-Rays\IDA Pro\plugins\`
   - **macOS**: `~/Library/Application Support/IDA Pro/plugins/`
   - **Linux**: `~/.idapro/plugins/`
3. Install the required packages to IDA's Python installation. Find which interpreter IDA is using by checking the following registry key:
   `Computer\HKEY_CURRENT_USER\Software\Hex-Rays\IDA` (default on Windows: `%LOCALAPPDATA%\Programs\Python\Python39`).
4. With the corresponding interpreter, simply run:
   ```bash
   [/path/to/python] -m pip install -r requirements.txt
   ```

⚠️ You will also need to edit the configuration file (found as `gepetto/config.ini`) and add your own API keys. For 
OpenAI, it can be found on [this page](https://platform.openai.com/api-keys).
Please note that API queries are usually not free (although not very expensive) and you will need to set up a payment 
method with the corresponding provider.

## Supported models

- [OpenAI](https://playground.openai.com/)
  - gpt-5
  - gpt-5-mini
  - gpt-5-nano
  - gpt-4-turbo
  - gpt-4o
  - o4-mini
  - gpt-4.1
  - o3
  - o3-pro
- [Google Gemini](https://ai.google.dev/)
  - gemini-2.0-flash
  - gemini-2.5-pro
  - gemini-2.5-flash
  - gemini-2.5-flash-lite-preview-06-17
- [Azure OpenAI](https://ai.azure.com/)
  - gpt-35-turbo
  - gpt-35-turbo-1106
  - gpt-35-turbo-16k
  - gpt-4-turbo
  - gpt-4-turbo-2024-0409-gs
- [Ollama](https://ollama.com/)
  - Any local model exposed through Ollama (will not appear if Ollama is not running)
- [Groq](https://console.groq.com/playground)
  - llama-3.1-70b-versatile
  - llama-3.2-90b-text-preview
  - mixtral-8x7b-32768
- [Together](https://api.together.ai/)
  - mistralai/Mixtral-8x22B-Instruct-v0.1 (does not support renaming variables)
- [Novita AI](https://novita.ai/)
  - deepseek/deepseek-r1
  - deepseek/deepseek-v3
  - meta-llama/llama-3.3-70b-instruct
  - meta-llama/llama-3.1-70b-instruct
  - meta-llama/llama-3.1-405b-instruct
- [Kluster.ai](https://kluster.ai/)
  - deepseek-ai/DeepSeek-R1
  - deepseek-ai/DeepSeek-V3-0324
  - google/gemma-3-27b-it
  - klusterai/Meta-Llama-3.1-8B-Instruct-Turbo
  - klusterai/Meta-Llama-3.1-405B-Instruct-Turbo
  - klusterai/Meta-Llama-3.3-70B-Instruct-Turbo
  - meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8
  - meta-llama/Llama-4-Scout-17B-16E-Instruct
  - Qwen/Qwen2.5-VL-7B-Instruct
- [LM Studio](https://lmstudio.ai/)
  - Any local model exposed through LM Studio (will not appear if LM Studio Developer server is not running)

Adding support for additional models shouldn't be too difficult, provided whatever provider you're considering exposes
an API similar to OpenAI's. Look into the `gepetto/models` folder for inspiration, or open an issue if you can't figure
it out.

## Usage

Once the plugin is installed properly, you should be able to invoke it from the context menu of IDA's pseudocode window,
as shown in the screenshot below:

![](https://github.com/JusticeRage/Gepetto/blob/main/readme/usage.png?raw=true)

Switch between models supported by Gepetto from the Edit > Gepetto menu:

![](https://github.com/JusticeRage/Gepetto/blob/main/readme/select_model.png?raw=true)

Gepetto also provides a CLI interface you can use to ask questions to the LLM directly from IDA. Make sure to select
`Gepetto` in the input bar:

![](https://github.com/JusticeRage/Gepetto/blob/main/readme/cli.png?raw=true)

### Hotkeys

The following hotkeys are available:

- Ask the model to explain the function: `Ctrl` + `Alt` + `G`
- Ask the model to add comments to the code: `Ctrl` + `Alt` + `K`
- Request better names for the function's variables: `Ctrl` + `Alt` + `R`

Initial testing shows that asking for better names works better if you ask for an explanation of the function first – I
assume because the model then uses its own comment to make more accurate suggestions.
There is an element of randomness to the AI's replies. If for some reason the initial response you get doesn't suit you,
you can always run the command again.

## Limitations

- The plugin requires access to the HexRays decompiler to function.
- All supported LLMs are general-purpose and may very well get things wrong! Always be 
  critical of results returned!

## Translations

You can change Gepetto's language by editing the locale in the configuration. For instance, to use the plugin
in French, you would simply add:

```ini
[Gepetto]
LANGUAGE = "fr_FR"
```

The chosen locale must match the folder names in `gepetto/locales`. If the desired language isn't available,
you can contribute to the project by adding it yourself! Create a new folder for the desired locale
(ex: `gepetto/locales/de_DE/LC_MESSAGES/`), and open a new pull request with the updated `.po` file, which you can
create by copying and editing `gepetto/locales/gepetto.pot` (replace all the lines starting with `msgstr` with the
localized version).

## Acknowledgements

- [OpenAI](https://openai.com), for making these incredible models, obviously
- [Hex Rays](https://hex-rays.com/), the makers of IDA for their lightning fast support
- [Kaspersky](https://kaspersky.com), for initially funding this project
- [HarfangLab](https://harfanglab.io/), the current backer making this work possible
- [@vanhauser-thc](https://github.com/vanhauser-thc) for contributing ideas of additional models and providers to support via his [fork](https://github.com/vanhauser-thc/gepetto/)
- Everyone who contributed translations: @seifreed, @kot-igor, @ruzgarkanar, @orangetw

```

`babel.cfg`:

```cfg
[python: **.py]
encoding = utf-8
keywords = _

```

`gepetto.py`:

```py
import gepetto.config


def PLUGIN_ENTRY():
    gepetto.config.load_config()  # Loads configuration data from gepetto/config.ini

    # Only import the rest of the code after the translations have been loaded, because the _ function (gettext)
    # needs to have been imported in the namespace first.
    from gepetto.ida.ui import GepettoPlugin
    return GepettoPlugin()

```

`gepetto/config.ini`:

```ini
[Gepetto]
MODEL = gpt-4o

# Specify the program language. It can be "fr_FR", "zh_CN", or any folder in locales. Defaults to English.
LANGUAGE =

# Specify a proxy here if you want to route requests via one.
PROXY =

# Where to place code comments: "above" (before the line/block) or "side" (inline/right).
COMMENT_POSITION = above
# Automatically focus the Gepetto status panel when a request starts.
AUTO_SHOW_STATUS_PANEL = true

[OpenAI]
# Set your API key here, or put it in the OPENAI_API_KEY environment variable.
API_KEY = 

# Base URL if you want to redirect requests to a different / local model.
# Can also be provided via the OPENAI_BASE_URL environment variable.
# Leave blank unless you know what you are doing :)
BASE_URL =

[Gemini]
# Set your Google Gemini API key here, or put it in the GEMINI_API_KEY environment variable.
API_KEY = 

# Base URL for the Gemini API. Defaults to Google's official endpoint.
# Leave blank unless you have a specific reason to change it.
BASE_URL =

[AzureOpenAI]
# Set your API key here, or put it in the AZURE_OPENAI_API_KEY environment variable.
# If API_KEY is not set, the plugin will use Entra ID auth
API_KEY =

# Set your Azure OpenAI resource URL here, or put it in the AZURE_OPENAI_URL environment variable.
BASE_URL =

[LMStudio]
BASE_URL =

[Groq]
# OPTIONAL, create a Groq account only if you want to use the LLaMA models.
# Set your API key here, or put it in the GROQ_API_KEY environment variable.
API_KEY =

# Base URL if you want to redirect requests to a different / local model.
# Can also be provided via the GROQ_BASE_URL environment variable.
BASE_URL =

[Together]
# Optional, create a Together account only if you want to use the Mistral models.
# Set your API key here, or put it in the TOGETHER_API_KEY environment variable.
API_KEY =

# Base URL if you want to redirect requests to a different / local model.
# Can also be provided via the TOGETHER_BASE_URL environment variable.
BASE_URL =

[NovitaAI]
# Optional, create a Novita.AI account as an alternate provider for LLaMA models
# and others. Set your API key here, or put it in the NOVITAAI_API_KEY environment variable.
API_KEY =

[Ollama]
# Endpoint used to connect to the Ollama API. Default is http://localhost:11434
HOST =

[DeepSeek]
# Set your API key here, or put it in the DEEPSEEK_API_KEY environment variable.
API_KEY =

# Base URL if you want to redirect requests to a different / local model.
# Can also be provided via the DEEPSEEK_BASE_URL environment variable.
BASE_URL =

[OpenRouter]
# Set your API key here, or put it in the OPENROUTER_API_KEY environment variable.
API_KEY =

# Base URL if you want to redirect requests to a different / local model.
# Can also be provided via the OPENROUTER_BASE_URL environment variable.
BASE_URL =

# Models that will be available in the model selection menu.
MODELS =

[SiliconFlow]
# Set your SiliconFlow API key here
API_KEY = 

# Base URL if you want to redirect requests to a different / local model.
BASE_URL =

# Models that will be available in the model selection menu.
MODELS = 

[Aliyun]
# Set your Aliyun API key here
API_KEY = 

# Base URL if you want to redirect requests to a different / local model.
BASE_URL =

# Models that will be available in the model selection menu.
MODELS = 

[OpenAICompatible]
# Set your OpenAI-compatible API provider name here.
NAME =

# Set your API key here, or put it in the OPENAI_COMPATIBLE_API_KEY environment variable.
API_KEY =

# Base URL if you want to redirect requests to a different / local model.
# Can also be provided via the OPENAI_COMPATIBLE_BASE_URL environment variable.
BASE_URL =

# Models that will be available in the model selection menu.
MODELS =

[Kluster]
# Set your Kluster.ai API key here
API_KEY = 

# Base URL if you want to redirect requests to a different endpoint.
# Default is https://api.kluster.ai/v1
BASE_URL =

# Optional: Override the default models that will be available in the model selection menu.
# Leave empty to use all available models.
MODELS =

```

`gepetto/config.py`:

```py
import configparser
import gettext
import os

from gepetto.models.model_manager import instantiate_model, load_available_models, get_fallback_model

# =============================================================================
# Global Fields
# =============================================================================

# Active language model instance for processing requests
model = None

# INI configuration file parser object
parsed_ini = None

# Translator function for message localization
_translator = None

# Current localization language, loaded from configuration file
language = None

# Available locales, loaded from the locales directory
available_locales = None

# =============================================================================


def _stringify_config_value(value) -> str:
    if isinstance(value, bool):
        return "true" if value else "false"
    return str(value)


def _get_translator():
    global _translator
    if _translator is None:
        load_config()

    return _translator


def _(message):
    """Translation function that lazy-loads the translator"""
    return _get_translator()(message)


def load_config():
    """
    Loads the configuration of the plugin from the INI file. Sets up the correct locale and language model.
    Also prepares an OpenAI client configured accordingly to the user specifications.
    :return:
    """
    global model, parsed_ini, _translator, language, available_locales
    parsed_ini = configparser.RawConfigParser()
    parsed_ini.read(os.path.join(os.path.abspath(os.path.dirname(__file__)), "config.ini"), encoding="utf-8")

    # Read available locales from the locales directory
    locales_dir = os.path.join(os.path.abspath(os.path.dirname(__file__)), "locales")
    available_locales = set()
    if os.path.exists(locales_dir):
        for item in os.listdir(locales_dir):
            item_path = os.path.join(locales_dir, item)
            if os.path.isdir(item_path) and not item.startswith('.'):
                available_locales.add(item)

    # Set up translations
    language = parsed_ini.get('Gepetto', 'LANGUAGE')
    translate = gettext.translation('gepetto',
                                    locales_dir,
                                    fallback=True,
                                    languages=[language])
    _translator = translate.gettext

    # Select model
    requested_model = parsed_ini.get('Gepetto', 'MODEL')
    load_available_models()
    # Attempt to load the requested model, otherwise get the first available one, or don't load Gepetto
    try:
        model = instantiate_model(requested_model)
    except RuntimeError:
        print(_("Attempting to load the first available model..."))
        try:
            model = get_fallback_model()
            print(f"Defaulted to {str(model)}.")
        except RuntimeError:
            print(_("No model available. Please edit the configuration file and try again."))
            model = None


def get_config(section, option, environment_variable=None, default=None):
    """
    Returns a value from the configuration, by looking successively in the configuration file and the environment
    variables, returning the default value provided if nothing can be found.
    :param section: The section containing the option.
    :param option: The requested option.
    :param environment_variable: The environment variable possibly containing the value.
    :param default: Default value to return if nothing can be found.
    :return: The value of the requested option.
    """
    global parsed_ini
    try:
        if parsed_ini and parsed_ini.get(section, option):
            return parsed_ini.get(section, option)
        if environment_variable and os.environ.get(environment_variable):
            return os.environ.get(environment_variable)
    except (configparser.NoSectionError, configparser.NoOptionError):
        print(_("Warning: Gepetto's configuration doesn't contain option {option} in section {section}!").format(
            option=option,
            section=section
        ))
    return default


def update_config(section, option, new_value):
    """
    Updates a single entry in the configuration.
    :param section: The section in which the option is located
    :param option: The option to update
    :param new_value: The new value to set
    :return:
    """
    path = os.path.join(os.path.abspath(os.path.dirname(__file__)), "config.ini")
    config = configparser.RawConfigParser()
    config.read(path, encoding="utf-8")
    config.set(section, option, _stringify_config_value(new_value))
    with open(path, "w", encoding="utf-8") as f:
        config.write(f)

    global parsed_ini
    if parsed_ini is not None:
        if not parsed_ini.has_section(section):
            parsed_ini.add_section(section)
        parsed_ini.set(section, option, _stringify_config_value(new_value))


def get_localization_locale():
    """
    Returns a valid language locale. If the current language is not valid,
    returns 'en_US' as the default.
    :return: Valid language locale string
    """
    global language, available_locales
    
    # Check if current language is valid
    if language and language in available_locales:
        return language
    
    # Return default locale if current language is invalid
    return 'en_US'

def auto_show_status_panel_enabled() -> bool:
    global parsed_ini
    if parsed_ini is None:
        load_config()
    try:
        return parsed_ini.getboolean("Gepetto", "AUTO_SHOW_STATUS_PANEL")
    except (configparser.NoOptionError, configparser.NoSectionError, ValueError):
        return True


def set_auto_show_status_panel(enabled: bool) -> None:
    update_config("Gepetto", "AUTO_SHOW_STATUS_PANEL", enabled)

```

`gepetto/ida/cli.py`:

```py
from types import SimpleNamespace

import ida_kernwin
import ida_idaapi

import gepetto.config
import gepetto.ida.handlers
from gepetto.ida.status_panel.panel_interface import LogCategory, LogLevel
from gepetto.ida.status_panel.status_panel_factory import get_status_panel
from gepetto.ida.tools.tools import TOOLS
import gepetto.ida.tools as ida_tools

_ = gepetto.config._
STATUS_PANEL = get_status_panel()
CLI: ida_kernwin.cli_t = None
MESSAGES: list[dict] = [
    {
        "role": "system",
        "content":
            f"You are a helpful assistant embedded in IDA Pro. Your role is to facilitate "
            f"reverse-engineering and answer programming questions.\n"
            f"Your response MUST ALWAYS be in the following locale: {gepetto.config.get_localization_locale()}\n"
            f"If you format your response, you MUST ALWAYS use basic Markdown; but formatting is not required.\n"
            f"Bullets start at column 0 with - or * ; do not indent lists, use HTML tags, or pre blocks.\n"
            f"Never repeat pseudocode back as the user can see it already.\n"
            f"In the context of a reverse-engineering session, the user will switch from function to function a lot. "
            f"Between messages, don't assume that the function is still the same and always confirm it by checking the "
            f"current EA. \"This\" function or the \"current\" function always mean the one at the current EA.\n"
            f"When encountering a symbol name like sub_401234 or off_415678, assume that the corresponding address is "
            f"0x401234 or 0x415678 (respectively) without calling tools.\n"
            f"When asked to perform an operation (such as renaming something), don't ask for confirmation. Just do it!\n"
            f"Always refresh the disassembly view after making a change in the IDB (renaming, etc.), so it is shown to"
            f"the user (no need to mention when you do it).\n"
            f"Addresses should always be shown as hex in the form 0x1234, but never convert decimal numbers to "
            f"hexadecimal yourself; always use the `to_hex` tool for that.\n"
            f"If you ever encounter a tool error, don't try again, print the exception and stop.",
    }
]  # Keep a history of the conversation to simulate LLM memory.

_REASONING_KEYS = ("reasoning", "thinking", "thought", "internal_monologue")
_REASONING_TYPES = {"reasoning", "thinking", "thought"}


def _collect_reasoning_text(value, collector, depth=0):
    if depth > 5 or value is None:
        return
    if isinstance(value, str):
        if value.strip():
            collector.append(value)
        return
    if isinstance(value, (list, tuple, set)):
        for item in value:
            _collect_reasoning_text(item, collector, depth + 1)
        return
    if isinstance(value, dict):
        value_type = value.get("type")
        if value_type in _REASONING_TYPES:
            candidate = value.get("text") or value.get("content")
            _collect_reasoning_text(candidate, collector, depth + 1)
            return
        for key in _REASONING_KEYS:
            if key in value:
                _collect_reasoning_text(value[key], collector, depth + 1)
        if "text" in value:
            _collect_reasoning_text(value["text"], collector, depth + 1)
        if "content" in value:
            _collect_reasoning_text(value["content"], collector, depth + 1)
        return

    for attr in ("text", "content"):
        if hasattr(value, attr):
            _collect_reasoning_text(getattr(value, attr), collector, depth + 1)
    remaining = str(value).strip()
    if remaining:
        collector.append(remaining)


def _append_reasoning_from_delta(delta) -> None:
    if delta is None:
        return
    containers = []
    if isinstance(delta, dict):
        containers.extend(delta.get(key) for key in _REASONING_KEYS if delta.get(key))
        content = delta.get("content")
    elif isinstance(delta, str):
        return
    else:
        for key in _REASONING_KEYS:
            value = getattr(delta, key, None)
            if value:
                containers.append(value)
        content = getattr(delta, "content", None)

    if isinstance(content, (list, tuple)):
        for item in content:
            item_type = None
            item_text = None
            if isinstance(item, dict):
                item_type = item.get("type")
                item_text = item.get("text") or item.get("content")
            else:
                item_type = getattr(item, "type", None)
                item_text = getattr(item, "text", None) or getattr(item, "content", None)
            if item_type in _REASONING_TYPES:
                containers.append(item_text)

    texts: list[str] = []
    for container in containers:
        _collect_reasoning_text(container, texts)

    for chunk in texts:
        STATUS_PANEL.append_reasoning(chunk)


class GepettoCLI(ida_kernwin.cli_t):
    flags = 0
    sname = "Gepetto"
    lname  = "Gepetto - " + _("LLM chat")
    hint = "Gepetto"

    def OnExecuteLine(self, line):
        if not line.strip():  # Don't do anything for empty sends.
            return True

        MESSAGES.append({"role": "user", "content": line})
        if gepetto.config.auto_show_status_panel_enabled():
            STATUS_PANEL.ensure_shown()
        STATUS_PANEL.set_stop_callback(getattr(gepetto.config.model, "cancel_current_request", None))
        STATUS_PANEL.log_user(line)
        STATUS_PANEL.start_stream()
        STATUS_PANEL.log_request_started()

        stream_retry_attempted = False

        def handle_response(response):
            if hasattr(response, "tool_calls") and response.tool_calls:
                tool_calls = []
                STATUS_PANEL.finish_stream(response.content or "")
                STATUS_PANEL.finish_reasoning()
                for tc in response.tool_calls:
                    tool_calls.append({
                        "id": tc.id,
                        "type": tc.type,
                        "function": {
                            "name": tc.function.name,
                            "arguments": tc.function.arguments,
                        },
                    })
                    STATUS_PANEL.log(
                        _("→ Model requested tool: {tool_name} ({tool_args}...)").format(
                            tool_name=tc.function.name,
                            tool_args=(tc.function.arguments or "")[:120],
                        ),
                        category=LogCategory.TOOL,
                    )
                MESSAGES.append(
                    {
                        "role": "assistant",
                        "content": response.content or "",
                        "tool_calls": tool_calls,
                    }
                )

                first_tool_name = response.tool_calls[0].function.name
                STATUS_PANEL.set_status(_("Using tool: {tool_name}").format(tool_name=first_tool_name), busy=True)
                for tc in response.tool_calls:
                    if tc.function.name == "get_screen_ea":
                        ida_tools.get_screen_ea.handle_get_screen_ea_tc(tc, MESSAGES)
                    elif tc.function.name == "get_current_function":
                        ida_tools.get_current_function.handle_get_current_function_tc(tc, MESSAGES)
                    elif tc.function.name == "get_ea":
                        ida_tools.get_ea.handle_get_ea_tc(tc, MESSAGES)
                    elif tc.function.name == "decompile_function":
                        ida_tools.decompile_function.handle_decompile_function_tc(tc, MESSAGES)
                    elif tc.function.name == "rename_lvar":
                        ida_tools.rename_lvar.handle_rename_lvar_tc(tc, MESSAGES)
                    elif tc.function.name == "rename_function":
                        ida_tools.rename_function.handle_rename_function_tc(tc, MESSAGES)
                    elif tc.function.name == "set_comment":
                        ida_tools.set_comment.handle_set_comment_tc(tc, MESSAGES)
                    elif tc.function.name == "get_xrefs":
                        ida_tools.get_xrefs.handle_get_xrefs_tc(tc, MESSAGES)
                    elif tc.function.name == "list_imports":
                        ida_tools.list_imports.handle_list_imports_tc(tc, MESSAGES)
                    elif tc.function.name == "list_functions":
                        ida_tools.list_functions.handle_list_functions_tc(tc, MESSAGES)
                    elif tc.function.name == "list_symbols":
                        ida_tools.list_symbols.handle_list_symbols_tc(tc, MESSAGES)
                    elif tc.function.name == "list_strings":
                        ida_tools.search.handle_list_strings_tc(tc, MESSAGES)
                    elif tc.function.name == "search":
                        ida_tools.search.handle_search_tc(tc, MESSAGES)
                    elif tc.function.name == "to_hex":
                        ida_tools.to_hex.handle_to_hex_tc(tc, MESSAGES)
                    elif tc.function.name == "get_disasm":
                        ida_tools.get_disasm.handle_get_disasm_tc(tc, MESSAGES)
                    elif tc.function.name == "disasm_function":
                        ida_tools.disasm_function.handle_disasm_function_tc(tc, MESSAGES)
                    elif tc.function.name == "get_bytes":
                        ida_tools.get_bytes.handle_get_bytes_tc(tc, MESSAGES)
                    elif tc.function.name == "declare_c_type":
                        ida_tools.declare_c_type.handle_declare_c_type_tc(tc, MESSAGES)
                    elif tc.function.name == "get_struct":
                        ida_tools.get_struct.handle_get_struct_tc(tc, MESSAGES)
                    elif tc.function.name == "refresh_view":
                        ida_tools.refresh_view.handle_refresh_view_tc(tc, MESSAGES)
                    elif tc.function.name == "run_python":
                        ida_tools.run_python.handle_run_python_tc(tc, MESSAGES)
                    elif tc.function.name == "rename_global":
                        ida_tools.rename_global.handle_rename_global_tc(tc, MESSAGES)
                STATUS_PANEL.start_stream()
                start_model_interaction()
            else:
                content = response.content or ""
                MESSAGES.append({"role": "assistant", "content": content})
                STATUS_PANEL.finish_stream(content)
                STATUS_PANEL.finish_reasoning()
                STATUS_PANEL.log(
                    _("✔ Completed turn"),
                    category=LogCategory.SYSTEM,
                    level=LogLevel.SUCCESS,
                )

        def handle_non_streaming_response(response):
            """Handle non-streaming response from the model."""
            if hasattr(response, "error"):
                error_text = str(response.error) if response.error else _("Model request failed.")
                STATUS_PANEL.mark_error(error_text)
                return
            
            # Print the response content to console
            if response.content:
                print(response.content, end="\n\n")
            
            handle_response(response)

        def handle_streaming():
            message = SimpleNamespace(content="", tool_calls=[])
            last_error_message = ""

            def handle_model_error(text: str | None) -> None:
                nonlocal last_error_message, stream_retry_attempted
                error_text = (text or "").strip()
                if not error_text:
                    error_text = _("Model request failed.")
                last_error_message = error_text
                STATUS_PANEL.finish_stream(message.content)
                STATUS_PANEL.finish_reasoning()
                supports_streaming_method = getattr(gepetto.config.model, "supports_streaming", None)
                should_retry_without_stream = (
                    not stream_retry_attempted
                    and callable(supports_streaming_method)
                    and not supports_streaming_method()
                )
                if should_retry_without_stream:
                    stream_retry_attempted = True
                    STATUS_PANEL.log(
                        _("Streaming unavailable for this model; retrying without streaming."),
                        category=LogCategory.SYSTEM,
                    )
                    STATUS_PANEL.set_status(_("Retrying without streaming"), busy=True)
                    STATUS_PANEL.start_stream()
                    start_model_interaction()
                    return
                STATUS_PANEL.mark_error(error_text)

            def on_chunk(delta, finish_reason):
                nonlocal last_error_message
                delta_error = None
                if isinstance(delta, dict):
                    delta_error = delta.get("error")
                elif delta is not None:
                    delta_error = getattr(delta, "error", None)
                if delta_error:
                    handle_model_error(str(delta_error))
                    return
                if isinstance(delta, str):
                    print(delta, end="", flush=True)
                    message.content += delta
                    STATUS_PANEL.append_stream(delta)
                    _append_reasoning_from_delta(delta)
                    return
                if getattr(delta, "content", None):
                    print(delta.content, end="", flush=True)
                    message.content += delta.content
                    STATUS_PANEL.append_stream(delta.content)
                _append_reasoning_from_delta(delta)
                if getattr(delta, "tool_calls", None):
                    for tc in delta.tool_calls:
                        idx = tc.index
                        while len(message.tool_calls) <= idx:
                            message.tool_calls.append(
                                SimpleNamespace(
                                    id="",
                                    type="",
                                    function=SimpleNamespace(name="", arguments=""),
                                )
                            )
                        current = message.tool_calls[idx]
                        if getattr(tc, "id", None):
                            current.id = tc.id
                        if getattr(tc, "type", None):
                            current.type = tc.type
                        if getattr(tc, "function", None):
                            fn = tc.function
                            if getattr(fn, "name", None):
                                current.function.name += fn.name
                            if getattr(fn, "arguments", None):
                                current.function.arguments += fn.arguments
                if finish_reason:
                    if finish_reason == "error":
                        handle_model_error(last_error_message)
                        return
                    if finish_reason != "tool_calls":
                        print("\n\n")  # Add a blank line after the model's reply for readability.
                    STATUS_PANEL.finish_reasoning()
                    handle_response(message)

            gepetto.config.model.query_model_async(
                MESSAGES,
                on_chunk,
                stream=True,
                additional_model_options={"tools": TOOLS},
            )

        def start_model_interaction():
            supports_streaming = True
            supports_streaming_method = getattr(gepetto.config.model, "supports_streaming", None)
            if callable(supports_streaming_method):
                try:
                    supports_streaming = bool(supports_streaming_method())
                except Exception:
                    supports_streaming = True
            if supports_streaming:
                handle_streaming()
            else:
                gepetto.config.model.query_model_async(
                    MESSAGES,
                    handle_non_streaming_response,
                    stream=False,
                    additional_model_options={"tools": TOOLS},
                )

        print()  # Add a line break before the model's response to improve readability
        start_model_interaction()
        return True

    def OnKeydown(self, line, x, sellen, vkey, shift):
        pass

# -----------------------------------------------------------------------------

def cli_lifecycle_callback(code, old=0):
    if code == ida_idaapi.NW_OPENIDB:
        CLI.register()
    elif code == ida_idaapi.NW_CLOSEIDB or code == ida_idaapi.NW_TERMIDA:
        CLI.unregister()

# -----------------------------------------------------------------------------

def register_cli():
    global CLI
    if CLI:
        CLI.unregister()
        cli_lifecycle_callback(ida_idaapi.NW_TERMIDA)
    CLI = GepettoCLI()
    if CLI.register():
        ida_idaapi.notify_when(ida_idaapi.NW_TERMIDA | ida_idaapi.NW_OPENIDB | ida_idaapi.NW_CLOSEIDB, cli_lifecycle_callback)

```

`gepetto/ida/comment_handler.py`:

```py
import functools
import json
import time

import idaapi  # type: ignore
import ida_hexrays  # type: ignore
import ida_kernwin  # type: ignore

import gepetto.config
from gepetto.ida.status_panel.panel_interface import LogCategory, LogLevel
from gepetto.ida.status_panel.status_panel_factory import get_status_panel
from gepetto.ida.tools.decompile_function import decompile_function
from gepetto.ida.utils.thread_helpers import run_on_main_thread, safe_get_screen_ea

_ = gepetto.config._

STATUS_PANEL = get_status_panel()

# -----------------------------------------------------------------------------

class CommentHandler(idaapi.action_handler_t):
    """
    This handler queries the model to generate a comment for the
    selected function. Once the reply is received, it is added
    as a function comment.
    """

    def __init__(self):
        idaapi.action_handler_t.__init__(self)

    def activate(self, ctx):
        start_time = time.time()
        localization_locale = gepetto.config.get_localization_locale()

        ea = safe_get_screen_ea()
        if ea == idaapi.BADADDR:
            try:
                ida_kernwin.warning(_("No focused view available; cannot determine current function."))
            except Exception:
                print(_("No focused view available; cannot determine current function."))
            return 1

        try:
            decompiler_output = decompile_function(ea=ea)
        except RuntimeError as exc:
            message = str(exc)
            try:
                ida_kernwin.warning(message)
            except Exception:
                print(message)
            return 1

        pseudocode_lines = get_commentable_lines(decompiler_output)
        formatted_lines = format_commentable_lines(pseudocode_lines)
        v = ida_hexrays.get_widget_vdui(ctx.widget)
              
        gepetto.config.model.query_model_async(
            f"""
                You are a reverse-engineering assistant adding helpful pseudocode comments.
                - Locale: {localization_locale}
                - Output format (strict): exactly one JSON object mapping integer lineNumber → string comment.
                  * No Markdown, no code fences, no explanations outside the JSON object.
                  * If no comments are warranted, return {{}}.
                - Scope: Only annotate lines that start with '+' in the listing below.
                - Guidance: Explain intent, side-effects, or non-obvious control flow. Skip trivial operations.
                - Style: Keep comments concise (one sentence when possible) and use imperative or descriptive voice.
                \n
                ```C
                {formatted_lines}
                ```
              """,
            functools.partial(comment_callback, decompiler_output=decompiler_output, pseudocode_lines=pseudocode_lines, view=v, start_time=start_time),
            additional_model_options={"response_format": {"type": "json_object"}})
        request_sent = STATUS_PANEL.log_request_started()
        print(request_sent)
        return 1

    # This action is always available.
    def update(self, ctx):
        return idaapi.AST_ENABLE_ALWAYS

# -----------------------------------------------------------------------------

def comment_callback(decompiler_output, pseudocode_lines, view, response, start_time):
    """Callback that sets comments returned by the model at given lines.

    The ``response`` parameter can either be a raw string or a message object
    returned by the OpenAI API.  This keeps compatibility with older behaviour
    while enabling tool-based interactions that return message objects.
    """
    try:
        elapsed_time = time.time() - start_time

        response_text = response.content if hasattr(response, "content") else response
        if not isinstance(response_text, str):
            response_text = str(response_text)
        print(f"Response: {response_text}")

        try:
            items = json.loads(response_text)
        except Exception as exc:
            error_message = _("[ERROR] comment callback JSON failure: {error}").format(error=str(exc))
            print(error_message)
            STATUS_PANEL.mark_error(error_message)
            STATUS_PANEL.log_request_finished(elapsed_time)
            return

        def _apply_comments():
            applied_local = 0
            for line_key, raw_comment in items.items():
                try:
                    line_index = int(line_key)
                except (TypeError, ValueError):
                    continue

                if line_index < 0 or line_index >= len(pseudocode_lines):
                    continue

                comment_address = pseudocode_lines[line_index][2]
                comment_placement = pseudocode_lines[line_index][3]
                if comment_placement is None:
                    comment_placement = idaapi.ITP_SEMI

                if comment_address is None or comment_address == idaapi.BADADDR:
                    continue

                comment_text = str(raw_comment).strip()
                if not comment_text:
                    continue

                target = idaapi.treeloc_t()
                target.ea = int(comment_address)
                target.itp = comment_placement
                decompiler_output.set_user_cmt(target, comment_text)
                applied_local += 1

            decompiler_output.save_user_cmts()
            decompiler_output.del_orphan_cmts()

            if view:
                view.refresh_view(True)
            return applied_local

        try:
            applied = run_on_main_thread(_apply_comments, write=True) or 0
        except Exception as exc:
            error_message = _("[ERROR] comment application failed: {error}").format(error=str(exc))
            print(error_message)
            STATUS_PANEL.mark_error(error_message)
            STATUS_PANEL.log_request_finished(elapsed_time)
            return

        if applied:
            STATUS_PANEL.log(
                _("Applied {count} comments.").format(count=applied),
                category=LogCategory.TOOL,
                level=LogLevel.SUCCESS,
            )
        else:
            STATUS_PANEL.log(
                _("No comments were applied."),
                category=LogCategory.TOOL,
            )

        response_finished = STATUS_PANEL.log_request_finished(elapsed_time)
        print(response_finished)

    except Exception as e:
        error_message = _("[ERROR] comment_callback: {error}").format(error=e)
        print(error_message)
        STATUS_PANEL.mark_error(error_message)
        raise


# -----------------------------------------------------------------------------

def get_commentable_lines(cfunc):
    """
    Extracts information for each line of decompiled pseudocode, including:
      - lineIndex: Line number in the pseudocode listing (starting from 0).
      - lineText: Cleaned text of the line (IDA formatting tags removed).
      - comment_address: Address in the decompiled function suitable for attaching a comment, or BADADDR if unavailable.
      - comment_placement: Comment placement type (e.g., ITP_SEMI, ITP_COLON), or 0 if unavailable.
      - has_user_comment: True if a user comment already exists for this line, otherwise False.

    Args:
        cfunc (idaapi.cfuncptr_t): Decompiled function object.

    Returns:
        List of tuples: (lineIndex, lineText, comment_address, comment_placement, has_user_comment)
    """
    result = []

    pseudocode_lines = cfunc.get_pseudocode()
    
    place_comments_above = (gepetto.config.get_config("Gepetto", "COMMENT_POSITION", default="above") == "above")

    for idx, line in enumerate(pseudocode_lines):
        # Clean line text from formatting tags
        try:
            line_text = idaapi.tag_remove(line.line)
        except Exception:
            line_text = str(line.line)

        # Lookup ctree item
        phead = idaapi.ctree_item_t()
        pitem = idaapi.ctree_item_t()
        ptail = idaapi.ctree_item_t()

        phead_addr = None
        phead_place = None
        ptail_addr = None
        ptail_place = None
        
        has_user_comment = False
        comment_address = None
        comment_placement = 0

        found = cfunc.get_line_item(line.line, 0, True, phead, pitem, ptail)
        if found:
            # Invert preferred locations order
            if not place_comments_above:
                tmp = phead
                phead = ptail
                ptail = tmp
                
            # Assign locations if available and valid
            if hasattr(phead, "loc") and phead.loc and phead.loc.ea != idaapi.BADADDR:
                has_user_comment |= (cfunc.get_user_cmt(phead.loc, True) is not None)
                phead_addr = phead.loc.ea
                phead_place = phead.loc.itp
            if hasattr(ptail, "loc") and ptail.loc and ptail.loc.ea != idaapi.BADADDR:
                has_user_comment |= (cfunc.get_user_cmt(ptail.loc, True) is not None)
                ptail_addr = ptail.loc.ea
                ptail_place = ptail.loc.itp

            # Pick final address and placement (prefer phead if present)
            if phead_addr is not None:
                comment_address = phead_addr
                comment_placement = phead_place
            elif ptail_addr is not None:
                comment_address = ptail_addr
                comment_placement = ptail_place

        result.append((idx, idaapi.tag_remove(line_text), comment_address, comment_placement, has_user_comment))

    return result

# -----------------------------------------------------------------------------

def format_commentable_lines(commentable_lines):
    """
    Formats the output of get_commentable_lines() for display.

    For each line:
      - Adds a "+" before the index if a comment address exists and the line does not already have a user comment.
      - Formats as: [+]index<TAB>text

    Args:
        commentable_lines: List of tuples (index, text, comment_address, comment_placement, has_user_comment)

    Returns:
        str: The formatted text as a single string, with one line per entry.
    """
    output = []
    for idx, text, comment_address, comment_placement, has_user_comment in commentable_lines:
        
        # Add "+" if the line can be commented and has no user comment yet
        prefix = "+" if comment_address is not None and not has_user_comment else ""
        
        output.append(f"{prefix}{idx}\t{text}")
    return "\n".join(output)

# -----------------------------------------------------------------------------

```

`gepetto/ida/handlers.py`:

```py
import functools
import re
import time
import textwrap

import idaapi  # type: ignore
import ida_hexrays  # type: ignore
import idc  # type: ignore

import gepetto.config
from gepetto.ida.utils.thread_helpers import *
from gepetto.models.model_manager import instantiate_model
from gepetto.ida.status_panel.panel_interface import LogCategory, LogLevel
from gepetto.ida.status_panel.status_panel_factory import get_status_panel

_ = gepetto.config._

STATUS_PANEL = get_status_panel()

def comment_callback(address, view, response, start_time):
    """Callback that sets a comment at the given address.

    :param address: The address of the function to comment
    :param view: A handle to the decompiler window
    :param response: The comment to add or a message object returned by the
        model.  When using tool-calling APIs the model might return a
        ``ChatCompletionMessage`` object instead of a simple string.  In order
    to remain backwards compatible this function accepts either and extracts
        the textual content when necessary.
    """

    response_text = response.content if hasattr(response, "content") else response
    if not isinstance(response_text, str):
        response_text = str(response_text)
    response_text = "\n".join(textwrap.wrap(response_text, 80, replace_whitespace=False))

    def _apply_comment():
        existing = idc.get_func_cmt(address, 0) or ""
        existing = re.sub(
            r'----- ' + _("Comment generated by Gepetto") + ' -----.*?----------------------------------------',
            r"",
            existing,
            flags=re.DOTALL,
        )
        new_comment = (
            '----- ' + _("Comment generated by Gepetto") + ' -----\n\n'
            f"{response_text.strip()}\n\n"
            "----------------------------------------\n\n"
            f"{existing.strip()}"
        )
        idc.set_func_cmt(address, new_comment, 0)
        if view:
            view.refresh_view(False)

    try:
        run_on_main_thread(_apply_comment, write=True)
    except Exception as exc:
        error_message = _("[ERROR] comment callback failed: {error}").format(error=str(exc))
        print(error_message)
        STATUS_PANEL.mark_error(error_message)
        return

    elapsed_time = time.time() - start_time
    response_finished = STATUS_PANEL.log_request_finished(elapsed_time)
    print(response_finished)

# -----------------------------------------------------------------------------

def conversation_callback(response, memory):
    """Callback that prints the model's response in IDA's output window.

    The ``response`` argument can either be a plain string or a message object
    returned by :mod:`openai`.  Only the textual content is relevant here, so
    extract it if needed.
    """

    text = response.content if hasattr(response, "content") else response
    if not isinstance(text, str):
        text = str(text)
    memory.append({"role": "assistant", "content": text})

    print()
    for line in text.split("\n"):
        if not line.strip():
            continue
        print(f"{str(gepetto.config.model)}> {line}")
    print()

# -----------------------------------------------------------------------------

class ExplainHandler(idaapi.action_handler_t):
    """
    This handler is tasked with querying the model for an explanation of the
    given function. Once the reply is received, it is added as a function
    comment.
    """

    def __init__(self):
        idaapi.action_handler_t.__init__(self)

    def activate(self, ctx):
        start_time = time.time()
        decompiler_output = ida_hexrays.decompile(idaapi.get_screen_ea())
        v = ida_hexrays.get_widget_vdui(ctx.widget)
        locale = gepetto.config.get_localization_locale()
        gepetto.config.model.query_model_async(
            f"""
                You are a reverse-engineering assistant. Output plain text only (no Markdown, no code fences).
                - Locale: {locale}
                - Task: Summarize what the C function does and propose a clearer function name if one stands out.
                - Observations: Use any existing Gepetto-generated comments as hints but do not repeat them verbatim.
                - Response structure:
                    1. Brief explanation (2-4 sentences) covering purpose, key behaviours, and notable side effects.
                    2. Final line: "Proposed name: <name>" (use "(no change)" if you cannot recommend an improvement).

                ```C
                {decompiler_output}
                ```
              """,
            functools.partial(comment_callback, address=idaapi.get_screen_ea(), view=v, start_time=start_time))
        request_sent = STATUS_PANEL.log_request_started()
        print(request_sent)
        return 1

    # This action is always available.
    def update(self, ctx):
        return idaapi.AST_ENABLE_ALWAYS


# -----------------------------------------------------------------------------

def rename_callback(address, view, response, start_time):
    """
    Callback that extracts a JSON object of old names and new names from the response,
    displays a table UI where the user can select which identifiers to rename,
    and applies renaming only to selected items.
    :param address: The address of the function to work on
    :param view: A handle to the decompiler window
    :param response: The response from the model
    """
    import ida_kernwin
    import json
    import re

    elapsed_time = time.time() - start_time
    timer_finished = STATUS_PANEL.log_request_finished(elapsed_time)
    print(timer_finished)

    response_text = response.content if hasattr(response, "content") else response
    if not isinstance(response_text, str):
        response_text = str(response_text)
    try:
        names = json.loads(response_text)
    except Exception as exc:
        error_message = _("[ERROR] rename callback JSON failure: {error}").format(error=str(exc))
        print(error_message)
        STATUS_PANEL.mark_error(error_message)
        return

    names = dict(names)  # Work on a copy to avoid accidental mutation leakage.

    def _apply():
        function = idaapi.get_func(address)
        if function is None:
            raise RuntimeError(_("Function at EA 0x{ea:X} not found.").format(ea=address))
        function_addr = function.start_ea
        func_name = idc.get_func_name(function_addr) or ""
        func_new_name = names.pop("__function__", None)

        rename_pairs = []
        if func_new_name and func_name.startswith("sub_"):
            rename_pairs.append((func_name, func_new_name))
        rename_pairs.extend((key, value) for key, value in names.items())
        rename_mapping = {old: new for old, new in rename_pairs}

        if not rename_pairs:
            return {"count": 0, "cancelled": False}

        class RenameChoose(ida_kernwin.Choose):
            def __init__(self, pairs):
                super().__init__(
                    _("Select names to rename"),
                    [[_("Old Name"), 20], [_("New Name"), 20]],
                    flags=ida_kernwin.Choose.CH_MULTI,
                )
                self.items = [[old, new] for old, new in pairs]
                self.selected_indices = []

            def OnGetLine(self, index):
                return self.items[index]

            def OnGetSize(self):
                return len(self.items)

            def OnSelectionChange(self, sel_list):
                self.selected_indices = sel_list

        chooser = RenameChoose(rename_pairs)
        if chooser.Show(modal=True) < 0:
            return {"count": 0, "cancelled": True}

        chosen_pairs = [rename_pairs[i] for i in chooser.selected_indices]
        replaced: list[str] = []
        for old, new in chosen_pairs:
            if old == func_name:
                if idc.set_name(function_addr, new, idaapi.SN_FORCE):
                    replaced.append(old)
            else:
                if idaapi.IDA_SDK_VERSION < 760 and view and hasattr(view, "cfunc"):
                    lvars = {lvar.name: lvar for lvar in view.cfunc.lvars}
                    if old in lvars and view.rename_lvar(lvars[old], new, True):
                        replaced.append(old)
                else:
                    if ida_hexrays.rename_lvar(function_addr, old, new):
                        replaced.append(old)

        comment_text = idc.get_func_cmt(address, 0) or ""
        if comment_text and replaced:
            for old in replaced:
                if old in rename_mapping:
                    comment_text = re.sub(r"\b%s\b" % re.escape(old), rename_mapping[old], comment_text)
            idc.set_func_cmt(address, comment_text, 0)

        if view:
            view.refresh_view(True)

        return {"count": len(replaced), "cancelled": False}

    try:
        outcome = run_on_main_thread(_apply, write=True) or {"count": 0, "cancelled": False}
    except Exception as exc:
        error_message = _("[ERROR] rename callback failed: {error}").format(error=str(exc))
        print(error_message)
        STATUS_PANEL.mark_error(error_message)
        return

    if outcome.get("cancelled"):
        STATUS_PANEL.log(_("Rename cancelled by user."), category=LogCategory.TOOL)
        print(_("Rename cancelled by user."))
        return

    count = int(outcome.get("count", 0) or 0)
    response_finished = _("Done! {count} name(s) renamed.").format(count=count)
    print(response_finished)
    STATUS_PANEL.log(response_finished, category=LogCategory.TOOL, level=LogLevel.SUCCESS)


# -----------------------------------------------------------------------------

class RenameHandler(idaapi.action_handler_t):
    """
    This handler requests new names for the function and its variables from the model
    and updates the decompiler's output.
    """

    def __init__(self):
        idaapi.action_handler_t.__init__(self)

    def activate(self, ctx):
        decompiler_output = ida_hexrays.decompile(idaapi.get_screen_ea())
        v = ida_hexrays.get_widget_vdui(ctx.widget)
        start_time = time.time()
        locale = gepetto.config.get_localization_locale()
        gepetto.config.model.query_model_async(
            f"""
                You are a reverse-engineering assistant refining identifiers.
                - Locale: {locale}
                - Task: Suggest better names for the function and its locals when the improvement is meaningful.
                - Output: Return exactly one JSON object (no Markdown, no backticks, no commentary).
                    Keys = original identifiers, values = suggested replacements.
                    Use the special key "__function__" to propose a new function name.
                - Guidance:
                    * Only include entries where the proposed name clearly improves clarity.
                    * Prefer descriptive, conventional names; avoid Hungarian notation and over-abbreviations.
                    * Leverage existing accurate comments (especially Gepetto banners) when inferring intent.
                - If nothing needs renaming, respond with {{}}.

                ```C
                {decompiler_output}
                ```
              """,
            functools.partial(rename_callback, address=idaapi.get_screen_ea(), view=v, start_time=start_time),
            additional_model_options={"response_format": {"type": "json_object"}})
        request_sent = STATUS_PANEL.log_request_started()
        print(request_sent)
        return 1

    # This action is always available.
    def update(self, ctx):
        return idaapi.AST_ENABLE_ALWAYS


# -----------------------------------------------------------------------------

class SwapModelHandler(idaapi.action_handler_t):
    """
    This handler replaces the model currently in use with another one selected by the user,
    and updates the configuration.
    """

    def __init__(self, new_model, plugin):
        self.new_model = new_model
        self.plugin = plugin

    def activate(self, ctx):
        try:
            gepetto.config.model = instantiate_model(self.new_model)
        except ValueError as e:  # Raised if an API key is missing. In which case, don't switch.
            error_msg = _("Couldn't change model to {model}: {error}").format(model=self.new_model, error=str(e))
            print(error_msg)
            STATUS_PANEL.log(error_msg, category=LogCategory.SYSTEM, level=LogLevel.ERROR)
            return
        gepetto.config.update_config("Gepetto", "MODEL", self.new_model)
        STATUS_PANEL.set_model(str(gepetto.config.model))
        # Refresh the menus to reflect which model is currently selected.
        self.plugin.generate_model_select_menu()

    def update(self, ctx):
        return idaapi.AST_ENABLE_ALWAYS


class GenerateCCodeHandler(idaapi.action_handler_t):
    """
    This handler requests the model to generate executable C code from the given
    decompiled C function. The generated code is saved to a file.
    """
    def __init__(self):
        idaapi.action_handler_t.__init__(self)

    def activate(self, ctx):
        decompiler_output = ida_hexrays.decompile(idaapi.get_screen_ea())
        v = ida_hexrays.get_widget_vdui(ctx.widget)
        if not decompiler_output:
            return 0

        start_time = time.time()
        gepetto.config.model.query_model_async(
            f"Please generate executable C code based on the following decompiled C code and ensure it includes all necessary header files and other information:\n"
            f"{decompiler_output}",
            functools.partial(self._save_c_code, view=v, start_time=start_time)
        )
        request_sent = STATUS_PANEL.log_request_started()
        print(request_sent)
        return 1

    def _save_c_code(self, view, response, start_time):
        """
        Callback that saves the generated C code to a file.
        :param view: A handle to the decompiler window
        :param response: The generated C code from the model
        :param start_time: When the request was initiated
        """

        elapsed_time = time.time() - start_time
        timer_finished = STATUS_PANEL.log_request_finished(elapsed_time)
        print(timer_finished)
        
        code_text = response.content if hasattr(response, "content") else response
        project_name = idaapi.get_root_filename()
        func_name = idc.get_func_name(idaapi.get_screen_ea())
        file_name = f"{project_name}_{func_name}.c"
        with open(file_name, "w", encoding="utf-8") as f:
            f.write(str(code_text))

        if view:
            run_on_main_thread(lambda: view.refresh_view(False))
        response_finished = _("{model} generated code saved to {file_name}").format(
            model=str(gepetto.config.model), file_name=file_name)

        STATUS_PANEL.log(response_finished, category=LogCategory.TOOL, level=LogLevel.SUCCESS)
        print(response_finished)

    def update(self, ctx):
        return idaapi.AST_ENABLE_ALWAYS


class GeneratePythonCodeHandler(idaapi.action_handler_t):
    """
    This handler requests the model to generate executable C code from the given
    decompiled C function. The generated code is saved to a file.
    """
    def __init__(self):
        idaapi.action_handler_t.__init__(self)

    def activate(self, ctx):
        decompiler_output = ida_hexrays.decompile(idaapi.get_screen_ea())
        v = ida_hexrays.get_widget_vdui(ctx.widget)
        if not decompiler_output:
            return 0

        start_time = time.time()
        gepetto.config.model.query_model_async(
            f"Please generate equivalent Python code based on the following decompiled C code, and provide an example of the function call:"
            f"{decompiler_output}",
            functools.partial(self._save_python_code, view=v, start_time=start_time)
        )
        request_sent = STATUS_PANEL.log_request_started()
        print(request_sent)
        return 1

    def _save_python_code(self, view, response, start_time):
        """
        Callback that saves the generated python code to a file.
        :param view: A handle to the decompiler window
        :param response: The generated python code from the model
        :param start_time: When the request was initiated
        """

        elapsed_time = time.time() - start_time
        timer_finished = STATUS_PANEL.log_request_finished(elapsed_time)
        print(timer_finished)
        
        code_text = response.content if hasattr(response, "content") else response
        if not isinstance(code_text, str):
            code_text = str(code_text)
        project_name = idaapi.get_root_filename()
        func_name = idc.get_func_name(idaapi.get_screen_ea())
        file_name = f"{project_name}_{func_name}.py"
        with open(file_name, "w", encoding="utf-8") as f:
            f.write(code_text)

        if view:
            run_on_main_thread(lambda: view.refresh_view(False))
        response_finished = _("{model} generated code saved to {file_name}").format(
            model=str(gepetto.config.model), file_name=file_name)

        STATUS_PANEL.log(response_finished, category=LogCategory.TOOL, level=LogLevel.SUCCESS)
        print(response_finished)

    def update(self, ctx):
        return idaapi.AST_ENABLE_ALWAYS

```

`gepetto/ida/status_panel/no_panel.py`:

```py
from typing import Callable, Optional
from .panel_interface import StatusPanel, LogCategory, LogLevel


class NoStatusPanel(StatusPanel):
    def __init__(self) -> None:
        self._stop_callback: Optional[Callable[[], None]] = None

    def ensure_shown(self) -> None:
        pass

    def form_closed(self) -> None:
        pass

    def on_form_ready(self) -> None:
        pass

    def set_model(self, model_name: str) -> None:
        pass

    def set_status(self, text: str, *, busy: bool = False, error: bool = False) -> None:
        pass

    def set_stop_callback(self, callback: Optional[Callable[[], None]]) -> None:
        self._stop_callback = callback

    def reset_stop(self) -> None:
        pass

    def has_stop_callback(self) -> bool:
        return self._stop_callback is not None

    def request_stop(self) -> None:
        pass

    def start_stream(self) -> None:
        pass

    def append_stream(self, chunk: str) -> None:
        pass

    def finish_stream(self, final_text: str) -> None:
        pass

    def append_reasoning(self, chunk: str) -> None:
        pass

    def finish_reasoning(self) -> None:
        pass

    def log(self, message: str, *, category: LogCategory = LogCategory.SYSTEM, level: LogLevel = LogLevel.INFO) -> None:
        pass

    def log_user(self, text: str) -> None:
        pass

    def log_request_started(self) -> str:
        pass

    def log_request_finished(self, elapsed_seconds: float) -> str:
        pass

    def mark_error(self, message: str) -> None:
        pass

    def clear_log(self) -> None:
        pass

    def close(self) -> None:
        pass

```

`gepetto/ida/status_panel/panel_interface.py`:

```py
from typing import Protocol, Callable, Optional
from enum import Enum


class LogLevel(Enum):
    INFO = "info"
    SUCCESS = "success"
    WARNING = "warning"
    ERROR = "error"
    DEBUG = "debug"


class LogCategory(Enum):
    SYSTEM = "system"
    USER = "user"
    TOOL = "tool"
    MODEL = "model"
    ASSISTANT = "assistant"

    def display_name(self) -> str:
        # You can keep your i18n here if you want
        from gepetto import config
        _ = config._
        labels = {
            LogCategory.SYSTEM: _("System"),
            LogCategory.USER: _("User"),
            LogCategory.TOOL: _("Tool"),
            LogCategory.MODEL: _("Model"),
            LogCategory.ASSISTANT: _("Assistant"),
        }
        return labels[self]


class StatusPanel(Protocol):
    def ensure_shown(self) -> None: ...
    def form_closed(self) -> None: ...
    def on_form_ready(self) -> None: ...

    def set_model(self, model_name: str) -> None: ...
    def set_status(self, text: str, *, busy: bool = False, error: bool = False) -> None: ...

    def set_stop_callback(self, callback: Optional[Callable[[], None]]) -> None: ...
    def reset_stop(self) -> None: ...
    def has_stop_callback(self) -> bool: ...
    def request_stop(self) -> None: ...

    def start_stream(self) -> None: ...
    def append_stream(self, chunk: str) -> None: ...
    def finish_stream(self, final_text: str) -> None: ...
    def append_reasoning(self, chunk: str) -> None: ...
    def finish_reasoning(self) -> None: ...

    def log_user(self, text: str) -> None: ...
    def log(
            self,
            message: str,
            *,
            category: LogCategory = LogCategory.SYSTEM,
            level: LogLevel = LogLevel.INFO,
    ) -> None: ...
    def log_request_started(self) -> str: ...
    def log_request_finished(self, elapsed_seconds: float) -> str: ...

    def mark_error(self, message: str) -> None: ...
    def clear_log(self) -> None: ...
    def close(self) -> None: ...

```

`gepetto/ida/status_panel/qt_compat.py`:

```py
from typing import Any

# Prefer native PySide6 (IDA 9.2+), fall back to PyQt5 (≤ 9.1 or shims)
try:
    from PySide6 import QtCore, QtGui, QtWidgets  # type: ignore
    QT_BINDING = "PySide6"
except Exception:  # pragma: no cover
    from PyQt5 import QtCore, QtGui, QtWidgets  # type: ignore
    QT_BINDING = "PyQt5"


def exec_menu(menu: QtWidgets.QMenu, *args: Any, **kwargs: Any) -> Any:
    """
    Call QMenu.exec/exec_ in a way that works on both PyQt5 & PySide6.
    """
    if hasattr(menu, "exec_"):
        # PyQt5 (and PyQt5 shims)
        return menu.exec_(*args, **kwargs)
    # PySide6
    return menu.exec(*args, **kwargs)

```

`gepetto/ida/status_panel/qt_panel.py`:

```py
"""
    Qt status panel for Gepetto streaming UX.
"""

import datetime
import html
from dataclasses import dataclass
from collections.abc import Callable

import ida_kernwin
from gepetto.ida.status_panel.qt_compat import QtCore, QtGui, QtWidgets, exec_menu

from gepetto.ida.utils.hooks import run_when_desktop_ready
from gepetto.ida.status_panel.panel_interface import StatusPanel, LogCategory, LogLevel
import gepetto.models.model_manager
import gepetto.config

_ = gepetto.config._

STATUS_PANEL_CAPTION = _("Gepetto")
_DEFAULT_DOCK_OPTIONS = (
        getattr(ida_kernwin.PluginForm, "WOPN_RESTORE", 0x04)
        | getattr(ida_kernwin.PluginForm, "WOPN_MENU", 0x10)
        | getattr(ida_kernwin.PluginForm, "WOPN_PERSIST", 0x40)
        | getattr(ida_kernwin.PluginForm, "WOPN_NOT_CLOSED_BY_ESC", 0x100)
        | getattr(ida_kernwin.PluginForm, "WOPN_DP_SZHINT", 0x200)
)


@dataclass
class LogEntry:
    timestamp: datetime.datetime
    level: LogLevel
    category: LogCategory
    message: str
    trailing_breaks: int = 0


def _html_escape(text: str) -> str:
    return html.escape(text).replace("\n", "<br>")


def _relative_luminance(color: QtGui.QColor) -> float:
    if not color.isValid():
        return 0.0

    def _channel(value: int) -> float:
        norm = value / 255.0
        if norm <= 0.03928:
            return norm / 12.92
        return ((norm + 0.055) / 1.055) ** 2.4

    r = _channel(color.red())
    g = _channel(color.green())
    b = _channel(color.blue())
    return 0.2126 * r + 0.7152 * g + 0.0722 * b


def _contrast_ratio(foreground: QtGui.QColor, background: QtGui.QColor) -> float:
    if not (foreground.isValid() and background.isValid()):
        return 1.0
    l1 = _relative_luminance(foreground)
    l2 = _relative_luminance(background)
    lighter = max(l1, l2)
    darker = min(l1, l2)
    return (lighter + 0.05) / (darker + 0.05)


def _best_text_color(background: QtGui.QColor, palette: QtGui.QPalette) -> QtGui.QColor:
    candidates = [
        palette.color(QtGui.QPalette.Shadow),
        palette.color(QtGui.QPalette.WindowText),
        palette.color(QtGui.QPalette.Text),
        palette.color(QtGui.QPalette.ButtonText),
        palette.color(QtGui.QPalette.BrightText),
        QtGui.QColor("black"),
        QtGui.QColor("white"),
    ]
    return max(candidates, key=lambda c: _contrast_ratio(c, background))


def _rgba(color: QtGui.QColor, alpha: float | None = None) -> str:
    if not color.isValid():
        color = QtGui.QColor(0, 0, 0, 0)
    result = QtGui.QColor(color)
    if alpha is not None:
        result.setAlphaF(max(0.0, min(alpha, 1.0)))
    return f"rgba({result.red()}, {result.green()}, {result.blue()}, {result.alphaF():.3f})"


class GepettoStatusForm(ida_kernwin.PluginForm):
    """Dockable widget that displays the streaming answer and structured log."""

    _CATEGORY_COLORS = {
        LogCategory.SYSTEM: "#bac2de",
        LogCategory.USER: "#89b4fa",
        LogCategory.TOOL: "#f9e2af",
        LogCategory.MODEL: "#f5c2e7",
        LogCategory.ASSISTANT: "#94e2d5",
        # LogCategory.REASONING: "#94e2d5",  # /TODO
    }
    _LEVEL_COLORS = {
        LogLevel.INFO: "",
        LogLevel.SUCCESS: "#a6e3a1",
        LogLevel.WARNING: "#f9e2af",
        LogLevel.ERROR: "#f38ba8",
        LogLevel.DEBUG: "#94e2d5",
    }

    def __init__(self, owner: StatusPanel) -> None:  # type: ignore[name-defined]
        super().__init__()
        self._owner = owner
        self._category_color_map = {
            category: QtGui.QColor(color)
            for category, color in self._CATEGORY_COLORS.items()
            if color
        }
        self._level_color_map = {
            level: QtGui.QColor(color)
            for level, color in self._LEVEL_COLORS.items()
            if color
        }
        self._widget: QtWidgets.QWidget | None = None
        self._twidget = None
        self._model_button: QtWidgets.QPushButton | None = None
        self._status_label: QtWidgets.QLabel | None = None
        self._stop_button: QtWidgets.QPushButton | None = None
        self._clear_button: QtWidgets.QPushButton | None = None
        self._progress_bar: QtWidgets.QProgressBar | None = None
        self._conversation_view: QtWidgets.QTextBrowser | None = None
        self._log_view: QtWidgets.QTextBrowser | None = None
        self._splitter: QtWidgets.QSplitter | None = None
        self._chat_input: QtWidgets.QLineEdit | None = None
        self._send_button: QtWidgets.QPushButton | None = None
        self._filter_buttons: dict[LogCategory, QtWidgets.QToolButton] = {}
        self._active_filters: set[LogCategory] = set(LogCategory)
        self._log_entries: list[LogEntry] = []
        self._conversation_segments: list[str] = []
        self._stream_text: list[str] = []
        self._stream_index: int | None = None
        self._stream_active = False
        self._stream_header: str | None = None
        self._reasoning_buffer: list[str] = []
        self._reasoning_log_index: int | None = None
        self._model_checked_icon: QtGui.QIcon | None = None
        self._ready = False

    # ------------------------------------------------------------------
    def OnCreate(self, form):  # noqa: N802 - IDA callback signature
        self._twidget = self.GetWidget()
        self._widget = self.FormToPyQtWidget(form)  # type: ignore[attr-defined]
        self._build_ui()
        self._ready = True
        self._owner.on_form_ready()

    # ------------------------------------------------------------------
    def OnClose(self, form):  # noqa: N802 - IDA callback signature
        del form
        self._twidget = None
        self._widget = self._model_button = self._status_label = None
        self._stop_button = self._clear_button = self._progress_bar = None
        self._conversation_view = self._log_view = None
        self._splitter = None
        self._chat_input = self._send_button = None
        self._filter_buttons = {}
        self._active_filters = set(LogCategory)
        self._log_entries.clear()
        self._conversation_segments.clear()
        self._stream_text.clear()
        self._reasoning_buffer.clear()
        self._stream_index = self._stream_header = None
        self._stream_active = False
        self._reasoning_log_index = None
        self._ready = False
        self._owner.form_closed()

    # ------------------------------------------------------------------
    def is_ready(self) -> bool:
        return bool(self._widget and self._ready)

    # ------------------------------------------------------------------
    def widget(self):
        return self._widget

    # ------------------------------------------------------------------
    def twidget(self):
        return self._twidget

    # ------------------------------------------------------------------
    def _build_ui(self) -> None:
        if self._widget is None:
            return

        root_layout = QtWidgets.QVBoxLayout(self._widget)
        root_layout.setContentsMargins(6, 6, 6, 6)
        root_layout.setSpacing(0)

        self._splitter = QtWidgets.QSplitter(QtCore.Qt.Orientation.Vertical)
        self._splitter.setChildrenCollapsible(False)
        self._splitter.setHandleWidth(4)

        self._conversation_view = QtWidgets.QTextBrowser()
        self._conversation_view.setReadOnly(True)
        self._conversation_view.setMinimumHeight(60)
        self._conversation_view.setOpenExternalLinks(False)
        self._splitter.addWidget(self._conversation_view)
        self._splitter.setStretchFactor(0, 7)

        log_container = QtWidgets.QWidget()
        log_container_layout = QtWidgets.QVBoxLayout(log_container)
        log_container_layout.setContentsMargins(0, 0, 0, 0)
        log_container_layout.setSpacing(2)

        log_header_layout = QtWidgets.QHBoxLayout()
        log_header_layout.setSpacing(6)
        log_header_layout.setContentsMargins(0, 6, 0, 0)

        filters_layout = QtWidgets.QHBoxLayout()
        filters_layout.setSpacing(4)
        filters_label = QtWidgets.QLabel(_("Log filters:"))
        filters_layout.addWidget(filters_label)
        for category in LogCategory:
            button = QtWidgets.QToolButton()
            button.setText(category.display_name())
            button.setCheckable(True)
            button.setChecked(True)
            button.clicked.connect(self._make_filter_callback(category))  # type: ignore[arg-type]
            filters_layout.addWidget(button)
            if category is LogCategory.ASSISTANT:
                button.click()
            self._filter_buttons[category] = button
        self._apply_filter_styles()
        filters_layout.addStretch(1)
        log_header_layout.addLayout(filters_layout, stretch=1)

        actions_layout = QtWidgets.QHBoxLayout()
        actions_layout.setSpacing(6)
        self._clear_button = QtWidgets.QPushButton(_("Clear"))
        self._clear_button.clicked.connect(self._owner.clear_log)  # type: ignore[arg-type]
        actions_layout.addWidget(self._clear_button)
        log_header_layout.addLayout(actions_layout)
        log_container_layout.addLayout(log_header_layout)

        self._log_view = QtWidgets.QTextBrowser()
        self._log_view.setReadOnly(True)
        self._log_view.setMinimumHeight(40)
        log_container_layout.addWidget(self._log_view, stretch=1)

        self._splitter.addWidget(log_container)
        self._splitter.setStretchFactor(1, 3)

        root_layout.addWidget(self._splitter, stretch=1)

        chat_row = QtWidgets.QHBoxLayout()
        chat_row.setSpacing(4)
        self._chat_input = QtWidgets.QLineEdit()
        self._chat_input.setPlaceholderText(_("Type a prompt and press Enter…"))
        self._chat_input.returnPressed.connect(self._handle_chat_submit)  # type: ignore[arg-type]
        chat_row.addWidget(self._chat_input, stretch=1)
        self._send_button = QtWidgets.QPushButton(_("Send"))
        self._send_button.clicked.connect(self._handle_chat_submit)  # type: ignore[arg-type]
        chat_row.addWidget(self._send_button)
        self._stop_button = QtWidgets.QPushButton(_("Stop"))
        self._stop_button.setEnabled(False)
        self._stop_button.clicked.connect(self._handle_stop_clicked)  # type: ignore[arg-type]
        chat_row.addWidget(self._stop_button)

        self._progress_bar = QtWidgets.QProgressBar()
        self._progress_bar.setRange(0, 1)
        self._progress_bar.setValue(0)
        self._progress_bar.setFixedHeight(6)
        self._progress_bar.setSizePolicy(
            QtWidgets.QSizePolicy.Expanding,
            QtWidgets.QSizePolicy.Fixed,
        )
        self._progress_bar.setTextVisible(False)
        self._apply_progress_bar_style()

        chat_container = QtWidgets.QVBoxLayout()
        chat_container.setSpacing(4)
        chat_container.setContentsMargins(0, 0, 0, 0)
        chat_container.addWidget(self._progress_bar)
        chat_container.addLayout(chat_row)
        root_layout.addLayout(chat_container)

        footer = QtWidgets.QHBoxLayout()
        footer.setSpacing(0)
        footer.setContentsMargins(2, 8, 2, 2)
        self._apply_model_button_style()
        self._model_button = QtWidgets.QPushButton(
            _("Model: {model} ▼").format(model=str(gepetto.config.model))
        )
        self._model_button.setObjectName("gepetto_status_model_button")
        self._model_button.setCursor(QtCore.Qt.PointingHandCursor)
        self._model_button.clicked.connect(self._handle_model_button_clicked)  # type: ignore[arg-type]
        footer.addWidget(self._model_button)
        footer.addStretch(1)
        self._status_label = QtWidgets.QLabel(_("Status: {status}").format(status=_("Idle")))
        self._status_label.setObjectName("gepetto_status_label")
        footer.addWidget(self._status_label)
        root_layout.addLayout(footer)

        self._refresh_conversation(scroll=False)
        self._refresh_log_widget(scroll=False)
        self._set_chat_controls_enabled(True)

    # ------------------------------------------------------------------
    def _make_filter_callback(self, category: LogCategory) -> Callable[[bool], None]:
        def toggle(checked: bool) -> None:
            if checked:
                self._active_filters.add(category)
            else:
                self._active_filters.discard(category)
            self._refresh_log_widget(scroll=False)

        return toggle

    # ------------------------------------------------------------------
    def _apply_progress_bar_style(self) -> None:
        if not (self._progress_bar and self._widget):
            return

        palette = self._widget.palette()
        chunk_color = palette.color(QtGui.QPalette.WindowText)
        track_color = QtGui.QColor("#00000000")
        text_color = _best_text_color(chunk_color, palette)
        style = (
            "QProgressBar {"
            f"  background-color: {_rgba(track_color)};"
            "  border: none;"
            "  padding: 0px;"
            "}"
            "QProgressBar::chunk {"
            f"  background-color: {_rgba(chunk_color, 0.33)};"
            f"  color: {_rgba(text_color)};"
            f"  border-color: {_rgba(chunk_color.lighter(110))};"
            "  border-radius: 1px;"
            "  margin: 1px;"
            "}"
        )
        self._progress_bar.setStyleSheet(style)

    # ------------------------------------------------------------------
    def _apply_model_button_style(self) -> None:
        if not self._widget:
            return

        palette = self._widget.palette()
        text_color = palette.color(QtGui.QPalette.WindowText).lighter(120)
        border_color = text_color.darker(130)
        hover_color = text_color.lighter(150)

        style = (
            "QPushButton#gepetto_status_model_button {"
            "  padding: 2px 6px;"
            "  border: 1px solid transparent;"
            "  border-radius: 3px;"
            "  text-align: left;"
            f"  color: {text_color.name()};"
            "  background: transparent;"
            "}"
            "QPushButton#gepetto_status_model_button:hover {"
            f"  border-color: {border_color.name()};"
            f"  background-color: {_rgba(hover_color, 0.1)};"
            "}"
            "QPushButton#gepetto_status_model_button:pressed {"
            f"  background-color: {_rgba(hover_color, 0.2)};"
            "}"
        )
        if self._model_button:
            self._model_button.setStyleSheet(style)

    # ------------------------------------------------------------------
    def _ensure_model_checked_icon(self) -> QtGui.QIcon | None:
        if self._model_checked_icon is not None:
            return self._model_checked_icon

        app = QtWidgets.QApplication.instance()
        if not app:
            return None

        icon = app.style().standardIcon(QtWidgets.QStyle.StandardPixmap.SP_DialogApplyButton)
        if icon and not icon.isNull():
            self._model_checked_icon = icon
        return self._model_checked_icon

    # ------------------------------------------------------------------
    def _apply_filter_styles(self) -> None:
        if not self._filter_buttons:
            return
        palette = self._widget.palette() if self._widget else QtWidgets.QApplication.palette()
        muted_text_color = palette.color(QtGui.QPalette.WindowText).lighter(120)

        for category, button in self._filter_buttons.items():
            base_color = QtGui.QColor(self._CATEGORY_COLORS.get(category, ""))
            if not button or not base_color.isValid():
                if button:
                    button.setStyleSheet("")
                continue

            text_color = _best_text_color(base_color, palette)
            border_color = QtGui.QColor(base_color).darker(180)
            if _relative_luminance(base_color) > _relative_luminance(
                    palette.color(QtGui.QPalette.Window)
            ):
                hover_color = base_color.darker(110)
                muted_color = base_color.darker(120)
            else:
                hover_color = base_color.lighter(120)
                muted_color = base_color.lighter(130)

            style = (
                "QToolButton {"
                "  padding: 2px 10px;"
                "  border-radius: 4px;"
                f"  border: 1px solid {_rgba(base_color)};"
                "}"
                "QToolButton:checked {"
                f"  color: {_rgba(text_color, 0.75)};"
                f"  background-color: {_rgba(base_color)};"
                f"  border-color: {_rgba(border_color)};"
                "  font-weight: bold;"
                "}"
                "QToolButton:hover {"
                f"  background-color: {_rgba(hover_color, 0.9)};"
                f"  border-color: {_rgba(border_color, 0.9)};"
                "}"
                "QToolButton:!checked {"
                f"  color: {_rgba(muted_text_color, 0.8)};"
                f"  background-color: {_rgba(muted_color, 0.33)};"
                f"  border-color: {_rgba(border_color, 0.55)};"
                "}"
                "QToolButton:!checked:hover {"
                f"  border-color: {_rgba(border_color, 0.75)};"
                "}"
            )
            button.setStyleSheet(style)

    # ------------------------------------------------------------------
    def _handle_stop_clicked(self) -> None:
        self._owner.request_stop()

    # ------------------------------------------------------------------
    @staticmethod
    def _set_text_browser_content(browser: QtWidgets.QTextBrowser, content: str) -> None:
        try:
            browser.setMarkdown(content)
        except Exception:
            browser.setPlainText(content)

    # ------------------------------------------------------------------
    def _set_chat_controls_enabled(self, enabled: bool) -> None:
        if self._chat_input:
            self._chat_input.setEnabled(enabled)
        if self._send_button:
            self._send_button.setEnabled(enabled)

    # ------------------------------------------------------------------
    def set_chat_enabled(self, enabled: bool) -> None:
        self._set_chat_controls_enabled(enabled)

    # ------------------------------------------------------------------
    def _handle_chat_submit(self) -> None:
        if not self._chat_input:
            return
        text = self._chat_input.text().strip()
        if not text:
            return
        self._chat_input.clear()
        self._set_chat_controls_enabled(False)
        try:
            self._owner.submit_chat(text)
        except Exception as exc:  # pragma: no cover
            self.append_log(
                _("Failed to send prompt: {err}").format(err=str(exc)),
                category=LogCategory.SYSTEM,
                level=LogLevel.ERROR,
            )
            self._set_chat_controls_enabled(True)

    # ------------------------------------------------------------------
    def _refresh_conversation(self, *, scroll: bool) -> None:
        if not self._conversation_view:
            return
        content = "\n\n".join(self._conversation_segments)
        self._set_text_browser_content(self._conversation_view, content)
        if scroll:
            cursor = self._conversation_view.textCursor()
            cursor.movePosition(QtGui.QTextCursor.End)
            self._conversation_view.setTextCursor(cursor)
            self._conversation_view.ensureCursorVisible()

    # ------------------------------------------------------------------
    def _refresh_log_widget(self, *, scroll: bool) -> None:
        if not self._log_view:
            return
        visible_entries = [
            entry for entry in self._log_entries if entry.category in self._active_filters
        ]
        palette = self._log_view.palette()
        base_color = palette.color(QtGui.QPalette.Base)
        default_text_color = palette.color(QtGui.QPalette.Text)

        if not visible_entries:
            self._log_view.clear()
        else:
            html_parts: list[str] = []
            for entry in visible_entries:
                level_color = self._LEVEL_COLORS.get(entry.level, "")
                category_color = self._CATEGORY_COLORS.get(entry.category, "#cdd6f4")
                color = level_color or category_color
                timestamp = entry.timestamp.strftime("%H:%M:%S")
                text = _html_escape(entry.message)
                line = f"[{timestamp}] {text}"
                desired = QtGui.QColor(color)
                effective = self._ensure_contrast_color(
                    desired,
                    fallback=default_text_color,
                    background=base_color,
                    palette=palette,
                )
                container_style = f"color: {effective.name()}; margin-bottom: 4px;"
                # Reasoning styling placeholder
                html_parts.append(f"<div style=\"{container_style}\">{line}</div>")
                if entry.trailing_breaks:
                    html_parts.append("<br>" * entry.trailing_breaks)
            self._log_view.setHtml("".join(html_parts))
        if scroll:
            self._log_view.moveCursor(QtGui.QTextCursor.End)
            self._log_view.ensureCursorVisible()

    # ------------------------------------------------------------------
    def _ensure_contrast_color(
            self,
            desired: QtGui.QColor,
            *,
            fallback: QtGui.QColor,
            background: QtGui.QColor,
            palette: QtGui.QPalette,
    ) -> QtGui.QColor:
        if not desired.isValid():
            return fallback

        if _contrast_ratio(desired, background) >= 4.0:
            return desired

        base = QtGui.QColor(desired)
        lighten = _relative_luminance(desired) <= _relative_luminance(background)
        for factor in (120, 140, 160, 180, 200):
            candidate = base.lighter(factor) if lighten else base.darker(factor)
            if _contrast_ratio(candidate, background) >= 4.0:
                return candidate

        best = _best_text_color(background, palette)
        return best if _contrast_ratio(best, background) >= 4.0 else fallback

    # ------------------------------------------------------------------
    def set_model(self, model_name: str) -> None:
        if self._model_button:
            self._model_button.setText(_("Model: {model} ▼").format(model=model_name))

    # ------------------------------------------------------------------
    def _handle_model_button_clicked(self) -> None:
        if not self._model_button:
            return

        menu = QtWidgets.QMenu(self._model_button)

        try:
            current_model_name = str(gepetto.config.model)
            check_icon = self._ensure_model_checked_icon()

            for provider in gepetto.models.model_manager.list_models():
                if provider.supported_models():
                    provider_menu = menu.addMenu(provider.get_menu_name())

                    for model in provider.supported_models():
                        action = provider_menu.addAction(model)

                        selected = model == current_model_name
                        if check_icon:
                            if selected:
                                action.setIcon(check_icon)
                            else:
                                action.setIcon(QtGui.QIcon())
                        else:
                            action.setCheckable(True)
                            action.setChecked(selected)

                        action.triggered.connect(self._make_model_switch_handler(model))  # type: ignore[arg-type]
        except Exception:
            error_action = menu.addAction(_("Error loading models"))
            error_action.setEnabled(False)

        pos = self._model_button.mapToGlobal(QtCore.QPoint(0, 0))
        exec_menu(menu, pos)

    # ------------------------------------------------------------------
    def _switch_model(self, model_name: str) -> None:
        try:
            instantiate_model = gepetto.models.model_manager.instantiate_model

            gepetto.config.model = instantiate_model(model_name)
            gepetto.config.update_config("Gepetto", "MODEL", model_name)

            self.set_model(str(gepetto.config.model))

            self.append_log(
                _("Model switched to {model}").format(model=model_name),
                category=LogCategory.MODEL,
                level=LogLevel.INFO,
            )

            try:
                from gepetto.ida.ui import trigger_model_select_menu_regeneration

                trigger_model_select_menu_regeneration()
            except Exception:
                pass

        except ValueError as e:
            error_msg = _("Couldn't change model to {model}: {error}").format(
                model=model_name,
                error=str(e),
            )
            self.append_log(error_msg, category=LogCategory.SYSTEM, level=LogLevel.ERROR)
        except Exception as exc:
            error_msg = _("Failed to switch model: {error}").format(error=str(exc))
            self.append_log(error_msg, category=LogCategory.SYSTEM, level=LogLevel.ERROR)

    # ------------------------------------------------------------------
    def _make_model_switch_handler(self, model_name: str) -> Callable[[bool], None]:
        def _handler(_checked: bool = False, _model: str = model_name) -> None:
            self._switch_model(_model)

        return _handler

    # ------------------------------------------------------------------
    def set_status(self, text: str, *, busy: bool = False, error: bool = False) -> None:
        if not self._status_label:
            return
        status_text = text or _("Idle")
        self._status_label.setText(_("Status: {status}").format(status=status_text))
        if error:
            self._status_label.setStyleSheet("color: #f38ba8;")
        else:
            self._status_label.setStyleSheet("")
        if self._progress_bar:
            if busy:
                self._progress_bar.setRange(0, 0)
            else:
                self._progress_bar.setRange(0, 1)
                self._progress_bar.setValue(0)
        self._set_chat_controls_enabled(not busy)

    # ------------------------------------------------------------------
    def reset_stop(self) -> None:
        if not self._stop_button:
            return
        self._stop_button.setText(_("Stop"))
        self._stop_button.setEnabled(self._owner.has_stop_callback())

    # ------------------------------------------------------------------
    def set_stop_callback(self, callback: Callable[[], None] | None) -> None:
        if not self._stop_button:
            return
        # Button enabled state is driven by manager.reset_stop()
        # Manager will call reset_stop() after setting callback.
        self._stop_button.setEnabled(callback is not None)

    # ------------------------------------------------------------------
    def mark_error(self, message: str) -> None:
        self.append_log(message, category=LogCategory.SYSTEM, level=LogLevel.ERROR)
        self.set_status(_("Error - Check Log"), error=True)
        if self._stop_button:
            self._stop_button.setEnabled(False)

    # ------------------------------------------------------------------
    def mark_cancelling(self) -> None:
        if self._stop_button:
            self._stop_button.setText(_("Cancelling…"))
            self._stop_button.setEnabled(False)
        self.set_status(_("Cancelling request"), busy=True)

    # ------------------------------------------------------------------
    def clear_log(self) -> None:
        for collection in (
                self._log_entries,
                self._conversation_segments,
                self._stream_text,
                self._reasoning_buffer,
        ):
            collection.clear()
        self._stream_index = self._stream_header = None
        self._stream_active = False
        self._reasoning_log_index = None
        self._refresh_conversation(scroll=False)
        self._refresh_log_widget(scroll=False)

    # ------------------------------------------------------------------
    def append_log(
            self,
            message: str,
            newline: bool = False,
            *,
            category: LogCategory = LogCategory.SYSTEM,
            level: LogLevel = LogLevel.INFO,
    ) -> None:
        entry = LogEntry(
            timestamp=datetime.datetime.now(),
            level=level,
            category=category,
            message=message,
            trailing_breaks=1 if newline else 0,
        )
        self._log_entries.append(entry)
        self._refresh_log_widget(scroll=True)

    # ------------------------------------------------------------------
    def log_user(self, text: str) -> None:
        if not text:
            return
        label = _("You")
        self._conversation_segments.append(f"**[{label}]**: {text}")
        self.append_log(_("User: {text}").format(text=text), category=LogCategory.USER)
        self._refresh_conversation(scroll=True)

    # ------------------------------------------------------------------
    def _log_assistant_event(self, text: str) -> None:
        if text:
            self.append_log(
                _("Assistant: {text}").format(text=text),
                category=LogCategory.ASSISTANT,
            )

    # ------------------------------------------------------------------
    def log_assistant(self, text: str) -> None:
        if not text:
            return
        label = _("Gepetto")
        self._conversation_segments.append(f"**[{label}]**: {text}")
        self._log_assistant_event(text)
        self._refresh_conversation(scroll=True)

    # ------------------------------------------------------------------
    def start_stream(self) -> None:
        model_name = str(gepetto.config.model)
        label = _("Gepetto")
        header = f"**[{label}]"
        if model_name:
            header += f" ({model_name})"
        header += "**: "
        self._stream_text = []
        self._stream_active = True
        self._stream_header = header
        self._stream_index = len(self._conversation_segments)
        self._conversation_segments.append(header)
        self._reasoning_buffer.clear()
        self._reasoning_log_index = None
        self._refresh_conversation(scroll=True)

    # ------------------------------------------------------------------
    def append_stream(self, chunk: str) -> None:
        if not self._stream_active or not chunk:
            return
        self._stream_text.append(chunk)
        if self._stream_index is not None and self._stream_index < len(
                self._conversation_segments
        ):
            self._conversation_segments[self._stream_index] += chunk
            self._refresh_conversation(scroll=True)

    # ------------------------------------------------------------------
    def finish_stream(self, final_text: str) -> None:
        content = final_text or "".join(self._stream_text)
        idx = self._stream_index
        if idx is not None and idx < len(self._conversation_segments):
            if content.strip():
                header = self._stream_header or ""
                segment = f"{header}{content}" if header else content
                self._conversation_segments[idx] = segment.rstrip()
            else:
                self._conversation_segments.pop(idx)
        self._stream_text = []
        self._stream_index = None
        self._stream_active = False
        self._stream_header = None
        if content:
            self._log_assistant_event(content)
        self._refresh_conversation(scroll=True)

    # ------------------------------------------------------------------
    def append_reasoning(self, chunk: str) -> None:
        return  # TODO
        """
        if not chunk:
            return
        self._reasoning_buffer.append(chunk)
        current_text = "".join(self._reasoning_buffer)
        if self._reasoning_log_index is None:
            entry = LogEntry(
                timestamp=datetime.datetime.now(),
                level=LogLevel.INFO,
                category=LogCategory.REASONING,
                message=current_text,
            )
            self._log_entries.append(entry)
            self._reasoning_log_index = len(self._log_entries) - 1
        else:
            entry = self._log_entries[self._reasoning_log_index]
            entry.message = current_text
            entry.timestamp = datetime.datetime.now()
        self._refresh_log_widget(scroll=True)
        """

    # ------------------------------------------------------------------
    def finish_reasoning(self) -> None:
        self._reasoning_log_index = None
        self._reasoning_buffer.clear()


class _StatusPanelManager(StatusPanel):
    """Controller keeping a singleton instance of the status panel.

    This is the StatusPanel implementation that the rest of Gepetto should
    depend on; it wraps a GepettoStatusForm and handles threading and IDA UI.
    """

    def __init__(self) -> None:
        self._form: GepettoStatusForm | None = None
        self._stop_callback: Callable[[], None] | None = None

    # ------------------------------------------------------------------
    def ensure_shown(self) -> None:
        if self._form is None:
            self._form = GepettoStatusForm(self)
            try:
                option = getattr(ida_kernwin.PluginForm, "WOPN_CREATE_ONLY", 0)
                self._form.Show(STATUS_PANEL_CAPTION, options=option)
            except Exception:
                print(_("Could not show Gepetto Status panel."))
                self._form = None

    # ------------------------------------------------------------------
    def form_closed(self) -> None:
        self._form = None

    # ------------------------------------------------------------------
    def on_form_ready(self) -> None:
        self.set_model(str(gepetto.config.model))
        if self._stop_callback and self._form is not None:
            self._form.set_stop_callback(self._stop_callback)
            self._form.reset_stop()

        def _show_and_dock() -> None:
            if self._form is not None:
                ida_kernwin.display_widget(
                    self._form.twidget(),
                    _DEFAULT_DOCK_OPTIONS,
                    None,
                )
            orient = getattr(ida_kernwin, "DP_RIGHT", 0)
            szhint = getattr(ida_kernwin, "DP_SZHINT", 0)
            ida_kernwin.set_dock_pos(STATUS_PANEL_CAPTION, "IDA", orient | szhint)

        run_when_desktop_ready(_show_and_dock)

    # ------------------------------------------------------------------
    def set_model(self, model_name: str) -> None:
        self._dispatch(lambda form: form.set_model(model_name))

    # ------------------------------------------------------------------
    def set_status(self, text: str, *, busy: bool = False, error: bool = False) -> None:
        self._dispatch(lambda form: form.set_status(text, busy=busy, error=error))

    # ------------------------------------------------------------------
    def reset_stop(self) -> None:
        self._dispatch(lambda form: form.reset_stop())

    # ------------------------------------------------------------------
    def set_stop_callback(self, callback: Callable[[], None] | None) -> None:
        self._stop_callback = callback

        def apply(form: GepettoStatusForm) -> None:
            form.set_stop_callback(callback)

        self._dispatch(apply)
        self.reset_stop()

    # ------------------------------------------------------------------
    def has_stop_callback(self) -> bool:
        return self._stop_callback is not None

    # ------------------------------------------------------------------
    def request_stop(self) -> None:
        if self._stop_callback:
            self._dispatch(lambda form: form.mark_cancelling())
            try:
                self._stop_callback()
            except Exception:
                self.mark_error(_("Failed to cancel request"))
                return
            self.set_status(_("Idle"), busy=False)
            self.reset_stop()

    # ------------------------------------------------------------------
    def submit_chat(self, text: str) -> None:
        """Submit chat text through the CLI (used by the embedded chat input)."""
        try:
            from gepetto.ida import cli as gepetto_cli
        except Exception as exc:  # pragma: no cover
            raise RuntimeError("Gepetto CLI is not available") from exc

        if getattr(gepetto_cli, "CLI", None) is None:
            try:
                gepetto_cli.register_cli(self)  # inject this StatusPanel if you want
            except Exception as exc:  # pragma: no cover
                raise RuntimeError("Failed to initialize Gepetto CLI") from exc

        cli_instance = getattr(gepetto_cli, "CLI", None)
        if cli_instance is None:
            raise RuntimeError("Gepetto CLI is not ready")

        result = cli_instance.OnExecuteLine(text)
        if result is False:
            raise RuntimeError("CLI rejected input")

    # ------------------------------------------------------------------
    def start_stream(self) -> None:
        self._dispatch(lambda form: form.start_stream())

    # ------------------------------------------------------------------
    def append_stream(self, chunk: str) -> None:
        self._dispatch(lambda form: form.append_stream(chunk))

    # ------------------------------------------------------------------
    def finish_stream(self, final_text: str) -> None:
        self._dispatch(lambda form: form.finish_stream(final_text))
        self.set_status(_("Done"), busy=False)
        self.reset_stop()

    # ------------------------------------------------------------------
    def append_reasoning(self, chunk: str) -> None:
        self._dispatch(lambda form: form.append_reasoning(chunk))

    # ------------------------------------------------------------------
    def finish_reasoning(self) -> None:
        self._dispatch(lambda form: form.finish_reasoning())

    # ------------------------------------------------------------------
    def log_user(self, text: str) -> None:
        self._dispatch(lambda form: form.log_user(text))

    # ------------------------------------------------------------------
    def log(
            self,
            message: str,
            *,
            category: LogCategory = LogCategory.SYSTEM,
            level: LogLevel = LogLevel.INFO,
    ) -> None:
        self._dispatch(
            lambda form: form.append_log(
                message,
                False,
                category=category,
                level=level,
            )
        )

    # ------------------------------------------------------------------
    def log_request_started(self) -> str:
        message = _("Request to {model} sent...").format(model=str(gepetto.config.model))
        self.log(message, category=LogCategory.MODEL)
        self.set_status(_("Waiting for model..."), busy=True)
        return message

    # ------------------------------------------------------------------
    def log_request_finished(self, elapsed_seconds: float) -> str:
        message = _("{model} query finished in {time:.2f} seconds!").format(
            model=str(gepetto.config.model),
            time=elapsed_seconds,
        )
        self.log(message, category=LogCategory.MODEL, level=LogLevel.SUCCESS)
        self.set_status(_("Done"), busy=False)
        self.reset_stop()
        return message

    # ------------------------------------------------------------------
    def mark_error(self, message: str) -> None:
        self._dispatch(lambda form: form.mark_error(message))

    # ------------------------------------------------------------------
    def clear_log(self) -> None:
        self._dispatch(lambda form: form.clear_log())

    # ------------------------------------------------------------------
    def close(self) -> None:
        if self._form and self._form.is_ready():
            try:
                ida_kernwin.close_widget(self._form.widget(), 0)
            except Exception:
                pass
        self._form = None
        self._stop_callback = None

    # ------------------------------------------------------------------
    def _dispatch(self, action: Callable[[GepettoStatusForm], None]) -> None:
        if self._form is None:
            return

        def runner() -> None:
            form = self._form
            if not form or not form.is_ready():
                return
            try:
                action(form)
            except Exception:
                pass

        widget = self._form.widget()
        if widget:
            current_thread = QtCore.QThread.currentThread()
            widget_thread = widget.thread()
            if widget_thread == current_thread:
                runner()
                return

        try:
            ida_kernwin.execute_sync(runner, ida_kernwin.MFF_FAST)
        except Exception:
            runner()

```

`gepetto/ida/status_panel/status_panel_factory.py`:

```py
from .panel_interface import StatusPanel
from .no_panel import NoStatusPanel

_panel: StatusPanel | None = None


def get_status_panel() -> StatusPanel:
    global _panel
    if _panel is not None:
        return _panel

    # Try to build a Qt one; fall back to null if anything goes wrong.
    try:
        from .qt_panel import _StatusPanelManager
        _panel = _StatusPanelManager()
    except Exception:
        _panel = NoStatusPanel()

    return _panel

```

`gepetto/ida/tools/__init__.py`:

```py
"""Expose IDA tool modules for convenient attribute access."""

# Import tool handler modules so ``gepetto.ida.tools.<module>`` works even if
# only the package is imported. This mirrors the legacy behaviour relied upon
# by the CLI and external integrations.

from . import declare_c_type  # noqa: F401
from . import decompile_function  # noqa: F401
from . import get_bytes  # noqa: F401
from . import get_current_function  # noqa: F401
from . import get_disasm  # noqa: F401
from . import disasm_function  # noqa: F401
from . import get_ea  # noqa: F401
from . import get_screen_ea  # noqa: F401
from . import get_struct  # noqa: F401
from . import get_xrefs  # noqa: F401
from . import list_functions  # noqa: F401
from . import list_imports  # noqa: F401
from . import list_symbols  # noqa: F401
from . import refresh_view  # noqa: F401
from . import rename_function  # noqa: F401
from . import rename_lvar  # noqa: F401
from . import search  # noqa: F401
from . import set_comment  # noqa: F401
from . import to_hex  # noqa: F401
from . import run_python  # noqa: F401
from . import rename_global  # noqa: F401

__all__ = [
    "declare_c_type",
    "decompile_function",
    "get_bytes",
    "get_current_function",
    "get_disasm",
    "disasm_function",
    "get_ea",
    "get_screen_ea",
    "get_struct",
    "get_xrefs",
    "list_functions",
    "list_imports",
    "list_symbols",
    "refresh_view",
    "rename_function",
    "rename_lvar",
    "search",
    "set_comment",
    "to_hex",
    "run_python",
    "rename_global",
]

```

`gepetto/ida/tools/declare_c_type.py`:

```py
import json
import re

import ida_typeinf
import idc

from gepetto.ida.tools.tools import (
    add_result_to_messages,
    tool_error_payload,
    tool_result_payload,
)


from gepetto.ida.utils.thread_helpers import ida_write


_TYPE_NAME_PATTERNS = [
    re.compile(r"\btypedef\s+.*?\b([A-Za-z_]\w*)\s*;", re.IGNORECASE | re.DOTALL),
    re.compile(r"\bstruct\s+([A-Za-z_]\w*)\b", re.IGNORECASE),
    re.compile(r"\bunion\s+([A-Za-z_]\w*)\b", re.IGNORECASE),
    re.compile(r"\benum\s+([A-Za-z_]\w*)\b", re.IGNORECASE),
    re.compile(r"\bclass\s+([A-Za-z_]\w*)\b", re.IGNORECASE),
    re.compile(r"\b([A-Za-z_]\w*)\s*\(", re.IGNORECASE),
]


def handle_declare_c_type_tc(tc, messages):
    """Handle a tool call to declare C types."""
    try:
        args = json.loads(getattr(tc.function, "arguments", "") or "{}")
    except Exception:
        args = {}

    c_decl = args.get("c_declaration")

    try:
        data = declare_c_type(c_declaration=c_decl)
        payload = tool_result_payload(data)
    except Exception as ex:
        payload = tool_error_payload(str(ex), c_declaration=c_decl)

    add_result_to_messages(messages, tc, payload)


# -----------------------------------------------------------------------------


@ida_write
def _apply_declarations(decl_text: str) -> dict[str, object]:
    til = ida_typeinf.get_idati()
    if til is None:
        raise RuntimeError("Local type library is unavailable.")

    errors = ida_typeinf.parse_decls(til, decl_text, None, ida_typeinf.PT_SIL)
    if errors:
        fallback_errors = idc.parse_decls(decl_text, ida_typeinf.PT_SIL)
        if fallback_errors:
            raise RuntimeError(f"Failed to parse declarations ({fallback_errors} errors).")

    candidates = _extract_type_candidates(decl_text)
    chosen = ""
    for candidate in candidates:
        try:
            if ida_typeinf.get_named_type(til, candidate, ida_typeinf.NTF_TYPE):
                chosen = candidate
                break
        except Exception:
            continue
    if not chosen and candidates:
        chosen = candidates[-1]

    return {"success": True, "type_name": chosen}


def _extract_type_candidates(text: str) -> list[str]:
    """Return best-effort candidate names declared by the snippet."""
    candidates: list[str] = []
    for pattern in _TYPE_NAME_PATTERNS:
        for match in pattern.finditer(text):
            candidate = match.group(1)
            if candidate and candidate not in candidates:
                candidates.append(candidate)
    return candidates


def declare_c_type(c_declaration: str | None) -> dict[str, object]:
    """Parse and register one or more C declarations."""
    if not c_declaration or not c_declaration.strip():
        raise ValueError("c_declaration must be a non-empty string")

    decl_text = c_declaration.strip()
    return _apply_declarations(decl_text)


```

`gepetto/ida/tools/decompile_function.py`:

```py
import json
from typing import Any

import idaapi  # type: ignore
import ida_funcs  # type: ignore
import ida_hexrays  # type: ignore

import gepetto.config
from gepetto.ida.utils.thread_helpers import hexrays_available, run_on_main_thread
from gepetto.ida.utils.function_helpers import (
    get_func_name,
    parse_ea,
    resolve_ea,
    resolve_func,
)
from gepetto.ida.tools.tools import (
    add_result_to_messages,
    tool_error_payload,
    tool_result_payload,
)

_ = gepetto.config._


def handle_decompile_function_tc(tc, messages):
    """Handle a tool call requesting function decompilation."""
    try:
        args = json.loads(getattr(tc.function, "arguments", "") or "{}")
    except Exception:
        args = {}

    ea_arg = args.get("ea")
    name = args.get("name")

    try:
        decompiled = decompile_function(ea=ea_arg, name=name)
        payload = tool_result_payload({"pseudocode": str(decompiled)})
    except Exception as ex:
        payload = tool_error_payload(
            str(ex),
            ea=ea_arg,
            name=name,
        )

    add_result_to_messages(messages, tc, payload)


def decompile_function(
    ea: Any | None = None,
    name: str | None = None,
) -> tuple[dict[str, Any], idaapi.cfuncptr_t]:
    """Decompile the function identified by ``ea`` or ``name``."""

    target_ea: int | None = None
    if ea is not None:
        target_ea = parse_ea(ea)

    function = resolve_func(ea=target_ea, name=name)
    func_name = name or get_func_name(function)
    if target_ea is None:
        target_ea = resolve_ea(func_name)

    if not isinstance(target_ea, int) or target_ea == idaapi.BADADDR:
        raise RuntimeError("Invalid function address for decompilation")
    if not hexrays_available():
        raise RuntimeError("Hex-Rays not available: install or enable the decompiler.")

    def _do() -> idaapi.cfuncptr_t:
        func = ida_funcs.get_func(target_ea) if ida_funcs is not None else None
        decompiled = ida_hexrays.decompile(func) if func is not None else ida_hexrays.decompile(target_ea)
        if decompiled is None:
            raise RuntimeError(f"Hex-Rays failed to decompile function at {target_ea:#x}")
        return decompiled

    return run_on_main_thread(_do)

```

`gepetto/ida/tools/disasm_function.py`:

```py
import json

import ida_lines
import idautils

from gepetto.ida.utils.function_helpers import get_func_name, parse_ea, resolve_func
from gepetto.ida.tools.tools import (
    add_result_to_messages,
    tool_error_payload,
    tool_result_payload,
)
from gepetto.ida.utils.thread_helpers import ida_read


def handle_disasm_function_tc(tc, messages):
    """Handle tool call for disasm_function."""
    try:
        args = json.loads(getattr(tc.function, "arguments", "") or "{}")
    except Exception:
        args = {}

    ea = args.get("ea")
    name = args.get("name")
    try:
        data = disasm_function(ea=ea, name=name)
        payload = tool_result_payload(data)
    except Exception as ex:
        payload = tool_error_payload(str(ex), ea=ea, name=name)

    add_result_to_messages(messages, tc, payload)


@ida_read
def _collect_disasm_lines(func_start_ea: int):
    lines = []
    for item_ea in idautils.FuncItems(func_start_ea):
        line = ida_lines.tag_remove(ida_lines.generate_disasm_line(item_ea, 0)) or ""
        if not line:
            continue
        lines.append(f"{int(item_ea):#x}: {line}")
    return lines


def disasm_function(ea=None, name=None) -> dict:
    """Return the disassembly for every item in a function."""
    target_ea = None
    if ea is not None:
        target_ea = parse_ea(ea)
    if target_ea is None and not name:
        raise ValueError("Provide either ea or name")

    func = resolve_func(ea=target_ea, name=name)
    func_name = get_func_name(func)
    lines = _collect_disasm_lines(int(func.start_ea))

    return {
        "function": {
            "name": func_name,
            "start_ea": int(func.start_ea),
            "end_ea": int(func.end_ea),
        },
        "disasm": "\n".join(lines),
    }

```

`gepetto/ida/tools/get_bytes.py`:

```py
import json

import ida_bytes
import idaapi
import ida_segment

from gepetto.ida.utils.function_helpers import parse_ea, get_ptr_size, get_endianness
from gepetto.ida.tools.tools import (
    add_result_to_messages,
    tool_error_payload,
    tool_result_payload,
)
from gepetto.ida.utils.thread_helpers import ida_read


# -----------------------------------------------------------------------------
# Tool call handler
# -----------------------------------------------------------------------------

def handle_get_bytes_tc(tc, messages):
    """Handle tool call for get_bytes."""
    try:
        args = json.loads(tc.function.arguments or "{}")
    except Exception:
        args = {}

    ea = args.get("ea")
    size = args.get("size", 0x20)
    auto_dereference = args.get("auto_dereference", True)

    try:
        ea_i = parse_ea(ea)
        size_i = int(size)
        auto_deref_b = bool(auto_dereference)

        data = get_bytes(ea_i, size_i, auto_dereference=auto_deref_b)
        payload = tool_result_payload(data)
    except Exception as ex:
        payload = tool_error_payload(
            str(ex),
            ea=ea if isinstance(ea, int) else None,
            size=size if isinstance(size, int) else None,
            auto_dereference=auto_dereference if isinstance(auto_dereference, bool) else None,
        )

    add_result_to_messages(messages, tc, payload)


@ida_read
def _read_bytes(ea: int, size: int) -> bytes:
    return ida_bytes.get_bytes(ea, size) or b""


# -----------------------------------------------------------------------------
# Formatting / decoding helpers
# -----------------------------------------------------------------------------

def _format_bytes(bs: bytes) -> str:
    return " ".join(f"0x{b:02X}" for b in bs)


def _is_printable_byte(b: int) -> bool:
    # Consider \t, \n, \r printable (in addition to ASCII printable range).
    return b in (9, 10, 13) or 32 <= b <= 126


def _decode_if_printable(
        bs: bytes,
        *,
        max_len: int | None = None,
        min_printable_ratio: float = 0.85,
) -> dict[str, str] | None:
    """
    If the bytes look like mostly-printable text, return:
      {"encoding": "...", "text": "..."}
    Always caps recognition to max_len (defaults to len(bs)).
    """
    if not bs:
        return None

    if max_len is None:
        max_len = len(bs)
    if max_len <= 0:
        return None

    capped = bs[:max_len]

    nul = capped.find(b"\x00")
    candidate = capped if nul < 0 else capped[:nul]
    if not candidate:
        return None

    printable = sum(1 for x in candidate if _is_printable_byte(x))
    ratio = printable / max(1, len(candidate))
    if ratio < min_printable_ratio:
        return None

    try:
        return {"encoding": "utf-8", "text": candidate.decode("utf-8")}
    except UnicodeDecodeError:
        return {"encoding": "latin-1", "text": candidate.decode("latin-1")}

# -----------------------------------------------------------------------------
# Pointer / offset detection
# -----------------------------------------------------------------------------

@ida_read
def _read_ptr_value(ea: int) -> int:
    ps = get_ptr_size()
    if ps == 8:
        return ida_bytes.get_qword(ea)
    if ps == 4:
        return ida_bytes.get_dword(ea)
    return ida_bytes.get_word(ea)

@ida_read
def _looks_like_pointer_value(ptr: int) -> bool:
    if ptr in (0, idaapi.BADADDR):
        return False

    try:
        inf = idaapi.get_inf_structure()
        if ptr < inf.min_ea or ptr >= inf.max_ea:
            return False
    except AttributeError:  # IDA 9.x API
        import ida_ida
        if ptr < ida_ida.inf_get_min_ea() or ptr >= ida_ida.inf_get_max_ea():
            return False

    if ida_segment.getseg(ptr) is None:
        return False

    if not ida_bytes.is_loaded(ptr):
        return False

    return True

@ida_read
def _ida_offset_target(ea: int) -> int | None:
    """
    If IDA has a data item at EA marked as an offset, return the offset target.
    """
    head = ida_bytes.get_item_head(ea)
    if head == idaapi.BADADDR:
        return None

    f = ida_bytes.get_full_flags(head)

    # For data items, operand 0 is the relevant "offsetness" in practice.
    if ida_bytes.is_data(f) and ida_bytes.is_off0(f):
        ptr = _read_ptr_value(head)
        return ptr if ptr not in (0, idaapi.BADADDR) else None

    return None


def _heuristic_pointer_target(ea: int, bs_at_ea: bytes) -> int | None:
    """
    Fallback heuristic: interpret the first pointer-sized bytes as an EA and
    accept if it lands in a loaded segment.
    """
    ps = get_ptr_size()
    if len(bs_at_ea) < ps:
        return None

    ptr = int.from_bytes(bs_at_ea[:ps], get_endianness(), signed=False)

    return ptr if _looks_like_pointer_value(ptr) else None


def _choose_deref_target(ea: int, bs_at_ea: bytes) -> tuple[int | None, str | None]:
    """
    Returns (target_ea, reason) or (None, None).
    reason is "ida_offset" or "heuristic".
    """
    ida_ptr = _ida_offset_target(ea)
    if ida_ptr is not None and _looks_like_pointer_value(ida_ptr):
        return ida_ptr, "ida_offset"

    heur_ptr = _heuristic_pointer_target(ea, bs_at_ea)
    if heur_ptr is not None:
        return heur_ptr, "heuristic"

    return None, None

# -----------------------------------------------------------------------------
# Public tool function
# -----------------------------------------------------------------------------

def get_bytes(ea: int, size: int = 0x20, *, auto_dereference: bool = True) -> dict[str, object]:
    """
    Return bytes starting at EA.

    If auto_dereference is True and EA looks like a pointer/offset, the returned
    primary "bytes"/"decoded" correspond to the pointed-to data, and the original
    bytes at EA are preserved under "pointer_bytes"/"pointer_decoded".
    """
    if size <= 0:
        raise ValueError("size must be a positive integer")

    bs_at_ea = _read_bytes(ea, size)

    # Default: primary view is the requested EA.
    primary_ea = ea
    primary_bs = bs_at_ea
    primary_reason = None
    pointer_info: dict[str, object] | None = None

    if auto_dereference:
        target, reason = _choose_deref_target(ea, bs_at_ea)
        if target is not None and target != ea:
            deref_bs = _read_bytes(target, size)

            # Primary becomes dereferenced data.
            primary_ea = target
            primary_bs = deref_bs
            primary_reason = reason

            # Preserve pointer bytes (what was at the requested EA).
            pointer_info = {
                "pointer_ea": ea,
                "pointer_size": get_ptr_size(),
                "pointer_bytes": _format_bytes(bs_at_ea),
            }
            pointer_dec = _decode_if_printable(bs_at_ea, max_len=size)
            if pointer_dec:
                pointer_info["pointer_decoded"] = pointer_dec

    out: dict[str, object] = {
        # The EA we actually returned bytes for (may differ if dereferenced)
        "ea": primary_ea,
        "size": size,
        "bytes": _format_bytes(primary_bs),
    }

    dec = _decode_if_printable(primary_bs, max_len=size)
    if dec:
        out["decoded"] = dec

    if pointer_info is not None:
        out["auto_dereference"] = {
            "enabled": True,
            "reason": primary_reason,     # "ida_offset" or "heuristic"
            "requested_ea": ea,
            "returned_ea": primary_ea,
            **pointer_info,
        }
    else:
        out["auto_dereference"] = {
            "enabled": bool(auto_dereference),
            "applied": False,
            "requested_ea": ea,
        }

    return out

```

`gepetto/ida/tools/get_current_function.py`:

```py
import json

import ida_funcs

from gepetto.ida.utils.function_helpers import get_func_name
from gepetto.ida.utils.thread_helpers import BADADDR, ida_read, safe_get_screen_ea
from gepetto.ida.tools.tools import (
    add_result_to_messages,
    tool_error_payload,
    tool_result_payload,
)


def handle_get_current_function_tc(tc, messages):
    """Handle a tool call to fetch the function at the screen EA."""
    try:
        json.loads(getattr(tc.function, "arguments", "") or "{}")
    except Exception:
        # No arguments expected; ignore malformed payloads.
        pass

    try:
        data = get_current_function()
        payload = tool_result_payload(data)
    except Exception as ex:
        payload = tool_error_payload(str(ex))

    add_result_to_messages(messages, tc, payload)


# -----------------------------------------------------------------------------


@ida_read
def _resolve_function_metadata(ea: int) -> dict[str, object]:
    fn = ida_funcs.get_func(ea)
    if not fn:
        raise RuntimeError(f"EA 0x{ea:X} is not inside a function.")
    return {
        "ea": int(ea),
        "start_ea": int(fn.start_ea),
        "end_ea": int(fn.end_ea),
        "name": get_func_name(fn) or "",
    }


def get_current_function() -> dict[str, object]:
    """Return metadata for the function under the current cursor."""
    screen_ea = safe_get_screen_ea()
    if screen_ea == BADADDR:
        raise RuntimeError("No active screen location is available.")

    return _resolve_function_metadata(screen_ea)


```

`gepetto/ida/tools/get_disasm.py`:

```py
import json

import ida_lines

from gepetto.ida.utils.function_helpers import parse_ea
from gepetto.ida.tools.tools import (
    add_result_to_messages,
    tool_error_payload,
    tool_result_payload,
)
from gepetto.ida.utils.thread_helpers import ida_read


def handle_get_disasm_tc(tc, messages):
    """Handle tool call for get_disasm."""
    try:
        args = json.loads(tc.function.arguments or "{}")
    except Exception:
        args = {}

    ea = args.get("ea")
    try:
        ea = parse_ea(ea)
        data = get_disasm(ea)
        payload = tool_result_payload(data)
    except Exception as ex:
        payload = tool_error_payload(
            str(ex),
            ea=ea if isinstance(ea, int) else None,
        )

    add_result_to_messages(messages, tc, payload)

# -----------------------------------------------------------------------------

@ida_read
def _get_disasm_line(ea: int) -> str:
    return ida_lines.tag_remove(ida_lines.generate_disasm_line(ea, 0)) or ""

# -----------------------------------------------------------------------------

def get_disasm(ea: int) -> dict[str, int | str]:
    """Return the disassembly line at a given EA."""

    line = _get_disasm_line(ea)
    return {
        "ea": ea,
        "disasm": line,
    }

```

`gepetto/ida/tools/get_ea.py`:

```py
import json

from gepetto.ida.utils.function_helpers import resolve_ea
from gepetto.ida.tools.tools import (
    add_result_to_messages,
    tool_error_payload,
    tool_result_payload,
)


def handle_get_ea_tc(tc, messages):
    try:
        args = json.loads(tc.function.arguments or "{}")
    except Exception:
        args = {}

    name = args.get("name")

    try:
        if not isinstance(name, str) or not name.strip():
            raise ValueError("name must be a non-empty string")
        ea = get_ea(name.strip())
        payload = tool_result_payload({"ea": ea})
    except Exception as ex:
        payload = tool_error_payload(str(ex), name=name)

    add_result_to_messages(messages, tc, payload)


def get_ea(name: str) -> int:
    """Return the effective address for a symbol name."""
    return resolve_ea(name)

```

`gepetto/ida/tools/get_screen_ea.py`:

```py
import json

from gepetto.ida.tools.tools import (
    add_result_to_messages,
    tool_error_payload,
    tool_result_payload,
)
from gepetto.ida.utils.thread_helpers import BADADDR, safe_get_screen_ea


def handle_get_screen_ea_tc(tc, messages):
    # The tool takes no arguments, but parse for forward compatibility.
    try:
        _ = json.loads(tc.function.arguments or "{}")
    except Exception:
        _ = {}

    ea = get_screen_ea()

    if ea is not None:
        payload = tool_result_payload({"ea": ea})
    else:
        payload = tool_error_payload(
            "The cursor isn't set to a valid address. Click in a disassembly view first."
        )

    add_result_to_messages(messages, tc, payload)

# -----------------------------------------------------------------------------

def get_screen_ea() -> str | None:
    ea = safe_get_screen_ea()
    return None if ea == BADADDR else hex(ea)


```

`gepetto/ida/tools/get_struct.py`:

```py
import json

import ida_typeinf
import idaapi

from gepetto.ida.tools.tools import (
    add_result_to_messages,
    tool_error_payload,
    tool_result_payload,
)

from gepetto.ida.utils.thread_helpers import ida_read


def handle_get_struct_tc(tc, messages):
    """Handle a tool call to fetch structure metadata."""
    try:
        args = json.loads(getattr(tc.function, "arguments", "") or "{}")
    except Exception:
        args = {}

    name = args.get("name")

    try:
        data = get_struct(name=name)
        payload = tool_result_payload(data)
    except Exception as ex:
        payload = tool_error_payload(str(ex), name=name)

    add_result_to_messages(messages, tc, payload)


# -----------------------------------------------------------------------------


@ida_read
def _snapshot_struct(query: str) -> dict[str, object]:
    tif, resolved_name = _resolve_struct_tinfo(query)

    udt = ida_typeinf.udt_type_data_t()
    if not tif.get_udt_details(udt):
        raise RuntimeError(f"Failed to fetch fields for structure {resolved_name!r}.")

    fields = [_format_member(member) for member in _iter_udt_members(udt)]
    size_value = 0
    if hasattr(tif, "get_size"):
        raw_size = tif.get_size()
        badsize = getattr(idaapi, "BADSIZE", None)
        if badsize is not None and raw_size == badsize:
            raw_size = 0
        size_value = int(raw_size)

    return {
        "name": resolved_name,
        "size": size_value,
        "fields": fields,
    }


def _resolve_struct_tinfo(name: str) -> tuple[ida_typeinf.tinfo_t, str]:
    """Locate a structure type by name using ida_typeinf APIs."""

    til = ida_typeinf.get_idati()
    if til is None:
        raise RuntimeError("Local type library is unavailable.")

    candidates = _struct_name_candidates(name)
    decl_types = _decl_type_candidates()

    for candidate in candidates:
        result = _find_udt_in_til(til, candidate, decl_types)
        if result is not None:
            return result

    raise RuntimeError(f"Structure {name!r} was not found.")


def _find_udt_in_til(
    til: ida_typeinf.til_t,
    candidate: str,
    decl_types: tuple[int, ...],
) -> tuple[ida_typeinf.tinfo_t, str] | None:
    for decl_type in decl_types:
        tif = ida_typeinf.tinfo_t()
        try:
            found = tif.get_named_type(til, candidate, decl_type, True, True)
        except TypeError:  # Older SDKs omit the extended signature
            found = tif.get_named_type(til, candidate)
        if not found:
            continue
        if _tinfo_is_udt(tif):
            return tif, candidate

    tid = ida_typeinf.get_named_type_tid(candidate) if hasattr(ida_typeinf, "get_named_type_tid") else idaapi.BADADDR
    bad = {idaapi.BADADDR, getattr(idaapi, "BADNODE", idaapi.BADADDR)}
    if tid in bad:
        return None

    tif = ida_typeinf.tinfo_t(tid=tid)
    if _tinfo_is_udt(tif):
        resolved = ida_typeinf.get_tid_name(tid) if hasattr(ida_typeinf, "get_tid_name") else None
        display_name = resolved or candidate
        return tif, display_name
    return None


def _tinfo_is_udt(tif: ida_typeinf.tinfo_t) -> bool:
    if not tif or not tif.is_correct():
        return False
    return bool(tif.is_udt())


def _struct_name_candidates(name: str) -> tuple[str, ...]:
    base = str(name or "").strip()
    if not base:
        return tuple()

    variants = [base]
    lowered = base.lower()
    if not lowered.startswith("struct "):
        variants.append(f"struct {base}")
    if not lowered.startswith("union "):
        variants.append(f"union {base}")

    head, _, tail = base.partition(" ")
    if tail and head.lower() in {"struct", "union"}:
        variants.append(tail.strip())

    seen: set[str] = set()
    ordered: list[str] = []
    for variant in variants:
        if variant not in seen:
            seen.add(variant)
            ordered.append(variant)
    return tuple(ordered)


def _decl_type_candidates() -> tuple[int, ...]:
    values = [ida_typeinf.BTF_STRUCT, ida_typeinf.BTF_UNION]
    if hasattr(ida_typeinf, "BTF_TYPEDEF"):
        values.append(ida_typeinf.BTF_TYPEDEF)
    values.append(0)
    return tuple(values)


def _iter_udt_members(udt: ida_typeinf.udt_type_data_t):
    size_fn = getattr(udt, "size", None)
    if callable(size_fn):
        count = size_fn()
        getter = getattr(udt, "__getitem__", None)
        if callable(getter):
            for index in range(count):
                yield getter(index)
            return
        at_fn = getattr(udt, "at", None)
        if callable(at_fn):
            for index in range(count):
                yield at_fn(index)
            return
    for member in udt:
        yield member


def _format_member(member) -> dict[str, object]:
    """Return a serialisable description for a struct member."""

    name = getattr(member, "name", "") or ""
    offset_bits = getattr(member, "offset", None)
    if offset_bits is None:
        offset_bits = getattr(member, "soff", 0)
    offset = int(offset_bits // 8) if isinstance(offset_bits, (int, float)) else 0

    type_info = getattr(member, "type", None)
    type_text = ""
    if isinstance(type_info, ida_typeinf.tinfo_t):
        try:
            if hasattr(type_info, "dstr"):
                type_text = type_info.dstr()
            else:
                type_text = ida_typeinf.dstr_tinfo(type_info) or ""
        except Exception:
            type_text = ""

    return {"offset": offset, "type": str(type_text or ""), "name": name}


def get_struct(name: str | None) -> dict[str, object]:
    """Return metadata for a structure defined in the local types."""
    if not name or not str(name).strip():
        raise ValueError("name is required")

    query = str(name).strip()
    return _snapshot_struct(query)

```

`gepetto/ida/tools/get_xrefs.py`:

```py
import json
from collections.abc import Iterable
from typing import Any

import idaapi
import ida_bytes
import ida_funcs
import ida_name
import ida_xref

from gepetto.ida.utils.function_helpers import parse_ea, resolve_ea, resolve_func, get_func_name
from gepetto.ida.tools.tools import (
    add_result_to_messages,
    tool_error_payload,
    tool_result_payload,
)

from gepetto.ida.utils.thread_helpers import ida_read

def handle_get_xrefs_tc(tc, messages):
    """Handle a tool call to fetch cross-references (EA/function/name)."""
    try:
        args = json.loads(tc.function.arguments or "{}")
    except Exception:
        args = {}

    # Primary args
    scope = args.get("scope", "ea")
    subject = args.get("subject")

    direction = args.get("direction", "both")
    kind = args.get("kind", "both")
    only_calls = bool(args.get("only_calls", False))
    exclude_flow = bool(args.get("exclude_flow", False))
    collapse_by = args.get("collapse_by", "site")
    enrich_names = bool(args.get("enrich_names", True))

    try:
        if not isinstance(subject, str) or not subject.strip():
            raise ValueError("subject must be a non-empty string")

        normalized_subject = subject.strip()

        data = get_xrefs_unified(
            scope=scope,
            subject=normalized_subject,
            direction=direction,
            kind=kind,
            only_calls=only_calls,
            exclude_flow=exclude_flow,
            collapse_by=collapse_by,
            enrich_names=enrich_names,
        )
        payload = tool_result_payload(data)
    except Exception as ex:
        payload = tool_error_payload(
            str(ex),
            scope=scope,
            subject=subject,
            direction=direction,
            kind=kind,
        )

    add_result_to_messages(messages, tc, payload)

# -----------------------------------------------------------------------------

@ida_read
def _gather_xrefs(
    scope: str,
    subject: str,
    direction: str,
    kind: str,
    only_calls: bool,
    exclude_flow: bool,
    collapse_by: str | None,
    enrich_names: bool,
) -> dict[str, Any]:
    idaapi.auto_wait()

    if scope == "ea":
        target_ea = parse_ea(subject)
        f = ida_funcs.get_func(target_ea)
        subject_kind = "function" if f and f.start_ea == target_ea else "item"
    elif scope == "function":
        f = resolve_func(ea=parse_ea(subject))
        target_ea = f.start_ea
        subject_kind = "function"
    else:  # scope == "name"
        name_ea = resolve_ea(subject)
        f = ida_funcs.get_func(name_ea)
        if f:
            target_ea = f.start_ea
            subject_kind = "function"
        else:
            target_ea = name_ea
            subject_kind = "item"

    subj_name = _ea_func_name(target_ea) if enrich_names else None

    filter_code = kind == "code"
    filter_data = kind == "data"

    out: dict[str, Any] = {
        "scope": scope,
        "subject": {"ea": int(target_ea), "name": subj_name or "", "kind": subject_kind},
        "direction": direction,
        "filters": {},
        "xrefs": [],
        "stats": {},
    }

    items: Iterable[int]
    if subject_kind == "function":
        fn = ida_funcs.get_func(target_ea)
        items = _iter_func_items(fn)
    else:
        items = (target_ea,)

    results = []
    for item_ea in items:
        if direction in {"to", "both"}:
            for frm_ea, t, iscode in _collect_xrefs_to(item_ea, ida_xref.XREF_ALL):
                results.append(("to", frm_ea, item_ea, t, iscode))
        if direction in {"from", "both"}:
            for to_ea, t, iscode in _collect_xrefs_from(item_ea, ida_xref.XREF_ALL):
                results.append(("from", item_ea, to_ea, t, iscode))

    filtered = []
    for dirn, frm, to, t, iscode in results:
        kind_label = "code" if iscode else "data"
        if filter_code and not iscode:
            continue
        if filter_data and iscode:
            continue
        if only_calls and not (iscode and _is_call(t)):
            continue
        if exclude_flow and (iscode and _is_flow(t)):
            continue
        filtered.append((dirn, frm, to, t, kind_label))

    seen = set()
    collapsed = []
    for dirn, frm, to, t, kind_label in filtered:
        if collapse_by == "pair":
            key = (frm, to)
        elif collapse_by == "from_func":
            key = (
                ida_funcs.get_func(frm).start_ea if ida_funcs.get_func(frm) else frm,
                to,
            )
        elif collapse_by == "to_func":
            key = (
                frm,
                ida_funcs.get_func(to).start_ea if ida_funcs.get_func(to) else to,
            )
        else:
            key = (dirn, frm, to, t, kind_label)
        if key in seen:
            continue
        seen.add(key)

        rec = {
            "from_ea": int(frm),
            "to_ea": int(to),
            "direction": dirn,
            "kind": kind_label,
            "type": int(t),
        }
        if enrich_names:
            rec["from_func"] = _ea_func_name(frm) or ""
            rec["to_func"] = _ea_func_name(to) or ""
        collapsed.append(rec)

    out["xrefs"] = collapsed
    out["stats"] = {
        "sites": len(filtered),
        "unique": len(collapsed),
    }
    out["filters"] = {
        "kind": kind,
        "only_calls": only_calls,
        "exclude_flow": exclude_flow,
        "collapse_by": collapse_by or "site",
    }
    return out


def _is_call(xref_type: int) -> bool:
    # IDA uses fl_* constants in ida_xref; calls are typically fl_CN/fl_CF (near/far)
    return xref_type in (ida_xref.fl_CN, ida_xref.fl_CF)

# -----------------------------------------------------------------------------

def _is_flow(xref_type: int) -> bool:
    # Jumps/flow: near/far jump & ordinary flow
    return xref_type in (ida_xref.fl_JN, ida_xref.fl_JF, ida_xref.fl_F)

# -----------------------------------------------------------------------------

def _iter_func_items(fn: ida_funcs.func_t) -> Iterable[int]:
    ea = fn.start_ea
    while ea < fn.end_ea:
        yield ea
        ea = ida_bytes.get_item_end(ea)

# -----------------------------------------------------------------------------

def _collect_xrefs_to(ea: int, kinds_mask=ida_xref.XREF_ALL) -> Iterable[tuple[int, int, bool]]:
    blk = ida_xref.xrefblk_t()
    if blk.first_to(ea, kinds_mask):
        while True:
            yield (blk.frm, blk.type, bool(blk.iscode))
            if not blk.next_to():
                break

# -----------------------------------------------------------------------------

def _collect_xrefs_from(ea: int, kinds_mask=ida_xref.XREF_ALL) -> Iterable[tuple[int, int, bool]]:
    blk = ida_xref.xrefblk_t()
    if blk.first_from(ea, kinds_mask):
        while True:
            yield (blk.to, blk.type, bool(blk.iscode))
            if not blk.next_from():
                break

# -----------------------------------------------------------------------------

def _ea_func_name(ea: int) -> str | None:
    f = ida_funcs.get_func(ea)
    if f:
        return get_func_name(f) or None
    name = ida_name.get_ea_name(ea)
    return name or None

# -----------------------------------------------------------------------------

def _normalize_kind(kind_value: str | None) -> str:
    """Validate the requested xref kind filter."""
    if not kind_value:
        return "both"
    kind_text = str(kind_value).lower()
    if kind_text not in {"code", "data", "both"}:
        raise ValueError("kind must be 'code', 'data', or 'both'")
    return kind_text

# -----------------------------------------------------------------------------

def get_xrefs_unified(
        scope: str,
        subject: str,
        direction: str = "both",
        kind: str | None = "both",
        only_calls: bool = False,
        exclude_flow: bool = False,
        collapse_by: str | None = None,
        enrich_names: bool = True,
) -> dict[str, Any]:
    """Return cross-reference information for the requested scope."""
    if direction not in {"to", "from", "both"}:
        raise ValueError("direction must be 'to'|'from'|'both'")
    if scope not in {"ea", "function", "name"}:
        raise ValueError("scope must be 'ea'|'function'|'name'")

    normalized_kind = _normalize_kind(kind)
    return _gather_xrefs(
        scope=scope,
        subject=subject,
        direction=direction,
        kind=normalized_kind,
        only_calls=only_calls,
        exclude_flow=exclude_flow,
        collapse_by=collapse_by,
        enrich_names=enrich_names,
    )

```

`gepetto/ida/tools/list_functions.py`:

```py
import json

import ida_funcs
import ida_name
import idautils
import idaapi

from gepetto.ida.utils.function_helpers import get_func_name
from gepetto.ida.tools.tools import (
    add_result_to_messages,
    tool_error_payload,
    tool_result_payload,
)
from gepetto.ida.utils.thread_helpers import ida_read



def handle_list_functions_tc(tc, messages):
    """Handle a tool call to enumerate functions."""
    try:
        args = json.loads(getattr(tc.function, "arguments", "") or "{}")
    except Exception:
        args = {}

    limit = int(args.get("limit", 256))
    offset = int(args.get("offset", 0))
    include_thunks = bool(args.get("include_thunks", True))

    try:
        data = list_functions(limit=limit, offset=offset, include_thunks=include_thunks)
        payload = tool_result_payload(data)
    except Exception as ex:
        payload = tool_error_payload(
            str(ex),
            limit=limit,
            offset=offset,
            include_thunks=include_thunks,
        )

    add_result_to_messages(messages, tc, payload)


# -----------------------------------------------------------------------------


@ida_read
def _enumerate_functions(include_thunks: bool) -> list[dict[str, object]]:
    """Collect functions on the UI thread with optional thunk filtering."""

    idaapi.auto_wait()
    thunk_flag = getattr(ida_funcs, "FUNC_THUNK", 0)
    funcs: list[dict[str, object]] = []

    for ea in idautils.Functions():
        fn = ida_funcs.get_func(ea)
        if not fn:
            continue
        if not include_thunks and thunk_flag and (fn.flags & thunk_flag):
            continue
        name = get_func_name(fn) or ida_name.get_ea_name(fn.start_ea) or ""
        funcs.append({"ea": int(fn.start_ea), "name": name})

    return funcs


def list_functions(
    limit: int = 256,
    offset: int = 0,
    include_thunks: bool = True,
) -> dict[str, object]:
    """Return paginated functions from the IDA database."""
    if limit <= 0:
        raise ValueError("limit must be a positive integer")
    if offset < 0:
        raise ValueError("offset must be non-negative")

    funcs = _enumerate_functions(include_thunks=include_thunks)

    total = len(funcs)
    start = min(offset, total)
    end = min(start + limit, total)
    items = funcs[start:end]
    next_offset = end if end < total else None

    return {
        "total": total,
        "next_offset": next_offset,
        "items": items,
    }


```

`gepetto/ida/tools/list_imports.py`:

```py
import json

import ida_nalt
import idaapi

from gepetto.ida.tools.tools import (
    add_result_to_messages,
    tool_error_payload,
    tool_result_payload,
)
from gepetto.ida.utils.thread_helpers import ida_read



def handle_list_imports_tc(tc, messages):
    """Handle a tool call to enumerate imports."""
    try:
        args = json.loads(getattr(tc.function, "arguments", "") or "{}")
    except Exception:
        args = {}

    limit = int(args.get("limit", 256))
    offset = int(args.get("offset", 0))
    module_filter = args.get("module_filter")

    try:
        data = list_imports(limit=limit, offset=offset, module_filter=module_filter)
        payload = tool_result_payload(data)
    except Exception as ex:
        payload = tool_error_payload(
            str(ex),
            limit=limit,
            offset=offset,
            module_filter=module_filter,
        )

    add_result_to_messages(messages, tc, payload)


# -----------------------------------------------------------------------------


@ida_read
def _collect_imports() -> list[dict[str, object]]:
    """Snapshot the import table on the IDA UI thread."""

    idaapi.auto_wait()
    results: list[dict[str, object]] = []
    qty = ida_nalt.get_import_module_qty()

    for mod_idx in range(qty):
        module = ida_nalt.get_import_module_name(mod_idx) or ""

        def _cb(ea, name, ordinal):
            results.append(
                {
                    "ea": int(ea),
                    "name": name or f"ord_{ordinal}",
                    "module": module,
                }
            )
            return 1

        ida_nalt.enum_import_names(mod_idx, _cb)

    return results




def list_imports(
    limit: int = 256,
    offset: int = 0,
    module_filter: str | None = None,
) -> dict[str, object]:
    """Return paginated import entries with optional module filtering."""
    if limit <= 0:
        raise ValueError("limit must be a positive integer")
    if offset < 0:
        raise ValueError("offset must be non-negative")

    imports = _collect_imports()
    filter_norm = (module_filter or "").strip().casefold()
    if filter_norm:
        imports = [item for item in imports if filter_norm in (item["module"] or "").casefold()]

    total = len(imports)
    start = min(offset, total)
    end = min(start + limit, total)
    items = imports[start:end]
    next_offset = end if end < total else None

    return {
        "total": total,
        "next_offset": next_offset,
        "items": items,
    }

```

`gepetto/ida/tools/list_symbols.py`:

```py
import json

import ida_funcs
import ida_name
import idautils

from gepetto.ida.tools.tools import (
    add_result_to_messages,
    tool_error_payload,
    tool_result_payload,
)
from gepetto.ida.utils.thread_helpers import ida_read



def handle_list_symbols_tc(tc, messages):
    """Handle a tool call to list symbols."""
    try:
        args = json.loads(getattr(tc.function, "arguments", "") or "{}")
    except Exception:
        args = {}

    prefix = args.get("prefix") or ""
    include_globals = bool(args.get("include_globals", False))

    try:
        data = list_symbols(prefix=prefix, include_globals=include_globals)
        payload = tool_result_payload(data)
    except Exception as ex:
        payload = tool_error_payload(str(ex), prefix=prefix, include_globals=include_globals)

    add_result_to_messages(messages, tc, payload)


# -----------------------------------------------------------------------------


@ida_read
def _enumerate_symbols(prefix: str, include_globals: bool) -> list[dict[str, object]]:
    pref = prefix or ""
    results: list[dict[str, object]] = []

    for ea in idautils.Functions():
        name = ida_funcs.get_func_name(ea) or ida_name.get_ea_name(ea) or ""
        if pref and not name.startswith(pref):
            continue
        results.append({"name": name, "ea": int(ea), "type": "function"})

    if include_globals:
        for ea, name in idautils.Names():
            if ida_funcs.get_func(ea):
                continue
            if pref and not name.startswith(pref):
                continue
            results.append({"name": name, "ea": int(ea), "type": "global"})

    return results


def list_symbols(prefix: str = "", include_globals: bool = False) -> dict:
    """Return names and EAs for functions and (optionally) global symbols."""
    symbols = _enumerate_symbols(prefix=prefix, include_globals=include_globals)
    return {"symbols": symbols}

```

`gepetto/ida/tools/refresh_view.py`:

```py
import json

import ida_kernwin

from gepetto.ida.tools.tools import (
    add_result_to_messages,
    tool_error_payload,
    tool_result_payload,
)
from gepetto.ida.utils.thread_helpers import ida_read


def handle_refresh_view_tc(tc, messages):
    """Handle a tool call to refresh the current IDA view."""
    # The tool takes no arguments but parse for forward compatibility.
    _ = json.loads(tc.function.arguments or "{}")

    try:
        data = refresh_view()
        payload = tool_result_payload(data)
    except Exception as ex:
        payload = tool_error_payload(str(ex))

    add_result_to_messages(messages, tc, payload)

# -----------------------------------------------------------------------------

@ida_read
def refresh_view() -> dict:
    """Refresh the current IDA disassembly view."""

    error: dict[str, str | None] = {"message": None}

    try:
        ida_kernwin.refresh_idaview_anyway()
    except Exception as e:
        error["message"] = str(e)

    if error["message"]:
        raise RuntimeError(error["message"])
    return {"status": "refreshed"}

```

`gepetto/ida/tools/rename_function.py`:

```py
import json
import re

import ida_name

from gepetto.ida.utils.function_helpers import parse_ea, resolve_ea, resolve_func, get_func_name
from gepetto.ida.utils.thread_helpers import ida_write
from gepetto.ida.tools.tools import (
    add_result_to_messages,
    tool_error_payload,
    tool_result_payload,
)

def _sanitize_identifier(candidate: str) -> str:
    """Convert arbitrary text into an IDA-safe identifier."""
    cleaned = re.sub(r"\W+", "_", candidate.strip())
    if not cleaned:
        cleaned = "func"
    if cleaned[0].isdigit():
        cleaned = f"_{cleaned}"
    return cleaned[: ida_name.MAXNAMELEN]


def handle_rename_function_tc(tc, messages):
    """Handle a tool call to rename a function."""
    try:
        args = json.loads(tc.function.arguments or "{}")
    except Exception:
        args = {}

    ea = args.get("ea")
    if ea is not None:
        ea = parse_ea(ea)
    name = args.get("name")
    new_name = args.get("new_name")

    try:
        data = rename_function(ea=ea, name=name, new_name=new_name)
        payload = tool_result_payload(data)
    except Exception as ex:
        payload = tool_error_payload(str(ex), ea=ea, name=name, new_name=new_name)
    add_result_to_messages(messages, tc, payload)


# -----------------------------------------------------------------------------


def rename_function(
    ea: int | None = None,
    name: str | None = None,
    new_name: str | None = None,
) -> dict:
    """Rename a function by EA or name."""
    if not new_name:
        raise ValueError("new_name is required")

    f = resolve_func(ea=ea, name=name)
    old_name = name or get_func_name(f)
    ea = int(f.start_ea)

    applied_name = _apply_function_rename(ea, old_name, new_name)

    result = {"ea": ea, "old_name": old_name, "new_name": applied_name}
    if applied_name != new_name:
        result["requested_name"] = new_name
    return result


@ida_write
def _apply_function_rename(ea: int, old_name: str, desired_name: str) -> str:
    flags = ida_name.SN_FORCE | getattr(ida_name, "SN_NOWARN", 0)
    if ida_name.set_name(ea, desired_name, flags):
        return desired_name

    sanitized = _sanitize_identifier(desired_name)
    if sanitized != desired_name and ida_name.set_name(ea, sanitized, flags):
        return sanitized

    if sanitized != desired_name:
        raise RuntimeError(
            f"Failed to rename function {old_name!r} -> {desired_name!r} (sanitized {sanitized!r})"
        )
    raise RuntimeError(f"Failed to rename function {old_name!r} -> {desired_name!r}")

```

`gepetto/ida/tools/rename_global.py`:

```py
import json

import ida_name
import idaapi

from gepetto.ida.utils.function_helpers import parse_ea, resolve_ea
from gepetto.ida.utils.thread_helpers import ida_write
from gepetto.ida.tools.tools import (
    add_result_to_messages,
    tool_error_payload,
    tool_result_payload,
)


def handle_rename_global_tc(tc, messages):
    """Handle a tool call to rename a global (data) symbol."""
    try:
        args = json.loads(tc.function.arguments or "{}")
    except Exception:
        args = {}

    ea = args.get("ea")
    if ea is not None:
        ea = parse_ea(ea)

    old_name = args.get("old_name")
    new_name = args.get("new_name")
    force = args.get("force", False)

    try:
        data = rename_global(ea=ea, old_name=old_name, new_name=new_name, force=force)
        payload = tool_result_payload(data)
    except Exception as ex:
        payload = tool_error_payload(str(ex), ea=ea, old_name=old_name, new_name=new_name, force=force)

    add_result_to_messages(messages, tc, payload)


def rename_global(*, ea: int | None = None, old_name: str | None = None, new_name: str | None = None, force: bool = False) -> dict:
    if not new_name or not isinstance(new_name, str) or not new_name.strip():
        raise ValueError("new_name is required")
    new_name = new_name.strip()

    if ea is None:
        if not old_name or not isinstance(old_name, str) or not old_name.strip():
            raise ValueError("Provide either ea or old_name")
        ea = resolve_ea(old_name.strip())

    if ea == idaapi.BADADDR:
        raise ValueError("Could not resolve address")

    _apply_global_rename(ea, new_name, force=bool(force))
    return {"ea": int(ea), "new_name": new_name, "changed": True}



@ida_write
def _apply_global_rename(ea: int, new_name: str, *, force: bool) -> None:
    # Guardrails: make sure the data to rename is a symbol
    f = idaapi.get_func(ea)
    if f is not None and f.start_ea != idaapi.BADADDR:
        raise ValueError("EA is inside a function; use the function-rename tool instead")

    if idaapi.is_code(idaapi.get_flags(ea)):
        raise ValueError("EA appears to be code; refusing to rename as global data")

    flags = ida_name.SN_FORCE if force else ida_name.SN_CHECK
    if not ida_name.set_name(ea, new_name, flags):
        raise RuntimeError(
            f"Failed to rename global at 0x{ea:X} to {new_name!r} "
            f"(invalid name or collision; try force=true if appropriate)"
        )

```

`gepetto/ida/tools/rename_lvar.py`:

```py
import json

import ida_hexrays  # type: ignore

from gepetto.ida.utils.function_helpers import parse_ea, resolve_ea, resolve_func, get_func_name
from gepetto.ida.utils.thread_helpers import ida_write
from gepetto.ida.tools.tools import (
    add_result_to_messages,
    tool_error_payload,
    tool_result_payload,
)



def handle_rename_lvar_tc(tc, messages):
    """Handle a tool call to rename a local variable."""
    try:
        args = json.loads(tc.function.arguments or "{}")
    except Exception:
        args = {}

    ea = args.get("ea")
    if ea is not None:
        ea = parse_ea(ea)
    func_name = args.get("func_name")
    old_name = args.get("old_name")
    new_name = args.get("new_name")

    try:
        data = rename_lvar(ea=ea, func_name=func_name, old_name=old_name, new_name=new_name)
        payload = tool_result_payload(data)
    except Exception as ex:
        payload = tool_error_payload(
            str(ex),
            ea=ea,
            func_name=func_name,
            old_name=old_name,
            new_name=new_name,
        )

    add_result_to_messages(messages, tc, payload)


# -----------------------------------------------------------------------------

def rename_lvar(
    ea: int | None = None,
    func_name: str | None = None,
    old_name: str | None = None,
    new_name: str | None = None,
) -> dict:
    """Rename a local variable in a function."""
    if not old_name or not new_name:
        raise ValueError("old_name and new_name are required")

    f = resolve_func(ea=ea, name=func_name)
    func_name = func_name or get_func_name(f)
    if ea is None:
        ea = resolve_ea(func_name)

    result = {"ea": int(f.start_ea), "func_name": func_name, "old_name": old_name, "new_name": new_name}
    _apply_lvar_rename(ea, old_name, new_name)
    return result


@ida_write
def _apply_lvar_rename(ea: int, old_name: str, new_name: str) -> None:
    if not ida_hexrays.rename_lvar(ea, old_name, new_name):
        raise RuntimeError(f"Failed to rename lvar {old_name!r}")

```

`gepetto/ida/tools/run_python.py`:

```py
import io
import json
import traceback
from contextlib import redirect_stdout, redirect_stderr

from gepetto.ida.tools.tools import (
    add_result_to_messages,
    tool_error_payload,
    tool_result_payload,
)


def handle_run_python_tc(tc, messages):
    """
    Tool: run_python
    Args (JSON):
      - code: str (required)

    Returns (success):
      - stdout: str
      - stderr: str

    Returns (error):
      - traceback: str
    """
    try:
        args = json.loads(tc.function.arguments or "{}")
    except Exception:
        args = {}

    code = args.get("code")

    try:
        if not isinstance(code, str) or not code.strip():
            raise ValueError("code must be a non-empty string")

        payload = tool_result_payload(run_python(code))
    except Exception:
        payload = tool_error_payload(
            traceback.format_exc(),
            code=code,
        )

    add_result_to_messages(messages, tc, payload)


def run_python(code: str) -> dict:
    stdout_buf = io.StringIO()
    stderr_buf = io.StringIO()

    env = {"__builtins__": __builtins__}

    try:
        with redirect_stdout(stdout_buf), redirect_stderr(stderr_buf):
            exec(code, env, env)  # intentional arbitrary execution

        return {
            "stdout": stdout_buf.getvalue(),
            "stderr": stderr_buf.getvalue(),
        }
    except Exception:
        # Always return traceback, never structured exception info
        raise RuntimeError(traceback.format_exc())

```

`gepetto/ida/tools/search.py`:

```py
import json
import hashlib
import ida_bytes
import ida_idaapi
import ida_kernwin
import ida_nalt
import ida_segment
import ida_search
import ida_funcs
import ida_name
import ida_xref
import idautils
import idaapi
import ida_strlist

from gepetto.ida.tools.tools import (
    add_result_to_messages,
    tool_error_payload,
    tool_result_payload,
)
from gepetto.ida.utils.thread_helpers import ida_read, ida_write, run_on_main_thread


# -----------------------------------------------------------------------------
# Tool call handlers
# -----------------------------------------------------------------------------

def handle_search_tc(tc, messages):
    """
    Handler for the 'search' tool. Searches for a specific text or hex pattern
    and returns matching EAs. (Targeted queries, not enumeration.)
    """
    try:
        args = json.loads(getattr(tc.function, "arguments", "") or "{}")
    except Exception:
        args = {}

    text = args.get("text")
    hex_pattern = args.get("hex")
    case_sensitive = bool(args.get("case_sensitive", False))

    try:
        data = search(text=text, hex=hex_pattern, case_sensitive=case_sensitive)
        payload = tool_result_payload(data)
    except Exception as ex:
        payload = tool_error_payload(str(ex), text=text, hex=hex_pattern)

    add_result_to_messages(messages, tc, payload)

# -----------------------------------------------------------------------------

def handle_list_strings_tc(tc, messages):
    """
    Handler for the 'list_strings' tool. Enumerates discovered strings with
    pagination and filters.
    """
    try:
        args = json.loads(getattr(tc.function, "arguments", "") or "{}")
    except Exception:
        args = {}

    try:
        data = list_strings(
            limit=int(args.get("limit", 200)),
            offset=int(args.get("offset", 0)),
            min_len=int(args.get("min_len", 4)),
            encodings=args.get("encodings"),
            segments=args.get("segments"),
            include_xrefs=bool(args.get("include_xrefs", False)),
            include_text=bool(args.get("include_text", True)),
            max_text_bytes=int(args.get("max_text_bytes", 256)),
            return_addresses_only=bool(args.get("return_addresses_only", False)),
            sort_by=(args.get("sort_by") or "ea").lower(),
        )
        payload = tool_result_payload(data)
    except Exception as ex:
        payload = tool_error_payload(str(ex))

    add_result_to_messages(messages, tc, payload)

# -----------------------------------------------------------------------------
# Shared snapshot utilities
# -----------------------------------------------------------------------------

@ida_write
def _snapshot_strings_and_segments():
    """Runs on UI thread: safely snapshot strings and segments for background use.

    Notes:
      - Executed via run_on_main_thread(..., write=True) because build_strlist()
        mutates IDA state.
      - Always returns a 2-tuple: (strings: list[dict], segs: list[tuple[int,int]]).
    """
    try:
        # Rebuild the Strings list so we don't iterate a stale/empty view.
        try:
            ida_strlist.build_strlist()
        except Exception:
            pass  # best-effort; continue even if rebuild fails

        string_view = idautils.Strings()
        string_view.refresh()  # pick up the freshly-built list

        strings = []
        for si in string_view:
            ea = int(si.ea)
            text = str(si)
            stype = int(si.strtype)  # IDA string type code
            seg = ida_segment.getseg(ea)
            seg_name = ida_segment.get_segm_name(seg) if seg else ""
            strings.append({
                "ea": ea,
                "text": text,
                "len": len(text),
                "stype": stype,
                "segment": seg_name or "",
            })

        segs = []
        for s in idautils.Segments():
            try:
                seg = ida_segment.getseg(s)
                if seg:
                    segs.append((int(seg.start_ea), int(seg.end_ea)))
            except Exception:
                continue

        return strings, segs

    except Exception:
        raise

# -----------------------------------------------------------------------------

def _ui_snapshot_wrapper():
    """Executes snapshot on the UI thread and returns a dict with 'strings' and 'segs'."""
    res = _snapshot_strings_and_segments()
    if not isinstance(res, tuple) or len(res) != 2:
        res = ([], [])
    strings, segs = res
    return {"strings": strings, "segs": segs}

# -----------------------------------------------------------------------------

def _strtype_to_label(stype: int) -> str:
    """Map IDA strtype enum to a coarse encoding label."""
    if stype == getattr(ida_nalt, "STRTYPE_C_16", -1):
        return "utf16"
    if stype == getattr(ida_nalt, "STRTYPE_C_32", -1):
        return "utf32"
    if stype == getattr(ida_nalt, "STRTYPE_C", -1):
        return "ascii"
    if stype == getattr(ida_nalt, "STRTYPE_PASCAL", -1):
        return "ascii"
    # fallback: most strings are 1-byte
    return "ascii"


# -----------------------------------------------------------------------------

@ida_read
def _iter_xrefs_to(ea: int, max_items: int = 64):
    """Best-effort, bounded xref collection."""
    out = []
    xb = ida_xref.xrefblk_t()
    if xb.first_to(ea, ida_xref.XREF_FAR):
        count = 0
        while True:
            out.append(int(xb.frm))
            count += 1
            if count >= max_items or not xb.next_to():
                break
    return out

# -----------------------------------------------------------------------------
# Public tool implementations
# -----------------------------------------------------------------------------

def search(text: str | None = None, hex: str | None = None, case_sensitive: bool = False) -> dict:
    """Targeted search for text or hex patterns. Returns matching EAs."""

    if not text and not hex:
        raise ValueError("Either text or hex must be provided")
    if text and hex:
        raise ValueError("Provide either text or hex, not both")

    snap = _ui_snapshot_wrapper()
    strings = snap.get("strings", [])
    segs = snap.get("segs", [])

    matches: list[int] = []

    # Text search: operate on the pre-extracted string list (no UI calls).
    if text:
        q = text if case_sensitive else text.casefold()
        for x in strings:
            hay = x["text"] if case_sensitive else x["text"].casefold()
            if q in hay:
                matches.append(int(x["ea"]))

    # Hex search: segment-bounded search to avoid scanning the whole VA space.
    if hex:
        def _scan_hex():
            hex_matches: list[int] = []
            if idaapi.IDA_SDK_VERSION < 900:
                for (start, end) in segs:
                    ea = ida_search.find_binary(start, end, hex, 16, ida_search.SEARCH_DOWN)
                    while ea != ida_idaapi.BADADDR and ea < end:
                        hex_matches.append(int(ea))
                        ea = ida_search.find_binary(ea + 1, end, hex, 16, ida_search.SEARCH_DOWN)
                    # Keep UI responsive when the helper runs inside IDA.
                    process = getattr(ida_kernwin, "process_ui_events", None)
                    if callable(process):
                        try:
                            process()
                        except Exception:
                            pass
            else:
                # IDA 9.x port: ida_search.find_binary removed; use ida_bytes.find_bytes
                # (see 'ida_search' removed functions, and 'ida_bytes' added functions
                # in the porting guide).
                flags = ida_bytes.BIN_SEARCH_FORWARD | ida_bytes.BIN_SEARCH_NOSHOW
                for (start, end) in segs:
                    ea = ida_bytes.find_bytes(
                        hex,
                        start,
                        range_end=end,
                        flags=flags,
                        radix=16,
                    )
                    while ea != ida_idaapi.BADADDR and ea < end:
                        hex_matches.append(int(ea))
                        ea = ida_bytes.find_bytes(
                            hex,
                            ea + 1,
                            range_end=end,
                            flags=flags,
                            radix=16,
                        )
            return hex_matches

        matches.extend(run_on_main_thread(_scan_hex, write=False) or [])

    if matches:
        matches = sorted(set(matches))

    return {"eas": matches}

# -----------------------------------------------------------------------------

def list_strings(
        limit: int = 200,
        offset: int = 0,
        min_len: int = 4,
        encodings: list[str] | None = None,
        segments: list[str] | None = None,
        include_xrefs: bool = False,
        include_text: bool = True,
        max_text_bytes: int = 256,
        return_addresses_only: bool = False,
        sort_by: str = "ea",
) -> dict:
    """Enumerate discovered strings with pagination and filters."""

    snap = _ui_snapshot_wrapper()
    items = snap.get("strings", [])

    # Filters
    if min_len > 1:
        items = [x for x in items if x["len"] >= min_len]

    if encodings:
        encset = {e.lower() for e in encodings}
        items = [x for x in items if _strtype_to_label(x["stype"]) in encset]

    if segments:
        segset = {s.lower() for s in segments}
        items = [x for x in items if (x["segment"].lower() in segset)]

    # Sorting
    if sort_by == "len":
        items.sort(key=lambda x: (x["len"], x["ea"]))
    elif sort_by == "segment":
        items.sort(key=lambda x: (x["segment"], x["ea"]))
    else:
        items.sort(key=lambda x: x["ea"])

    total = len(items)

    # Pagination
    start = max(0, int(offset))
    end = min(total, start + max(1, int(limit)))
    page = items[start:end]

    next_offset = end if end < total else None

    # Build payload
    results = []
    for x in page:
        if return_addresses_only:
            results.append(int(x["ea"]))
            continue

        entry = {
            "ea": int(x["ea"]),
            "len": int(x["len"]),
            "segment": x["segment"],
            "encoding": _strtype_to_label(x["stype"]),
        }

        if include_text:
            # Truncate text to keep payload small and provide a digest for dedup.
            t = x["text"]
            clipped = t.encode(errors="replace")[:max_text_bytes]
            try:
                clipped_text = clipped.decode(errors="replace")
            except Exception:
                clipped_text = t[:max_text_bytes]
            entry["text"] = clipped_text
            entry["text_truncated"] = (len(t.encode(errors="replace")) > len(clipped))
            entry["sha1"] = hashlib.sha1(t.encode(errors="replace")).hexdigest()

        if include_xrefs:
            entry["xrefs_to"] = _iter_xrefs_to(x["ea"])

        results.append(entry)

    return {
        "total": total,
        "next_offset": next_offset,
        "items": results,
    }

```

`gepetto/ida/tools/set_comment.py`:

```py
import json

import idc


from gepetto.ida.utils.function_helpers import parse_ea
from gepetto.ida.tools.tools import (
    add_result_to_messages,
    tool_error_payload,
    tool_result_payload,
)
from gepetto.ida.utils.thread_helpers import ida_write

def handle_set_comment_tc(tc, messages):
    """Handle a tool call to apply a comment."""
    try:
        args = json.loads(getattr(tc.function, "arguments", "") or "{}")
    except Exception:
        args = {}

    ea_arg = args.get("ea")
    comment = args.get("comment")

    try:
        ea = parse_ea(ea_arg)
        data = set_comment(ea=ea, comment=comment)
        payload = tool_result_payload(data)
    except Exception as ex:
        payload = tool_error_payload(str(ex), ea=ea_arg, comment=comment)

    add_result_to_messages(messages, tc, payload)


# -----------------------------------------------------------------------------


@ida_write
def _apply_comment(ea: int, comment: str) -> None:
    if not idc.set_func_cmt(ea, comment, False):
        raise RuntimeError("Failed to set comment")


def set_comment(ea: int, comment: str | None) -> dict[str, object]:
    """Set a non-repeatable comment at the given EA."""
    if comment is None:
        raise ValueError("comment is required")

    normalized = comment.rstrip("\r\n")
    _apply_comment(ea, normalized)
    return {"ok": True, "ea": ea}

```

`gepetto/ida/tools/to_hex.py`:

```py
"""Tool to convert decimal integers to hexadecimal strings."""

import json

from gepetto.ida.tools.tools import (
    add_result_to_messages,
    tool_error_payload,
    tool_result_payload,
)


def handle_to_hex_tc(tc, messages):
    """Handle a `to_hex` tool call."""
    try:
        args = json.loads(tc.function.arguments or "{}")
    except Exception:
        args = {}

    value = args.get("value")

    try:
        hex_value = to_hex(value)
        payload = tool_result_payload({"hex": hex_value})
    except Exception as ex:
        payload = tool_error_payload(str(ex), value=value)

    add_result_to_messages(messages, tc, payload)


def to_hex(value) -> str:
    """Return the hexadecimal string for a decimal integer."""
    return hex(int(value))


```

`gepetto/ida/tools/tools.py`:

```py
import json
from typing import Any


TOOLS = [
    {
        "type": "function",
        "function": {
            "name": "get_screen_ea",
            "description": "Return the current effective address (EA).",
            "parameters": {
                "type": "object",
                "properties": {},
            },
        },
    },
    {
        "type": "function",
        "function": {
            "name": "get_current_function",
            "description": "Return the current function under the cursor.",
            "parameters": {
                "type": "object",
                "properties": {},
            },
        },
    },
    {
        "type": "function",
        "function": {
            "name": "get_ea",
            "description": "Return EA for a symbol name.",
            "parameters": {
                "type": "object",
                "properties": {
                    "name": {
                        "type": "string",
                        "description": "Symbol or function name.",
                    },
                },
                "required": ["name"],
            },
        },
    },
    {
        "type": "function",
        "function": {
            "name": "to_hex",
            "description": "Convert a decimal integer to a hexadecimal string (returns {\"hex\": \"0x...\"}).",
            "parameters": {
                "type": "object",
                "properties": {
                    "value": {
                        "type": "integer",
                        "description": "Decimal integer to convert.",
                    },
                },
                "required": ["value"],
            },
        },
    },
    {
        "type": "function",
        "function": {
            "name": "get_disasm",
            "description": "Return disassembly for an effective address.",
            "parameters": {
                "type": "object",
                "properties": {
                    "ea": {
                        "type": "string",
                        "description": "EA (int or hex string) to disassemble.",
                    },
                },
                "required": ["ea"],
            },
        },
    },
    {
        "type": "function",
        "function": {
            "name": "disasm_function",
            "description": "Return disassembly for an entire function. Provide either `ea` or `name`.",
            "parameters": {
                "type": "object",
                "properties": {
                    "ea": {
                        "type": "string",
                        "description": "EA (int or hex string) inside the target function.",
                    },
                    "name": {
                        "type": "string",
                        "description": "Function name to resolve if no EA is supplied.",
                    },
                },
            },
        },
    },
    {
        "type": "function",
        "function": {
            "name": "get_bytes",
            "description": "Return raw bytes for an effective address. If the address is an offset/pointer, the tool may automatically dereference it and return the pointed data instead.",
            "parameters": {
                "type": "object",
                "properties": {
                    "ea": {
                        "type": "string",
                        "description": "EA (integer or hex string) to read from."
                    },
                    "size": {
                        "type": "integer",
                        "description": "Maximum number of bytes to retrieve starting at the address (also caps string decoding).",
                        "default": 32,
                        "minimum": 1
                    },
                    "auto_dereference": {
                        "type": "boolean",
                        "description": "If true (default), automatically dereference the address when it is marked as an offset by IDA or clearly looks like a pointer.",
                        "default": True
                    }
                },
                "required": ["ea"]
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "decompile_function",
            "description": "Decompile a function by EA or name and return annotated pseudocode (with per-line metadata). Provide either `ea` or `name`.",
            "parameters": {
                "type": "object",
                "properties": {
                    "ea": {
                        "type": "string",
                        "description": "EA (int or hex string) inside the target function.",
                    },
                    "name": {
                        "type": "string",
                        "description": "Function name to resolve if no EA is supplied.",
                    },
                },
            },
        },
    },
    {
        "type": "function",
        "function": {
            "name": "rename_lvar",
            "description": "Rename a local variable within a function.",
            "parameters": {
                "type": "object",
                "properties": {
                    "ea": {
                        "type": "string",
                        "description": "EA (int or hex string) inside the target function.",
                    },
                    "func_name": {
                        "type": "string",
                        "description": "Name of the function if EA is not provided.",
                    },
                    "old_name": {
                        "type": "string",
                        "description": "Current local variable name to be changed.",
                    },
                    "new_name": {
                        "type": "string",
                        "description": "Desired new name for the local variable.",
                    },
                },
                "required": ["new_name", "old_name"],
            },
        },
    },
    {
        "type": "function",
        "function": {
            "name": "rename_function",
            "description": "Rename a function.",
            "parameters": {
                "type": "object",
                "properties": {
                    "ea": {
                        "type": "string",
                        "description": "EA (int or hex string) inside the target function.",
                    },
                    "name": {
                        "type": "string",
                        "description": "Existing function name if EA is not provided.",
                    },
                    "new_name": {
                        "type": "string",
                        "description": "Desired new name for the function.",
                    },
                },
                "required": ["new_name"],
            },
        },
    },
    {
        "type": "function",
        "function": {
            "name": "set_comment",
            "description": "Set a comment for the function containing the given EA. Supports multiline input.",
            "parameters": {
                "type": "object",
                "properties": {
                    "ea": {
                        "type": "string",
                        "description": "EA (int or hex string) of a function to which the comment should be applied.",
                    },
                    "comment": {
                        "type": "string",
                        "description": "Comment text to store for the function.",
                    },
                },
                "required": ["ea", "comment"],
            },
        },
    },
    {
        "type": "function",
        "function": {
            "name": "get_xrefs",
                "description": (
                    "Return cross-references (code/data) for an address, a whole function, "
                    "or a named symbol. Supports incoming, outgoing, or both directions, "
                    "with practical filters (kind/only_calls/exclude_flow) and "
                "deduping (collapse_by)."
            ),
            "parameters": {
                "type": "object",
                "properties": {
                    "scope": {
                        "type": "string",
                        "description": "Scope of the query: single EA, the whole function, or a name.",
                        "enum": ["ea", "function", "name"],
                        "default": "ea"
                    },
                    "subject": {
                        "type": "string",
                        "description": (
                            "Subject to inspect. If scope=='ea' or 'function', this may be an EA "
                            "as decimal or hex string ('0x401000', '401000h'). "
                            "If scope=='name', this must be a symbol name."
                        )
                    },
                    "direction": {
                        "type": "string",
                        "description": "Which direction of xrefs to return.",
                        "enum": ["to", "from", "both"],
                        "default": "both"
                    },
                    "kind": {
                        "type": "string",
                        "description": "Limit results to code, data, or both kinds of xrefs.",
                        "enum": ["code", "data", "both"],
                        "default": "both"
                    },
                    "only_calls": {
                        "type": "boolean",
                        "description": "For code xrefs, keep only call sites.",
                        "default": False
                    },
                    "exclude_flow": {
                        "type": "boolean",
                        "description": "Exclude simple flow xrefs (falls-through/jumps).",
                        "default": False
                    },
                    "collapse_by": {
                        "type": "string",
                        "description": (
                            "Dedup granularity: 'site' (no dedup), 'pair' (from_ea→to_ea), "
                            "'from_func' (collapse by caller function), or 'to_func' (callee function)."
                        ),
                        "enum": ["site", "pair", "from_func", "to_func"],
                        "default": "site"
                    },
                    "enrich_names": {
                        "type": "boolean",
                        "description": "Add best-effort names for endpoints (functions/data).",
                        "default": True
                    },
                },
                "required": []
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "list_imports",
            "description": "Enumerate imported functions; supports pagination and module filtering.",
            "parameters": {
                "type": "object",
                "properties": {
                    "limit": {"type": "integer", "default": 256, "minimum": 1},
                    "offset": {"type": "integer", "default": 0, "minimum": 0},
                    "module_filter": {
                        "type": "string",
                        "description": "Substring to match import module name (case-insensitive).",
                    },
                },
                "required": [],
            },
        },
    },
    {
        "type": "function",
        "function": {
            "name": "list_functions",
            "description": "Paginated function enumeration with optional thunk filtering.",
            "parameters": {
                "type": "object",
                "properties": {
                    "limit": {"type": "integer", "default": 256, "minimum": 1},
                    "offset": {"type": "integer", "default": 0, "minimum": 0},
                    "include_thunks": {"type": "boolean", "default": True},
                },
                "required": [],
            },
        },
    },
    {
        "type": "function",
        "function": {
            "name": "list_symbols",
            "description": (
                "Return names and EAs for functions, optionally including globals. "
                "Supports prefix filtering."
            ),
            "parameters": {
                "type": "object",
                "properties": {
                    "prefix": {
                        "type": "string",
                        "description": "Only include symbols whose name starts with this prefix.",
                    },
                    "include_globals": {
                        "type": "boolean",
                        "description": "Include global (non-function) symbols.",
                        "default": False,
                    },
                },
                "required": [],
            },
        },
    },
    {
        "type": "function",
        "function": {
            "name": "search",
            "description": "Search the binary for specific text strings or hex byte patterns. Returns the addresses (EAs) where matches were found. For enumerating all strings, use the `list_strings` function instead.",
            "parameters": {
                "type": "object",
                "properties": {
                    "text": {
                        "type": "string",
                        "description": "Text to search for (ASCII/Unicode, case-insensitive by default)."
                    },
                    "hex": {
                        "type": "string",
                        "description": "Hex byte pattern like '90 90 ?? FF'."
                    },
                    "case_sensitive": {
                        "type": "boolean",
                        "description": "Whether the text search should be case-sensitive.",
                        "default": False
                    }
                },
                "required": []
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "list_strings",
            "description": "Enumerate discovered strings with pagination and filters.",
            "parameters": {
                "type": "object",
                "properties": {
                    "limit": { "type": "integer", "default": 200, "minimum": 1 },
                    "offset": { "type": "integer", "default": 0, "minimum": 0 },
                    "min_len": { "type": "integer", "default": 4, "minimum": 1 },
                    "encodings": {
                        "type": "array",
                        "items": { "type": "string", "enum": ["ascii","utf8","utf16","utf32"] }
                    },
                    "segments": {
                        "type": "array",
                        "description": "Optional segment names to restrict enumeration (e.g., ['.text', '.rdata']).",
                        "items": { "type": "string" }
                    },
                    "include_xrefs": { "type": "boolean", "default": False },
                    "include_text": { "type": "boolean", "default": True },
                    "max_text_bytes": { "type": "integer", "default": 256, "minimum": 1 },
                    "return_addresses_only": { "type": "boolean", "default": False },
                    "sort_by": {
                        "type": "string",
                        "enum": ["ea","len","segment"],
                        "default": "ea"
                    }
                },
                "required": []
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "declare_c_type",
            "description": "Parse and declare C types into the local type library.",
            "parameters": {
                "type": "object",
                "properties": {
                    "c_declaration": {
                        "type": "string",
                        "description": "C declaration(s) to add to the Local Types view.",
                    },
                },
                "required": ["c_declaration"],
            },
        },
    },
    {
        "type": "function",
        "function": {
            "name": "get_struct",
            "description": "Return structure fields and metadata by name.",
            "parameters": {
                "type": "object",
                "properties": {
                    "name": {
                        "type": "string",
                        "description": "Structure name to query from the Local Types view.",
                    },
                },
                "required": ["name"],
            },
        },
    },
    {
        "type": "function",
        "function": {
            "name": "refresh_view",
            "description": "Force IDA to repaint views after renames or patches.",
            "parameters": {
                "type": "object",
                "properties": {},
            },
        },
    },
    {
        "type": "function",
        "function": {
            "name": "run_python",
            "description": (
                "Execute arbitrary Python code and return its stdout and stderr. "
                "The code is executed in a fresh environment with no persistence "
                "between calls. Use this tool to decode and decrypt as needed, assume "
                "pycryptodome is present in the environment."
            ),
            "parameters": {
                "type": "object",
                "properties": {
                    "code": {
                        "type": "string",
                        "description": "Python source code to execute.",
                    },
                },
                "required": ["code"],
            },
        },
    },
    {
        "type": "function",
        "function": {
            "name": "rename_global",
            "description": (
                "Rename a global (data) symbol such as unk_401234, dword_40ABCD, qword_*, off_* "
                "at a given address. Refuses to rename anything inside a function (use the dedicated "
                "function renaming tool for that). Provide either `ea` or `old_name`."
            ),
            "parameters": {
                "type": "object",
                "properties": {
                    "ea": {
                        "type": "string",
                        "description": "EA (int or hex string) of the global/data item to rename.",
                    },
                    "old_name": {
                        "type": "string",
                        "description": "Existing global symbol name to resolve if EA is not provided.",
                    },
                    "new_name": {
                        "type": "string",
                        "description": "Desired new name for the global/data symbol.",
                    },
                    "force": {
                        "type": "boolean",
                        "description": "If true, overwrite existing name on collision (use sparingly). Default false.",
                    },
                },
                "required": ["new_name"],
            },
        },
    },

]

def tool_result_payload(data: Any) -> dict[str, Any]:
    """Wrap successful tool results in a standard payload structure."""

    return {"type": "result", "data": data}


def tool_error_payload(message: str, **context: Any) -> dict[str, Any]:
    """Create an error payload with an optional context dictionary."""

    error: dict[str, Any] = {"message": message}
    if context:
        error["context"] = context
    return {"type": "error", "error": error}


def add_result_to_messages(messages, tc, result):
    tc_id = getattr(tc, "id", None) or tc.get("id")
    fn_name = getattr(getattr(tc, "function", None), "name", None) \
              or (tc.get("function") or {}).get("name", "get_xrefs")
    messages.append(
        {
            "role": "tool",
            "tool_call_id": tc_id,
            "name": fn_name,
            "content": json.dumps(result, ensure_ascii=False),
        }
    )

```

`gepetto/ida/ui.py`:

```py
import functools
import random
import string
import threading

import idaapi  # type: ignore
import ida_kernwin  # type: ignore
import ida_hexrays  # type: ignore

import gepetto.config
from gepetto.ida.handlers import (
    ExplainHandler,
    GenerateCCodeHandler,
    GeneratePythonCodeHandler,
    RenameHandler,
    SwapModelHandler,
)
from gepetto.ida.comment_handler import CommentHandler
from gepetto.ida.cli import register_cli
from gepetto.ida.status_panel.status_panel_factory import get_status_panel
import gepetto.models.model_manager

_ = gepetto.config._


PLUGIN_INSTANCE: "GepettoPlugin | None" = None


def get_plugin_instance() -> "GepettoPlugin | None":
    return PLUGIN_INSTANCE


def trigger_model_select_menu_regeneration() -> None:
    plugin = get_plugin_instance()
    if not plugin:
        return
    try:
        plugin.generate_model_select_menu()
    except Exception:
        pass


def _safe_execute_sync(callback):
    try:
        ida_kernwin.execute_sync(callback, ida_kernwin.MFF_FAST)
    except Exception:
        pass


# =============================================================================
# Setup the menus, hotkeys and cli in IDA
# =============================================================================

class GepettoPlugin(idaapi.plugin_t):
    flags = 0
    explain_action_name = "gepetto:explain_function"
    explain_menu_path = "Edit/Gepetto/" + _("Explain function")
    comment_action_name = "gepetto:comment_function"
    comment_menu_path = "Edit/Gepetto/" + _("Comment function")
    rename_action_name = "gepetto:rename_function"
    rename_menu_path = "Edit/Gepetto/" + _("Auto-rename")
    c_code_action_name = "gepetto:generate_c_code"
    c_code_menu_path = "Edit/Gepetto/" + _("Generate C Code")
    python_code_action_name = "gepetto:generate_python_code"
    python_code_menu_path = "Edit/Gepetto/" + _("Generate Python Code")
    auto_show_action_name = "gepetto:toggle_status_panel_auto_show"
    wanted_name = 'Gepetto'
    wanted_hotkey = ''
    comment = _("Uses {model} to enrich the decompiler's output").format(model=str(gepetto.config.model))
    help = _("See usage instructions on GitHub")
    menu = None
    model_action_map = {}

    # -----------------------------------------------------------------------------

    def init(self):
        global PLUGIN_INSTANCE
        # Check whether the decompiler is available
        if not ida_hexrays.init_hexrays_plugin():
            return idaapi.PLUGIN_SKIP
        # Only launch in interactive mode
        if not ida_kernwin.is_idaq():
            return idaapi.PLUGIN_SKIP
        # Check if Gepetto loaded at least one model properly
        if not gepetto.config.model:
            return idaapi.PLUGIN_SKIP

        # Function explaining action
        explain_action = idaapi.action_desc_t(self.explain_action_name,
                                              _('Explain function'),
                                              ExplainHandler(),
                                              "Ctrl+Alt+G",
                                              _('Use {model} to explain the currently selected function').format(
                                                  model=str(gepetto.config.model)),
                                              452)
        idaapi.register_action(explain_action)

        # Function commenting action
        comment_action = idaapi.action_desc_t(self.comment_action_name,
                                              _('Comment function'),
                                              CommentHandler(),
                                              "Ctrl+Alt+K",
                                              _('Adds comments to lines in the current function using {model}').format(
                                                  model=str(gepetto.config.model)),
                                              453)
        idaapi.register_action(comment_action)

        # Variable and function renaming action
        rename_action = idaapi.action_desc_t(self.rename_action_name,
                                             _('Auto-rename'),
                                             RenameHandler(),
                                             "Ctrl+Alt+R",
                                             _("Use {model} to auto-rename this function and its variables").format(
                                                 model=str(gepetto.config.model)),
                                             19)
        idaapi.register_action(rename_action)

        # Generate Python Code action
        generate_python_code_action = idaapi.action_desc_t(
            self.python_code_action_name,
            _('Generate Python Code'),
            GeneratePythonCodeHandler(),
            "Ctrl+Alt+P",
            _("Generate python code from the currently selected function using {model}").format(
                model=str(gepetto.config.model)
            ),
            201
        )
        idaapi.register_action(generate_python_code_action)

        # Generate C Code action
        generate_c_code_action = idaapi.action_desc_t(
            self.c_code_action_name,
            _('Generate C Code'),
            GenerateCCodeHandler(),
            "Ctrl+Alt+C",
            _("Generate executable C code from the currently selected function using {model}").format(
                model=str(gepetto.config.model)
            ),
            200
        )
        idaapi.register_action(generate_c_code_action)

        idaapi.attach_action_to_menu(self.explain_menu_path, self.explain_action_name, idaapi.SETMENU_APP)
        idaapi.attach_action_to_menu(self.comment_menu_path, self.comment_action_name, idaapi.SETMENU_APP)
        idaapi.attach_action_to_menu(self.rename_menu_path, self.rename_action_name, idaapi.SETMENU_APP)
        idaapi.attach_action_to_menu(self.c_code_menu_path, self.c_code_action_name, idaapi.SETMENU_APP)
        idaapi.attach_action_to_menu(self.python_code_menu_path, self.python_code_action_name, idaapi.SETMENU_APP)

        PLUGIN_INSTANCE = self

        self._menu_refresh_lock = threading.Lock()
        self._menu_refresh_thread: threading.Thread | None = None
        self._menu_refresh_pending = False

        self.generate_model_select_menu()

        # Register context menu actions
        self.menu = ContextMenuHooks()
        self.menu.hook()

        # Register CLI
        register_cli()

        options_menu = "Edit/Gepetto/" + _("Options")
        toggle_label = _("Auto-open status panel")
        self.auto_show_menu_path = f"{options_menu}/{toggle_label}"
        self._register_auto_show_action()
        if gepetto.config.auto_show_status_panel_enabled():
            ida_kernwin.execute_sync(lambda: get_status_panel().ensure_shown(), ida_kernwin.MFF_FAST)

        return idaapi.PLUGIN_KEEP

    # -----------------------------------------------------------------------------

    def bind_model_switch_action(self, menu_path, action_name, model_name):
        """
        Helper function which facilitates the binding between a menu item and the action
        of switching the selected model.
        :param menu_path: The path associated to the option
        :param action_name: The name of the action
        :param model_name: The name of the model to use when this action is clicked.
        :return: None
        """
        action = idaapi.action_desc_t(action_name,
                                      model_name,
                                      None if str(gepetto.config.model) == model_name
                                      else SwapModelHandler(model_name, self),
                                      "",
                                      "",
                                      208 if str(gepetto.config.model) == model_name else -1)  # Icon #208 == check mark.
        ida_kernwin.execute_sync(functools.partial(idaapi.register_action, action), ida_kernwin.MFF_FAST)
        ida_kernwin.execute_sync(functools.partial(idaapi.attach_action_to_menu, menu_path, action_name, idaapi.SETMENU_APP),
                                 ida_kernwin.MFF_FAST)

    # -----------------------------------------------------------------------------

    def detach_actions(self):
        for provider in gepetto.models.model_manager.list_models():
            for model in provider.supported_models():
                if model in self.model_action_map:
                    ida_kernwin.execute_sync(functools.partial(idaapi.unregister_action, self.model_action_map[model]),
                                             ida_kernwin.MFF_FAST)
                    ida_kernwin.execute_sync(functools.partial(idaapi.detach_action_from_menu,
                                                               "Edit/Gepetto/" + _("Select model") +
                                                               f"/{provider.get_menu_name()}/{model}",
                                                               self.model_action_map[model]),
                                             ida_kernwin.MFF_FAST)

    # -----------------------------------------------------------------------------

    def generate_model_select_menu(self):
        def do_generate_model_select_menu():
            while True:
                with self._menu_refresh_lock:
                    # Delete any possible previous entries
                    self.detach_actions()
                    self.model_action_map.clear()

                    for provider in gepetto.models.model_manager.list_models():
                        for model in provider.supported_models():
                            action_name = f"gepetto:{model}_{''.join(random.choices(string.ascii_lowercase, k=7))}"
                            self.model_action_map[model] = action_name
                            self.bind_model_switch_action(
                                "Edit/Gepetto/" + _("Select model") + f"/{provider.get_menu_name()}/{model}",
                                action_name,
                                model,
                            )
                with self._menu_refresh_lock:
                    if not self._menu_refresh_pending:
                        self._menu_refresh_thread = None
                        break
                    self._menu_refresh_pending = False

        with getattr(self, "_menu_refresh_lock", threading.Lock()):
            if self._menu_refresh_thread and self._menu_refresh_thread.is_alive():
                self._menu_refresh_pending = True
                return
            self._menu_refresh_pending = False
            self._menu_refresh_thread = threading.Thread(
                target=do_generate_model_select_menu,
                name="GepettoModelMenuRefresh",
                daemon=True,
            )
            self._menu_refresh_thread.start()

    # -----------------------------------------------------------------------------

    def _register_auto_show_action(self, force_state=None):
        if not hasattr(self, "auto_show_menu_path"):
            return
        _safe_execute_sync(
            functools.partial(
                idaapi.detach_action_from_menu,
                self.auto_show_menu_path,
                self.auto_show_action_name,
            )
        )
        _safe_execute_sync(
            functools.partial(idaapi.unregister_action, self.auto_show_action_name)
        )

        if force_state is None:
            enabled = gepetto.config.auto_show_status_panel_enabled()
        else:
            enabled = bool(force_state)
        icon = 208 if enabled else -1
        self._auto_show_handler = ToggleStatusPanelAutoShowHandler(self)
        action = idaapi.action_desc_t(
            self.auto_show_action_name,
            _("Auto-open status panel"),
            self._auto_show_handler,
            "",
            _("Automatically focus the Gepetto status panel when a request starts."),
            icon,
        )
        _safe_execute_sync(functools.partial(idaapi.register_action, action))
        _safe_execute_sync(
            functools.partial(
                idaapi.attach_action_to_menu,
                self.auto_show_menu_path,
                self.auto_show_action_name,
                idaapi.SETMENU_APP,
            )
        )
        _safe_execute_sync(
            functools.partial(
                ida_kernwin.update_action_icon,
                self.auto_show_action_name,
                icon,
            )
        )

    # -----------------------------------------------------------------------------

    def refresh_auto_show_action(self, force_state=None):
        self._register_auto_show_action(force_state=force_state)

    # -----------------------------------------------------------------------------
    def _unregister_auto_show_action(self):
        if not hasattr(self, "auto_show_menu_path"):
            return
        _safe_execute_sync(
            functools.partial(
                idaapi.detach_action_from_menu,
                self.auto_show_menu_path,
                self.auto_show_action_name,
            )
        )
        _safe_execute_sync(
            functools.partial(idaapi.unregister_action, self.auto_show_action_name)
        )

    # -----------------------------------------------------------------------------

    def run(self, arg):
        pass

    # -----------------------------------------------------------------------------

    def term(self):
        global PLUGIN_INSTANCE
        self.detach_actions()
        if self.menu:
            self.menu.unhook()
        self._unregister_auto_show_action()
        get_status_panel().close()
        PLUGIN_INSTANCE = None
        return


# -----------------------------------------------------------------------------

class ToggleStatusPanelAutoShowHandler(idaapi.action_handler_t):
    def __init__(self, plugin: "GepettoPlugin"):
        super().__init__()
        self._plugin = plugin

    def activate(self, ctx):
        current_state = gepetto.config.auto_show_status_panel_enabled()
        new_state = not current_state
        gepetto.config.set_auto_show_status_panel(new_state)
        if new_state:
            ida_kernwin.execute_sync(lambda: get_status_panel().ensure_shown(), ida_kernwin.MFF_FAST)
        self._plugin.refresh_auto_show_action(force_state=new_state)
        return 1

    def update(self, ctx):
        return idaapi.AST_ENABLE_ALWAYS


# -----------------------------------------------------------------------------

class ContextMenuHooks(idaapi.UI_Hooks):
    def populating_widget_popup(self, form, popup, ctx=None):
        """Accept both legacy and new signatures to avoid SWIG signature introspection issues."""
        return 0

    def finish_populating_widget_popup(self, form, popup, ctx=None):
        widget = form
        popup_handle = popup

        if ctx is not None:
            try:
                ctx_widget = getattr(ctx, "widget", None)
            except Exception:
                ctx_widget = None
            if ctx_widget is not None:
                widget = ctx_widget

            try:
                ctx_popup = getattr(ctx, "popup", None)
            except Exception:
                ctx_popup = None
            if ctx_popup is not None:
                popup_handle = ctx_popup

        try:
            widget_type = idaapi.get_widget_type(widget)
        except Exception:
            return 0

        if widget_type == idaapi.BWN_PSEUDOCODE:
            idaapi.attach_action_to_popup(widget, popup_handle, GepettoPlugin.explain_action_name, "Gepetto/")
            idaapi.attach_action_to_popup(widget, popup_handle, GepettoPlugin.comment_action_name, "Gepetto/")
            idaapi.attach_action_to_popup(widget, popup_handle, GepettoPlugin.rename_action_name, "Gepetto/")
            idaapi.attach_action_to_popup(widget, popup_handle, GepettoPlugin.c_code_action_name, "Gepetto/")
            idaapi.attach_action_to_popup(widget, popup_handle, GepettoPlugin.python_code_action_name, "Gepetto/")

        return 0

```

`gepetto/ida/utils/function_helpers.py`:

```py
from typing import Literal

import idaapi
import ida_funcs
import ida_kernwin
import ida_name

from gepetto.ida.utils.thread_helpers import ida_read


def parse_ea(ea_val):
    """Accept ints or hex-like strings ('0x22A38', '22A38', '22A38h').
    Return int EA or raise ValueError."""
    if ea_val is None:
        raise ValueError("No EA provided")
    if isinstance(ea_val, int):
        return ea_val
    if isinstance(ea_val, str):
        s = ea_val.strip()
        if s[-1:] in ("h", "H"):
            s = "0x" + s[:-1]
        return int(s, 0)
    raise ValueError(f"Unsupported EA type: {type(ea_val).__name__}")

# ---------------------------------------------------------------------------

def resolve_ea(name) -> int:
    """Resolve a name to its effective address."""
    out = {"ea": None, "err": None}

    def _do():
        try:
            ne = ida_name.get_name_ea(idaapi.BADADDR, name)
            if ne == idaapi.BADADDR:
                out["err"] = f"Name not found: {name!r}"
                return 0
            out["ea"] = int(ne)
            return 1
        except Exception as e:
            out["err"] = str(e)
            return 0

    ida_kernwin.execute_sync(_do, ida_kernwin.MFF_READ)
    if out["ea"] is None:
        raise ValueError(out["err"] or "Failed to resolve EA")
    return out["ea"]

# ---------------------------------------------------------------------------

def resolve_func(ea=None, name=None):
    """Resolve a function by EA or name on the UI thread."""
    out = {"func": None, "err": None}

    def _do():
        try:
            if name:
                name_ea = ida_name.get_name_ea(idaapi.BADADDR, name)
                if name_ea == idaapi.BADADDR:
                    out["err"] = f"Name not found: {name!r}"
                    return 0
                f = ida_funcs.get_func(name_ea)
                if not f:
                    out["err"] = f"Symbol {name!r} not inside a function."
                    return 0
                out["func"] = f
                return 1
            f = ida_funcs.get_func(ea)
            if not f:
                out["err"] = f"EA 0x{ea:X} is not inside a function."
                return 0
            out["func"] = f
            return 1
        except Exception as e:
            out["err"] = str(e); return 0

    ida_kernwin.execute_sync(_do, ida_kernwin.MFF_READ)
    if not out["func"]:
        raise ValueError(out["err"] or "Failed to resolve function")
    return out["func"]

# ---------------------------------------------------------------------------

def get_func_name(f) -> str:
    """Fetch a function's name on the main thread with a read lock."""
    out = {"name": ""}

    def _do():
        out["name"] = (
            ida_funcs.get_func_name(f.start_ea)
            or ida_name.get_ea_name(f.start_ea)
            or ""
        )
        return 1

    ida_kernwin.execute_sync(_do, ida_kernwin.MFF_READ)
    return out["name"]

# ---------------------------------------------------------------------------

@ida_read
def get_ptr_size() -> int:
    try:
        inf = idaapi.get_inf_structure()
        if inf.is_64bit():
            return 8
        if inf.is_32bit():
            return 4
        return 2
    except AttributeError:  # IDA 9.x API
        import ida_ida
        if ida_ida.inf_is_64bit():
            return 8
        if ida_ida.inf_is_32bit_exactly():
            return 4
        if ida_ida.inf_is_16bit():
            return 2
        raise NotImplementedError("Unable to determine pointer size!")

# ---------------------------------------------------------------------------

@ida_read
def get_endianness() -> Literal['little', 'big']:
    try:
        return "big" if idaapi.get_inf_structure().is_be() else "little"
    except AttributeError:  # IDA 9.x API
        import ida_ida
        return "big" if ida_ida.inf_is_be() else "little"
```

`gepetto/ida/utils/hooks.py`:

```py
from __future__ import annotations

from collections.abc import Callable
import ida_kernwin

_desktop_ready = False
_desktop_hook: _DesktopHooks | None = None
_pending: list[Callable[[], None]] = []

class _DesktopHooks(ida_kernwin.UI_Hooks):
    def desktop_applied(self, name, from_idb, layout_type):  # noqa: ANN001
        _mark_desktop_ready()
        return 0

def _mark_desktop_ready() -> None:
    global _desktop_ready, _desktop_hook
    _desktop_ready = True
    if _desktop_hook is not None:
        try:
            _desktop_hook.unhook()
        except Exception:
            pass
        _desktop_hook = None
    _flush_pending()

def _flush_pending() -> None:
    while _pending:
        callback = _pending.pop(0)
        try:
            callback()
        except Exception:
            pass

def _ensure_desktop_hook() -> None:
    global _desktop_hook
    if _desktop_ready:
        return
    if _desktop_hook is None:
        hook = _DesktopHooks()
        try:
            hook.hook()
            _desktop_hook = hook
        except Exception:
            _desktop_hook = None
            _mark_desktop_ready()

def _guess_desktop_ready() -> bool:
    try:
        return bool(ida_kernwin.find_widget("IDA View-A"))
    except Exception:
        return False

def run_when_desktop_ready(callback: Callable[[], None]) -> None:
    if _desktop_ready or _guess_desktop_ready():
        _mark_desktop_ready()
        callback()
        return
    _pending.append(callback)
    _ensure_desktop_hook()

```

`gepetto/ida/utils/thread_helpers.py`:

```py
"""
Thread-safety helpers for interacting with IDA APIs.

These utilities wrap ``execute_sync`` and related primitives so callers can
schedule database mutations on the main thread without littering defensive
boilerplate throughout the codebase.  The goal is to operate correctly on
IDA 7.x (PyQt5) while remaining tolerant of headless or partially initialised
environments, which is important when unit tests import modules outside IDA.
"""

from __future__ import annotations

import functools
from collections.abc import Callable
from typing import Any

import idaapi  # type: ignore
import ida_kernwin  # type: ignore
import ida_hexrays  # type: ignore
import idc  # type: ignore

# Sentinel BADADDR for environments without idaapi.
BADADDR = getattr(idaapi, "BADADDR", -1)


def _is_main_thread() -> bool:
    """Return True if the current thread is the IDA main/UI thread."""
    fn = getattr(ida_kernwin, "is_main_thread", None)
    if callable(fn):
        try:
            return bool(fn())
        except Exception:
            return False
    return False


def _execute_sync(callable_: Callable[[], Any], write: bool) -> Any:
    """
    Execute ``callable_`` on IDA's main thread and return its result.

    The helper captures exceptions raised by ``callable_`` and re-raises them
    on the caller's thread so upstream code can handle failures consistently.
    """
    if ida_kernwin is None and idaapi is None:
        return callable_()

    flag = None
    if ida_kernwin is not None:
        flag = ida_kernwin.MFF_WRITE if write else ida_kernwin.MFF_READ
        exec_fn = getattr(ida_kernwin, "execute_sync", None)
    else:
        exec_fn = getattr(idaapi, "execute_sync", None)
        if idaapi is not None:
            flag = idaapi.MFF_WRITE if write else idaapi.MFF_READ

    if callable(exec_fn):
        slot: dict[str, Any] = {}

        def runner() -> int:
            try:
                slot["result"] = callable_()
                slot["ok"] = True
            except Exception as exc:  # pragma: no cover - IDA specific.
                slot["error"] = exc
                slot["ok"] = False
            return 1

        exec_fn(runner, flag)
        if not slot.get("ok", False):
            error = slot.get("error")
            if isinstance(error, Exception):
                raise error
            raise RuntimeError("Failed to execute on IDA main thread")
        return slot.get("result")

    # execute_sync unavailable (e.g., unit tests): run inline.
    return callable_()


def run_on_main_thread(func: Callable[[], Any], write: bool = False) -> Any:
    """
    Execute ``func`` on the IDA main thread if necessary.

    Args:
        func: Callable to execute.
        write: Whether the callable mutates the database (selects MFF_WRITE).

    Returns:
        The return value of ``func``.
    """
    if _is_main_thread():
        return func()
    return _execute_sync(func, write=write)


def sync_on_main_thread(write: bool = False):
    """Decorator factory that executes the wrapped callable on IDA's main thread."""

    def decorator(func: Callable[..., Any]) -> Callable[..., Any]:
        @functools.wraps(func)
        def wrapper(*args: Any, **kwargs: Any) -> Any:
            return run_on_main_thread(lambda: func(*args, **kwargs), write=write)

        return wrapper

    return decorator


def ida_read(func: Callable[..., Any]) -> Callable[..., Any]:
    """Decorator enforcing main-thread execution with a read lock."""
    return sync_on_main_thread(write=False)(func)


def ida_write(func: Callable[..., Any]) -> Callable[..., Any]:
    """Decorator enforcing main-thread execution with a write lock."""
    return sync_on_main_thread(write=True)(func)


def safe_get_screen_ea() -> int:
    """
    Fetch the current screen EA while tolerating headless execution.

    Returns:
        Current EA or BADADDR if unavailable.
    """
    if ida_kernwin is not None:
        try:
            ea = run_on_main_thread(lambda: ida_kernwin.get_screen_ea(), write=False)
            if isinstance(ea, int) and ea != BADADDR:
                return int(ea)
        except Exception:
            pass

    if idc is not None:
        try:
            ea = idc.get_screen_ea()
            if isinstance(ea, int) and ea != BADADDR:
                return int(ea)
        except Exception:
            pass

    return BADADDR


def hexrays_available() -> bool:
    """Return True if Hex-Rays is available and initialised."""
    if ida_hexrays is None:
        return False

    def _init() -> bool:
        try:
            return bool(ida_hexrays.init_hexrays_plugin())
        except Exception:
            return False

    try:
        return bool(run_on_main_thread(_init, write=False))
    except Exception:
        return False


__all__ = [
    "BADADDR",
    "hexrays_available",
    "ida_read",
    "ida_write",
    "run_on_main_thread",
    "safe_get_screen_ea",
    "sync_on_main_thread",
]

```

`gepetto/locales/ca_ES/LC_MESSAGES/gepetto.po`:

```po
# Catalan (Spain) translations for Gepetto.
# This file is distributed under the same license as Gepetto (GPLv3).
# Ivan Kwiatkowski, 2023.
#
# Translators:
# Ivan K., 2023
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: Gepetto 1.1\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-10-18 18:57+0000\n"
"PO-Revision-Date: 2025-10-18 18:59+0000\n"
"Last-Translator: Ivan K., 2023\n"
"Language-Team: Catalan (Spain) (https://app.transifex.com/gepetto/teams/164045/ca_ES/)\n"
"Language: ca_ES\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Plural-Forms: nplurals=2; plural=(n != 1);\n"
"Generated-By: Babel 2.17.0\n"

#: gepetto/config.py:82
msgid "Attempting to load the first available model..."
msgstr "S'està intentant carregar el primer model disponible..."

#: gepetto/config.py:87
msgid "No model available. Please edit the configuration file and try again."
msgstr ""
"No hi ha cap model disponible. Si us plau, editeu el fitxer de configuració "
"i torneu-ho a intentar."

#: gepetto/config.py:108
#, python-brace-format
msgid ""
"Warning: Gepetto's configuration doesn't contain option {option} in section "
"{section}!"
msgstr ""
"Advertència: La configuració de Gepetto no conté l'opció {option} a la "
"secció {section}!"

#: gepetto/ida/cli.py:120
msgid "LLM chat"
msgstr "Conversación de LLM"

#: gepetto/ida/cli.py:152
#, python-brace-format
msgid "→ Model requested tool: {tool_name} ({tool_args}...)"
msgstr "→ El model ha demanat l'eina: {tool_name} ({tool_args}...)"

#: gepetto/ida/cli.py:167
#, python-brace-format
msgid "Using tool: {tool_name}"
msgstr "S'està utilitzant l'eina: {tool_name}"

#: gepetto/ida/cli.py:219
msgid "✔ Completed turn"
msgstr "✔ Torn completat"

#: gepetto/ida/cli.py:227 gepetto/ida/cli.py:245
msgid "Model request failed."
msgstr "La sol·licitud al model ha fallat."

#: gepetto/ida/cli.py:258
msgid "Streaming unavailable for this model; retrying without streaming."
msgstr ""
"La transmissió no està disponible per a aquest model; es torna a intentar "
"sense transmissió."

#: gepetto/ida/cli.py:261
msgid "Retrying without streaming"
msgstr "Tornant a intentar sense transmissió"

#: gepetto/ida/comment_handler.py:39 gepetto/ida/comment_handler.py:41
msgid "No focused view available; cannot determine current function."
msgstr ""
"No hi ha cap vista enfocada disponible; no es pot determinar la funció "
"actual."

#: gepetto/ida/comment_handler.py:103
#, python-brace-format
msgid "[ERROR] comment callback JSON failure: {error}"
msgstr "[ERROR] error JSON a la devolució de comentaris: {error}"

#: gepetto/ida/comment_handler.py:148
#, python-brace-format
msgid "[ERROR] comment application failed: {error}"
msgstr "[ERROR] no s'han pogut aplicar els comentaris: {error}"

#: gepetto/ida/comment_handler.py:156
#, python-brace-format
msgid "Applied {count} comments."
msgstr "S'han aplicat {count} comentaris."

#: gepetto/ida/comment_handler.py:162
msgid "No comments were applied."
msgstr "No s'ha aplicat cap comentari."

#: gepetto/ida/comment_handler.py:170
#, python-brace-format
msgid "[ERROR] comment_callback: {error}"
msgstr "[ERROR] callback de comentaris: {error}"

#: gepetto/ida/handlers.py:40 gepetto/ida/handlers.py:46
msgid "Comment generated by Gepetto"
msgstr "Comentari generat per Gepetto"

#: gepetto/ida/handlers.py:58
#, python-brace-format
msgid "[ERROR] comment callback failed: {error}"
msgstr "[ERROR] la devolució de comentaris ha fallat: {error}"

#: gepetto/ida/handlers.py:155
#, python-brace-format
msgid "[ERROR] rename callback JSON failure: {error}"
msgstr "[ERROR] error JSON a la devolució de canvi de nom: {error}"

#: gepetto/ida/handlers.py:165
#, python-brace-format
msgid "Function at EA 0x{ea:X} not found."
msgstr "No s'ha trobat la funció a EA 0x{ea:X}."

#: gepetto/ida/handlers.py:182
#, python-brace-format
msgid "Select names to rename"
msgstr "Seleccioneu els noms a canviar"

#: gepetto/ida/handlers.py:183
msgid "Old Name"
msgstr "Nom antic"

#: gepetto/ida/handlers.py:183
msgid "New Name"
msgstr "Nom nou"

#: gepetto/ida/handlers.py:232
#, python-brace-format
msgid "[ERROR] rename callback failed: {error}"
msgstr "[ERROR] la devolució de canvi de nom ha fallat: {error}"

#: gepetto/ida/handlers.py:238 gepetto/ida/handlers.py:239
msgid "Rename cancelled by user."
msgstr "Canvi de nom cancel·lat per l'usuari."

#: gepetto/ida/handlers.py:243
#, python-brace-format
msgid "Done! {count} name(s) renamed."
msgstr "Fet! S'han reanomenat {count} nom(s)."

#: gepetto/ida/handlers.py:309 gepetto/ida/status_panel.py:654
#, python-brace-format
msgid "Couldn't change model to {model}: {error}"
msgstr "No s'ha pogut canviar el model a {model}: {error}"

#: gepetto/ida/handlers.py:367 gepetto/ida/handlers.py:424
#, python-brace-format
msgid "{model} generated code saved to {file_name}"
msgstr "Codi generat per {model} desat a {file_name}"

#: gepetto/ida/status_panel.py:20 gepetto/ida/status_panel.py:765
#: gepetto/ida/status_panel.py:773
msgid "Gepetto"
msgstr "Gepetto"

#: gepetto/ida/status_panel.py:48
msgid "System"
msgstr "Sistema"

#: gepetto/ida/status_panel.py:49
msgid "User"
msgstr "Usuari"

#: gepetto/ida/status_panel.py:50
msgid "Tool"
msgstr "Eina"

#: gepetto/ida/status_panel.py:51
msgid "Model"
msgstr "Model actual"

#: gepetto/ida/status_panel.py:52
msgid "Assistant"
msgstr "Assistent"

#: gepetto/ida/status_panel.py:248
msgid "Log filters:"
msgstr "Filtres del registre:"

#: gepetto/ida/status_panel.py:266
msgid "Clear"
msgstr "Neteja"

#: gepetto/ida/status_panel.py:285
msgid "Type a prompt and press Enter…"
msgstr "Escriviu una petició i premeu Retorn…"

#: gepetto/ida/status_panel.py:288
msgid "Send"
msgstr "Envia"

#: gepetto/ida/status_panel.py:291 gepetto/ida/status_panel.py:690
msgid "Stop"
msgstr "Atura"

#: gepetto/ida/status_panel.py:315 gepetto/ida/status_panel.py:585
#, python-brace-format
msgid "Model: {model} ▼"
msgstr "Model seleccionat: {model} ▼"

#: gepetto/ida/status_panel.py:321 gepetto/ida/status_panel.py:673
#, python-brace-format
msgid "Status: {status}"
msgstr "Estat: {status}"

#: gepetto/ida/status_panel.py:321 gepetto/ida/status_panel.py:672
#: gepetto/ida/status_panel.py:919
msgid "Idle"
msgstr "Inactiu"

#: gepetto/ida/status_panel.py:499
#, python-brace-format
msgid "Failed to send prompt: {err}"
msgstr "Error en enviar la sol·licitud: {err}"

#: gepetto/ida/status_panel.py:620
msgid "Error loading models"
msgstr "Error en carregar els models"

#: gepetto/ida/status_panel.py:640
#, python-brace-format
msgid "Model switched to {model}"
msgstr "Model canviat a {model}"

#: gepetto/ida/status_panel.py:658
#, python-brace-format
msgid "Failed to switch model: {error}"
msgstr "Error en canviar el model: {error}"

#: gepetto/ida/status_panel.py:702
msgid "Error - Check Log"
msgstr "Error: comproveu el registre"

#: gepetto/ida/status_panel.py:709
msgid "Cancelling…"
msgstr "Cancel·lant…"

#: gepetto/ida/status_panel.py:711
msgid "Cancelling request"
msgstr "Cancel·lant la sol·licitud"

#: gepetto/ida/status_panel.py:751
msgid "You"
msgstr "Tu"

#: gepetto/ida/status_panel.py:753
#, python-brace-format
msgid "User: {text}"
msgstr "Usuari: {text}"

#: gepetto/ida/status_panel.py:759
#, python-brace-format
msgid "Assistant: {text}"
msgstr "Assistent: {text}"

#: gepetto/ida/status_panel.py:861
msgid "Could not show Gepetto Status panel."
msgstr "No s'ha pogut mostrar el panell d'estat de Gepetto."

#: gepetto/ida/status_panel.py:917
msgid "Failed to cancel request"
msgstr "No s'ha pogut cancel·lar la sol·licitud"

#: gepetto/ida/status_panel.py:954 gepetto/ida/status_panel.py:993
msgid "Done"
msgstr "Fet"

#: gepetto/ida/status_panel.py:981
#, python-brace-format
msgid "Request to {model} sent..."
msgstr "Sol·licitud enviada a {model}..."

#: gepetto/ida/status_panel.py:983
msgid "Waiting for model..."
msgstr "Esperant el model..."

#: gepetto/ida/status_panel.py:988
#, python-brace-format
msgid "{model} query finished in {time:.2f} seconds!"
msgstr "La consulta de {model} ha finalitzat en {time:.2f} segons!"

#: gepetto/ida/ui.py:60 gepetto/ida/ui.py:93
msgid "Explain function"
msgstr "Explicar la funció"

#: gepetto/ida/ui.py:62 gepetto/ida/ui.py:103
msgid "Comment function"
msgstr "Explicar la funció"

#: gepetto/ida/ui.py:64 gepetto/ida/ui.py:113
msgid "Auto-rename"
msgstr "Reanomenar automàticament"

#: gepetto/ida/ui.py:66 gepetto/ida/ui.py:137
msgid "Generate C Code"
msgstr "Genera codi C"

#: gepetto/ida/ui.py:68 gepetto/ida/ui.py:124
msgid "Generate Python Code"
msgstr "Genera codi Python"

#: gepetto/ida/ui.py:72
#, python-brace-format
msgid "Uses {model} to enrich the decompiler's output"
msgstr "Utilitza {model} per enriquir la sortida del descompilador"

#: gepetto/ida/ui.py:73
msgid "See usage instructions on GitHub"
msgstr "Veure instruccions d'ús a GitHub"

#: gepetto/ida/ui.py:96
#, python-brace-format
msgid "Use {model} to explain the currently selected function"
msgstr "Utilitzar {model} per explicar la funció seleccionada actualment"

#: gepetto/ida/ui.py:106
#, python-brace-format
msgid "Adds comments to lines in the current function using {model}"
msgstr "Afegeix comentaris a les línies de la funció actual amb {model}"

#: gepetto/ida/ui.py:116
#, python-brace-format
msgid "Use {model} to auto-rename this function and its variables"
msgstr ""
"Utilitzar {model} per reanomenar automàticament aquesta funció i les seves "
"variables"

#: gepetto/ida/ui.py:127
#, python-brace-format
msgid ""
"Generate python code from the currently selected function using {model}"
msgstr "Utilitzar {model} per explicar la funció seleccionada actualment"

#: gepetto/ida/ui.py:140
#, python-brace-format
msgid ""
"Generate executable C code from the currently selected function using "
"{model}"
msgstr "Genera codi C executable de la funció seleccionada amb {model}"

#: gepetto/ida/ui.py:168
msgid "Options"
msgstr "Opcions"

#: gepetto/ida/ui.py:169 gepetto/ida/ui.py:274
msgid "Auto-open status panel"
msgstr "Obre automàticament el panell d'estat"

#: gepetto/ida/ui.py:208 gepetto/ida/ui.py:228
msgid "Select model"
msgstr "Selecciona el model"

#: gepetto/ida/ui.py:277
msgid "Automatically focus the Gepetto status panel when a request starts."
msgstr ""
"Centra automàticament el panell d'estat de Gepetto quan comença una "
"sol·licitud."

#: gepetto/models/aliyun.py:62 gepetto/models/deepseek.py:36
#: gepetto/models/gemini.py:541 gepetto/models/groq.py:37
#: gepetto/models/kluster.py:58 gepetto/models/novita_ai.py:41
#: gepetto/models/openai.py:302 gepetto/models/openai_compatible.py:63
#: gepetto/models/openrouter.py:52 gepetto/models/siliconflow.py:60
#: gepetto/models/together.py:34
#, python-brace-format
msgid ""
"Please edit the configuration file to insert your {api_provider} API key!"
msgstr ""
"Si us plau, editeu aquest script per inserir la vostra clau d'API de "
"{api_provider}!"

#: gepetto/models/gemini.py:111
#, python-brace-format
msgid "Failed to fetch Gemini models: {error}"
msgstr "No s'han pogut obtenir els models de Gemini: {error}"

#: gepetto/models/gemini.py:159
#, python-brace-format
msgid "Failed to configure proxy for Gemini models: {error}"
msgstr ""
"No s'ha pogut configurar el servidor intermediari per als models Gemini: "
"{error}"

#: gepetto/models/gemini.py:179 gepetto/models/local_lmstudio.py:140
#: gepetto/models/local_ollama.py:116 gepetto/models/openai.py:195
#, python-brace-format
msgid "Failed to fetch models from {base_url}: {status_code}"
msgstr "No s'han pogut obtenir models des de {base_url}: {status_code}"

#: gepetto/models/gemini.py:723 gepetto/models/openai.py:414
#: gepetto/models/openai.py:431
#, python-brace-format
msgid "General exception encountered while running the query: {error}"
msgstr ""
"S'ha trobat una excepció general mentre s'executava la consulta: {error}"

#: gepetto/models/local_lmstudio.py:62 gepetto/models/local_ollama.py:60
#, python-brace-format
msgid "Failed to fetch models from {base_url}: {error}"
msgstr "No s'han pogut obtenir models des de {base_url}: {error}"

#: gepetto/models/local_lmstudio.py:123
#, python-brace-format
msgid "Failed to configure proxy for LM Studio models: {error}"
msgstr ""
"No s'ha pogut configurar el servidor intermediari per als models de LM "
"Studio: {error}"

#: gepetto/models/openai.py:120
#, python-brace-format
msgid "Failed to fetch OpenAI models: {error}"
msgstr "No s'han pogut obtenir els models d'OpenAI: {error}"

#: gepetto/models/openai.py:178
#, python-brace-format
msgid "Failed to configure proxy for OpenAI models: {error}"
msgstr ""
"No s'ha pogut configurar el servidor intermediari per als models d'OpenAI: "
"{error}"

#: gepetto/models/openai.py:336
#, python-brace-format
msgid ""
"Streaming rejected for {model}: {error}\n"
"Retrying without streaming."
msgstr ""
"Transmissió rebutjada per a {model}: {error}\\nS'està tornant a intentar "
"sense transmissió."

#: gepetto/models/openai.py:340
#, python-brace-format
msgid ""
"Streaming disabled for {model} after a previous API rejection; retrying "
"without streaming."
msgstr ""
"Transmissió desactivada per a {model} després d'un rebuig previ de l'API; es"
" torna a intentar sense transmissió."

#: gepetto/models/openai.py:404
msgid "Streaming rejected by API; retrying without streaming."
msgstr ""
"L'API ha rebutjat la transmissió; es torna a intentar sense transmissió."

#: gepetto/models/openai.py:410
msgid ""
"Unfortunately, this function is too big to be analyzed with the model's "
"current API limits."
msgstr ""
"Desafortunadament, aquesta funció és massa gran per ser analitzada amb els "
"límits actuals de l'API del model."

#: gepetto/models/openai.py:422
#, python-brace-format
msgid "{model} could not complete the request: {error}"
msgstr "{model} no ha pogut completar la sol·licitud: {error}"

#~ msgid ""
#~ "Could not obtain valid data from the model, giving up. Dumping the response "
#~ "for manual import:"
#~ msgstr ""
#~ "No s'ha pogut obtenir dades vàlides del model, abandonant. Volcant la "
#~ "resposta per a la importació manual:"

#~ msgid ""
#~ "Cannot extract valid JSON from the response. Asking the model to fix it..."
#~ msgstr ""
#~ "No es pot extreure un JSON vàlid de la resposta. Demanant al model que ho "
#~ "arregli..."

#~ msgid "The JSON document returned is invalid. Asking the model to fix it..."
#~ msgstr ""
#~ "El document JSON retornat no és vàlid. Demanant al model que ho arregli..."

#~ msgid "Please fix the following JSON document:\n"
#~ msgstr "Si us plau, arregleu el següent document JSON:\n"

#~ msgid ""
#~ "The JSON document provided in this response is invalid. Can you fix it?\n"
#~ "{response}"
#~ msgstr ""
#~ "El document JSON proporcionat en aquesta resposta no és vàlid. Podeu arreglar-lo?\n"
#~ "{response}"

#~ msgid "{model} query finished! {replaced} variable(s) renamed."
#~ msgstr "Consulta de {model} acabada! S'han reanomenat {replaced} variable(s)."

#~ msgid ""
#~ "Context length exceeded! Reducing the completion tokens to {max_tokens}..."
#~ msgstr ""
#~ "S'ha superat la longitud de context! Reduint el nombre de tokens de "
#~ "finalització a {max_tokens}..."

#~ msgid ""
#~ "Proxy configuration for Gemini via google-generativeai library might require"
#~ " manual setup of HTTPS_PROXY environment variable."
#~ msgstr ""
#~ "Proxy configuration for Gemini via google-generativeai library might require"
#~ " manual setup of HTTPS_PROXY environment variable."

#~ msgid ""
#~ "Please generate executable C code based on the following decompiled C code and ensure it includes all necessary header files and other information:\n"
#~ "{decompiler_output}"
#~ msgstr ""
#~ "Please generate executable C code based on the following decompiled C code and ensure it includes all necessary header files and other information:\n"
#~ "{decompiler_output}"

#~ msgid ""
#~ "Please generate equivalent Python code based on the following decompiled C code, and provide an example of the function call:\n"
#~ "{decompiler_output}"
#~ msgstr ""
#~ "Please generate equivalent Python code based on the following decompiled C code, and provide an example of the function call:\n"
#~ "{decompiler_output}"

#~ msgid ""
#~ "You are a helpful assistant embedded in IDA Pro. Your role is to facilitate "
#~ "reverse-engineering and answer programming questions."
#~ msgstr ""
#~ "Ets un assistent útil integrat a IDA Pro. El teu paper és facilitar la "
#~ "retroenginyeria i respondre preguntes de programació."

```

`gepetto/locales/es_ES/LC_MESSAGES/gepetto.po`:

```po
# Spanish (Spain) translations for Gepetto.
# This file is distributed under the same license as Gepetto (GPLv3).
# Ivan Kwiatkowski, 2023.
#
# Translators:
# Ivan K., 2023
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: Gepetto 1.1\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-10-18 18:57+0000\n"
"PO-Revision-Date: 2025-10-18 18:59+0000\n"
"Last-Translator: Ivan K., 2023\n"
"Language-Team: Spanish (Spain) (https://app.transifex.com/gepetto/teams/164045/es_ES/)\n"
"Language: es_ES\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Plural-Forms: nplurals=3; plural=n == 1 ? 0 : n != 0 && n % 1000000 == 0 ? 1 : 2;\n"
"Generated-By: Babel 2.17.0\n"

#: gepetto/config.py:82
msgid "Attempting to load the first available model..."
msgstr "Intentando cargar el primer modelo disponible..."

#: gepetto/config.py:87
msgid "No model available. Please edit the configuration file and try again."
msgstr ""
"No hay ningún modelo disponible. Por favor, edite el archivo de "
"configuración e inténtelo de nuevo."

#: gepetto/config.py:108
#, python-brace-format
msgid ""
"Warning: Gepetto's configuration doesn't contain option {option} in section "
"{section}!"
msgstr ""
"Advertencia: La configuración de Gepetto no contiene la opción {option} en "
"la sección {section}!"

#: gepetto/ida/cli.py:120
msgid "LLM chat"
msgstr "Conversación de LLM"

#: gepetto/ida/cli.py:152
#, python-brace-format
msgid "→ Model requested tool: {tool_name} ({tool_args}...)"
msgstr "→ El modelo solicitó la herramienta: {tool_name} ({tool_args}...)"

#: gepetto/ida/cli.py:167
#, python-brace-format
msgid "Using tool: {tool_name}"
msgstr "Usando la herramienta: {tool_name}"

#: gepetto/ida/cli.py:219
msgid "✔ Completed turn"
msgstr "✔ Turno completado"

#: gepetto/ida/cli.py:227 gepetto/ida/cli.py:245
msgid "Model request failed."
msgstr "La solicitud al modelo falló."

#: gepetto/ida/cli.py:258
msgid "Streaming unavailable for this model; retrying without streaming."
msgstr ""
"La transmisión no está disponible para este modelo; reintentando sin "
"transmisión."

#: gepetto/ida/cli.py:261
msgid "Retrying without streaming"
msgstr "Reintentando sin transmisión"

#: gepetto/ida/comment_handler.py:39 gepetto/ida/comment_handler.py:41
msgid "No focused view available; cannot determine current function."
msgstr ""
"No hay una vista enfocada disponible; no se puede determinar la función "
"actual."

#: gepetto/ida/comment_handler.py:103
#, python-brace-format
msgid "[ERROR] comment callback JSON failure: {error}"
msgstr "[ERROR] fallo JSON en la devolución de comentario: {error}"

#: gepetto/ida/comment_handler.py:148
#, python-brace-format
msgid "[ERROR] comment application failed: {error}"
msgstr "[ERROR] error al aplicar comentarios: {error}"

#: gepetto/ida/comment_handler.py:156
#, python-brace-format
msgid "Applied {count} comments."
msgstr "Se aplicaron {count} comentarios."

#: gepetto/ida/comment_handler.py:162
msgid "No comments were applied."
msgstr "No se aplicó ningún comentario."

#: gepetto/ida/comment_handler.py:170
#, python-brace-format
msgid "[ERROR] comment_callback: {error}"
msgstr "[ERROR] callback de comentarios: {error}"

#: gepetto/ida/handlers.py:40 gepetto/ida/handlers.py:46
msgid "Comment generated by Gepetto"
msgstr "Comentario generado por Gepetto"

#: gepetto/ida/handlers.py:58
#, python-brace-format
msgid "[ERROR] comment callback failed: {error}"
msgstr "[ERROR] falló la devolución de comentario: {error}"

#: gepetto/ida/handlers.py:155
#, python-brace-format
msgid "[ERROR] rename callback JSON failure: {error}"
msgstr "[ERROR] fallo JSON en la devolución de cambio de nombre: {error}"

#: gepetto/ida/handlers.py:165
#, python-brace-format
msgid "Function at EA 0x{ea:X} not found."
msgstr "No se encontró la función en EA 0x{ea:X}."

#: gepetto/ida/handlers.py:182
#, python-brace-format
msgid "Select names to rename"
msgstr "Seleccionar nombres para renombrar"

#: gepetto/ida/handlers.py:183
msgid "Old Name"
msgstr "Nombre anterior"

#: gepetto/ida/handlers.py:183
msgid "New Name"
msgstr "Nombre nuevo"

#: gepetto/ida/handlers.py:232
#, python-brace-format
msgid "[ERROR] rename callback failed: {error}"
msgstr "[ERROR] falló la devolución de cambio de nombre: {error}"

#: gepetto/ida/handlers.py:238 gepetto/ida/handlers.py:239
msgid "Rename cancelled by user."
msgstr "Cambio de nombre cancelado por el usuario."

#: gepetto/ida/handlers.py:243
#, python-brace-format
msgid "Done! {count} name(s) renamed."
msgstr "¡Hecho! Se ha(n) renombrado {count} nombre(s)."

#: gepetto/ida/handlers.py:309 gepetto/ida/status_panel.py:654
#, python-brace-format
msgid "Couldn't change model to {model}: {error}"
msgstr "No se pudo cambiar el modelo a {model}: {error}"

#: gepetto/ida/handlers.py:367 gepetto/ida/handlers.py:424
#, python-brace-format
msgid "{model} generated code saved to {file_name}"
msgstr "Código generado por {model} guardado en {file_name}"

#: gepetto/ida/status_panel.py:20 gepetto/ida/status_panel.py:765
#: gepetto/ida/status_panel.py:773
msgid "Gepetto"
msgstr "Gepetto"

#: gepetto/ida/status_panel.py:48
msgid "System"
msgstr "Sistema"

#: gepetto/ida/status_panel.py:49
msgid "User"
msgstr "Usuario"

#: gepetto/ida/status_panel.py:50
msgid "Tool"
msgstr "Herramienta"

#: gepetto/ida/status_panel.py:51
msgid "Model"
msgstr "Modelo"

#: gepetto/ida/status_panel.py:52
msgid "Assistant"
msgstr "Asistente"

#: gepetto/ida/status_panel.py:248
msgid "Log filters:"
msgstr "Filtros del registro:"

#: gepetto/ida/status_panel.py:266
msgid "Clear"
msgstr "Limpiar"

#: gepetto/ida/status_panel.py:285
msgid "Type a prompt and press Enter…"
msgstr "Escriba una solicitud y presione Intro…"

#: gepetto/ida/status_panel.py:288
msgid "Send"
msgstr "Enviar"

#: gepetto/ida/status_panel.py:291 gepetto/ida/status_panel.py:690
msgid "Stop"
msgstr "Detener"

#: gepetto/ida/status_panel.py:315 gepetto/ida/status_panel.py:585
#, python-brace-format
msgid "Model: {model} ▼"
msgstr "Modelo: {model} ▼"

#: gepetto/ida/status_panel.py:321 gepetto/ida/status_panel.py:673
#, python-brace-format
msgid "Status: {status}"
msgstr "Estado: {status}"

#: gepetto/ida/status_panel.py:321 gepetto/ida/status_panel.py:672
#: gepetto/ida/status_panel.py:919
msgid "Idle"
msgstr "Inactivo"

#: gepetto/ida/status_panel.py:499
#, python-brace-format
msgid "Failed to send prompt: {err}"
msgstr "Error al enviar la solicitud: {err}"

#: gepetto/ida/status_panel.py:620
msgid "Error loading models"
msgstr "Error al cargar los modelos"

#: gepetto/ida/status_panel.py:640
#, python-brace-format
msgid "Model switched to {model}"
msgstr "Modelo cambiado a {model}"

#: gepetto/ida/status_panel.py:658
#, python-brace-format
msgid "Failed to switch model: {error}"
msgstr "Error al cambiar el modelo: {error}"

#: gepetto/ida/status_panel.py:702
msgid "Error - Check Log"
msgstr "Error: revise el registro"

#: gepetto/ida/status_panel.py:709
msgid "Cancelling…"
msgstr "Cancelando…"

#: gepetto/ida/status_panel.py:711
msgid "Cancelling request"
msgstr "Cancelando la solicitud"

#: gepetto/ida/status_panel.py:751
msgid "You"
msgstr "Tú"

#: gepetto/ida/status_panel.py:753
#, python-brace-format
msgid "User: {text}"
msgstr "Usuario: {text}"

#: gepetto/ida/status_panel.py:759
#, python-brace-format
msgid "Assistant: {text}"
msgstr "Asistente: {text}"

#: gepetto/ida/status_panel.py:861
msgid "Could not show Gepetto Status panel."
msgstr "No se pudo mostrar el panel de estado de Gepetto."

#: gepetto/ida/status_panel.py:917
msgid "Failed to cancel request"
msgstr "No se pudo cancelar la solicitud"

#: gepetto/ida/status_panel.py:954 gepetto/ida/status_panel.py:993
msgid "Done"
msgstr "Listo"

#: gepetto/ida/status_panel.py:981
#, python-brace-format
msgid "Request to {model} sent..."
msgstr "Solicitud enviada a {model}..."

#: gepetto/ida/status_panel.py:983
msgid "Waiting for model..."
msgstr "Esperando al modelo..."

#: gepetto/ida/status_panel.py:988
#, python-brace-format
msgid "{model} query finished in {time:.2f} seconds!"
msgstr "¡Consulta de {model} finalizada en {time:.2f} segundos!"

#: gepetto/ida/ui.py:60 gepetto/ida/ui.py:93
msgid "Explain function"
msgstr "Explicar la función"

#: gepetto/ida/ui.py:62 gepetto/ida/ui.py:103
msgid "Comment function"
msgstr "Explicar la función"

#: gepetto/ida/ui.py:64 gepetto/ida/ui.py:113
msgid "Auto-rename"
msgstr "Renombrado automático"

#: gepetto/ida/ui.py:66 gepetto/ida/ui.py:137
msgid "Generate C Code"
msgstr "Generar código C"

#: gepetto/ida/ui.py:68 gepetto/ida/ui.py:124
msgid "Generate Python Code"
msgstr "Generar código Python"

#: gepetto/ida/ui.py:72
#, python-brace-format
msgid "Uses {model} to enrich the decompiler's output"
msgstr "Utiliza {model} para enriquecer la salida del descompilador"

#: gepetto/ida/ui.py:73
msgid "See usage instructions on GitHub"
msgstr "Ver instrucciones de uso en GitHub"

#: gepetto/ida/ui.py:96
#, python-brace-format
msgid "Use {model} to explain the currently selected function"
msgstr "Usar {model} para explicar la función seleccionada actualmente"

#: gepetto/ida/ui.py:106
#, python-brace-format
msgid "Adds comments to lines in the current function using {model}"
msgstr "Añade comentarios a las líneas de la función actual usando {model}"

#: gepetto/ida/ui.py:116
#, python-brace-format
msgid "Use {model} to auto-rename this function and its variables"
msgstr ""
"Usar {model} para renombrar automáticamente esta función y sus variables"

#: gepetto/ida/ui.py:127
#, python-brace-format
msgid ""
"Generate python code from the currently selected function using {model}"
msgstr "Usar {model} para explicar la función seleccionada actualmente"

#: gepetto/ida/ui.py:140
#, python-brace-format
msgid ""
"Generate executable C code from the currently selected function using "
"{model}"
msgstr "Genera código C ejecutable de la función seleccionada usando {model}"

#: gepetto/ida/ui.py:168
msgid "Options"
msgstr "Opciones"

#: gepetto/ida/ui.py:169 gepetto/ida/ui.py:274
msgid "Auto-open status panel"
msgstr "Abrir automáticamente el panel de estado"

#: gepetto/ida/ui.py:208 gepetto/ida/ui.py:228
msgid "Select model"
msgstr "Seleccionar modelo"

#: gepetto/ida/ui.py:277
msgid "Automatically focus the Gepetto status panel when a request starts."
msgstr ""
"Enfocar automáticamente el panel de estado de Gepetto al iniciar una "
"solicitud."

#: gepetto/models/aliyun.py:62 gepetto/models/deepseek.py:36
#: gepetto/models/gemini.py:541 gepetto/models/groq.py:37
#: gepetto/models/kluster.py:58 gepetto/models/novita_ai.py:41
#: gepetto/models/openai.py:302 gepetto/models/openai_compatible.py:63
#: gepetto/models/openrouter.py:52 gepetto/models/siliconflow.py:60
#: gepetto/models/together.py:34
#, python-brace-format
msgid ""
"Please edit the configuration file to insert your {api_provider} API key!"
msgstr "¡Edite este script para insertar su clave de API de {api_provider}!"

#: gepetto/models/gemini.py:111
#, python-brace-format
msgid "Failed to fetch Gemini models: {error}"
msgstr "No se pudieron obtener los modelos de Gemini: {error}"

#: gepetto/models/gemini.py:159
#, python-brace-format
msgid "Failed to configure proxy for Gemini models: {error}"
msgstr "No se pudo configurar el proxy para los modelos Gemini: {error}"

#: gepetto/models/gemini.py:179 gepetto/models/local_lmstudio.py:140
#: gepetto/models/local_ollama.py:116 gepetto/models/openai.py:195
#, python-brace-format
msgid "Failed to fetch models from {base_url}: {status_code}"
msgstr "No se pudieron obtener modelos de {base_url}: {status_code}"

#: gepetto/models/gemini.py:723 gepetto/models/openai.py:414
#: gepetto/models/openai.py:431
#, python-brace-format
msgid "General exception encountered while running the query: {error}"
msgstr ""
"Se encontró una excepción general mientras se ejecutaba la consulta: {error}"

#: gepetto/models/local_lmstudio.py:62 gepetto/models/local_ollama.py:60
#, python-brace-format
msgid "Failed to fetch models from {base_url}: {error}"
msgstr "No se pudieron obtener modelos de {base_url}: {error}"

#: gepetto/models/local_lmstudio.py:123
#, python-brace-format
msgid "Failed to configure proxy for LM Studio models: {error}"
msgstr "No se pudo configurar el proxy para los modelos de LM Studio: {error}"

#: gepetto/models/openai.py:120
#, python-brace-format
msgid "Failed to fetch OpenAI models: {error}"
msgstr "No se pudieron obtener los modelos de OpenAI: {error}"

#: gepetto/models/openai.py:178
#, python-brace-format
msgid "Failed to configure proxy for OpenAI models: {error}"
msgstr "No se pudo configurar el proxy para los modelos de OpenAI: {error}"

#: gepetto/models/openai.py:336
#, python-brace-format
msgid ""
"Streaming rejected for {model}: {error}\n"
"Retrying without streaming."
msgstr ""
"Transmisión rechazada para {model}: {error}\\nReintentando sin transmisión."

#: gepetto/models/openai.py:340
#, python-brace-format
msgid ""
"Streaming disabled for {model} after a previous API rejection; retrying "
"without streaming."
msgstr ""
"Transmisión deshabilitada para {model} tras un rechazo previo de la API; "
"reintentando sin transmisión."

#: gepetto/models/openai.py:404
msgid "Streaming rejected by API; retrying without streaming."
msgstr "La API rechazó la transmisión; reintentando sin transmisión."

#: gepetto/models/openai.py:410
msgid ""
"Unfortunately, this function is too big to be analyzed with the model's "
"current API limits."
msgstr ""
"Desafortunadamente, esta función es demasiado grande para ser analizada con "
"los límites actuales de la API del modelo."

#: gepetto/models/openai.py:422
#, python-brace-format
msgid "{model} could not complete the request: {error}"
msgstr "{model} no pudo completar la solicitud: {error}"

#~ msgid ""
#~ "Could not obtain valid data from the model, giving up. Dumping the response "
#~ "for manual import:"
#~ msgstr ""
#~ "No se pudo obtener datos válidos del modelo, abandonando. Volcando la "
#~ "respuesta para la importación manual:"

#~ msgid ""
#~ "Cannot extract valid JSON from the response. Asking the model to fix it..."
#~ msgstr ""
#~ "No se puede extraer un JSON válido de la respuesta. Pidiendo al modelo que "
#~ "lo arregle..."

#~ msgid "The JSON document returned is invalid. Asking the model to fix it..."
#~ msgstr ""
#~ "El documento JSON devuelto no es válido. Pidiendo al modelo que lo "
#~ "arregle..."

#~ msgid "Please fix the following JSON document:\n"
#~ msgstr "Por favor, arregle el siguiente documento JSON:\n"

#~ msgid ""
#~ "The JSON document provided in this response is invalid. Can you fix it?\n"
#~ "{response}"
#~ msgstr ""
#~ "El documento JSON proporcionado en esta respuesta no es válido. ¿Puede arreglarlo?\n"
#~ "{response}"

#~ msgid "{model} query finished! {replaced} variable(s) renamed."
#~ msgstr ""
#~ "Consulta de {model} finalizada! Se renombraron {replaced} variable(s)."

#~ msgid ""
#~ "Context length exceeded! Reducing the completion tokens to {max_tokens}..."
#~ msgstr ""
#~ "¡Límite de longitud del contexto excedido! Reduciendo los tokens de "
#~ "finalización a {max_tokens}..."

#~ msgid ""
#~ "Proxy configuration for Gemini via google-generativeai library might require"
#~ " manual setup of HTTPS_PROXY environment variable."
#~ msgstr ""
#~ "Proxy configuration for Gemini via google-generativeai library might require"
#~ " manual setup of HTTPS_PROXY environment variable."

#~ msgid ""
#~ "Please generate executable C code based on the following decompiled C code and ensure it includes all necessary header files and other information:\n"
#~ "{decompiler_output}"
#~ msgstr ""
#~ "Please generate executable C code based on the following decompiled C code and ensure it includes all necessary header files and other information:\n"
#~ "{decompiler_output}"

#~ msgid ""
#~ "Please generate equivalent Python code based on the following decompiled C code, and provide an example of the function call:\n"
#~ "{decompiler_output}"
#~ msgstr ""
#~ "Please generate equivalent Python code based on the following decompiled C code, and provide an example of the function call:\n"
#~ "{decompiler_output}"

#~ msgid ""
#~ "You are a helpful assistant embedded in IDA Pro. Your role is to facilitate "
#~ "reverse-engineering and answer programming questions."
#~ msgstr ""
#~ "Eres un asistente útil integrado en IDA Pro. Tu papel es facilitar la "
#~ "ingeniería inversa y responder preguntas de programación."

```

`gepetto/locales/fr_FR/LC_MESSAGES/gepetto.po`:

```po
# French translations for Gepetto.
# This file is distributed under the same license as Gepetto (GPLv3).
# Ivan Kwiatkowski, 2023.
#
# Translators:
# Ivan K., 2023
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: Gepetto 1.1\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-10-18 18:57+0000\n"
"PO-Revision-Date: 2025-10-18 18:59+0000\n"
"Last-Translator: Ivan K., 2023\n"
"Language-Team: French (https://app.transifex.com/gepetto/teams/164045/fr/)\n"
"Language: fr\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Plural-Forms: nplurals=3; plural=(n == 0 || n == 1) ? 0 : n != 0 && n % 1000000 == 0 ? 1 : 2;\n"
"Generated-By: Babel 2.17.0\n"

#: gepetto/config.py:82
msgid "Attempting to load the first available model..."
msgstr "Tentative de chargement du premier modèle disponible..."

#: gepetto/config.py:87
msgid "No model available. Please edit the configuration file and try again."
msgstr ""
"Aucun modèle disponible. Veuillez modifier le fichier de configuration et "
"réessayer."

#: gepetto/config.py:108
#, python-brace-format
msgid ""
"Warning: Gepetto's configuration doesn't contain option {option} in section "
"{section}!"
msgstr ""
"Attention: la configuration de Gepetto ne contient pas l'option {option} "
"dans la section {section} !"

#: gepetto/ida/cli.py:120
msgid "LLM chat"
msgstr "Conversation avec le LLM"

#: gepetto/ida/cli.py:152
#, python-brace-format
msgid "→ Model requested tool: {tool_name} ({tool_args}...)"
msgstr "→ Le modèle a demandé l'outil : {tool_name} ({tool_args}...)"

#: gepetto/ida/cli.py:167
#, python-brace-format
msgid "Using tool: {tool_name}"
msgstr "Utilisation de l'outil : {tool_name}"

#: gepetto/ida/cli.py:219
msgid "✔ Completed turn"
msgstr "✔ Tour terminé"

#: gepetto/ida/cli.py:227 gepetto/ida/cli.py:245
msgid "Model request failed."
msgstr "La requête du modèle a échoué."

#: gepetto/ida/cli.py:258
msgid "Streaming unavailable for this model; retrying without streaming."
msgstr "Diffusion indisponible pour ce modèle ; nouvel essai sans diffusion."

#: gepetto/ida/cli.py:261
msgid "Retrying without streaming"
msgstr "Nouvel essai sans diffusion"

#: gepetto/ida/comment_handler.py:39 gepetto/ida/comment_handler.py:41
msgid "No focused view available; cannot determine current function."
msgstr ""
"Aucune vue focalisée disponible ; impossible de déterminer la fonction "
"actuelle."

#: gepetto/ida/comment_handler.py:103
#, python-brace-format
msgid "[ERROR] comment callback JSON failure: {error}"
msgstr "[ERREUR] échec JSON du rappel de commentaire : {error}"

#: gepetto/ida/comment_handler.py:148
#, python-brace-format
msgid "[ERROR] comment application failed: {error}"
msgstr "[ERREUR] l'application des commentaires a échoué : {error}"

#: gepetto/ida/comment_handler.py:156
#, python-brace-format
msgid "Applied {count} comments."
msgstr "{count} commentaires appliqués."

#: gepetto/ida/comment_handler.py:162
msgid "No comments were applied."
msgstr "Aucun commentaire n'a été appliqué."

#: gepetto/ida/comment_handler.py:170
#, python-brace-format
msgid "[ERROR] comment_callback: {error}"
msgstr "[ERREUR] comment_callback : {error}"

#: gepetto/ida/handlers.py:40 gepetto/ida/handlers.py:46
msgid "Comment generated by Gepetto"
msgstr "Commentaire généré par Gepetto"

#: gepetto/ida/handlers.py:58
#, python-brace-format
msgid "[ERROR] comment callback failed: {error}"
msgstr "[ERREUR] le rappel de commentaire a échoué : {error}"

#: gepetto/ida/handlers.py:155
#, python-brace-format
msgid "[ERROR] rename callback JSON failure: {error}"
msgstr "[ERREUR] échec JSON du rappel de renommage : {error}"

#: gepetto/ida/handlers.py:165
#, python-brace-format
msgid "Function at EA 0x{ea:X} not found."
msgstr "Fonction introuvable à l'adresse EA 0x{ea:X}."

#: gepetto/ida/handlers.py:182
#, python-brace-format
msgid "Select names to rename"
msgstr "Sélectionnez les noms à renommer"

#: gepetto/ida/handlers.py:183
msgid "Old Name"
msgstr "Ancien nom"

#: gepetto/ida/handlers.py:183
msgid "New Name"
msgstr "Nouveau nom"

#: gepetto/ida/handlers.py:232
#, python-brace-format
msgid "[ERROR] rename callback failed: {error}"
msgstr "[ERREUR] le rappel de renommage a échoué : {error}"

#: gepetto/ida/handlers.py:238 gepetto/ida/handlers.py:239
msgid "Rename cancelled by user."
msgstr "Renommage annulé par l'utilisateur."

#: gepetto/ida/handlers.py:243
#, python-brace-format
msgid "Done! {count} name(s) renamed."
msgstr "Terminé ! {count} nom(s) renommé(s)."

#: gepetto/ida/handlers.py:309 gepetto/ida/status_panel.py:654
#, python-brace-format
msgid "Couldn't change model to {model}: {error}"
msgstr "Impossible de changer le modèle pour {model} : {error}"

#: gepetto/ida/handlers.py:367 gepetto/ida/handlers.py:424
#, python-brace-format
msgid "{model} generated code saved to {file_name}"
msgstr "Code généré par {model} enregistré dans {file_name}"

#: gepetto/ida/status_panel.py:20 gepetto/ida/status_panel.py:765
#: gepetto/ida/status_panel.py:773
msgid "Gepetto"
msgstr "Gepetto"

#: gepetto/ida/status_panel.py:48
msgid "System"
msgstr "Système"

#: gepetto/ida/status_panel.py:49
msgid "User"
msgstr "Utilisateur"

#: gepetto/ida/status_panel.py:50
msgid "Tool"
msgstr "Outil"

#: gepetto/ida/status_panel.py:51
msgid "Model"
msgstr "Modèle"

#: gepetto/ida/status_panel.py:52
msgid "Assistant"
msgstr "Assistant(e)"

#: gepetto/ida/status_panel.py:248
msgid "Log filters:"
msgstr "Filtres du journal :"

#: gepetto/ida/status_panel.py:266
msgid "Clear"
msgstr "Effacer"

#: gepetto/ida/status_panel.py:285
msgid "Type a prompt and press Enter…"
msgstr "Saisissez une requête puis appuyez sur Entrée…"

#: gepetto/ida/status_panel.py:288
msgid "Send"
msgstr "Envoyer"

#: gepetto/ida/status_panel.py:291 gepetto/ida/status_panel.py:690
msgid "Stop"
msgstr "Arrêter"

#: gepetto/ida/status_panel.py:315 gepetto/ida/status_panel.py:585
#, python-brace-format
msgid "Model: {model} ▼"
msgstr "Modèle : {model} ▼"

#: gepetto/ida/status_panel.py:321 gepetto/ida/status_panel.py:673
#, python-brace-format
msgid "Status: {status}"
msgstr "Statut : {status}"

#: gepetto/ida/status_panel.py:321 gepetto/ida/status_panel.py:672
#: gepetto/ida/status_panel.py:919
msgid "Idle"
msgstr "Au repos"

#: gepetto/ida/status_panel.py:499
#, python-brace-format
msgid "Failed to send prompt: {err}"
msgstr "Échec de l'envoi de la requête : {err}"

#: gepetto/ida/status_panel.py:620
msgid "Error loading models"
msgstr "Erreur lors du chargement des modèles"

#: gepetto/ida/status_panel.py:640
#, python-brace-format
msgid "Model switched to {model}"
msgstr "Modèle changé pour {model}"

#: gepetto/ida/status_panel.py:658
#, python-brace-format
msgid "Failed to switch model: {error}"
msgstr "Échec du changement de modèle : {error}"

#: gepetto/ida/status_panel.py:702
msgid "Error - Check Log"
msgstr "Erreur : consultez le journal"

#: gepetto/ida/status_panel.py:709
msgid "Cancelling…"
msgstr "Annulation…"

#: gepetto/ida/status_panel.py:711
msgid "Cancelling request"
msgstr "Annulation de la requête"

#: gepetto/ida/status_panel.py:751
msgid "You"
msgstr "Vous"

#: gepetto/ida/status_panel.py:753
#, python-brace-format
msgid "User: {text}"
msgstr "Utilisateur : {text}"

#: gepetto/ida/status_panel.py:759
#, python-brace-format
msgid "Assistant: {text}"
msgstr "Assistant : {text}"

#: gepetto/ida/status_panel.py:861
msgid "Could not show Gepetto Status panel."
msgstr "Impossible d'afficher le panneau d'état de Gepetto."

#: gepetto/ida/status_panel.py:917
msgid "Failed to cancel request"
msgstr "Échec de l'annulation de la requête"

#: gepetto/ida/status_panel.py:954 gepetto/ida/status_panel.py:993
msgid "Done"
msgstr "Terminé"

#: gepetto/ida/status_panel.py:981
#, python-brace-format
msgid "Request to {model} sent..."
msgstr "Requête envoyée à {model}..."

#: gepetto/ida/status_panel.py:983
msgid "Waiting for model..."
msgstr "En attente du modèle..."

#: gepetto/ida/status_panel.py:988
#, python-brace-format
msgid "{model} query finished in {time:.2f} seconds!"
msgstr "Requête {model} terminée en {time:.2f} secondes !"

#: gepetto/ida/ui.py:60 gepetto/ida/ui.py:93
msgid "Explain function"
msgstr "Expliquer la fonction"

#: gepetto/ida/ui.py:62 gepetto/ida/ui.py:103
msgid "Comment function"
msgstr "Expliquer la fonction"

#: gepetto/ida/ui.py:64 gepetto/ida/ui.py:113
msgid "Auto-rename"
msgstr "Renommage automatique"

#: gepetto/ida/ui.py:66 gepetto/ida/ui.py:137
msgid "Generate C Code"
msgstr "Générer du code C"

#: gepetto/ida/ui.py:68 gepetto/ida/ui.py:124
msgid "Generate Python Code"
msgstr "Générer du code Python"

#: gepetto/ida/ui.py:72
#, python-brace-format
msgid "Uses {model} to enrich the decompiler's output"
msgstr "Enrichit la sortie du décompilateur avec {model}"

#: gepetto/ida/ui.py:73
msgid "See usage instructions on GitHub"
msgstr "Aide du programme disponible sur GitHub"

#: gepetto/ida/ui.py:96
#, python-brace-format
msgid "Use {model} to explain the currently selected function"
msgstr "Utiliser {model} pour expliquer la fonction sélectionnée"

#: gepetto/ida/ui.py:106
#, python-brace-format
msgid "Adds comments to lines in the current function using {model}"
msgstr ""
"Ajoute des commentaires aux lignes de la fonction actuelle avec {model}"

#: gepetto/ida/ui.py:116
#, python-brace-format
msgid "Use {model} to auto-rename this function and its variables"
msgstr ""
"Utiliser {model} pour renommer automatiquement cette fonction et ses "
"variables"

#: gepetto/ida/ui.py:127
#, python-brace-format
msgid ""
"Generate python code from the currently selected function using {model}"
msgstr "Utiliser {model} pour expliquer la fonction sélectionnée"

#: gepetto/ida/ui.py:140
#, python-brace-format
msgid ""
"Generate executable C code from the currently selected function using "
"{model}"
msgstr ""
"Génère du code C exécutable pour la fonction sélectionnée avec {model}"

#: gepetto/ida/ui.py:168
msgid "Options"
msgstr "Paramètres"

#: gepetto/ida/ui.py:169 gepetto/ida/ui.py:274
msgid "Auto-open status panel"
msgstr "Ouvrir automatiquement le panneau d'état"

#: gepetto/ida/ui.py:208 gepetto/ida/ui.py:228
msgid "Select model"
msgstr "Sélectionner un modèle"

#: gepetto/ida/ui.py:277
msgid "Automatically focus the Gepetto status panel when a request starts."
msgstr ""
"Activer automatiquement le panneau d'état de Gepetto au début d'une requête."

#: gepetto/models/aliyun.py:62 gepetto/models/deepseek.py:36
#: gepetto/models/gemini.py:541 gepetto/models/groq.py:37
#: gepetto/models/kluster.py:58 gepetto/models/novita_ai.py:41
#: gepetto/models/openai.py:302 gepetto/models/openai_compatible.py:63
#: gepetto/models/openrouter.py:52 gepetto/models/siliconflow.py:60
#: gepetto/models/together.py:34
#, python-brace-format
msgid ""
"Please edit the configuration file to insert your {api_provider} API key!"
msgstr ""
"Merci d'ajouter votre clé API {api_provider} dans le fichier de "
"configuration !"

#: gepetto/models/gemini.py:111
#, python-brace-format
msgid "Failed to fetch Gemini models: {error}"
msgstr "Échec de la récupération des modèles Gemini : {error}"

#: gepetto/models/gemini.py:159
#, python-brace-format
msgid "Failed to configure proxy for Gemini models: {error}"
msgstr "Échec de la configuration du proxy pour les modèles Gemini : {error}"

#: gepetto/models/gemini.py:179 gepetto/models/local_lmstudio.py:140
#: gepetto/models/local_ollama.py:116 gepetto/models/openai.py:195
#, python-brace-format
msgid "Failed to fetch models from {base_url}: {status_code}"
msgstr ""
"Échec de la récupération des modèles depuis {base_url} : {status_code}"

#: gepetto/models/gemini.py:723 gepetto/models/openai.py:414
#: gepetto/models/openai.py:431
#, python-brace-format
msgid "General exception encountered while running the query: {error}"
msgstr ""
"Erreur générale rencontrée lors de l'exécution de la requête : {error}"

#: gepetto/models/local_lmstudio.py:62 gepetto/models/local_ollama.py:60
#, python-brace-format
msgid "Failed to fetch models from {base_url}: {error}"
msgstr "Échec de la récupération des modèles depuis {base_url} : {error}"

#: gepetto/models/local_lmstudio.py:123
#, python-brace-format
msgid "Failed to configure proxy for LM Studio models: {error}"
msgstr ""
"Échec de la configuration du proxy pour les modèles LM Studio : {error}"

#: gepetto/models/openai.py:120
#, python-brace-format
msgid "Failed to fetch OpenAI models: {error}"
msgstr "Échec de la récupération des modèles OpenAI : {error}"

#: gepetto/models/openai.py:178
#, python-brace-format
msgid "Failed to configure proxy for OpenAI models: {error}"
msgstr "Échec de la configuration du proxy pour les modèles OpenAI : {error}"

#: gepetto/models/openai.py:336
#, python-brace-format
msgid ""
"Streaming rejected for {model}: {error}\n"
"Retrying without streaming."
msgstr "Diffusion refusée pour {model} : {error}\\nNouvel essai sans diffusion."

#: gepetto/models/openai.py:340
#, python-brace-format
msgid ""
"Streaming disabled for {model} after a previous API rejection; retrying "
"without streaming."
msgstr ""
"Diffusion désactivée pour {model} après un rejet précédent de l'API ; nouvel"
" essai sans diffusion."

#: gepetto/models/openai.py:404
msgid "Streaming rejected by API; retrying without streaming."
msgstr "La diffusion a été refusée par l'API ; nouvel essai sans diffusion."

#: gepetto/models/openai.py:410
msgid ""
"Unfortunately, this function is too big to be analyzed with the model's "
"current API limits."
msgstr ""
"Malheureusement, les limites de l'API du modèle ne permettent pas d'analyser"
" une fonction aussi grande."

#: gepetto/models/openai.py:422
#, python-brace-format
msgid "{model} could not complete the request: {error}"
msgstr "{model} n'a pas pu satisfaire la requête : {error}"

#~ msgid ""
#~ "Could not obtain valid data from the model, giving up. Dumping the response "
#~ "for manual import:"
#~ msgstr ""
#~ "Impossible d'obtenir une réponse valide du modèle et abandon. La réponse est"
#~ " reproduite ci-dessous pour import manuel :"

#~ msgid ""
#~ "Cannot extract valid JSON from the response. Asking the model to fix it..."
#~ msgstr ""
#~ "Impossible d'extraire un document JSON valide de la réponse. Envoi d'une "
#~ "requête au modèle pour le réparer..."

#~ msgid "The JSON document returned is invalid. Asking the model to fix it..."
#~ msgstr ""
#~ "Le document JSON retourné par le modèle est invalide. Envoi d'une requête "
#~ "pour le réparer..."

#~ msgid "Please fix the following JSON document:\n"
#~ msgstr "Peux-tu réparer le document JSON suivant ?\n"

#~ msgid ""
#~ "The JSON document provided in this response is invalid. Can you fix it?\n"
#~ "{response}"
#~ msgstr ""
#~ "Le document JSON fourni dans cette réponse est invalide. Peux-tu le réparer "
#~ "? {response}"

#~ msgid "{model} query finished! {replaced} variable(s) renamed."
#~ msgstr "Requête vers {model} achevée ! {replaced} variable(s) remplacée(s)."

#~ msgid ""
#~ "Context length exceeded! Reducing the completion tokens to {max_tokens}..."
#~ msgstr ""
#~ "Longueur maximale de contenu dépassée ! Nouvel essai en réduisant le nombre "
#~ "de jetons de complétion demandés à {max_tokens}..."

#~ msgid ""
#~ "Proxy configuration for Gemini via google-generativeai library might require"
#~ " manual setup of HTTPS_PROXY environment variable."
#~ msgstr ""
#~ "Proxy configuration for Gemini via google-generativeai library might require"
#~ " manual setup of HTTPS_PROXY environment variable."

#~ msgid ""
#~ "Please generate executable C code based on the following decompiled C code and ensure it includes all necessary header files and other information:\n"
#~ "{decompiler_output}"
#~ msgstr ""
#~ "Please generate executable C code based on the following decompiled C code and ensure it includes all necessary header files and other information:\n"
#~ "{decompiler_output}"

#~ msgid ""
#~ "Please generate equivalent Python code based on the following decompiled C code, and provide an example of the function call:\n"
#~ "{decompiler_output}"
#~ msgstr ""
#~ "Please generate equivalent Python code based on the following decompiled C code, and provide an example of the function call:\n"
#~ "{decompiler_output}"

#~ msgid ""
#~ "You are a helpful assistant embedded in IDA Pro. Your role is to facilitate "
#~ "reverse-engineering and answer programming questions."
#~ msgstr ""
#~ "Tu es un assistant serviable intégré dans IDA Pro. Ton rôle est de faciliter"
#~ " la rétro-ingénierie et répondre aux questions liées à la programmation."

```

`gepetto/locales/generate_mo_files.sh`:

```sh
for PO_FILE in */LC_MESSAGES/*.po
do
    MO_FILE="${PO_FILE/.po/.mo}"
    "C:\Program Files (x86)\GnuWin32\bin\msgfmt.exe" -o "$MO_FILE" "$PO_FILE"
done

```

`gepetto/locales/gepetto.pot`:

```pot
# Translations template for PROJECT.
# Copyright (C) 2025 ORGANIZATION
# This file is distributed under the same license as the PROJECT project.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2025.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: PROJECT VERSION\n"
"Report-Msgid-Bugs-To: EMAIL@ADDRESS\n"
"POT-Creation-Date: 2025-10-18 18:57+0000\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.17.0\n"

#: gepetto/config.py:82
msgid "Attempting to load the first available model..."
msgstr ""

#: gepetto/config.py:87
msgid "No model available. Please edit the configuration file and try again."
msgstr ""

#: gepetto/config.py:108
#, python-brace-format
msgid ""
"Warning: Gepetto's configuration doesn't contain option {option} in "
"section {section}!"
msgstr ""

#: gepetto/ida/cli.py:120
msgid "LLM chat"
msgstr ""

#: gepetto/ida/cli.py:152
#, python-brace-format
msgid "→ Model requested tool: {tool_name} ({tool_args}...)"
msgstr ""

#: gepetto/ida/cli.py:167
#, python-brace-format
msgid "Using tool: {tool_name}"
msgstr ""

#: gepetto/ida/cli.py:219
msgid "✔ Completed turn"
msgstr ""

#: gepetto/ida/cli.py:227 gepetto/ida/cli.py:245
msgid "Model request failed."
msgstr ""

#: gepetto/ida/cli.py:258
msgid "Streaming unavailable for this model; retrying without streaming."
msgstr ""

#: gepetto/ida/cli.py:261
msgid "Retrying without streaming"
msgstr ""

#: gepetto/ida/comment_handler.py:39 gepetto/ida/comment_handler.py:41
msgid "No focused view available; cannot determine current function."
msgstr ""

#: gepetto/ida/comment_handler.py:103
#, python-brace-format
msgid "[ERROR] comment callback JSON failure: {error}"
msgstr ""

#: gepetto/ida/comment_handler.py:148
#, python-brace-format
msgid "[ERROR] comment application failed: {error}"
msgstr ""

#: gepetto/ida/comment_handler.py:156
#, python-brace-format
msgid "Applied {count} comments."
msgstr ""

#: gepetto/ida/comment_handler.py:162
msgid "No comments were applied."
msgstr ""

#: gepetto/ida/comment_handler.py:170
#, python-brace-format
msgid "[ERROR] comment_callback: {error}"
msgstr ""

#: gepetto/ida/handlers.py:40 gepetto/ida/handlers.py:46
msgid "Comment generated by Gepetto"
msgstr ""

#: gepetto/ida/handlers.py:58
#, python-brace-format
msgid "[ERROR] comment callback failed: {error}"
msgstr ""

#: gepetto/ida/handlers.py:155
#, python-brace-format
msgid "[ERROR] rename callback JSON failure: {error}"
msgstr ""

#: gepetto/ida/handlers.py:165
#, python-brace-format
msgid "Function at EA 0x{ea:X} not found."
msgstr ""

#: gepetto/ida/handlers.py:182
msgid "Select names to rename"
msgstr ""

#: gepetto/ida/handlers.py:183
msgid "Old Name"
msgstr ""

#: gepetto/ida/handlers.py:183
msgid "New Name"
msgstr ""

#: gepetto/ida/handlers.py:232
#, python-brace-format
msgid "[ERROR] rename callback failed: {error}"
msgstr ""

#: gepetto/ida/handlers.py:238 gepetto/ida/handlers.py:239
msgid "Rename cancelled by user."
msgstr ""

#: gepetto/ida/handlers.py:243
#, python-brace-format
msgid "Done! {count} name(s) renamed."
msgstr ""

#: gepetto/ida/handlers.py:309 gepetto/ida/status_panel.py:654
#, python-brace-format
msgid "Couldn't change model to {model}: {error}"
msgstr ""

#: gepetto/ida/handlers.py:367 gepetto/ida/handlers.py:424
#, python-brace-format
msgid "{model} generated code saved to {file_name}"
msgstr ""

#: gepetto/ida/status_panel.py:20 gepetto/ida/status_panel.py:765
#: gepetto/ida/status_panel.py:773
msgid "Gepetto"
msgstr ""

#: gepetto/ida/status_panel.py:48
msgid "System"
msgstr ""

#: gepetto/ida/status_panel.py:49
msgid "User"
msgstr ""

#: gepetto/ida/status_panel.py:50
msgid "Tool"
msgstr ""

#: gepetto/ida/status_panel.py:51
msgid "Model"
msgstr ""

#: gepetto/ida/status_panel.py:52
msgid "Assistant"
msgstr ""

#: gepetto/ida/status_panel.py:248
msgid "Log filters:"
msgstr ""

#: gepetto/ida/status_panel.py:266
msgid "Clear"
msgstr ""

#: gepetto/ida/status_panel.py:285
msgid "Type a prompt and press Enter…"
msgstr ""

#: gepetto/ida/status_panel.py:288
msgid "Send"
msgstr ""

#: gepetto/ida/status_panel.py:291 gepetto/ida/status_panel.py:690
msgid "Stop"
msgstr ""

#: gepetto/ida/status_panel.py:315 gepetto/ida/status_panel.py:585
#, python-brace-format
msgid "Model: {model} ▼"
msgstr ""

#: gepetto/ida/status_panel.py:321 gepetto/ida/status_panel.py:673
#, python-brace-format
msgid "Status: {status}"
msgstr ""

#: gepetto/ida/status_panel.py:321 gepetto/ida/status_panel.py:672
#: gepetto/ida/status_panel.py:919
msgid "Idle"
msgstr ""

#: gepetto/ida/status_panel.py:499
#, python-brace-format
msgid "Failed to send prompt: {err}"
msgstr ""

#: gepetto/ida/status_panel.py:620
msgid "Error loading models"
msgstr ""

#: gepetto/ida/status_panel.py:640
#, python-brace-format
msgid "Model switched to {model}"
msgstr ""

#: gepetto/ida/status_panel.py:658
#, python-brace-format
msgid "Failed to switch model: {error}"
msgstr ""

#: gepetto/ida/status_panel.py:702
msgid "Error - Check Log"
msgstr ""

#: gepetto/ida/status_panel.py:709
msgid "Cancelling…"
msgstr ""

#: gepetto/ida/status_panel.py:711
msgid "Cancelling request"
msgstr ""

#: gepetto/ida/status_panel.py:751
msgid "You"
msgstr ""

#: gepetto/ida/status_panel.py:753
#, python-brace-format
msgid "User: {text}"
msgstr ""

#: gepetto/ida/status_panel.py:759
#, python-brace-format
msgid "Assistant: {text}"
msgstr ""

#: gepetto/ida/status_panel.py:861
msgid "Could not show Gepetto Status panel."
msgstr ""

#: gepetto/ida/status_panel.py:917
msgid "Failed to cancel request"
msgstr ""

#: gepetto/ida/status_panel.py:954 gepetto/ida/status_panel.py:993
msgid "Done"
msgstr ""

#: gepetto/ida/status_panel.py:981
#, python-brace-format
msgid "Request to {model} sent..."
msgstr ""

#: gepetto/ida/status_panel.py:983
msgid "Waiting for model..."
msgstr ""

#: gepetto/ida/status_panel.py:988
#, python-brace-format
msgid "{model} query finished in {time:.2f} seconds!"
msgstr ""

#: gepetto/ida/ui.py:60 gepetto/ida/ui.py:93
msgid "Explain function"
msgstr ""

#: gepetto/ida/ui.py:62 gepetto/ida/ui.py:103
msgid "Comment function"
msgstr ""

#: gepetto/ida/ui.py:64 gepetto/ida/ui.py:113
msgid "Auto-rename"
msgstr ""

#: gepetto/ida/ui.py:66 gepetto/ida/ui.py:137
msgid "Generate C Code"
msgstr ""

#: gepetto/ida/ui.py:68 gepetto/ida/ui.py:124
msgid "Generate Python Code"
msgstr ""

#: gepetto/ida/ui.py:72
#, python-brace-format
msgid "Uses {model} to enrich the decompiler's output"
msgstr ""

#: gepetto/ida/ui.py:73
msgid "See usage instructions on GitHub"
msgstr ""

#: gepetto/ida/ui.py:96
#, python-brace-format
msgid "Use {model} to explain the currently selected function"
msgstr ""

#: gepetto/ida/ui.py:106
#, python-brace-format
msgid "Adds comments to lines in the current function using {model}"
msgstr ""

#: gepetto/ida/ui.py:116
#, python-brace-format
msgid "Use {model} to auto-rename this function and its variables"
msgstr ""

#: gepetto/ida/ui.py:127
#, python-brace-format
msgid "Generate python code from the currently selected function using {model}"
msgstr ""

#: gepetto/ida/ui.py:140
#, python-brace-format
msgid ""
"Generate executable C code from the currently selected function using "
"{model}"
msgstr ""

#: gepetto/ida/ui.py:168
msgid "Options"
msgstr ""

#: gepetto/ida/ui.py:169 gepetto/ida/ui.py:274
msgid "Auto-open status panel"
msgstr ""

#: gepetto/ida/ui.py:208 gepetto/ida/ui.py:228
msgid "Select model"
msgstr ""

#: gepetto/ida/ui.py:277
msgid "Automatically focus the Gepetto status panel when a request starts."
msgstr ""

#: gepetto/models/aliyun.py:62 gepetto/models/deepseek.py:36
#: gepetto/models/gemini.py:541 gepetto/models/groq.py:37
#: gepetto/models/kluster.py:58 gepetto/models/novita_ai.py:41
#: gepetto/models/openai.py:302 gepetto/models/openai_compatible.py:63
#: gepetto/models/openrouter.py:52 gepetto/models/siliconflow.py:60
#: gepetto/models/together.py:34
#, python-brace-format
msgid "Please edit the configuration file to insert your {api_provider} API key!"
msgstr ""

#: gepetto/models/gemini.py:111
#, python-brace-format
msgid "Failed to fetch Gemini models: {error}"
msgstr ""

#: gepetto/models/gemini.py:159
#, python-brace-format
msgid "Failed to configure proxy for Gemini models: {error}"
msgstr ""

#: gepetto/models/gemini.py:179 gepetto/models/local_lmstudio.py:140
#: gepetto/models/local_ollama.py:116 gepetto/models/openai.py:195
#, python-brace-format
msgid "Failed to fetch models from {base_url}: {status_code}"
msgstr ""

#: gepetto/models/gemini.py:723 gepetto/models/openai.py:414
#: gepetto/models/openai.py:431
#, python-brace-format
msgid "General exception encountered while running the query: {error}"
msgstr ""

#: gepetto/models/local_lmstudio.py:62 gepetto/models/local_ollama.py:60
#, python-brace-format
msgid "Failed to fetch models from {base_url}: {error}"
msgstr ""

#: gepetto/models/local_lmstudio.py:123
#, python-brace-format
msgid "Failed to configure proxy for LM Studio models: {error}"
msgstr ""

#: gepetto/models/openai.py:120
#, python-brace-format
msgid "Failed to fetch OpenAI models: {error}"
msgstr ""

#: gepetto/models/openai.py:178
#, python-brace-format
msgid "Failed to configure proxy for OpenAI models: {error}"
msgstr ""

#: gepetto/models/openai.py:336
#, python-brace-format
msgid ""
"Streaming rejected for {model}: {error}\n"
"Retrying without streaming."
msgstr ""

#: gepetto/models/openai.py:340
#, python-brace-format
msgid ""
"Streaming disabled for {model} after a previous API rejection; retrying "
"without streaming."
msgstr ""

#: gepetto/models/openai.py:404
msgid "Streaming rejected by API; retrying without streaming."
msgstr ""

#: gepetto/models/openai.py:410
msgid ""
"Unfortunately, this function is too big to be analyzed with the model's "
"current API limits."
msgstr ""

#: gepetto/models/openai.py:422
#, python-brace-format
msgid "{model} could not complete the request: {error}"
msgstr ""


```

`gepetto/locales/it_IT/LC_MESSAGES/gepetto.po`:

```po
# Italian (Italy) translations for Gepetto.
# This file is distributed under the same license as Gepetto (GPLv3).
# Ivan Kwiatkowski, 2023.
#
# Translators:
# Ivan K., 2023
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: Gepetto 1.1\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-10-18 18:57+0000\n"
"PO-Revision-Date: 2025-10-18 18:59+0000\n"
"Last-Translator: Ivan K., 2023\n"
"Language-Team: Italian (Italy) (https://app.transifex.com/gepetto/teams/164045/it_IT/)\n"
"Language: it_IT\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Plural-Forms: nplurals=3; plural=n == 1 ? 0 : n != 0 && n % 1000000 == 0 ? 1 : 2;\n"
"Generated-By: Babel 2.17.0\n"

#: gepetto/config.py:82
msgid "Attempting to load the first available model..."
msgstr "Tentativo di caricare il primo modello disponibile..."

#: gepetto/config.py:87
msgid "No model available. Please edit the configuration file and try again."
msgstr ""
"Nessun modello disponibile. Modifica il file di configurazione e riprova."

#: gepetto/config.py:108
#, python-brace-format
msgid ""
"Warning: Gepetto's configuration doesn't contain option {option} in section "
"{section}!"
msgstr ""
"Avviso: La configurazione di Gepetto non contiene l'opzione {option} nella "
"sezione {section}!"

#: gepetto/ida/cli.py:120
msgid "LLM chat"
msgstr "Conversazione LLM"

#: gepetto/ida/cli.py:152
#, python-brace-format
msgid "→ Model requested tool: {tool_name} ({tool_args}...)"
msgstr "→ Il modello ha richiesto lo strumento: {tool_name} ({tool_args}...)"

#: gepetto/ida/cli.py:167
#, python-brace-format
msgid "Using tool: {tool_name}"
msgstr "Uso dello strumento: {tool_name}"

#: gepetto/ida/cli.py:219
msgid "✔ Completed turn"
msgstr "✔ Turno completato"

#: gepetto/ida/cli.py:227 gepetto/ida/cli.py:245
msgid "Model request failed."
msgstr "La richiesta al modello è fallita."

#: gepetto/ida/cli.py:258
msgid "Streaming unavailable for this model; retrying without streaming."
msgstr ""
"Lo streaming non è disponibile per questo modello; nuovo tentativo senza "
"streaming."

#: gepetto/ida/cli.py:261
msgid "Retrying without streaming"
msgstr "Nuovo tentativo senza streaming"

#: gepetto/ida/comment_handler.py:39 gepetto/ida/comment_handler.py:41
msgid "No focused view available; cannot determine current function."
msgstr ""
"Nessuna vista con focus disponibile; impossibile determinare la funzione "
"corrente."

#: gepetto/ida/comment_handler.py:103
#, python-brace-format
msgid "[ERROR] comment callback JSON failure: {error}"
msgstr "[ERRORE] errore JSON del callback dei commenti: {error}"

#: gepetto/ida/comment_handler.py:148
#, python-brace-format
msgid "[ERROR] comment application failed: {error}"
msgstr "[ERRORE] applicazione dei commenti non riuscita: {error}"

#: gepetto/ida/comment_handler.py:156
#, python-brace-format
msgid "Applied {count} comments."
msgstr "Applicati {count} commenti."

#: gepetto/ida/comment_handler.py:162
msgid "No comments were applied."
msgstr "Nessun commento è stato applicato."

#: gepetto/ida/comment_handler.py:170
#, python-brace-format
msgid "[ERROR] comment_callback: {error}"
msgstr "[ERRORE] comment_callback: {error}"

#: gepetto/ida/handlers.py:40 gepetto/ida/handlers.py:46
msgid "Comment generated by Gepetto"
msgstr "Commento generato da Geppetto"

#: gepetto/ida/handlers.py:58
#, python-brace-format
msgid "[ERROR] comment callback failed: {error}"
msgstr "[ERRORE] callback dei commenti non riuscito: {error}"

#: gepetto/ida/handlers.py:155
#, python-brace-format
msgid "[ERROR] rename callback JSON failure: {error}"
msgstr "[ERRORE] errore JSON del callback di rinomina: {error}"

#: gepetto/ida/handlers.py:165
#, python-brace-format
msgid "Function at EA 0x{ea:X} not found."
msgstr "Funzione a EA 0x{ea:X} non trovata."

#: gepetto/ida/handlers.py:182
#, python-brace-format
msgid "Select names to rename"
msgstr "Seleziona i nomi da rinominare"

#: gepetto/ida/handlers.py:183
msgid "Old Name"
msgstr "Vecchio nome"

#: gepetto/ida/handlers.py:183
msgid "New Name"
msgstr "Nuovo nome"

#: gepetto/ida/handlers.py:232
#, python-brace-format
msgid "[ERROR] rename callback failed: {error}"
msgstr "[ERRORE] callback di rinomina non riuscito: {error}"

#: gepetto/ida/handlers.py:238 gepetto/ida/handlers.py:239
msgid "Rename cancelled by user."
msgstr "Rinomina annullata dall'utente."

#: gepetto/ida/handlers.py:243
#, python-brace-format
msgid "Done! {count} name(s) renamed."
msgstr "Fatto! Rinominati {count} nome(i)."

#: gepetto/ida/handlers.py:309 gepetto/ida/status_panel.py:654
#, python-brace-format
msgid "Couldn't change model to {model}: {error}"
msgstr "Impossibile cambiare il modello in {model}: {error}"

#: gepetto/ida/handlers.py:367 gepetto/ida/handlers.py:424
#, python-brace-format
msgid "{model} generated code saved to {file_name}"
msgstr "Codice generato da {model} salvato in {file_name}"

#: gepetto/ida/status_panel.py:20 gepetto/ida/status_panel.py:765
#: gepetto/ida/status_panel.py:773
msgid "Gepetto"
msgstr "Gepetto"

#: gepetto/ida/status_panel.py:48
msgid "System"
msgstr "Sistema"

#: gepetto/ida/status_panel.py:49
msgid "User"
msgstr "Utente"

#: gepetto/ida/status_panel.py:50
msgid "Tool"
msgstr "Strumento"

#: gepetto/ida/status_panel.py:51
msgid "Model"
msgstr "Modello"

#: gepetto/ida/status_panel.py:52
msgid "Assistant"
msgstr "Assistente"

#: gepetto/ida/status_panel.py:248
msgid "Log filters:"
msgstr "Filtri del log:"

#: gepetto/ida/status_panel.py:266
msgid "Clear"
msgstr "Cancella"

#: gepetto/ida/status_panel.py:285
msgid "Type a prompt and press Enter…"
msgstr "Digita un prompt e premi Invio…"

#: gepetto/ida/status_panel.py:288
msgid "Send"
msgstr "Invia"

#: gepetto/ida/status_panel.py:291 gepetto/ida/status_panel.py:690
msgid "Stop"
msgstr "Ferma"

#: gepetto/ida/status_panel.py:315 gepetto/ida/status_panel.py:585
#, python-brace-format
msgid "Model: {model} ▼"
msgstr "Modello: {model} ▼"

#: gepetto/ida/status_panel.py:321 gepetto/ida/status_panel.py:673
#, python-brace-format
msgid "Status: {status}"
msgstr "Stato: {status}"

#: gepetto/ida/status_panel.py:321 gepetto/ida/status_panel.py:672
#: gepetto/ida/status_panel.py:919
msgid "Idle"
msgstr "Inattivo"

#: gepetto/ida/status_panel.py:499
#, python-brace-format
msgid "Failed to send prompt: {err}"
msgstr "Errore nell'invio della richiesta: {err}"

#: gepetto/ida/status_panel.py:620
msgid "Error loading models"
msgstr "Errore nel caricamento dei modelli"

#: gepetto/ida/status_panel.py:640
#, python-brace-format
msgid "Model switched to {model}"
msgstr "Modello cambiato in {model}"

#: gepetto/ida/status_panel.py:658
#, python-brace-format
msgid "Failed to switch model: {error}"
msgstr "Errore nel cambio del modello: {error}"

#: gepetto/ida/status_panel.py:702
msgid "Error - Check Log"
msgstr "Errore: controlla il log"

#: gepetto/ida/status_panel.py:709
msgid "Cancelling…"
msgstr "Annullamento…"

#: gepetto/ida/status_panel.py:711
msgid "Cancelling request"
msgstr "Annullamento della richiesta"

#: gepetto/ida/status_panel.py:751
msgid "You"
msgstr "Tu"

#: gepetto/ida/status_panel.py:753
#, python-brace-format
msgid "User: {text}"
msgstr "Utente: {text}"

#: gepetto/ida/status_panel.py:759
#, python-brace-format
msgid "Assistant: {text}"
msgstr "Assistente: {text}"

#: gepetto/ida/status_panel.py:861
msgid "Could not show Gepetto Status panel."
msgstr "Impossibile mostrare il pannello di stato di Gepetto."

#: gepetto/ida/status_panel.py:917
msgid "Failed to cancel request"
msgstr "Impossibile annullare la richiesta"

#: gepetto/ida/status_panel.py:954 gepetto/ida/status_panel.py:993
msgid "Done"
msgstr "Fatto"

#: gepetto/ida/status_panel.py:981
#, python-brace-format
msgid "Request to {model} sent..."
msgstr "Richiesta inviata a {model}..."

#: gepetto/ida/status_panel.py:983
msgid "Waiting for model..."
msgstr "In attesa del modello..."

#: gepetto/ida/status_panel.py:988
#, python-brace-format
msgid "{model} query finished in {time:.2f} seconds!"
msgstr "Richiesta a {model} completata in {time:.2f} secondi!"

#: gepetto/ida/ui.py:60 gepetto/ida/ui.py:93
msgid "Explain function"
msgstr "Descrivi la funzione"

#: gepetto/ida/ui.py:62 gepetto/ida/ui.py:103
msgid "Comment function"
msgstr "Descrivi la funzione"

#: gepetto/ida/ui.py:64 gepetto/ida/ui.py:113
msgid "Auto-rename"
msgstr "Rinominazione automatica"

#: gepetto/ida/ui.py:66 gepetto/ida/ui.py:137
msgid "Generate C Code"
msgstr "Genera codice C"

#: gepetto/ida/ui.py:68 gepetto/ida/ui.py:124
msgid "Generate Python Code"
msgstr "Genera codice Python"

#: gepetto/ida/ui.py:72
#, python-brace-format
msgid "Uses {model} to enrich the decompiler's output"
msgstr "Usa {model} per arricchire il codice decompilato"

#: gepetto/ida/ui.py:73
msgid "See usage instructions on GitHub"
msgstr "Consulta le istruzioni d'uso su GitHub"

#: gepetto/ida/ui.py:96
#, python-brace-format
msgid "Use {model} to explain the currently selected function"
msgstr "Usa {model} per descrivere la funzione selezionata"

#: gepetto/ida/ui.py:106
#, python-brace-format
msgid "Adds comments to lines in the current function using {model}"
msgstr "Aggiunge commenti alle righe della funzione corrente con {model}"

#: gepetto/ida/ui.py:116
#, python-brace-format
msgid "Use {model} to auto-rename this function and its variables"
msgstr ""
"Usa {model} per rinominare automaticamente questa funzione e le sue "
"variabili"

#: gepetto/ida/ui.py:127
#, python-brace-format
msgid ""
"Generate python code from the currently selected function using {model}"
msgstr "Usa {model} per descrivere la funzione selezionata"

#: gepetto/ida/ui.py:140
#, python-brace-format
msgid ""
"Generate executable C code from the currently selected function using "
"{model}"
msgstr "Genera codice C eseguibile dalla funzione selezionata usando {model}"

#: gepetto/ida/ui.py:168
msgid "Options"
msgstr "Opzioni"

#: gepetto/ida/ui.py:169 gepetto/ida/ui.py:274
msgid "Auto-open status panel"
msgstr "Apri automaticamente il pannello di stato"

#: gepetto/ida/ui.py:208 gepetto/ida/ui.py:228
msgid "Select model"
msgstr "Seleziona modello"

#: gepetto/ida/ui.py:277
msgid "Automatically focus the Gepetto status panel when a request starts."
msgstr ""
"Metti automaticamente a fuoco il pannello di stato di Gepetto all'avvio di "
"una richiesta."

#: gepetto/models/aliyun.py:62 gepetto/models/deepseek.py:36
#: gepetto/models/gemini.py:541 gepetto/models/groq.py:37
#: gepetto/models/kluster.py:58 gepetto/models/novita_ai.py:41
#: gepetto/models/openai.py:302 gepetto/models/openai_compatible.py:63
#: gepetto/models/openrouter.py:52 gepetto/models/siliconflow.py:60
#: gepetto/models/together.py:34
#, python-brace-format
msgid ""
"Please edit the configuration file to insert your {api_provider} API key!"
msgstr "Per favore, modifica lo script insendo la tua {api_provider} API key!"

#: gepetto/models/gemini.py:111
#, python-brace-format
msgid "Failed to fetch Gemini models: {error}"
msgstr "Impossibile recuperare i modelli Gemini: {error}"

#: gepetto/models/gemini.py:159
#, python-brace-format
msgid "Failed to configure proxy for Gemini models: {error}"
msgstr "Impossibile configurare il proxy per i modelli Gemini: {error}"

#: gepetto/models/gemini.py:179 gepetto/models/local_lmstudio.py:140
#: gepetto/models/local_ollama.py:116 gepetto/models/openai.py:195
#, python-brace-format
msgid "Failed to fetch models from {base_url}: {status_code}"
msgstr "Impossibile recuperare modelli da {base_url}: {status_code}"

#: gepetto/models/gemini.py:723 gepetto/models/openai.py:414
#: gepetto/models/openai.py:431
#, python-brace-format
msgid "General exception encountered while running the query: {error}"
msgstr "Eccezione rilevata durante l'esecuzione della query: {error}"

#: gepetto/models/local_lmstudio.py:62 gepetto/models/local_ollama.py:60
#, python-brace-format
msgid "Failed to fetch models from {base_url}: {error}"
msgstr "Impossibile recuperare modelli da {base_url}: {error}"

#: gepetto/models/local_lmstudio.py:123
#, python-brace-format
msgid "Failed to configure proxy for LM Studio models: {error}"
msgstr "Impossibile configurare il proxy per i modelli LM Studio: {error}"

#: gepetto/models/openai.py:120
#, python-brace-format
msgid "Failed to fetch OpenAI models: {error}"
msgstr "Impossibile recuperare i modelli OpenAI: {error}"

#: gepetto/models/openai.py:178
#, python-brace-format
msgid "Failed to configure proxy for OpenAI models: {error}"
msgstr "Impossibile configurare il proxy per i modelli OpenAI: {error}"

#: gepetto/models/openai.py:336
#, python-brace-format
msgid ""
"Streaming rejected for {model}: {error}\n"
"Retrying without streaming."
msgstr ""
"Streaming rifiutato per {model}: {error}\\nNuovo tentativo senza streaming."

#: gepetto/models/openai.py:340
#, python-brace-format
msgid ""
"Streaming disabled for {model} after a previous API rejection; retrying "
"without streaming."
msgstr ""
"Streaming disabilitato per {model} dopo un precedente rifiuto dell'API; "
"nuovo tentativo senza streaming."

#: gepetto/models/openai.py:404
msgid "Streaming rejected by API; retrying without streaming."
msgstr "Streaming rifiutato dall'API; nuovo tentativo senza streaming."

#: gepetto/models/openai.py:410
msgid ""
"Unfortunately, this function is too big to be analyzed with the model's "
"current API limits."
msgstr ""
"Sfortunatamente, questa funzione è troppo grande per essere analizzata con "
"gli attuali limiti API del modello."

#: gepetto/models/openai.py:422
#, python-brace-format
msgid "{model} could not complete the request: {error}"
msgstr "{model} non può soddisfare la richiesta: {error}"

#~ msgid ""
#~ "Could not obtain valid data from the model, giving up. Dumping the response "
#~ "for manual import:"
#~ msgstr ""
#~ "Impossibile ottenere dati validi dal modello. L'operazione è stata "
#~ "interrotta. Riporto di seguito la risposta per l'importazione manuale:"

#~ msgid ""
#~ "Cannot extract valid JSON from the response. Asking the model to fix it..."
#~ msgstr ""
#~ "Impossibile estrarre un JSON valido dalla risposta. Chiedo al modello di "
#~ "aggiustarlo..."

#~ msgid "The JSON document returned is invalid. Asking the model to fix it..."
#~ msgstr ""
#~ "Il documento JSON ottenuto non è valido. Chiedo al modello di aggiustarlo..."

#~ msgid "Please fix the following JSON document:\n"
#~ msgstr "Per favore aggiusta il seguente documento JSON:\n"

#~ msgid ""
#~ "The JSON document provided in this response is invalid. Can you fix it?\n"
#~ "{response}"
#~ msgstr ""
#~ "Il documento JSON fornito in questa risposta non è valido. Potresti aggiustarlo?\n"
#~ "{response}"

#~ msgid "{model} query finished! {replaced} variable(s) renamed."
#~ msgstr "{model} query terminata! {replaced} variabili rinominate."

#~ msgid ""
#~ "Context length exceeded! Reducing the completion tokens to {max_tokens}..."
#~ msgstr ""
#~ "Lunghezza massima del contenuto superata! Riprova riducendo il numero dei "
#~ "token di completamento a {max_tokens}..."

#~ msgid ""
#~ "Proxy configuration for Gemini via google-generativeai library might require"
#~ " manual setup of HTTPS_PROXY environment variable."
#~ msgstr ""
#~ "Proxy configuration for Gemini via google-generativeai library might require"
#~ " manual setup of HTTPS_PROXY environment variable."

#~ msgid ""
#~ "Please generate executable C code based on the following decompiled C code and ensure it includes all necessary header files and other information:\n"
#~ "{decompiler_output}"
#~ msgstr ""
#~ "Please generate executable C code based on the following decompiled C code and ensure it includes all necessary header files and other information:\n"
#~ "{decompiler_output}"

#~ msgid ""
#~ "Please generate equivalent Python code based on the following decompiled C code, and provide an example of the function call:\n"
#~ "{decompiler_output}"
#~ msgstr ""
#~ "Please generate equivalent Python code based on the following decompiled C code, and provide an example of the function call:\n"
#~ "{decompiler_output}"

#~ msgid ""
#~ "You are a helpful assistant embedded in IDA Pro. Your role is to facilitate "
#~ "reverse-engineering and answer programming questions."
#~ msgstr ""
#~ "Sei un assistente utile integrato in IDA Pro. Il tuo ruolo è facilitare "
#~ "l'ingegneria inversa e rispondere alle domande di programmazione."

```

`gepetto/locales/ko_KR/LC_MESSAGES/gepetto.po`:

```po
# Korean (South Korea) translations for Gepetto.
# This file is distributed under the same license as Gepetto (GPLv3).
# Ivan Kwiatkowski, 2023.
#
# Translators:
# Ivan K., 2023
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: Gepetto 1.1\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-10-18 18:57+0000\n"
"PO-Revision-Date: 2025-10-18 18:59+0000\n"
"Last-Translator: Ivan K., 2023\n"
"Language-Team: Korean (Korea) (https://app.transifex.com/gepetto/teams/164045/ko_KR/)\n"
"Language: ko_KR\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"Generated-By: Babel 2.17.0\n"

#: gepetto/config.py:82
msgid "Attempting to load the first available model..."
msgstr "사용 가능한 첫 번째 모델을 로드하는 중입니다..."

#: gepetto/config.py:87
msgid "No model available. Please edit the configuration file and try again."
msgstr "사용 가능한 모델이 없습니다. 구성 파일을 수정하고 다시 시도하세요."

#: gepetto/config.py:108
#, python-brace-format
msgid ""
"Warning: Gepetto's configuration doesn't contain option {option} in section "
"{section}!"
msgstr "경고: Gepetto의 설정에 섹션 {section}에서 옵션 {option}이(가) 포함되어 있지 않습니다!"

#: gepetto/ida/cli.py:120
msgid "LLM chat"
msgstr "LLM 대화"

#: gepetto/ida/cli.py:152
#, python-brace-format
msgid "→ Model requested tool: {tool_name} ({tool_args}...)"
msgstr "→ 모델이 요청한 도구: {tool_name} ({tool_args}...)"

#: gepetto/ida/cli.py:167
#, python-brace-format
msgid "Using tool: {tool_name}"
msgstr "도구 사용 중: {tool_name}"

#: gepetto/ida/cli.py:219
msgid "✔ Completed turn"
msgstr "✔ 턴 완료"

#: gepetto/ida/cli.py:227 gepetto/ida/cli.py:245
msgid "Model request failed."
msgstr "모델 요청에 실패했습니다."

#: gepetto/ida/cli.py:258
msgid "Streaming unavailable for this model; retrying without streaming."
msgstr "이 모델은 스트리밍을 지원하지 않습니다. 스트리밍 없이 다시 시도합니다."

#: gepetto/ida/cli.py:261
msgid "Retrying without streaming"
msgstr "스트리밍 없이 다시 시도"

#: gepetto/ida/comment_handler.py:39 gepetto/ida/comment_handler.py:41
msgid "No focused view available; cannot determine current function."
msgstr "포커스된 뷰가 없어 현재 함수를 확인할 수 없습니다."

#: gepetto/ida/comment_handler.py:103
#, python-brace-format
msgid "[ERROR] comment callback JSON failure: {error}"
msgstr "[오류] 주석 콜백 JSON 오류: {error}"

#: gepetto/ida/comment_handler.py:148
#, python-brace-format
msgid "[ERROR] comment application failed: {error}"
msgstr "[오류] 주석 적용에 실패했습니다: {error}"

#: gepetto/ida/comment_handler.py:156
#, python-brace-format
msgid "Applied {count} comments."
msgstr "{count}개의 주석을 적용했습니다."

#: gepetto/ida/comment_handler.py:162
msgid "No comments were applied."
msgstr "적용된 주석이 없습니다."

#: gepetto/ida/comment_handler.py:170
#, python-brace-format
msgid "[ERROR] comment_callback: {error}"
msgstr "[오류] comment_callback: {error}"

#: gepetto/ida/handlers.py:40 gepetto/ida/handlers.py:46
msgid "Comment generated by Gepetto"
msgstr "Gepetto에서 생성된 주석"

#: gepetto/ida/handlers.py:58
#, python-brace-format
msgid "[ERROR] comment callback failed: {error}"
msgstr "[오류] 주석 콜백 실패: {error}"

#: gepetto/ida/handlers.py:155
#, python-brace-format
msgid "[ERROR] rename callback JSON failure: {error}"
msgstr "[오류] 이름 변경 콜백 JSON 오류: {error}"

#: gepetto/ida/handlers.py:165
#, python-brace-format
msgid "Function at EA 0x{ea:X} not found."
msgstr "EA 0x{ea:X} 위치에서 함수를 찾을 수 없습니다."

#: gepetto/ida/handlers.py:182
#, python-brace-format
msgid "Select names to rename"
msgstr "변경할 이름 선택"

#: gepetto/ida/handlers.py:183
msgid "Old Name"
msgstr "이전 이름"

#: gepetto/ida/handlers.py:183
msgid "New Name"
msgstr "새 이름"

#: gepetto/ida/handlers.py:232
#, python-brace-format
msgid "[ERROR] rename callback failed: {error}"
msgstr "[오류] 이름 변경 콜백 실패: {error}"

#: gepetto/ida/handlers.py:238 gepetto/ida/handlers.py:239
msgid "Rename cancelled by user."
msgstr "사용자가 이름 변경을 취소했습니다."

#: gepetto/ida/handlers.py:243
#, python-brace-format
msgid "Done! {count} name(s) renamed."
msgstr "완료! {count}개의 이름이 변경되었습니다."

#: gepetto/ida/handlers.py:309 gepetto/ida/status_panel.py:654
#, python-brace-format
msgid "Couldn't change model to {model}: {error}"
msgstr "{model} 모델로 변경할 수 없습니다: {error}"

#: gepetto/ida/handlers.py:367 gepetto/ida/handlers.py:424
#, python-brace-format
msgid "{model} generated code saved to {file_name}"
msgstr "{model}이(가) 생성한 코드가 {file_name}에 저장되었습니다"

#: gepetto/ida/status_panel.py:20 gepetto/ida/status_panel.py:765
#: gepetto/ida/status_panel.py:773
msgid "Gepetto"
msgstr "Gepetto"

#: gepetto/ida/status_panel.py:48
msgid "System"
msgstr "시스템"

#: gepetto/ida/status_panel.py:49
msgid "User"
msgstr "사용자"

#: gepetto/ida/status_panel.py:50
msgid "Tool"
msgstr "도구"

#: gepetto/ida/status_panel.py:51
msgid "Model"
msgstr "모델"

#: gepetto/ida/status_panel.py:52
msgid "Assistant"
msgstr "어시스턴트"

#: gepetto/ida/status_panel.py:248
msgid "Log filters:"
msgstr "로그 필터:"

#: gepetto/ida/status_panel.py:266
msgid "Clear"
msgstr "지우기"

#: gepetto/ida/status_panel.py:285
msgid "Type a prompt and press Enter…"
msgstr "프롬프트를 입력하고 Enter 키를 누르세요…"

#: gepetto/ida/status_panel.py:288
msgid "Send"
msgstr "보내기"

#: gepetto/ida/status_panel.py:291 gepetto/ida/status_panel.py:690
msgid "Stop"
msgstr "중지"

#: gepetto/ida/status_panel.py:315 gepetto/ida/status_panel.py:585
#, python-brace-format
msgid "Model: {model} ▼"
msgstr "모델: {model} ▼"

#: gepetto/ida/status_panel.py:321 gepetto/ida/status_panel.py:673
#, python-brace-format
msgid "Status: {status}"
msgstr "상태: {status}"

#: gepetto/ida/status_panel.py:321 gepetto/ida/status_panel.py:672
#: gepetto/ida/status_panel.py:919
msgid "Idle"
msgstr "대기 중"

#: gepetto/ida/status_panel.py:499
#, python-brace-format
msgid "Failed to send prompt: {err}"
msgstr "프롬프트 전송에 실패했습니다: {err}"

#: gepetto/ida/status_panel.py:620
msgid "Error loading models"
msgstr "모델 로드 실패"

#: gepetto/ida/status_panel.py:640
#, python-brace-format
msgid "Model switched to {model}"
msgstr "모델이 {model}(으)로 변경되었습니다"

#: gepetto/ida/status_panel.py:658
#, python-brace-format
msgid "Failed to switch model: {error}"
msgstr "모델 전환에 실패했습니다: {error}"

#: gepetto/ida/status_panel.py:702
msgid "Error - Check Log"
msgstr "오류 - 로그를 확인하세요"

#: gepetto/ida/status_panel.py:709
msgid "Cancelling…"
msgstr "취소 중…"

#: gepetto/ida/status_panel.py:711
msgid "Cancelling request"
msgstr "요청 취소 중"

#: gepetto/ida/status_panel.py:751
msgid "You"
msgstr "나"

#: gepetto/ida/status_panel.py:753
#, python-brace-format
msgid "User: {text}"
msgstr "사용자: {text}"

#: gepetto/ida/status_panel.py:759
#, python-brace-format
msgid "Assistant: {text}"
msgstr "어시스턴트: {text}"

#: gepetto/ida/status_panel.py:861
msgid "Could not show Gepetto Status panel."
msgstr "Gepetto 상태 패널을 표시할 수 없습니다."

#: gepetto/ida/status_panel.py:917
msgid "Failed to cancel request"
msgstr "요청을 취소하지 못했습니다"

#: gepetto/ida/status_panel.py:954 gepetto/ida/status_panel.py:993
msgid "Done"
msgstr "완료"

#: gepetto/ida/status_panel.py:981
#, python-brace-format
msgid "Request to {model} sent..."
msgstr "{model}로 요청이 전송되었습니다..."

#: gepetto/ida/status_panel.py:983
msgid "Waiting for model..."
msgstr "모델을 기다리는 중..."

#: gepetto/ida/status_panel.py:988
#, python-brace-format
msgid "{model} query finished in {time:.2f} seconds!"
msgstr "{model} 요청이 {time:.2f}초 만에 완료되었습니다!"

#: gepetto/ida/ui.py:60 gepetto/ida/ui.py:93
msgid "Explain function"
msgstr "함수 설명"

#: gepetto/ida/ui.py:62 gepetto/ida/ui.py:103
msgid "Comment function"
msgstr "함수 설명"

#: gepetto/ida/ui.py:64 gepetto/ida/ui.py:113
msgid "Auto-rename"
msgstr "자동 이름 바꾸기"

#: gepetto/ida/ui.py:66 gepetto/ida/ui.py:137
msgid "Generate C Code"
msgstr "C 코드 생성"

#: gepetto/ida/ui.py:68 gepetto/ida/ui.py:124
msgid "Generate Python Code"
msgstr "파이썬 코드 생성"

#: gepetto/ida/ui.py:72
#, python-brace-format
msgid "Uses {model} to enrich the decompiler's output"
msgstr "{model}을(를) 사용하여 디컴파일러의 출력을 향상시킵니다"

#: gepetto/ida/ui.py:73
msgid "See usage instructions on GitHub"
msgstr "GitHub에서 사용법을 확인하세요"

#: gepetto/ida/ui.py:96
#, python-brace-format
msgid "Use {model} to explain the currently selected function"
msgstr "{model}을(를) 사용하여 선택된 함수를 설명합니다"

#: gepetto/ida/ui.py:106
#, python-brace-format
msgid "Adds comments to lines in the current function using {model}"
msgstr "현재 함수의 줄에 {model}로 주석을 추가합니다"

#: gepetto/ida/ui.py:116
#, python-brace-format
msgid "Use {model} to auto-rename this function and its variables"
msgstr "{model}을(를) 사용하여 이 함수와 그 변수의 이름을 자동으로 바꿉니다"

#: gepetto/ida/ui.py:127
#, python-brace-format
msgid ""
"Generate python code from the currently selected function using {model}"
msgstr "{model}을(를) 사용하여 선택된 함수를 설명합니다"

#: gepetto/ida/ui.py:140
#, python-brace-format
msgid ""
"Generate executable C code from the currently selected function using "
"{model}"
msgstr "선택한 함수에서 {model}로 실행 가능한 C 코드를 생성합니다"

#: gepetto/ida/ui.py:168
msgid "Options"
msgstr "옵션"

#: gepetto/ida/ui.py:169 gepetto/ida/ui.py:274
msgid "Auto-open status panel"
msgstr "상태 패널 자동 열기"

#: gepetto/ida/ui.py:208 gepetto/ida/ui.py:228
msgid "Select model"
msgstr "모델 선택"

#: gepetto/ida/ui.py:277
msgid "Automatically focus the Gepetto status panel when a request starts."
msgstr "요청이 시작될 때 Gepetto 상태 패널을 자동으로 포커스합니다."

#: gepetto/models/aliyun.py:62 gepetto/models/deepseek.py:36
#: gepetto/models/gemini.py:541 gepetto/models/groq.py:37
#: gepetto/models/kluster.py:58 gepetto/models/novita_ai.py:41
#: gepetto/models/openai.py:302 gepetto/models/openai_compatible.py:63
#: gepetto/models/openrouter.py:52 gepetto/models/siliconflow.py:60
#: gepetto/models/together.py:34
#, python-brace-format
msgid ""
"Please edit the configuration file to insert your {api_provider} API key!"
msgstr "설정 파일을 수정하여 {api_provider} API 키를 입력하세요!"

#: gepetto/models/gemini.py:111
#, python-brace-format
msgid "Failed to fetch Gemini models: {error}"
msgstr "Gemini 모델을 가져오지 못했습니다: {error}"

#: gepetto/models/gemini.py:159
#, python-brace-format
msgid "Failed to configure proxy for Gemini models: {error}"
msgstr "Gemini 모델용 프록시 구성에 실패했습니다: {error}"

#: gepetto/models/gemini.py:179 gepetto/models/local_lmstudio.py:140
#: gepetto/models/local_ollama.py:116 gepetto/models/openai.py:195
#, python-brace-format
msgid "Failed to fetch models from {base_url}: {status_code}"
msgstr "{base_url}에서 모델을 가져오지 못했습니다: {status_code}"

#: gepetto/models/gemini.py:723 gepetto/models/openai.py:414
#: gepetto/models/openai.py:431
#, python-brace-format
msgid "General exception encountered while running the query: {error}"
msgstr "쿼리 실행 중 일반 예외가 발생했습니다: {error}"

#: gepetto/models/local_lmstudio.py:62 gepetto/models/local_ollama.py:60
#, python-brace-format
msgid "Failed to fetch models from {base_url}: {error}"
msgstr "{base_url}에서 모델을 가져오지 못했습니다: {error}"

#: gepetto/models/local_lmstudio.py:123
#, python-brace-format
msgid "Failed to configure proxy for LM Studio models: {error}"
msgstr "LM Studio 모델용 프록시 구성에 실패했습니다: {error}"

#: gepetto/models/openai.py:120
#, python-brace-format
msgid "Failed to fetch OpenAI models: {error}"
msgstr "OpenAI 모델을 가져오지 못했습니다: {error}"

#: gepetto/models/openai.py:178
#, python-brace-format
msgid "Failed to configure proxy for OpenAI models: {error}"
msgstr "OpenAI 모델용 프록시 구성에 실패했습니다: {error}"

#: gepetto/models/openai.py:336
#, python-brace-format
msgid ""
"Streaming rejected for {model}: {error}\n"
"Retrying without streaming."
msgstr "{model}에 대한 스트리밍이 거부되었습니다: {error}\\n스트리밍 없이 다시 시도합니다."

#: gepetto/models/openai.py:340
#, python-brace-format
msgid ""
"Streaming disabled for {model} after a previous API rejection; retrying "
"without streaming."
msgstr "이전에 API에서 거부되어 {model}의 스트리밍이 비활성화되었습니다. 스트리밍 없이 다시 시도합니다."

#: gepetto/models/openai.py:404
msgid "Streaming rejected by API; retrying without streaming."
msgstr "API에서 스트리밍을 거부했습니다. 스트리밍 없이 다시 시도합니다."

#: gepetto/models/openai.py:410
msgid ""
"Unfortunately, this function is too big to be analyzed with the model's "
"current API limits."
msgstr "불행히도 이 함수는 모델의 현재 API 제한으로 분석하기에는 너무 큽니다."

#: gepetto/models/openai.py:422
#, python-brace-format
msgid "{model} could not complete the request: {error}"
msgstr "{model}이(가) 요청을 완료할 수 없습니다: {error}"

#~ msgid ""
#~ "Could not obtain valid data from the model, giving up. Dumping the response "
#~ "for manual import:"
#~ msgstr "모델에서 유효한 데이터를 얻지 못했습니다. 수동으로 가져오기 위해 응답을 덤프합니다:"

#~ msgid ""
#~ "Cannot extract valid JSON from the response. Asking the model to fix it..."
#~ msgstr "응답에서 유효한 JSON을 추출할 수 없습니다. 모델에 수정 요청 중..."

#~ msgid "The JSON document returned is invalid. Asking the model to fix it..."
#~ msgstr "반환된 JSON 문서가 유효하지 않습니다. 모델에 수정 요청 중..."

#~ msgid "Please fix the following JSON document:\n"
#~ msgstr "다음 JSON 문서를 수정해 주세요:\n"

#~ msgid ""
#~ "The JSON document provided in this response is invalid. Can you fix it?\n"
#~ "{response}"
#~ msgstr ""
#~ "이 응답에서 제공된 JSON 문서가 유효하지 않습니다. 수정해 주실 수 있나요?\n"
#~ "{response}"

#~ msgid "{model} query finished! {replaced} variable(s) renamed."
#~ msgstr "{model} 쿼리가 완료되었습니다! {replaced}개의 변수가 이름이 변경되었습니다."

#~ msgid ""
#~ "Context length exceeded! Reducing the completion tokens to {max_tokens}..."
#~ msgstr "컨텍스트 길이를 초과했습니다! 완료 토큰을 {max_tokens}로 줄이는 중..."

#~ msgid ""
#~ "Proxy configuration for Gemini via google-generativeai library might require"
#~ " manual setup of HTTPS_PROXY environment variable."
#~ msgstr ""
#~ "Proxy configuration for Gemini via google-generativeai library might require"
#~ " manual setup of HTTPS_PROXY environment variable."

#~ msgid ""
#~ "Please generate executable C code based on the following decompiled C code and ensure it includes all necessary header files and other information:\n"
#~ "{decompiler_output}"
#~ msgstr ""
#~ "Please generate executable C code based on the following decompiled C code and ensure it includes all necessary header files and other information:\n"
#~ "{decompiler_output}"

#~ msgid ""
#~ "Please generate equivalent Python code based on the following decompiled C code, and provide an example of the function call:\n"
#~ "{decompiler_output}"
#~ msgstr ""
#~ "Please generate equivalent Python code based on the following decompiled C code, and provide an example of the function call:\n"
#~ "{decompiler_output}"

#~ msgid ""
#~ "You are a helpful assistant embedded in IDA Pro. Your role is to facilitate "
#~ "reverse-engineering and answer programming questions."
#~ msgstr ""
#~ "당신은 IDA Pro에 내장된 유용한 도우미입니다. 당신의 역할은 리버스 엔지니어링을 돕고 프로그래밍 질문에 답하는 것입니다."

```

`gepetto/locales/ru/LC_MESSAGES/gepetto.po`:

```po
# Russian translations for Gepetto.
# This file is distributed under the same license as Gepetto (GPLv3).
# Ivan Kwiatkowski, 2023.
#
# Translators:
# Igor Kot, 2023
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: Gepetto 1.1\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-10-18 18:57+0000\n"
"PO-Revision-Date: 2025-10-18 18:59+0000\n"
"Last-Translator: Igor Kot, 2023\n"
"Language-Team: Russian (https://app.transifex.com/gepetto/teams/164045/ru/)\n"
"Language: ru\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Plural-Forms: nplurals=4; plural=(n%10==1 && n%100!=11 ? 0 : n%10>=2 && n%10<=4 && (n%100<12 || n%100>14) ? 1 : n%10==0 || (n%10>=5 && n%10<=9) || (n%100>=11 && n%100<=14)? 2 : 3);\n"
"Generated-By: Babel 2.17.0\n"

#: gepetto/config.py:82
msgid "Attempting to load the first available model..."
msgstr "Попытка загрузить первую доступную модель..."

#: gepetto/config.py:87
msgid "No model available. Please edit the configuration file and try again."
msgstr ""
"Модель недоступна. Пожалуйста, измените файл конфигурации и попробуйте "
"снова."

#: gepetto/config.py:108
#, python-brace-format
msgid ""
"Warning: Gepetto's configuration doesn't contain option {option} in section "
"{section}!"
msgstr ""
"Предупреждение: В конфигурации Gepetto отсутствует опция {option} в разделе "
"{section}!"

#: gepetto/ida/cli.py:120
msgid "LLM chat"
msgstr "Чат LLM"

#: gepetto/ida/cli.py:152
#, python-brace-format
msgid "→ Model requested tool: {tool_name} ({tool_args}...)"
msgstr "→ Модель запросила инструмент: {tool_name} ({tool_args}...)"

#: gepetto/ida/cli.py:167
#, python-brace-format
msgid "Using tool: {tool_name}"
msgstr "Используется инструмент: {tool_name}"

#: gepetto/ida/cli.py:219
msgid "✔ Completed turn"
msgstr "✔ Ход завершён"

#: gepetto/ida/cli.py:227 gepetto/ida/cli.py:245
msgid "Model request failed."
msgstr "Запрос к модели не выполнен."

#: gepetto/ida/cli.py:258
msgid "Streaming unavailable for this model; retrying without streaming."
msgstr ""
"Для этой модели потоковая передача недоступна; повторная попытка без потока."

#: gepetto/ida/cli.py:261
msgid "Retrying without streaming"
msgstr "Повторная попытка без потоковой передачи"

#: gepetto/ida/comment_handler.py:39 gepetto/ida/comment_handler.py:41
msgid "No focused view available; cannot determine current function."
msgstr "Нет активного представления; не удаётся определить текущую функцию."

#: gepetto/ida/comment_handler.py:103
#, python-brace-format
msgid "[ERROR] comment callback JSON failure: {error}"
msgstr "[ОШИБКА] ошибка JSON обратного вызова комментариев: {error}"

#: gepetto/ida/comment_handler.py:148
#, python-brace-format
msgid "[ERROR] comment application failed: {error}"
msgstr "[ОШИБКА] не удалось применить комментарии: {error}"

#: gepetto/ida/comment_handler.py:156
#, python-brace-format
msgid "Applied {count} comments."
msgstr "Применено {count} комментариев."

#: gepetto/ida/comment_handler.py:162
msgid "No comments were applied."
msgstr "Комментарии не были применены."

#: gepetto/ida/comment_handler.py:170
#, python-brace-format
msgid "[ERROR] comment_callback: {error}"
msgstr "[ОШИБКА] comment_callback: {error}"

#: gepetto/ida/handlers.py:40 gepetto/ida/handlers.py:46
msgid "Comment generated by Gepetto"
msgstr "Комментарий создан Gepetto"

#: gepetto/ida/handlers.py:58
#, python-brace-format
msgid "[ERROR] comment callback failed: {error}"
msgstr "[ОШИБКА] обратный вызов комментариев завершился с ошибкой: {error}"

#: gepetto/ida/handlers.py:155
#, python-brace-format
msgid "[ERROR] rename callback JSON failure: {error}"
msgstr "[ОШИБКА] ошибка JSON обратного вызова переименования: {error}"

#: gepetto/ida/handlers.py:165
#, python-brace-format
msgid "Function at EA 0x{ea:X} not found."
msgstr "Функция по адресу EA 0x{ea:X} не найдена."

#: gepetto/ida/handlers.py:182
#, python-brace-format
msgid "Select names to rename"
msgstr "Выберите имена для переименования"

#: gepetto/ida/handlers.py:183
msgid "Old Name"
msgstr "Старое имя"

#: gepetto/ida/handlers.py:183
msgid "New Name"
msgstr "Новое имя"

#: gepetto/ida/handlers.py:232
#, python-brace-format
msgid "[ERROR] rename callback failed: {error}"
msgstr "[ОШИБКА] обратный вызов переименования завершился с ошибкой: {error}"

#: gepetto/ida/handlers.py:238 gepetto/ida/handlers.py:239
msgid "Rename cancelled by user."
msgstr "Переименование отменено пользователем."

#: gepetto/ida/handlers.py:243
#, python-brace-format
msgid "Done! {count} name(s) renamed."
msgstr "Готово! Переименовано {count} имён."

#: gepetto/ida/handlers.py:309 gepetto/ida/status_panel.py:654
#, python-brace-format
msgid "Couldn't change model to {model}: {error}"
msgstr "Не удалось переключить модель на {model}: {error}"

#: gepetto/ida/handlers.py:367 gepetto/ida/handlers.py:424
#, python-brace-format
msgid "{model} generated code saved to {file_name}"
msgstr "Сгенерированный {model} код сохранён в {file_name}"

#: gepetto/ida/status_panel.py:20 gepetto/ida/status_panel.py:765
#: gepetto/ida/status_panel.py:773
msgid "Gepetto"
msgstr "Gepetto"

#: gepetto/ida/status_panel.py:48
msgid "System"
msgstr "Система"

#: gepetto/ida/status_panel.py:49
msgid "User"
msgstr "Пользователь"

#: gepetto/ida/status_panel.py:50
msgid "Tool"
msgstr "Инструмент"

#: gepetto/ida/status_panel.py:51
msgid "Model"
msgstr "Модель"

#: gepetto/ida/status_panel.py:52
msgid "Assistant"
msgstr "Ассистент"

#: gepetto/ida/status_panel.py:248
msgid "Log filters:"
msgstr "Фильтры журнала:"

#: gepetto/ida/status_panel.py:266
msgid "Clear"
msgstr "Очистить"

#: gepetto/ida/status_panel.py:285
msgid "Type a prompt and press Enter…"
msgstr "Введите запрос и нажмите Enter…"

#: gepetto/ida/status_panel.py:288
msgid "Send"
msgstr "Отправить"

#: gepetto/ida/status_panel.py:291 gepetto/ida/status_panel.py:690
msgid "Stop"
msgstr "Остановить"

#: gepetto/ida/status_panel.py:315 gepetto/ida/status_panel.py:585
#, python-brace-format
msgid "Model: {model} ▼"
msgstr "Модель: {model} ▼"

#: gepetto/ida/status_panel.py:321 gepetto/ida/status_panel.py:673
#, python-brace-format
msgid "Status: {status}"
msgstr "Статус: {status}"

#: gepetto/ida/status_panel.py:321 gepetto/ida/status_panel.py:672
#: gepetto/ida/status_panel.py:919
msgid "Idle"
msgstr "Ожидание"

#: gepetto/ida/status_panel.py:499
#, python-brace-format
msgid "Failed to send prompt: {err}"
msgstr "Не удалось отправить запрос: {err}"

#: gepetto/ida/status_panel.py:620
msgid "Error loading models"
msgstr "Ошибка загрузки моделей"

#: gepetto/ida/status_panel.py:640
#, python-brace-format
msgid "Model switched to {model}"
msgstr "Модель переключена на {model}"

#: gepetto/ida/status_panel.py:658
#, python-brace-format
msgid "Failed to switch model: {error}"
msgstr "Не удалось сменить модель: {error}"

#: gepetto/ida/status_panel.py:702
msgid "Error - Check Log"
msgstr "Ошибка — проверьте журнал"

#: gepetto/ida/status_panel.py:709
msgid "Cancelling…"
msgstr "Отмена…"

#: gepetto/ida/status_panel.py:711
msgid "Cancelling request"
msgstr "Отмена запроса"

#: gepetto/ida/status_panel.py:751
msgid "You"
msgstr "Вы"

#: gepetto/ida/status_panel.py:753
#, python-brace-format
msgid "User: {text}"
msgstr "Пользователь: {text}"

#: gepetto/ida/status_panel.py:759
#, python-brace-format
msgid "Assistant: {text}"
msgstr "Ассистент: {text}"

#: gepetto/ida/status_panel.py:861
msgid "Could not show Gepetto Status panel."
msgstr "Не удалось открыть панель состояния Gepetto."

#: gepetto/ida/status_panel.py:917
msgid "Failed to cancel request"
msgstr "Не удалось отменить запрос"

#: gepetto/ida/status_panel.py:954 gepetto/ida/status_panel.py:993
msgid "Done"
msgstr "Готово"

#: gepetto/ida/status_panel.py:981
#, python-brace-format
msgid "Request to {model} sent..."
msgstr "Запрос к {model} отправлен..."

#: gepetto/ida/status_panel.py:983
msgid "Waiting for model..."
msgstr "Ожидание модели..."

#: gepetto/ida/status_panel.py:988
#, python-brace-format
msgid "{model} query finished in {time:.2f} seconds!"
msgstr "Запрос к {model} завершён за {time:.2f} с!"

#: gepetto/ida/ui.py:60 gepetto/ida/ui.py:93
msgid "Explain function"
msgstr "Объяснить функцию"

#: gepetto/ida/ui.py:62 gepetto/ida/ui.py:103
msgid "Comment function"
msgstr "Комментировать функцию"

#: gepetto/ida/ui.py:64 gepetto/ida/ui.py:113
msgid "Auto-rename"
msgstr "Автопереименование"

#: gepetto/ida/ui.py:66 gepetto/ida/ui.py:137
msgid "Generate C Code"
msgstr "Сгенерировать код C"

#: gepetto/ida/ui.py:68 gepetto/ida/ui.py:124
msgid "Generate Python Code"
msgstr "Сгенерировать код Python"

#: gepetto/ida/ui.py:72
#, python-brace-format
msgid "Uses {model} to enrich the decompiler's output"
msgstr "Использует {model} для обогащения вывода декомпилятора"

#: gepetto/ida/ui.py:73
msgid "See usage instructions on GitHub"
msgstr "См. инструкции по использованию на GitHub."

#: gepetto/ida/ui.py:96
#, python-brace-format
msgid "Use {model} to explain the currently selected function"
msgstr "Используйте {model} для объяснения выбранной функции"

#: gepetto/ida/ui.py:106
#, python-brace-format
msgid "Adds comments to lines in the current function using {model}"
msgstr "Добавляет комментарии к строкам текущей функции с помощью {model}"

#: gepetto/ida/ui.py:116
#, python-brace-format
msgid "Use {model} to auto-rename this function and its variables"
msgstr ""
"Используйте {model} для автоматического переименования этой функции и её "
"переменных"

#: gepetto/ida/ui.py:127
#, python-brace-format
msgid ""
"Generate python code from the currently selected function using {model}"
msgstr "Добавляет комментарии к строкам текущей функции, используя {model}"

#: gepetto/ida/ui.py:140
#, python-brace-format
msgid ""
"Generate executable C code from the currently selected function using "
"{model}"
msgstr "Сгенерируйте исполняемый код C из выбранной функции с помощью {model}"

#: gepetto/ida/ui.py:168
msgid "Options"
msgstr "Параметры"

#: gepetto/ida/ui.py:169 gepetto/ida/ui.py:274
msgid "Auto-open status panel"
msgstr "Автоматически открывать панель состояния"

#: gepetto/ida/ui.py:208 gepetto/ida/ui.py:228
msgid "Select model"
msgstr "Выберите модель"

#: gepetto/ida/ui.py:277
msgid "Automatically focus the Gepetto status panel when a request starts."
msgstr ""
"Автоматически переводить фокус на панель состояния Gepetto при запуске "
"запроса."

#: gepetto/models/aliyun.py:62 gepetto/models/deepseek.py:36
#: gepetto/models/gemini.py:541 gepetto/models/groq.py:37
#: gepetto/models/kluster.py:58 gepetto/models/novita_ai.py:41
#: gepetto/models/openai.py:302 gepetto/models/openai_compatible.py:63
#: gepetto/models/openrouter.py:52 gepetto/models/siliconflow.py:60
#: gepetto/models/together.py:34
#, python-brace-format
msgid ""
"Please edit the configuration file to insert your {api_provider} API key!"
msgstr ""
"Пожалуйста, отредактируйте этот скрипт, чтобы добавить свой {api_provider} "
"API ключ!"

#: gepetto/models/gemini.py:111
#, python-brace-format
msgid "Failed to fetch Gemini models: {error}"
msgstr "Не удалось получить модели Gemini: {error}"

#: gepetto/models/gemini.py:159
#, python-brace-format
msgid "Failed to configure proxy for Gemini models: {error}"
msgstr "Не удалось настроить прокси для моделей Gemini: {error}"

#: gepetto/models/gemini.py:179 gepetto/models/local_lmstudio.py:140
#: gepetto/models/local_ollama.py:116 gepetto/models/openai.py:195
#, python-brace-format
msgid "Failed to fetch models from {base_url}: {status_code}"
msgstr "Не удалось получить модели с {base_url}: {status_code}"

#: gepetto/models/gemini.py:723 gepetto/models/openai.py:414
#: gepetto/models/openai.py:431
#, python-brace-format
msgid "General exception encountered while running the query: {error}"
msgstr "Общее исключение, возникшее при выполнении запроса: {error}"

#: gepetto/models/local_lmstudio.py:62 gepetto/models/local_ollama.py:60
#, python-brace-format
msgid "Failed to fetch models from {base_url}: {error}"
msgstr "Не удалось получить модели с {base_url}: {error}"

#: gepetto/models/local_lmstudio.py:123
#, python-brace-format
msgid "Failed to configure proxy for LM Studio models: {error}"
msgstr "Не удалось настроить прокси для моделей LM Studio: {error}"

#: gepetto/models/openai.py:120
#, python-brace-format
msgid "Failed to fetch OpenAI models: {error}"
msgstr "Не удалось получить модели OpenAI: {error}"

#: gepetto/models/openai.py:178
#, python-brace-format
msgid "Failed to configure proxy for OpenAI models: {error}"
msgstr "Не удалось настроить прокси для моделей OpenAI: {error}"

#: gepetto/models/openai.py:336
#, python-brace-format
msgid ""
"Streaming rejected for {model}: {error}\n"
"Retrying without streaming."
msgstr ""
"Потоковая передача для {model} отклонена: {error}\\nПовторная попытка без "
"потоковой передачи."

#: gepetto/models/openai.py:340
#, python-brace-format
msgid ""
"Streaming disabled for {model} after a previous API rejection; retrying "
"without streaming."
msgstr ""
"Потоковая передача для {model} отключена после предыдущего отказа API; "
"выполняется повторная попытка без потока."

#: gepetto/models/openai.py:404
msgid "Streaming rejected by API; retrying without streaming."
msgstr "API отклонил потоковую передачу; повторная попытка без потока."

#: gepetto/models/openai.py:410
msgid ""
"Unfortunately, this function is too big to be analyzed with the model's "
"current API limits."
msgstr ""
"К сожалению, данная функция слишком большая для анализа с текущими "
"ограничениями API модели."

#: gepetto/models/openai.py:422
#, python-brace-format
msgid "{model} could not complete the request: {error}"
msgstr "{model} не смогла выполнить запрос: {error}"

#~ msgid ""
#~ "Could not obtain valid data from the model, giving up. Dumping the response "
#~ "for manual import:"
#~ msgstr ""
#~ "Не удалось получить валидные данные от модели, сдаюсь. Выгружаю ответ для "
#~ "ручного импорта:"

#~ msgid ""
#~ "Cannot extract valid JSON from the response. Asking the model to fix it..."
#~ msgstr ""
#~ "Не удается извлечь валидный JSON из ответа. Прошу модель исправить это.."

#~ msgid "The JSON document returned is invalid. Asking the model to fix it..."
#~ msgstr "Возвращенный документ JSON невалидный. Прошу модель исправить это..."

#~ msgid "Please fix the following JSON document:\n"
#~ msgstr "Пожалуйста, исправьте следующий документ JSON:\n"

#~ msgid ""
#~ "The JSON document provided in this response is invalid. Can you fix it?\n"
#~ "{response}"
#~ msgstr ""
#~ "JSON-документ, предоставленный в этом ответе, невалидный. Вы можете его исправить?\n"
#~ "{response}"

#~ msgid "{model} query finished! {replaced} variable(s) renamed."
#~ msgstr ""
#~ "Запрос {model} завершен! Кол-во переименованных переменных: {replaced}."

#~ msgid ""
#~ "Context length exceeded! Reducing the completion tokens to {max_tokens}..."
#~ msgstr ""
#~ "Превышена максимальная длина! Уменьшение количества токенов завершения до "
#~ "{max_tokens}..."

#~ msgid ""
#~ "Proxy configuration for Gemini via google-generativeai library might require"
#~ " manual setup of HTTPS_PROXY environment variable."
#~ msgstr ""
#~ "Proxy configuration for Gemini via google-generativeai library might require"
#~ " manual setup of HTTPS_PROXY environment variable."

#~ msgid ""
#~ "Please generate executable C code based on the following decompiled C code and ensure it includes all necessary header files and other information:\n"
#~ "{decompiler_output}"
#~ msgstr ""
#~ "Please generate executable C code based on the following decompiled C code and ensure it includes all necessary header files and other information:\n"
#~ "{decompiler_output}"

#~ msgid ""
#~ "Please generate equivalent Python code based on the following decompiled C code, and provide an example of the function call:\n"
#~ "{decompiler_output}"
#~ msgstr ""
#~ "Please generate equivalent Python code based on the following decompiled C code, and provide an example of the function call:\n"
#~ "{decompiler_output}"

#~ msgid ""
#~ "You are a helpful assistant embedded in IDA Pro. Your role is to facilitate "
#~ "reverse-engineering and answer programming questions."
#~ msgstr ""
#~ "Вы — полезный помощник, встроенный в IDA Pro. Ваша роль — помогать в реверс-"
#~ "инжиниринге и отвечать на вопросы по программированию."

```

`gepetto/locales/tr/LC_MESSAGES/gepetto.po`:

```po
# Turkish translations for Gepetto.
# This file is distributed under the same license as Gepetto (GPLv3).
# Ivan Kwiatkowski, 2023.
#
# Translators:
# rüzgar can, 2023
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: Gepetto 1.1\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-10-18 18:57+0000\n"
"PO-Revision-Date: 2025-10-18 18:59+0000\n"
"Last-Translator: rüzgar can, 2023\n"
"Language-Team: Turkish (https://app.transifex.com/gepetto/teams/164045/tr/)\n"
"Language: tr\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Plural-Forms: nplurals=2; plural=(n > 1);\n"
"Generated-By: Babel 2.17.0\n"

#: gepetto/config.py:82
msgid "Attempting to load the first available model..."
msgstr "Kullanılabilir ilk modeli yüklemeye çalışılıyor..."

#: gepetto/config.py:87
msgid "No model available. Please edit the configuration file and try again."
msgstr ""
"Kullanılabilir model yok. Lütfen yapılandırma dosyasını düzenleyin ve tekrar"
" deneyin."

#: gepetto/config.py:108
#, python-brace-format
msgid ""
"Warning: Gepetto's configuration doesn't contain option {option} in section "
"{section}!"
msgstr ""
"Uyarı: Gepetto'nun yapılandırmasında {section} bölümünde {option} seçeneği "
"yok!"

#: gepetto/ida/cli.py:120
msgid "LLM chat"
msgstr "LLM sohbet"

#: gepetto/ida/cli.py:152
#, python-brace-format
msgid "→ Model requested tool: {tool_name} ({tool_args}...)"
msgstr "→ Modelin istediği araç: {tool_name} ({tool_args}...)"

#: gepetto/ida/cli.py:167
#, python-brace-format
msgid "Using tool: {tool_name}"
msgstr "Kullanılan araç: {tool_name}"

#: gepetto/ida/cli.py:219
msgid "✔ Completed turn"
msgstr "✔ Tur tamamlandı"

#: gepetto/ida/cli.py:227 gepetto/ida/cli.py:245
msgid "Model request failed."
msgstr "Model isteği başarısız oldu."

#: gepetto/ida/cli.py:258
msgid "Streaming unavailable for this model; retrying without streaming."
msgstr "Bu model için yayın kullanılamıyor; yayın olmadan yeniden deneniyor."

#: gepetto/ida/cli.py:261
msgid "Retrying without streaming"
msgstr "Yayın olmadan yeniden deneniyor"

#: gepetto/ida/comment_handler.py:39 gepetto/ida/comment_handler.py:41
msgid "No focused view available; cannot determine current function."
msgstr "Odaklanılmış bir görünüm yok; geçerli fonksiyon belirlenemiyor."

#: gepetto/ida/comment_handler.py:103
#, python-brace-format
msgid "[ERROR] comment callback JSON failure: {error}"
msgstr "[HATA] yorum geri çağrısında JSON hatası: {error}"

#: gepetto/ida/comment_handler.py:148
#, python-brace-format
msgid "[ERROR] comment application failed: {error}"
msgstr "[HATA] yorum uygulanamadı: {error}"

#: gepetto/ida/comment_handler.py:156
#, python-brace-format
msgid "Applied {count} comments."
msgstr "{count} yorum uygulandı."

#: gepetto/ida/comment_handler.py:162
msgid "No comments were applied."
msgstr "Herhangi bir yorum uygulanmadı."

#: gepetto/ida/comment_handler.py:170
#, python-brace-format
msgid "[ERROR] comment_callback: {error}"
msgstr "[HATA] comment_callback: {error}"

#: gepetto/ida/handlers.py:40 gepetto/ida/handlers.py:46
msgid "Comment generated by Gepetto"
msgstr "Gepetto tarafından oluşturulan yorum"

#: gepetto/ida/handlers.py:58
#, python-brace-format
msgid "[ERROR] comment callback failed: {error}"
msgstr "[HATA] yorum geri çağrısı başarısız oldu: {error}"

#: gepetto/ida/handlers.py:155
#, python-brace-format
msgid "[ERROR] rename callback JSON failure: {error}"
msgstr "[HATA] yeniden adlandırma geri çağrısında JSON hatası: {error}"

#: gepetto/ida/handlers.py:165
#, python-brace-format
msgid "Function at EA 0x{ea:X} not found."
msgstr "EA 0x{ea:X} adresindeki fonksiyon bulunamadı."

#: gepetto/ida/handlers.py:182
#, python-brace-format
msgid "Select names to rename"
msgstr "Yeniden adlandırılacak adları seç"

#: gepetto/ida/handlers.py:183
msgid "Old Name"
msgstr "Eski ad"

#: gepetto/ida/handlers.py:183
msgid "New Name"
msgstr "Yeni ad"

#: gepetto/ida/handlers.py:232
#, python-brace-format
msgid "[ERROR] rename callback failed: {error}"
msgstr "[HATA] yeniden adlandırma geri çağrısı başarısız oldu: {error}"

#: gepetto/ida/handlers.py:238 gepetto/ida/handlers.py:239
msgid "Rename cancelled by user."
msgstr "Yeniden adlandırma kullanıcı tarafından iptal edildi."

#: gepetto/ida/handlers.py:243
#, python-brace-format
msgid "Done! {count} name(s) renamed."
msgstr "Bitti! {count} ad yeniden adlandırıldı."

#: gepetto/ida/handlers.py:309 gepetto/ida/status_panel.py:654
#, python-brace-format
msgid "Couldn't change model to {model}: {error}"
msgstr "Model {model} olarak değiştirilemedi: {error}"

#: gepetto/ida/handlers.py:367 gepetto/ida/handlers.py:424
#, python-brace-format
msgid "{model} generated code saved to {file_name}"
msgstr "{model} tarafından oluşturulan kod {file_name} dosyasına kaydedildi"

#: gepetto/ida/status_panel.py:20 gepetto/ida/status_panel.py:765
#: gepetto/ida/status_panel.py:773
msgid "Gepetto"
msgstr "Gepetto"

#: gepetto/ida/status_panel.py:48
msgid "System"
msgstr "Sistem"

#: gepetto/ida/status_panel.py:49
msgid "User"
msgstr "Kullanıcı"

#: gepetto/ida/status_panel.py:50
msgid "Tool"
msgstr "Araç"

#: gepetto/ida/status_panel.py:51
msgid "Model"
msgstr "Model (seçili)"

#: gepetto/ida/status_panel.py:52
msgid "Assistant"
msgstr "Asistan"

#: gepetto/ida/status_panel.py:248
msgid "Log filters:"
msgstr "Günlük filtreleri:"

#: gepetto/ida/status_panel.py:266
msgid "Clear"
msgstr "Temizle"

#: gepetto/ida/status_panel.py:285
msgid "Type a prompt and press Enter…"
msgstr "Bir istem yazın ve Enter'a basın…"

#: gepetto/ida/status_panel.py:288
msgid "Send"
msgstr "Gönder"

#: gepetto/ida/status_panel.py:291 gepetto/ida/status_panel.py:690
msgid "Stop"
msgstr "Durdur"

#: gepetto/ida/status_panel.py:315 gepetto/ida/status_panel.py:585
#, python-brace-format
msgid "Model: {model} ▼"
msgstr "Model: {model} ▼ (seçili)"

#: gepetto/ida/status_panel.py:321 gepetto/ida/status_panel.py:673
#, python-brace-format
msgid "Status: {status}"
msgstr "Durum: {status}"

#: gepetto/ida/status_panel.py:321 gepetto/ida/status_panel.py:672
#: gepetto/ida/status_panel.py:919
msgid "Idle"
msgstr "Boşta"

#: gepetto/ida/status_panel.py:499
#, python-brace-format
msgid "Failed to send prompt: {err}"
msgstr "İstek gönderilemedi: {err}"

#: gepetto/ida/status_panel.py:620
msgid "Error loading models"
msgstr "Modeller yüklenemedi"

#: gepetto/ida/status_panel.py:640
#, python-brace-format
msgid "Model switched to {model}"
msgstr "Model {model} olarak değiştirildi"

#: gepetto/ida/status_panel.py:658
#, python-brace-format
msgid "Failed to switch model: {error}"
msgstr "Model değiştirilemedi: {error}"

#: gepetto/ida/status_panel.py:702
msgid "Error - Check Log"
msgstr "Hata - Günlüğü kontrol edin"

#: gepetto/ida/status_panel.py:709
msgid "Cancelling…"
msgstr "İptal ediliyor…"

#: gepetto/ida/status_panel.py:711
msgid "Cancelling request"
msgstr "İstek iptal ediliyor"

#: gepetto/ida/status_panel.py:751
msgid "You"
msgstr "Siz"

#: gepetto/ida/status_panel.py:753
#, python-brace-format
msgid "User: {text}"
msgstr "Kullanıcı: {text}"

#: gepetto/ida/status_panel.py:759
#, python-brace-format
msgid "Assistant: {text}"
msgstr "Asistan: {text}"

#: gepetto/ida/status_panel.py:861
msgid "Could not show Gepetto Status panel."
msgstr "Gepetto Durum paneli gösterilemedi."

#: gepetto/ida/status_panel.py:917
msgid "Failed to cancel request"
msgstr "İstek iptal edilemedi"

#: gepetto/ida/status_panel.py:954 gepetto/ida/status_panel.py:993
msgid "Done"
msgstr "Bitti"

#: gepetto/ida/status_panel.py:981
#, python-brace-format
msgid "Request to {model} sent..."
msgstr "{model}'a istek gönderildi..."

#: gepetto/ida/status_panel.py:983
msgid "Waiting for model..."
msgstr "Model bekleniyor..."

#: gepetto/ida/status_panel.py:988
#, python-brace-format
msgid "{model} query finished in {time:.2f} seconds!"
msgstr "{model} isteği {time:.2f} saniyede tamamlandı!"

#: gepetto/ida/ui.py:60 gepetto/ida/ui.py:93
msgid "Explain function"
msgstr "işlevi açıkla"

#: gepetto/ida/ui.py:62 gepetto/ida/ui.py:103
msgid "Comment function"
msgstr "işlevi açıkla"

#: gepetto/ida/ui.py:64 gepetto/ida/ui.py:113
msgid "Auto-rename"
msgstr "Otomatik yeniden adlandırma"

#: gepetto/ida/ui.py:66 gepetto/ida/ui.py:137
msgid "Generate C Code"
msgstr "C kodu oluştur"

#: gepetto/ida/ui.py:68 gepetto/ida/ui.py:124
msgid "Generate Python Code"
msgstr "Python kodu oluştur"

#: gepetto/ida/ui.py:72
#, python-brace-format
msgid "Uses {model} to enrich the decompiler's output"
msgstr "Derleyicinin çıktısını zenginleştirmek için {model} kullanır"

#: gepetto/ida/ui.py:73
msgid "See usage instructions on GitHub"
msgstr "GitHub'daki kullanım talimatlarına bakın"

#: gepetto/ida/ui.py:96
#, python-brace-format
msgid "Use {model} to explain the currently selected function"
msgstr "Seçili olan fonksiyonu açıklamak için {model} kullanın"

#: gepetto/ida/ui.py:106
#, python-brace-format
msgid "Adds comments to lines in the current function using {model}"
msgstr "{model} kullanarak geçerli fonksiyonun satırlarına açıklamalar ekler"

#: gepetto/ida/ui.py:116
#, python-brace-format
msgid "Use {model} to auto-rename this function and its variables"
msgstr ""
"Bu fonksiyonu ve değişkenlerini otomatik olarak yeniden adlandırmak için "
"{model} kullanın"

#: gepetto/ida/ui.py:127
#, python-brace-format
msgid ""
"Generate python code from the currently selected function using {model}"
msgstr "Seçili olan fonksiyonu açıklamak için {model} kullanın"

#: gepetto/ida/ui.py:140
#, python-brace-format
msgid ""
"Generate executable C code from the currently selected function using "
"{model}"
msgstr "Seçilen fonksiyondan {model} ile çalıştırılabilir C kodu üretir"

#: gepetto/ida/ui.py:168
msgid "Options"
msgstr "Seçenekler"

#: gepetto/ida/ui.py:169 gepetto/ida/ui.py:274
msgid "Auto-open status panel"
msgstr "Durum panelini otomatik aç"

#: gepetto/ida/ui.py:208 gepetto/ida/ui.py:228
msgid "Select model"
msgstr "Model seç"

#: gepetto/ida/ui.py:277
msgid "Automatically focus the Gepetto status panel when a request starts."
msgstr "Bir istek başladığında Gepetto durum panelini otomatik olarak odakla."

#: gepetto/models/aliyun.py:62 gepetto/models/deepseek.py:36
#: gepetto/models/gemini.py:541 gepetto/models/groq.py:37
#: gepetto/models/kluster.py:58 gepetto/models/novita_ai.py:41
#: gepetto/models/openai.py:302 gepetto/models/openai_compatible.py:63
#: gepetto/models/openrouter.py:52 gepetto/models/siliconflow.py:60
#: gepetto/models/together.py:34
#, python-brace-format
msgid ""
"Please edit the configuration file to insert your {api_provider} API key!"
msgstr ""
"{api_provider} API anahtarınızı eklemek için lütfen bu betiği düzenleyin!"

#: gepetto/models/gemini.py:111
#, python-brace-format
msgid "Failed to fetch Gemini models: {error}"
msgstr "Gemini modelleri alınamadı: {error}"

#: gepetto/models/gemini.py:159
#, python-brace-format
msgid "Failed to configure proxy for Gemini models: {error}"
msgstr "Gemini modelleri için vekil sunucu yapılandırılamadı: {error}"

#: gepetto/models/gemini.py:179 gepetto/models/local_lmstudio.py:140
#: gepetto/models/local_ollama.py:116 gepetto/models/openai.py:195
#, python-brace-format
msgid "Failed to fetch models from {base_url}: {status_code}"
msgstr "{base_url} adresinden modeller alınamadı: {status_code}"

#: gepetto/models/gemini.py:723 gepetto/models/openai.py:414
#: gepetto/models/openai.py:431
#, python-brace-format
msgid "General exception encountered while running the query: {error}"
msgstr "Sorgu çalıştırılırken genel istisna ile karşılaşıldı: {error}"

#: gepetto/models/local_lmstudio.py:62 gepetto/models/local_ollama.py:60
#, python-brace-format
msgid "Failed to fetch models from {base_url}: {error}"
msgstr "{base_url} adresinden modeller alınamadı: {error}"

#: gepetto/models/local_lmstudio.py:123
#, python-brace-format
msgid "Failed to configure proxy for LM Studio models: {error}"
msgstr "LM Studio modelleri için vekil sunucu yapılandırılamadı: {error}"

#: gepetto/models/openai.py:120
#, python-brace-format
msgid "Failed to fetch OpenAI models: {error}"
msgstr "OpenAI modelleri alınamadı: {error}"

#: gepetto/models/openai.py:178
#, python-brace-format
msgid "Failed to configure proxy for OpenAI models: {error}"
msgstr "OpenAI modelleri için vekil sunucu yapılandırılamadı: {error}"

#: gepetto/models/openai.py:336
#, python-brace-format
msgid ""
"Streaming rejected for {model}: {error}\n"
"Retrying without streaming."
msgstr ""
"{model} için yayın reddedildi: {error}\\nYayın olmadan yeniden deneniyor."

#: gepetto/models/openai.py:340
#, python-brace-format
msgid ""
"Streaming disabled for {model} after a previous API rejection; retrying "
"without streaming."
msgstr ""
"Önceki bir API reddi nedeniyle {model} için yayın devre dışı bırakıldı; "
"yayın olmadan yeniden deneniyor."

#: gepetto/models/openai.py:404
msgid "Streaming rejected by API; retrying without streaming."
msgstr "API yayını reddetti; yayın olmadan yeniden deneniyor."

#: gepetto/models/openai.py:410
msgid ""
"Unfortunately, this function is too big to be analyzed with the model's "
"current API limits."
msgstr ""
"Ne yazık ki bu fonksiyon, modelin mevcut API limitleri ile analiz "
"edilemeyecek kadar büyüktür."

#: gepetto/models/openai.py:422
#, python-brace-format
msgid "{model} could not complete the request: {error}"
msgstr "{model} istek tamamlanamadı: {error}"

#~ msgid ""
#~ "Could not obtain valid data from the model, giving up. Dumping the response "
#~ "for manual import:"
#~ msgstr ""
#~ "Modelden geçerli veriler alınamadı, vazgeçildi. Yanıtı manuel içe aktarma "
#~ "için boşaltma:"

#~ msgid ""
#~ "Cannot extract valid JSON from the response. Asking the model to fix it..."
#~ msgstr "Yanıttan geçerli JSON çıkarılamıyor. Modelden düzeltmesi isteniyor..."

#~ msgid "The JSON document returned is invalid. Asking the model to fix it..."
#~ msgstr "Döndürülen JSON belgesi geçersiz. Modelden düzeltmesi isteniyor..."

#~ msgid "Please fix the following JSON document:\n"
#~ msgstr ""
#~ "Lütfen aşağıdaki JSON belgesini düzeltin:\n"
#~ "\n"

#~ msgid ""
#~ "The JSON document provided in this response is invalid. Can you fix it?\n"
#~ "{response}"
#~ msgstr ""
#~ "Bu yanıtta sağlanan JSON belgesi geçersiz. Tamir edebilir misin?\n"
#~ "{response}"

#~ msgid "{model} query finished! {replaced} variable(s) renamed."
#~ msgstr ""
#~ "{model} sorgusu tamamlandı! {replaced} değişken(s) yeniden adlandırıldı."

#~ msgid ""
#~ "Context length exceeded! Reducing the completion tokens to {max_tokens}..."
#~ msgstr ""
#~ "Bağlam uzunluğu aşıldı! Tamamlama belirteçleri {max_tokens}'a indiriliyor..."

#~ msgid ""
#~ "Proxy configuration for Gemini via google-generativeai library might require"
#~ " manual setup of HTTPS_PROXY environment variable."
#~ msgstr ""
#~ "Proxy configuration for Gemini via google-generativeai library might require"
#~ " manual setup of HTTPS_PROXY environment variable."

#~ msgid ""
#~ "Please generate executable C code based on the following decompiled C code and ensure it includes all necessary header files and other information:\n"
#~ "{decompiler_output}"
#~ msgstr ""
#~ "Please generate executable C code based on the following decompiled C code and ensure it includes all necessary header files and other information:\n"
#~ "{decompiler_output}"

#~ msgid ""
#~ "Please generate equivalent Python code based on the following decompiled C code, and provide an example of the function call:\n"
#~ "{decompiler_output}"
#~ msgstr ""
#~ "Please generate equivalent Python code based on the following decompiled C code, and provide an example of the function call:\n"
#~ "{decompiler_output}"

#~ msgid ""
#~ "You are a helpful assistant embedded in IDA Pro. Your role is to facilitate "
#~ "reverse-engineering and answer programming questions."
#~ msgstr ""
#~ "IDA Pro'ya gömülü yararlı bir asistansınız. Rolünüz, tersine mühendisliği "
#~ "kolaylaştırmak ve programlama sorularını yanıtlamaktır."

```

`gepetto/locales/zh_CN/LC_MESSAGES/gepetto.po`:

```po
# Chinese (Simplified, China) translations for Gepetto.
# This file is distributed under the same license as Gepetto (GPLv3).
# Ivan Kwiatkowski, 2023.
#
# Translators:
# Ivan K., 2023
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: Gepetto 1.1\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-10-18 18:57+0000\n"
"PO-Revision-Date: 2025-10-18 18:59+0000\n"
"Last-Translator: jindaxia, 2025\n"
"Language-Team: Chinese (China) (https://app.transifex.com/gepetto/teams/164045/zh_CN/)\n"
"Language: zh_CN\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"Generated-By: Babel 2.17.0\n"

#: gepetto/config.py:82
msgid "Attempting to load the first available model..."
msgstr "正在尝试加载首个可用模型..."

#: gepetto/config.py:87
msgid "No model available. Please edit the configuration file and try again."
msgstr "没有可用模型，请编辑配置文件后重试。"

#: gepetto/config.py:108
#, python-brace-format
msgid ""
"Warning: Gepetto's configuration doesn't contain option {option} in section "
"{section}!"
msgstr "警告：Gepetto 配置文件的 {section} 章节中缺少 {option} 选项！"

#: gepetto/ida/cli.py:120
msgid "LLM chat"
msgstr "LLM 对话"

#: gepetto/ida/cli.py:152
#, python-brace-format
msgid "→ Model requested tool: {tool_name} ({tool_args}...)"
msgstr "→ 模型请求的工具：{tool_name} ({tool_args}...)"

#: gepetto/ida/cli.py:167
#, python-brace-format
msgid "Using tool: {tool_name}"
msgstr "正在使用工具：{tool_name}"

#: gepetto/ida/cli.py:219
msgid "✔ Completed turn"
msgstr "✔ 回合完成"

#: gepetto/ida/cli.py:227 gepetto/ida/cli.py:245
msgid "Model request failed."
msgstr "模型请求失败。"

#: gepetto/ida/cli.py:258
msgid "Streaming unavailable for this model; retrying without streaming."
msgstr "此模型不支持流式；正在改为非流式重试。"

#: gepetto/ida/cli.py:261
msgid "Retrying without streaming"
msgstr "正在重试（不启用流式）"

#: gepetto/ida/comment_handler.py:39 gepetto/ida/comment_handler.py:41
msgid "No focused view available; cannot determine current function."
msgstr "没有可用的聚焦视图，无法确定当前函数。"

#: gepetto/ida/comment_handler.py:103
#, python-brace-format
msgid "[ERROR] comment callback JSON failure: {error}"
msgstr "[错误] 注释回调 JSON 错误：{error}"

#: gepetto/ida/comment_handler.py:148
#, python-brace-format
msgid "[ERROR] comment application failed: {error}"
msgstr "[错误] 应用注释失败：{error}"

#: gepetto/ida/comment_handler.py:156
#, python-brace-format
msgid "Applied {count} comments."
msgstr "已应用 {count} 条注释。"

#: gepetto/ida/comment_handler.py:162
msgid "No comments were applied."
msgstr "未应用任何注释。"

#: gepetto/ida/comment_handler.py:170
#, python-brace-format
msgid "[ERROR] comment_callback: {error}"
msgstr "[错误] comment_callback：{error}"

#: gepetto/ida/handlers.py:40 gepetto/ida/handlers.py:46
msgid "Comment generated by Gepetto"
msgstr "由 Gepetto 生成的注释"

#: gepetto/ida/handlers.py:58
#, python-brace-format
msgid "[ERROR] comment callback failed: {error}"
msgstr "[错误] 注释回调失败：{error}"

#: gepetto/ida/handlers.py:155
#, python-brace-format
msgid "[ERROR] rename callback JSON failure: {error}"
msgstr "[错误] 重命名回调 JSON 错误：{error}"

#: gepetto/ida/handlers.py:165
#, python-brace-format
msgid "Function at EA 0x{ea:X} not found."
msgstr "在 EA 0x{ea:X} 未找到函数。"

#: gepetto/ida/handlers.py:182
#, python-brace-format
msgid "Select names to rename"
msgstr "选择要重命名的名称"

#: gepetto/ida/handlers.py:183
msgid "Old Name"
msgstr "旧名称"

#: gepetto/ida/handlers.py:183
msgid "New Name"
msgstr "新名称"

#: gepetto/ida/handlers.py:232
#, python-brace-format
msgid "[ERROR] rename callback failed: {error}"
msgstr "[错误] 重命名回调失败：{error}"

#: gepetto/ida/handlers.py:238 gepetto/ida/handlers.py:239
msgid "Rename cancelled by user."
msgstr "用户取消了重命名。"

#: gepetto/ida/handlers.py:243
#, python-brace-format
msgid "Done! {count} name(s) renamed."
msgstr "完成！已重命名 {count} 个名称。"

#: gepetto/ida/handlers.py:309 gepetto/ida/status_panel.py:654
#, python-brace-format
msgid "Couldn't change model to {model}: {error}"
msgstr "无法将模型切换为 {model}：{error}"

#: gepetto/ida/handlers.py:367 gepetto/ida/handlers.py:424
#, python-brace-format
msgid "{model} generated code saved to {file_name}"
msgstr "{model} 生成的代码已保存到 {file_name}"

#: gepetto/ida/status_panel.py:20 gepetto/ida/status_panel.py:765
#: gepetto/ida/status_panel.py:773
msgid "Gepetto"
msgstr "Gepetto"

#: gepetto/ida/status_panel.py:48
msgid "System"
msgstr "系统"

#: gepetto/ida/status_panel.py:49
msgid "User"
msgstr "用户"

#: gepetto/ida/status_panel.py:50
msgid "Tool"
msgstr "工具"

#: gepetto/ida/status_panel.py:51
msgid "Model"
msgstr "模型"

#: gepetto/ida/status_panel.py:52
msgid "Assistant"
msgstr "助手"

#: gepetto/ida/status_panel.py:248
msgid "Log filters:"
msgstr "日志筛选："

#: gepetto/ida/status_panel.py:266
msgid "Clear"
msgstr "清除"

#: gepetto/ida/status_panel.py:285
msgid "Type a prompt and press Enter…"
msgstr "输入提示并按回车…"

#: gepetto/ida/status_panel.py:288
msgid "Send"
msgstr "发送"

#: gepetto/ida/status_panel.py:291 gepetto/ida/status_panel.py:690
msgid "Stop"
msgstr "停止"

#: gepetto/ida/status_panel.py:315 gepetto/ida/status_panel.py:585
#, python-brace-format
msgid "Model: {model} ▼"
msgstr "模型：{model} ▼"

#: gepetto/ida/status_panel.py:321 gepetto/ida/status_panel.py:673
#, python-brace-format
msgid "Status: {status}"
msgstr "状态：{status}"

#: gepetto/ida/status_panel.py:321 gepetto/ida/status_panel.py:672
#: gepetto/ida/status_panel.py:919
msgid "Idle"
msgstr "空闲"

#: gepetto/ida/status_panel.py:499
#, python-brace-format
msgid "Failed to send prompt: {err}"
msgstr "发送请求失败：{err}"

#: gepetto/ida/status_panel.py:620
msgid "Error loading models"
msgstr "加载模型时出错"

#: gepetto/ida/status_panel.py:640
#, python-brace-format
msgid "Model switched to {model}"
msgstr "模型已切换为 {model}"

#: gepetto/ida/status_panel.py:658
#, python-brace-format
msgid "Failed to switch model: {error}"
msgstr "切换模型失败：{error}"

#: gepetto/ida/status_panel.py:702
msgid "Error - Check Log"
msgstr "错误 - 请检查日志"

#: gepetto/ida/status_panel.py:709
msgid "Cancelling…"
msgstr "正在取消…"

#: gepetto/ida/status_panel.py:711
msgid "Cancelling request"
msgstr "正在取消请求"

#: gepetto/ida/status_panel.py:751
msgid "You"
msgstr "你"

#: gepetto/ida/status_panel.py:753
#, python-brace-format
msgid "User: {text}"
msgstr "用户：{text}"

#: gepetto/ida/status_panel.py:759
#, python-brace-format
msgid "Assistant: {text}"
msgstr "助手：{text}"

#: gepetto/ida/status_panel.py:861
msgid "Could not show Gepetto Status panel."
msgstr "无法显示 Gepetto 状态面板。"

#: gepetto/ida/status_panel.py:917
msgid "Failed to cancel request"
msgstr "无法取消请求"

#: gepetto/ida/status_panel.py:954 gepetto/ida/status_panel.py:993
msgid "Done"
msgstr "完成"

#: gepetto/ida/status_panel.py:981
#, python-brace-format
msgid "Request to {model} sent..."
msgstr "{model} 请求发送中..."

#: gepetto/ida/status_panel.py:983
msgid "Waiting for model..."
msgstr "正在等待模型..."

#: gepetto/ida/status_panel.py:988
#, python-brace-format
msgid "{model} query finished in {time:.2f} seconds!"
msgstr "{model} 查询在 {time:.2f} 秒内完成！"

#: gepetto/ida/ui.py:60 gepetto/ida/ui.py:93
msgid "Explain function"
msgstr "解释函数"

#: gepetto/ida/ui.py:62 gepetto/ida/ui.py:103
msgid "Comment function"
msgstr "解释函数"

#: gepetto/ida/ui.py:64 gepetto/ida/ui.py:113
msgid "Auto-rename"
msgstr "自动重命名"

#: gepetto/ida/ui.py:66 gepetto/ida/ui.py:137
msgid "Generate C Code"
msgstr "生成 C 代码"

#: gepetto/ida/ui.py:68 gepetto/ida/ui.py:124
msgid "Generate Python Code"
msgstr "生成 Python 代码"

#: gepetto/ida/ui.py:72
#, python-brace-format
msgid "Uses {model} to enrich the decompiler's output"
msgstr "使用 {model} 增强反编译器的输出"

#: gepetto/ida/ui.py:73
msgid "See usage instructions on GitHub"
msgstr "请查看 GitHub 上的使用说明"

#: gepetto/ida/ui.py:96
#, python-brace-format
msgid "Use {model} to explain the currently selected function"
msgstr "使用 {model} 解释当前选择的函数"

#: gepetto/ida/ui.py:106
#, python-brace-format
msgid "Adds comments to lines in the current function using {model}"
msgstr "使用 {model} 为当前函数的各行添加注释"

#: gepetto/ida/ui.py:116
#, python-brace-format
msgid "Use {model} to auto-rename this function and its variables"
msgstr "使用 {model} 自动重命名此函数及其变量"

#: gepetto/ida/ui.py:127
#, python-brace-format
msgid ""
"Generate python code from the currently selected function using {model}"
msgstr "使用 {model} 从当前选择的函数生成 python 代码"

#: gepetto/ida/ui.py:140
#, python-brace-format
msgid ""
"Generate executable C code from the currently selected function using "
"{model}"
msgstr "使用 {model} 从当前选定的函数生成可执行的 C 代码"

#: gepetto/ida/ui.py:168
msgid "Options"
msgstr "选项"

#: gepetto/ida/ui.py:169 gepetto/ida/ui.py:274
msgid "Auto-open status panel"
msgstr "自动打开状态面板"

#: gepetto/ida/ui.py:208 gepetto/ida/ui.py:228
msgid "Select model"
msgstr "选择模型"

#: gepetto/ida/ui.py:277
msgid "Automatically focus the Gepetto status panel when a request starts."
msgstr "在请求开始时自动聚焦 Gepetto 状态面板。"

#: gepetto/models/aliyun.py:62 gepetto/models/deepseek.py:36
#: gepetto/models/gemini.py:541 gepetto/models/groq.py:37
#: gepetto/models/kluster.py:58 gepetto/models/novita_ai.py:41
#: gepetto/models/openai.py:302 gepetto/models/openai_compatible.py:63
#: gepetto/models/openrouter.py:52 gepetto/models/siliconflow.py:60
#: gepetto/models/together.py:34
#, python-brace-format
msgid ""
"Please edit the configuration file to insert your {api_provider} API key!"
msgstr "请编辑配置文件并插入您的 {api_provider} API 密钥！"

#: gepetto/models/gemini.py:111
#, python-brace-format
msgid "Failed to fetch Gemini models: {error}"
msgstr "获取 Gemini 模型失败：{error}"

#: gepetto/models/gemini.py:159
#, python-brace-format
msgid "Failed to configure proxy for Gemini models: {error}"
msgstr "配置 Gemini 模型的代理失败：{error}"

#: gepetto/models/gemini.py:179 gepetto/models/local_lmstudio.py:140
#: gepetto/models/local_ollama.py:116 gepetto/models/openai.py:195
#, python-brace-format
msgid "Failed to fetch models from {base_url}: {status_code}"
msgstr "从 {base_url} 获取模型失败：{status_code}"

#: gepetto/models/gemini.py:723 gepetto/models/openai.py:414
#: gepetto/models/openai.py:431
#, python-brace-format
msgid "General exception encountered while running the query: {error}"
msgstr "运行查询时遇到常规异常：{error}"

#: gepetto/models/local_lmstudio.py:62 gepetto/models/local_ollama.py:60
#, python-brace-format
msgid "Failed to fetch models from {base_url}: {error}"
msgstr "从 {base_url} 获取模型失败：{error}"

#: gepetto/models/local_lmstudio.py:123
#, python-brace-format
msgid "Failed to configure proxy for LM Studio models: {error}"
msgstr "配置 LM Studio 模型的代理失败：{error}"

#: gepetto/models/openai.py:120
#, python-brace-format
msgid "Failed to fetch OpenAI models: {error}"
msgstr "获取 OpenAI 模型失败：{error}"

#: gepetto/models/openai.py:178
#, python-brace-format
msgid "Failed to configure proxy for OpenAI models: {error}"
msgstr "配置 OpenAI 模型的代理失败：{error}"

#: gepetto/models/openai.py:336
#, python-brace-format
msgid ""
"Streaming rejected for {model}: {error}\n"
"Retrying without streaming."
msgstr "{model} 的流式被拒绝：{error}\\n正在改为非流式重试。"

#: gepetto/models/openai.py:340
#, python-brace-format
msgid ""
"Streaming disabled for {model} after a previous API rejection; retrying "
"without streaming."
msgstr "由于此前 API 拒绝，{model} 的流式已禁用；正在改为非流式重试。"

#: gepetto/models/openai.py:404
msgid "Streaming rejected by API; retrying without streaming."
msgstr "API 拒绝了流式；正在改为非流式重试。"

#: gepetto/models/openai.py:410
msgid ""
"Unfortunately, this function is too big to be analyzed with the model's "
"current API limits."
msgstr "很遗憾，此函数规模过大，已超出模型当前 API 的限制范围。"

#: gepetto/models/openai.py:422
#, python-brace-format
msgid "{model} could not complete the request: {error}"
msgstr "{model} 无法完成请求：{error}"

#~ msgid ""
#~ "Could not obtain valid data from the model, giving up. Dumping the response "
#~ "for manual import:"
#~ msgstr "无法从模型获取有效数据，放弃操作。正在转储响应以供手动导入："

#~ msgid ""
#~ "Cannot extract valid JSON from the response. Asking the model to fix it..."
#~ msgstr "无法从响应中提取有效 JSON，正在请求模型修复..."

#~ msgid "The JSON document returned is invalid. Asking the model to fix it..."
#~ msgstr "返回的 JSON 文档无效，正在请求模型修复..."

#~ msgid "Please fix the following JSON document:\n"
#~ msgstr "请修复以下 JSON 文档：\n"

#~ msgid ""
#~ "The JSON document provided in this response is invalid. Can you fix it?\n"
#~ "{response}"
#~ msgstr ""
#~ "此响应提供的 JSON 文档无效，请修复：\n"
#~ "{response}"

#~ msgid "{model} query finished! {replaced} variable(s) renamed."
#~ msgstr "{model} 请求完成！已重命名 {replaced} 个变量。"

#~ msgid ""
#~ "Context length exceeded! Reducing the completion tokens to {max_tokens}..."
#~ msgstr "上下文长度超出限制！正在将完成令牌数缩减至 {max_tokens}..."

#~ msgid "Auto Analyze"
#~ msgstr "自动分析"

#~ msgid ""
#~ "Proxy configuration for Gemini via google-generativeai library might require"
#~ " manual setup of HTTPS_PROXY environment variable."
#~ msgstr ""
#~ "Proxy configuration for Gemini via google-generativeai library might require"
#~ " manual setup of HTTPS_PROXY environment variable."

#~ msgid ""
#~ "Please generate executable C code based on the following decompiled C code and ensure it includes all necessary header files and other information:\n"
#~ "{decompiler_output}"
#~ msgstr ""
#~ "请根据以下C反编译代码生成可执行的C代码，并确保包含所有必要的头文件和其他信息：\n"
#~ "{decompiler_output}"

#~ msgid ""
#~ "Please generate equivalent Python code based on the following decompiled C code, and provide an example of the function call:\n"
#~ "{decompiler_output}"
#~ msgstr ""
#~ "请根据以下C反编译代码生成等价的python代码，并提供这个函数的调用示例：\n"
#~ "{decompiler_output}"

#~ msgid ""
#~ "You are a helpful assistant embedded in IDA Pro. Your role is to facilitate "
#~ "reverse-engineering and answer programming questions."
#~ msgstr "您是内置于 IDA Pro 的智能助手，主要职责是协助逆向工程分析和解答编程相关问题。"

```

`gepetto/locales/zh_TW/LC_MESSAGES/gepetto.po`:

```po
# Chinese (Traditional, Taiwan) translations for Gepetto.
# This file is distributed under the same license as Gepetto (GPLv3).
# Ivan Kwiatkowski, 2023.
#
# Translators:
# Orange Tsai <Orange.8361@gmail.com>, 2024
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: Gepetto 1.1\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-10-18 18:57+0000\n"
"PO-Revision-Date: 2025-10-18 18:59+0000\n"
"Last-Translator: Orange Tsai <Orange.8361@gmail.com>, 2024\n"
"Language-Team: Chinese (Taiwan) (https://app.transifex.com/gepetto/teams/164045/zh_TW/)\n"
"Language: zh_TW\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"Generated-By: Babel 2.17.0\n"

#: gepetto/config.py:82
msgid "Attempting to load the first available model..."
msgstr "正在嘗試載入第一個可用的模型..."

#: gepetto/config.py:87
msgid "No model available. Please edit the configuration file and try again."
msgstr "沒有可用的模型。請編輯配置檔案後重試。"

#: gepetto/config.py:108
#, python-brace-format
msgid ""
"Warning: Gepetto's configuration doesn't contain option {option} in section "
"{section}!"
msgstr "警告: Gepetto 的配置在 {section} 部分中不包含選項 {option}!"

#: gepetto/ida/cli.py:120
msgid "LLM chat"
msgstr "LLM 對話"

#: gepetto/ida/cli.py:152
#, python-brace-format
msgid "→ Model requested tool: {tool_name} ({tool_args}...)"
msgstr "→ 模型要求的工具：{tool_name} ({tool_args}...)"

#: gepetto/ida/cli.py:167
#, python-brace-format
msgid "Using tool: {tool_name}"
msgstr "使用工具：{tool_name}"

#: gepetto/ida/cli.py:219
msgid "✔ Completed turn"
msgstr "✔ 回合完成"

#: gepetto/ida/cli.py:227 gepetto/ida/cli.py:245
msgid "Model request failed."
msgstr "模型請求失敗。"

#: gepetto/ida/cli.py:258
msgid "Streaming unavailable for this model; retrying without streaming."
msgstr "此模型無法使用串流；改以非串流方式重試。"

#: gepetto/ida/cli.py:261
msgid "Retrying without streaming"
msgstr "正在重新嘗試（不啟用串流）"

#: gepetto/ida/comment_handler.py:39 gepetto/ida/comment_handler.py:41
msgid "No focused view available; cannot determine current function."
msgstr "沒有可聚焦的檢視，無法判斷目前的函式。"

#: gepetto/ida/comment_handler.py:103
#, python-brace-format
msgid "[ERROR] comment callback JSON failure: {error}"
msgstr "[錯誤] 註解回呼 JSON 錯誤：{error}"

#: gepetto/ida/comment_handler.py:148
#, python-brace-format
msgid "[ERROR] comment application failed: {error}"
msgstr "[錯誤] 套用註解失敗：{error}"

#: gepetto/ida/comment_handler.py:156
#, python-brace-format
msgid "Applied {count} comments."
msgstr "已套用 {count} 則註解。"

#: gepetto/ida/comment_handler.py:162
msgid "No comments were applied."
msgstr "未套用任何註解。"

#: gepetto/ida/comment_handler.py:170
#, python-brace-format
msgid "[ERROR] comment_callback: {error}"
msgstr "[錯誤] comment_callback：{error}"

#: gepetto/ida/handlers.py:40 gepetto/ida/handlers.py:46
msgid "Comment generated by Gepetto"
msgstr "此註解由 Gepetto 產生"

#: gepetto/ida/handlers.py:58
#, python-brace-format
msgid "[ERROR] comment callback failed: {error}"
msgstr "[錯誤] 註解回呼失敗：{error}"

#: gepetto/ida/handlers.py:155
#, python-brace-format
msgid "[ERROR] rename callback JSON failure: {error}"
msgstr "[錯誤] 重新命名回呼 JSON 錯誤：{error}"

#: gepetto/ida/handlers.py:165
#, python-brace-format
msgid "Function at EA 0x{ea:X} not found."
msgstr "在 EA 0x{ea:X} 找不到函式。"

#: gepetto/ida/handlers.py:182
#, python-brace-format
msgid "Select names to rename"
msgstr "選擇要重新命名的名稱"

#: gepetto/ida/handlers.py:183
msgid "Old Name"
msgstr "舊名稱"

#: gepetto/ida/handlers.py:183
msgid "New Name"
msgstr "新名稱"

#: gepetto/ida/handlers.py:232
#, python-brace-format
msgid "[ERROR] rename callback failed: {error}"
msgstr "[錯誤] 重新命名回呼失敗：{error}"

#: gepetto/ida/handlers.py:238 gepetto/ida/handlers.py:239
msgid "Rename cancelled by user."
msgstr "使用者已取消重新命名。"

#: gepetto/ida/handlers.py:243
#, python-brace-format
msgid "Done! {count} name(s) renamed."
msgstr "完成！已重新命名 {count} 個名稱。"

#: gepetto/ida/handlers.py:309 gepetto/ida/status_panel.py:654
#, python-brace-format
msgid "Couldn't change model to {model}: {error}"
msgstr "無法將模型切換為 {model}：{error}"

#: gepetto/ida/handlers.py:367 gepetto/ida/handlers.py:424
#, python-brace-format
msgid "{model} generated code saved to {file_name}"
msgstr "{model} 產生的程式碼已儲存到 {file_name}"

#: gepetto/ida/status_panel.py:20 gepetto/ida/status_panel.py:765
#: gepetto/ida/status_panel.py:773
msgid "Gepetto"
msgstr "Gepetto"

#: gepetto/ida/status_panel.py:48
msgid "System"
msgstr "系統"

#: gepetto/ida/status_panel.py:49
msgid "User"
msgstr "使用者"

#: gepetto/ida/status_panel.py:50
msgid "Tool"
msgstr "工具"

#: gepetto/ida/status_panel.py:51
msgid "Model"
msgstr "模型"

#: gepetto/ida/status_panel.py:52
msgid "Assistant"
msgstr "助理"

#: gepetto/ida/status_panel.py:248
msgid "Log filters:"
msgstr "記錄篩選："

#: gepetto/ida/status_panel.py:266
msgid "Clear"
msgstr "清除"

#: gepetto/ida/status_panel.py:285
msgid "Type a prompt and press Enter…"
msgstr "輸入提示並按 Enter…"

#: gepetto/ida/status_panel.py:288
msgid "Send"
msgstr "傳送"

#: gepetto/ida/status_panel.py:291 gepetto/ida/status_panel.py:690
msgid "Stop"
msgstr "停止"

#: gepetto/ida/status_panel.py:315 gepetto/ida/status_panel.py:585
#, python-brace-format
msgid "Model: {model} ▼"
msgstr "模型：{model} ▼"

#: gepetto/ida/status_panel.py:321 gepetto/ida/status_panel.py:673
#, python-brace-format
msgid "Status: {status}"
msgstr "狀態：{status}"

#: gepetto/ida/status_panel.py:321 gepetto/ida/status_panel.py:672
#: gepetto/ida/status_panel.py:919
msgid "Idle"
msgstr "閒置"

#: gepetto/ida/status_panel.py:499
#, python-brace-format
msgid "Failed to send prompt: {err}"
msgstr "傳送請求失敗：{err}"

#: gepetto/ida/status_panel.py:620
msgid "Error loading models"
msgstr "載入模型時發生錯誤"

#: gepetto/ida/status_panel.py:640
#, python-brace-format
msgid "Model switched to {model}"
msgstr "模型已切換為 {model}"

#: gepetto/ida/status_panel.py:658
#, python-brace-format
msgid "Failed to switch model: {error}"
msgstr "切換模型失敗：{error}"

#: gepetto/ida/status_panel.py:702
msgid "Error - Check Log"
msgstr "錯誤 - 請檢查記錄"

#: gepetto/ida/status_panel.py:709
msgid "Cancelling…"
msgstr "正在取消…"

#: gepetto/ida/status_panel.py:711
msgid "Cancelling request"
msgstr "正在取消請求"

#: gepetto/ida/status_panel.py:751
msgid "You"
msgstr "你"

#: gepetto/ida/status_panel.py:753
#, python-brace-format
msgid "User: {text}"
msgstr "使用者：{text}"

#: gepetto/ida/status_panel.py:759
#, python-brace-format
msgid "Assistant: {text}"
msgstr "助理：{text}"

#: gepetto/ida/status_panel.py:861
msgid "Could not show Gepetto Status panel."
msgstr "無法顯示 Gepetto 狀態面板。"

#: gepetto/ida/status_panel.py:917
msgid "Failed to cancel request"
msgstr "無法取消請求"

#: gepetto/ida/status_panel.py:954 gepetto/ida/status_panel.py:993
msgid "Done"
msgstr "完成"

#: gepetto/ida/status_panel.py:981
#, python-brace-format
msgid "Request to {model} sent..."
msgstr "已向 {model} 送出請求"

#: gepetto/ida/status_panel.py:983
msgid "Waiting for model..."
msgstr "正在等待模型..."

#: gepetto/ida/status_panel.py:988
#, python-brace-format
msgid "{model} query finished in {time:.2f} seconds!"
msgstr "{model} 查詢在 {time:.2f} 秒內完成！"

#: gepetto/ida/ui.py:60 gepetto/ida/ui.py:93
msgid "Explain function"
msgstr "解釋函數"

#: gepetto/ida/ui.py:62 gepetto/ida/ui.py:103
msgid "Comment function"
msgstr "解釋函數"

#: gepetto/ida/ui.py:64 gepetto/ida/ui.py:113
msgid "Auto-rename"
msgstr "自動重新命名"

#: gepetto/ida/ui.py:66 gepetto/ida/ui.py:137
msgid "Generate C Code"
msgstr "產生 C 程式碼"

#: gepetto/ida/ui.py:68 gepetto/ida/ui.py:124
msgid "Generate Python Code"
msgstr "產生 Python 程式碼"

#: gepetto/ida/ui.py:72
#, python-brace-format
msgid "Uses {model} to enrich the decompiler's output"
msgstr "使用 {model} 來豐富反編譯器的結果"

#: gepetto/ida/ui.py:73
msgid "See usage instructions on GitHub"
msgstr "請參閱 GitHub 上的操作說明"

#: gepetto/ida/ui.py:96
#, python-brace-format
msgid "Use {model} to explain the currently selected function"
msgstr "使用 {model} 來解釋當前選擇的函數"

#: gepetto/ida/ui.py:106
#, python-brace-format
msgid "Adds comments to lines in the current function using {model}"
msgstr "使用 {model} 為目前函式的各行加入註解"

#: gepetto/ida/ui.py:116
#, python-brace-format
msgid "Use {model} to auto-rename this function and its variables"
msgstr "使用 {model} 自動重新命名此函式及其變數"

#: gepetto/ida/ui.py:127
#, python-brace-format
msgid ""
"Generate python code from the currently selected function using {model}"
msgstr "使用 {model} 來解釋當前選擇的函數"

#: gepetto/ida/ui.py:140
#, python-brace-format
msgid ""
"Generate executable C code from the currently selected function using "
"{model}"
msgstr "使用 {model} 從目前選取的函式產生可執行的 C 程式碼"

#: gepetto/ida/ui.py:168
msgid "Options"
msgstr "選項"

#: gepetto/ida/ui.py:169 gepetto/ida/ui.py:274
msgid "Auto-open status panel"
msgstr "自動開啟狀態面板"

#: gepetto/ida/ui.py:208 gepetto/ida/ui.py:228
msgid "Select model"
msgstr "選取模型"

#: gepetto/ida/ui.py:277
msgid "Automatically focus the Gepetto status panel when a request starts."
msgstr "在請求開始時自動聚焦 Gepetto 狀態面板。"

#: gepetto/models/aliyun.py:62 gepetto/models/deepseek.py:36
#: gepetto/models/gemini.py:541 gepetto/models/groq.py:37
#: gepetto/models/kluster.py:58 gepetto/models/novita_ai.py:41
#: gepetto/models/openai.py:302 gepetto/models/openai_compatible.py:63
#: gepetto/models/openrouter.py:52 gepetto/models/siliconflow.py:60
#: gepetto/models/together.py:34
#, python-brace-format
msgid ""
"Please edit the configuration file to insert your {api_provider} API key!"
msgstr "請編輯程式碼以新增您的 {api_provider} API 金鑰!"

#: gepetto/models/gemini.py:111
#, python-brace-format
msgid "Failed to fetch Gemini models: {error}"
msgstr "取得 Gemini 模型失敗：{error}"

#: gepetto/models/gemini.py:159
#, python-brace-format
msgid "Failed to configure proxy for Gemini models: {error}"
msgstr "設定 Gemini 模型的 Proxy 失敗：{error}"

#: gepetto/models/gemini.py:179 gepetto/models/local_lmstudio.py:140
#: gepetto/models/local_ollama.py:116 gepetto/models/openai.py:195
#, python-brace-format
msgid "Failed to fetch models from {base_url}: {status_code}"
msgstr "自 {base_url} 取得模型失敗：{status_code}"

#: gepetto/models/gemini.py:723 gepetto/models/openai.py:414
#: gepetto/models/openai.py:431
#, python-brace-format
msgid "General exception encountered while running the query: {error}"
msgstr "執行查詢時遭遇例外錯誤: {error}"

#: gepetto/models/local_lmstudio.py:62 gepetto/models/local_ollama.py:60
#, python-brace-format
msgid "Failed to fetch models from {base_url}: {error}"
msgstr "自 {base_url} 取得模型失敗：{error}"

#: gepetto/models/local_lmstudio.py:123
#, python-brace-format
msgid "Failed to configure proxy for LM Studio models: {error}"
msgstr "設定 LM Studio 模型的 Proxy 失敗：{error}"

#: gepetto/models/openai.py:120
#, python-brace-format
msgid "Failed to fetch OpenAI models: {error}"
msgstr "取得 OpenAI 模型失敗：{error}"

#: gepetto/models/openai.py:178
#, python-brace-format
msgid "Failed to configure proxy for OpenAI models: {error}"
msgstr "設定 OpenAI 模型的 Proxy 失敗：{error}"

#: gepetto/models/openai.py:336
#, python-brace-format
msgid ""
"Streaming rejected for {model}: {error}\n"
"Retrying without streaming."
msgstr "{model} 的串流被拒絕：{error}\\n改以非串流方式重試。"

#: gepetto/models/openai.py:340
#, python-brace-format
msgid ""
"Streaming disabled for {model} after a previous API rejection; retrying "
"without streaming."
msgstr "先前 API 拒絕後，已停用 {model} 的串流；改以非串流方式重試。"

#: gepetto/models/openai.py:404
msgid "Streaming rejected by API; retrying without streaming."
msgstr "API 拒絕了串流；改以非串流方式重試。"

#: gepetto/models/openai.py:410
msgid ""
"Unfortunately, this function is too big to be analyzed with the model's "
"current API limits."
msgstr "看來這個函數太大了，超過當前模型的 API 限制無法分析。"

#: gepetto/models/openai.py:422
#, python-brace-format
msgid "{model} could not complete the request: {error}"
msgstr "{model} 無法完成請求: {error}"

#~ msgid ""
#~ "Could not obtain valid data from the model, giving up. Dumping the response "
#~ "for manual import:"
#~ msgstr "無法從模型取得有效的資料。印出回應以提供手動導入:"

#~ msgid ""
#~ "Cannot extract valid JSON from the response. Asking the model to fix it..."
#~ msgstr "無法從回應解析出合法 JSON 的資料。請要求模型去修復它..."

#~ msgid "The JSON document returned is invalid. Asking the model to fix it..."
#~ msgstr "所回傳的 JSON 格式不合法。請要求模型去修復它..."

#~ msgid "Please fix the following JSON document:\n"
#~ msgstr "請修復下列 JSON 格式:\n"

#~ msgid ""
#~ "The JSON document provided in this response is invalid. Can you fix it?\n"
#~ "{response}"
#~ msgstr ""
#~ "這個回應所提供的 JSON 格式並不合法，你能修復它嗎?\n"
#~ "{response}"

#~ msgid "{model} query finished! {replaced} variable(s) renamed."
#~ msgstr "{model} 查詢完成! {replaced} 個變數(們)已重新命名。"

#~ msgid ""
#~ "Context length exceeded! Reducing the completion tokens to {max_tokens}..."
#~ msgstr "上下文超過長度! 請減少 Token 數量到 {max_tokens}..."

#~ msgid ""
#~ "Proxy configuration for Gemini via google-generativeai library might require"
#~ " manual setup of HTTPS_PROXY environment variable."
#~ msgstr ""
#~ "Proxy configuration for Gemini via google-generativeai library might require"
#~ " manual setup of HTTPS_PROXY environment variable."

#~ msgid ""
#~ "Please generate executable C code based on the following decompiled C code and ensure it includes all necessary header files and other information:\n"
#~ "{decompiler_output}"
#~ msgstr ""
#~ "Please generate executable C code based on the following decompiled C code and ensure it includes all necessary header files and other information:\n"
#~ "{decompiler_output}"

#~ msgid ""
#~ "Please generate equivalent Python code based on the following decompiled C code, and provide an example of the function call:\n"
#~ "{decompiler_output}"
#~ msgstr ""
#~ "Please generate equivalent Python code based on the following decompiled C code, and provide an example of the function call:\n"
#~ "{decompiler_output}"

#~ msgid ""
#~ "You are a helpful assistant embedded in IDA Pro. Your role is to facilitate "
#~ "reverse-engineering and answer programming questions."
#~ msgstr "你是嵌入在 IDA Pro 中的有用助手。你的角色是促進逆向工程並回答程式設計問題。"

```

`gepetto/models/aliyun.py`:

```py
import openai
import json
import httpx as _httpx

import gepetto.config
import gepetto.models.model_manager
from gepetto.models.openai import GPT

_ = gepetto.config._

DEFAULT_ALIYUN_MODELS = [
    "qwen-max",
    "qwen-plus",
    "qwq-plus",
    "qwq-32b",
    "deepseek-v3",
    "deepseek-r1",
    "qwen-coder-plus",
    "qwen-omni-turbo",
]


class Aliyun(GPT):

    @staticmethod
    def get_menu_name() -> str:
        return "Aliyun"

    @staticmethod
    def supported_models():
        # Check if custom models are defined in config
        # If not, use the default models
        config_models = gepetto.config.get_config("Aliyun", "MODELS")
        if config_models:
            try:
                return json.loads(config_models)
            except json.JSONDecodeError:
                # If it's not valid JSON, treat it as comma-separated list
                return [model.strip() for model in config_models.split(",")]
        return DEFAULT_ALIYUN_MODELS

    @staticmethod
    def is_configured_properly() -> bool:

        # The plugin is configured properly if the API key is provided, otherwise it should not be shown.
        return bool(
            gepetto.config.get_config("Aliyun", "API_KEY",
                                      "ALIYUN_API_KEY"))

    def __init__(self, model):
        try:

            super().__init__(model)
        except ValueError:
            pass  # May throw if the OpenAI API key isn't given, but we don't need any to use DeepSeek.

        self.model = model
        api_key = gepetto.config.get_config("Aliyun", "API_KEY",
                                            "ALIYUN_API_KEY")
        if not api_key:
            raise ValueError(
                _("Please edit the configuration file to insert your {api_provider} API key!"
                  ).format(api_provider="Aliyun"))

        proxy = gepetto.config.get_config("Gepetto", "PROXY")
        base_url = gepetto.config.get_config("Aliyun", "BASE_URL",
                                             "ALIYUN_BASE_URL",
                                             "https://dashscope.aliyuncs.com/compatible-mode/v1")

        self.client = openai.OpenAI(
            api_key=api_key,
            base_url=base_url,
            http_client=_httpx.Client(proxy=proxy) if proxy else None)


gepetto.models.model_manager.register_model(Aliyun)

```

`gepetto/models/azure_openai.py`:

```py
import httpx as _httpx

import openai
from azure.identity import InteractiveBrowserCredential, get_bearer_token_provider
from gepetto.models.openai import GPT
import gepetto.models.model_manager
import gepetto.config

_ = gepetto.config._

AZURE_OPENAI_MODELS = [
    "gpt-35-turbo",
    "gpt-35-turbo-1106",
    "gpt-35-turbo-16k",
    "gpt-4-turbo",
    "gpt-4-turbo-2024-0409-gs"
]


class AzureOpenAI(GPT):
    API_VERSION = "2024-05-01-preview"

    @staticmethod
    def get_menu_name() -> str:
        return "Azure OpenAI"

    @staticmethod
    def supported_models():
        return AZURE_OPENAI_MODELS

    @staticmethod
    def is_configured_properly() -> bool:
        # The plugin is configured properly if the resource URL is provided, otherwise it should not be shown.
        return bool(gepetto.config.get_config("AzureOpenAI", "BASE_URL", "AZURE_OPENAI_URL"))

    def __init__(self, model):
        self.model = model
        proxy = gepetto.config.get_config("Gepetto", "PROXY")
        base_url = gepetto.config.get_config("AzureOpenAI", "BASE_URL", "AZURE_OPENAI_URL")
        api_key = gepetto.config.get_config(
            "AzureOpenAI", "API_KEY", "AZURE_OPENAI_API_KEY")

        if api_key:
            self.client = openai.AzureOpenAI(
                azure_endpoint=base_url,
                api_key=api_key,
                api_version=self.API_VERSION,
                http_client=_httpx.Client(
                    proxy=proxy,
                ) if proxy else None
            )
        else:
            # Entra ID authentication
            token_provider = get_bearer_token_provider(
                InteractiveBrowserCredential(),
                "https://cognitiveservices.azure.com/.default"
            )
            self.client = openai.AzureOpenAI(
                azure_endpoint=base_url,
                azure_ad_token_provider=token_provider,
                api_version=self.API_VERSION,
                http_client=_httpx.Client(
                    proxy=proxy,
                ) if proxy else None
            )


gepetto.models.model_manager.register_model(AzureOpenAI)

```

`gepetto/models/base.py`:

```py
import abc


class LanguageModel(metaclass=abc.ABCMeta):
    @abc.abstractmethod
    def query_model_async(self, query, cb, stream, additional_model_options) -> None:
        pass

    def __eq__(self, other):
        return self.get_menu_name() == other.get_menu_name()

    def __hash__(self):
        return self.get_menu_name().__hash__()

    @staticmethod
    @abc.abstractmethod
    def supported_models() -> list[str]:
        pass

    @staticmethod
    @abc.abstractmethod
    def get_menu_name() -> str:
        pass

    @staticmethod
    @abc.abstractmethod
    def is_configured_properly() -> bool:
        pass
```

`gepetto/models/deepseek.py`:

```py
import openai
import httpx as _httpx

import gepetto.config
import gepetto.models.model_manager
from gepetto.models.openai import GPT

_ = gepetto.config._

DEEPSEEK_CHAT_NAME = "deepseek-chat"
DEEPSEEK_REASONER_NAME = "deepseek-reasoner"

class DeepSeek(GPT):
    @staticmethod
    def get_menu_name() -> str:
        return "DeepSeek"

    @staticmethod
    def supported_models():
        return [DEEPSEEK_CHAT_NAME, DEEPSEEK_REASONER_NAME]

    @staticmethod
    def is_configured_properly() -> bool:
        # The plugin is configured properly if the API key is provided, otherwise it should not be shown.
        return bool(gepetto.config.get_config("DeepSeek", "API_KEY", "DEEPSEEK_API_KEY"))

    def __init__(self, model):
        try:
            super().__init__(model)
        except ValueError:
            pass  # May throw if the OpenAI API key isn't given, but we don't need any to use DeepSeek.

        self.model = model
        api_key = gepetto.config.get_config("DeepSeek", "API_KEY", "DEEPSEEK_API_KEY")
        if not api_key:
            raise ValueError(_("Please edit the configuration file to insert your {api_provider} API key!")
                             .format(api_provider="DeepSeek"))
                             
        proxy = gepetto.config.get_config("Gepetto", "PROXY")
        base_url = gepetto.config.get_config("DeepSeek", "BASE_URL", "DEEPSEEK_BASE_URL", "https://api.deepseek.com/v1")

        self.client = openai.OpenAI(
            api_key=api_key,
            base_url=base_url,
            http_client=_httpx.Client(
                proxy=proxy,
            ) if proxy else None
        )

gepetto.models.model_manager.register_model(DeepSeek)

```

`gepetto/models/gemini.py`:

```py
from __future__ import annotations

import asyncio
import functools
import json
import re
import threading
import time
from collections.abc import Iterable
from types import SimpleNamespace
from typing import Any

import httpx as _httpx
import ida_kernwin
from google import genai
from google.genai import types

from gepetto.models.base import LanguageModel
import gepetto.models.model_manager
import gepetto.config

_ = gepetto.config._

GEMINI_PRO_LATEST_MODEL_NAME = "gemini-pro-latest"
GEMINI_FLASH_LATEST_MODEL_NAME = "gemini-flash-latest"
GEMINI_FLASH_LITE_LATEST_MODEL_NAME = "gemini-flash-lite-latest"
GEMINI_2_5_PRO_MODEL_NAME = "gemini-2.5-pro"
GEMINI_2_5_FLASH_MODEL_NAME = "gemini-2.5-flash"
GEMINI_2_5_FLASH_LITE_MODEL_NAME = "gemini-2.5-flash-lite"
GEMINI_2_0_FLASH_MODEL_NAME = "gemini-2.0-flash"

_DEFAULT_GEMINI_MODELS = [
    GEMINI_PRO_LATEST_MODEL_NAME,
    GEMINI_FLASH_LATEST_MODEL_NAME,
    GEMINI_FLASH_LITE_LATEST_MODEL_NAME,
    GEMINI_2_5_PRO_MODEL_NAME,
    GEMINI_2_5_FLASH_MODEL_NAME,
    GEMINI_2_5_FLASH_LITE_MODEL_NAME,
    GEMINI_2_0_FLASH_MODEL_NAME,
]

_GEMINI_MODELS: list[str] | None = None
_GEMINI_MODELS_LOCK = threading.Lock()
_GEMINI_REFRESH_THREAD: threading.Thread | None = None
_GEMINI_LAST_REFRESH: float = 0.0

_DEFAULT_SAFETY_SETTINGS: tuple[types.SafetySetting, ...] = (
    types.SafetySetting(
        category=types.HarmCategory.HARM_CATEGORY_HARASSMENT,
        threshold=types.HarmBlockThreshold.BLOCK_NONE,
    ),
    types.SafetySetting(
        category=types.HarmCategory.HARM_CATEGORY_HATE_SPEECH,
        threshold=types.HarmBlockThreshold.BLOCK_NONE,
    ),
    types.SafetySetting(
        category=types.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,
        threshold=types.HarmBlockThreshold.BLOCK_NONE,
    ),
    types.SafetySetting(
        category=types.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,
        threshold=types.HarmBlockThreshold.BLOCK_NONE,
    ),
)

_FINISH_REASON_MAP = {
    "FINISH_REASON_STOP": "stop",
    "FINISH_REASON_MAX_TOKENS": "length",
    "FINISH_REASON_SAFETY": "safety",
    "FINISH_REASON_RECITATION": "content_filter",
    "FINISH_REASON_TOOL_CALL": "tool_calls",
}


def _sort_gemini_models(models: list[str]) -> list[str]:
    deduped = list(dict.fromkeys(models))
    default_models_set = set(_DEFAULT_GEMINI_MODELS)
    default_models = [m for m in deduped if m in default_models_set]
    other_models = [m for m in deduped if m not in default_models_set]
    default_models.sort(key=str, reverse=True)
    other_models.sort(key=str, reverse=True)
    return default_models + other_models


def _trigger_menu_refresh() -> None:
    try:
        from gepetto.ida import ui as ida_ui
        ida_ui.trigger_model_select_menu_regeneration()
    except Exception:
        pass


def _update_gemini_models(models: list[str], *, notify: bool = True) -> None:
    global _GEMINI_MODELS
    normalized = _sort_gemini_models(models)
    with _GEMINI_MODELS_LOCK:
        current = list(_GEMINI_MODELS) if _GEMINI_MODELS is not None else []
        if normalized == current:
            return
        _GEMINI_MODELS = normalized
    if notify:
        _trigger_menu_refresh()


def _execute_gemini_fetch(api_key: str, proxy: str | None, timeout: _httpx.Timeout) -> list[str]:
    loop = asyncio.new_event_loop()
    try:
        asyncio.set_event_loop(loop)
        return loop.run_until_complete(_fetch_gemini_models_async(api_key, proxy, timeout))
    except Exception as exc:
        print(_("Failed to fetch Gemini models: {error}").format(error=exc))
        return []
    finally:
        asyncio.set_event_loop(None)
        loop.close()


def _schedule_gemini_refresh(api_key: str, proxy: str | None, timeout: _httpx.Timeout) -> None:
    global _GEMINI_REFRESH_THREAD, _GEMINI_LAST_REFRESH
    with _GEMINI_MODELS_LOCK:
        if _GEMINI_REFRESH_THREAD and _GEMINI_REFRESH_THREAD.is_alive():
            return
        now = time.monotonic()
        if now - _GEMINI_LAST_REFRESH < 5.0:
            return
        _GEMINI_LAST_REFRESH = now
        _GEMINI_REFRESH_THREAD = threading.Thread(
            target=_refresh_gemini_models_background,
            args=(api_key, proxy, timeout),
            name="GepettoGeminiModelRefresh",
            daemon=True,
        )
        _GEMINI_REFRESH_THREAD.start()


def _refresh_gemini_models_background(api_key: str, proxy: str | None, timeout: _httpx.Timeout) -> None:
    global _GEMINI_REFRESH_THREAD
    try:
        models = _execute_gemini_fetch(api_key, proxy, timeout)
        if models:
            _update_gemini_models(models)
    finally:
        with _GEMINI_MODELS_LOCK:
            _GEMINI_REFRESH_THREAD = None


async def _fetch_gemini_models_async(
    api_key: str,
    proxy: str | None,
    timeout: _httpx.Timeout,
) -> list[str]:
    params = {"key": api_key}
    transport = None
    if proxy:
        try:
            transport = _httpx.AsyncHTTPTransport(proxy=proxy)
        except Exception as transport_exc:
            print(
                _("Failed to configure proxy for Gemini models: {error}").format(
                    error=transport_exc
                )
            )
    try:
        async with _httpx.AsyncClient(timeout=timeout, transport=transport) as client:
            response = await client.get(
                "https://generativelanguage.googleapis.com/v1beta/models",
                params=params,
            )
    except (
        _httpx.ConnectError,
        _httpx.ConnectTimeout,
        _httpx.ReadTimeout,
        _httpx.TimeoutException,
    ):
        return []

    if response.status_code != 200:
        print(
            _("Failed to fetch models from {base_url}: {status_code}").format(
                base_url="https://generativelanguage.googleapis.com/v1beta/models",
                status_code=response.status_code,
            )
        )
        return []

    payload = response.json() or {}
    models: list[str] = []
    for model in payload.get("models", []):
        supported_methods = model.get("supportedGenerationMethods") or []
        if supported_methods and "generateContent" not in supported_methods:
            continue
        identifier = model.get("name") or ""
        if identifier.startswith("models/"):
            identifier = identifier.split("/", 1)[1]
        lowered = identifier.lower()
        if re.search(
            r"tts|robot|computer|image|learnlm",
            lowered,
        ):
            continue
        if identifier:
            models.append(identifier)
    return _sort_gemini_models(models or list(_DEFAULT_GEMINI_MODELS))


def _notify_error(cb, message: str) -> None:
    if cb is None:
        return

    payload = SimpleNamespace(error=message)

    def runner() -> None:
        try:
            cb(payload, "error")
        except TypeError:
            cb(payload)

    try:
        ida_kernwin.execute_sync(runner, ida_kernwin.MFF_FAST)
    except Exception:
        runner()


def _to_plain(value: Any) -> Any:
    if isinstance(value, dict):
        return {k: _to_plain(v) for k, v in value.items()}
    if isinstance(value, list):
        return [_to_plain(v) for v in value]
    if isinstance(value, SimpleNamespace):
        return {k: _to_plain(v) for k, v in vars(value).items()}
    return value


def _safe_json(value: Any) -> Any:
    if isinstance(value, str):
        stripped = value.strip()
        if not stripped:
            return {}
        try:
            return json.loads(stripped)
        except Exception:
            return stripped
    return value


def _convert_tools(tools_spec: Iterable[Any] | None) -> list[types.Tool] | None:
    if not tools_spec:
        return None

    declarations: list[types.FunctionDeclaration] = []
    for tool in tools_spec:
        data = _to_plain(tool)
        if data.get("type") != "function":
            continue
        fn = data.get("function") or {}
        name = fn.get("name")
        if not name:
            continue
        declarations.append(
            types.FunctionDeclaration(
                name=name,
                description=fn.get("description"),
                parameters=fn.get("parameters"),
            )
        )

    if not declarations:
        return None

    return [types.Tool(function_declarations=declarations)]


def _as_text_part(text: str) -> types.Part:
    return types.Part.from_text(text=text)


def _convert_messages(query: Any) -> tuple[str | None, list[types.Content]]:
    if isinstance(query, str):
        query = [{"role": "user", "content": query}]

    system_lines: list[str] = []
    contents: list[types.Content] = []
    pending_tool_parts: list[types.Part] = []

    def flush_tool_parts() -> None:
        nonlocal pending_tool_parts
        if not pending_tool_parts:
            return
        contents.append(types.Content(role="tool", parts=pending_tool_parts))
        pending_tool_parts = []

    for raw in query or []:
        message = _to_plain(raw)
        role = (message.get("role") or "").lower()

        if role == "system":
            flush_tool_parts()
            content = message.get("content")
            if content:
                system_lines.append(str(content))
            continue

        if role == "tool":
            name = message.get("name") or message.get("tool_call_id") or ""
            payload = _safe_json(message.get("content"))
            if not isinstance(payload, (dict, list)):
                payload = {"result": payload}
            part = types.Part.from_function_response(
                name=name,
                response=_to_plain(payload),
            )
            call_id = (
                message.get("tool_call_id")
                or message.get("call_id")
                or message.get("id")
            )
            if call_id and hasattr(part, "function_response"):
                fr = part.function_response
                call_id_str = str(call_id)
                try:
                    fr.id = call_id_str
                except Exception:
                    pass
                try:
                    object.__setattr__(fr, "call_id", call_id_str)
                except Exception:
                    pass
            pending_tool_parts.append(part)
            continue

        if role == "assistant":
            flush_tool_parts()
            parts: list[types.Part] = []
            for part in message.get("parts") or message.get("gemini_parts") or []:
                part = _to_plain(part)
                if "text" in part:
                    parts.append(_as_text_part(str(part["text"])))
                elif "function_call" in part:
                    fc = _to_plain(part["function_call"])
                    args = _safe_json(fc.get("args"))
                    if not isinstance(args, dict):
                        args = {}
                    parts.append(
                        types.Part.from_function_call(
                            name=fc.get("name", ""),
                            args=_to_plain(args),
                        )
                    )
            text = message.get("content")
            if isinstance(text, str) and text:
                parts.append(_as_text_part(text))
            for tool_call in message.get("tool_calls") or []:
                call = _to_plain(tool_call)
                fn = _to_plain(call.get("function") or {})
                args = _safe_json(fn.get("arguments"))
                if not isinstance(args, dict):
                    args = {}
                parts.append(
                    types.Part.from_function_call(
                        name=fn.get("name", ""),
                        args=_to_plain(args),
                    )
                )
            if parts:
                contents.append(types.Content(role="model", parts=parts))
            continue

        flush_tool_parts()
        parts: list[types.Part] = []
        for part in message.get("parts") or []:
            part = _to_plain(part)
            if isinstance(part, dict) and "text" in part:
                parts.append(_as_text_part(str(part["text"])))
            else:
                parts.append(_as_text_part(str(part)))
        if not parts:
            content = message.get("content")
            if isinstance(content, list):
                for item in content:
                    parts.append(_as_text_part(str(item)))
            elif content is not None:
                parts.append(_as_text_part(str(content)))
            if parts:
                contents.append(types.Content(role="user", parts=parts))

    flush_tool_parts()

    system_instruction = "\n".join(system_lines) if system_lines else None
    return system_instruction, contents


def _convert_finish_reason(reason: Any, has_tool_calls: bool) -> str:
    if reason is None:
        return "tool_calls" if has_tool_calls else "stop"
    label = str(getattr(reason, "name", reason)).upper()
    if has_tool_calls and label in {"FINISH_REASON_STOP", "FINISH_REASON_UNSPECIFIED"}:
        return "tool_calls"
    return _FINISH_REASON_MAP.get(label, "tool_calls" if has_tool_calls else "stop")


class _ResponseBuilder:
    def __init__(self) -> None:
        self._text = ""
        self._tool_calls: dict[str, SimpleNamespace] = {}
        self._order: list[str] = []

    @property
    def content(self) -> str:
        return self._text

    def add_text(self, text: str) -> str:
        if not isinstance(text, str) or not text:
            return ""

        previous = self._text
        if text.startswith(previous):
            delta = text[len(previous):]
            if not delta:
                return ""
            self._text = previous + delta
            return delta

        self._text = text
        return text

    def add_tool_call(self, name: Any, args: Any, call_id: Any) -> tuple[SimpleNamespace, str, str]:
        call_id_str = str(call_id) if call_id else f"tool_call_{len(self._order)}"
        if call_id_str in self._tool_calls:
            call = self._tool_calls[call_id_str]
        else:
            call = SimpleNamespace(
                index=len(self._order),
                id=call_id_str,
                type="function",
                function=SimpleNamespace(name="", arguments=""),
            )
            self._tool_calls[call_id_str] = call
            self._order.append(call_id_str)

        name_delta = ""
        args_delta = ""

        if isinstance(name, str) and name:
            previous = call.function.name
            call.function.name = name
            if name.startswith(previous):
                name_delta = name[len(previous):]
            else:
                name_delta = name

        if args is not None:
            plain_args = _to_plain(args)
            if isinstance(plain_args, str):
                serialized = plain_args
            else:
                try:
                    serialized = json.dumps(plain_args, ensure_ascii=False)
                except TypeError:
                    serialized = json.dumps(plain_args, ensure_ascii=False, default=str)
            previous = call.function.arguments
            call.function.arguments = serialized
            if serialized.startswith(previous):
                args_delta = serialized[len(previous):]
            else:
                args_delta = serialized

        return call, name_delta, args_delta

    def has_tool_calls(self) -> bool:
        return bool(self._order)

    def build_message(self) -> SimpleNamespace:
        tool_calls: list[SimpleNamespace] = []
        for call_pos, call_id in enumerate(self._order):
            call = self._tool_calls[call_id]
            index = getattr(call, "index", call_pos)
            tool_calls.append(
                SimpleNamespace(
                    index=index,
                    id=call.id,
                    type=call.type,
                    function=SimpleNamespace(
                        name=call.function.name,
                        arguments=call.function.arguments,
                    ),
                )
            )
        return SimpleNamespace(
            content=self._text,
            tool_calls=tool_calls,
        )


class Gemini(LanguageModel):
    @staticmethod
    def get_menu_name() -> str:
        return "Google Gemini"

    @staticmethod
    def supported_models():
        global _GEMINI_MODELS
        fallback = _sort_gemini_models(list(_DEFAULT_GEMINI_MODELS))
        with _GEMINI_MODELS_LOCK:
            if _GEMINI_MODELS is None:
                _GEMINI_MODELS = list(fallback)
            current = list(_GEMINI_MODELS)

        api_key = gepetto.config.get_config("Gemini", "API_KEY", "GEMINI_API_KEY")
        if not api_key:
            return current

        proxy = gepetto.config.get_config("Gepetto", "PROXY")
        timeout = _httpx.Timeout(2.0, connect=2.0)
        _schedule_gemini_refresh(api_key, proxy, timeout)
        return current

    @staticmethod
    def refresh_models_sync() -> list[str]:
        api_key = gepetto.config.get_config("Gemini", "API_KEY", "GEMINI_API_KEY")
        if not api_key:
            return Gemini.supported_models()

        proxy = gepetto.config.get_config("Gepetto", "PROXY")
        timeout = _httpx.Timeout(2.0, connect=2.0)
        models = _execute_gemini_fetch(api_key, proxy, timeout)
        if models:
            _update_gemini_models(models)
        with _GEMINI_MODELS_LOCK:
            current = list(_GEMINI_MODELS) if _GEMINI_MODELS is not None else list(_DEFAULT_GEMINI_MODELS)
        return current

    @staticmethod
    def is_configured_properly() -> bool:
        return bool(gepetto.config.get_config("Gemini", "API_KEY", "GEMINI_API_KEY"))

    def __init__(self, model_name):
        self.model_name = model_name
        api_key = gepetto.config.get_config("Gemini", "API_KEY", "GEMINI_API_KEY")
        if not api_key:
            raise ValueError(
                _("Please edit the configuration file to insert your {api_provider} API key!")
                .format(api_provider="Google Gemini")
            )

        proxy = gepetto.config.get_config("Gepetto", "PROXY")
        http_options = None
        if proxy:
            http_options = types.HttpOptions(
                client_args={"proxy": proxy},
                async_client_args={"proxy": proxy},
            )

        self.client = genai.Client(api_key=api_key, http_options=http_options)

    def __str__(self):
        return self.model_name

    def _prepare_call_kwargs(
        self,
        query: Any,
        additional_model_options: dict[str, Any] | None,
    ) -> dict[str, Any]:
        options = dict(additional_model_options or {})
        system_instruction, contents = _convert_messages(query)
        if not contents:
            raise ValueError("Gemini requires at least one message to generate a response.")

        response_format = options.pop("response_format", None)
        tools_spec = options.pop("tools", None)

        tools = _convert_tools(tools_spec)

        config_kwargs: dict[str, Any] = {
            "safety_settings": list(_DEFAULT_SAFETY_SETTINGS),
        }
        try:
            # Disable Gemini "thinking" stream to avoid spamming reasoning steps (for now).
            # This needs to be in a try/except for older genai SDKs that do not support it
            config_kwargs["thinking_config"] = types.ThinkingConfig(include_thoughts=False)
        except:
            pass
        if system_instruction:
            config_kwargs["system_instruction"] = system_instruction
        if tools:
            config_kwargs["tools"] = tools
        if isinstance(response_format, dict) and response_format.get("type") == "json_object":
            config_kwargs["response_mime_type"] = "application/json"

        config_override = options.pop("config", None)
        if config_override is None:
            config = types.GenerateContentConfig(**config_kwargs)
        elif isinstance(config_override, dict):
            merged = {**config_kwargs, **config_override}
            config = types.GenerateContentConfig(**merged)
        else:
            config = config_override

        call_kwargs = {
            "model": self.model_name,
            "contents": contents,
            "config": config,
        }

        return call_kwargs

    def _emit_parts(
        self,
        parts: Iterable[Any],
        builder: _ResponseBuilder,
        stream_cb=None,
    ) -> bool:
        emitted = False
        for part in parts:
            text = getattr(part, "text", None)
            if isinstance(text, str) and text:
                delta = builder.add_text(text)
                if stream_cb and delta:
                    stream_cb(delta, None)
                if delta:
                    emitted = True
                continue

            function_call = getattr(part, "function_call", None)
            if function_call:
                call, name_delta, args_delta = builder.add_tool_call(
                    getattr(function_call, "name", None),
                    getattr(function_call, "args", None),
                    getattr(function_call, "id", None),
                )
                if stream_cb and (name_delta or args_delta):
                    call_index = getattr(call, "index", None)
                    if call_index is None:
                        try:
                            call_index = builder._order.index(call.id)
                        except ValueError:
                            call_index = len(builder._order)
                        call.index = call_index
                    stream_cb(
                        SimpleNamespace(
                            tool_calls=[
                                SimpleNamespace(
                                    index=call_index,
                                    id=call.id,
                                    type=call.type,
                                    function=SimpleNamespace(
                                        name=name_delta,
                                        arguments=args_delta,
                                    ),
                                )
                            ]
                        ),
                        None,
                    )
                emitted = True

        return emitted

    def _ingest_response(self, response: Any, builder: _ResponseBuilder) -> Any:
        finish_reason = None
        for candidate in getattr(response, "candidates", None) or []:
            finish_reason = getattr(candidate, "finish_reason", finish_reason)
            content = getattr(candidate, "content", None)
            parts = getattr(content, "parts", None) or []
            if parts:
                if self._emit_parts(parts, builder):
                    continue

            text = getattr(content, "text", None)
            if isinstance(text, str) and text:
                builder.add_text(text)
        if not builder.content:
            text = getattr(response, "text", None)
            if isinstance(text, str) and text:
                builder.add_text(text)
        return finish_reason

    def query_model(self, query, cb, stream=False, additional_model_options=None):
        try:
            call_kwargs = self._prepare_call_kwargs(query, additional_model_options)
            if stream:
                response_stream = self.client.models.generate_content_stream(**call_kwargs)
                builder = _ResponseBuilder()
                finish_reason = None
                for chunk in response_stream:
                    chunk_emitted = False
                    for candidate in getattr(chunk, "candidates", None) or []:
                        finish_reason = getattr(candidate, "finish_reason", finish_reason)
                        content = getattr(candidate, "content", None)
                        parts = getattr(content, "parts", None) or []
                        if parts:
                            if self._emit_parts(parts, builder, cb):
                                chunk_emitted = True
                            continue

                        text = getattr(content, "text", None)
                        if isinstance(text, str) and text:
                            delta = builder.add_text(text)
                            if delta:
                                if cb:
                                    cb(delta, None)
                                chunk_emitted = True

                    if not chunk_emitted:
                        text = getattr(chunk, "text", None)
                        if isinstance(text, str) and text:
                            delta = builder.add_text(text)
                            if delta:
                                if cb:
                                    cb(delta, None)
                                chunk_emitted = True
                final_reason = _convert_finish_reason(finish_reason, builder.has_tool_calls())
                cb(None, final_reason)
            else:
                response = self.client.models.generate_content(**call_kwargs)
                builder = _ResponseBuilder()
                self._ingest_response(response, builder)
                message = builder.build_message()
                ida_kernwin.execute_sync(
                    functools.partial(cb, response=message),
                    ida_kernwin.MFF_WRITE,
                )
        except Exception as exc:
            error_message = _("General exception encountered while running the query: {error}").format(error=str(exc))
            print(error_message)
            _notify_error(cb, error_message)

    def query_model_async(self, query, cb, stream=False, additional_model_options=None):
        thread = threading.Thread(
            target=self.query_model,
            args=(query, cb, stream, additional_model_options),
            daemon=True,
        )
        thread.start()


gepetto.models.model_manager.register_model(Gemini)

```

`gepetto/models/groq.py`:

```py
import groq
import httpx as _httpx

import gepetto.config
import gepetto.models.model_manager
from gepetto.models.openai import GPT

_ = gepetto.config._

LLAMA_31_MODEL_NAME = "llama-3.1-70b-versatile"
LLAMA_32_MODEL_NAME = "llama-3.2-90b-text-preview"
MIXTRAL_MODEL_NAME = "mixtral-8x7b-32768"

class Groq(GPT):
    @staticmethod
    def get_menu_name() -> str:
        return "Groq"

    @staticmethod
    def supported_models():
        return [LLAMA_31_MODEL_NAME, LLAMA_32_MODEL_NAME, MIXTRAL_MODEL_NAME]

    @staticmethod
    def is_configured_properly() -> bool:
        # The plugin is configured properly if the API key is provided, otherwise it should not be shown.
        return bool(gepetto.config.get_config("Groq", "API_KEY", "GROQ_API_KEY"))

    def __init__(self, model):
        try:
            super().__init__(model)
        except ValueError:
            pass  # May throw if the OpenAI API key isn't given, but we don't need any to use Groq.

        self.model = model
        api_key = gepetto.config.get_config("Groq", "API_KEY", "GROQ_API_KEY")
        if not api_key:
            raise ValueError(_("Please edit the configuration file to insert your {api_provider} API key!")
                             .format(api_provider="Groq"))

        proxy = gepetto.config.get_config("Gepetto", "PROXY")
        base_url = gepetto.config.get_config("Groq", "BASE_URL", "GROQ_BASE_URL")

        self.client = groq.Groq(
            api_key=api_key,
            base_url=base_url,
            http_client=_httpx.Client(
                proxy=proxy,
            ) if proxy else None
        )

gepetto.models.model_manager.register_model(Groq)

```

`gepetto/models/kluster.py`:

```py
import openai
import json
import httpx as _httpx

import gepetto.config
import gepetto.models.model_manager
from gepetto.models.openai import GPT

_ = gepetto.config._

# Define all available Kluster.ai models
DEFAULT_MODELS = [
    "deepseek-ai/DeepSeek-R1",
    "deepseek-ai/DeepSeek-V3",
    "deepseek-ai/DeepSeek-V3-0324",
    "google/gemma-3-27b-it",
    "klusterai/Meta-Llama-3.1-8B-Instruct-Turbo",
    "klusterai/Meta-Llama-3.1-405B-Instruct-Turbo",
    "klusterai/Meta-Llama-3.3-70B-Instruct-Turbo",
    "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8",
    "meta-llama/Llama-4-Scout-17B-16E-Instruct",
    "Qwen/Qwen2.5-VL-7B-Instruct"
]

class Kluster(GPT):
    @staticmethod
    def get_menu_name() -> str:
        return "Kluster.ai"

    @staticmethod
    def supported_models():
        # Check if custom models are defined in config
        # If not, use the default models
        config_models = gepetto.config.get_config("Kluster", "MODELS")
        if config_models:
            try:
                return json.loads(config_models)
            except json.JSONDecodeError:
                # If it's not valid JSON, treat it as comma-separated list
                return [model.strip() for model in config_models.split(",")]
        return DEFAULT_MODELS

    @staticmethod
    def is_configured_properly() -> bool:
        # The plugin is configured properly if the API key is provided
        return bool(gepetto.config.get_config("Kluster", "API_KEY"))

    def __init__(self, model):
        try:
            super().__init__(model)
        except ValueError:
            pass

        self.model = model
        api_key = gepetto.config.get_config("Kluster", "API_KEY")
        if not api_key:
            raise ValueError(
                _("Please edit the configuration file to insert your {api_provider} API key!")
                .format(api_provider=Kluster.get_menu_name()))

        proxy = gepetto.config.get_config("Gepetto", "PROXY")
        base_url = gepetto.config.get_config("Kluster", "BASE_URL",
                                           default="https://api.kluster.ai/v1")

        self.client = openai.OpenAI(
            api_key=api_key,
            base_url=base_url,
            http_client=_httpx.Client(proxy=proxy) if proxy else None)

gepetto.models.model_manager.register_model(Kluster) 
```

`gepetto/models/local_lmstudio.py`:

```py
import asyncio
import threading
import time

import httpx as _httpx
from gepetto.models.openai import GPT
import openai

import gepetto.models.model_manager
import gepetto.config

_ = gepetto.config._

LMSTUDIO_MODELS = None
_LMSTUDIO_MODELS_LOCK = threading.Lock()
_LMSTUDIO_REFRESH_THREAD: threading.Thread | None = None
_LMSTUDIO_LAST_REFRESH: float = 0.0


def _trigger_menu_refresh() -> None:
    try:
        from gepetto.ida import ui as ida_ui
        ida_ui.trigger_model_select_menu_regeneration()
    except Exception:
        pass


def _normalize_base_url(base_url: str | None) -> str:
    if not base_url:
        base_url = "http://127.0.0.1:1234/v1/"
    if not base_url.endswith("/"):
        base_url = base_url + "/"
    return base_url


def _update_lmstudio_models(models: list[str], *, notify: bool = True) -> None:
    global LMSTUDIO_MODELS
    normalized = sorted(dict.fromkeys(models))
    with _LMSTUDIO_MODELS_LOCK:
        current = list(LMSTUDIO_MODELS) if LMSTUDIO_MODELS is not None else []
        if normalized == current:
            return
        LMSTUDIO_MODELS = normalized
    if notify:
        _trigger_menu_refresh()


def _execute_lmstudio_fetch(
    base_url: str | None,
    proxy: str | None,
    timeout: _httpx.Timeout,
) -> list[str]:
    loop = asyncio.new_event_loop()
    try:
        asyncio.set_event_loop(loop)
        return loop.run_until_complete(
            _fetch_lmstudio_models_async(base_url, proxy, timeout)
        )
    except Exception as exc:
        resolved_base = _normalize_base_url(base_url)
        print(
            _("Failed to fetch models from {base_url}: {error}").format(
                base_url=f"{resolved_base}models",
                error=exc,
            )
        )
        return []
    finally:
        asyncio.set_event_loop(None)
        loop.close()


def _schedule_lmstudio_refresh(
    base_url: str | None,
    proxy: str | None,
    timeout: _httpx.Timeout,
) -> None:
    global _LMSTUDIO_REFRESH_THREAD, _LMSTUDIO_LAST_REFRESH
    with _LMSTUDIO_MODELS_LOCK:
        if _LMSTUDIO_REFRESH_THREAD and _LMSTUDIO_REFRESH_THREAD.is_alive():
            return
        now = time.monotonic()
        if now - _LMSTUDIO_LAST_REFRESH < 5.0:
            return
        _LMSTUDIO_LAST_REFRESH = now
        _LMSTUDIO_REFRESH_THREAD = threading.Thread(
            target=_refresh_lmstudio_models_background,
            args=(base_url, proxy, timeout),
            name="GepettoLMStudioModelRefresh",
            daemon=True,
        )
        _LMSTUDIO_REFRESH_THREAD.start()


def _refresh_lmstudio_models_background(
    base_url: str | None,
    proxy: str | None,
    timeout: _httpx.Timeout,
) -> None:
    global _LMSTUDIO_REFRESH_THREAD
    try:
        models = _execute_lmstudio_fetch(base_url, proxy, timeout)
        if models:
            _update_lmstudio_models(models)
    finally:
        with _LMSTUDIO_MODELS_LOCK:
            _LMSTUDIO_REFRESH_THREAD = None


async def _fetch_lmstudio_models_async(
    base_url: str | None,
    proxy: str | None,
    timeout: _httpx.Timeout,
) -> list[str]:
    resolved_base = _normalize_base_url(base_url)
    endpoint = f"{resolved_base}models"
    transport = None
    if proxy:
        try:
            transport = _httpx.AsyncHTTPTransport(proxy=proxy)
        except Exception as transport_exc:
            print(
                _("Failed to configure proxy for LM Studio models: {error}").format(
                    error=transport_exc
                )
            )
    try:
        async with _httpx.AsyncClient(timeout=timeout, transport=transport) as client:
            response = await client.get(endpoint)
    except (
        _httpx.ConnectError,
        _httpx.ConnectTimeout,
        _httpx.ReadTimeout,
        _httpx.TimeoutException,
    ):
        return []

    if response.status_code != 200:
        print(
            _("Failed to fetch models from {base_url}: {status_code}").format(
                base_url=endpoint,
                status_code=response.status_code,
            )
        )
        return []

    payload = response.json() or {}
    models = [
        model.get("id")
        for model in payload.get("data", [])
        if isinstance(model, dict) and model.get("id")
    ]
    models.sort()
    return models

class LMStudio(GPT):
    @staticmethod
    def get_menu_name() -> str:
        return "LM Studio"

    @staticmethod
    def supported_models() -> list:
        global LMSTUDIO_MODELS
        with _LMSTUDIO_MODELS_LOCK:
            if LMSTUDIO_MODELS is None:
                LMSTUDIO_MODELS = []
            models = list(LMSTUDIO_MODELS)

        base_url = gepetto.config.get_config("LMStudio", "BASE_URL", default="http://127.0.0.1:1234/v1/")
        proxy = gepetto.config.get_config("Gepetto", "PROXY")
        timeout = _httpx.Timeout(2.0, connect=2.0)
        _schedule_lmstudio_refresh(base_url, proxy, timeout)
        return models

    @staticmethod
    def is_configured_properly() -> bool:
        return True

    @staticmethod
    def refresh_models_sync() -> list[str]:
        base_url = gepetto.config.get_config("LMStudio", "BASE_URL", default="http://127.0.0.1:1234/v1/")
        proxy = gepetto.config.get_config("Gepetto", "PROXY")
        timeout = _httpx.Timeout(2.0, connect=2.0)
        models = _execute_lmstudio_fetch(base_url, proxy, timeout)
        if models:
            _update_lmstudio_models(models)
        with _LMSTUDIO_MODELS_LOCK:
            current = list(LMSTUDIO_MODELS) if LMSTUDIO_MODELS is not None else []
        return current

    def __init__(self, model):
        try:
            super().__init__(model)
        except ValueError:
            # Ensure streaming state flags exist even when OpenAI init fails
            self._streaming_restriction_active = False
            self._fallback_notice_sent = False

        base_url = gepetto.config.get_config("LMStudio", "BASE_URL", default="http://127.0.0.1:1234/v1/")
        proxy = gepetto.config.get_config("Gepetto", "PROXY")

        self.model = model
        self.client = openai.OpenAI(
            api_key="NO_API_KEY",
            base_url=base_url,
            http_client=_httpx.Client(
                proxy=proxy,
            ) if proxy else None
        )

    def query_model(self, query, cb, stream=False, *args, **kwargs):
        """
        Compatible with callers that may pass an extra positional 'context' arg:
          - query_model(query, cb, additional_model_options)
          - query_model(query, cb, context, additional_model_options)
          - query_model(query, cb, context=?, additional_model_options=?)
        We ACCEPT extra positional args but DO NOT forward them to super(),
        because GPT.query_model(...) doesn't expect them.
        """
        pos_args = list(args)

        # 1) Determine additional_model_options from kwargs or trailing positional dict
        if "additional_model_options" in kwargs:
            opt = kwargs.get("additional_model_options") or {}
            # If caller also gave a trailing positional dict, drop it to avoid dup binding
            if pos_args and isinstance(pos_args[-1], dict):
                pos_args.pop()
        else:
            if pos_args and isinstance(pos_args[-1], dict):
                opt = pos_args.pop()  # treat last positional dict as options
            else:
                opt = {}

        # 2) Make a copy so we don't mutate caller state
        additional_model_options = dict(opt)

        # 3) Keep your JSON response_format compatibility shim
        rf = additional_model_options.get("response_format", {})
        if isinstance(rf, dict) and rf.get("type") == "json_object":
            additional_model_options["response_format"] = {
                "type": "json_schema",
                "json_schema": {"schema": {"type": "object"}}
            }

        # 4) IMPORTANT: do NOT forward any other positional args to super()
        #    GPT.query_model only expects (query, cb, additional_model_options)
        return super().query_model(
            query,
            cb,
            stream,
            additional_model_options=additional_model_options,
        )

gepetto.models.model_manager.register_model(LMStudio)

```

`gepetto/models/local_ollama.py`:

```py
import asyncio
import functools
import threading
import time

import httpx as _httpx
import ida_kernwin
import ollama

from gepetto.models.base import LanguageModel
import gepetto.models.model_manager
import gepetto.config

_ = gepetto.config._

OLLAMA_MODELS = None
_OLLAMA_MODELS_LOCK = threading.Lock()
_OLLAMA_REFRESH_THREAD: threading.Thread | None = None
_OLLAMA_LAST_REFRESH: float = 0.0

def create_client(**kwargs):
    host = gepetto.config.get_config("Ollama", "HOST", default="http://localhost:11434")
    return ollama.Client(host=host, **kwargs)


def _trigger_menu_refresh() -> None:
    try:
        from gepetto.ida import ui as ida_ui
        ida_ui.trigger_model_select_menu_regeneration()
    except Exception:
        pass


def _normalize_host(host: str | None) -> str:
    if not host:
        host = "http://localhost:11434"
    return host.rstrip("/")


def _update_ollama_models(models: list[str], *, notify: bool = True) -> None:
    global OLLAMA_MODELS
    normalized = sorted(dict.fromkeys(models))
    with _OLLAMA_MODELS_LOCK:
        current = list(OLLAMA_MODELS) if OLLAMA_MODELS is not None else []
        if normalized == current:
            return
        OLLAMA_MODELS = normalized
    if notify:
        _trigger_menu_refresh()


def _execute_ollama_fetch(host: str | None, timeout: _httpx.Timeout) -> list[str]:
    loop = asyncio.new_event_loop()
    try:
        asyncio.set_event_loop(loop)
        return loop.run_until_complete(_fetch_ollama_models_async(host, timeout))
    except Exception as exc:
        resolved_host = _normalize_host(host)
        print(
            _("Failed to fetch models from {base_url}: {error}").format(
                base_url=f"{resolved_host}/api/tags",
                error=exc,
            )
        )
        return []
    finally:
        asyncio.set_event_loop(None)
        loop.close()


def _schedule_ollama_refresh(host: str | None, timeout: _httpx.Timeout) -> None:
    global _OLLAMA_REFRESH_THREAD, _OLLAMA_LAST_REFRESH
    with _OLLAMA_MODELS_LOCK:
        if _OLLAMA_REFRESH_THREAD and _OLLAMA_REFRESH_THREAD.is_alive():
            return
        now = time.monotonic()
        if now - _OLLAMA_LAST_REFRESH < 5.0:
            return
        _OLLAMA_LAST_REFRESH = now
        _OLLAMA_REFRESH_THREAD = threading.Thread(
            target=_refresh_ollama_models_background,
            args=(host, timeout),
            name="GepettoOllamaModelRefresh",
            daemon=True,
        )
        _OLLAMA_REFRESH_THREAD.start()


def _refresh_ollama_models_background(host: str | None, timeout: _httpx.Timeout) -> None:
    global _OLLAMA_REFRESH_THREAD
    try:
        models = _execute_ollama_fetch(host, timeout)
        if models:
            _update_ollama_models(models)
    finally:
        with _OLLAMA_MODELS_LOCK:
            _OLLAMA_REFRESH_THREAD = None


async def _fetch_ollama_models_async(host: str | None, timeout: _httpx.Timeout) -> list[str]:
    resolved_host = _normalize_host(host)
    endpoint = f"{resolved_host}/api/tags"
    try:
        async with _httpx.AsyncClient(timeout=timeout) as client:
            response = await client.get(endpoint)
    except (
        _httpx.ConnectError,
        _httpx.ConnectTimeout,
        _httpx.ReadTimeout,
        _httpx.TimeoutException,
    ):
        return []

    if response.status_code != 200:
        print(
            _("Failed to fetch models from {base_url}: {status_code}").format(
                base_url=endpoint,
                status_code=response.status_code,
            )
        )
        return []

    payload = response.json() or {}
    models = [
        model.get("model")
        for model in payload.get("models", [])
        if isinstance(model, dict) and model.get("model")
    ]
    models.sort()
    return models

class Ollama(LanguageModel):
    @staticmethod
    def get_menu_name() -> str:
        return "Ollama"

    @staticmethod
    def supported_models():
        global OLLAMA_MODELS
        with _OLLAMA_MODELS_LOCK:
            if OLLAMA_MODELS is None:
                OLLAMA_MODELS = []
            models = list(OLLAMA_MODELS)

        host = gepetto.config.get_config("Ollama", "HOST", default="http://localhost:11434")
        timeout = _httpx.Timeout(2.0, connect=2.0)
        _schedule_ollama_refresh(host, timeout)
        return models

    @staticmethod
    def is_configured_properly() -> bool:
        return True

    @staticmethod
    def refresh_models_sync() -> list[str]:
        host = gepetto.config.get_config("Ollama", "HOST", default="http://localhost:11434")
        timeout = _httpx.Timeout(2.0, connect=2.0)
        models = _execute_ollama_fetch(host, timeout)
        if models:
            _update_ollama_models(models)
        with _OLLAMA_MODELS_LOCK:
            current = list(OLLAMA_MODELS) if OLLAMA_MODELS is not None else []
        return current

    def __str__(self):
        return self.model

    def __init__(self, model):
        self.model = model
        self.client = create_client()

    def query_model_async(self, query, cb, stream=False, additional_model_options = None):
        if additional_model_options is None:
            additional_model_options = {}
        t = threading.Thread(target=self.query_model, args=[query, cb, stream, additional_model_options])
        t.start()

    def query_model(self, query, cb, stream=False, additional_model_options=None):
        # Convert the OpenAI json parameter for Ollama
        kwargs = {}
        if "response_format" in additional_model_options and additional_model_options["response_format"]["type"] == "json_object":
            kwargs["format"] = "json"

        try:
            if type(query) is str:
                conversation = [
                    {"role": "user", "content": query}
                ]
            else:
                conversation = query

            response = self.client.chat(model=self.model,
                                        messages=conversation,
                                        stream=stream,
                                        **kwargs)
            if not stream:
                ida_kernwin.execute_sync(functools.partial(cb, response=response["message"]["content"]),
                                         ida_kernwin.MFF_WRITE)
            else:
                for chunk in response:
                    cb(chunk['message'], chunk.done_reason)
        except Exception as e:
            print(e)


gepetto.models.model_manager.register_model(Ollama)

```

`gepetto/models/model_manager.py`:

```py
import importlib.util
import os
import pathlib

from gepetto.models.base import LanguageModel

MODEL_LIST: list[LanguageModel] = list()


def register_model(model: LanguageModel):
    if not issubclass(model, LanguageModel):
        return
    if any(
        existing.get_menu_name() == model.get_menu_name() for existing in MODEL_LIST
    ):
        return
    if not model.is_configured_properly():
        return
    MODEL_LIST.append(model)


def list_models():
    return MODEL_LIST


def instantiate_model(model):
    """
    Instantiates a model based on its name
    :param model: The model to use
    :return:
    """
    for m in MODEL_LIST:
        available = m.supported_models()
        if model in available:
            return m(model)
        refresher = getattr(m, "refresh_models_sync", None)
        if callable(refresher):
            try:
                refreshed = refresher()
            except Exception:
                continue
            if model in (refreshed or []):
                return m(model)
    raise RuntimeError(f"{model} does not exist!")


def get_fallback_model():
    """
    This function returns the first model that can be instantiated properly.
    :return:
    """
    for model_plugin in MODEL_LIST:
        available = model_plugin.supported_models()
        for m in available:
            try:
                return model_plugin(m)
            except:
                continue
    raise RuntimeError(
        "No models available! Edit your configuration file and try again."
    )


def load_available_models():
    folder = pathlib.Path(os.path.dirname(__file__))
    for py_file in folder.glob("*.py"):
        module_name = py_file.stem  # Get the file name without extension
        spec = importlib.util.spec_from_file_location(module_name, py_file)
        module = importlib.util.module_from_spec(spec)
        try:
            spec.loader.exec_module(module)
        except (ImportError, ModuleNotFoundError) as e:
            print("Module", module_name, "loading failed:", repr(e), "Skipping..")

```

`gepetto/models/novita_ai.py`:

```py
import openai
import httpx as _httpx

import gepetto.config
import gepetto.models.model_manager
from gepetto.models.openai import GPT

_ = gepetto.config._

NOVITA_MODELS = [
  "deepseek/deepseek-r1",
  "deepseek/deepseek_v3",
  "meta-llama/llama-3.3-70b-instruct",
  "meta-llama/llama-3.1-70b-instruct",
  "meta-llama/llama-3.1-405b-instruct",
]

class NovitaAI(GPT):
    @staticmethod
    def get_menu_name() -> str:
        return "Novita AI"

    @staticmethod
    def supported_models():
        return NOVITA_MODELS

    @staticmethod
    def is_configured_properly() -> bool:
        # The plugin is configured properly if the API key is provided, otherwise it should not be shown.
        return bool(gepetto.config.get_config("NovitaAI", "API_KEY", "NOVITAAI_API_KEY"))

    def __init__(self, model):
        try:
            super().__init__(model)
        except ValueError:
            pass  # May throw if the OpenAI API key isn't given, but we don't need it.

        self.model = model
        api_key = gepetto.config.get_config("NovitaAI", "API_KEY", "NOVITAAI_API_KEY")
        if not api_key:
            print(_("Please edit the configuration file to insert your {api_provider} API key!")
                  .format(api_provider="Novita AI"))
            raise ValueError("No valid Novita AI API key found")

        proxy = gepetto.config.get_config("Gepetto", "PROXY")

        self.client = openai.OpenAI(
            api_key=api_key,
            base_url="https://api.novita.ai/v3/openai",
            http_client=_httpx.Client(
                proxy=proxy,
            ) if proxy else None
        )

gepetto.models.model_manager.register_model(NovitaAI)

```

`gepetto/models/openai.py`:

```py
import asyncio
import functools
import re
import threading
import time

import httpx as _httpx
import ida_kernwin
import openai
from types import SimpleNamespace

from gepetto.models.base import LanguageModel
import gepetto.models.model_manager
import gepetto.config

_ = gepetto.config._

GPT_52_MODEL_NAME = "gpt-5.2"
GPT_51_MODEL_NAME = "gpt-5.1"
GPT_5_MODEL_NAME = "gpt-5"
GPT_5_MINI_MODEL_NAME = "gpt-5-mini"
GPT_5_NANO_MODEL_NAME = "gpt-5-nano"
GPT4_MODEL_NAME = "gpt-4-turbo"
GPT4o_MODEL_NAME = "gpt-4o"
GPTo4_MINI_MODEL_NAME = "o4-mini"
GPT41_MODEL_NAME = "gpt-4.1"
GPTo3_MODEL_NAME = "o3"
GPTo3_PRO_MODEL_NAME = "o3-pro"

_DEFAULT_OPENAI_MODELS = [
    GPT_52_MODEL_NAME,
    GPT_51_MODEL_NAME,
    GPT_5_MODEL_NAME,
    GPT_5_MINI_MODEL_NAME,
    GPT_5_NANO_MODEL_NAME,
    GPT4_MODEL_NAME,
    GPT4o_MODEL_NAME,
    GPTo4_MINI_MODEL_NAME,
    GPT41_MODEL_NAME,
    GPTo3_MODEL_NAME,
    GPTo3_PRO_MODEL_NAME,
]

_OPENAI_MODELS: list[str] | None = None
_OPENAI_MODELS_LOCK = threading.Lock()
_OPENAI_REFRESH_THREAD: threading.Thread | None = None
_OPENAI_LAST_REFRESH: float = 0.0

OPENAI_RESTRICTED_MODELS = {
    GPT_52_MODEL_NAME,
    GPT_51_MODEL_NAME,
    GPT_5_MODEL_NAME,
    GPT_5_MINI_MODEL_NAME,
    GPTo3_MODEL_NAME,
    GPTo3_PRO_MODEL_NAME,
}

_STREAMING_RESTRICTION_PATTERNS = (
    "organization is not verified",
    "must be verified",
    "'param': 'stream'",
    '"param": "stream"',
    "verify your organization",
    "unsupported for this organization",
    "streaming is not currently supported",
)


def _is_supported_openai_model(model_id: str) -> bool:
    """Return True if model_id should appear in the chat model menu."""
    if not model_id:
        return False
    lowered = model_id.lower()
    if re.search(
        r"-\d{4}(-\d{2}-\d{2})?$|tts|omni|realtime|image|audio|transcribe", 
        lowered
    ):
        return False
    if lowered.startswith("gpt-"):
        return True
    # Accept optimized o* chat models (o3, o4-mini, etc.)
    return lowered.startswith("o")


def _sort_openai_models(models: list[str]) -> list[str]:
    deduped = list(dict.fromkeys(models))
    default_models_set = set(_DEFAULT_OPENAI_MODELS)
    other_models = [m for m in deduped if m not in default_models_set]
    other_models.sort(key=str, reverse=True)
    return _DEFAULT_OPENAI_MODELS + other_models


def _trigger_menu_refresh() -> None:
    try:
        from gepetto.ida import ui as ida_ui
        ida_ui.trigger_model_select_menu_regeneration()
    except Exception:
        pass


def _update_openai_models(models: list[str], *, notify: bool = True) -> None:
    global _OPENAI_MODELS
    normalized = _sort_openai_models(models)
    with _OPENAI_MODELS_LOCK:
        current = list(_OPENAI_MODELS) if _OPENAI_MODELS is not None else []
        if normalized == current:
            return
        _OPENAI_MODELS = normalized
    if notify:
        _trigger_menu_refresh()


def _execute_openai_fetch(
    endpoint: str,
    headers: dict[str, str],
    proxy: str | None,
    timeout: _httpx.Timeout,
) -> list[str]:
    loop = asyncio.new_event_loop()
    try:
        asyncio.set_event_loop(loop)
        return loop.run_until_complete(
            _fetch_openai_models_async(endpoint, headers, proxy, timeout)
        )
    except Exception as exc:
        print(_("Failed to fetch OpenAI models: {error}").format(error=exc))
        return []
    finally:
        asyncio.set_event_loop(None)
        loop.close()


def _schedule_openai_refresh(
    endpoint: str,
    headers: dict[str, str],
    proxy: str | None,
    timeout: _httpx.Timeout,
) -> None:
    global _OPENAI_REFRESH_THREAD, _OPENAI_LAST_REFRESH
    with _OPENAI_MODELS_LOCK:
        if _OPENAI_REFRESH_THREAD and _OPENAI_REFRESH_THREAD.is_alive():
            return
        now = time.monotonic()
        if now - _OPENAI_LAST_REFRESH < 5.0:
            return
        _OPENAI_LAST_REFRESH = now
        _OPENAI_REFRESH_THREAD = threading.Thread(
            target=_refresh_openai_models_background,
            args=(endpoint, headers, proxy, timeout),
            name="GepettoOpenAIModelRefresh",
            daemon=True,
        )
        _OPENAI_REFRESH_THREAD.start()


def _refresh_openai_models_background(
    endpoint: str,
    headers: dict[str, str],
    proxy: str | None,
    timeout: _httpx.Timeout,
) -> None:
    global _OPENAI_REFRESH_THREAD
    try:
        models = _execute_openai_fetch(endpoint, headers, proxy, timeout)
        if models:
            _update_openai_models(models)
    finally:
        with _OPENAI_MODELS_LOCK:
            _OPENAI_REFRESH_THREAD = None


async def _fetch_openai_models_async(
    endpoint: str,
    headers: dict[str, str],
    proxy: str | None,
    timeout: _httpx.Timeout,
) -> list[str]:
    transport = None
    if proxy:
        try:
            transport = _httpx.AsyncHTTPTransport(proxy=proxy)
        except Exception as transport_exc:
            print(
                _("Failed to configure proxy for OpenAI models: {error}").format(
                    error=transport_exc
                )
            )
    try:
        async with _httpx.AsyncClient(timeout=timeout, transport=transport) as client:
            response = await client.get(endpoint, headers=headers)
    except (
        _httpx.ConnectError,
        _httpx.ConnectTimeout,
        _httpx.ReadTimeout,
        _httpx.TimeoutException,
    ):
        return []

    if response.status_code != 200:
        print(
            _("Failed to fetch models from {base_url}: {status_code}").format(
                base_url=endpoint,
                status_code=response.status_code,
            )
        )
        return []

    data = response.json() or {}
    models = [
        model_id
        for model_id in (
            (model or {}).get("id")
            for model in data.get("data", [])
        )
        if _is_supported_openai_model(model_id)
    ]
    return _sort_openai_models(models or list(_DEFAULT_OPENAI_MODELS))


def _notify_stream_error(callback, message: str) -> None:
    if callback is None:
        return

    payload = SimpleNamespace(error=message)
    try:
        callback(payload, "error")
    except TypeError:
        callback(payload)


def _notify_non_stream_error(callback, message: str) -> None:
    if callback is None:
        return

    payload = SimpleNamespace(error=message)

    def _invoke_callback():
        try:
            callback(payload)
        except TypeError:
            callback(payload, "error")

    ida_kernwin.execute_sync(_invoke_callback, ida_kernwin.MFF_WRITE)


class GPT(LanguageModel):
    @staticmethod
    def get_menu_name() -> str:
        return "OpenAI"

    @staticmethod
    def supported_models():
        global _OPENAI_MODELS
        fallback = _sort_openai_models(list(_DEFAULT_OPENAI_MODELS))
        with _OPENAI_MODELS_LOCK:
            if _OPENAI_MODELS is None:
                _OPENAI_MODELS = list(fallback)
            current = list(_OPENAI_MODELS)

        api_key = gepetto.config.get_config("OpenAI", "API_KEY", "OPENAI_API_KEY")
        if not api_key:
            return current

        base_url = gepetto.config.get_config("OpenAI", "BASE_URL", "OPENAI_BASE_URL")
        if not base_url:
            base_url = "https://api.openai.com/v1"
        base_url = base_url.rstrip("/")
        endpoint = f"{base_url}/models"

        proxy = gepetto.config.get_config("Gepetto", "PROXY")
        headers = {"Authorization": f"Bearer {api_key}"}
        timeout = _httpx.Timeout(2.0, connect=2.0)
        _schedule_openai_refresh(endpoint, headers, proxy, timeout)
        return current

    @staticmethod
    def refresh_models_sync() -> list[str]:
        api_key = gepetto.config.get_config("OpenAI", "API_KEY", "OPENAI_API_KEY")
        if not api_key:
            return GPT.supported_models()

        base_url = gepetto.config.get_config("OpenAI", "BASE_URL", "OPENAI_BASE_URL")
        if not base_url:
            base_url = "https://api.openai.com/v1"
        base_url = base_url.rstrip("/")
        endpoint = f"{base_url}/models"

        proxy = gepetto.config.get_config("Gepetto", "PROXY")
        headers = {"Authorization": f"Bearer {api_key}"}
        timeout = _httpx.Timeout(2.0, connect=2.0)
        models = _execute_openai_fetch(endpoint, headers, proxy, timeout)
        if models:
            _update_openai_models(models)
        with _OPENAI_MODELS_LOCK:
            current = list(_OPENAI_MODELS) if _OPENAI_MODELS is not None else list(_DEFAULT_OPENAI_MODELS)
        return current

    @staticmethod
    def is_configured_properly() -> bool:
        # The plugin is configured properly if the API key is provided, otherwise it should not be shown.
        return bool(gepetto.config.get_config("OpenAI", "API_KEY", "OPENAI_API_KEY"))

    def __init__(self, model):
        self.model = model
        self.input_tokens = 0
        self.output_tokens = 0
        self._streaming_restriction_active = False
        self._fallback_notice_sent = False

        # Get API key
        api_key = gepetto.config.get_config("OpenAI", "API_KEY", "OPENAI_API_KEY")
        if not api_key:
            raise ValueError(_("Please edit the configuration file to insert your {api_provider} API key!")
                             .format(api_provider="OpenAI"))

        proxy = gepetto.config.get_config("Gepetto", "PROXY")
        base_url = gepetto.config.get_config("OpenAI", "BASE_URL", "OPENAI_BASE_URL")

        self.client = openai.OpenAI(
            api_key=api_key,
            base_url=base_url,
            http_client=_httpx.Client(
                proxy=proxy,
            ) if proxy else None
        )


    def __str__(self):
        return self.model

    def _is_restricted_model(self) -> bool:
        return self.model in OPENAI_RESTRICTED_MODELS

    @staticmethod
    def _matches_streaming_restriction(message: str) -> bool:
        if not message:
            return False
        lowered = message.lower()
        return any(pattern in lowered for pattern in _STREAMING_RESTRICTION_PATTERNS)

    def _emit_streaming_notice(self, error_message: str | None = None) -> None:
        if self._fallback_notice_sent:
            return
        if error_message:
            notice = _(
                "Streaming rejected for {model}: {error}\nRetrying without streaming."
            ).format(model=self.model, error=error_message)
        else:
            notice = _(
                "Streaming disabled for {model} after a previous API rejection; retrying without streaming."
            ).format(model=self.model)
        try:
            print(notice)
        except Exception:
            pass
        self._fallback_notice_sent = True

    def supports_streaming(self) -> bool:
        if self._streaming_restriction_active and self._is_restricted_model():
            return False
        return True

    def query_model(self, query, cb, stream=False, additional_model_options=None):
        """
        Function which sends a query to a GPT-API-compatible model and calls a callback when the response is available.
        Blocks until the response is received
        :param query: The request to send to the model. It can be a single string, or a sequence of messages in a
        dictionary for a whole conversation.
        :param cb: The function to which the response will be passed to.
        :param additional_model_options: Additional parameters used when creating the model object. Typically, for
        OpenAI, response_format={"type": "json_object"}.
        """
        if additional_model_options is None:
            additional_model_options = {}
        
        # Disable streaming for models that don't support it
        if stream and not self.supports_streaming():
            stream = False
            self._emit_streaming_notice()
        
        try:
            if type(query) is str:
                conversation = [
                    {"role": "user", "content": query}
                ]
            else:
                conversation = query

            response = self.client.chat.completions.create(
                model=self.model,
                messages=conversation,
                stream=stream,
                **additional_model_options
            )

            if not stream:
                # Return the full message object so that callers can access
                # additional data such as tool calls when using the OpenAI
                # function calling API.
                message = response.choices[0].message
                ida_kernwin.execute_sync(
                    functools.partial(cb, response=message),
                    ida_kernwin.MFF_WRITE,
                )
                self.input_tokens += response.usage.prompt_tokens
                self.output_tokens += response.usage.completion_tokens
            else:
                for chunk in response:
                    delta = chunk.choices[0].delta
                    finished = chunk.choices[0].finish_reason
                    if hasattr(chunk, "usage") and chunk.usage:  # If this is a last chunk, record token count.
                        self.input_tokens += chunk.usage.prompt_tokens
                        self.output_tokens += chunk.usage.completion_tokens
                    cb(delta, finished)
        except openai.BadRequestError as e:
            error_message = str(e)
            if stream and self._is_restricted_model() and self._matches_streaming_restriction(error_message):
                self._streaming_restriction_active = True
                self._emit_streaming_notice(error_message)
                _notify_stream_error(cb, _("Streaming rejected by API; retrying without streaming."))
                return
            # Context length exceeded. Determine the max number of tokens we can ask for and retry.
            m = re.search(r'maximum context length is \d+ tokens, however you requested \d+ tokens', error_message)
            if m:
                error_message = _(
                    "Unfortunately, this function is too big to be analyzed with the model's current API limits."
                )
            else:
                error_message = _(
                    "General exception encountered while running the query: {error}"
                ).format(error=error_message)
            print(error_message)
            if stream:
                _notify_stream_error(cb, error_message)
            else:
                _notify_non_stream_error(cb, error_message)
        except openai.OpenAIError as e:
            error_message = _("{model} could not complete the request: {error}").format(
                model=self.model, error=str(e)
            )
            print(error_message)
            if stream:
                _notify_stream_error(cb, error_message)
            else:
                _notify_non_stream_error(cb, error_message)
        except Exception as e:
            error_message = _("General exception encountered while running the query: {error}").format(error=str(e))
            print(error_message)
            if stream:
                _notify_stream_error(cb, error_message)
            else:
                _notify_non_stream_error(cb, error_message)

    # -----------------------------------------------------------------------------

    def query_model_async(self, query, cb, stream=False, additional_model_options=None):
        """
        Function which sends a query to {model} and calls a callback when the response is available.
        :param query: The request to send to {model}
        :param cb: Tu function to which the response will be passed to.
        :param additional_model_options: Additional parameters used when creating the model object. Typically, for
        OpenAI, response_format={"type": "json_object"}.
        """
        t = threading.Thread(target=self.query_model, args=[query, cb, stream, additional_model_options])
        t.start()

gepetto.models.model_manager.register_model(GPT)

```

`gepetto/models/openai_compatible.py`:

```py
import openai
import json
import httpx as _httpx

import gepetto.config
import gepetto.models.model_manager
from gepetto.models.openai import GPT

_ = gepetto.config._

DEFAULT_MODELS = [
    "default"
]


class OpenAICompatible(GPT):

    @staticmethod
    def get_menu_name() -> str:
        # name from config file
        name = gepetto.config.get_config("OpenAICompatible", "NAME")
        if name:
            return name
        else:
            return "OpenAICompatible"

    @staticmethod
    def supported_models():
        # Check if custom models are defined in config
        # If not, use the default models
        config_models = gepetto.config.get_config("OpenAICompatible", "MODELS")
        if config_models:
            try:
                return json.loads(config_models)
            except json.JSONDecodeError:
                # If it's not valid JSON, treat it as comma-separated list
                return [model.strip() for model in config_models.split(",")]
        return DEFAULT_MODELS

    @staticmethod
    def is_configured_properly() -> bool:

        # The plugin is configured properly if the API key is provided, otherwise it should not be shown.
        return bool(
            gepetto.config.get_config("OpenAICompatible", "API_KEY"))

    def __init__(self, model):
        try:
            super().__init__(model)
        except ValueError:
            # May throw if the OpenAI API key isn't given, but it's optional for
            # OpenAI-compatible providers.
            pass

        self.model = model
        api_key = gepetto.config.get_config(
            "OpenAICompatible",
            "API_KEY",
            "OPENAI_COMPATIBLE_API_KEY",
        )
        if not api_key:
            raise ValueError(
                _("Please edit the configuration file to insert your {api_provider} API key!"
                  ).format(api_provider=OpenAICompatible.get_menu_name()))

        proxy = gepetto.config.get_config("Gepetto", "PROXY")
        base_url = gepetto.config.get_config(
            "OpenAICompatible",
            "BASE_URL",
            "OPENAI_COMPATIBLE_BASE_URL",
        )

        self.client = openai.OpenAI(
            api_key=api_key,
            base_url=base_url,
            http_client=_httpx.Client(proxy=proxy) if proxy else None)


gepetto.models.model_manager.register_model(OpenAICompatible)

```

`gepetto/models/openrouter.py`:

```py
import openai
import httpx as _httpx
import json
import os

import gepetto.config
import gepetto.models.model_manager
from gepetto.models.openai import GPT

_ = gepetto.config._

# Default models to expose through OpenRouter
# You can override these in config.ini
DEFAULT_OPENROUTER_MODELS = [
    "anthropic/claude-3-5-sonnet",
    "anthropic/claude-3.7-sonnet",
    "google/gemini-2.0-flash-thinking-exp:free",
    "deepseek/deepseek-r1",
]

class OpenRouter(GPT):
    @staticmethod
    def get_menu_name() -> str:
        return "OpenRouter"

    @staticmethod
    def supported_models():
        # Check if custom models are defined in config
        config_models = gepetto.config.get_config("OpenRouter", "MODELS")
        if config_models:
            try:
                return json.loads(config_models)
            except json.JSONDecodeError:
                # If it's not valid JSON, treat it as comma-separated list
                return [model.strip() for model in config_models.split(",")]
        return DEFAULT_OPENROUTER_MODELS

    @staticmethod
    def is_configured_properly() -> bool:
        # The plugin is configured properly if the API key is provided
        return bool(gepetto.config.get_config("OpenRouter", "API_KEY", "OPENROUTER_API_KEY"))

    def __init__(self, model):
        try:
            super().__init__(model)
        except ValueError:
            pass  # May throw if the OpenAI API key isn't given, but we don't need it

        self.model = model
        api_key = gepetto.config.get_config("OpenRouter", "API_KEY", "OPENROUTER_API_KEY")
        if not api_key:
            raise ValueError(_("Please edit the configuration file to insert your {api_provider} API key!")
                             .format(api_provider="OpenRouter"))
        
        proxy = gepetto.config.get_config("Gepetto", "PROXY")
        base_url = gepetto.config.get_config("OpenRouter", "BASE_URL", "OPENROUTER_BASE_URL", "https://openrouter.ai/api/v1")

        self.client = openai.OpenAI(
            api_key=api_key,
            base_url=base_url,
            http_client=_httpx.Client(
                proxy=proxy,
            ) if proxy else None
        )

gepetto.models.model_manager.register_model(OpenRouter)

```

`gepetto/models/siliconflow.py`:

```py
import openai
import json
import httpx as _httpx

import gepetto.config
import gepetto.models.model_manager
from gepetto.models.openai import GPT

_ = gepetto.config._

DEFAULT_SILICONFLOW_MODELS = [
    "deepseek-ai/DeepSeek-V3",
    "deepseek-ai/DeepSeek-R1",
    "Pro/deepseek-ai/DeepSeek-V3",
    "Pro/deepseek-ai/DeepSeek-R1",
    "Qwen/Qwen2.5-72B-Instruct-128K",
    "Qwen/Qwen2.5-Coder-32B-Instruct",
    "Qwen/Qwen2.5-Coder-7B-Instruct",
]


class SiliconFlow(GPT):

    @staticmethod
    def get_menu_name() -> str:
        return "SiliconFlow"

    @staticmethod
    def supported_models():
        # Check if custom models are defined in config
        # If not, use the default models
        config_models = gepetto.config.get_config("SiliconFlow", "MODELS")
        if config_models:
            try:
                return json.loads(config_models)
            except json.JSONDecodeError:
                # If it's not valid JSON, treat it as comma-separated list
                return [model.strip() for model in config_models.split(",")]
        return DEFAULT_SILICONFLOW_MODELS

    @staticmethod
    def is_configured_properly() -> bool:

        # The plugin is configured properly if the API key is provided, otherwise it should not be shown.
        return bool(
            gepetto.config.get_config("SiliconFlow", "API_KEY",
                                      "SILICONFLOW_API_KEY"))

    def __init__(self, model):
        try:
            super().__init__(model)
        except ValueError:
            pass  # May throw if the OpenAI API key isn't given, but we don't need any to use DeepSeek.

        self.model = model
        api_key = gepetto.config.get_config("SiliconFlow", "API_KEY",
                                            "SILICONFLOW_API_KEY")
        if not api_key:
            raise ValueError(
                _("Please edit the configuration file to insert your {api_provider} API key!"
                  ).format(api_provider="SiliconFlow"))

        proxy = gepetto.config.get_config("Gepetto", "PROXY")
        base_url = gepetto.config.get_config("SiliconFlow", "BASE_URL",
                                             "SILICONFLOW_BASE_URL",
                                             "https://api.siliconflow.cn/v1")

        self.client = openai.OpenAI(
            api_key=api_key,
            base_url=base_url,
            http_client=_httpx.Client(proxy=proxy) if proxy else None)

gepetto.models.model_manager.register_model(SiliconFlow)

```

`gepetto/models/together.py`:

```py
import together

import gepetto.config
import gepetto.models.model_manager
from gepetto.models.openai import GPT

_ = gepetto.config._

MISTRAL_MODEL_NAME = "mistralai/Mixtral-8x22B-Instruct-v0.1"

class Together(GPT):
    @staticmethod
    def get_menu_name() -> str:
        return "Together"

    @staticmethod
    def supported_models():
        return [MISTRAL_MODEL_NAME]

    @staticmethod
    def is_configured_properly() -> bool:
        # The plugin is configured properly if the API key is provided, otherwise it should not be shown.
        return bool(gepetto.config.get_config("Together", "API_KEY", "TOGETHER_API_KEY"))

    def __init__(self, model):
        try:
            super().__init__(model)
        except ValueError:
            pass  # May throw if the OpenAI API key isn't given, but we don't need any to use Together.

        self.model = model
        api_key = gepetto.config.get_config("Together", "API_KEY", "TOGETHER_API_KEY")
        if not api_key:
            raise ValueError(_("Please edit the configuration file to insert your {api_provider} API key!")
                             .format(api_provider="Together"))

        base_url = gepetto.config.get_config("Together", "BASE_URL", "TOGETHER_BASE_URL")

        self.client = together.Together(
            api_key=api_key,
            base_url=base_url)

gepetto.models.model_manager.register_model(Together)

```

`ida-plugin.json`:

```json
{
  "IDAMetadataDescriptorVersion": 1,
  "plugin": {
    "name": "gepetto",
    "entryPoint": "gepetto.py",
    "version": "1.5.0",
    "idaVersions": ">=7.6",
    "description": "Gepetto is a Python plugin which uses various large language models to provide meaning to functions decompiled by IDA Pro. It can leverage them to explain what a function does, and to automatically rename its variables.",
    "license": "GPL-3.0",
    "categories": [
      "integration-with-third-parties-interoperability",
      "decompilation",
      "api-scripting-and-automation",
      "collaboration-and-productivity"
    ],
    "pythonDependencies": [
      "openai>=1.101.0",
      "groq>=0.8.0",
      "together>=1.2.0",
      "ollama>=0.4.5",
      "azure-identity>=1.21.0",
      "google-genai>=1.36.0"
    ],
    "urls": {
      "repository": "https://github.com/JusticeRage/Gepetto"
    },
    "authors": [{
      "name": "Ivan Kwiatkowski",
      "email": "justicerage@manalyzer.org"
    }],
    "keywords": [
      "llm",
      "ai",
      "gpt",
      "openai",
      "gemini",
      "ollama",
      "code-explanation",
      "variable-renaming",
      "auto-comment",
      "decompilation-analysis",
      "reverse-engineering-assistant",
      "natural-language",
      "azure-openai",
      "groq",
      "together",
      "deepseek",
      "agent"
    ]
  }
}

```

`pyproject.toml`:

```toml
[project]
name = "gepetto"
version = "1.5.0"
requires-python = ">=3.10"
dependencies = [
    "azure-identity>=1.21.0",
    "google-genai>=1.36.0",
    "groq>=0.8.0",
    "ollama>=0.4.5",
    "openai>=1.101.0",
    "pytest>=8.4.2",
    "together>=1.2.0",
]

[dependency-groups]
dev = [
    "idapro>=0.0.6",
]

```

`requirements.txt`:

```txt
openai >= 1.101.0
groq >= 0.8.0
together >= 1.2.0
ollama >= 0.4.5
azure-identity >= 1.21.0
google-genai >= 1.36.0
pytest >= 9.0.1
pycryptodome >= 3.23.0
```

`tests/conftest.py`:

```py
import pytest
import warnings

# Silence IDA SWIG types missing __module__ on Py 3.13 (before IDA imports)
warnings.filterwarnings(
    "ignore",
    message=r"builtin type (SwigPyPacked|SwigPyObject|swigvarlink) has no __module__ attribute",
    category=DeprecationWarning,
    module=r"importlib\._bootstrap",
)

import idapro
import ida_auto

@pytest.fixture(scope="session")
def create_idb():
    idapro.open_database("tests/testfiles/manalyze.exe", True)
    ida_auto.auto_wait()
    yield
    idapro.close_database()

```

`tests/test_config_parser.py`:

```py
import configparser
import importlib
import textwrap

import pytest


@pytest.fixture()
def config_env(tmp_path, monkeypatch):
    """Prepare an isolated configuration environment for each test."""

    config_dir = tmp_path / "config_env"
    config_dir.mkdir()

    locales_dir = config_dir / "locales" / "en_US" / "LC_MESSAGES"
    locales_dir.mkdir(parents=True)

    def write_config(content: str):
        (config_dir / "config.ini").write_text(
            textwrap.dedent(content).strip() + "\n",
            encoding="utf-8",
            )

    write_config(
        """
        [Gepetto]
        MODEL = gpt-4
        LANGUAGE = en_US
        AUTO_SHOW_STATUS_PANEL = true

        [OpenAI]
        API_KEY =
        """
    )

    import gepetto.config as config

    config = importlib.reload(config)

    # Force config to use the temporary directory for all filesystem operations
    monkeypatch.setattr(config.os.path, "dirname", lambda _: str(config_dir))

    return config, write_config, config_dir


@pytest.fixture()
def loaded_config(config_env, monkeypatch):
    """Return a loaded config module with standard model stubs applied."""

    config, write_config, config_dir = config_env

    sentinel_model = object()

    monkeypatch.setattr(config, "load_available_models", lambda: None)
    monkeypatch.setattr(config, "instantiate_model", lambda _: sentinel_model)
    monkeypatch.setattr(config, "get_fallback_model", lambda: None)

    config.load_config()

    return config, write_config, config_dir


def test_load_config_successfully_initializes_environment(config_env, monkeypatch):
    config, _, _ = config_env

    sentinel_model = object()
    load_calls = []

    monkeypatch.setattr(config, "load_available_models", lambda: load_calls.append("called"))
    monkeypatch.setattr(config, "instantiate_model", lambda model_name: sentinel_model)
    monkeypatch.setattr(config, "get_fallback_model", lambda: None)

    config.load_config()

    assert config.model is sentinel_model
    assert config.parsed_ini.get("Gepetto", "MODEL") == "gpt-4"
    assert config.language == "en_US"
    assert config.available_locales == {"en_US"}
    assert config.parsed_ini.get("Gepetto", "AUTO_SHOW_STATUS_PANEL") == "true"
    assert load_calls == ["called"]


def test_load_config_uses_fallback_model_when_requested_model_fails(config_env, monkeypatch, capsys):
    config, _, _ = config_env

    fallback_model = object()

    monkeypatch.setattr(config, "load_available_models", lambda: None)

    def _raise(_: str):
        raise RuntimeError("boom")

    monkeypatch.setattr(config, "instantiate_model", _raise)
    monkeypatch.setattr(config, "get_fallback_model", lambda: fallback_model)

    config.load_config()

    captured = capsys.readouterr()
    assert "Attempting to load the first available model" in captured.out
    assert "Defaulted to" in captured.out
    assert config.model is fallback_model


def test_load_config_gracefully_handles_missing_models(config_env, monkeypatch, capsys):
    config, _, _ = config_env

    monkeypatch.setattr(config, "load_available_models", lambda: None)

    def _raise(_: str):
        raise RuntimeError("boom")

    monkeypatch.setattr(config, "instantiate_model", _raise)

    def _raise_fallback():
        raise RuntimeError("no models")

    monkeypatch.setattr(config, "get_fallback_model", _raise_fallback)

    config.load_config()

    captured = capsys.readouterr()
    assert "No model available" in captured.out
    assert config.model is None


def test_get_config_prefers_configuration_value(loaded_config):
    config, _, _ = loaded_config

    value = config.get_config("Gepetto", "MODEL", environment_variable="MODEL_ENV", default="default")
    assert value == "gpt-4"


def test_get_config_uses_environment_variable_when_config_empty(config_env, monkeypatch):
    config, write_config, _ = config_env

    write_config(
        """
        [Gepetto]
        MODEL = gpt-4
        LANGUAGE = en_US
        AUTO_SHOW_STATUS_PANEL = true

        [OpenAI]
        API_KEY =
        """
    )

    monkeypatch.setattr(config, "load_available_models", lambda: None)
    monkeypatch.setattr(config, "instantiate_model", lambda _: object())
    monkeypatch.setattr(config, "get_fallback_model", lambda: None)

    monkeypatch.setenv("OPENAI_API_KEY", "secret")

    config.load_config()

    value = config.get_config("OpenAI", "API_KEY", environment_variable="OPENAI_API_KEY", default="default")
    assert value == "secret"


def test_get_config_returns_default_when_missing(loaded_config):
    config, _, _ = loaded_config

    value = config.get_config("Missing", "Option", default="fallback")
    assert value == "fallback"


def test_update_config_updates_file_and_cache(loaded_config):
    config, _, config_dir = loaded_config

    config.update_config("Gepetto", "MODEL", "gpt-3.5")

    assert config.parsed_ini.get("Gepetto", "MODEL") == "gpt-3.5"
    file_config = configparser.RawConfigParser()
    file_config.read(config_dir / "config.ini", encoding="utf-8")
    assert file_config.get("Gepetto", "MODEL") == "gpt-3.5"


def test_get_localization_locale_returns_valid_language(loaded_config):
    config, _, _ = loaded_config

    assert config.get_localization_locale() == "en_US"


def test_get_localization_locale_returns_default_for_invalid_language(loaded_config):
    config, _, _ = loaded_config
    config.language = "fr_FR"

    assert config.get_localization_locale() == "en_US"

```

`tests/test_gemini_convert_messages.py`:

```py
import pytest


@pytest.mark.usefixtures("_ensure_google_genai")
def test_tool_messages_batch_into_single_function_response():
    pytest.importorskip("google.genai.types")
    from gepetto.models import gemini

    query = [
        {"role": "user", "content": "hello"},
        {
            "role": "tool",
            "tool_call_id": "tool_call_0",
            "name": "decompile_function",
            "content": '{"type": "result", "data": {"ok": true}}',
        },
        {
            "role": "tool",
            "tool_call_id": "tool_call_1",
            "name": "rename_lvar",
            "content": '{"type": "result", "data": {"old_name": "Buf1"}}',
        },
        {"role": "assistant", "content": "done"},
    ]

    system_instruction, contents = gemini._convert_messages(query)

    assert system_instruction is None
    assert len(contents) == 3

    tool_content = contents[1]
    assert tool_content.role == "tool"
    assert len(tool_content.parts) == 2

    expected = [
        ("tool_call_0", "decompile_function"),
        ("tool_call_1", "rename_lvar"),
    ]
    observed = []
    for part, (call_id, name) in zip(tool_content.parts, expected):
        fr = getattr(part, "function_response", None)
        assert fr is not None
        observed.append((getattr(fr, "call_id", None), fr.name))

    assert observed == expected


@pytest.fixture
def _ensure_google_genai():
    pytest.importorskip("google.genai")

```

`tests/test_tools.py`:

```py
from typing import Any

import pytest
import uuid

import ida_bytes
import idc

from gepetto.ida.utils import function_helpers
from gepetto.ida.tools import (
    declare_c_type,
    decompile_function,
    get_bytes,
    get_disasm,
    get_struct,
    get_xrefs,
    list_functions,
    list_imports,
    rename_function,
    search,
    set_comment,
    disasm_function,
)


@pytest.fixture
def main_ea(create_idb):
    return function_helpers.resolve_ea("main")


@pytest.fixture
def comment_context(main_ea):
    original = idc.get_func_cmt(main_ea, 0) or ""
    yield main_ea, original
    idc.set_func_cmt(main_ea, original, 0)


@pytest.fixture
def imports_snapshot(create_idb):
    snapshot = list_imports.list_imports(limit=64)
    if not snapshot["items"]:
        pytest.skip("No imports discovered in the sample database.")
    return snapshot


@pytest.fixture
def function_pages(create_idb):
    return {
        include: _collect_all_functions(include)
        for include in (True, False)
    }


@pytest.fixture
def unique_struct_name():
    return f"gp_test_{uuid.uuid4().hex[:8]}"


def test_resolve_name(create_idb):
    ea = function_helpers.resolve_ea("main")
    assert ea == 0x140017F60

# ---------------------------------------------------------------------------

@pytest.mark.parametrize(
    ("target_name", "start_ea", "end_ea"),
    [
        pytest.param("main",          0x140017F60, 0x140019417, id="main"),
        pytest.param("sub_140019700", 0x140019700, 0x1400198C2, id="sub_140019700"),
    ],
)
def test_resolve_function_by_name(create_idb, target_name, start_ea, end_ea):
    f = function_helpers.resolve_func(name=target_name)
    assert f.start_ea == start_ea
    assert f.end_ea == end_ea

# ---------------------------------------------------------------------------

@pytest.mark.parametrize(
    ("target_name", "expected_ea"),
    [
        pytest.param("sub_1400D38F0", 0x1400D38F0, id="function"),
        pytest.param("??_7_Facet_base@std@@6B@", 0x1400D57D0, id="const"),
    ],
)
def test_resolve_ea(create_idb, target_name, expected_ea):
    ea = function_helpers.resolve_ea(target_name)
    assert ea == expected_ea

# ---------------------------------------------------------------------------

@pytest.mark.parametrize(
    ("target_ea", "start_ea", "end_ea", "expected_name"),
    [
        pytest.param(0x140017F70, 0x140017F60, 0x140019417, "main", id="main"),
        pytest.param(0x140019800, 0x140019700, 0x1400198C2, "sub_140019700", id="sub_140019700"),
    ],
)
def test_resolve_function_by_ea(create_idb, target_ea, start_ea, end_ea, expected_name):
    f = function_helpers.resolve_func(ea=target_ea)
    assert f.start_ea == start_ea
    assert f.end_ea == end_ea
    assert function_helpers.get_func_name(f) == expected_name

# ---------------------------------------------------------------------------

@pytest.mark.parametrize(
    ("target_name", "expected_fragment"),
    [
        pytest.param("main",            "mov     [rsp-8+arg_10], rbx",  id="main"),
        pytest.param("sub_140019700",   "mov     [rsp+arg_0], rbx",     id="sub_140019700"),
    ],
)
def test_get_disasm(create_idb, target_name, expected_fragment):
    ea = function_helpers.resolve_ea(target_name)
    result = get_disasm.get_disasm(ea)

    assert result["ea"] == ea
    assert isinstance(result["disasm"], str)
    assert result["disasm"].strip() != ""
    assert expected_fragment == result["disasm"]

# ---------------------------------------------------------------------------

@pytest.mark.parametrize(
    ("target_name", "size", "expected_prefix"),
    [
        pytest.param("main", 4, "0x48 0x89 0x5C 0x24", id="main-bytes"),
        pytest.param("xmmword_1400D69F0", 4, "0x00 0x00 0x00 0x00", id="constant-bytes")
    ],
)
def test_get_bytes(create_idb, target_name, size, expected_prefix):
    ea = function_helpers.resolve_ea(target_name)
    result = get_bytes.get_bytes(ea, size)

    assert result["ea"] == ea
    assert result["size"] == size

    byte_tokens = result["bytes"].split()
    assert len(byte_tokens) == size
    for token in byte_tokens:
        assert token.startswith("0x")

    assert result["bytes"].startswith(expected_prefix)

# ---------------------------------------------------------------------------

@pytest.mark.parametrize(
    ("target_name", "expected_fragment"),
    [
        pytest.param("sub_1400D3720", "void __fastcall sub_1400D3720()\n{\n  unknown_libname_7(&unk_140118890);\n}\n", id="full"),
        pytest.param("main", "manalyze.conf", id="main"),
    ],
)
def test_get_function_code(create_idb, target_name, expected_fragment):
    result = decompile_function.decompile_function(name=target_name)
    assert expected_fragment in str(result)

# ---------------------------------------------------------------------------

def test_list_imports_pagination_and_filter(imports_snapshot):
    total = imports_snapshot["total"]
    items = imports_snapshot["items"]
    assert total >= len(items)

    next_offset = imports_snapshot.get("next_offset")
    if next_offset is not None:
        follow_up = list_imports.list_imports(limit=len(items), offset=next_offset)
        assert follow_up["total"] == total
    else:
        assert len(items) == total

    sample = items[0]
    assert isinstance(sample["ea"], int)
    assert isinstance(sample["name"], str)
    assert isinstance(sample["module"], str)

# ---------------------------------------------------------------------------

@pytest.mark.parametrize("normalizer", [str.lower, str.upper], ids=["lower", "upper"])
def test_list_imports_module_filter(imports_snapshot, normalizer):
    module = next((item["module"] for item in imports_snapshot["items"] if item["module"]), None)
    if module is None:
        pytest.skip("Sample database does not expose module names.")

    filtered = list_imports.list_imports(limit=imports_snapshot["total"], module_filter=normalizer(module))
    assert filtered["total"] == len(filtered["items"])
    assert all(module.lower() in entry["module"].lower() for entry in filtered["items"])

# ---------------------------------------------------------------------------

def _collect_all_functions(include_thunks: bool) -> tuple[int, list[dict[str, object]]]:
    collected: list[dict[str, object]] = []
    seen_offsets: set[int] = set()
    offset = 0
    total = None

    while True:
        page = list_functions.list_functions(limit=128, offset=offset, include_thunks=include_thunks)
        if total is None:
            total = page["total"]
        else:
            assert total == page["total"]

        collected.extend(page["items"])
        next_offset = page["next_offset"]
        if next_offset is None:
            break
        assert next_offset not in seen_offsets
        seen_offsets.add(next_offset)
        offset = next_offset

    return total or 0, collected


@pytest.mark.parametrize("include_thunks", [True, False], ids=["with-thunks", "without-thunks"])
def test_list_functions_pagination(function_pages, include_thunks):
    total, funcs = function_pages[include_thunks]
    assert total == len(funcs)
    assert all(isinstance(item["ea"], int) and isinstance(item["name"], str) for item in funcs)


def test_list_functions_thunk_filtering(function_pages):
    total_with, funcs_with = function_pages[True]
    total_without, funcs_without = function_pages[False]

    assert total_without <= total_with
    assert len(funcs_with) == total_with

    names_with = {item["name"] for item in funcs_with}
    assert "main" in names_with

    names_without = {item["name"] for item in funcs_without}
    assert names_without.issubset(names_with)

# ---------------------------------------------------------------------------

def test_set_comment_round_trip(comment_context):
    ea, _ = comment_context
    new_comment = "gepetto test comment\nsecond line"

    result = set_comment.set_comment(ea=ea, comment=new_comment)
    assert result["ok"] is True
    assert result["ea"] == ea
    round_trip = idc.get_func_cmt(ea, 0) or ""
    assert round_trip == new_comment.rstrip("\r\n")

# ---------------------------------------------------------------------------

def test_declare_c_type_and_get_struct(create_idb, unique_struct_name):
    struct_name = unique_struct_name
    decl = f"struct {struct_name} {{ int a; int b; }};"

    first = declare_c_type.declare_c_type(decl)
    assert first["success"] is True
    assert first["type_name"] == struct_name

    second = declare_c_type.declare_c_type(decl)
    assert second["success"] is True

    struct_info = get_struct.get_struct(struct_name)
    assert struct_info["name"].endswith(struct_name)
    assert struct_info["size"] >= 8

    fields = {field["name"]: field for field in struct_info["fields"]}
    assert {"a", "b"} <= fields.keys()
    assert fields["a"]["offset"] == 0
    assert fields["b"]["offset"] >= 4

# ---------------------------------------------------------------------------

@pytest.fixture
def function_rename_guard():
    """Track function renames so they can be reverted after the test."""

    to_restore: list[tuple[int, str]] = []

    def _rename(target_name: str, new_name: str):
        func = function_helpers.resolve_func(name=target_name)
        original_name = function_helpers.get_func_name(func)
        ea = int(func.start_ea)
        to_restore.append((ea, original_name))
        result = rename_function.rename_function(ea=ea, new_name=new_name)
        return result, original_name

    yield _rename

    while to_restore:
        ea, original_name = to_restore.pop()
        rename_function.rename_function(ea=ea, new_name=original_name)


@pytest.mark.parametrize(
    ("target_name", "new_name", "expected_old_name"),
    [
        pytest.param("main", "main__gepetto_test", "main", id="main"),
    ],
)
def test_rename_function_round_trip(create_idb, function_rename_guard, target_name, new_name, expected_old_name):
    result, original_name = function_rename_guard(target_name, new_name)
    assert result["new_name"] == new_name
    assert result["ea"] == function_helpers.resolve_ea(new_name)
    assert result["old_name"] == original_name
    assert original_name == expected_old_name

# ---------------------------------------------------------------------------

@pytest.mark.parametrize(
    ("scope", "subject", "direction", "expected", "kwargs"),
    [
        pytest.param(
            "function",
            "0x140017F60",
            "to",
            [{
                "from_ea": 5369367579,
                "to_ea": 5368807264,
                "direction": "to",
                "kind": "code",
                "type": 17,
                "from_func": "?__scrt_common_main_seh@@YAHXZ",
                "to_func": "main",
            }],
            {"only_calls": True},
            id="function-to",
        ),
        pytest.param(
            "name",
            "unk_1400D6A20",
            "both",
            [
                {
                    "from_ea": 5368821643,
                    "to_ea": 5369588256,
                    "direction": "to",
                    "kind": "data",
                    "type": 1,
                    "from_func": "sub_14001B300",
                    "to_func": "unk_1400D6A20"
                }
            ],
            {},
            id="constant-both",
        )
    ]
)
def test_get_xrefs_unified(create_idb, scope: str, subject: str, direction: str,
                           expected: list[dict[str, Any]], kwargs: dict[str, Any]):
    # pass the arbitrary kwargs straight through
    result = get_xrefs.get_xrefs_unified(scope=scope, subject=subject, direction=direction, **kwargs)

    assert result["scope"] == scope
    assert result["direction"] == direction
    assert result["xrefs"] == expected

# ---------------------------------------------------------------------------

def test_get_xrefs_consistency(create_idb):
    name_res = get_xrefs.get_xrefs_unified(scope="name", subject="main", direction="to", only_calls=True)
    func_res = get_xrefs.get_xrefs_unified(scope="function", subject="0x140017F70", direction="to", only_calls=True)
    ea_res = get_xrefs.get_xrefs_unified(scope="ea", subject="0x140017F60", direction="to", only_calls=True)
    del name_res["scope"]
    del func_res["scope"]
    del ea_res["scope"]

    assert name_res == func_res
    assert name_res == ea_res

# ---------------------------------------------------------------------------

@pytest.mark.parametrize(
    ("kwargs", "expected_eas"),
    [
        pytest.param({"text": "manalyze"}, [0x1400D6408, 0x1400D65E8], id="text-search"),
        pytest.param({"hex": "6F 75 74 70 75 74 2C 6F"}, [0x1400D61A8], id="hex-search"),
    ],
)
def test_search_results(create_idb, kwargs, expected_eas):
    result = search.search(**kwargs)
    assert sorted(result["eas"]) == sorted(expected_eas)

# ---------------------------------------------------------------------------

@pytest.mark.parametrize(
    ("kwargs", "expected"),
    [
        pytest.param({"limit": 3}, {
            "total": 1638,
            "next_offset": 3,
            "items": [
                {
                    "ea": 5369583136,
                    "len": 17,
                    "segment": ".rdata",
                    "encoding": "ascii",
                    "text": "Unknown exception",
                    "text_truncated": False,
                    "sha1": "de24f2d0a243ca2d28955e986528d14a53ed242c"
                },
                {
                    "ea": 5369583208,
                    "len": 20,
                    "segment": ".rdata",
                    "encoding": "ascii",
                    "text": "bad array new length",
                    "text_truncated": False,
                    "sha1": "c3c079b6d2d19707022ad64ce1f44e7e50dc71f6"
                },
                {
                    "ea": 5369583232,
                    "len": 15,
                    "segment": ".rdata",
                    "encoding": "ascii",
                    "text": "string too long",
                    "text_truncated": False,
                    "sha1": "68070eb3d83e80582f9979decc53730976549a3d"
                }
            ]
        }, id="default"),
        pytest.param(
            {
                "limit": 2,
                "offset": 1,
                "min_len": 30,
                "include_xrefs": True,
                "include_text": False
            },
            {
                "total": 519,
                "next_offset": 3,
                "items": [
                    {
                        "ea": 5369584888,
                        "len": 30,
                        "segment": ".rdata",
                        "encoding": "ascii",
                        "xrefs_to": [
                            5368769580
                        ]
                    },
                    {
                        "ea": 5369585048,
                        "len": 39,
                        "segment": ".rdata",
                        "encoding": "ascii",
                        "xrefs_to": [
                            5368794794
                        ]
                    }
                ]
            }, id="pagination"),
    ]
)
def test_list_strings_pagination_and_filters(create_idb, kwargs, expected):
    result = search.list_strings(**kwargs)
    assert result == expected

def test_get_disasm_function(create_idb):
    res = disasm_function.disasm_function(name="sub_140060FC0")
    res = res["disasm"]
    assert res == """0x140060fc0: sub     rsp, 28h
0x140060fc4: call    sub_140062890
0x140060fc9: mov     rcx, rax
0x140060fcc: add     rsp, 28h
0x140060fd0: jmp     sub_1400608C0"""

```