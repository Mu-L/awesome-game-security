Project Path: arc_nfrechette_rtm_wa__8xda

Source Tree:

```txt
arc_nfrechette_rtm_wa__8xda
â”œâ”€â”€ CHANGELOG.md
â”œâ”€â”€ CMakeLists.txt
â”œâ”€â”€ CODE_OF_CONDUCT.md
â”œâ”€â”€ CONTRIBUTING.md
â”œâ”€â”€ LICENSE
â”œâ”€â”€ README.md
â”œâ”€â”€ cmake
â”‚   â”œâ”€â”€ CMakeCompiler.cmake
â”‚   â”œâ”€â”€ CMakePlatforms.cmake
â”‚   â”œâ”€â”€ CMakeUtils.cmake
â”‚   â”œâ”€â”€ Toolchain-Android.cmake
â”‚   â””â”€â”€ Toolchain-iOS.cmake
â”œâ”€â”€ docs
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ api_conventions.md
â”‚   â”œâ”€â”€ getting_started.md
â”‚   â”œâ”€â”€ handling_asserts.md
â”‚   â”œâ”€â”€ simd_support.md
â”‚   â””â”€â”€ types_supported.md
â”œâ”€â”€ external
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ benchmark
â”‚   â””â”€â”€ catch2
â”œâ”€â”€ includes
â”‚   â””â”€â”€ rtm
â”‚       â”œâ”€â”€ camera_utilsd.h
â”‚       â”œâ”€â”€ camera_utilsf.h
â”‚       â”œâ”€â”€ config.h
â”‚       â”œâ”€â”€ constants.h
â”‚       â”œâ”€â”€ experimental
â”‚       â”‚   â”œâ”€â”€ impl
â”‚       â”‚   â”‚   â””â”€â”€ vqm_common.h
â”‚       â”‚   â”œâ”€â”€ types.h
â”‚       â”‚   â”œâ”€â”€ vqmd.h
â”‚       â”‚   â””â”€â”€ vqmf.h
â”‚       â”œâ”€â”€ fwd.h
â”‚       â”œâ”€â”€ impl
â”‚       â”‚   â”œâ”€â”€ bit_cast.impl.h
â”‚       â”‚   â”œâ”€â”€ cmath.impl.h
â”‚       â”‚   â”œâ”€â”€ compiler_utils.h
â”‚       â”‚   â”œâ”€â”€ detect_arch.h
â”‚       â”‚   â”œâ”€â”€ detect_compiler.h
â”‚       â”‚   â”œâ”€â”€ detect_cpp_version.h
â”‚       â”‚   â”œâ”€â”€ detect_features.h
â”‚       â”‚   â”œâ”€â”€ error.h
â”‚       â”‚   â”œâ”€â”€ macros.mask4.impl.h
â”‚       â”‚   â”œâ”€â”€ macros.matrix.impl.h
â”‚       â”‚   â”œâ”€â”€ macros.vector4.impl.h
â”‚       â”‚   â”œâ”€â”€ mask_common.h
â”‚       â”‚   â”œâ”€â”€ matrix_affine_common.h
â”‚       â”‚   â”œâ”€â”€ matrix_cast.h
â”‚       â”‚   â”œâ”€â”€ matrix_common.h
â”‚       â”‚   â”œâ”€â”€ memory_utils.h
â”‚       â”‚   â”œâ”€â”€ quat_common.h
â”‚       â”‚   â”œâ”€â”€ qv_common.h
â”‚       â”‚   â”œâ”€â”€ qvs_common.h
â”‚       â”‚   â”œâ”€â”€ qvv_common.h
â”‚       â”‚   â”œâ”€â”€ scalar_common.h
â”‚       â”‚   â”œâ”€â”€ type_args.h
â”‚       â”‚   â”œâ”€â”€ type_args.neon.impl.h
â”‚       â”‚   â”œâ”€â”€ type_args.neon64.impl.h
â”‚       â”‚   â”œâ”€â”€ type_args.other.impl.h
â”‚       â”‚   â”œâ”€â”€ type_args.vectorcall.impl.h
â”‚       â”‚   â”œâ”€â”€ type_args.x64_clang.impl.h
â”‚       â”‚   â”œâ”€â”€ type_args.x64_gcc.impl.h
â”‚       â”‚   â””â”€â”€ vector_common.h
â”‚       â”œâ”€â”€ macros.h
â”‚       â”œâ”€â”€ mask4d.h
â”‚       â”œâ”€â”€ mask4f.h
â”‚       â”œâ”€â”€ mask4i.h
â”‚       â”œâ”€â”€ mask4q.h
â”‚       â”œâ”€â”€ math.h
â”‚       â”œâ”€â”€ matrix3x3d.h
â”‚       â”œâ”€â”€ matrix3x3f.h
â”‚       â”œâ”€â”€ matrix3x4d.h
â”‚       â”œâ”€â”€ matrix3x4f.h
â”‚       â”œâ”€â”€ matrix4x4d.h
â”‚       â”œâ”€â”€ matrix4x4f.h
â”‚       â”œâ”€â”€ packing
â”‚       â”‚   â”œâ”€â”€ quatd.h
â”‚       â”‚   â””â”€â”€ quatf.h
â”‚       â”œâ”€â”€ quatd.h
â”‚       â”œâ”€â”€ quatf.h
â”‚       â”œâ”€â”€ qvd.h
â”‚       â”œâ”€â”€ qvf.h
â”‚       â”œâ”€â”€ qvsd.h
â”‚       â”œâ”€â”€ qvsf.h
â”‚       â”œâ”€â”€ qvvd.h
â”‚       â”œâ”€â”€ qvvf.h
â”‚       â”œâ”€â”€ scalard.h
â”‚       â”œâ”€â”€ scalarf.h
â”‚       â”œâ”€â”€ type_traits.h
â”‚       â”œâ”€â”€ types.h
â”‚       â”œâ”€â”€ vector4d.h
â”‚       â”œâ”€â”€ vector4f.h
â”‚       â””â”€â”€ version.h
â”œâ”€â”€ make.py
â”œâ”€â”€ sonar-project.properties
â”œâ”€â”€ tests
â”‚   â”œâ”€â”€ CMakeLists.txt
â”‚   â”œâ”€â”€ main_android
â”‚   â”‚   â”œâ”€â”€ CMakeLists.txt
â”‚   â”‚   â””â”€â”€ app
â”‚   â”‚       â””â”€â”€ src
â”‚   â”‚           â””â”€â”€ main
â”‚   â”‚               â”œâ”€â”€ cpp
â”‚   â”‚               â”‚   â”œâ”€â”€ CMakeLists.txt
â”‚   â”‚               â”‚   â””â”€â”€ main.cpp
â”‚   â”‚               â””â”€â”€ java
â”‚   â”‚                   â””â”€â”€ com
â”‚   â”‚                       â””â”€â”€ rtm
â”‚   â”‚                           â””â”€â”€ unit_tests
â”‚   â”‚                               â””â”€â”€ MainActivity.java
â”‚   â”œâ”€â”€ main_emscripten
â”‚   â”‚   â”œâ”€â”€ CMakeLists.txt
â”‚   â”‚   â””â”€â”€ main.cpp
â”‚   â”œâ”€â”€ main_generic
â”‚   â”‚   â”œâ”€â”€ CMakeLists.txt
â”‚   â”‚   â””â”€â”€ main.cpp
â”‚   â”œâ”€â”€ main_ios
â”‚   â”‚   â”œâ”€â”€ CMakeLists.txt
â”‚   â”‚   â””â”€â”€ main.cpp
â”‚   â”œâ”€â”€ sources
â”‚   â”‚   â”œâ”€â”€ catch2.impl.h
â”‚   â”‚   â”œâ”€â”€ test_constants.cpp
â”‚   â”‚   â”œâ”€â”€ test_header_fwd.cpp
â”‚   â”‚   â”œâ”€â”€ test_macros_matrix.cpp
â”‚   â”‚   â”œâ”€â”€ test_mask4.cpp
â”‚   â”‚   â”œâ”€â”€ test_matrix3x3_impl.h
â”‚   â”‚   â”œâ”€â”€ test_matrix3x3d.cpp
â”‚   â”‚   â”œâ”€â”€ test_matrix3x3f.cpp
â”‚   â”‚   â”œâ”€â”€ test_matrix3x4.cpp
â”‚   â”‚   â”œâ”€â”€ test_matrix4x4.cpp
â”‚   â”‚   â”œâ”€â”€ test_memory_utils.cpp
â”‚   â”‚   â”œâ”€â”€ test_packing_quat.cpp
â”‚   â”‚   â”œâ”€â”€ test_quat.cpp
â”‚   â”‚   â”œâ”€â”€ test_qv.cpp
â”‚   â”‚   â”œâ”€â”€ test_qvs.cpp
â”‚   â”‚   â”œâ”€â”€ test_qvv.cpp
â”‚   â”‚   â”œâ”€â”€ test_scalar.cpp
â”‚   â”‚   â”œâ”€â”€ test_vector4_cast.cpp
â”‚   â”‚   â”œâ”€â”€ test_vector4_impl.h
â”‚   â”‚   â”œâ”€â”€ test_vector4d.cpp
â”‚   â”‚   â”œâ”€â”€ test_vector4f.cpp
â”‚   â”‚   â”œâ”€â”€ test_vqm.cpp
â”‚   â”‚   â””â”€â”€ vector_mix
â”‚   â”‚       â”œâ”€â”€ test_vector4_mix_impl.h
â”‚   â”‚       â”œâ”€â”€ test_vector4d_mix_aa.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4d_mix_ab.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4d_mix_ac.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4d_mix_ad.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4d_mix_aw.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4d_mix_ax.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4d_mix_ay.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4d_mix_az.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4d_mix_ba.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4d_mix_bb.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4d_mix_bc.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4d_mix_bd.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4d_mix_bw.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4d_mix_bx.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4d_mix_by.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4d_mix_bz.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4d_mix_ca.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4d_mix_cb.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4d_mix_cc.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4d_mix_cd.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4d_mix_cw.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4d_mix_cx.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4d_mix_cy.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4d_mix_cz.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4d_mix_da.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4d_mix_db.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4d_mix_dc.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4d_mix_dd.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4d_mix_dw.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4d_mix_dx.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4d_mix_dy.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4d_mix_dz.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4d_mix_wa.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4d_mix_wb.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4d_mix_wc.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4d_mix_wd.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4d_mix_ww.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4d_mix_wx.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4d_mix_wy.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4d_mix_wz.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4d_mix_xa.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4d_mix_xb.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4d_mix_xc.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4d_mix_xd.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4d_mix_xw.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4d_mix_xx.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4d_mix_xy.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4d_mix_xz.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4d_mix_ya.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4d_mix_yb.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4d_mix_yc.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4d_mix_yd.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4d_mix_yw.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4d_mix_yx.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4d_mix_yy.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4d_mix_yz.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4d_mix_za.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4d_mix_zb.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4d_mix_zc.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4d_mix_zd.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4d_mix_zw.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4d_mix_zx.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4d_mix_zy.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4d_mix_zz.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4f_mix_aa.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4f_mix_ab.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4f_mix_ac.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4f_mix_ad.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4f_mix_aw.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4f_mix_ax.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4f_mix_ay.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4f_mix_az.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4f_mix_ba.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4f_mix_bb.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4f_mix_bc.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4f_mix_bd.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4f_mix_bw.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4f_mix_bx.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4f_mix_by.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4f_mix_bz.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4f_mix_ca.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4f_mix_cb.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4f_mix_cc.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4f_mix_cd.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4f_mix_cw.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4f_mix_cx.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4f_mix_cy.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4f_mix_cz.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4f_mix_da.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4f_mix_db.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4f_mix_dc.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4f_mix_dd.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4f_mix_dw.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4f_mix_dx.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4f_mix_dy.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4f_mix_dz.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4f_mix_wa.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4f_mix_wb.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4f_mix_wc.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4f_mix_wd.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4f_mix_ww.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4f_mix_wx.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4f_mix_wy.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4f_mix_wz.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4f_mix_xa.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4f_mix_xb.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4f_mix_xc.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4f_mix_xd.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4f_mix_xw.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4f_mix_xx.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4f_mix_xy.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4f_mix_xz.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4f_mix_ya.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4f_mix_yb.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4f_mix_yc.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4f_mix_yd.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4f_mix_yw.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4f_mix_yx.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4f_mix_yy.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4f_mix_yz.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4f_mix_za.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4f_mix_zb.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4f_mix_zc.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4f_mix_zd.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4f_mix_zw.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4f_mix_zx.cpp
â”‚   â”‚       â”œâ”€â”€ test_vector4f_mix_zy.cpp
â”‚   â”‚       â””â”€â”€ test_vector4f_mix_zz.cpp
â”‚   â””â”€â”€ validate_includes
â”‚       â”œâ”€â”€ CMakeLists.txt
â”‚       â”œâ”€â”€ dummy.cpp
â”‚       â””â”€â”€ single_include.cpp.in
â””â”€â”€ tools
    â”œâ”€â”€ android_misc
    â”‚   â”œâ”€â”€ README.md
    â”‚   â”œâ”€â”€ app
    â”‚   â”‚   â”œâ”€â”€ build.gradle.in
    â”‚   â”‚   â”œâ”€â”€ proguard-rules.pro
    â”‚   â”‚   â””â”€â”€ src
    â”‚   â”‚       â””â”€â”€ main
    â”‚   â”‚           â”œâ”€â”€ AndroidManifest.xml.in
    â”‚   â”‚           â””â”€â”€ res
    â”‚   â”‚               â””â”€â”€ values
    â”‚   â”‚                   â””â”€â”€ strings.xml.in
    â”‚   â”œâ”€â”€ build.gradle
    â”‚   â”œâ”€â”€ gradle
    â”‚   â”‚   â””â”€â”€ wrapper
    â”‚   â”‚       â”œâ”€â”€ gradle-wrapper.jar
    â”‚   â”‚       â””â”€â”€ gradle-wrapper.properties
    â”‚   â”œâ”€â”€ gradle.properties
    â”‚   â”œâ”€â”€ gradlew
    â”‚   â”œâ”€â”€ gradlew.bat
    â”‚   â””â”€â”€ settings.gradle.in
    â”œâ”€â”€ appveyor_ci.bat
    â”œâ”€â”€ bench
    â”‚   â”œâ”€â”€ CMakeLists.txt
    â”‚   â”œâ”€â”€ main_android
    â”‚   â”‚   â”œâ”€â”€ CMakeLists.txt
    â”‚   â”‚   â””â”€â”€ app
    â”‚   â”‚       â””â”€â”€ src
    â”‚   â”‚           â””â”€â”€ main
    â”‚   â”‚               â”œâ”€â”€ cpp
    â”‚   â”‚               â”‚   â”œâ”€â”€ CMakeLists.txt
    â”‚   â”‚               â”‚   â””â”€â”€ main.cpp
    â”‚   â”‚               â””â”€â”€ java
    â”‚   â”‚                   â””â”€â”€ com
    â”‚   â”‚                       â””â”€â”€ rtm
    â”‚   â”‚                           â””â”€â”€ benchmark
    â”‚   â”‚                               â””â”€â”€ MainActivity.java
    â”‚   â”œâ”€â”€ main_generic
    â”‚   â”‚   â”œâ”€â”€ CMakeLists.txt
    â”‚   â”‚   â””â”€â”€ main.cpp
    â”‚   â”œâ”€â”€ main_ios
    â”‚   â”‚   â”œâ”€â”€ CMakeLists.txt
    â”‚   â”‚   â””â”€â”€ main.cpp
    â”‚   â””â”€â”€ sources
    â”‚       â”œâ”€â”€ bench_mask_all_equal.cpp
    â”‚       â”œâ”€â”€ bench_mask_any_equal3.cpp
    â”‚       â”œâ”€â”€ bench_matrix3x3d_arg_passing.cpp
    â”‚       â”œâ”€â”€ bench_matrix3x3f_arg_passing.cpp
    â”‚       â”œâ”€â”€ bench_quat_conjugate.cpp
    â”‚       â”œâ”€â”€ bench_quat_ensure_positive_w.cpp
    â”‚       â”œâ”€â”€ bench_quat_from_positive_w.cpp
    â”‚       â”œâ”€â”€ bench_quat_mul.cpp
    â”‚       â”œâ”€â”€ bench_quat_mul_vector3.cpp
    â”‚       â”œâ”€â”€ bench_qvv_mul.cpp
    â”‚       â”œâ”€â”€ bench_scalar_abs.cpp
    â”‚       â”œâ”€â”€ bench_scalar_ceil.cpp
    â”‚       â”œâ”€â”€ bench_scalar_floor.cpp
    â”‚       â”œâ”€â”€ bench_scalar_reciprocal.cpp
    â”‚       â”œâ”€â”€ bench_scalar_round_bankers.cpp
    â”‚       â”œâ”€â”€ bench_scalar_round_symmetric.cpp
    â”‚       â”œâ”€â”€ bench_scalar_sin.cpp
    â”‚       â”œâ”€â”€ bench_vector_abs.cpp
    â”‚       â”œâ”€â”€ bench_vector_acos.cpp
    â”‚       â”œâ”€â”€ bench_vector_asin.cpp
    â”‚       â”œâ”€â”€ bench_vector_atan.cpp
    â”‚       â”œâ”€â”€ bench_vector_atan2.cpp
    â”‚       â”œâ”€â”€ bench_vector_ceil.cpp
    â”‚       â”œâ”€â”€ bench_vector_cos.cpp
    â”‚       â”œâ”€â”€ bench_vector_cross3.cpp
    â”‚       â”œâ”€â”€ bench_vector_dot.cpp
    â”‚       â”œâ”€â”€ bench_vector_dot3.cpp
    â”‚       â”œâ”€â”€ bench_vector_dot3_v.cpp
    â”‚       â”œâ”€â”€ bench_vector_dot_v.cpp
    â”‚       â”œâ”€â”€ bench_vector_floor.cpp
    â”‚       â”œâ”€â”€ bench_vector_reciprocal.cpp
    â”‚       â”œâ”€â”€ bench_vector_round_bankers.cpp
    â”‚       â”œâ”€â”€ bench_vector_round_symmetric.cpp
    â”‚       â”œâ”€â”€ bench_vector_sign.cpp
    â”‚       â””â”€â”€ bench_vector_sin.cpp
    â”œâ”€â”€ gen_vector_mix_sse.py
    â”œâ”€â”€ release_scripts
    â”‚   â”œâ”€â”€ README.md
    â”‚   â””â”€â”€ test_everything.py
    â”œâ”€â”€ setup_linux_compiler.sh
    â”œâ”€â”€ setup_osx_compiler.sh
    â””â”€â”€ vs_visualizers
        â””â”€â”€ rtm.natvis

```

`CHANGELOG.md`:

```md
# Significant changes per release

## 2.3.1

*  Fix missing macro error when deprecation is disabled

## 2.3.0

*  Added missing 2D/4D variants of some vector arithmetic
*  Added support for Windows min/max macros
*  Added vector_get_component/vector_set_component
*  Added matrix_get_axis/matrix_set_axis and component versions
*  Added qv/qvs/qvv from matrix
*  Improved type traits
*  Improved code generation for float64 types
*  Added camera related matrix utilities
*  Deprecated scalar/vector coersion for some arithmetic functions in favor of `as_scalar` and `as_vector` suffixes
*  Other improvements and cleanup

## 2.2.1

*  Fix matrix 4x4 multiplication and add unit tests

## 2.2.0

*  Added support for Clang 15
*  Added support for GCC 12 and 13
*  Added support for XCode 14
*  Added support for VS2022
*  CI now uses docker where possible
*  Updated Catch2 to 2.13.7
*  Tons of static analysis fixes and polish
*  Added new QV transform type
*  Added new QVS transform type
*  Tons of new minor additions
*  Tons of optimizations, especially on ARM64
*  Many other improvements and cleanup

## 2.1.5

*  Force macro expansion in version namespace identifier

## 2.1.4

*  Add support for clang 12, 13, and 14
*  Add support for GCC 11
*  Add support for XCode 12 and 13
*  Add support for Arm64 development on OS X and Linux
*  Misc CI improvements

## 2.1.3

*  Add versioned namespace to allow multiple versions to coexist within a binary

## 2.1.2

*  Fix MSVC/clang static analysis warnings
*  Improve MSVC ARM64 intrinsic support detection

## 2.1.1

*  Avoid enabling NEON when just `__arm__` is present

## 2.1.0

*  Force inlined most functions to improve MSVC code generation
*  Migrated from Travis to GitHub Actions
*  Added support for clang 11
*  Lots of minor additions, fixes, and improvements

## 2.0.0

*  Added support for GCC10, clang8, clang9, clang10, VS 2019 clang, and emscripten
*  Added a lot of matrix math
*  Added trigonometric functions (scalar and vector)
*  Angle types have been removed
*  Lots of optimizations and improvements
*  Tons of cleanup to ensure a consistent API

## 1.1.0

*  Added support for Windows ARM64
*  Added support for VS2019, GCC9, clang7, and Xcode 11
*  Added support for Intel FMA and ARM64 FMA
*  Many optimizations, minor fixes, and cleanup

## 1.0.0

Initial release migrating and improving the code from the Animation Compression Library.


```

`CMakeLists.txt`:

```txt
cmake_minimum_required(VERSION 3.2...3.25)
project(rtm CXX)

set(CMAKE_MODULE_PATH ${CMAKE_MODULE_PATH} "${PROJECT_SOURCE_DIR}/cmake")

include(CMakeUtils)
include(CMakeCompiler)
include(CMakePlatforms)

set(USE_AVX_INSTRUCTIONS false CACHE BOOL "Use AVX instructions")
set(USE_AVX2_INSTRUCTIONS false CACHE BOOL "Use AVX2 instructions")
set(USE_SIMD_INSTRUCTIONS true CACHE BOOL "Use SIMD instructions")
set(WITH_VECTOR_MIX_TESTS false CACHE BOOL "Enable vector_mix unit tests")
set(CPU_INSTRUCTION_SET false CACHE STRING "CPU instruction set")
set(BUILD_BENCHMARK_EXE false CACHE BOOL "Enable the benchmark projects")
set(IS_CROSS_COMPILING false CACHE BOOL "Whether or not we are cross-compiling")

if(CMAKE_CONFIGURATION_TYPES)
	set(CMAKE_CONFIGURATION_TYPES Debug Release)
	set(CMAKE_CONFIGURATION_TYPES "${CMAKE_CONFIGURATION_TYPES}" CACHE STRING "Reset the configurations to what we need" FORCE)
endif()

# Grab all of our include files
file(GLOB_RECURSE RTM_INCLUDE_FILES LIST_DIRECTORIES false
	${PROJECT_SOURCE_DIR}/includes/*.h
	${PROJECT_SOURCE_DIR}/docs/*.md
	${PROJECT_SOURCE_DIR}/cmake/*.cmake
	${PROJECT_SOURCE_DIR}/tools/release_scripts/*.py
	${PROJECT_SOURCE_DIR}/tools/vs_visualizers/*.natvis
	)

create_source_groups("${RTM_INCLUDE_FILES}" ${PROJECT_SOURCE_DIR})

file(GLOB RTM_ROOT_FILES LIST_DIRECTORIES false
	${PROJECT_SOURCE_DIR}/*.md
	${PROJECT_SOURCE_DIR}/*.py)

# Create a dummy target so they show up in the IDE
add_custom_target(${PROJECT_NAME} SOURCES ${RTM_INCLUDE_FILES} ${RTM_ROOT_FILES})

# Enable CTest
enable_testing()

# Add other projects
add_subdirectory("${PROJECT_SOURCE_DIR}/tests")

if(BUILD_BENCHMARK_EXE)
	# Our benchmark executable
	add_subdirectory("${PROJECT_SOURCE_DIR}/tools/bench")
endif()

```

`CODE_OF_CONDUCT.md`:

```md
# Contributor Covenant Code of Conduct

## Our Pledge

In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, gender identity and expression, level of experience, nationality, personal appearance, race, religion, or sexual identity and orientation.

## Our Standards

Examples of behavior that contributes to creating a positive environment include:

* Using welcoming and inclusive language
* Being respectful of differing viewpoints and experiences
* Gracefully accepting constructive criticism
* Focusing on what is best for the community
* Showing empathy towards other community members

Examples of unacceptable behavior by participants include:

* The use of sexualized language or imagery and unwelcome sexual attention or advances
* Trolling, insulting/derogatory comments, and personal or political attacks
* Public or private harassment
* Publishing others' private information, such as a physical or electronic address, without explicit permission
* Other conduct which could reasonably be considered inappropriate in a professional setting

## Our Responsibilities

Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior.

Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.

## Scope

This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.

## Enforcement

Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at zeno490@gmail.com. The project team will review and investigate all complaints, and will respond in a way that it deems appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately.

Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership.

## Attribution

This Code of Conduct is adapted from the [Contributor Covenant][homepage], version 1.4, available at [http://contributor-covenant.org/version/1/4][version]

[homepage]: http://contributor-covenant.org
[version]: http://contributor-covenant.org/version/1/4/

```

`CONTRIBUTING.md`:

```md
# How to contribute

Thank you for your interest in the Realtime Math library! All contributions that follow our guidelines below and abide by our [code of conduct](CODE_OF_CONDUCT.md) are welcome.

In this document you will find relevant reading material, what contributions we are looking for, and what we are *not* looking for.

Project contact email: zeno490@gmail.com

# Getting set up

See the [getting started](./docs/getting_started.md) section for details on how to generate project solutions, build, and run the unit tests.

Every pull request should trigger continuous integration on every platform we support and most of them will also execute the unit tests automatically (except on Android and iOS).

The project roadmap for the next few milestones is tracked with [GitHub issues](https://github.com/nfrechette/rtm/issues). [Backlog issues](https://github.com/nfrechette/rtm/milestone/1) are things I would like to get done eventually but that have not been prioritized yet.

Whether you create an issue or a pull request, I will do my best to comment or reply within 48 hours.

# Contributions we are looking for

Several issues already have a [help wanted](https://github.com/nfrechette/rtm/issues?q=is%3Aopen+is%3Aissue+label%3A%22help+wanted%22) label. Those tasks should be either reasonably simple or tasks that I do not think I will get the chance to get to very soon.

Feature requests are welcome providing that they fit within the project scope. For smaller features, you are welcome to create an issue with as much relevant information as you can. If it makes sense, I will prioritize it or add it to the backlog, and if I feel it is beyond the scope of the project, I will tell you why and close the issue.

# Contributions we are *not* looking for

A lot of older compilers do not properly support **C++11** and there is no plan to support them. 

If you aren't sure, don't be afraid to reach out by email or on discord!

```

`LICENSE`:

```
MIT License

Copyright (c) 2018 Nicholas Frechette

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

```

`README.md`:

```md
[![CLA assistant](https://cla-assistant.io/readme/badge/nfrechette/rtm)](https://cla-assistant.io/nfrechette/rtm)
[![All Contributors](https://img.shields.io/github/all-contributors/nfrechette/rtm)](#contributors-)
[![Build status](https://github.com/nfrechette/rtm/actions/workflows/build_push.yml/badge.svg)](https://github.com/nfrechette/rtm/actions/workflows/build_push.yml)
[![Sonar Status](https://sonarcloud.io/api/project_badges/measure?project=nfrechette_rtm&metric=alert_status)](https://sonarcloud.io/dashboard?id=nfrechette_rtm)
[![GitHub release](https://img.shields.io/github/release/nfrechette/rtm.svg)](https://github.com/nfrechette/rtm/releases)
[![GitHub license](https://img.shields.io/badge/license-MIT-blue.svg)](https://raw.githubusercontent.com/nfrechette/rtm/master/LICENSE)
[![Conan](https://img.shields.io/conan/v/rtm)](https://conan.io/center/recipes/rtm)
[![Discord](https://img.shields.io/discord/691048241864769647?label=discord)](https://discord.gg/UERt4bS)

# Realtime Math

This library is geared towards realtime applications that require their math to be as fast as possible. Much care was taken to maximize inlining opportunities and for code generation to be optimal when a function isn't inlined by passing values in registers whenever possible. It contains 3D and 4D arithmetic commonly used in video games and realtime applications.

It offers an alternative to [GLM](https://github.com/g-truc/glm) and [DirectX Math](https://github.com/Microsoft/DirectXMath). See [here](https://nfrechette.github.io/2019/01/19/introducing_realtime_math/) for a comparison with similar libraries.

## Philosophy

Much thought was put into designing the library for it to be as flexible and powerful as possible. To this end, the following decisions were made:

*  The library consists of **100% C++11** header files and is thus easy to integrate in any project
*  The interface follows C-style conventions to ensure optimal code generation
*  Both *float32* and *float64* arithmetic are supported
*  Row vectors are used
*  See [here](./docs/api_conventions.md) for more details

## Supported platforms

Continuous integration tests a variety of platforms and configurations but it generally runs as-is anywhere where C++11 (or later) is supported. CI currently tests:

*  Windows VS2022: x86, x64, ARM64, ARM64EC
*  Linux GCC 12+: x86, x64
*  Linux Clang 15+: x86, x64
*  OS X XCode 15+: ARM64
*  Emscripten 1.39.11: WASM

Each releases is also manually tested on iOS and Android.

## Getting started

This library is **100%** headers as such you just need to include them in your own project to start using it. However, if you wish to run the unit tests or to contribute to RTM head on over to the [getting started](./docs/getting_started.md) section in order to setup your environment and make sure to check out the [contributing guidelines](CONTRIBUTING.md).

You can install `rtm` with [Conan](https://conan.io/center/recipes/rtm).

## External dependencies

You don't need anything else to get started: everything is self contained.
See [here](./external) for details.

## License, copyright, and code of conduct

This project uses the [MIT license](LICENSE).

Copyright (c) 2018 Nicholas Frechette & Realtime Math contributors

This project was started from the math code found in the [Animation Compression Library](https://github.com/nfrechette/acl) v1.1.0 and it retains the copyright of the original contributors.

Please note that this project is released with a [Contributor Code of Conduct](CODE_OF_CONDUCT.md). By participating in this project you agree to abide by its terms.

## Contributors âœ¨

Thanks goes to these wonderful people ([emoji key](https://allcontributors.org/docs/en/emoji-key)):

<!-- ALL-CONTRIBUTORS-LIST:START - Do not remove or modify this section -->
<!-- prettier-ignore-start -->
<!-- markdownlint-disable -->
<table>
  <tbody>
    <tr>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/CodyDWJones"><img src="https://avatars.githubusercontent.com/u/28773740?v=4?s=100" width="100px;" alt="CodyDWJones"/><br /><sub><b>CodyDWJones</b></sub></a><br /><a href="https://github.com/nfrechette/rtm/commits?author=CodyDWJones" title="Code">ðŸ’»</a> <a href="#platform-CodyDWJones" title="Packaging/porting to new platform">ðŸ“¦</a> <a href="#infra-CodyDWJones" title="Infrastructure (Hosting, Build-Tools, etc)">ðŸš‡</a></td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/mwerschy"><img src="https://avatars.githubusercontent.com/u/6616804?v=4?s=100" width="100px;" alt="Malte Werschy"/><br /><sub><b>Malte Werschy</b></sub></a><br /><a href="https://github.com/nfrechette/rtm/commits?author=mwerschy" title="Code">ðŸ’»</a> <a href="#platform-mwerschy" title="Packaging/porting to new platform">ðŸ“¦</a></td>
      <td align="center" valign="top" width="14.28%"><a href="https://gitlab.com/intelligide"><img src="https://avatars.githubusercontent.com/u/12529837?v=4?s=100" width="100px;" alt="Yoann Potinet"/><br /><sub><b>Yoann Potinet</b></sub></a><br /><a href="#infra-intelligide" title="Infrastructure (Hosting, Build-Tools, etc)">ðŸš‡</a></td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/operasfantom"><img src="https://avatars.githubusercontent.com/u/31903947?v=4?s=100" width="100px;" alt="Pavel Iatchenii"/><br /><sub><b>Pavel Iatchenii</b></sub></a><br /><a href="https://github.com/nfrechette/rtm/commits?author=operasfantom" title="Code">ðŸ’»</a> <a href="#maintenance-operasfantom" title="Maintenance">ðŸš§</a></td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/GermanAizek"><img src="https://avatars.githubusercontent.com/u/21138600?v=4?s=100" width="100px;" alt="Herman Semenov"/><br /><sub><b>Herman Semenov</b></sub></a><br /><a href="https://github.com/nfrechette/rtm/commits?author=GermanAizek" title="Code">ðŸ’»</a></td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/valgur"><img src="https://avatars.githubusercontent.com/u/1778160?v=4?s=100" width="100px;" alt="Martin Valgur"/><br /><sub><b>Martin Valgur</b></sub></a><br /><a href="https://github.com/nfrechette/rtm/commits?author=valgur" title="Code">ðŸ’»</a></td>
      <td align="center" valign="top" width="14.28%"><a href="https://github.com/Kethers"><img src="https://avatars.githubusercontent.com/u/49612587?v=4?s=100" width="100px;" alt="Kethers"/><br /><sub><b>Kethers</b></sub></a><br /><a href="https://github.com/nfrechette/rtm/commits?author=Kethers" title="Code">ðŸ’»</a></td>
    </tr>
  </tbody>
</table>

<!-- markdownlint-restore -->
<!-- prettier-ignore-end -->

<!-- ALL-CONTRIBUTORS-LIST:END -->

This project follows the [all-contributors](https://github.com/all-contributors/all-contributors) specification. Contributions of any kind welcome!

```

`cmake/CMakeCompiler.cmake`:

```cmake
macro(setup_default_compiler_flags _project_name)
	if(MSVC) # That's also clang-cl
		# Replace some default compiler switches and add new ones
		STRING(REPLACE "/GR" "" CMAKE_CXX_FLAGS ${CMAKE_CXX_FLAGS})			# Disable RTTI
		if(CMAKE_CXX_COMPILER_ID MATCHES "Clang" OR BUILD_BENCHMARK_EXE)
			STRING(REPLACE "/W3" "/W4" CMAKE_CXX_FLAGS ${CMAKE_CXX_FLAGS})			# Enable level 4 warnings
		else()
			if(MSVC_VERSION GREATER 1920)
				# VS2019 and above
				STRING(REPLACE "/W3" "/Wall" CMAKE_CXX_FLAGS ${CMAKE_CXX_FLAGS})	# Enable all warnings
			else()
				STRING(REPLACE "/W3" "/W4" CMAKE_CXX_FLAGS ${CMAKE_CXX_FLAGS})		# Enable level 4 warnings
			endif()
		endif()
		target_compile_options(${_project_name} PRIVATE /Zi)				# Add debug info
		target_compile_options(${_project_name} PRIVATE /Oi)				# Generate intrinsic functions
		target_compile_options(${_project_name} PRIVATE /WX)				# Treat warnings as errors

		if (CMAKE_CXX_COMPILER_ID MATCHES "MSVC" OR CMAKE_CXX_COMPILER_ID MATCHES "Clang") # no Intel
			target_compile_options(${_project_name} PRIVATE /MP)            # Enable parallel compilation
		endif ()

		if(MSVC_VERSION GREATER 1900)
			# VS2017 and above
			target_compile_options(${_project_name} PRIVATE /permissive-)
		endif()

		if(USE_SIMD_INSTRUCTIONS)
			if(USE_AVX2_INSTRUCTIONS)
				target_compile_options(${_project_name} PRIVATE "/arch:AVX2")
			elseif(USE_AVX_INSTRUCTIONS)
				target_compile_options(${_project_name} PRIVATE "/arch:AVX")
			endif()
		else()
			add_definitions(-DRTM_NO_INTRINSICS)
		endif()

		# Disable various warnings that are harmless
		target_compile_options(${_project_name} PRIVATE /wd4514)			# Unreferenced inline function removed
		target_compile_options(${_project_name} PRIVATE /wd4619)			# No warning with specified number
		target_compile_options(${_project_name} PRIVATE /wd4820)			# Padding added after data member
		target_compile_options(${_project_name} PRIVATE /wd4710)			# Function not inlined
		target_compile_options(${_project_name} PRIVATE /wd4711)			# Function selected for automatic inlining
		target_compile_options(${_project_name} PRIVATE /wd4738)			# Storing 32-bit float in memory leads to rounding (x86)
		target_compile_options(${_project_name} PRIVATE /wd4746)			# Volatile access
		target_compile_options(${_project_name} PRIVATE /wd5045)			# Spectre mitigation for memory load

		if(CMAKE_CXX_COMPILER_ID MATCHES "Clang")
			target_compile_options(${_project_name} PRIVATE -Wno-c++98-compat)				# No need to support C++98
			target_compile_options(${_project_name} PRIVATE -Wno-c++98-compat-pedantic)		# No need to support C++98
		endif()

		# Add linker flags
		set(CMAKE_EXE_LINKER_FLAGS "${CMAKE_EXE_LINKER_FLAGS} /DEBUG")
	else()
		# TODO: Handle OS X properly: https://stackoverflow.com/questions/5334095/cmake-multiarchitecture-compilation
		if(CPU_INSTRUCTION_SET MATCHES "x86")
			target_compile_options(${_project_name} PRIVATE "-m32")
			target_link_libraries(${_project_name} PRIVATE "-m32")
		elseif(CPU_INSTRUCTION_SET MATCHES "x64")
			target_compile_options(${_project_name} PRIVATE "-m64")
			target_link_libraries(${_project_name} PRIVATE "-m64")
		endif()

		if(CPU_INSTRUCTION_SET MATCHES "x86" OR CPU_INSTRUCTION_SET MATCHES "x64")
			if(USE_SIMD_INSTRUCTIONS)
				if(USE_AVX2_INSTRUCTIONS)
					target_compile_options(${_project_name} PRIVATE "-mavx2")
					target_compile_options(${_project_name} PRIVATE "-mavx")
					target_compile_options(${_project_name} PRIVATE "-mbmi")
					target_compile_options(${_project_name} PRIVATE "-mfma")
				elseif(USE_AVX_INSTRUCTIONS)
					target_compile_options(${_project_name} PRIVATE "-mavx")
					target_compile_options(${_project_name} PRIVATE "-mbmi")
				else()
					target_compile_options(${_project_name} PRIVATE "-msse4.1")
				endif()
			else()
				add_definitions(-DRTM_NO_INTRINSICS)
			endif()
		else()
			if(NOT USE_SIMD_INSTRUCTIONS)
				add_definitions(-DRTM_NO_INTRINSICS)
			endif()
		endif()

		target_compile_options(${_project_name} PRIVATE -Wall -Wextra)		# Enable all warnings
		target_compile_options(${_project_name} PRIVATE -Wshadow)			# Enable shadowing warnings
		target_compile_options(${_project_name} PRIVATE -Werror)			# Treat warnings as errors

		# Disable various warnings that are harmless
		if(CMAKE_CXX_COMPILER_ID MATCHES "Clang")
			target_compile_options(${_project_name} PRIVATE -Wno-c++98-compat)	# No need to support C++98
		elseif(CMAKE_CXX_COMPILER_ID MATCHES "GNU")
			if(CMAKE_CXX_COMPILER_VERSION VERSION_LESS 9)
				target_compile_options(${_project_name} PRIVATE -Wno-attributes)	# False positive with earlier versions of GCC
			endif()
		endif()

		target_compile_options(${_project_name} PRIVATE -g)					# Enable debug symbols
	endif()
endmacro()

```

`cmake/CMakePlatforms.cmake`:

```cmake
if(PLATFORM_NAME)
	return()	# Already set
endif()

# Detect which platform we have
if(${CMAKE_SYSTEM_NAME} STREQUAL "Windows")
	set(PLATFORM_WINDOWS 1)
	set(PLATFORM_NAME "Windows")
elseif(${CMAKE_SYSTEM_NAME} STREQUAL "Darwin")
	if(PLATFORM_IOS)
		set(PLATFORM_NAME "iOS")
	else()
		set(PLATFORM_OSX 1)
		set(PLATFORM_NAME "OS X")
	endif()

	# Get the Xcode version being used.
	execute_process(COMMAND xcodebuild -version
		OUTPUT_VARIABLE PLATFORM_XCODE_VERSION
		ERROR_QUIET
		OUTPUT_STRIP_TRAILING_WHITESPACE)
	string(REGEX MATCH "Xcode [0-9\\.]+" PLATFORM_XCODE_VERSION "${PLATFORM_XCODE_VERSION}")
	string(REGEX REPLACE "Xcode ([0-9\\.]+)" "\\1" PLATFORM_XCODE_VERSION "${PLATFORM_XCODE_VERSION}")
	message(STATUS "Building with Xcode version: ${PLATFORM_XCODE_VERSION}")
elseif(${CMAKE_SYSTEM_NAME} STREQUAL "Linux")
	set(PLATFORM_LINUX 1)
	set(PLATFORM_NAME "Linux")
elseif(${CMAKE_SYSTEM_NAME} STREQUAL "Android")
	set(PLATFORM_ANDROID 1)
	set(PLATFORM_NAME "Android")
elseif(${CMAKE_SYSTEM_NAME} STREQUAL "Emscripten")
	set(PLATFORM_EMSCRIPTEN 1)
	set(PLATFORM_NAME "Emscripten")
elseif(${UNIX})
	set(PLATFORM_UNIX 1)
	set(PLATFORM_NAME "UNIX")
else()
	set(PLATFORM_NAME ${CMAKE_SYSTEM_NAME})
endif()

message(STATUS "Detected platform: ${PLATFORM_NAME}")

```

`cmake/CMakeUtils.cmake`:

```cmake
# Create groups for the test source files, this creates the proper directory structure under the
# Visual Studio filters
macro(create_source_groups _source_files _relative_directory)
	foreach(_file IN ITEMS ${_source_files})
		get_filename_component(_file_path "${_file}" PATH)
		file(RELATIVE_PATH _file_path_rel "${_relative_directory}" "${_file_path}")
		string(REPLACE "/" "\\" _group_path "${_file_path_rel}")
		string(REPLACE "..\\" "" _group_path "${_group_path}")
		source_group("${_group_path}" FILES "${_file}")
	endforeach()
endmacro()

```

`cmake/Toolchain-Android.cmake`:

```cmake
# For Android, we just set the platform name as we won't be using CMake to build anything.
# Instead Gradle is used through CMake.

set(PLATFORM_ANDROID 1)
set(PLATFORM_NAME "Android")

# Remap our CPU instruction set
if(CPU_INSTRUCTION_SET MATCHES "armv7")
	set(CPU_INSTRUCTION_SET "armeabi-v7a")
elseif(CPU_INSTRUCTION_SET MATCHES "arm64")
	set(CPU_INSTRUCTION_SET "arm64-v8a")
endif()

# Set our misc asset directory
set(RTM_ANDROID_MISC_DIR ${CMAKE_SOURCE_DIR}/tools/android_misc)

```

`cmake/Toolchain-iOS.cmake`:

```cmake
set(CMAKE_SYSTEM_NAME Darwin)

# Set here instead of CMakePlatforms.cmake since we can't distinguis otherwise
set(PLATFORM_IOS 1)

# Find and set the C/C++ compiler paths, cmake doesn't seem to do this properly on its own
execute_process(COMMAND xcrun --sdk iphoneos --find clang OUTPUT_VARIABLE CMAKE_C_COMPILER OUTPUT_STRIP_TRAILING_WHITESPACE)
execute_process(COMMAND xcrun --sdk iphoneos --find clang++ OUTPUT_VARIABLE CMAKE_CXX_COMPILER OUTPUT_STRIP_TRAILING_WHITESPACE)

set(CMAKE_MACOSX_BUNDLE YES)
set(CMAKE_XCODE_ATTRIBUTE_CODE_SIGN_IDENTITY "")
set(CMAKE_XCODE_ATTRIBUTE_CODE_SIGN_ENTITLEMENTS "")
set(CMAKE_OSX_SYSROOT iphoneos CACHE STRING "")
set(CMAKE_OSX_ARCHITECTURES arm64 CACHE STRING "")
set(CMAKE_XCODE_ATTRIBUTE_IPHONEOS_DEPLOYMENT_TARGET 17.0)

```

`docs/README.md`:

```md
# Documentation

*  [API and other conventions](api_conventions.md)
*  [Types supported](types_supported.md)
*  [SIMD support](simd_support.md)
*  [Handling asserts](handling_asserts.md)
*  [Getting started](getting_started.md)

```

`docs/api_conventions.md`:

```md
# API and other conventions

## C-style

C-style interfaces are generally preferred with a few supporting types. The simpler C-style interface leads to less intermediate code that the compiler needs to work with and it increases the likelihood that functions will be inlined. Inlining is a critical and integral part of performance for any math library. In general, code commonly used and that would benefit the most from being inlined uses a C-style API (vectors, quaternions, etc) while less critical code relies on various helper types (angles, helpers, etc).

## C++11

We restrict the code to use C++11 as it is the most widely used flavor on the platforms we currently support. *constexpr* in particular is used as much as possible. Despite the C-style API, everything lives under the *rtm* namespace and implementation details not meant to be consumed by clients is hidden inside the *rtm_impl* namespace as well as the *impl* header directory.

The library is 100% comprised of C++ headers and no linking is required. This makes for the easiest integration possible and it also gives us more freedom with what and when we can change things.

## Argument passing

This library supports many architectures, platforms, and compilers and sadly there is no consensus on how SIMD types should be passed by argument or returned by value. It is generally best to pass as many things by register, when possible, but usually only a certain number of registers can be used for it. Some platforms support aggregate types being passed by register (either by argument and/or by return value), others do not. To keep things as simple as possible, aliases are used for every type such as: *vector4f_arg0, vector4f_arg1, ..., vector4f_argn*.

RTM tries its best to do things optimally. Generally speaking, for code that isn't performance critical the difference will be very small and you are free to pass things by value or *const&* in your own code but using the argument aliases is encouraged.

## Matrix multiplication ordering

Whether you call it pre or post-multiplication, or left or right multiplication, it boils down to whether vectors are represented as rows or as columns. 

In the former case, multiplication with a matrix takes the form `v' = vM`  while in the later case we have `v' = Mv`.  Linear algebra typically treats vectors as *columns* and OpenGL opted to use that convention for [that reason](http://steve.hollasch.net/cgindex/math/matrix/column-vec.html). If you think of matrices as functions that modify an input and return an output it ends up reading like this: `result = object_to_world(local_to_object(input))`. This reads right-to-left as is common with nested function evaluation. Sadly, this somewhat breaks down when using multiplication operators: `result = object_to_world * local_to_object * input`. Regardless of how operator precedence works, the result will be identical but most modern programming languages (and western languages) read left-to-right.

On the other hand, DirectX uses *row* vectors and ends up with the much more natural: `result = input * local_to_object * object_to_world`. Your input is in local space, it gets transformed into object space before finally ending up in world space. Clean, clear, and readable. If you instead multiply the two matrices together on their own, you get the clear `local_to_world = local_to_object * object_to_world` instead of the awkward `local_to_world = object_to_world * local_to_object` you would get with OpenGL and *column* vectors.

An executive decision was made for all vectors to be row vectors for their improved readability.

## Coordinate frame

When relevant, a left-handed coordinate system is used:

* X axis = forward
* Y axis = right
* Z axis = up


```

`docs/getting_started.md`:

```md
# Getting started

In order to contribute to RTM you will first need to setup your environment.

## Setting up your environment

### Windows, Linux, and OS X for x86 and x64

1. Install *CMake 3.2* or higher (*3.14* for Visual Studio 2019, or *3.10* on OS X with *Xcode 10*), *Python 2.7 or 3*, and the proper compiler for your platform.
2. Execute `git submodule update --init` to get the files of external submodules (e.g. Catch2).
3. Generate the IDE solution with: `python make.py`  
   The solution is generated under `./build`
4. Build the IDE solution with: `python make.py -build`
5. Run the unit tests with: `python make.py -unit_test`
6. Build and run benchmarks with the `-bench` switch

On all three platforms, *AVX* support can be enabled by using the `-avx` switch and *AVX2* with `-avx2`. Intrinsic usage can be turned off with `-nosimd`.

### Windows ARM64

For *Windows on ARM64*, the steps are identical to *x86 and x64* but you will need *CMake 3.13 or higher* and you must provide the architecture on the command line: `python make.py -compiler vs2017 -cpu arm64`

### Android

For *Android*, the steps are identical to *Windows, Linux, and OS X* but you also need to install *Android NDK 21* (or higher). The build uses `gradle` and `-unit_test` as well as `-bench` will deploy and run on the device when executed (make sure that the `adb` executable is in your `PATH` for this to work).

*Android Studio v3.5* can be used to launch and debug. After running *CMake* to build and generate everything, the *Android Studio* projects can be found under the `./build` directory.

Note that running the benchmark on Android can be done but it currently involves a number of manual steps.

### iOS

For *iOS*, the steps are identical to the other platforms but due to code signing, you will need to perform the builds from *Xcode* manually. Note that this is only an issue if you attempt to use the unit tests locally.

In order to run these manually:

*  Open the *Xcode* project for the appropriate tool
*  In the project settings, enable automatic code signing and select your development team
*  Build and run on your device

Note that *iOS* builds have never been tested on an emulator.

### Emscripten

Emscripten support currently only has been tested on OS X and Linux. To use it, make sure to install a recent version of Emscripten SDK 1.39.11+.

## Commit message format

This library uses the [angular.js message format](https://github.com/angular/angular.js/blob/master/DEVELOPERS.md#commits) and it is enforced with commit linting through every pull request.

```

`docs/handling_asserts.md`:

```md
# Handling asserts

This library uses a simple system to handle asserts. Asserts are fatal and must terminate otherwise the behavior is undefined if execution continues.

A total of 4 behaviors are supported:

*  We can print to `stderr` and `abort`
*  We can `throw` and exception
*  We can call a custom function
*  Do nothing and strip the check at compile time (**default behavior**)

Everything necessary is implemented in [**rtm/impl/error.h**](../includes/rtm/impl/error.h).

*Note: It is **NOT** possible to have different assert behaviors in two or more C++ object files. All the C++ files within your static or dynamic library that reference RTM must all use the same assert handling strategy.*

## Aborting

In order to enable the aborting behavior, simply define the macro `RTM_ON_ASSERT_ABORT`:

`#define RTM_ON_ASSERT_ABORT`

## Throwing

In order to enable the throwing behavior, simply define the macro `RTM_ON_ASSERT_THROW`:

`#define RTM_ON_ASSERT_THROW`

Note that the type of the exception thrown is `std::runtime_error`.

## Custom function

In order to enable the custom function calling behavior, define the macro `RTM_ON_ASSERT_CUSTOM` with the name of the function to call:

`#define RTM_ON_ASSERT_CUSTOM on_custom_assert_impl`

Note that the function signature is as follow: `void on_custom_assert_impl(const char* expression, int line, const char* file, const char* format, ...) {}`

You can also define your own assert implementation by defining the `RTM_ASSERT` macro as well:

```c++
#define RTM_ON_ASSERT_CUSTOM
#define RTM_ASSERT(expression, format, ...) checkf(expression, ANSI_TO_TCHAR(format), #__VA_ARGS__)
```

## No checks

By default if no macro mentioned above is defined, all asserts will be stripped at compile time.

```

`docs/simd_support.md`:

```md
# SIMD support

## x86 and x64

Various versions of SSE are supported: SSE2, SSE3, SSE4, AVX, AVX2, and FMA.

*Note that even when FMA is enabled, its intrinsics are not used because they appear slower on at least Haswell and Ryzen.*

## ARM

Both ARM NEON and ARM64 NEON are supported.


```

`docs/types_supported.md`:

```md
# Types supported

Realtime Math supports the most commonly used types in video games and realtime applications in a SIMD friendly way with a strong focus on performance.

All types come in various flavors with a corresponding suffix for their width (e.g. vector4f):

*  **f** for *float32*
*  **d** for *float64*
*  **s** for *int16*
*  **i** for *int32*
*  **q** for *int64*

## Scalar

Both regular *float & double* types are supported for most operations and a special SIMD friendly scalar type as well: *scalarf & scalard*. Different architectures have an easier or harder time working with scalar floating point numbers. For example, older PowerPC processors had to write to memory and reload from it to transfer from one register file into another (e.g convert from a float to a SIMD vector). Modern processors handle this much better but inefficiencies remain, especially with SSE. While it is free to convert a SIMD scalar into a float with *_mm_cvtss_f32(..)* the reverse generally requires the compiler to fill the unused SIMD lanes with known values (either zero or the same). This introduces an extra instruction that isn't always required when only the first lane is used such as with *scalar_sqrt_reciprocal(..)*. By introducing a type for SIMD scalar values, each platform is free to make an optimal choice.

## Vector 4D

Lower dimensions vectors will not be supported with proper types but functions can support them with an appropriate suffix (e.g. *vector_dot3(..)*). If no suffix is specified, functions operate on the full width of the vector. When a function operates on a reduced number of SIMD lanes, the extra unused lanes will have undefined values.

*Vectors are row vectors in RTM and thus multiply on the left of matrices.*

## Mask 4D

A comparison mask used by vector selection/blending. Each SIMD lane consists of all ones (true) or zeroes (false) depending on the condition.

## Quaternion

Quaternions are 4D complex numbers commonly used to represent 3D rotations (when normalized). The **[xyz]** components are the real part while the **[w]** component is the imaginary part.

## QV (quaternion-vector)

A QV represents an affine transform in two distinct parts: a rotation quaternion and a vector3 translation. This type is commonly used in video games as it is very fast to work with and more compact than a full affine matrix.

## QVS (quaternion-vector-scalar)

A QVS represents an affine transform in three distinct parts: a rotation quaternion, a vector4 translation, and a scalar uniform scale. This type is commonly used in video games as it is very fast to work with and more compact than a full affine matrix. It represents uniform positive/negative scale through a single scalar value. When multiplying transforms of this type, scale combines as it would with matrices.

## QVV (quaternion-vector-vector)

A QVV represents an affine transform in three distinct parts: a rotation quaternion, a vector3 translation, and a vector3 non-uniform scale. This type is commonly used in video games as it is very fast to work with and more compact than a full affine matrix. It properly handles positive non-uniform scaling but negative scaling is a bit more problematic. A best effort is made by converting the quaternion to a matrix when necessary. If scale fidelity is important, consider using an affine matrix 3x4 instead. When multiplying transforms of this type, scale combines as it would with matrices.

## Matrix 3x3

A generic 3x3 matrix. Suitable to represent rotations mixed with 3D scale or anything else that might fit.

## Matrix 3x4

An 3x4 affine matrix represents a 3D rotation, 3D translation, and 3D scale. It properly deals with skew/shear when present but once scale with mirroring is combined, it cannot be safely extracted back. Affine matrices are 4x4 but have their last row always equal to **[0, 0, 0, 1]** which is why it is named 3x4.

## Matrix 4x4

A generic 4x4 matrix. Suitable to represent 3D projection matrices and the likes.

## Unaligned and storage friendly types

When manipulating vectors of various width, it is often desirable to store them as an unaligned sequence of floats with no padding. For example, while a 3D mesh has a number of `float3` vertices, storing and manipulating them as `vector4f` would use 33% more memory. To that end, a number of types are provided to help with this: `float2f, float2d, float3f, float3d, float4f, float4d`. These types have no alignment requirement beyond the natural float/double alignment. Functions such as `vector_load3(const float3f* input)` can load them from memory and return a vector4 of the correct type.

```

`external/README.md`:

```md
# External dependencies

Good news! There are no external dependencies needed by this library at runtime!

## Benchmark

[Google Benchmark v1.6.1](https://github.com/google/benchmark/releases/tag/v1.6.1) (Apache License 2.0) is used to benchmark various functions. You will only need it if you run the benchmark tool and it is included as-is without modifications.

## Catch2

[Catch2 v2.13.7](https://github.com/catchorg/Catch2/releases/tag/v2.13.7) (Boost Software License v1.0) is used by our [unit tests](../tests). You will only need it if you run the unit tests and it is included as-is without modifications.

```

`includes/rtm/camera_utilsd.h`:

```h
#pragma once

////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2024 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include "rtm/macros.h"
#include "rtm/math.h"
#include "rtm/scalard.h"
#include "rtm/vector4d.h"
#include "rtm/version.h"
#include "rtm/impl/compiler_utils.h"
#include "rtm/impl/error.h"

RTM_IMPL_FILE_PRAGMA_PUSH

namespace rtm
{
	RTM_IMPL_VERSION_NAMESPACE_BEGIN

	//////////////////////////////////////////////////////////////////////////
	// Returns a left-handed affine 3x4 matrix representing a camera transform.
	// A camera transform multiplied by a point3 transforms a point3 local to the camera
	// into world space.
	// The camera transform is located at the specified position,
	// looking towards the specified direction, and using the specified up direction.
	//
	// The look to direction and up inputs do not need to be normalized.
	//
	// In this frame of reference:
	//    - X axis: right direction
	//    - Y axis: up direction
	//    - Z axis: forward/look direction
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline matrix3x4d RTM_SIMD_CALL matrix_look_to(vector4d_arg0 position, vector4d_arg1 direction, vector4d_arg2 up) RTM_NO_EXCEPT
	{
		RTM_ASSERT(vector_is_finite3(position), "Look position must be finite");
		RTM_ASSERT(!vector_all_equal3(direction, vector_zero()) && vector_is_finite3(direction), "Look towards direction must be non-zero and finite");
		RTM_ASSERT(!vector_all_equal3(up, vector_zero()) && vector_is_finite3(up), "Look up direction must be non-zero and finite");

		// Constructs our orientation ortho-normal frame
		vector4d forward_axis = vector_normalize3(direction);
		vector4d right_axis = vector_normalize3(vector_cross3(up, forward_axis));
		vector4d up_axis = vector_cross3(forward_axis, right_axis);	// already normalized since inputs are normalized

		return matrix3x4d{ right_axis, up_axis, forward_axis, position };
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns a left-handed affine 3x4 matrix representing a camera transform.
	// A camera transform multiplied by a point3 transforms a point3 local to the camera
	// into world space.
	// The camera transform is located at the specified position,
	// looking towards the specified location, and using the specified up direction.
	//
	// The up input does not need to be normalized.
	//
	// In this frame of reference:
	//    - X axis: right direction
	//    - Y axis: up direction
	//    - Z axis: forward/look direction
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline matrix3x4d RTM_SIMD_CALL matrix_look_at(vector4d_arg0 position, vector4d_arg1 location, vector4d_arg2 up) RTM_NO_EXCEPT
	{
		vector4d look_direction = vector_sub(location, position);
		return matrix_look_to(position, look_direction, up);
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns a left-handed affine 3x4 matrix representing a camera view transform.
	// A camera view transform is the inverse of a camera transform.
	// A camera transform multiplied by a point3 transforms a point3 local to the camera
	// into world space while a camera view transform multiplied by a point3 does the
	// opposite: it transforms a world space point3 into local space of the camera.
	// This is typically used before applying a projection matrix.
	// The camera transform is located at the specified position,
	// looking towards the specified direction, and using the specified up direction.
	// The camera view transform is then constructed from its inverse.
	//
	// The look to direction and up inputs do not need to be normalized.
	//
	// In this frame of reference:
	//    - X axis: right direction
	//    - Y axis: up direction
	//    - Z axis: forward/look direction
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline matrix3x4d RTM_SIMD_CALL view_look_to(vector4d_arg0 position, vector4d_arg1 direction, vector4d_arg2 up) RTM_NO_EXCEPT
	{
		RTM_ASSERT(vector_is_finite3(position), "Look position must be finite");
		RTM_ASSERT(!vector_all_equal3(direction, vector_zero()) && vector_is_finite3(direction), "Look towards direction must be non-zero and finite");
		RTM_ASSERT(!vector_all_equal3(up, vector_zero()) && vector_is_finite3(up), "Look up direction must be non-zero and finite");

		// Constructs our orientation ortho-normal frame
		vector4d forward_axis = vector_normalize3(direction);
		vector4d right_axis = vector_normalize3(vector_cross3(up, forward_axis));
		vector4d up_axis = vector_cross3(forward_axis, right_axis);	// already normalized since inputs are normalized

		// The look-to matrix converts world space inputs into the view space or the look to camera
		// To that end, we thus need the inverse of the world space camera transform
		// Because our affine matrix is ortho-normalized, we can calculate its inverse using a transpose
		// We wish for the translation part to be in the W axis, and so we would have to set the camera
		// position in the W column before we perform a 4x4 transpose and negation of the W axis
		// To avoid this unnecessary 4x4 step, we instead transpose the upper 3x3 portion (in place) and set
		// the W axis directly

		// Negate the position to invert it
		vector4d neg_position = vector_neg(position);

		scalard pos_x = vector_dot3_as_scalar(right_axis, neg_position);
		scalard pos_y = vector_dot3_as_scalar(up_axis, neg_position);
		scalard pos_z = vector_dot3_as_scalar(forward_axis, neg_position);
		vector4d position_axis = vector_set(pos_x, pos_y, pos_z);

		// Transpose in place to invert the rotation
		RTM_MATRIXD_TRANSPOSE_3X3(right_axis, up_axis, forward_axis, right_axis, up_axis, forward_axis);

		return matrix3x4d{ right_axis, up_axis, forward_axis, position_axis };
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns a left-handed affine 3x4 matrix representing a camera view transform.
	// A camera view transform is the inverse of a camera transform.
	// A camera transform multiplied by a point3 transforms a point3 local to the camera
	// into world space while a camera view transform multiplied by a point3 does the
	// opposite: it transforms a world space point3 into local space of the camera.
	// This is typically used before applying a projection matrix.
	// The camera transform is located at the specified position,
	// looking towards the specified location, and using the specified up direction.
	// The camera view transform is then constructed from its inverse.
	//
	// The look to direction and up inputs do not need to be normalized.
	//
	// In this frame of reference:
	//    - X axis: right direction
	//    - Y axis: up direction
	//    - Z axis: forward/look direction
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline matrix3x4d RTM_SIMD_CALL view_look_at(vector4d_arg0 position, vector4d_arg1 location, vector4d_arg2 up) RTM_NO_EXCEPT
	{
		vector4d look_direction = vector_sub(location, position);
		return view_look_to(position, look_direction, up);
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns a left-handed 4x4 perspective projection matrix using the screen dimensions.
	//
	// In this frame of reference:
	//    - X axis: right direction
	//    - Y axis: up direction
	//    - Z axis: forward/look direction
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline matrix4x4d RTM_SIMD_CALL proj_perspective(double view_width, double view_height, double near_distance, double far_distance) RTM_NO_EXCEPT
	{
		RTM_ASSERT(view_width > 1.0E-8, "View width must be non-zero and positive");
		RTM_ASSERT(view_height > 1.0E-8, "View height must be non-zero and positive");
		RTM_ASSERT(near_distance > 0.0 && far_distance > 0.0, "Near and far distances must be non-zero and positive");
		RTM_ASSERT((far_distance - near_distance) > 1.0E-8, "Near and far distances cannot be equal");

		double double_near_distance = near_distance + near_distance;
		double scaled_far_distance = far_distance / (far_distance - near_distance);

		vector4d x_axis = vector_set(double_near_distance / view_width, 0.0, 0.0, 0.0);
		vector4d y_axis = vector_set(0.0, double_near_distance / view_height, 0.0, 0.0);
		vector4d z_axis = vector_set(0.0, 0.0, scaled_far_distance, 1.0);
		vector4d w_axis = vector_set(0.0, 0.0, -scaled_far_distance * near_distance, 0.0);

		return matrix4x4d{ x_axis, y_axis, z_axis, w_axis };
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns a left-handed 4x4 perspective projection matrix using the screen field of view.
	//
	// In this frame of reference:
	//    - X axis: right direction
	//    - Y axis: up direction
	//    - Z axis: forward/look direction
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline matrix4x4d RTM_SIMD_CALL proj_perspective_fov(double fov_angle_y, double aspect_ratio, double near_distance, double far_distance) RTM_NO_EXCEPT
	{
		RTM_ASSERT(fov_angle_y > 2.0E-8, "View FoV must be non-zero and positive");
		RTM_ASSERT(aspect_ratio > 1.0E-8, "View aspect ratio must be non-zero and positive");
		RTM_ASSERT(near_distance > 0.0 && far_distance > 0.0, "Near and far distances must be non-zero and positive");
		RTM_ASSERT((far_distance - near_distance) > 1.0E-8, "Near and far distances cannot be equal");

		double sin_fov;
		double cos_fov;
		scalar_sincos(fov_angle_y * 0.5, sin_fov, cos_fov);

		double scaled_view_height = cos_fov / sin_fov;
		double scaled_view_width = scaled_view_height / aspect_ratio;
		double scaled_far_distance = far_distance / (far_distance - near_distance);

		vector4d x_axis = vector_set(scaled_view_width, 0.0, 0.0, 0.0);
		vector4d y_axis = vector_set(0.0, scaled_view_height, 0.0, 0.0);
		vector4d z_axis = vector_set(0.0, 0.0, scaled_far_distance, 1.0);
		vector4d w_axis = vector_set(0.0, 0.0, -scaled_far_distance * near_distance, 0.0);

		return matrix4x4d{ x_axis, y_axis, z_axis, w_axis };
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns a left-handed 4x4 orthographic projection matrix.
	//
	// In this frame of reference:
	//    - X axis: right direction
	//    - Y axis: up direction
	//    - Z axis: forward/look direction
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline matrix4x4d RTM_SIMD_CALL proj_orthographic(double view_width, double view_height, double near_distance, double far_distance) RTM_NO_EXCEPT
	{
		RTM_ASSERT(view_width > 1.0E-8, "View width must be non-zero and positive");
		RTM_ASSERT(view_height > 1.0E-8, "View height must be non-zero and positive");
		RTM_ASSERT(near_distance > 0.0 && far_distance > 0.0, "Near and far distances must be non-zero and positive");
		RTM_ASSERT((far_distance - near_distance) > 1.0E-8, "Near and far distances cannot be equal");

		double inverse_distance_range = 1.0 / (far_distance - near_distance);

		vector4d x_axis = vector_set(2.0 / view_width, 0.0, 0.0, 0.0);
		vector4d y_axis = vector_set(0.0, 2.0 / view_height, 0.0, 0.0);
		vector4d z_axis = vector_set(0.0, 0.0, inverse_distance_range, 0.0);
		vector4d w_axis = vector_set(0.0, 0.0, -inverse_distance_range * near_distance, 1.0);

		return matrix4x4d{ x_axis, y_axis, z_axis, w_axis };
	}

	RTM_IMPL_VERSION_NAMESPACE_END
}

RTM_IMPL_FILE_PRAGMA_POP

```

`includes/rtm/camera_utilsf.h`:

```h
#pragma once

////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2024 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include "rtm/macros.h"
#include "rtm/math.h"
#include "rtm/scalarf.h"
#include "rtm/vector4f.h"
#include "rtm/version.h"
#include "rtm/impl/compiler_utils.h"
#include "rtm/impl/error.h"

RTM_IMPL_FILE_PRAGMA_PUSH

namespace rtm
{
	RTM_IMPL_VERSION_NAMESPACE_BEGIN

	//////////////////////////////////////////////////////////////////////////
	// Returns a left-handed affine 3x4 matrix representing a camera transform.
	// A camera transform multiplied by a point3 transforms a point3 local to the camera
	// into world space.
	// The camera transform is located at the specified position,
	// looking towards the specified direction, and using the specified up direction.
	//
	// The look to direction and up inputs do not need to be normalized.
	//
	// In this frame of reference:
	//    - X axis: right direction
	//    - Y axis: up direction
	//    - Z axis: forward/look direction
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline matrix3x4f RTM_SIMD_CALL matrix_look_to(vector4f_arg0 position, vector4f_arg1 direction, vector4f_arg2 up) RTM_NO_EXCEPT
	{
		RTM_ASSERT(vector_is_finite3(position), "Look position must be finite");
		RTM_ASSERT(!vector_all_equal3(direction, vector_zero()) && vector_is_finite3(direction), "Look towards direction must be non-zero and finite");
		RTM_ASSERT(!vector_all_equal3(up, vector_zero()) && vector_is_finite3(up), "Look up direction must be non-zero and finite");

		// Constructs our orientation ortho-normal frame
		vector4f forward_axis = vector_normalize3(direction);
		vector4f right_axis = vector_normalize3(vector_cross3(up, forward_axis));
		vector4f up_axis = vector_cross3(forward_axis, right_axis);	// already normalized since inputs are normalized

		return matrix3x4f{ right_axis, up_axis, forward_axis, position };
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns a left-handed affine 3x4 matrix representing a camera transform.
	// A camera transform multiplied by a point3 transforms a point3 local to the camera
	// into world space.
	// The camera transform is located at the specified position,
	// looking towards the specified location, and using the specified up direction.
	//
	// The up input does not need to be normalized.
	//
	// In this frame of reference:
	//    - X axis: right direction
	//    - Y axis: up direction
	//    - Z axis: forward/look direction
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline matrix3x4f RTM_SIMD_CALL matrix_look_at(vector4f_arg0 position, vector4f_arg1 location, vector4f_arg2 up) RTM_NO_EXCEPT
	{
		vector4f look_direction = vector_sub(location, position);
		return matrix_look_to(position, look_direction, up);
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns a left-handed affine 3x4 matrix representing a camera view transform.
	// A camera view transform is the inverse of a camera transform.
	// A camera transform multiplied by a point3 transforms a point3 local to the camera
	// into world space while a camera view transform multiplied by a point3 does the
	// opposite: it transforms a world space point3 into local space of the camera.
	// This is typically used before applying a projection matrix.
	// The camera transform is located at the specified position,
	// looking towards the specified direction, and using the specified up direction.
	// The camera view transform is then constructed from its inverse.
	//
	// The look to direction and up inputs do not need to be normalized.
	//
	// In this frame of reference:
	//    - X axis: right direction
	//    - Y axis: up direction
	//    - Z axis: forward/look direction
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline matrix3x4f RTM_SIMD_CALL view_look_to(vector4f_arg0 position, vector4f_arg1 direction, vector4f_arg2 up) RTM_NO_EXCEPT
	{
		RTM_ASSERT(vector_is_finite3(position), "Look position must be finite");
		RTM_ASSERT(!vector_all_equal3(direction, vector_zero()) && vector_is_finite3(direction), "Look towards direction must be non-zero and finite");
		RTM_ASSERT(!vector_all_equal3(up, vector_zero()) && vector_is_finite3(up), "Look up direction must be non-zero and finite");

		// Constructs our orientation ortho-normal frame
		vector4f forward_axis = vector_normalize3(direction);
		vector4f right_axis = vector_normalize3(vector_cross3(up, forward_axis));
		vector4f up_axis = vector_cross3(forward_axis, right_axis);	// already normalized since inputs are normalized

		// The look-to matrix converts world space inputs into the view space or the look to camera
		// To that end, we thus need the inverse of the world space camera transform
		// Because our affine matrix is ortho-normalized, we can calculate its inverse using a transpose
		// We wish for the translation part to be in the W axis, and so we would have to set the camera
		// position in the W column before we perform a 4x4 transpose and negation of the W axis
		// To avoid this unnecessary 4x4 step, we instead transpose the upper 3x3 portion (in place) and set
		// the W axis directly

		// Negate the position to invert it
		vector4f neg_position = vector_neg(position);

		scalarf pos_x = vector_dot3_as_scalar(right_axis, neg_position);
		scalarf pos_y = vector_dot3_as_scalar(up_axis, neg_position);
		scalarf pos_z = vector_dot3_as_scalar(forward_axis, neg_position);
		vector4f position_axis = vector_set(pos_x, pos_y, pos_z);

		// Transpose in place to invert the rotation
		RTM_MATRIXF_TRANSPOSE_3X3(right_axis, up_axis, forward_axis, right_axis, up_axis, forward_axis);

		return matrix3x4f{ right_axis, up_axis, forward_axis, position_axis };
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns a left-handed affine 3x4 matrix representing a camera view transform.
	// A camera view transform is the inverse of a camera transform.
	// A camera transform multiplied by a point3 transforms a point3 local to the camera
	// into world space while a camera view transform multiplied by a point3 does the
	// opposite: it transforms a world space point3 into local space of the camera.
	// This is typically used before applying a projection matrix.
	// The camera transform is located at the specified position,
	// looking towards the specified location, and using the specified up direction.
	// The camera view transform is then constructed from its inverse.
	//
	// The look to direction and up inputs do not need to be normalized.
	//
	// In this frame of reference:
	//    - X axis: right direction
	//    - Y axis: up direction
	//    - Z axis: forward/look direction
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline matrix3x4f RTM_SIMD_CALL view_look_at(vector4f_arg0 position, vector4f_arg1 location, vector4f_arg2 up) RTM_NO_EXCEPT
	{
		vector4f look_direction = vector_sub(location, position);
		return view_look_to(position, look_direction, up);
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns a left-handed 4x4 perspective projection matrix using the screen dimensions.
	//
	// In this frame of reference:
	//    - X axis: right direction
	//    - Y axis: up direction
	//    - Z axis: forward/look direction
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline matrix4x4f RTM_SIMD_CALL proj_perspective(float view_width, float view_height, float near_distance, float far_distance) RTM_NO_EXCEPT
	{
		RTM_ASSERT(view_width > 1.0E-8F, "View width must be non-zero and positive");
		RTM_ASSERT(view_height > 1.0E-8F, "View height must be non-zero and positive");
		RTM_ASSERT(near_distance > 0.0F && far_distance > 0.0F, "Near and far distances must be non-zero and positive");
		RTM_ASSERT((far_distance - near_distance) > 1.0E-8F, "Near and far distances cannot be equal");

		float double_near_distance = near_distance + near_distance;
		float scaled_far_distance = far_distance / (far_distance - near_distance);

		vector4f x_axis = vector_set(double_near_distance / view_width, 0.0F, 0.0F, 0.0F);
		vector4f y_axis = vector_set(0.0F, double_near_distance / view_height, 0.0F, 0.0F);
		vector4f z_axis = vector_set(0.0F, 0.0F, scaled_far_distance, 1.0F);
		vector4f w_axis = vector_set(0.0F, 0.0F, -scaled_far_distance * near_distance, 0.0F);

		return matrix4x4f{ x_axis, y_axis, z_axis, w_axis };
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns a left-handed 4x4 perspective projection matrix using the screen field of view.
	//
	// In this frame of reference:
	//    - X axis: right direction
	//    - Y axis: up direction
	//    - Z axis: forward/look direction
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline matrix4x4f RTM_SIMD_CALL proj_perspective_fov(float fov_angle_y, float aspect_ratio, float near_distance, float far_distance) RTM_NO_EXCEPT
	{
		RTM_ASSERT(fov_angle_y > 2.0E-8F, "View FoV must be non-zero and positive");
		RTM_ASSERT(aspect_ratio > 1.0E-8F, "View aspect ratio must be non-zero and positive");
		RTM_ASSERT(near_distance > 0.0F && far_distance > 0.0F, "Near and far distances must be non-zero and positive");
		RTM_ASSERT((far_distance - near_distance) > 1.0E-8F, "Near and far distances cannot be equal");

		float sin_fov;
		float cos_fov;
		scalar_sincos(fov_angle_y * 0.5F, sin_fov, cos_fov);

		float scaled_view_height = cos_fov / sin_fov;
		float scaled_view_width = scaled_view_height / aspect_ratio;
		float scaled_far_distance = far_distance / (far_distance - near_distance);

		vector4f x_axis = vector_set(scaled_view_width, 0.0F, 0.0F, 0.0F);
		vector4f y_axis = vector_set(0.0F, scaled_view_height, 0.0F, 0.0F);
		vector4f z_axis = vector_set(0.0F, 0.0F, scaled_far_distance, 1.0F);
		vector4f w_axis = vector_set(0.0F, 0.0F, -scaled_far_distance * near_distance, 0.0F);

		return matrix4x4f{ x_axis, y_axis, z_axis, w_axis };
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns a left-handed 4x4 orthographic projection matrix.
	//
	// In this frame of reference:
	//    - X axis: right direction
	//    - Y axis: up direction
	//    - Z axis: forward/look direction
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline matrix4x4f RTM_SIMD_CALL proj_orthographic(float view_width, float view_height, float near_distance, float far_distance) RTM_NO_EXCEPT
	{
		RTM_ASSERT(view_width > 1.0E-8F, "View width must be non-zero and positive");
		RTM_ASSERT(view_height > 1.0E-8F, "View height must be non-zero and positive");
		RTM_ASSERT(near_distance > 0.0F && far_distance > 0.0F, "Near and far distances must be non-zero and positive");
		RTM_ASSERT((far_distance - near_distance) > 1.0E-8F, "Near and far distances cannot be equal");

		float inverse_distance_range = 1.0F / (far_distance - near_distance);

		vector4f x_axis = vector_set(2.0F / view_width, 0.0F, 0.0F, 0.0F);
		vector4f y_axis = vector_set(0.0F, 2.0F / view_height, 0.0F, 0.0F);
		vector4f z_axis = vector_set(0.0F, 0.0F, inverse_distance_range, 0.0F);
		vector4f w_axis = vector_set(0.0F, 0.0F, -inverse_distance_range * near_distance, 1.0F);

		return matrix4x4f{ x_axis, y_axis, z_axis, w_axis };
	}

	RTM_IMPL_VERSION_NAMESPACE_END
}

RTM_IMPL_FILE_PRAGMA_POP

```

`includes/rtm/config.h`:

```h
#pragma once

////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2025 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

// This file gathers all defines that can be used to control RTM's behavior
// along with context.

//////////////////////////////////////////////////////////////////////////
// This library uses a simple system to handle asserts. Asserts are fatal and must terminate
// otherwise the behavior is undefined if execution continues.
//
// A total of 4 behaviors are supported:
//    - We can print to stderr and abort
//    - We can throw and exception
//    - We can call a custom function
//    - Do nothing and strip the check at compile time (default behavior)
//
// Aborting:
//    In order to enable the aborting behavior, simply define the macro RTM_ON_ASSERT_ABORT:
//    #define RTM_ON_ASSERT_ABORT
//
// Throwing:
//    In order to enable the throwing behavior, simply define the macro RTM_ON_ASSERT_THROW:
//    #define RTM_ON_ASSERT_THROW
//    Note that the type of the exception thrown is rtm::runtime_assert.
//
// Custom function:
//    In order to enable the custom function calling behavior, define the macro RTM_ON_ASSERT_CUSTOM
//    with the name of the function to call:
//    #define RTM_ON_ASSERT_CUSTOM on_custom_assert_impl
//    Note that the function signature is as follow:
//    void on_custom_assert_impl(const char* expression, int line, const char* file, const char* format, ...) {}
//
//    You can also define your own assert implementation by defining the RTM_ASSERT macro as well:
//    #define RTM_ON_ASSERT_CUSTOM
//    #define RTM_ASSERT(expression, format, ...) checkf(expression, ANSI_TO_TCHAR(format), #__VA_ARGS__)
//
//    [Custom String Format Specifier]
//    Note that if you use a custom function, you may need to override the RTM_ASSERT_STRING_FORMAT_SPECIFIER
//    to properly handle ANSI/Unicode support. The C++11 standard does not support a way to say that '%s'
//    always means an ANSI string (with 'const char*' as type). MSVC does support '%hs' but other compilers
//    do not.
//
// No checks:
//    By default if no macro mentioned above is defined, all asserts will be stripped
//    at compile time.
//////////////////////////////////////////////////////////////////////////

// You can uncomment one of these or specify it on the compilation command line
//#define RTM_ON_ASSERT_ABORT
//#define RTM_ON_ASSERT_THROW
//#define RTM_ON_ASSERT_CUSTOM(expression, line, file, format, ...) (void)expression

// See [Custom String Format Specifier] for details
#if !defined(RTM_ASSERT_STRING_FORMAT_SPECIFIER)
	#define RTM_ASSERT_STRING_FORMAT_SPECIFIER "%s"
#endif

// You can disable deprecation warnings by defining RTM_NO_DEPRECATION
//#define RTM_NO_DEPRECATION

// By default, RTM uses the intrinsics of the target platform as specified on the
// compiler command line. You can disable SIMD intrinsic usage by defining RTM_NO_INTRINSICS
//#define RTM_NO_INTRINSICS

// If you use C++20 or greater, you can disable std::bit_cast usage by defining RTM_NO_BIT_CAST
// This is sometimes necessary if you compile with a modern compiler but use an older stdlib
// that does not contain the bit_cast header
//#define RTM_NO_BIT_CAST

```

`includes/rtm/constants.h`:

```h
#pragma once

////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include "rtm/version.h"
#include "rtm/impl/compiler_utils.h"

RTM_IMPL_FILE_PRAGMA_PUSH

//////////////////////////////////////////////////////////////////////////
// This macro allows us to easily define constants that coerce to the proper desired type.
//////////////////////////////////////////////////////////////////////////
#define RTM_DEFINE_FLOAT_CONSTANT(name, value) \
	namespace rtm_impl \
	{ \
		/* Structure that holds our value at compile time */ \
		template<bool sign_bias> \
		struct RTM_JOIN_TOKENS(float_constant_, name) \
		{ \
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr operator double() const noexcept { return sign_bias ? value : -value; } \
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr operator float() const noexcept { return sign_bias ? static_cast<float>(value) : static_cast<float>(-value); } \
			/* Unary helpers */ \
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr RTM_JOIN_TOKENS(float_constant_, name) <!sign_bias> operator-() const noexcept { return RTM_JOIN_TOKENS(float_constant_, name) <!sign_bias>{}; } \
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr RTM_JOIN_TOKENS(float_constant_, name) <sign_bias> operator+() const noexcept { return RTM_JOIN_TOKENS(float_constant_, name) <sign_bias>{}; } \
		}; \
		/* Multiplication helpers */ \
		template<bool sign_bias> RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr double operator*(double lhs, RTM_JOIN_TOKENS(float_constant_, name) <sign_bias> rhs) noexcept { return lhs * double(rhs); } \
		template<bool sign_bias> RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr double operator*(RTM_JOIN_TOKENS(float_constant_, name) <sign_bias> lhs, double rhs) noexcept { return double(lhs) * rhs; } \
		template<bool sign_bias> RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr float operator*(float lhs, RTM_JOIN_TOKENS(float_constant_, name) <sign_bias> rhs) noexcept { return lhs * float(rhs); } \
		template<bool sign_bias> RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr float operator*(RTM_JOIN_TOKENS(float_constant_, name) <sign_bias> lhs, float rhs) noexcept { return float(lhs) * rhs; } \
		/* Division helpers */ \
		template<bool sign_bias> RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr double operator/(double lhs, RTM_JOIN_TOKENS(float_constant_, name) <sign_bias> rhs) noexcept { return lhs / double(rhs); } \
		template<bool sign_bias> RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr double operator/(RTM_JOIN_TOKENS(float_constant_, name) <sign_bias> lhs, double rhs) noexcept { return double(lhs) / rhs; } \
		template<bool sign_bias> RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr float operator/(float lhs, RTM_JOIN_TOKENS(float_constant_, name) <sign_bias> rhs) noexcept { return lhs / float(rhs); } \
		template<bool sign_bias> RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr float operator/(RTM_JOIN_TOKENS(float_constant_, name) <sign_bias> lhs, float rhs) noexcept { return float(lhs) / rhs; } \
		/* Addition helpers */ \
		template<bool sign_bias> RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr double operator+(double lhs, RTM_JOIN_TOKENS(float_constant_, name) <sign_bias> rhs) noexcept { return lhs + double(rhs); } \
		template<bool sign_bias> RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr double operator+(RTM_JOIN_TOKENS(float_constant_, name) <sign_bias> lhs, double rhs) noexcept { return double(lhs) + rhs; } \
		template<bool sign_bias> RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr float operator+(float lhs, RTM_JOIN_TOKENS(float_constant_, name) <sign_bias> rhs) noexcept { return lhs + float(rhs); } \
		template<bool sign_bias> RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr float operator+(RTM_JOIN_TOKENS(float_constant_, name) <sign_bias> lhs, float rhs) noexcept { return float(lhs) + rhs; } \
		/* Subtraction helpers */ \
		template<bool sign_bias> RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr double operator-(double lhs, RTM_JOIN_TOKENS(float_constant_, name) <sign_bias> rhs) noexcept { return lhs - double(rhs); } \
		template<bool sign_bias> RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr double operator-(RTM_JOIN_TOKENS(float_constant_, name) <sign_bias> lhs, double rhs) noexcept { return double(lhs) - rhs; } \
		template<bool sign_bias> RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr float operator-(float lhs, RTM_JOIN_TOKENS(float_constant_, name) <sign_bias> rhs) noexcept { return lhs - float(rhs); } \
		template<bool sign_bias> RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr float operator-(RTM_JOIN_TOKENS(float_constant_, name) <sign_bias> lhs, float rhs) noexcept { return float(lhs) - rhs; } \
		/* Relational helpers */ \
		template<bool sign_bias> RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr bool operator<(double lhs, RTM_JOIN_TOKENS(float_constant_, name) <sign_bias> rhs) noexcept { return lhs < double(rhs); } \
		template<bool sign_bias> RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr bool operator<(RTM_JOIN_TOKENS(float_constant_, name) <sign_bias> lhs, double rhs) noexcept { return double(lhs) < rhs; } \
		template<bool sign_bias> RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr bool operator<(float lhs, RTM_JOIN_TOKENS(float_constant_, name) <sign_bias> rhs) noexcept { return lhs < float(rhs); } \
		template<bool sign_bias> RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr bool operator<(RTM_JOIN_TOKENS(float_constant_, name) <sign_bias> lhs, float rhs) noexcept { return float(lhs) < rhs; } \
		template<bool sign_bias> RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr bool operator<=(double lhs, RTM_JOIN_TOKENS(float_constant_, name) <sign_bias> rhs) noexcept { return lhs <= double(rhs); } \
		template<bool sign_bias> RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr bool operator<=(RTM_JOIN_TOKENS(float_constant_, name) <sign_bias> lhs, double rhs) noexcept { return double(lhs) <= rhs; } \
		template<bool sign_bias> RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr bool operator<=(float lhs, RTM_JOIN_TOKENS(float_constant_, name) <sign_bias> rhs) noexcept { return lhs <= float(rhs); } \
		template<bool sign_bias> RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr bool operator<=(RTM_JOIN_TOKENS(float_constant_, name) <sign_bias> lhs, float rhs) noexcept { return float(lhs) <= rhs; } \
		template<bool sign_bias> RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr bool operator>(double lhs, RTM_JOIN_TOKENS(float_constant_, name) <sign_bias> rhs) noexcept { return lhs > double(rhs); } \
		template<bool sign_bias> RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr bool operator>(RTM_JOIN_TOKENS(float_constant_, name) <sign_bias> lhs, double rhs) noexcept { return double(lhs) > rhs; } \
		template<bool sign_bias> RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr bool operator>(float lhs, RTM_JOIN_TOKENS(float_constant_, name) <sign_bias> rhs) noexcept { return lhs > float(rhs); } \
		template<bool sign_bias> RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr bool operator>(RTM_JOIN_TOKENS(float_constant_, name) <sign_bias> lhs, float rhs) noexcept { return float(lhs) > rhs; } \
		template<bool sign_bias> RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr bool operator>=(double lhs, RTM_JOIN_TOKENS(float_constant_, name) <sign_bias> rhs) noexcept { return lhs >= double(rhs); } \
		template<bool sign_bias> RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr bool operator>=(RTM_JOIN_TOKENS(float_constant_, name) <sign_bias> lhs, double rhs) noexcept { return double(lhs) >= rhs; } \
		template<bool sign_bias> RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr bool operator>=(float lhs, RTM_JOIN_TOKENS(float_constant_, name) <sign_bias> rhs) noexcept { return lhs >= float(rhs); } \
		template<bool sign_bias> RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr bool operator>=(RTM_JOIN_TOKENS(float_constant_, name) <sign_bias> lhs, float rhs) noexcept { return float(lhs) >= rhs; } \
		template<bool sign_bias> RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr bool operator==(double lhs, RTM_JOIN_TOKENS(float_constant_, name) <sign_bias> rhs) noexcept { return lhs == double(rhs); } \
		template<bool sign_bias> RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr bool operator==(RTM_JOIN_TOKENS(float_constant_, name) <sign_bias> lhs, double rhs) noexcept { return double(lhs) == rhs; } \
		template<bool sign_bias> RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr bool operator==(float lhs, RTM_JOIN_TOKENS(float_constant_, name) <sign_bias> rhs) noexcept { return lhs == float(rhs); } \
		template<bool sign_bias> RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr bool operator==(RTM_JOIN_TOKENS(float_constant_, name) <sign_bias> lhs, float rhs) noexcept { return float(lhs) == rhs; } \
		template<bool sign_bias> RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr bool operator!=(double lhs, RTM_JOIN_TOKENS(float_constant_, name) <sign_bias> rhs) noexcept { return lhs != double(rhs); } \
		template<bool sign_bias> RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr bool operator!=(RTM_JOIN_TOKENS(float_constant_, name) <sign_bias> lhs, double rhs) noexcept { return double(lhs) != rhs; } \
		template<bool sign_bias> RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr bool operator!=(float lhs, RTM_JOIN_TOKENS(float_constant_, name) <sign_bias> rhs) noexcept { return lhs != float(rhs); } \
		template<bool sign_bias> RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr bool operator!=(RTM_JOIN_TOKENS(float_constant_, name) <sign_bias> lhs, float rhs) noexcept { return float(lhs) != rhs; } \
	} \
	/* Function user code calls to return the constant value */ \
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr RTM_JOIN_TOKENS(rtm_impl::float_constant_, name) <true> name() noexcept { return RTM_JOIN_TOKENS(rtm_impl::float_constant_, name) <true>{}; }

namespace rtm
{

	RTM_IMPL_VERSION_NAMESPACE_BEGIN

	namespace constants
	{
		//////////////////////////////////////////////////////////////////////////
		// Constants are defined here. Usage is meant to be as natural as possible.
		// e.g. float foo = rtm::constants::pi();
		//      double bar = rtm::constants::pi() * 0.5;
		//////////////////////////////////////////////////////////////////////////

		//////////////////////////////////////////////////////////////////////////
		// PI
		RTM_DEFINE_FLOAT_CONSTANT(pi, 3.141592653589793238462643383279502884)

		//////////////////////////////////////////////////////////////////////////
		// PI / 2
		RTM_DEFINE_FLOAT_CONSTANT(half_pi, 1.570796326794896619231321691639751442)

		//////////////////////////////////////////////////////////////////////////
		// PI * 2
		RTM_DEFINE_FLOAT_CONSTANT(two_pi, 6.283185307179586476925286766559005768)

		//////////////////////////////////////////////////////////////////////////
		// 1.0 / (PI * 2)
		RTM_DEFINE_FLOAT_CONSTANT(one_div_two_pi, 1.591549430918953357688837633725143620e-01)

		//////////////////////////////////////////////////////////////////////////
		// PI / 180
		RTM_DEFINE_FLOAT_CONSTANT(pi_div_one_eighty, 0.01745329251994329576923690768489)

		//////////////////////////////////////////////////////////////////////////
		// 180 / PI
		RTM_DEFINE_FLOAT_CONSTANT(one_eighty_div_pi, 57.295779513082320876798154814105)
	}

	RTM_IMPL_VERSION_NAMESPACE_END
}

RTM_IMPL_FILE_PRAGMA_POP

```

`includes/rtm/experimental/impl/vqm_common.h`:

```h
#pragma once

////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2024 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include "rtm/math.h"
#include "rtm/version.h"
#include "rtm/experimental/types.h"
#include "rtm/impl/compiler_utils.h"

RTM_IMPL_FILE_PRAGMA_PUSH

namespace rtm
{
	RTM_IMPL_VERSION_NAMESPACE_BEGIN

	namespace rtm_impl
	{
		//////////////////////////////////////////////////////////////////////////
		// This is a helper struct to allow a single consistent API between
		// various VQM transform types when the semantics are identical but the return
		// type differs. Implicit coercion is used to return the desired value
		// at the call site.
		//////////////////////////////////////////////////////////////////////////
		struct vqm_identity_impl
		{
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator vqmd() const RTM_NO_EXCEPT
			{
				vqmd result;
				result.rotation = quat_identity();
				result.translation = vector_zero();
				result.x_axis = vector_set(1.0, 0.0, 0.0, 0.0);
				result.y_axis = vector_set(0.0, 1.0, 0.0, 0.0);
				result.z_axis = vector_set(0.0, 0.0, 1.0, 0.0);

				return result;
			}

			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator vqmf() const RTM_NO_EXCEPT
			{
				vqmf result;
				result.rotation = quat_identity();
				result.translation = vector_zero();
				result.x_axis = vector_set(1.0F, 0.0F, 0.0F, 0.0F);
				result.y_axis = vector_set(0.0F, 1.0F, 0.0F, 0.0F);
				result.z_axis = vector_set(0.0F, 0.0F, 1.0F, 0.0F);

				return result;
			}
		};
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the identity VQM transform.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr rtm_impl::vqm_identity_impl RTM_SIMD_CALL vqm_identity() RTM_NO_EXCEPT
	{
		return rtm_impl::vqm_identity_impl();
	}

	RTM_IMPL_VERSION_NAMESPACE_END
}

RTM_IMPL_FILE_PRAGMA_POP

```

`includes/rtm/experimental/types.h`:

```h
#pragma once

////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2024 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include "rtm/math.h"
#include "rtm/version.h"
#include "rtm/impl/compiler_utils.h"

RTM_IMPL_FILE_PRAGMA_PUSH

namespace rtm
{
	RTM_IMPL_VERSION_NAMESPACE_BEGIN

	struct vqmd
	{
		// The internal format is meant to be opaque, use vqm_* accessors
		quatd		rotation;
		vector4d	translation;
		vector4d	x_axis;
		vector4d	y_axis;
		vector4d	z_axis;
	};

	struct vqmf
	{
		// The internal format is meant to be opaque, use vqm_* accessors
		quatf		rotation;
		vector4f	translation;
		vector4f	x_axis;
		vector4f	y_axis;
		vector4f	z_axis;
	};

	RTM_IMPL_VERSION_NAMESPACE_END
}

RTM_IMPL_FILE_PRAGMA_POP

```

`includes/rtm/experimental/vqmd.h`:

```h
#pragma once

////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2024 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include "rtm/math.h"
#include "rtm/matrix3x3d.h"
#include "rtm/quatd.h"
#include "rtm/vector4d.h"
#include "rtm/version.h"
#include "rtm/impl/compiler_utils.h"
#include "rtm/experimental/impl/vqm_common.h"

RTM_IMPL_FILE_PRAGMA_PUSH

namespace rtm
{
	RTM_IMPL_VERSION_NAMESPACE_BEGIN

	//////////////////////////////////////////////////////////////////////////
	// See here for details: The VQM-Group and its Applications
	//                       By Michael Aristidou and Xin Li
	//
	// Key insights:
	// - We wish to maintain rotation, translation, and scale/shear separately as they do not
	//   interpolate the same way. In particular, scale/shear and rotation mix together in the
	//   upper 3x3 part of affine matrices and it is difficult to manage them correctly as there
	//   is no unique way to decompose them.
	// - Let us define [T1, R1, S1] and [T2, R2, S2] as two transforms with 3 affine matrices each
	//   We will ignore translation since it mostly lives in its own dimension (4th) and does not
	//   interfere with scale/shear/rotation.
	// - We define multiplication as follow:
	//   (R2 * S2) * (R1 * S1) = (R3 * S3)
	//   By construction, we wish R3 = R2 * R1, we substitute
	//   (R2 * S2) * (R1 * S1) = (R2 * R1 * S3)
	//   We solve for S3, which is the scale/shear matrix we are looking for
	//   (R2^-1 * R2) * S2 * (R1 * S1) = R1 * S3
	//   R1^-1 * (R2^-1 * R2) * S2 * (R1 * S1) = S3
	//   R2^-1 * R2 cancel out and we get
	//   R1^-1 * S2 * R1 * S1 = S3
	//   In plain english, to compute our scale/shear matrix, we rotate S1 into the space of S2
	//   by multiplying with R1, then we scale/shear the result, and return into the space of S1
	//   by applying the inverse R1 rotation. A sensible result.
	// - This is all well and good with matrices, but we wish to retain rotation as a quaternion
	//   for its numerical stability, compact nature, and superior interpolation. How do we multiply
	//   a matrix with a quaternion?
	// - A key insight is that if we apply a rotation matrix onto any other affine matrix (e.g. a
	//   scale/shear or other pure rotation matrix), what occurs under the matrix multiplication is
	//   that each column of the affine matrix is rotated by our rotation matrix. This is something
	//   we can easily achieve as well with a quaternion: by using the sandwich product. From this
	//   key insight, the various identities in the paper follow as matrix multiplication with a pure
	//   rotation matrix is equivalent to the sandwich product of each column of the other matrix.
	//
	// Some VQM identities:
	// - If we treat M as a homogenous quaternion matrix, q a quaternion, and r a pure quaternion,
	//   then: q * (M * r) * q^-1 = (q * M * q^-1) * r
	//   In plain english, applying scale/shear to a point and rotating that point is equivalent to
	//   rotating the scale/shear matrix and applying the result to that point.
	//
	// - If M and N are homogenous quaternion matrices, and q1 and q2 quaternions then:
	//   (q2 * N * q2^-1) * (q1 * M * q1^-1) = q2 * (N * (q1 * M * q1^-1)) * q2^-1
	//   In plain english, the product of two rotated scale/shear matrices is equivalent to the
	//   rotated product of one scale/shear matrix with another rotated scale/shear matrix meaning
	//   rotation can occur before the multiplication or after due to assossiativity. This is
	//   straightforward to see if the rotations are expressed in matrix form.
	//////////////////////////////////////////////////////////////////////////

	//////////////////////////////////////////////////////////////////////////
	// A VQM transform represents a 3D rotation (quaternion), 3D translation (vector3),
	// and 3D non-uniform scale and shear (matrix 3x3).
	// VQM forms a group with a well defined multiplication and inverse. Its
	// multiplication is assossiative but not commutative (like quaternions/matrices).
	// Rotations are assumed to represent a single turn (normalized quaternion).
	//////////////////////////////////////////////////////////////////////////
#if 0	// defined in types.h
	struct vqmd
	{
		// The internal format is meant to be opaque, use vqm_* accessors
		quatd		rotation;
		vector4d	translation;
		vector4d	x_axis;
		vector4d	y_axis;
		vector4d	z_axis;
	};
#endif

	template<> struct related_types<vqmd> : related_types<double> {};

	//////////////////////////////////////////////////////////////////////////
	// Creates a VQM transform from a rotation quaternion, a translation, and a 3D scale.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vqmd RTM_SIMD_CALL vqm_set(vector4d_arg0 translation, quatd_arg1 rotation, vector4d_arg2 scale) RTM_NO_EXCEPT
	{
		vector4d zero = vector_zero();

		vqmd result;
		result.rotation = rotation;
		result.translation = translation;
		result.x_axis = vector_set_x(zero, vector_get_x_as_scalar(scale));
		result.y_axis = vector_set_y(zero, vector_get_y_as_scalar(scale));
		result.z_axis = vector_set_z(zero, vector_get_z_as_scalar(scale));

		return result;
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the rotation part of a VQM transform.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE quatd RTM_SIMD_CALL vqm_get_rotation(const vqmd& input) RTM_NO_EXCEPT
	{
		return input.rotation;
	}

	//////////////////////////////////////////////////////////////////////////
	// Sets the rotation part of a VQM and returns the new value.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vqmd RTM_SIMD_CALL vqm_set_rotation(const vqmd& qvm, quatd_arg0 rotation) RTM_NO_EXCEPT
	{
		vqmd result = qvm;
		result.rotation = rotation;
		return result;
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the translation part of a VQM transform.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4d RTM_SIMD_CALL vqm_get_translation(const vqmd& input) RTM_NO_EXCEPT
	{
		return input.translation;
	}

	//////////////////////////////////////////////////////////////////////////
	// Sets the translation part of a VQM and returns the new value.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vqmd RTM_SIMD_CALL vqm_set_translation(const vqmd& qvm, vector4d_arg0 translation) RTM_NO_EXCEPT
	{
		vqmd result = qvm;
		result.translation = translation;
		return result;
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the scale part of a VQM transform.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4d RTM_SIMD_CALL vqm_get_scale(const vqmd& input) RTM_NO_EXCEPT
	{
		vector4d xyxy = vector_mix<mix4::x, mix4::b, mix4::x, mix4::b>(input.x_axis, input.y_axis);
		return vector_mix<mix4::x, mix4::y, mix4::c, mix4::d>(xyxy, input.z_axis);
	}

	//////////////////////////////////////////////////////////////////////////
	// Sets the scale part of a VQM and returns the new value.
	// This preserves existing shear.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vqmd RTM_SIMD_CALL vqm_set_scale(const vqmd& qvm, vector4d_arg0 scale) RTM_NO_EXCEPT
	{
		vqmd result = qvm;
		result.x_axis = vector_set_x(qvm.x_axis, vector_get_x_as_scalar(scale));
		result.y_axis = vector_set_y(qvm.y_axis, vector_get_y_as_scalar(scale));
		result.z_axis = vector_set_z(qvm.z_axis, vector_get_z_as_scalar(scale));
		return result;
	}

	//////////////////////////////////////////////////////////////////////////
	// Adds two VQM transforms.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline vqmd RTM_SIMD_CALL vqm_add(const vqmd& lhs, const vqmd& rhs) RTM_NO_EXCEPT
	{
		// T2 + T1 = [v2, q2, M2] + [v1, q1, M1] = [v2 + v1, q2 + q1, M2 + M1]
		vqmd result;
		result.rotation = quat_add(lhs.rotation, rhs.rotation);
		result.translation = vector_add(lhs.translation, rhs.translation);
		result.x_axis = vector_add(lhs.x_axis, rhs.x_axis);
		result.y_axis = vector_add(lhs.y_axis, rhs.y_axis);
		result.z_axis = vector_add(lhs.z_axis, rhs.z_axis);

		return result;
	}

	//////////////////////////////////////////////////////////////////////////
	// Multiplies two VQM transforms.
	// Multiplication order is as follow: local_to_world = vqm_mul(local_to_object, object_to_world)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline vqmd RTM_SIMD_CALL vqm_mul(const vqmd& lhs, const vqmd& rhs) RTM_NO_EXCEPT
	{
		// T2 * T1 = [v2, q2, M2] * [v1, q1, M1] = [q2 * (M2 * v1) * q2^-1 + v2, q2 * q1, (q1^-1 * M2 * q1)(q1 * M1 * q1^-1)]

		const quatd inv_lhs_rotation = quat_conjugate(lhs.rotation);

		matrix3x3d lhs_scale_shear = matrix_set(lhs.x_axis, lhs.y_axis, lhs.z_axis);
		matrix3x3d rhs_scale_shear = matrix_set(rhs.x_axis, rhs.y_axis, rhs.z_axis);

		vqmd result;
		result.rotation = quat_mul(lhs.rotation, rhs.rotation);
		result.translation = vector_add(quat_mul_vector3(matrix_mul_vector3(lhs.translation, rhs_scale_shear), rhs.rotation), rhs.translation);

		rhs_scale_shear.x_axis = quat_mul_vector3(rhs_scale_shear.x_axis, inv_lhs_rotation);
		rhs_scale_shear.y_axis = quat_mul_vector3(rhs_scale_shear.y_axis, inv_lhs_rotation);
		rhs_scale_shear.z_axis = quat_mul_vector3(rhs_scale_shear.z_axis, inv_lhs_rotation);

		lhs_scale_shear.x_axis = quat_mul_vector3(lhs_scale_shear.x_axis, lhs.rotation);
		lhs_scale_shear.y_axis = quat_mul_vector3(lhs_scale_shear.y_axis, lhs.rotation);
		lhs_scale_shear.z_axis = quat_mul_vector3(lhs_scale_shear.z_axis, lhs.rotation);

		matrix3x3d scale_shear = matrix_mul(lhs_scale_shear, rhs_scale_shear);
		result.x_axis = scale_shear.x_axis;
		result.y_axis = scale_shear.y_axis;
		result.z_axis = scale_shear.z_axis;

		return result;
	}

	//////////////////////////////////////////////////////////////////////////
	// Multiplies a VQM transform with a scalar.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vqmd RTM_SIMD_CALL vqm_mul(const vqmd& vqm, double scalar) RTM_NO_EXCEPT
	{
		// s * T = s * [v, q, M] = [s * v, s * q, s * M]
		vector4d scalar_v = vector_set(scalar);

		vqmd result;
		result.rotation = quat_mul(vqm.rotation, scalar);
		result.translation = vector_mul(vqm.translation, scalar_v);
		result.x_axis = vector_mul(vqm.x_axis, scalar_v);
		result.y_axis = vector_mul(vqm.y_axis, scalar_v);
		result.z_axis = vector_mul(vqm.z_axis, scalar_v);

		return result;
	}

	//////////////////////////////////////////////////////////////////////////
	// Multiplies a VQM transform and a 3D point.
	// Multiplication order is as follow: world_position = vqm_mul_point3(local_position, local_to_world)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline vector4d RTM_SIMD_CALL vqm_mul_point3(vector4d_arg0 point, const vqmd& vqm) RTM_NO_EXCEPT
	{
		// T * p = [v, q, M] * p = (q * (M * p) * q^-1) + v

		const matrix3x3d scale_shear = matrix_set(vqm.x_axis, vqm.y_axis, vqm.z_axis);
		return vector_add(quat_mul_vector3(matrix_mul_vector3(point, scale_shear), vqm.rotation), vqm.translation);
	}

	//////////////////////////////////////////////////////////////////////////
	// Multiplies a VQM transform and a 3D vector.
	// Multiplication order is as follow: world_position = vqm_mul_point3(local_position, local_to_world)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline vector4d RTM_SIMD_CALL vqm_mul_vector3(vector4d_arg0 vec3, const vqmd& vqm) RTM_NO_EXCEPT
	{
		// T * vec3 = [v, q, M] * p = (q * (M * vec3) * q^-1)

		const matrix3x3d scale_shear = matrix_set(vqm.x_axis, vqm.y_axis, vqm.z_axis);
		return quat_mul_vector3(matrix_mul_vector3(vec3, scale_shear), vqm.rotation);
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the inverse of the input VQM transform.
	// If zero scale is contained, the result is undefined.
	// For a safe alternative, supply a fallback scale value and a threshold.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline vqmd RTM_SIMD_CALL vqm_inverse(const vqmd& input) RTM_NO_EXCEPT
	{
		// T^-1 = [v, q, M]^-1 = [M^-1 * (q^-1 * -v * q), q^-1, q * (q * M * q^-1)^-1 * q^-1]
		// Note that (q * M * q^-1)^-1 != (q^-1 * M^-1 * q)
		// However, let us convert that last part into matrix representation
		// q * (q * M * q^-1)^-1 * q^-1 = Mq * (Mq * M)^-1
		// Mq * (Mq * M)^-1 = (Mq * M^-1) * Mq^-1
		// (Mq * M^-1) * Mq^-1 = (q * M^-1 * q^-1) * Mq^-1
		// Unfortunately, we cannot convert the remaining Mq^-1 matrix back into a quaternion product
		// because it does not rotate anything (multiplication is on left side instead of right)
		// However, we can solve this by introducing the identity matrix
		// (q * M^-1 * q^-1) * (Mq^-1 * I) = (q * M^-1 * q^-1) * (q^-1 * I * q)
		// This is better because it allows us to compute a single matrix inverse as opposed to two

		const matrix3x3d scale_shear = matrix_set(input.x_axis, input.y_axis, input.z_axis);

		const matrix3x3d inv_scale_shear = matrix_inverse(scale_shear);
		const quatd inv_rotation = quat_conjugate(input.rotation);

		// Rotate the inverse scale/shear matrix
		matrix3x3d inv_rotated_scale_shear;
		inv_rotated_scale_shear.x_axis = quat_mul_vector3(inv_scale_shear.x_axis, input.rotation);
		inv_rotated_scale_shear.y_axis = quat_mul_vector3(inv_scale_shear.y_axis, input.rotation);
		inv_rotated_scale_shear.z_axis = quat_mul_vector3(inv_scale_shear.z_axis, input.rotation);

		// Build our inverse rotation matrix
		// TODO: We could build the matrix directly from the quaternion which is cheaper than rotating 3 axes, need to profile
		matrix3x3d inv_rotation_mtx = matrix_identity();
		inv_rotation_mtx.x_axis = quat_mul_vector3(inv_rotation_mtx.x_axis, inv_rotation);
		inv_rotation_mtx.y_axis = quat_mul_vector3(inv_rotation_mtx.y_axis, inv_rotation);
		inv_rotation_mtx.z_axis = quat_mul_vector3(inv_rotation_mtx.z_axis, inv_rotation);

		// Multiply our two matrices
		inv_rotated_scale_shear = matrix_mul(inv_rotation_mtx, inv_rotated_scale_shear);

		vqmd result;
		result.rotation = inv_rotation;
		result.x_axis = inv_rotated_scale_shear.x_axis;
		result.y_axis = inv_rotated_scale_shear.y_axis;
		result.z_axis = inv_rotated_scale_shear.z_axis;
		result.translation = matrix_mul_vector3(quat_mul_vector3(vector_neg(input.translation), inv_rotation), inv_scale_shear);

		return result;
	}

	//////////////////////////////////////////////////////////////////////////
	// Converts a VQM transform into a 3x4 affine matrix.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline matrix3x4d RTM_SIMD_CALL vqm_to_matrix(const vqmd& input) RTM_NO_EXCEPT
	{
		matrix3x4d result = matrix_set(input.x_axis, input.y_axis, input.z_axis, input.translation);

		result.x_axis = quat_mul_vector3(result.x_axis, input.rotation);
		result.y_axis = quat_mul_vector3(result.y_axis, input.rotation);
		result.z_axis = quat_mul_vector3(result.z_axis, input.rotation);

		return result;
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns a VQM transforms with the rotation part normalized.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vqmd RTM_SIMD_CALL qvv_normalize(const vqmd& input) RTM_NO_EXCEPT
	{
		vqmd result;
		result.rotation = quat_normalize(input.rotation);
		result.x_axis = input.x_axis;
		result.y_axis = input.y_axis;
		result.z_axis = input.z_axis;
		result.translation = input.translation;

		return result;
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if the input VQM does not contain any NaN or Inf, otherwise false.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL qvv_is_finite(const vqmd& input) RTM_NO_EXCEPT
	{
		return quat_is_finite(input.rotation)
			&& vector_is_finite3(input.translation)
			&& vector_is_finite3(input.x_axis)
			&& vector_is_finite3(input.y_axis)
			&& vector_is_finite3(input.z_axis);
	}

	RTM_IMPL_VERSION_NAMESPACE_END
}

RTM_IMPL_FILE_PRAGMA_POP

```

`includes/rtm/experimental/vqmf.h`:

```h
#pragma once

////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2024 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include "rtm/math.h"
#include "rtm/matrix3x3f.h"
#include "rtm/quatf.h"
#include "rtm/vector4f.h"
#include "rtm/version.h"
#include "rtm/impl/compiler_utils.h"
#include "rtm/experimental/impl/vqm_common.h"

RTM_IMPL_FILE_PRAGMA_PUSH

namespace rtm
{
	RTM_IMPL_VERSION_NAMESPACE_BEGIN

	//////////////////////////////////////////////////////////////////////////
	// See here for details: The VQM-Group and its Applications
	//                       By Michael Aristidou and Xin Li
	//
	// Key insights:
	// - We wish to maintain rotation, translation, and scale/shear separately as they do not
	//   interpolate the same way. In particular, scale/shear and rotation mix together in the
	//   upper 3x3 part of affine matrices and it is difficult to manage them correctly as there
	//   is no unique way to decompose them.
	// - Let us define [T1, R1, S1] and [T2, R2, S2] as two transforms with 3 affine matrices each
	//   We will ignore translation since it mostly lives in its own dimension (4th) and does not
	//   interfere with scale/shear/rotation.
	// - We define multiplication as follow:
	//   (R2 * S2) * (R1 * S1) = (R3 * S3)
	//   By construction, we wish R3 = R2 * R1, we substitute
	//   (R2 * S2) * (R1 * S1) = (R2 * R1 * S3)
	//   We solve for S3, which is the scale/shear matrix we are looking for
	//   (R2^-1 * R2) * S2 * (R1 * S1) = R1 * S3
	//   R1^-1 * (R2^-1 * R2) * S2 * (R1 * S1) = S3
	//   R2^-1 * R2 cancel out and we get
	//   R1^-1 * S2 * R1 * S1 = S3
	//   In plain english, to compute our scale/shear matrix, we rotate S1 into the space of S2
	//   by multiplying with R1, then we scale/shear the result, and return into the space of S1
	//   by applying the inverse R1 rotation. A sensible result.
	// - This is all well and good with matrices, but we wish to retain rotation as a quaternion
	//   for its numerical stability, compact nature, and superior interpolation. How do we multiply
	//   a matrix with a quaternion?
	// - A key insight is that if we apply a rotation matrix onto any other affine matrix (e.g. a
	//   scale/shear or other pure rotation matrix), what occurs under the matrix multiplication is
	//   that each column of the affine matrix is rotated by our rotation matrix. This is something
	//   we can easily achieve as well with a quaternion: by using the sandwich product. From this
	//   key insight, the various identities in the paper follow as matrix multiplication with a pure
	//   rotation matrix is equivalent to the sandwich product of each column of the other matrix.
	//
	// Some VQM identities:
	// - If we treat M as a homogenous quaternion matrix, q a quaternion, and r a pure quaternion,
	//   then: q * (M * r) * q^-1 = (q * M * q^-1) * r
	//   In plain english, applying scale/shear to a point and rotating that point is equivalent to
	//   rotating the scale/shear matrix and applying the result to that point.
	//
	// - If M and N are homogenous quaternion matrices, and q1 and q2 quaternions then:
	//   (q2 * N * q2^-1) * (q1 * M * q1^-1) = q2 * (N * (q1 * M * q1^-1)) * q2^-1
	//   In plain english, the product of two rotated scale/shear matrices is equivalent to the
	//   rotated product of one scale/shear matrix with another rotated scale/shear matrix meaning
	//   rotation can occur before the multiplication or after due to assossiativity. This is
	//   straightforward to see if the rotations are expressed in matrix form.
	//////////////////////////////////////////////////////////////////////////

	//////////////////////////////////////////////////////////////////////////
	// A VQM transform represents a 3D rotation (quaternion), 3D translation (vector3),
	// and 3D non-uniform scale and shear (matrix 3x3).
	// VQM forms a group with a well defined multiplication and inverse. Its
	// multiplication is assossiative but not commutative (like quaternions/matrices).
	// Rotations are assumed to represent a single turn (normalized quaternion).
	//////////////////////////////////////////////////////////////////////////
#if 0	// defined in types.h
	struct vqmf
	{
		// The internal format is meant to be opaque, use vqm_* accessors
		quatf		rotation;
		vector4f	translation;
		vector4f	x_axis;
		vector4f	y_axis;
		vector4f	z_axis;
	};
#endif

	template<> struct related_types<vqmf> : related_types<float> {};

	//////////////////////////////////////////////////////////////////////////
	// Creates a VQM transform from a rotation quaternion, a translation, and a 3D scale.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vqmf RTM_SIMD_CALL vqm_set(vector4f_arg0 translation, quatf_arg1 rotation, vector4f_arg2 scale) RTM_NO_EXCEPT
	{
		vector4f zero = vector_zero();

		vqmf result;
		result.rotation = rotation;
		result.translation = translation;
		result.x_axis = vector_set_x(zero, vector_get_x_as_scalar(scale));
		result.y_axis = vector_set_y(zero, vector_get_y_as_scalar(scale));
		result.z_axis = vector_set_z(zero, vector_get_z_as_scalar(scale));

		return result;
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the rotation part of a VQM transform.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE quatf RTM_SIMD_CALL vqm_get_rotation(const vqmf& input) RTM_NO_EXCEPT
	{
		return input.rotation;
	}

	//////////////////////////////////////////////////////////////////////////
	// Sets the rotation part of a VQM and returns the new value.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vqmf RTM_SIMD_CALL vqm_set_rotation(const vqmf& qvm, quatf_arg0 rotation) RTM_NO_EXCEPT
	{
		vqmf result = qvm;
		result.rotation = rotation;
		return result;
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the translation part of a VQM transform.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4f RTM_SIMD_CALL vqm_get_translation(const vqmf& input) RTM_NO_EXCEPT
	{
		return input.translation;
	}

	//////////////////////////////////////////////////////////////////////////
	// Sets the translation part of a VQM and returns the new value.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vqmf RTM_SIMD_CALL vqm_set_translation(const vqmf& qvm, vector4f_arg0 translation) RTM_NO_EXCEPT
	{
		vqmf result = qvm;
		result.translation = translation;
		return result;
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the scale part of a VQM transform.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4f RTM_SIMD_CALL vqm_get_scale(const vqmf& input) RTM_NO_EXCEPT
	{
		vector4f xyxy = vector_mix<mix4::x, mix4::b, mix4::x, mix4::b>(input.x_axis, input.y_axis);
		return vector_mix<mix4::x, mix4::y, mix4::c, mix4::d>(xyxy, input.z_axis);
	}

	//////////////////////////////////////////////////////////////////////////
	// Sets the scale part of a VQM and returns the new value.
	// This preserves existing shear.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vqmf RTM_SIMD_CALL vqm_set_scale(const vqmf& qvm, vector4f_arg0 scale) RTM_NO_EXCEPT
	{
		vqmf result = qvm;
		result.x_axis = vector_set_x(qvm.x_axis, vector_get_x_as_scalar(scale));
		result.y_axis = vector_set_y(qvm.y_axis, vector_get_y_as_scalar(scale));
		result.z_axis = vector_set_z(qvm.z_axis, vector_get_z_as_scalar(scale));
		return result;
	}

	//////////////////////////////////////////////////////////////////////////
	// Adds two VQM transforms.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vqmf RTM_SIMD_CALL vqm_add(const vqmf& lhs, const vqmf& rhs) RTM_NO_EXCEPT
	{
		// T2 + T1 = [v2, q2, M2] + [v1, q1, M1] = [v2 + v1, q2 + q1, M2 + M1]
		vqmf result;
		result.rotation = quat_add(lhs.rotation, rhs.rotation);
		result.translation = vector_add(lhs.translation, rhs.translation);
		result.x_axis = vector_add(lhs.x_axis, rhs.x_axis);
		result.y_axis = vector_add(lhs.y_axis, rhs.y_axis);
		result.z_axis = vector_add(lhs.z_axis, rhs.z_axis);

		return result;
	}

	//////////////////////////////////////////////////////////////////////////
	// Multiplies two VQM transforms.
	// Multiplication order is as follow: local_to_world = vqm_mul(local_to_object, object_to_world)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline vqmf RTM_SIMD_CALL vqm_mul(const vqmf& lhs, const vqmf& rhs) RTM_NO_EXCEPT
	{
		// T2 * T1 = [v2, q2, M2] * [v1, q1, M1] = [q2 * (M2 * v1) * q2^-1 + v2, q2 * q1, (q1^-1 * M2 * q1)(q1 * M1 * q1^-1)]

		const quatf inv_lhs_rotation = quat_conjugate(lhs.rotation);

		matrix3x3f lhs_scale_shear = matrix_set(lhs.x_axis, lhs.y_axis, lhs.z_axis);
		matrix3x3f rhs_scale_shear = matrix_set(rhs.x_axis, rhs.y_axis, rhs.z_axis);

		vqmf result;
		result.rotation = quat_mul(lhs.rotation, rhs.rotation);
		result.translation = vector_add(quat_mul_vector3(matrix_mul_vector3(lhs.translation, rhs_scale_shear), rhs.rotation), rhs.translation);

		rhs_scale_shear.x_axis = quat_mul_vector3(rhs_scale_shear.x_axis, inv_lhs_rotation);
		rhs_scale_shear.y_axis = quat_mul_vector3(rhs_scale_shear.y_axis, inv_lhs_rotation);
		rhs_scale_shear.z_axis = quat_mul_vector3(rhs_scale_shear.z_axis, inv_lhs_rotation);

		lhs_scale_shear.x_axis = quat_mul_vector3(lhs_scale_shear.x_axis, lhs.rotation);
		lhs_scale_shear.y_axis = quat_mul_vector3(lhs_scale_shear.y_axis, lhs.rotation);
		lhs_scale_shear.z_axis = quat_mul_vector3(lhs_scale_shear.z_axis, lhs.rotation);

		matrix3x3f scale_shear = matrix_mul(lhs_scale_shear, rhs_scale_shear);
		result.x_axis = scale_shear.x_axis;
		result.y_axis = scale_shear.y_axis;
		result.z_axis = scale_shear.z_axis;

		return result;
	}

	//////////////////////////////////////////////////////////////////////////
	// Multiplies a VQM transform with a scalar.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vqmf RTM_SIMD_CALL vqm_mul(const vqmf& vqm, float scalar) RTM_NO_EXCEPT
	{
		// s * T = s * [v, q, M] = [s * v, s * q, s * M]
		vector4f scalar_v = vector_set(scalar);

		vqmf result;
		result.rotation = quat_mul(vqm.rotation, scalar);
		result.translation = vector_mul(vqm.translation, scalar_v);
		result.x_axis = vector_mul(vqm.x_axis, scalar_v);
		result.y_axis = vector_mul(vqm.y_axis, scalar_v);
		result.z_axis = vector_mul(vqm.z_axis, scalar_v);

		return result;
	}

	//////////////////////////////////////////////////////////////////////////
	// Multiplies a VQM transform and a 3D point.
	// Multiplication order is as follow: world_position = vqm_mul_point3(local_position, local_to_world)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline vector4f RTM_SIMD_CALL vqm_mul_point3(vector4f_arg0 point, const vqmf& vqm) RTM_NO_EXCEPT
	{
		// T * p = [v, q, M] * p = (q * (M * p) * q^-1) + v

		const matrix3x3f scale_shear = matrix_set(vqm.x_axis, vqm.y_axis, vqm.z_axis);
		return vector_add(quat_mul_vector3(matrix_mul_vector3(point, scale_shear), vqm.rotation), vqm.translation);
	}

	//////////////////////////////////////////////////////////////////////////
	// Multiplies a VQM transform and a 3D vector.
	// Multiplication order is as follow: world_position = vqm_mul_point3(local_position, local_to_world)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline vector4f RTM_SIMD_CALL vqm_mul_vector3(vector4f_arg0 vec3, const vqmf& vqm) RTM_NO_EXCEPT
	{
		// T * vec3 = [v, q, M] * p = (q * (M * vec3) * q^-1)

		const matrix3x3f scale_shear = matrix_set(vqm.x_axis, vqm.y_axis, vqm.z_axis);
		return quat_mul_vector3(matrix_mul_vector3(vec3, scale_shear), vqm.rotation);
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the inverse of the input VQM transform.
	// If zero scale is contained, the result is undefined.
	// For a safe alternative, supply a fallback scale value and a threshold.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline vqmf RTM_SIMD_CALL vqm_inverse(const vqmf& input) RTM_NO_EXCEPT
	{
		// T^-1 = [v, q, M]^-1 = [M^-1 * (q^-1 * -v * q), q^-1, q * (q * M * q^-1)^-1 * q^-1]
		// Note that (q * M * q^-1)^-1 != (q^-1 * M^-1 * q)
		// However, let us convert that last part into matrix representation
		// q * (q * M * q^-1)^-1 * q^-1 = Mq * (Mq * M)^-1
		// Mq * (Mq * M)^-1 = (Mq * M^-1) * Mq^-1
		// (Mq * M^-1) * Mq^-1 = (q * M^-1 * q^-1) * Mq^-1
		// Unfortunately, we cannot convert the remaining Mq^-1 matrix back into a quaternion product
		// because it does not rotate anything (multiplication is on left side instead of right)
		// However, we can solve this by introducing the identity matrix
		// (q * M^-1 * q^-1) * (Mq^-1 * I) = (q * M^-1 * q^-1) * (q^-1 * I * q)
		// This is better because it allows us to compute a single matrix inverse as opposed to two

		const matrix3x3f scale_shear = matrix_set(input.x_axis, input.y_axis, input.z_axis);

		const matrix3x3f inv_scale_shear = matrix_inverse(scale_shear);
		const quatf inv_rotation = quat_conjugate(input.rotation);

		// Rotate the inverse scale/shear matrix
		matrix3x3f inv_rotated_scale_shear;
		inv_rotated_scale_shear.x_axis = quat_mul_vector3(inv_scale_shear.x_axis, input.rotation);
		inv_rotated_scale_shear.y_axis = quat_mul_vector3(inv_scale_shear.y_axis, input.rotation);
		inv_rotated_scale_shear.z_axis = quat_mul_vector3(inv_scale_shear.z_axis, input.rotation);

		// Build our inverse rotation matrix
		// TODO: We could build the matrix directly from the quaternion which is cheaper than rotating 3 axes, need to profile
		matrix3x3f inv_rotation_mtx = matrix_identity();
		inv_rotation_mtx.x_axis = quat_mul_vector3(inv_rotation_mtx.x_axis, inv_rotation);
		inv_rotation_mtx.y_axis = quat_mul_vector3(inv_rotation_mtx.y_axis, inv_rotation);
		inv_rotation_mtx.z_axis = quat_mul_vector3(inv_rotation_mtx.z_axis, inv_rotation);

		// Multiply our two matrices
		inv_rotated_scale_shear = matrix_mul(inv_rotation_mtx, inv_rotated_scale_shear);

		vqmf result;
		result.rotation = inv_rotation;
		result.x_axis = inv_rotated_scale_shear.x_axis;
		result.y_axis = inv_rotated_scale_shear.y_axis;
		result.z_axis = inv_rotated_scale_shear.z_axis;
		result.translation = matrix_mul_vector3(quat_mul_vector3(vector_neg(input.translation), inv_rotation), inv_scale_shear);

		return result;
	}

	//////////////////////////////////////////////////////////////////////////
	// Converts a VQM transform into a 3x4 affine matrix.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline matrix3x4f RTM_SIMD_CALL vqm_to_matrix(const vqmf& input) RTM_NO_EXCEPT
	{
		matrix3x4f result = matrix_set(input.x_axis, input.y_axis, input.z_axis, input.translation);

		result.x_axis = quat_mul_vector3(result.x_axis, input.rotation);
		result.y_axis = quat_mul_vector3(result.y_axis, input.rotation);
		result.z_axis = quat_mul_vector3(result.z_axis, input.rotation);

		return result;
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns a VQM transforms with the rotation part normalized.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vqmf RTM_SIMD_CALL qvv_normalize(const vqmf& input) RTM_NO_EXCEPT
	{
		vqmf result;
		result.rotation = quat_normalize(input.rotation);
		result.x_axis = input.x_axis;
		result.y_axis = input.y_axis;
		result.z_axis = input.z_axis;
		result.translation = input.translation;

		return result;
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if the input VQM does not contain any NaN or Inf, otherwise false.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL qvv_is_finite(const vqmf& input) RTM_NO_EXCEPT
	{
		return quat_is_finite(input.rotation)
			&& vector_is_finite3(input.translation)
			&& vector_is_finite3(input.x_axis)
			&& vector_is_finite3(input.y_axis)
			&& vector_is_finite3(input.z_axis);
	}

	RTM_IMPL_VERSION_NAMESPACE_END
}

RTM_IMPL_FILE_PRAGMA_POP

```

`includes/rtm/fwd.h`:

```h
#pragma once

////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2022 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include "rtm/version.h"

////////////////////////////////////////////////////////////////////////////////
// This header provides forward declarations for all public Realtime Math types.
// Forward declaring symbols from a 3rd party library is a bad idea, use this
// header instead.
// See also: https://blog.libtorrent.org/2017/12/forward-declarations-and-abi/
////////////////////////////////////////////////////////////////////////////////

// For now, just include the types header since it is already very compact and lightweight
#include "rtm/types.h"

```

`includes/rtm/impl/bit_cast.impl.h`:

```h
#pragma once

////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2023 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include "rtm/config.h"
#include "rtm/version.h"
#include "rtm/impl/compiler_utils.h"
#include "rtm/impl/detect_cpp_version.h"

// See config.h for details on how to configure std::bit_cast for your project

// Use RTM_NO_BIT_CAST to disable std::bit_cast
#if RTM_CPP_VERSION >= RTM_CPP_VERSION_20 && !defined(RTM_NO_BIT_CAST)
	#include <bit>
#endif

RTM_IMPL_FILE_PRAGMA_PUSH

namespace rtm
{
	RTM_IMPL_VERSION_NAMESPACE_BEGIN

	namespace rtm_impl
	{
		//////////////////////////////////////////////////////////////////////////
		// C++20 introduced std::bit_cast which is safer than reinterpret_cast
		//////////////////////////////////////////////////////////////////////////

	#if RTM_CPP_VERSION >= RTM_CPP_VERSION_20 && !defined(RTM_NO_BIT_CAST)
		using std::bit_cast;
	#else
		template<class dest_type_t, class src_type_t>
		RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr dest_type_t bit_cast(src_type_t input) noexcept
		{
			return reinterpret_cast<dest_type_t>(input);
		}
	#endif
	}

	RTM_IMPL_VERSION_NAMESPACE_END
}

RTM_IMPL_FILE_PRAGMA_POP

```

`includes/rtm/impl/cmath.impl.h`:

```h
#pragma once

////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2021 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include "rtm/version.h"
#include "rtm/impl/compiler_utils.h"

//////////////////////////////////////////////////////////////////////////
// Include cmath header and polyfill what is missing for proper C++11 support
//////////////////////////////////////////////////////////////////////////
#include <cmath>

RTM_IMPL_FILE_PRAGMA_PUSH

namespace rtm
{
	RTM_IMPL_VERSION_NAMESPACE_BEGIN

	namespace rtm_impl
	{
		//////////////////////////////////////////////////////////////////////////
		// The version of the STL shipped with versions of GCC older than 5.1 are missing a number of type traits and functions,
		// such as std::is_trivially_default_constructible for proper C++11 support.
		// We thus manually polyfill what is missing.
		// This must also be done when the compiler is clang when it makes use of the GCC implementation of the STL,
		// which is the default behavior on linux. Properly detecting the version of the GCC STL used by clang cannot
		// be done with the __GNUC__  macro, which is overridden by clang. Instead, we check for the definition
		// of the macro _GLIBCXX_USE_CXX11_ABI which is only defined with GCC versions greater than 5.
		// _GLIBCXX_USE_C99_MATH_TR1 is sometimes undefined when some C++11 functions are unimplemented which strips most of them out.
		// This is the case for various 32 bit platforms (e.g. ARMv7)
		//////////////////////////////////////////////////////////////////////////

#if defined(__GNUG__) && !defined(_LIBCPP_VERSION) && !defined(_GLIBCXX_USE_CXX11_ABI) && !defined(_GLIBCXX_USE_C99_MATH_TR1)
		inline float copysign(float x, float y) { return ::copysignf(x, y); }
		inline double copysign(double x, double y) { return ::copysign(x, y); }
#else
		inline float copysign(float x, float y) { return std::copysign(x, y); }
		inline double copysign(double x, double y) { return std::copysign(x, y); }
#endif
	}

	RTM_IMPL_VERSION_NAMESPACE_END
}

RTM_IMPL_FILE_PRAGMA_POP

```

`includes/rtm/impl/compiler_utils.h`:

```h
#pragma once

////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2019 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include "rtm/impl/detect_compiler.h"

//////////////////////////////////////////////////////////////////////////
// Because this library is made entirely of headers, we have no control over the
// compilation flags used. However, in some cases, certain options must be forced.
// To do this, every header is wrapped in two macros to push and pop the necessary
// pragmas.
//
// Options we use:
//    - Disable fast math, it can hurt precision for little to no performance gain due to the high level of hand tuned optimizations.
//////////////////////////////////////////////////////////////////////////
#if defined(RTM_COMPILER_MSVC)
	#define RTM_IMPL_FILE_PRAGMA_PUSH \
		__pragma(float_control(precise, on, push))

	#define RTM_IMPL_FILE_PRAGMA_POP \
		__pragma(float_control(pop))
#elif defined(RTM_COMPILER_CLANG) && 0
	// For some reason, clang doesn't appear to support disabling fast-math through pragmas
	// See: https://github.com/llvm/llvm-project/issues/55392
	#define RTM_IMPL_FILE_PRAGMA_PUSH \
		_Pragma("float_control(precise, on, push)")

	#define RTM_IMPL_FILE_PRAGMA_POP \
		_Pragma("float_control(pop)")
#elif defined(RTM_COMPILER_GCC)
	#define RTM_IMPL_FILE_PRAGMA_PUSH \
		_Pragma("GCC push_options") \
		_Pragma("GCC optimize (\"no-fast-math\")")

	#define RTM_IMPL_FILE_PRAGMA_POP \
		_Pragma("GCC pop_options")
#else
	#define RTM_IMPL_FILE_PRAGMA_PUSH
	#define RTM_IMPL_FILE_PRAGMA_POP
#endif

//////////////////////////////////////////////////////////////////////////
// In some cases, for performance reasons, we wish to disable stack security
// check cookies. This macro serves this purpose.
//////////////////////////////////////////////////////////////////////////
#if defined(RTM_COMPILER_MSVC)
	#define RTM_DISABLE_SECURITY_COOKIE_CHECK __declspec(safebuffers)
#else
	#define RTM_DISABLE_SECURITY_COOKIE_CHECK
#endif

//////////////////////////////////////////////////////////////////////////
// Force inline macros for when it is necessary.
//////////////////////////////////////////////////////////////////////////
#if defined(RTM_COMPILER_MSVC)
	#define RTM_FORCE_INLINE __forceinline
#elif defined(RTM_COMPILER_GCC) || defined(RTM_COMPILER_CLANG)
	#define RTM_FORCE_INLINE __attribute__((always_inline)) inline
#else
	#define RTM_FORCE_INLINE inline
#endif

//////////////////////////////////////////////////////////////////////////
// Force no-inline macros for when it is necessary.
//////////////////////////////////////////////////////////////////////////
#if defined(RTM_COMPILER_MSVC)
	#define RTM_FORCE_NOINLINE __declspec(noinline)
#elif defined(RTM_COMPILER_GCC) || defined(RTM_COMPILER_CLANG)
	#define RTM_FORCE_NOINLINE __attribute__((noinline))
#else
	#define RTM_FORCE_NOINLINE
#endif

//////////////////////////////////////////////////////////////////////////
// Joins two pre-processor tokens: RTM_JOIN_TOKENS(foo, bar) yields 'foobar'
//////////////////////////////////////////////////////////////////////////
#define RTM_JOIN_TOKENS(a, b) a ## b

//////////////////////////////////////////////////////////////////////////
// Wraps the __has_builtin pre-processor macro to handle non-clang and early
// GCC compilers
//////////////////////////////////////////////////////////////////////////
#if defined(__has_builtin)
	#define RTM_HAS_BUILTIN(x) __has_builtin(x)
#else
	#define RTM_HAS_BUILTIN(x) 0
#endif

```

`includes/rtm/impl/detect_arch.h`:

```h
#pragma once

////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2021 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

//////////////////////////////////////////////////////////////////////////
// Macro to identify our platform architecture
//////////////////////////////////////////////////////////////////////////

// Test ARM64 first because ARM64EC also defines _M_X64, it's a hybrid architecture
// used by MSVC to facilitate SIMD porting to ARM platforms
#if defined(_M_ARM64) || defined(__aarch64__) || defined(_M_ARM64EC)
	#define RTM_ARCH_ARM64
#elif defined(_M_ARM) || defined(__ARM_NEON)
	#define RTM_ARCH_ARM
#elif defined(_M_X64) || defined(__x86_64__)
	#define RTM_ARCH_X64
#elif defined(_M_IX86) || defined(__i386__)
	#define RTM_ARCH_X86
#endif

```

`includes/rtm/impl/detect_compiler.h`:

```h
#pragma once

////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2021 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

//////////////////////////////////////////////////////////////////////////
// Macro to identify individual compilers
//////////////////////////////////////////////////////////////////////////
#if defined(__GNUG__) && !defined(__clang__)
	#define RTM_COMPILER_GCC
#elif defined(__clang__)
	#define RTM_COMPILER_CLANG		__clang_major__
#elif defined(_MSC_VER) && !defined(__clang__)
	#define RTM_COMPILER_MSVC		_MSC_VER
	#define RTM_COMPILER_MSVC_2013	1800
	#define RTM_COMPILER_MSVC_2015	1900
	#define RTM_COMPILER_MSVC_2017	1910
	#define RTM_COMPILER_MSVC_2019	1920
	#define RTM_COMPILER_MSVC_2022	1930

	#if RTM_COMPILER_MSVC < RTM_COMPILER_MSVC_2015
		#pragma message("Warning: This version of visual studio isn't officially supported")
	#endif
#endif

```

`includes/rtm/impl/detect_cpp_version.h`:

```h
#pragma once

////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2023 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

//////////////////////////////////////////////////////////////////////////
// Macros to identify individual C++ versions
//////////////////////////////////////////////////////////////////////////
#if defined(_MSVC_LANG)
	// Test MSVC first since it used to define __cplusplus to some old version by default

	#define RTM_CPP_VERSION _MSVC_LANG

	#define RTM_CPP_VERSION_14 201402L
	#define RTM_CPP_VERSION_17 201703L
	#define RTM_CPP_VERSION_20 202002L
#elif defined(__cplusplus)
	#define RTM_CPP_VERSION __cplusplus

	#define RTM_CPP_VERSION_14 201402L
	#define RTM_CPP_VERSION_17 201703L
	#define RTM_CPP_VERSION_20 202002L
#else
	// C++ version is unknown or older than C++14
	// Use a made up value to denote C++11
	#define RTM_CPP_VERSION 201100L

	#define RTM_CPP_VERSION_14 201402L
	#define RTM_CPP_VERSION_17 201703L
	#define RTM_CPP_VERSION_20 202002L
#endif

```

`includes/rtm/impl/detect_features.h`:

```h
#pragma once

////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2021 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include "rtm/impl/detect_arch.h"
#include "rtm/impl/detect_compiler.h"

//////////////////////////////////////////////////////////////////////////
// Helper macro to determine if vrndns_f32 is supported (ARM64 only)
//////////////////////////////////////////////////////////////////////////
#if defined(RTM_ARCH_ARM64) && !defined(RTM_IMPL_VRNDNS_SUPPORTED)
	// ARM documentation states __ARM_FEATURE_DIRECTED_ROUNDING must be defined
	#if defined(__ARM_FEATURE_DIRECTED_ROUNDING)
		// Only support it with clang for now
		#if defined(RTM_COMPILER_CLANG)
			// Apple redefines __clang_major__ to match the XCode version
			#if defined(__APPLE__)
				#if __clang_major__ >= 10
					// Apple clang supports it starting with XCode 10
					#define RTM_IMPL_VRNDNS_SUPPORTED
				#endif
			#else
				#if __clang_major__ >= 6
					// Ordinary clang supports it starting with clang 6
					#define RTM_IMPL_VRNDNS_SUPPORTED
				#endif
			#endif
		#endif
	#endif

	// MSVC doesn't appear to define __ARM_FEATURE_DIRECTED_ROUNDING but it supports the
	// intrinsic as of VS2019
	// MSVC uses defines for the ARM intrinsics, use them to perform feature detection
	#if !defined(RTM_IMPL_VRNDNS_SUPPORTED) && defined(RTM_COMPILER_MSVC) && defined(vrndns_f32)
		#define RTM_IMPL_VRNDNS_SUPPORTED
	#endif
#endif

//////////////////////////////////////////////////////////////////////////
// Helper macro to determine if the vca* (e.g vcagtq_f32) family of intrinsics are supported (ARM64 only)
//////////////////////////////////////////////////////////////////////////
#if defined(RTM_ARCH_ARM64) && !defined(RTM_IMPL_VCA_SUPPORTED)
	#if defined(RTM_COMPILER_MSVC)
		#if defined(vcaleq_f32)
			// Support was introduced in VS2019
			// MSVC uses defines for the ARM intrinsics, use them to perform feature detection
			#define RTM_IMPL_VCA_SUPPORTED
		#endif
	#else
		// Always enable with GCC and clang for now
		#define RTM_IMPL_VCA_SUPPORTED
	#endif
#endif

//////////////////////////////////////////////////////////////////////////
// Helper macro to determine if the vc*z* (e.g vceqq_f32) family of intrinsics are supported (ARM64 only)
//////////////////////////////////////////////////////////////////////////
#if defined(RTM_ARCH_ARM64) && !defined(RTM_IMPL_VCZ_SUPPORTED)
	#if defined(RTM_COMPILER_MSVC)
		#if defined(vceqzq_f32)
			// Support was introduced in VS2019
			// MSVC uses defines for the ARM intrinsics, use them to perform feature detection
			#define RTM_IMPL_VCZ_SUPPORTED
		#endif
	#else
		// Always enable with GCC and clang for now
		#define RTM_IMPL_VCZ_SUPPORTED
	#endif
#endif

//////////////////////////////////////////////////////////////////////////
// Helper macro to determine if the vsqrtq_f32 intrinsic is supported (ARM64 only)
//////////////////////////////////////////////////////////////////////////
#if defined(RTM_ARCH_ARM64) && !defined(RTM_IMPL_VSQRT_SUPPORTED)
	#if defined(RTM_COMPILER_MSVC)
		#if defined(vsqrtq_f32)
			// Support was introduced in VS2019
			// MSVC uses defines for the ARM intrinsics, use them to perform feature detection
			#define RTM_IMPL_VSQRT_SUPPORTED
		#endif
	#else
		// Always enable with GCC and clang for now
		#define RTM_IMPL_VSQRT_SUPPORTED
	#endif
#endif

//////////////////////////////////////////////////////////////////////////
// Helper macro to determine if vfmss_laneq_f32 is supported (ARM64 only)
//////////////////////////////////////////////////////////////////////////
#if defined(RTM_ARCH_ARM64) && !defined(RTM_IMPL_VFMSS_SUPPORTED)
	#if defined(RTM_COMPILER_MSVC)
		#if defined(vfmss_laneq_f32)
			// Support was introduced in VS2019
			// MSVC uses defines for the ARM intrinsics, use them to perform feature detection
			#define RTM_IMPL_VFMSS_SUPPORTED
		#endif
	#else
		// Always enable with GCC and clang for now
		#define RTM_IMPL_VFMSS_SUPPORTED
	#endif
#endif

//////////////////////////////////////////////////////////////////////////
// Helper macro to determine if vaddvq_f32 is supported (ARM64 only)
//////////////////////////////////////////////////////////////////////////
#if defined(RTM_ARCH_ARM64) && !defined(RTM_IMPL_VADDVQ_SUPPORTED)
	#if defined(RTM_COMPILER_MSVC)
		#if defined(vaddvq_f32)
			// Support was introduced in VS2019
			// MSVC uses defines for the ARM intrinsics, use them to perform feature detection
			#define RTM_IMPL_VADDVQ_SUPPORTED
		#endif
	#else
		// Always enable with GCC and clang for now
		#define RTM_IMPL_VADDVQ_SUPPORTED
	#endif
#endif

```

`includes/rtm/impl/error.h`:

```h
#pragma once

////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2017 Nicholas Frechette & Animation Compression Library contributors
// Copyright (c) 2018 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include "rtm/config.h"
#include "rtm/version.h"
#include "rtm/impl/compiler_utils.h"
#include "rtm/impl/detect_compiler.h"
#include "rtm/impl/detect_cpp_version.h"

RTM_IMPL_FILE_PRAGMA_PUSH

// See config.h for details on how to configure asserts for your project

#if defined(RTM_ON_ASSERT_ABORT)

	#include <cstdio>
	#include <cstdarg>
	#include <cstdlib>

	namespace rtm
	{
		RTM_IMPL_VERSION_NAMESPACE_BEGIN

		namespace rtm_impl
		{
			[[noreturn]] RTM_DISABLE_SECURITY_COOKIE_CHECK inline void on_assert_abort(const char* expression, int line, const char* file, const char* format, ...)
			{
				(void)expression;
				(void)line;
				(void)file;

				va_list args;
				va_start(args, format);

				std::vfprintf(stderr, format, args);
				std::fprintf(stderr, "\n");

				va_end(args);

				std::abort();
			}
		}

		RTM_IMPL_VERSION_NAMESPACE_END
	}

	#define RTM_ASSERT(expression, format, ...) do { if (!(expression)) RTM_IMPL_NAMESPACE::rtm_impl::on_assert_abort(#expression, __LINE__, __FILE__, (format), ## __VA_ARGS__); } while(false)
	#define RTM_HAS_ASSERT_CHECKS
	#define RTM_NO_EXCEPT noexcept

#elif defined(RTM_ON_ASSERT_THROW)

	#include <cstdio>
	#include <cstdarg>
	#include <string>
	#include <stdexcept>

	namespace rtm
	{
		RTM_IMPL_VERSION_NAMESPACE_BEGIN

		class runtime_assert final : public std::runtime_error
		{
			runtime_assert() = delete;					// Default constructor not needed

			using std::runtime_error::runtime_error;	// Inherit constructors
		};

		namespace rtm_impl
		{
			[[noreturn]] RTM_DISABLE_SECURITY_COOKIE_CHECK inline void on_assert_throw(const char* expression, int line, const char* file, const char* format, ...)
			{
				(void)expression;
				(void)line;
				(void)file;

				constexpr int buffer_size = 1 * 1024;
				char buffer[buffer_size];

				va_list args;
				va_start(args, format);

				const int count = vsnprintf(buffer, buffer_size, format, args);

				va_end(args);

				if (count >= 0 && count < buffer_size)
					throw runtime_assert(std::string(&buffer[0], static_cast<std::string::size_type>(count)));
				else
					throw runtime_assert("Failed to format assert message!\n");
			}
		}

		RTM_IMPL_VERSION_NAMESPACE_END
	}

	#define RTM_ASSERT(expression, format, ...) do { if (!(expression)) RTM_IMPL_NAMESPACE::rtm_impl::on_assert_throw(#expression, __LINE__, __FILE__, (format), ## __VA_ARGS__); } while(false)
	#define RTM_HAS_ASSERT_CHECKS
	#define RTM_NO_EXCEPT

#elif defined(RTM_ON_ASSERT_CUSTOM)

	#if !defined(RTM_ASSERT)
		#define RTM_ASSERT(expression, format, ...) do { if (!(expression)) RTM_ON_ASSERT_CUSTOM(#expression, __LINE__, __FILE__, (format), ## __VA_ARGS__); } while(false)
	#endif

	#define RTM_HAS_ASSERT_CHECKS
	#define RTM_NO_EXCEPT

#else

	#define RTM_ASSERT(expression, format, ...) ((void)0)
	#define RTM_NO_EXCEPT noexcept

#endif

//////////////////////////////////////////////////////////////////////////
// Deprecation support
//////////////////////////////////////////////////////////////////////////

// Use RTM_NO_DEPRECATION to disable all deprecation warnings
#if !defined(RTM_NO_DEPRECATION)
	#if defined(__has_cpp_attribute) && RTM_CPP_VERSION >= RTM_CPP_VERSION_14
		#if __has_cpp_attribute(deprecated)
			#define RTM_DEPRECATED(msg) [[deprecated(msg)]]
		#endif
	#endif

	#if !defined(RTM_DEPRECATED)
		#if defined(RTM_COMPILER_GCC) || defined(RTM_COMPILER_CLANG)
			#define RTM_DEPRECATED(msg) __attribute__((deprecated))
		#elif defined(RTM_COMPILER_MSVC)
			#define RTM_DEPRECATED(msg) __declspec(deprecated)
		#endif
	#endif
#endif

// If not defined, suppress all deprecation warnings
#if !defined(RTM_DEPRECATED)
	#define RTM_DEPRECATED(msg)
#endif

RTM_IMPL_FILE_PRAGMA_POP

```

`includes/rtm/impl/macros.mask4.impl.h`:

```h
#pragma once

////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2022 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

// Included only once from macros.h

#include "rtm/math.h"
#include "rtm/impl/compiler_utils.h"

RTM_IMPL_FILE_PRAGMA_PUSH

//////////////////////////////////////////////////////////////////////////
// This file contains helper macros to help improve code generation where required.
//////////////////////////////////////////////////////////////////////////

namespace rtm
{
	RTM_IMPL_VERSION_NAMESPACE_BEGIN

	namespace rtm_impl
	{
#if defined(RTM_NEON_INTRINSICS)
		RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE uint32x4_t RTM_SIMD_CALL cast_to_u32(uint32x4_t value) RTM_NO_EXCEPT
		{
			return value;
		}

		RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE uint32x2_t RTM_SIMD_CALL cast_to_u32(uint32x2_t value) RTM_NO_EXCEPT
		{
			return value;
		}

		// MSVC has uint32x4_t and float32x4_t as aliases, we cannot have an override
#if !defined(RTM_COMPILER_MSVC)
		RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE uint32x4_t RTM_SIMD_CALL cast_to_u32(float32x4_t value) RTM_NO_EXCEPT
		{
			return vreinterpretq_u32_f32(value);
		}

		RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE uint32x2_t RTM_SIMD_CALL cast_to_u32(float32x2_t value) RTM_NO_EXCEPT
		{
			return vreinterpret_u32_f32(value);
		}
#endif
#endif
	}

	RTM_IMPL_VERSION_NAMESPACE_END
}

#if defined(RTM_NEON_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Returns true if all [xyzw] SIMD lanes are true (aka ~0).
	// Input must be a rtm::mask4f or a like type: rtm::vector4f, rtm::quatf, _m128, float32x4_t, uint32x4_t.
	// Output must be a bool.
	//////////////////////////////////////////////////////////////////////////
	#define RTM_MASK4F_ALL_TRUE(input_mask, output) \
		output = vget_lane_u64(vreinterpret_u64_u16(vmovn_u32(rtm::rtm_impl::cast_to_u32(input_mask))), 0) == 0xFFFFFFFFFFFFFFFFULL
#elif defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Returns true if all [xyzw] SIMD lanes are true (aka ~0).
	// Input must be a rtm::mask4f.
	// Output must be a bool.
	//////////////////////////////////////////////////////////////////////////
	#define RTM_MASK4F_ALL_TRUE(input_mask, output) \
		output = _mm_movemask_ps(input_mask) == 0xF
#else
	//////////////////////////////////////////////////////////////////////////
	// Returns true if all [xyzw] SIMD lanes are true (aka ~0).
	// Input must be a rtm::mask4f.
	// Output must be a bool.
	//////////////////////////////////////////////////////////////////////////
	#define RTM_MASK4F_ALL_TRUE(input_mask, output) \
		output = input_mask.x != 0 && input_mask.y != 0 && input_mask.z != 0 && input_mask.w != 0
#endif

#if defined(RTM_NEON_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Returns true if all [xy] SIMD lanes are true (aka ~0).
	// Input must be a rtm::mask4f or a like type: rtm::vector4f, rtm::quatf, _m128, float32x4_t, uint32x4_t.
	// Output must be a bool.
	//////////////////////////////////////////////////////////////////////////
	#define RTM_MASK4F_ALL_TRUE2(input_mask, output) \
		output = vgetq_lane_u64(vreinterpretq_u64_u32(rtm::rtm_impl::cast_to_u32(input_mask)), 0) == 0xFFFFFFFFFFFFFFFFULL

	//////////////////////////////////////////////////////////////////////////
	// Returns true if all [xy] SIMD lanes are true (aka ~0).
	// Input must be a float32x2_t or uint32x2_t.
	// Output must be a bool.
	//////////////////////////////////////////////////////////////////////////
	#define RTM_MASK2F_ALL_TRUE(input_mask, output) \
		output = vget_lane_u64(vreinterpret_u64_u32(rtm::rtm_impl::cast_to_u32(input_mask)), 0) == 0xFFFFFFFFFFFFFFFFULL
#elif defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Returns true if all [xy] SIMD lanes are true (aka ~0).
	// Input must be a rtm::mask4f.
	// Output must be a bool.
	//////////////////////////////////////////////////////////////////////////
	#define RTM_MASK4F_ALL_TRUE2(input_mask, output) \
		output = (_mm_movemask_ps(input_mask) & 0x3) == 0x3
#else
	//////////////////////////////////////////////////////////////////////////
	// Returns true if all [xy] SIMD lanes are true (aka ~0).
	// Input must be a rtm::mask4f.
	// Output must be a bool.
	//////////////////////////////////////////////////////////////////////////
	#define RTM_MASK4F_ALL_TRUE2(input_mask, output) \
		output = input_mask.x != 0 && input_mask.y != 0
#endif

#if defined(RTM_NEON_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Returns true if all [xyz] SIMD lanes are true (aka ~0).
	// Input must be a rtm::mask4f or a like type: rtm::vector4f, rtm::quatf, _m128, float32x4_t, uint32x4_t.
	// Output must be a bool.
	//////////////////////////////////////////////////////////////////////////
	#define RTM_MASK4F_ALL_TRUE3(input_mask, output) \
		output = (vget_lane_u64(vreinterpret_u64_u16(vmovn_u32(rtm::rtm_impl::cast_to_u32(input_mask))), 0) & 0x0000FFFFFFFFFFFFULL) == 0x0000FFFFFFFFFFFFULL
#elif defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Returns true if all [xyz] SIMD lanes are true (aka ~0).
	// Input must be a rtm::mask4f.
	// Output must be a bool.
	//////////////////////////////////////////////////////////////////////////
	#define RTM_MASK4F_ALL_TRUE3(input_mask, output) \
		output = (_mm_movemask_ps(input_mask) & 0x7) == 0x7
#else
	//////////////////////////////////////////////////////////////////////////
	// Returns true if all [xyz] SIMD lanes are true (aka ~0).
	// Input must be a rtm::mask4f.
	// Output must be a bool.
	//////////////////////////////////////////////////////////////////////////
	#define RTM_MASK4F_ALL_TRUE3(input_mask, output) \
		output = input_mask.x != 0 && input_mask.y != 0 && input_mask.z != 0
#endif

#if defined(RTM_NEON_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Returns true if any [xyzw] SIMD lanes is true (aka ~0).
	// Input must be a rtm::mask4f or a like type: rtm::vector4f, rtm::quatf, _m128, float32x4_t, uint32x4_t.
	// Output must be a bool.
	//////////////////////////////////////////////////////////////////////////
	#define RTM_MASK4F_ANY_TRUE(input_mask, output) \
		output = vget_lane_u64(vreinterpret_u64_u16(vmovn_u32(rtm::rtm_impl::cast_to_u32(input_mask))), 0) != 0
#elif defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Returns true if any [xyzw] SIMD lanes is true (aka ~0).
	// Input must be a rtm::mask4f.
	// Output must be a bool.
	//////////////////////////////////////////////////////////////////////////
	#define RTM_MASK4F_ANY_TRUE(input_mask, output) \
		output = _mm_movemask_ps(input_mask) != 0
#else
	//////////////////////////////////////////////////////////////////////////
	// Returns true if any [xyzw] SIMD lanes is true (aka ~0).
	// Input must be a rtm::mask4f.
	// Output must be a bool.
	//////////////////////////////////////////////////////////////////////////
	#define RTM_MASK4F_ANY_TRUE(input_mask, output) \
		output = input_mask.x != 0 || input_mask.y != 0 || input_mask.z != 0 || input_mask.w != 0
#endif

#if defined(RTM_NEON_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Returns true if any [xy] SIMD lanes is true (aka ~0).
	// Input must be a rtm::mask4f or a like type: rtm::vector4f, rtm::quatf, _m128, float32x4_t, uint32x4_t.
	// Output must be a bool.
	//////////////////////////////////////////////////////////////////////////
	#define RTM_MASK4F_ANY_TRUE2(input_mask, output) \
		output = vgetq_lane_u64(vreinterpretq_u64_u32(rtm::rtm_impl::cast_to_u32(input_mask)), 0) != 0

	//////////////////////////////////////////////////////////////////////////
	// Returns true if any [xy] SIMD lanes is true (aka ~0).
	// Input must be a float32x2_t or uint32x2_t.
	// Output must be a bool.
	//////////////////////////////////////////////////////////////////////////
	#define RTM_MASK2F_ANY_TRUE(input_mask, output) \
		output = vget_lane_u64(vreinterpret_u64_u32(rtm::rtm_impl::cast_to_u32(input_mask)), 0) != 0
#elif defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Returns true if any [xy] SIMD lanes is true (aka ~0).
	// Input must be a rtm::mask4f.
	// Output must be a bool.
	//////////////////////////////////////////////////////////////////////////
	#define RTM_MASK4F_ANY_TRUE2(input_mask, output) \
		output = (_mm_movemask_ps(input_mask) & 0x3) != 0
#else
	//////////////////////////////////////////////////////////////////////////
	// Returns true if any [xy] SIMD lanes is true (aka ~0).
	// Input must be a rtm::mask4f.
	// Output must be a bool.
	//////////////////////////////////////////////////////////////////////////
	#define RTM_MASK4F_ANY_TRUE2(input_mask, output) \
		output = input_mask.x != 0 || input_mask.y != 0
#endif

#if defined(RTM_NEON_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Returns true if any [xyz] SIMD lanes is true (aka ~0).
	// Input must be a rtm::mask4f or a like type: rtm::vector4f, rtm::quatf, _m128, float32x4_t, uint32x4_t.
	// Output must be a bool.
	//////////////////////////////////////////////////////////////////////////
	#define RTM_MASK4F_ANY_TRUE3(input_mask, output) \
		output = (vget_lane_u64(vreinterpret_u64_u16(vmovn_u32(rtm::rtm_impl::cast_to_u32(input_mask))), 0) & 0x0000FFFFFFFFFFFFULL) != 0
#elif defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Returns true if any [xyz] SIMD lanes is true (aka ~0).
	// Input must be a rtm::mask4f.
	// Output must be a bool.
	//////////////////////////////////////////////////////////////////////////
	#define RTM_MASK4F_ANY_TRUE3(input_mask, output) \
		output = (_mm_movemask_ps(input_mask) & 0x7) != 0
#else
	//////////////////////////////////////////////////////////////////////////
	// Returns true if any [xyz] SIMD lanes is true (aka ~0).
	// Input must be a rtm::mask4f.
	// Output must be a bool.
	//////////////////////////////////////////////////////////////////////////
	#define RTM_MASK4F_ANY_TRUE3(input_mask, output) \
		output = input_mask.x != 0 || input_mask.y != 0 || input_mask.z != 0
#endif

RTM_IMPL_FILE_PRAGMA_POP

```

`includes/rtm/impl/macros.matrix.impl.h`:

```h
#pragma once

////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2022 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

// Included only once from macros.h

#include "rtm/math.h"
#include "rtm/impl/compiler_utils.h"

RTM_IMPL_FILE_PRAGMA_PUSH

//////////////////////////////////////////////////////////////////////////
// This file contains helper macros to help improve code generation where required.
//////////////////////////////////////////////////////////////////////////

#if defined(RTM_NEON_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Transposes a 4x4 matrix.
	// All inputs and outputs must be rtm::vector4f.
	//////////////////////////////////////////////////////////////////////////
	#define RTM_MATRIXF_TRANSPOSE_4X4(input_xyzw0, input_xyzw1, input_xyzw2, input_xyzw3, output_xxxx, output_yyyy, output_zzzz, output_wwww) \
		do { \
			const float32x4x2_t x0x2y0y2_z0z2w0w2 = vzipq_f32((input_xyzw0), (input_xyzw2)); \
			const float32x4x2_t x1x3y1y3_z1z3w1w3 = vzipq_f32((input_xyzw1), (input_xyzw3)); \
			const float32x4x2_t x0x1x2x3_y0y1y2y3 = vzipq_f32(x0x2y0y2_z0z2w0w2.val[0], x1x3y1y3_z1z3w1w3.val[0]); \
			const float32x4x2_t z0z1z2z3_w0w1w2w3 = vzipq_f32(x0x2y0y2_z0z2w0w2.val[1], x1x3y1y3_z1z3w1w3.val[1]); \
			(output_xxxx) = x0x1x2x3_y0y1y2y3.val[0]; \
			(output_yyyy) = x0x1x2x3_y0y1y2y3.val[1]; \
			(output_zzzz) = z0z1z2z3_w0w1w2w3.val[0]; \
			(output_wwww) = z0z1z2z3_w0w1w2w3.val[1]; \
		} while(0)
#elif defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Transposes a 4x4 matrix.
	// All inputs and outputs must be rtm::vector4f.
	//////////////////////////////////////////////////////////////////////////
	#define RTM_MATRIXF_TRANSPOSE_4X4(input_xyzw0, input_xyzw1, input_xyzw2, input_xyzw3, output_xxxx, output_yyyy, output_zzzz, output_wwww) \
		do { \
			const __m128 x0y0x1y1 = _mm_shuffle_ps((input_xyzw0), (input_xyzw1), _MM_SHUFFLE(1, 0, 1, 0)); \
			const __m128 z0w0z1w1 = _mm_shuffle_ps((input_xyzw0), (input_xyzw1), _MM_SHUFFLE(3, 2, 3, 2)); \
			const __m128 x2y2x3y3 = _mm_shuffle_ps((input_xyzw2), (input_xyzw3), _MM_SHUFFLE(1, 0, 1, 0)); \
			const __m128 z2w2z3w3 = _mm_shuffle_ps((input_xyzw2), (input_xyzw3), _MM_SHUFFLE(3, 2, 3, 2)); \
			(output_xxxx) = _mm_shuffle_ps(x0y0x1y1, x2y2x3y3, _MM_SHUFFLE(2, 0, 2, 0)); \
			(output_yyyy) = _mm_shuffle_ps(x0y0x1y1, x2y2x3y3, _MM_SHUFFLE(3, 1, 3, 1)); \
			(output_zzzz) = _mm_shuffle_ps(z0w0z1w1, z2w2z3w3, _MM_SHUFFLE(2, 0, 2, 0)); \
			(output_wwww) = _mm_shuffle_ps(z0w0z1w1, z2w2z3w3, _MM_SHUFFLE(3, 1, 3, 1)); \
		} while(0)
#else
	//////////////////////////////////////////////////////////////////////////
	// Transposes a 4x4 matrix.
	// All inputs and outputs must be rtm::vector4f.
	//////////////////////////////////////////////////////////////////////////
	#define RTM_MATRIXF_TRANSPOSE_4X4(input_xyzw0, input_xyzw1, input_xyzw2, input_xyzw3, output_xxxx, output_yyyy, output_zzzz, output_wwww) \
		do { \
			const float input_x0 = (input_xyzw0).x; \
			const float input_y0 = (input_xyzw0).y; \
			const float input_z0 = (input_xyzw0).z; \
			const float input_w0 = (input_xyzw0).w; \
			const float input_x1 = (input_xyzw1).x; \
			const float input_y1 = (input_xyzw1).y; \
			const float input_z1 = (input_xyzw1).z; \
			const float input_w1 = (input_xyzw1).w; \
			const float input_x2 = (input_xyzw2).x; \
			const float input_y2 = (input_xyzw2).y; \
			const float input_z2 = (input_xyzw2).z; \
			const float input_w2 = (input_xyzw2).w; \
			const float input_x3 = (input_xyzw3).x; \
			const float input_y3 = (input_xyzw3).y; \
			const float input_z3 = (input_xyzw3).z; \
			const float input_w3 = (input_xyzw3).w; \
			(output_xxxx) = RTM_IMPL_NAMESPACE::vector4f { input_x0, input_x1, input_x2, input_x3 }; \
			(output_yyyy) = RTM_IMPL_NAMESPACE::vector4f { input_y0, input_y1, input_y2, input_y3 }; \
			(output_zzzz) = RTM_IMPL_NAMESPACE::vector4f { input_z0, input_z1, input_z2, input_z3 }; \
			(output_wwww) = RTM_IMPL_NAMESPACE::vector4f { input_w0, input_w1, input_w2, input_w3 }; \
		} while(0)
#endif

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Transposes a 4x4 matrix.
	// All inputs and outputs must be rtm::vector4d.
	//////////////////////////////////////////////////////////////////////////
	#define RTM_MATRIXD_TRANSPOSE_4X4(input_xyzw0, input_xyzw1, input_xyzw2, input_xyzw3, output_xxxx, output_yyyy, output_zzzz, output_wwww) \
		do { \
			const __m128d x0x1 = _mm_unpacklo_pd(input_xyzw0.xy, input_xyzw1.xy); \
			const __m128d y0y1 = _mm_unpackhi_pd(input_xyzw0.xy, input_xyzw1.xy); \
			const __m128d x2x3 = _mm_unpacklo_pd(input_xyzw2.xy, input_xyzw3.xy); \
			const __m128d y2y3 = _mm_unpackhi_pd(input_xyzw2.xy, input_xyzw3.xy); \
			const __m128d z0z1 = _mm_unpacklo_pd(input_xyzw0.zw, input_xyzw1.zw); \
			const __m128d w0w1 = _mm_unpackhi_pd(input_xyzw0.zw, input_xyzw1.zw); \
			const __m128d z2z3 = _mm_unpacklo_pd(input_xyzw2.zw, input_xyzw3.zw); \
			const __m128d w2w3 = _mm_unpackhi_pd(input_xyzw2.zw, input_xyzw3.zw); \
			(output_xxxx) = RTM_IMPL_NAMESPACE::vector4d { x0x1, x2x3 }; \
			(output_yyyy) = RTM_IMPL_NAMESPACE::vector4d { y0y1, y2y3 }; \
			(output_zzzz) = RTM_IMPL_NAMESPACE::vector4d { z0z1, z2z3 }; \
			(output_wwww) = RTM_IMPL_NAMESPACE::vector4d { w0w1, w2w3 }; \
		} while(0)
#else
	//////////////////////////////////////////////////////////////////////////
	// Transposes a 4x4 matrix.
	// All inputs and outputs must be rtm::vector4d.
	//////////////////////////////////////////////////////////////////////////
	#define RTM_MATRIXD_TRANSPOSE_4X4(input_xyzw0, input_xyzw1, input_xyzw2, input_xyzw3, output_xxxx, output_yyyy, output_zzzz, output_wwww) \
		do { \
			const double input_x0 = (input_xyzw0).x; \
			const double input_y0 = (input_xyzw0).y; \
			const double input_z0 = (input_xyzw0).z; \
			const double input_w0 = (input_xyzw0).w; \
			const double input_x1 = (input_xyzw1).x; \
			const double input_y1 = (input_xyzw1).y; \
			const double input_z1 = (input_xyzw1).z; \
			const double input_w1 = (input_xyzw1).w; \
			const double input_x2 = (input_xyzw2).x; \
			const double input_y2 = (input_xyzw2).y; \
			const double input_z2 = (input_xyzw2).z; \
			const double input_w2 = (input_xyzw2).w; \
			const double input_x3 = (input_xyzw3).x; \
			const double input_y3 = (input_xyzw3).y; \
			const double input_z3 = (input_xyzw3).z; \
			const double input_w3 = (input_xyzw3).w; \
			(output_xxxx) = RTM_IMPL_NAMESPACE::vector4d { input_x0, input_x1, input_x2, input_x3 }; \
			(output_yyyy) = RTM_IMPL_NAMESPACE::vector4d { input_y0, input_y1, input_y2, input_y3 }; \
			(output_zzzz) = RTM_IMPL_NAMESPACE::vector4d { input_z0, input_z1, input_z2, input_z3 }; \
			(output_wwww) = RTM_IMPL_NAMESPACE::vector4d { input_w0, input_w1, input_w2, input_w3 }; \
		} while(0)
#endif

#if defined(RTM_NEON_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Transposes a 3x3 matrix.
	// All inputs and outputs must be rtm::vector4f.
	//////////////////////////////////////////////////////////////////////////
	#define RTM_MATRIXF_TRANSPOSE_3X3(input_xyz0, input_xyz1, input_xyz2, output_xxx, output_yyy, output_zzz) \
		do { \
			const float32x4x2_t x0x2y0y2_z0z2w0w2 = vzipq_f32((input_xyz0), (input_xyz2)); \
			const float32x4x2_t x1x1y1y1_z1z1w1w1 = vzipq_f32((input_xyz1), (input_xyz1)); \
			const float32x4x2_t x0x1x2x1_y0y1y2y1 = vzipq_f32(x0x2y0y2_z0z2w0w2.val[0], x1x1y1y1_z1z1w1w1.val[0]); \
			const float32x4x2_t z0z1z2z1_w0w1w2w1 = vzipq_f32(x0x2y0y2_z0z2w0w2.val[1], x1x1y1y1_z1z1w1w1.val[1]); \
			(output_xxx) = x0x1x2x1_y0y1y2y1.val[0]; \
			(output_yyy) = x0x1x2x1_y0y1y2y1.val[1]; \
			(output_zzz) = z0z1z2z1_w0w1w2w1.val[0]; \
		} while(0)
#elif defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Transposes a 3x3 matrix.
	// All inputs and outputs must be rtm::vector4f.
	//////////////////////////////////////////////////////////////////////////
	#define RTM_MATRIXF_TRANSPOSE_3X3(input_xyz0, input_xyz1, input_xyz2, output_xxx, output_yyy, output_zzz) \
		do { \
			const __m128 input_xyz2_ = (input_xyz2); \
			const __m128 x0y0x1y1 = _mm_shuffle_ps((input_xyz0), (input_xyz1), _MM_SHUFFLE(1, 0, 1, 0)); \
			const __m128 z0w0z1w1 = _mm_shuffle_ps((input_xyz0), (input_xyz1), _MM_SHUFFLE(3, 2, 3, 2)); \
			(output_xxx) = _mm_shuffle_ps(x0y0x1y1, input_xyz2_, _MM_SHUFFLE(2, 0, 2, 0)); \
			(output_yyy) = _mm_shuffle_ps(x0y0x1y1, input_xyz2_, _MM_SHUFFLE(3, 1, 3, 1)); \
			(output_zzz) = _mm_shuffle_ps(z0w0z1w1, input_xyz2_, _MM_SHUFFLE(2, 2, 2, 0)); \
		} while(0)
#else
	//////////////////////////////////////////////////////////////////////////
	// Transposes a 3x3 matrix.
	// All inputs and outputs must be rtm::vector4f.
	//////////////////////////////////////////////////////////////////////////
	#define RTM_MATRIXF_TRANSPOSE_3X3(input_xyz0, input_xyz1, input_xyz2, output_xxx, output_yyy, output_zzz) \
		do { \
			const float input_x0 = (input_xyz0).x; \
			const float input_y0 = (input_xyz0).y; \
			const float input_z0 = (input_xyz0).z; \
			const float input_x1 = (input_xyz1).x; \
			const float input_y1 = (input_xyz1).y; \
			const float input_z1 = (input_xyz1).z; \
			const float input_x2 = (input_xyz2).x; \
			const float input_y2 = (input_xyz2).y; \
			const float input_z2 = (input_xyz2).z; \
			(output_xxx) = RTM_IMPL_NAMESPACE::vector4f { input_x0, input_x1, input_x2, input_x2 }; \
			(output_yyy) = RTM_IMPL_NAMESPACE::vector4f { input_y0, input_y1, input_y2, input_y2 }; \
			(output_zzz) = RTM_IMPL_NAMESPACE::vector4f { input_z0, input_z1, input_z2, input_z2 }; \
		} while(0)
#endif

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Transposes a 3x3 matrix.
	// All inputs and outputs must be rtm::vector4d.
	//////////////////////////////////////////////////////////////////////////
	#define RTM_MATRIXD_TRANSPOSE_3X3(input_xyz0, input_xyz1, input_xyz2, output_xxx, output_yyy, output_zzz) \
		do { \
			const __m128d x0x1 = _mm_shuffle_pd(input_xyz0.xy, input_xyz1.xy, _MM_SHUFFLE2(0, 0)); \
			const __m128d y0y1 = _mm_shuffle_pd(input_xyz0.xy, input_xyz1.xy, _MM_SHUFFLE2(1, 1)); \
			const __m128d y2 = _mm_shuffle_pd(input_xyz2.xy, input_xyz2.xy, _MM_SHUFFLE2(1, 1)); \
			const __m128d z0z1 = _mm_shuffle_pd(input_xyz0.zw, input_xyz1.zw, _MM_SHUFFLE2(0, 0)); \
			(output_xxx) = RTM_IMPL_NAMESPACE::vector4d { x0x1, input_xyz2.xy }; \
			(output_yyy) = RTM_IMPL_NAMESPACE::vector4d { y0y1, y2 }; \
			(output_zzz) = RTM_IMPL_NAMESPACE::vector4d { z0z1, input_xyz2.zw }; \
		} while(0)
#else
	//////////////////////////////////////////////////////////////////////////
	// Transposes a 3x3 matrix.
	// All inputs and outputs must be rtm::vector4d.
	//////////////////////////////////////////////////////////////////////////
	#define RTM_MATRIXD_TRANSPOSE_3X3(input_xyz0, input_xyz1, input_xyz2, output_xxx, output_yyy, output_zzz) \
		do { \
			const double input_x0 = (input_xyz0).x; \
			const double input_y0 = (input_xyz0).y; \
			const double input_z0 = (input_xyz0).z; \
			const double input_x1 = (input_xyz1).x; \
			const double input_y1 = (input_xyz1).y; \
			const double input_z1 = (input_xyz1).z; \
			const double input_x2 = (input_xyz2).x; \
			const double input_y2 = (input_xyz2).y; \
			const double input_z2 = (input_xyz2).z; \
			(output_xxx) = RTM_IMPL_NAMESPACE::vector4d { input_x0, input_x1, input_x2, input_x2 }; \
			(output_yyy) = RTM_IMPL_NAMESPACE::vector4d { input_y0, input_y1, input_y2, input_y2 }; \
			(output_zzz) = RTM_IMPL_NAMESPACE::vector4d { input_z0, input_z1, input_z2, input_z2 }; \
		} while(0)
#endif

#if defined(RTM_NEON_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Transposes a 2x2 matrix.
	// All inputs and outputs must be rtm::vector4f.
	//////////////////////////////////////////////////////////////////////////
	#define RTM_MATRIXF_TRANSPOSE_2X2(input_xy0, input_xy1, output_xx, output_yy) \
		do { \
			const float32x4_t x0x1y0y1 = vzip1q_f32((input_xy0), (input_xy1)); \
			(output_xx) = x0x1y0y1; \
			(output_yy) = vextq_f32(x0x1y0y1, x0x1y0y1, 2); \
		} while(0)
#elif defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Transposes a 2x2 matrix.
	// All inputs and outputs must be rtm::vector4f.
	//////////////////////////////////////////////////////////////////////////
	#define RTM_MATRIXF_TRANSPOSE_2X2(input_xy0, input_xy1, output_xx, output_yy) \
		do { \
			const __m128 x0x1y0y1 = _mm_unpacklo_ps((input_xy0), (input_xy1)); \
			(output_xx) = x0x1y0y1; \
			(output_yy) = _mm_movehl_ps(x0x1y0y1, x0x1y0y1); \
		} while(0)
#else
	//////////////////////////////////////////////////////////////////////////
	// Transposes a 2x2 matrix.
	// All inputs and outputs must be rtm::vector4f.
	//////////////////////////////////////////////////////////////////////////
	#define RTM_MATRIXF_TRANSPOSE_2X2(input_xy0, input_xy1, output_xx, output_yy) \
		do { \
			const float input_x0 = (input_xy0).x; \
			const float input_y0 = (input_xy0).y; \
			const float input_x1 = (input_xy1).x; \
			const float input_y1 = (input_xy1).y; \
			(output_xx) = RTM_IMPL_NAMESPACE::vector4f { input_x0, input_x1, input_x0, input_x1 }; \
			(output_yy) = RTM_IMPL_NAMESPACE::vector4f { input_y0, input_y1, input_y0, input_y1 }; \
		} while(0)
#endif

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Transposes a 2x2 matrix.
	// All inputs and outputs must be rtm::vector4d.
	//////////////////////////////////////////////////////////////////////////
	#define RTM_MATRIXD_TRANSPOSE_2X2(input_xy0, input_xy1, output_xx, output_yy) \
		do { \
			const __m128d x0x1 = _mm_unpacklo_pd((input_xy0).xy, (input_xy1).xy); \
			const __m128d y0y1 = _mm_unpackhi_pd((input_xy0).xy, (input_xy1).xy); \
			(output_xx) = RTM_IMPL_NAMESPACE::vector4d { x0x1, x0x1 }; \
			(output_yy) = RTM_IMPL_NAMESPACE::vector4d { y0y1, y0y1 }; \
		} while(0)
#else
	//////////////////////////////////////////////////////////////////////////
	// Transposes a 2x2 matrix.
	// All inputs and outputs must be rtm::vector4d.
	//////////////////////////////////////////////////////////////////////////
	#define RTM_MATRIXD_TRANSPOSE_2X2(input_xy0, input_xy1, output_xx, output_yy) \
		do { \
			const double input_x0 = (input_xy0).x; \
			const double input_y0 = (input_xy0).y; \
			const double input_x1 = (input_xy1).x; \
			const double input_y1 = (input_xy1).y; \
			(output_xx) = RTM_IMPL_NAMESPACE::vector4d { input_x0, input_x1, input_x0, input_x1 }; \
			(output_yy) = RTM_IMPL_NAMESPACE::vector4d { input_y0, input_y1, input_y0, input_y1 }; \
		} while(0)
#endif

#if defined(RTM_NEON_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Transposes a 4x3 matrix.
	// All inputs and outputs must be rtm::vector4f.
	//////////////////////////////////////////////////////////////////////////
	#define RTM_MATRIXF_TRANSPOSE_4X3(input_xyz0, input_xyz1, input_xyz2, input_xyz3, output_xxxx, output_yyyy, output_zzzz) \
		do { \
			const float32x4x2_t x0x2y0y2_z0z2w0w2 = vzipq_f32((input_xyz0), (input_xyz2)); \
			const float32x4x2_t x1x3y1y3_z1z3w1w3 = vzipq_f32((input_xyz1), (input_xyz3)); \
			const float32x4x2_t x0x1x2x3_y0y1y2y3 = vzipq_f32(x0x2y0y2_z0z2w0w2.val[0], x1x3y1y3_z1z3w1w3.val[0]); \
			const float32x4x2_t z0z1z2z3_w0w1w2w3 = vzipq_f32(x0x2y0y2_z0z2w0w2.val[1], x1x3y1y3_z1z3w1w3.val[1]); \
			(output_xxxx) = x0x1x2x3_y0y1y2y3.val[0]; \
			(output_yyyy) = x0x1x2x3_y0y1y2y3.val[1]; \
			(output_zzzz) = z0z1z2z3_w0w1w2w3.val[0]; \
		} while(0)
#elif defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Transposes a 4x3 matrix.
	// All inputs and outputs must be rtm::vector4f.
	//////////////////////////////////////////////////////////////////////////
	#define RTM_MATRIXF_TRANSPOSE_4X3(input_xyz0, input_xyz1, input_xyz2, input_xyz3, output_xxxx, output_yyyy, output_zzzz) \
		do { \
			const __m128 x0y0x1y1 = _mm_shuffle_ps((input_xyz0), (input_xyz1), _MM_SHUFFLE(1, 0, 1, 0)); \
			const __m128 z0w0z1w1 = _mm_shuffle_ps((input_xyz0), (input_xyz1), _MM_SHUFFLE(3, 2, 3, 2)); \
			const __m128 x2y2x3y3 = _mm_shuffle_ps((input_xyz2), (input_xyz3), _MM_SHUFFLE(1, 0, 1, 0)); \
			const __m128 z2w2z3w3 = _mm_shuffle_ps((input_xyz2), (input_xyz3), _MM_SHUFFLE(3, 2, 3, 2)); \
			(output_xxxx) = _mm_shuffle_ps(x0y0x1y1, x2y2x3y3, _MM_SHUFFLE(2, 0, 2, 0)); \
			(output_yyyy) = _mm_shuffle_ps(x0y0x1y1, x2y2x3y3, _MM_SHUFFLE(3, 1, 3, 1)); \
			(output_zzzz) = _mm_shuffle_ps(z0w0z1w1, z2w2z3w3, _MM_SHUFFLE(2, 0, 2, 0)); \
		} while(0)
#else
	//////////////////////////////////////////////////////////////////////////
	// Transposes a 4x3 matrix.
	// All inputs and outputs must be rtm::vector4f.
	//////////////////////////////////////////////////////////////////////////
	#define RTM_MATRIXF_TRANSPOSE_4X3(input_xyz0, input_xyz1, input_xyz2, input_xyz3, output_xxxx, output_yyyy, output_zzzz) \
		do { \
			const float input_x0 = (input_xyz0).x; \
			const float input_y0 = (input_xyz0).y; \
			const float input_z0 = (input_xyz0).z; \
			const float input_x1 = (input_xyz1).x; \
			const float input_y1 = (input_xyz1).y; \
			const float input_z1 = (input_xyz1).z; \
			const float input_x2 = (input_xyz2).x; \
			const float input_y2 = (input_xyz2).y; \
			const float input_z2 = (input_xyz2).z; \
			const float input_x3 = (input_xyz3).x; \
			const float input_y3 = (input_xyz3).y; \
			const float input_z3 = (input_xyz3).z; \
			(output_xxxx) = RTM_IMPL_NAMESPACE::vector4f { input_x0, input_x1, input_x2, input_x3 }; \
			(output_yyyy) = RTM_IMPL_NAMESPACE::vector4f { input_y0, input_y1, input_y2, input_y3 }; \
			(output_zzzz) = RTM_IMPL_NAMESPACE::vector4f { input_z0, input_z1, input_z2, input_z3 }; \
		} while(0)
#endif

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Transposes a 4x3 matrix.
	// All inputs and outputs must be rtm::vector4d.
	//////////////////////////////////////////////////////////////////////////
	#define RTM_MATRIXD_TRANSPOSE_4X3(input_xyz0, input_xyz1, input_xyz2, input_xyz3, output_xxxx, output_yyyy, output_zzzz) \
		do { \
			const __m128d x0x1 = _mm_unpacklo_pd(input_xyz0.xy, input_xyz1.xy); \
			const __m128d y0y1 = _mm_unpackhi_pd(input_xyz0.xy, input_xyz1.xy); \
			const __m128d x2x3 = _mm_unpacklo_pd(input_xyz2.xy, input_xyz3.xy); \
			const __m128d y2y3 = _mm_unpackhi_pd(input_xyz2.xy, input_xyz3.xy); \
			const __m128d z0z1 = _mm_unpacklo_pd(input_xyz0.zw, input_xyz1.zw); \
			const __m128d z2z3 = _mm_unpacklo_pd(input_xyz2.zw, input_xyz3.zw); \
			(output_xxxx) = RTM_IMPL_NAMESPACE::vector4d { x0x1, x2x3 }; \
			(output_yyyy) = RTM_IMPL_NAMESPACE::vector4d { y0y1, y2y3 }; \
			(output_zzzz) = RTM_IMPL_NAMESPACE::vector4d { z0z1, z2z3 }; \
		} while(0)
#else
	//////////////////////////////////////////////////////////////////////////
	// Transposes a 4x3 matrix.
	// All inputs and outputs must be rtm::vector4d.
	//////////////////////////////////////////////////////////////////////////
	#define RTM_MATRIXD_TRANSPOSE_4X3(input_xyz0, input_xyz1, input_xyz2, input_xyz3, output_xxxx, output_yyyy, output_zzzz) \
		do { \
			const double input_x0 = (input_xyz0).x; \
			const double input_y0 = (input_xyz0).y; \
			const double input_z0 = (input_xyz0).z; \
			const double input_x1 = (input_xyz1).x; \
			const double input_y1 = (input_xyz1).y; \
			const double input_z1 = (input_xyz1).z; \
			const double input_x2 = (input_xyz2).x; \
			const double input_y2 = (input_xyz2).y; \
			const double input_z2 = (input_xyz2).z; \
			const double input_x3 = (input_xyz3).x; \
			const double input_y3 = (input_xyz3).y; \
			const double input_z3 = (input_xyz3).z; \
			(output_xxxx) = RTM_IMPL_NAMESPACE::vector4d { input_x0, input_x1, input_x2, input_x3 }; \
			(output_yyyy) = RTM_IMPL_NAMESPACE::vector4d { input_y0, input_y1, input_y2, input_y3 }; \
			(output_zzzz) = RTM_IMPL_NAMESPACE::vector4d { input_z0, input_z1, input_z2, input_z3 }; \
		} while(0)
#endif

#if defined(RTM_NEON_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Transposes a 3x4 matrix.
	// All inputs and outputs must be rtm::vector4f.
	//////////////////////////////////////////////////////////////////////////
	#define RTM_MATRIXF_TRANSPOSE_3X4(input_xyzw0, input_xyzw1, input_xyzw2, output_xxx, output_yyy, output_zzz, output_www) \
		do { \
			const float32x4x2_t x0x2y0y2_z0z2w0w2 = vzipq_f32((input_xyzw0), (input_xyzw2)); \
			const float32x4x2_t x1x1y1y1_z1z1w1w1 = vzipq_f32((input_xyzw1), (input_xyzw1)); \
			const float32x4x2_t x0x1x2x1_y0y1y2y1 = vzipq_f32(x0x2y0y2_z0z2w0w2.val[0], x1x1y1y1_z1z1w1w1.val[0]); \
			const float32x4x2_t z0z1z2z1_w0w1w2w1 = vzipq_f32(x0x2y0y2_z0z2w0w2.val[1], x1x1y1y1_z1z1w1w1.val[1]); \
			(output_xxx) = x0x1x2x1_y0y1y2y1.val[0]; \
			(output_yyy) = x0x1x2x1_y0y1y2y1.val[1]; \
			(output_zzz) = z0z1z2z1_w0w1w2w1.val[0]; \
			(output_www) = z0z1z2z1_w0w1w2w1.val[1]; \
		} while(0)
#elif defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Transposes a 3x4 matrix.
	// All inputs and outputs must be rtm::vector4f.
	//////////////////////////////////////////////////////////////////////////
	#define RTM_MATRIXF_TRANSPOSE_3X4(input_xyzw0, input_xyzw1, input_xyzw2, output_xxx, output_yyy, output_zzz, output_www) \
		do { \
			const __m128 input_xyzw2_ = (input_xyzw2); \
			const __m128 x0y0x1y1 = _mm_shuffle_ps((input_xyzw0), (input_xyzw1), _MM_SHUFFLE(1, 0, 1, 0)); \
			const __m128 z0w0z1w1 = _mm_shuffle_ps((input_xyzw0), (input_xyzw1), _MM_SHUFFLE(3, 2, 3, 2)); \
			(output_xxx) = _mm_shuffle_ps(x0y0x1y1, input_xyzw2_, _MM_SHUFFLE(0, 0, 2, 0)); \
			(output_yyy) = _mm_shuffle_ps(x0y0x1y1, input_xyzw2_, _MM_SHUFFLE(1, 1, 3, 1)); \
			(output_zzz) = _mm_shuffle_ps(z0w0z1w1, input_xyzw2_, _MM_SHUFFLE(2, 2, 2, 0)); \
			(output_www) = _mm_shuffle_ps(z0w0z1w1, input_xyzw2_, _MM_SHUFFLE(3, 3, 3, 1)); \
		} while(0)
#else
	//////////////////////////////////////////////////////////////////////////
	// Transposes a 3x4 matrix.
	// All inputs and outputs must be rtm::vector4f.
	//////////////////////////////////////////////////////////////////////////
	#define RTM_MATRIXF_TRANSPOSE_3X4(input_xyzw0, input_xyzw1, input_xyzw2, output_xxx, output_yyy, output_zzz, output_www) \
		do { \
			const float input_x0 = (input_xyzw0).x; \
			const float input_y0 = (input_xyzw0).y; \
			const float input_z0 = (input_xyzw0).z; \
			const float input_w0 = (input_xyzw0).w; \
			const float input_x1 = (input_xyzw1).x; \
			const float input_y1 = (input_xyzw1).y; \
			const float input_z1 = (input_xyzw1).z; \
			const float input_w1 = (input_xyzw1).w; \
			const float input_x2 = (input_xyzw2).x; \
			const float input_y2 = (input_xyzw2).y; \
			const float input_z2 = (input_xyzw2).z; \
			const float input_w2 = (input_xyzw2).w; \
			(output_xxx) = RTM_IMPL_NAMESPACE::vector4f { input_x0, input_x1, input_x2, input_x2 }; \
			(output_yyy) = RTM_IMPL_NAMESPACE::vector4f { input_y0, input_y1, input_y2, input_y2 }; \
			(output_zzz) = RTM_IMPL_NAMESPACE::vector4f { input_z0, input_z1, input_z2, input_z2 }; \
			(output_www) = RTM_IMPL_NAMESPACE::vector4f { input_w0, input_w1, input_w2, input_w2 }; \
		} while(0)
#endif

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Transposes a 3x4 matrix.
	// All inputs and outputs must be rtm::vector4d.
	//////////////////////////////////////////////////////////////////////////
	#define RTM_MATRIXD_TRANSPOSE_3X4(input_xyzw0, input_xyzw1, input_xyzw2, output_xxx, output_yyy, output_zzz, output_www) \
		do { \
			const __m128d x0x1 = _mm_unpacklo_pd(input_xyzw0.xy, input_xyzw1.xy); \
			const __m128d y0y1 = _mm_unpackhi_pd(input_xyzw0.xy, input_xyzw1.xy); \
			const __m128d y2y2 = _mm_unpackhi_pd(input_xyzw2.xy, input_xyzw2.xy); \
			const __m128d z0z1 = _mm_unpacklo_pd(input_xyzw0.zw, input_xyzw1.zw); \
			const __m128d w0w1 = _mm_unpackhi_pd(input_xyzw0.zw, input_xyzw1.zw); \
			const __m128d w2w2 = _mm_unpackhi_pd(input_xyzw2.zw, input_xyzw2.zw); \
			(output_xxx) = RTM_IMPL_NAMESPACE::vector4d { x0x1, input_xyzw2.xy }; \
			(output_yyy) = RTM_IMPL_NAMESPACE::vector4d { y0y1, y2y2 }; \
			(output_zzz) = RTM_IMPL_NAMESPACE::vector4d { z0z1, input_xyzw2.zw }; \
			(output_www) = RTM_IMPL_NAMESPACE::vector4d { w0w1, w2w2 }; \
		} while(0)
#else
	//////////////////////////////////////////////////////////////////////////
	// Transposes a 3x4 matrix.
	// All inputs and outputs must be rtm::vector4d.
	//////////////////////////////////////////////////////////////////////////
	#define RTM_MATRIXD_TRANSPOSE_3X4(input_xyzw0, input_xyzw1, input_xyzw2, output_xxx, output_yyy, output_zzz, output_www) \
		do { \
			const double input_x0 = (input_xyzw0).x; \
			const double input_y0 = (input_xyzw0).y; \
			const double input_z0 = (input_xyzw0).z; \
			const double input_w0 = (input_xyzw0).w; \
			const double input_x1 = (input_xyzw1).x; \
			const double input_y1 = (input_xyzw1).y; \
			const double input_z1 = (input_xyzw1).z; \
			const double input_w1 = (input_xyzw1).w; \
			const double input_x2 = (input_xyzw2).x; \
			const double input_y2 = (input_xyzw2).y; \
			const double input_z2 = (input_xyzw2).z; \
			const double input_w2 = (input_xyzw2).w; \
			(output_xxx) = RTM_IMPL_NAMESPACE::vector4d { input_x0, input_x1, input_x2, input_x2 }; \
			(output_yyy) = RTM_IMPL_NAMESPACE::vector4d { input_y0, input_y1, input_y2, input_y2 }; \
			(output_zzz) = RTM_IMPL_NAMESPACE::vector4d { input_z0, input_z1, input_z2, input_z2 }; \
			(output_www) = RTM_IMPL_NAMESPACE::vector4d { input_w0, input_w1, input_w2, input_w2 }; \
		} while(0)
#endif

RTM_IMPL_FILE_PRAGMA_POP

```

`includes/rtm/impl/macros.vector4.impl.h`:

```h
#pragma once

////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2022 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

// Included only once from macros.h

#include "rtm/math.h"
#include "rtm/impl/compiler_utils.h"

RTM_IMPL_FILE_PRAGMA_PUSH

//////////////////////////////////////////////////////////////////////////
// This file contains helper macros to help improve code generation where required.
//////////////////////////////////////////////////////////////////////////

#if defined(RTM_NEON64_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Per component multiplication/addition of the three inputs: v2 + (v0 * v1)
	// All three inputs must be an rtm::vector4f.
	//////////////////////////////////////////////////////////////////////////
	#define RTM_VECTOR4F_MULV_ADD(v0, v1, v2) vfmaq_f32((v2), (v0), (v1))

	//////////////////////////////////////////////////////////////////////////
	// Per component multiplication/addition of the three inputs: v2 + (v0 * v1)
	// All three inputs must be an float32x2_t.
	//////////////////////////////////////////////////////////////////////////
	#define RTM_VECTOR2F_MULV_ADD(v0, v1, v2) vfma_f32((v2), (v0), (v1))
#elif defined(RTM_NEON_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Per component multiplication/addition of the three inputs: v2 + (v0 * v1)
	// All three inputs must be an rtm::vector4f.
	//////////////////////////////////////////////////////////////////////////
	#define RTM_VECTOR4F_MULV_ADD(v0, v1, v2) vmlaq_f32((v2), (v0), (v1))

	//////////////////////////////////////////////////////////////////////////
	// Per component multiplication/addition of the three inputs: v2 + (v0 * v1)
	// All three inputs must be an float32x2_t.
	//////////////////////////////////////////////////////////////////////////
	#define RTM_VECTOR2F_MULV_ADD(v0, v1, v2) vmla_f32((v2), (v0), (v1))
#elif defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Per component multiplication/addition of the three inputs: v2 + (v0 * v1)
	// All three inputs must be an rtm::vector4f.
	//////////////////////////////////////////////////////////////////////////
	#define RTM_VECTOR4F_MULV_ADD(v0, v1, v2) _mm_add_ps(_mm_mul_ps((v0), (v1)), (v2))
#else
	//////////////////////////////////////////////////////////////////////////
	// Per component multiplication/addition of the three inputs: v2 + (v0 * v1)
	// All three inputs must be an rtm::vector4f.
	//////////////////////////////////////////////////////////////////////////
	#define RTM_VECTOR4F_MULV_ADD(v0, v1, v2) RTM_IMPL_NAMESPACE::vector4f { (v2).x + ((v0).x * (v1).x), (v2).y + ((v0).y * (v1).y), (v2).z + ((v0).z * (v1).z), (v2).w + ((v0).w * (v1).w) }
#endif

#if defined(RTM_NEON64_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Per component multiplication/addition of the three inputs: v2 + (v0 * s1)
	// The v0 and v2 inputs must be a rtm::vector4f and s1 must be a float.
	//////////////////////////////////////////////////////////////////////////
	#define RTM_VECTOR4F_MULS_ADD(v0, s1, v2) vfmaq_n_f32((v2), (v0), (s1))
#elif defined(RTM_NEON_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Per component multiplication/addition of the three inputs: v2 + (v0 * s1)
	// The v0 and v2 inputs must be a rtm::vector4f and s1 must be a float.
	//////////////////////////////////////////////////////////////////////////
	#define RTM_VECTOR4F_MULS_ADD(v0, s1, v2) vmlaq_n_f32((v2), (v0), (s1))
#elif defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Per component multiplication/addition of the three inputs: v2 + (v0 * s1)
	// The v0 and v2 inputs must be a rtm::vector4f and s1 must be a float.
	//////////////////////////////////////////////////////////////////////////
	#define RTM_VECTOR4F_MULS_ADD(v0, s1, v2) _mm_add_ps(_mm_mul_ps((v0), _mm_set_ps1((s1))), (v2))
#else
	//////////////////////////////////////////////////////////////////////////
	// Per component multiplication/addition of the three inputs: v2 + (v0 * s1)
	// The v0 and v2 inputs must be a rtm::vector4f and s1 must be a float.
	//////////////////////////////////////////////////////////////////////////
	#define RTM_VECTOR4F_MULS_ADD(v0, s1, v2) RTM_IMPL_NAMESPACE::vector4f { (v2).x + ((v0).x * (s1)), (v2).y + ((v0).y * (s1)), (v2).z + ((v0).z * (s1)), (v2).w + ((v0).w * (s1)) }
#endif

#if defined(RTM_NEON64_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Per component negative multiplication/subtraction of the three inputs: -((v0 * v1) - v2)
	// This is mathematically equivalent to: v2 - (v0 * v1)
	// All three inputs must be an rtm::vector4f.
	//////////////////////////////////////////////////////////////////////////
	#define RTM_VECTOR4F_NEG_MULV_SUB(v0, v1, v2) vfmsq_f32((v2), (v0), (v1))
#elif defined(RTM_NEON_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Per component negative multiplication/subtraction of the three inputs: -((v0 * v1) - v2)
	// This is mathematically equivalent to: v2 - (v0 * v1)
	// All three inputs must be an rtm::vector4f.
	//////////////////////////////////////////////////////////////////////////
	#define RTM_VECTOR4F_NEG_MULV_SUB(v0, v1, v2) vmlsq_f32((v2), (v0), (v1))
#elif defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Per component negative multiplication/subtraction of the three inputs: -((v0 * v1) - v2)
	// This is mathematically equivalent to: v2 - (v0 * v1)
	// All three inputs must be an rtm::vector4f.
	//////////////////////////////////////////////////////////////////////////
	#define RTM_VECTOR4F_NEG_MULV_SUB(v0, v1, v2) _mm_sub_ps((v2), _mm_mul_ps((v0), (v1)))
#else
	//////////////////////////////////////////////////////////////////////////
	// Per component negative multiplication/subtraction of the three inputs: -((v0 * v1) - v2)
	// This is mathematically equivalent to: v2 - (v0 * v1)
	// All three inputs must be an rtm::vector4f.
	//////////////////////////////////////////////////////////////////////////
	#define RTM_VECTOR4F_NEG_MULV_SUB(v0, v1, v2) RTM_IMPL_NAMESPACE::vector4f { (v2).x - ((v0).x * (v1).x), (v2).y - ((v0).y * (v1).y), (v2).z - ((v0).z * (v1).z), (v2).w - ((v0).w * (v1).w) }
#endif

#if defined(RTM_NEON64_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Per component negative multiplication/subtraction of the three inputs: -((v0 * s1) - v2)
	// This is mathematically equivalent to: v2 - (v0 * s1)
	// The v0 and v2 inputs must be a rtm::vector4f and s1 must be a float.
	//////////////////////////////////////////////////////////////////////////
	#define RTM_VECTOR4F_NEG_MULS_SUB(v0, s1, v2) vfmsq_n_f32((v2), (v0), (s1))
#elif defined(RTM_NEON_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Per component negative multiplication/subtraction of the three inputs: -((v0 * s1) - v2)
	// This is mathematically equivalent to: v2 - (v0 * s1)
	// The v0 and v2 inputs must be a rtm::vector4f and s1 must be a float.
	//////////////////////////////////////////////////////////////////////////
	#define RTM_VECTOR4F_NEG_MULS_SUB(v0, s1, v2) vmlsq_n_f32((v2), (v0), (s1))
#elif defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Per component negative multiplication/subtraction of the three inputs: -((v0 * s1) - v2)
	// This is mathematically equivalent to: v2 - (v0 * s1)
	// The v0 and v2 inputs must be a rtm::vector4f and s1 must be a float.
	//////////////////////////////////////////////////////////////////////////
	#define RTM_VECTOR4F_NEG_MULS_SUB(v0, s1, v2) _mm_sub_ps((v2), _mm_mul_ps((v0), _mm_set_ps1((s1))))
#else
	//////////////////////////////////////////////////////////////////////////
	// Per component negative multiplication/subtraction of the three inputs: -((v0 * s1) - v2)
	// This is mathematically equivalent to: v2 - (v0 * s1)
	// The v0 and v2 inputs must be a rtm::vector4f and s1 must be a float.
	//////////////////////////////////////////////////////////////////////////
	#define RTM_VECTOR4F_NEG_MULS_SUB(v0, s1, v2) RTM_IMPL_NAMESPACE::vector4f { (v2).x - ((v0).x * (s1)), (v2).y - ((v0).y * (s1)), (v2).z - ((v0).z * (s1)), (v2).w - ((v0).w * (s1)) }
#endif

#if defined(RTM_AVX_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Per component selection depending on the mask: mask != 0 ? if_true : if_false
	//////////////////////////////////////////////////////////////////////////
	#define RTM_VECTOR4F_SELECT(mask, if_true, if_false) _mm_blendv_ps(if_false, if_true, mask)

	//////////////////////////////////////////////////////////////////////////
	// Per component selection depending on the mask: mask != 0 ? if_true : if_false
	//////////////////////////////////////////////////////////////////////////
	#define RTM_VECTOR2D_SELECT(mask, if_true, if_false) _mm_blendv_pd(if_false, if_true, mask)
#elif defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Per component selection depending on the mask: mask != 0 ? if_true : if_false
	//////////////////////////////////////////////////////////////////////////
	#define RTM_VECTOR4F_SELECT(mask, if_true, if_false) _mm_or_ps(_mm_andnot_ps(mask, if_false), _mm_and_ps(if_true, mask))

	//////////////////////////////////////////////////////////////////////////
	// Per component selection depending on the mask: mask != 0 ? if_true : if_false
	//////////////////////////////////////////////////////////////////////////
	#define RTM_VECTOR2D_SELECT(mask, if_true, if_false) _mm_or_pd(_mm_andnot_pd(mask, if_false), _mm_and_pd(if_true, mask))
#elif defined(RTM_NEON_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Per component selection depending on the mask: mask != 0 ? if_true : if_false
	//////////////////////////////////////////////////////////////////////////
	#define RTM_VECTOR4F_SELECT(mask, if_true, if_false) vbslq_f32(mask, if_true, if_false)

	// RTM_VECTOR2D_SELECT not defined for NEON yet, TODO
#else
	// Macros not defined for scalar code path
#endif

#if defined(RTM_SSE2_INTRINSICS)
	#if defined(RTM_COMPILER_MSVC)
		//////////////////////////////////////////////////////////////////////////
		// Creates a vector constant from its 4 components
		//////////////////////////////////////////////////////////////////////////
		#define RTM_VECTOR4F_MAKE(x, y, z, w) { { x, y, z, w } }

		//////////////////////////////////////////////////////////////////////////
		// Creates a vector constant from its 2 components
		//////////////////////////////////////////////////////////////////////////
		#define RTM_VECTOR2D_MAKE(x, y) { { x, y } }
	#else
		//////////////////////////////////////////////////////////////////////////
		// Creates a vector constant from its 4 components
		//////////////////////////////////////////////////////////////////////////
		#define RTM_VECTOR4F_MAKE(x, y, z, w) { x, y, z, w }

		//////////////////////////////////////////////////////////////////////////
		// Creates a vector constant from its 2 components
		//////////////////////////////////////////////////////////////////////////
		#define RTM_VECTOR2D_MAKE(x, y) { x, y }
	#endif
#elif defined(RTM_NEON_INTRINSICS)
	// RTM_VECTOR2D_SELECT not defined for NEON yet, TODO
#else
	// Macros not defined for scalar code path
#endif

RTM_IMPL_FILE_PRAGMA_POP

```

`includes/rtm/impl/mask_common.h`:

```h
#pragma once

////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2019 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include "rtm/math.h"
#include "rtm/version.h"
#include "rtm/impl/compiler_utils.h"

RTM_IMPL_FILE_PRAGMA_PUSH

namespace rtm
{
	RTM_IMPL_VERSION_NAMESPACE_BEGIN

	namespace rtm_impl
	{
		//////////////////////////////////////////////////////////////////////////
		// This is a helper struct to allow a single consistent API between
		// various vector types when the semantics are identical but the return
		// type differs. Implicit coercion is used to return the desired value
		// at the call site.
		//////////////////////////////////////////////////////////////////////////
		struct mask4_bool_set
		{
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator mask4d() const RTM_NO_EXCEPT
			{
#if defined(RTM_SSE2_INTRINSICS)
				const uint64_t x_mask = x ? 0xFFFFFFFFFFFFFFFFULL : 0;
				const uint64_t y_mask = y ? 0xFFFFFFFFFFFFFFFFULL : 0;
				const uint64_t z_mask = z ? 0xFFFFFFFFFFFFFFFFULL : 0;
				const uint64_t w_mask = w ? 0xFFFFFFFFFFFFFFFFULL : 0;

				return mask4d{ _mm_castsi128_pd(_mm_set_epi64x(static_cast<int64_t>(y_mask), static_cast<int64_t>(x_mask))), _mm_castsi128_pd(_mm_set_epi64x(static_cast<int64_t>(w_mask), static_cast<int64_t>(z_mask))) };
#else
				const uint64_t x_mask = x ? 0xFFFFFFFFFFFFFFFFULL : 0;
				const uint64_t y_mask = y ? 0xFFFFFFFFFFFFFFFFULL : 0;
				const uint64_t z_mask = z ? 0xFFFFFFFFFFFFFFFFULL : 0;
				const uint64_t w_mask = w ? 0xFFFFFFFFFFFFFFFFULL : 0;

				return mask4d{ x_mask, y_mask, z_mask, w_mask };
#endif
			}

			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator mask4q() const RTM_NO_EXCEPT
			{
#if defined(RTM_SSE2_INTRINSICS)
				const uint64_t x_mask = x ? 0xFFFFFFFFFFFFFFFFULL : 0;
				const uint64_t y_mask = y ? 0xFFFFFFFFFFFFFFFFULL : 0;
				const uint64_t z_mask = z ? 0xFFFFFFFFFFFFFFFFULL : 0;
				const uint64_t w_mask = w ? 0xFFFFFFFFFFFFFFFFULL : 0;

				return mask4q{ _mm_set_epi64x(static_cast<int64_t>(y_mask), static_cast<int64_t>(x_mask)), _mm_set_epi64x(static_cast<int64_t>(w_mask), static_cast<int64_t>(z_mask)) };
#else
				const uint64_t x_mask = x ? 0xFFFFFFFFFFFFFFFFULL : 0;
				const uint64_t y_mask = y ? 0xFFFFFFFFFFFFFFFFULL : 0;
				const uint64_t z_mask = z ? 0xFFFFFFFFFFFFFFFFULL : 0;
				const uint64_t w_mask = w ? 0xFFFFFFFFFFFFFFFFULL : 0;

				return mask4q{ x_mask, y_mask, z_mask, w_mask };
#endif
			}

			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator mask4f() const RTM_NO_EXCEPT
			{
				const uint32_t x_mask = x ? 0xFFFFFFFFU : 0;
				const uint32_t y_mask = y ? 0xFFFFFFFFU : 0;
				const uint32_t z_mask = z ? 0xFFFFFFFFU : 0;
				const uint32_t w_mask = w ? 0xFFFFFFFFU : 0;

#if defined(RTM_SSE2_INTRINSICS)
				return _mm_castsi128_ps(_mm_set_epi32(static_cast<int32_t>(w_mask), static_cast<int32_t>(z_mask), static_cast<int32_t>(y_mask), static_cast<int32_t>(x_mask)));
#elif defined(RTM_NEON_INTRINSICS)
				uint32x2_t V0 = vcreate_u32(((uint64_t)x_mask) | ((uint64_t)(y_mask) << 32));
				uint32x2_t V1 = vcreate_u32(((uint64_t)z_mask) | ((uint64_t)(w_mask) << 32));
				return vcombine_u32(V0, V1);
#else
				return mask4f{ x_mask, y_mask, z_mask, w_mask };
#endif
			}

			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator mask4i() const RTM_NO_EXCEPT
			{
				const uint32_t x_mask = x ? 0xFFFFFFFFU : 0;
				const uint32_t y_mask = y ? 0xFFFFFFFFU : 0;
				const uint32_t z_mask = z ? 0xFFFFFFFFU : 0;
				const uint32_t w_mask = w ? 0xFFFFFFFFU : 0;

#if defined(RTM_SSE2_INTRINSICS)
				return _mm_set_epi32(static_cast<int32_t>(w_mask), static_cast<int32_t>(z_mask), static_cast<int32_t>(y_mask), static_cast<int32_t>(x_mask));
#elif defined(RTM_NEON_INTRINSICS)
				uint32x2_t V0 = vcreate_u32(((uint64_t)x_mask) | ((uint64_t)(y_mask) << 32));
				uint32x2_t V1 = vcreate_u32(((uint64_t)z_mask) | ((uint64_t)(w_mask) << 32));
				return RTM_IMPL_MASK4i_SET(vcombine_u32(V0, V1));
#else
				return mask4i{ x_mask, y_mask, z_mask, w_mask };
#endif
			}

			bool x;
			bool y;
			bool z;
			bool w;
		};
	}

	//////////////////////////////////////////////////////////////////////////
	// Creates a mask4 from all 4 bool components.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr rtm_impl::mask4_bool_set RTM_SIMD_CALL mask_set(bool x, bool y, bool z, bool w) RTM_NO_EXCEPT
	{
		return rtm_impl::mask4_bool_set{ x, y, z, w };
	}

	namespace rtm_impl
	{
		//////////////////////////////////////////////////////////////////////////
		// This is a helper struct to allow a single consistent API between
		// various vector types when the semantics are identical but the return
		// type differs. Implicit coercion is used to return the desired value
		// at the call site.
		//////////////////////////////////////////////////////////////////////////
		struct mask4_uint32_set
		{
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator mask4f() const RTM_NO_EXCEPT
			{
#if defined(RTM_SSE2_INTRINSICS)
				return _mm_castsi128_ps(_mm_set_epi32(static_cast<int32_t>(w), static_cast<int32_t>(z), static_cast<int32_t>(y), static_cast<int32_t>(x)));
#elif defined(RTM_NEON_INTRINSICS)
				uint32x2_t V0 = vcreate_u32(((uint64_t)x) | ((uint64_t)(y) << 32));
				uint32x2_t V1 = vcreate_u32(((uint64_t)z) | ((uint64_t)(w) << 32));
				return vcombine_u32(V0, V1);
#else
				return mask4f{ x, y, z, w };
#endif
			}

			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator mask4i() const RTM_NO_EXCEPT
			{
#if defined(RTM_SSE2_INTRINSICS)
				return _mm_set_epi32(static_cast<int32_t>(w), static_cast<int32_t>(z), static_cast<int32_t>(y), static_cast<int32_t>(x));
#elif defined(RTM_NEON_INTRINSICS)
				uint32x2_t V0 = vcreate_u32(((uint64_t)x) | ((uint64_t)(y) << 32));
				uint32x2_t V1 = vcreate_u32(((uint64_t)z) | ((uint64_t)(w) << 32));
				return RTM_IMPL_MASK4i_SET(vcombine_u32(V0, V1));
#else
				return mask4i{ x, y, z, w };
#endif
			}

			uint32_t x;
			uint32_t y;
			uint32_t z;
			uint32_t w;
		};
	}

	//////////////////////////////////////////////////////////////////////////
	// Creates a mask4 from 4 uint32 components.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr rtm_impl::mask4_uint32_set RTM_SIMD_CALL mask_set(uint32_t x, uint32_t y, uint32_t z, uint32_t w) RTM_NO_EXCEPT
	{
		return rtm_impl::mask4_uint32_set{ x, y, z, w };
	}

	namespace rtm_impl
	{
		//////////////////////////////////////////////////////////////////////////
		// This is a helper struct to allow a single consistent API between
		// various vector types when the semantics are identical but the return
		// type differs. Implicit coercion is used to return the desired value
		// at the call site.
		//////////////////////////////////////////////////////////////////////////
		struct mask4_uint64_set
		{
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator mask4d() const RTM_NO_EXCEPT
			{
#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// HACK ALERT!
	// VS2015 and VS2017 crash when compiling with _mm_set_epi64x() here.
	// To work around this, we use alternative code. We assume that the high and low words
	// are identical in the mask, which should be true.
	// See: https://github.com/nfrechette/rtm/issues/84
	//////////////////////////////////////////////////////////////////////////
	#if defined(RTM_COMPILER_MSVC) && RTM_COMPILER_MSVC < RTM_COMPILER_MSVC_2019 && defined(RTM_ARCH_X86) && !defined(NDEBUG)
				const uint32_t x_mask = x ? 0xFFFFFFFFU : 0;
				const uint32_t y_mask = y ? 0xFFFFFFFFU : 0;
				const uint32_t z_mask = z ? 0xFFFFFFFFU : 0;
				const uint32_t w_mask = w ? 0xFFFFFFFFU : 0;

				return mask4d{ _mm_castsi128_pd(_mm_set_epi32(static_cast<int32_t>(y_mask), static_cast<int32_t>(y_mask), static_cast<int32_t>(x_mask), static_cast<int32_t>(x_mask))), _mm_castsi128_pd(_mm_set_epi32(static_cast<int32_t>(w_mask), static_cast<int32_t>(w_mask), static_cast<int32_t>(z_mask), static_cast<int32_t>(z_mask))) };
	#else
				return mask4d{ _mm_castsi128_pd(_mm_set_epi64x(static_cast<int64_t>(y), static_cast<int64_t>(x))), _mm_castsi128_pd(_mm_set_epi64x(static_cast<int64_t>(w), static_cast<int64_t>(z))) };
	#endif
#else
				return mask4d{ x, y, z, w };
#endif
			}

			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator mask4q() const RTM_NO_EXCEPT
			{
#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// HACK ALERT!
	// VS2015 and VS2017 crash when compiling with _mm_set_epi64x() here.
	// To work around this, we use alternative code. We assume that the high and low words
	// are identical in the mask, which should be true.
	// See: https://github.com/nfrechette/rtm/issues/84
	//////////////////////////////////////////////////////////////////////////
	#if defined(RTM_COMPILER_MSVC) && RTM_COMPILER_MSVC < RTM_COMPILER_MSVC_2019 && defined(RTM_ARCH_X86) && !defined(NDEBUG)
				const uint32_t x_mask = x ? 0xFFFFFFFFU : 0;
				const uint32_t y_mask = y ? 0xFFFFFFFFU : 0;
				const uint32_t z_mask = z ? 0xFFFFFFFFU : 0;
				const uint32_t w_mask = w ? 0xFFFFFFFFU : 0;

				return mask4q{ _mm_set_epi32(static_cast<int32_t>(y_mask), static_cast<int32_t>(y_mask), static_cast<int32_t>(x_mask), static_cast<int32_t>(x_mask)), _mm_set_epi32(static_cast<int32_t>(w_mask), static_cast<int32_t>(w_mask), static_cast<int32_t>(z_mask), static_cast<int32_t>(z_mask)) };
	#else
				return mask4q{ _mm_set_epi64x(static_cast<int64_t>(y), static_cast<int64_t>(x)), _mm_set_epi64x(static_cast<int64_t>(w), static_cast<int64_t>(z)) };
	#endif
#else
				return mask4q{ x, y, z, w };
#endif
			}

			uint64_t x;
			uint64_t y;
			uint64_t z;
			uint64_t w;
		};
	}

	//////////////////////////////////////////////////////////////////////////
	// Creates a mask4 from 4 uint64 components.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr rtm_impl::mask4_uint64_set RTM_SIMD_CALL mask_set(uint64_t x, uint64_t y, uint64_t z, uint64_t w) RTM_NO_EXCEPT
	{
		return rtm_impl::mask4_uint64_set{ x, y, z, w };
	}

	//////////////////////////////////////////////////////////////////////////
	// Creates a mask4 with all 4 components set to true.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr rtm_impl::mask4_bool_set RTM_SIMD_CALL mask_true() RTM_NO_EXCEPT
	{
		return rtm_impl::mask4_bool_set{ true, true, true, true };
	}

	//////////////////////////////////////////////////////////////////////////
	// Creates a mask4 with all 4 components set to false.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr rtm_impl::mask4_bool_set RTM_SIMD_CALL mask_false() RTM_NO_EXCEPT
	{
		return rtm_impl::mask4_bool_set{ false, false, false, false };
	}

	RTM_IMPL_VERSION_NAMESPACE_END
}

RTM_IMPL_FILE_PRAGMA_POP

```

`includes/rtm/impl/matrix_affine_common.h`:

```h
#pragma once

////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2018 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include "rtm/math.h"
#include "rtm/vector4f.h"
#include "rtm/vector4d.h"
#include "rtm/version.h"
#include "rtm/quatf.h"
#include "rtm/quatd.h"
#include "rtm/type_traits.h"
#include "rtm/impl/compiler_utils.h"

RTM_IMPL_FILE_PRAGMA_PUSH

namespace rtm
{
	RTM_IMPL_VERSION_NAMESPACE_BEGIN

	namespace rtm_impl
	{
		//////////////////////////////////////////////////////////////////////////
		// A helper struct to convert a quaternion to matrices of similar width.
		// Note: We use a float type as a template argument because GCC loses alignment
		// attributes on template argument types.
		//////////////////////////////////////////////////////////////////////////
		template<typename float_type>
		struct matrix_from_quat_helper
		{
			using quat = typename related_types<float_type>::quat;
			using vector4 = typename related_types<float_type>::vector4;
			using matrix3x3 = typename related_types<float_type>::matrix3x3;
			using matrix3x4 = typename related_types<float_type>::matrix3x4;

			RTM_DISABLE_SECURITY_COOKIE_CHECK inline RTM_SIMD_CALL operator matrix3x3() const RTM_NO_EXCEPT
			{
				RTM_ASSERT(quat_is_normalized(quat_input), "Quaternion is not normalized");

				const float_type x2 = quat_get_x(quat_input) + quat_get_x(quat_input);
				const float_type y2 = quat_get_y(quat_input) + quat_get_y(quat_input);
				const float_type z2 = quat_get_z(quat_input) + quat_get_z(quat_input);
				const float_type xx = quat_get_x(quat_input) * x2;
				const float_type xy = quat_get_x(quat_input) * y2;
				const float_type xz = quat_get_x(quat_input) * z2;
				const float_type yy = quat_get_y(quat_input) * y2;
				const float_type yz = quat_get_y(quat_input) * z2;
				const float_type zz = quat_get_z(quat_input) * z2;
				const float_type wx = quat_get_w(quat_input) * x2;
				const float_type wy = quat_get_w(quat_input) * y2;
				const float_type wz = quat_get_w(quat_input) * z2;

				const vector4 x_axis = vector_set(float_type(1.0) - (yy + zz), xy + wz, xz - wy, float_type(0.0));
				const vector4 y_axis = vector_set(xy - wz, float_type(1.0) - (xx + zz), yz + wx, float_type(0.0));
				const vector4 z_axis = vector_set(xz + wy, yz - wx, float_type(1.0) - (xx + yy), float_type(0.0));
				return matrix3x3{ x_axis, y_axis, z_axis };
			}

			RTM_DISABLE_SECURITY_COOKIE_CHECK inline RTM_SIMD_CALL operator matrix3x4() const RTM_NO_EXCEPT
			{
				RTM_ASSERT(quat_is_normalized(quat_input), "Quaternion is not normalized");

				const float_type x2 = quat_get_x(quat_input) + quat_get_x(quat_input);
				const float_type y2 = quat_get_y(quat_input) + quat_get_y(quat_input);
				const float_type z2 = quat_get_z(quat_input) + quat_get_z(quat_input);
				const float_type xx = quat_get_x(quat_input) * x2;
				const float_type xy = quat_get_x(quat_input) * y2;
				const float_type xz = quat_get_x(quat_input) * z2;
				const float_type yy = quat_get_y(quat_input) * y2;
				const float_type yz = quat_get_y(quat_input) * z2;
				const float_type zz = quat_get_z(quat_input) * z2;
				const float_type wx = quat_get_w(quat_input) * x2;
				const float_type wy = quat_get_w(quat_input) * y2;
				const float_type wz = quat_get_w(quat_input) * z2;

				const vector4 x_axis = vector_set(float_type(1.0) - (yy + zz), xy + wz, xz - wy, float_type(0.0));
				const vector4 y_axis = vector_set(xy - wz, float_type(1.0) - (xx + zz), yz + wx, float_type(0.0));
				const vector4 z_axis = vector_set(xz + wy, yz - wx, float_type(1.0) - (xx + yy), float_type(0.0));
				const vector4 w_axis = vector_zero();
				return matrix3x4{ x_axis, y_axis, z_axis, w_axis };
			}

			quat quat_input;
		};

		//////////////////////////////////////////////////////////////////////////
		// A helper struct to convert a 3D scale vector to matrices of similar width.
		// Note: We use a float type as a template argument because GCC loses alignment
		// attributes on template argument types.
		//////////////////////////////////////////////////////////////////////////
		template<typename float_type>
		struct matrix_from_scale_helper
		{
			using vector4 = typename related_types<float_type>::vector4;
			using matrix3x3 = typename related_types<float_type>::matrix3x3;
			using matrix3x4 = typename related_types<float_type>::matrix3x4;

			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator matrix3x3() const RTM_NO_EXCEPT
			{
				const vector4 zero = vector_zero();
				const vector4 x_axis = vector_mix<mix4::x, mix4::b, mix4::c, mix4::d>(scale, zero);
				const vector4 y_axis = vector_mix<mix4::a, mix4::y, mix4::c, mix4::d>(scale, zero);
				const vector4 z_axis = vector_mix<mix4::a, mix4::b, mix4::z, mix4::d>(scale, zero);
				return matrix3x3{ x_axis, y_axis, z_axis };
			}

			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator matrix3x4() const RTM_NO_EXCEPT
			{
				const vector4 zero = vector_zero();
				const vector4 x_axis = vector_mix<mix4::x, mix4::b, mix4::c, mix4::d>(scale, zero);
				const vector4 y_axis = vector_mix<mix4::a, mix4::y, mix4::c, mix4::d>(scale, zero);
				const vector4 z_axis = vector_mix<mix4::a, mix4::b, mix4::z, mix4::d>(scale, zero);
				return matrix3x4{ x_axis, y_axis, z_axis, zero };
			}

			vector4 scale;
		};

		RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr vector4f RTM_SIMD_CALL matrix_get_axis(vector4f_arg0 x_axis, vector4f_arg1 y_axis, vector4f_arg2 z_axis, axis3 axis)
		{
			return axis == axis3::x ? x_axis : (axis == axis3::y ? y_axis : z_axis);
		}

		RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr vector4d RTM_SIMD_CALL matrix_get_axis(vector4d_arg0 x_axis, vector4d_arg1 y_axis, vector4d_arg2 z_axis, axis3 axis)
		{
			return axis == axis3::x ? x_axis : (axis == axis3::y ? y_axis : z_axis);
		}

		//////////////////////////////////////////////////////////////////////////
		// Converts a 3x3 matrix into a rotation quaternion.
		//////////////////////////////////////////////////////////////////////////
		RTM_DISABLE_SECURITY_COOKIE_CHECK inline quatf RTM_SIMD_CALL quat_from_matrix(vector4f_arg0 x_axis, vector4f_arg1 y_axis, vector4f_arg2 z_axis) RTM_NO_EXCEPT
		{
			// TODO: Rework this function, we should be able to handle one axis with zero scale by using the largest diagonal element
			// See here for details: https://www.euclideanspace.com/maths/geometry/rotations/conversions/matrixToQuaternion/
			// We should be able to remove the case with trace > 0 as well by always using the diagonal derivation
			// This should give some result as well if at least one axis has non-zero scale.
			// Unclear what the result will be, if any, with a zero matrix

			const vector4f zero = vector_zero();
			if (vector_all_near_equal3(x_axis, zero) || vector_all_near_equal3(y_axis, zero) || vector_all_near_equal3(z_axis, zero))
				return quat_identity();	// Zero scale not supported, return the identity

			const float x_axis_x = vector_get_x(x_axis);
			const float y_axis_y = vector_get_y(y_axis);
			const float z_axis_z = vector_get_z(z_axis);

			const float mtx_trace = x_axis_x + y_axis_y + z_axis_z;
			if (mtx_trace > 0.0F)
			{
				const float x_axis_y = vector_get_y(x_axis);
				const float x_axis_z = vector_get_z(x_axis);

				const float y_axis_x = vector_get_x(y_axis);
				const float y_axis_z = vector_get_z(y_axis);

				const float z_axis_x = vector_get_x(z_axis);
				const float z_axis_y = vector_get_y(z_axis);

				const float inv_trace = scalar_sqrt_reciprocal(mtx_trace + 1.0F);
				const float half_inv_trace = inv_trace * 0.5F;

				const float x = (y_axis_z - z_axis_y) * half_inv_trace;
				const float y = (z_axis_x - x_axis_z) * half_inv_trace;
				const float z = (x_axis_y - y_axis_x) * half_inv_trace;
				const float w = scalar_reciprocal(inv_trace) * 0.5F;

				return quat_normalize(quat_set(x, y, z, w));
			}
			else
			{
				// Note that axis3::xyz have the same values as component4::xyz
				int32_t best_axis = (int32_t)axis4::x;
				if (y_axis_y > x_axis_x)
					best_axis = (int32_t)axis4::y;
				if (z_axis_z > vector_get_component(matrix_get_axis(x_axis, y_axis, z_axis, axis3(best_axis)), component4(best_axis)))
					best_axis = (int32_t)axis4::z;

				const int32_t next_best_axis = (best_axis + 1) % 3;
				const int32_t next_next_best_axis = (next_best_axis + 1) % 3;

				const float mtx_pseudo_trace = 1.0F +
					vector_get_component(matrix_get_axis(x_axis, y_axis, z_axis, axis3(best_axis)), component4(best_axis)) -
					vector_get_component(matrix_get_axis(x_axis, y_axis, z_axis, axis3(next_best_axis)), component4(next_best_axis)) -
					vector_get_component(matrix_get_axis(x_axis, y_axis, z_axis, axis3(next_next_best_axis)), component4(next_next_best_axis));

				const float inv_pseudo_trace = scalar_sqrt_reciprocal(mtx_pseudo_trace);
				const float half_inv_pseudo_trace = inv_pseudo_trace * 0.5F;

				float quat_values[4];
				quat_values[best_axis] = scalar_reciprocal(inv_pseudo_trace) * 0.5F;
				quat_values[next_best_axis] = half_inv_pseudo_trace *
					(vector_get_component(matrix_get_axis(x_axis, y_axis, z_axis, axis3(best_axis)), component4(next_best_axis)) +
						vector_get_component(matrix_get_axis(x_axis, y_axis, z_axis, axis3(next_best_axis)), component4(best_axis)));
				quat_values[next_next_best_axis] = half_inv_pseudo_trace *
					(vector_get_component(matrix_get_axis(x_axis, y_axis, z_axis, axis3(best_axis)), component4(next_next_best_axis)) +
						vector_get_component(matrix_get_axis(x_axis, y_axis, z_axis, axis3(next_next_best_axis)), component4(best_axis)));
				quat_values[3] = half_inv_pseudo_trace *
					(vector_get_component(matrix_get_axis(x_axis, y_axis, z_axis, axis3(next_best_axis)), component4(next_next_best_axis)) -
						vector_get_component(matrix_get_axis(x_axis, y_axis, z_axis, axis3(next_next_best_axis)), component4(next_best_axis)));

				return quat_normalize(quat_load(&quat_values[0]));
			}
		}

		//////////////////////////////////////////////////////////////////////////
		// Converts a 3x3 matrix into a rotation quaternion.
		//////////////////////////////////////////////////////////////////////////
		RTM_DISABLE_SECURITY_COOKIE_CHECK inline quatd RTM_SIMD_CALL quat_from_matrix(vector4d_arg0 x_axis, vector4d_arg1 y_axis, vector4d_arg2 z_axis) RTM_NO_EXCEPT
		{
			const vector4d zero = vector_zero();
			if (vector_all_near_equal3(x_axis, zero) || vector_all_near_equal3(y_axis, zero) || vector_all_near_equal3(z_axis, zero))
				return quat_identity();	// Zero scale not supported, return the identity

			const double x_axis_x = vector_get_x(x_axis);
			const double y_axis_y = vector_get_y(y_axis);
			const double z_axis_z = vector_get_z(z_axis);

			const double mtx_trace = x_axis_x + y_axis_y + z_axis_z;
			if (mtx_trace > 0.0)
			{
				const double x_axis_y = vector_get_y(x_axis);
				const double x_axis_z = vector_get_z(x_axis);

				const double y_axis_x = vector_get_x(y_axis);
				const double y_axis_z = vector_get_z(y_axis);

				const double z_axis_x = vector_get_x(z_axis);
				const double z_axis_y = vector_get_y(z_axis);

				const double inv_trace = scalar_sqrt_reciprocal(mtx_trace + 1.0);
				const double half_inv_trace = inv_trace * 0.5;

				const double x = (y_axis_z - z_axis_y) * half_inv_trace;
				const double y = (z_axis_x - x_axis_z) * half_inv_trace;
				const double z = (x_axis_y - y_axis_x) * half_inv_trace;
				const double w = scalar_reciprocal(inv_trace) * 0.5;

				return quat_normalize(quat_set(x, y, z, w));
			}
			else
			{
				// Note that axis3::xyz have the same values as component4::xyz
				int32_t best_axis = (int32_t)axis3::x;
				if (y_axis_y > x_axis_x)
					best_axis = (int32_t)axis3::y;
				if (z_axis_z > vector_get_component(matrix_get_axis(x_axis, y_axis, z_axis, axis3(best_axis)), component4(best_axis)))
					best_axis = (int32_t)axis3::z;

				const int32_t next_best_axis = (best_axis + 1) % 3;
				const int32_t next_next_best_axis = (next_best_axis + 1) % 3;

				const double mtx_pseudo_trace = 1.0 +
					vector_get_component(matrix_get_axis(x_axis, y_axis, z_axis, axis3(best_axis)), component4(best_axis)) -
					vector_get_component(matrix_get_axis(x_axis, y_axis, z_axis, axis3(next_best_axis)), component4(next_best_axis)) -
					vector_get_component(matrix_get_axis(x_axis, y_axis, z_axis, axis3(next_next_best_axis)), component4(next_next_best_axis));

				const double inv_pseudo_trace = scalar_sqrt_reciprocal(mtx_pseudo_trace);
				const double half_inv_pseudo_trace = inv_pseudo_trace * 0.5;

				double quat_values[4];
				quat_values[best_axis] = scalar_reciprocal(inv_pseudo_trace) * 0.5;
				quat_values[next_best_axis] = half_inv_pseudo_trace *
					(vector_get_component(matrix_get_axis(x_axis, y_axis, z_axis, axis3(best_axis)), component4(next_best_axis)) +
						vector_get_component(matrix_get_axis(x_axis, y_axis, z_axis, axis3(next_best_axis)), component4(best_axis)));
				quat_values[next_next_best_axis] = half_inv_pseudo_trace *
					(vector_get_component(matrix_get_axis(x_axis, y_axis, z_axis, axis3(best_axis)), component4(next_next_best_axis)) +
						vector_get_component(matrix_get_axis(x_axis, y_axis, z_axis, axis3(next_next_best_axis)), component4(best_axis)));
				quat_values[3] = half_inv_pseudo_trace *
					(vector_get_component(matrix_get_axis(x_axis, y_axis, z_axis, axis3(next_best_axis)), component4(next_next_best_axis)) -
						vector_get_component(matrix_get_axis(x_axis, y_axis, z_axis, axis3(next_next_best_axis)), component4(next_best_axis)));

				return quat_normalize(quat_load(&quat_values[0]));
			}
		}
	}

	//////////////////////////////////////////////////////////////////////////
	// Converts a rotation quaternion into a 3x3 or 3x4 affine matrix.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK constexpr rtm_impl::matrix_from_quat_helper<float> RTM_SIMD_CALL matrix_from_quat(quatf_arg0 quat) RTM_NO_EXCEPT
	{
		return rtm_impl::matrix_from_quat_helper<float>{ quat };
	}

	//////////////////////////////////////////////////////////////////////////
	// Converts a rotation quaternion into a 3x3 or 3x4 affine matrix.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK constexpr rtm_impl::matrix_from_quat_helper<double> RTM_SIMD_CALL matrix_from_quat(quatd_arg0 quat) RTM_NO_EXCEPT
	{
		return rtm_impl::matrix_from_quat_helper<double>{ quat };
	}

	//////////////////////////////////////////////////////////////////////////
	// Converts a rotation quaternion into a 3x3 or 3x4 affine matrix.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK constexpr rtm_impl::matrix_from_quat_helper<float> RTM_SIMD_CALL matrix_from_rotation(quatf_arg0 quat) RTM_NO_EXCEPT
	{
		return rtm_impl::matrix_from_quat_helper<float>{ quat };
	}

	//////////////////////////////////////////////////////////////////////////
	// Converts a rotation quaternion into a 3x3 or 3x4 affine matrix.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK constexpr rtm_impl::matrix_from_quat_helper<double> RTM_SIMD_CALL matrix_from_rotation(quatd_arg0 quat) RTM_NO_EXCEPT
	{
		return rtm_impl::matrix_from_quat_helper<double>{ quat };
	}

	//////////////////////////////////////////////////////////////////////////
	// Converts a 3D scale vector into a 3x3 or 3x4 affine matrix.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr rtm_impl::matrix_from_scale_helper<float> RTM_SIMD_CALL matrix_from_scale(vector4f_arg0 scale) RTM_NO_EXCEPT
	{
		return rtm_impl::matrix_from_scale_helper<float>{ scale };
	}

	//////////////////////////////////////////////////////////////////////////
	// Converts a 3D scale vector into a 3x3 or 3x4 affine matrix.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr rtm_impl::matrix_from_scale_helper<double> RTM_SIMD_CALL matrix_from_scale(vector4d_arg0 scale) RTM_NO_EXCEPT
	{
		return rtm_impl::matrix_from_scale_helper<double>{ scale };
	}

	RTM_IMPL_VERSION_NAMESPACE_END
}

RTM_IMPL_FILE_PRAGMA_POP

```

`includes/rtm/impl/matrix_cast.h`:

```h
#pragma once

////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2018 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include "rtm/math.h"
#include "rtm/vector4f.h"
#include "rtm/vector4d.h"
#include "rtm/version.h"
#include "rtm/impl/compiler_utils.h"

RTM_IMPL_FILE_PRAGMA_PUSH

namespace rtm
{
	RTM_IMPL_VERSION_NAMESPACE_BEGIN

	namespace rtm_impl
	{
		//////////////////////////////////////////////////////////////////////////
		// A helper struct to cast matrices with similar width.
		//////////////////////////////////////////////////////////////////////////
		template<typename src_matrix_type>
		struct matrix_caster {};

		template<>
		struct matrix_caster<matrix3x3f>
		{
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr explicit matrix_caster(const matrix3x3f& mtx_) RTM_NO_EXCEPT : mtx(mtx_) {}
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr explicit matrix_caster(matrix3x3f&& mtx_) RTM_NO_EXCEPT : mtx(std::move(mtx_)) {}

			matrix_caster(const matrix_caster&) = default;
			matrix_caster& operator=(const matrix_caster&) = delete;

			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr RTM_SIMD_CALL operator matrix3x3f() const RTM_NO_EXCEPT
			{
				return mtx;
			}

			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator matrix3x3d() const RTM_NO_EXCEPT
			{
				return matrix3x3d{ vector_cast(mtx.x_axis), vector_cast(mtx.y_axis), vector_cast(mtx.z_axis) };
			}

			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator matrix3x4f() const RTM_NO_EXCEPT
			{
				return matrix3x4f{ mtx.x_axis, mtx.y_axis, mtx.z_axis, (vector4f)vector_zero() };
			}

			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator matrix3x4d() const RTM_NO_EXCEPT
			{
				const vector4d z_axis = vector_cast(mtx.z_axis);
				return matrix3x4d{ vector_cast(mtx.x_axis), vector_cast(mtx.y_axis), z_axis, z_axis };
			}

			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr RTM_SIMD_CALL operator matrix4x4f() const RTM_NO_EXCEPT
			{
				return matrix4x4f{ mtx.x_axis, mtx.y_axis, mtx.z_axis, mtx.z_axis };
			}

			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator matrix4x4d() const RTM_NO_EXCEPT
			{
				const vector4d z_axis = vector_cast(mtx.z_axis);
				return matrix4x4d{ vector_cast(mtx.x_axis), vector_cast(mtx.y_axis), z_axis, z_axis };
			}

			const matrix3x3f& mtx;
		};

		template<>
		struct matrix_caster<matrix3x3d>
		{
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr explicit matrix_caster(const matrix3x3d& mtx_) RTM_NO_EXCEPT : mtx(mtx_) {}
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr explicit matrix_caster(matrix3x3d&& mtx_) RTM_NO_EXCEPT : mtx(std::move(mtx_)) {}

			matrix_caster(const matrix_caster&) = default;
			matrix_caster& operator=(const matrix_caster&) = delete;

			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator matrix3x3f() const RTM_NO_EXCEPT
			{
				return matrix3x3f{ vector_cast(mtx.x_axis), vector_cast(mtx.y_axis), vector_cast(mtx.z_axis) };
			}

			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr RTM_SIMD_CALL operator matrix3x3d() const RTM_NO_EXCEPT
			{
				return mtx;
			}

			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator matrix3x4f() const RTM_NO_EXCEPT
			{
				const vector4f z_axis = vector_cast(mtx.z_axis);
				return matrix3x4f{ vector_cast(mtx.x_axis), vector_cast(mtx.y_axis), z_axis, z_axis };
			}

			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator matrix3x4d() const RTM_NO_EXCEPT
			{
				return matrix3x4d{ mtx.x_axis, mtx.y_axis, mtx.z_axis, (vector4d)vector_zero() };
			}

			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator matrix4x4f() const RTM_NO_EXCEPT
			{
				const vector4f z_axis = vector_cast(mtx.z_axis);
				return matrix4x4f{ vector_cast(mtx.x_axis), vector_cast(mtx.y_axis), z_axis, z_axis };
			}

			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr RTM_SIMD_CALL operator matrix4x4d() const RTM_NO_EXCEPT
			{
				return matrix4x4d{ mtx.x_axis, mtx.y_axis, mtx.z_axis, mtx.z_axis };
			}

			const matrix3x3d& mtx;
		};

		template<>
		struct matrix_caster<matrix3x4f>
		{
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr explicit matrix_caster(const matrix3x4f& mtx_) RTM_NO_EXCEPT : mtx(mtx_) {}
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr explicit matrix_caster(matrix3x4f&& mtx_) RTM_NO_EXCEPT : mtx(std::move(mtx_)) {}

			matrix_caster(const matrix_caster&) = default;
			matrix_caster& operator=(const matrix_caster&) = delete;

			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr RTM_SIMD_CALL operator matrix3x3f() const RTM_NO_EXCEPT
			{
				return matrix3x3f{ mtx.x_axis, mtx.y_axis, mtx.z_axis };
			}

			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator matrix3x3d() const RTM_NO_EXCEPT
			{
				return matrix3x3d{ vector_cast(mtx.x_axis), vector_cast(mtx.y_axis), vector_cast(mtx.z_axis) };
			}

			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr RTM_SIMD_CALL operator matrix3x4f() const RTM_NO_EXCEPT
			{
				return mtx;
			}

			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator matrix3x4d() const RTM_NO_EXCEPT
			{
				return matrix3x4d{ vector_cast(mtx.x_axis), vector_cast(mtx.y_axis), vector_cast(mtx.z_axis), vector_cast(mtx.w_axis) };
			}

			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator matrix4x4f() const RTM_NO_EXCEPT
			{
				const scalarf zero = scalar_set(0.0F);
				const scalarf one = scalar_set(1.0F);
				return matrix4x4f{ vector_set_w(mtx.x_axis, zero), vector_set_w(mtx.y_axis, zero), vector_set_w(mtx.z_axis, zero), vector_set_w(mtx.w_axis, one) };
			}

			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator matrix4x4d() const RTM_NO_EXCEPT
			{
				const scalarf zero = scalar_set(0.0F);
				const scalarf one = scalar_set(1.0F);
				return matrix4x4d{ vector_cast(vector_set_w(mtx.x_axis, zero)), vector_cast(vector_set_w(mtx.y_axis, zero)), vector_cast(vector_set_w(mtx.z_axis, zero)), vector_cast(vector_set_w(mtx.w_axis, one)) };
			}

			const matrix3x4f& mtx;
		};

		template<>
		struct matrix_caster<matrix3x4d>
		{
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr explicit matrix_caster(const matrix3x4d& mtx_) RTM_NO_EXCEPT : mtx(mtx_) {}
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr explicit matrix_caster(matrix3x4d&& mtx_) RTM_NO_EXCEPT : mtx(std::move(mtx_)) {}

			matrix_caster(const matrix_caster&) = default;
			matrix_caster& operator=(const matrix_caster&) = delete;

			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator matrix3x3f() const RTM_NO_EXCEPT
			{
				return matrix3x3f{ vector_cast(mtx.x_axis), vector_cast(mtx.y_axis), vector_cast(mtx.z_axis) };
			}

			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr RTM_SIMD_CALL operator matrix3x3d() const RTM_NO_EXCEPT
			{
				return matrix3x3d{ mtx.x_axis, mtx.y_axis, mtx.z_axis };
			}

			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator matrix3x4f() const RTM_NO_EXCEPT
			{
				return matrix3x4f{ vector_cast(mtx.x_axis), vector_cast(mtx.y_axis), vector_cast(mtx.z_axis), vector_cast(mtx.w_axis) };
			}

			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr RTM_SIMD_CALL operator matrix3x4d() const RTM_NO_EXCEPT
			{
				return mtx;
			}

			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator matrix4x4f() const RTM_NO_EXCEPT
			{
				const scalard zero = scalar_set(0.0);
				const scalard one = scalar_set(1.0);
				return matrix4x4f{ vector_cast(vector_set_w(mtx.x_axis, zero)), vector_cast(vector_set_w(mtx.y_axis, zero)), vector_cast(vector_set_w(mtx.z_axis, zero)), vector_cast(vector_set_w(mtx.w_axis, one)) };
			}

			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator matrix4x4d() const RTM_NO_EXCEPT
			{
				const scalard zero = scalar_set(0.0);
				const scalard one = scalar_set(1.0);
				return matrix4x4d{ vector_set_w(mtx.x_axis, zero), vector_set_w(mtx.y_axis, zero), vector_set_w(mtx.z_axis, zero), vector_set_w(mtx.w_axis, one) };
			}

			const matrix3x4d& mtx;
		};

		template<>
		struct matrix_caster<matrix4x4f>
		{
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr explicit matrix_caster(const matrix4x4f& mtx_) RTM_NO_EXCEPT : mtx(mtx_) {}
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr explicit matrix_caster(matrix4x4f&& mtx_) RTM_NO_EXCEPT : mtx(std::move(mtx_)) {}

			matrix_caster(const matrix_caster&) = default;
			matrix_caster& operator=(const matrix_caster&) = delete;

			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr RTM_SIMD_CALL operator matrix3x3f() const RTM_NO_EXCEPT
			{
				return matrix3x3f{ mtx.x_axis, mtx.y_axis, mtx.z_axis };
			}

			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator matrix3x3d() const RTM_NO_EXCEPT
			{
				return matrix3x3d{ vector_cast(mtx.x_axis), vector_cast(mtx.y_axis), vector_cast(mtx.z_axis) };
			}

			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr RTM_SIMD_CALL operator matrix3x4f() const RTM_NO_EXCEPT
			{
				return matrix3x4f{ mtx.x_axis, mtx.y_axis, mtx.z_axis, mtx.w_axis };
			}

			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator matrix3x4d() const RTM_NO_EXCEPT
			{
				return matrix3x4d{ vector_cast(mtx.x_axis), vector_cast(mtx.y_axis), vector_cast(mtx.z_axis), vector_cast(mtx.w_axis) };
			}

			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr RTM_SIMD_CALL operator matrix4x4f() const RTM_NO_EXCEPT
			{
				return mtx;
			}

			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator matrix4x4d() const RTM_NO_EXCEPT
			{
				return matrix4x4d{ vector_cast(mtx.x_axis), vector_cast(mtx.y_axis), vector_cast(mtx.z_axis), vector_cast(mtx.w_axis) };
			}

			const matrix4x4f& mtx;
		};

		template<>
		struct matrix_caster<matrix4x4d>
		{
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr explicit matrix_caster(const matrix4x4d& mtx_) RTM_NO_EXCEPT : mtx(mtx_) {}
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr explicit matrix_caster(matrix4x4d&& mtx_) RTM_NO_EXCEPT : mtx(std::move(mtx_)) {}

			matrix_caster(const matrix_caster&) = default;
			matrix_caster& operator=(const matrix_caster&) = delete;

			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator matrix3x3f() const RTM_NO_EXCEPT
			{
				return matrix3x3f{ vector_cast(mtx.x_axis), vector_cast(mtx.y_axis), vector_cast(mtx.z_axis) };
			}

			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr RTM_SIMD_CALL operator matrix3x3d() const RTM_NO_EXCEPT
			{
				return matrix3x3d{ mtx.x_axis, mtx.y_axis, mtx.z_axis };
			}

			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator matrix3x4f() const RTM_NO_EXCEPT
			{
				return matrix3x4f{ vector_cast(mtx.x_axis), vector_cast(mtx.y_axis), vector_cast(mtx.z_axis), vector_cast(mtx.w_axis) };
			}

			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr RTM_SIMD_CALL operator matrix3x4d() const RTM_NO_EXCEPT
			{
				return matrix3x4d{ mtx.x_axis, mtx.y_axis, mtx.z_axis, mtx.w_axis };
			}

			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator matrix4x4f() const RTM_NO_EXCEPT
			{
				return matrix4x4f{ vector_cast(mtx.x_axis), vector_cast(mtx.y_axis), vector_cast(mtx.z_axis), vector_cast(mtx.w_axis) };
			}

			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr RTM_SIMD_CALL operator matrix4x4d() const RTM_NO_EXCEPT
			{
				return mtx;
			}

			const matrix4x4d& mtx;
		};
	}

	//////////////////////////////////////////////////////////////////////////
	// Casts a matrix from one variant to another.
	// Note: When casting a smaller matrix into a larger one, new elements are
	// undefined.
	//////////////////////////////////////////////////////////////////////////
	template<typename matrix_type>
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr rtm_impl::matrix_caster<matrix_type> RTM_SIMD_CALL matrix_cast(const matrix_type& input) RTM_NO_EXCEPT
	{
		return rtm_impl::matrix_caster<matrix_type>(input);
	}

	RTM_IMPL_VERSION_NAMESPACE_END
}

RTM_IMPL_FILE_PRAGMA_POP

```

`includes/rtm/impl/matrix_common.h`:

```h
#pragma once

////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2018 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include "rtm/math.h"
#include "rtm/vector4f.h"
#include "rtm/vector4d.h"
#include "rtm/version.h"
#include "rtm/type_traits.h"
#include "rtm/impl/compiler_utils.h"
#include "rtm/impl/matrix_cast.h"

RTM_IMPL_FILE_PRAGMA_PUSH

namespace rtm
{
	RTM_IMPL_VERSION_NAMESPACE_BEGIN

	namespace rtm_impl
	{
		//////////////////////////////////////////////////////////////////////////
		// This is a helper struct to allow a single consistent API between
		// various matrix types when the semantics are identical but the return
		// type differs. Implicit coercion is used to return the desired value
		// at the call site.
		//////////////////////////////////////////////////////////////////////////
		struct matrix_identity_impl
		{
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator matrix3x3d() const RTM_NO_EXCEPT
			{
				return matrix3x3d{ vector_set(1.0, 0.0, 0.0, 0.0), vector_set(0.0, 1.0, 0.0, 0.0), vector_set(0.0, 0.0, 1.0, 0.0) };
			}

			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator matrix3x3f() const RTM_NO_EXCEPT
			{
				return matrix3x3f{ vector_set(1.0F, 0.0F, 0.0F, 0.0F), vector_set(0.0F, 1.0F, 0.0F, 0.0F), vector_set(0.0F, 0.0F, 1.0F, 0.0F) };
			}

			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator matrix3x4d() const RTM_NO_EXCEPT
			{
				return matrix3x4d{ vector_set(1.0, 0.0, 0.0, 0.0), vector_set(0.0, 1.0, 0.0, 0.0), vector_set(0.0, 0.0, 1.0, 0.0), vector_set(0.0, 0.0, 0.0, 1.0) };
			}

			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator matrix3x4f() const RTM_NO_EXCEPT
			{
				return matrix3x4f{ vector_set(1.0F, 0.0F, 0.0F, 0.0F), vector_set(0.0F, 1.0F, 0.0F, 0.0F), vector_set(0.0F, 0.0F, 1.0F, 0.0F), vector_set(0.0F, 0.0F, 0.0F, 1.0F) };
			}

			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator matrix4x4d() const RTM_NO_EXCEPT
			{
				return matrix4x4d{ vector_set(1.0, 0.0, 0.0, 0.0), vector_set(0.0, 1.0, 0.0, 0.0), vector_set(0.0, 0.0, 1.0, 0.0), vector_set(0.0, 0.0, 0.0, 1.0) };
			}

			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator matrix4x4f() const RTM_NO_EXCEPT
			{
				return matrix4x4f{ vector_set(1.0F, 0.0F, 0.0F, 0.0F), vector_set(0.0F, 1.0F, 0.0F, 0.0F), vector_set(0.0F, 0.0F, 1.0F, 0.0F), vector_set(0.0F, 0.0F, 0.0F, 1.0F) };
			}
		};

		//////////////////////////////////////////////////////////////////////////
		// A helper struct to set matrices with similar width.
		// Note: We use a float type as a template argument because GCC loses alignment
		// attributes on template argument types.
		//////////////////////////////////////////////////////////////////////////
		template<typename float_type>
		struct matrix_setter4x4
		{
			using vector4 = typename related_types<float_type>::vector4;

			//////////////////////////////////////////////////////////////////////////
			// Sets all 4 axes and creates a 3x4 affine matrix.
			//////////////////////////////////////////////////////////////////////////
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr RTM_SIMD_CALL operator matrix3x4d() const RTM_NO_EXCEPT
			{
				return matrix3x4d{ x_axis, y_axis, z_axis, w_axis };
			}

			//////////////////////////////////////////////////////////////////////////
			// Sets all 4 axes and creates a 3x4 affine matrix.
			//////////////////////////////////////////////////////////////////////////
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr RTM_SIMD_CALL operator matrix3x4f() const RTM_NO_EXCEPT
			{
				return matrix3x4f{ x_axis, y_axis, z_axis, w_axis };
			}

			//////////////////////////////////////////////////////////////////////////
			// Sets all 4 axes and creates a 4x4 matrix.
			//////////////////////////////////////////////////////////////////////////
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr RTM_SIMD_CALL operator matrix4x4d() const RTM_NO_EXCEPT
			{
				return matrix4x4d{ x_axis, y_axis, z_axis, w_axis };
			}

			//////////////////////////////////////////////////////////////////////////
			// Sets all 4 axes and creates a 4x4 matrix.
			//////////////////////////////////////////////////////////////////////////
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr RTM_SIMD_CALL operator matrix4x4f() const RTM_NO_EXCEPT
			{
				return matrix4x4f{ x_axis, y_axis, z_axis, w_axis };
			}

			vector4	x_axis;
			vector4	y_axis;
			vector4	z_axis;
			vector4	w_axis;
		};
	}

	//////////////////////////////////////////////////////////////////////////
	// Sets all 3 axes and creates a matrix.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr matrix3x3f RTM_SIMD_CALL matrix_set(vector4f_arg0 x_axis, vector4f_arg1 y_axis, vector4f_arg2 z_axis) RTM_NO_EXCEPT
	{
		return matrix3x3f{ x_axis, y_axis, z_axis };
	}

	//////////////////////////////////////////////////////////////////////////
	// Sets all 3 axes and creates a matrix.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr matrix3x3d RTM_SIMD_CALL matrix_set(vector4d_arg0 x_axis, vector4d_arg1 y_axis, vector4d_arg2 z_axis) RTM_NO_EXCEPT
	{
		return matrix3x3d{ x_axis, y_axis, z_axis };
	}

	//////////////////////////////////////////////////////////////////////////
	// Sets all 4 axes and creates a matrix.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr rtm_impl::matrix_setter4x4<float> RTM_SIMD_CALL matrix_set(vector4f_arg0 x_axis, vector4f_arg1 y_axis, vector4f_arg2 z_axis, vector4f_arg3 w_axis) RTM_NO_EXCEPT
	{
		return rtm_impl::matrix_setter4x4<float>{ x_axis, y_axis, z_axis, w_axis };
	}

	//////////////////////////////////////////////////////////////////////////
	// Sets all 4 axes and creates a matrix.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr rtm_impl::matrix_setter4x4<double> RTM_SIMD_CALL matrix_set(vector4d_arg0 x_axis, vector4d_arg1 y_axis, vector4d_arg2 z_axis, vector4d_arg3 w_axis) RTM_NO_EXCEPT
	{
		return rtm_impl::matrix_setter4x4<double>{ x_axis, y_axis, z_axis, w_axis };
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the identity matrix.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr rtm_impl::matrix_identity_impl RTM_SIMD_CALL matrix_identity() RTM_NO_EXCEPT
	{
		return rtm_impl::matrix_identity_impl();
	}

	RTM_IMPL_VERSION_NAMESPACE_END
}

RTM_IMPL_FILE_PRAGMA_POP

```

`includes/rtm/impl/memory_utils.h`:

```h
#pragma once

////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2017 Nicholas Frechette & Animation Compression Library contributors
// Copyright (c) 2018 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include "rtm/version.h"
#include "rtm/impl/bit_cast.impl.h"
#include "rtm/impl/compiler_utils.h"
#include "rtm/impl/error.h"

#include <cstdint>
#include <cstring>
#include <type_traits>
#include <limits>
#include <memory>
#include <algorithm>

//////////////////////////////////////////////////////////////////////////
// This file contains various memory related utility functions and other misc helpers.
// Everything is hidden in an impl namespace even though they could be useful and used
// by anyone only to avoid polluting the rtm namespace. If these become useful enough,
// they might be moved into their own external dependency.
//////////////////////////////////////////////////////////////////////////

RTM_IMPL_FILE_PRAGMA_PUSH

namespace rtm
{
	RTM_IMPL_VERSION_NAMESPACE_BEGIN

	namespace rtm_impl
	{
		//////////////////////////////////////////////////////////////////////////
		// Allows static branching without any warnings
		//////////////////////////////////////////////////////////////////////////
		template<bool expression_result>
		struct static_condition { static constexpr bool test() RTM_NO_EXCEPT { return true; } };

		template<>
		struct static_condition<false> { static constexpr bool test() RTM_NO_EXCEPT { return false; } };

		//////////////////////////////////////////////////////////////////////////
		// Returns true if the input is a power of two, false otherwise.
		//////////////////////////////////////////////////////////////////////////
		RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr bool is_power_of_two(size_t input) RTM_NO_EXCEPT
		{
			return input != 0 && (input & (input - 1)) == 0;
		}

		//////////////////////////////////////////////////////////////////////////
		// Returns true if the alignment provided satisfies the requirement for the provided Type, false otherwise.
		//////////////////////////////////////////////////////////////////////////
		template<typename Type>
		RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr bool is_alignment_valid(size_t alignment) RTM_NO_EXCEPT
		{
			return is_power_of_two(alignment) && alignment >= alignof(Type);
		}

		//////////////////////////////////////////////////////////////////////////
		// Returns true if the pointer provided satisfies the specified alignment, false otherwise.
		//////////////////////////////////////////////////////////////////////////
		template<typename PtrType>
		RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool is_aligned_to(PtrType* value, size_t alignment) RTM_NO_EXCEPT
		{
			RTM_ASSERT(is_power_of_two(alignment), "Alignment value must be a power of two");
			return (bit_cast<intptr_t>(value) & (alignment - 1)) == 0;
		}

		//////////////////////////////////////////////////////////////////////////
		// Returns true if the integral value provided satisfies the specified alignment, false otherwise.
		//////////////////////////////////////////////////////////////////////////
		template<typename IntegralType>
		RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool is_aligned_to(IntegralType value, size_t alignment) RTM_NO_EXCEPT
		{
			RTM_ASSERT(is_power_of_two(alignment), "Alignment value must be a power of two");
			return (static_cast<size_t>(value) & (alignment - 1)) == 0;
		}

		//////////////////////////////////////////////////////////////////////////
		// Returns true if the provided pointer satisfies the alignment of PtrType, false otherwise.
		//////////////////////////////////////////////////////////////////////////
		template<typename PtrType>
		RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr bool is_aligned(PtrType* value) RTM_NO_EXCEPT
		{
			return is_aligned_to(value, alignof(PtrType));
		}

		//////////////////////////////////////////////////////////////////////////
		// The input pointer is rounded up to the desired alignment.
		//////////////////////////////////////////////////////////////////////////
		template<typename PtrType>
		RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE PtrType* align_to(PtrType* value, size_t alignment) RTM_NO_EXCEPT
		{
			RTM_ASSERT(is_power_of_two(alignment), "Alignment value must be a power of two");
			return bit_cast<PtrType*>((bit_cast<intptr_t>(value) + (alignment - 1)) & ~(alignment - 1));
		}

		//////////////////////////////////////////////////////////////////////////
		// The input integral value is rounded up to the desired alignment.
		//////////////////////////////////////////////////////////////////////////
		template<typename IntegralType>
		RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE IntegralType align_to(IntegralType value, size_t alignment) RTM_NO_EXCEPT
		{
			RTM_ASSERT(is_power_of_two(alignment), "Alignment value must be a power of two");
			return static_cast<IntegralType>((static_cast<size_t>(value) + (alignment - 1)) & ~(alignment - 1));
		}

		//////////////////////////////////////////////////////////////////////////
		// Returns the array size for the provided array.
		//////////////////////////////////////////////////////////////////////////
		template<typename ElementType, size_t num_elements>
		RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr size_t get_array_size(ElementType const (&)[num_elements]) RTM_NO_EXCEPT { return num_elements; }

		//////////////////////////////////////////////////////////////////////////
		// Type safe casting
		//////////////////////////////////////////////////////////////////////////
		template<typename DestPtrType, typename SrcType>
		struct safe_ptr_to_ptr_cast_impl
		{
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE static DestPtrType* cast(SrcType* input) RTM_NO_EXCEPT
			{
				RTM_ASSERT(is_aligned_to(input, alignof(DestPtrType)), "Cast would result in an unaligned pointer");
				return bit_cast<DestPtrType*>(input);
			}
		};

		template<typename SrcType>
		struct safe_ptr_to_ptr_cast_impl<void, SrcType>
		{
			RTM_DISABLE_SECURITY_COOKIE_CHECK static RTM_FORCE_INLINE constexpr void* cast(SrcType* input) RTM_NO_EXCEPT { return input; }
		};

		template<typename DestPtrType, typename SrcType>
		struct safe_int_to_ptr_cast_impl
		{
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE static DestPtrType* cast(SrcType input) RTM_NO_EXCEPT
			{
				RTM_ASSERT(is_aligned_to(input, alignof(DestPtrType)), "Cast would result in an unaligned pointer");
				return bit_cast<DestPtrType*>(input);
			}
		};

		template<typename SrcType>
		struct safe_int_to_ptr_cast_impl<void, SrcType>
		{
			RTM_DISABLE_SECURITY_COOKIE_CHECK static RTM_FORCE_INLINE constexpr void* cast(SrcType input) RTM_NO_EXCEPT { return bit_cast<void*>(input); }
		};

		template<typename DestPtrType, typename SrcType>
		RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE DestPtrType* safe_ptr_cast(SrcType* input) RTM_NO_EXCEPT
		{
			return safe_ptr_to_ptr_cast_impl<DestPtrType, SrcType>::cast(input);
		}

		template<typename DestPtrType, typename SrcType>
		RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE DestPtrType* safe_ptr_cast(SrcType input) RTM_NO_EXCEPT
		{
			return safe_int_to_ptr_cast_impl<DestPtrType, SrcType>::cast(input);
		}

#if defined(RTM_COMPILER_GCC)
		// GCC sometimes complains about comparisons being always true due to partial template
		// evaluation. Disable that warning since we know it is safe.
		#pragma GCC diagnostic push
		#pragma GCC diagnostic ignored "-Wtype-limits"
#endif

		template<typename Type, bool is_enum = true>
		struct safe_underlying_type { using type = typename std::underlying_type<Type>::type; };

		template<typename Type>
		struct safe_underlying_type<Type, false> { using type = Type; };

		template<typename DstType, typename SrcType, bool is_floating_point = false>
		struct is_static_cast_safe_s
		{
			RTM_DISABLE_SECURITY_COOKIE_CHECK static RTM_FORCE_INLINE bool test(SrcType input) RTM_NO_EXCEPT
			{
				using SrcRealType = typename safe_underlying_type<SrcType, std::is_enum<SrcType>::value>::type;

				if (static_condition<std::is_signed<DstType>::value == std::is_signed<SrcRealType>::value>::test())
					return SrcType(DstType(input)) == input;
				else if (static_condition<std::is_signed<SrcRealType>::value>::test())
					return int64_t(input) >= 0 && SrcType(DstType(input)) == input;
				else
					return uint64_t(input) <= uint64_t((std::numeric_limits<DstType>::max)());
			};
		};

		template<typename DstType, typename SrcType>
		struct is_static_cast_safe_s<DstType, SrcType, true>
		{
			RTM_DISABLE_SECURITY_COOKIE_CHECK static RTM_FORCE_INLINE bool test(SrcType input) RTM_NO_EXCEPT
			{
				return SrcType(DstType(input)) == input;
			}
		};

		template<typename DstType, typename SrcType>
		RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool is_static_cast_safe(SrcType input) RTM_NO_EXCEPT
		{
			// TODO: In C++17 this should be folded to constexpr if
			return is_static_cast_safe_s<DstType, SrcType, static_condition<std::is_floating_point<SrcType>::value || std::is_floating_point<DstType>::value>::test()>::test(input);
		}

		template<typename DstType, typename SrcType>
		RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE DstType safe_static_cast(SrcType input) RTM_NO_EXCEPT
		{
#if defined(RTM_HAS_ASSERT_CHECKS)
			const bool is_safe = is_static_cast_safe<DstType, SrcType>(input);
			RTM_ASSERT(is_safe, "Unsafe static cast resulted in data loss");
#endif

			return static_cast<DstType>(input);
		}

#if defined(RTM_COMPILER_GCC)
		#pragma GCC diagnostic pop
#endif

		//////////////////////////////////////////////////////////////////////////
		// Reads a DataType from the input pointer regardless of its alignment.
		//////////////////////////////////////////////////////////////////////////
		template<typename DataType>
		RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE DataType unaligned_read(const void* input) RTM_NO_EXCEPT
		{
			DataType result;
			std::memcpy(&result, input, sizeof(DataType));
			return result;
		}

		//////////////////////////////////////////////////////////////////////////
		// Reads a DataType from the input pointer with an alignment check.
		//////////////////////////////////////////////////////////////////////////
		template<typename DataType>
		RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE DataType aligned_read(const void* input) RTM_NO_EXCEPT
		{
			return *safe_ptr_cast<const DataType, const void*>(input);
		}

		//////////////////////////////////////////////////////////////////////////
		// Writes a DataType into the output pointer regardless of its alignment.
		//////////////////////////////////////////////////////////////////////////
		template<typename DataType>
		RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE void unaligned_write(DataType input, void* output) RTM_NO_EXCEPT
		{
			std::memcpy(output, &input, sizeof(DataType));
		}
	}

	RTM_IMPL_VERSION_NAMESPACE_END
}

RTM_IMPL_FILE_PRAGMA_POP

```

`includes/rtm/impl/quat_common.h`:

```h
#pragma once

////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2018 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include "rtm/math.h"
#include "rtm/version.h"
#include "rtm/impl/compiler_utils.h"

RTM_IMPL_FILE_PRAGMA_PUSH

namespace rtm
{
	RTM_IMPL_VERSION_NAMESPACE_BEGIN

	//////////////////////////////////////////////////////////////////////////
	// Creates a quaternion from all 4 components.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE quatf RTM_SIMD_CALL quat_set(float x, float y, float z, float w) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return _mm_set_ps(w, z, y, x);
#elif defined(RTM_NEON_INTRINSICS)
		float32x2_t V0 = vset_lane_f32(y, vmov_n_f32(x), 1);
		float32x2_t V1 = vset_lane_f32(w, vmov_n_f32(z), 1);
		return vcombine_f32(V0, V1);
#else
		return quatf{ x, y, z, w };
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Creates a quaternion from all 4 components.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE quatd RTM_SIMD_CALL quat_set(double x, double y, double z, double w) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return quatd{ _mm_set_pd(y, x), _mm_set_pd(w, z) };
#else
		return quatd{ x, y, z, w };
#endif
	}

	namespace rtm_impl
	{
		//////////////////////////////////////////////////////////////////////////
		// This is a helper struct to allow a single consistent API between
		// various quaternion types when the semantics are identical but the return
		// type differs. Implicit coercion is used to return the desired value
		// at the call site.
		//////////////////////////////////////////////////////////////////////////
		struct quat_identity_impl
		{
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator quatd() const RTM_NO_EXCEPT
			{
				return quat_set(0.0, 0.0, 0.0, 1.0);
			}

			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator quatf() const RTM_NO_EXCEPT
			{
				return quat_set(0.0F, 0.0F, 0.0F, 1.0F);
			}
		};
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the identity quaternion.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr rtm_impl::quat_identity_impl RTM_SIMD_CALL quat_identity() RTM_NO_EXCEPT
	{
		return rtm_impl::quat_identity_impl();
	}

	RTM_IMPL_VERSION_NAMESPACE_END
}

RTM_IMPL_FILE_PRAGMA_POP

```

`includes/rtm/impl/qv_common.h`:

```h
#pragma once

////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2023 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include "rtm/math.h"
#include "rtm/version.h"
#include "rtm/impl/compiler_utils.h"

RTM_IMPL_FILE_PRAGMA_PUSH

namespace rtm
{
	RTM_IMPL_VERSION_NAMESPACE_BEGIN

	//////////////////////////////////////////////////////////////////////////
	// Creates a QV transform from a rotation quaternion and a translation.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr qvf RTM_SIMD_CALL qv_set(quatf_arg0 rotation, vector4f_arg1 translation) RTM_NO_EXCEPT
	{
		return qvf{ rotation, translation };
	}

	//////////////////////////////////////////////////////////////////////////
	// Creates a QV transform from a rotation quaternion and a translation.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr qvd RTM_SIMD_CALL qv_set(quatd_arg0 rotation, vector4d_arg1 translation) RTM_NO_EXCEPT
	{
		return qvd{ rotation, translation };
	}

	namespace rtm_impl
	{
		//////////////////////////////////////////////////////////////////////////
		// This is a helper struct to allow a single consistent API between
		// various QV transform types when the semantics are identical but the return
		// type differs. Implicit coercion is used to return the desired value
		// at the call site.
		//////////////////////////////////////////////////////////////////////////
		struct qv_identity_impl
		{
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator qvd() const RTM_NO_EXCEPT
			{
				const quatd identity = quat_identity();
				const vector4d zero = vector_zero();
				return qv_set(identity, zero);
			}

			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator qvf() const RTM_NO_EXCEPT
			{
				const quatf identity = quat_identity();
				const vector4f zero = vector_zero();
				return qv_set(identity, zero);
			}
		};
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the identity QV transform.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr rtm_impl::qv_identity_impl RTM_SIMD_CALL qv_identity() RTM_NO_EXCEPT
	{
		return rtm_impl::qv_identity_impl();
	}

	RTM_IMPL_VERSION_NAMESPACE_END
}

RTM_IMPL_FILE_PRAGMA_POP

```

`includes/rtm/impl/qvs_common.h`:

```h
#pragma once

////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2023 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include "rtm/math.h"
#include "rtm/version.h"
#include "rtm/quatd.h"
#include "rtm/quatf.h"
#include "rtm/vector4d.h"
#include "rtm/vector4f.h"
#include "rtm/impl/compiler_utils.h"

RTM_IMPL_FILE_PRAGMA_PUSH

namespace rtm
{
	RTM_IMPL_VERSION_NAMESPACE_BEGIN

	//////////////////////////////////////////////////////////////////////////
	// Creates a QVS transform from a rotation quaternion, a translation, and scalar scale.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE qvsf RTM_SIMD_CALL qvs_set(quatf_arg0 rotation, vector4f_arg1 translation, float scale) RTM_NO_EXCEPT
	{
		const vector4f translation_scale = vector_set_w(translation, scale);
		return qvsf{ rotation, translation_scale };
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Creates a QVS transform from a rotation quaternion, a translation, and scalar scale.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE qvsf RTM_SIMD_CALL qvs_set(quatf_arg0 rotation, vector4f_arg1 translation, scalarf_arg2 scale) RTM_NO_EXCEPT
	{
		const vector4f translation_scale = vector_set_w(translation, scale);
		return qvsf{ rotation, translation_scale };
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Creates a QVS transform from a rotation quaternion, a translation, and scale scale.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE qvsd RTM_SIMD_CALL qvs_set(quatd_arg0 rotation, vector4d_arg1 translation, double scale) RTM_NO_EXCEPT
	{
		const vector4d translation_scale = vector_set_w(translation, scale);
		return qvsd{ rotation, translation_scale };
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Creates a QVS transform from a rotation quaternion, a translation, and scalar scale.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE qvsd RTM_SIMD_CALL qvs_set(quatd_arg0 rotation, vector4d_arg1 translation, scalard_arg5 scale) RTM_NO_EXCEPT
	{
		const vector4d translation_scale = vector_set_w(translation, scale);
		return qvsd{ rotation, translation_scale };
	}
#endif

	namespace rtm_impl
	{
		//////////////////////////////////////////////////////////////////////////
		// This is a helper struct to allow a single consistent API between
		// various QVS transform types when the semantics are identical but the return
		// type differs. Implicit coercion is used to return the desired value
		// at the call site.
		//////////////////////////////////////////////////////////////////////////
		struct qvs_identity_impl
		{
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator qvsd() const RTM_NO_EXCEPT
			{
				return qvsd{ (quatd)quat_identity(), vector_set(0.0, 0.0, 0.0, 1.0) };
			}

			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator qvsf() const RTM_NO_EXCEPT
			{
				return qvsf{ (quatf)quat_identity(), vector_set(0.0F, 0.0F, 0.0F, 1.0F) };
			}
		};
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the identity QVS transform.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr rtm_impl::qvs_identity_impl RTM_SIMD_CALL qvs_identity() RTM_NO_EXCEPT
	{
		return rtm_impl::qvs_identity_impl();
	}

	RTM_IMPL_VERSION_NAMESPACE_END
}

RTM_IMPL_FILE_PRAGMA_POP

```

`includes/rtm/impl/qvv_common.h`:

```h
#pragma once

////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2018 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include "rtm/math.h"
#include "rtm/version.h"
#include "rtm/impl/compiler_utils.h"

RTM_IMPL_FILE_PRAGMA_PUSH

namespace rtm
{
	RTM_IMPL_VERSION_NAMESPACE_BEGIN

	//////////////////////////////////////////////////////////////////////////
	// Creates a QVV transform from a rotation quaternion, a translation, and a 3D scale.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr qvvf RTM_SIMD_CALL qvv_set(quatf_arg0 rotation, vector4f_arg1 translation, vector4f_arg2 scale) RTM_NO_EXCEPT
	{
		return qvvf{ rotation, translation, scale };
	}

	//////////////////////////////////////////////////////////////////////////
	// Creates a QVV transform from a rotation quaternion, a translation, and a 3D scale.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr qvvd RTM_SIMD_CALL qvv_set(quatd_arg0 rotation, vector4d_arg1 translation, vector4d_arg2 scale) RTM_NO_EXCEPT
	{
		return qvvd{ rotation, translation, scale };
	}

	namespace rtm_impl
	{
		//////////////////////////////////////////////////////////////////////////
		// This is a helper struct to allow a single consistent API between
		// various QVV transform types when the semantics are identical but the return
		// type differs. Implicit coercion is used to return the desired value
		// at the call site.
		//////////////////////////////////////////////////////////////////////////
		struct qvv_identity_impl
		{
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator qvvd() const RTM_NO_EXCEPT
			{
				return qvv_set(quat_identity(), vector_zero(), vector_set(1.0));
			}

			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator qvvf() const RTM_NO_EXCEPT
			{
				return qvv_set(quat_identity(), vector_zero(), vector_set(1.0F));
			}
		};
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the identity QVV transform.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr rtm_impl::qvv_identity_impl RTM_SIMD_CALL qvv_identity() RTM_NO_EXCEPT
	{
		return rtm_impl::qvv_identity_impl();
	}

	RTM_IMPL_VERSION_NAMESPACE_END
}

RTM_IMPL_FILE_PRAGMA_POP

```

`includes/rtm/impl/scalar_common.h`:

```h
#pragma once

////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2019 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include "rtm/math.h"
#include "rtm/version.h"
#include "rtm/impl/compiler_utils.h"

RTM_IMPL_FILE_PRAGMA_PUSH

namespace rtm
{
	RTM_IMPL_VERSION_NAMESPACE_BEGIN

	namespace rtm_impl
	{
		//////////////////////////////////////////////////////////////////////////
		// This is a helper struct to allow a single consistent API between
		// various scalar types when the semantics are identical but the return
		// type differs. Implicit coercion is used to return the desired value
		// at the call site.
		//////////////////////////////////////////////////////////////////////////
		struct scalar_loaderf
		{
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr RTM_SIMD_CALL operator float() const RTM_NO_EXCEPT
			{
				return *ptr;
			}

#if defined(RTM_SSE2_INTRINSICS)
			RTM_DEPRECATED("Use 'as_scalar' suffix instead. To be removed in 2.4.")
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator scalarf() const RTM_NO_EXCEPT
			{
				return scalarf{ _mm_load_ss(ptr) };
			}
#endif

			const float* ptr;
		};

		//////////////////////////////////////////////////////////////////////////
		// This is a helper struct to allow a single consistent API between
		// various scalar types when the semantics are identical but the return
		// type differs. Implicit coercion is used to return the desired value
		// at the call site.
		//////////////////////////////////////////////////////////////////////////
		struct scalar_loaderd
		{
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr RTM_SIMD_CALL operator double() const RTM_NO_EXCEPT
			{
				return *ptr;
			}

#if defined(RTM_SSE2_INTRINSICS)
			RTM_DEPRECATED("Use 'as_scalar' suffix instead. To be removed in 2.4.")
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator scalard() const RTM_NO_EXCEPT
			{
				return scalard{ _mm_load_sd(ptr) };
			}
#endif

			const double* ptr;
		};
	}

	//////////////////////////////////////////////////////////////////////////
	// Loads a scalar from memory.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr rtm_impl::scalar_loaderf RTM_SIMD_CALL scalar_load(const float* input) RTM_NO_EXCEPT
	{
		return rtm_impl::scalar_loaderf{ input };
	}

	//////////////////////////////////////////////////////////////////////////
	// Loads a scalar from memory.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE scalarf RTM_SIMD_CALL scalar_load_as_scalar(const float* input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return scalarf{ _mm_load_ss(input) };
#else
		return scalar_load(input);
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Loads a scalar from memory.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr rtm_impl::scalar_loaderd RTM_SIMD_CALL scalar_load(const double* input) RTM_NO_EXCEPT
	{
		return rtm_impl::scalar_loaderd{ input };
	}

	//////////////////////////////////////////////////////////////////////////
	// Loads a scalar from memory.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE scalard RTM_SIMD_CALL scalar_load_as_scalar(const double* input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return scalard{ _mm_load_sd(input) };
#else
		return scalar_load(input);
#endif
	}

	RTM_IMPL_VERSION_NAMESPACE_END
}

RTM_IMPL_FILE_PRAGMA_POP

```

`includes/rtm/impl/type_args.h`:

```h
#pragma once

////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2018 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include "rtm/types.h"
#include "rtm/version.h"

//////////////////////////////////////////////////////////////////////////
// Register passing typedefs
//////////////////////////////////////////////////////////////////////////

#if defined(RTM_USE_VECTORCALL)
	#include "rtm/impl/type_args.vectorcall.impl.h"
#elif defined(RTM_NEON64_INTRINSICS)
	#include "rtm/impl/type_args.neon64.impl.h"
#elif defined(RTM_NEON_INTRINSICS)
	#include "rtm/impl/type_args.neon.impl.h"
#elif defined(RTM_ARCH_X64) && defined(RTM_COMPILER_GCC)
	#include "rtm/impl/type_args.x64_gcc.impl.h"
#elif defined(RTM_ARCH_X64) && defined(RTM_COMPILER_CLANG)
	#include "rtm/impl/type_args.x64_clang.impl.h"
#else
	#include "rtm/impl/type_args.other.impl.h"
#endif

```

`includes/rtm/impl/type_args.neon.impl.h`:

```h
#pragma once

////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2024 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include "rtm/types.h"
#include "rtm/version.h"

namespace rtm
{
	RTM_IMPL_VERSION_NAMESPACE_BEGIN

	//////////////////////////////////////////////////////////////////////////
	// Register passing typedefs
	//////////////////////////////////////////////////////////////////////////

#if defined(RTM_NEON_INTRINSICS)
	// On ARM NEON, the first 4x vector4f/quatf arguments can be passed by value in a register,
	// everything else afterwards is passed by const&. They can also be returned by register.

	using vector4f_arg0 = const vector4f;
	using vector4f_arg1 = const vector4f;
	using vector4f_arg2 = const vector4f;
	using vector4f_arg3 = const vector4f;
	using vector4f_arg4 = const vector4f&;
	using vector4f_arg5 = const vector4f&;
	using vector4f_arg6 = const vector4f&;
	using vector4f_arg7 = const vector4f&;
	using vector4f_argn = const vector4f&;

	using quatf_arg0 = const quatf;
	using quatf_arg1 = const quatf;
	using quatf_arg2 = const quatf;
	using quatf_arg3 = const quatf;
	using quatf_arg4 = const quatf&;
	using quatf_arg5 = const quatf&;
	using quatf_arg6 = const quatf&;
	using quatf_arg7 = const quatf&;
	using quatf_argn = const quatf&;

	using scalarf_arg0 = const scalarf;
	using scalarf_arg1 = const scalarf;
	using scalarf_arg2 = const scalarf;
	using scalarf_arg3 = const scalarf;
	using scalarf_arg4 = const scalarf;
	using scalarf_arg5 = const scalarf;
	using scalarf_arg6 = const scalarf;
	using scalarf_arg7 = const scalarf;
	using scalarf_argn = const scalarf;

	using scalard_arg0 = const scalard;
	using scalard_arg1 = const scalard;
	using scalard_arg2 = const scalard;
	using scalard_arg3 = const scalard;
	using scalard_arg4 = const scalard;
	using scalard_arg5 = const scalard;
	using scalard_arg6 = const scalard;
	using scalard_arg7 = const scalard;
	using scalard_argn = const scalard;

	using mask4f_arg0 = const mask4f;
	using mask4f_arg1 = const mask4f;
	using mask4f_arg2 = const mask4f;
	using mask4f_arg3 = const mask4f;
	using mask4f_arg4 = const mask4f&;
	using mask4f_arg5 = const mask4f&;
	using mask4f_arg6 = const mask4f&;
	using mask4f_arg7 = const mask4f&;
	using mask4f_argn = const mask4f&;

	using mask4i_arg0 = const mask4i;
	using mask4i_arg1 = const mask4i;
	using mask4i_arg2 = const mask4i;
	using mask4i_arg3 = const mask4i;
	using mask4i_arg4 = const mask4i&;
	using mask4i_arg5 = const mask4i&;
	using mask4i_arg6 = const mask4i&;
	using mask4i_arg7 = const mask4i&;
	using mask4i_argn = const mask4i&;

	// ARM NEON does not support passing aggregates by register
	// Non-scalar types using doubles are aggregate types

	using vector4d_arg0 = const vector4d&;
	using vector4d_arg1 = const vector4d&;
	using vector4d_arg2 = const vector4d&;
	using vector4d_arg3 = const vector4d&;
	using vector4d_arg4 = const vector4d&;
	using vector4d_arg5 = const vector4d&;
	using vector4d_arg6 = const vector4d&;
	using vector4d_arg7 = const vector4d&;
	using vector4d_argn = const vector4d&;

	using quatd_arg0 = const quatd&;
	using quatd_arg1 = const quatd&;
	using quatd_arg2 = const quatd&;
	using quatd_arg3 = const quatd&;
	using quatd_arg4 = const quatd&;
	using quatd_arg5 = const quatd&;
	using quatd_arg6 = const quatd&;
	using quatd_arg7 = const quatd&;
	using quatd_argn = const quatd&;

	using mask4d_arg0 = const mask4d&;
	using mask4d_arg1 = const mask4d&;
	using mask4d_arg2 = const mask4d&;
	using mask4d_arg3 = const mask4d&;
	using mask4d_arg4 = const mask4d&;
	using mask4d_arg5 = const mask4d&;
	using mask4d_arg6 = const mask4d&;
	using mask4d_arg7 = const mask4d&;
	using mask4d_argn = const mask4d&;

	using mask4q_arg0 = const mask4q&;
	using mask4q_arg1 = const mask4q&;
	using mask4q_arg2 = const mask4q&;
	using mask4q_arg3 = const mask4q&;
	using mask4q_arg4 = const mask4q&;
	using mask4q_arg5 = const mask4q&;
	using mask4q_arg6 = const mask4q&;
	using mask4q_arg7 = const mask4q&;
	using mask4q_argn = const mask4q&;

	using qvf_arg0 = const qvf&;
	using qvf_arg1 = const qvf&;
	using qvf_argn = const qvf&;

	using qvd_arg0 = const qvd&;
	using qvd_arg1 = const qvd&;
	using qvd_argn = const qvd&;

	using qvsf_arg0 = const qvsf&;
	using qvsf_arg1 = const qvsf&;
	using qvsf_argn = const qvsf&;

	using qvsd_arg0 = const qvsd&;
	using qvsd_arg1 = const qvsd&;
	using qvsd_argn = const qvsd&;

	using qvvf_arg0 = const qvvf&;
	using qvvf_arg1 = const qvvf&;
	using qvvf_argn = const qvvf&;

	using qvvd_arg0 = const qvvd&;
	using qvvd_arg1 = const qvvd&;
	using qvvd_argn = const qvvd&;

	using matrix3x3f_arg0 = const matrix3x3f&;
	using matrix3x3f_arg1 = const matrix3x3f&;
	using matrix3x3f_argn = const matrix3x3f&;

	using matrix3x3d_arg0 = const matrix3x3d&;
	using matrix3x3d_arg1 = const matrix3x3d&;
	using matrix3x3d_argn = const matrix3x3d&;

	using matrix3x4f_arg0 = const matrix3x4f&;
	using matrix3x4f_arg1 = const matrix3x4f&;
	using matrix3x4f_argn = const matrix3x4f&;

	using matrix3x4d_arg0 = const matrix3x4d&;
	using matrix3x4d_arg1 = const matrix3x4d&;
	using matrix3x4d_argn = const matrix3x4d&;

	using matrix4x4f_arg0 = const matrix4x4f&;
	using matrix4x4f_arg1 = const matrix4x4f&;
	using matrix4x4f_argn = const matrix4x4f&;

	using matrix4x4d_arg0 = const matrix4x4d&;
	using matrix4x4d_arg1 = const matrix4x4d&;
	using matrix4x4d_argn = const matrix4x4d&;
#endif

	RTM_IMPL_VERSION_NAMESPACE_END
}

```

`includes/rtm/impl/type_args.neon64.impl.h`:

```h
#pragma once

////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2024 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include "rtm/types.h"
#include "rtm/version.h"

namespace rtm
{
	RTM_IMPL_VERSION_NAMESPACE_BEGIN

	//////////////////////////////////////////////////////////////////////////
	// Register passing typedefs
	//////////////////////////////////////////////////////////////////////////

#if defined(RTM_NEON64_INTRINSICS)
	// On ARM64 NEON, the first 8x vector4f/quatf arguments can be passed by value in a register,
	// everything else afterwards is passed by const&. They can also be returned by register.

	using vector4f_arg0 = const vector4f;
	using vector4f_arg1 = const vector4f;
	using vector4f_arg2 = const vector4f;
	using vector4f_arg3 = const vector4f;
	using vector4f_arg4 = const vector4f;
	using vector4f_arg5 = const vector4f;
	using vector4f_arg6 = const vector4f;
	using vector4f_arg7 = const vector4f;
	using vector4f_argn = const vector4f&;

	using quatf_arg0 = const quatf;
	using quatf_arg1 = const quatf;
	using quatf_arg2 = const quatf;
	using quatf_arg3 = const quatf;
	using quatf_arg4 = const quatf;
	using quatf_arg5 = const quatf;
	using quatf_arg6 = const quatf;
	using quatf_arg7 = const quatf;
	using quatf_argn = const quatf&;

	using scalarf_arg0 = const scalarf;
	using scalarf_arg1 = const scalarf;
	using scalarf_arg2 = const scalarf;
	using scalarf_arg3 = const scalarf;
	using scalarf_arg4 = const scalarf;
	using scalarf_arg5 = const scalarf;
	using scalarf_arg6 = const scalarf;
	using scalarf_arg7 = const scalarf;
	using scalarf_argn = const scalarf;

	using scalard_arg0 = const scalard;
	using scalard_arg1 = const scalard;
	using scalard_arg2 = const scalard;
	using scalard_arg3 = const scalard;
	using scalard_arg4 = const scalard;
	using scalard_arg5 = const scalard;
	using scalard_arg6 = const scalard;
	using scalard_arg7 = const scalard;
	using scalard_argn = const scalard;

	using mask4f_arg0 = const mask4f;
	using mask4f_arg1 = const mask4f;
	using mask4f_arg2 = const mask4f;
	using mask4f_arg3 = const mask4f;
	using mask4f_arg4 = const mask4f;
	using mask4f_arg5 = const mask4f;
	using mask4f_arg6 = const mask4f;
	using mask4f_arg7 = const mask4f;
	using mask4f_argn = const mask4f&;

	using mask4i_arg0 = const mask4i;
	using mask4i_arg1 = const mask4i;
	using mask4i_arg2 = const mask4i;
	using mask4i_arg3 = const mask4i;
	using mask4i_arg4 = const mask4i;
	using mask4i_arg5 = const mask4i;
	using mask4i_arg6 = const mask4i;
	using mask4i_arg7 = const mask4i;
	using mask4i_argn = const mask4i&;

	// With ARM64 NEON, vector aggregates are also passed by register but the whole aggregate
	// must fit in the number of registers available (e.g. we can pass 2x qvvf but not 3x).
	// A qvvf can also be returned by register.
	// Non-scalar types using doubles are aggregate types

	using vector4d_arg0 = const vector4d;
	using vector4d_arg1 = const vector4d;
	using vector4d_arg2 = const vector4d;
	using vector4d_arg3 = const vector4d;
	using vector4d_arg4 = const vector4d&;
	using vector4d_arg5 = const vector4d&;
	using vector4d_arg6 = const vector4d&;
	using vector4d_arg7 = const vector4d&;
	using vector4d_argn = const vector4d&;

	using quatd_arg0 = const quatd;
	using quatd_arg1 = const quatd;
	using quatd_arg2 = const quatd;
	using quatd_arg3 = const quatd;
	using quatd_arg4 = const quatd&;
	using quatd_arg5 = const quatd&;
	using quatd_arg6 = const quatd&;
	using quatd_arg7 = const quatd&;
	using quatd_argn = const quatd&;

	using mask4d_arg0 = const mask4d;
	using mask4d_arg1 = const mask4d;
	using mask4d_arg2 = const mask4d;
	using mask4d_arg3 = const mask4d;
	using mask4d_arg4 = const mask4d&;
	using mask4d_arg5 = const mask4d&;
	using mask4d_arg6 = const mask4d&;
	using mask4d_arg7 = const mask4d&;
	using mask4d_argn = const mask4d&;

	using mask4q_arg0 = const mask4q;
	using mask4q_arg1 = const mask4q;
	using mask4q_arg2 = const mask4q;
	using mask4q_arg3 = const mask4q;
	using mask4q_arg4 = const mask4q&;
	using mask4q_arg5 = const mask4q&;
	using mask4q_arg6 = const mask4q&;
	using mask4q_arg7 = const mask4q&;
	using mask4q_argn = const mask4q&;

	using qvf_arg0 = const qvf;
	using qvf_arg1 = const qvf;
	using qvf_argn = const qvf&;

	using qvd_arg0 = const qvd;
	using qvd_arg1 = const qvd&;
	using qvd_argn = const qvd&;

	using qvsf_arg0 = const qvsf;
	using qvsf_arg1 = const qvsf;
	using qvsf_argn = const qvsf&;

	using qvsd_arg0 = const qvsd;
	using qvsd_arg1 = const qvsd&;
	using qvsd_argn = const qvsd&;

	using qvvf_arg0 = const qvvf;
	using qvvf_arg1 = const qvvf;
	using qvvf_argn = const qvvf&;

	using qvvd_arg0 = const qvvd;
	using qvvd_arg1 = const qvvd&;
	using qvvd_argn = const qvvd&;

	using matrix3x3f_arg0 = const matrix3x3f;
	using matrix3x3f_arg1 = const matrix3x3f;
	using matrix3x3f_argn = const matrix3x3f&;

	using matrix3x3d_arg0 = const matrix3x3d;
	using matrix3x3d_arg1 = const matrix3x3d&;
	using matrix3x3d_argn = const matrix3x3d&;

	using matrix3x4f_arg0 = const matrix3x4f;
	using matrix3x4f_arg1 = const matrix3x4f;
	using matrix3x4f_argn = const matrix3x4f&;

	using matrix3x4d_arg0 = const matrix3x4d;
	using matrix3x4d_arg1 = const matrix3x4d&;
	using matrix3x4d_argn = const matrix3x4d&;

	using matrix4x4f_arg0 = const matrix4x4f;
	using matrix4x4f_arg1 = const matrix4x4f;
	using matrix4x4f_argn = const matrix4x4f&;

	using matrix4x4d_arg0 = const matrix4x4d;
	using matrix4x4d_arg1 = const matrix4x4d&;
	using matrix4x4d_argn = const matrix4x4d&;
#endif

	RTM_IMPL_VERSION_NAMESPACE_END
}

```

`includes/rtm/impl/type_args.other.impl.h`:

```h
#pragma once

////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2024 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include "rtm/types.h"
#include "rtm/version.h"

namespace rtm
{
	RTM_IMPL_VERSION_NAMESPACE_BEGIN

	//////////////////////////////////////////////////////////////////////////
	// Register passing typedefs
	//////////////////////////////////////////////////////////////////////////

	// On every other platform, everything is passed by const&

	using vector4f_arg0 = const vector4f&;
	using vector4f_arg1 = const vector4f&;
	using vector4f_arg2 = const vector4f&;
	using vector4f_arg3 = const vector4f&;
	using vector4f_arg4 = const vector4f&;
	using vector4f_arg5 = const vector4f&;
	using vector4f_arg6 = const vector4f&;
	using vector4f_arg7 = const vector4f&;
	using vector4f_argn = const vector4f&;

	using vector4d_arg0 = const vector4d&;
	using vector4d_arg1 = const vector4d&;
	using vector4d_arg2 = const vector4d&;
	using vector4d_arg3 = const vector4d&;
	using vector4d_arg4 = const vector4d&;
	using vector4d_arg5 = const vector4d&;
	using vector4d_arg6 = const vector4d&;
	using vector4d_arg7 = const vector4d&;
	using vector4d_argn = const vector4d&;

	using quatf_arg0 = const quatf&;
	using quatf_arg1 = const quatf&;
	using quatf_arg2 = const quatf&;
	using quatf_arg3 = const quatf&;
	using quatf_arg4 = const quatf&;
	using quatf_arg5 = const quatf&;
	using quatf_arg6 = const quatf&;
	using quatf_arg7 = const quatf&;
	using quatf_argn = const quatf&;

	using quatd_arg0 = const quatd&;
	using quatd_arg1 = const quatd&;
	using quatd_arg2 = const quatd&;
	using quatd_arg3 = const quatd&;
	using quatd_arg4 = const quatd&;
	using quatd_arg5 = const quatd&;
	using quatd_arg6 = const quatd&;
	using quatd_arg7 = const quatd&;
	using quatd_argn = const quatd&;

	using scalarf_arg0 = const scalarf;
	using scalarf_arg1 = const scalarf;
	using scalarf_arg2 = const scalarf;
	using scalarf_arg3 = const scalarf;
	using scalarf_arg4 = const scalarf;
	using scalarf_arg5 = const scalarf;
	using scalarf_arg6 = const scalarf;
	using scalarf_arg7 = const scalarf;
	using scalarf_argn = const scalarf;

	using scalard_arg0 = const scalard;
	using scalard_arg1 = const scalard;
	using scalard_arg2 = const scalard;
	using scalard_arg3 = const scalard;
	using scalard_arg4 = const scalard;
	using scalard_arg5 = const scalard;
	using scalard_arg6 = const scalard;
	using scalard_arg7 = const scalard;
	using scalard_argn = const scalard;

	using mask4f_arg0 = const mask4f&;
	using mask4f_arg1 = const mask4f&;
	using mask4f_arg2 = const mask4f&;
	using mask4f_arg3 = const mask4f&;
	using mask4f_arg4 = const mask4f&;
	using mask4f_arg5 = const mask4f&;
	using mask4f_arg6 = const mask4f&;
	using mask4f_arg7 = const mask4f&;
	using mask4f_argn = const mask4f&;

	using mask4d_arg0 = const mask4d&;
	using mask4d_arg1 = const mask4d&;
	using mask4d_arg2 = const mask4d&;
	using mask4d_arg3 = const mask4d&;
	using mask4d_arg4 = const mask4d&;
	using mask4d_arg5 = const mask4d&;
	using mask4d_arg6 = const mask4d&;
	using mask4d_arg7 = const mask4d&;
	using mask4d_argn = const mask4d&;

	using mask4i_arg0 = const mask4i&;
	using mask4i_arg1 = const mask4i&;
	using mask4i_arg2 = const mask4i&;
	using mask4i_arg3 = const mask4i&;
	using mask4i_arg4 = const mask4i&;
	using mask4i_arg5 = const mask4i&;
	using mask4i_arg6 = const mask4i&;
	using mask4i_arg7 = const mask4i&;
	using mask4i_argn = const mask4i&;

	using mask4q_arg0 = const mask4q&;
	using mask4q_arg1 = const mask4q&;
	using mask4q_arg2 = const mask4q&;
	using mask4q_arg3 = const mask4q&;
	using mask4q_arg4 = const mask4q&;
	using mask4q_arg5 = const mask4q&;
	using mask4q_arg6 = const mask4q&;
	using mask4q_arg7 = const mask4q&;
	using mask4q_argn = const mask4q&;

	using qvf_arg0 = const qvf&;
	using qvf_arg1 = const qvf&;
	using qvf_argn = const qvf&;

	using qvd_arg0 = const qvd&;
	using qvd_arg1 = const qvd&;
	using qvd_argn = const qvd&;

	using qvsf_arg0 = const qvsf&;
	using qvsf_arg1 = const qvsf&;
	using qvsf_argn = const qvsf&;

	using qvsd_arg0 = const qvsd&;
	using qvsd_arg1 = const qvsd&;
	using qvsd_argn = const qvsd&;

	using qvvf_arg0 = const qvvf&;
	using qvvf_arg1 = const qvvf&;
	using qvvf_argn = const qvvf&;

	using qvvd_arg0 = const qvvd&;
	using qvvd_arg1 = const qvvd&;
	using qvvd_argn = const qvvd&;

	using matrix3x3f_arg0 = const matrix3x3f&;
	using matrix3x3f_arg1 = const matrix3x3f&;
	using matrix3x3f_argn = const matrix3x3f&;

	using matrix3x3d_arg0 = const matrix3x3d&;
	using matrix3x3d_arg1 = const matrix3x3d&;
	using matrix3x3d_argn = const matrix3x3d&;

	using matrix3x4f_arg0 = const matrix3x4f&;
	using matrix3x4f_arg1 = const matrix3x4f&;
	using matrix3x4f_argn = const matrix3x4f&;

	using matrix3x4d_arg0 = const matrix3x4d&;
	using matrix3x4d_arg1 = const matrix3x4d&;
	using matrix3x4d_argn = const matrix3x4d&;

	using matrix4x4f_arg0 = const matrix4x4f&;
	using matrix4x4f_arg1 = const matrix4x4f&;
	using matrix4x4f_argn = const matrix4x4f&;

	using matrix4x4d_arg0 = const matrix4x4d&;
	using matrix4x4d_arg1 = const matrix4x4d&;
	using matrix4x4d_argn = const matrix4x4d&;

	RTM_IMPL_VERSION_NAMESPACE_END
}

```

`includes/rtm/impl/type_args.vectorcall.impl.h`:

```h
#pragma once

////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2024 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include "rtm/types.h"
#include "rtm/version.h"

namespace rtm
{
	RTM_IMPL_VERSION_NAMESPACE_BEGIN

	//////////////////////////////////////////////////////////////////////////
	// Register passing typedefs
	//////////////////////////////////////////////////////////////////////////

#if defined(RTM_USE_VECTORCALL)
	// On x64 with __vectorcall, the first 6x vector4f/quatf arguments can be passed by value in a register,
	// everything else afterwards is passed by const&. They can also be returned by register.
	using vector4f_arg0 = const vector4f;
	using vector4f_arg1 = const vector4f;
	using vector4f_arg2 = const vector4f;
	using vector4f_arg3 = const vector4f;
	using vector4f_arg4 = const vector4f;
	using vector4f_arg5 = const vector4f;
	using vector4f_arg6 = const vector4f&;
	using vector4f_arg7 = const vector4f&;
	using vector4f_argn = const vector4f&;

	using quatf_arg0 = const quatf;
	using quatf_arg1 = const quatf;
	using quatf_arg2 = const quatf;
	using quatf_arg3 = const quatf;
	using quatf_arg4 = const quatf;
	using quatf_arg5 = const quatf;
	using quatf_arg6 = const quatf&;
	using quatf_arg7 = const quatf&;
	using quatf_argn = const quatf&;

	using scalarf_arg0 = const scalarf;
	using scalarf_arg1 = const scalarf;
	using scalarf_arg2 = const scalarf;
	using scalarf_arg3 = const scalarf;
	using scalarf_arg4 = const scalarf;
	using scalarf_arg5 = const scalarf;
	using scalarf_arg6 = const scalarf&;
	using scalarf_arg7 = const scalarf&;
	using scalarf_argn = const scalarf&;

	using scalard_arg0 = const scalard;
	using scalard_arg1 = const scalard;
	using scalard_arg2 = const scalard;
	using scalard_arg3 = const scalard;
	using scalard_arg4 = const scalard;
	using scalard_arg5 = const scalard;
	using scalard_arg6 = const scalard&;
	using scalard_arg7 = const scalard&;
	using scalard_argn = const scalard&;

	using mask4f_arg0 = const mask4f;
	using mask4f_arg1 = const mask4f;
	using mask4f_arg2 = const mask4f;
	using mask4f_arg3 = const mask4f;
	using mask4f_arg4 = const mask4f;
	using mask4f_arg5 = const mask4f;
	using mask4f_arg6 = const mask4f&;
	using mask4f_arg7 = const mask4f&;
	using mask4f_argn = const mask4f&;

	using mask4i_arg0 = const mask4i;
	using mask4i_arg1 = const mask4i;
	using mask4i_arg2 = const mask4i;
	using mask4i_arg3 = const mask4i;
	using mask4i_arg4 = const mask4i;
	using mask4i_arg5 = const mask4i;
	using mask4i_arg6 = const mask4i&;
	using mask4i_arg7 = const mask4i&;
	using mask4i_argn = const mask4i&;

	// With __vectorcall, vector aggregates are also passed by register and they can use up to 6 registers.
	// Due to how registers are assigned, we reserve some slack and might not use all 6 available registers.
	// Non-scalar types using doubles are aggregate types

	using vector4d_arg0 = const vector4d;
	using vector4d_arg1 = const vector4d;
	using vector4d_arg2 = const vector4d&;
	using vector4d_arg3 = const vector4d&;
	using vector4d_arg4 = const vector4d&;
	using vector4d_arg5 = const vector4d&;
	using vector4d_arg6 = const vector4d&;
	using vector4d_arg7 = const vector4d&;
	using vector4d_argn = const vector4d&;

	using quatd_arg0 = const quatd;
	using quatd_arg1 = const quatd;
	using quatd_arg2 = const quatd&;
	using quatd_arg3 = const quatd&;
	using quatd_arg4 = const quatd&;
	using quatd_arg5 = const quatd&;
	using quatd_arg6 = const quatd&;
	using quatd_arg7 = const quatd&;
	using quatd_argn = const quatd&;

	using mask4d_arg0 = const mask4d;
	using mask4d_arg1 = const mask4d;
	using mask4d_arg2 = const mask4d&;
	using mask4d_arg3 = const mask4d&;
	using mask4d_arg4 = const mask4d&;
	using mask4d_arg5 = const mask4d&;
	using mask4d_arg6 = const mask4d&;
	using mask4d_arg7 = const mask4d&;
	using mask4d_argn = const mask4d&;

	using mask4q_arg0 = const mask4q;
	using mask4q_arg1 = const mask4q;
	using mask4q_arg2 = const mask4q&;
	using mask4q_arg3 = const mask4q&;
	using mask4q_arg4 = const mask4q&;
	using mask4q_arg5 = const mask4q&;
	using mask4q_arg6 = const mask4q&;
	using mask4q_arg7 = const mask4q&;
	using mask4q_argn = const mask4q&;

	using qvf_arg0 = const qvf;
	using qvf_arg1 = const qvf;
	using qvf_argn = const qvf&;

	using qvd_arg0 = const qvd;
	using qvd_arg1 = const qvd&;
	using qvd_argn = const qvd&;

	using qvsf_arg0 = const qvsf;
	using qvsf_arg1 = const qvsf;
	using qvsf_argn = const qvsf&;

	using qvsd_arg0 = const qvsd;
	using qvsd_arg1 = const qvsd&;
	using qvsd_argn = const qvsd&;

	using qvvf_arg0 = const qvvf;
	using qvvf_arg1 = const qvvf;
	using qvvf_argn = const qvvf&;

	using qvvd_arg0 = const qvvd&;
	using qvvd_arg1 = const qvvd&;
	using qvvd_argn = const qvvd&;

	using matrix3x3f_arg0 = const matrix3x3f;
	using matrix3x3f_arg1 = const matrix3x3f&;
	using matrix3x3f_argn = const matrix3x3f&;

	using matrix3x3d_arg0 = const matrix3x3d&;
	using matrix3x3d_arg1 = const matrix3x3d&;
	using matrix3x3d_argn = const matrix3x3d&;

	using matrix3x4f_arg0 = const matrix3x4f;
	using matrix3x4f_arg1 = const matrix3x4f&;
	using matrix3x4f_argn = const matrix3x4f&;

	using matrix3x4d_arg0 = const matrix3x4d&;
	using matrix3x4d_arg1 = const matrix3x4d&;
	using matrix3x4d_argn = const matrix3x4d&;

	using matrix4x4f_arg0 = const matrix4x4f;
	using matrix4x4f_arg1 = const matrix4x4f&;
	using matrix4x4f_argn = const matrix4x4f&;

	using matrix4x4d_arg0 = const matrix4x4d&;
	using matrix4x4d_arg1 = const matrix4x4d&;
	using matrix4x4d_argn = const matrix4x4d&;
#endif

	RTM_IMPL_VERSION_NAMESPACE_END
}

```

`includes/rtm/impl/type_args.x64_clang.impl.h`:

```h
#pragma once

////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2024 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include "rtm/types.h"
#include "rtm/version.h"

namespace rtm
{
	RTM_IMPL_VERSION_NAMESPACE_BEGIN

	//////////////////////////////////////////////////////////////////////////
	// Register passing typedefs
	//////////////////////////////////////////////////////////////////////////

#if defined(RTM_ARCH_X64) && defined(RTM_COMPILER_CLANG)
	// On x64 with clang, the first 8x vector4f/quatf arguments can be passed by value in a register,
	// everything else afterwards is passed by const&. They can also be returned by register.

	using vector4f_arg0 = const vector4f;
	using vector4f_arg1 = const vector4f;
	using vector4f_arg2 = const vector4f;
	using vector4f_arg3 = const vector4f;
	using vector4f_arg4 = const vector4f;
	using vector4f_arg5 = const vector4f;
	using vector4f_arg6 = const vector4f;
	using vector4f_arg7 = const vector4f;
	using vector4f_argn = const vector4f&;

	using quatf_arg0 = const quatf;
	using quatf_arg1 = const quatf;
	using quatf_arg2 = const quatf;
	using quatf_arg3 = const quatf;
	using quatf_arg4 = const quatf;
	using quatf_arg5 = const quatf;
	using quatf_arg6 = const quatf;
	using quatf_arg7 = const quatf;
	using quatf_argn = const quatf&;

	using scalarf_arg0 = const scalarf;
	using scalarf_arg1 = const scalarf;
	using scalarf_arg2 = const scalarf;
	using scalarf_arg3 = const scalarf;
	using scalarf_arg4 = const scalarf;
	using scalarf_arg5 = const scalarf;
	using scalarf_arg6 = const scalarf;
	using scalarf_arg7 = const scalarf;
	using scalarf_argn = const scalarf&;

	using scalard_arg0 = const scalard;
	using scalard_arg1 = const scalard;
	using scalard_arg2 = const scalard;
	using scalard_arg3 = const scalard;
	using scalard_arg4 = const scalard;
	using scalard_arg5 = const scalard;
	using scalard_arg6 = const scalard;
	using scalard_arg7 = const scalard;
	using scalard_argn = const scalard&;

	using mask4f_arg0 = const mask4f;
	using mask4f_arg1 = const mask4f;
	using mask4f_arg2 = const mask4f;
	using mask4f_arg3 = const mask4f;
	using mask4f_arg4 = const mask4f;
	using mask4f_arg5 = const mask4f;
	using mask4f_arg6 = const mask4f;
	using mask4f_arg7 = const mask4f;
	using mask4f_argn = const mask4f&;

	using mask4i_arg0 = const mask4i;
	using mask4i_arg1 = const mask4i;
	using mask4i_arg2 = const mask4i;
	using mask4i_arg3 = const mask4i;
	using mask4i_arg4 = const mask4i;
	using mask4i_arg5 = const mask4i;
	using mask4i_arg6 = const mask4i;
	using mask4i_arg7 = const mask4i;
	using mask4i_argn = const mask4i&;

	// We could pass up to 2 full qvvf types by register and the rotation/translation of
	// the third but aggregates are not returned by register.
	// TODO: Measure the impact of this because it could potentially degrade performance
	// if multiple qvv_mul(..) are called, forcing the return value to be written and read back
	// before the next function call can be made. It might be faster regardless as the compiler
	// might be able to insert other instructions in between.
	// Non-scalar types using doubles are aggregate types

	using vector4d_arg0 = const vector4d&;
	using vector4d_arg1 = const vector4d&;
	using vector4d_arg2 = const vector4d&;
	using vector4d_arg3 = const vector4d&;
	using vector4d_arg4 = const vector4d&;
	using vector4d_arg5 = const vector4d&;
	using vector4d_arg6 = const vector4d&;
	using vector4d_arg7 = const vector4d&;
	using vector4d_argn = const vector4d&;

	using quatd_arg0 = const quatd&;
	using quatd_arg1 = const quatd&;
	using quatd_arg2 = const quatd&;
	using quatd_arg3 = const quatd&;
	using quatd_arg4 = const quatd&;
	using quatd_arg5 = const quatd&;
	using quatd_arg6 = const quatd&;
	using quatd_arg7 = const quatd&;
	using quatd_argn = const quatd&;

	using mask4d_arg0 = const mask4d&;
	using mask4d_arg1 = const mask4d&;
	using mask4d_arg2 = const mask4d&;
	using mask4d_arg3 = const mask4d&;
	using mask4d_arg4 = const mask4d&;
	using mask4d_arg5 = const mask4d&;
	using mask4d_arg6 = const mask4d&;
	using mask4d_arg7 = const mask4d&;
	using mask4d_argn = const mask4d&;

	using mask4q_arg0 = const mask4q&;
	using mask4q_arg1 = const mask4q&;
	using mask4q_arg2 = const mask4q&;
	using mask4q_arg3 = const mask4q&;
	using mask4q_arg4 = const mask4q&;
	using mask4q_arg5 = const mask4q&;
	using mask4q_arg6 = const mask4q&;
	using mask4q_arg7 = const mask4q&;
	using mask4q_argn = const mask4q&;

	using qvf_arg0 = const qvf&;
	using qvf_arg1 = const qvf&;
	using qvf_argn = const qvf&;

	using qvd_arg0 = const qvd&;
	using qvd_arg1 = const qvd&;
	using qvd_argn = const qvd&;

	using qvsf_arg0 = const qvsf&;
	using qvsf_arg1 = const qvsf&;
	using qvsf_argn = const qvsf&;

	using qvsd_arg0 = const qvsd&;
	using qvsd_arg1 = const qvsd&;
	using qvsd_argn = const qvsd&;

	using qvvf_arg0 = const qvvf&;
	using qvvf_arg1 = const qvvf&;
	using qvvf_argn = const qvvf&;

	using qvvd_arg0 = const qvvd&;
	using qvvd_arg1 = const qvvd&;
	using qvvd_argn = const qvvd&;

	using matrix3x3f_arg0 = const matrix3x3f&;
	using matrix3x3f_arg1 = const matrix3x3f&;
	using matrix3x3f_argn = const matrix3x3f&;

	using matrix3x3d_arg0 = const matrix3x3d&;
	using matrix3x3d_arg1 = const matrix3x3d&;
	using matrix3x3d_argn = const matrix3x3d&;

	using matrix3x4f_arg0 = const matrix3x4f&;
	using matrix3x4f_arg1 = const matrix3x4f&;
	using matrix3x4f_argn = const matrix3x4f&;

	using matrix3x4d_arg0 = const matrix3x4d&;
	using matrix3x4d_arg1 = const matrix3x4d&;
	using matrix3x4d_argn = const matrix3x4d&;

	using matrix4x4f_arg0 = const matrix4x4f&;
	using matrix4x4f_arg1 = const matrix4x4f&;
	using matrix4x4f_argn = const matrix4x4f&;

	using matrix4x4d_arg0 = const matrix4x4d&;
	using matrix4x4d_arg1 = const matrix4x4d&;
	using matrix4x4d_argn = const matrix4x4d&;
#endif

	RTM_IMPL_VERSION_NAMESPACE_END
}

```

`includes/rtm/impl/type_args.x64_gcc.impl.h`:

```h
#pragma once

////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2024 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include "rtm/types.h"
#include "rtm/version.h"

namespace rtm
{
	RTM_IMPL_VERSION_NAMESPACE_BEGIN

	//////////////////////////////////////////////////////////////////////////
	// Register passing typedefs
	//////////////////////////////////////////////////////////////////////////

#if defined(RTM_ARCH_X64) && defined(RTM_COMPILER_GCC)
	// On x64 with gcc, the first 8x vector4f/quatf arguments can be passed by value in a register,
	// everything else afterwards is passed by const&. They can also be returned by register.

	using vector4f_arg0 = const vector4f;
	using vector4f_arg1 = const vector4f;
	using vector4f_arg2 = const vector4f;
	using vector4f_arg3 = const vector4f;
	using vector4f_arg4 = const vector4f;
	using vector4f_arg5 = const vector4f;
	using vector4f_arg6 = const vector4f;
	using vector4f_arg7 = const vector4f;
	using vector4f_argn = const vector4f&;

	using quatf_arg0 = const quatf;
	using quatf_arg1 = const quatf;
	using quatf_arg2 = const quatf;
	using quatf_arg3 = const quatf;
	using quatf_arg4 = const quatf;
	using quatf_arg5 = const quatf;
	using quatf_arg6 = const quatf;
	using quatf_arg7 = const quatf;
	using quatf_argn = const quatf&;

	using scalarf_arg0 = const scalarf;
	using scalarf_arg1 = const scalarf;
	using scalarf_arg2 = const scalarf;
	using scalarf_arg3 = const scalarf;
	using scalarf_arg4 = const scalarf;
	using scalarf_arg5 = const scalarf;
	using scalarf_arg6 = const scalarf;
	using scalarf_arg7 = const scalarf;
	using scalarf_argn = const scalarf&;

	using scalard_arg0 = const scalard;
	using scalard_arg1 = const scalard;
	using scalard_arg2 = const scalard;
	using scalard_arg3 = const scalard;
	using scalard_arg4 = const scalard;
	using scalard_arg5 = const scalard;
	using scalard_arg6 = const scalard;
	using scalard_arg7 = const scalard;
	using scalard_argn = const scalard&;

	using mask4f_arg0 = const mask4f;
	using mask4f_arg1 = const mask4f;
	using mask4f_arg2 = const mask4f;
	using mask4f_arg3 = const mask4f;
	using mask4f_arg4 = const mask4f;
	using mask4f_arg5 = const mask4f;
	using mask4f_arg6 = const mask4f;
	using mask4f_arg7 = const mask4f;
	using mask4f_argn = const mask4f&;

	using mask4i_arg0 = const mask4i;
	using mask4i_arg1 = const mask4i;
	using mask4i_arg2 = const mask4i;
	using mask4i_arg3 = const mask4i;
	using mask4i_arg4 = const mask4i;
	using mask4i_arg5 = const mask4i;
	using mask4i_arg6 = const mask4i;
	using mask4i_arg7 = const mask4i;
	using mask4i_argn = const mask4i&;

	// gcc does not appear to support passing and returning aggregates by register
	// Non-scalar types using doubles are aggregate types

	using vector4d_arg0 = const vector4d&;
	using vector4d_arg1 = const vector4d&;
	using vector4d_arg2 = const vector4d&;
	using vector4d_arg3 = const vector4d&;
	using vector4d_arg4 = const vector4d&;
	using vector4d_arg5 = const vector4d&;
	using vector4d_arg6 = const vector4d&;
	using vector4d_arg7 = const vector4d&;
	using vector4d_argn = const vector4d&;

	using quatd_arg0 = const quatd&;
	using quatd_arg1 = const quatd&;
	using quatd_arg2 = const quatd&;
	using quatd_arg3 = const quatd&;
	using quatd_arg4 = const quatd&;
	using quatd_arg5 = const quatd&;
	using quatd_arg6 = const quatd&;
	using quatd_arg7 = const quatd&;
	using quatd_argn = const quatd&;

	using mask4d_arg0 = const mask4d&;
	using mask4d_arg1 = const mask4d&;
	using mask4d_arg2 = const mask4d&;
	using mask4d_arg3 = const mask4d&;
	using mask4d_arg4 = const mask4d&;
	using mask4d_arg5 = const mask4d&;
	using mask4d_arg6 = const mask4d&;
	using mask4d_arg7 = const mask4d&;
	using mask4d_argn = const mask4d&;

	using mask4q_arg0 = const mask4q&;
	using mask4q_arg1 = const mask4q&;
	using mask4q_arg2 = const mask4q&;
	using mask4q_arg3 = const mask4q&;
	using mask4q_arg4 = const mask4q&;
	using mask4q_arg5 = const mask4q&;
	using mask4q_arg6 = const mask4q&;
	using mask4q_arg7 = const mask4q&;
	using mask4q_argn = const mask4q&;

	using qvf_arg0 = const qvf&;
	using qvf_arg1 = const qvf&;
	using qvf_argn = const qvf&;

	using qvd_arg0 = const qvd&;
	using qvd_arg1 = const qvd&;
	using qvd_argn = const qvd&;

	using qvsf_arg0 = const qvsf&;
	using qvsf_arg1 = const qvsf&;
	using qvsf_argn = const qvsf&;

	using qvsd_arg0 = const qvsd&;
	using qvsd_arg1 = const qvsd&;
	using qvsd_argn = const qvsd&;

	using qvvf_arg0 = const qvvf&;
	using qvvf_arg1 = const qvvf&;
	using qvvf_argn = const qvvf&;

	using qvvd_arg0 = const qvvd&;
	using qvvd_arg1 = const qvvd&;
	using qvvd_argn = const qvvd&;

	using matrix3x3f_arg0 = const matrix3x3f&;
	using matrix3x3f_arg1 = const matrix3x3f&;
	using matrix3x3f_argn = const matrix3x3f&;

	using matrix3x3d_arg0 = const matrix3x3d&;
	using matrix3x3d_arg1 = const matrix3x3d&;
	using matrix3x3d_argn = const matrix3x3d&;

	using matrix3x4f_arg0 = const matrix3x4f&;
	using matrix3x4f_arg1 = const matrix3x4f&;
	using matrix3x4f_argn = const matrix3x4f&;

	using matrix3x4d_arg0 = const matrix3x4d&;
	using matrix3x4d_arg1 = const matrix3x4d&;
	using matrix3x4d_argn = const matrix3x4d&;

	using matrix4x4f_arg0 = const matrix4x4f&;
	using matrix4x4f_arg1 = const matrix4x4f&;
	using matrix4x4f_argn = const matrix4x4f&;

	using matrix4x4d_arg0 = const matrix4x4d&;
	using matrix4x4d_arg1 = const matrix4x4d&;
	using matrix4x4d_argn = const matrix4x4d&;
#endif

	RTM_IMPL_VERSION_NAMESPACE_END
}

```

`includes/rtm/impl/vector_common.h`:

```h
#pragma once

////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2018 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include "rtm/types.h"
#include "rtm/impl/compiler_utils.h"
#include "rtm/scalarf.h"
#include "rtm/scalard.h"
#include "rtm/version.h"

#include <cstdint>
#include <cstring>

RTM_IMPL_FILE_PRAGMA_PUSH

namespace rtm
{
	RTM_IMPL_VERSION_NAMESPACE_BEGIN

	//////////////////////////////////////////////////////////////////////////
	// Creates a vector4 from all 4 components.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4f RTM_SIMD_CALL vector_set(float x, float y, float z, float w) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return _mm_set_ps(w, z, y, x);
#elif defined(RTM_NEON_INTRINSICS)
		float32x2_t V0 = vset_lane_f32(y, vmov_n_f32(x), 1);
		float32x2_t V1 = vset_lane_f32(w, vmov_n_f32(z), 1);
		return vcombine_f32(V0, V1);
#else
		return vector4f{ x, y, z, w };
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Creates a vector4 from the [xyz] components and sets [w] to 0.0.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4f RTM_SIMD_CALL vector_set(float x, float y, float z) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return _mm_set_ps(0.0F, z, y, x);
#elif defined(RTM_NEON_INTRINSICS)
		float32x2_t V0 = vset_lane_f32(y, vmov_n_f32(x), 1);
		float32x2_t V1 = vset_lane_f32(z, vdup_n_f32(0.0F), 0);
		return vcombine_f32(V0, V1);
#else
		return vector4f{ x, y, z, 0.0f };
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Creates a vector4 from a single value for all 4 components.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4f RTM_SIMD_CALL vector_set(float xyzw) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return _mm_set_ps1(xyzw);
#elif defined(RTM_NEON_INTRINSICS)
		return vdupq_n_f32(xyzw);
#else
		return vector4f{ xyzw, xyzw, xyzw, xyzw };
#endif
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Creates a vector4 from all 4 components.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4f RTM_SIMD_CALL vector_set(scalarf_arg0 x, scalarf_arg1 y, scalarf_arg2 z, scalarf_arg3 w) RTM_NO_EXCEPT
	{
		const __m128 xy = _mm_unpacklo_ps(x.value, y.value);
		const __m128 zw = _mm_unpacklo_ps(z.value, w.value);
		return _mm_shuffle_ps(xy, zw, _MM_SHUFFLE(1, 0, 1, 0));
	}

	//////////////////////////////////////////////////////////////////////////
	// Creates a vector4 from the [xyz] components and sets [w] to 0.0.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4f RTM_SIMD_CALL vector_set(scalarf_arg0 x, scalarf_arg1 y, scalarf_arg2 z) RTM_NO_EXCEPT
	{
		const __m128 xy = _mm_unpacklo_ps(x.value, y.value);
		const __m128 zw = _mm_unpacklo_ps(z.value, _mm_setzero_ps());
		return _mm_shuffle_ps(xy, zw, _MM_SHUFFLE(1, 0, 1, 0));
	}

	//////////////////////////////////////////////////////////////////////////
	// Creates a vector4 from a single value for all 4 components.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4f RTM_SIMD_CALL vector_set(scalarf_arg0 xyzw) RTM_NO_EXCEPT
	{
		return _mm_shuffle_ps(xyzw.value, xyzw.value, _MM_SHUFFLE(0, 0, 0, 0));
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Creates a vector4 from all 4 components.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4d RTM_SIMD_CALL vector_set(double x, double y, double z, double w) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return vector4d{ _mm_set_pd(y, x), _mm_set_pd(w, z) };
#else
		return vector4d{ x, y, z, w };
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Creates a vector4 from the [xyz] components and sets [w] to 0.0.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4d RTM_SIMD_CALL vector_set(double x, double y, double z) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return vector4d{ _mm_set_pd(y, x), _mm_set_pd(0.0, z) };
#else
		return vector4d{ x, y, z, 0.0 };
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Creates a vector4 from a single value for all 4 components.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4d RTM_SIMD_CALL vector_set(double xyzw) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		const __m128d xyzw_pd = _mm_set1_pd(xyzw);
		return vector4d{ xyzw_pd, xyzw_pd };
#else
		return vector4d{ xyzw, xyzw, xyzw, xyzw };
#endif
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Creates a vector4 from all 4 components.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4d RTM_SIMD_CALL vector_set(scalard_arg0 x, scalard_arg1 y, scalard_arg2 z, scalard_arg3 w) RTM_NO_EXCEPT
	{
		const __m128d xy = _mm_unpacklo_pd(x.value, y.value);
		const __m128d zw = _mm_unpacklo_pd(z.value, w.value);
		return vector4d{ xy, zw };
	}

	//////////////////////////////////////////////////////////////////////////
	// Creates a vector4 from the [xyz] components and sets [w] to 0.0.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4d RTM_SIMD_CALL vector_set(scalard_arg0 x, scalard_arg1 y, scalard_arg2 z) RTM_NO_EXCEPT
	{
		const __m128d xy = _mm_unpacklo_pd(x.value, y.value);
		const __m128d zw = _mm_unpacklo_pd(z.value, _mm_setzero_pd());
		return vector4d{ xy, zw };
	}

	//////////////////////////////////////////////////////////////////////////
	// Creates a vector4 from a single value for all 4 components.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4d RTM_SIMD_CALL vector_set(scalard_arg0 xyzw) RTM_NO_EXCEPT
	{
		const __m128d xyzw_pd = _mm_shuffle_pd(xyzw.value, xyzw.value, 0);
		return vector4d{ xyzw_pd, xyzw_pd };
	}
#endif

	namespace rtm_impl
	{
		//////////////////////////////////////////////////////////////////////////
		// Returns true if mix4 component is one of [xyzw]
		//////////////////////////////////////////////////////////////////////////
		RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr bool is_mix_xyzw(mix4 arg) RTM_NO_EXCEPT { return uint32_t(arg) <= uint32_t(mix4::w); }

		//////////////////////////////////////////////////////////////////////////
		// Returns true if mix4 component is one of [abcd]
		//////////////////////////////////////////////////////////////////////////
		RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr bool is_mix_abcd(mix4 arg) RTM_NO_EXCEPT { return uint32_t(arg) >= uint32_t(mix4::a); }

		//////////////////////////////////////////////////////////////////////////
		// Converts a mix4 value to a component4 value
		//////////////////////////////////////////////////////////////////////////
		RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr component4 mix_to_component(mix4 arg) RTM_NO_EXCEPT { return static_cast<component4>(static_cast<uint32_t>(arg) % 4); }

		//////////////////////////////////////////////////////////////////////////
		// This is a helper struct to help manipulate SIMD masks.
		//////////////////////////////////////////////////////////////////////////
		union mask_converter
		{
			uint64_t u64;
			uint32_t u32[2];

			RTM_DISABLE_SECURITY_COOKIE_CHECK explicit RTM_FORCE_INLINE constexpr mask_converter(uint64_t value) RTM_NO_EXCEPT : u64(value) {}

			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr operator uint32_t() const RTM_NO_EXCEPT { return u32[0]; }
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr operator uint64_t() const RTM_NO_EXCEPT { return u64; }
		};

		//////////////////////////////////////////////////////////////////////////
		// Returns a SIMD mask value from a boolean.
		//////////////////////////////////////////////////////////////////////////
		RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr mask_converter get_mask_value(bool is_true) RTM_NO_EXCEPT
		{
			return mask_converter(is_true ? uint64_t(0xFFFFFFFFFFFFFFFFULL) : uint64_t(0));
		}

		//////////////////////////////////////////////////////////////////////////
		// Selects if_false if the SIMD mask value is 0, otherwise if_true.
		//////////////////////////////////////////////////////////////////////////
		RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr double RTM_SIMD_CALL select(uint64_t mask, double if_true, double if_false) RTM_NO_EXCEPT
		{
			return mask == 0 ? if_false : if_true;
		}

		//////////////////////////////////////////////////////////////////////////
		// Selects if_false if the SIMD mask value is 0, otherwise if_true.
		//////////////////////////////////////////////////////////////////////////
		RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr float RTM_SIMD_CALL select(uint32_t mask, float if_true, float if_false) RTM_NO_EXCEPT
		{
			return mask == 0 ? if_false : if_true;
		}

		//////////////////////////////////////////////////////////////////////////
		// This is a helper struct to allow a single consistent API between
		// various vector types when the semantics are identical but the return
		// type differs. Implicit coercion is used to return the desired value
		// at the call site.
		//////////////////////////////////////////////////////////////////////////
		struct vector_zero_impl
		{
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator vector4d() const RTM_NO_EXCEPT
			{
#if defined(RTM_SSE2_INTRINSICS)
				const __m128d zero_pd = _mm_setzero_pd();
				return vector4d{ zero_pd, zero_pd };
#else
				return vector_set(0.0);
#endif
			}

			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator vector4f() const RTM_NO_EXCEPT
			{
#if defined(RTM_SSE2_INTRINSICS)
				return _mm_setzero_ps();
#else
				return vector_set(0.0F);
#endif
			}
		};

		enum class vector_constant
		{
			zero,
			one,
		};

		RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr float get_constant32(vector_constant constant)
		{
			return constant == vector_constant::zero ? 0.0F : 1.0F;
		}

		RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr double get_constant64(vector_constant constant)
		{
			return constant == vector_constant::zero ? 0.0 : 1.0;
		}

		//////////////////////////////////////////////////////////////////////////
		// This is a helper struct to allow a single consistent API between
		// various vector types when the semantics are identical but the return
		// type differs. Implicit coercion is used to return the desired value
		// at the call site.
		//////////////////////////////////////////////////////////////////////////
		template<vector_constant x, vector_constant y, vector_constant z, vector_constant w>
		struct vector_constant_impl
		{
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator vector4d() const RTM_NO_EXCEPT
			{
#if defined(RTM_SSE2_INTRINSICS)
				constexpr __m128d constant_xy = RTM_VECTOR2D_MAKE(get_constant64(x), get_constant64(y));
				constexpr __m128d constant_zw = RTM_VECTOR2D_MAKE(get_constant64(z), get_constant64(w));
				return vector4d{ constant_xy, constant_zw };
#else
				return vector_set(get_constant64(x), get_constant64(y), get_constant64(z), get_constant64(w));
#endif
			}

			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator vector4f() const RTM_NO_EXCEPT
			{
#if defined(RTM_SSE2_INTRINSICS)
				constexpr __m128 constant = RTM_VECTOR4F_MAKE(get_constant32(x), get_constant32(y), get_constant32(z), get_constant32(w));
				return constant;
#else
				return vector_set(get_constant32(x), get_constant32(y), get_constant32(z), get_constant32(w));
#endif
			}
		};

		//////////////////////////////////////////////////////////////////////////
		// Various vector widths we can load
		//////////////////////////////////////////////////////////////////////////
		enum class vector_unaligned_loader_width
		{
			vec1,
			vec2,
			vec3,
			vec4,
		};

		//////////////////////////////////////////////////////////////////////////
		// This is a helper struct to allow a single consistent API between
		// various vector types when the semantics are identical but the return
		// type differs. Implicit coercion is used to return the desired value
		// at the call site.
		//////////////////////////////////////////////////////////////////////////
		template<vector_unaligned_loader_width width>
		struct vector_unaligned_loader
		{
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator vector4d() const RTM_NO_EXCEPT
			{
				switch (width)
				{
				case vector_unaligned_loader_width::vec1:
				{
					double data[1];
					std::memcpy(&data[0], ptr, sizeof(double) * 1);
					return vector_set(data[0], 0.0, 0.0, 0.0);
				}
				case vector_unaligned_loader_width::vec2:
				{
					double data[2];
					std::memcpy(&data[0], ptr, sizeof(double) * 2);
					return vector_set(data[0], data[1], 0.0, 0.0);
				}
				case vector_unaligned_loader_width::vec3:
				{
					double data[3];
					std::memcpy(&data[0], ptr, sizeof(double) * 3);
					return vector_set(data[0], data[1], data[2], 0.0);
				}
				case vector_unaligned_loader_width::vec4:
				default:
				{
					vector4d result;
					std::memcpy(&result, ptr, sizeof(vector4d));
					return result;
				}
				}
			}

			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator vector4f() const RTM_NO_EXCEPT
			{
				switch (width)
				{
				case vector_unaligned_loader_width::vec1:
				{
					float data[1];
					std::memcpy(&data[0], ptr, sizeof(float) * 1);
					return vector_set(data[0], 0.0F, 0.0F, 0.0F);
				}
				case vector_unaligned_loader_width::vec2:
				{
					float data[2];
					std::memcpy(&data[0], ptr, sizeof(float) * 2);
					return vector_set(data[0], data[1], 0.0F, 0.0F);
				}
				case vector_unaligned_loader_width::vec3:
				{
					float data[3];
					std::memcpy(&data[0], ptr, sizeof(float) * 3);
					return vector_set(data[0], data[1], data[2], 0.0F);
				}
				case vector_unaligned_loader_width::vec4:
				default:
				{
#if defined(RTM_SSE2_INTRINSICS)
					return _mm_loadu_ps((const float*)ptr);
#elif defined(RTM_NEON_INTRINSICS)
					return vreinterpretq_f32_u8(vld1q_u8(ptr));
#else
					vector4f result;
					std::memcpy(&result, ptr, sizeof(vector4f));
					return result;
#endif
				}
				}
			}

			const uint8_t* ptr;
		};

		//////////////////////////////////////////////////////////////////////////
		// This is a helper struct to allow a single consistent API between
		// various vector types when the semantics are identical but the return
		// type differs. Implicit coercion is used to return the desired value
		// at the call site.
		//////////////////////////////////////////////////////////////////////////
		struct vector4f_to_scalarf
		{
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator float() const RTM_NO_EXCEPT
			{
#if defined(RTM_SSE2_INTRINSICS)
				return _mm_cvtss_f32(value);
#elif defined(RTM_NEON_INTRINSICS)
				return vgetq_lane_f32(value, 0);
#else
				return value.x;
#endif
			}

#if defined(RTM_SSE2_INTRINSICS)
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator scalarf() const RTM_NO_EXCEPT
			{
				return scalarf{ value };
			}
#endif

			vector4f value;
		};

		//////////////////////////////////////////////////////////////////////////
		// This is a helper struct to allow a single consistent API between
		// various vector types when the semantics are identical but the return
		// type differs. Implicit coercion is used to return the desired value
		// at the call site.
		//////////////////////////////////////////////////////////////////////////
		struct vector4d_to_scalard
		{
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator double() const RTM_NO_EXCEPT
			{
#if defined(RTM_SSE2_INTRINSICS)
				return _mm_cvtsd_f64(value.xy);
#else
				return value.x;
#endif
			}

#if defined(RTM_SSE2_INTRINSICS)
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator scalard() const RTM_NO_EXCEPT
			{
				return scalard{ value.xy };
			}
#endif

			vector4d value;
		};

		//////////////////////////////////////////////////////////////////////////
		// This is a helper struct to allow a single consistent API between
		// various vector types when the semantics are identical but the return
		// type differs. Implicit coercion is used to return the desired value
		// at the call site.
		//////////////////////////////////////////////////////////////////////////
		struct vector4f_get_min_component
		{
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator float() const RTM_NO_EXCEPT
			{
#if defined(RTM_SSE2_INTRINSICS)
				__m128 zwzw = _mm_movehl_ps(value, value);
				__m128 xz_yw_zz_ww = _mm_min_ps(value, zwzw);
				__m128 yw_yw_yw_yw = _mm_shuffle_ps(xz_yw_zz_ww, xz_yw_zz_ww, _MM_SHUFFLE(1, 1, 1, 1));
				return _mm_cvtss_f32(_mm_min_ps(xz_yw_zz_ww, yw_yw_yw_yw));
#elif defined(RTM_NEON_INTRINSICS)
				float32x2_t xy_zw = vpmin_f32(vget_low_f32(value), vget_high_f32(value));
				return vget_lane_f32(vpmin_f32(xy_zw, xy_zw), 0);
#else
				return scalar_min(scalar_min(value.x, value.y), scalar_min(value.z, value.w));
#endif
			}

#if defined(RTM_SSE2_INTRINSICS)
			RTM_DEPRECATED("Use 'as_scalar' suffix instead. To be removed in 2.4.")
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator scalarf() const RTM_NO_EXCEPT
			{
				__m128 zwzw = _mm_movehl_ps(value, value);
				__m128 xz_yw_zz_ww = _mm_min_ps(value, zwzw);
				__m128 yw_yw_yw_yw = _mm_shuffle_ps(xz_yw_zz_ww, xz_yw_zz_ww, _MM_SHUFFLE(1, 1, 1, 1));
				return scalarf{ _mm_min_ps(xz_yw_zz_ww, yw_yw_yw_yw) };
			}
#endif

			vector4f value;
		};

		//////////////////////////////////////////////////////////////////////////
		// This is a helper struct to allow a single consistent API between
		// various vector types when the semantics are identical but the return
		// type differs. Implicit coercion is used to return the desired value
		// at the call site.
		//////////////////////////////////////////////////////////////////////////
		struct vector4f_get_max_component
		{
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator float() const RTM_NO_EXCEPT
			{
#if defined(RTM_SSE2_INTRINSICS)
				__m128 zwzw = _mm_movehl_ps(value, value);
				__m128 xz_yw_zz_ww = _mm_max_ps(value, zwzw);
				__m128 yw_yw_yw_yw = _mm_shuffle_ps(xz_yw_zz_ww, xz_yw_zz_ww, _MM_SHUFFLE(1, 1, 1, 1));
				return _mm_cvtss_f32(_mm_max_ps(xz_yw_zz_ww, yw_yw_yw_yw));
#elif defined(RTM_NEON_INTRINSICS)
				float32x2_t xy_zw = vpmax_f32(vget_low_f32(value), vget_high_f32(value));
				return vget_lane_f32(vpmax_f32(xy_zw, xy_zw), 0);
#else
				return scalar_max(scalar_max(value.x, value.y), scalar_max(value.z, value.w));
#endif
			}

#if defined(RTM_SSE2_INTRINSICS)
			RTM_DEPRECATED("Use 'as_scalar' suffix instead. To be removed in 2.4.")
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator scalarf() const RTM_NO_EXCEPT
			{
				__m128 zwzw = _mm_movehl_ps(value, value);
				__m128 xz_yw_zz_ww = _mm_max_ps(value, zwzw);
				__m128 yw_yw_yw_yw = _mm_shuffle_ps(xz_yw_zz_ww, xz_yw_zz_ww, _MM_SHUFFLE(1, 1, 1, 1));
				return scalarf{ _mm_max_ps(xz_yw_zz_ww, yw_yw_yw_yw) };
			}
#endif

			vector4f value;
		};

		//////////////////////////////////////////////////////////////////////////
		// This is a helper struct to allow a single consistent API between
		// various vector types when the semantics are identical but the return
		// type differs. Implicit coercion is used to return the desired value
		// at the call site.
		//////////////////////////////////////////////////////////////////////////
		struct vector4d_get_min_component
		{
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator double() const RTM_NO_EXCEPT
			{
#if defined(RTM_SSE2_INTRINSICS)
				__m128d xz_yw = _mm_min_pd(value.xy, value.zw);
				__m128d yw_yw = _mm_shuffle_pd(xz_yw, xz_yw, 1);
				return _mm_cvtsd_f64(_mm_min_pd(xz_yw, yw_yw));
#else
				return scalar_min(scalar_min(value.x, value.y), scalar_min(value.z, value.w));
#endif
			}

#if defined(RTM_SSE2_INTRINSICS)
			RTM_DEPRECATED("Use 'as_scalar' suffix instead. To be removed in 2.4.")
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator scalard() const RTM_NO_EXCEPT
			{
				__m128d xz_yw = _mm_min_pd(value.xy, value.zw);
				__m128d yw_yw = _mm_shuffle_pd(xz_yw, xz_yw, 1);
				return scalard{ _mm_min_pd(xz_yw, yw_yw) };
			}
#endif

			vector4d value;
		};

		//////////////////////////////////////////////////////////////////////////
		// This is a helper struct to allow a single consistent API between
		// various vector types when the semantics are identical but the return
		// type differs. Implicit coercion is used to return the desired value
		// at the call site.
		//////////////////////////////////////////////////////////////////////////
		struct vector4d_get_max_component
		{
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator double() const RTM_NO_EXCEPT
			{
#if defined(RTM_SSE2_INTRINSICS)
				__m128d xz_yw = _mm_max_pd(value.xy, value.zw);
				__m128d yw_yw = _mm_shuffle_pd(xz_yw, xz_yw, 1);
				return _mm_cvtsd_f64(_mm_max_pd(xz_yw, yw_yw));
#else
				return scalar_max(scalar_max(value.x, value.y), scalar_max(value.z, value.w));
#endif
			}

#if defined(RTM_SSE2_INTRINSICS)
			RTM_DEPRECATED("Use 'as_scalar' suffix instead. To be removed in 2.4.")
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator scalard() const RTM_NO_EXCEPT
			{
				__m128d xz_yw = _mm_max_pd(value.xy, value.zw);
				__m128d yw_yw = _mm_shuffle_pd(xz_yw, xz_yw, 1);
				return scalard{ _mm_max_pd(xz_yw, yw_yw) };
			}
#endif

			vector4d value;
		};
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns a vector consisting of all zeros.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr rtm_impl::vector_zero_impl RTM_SIMD_CALL vector_zero() RTM_NO_EXCEPT
	{
		return rtm_impl::vector_zero_impl();
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns a unit vector pointing in the forward direction of the default coordinate system (Z+).
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr rtm_impl::vector_constant_impl<rtm_impl::vector_constant::zero, rtm_impl::vector_constant::zero, rtm_impl::vector_constant::one, rtm_impl::vector_constant::zero> RTM_SIMD_CALL vector_coord_forward() RTM_NO_EXCEPT
	{
		return rtm_impl::vector_constant_impl<
			rtm_impl::vector_constant::zero,
			rtm_impl::vector_constant::zero,
			rtm_impl::vector_constant::one,
			rtm_impl::vector_constant::zero>();
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns a unit vector pointing in the up direction of the default coordinate system (Y+).
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr rtm_impl::vector_constant_impl<rtm_impl::vector_constant::zero, rtm_impl::vector_constant::one, rtm_impl::vector_constant::zero, rtm_impl::vector_constant::zero> RTM_SIMD_CALL vector_coord_up() RTM_NO_EXCEPT
	{
		return rtm_impl::vector_constant_impl<
			rtm_impl::vector_constant::zero,
			rtm_impl::vector_constant::one,
			rtm_impl::vector_constant::zero,
			rtm_impl::vector_constant::zero>();
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns a unit vector pointing in the cross direction of the default coordinate system (X+).
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr rtm_impl::vector_constant_impl<rtm_impl::vector_constant::one, rtm_impl::vector_constant::zero, rtm_impl::vector_constant::zero, rtm_impl::vector_constant::zero> RTM_SIMD_CALL vector_coord_cross() RTM_NO_EXCEPT
	{
		return rtm_impl::vector_constant_impl<
			rtm_impl::vector_constant::one,
			rtm_impl::vector_constant::zero,
			rtm_impl::vector_constant::zero,
			rtm_impl::vector_constant::zero>();
	}

	//////////////////////////////////////////////////////////////////////////
	// Loads an unaligned vector4 from memory.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr rtm_impl::vector_unaligned_loader<rtm_impl::vector_unaligned_loader_width::vec4> RTM_SIMD_CALL vector_load(const uint8_t* input) RTM_NO_EXCEPT
	{
		return rtm_impl::vector_unaligned_loader<rtm_impl::vector_unaligned_loader_width::vec4>{ input };
	}

	//////////////////////////////////////////////////////////////////////////
	// Loads an unaligned vector1 from memory and sets the [yzw] components to zero.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr rtm_impl::vector_unaligned_loader<rtm_impl::vector_unaligned_loader_width::vec1> RTM_SIMD_CALL vector_load1(const uint8_t* input) RTM_NO_EXCEPT
	{
		return rtm_impl::vector_unaligned_loader<rtm_impl::vector_unaligned_loader_width::vec1>{ input };
	}

	//////////////////////////////////////////////////////////////////////////
	// Loads an unaligned vector2 from memory and sets the [zw] components to zero.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr rtm_impl::vector_unaligned_loader<rtm_impl::vector_unaligned_loader_width::vec2> RTM_SIMD_CALL vector_load2(const uint8_t* input) RTM_NO_EXCEPT
	{
		return rtm_impl::vector_unaligned_loader<rtm_impl::vector_unaligned_loader_width::vec2>{ input };
	}

	//////////////////////////////////////////////////////////////////////////
	// Loads an unaligned vector3 from memory and sets the [w] component to zero.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr rtm_impl::vector_unaligned_loader<rtm_impl::vector_unaligned_loader_width::vec3> RTM_SIMD_CALL vector_load3(const uint8_t* input) RTM_NO_EXCEPT
	{
		return rtm_impl::vector_unaligned_loader<rtm_impl::vector_unaligned_loader_width::vec3>{ input };
	}

	//////////////////////////////////////////////////////////////////////////
	// Coerces an vector4 input into a scalar by grabbing the first SIMD lane.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr rtm_impl::vector4f_to_scalarf RTM_SIMD_CALL vector_as_scalar(vector4f_arg0 input) RTM_NO_EXCEPT
	{
		return rtm_impl::vector4f_to_scalarf{ input };
	}

	//////////////////////////////////////////////////////////////////////////
	// Coerces an vector4 input into a scalar by grabbing the first SIMD lane.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr rtm_impl::vector4d_to_scalard RTM_SIMD_CALL vector_as_scalar(vector4d_arg0 input) RTM_NO_EXCEPT
	{
		return rtm_impl::vector4d_to_scalard{ input };
	}

	RTM_IMPL_VERSION_NAMESPACE_END
}

RTM_IMPL_FILE_PRAGMA_POP

```

`includes/rtm/macros.h`:

```h
#pragma once

////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include "rtm/impl/macros.mask4.impl.h"
#include "rtm/impl/macros.matrix.impl.h"
#include "rtm/impl/macros.vector4.impl.h"

```

`includes/rtm/mask4d.h`:

```h
#pragma once

////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include "rtm/math.h"
#include "rtm/version.h"
#include "rtm/impl/bit_cast.impl.h"
#include "rtm/impl/compiler_utils.h"
#include "rtm/impl/mask_common.h"

#if !defined(RTM_SSE2_INTRINSICS)
	#include <cstring>
#endif

RTM_IMPL_FILE_PRAGMA_PUSH

namespace rtm
{
	RTM_IMPL_VERSION_NAMESPACE_BEGIN

	//////////////////////////////////////////////////////////////////////////
	// Returns the mask4d [x] component.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE uint64_t RTM_SIMD_CALL mask_get_x(mask4d_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
#if defined(RTM_ARCH_X64)
		return static_cast<uint64_t>(_mm_cvtsi128_si64(_mm_castpd_si128(input.xy)));
#else
		// Just sign extend on 32bit systems
		return static_cast<uint64_t>(_mm_cvtsi128_si32(_mm_castpd_si128(input.xy)));
#endif
#else
		return input.x;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the mask4d [y] component.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE uint64_t RTM_SIMD_CALL mask_get_y(mask4d_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
#if defined(RTM_ARCH_X64)
		return static_cast<uint64_t>(_mm_cvtsi128_si64(_mm_castpd_si128(_mm_shuffle_pd(input.xy, input.xy, 1))));
#else
		// Just sign extend on 32bit systems
		return static_cast<uint64_t>(_mm_cvtsi128_si32(_mm_castpd_si128(_mm_shuffle_pd(input.xy, input.xy, 1))));
#endif
#else
		return input.y;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the mask4d [z] component.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE uint64_t RTM_SIMD_CALL mask_get_z(mask4d_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
#if defined(RTM_ARCH_X64)
		return static_cast<uint64_t>(_mm_cvtsi128_si64(_mm_castpd_si128(input.zw)));
#else
		// Just sign extend on 32bit systems
		return static_cast<uint64_t>(_mm_cvtsi128_si32(_mm_castpd_si128(input.zw)));
#endif
#else
		return input.z;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the mask4d [w] component.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE uint64_t RTM_SIMD_CALL mask_get_w(mask4d_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
#if defined(RTM_ARCH_X64)
		return static_cast<uint64_t>(_mm_cvtsi128_si64(_mm_castpd_si128(_mm_shuffle_pd(input.zw, input.zw, 1))));
#else
		// Just sign extend on 32bit systems
		return static_cast<uint64_t>(_mm_cvtsi128_si32(_mm_castpd_si128(_mm_shuffle_pd(input.zw, input.zw, 1))));
#endif
#else
		return input.w;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if all 4 components are true, otherwise false: all(input.xyzw != 0)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL mask_all_true(mask4d_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return (_mm_movemask_pd(input.xy) & _mm_movemask_pd(input.zw)) == 3;
#else
		return input.x != 0 && input.y != 0 && input.z != 0 && input.w != 0;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if all [xy] components are true, otherwise false: all(input.xy != 0)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL mask_all_true2(mask4d_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return _mm_movemask_pd(input.xy) == 3;
#else
		return input.x != 0 && input.y != 0;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if all [xyz] components are true, otherwise false: all(input.xyz != 0)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL mask_all_true3(mask4d_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return _mm_movemask_pd(input.xy) == 3 && (_mm_movemask_pd(input.zw) & 1) != 0;
#else
		return input.x != 0 && input.y != 0 && input.z != 0;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if any 4 components are true, otherwise false: any(input.xyzw != 0)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL mask_any_true(mask4d_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return (_mm_movemask_pd(input.xy) | _mm_movemask_pd(input.zw)) != 0;
#else
		return input.x != 0 || input.y != 0 || input.z != 0 || input.w != 0;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if any [xy] components are true, otherwise false: any(input.xy != 0)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL mask_any_true2(mask4d_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return _mm_movemask_pd(input.xy) != 0;
#else
		return input.x != 0 || input.y != 0;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if any [xyz] components are true, otherwise false: any(input.xyz != 0)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL mask_any_true3(mask4d_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return _mm_movemask_pd(input.xy) != 0 || (_mm_movemask_pd(input.zw) & 1) != 0;
#else
		return input.x != 0 || input.y != 0 || input.z != 0;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if all 4 components are equal, otherwise false: all(lhs.xyzw == rhs.xyzw)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL mask_all_equal(mask4d_arg0 lhs, mask4d_arg1 rhs) RTM_NO_EXCEPT
	{
		// Cannot use == and != with NaN floats
#if defined(RTM_SSE2_INTRINSICS)
		// WARNING: SSE2 doesn't have a 64 bit int compare, we use 32 bit here and we assume
		// that in a mask all bits are equal
		__m128i lhs_xy = _mm_castpd_si128(lhs.xy);
		__m128i lhs_zw = _mm_castpd_si128(lhs.zw);
		__m128i rhs_xy = _mm_castpd_si128(rhs.xy);
		__m128i rhs_zw = _mm_castpd_si128(rhs.zw);
		__m128i xy_eq = _mm_cmpeq_epi32(lhs_xy, rhs_xy);
		__m128i zw_eq = _mm_cmpeq_epi32(lhs_zw, rhs_zw);
		return (_mm_movemask_epi8(xy_eq) & _mm_movemask_epi8(zw_eq)) == 0xFFFF;
#elif defined(RTM_SSE4_INTRINSICS) && 0
		// TODO
#else
		return std::memcmp(&lhs, &rhs, sizeof(uint64_t) * 4) == 0;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if all [xy] components are equal, otherwise false: all(lhs.xy == rhs.xy)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL mask_all_equal2(mask4d_arg0 lhs, mask4d_arg1 rhs) RTM_NO_EXCEPT
	{
		// Cannot use == and != with NaN floats
#if defined(RTM_SSE2_INTRINSICS)
		// WARNING: SSE2 doesn't have a 64 bit int compare, we use 32 bit here and we assume
		// that in a mask all bits are equal
		__m128i lhs_xy = _mm_castpd_si128(lhs.xy);
		__m128i rhs_xy = _mm_castpd_si128(rhs.xy);
		return _mm_movemask_epi8(_mm_cmpeq_epi32(lhs_xy, rhs_xy)) == 0xFFFF;
#elif defined(RTM_SSE4_INTRINSICS) && 0
		// TODO
#else
		return std::memcmp(&lhs, &rhs, sizeof(uint64_t) * 2) == 0;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if all [xyz] components are equal, otherwise false: all(lhs.xyz == rhs.xyz)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL mask_all_equal3(mask4d_arg0 lhs, mask4d_arg1 rhs) RTM_NO_EXCEPT
	{
		// Cannot use == and != with NaN floats
#if defined(RTM_SSE2_INTRINSICS)
		// WARNING: SSE2 doesn't have a 64 bit int compare, we use 32 bit here and we assume
		// that in a mask all bits are equal
		__m128i lhs_xy = _mm_castpd_si128(lhs.xy);
		__m128i lhs_zw = _mm_castpd_si128(lhs.zw);
		__m128i rhs_xy = _mm_castpd_si128(rhs.xy);
		__m128i rhs_zw = _mm_castpd_si128(rhs.zw);
		__m128i xy_eq = _mm_cmpeq_epi32(lhs_xy, rhs_xy);
		__m128i zw_eq = _mm_cmpeq_epi32(lhs_zw, rhs_zw);
		return _mm_movemask_epi8(xy_eq) == 0xFFFF && (_mm_movemask_epi8(zw_eq) & 0x00FF) == 0x00FF;
#elif defined(RTM_SSE4_INTRINSICS) && 0
		// TODO
#else
		return std::memcmp(&lhs, &rhs, sizeof(uint64_t) * 3) == 0;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if any 4 components are equal, otherwise false: any(lhs.xyzw == rhs.xyzw)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL mask_any_equal(mask4d_arg0 lhs, mask4d_arg1 rhs) RTM_NO_EXCEPT
	{
		// Cannot use == and != with NaN floats
#if defined(RTM_SSE2_INTRINSICS)
		// WARNING: SSE2 doesn't have a 64 bit int compare, we use 32 bit here and we assume
		// that in a mask all bits are equal
		__m128i lhs_xy = _mm_castpd_si128(lhs.xy);
		__m128i lhs_zw = _mm_castpd_si128(lhs.zw);
		__m128i rhs_xy = _mm_castpd_si128(rhs.xy);
		__m128i rhs_zw = _mm_castpd_si128(rhs.zw);
		__m128i xy_eq = _mm_cmpeq_epi32(lhs_xy, rhs_xy);
		__m128i zw_eq = _mm_cmpeq_epi32(lhs_zw, rhs_zw);
		return (_mm_movemask_epi8(xy_eq) | _mm_movemask_epi8(zw_eq)) != 0;
#elif defined(RTM_SSE4_INTRINSICS) && 0
		// TODO
#else
		return std::memcmp(&lhs.x, &rhs.x, sizeof(uint64_t)) == 0
			|| std::memcmp(&lhs.y, &rhs.y, sizeof(uint64_t)) == 0
			|| std::memcmp(&lhs.z, &rhs.z, sizeof(uint64_t)) == 0
			|| std::memcmp(&lhs.w, &rhs.w, sizeof(uint64_t)) == 0;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if any [xy] components are equal, otherwise false: any(lhs.xy == rhs.xy)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL mask_any_equal2(mask4d_arg0 lhs, mask4d_arg1 rhs) RTM_NO_EXCEPT
	{
		// Cannot use == and != with NaN floats
#if defined(RTM_SSE2_INTRINSICS)
		// WARNING: SSE2 doesn't have a 64 bit int compare, we use 32 bit here and we assume
		// that in a mask all bits are equal
		__m128i lhs_xy = _mm_castpd_si128(lhs.xy);
		__m128i rhs_xy = _mm_castpd_si128(rhs.xy);
		return _mm_movemask_epi8(_mm_cmpeq_epi32(lhs_xy, rhs_xy)) != 0;
#elif defined(RTM_SSE4_INTRINSICS) && 0
		// TODO
#else
		return std::memcmp(&lhs.x, &rhs.x, sizeof(uint64_t)) == 0
			|| std::memcmp(&lhs.y, &rhs.y, sizeof(uint64_t)) == 0;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if any [xyz] components are equal, otherwise false: any(lhs.xyz == rhs.xyz)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL mask_any_equal3(mask4d_arg0 lhs, mask4d_arg1 rhs) RTM_NO_EXCEPT
	{
		// Cannot use == and != with NaN floats
#if defined(RTM_SSE2_INTRINSICS)
		// WARNING: SSE2 doesn't have a 64 bit int compare, we use 32 bit here and we assume
		// that in a mask all bits are equal
		__m128i lhs_xy = _mm_castpd_si128(lhs.xy);
		__m128i lhs_zw = _mm_castpd_si128(lhs.zw);
		__m128i rhs_xy = _mm_castpd_si128(rhs.xy);
		__m128i rhs_zw = _mm_castpd_si128(rhs.zw);
		__m128i xy_eq = _mm_cmpeq_epi32(lhs_xy, rhs_xy);
		__m128i zw_eq = _mm_cmpeq_epi32(lhs_zw, rhs_zw);
		return _mm_movemask_epi8(xy_eq) != 0 || (_mm_movemask_epi8(zw_eq) & 0x00FF) != 0;
#elif defined(RTM_SSE4_INTRINSICS) && 0
		// TODO
#else
		return std::memcmp(&lhs.x, &rhs.x, sizeof(uint64_t)) == 0
			|| std::memcmp(&lhs.y, &rhs.y, sizeof(uint64_t)) == 0
			|| std::memcmp(&lhs.z, &rhs.z, sizeof(uint64_t)) == 0;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Per component logical AND between the inputs: lhs & rhs
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE mask4d RTM_SIMD_CALL mask_and(mask4d_arg0 lhs, mask4d_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		__m128d xy = _mm_and_pd(lhs.xy, rhs.xy);
		__m128d zw = _mm_and_pd(lhs.zw, rhs.zw);
		return mask4d{ xy, zw };
#else
		const uint64_t* lhs_ = rtm_impl::bit_cast<const uint64_t*>(&lhs);
		const uint64_t* rhs_ = rtm_impl::bit_cast<const uint64_t*>(&rhs);

		union
		{
			mask4d vector;
			uint64_t scalar[4];
		} result;

		result.scalar[0] = lhs_[0] & rhs_[0];
		result.scalar[1] = lhs_[1] & rhs_[1];
		result.scalar[2] = lhs_[2] & rhs_[2];
		result.scalar[3] = lhs_[3] & rhs_[3];

		return result.vector;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Per component logical OR between the inputs: lhs | rhs
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE mask4d RTM_SIMD_CALL mask_or(mask4d_arg0 lhs, mask4d_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		__m128d xy = _mm_or_pd(lhs.xy, rhs.xy);
		__m128d zw = _mm_or_pd(lhs.zw, rhs.zw);
		return mask4d{ xy, zw };
#else
		const uint64_t* lhs_ = rtm_impl::bit_cast<const uint64_t*>(&lhs);
		const uint64_t* rhs_ = rtm_impl::bit_cast<const uint64_t*>(&rhs);

		union
		{
			mask4d vector;
			uint64_t scalar[4];
		} result;

		result.scalar[0] = lhs_[0] | rhs_[0];
		result.scalar[1] = lhs_[1] | rhs_[1];
		result.scalar[2] = lhs_[2] | rhs_[2];
		result.scalar[3] = lhs_[3] | rhs_[3];

		return result.vector;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Per component logical XOR between the inputs: lhs ^ rhs
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE mask4d RTM_SIMD_CALL mask_xor(mask4d_arg0 lhs, mask4d_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		__m128d xy = _mm_xor_pd(lhs.xy, rhs.xy);
		__m128d zw = _mm_xor_pd(lhs.zw, rhs.zw);
		return mask4d{ xy, zw };
#else
		const uint64_t* lhs_ = rtm_impl::bit_cast<const uint64_t*>(&lhs);
		const uint64_t* rhs_ = rtm_impl::bit_cast<const uint64_t*>(&rhs);

		union
		{
			mask4d vector;
			uint64_t scalar[4];
		} result;

		result.scalar[0] = lhs_[0] ^ rhs_[0];
		result.scalar[1] = lhs_[1] ^ rhs_[1];
		result.scalar[2] = lhs_[2] ^ rhs_[2];
		result.scalar[3] = lhs_[3] ^ rhs_[3];

		return result.vector;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Per component logical NOT of the input: ~input
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE mask4d RTM_SIMD_CALL mask_not(mask4d_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		const __m128i true_mask = _mm_set_epi64x(0xFFFFFFFFFFFFFFFFULL, 0xFFFFFFFFFFFFFFFFULL);
		__m128d xy = _mm_andnot_pd(input.xy, _mm_castsi128_pd(true_mask));
		__m128d zw = _mm_andnot_pd(input.zw, _mm_castsi128_pd(true_mask));
		return mask4d{ xy, zw };
#else
		const uint64_t* input_ = rtm_impl::bit_cast<const uint64_t*>(&input);

		union
		{
			mask4d vector;
			uint64_t scalar[4];
		} result;

		result.scalar[0] = ~input_[0];
		result.scalar[1] = ~input_[1];
		result.scalar[2] = ~input_[2];
		result.scalar[3] = ~input_[3];

		return result.vector;
#endif
	}

	RTM_IMPL_VERSION_NAMESPACE_END
}

RTM_IMPL_FILE_PRAGMA_POP

```

`includes/rtm/mask4f.h`:

```h
#pragma once

////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include "rtm/math.h"
#include "rtm/version.h"
#include "rtm/impl/bit_cast.impl.h"
#include "rtm/impl/compiler_utils.h"
#include "rtm/impl/macros.mask4.impl.h"
#include "rtm/impl/mask_common.h"

#if !defined(RTM_SSE2_INTRINSICS)
	#include <cstring>
#endif

RTM_IMPL_FILE_PRAGMA_PUSH

namespace rtm
{
	RTM_IMPL_VERSION_NAMESPACE_BEGIN

	//////////////////////////////////////////////////////////////////////////
	// Returns the mask4f [x] component.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE uint32_t RTM_SIMD_CALL mask_get_x(mask4f_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return static_cast<uint32_t>(_mm_cvtsi128_si32(_mm_castps_si128(input)));
#elif defined(RTM_NEON_INTRINSICS)
		return vgetq_lane_u32(input, 0);
#else
		return input.x;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the mask4f [y] component.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE uint32_t RTM_SIMD_CALL mask_get_y(mask4f_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return static_cast<uint32_t>(_mm_cvtsi128_si32(_mm_castps_si128(_mm_shuffle_ps(input, input, _MM_SHUFFLE(1, 1, 1, 1)))));
#elif defined(RTM_NEON_INTRINSICS)
		return vgetq_lane_u32(input, 1);
#else
		return input.y;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the mask4f [z] component.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE uint32_t RTM_SIMD_CALL mask_get_z(mask4f_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return static_cast<uint32_t>(_mm_cvtsi128_si32(_mm_castps_si128(_mm_shuffle_ps(input, input, _MM_SHUFFLE(2, 2, 2, 2)))));
#elif defined(RTM_NEON_INTRINSICS)
		return vgetq_lane_u32(input, 2);
#else
		return input.z;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the mask4f [w] component.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE uint32_t RTM_SIMD_CALL mask_get_w(mask4f_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return static_cast<uint32_t>(_mm_cvtsi128_si32(_mm_castps_si128(_mm_shuffle_ps(input, input, _MM_SHUFFLE(3, 3, 3, 3)))));
#elif defined(RTM_NEON_INTRINSICS)
		return vgetq_lane_u32(input, 3);
#else
		return input.w;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if all 4 components are true, otherwise false: all(input.xyzw != 0)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL mask_all_true(mask4f_arg0 input) RTM_NO_EXCEPT
	{
		bool result;
		RTM_MASK4F_ALL_TRUE(input, result);
		return result;
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if all [xy] components are true, otherwise false: all(input.xy != 0)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL mask_all_true2(mask4f_arg0 input) RTM_NO_EXCEPT
	{
		bool result;
		RTM_MASK4F_ALL_TRUE2(input, result);
		return result;
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if all [xyz] components are true, otherwise false: all(input.xyz != 0)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL mask_all_true3(mask4f_arg0 input) RTM_NO_EXCEPT
	{
		bool result;
		RTM_MASK4F_ALL_TRUE3(input, result);
		return result;
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if any 4 components are true, otherwise false: any(input.xyzw != 0)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL mask_any_true(mask4f_arg0 input) RTM_NO_EXCEPT
	{
		bool result;
		RTM_MASK4F_ANY_TRUE(input, result);
		return result;
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if any [xy] components are true, otherwise false: any(input.xy != 0)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL mask_any_true2(mask4f_arg0 input) RTM_NO_EXCEPT
	{
		bool result;
		RTM_MASK4F_ANY_TRUE2(input, result);
		return result;
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if any [xyz] components are true, otherwise false: any(input.xyz != 0)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL mask_any_true3(mask4f_arg0 input) RTM_NO_EXCEPT
	{
		bool result;
		RTM_MASK4F_ANY_TRUE3(input, result);
		return result;
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if all 4 components are equal, otherwise false: all(lhs.xyzw == rhs.xyzw)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL mask_all_equal(mask4f_arg0 lhs, mask4f_arg1 rhs) RTM_NO_EXCEPT
	{
		// Cannot use == and != with NaN floats
		// Masks are always 0 or ~0, use this to our advantage
		// lhs ^ rhs = 0 if both are equal and != 0 if they are not
#if defined(RTM_SSE2_INTRINSICS)
		return _mm_movemask_ps(_mm_xor_ps(lhs, rhs)) == 0;
#elif defined(RTM_NEON_INTRINSICS)
		uint32x4_t mask = veorq_u32(lhs, rhs);
		uint16x4_t truncated_mask = vmovn_u32(mask);
		return vget_lane_u64(vreinterpret_u64_u16(truncated_mask), 0) == 0;
#else
		return std::memcmp(&lhs, &rhs, sizeof(uint32_t) * 4) == 0;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if all [xy] components are equal, otherwise false: all(lhs.xy == rhs.xy)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL mask_all_equal2(mask4f_arg0 lhs, mask4f_arg1 rhs) RTM_NO_EXCEPT
	{
		// Cannot use == and != with NaN floats
		// Masks are always 0 or ~0, use this to our advantage
		// lhs ^ rhs = 0 if both are equal and != 0 if they are not
#if defined(RTM_SSE2_INTRINSICS)
		return (_mm_movemask_ps(_mm_xor_ps(lhs, rhs)) & 0x03) == 0;
#elif defined(RTM_NEON_INTRINSICS)
		uint32x2_t mask = veor_u32(vget_low_u32(lhs), vget_low_u32(rhs));
		return vget_lane_u64(vreinterpret_u64_u32(mask), 0) == 0;
#else
		return std::memcmp(&lhs, &rhs, sizeof(uint32_t) * 2) == 0;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if all [xyz] components are equal, otherwise false: all(lhs.xyz == rhs.xyz)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL mask_all_equal3(mask4f_arg0 lhs, mask4f_arg1 rhs) RTM_NO_EXCEPT
	{
		// Cannot use == and != with NaN floats
		// Masks are always 0 or ~0, use this to our advantage
		// lhs ^ rhs = 0 if both are equal and != 0 if they are not
#if defined(RTM_SSE2_INTRINSICS)
		return (_mm_movemask_ps(_mm_xor_ps(lhs, rhs)) & 0x07) == 0;
#elif defined(RTM_NEON_INTRINSICS)
		uint32x4_t mask = veorq_u32(lhs, rhs);
		uint16x4_t truncated_mask = vmovn_u32(mask);
		return (vget_lane_u64(vreinterpret_u64_u16(truncated_mask), 0) & 0x0000FFFFFFFFFFFFULL) == 0;
#else
		return std::memcmp(&lhs, &rhs, sizeof(uint32_t) * 3) == 0;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if any 4 components are equal, otherwise false: any(lhs.xyzw == rhs.xyzw)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL mask_any_equal(mask4f_arg0 lhs, mask4f_arg1 rhs) RTM_NO_EXCEPT
	{
		// Cannot use == and != with NaN floats
		// Masks are always 0 or ~0, use this to our advantage
		// lhs ^ rhs = 0 if both are equal and != 0 if they are not
#if defined(RTM_SSE2_INTRINSICS)
		return _mm_movemask_ps(_mm_xor_ps(lhs, rhs)) != 0x0F;
#elif defined(RTM_NEON_INTRINSICS)
		uint32x4_t mask = veorq_u32(lhs, rhs);
		uint16x4_t truncated_mask = vmovn_u32(mask);
		return vget_lane_u64(vreinterpret_u64_u16(truncated_mask), 0) != 0xFFFFFFFFFFFFFFFFULL;
#else
		return std::memcmp(&lhs.x, &rhs.x, sizeof(uint32_t)) == 0
			|| std::memcmp(&lhs.y, &rhs.y, sizeof(uint32_t)) == 0
			|| std::memcmp(&lhs.z, &rhs.z, sizeof(uint32_t)) == 0
			|| std::memcmp(&lhs.w, &rhs.w, sizeof(uint32_t)) == 0;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if any [xy] components are equal, otherwise false: any(lhs.xy == rhs.xy)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL mask_any_equal2(mask4f_arg0 lhs, mask4f_arg1 rhs) RTM_NO_EXCEPT
	{
		// Cannot use == and != with NaN floats
		// Masks are always 0 or ~0, use this to our advantage
		// lhs ^ rhs = 0 if both are equal and != 0 if they are not
#if defined(RTM_SSE2_INTRINSICS)
		return (_mm_movemask_ps(_mm_xor_ps(lhs, rhs)) & 0x03) != 0x03;
#elif defined(RTM_NEON_INTRINSICS)
		uint32x2_t mask = veor_u32(vget_low_u32(lhs), vget_low_u32(rhs));
		return vget_lane_u64(vreinterpret_u64_u32(mask), 0) != 0xFFFFFFFFFFFFFFFFULL;
#else
		return std::memcmp(&lhs.x, &rhs.x, sizeof(uint32_t)) == 0
			|| std::memcmp(&lhs.y, &rhs.y, sizeof(uint32_t)) == 0;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if any [xyz] components are equal, otherwise false: any(lhs.xyz == rhs.xyz)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL mask_any_equal3(mask4f_arg0 lhs, mask4f_arg1 rhs) RTM_NO_EXCEPT
	{
		// Cannot use == and != with NaN floats
		// Masks are always 0 or ~0, use this to our advantage
		// lhs ^ rhs = 0 if both are equal and != 0 if they are not
#if defined(RTM_SSE2_INTRINSICS)
		return (_mm_movemask_ps(_mm_xor_ps(lhs, rhs)) & 0x07) != 0x07;
#elif defined(RTM_NEON_INTRINSICS)
		uint32x4_t mask = veorq_u32(lhs, rhs);
		uint16x4_t truncated_mask = vmovn_u32(mask);
		return (vget_lane_u64(vreinterpret_u64_u16(truncated_mask), 0) & 0x0000FFFFFFFFFFFFULL) != 0x0000FFFFFFFFFFFFULL;
#else
		return std::memcmp(&lhs.x, &rhs.x, sizeof(uint32_t)) == 0
			|| std::memcmp(&lhs.y, &rhs.y, sizeof(uint32_t)) == 0
			|| std::memcmp(&lhs.z, &rhs.z, sizeof(uint32_t)) == 0;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Per component logical AND between the inputs: lhs & rhs
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE mask4f RTM_SIMD_CALL mask_and(mask4f_arg0 lhs, mask4f_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return _mm_and_ps(lhs, rhs);
#elif defined(RTM_NEON_INTRINSICS)
		return vandq_u32(lhs, rhs);
#else
		const uint32_t* lhs_ = rtm_impl::bit_cast<const uint32_t*>(&lhs);
		const uint32_t* rhs_ = rtm_impl::bit_cast<const uint32_t*>(&rhs);

		union
		{
			mask4f vector;
			uint32_t scalar[4];
		} result;

		result.scalar[0] = lhs_[0] & rhs_[0];
		result.scalar[1] = lhs_[1] & rhs_[1];
		result.scalar[2] = lhs_[2] & rhs_[2];
		result.scalar[3] = lhs_[3] & rhs_[3];

		return result.vector;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Per component logical OR between the inputs: lhs | rhs
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE mask4f RTM_SIMD_CALL mask_or(mask4f_arg0 lhs, mask4f_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return _mm_or_ps(lhs, rhs);
#elif defined(RTM_NEON_INTRINSICS)
		return vorrq_u32(lhs, rhs);
#else
		const uint32_t* lhs_ = rtm_impl::bit_cast<const uint32_t*>(&lhs);
		const uint32_t* rhs_ = rtm_impl::bit_cast<const uint32_t*>(&rhs);

		union
		{
			mask4f vector;
			uint32_t scalar[4];
		} result;

		result.scalar[0] = lhs_[0] | rhs_[0];
		result.scalar[1] = lhs_[1] | rhs_[1];
		result.scalar[2] = lhs_[2] | rhs_[2];
		result.scalar[3] = lhs_[3] | rhs_[3];

		return result.vector;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Per component logical XOR between the inputs: lhs ^ rhs
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE mask4f RTM_SIMD_CALL mask_xor(mask4f_arg0 lhs, mask4f_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return _mm_xor_ps(lhs, rhs);
#elif defined(RTM_NEON_INTRINSICS)
		return veorq_u32(lhs, rhs);
#else
		const uint32_t* lhs_ = rtm_impl::bit_cast<const uint32_t*>(&lhs);
		const uint32_t* rhs_ = rtm_impl::bit_cast<const uint32_t*>(&rhs);

		union
		{
			mask4f vector;
			uint32_t scalar[4];
		} result;

		result.scalar[0] = lhs_[0] ^ rhs_[0];
		result.scalar[1] = lhs_[1] ^ rhs_[1];
		result.scalar[2] = lhs_[2] ^ rhs_[2];
		result.scalar[3] = lhs_[3] ^ rhs_[3];

		return result.vector;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Per component logical NOT of the input: ~input
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE mask4f RTM_SIMD_CALL mask_not(mask4f_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		const __m128i true_mask = _mm_set_epi32(0xFFFFFFFFULL, 0xFFFFFFFFULL, 0xFFFFFFFFULL, 0xFFFFFFFFULL);
		return _mm_andnot_ps(input, _mm_castsi128_ps(true_mask));
#elif defined(RTM_NEON_INTRINSICS)
		return vmvnq_u32(input);
#else
		const uint32_t* input_ = rtm_impl::bit_cast<const uint32_t*>(&input);

		union
		{
			mask4f vector;
			uint32_t scalar[4];
		} result;

		result.scalar[0] = ~input_[0];
		result.scalar[1] = ~input_[1];
		result.scalar[2] = ~input_[2];
		result.scalar[3] = ~input_[3];

		return result.vector;
#endif
	}

	RTM_IMPL_VERSION_NAMESPACE_END
}

RTM_IMPL_FILE_PRAGMA_POP

```

`includes/rtm/mask4i.h`:

```h
#pragma once

////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2019 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include "rtm/math.h"
#include "rtm/version.h"
#include "rtm/impl/compiler_utils.h"
#include "rtm/impl/mask_common.h"

RTM_IMPL_FILE_PRAGMA_PUSH

namespace rtm
{
	RTM_IMPL_VERSION_NAMESPACE_BEGIN

	//////////////////////////////////////////////////////////////////////////
	// Returns the mask4i [x] component.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE uint32_t RTM_SIMD_CALL mask_get_x(mask4i_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return static_cast<uint32_t>(_mm_cvtsi128_si32(input));
#elif defined(RTM_NEON_INTRINSICS)
		return vgetq_lane_u32(RTM_IMPL_MASK4i_GET(input), 0);
#else
		return input.x;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the mask4i [y] component.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE uint32_t RTM_SIMD_CALL mask_get_y(mask4i_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return static_cast<uint32_t>(_mm_cvtsi128_si32(_mm_shuffle_epi32(input, _MM_SHUFFLE(1, 1, 1, 1))));
#elif defined(RTM_NEON_INTRINSICS)
		return vgetq_lane_u32(RTM_IMPL_MASK4i_GET(input), 1);
#else
		return input.y;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the mask4i [z] component.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE uint32_t RTM_SIMD_CALL mask_get_z(mask4i_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return static_cast<uint32_t>(_mm_cvtsi128_si32(_mm_shuffle_epi32(input, _MM_SHUFFLE(2, 2, 2, 2))));
#elif defined(RTM_NEON_INTRINSICS)
		return vgetq_lane_u32(RTM_IMPL_MASK4i_GET(input), 2);
#else
		return input.z;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the mask4i [w] component.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE uint32_t RTM_SIMD_CALL mask_get_w(mask4i_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return static_cast<uint32_t>(_mm_cvtsi128_si32(_mm_shuffle_epi32(input, _MM_SHUFFLE(3, 3, 3, 3))));
#elif defined(RTM_NEON_INTRINSICS)
		return vgetq_lane_u32(RTM_IMPL_MASK4i_GET(input), 3);
#else
		return input.w;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if all 4 components are true, otherwise false: all(input.xyzw != 0)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL mask_all_true(mask4i_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return _mm_movemask_epi8(input) == 0xFFFF;
#elif defined(RTM_NEON_INTRINSICS)
		uint8x16_t mask = vreinterpretq_u8_u32(RTM_IMPL_MASK4i_GET(input));
		uint8x8x2_t mask_0_8_1_9_2_10_3_11_4_12_5_13_6_14_7_15 = vzip_u8(vget_low_u8(mask), vget_high_u8(mask));
		uint16x4x2_t mask_0_8_4_12_1_9_5_13_2_10_6_14_3_11_7_15 = vzip_u16(vreinterpret_u16_u8(mask_0_8_1_9_2_10_3_11_4_12_5_13_6_14_7_15.val[0]), vreinterpret_u16_u8(mask_0_8_1_9_2_10_3_11_4_12_5_13_6_14_7_15.val[1]));
		return vget_lane_u32(vreinterpret_u32_u16(mask_0_8_4_12_1_9_5_13_2_10_6_14_3_11_7_15.val[0]), 0) == 0xFFFFFFFFU;
#else
		return input.x != 0 && input.y != 0 && input.z != 0 && input.w != 0;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if all [xy] components are true, otherwise false: all(input.xy != 0)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL mask_all_true2(mask4i_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return (_mm_movemask_epi8(input) & 0x00FF) == 0x00FF;
#elif defined(RTM_NEON_INTRINSICS)
		return vgetq_lane_u64(vreinterpretq_u64_u32(RTM_IMPL_MASK4i_GET(input)), 0) == 0xFFFFFFFFFFFFFFFFULL;
#else
		return input.x != 0 && input.y != 0;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if all [xyz] components are true, otherwise false: all(input.xyz != 0)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL mask_all_true3(mask4i_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return (_mm_movemask_epi8(input) & 0x0FFF) == 0x0FFF;
#elif defined(RTM_NEON_INTRINSICS)
		uint8x16_t mask = vreinterpretq_u8_u32(RTM_IMPL_MASK4i_GET(input));
		uint8x8x2_t mask_0_8_1_9_2_10_3_11_4_12_5_13_6_14_7_15 = vzip_u8(vget_low_u8(mask), vget_high_u8(mask));
		uint16x4x2_t mask_0_8_4_12_1_9_5_13_2_10_6_14_3_11_7_15 = vzip_u16(vreinterpret_u16_u8(mask_0_8_1_9_2_10_3_11_4_12_5_13_6_14_7_15.val[0]), vreinterpret_u16_u8(mask_0_8_1_9_2_10_3_11_4_12_5_13_6_14_7_15.val[1]));
		return (vget_lane_u32(vreinterpret_u32_u16(mask_0_8_4_12_1_9_5_13_2_10_6_14_3_11_7_15.val[0]), 0) & 0x00FFFFFFU) == 0x00FFFFFFU;
#else
		return input.x != 0 && input.y != 0 && input.z != 0;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if any 4 components are true, otherwise false: any(input.xyzw != 0)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL mask_any_true(mask4i_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return _mm_movemask_epi8(input) != 0;
#elif defined(RTM_NEON_INTRINSICS)
		uint8x16_t mask = vreinterpretq_u8_u32(RTM_IMPL_MASK4i_GET(input));
		uint8x8x2_t mask_0_8_1_9_2_10_3_11_4_12_5_13_6_14_7_15 = vzip_u8(vget_low_u8(mask), vget_high_u8(mask));
		uint16x4x2_t mask_0_8_4_12_1_9_5_13_2_10_6_14_3_11_7_15 = vzip_u16(vreinterpret_u16_u8(mask_0_8_1_9_2_10_3_11_4_12_5_13_6_14_7_15.val[0]), vreinterpret_u16_u8(mask_0_8_1_9_2_10_3_11_4_12_5_13_6_14_7_15.val[1]));
		return vget_lane_u32(vreinterpret_u32_u16(mask_0_8_4_12_1_9_5_13_2_10_6_14_3_11_7_15.val[0]), 0) != 0;
#else
		return input.x != 0 || input.y != 0 || input.z != 0 || input.w != 0;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if any [xy] components are true, otherwise false: any(input.xy != 0)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL mask_any_true2(mask4i_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return (_mm_movemask_epi8(input) & 0x00FF) != 0;
#elif defined(RTM_NEON_INTRINSICS)
		return vgetq_lane_u64(vreinterpretq_u64_u32(RTM_IMPL_MASK4i_GET(input)), 0) != 0;
#else
		return input.x != 0 || input.y != 0;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if any [xyz] components are true, otherwise false: any(input.xyz != 0)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL mask_any_true3(mask4i_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return (_mm_movemask_epi8(input) & 0x0FFF) != 0;
#elif defined(RTM_NEON_INTRINSICS)
		uint8x16_t mask = vreinterpretq_u8_u32(RTM_IMPL_MASK4i_GET(input));
		uint8x8x2_t mask_0_8_1_9_2_10_3_11_4_12_5_13_6_14_7_15 = vzip_u8(vget_low_u8(mask), vget_high_u8(mask));
		uint16x4x2_t mask_0_8_4_12_1_9_5_13_2_10_6_14_3_11_7_15 = vzip_u16(vreinterpret_u16_u8(mask_0_8_1_9_2_10_3_11_4_12_5_13_6_14_7_15.val[0]), vreinterpret_u16_u8(mask_0_8_1_9_2_10_3_11_4_12_5_13_6_14_7_15.val[1]));
		return (vget_lane_u32(vreinterpret_u32_u16(mask_0_8_4_12_1_9_5_13_2_10_6_14_3_11_7_15.val[0]), 0) & 0x00FFFFFFU) != 0;
#else
		return input.x != 0 || input.y != 0 || input.z != 0;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if all 4 components are equal, otherwise false: all(lhs.xyzw == rhs.xyzw)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL mask_all_equal(mask4i_arg0 lhs, mask4i_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return _mm_movemask_epi8(_mm_cmpeq_epi32(lhs, rhs)) == 0xFFFF;
#elif defined(RTM_NEON_INTRINSICS)
		uint8x16_t mask = vreinterpretq_u8_u32(vceqq_u32(RTM_IMPL_MASK4i_GET(lhs), RTM_IMPL_MASK4i_GET(rhs)));
		uint8x8x2_t mask_0_8_1_9_2_10_3_11_4_12_5_13_6_14_7_15 = vzip_u8(vget_low_u8(mask), vget_high_u8(mask));
		uint16x4x2_t mask_0_8_4_12_1_9_5_13_2_10_6_14_3_11_7_15 = vzip_u16(vreinterpret_u16_u8(mask_0_8_1_9_2_10_3_11_4_12_5_13_6_14_7_15.val[0]), vreinterpret_u16_u8(mask_0_8_1_9_2_10_3_11_4_12_5_13_6_14_7_15.val[1]));
		return vget_lane_u32(vreinterpret_u32_u16(mask_0_8_4_12_1_9_5_13_2_10_6_14_3_11_7_15.val[0]), 0) == 0xFFFFFFFFU;
#else
		return lhs.x == rhs.x && lhs.y == rhs.y && lhs.z == rhs.z && lhs.w == rhs.w;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if all [xy] components are equal, otherwise false: all(lhs.xy == rhs.xy)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL mask_all_equal2(mask4i_arg0 lhs, mask4i_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return (_mm_movemask_epi8(_mm_cmpeq_epi32(lhs, rhs)) & 0x00FF) == 0x00FF;
#elif defined(RTM_NEON_INTRINSICS)
		uint32x2_t mask = vceq_u32(vget_low_u32(RTM_IMPL_MASK4i_GET(lhs)), vget_low_u32(RTM_IMPL_MASK4i_GET(rhs)));
		return vget_lane_u64(vreinterpret_u64_u32(mask), 0) == 0xFFFFFFFFFFFFFFFFu;
#else
		return lhs.x == rhs.x && lhs.y == rhs.y;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if all [xyz] components are equal, otherwise false: all(lhs.xyz == rhs.xyz)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL mask_all_equal3(mask4i_arg0 lhs, mask4i_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return (_mm_movemask_epi8(_mm_cmpeq_epi32(lhs, rhs)) & 0x0FFF) == 0x0FFF;
#elif defined(RTM_NEON_INTRINSICS)
		uint8x16_t mask = vreinterpretq_u8_u32(vceqq_u32(RTM_IMPL_MASK4i_GET(lhs), RTM_IMPL_MASK4i_GET(rhs)));
		uint8x8x2_t mask_0_8_1_9_2_10_3_11_4_12_5_13_6_14_7_15 = vzip_u8(vget_low_u8(mask), vget_high_u8(mask));
		uint16x4x2_t mask_0_8_4_12_1_9_5_13_2_10_6_14_3_11_7_15 = vzip_u16(vreinterpret_u16_u8(mask_0_8_1_9_2_10_3_11_4_12_5_13_6_14_7_15.val[0]), vreinterpret_u16_u8(mask_0_8_1_9_2_10_3_11_4_12_5_13_6_14_7_15.val[1]));
		return (vget_lane_u32(vreinterpret_u32_u16(mask_0_8_4_12_1_9_5_13_2_10_6_14_3_11_7_15.val[0]), 0) & 0x00FFFFFFU) == 0x00FFFFFFU;
#else
		return lhs.x == rhs.x && lhs.y == rhs.y && lhs.z == rhs.z;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if any 4 components are equal, otherwise false: any(lhs.xyzw == rhs.xyzw)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL mask_any_equal(mask4i_arg0 lhs, mask4i_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return _mm_movemask_epi8(_mm_cmpeq_epi32(lhs, rhs)) != 0;
#elif defined(RTM_NEON_INTRINSICS)
		uint8x16_t mask = vreinterpretq_u8_u32(vceqq_u32(RTM_IMPL_MASK4i_GET(lhs), RTM_IMPL_MASK4i_GET(rhs)));
		uint8x8x2_t mask_0_8_1_9_2_10_3_11_4_12_5_13_6_14_7_15 = vzip_u8(vget_low_u8(mask), vget_high_u8(mask));
		uint16x4x2_t mask_0_8_4_12_1_9_5_13_2_10_6_14_3_11_7_15 = vzip_u16(vreinterpret_u16_u8(mask_0_8_1_9_2_10_3_11_4_12_5_13_6_14_7_15.val[0]), vreinterpret_u16_u8(mask_0_8_1_9_2_10_3_11_4_12_5_13_6_14_7_15.val[1]));
		return vget_lane_u32(vreinterpret_u32_u16(mask_0_8_4_12_1_9_5_13_2_10_6_14_3_11_7_15.val[0]), 0) != 0;
#else
		return lhs.x == rhs.x || lhs.y == rhs.y || lhs.z == rhs.z || lhs.w == rhs.w;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if any [xy] components are equal, otherwise false: any(lhs.xy == rhs.xy)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL mask_any_equal2(mask4i_arg0 lhs, mask4i_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return (_mm_movemask_epi8(_mm_cmpeq_epi32(lhs, rhs)) & 0x00FF) != 0;
#elif defined(RTM_NEON_INTRINSICS)
		uint32x2_t mask = vceq_u32(vget_low_u32(RTM_IMPL_MASK4i_GET(lhs)), vget_low_u32(RTM_IMPL_MASK4i_GET(rhs)));
		return vget_lane_u64(vreinterpret_u64_u32(mask), 0) != 0;
#else
		return lhs.x == rhs.x || lhs.y == rhs.y;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if any [xyz] components are equal, otherwise false: any(lhs.xyz == rhs.xyz)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL mask_any_equal3(mask4i_arg0 lhs, mask4i_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return (_mm_movemask_epi8(_mm_cmpeq_epi32(lhs, rhs)) & 0x0FFF) != 0;
#elif defined(RTM_NEON_INTRINSICS)
		uint8x16_t mask = vreinterpretq_u8_u32(vceqq_u32(RTM_IMPL_MASK4i_GET(lhs), RTM_IMPL_MASK4i_GET(rhs)));
		uint8x8x2_t mask_0_8_1_9_2_10_3_11_4_12_5_13_6_14_7_15 = vzip_u8(vget_low_u8(mask), vget_high_u8(mask));
		uint16x4x2_t mask_0_8_4_12_1_9_5_13_2_10_6_14_3_11_7_15 = vzip_u16(vreinterpret_u16_u8(mask_0_8_1_9_2_10_3_11_4_12_5_13_6_14_7_15.val[0]), vreinterpret_u16_u8(mask_0_8_1_9_2_10_3_11_4_12_5_13_6_14_7_15.val[1]));
		return (vget_lane_u32(vreinterpret_u32_u16(mask_0_8_4_12_1_9_5_13_2_10_6_14_3_11_7_15.val[0]), 0) & 0x00FFFFFFU) != 0;
#else
		return lhs.x == rhs.x || lhs.y == rhs.y || lhs.z == rhs.z;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Per component logical AND between the inputs: lhs & rhs
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE mask4i RTM_SIMD_CALL mask_and(mask4i_arg0 lhs, mask4i_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return _mm_and_si128(lhs, rhs);
#elif defined(RTM_NEON_INTRINSICS)
		return RTM_IMPL_MASK4i_SET(vandq_u32(RTM_IMPL_MASK4i_GET(lhs), RTM_IMPL_MASK4i_GET(rhs)));
#else
		return mask4i{ lhs.x & rhs.x, lhs.y & rhs.y, lhs.z & rhs.z, lhs.w & rhs.w };
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Per component logical OR between the inputs: lhs | rhs
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE mask4i RTM_SIMD_CALL mask_or(mask4i_arg0 lhs, mask4i_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return _mm_or_si128(lhs, rhs);
#elif defined(RTM_NEON_INTRINSICS)
		return RTM_IMPL_MASK4i_SET(vorrq_u32(RTM_IMPL_MASK4i_GET(lhs), RTM_IMPL_MASK4i_GET(rhs)));
#else
		return mask4i{ lhs.x | rhs.x, lhs.y | rhs.y, lhs.z | rhs.z, lhs.w | rhs.w };
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Per component logical XOR between the inputs: lhs ^ rhs
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE mask4i RTM_SIMD_CALL mask_xor(mask4i_arg0 lhs, mask4i_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return _mm_xor_si128(lhs, rhs);
#elif defined(RTM_NEON_INTRINSICS)
		return RTM_IMPL_MASK4i_SET(veorq_u32(RTM_IMPL_MASK4i_GET(lhs), RTM_IMPL_MASK4i_GET(rhs)));
#else
		return mask4i{ lhs.x ^ rhs.x, lhs.y ^ rhs.y, lhs.z ^ rhs.z, lhs.w ^ rhs.w };
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Per component logical NOT of the input: ~input
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE mask4i RTM_SIMD_CALL mask_not(mask4i_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		const __m128i true_mask = _mm_set_epi32(0xFFFFFFFFULL, 0xFFFFFFFFULL, 0xFFFFFFFFULL, 0xFFFFFFFFULL);
		return _mm_andnot_si128(input, true_mask);
#elif defined(RTM_NEON_INTRINSICS)
		return RTM_IMPL_MASK4i_SET(vmvnq_u32(RTM_IMPL_MASK4i_GET(input)));
#else
		return mask4i{ ~input.x, ~input.y, ~input.z, ~input.w };
#endif
	}

	RTM_IMPL_VERSION_NAMESPACE_END
}

RTM_IMPL_FILE_PRAGMA_POP

```

`includes/rtm/mask4q.h`:

```h
#pragma once

////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2019 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include "rtm/math.h"
#include "rtm/version.h"
#include "rtm/impl/bit_cast.impl.h"
#include "rtm/impl/compiler_utils.h"
#include "rtm/impl/mask_common.h"

RTM_IMPL_FILE_PRAGMA_PUSH

namespace rtm
{
	RTM_IMPL_VERSION_NAMESPACE_BEGIN

	//////////////////////////////////////////////////////////////////////////
	// Returns the mask4q [x] component.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE uint64_t RTM_SIMD_CALL mask_get_x(mask4q_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
#if defined(RTM_ARCH_X64)
		return static_cast<uint64_t>(_mm_cvtsi128_si64(input.xy));
#else
		// Just sign extend on 32bit systems
		return static_cast<uint64_t>(_mm_cvtsi128_si32(input.xy));
#endif
#else
		return input.x;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the mask4q [y] component.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE uint64_t RTM_SIMD_CALL mask_get_y(mask4q_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
#if defined(RTM_ARCH_X64)
		return static_cast<uint64_t>(_mm_cvtsi128_si64(_mm_shuffle_epi32(input.xy, _MM_SHUFFLE(3, 2, 3, 2))));
#else
		// Just sign extend on 32bit systems
		return static_cast<uint64_t>(_mm_cvtsi128_si32(_mm_shuffle_epi32(input.xy, _MM_SHUFFLE(3, 2, 3, 2))));
#endif
#else
		return input.y;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the mask4q [z] component.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE uint64_t RTM_SIMD_CALL mask_get_z(mask4q_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
#if defined(RTM_ARCH_X64)
		return static_cast<uint64_t>(_mm_cvtsi128_si64(input.zw));
#else
		// Just sign extend on 32bit systems
		return static_cast<uint64_t>(_mm_cvtsi128_si32(input.zw));
#endif
#else
		return input.z;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the mask4q [w] component.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE uint64_t RTM_SIMD_CALL mask_get_w(mask4q_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
#if defined(RTM_ARCH_X64)
		return static_cast<uint64_t>(_mm_cvtsi128_si64(_mm_shuffle_epi32(input.zw, _MM_SHUFFLE(3, 2, 3, 2))));
#else
		// Just sign extend on 32bit systems
		return static_cast<uint64_t>(_mm_cvtsi128_si32(_mm_shuffle_epi32(input.zw, _MM_SHUFFLE(3, 2, 3, 2))));
#endif
#else
		return input.w;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if all 4 components are true, otherwise false: all(input.xyzw != 0)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL mask_all_true(mask4q_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return (_mm_movemask_epi8(input.xy) & _mm_movemask_epi8(input.zw)) == 0xFFFF;
#else
		return input.x != 0 && input.y != 0 && input.z != 0 && input.w != 0;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if all [xy] components are true, otherwise false: all(input.xy != 0)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL mask_all_true2(mask4q_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return _mm_movemask_epi8(input.xy) == 0xFFFF;
#else
		return input.x != 0 && input.y != 0;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if all [xyz] components are true, otherwise false: all(input.xyz != 0)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL mask_all_true3(mask4q_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return _mm_movemask_epi8(input.xy) == 0xFFFF && (_mm_movemask_epi8(input.zw) & 0x00FF) == 0x00FF;
#else
		return input.x != 0 && input.y != 0 && input.z != 0;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if any 4 components are true, otherwise false: any(input.xyzw != 0)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL mask_any_true(mask4q_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return (_mm_movemask_epi8(input.xy) | _mm_movemask_epi8(input.zw)) != 0;
#else
		return input.x != 0 || input.y != 0 || input.z != 0 || input.w != 0;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if any [xy] components are true, otherwise false: any(input.xy != 0)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL mask_any_true2(mask4q_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return _mm_movemask_epi8(input.xy) != 0;
#else
		return input.x != 0 || input.y != 0;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if any [xyz] components are true, otherwise false: any(input.xyz != 0)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL mask_any_true3(mask4q_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return _mm_movemask_epi8(input.xy) != 0 || (_mm_movemask_epi8(input.zw) & 0x00FF) != 0;
#else
		return input.x != 0 || input.y != 0 || input.z != 0;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if all 4 components are equal, otherwise false: all(lhs.xyzw == rhs.xyzw)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL mask_all_equal(mask4q_arg0 lhs, mask4q_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		// WARNING: SSE2 doesn't have a 64 bit int compare, we use 32 bit here and we assume
		// that in a mask all bits are equal
		__m128i xy_eq = _mm_cmpeq_epi32(lhs.xy, rhs.xy);
		__m128i zw_eq = _mm_cmpeq_epi32(lhs.zw, rhs.zw);
		return (_mm_movemask_epi8(xy_eq) & _mm_movemask_epi8(zw_eq)) == 0xFFFF;
#elif defined(RTM_SSE4_INTRINSICS) && 0
		// TODO
#else
		return lhs.x == rhs.x && lhs.y == rhs.y && lhs.z == rhs.z && lhs.w == rhs.w;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if all [xy] components are equal, otherwise false: all(lhs.xy == rhs.xy)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL mask_all_equal2(mask4q_arg0 lhs, mask4q_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		// WARNING: SSE2 doesn't have a 64 bit int compare, we use 32 bit here and we assume
		// that in a mask all bits are equal
		return _mm_movemask_epi8(_mm_cmpeq_epi32(lhs.xy, rhs.xy)) == 0xFFFF;
#elif defined(RTM_SSE4_INTRINSICS) && 0
		// TODO
#else
		return lhs.x == rhs.x && lhs.y == rhs.y;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if all [xyz] components are equal, otherwise false: all(lhs.xyz == rhs.xyz)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL mask_all_equal3(mask4q_arg0 lhs, mask4q_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		// WARNING: SSE2 doesn't have a 64 bit int compare, we use 32 bit here and we assume
		// that in a mask all bits are equal
		__m128i xy_eq = _mm_cmpeq_epi32(lhs.xy, rhs.xy);
		__m128i zw_eq = _mm_cmpeq_epi32(lhs.zw, rhs.zw);
		return _mm_movemask_epi8(xy_eq) == 0xFFFF && (_mm_movemask_epi8(zw_eq) & 0x00FF) == 0x00FF;
#elif defined(RTM_SSE4_INTRINSICS) && 0
		// TODO
#else
		return lhs.x == rhs.x && lhs.y == rhs.y && lhs.z == rhs.z;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if any 4 components are equal, otherwise false: any(lhs.xyzw == rhs.xyzw)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL mask_any_equal(mask4q_arg0 lhs, mask4q_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		// WARNING: SSE2 doesn't have a 64 bit int compare, we use 32 bit here and we assume
		// that in a mask all bits are equal
		__m128i xy_eq = _mm_cmpeq_epi32(lhs.xy, rhs.xy);
		__m128i zw_eq = _mm_cmpeq_epi32(lhs.zw, rhs.zw);
		return (_mm_movemask_epi8(xy_eq) | _mm_movemask_epi8(zw_eq)) != 0;
#elif defined(RTM_SSE4_INTRINSICS) && 0
		// TODO
#else
		return lhs.x == rhs.x || lhs.y == rhs.y || lhs.z == rhs.z || lhs.w == rhs.w;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if any [xy] components are equal, otherwise false: any(lhs.xy == rhs.xy)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL mask_any_equal2(mask4q_arg0 lhs, mask4q_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		// WARNING: SSE2 doesn't have a 64 bit int compare, we use 32 bit here and we assume
		// that in a mask all bits are equal
		return _mm_movemask_epi8(_mm_cmpeq_epi32(lhs.xy, rhs.xy)) != 0;
#elif defined(RTM_SSE4_INTRINSICS) && 0
		// TODO
#else
		return lhs.x == rhs.x || lhs.y == rhs.y;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if any [xyz] components are equal, otherwise false: any(lhs.xyz == rhs.xyz)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL mask_any_equal3(mask4q_arg0 lhs, mask4q_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		// WARNING: SSE2 doesn't have a 64 bit int compare, we use 32 bit here and we assume
		// that in a mask all bits are equal
		__m128i xy_eq = _mm_cmpeq_epi32(lhs.xy, rhs.xy);
		__m128i zw_eq = _mm_cmpeq_epi32(lhs.zw, rhs.zw);
		return _mm_movemask_epi8(xy_eq) != 0 || (_mm_movemask_epi8(zw_eq) & 0x00FF) != 0;
#elif defined(RTM_SSE4_INTRINSICS) && 0
		// TODO
#else
		return lhs.x == rhs.x || lhs.y == rhs.y || lhs.z == rhs.z;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Per component logical AND between the inputs: lhs & rhs
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE mask4q RTM_SIMD_CALL mask_and(mask4q_arg0 lhs, mask4q_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		__m128i xy = _mm_and_si128(lhs.xy, rhs.xy);
		__m128i zw = _mm_and_si128(lhs.zw, rhs.zw);
		return mask4q{ xy, zw };
#else
		const uint64_t* lhs_ = rtm_impl::bit_cast<const uint64_t*>(&lhs);
		const uint64_t* rhs_ = rtm_impl::bit_cast<const uint64_t*>(&rhs);

		union
		{
			mask4q vector;
			uint64_t scalar[4];
		} result;

		result.scalar[0] = lhs_[0] & rhs_[0];
		result.scalar[1] = lhs_[1] & rhs_[1];
		result.scalar[2] = lhs_[2] & rhs_[2];
		result.scalar[3] = lhs_[3] & rhs_[3];

		return result.vector;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Per component logical OR between the inputs: lhs | rhs
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE mask4q RTM_SIMD_CALL mask_or(mask4q_arg0 lhs, mask4q_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		__m128i xy = _mm_or_si128(lhs.xy, rhs.xy);
		__m128i zw = _mm_or_si128(lhs.zw, rhs.zw);
		return mask4q{ xy, zw };
#else
		const uint64_t* lhs_ = rtm_impl::bit_cast<const uint64_t*>(&lhs);
		const uint64_t* rhs_ = rtm_impl::bit_cast<const uint64_t*>(&rhs);

		union
		{
			mask4q vector;
			uint64_t scalar[4];
		} result;

		result.scalar[0] = lhs_[0] | rhs_[0];
		result.scalar[1] = lhs_[1] | rhs_[1];
		result.scalar[2] = lhs_[2] | rhs_[2];
		result.scalar[3] = lhs_[3] | rhs_[3];

		return result.vector;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Per component logical XOR between the inputs: lhs ^ rhs
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE mask4q RTM_SIMD_CALL mask_xor(mask4q_arg0 lhs, mask4q_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		__m128i xy = _mm_xor_si128(lhs.xy, rhs.xy);
		__m128i zw = _mm_xor_si128(lhs.zw, rhs.zw);
		return mask4q{ xy, zw };
#else
		const uint64_t* lhs_ = rtm_impl::bit_cast<const uint64_t*>(&lhs);
		const uint64_t* rhs_ = rtm_impl::bit_cast<const uint64_t*>(&rhs);

		union
		{
			mask4q vector;
			uint64_t scalar[4];
		} result;

		result.scalar[0] = lhs_[0] ^ rhs_[0];
		result.scalar[1] = lhs_[1] ^ rhs_[1];
		result.scalar[2] = lhs_[2] ^ rhs_[2];
		result.scalar[3] = lhs_[3] ^ rhs_[3];

		return result.vector;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Per component logical NOT of the input: ~input
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE mask4q RTM_SIMD_CALL mask_not(mask4q_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		const __m128i true_mask = _mm_set_epi64x(0xFFFFFFFFFFFFFFFFULL, 0xFFFFFFFFFFFFFFFFULL);
		__m128i xy = _mm_andnot_si128(input.xy, true_mask);
		__m128i zw = _mm_andnot_si128(input.zw, true_mask);
		return mask4q{ xy, zw };
#else
		const uint64_t* input_ = rtm_impl::bit_cast<const uint64_t*>(&input);

		union
		{
			mask4q vector;
			uint64_t scalar[4];
		} result;

		result.scalar[0] = ~input_[0];
		result.scalar[1] = ~input_[1];
		result.scalar[2] = ~input_[2];
		result.scalar[3] = ~input_[3];

		return result.vector;
#endif
	}

	RTM_IMPL_VERSION_NAMESPACE_END
}

RTM_IMPL_FILE_PRAGMA_POP

```

`includes/rtm/math.h`:

```h
#pragma once

////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2017 Nicholas Frechette & Animation Compression Library contributors
// Copyright (c) 2018 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include "rtm/config.h"
#include "rtm/impl/detect_arch.h"
#include "rtm/impl/detect_compiler.h"

//////////////////////////////////////////////////////////////////////////
// Detect which intrinsics the current compilation environment supports.
//////////////////////////////////////////////////////////////////////////

#if !defined(RTM_NO_INTRINSICS)
	#if defined(__AVX2__)
		#define RTM_AVX2_INTRINSICS
		#define RTM_FMA_INTRINSICS
		#define RTM_AVX_INTRINSICS
		#define RTM_SSE4_INTRINSICS
		#define RTM_SSE3_INTRINSICS
		#define RTM_SSE2_INTRINSICS
	#elif defined(__AVX__)
		#define RTM_AVX_INTRINSICS
		#define RTM_SSE4_INTRINSICS
		#define RTM_SSE3_INTRINSICS
		#define RTM_SSE2_INTRINSICS
	#elif defined(__SSE4_1__)
		#define RTM_SSE4_INTRINSICS
		#define RTM_SSE3_INTRINSICS
		#define RTM_SSE2_INTRINSICS
	#elif defined(__SSSE3__)
		#define RTM_SSE3_INTRINSICS
		#define RTM_SSE2_INTRINSICS
	#elif defined(__SSE2__) || defined(RTM_ARCH_X86) || defined(RTM_ARCH_X64)
		#define RTM_SSE2_INTRINSICS
	#endif

	#if defined(RTM_ARCH_ARM64)
		#define RTM_NEON_INTRINSICS
		#define RTM_NEON64_INTRINSICS
	#elif defined(RTM_ARCH_ARM)
		#define RTM_NEON_INTRINSICS
	#endif

	// If SSE2 and NEON aren't used, we default to the scalar implementation
	#if !defined(RTM_SSE2_INTRINSICS) && !defined(RTM_NEON_INTRINSICS)
		#define RTM_NO_INTRINSICS
	#endif
#endif

#if defined(RTM_SSE2_INTRINSICS)
	#include <xmmintrin.h>
	#include <emmintrin.h>

	// With MSVC and SSE2, we can use the __vectorcall calling convention to pass vector types and aggregates by value through registers
	// for improved code generation
	#if defined(RTM_COMPILER_MSVC) && !defined(_MANAGED) && !defined(_M_CEE) && (!defined(_M_IX86_FP) || (_M_IX86_FP > 1)) && !defined(RTM_SIMD_CALL)
		#if RTM_COMPILER_MSVC >= RTM_COMPILER_MSVC_2015
			#define RTM_USE_VECTORCALL
		#endif
	// With Clang on windows, we can use __vectorcall as well
	// Otherwise __m128 arguments are passed by reference which is terrible for performance
	#elif defined(_MSC_VER) && defined(__clang__) && !defined(RTM_SIMD_CALL)
		#define RTM_USE_VECTORCALL
	#endif
#endif

#if defined(RTM_SSE3_INTRINSICS)
	#include <pmmintrin.h>
	#include <tmmintrin.h>
#endif

#if defined(RTM_SSE4_INTRINSICS)
	#include <smmintrin.h>
#endif

#if defined(RTM_AVX_INTRINSICS)
	#include <immintrin.h>
#endif

#if defined(RTM_NEON64_INTRINSICS) && defined(RTM_COMPILER_MSVC)
	// MSVC specific header
	#include <arm64_neon.h>
#elif defined(RTM_NEON_INTRINSICS)
	#include <arm_neon.h>
#endif

// Specify the SIMD calling convention if we can
#if !defined(RTM_SIMD_CALL)
	#if defined(RTM_USE_VECTORCALL)
		#define RTM_SIMD_CALL __vectorcall
	#else
		#define RTM_SIMD_CALL
	#endif
#endif

// By default, we include the type definitions, feature detection, and error handling
#include "rtm/impl/detect_features.h"
#include "rtm/impl/error.h"
#include "rtm/types.h"

```

`includes/rtm/matrix3x3d.h`:

```h
#pragma once

////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2018 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include "rtm/math.h"
#include "rtm/vector4d.h"
#include "rtm/version.h"
#include "rtm/impl/compiler_utils.h"
#include "rtm/impl/matrix_common.h"
#include "rtm/impl/matrix_affine_common.h"

RTM_IMPL_FILE_PRAGMA_PUSH

namespace rtm
{
	RTM_IMPL_VERSION_NAMESPACE_BEGIN

	//////////////////////////////////////////////////////////////////////////
	// Returns the axis pointing in the forward direction of the default coordinate system (Z+).
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr vector4d RTM_SIMD_CALL matrix_get_coord_forward(matrix3x3d_arg0 input) RTM_NO_EXCEPT
	{
		return input.z_axis;
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the axis pointing in the up direction of the default coordinate system (Y+).
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr vector4d RTM_SIMD_CALL matrix_get_coord_up(matrix3x3d_arg0 input) RTM_NO_EXCEPT
	{
		return input.y_axis;
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the axis pointing in the cross direction of the default coordinate system (X+).
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr vector4d RTM_SIMD_CALL matrix_get_coord_cross(matrix3x3d_arg0 input) RTM_NO_EXCEPT
	{
		return input.x_axis;
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the desired 3x3 matrix axis.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr vector4d RTM_SIMD_CALL matrix_get_axis(matrix3x3d_arg0 input, axis3 axis) RTM_NO_EXCEPT
	{
		return axis == axis3::x ? input.x_axis : (axis == axis3::y ? input.y_axis : input.z_axis);
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns a new 3x3 matrix where the specified axis has been replaced on the input matrix.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline matrix3x3d RTM_SIMD_CALL matrix_set_axis(matrix3x3d_arg0 input, vector4d_arg0 axis_value, axis3 axis) RTM_NO_EXCEPT
	{
		switch (axis)
		{
			default:
			case axis3::x:	return matrix3x3d{ axis_value, input.y_axis, input.z_axis };
			case axis3::y:	return matrix3x3d{ input.x_axis, axis_value, input.z_axis };
			case axis3::z:	return matrix3x3d{ input.x_axis, input.y_axis, axis_value };
		}
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the desired 3x3 matrix component from the specified axis.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline rtm_impl::vector4d_vector_get_component RTM_SIMD_CALL matrix_get_component(matrix3x3d_arg0 input, axis3 axis, component3 component) RTM_NO_EXCEPT
	{
		switch (axis)
		{
			default:
			case axis3::x:	return vector_get_component3(input.x_axis, component);
			case axis3::y:	return vector_get_component3(input.y_axis, component);
			case axis3::z:	return vector_get_component3(input.z_axis, component);
		}
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the desired 3x3 matrix component from the specified axis.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline scalard RTM_SIMD_CALL matrix_get_component_as_scalar(matrix3x3d_arg0 input, axis3 axis, component3 component) RTM_NO_EXCEPT
	{
		switch (axis)
		{
			default:
			case axis3::x:	return vector_get_component3_as_scalar(input.x_axis, component);
			case axis3::y:	return vector_get_component3_as_scalar(input.y_axis, component);
			case axis3::z:	return vector_get_component3_as_scalar(input.z_axis, component);
		}
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns a new 3x3 matrix where the specified axis/component has been replaced on the input matrix.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline matrix3x3d RTM_SIMD_CALL matrix_set_component(matrix3x3d_arg0 input, double component_value, axis3 axis, component3 component) RTM_NO_EXCEPT
	{
		switch (axis)
		{
			default:
			case axis3::x:	return matrix3x3d{ vector_set_component3(input.x_axis, component_value, component), input.y_axis, input.z_axis };
			case axis3::y:	return matrix3x3d{ input.x_axis, vector_set_component3(input.y_axis, component_value, component), input.z_axis };
			case axis3::z:	return matrix3x3d{ input.x_axis, input.y_axis, vector_set_component3(input.z_axis, component_value, component) };
		}
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Returns a new 3x3 matrix where the specified axis/component has been replaced on the input matrix.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline matrix3x3d RTM_SIMD_CALL matrix_set_component(matrix3x3d_arg0 input, scalard_arg7 component_value, axis3 axis, component3 component) RTM_NO_EXCEPT
	{
		switch (axis)
		{
			default:
			case axis3::x:	return matrix3x3d{ vector_set_component3(input.x_axis, component_value, component), input.y_axis, input.z_axis };
			case axis3::y:	return matrix3x3d{ input.x_axis, vector_set_component3(input.y_axis, component_value, component), input.z_axis };
			case axis3::z:	return matrix3x3d{ input.x_axis, input.y_axis, vector_set_component3(input.z_axis, component_value, component) };
		}
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Converts a 3x3 matrix into a rotation quaternion.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline quatd RTM_SIMD_CALL quat_from_matrix(matrix3x3d_arg0 input) RTM_NO_EXCEPT
	{
		return rtm_impl::quat_from_matrix(input.x_axis, input.y_axis, input.z_axis);
	}

	//////////////////////////////////////////////////////////////////////////
	// Multiplies two 3x3 matrices.
	// Multiplication order is as follow: local_to_world = matrix_mul(local_to_object, object_to_world)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline matrix3x3d RTM_SIMD_CALL matrix_mul(matrix3x3d_arg0 lhs, matrix3x3d_arg1 rhs) RTM_NO_EXCEPT
	{
		vector4d tmp = vector_mul(vector_dup_x(lhs.x_axis), rhs.x_axis);
		tmp = vector_mul_add(vector_dup_y(lhs.x_axis), rhs.y_axis, tmp);
		tmp = vector_mul_add(vector_dup_z(lhs.x_axis), rhs.z_axis, tmp);
		vector4d x_axis = tmp;

		tmp = vector_mul(vector_dup_x(lhs.y_axis), rhs.x_axis);
		tmp = vector_mul_add(vector_dup_y(lhs.y_axis), rhs.y_axis, tmp);
		tmp = vector_mul_add(vector_dup_z(lhs.y_axis), rhs.z_axis, tmp);
		vector4d y_axis = tmp;

		tmp = vector_mul(vector_dup_x(lhs.z_axis), rhs.x_axis);
		tmp = vector_mul_add(vector_dup_y(lhs.z_axis), rhs.y_axis, tmp);
		tmp = vector_mul_add(vector_dup_z(lhs.z_axis), rhs.z_axis, tmp);
		vector4d z_axis = tmp;

		return matrix3x3d{ x_axis, y_axis, z_axis };
	}

	//////////////////////////////////////////////////////////////////////////
	// Multiplies a 3x3 matrix and a 3D vector.
	// Multiplication order is as follow: world_position = matrix_mul(local_vector, local_to_world)
	// Note: The proper way to transform a normal by a matrix with non-uniform scale
	// is to multiply the normal with the cofactor matrix.
	// See: https://github.com/graphitemaster/normals_revisited
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4d RTM_SIMD_CALL matrix_mul_vector3(vector4d_arg0 vec3, matrix3x3d_argn mtx) RTM_NO_EXCEPT
	{
		vector4d tmp;

		tmp = vector_mul(vector_dup_x(vec3), mtx.x_axis);
		tmp = vector_mul_add(vector_dup_y(vec3), mtx.y_axis, tmp);
		tmp = vector_mul_add(vector_dup_z(vec3), mtx.z_axis, tmp);

		return tmp;
	}

	//////////////////////////////////////////////////////////////////////////
	// Transposes a 3x3 matrix.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE matrix3x3d RTM_SIMD_CALL matrix_transpose(matrix3x3d_arg0 input) RTM_NO_EXCEPT
	{
		vector4d x_axis;
		vector4d y_axis;
		vector4d z_axis;
		RTM_MATRIXD_TRANSPOSE_3X3(input.x_axis, input.y_axis, input.z_axis, x_axis, y_axis, z_axis);
		return matrix3x3d{ x_axis, y_axis, z_axis };
	}

	//////////////////////////////////////////////////////////////////////////
	// Inverses a 3x3 matrix.
	// If the input matrix is not invertible, the result is undefined.
	// For a safe alternative, supply a fallback value and a threshold.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline matrix3x3d RTM_SIMD_CALL matrix_inverse(matrix3x3d_arg0 input) RTM_NO_EXCEPT
	{
		const vector4d v00_v01_v10_v11 = vector_mix<mix4::x, mix4::y, mix4::a, mix4::b>(input.x_axis, input.y_axis);
		const vector4d v02_v03_v12_v13 = vector_mix<mix4::z, mix4::w, mix4::c, mix4::d>(input.x_axis, input.y_axis);

		const vector4d v00_v10_v20_v22 = vector_mix<mix4::x, mix4::z, mix4::a, mix4::c>(v00_v01_v10_v11, input.z_axis);
		const vector4d v01_v11_v21_v23 = vector_mix<mix4::y, mix4::w, mix4::b, mix4::d>(v00_v01_v10_v11, input.z_axis);
		const vector4d v02_v12_v22_v22 = vector_mix<mix4::x, mix4::z, mix4::c, mix4::c>(v02_v03_v12_v13, input.z_axis);

		const vector4d v11_v21_v01 = vector_mix<mix4::y, mix4::z, mix4::b, mix4::c>(v01_v11_v21_v23, input.x_axis);
		const vector4d v22_v02_v12_v10 = vector_mix<mix4::z, mix4::x, mix4::c, mix4::a>(v02_v12_v22_v22, input.y_axis);
		const vector4d v11v22_v21v02_v01v12 = vector_mul(v11_v21_v01, v22_v02_v12_v10);

		const vector4d v01_v02_v11_v12 = vector_mix<mix4::y, mix4::z, mix4::b, mix4::c>(input.x_axis, input.y_axis);

		const vector4d v12_v01_v11 = vector_mix<mix4::w, mix4::x, mix4::b, mix4::c>(v01_v02_v11_v12, v01_v11_v21_v23);
		const vector4d v21_v22_v02 = vector_mix<mix4::y, mix4::z, mix4::b, mix4::c>(input.z_axis, v22_v02_v12_v10);

		vector4d x_axis = vector_neg_mul_sub(v12_v01_v11, v21_v22_v02, v11v22_v21v02_v01v12);

		const vector4d v20_v00_v10_v22 = vector_mix<mix4::z, mix4::x, mix4::d, mix4::a>(v00_v10_v20_v22, v22_v02_v12_v10);
		const vector4d v12_v22_v02 = vector_mix<mix4::y, mix4::z, mix4::c, mix4::c>(v02_v12_v22_v22, v21_v22_v02);
		const vector4d v20v12_v00v22_v10v02 = vector_mul(v20_v00_v10_v22, v12_v22_v02);

		const vector4d v10_v02_v00 = vector_mix<mix4::w, mix4::y, mix4::b, mix4::c>(v22_v02_v12_v10, v20_v00_v10_v22);
		const vector4d v22_v20_v12 = vector_mix<mix4::w, mix4::x, mix4::a, mix4::c>(v20_v00_v10_v22, v12_v22_v02);

		vector4d y_axis = vector_neg_mul_sub(v10_v02_v00, v22_v20_v12, v20v12_v00v22_v10v02);

		const vector4d v10_v20_v00 = vector_mix<mix4::z, mix4::x, mix4::c, mix4::c>(v20_v00_v10_v22, v10_v02_v00);
		const vector4d v21_v01_v11 = vector_mix<mix4::y, mix4::z, mix4::c, mix4::c>(v11_v21_v01, v12_v01_v11);
		const vector4d v10v21_v20v01_v00v11 = vector_mul(v10_v20_v00, v21_v01_v11);

		const vector4d v20_v00_v01 = vector_mix<mix4::y, mix4::z, mix4::c, mix4::c>(v10_v20_v00, v11_v21_v01);
		const vector4d v11_v21_v10 = vector_mix<mix4::x, mix4::y, mix4::a, mix4::c>(v11_v21_v01, v10_v20_v00);

		vector4d z_axis = vector_neg_mul_sub(v20_v00_v01, v11_v21_v10, v10v21_v20v01_v00v11);

		const vector4d o00_o00_o10_o10 = vector_mix<mix4::x, mix4::x, mix4::a, mix4::a>(x_axis, y_axis);
		const vector4d o00_o10_o20 = vector_mix<mix4::x, mix4::z, mix4::a, mix4::a>(o00_o00_o10_o10, z_axis);

		const double det = vector_dot3(o00_o10_o20, input.x_axis);
		const vector4d inv_det = vector_set(scalar_reciprocal(det));

		x_axis = vector_mul(x_axis, inv_det);
		y_axis = vector_mul(y_axis, inv_det);
		z_axis = vector_mul(z_axis, inv_det);

		return matrix3x3d{ x_axis, y_axis, z_axis };
	}

	//////////////////////////////////////////////////////////////////////////
	// Inverses a 3x3 matrix.
	// If the input matrix has a determinant whose absolute value is below the supplied threshold, the
	// fall back value is returned instead.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline matrix3x3d RTM_SIMD_CALL matrix_inverse(matrix3x3d_arg0 input, matrix3x3d_arg1 fallback, double threshold = 1.0E-8) RTM_NO_EXCEPT
	{
		const vector4d v00_v01_v10_v11 = vector_mix<mix4::x, mix4::y, mix4::a, mix4::b>(input.x_axis, input.y_axis);
		const vector4d v02_v03_v12_v13 = vector_mix<mix4::z, mix4::w, mix4::c, mix4::d>(input.x_axis, input.y_axis);

		const vector4d v00_v10_v20_v22 = vector_mix<mix4::x, mix4::z, mix4::a, mix4::c>(v00_v01_v10_v11, input.z_axis);
		const vector4d v01_v11_v21_v23 = vector_mix<mix4::y, mix4::w, mix4::b, mix4::d>(v00_v01_v10_v11, input.z_axis);
		const vector4d v02_v12_v22_v22 = vector_mix<mix4::x, mix4::z, mix4::c, mix4::c>(v02_v03_v12_v13, input.z_axis);

		const vector4d v11_v21_v01 = vector_mix<mix4::y, mix4::z, mix4::b, mix4::c>(v01_v11_v21_v23, input.x_axis);
		const vector4d v22_v02_v12_v10 = vector_mix<mix4::z, mix4::x, mix4::c, mix4::a>(v02_v12_v22_v22, input.y_axis);
		const vector4d v11v22_v21v02_v01v12 = vector_mul(v11_v21_v01, v22_v02_v12_v10);

		const vector4d v01_v02_v11_v12 = vector_mix<mix4::y, mix4::z, mix4::b, mix4::c>(input.x_axis, input.y_axis);

		const vector4d v12_v01_v11 = vector_mix<mix4::w, mix4::x, mix4::b, mix4::c>(v01_v02_v11_v12, v01_v11_v21_v23);
		const vector4d v21_v22_v02 = vector_mix<mix4::y, mix4::z, mix4::b, mix4::c>(input.z_axis, v22_v02_v12_v10);

		vector4d x_axis = vector_neg_mul_sub(v12_v01_v11, v21_v22_v02, v11v22_v21v02_v01v12);

		const vector4d v20_v00_v10_v22 = vector_mix<mix4::z, mix4::x, mix4::d, mix4::a>(v00_v10_v20_v22, v22_v02_v12_v10);
		const vector4d v12_v22_v02 = vector_mix<mix4::y, mix4::z, mix4::c, mix4::c>(v02_v12_v22_v22, v21_v22_v02);
		const vector4d v20v12_v00v22_v10v02 = vector_mul(v20_v00_v10_v22, v12_v22_v02);

		const vector4d v10_v02_v00 = vector_mix<mix4::w, mix4::y, mix4::b, mix4::c>(v22_v02_v12_v10, v20_v00_v10_v22);
		const vector4d v22_v20_v12 = vector_mix<mix4::w, mix4::x, mix4::a, mix4::c>(v20_v00_v10_v22, v12_v22_v02);

		vector4d y_axis = vector_neg_mul_sub(v10_v02_v00, v22_v20_v12, v20v12_v00v22_v10v02);

		const vector4d v10_v20_v00 = vector_mix<mix4::z, mix4::x, mix4::c, mix4::c>(v20_v00_v10_v22, v10_v02_v00);
		const vector4d v21_v01_v11 = vector_mix<mix4::y, mix4::z, mix4::c, mix4::c>(v11_v21_v01, v12_v01_v11);
		const vector4d v10v21_v20v01_v00v11 = vector_mul(v10_v20_v00, v21_v01_v11);

		const vector4d v20_v00_v01 = vector_mix<mix4::y, mix4::z, mix4::c, mix4::c>(v10_v20_v00, v11_v21_v01);
		const vector4d v11_v21_v10 = vector_mix<mix4::x, mix4::y, mix4::a, mix4::c>(v11_v21_v01, v10_v20_v00);

		vector4d z_axis = vector_neg_mul_sub(v20_v00_v01, v11_v21_v10, v10v21_v20v01_v00v11);

		const vector4d o00_o00_o10_o10 = vector_mix<mix4::x, mix4::x, mix4::a, mix4::a>(x_axis, y_axis);
		const vector4d o00_o10_o20 = vector_mix<mix4::x, mix4::z, mix4::a, mix4::a>(o00_o00_o10_o10, z_axis);

		const double det = vector_dot3(o00_o10_o20, input.x_axis);
		if (scalar_abs(det) < threshold)
			return fallback;

		const vector4d inv_det = vector_set(scalar_reciprocal(det));

		x_axis = vector_mul(x_axis, inv_det);
		y_axis = vector_mul(y_axis, inv_det);
		z_axis = vector_mul(z_axis, inv_det);

		return matrix3x3d{ x_axis, y_axis, z_axis };
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the determinant of the input 3x3 matrix.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline scalard RTM_SIMD_CALL matrix_determinant(matrix3x3d_arg0 input) RTM_NO_EXCEPT
	{
		const vector4d v00_v01_v10_v11 = vector_mix<mix4::x, mix4::y, mix4::a, mix4::b>(input.x_axis, input.y_axis);
		const vector4d v02_v03_v12_v13 = vector_mix<mix4::z, mix4::w, mix4::c, mix4::d>(input.x_axis, input.y_axis);

		const vector4d v00_v10_v20_v22 = vector_mix<mix4::x, mix4::z, mix4::a, mix4::c>(v00_v01_v10_v11, input.z_axis);
		const vector4d v01_v11_v21_v23 = vector_mix<mix4::y, mix4::w, mix4::b, mix4::d>(v00_v01_v10_v11, input.z_axis);
		const vector4d v02_v12_v22_v22 = vector_mix<mix4::x, mix4::z, mix4::c, mix4::c>(v02_v03_v12_v13, input.z_axis);

		const vector4d v11_v21_v01 = vector_mix<mix4::y, mix4::z, mix4::b, mix4::c>(v01_v11_v21_v23, input.x_axis);
		const vector4d v22_v02_v12_v10 = vector_mix<mix4::z, mix4::x, mix4::c, mix4::a>(v02_v12_v22_v22, input.y_axis);
		const vector4d v11v22_v21v02_v01v12 = vector_mul(v11_v21_v01, v22_v02_v12_v10);

		const vector4d v01_v02_v11_v12 = vector_mix<mix4::y, mix4::z, mix4::b, mix4::c>(input.x_axis, input.y_axis);

		const vector4d v12_v01_v11 = vector_mix<mix4::w, mix4::x, mix4::b, mix4::c>(v01_v02_v11_v12, v01_v11_v21_v23);
		const vector4d v21_v22_v02 = vector_mix<mix4::y, mix4::z, mix4::b, mix4::c>(input.z_axis, v22_v02_v12_v10);

		vector4d x_axis = vector_neg_mul_sub(v12_v01_v11, v21_v22_v02, v11v22_v21v02_v01v12);

		const vector4d v20_v00_v10_v22 = vector_mix<mix4::z, mix4::x, mix4::d, mix4::a>(v00_v10_v20_v22, v22_v02_v12_v10);
		const vector4d v12_v22_v02 = vector_mix<mix4::y, mix4::z, mix4::c, mix4::c>(v02_v12_v22_v22, v21_v22_v02);
		const vector4d v20v12_v00v22_v10v02 = vector_mul(v20_v00_v10_v22, v12_v22_v02);

		const vector4d v10_v02_v00 = vector_mix<mix4::w, mix4::y, mix4::b, mix4::c>(v22_v02_v12_v10, v20_v00_v10_v22);
		const vector4d v22_v20_v12 = vector_mix<mix4::w, mix4::x, mix4::a, mix4::c>(v20_v00_v10_v22, v12_v22_v02);

		vector4d y_axis = vector_neg_mul_sub(v10_v02_v00, v22_v20_v12, v20v12_v00v22_v10v02);

		const vector4d v10_v20_v00 = vector_mix<mix4::z, mix4::x, mix4::c, mix4::c>(v20_v00_v10_v22, v10_v02_v00);
		const vector4d v21_v01_v11 = vector_mix<mix4::y, mix4::z, mix4::c, mix4::c>(v11_v21_v01, v12_v01_v11);
		const vector4d v10v21_v20v01_v00v11 = vector_mul(v10_v20_v00, v21_v01_v11);

		const vector4d v20_v00_v01 = vector_mix<mix4::y, mix4::z, mix4::c, mix4::c>(v10_v20_v00, v11_v21_v01);
		const vector4d v11_v21_v10 = vector_mix<mix4::x, mix4::y, mix4::a, mix4::c>(v11_v21_v01, v10_v20_v00);

		vector4d z_axis = vector_neg_mul_sub(v20_v00_v01, v11_v21_v10, v10v21_v20v01_v00v11);

		const vector4d o00_o00_o10_o10 = vector_mix<mix4::x, mix4::x, mix4::a, mix4::a>(x_axis, y_axis);
		const vector4d o00_o10_o20 = vector_mix<mix4::x, mix4::z, mix4::a, mix4::a>(o00_o00_o10_o10, z_axis);

		return vector_dot3_as_scalar(o00_o10_o20, input.x_axis);
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the minor of the input 3x3 matrix.
	// See: https://en.wikipedia.org/wiki/Minor_(linear_algebra)
	// The minor is the determinant of the sub-matrix input when the specified
	// row and column are removed.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline scalard RTM_SIMD_CALL matrix_minor(matrix3x3d_arg0 input, axis3 row, axis3 column) RTM_NO_EXCEPT
	{
		// The minor boils down to calculating the determinant of a 2x2 matrix.
		// det([a, b], [c, d]) = (a * d) - (b * c)

		// Find which two rows we need.
		vector4d row0;
		vector4d row1;
		if (row == axis3::x)
		{
			row0 = input.y_axis;
			row1 = input.z_axis;
		}
		else if (row == axis3::y)
		{
			row0 = input.x_axis;
			row1 = input.z_axis;
		}
		else
		{
			row0 = input.x_axis;
			row1 = input.y_axis;
		}

		// Because our input is a 3x3 matrix, there are only a few possibilities for the 2x2 part:
		// row0 = [x0, y0, z0]
		// row1 = [x1, y1, z1]
		// det([x0, y0], [x1, y1]) = (x0 * y1) - (y0 * x1) (z removed)
		// det([x0, z0], [x1, z1]) = (x0 * z1) - (z0 * x1) (y removed)
		// det([y0, z0], [y1, z1]) = (y0 * z1) - (z0 * y1) (x removed)
		// det([column0, column1], [column2, column3]) = (column0 * column3) - (column1 * column2)

		// For performance reasons, we can compute all three determinants at the same time.
		const vector4d column0 = vector_mix<mix4::x, mix4::x, mix4::y, mix4::y>(row0, row0);
		const vector4d column1 = vector_mix<mix4::y, mix4::z, mix4::z, mix4::z>(row0, row0);
		const vector4d column2 = vector_mix<mix4::x, mix4::x, mix4::y, mix4::y>(row1, row1);
		const vector4d column3 = vector_mix<mix4::y, mix4::z, mix4::z, mix4::z>(row1, row1);

		const vector4d determinants = vector_neg_mul_sub(column1, column2, vector_mul(column0, column3));

		// Extract the one we need
		if (column == axis3::x)
			return vector_get_z_as_scalar(determinants);
		else if (column == axis3::y)
			return vector_get_y_as_scalar(determinants);
		else
			return vector_get_x_as_scalar(determinants);
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the cofactor matrix of the input 3x3 matrix.
	// See: https://en.wikipedia.org/wiki/Minor_(linear_algebra)#Cofactor_expansion_of_the_determinant
	// Note: The proper way to transform a normal by a matrix with non-uniform scale
	// is to multiply the normal with the cofactor matrix.
	// See: https://github.com/graphitemaster/normals_revisited
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline matrix3x3d RTM_SIMD_CALL matrix_cofactor(matrix3x3d_arg0 input) RTM_NO_EXCEPT
	{
		const vector4d x_axis = vector_cross3(input.y_axis, input.z_axis);
		const vector4d y_axis = vector_cross3(input.z_axis, input.x_axis);
		const vector4d z_axis = vector_cross3(input.x_axis, input.y_axis);
		return matrix3x3d{ x_axis, y_axis, z_axis };
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the adjugate of the input matrix.
	// See: https://en.wikipedia.org/wiki/Adjugate_matrix
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline matrix3x3d RTM_SIMD_CALL matrix_adjugate(matrix3x3d_arg0 input) RTM_NO_EXCEPT
	{
		return matrix_transpose(matrix_cofactor(input));
	}

	//////////////////////////////////////////////////////////////////////////
	// Removes the 3D scale from a 3x3 matrix.
	// Note that if the scaling is 0.0 for a particular axis, the original rotation axis cannot
	// be recovered trivially and no attempt is done to do so. In theory, we could use the other axes
	// to try and recover it.
	// TODO: Implement rotation recovering, perhaps in a separate function and rename this
	// one to matrix_remove_non_zero_scale(..)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline matrix3x3d RTM_SIMD_CALL matrix_remove_scale(matrix3x3d_arg0 input) RTM_NO_EXCEPT
	{
		matrix3x3d result;
		result.x_axis = vector_normalize3(input.x_axis, input.x_axis);
		result.y_axis = vector_normalize3(input.y_axis, input.y_axis);
		result.z_axis = vector_normalize3(input.z_axis, input.z_axis);
		return result;
	}

	RTM_IMPL_VERSION_NAMESPACE_END
}

RTM_IMPL_FILE_PRAGMA_POP

```

`includes/rtm/matrix3x3f.h`:

```h
#pragma once

////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2018 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include "rtm/macros.h"
#include "rtm/math.h"
#include "rtm/vector4f.h"
#include "rtm/version.h"
#include "rtm/impl/compiler_utils.h"
#include "rtm/impl/matrix_common.h"
#include "rtm/impl/matrix_affine_common.h"

RTM_IMPL_FILE_PRAGMA_PUSH

namespace rtm
{
	RTM_IMPL_VERSION_NAMESPACE_BEGIN

	//////////////////////////////////////////////////////////////////////////
	// Returns the axis pointing in the forward direction of the default coordinate system (Z+).
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr vector4f RTM_SIMD_CALL matrix_get_coord_forward(matrix3x3f_arg0 input) RTM_NO_EXCEPT
	{
		return input.z_axis;
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the axis pointing in the up direction of the default coordinate system (Y+).
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr vector4f RTM_SIMD_CALL matrix_get_coord_up(matrix3x3f_arg0 input) RTM_NO_EXCEPT
	{
		return input.y_axis;
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the axis pointing in the cross direction of the default coordinate system (X+).
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr vector4f RTM_SIMD_CALL matrix_get_coord_cross(matrix3x3f_arg0 input) RTM_NO_EXCEPT
	{
		return input.x_axis;
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the desired 3x3 matrix axis.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr vector4f RTM_SIMD_CALL matrix_get_axis(matrix3x3f_arg0 input, axis3 axis) RTM_NO_EXCEPT
	{
		return axis == axis3::x ? input.x_axis : (axis == axis3::y ? input.y_axis : input.z_axis);
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns a new 3x3 matrix where the specified axis has been replaced on the input matrix.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline matrix3x3f RTM_SIMD_CALL matrix_set_axis(matrix3x3f_arg0 input, vector4f_arg4 axis_value, axis3 axis) RTM_NO_EXCEPT
	{
		switch (axis)
		{
			default:
			case axis3::x:	return matrix3x3f{ axis_value, input.y_axis, input.z_axis };
			case axis3::y:	return matrix3x3f{ input.x_axis, axis_value, input.z_axis };
			case axis3::z:	return matrix3x3f{ input.x_axis, input.y_axis, axis_value };
		}
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the desired 3x3 matrix component from the specified axis.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline rtm_impl::vector4f_vector_get_component RTM_SIMD_CALL matrix_get_component(matrix3x3f_arg0 input, axis3 axis, component3 component) RTM_NO_EXCEPT
	{
		switch (axis)
		{
			default:
			case axis3::x:	return vector_get_component3(input.x_axis, component);
			case axis3::y:	return vector_get_component3(input.y_axis, component);
			case axis3::z:	return vector_get_component3(input.z_axis, component);
		}
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the desired 3x3 matrix component from the specified axis.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline scalarf RTM_SIMD_CALL matrix_get_component_as_scalar(matrix3x3f_arg0 input, axis3 axis, component3 component) RTM_NO_EXCEPT
	{
		switch (axis)
		{
			default:
			case axis3::x:	return vector_get_component3_as_scalar(input.x_axis, component);
			case axis3::y:	return vector_get_component3_as_scalar(input.y_axis, component);
			case axis3::z:	return vector_get_component3_as_scalar(input.z_axis, component);
		}
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns a new 3x3 matrix where the specified axis/component has been replaced on the input matrix.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline matrix3x3f RTM_SIMD_CALL matrix_set_component(matrix3x3f_arg0 input, float component_value, axis3 axis, component3 component) RTM_NO_EXCEPT
	{
		switch (axis)
		{
			default:
			case axis3::x:	return matrix3x3f{ vector_set_component3(input.x_axis, component_value, component), input.y_axis, input.z_axis };
			case axis3::y:	return matrix3x3f{ input.x_axis, vector_set_component3(input.y_axis, component_value, component), input.z_axis };
			case axis3::z:	return matrix3x3f{ input.x_axis, input.y_axis, vector_set_component3(input.z_axis, component_value, component) };
		}
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Returns a new 3x3 matrix where the specified axis/component has been replaced on the input matrix.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline matrix3x3f RTM_SIMD_CALL matrix_set_component(matrix3x3f_arg0 input, scalarf_arg4 component_value, axis3 axis, component3 component) RTM_NO_EXCEPT
	{
		switch (axis)
		{
			default:
			case axis3::x:	return matrix3x3f{ vector_set_component3(input.x_axis, component_value, component), input.y_axis, input.z_axis };
			case axis3::y:	return matrix3x3f{ input.x_axis, vector_set_component3(input.y_axis, component_value, component), input.z_axis };
			case axis3::z:	return matrix3x3f{ input.x_axis, input.y_axis, vector_set_component3(input.z_axis, component_value, component) };
		}
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Converts a 3x3 matrix into a rotation quaternion.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline quatf RTM_SIMD_CALL quat_from_matrix(matrix3x3f_arg0 input) RTM_NO_EXCEPT
	{
		return rtm_impl::quat_from_matrix(input.x_axis, input.y_axis, input.z_axis);
	}

	//////////////////////////////////////////////////////////////////////////
	// Multiplies two 3x3 matrices.
	// Multiplication order is as follow: local_to_world = matrix_mul(local_to_object, object_to_world)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline matrix3x3f RTM_SIMD_CALL matrix_mul(matrix3x3f_arg0 lhs, matrix3x3f_arg1 rhs) RTM_NO_EXCEPT
	{
		vector4f tmp = vector_mul(vector_dup_x(lhs.x_axis), rhs.x_axis);
		tmp = vector_mul_add(vector_dup_y(lhs.x_axis), rhs.y_axis, tmp);
		tmp = vector_mul_add(vector_dup_z(lhs.x_axis), rhs.z_axis, tmp);
		vector4f x_axis = tmp;

		tmp = vector_mul(vector_dup_x(lhs.y_axis), rhs.x_axis);
		tmp = vector_mul_add(vector_dup_y(lhs.y_axis), rhs.y_axis, tmp);
		tmp = vector_mul_add(vector_dup_z(lhs.y_axis), rhs.z_axis, tmp);
		vector4f y_axis = tmp;

		tmp = vector_mul(vector_dup_x(lhs.z_axis), rhs.x_axis);
		tmp = vector_mul_add(vector_dup_y(lhs.z_axis), rhs.y_axis, tmp);
		tmp = vector_mul_add(vector_dup_z(lhs.z_axis), rhs.z_axis, tmp);
		vector4f z_axis = tmp;

		return matrix3x3f{ x_axis, y_axis, z_axis };
	}

	//////////////////////////////////////////////////////////////////////////
	// Multiplies a 3x3 matrix and a 3D vector.
	// Multiplication order is as follow: world_position = matrix_mul(local_vector, local_to_world)
	// Note: The proper way to transform a normal by a matrix with non-uniform scale
	// is to multiply the normal with the cofactor matrix.
	// See: https://github.com/graphitemaster/normals_revisited
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4f RTM_SIMD_CALL matrix_mul_vector3(vector4f_arg0 vec3, matrix3x3f_arg0 mtx) RTM_NO_EXCEPT
	{
		vector4f tmp;

		tmp = vector_mul(vector_dup_x(vec3), mtx.x_axis);
		tmp = vector_mul_add(vector_dup_y(vec3), mtx.y_axis, tmp);
		tmp = vector_mul_add(vector_dup_z(vec3), mtx.z_axis, tmp);

		return tmp;
	}

	//////////////////////////////////////////////////////////////////////////
	// Transposes a 3x3 matrix.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE matrix3x3f RTM_SIMD_CALL matrix_transpose(matrix3x3f_arg0 input) RTM_NO_EXCEPT
	{
		vector4f x_axis;
		vector4f y_axis;
		vector4f z_axis;
		RTM_MATRIXF_TRANSPOSE_3X3(input.x_axis, input.y_axis, input.z_axis, x_axis, y_axis, z_axis);
		return matrix3x3f{ x_axis, y_axis, z_axis };
	}

	//////////////////////////////////////////////////////////////////////////
	// Inverses a 3x3 matrix.
	// If the input matrix is not invertible, the result is undefined.
	// For a safe alternative, supply a fallback value and a threshold.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline matrix3x3f RTM_SIMD_CALL matrix_inverse(matrix3x3f_arg0 input) RTM_NO_EXCEPT
	{
		const vector4f v00_v01_v10_v11 = vector_mix<mix4::x, mix4::y, mix4::a, mix4::b>(input.x_axis, input.y_axis);
		const vector4f v02_v03_v12_v13 = vector_mix<mix4::z, mix4::w, mix4::c, mix4::d>(input.x_axis, input.y_axis);

		const vector4f v00_v10_v20_v22 = vector_mix<mix4::x, mix4::z, mix4::a, mix4::c>(v00_v01_v10_v11, input.z_axis);
		const vector4f v01_v11_v21_v23 = vector_mix<mix4::y, mix4::w, mix4::b, mix4::d>(v00_v01_v10_v11, input.z_axis);
		const vector4f v02_v12_v22_v22 = vector_mix<mix4::x, mix4::z, mix4::c, mix4::c>(v02_v03_v12_v13, input.z_axis);

		const vector4f v11_v21_v01 = vector_mix<mix4::y, mix4::z, mix4::b, mix4::c>(v01_v11_v21_v23, input.x_axis);
		const vector4f v22_v02_v12_v10 = vector_mix<mix4::z, mix4::x, mix4::c, mix4::a>(v02_v12_v22_v22, input.y_axis);
		const vector4f v11v22_v21v02_v01v12 = vector_mul(v11_v21_v01, v22_v02_v12_v10);

		const vector4f v01_v02_v11_v12 = vector_mix<mix4::y, mix4::z, mix4::b, mix4::c>(input.x_axis, input.y_axis);

		const vector4f v12_v01_v11 = vector_mix<mix4::w, mix4::x, mix4::b, mix4::c>(v01_v02_v11_v12, v01_v11_v21_v23);
		const vector4f v21_v22_v02 = vector_mix<mix4::y, mix4::z, mix4::b, mix4::c>(input.z_axis, v22_v02_v12_v10);

		vector4f x_axis = vector_neg_mul_sub(v12_v01_v11, v21_v22_v02, v11v22_v21v02_v01v12);

		const vector4f v20_v00_v10_v22 = vector_mix<mix4::z, mix4::x, mix4::d, mix4::a>(v00_v10_v20_v22, v22_v02_v12_v10);
		const vector4f v12_v22_v02 = vector_mix<mix4::y, mix4::z, mix4::c, mix4::c>(v02_v12_v22_v22, v21_v22_v02);
		const vector4f v20v12_v00v22_v10v02 = vector_mul(v20_v00_v10_v22, v12_v22_v02);

		const vector4f v10_v02_v00 = vector_mix<mix4::w, mix4::y, mix4::b, mix4::c>(v22_v02_v12_v10, v20_v00_v10_v22);
		const vector4f v22_v20_v12 = vector_mix<mix4::w, mix4::x, mix4::a, mix4::c>(v20_v00_v10_v22, v12_v22_v02);

		vector4f y_axis = vector_neg_mul_sub(v10_v02_v00, v22_v20_v12, v20v12_v00v22_v10v02);

		const vector4f v10_v20_v00 = vector_mix<mix4::z, mix4::x, mix4::c, mix4::c>(v20_v00_v10_v22, v10_v02_v00);
		const vector4f v21_v01_v11 = vector_mix<mix4::y, mix4::z, mix4::c, mix4::c>(v11_v21_v01, v12_v01_v11);
		const vector4f v10v21_v20v01_v00v11 = vector_mul(v10_v20_v00, v21_v01_v11);

		const vector4f v20_v00_v01 = vector_mix<mix4::y, mix4::z, mix4::c, mix4::c>(v10_v20_v00, v11_v21_v01);
		const vector4f v11_v21_v10 = vector_mix<mix4::x, mix4::y, mix4::a, mix4::c>(v11_v21_v01, v10_v20_v00);

		vector4f z_axis = vector_neg_mul_sub(v20_v00_v01, v11_v21_v10, v10v21_v20v01_v00v11);

		const vector4f o00_o00_o10_o10 = vector_mix<mix4::x, mix4::x, mix4::a, mix4::a>(x_axis, y_axis);
		const vector4f o00_o10_o20 = vector_mix<mix4::x, mix4::z, mix4::a, mix4::a>(o00_o00_o10_o10, z_axis);

		const scalarf det = vector_dot3_as_scalar(o00_o10_o20, input.x_axis);
		const vector4f inv_det = vector_set(scalar_reciprocal(det));

		x_axis = vector_mul(x_axis, inv_det);
		y_axis = vector_mul(y_axis, inv_det);
		z_axis = vector_mul(z_axis, inv_det);

		return matrix3x3f{ x_axis, y_axis, z_axis };
	}

	//////////////////////////////////////////////////////////////////////////
	// Inverses a 3x3 matrix.
	// If the input matrix has a determinant whose absolute value is below the supplied threshold, the
	// fall back value is returned instead.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline matrix3x3f RTM_SIMD_CALL matrix_inverse(matrix3x3f_arg0 input, matrix3x3f_arg1 fallback, float threshold = 1.0E-8F) RTM_NO_EXCEPT
	{
		const vector4f v00_v01_v10_v11 = vector_mix<mix4::x, mix4::y, mix4::a, mix4::b>(input.x_axis, input.y_axis);
		const vector4f v02_v03_v12_v13 = vector_mix<mix4::z, mix4::w, mix4::c, mix4::d>(input.x_axis, input.y_axis);

		const vector4f v00_v10_v20_v22 = vector_mix<mix4::x, mix4::z, mix4::a, mix4::c>(v00_v01_v10_v11, input.z_axis);
		const vector4f v01_v11_v21_v23 = vector_mix<mix4::y, mix4::w, mix4::b, mix4::d>(v00_v01_v10_v11, input.z_axis);
		const vector4f v02_v12_v22_v22 = vector_mix<mix4::x, mix4::z, mix4::c, mix4::c>(v02_v03_v12_v13, input.z_axis);

		const vector4f v11_v21_v01 = vector_mix<mix4::y, mix4::z, mix4::b, mix4::c>(v01_v11_v21_v23, input.x_axis);
		const vector4f v22_v02_v12_v10 = vector_mix<mix4::z, mix4::x, mix4::c, mix4::a>(v02_v12_v22_v22, input.y_axis);
		const vector4f v11v22_v21v02_v01v12 = vector_mul(v11_v21_v01, v22_v02_v12_v10);

		const vector4f v01_v02_v11_v12 = vector_mix<mix4::y, mix4::z, mix4::b, mix4::c>(input.x_axis, input.y_axis);

		const vector4f v12_v01_v11 = vector_mix<mix4::w, mix4::x, mix4::b, mix4::c>(v01_v02_v11_v12, v01_v11_v21_v23);
		const vector4f v21_v22_v02 = vector_mix<mix4::y, mix4::z, mix4::b, mix4::c>(input.z_axis, v22_v02_v12_v10);

		vector4f x_axis = vector_neg_mul_sub(v12_v01_v11, v21_v22_v02, v11v22_v21v02_v01v12);

		const vector4f v20_v00_v10_v22 = vector_mix<mix4::z, mix4::x, mix4::d, mix4::a>(v00_v10_v20_v22, v22_v02_v12_v10);
		const vector4f v12_v22_v02 = vector_mix<mix4::y, mix4::z, mix4::c, mix4::c>(v02_v12_v22_v22, v21_v22_v02);
		const vector4f v20v12_v00v22_v10v02 = vector_mul(v20_v00_v10_v22, v12_v22_v02);

		const vector4f v10_v02_v00 = vector_mix<mix4::w, mix4::y, mix4::b, mix4::c>(v22_v02_v12_v10, v20_v00_v10_v22);
		const vector4f v22_v20_v12 = vector_mix<mix4::w, mix4::x, mix4::a, mix4::c>(v20_v00_v10_v22, v12_v22_v02);

		vector4f y_axis = vector_neg_mul_sub(v10_v02_v00, v22_v20_v12, v20v12_v00v22_v10v02);

		const vector4f v10_v20_v00 = vector_mix<mix4::z, mix4::x, mix4::c, mix4::c>(v20_v00_v10_v22, v10_v02_v00);
		const vector4f v21_v01_v11 = vector_mix<mix4::y, mix4::z, mix4::c, mix4::c>(v11_v21_v01, v12_v01_v11);
		const vector4f v10v21_v20v01_v00v11 = vector_mul(v10_v20_v00, v21_v01_v11);

		const vector4f v20_v00_v01 = vector_mix<mix4::y, mix4::z, mix4::c, mix4::c>(v10_v20_v00, v11_v21_v01);
		const vector4f v11_v21_v10 = vector_mix<mix4::x, mix4::y, mix4::a, mix4::c>(v11_v21_v01, v10_v20_v00);

		vector4f z_axis = vector_neg_mul_sub(v20_v00_v01, v11_v21_v10, v10v21_v20v01_v00v11);

		const vector4f o00_o00_o10_o10 = vector_mix<mix4::x, mix4::x, mix4::a, mix4::a>(x_axis, y_axis);
		const vector4f o00_o10_o20 = vector_mix<mix4::x, mix4::z, mix4::a, mix4::a>(o00_o00_o10_o10, z_axis);

		const scalarf det = vector_dot3_as_scalar(o00_o10_o20, input.x_axis);
		if (scalar_cast(scalar_abs(det)) < threshold)
			return fallback;

		const vector4f inv_det = vector_set(scalar_reciprocal(det));

		x_axis = vector_mul(x_axis, inv_det);
		y_axis = vector_mul(y_axis, inv_det);
		z_axis = vector_mul(z_axis, inv_det);

		return matrix3x3f{ x_axis, y_axis, z_axis };
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the determinant of the input 3x3 matrix.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline scalarf RTM_SIMD_CALL matrix_determinant(matrix3x3f_arg0 input) RTM_NO_EXCEPT
	{
		const vector4f v00_v01_v10_v11 = vector_mix<mix4::x, mix4::y, mix4::a, mix4::b>(input.x_axis, input.y_axis);
		const vector4f v02_v03_v12_v13 = vector_mix<mix4::z, mix4::w, mix4::c, mix4::d>(input.x_axis, input.y_axis);

		const vector4f v00_v10_v20_v22 = vector_mix<mix4::x, mix4::z, mix4::a, mix4::c>(v00_v01_v10_v11, input.z_axis);
		const vector4f v01_v11_v21_v23 = vector_mix<mix4::y, mix4::w, mix4::b, mix4::d>(v00_v01_v10_v11, input.z_axis);
		const vector4f v02_v12_v22_v22 = vector_mix<mix4::x, mix4::z, mix4::c, mix4::c>(v02_v03_v12_v13, input.z_axis);

		const vector4f v11_v21_v01 = vector_mix<mix4::y, mix4::z, mix4::b, mix4::c>(v01_v11_v21_v23, input.x_axis);
		const vector4f v22_v02_v12_v10 = vector_mix<mix4::z, mix4::x, mix4::c, mix4::a>(v02_v12_v22_v22, input.y_axis);
		const vector4f v11v22_v21v02_v01v12 = vector_mul(v11_v21_v01, v22_v02_v12_v10);

		const vector4f v01_v02_v11_v12 = vector_mix<mix4::y, mix4::z, mix4::b, mix4::c>(input.x_axis, input.y_axis);

		const vector4f v12_v01_v11 = vector_mix<mix4::w, mix4::x, mix4::b, mix4::c>(v01_v02_v11_v12, v01_v11_v21_v23);
		const vector4f v21_v22_v02 = vector_mix<mix4::y, mix4::z, mix4::b, mix4::c>(input.z_axis, v22_v02_v12_v10);

		vector4f x_axis = vector_neg_mul_sub(v12_v01_v11, v21_v22_v02, v11v22_v21v02_v01v12);

		const vector4f v20_v00_v10_v22 = vector_mix<mix4::z, mix4::x, mix4::d, mix4::a>(v00_v10_v20_v22, v22_v02_v12_v10);
		const vector4f v12_v22_v02 = vector_mix<mix4::y, mix4::z, mix4::c, mix4::c>(v02_v12_v22_v22, v21_v22_v02);
		const vector4f v20v12_v00v22_v10v02 = vector_mul(v20_v00_v10_v22, v12_v22_v02);

		const vector4f v10_v02_v00 = vector_mix<mix4::w, mix4::y, mix4::b, mix4::c>(v22_v02_v12_v10, v20_v00_v10_v22);
		const vector4f v22_v20_v12 = vector_mix<mix4::w, mix4::x, mix4::a, mix4::c>(v20_v00_v10_v22, v12_v22_v02);

		vector4f y_axis = vector_neg_mul_sub(v10_v02_v00, v22_v20_v12, v20v12_v00v22_v10v02);

		const vector4f v10_v20_v00 = vector_mix<mix4::z, mix4::x, mix4::c, mix4::c>(v20_v00_v10_v22, v10_v02_v00);
		const vector4f v21_v01_v11 = vector_mix<mix4::y, mix4::z, mix4::c, mix4::c>(v11_v21_v01, v12_v01_v11);
		const vector4f v10v21_v20v01_v00v11 = vector_mul(v10_v20_v00, v21_v01_v11);

		const vector4f v20_v00_v01 = vector_mix<mix4::y, mix4::z, mix4::c, mix4::c>(v10_v20_v00, v11_v21_v01);
		const vector4f v11_v21_v10 = vector_mix<mix4::x, mix4::y, mix4::a, mix4::c>(v11_v21_v01, v10_v20_v00);

		vector4f z_axis = vector_neg_mul_sub(v20_v00_v01, v11_v21_v10, v10v21_v20v01_v00v11);

		const vector4f o00_o00_o10_o10 = vector_mix<mix4::x, mix4::x, mix4::a, mix4::a>(x_axis, y_axis);
		const vector4f o00_o10_o20 = vector_mix<mix4::x, mix4::z, mix4::a, mix4::a>(o00_o00_o10_o10, z_axis);

		return vector_dot3_as_scalar(o00_o10_o20, input.x_axis);
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the minor of the input 3x3 matrix.
	// See: https://en.wikipedia.org/wiki/Minor_(linear_algebra)
	// The minor is the determinant of the sub-matrix input when the specified
	// row and column are removed.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline scalarf RTM_SIMD_CALL matrix_minor(matrix3x3f_arg0 input, axis3 row, axis3 column) RTM_NO_EXCEPT
	{
		// The minor boils down to calculating the determinant of a 2x2 matrix.
		// det([a, b], [c, d]) = (a * d) - (b * c)

		// Find which two rows we need.
		vector4f row0;
		vector4f row1;
		if (row == axis3::x)
		{
			row0 = input.y_axis;
			row1 = input.z_axis;
		}
		else if (row == axis3::y)
		{
			row0 = input.x_axis;
			row1 = input.z_axis;
		}
		else
		{
			row0 = input.x_axis;
			row1 = input.y_axis;
		}

		// Because our input is a 3x3 matrix, there are only a few possibilities for the 2x2 part:
		// row0 = [x0, y0, z0]
		// row1 = [x1, y1, z1]
		// det([x0, y0], [x1, y1]) = (x0 * y1) - (y0 * x1) (z removed)
		// det([x0, z0], [x1, z1]) = (x0 * z1) - (z0 * x1) (y removed)
		// det([y0, z0], [y1, z1]) = (y0 * z1) - (z0 * y1) (x removed)
		// det([column0, column1], [column2, column3]) = (column0 * column3) - (column1 * column2)

		// For performance reasons, we can compute all three determinants at the same time.
		const vector4f column0 = vector_mix<mix4::x, mix4::x, mix4::y, mix4::y>(row0, row0);
		const vector4f column1 = vector_mix<mix4::y, mix4::z, mix4::z, mix4::z>(row0, row0);
		const vector4f column2 = vector_mix<mix4::x, mix4::x, mix4::y, mix4::y>(row1, row1);
		const vector4f column3 = vector_mix<mix4::y, mix4::z, mix4::z, mix4::z>(row1, row1);

		const vector4f determinants = vector_neg_mul_sub(column1, column2, vector_mul(column0, column3));

		// Extract the one we need
		if (column == axis3::x)
			return vector_get_z_as_scalar(determinants);
		else if (column == axis3::y)
			return vector_get_y_as_scalar(determinants);
		else
			return vector_get_x_as_scalar(determinants);
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the cofactor matrix of the input 3x3 matrix.
	// See: https://en.wikipedia.org/wiki/Minor_(linear_algebra)#Cofactor_expansion_of_the_determinant
	// Note: The proper way to transform a normal by a matrix with non-uniform scale
	// is to multiply the normal with the cofactor matrix.
	// See: https://github.com/graphitemaster/normals_revisited
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline matrix3x3f RTM_SIMD_CALL matrix_cofactor(matrix3x3f_arg0 input) RTM_NO_EXCEPT
	{
		const vector4f x_axis = vector_cross3(input.y_axis, input.z_axis);
		const vector4f y_axis = vector_cross3(input.z_axis, input.x_axis);
		const vector4f z_axis = vector_cross3(input.x_axis, input.y_axis);
		return matrix3x3f{ x_axis, y_axis, z_axis };
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the adjugate of the input matrix.
	// See: https://en.wikipedia.org/wiki/Adjugate_matrix
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline matrix3x3f RTM_SIMD_CALL matrix_adjugate(matrix3x3f_arg0 input) RTM_NO_EXCEPT
	{
		return matrix_transpose(matrix_cofactor(input));
	}

	//////////////////////////////////////////////////////////////////////////
	// Removes the 3D scale from a 3x3 matrix.
	// Note that if the scaling is 0.0 for a particular axis, the original rotation axis cannot
	// be recovered trivially and no attempt is done to do so. In theory, we could use the other axes
	// to try and recover it.
	// TODO: Implement rotation recovering, perhaps in a separate function and rename this
	// one to matrix_remove_non_zero_scale(..)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline matrix3x3f RTM_SIMD_CALL matrix_remove_scale(matrix3x3f_arg0 input) RTM_NO_EXCEPT
	{
		matrix3x3f result;
		result.x_axis = vector_normalize3(input.x_axis, input.x_axis);
		result.y_axis = vector_normalize3(input.y_axis, input.y_axis);
		result.z_axis = vector_normalize3(input.z_axis, input.z_axis);
		return result;
	}

	RTM_IMPL_VERSION_NAMESPACE_END
}

RTM_IMPL_FILE_PRAGMA_POP

```

`includes/rtm/matrix3x4d.h`:

```h
#pragma once

////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2017 Nicholas Frechette & Animation Compression Library contributors
// Copyright (c) 2018 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include "rtm/math.h"
#include "rtm/matrix3x3d.h"
#include "rtm/quatd.h"
#include "rtm/qvsd.h"
#include "rtm/vector4d.h"
#include "rtm/version.h"
#include "rtm/impl/compiler_utils.h"
#include "rtm/impl/matrix_common.h"
#include "rtm/impl/matrix_affine_common.h"

RTM_IMPL_FILE_PRAGMA_PUSH

namespace rtm
{
	RTM_IMPL_VERSION_NAMESPACE_BEGIN

	//////////////////////////////////////////////////////////////////////////
	// Converts a rotation 3x3 matrix into a 3x4 affine matrix.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE matrix3x4d RTM_SIMD_CALL matrix_from_rotation(matrix3x3d_arg0 rotation) RTM_NO_EXCEPT
	{
		return matrix3x4d{ rotation.x_axis, rotation.y_axis, rotation.z_axis, (vector4d)vector_zero() };
	}

	//////////////////////////////////////////////////////////////////////////
	// Converts a translation vector into a 3x4 affine matrix.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE matrix3x4d RTM_SIMD_CALL matrix_from_translation(vector4d_arg0 translation) RTM_NO_EXCEPT
	{
		return matrix3x4d{ vector_set(1.0, 0.0, 0.0, 0.0), vector_set(0.0, 1.0, 0.0, 0.0), vector_set(0.0, 0.0, 1.0, 0.0), translation };
	}

	//////////////////////////////////////////////////////////////////////////
	// Sets a 3x4 affine matrix from a rotation quaternion and translation.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline matrix3x4d RTM_SIMD_CALL matrix_from_qv(quatd_arg0 quat, vector4d_arg1 translation) RTM_NO_EXCEPT
	{
		RTM_ASSERT(quat_is_normalized(quat), "Quaternion is not normalized");

		const double x2 = quat_get_x(quat) + quat_get_x(quat);
		const double y2 = quat_get_y(quat) + quat_get_y(quat);
		const double z2 = quat_get_z(quat) + quat_get_z(quat);
		const double xx = quat_get_x(quat) * x2;
		const double xy = quat_get_x(quat) * y2;
		const double xz = quat_get_x(quat) * z2;
		const double yy = quat_get_y(quat) * y2;
		const double yz = quat_get_y(quat) * z2;
		const double zz = quat_get_z(quat) * z2;
		const double wx = quat_get_w(quat) * x2;
		const double wy = quat_get_w(quat) * y2;
		const double wz = quat_get_w(quat) * z2;

		const vector4d x_axis = vector_set(1.0 - (yy + zz), xy + wz, xz - wy, 0.0);
		const vector4d y_axis = vector_set(xy - wz, 1.0 - (xx + zz), yz + wx, 0.0);
		const vector4d z_axis = vector_set(xz + wy, yz - wx, 1.0 - (xx + yy), 0.0);
		return matrix3x4d{ x_axis, y_axis, z_axis, translation };
	}

	//////////////////////////////////////////////////////////////////////////
	// Converts a QV transform into a 3x4 affine matrix.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline matrix3x4d RTM_SIMD_CALL matrix_from_qv(qvd_arg0 transform) RTM_NO_EXCEPT
	{
		return matrix_from_qv(transform.rotation, transform.translation);
	}

	//////////////////////////////////////////////////////////////////////////
	// Sets a 3x4 affine matrix from a rotation quaternion, translation, and scalar scale.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline matrix3x4d RTM_SIMD_CALL matrix_from_qvs(quatd_arg0 quat, vector4d_arg1 translation, double scale) RTM_NO_EXCEPT
	{
		RTM_ASSERT(quat_is_normalized(quat), "Quaternion is not normalized");

		const double x2 = quat_get_x(quat) + quat_get_x(quat);
		const double y2 = quat_get_y(quat) + quat_get_y(quat);
		const double z2 = quat_get_z(quat) + quat_get_z(quat);
		const double xx = quat_get_x(quat) * x2;
		const double xy = quat_get_x(quat) * y2;
		const double xz = quat_get_x(quat) * z2;
		const double yy = quat_get_y(quat) * y2;
		const double yz = quat_get_y(quat) * z2;
		const double zz = quat_get_z(quat) * z2;
		const double wx = quat_get_w(quat) * x2;
		const double wy = quat_get_w(quat) * y2;
		const double wz = quat_get_w(quat) * z2;

		const scalard scale_s = scalar_set(scale);

		const vector4d x_axis = vector_mul(vector_set(1.0 - (yy + zz), xy + wz, xz - wy, 0.0), scale_s);
		const vector4d y_axis = vector_mul(vector_set(xy - wz, 1.0 - (xx + zz), yz + wx, 0.0), scale_s);
		const vector4d z_axis = vector_mul(vector_set(xz + wy, yz - wx, 1.0 - (xx + yy), 0.0), scale_s);
		return matrix3x4d{ x_axis, y_axis, z_axis, translation };
	}

	//////////////////////////////////////////////////////////////////////////
	// Converts a QVS transform into a 3x4 affine matrix.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline matrix3x4d RTM_SIMD_CALL matrix_from_qvs(qvsd_arg0 transform) RTM_NO_EXCEPT
	{
		return matrix_from_qvs(transform.rotation, transform.translation_scale, qvs_get_scale(transform));
	}

	//////////////////////////////////////////////////////////////////////////
	// Sets a 3x4 affine matrix from a rotation quaternion, translation, and 3D scale.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline matrix3x4d RTM_SIMD_CALL matrix_from_qvv(quatd_arg0 quat, vector4d_arg1 translation, vector4d_arg2 scale) RTM_NO_EXCEPT
	{
		RTM_ASSERT(quat_is_normalized(quat), "Quaternion is not normalized");

		const double x2 = quat_get_x(quat) + quat_get_x(quat);
		const double y2 = quat_get_y(quat) + quat_get_y(quat);
		const double z2 = quat_get_z(quat) + quat_get_z(quat);
		const double xx = quat_get_x(quat) * x2;
		const double xy = quat_get_x(quat) * y2;
		const double xz = quat_get_x(quat) * z2;
		const double yy = quat_get_y(quat) * y2;
		const double yz = quat_get_y(quat) * z2;
		const double zz = quat_get_z(quat) * z2;
		const double wx = quat_get_w(quat) * x2;
		const double wy = quat_get_w(quat) * y2;
		const double wz = quat_get_w(quat) * z2;

		const scalard scale_x = vector_get_x_as_scalar(scale);
		const scalard scale_y = vector_get_y_as_scalar(scale);
		const scalard scale_z = vector_get_z_as_scalar(scale);

		const vector4d x_axis = vector_mul(vector_set(1.0 - (yy + zz), xy + wz, xz - wy, 0.0), scale_x);
		const vector4d y_axis = vector_mul(vector_set(xy - wz, 1.0 - (xx + zz), yz + wx, 0.0), scale_y);
		const vector4d z_axis = vector_mul(vector_set(xz + wy, yz - wx, 1.0 - (xx + yy), 0.0), scale_z);
		return matrix3x4d{ x_axis, y_axis, z_axis, translation };
	}

	//////////////////////////////////////////////////////////////////////////
	// Converts a QVV transform into a 3x4 affine matrix.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline matrix3x4d RTM_SIMD_CALL matrix_from_qvv(qvvd_arg0 transform) RTM_NO_EXCEPT
	{
		return matrix_from_qvv(transform.rotation, transform.translation, transform.scale);
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the axis pointing in the forward direction of the default coordinate system (Z+).
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr vector4d RTM_SIMD_CALL matrix_get_coord_forward(matrix3x4d_arg0 input) RTM_NO_EXCEPT
	{
		return input.z_axis;
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the axis pointing in the up direction of the default coordinate system (Y+).
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr vector4d RTM_SIMD_CALL matrix_get_coord_up(matrix3x4d_arg0 input) RTM_NO_EXCEPT
	{
		return input.y_axis;
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the axis pointing in the cross direction of the default coordinate system (X+).
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr vector4d RTM_SIMD_CALL matrix_get_coord_cross(matrix3x4d_arg0 input) RTM_NO_EXCEPT
	{
		return input.x_axis;
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the axis holding the position of the default coordinate system (W+).
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr vector4d RTM_SIMD_CALL matrix_get_coord_position(matrix3x4d_arg0 input) RTM_NO_EXCEPT
	{
		return input.w_axis;
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the desired 3x4 affine matrix axis.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr vector4d RTM_SIMD_CALL matrix_get_axis(matrix3x4d_arg0 input, axis4 axis) RTM_NO_EXCEPT
	{
		return axis == axis4::x ? input.x_axis : (axis == axis4::y ? input.y_axis : (axis == axis4::z ? input.z_axis : input.w_axis));
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns a new 3x4 matrix where the specified axis has been replaced on the input matrix.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline matrix3x4d RTM_SIMD_CALL matrix_set_axis(matrix3x4d_arg0 input, vector4d_arg4 axis_value, axis4 axis) RTM_NO_EXCEPT
	{
		switch (axis)
		{
			default:
			case axis4::x:	return matrix3x4d{ axis_value, input.y_axis, input.z_axis, input.w_axis };
			case axis4::y:	return matrix3x4d{ input.x_axis, axis_value, input.z_axis, input.w_axis };
			case axis4::z:	return matrix3x4d{ input.x_axis, input.y_axis, axis_value, input.w_axis };
			case axis4::w:	return matrix3x4d{ input.x_axis, input.y_axis, input.z_axis, axis_value };
		}
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the desired 3x4 matrix component from the specified axis.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline rtm_impl::vector4d_vector_get_component RTM_SIMD_CALL matrix_get_component(matrix3x4d_arg0 input, axis4 axis, component3 component) RTM_NO_EXCEPT
	{
		switch (axis)
		{
			default:
			case axis4::x:	return vector_get_component3(input.x_axis, component);
			case axis4::y:	return vector_get_component3(input.y_axis, component);
			case axis4::z:	return vector_get_component3(input.z_axis, component);
			case axis4::w:	return vector_get_component3(input.w_axis, component);
		}
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the desired 3x4 matrix component from the specified axis.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline scalard RTM_SIMD_CALL matrix_get_component_as_scalar(matrix3x4d_arg0 input, axis4 axis, component3 component) RTM_NO_EXCEPT
	{
		switch (axis)
		{
			default:
			case axis4::x:	return vector_get_component3_as_scalar(input.x_axis, component);
			case axis4::y:	return vector_get_component3_as_scalar(input.y_axis, component);
			case axis4::z:	return vector_get_component3_as_scalar(input.z_axis, component);
			case axis4::w:	return vector_get_component3_as_scalar(input.w_axis, component);
		}
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns a new 3x4 matrix where the specified axis/component has been replaced on the input matrix.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline matrix3x4d RTM_SIMD_CALL matrix_set_component(matrix3x4d_arg0 input, double component_value, axis4 axis, component3 component) RTM_NO_EXCEPT
	{
		switch (axis)
		{
			default:
			case axis4::x:	return matrix3x4d{ vector_set_component3(input.x_axis, component_value, component), input.y_axis, input.z_axis, input.w_axis };
			case axis4::y:	return matrix3x4d{ input.x_axis, vector_set_component3(input.y_axis, component_value, component), input.z_axis, input.w_axis };
			case axis4::z:	return matrix3x4d{ input.x_axis, input.y_axis, vector_set_component3(input.z_axis, component_value, component), input.w_axis };
			case axis4::w:	return matrix3x4d{ input.x_axis, input.y_axis, input.z_axis, vector_set_component3(input.w_axis, component_value, component) };
		}
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Returns a new 3x4 matrix where the specified axis/component has been replaced on the input matrix.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline matrix3x4d RTM_SIMD_CALL matrix_set_component(matrix3x4d_arg0 input, scalard_arg4 component_value, axis4 axis, component3 component) RTM_NO_EXCEPT
	{
		switch (axis)
		{
			default:
			case axis4::x:	return matrix3x4d{ vector_set_component3(input.x_axis, component_value, component), input.y_axis, input.z_axis, input.w_axis };
			case axis4::y:	return matrix3x4d{ input.x_axis, vector_set_component3(input.y_axis, component_value, component), input.z_axis, input.w_axis };
			case axis4::z:	return matrix3x4d{ input.x_axis, input.y_axis, vector_set_component3(input.z_axis, component_value, component), input.w_axis };
			case axis4::w:	return matrix3x4d{ input.x_axis, input.y_axis, input.z_axis, vector_set_component3(input.w_axis, component_value, component) };
		}
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Converts a 3x4 affine matrix into a rotation quaternion.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline quatd RTM_SIMD_CALL quat_from_matrix(matrix3x4d_arg0 input) RTM_NO_EXCEPT
	{
		return rtm_impl::quat_from_matrix(input.x_axis, input.y_axis, input.z_axis);
	}

	//////////////////////////////////////////////////////////////////////////
	// Multiplies two 3x4 affine matrices.
	// Multiplication order is as follow: local_to_world = matrix_mul(local_to_object, object_to_world)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline matrix3x4d RTM_SIMD_CALL matrix_mul(matrix3x4d_arg0 lhs, matrix3x4d_arg1 rhs) RTM_NO_EXCEPT
	{
		vector4d tmp = vector_mul(vector_dup_x(lhs.x_axis), rhs.x_axis);
		tmp = vector_mul_add(vector_dup_y(lhs.x_axis), rhs.y_axis, tmp);
		tmp = vector_mul_add(vector_dup_z(lhs.x_axis), rhs.z_axis, tmp);
		vector4d x_axis = tmp;

		tmp = vector_mul(vector_dup_x(lhs.y_axis), rhs.x_axis);
		tmp = vector_mul_add(vector_dup_y(lhs.y_axis), rhs.y_axis, tmp);
		tmp = vector_mul_add(vector_dup_z(lhs.y_axis), rhs.z_axis, tmp);
		vector4d y_axis = tmp;

		tmp = vector_mul(vector_dup_x(lhs.z_axis), rhs.x_axis);
		tmp = vector_mul_add(vector_dup_y(lhs.z_axis), rhs.y_axis, tmp);
		tmp = vector_mul_add(vector_dup_z(lhs.z_axis), rhs.z_axis, tmp);
		vector4d z_axis = tmp;

		tmp = vector_mul(vector_dup_x(lhs.w_axis), rhs.x_axis);
		tmp = vector_mul_add(vector_dup_y(lhs.w_axis), rhs.y_axis, tmp);
		tmp = vector_mul_add(vector_dup_z(lhs.w_axis), rhs.z_axis, tmp);
		vector4d w_axis = vector_add(rhs.w_axis, tmp);
		return matrix3x4d{ x_axis, y_axis, z_axis, w_axis };
	}

	//////////////////////////////////////////////////////////////////////////
	// Multiplies a 3x4 affine matrix and a 3D point.
	// Multiplication order is as follow: world_position = matrix_mul(local_position, local_to_world)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4d RTM_SIMD_CALL matrix_mul_point3(vector4d_arg0 point, matrix3x4d_argn mtx) RTM_NO_EXCEPT
	{
		vector4d tmp0;
		vector4d tmp1;

		tmp0 = vector_mul(vector_dup_x(point), mtx.x_axis);
		tmp0 = vector_mul_add(vector_dup_y(point), mtx.y_axis, tmp0);
		tmp1 = vector_mul_add(vector_dup_z(point), mtx.z_axis, mtx.w_axis);

		return vector_add(tmp0, tmp1);
	}

	//////////////////////////////////////////////////////////////////////////
	// Multiplies a 3x4 affine matrix and a 3D vector.
	// Multiplication order is as follow: world_position = matrix_mul(local_vector, local_to_world)
	// Note: The proper way to transform a normal by an affine matrix with non-uniform scale
	// is to multiply the normal with the cofactor matrix of the 3x3 rotation/scale part.
	// See: https://github.com/graphitemaster/normals_revisited
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4d RTM_SIMD_CALL matrix_mul_vector3(vector4d_arg0 vec3, matrix3x4d_argn mtx) RTM_NO_EXCEPT
	{
		vector4d tmp;

		tmp = vector_mul(vector_dup_x(vec3), mtx.x_axis);
		tmp = vector_mul_add(vector_dup_y(vec3), mtx.y_axis, tmp);
		tmp = vector_mul_add(vector_dup_z(vec3), mtx.z_axis, tmp);

		return tmp;
	}

	//////////////////////////////////////////////////////////////////////////
	// Transposes a 3x4 affine matrix.
	// Note: This transposes the upper 3x3 rotation/scale part of the matrix
	// and it discards the translation. This is because a transposed 4x4 affine
	// matrix is no longer affine due to its last row no longer being [0, 0, 0, 1].
	// The most common usage of an affine transpose operation is to construct the
	// inverse transpose used to transform normal bi-vectors.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE matrix3x3d RTM_SIMD_CALL matrix_transpose(matrix3x4d_arg0 input) RTM_NO_EXCEPT
	{
		vector4d x_axis;
		vector4d y_axis;
		vector4d z_axis;
		RTM_MATRIXD_TRANSPOSE_3X3(input.x_axis, input.y_axis, input.z_axis, x_axis, y_axis, z_axis);
		return matrix3x3d{ x_axis, y_axis, z_axis };
	}

	//////////////////////////////////////////////////////////////////////////
	// Inverses a 3x4 affine matrix.
	// If the input matrix is not invertible, the result is undefined.
	// For a safe alternative, supply a fallback value and a threshold.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline matrix3x4d RTM_SIMD_CALL matrix_inverse(matrix3x4d_arg0 input) RTM_NO_EXCEPT
	{
		// Invert the 3x3 portion of the matrix that contains the rotation and 3D scale
		const vector4d v00_v01_v10_v11 = vector_mix<mix4::x, mix4::y, mix4::a, mix4::b>(input.x_axis, input.y_axis);
		const vector4d v02_v03_v12_v13 = vector_mix<mix4::z, mix4::w, mix4::c, mix4::d>(input.x_axis, input.y_axis);

		const vector4d v00_v10_v20_v22 = vector_mix<mix4::x, mix4::z, mix4::a, mix4::c>(v00_v01_v10_v11, input.z_axis);
		const vector4d v01_v11_v21_v23 = vector_mix<mix4::y, mix4::w, mix4::b, mix4::d>(v00_v01_v10_v11, input.z_axis);
		const vector4d v02_v12_v22_v22 = vector_mix<mix4::x, mix4::z, mix4::c, mix4::c>(v02_v03_v12_v13, input.z_axis);

		const vector4d v11_v21_v01 = vector_mix<mix4::y, mix4::z, mix4::b, mix4::c>(v01_v11_v21_v23, input.x_axis);
		const vector4d v22_v02_v12_v10 = vector_mix<mix4::z, mix4::x, mix4::c, mix4::a>(v02_v12_v22_v22, input.y_axis);
		const vector4d v11v22_v21v02_v01v12 = vector_mul(v11_v21_v01, v22_v02_v12_v10);

		const vector4d v01_v02_v11_v12 = vector_mix<mix4::y, mix4::z, mix4::b, mix4::c>(input.x_axis, input.y_axis);

		const vector4d v12_v01_v11 = vector_mix<mix4::w, mix4::x, mix4::b, mix4::c>(v01_v02_v11_v12, v01_v11_v21_v23);
		const vector4d v21_v22_v02 = vector_mix<mix4::y, mix4::z, mix4::b, mix4::c>(input.z_axis, v22_v02_v12_v10);

		vector4d x_axis = vector_neg_mul_sub(v12_v01_v11, v21_v22_v02, v11v22_v21v02_v01v12);

		const vector4d v20_v00_v10_v22 = vector_mix<mix4::z, mix4::x, mix4::d, mix4::a>(v00_v10_v20_v22, v22_v02_v12_v10);
		const vector4d v12_v22_v02 = vector_mix<mix4::y, mix4::z, mix4::c, mix4::c>(v02_v12_v22_v22, v21_v22_v02);
		const vector4d v20v12_v00v22_v10v02 = vector_mul(v20_v00_v10_v22, v12_v22_v02);

		const vector4d v10_v02_v00 = vector_mix<mix4::w, mix4::y, mix4::b, mix4::c>(v22_v02_v12_v10, v20_v00_v10_v22);
		const vector4d v22_v20_v12 = vector_mix<mix4::w, mix4::x, mix4::a, mix4::c>(v20_v00_v10_v22, v12_v22_v02);

		vector4d y_axis = vector_neg_mul_sub(v10_v02_v00, v22_v20_v12, v20v12_v00v22_v10v02);

		const vector4d v10_v20_v00 = vector_mix<mix4::z, mix4::x, mix4::c, mix4::c>(v20_v00_v10_v22, v10_v02_v00);
		const vector4d v21_v01_v11 = vector_mix<mix4::y, mix4::z, mix4::c, mix4::c>(v11_v21_v01, v12_v01_v11);
		const vector4d v10v21_v20v01_v00v11 = vector_mul(v10_v20_v00, v21_v01_v11);

		const vector4d v20_v00_v01 = vector_mix<mix4::y, mix4::z, mix4::c, mix4::c>(v10_v20_v00, v11_v21_v01);
		const vector4d v11_v21_v10 = vector_mix<mix4::x, mix4::y, mix4::a, mix4::c>(v11_v21_v01, v10_v20_v00);

		vector4d z_axis = vector_neg_mul_sub(v20_v00_v01, v11_v21_v10, v10v21_v20v01_v00v11);

		const vector4d o00_o00_o10_o10 = vector_mix<mix4::x, mix4::x, mix4::a, mix4::a>(x_axis, y_axis);
		const vector4d o00_o10_o20 = vector_mix<mix4::x, mix4::z, mix4::a, mix4::a>(o00_o00_o10_o10, z_axis);

		const scalard det = vector_dot3_as_scalar(o00_o10_o20, input.x_axis);
		const scalard inv_det_s = scalar_reciprocal(det);
		const vector4d inv_det = vector_set(inv_det_s);

		x_axis = vector_mul(x_axis, inv_det);
		y_axis = vector_mul(y_axis, inv_det);
		z_axis = vector_mul(z_axis, inv_det);

		// Invert the translation
		const vector4d tmp0 = vector_mul(vector_dup_z(input.w_axis), z_axis);
		const vector4d tmp1 = vector_mul_add(vector_dup_y(input.w_axis), y_axis, tmp0);
		vector4d w_axis = vector_neg(vector_mul_add(vector_dup_x(input.w_axis), x_axis, tmp1));

		return matrix3x4d{ x_axis, y_axis, z_axis, w_axis };
	}

	//////////////////////////////////////////////////////////////////////////
	// Inverses a 3x4 affine matrix.
	// If the input matrix has a determinant whose absolute value is below the supplied threshold, the
	// fall back value is returned instead.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline matrix3x4d RTM_SIMD_CALL matrix_inverse(matrix3x4d_arg0 input, matrix3x4d_argn fallback, double threshold = 1.0E-8) RTM_NO_EXCEPT
	{
		// Invert the 3x3 portion of the matrix that contains the rotation and 3D scale
		const vector4d v00_v01_v10_v11 = vector_mix<mix4::x, mix4::y, mix4::a, mix4::b>(input.x_axis, input.y_axis);
		const vector4d v02_v03_v12_v13 = vector_mix<mix4::z, mix4::w, mix4::c, mix4::d>(input.x_axis, input.y_axis);

		const vector4d v00_v10_v20_v22 = vector_mix<mix4::x, mix4::z, mix4::a, mix4::c>(v00_v01_v10_v11, input.z_axis);
		const vector4d v01_v11_v21_v23 = vector_mix<mix4::y, mix4::w, mix4::b, mix4::d>(v00_v01_v10_v11, input.z_axis);
		const vector4d v02_v12_v22_v22 = vector_mix<mix4::x, mix4::z, mix4::c, mix4::c>(v02_v03_v12_v13, input.z_axis);

		const vector4d v11_v21_v01 = vector_mix<mix4::y, mix4::z, mix4::b, mix4::c>(v01_v11_v21_v23, input.x_axis);
		const vector4d v22_v02_v12_v10 = vector_mix<mix4::z, mix4::x, mix4::c, mix4::a>(v02_v12_v22_v22, input.y_axis);
		const vector4d v11v22_v21v02_v01v12 = vector_mul(v11_v21_v01, v22_v02_v12_v10);

		const vector4d v01_v02_v11_v12 = vector_mix<mix4::y, mix4::z, mix4::b, mix4::c>(input.x_axis, input.y_axis);

		const vector4d v12_v01_v11 = vector_mix<mix4::w, mix4::x, mix4::b, mix4::c>(v01_v02_v11_v12, v01_v11_v21_v23);
		const vector4d v21_v22_v02 = vector_mix<mix4::y, mix4::z, mix4::b, mix4::c>(input.z_axis, v22_v02_v12_v10);

		vector4d x_axis = vector_neg_mul_sub(v12_v01_v11, v21_v22_v02, v11v22_v21v02_v01v12);

		const vector4d v20_v00_v10_v22 = vector_mix<mix4::z, mix4::x, mix4::d, mix4::a>(v00_v10_v20_v22, v22_v02_v12_v10);
		const vector4d v12_v22_v02 = vector_mix<mix4::y, mix4::z, mix4::c, mix4::c>(v02_v12_v22_v22, v21_v22_v02);
		const vector4d v20v12_v00v22_v10v02 = vector_mul(v20_v00_v10_v22, v12_v22_v02);

		const vector4d v10_v02_v00 = vector_mix<mix4::w, mix4::y, mix4::b, mix4::c>(v22_v02_v12_v10, v20_v00_v10_v22);
		const vector4d v22_v20_v12 = vector_mix<mix4::w, mix4::x, mix4::a, mix4::c>(v20_v00_v10_v22, v12_v22_v02);

		vector4d y_axis = vector_neg_mul_sub(v10_v02_v00, v22_v20_v12, v20v12_v00v22_v10v02);

		const vector4d v10_v20_v00 = vector_mix<mix4::z, mix4::x, mix4::c, mix4::c>(v20_v00_v10_v22, v10_v02_v00);
		const vector4d v21_v01_v11 = vector_mix<mix4::y, mix4::z, mix4::c, mix4::c>(v11_v21_v01, v12_v01_v11);
		const vector4d v10v21_v20v01_v00v11 = vector_mul(v10_v20_v00, v21_v01_v11);

		const vector4d v20_v00_v01 = vector_mix<mix4::y, mix4::z, mix4::c, mix4::c>(v10_v20_v00, v11_v21_v01);
		const vector4d v11_v21_v10 = vector_mix<mix4::x, mix4::y, mix4::a, mix4::c>(v11_v21_v01, v10_v20_v00);

		vector4d z_axis = vector_neg_mul_sub(v20_v00_v01, v11_v21_v10, v10v21_v20v01_v00v11);

		const vector4d o00_o00_o10_o10 = vector_mix<mix4::x, mix4::x, mix4::a, mix4::a>(x_axis, y_axis);
		const vector4d o00_o10_o20 = vector_mix<mix4::x, mix4::z, mix4::a, mix4::a>(o00_o00_o10_o10, z_axis);

		const scalard det = vector_dot3_as_scalar(o00_o10_o20, input.x_axis);
		if (scalar_cast(scalar_abs(det)) < threshold)
			return fallback;

		const scalard inv_det_s = scalar_reciprocal(det);
		const vector4d inv_det = vector_set(inv_det_s);

		x_axis = vector_mul(x_axis, inv_det);
		y_axis = vector_mul(y_axis, inv_det);
		z_axis = vector_mul(z_axis, inv_det);

		// Invert the translation
		const vector4d tmp0 = vector_mul(vector_dup_z(input.w_axis), z_axis);
		const vector4d tmp1 = vector_mul_add(vector_dup_y(input.w_axis), y_axis, tmp0);
		vector4d w_axis = vector_neg(vector_mul_add(vector_dup_x(input.w_axis), x_axis, tmp1));

		return matrix3x4d{ x_axis, y_axis, z_axis, w_axis };
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the determinant of the 3x3 rotation/scale part of the input 3x4 matrix.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline scalard RTM_SIMD_CALL matrix_determinant(matrix3x4d_arg0 input) RTM_NO_EXCEPT
	{
		const vector4d v00_v01_v10_v11 = vector_mix<mix4::x, mix4::y, mix4::a, mix4::b>(input.x_axis, input.y_axis);
		const vector4d v02_v03_v12_v13 = vector_mix<mix4::z, mix4::w, mix4::c, mix4::d>(input.x_axis, input.y_axis);

		const vector4d v00_v10_v20_v22 = vector_mix<mix4::x, mix4::z, mix4::a, mix4::c>(v00_v01_v10_v11, input.z_axis);
		const vector4d v01_v11_v21_v23 = vector_mix<mix4::y, mix4::w, mix4::b, mix4::d>(v00_v01_v10_v11, input.z_axis);
		const vector4d v02_v12_v22_v22 = vector_mix<mix4::x, mix4::z, mix4::c, mix4::c>(v02_v03_v12_v13, input.z_axis);

		const vector4d v11_v21_v01 = vector_mix<mix4::y, mix4::z, mix4::b, mix4::c>(v01_v11_v21_v23, input.x_axis);
		const vector4d v22_v02_v12_v10 = vector_mix<mix4::z, mix4::x, mix4::c, mix4::a>(v02_v12_v22_v22, input.y_axis);
		const vector4d v11v22_v21v02_v01v12 = vector_mul(v11_v21_v01, v22_v02_v12_v10);

		const vector4d v01_v02_v11_v12 = vector_mix<mix4::y, mix4::z, mix4::b, mix4::c>(input.x_axis, input.y_axis);

		const vector4d v12_v01_v11 = vector_mix<mix4::w, mix4::x, mix4::b, mix4::c>(v01_v02_v11_v12, v01_v11_v21_v23);
		const vector4d v21_v22_v02 = vector_mix<mix4::y, mix4::z, mix4::b, mix4::c>(input.z_axis, v22_v02_v12_v10);

		vector4d x_axis = vector_neg_mul_sub(v12_v01_v11, v21_v22_v02, v11v22_v21v02_v01v12);

		const vector4d v20_v00_v10_v22 = vector_mix<mix4::z, mix4::x, mix4::d, mix4::a>(v00_v10_v20_v22, v22_v02_v12_v10);
		const vector4d v12_v22_v02 = vector_mix<mix4::y, mix4::z, mix4::c, mix4::c>(v02_v12_v22_v22, v21_v22_v02);
		const vector4d v20v12_v00v22_v10v02 = vector_mul(v20_v00_v10_v22, v12_v22_v02);

		const vector4d v10_v02_v00 = vector_mix<mix4::w, mix4::y, mix4::b, mix4::c>(v22_v02_v12_v10, v20_v00_v10_v22);
		const vector4d v22_v20_v12 = vector_mix<mix4::w, mix4::x, mix4::a, mix4::c>(v20_v00_v10_v22, v12_v22_v02);

		vector4d y_axis = vector_neg_mul_sub(v10_v02_v00, v22_v20_v12, v20v12_v00v22_v10v02);

		const vector4d v10_v20_v00 = vector_mix<mix4::z, mix4::x, mix4::c, mix4::c>(v20_v00_v10_v22, v10_v02_v00);
		const vector4d v21_v01_v11 = vector_mix<mix4::y, mix4::z, mix4::c, mix4::c>(v11_v21_v01, v12_v01_v11);
		const vector4d v10v21_v20v01_v00v11 = vector_mul(v10_v20_v00, v21_v01_v11);

		const vector4d v20_v00_v01 = vector_mix<mix4::y, mix4::z, mix4::c, mix4::c>(v10_v20_v00, v11_v21_v01);
		const vector4d v11_v21_v10 = vector_mix<mix4::x, mix4::y, mix4::a, mix4::c>(v11_v21_v01, v10_v20_v00);

		vector4d z_axis = vector_neg_mul_sub(v20_v00_v01, v11_v21_v10, v10v21_v20v01_v00v11);

		const vector4d o00_o00_o10_o10 = vector_mix<mix4::x, mix4::x, mix4::a, mix4::a>(x_axis, y_axis);
		const vector4d o00_o10_o20 = vector_mix<mix4::x, mix4::z, mix4::a, mix4::a>(o00_o00_o10_o10, z_axis);

		return vector_dot3_as_scalar(o00_o10_o20, input.x_axis);
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the minor of the 3x3 rotation/scale part of the input 3x4 matrix.
	// See: https://en.wikipedia.org/wiki/Minor_(linear_algebra)
	// The minor is the determinant of the sub-matrix input when the specified
	// row and column are removed.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline scalard RTM_SIMD_CALL matrix_minor(matrix3x4d_arg0 input, axis3 row, axis3 column) RTM_NO_EXCEPT
	{
		// The minor boils down to calculating the determinant of a 2x2 matrix.
		// det([a, b], [c, d]) = (a * d) - (b * c)

		// Find which two rows we need.
		vector4d row0;
		vector4d row1;
		if (row == axis3::x)
		{
			row0 = input.y_axis;
			row1 = input.z_axis;
		}
		else if (row == axis3::y)
		{
			row0 = input.x_axis;
			row1 = input.z_axis;
		}
		else
		{
			row0 = input.x_axis;
			row1 = input.y_axis;
		}

		// Because our input is a 3x3 matrix, there are only a few possibilities for the 2x2 part:
		// row0 = [x0, y0, z0]
		// row1 = [x1, y1, z1]
		// det([x0, y0], [x1, y1]) = (x0 * y1) - (y0 * x1) (z removed)
		// det([x0, z0], [x1, z1]) = (x0 * z1) - (z0 * x1) (y removed)
		// det([y0, z0], [y1, z1]) = (y0 * z1) - (z0 * y1) (x removed)
		// det([column0, column1], [column2, column3]) = (column0 * column3) - (column1 * column2)

		// For performance reasons, we can compute all three determinants at the same time.
		const vector4d column0 = vector_mix<mix4::x, mix4::x, mix4::y, mix4::y>(row0, row0);
		const vector4d column1 = vector_mix<mix4::y, mix4::z, mix4::z, mix4::z>(row0, row0);
		const vector4d column2 = vector_mix<mix4::x, mix4::x, mix4::y, mix4::y>(row1, row1);
		const vector4d column3 = vector_mix<mix4::y, mix4::z, mix4::z, mix4::z>(row1, row1);

		const vector4d determinants = vector_neg_mul_sub(column1, column2, vector_mul(column0, column3));

		// Extract the one we need
		if (column == axis3::x)
			return vector_get_z_as_scalar(determinants);
		else if (column == axis3::y)
			return vector_get_y_as_scalar(determinants);
		else
			return vector_get_x_as_scalar(determinants);
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the cofactor matrix of the 3x3 rotation/scale part of the input 3x4 matrix.
	// See: https://en.wikipedia.org/wiki/Minor_(linear_algebra)#Cofactor_expansion_of_the_determinant
	// Note: The proper way to transform a normal by an affine matrix with non-uniform scale
	// is to multiply the normal with the cofactor matrix of the 3x3 rotation/scale part.
	// See: https://github.com/graphitemaster/normals_revisited
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline matrix3x3d RTM_SIMD_CALL matrix_cofactor(matrix3x4d_arg0 input) RTM_NO_EXCEPT
	{
		const vector4d x_axis = vector_cross3(input.y_axis, input.z_axis);
		const vector4d y_axis = vector_cross3(input.z_axis, input.x_axis);
		const vector4d z_axis = vector_cross3(input.x_axis, input.y_axis);
		return matrix3x3d{ x_axis, y_axis, z_axis };
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the adjugate of the input matrix.
	// See: https://en.wikipedia.org/wiki/Adjugate_matrix
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline matrix3x3d RTM_SIMD_CALL matrix_adjugate(matrix3x4d_arg0 input) RTM_NO_EXCEPT
	{
		return matrix_transpose(matrix_cofactor(input));
	}

	//////////////////////////////////////////////////////////////////////////
	// Removes the 3D scale from a 3x4 affine matrix.
	// Note that if the scaling is 0.0 for a particular axis, the original rotation axis cannot
	// be recovered trivially and no attempt is done to do so. In theory, we could use the other axes
	// to try and recover it.
	// TODO: Implement rotation recovering, perhaps in a separate function and rename this
	// one to matrix_remove_non_zero_scale(..)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline matrix3x4d RTM_SIMD_CALL matrix_remove_scale(matrix3x4d_arg0 input) RTM_NO_EXCEPT
	{
		matrix3x4d result;
		result.x_axis = vector_normalize3(input.x_axis, input.x_axis);
		result.y_axis = vector_normalize3(input.y_axis, input.y_axis);
		result.z_axis = vector_normalize3(input.z_axis, input.z_axis);
		result.w_axis = input.w_axis;
		return result;
	}

	RTM_IMPL_VERSION_NAMESPACE_END
}

RTM_IMPL_FILE_PRAGMA_POP

```

`includes/rtm/matrix3x4f.h`:

```h
#pragma once

////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2017 Nicholas Frechette & Animation Compression Library contributors
// Copyright (c) 2018 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include "rtm/macros.h"
#include "rtm/math.h"
#include "rtm/matrix3x3f.h"
#include "rtm/quatf.h"
#include "rtm/qvsf.h"
#include "rtm/vector4f.h"
#include "rtm/version.h"
#include "rtm/impl/compiler_utils.h"
#include "rtm/impl/matrix_common.h"
#include "rtm/impl/matrix_affine_common.h"

RTM_IMPL_FILE_PRAGMA_PUSH

namespace rtm
{
	RTM_IMPL_VERSION_NAMESPACE_BEGIN

	//////////////////////////////////////////////////////////////////////////
	// Converts a rotation 3x3 matrix into a 3x4 affine matrix.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE matrix3x4f RTM_SIMD_CALL matrix_from_rotation(matrix3x3f_arg0 rotation) RTM_NO_EXCEPT
	{
		return matrix3x4f{ rotation.x_axis, rotation.y_axis, rotation.z_axis, (vector4f)vector_zero() };
	}

	//////////////////////////////////////////////////////////////////////////
	// Converts a translation vector into a 3x4 affine matrix.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE matrix3x4f RTM_SIMD_CALL matrix_from_translation(vector4f_arg0 translation) RTM_NO_EXCEPT
	{
		return matrix3x4f{ vector_set(1.0F, 0.0F, 0.0F, 0.0F), vector_set(0.0F, 1.0F, 0.0F, 0.0F), vector_set(0.0F, 0.0F, 1.0F, 0.0F), translation };
	}

	//////////////////////////////////////////////////////////////////////////
	// Sets a 3x4 affine matrix from a rotation quaternion and translation.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline matrix3x4f RTM_SIMD_CALL matrix_from_qv(quatf_arg0 quat, vector4f_arg1 translation) RTM_NO_EXCEPT
	{
		RTM_ASSERT(quat_is_normalized(quat), "Quaternion is not normalized");

		const float x2 = quat_get_x(quat) + quat_get_x(quat);
		const float y2 = quat_get_y(quat) + quat_get_y(quat);
		const float z2 = quat_get_z(quat) + quat_get_z(quat);
		const float xx = quat_get_x(quat) * x2;
		const float xy = quat_get_x(quat) * y2;
		const float xz = quat_get_x(quat) * z2;
		const float yy = quat_get_y(quat) * y2;
		const float yz = quat_get_y(quat) * z2;
		const float zz = quat_get_z(quat) * z2;
		const float wx = quat_get_w(quat) * x2;
		const float wy = quat_get_w(quat) * y2;
		const float wz = quat_get_w(quat) * z2;

		const vector4f x_axis = vector_set(1.0F - (yy + zz), xy + wz, xz - wy, 0.0F);
		const vector4f y_axis = vector_set(xy - wz, 1.0F - (xx + zz), yz + wx, 0.0F);
		const vector4f z_axis = vector_set(xz + wy, yz - wx, 1.0F - (xx + yy), 0.0F);
		return matrix3x4f{ x_axis, y_axis, z_axis, translation };
	}

	//////////////////////////////////////////////////////////////////////////
	// Converts a QV transform into a 3x4 affine matrix.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline matrix3x4f RTM_SIMD_CALL matrix_from_qv(qvf_arg0 transform) RTM_NO_EXCEPT
	{
		return matrix_from_qv(transform.rotation, transform.translation);
	}

	//////////////////////////////////////////////////////////////////////////
	// Sets a 3x4 affine matrix from a rotation quaternion, translation, and scalar scale.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline matrix3x4f RTM_SIMD_CALL matrix_from_qvs(quatf_arg0 quat, vector4f_arg1 translation, float scale) RTM_NO_EXCEPT
	{
		RTM_ASSERT(quat_is_normalized(quat), "Quaternion is not normalized");

		const float x2 = quat_get_x(quat) + quat_get_x(quat);
		const float y2 = quat_get_y(quat) + quat_get_y(quat);
		const float z2 = quat_get_z(quat) + quat_get_z(quat);
		const float xx = quat_get_x(quat) * x2;
		const float xy = quat_get_x(quat) * y2;
		const float xz = quat_get_x(quat) * z2;
		const float yy = quat_get_y(quat) * y2;
		const float yz = quat_get_y(quat) * z2;
		const float zz = quat_get_z(quat) * z2;
		const float wx = quat_get_w(quat) * x2;
		const float wy = quat_get_w(quat) * y2;
		const float wz = quat_get_w(quat) * z2;

		const scalarf scale_s = scalar_set(scale);

		const vector4f x_axis = vector_mul(vector_set(1.0F - (yy + zz), xy + wz, xz - wy, 0.0F), scale_s);
		const vector4f y_axis = vector_mul(vector_set(xy - wz, 1.0F - (xx + zz), yz + wx, 0.0F), scale_s);
		const vector4f z_axis = vector_mul(vector_set(xz + wy, yz - wx, 1.0F - (xx + yy), 0.0F), scale_s);
		return matrix3x4f{ x_axis, y_axis, z_axis, translation };
	}

	//////////////////////////////////////////////////////////////////////////
	// Converts a QVS transform into a 3x4 affine matrix.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline matrix3x4f RTM_SIMD_CALL matrix_from_qvs(qvsf_arg0 transform) RTM_NO_EXCEPT
	{
		return matrix_from_qvs(transform.rotation, transform.translation_scale, qvs_get_scale(transform));
	}

	//////////////////////////////////////////////////////////////////////////
	// Sets a 3x4 affine matrix from a rotation quaternion, translation, and 3D scale.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline matrix3x4f RTM_SIMD_CALL matrix_from_qvv(quatf_arg0 quat, vector4f_arg1 translation, vector4f_arg2 scale) RTM_NO_EXCEPT
	{
		RTM_ASSERT(quat_is_normalized(quat), "Quaternion is not normalized");

		const float x2 = quat_get_x(quat) + quat_get_x(quat);
		const float y2 = quat_get_y(quat) + quat_get_y(quat);
		const float z2 = quat_get_z(quat) + quat_get_z(quat);
		const float xx = quat_get_x(quat) * x2;
		const float xy = quat_get_x(quat) * y2;
		const float xz = quat_get_x(quat) * z2;
		const float yy = quat_get_y(quat) * y2;
		const float yz = quat_get_y(quat) * z2;
		const float zz = quat_get_z(quat) * z2;
		const float wx = quat_get_w(quat) * x2;
		const float wy = quat_get_w(quat) * y2;
		const float wz = quat_get_w(quat) * z2;

		const scalarf scale_x = vector_get_x_as_scalar(scale);
		const scalarf scale_y = vector_get_y_as_scalar(scale);
		const scalarf scale_z = vector_get_z_as_scalar(scale);

		const vector4f x_axis = vector_mul(vector_set(1.0F - (yy + zz), xy + wz, xz - wy, 0.0F), scale_x);
		const vector4f y_axis = vector_mul(vector_set(xy - wz, 1.0F - (xx + zz), yz + wx, 0.0F), scale_y);
		const vector4f z_axis = vector_mul(vector_set(xz + wy, yz - wx, 1.0F - (xx + yy), 0.0F), scale_z);
		return matrix3x4f{ x_axis, y_axis, z_axis, translation };
	}

	//////////////////////////////////////////////////////////////////////////
	// Converts a QVV transform into a 3x4 affine matrix.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline matrix3x4f RTM_SIMD_CALL matrix_from_qvv(qvvf_arg0 transform) RTM_NO_EXCEPT
	{
		return matrix_from_qvv(transform.rotation, transform.translation, transform.scale);
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the axis pointing in the forward direction of the default coordinate system (Z+).
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr vector4f RTM_SIMD_CALL matrix_get_coord_forward(matrix3x4f_arg0 input) RTM_NO_EXCEPT
	{
		return input.z_axis;
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the axis pointing in the up direction of the default coordinate system (Y+).
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr vector4f RTM_SIMD_CALL matrix_get_coord_up(matrix3x4f_arg0 input) RTM_NO_EXCEPT
	{
		return input.y_axis;
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the axis pointing in the cross direction of the default coordinate system (X+).
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr vector4f RTM_SIMD_CALL matrix_get_coord_cross(matrix3x4f_arg0 input) RTM_NO_EXCEPT
	{
		return input.x_axis;
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the axis holding the position of the default coordinate system (W+).
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr vector4f RTM_SIMD_CALL matrix_get_coord_position(matrix3x4f_arg0 input) RTM_NO_EXCEPT
	{
		return input.w_axis;
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the desired 3x4 affine matrix axis.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr const vector4f& matrix_get_axis(const matrix3x4f& input, axis4 axis) RTM_NO_EXCEPT
	{
		return axis == axis4::x ? input.x_axis : (axis == axis4::y ? input.y_axis : (axis == axis4::z ? input.z_axis : input.w_axis));
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns a new 3x4 matrix where the specified axis has been replaced on the input matrix.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline matrix3x4f RTM_SIMD_CALL matrix_set_axis(matrix3x4f_arg0 input, vector4f_arg5 axis_value, axis4 axis) RTM_NO_EXCEPT
	{
		switch (axis)
		{
			default:
			case axis4::x:	return matrix3x4f{ axis_value, input.y_axis, input.z_axis, input.w_axis };
			case axis4::y:	return matrix3x4f{ input.x_axis, axis_value, input.z_axis, input.w_axis };
			case axis4::z:	return matrix3x4f{ input.x_axis, input.y_axis, axis_value, input.w_axis };
			case axis4::w:	return matrix3x4f{ input.x_axis, input.y_axis, input.z_axis, axis_value };
		}
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the desired 3x4 matrix component from the specified axis.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline rtm_impl::vector4f_vector_get_component RTM_SIMD_CALL matrix_get_component(matrix3x4f_arg0 input, axis4 axis, component3 component) RTM_NO_EXCEPT
	{
		switch (axis)
		{
			default:
			case axis4::x:	return vector_get_component3(input.x_axis, component);
			case axis4::y:	return vector_get_component3(input.y_axis, component);
			case axis4::z:	return vector_get_component3(input.z_axis, component);
			case axis4::w:	return vector_get_component3(input.w_axis, component);
		}
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the desired 3x4 matrix component from the specified axis.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline scalarf RTM_SIMD_CALL matrix_get_component_as_scalar(matrix3x4f_arg0 input, axis4 axis, component3 component) RTM_NO_EXCEPT
	{
		switch (axis)
		{
			default:
			case axis4::x:	return vector_get_component3_as_scalar(input.x_axis, component);
			case axis4::y:	return vector_get_component3_as_scalar(input.y_axis, component);
			case axis4::z:	return vector_get_component3_as_scalar(input.z_axis, component);
			case axis4::w:	return vector_get_component3_as_scalar(input.w_axis, component);
		}
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns a new 3x4 matrix where the specified axis/component has been replaced on the input matrix.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline matrix3x4f RTM_SIMD_CALL matrix_set_component(matrix3x4f_arg0 input, float component_value, axis4 axis, component3 component) RTM_NO_EXCEPT
	{
		switch (axis)
		{
			default:
			case axis4::x:	return matrix3x4f{ vector_set_component3(input.x_axis, component_value, component), input.y_axis, input.z_axis, input.w_axis };
			case axis4::y:	return matrix3x4f{ input.x_axis, vector_set_component3(input.y_axis, component_value, component), input.z_axis, input.w_axis };
			case axis4::z:	return matrix3x4f{ input.x_axis, input.y_axis, vector_set_component3(input.z_axis, component_value, component), input.w_axis };
			case axis4::w:	return matrix3x4f{ input.x_axis, input.y_axis, input.z_axis, vector_set_component3(input.w_axis, component_value, component) };
		}
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Returns a new 3x4 matrix where the specified axis/component has been replaced on the input matrix.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline matrix3x4f RTM_SIMD_CALL matrix_set_component(matrix3x4f_arg0 input, scalarf_arg4 component_value, axis4 axis, component3 component) RTM_NO_EXCEPT
	{
		switch (axis)
		{
			default:
			case axis4::x:	return matrix3x4f{ vector_set_component3(input.x_axis, component_value, component), input.y_axis, input.z_axis, input.w_axis };
			case axis4::y:	return matrix3x4f{ input.x_axis, vector_set_component3(input.y_axis, component_value, component), input.z_axis, input.w_axis };
			case axis4::z:	return matrix3x4f{ input.x_axis, input.y_axis, vector_set_component3(input.z_axis, component_value, component), input.w_axis };
			case axis4::w:	return matrix3x4f{ input.x_axis, input.y_axis, input.z_axis, vector_set_component3(input.w_axis, component_value, component) };
		}
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Converts a 3x4 affine matrix into a rotation quaternion.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline quatf RTM_SIMD_CALL quat_from_matrix(matrix3x4f_arg0 input) RTM_NO_EXCEPT
	{
		return rtm_impl::quat_from_matrix(input.x_axis, input.y_axis, input.z_axis);
	}

	//////////////////////////////////////////////////////////////////////////
	// Multiplies two 3x4 affine matrices.
	// Multiplication order is as follow: local_to_world = matrix_mul(local_to_object, object_to_world)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline matrix3x4f RTM_SIMD_CALL matrix_mul(matrix3x4f_arg0 lhs, matrix3x4f_arg1 rhs) RTM_NO_EXCEPT
	{
		vector4f tmp = vector_mul(vector_dup_x(lhs.x_axis), rhs.x_axis);
		tmp = vector_mul_add(vector_dup_y(lhs.x_axis), rhs.y_axis, tmp);
		tmp = vector_mul_add(vector_dup_z(lhs.x_axis), rhs.z_axis, tmp);
		vector4f x_axis = tmp;

		tmp = vector_mul(vector_dup_x(lhs.y_axis), rhs.x_axis);
		tmp = vector_mul_add(vector_dup_y(lhs.y_axis), rhs.y_axis, tmp);
		tmp = vector_mul_add(vector_dup_z(lhs.y_axis), rhs.z_axis, tmp);
		vector4f y_axis = tmp;

		tmp = vector_mul(vector_dup_x(lhs.z_axis), rhs.x_axis);
		tmp = vector_mul_add(vector_dup_y(lhs.z_axis), rhs.y_axis, tmp);
		tmp = vector_mul_add(vector_dup_z(lhs.z_axis), rhs.z_axis, tmp);
		vector4f z_axis = tmp;

		tmp = vector_mul(vector_dup_x(lhs.w_axis), rhs.x_axis);
		tmp = vector_mul_add(vector_dup_y(lhs.w_axis), rhs.y_axis, tmp);
		tmp = vector_mul_add(vector_dup_z(lhs.w_axis), rhs.z_axis, tmp);
		vector4f w_axis = vector_add(rhs.w_axis, tmp);
		return matrix3x4f{ x_axis, y_axis, z_axis, w_axis };
	}

	//////////////////////////////////////////////////////////////////////////
	// Multiplies a 3x4 affine matrix and a 3D point.
	// Multiplication order is as follow: world_position = matrix_mul(local_position, local_to_world)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4f RTM_SIMD_CALL matrix_mul_point3(vector4f_arg0 point, matrix3x4f_arg0 mtx) RTM_NO_EXCEPT
	{
		vector4f tmp0;
		vector4f tmp1;

		tmp0 = vector_mul(vector_dup_x(point), mtx.x_axis);
		tmp0 = vector_mul_add(vector_dup_y(point), mtx.y_axis, tmp0);
		tmp1 = vector_mul_add(vector_dup_z(point), mtx.z_axis, mtx.w_axis);

		return vector_add(tmp0, tmp1);
	}

	//////////////////////////////////////////////////////////////////////////
	// Multiplies a 3x4 affine matrix and a 3D vector.
	// Multiplication order is as follow: world_position = matrix_mul(local_vector, local_to_world)
	// Note: The proper way to transform a normal by an affine matrix with non-uniform scale
	// is to multiply the normal with the cofactor matrix of the 3x3 rotation/scale part.
	// See: https://github.com/graphitemaster/normals_revisited
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4f RTM_SIMD_CALL matrix_mul_vector3(vector4f_arg0 vec3, matrix3x4f_arg0 mtx) RTM_NO_EXCEPT
	{
		vector4f tmp;

		tmp = vector_mul(vector_dup_x(vec3), mtx.x_axis);
		tmp = vector_mul_add(vector_dup_y(vec3), mtx.y_axis, tmp);
		tmp = vector_mul_add(vector_dup_z(vec3), mtx.z_axis, tmp);

		return tmp;
	}

	//////////////////////////////////////////////////////////////////////////
	// Transposes a 3x4 affine matrix.
	// Note: This transposes the upper 3x3 rotation/scale part of the matrix
	// and it discards the translation. This is because a transposed 4x4 affine
	// matrix is no longer affine due to its last row no longer being [0, 0, 0, 1].
	// The most common usage of an affine transpose operation is to construct the
	// inverse transpose used to transform normal bi-vectors.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE matrix3x3f RTM_SIMD_CALL matrix_transpose(matrix3x4f_arg0 input) RTM_NO_EXCEPT
	{
		vector4f x_axis;
		vector4f y_axis;
		vector4f z_axis;
		RTM_MATRIXF_TRANSPOSE_3X3(input.x_axis, input.y_axis, input.z_axis, x_axis, y_axis, z_axis);
		return matrix3x3f{ x_axis, y_axis, z_axis };
	}

	//////////////////////////////////////////////////////////////////////////
	// Inverses a 3x4 affine matrix.
	// If the input matrix is not invertible, the result is undefined.
	// For a safe alternative, supply a fallback value and a threshold.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline matrix3x4f RTM_SIMD_CALL matrix_inverse(matrix3x4f_arg0 input) RTM_NO_EXCEPT
	{
		// Invert the 3x3 portion of the matrix that contains the rotation and 3D scale
		const vector4f v00_v01_v10_v11 = vector_mix<mix4::x, mix4::y, mix4::a, mix4::b>(input.x_axis, input.y_axis);
		const vector4f v02_v03_v12_v13 = vector_mix<mix4::z, mix4::w, mix4::c, mix4::d>(input.x_axis, input.y_axis);

		const vector4f v00_v10_v20_v22 = vector_mix<mix4::x, mix4::z, mix4::a, mix4::c>(v00_v01_v10_v11, input.z_axis);
		const vector4f v01_v11_v21_v23 = vector_mix<mix4::y, mix4::w, mix4::b, mix4::d>(v00_v01_v10_v11, input.z_axis);
		const vector4f v02_v12_v22_v22 = vector_mix<mix4::x, mix4::z, mix4::c, mix4::c>(v02_v03_v12_v13, input.z_axis);

		const vector4f v11_v21_v01 = vector_mix<mix4::y, mix4::z, mix4::b, mix4::c>(v01_v11_v21_v23, input.x_axis);
		const vector4f v22_v02_v12_v10 = vector_mix<mix4::z, mix4::x, mix4::c, mix4::a>(v02_v12_v22_v22, input.y_axis);
		const vector4f v11v22_v21v02_v01v12 = vector_mul(v11_v21_v01, v22_v02_v12_v10);

		const vector4f v01_v02_v11_v12 = vector_mix<mix4::y, mix4::z, mix4::b, mix4::c>(input.x_axis, input.y_axis);

		const vector4f v12_v01_v11 = vector_mix<mix4::w, mix4::x, mix4::b, mix4::c>(v01_v02_v11_v12, v01_v11_v21_v23);
		const vector4f v21_v22_v02 = vector_mix<mix4::y, mix4::z, mix4::b, mix4::c>(input.z_axis, v22_v02_v12_v10);

		vector4f x_axis = vector_neg_mul_sub(v12_v01_v11, v21_v22_v02, v11v22_v21v02_v01v12);

		const vector4f v20_v00_v10_v22 = vector_mix<mix4::z, mix4::x, mix4::d, mix4::a>(v00_v10_v20_v22, v22_v02_v12_v10);
		const vector4f v12_v22_v02 = vector_mix<mix4::y, mix4::z, mix4::c, mix4::c>(v02_v12_v22_v22, v21_v22_v02);
		const vector4f v20v12_v00v22_v10v02 = vector_mul(v20_v00_v10_v22, v12_v22_v02);

		const vector4f v10_v02_v00 = vector_mix<mix4::w, mix4::y, mix4::b, mix4::c>(v22_v02_v12_v10, v20_v00_v10_v22);
		const vector4f v22_v20_v12 = vector_mix<mix4::w, mix4::x, mix4::a, mix4::c>(v20_v00_v10_v22, v12_v22_v02);

		vector4f y_axis = vector_neg_mul_sub(v10_v02_v00, v22_v20_v12, v20v12_v00v22_v10v02);

		const vector4f v10_v20_v00 = vector_mix<mix4::z, mix4::x, mix4::c, mix4::c>(v20_v00_v10_v22, v10_v02_v00);
		const vector4f v21_v01_v11 = vector_mix<mix4::y, mix4::z, mix4::c, mix4::c>(v11_v21_v01, v12_v01_v11);
		const vector4f v10v21_v20v01_v00v11 = vector_mul(v10_v20_v00, v21_v01_v11);

		const vector4f v20_v00_v01 = vector_mix<mix4::y, mix4::z, mix4::c, mix4::c>(v10_v20_v00, v11_v21_v01);
		const vector4f v11_v21_v10 = vector_mix<mix4::x, mix4::y, mix4::a, mix4::c>(v11_v21_v01, v10_v20_v00);

		vector4f z_axis = vector_neg_mul_sub(v20_v00_v01, v11_v21_v10, v10v21_v20v01_v00v11);

		const vector4f o00_o00_o10_o10 = vector_mix<mix4::x, mix4::x, mix4::a, mix4::a>(x_axis, y_axis);
		const vector4f o00_o10_o20 = vector_mix<mix4::x, mix4::z, mix4::a, mix4::a>(o00_o00_o10_o10, z_axis);

		const scalarf det = vector_dot3_as_scalar(o00_o10_o20, input.x_axis);
		const scalarf inv_det_s = scalar_reciprocal(det);
		const vector4f inv_det = vector_set(inv_det_s);

		x_axis = vector_mul(x_axis, inv_det);
		y_axis = vector_mul(y_axis, inv_det);
		z_axis = vector_mul(z_axis, inv_det);

		// Invert the translation
		const vector4f tmp0 = vector_mul(vector_dup_z(input.w_axis), z_axis);
		const vector4f tmp1 = vector_mul_add(vector_dup_y(input.w_axis), y_axis, tmp0);
		vector4f w_axis = vector_neg(vector_mul_add(vector_dup_x(input.w_axis), x_axis, tmp1));

		return matrix3x4f{ x_axis, y_axis, z_axis, w_axis };
	}

	//////////////////////////////////////////////////////////////////////////
	// Inverses a 3x4 affine matrix.
	// If the input matrix has a determinant whose absolute value is below the supplied threshold, the
	// fall back value is returned instead.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline matrix3x4f RTM_SIMD_CALL matrix_inverse(matrix3x4f_arg0 input, matrix3x4f_arg1 fallback, float threshold = 1.0E-8F) RTM_NO_EXCEPT
	{
		// Invert the 3x3 portion of the matrix that contains the rotation and 3D scale
		const vector4f v00_v01_v10_v11 = vector_mix<mix4::x, mix4::y, mix4::a, mix4::b>(input.x_axis, input.y_axis);
		const vector4f v02_v03_v12_v13 = vector_mix<mix4::z, mix4::w, mix4::c, mix4::d>(input.x_axis, input.y_axis);

		const vector4f v00_v10_v20_v22 = vector_mix<mix4::x, mix4::z, mix4::a, mix4::c>(v00_v01_v10_v11, input.z_axis);
		const vector4f v01_v11_v21_v23 = vector_mix<mix4::y, mix4::w, mix4::b, mix4::d>(v00_v01_v10_v11, input.z_axis);
		const vector4f v02_v12_v22_v22 = vector_mix<mix4::x, mix4::z, mix4::c, mix4::c>(v02_v03_v12_v13, input.z_axis);

		const vector4f v11_v21_v01 = vector_mix<mix4::y, mix4::z, mix4::b, mix4::c>(v01_v11_v21_v23, input.x_axis);
		const vector4f v22_v02_v12_v10 = vector_mix<mix4::z, mix4::x, mix4::c, mix4::a>(v02_v12_v22_v22, input.y_axis);
		const vector4f v11v22_v21v02_v01v12 = vector_mul(v11_v21_v01, v22_v02_v12_v10);

		const vector4f v01_v02_v11_v12 = vector_mix<mix4::y, mix4::z, mix4::b, mix4::c>(input.x_axis, input.y_axis);

		const vector4f v12_v01_v11 = vector_mix<mix4::w, mix4::x, mix4::b, mix4::c>(v01_v02_v11_v12, v01_v11_v21_v23);
		const vector4f v21_v22_v02 = vector_mix<mix4::y, mix4::z, mix4::b, mix4::c>(input.z_axis, v22_v02_v12_v10);

		vector4f x_axis = vector_neg_mul_sub(v12_v01_v11, v21_v22_v02, v11v22_v21v02_v01v12);

		const vector4f v20_v00_v10_v22 = vector_mix<mix4::z, mix4::x, mix4::d, mix4::a>(v00_v10_v20_v22, v22_v02_v12_v10);
		const vector4f v12_v22_v02 = vector_mix<mix4::y, mix4::z, mix4::c, mix4::c>(v02_v12_v22_v22, v21_v22_v02);
		const vector4f v20v12_v00v22_v10v02 = vector_mul(v20_v00_v10_v22, v12_v22_v02);

		const vector4f v10_v02_v00 = vector_mix<mix4::w, mix4::y, mix4::b, mix4::c>(v22_v02_v12_v10, v20_v00_v10_v22);
		const vector4f v22_v20_v12 = vector_mix<mix4::w, mix4::x, mix4::a, mix4::c>(v20_v00_v10_v22, v12_v22_v02);

		vector4f y_axis = vector_neg_mul_sub(v10_v02_v00, v22_v20_v12, v20v12_v00v22_v10v02);

		const vector4f v10_v20_v00 = vector_mix<mix4::z, mix4::x, mix4::c, mix4::c>(v20_v00_v10_v22, v10_v02_v00);
		const vector4f v21_v01_v11 = vector_mix<mix4::y, mix4::z, mix4::c, mix4::c>(v11_v21_v01, v12_v01_v11);
		const vector4f v10v21_v20v01_v00v11 = vector_mul(v10_v20_v00, v21_v01_v11);

		const vector4f v20_v00_v01 = vector_mix<mix4::y, mix4::z, mix4::c, mix4::c>(v10_v20_v00, v11_v21_v01);
		const vector4f v11_v21_v10 = vector_mix<mix4::x, mix4::y, mix4::a, mix4::c>(v11_v21_v01, v10_v20_v00);

		vector4f z_axis = vector_neg_mul_sub(v20_v00_v01, v11_v21_v10, v10v21_v20v01_v00v11);

		const vector4f o00_o00_o10_o10 = vector_mix<mix4::x, mix4::x, mix4::a, mix4::a>(x_axis, y_axis);
		const vector4f o00_o10_o20 = vector_mix<mix4::x, mix4::z, mix4::a, mix4::a>(o00_o00_o10_o10, z_axis);

		const scalarf det = vector_dot3_as_scalar(o00_o10_o20, input.x_axis);
		if (scalar_cast(scalar_abs(det)) < threshold)
			return fallback;

		const scalarf inv_det_s = scalar_reciprocal(det);
		const vector4f inv_det = vector_set(inv_det_s);

		x_axis = vector_mul(x_axis, inv_det);
		y_axis = vector_mul(y_axis, inv_det);
		z_axis = vector_mul(z_axis, inv_det);

		// Invert the translation
		const vector4f tmp0 = vector_mul(vector_dup_z(input.w_axis), z_axis);
		const vector4f tmp1 = vector_mul_add(vector_dup_y(input.w_axis), y_axis, tmp0);
		vector4f w_axis = vector_neg(vector_mul_add(vector_dup_x(input.w_axis), x_axis, tmp1));

		return matrix3x4f{ x_axis, y_axis, z_axis, w_axis };
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the determinant of the 3x3 rotation/scale part of the input 3x4 matrix.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline scalarf RTM_SIMD_CALL matrix_determinant(matrix3x4f_arg0 input) RTM_NO_EXCEPT
	{
		const vector4f v00_v01_v10_v11 = vector_mix<mix4::x, mix4::y, mix4::a, mix4::b>(input.x_axis, input.y_axis);
		const vector4f v02_v03_v12_v13 = vector_mix<mix4::z, mix4::w, mix4::c, mix4::d>(input.x_axis, input.y_axis);

		const vector4f v00_v10_v20_v22 = vector_mix<mix4::x, mix4::z, mix4::a, mix4::c>(v00_v01_v10_v11, input.z_axis);
		const vector4f v01_v11_v21_v23 = vector_mix<mix4::y, mix4::w, mix4::b, mix4::d>(v00_v01_v10_v11, input.z_axis);
		const vector4f v02_v12_v22_v22 = vector_mix<mix4::x, mix4::z, mix4::c, mix4::c>(v02_v03_v12_v13, input.z_axis);

		const vector4f v11_v21_v01 = vector_mix<mix4::y, mix4::z, mix4::b, mix4::c>(v01_v11_v21_v23, input.x_axis);
		const vector4f v22_v02_v12_v10 = vector_mix<mix4::z, mix4::x, mix4::c, mix4::a>(v02_v12_v22_v22, input.y_axis);
		const vector4f v11v22_v21v02_v01v12 = vector_mul(v11_v21_v01, v22_v02_v12_v10);

		const vector4f v01_v02_v11_v12 = vector_mix<mix4::y, mix4::z, mix4::b, mix4::c>(input.x_axis, input.y_axis);

		const vector4f v12_v01_v11 = vector_mix<mix4::w, mix4::x, mix4::b, mix4::c>(v01_v02_v11_v12, v01_v11_v21_v23);
		const vector4f v21_v22_v02 = vector_mix<mix4::y, mix4::z, mix4::b, mix4::c>(input.z_axis, v22_v02_v12_v10);

		vector4f x_axis = vector_neg_mul_sub(v12_v01_v11, v21_v22_v02, v11v22_v21v02_v01v12);

		const vector4f v20_v00_v10_v22 = vector_mix<mix4::z, mix4::x, mix4::d, mix4::a>(v00_v10_v20_v22, v22_v02_v12_v10);
		const vector4f v12_v22_v02 = vector_mix<mix4::y, mix4::z, mix4::c, mix4::c>(v02_v12_v22_v22, v21_v22_v02);
		const vector4f v20v12_v00v22_v10v02 = vector_mul(v20_v00_v10_v22, v12_v22_v02);

		const vector4f v10_v02_v00 = vector_mix<mix4::w, mix4::y, mix4::b, mix4::c>(v22_v02_v12_v10, v20_v00_v10_v22);
		const vector4f v22_v20_v12 = vector_mix<mix4::w, mix4::x, mix4::a, mix4::c>(v20_v00_v10_v22, v12_v22_v02);

		vector4f y_axis = vector_neg_mul_sub(v10_v02_v00, v22_v20_v12, v20v12_v00v22_v10v02);

		const vector4f v10_v20_v00 = vector_mix<mix4::z, mix4::x, mix4::c, mix4::c>(v20_v00_v10_v22, v10_v02_v00);
		const vector4f v21_v01_v11 = vector_mix<mix4::y, mix4::z, mix4::c, mix4::c>(v11_v21_v01, v12_v01_v11);
		const vector4f v10v21_v20v01_v00v11 = vector_mul(v10_v20_v00, v21_v01_v11);

		const vector4f v20_v00_v01 = vector_mix<mix4::y, mix4::z, mix4::c, mix4::c>(v10_v20_v00, v11_v21_v01);
		const vector4f v11_v21_v10 = vector_mix<mix4::x, mix4::y, mix4::a, mix4::c>(v11_v21_v01, v10_v20_v00);

		vector4f z_axis = vector_neg_mul_sub(v20_v00_v01, v11_v21_v10, v10v21_v20v01_v00v11);

		const vector4f o00_o00_o10_o10 = vector_mix<mix4::x, mix4::x, mix4::a, mix4::a>(x_axis, y_axis);
		const vector4f o00_o10_o20 = vector_mix<mix4::x, mix4::z, mix4::a, mix4::a>(o00_o00_o10_o10, z_axis);

		return vector_dot3_as_scalar(o00_o10_o20, input.x_axis);
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the minor of the 3x3 rotation/scale part of the input 3x4 matrix.
	// See: https://en.wikipedia.org/wiki/Minor_(linear_algebra)
	// The minor is the determinant of the sub-matrix input when the specified
	// row and column are removed.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline scalarf RTM_SIMD_CALL matrix_minor(matrix3x4f_arg0 input, axis3 row, axis3 column) RTM_NO_EXCEPT
	{
		// The minor boils down to calculating the determinant of a 2x2 matrix.
		// det([a, b], [c, d]) = (a * d) - (b * c)

		// Find which two rows we need.
		vector4f row0;
		vector4f row1;
		if (row == axis3::x)
		{
			row0 = input.y_axis;
			row1 = input.z_axis;
		}
		else if (row == axis3::y)
		{
			row0 = input.x_axis;
			row1 = input.z_axis;
		}
		else
		{
			row0 = input.x_axis;
			row1 = input.y_axis;
		}

		// Because our input is a 3x3 matrix, there are only a few possibilities for the 2x2 part:
		// row0 = [x0, y0, z0]
		// row1 = [x1, y1, z1]
		// det([x0, y0], [x1, y1]) = (x0 * y1) - (y0 * x1) (z removed)
		// det([x0, z0], [x1, z1]) = (x0 * z1) - (z0 * x1) (y removed)
		// det([y0, z0], [y1, z1]) = (y0 * z1) - (z0 * y1) (x removed)
		// det([column0, column1], [column2, column3]) = (column0 * column3) - (column1 * column2)

		// For performance reasons, we can compute all three determinants at the same time.
		const vector4f column0 = vector_mix<mix4::x, mix4::x, mix4::y, mix4::y>(row0, row0);
		const vector4f column1 = vector_mix<mix4::y, mix4::z, mix4::z, mix4::z>(row0, row0);
		const vector4f column2 = vector_mix<mix4::x, mix4::x, mix4::y, mix4::y>(row1, row1);
		const vector4f column3 = vector_mix<mix4::y, mix4::z, mix4::z, mix4::z>(row1, row1);

		const vector4f determinants = vector_neg_mul_sub(column1, column2, vector_mul(column0, column3));

		// Extract the one we need
		if (column == axis3::x)
			return vector_get_z_as_scalar(determinants);
		else if (column == axis3::y)
			return vector_get_y_as_scalar(determinants);
		else
			return vector_get_x_as_scalar(determinants);
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the cofactor matrix of the 3x3 rotation/scale part of the input 3x4 matrix.
	// See: https://en.wikipedia.org/wiki/Minor_(linear_algebra)#Cofactor_expansion_of_the_determinant
	// Note: The proper way to transform a normal by an affine matrix with non-uniform scale
	// is to multiply the normal with the cofactor matrix of the 3x3 rotation/scale part.
	// See: https://github.com/graphitemaster/normals_revisited
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline matrix3x3f RTM_SIMD_CALL matrix_cofactor(matrix3x4f_arg0 input) RTM_NO_EXCEPT
	{
		const vector4f x_axis = vector_cross3(input.y_axis, input.z_axis);
		const vector4f y_axis = vector_cross3(input.z_axis, input.x_axis);
		const vector4f z_axis = vector_cross3(input.x_axis, input.y_axis);
		return matrix3x3f{ x_axis, y_axis, z_axis };
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the adjugate of the input matrix.
	// See: https://en.wikipedia.org/wiki/Adjugate_matrix
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline matrix3x3f RTM_SIMD_CALL matrix_adjugate(matrix3x4f_arg0 input) RTM_NO_EXCEPT
	{
		return matrix_transpose(matrix_cofactor(input));
	}

	//////////////////////////////////////////////////////////////////////////
	// Removes the 3D scale from a 3x4 affine matrix.
	// Note that if the scaling is 0.0 for a particular axis, the original rotation axis cannot
	// be recovered trivially and no attempt is done to do so. In theory, we could use the other axes
	// to try and recover it.
	// TODO: Implement rotation recovering, perhaps in a separate function and rename this
	// one to matrix_remove_non_zero_scale(..)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline matrix3x4f RTM_SIMD_CALL matrix_remove_scale(matrix3x4f_arg0 input) RTM_NO_EXCEPT
	{
		matrix3x4f result;
		result.x_axis = vector_normalize3(input.x_axis, input.x_axis);
		result.y_axis = vector_normalize3(input.y_axis, input.y_axis);
		result.z_axis = vector_normalize3(input.z_axis, input.z_axis);
		result.w_axis = input.w_axis;
		return result;
	}

	RTM_IMPL_VERSION_NAMESPACE_END
}

RTM_IMPL_FILE_PRAGMA_POP

```

`includes/rtm/matrix4x4d.h`:

```h
#pragma once

////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2018 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include "rtm/math.h"
#include "rtm/matrix3x3d.h"
#include "rtm/vector4d.h"
#include "rtm/version.h"
#include "rtm/impl/compiler_utils.h"
#include "rtm/impl/matrix_common.h"

RTM_IMPL_FILE_PRAGMA_PUSH

namespace rtm
{
	RTM_IMPL_VERSION_NAMESPACE_BEGIN

	//////////////////////////////////////////////////////////////////////////
	// Returns the axis pointing in the forward direction of the default coordinate system (Z+).
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr vector4d RTM_SIMD_CALL matrix_get_coord_forward(matrix4x4d_arg0 input) RTM_NO_EXCEPT
	{
		return input.z_axis;
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the axis pointing in the up direction of the default coordinate system (Y+).
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr vector4d RTM_SIMD_CALL matrix_get_coord_up(matrix4x4d_arg0 input) RTM_NO_EXCEPT
	{
		return input.y_axis;
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the axis pointing in the cross direction of the default coordinate system (X+).
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr vector4d RTM_SIMD_CALL matrix_get_coord_cross(matrix4x4d_arg0 input) RTM_NO_EXCEPT
	{
		return input.x_axis;
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the axis holding the position of the default coordinate system (W+).
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr vector4d RTM_SIMD_CALL matrix_get_coord_position(matrix4x4d_arg0 input) RTM_NO_EXCEPT
	{
		return input.w_axis;
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the desired 4x4 matrix axis.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr vector4d RTM_SIMD_CALL matrix_get_axis(matrix4x4d_arg0 input, axis4 axis) RTM_NO_EXCEPT
	{
		return axis == axis4::x ? input.x_axis : (axis == axis4::y ? input.y_axis : (axis == axis4::z ? input.z_axis : input.w_axis));
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns a new 4x4 matrix where the specified axis has been replaced on the input matrix.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline matrix4x4d RTM_SIMD_CALL matrix_set_axis(matrix4x4d_arg0 input, vector4d_arg4 axis_value, axis4 axis) RTM_NO_EXCEPT
	{
		switch (axis)
		{
			default:
			case axis4::x:	return matrix4x4d{ axis_value, input.y_axis, input.z_axis, input.w_axis };
			case axis4::y:	return matrix4x4d{ input.x_axis, axis_value, input.z_axis, input.w_axis };
			case axis4::z:	return matrix4x4d{ input.x_axis, input.y_axis, axis_value, input.w_axis };
			case axis4::w:	return matrix4x4d{ input.x_axis, input.y_axis, input.z_axis, axis_value };
		}
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the desired 4x4 matrix component from the specified axis.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline rtm_impl::vector4d_vector_get_component RTM_SIMD_CALL matrix_get_component(matrix4x4d_arg0 input, axis4 axis, component4 component) RTM_NO_EXCEPT
	{
		switch (axis)
		{
			default:
			case axis4::x:	return vector_get_component(input.x_axis, component);
			case axis4::y:	return vector_get_component(input.y_axis, component);
			case axis4::z:	return vector_get_component(input.z_axis, component);
			case axis4::w:	return vector_get_component(input.w_axis, component);
		}
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the desired 4x4 matrix component from the specified axis.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline scalard RTM_SIMD_CALL matrix_get_component_as_scalar(matrix4x4d_arg0 input, axis4 axis, component4 component) RTM_NO_EXCEPT
	{
		switch (axis)
		{
			default:
			case axis4::x:	return vector_get_component_as_scalar(input.x_axis, component);
			case axis4::y:	return vector_get_component_as_scalar(input.y_axis, component);
			case axis4::z:	return vector_get_component_as_scalar(input.z_axis, component);
			case axis4::w:	return vector_get_component_as_scalar(input.w_axis, component);
		}
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns a new 4x4 matrix where the specified axis/component has been replaced on the input matrix.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline matrix4x4d RTM_SIMD_CALL matrix_set_component(matrix4x4d_arg0 input, double component_value, axis4 axis, component4 component) RTM_NO_EXCEPT
	{
		switch (axis)
		{
			default:
			case axis4::x:	return matrix4x4d{ vector_set_component(input.x_axis, component_value, component), input.y_axis, input.z_axis, input.w_axis };
			case axis4::y:	return matrix4x4d{ input.x_axis, vector_set_component(input.y_axis, component_value, component), input.z_axis, input.w_axis };
			case axis4::z:	return matrix4x4d{ input.x_axis, input.y_axis, vector_set_component(input.z_axis, component_value, component), input.w_axis };
			case axis4::w:	return matrix4x4d{ input.x_axis, input.y_axis, input.z_axis, vector_set_component(input.w_axis, component_value, component) };
		}
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Returns a new 4x4 matrix where the specified axis/component has been replaced on the input matrix.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline matrix4x4d RTM_SIMD_CALL matrix_set_component(matrix4x4d_arg0 input, scalard_arg0 component_value, axis4 axis, component4 component) RTM_NO_EXCEPT
	{
		switch (axis)
		{
			default:
			case axis4::x:	return matrix4x4d{ vector_set_component(input.x_axis, component_value, component), input.y_axis, input.z_axis, input.w_axis };
			case axis4::y:	return matrix4x4d{ input.x_axis, vector_set_component(input.y_axis, component_value, component), input.z_axis, input.w_axis };
			case axis4::z:	return matrix4x4d{ input.x_axis, input.y_axis, vector_set_component(input.z_axis, component_value, component), input.w_axis };
			case axis4::w:	return matrix4x4d{ input.x_axis, input.y_axis, input.z_axis, vector_set_component(input.w_axis, component_value, component) };
		}
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Multiplies two 4x4 matrices.
	// Multiplication order is as follow: local_to_world = matrix_mul(local_to_object, object_to_world)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline matrix4x4d RTM_SIMD_CALL matrix_mul(matrix4x4d_arg0 lhs, matrix4x4d_arg1 rhs) RTM_NO_EXCEPT
	{
		vector4d tmp = vector_mul(vector_dup_x(lhs.x_axis), rhs.x_axis);
		tmp = vector_mul_add(vector_dup_y(lhs.x_axis), rhs.y_axis, tmp);
		tmp = vector_mul_add(vector_dup_z(lhs.x_axis), rhs.z_axis, tmp);
		tmp = vector_mul_add(vector_dup_w(lhs.x_axis), rhs.w_axis, tmp);
		vector4d x_axis = tmp;

		tmp = vector_mul(vector_dup_x(lhs.y_axis), rhs.x_axis);
		tmp = vector_mul_add(vector_dup_y(lhs.y_axis), rhs.y_axis, tmp);
		tmp = vector_mul_add(vector_dup_z(lhs.y_axis), rhs.z_axis, tmp);
		tmp = vector_mul_add(vector_dup_w(lhs.y_axis), rhs.w_axis, tmp);
		vector4d y_axis = tmp;

		tmp = vector_mul(vector_dup_x(lhs.z_axis), rhs.x_axis);
		tmp = vector_mul_add(vector_dup_y(lhs.z_axis), rhs.y_axis, tmp);
		tmp = vector_mul_add(vector_dup_z(lhs.z_axis), rhs.z_axis, tmp);
		tmp = vector_mul_add(vector_dup_w(lhs.z_axis), rhs.w_axis, tmp);
		vector4d z_axis = tmp;

		tmp = vector_mul(vector_dup_x(lhs.w_axis), rhs.x_axis);
		tmp = vector_mul_add(vector_dup_y(lhs.w_axis), rhs.y_axis, tmp);
		tmp = vector_mul_add(vector_dup_z(lhs.w_axis), rhs.z_axis, tmp);
		tmp = vector_mul_add(vector_dup_w(lhs.w_axis), rhs.w_axis, tmp);
		vector4d w_axis = tmp;

		return matrix4x4d{ x_axis, y_axis, z_axis, w_axis };
	}

	//////////////////////////////////////////////////////////////////////////
	// Multiplies a 4x4 matrix and a 4D vector.
	// Multiplication order is as follow: world_position = matrix_mul(local_position, local_to_world)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4d RTM_SIMD_CALL matrix_mul_vector(vector4d_arg0 vec4, matrix4x4d_argn mtx) RTM_NO_EXCEPT
	{
		vector4d tmp;

		tmp = vector_mul(vector_dup_x(vec4), mtx.x_axis);
		tmp = vector_mul_add(vector_dup_y(vec4), mtx.y_axis, tmp);
		tmp = vector_mul_add(vector_dup_z(vec4), mtx.z_axis, tmp);
		tmp = vector_mul_add(vector_dup_w(vec4), mtx.w_axis, tmp);

		return tmp;
	}

	//////////////////////////////////////////////////////////////////////////
	// Transposes a 4x4 matrix.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE matrix4x4d RTM_SIMD_CALL matrix_transpose(matrix4x4d_arg0 input) RTM_NO_EXCEPT
	{
		vector4d tmp0 = vector_mix<mix4::x, mix4::y, mix4::a, mix4::b>(input.x_axis, input.y_axis);
		vector4d tmp1 = vector_mix<mix4::z, mix4::w, mix4::c, mix4::d>(input.x_axis, input.y_axis);
		vector4d tmp2 = vector_mix<mix4::x, mix4::y, mix4::a, mix4::b>(input.z_axis, input.w_axis);
		vector4d tmp3 = vector_mix<mix4::z, mix4::w, mix4::c, mix4::d>(input.z_axis, input.w_axis);

		vector4d x_axis = vector_mix<mix4::x, mix4::z, mix4::a, mix4::c>(tmp0, tmp2);
		vector4d y_axis = vector_mix<mix4::y, mix4::w, mix4::b, mix4::d>(tmp0, tmp2);
		vector4d z_axis = vector_mix<mix4::x, mix4::z, mix4::a, mix4::c>(tmp1, tmp3);
		vector4d w_axis = vector_mix<mix4::y, mix4::w, mix4::b, mix4::d>(tmp1, tmp3);
		return matrix4x4d{ x_axis, y_axis, z_axis, w_axis };
	}

	//////////////////////////////////////////////////////////////////////////
	// Inverses a 4x4 matrix.
	// If the input matrix is not invertible, the result is undefined.
	// For a safe alternative, supply a fallback value and a threshold.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline matrix4x4d RTM_SIMD_CALL matrix_inverse(matrix4x4d_arg0 input) RTM_NO_EXCEPT
	{
		matrix4x4d input_transposed = matrix_transpose(input);

		vector4d v00 = vector_mix<mix4::x, mix4::x, mix4::y, mix4::y>(input_transposed.z_axis, input_transposed.z_axis);
		vector4d v01 = vector_mix<mix4::x, mix4::x, mix4::y, mix4::y>(input_transposed.x_axis, input_transposed.x_axis);
		vector4d v02 = vector_mix<mix4::x, mix4::z, mix4::a, mix4::c>(input_transposed.z_axis, input_transposed.x_axis);
		vector4d v10 = vector_mix<mix4::z, mix4::w, mix4::z, mix4::w>(input_transposed.w_axis, input_transposed.w_axis);
		vector4d v11 = vector_mix<mix4::z, mix4::w, mix4::z, mix4::w>(input_transposed.y_axis, input_transposed.y_axis);
		vector4d v12 = vector_mix<mix4::y, mix4::w, mix4::b, mix4::d>(input_transposed.w_axis, input_transposed.y_axis);

		vector4d d0 = vector_mul(v00, v10);
		vector4d d1 = vector_mul(v01, v11);
		vector4d d2 = vector_mul(v02, v12);

		v00 = vector_mix<mix4::z, mix4::w, mix4::z, mix4::w>(input_transposed.z_axis, input_transposed.z_axis);
		v01 = vector_mix<mix4::z, mix4::w, mix4::z, mix4::w>(input_transposed.x_axis, input_transposed.x_axis);
		v02 = vector_mix<mix4::y, mix4::w, mix4::b, mix4::d>(input_transposed.z_axis, input_transposed.x_axis);
		v10 = vector_mix<mix4::x, mix4::x, mix4::y, mix4::y>(input_transposed.w_axis, input_transposed.w_axis);
		v11 = vector_mix<mix4::x, mix4::x, mix4::y, mix4::y>(input_transposed.y_axis, input_transposed.y_axis);
		v12 = vector_mix<mix4::x, mix4::z, mix4::a, mix4::c>(input_transposed.w_axis, input_transposed.y_axis);

		d0 = vector_neg_mul_sub(v00, v10, d0);
		d1 = vector_neg_mul_sub(v01, v11, d1);
		d2 = vector_neg_mul_sub(v02, v12, d2);

		v00 = vector_mix<mix4::y, mix4::z, mix4::x, mix4::y>(input_transposed.y_axis, input_transposed.y_axis);
		v01 = vector_mix<mix4::z, mix4::x, mix4::y, mix4::x>(input_transposed.x_axis, input_transposed.x_axis);
		v02 = vector_mix<mix4::y, mix4::z, mix4::x, mix4::y>(input_transposed.w_axis, input_transposed.w_axis);
		vector4d v03 = vector_mix<mix4::z, mix4::x, mix4::y, mix4::x>(input_transposed.z_axis, input_transposed.z_axis);
		v10 = vector_mix<mix4::b, mix4::y, mix4::w, mix4::x>(d0, d2);
		v11 = vector_mix<mix4::w, mix4::b, mix4::y, mix4::z>(d0, d2);
		v12 = vector_mix<mix4::d, mix4::y, mix4::w, mix4::x>(d1, d2);
		vector4d v13 = vector_mix<mix4::w, mix4::d, mix4::y, mix4::z>(d1, d2);

		vector4d c0 = vector_mul(v00, v10);
		vector4d c2 = vector_mul(v01, v11);
		vector4d c4 = vector_mul(v02, v12);
		vector4d c6 = vector_mul(v03, v13);

		v00 = vector_mix<mix4::z, mix4::w, mix4::y, mix4::z>(input_transposed.y_axis, input_transposed.y_axis);
		v01 = vector_mix<mix4::w, mix4::z, mix4::w, mix4::y>(input_transposed.x_axis, input_transposed.x_axis);
		v02 = vector_mix<mix4::z, mix4::w, mix4::y, mix4::z>(input_transposed.w_axis, input_transposed.w_axis);
		v03 = vector_mix<mix4::w, mix4::z, mix4::w, mix4::y>(input_transposed.z_axis, input_transposed.z_axis);
		v10 = vector_mix<mix4::w, mix4::x, mix4::y, mix4::a>(d0, d2);
		v11 = vector_mix<mix4::z, mix4::y, mix4::a, mix4::x>(d0, d2);
		v12 = vector_mix<mix4::w, mix4::x, mix4::y, mix4::c>(d1, d2);
		v13 = vector_mix<mix4::z, mix4::y, mix4::c, mix4::x>(d1, d2);

		c0 = vector_neg_mul_sub(v00, v10, c0);
		c2 = vector_neg_mul_sub(v01, v11, c2);
		c4 = vector_neg_mul_sub(v02, v12, c4);
		c6 = vector_neg_mul_sub(v03, v13, c6);

		v00 = vector_mix<mix4::w, mix4::x, mix4::w, mix4::x>(input_transposed.y_axis, input_transposed.y_axis);
		v01 = vector_mix<mix4::y, mix4::w, mix4::x, mix4::z>(input_transposed.x_axis, input_transposed.x_axis);
		v02 = vector_mix<mix4::w, mix4::x, mix4::w, mix4::x>(input_transposed.w_axis, input_transposed.w_axis);
		v03 = vector_mix<mix4::y, mix4::w, mix4::x, mix4::z>(input_transposed.z_axis, input_transposed.z_axis);
		v10 = vector_mix<mix4::z, mix4::b, mix4::a, mix4::z>(d0, d2);
		v11 = vector_mix<mix4::b, mix4::x, mix4::w, mix4::a>(d0, d2);
		v12 = vector_mix<mix4::z, mix4::d, mix4::c, mix4::z>(d1, d2);
		v13 = vector_mix<mix4::d, mix4::x, mix4::w, mix4::c>(d1, d2);

		vector4d c1 = vector_neg_mul_sub(v00, v10, c0);
		c0 = vector_mul_add(v00, v10, c0);
		vector4d c3 = vector_mul_add(v01, v11, c2);
		c2 = vector_neg_mul_sub(v01, v11, c2);
		vector4d c5 = vector_neg_mul_sub(v02, v12, c4);
		c4 = vector_mul_add(v02, v12, c4);
		vector4d c7 = vector_mul_add(v03, v13, c6);
		c6 = vector_neg_mul_sub(v03, v13, c6);

		vector4d x_axis = vector_mix<mix4::x, mix4::b, mix4::z, mix4::d>(c0, c1);
		vector4d y_axis = vector_mix<mix4::x, mix4::b, mix4::z, mix4::d>(c2, c3);
		vector4d z_axis = vector_mix<mix4::x, mix4::b, mix4::z, mix4::d>(c4, c5);
		vector4d w_axis = vector_mix<mix4::x, mix4::b, mix4::z, mix4::d>(c6, c7);

		const scalard det = vector_dot_as_scalar(x_axis, input_transposed.x_axis);
		const scalard inv_det_s = scalar_reciprocal(det);
		vector4d inv_det = vector_set(inv_det_s);

		x_axis = vector_mul(x_axis, inv_det);
		y_axis = vector_mul(y_axis, inv_det);
		z_axis = vector_mul(z_axis, inv_det);
		w_axis = vector_mul(w_axis, inv_det);

		return matrix4x4d{ x_axis, y_axis, z_axis, w_axis };
	}

	//////////////////////////////////////////////////////////////////////////
	// Inverses a 4x4 matrix.
	// If the input matrix has a determinant whose absolute value is below the supplied threshold, the
	// fall back value is returned instead.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline matrix4x4d RTM_SIMD_CALL matrix_inverse(matrix4x4d_arg0 input, matrix4x4d_argn fallback, double threshold = 1.0E-8) RTM_NO_EXCEPT
	{
		matrix4x4d input_transposed = matrix_transpose(input);

		vector4d v00 = vector_mix<mix4::x, mix4::x, mix4::y, mix4::y>(input_transposed.z_axis, input_transposed.z_axis);
		vector4d v01 = vector_mix<mix4::x, mix4::x, mix4::y, mix4::y>(input_transposed.x_axis, input_transposed.x_axis);
		vector4d v02 = vector_mix<mix4::x, mix4::z, mix4::a, mix4::c>(input_transposed.z_axis, input_transposed.x_axis);
		vector4d v10 = vector_mix<mix4::z, mix4::w, mix4::z, mix4::w>(input_transposed.w_axis, input_transposed.w_axis);
		vector4d v11 = vector_mix<mix4::z, mix4::w, mix4::z, mix4::w>(input_transposed.y_axis, input_transposed.y_axis);
		vector4d v12 = vector_mix<mix4::y, mix4::w, mix4::b, mix4::d>(input_transposed.w_axis, input_transposed.y_axis);

		vector4d d0 = vector_mul(v00, v10);
		vector4d d1 = vector_mul(v01, v11);
		vector4d d2 = vector_mul(v02, v12);

		v00 = vector_mix<mix4::z, mix4::w, mix4::z, mix4::w>(input_transposed.z_axis, input_transposed.z_axis);
		v01 = vector_mix<mix4::z, mix4::w, mix4::z, mix4::w>(input_transposed.x_axis, input_transposed.x_axis);
		v02 = vector_mix<mix4::y, mix4::w, mix4::b, mix4::d>(input_transposed.z_axis, input_transposed.x_axis);
		v10 = vector_mix<mix4::x, mix4::x, mix4::y, mix4::y>(input_transposed.w_axis, input_transposed.w_axis);
		v11 = vector_mix<mix4::x, mix4::x, mix4::y, mix4::y>(input_transposed.y_axis, input_transposed.y_axis);
		v12 = vector_mix<mix4::x, mix4::z, mix4::a, mix4::c>(input_transposed.w_axis, input_transposed.y_axis);

		d0 = vector_neg_mul_sub(v00, v10, d0);
		d1 = vector_neg_mul_sub(v01, v11, d1);
		d2 = vector_neg_mul_sub(v02, v12, d2);

		v00 = vector_mix<mix4::y, mix4::z, mix4::x, mix4::y>(input_transposed.y_axis, input_transposed.y_axis);
		v01 = vector_mix<mix4::z, mix4::x, mix4::y, mix4::x>(input_transposed.x_axis, input_transposed.x_axis);
		v02 = vector_mix<mix4::y, mix4::z, mix4::x, mix4::y>(input_transposed.w_axis, input_transposed.w_axis);
		vector4d v03 = vector_mix<mix4::z, mix4::x, mix4::y, mix4::x>(input_transposed.z_axis, input_transposed.z_axis);
		v10 = vector_mix<mix4::b, mix4::y, mix4::w, mix4::x>(d0, d2);
		v11 = vector_mix<mix4::w, mix4::b, mix4::y, mix4::z>(d0, d2);
		v12 = vector_mix<mix4::d, mix4::y, mix4::w, mix4::x>(d1, d2);
		vector4d v13 = vector_mix<mix4::w, mix4::d, mix4::y, mix4::z>(d1, d2);

		vector4d c0 = vector_mul(v00, v10);
		vector4d c2 = vector_mul(v01, v11);
		vector4d c4 = vector_mul(v02, v12);
		vector4d c6 = vector_mul(v03, v13);

		v00 = vector_mix<mix4::z, mix4::w, mix4::y, mix4::z>(input_transposed.y_axis, input_transposed.y_axis);
		v01 = vector_mix<mix4::w, mix4::z, mix4::w, mix4::y>(input_transposed.x_axis, input_transposed.x_axis);
		v02 = vector_mix<mix4::z, mix4::w, mix4::y, mix4::z>(input_transposed.w_axis, input_transposed.w_axis);
		v03 = vector_mix<mix4::w, mix4::z, mix4::w, mix4::y>(input_transposed.z_axis, input_transposed.z_axis);
		v10 = vector_mix<mix4::w, mix4::x, mix4::y, mix4::a>(d0, d2);
		v11 = vector_mix<mix4::z, mix4::y, mix4::a, mix4::x>(d0, d2);
		v12 = vector_mix<mix4::w, mix4::x, mix4::y, mix4::c>(d1, d2);
		v13 = vector_mix<mix4::z, mix4::y, mix4::c, mix4::x>(d1, d2);

		c0 = vector_neg_mul_sub(v00, v10, c0);
		c2 = vector_neg_mul_sub(v01, v11, c2);
		c4 = vector_neg_mul_sub(v02, v12, c4);
		c6 = vector_neg_mul_sub(v03, v13, c6);

		v00 = vector_mix<mix4::w, mix4::x, mix4::w, mix4::x>(input_transposed.y_axis, input_transposed.y_axis);
		v01 = vector_mix<mix4::y, mix4::w, mix4::x, mix4::z>(input_transposed.x_axis, input_transposed.x_axis);
		v02 = vector_mix<mix4::w, mix4::x, mix4::w, mix4::x>(input_transposed.w_axis, input_transposed.w_axis);
		v03 = vector_mix<mix4::y, mix4::w, mix4::x, mix4::z>(input_transposed.z_axis, input_transposed.z_axis);
		v10 = vector_mix<mix4::z, mix4::b, mix4::a, mix4::z>(d0, d2);
		v11 = vector_mix<mix4::b, mix4::x, mix4::w, mix4::a>(d0, d2);
		v12 = vector_mix<mix4::z, mix4::d, mix4::c, mix4::z>(d1, d2);
		v13 = vector_mix<mix4::d, mix4::x, mix4::w, mix4::c>(d1, d2);

		vector4d c1 = vector_neg_mul_sub(v00, v10, c0);
		c0 = vector_mul_add(v00, v10, c0);
		vector4d c3 = vector_mul_add(v01, v11, c2);
		c2 = vector_neg_mul_sub(v01, v11, c2);
		vector4d c5 = vector_neg_mul_sub(v02, v12, c4);
		c4 = vector_mul_add(v02, v12, c4);
		vector4d c7 = vector_mul_add(v03, v13, c6);
		c6 = vector_neg_mul_sub(v03, v13, c6);

		vector4d x_axis = vector_mix<mix4::x, mix4::b, mix4::z, mix4::d>(c0, c1);
		vector4d y_axis = vector_mix<mix4::x, mix4::b, mix4::z, mix4::d>(c2, c3);
		vector4d z_axis = vector_mix<mix4::x, mix4::b, mix4::z, mix4::d>(c4, c5);
		vector4d w_axis = vector_mix<mix4::x, mix4::b, mix4::z, mix4::d>(c6, c7);

		scalard det = vector_dot_as_scalar(x_axis, input_transposed.x_axis);
		if (scalar_cast(scalar_abs(det)) < threshold)
			return fallback;

		scalard inv_det_s = scalar_reciprocal(det);
		vector4d inv_det = vector_set(inv_det_s);

		x_axis = vector_mul(x_axis, inv_det);
		y_axis = vector_mul(y_axis, inv_det);
		z_axis = vector_mul(z_axis, inv_det);
		w_axis = vector_mul(w_axis, inv_det);

		return matrix4x4d{ x_axis, y_axis, z_axis, w_axis };
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the determinant of the input 4x4 matrix.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline scalard RTM_SIMD_CALL matrix_determinant(matrix4x4d_arg0 input) RTM_NO_EXCEPT
	{
		matrix4x4d input_transposed = matrix_transpose(input);

		vector4d v00 = vector_mix<mix4::x, mix4::x, mix4::y, mix4::y>(input_transposed.z_axis, input_transposed.z_axis);
		vector4d v01 = vector_mix<mix4::x, mix4::x, mix4::y, mix4::y>(input_transposed.x_axis, input_transposed.x_axis);
		vector4d v02 = vector_mix<mix4::x, mix4::z, mix4::a, mix4::c>(input_transposed.z_axis, input_transposed.x_axis);
		vector4d v10 = vector_mix<mix4::z, mix4::w, mix4::z, mix4::w>(input_transposed.w_axis, input_transposed.w_axis);
		vector4d v11 = vector_mix<mix4::z, mix4::w, mix4::z, mix4::w>(input_transposed.y_axis, input_transposed.y_axis);
		vector4d v12 = vector_mix<mix4::y, mix4::w, mix4::b, mix4::d>(input_transposed.w_axis, input_transposed.y_axis);

		vector4d d0 = vector_mul(v00, v10);
		vector4d d1 = vector_mul(v01, v11);
		vector4d d2 = vector_mul(v02, v12);

		v00 = vector_mix<mix4::z, mix4::w, mix4::z, mix4::w>(input_transposed.z_axis, input_transposed.z_axis);
		v01 = vector_mix<mix4::z, mix4::w, mix4::z, mix4::w>(input_transposed.x_axis, input_transposed.x_axis);
		v02 = vector_mix<mix4::y, mix4::w, mix4::b, mix4::d>(input_transposed.z_axis, input_transposed.x_axis);
		v10 = vector_mix<mix4::x, mix4::x, mix4::y, mix4::y>(input_transposed.w_axis, input_transposed.w_axis);
		v11 = vector_mix<mix4::x, mix4::x, mix4::y, mix4::y>(input_transposed.y_axis, input_transposed.y_axis);
		v12 = vector_mix<mix4::x, mix4::z, mix4::a, mix4::c>(input_transposed.w_axis, input_transposed.y_axis);

		d0 = vector_neg_mul_sub(v00, v10, d0);
		d1 = vector_neg_mul_sub(v01, v11, d1);
		d2 = vector_neg_mul_sub(v02, v12, d2);

		v00 = vector_mix<mix4::y, mix4::z, mix4::x, mix4::y>(input_transposed.y_axis, input_transposed.y_axis);
		v01 = vector_mix<mix4::z, mix4::x, mix4::y, mix4::x>(input_transposed.x_axis, input_transposed.x_axis);
		v02 = vector_mix<mix4::y, mix4::z, mix4::x, mix4::y>(input_transposed.w_axis, input_transposed.w_axis);
		vector4d v03 = vector_mix<mix4::z, mix4::x, mix4::y, mix4::x>(input_transposed.z_axis, input_transposed.z_axis);
		v10 = vector_mix<mix4::b, mix4::y, mix4::w, mix4::x>(d0, d2);
		v11 = vector_mix<mix4::w, mix4::b, mix4::y, mix4::z>(d0, d2);
		v12 = vector_mix<mix4::d, mix4::y, mix4::w, mix4::x>(d1, d2);
		vector4d v13 = vector_mix<mix4::w, mix4::d, mix4::y, mix4::z>(d1, d2);

		vector4d c0 = vector_mul(v00, v10);
		vector4d c2 = vector_mul(v01, v11);
		vector4d c4 = vector_mul(v02, v12);
		vector4d c6 = vector_mul(v03, v13);

		v00 = vector_mix<mix4::z, mix4::w, mix4::y, mix4::z>(input_transposed.y_axis, input_transposed.y_axis);
		v01 = vector_mix<mix4::w, mix4::z, mix4::w, mix4::y>(input_transposed.x_axis, input_transposed.x_axis);
		v02 = vector_mix<mix4::z, mix4::w, mix4::y, mix4::z>(input_transposed.w_axis, input_transposed.w_axis);
		v03 = vector_mix<mix4::w, mix4::z, mix4::w, mix4::y>(input_transposed.z_axis, input_transposed.z_axis);
		v10 = vector_mix<mix4::w, mix4::x, mix4::y, mix4::a>(d0, d2);
		v11 = vector_mix<mix4::z, mix4::y, mix4::a, mix4::x>(d0, d2);
		v12 = vector_mix<mix4::w, mix4::x, mix4::y, mix4::c>(d1, d2);
		v13 = vector_mix<mix4::z, mix4::y, mix4::c, mix4::x>(d1, d2);

		c0 = vector_neg_mul_sub(v00, v10, c0);
		c2 = vector_neg_mul_sub(v01, v11, c2);
		c4 = vector_neg_mul_sub(v02, v12, c4);
		c6 = vector_neg_mul_sub(v03, v13, c6);

		v00 = vector_mix<mix4::w, mix4::x, mix4::w, mix4::x>(input_transposed.y_axis, input_transposed.y_axis);
		v01 = vector_mix<mix4::y, mix4::w, mix4::x, mix4::z>(input_transposed.x_axis, input_transposed.x_axis);
		v02 = vector_mix<mix4::w, mix4::x, mix4::w, mix4::x>(input_transposed.w_axis, input_transposed.w_axis);
		v03 = vector_mix<mix4::y, mix4::w, mix4::x, mix4::z>(input_transposed.z_axis, input_transposed.z_axis);
		v10 = vector_mix<mix4::z, mix4::b, mix4::a, mix4::z>(d0, d2);
		v11 = vector_mix<mix4::b, mix4::x, mix4::w, mix4::a>(d0, d2);
		v12 = vector_mix<mix4::z, mix4::d, mix4::c, mix4::z>(d1, d2);
		v13 = vector_mix<mix4::d, mix4::x, mix4::w, mix4::c>(d1, d2);

		vector4d c1 = vector_neg_mul_sub(v00, v10, c0);
		c0 = vector_mul_add(v00, v10, c0);
		c2 = vector_neg_mul_sub(v01, v11, c2);
		c4 = vector_mul_add(v02, v12, c4);
		c6 = vector_neg_mul_sub(v03, v13, c6);

		vector4d x_axis = vector_mix<mix4::x, mix4::b, mix4::z, mix4::d>(c0, c1);

		return vector_dot_as_scalar(x_axis, input_transposed.x_axis);
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the minor of the input 4x4 matrix.
	// See: https://en.wikipedia.org/wiki/Minor_(linear_algebra)
	// The minor is the determinant of the sub-matrix input when the specified
	// row and column are removed.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline scalard RTM_SIMD_CALL matrix_minor(matrix4x4d_arg0 input, axis4 row, axis4 column) RTM_NO_EXCEPT
	{
		vector4d row0;
		vector4d row1;
		vector4d row2;

		// Find which rows we need.
		if (row == axis4::x)
		{
			row0 = input.y_axis;
			row1 = input.z_axis;
			row2 = input.w_axis;
		}
		else if (row == axis4::y)
		{
			row0 = input.x_axis;
			row1 = input.z_axis;
			row2 = input.w_axis;
		}
		else if (row == axis4::z)
		{
			row0 = input.x_axis;
			row1 = input.y_axis;
			row2 = input.w_axis;
		}
		else
		{
			row0 = input.x_axis;
			row1 = input.y_axis;
			row2 = input.z_axis;
		}

		// Shift our row values into a proper 3x3 matrix
		if (column == axis4::x)
		{
			row0 = vector_mix<mix4::y, mix4::z, mix4::w, mix4::w>(row0, row0);
			row1 = vector_mix<mix4::y, mix4::z, mix4::w, mix4::w>(row1, row1);
			row2 = vector_mix<mix4::y, mix4::z, mix4::w, mix4::w>(row2, row2);
		}
		else if (column == axis4::y)
		{
			row0 = vector_mix<mix4::x, mix4::z, mix4::w, mix4::w>(row0, row0);
			row1 = vector_mix<mix4::x, mix4::z, mix4::w, mix4::w>(row1, row1);
			row2 = vector_mix<mix4::x, mix4::z, mix4::w, mix4::w>(row2, row2);
		}
		else if (column == axis4::z)
		{
			row0 = vector_mix<mix4::x, mix4::y, mix4::w, mix4::w>(row0, row0);
			row1 = vector_mix<mix4::x, mix4::y, mix4::w, mix4::w>(row1, row1);
			row2 = vector_mix<mix4::x, mix4::y, mix4::w, mix4::w>(row2, row2);
		}
		else
		{
			// Already lined up
		}

		const matrix3x3d mtx = matrix_set(row0, row1, row2);
		return matrix_determinant(mtx);
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the cofactor matrix of the input 4x4 matrix.
	// See: https://en.wikipedia.org/wiki/Minor_(linear_algebra)#Cofactor_expansion_of_the_determinant
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline matrix4x4d RTM_SIMD_CALL matrix_cofactor(matrix4x4d_arg0 input) RTM_NO_EXCEPT
	{
		const scalard minor_xx = matrix_minor(input, axis4::x, axis4::x);
		const scalard minor_xy = matrix_minor(input, axis4::x, axis4::y);
		const scalard minor_xz = matrix_minor(input, axis4::x, axis4::z);
		const scalard minor_xw = matrix_minor(input, axis4::x, axis4::w);

		const scalard minor_yx = matrix_minor(input, axis4::y, axis4::x);
		const scalard minor_yy = matrix_minor(input, axis4::y, axis4::y);
		const scalard minor_yz = matrix_minor(input, axis4::y, axis4::z);
		const scalard minor_yw = matrix_minor(input, axis4::y, axis4::w);

		const scalard minor_zx = matrix_minor(input, axis4::z, axis4::x);
		const scalard minor_zy = matrix_minor(input, axis4::z, axis4::y);
		const scalard minor_zz = matrix_minor(input, axis4::z, axis4::z);
		const scalard minor_zw = matrix_minor(input, axis4::z, axis4::w);

		const scalard minor_wx = matrix_minor(input, axis4::w, axis4::x);
		const scalard minor_wy = matrix_minor(input, axis4::w, axis4::y);
		const scalard minor_wz = matrix_minor(input, axis4::w, axis4::z);
		const scalard minor_ww = matrix_minor(input, axis4::w, axis4::w);

		const vector4d xz_axis_signs = vector_set(1.0, -1.0, 1.0, -1.0);
		const vector4d yw_axis_signs = vector_set(-1.0, 1.0, -1.0, 1.0);

		const vector4d x_axis = vector_mul(vector_set(minor_xx, minor_xy, minor_xz, minor_xw), xz_axis_signs);
		const vector4d y_axis = vector_mul(vector_set(minor_yx, minor_yy, minor_yz, minor_yw), yw_axis_signs);
		const vector4d z_axis = vector_mul(vector_set(minor_zx, minor_zy, minor_zz, minor_zw), xz_axis_signs);
		const vector4d w_axis = vector_mul(vector_set(minor_wx, minor_wy, minor_wz, minor_ww), yw_axis_signs);
		return matrix4x4d{ x_axis, y_axis, z_axis, w_axis };
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the adjugate of the input matrix.
	// See: https://en.wikipedia.org/wiki/Adjugate_matrix
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline matrix4x4d RTM_SIMD_CALL matrix_adjugate(matrix4x4d_arg0 input) RTM_NO_EXCEPT
	{
		return matrix_transpose(matrix_cofactor(input));
	}

	RTM_IMPL_VERSION_NAMESPACE_END
}

RTM_IMPL_FILE_PRAGMA_POP

```

`includes/rtm/matrix4x4f.h`:

```h
#pragma once

////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2018 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include "rtm/macros.h"
#include "rtm/math.h"
#include "rtm/matrix3x3f.h"
#include "rtm/vector4f.h"
#include "rtm/version.h"
#include "rtm/impl/compiler_utils.h"
#include "rtm/impl/matrix_common.h"

RTM_IMPL_FILE_PRAGMA_PUSH

namespace rtm
{
	RTM_IMPL_VERSION_NAMESPACE_BEGIN

	//////////////////////////////////////////////////////////////////////////
	// Returns the axis pointing in the forward direction of the default coordinate system (Z+).
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr vector4f RTM_SIMD_CALL matrix_get_coord_forward(matrix4x4f_arg0 input) RTM_NO_EXCEPT
	{
		return input.z_axis;
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the axis pointing in the up direction of the default coordinate system (Y+).
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr vector4f RTM_SIMD_CALL matrix_get_coord_up(matrix4x4f_arg0 input) RTM_NO_EXCEPT
	{
		return input.y_axis;
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the axis pointing in the cross direction of the default coordinate system (X+).
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr vector4f RTM_SIMD_CALL matrix_get_coord_cross(matrix4x4f_arg0 input) RTM_NO_EXCEPT
	{
		return input.x_axis;
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the axis holding the position of the default coordinate system (W+).
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr vector4f RTM_SIMD_CALL matrix_get_coord_position(matrix4x4f_arg0 input) RTM_NO_EXCEPT
	{
		return input.w_axis;
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the desired 4x4 matrix axis.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr vector4f RTM_SIMD_CALL matrix_get_axis(matrix4x4f_arg0 input, axis4 axis) RTM_NO_EXCEPT
	{
		return axis == axis4::x ? input.x_axis : (axis == axis4::y ? input.y_axis : (axis == axis4::z ? input.z_axis : input.w_axis));
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns a new 4x4 matrix where the specified axis has been replaced on the input matrix.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline matrix4x4f RTM_SIMD_CALL matrix_set_axis(matrix4x4f_arg0 input, vector4f_arg5 axis_value, axis4 axis) RTM_NO_EXCEPT
	{
		switch (axis)
		{
			default:
			case axis4::x:	return matrix4x4f{ axis_value, input.y_axis, input.z_axis, input.w_axis };
			case axis4::y:	return matrix4x4f{ input.x_axis, axis_value, input.z_axis, input.w_axis };
			case axis4::z:	return matrix4x4f{ input.x_axis, input.y_axis, axis_value, input.w_axis };
			case axis4::w:	return matrix4x4f{ input.x_axis, input.y_axis, input.z_axis, axis_value };
		}
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the desired 4x4 matrix component from the specified axis.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline rtm_impl::vector4f_vector_get_component RTM_SIMD_CALL matrix_get_component(matrix4x4f_arg0 input, axis4 axis, component4 component) RTM_NO_EXCEPT
	{
		switch (axis)
		{
			default:
			case axis4::x:	return vector_get_component(input.x_axis, component);
			case axis4::y:	return vector_get_component(input.y_axis, component);
			case axis4::z:	return vector_get_component(input.z_axis, component);
			case axis4::w:	return vector_get_component(input.w_axis, component);
		}
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the desired 4x4 matrix component from the specified axis.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline scalarf RTM_SIMD_CALL matrix_get_component_as_scalar(matrix4x4f_arg0 input, axis4 axis, component4 component) RTM_NO_EXCEPT
	{
		switch (axis)
		{
			default:
			case axis4::x:	return vector_get_component_as_scalar(input.x_axis, component);
			case axis4::y:	return vector_get_component_as_scalar(input.y_axis, component);
			case axis4::z:	return vector_get_component_as_scalar(input.z_axis, component);
			case axis4::w:	return vector_get_component_as_scalar(input.w_axis, component);
		}
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns a new 4x4 matrix where the specified axis/component has been replaced on the input matrix.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline matrix4x4f RTM_SIMD_CALL matrix_set_component(matrix4x4f_arg0 input, float component_value, axis4 axis, component4 component) RTM_NO_EXCEPT
	{
		switch (axis)
		{
			default:
			case axis4::x:	return matrix4x4f{ vector_set_component(input.x_axis, component_value, component), input.y_axis, input.z_axis, input.w_axis };
			case axis4::y:	return matrix4x4f{ input.x_axis, vector_set_component(input.y_axis, component_value, component), input.z_axis, input.w_axis };
			case axis4::z:	return matrix4x4f{ input.x_axis, input.y_axis, vector_set_component(input.z_axis, component_value, component), input.w_axis };
			case axis4::w:	return matrix4x4f{ input.x_axis, input.y_axis, input.z_axis, vector_set_component(input.w_axis, component_value, component) };
		}
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Returns a new 4x4 matrix where the specified axis/component has been replaced on the input matrix.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline matrix4x4f RTM_SIMD_CALL matrix_set_component(matrix4x4f_arg0 input, scalarf_arg4 component_value, axis4 axis, component4 component) RTM_NO_EXCEPT
	{
		switch (axis)
		{
			default:
			case axis4::x:	return matrix4x4f{ vector_set_component(input.x_axis, component_value, component), input.y_axis, input.z_axis, input.w_axis };
			case axis4::y:	return matrix4x4f{ input.x_axis, vector_set_component(input.y_axis, component_value, component), input.z_axis, input.w_axis };
			case axis4::z:	return matrix4x4f{ input.x_axis, input.y_axis, vector_set_component(input.z_axis, component_value, component), input.w_axis };
			case axis4::w:	return matrix4x4f{ input.x_axis, input.y_axis, input.z_axis, vector_set_component(input.w_axis, component_value, component) };
		}
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Multiplies two 4x4 matrices.
	// Multiplication order is as follow: local_to_world = matrix_mul(local_to_object, object_to_world)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline matrix4x4f RTM_SIMD_CALL matrix_mul(matrix4x4f_arg0 lhs, matrix4x4f_arg1 rhs) RTM_NO_EXCEPT
	{
		vector4f tmp = vector_mul(vector_dup_x(lhs.x_axis), rhs.x_axis);
		tmp = vector_mul_add(vector_dup_y(lhs.x_axis), rhs.y_axis, tmp);
		tmp = vector_mul_add(vector_dup_z(lhs.x_axis), rhs.z_axis, tmp);
		tmp = vector_mul_add(vector_dup_w(lhs.x_axis), rhs.w_axis, tmp);
		vector4f x_axis = tmp;

		tmp = vector_mul(vector_dup_x(lhs.y_axis), rhs.x_axis);
		tmp = vector_mul_add(vector_dup_y(lhs.y_axis), rhs.y_axis, tmp);
		tmp = vector_mul_add(vector_dup_z(lhs.y_axis), rhs.z_axis, tmp);
		tmp = vector_mul_add(vector_dup_w(lhs.y_axis), rhs.w_axis, tmp);
		vector4f y_axis = tmp;

		tmp = vector_mul(vector_dup_x(lhs.z_axis), rhs.x_axis);
		tmp = vector_mul_add(vector_dup_y(lhs.z_axis), rhs.y_axis, tmp);
		tmp = vector_mul_add(vector_dup_z(lhs.z_axis), rhs.z_axis, tmp);
		tmp = vector_mul_add(vector_dup_w(lhs.z_axis), rhs.w_axis, tmp);
		vector4f z_axis = tmp;

		tmp = vector_mul(vector_dup_x(lhs.w_axis), rhs.x_axis);
		tmp = vector_mul_add(vector_dup_y(lhs.w_axis), rhs.y_axis, tmp);
		tmp = vector_mul_add(vector_dup_z(lhs.w_axis), rhs.z_axis, tmp);
		tmp = vector_mul_add(vector_dup_w(lhs.w_axis), rhs.w_axis, tmp);
		vector4f w_axis = tmp;

		return matrix4x4f{ x_axis, y_axis, z_axis, w_axis };
	}

	//////////////////////////////////////////////////////////////////////////
	// Multiplies a 4x4 matrix and a 4D vector.
	// Multiplication order is as follow: world_position = matrix_mul(local_position, local_to_world)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4f RTM_SIMD_CALL matrix_mul_vector(vector4f_arg0 vec4, matrix4x4f_arg0 mtx) RTM_NO_EXCEPT
	{
		vector4f tmp;

		tmp = vector_mul(vector_dup_x(vec4), mtx.x_axis);
		tmp = vector_mul_add(vector_dup_y(vec4), mtx.y_axis, tmp);
		tmp = vector_mul_add(vector_dup_z(vec4), mtx.z_axis, tmp);
		tmp = vector_mul_add(vector_dup_w(vec4), mtx.w_axis, tmp);

		return tmp;
	}

	//////////////////////////////////////////////////////////////////////////
	// Transposes a 4x4 matrix.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE matrix4x4f RTM_SIMD_CALL matrix_transpose(matrix4x4f_arg0 input) RTM_NO_EXCEPT
	{
		vector4f x_axis;
		vector4f y_axis;
		vector4f z_axis;
		vector4f w_axis;
		RTM_MATRIXF_TRANSPOSE_4X4(input.x_axis, input.y_axis, input.z_axis, input.w_axis, x_axis, y_axis, z_axis, w_axis);
		return matrix4x4f{ x_axis, y_axis, z_axis, w_axis };
	}

	//////////////////////////////////////////////////////////////////////////
	// Inverses a 4x4 matrix.
	// If the input matrix is not invertible, the result is undefined.
	// For a safe alternative, supply a fallback value and a threshold.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline matrix4x4f RTM_SIMD_CALL matrix_inverse(matrix4x4f_arg0 input) RTM_NO_EXCEPT
	{
		matrix4x4f input_transposed = matrix_transpose(input);

		vector4f v00 = vector_mix<mix4::x, mix4::x, mix4::y, mix4::y>(input_transposed.z_axis, input_transposed.z_axis);
		vector4f v01 = vector_mix<mix4::x, mix4::x, mix4::y, mix4::y>(input_transposed.x_axis, input_transposed.x_axis);
		vector4f v02 = vector_mix<mix4::x, mix4::z, mix4::a, mix4::c>(input_transposed.z_axis, input_transposed.x_axis);
		vector4f v10 = vector_mix<mix4::z, mix4::w, mix4::z, mix4::w>(input_transposed.w_axis, input_transposed.w_axis);
		vector4f v11 = vector_mix<mix4::z, mix4::w, mix4::z, mix4::w>(input_transposed.y_axis, input_transposed.y_axis);
		vector4f v12 = vector_mix<mix4::y, mix4::w, mix4::b, mix4::d>(input_transposed.w_axis, input_transposed.y_axis);

		vector4f d0 = vector_mul(v00, v10);
		vector4f d1 = vector_mul(v01, v11);
		vector4f d2 = vector_mul(v02, v12);

		v00 = vector_mix<mix4::z, mix4::w, mix4::z, mix4::w>(input_transposed.z_axis, input_transposed.z_axis);
		v01 = vector_mix<mix4::z, mix4::w, mix4::z, mix4::w>(input_transposed.x_axis, input_transposed.x_axis);
		v02 = vector_mix<mix4::y, mix4::w, mix4::b, mix4::d>(input_transposed.z_axis, input_transposed.x_axis);
		v10 = vector_mix<mix4::x, mix4::x, mix4::y, mix4::y>(input_transposed.w_axis, input_transposed.w_axis);
		v11 = vector_mix<mix4::x, mix4::x, mix4::y, mix4::y>(input_transposed.y_axis, input_transposed.y_axis);
		v12 = vector_mix<mix4::x, mix4::z, mix4::a, mix4::c>(input_transposed.w_axis, input_transposed.y_axis);

		d0 = vector_neg_mul_sub(v00, v10, d0);
		d1 = vector_neg_mul_sub(v01, v11, d1);
		d2 = vector_neg_mul_sub(v02, v12, d2);

		v00 = vector_mix<mix4::y, mix4::z, mix4::x, mix4::y>(input_transposed.y_axis, input_transposed.y_axis);
		v01 = vector_mix<mix4::z, mix4::x, mix4::y, mix4::x>(input_transposed.x_axis, input_transposed.x_axis);
		v02 = vector_mix<mix4::y, mix4::z, mix4::x, mix4::y>(input_transposed.w_axis, input_transposed.w_axis);
		vector4f v03 = vector_mix<mix4::z, mix4::x, mix4::y, mix4::x>(input_transposed.z_axis, input_transposed.z_axis);
		v10 = vector_mix<mix4::b, mix4::y, mix4::w, mix4::x>(d0, d2);
		v11 = vector_mix<mix4::w, mix4::b, mix4::y, mix4::z>(d0, d2);
		v12 = vector_mix<mix4::d, mix4::y, mix4::w, mix4::x>(d1, d2);
		vector4f v13 = vector_mix<mix4::w, mix4::d, mix4::y, mix4::z>(d1, d2);

		vector4f c0 = vector_mul(v00, v10);
		vector4f c2 = vector_mul(v01, v11);
		vector4f c4 = vector_mul(v02, v12);
		vector4f c6 = vector_mul(v03, v13);

		v00 = vector_mix<mix4::z, mix4::w, mix4::y, mix4::z>(input_transposed.y_axis, input_transposed.y_axis);
		v01 = vector_mix<mix4::w, mix4::z, mix4::w, mix4::y>(input_transposed.x_axis, input_transposed.x_axis);
		v02 = vector_mix<mix4::z, mix4::w, mix4::y, mix4::z>(input_transposed.w_axis, input_transposed.w_axis);
		v03 = vector_mix<mix4::w, mix4::z, mix4::w, mix4::y>(input_transposed.z_axis, input_transposed.z_axis);
		v10 = vector_mix<mix4::w, mix4::x, mix4::y, mix4::a>(d0, d2);
		v11 = vector_mix<mix4::z, mix4::y, mix4::a, mix4::x>(d0, d2);
		v12 = vector_mix<mix4::w, mix4::x, mix4::y, mix4::c>(d1, d2);
		v13 = vector_mix<mix4::z, mix4::y, mix4::c, mix4::x>(d1, d2);

		c0 = vector_neg_mul_sub(v00, v10, c0);
		c2 = vector_neg_mul_sub(v01, v11, c2);
		c4 = vector_neg_mul_sub(v02, v12, c4);
		c6 = vector_neg_mul_sub(v03, v13, c6);

		v00 = vector_mix<mix4::w, mix4::x, mix4::w, mix4::x>(input_transposed.y_axis, input_transposed.y_axis);
		v01 = vector_mix<mix4::y, mix4::w, mix4::x, mix4::z>(input_transposed.x_axis, input_transposed.x_axis);
		v02 = vector_mix<mix4::w, mix4::x, mix4::w, mix4::x>(input_transposed.w_axis, input_transposed.w_axis);
		v03 = vector_mix<mix4::y, mix4::w, mix4::x, mix4::z>(input_transposed.z_axis, input_transposed.z_axis);
		v10 = vector_mix<mix4::z, mix4::b, mix4::a, mix4::z>(d0, d2);
		v11 = vector_mix<mix4::b, mix4::x, mix4::w, mix4::a>(d0, d2);
		v12 = vector_mix<mix4::z, mix4::d, mix4::c, mix4::z>(d1, d2);
		v13 = vector_mix<mix4::d, mix4::x, mix4::w, mix4::c>(d1, d2);

		vector4f c1 = vector_neg_mul_sub(v00, v10, c0);
		c0 = vector_mul_add(v00, v10, c0);
		vector4f c3 = vector_mul_add(v01, v11, c2);
		c2 = vector_neg_mul_sub(v01, v11, c2);
		vector4f c5 = vector_neg_mul_sub(v02, v12, c4);
		c4 = vector_mul_add(v02, v12, c4);
		vector4f c7 = vector_mul_add(v03, v13, c6);
		c6 = vector_neg_mul_sub(v03, v13, c6);

		vector4f x_axis = vector_mix<mix4::x, mix4::b, mix4::z, mix4::d>(c0, c1);
		vector4f y_axis = vector_mix<mix4::x, mix4::b, mix4::z, mix4::d>(c2, c3);
		vector4f z_axis = vector_mix<mix4::x, mix4::b, mix4::z, mix4::d>(c4, c5);
		vector4f w_axis = vector_mix<mix4::x, mix4::b, mix4::z, mix4::d>(c6, c7);

		const scalarf det = vector_dot_as_scalar(x_axis, input_transposed.x_axis);
		const scalarf inv_det_s = scalar_reciprocal(det);
		const vector4f inv_det = vector_set(inv_det_s);

		x_axis = vector_mul(x_axis, inv_det);
		y_axis = vector_mul(y_axis, inv_det);
		z_axis = vector_mul(z_axis, inv_det);
		w_axis = vector_mul(w_axis, inv_det);

		return matrix4x4f{ x_axis, y_axis, z_axis, w_axis };
	}

	//////////////////////////////////////////////////////////////////////////
	// Inverses a 4x4 matrix.
	// If the input matrix has a determinant whose absolute value is below the supplied threshold, the
	// fall back value is returned instead.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline matrix4x4f RTM_SIMD_CALL matrix_inverse(matrix4x4f_arg0 input, matrix4x4f_arg1 fallback, float threshold = 1.0E-8F) RTM_NO_EXCEPT
	{
		matrix4x4f input_transposed = matrix_transpose(input);

		vector4f v00 = vector_mix<mix4::x, mix4::x, mix4::y, mix4::y>(input_transposed.z_axis, input_transposed.z_axis);
		vector4f v01 = vector_mix<mix4::x, mix4::x, mix4::y, mix4::y>(input_transposed.x_axis, input_transposed.x_axis);
		vector4f v02 = vector_mix<mix4::x, mix4::z, mix4::a, mix4::c>(input_transposed.z_axis, input_transposed.x_axis);
		vector4f v10 = vector_mix<mix4::z, mix4::w, mix4::z, mix4::w>(input_transposed.w_axis, input_transposed.w_axis);
		vector4f v11 = vector_mix<mix4::z, mix4::w, mix4::z, mix4::w>(input_transposed.y_axis, input_transposed.y_axis);
		vector4f v12 = vector_mix<mix4::y, mix4::w, mix4::b, mix4::d>(input_transposed.w_axis, input_transposed.y_axis);

		vector4f d0 = vector_mul(v00, v10);
		vector4f d1 = vector_mul(v01, v11);
		vector4f d2 = vector_mul(v02, v12);

		v00 = vector_mix<mix4::z, mix4::w, mix4::z, mix4::w>(input_transposed.z_axis, input_transposed.z_axis);
		v01 = vector_mix<mix4::z, mix4::w, mix4::z, mix4::w>(input_transposed.x_axis, input_transposed.x_axis);
		v02 = vector_mix<mix4::y, mix4::w, mix4::b, mix4::d>(input_transposed.z_axis, input_transposed.x_axis);
		v10 = vector_mix<mix4::x, mix4::x, mix4::y, mix4::y>(input_transposed.w_axis, input_transposed.w_axis);
		v11 = vector_mix<mix4::x, mix4::x, mix4::y, mix4::y>(input_transposed.y_axis, input_transposed.y_axis);
		v12 = vector_mix<mix4::x, mix4::z, mix4::a, mix4::c>(input_transposed.w_axis, input_transposed.y_axis);

		d0 = vector_neg_mul_sub(v00, v10, d0);
		d1 = vector_neg_mul_sub(v01, v11, d1);
		d2 = vector_neg_mul_sub(v02, v12, d2);

		v00 = vector_mix<mix4::y, mix4::z, mix4::x, mix4::y>(input_transposed.y_axis, input_transposed.y_axis);
		v01 = vector_mix<mix4::z, mix4::x, mix4::y, mix4::x>(input_transposed.x_axis, input_transposed.x_axis);
		v02 = vector_mix<mix4::y, mix4::z, mix4::x, mix4::y>(input_transposed.w_axis, input_transposed.w_axis);
		vector4f v03 = vector_mix<mix4::z, mix4::x, mix4::y, mix4::x>(input_transposed.z_axis, input_transposed.z_axis);
		v10 = vector_mix<mix4::b, mix4::y, mix4::w, mix4::x>(d0, d2);
		v11 = vector_mix<mix4::w, mix4::b, mix4::y, mix4::z>(d0, d2);
		v12 = vector_mix<mix4::d, mix4::y, mix4::w, mix4::x>(d1, d2);
		vector4f v13 = vector_mix<mix4::w, mix4::d, mix4::y, mix4::z>(d1, d2);

		vector4f c0 = vector_mul(v00, v10);
		vector4f c2 = vector_mul(v01, v11);
		vector4f c4 = vector_mul(v02, v12);
		vector4f c6 = vector_mul(v03, v13);

		v00 = vector_mix<mix4::z, mix4::w, mix4::y, mix4::z>(input_transposed.y_axis, input_transposed.y_axis);
		v01 = vector_mix<mix4::w, mix4::z, mix4::w, mix4::y>(input_transposed.x_axis, input_transposed.x_axis);
		v02 = vector_mix<mix4::z, mix4::w, mix4::y, mix4::z>(input_transposed.w_axis, input_transposed.w_axis);
		v03 = vector_mix<mix4::w, mix4::z, mix4::w, mix4::y>(input_transposed.z_axis, input_transposed.z_axis);
		v10 = vector_mix<mix4::w, mix4::x, mix4::y, mix4::a>(d0, d2);
		v11 = vector_mix<mix4::z, mix4::y, mix4::a, mix4::x>(d0, d2);
		v12 = vector_mix<mix4::w, mix4::x, mix4::y, mix4::c>(d1, d2);
		v13 = vector_mix<mix4::z, mix4::y, mix4::c, mix4::x>(d1, d2);

		c0 = vector_neg_mul_sub(v00, v10, c0);
		c2 = vector_neg_mul_sub(v01, v11, c2);
		c4 = vector_neg_mul_sub(v02, v12, c4);
		c6 = vector_neg_mul_sub(v03, v13, c6);

		v00 = vector_mix<mix4::w, mix4::x, mix4::w, mix4::x>(input_transposed.y_axis, input_transposed.y_axis);
		v01 = vector_mix<mix4::y, mix4::w, mix4::x, mix4::z>(input_transposed.x_axis, input_transposed.x_axis);
		v02 = vector_mix<mix4::w, mix4::x, mix4::w, mix4::x>(input_transposed.w_axis, input_transposed.w_axis);
		v03 = vector_mix<mix4::y, mix4::w, mix4::x, mix4::z>(input_transposed.z_axis, input_transposed.z_axis);
		v10 = vector_mix<mix4::z, mix4::b, mix4::a, mix4::z>(d0, d2);
		v11 = vector_mix<mix4::b, mix4::x, mix4::w, mix4::a>(d0, d2);
		v12 = vector_mix<mix4::z, mix4::d, mix4::c, mix4::z>(d1, d2);
		v13 = vector_mix<mix4::d, mix4::x, mix4::w, mix4::c>(d1, d2);

		vector4f c1 = vector_neg_mul_sub(v00, v10, c0);
		c0 = vector_mul_add(v00, v10, c0);
		vector4f c3 = vector_mul_add(v01, v11, c2);
		c2 = vector_neg_mul_sub(v01, v11, c2);
		vector4f c5 = vector_neg_mul_sub(v02, v12, c4);
		c4 = vector_mul_add(v02, v12, c4);
		vector4f c7 = vector_mul_add(v03, v13, c6);
		c6 = vector_neg_mul_sub(v03, v13, c6);

		vector4f x_axis = vector_mix<mix4::x, mix4::b, mix4::z, mix4::d>(c0, c1);
		vector4f y_axis = vector_mix<mix4::x, mix4::b, mix4::z, mix4::d>(c2, c3);
		vector4f z_axis = vector_mix<mix4::x, mix4::b, mix4::z, mix4::d>(c4, c5);
		vector4f w_axis = vector_mix<mix4::x, mix4::b, mix4::z, mix4::d>(c6, c7);

		const scalarf det = vector_dot_as_scalar(x_axis, input_transposed.x_axis);
		if (scalar_cast(scalar_abs(det)) < threshold)
			return fallback;

		const scalarf inv_det_s = scalar_reciprocal(det);
		const vector4f inv_det = vector_set(inv_det_s);

		x_axis = vector_mul(x_axis, inv_det);
		y_axis = vector_mul(y_axis, inv_det);
		z_axis = vector_mul(z_axis, inv_det);
		w_axis = vector_mul(w_axis, inv_det);

		return matrix4x4f{ x_axis, y_axis, z_axis, w_axis };
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the determinant of the input 4x4 matrix.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline scalarf RTM_SIMD_CALL matrix_determinant(matrix4x4f_arg0 input) RTM_NO_EXCEPT
	{
		matrix4x4f input_transposed = matrix_transpose(input);

		vector4f v00 = vector_mix<mix4::x, mix4::x, mix4::y, mix4::y>(input_transposed.z_axis, input_transposed.z_axis);
		vector4f v02 = vector_mix<mix4::x, mix4::z, mix4::a, mix4::c>(input_transposed.z_axis, input_transposed.x_axis);
		vector4f v10 = vector_mix<mix4::z, mix4::w, mix4::z, mix4::w>(input_transposed.w_axis, input_transposed.w_axis);
		vector4f v12 = vector_mix<mix4::y, mix4::w, mix4::b, mix4::d>(input_transposed.w_axis, input_transposed.y_axis);

		vector4f d0 = vector_mul(v00, v10);
		vector4f d2 = vector_mul(v02, v12);

		v00 = vector_mix<mix4::z, mix4::w, mix4::z, mix4::w>(input_transposed.z_axis, input_transposed.z_axis);
		v02 = vector_mix<mix4::y, mix4::w, mix4::b, mix4::d>(input_transposed.z_axis, input_transposed.x_axis);
		v10 = vector_mix<mix4::x, mix4::x, mix4::y, mix4::y>(input_transposed.w_axis, input_transposed.w_axis);
		v12 = vector_mix<mix4::x, mix4::z, mix4::a, mix4::c>(input_transposed.w_axis, input_transposed.y_axis);

		d0 = vector_neg_mul_sub(v00, v10, d0);
		d2 = vector_neg_mul_sub(v02, v12, d2);

		v00 = vector_mix<mix4::y, mix4::z, mix4::x, mix4::y>(input_transposed.y_axis, input_transposed.y_axis);
		v10 = vector_mix<mix4::b, mix4::y, mix4::w, mix4::x>(d0, d2);

		vector4f c0 = vector_mul(v00, v10);

		v00 = vector_mix<mix4::z, mix4::w, mix4::y, mix4::z>(input_transposed.y_axis, input_transposed.y_axis);
		v10 = vector_mix<mix4::w, mix4::x, mix4::y, mix4::a>(d0, d2);

		c0 = vector_neg_mul_sub(v00, v10, c0);

		v00 = vector_mix<mix4::w, mix4::x, mix4::w, mix4::x>(input_transposed.y_axis, input_transposed.y_axis);
		v10 = vector_mix<mix4::z, mix4::b, mix4::a, mix4::z>(d0, d2);

		vector4f c1 = vector_neg_mul_sub(v00, v10, c0);
		c0 = vector_mul_add(v00, v10, c0);

		vector4f x_axis = vector_mix<mix4::x, mix4::b, mix4::z, mix4::d>(c0, c1);

		return vector_dot_as_scalar(x_axis, input_transposed.x_axis);
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the minor of the input 4x4 matrix.
	// See: https://en.wikipedia.org/wiki/Minor_(linear_algebra)
	// The minor is the determinant of the sub-matrix input when the specified
	// row and column are removed.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline scalarf RTM_SIMD_CALL matrix_minor(matrix4x4f_arg0 input, axis4 row, axis4 column) RTM_NO_EXCEPT
	{
		vector4f row0;
		vector4f row1;
		vector4f row2;

		// Find which rows we need.
		if (row == axis4::x)
		{
			row0 = input.y_axis;
			row1 = input.z_axis;
			row2 = input.w_axis;
		}
		else if (row == axis4::y)
		{
			row0 = input.x_axis;
			row1 = input.z_axis;
			row2 = input.w_axis;
		}
		else if (row == axis4::z)
		{
			row0 = input.x_axis;
			row1 = input.y_axis;
			row2 = input.w_axis;
		}
		else
		{
			row0 = input.x_axis;
			row1 = input.y_axis;
			row2 = input.z_axis;
		}

		// Shift our row values into a proper 3x3 matrix
		if (column == axis4::x)
		{
			row0 = vector_mix<mix4::y, mix4::z, mix4::w, mix4::w>(row0, row0);
			row1 = vector_mix<mix4::y, mix4::z, mix4::w, mix4::w>(row1, row1);
			row2 = vector_mix<mix4::y, mix4::z, mix4::w, mix4::w>(row2, row2);
		}
		else if (column == axis4::y)
		{
			row0 = vector_mix<mix4::x, mix4::z, mix4::w, mix4::w>(row0, row0);
			row1 = vector_mix<mix4::x, mix4::z, mix4::w, mix4::w>(row1, row1);
			row2 = vector_mix<mix4::x, mix4::z, mix4::w, mix4::w>(row2, row2);
		}
		else if (column == axis4::z)
		{
			row0 = vector_mix<mix4::x, mix4::y, mix4::w, mix4::w>(row0, row0);
			row1 = vector_mix<mix4::x, mix4::y, mix4::w, mix4::w>(row1, row1);
			row2 = vector_mix<mix4::x, mix4::y, mix4::w, mix4::w>(row2, row2);
		}
		else
		{
			// Already lined up
		}

		const matrix3x3f mtx = matrix_set(row0, row1, row2);
		return matrix_determinant(mtx);
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the cofactor matrix of the input 4x4 matrix.
	// See: https://en.wikipedia.org/wiki/Minor_(linear_algebra)#Cofactor_expansion_of_the_determinant
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline matrix4x4f RTM_SIMD_CALL matrix_cofactor(matrix4x4f_arg0 input) RTM_NO_EXCEPT
	{
		const scalarf minor_xx = matrix_minor(input, axis4::x, axis4::x);
		const scalarf minor_xy = matrix_minor(input, axis4::x, axis4::y);
		const scalarf minor_xz = matrix_minor(input, axis4::x, axis4::z);
		const scalarf minor_xw = matrix_minor(input, axis4::x, axis4::w);

		const scalarf minor_yx = matrix_minor(input, axis4::y, axis4::x);
		const scalarf minor_yy = matrix_minor(input, axis4::y, axis4::y);
		const scalarf minor_yz = matrix_minor(input, axis4::y, axis4::z);
		const scalarf minor_yw = matrix_minor(input, axis4::y, axis4::w);

		const scalarf minor_zx = matrix_minor(input, axis4::z, axis4::x);
		const scalarf minor_zy = matrix_minor(input, axis4::z, axis4::y);
		const scalarf minor_zz = matrix_minor(input, axis4::z, axis4::z);
		const scalarf minor_zw = matrix_minor(input, axis4::z, axis4::w);

		const scalarf minor_wx = matrix_minor(input, axis4::w, axis4::x);
		const scalarf minor_wy = matrix_minor(input, axis4::w, axis4::y);
		const scalarf minor_wz = matrix_minor(input, axis4::w, axis4::z);
		const scalarf minor_ww = matrix_minor(input, axis4::w, axis4::w);

		const vector4f xz_axis_signs = vector_set(1.0F, -1.0F, 1.0F, -1.0F);
		const vector4f yw_axis_signs = vector_set(-1.0F, 1.0F, -1.0F, 1.0F);

		const vector4f x_axis = vector_mul(vector_set(minor_xx, minor_xy, minor_xz, minor_xw), xz_axis_signs);
		const vector4f y_axis = vector_mul(vector_set(minor_yx, minor_yy, minor_yz, minor_yw), yw_axis_signs);
		const vector4f z_axis = vector_mul(vector_set(minor_zx, minor_zy, minor_zz, minor_zw), xz_axis_signs);
		const vector4f w_axis = vector_mul(vector_set(minor_wx, minor_wy, minor_wz, minor_ww), yw_axis_signs);
		return matrix4x4f{ x_axis, y_axis, z_axis, w_axis };
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the adjugate of the input matrix.
	// See: https://en.wikipedia.org/wiki/Adjugate_matrix
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline matrix4x4f RTM_SIMD_CALL matrix_adjugate(matrix4x4f_arg0 input) RTM_NO_EXCEPT
	{
		return matrix_transpose(matrix_cofactor(input));
	}

	RTM_IMPL_VERSION_NAMESPACE_END
}

RTM_IMPL_FILE_PRAGMA_POP

```

`includes/rtm/packing/quatd.h`:

```h
#pragma once

////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2019 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include "rtm/math.h"
#include "rtm/quatd.h"
#include "rtm/vector4d.h"
#include "rtm/version.h"
#include "rtm/impl/compiler_utils.h"

RTM_IMPL_FILE_PRAGMA_PUSH

namespace rtm
{
	RTM_IMPL_VERSION_NAMESPACE_BEGIN

	//////////////////////////////////////////////////////////////////////////
	// Returns the quaternion on the hypersphere with a positive [w] component
	// that represents the same 3D rotation as the input.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE quatd quat_ensure_positive_w(const quatd& input) RTM_NO_EXCEPT
	{
		return quat_get_w(input) >= 0.0 ? input : quat_neg(input);
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns a quaternion constructed from a vector3 representing the [xyz]
	// components while reconstructing the [w] component by assuming it is positive.
	// Note: When the squared length of [xyz] is very small, the square-root might not
	// be very accurate when [w] is reconstructed. As such, the resulting quaternion
	// might be near but not quite normalized. If high accuracy is required, make
	// sure to normalize explicitly afterwards.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE quatd quat_from_positive_w(const vector4d& input) RTM_NO_EXCEPT
	{
		const double input_x = vector_get_x(input);
		const double input_y = vector_get_y(input);
		const double input_z = vector_get_z(input);

		// Operation order is important here, due to rounding, ((1.0 - (X*X)) - Y*Y) - Z*Z is more accurate than 1.0 - dot3(xyz, xyz)
		const double w_squared = ((1.0 - (input_x * input_x)) - (input_y * input_y)) - (input_z * input_z);

		// w_squared can be negative either due to rounding or due to quantization imprecision, we take the absolute value
		// to ensure the resulting quaternion is always normalized with a positive W component
		const double w = scalar_sqrt(scalar_abs(w_squared));
		return quat_set_w(vector_to_quat(input), w);
	}

	RTM_IMPL_VERSION_NAMESPACE_END
}

RTM_IMPL_FILE_PRAGMA_POP

```

`includes/rtm/packing/quatf.h`:

```h
#pragma once

////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2019 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include "rtm/math.h"
#include "rtm/quatf.h"
#include "rtm/vector4f.h"
#include "rtm/version.h"
#include "rtm/impl/bit_cast.impl.h"
#include "rtm/impl/compiler_utils.h"

RTM_IMPL_FILE_PRAGMA_PUSH

namespace rtm
{
	RTM_IMPL_VERSION_NAMESPACE_BEGIN

	//////////////////////////////////////////////////////////////////////////
	// Returns the quaternion on the hypersphere with a positive [w] component
	// that represents the same 3D rotation as the input.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE quatf RTM_SIMD_CALL quat_ensure_positive_w(quatf_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		constexpr __m128 sign_bit = RTM_VECTOR4F_MAKE(-0.0F, -0.0F, -0.0F, -0.0F);
		const __m128 input_sign = _mm_and_ps(input, sign_bit);
		const __m128 bias = _mm_shuffle_ps(input_sign, input_sign, _MM_SHUFFLE(3, 3, 3, 3));
		return _mm_xor_ps(input, bias);
#elif defined(RTM_NEON_INTRINSICS)
		alignas(16) constexpr uint32_t sign_bit_i[4] = { 0x80000000U, 0x80000000U, 0x80000000U, 0x80000000U };
		const uint32x4_t sign_bit = *rtm_impl::bit_cast<const uint32x4_t*>(&sign_bit_i[0]);
		const uint32x4_t input_u32 = vreinterpretq_u32_f32(input);
		const uint32x4_t input_sign = vandq_u32(input_u32, sign_bit);
		const uint32_t input_sign_w = vgetq_lane_u32(input_sign, 3);
#if defined(RTM_COMPILER_MSVC)
		// MSVC's intrinsic is an alias to the unsigned variant
		const uint32x4_t bias = vmovq_n_u32(static_cast<int32_t>(input_sign_w));
#else
		const uint32x4_t bias = vmovq_n_u32(input_sign_w);
#endif
		return vreinterpretq_f32_u32(veorq_u32(input_u32, bias));
#else
		return quat_get_w(input) >= 0.f ? input : quat_neg(input);
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns a quaternion constructed from a vector3 representing the [xyz]
	// components while reconstructing the [w] component by assuming it is positive.
	// Note: When the squared length of [xyz] is very small, the square-root might not
	// be very accurate when [w] is reconstructed. As such, the resulting quaternion
	// might be near but not quite normalized. If high accuracy is required, make
	// sure to normalize explicitly afterwards.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE quatf RTM_SIMD_CALL quat_from_positive_w(vector4f_arg0 input) RTM_NO_EXCEPT
	{
		// w_squared can be negative either due to rounding or due to quantization imprecision, we take the absolute value
		// to ensure the resulting quaternion is always normalized with a positive W component

#if defined(RTM_SSE2_INTRINSICS)
		const __m128i abs_mask = _mm_set_epi32(0x7FFFFFFFULL, 0x7FFFFFFFULL, 0x7FFFFFFFULL, 0x7FFFFFFFULL);

		__m128 x2y2z2 = _mm_mul_ps(input, input);
		__m128 one = _mm_set_ss(1.0F);
		__m128 w_squared = _mm_sub_ss(_mm_sub_ss(_mm_sub_ss(one, x2y2z2), _mm_shuffle_ps(x2y2z2, x2y2z2, _MM_SHUFFLE(1, 1, 1, 1))), _mm_shuffle_ps(x2y2z2, x2y2z2, _MM_SHUFFLE(2, 2, 2, 2)));
		w_squared = _mm_and_ps(w_squared, _mm_castsi128_ps(abs_mask));
		__m128 w = _mm_sqrt_ss(w_squared);

#if defined(RTM_SSE4_INTRINSICS)
		return _mm_insert_ps(input, w, 0x30);
#else
		__m128 input_wyzx = _mm_shuffle_ps(input, input, _MM_SHUFFLE(0, 2, 1, 3));
		__m128 result_wyzx = _mm_move_ss(input_wyzx, w);
		return _mm_shuffle_ps(result_wyzx, result_wyzx, _MM_SHUFFLE(0, 2, 1, 3));
#endif
#elif defined(RTM_NEON64_INTRINSICS) && defined(RTM_IMPL_VFMSS_SUPPORTED)
		// 1.0 - (x * x)
		float result = vfmss_laneq_f32(1.0F, vgetq_lane_f32(input, 0), input, 0);
		// result - (y * y)
		result = vfmss_laneq_f32(result, vgetq_lane_f32(input, 1), input, 1);
		// result - (z * z)
		float w_squared = vfmss_laneq_f32(result, vgetq_lane_f32(input, 2), input, 2);
		float w = scalar_sqrt(scalar_abs(w_squared));
		return vsetq_lane_f32(w, input, 3);
#else
		// Operation order is important here, due to rounding, ((1.0 - (X*X)) - Y*Y) - Z*Z is more accurate than 1.0 - dot3(xyz, xyz)
		float w_squared = ((1.0F - vector_get_x(input) * vector_get_x(input)) - vector_get_y(input) * vector_get_y(input)) - vector_get_z(input) * vector_get_z(input);
		// w_squared can be negative either due to rounding or due to quantization imprecision, we take the absolute value
		// to ensure the resulting quaternion is always normalized with a positive W component
		float w = scalar_sqrt(scalar_abs(w_squared));
		return quat_set_w(vector_to_quat(input), w);
#endif
	}

	RTM_IMPL_VERSION_NAMESPACE_END
}

RTM_IMPL_FILE_PRAGMA_POP

```

`includes/rtm/quatd.h`:

```h
#pragma once

////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2017 Nicholas Frechette & Animation Compression Library contributors
// Copyright (c) 2018 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include "rtm/math.h"
#include "rtm/scalard.h"
#include "rtm/vector4d.h"
#include "rtm/version.h"
#include "rtm/impl/compiler_utils.h"
#include "rtm/impl/memory_utils.h"
#include "rtm/impl/quat_common.h"

#include <limits>

RTM_IMPL_FILE_PRAGMA_PUSH

namespace rtm
{
	RTM_IMPL_VERSION_NAMESPACE_BEGIN

	//////////////////////////////////////////////////////////////////////////
	// Setters, getters, and casts
	//////////////////////////////////////////////////////////////////////////



	//////////////////////////////////////////////////////////////////////////
	// Loads an unaligned quaternion from memory.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE quatd RTM_SIMD_CALL quat_load(const double* input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return quatd{ _mm_loadu_pd(input), _mm_loadu_pd(input + 2) };
#else
		return quat_set(input[0], input[1], input[2], input[3]);
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Loads an unaligned quat from memory.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE quatd RTM_SIMD_CALL quat_load(const float4d* input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return quatd{ _mm_loadu_pd(&input->x), _mm_loadu_pd(&input->z) };
#else
		return quat_set(input->x, input->y, input->z, input->w);
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Casts a vector4 to a quaternion.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE quatd RTM_SIMD_CALL vector_to_quat(vector4d_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return quatd{ input.xy, input.zw };
#else
		return quatd{ input.x, input.y, input.z, input.w };
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Casts a quaternion float32 variant to a float64 variant.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE quatd RTM_SIMD_CALL quat_cast(quatf_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return quatd{ _mm_cvtps_pd(input), _mm_cvtps_pd(_mm_shuffle_ps(input, input, _MM_SHUFFLE(3, 2, 3, 2))) };
#elif defined(RTM_NEON_INTRINSICS)
		return quatd{ double(vgetq_lane_f32(input, 0)), double(vgetq_lane_f32(input, 1)), double(vgetq_lane_f32(input, 2)), double(vgetq_lane_f32(input, 3)) };
#else
		return quatd{ double(input.x), double(input.y), double(input.z), double(input.w) };
#endif
	}

	namespace rtm_impl
	{
		//////////////////////////////////////////////////////////////////////////
		// This is a helper struct to allow a single consistent API between
		// various vector types when the semantics are identical but the return
		// type differs. Implicit coercion is used to return the desired value
		// at the call site.
		//////////////////////////////////////////////////////////////////////////
		struct quatd_quat_get_x
		{
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator double() const RTM_NO_EXCEPT
			{
#if defined(RTM_SSE2_INTRINSICS)
				return _mm_cvtsd_f64(input.xy);
#else
				return input.x;
#endif
			}

#if defined(RTM_SSE2_INTRINSICS)
			RTM_DEPRECATED("Use 'as_scalar' suffix instead. To be removed in 2.4.")
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator scalard() const RTM_NO_EXCEPT
			{
				return scalard{ input.xy };
			}
#endif

			quatd input;
		};
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the quaternion [x] component (real part).
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr rtm_impl::quatd_quat_get_x RTM_SIMD_CALL quat_get_x(quatd_arg0 input) RTM_NO_EXCEPT
	{
		return rtm_impl::quatd_quat_get_x{ input };
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the quaternion [x] component (real part).
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE scalard RTM_SIMD_CALL quat_get_x_as_scalar(quatd_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return scalard{ input.xy };
#else
		return quat_get_x(input);
#endif
	}

	namespace rtm_impl
	{
		//////////////////////////////////////////////////////////////////////////
		// This is a helper struct to allow a single consistent API between
		// various vector types when the semantics are identical but the return
		// type differs. Implicit coercion is used to return the desired value
		// at the call site.
		//////////////////////////////////////////////////////////////////////////
		struct quatd_quat_get_y
		{
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator double() const RTM_NO_EXCEPT
			{
#if defined(RTM_SSE2_INTRINSICS)
				return _mm_cvtsd_f64(_mm_shuffle_pd(input.xy, input.xy, _MM_SHUFFLE2(0, 1)));
#else
				return input.y;
#endif
			}

#if defined(RTM_SSE2_INTRINSICS)
			RTM_DEPRECATED("Use 'as_scalar' suffix instead. To be removed in 2.4.")
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator scalard() const RTM_NO_EXCEPT
			{
				return scalard{ _mm_shuffle_pd(input.xy, input.xy, _MM_SHUFFLE2(0, 1)) };
			}
#endif

			quatd input;
		};
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the quaternion [y] component (real part).
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr rtm_impl::quatd_quat_get_y RTM_SIMD_CALL quat_get_y(quatd_arg0 input) RTM_NO_EXCEPT
	{
		return rtm_impl::quatd_quat_get_y{ input };
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the quaternion [y] component (real part).
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE scalard RTM_SIMD_CALL quat_get_y_as_scalar(quatd_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return scalard{ _mm_shuffle_pd(input.xy, input.xy, _MM_SHUFFLE2(0, 1)) };
#else
		return quat_get_y(input);
#endif
	}

	namespace rtm_impl
	{
		//////////////////////////////////////////////////////////////////////////
		// This is a helper struct to allow a single consistent API between
		// various vector types when the semantics are identical but the return
		// type differs. Implicit coercion is used to return the desired value
		// at the call site.
		//////////////////////////////////////////////////////////////////////////
		struct quatd_quat_get_z
		{
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator double() const RTM_NO_EXCEPT
			{
#if defined(RTM_SSE2_INTRINSICS)
				return _mm_cvtsd_f64(input.zw);
#else
				return input.z;
#endif
			}

#if defined(RTM_SSE2_INTRINSICS)
			RTM_DEPRECATED("Use 'as_scalar' suffix instead. To be removed in 2.4.")
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator scalard() const RTM_NO_EXCEPT
			{
				return scalard{ input.zw };
			}
#endif

			quatd input;
		};
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the quaternion [z] component (real part).
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr rtm_impl::quatd_quat_get_z RTM_SIMD_CALL quat_get_z(quatd_arg0 input) RTM_NO_EXCEPT
	{
		return rtm_impl::quatd_quat_get_z{ input };
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the quaternion [z] component (real part).
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE scalard RTM_SIMD_CALL quat_get_z_as_scalar(quatd_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return scalard{ input.zw };
#else
		return quat_get_z(input);
#endif
	}

	namespace rtm_impl
	{
		//////////////////////////////////////////////////////////////////////////
		// This is a helper struct to allow a single consistent API between
		// various vector types when the semantics are identical but the return
		// type differs. Implicit coercion is used to return the desired value
		// at the call site.
		//////////////////////////////////////////////////////////////////////////
		struct quatd_quat_get_w
		{
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator double() const RTM_NO_EXCEPT
			{
#if defined(RTM_SSE2_INTRINSICS)
				return _mm_cvtsd_f64(_mm_shuffle_pd(input.zw, input.zw, _MM_SHUFFLE2(0, 1)));
#else
				return input.w;
#endif
			}

#if defined(RTM_SSE2_INTRINSICS)
			RTM_DEPRECATED("Use 'as_scalar' suffix instead. To be removed in 2.4.")
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator scalard() const RTM_NO_EXCEPT
			{
				return scalard{ _mm_shuffle_pd(input.zw, input.zw, _MM_SHUFFLE2(0, 1)) };
			}
#endif

			quatd input;
		};
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the quaternion [w] component (imaginary part).
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr rtm_impl::quatd_quat_get_w RTM_SIMD_CALL quat_get_w(quatd_arg0 input) RTM_NO_EXCEPT
	{
		return rtm_impl::quatd_quat_get_w{ input };
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the quaternion [w] component (imaginary part).
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE scalard RTM_SIMD_CALL quat_get_w_as_scalar(quatd_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return scalard{ _mm_shuffle_pd(input.zw, input.zw, _MM_SHUFFLE2(0, 1)) };
#else
		return quat_get_w(input);
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Sets the quaternion [x] component (real part) and returns the new value.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE quatd RTM_SIMD_CALL quat_set_x(quatd_arg0 input, double lane_value) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return quatd{ _mm_move_sd(input.xy, _mm_set_sd(lane_value)), input.zw };
#else
		return quatd{ lane_value, input.y, input.z, input.w };
#endif
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Sets the quaternion [x] component (real part) and returns the new value.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE quatd RTM_SIMD_CALL quat_set_x(quatd_arg0 input, scalard_arg2 lane_value) RTM_NO_EXCEPT
	{
		return quatd{ _mm_move_sd(input.xy, lane_value.value), input.zw };
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Sets the quaternion [y] component (real part) and returns the new value.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE quatd RTM_SIMD_CALL quat_set_y(quatd_arg0 input, double lane_value) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return quatd{ _mm_shuffle_pd(input.xy, _mm_set_sd(lane_value), _MM_SHUFFLE2(0, 0)), input.zw };
#else
		return quatd{ input.x, lane_value, input.z, input.w };
#endif
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Sets the quaternion [y] component (real part) and returns the new value.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE quatd RTM_SIMD_CALL quat_set_y(quatd_arg0 input, scalard_arg2 lane_value) RTM_NO_EXCEPT
	{
		return quatd{ _mm_shuffle_pd(input.xy, lane_value.value, _MM_SHUFFLE2(0, 0)), input.zw };
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Sets the quaternion [z] component (real part) and returns the new value.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE quatd RTM_SIMD_CALL quat_set_z(quatd_arg0 input, double lane_value) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return quatd{ input.xy, _mm_move_sd(input.zw, _mm_set_sd(lane_value)) };
#else
		return quatd{ input.x, input.y, lane_value, input.w };
#endif
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Sets the quaternion [z] component (real part) and returns the new value.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE quatd RTM_SIMD_CALL quat_set_z(quatd_arg0 input, scalard_arg2 lane_value) RTM_NO_EXCEPT
	{
		return quatd{ input.xy, _mm_move_sd(input.zw, lane_value.value) };
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Sets the quaternion [w] component (imaginary part) and returns the new value.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE quatd RTM_SIMD_CALL quat_set_w(quatd_arg0 input, double lane_value) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return quatd{ input.xy, _mm_shuffle_pd(input.zw, _mm_set_sd(lane_value), _MM_SHUFFLE2(0, 0)) };
#else
		return quatd{ input.x, input.y, input.z, lane_value };
#endif
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Sets the quaternion [w] component (imaginary part) and returns the new value.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE quatd RTM_SIMD_CALL quat_set_w(quatd_arg0 input, scalard_arg2 lane_value) RTM_NO_EXCEPT
	{
		return quatd{ input.xy, _mm_shuffle_pd(input.zw, lane_value.value, _MM_SHUFFLE2(0, 0)) };
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Writes a quaternion to unaligned memory.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE void RTM_SIMD_CALL quat_store(quatd_arg0 input, double* output) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		_mm_storeu_pd(output, input.xy);
		_mm_storeu_pd(output + 2, input.zw);
#else
		output[0] = quat_get_x(input);
		output[1] = quat_get_y(input);
		output[2] = quat_get_z(input);
		output[3] = quat_get_w(input);
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Writes a quaternion to unaligned memory.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE void RTM_SIMD_CALL quat_store(quatd_arg0 input, float4d* output) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		_mm_storeu_pd(&output->x, input.xy);
		_mm_storeu_pd(&output->z, input.zw);
#else
		output->x = quat_get_x(input);
		output->y = quat_get_y(input);
		output->z = quat_get_z(input);
		output->w = quat_get_w(input);
#endif
	}



	//////////////////////////////////////////////////////////////////////////
	// Arithmetic
	//////////////////////////////////////////////////////////////////////////



	//////////////////////////////////////////////////////////////////////////
	// Returns the quaternion conjugate.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE quatd RTM_SIMD_CALL quat_conjugate(quatd_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		constexpr __m128d signs_xy = RTM_VECTOR2D_MAKE(-0.0, -0.0);
		constexpr __m128d signs_zw = RTM_VECTOR2D_MAKE(-0.0, 0.0);
		return quatd{ _mm_xor_pd(input.xy, signs_xy), _mm_xor_pd(input.zw, signs_zw) };
#else
		return quat_set(-quat_get_x(input), -quat_get_y(input), -quat_get_z(input), quat_get_w(input));
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Adds two quaternions.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline quatd RTM_SIMD_CALL quat_add(quatd_arg0 lhs, quatd_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return quatd{ _mm_add_pd(lhs.xy, rhs.xy), _mm_add_pd(lhs.zw, rhs.zw) };
#else
		return quat_set(quat_get_x(lhs) + quat_get_x(rhs), quat_get_y(lhs) + quat_get_y(rhs), quat_get_z(lhs) + quat_get_z(rhs), quat_get_w(lhs) + quat_get_w(rhs));
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Multiplies two quaternions.
	// Note that due to floating point rounding, the result might not be perfectly normalized.
	// Multiplication order is as follow: local_to_world = quat_mul(local_to_object, object_to_world)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline quatd RTM_SIMD_CALL quat_mul(quatd_arg0 lhs, quatd_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		constexpr __m128d signs_pos_neg = RTM_VECTOR2D_MAKE(0.0, -0.0);
		constexpr __m128d signs_neg_neg = RTM_VECTOR2D_MAKE(-0.0, -0.0);
		constexpr __m128d signs_neg_pos = RTM_VECTOR2D_MAKE(-0.0, 0.0);

		const __m128d r_xx = _mm_shuffle_pd(rhs.xy, rhs.xy, _MM_SHUFFLE2(0, 0));
		const __m128d r_yy = _mm_shuffle_pd(rhs.xy, rhs.xy, _MM_SHUFFLE2(1, 1));
		const __m128d r_zz = _mm_shuffle_pd(rhs.zw, rhs.zw, _MM_SHUFFLE2(0, 0));
		const __m128d r_ww = _mm_shuffle_pd(rhs.zw, rhs.zw, _MM_SHUFFLE2(1, 1));

		const __m128d lxrw_lyrw = _mm_mul_pd(r_ww, lhs.xy);
		const __m128d lzrw_lwrw = _mm_mul_pd(r_ww, lhs.zw);
		const __m128d l_wz = _mm_shuffle_pd(lhs.zw, lhs.zw, _MM_SHUFFLE2(0, 1));
		const __m128d l_yx = _mm_shuffle_pd(lhs.xy, lhs.xy, _MM_SHUFFLE2(0, 1));

		const __m128d lwrx_lzrx = _mm_mul_pd(r_xx, l_wz);
		const __m128d lyrx_lxrx = _mm_mul_pd(r_xx, l_yx);

		const __m128d lwrx_nlzrx = _mm_xor_pd(lwrx_lzrx, signs_pos_neg);
		const __m128d lyrx_nlxrx = _mm_xor_pd(lyrx_lxrx, signs_pos_neg);

		const __m128d lzry_lwry = _mm_mul_pd(r_yy, lhs.zw);
		const __m128d lxry_lyry = _mm_mul_pd(r_yy, lhs.xy);

		const __m128d nlxry_nlyry = _mm_xor_pd(lxry_lyry, signs_neg_neg);

		const __m128d lyrz_lxrz = _mm_mul_pd(r_zz, l_yx);
		const __m128d lwrz_lzrz = _mm_mul_pd(r_zz, l_wz);
		const __m128d result0_xy = _mm_add_pd(lxrw_lyrw, lwrx_nlzrx);
		const __m128d result0_zw = _mm_add_pd(lzrw_lwrw, lyrx_nlxrx);

		const __m128d nlyrz_lxrz = _mm_xor_pd(lyrz_lxrz, signs_neg_pos);
		const __m128d lwrz_nlzrz = _mm_xor_pd(lwrz_lzrz, signs_pos_neg);

		const __m128d result1_xy = _mm_add_pd(lzry_lwry, nlyrz_lxrz);
		const __m128d result1_zw = _mm_add_pd(nlxry_nlyry, lwrz_nlzrz);

		return quatd{ _mm_add_pd(result0_xy, result1_xy), _mm_add_pd(result0_zw, result1_zw) };
#else
		double lhs_x = quat_get_x(lhs);
		double lhs_y = quat_get_y(lhs);
		double lhs_z = quat_get_z(lhs);
		double lhs_w = quat_get_w(lhs);

		double rhs_x = quat_get_x(rhs);
		double rhs_y = quat_get_y(rhs);
		double rhs_z = quat_get_z(rhs);
		double rhs_w = quat_get_w(rhs);

		double x = (rhs_w * lhs_x) + (rhs_x * lhs_w) + (rhs_y * lhs_z) - (rhs_z * lhs_y);
		double y = (rhs_w * lhs_y) - (rhs_x * lhs_z) + (rhs_y * lhs_w) + (rhs_z * lhs_x);
		double z = (rhs_w * lhs_z) + (rhs_x * lhs_y) - (rhs_y * lhs_x) + (rhs_z * lhs_w);
		double w = (rhs_w * lhs_w) - (rhs_x * lhs_x) - (rhs_y * lhs_y) - (rhs_z * lhs_z);

		return quat_set(x, y, z, w);
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Multiplies a quaternion with a scalar.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE quatd RTM_SIMD_CALL quat_mul(quatd_arg0 quat, double scalar) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		const __m128d scalar_v = _mm_set1_pd(scalar);
		return quatd{ _mm_mul_pd(quat.xy, scalar_v), _mm_mul_pd(quat.zw, scalar_v) };
#else
		return quat_set(quat_get_x(quat) * scalar, quat_get_y(quat) * scalar, quat_get_z(quat) * scalar, quat_get_w(quat) * scalar);
#endif
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Multiplies a quaternion with a scalar.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE quatd RTM_SIMD_CALL quat_mul(quatd_arg0 quat, scalard_arg1 scalar) RTM_NO_EXCEPT
	{
		return quatd{ _mm_mul_pd(quat.xy, scalar.value), _mm_mul_pd(quat.zw, scalar.value) };
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Multiplies a quaternion and a 3D vector, rotating it.
	// Multiplication order is as follow: world_position = quat_mul_vector3(local_vector, local_to_world)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline vector4d RTM_SIMD_CALL quat_mul_vector3(vector4d_arg0 vector, quatd_arg1 rotation) RTM_NO_EXCEPT
	{
		quatd vector_quat = quat_set_w(vector_to_quat(vector), 0.0);
		quatd inv_rotation = quat_conjugate(rotation);
		return quat_to_vector(quat_mul(quat_mul(inv_rotation, vector_quat), rotation));
	}

	namespace rtm_impl
	{
		//////////////////////////////////////////////////////////////////////////
		// This is a helper struct to allow a single consistent API between
		// various vector types when the semantics are identical but the return
		// type differs. Implicit coercion is used to return the desired value
		// at the call site.
		//////////////////////////////////////////////////////////////////////////
		struct quatd_quat_dot
		{
			RTM_DISABLE_SECURITY_COOKIE_CHECK inline RTM_SIMD_CALL operator double() const RTM_NO_EXCEPT
			{
#if defined(RTM_SSE2_INTRINSICS)
				const __m128d x2_y2 = _mm_mul_pd(lhs.xy, rhs.xy);
				const __m128d z2_w2 = _mm_mul_pd(lhs.zw, rhs.zw);
				const __m128d x2z2_y2w2 = _mm_add_pd(x2_y2, z2_w2);
				const __m128d y2w2 = _mm_shuffle_pd(x2z2_y2w2, x2z2_y2w2, _MM_SHUFFLE2(1, 1));
				const __m128d x2y2z2w2 = _mm_add_pd(x2z2_y2w2, y2w2);
				return _mm_cvtsd_f64(x2y2z2w2);
#else
				const scalard lhs_x = quat_get_x_as_scalar(lhs);
				const scalard lhs_y = quat_get_y_as_scalar(lhs);
				const scalard lhs_z = quat_get_z_as_scalar(lhs);
				const scalard lhs_w = quat_get_w_as_scalar(lhs);
				const scalard rhs_x = quat_get_x_as_scalar(rhs);
				const scalard rhs_y = quat_get_y_as_scalar(rhs);
				const scalard rhs_z = quat_get_z_as_scalar(rhs);
				const scalard rhs_w = quat_get_w_as_scalar(rhs);
				const scalard xx = scalar_mul(lhs_x, rhs_x);
				const scalard yy = scalar_mul(lhs_y, rhs_y);
				const scalard zz = scalar_mul(lhs_z, rhs_z);
				const scalard ww = scalar_mul(lhs_w, rhs_w);
				return scalar_cast(scalar_add(scalar_add(xx, yy), scalar_add(zz, ww)));
#endif
			}

#if defined(RTM_SSE2_INTRINSICS)
			RTM_DEPRECATED("Use 'as_scalar' suffix instead. To be removed in 2.4.")
			RTM_DISABLE_SECURITY_COOKIE_CHECK inline RTM_SIMD_CALL operator scalard() const RTM_NO_EXCEPT
			{
				const __m128d x2_y2 = _mm_mul_pd(lhs.xy, rhs.xy);
				const __m128d z2_w2 = _mm_mul_pd(lhs.zw, rhs.zw);
				const __m128d x2z2_y2w2 = _mm_add_pd(x2_y2, z2_w2);
				const __m128d y2w2 = _mm_shuffle_pd(x2z2_y2w2, x2z2_y2w2, _MM_SHUFFLE2(1, 1));
				const __m128d x2y2z2w2 = _mm_add_pd(x2z2_y2w2, y2w2);
				return scalard{ x2y2z2w2 };
			}
#endif

			quatd lhs;
			quatd rhs;
		};
	}

	//////////////////////////////////////////////////////////////////////////
	// Quaternion dot product: lhs . rhs
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK constexpr rtm_impl::quatd_quat_dot RTM_SIMD_CALL quat_dot(quatd_arg0 lhs, quatd_arg1 rhs) RTM_NO_EXCEPT
	{
		return rtm_impl::quatd_quat_dot{ lhs, rhs };
	}

	//////////////////////////////////////////////////////////////////////////
	// Quaternion dot product: lhs . rhs
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline scalard RTM_SIMD_CALL quat_dot_as_scalar(quatd_arg0 lhs, quatd_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		const __m128d x2_y2 = _mm_mul_pd(lhs.xy, rhs.xy);
		const __m128d z2_w2 = _mm_mul_pd(lhs.zw, rhs.zw);
		const __m128d x2z2_y2w2 = _mm_add_pd(x2_y2, z2_w2);
		const __m128d y2w2 = _mm_shuffle_pd(x2z2_y2w2, x2z2_y2w2, _MM_SHUFFLE2(1, 1));
		const __m128d x2y2z2w2 = _mm_add_pd(x2z2_y2w2, y2w2);
		return scalard{ x2y2z2w2 };
#else
		const scalard lhs_x = quat_get_x_as_scalar(lhs);
		const scalard lhs_y = quat_get_y_as_scalar(lhs);
		const scalard lhs_z = quat_get_z_as_scalar(lhs);
		const scalard lhs_w = quat_get_w_as_scalar(lhs);
		const scalard rhs_x = quat_get_x_as_scalar(rhs);
		const scalard rhs_y = quat_get_y_as_scalar(rhs);
		const scalard rhs_z = quat_get_z_as_scalar(rhs);
		const scalard rhs_w = quat_get_w_as_scalar(rhs);
		const scalard xx = scalar_mul(lhs_x, rhs_x);
		const scalard yy = scalar_mul(lhs_y, rhs_y);
		const scalard zz = scalar_mul(lhs_z, rhs_z);
		const scalard ww = scalar_mul(lhs_w, rhs_w);
		return scalar_add(scalar_add(xx, yy), scalar_add(zz, ww));
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the squared length/norm of the quaternion.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK constexpr rtm_impl::quatd_quat_dot RTM_SIMD_CALL quat_length_squared(quatd_arg0 input) RTM_NO_EXCEPT
	{
		return rtm_impl::quatd_quat_dot{ input, input };
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the squared length/norm of the quaternion.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline scalard RTM_SIMD_CALL quat_length_squared_as_scalar(quatd_arg0 input) RTM_NO_EXCEPT
	{
		return quat_dot_as_scalar(input, input);
	}

	namespace rtm_impl
	{
		//////////////////////////////////////////////////////////////////////////
		// This is a helper struct to allow a single consistent API between
		// various vector types when the semantics are identical but the return
		// type differs. Implicit coercion is used to return the desired value
		// at the call site.
		//////////////////////////////////////////////////////////////////////////
		struct quatd_quat_length
		{
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator double() const RTM_NO_EXCEPT
			{
				const scalard len_sq = quat_length_squared_as_scalar(input);
				return scalar_cast(scalar_sqrt(len_sq));
			}

#if defined(RTM_SSE2_INTRINSICS)
			RTM_DEPRECATED("Use 'as_scalar' suffix instead. To be removed in 2.4.")
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator scalard() const RTM_NO_EXCEPT
			{
				const scalard len_sq = quat_length_squared_as_scalar(input);
				return scalar_sqrt(len_sq);
			}
#endif

			quatd input;
		};
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the length/norm of the quaternion.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr rtm_impl::quatd_quat_length RTM_SIMD_CALL quat_length(quatd_arg0 input) RTM_NO_EXCEPT
	{
		return rtm_impl::quatd_quat_length{ input };
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the length/norm of the quaternion.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE scalard RTM_SIMD_CALL quat_length_as_scalar(quatd_arg0 input) RTM_NO_EXCEPT
	{
		const scalard len_sq = quat_length_squared_as_scalar(input);
		return scalar_sqrt(len_sq);
	}

	namespace rtm_impl
	{
		//////////////////////////////////////////////////////////////////////////
		// This is a helper struct to allow a single consistent API between
		// various vector types when the semantics are identical but the return
		// type differs. Implicit coercion is used to return the desired value
		// at the call site.
		//////////////////////////////////////////////////////////////////////////
		struct quatd_quat_length_reciprocal
		{
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator double() const RTM_NO_EXCEPT
			{
				const scalard len_sq = quat_length_squared_as_scalar(input);
				return scalar_cast(scalar_sqrt_reciprocal(len_sq));
			}

#if defined(RTM_SSE2_INTRINSICS)
			RTM_DEPRECATED("Use 'as_scalar' suffix instead. To be removed in 2.4.")
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator scalard() const RTM_NO_EXCEPT
			{
				const scalard len_sq = quat_length_squared_as_scalar(input);
				return scalar_sqrt_reciprocal(len_sq);
			}
#endif

			quatd input;
		};
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the reciprocal length/norm of the quaternion.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr rtm_impl::quatd_quat_length_reciprocal RTM_SIMD_CALL quat_length_reciprocal(quatd_arg0 input) RTM_NO_EXCEPT
	{
		return rtm_impl::quatd_quat_length_reciprocal{ input };
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the reciprocal length/norm of the quaternion.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE scalard RTM_SIMD_CALL quat_length_reciprocal_as_scalar(quatd_arg0 input) RTM_NO_EXCEPT
	{
		const scalard len_sq = quat_length_squared_as_scalar(input);
		return scalar_sqrt_reciprocal(len_sq);
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns a normalized quaternion.
	// Note that if the input quaternion is invalid (pure zero or with NaN/Inf),
	// the result is undefined.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE quatd RTM_SIMD_CALL quat_normalize(quatd_arg0 input) RTM_NO_EXCEPT
	{
		// TODO: Use high precision recip sqrt function and vector_mul
		double length = quat_length(input);
		//float length_recip = quat_length_reciprocal(input);
		vector4d input_vector = quat_to_vector(input);
		//return vector_to_quat(vector_mul(input_vector, length_recip));
		return vector_to_quat(vector_div(input_vector, vector_set(length)));
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns a normalized quaternion using a deterministic algorithm.
	// This ensures that for a given input, the output will be identical on all
	// platforms that implement IEEE-754. This can be slower than `quat_normalize`.
	// Note that if the input quaternion is invalid (pure zero or with NaN/Inf),
	// the result is undefined.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE quatd RTM_SIMD_CALL quat_normalize_deterministic(quatd_arg0 input) RTM_NO_EXCEPT
	{
		vector4d inputv = quat_to_vector(input);

		// Multiply once and retrieve floats, can't use scalarf because we need to use volatile
		// volatile will force a roundtrip to memory and should prevent re-ordering as well as
		// other optimizations such as FMA.
		vector4d input_sq = vector_mul(inputv, inputv);
		volatile double x_sq = vector_get_x(input_sq);
		volatile double y_sq = vector_get_y(input_sq);
		volatile double z_sq = vector_get_z(input_sq);
		volatile double w_sq = vector_get_w(input_sq);
		volatile double sum_xy_sq = x_sq + y_sq;
		volatile double sum_zw_sq = z_sq + w_sq;
		double len_sq = sum_xy_sq + sum_zw_sq;

		// We add volatile to ensure rsqrt or similar isn't used
		volatile double len = scalar_sqrt(len_sq);
		return vector_to_quat(vector_div(inputv, vector_set(len)));
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the linear interpolation between start and end for a given alpha value.
	// The formula used is: ((1.0 - alpha) * start) + (alpha * end).
	// Interpolation is stable and will return 'start' when 'alpha' is 0.0 and 'end' when it is 1.0.
	// This is the same instruction count when FMA is present but it might be slightly slower
	// due to the extra multiplication compared to: start + (alpha * (end - start)).
	// Note that if 'start' and 'end' are on the opposite ends of the hypersphere, 'end' is negated
	// before we interpolate. As such, when 'alpha' is 1.0, either 'end' or its negated equivalent
	// is returned. Furthermore, if 'start' and 'end' aren't exactly normalized, the result might
	// not match exactly when 'alpha' is 0.0 or 1.0 because we normalize the resulting quaternion.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline quatd RTM_SIMD_CALL quat_lerp(quatd_arg0 start, quatd_arg1 end, double alpha) RTM_NO_EXCEPT
	{
		// To ensure we take the shortest path, we apply a bias if the dot product is negative
		vector4d start_vector = quat_to_vector(start);
		vector4d end_vector = quat_to_vector(end);
		double dot = vector_dot(start_vector, end_vector);
		double bias = dot >= 0.0 ? 1.0 : -1.0;
		// ((1.0 - alpha) * start) + (alpha * (end * bias)) == (start - alpha * start) + (alpha * (end * bias))
		vector4d value = vector_mul_add(vector_mul(end_vector, bias), alpha, vector_neg_mul_sub(start_vector, alpha, start_vector));
		return quat_normalize(vector_to_quat(value));
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Returns the linear interpolation between start and end for a given alpha value.
	// The formula used is: ((1.0 - alpha) * start) + (alpha * end).
	// Interpolation is stable and will return 'start' when 'alpha' is 0.0 and 'end' when it is 1.0.
	// This is the same instruction count when FMA is present but it might be slightly slower
	// due to the extra multiplication compared to: start + (alpha * (end - start)).
	// Note that if 'start' and 'end' are on the opposite ends of the hypersphere, 'end' is negated
	// before we interpolate. As such, when 'alpha' is 1.0, either 'end' or its negated equivalent
	// is returned. Furthermore, if 'start' and 'end' aren't exactly normalized, the result might
	// not match exactly when 'alpha' is 0.0 or 1.0 because we normalize the resulting quaternion.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline quatd RTM_SIMD_CALL quat_lerp(quatd_arg0 start, quatd_arg1 end, scalard_arg4 alpha) RTM_NO_EXCEPT
	{
		// To ensure we take the shortest path, we apply a bias if the dot product is negative
		vector4d start_vector = quat_to_vector(start);
		vector4d end_vector = quat_to_vector(end);
		double dot = vector_dot(start_vector, end_vector);
		double bias = dot >= 0.0 ? 1.0 : -1.0;
		// ((1.0 - alpha) * start) + (alpha * (end * bias)) == (start - alpha * start) + (alpha * (end * bias))
		vector4d alpha_v = vector_set(alpha);
		vector4d value = vector_mul_add(vector_mul(end_vector, bias), alpha_v, vector_neg_mul_sub(start_vector, alpha_v, start_vector));
		return quat_normalize(vector_to_quat(value));
	}
#endif

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Returns the spherical interpolation between start and end for a given alpha value.
	// See: https://www.euclideanspace.com/maths/algebra/realNormedAlgebra/quaternions/slerp/index.htm
	// Perhaps try this someday: http://number-none.com/product/Understanding%20Slerp,%20Then%20Not%20Using%20It/
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline quatd RTM_SIMD_CALL quat_slerp(quatd_arg0 start, quatd_arg1 end, scalard_arg4 alpha) RTM_NO_EXCEPT
	{
		vector4d start_v = quat_to_vector(start);
		vector4d end_v = quat_to_vector(end);

		vector4d cos_half_angle_v = vector_dot_as_vector(start_v, end_v);
		mask4d is_angle_negative = vector_less_than(cos_half_angle_v, vector_zero());

		// If the two input quaternions aren't on the same half of the hypersphere, flip one and the angle sign
		end_v = vector_select(is_angle_negative, vector_neg(end_v), end_v);
		cos_half_angle_v = vector_select(is_angle_negative, vector_neg(cos_half_angle_v), cos_half_angle_v);

		// Clamp our half angle cosine
		cos_half_angle_v = vector_clamp(cos_half_angle_v, vector_set(-1.0), vector_set(1.0));

		scalard cos_half_angle = vector_get_x_as_scalar(cos_half_angle_v);
		scalard half_angle = scalar_acos(cos_half_angle);
		scalard sin_half_angle = scalar_sqrt(scalar_sub(scalar_set(1.0), scalar_mul(cos_half_angle, cos_half_angle)));
		scalard inv_sin_half_angle = scalar_reciprocal(sin_half_angle);

		scalard start_contribution_angle = scalar_mul(scalar_sub(scalar_set(1.0), alpha), half_angle);
		scalard end_contribution_angle = scalar_mul(alpha, half_angle);
		vector4d contribution_angles = vector_set(start_contribution_angle, end_contribution_angle, start_contribution_angle, end_contribution_angle);
		vector4d contributions = vector_mul(vector_sin(contribution_angles), inv_sin_half_angle);
		vector4d start_contribution = vector_dup_x(contributions);
		vector4d end_contribution = vector_dup_y(contributions);

		vector4d result = vector_add(vector_mul(start_v, start_contribution), vector_mul(end_v, end_contribution));
		return vector_to_quat(result);
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Returns the spherical interpolation between start and end for a given alpha value.
	// See: https://www.euclideanspace.com/maths/algebra/realNormedAlgebra/quaternions/slerp/index.htm
	// Perhaps try this someday: http://number-none.com/product/Understanding%20Slerp,%20Then%20Not%20Using%20It/
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline quatd RTM_SIMD_CALL quat_slerp(quatd_arg0 start, quatd_arg0 end, double alpha) RTM_NO_EXCEPT
	{
		vector4d start_v = quat_to_vector(start);
		vector4d end_v = quat_to_vector(end);
		scalard alpha_s = scalar_set(alpha);

		vector4d cos_half_angle_v = vector_dot_as_vector(start_v, end_v);
		mask4d is_angle_negative = vector_less_than(cos_half_angle_v, vector_zero());

		// If the two input quaternions aren't on the same half of the hypersphere, flip one and the angle sign
		end_v = vector_select(is_angle_negative, vector_neg(end_v), end_v);
		cos_half_angle_v = vector_select(is_angle_negative, vector_neg(cos_half_angle_v), cos_half_angle_v);

		// Clamp our half angle cosine
		cos_half_angle_v = vector_clamp(cos_half_angle_v, vector_set(-1.0), vector_set(1.0));

		scalard cos_half_angle = vector_get_x_as_scalar(cos_half_angle_v);
		scalard half_angle = scalar_acos(cos_half_angle);
		scalard sin_half_angle = scalar_sqrt(scalar_sub(scalar_set(1.0), scalar_mul(cos_half_angle, cos_half_angle)));
		scalard inv_sin_half_angle = scalar_reciprocal(sin_half_angle);

		scalard start_contribution_angle = scalar_mul(scalar_sub(scalar_set(1.0), alpha_s), half_angle);
		scalard end_contribution_angle = scalar_mul(alpha_s, half_angle);
		vector4d contribution_angles = vector_set(start_contribution_angle, end_contribution_angle, start_contribution_angle, end_contribution_angle);
		vector4d contributions = vector_mul(vector_sin(contribution_angles), inv_sin_half_angle);
		vector4d start_contribution = vector_dup_x(contributions);
		vector4d end_contribution = vector_dup_y(contributions);

		vector4d result = vector_add(vector_mul(start_v, start_contribution), vector_mul(end_v, end_contribution));
		return vector_to_quat(result);
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns a component wise negated quaternion.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE quatd RTM_SIMD_CALL quat_neg(quatd_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		constexpr __m128d signs = RTM_VECTOR2D_MAKE(-0.0, -0.0);
		return quatd{ _mm_xor_pd(input.xy, signs), _mm_xor_pd(input.zw, signs) };
#else
		return vector_to_quat(vector_mul(quat_to_vector(input), -1.0));
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the quaternion logarithm for a 3D rotation.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE quatd RTM_SIMD_CALL quat_rotation_log(quatd_arg0 input) RTM_NO_EXCEPT
	{
		// The logarithm equation is as follow:
		// log(q).xyz = (q.xyz / ||q.xyz||) * acos(q.w / ||q||)
		// log(q).w = ln(||q||)
		//
		// If our quaternion is normalized (a rotation), our output W is always 0.0 since ln(1) = 0.0
		// Our XYZ simplifies as well down to: normalize(q.xyz) * acos(q.w)
		// The length of our input XYZ can be calculated either as sqrt(dot(q.xyz, q.xyz)) or
		// by taking the sine of the quaternion half angle with sin(acos(q.w))
		//
		// If our input rotation is near the identity, we cannot calculate the output XYZ since we would
		// divide by zero. If this happens we return a quaternion near zero which should reconstruct as the
		// identity with quat_rotation_exp(..).
		//
		// If our quaternion isn't normalized, more math is required

		const vector4d input_v = quat_to_vector(input);
		const scalard input_w = scalar_clamp(quat_get_w_as_scalar(input), scalar_set(-1.0), scalar_set(1.0));
		const scalard half_angle = scalar_acos(input_w);
		const scalard xyz_inv_len = vector_length_reciprocal3_as_scalar(input_v);
		vector4d result_xyz = vector_mul(input_v, scalar_mul(xyz_inv_len, half_angle));

		// If we are near the identity, xyz will be set to our input xyz which should be near zero
		// For a true identity input, we'll output zero
		const mask4d is_input_near_identity = vector_greater_than(vector_set(input_w), vector_set(1.0 - 1.0E-8));
		result_xyz = vector_select(is_input_near_identity, input_v, result_xyz);

		return vector_to_quat(vector_set_w(result_xyz, 0.0));
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the quaternion exponential for a 3D rotation.
	// The result might not be fully normalized if the input is near zero.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE quatd RTM_SIMD_CALL quat_rotation_exp(quatd_arg0 input) RTM_NO_EXCEPT
	{
		// The exponential equation is as follow:
		// exp(q).xyz = exp(q.w) * (q.xyz / ||q.xyz||) * sin(||q.xyz||)
		// exp(q).w = exp(q.w) * cos(||q.xyz||)
		//
		// If our output (normalized) quaternion is to represent a rotation, its logarithm passed as input has a W component equal to 0.0
		// Furthermore our input XYZ length is equal to our rotation half angle
		// Since exp(0.0) = 1.0, our equation simplifies as follow:
		// exp(q).xyz = (q.xyz / ||q.xyz||) * sin(||q.xyz||) = normalize(q.xyz) * sin(||q.xyz||)
		// exp(q).w = cos(||q.xyz||)
		//
		// If our half angle is 0.0, the whole input is 0,0,0,0 and the result is undefined since we can use any rotation axis.
		// When this happens, we return the approximated identity.
		// This is easily achieved because cos(||q.xyz||) = cos(0.0) = 1.0
		//
		// If our output quaternion does not represent a rotation, more math is required

		const vector4d input_v = quat_to_vector(input);
		const scalard input_len = vector_length3_as_scalar(input_v);
		const vector4d input_len_v = vector_set(input_len);
		const vector4d input_normalized = vector_div(input_v, input_len_v);
		const vector4d sincos = scalar_sincos(input_len);

		vector4d result_xyz = vector_mul(input_normalized, vector_dup_x(sincos));

		// If we are near zero, xyz will be set to our input xyz which should be near zero
		// For a true zero input, we'll output the identity
		const mask4d is_input_near_zero = vector_less_than(input_len_v, vector_set(1.0E-8));
		result_xyz = vector_select(is_input_near_zero, input_v, result_xyz);

		const vector4d result_w = vector_dup_y(sincos);
		const vector4d result = vector_mix<mix4::x, mix4::y, mix4::z, mix4::d>(result_xyz, result_w);
		return vector_to_quat(result);
	}



	//////////////////////////////////////////////////////////////////////////
	// Conversion to/from axis/angle/euler
	//////////////////////////////////////////////////////////////////////////



	//////////////////////////////////////////////////////////////////////////
	// Returns the rotation axis and rotation angle that make up the input quaternion.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline void RTM_SIMD_CALL quat_to_axis_angle(quatd_arg0 input, vector4d& out_axis, double& out_angle) RTM_NO_EXCEPT
	{
		constexpr double epsilon = 1.0E-8;
		constexpr double epsilon_squared = epsilon * epsilon;

		const scalard input_w = scalar_clamp(quat_get_w_as_scalar(input), scalar_set(-1.0), scalar_set(1.0));
		out_angle = scalar_cast(scalar_acos(input_w)) * 2.0;

		const double scale_sq = scalar_max(1.0 - quat_get_w(input) * quat_get_w(input), 0.0);
		out_axis = scale_sq >= epsilon_squared ? vector_mul(quat_to_vector(input), vector_set(scalar_sqrt_reciprocal(scale_sq))) : vector_set(1.0, 0.0, 0.0);
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the rotation axis part of the input quaternion.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline vector4d RTM_SIMD_CALL quat_get_axis(quatd_arg0 input) RTM_NO_EXCEPT
	{
		constexpr double epsilon = 1.0E-8;
		constexpr double epsilon_squared = epsilon * epsilon;

		const double scale_sq = scalar_max(1.0 - quat_get_w(input) * quat_get_w(input), 0.0);
		return scale_sq >= epsilon_squared ? vector_mul(quat_to_vector(input), vector_set(scalar_sqrt_reciprocal(scale_sq))) : vector_set(1.0, 0.0, 0.0);
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the rotation angle part of the input quaternion.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline double RTM_SIMD_CALL quat_get_angle(quatd_arg0 input) RTM_NO_EXCEPT
	{
		const scalard input_w = scalar_clamp(quat_get_w_as_scalar(input), scalar_set(-1.0), scalar_set(1.0));
		return scalar_cast(scalar_acos(input_w)) * 2.0;
	}

	//////////////////////////////////////////////////////////////////////////
	// Creates a quaternion from a rotation axis and a rotation angle.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline quatd RTM_SIMD_CALL quat_from_axis_angle(vector4d_arg0 axis, double angle) RTM_NO_EXCEPT
	{
		vector4d sincos_ = scalar_sincos(0.5 * angle);
		vector4d sin_ = vector_dup_x(sincos_);
		scalard cos_ = vector_get_y_as_scalar(sincos_);

		return vector_to_quat(vector_set_w(vector_mul(sin_, axis), cos_));
	}

	//////////////////////////////////////////////////////////////////////////
	// Creates a quaternion from Euler Pitch/Yaw/Roll angles.
	// Pitch is around the Y axis (right)
	// Yaw is around the Z axis (up)
	// Roll is around the X axis (forward)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline quatd RTM_SIMD_CALL quat_from_euler(double pitch, double yaw, double roll) RTM_NO_EXCEPT
	{
		double sp;
		double sy;
		double sr;
		double cp;
		double cy;
		double cr;

		scalar_sincos(pitch * 0.5, sp, cp);
		scalar_sincos(yaw * 0.5, sy, cy);
		scalar_sincos(roll * 0.5, sr, cr);

		return quat_set(cr * sp * sy - sr * cp * cy,
			-cr * sp * cy - sr * cp * sy,
			cr * cp * sy - sr * sp * cy,
			cr * cp * cy + sr * sp * sy);
	}



	//////////////////////////////////////////////////////////////////////////
	// Comparisons and masking
	//////////////////////////////////////////////////////////////////////////



	//////////////////////////////////////////////////////////////////////////
	// Returns true if the input quaternion does not contain any NaN or Inf, otherwise false.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline bool RTM_SIMD_CALL quat_is_finite(quatd_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		const __m128i abs_mask = _mm_set_epi64x(0x7FFFFFFFFFFFFFFFULL, 0x7FFFFFFFFFFFFFFFULL);
		__m128d abs_input_xy = _mm_and_pd(input.xy, _mm_castsi128_pd(abs_mask));
		__m128d abs_input_zw = _mm_and_pd(input.zw, _mm_castsi128_pd(abs_mask));

		const __m128d infinity = _mm_set1_pd(std::numeric_limits<double>::infinity());
		__m128d is_infinity_xy = _mm_cmpeq_pd(abs_input_xy, infinity);
		__m128d is_infinity_zw = _mm_cmpeq_pd(abs_input_zw, infinity);

		__m128d is_nan_xy = _mm_cmpneq_pd(input.xy, input.xy);
		__m128d is_nan_zw = _mm_cmpneq_pd(input.zw, input.zw);

		__m128d is_not_finite_xy = _mm_or_pd(is_infinity_xy, is_nan_xy);
		__m128d is_not_finite_zw = _mm_or_pd(is_infinity_zw, is_nan_zw);
		__m128d is_not_finite = _mm_or_pd(is_not_finite_xy, is_not_finite_zw);
		return _mm_movemask_pd(is_not_finite) == 0;
#else
		return scalar_is_finite(quat_get_x(input)) && scalar_is_finite(quat_get_y(input)) && scalar_is_finite(quat_get_z(input)) && scalar_is_finite(quat_get_w(input));
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if the input quaternion is normalized, otherwise false.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL quat_is_normalized(quatd_arg0 input, double threshold = 0.00001) RTM_NO_EXCEPT
	{
		double length_squared = quat_length_squared(input);
		return scalar_abs(length_squared - 1.0) <= threshold;
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if the two quaternions are equal component wise, otherwise false.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL quat_are_equal(quatd_arg0 lhs, quatd_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		__m128d xy_eq_pd = _mm_cmpeq_pd(lhs.xy, rhs.xy);
		__m128d zw_eq_pd = _mm_cmpeq_pd(lhs.zw, rhs.zw);
		return (_mm_movemask_pd(xy_eq_pd) & _mm_movemask_pd(zw_eq_pd)) == 3;
#else
		return lhs.x == rhs.x && lhs.y == rhs.y && lhs.z == rhs.z && lhs.w == rhs.w;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if the two quaternions are nearly equal component wise, otherwise false.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL quat_near_equal(quatd_arg0 lhs, quatd_arg1 rhs, double threshold = 0.00001) RTM_NO_EXCEPT
	{
		return vector_all_near_equal(quat_to_vector(lhs), quat_to_vector(rhs), threshold);
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if the input quaternion is nearly equal to the identity quaternion
	// by comparing its rotation angle.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline bool RTM_SIMD_CALL quat_near_identity(quatd_arg0 input, double threshold_angle = 0.00284714461) RTM_NO_EXCEPT
	{
		// See the quatf version of quat_near_identity for details.
		const scalard input_w = quat_get_w_as_scalar(input);
		const scalard input_abs_w = scalar_min(scalar_abs(input_w), scalar_set(1.0));
		const double positive_w_angle = scalar_acos(scalar_cast(input_abs_w)) * 2.0;
		return positive_w_angle <= threshold_angle;
	}

	//////////////////////////////////////////////////////////////////////////
	// Per component selection depending on the mask: mask != 0 ? if_true : if_false
	// Note that if the mask lanes are not all identical, the resulting quaternion
	// may not be normalized.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE quatd RTM_SIMD_CALL quat_select(mask4d_arg0 mask, quatd_arg1 if_true, quatd_arg2 if_false) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		__m128d xy = RTM_VECTOR2D_SELECT(mask.xy, if_true.xy, if_false.xy);
		__m128d zw = RTM_VECTOR2D_SELECT(mask.zw, if_true.zw, if_false.zw);
		return quatd{ xy, zw };
#else
		return quatd{ rtm_impl::select(mask.x, if_true.x, if_false.x), rtm_impl::select(mask.y, if_true.y, if_false.y), rtm_impl::select(mask.z, if_true.z, if_false.z), rtm_impl::select(mask.w, if_true.w, if_false.w) };
#endif
	}

	RTM_IMPL_VERSION_NAMESPACE_END
}

RTM_IMPL_FILE_PRAGMA_POP

```

`includes/rtm/quatf.h`:

```h
#pragma once

////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2017 Nicholas Frechette & Animation Compression Library contributors
// Copyright (c) 2018 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include "rtm/math.h"
#include "rtm/scalarf.h"
#include "rtm/vector4f.h"
#include "rtm/version.h"
#include "rtm/impl/compiler_utils.h"
#include "rtm/impl/macros.mask4.impl.h"
#include "rtm/impl/memory_utils.h"
#include "rtm/impl/quat_common.h"

#include <limits>

RTM_IMPL_FILE_PRAGMA_PUSH

namespace rtm
{
	RTM_IMPL_VERSION_NAMESPACE_BEGIN

	//////////////////////////////////////////////////////////////////////////
	// Setters, getters, and casts
	//////////////////////////////////////////////////////////////////////////



	//////////////////////////////////////////////////////////////////////////
	// Loads an unaligned quaternion from memory.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE quatf RTM_SIMD_CALL quat_load(const float* input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return _mm_loadu_ps(input);
#elif defined(RTM_NEON_INTRINSICS)
		return vld1q_f32(input);
#else
		return quat_set(input[0], input[1], input[2], input[3]);
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Loads an unaligned quat from memory.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE quatf RTM_SIMD_CALL quat_load(const float4f* input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return _mm_loadu_ps(&input->x);
#elif defined(RTM_NEON_INTRINSICS)
		return vld1q_f32(&input->x);
#else
		return quat_set(input->x, input->y, input->z, input->w);
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Casts a vector4 to a quaternion.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE quatf RTM_SIMD_CALL vector_to_quat(vector4f_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS) || defined(RTM_NEON_INTRINSICS)
		return input;
#else
		return quatf{ input.x, input.y, input.z, input.w };
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Casts a quaternion float64 variant to a float32 variant.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE quatf RTM_SIMD_CALL quat_cast(const quatd& input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return _mm_shuffle_ps(_mm_cvtpd_ps(input.xy), _mm_cvtpd_ps(input.zw), _MM_SHUFFLE(1, 0, 1, 0));
#else
		return quat_set(float(input.x), float(input.y), float(input.z), float(input.w));
#endif
	}

	namespace rtm_impl
	{
		//////////////////////////////////////////////////////////////////////////
		// This is a helper struct to allow a single consistent API between
		// various vector types when the semantics are identical but the return
		// type differs. Implicit coercion is used to return the desired value
		// at the call site.
		//////////////////////////////////////////////////////////////////////////
		struct quatf_quat_get_x
		{
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator float() const RTM_NO_EXCEPT
			{
#if defined(RTM_SSE2_INTRINSICS)
				return _mm_cvtss_f32(input);
#elif defined(RTM_NEON_INTRINSICS)
				return vgetq_lane_f32(input, 0);
#else
				return input.x;
#endif
			}

#if defined(RTM_SSE2_INTRINSICS)
			RTM_DEPRECATED("Use 'as_scalar' suffix instead. To be removed in 2.4.")
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator scalarf() const RTM_NO_EXCEPT
			{
				return scalarf{ input };
			}
#endif

			quatf input;
		};
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the quaternion [x] component (real part).
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr rtm_impl::quatf_quat_get_x RTM_SIMD_CALL quat_get_x(quatf_arg0 input) RTM_NO_EXCEPT
	{
		return rtm_impl::quatf_quat_get_x{ input };
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the quaternion [x] component (real part).
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE scalarf RTM_SIMD_CALL quat_get_x_as_scalar(quatf_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return scalarf{ input };
#else
		return quat_get_x(input);
#endif
	}

	namespace rtm_impl
	{
		//////////////////////////////////////////////////////////////////////////
		// This is a helper struct to allow a single consistent API between
		// various vector types when the semantics are identical but the return
		// type differs. Implicit coercion is used to return the desired value
		// at the call site.
		//////////////////////////////////////////////////////////////////////////
		struct quatf_quat_get_y
		{
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator float() const RTM_NO_EXCEPT
			{
#if defined(RTM_SSE2_INTRINSICS)
				return _mm_cvtss_f32(_mm_shuffle_ps(input, input, _MM_SHUFFLE(1, 1, 1, 1)));
#elif defined(RTM_NEON_INTRINSICS)
				return vgetq_lane_f32(input, 1);
#else
				return input.y;
#endif
			}

#if defined(RTM_SSE2_INTRINSICS)
			RTM_DEPRECATED("Use 'as_scalar' suffix instead. To be removed in 2.4.")
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator scalarf() const RTM_NO_EXCEPT
			{
				return scalarf{ _mm_shuffle_ps(input, input, _MM_SHUFFLE(1, 1, 1, 1)) };
			}
#endif

			quatf input;
		};
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the quaternion [y] component (real part).
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr rtm_impl::quatf_quat_get_y RTM_SIMD_CALL quat_get_y(quatf_arg0 input) RTM_NO_EXCEPT
	{
		return rtm_impl::quatf_quat_get_y{ input };
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the quaternion [y] component (real part).
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE scalarf RTM_SIMD_CALL quat_get_y_as_scalar(quatf_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return scalarf{ _mm_shuffle_ps(input, input, _MM_SHUFFLE(1, 1, 1, 1)) };
#else
		return quat_get_y(input);
#endif
	}

	namespace rtm_impl
	{
		//////////////////////////////////////////////////////////////////////////
		// This is a helper struct to allow a single consistent API between
		// various vector types when the semantics are identical but the return
		// type differs. Implicit coercion is used to return the desired value
		// at the call site.
		//////////////////////////////////////////////////////////////////////////
		struct quatf_quat_get_z
		{
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator float() const RTM_NO_EXCEPT
			{
#if defined(RTM_SSE2_INTRINSICS)
				return _mm_cvtss_f32(_mm_shuffle_ps(input, input, _MM_SHUFFLE(2, 2, 2, 2)));
#elif defined(RTM_NEON_INTRINSICS)
				return vgetq_lane_f32(input, 2);
#else
				return input.z;
#endif
			}

#if defined(RTM_SSE2_INTRINSICS)
			RTM_DEPRECATED("Use 'as_scalar' suffix instead. To be removed in 2.4.")
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator scalarf() const RTM_NO_EXCEPT
			{
				return scalarf{ _mm_shuffle_ps(input, input, _MM_SHUFFLE(2, 2, 2, 2)) };
			}
#endif

			quatf input;
		};
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the quaternion [z] component (real part).
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr rtm_impl::quatf_quat_get_z RTM_SIMD_CALL quat_get_z(quatf_arg0 input) RTM_NO_EXCEPT
	{
		return rtm_impl::quatf_quat_get_z{ input };
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the quaternion [z] component (real part).
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE scalarf RTM_SIMD_CALL quat_get_z_as_scalar(quatf_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return scalarf{ _mm_shuffle_ps(input, input, _MM_SHUFFLE(2, 2, 2, 2)) };
#else
		return quat_get_z(input);
#endif
	}

	namespace rtm_impl
	{
		//////////////////////////////////////////////////////////////////////////
		// This is a helper struct to allow a single consistent API between
		// various vector types when the semantics are identical but the return
		// type differs. Implicit coercion is used to return the desired value
		// at the call site.
		//////////////////////////////////////////////////////////////////////////
		struct quatf_quat_get_w
		{
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator float() const RTM_NO_EXCEPT
			{
#if defined(RTM_SSE2_INTRINSICS)
				return _mm_cvtss_f32(_mm_shuffle_ps(input, input, _MM_SHUFFLE(3, 3, 3, 3)));
#elif defined(RTM_NEON_INTRINSICS)
				return vgetq_lane_f32(input, 3);
#else
				return input.w;
#endif
			}

#if defined(RTM_SSE2_INTRINSICS)
			RTM_DEPRECATED("Use 'as_scalar' suffix instead. To be removed in 2.4.")
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator scalarf() const RTM_NO_EXCEPT
			{
				return scalarf{ _mm_shuffle_ps(input, input, _MM_SHUFFLE(3, 3, 3, 3)) };
			}
#endif

			quatf input;
		};
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the quaternion [w] component (imaginary part).
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr rtm_impl::quatf_quat_get_w RTM_SIMD_CALL quat_get_w(quatf_arg0 input) RTM_NO_EXCEPT
	{
		return rtm_impl::quatf_quat_get_w{ input };
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the quaternion [w] component (imaginary part).
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE scalarf RTM_SIMD_CALL quat_get_w_as_scalar(quatf_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return scalarf{ _mm_shuffle_ps(input, input, _MM_SHUFFLE(3, 3, 3, 3)) };
#else
		return quat_get_w(input);
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Sets the quaternion [x] component (real part) and returns the new value.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE quatf RTM_SIMD_CALL quat_set_x(quatf_arg0 input, float lane_value) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return _mm_move_ss(input, _mm_set_ss(lane_value));
#elif defined(RTM_NEON_INTRINSICS)
		return vsetq_lane_f32(lane_value, input, 0);
#else
		return quatf{ lane_value, input.y, input.z, input.w };
#endif
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Sets the quaternion [x] component (real part) and returns the new value.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE quatf RTM_SIMD_CALL quat_set_x(quatf_arg0 input, scalarf_arg1 lane_value) RTM_NO_EXCEPT
	{
		return _mm_move_ss(input, lane_value.value);
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Sets the quaternion [y] component (real part) and returns the new value.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE quatf RTM_SIMD_CALL quat_set_y(quatf_arg0 input, float lane_value) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE4_INTRINSICS)
		return _mm_insert_ps(input, _mm_set_ss(lane_value), 0x10);
#elif defined(RTM_SSE2_INTRINSICS)
		const __m128 yxzw = _mm_shuffle_ps(input, input, _MM_SHUFFLE(3, 2, 0, 1));
		const __m128 vxzw = _mm_move_ss(yxzw, _mm_set_ss(lane_value));
		return _mm_shuffle_ps(vxzw, vxzw, _MM_SHUFFLE(3, 2, 0, 1));
#elif defined(RTM_NEON_INTRINSICS)
		return vsetq_lane_f32(lane_value, input, 1);
#else
		return quatf{ input.x, lane_value, input.z, input.w };
#endif
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Sets the quaternion [y] component (real part) and returns the new value.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE quatf RTM_SIMD_CALL quat_set_y(quatf_arg0 input, scalarf_arg1 lane_value) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE4_INTRINSICS)
		return _mm_insert_ps(input, lane_value.value, 0x10);
#else
		const __m128 yxzw = _mm_shuffle_ps(input, input, _MM_SHUFFLE(3, 2, 0, 1));
		const __m128 vxzw = _mm_move_ss(yxzw, lane_value.value);
		return _mm_shuffle_ps(vxzw, vxzw, _MM_SHUFFLE(3, 2, 0, 1));
#endif
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Sets the quaternion [z] component (real part) and returns the new value.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE quatf RTM_SIMD_CALL quat_set_z(quatf_arg0 input, float lane_value) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE4_INTRINSICS)
		return _mm_insert_ps(input, _mm_set_ss(lane_value), 0x20);
#elif defined(RTM_SSE2_INTRINSICS)
		const __m128 zyxw = _mm_shuffle_ps(input, input, _MM_SHUFFLE(3, 0, 1, 2));
		const __m128 vyxw = _mm_move_ss(zyxw, _mm_set_ss(lane_value));
		return _mm_shuffle_ps(vyxw, vyxw, _MM_SHUFFLE(3, 0, 1, 2));
#elif defined(RTM_NEON_INTRINSICS)
		return vsetq_lane_f32(lane_value, input, 2);
#else
		return quatf{ input.x, input.y, lane_value, input.w };
#endif
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Sets the quaternion [z] component (real part) and returns the new value.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE quatf RTM_SIMD_CALL quat_set_z(quatf_arg0 input, scalarf_arg1 lane_value) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE4_INTRINSICS)
		return _mm_insert_ps(input, lane_value.value, 0x20);
#else
		const __m128 zyxw = _mm_shuffle_ps(input, input, _MM_SHUFFLE(3, 0, 1, 2));
		const __m128 vyxw = _mm_move_ss(zyxw, lane_value.value);
		return _mm_shuffle_ps(vyxw, vyxw, _MM_SHUFFLE(3, 0, 1, 2));
#endif
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Sets the quaternion [w] component (imaginary part) and returns the new value.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE quatf RTM_SIMD_CALL quat_set_w(quatf_arg0 input, float lane_value) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE4_INTRINSICS)
		return _mm_insert_ps(input, _mm_set_ss(lane_value), 0x30);
#elif defined(RTM_SSE2_INTRINSICS)
		const __m128 wyzx = _mm_shuffle_ps(input, input, _MM_SHUFFLE(0, 2, 1, 3));
		const __m128 vyzx = _mm_move_ss(wyzx, _mm_set_ss(lane_value));
		return _mm_shuffle_ps(vyzx, vyzx, _MM_SHUFFLE(0, 2, 1, 3));
#elif defined(RTM_NEON_INTRINSICS)
		return vsetq_lane_f32(lane_value, input, 3);
#else
		return quatf{ input.x, input.y, input.z, lane_value };
#endif
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Sets the quaternion [w] component (imaginary part) and returns the new value.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE quatf RTM_SIMD_CALL quat_set_w(quatf_arg0 input, scalarf_arg1 lane_value) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE4_INTRINSICS)
		return _mm_insert_ps(input, lane_value.value, 0x30);
#else
		const __m128 wyzx = _mm_shuffle_ps(input, input, _MM_SHUFFLE(0, 2, 1, 3));
		const __m128 vyzx = _mm_move_ss(wyzx, lane_value.value);
		return _mm_shuffle_ps(vyzx, vyzx, _MM_SHUFFLE(0, 2, 1, 3));
#endif
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Writes a quaternion to unaligned memory.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE void RTM_SIMD_CALL quat_store(quatf_arg0 input, float* output) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		_mm_storeu_ps(output, input);
#else
		output[0] = quat_get_x(input);
		output[1] = quat_get_y(input);
		output[2] = quat_get_z(input);
		output[3] = quat_get_w(input);
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Writes a quaternion to unaligned memory.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE void RTM_SIMD_CALL quat_store(quatf_arg0 input, float4f* output) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		_mm_storeu_ps(&output->x, input);
#else
		output->x = quat_get_x(input);
		output->y = quat_get_y(input);
		output->z = quat_get_z(input);
		output->w = quat_get_w(input);
#endif
	}



	//////////////////////////////////////////////////////////////////////////
	// Arithmetic
	//////////////////////////////////////////////////////////////////////////



	//////////////////////////////////////////////////////////////////////////
	// Returns the quaternion conjugate.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE quatf RTM_SIMD_CALL quat_conjugate(quatf_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		constexpr __m128 signs = RTM_VECTOR4F_MAKE(-0.0F, -0.0F, -0.0F, 0.0F);
		return _mm_xor_ps(input, signs);
#else
		// On ARMv7 the scalar version performs best or among the best while on ARM64 it beats the others.
		return quat_set(-quat_get_x(input), -quat_get_y(input), -quat_get_z(input), quat_get_w(input));
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Adds two quaternions.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline quatf RTM_SIMD_CALL quat_add(quatf_arg0 lhs, quatf_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return _mm_add_ps(lhs, rhs);
#elif defined(RTM_NEON_INTRINSICS)
		return vaddq_f32(lhs, rhs);
#else
		return quat_set(quat_get_x(lhs) + quat_get_x(rhs), quat_get_y(lhs) + quat_get_y(rhs), quat_get_z(lhs) + quat_get_z(rhs), quat_get_w(lhs) + quat_get_w(rhs));
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Multiplies two quaternions.
	// Note that due to floating point rounding, the result might not be perfectly normalized.
	// Multiplication order is as follow: local_to_world = quat_mul(local_to_object, object_to_world)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline quatf RTM_SIMD_CALL quat_mul(quatf_arg0 lhs, quatf_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE4_INTRINSICS) && 0
		// TODO: Profile this, the accuracy is the same as with SSE2, should be binary exact
		constexpr __m128 signs_x = RTM_VECTOR4F_MAKE(1.0F,  1.0F,  1.0F, -1.0F);
		constexpr __m128 signs_y = RTM_VECTOR4F_MAKE(1.0F, -1.0F,  1.0F,  1.0F);
		constexpr __m128 signs_z = RTM_VECTOR4F_MAKE(1.0F,  1.0F, -1.0F,  1.0F);
		constexpr __m128 signs_w = RTM_VECTOR4F_MAKE(1.0F, -1.0F, -1.0F, -1.0F);
		// x = dot(rhs.wxyz, lhs.xwzy * signs_x)
		// y = dot(rhs.wxyz, lhs.yzwx * signs_y)
		// z = dot(rhs.wxyz, lhs.zyxw * signs_z)
		// w = dot(rhs.wxyz, lhs.wxyz * signs_w)
		__m128 rhs_wxyz = _mm_shuffle_ps(rhs, rhs, _MM_SHUFFLE(2, 1, 0, 3));
		__m128 lhs_xwzy = _mm_shuffle_ps(lhs, lhs, _MM_SHUFFLE(1, 2, 3, 0));
		__m128 lhs_yzwx = _mm_shuffle_ps(lhs, lhs, _MM_SHUFFLE(0, 3, 2, 1));
		__m128 lhs_zyxw = _mm_shuffle_ps(lhs, lhs, _MM_SHUFFLE(3, 0, 1, 2));
		__m128 lhs_wxyz = _mm_shuffle_ps(lhs, lhs, _MM_SHUFFLE(2, 1, 0, 3));
		__m128 x = _mm_dp_ps(rhs_wxyz, _mm_mul_ps(lhs_xwzy, signs_x), 0xFF);
		__m128 y = _mm_dp_ps(rhs_wxyz, _mm_mul_ps(lhs_yzwx, signs_y), 0xFF);
		__m128 z = _mm_dp_ps(rhs_wxyz, _mm_mul_ps(lhs_zyxw, signs_z), 0xFF);
		__m128 w = _mm_dp_ps(rhs_wxyz, _mm_mul_ps(lhs_wxyz, signs_w), 0xFF);
		__m128 xxyy = _mm_shuffle_ps(x, y, _MM_SHUFFLE(0, 0, 0, 0));
		__m128 zzww = _mm_shuffle_ps(z, w, _MM_SHUFFLE(0, 0, 0, 0));
		return _mm_shuffle_ps(xxyy, zzww, _MM_SHUFFLE(2, 0, 2, 0));
#elif defined(RTM_SSE2_INTRINSICS)
		constexpr __m128 control_wzyx = RTM_VECTOR4F_MAKE(0.0F, -0.0F,  0.0F, -0.0F);
		constexpr __m128 control_zwxy = RTM_VECTOR4F_MAKE(0.0F,  0.0F, -0.0F, -0.0F);
		constexpr __m128 control_yxwz = RTM_VECTOR4F_MAKE(-0.0F,  0.0F,  0.0F, -0.0F);

		const __m128 r_xxxx = _mm_shuffle_ps(rhs, rhs, _MM_SHUFFLE(0, 0, 0, 0));
		const __m128 r_yyyy = _mm_shuffle_ps(rhs, rhs, _MM_SHUFFLE(1, 1, 1, 1));
		const __m128 r_zzzz = _mm_shuffle_ps(rhs, rhs, _MM_SHUFFLE(2, 2, 2, 2));
		const __m128 r_wwww = _mm_shuffle_ps(rhs, rhs, _MM_SHUFFLE(3, 3, 3, 3));

		const __m128 lxrw_lyrw_lzrw_lwrw = _mm_mul_ps(r_wwww, lhs);
		const __m128 l_wzyx = _mm_shuffle_ps(lhs, lhs,_MM_SHUFFLE(0, 1, 2, 3));

		const __m128 lwrx_lzrx_lyrx_lxrx = _mm_mul_ps(r_xxxx, l_wzyx);
		const __m128 l_zwxy = _mm_shuffle_ps(l_wzyx, l_wzyx,_MM_SHUFFLE(2, 3, 0, 1));

		const __m128 lwrx_nlzrx_lyrx_nlxrx = _mm_xor_ps(lwrx_lzrx_lyrx_lxrx, control_wzyx);

		const __m128 lzry_lwry_lxry_lyry = _mm_mul_ps(r_yyyy, l_zwxy);
		const __m128 l_yxwz = _mm_shuffle_ps(l_zwxy, l_zwxy,_MM_SHUFFLE(0, 1, 2, 3));

		const __m128 lzry_lwry_nlxry_nlyry = _mm_xor_ps(lzry_lwry_lxry_lyry, control_zwxy);

		const __m128 lyrz_lxrz_lwrz_lzrz = _mm_mul_ps(r_zzzz, l_yxwz);
		const __m128 result0 = _mm_add_ps(lxrw_lyrw_lzrw_lwrw, lwrx_nlzrx_lyrx_nlxrx);

		const __m128 nlyrz_lxrz_lwrz_nlzrz = _mm_xor_ps(lyrz_lxrz_lwrz_lzrz, control_yxwz);
		const __m128 result1 = _mm_add_ps(lzry_lwry_nlxry_nlyry, nlyrz_lxrz_lwrz_nlzrz);
		return _mm_add_ps(result0, result1);
#elif defined(RTM_NEON64_INTRINSICS)
		// Use shuffles and negation instead of loading constants and doing mul/xor.
		// On ARM64, this is the fastest version.

		// Dispatch rev first, if we can't dual dispatch with neg below, we won't stall it
		// [l.y, l.x, l.w, l.z]
		const float32x4_t y_x_w_z = vrev64q_f32(lhs);

		// [-l.x, -l.y, -l.z, -l.w]
		const float32x4_t neg_lhs = vnegq_f32(lhs);

		// trn([l.y, l.x, l.w, l.z], [-l.x, -l.y, -l.z, -l.w]) = [l.y, -l.x, l.w, -l.z], [l.x, -l.y, l.z, -l.w]
		float32x4x2_t y_nx_w_nz__x_ny_z_nw = vtrnq_f32(y_x_w_z, neg_lhs);

		// [l.w, -l.z, l.y, -l.x]
		float32x4_t l_wzyx = vcombine_f32(vget_high_f32(y_nx_w_nz__x_ny_z_nw.val[0]), vget_low_f32(y_nx_w_nz__x_ny_z_nw.val[0]));

		// [l.z, l.w, -l.x, -l.y]
		float32x4_t l_zwxy = vcombine_f32(vget_high_f32(lhs), vget_low_f32(neg_lhs));

		// neg([l.w, -l.z, l.y, -l.x]) = [-l.w, l.z, -l.y, l.x]
		float32x4_t nw_z_ny_x = vnegq_f32(l_wzyx);

		// [-l.y, l.x, l.w, -l.z]
		float32x4_t l_yxwz = vcombine_f32(vget_high_f32(nw_z_ny_x), vget_low_f32(l_wzyx));

		const float32x2_t r_xy = vget_low_f32(rhs);
		const float32x2_t r_zw = vget_high_f32(rhs);

		const float32x4_t lxrw_lyrw_lzrw_lwrw = vmulq_lane_f32(lhs, r_zw, 1);

	#if defined(RTM_NEON64_INTRINSICS)
		const float32x4_t result0 = vfmaq_lane_f32(lxrw_lyrw_lzrw_lwrw, l_wzyx, r_xy, 0);
		const float32x4_t result1 = vfmaq_lane_f32(result0, l_zwxy, r_xy, 1);
		return vfmaq_lane_f32(result1, l_yxwz, r_zw, 0);
	#else
		const float32x4_t result0 = vmlaq_lane_f32(lxrw_lyrw_lzrw_lwrw, l_wzyx, r_xy, 0);
		const float32x4_t result1 = vmlaq_lane_f32(result0, l_zwxy, r_xy, 1);
		return vmlaq_lane_f32(result1, l_yxwz, r_zw, 0);
	#endif
#else
		// On ARMv7, the scalar version is often the fastest.
		const float lhs_x = quat_get_x(lhs);
		const float lhs_y = quat_get_y(lhs);
		const float lhs_z = quat_get_z(lhs);
		const float lhs_w = quat_get_w(lhs);

		const float rhs_x = quat_get_x(rhs);
		const float rhs_y = quat_get_y(rhs);
		const float rhs_z = quat_get_z(rhs);
		const float rhs_w = quat_get_w(rhs);

		const float x = (rhs_w * lhs_x) + (rhs_x * lhs_w) + (rhs_y * lhs_z) - (rhs_z * lhs_y);
		const float y = (rhs_w * lhs_y) - (rhs_x * lhs_z) + (rhs_y * lhs_w) + (rhs_z * lhs_x);
		const float z = (rhs_w * lhs_z) + (rhs_x * lhs_y) - (rhs_y * lhs_x) + (rhs_z * lhs_w);
		const float w = (rhs_w * lhs_w) - (rhs_x * lhs_x) - (rhs_y * lhs_y) - (rhs_z * lhs_z);

		return quat_set(x, y, z, w);
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Multiplies a quaternion with a scalar.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE quatf RTM_SIMD_CALL quat_mul(quatf_arg0 quat, float scalar) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return _mm_mul_ps(quat, _mm_set_ps1(scalar));
#elif defined(RTM_NEON_INTRINSICS)
		return vmulq_n_f32(quat, scalar);
#else
		return quat_set(quat_get_x(quat) * scalar, quat_get_y(quat) * scalar, quat_get_z(quat) * scalar, quat_get_w(quat) * scalar);
#endif
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Multiplies a quaternion with a scalar.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE quatf RTM_SIMD_CALL quat_mul(quatf_arg0 quat, scalarf_arg1 scalar) RTM_NO_EXCEPT
	{
		return _mm_mul_ps(quat, _mm_shuffle_ps(scalar.value, scalar.value, _MM_SHUFFLE(0, 0, 0, 0)));
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Multiplies a quaternion and a 3D vector, rotating it.
	// Multiplication order is as follow: world_position = quat_mul_vector3(local_vector, local_to_world)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline vector4f RTM_SIMD_CALL quat_mul_vector3(vector4f_arg0 vector, quatf_arg1 rotation) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		const __m128 inv_rotation = quat_conjugate(rotation);

		// Normally when we multiply our inverse rotation quaternion with the input vector as a quaternion with W = 0.0.
		// As a result, we can strip the whole part that uses W saving a few instructions.
		// Because we have the rotation and its inverse, we also can use them to avoid flipping the signs
		// when lining up our SIMD additions. For the first quaternion multiplication, we can avoid 3 XORs by
		// doing 2 shuffles instead. The same trick can also be used with the second quaternion multiplication.
		// We also don't care about the result W lane but it comes for free.

		// temp = quat_mul(inv_rotation, vector_quat)
		__m128 temp;
		{
			const __m128 rotation_tmp0 = _mm_shuffle_ps(rotation, inv_rotation, _MM_SHUFFLE(3, 0, 2, 1));		// r.y, r.z, -r.x, -r.w
			const __m128 rotation_tmp1 = _mm_shuffle_ps(rotation, inv_rotation, _MM_SHUFFLE(3, 1, 2, 0));		// r.x, r.z, -r.y, -r.w

			const __m128 v_xxxx = _mm_shuffle_ps(vector, vector, _MM_SHUFFLE(0, 0, 0, 0));
			const __m128 v_yyyy = _mm_shuffle_ps(vector, vector, _MM_SHUFFLE(1, 1, 1, 1));
			const __m128 v_zzzz = _mm_shuffle_ps(vector, vector, _MM_SHUFFLE(2, 2, 2, 2));

			const __m128 rotation_tmp2 = _mm_shuffle_ps(rotation_tmp0, rotation_tmp1, _MM_SHUFFLE(0, 2, 1, 3));	// -r.w, r.z, -r.y, r.x
			const __m128 lwrx_lzrx_lyrx_lxrx = _mm_mul_ps(v_xxxx, rotation_tmp2);

			const __m128 rotation_tmp3 = _mm_shuffle_ps(inv_rotation, rotation, _MM_SHUFFLE(1, 0, 3, 2));		// -r.z, -r.w, r.x, r.y
			const __m128 lzry_lwry_lxry_lyry = _mm_mul_ps(v_yyyy, rotation_tmp3);

			const __m128 rotation_tmp4 = _mm_shuffle_ps(rotation_tmp0, rotation_tmp1, _MM_SHUFFLE(1, 3, 2, 0));	// r.y, -r.x, -r.w, r.z
			const __m128 lyrz_lxrz_lwrz_lzrz = _mm_mul_ps(v_zzzz, rotation_tmp4);

			temp = _mm_add_ps(_mm_add_ps(lwrx_lzrx_lyrx_lxrx, lzry_lwry_lxry_lyry), lyrz_lxrz_lwrz_lzrz);
		}

		// result = quat_mul(temp, rotation)
		{
			const __m128 rotation_tmp0 = _mm_shuffle_ps(rotation, inv_rotation, _MM_SHUFFLE(2, 0, 2, 0));		// r.x, r.z, -r.x, -r.z

			__m128 r_xxxx = _mm_shuffle_ps(rotation_tmp0, rotation_tmp0, _MM_SHUFFLE(2, 0, 2, 0));				// r.x, -r.x, r.x, -r.x
			__m128 r_yyyy = _mm_shuffle_ps(rotation, inv_rotation, _MM_SHUFFLE(1, 1, 1, 1));					// r.y, r.y, -r.y, -r.y
			__m128 r_zzzz = _mm_shuffle_ps(rotation_tmp0, rotation_tmp0, _MM_SHUFFLE(3, 1, 1, 3));				// -r.z, r.z, r.z, -r.z
			__m128 r_wwww = _mm_shuffle_ps(rotation, rotation, _MM_SHUFFLE(3, 3, 3, 3));						// r.w, r.w, r.w, r.w

			__m128 lxrw_lyrw_lzrw_lwrw = _mm_mul_ps(r_wwww, temp);

			__m128 t_wzyx = _mm_shuffle_ps(temp, temp, _MM_SHUFFLE(0, 1, 2, 3));
			__m128 lwrx_lzrx_lyrx_lxrx = _mm_mul_ps(r_xxxx, t_wzyx);

			__m128 t_zwxy = _mm_shuffle_ps(t_wzyx, t_wzyx, _MM_SHUFFLE(2, 3, 0, 1));
			__m128 lzry_lwry_lxry_lyry = _mm_mul_ps(r_yyyy, t_zwxy);

			__m128 t_yxwz = _mm_shuffle_ps(t_zwxy, t_zwxy, _MM_SHUFFLE(0, 1, 2, 3));
			__m128 lyrz_lxrz_lwrz_lzrz = _mm_mul_ps(r_zzzz, t_yxwz);

			__m128 result0 = _mm_add_ps(lxrw_lyrw_lzrw_lwrw, lwrx_lzrx_lyrx_lxrx);
			__m128 result1 = _mm_add_ps(lzry_lwry_lxry_lyry, lyrz_lxrz_lwrz_lzrz);
			return _mm_add_ps(result0, result1);
		}
#elif defined(RTM_NEON_INTRINSICS)
		// On ARMv7 and ARM64, the scalar version is often the fastest.
		const float32x4_t n_rotation = vnegq_f32(rotation);

		// temp = quat_mul(inv_rotation, vector_quat)
		float temp_x;
		float temp_y;
		float temp_z;
		float temp_w;
		{
			const float lhs_x = quat_get_x(n_rotation);
			const float lhs_y = quat_get_y(n_rotation);
			const float lhs_z = quat_get_z(n_rotation);
			const float lhs_w = quat_get_w(rotation);

			const float rhs_x = quat_get_x(vector);
			const float rhs_y = quat_get_y(vector);
			const float rhs_z = quat_get_z(vector);

			temp_x = (rhs_x * lhs_w) + (rhs_y * lhs_z) - (rhs_z * lhs_y);
			temp_y = -(rhs_x * lhs_z) + (rhs_y * lhs_w) + (rhs_z * lhs_x);
			temp_z = (rhs_x * lhs_y) - (rhs_y * lhs_x) + (rhs_z * lhs_w);
			temp_w = -(rhs_x * lhs_x) - (rhs_y * lhs_y) - (rhs_z * lhs_z);
		}

		// result = quat_mul(temp, rotation)
		{
			const float lhs_x = temp_x;
			const float lhs_y = temp_y;
			const float lhs_z = temp_z;
			const float lhs_w = temp_w;

			const float rhs_x = quat_get_x(rotation);
			const float rhs_y = quat_get_y(rotation);
			const float rhs_z = quat_get_z(rotation);
			const float rhs_w = quat_get_w(rotation);

			const float x = (rhs_w * lhs_x) + (rhs_x * lhs_w) + (rhs_y * lhs_z) - (rhs_z * lhs_y);
			const float y = (rhs_w * lhs_y) - (rhs_x * lhs_z) + (rhs_y * lhs_w) + (rhs_z * lhs_x);
			const float z = (rhs_w * lhs_z) + (rhs_x * lhs_y) - (rhs_y * lhs_x) + (rhs_z * lhs_w);

			return vector_set(x, y, z, z);
		}
#else
		quatf vector_quat = quat_set_w(vector_to_quat(vector), 0.0f);
		quatf inv_rotation = quat_conjugate(rotation);
		return quat_to_vector(quat_mul(quat_mul(inv_rotation, vector_quat), rotation));
#endif
	}

	namespace rtm_impl
	{
		//////////////////////////////////////////////////////////////////////////
		// This is a helper struct to allow a single consistent API between
		// various vector types when the semantics are identical but the return
		// type differs. Implicit coercion is used to return the desired value
		// at the call site.
		//////////////////////////////////////////////////////////////////////////
		struct quatf_quat_dot
		{
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator float() const RTM_NO_EXCEPT
			{
#if defined(RTM_SSE4_INTRINSICS) && 0
				// SSE4 dot product instruction appears slower on Zen2, is it the case elsewhere as well?
				return _mm_cvtss_f32(_mm_dp_ps(lhs, rhs, 0xFF));
#elif defined(RTM_SSE2_INTRINSICS)
				__m128 x2_y2_z2_w2 = _mm_mul_ps(lhs, rhs);
				__m128 z2_w2_0_0 = _mm_shuffle_ps(x2_y2_z2_w2, x2_y2_z2_w2, _MM_SHUFFLE(0, 0, 3, 2));
				__m128 x2z2_y2w2_0_0 = _mm_add_ps(x2_y2_z2_w2, z2_w2_0_0);
				__m128 y2w2_0_0_0 = _mm_shuffle_ps(x2z2_y2w2_0_0, x2z2_y2w2_0_0, _MM_SHUFFLE(0, 0, 0, 1));
				__m128 x2y2z2w2_0_0_0 = _mm_add_ps(x2z2_y2w2_0_0, y2w2_0_0_0);
				return _mm_cvtss_f32(x2y2z2w2_0_0_0);
#else
				return (quat_get_x(lhs) * quat_get_x(rhs)) + (quat_get_y(lhs) * quat_get_y(rhs)) + (quat_get_z(lhs) * quat_get_z(rhs)) + (quat_get_w(lhs) * quat_get_w(rhs));
#endif
			}

#if defined(RTM_SSE2_INTRINSICS)
			RTM_DEPRECATED("Use 'as_scalar' suffix instead. To be removed in 2.4.")
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator scalarf() const RTM_NO_EXCEPT
			{
#if defined(RTM_SSE4_INTRINSICS) && 0
				// SSE4 dot product instruction appears slower on Zen2, is it the case elsewhere as well?
				return scalarf{ _mm_cvtss_f32(_mm_dp_ps(lhs, rhs, 0xFF)) };
#else
				__m128 x2_y2_z2_w2 = _mm_mul_ps(lhs, rhs);
				__m128 z2_w2_0_0 = _mm_shuffle_ps(x2_y2_z2_w2, x2_y2_z2_w2, _MM_SHUFFLE(0, 0, 3, 2));
				__m128 x2z2_y2w2_0_0 = _mm_add_ps(x2_y2_z2_w2, z2_w2_0_0);
				__m128 y2w2_0_0_0 = _mm_shuffle_ps(x2z2_y2w2_0_0, x2z2_y2w2_0_0, _MM_SHUFFLE(0, 0, 0, 1));
				__m128 x2y2z2w2_0_0_0 = _mm_add_ps(x2z2_y2w2_0_0, y2w2_0_0_0);
				return scalarf{ x2y2z2w2_0_0_0 };
#endif
			}
#endif

			quatf lhs;
			quatf rhs;
		};
	}

	//////////////////////////////////////////////////////////////////////////
	// Quaternion dot product: lhs . rhs
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr rtm_impl::quatf_quat_dot RTM_SIMD_CALL quat_dot(quatf_arg0 lhs, quatf_arg1 rhs) RTM_NO_EXCEPT
	{
		return rtm_impl::quatf_quat_dot{ lhs, rhs };
	}

	//////////////////////////////////////////////////////////////////////////
	// Quaternion dot product: lhs . rhs
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE scalarf RTM_SIMD_CALL quat_dot_as_scalar(quatf_arg0 lhs, quatf_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
	#if defined(RTM_SSE4_INTRINSICS) && 0
		// SSE4 dot product instruction appears slower on Zen2, is it the case elsewhere as well?
		return scalarf{ _mm_cvtss_f32(_mm_dp_ps(lhs, rhs, 0xFF)) };
	#else
		__m128 x2_y2_z2_w2 = _mm_mul_ps(lhs, rhs);
		__m128 z2_w2_0_0 = _mm_shuffle_ps(x2_y2_z2_w2, x2_y2_z2_w2, _MM_SHUFFLE(0, 0, 3, 2));
		__m128 x2z2_y2w2_0_0 = _mm_add_ps(x2_y2_z2_w2, z2_w2_0_0);
		__m128 y2w2_0_0_0 = _mm_shuffle_ps(x2z2_y2w2_0_0, x2z2_y2w2_0_0, _MM_SHUFFLE(0, 0, 0, 1));
		__m128 x2y2z2w2_0_0_0 = _mm_add_ps(x2z2_y2w2_0_0, y2w2_0_0_0);
		return scalarf{ x2y2z2w2_0_0_0 };
	#endif
#else
		return quat_dot(lhs, rhs);
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the squared length/norm of the quaternion.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr rtm_impl::quatf_quat_dot RTM_SIMD_CALL quat_length_squared(quatf_arg0 input) RTM_NO_EXCEPT
	{
		return rtm_impl::quatf_quat_dot{ input, input };
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the squared length/norm of the quaternion.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE scalarf RTM_SIMD_CALL quat_length_squared_as_scalar(quatf_arg0 input) RTM_NO_EXCEPT
	{
		return quat_dot_as_scalar(input, input);
	}

	namespace rtm_impl
	{
		//////////////////////////////////////////////////////////////////////////
		// This is a helper struct to allow a single consistent API between
		// various vector types when the semantics are identical but the return
		// type differs. Implicit coercion is used to return the desired value
		// at the call site.
		//////////////////////////////////////////////////////////////////////////
		struct quatf_quat_length
		{
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator float() const RTM_NO_EXCEPT
			{
				const scalarf len_sq = quat_length_squared_as_scalar(input);
				return scalar_cast(scalar_sqrt(len_sq));
			}

#if defined(RTM_SSE2_INTRINSICS)
			RTM_DEPRECATED("Use 'as_scalar' suffix instead. To be removed in 2.4.")
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator scalarf() const RTM_NO_EXCEPT
			{
				const scalarf len_sq = quat_length_squared_as_scalar(input);
				return scalar_sqrt(len_sq);
			}
#endif

			quatf input;
		};
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the length/norm of the quaternion.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr rtm_impl::quatf_quat_length RTM_SIMD_CALL quat_length(quatf_arg0 input) RTM_NO_EXCEPT
	{
		return rtm_impl::quatf_quat_length{ input };
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the length/norm of the quaternion.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE scalarf RTM_SIMD_CALL quat_length_as_scalar(quatf_arg0 input) RTM_NO_EXCEPT
	{
		const scalarf len_sq = quat_length_squared_as_scalar(input);
		return scalar_sqrt(len_sq);
	}

	namespace rtm_impl
	{
		//////////////////////////////////////////////////////////////////////////
		// This is a helper struct to allow a single consistent API between
		// various vector types when the semantics are identical but the return
		// type differs. Implicit coercion is used to return the desired value
		// at the call site.
		//////////////////////////////////////////////////////////////////////////
		struct quatf_quat_length_reciprocal
		{
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator float() const RTM_NO_EXCEPT
			{
				const scalarf len_sq = quat_length_squared_as_scalar(input);
				return scalar_cast(scalar_sqrt_reciprocal(len_sq));
			}

#if defined(RTM_SSE2_INTRINSICS)
			RTM_DEPRECATED("Use 'as_scalar' suffix instead. To be removed in 2.4.")
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator scalarf() const RTM_NO_EXCEPT
			{
				const scalarf len_sq = quat_length_squared_as_scalar(input);
				return scalar_sqrt_reciprocal(len_sq);
			}
#endif

			quatf input;
		};
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the reciprocal length/norm of the quaternion.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr rtm_impl::quatf_quat_length_reciprocal RTM_SIMD_CALL quat_length_reciprocal(quatf_arg0 input) RTM_NO_EXCEPT
	{
		return rtm_impl::quatf_quat_length_reciprocal{ input };
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the reciprocal length/norm of the quaternion.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE scalarf RTM_SIMD_CALL quat_length_reciprocal_as_scalar(quatf_arg0 input) RTM_NO_EXCEPT
	{
		const scalarf len_sq = quat_length_squared_as_scalar(input);
		return scalar_sqrt_reciprocal(len_sq);
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns a normalized quaternion.
	// Note that if the input quaternion is invalid (pure zero or with NaN/Inf),
	// the result is undefined.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE quatf RTM_SIMD_CALL quat_normalize(quatf_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		// We first calculate the dot product to get the length squared: dot(input, input)
		__m128 x2_y2_z2_w2 = _mm_mul_ps(input, input);
		__m128 z2_w2_0_0 = _mm_shuffle_ps(x2_y2_z2_w2, x2_y2_z2_w2, _MM_SHUFFLE(0, 0, 3, 2));
		__m128 x2z2_y2w2_0_0 = _mm_add_ps(x2_y2_z2_w2, z2_w2_0_0);
		__m128 y2w2_0_0_0 = _mm_shuffle_ps(x2z2_y2w2_0_0, x2z2_y2w2_0_0, _MM_SHUFFLE(0, 0, 0, 1));
		__m128 x2y2z2w2_0_0_0 = _mm_add_ps(x2z2_y2w2_0_0, y2w2_0_0_0);

		// Keep the dot product result as a scalar within the first lane, it is faster to
		// calculate the reciprocal square root of a single lane VS all 4 lanes
		__m128 dot = x2y2z2w2_0_0_0;

		// Calculate the reciprocal square root to get the inverse length of our vector
		// Perform two passes of Newton-Raphson iteration on the hardware estimate
		__m128 half = _mm_set_ss(0.5F);
		__m128 input_half_v = _mm_mul_ss(dot, half);
		__m128 x0 = _mm_rsqrt_ss(dot);

		// First iteration
		__m128 x1 = _mm_mul_ss(x0, x0);
		x1 = _mm_sub_ss(half, _mm_mul_ss(input_half_v, x1));
		x1 = _mm_add_ss(_mm_mul_ss(x0, x1), x0);

		// Second iteration
		__m128 x2 = _mm_mul_ss(x1, x1);
		x2 = _mm_sub_ss(half, _mm_mul_ss(input_half_v, x2));
		x2 = _mm_add_ss(_mm_mul_ss(x1, x2), x1);

		// Broadcast the vector length reciprocal to all 4 lanes in order to multiply it with the vector
		__m128 inv_len = _mm_shuffle_ps(x2, x2, _MM_SHUFFLE(0, 0, 0, 0));

		// Multiply the rotation by it's inverse length in order to normalize it
		return _mm_mul_ps(input, inv_len);
#elif defined (RTM_NEON_INTRINSICS)
		// Use sqrt/div/mul to normalize because the sqrt/div are faster than rsqrt
		float inv_len = 1.0F / scalar_sqrt(vector_length_squared(input));
		return vector_mul(input, inv_len);
#else
		// Reciprocal is more accurate to normalize with
		float inv_len = quat_length_reciprocal(input);
		return vector_to_quat(vector_mul(quat_to_vector(input), inv_len));
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns a normalized quaternion using a deterministic algorithm.
	// This ensures that for a given input, the output will be identical on all
	// platforms that implement IEEE-754. This can be slower than `quat_normalize`.
	// This is only guaranteed if the rounding modes are consistent.
	// Note that if the input quaternion is invalid (pure zero or with NaN/Inf),
	// the result is undefined.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE quatf RTM_SIMD_CALL quat_normalize_deterministic(quatf_arg0 input) RTM_NO_EXCEPT
	{
		vector4f inputv = quat_to_vector(input);

		// Multiply once and retrieve floats, can't use scalarf because we need to use volatile
		// volatile will force a roundtrip to memory and should prevent re-ordering as well as
		// other optimizations such as FMA.
		vector4f input_sq = vector_mul(inputv, inputv);
		volatile float x_sq = vector_get_x(input_sq);
		volatile float y_sq = vector_get_y(input_sq);
		volatile float z_sq = vector_get_z(input_sq);
		volatile float w_sq = vector_get_w(input_sq);
		volatile float sum_xy_sq = x_sq + y_sq;
		volatile float sum_zw_sq = z_sq + w_sq;
		float len_sq = sum_xy_sq + sum_zw_sq;

		// sqrt(float) might not be exact when fast math or similar optimizations are enabled
		// We might not be able to disable these
		// As such, we promote to a double to ensure a deterministic result
		// We add volatile to ensure rsqrt or similar isn't used
		volatile double len = scalar_sqrt(double(len_sq));
		return vector_to_quat(vector_div(inputv, vector_set(float(len))));
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the linear interpolation between start and end for a given alpha value.
	// The formula used is: ((1.0 - alpha) * start) + (alpha * end).
	// Interpolation is stable and will return 'start' when 'alpha' is 0.0 and 'end' when it is 1.0.
	// This is the same instruction count when FMA is present but it might be slightly slower
	// due to the extra multiplication compared to: start + (alpha * (end - start)).
	// Note that if 'start' and 'end' are on the opposite ends of the hypersphere, 'end' is negated
	// before we interpolate. As such, when 'alpha' is 1.0, either 'end' or its negated equivalent
	// is returned. Furthermore, if 'start' and 'end' aren't exactly normalized, the result might
	// not match exactly when 'alpha' is 0.0 or 1.0 because we normalize the resulting quaternion.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline quatf RTM_SIMD_CALL quat_lerp(quatf_arg0 start, quatf_arg1 end, float alpha) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		// Calculate the vector4 dot product: dot(start, end)
		__m128 dot;
#if defined(RTM_SSE4_INTRINSICS)
		// If both rotations are on opposite ends of the hypersphere, the result will be
		// very negative. If we are on the edge, the rotations are nearly opposite but not quite which
		// means that the linear interpolation here will have terrible accuracy to begin with. It is designed
		// for interpolating rotations that are reasonably close together. The bias check is mainly necessary
		// because the W component is often kept positive which flips the sign.
		// Using the dpps instruction reduces the number of registers that we need and helps the function get
		// inlined.
		dot = _mm_dp_ps(start, end, 0xFF);
#else
		{
			__m128 x2_y2_z2_w2 = _mm_mul_ps(start, end);
			__m128 z2_w2_0_0 = _mm_shuffle_ps(x2_y2_z2_w2, x2_y2_z2_w2, _MM_SHUFFLE(0, 0, 3, 2));
			__m128 x2z2_y2w2_0_0 = _mm_add_ps(x2_y2_z2_w2, z2_w2_0_0);
			__m128 y2w2_0_0_0 = _mm_shuffle_ps(x2z2_y2w2_0_0, x2z2_y2w2_0_0, _MM_SHUFFLE(0, 0, 0, 1));
			__m128 x2y2z2w2_0_0_0 = _mm_add_ps(x2z2_y2w2_0_0, y2w2_0_0_0);
			// Shuffle the dot product to all SIMD lanes, there is no _mm_and_ss and loading
			// the constant from memory with the 'and' instruction is faster, it uses fewer registers
			// and fewer instructions
			dot = _mm_shuffle_ps(x2y2z2w2_0_0_0, x2y2z2w2_0_0_0, _MM_SHUFFLE(0, 0, 0, 0));
		}
#endif

		// Calculate the bias, if the dot product is positive or zero, there is no bias
		// but if it is negative, we want to flip the 'end' rotation XYZW components
		__m128 bias = _mm_and_ps(dot, _mm_set_ps1(-0.0F));

		// Lerp the rotation after applying the bias
		// ((1.0 - alpha) * start) + (alpha * (end ^ bias)) == (start - alpha * start) + (alpha * (end ^ bias))
		__m128 alpha_ = _mm_set_ps1(alpha);
		__m128 interpolated_rotation = _mm_add_ps(_mm_sub_ps(start, _mm_mul_ps(alpha_, start)), _mm_mul_ps(alpha_, _mm_xor_ps(end, bias)));

		// Now we need to normalize the resulting rotation. We first calculate the
		// dot product to get the length squared: dot(interpolated_rotation, interpolated_rotation)
		__m128 x2_y2_z2_w2 = _mm_mul_ps(interpolated_rotation, interpolated_rotation);
		__m128 z2_w2_0_0 = _mm_shuffle_ps(x2_y2_z2_w2, x2_y2_z2_w2, _MM_SHUFFLE(0, 0, 3, 2));
		__m128 x2z2_y2w2_0_0 = _mm_add_ps(x2_y2_z2_w2, z2_w2_0_0);
		__m128 y2w2_0_0_0 = _mm_shuffle_ps(x2z2_y2w2_0_0, x2z2_y2w2_0_0, _MM_SHUFFLE(0, 0, 0, 1));
		__m128 x2y2z2w2_0_0_0 = _mm_add_ps(x2z2_y2w2_0_0, y2w2_0_0_0);

		// Keep the dot product result as a scalar within the first lane, it is faster to
		// calculate the reciprocal square root of a single lane VS all 4 lanes
		dot = x2y2z2w2_0_0_0;

		// Calculate the reciprocal square root to get the inverse length of our vector
		// Perform two passes of Newton-Raphson iteration on the hardware estimate
		__m128 half = _mm_set_ss(0.5F);
		__m128 input_half_v = _mm_mul_ss(dot, half);
		__m128 x0 = _mm_rsqrt_ss(dot);

		// First iteration
		__m128 x1 = _mm_mul_ss(x0, x0);
		x1 = _mm_sub_ss(half, _mm_mul_ss(input_half_v, x1));
		x1 = _mm_add_ss(_mm_mul_ss(x0, x1), x0);

		// Second iteration
		__m128 x2 = _mm_mul_ss(x1, x1);
		x2 = _mm_sub_ss(half, _mm_mul_ss(input_half_v, x2));
		x2 = _mm_add_ss(_mm_mul_ss(x1, x2), x1);

		// Broadcast the vector length reciprocal to all 4 lanes in order to multiply it with the vector
		__m128 inv_len = _mm_shuffle_ps(x2, x2, _MM_SHUFFLE(0, 0, 0, 0));

		// Multiply the rotation by it's inverse length in order to normalize it
		return _mm_mul_ps(interpolated_rotation, inv_len);
#elif defined (RTM_NEON64_INTRINSICS)
		// On ARM64 with NEON, we load 1.0 once and use it twice which is faster than
		// using a AND/XOR with the bias (same number of instructions)
		float dot = vector_dot(start, end);
		float bias = dot >= 0.0F ? 1.0F : -1.0F;
		// ((1.0 - alpha) * start) + (alpha * (end * bias)) == (start - alpha * start) + (alpha * (end * bias))
		vector4f interpolated_rotation = vector_mul_add(vector_mul(end, bias), alpha, vector_neg_mul_sub(start, alpha, start));
		// Use sqrt/div/mul to normalize because the sqrt/div are faster than rsqrt
		float inv_len = 1.0F / scalar_sqrt(vector_length_squared(interpolated_rotation));
		return vector_mul(interpolated_rotation, inv_len);
#elif defined(RTM_NEON_INTRINSICS)
		// Calculate the vector4 dot product: dot(start, end)
		float32x4_t x2_y2_z2_w2 = vmulq_f32(start, end);
		float32x2_t x2_y2 = vget_low_f32(x2_y2_z2_w2);
		float32x2_t z2_w2 = vget_high_f32(x2_y2_z2_w2);
		float32x2_t x2z2_y2w2 = vadd_f32(x2_y2, z2_w2);
		float32x2_t x2y2z2w2 = vpadd_f32(x2z2_y2w2, x2z2_y2w2);

		// Calculate the bias, if the dot product is positive or zero, there is no bias
		// but if it is negative, we want to flip the 'end' rotation XYZW components
		// On ARM-v7-A, the AND/XOR trick is faster than the cmp/fsel
		uint32x2_t bias = vand_u32(vreinterpret_u32_f32(x2y2z2w2), vdup_n_u32(0x80000000));

		// Lerp the rotation after applying the bias
		// ((1.0 - alpha) * start) + (alpha * (end ^ bias)) == (start - alpha * start) + (alpha * (end ^ bias))
		float32x4_t end_biased = vreinterpretq_f32_u32(veorq_u32(vreinterpretq_u32_f32(end), vcombine_u32(bias, bias)));
		float32x4_t interpolated_rotation = vmlaq_n_f32(vmlsq_n_f32(start, start, alpha), end_biased, alpha);

		// Now we need to normalize the resulting rotation. We first calculate the
		// dot product to get the length squared: dot(interpolated_rotation, interpolated_rotation)
		x2_y2_z2_w2 = vmulq_f32(interpolated_rotation, interpolated_rotation);
		x2_y2 = vget_low_f32(x2_y2_z2_w2);
		z2_w2 = vget_high_f32(x2_y2_z2_w2);
		x2z2_y2w2 = vadd_f32(x2_y2, z2_w2);
		x2y2z2w2 = vpadd_f32(x2z2_y2w2, x2z2_y2w2);

		float dot = vget_lane_f32(x2y2z2w2, 0);

		// Use sqrt/div/mul to normalize because the sqrt/div are faster than rsqrt
		float inv_len = 1.0F / scalar_sqrt(dot);
		return vector_mul(interpolated_rotation, inv_len);
#else
		// To ensure we take the shortest path, we apply a bias if the dot product is negative
		vector4f start_vector = quat_to_vector(start);
		vector4f end_vector = quat_to_vector(end);
		float dot = vector_dot(start_vector, end_vector);
		float bias = dot >= 0.0F ? 1.0F : -1.0F;
		// ((1.0 - alpha) * start) + (alpha * (end * bias)) == (start - alpha * start) + (alpha * (end * bias))
		vector4f interpolated_rotation = vector_mul_add(vector_mul(end_vector, bias), alpha, vector_neg_mul_sub(start_vector, alpha, start_vector));
		return quat_normalize(vector_to_quat(interpolated_rotation));
#endif
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Returns the linear interpolation between start and end for a given alpha value.
	// The formula used is: ((1.0 - alpha) * start) + (alpha * end).
	// Interpolation is stable and will return 'start' when 'alpha' is 0.0 and 'end' when it is 1.0.
	// This is the same instruction count when FMA is present but it might be slightly slower
	// due to the extra multiplication compared to: start + (alpha * (end - start)).
	// Note that if 'start' and 'end' are on the opposite ends of the hypersphere, 'end' is negated
	// before we interpolate. As such, when 'alpha' is 1.0, either 'end' or its negated equivalent
	// is returned. Furthermore, if 'start' and 'end' aren't exactly normalized, the result might
	// not match exactly when 'alpha' is 0.0 or 1.0 because we normalize the resulting quaternion.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline quatf RTM_SIMD_CALL quat_lerp(quatf_arg0 start, quatf_arg1 end, scalarf_arg2 alpha) RTM_NO_EXCEPT
	{
		// Calculate the vector4 dot product: dot(start, end)
		__m128 dot;
#if defined(RTM_SSE4_INTRINSICS)
		// If both rotations are on opposite ends of the hypersphere, the result will be
		// very negative. If we are on the edge, the rotations are nearly opposite but not quite which
		// means that the linear interpolation here will have terrible accuracy to begin with. It is designed
		// for interpolating rotations that are reasonably close together. The bias check is mainly necessary
		// because the W component is often kept positive which flips the sign.
		// Using the dpps instruction reduces the number of registers that we need and helps the function get
		// inlined.
		dot = _mm_dp_ps(start, end, 0xFF);
#else
		{
			__m128 x2_y2_z2_w2 = _mm_mul_ps(start, end);
			__m128 z2_w2_0_0 = _mm_shuffle_ps(x2_y2_z2_w2, x2_y2_z2_w2, _MM_SHUFFLE(0, 0, 3, 2));
			__m128 x2z2_y2w2_0_0 = _mm_add_ps(x2_y2_z2_w2, z2_w2_0_0);
			__m128 y2w2_0_0_0 = _mm_shuffle_ps(x2z2_y2w2_0_0, x2z2_y2w2_0_0, _MM_SHUFFLE(0, 0, 0, 1));
			__m128 x2y2z2w2_0_0_0 = _mm_add_ps(x2z2_y2w2_0_0, y2w2_0_0_0);
			// Shuffle the dot product to all SIMD lanes, there is no _mm_and_ss and loading
			// the constant from memory with the 'and' instruction is faster, it uses fewer registers
			// and fewer instructions
			dot = _mm_shuffle_ps(x2y2z2w2_0_0_0, x2y2z2w2_0_0_0, _MM_SHUFFLE(0, 0, 0, 0));
		}
#endif

		// Calculate the bias, if the dot product is positive or zero, there is no bias
		// but if it is negative, we want to flip the 'end' rotation XYZW components
		__m128 bias = _mm_and_ps(dot, _mm_set_ps1(-0.0F));

		// Lerp the rotation after applying the bias
		// ((1.0 - alpha) * start) + (alpha * (end ^ bias)) == (start - alpha * start) + (alpha * (end ^ bias))
		__m128 alpha_ = _mm_shuffle_ps(alpha.value, alpha.value, _MM_SHUFFLE(0, 0, 0, 0));
		__m128 interpolated_rotation = _mm_add_ps(_mm_sub_ps(start, _mm_mul_ps(alpha_, start)), _mm_mul_ps(alpha_, _mm_xor_ps(end, bias)));

		// Now we need to normalize the resulting rotation. We first calculate the
		// dot product to get the length squared: dot(interpolated_rotation, interpolated_rotation)
		__m128 x2_y2_z2_w2 = _mm_mul_ps(interpolated_rotation, interpolated_rotation);
		__m128 z2_w2_0_0 = _mm_shuffle_ps(x2_y2_z2_w2, x2_y2_z2_w2, _MM_SHUFFLE(0, 0, 3, 2));
		__m128 x2z2_y2w2_0_0 = _mm_add_ps(x2_y2_z2_w2, z2_w2_0_0);
		__m128 y2w2_0_0_0 = _mm_shuffle_ps(x2z2_y2w2_0_0, x2z2_y2w2_0_0, _MM_SHUFFLE(0, 0, 0, 1));
		__m128 x2y2z2w2_0_0_0 = _mm_add_ps(x2z2_y2w2_0_0, y2w2_0_0_0);

		// Keep the dot product result as a scalar within the first lane, it is faster to
		// calculate the reciprocal square root of a single lane VS all 4 lanes
		dot = x2y2z2w2_0_0_0;

		// Calculate the reciprocal square root to get the inverse length of our vector
		// Perform two passes of Newton-Raphson iteration on the hardware estimate
		__m128 half = _mm_set_ss(0.5F);
		__m128 input_half_v = _mm_mul_ss(dot, half);
		__m128 x0 = _mm_rsqrt_ss(dot);

		// First iteration
		__m128 x1 = _mm_mul_ss(x0, x0);
		x1 = _mm_sub_ss(half, _mm_mul_ss(input_half_v, x1));
		x1 = _mm_add_ss(_mm_mul_ss(x0, x1), x0);

		// Second iteration
		__m128 x2 = _mm_mul_ss(x1, x1);
		x2 = _mm_sub_ss(half, _mm_mul_ss(input_half_v, x2));
		x2 = _mm_add_ss(_mm_mul_ss(x1, x2), x1);

		// Broadcast the vector length reciprocal to all 4 lanes in order to multiply it with the vector
		__m128 inv_len = _mm_shuffle_ps(x2, x2, _MM_SHUFFLE(0, 0, 0, 0));

		// Multiply the rotation by it's inverse length in order to normalize it
		return _mm_mul_ps(interpolated_rotation, inv_len);
	}
#endif

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Returns the spherical interpolation between start and end for a given alpha value.
	// See: https://www.euclideanspace.com/maths/algebra/realNormedAlgebra/quaternions/slerp/index.htm
	// Perhaps try this someday: http://number-none.com/product/Understanding%20Slerp,%20Then%20Not%20Using%20It/
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline quatf RTM_SIMD_CALL quat_slerp(quatf_arg0 start, quatf_arg1 end, scalarf_arg2 alpha) RTM_NO_EXCEPT
	{
		vector4f start_v = quat_to_vector(start);
		vector4f end_v = quat_to_vector(end);

		vector4f cos_half_angle_v = vector_dot_as_vector(start_v, end_v);
		mask4f is_angle_negative = vector_less_than(cos_half_angle_v, vector_zero());

		// If the two input quaternions aren't on the same half of the hypersphere, flip one and the angle sign
		end_v = vector_select(is_angle_negative, vector_neg(end_v), end_v);
		cos_half_angle_v = vector_select(is_angle_negative, vector_neg(cos_half_angle_v), cos_half_angle_v);

		// Clamp our half angle cosine
		cos_half_angle_v = vector_clamp(cos_half_angle_v, vector_set(-1.0F), vector_set(1.0F));

		scalarf cos_half_angle = vector_get_x_as_scalar(cos_half_angle_v);
		scalarf half_angle = scalar_acos(cos_half_angle);
		scalarf sin_half_angle = scalar_sqrt(scalar_sub(scalar_set(1.0F), scalar_mul(cos_half_angle, cos_half_angle)));
		scalarf inv_sin_half_angle = scalar_reciprocal(sin_half_angle);

		scalarf start_contribution_angle = scalar_mul(scalar_sub(scalar_set(1.0F), alpha), half_angle);
		scalarf end_contribution_angle = scalar_mul(alpha, half_angle);
		vector4f contribution_angles = vector_set(start_contribution_angle, end_contribution_angle, start_contribution_angle, end_contribution_angle);
		vector4f contributions = vector_mul(vector_sin(contribution_angles), inv_sin_half_angle);
		vector4f start_contribution = vector_dup_x(contributions);
		vector4f end_contribution = vector_dup_y(contributions);

		vector4f result = vector_add(vector_mul(start_v, start_contribution), vector_mul(end_v, end_contribution));
		return vector_to_quat(result);
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Returns the spherical interpolation between start and end for a given alpha value.
	// See: https://www.euclideanspace.com/maths/algebra/realNormedAlgebra/quaternions/slerp/index.htm
	// Perhaps try this someday: http://number-none.com/product/Understanding%20Slerp,%20Then%20Not%20Using%20It/
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline quatf RTM_SIMD_CALL quat_slerp(quatf_arg0 start, quatf_arg1 end, float alpha) RTM_NO_EXCEPT
	{
		vector4f start_v = quat_to_vector(start);
		vector4f end_v = quat_to_vector(end);
		scalarf alpha_s = scalar_set(alpha);

		vector4f cos_half_angle_v = vector_dot_as_vector(start_v, end_v);
		mask4f is_angle_negative = vector_less_than(cos_half_angle_v, vector_zero());

		// If the two input quaternions aren't on the same half of the hypersphere, flip one and the angle sign
		end_v = vector_select(is_angle_negative, vector_neg(end_v), end_v);
		cos_half_angle_v = vector_select(is_angle_negative, vector_neg(cos_half_angle_v), cos_half_angle_v);

		// Clamp our half angle cosine
		cos_half_angle_v = vector_clamp(cos_half_angle_v, vector_set(-1.0F), vector_set(1.0F));

		scalarf cos_half_angle = vector_get_x_as_scalar(cos_half_angle_v);
		scalarf half_angle = scalar_acos(cos_half_angle);
		scalarf sin_half_angle = scalar_sqrt(scalar_sub(scalar_set(1.0F), scalar_mul(cos_half_angle, cos_half_angle)));
		scalarf inv_sin_half_angle = scalar_reciprocal(sin_half_angle);

		scalarf start_contribution_angle = scalar_mul(scalar_sub(scalar_set(1.0F), alpha_s), half_angle);
		scalarf end_contribution_angle = scalar_mul(alpha_s, half_angle);
		vector4f contribution_angles = vector_set(start_contribution_angle, end_contribution_angle, start_contribution_angle, end_contribution_angle);
		vector4f contributions = vector_mul(vector_sin(contribution_angles), inv_sin_half_angle);
		vector4f start_contribution = vector_dup_x(contributions);
		vector4f end_contribution = vector_dup_y(contributions);

		vector4f result = vector_add(vector_mul(start_v, start_contribution), vector_mul(end_v, end_contribution));
		return vector_to_quat(result);
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns a component wise negated quaternion.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE quatf RTM_SIMD_CALL quat_neg(quatf_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		constexpr __m128 signs = RTM_VECTOR4F_MAKE(-0.0F, -0.0F, -0.0F, -0.0F);
		return _mm_xor_ps(input, signs);
#elif defined(RTM_NEON_INTRINSICS)
		return vnegq_f32(input);
#else
		return vector_to_quat(vector_mul(quat_to_vector(input), -1.0F));
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the quaternion logarithm for a 3D rotation.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE quatf RTM_SIMD_CALL quat_rotation_log(quatf_arg0 input) RTM_NO_EXCEPT
	{
		// The logarithm equation is as follow:
		// log(q).xyz = (q.xyz / ||q.xyz||) * acos(q.w / ||q||)
		// log(q).w = ln(||q||)
		//
		// If our quaternion is normalized (a rotation), our output W is always 0.0 since ln(1) = 0.0
		// Our XYZ simplifies as well down to: normalize(q.xyz) * acos(q.w)
		// The length of our input XYZ can be calculated either as sqrt(dot(q.xyz, q.xyz)) or
		// by taking the sine of the quaternion half angle with sin(acos(q.w))
		//
		// If our input rotation is near the identity, we cannot calculate the output XYZ since we would
		// divide by zero. If this happens we return a quaternion near zero which should reconstruct as the
		// identity with quat_rotation_exp(..).
		//
		// If our quaternion isn't normalized, more math is required

		const vector4f input_v = quat_to_vector(input);
		const scalarf input_w = scalar_clamp(quat_get_w_as_scalar(input), scalar_set(-1.0F), scalar_set(1.0F));
		const scalarf half_angle = scalar_acos(input_w);
		const scalarf xyz_inv_len = vector_length_reciprocal3_as_scalar(input_v);
		vector4f result_xyz = vector_mul(input_v, scalar_mul(xyz_inv_len, half_angle));

		// If we are near the identity, xyz will be set to our input xyz which should be near zero
		// For a true identity input, we'll output zero
		const mask4f is_input_near_identity = vector_greater_than(vector_set(input_w), vector_set(1.0F - 1.0E-6F));
		result_xyz = vector_select(is_input_near_identity, input_v, result_xyz);

		return vector_to_quat(vector_set_w(result_xyz, 0.0F));
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the quaternion exponential for a 3D rotation.
	// The result might not be fully normalized if the input is near zero.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE quatf RTM_SIMD_CALL quat_rotation_exp(quatf_arg0 input) RTM_NO_EXCEPT
	{
		// The exponential equation is as follow:
		// exp(q).xyz = exp(q.w) * (q.xyz / ||q.xyz||) * sin(||q.xyz||)
		// exp(q).w = exp(q.w) * cos(||q.xyz||)
		//
		// If our output (normalized) quaternion is to represent a rotation, its logarithm passed as input has a W component equal to 0.0
		// Furthermore our input XYZ length is equal to our rotation half angle
		// Since exp(0.0) = 1.0, our equation simplifies as follow:
		// exp(q).xyz = (q.xyz / ||q.xyz||) * sin(||q.xyz||) = normalize(q.xyz) * sin(||q.xyz||)
		// exp(q).w = cos(||q.xyz||)
		//
		// If our half angle is 0.0, the whole input is 0,0,0,0 and the result is undefined since we can use any rotation axis.
		// When this happens, we return the approximated identity.
		// This is easily achieved because cos(||q.xyz||) = cos(0.0) = 1.0
		//
		// If our output quaternion does not represent a rotation, more math is required

		const vector4f input_v = quat_to_vector(input);
		const scalarf input_len = vector_length3_as_scalar(input_v);
		const vector4f input_len_v = vector_set(input_len);
		const vector4f input_normalized = vector_div(input_v, input_len_v);
		const vector4f sincos = scalar_sincos(input_len);

		vector4f result_xyz = vector_mul(input_normalized, vector_dup_x(sincos));

		// If we are near zero, xyz will be set to our input xyz which should be near zero
		// For a true zero input, we'll output the identity
		const mask4f is_input_near_zero = vector_less_than(input_len_v, vector_set(1.0E-6F));
		result_xyz = vector_select(is_input_near_zero, input_v, result_xyz);

		const vector4f result_w = vector_dup_y(sincos);
		const vector4f result = vector_mix<mix4::x, mix4::y, mix4::z, mix4::d>(result_xyz, result_w);
		return vector_to_quat(result);
	}



	//////////////////////////////////////////////////////////////////////////
	// Conversion to/from axis/angle/euler
	//////////////////////////////////////////////////////////////////////////



	//////////////////////////////////////////////////////////////////////////
	// Returns the rotation axis and rotation angle that make up the input quaternion.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline void RTM_SIMD_CALL quat_to_axis_angle(quatf_arg0 input, vector4f& out_axis, float& out_angle) RTM_NO_EXCEPT
	{
		constexpr float epsilon = 1.0E-8F;
		constexpr float epsilon_squared = epsilon * epsilon;

		const scalarf input_w = scalar_clamp(quat_get_w_as_scalar(input), scalar_set(-1.0F), scalar_set(1.0F));
		out_angle = scalar_cast(scalar_acos(input_w)) * 2.0F;

		const float scale_sq = scalar_max(1.0F - quat_get_w(input) * quat_get_w(input), 0.0F);
		out_axis = scale_sq >= epsilon_squared ? vector_mul(quat_to_vector(input), vector_set(scalar_sqrt_reciprocal(scale_sq))) : vector_set(1.0F, 0.0F, 0.0F);
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the rotation axis part of the input quaternion.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline vector4f RTM_SIMD_CALL quat_get_axis(quatf_arg0 input) RTM_NO_EXCEPT
	{
		constexpr float epsilon = 1.0E-8F;
		constexpr float epsilon_squared = epsilon * epsilon;

		const float scale_sq = scalar_max(1.0F - quat_get_w(input) * quat_get_w(input), 0.0F);
		return scale_sq >= epsilon_squared ? vector_mul(quat_to_vector(input), vector_set(scalar_sqrt_reciprocal(scale_sq))) : vector_set(1.0F, 0.0F, 0.0F);
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the rotation angle part of the input quaternion.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline float RTM_SIMD_CALL quat_get_angle(quatf_arg0 input) RTM_NO_EXCEPT
	{
		const scalarf input_w = scalar_clamp(quat_get_w_as_scalar(input), scalar_set(-1.0F), scalar_set(1.0F));
		return scalar_cast(scalar_acos(input_w)) * 2.0F;
	}

	//////////////////////////////////////////////////////////////////////////
	// Creates a quaternion from a rotation axis and a rotation angle.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline quatf RTM_SIMD_CALL quat_from_axis_angle(vector4f_arg0 axis, float angle) RTM_NO_EXCEPT
	{
		vector4f sincos_ = scalar_sincos(0.5F * angle);
		vector4f sin_ = vector_dup_x(sincos_);
		scalarf cos_ = vector_get_y_as_scalar(sincos_);

		return vector_to_quat(vector_set_w(vector_mul(sin_, axis), cos_));
	}

	//////////////////////////////////////////////////////////////////////////
	// Creates a quaternion from Euler Pitch/Yaw/Roll angles.
	// Pitch is around the Y axis (right)
	// Yaw is around the Z axis (up)
	// Roll is around the X axis (forward)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline quatf RTM_SIMD_CALL quat_from_euler(float pitch, float yaw, float roll) RTM_NO_EXCEPT
	{
		float sp;
		float sy;
		float sr;
		float cp;
		float cy;
		float cr;

		scalar_sincos(pitch * 0.5F, sp, cp);
		scalar_sincos(yaw * 0.5F, sy, cy);
		scalar_sincos(roll * 0.5F, sr, cr);

		return quat_set(cr * sp * sy - sr * cp * cy,
			-cr * sp * cy - sr * cp * sy,
			cr * cp * sy - sr * sp * cy,
			cr * cp * cy + sr * sp * sy);
	}



	//////////////////////////////////////////////////////////////////////////
	// Comparisons and masking
	//////////////////////////////////////////////////////////////////////////



	//////////////////////////////////////////////////////////////////////////
	// Returns true if the input quaternion does not contain any NaN or Inf, otherwise false.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL quat_is_finite(quatf_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		const __m128i abs_mask = _mm_set_epi32(0x7FFFFFFFULL, 0x7FFFFFFFULL, 0x7FFFFFFFULL, 0x7FFFFFFFULL);
		__m128 abs_input = _mm_and_ps(input, _mm_castsi128_ps(abs_mask));

		const __m128 infinity = _mm_set_ps1(std::numeric_limits<float>::infinity());
		__m128 is_infinity = _mm_cmpeq_ps(abs_input, infinity);

		__m128 is_nan = _mm_cmpneq_ps(input, input);

		__m128 is_not_finite = _mm_or_ps(is_infinity, is_nan);
		return _mm_movemask_ps(is_not_finite) == 0;
#else
		return scalar_is_finite(quat_get_x(input)) && scalar_is_finite(quat_get_y(input)) && scalar_is_finite(quat_get_z(input)) && scalar_is_finite(quat_get_w(input));
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if the input quaternion is normalized, otherwise false.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL quat_is_normalized(quatf_arg0 input, float threshold = 0.00001F) RTM_NO_EXCEPT
	{
		float length_squared = quat_length_squared(input);
		return scalar_abs(length_squared - 1.0F) <= threshold;
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if the two quaternions are equal component wise, otherwise false.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL quat_are_equal(quatf_arg0 lhs, quatf_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		const __m128 mask = _mm_cmpeq_ps(lhs, rhs);

		bool result;
		RTM_MASK4F_ALL_TRUE(mask, result);
		return result;
#elif defined(RTM_NEON_INTRINSICS)
		const uint32x4_t mask = vceqq_f32(lhs, rhs);

		bool result;
		RTM_MASK4F_ALL_TRUE(mask, result);
		return result;
#else
		return lhs.x == rhs.x && lhs.y == rhs.y && lhs.z == rhs.z && lhs.w == rhs.w;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if the two quaternions are nearly equal component wise, otherwise false.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL quat_near_equal(quatf_arg0 lhs, quatf_arg1 rhs, float threshold = 0.00001F) RTM_NO_EXCEPT
	{
		return vector_all_near_equal(quat_to_vector(lhs), quat_to_vector(rhs), threshold);
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if the input quaternion is nearly equal to the identity quaternion
	// by comparing its rotation angle.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline bool RTM_SIMD_CALL quat_near_identity(quatf_arg0 input, float threshold_angle = 0.00284714461F) RTM_NO_EXCEPT
	{
		// Because of floating point precision, we cannot represent very small rotations.
		// The closest float to 1.0 that is not 1.0 itself yields:
		// acos(0.99999994f) * 2.0f  = 0.000690533954 rad
		//
		// An error threshold of 1.e-6f is used by default.
		// acos(1.f - 1.e-6f) * 2.0f = 0.00284714461 rad
		// acos(1.f - 1.e-7f) * 2.0f = 0.00097656250 rad
		//
		// We don't really care about the angle value itself, only if it's close to 0.
		// This will happen whenever quat.w is close to 1.0.
		// If the quat.w is close to -1.0, the angle will be near 2*PI which is close to
		// a negative 0 rotation. By forcing quat.w to be positive, we'll end up with
		// the shortest path.
		const scalarf input_w = quat_get_w_as_scalar(input);
		const scalarf input_abs_w = scalar_min(scalar_abs(input_w), scalar_set(1.0F));
		const float positive_w_angle = scalar_acos(scalar_cast(input_abs_w)) * 2.0F;
		return positive_w_angle <= threshold_angle;
	}

	//////////////////////////////////////////////////////////////////////////
	// Per component selection depending on the mask: mask != 0 ? if_true : if_false
	// Note that if the mask lanes are not all identical, the resulting quaternion
	// may not be normalized.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE quatf RTM_SIMD_CALL quat_select(mask4f_arg0 mask, quatf_arg1 if_true, quatf_arg2 if_false) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS) || defined(RTM_NEON_INTRINSICS)
		return RTM_VECTOR4F_SELECT(mask, if_true, if_false);
#else
		return quatf{ rtm_impl::select(mask.x, if_true.x, if_false.x), rtm_impl::select(mask.y, if_true.y, if_false.y), rtm_impl::select(mask.z, if_true.z, if_false.z), rtm_impl::select(mask.w, if_true.w, if_false.w) };
#endif
	}

	RTM_IMPL_VERSION_NAMESPACE_END
}

RTM_IMPL_FILE_PRAGMA_POP

```

`includes/rtm/qvd.h`:

```h
#pragma once

////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2023 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include "rtm/math.h"
#include "rtm/matrix3x3d.h"
#include "rtm/matrix3x4d.h"
#include "rtm/quatd.h"
#include "rtm/vector4d.h"
#include "rtm/version.h"
#include "rtm/impl/compiler_utils.h"
#include "rtm/impl/matrix_affine_common.h"
#include "rtm/impl/qv_common.h"

RTM_IMPL_FILE_PRAGMA_PUSH

namespace rtm
{
	RTM_IMPL_VERSION_NAMESPACE_BEGIN

	//////////////////////////////////////////////////////////////////////////
	// Casts a QV transform float32 variant to a float64 variant.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE qvd RTM_SIMD_CALL qv_cast(qvf_arg0 input) RTM_NO_EXCEPT
	{
		return qvd{ quat_cast(input.rotation), vector_cast(input.translation) };
	}

	//////////////////////////////////////////////////////////////////////////
	// Converts a rotation 3x3 matrix into a QV transform.
	// The translation will be the identity.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline qvd RTM_SIMD_CALL qv_from_matrix(matrix3x3d_arg0 input) RTM_NO_EXCEPT
	{
		const quatd rotation = rtm_impl::quat_from_matrix(input.x_axis, input.y_axis, input.z_axis);
		const vector4d translation = vector_zero();
		return qvd{ rotation, translation };
	}

	//////////////////////////////////////////////////////////////////////////
	// Converts a rotation 3x4 affine matrix into a QV transform.
	// If the matrix contains scale, it is stripped and lost.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline qvd RTM_SIMD_CALL qv_from_matrix(matrix3x4d_arg0 input) RTM_NO_EXCEPT
	{
		// See qvv_from_matrix for details, the process here is very similar
		// We still treat every axis independently as if they have unique scale values
		// to be able to handle ill-formed matrices with non-uniform scale.

		// Translation is always in the W axis
		const vector4d translation = input.w_axis;

		// First, we compute our absolute scale values
		const scalard x_scale = vector_length3_as_scalar(input.x_axis);
		const scalard y_scale = vector_length3_as_scalar(input.y_axis);
		const scalard z_scale = vector_length3_as_scalar(input.z_axis);
		const vector4d scale = vector_set(x_scale, y_scale, z_scale);

		// Grab a pointer to the first scale value, we'll index into it
		const double* scale_ptr = rtm_impl::bit_cast<const double*>(&scale);

		// Next, we find the largest scale components and sort them from largest to smallest
		component3 largest_scale_component;
		component3 second_largest_scale_component;
		component3 third_largest_scale_component;

		if (scalar_greater_equal(x_scale, y_scale))
		{
			// X >= Y
			if (scalar_greater_equal(y_scale, z_scale))
			{
				// X >= Y >= Z
				largest_scale_component = component3::x;
				second_largest_scale_component = component3::y;
				third_largest_scale_component = component3::z;
			}
			else
			{
				if (scalar_greater_equal(x_scale, z_scale))
				{
					// X >= Z >= Y
					largest_scale_component = component3::x;
					second_largest_scale_component = component3::z;
				}
				else
				{
					// Z >= X >= Y
					largest_scale_component = component3::z;
					second_largest_scale_component = component3::x;
				}

				third_largest_scale_component = component3::y;
			}
		}
		else
		{
			if (scalar_greater_equal(x_scale, z_scale))
			{
				// Y > X >= Z
				largest_scale_component = component3::y;
				second_largest_scale_component = component3::x;
				third_largest_scale_component = component3::z;
			}
			else
			{
				if (scalar_greater_equal(y_scale, z_scale))
				{
					// Y >= Z > X
					largest_scale_component = component3::y;
					second_largest_scale_component = component3::z;
				}
				else
				{
					// Z > Y > X
					largest_scale_component = component3::z;
					second_largest_scale_component = component3::y;
				}

				third_largest_scale_component = component3::x;
			}
		}

		// Cast to integer for array indexing
		const int32_t largest_scale_component_i = static_cast<int32_t>(largest_scale_component);
		const int32_t second_largest_scale_component_i = static_cast<int32_t>(second_largest_scale_component);
		const int32_t third_largest_scale_component_i = static_cast<int32_t>(third_largest_scale_component);

		const matrix3x3d identity3x3 = matrix_identity();

		// Copy the rotation part from our input
		matrix3x3d rotation = matrix_cast(input);

		// Grab a pointer to the first axis, we'll index into it
		vector4d* rotation_axes = &rotation.x_axis;

		// We use a threshold to test for zero because division with near zero values is imprecise
		// and might not yield a normalized result
		const double zero_scale_threshold = 1.0E-8;
		double largest_scale = scale_ptr[largest_scale_component_i];
		if (largest_scale < zero_scale_threshold)
		{
			// The largest scale value is zero which means all three are zero
			// We'll return the identity rotation since its value is not recoverable
			const quatd rotation_q = quat_identity();
			return qvd{ rotation_q, translation };
		}

		// Normalize the largest scale axis which is non-zero
		vector4d largest_scale_axis = rotation_axes[largest_scale_component_i];
		largest_scale_axis = vector_div(largest_scale_axis, vector_set(largest_scale));

		vector4d second_largest_scale_axis = rotation_axes[second_largest_scale_component_i];
		double second_largest_scale = scale_ptr[second_largest_scale_component_i];
		if (second_largest_scale < zero_scale_threshold)
		{
			// The second largest scale value is zero which means the two smallest axes are zero
			// We'll use the largest axis and build an orthogonal basis around it
			const vector4d largest_y_dot = vector_dot3_as_vector(largest_scale_axis, identity3x3.y_axis);
			const mask4d is_largest_parallel_to_y = vector_greater_equal(vector_abs(largest_y_dot), vector_set(1.0F - zero_scale_threshold));
			const vector4d orthogonal_axis = vector_select(is_largest_parallel_to_y, identity3x3.z_axis, identity3x3.y_axis);

			second_largest_scale_axis = vector_cross3(largest_scale_axis, orthogonal_axis);

			// Recompute the scale to ensure we properly normalize
			// Here, the second largest axis should alread be normalized
			second_largest_scale = vector_length3(second_largest_scale_axis);
		}

		// Normalize the second largest axis which is non-zero
		second_largest_scale_axis = vector_div(second_largest_scale_axis, vector_set(second_largest_scale));

		vector4d third_largest_scale_axis = rotation_axes[third_largest_scale_component_i];
		double third_largest_scale = scale_ptr[third_largest_scale_component_i];
		if (third_largest_scale < zero_scale_threshold)
		{
			// The third largest scale value is zero
			// We'll use the two larger axes to build an orthogonal basis
			third_largest_scale_axis = vector_cross3(largest_scale_axis, second_largest_scale_axis);

			// Recompute the scale to ensure we properly normalize
			// Here, the third largest axis should alread be normalized
			third_largest_scale = vector_length3(third_largest_scale_axis);
		}

		// Normalize the third largest axis which is non-zero
		third_largest_scale_axis = vector_div(third_largest_scale_axis, vector_set(third_largest_scale));

		// Set our new basis
		rotation_axes[largest_scale_component_i] = largest_scale_axis;
		rotation_axes[second_largest_scale_component_i] = second_largest_scale_axis;
		rotation_axes[third_largest_scale_component_i] = third_largest_scale_axis;

		// Now that we have built the ortho-normal basis part of our rotation, we check its winding
		const double determinant = scalar_cast(matrix_determinant(rotation));
		if (determinant < 0.0)
		{
			// Our winding is reversed meaning one or three scale axes contain reflection
			// We negate one axis to flip the winding to be able to find some valid rotation
			largest_scale_axis = vector_neg(largest_scale_axis);

			// Update our rotation basis
			rotation_axes[largest_scale_component_i] = largest_scale_axis;
		}

		// Build a quaternion from the ortho-normal basis we found
		const quatd rotation_q = rtm_impl::quat_from_matrix(rotation.x_axis, rotation.y_axis, rotation.z_axis);

		return qvd{ rotation_q, translation };
	}

	//////////////////////////////////////////////////////////////////////////
	// Multiplies two QV transforms.
	// Multiplication order is as follow: local_to_world = qv_mul(local_to_object, object_to_world)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline qvd RTM_SIMD_CALL qv_mul(qvd_arg0 lhs, qvd_arg1 rhs) RTM_NO_EXCEPT
	{
		const quatd rotation = quat_mul(lhs.rotation, rhs.rotation);
		const vector4d translation = vector_add(quat_mul_vector3(lhs.translation, rhs.rotation), rhs.translation);
		return qv_set(rotation, translation);
	}

	//////////////////////////////////////////////////////////////////////////
	// Multiplies a QV transform and a 3D point.
	// Multiplication order is as follow: world_position = qv_mul_point3(local_position, local_to_world)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline vector4d RTM_SIMD_CALL qv_mul_point3(vector4d_arg0 point, qvd_arg1 qv) RTM_NO_EXCEPT
	{
		return vector_add(quat_mul_vector3(point, qv.rotation), qv.translation);
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the inverse of the input QV transform.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline qvd RTM_SIMD_CALL qv_inverse(qvd_arg0 input) RTM_NO_EXCEPT
	{
		const quatd inv_rotation = quat_conjugate(input.rotation);
		const vector4d inv_translation = vector_neg(quat_mul_vector3(input.translation, inv_rotation));
		return qv_set(inv_rotation, inv_translation);
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns a QV transforms with the rotation part normalized.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE qvd RTM_SIMD_CALL qv_normalize(qvd_arg0 input) RTM_NO_EXCEPT
	{
		const quatd rotation = quat_normalize(input.rotation);
		return qv_set(rotation, input.translation);
	}

	//////////////////////////////////////////////////////////////////////////
	// Per component linear interpolation of the two inputs at the specified alpha.
	// The formula used is: ((1.0 - alpha) * start) + (alpha * end).
	// Interpolation is stable and will return 'start' when alpha is 0.0 and 'end' when it is 1.0.
	// This is the same instruction count when FMA is present but it might be slightly slower
	// due to the extra multiplication compared to: start + (alpha * (end - start)).
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE qvd RTM_SIMD_CALL qv_lerp(qvd_arg0 start, qvd_arg1 end, double alpha) RTM_NO_EXCEPT
	{
		const quatd rotation = quat_lerp(start.rotation, end.rotation, alpha);
		const vector4d translation = vector_lerp(start.translation, end.translation, alpha);
		return qv_set(rotation, translation);
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Per component linear interpolation of the two inputs at the specified alpha.
	// The formula used is: ((1.0 - alpha) * start) + (alpha * end).
	// Interpolation is stable and will return 'start' when alpha is 0.0 and 'end' when it is 1.0.
	// This is the same instruction count when FMA is present but it might be slightly slower
	// due to the extra multiplication compared to: start + (alpha * (end - start)).
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE qvd RTM_SIMD_CALL qv_lerp(qvd_arg0 start, qvd_arg1 end, scalard_argn alpha) RTM_NO_EXCEPT
	{
		const quatd rotation = quat_lerp(start.rotation, end.rotation, alpha);
		const vector4d translation = vector_lerp(start.translation, end.translation, alpha);
		return qv_set(rotation, translation);
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Per component spherical interpolation of the two inputs at the specified alpha.
	// See quat_slerp(..)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE qvd RTM_SIMD_CALL qv_slerp(qvd_arg0 start, qvd_arg0 end, double alpha) RTM_NO_EXCEPT
	{
		const quatd rotation = quat_slerp(start.rotation, end.rotation, alpha);
		const vector4d translation = vector_lerp(start.translation, end.translation, alpha);
		return qv_set(rotation, translation);
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Per component spherical interpolation of the two inputs at the specified alpha.
	// See quat_slerp(..)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE qvd RTM_SIMD_CALL qv_slerp(qvd_arg0 start, qvd_arg1 end, scalard_argn alpha) RTM_NO_EXCEPT
	{
		const quatd rotation = quat_slerp(start.rotation, end.rotation, alpha);
		const vector4d translation = vector_lerp(start.translation, end.translation, alpha);
		return qv_set(rotation, translation);
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Returns true if the input QV does not contain any NaN or Inf, otherwise false.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL qv_is_finite(qvd_arg0 input) RTM_NO_EXCEPT
	{
		return quat_is_finite(input.rotation) && vector_is_finite3(input.translation);
	}

	RTM_IMPL_VERSION_NAMESPACE_END
}

RTM_IMPL_FILE_PRAGMA_POP

```

`includes/rtm/qvf.h`:

```h
#pragma once

////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2023 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include "rtm/math.h"
#include "rtm/matrix3x3f.h"
#include "rtm/matrix3x4f.h"
#include "rtm/quatf.h"
#include "rtm/vector4f.h"
#include "rtm/version.h"
#include "rtm/impl/compiler_utils.h"
#include "rtm/impl/matrix_affine_common.h"
#include "rtm/impl/qv_common.h"

RTM_IMPL_FILE_PRAGMA_PUSH

namespace rtm
{
	RTM_IMPL_VERSION_NAMESPACE_BEGIN

	//////////////////////////////////////////////////////////////////////////
	// Casts a QV transform float64 variant to a float32 variant.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE qvf RTM_SIMD_CALL qv_cast(const qvd& input) RTM_NO_EXCEPT
	{
		return qvf{ quat_cast(input.rotation), vector_cast(input.translation) };
	}

	//////////////////////////////////////////////////////////////////////////
	// Converts a rotation 3x3 matrix into a QV transform.
	// The translation will be the identity.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline qvf RTM_SIMD_CALL qv_from_matrix(matrix3x3f_arg0 input) RTM_NO_EXCEPT
	{
		const quatf rotation = rtm_impl::quat_from_matrix(input.x_axis, input.y_axis, input.z_axis);
		const vector4f translation = vector_zero();
		return qvf{ rotation, translation };
	}

	//////////////////////////////////////////////////////////////////////////
	// Converts a rotation 3x4 affine matrix into a QV transform.
	// If the matrix contains scale, it is stripped and lost.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline qvf RTM_SIMD_CALL qv_from_matrix(matrix3x4f_arg0 input) RTM_NO_EXCEPT
	{
		// See qvv_from_matrix for details, the process here is very similar
		// We still treat every axis independently as if they have unique scale values
		// to be able to handle ill-formed matrices with non-uniform scale.

		// Translation is always in the W axis
		const vector4f translation = input.w_axis;

		// First, we compute our absolute scale values
		const scalarf x_scale = vector_length3_as_scalar(input.x_axis);
		const scalarf y_scale = vector_length3_as_scalar(input.y_axis);
		const scalarf z_scale = vector_length3_as_scalar(input.z_axis);
		const vector4f scale = vector_set(x_scale, y_scale, z_scale);

		// Grab a pointer to the first scale value, we'll index into it
		const float* scale_ptr = rtm_impl::bit_cast<const float*>(&scale);

		// Next, we find the largest scale components and sort them from largest to smallest
		component3 largest_scale_component;
		component3 second_largest_scale_component;
		component3 third_largest_scale_component;

		if (scalar_greater_equal(x_scale, y_scale))
		{
			// X >= Y
			if (scalar_greater_equal(y_scale, z_scale))
			{
				// X >= Y >= Z
				largest_scale_component = component3::x;
				second_largest_scale_component = component3::y;
				third_largest_scale_component = component3::z;
			}
			else
			{
				if (scalar_greater_equal(x_scale, z_scale))
				{
					// X >= Z >= Y
					largest_scale_component = component3::x;
					second_largest_scale_component = component3::z;
				}
				else
				{
					// Z >= X >= Y
					largest_scale_component = component3::z;
					second_largest_scale_component = component3::x;
				}

				third_largest_scale_component = component3::y;
			}
		}
		else
		{
			if (scalar_greater_equal(x_scale, z_scale))
			{
				// Y > X >= Z
				largest_scale_component = component3::y;
				second_largest_scale_component = component3::x;
				third_largest_scale_component = component3::z;
			}
			else
			{
				if (scalar_greater_equal(y_scale, z_scale))
				{
					// Y >= Z > X
					largest_scale_component = component3::y;
					second_largest_scale_component = component3::z;
				}
				else
				{
					// Z > Y > X
					largest_scale_component = component3::z;
					second_largest_scale_component = component3::y;
				}

				third_largest_scale_component = component3::x;
			}
		}

		// Cast to integer for array indexing
		const int32_t largest_scale_component_i = static_cast<int32_t>(largest_scale_component);
		const int32_t second_largest_scale_component_i = static_cast<int32_t>(second_largest_scale_component);
		const int32_t third_largest_scale_component_i = static_cast<int32_t>(third_largest_scale_component);

		const matrix3x3f identity3x3 = matrix_identity();

		// Copy the rotation part from our input
		matrix3x3f rotation = matrix_cast(input);

		// Grab a pointer to the first axis, we'll index into it
		vector4f* rotation_axes = &rotation.x_axis;

		// We use a threshold to test for zero because division with near zero values is imprecise
		// and might not yield a normalized result
		const float zero_scale_threshold = 1.0E-8F;
		float largest_scale = scale_ptr[largest_scale_component_i];
		if (largest_scale < zero_scale_threshold)
		{
			// The largest scale value is zero which means all three are zero
			// We'll return the identity rotation since its value is not recoverable
			const quatf rotation_q = quat_identity();
			return qvf{ rotation_q, translation };
		}

		// Normalize the largest scale axis which is non-zero
		vector4f largest_scale_axis = rotation_axes[largest_scale_component_i];
		largest_scale_axis = vector_div(largest_scale_axis, vector_set(largest_scale));

		vector4f second_largest_scale_axis = rotation_axes[second_largest_scale_component_i];
		float second_largest_scale = scale_ptr[second_largest_scale_component_i];
		if (second_largest_scale < zero_scale_threshold)
		{
			// The second largest scale value is zero which means the two smallest axes are zero
			// We'll use the largest axis and build an orthogonal basis around it
			const vector4f largest_y_dot = vector_dot3_as_vector(largest_scale_axis, identity3x3.y_axis);
			const mask4f is_largest_parallel_to_y = vector_greater_equal(vector_abs(largest_y_dot), vector_set(1.0F - zero_scale_threshold));
			const vector4f orthogonal_axis = vector_select(is_largest_parallel_to_y, identity3x3.z_axis, identity3x3.y_axis);

			second_largest_scale_axis = vector_cross3(largest_scale_axis, orthogonal_axis);

			// Recompute the scale to ensure we properly normalize
			// Here, the second largest axis should alread be normalized
			second_largest_scale = vector_length3(second_largest_scale_axis);
		}

		// Normalize the second largest axis which is non-zero
		second_largest_scale_axis = vector_div(second_largest_scale_axis, vector_set(second_largest_scale));

		vector4f third_largest_scale_axis = rotation_axes[third_largest_scale_component_i];
		float third_largest_scale = scale_ptr[third_largest_scale_component_i];
		if (third_largest_scale < zero_scale_threshold)
		{
			// The third largest scale value is zero
			// We'll use the two larger axes to build an orthogonal basis
			third_largest_scale_axis = vector_cross3(largest_scale_axis, second_largest_scale_axis);

			// Recompute the scale to ensure we properly normalize
			// Here, the third largest axis should alread be normalized
			third_largest_scale = vector_length3(third_largest_scale_axis);
		}

		// Normalize the third largest axis which is non-zero
		third_largest_scale_axis = vector_div(third_largest_scale_axis, vector_set(third_largest_scale));

		// Set our new basis
		rotation_axes[largest_scale_component_i] = largest_scale_axis;
		rotation_axes[second_largest_scale_component_i] = second_largest_scale_axis;
		rotation_axes[third_largest_scale_component_i] = third_largest_scale_axis;

		// Now that we have built the ortho-normal basis part of our rotation, we check its winding
		const float determinant = scalar_cast(matrix_determinant(rotation));
		if (determinant < 0.0F)
		{
			// Our winding is reversed meaning one or three scale axes contain reflection
			// We negate one axis to flip the winding to be able to find some valid rotation
			largest_scale_axis = vector_neg(largest_scale_axis);

			// Update our rotation basis
			rotation_axes[largest_scale_component_i] = largest_scale_axis;
		}

		// Build a quaternion from the ortho-normal basis we found
		const quatf rotation_q = rtm_impl::quat_from_matrix(rotation.x_axis, rotation.y_axis, rotation.z_axis);

		return qvf{ rotation_q, translation };
	}

	//////////////////////////////////////////////////////////////////////////
	// Multiplies two QV transforms.
	// Multiplication order is as follow: local_to_world = qv_mul(local_to_object, object_to_world)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline qvf RTM_SIMD_CALL qv_mul(qvf_arg0 lhs, qvf_arg1 rhs) RTM_NO_EXCEPT
	{
		const quatf rotation = quat_mul(lhs.rotation, rhs.rotation);
		const vector4f translation = vector_add(quat_mul_vector3(lhs.translation, rhs.rotation), rhs.translation);
		return qv_set(rotation, translation);
	}

	//////////////////////////////////////////////////////////////////////////
	// Multiplies a QV transform and a 3D point.
	// Multiplication order is as follow: world_position = qv_mul_point3(local_position, local_to_world)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline vector4f RTM_SIMD_CALL qv_mul_point3(vector4f_arg0 point, qvf_arg1 qv) RTM_NO_EXCEPT
	{
		return vector_add(quat_mul_vector3(point, qv.rotation), qv.translation);
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the inverse of the input QV transform.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline qvf RTM_SIMD_CALL qv_inverse(qvf_arg0 input) RTM_NO_EXCEPT
	{
		const quatf inv_rotation = quat_conjugate(input.rotation);
		const vector4f inv_translation = vector_neg(quat_mul_vector3(input.translation, inv_rotation));
		return qv_set(inv_rotation, inv_translation);
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns a QV transforms with the rotation part normalized.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE qvf RTM_SIMD_CALL qv_normalize(qvf_arg0 input) RTM_NO_EXCEPT
	{
		const quatf rotation = quat_normalize(input.rotation);
		return qv_set(rotation, input.translation);
	}

	//////////////////////////////////////////////////////////////////////////
	// Per component linear interpolation of the two inputs at the specified alpha.
	// The formula used is: ((1.0 - alpha) * start) + (alpha * end).
	// Interpolation is stable and will return 'start' when alpha is 0.0 and 'end' when it is 1.0.
	// This is the same instruction count when FMA is present but it might be slightly slower
	// due to the extra multiplication compared to: start + (alpha * (end - start)).
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE qvf RTM_SIMD_CALL qv_lerp(qvf_arg0 start, qvf_arg1 end, float alpha) RTM_NO_EXCEPT
	{
		const quatf rotation = quat_lerp(start.rotation, end.rotation, alpha);
		const vector4f translation = vector_lerp(start.translation, end.translation, alpha);
		return qv_set(rotation, translation);
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Per component linear interpolation of the two inputs at the specified alpha.
	// The formula used is: ((1.0 - alpha) * start) + (alpha * end).
	// Interpolation is stable and will return 'start' when alpha is 0.0 and 'end' when it is 1.0.
	// This is the same instruction count when FMA is present but it might be slightly slower
	// due to the extra multiplication compared to: start + (alpha * (end - start)).
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE qvf RTM_SIMD_CALL qv_lerp(qvf_arg0 start, qvf_arg1 end, scalarf_arg2 alpha) RTM_NO_EXCEPT
	{
		const quatf rotation = quat_lerp(start.rotation, end.rotation, alpha);
		const vector4f translation = vector_lerp(start.translation, end.translation, alpha);
		return qv_set(rotation, translation);
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Per component spherical interpolation of the two inputs at the specified alpha.
	// See quat_slerp(..)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE qvf RTM_SIMD_CALL qv_slerp(qvf_arg0 start, qvf_arg1 end, float alpha) RTM_NO_EXCEPT
	{
		const quatf rotation = quat_slerp(start.rotation, end.rotation, alpha);
		const vector4f translation = vector_lerp(start.translation, end.translation, alpha);
		return qv_set(rotation, translation);
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Per component spherical interpolation of the two inputs at the specified alpha.
	// See quat_slerp(..)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE qvf RTM_SIMD_CALL qv_slerp(qvf_arg0 start, qvf_arg1 end, scalarf_arg2 alpha) RTM_NO_EXCEPT
	{
		const quatf rotation = quat_slerp(start.rotation, end.rotation, alpha);
		const vector4f translation = vector_lerp(start.translation, end.translation, alpha);
		return qv_set(rotation, translation);
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Returns true if the input QV does not contain any NaN or Inf, otherwise false.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL qv_is_finite(qvf_arg0 input) RTM_NO_EXCEPT
	{
		return quat_is_finite(input.rotation) && vector_is_finite3(input.translation);
	}

	RTM_IMPL_VERSION_NAMESPACE_END
}

RTM_IMPL_FILE_PRAGMA_POP

```

`includes/rtm/qvsd.h`:

```h
#pragma once

////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2023 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include "rtm/math.h"
#include "rtm/matrix3x3d.h"
#include "rtm/quatd.h"
#include "rtm/vector4d.h"
#include "rtm/version.h"
#include "rtm/impl/compiler_utils.h"
#include "rtm/impl/qvs_common.h"
#include "rtm/impl/matrix_affine_common.h"

RTM_IMPL_FILE_PRAGMA_PUSH

namespace rtm
{
	RTM_IMPL_VERSION_NAMESPACE_BEGIN

	//////////////////////////////////////////////////////////////////////////
	// Casts a QVS transform float32 variant to a float64 variant.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE qvsd RTM_SIMD_CALL qvs_cast(qvsf_arg0 input) RTM_NO_EXCEPT
	{
		return qvsd{ quat_cast(input.rotation), vector_cast(input.translation_scale) };
	}

	//////////////////////////////////////////////////////////////////////////
	// Converts a rotation 3x3 matrix into a QVS transform.
	// The translation/scale will be the identity.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline qvsd RTM_SIMD_CALL qvs_from_matrix(matrix3x3d_arg0 input) RTM_NO_EXCEPT
	{
		const quatd rotation = rtm_impl::quat_from_matrix(input.x_axis, input.y_axis, input.z_axis);
		return qvsd{ rotation, vector_set(0.0, 0.0, 0.0, 1.0) };
	}

	//////////////////////////////////////////////////////////////////////////
	// Converts a rotation 3x4 affine matrix into a QVS transform.
	// If the matrix scale is non-uniform, the scale of the largest axis will be returned.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline qvsd RTM_SIMD_CALL qvs_from_matrix(matrix3x4d_arg0 input) RTM_NO_EXCEPT
	{
		// See qvv_from_matrix for details, the process here is very similar
		// We still treat every axis independently as if they have unique scale values
		// to be able to handle ill-formed matrices with non-uniform scale.
		// Unlike with QVV transforms, if the scale is uniform and negative, we know
		// that all three axes have reflection and we can recover the original rotation.

		// Translation is always in the W axis
		const vector4d translation = input.w_axis;

		// First, we compute our absolute scale values
		const scalard x_scale = vector_length3_as_scalar(input.x_axis);
		const scalard y_scale = vector_length3_as_scalar(input.y_axis);
		const scalard z_scale = vector_length3_as_scalar(input.z_axis);
		const vector4d scale = vector_set(x_scale, y_scale, z_scale);

		// Grab a pointer to the first scale value, we'll index into it
		const double* scale_ptr = rtm_impl::bit_cast<const double*>(&scale);

		// Next, we find the largest scale components and sort them from largest to smallest
		component3 largest_scale_component;
		component3 second_largest_scale_component;
		component3 third_largest_scale_component;

		if (scalar_greater_equal(x_scale, y_scale))
		{
			// X >= Y
			if (scalar_greater_equal(y_scale, z_scale))
			{
				// X >= Y >= Z
				largest_scale_component = component3::x;
				second_largest_scale_component = component3::y;
				third_largest_scale_component = component3::z;
			}
			else
			{
				if (scalar_greater_equal(x_scale, z_scale))
				{
					// X >= Z >= Y
					largest_scale_component = component3::x;
					second_largest_scale_component = component3::z;
				}
				else
				{
					// Z >= X >= Y
					largest_scale_component = component3::z;
					second_largest_scale_component = component3::x;
				}

				third_largest_scale_component = component3::y;
			}
		}
		else
		{
			if (scalar_greater_equal(x_scale, z_scale))
			{
				// Y > X >= Z
				largest_scale_component = component3::y;
				second_largest_scale_component = component3::x;
				third_largest_scale_component = component3::z;
			}
			else
			{
				if (scalar_greater_equal(y_scale, z_scale))
				{
					// Y >= Z > X
					largest_scale_component = component3::y;
					second_largest_scale_component = component3::z;
				}
				else
				{
					// Z > Y > X
					largest_scale_component = component3::z;
					second_largest_scale_component = component3::y;
				}

				third_largest_scale_component = component3::x;
			}
		}

		// Cast to integer for array indexing
		const int32_t largest_scale_component_i = static_cast<int32_t>(largest_scale_component);
		const int32_t second_largest_scale_component_i = static_cast<int32_t>(second_largest_scale_component);
		const int32_t third_largest_scale_component_i = static_cast<int32_t>(third_largest_scale_component);

		const matrix3x3d identity3x3 = matrix_identity();

		// Copy the rotation part from our input
		matrix3x3d rotation = matrix_cast(input);

		// Grab a pointer to the first axis, we'll index into it
		vector4d* rotation_axes = &rotation.x_axis;

		// We use a threshold to test for zero because division with near zero values is imprecise
		// and might not yield a normalized result
		const double zero_scale_threshold = 1.0E-8;
		double largest_scale = scale_ptr[largest_scale_component_i];
		if (largest_scale < zero_scale_threshold)
		{
			// The largest scale value is zero which means all three are zero
			// We'll return the identity rotation since its value is not recoverable
			const quatd rotation_q = quat_identity();
			const vector4d translation_scale = vector_set_w(translation, largest_scale);
			return qvsd{ rotation_q, translation_scale };
		}

		// Normalize the largest scale axis which is non-zero
		vector4d largest_scale_axis = rotation_axes[largest_scale_component_i];
		largest_scale_axis = vector_div(largest_scale_axis, vector_set(largest_scale));

		vector4d second_largest_scale_axis = rotation_axes[second_largest_scale_component_i];
		double second_largest_scale = scale_ptr[second_largest_scale_component_i];
		if (second_largest_scale < zero_scale_threshold)
		{
			// The second largest scale value is zero which means the two smallest axes are zero
			// We'll use the largest axis and build an orthogonal basis around it
			const vector4d largest_y_dot = vector_dot3_as_vector(largest_scale_axis, identity3x3.y_axis);
			const mask4d is_largest_parallel_to_y = vector_greater_equal(vector_abs(largest_y_dot), vector_set(1.0F - zero_scale_threshold));
			const vector4d orthogonal_axis = vector_select(is_largest_parallel_to_y, identity3x3.z_axis, identity3x3.y_axis);

			second_largest_scale_axis = vector_cross3(largest_scale_axis, orthogonal_axis);

			// Recompute the scale to ensure we properly normalize
			// Here, the second largest axis should alread be normalized
			second_largest_scale = vector_length3(second_largest_scale_axis);
		}

		// Normalize the second largest axis which is non-zero
		second_largest_scale_axis = vector_div(second_largest_scale_axis, vector_set(second_largest_scale));

		vector4d third_largest_scale_axis = rotation_axes[third_largest_scale_component_i];
		double third_largest_scale = scale_ptr[third_largest_scale_component_i];
		if (third_largest_scale < zero_scale_threshold)
		{
			// The third largest scale value is zero
			// We'll use the two larger axes to build an orthogonal basis
			third_largest_scale_axis = vector_cross3(largest_scale_axis, second_largest_scale_axis);

			// Recompute the scale to ensure we properly normalize
			// Here, the third largest axis should alread be normalized
			third_largest_scale = vector_length3(third_largest_scale_axis);
		}

		// Normalize the third largest axis which is non-zero
		third_largest_scale_axis = vector_div(third_largest_scale_axis, vector_set(third_largest_scale));

		// Set our new basis
		rotation_axes[largest_scale_component_i] = largest_scale_axis;
		rotation_axes[second_largest_scale_component_i] = second_largest_scale_axis;
		rotation_axes[third_largest_scale_component_i] = third_largest_scale_axis;

		// Now that we have built the ortho-normal basis part of our rotation, we check its winding
		const double determinant = scalar_cast(matrix_determinant(rotation));
		if (determinant < 0.0)
		{
			// Our winding is reversed meaning one or three of the scale axes contains reflection
			// We negate an axis to flip the winding and negate the scale to match
			rotation.x_axis = vector_neg(rotation.x_axis);
			rotation.y_axis = vector_neg(rotation.y_axis);
			rotation.z_axis = vector_neg(rotation.z_axis);
			largest_scale = -largest_scale;
		}

		// Build a quaternion from the ortho-normal basis we found
		const quatd rotation_q = rtm_impl::quat_from_matrix(rotation.x_axis, rotation.y_axis, rotation.z_axis);

		return qvsd{ rotation_q, vector_set_w(translation, largest_scale) };
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the rotation part of a QVS transform.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE quatd RTM_SIMD_CALL qvs_get_rotation(qvsd_arg0 input) RTM_NO_EXCEPT
	{
		return input.rotation;
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the translation part of a QVS transform.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4d RTM_SIMD_CALL qvs_get_translation(qvsd_arg0 input) RTM_NO_EXCEPT
	{
		return input.translation_scale;
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the scale part of a QVS transform.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr rtm_impl::vector4d_vector_get_w RTM_SIMD_CALL qvs_get_scale(qvsd_arg0 input) RTM_NO_EXCEPT
	{
		return rtm_impl::vector4d_vector_get_w{ input.translation_scale };
	}

	//////////////////////////////////////////////////////////////////////////
	// Multiplies two QVS transforms.
	// Multiplication order is as follow: local_to_world = qvs_mul(local_to_object, object_to_world)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline qvsd RTM_SIMD_CALL qvs_mul(qvsd_arg0 lhs, qvsd_arg1 rhs) RTM_NO_EXCEPT
	{
		const vector4d rhs_scale = vector_dup_w(rhs.translation_scale);

		const quatd rotation = quat_mul(lhs.rotation, rhs.rotation);
		const vector4d translation = vector_add(quat_mul_vector3(vector_mul(lhs.translation_scale, rhs_scale), rhs.rotation), rhs.translation_scale);
		const vector4d scale_w = vector_mul(lhs.translation_scale, rhs.translation_scale);
		const vector4d translation_scale = vector_mix<mix4::x, mix4::y, mix4::z, mix4::d>(translation, scale_w);

		return qvsd{ rotation, translation_scale };
	}

	//////////////////////////////////////////////////////////////////////////
	// Multiplies two QVS transforms ignoring scale.
	// The resulting QVS transform will have the LHS scale.
	// Multiplication order is as follow: local_to_world = qvs_mul(local_to_object, object_to_world)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline qvsd RTM_SIMD_CALL qvs_mul_no_scale(qvsd_arg0 lhs, qvsd_arg1 rhs) RTM_NO_EXCEPT
	{
		const quatd rotation = quat_mul(lhs.rotation, rhs.rotation);
		const vector4d translation = vector_add(quat_mul_vector3(lhs.translation_scale, rhs.rotation), rhs.translation_scale);
		const vector4d translation_scale = vector_mix<mix4::x, mix4::y, mix4::z, mix4::d>(translation, lhs.translation_scale);

		return qvsd{ rotation, translation_scale };
	}

	//////////////////////////////////////////////////////////////////////////
	// Multiplies a QVS transform and a 3D point.
	// Multiplication order is as follow: world_position = qvs_mul_point3(local_position, local_to_world)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline vector4d RTM_SIMD_CALL qvs_mul_point3(vector4d_arg0 point, qvsd_arg1 qvs) RTM_NO_EXCEPT
	{
		const vector4d scale = vector_dup_w(qvs.translation_scale);
		return vector_add(quat_mul_vector3(vector_mul(scale, point), qvs.rotation), qvs.translation_scale);
	}

	//////////////////////////////////////////////////////////////////////////
	// Multiplies a QVS transform and a 3D point ignoring 3D scale.
	// Multiplication order is as follow: world_position = qvs_mul_point3_no_scale(local_position, local_to_world)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline vector4d RTM_SIMD_CALL qvs_mul_point3_no_scale(vector4d_arg0 point, qvsd_arg1 qvs) RTM_NO_EXCEPT
	{
		return vector_add(quat_mul_vector3(point, qvs.rotation), qvs.translation_scale);
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the inverse of the input QVS transform.
	// If zero scale is contained, the result is undefined.
	// For a safe alternative, supply a fallback scale value and a threshold.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline qvsd RTM_SIMD_CALL qvs_inverse(qvsd_arg0 input) RTM_NO_EXCEPT
	{
		const quatd inv_rotation = quat_conjugate(input.rotation);
		const vector4d inv_scale_w = vector_reciprocal(input.translation_scale);
		const vector4d inv_scale = vector_dup_w(inv_scale_w);
		const vector4d inv_translation = vector_neg(quat_mul_vector3(vector_mul(input.translation_scale, inv_scale), inv_rotation));
		const vector4d inv_translation_scale = vector_mix<mix4::x, mix4::y, mix4::z, mix4::d>(inv_translation, inv_scale_w);

		return qvsd{ inv_rotation, inv_translation_scale };
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the inverse of the input QVS transform.
	// If the input scale has an absolute value below the supplied threshold, the
	// fallback value is used instead.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline qvsd RTM_SIMD_CALL qvs_inverse(qvsd_arg0 input, double fallback_scale, double threshold = 1.0E-8) RTM_NO_EXCEPT
	{
		const quatd inv_rotation = quat_conjugate(input.rotation);
		const mask4d is_scale_w_zero = vector_less_equal(vector_abs(input.translation_scale), vector_set(threshold));
		const vector4d scale_w = vector_select(is_scale_w_zero, vector_set(fallback_scale), input.translation_scale);
		const vector4d inv_scale_w = vector_reciprocal(scale_w);
		const vector4d inv_scale = vector_dup_w(inv_scale_w);
		const vector4d inv_translation = vector_neg(quat_mul_vector3(vector_mul(input.translation_scale, inv_scale), inv_rotation));
		const vector4d inv_translation_scale = vector_mix<mix4::x, mix4::y, mix4::z, mix4::d>(inv_translation, inv_scale_w);

		return qvsd{ inv_rotation, inv_translation_scale };
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the inverse of the input QVS transform ignoring 3D scale.
	// The resulting QVS transform will have the input scale.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline qvsd RTM_SIMD_CALL qvs_inverse_no_scale(qvsd_arg0 input) RTM_NO_EXCEPT
	{
		const quatd inv_rotation = quat_conjugate(input.rotation);
		const vector4d inv_translation = vector_neg(quat_mul_vector3(input.translation_scale, inv_rotation));
		const vector4d inv_translation_scale = vector_mix<mix4::x, mix4::y, mix4::z, mix4::d>(inv_translation, input.translation_scale);

		return qvsd{ inv_rotation, inv_translation_scale };
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns a QVS transforms with the rotation part normalized.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE qvsd RTM_SIMD_CALL qvs_normalize(qvsd_arg0 input) RTM_NO_EXCEPT
	{
		const quatd rotation = quat_normalize(input.rotation);
		return qvsd{ rotation, input.translation_scale };
	}

	//////////////////////////////////////////////////////////////////////////
	// Per component linear interpolation of the two inputs at the specified alpha.
	// The formula used is: ((1.0 - alpha) * start) + (alpha * end).
	// Interpolation is stable and will return 'start' when alpha is 0.0 and 'end' when it is 1.0.
	// This is the same instruction count when FMA is present but it might be slightly slower
	// due to the extra multiplication compared to: start + (alpha * (end - start)).
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE qvsd RTM_SIMD_CALL qvs_lerp(qvsd_arg0 start, qvsd_arg0 end, double alpha) RTM_NO_EXCEPT
	{
		const quatd rotation = quat_lerp(start.rotation, end.rotation, alpha);
		const vector4d translation_scale = vector_lerp(start.translation_scale, end.translation_scale, alpha);
		return qvsd{ rotation, translation_scale };
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Per component linear interpolation of the two inputs at the specified alpha.
	// The formula used is: ((1.0 - alpha) * start) + (alpha * end).
	// Interpolation is stable and will return 'start' when alpha is 0.0 and 'end' when it is 1.0.
	// This is the same instruction count when FMA is present but it might be slightly slower
	// due to the extra multiplication compared to: start + (alpha * (end - start)).
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE qvsd RTM_SIMD_CALL qvs_lerp(qvsd_arg0 start, qvsd_arg0 end, scalard_argn alpha) RTM_NO_EXCEPT
	{
		const quatd rotation = quat_lerp(start.rotation, end.rotation, alpha);
		const vector4d translation_scale = vector_lerp(start.translation_scale, end.translation_scale, alpha);
		return qvsd{ rotation, translation_scale };
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Per component linear interpolation of the two inputs at the specified alpha.
	// The formula used is: ((1.0 - alpha) * start) + (alpha * end).
	// Interpolation is stable and will return 'start' when alpha is 0.0 and 'end' when it is 1.0.
	// This is the same instruction count when FMA is present but it might be slightly slower
	// due to the extra multiplication compared to: start + (alpha * (end - start)).
	// The resulting QVV transform will have the start scale.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE qvsd RTM_SIMD_CALL qvs_lerp_no_scale(qvsd_arg0 start, qvsd_arg0 end, double alpha) RTM_NO_EXCEPT
	{
		const quatd rotation = quat_lerp(start.rotation, end.rotation, alpha);
		const vector4d translation_scale_lerp = vector_lerp(start.translation_scale, end.translation_scale, alpha);
		const vector4d translation_scale = vector_mix<mix4::x, mix4::y, mix4::z, mix4::d>(translation_scale_lerp, start.translation_scale);
		return qvsd{ rotation, translation_scale };
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Per component linear interpolation of the two inputs at the specified alpha.
	// The formula used is: ((1.0 - alpha) * start) + (alpha * end).
	// Interpolation is stable and will return 'start' when alpha is 0.0 and 'end' when it is 1.0.
	// This is the same instruction count when FMA is present but it might be slightly slower
	// due to the extra multiplication compared to: start + (alpha * (end - start)).
	// The resulting QVV transform will have the start scale.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE qvsd RTM_SIMD_CALL qvs_lerp_no_scale(qvsd_arg0 start, qvsd_arg0 end, scalard_argn alpha) RTM_NO_EXCEPT
	{
		const quatd rotation = quat_lerp(start.rotation, end.rotation, alpha);
		const vector4d translation_scale_lerp = vector_lerp(start.translation_scale, end.translation_scale, alpha);
		const vector4d translation_scale = vector_mix<mix4::x, mix4::y, mix4::z, mix4::d>(translation_scale_lerp, start.translation_scale);
		return qvsd{ rotation, translation_scale };
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Per component spherical interpolation of the two inputs at the specified alpha.
	// See quat_slerp(..)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE qvsd RTM_SIMD_CALL qvs_slerp(qvsd_arg0 start, qvsd_arg0 end, double alpha) RTM_NO_EXCEPT
	{
		const quatd rotation = quat_slerp(start.rotation, end.rotation, alpha);
		const vector4d translation_scale = vector_lerp(start.translation_scale, end.translation_scale, alpha);
		return qvsd{ rotation, translation_scale };
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Per component spherical interpolation of the two inputs at the specified alpha.
	// See quat_slerp(..)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE qvsd RTM_SIMD_CALL qvs_slerp(qvsd_arg0 start, qvsd_arg0 end, scalard_argn alpha) RTM_NO_EXCEPT
	{
		const quatd rotation = quat_slerp(start.rotation, end.rotation, alpha);
		const vector4d translation_scale = vector_lerp(start.translation_scale, end.translation_scale, alpha);
		return qvsd{ rotation, translation_scale };
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Per component spherical interpolation of the two inputs at the specified alpha.
	// See quat_slerp(..)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE qvsd RTM_SIMD_CALL qvs_slerp_no_scale(qvsd_arg0 start, qvsd_arg1 end, double alpha) RTM_NO_EXCEPT
	{
		const quatd rotation = quat_slerp(start.rotation, end.rotation, alpha);
		const vector4d translation_scale_lerp = vector_lerp(start.translation_scale, end.translation_scale, alpha);
		const vector4d translation_scale = vector_mix<mix4::x, mix4::y, mix4::z, mix4::d>(translation_scale_lerp, start.translation_scale);
		return qvsd{ rotation, translation_scale };
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Per component spherical interpolation of the two inputs at the specified alpha.
	// See quat_slerp(..)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE qvsd RTM_SIMD_CALL qvs_slerp_no_scale(qvsd_arg0 start, qvsd_arg1 end, scalard_argn alpha) RTM_NO_EXCEPT
	{
		const quatd rotation = quat_slerp(start.rotation, end.rotation, alpha);
		const vector4d translation_scale_lerp = vector_lerp(start.translation_scale, end.translation_scale, alpha);
		const vector4d translation_scale = vector_mix<mix4::x, mix4::y, mix4::z, mix4::d>(translation_scale_lerp, start.translation_scale);
		return qvsd{ rotation, translation_scale };
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Returns true if the input QVS does not contain any NaN or Inf, otherwise false.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL qvs_is_finite(qvsd_arg0 input) RTM_NO_EXCEPT
	{
		return quat_is_finite(input.rotation) && vector_is_finite(input.translation_scale);
	}

	RTM_IMPL_VERSION_NAMESPACE_END
}

RTM_IMPL_FILE_PRAGMA_POP

```

`includes/rtm/qvsf.h`:

```h
#pragma once

////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2023 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include "rtm/math.h"
#include "rtm/matrix3x3f.h"
#include "rtm/quatf.h"
#include "rtm/vector4f.h"
#include "rtm/version.h"
#include "rtm/impl/compiler_utils.h"
#include "rtm/impl/qvs_common.h"
#include "rtm/impl/matrix_affine_common.h"

RTM_IMPL_FILE_PRAGMA_PUSH

namespace rtm
{
	RTM_IMPL_VERSION_NAMESPACE_BEGIN

	//////////////////////////////////////////////////////////////////////////
	// Casts a QVS transform float64 variant to a float32 variant.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE qvsf RTM_SIMD_CALL qvs_cast(const qvsd& input) RTM_NO_EXCEPT
	{
		return qvsf{ quat_cast(input.rotation), vector_cast(input.translation_scale) };
	}

	//////////////////////////////////////////////////////////////////////////
	// Converts a rotation 3x3 matrix into a QVS transform.
	// The translation/scale will be the identity.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline qvsf RTM_SIMD_CALL qvs_from_matrix(matrix3x3f_arg0 input) RTM_NO_EXCEPT
	{
		const quatf rotation = rtm_impl::quat_from_matrix(input.x_axis, input.y_axis, input.z_axis);
		return qvsf{ rotation, vector_set(0.0F, 0.0F, 0.0F, 1.0F) };
	}

	//////////////////////////////////////////////////////////////////////////
	// Converts a rotation 3x4 affine matrix into a QVS transform.
	// If the matrix scale is non-uniform, the scale of the largest axis will be returned.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline qvsf RTM_SIMD_CALL qvs_from_matrix(matrix3x4f_arg0 input) RTM_NO_EXCEPT
	{
		// See qvv_from_matrix for details, the process here is very similar
		// We still treat every axis independently as if they have unique scale values
		// to be able to handle ill-formed matrices with non-uniform scale.
		// Unlike with QVV transforms, if the scale is uniform and negative, we know
		// that all three axes have reflection and we can recover the original rotation.

		// Translation is always in the W axis
		const vector4f translation = input.w_axis;

		// First, we compute our absolute scale values
		const scalarf x_scale = vector_length3_as_scalar(input.x_axis);
		const scalarf y_scale = vector_length3_as_scalar(input.y_axis);
		const scalarf z_scale = vector_length3_as_scalar(input.z_axis);
		const vector4f scale = vector_set(x_scale, y_scale, z_scale);

		// Grab a pointer to the first scale value, we'll index into it
		const float* scale_ptr = rtm_impl::bit_cast<const float*>(&scale);

		// Next, we find the largest scale components and sort them from largest to smallest
		component3 largest_scale_component;
		component3 second_largest_scale_component;
		component3 third_largest_scale_component;

		if (scalar_greater_equal(x_scale, y_scale))
		{
			// X >= Y
			if (scalar_greater_equal(y_scale, z_scale))
			{
				// X >= Y >= Z
				largest_scale_component = component3::x;
				second_largest_scale_component = component3::y;
				third_largest_scale_component = component3::z;
			}
			else
			{
				if (scalar_greater_equal(x_scale, z_scale))
				{
					// X >= Z >= Y
					largest_scale_component = component3::x;
					second_largest_scale_component = component3::z;
				}
				else
				{
					// Z >= X >= Y
					largest_scale_component = component3::z;
					second_largest_scale_component = component3::x;
				}

				third_largest_scale_component = component3::y;
			}
		}
		else
		{
			if (scalar_greater_equal(x_scale, z_scale))
			{
				// Y > X >= Z
				largest_scale_component = component3::y;
				second_largest_scale_component = component3::x;
				third_largest_scale_component = component3::z;
			}
			else
			{
				if (scalar_greater_equal(y_scale, z_scale))
				{
					// Y >= Z > X
					largest_scale_component = component3::y;
					second_largest_scale_component = component3::z;
				}
				else
				{
					// Z > Y > X
					largest_scale_component = component3::z;
					second_largest_scale_component = component3::y;
				}

				third_largest_scale_component = component3::x;
			}
		}

		// Cast to integer for array indexing
		const int32_t largest_scale_component_i = static_cast<int32_t>(largest_scale_component);
		const int32_t second_largest_scale_component_i = static_cast<int32_t>(second_largest_scale_component);
		const int32_t third_largest_scale_component_i = static_cast<int32_t>(third_largest_scale_component);

		const matrix3x3f identity3x3 = matrix_identity();

		// Copy the rotation part from our input
		matrix3x3f rotation = matrix_cast(input);

		// Grab a pointer to the first axis, we'll index into it
		vector4f* rotation_axes = &rotation.x_axis;

		// We use a threshold to test for zero because division with near zero values is imprecise
		// and might not yield a normalized result
		const float zero_scale_threshold = 1.0E-8F;
		float largest_scale = scale_ptr[largest_scale_component_i];
		if (largest_scale < zero_scale_threshold)
		{
			// The largest scale value is zero which means all three are zero
			// We'll return the identity rotation since its value is not recoverable
			const quatf rotation_q = quat_identity();
			const vector4f translation_scale = vector_set_w(translation, largest_scale);
			return qvsf{ rotation_q, translation_scale };
		}

		// Normalize the largest scale axis which is non-zero
		vector4f largest_scale_axis = rotation_axes[largest_scale_component_i];
		largest_scale_axis = vector_div(largest_scale_axis, vector_set(largest_scale));

		vector4f second_largest_scale_axis = rotation_axes[second_largest_scale_component_i];
		float second_largest_scale = scale_ptr[second_largest_scale_component_i];
		if (second_largest_scale < zero_scale_threshold)
		{
			// The second largest scale value is zero which means the two smallest axes are zero
			// We'll use the largest axis and build an orthogonal basis around it
			const vector4f largest_y_dot = vector_dot3_as_vector(largest_scale_axis, identity3x3.y_axis);
			const mask4f is_largest_parallel_to_y = vector_greater_equal(vector_abs(largest_y_dot), vector_set(1.0F - zero_scale_threshold));
			const vector4f orthogonal_axis = vector_select(is_largest_parallel_to_y, identity3x3.z_axis, identity3x3.y_axis);

			second_largest_scale_axis = vector_cross3(largest_scale_axis, orthogonal_axis);

			// Recompute the scale to ensure we properly normalize
			// Here, the second largest axis should alread be normalized
			second_largest_scale = vector_length3(second_largest_scale_axis);
		}

		// Normalize the second largest axis which is non-zero
		second_largest_scale_axis = vector_div(second_largest_scale_axis, vector_set(second_largest_scale));

		vector4f third_largest_scale_axis = rotation_axes[third_largest_scale_component_i];
		float third_largest_scale = scale_ptr[third_largest_scale_component_i];
		if (third_largest_scale < zero_scale_threshold)
		{
			// The third largest scale value is zero
			// We'll use the two larger axes to build an orthogonal basis
			third_largest_scale_axis = vector_cross3(largest_scale_axis, second_largest_scale_axis);

			// Recompute the scale to ensure we properly normalize
			// Here, the third largest axis should alread be normalized
			third_largest_scale = vector_length3(third_largest_scale_axis);
		}

		// Normalize the third largest axis which is non-zero
		third_largest_scale_axis = vector_div(third_largest_scale_axis, vector_set(third_largest_scale));

		// Set our new basis
		rotation_axes[largest_scale_component_i] = largest_scale_axis;
		rotation_axes[second_largest_scale_component_i] = second_largest_scale_axis;
		rotation_axes[third_largest_scale_component_i] = third_largest_scale_axis;

		// Now that we have built the ortho-normal basis part of our rotation, we check its winding
		const float determinant = scalar_cast(matrix_determinant(rotation));
		if (determinant < 0.0F)
		{
			// Our winding is reversed meaning our three scale axes contain reflection
			// We negate every axis to flip the winding and negate the scale to match
			rotation.x_axis = vector_neg(rotation.x_axis);
			rotation.y_axis = vector_neg(rotation.y_axis);
			rotation.z_axis = vector_neg(rotation.z_axis);
			largest_scale = -largest_scale;
		}

		// Build a quaternion from the ortho-normal basis we found
		const quatf rotation_q = rtm_impl::quat_from_matrix(rotation.x_axis, rotation.y_axis, rotation.z_axis);

		return qvsf{ rotation_q, vector_set_w(translation, largest_scale) };
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the rotation part of a QVS transform.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE quatf RTM_SIMD_CALL qvs_get_rotation(qvsf_arg0 input) RTM_NO_EXCEPT
	{
		return input.rotation;
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the translation part of a QVS transform.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4f RTM_SIMD_CALL qvs_get_translation(qvsf_arg0 input) RTM_NO_EXCEPT
	{
		return input.translation_scale;
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the scale part of a QVS transform.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr rtm_impl::vector4f_vector_get_w RTM_SIMD_CALL qvs_get_scale(qvsf_arg0 input) RTM_NO_EXCEPT
	{
		return rtm_impl::vector4f_vector_get_w{ input.translation_scale };
	}

	//////////////////////////////////////////////////////////////////////////
	// Multiplies two QVS transforms.
	// Multiplication order is as follow: local_to_world = qvs_mul(local_to_object, object_to_world)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline qvsf RTM_SIMD_CALL qvs_mul(qvsf_arg0 lhs, qvsf_arg1 rhs) RTM_NO_EXCEPT
	{
		const vector4f rhs_scale = vector_dup_w(rhs.translation_scale);

		const quatf rotation = quat_mul(lhs.rotation, rhs.rotation);
		const vector4f translation = vector_add(quat_mul_vector3(vector_mul(lhs.translation_scale, rhs_scale), rhs.rotation), rhs.translation_scale);
		const vector4f scale_w = vector_mul(lhs.translation_scale, rhs.translation_scale);
		const vector4f translation_scale = vector_mix<mix4::x, mix4::y, mix4::z, mix4::d>(translation, scale_w);

		return qvsf{ rotation, translation_scale };
	}

	//////////////////////////////////////////////////////////////////////////
	// Multiplies two QVS transforms ignoring scale.
	// The resulting QVS transform will have the LHS scale.
	// Multiplication order is as follow: local_to_world = qvs_mul(local_to_object, object_to_world)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline qvsf RTM_SIMD_CALL qvs_mul_no_scale(qvsf_arg0 lhs, qvsf_arg1 rhs) RTM_NO_EXCEPT
	{
		const quatf rotation = quat_mul(lhs.rotation, rhs.rotation);
		const vector4f translation = vector_add(quat_mul_vector3(lhs.translation_scale, rhs.rotation), rhs.translation_scale);
		const vector4f translation_scale = vector_mix<mix4::x, mix4::y, mix4::z, mix4::d>(translation, lhs.translation_scale);

		return qvsf{ rotation, translation_scale };
	}

	//////////////////////////////////////////////////////////////////////////
	// Multiplies a QVS transform and a 3D point.
	// Multiplication order is as follow: world_position = qvs_mul_point3(local_position, local_to_world)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline vector4f RTM_SIMD_CALL qvs_mul_point3(vector4f_arg0 point, qvsf_arg1 qvs) RTM_NO_EXCEPT
	{
		const vector4f scale = vector_dup_w(qvs.translation_scale);
		return vector_add(quat_mul_vector3(vector_mul(scale, point), qvs.rotation), qvs.translation_scale);
	}

	//////////////////////////////////////////////////////////////////////////
	// Multiplies a QVS transform and a 3D point ignoring 3D scale.
	// Multiplication order is as follow: world_position = qvs_mul_point3_no_scale(local_position, local_to_world)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline vector4f RTM_SIMD_CALL qvs_mul_point3_no_scale(vector4f_arg0 point, qvsf_arg1 qvs) RTM_NO_EXCEPT
	{
		return vector_add(quat_mul_vector3(point, qvs.rotation), qvs.translation_scale);
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the inverse of the input QVS transform.
	// If zero scale is contained, the result is undefined.
	// For a safe alternative, supply a fallback scale value and a threshold.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline qvsf RTM_SIMD_CALL qvs_inverse(qvsf_arg0 input) RTM_NO_EXCEPT
	{
		const quatf inv_rotation = quat_conjugate(input.rotation);
		const vector4f inv_scale_w = vector_reciprocal(input.translation_scale);
		const vector4f inv_scale = vector_dup_w(inv_scale_w);
		const vector4f inv_translation = vector_neg(quat_mul_vector3(vector_mul(input.translation_scale, inv_scale), inv_rotation));
		const vector4f inv_translation_scale = vector_mix<mix4::x, mix4::y, mix4::z, mix4::d>(inv_translation, inv_scale_w);

		return qvsf{ inv_rotation, inv_translation_scale };
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the inverse of the input QVS transform.
	// If the input scale has an absolute value below the supplied threshold, the
	// fallback value is used instead.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline qvsf RTM_SIMD_CALL qvs_inverse(qvsf_arg0 input, float fallback_scale, float threshold = 1.0E-8F) RTM_NO_EXCEPT
	{
		const quatf inv_rotation = quat_conjugate(input.rotation);
		const mask4f is_scale_w_zero = vector_less_equal(vector_abs(input.translation_scale), vector_set(threshold));
		const vector4f scale_w = vector_select(is_scale_w_zero, vector_set(fallback_scale), input.translation_scale);
		const vector4f inv_scale_w = vector_reciprocal(scale_w);
		const vector4f inv_scale = vector_dup_w(inv_scale_w);
		const vector4f inv_translation = vector_neg(quat_mul_vector3(vector_mul(input.translation_scale, inv_scale), inv_rotation));
		const vector4f inv_translation_scale = vector_mix<mix4::x, mix4::y, mix4::z, mix4::d>(inv_translation, inv_scale_w);

		return qvsf{ inv_rotation, inv_translation_scale };
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the inverse of the input QVS transform ignoring 3D scale.
	// The resulting QVS transform will have the input scale.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline qvsf RTM_SIMD_CALL qvs_inverse_no_scale(qvsf_arg0 input) RTM_NO_EXCEPT
	{
		const quatf inv_rotation = quat_conjugate(input.rotation);
		const vector4f inv_translation = vector_neg(quat_mul_vector3(input.translation_scale, inv_rotation));
		const vector4f inv_translation_scale = vector_mix<mix4::x, mix4::y, mix4::z, mix4::d>(inv_translation, input.translation_scale);

		return qvsf{ inv_rotation, inv_translation_scale };
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns a QVS transforms with the rotation part normalized.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE qvsf RTM_SIMD_CALL qvs_normalize(qvsf_arg0 input) RTM_NO_EXCEPT
	{
		const quatf rotation = quat_normalize(input.rotation);
		return qvsf{ rotation, input.translation_scale };
	}

	//////////////////////////////////////////////////////////////////////////
	// Per component linear interpolation of the two inputs at the specified alpha.
	// The formula used is: ((1.0 - alpha) * start) + (alpha * end).
	// Interpolation is stable and will return 'start' when alpha is 0.0 and 'end' when it is 1.0.
	// This is the same instruction count when FMA is present but it might be slightly slower
	// due to the extra multiplication compared to: start + (alpha * (end - start)).
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE qvsf RTM_SIMD_CALL qvs_lerp(qvsf_arg0 start, qvsf_arg1 end, float alpha) RTM_NO_EXCEPT
	{
		const quatf rotation = quat_lerp(start.rotation, end.rotation, alpha);
		const vector4f translation_scale = vector_lerp(start.translation_scale, end.translation_scale, alpha);
		return qvsf{ rotation, translation_scale };
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Per component linear interpolation of the two inputs at the specified alpha.
	// The formula used is: ((1.0 - alpha) * start) + (alpha * end).
	// Interpolation is stable and will return 'start' when alpha is 0.0 and 'end' when it is 1.0.
	// This is the same instruction count when FMA is present but it might be slightly slower
	// due to the extra multiplication compared to: start + (alpha * (end - start)).
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE qvsf RTM_SIMD_CALL qvs_lerp(qvsf_arg0 start, qvsf_arg1 end, scalarf_arg2 alpha) RTM_NO_EXCEPT
	{
		const quatf rotation = quat_lerp(start.rotation, end.rotation, alpha);
		const vector4f translation_scale = vector_lerp(start.translation_scale, end.translation_scale, alpha);
		return qvsf{ rotation, translation_scale };
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Per component linear interpolation of the two inputs at the specified alpha.
	// The formula used is: ((1.0 - alpha) * start) + (alpha * end).
	// Interpolation is stable and will return 'start' when alpha is 0.0 and 'end' when it is 1.0.
	// This is the same instruction count when FMA is present but it might be slightly slower
	// due to the extra multiplication compared to: start + (alpha * (end - start)).
	// The resulting QVV transform will have the start scale.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE qvsf RTM_SIMD_CALL qvs_lerp_no_scale(qvsf_arg0 start, qvsf_arg1 end, float alpha) RTM_NO_EXCEPT
	{
		const quatf rotation = quat_lerp(start.rotation, end.rotation, alpha);
		const vector4f translation_scale_lerp = vector_lerp(start.translation_scale, end.translation_scale, alpha);
		const vector4f translation_scale = vector_mix<mix4::x, mix4::y, mix4::z, mix4::d>(translation_scale_lerp, start.translation_scale);
		return qvsf{ rotation, translation_scale };
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Per component linear interpolation of the two inputs at the specified alpha.
	// The formula used is: ((1.0 - alpha) * start) + (alpha * end).
	// Interpolation is stable and will return 'start' when alpha is 0.0 and 'end' when it is 1.0.
	// This is the same instruction count when FMA is present but it might be slightly slower
	// due to the extra multiplication compared to: start + (alpha * (end - start)).
	// The resulting QVV transform will have the start scale.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE qvsf RTM_SIMD_CALL qvs_lerp_no_scale(qvsf_arg0 start, qvsf_arg1 end, scalarf_arg2 alpha) RTM_NO_EXCEPT
	{
		const quatf rotation = quat_lerp(start.rotation, end.rotation, alpha);
		const vector4f translation_scale_lerp = vector_lerp(start.translation_scale, end.translation_scale, alpha);
		const vector4f translation_scale = vector_mix<mix4::x, mix4::y, mix4::z, mix4::d>(translation_scale_lerp, start.translation_scale);
		return qvsf{ rotation, translation_scale };
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Per component spherical interpolation of the two inputs at the specified alpha.
	// See quat_slerp(..)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE qvsf RTM_SIMD_CALL qvs_slerp(qvsf_arg0 start, qvsf_arg1 end, float alpha) RTM_NO_EXCEPT
	{
		const quatf rotation = quat_slerp(start.rotation, end.rotation, alpha);
		const vector4f translation_scale = vector_lerp(start.translation_scale, end.translation_scale, alpha);
		return qvsf{ rotation, translation_scale };
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Per component spherical interpolation of the two inputs at the specified alpha.
	// See quat_slerp(..)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE qvsf RTM_SIMD_CALL qvs_slerp(qvsf_arg0 start, qvsf_arg1 end, scalarf_arg2 alpha) RTM_NO_EXCEPT
	{
		const quatf rotation = quat_slerp(start.rotation, end.rotation, alpha);
		const vector4f translation_scale = vector_lerp(start.translation_scale, end.translation_scale, alpha);
		return qvsf{ rotation, translation_scale };
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Per component spherical interpolation of the two inputs at the specified alpha.
	// See quat_slerp(..)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE qvsf RTM_SIMD_CALL qvs_slerp_no_scale(qvsf_arg0 start, qvsf_arg1 end, float alpha) RTM_NO_EXCEPT
	{
		const quatf rotation = quat_slerp(start.rotation, end.rotation, alpha);
		const vector4f translation_scale_lerp = vector_lerp(start.translation_scale, end.translation_scale, alpha);
		const vector4f translation_scale = vector_mix<mix4::x, mix4::y, mix4::z, mix4::d>(translation_scale_lerp, start.translation_scale);
		return qvsf{ rotation, translation_scale };
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Per component spherical interpolation of the two inputs at the specified alpha.
	// See quat_slerp(..)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE qvsf RTM_SIMD_CALL qvs_slerp_no_scale(qvsf_arg0 start, qvsf_arg1 end, scalarf_arg2 alpha) RTM_NO_EXCEPT
	{
		const quatf rotation = quat_slerp(start.rotation, end.rotation, alpha);
		const vector4f translation_scale_lerp = vector_lerp(start.translation_scale, end.translation_scale, alpha);
		const vector4f translation_scale = vector_mix<mix4::x, mix4::y, mix4::z, mix4::d>(translation_scale_lerp, start.translation_scale);
		return qvsf{ rotation, translation_scale };
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Returns true if the input QVS does not contain any NaN or Inf, otherwise false.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL qvs_is_finite(qvsf_arg0 input) RTM_NO_EXCEPT
	{
		return quat_is_finite(input.rotation) && vector_is_finite(input.translation_scale);
	}

	RTM_IMPL_VERSION_NAMESPACE_END
}

RTM_IMPL_FILE_PRAGMA_POP

```

`includes/rtm/qvvd.h`:

```h
#pragma once

////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2017 Nicholas Frechette & Animation Compression Library contributors
// Copyright (c) 2018 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include "rtm/math.h"
#include "rtm/quatd.h"
#include "rtm/vector4d.h"
#include "rtm/matrix3x4d.h"
#include "rtm/version.h"
#include "rtm/impl/compiler_utils.h"
#include "rtm/impl/qvv_common.h"
#include "rtm/impl/matrix_affine_common.h"

RTM_IMPL_FILE_PRAGMA_PUSH

////////////////////////////////////////////////////////////////////////////////
// WARNING: QVV transforms do _NOT_ form a group. It has no well defined multiplication and inverse.
// This is because when non-uniform scale is rotated (e.g. as the result of two QVV multiplying),
// shear is introduced and there is no place to store it. As such, QVV discards the introduced shear
// which is mathematically incorrect. As a result of this, QVV multiplication is _NOT_ assossiative and
// the inverse is incorrect. USE AT YOUR OWN RISK.
////////////////////////////////////////////////////////////////////////////////

namespace rtm
{
	RTM_IMPL_VERSION_NAMESPACE_BEGIN

	//////////////////////////////////////////////////////////////////////////
	// Casts a QVV transform float32 variant to a float64 variant.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE qvvd RTM_SIMD_CALL qvv_cast(qvvf_arg0 input) RTM_NO_EXCEPT
	{
		return qvvd{ quat_cast(input.rotation), vector_cast(input.translation), vector_cast(input.scale) };
	}

	//////////////////////////////////////////////////////////////////////////
	// Converts a pure rotation 3x3 matrix into a QVV transform.
	// The translation/scale will be the identity.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline qvvd RTM_SIMD_CALL qvv_from_matrix(matrix3x3d_arg0 input) RTM_NO_EXCEPT
	{
		const quatd rotation = rtm_impl::quat_from_matrix(input.x_axis, input.y_axis, input.z_axis);
		const vector4d translation = vector_zero();
		const vector4d scale = vector_set(1.0);
		return qvvd{ rotation, translation, scale };
	}

	//////////////////////////////////////////////////////////////////////////
	// Converts a 3x4 affine matrix into a QVV transform.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline qvvd RTM_SIMD_CALL qvv_from_matrix(matrix3x4d_arg0 input) RTM_NO_EXCEPT
	{
		// Let us define the following:
		// A = S * R * T
		// A is an affine matrix that scales, rotates, then translates (an SRT transform)
		// The translation part lives in the W row and is easily recovered as the scale/rotation
		// components are stored in the XYZ rows. For the remainder of the discussion, we'll omit
		// translation and only consider scale/rotation.
		//
		// For scale and rotation, recovering the original values S and R is more complicated
		// if we allow scale to be zero or negative.
		//
		// Let us first consider the simple case where scale is always non-zero and positive.
		// When this is the case, we can recover the matrix S by computing the length of each
		// of the XYZ rows (R is ortho-normalized and its rows thus have unit length). R can
		// then be recovered by normalizing the XYZ rows of A.
		//
		// If scale can be zero, we can recover S in the same way but R requires a bit more care.
		// With a zero row, we cannot normalize it (due to division by zero). To retain orthogonality,
		// we must use the cross product to recover our missing values where possible. To that end,
		// the scale axes are sorted from largest to smallest and we process them starting with the largest.
		//     - If the largest scale axis is below our precision threshold, we cannot normalize it. It also means
		//       that the remaining two axes are also below our precision threshold. As such, we will return a zero
		//       scale matrix and the identity rotation matrix. The rotation matrix doesn't matter when we have zero scale.
		//     - If the second largest scale axis is below our precision threshold, we cannot normalize it. It also
		//       means that the third axis is also below our precision threshold. We thus need to find two axes orthogonal
		//       to the first. To that end, we test the dot product of the first axis with the identity Y axis. If
		//       the two are parallel, we will use the identity Z axis for the first cross product, otherwise we use the Y axis.
		//       This yields our second axis that has zero scale (the axis doesn't matter since we have zero scale).
		//       We find our third axis by taking the cross product of the first two, which also has zero scale.
		//     - If the third largest scale axis is below our precision threshold, we cannot normalize it. Since we already
		//       have two valid axes, we can find our third using the cross product and it will have zero scale.
		// As such, if one or more axes are zero, we can recover the original S matrix but not the original R matrix.
		// However, when the new rotation matrix R' is multiplied with the S matrix, it yields the same affine
		// matrix A because anything multiplied with zero yields zero:
		// A = S * R'
		//
		// Negative scale throws a wrench in things. Negative scaling along one axis corresponds to
		// performing a reflection along that axis. When a single axis is reflected, the matrix A
		// will no longer have the same winding (it flips with every reflected axis). As such, if we
		// extract S by using the row lengths, we cannot recover R as the resulting rotation matrix
		// will be ill formed due to its winding.
		// As such, when negative scale is present, we will not be able to recover the original S and R
		// matrices. Let us look at every possible scenario:
		//     - If two axes are reflected, they are combined into a new rotation matrix. This happens because
		//       two successive reflections (F1 and F2) can be represented by a single rotation (RF) like so:
		//       RF = F1 * F2
		//       And two rotations (R1 and R2) can be combined into a single rotation R' with: R' = R1 * R2
		//       Let us express everything now:
		//       S = S' * F1 * F2 (some scaling with 2 reflections)
		//       A = S' * F1 * F2 * R
		//       A = S' * R'
		//       And so while we cannot recover the original S and R matrices, we can find two matrices
		//       that yield the same resulting affine transform.
		//     - If three axes are reflected, the winding is reversed and we have the following:
		//       S = S' * F1 * F2 * F3
		//       A = S' * F1 * F2 * F3 * R
		//       A = S' * F1 * R'
		//       And so we see that the case of three reflected axes is equivalent to the single reflection case.
		//     - If a single axis is reflected, the winding is reversed and we have the following:
		//       S = S' * F1
		//       A = S' * F1 * R
		//       When this occurs, we cannot tell which axis was originally reflected. But thankfully, it does
		//       not matter. To find a solution, we negate one axis (any) in both the scale matrix
		//       and the rotation matrix. When the two will be multiplied, the two negations will cancel out.
		//       By negating an axis on the rotation matrix, we will reverse the winding back into the one we
		//       want to find the rotation we are looking for. We negate the equivalent scale axis only to be
		//       able to recover the original matrix. And so we have:
		//       A = S' * R'
		//       The axis we choose to negate doesn't matter as long as it doesn't have zero scale (zero cannot be negated).
		//       To that end, we negate the largest scale axis.
		//
		// Taking everything together, we first handle positive and zero scale together by finding our
		// orthogonal basis and positive (or zero) scale values. Then, if the determinant is negative, the winding
		// is reversed and we must apply our reflection fixup as described above by negating the largest scale axis
		// in both the orthogonal basis (to restore the correct winding) and on our scale to cancel it out.
		// If all three axes have zero scale, we must early out and skip this last step as we know there is no reflection.
		// If one or two axes have zero scale, they will be reconstructed using the cross product which means that the
		// winding will never be reversed and no reflection fixup will be performed.

		// Translation is always in the W axis
		const vector4d translation = input.w_axis;

		// First, we compute our absolute scale values
		const scalard x_scale = vector_length3_as_scalar(input.x_axis);
		const scalard y_scale = vector_length3_as_scalar(input.y_axis);
		const scalard z_scale = vector_length3_as_scalar(input.z_axis);
		vector4d scale = vector_set(x_scale, y_scale, z_scale);

		// Grab a pointer to the first scale value, we'll index into it
		double* scale_ptr = rtm_impl::bit_cast<double*>(&scale);

		// Next, we find the largest scale components and sort them from largest to smallest
		component3 largest_scale_component;
		component3 second_largest_scale_component;
		component3 third_largest_scale_component;

		if (scalar_greater_equal(x_scale, y_scale))
		{
			// X >= Y
			if (scalar_greater_equal(y_scale, z_scale))
			{
				// X >= Y >= Z
				largest_scale_component = component3::x;
				second_largest_scale_component = component3::y;
				third_largest_scale_component = component3::z;
			}
			else
			{
				if (scalar_greater_equal(x_scale, z_scale))
				{
					// X >= Z >= Y
					largest_scale_component = component3::x;
					second_largest_scale_component = component3::z;
				}
				else
				{
					// Z >= X >= Y
					largest_scale_component = component3::z;
					second_largest_scale_component = component3::x;
				}

				third_largest_scale_component = component3::y;
			}
		}
		else
		{
			if (scalar_greater_equal(x_scale, z_scale))
			{
				// Y > X >= Z
				largest_scale_component = component3::y;
				second_largest_scale_component = component3::x;
				third_largest_scale_component = component3::z;
			}
			else
			{
				if (scalar_greater_equal(y_scale, z_scale))
				{
					// Y >= Z > X
					largest_scale_component = component3::y;
					second_largest_scale_component = component3::z;
				}
				else
				{
					// Z > Y > X
					largest_scale_component = component3::z;
					second_largest_scale_component = component3::y;
				}

				third_largest_scale_component = component3::x;
			}
		}

		// Cast to integer for array indexing
		const int32_t largest_scale_component_i = static_cast<int32_t>(largest_scale_component);
		const int32_t second_largest_scale_component_i = static_cast<int32_t>(second_largest_scale_component);
		const int32_t third_largest_scale_component_i = static_cast<int32_t>(third_largest_scale_component);

		const matrix3x3d identity3x3 = matrix_identity();

		// Copy the rotation part from our input
		matrix3x3d rotation = matrix_cast(input);

		// Grab a pointer to the first axis, we'll index into it
		vector4d* rotation_axes = &rotation.x_axis;

		// We use a threshold to test for zero because division with near zero values is imprecise
		// and might not yield a normalized result
		const double zero_scale_threshold = 1.0E-8;
		double largest_scale = scale_ptr[largest_scale_component_i];
		if (largest_scale < zero_scale_threshold)
		{
			// The largest scale value is zero which means all three are zero
			// We'll return the identity rotation since its value is not recoverable
			const quatd rotation_q = quat_identity();
			return qvvd{ rotation_q, translation, scale };
		}

		// Normalize the largest scale axis which is non-zero
		vector4d largest_scale_axis = rotation_axes[largest_scale_component_i];
		largest_scale_axis = vector_div(largest_scale_axis, vector_set(largest_scale));

		vector4d second_largest_scale_axis = rotation_axes[second_largest_scale_component_i];
		double second_largest_scale = scale_ptr[second_largest_scale_component_i];
		if (second_largest_scale < zero_scale_threshold)
		{
			// The second largest scale value is zero which means the two smallest axes are zero
			// We'll use the largest axis and build an orthogonal basis around it
			const vector4d largest_y_dot = vector_dot3_as_vector(largest_scale_axis, identity3x3.y_axis);
			const mask4d is_largest_parallel_to_y = vector_greater_equal(vector_abs(largest_y_dot), vector_set(1.0F - zero_scale_threshold));
			const vector4d orthogonal_axis = vector_select(is_largest_parallel_to_y, identity3x3.z_axis, identity3x3.y_axis);

			second_largest_scale_axis = vector_cross3(largest_scale_axis, orthogonal_axis);

			// Recompute the scale to ensure we properly normalize
			// Here, the second largest axis should alread be normalized
			second_largest_scale = vector_length3(second_largest_scale_axis);
		}

		// Normalize the second largest axis which is non-zero
		second_largest_scale_axis = vector_div(second_largest_scale_axis, vector_set(second_largest_scale));

		vector4d third_largest_scale_axis = rotation_axes[third_largest_scale_component_i];
		double third_largest_scale = scale_ptr[third_largest_scale_component_i];
		if (third_largest_scale < zero_scale_threshold)
		{
			// The third largest scale value is zero
			// We'll use the two larger axes to build an orthogonal basis
			third_largest_scale_axis = vector_cross3(largest_scale_axis, second_largest_scale_axis);

			// Recompute the scale to ensure we properly normalize
			// Here, the third largest axis should alread be normalized
			third_largest_scale = vector_length3(third_largest_scale_axis);
		}

		// Normalize the third largest axis which is non-zero
		third_largest_scale_axis = vector_div(third_largest_scale_axis, vector_set(third_largest_scale));

		// Set our new basis
		rotation_axes[largest_scale_component_i] = largest_scale_axis;
		rotation_axes[second_largest_scale_component_i] = second_largest_scale_axis;
		rotation_axes[third_largest_scale_component_i] = third_largest_scale_axis;

		// Now that we have built the ortho-normal basis part of our rotation, we check its winding
		const double determinant = scalar_cast(matrix_determinant(rotation));
		if (determinant < 0.0)
		{
			// Our winding is reversed meaning one or three of the scale axes contains reflection
			// We negate an axis to flip the winding and negate the scale to match
			largest_scale_axis = vector_neg(largest_scale_axis);
			largest_scale = -largest_scale;

			// Update our scale output and rotation basis
			rotation_axes[largest_scale_component_i] = largest_scale_axis;
			scale_ptr[largest_scale_component_i] = largest_scale;
		}

		// Build a quaternion from the ortho-normal basis we found
		const quatd rotation_q = rtm_impl::quat_from_matrix(rotation.x_axis, rotation.y_axis, rotation.z_axis);

		return qvvd{ rotation_q, translation, scale };
	}

	//////////////////////////////////////////////////////////////////////////
	// Multiplies two QVV transforms.
	// Multiplication order is as follow: local_to_world = qvv_mul(local_to_object, object_to_world)
	// NOTE: When scale is present, multiplication will not properly handle skew/shear,
	// use affine matrices if you have issues.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline qvvd RTM_SIMD_CALL qvv_mul(qvvd_arg0 lhs, qvvd_arg1 rhs) RTM_NO_EXCEPT
	{
		const vector4d min_scale = vector_min(lhs.scale, rhs.scale);
		const vector4d scale = vector_mul(lhs.scale, rhs.scale);

		if (vector_any_less_than3(min_scale, vector_zero()))
		{
			// If we have negative scale, we go through a matrix
			const matrix3x4d lhs_mtx = matrix_from_qvv(lhs);
			const matrix3x4d rhs_mtx = matrix_from_qvv(rhs);
			matrix3x4d result_mtx = matrix_mul(lhs_mtx, rhs_mtx);
			result_mtx = matrix_remove_scale(result_mtx);

			const vector4d sign = vector_sign(scale);
			result_mtx.x_axis = vector_mul(result_mtx.x_axis, vector_dup_x(sign));
			result_mtx.y_axis = vector_mul(result_mtx.y_axis, vector_dup_y(sign));
			result_mtx.z_axis = vector_mul(result_mtx.z_axis, vector_dup_z(sign));

			const quatd rotation = quat_from_matrix(result_mtx);
			const vector4d translation = result_mtx.w_axis;
			return qvv_set(rotation, translation, scale);
		}
		else
		{
			const quatd rotation = quat_mul(lhs.rotation, rhs.rotation);
			const vector4d translation = vector_add(quat_mul_vector3(vector_mul(lhs.translation, rhs.scale), rhs.rotation), rhs.translation);
			return qvv_set(rotation, translation, scale);
		}
	}

	//////////////////////////////////////////////////////////////////////////
	// Multiplies two QVV transforms ignoring 3D scale.
	// The resulting QVV transform will have the LHS scale.
	// Multiplication order is as follow: local_to_world = qvv_mul(local_to_object, object_to_world)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline qvvd RTM_SIMD_CALL qvv_mul_no_scale(qvvd_arg0 lhs, qvvd_arg1 rhs) RTM_NO_EXCEPT
	{
		const quatd rotation = quat_mul(lhs.rotation, rhs.rotation);
		const vector4d translation = vector_add(quat_mul_vector3(lhs.translation, rhs.rotation), rhs.translation);
		return qvv_set(rotation, translation, lhs.scale);
	}

	//////////////////////////////////////////////////////////////////////////
	// Multiplies a QVV transform and a 3D point.
	// Multiplication order is as follow: world_position = qvv_mul_point3(local_position, local_to_world)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline vector4d RTM_SIMD_CALL qvv_mul_point3(vector4d_arg0 point, qvvd_arg1 qvv) RTM_NO_EXCEPT
	{
		return vector_add(quat_mul_vector3(vector_mul(point, qvv.scale), qvv.rotation), qvv.translation);
	}

	//////////////////////////////////////////////////////////////////////////
	// Multiplies a QVV transform and a 3D point ignoring 3D scale.
	// Multiplication order is as follow: world_position = qvv_mul_point3_no_scale(local_position, local_to_world)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline vector4d RTM_SIMD_CALL qvv_mul_point3_no_scale(vector4d_arg0 point, qvvd_arg1 qvv) RTM_NO_EXCEPT
	{
		return vector_add(quat_mul_vector3(point, qvv.rotation), qvv.translation);
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the inverse of the input QVV transform.
	// If zero scale is contained, the result is undefined.
	// For a safe alternative, supply a fallback scale value and a threshold.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline qvvd RTM_SIMD_CALL qvv_inverse(qvvd_arg0 input) RTM_NO_EXCEPT
	{
		const quatd inv_rotation = quat_conjugate(input.rotation);
		const vector4d inv_scale = vector_reciprocal(input.scale);
		const vector4d inv_translation = vector_neg(quat_mul_vector3(vector_mul(input.translation, inv_scale), inv_rotation));
		return qvv_set(inv_rotation, inv_translation, inv_scale);
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the inverse of the input QVV transform.
	// If the input scale has an absolute value below the supplied threshold, the
	// fallback value is used instead.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline qvvd RTM_SIMD_CALL qvv_inverse(qvvd_arg0 input, vector4d_arg3 fallback_scale, double threshold = 1.0E-8) RTM_NO_EXCEPT
	{
		const quatd inv_rotation = quat_conjugate(input.rotation);
		const mask4d is_scale_zero = vector_less_equal(vector_abs(input.scale), vector_set(threshold));
		const vector4d scale = vector_select(is_scale_zero, fallback_scale, input.scale);
		const vector4d inv_scale = vector_reciprocal(scale);
		const vector4d inv_translation = vector_neg(quat_mul_vector3(vector_mul(input.translation, inv_scale), inv_rotation));
		return qvv_set(inv_rotation, inv_translation, inv_scale);
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the inverse of the input QVV transform ignoring 3D scale.
	// The resulting QVV transform will have the input scale.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline qvvd RTM_SIMD_CALL qvv_inverse_no_scale(qvvd_arg0 input) RTM_NO_EXCEPT
	{
		const quatd inv_rotation = quat_conjugate(input.rotation);
		const vector4d inv_translation = vector_neg(quat_mul_vector3(input.translation, inv_rotation));
		return qvv_set(inv_rotation, inv_translation, input.scale);
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns a QVV transforms with the rotation part normalized.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE qvvd RTM_SIMD_CALL qvv_normalize(qvvd_arg0 input) RTM_NO_EXCEPT
	{
		const quatd rotation = quat_normalize(input.rotation);
		return qvv_set(rotation, input.translation, input.scale);
	}

	//////////////////////////////////////////////////////////////////////////
	// Per component linear interpolation of the two inputs at the specified alpha.
	// The formula used is: ((1.0 - alpha) * start) + (alpha * end).
	// Interpolation is stable and will return 'start' when alpha is 0.0 and 'end' when it is 1.0.
	// This is the same instruction count when FMA is present but it might be slightly slower
	// due to the extra multiplication compared to: start + (alpha * (end - start)).
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE qvvd RTM_SIMD_CALL qvv_lerp(qvvd_arg0 start, qvvd_arg1 end, double alpha) RTM_NO_EXCEPT
	{
		const quatd rotation = quat_lerp(start.rotation, end.rotation, alpha);
		const vector4d translation = vector_lerp(start.translation, end.translation, alpha);
		const vector4d scale = vector_lerp(start.scale, end.scale, alpha);
		return qvv_set(rotation, translation, scale);
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Per component linear interpolation of the two inputs at the specified alpha.
	// The formula used is: ((1.0 - alpha) * start) + (alpha * end).
	// Interpolation is stable and will return 'start' when alpha is 0.0 and 'end' when it is 1.0.
	// This is the same instruction count when FMA is present but it might be slightly slower
	// due to the extra multiplication compared to: start + (alpha * (end - start)).
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE qvvd RTM_SIMD_CALL qvv_lerp(qvvd_arg0 start, qvvd_arg1 end, scalard_argn alpha) RTM_NO_EXCEPT
	{
		const quatd rotation = quat_lerp(start.rotation, end.rotation, alpha);
		const vector4d translation = vector_lerp(start.translation, end.translation, alpha);
		const vector4d scale = vector_lerp(start.scale, end.scale, alpha);
		return qvv_set(rotation, translation, scale);
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Per component linear interpolation of the two inputs at the specified alpha.
	// The formula used is: ((1.0 - alpha) * start) + (alpha * end).
	// Interpolation is stable and will return 'start' when alpha is 0.0 and 'end' when it is 1.0.
	// This is the same instruction count when FMA is present but it might be slightly slower
	// due to the extra multiplication compared to: start + (alpha * (end - start)).
	// The resulting QVV transform will have the start scale.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE qvvd RTM_SIMD_CALL qvv_lerp_no_scale(qvvd_arg0 start, qvvd_arg1 end, double alpha) RTM_NO_EXCEPT
	{
		const quatd rotation = quat_lerp(start.rotation, end.rotation, alpha);
		const vector4d translation = vector_lerp(start.translation, end.translation, alpha);
		return qvv_set(rotation, translation, start.scale);
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Per component linear interpolation of the two inputs at the specified alpha.
	// The formula used is: ((1.0 - alpha) * start) + (alpha * end).
	// Interpolation is stable and will return 'start' when alpha is 0.0 and 'end' when it is 1.0.
	// This is the same instruction count when FMA is present but it might be slightly slower
	// due to the extra multiplication compared to: start + (alpha * (end - start)).
	// The resulting QVV transform will have the start scale.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE qvvd RTM_SIMD_CALL qvv_lerp_no_scale(qvvd_arg0 start, qvvd_arg1 end, scalard_argn alpha) RTM_NO_EXCEPT
	{
		const quatd rotation = quat_lerp(start.rotation, end.rotation, alpha);
		const vector4d translation = vector_lerp(start.translation, end.translation, alpha);
		return qvv_set(rotation, translation, start.scale);
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Per component spherical interpolation of the two inputs at the specified alpha.
	// See quat_slerp(..)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE qvvd RTM_SIMD_CALL qvv_slerp(qvvd_arg0 start, qvvd_arg1 end, double alpha) RTM_NO_EXCEPT
	{
		const quatd rotation = quat_slerp(start.rotation, end.rotation, alpha);
		const vector4d translation = vector_lerp(start.translation, end.translation, alpha);
		const vector4d scale = vector_lerp(start.scale, end.scale, alpha);
		return qvv_set(rotation, translation, scale);
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Per component spherical interpolation of the two inputs at the specified alpha.
	// See quat_slerp(..)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE qvvd RTM_SIMD_CALL qvv_slerp(qvvd_arg0 start, qvvd_arg1 end, scalard_argn alpha) RTM_NO_EXCEPT
	{
		const quatd rotation = quat_slerp(start.rotation, end.rotation, alpha);
		const vector4d translation = vector_lerp(start.translation, end.translation, alpha);
		const vector4d scale = vector_lerp(start.scale, end.scale, alpha);
		return qvv_set(rotation, translation, scale);
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Per component spherical interpolation of the two inputs at the specified alpha.
	// See quat_slerp(..)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE qvvd RTM_SIMD_CALL qvv_slerp_no_scale(qvvd_arg0 start, qvvd_arg1 end, double alpha) RTM_NO_EXCEPT
	{
		const quatd rotation = quat_slerp(start.rotation, end.rotation, alpha);
		const vector4d translation = vector_lerp(start.translation, end.translation, alpha);
		return qvv_set(rotation, translation, start.scale);
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Per component spherical interpolation of the two inputs at the specified alpha.
	// See quat_slerp(..)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE qvvd RTM_SIMD_CALL qvv_slerp_no_scale(qvvd_arg0 start, qvvd_arg1 end, scalard_argn alpha) RTM_NO_EXCEPT
	{
		const quatd rotation = quat_slerp(start.rotation, end.rotation, alpha);
		const vector4d translation = vector_lerp(start.translation, end.translation, alpha);
		return qvv_set(rotation, translation, start.scale);
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Returns true if the input QVV does not contain any NaN or Inf, otherwise false.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL qvv_is_finite(qvvd_arg0 input) RTM_NO_EXCEPT
	{
		return quat_is_finite(input.rotation) && vector_is_finite3(input.translation) && vector_is_finite3(input.scale);
	}

	RTM_IMPL_VERSION_NAMESPACE_END
}

RTM_IMPL_FILE_PRAGMA_POP

```

`includes/rtm/qvvf.h`:

```h
#pragma once

////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2017 Nicholas Frechette & Animation Compression Library contributors
// Copyright (c) 2018 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include "rtm/math.h"
#include "rtm/quatf.h"
#include "rtm/vector4f.h"
#include "rtm/matrix3x4f.h"
#include "rtm/version.h"
#include "rtm/impl/compiler_utils.h"
#include "rtm/impl/qvv_common.h"
#include "rtm/impl/matrix_affine_common.h"

RTM_IMPL_FILE_PRAGMA_PUSH

////////////////////////////////////////////////////////////////////////////////
// WARNING: QVV transforms do _NOT_ form a group. It has no well defined multiplication and inverse.
// This is because when non-uniform scale is rotated (e.g. as the result of two QVV multiplying),
// shear is introduced and there is no place to store it. As such, QVV discards the introduced shear
// which is mathematically incorrect. As a result of this, QVV multiplication is _NOT_ assossiative and
// the inverse is incorrect. USE AT YOUR OWN RISK.
////////////////////////////////////////////////////////////////////////////////

namespace rtm
{
	RTM_IMPL_VERSION_NAMESPACE_BEGIN

	//////////////////////////////////////////////////////////////////////////
	// Casts a QVV transform float64 variant to a float32 variant.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE qvvf RTM_SIMD_CALL qvv_cast(const qvvd& input) RTM_NO_EXCEPT
	{
		return qvvf{ quat_cast(input.rotation), vector_cast(input.translation), vector_cast(input.scale) };
	}

	//////////////////////////////////////////////////////////////////////////
	// Converts a pure rotation 3x3 matrix into a QVV transform.
	// The translation/scale will be the identity.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline qvvf RTM_SIMD_CALL qvv_from_matrix(matrix3x3f_arg0 input) RTM_NO_EXCEPT
	{
		const quatf rotation = rtm_impl::quat_from_matrix(input.x_axis, input.y_axis, input.z_axis);
		const vector4f translation = vector_zero();
		const vector4f scale = vector_set(1.0F);
		return qvvf{ rotation, translation, scale };
	}

	//////////////////////////////////////////////////////////////////////////
	// Converts a 3x4 affine matrix into a QVV transform.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline qvvf RTM_SIMD_CALL qvv_from_matrix(matrix3x4f_arg0 input) RTM_NO_EXCEPT
	{
		// Let us define the following:
		// A = S * R * T
		// A is an affine matrix that scales, rotates, then translates (an SRT transform)
		// The translation part lives in the W row and is easily recovered as the scale/rotation
		// components are stored in the XYZ rows. For the remainder of the discussion, we'll omit
		// translation and only consider scale/rotation.
		//
		// For scale and rotation, recovering the original values S and R is more complicated
		// if we allow scale to be zero or negative.
		//
		// Let us first consider the simple case where scale is always non-zero and positive.
		// When this is the case, we can recover the matrix S by computing the length of each
		// of the XYZ rows (R is ortho-normalized and its rows thus have unit length). R can
		// then be recovered by normalizing the XYZ rows of A.
		//
		// If scale can be zero, we can recover S in the same way but R requires a bit more care.
		// With a zero row, we cannot normalize it (due to division by zero). To retain orthogonality,
		// we must use the cross product to recover our missing values where possible. To that end,
		// the scale axes are sorted from largest to smallest and we process them starting with the largest.
		//     - If the largest scale axis is below our precision threshold, we cannot normalize it. It also means
		//       that the remaining two axes are also below our precision threshold. As such, we will return a zero
		//       scale matrix and the identity rotation matrix. The rotation matrix doesn't matter when we have zero scale.
		//     - If the second largest scale axis is below our precision threshold, we cannot normalize it. It also
		//       means that the third axis is also below our precision threshold. We thus need to find two axes orthogonal
		//       to the first. To that end, we test the dot product of the first axis with the identity Y axis. If
		//       the two are parallel, we will use the identity Z axis for the first cross product, otherwise we use the Y axis.
		//       This yields our second axis that has zero scale (the axis doesn't matter since we have zero scale).
		//       We find our third axis by taking the cross product of the first two, which also has zero scale.
		//     - If the third largest scale axis is below our precision threshold, we cannot normalize it. Since we already
		//       have two valid axes, we can find our third using the cross product and it will have zero scale.
		// As such, if one or more axes are zero, we can recover the original S matrix but not the original R matrix.
		// However, when the new rotation matrix R' is multiplied with the S matrix, it yields the same affine
		// matrix A because anything multiplied with zero yields zero:
		// A = S * R'
		//
		// Negative scale throws a wrench in things. Negative scaling along one axis corresponds to
		// performing a reflection along that axis. When a single axis is reflected, the matrix A
		// will no longer have the same winding (it flips with every reflected axis). As such, if we
		// extract S by using the row lengths, we cannot recover R as the resulting rotation matrix
		// will be ill formed due to its winding.
		// As such, when negative scale is present, we will not be able to recover the original S and R
		// matrices. Let us look at every possible scenario:
		//     - If two axes are reflected, they are combined into a new rotation matrix. This happens because
		//       two successive reflections (F1 and F2) can be represented by a single rotation (RF) like so:
		//       RF = F1 * F2
		//       And two rotations (R1 and R2) can be combined into a single rotation R' with: R' = R1 * R2
		//       Let us express everything now:
		//       S = S' * F1 * F2 (some scaling with 2 reflections)
		//       A = S' * F1 * F2 * R
		//       A = S' * R'
		//       And so while we cannot recover the original S and R matrices, we can find two matrices
		//       that yield the same resulting affine transform.
		//     - If three axes are reflected, the winding is reversed and we have the following:
		//       S = S' * F1 * F2 * F3
		//       A = S' * F1 * F2 * F3 * R
		//       A = S' * F1 * R'
		//       And so we see that the case of three reflected axes is equivalent to the single reflection case.
		//     - If a single axis is reflected, the winding is reversed and we have the following:
		//       S = S' * F1
		//       A = S' * F1 * R
		//       When this occurs, we cannot tell which axis was originally reflected. But thankfully, it does
		//       not matter. To find a solution, we negate one axis (any) in both the scale matrix
		//       and the rotation matrix. When the two will be multiplied, the two negations will cancel out.
		//       By negating an axis on the rotation matrix, we will reverse the winding back into the one we
		//       want to find the rotation we are looking for. We negate the equivalent scale axis only to be
		//       able to recover the original matrix. And so we have:
		//       A = S' * R'
		//       The axis we choose to negate doesn't matter as long as it doesn't have zero scale (zero cannot be negated).
		//       To that end, we negate the largest scale axis.
		//
		// Taking everything together, we first handle positive and zero scale together by finding our
		// orthogonal basis and positive (or zero) scale values. Then, if the determinant is negative, the winding
		// is reversed and we must apply our reflection fixup as described above by negating the largest scale axis
		// in both the orthogonal basis (to restore the correct winding) and on our scale to cancel it out.
		// If all three axes have zero scale, we must early out and skip this last step as we know there is no reflection.
		// If one or two axes have zero scale, they will be reconstructed using the cross product which means that the
		// winding will never be reversed and no reflection fixup will be performed.

		// Translation is always in the W axis
		const vector4f translation = input.w_axis;

		// First, we compute our absolute scale values
		const scalarf x_scale = vector_length3_as_scalar(input.x_axis);
		const scalarf y_scale = vector_length3_as_scalar(input.y_axis);
		const scalarf z_scale = vector_length3_as_scalar(input.z_axis);
		vector4f scale = vector_set(x_scale, y_scale, z_scale);

		// Grab a pointer to the first scale value, we'll index into it
		float* scale_ptr = rtm_impl::bit_cast<float*>(&scale);

		// Next, we find the largest scale components and sort them from largest to smallest
		component3 largest_scale_component;
		component3 second_largest_scale_component;
		component3 third_largest_scale_component;

		if (scalar_greater_equal(x_scale, y_scale))
		{
			// X >= Y
			if (scalar_greater_equal(y_scale, z_scale))
			{
				// X >= Y >= Z
				largest_scale_component = component3::x;
				second_largest_scale_component = component3::y;
				third_largest_scale_component = component3::z;
			}
			else
			{
				if (scalar_greater_equal(x_scale, z_scale))
				{
					// X >= Z >= Y
					largest_scale_component = component3::x;
					second_largest_scale_component = component3::z;
				}
				else
				{
					// Z >= X >= Y
					largest_scale_component = component3::z;
					second_largest_scale_component = component3::x;
				}

				third_largest_scale_component = component3::y;
			}
		}
		else
		{
			if (scalar_greater_equal(x_scale, z_scale))
			{
				// Y > X >= Z
				largest_scale_component = component3::y;
				second_largest_scale_component = component3::x;
				third_largest_scale_component = component3::z;
			}
			else
			{
				if (scalar_greater_equal(y_scale, z_scale))
				{
					// Y >= Z > X
					largest_scale_component = component3::y;
					second_largest_scale_component = component3::z;
				}
				else
				{
					// Z > Y > X
					largest_scale_component = component3::z;
					second_largest_scale_component = component3::y;
				}

				third_largest_scale_component = component3::x;
			}
		}

		// Cast to integer for array indexing
		const int32_t largest_scale_component_i = static_cast<int32_t>(largest_scale_component);
		const int32_t second_largest_scale_component_i = static_cast<int32_t>(second_largest_scale_component);
		const int32_t third_largest_scale_component_i = static_cast<int32_t>(third_largest_scale_component);

		const matrix3x3f identity3x3 = matrix_identity();

		// Copy the rotation part from our input
		matrix3x3f rotation = matrix_cast(input);

		// Grab a pointer to the first axis, we'll index into it
		vector4f* rotation_axes = &rotation.x_axis;

		// We use a threshold to test for zero because division with near zero values is imprecise
		// and might not yield a normalized result
		const float zero_scale_threshold = 1.0E-8F;
		float largest_scale = scale_ptr[largest_scale_component_i];
		if (largest_scale < zero_scale_threshold)
		{
			// The largest scale value is zero which means all three are zero
			// We'll return the identity rotation since its value is not recoverable
			const quatf rotation_q = quat_identity();
			return qvvf{ rotation_q, translation, scale };
		}

		// Normalize the largest scale axis which is non-zero
		vector4f largest_scale_axis = rotation_axes[largest_scale_component_i];
		largest_scale_axis = vector_div(largest_scale_axis, vector_set(largest_scale));

		vector4f second_largest_scale_axis = rotation_axes[second_largest_scale_component_i];
		float second_largest_scale = scale_ptr[second_largest_scale_component_i];
		if (second_largest_scale < zero_scale_threshold)
		{
			// The second largest scale value is zero which means the two smallest axes are zero
			// We'll use the largest axis and build an orthogonal basis around it
			const vector4f largest_y_dot = vector_dot3_as_vector(largest_scale_axis, identity3x3.y_axis);
			const mask4f is_largest_parallel_to_y = vector_greater_equal(vector_abs(largest_y_dot), vector_set(1.0F - zero_scale_threshold));
			const vector4f orthogonal_axis = vector_select(is_largest_parallel_to_y, identity3x3.z_axis, identity3x3.y_axis);

			second_largest_scale_axis = vector_cross3(largest_scale_axis, orthogonal_axis);

			// Recompute the scale to ensure we properly normalize
			// Here, the second largest axis should alread be normalized
			second_largest_scale = vector_length3(second_largest_scale_axis);
		}

		// Normalize the second largest axis which is non-zero
		second_largest_scale_axis = vector_div(second_largest_scale_axis, vector_set(second_largest_scale));

		vector4f third_largest_scale_axis = rotation_axes[third_largest_scale_component_i];
		float third_largest_scale = scale_ptr[third_largest_scale_component_i];
		if (third_largest_scale < zero_scale_threshold)
		{
			// The third largest scale value is zero
			// We'll use the two larger axes to build an orthogonal basis
			third_largest_scale_axis = vector_cross3(largest_scale_axis, second_largest_scale_axis);

			// Recompute the scale to ensure we properly normalize
			// Here, the third largest axis should alread be normalized
			third_largest_scale = vector_length3(third_largest_scale_axis);
		}

		// Normalize the third largest axis which is non-zero
		third_largest_scale_axis = vector_div(third_largest_scale_axis, vector_set(third_largest_scale));

		// Set our new basis
		rotation_axes[largest_scale_component_i] = largest_scale_axis;
		rotation_axes[second_largest_scale_component_i] = second_largest_scale_axis;
		rotation_axes[third_largest_scale_component_i] = third_largest_scale_axis;

		// Now that we have built the ortho-normal basis part of our rotation, we check its winding
		const float determinant = scalar_cast(matrix_determinant(rotation));
		if (determinant < 0.0F)
		{
			// Our winding is reversed meaning one or three of the scale axes contains reflection
			// We negate an axis to flip the winding and negate the scale to match
			largest_scale_axis = vector_neg(largest_scale_axis);
			largest_scale = -largest_scale;

			// Update our scale output and rotation basis
			rotation_axes[largest_scale_component_i] = largest_scale_axis;
			scale_ptr[largest_scale_component_i] = largest_scale;
		}

		// Build a quaternion from the ortho-normal basis we found
		const quatf rotation_q = rtm_impl::quat_from_matrix(rotation.x_axis, rotation.y_axis, rotation.z_axis);

		return qvvf{ rotation_q, translation, scale };
	}

	//////////////////////////////////////////////////////////////////////////
	// Multiplies two QVV transforms.
	// Multiplication order is as follow: local_to_world = qvv_mul(local_to_object, object_to_world)
	// NOTE: When scale is present, multiplication will not properly handle skew/shear,
	// use affine matrices if you have issues.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline qvvf RTM_SIMD_CALL qvv_mul(qvvf_arg0 lhs, qvvf_arg1 rhs) RTM_NO_EXCEPT
	{
		const vector4f min_scale = vector_min(lhs.scale, rhs.scale);
		const vector4f scale = vector_mul(lhs.scale, rhs.scale);

		if (vector_any_less_than3(min_scale, vector_zero()))
		{
			// If we have negative scale, we go through a matrix
			const matrix3x4f lhs_mtx = matrix_from_qvv(lhs);
			const matrix3x4f rhs_mtx = matrix_from_qvv(rhs);
			matrix3x4f result_mtx = matrix_mul(lhs_mtx, rhs_mtx);
			result_mtx = matrix_remove_scale(result_mtx);

#if defined(RTM_SSE2_INTRINSICS)
			constexpr __m128 signs = RTM_VECTOR4F_MAKE(-0.0F, -0.0F, -0.0F, -0.0F);
			const __m128 sign_bits = _mm_and_ps(scale, signs);	// Mask out the sign bit

			result_mtx.x_axis = _mm_xor_ps(result_mtx.x_axis, _mm_shuffle_ps(sign_bits, sign_bits, _MM_SHUFFLE(0, 0, 0, 0)));
			result_mtx.y_axis = _mm_xor_ps(result_mtx.y_axis, _mm_shuffle_ps(sign_bits, sign_bits, _MM_SHUFFLE(1, 1, 1, 1)));
			result_mtx.z_axis = _mm_xor_ps(result_mtx.z_axis, _mm_shuffle_ps(sign_bits, sign_bits, _MM_SHUFFLE(2, 2, 2, 2)));
#else
			const vector4f sign = vector_sign(scale);
			result_mtx.x_axis = vector_mul(result_mtx.x_axis, vector_dup_x(sign));
			result_mtx.y_axis = vector_mul(result_mtx.y_axis, vector_dup_y(sign));
			result_mtx.z_axis = vector_mul(result_mtx.z_axis, vector_dup_z(sign));
#endif

			const quatf rotation = quat_from_matrix(result_mtx);
			const vector4f translation = result_mtx.w_axis;
			return qvv_set(rotation, translation, scale);
		}
		else
		{
			const quatf rotation = quat_mul(lhs.rotation, rhs.rotation);
			const vector4f translation = vector_add(quat_mul_vector3(vector_mul(lhs.translation, rhs.scale), rhs.rotation), rhs.translation);
			return qvv_set(rotation, translation, scale);
		}
	}

	//////////////////////////////////////////////////////////////////////////
	// Multiplies two QVV transforms ignoring 3D scale.
	// The resulting QVV transform will have the LHS scale.
	// Multiplication order is as follow: local_to_world = qvv_mul(local_to_object, object_to_world)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline qvvf RTM_SIMD_CALL qvv_mul_no_scale(qvvf_arg0 lhs, qvvf_arg1 rhs) RTM_NO_EXCEPT
	{
		const quatf rotation = quat_mul(lhs.rotation, rhs.rotation);
		const vector4f translation = vector_add(quat_mul_vector3(lhs.translation, rhs.rotation), rhs.translation);
		return qvv_set(rotation, translation, lhs.scale);
	}

	//////////////////////////////////////////////////////////////////////////
	// Multiplies a QVV transform and a 3D point.
	// Multiplication order is as follow: world_position = qvv_mul_point3(local_position, local_to_world)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline vector4f RTM_SIMD_CALL qvv_mul_point3(vector4f_arg0 point, qvvf_arg1 qvv) RTM_NO_EXCEPT
	{
		return vector_add(quat_mul_vector3(vector_mul(qvv.scale, point), qvv.rotation), qvv.translation);
	}

	//////////////////////////////////////////////////////////////////////////
	// Multiplies a QVV transform and a 3D point ignoring 3D scale.
	// Multiplication order is as follow: world_position = qvv_mul_point3_no_scale(local_position, local_to_world)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline vector4f RTM_SIMD_CALL qvv_mul_point3_no_scale(vector4f_arg0 point, qvvf_arg1 qvv) RTM_NO_EXCEPT
	{
		return vector_add(quat_mul_vector3(point, qvv.rotation), qvv.translation);
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the inverse of the input QVV transform.
	// If zero scale is contained, the result is undefined.
	// For a safe alternative, supply a fallback scale value and a threshold.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline qvvf RTM_SIMD_CALL qvv_inverse(qvvf_arg0 input) RTM_NO_EXCEPT
	{
		const quatf inv_rotation = quat_conjugate(input.rotation);
		const vector4f inv_scale = vector_reciprocal(input.scale);
		const vector4f inv_translation = vector_neg(quat_mul_vector3(vector_mul(input.translation, inv_scale), inv_rotation));
		return qvv_set(inv_rotation, inv_translation, inv_scale);
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the inverse of the input QVV transform.
	// If the input scale has an absolute value below the supplied threshold, the
	// fallback value is used instead.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline qvvf RTM_SIMD_CALL qvv_inverse(qvvf_arg0 input, vector4f_arg3 fallback_scale, float threshold = 1.0E-8F) RTM_NO_EXCEPT
	{
		const quatf inv_rotation = quat_conjugate(input.rotation);
		const mask4f is_scale_zero = vector_less_equal(vector_abs(input.scale), vector_set(threshold));
		const vector4f scale = vector_select(is_scale_zero, fallback_scale, input.scale);
		const vector4f inv_scale = vector_reciprocal(scale);
		const vector4f inv_translation = vector_neg(quat_mul_vector3(vector_mul(input.translation, inv_scale), inv_rotation));
		return qvv_set(inv_rotation, inv_translation, inv_scale);
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the inverse of the input QVV transform ignoring 3D scale.
	// The resulting QVV transform will have the input scale.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline qvvf RTM_SIMD_CALL qvv_inverse_no_scale(qvvf_arg0 input) RTM_NO_EXCEPT
	{
		const quatf inv_rotation = quat_conjugate(input.rotation);
		const vector4f inv_translation = vector_neg(quat_mul_vector3(input.translation, inv_rotation));
		return qvv_set(inv_rotation, inv_translation, input.scale);
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns a QVV transforms with the rotation part normalized.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE qvvf RTM_SIMD_CALL qvv_normalize(qvvf_arg0 input) RTM_NO_EXCEPT
	{
		const quatf rotation = quat_normalize(input.rotation);
		return qvv_set(rotation, input.translation, input.scale);
	}

	//////////////////////////////////////////////////////////////////////////
	// Per component linear interpolation of the two inputs at the specified alpha.
	// The formula used is: ((1.0 - alpha) * start) + (alpha * end).
	// Interpolation is stable and will return 'start' when alpha is 0.0 and 'end' when it is 1.0.
	// This is the same instruction count when FMA is present but it might be slightly slower
	// due to the extra multiplication compared to: start + (alpha * (end - start)).
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE qvvf RTM_SIMD_CALL qvv_lerp(qvvf_arg0 start, qvvf_arg1 end, float alpha) RTM_NO_EXCEPT
	{
		const quatf rotation = quat_lerp(start.rotation, end.rotation, alpha);
		const vector4f translation = vector_lerp(start.translation, end.translation, alpha);
		const vector4f scale = vector_lerp(start.scale, end.scale, alpha);
		return qvv_set(rotation, translation, scale);
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Per component linear interpolation of the two inputs at the specified alpha.
	// The formula used is: ((1.0 - alpha) * start) + (alpha * end).
	// Interpolation is stable and will return 'start' when alpha is 0.0 and 'end' when it is 1.0.
	// This is the same instruction count when FMA is present but it might be slightly slower
	// due to the extra multiplication compared to: start + (alpha * (end - start)).
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE qvvf RTM_SIMD_CALL qvv_lerp(qvvf_arg0 start, qvvf_arg1 end, scalarf_arg2 alpha) RTM_NO_EXCEPT
	{
		const quatf rotation = quat_lerp(start.rotation, end.rotation, alpha);
		const vector4f translation = vector_lerp(start.translation, end.translation, alpha);
		const vector4f scale = vector_lerp(start.scale, end.scale, alpha);
		return qvv_set(rotation, translation, scale);
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Per component linear interpolation of the two inputs at the specified alpha.
	// The formula used is: ((1.0 - alpha) * start) + (alpha * end).
	// Interpolation is stable and will return 'start' when alpha is 0.0 and 'end' when it is 1.0.
	// This is the same instruction count when FMA is present but it might be slightly slower
	// due to the extra multiplication compared to: start + (alpha * (end - start)).
	// The resulting QVV transform will have the start scale.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE qvvf RTM_SIMD_CALL qvv_lerp_no_scale(qvvf_arg0 start, qvvf_arg1 end, float alpha) RTM_NO_EXCEPT
	{
		const quatf rotation = quat_lerp(start.rotation, end.rotation, alpha);
		const vector4f translation = vector_lerp(start.translation, end.translation, alpha);
		return qvv_set(rotation, translation, start.scale);
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Per component linear interpolation of the two inputs at the specified alpha.
	// The formula used is: ((1.0 - alpha) * start) + (alpha * end).
	// Interpolation is stable and will return 'start' when alpha is 0.0 and 'end' when it is 1.0.
	// This is the same instruction count when FMA is present but it might be slightly slower
	// due to the extra multiplication compared to: start + (alpha * (end - start)).
	// The resulting QVV transform will have the start scale.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE qvvf RTM_SIMD_CALL qvv_lerp_no_scale(qvvf_arg0 start, qvvf_arg1 end, scalarf_arg2 alpha) RTM_NO_EXCEPT
	{
		const quatf rotation = quat_lerp(start.rotation, end.rotation, alpha);
		const vector4f translation = vector_lerp(start.translation, end.translation, alpha);
		return qvv_set(rotation, translation, start.scale);
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Per component spherical interpolation of the two inputs at the specified alpha.
	// See quat_slerp(..)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE qvvf RTM_SIMD_CALL qvv_slerp(qvvf_arg0 start, qvvf_arg1 end, float alpha) RTM_NO_EXCEPT
	{
		const quatf rotation = quat_slerp(start.rotation, end.rotation, alpha);
		const vector4f translation = vector_lerp(start.translation, end.translation, alpha);
		const vector4f scale = vector_lerp(start.scale, end.scale, alpha);
		return qvv_set(rotation, translation, scale);
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Per component spherical interpolation of the two inputs at the specified alpha.
	// See quat_slerp(..)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE qvvf RTM_SIMD_CALL qvv_slerp(qvvf_arg0 start, qvvf_arg1 end, scalarf_arg2 alpha) RTM_NO_EXCEPT
	{
		const quatf rotation = quat_slerp(start.rotation, end.rotation, alpha);
		const vector4f translation = vector_lerp(start.translation, end.translation, alpha);
		const vector4f scale = vector_lerp(start.scale, end.scale, alpha);
		return qvv_set(rotation, translation, scale);
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Per component spherical interpolation of the two inputs at the specified alpha.
	// See quat_slerp(..)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE qvvf RTM_SIMD_CALL qvv_slerp_no_scale(qvvf_arg0 start, qvvf_arg1 end, float alpha) RTM_NO_EXCEPT
	{
		const quatf rotation = quat_slerp(start.rotation, end.rotation, alpha);
		const vector4f translation = vector_lerp(start.translation, end.translation, alpha);
		return qvv_set(rotation, translation, start.scale);
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Per component spherical interpolation of the two inputs at the specified alpha.
	// See quat_slerp(..)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE qvvf RTM_SIMD_CALL qvv_slerp_no_scale(qvvf_arg0 start, qvvf_arg1 end, scalarf_arg2 alpha) RTM_NO_EXCEPT
	{
		const quatf rotation = quat_slerp(start.rotation, end.rotation, alpha);
		const vector4f translation = vector_lerp(start.translation, end.translation, alpha);
		return qvv_set(rotation, translation, start.scale);
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Returns true if the input QVV does not contain any NaN or Inf, otherwise false.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL qvv_is_finite(qvvf_arg0 input) RTM_NO_EXCEPT
	{
		return quat_is_finite(input.rotation) && vector_is_finite3(input.translation) && vector_is_finite3(input.scale);
	}

	RTM_IMPL_VERSION_NAMESPACE_END
}

RTM_IMPL_FILE_PRAGMA_POP

```

`includes/rtm/scalard.h`:

```h
#pragma once

////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2017 Nicholas Frechette & Animation Compression Library contributors
// Copyright (c) 2018 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include "rtm/constants.h"
#include "rtm/math.h"
#include "rtm/version.h"
#include "rtm/impl/cmath.impl.h"
#include "rtm/impl/compiler_utils.h"
#include "rtm/impl/scalar_common.h"

#if defined(RTM_SSE2_INTRINSICS)
#include "rtm/macros.h"
#endif

#include <algorithm>
#include <limits>

RTM_IMPL_FILE_PRAGMA_PUSH

namespace rtm
{
	RTM_IMPL_VERSION_NAMESPACE_BEGIN

	//////////////////////////////////////////////////////////////////////////
	// Creates a scalar from a floating point value.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE scalard RTM_SIMD_CALL scalar_set(double xyzw) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return scalard{ _mm_set1_pd(xyzw) };
#else
		return xyzw;
#endif
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Writes a scalar to memory.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE void RTM_SIMD_CALL scalar_store(scalard_arg0 input, double* output) RTM_NO_EXCEPT
	{
		_mm_store_sd(output, input.value);
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Writes a scalar to memory.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE void RTM_SIMD_CALL scalar_store(double input, double* output) RTM_NO_EXCEPT
	{
		*output = input;
	}

	//////////////////////////////////////////////////////////////////////////
	// Casts a scalar into a floating point value.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE double RTM_SIMD_CALL scalar_cast(scalard_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return _mm_cvtsd_f64(input.value);
#else
		return input;
#endif
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Returns the largest integer value not greater than the input (round towards minus infinity).
	// scalar_floor(1.8) = 1.0
	// scalar_floor(-1.8) = -2.0
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE scalard RTM_SIMD_CALL scalar_floor(scalard_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE4_INTRINSICS)
		return scalard{ _mm_round_sd(input.value, input.value, 0x9) };
#else
		// NaN, +- Infinity, and numbers larger or equal to 2^23 remain unchanged
		// since they have no fractional part.

		const __m128i abs_mask = _mm_set_epi64x(0x7FFFFFFFFFFFFFFFULL, 0x7FFFFFFFFFFFFFFFULL);
		const __m128d fractional_limit = _mm_set1_pd(4503599627370496.0); // 2^52

		// Build our mask, larger values that have no fractional part, and infinities will be true
		// Smaller values and NaN will be false
		__m128d abs_input = _mm_and_pd(input.value, _mm_castsi128_pd(abs_mask));
		__m128d is_input_large = _mm_cmpge_sd(abs_input, fractional_limit);

		// Test if our input is NaN with (value != value), it is only true for NaN
		__m128d is_nan = _mm_cmpneq_sd(input.value, input.value);

		// Combine our masks to determine if we should return the original value
		__m128d use_original_input = _mm_or_pd(is_input_large, is_nan);

		// Convert to an integer and back. This does banker's rounding by default
		__m128d integer_part = _mm_cvtepi32_pd(_mm_cvtpd_epi32(input.value));

		// Test if the returned value is greater than the original.
		// A negative input will round towards zero and be greater when we need it to be smaller.
		__m128d is_negative = _mm_cmpgt_sd(integer_part, input.value);

		// Convert our mask to a float, ~0 yields -1.0 since it is a valid signed integer
		// Positive values will yield a 0.0 bias
		__m128d bias = _mm_cvtepi32_pd(_mm_castpd_si128(is_negative));

		// Add our bias to properly handle negative values
		integer_part = _mm_add_sd(integer_part, bias);

		__m128d result = _mm_or_pd(_mm_and_pd(use_original_input, input.value), _mm_andnot_pd(use_original_input, integer_part));
		return scalard{ result };
#endif
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Returns the largest integer value not greater than the input (round towards negative infinity).
	// scalar_floor(1.8) = 1.0
	// scalar_floor(-1.8) = -2.0
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE double RTM_SIMD_CALL scalar_floor(double input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return scalar_cast(scalar_floor(scalar_set(input)));
#else
		return std::floor(input);
#endif
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Returns the smallest integer value not less than the input (round towards positive infinity).
	// scalar_ceil(1.8) = 2.0
	// scalar_ceil(-1.8) = -1.0
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE scalard RTM_SIMD_CALL scalar_ceil(scalard_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE4_INTRINSICS)
		return scalard{ _mm_round_sd(input.value, input.value, 0xA) };
#else
		// NaN, +- Infinity, and numbers larger or equal to 2^23 remain unchanged
		// since they have no fractional part.

		const __m128i abs_mask = _mm_set_epi64x(0x7FFFFFFFFFFFFFFFULL, 0x7FFFFFFFFFFFFFFFULL);
		const __m128d fractional_limit = _mm_set1_pd(4503599627370496.0); // 2^52

		// Build our mask, larger values that have no fractional part, and infinities will be true
		// Smaller values and NaN will be false
		__m128d abs_input = _mm_and_pd(input.value, _mm_castsi128_pd(abs_mask));
		__m128d is_input_large = _mm_cmpge_sd(abs_input, fractional_limit);

		// Test if our input is NaN with (value != value), it is only true for NaN
		__m128d is_nan = _mm_cmpneq_sd(input.value, input.value);

		// Combine our masks to determine if we should return the original value
		__m128d use_original_input = _mm_or_pd(is_input_large, is_nan);

		// Convert to an integer and back. This does banker's rounding by default
		__m128d integer_part = _mm_cvtepi32_pd(_mm_cvtpd_epi32(input.value));

		// Test if the returned value is smaller than the original.
		// A positive input will round towards zero and be lower when we need it to be greater.
		__m128d is_positive = _mm_cmplt_sd(integer_part, input.value);

		// Convert our mask to a float, ~0 yields -1.0 since it is a valid signed integer
		// Negative values will yield a 0.0 bias
		__m128d bias = _mm_cvtepi32_pd(_mm_castpd_si128(is_positive));

		// Subtract our bias to properly handle positive values
		integer_part = _mm_sub_sd(integer_part, bias);

		__m128d result = _mm_or_pd(_mm_and_pd(use_original_input, input.value), _mm_andnot_pd(use_original_input, integer_part));
		return scalard{ result };
#endif
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Returns the smallest integer value not less than the input (round towards positive infinity).
	// scalar_ceil(1.8) = 2.0
	// scalar_ceil(-1.8) = -1.0
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE double RTM_SIMD_CALL scalar_ceil(double input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return scalar_cast(scalar_ceil(scalar_set(input)));
#else
		return std::ceil(input);
#endif
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Returns the input if it is within the min/max values otherwise the
	// exceeded boundary is returned.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE scalard RTM_SIMD_CALL scalar_clamp(scalard_arg0 input, scalard_arg1 min, scalard_arg2 max) RTM_NO_EXCEPT
	{
		return scalard{ _mm_min_sd(_mm_max_sd(input.value, min.value), max.value) };
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Returns the input if it is within the min/max values otherwise the
	// exceeded boundary is returned.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE double RTM_SIMD_CALL scalar_clamp(double input, double min, double max) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return _mm_cvtsd_f64(_mm_min_sd(_mm_max_sd(_mm_set1_pd(input), _mm_set1_pd(min)), _mm_set1_pd(max)));
#else
		return (std::min)((std::max)(input, min), max);
#endif
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Returns the absolute value of the input.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE scalard RTM_SIMD_CALL scalar_abs(scalard_arg0 input) RTM_NO_EXCEPT
	{
		const __m128i abs_mask = _mm_set_epi64x(0x7FFFFFFFFFFFFFFFULL, 0x7FFFFFFFFFFFFFFFULL);
		return scalard{ _mm_and_pd(input.value, _mm_castsi128_pd(abs_mask)) };
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Returns the absolute value of the input.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE double RTM_SIMD_CALL scalar_abs(double input) RTM_NO_EXCEPT
	{
		return std::fabs(input);
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Returns the square root of the input.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE scalard RTM_SIMD_CALL scalar_sqrt(scalard_arg0 input) RTM_NO_EXCEPT
	{
		return scalard{ _mm_sqrt_sd(input.value, input.value) };
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Returns the square root of the input.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE double RTM_SIMD_CALL scalar_sqrt(double input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		const __m128d input_v = _mm_set1_pd(input);
		return _mm_cvtsd_f64(_mm_sqrt_sd(input_v, input_v));
#else
		return std::sqrt(input);
#endif
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Returns the reciprocal square root of the input.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE scalard RTM_SIMD_CALL scalar_sqrt_reciprocal(scalard_arg0 input) RTM_NO_EXCEPT
	{
		const __m128d input_sqrt = _mm_sqrt_sd(input.value, input.value);
		const __m128d result = _mm_div_sd(_mm_set_sd(1.0), input_sqrt);
		return scalard{ result };
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Returns the reciprocal square root of the input.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE double RTM_SIMD_CALL scalar_sqrt_reciprocal(double input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		const __m128d input_v = _mm_set1_pd(input);
		const __m128d input_sqrt = _mm_sqrt_sd(input_v, input_v);
		return _mm_cvtsd_f64(_mm_div_sd(_mm_set_sd(1.0), input_sqrt));
#else
		return 1.0 / scalar_sqrt(input);
#endif
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Returns the reciprocal of the input.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE scalard RTM_SIMD_CALL scalar_reciprocal(scalard_arg0 input) RTM_NO_EXCEPT
	{
		return scalard{ _mm_div_sd(_mm_set1_pd(1.0), input.value) };
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Returns the reciprocal of the input.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr double RTM_SIMD_CALL scalar_reciprocal(double input) RTM_NO_EXCEPT
	{
		return 1.0 / input;
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Returns the addition of the two scalar inputs.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE scalard RTM_SIMD_CALL scalar_add(scalard_arg0 lhs, scalard_arg1 rhs) RTM_NO_EXCEPT
	{
		return scalard{ _mm_add_sd(lhs.value, rhs.value) };
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Returns the addition of the two scalar inputs.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr double RTM_SIMD_CALL scalar_add(double lhs, double rhs) RTM_NO_EXCEPT
	{
		return lhs + rhs;
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Returns the subtraction of the two scalar inputs.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE scalard RTM_SIMD_CALL scalar_sub(scalard_arg0 lhs, scalard_arg1 rhs) RTM_NO_EXCEPT
	{
		return scalard{ _mm_sub_sd(lhs.value, rhs.value) };
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Returns the subtraction of the two scalar inputs.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr double RTM_SIMD_CALL scalar_sub(double lhs, double rhs) RTM_NO_EXCEPT
	{
		return lhs - rhs;
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Returns the multiplication of the two scalar inputs.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE scalard RTM_SIMD_CALL scalar_mul(scalard_arg0 lhs, scalard_arg1 rhs) RTM_NO_EXCEPT
	{
		return scalard{ _mm_mul_sd(lhs.value, rhs.value) };
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Returns the multiplication of the two scalar inputs.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr double RTM_SIMD_CALL scalar_mul(double lhs, double rhs) RTM_NO_EXCEPT
	{
		return lhs * rhs;
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Returns the division of the two scalar inputs.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE scalard RTM_SIMD_CALL scalar_div(scalard_arg0 lhs, scalard_arg1 rhs) RTM_NO_EXCEPT
	{
		return scalard{ _mm_div_sd(lhs.value, rhs.value) };
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Returns the division of the two scalar inputs.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr double RTM_SIMD_CALL scalar_div(double lhs, double rhs) RTM_NO_EXCEPT
	{
		return lhs / rhs;
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Returns the multiplication/addition of the three inputs: s2 + (s0 * s1)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE scalard RTM_SIMD_CALL scalar_mul_add(scalard_arg0 s0, scalard_arg1 s1, scalard_arg2 s2) RTM_NO_EXCEPT
	{
		return scalard{ _mm_add_sd(_mm_mul_sd(s0.value, s1.value), s2.value) };
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Returns the multiplication/addition of the three inputs: s2 + (s0 * s1)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE double RTM_SIMD_CALL scalar_mul_add(double s0, double s1, double s2) RTM_NO_EXCEPT
	{
#if defined(RTM_NEON_INTRINSICS)
		return std::fma(s0, s1, s2);
#else
		return (s0 * s1) + s2;
#endif
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Returns the negative multiplication/subtraction of the three inputs: -((s0 * s1) - s2)
	// This is mathematically equivalent to: s2 - (s0 * s1)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE scalard RTM_SIMD_CALL scalar_neg_mul_sub(scalard_arg0 s0, scalard_arg1 s1, scalard_arg2 s2) RTM_NO_EXCEPT
	{
		return scalard{ _mm_sub_sd(s2.value, _mm_mul_sd(s0.value, s1.value)) };
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Returns the negative multiplication/subtraction of the three inputs: -((s0 * s1) - s2)
	// This is mathematically equivalent to: s2 - (s0 * s1)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr double RTM_SIMD_CALL scalar_neg_mul_sub(double s0, double s1, double s2) RTM_NO_EXCEPT
	{
		return s2 - (s0 * s1);
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Returns the linear interpolation of the two inputs at the specified alpha.
	// The formula used is: ((1.0 - alpha) * start) + (alpha * end).
	// Interpolation is stable and will return 'start' when alpha is 0.0 and 'end' when it is 1.0.
	// This is the same instruction count when FMA is present but it might be slightly slower
	// due to the extra multiplication compared to: start + (alpha * (end - start)).
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE scalard RTM_SIMD_CALL scalar_lerp(scalard_arg0 start, scalard_arg1 end, scalard_arg2 alpha) RTM_NO_EXCEPT
	{
		// ((1.0 - alpha) * start) + (alpha * end) == (start - alpha * start) + (alpha * end)
		return scalar_mul_add(end, alpha, scalar_neg_mul_sub(start, alpha, start));
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Returns the linear interpolation of the two inputs at the specified alpha.
	// The formula used is: ((1.0 - alpha) * start) + (alpha * end).
	// Interpolation is stable and will return 'start' when alpha is 0.0 and 'end' when it is 1.0.
	// This is the same instruction count when FMA is present but it might be slightly slower
	// due to the extra multiplication compared to: start + (alpha * (end - start)).
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE double RTM_SIMD_CALL scalar_lerp(double start, double end, double alpha) RTM_NO_EXCEPT
	{
		// ((1.0 - alpha) * start) + (alpha * end) == (start - alpha * start) + (alpha * end)
		return scalar_mul_add(end, alpha, scalar_neg_mul_sub(start, alpha, start));
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Returns the smallest of the two inputs.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE scalard RTM_SIMD_CALL scalar_min(scalard_arg0 lhs, scalard_arg1 rhs) RTM_NO_EXCEPT
	{
		return scalard{ _mm_min_sd(lhs.value, rhs.value) };
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Returns the smallest of the two inputs.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE double RTM_SIMD_CALL scalar_min(double left, double right) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return _mm_cvtsd_f64(_mm_min_sd(_mm_set1_pd(left), _mm_set1_pd(right)));
#else
		return (std::min)(left, right);
#endif
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Returns the largest of the two inputs.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE scalard RTM_SIMD_CALL scalar_max(scalard_arg0 lhs, scalard_arg1 rhs) RTM_NO_EXCEPT
	{
		return scalard{ _mm_max_sd(lhs.value, rhs.value) };
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Returns the largest of the two inputs.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE double RTM_SIMD_CALL scalar_max(double left, double right) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return _mm_cvtsd_f64(_mm_max_sd(_mm_set1_pd(left), _mm_set1_pd(right)));
#else
		return (std::max)(left, right);
#endif
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Returns true if both inputs are equal, false otherwise.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL scalar_equal(scalard_arg0 lhs, scalard_arg1 rhs) RTM_NO_EXCEPT
	{
		return _mm_comieq_sd(lhs.value, rhs.value) != 0;
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Returns true if both inputs are equal, false otherwise.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr bool RTM_SIMD_CALL scalar_equal(double lhs, double rhs) RTM_NO_EXCEPT
	{
		return lhs == rhs;
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Returns true if lhs < rhs, false otherwise.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL scalar_lower_than(scalard_arg0 lhs, scalard_arg1 rhs) RTM_NO_EXCEPT
	{
		return _mm_comilt_sd(lhs.value, rhs.value) != 0;
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Returns true if lhs < rhs, false otherwise.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr bool RTM_SIMD_CALL scalar_lower_than(double lhs, double rhs) RTM_NO_EXCEPT
	{
		return lhs < rhs;
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Returns true if lhs <= rhs, false otherwise.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL scalar_lower_equal(scalard_arg0 lhs, scalard_arg1 rhs) RTM_NO_EXCEPT
	{
		return _mm_comile_sd(lhs.value, rhs.value) != 0;
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Returns true if lhs <= rhs, false otherwise.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr bool RTM_SIMD_CALL scalar_lower_equal(double lhs, double rhs) RTM_NO_EXCEPT
	{
		return lhs <= rhs;
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Returns true if lhs > rhs, false otherwise.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL scalar_greater_than(scalard_arg0 lhs, scalard_arg1 rhs) RTM_NO_EXCEPT
	{
		return _mm_comigt_sd(lhs.value, rhs.value) != 0;
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Returns true if lhs > rhs, false otherwise.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr bool RTM_SIMD_CALL scalar_greater_than(double lhs, double rhs) RTM_NO_EXCEPT
	{
		return lhs > rhs;
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Returns true if lhs >= rhs, false otherwise.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL scalar_greater_equal(scalard_arg0 lhs, scalard_arg1 rhs) RTM_NO_EXCEPT
	{
		return _mm_comige_sd(lhs.value, rhs.value) != 0;
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Returns true if lhs >= rhs, false otherwise.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr bool RTM_SIMD_CALL scalar_greater_equal(double lhs, double rhs) RTM_NO_EXCEPT
	{
		return lhs >= rhs;
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Returns true if both inputs are nearly equal, otherwise false: abs(lhs - rhs) <= threshold
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL scalar_near_equal(scalard_arg0 lhs, scalard_arg1 rhs, scalard_arg2 threshold) RTM_NO_EXCEPT
	{
		return scalar_lower_equal(scalar_abs(scalar_sub(lhs, rhs)), threshold);
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Returns true if both inputs are nearly equal, otherwise false: abs(lhs - rhs) <= threshold
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL scalar_near_equal(double lhs, double rhs, double threshold) RTM_NO_EXCEPT
	{
		return scalar_abs(lhs - rhs) <= threshold;
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Returns true if the input is finite (not NaN or Inf), false otherwise.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL scalar_is_finite(scalard_arg0 input) RTM_NO_EXCEPT
	{
		const __m128i abs_mask = _mm_set_epi64x(0x7FFFFFFFFFFFFFFFULL, 0x7FFFFFFFFFFFFFFFULL);
		__m128d abs_input = _mm_and_pd(input.value, _mm_castsi128_pd(abs_mask));

		const __m128d infinity = _mm_set1_pd(std::numeric_limits<double>::infinity());
		__m128d is_infinity = _mm_cmpeq_sd(abs_input, infinity);

		__m128d is_nan = _mm_cmpneq_sd(input.value, input.value);

		__m128d is_not_finite = _mm_or_pd(is_infinity, is_nan);
		return (_mm_movemask_pd(is_not_finite) & 0x1) == 0;
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Returns true if the input is finite (not NaN or Inf), false otherwise.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL scalar_is_finite(double input) RTM_NO_EXCEPT
	{
		return std::isfinite(input);
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Returns the rounded input using a symmetric algorithm.
	// scalar_symmetric_round(1.5) = 2.0
	// scalar_symmetric_round(1.2) = 1.0
	// scalar_symmetric_round(-1.5) = -2.0
	// scalar_symmetric_round(-1.2) = -1.0
	// Note: This function relies on the default floating point rounding mode (banker's rounding).
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline scalard RTM_SIMD_CALL scalar_round_symmetric(scalard_arg0 input) RTM_NO_EXCEPT
	{
		// NaN, +- Infinity, and numbers larger or equal to 2^23 remain unchanged
		// since they have no fractional part.

#if defined(RTM_SSE4_INTRINSICS)
		__m128d is_positive = _mm_cmpge_sd(input.value, _mm_setzero_pd());

		const __m128d sign_mask = _mm_set_pd(-0.0, -0.0);
		__m128d sign = _mm_andnot_pd(is_positive, sign_mask);

		// For positive values, we add a bias of 0.5.
		// For negative values, we add a bias of -0.5.
		__m128d bias = _mm_or_pd(sign, _mm_set1_pd(0.5));
		__m128d biased_input = _mm_add_sd(input.value, bias);

		__m128d floored = _mm_floor_sd(biased_input, biased_input);
		__m128d ceiled = _mm_ceil_sd(biased_input, biased_input);

		__m128d result = RTM_VECTOR2D_SELECT(is_positive, floored, ceiled);
		return scalard{ result };
#else
		const __m128i abs_mask = _mm_set_epi64x(0x7FFFFFFFFFFFFFFFULL, 0x7FFFFFFFFFFFFFFFULL);
		const __m128d fractional_limit = _mm_set1_pd(4503599627370496.0); // 2^52

		// Build our mask, larger values that have no fractional part, and infinities will be true
		// Smaller values and NaN will be false
		__m128d abs_input = _mm_and_pd(input.value, _mm_castsi128_pd(abs_mask));
		__m128d is_input_large = _mm_cmpge_sd(abs_input, fractional_limit);

		// Test if our input is NaN with (value != value), it is only true for NaN
		__m128d is_nan = _mm_cmpneq_sd(input.value, input.value);

		// Combine our masks to determine if we should return the original value
		__m128d use_original_input = _mm_or_pd(is_input_large, is_nan);

		const __m128d sign_mask = _mm_set_pd(-0.0, -0.0);
		__m128d sign = _mm_and_pd(input.value, sign_mask);

		// For positive values, we add a bias of 0.5.
		// For negative values, we add a bias of -0.5.
		__m128d bias = _mm_or_pd(sign, _mm_set1_pd(0.5));
		__m128d biased_input = _mm_add_sd(input.value, bias);

		// Convert to an integer with truncation and back, this rounds towards zero.
		__m128d integer_part = _mm_cvtepi32_pd(_mm_cvttpd_epi32(biased_input));

		__m128d result = _mm_or_pd(_mm_and_pd(use_original_input, input.value), _mm_andnot_pd(use_original_input, integer_part));

		return scalard{ result };
#endif
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Returns the rounded input using a symmetric algorithm.
	// scalar_round_symmetric(1.5) = 2.0
	// scalar_round_symmetric(1.2) = 1.0
	// scalar_round_symmetric(-1.5) = -2.0
	// scalar_round_symmetric(-1.2) = -1.0
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline double RTM_SIMD_CALL scalar_round_symmetric(double input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return scalar_cast(scalar_round_symmetric(scalar_set(input)));
#else
		return input >= 0.0 ? scalar_floor(input + 0.5) : scalar_ceil(input - 0.5);
#endif
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Returns the rounded input using banker's rounding (half to even).
	// scalar_round_bankers(2.5) = 2.0
	// scalar_round_bankers(1.5) = 2.0
	// scalar_round_bankers(1.2) = 1.0
	// scalar_round_bankers(-2.5) = -2.0
	// scalar_round_bankers(-1.5) = -2.0
	// scalar_round_bankers(-1.2) = -1.0
	// Note: This function relies on the default floating point rounding mode (banker's rounding).
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE scalard RTM_SIMD_CALL scalar_round_bankers(scalard_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE4_INTRINSICS)
		return scalard{ _mm_round_sd(input.value, input.value, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC) };
#else
		const __m128i abs_mask = _mm_set_epi64x(0x7FFFFFFFFFFFFFFFULL, 0x7FFFFFFFFFFFFFFFULL);

		const __m128d sign_mask = _mm_set_pd(-0.0, -0.0);
		__m128d sign = _mm_and_pd(input.value, sign_mask);

		// We add the largest integer that a 64 bit floating point number can represent and subtract it afterwards.
		// This relies on the fact that if we had a fractional part, the new value cannot be represented accurately
		// and IEEE 754 will perform rounding for us. The default rounding mode is Banker's rounding.
		// This has the effect of removing the fractional part while simultaneously rounding.
		// Use the same sign as the input value to make sure we handle positive and negative values.
		const __m128d fractional_limit = _mm_set1_pd(4503599627370496.0); // 2^52
		__m128d truncating_offset = _mm_or_pd(sign, fractional_limit);
		__m128d integer_part = _mm_sub_sd(_mm_add_sd(input.value, truncating_offset), truncating_offset);

		__m128d abs_input = _mm_and_pd(input.value, _mm_castsi128_pd(abs_mask));
		__m128d is_input_large = _mm_cmpge_sd(abs_input, fractional_limit);
		__m128d result = _mm_or_pd(_mm_and_pd(is_input_large, input.value), _mm_andnot_pd(is_input_large, integer_part));

		return scalard{ result };
#endif
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Returns the rounded input using banker's rounding (half to even).
	// scalar_round_bankers(2.5) = 2.0
	// scalar_round_bankers(1.5) = 2.0
	// scalar_round_bankers(1.2) = 1.0
	// scalar_round_bankers(-2.5) = -2.0
	// scalar_round_bankers(-1.5) = -2.0
	// scalar_round_bankers(-1.2) = -1.0
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE double RTM_SIMD_CALL scalar_round_bankers(double input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return scalar_cast(scalar_round_bankers(scalar_set(input)));
#else
		if (!scalar_is_finite(input))
			return input;

		int64_t whole = static_cast<int64_t>(input);
		double whole_f = static_cast<double>(whole);
		double remainder = scalar_abs(input - whole_f);
		if (remainder < 0.5)
			return whole_f;
		if (remainder > 0.5)
			return input >= 0.0 ? (whole_f + 1.0) : (whole_f - 1.0);

		if ((whole % 2) == 0)
			return whole_f;
		else
			return input >= 0.0 ? (whole_f + 1.0) : (whole_f - 1.0);
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the fractional part of the input.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE double RTM_SIMD_CALL scalar_fraction(double value) RTM_NO_EXCEPT
	{
		return value - scalar_floor(value);
	}

	//////////////////////////////////////////////////////////////////////////
	// Safely casts an integral input into a float64 output.
	//////////////////////////////////////////////////////////////////////////
	template<typename SrcIntegralType>
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE double RTM_SIMD_CALL scalar_safe_to_double(SrcIntegralType input) RTM_NO_EXCEPT
	{
		double input_f = double(input);
		RTM_ASSERT(SrcIntegralType(input_f) == input, "Conversion to double would result in truncation");
		return input_f;
	}

	//////////////////////////////////////////////////////////////////////////
	// Trigonometric functions
	//////////////////////////////////////////////////////////////////////////

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Returns the sine of the input angle.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline scalard RTM_SIMD_CALL scalar_sin(scalard_arg0 angle) RTM_NO_EXCEPT
	{
		return scalar_set(std::sin(scalar_cast(angle)));
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Returns the sine of the input angle.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline double RTM_SIMD_CALL scalar_sin(double angle) RTM_NO_EXCEPT
	{
		return std::sin(angle);
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Returns the cosine of the input angle.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline scalard RTM_SIMD_CALL scalar_cos(scalard_arg0 angle) RTM_NO_EXCEPT
	{
		return scalar_set(std::cos(scalar_cast(angle)));
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Returns the cosine of the input angle.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline double RTM_SIMD_CALL scalar_cos(double angle) RTM_NO_EXCEPT
	{
		return std::cos(angle);
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Returns both sine and cosine of the input angle.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline vector4d RTM_SIMD_CALL scalar_sincos(scalard_arg0 angle) RTM_NO_EXCEPT
	{
		scalard sin_ = scalar_sin(angle);
		scalard cos_ = scalar_cos(angle);

		__m128d xy = _mm_unpacklo_pd(sin_.value, cos_.value);
		return vector4d{ xy, xy };
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Returns both sine and cosine of the input angle.
	// The result's [x] component contains sin(angle).
	// The result's [y] component contains cos(angle).
	// [zw] are undefined.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline vector4d RTM_SIMD_CALL scalar_sincos(double angle) RTM_NO_EXCEPT
	{
		scalard angle_ = scalar_set(angle);
		scalard sin_ = scalar_sin(angle_);
		scalard cos_ = scalar_cos(angle_);

#if defined(RTM_SSE2_INTRINSICS)
		__m128d xy = _mm_unpacklo_pd(sin_.value, cos_.value);
		return vector4d{ xy, xy };
#else
		return vector4d{ sin_, cos_, sin_, cos_ };
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns both sine and cosine of the input angle.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline void RTM_SIMD_CALL scalar_sincos(double angle, double& out_sin, double& out_cos) RTM_NO_EXCEPT
	{
		out_sin = scalar_sin(angle);
		out_cos = scalar_cos(angle);
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Returns the arc-sine of the input.
	// Input value must be in the range [-1.0, 1.0].
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline scalard RTM_SIMD_CALL scalar_asin(scalard_arg0 value) RTM_NO_EXCEPT
	{
		return scalar_set(std::asin(scalar_cast(value)));
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Returns the arc-sine of the input.
	// Input value must be in the range [-1.0, 1.0].
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline double RTM_SIMD_CALL scalar_asin(double value) RTM_NO_EXCEPT
	{
		return std::asin(value);
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Returns the arc-cosine of the input.
	// Input value must be in the range [-1.0, 1.0].
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline scalard RTM_SIMD_CALL scalar_acos(scalard_arg0 value) RTM_NO_EXCEPT
	{
		return scalar_set(std::acos(scalar_cast(value)));
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Returns the arc-cosine of the input.
	// Input value must be in the range [-1.0, 1.0].
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline double RTM_SIMD_CALL scalar_acos(double value) RTM_NO_EXCEPT
	{
		return std::acos(value);
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Returns the tangent of the input angle.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline scalard RTM_SIMD_CALL scalar_tan(scalard_arg0 angle) RTM_NO_EXCEPT
	{
		return scalar_set(std::tan(scalar_cast(angle)));
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Returns the tangent of the input angle.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline double RTM_SIMD_CALL scalar_tan(double angle) RTM_NO_EXCEPT
	{
		return std::tan(angle);
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Returns the arc-tangent of the input.
	// Note that due to the sign ambiguity, atan cannot determine which quadrant
	// the value resides in. See scalar_atan2.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline scalard RTM_SIMD_CALL scalar_atan(scalard_arg0 value) RTM_NO_EXCEPT
	{
		return scalar_set(std::atan(scalar_cast(value)));
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Returns the arc-tangent of the input.
	// Note that due to the sign ambiguity, atan cannot determine which quadrant
	// the value resides in. See scalar_atan2.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline double RTM_SIMD_CALL scalar_atan(double value) RTM_NO_EXCEPT
	{
		return std::atan(value);
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Returns the arc-tangent of [y/x] using the sign of the arguments to
	// determine the correct quadrant.
	// Y represents the proportion of the y-coordinate.
	// X represents the proportion of the x-coordinate.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline scalard RTM_SIMD_CALL scalar_atan2(scalard_arg0 y, scalard_arg1 x) RTM_NO_EXCEPT
	{
		// If X == 0.0 and Y != 0.0, we return PI/2 with the sign of Y
		// If X == 0.0 and Y == 0.0, we return 0.0
		// If X > 0.0, we return atan(y/x)
		// If X < 0.0, we return atan(y/x) + sign(Y) * PI
		// See: https://en.wikipedia.org/wiki/Atan2#Definition_and_computation

		const __m128d zero = _mm_setzero_pd();
		__m128d is_x_zero = _mm_cmpeq_sd(x.value, zero);
		__m128d is_y_zero = _mm_cmpeq_sd(y.value, zero);
		__m128d inputs_are_zero = _mm_and_pd(is_x_zero, is_y_zero);

		__m128d is_x_positive = _mm_cmpgt_sd(x.value, zero);

		const __m128d sign_mask = _mm_set_pd(-0.0, -0.0);
		__m128d y_sign = _mm_and_pd(y.value, sign_mask);

		// If X == 0.0, our offset is PI/2 otherwise it is PI both with the sign of Y
		__m128d half_pi = _mm_set1_pd(constants::half_pi());
		__m128d pi = _mm_set1_pd(constants::pi());
		__m128d offset = _mm_or_pd(_mm_and_pd(is_x_zero, half_pi), _mm_andnot_pd(is_x_zero, pi));
		offset = _mm_or_pd(offset, y_sign);

		// If X > 0.0, our offset is 0.0
		offset = _mm_andnot_pd(is_x_positive, offset);

		// If X == 0.0 and Y == 0.0, our offset is 0.0
		offset = _mm_andnot_pd(inputs_are_zero, offset);

		__m128d angle = _mm_div_sd(y.value, x.value);
		__m128d value = scalar_atan(scalard{ angle }).value;

		// If X == 0.0, our value is 0.0 otherwise it is atan(y/x)
		value = _mm_or_pd(_mm_and_pd(is_x_zero, zero), _mm_andnot_pd(is_x_zero, value));

		// If X == 0.0 and Y == 0.0, our value is 0.0
		value = _mm_andnot_pd(inputs_are_zero, value);

		__m128d result = _mm_add_sd(value, offset);
		return scalard{ result };
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Returns the arc-tangent of [y/x] using the sign of the arguments to
	// determine the correct quadrant.
	// Y represents the proportion of the y-coordinate.
	// X represents the proportion of the x-coordinate.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline double RTM_SIMD_CALL scalar_atan2(double y, double x) RTM_NO_EXCEPT
	{
		// If X == 0.0 and Y != 0.0, we return PI/2 with the sign of Y
		// If X == 0.0 and Y == 0.0, we return 0.0
		// If X > 0.0, we return atan(y/x)
		// If X < 0.0, we return atan(y/x) + sign(Y) * PI
		// See: https://en.wikipedia.org/wiki/Atan2#Definition_and_computation

		if (x == 0.0)
		{
			if (y == 0.0)
				return 0.0;

			return rtm_impl::copysign(constants::half_pi(), y);
		}

		double value = scalar_atan(y / x);
		if (x > 0.0)
			return value;

		double offset = rtm_impl::copysign(constants::pi(), y);
		return value + offset;
	}

	//////////////////////////////////////////////////////////////////////////
	// Converts degrees into radians.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr double RTM_SIMD_CALL scalar_deg_to_rad(double deg) RTM_NO_EXCEPT
	{
		return deg * constants::pi_div_one_eighty();
	}

	//////////////////////////////////////////////////////////////////////////
	// Converts radians into degrees.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr double RTM_SIMD_CALL scalar_rad_to_deg(double rad) RTM_NO_EXCEPT
	{
		return rad * constants::one_eighty_div_pi();
	}

	RTM_IMPL_VERSION_NAMESPACE_END
}

RTM_IMPL_FILE_PRAGMA_POP

```

`includes/rtm/scalarf.h`:

```h
#pragma once

////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2017 Nicholas Frechette & Animation Compression Library contributors
// Copyright (c) 2018 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include "rtm/constants.h"
#include "rtm/macros.h"
#include "rtm/math.h"
#include "rtm/version.h"
#include "rtm/impl/cmath.impl.h"
#include "rtm/impl/compiler_utils.h"
#include "rtm/impl/scalar_common.h"

#include <algorithm>
#include <limits>

RTM_IMPL_FILE_PRAGMA_PUSH

namespace rtm
{
	RTM_IMPL_VERSION_NAMESPACE_BEGIN

	//////////////////////////////////////////////////////////////////////////
	// Creates a scalar from a floating point value.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE scalarf RTM_SIMD_CALL scalar_set(float xyzw) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return scalarf{ _mm_set_ps1(xyzw) };
#else
		return xyzw;
#endif
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Writes a scalar to memory.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE void RTM_SIMD_CALL scalar_store(scalarf_arg0 input, float* output) RTM_NO_EXCEPT
	{
		_mm_store_ss(output, input.value);
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Writes a scalar to memory.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE void scalar_store(float input, float* output) RTM_NO_EXCEPT
	{
		*output = input;
	}

	//////////////////////////////////////////////////////////////////////////
	// Casts a scalar into a floating point value.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE float RTM_SIMD_CALL scalar_cast(scalarf_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return _mm_cvtss_f32(input.value);
#else
		return input;
#endif
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Returns the largest integer value not greater than the input (round towards minus infinity).
	// scalar_floor(1.8) = 1.0
	// scalar_floor(-1.8) = -2.0
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE scalarf RTM_SIMD_CALL scalar_floor(scalarf_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE4_INTRINSICS)
		return scalarf{ _mm_round_ss(input.value, input.value, 0x9) };
#else
		// NaN, +- Infinity, and numbers larger or equal to 2^23 remain unchanged
		// since they have no fractional part.

		const __m128i abs_mask = _mm_set_epi32(0x7FFFFFFFULL, 0x7FFFFFFFULL, 0x7FFFFFFFULL, 0x7FFFFFFFULL);
		const __m128 fractional_limit = _mm_set_ps1(8388608.0F); // 2^23

		// Build our mask, larger values that have no fractional part, and infinities will be true
		// Smaller values and NaN will be false
		__m128 abs_input = _mm_and_ps(input.value, _mm_castsi128_ps(abs_mask));
		__m128 is_input_large = _mm_cmpge_ss(abs_input, fractional_limit);

		// Test if our input is NaN with (value != value), it is only true for NaN
		__m128 is_nan = _mm_cmpneq_ss(input.value, input.value);

		// Combine our masks to determine if we should return the original value
		__m128 use_original_input = _mm_or_ps(is_input_large, is_nan);

		// Convert to an integer and back. This does banker's rounding by default
		__m128 integer_part = _mm_cvtepi32_ps(_mm_cvtps_epi32(input.value));

		// Test if the returned value is greater than the original.
		// A negative input will round towards zero and be greater when we need it to be smaller.
		__m128 is_negative = _mm_cmpgt_ss(integer_part, input.value);

		// Convert our mask to a float, ~0 yields -1.0 since it is a valid signed integer
		// Positive values will yield a 0.0 bias
		__m128 bias = _mm_cvtepi32_ps(_mm_castps_si128(is_negative));

		// Add our bias to properly handle negative values
		integer_part = _mm_add_ss(integer_part, bias);

		__m128 result = _mm_or_ps(_mm_and_ps(use_original_input, input.value), _mm_andnot_ps(use_original_input, integer_part));
		return scalarf{ result };
#endif
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Returns the largest integer value not greater than the input (round towards negative infinity).
	// scalar_floor(1.8) = 1.0
	// scalar_floor(-1.8) = -2.0
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE float scalar_floor(float input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return scalar_cast(scalar_floor(scalar_set(input)));
#else
		return std::floor(input);
#endif
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Returns the smallest integer value not less than the input (round towards positive infinity).
	// scalar_ceil(1.8) = 2.0
	// scalar_ceil(-1.8) = -1.0
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE scalarf RTM_SIMD_CALL scalar_ceil(scalarf_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE4_INTRINSICS)
		return scalarf{ _mm_round_ss(input.value, input.value, 0xA) };
#else
		// NaN, +- Infinity, and numbers larger or equal to 2^23 remain unchanged
		// since they have no fractional part.

		const __m128i abs_mask = _mm_set_epi32(0x7FFFFFFFULL, 0x7FFFFFFFULL, 0x7FFFFFFFULL, 0x7FFFFFFFULL);
		const __m128 fractional_limit = _mm_set_ps1(8388608.0F); // 2^23

		// Build our mask, larger values that have no fractional part, and infinities will be true
		// Smaller values and NaN will be false
		__m128 abs_input = _mm_and_ps(input.value, _mm_castsi128_ps(abs_mask));
		__m128 is_input_large = _mm_cmpge_ss(abs_input, fractional_limit);

		// Test if our input is NaN with (value != value), it is only true for NaN
		__m128 is_nan = _mm_cmpneq_ss(input.value, input.value);

		// Combine our masks to determine if we should return the original value
		__m128 use_original_input = _mm_or_ps(is_input_large, is_nan);

		// Convert to an integer and back. This does banker's rounding by default
		__m128 integer_part = _mm_cvtepi32_ps(_mm_cvtps_epi32(input.value));

		// Test if the returned value is smaller than the original.
		// A positive input will round towards zero and be lower when we need it to be greater.
		__m128 is_positive = _mm_cmplt_ss(integer_part, input.value);

		// Convert our mask to a float, ~0 yields -1.0 since it is a valid signed integer
		// Negative values will yield a 0.0 bias
		__m128 bias = _mm_cvtepi32_ps(_mm_castps_si128(is_positive));

		// Subtract our bias to properly handle positive values
		integer_part = _mm_sub_ss(integer_part, bias);

		__m128 result = _mm_or_ps(_mm_and_ps(use_original_input, input.value), _mm_andnot_ps(use_original_input, integer_part));
		return scalarf{ result };
#endif
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Returns the smallest integer value not less than the input (round towards positive infinity).
	// scalar_ceil(1.8) = 2.0
	// scalar_ceil(-1.8) = -1.0
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE float scalar_ceil(float input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return scalar_cast(scalar_ceil(scalar_set(input)));
#else
		return std::ceil(input);
#endif
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Returns the input if it is within the min/max values otherwise the
	// exceeded boundary is returned.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE scalarf RTM_SIMD_CALL scalar_clamp(scalarf_arg0 input, scalarf_arg1 min, scalarf_arg2 max) RTM_NO_EXCEPT
	{
		return scalarf{ _mm_min_ss(_mm_max_ss(input.value, min.value), max.value) };
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Returns the input if it is within the min/max values otherwise the
	// exceeded boundary is returned.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE float scalar_clamp(float input, float min, float max) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return _mm_cvtss_f32(_mm_min_ss(_mm_max_ss(_mm_set_ps1(input), _mm_set_ps1(min)), _mm_set_ps1(max)));
#else
		return (std::min)((std::max)(input, min), max);
#endif
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Returns the absolute value of the input.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE scalarf RTM_SIMD_CALL scalar_abs(scalarf_arg0 input) RTM_NO_EXCEPT
	{
		const __m128i abs_mask = _mm_set_epi32(0x7FFFFFFFULL, 0x7FFFFFFFULL, 0x7FFFFFFFULL, 0x7FFFFFFFULL);
		return scalarf{ _mm_and_ps(input.value, _mm_castsi128_ps(abs_mask)) };
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Returns the absolute value of the input.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE float scalar_abs(float input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		const __m128i abs_mask = _mm_set_epi32(0x7FFFFFFFULL, 0x7FFFFFFFULL, 0x7FFFFFFFULL, 0x7FFFFFFFULL);
		return _mm_cvtss_f32(_mm_and_ps(_mm_set_ps1(input), _mm_castsi128_ps(abs_mask)));
#else
		return std::fabs(input);
#endif
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Returns the square root of the input.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE scalarf RTM_SIMD_CALL scalar_sqrt(scalarf_arg0 input) RTM_NO_EXCEPT
	{
		return scalarf{ _mm_sqrt_ss(input.value) };
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Returns the square root of the input.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE float RTM_SIMD_CALL scalar_sqrt(float input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return _mm_cvtss_f32(_mm_sqrt_ss(_mm_set_ps1(input)));
#else
		return std::sqrt(input);
#endif
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Returns the reciprocal square root of the input.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE scalarf RTM_SIMD_CALL scalar_sqrt_reciprocal(scalarf_arg0 input) RTM_NO_EXCEPT
	{
		return scalarf{ _mm_div_ss(_mm_set_ps1(1.0F), _mm_sqrt_ss(input.value)) };
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Returns the reciprocal square root of the input.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE float RTM_SIMD_CALL scalar_sqrt_reciprocal(float input) RTM_NO_EXCEPT
	{
		// Performance note:
		// With modern out-of-order executing processors, it is typically faster to use
		// a full division/square root instead of a reciprocal estimate + Newton-Raphson iterations
		// because the resulting code is more dense and is more likely to inline and
		// as it uses fewer instructions.
		return 1.0F / scalar_sqrt(input);
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Returns the reciprocal of the input.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE scalarf RTM_SIMD_CALL scalar_reciprocal(scalarf_arg0 input) RTM_NO_EXCEPT
	{
		return scalarf{ _mm_div_ss(_mm_set_ps1(1.0F), input.value) };
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Returns the reciprocal of the input.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE float RTM_SIMD_CALL scalar_reciprocal(float input) RTM_NO_EXCEPT
	{
		// Performance note:
		// With modern out-of-order executing processors, it is typically faster to use
		// a full division instead of a reciprocal estimate + Newton-Raphson iterations
		// because the resulting code is more dense and is more likely to inline and
		// as it uses fewer instructions.
		return 1.0F / input;
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Returns the addition of the two scalar inputs.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE scalarf RTM_SIMD_CALL scalar_add(scalarf_arg0 lhs, scalarf_arg1 rhs) RTM_NO_EXCEPT
	{
		return scalarf{ _mm_add_ss(lhs.value, rhs.value) };
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Returns the addition of the two scalar inputs.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr float scalar_add(float lhs, float rhs) RTM_NO_EXCEPT
	{
		return lhs + rhs;
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Returns the subtraction of the two scalar inputs.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE scalarf RTM_SIMD_CALL scalar_sub(scalarf_arg0 lhs, scalarf_arg1 rhs) RTM_NO_EXCEPT
	{
		return scalarf{ _mm_sub_ss(lhs.value, rhs.value) };
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Returns the subtraction of the two scalar inputs.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr float scalar_sub(float lhs, float rhs) RTM_NO_EXCEPT
	{
		return lhs - rhs;
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Returns the multiplication of the two scalar inputs.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE scalarf RTM_SIMD_CALL scalar_mul(scalarf_arg0 lhs, scalarf_arg1 rhs) RTM_NO_EXCEPT
	{
		return scalarf{ _mm_mul_ss(lhs.value, rhs.value) };
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Returns the multiplication of the two scalar inputs.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr float scalar_mul(float lhs, float rhs) RTM_NO_EXCEPT
	{
		return lhs * rhs;
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Returns the division of the two scalar inputs.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE scalarf RTM_SIMD_CALL scalar_div(scalarf_arg0 lhs, scalarf_arg1 rhs) RTM_NO_EXCEPT
	{
		return scalarf{ _mm_div_ss(lhs.value, rhs.value) };
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Returns the division of the two scalar inputs.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr float scalar_div(float lhs, float rhs) RTM_NO_EXCEPT
	{
		return lhs / rhs;
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Returns the multiplication/addition of the three inputs: s2 + (s0 * s1)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE scalarf RTM_SIMD_CALL scalar_mul_add(scalarf_arg0 s0, scalarf_arg1 s1, scalarf_arg2 s2) RTM_NO_EXCEPT
	{
		return scalarf{ _mm_add_ss(_mm_mul_ss(s0.value, s1.value), s2.value) };
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Returns the multiplication/addition of the three inputs: s2 + (s0 * s1)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE float scalar_mul_add(float s0, float s1, float s2) RTM_NO_EXCEPT
	{
#if defined(RTM_NEON_INTRINSICS)
		return std::fma(s0, s1, s2);
#else
		return (s0 * s1) + s2;
#endif
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Returns the negative multiplication/subtraction of the three inputs: -((s0 * s1) - s2)
	// This is mathematically equivalent to: s2 - (s0 * s1)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE scalarf RTM_SIMD_CALL scalar_neg_mul_sub(scalarf_arg0 s0, scalarf_arg1 s1, scalarf_arg2 s2) RTM_NO_EXCEPT
	{
		return scalarf{ _mm_sub_ss(s2.value, _mm_mul_ss(s0.value, s1.value)) };
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Returns the negative multiplication/subtraction of the three inputs: -((s0 * s1) - s2)
	// This is mathematically equivalent to: s2 - (s0 * s1)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr float scalar_neg_mul_sub(float s0, float s1, float s2) RTM_NO_EXCEPT
	{
		return s2 - (s0 * s1);
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Returns the linear interpolation of the two inputs at the specified alpha.
	// The formula used is: ((1.0 - alpha) * start) + (alpha * end).
	// Interpolation is stable and will return 'start' when alpha is 0.0 and 'end' when it is 1.0.
	// This is the same instruction count when FMA is present but it might be slightly slower
	// due to the extra multiplication compared to: start + (alpha * (end - start)).
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE scalarf RTM_SIMD_CALL scalar_lerp(scalarf_arg0 start, scalarf_arg1 end, scalarf_arg2 alpha) RTM_NO_EXCEPT
	{
		// ((1.0 - alpha) * start) + (alpha * end) == (start - alpha * start) + (alpha * end)
		return scalar_mul_add(end, alpha, scalar_neg_mul_sub(start, alpha, start));
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Returns the linear interpolation of the two inputs at the specified alpha.
	// The formula used is: ((1.0 - alpha) * start) + (alpha * end).
	// Interpolation is stable and will return 'start' when alpha is 0.0 and 'end' when it is 1.0.
	// This is the same instruction count when FMA is present but it might be slightly slower
	// due to the extra multiplication compared to: start + (alpha * (end - start)).
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE float scalar_lerp(float start, float end, float alpha) RTM_NO_EXCEPT
	{
		// ((1.0 - alpha) * start) + (alpha * end) == (start - alpha * start) + (alpha * end)
		return scalar_mul_add(end, alpha, scalar_neg_mul_sub(start, alpha, start));
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Returns the smallest of the two inputs.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE scalarf RTM_SIMD_CALL scalar_min(scalarf_arg0 lhs, scalarf_arg1 rhs) RTM_NO_EXCEPT
	{
		return scalarf{ _mm_min_ss(lhs.value, rhs.value) };
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Returns the smallest of the two inputs.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE float scalar_min(float left, float right) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return _mm_cvtss_f32(_mm_min_ss(_mm_set_ps1(left), _mm_set_ps1(right)));
#else
		return (std::min)(left, right);
#endif
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Returns the largest of the two inputs.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE scalarf RTM_SIMD_CALL scalar_max(scalarf_arg0 lhs, scalarf_arg1 rhs) RTM_NO_EXCEPT
	{
		return scalarf{ _mm_max_ss(lhs.value, rhs.value) };
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Returns the largest of the two inputs.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE float scalar_max(float left, float right) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return _mm_cvtss_f32(_mm_max_ss(_mm_set_ps1(left), _mm_set_ps1(right)));
#else
		return (std::max)(left, right);
#endif
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Returns true if both inputs are equal, false otherwise.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL scalar_equal(scalarf_arg0 lhs, scalarf_arg1 rhs) RTM_NO_EXCEPT
	{
		return _mm_comieq_ss(lhs.value, rhs.value) != 0;
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Returns true if both inputs are equal, false otherwise.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr bool scalar_equal(float lhs, float rhs) RTM_NO_EXCEPT
	{
		return lhs == rhs;
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Returns true if lhs < rhs, false otherwise.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL scalar_lower_than(scalarf_arg0 lhs, scalarf_arg1 rhs) RTM_NO_EXCEPT
	{
		return _mm_comilt_ss(lhs.value, rhs.value) != 0;
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Returns true if lhs < rhs, false otherwise.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr bool scalar_lower_than(float lhs, float rhs) RTM_NO_EXCEPT
	{
		return lhs < rhs;
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Returns true if lhs <= rhs, false otherwise.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL scalar_lower_equal(scalarf_arg0 lhs, scalarf_arg1 rhs) RTM_NO_EXCEPT
	{
		return _mm_comile_ss(lhs.value, rhs.value) != 0;
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Returns true if lhs <= rhs, false otherwise.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr bool scalar_lower_equal(float lhs, float rhs) RTM_NO_EXCEPT
	{
		return lhs <= rhs;
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Returns true if lhs > rhs, false otherwise.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL scalar_greater_than(scalarf_arg0 lhs, scalarf_arg1 rhs) RTM_NO_EXCEPT
	{
		return _mm_comigt_ss(lhs.value, rhs.value) != 0;
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Returns true if lhs > rhs, false otherwise.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr bool scalar_greater_than(float lhs, float rhs) RTM_NO_EXCEPT
	{
		return lhs > rhs;
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Returns true if lhs >= rhs, false otherwise.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL scalar_greater_equal(scalarf_arg0 lhs, scalarf_arg1 rhs) RTM_NO_EXCEPT
	{
		return _mm_comige_ss(lhs.value, rhs.value) != 0;
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Returns true if lhs >= rhs, false otherwise.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr bool scalar_greater_equal(float lhs, float rhs) RTM_NO_EXCEPT
	{
		return lhs >= rhs;
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Returns true if both inputs are nearly equal, otherwise false: abs(lhs - rhs) <= threshold
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL scalar_near_equal(scalarf_arg0 lhs, scalarf_arg1 rhs, scalarf_arg2 threshold) RTM_NO_EXCEPT
	{
		return scalar_lower_equal(scalar_abs(scalar_sub(lhs, rhs)), threshold);
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Returns true if both inputs are nearly equal, otherwise false: abs(lhs - rhs) <= threshold
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool scalar_near_equal(float lhs, float rhs, float threshold) RTM_NO_EXCEPT
	{
		return scalar_abs(lhs - rhs) <= threshold;
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Returns true if the input is finite (not NaN or Inf), false otherwise.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL scalar_is_finite(scalarf_arg0 input) RTM_NO_EXCEPT
	{
		const __m128i abs_mask = _mm_set_epi32(0x7FFFFFFFULL, 0x7FFFFFFFULL, 0x7FFFFFFFULL, 0x7FFFFFFFULL);
		__m128 abs_input = _mm_and_ps(input.value, _mm_castsi128_ps(abs_mask));

		const __m128 infinity = _mm_set_ps1(std::numeric_limits<float>::infinity());
		__m128 is_infinity = _mm_cmpeq_ss(abs_input, infinity);

		__m128 is_nan = _mm_cmpneq_ss(input.value, input.value);

		__m128 is_not_finite = _mm_or_ps(is_infinity, is_nan);
		return (_mm_movemask_ps(is_not_finite) & 0x1) == 0;
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Returns true if the input is finite (not NaN or Inf), false otherwise.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool scalar_is_finite(float input) RTM_NO_EXCEPT
	{
		return std::isfinite(input);
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Returns the rounded input using a symmetric algorithm.
	// scalar_symmetric_round(1.5) = 2.0
	// scalar_symmetric_round(1.2) = 1.0
	// scalar_symmetric_round(-1.5) = -2.0
	// scalar_symmetric_round(-1.2) = -1.0
	// Note: This function relies on the default floating point rounding mode (banker's rounding).
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline scalarf RTM_SIMD_CALL scalar_round_symmetric(scalarf_arg0 input) RTM_NO_EXCEPT
	{
		// NaN, +- Infinity, and numbers larger or equal to 2^23 remain unchanged
		// since they have no fractional part.

#if defined(RTM_SSE4_INTRINSICS)
		__m128 is_positive = _mm_cmpge_ss(input.value, _mm_setzero_ps());

		const __m128 sign_mask = _mm_set_ps(-0.0F, -0.0F, -0.0F, -0.0F);
		__m128 sign = _mm_andnot_ps(is_positive, sign_mask);

		// For positive values, we add a bias of 0.5.
		// For negative values, we add a bias of -0.5.
		__m128 bias = _mm_or_ps(sign, _mm_set_ps1(0.5F));
		__m128 biased_input = _mm_add_ss(input.value, bias);

		__m128 floored = _mm_floor_ss(biased_input, biased_input);
		__m128 ceiled = _mm_ceil_ss(biased_input, biased_input);

		__m128 result = RTM_VECTOR4F_SELECT(is_positive, floored, ceiled);
		return scalarf{ result };
#else
		const __m128i abs_mask = _mm_set_epi32(0x7FFFFFFFULL, 0x7FFFFFFFULL, 0x7FFFFFFFULL, 0x7FFFFFFFULL);
		const __m128 fractional_limit = _mm_set_ps1(8388608.0F); // 2^23

		// Build our mask, larger values that have no fractional part, and infinities will be true
		// Smaller values and NaN will be false
		__m128 abs_input = _mm_and_ps(input.value, _mm_castsi128_ps(abs_mask));
		__m128 is_input_large = _mm_cmpge_ss(abs_input, fractional_limit);

		// Test if our input is NaN with (value != value), it is only true for NaN
		__m128 is_nan = _mm_cmpneq_ss(input.value, input.value);

		// Combine our masks to determine if we should return the original value
		__m128 use_original_input = _mm_or_ps(is_input_large, is_nan);

		const __m128 sign_mask = _mm_set_ps(-0.0F, -0.0F, -0.0F, -0.0F);
		__m128 sign = _mm_and_ps(input.value, sign_mask);

		// For positive values, we add a bias of 0.5.
		// For negative values, we add a bias of -0.5.
		__m128 bias = _mm_or_ps(sign, _mm_set_ps1(0.5F));
		__m128 biased_input = _mm_add_ss(input.value, bias);

		// Convert to an integer with truncation and back, this rounds towards zero.
		__m128 integer_part = _mm_cvtepi32_ps(_mm_cvttps_epi32(biased_input));

		__m128 result = _mm_or_ps(_mm_and_ps(use_original_input, input.value), _mm_andnot_ps(use_original_input, integer_part));

		return scalarf{ result };
#endif
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Returns the rounded input using a symmetric algorithm.
	// scalar_round_symmetric(1.5) = 2.0
	// scalar_round_symmetric(1.2) = 1.0
	// scalar_round_symmetric(-1.5) = -2.0
	// scalar_round_symmetric(-1.2) = -1.0
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline float scalar_round_symmetric(float input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return scalar_cast(scalar_round_symmetric(scalar_set(input)));
#elif defined(RTM_NEON64_INTRINSICS)
		// arm64 has floor/ceil instructions
		return input >= 0.0F ? scalar_floor(input + 0.5F) : scalar_ceil(input - 0.5F);
#else
		// NaN, +- Infinity, and numbers larger or equal to 2^23 remain unchanged
		// since they have no fractional part.

		const float fractional_limit = 8388608.0F; // 2^23

		// Build our mask, larger values that have no fractional part, and infinities will be true
		// Smaller values and NaN will be false
		float abs_input = scalar_abs(input);
		bool is_input_large = abs_input >= fractional_limit;

		// Test if our input is NaN with (value != value), it is only true for NaN
		bool is_nan = input != input;

		// Combine our masks to determine if we should return the original value
		bool use_original_input = is_input_large | is_nan;

		// For positive values, we add a bias of 0.5.
		// For negative values, we add a bias of -0.5.
		float bias = input >= 0.0F ? 0.5F : -0.5F;
		float biased_input = input + bias;

		// Convert to an integer with truncation and back, this rounds towards zero.
		float integer_part = static_cast<float>(static_cast<int32_t>(biased_input));

		return use_original_input ? input : integer_part;
#endif
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Returns the rounded input using banker's rounding (half to even).
	// scalar_round_bankers(2.5) = 2.0
	// scalar_round_bankers(1.5) = 2.0
	// scalar_round_bankers(1.2) = 1.0
	// scalar_round_bankers(-2.5) = -2.0
	// scalar_round_bankers(-1.5) = -2.0
	// scalar_round_bankers(-1.2) = -1.0
	// Note: This function relies on the default floating point rounding mode (banker's rounding).
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE scalarf RTM_SIMD_CALL scalar_round_bankers(scalarf_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE4_INTRINSICS)
		return scalarf{ _mm_round_ss(input.value, input.value, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC) };
#else
		const __m128 sign_mask = _mm_set_ps(-0.0F, -0.0F, -0.0F, -0.0F);
		__m128 sign = _mm_and_ps(input.value, sign_mask);

		// We add the largest integer that a 32 bit floating point number can represent and subtract it afterwards.
		// This relies on the fact that if we had a fractional part, the new value cannot be represented accurately
		// and IEEE 754 will perform rounding for us. The default rounding mode is Banker's rounding.
		// This has the effect of removing the fractional part while simultaneously rounding.
		// Use the same sign as the input value to make sure we handle positive and negative values.
		const __m128 fractional_limit = _mm_set_ps1(8388608.0F); // 2^23
		__m128 truncating_offset = _mm_or_ps(sign, fractional_limit);
		__m128 integer_part = _mm_sub_ss(_mm_add_ss(input.value, truncating_offset), truncating_offset);

		// If our input was so large that it had no fractional part, return it unchanged
		// Otherwise return our integer part
		const __m128i abs_mask = _mm_set_epi32(0x7FFFFFFFULL, 0x7FFFFFFFULL, 0x7FFFFFFFULL, 0x7FFFFFFFULL);
		__m128 abs_input = _mm_and_ps(input.value, _mm_castsi128_ps(abs_mask));
		__m128 is_input_large = _mm_cmpge_ss(abs_input, fractional_limit);
		__m128 result = _mm_or_ps(_mm_and_ps(is_input_large, input.value), _mm_andnot_ps(is_input_large, integer_part));

		return scalarf{ result };
#endif
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Returns the rounded input using banker's rounding (half to even).
	// scalar_round_bankers(2.5) = 2.0
	// scalar_round_bankers(1.5) = 2.0
	// scalar_round_bankers(1.2) = 1.0
	// scalar_round_bankers(-2.5) = -2.0
	// scalar_round_bankers(-1.5) = -2.0
	// scalar_round_bankers(-1.2) = -1.0
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE float scalar_round_bankers(float input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return scalar_cast(scalar_round_bankers(scalar_set(input)));
#elif defined(RTM_NEON64_INTRINSICS) && defined(RTM_IMPL_VRNDNS_SUPPORTED)
		return vrndns_f32(input);
#else
		if (!scalar_is_finite(input))
			return input;

		int32_t whole = static_cast<int32_t>(input);
		float whole_f = static_cast<float>(whole);
		float remainder = scalar_abs(input - whole_f);
		if (remainder < 0.5F)
			return whole_f;
		if (remainder > 0.5F)
			return input >= 0.0F ? (whole_f + 1.0F) : (whole_f - 1.0F);

		if ((whole % 2) == 0)
			return whole_f;
		else
			return input >= 0.0F ? (whole_f + 1.0F) : (whole_f - 1.0F);
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the fractional part of the input.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE float scalar_fraction(float value) RTM_NO_EXCEPT
	{
		return value - scalar_floor(value);
	}

	//////////////////////////////////////////////////////////////////////////
	// Safely casts an integral input into a float64 output.
	//////////////////////////////////////////////////////////////////////////
	template<typename SrcIntegralType>
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE float scalar_safe_to_float(SrcIntegralType input) RTM_NO_EXCEPT
	{
		float input_f = float(input);
		RTM_ASSERT(SrcIntegralType(input_f) == input, "Conversion to float would result in truncation");
		return input_f;
	}

	//////////////////////////////////////////////////////////////////////////
	// Trigonometric functions
	//////////////////////////////////////////////////////////////////////////

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Returns the sine of the input angle.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline scalarf RTM_SIMD_CALL scalar_sin(scalarf_arg0 angle) RTM_NO_EXCEPT
	{
		// Use a degree 11 minimax approximation polynomial
		// See: GPGPU Programming for Games and Science (David H. Eberly)

		// Remap our input in the [-pi, pi] range
		__m128 quotient = _mm_mul_ss(angle.value, _mm_set_ps1(constants::one_div_two_pi()));
		quotient = scalar_round_bankers(scalarf{ quotient }).value;
		quotient = _mm_mul_ss(quotient, _mm_set_ps1(constants::two_pi()));
		__m128 x = _mm_sub_ss(angle.value, quotient);

		// Remap our input in the [-pi/2, pi/2] range
		const __m128 sign_mask = _mm_set_ps(-0.0F, -0.0F, -0.0F, -0.0F);
		__m128 sign = _mm_and_ps(x, sign_mask);
		__m128 reference = _mm_or_ps(sign, _mm_set_ps1(constants::pi()));

		const __m128 reflection = _mm_sub_ss(reference, x);
		const __m128i abs_mask = _mm_set_epi32(0x7FFFFFFFULL, 0x7FFFFFFFULL, 0x7FFFFFFFULL, 0x7FFFFFFFULL);
		const __m128 x_abs = _mm_and_ps(x, _mm_castsi128_ps(abs_mask));

		__m128 is_less_equal_than_half_pi = _mm_cmple_ss(x_abs, _mm_set_ps1(constants::half_pi()));

		x = RTM_VECTOR4F_SELECT(is_less_equal_than_half_pi, x, reflection);

		// Calculate our value
		const float x2 = _mm_cvtss_f32(_mm_mul_ss(x, x));
		float result = (x2 * -2.3828544692960918e-8F) + 2.7521557770526783e-6F;
		result = (result * x2) - 1.9840782426250314e-4F;
		result = (result * x2) + 8.3333303183525942e-3F;
		result = (result * x2) - 1.6666666601721269e-1F;
		result = (result * x2) + 1.0F;
		result = result * _mm_cvtss_f32(x);
		return scalar_set(result);
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Returns the sine of the input angle.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline float RTM_SIMD_CALL scalar_sin(float angle) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return scalar_cast(scalar_sin(scalar_set(angle)));
#elif defined(RTM_NEON_INTRINSICS)
		return std::sin(angle);
#else
		// Use a degree 11 minimax approximation polynomial
		// See: GPGPU Programming for Games and Science (David H. Eberly)

		// Remap our input in the [-pi, pi] range
		float quotient = angle * constants::one_div_two_pi();
		quotient = scalar_round_bankers(quotient);
		quotient = quotient * constants::two_pi();
		float x = angle - quotient;

		// Remap our input in the [-pi/2, pi/2] range
		const float reference = rtm_impl::copysign(constants::pi(), x);
		const float reflection = reference - x;
		const float x_abs = scalar_abs(x);
		x = x_abs <= constants::half_pi() ? x : reflection;

		// Calculate our value
		const float x2 = x * x;
		float result = (x2 * -2.3828544692960918e-8F) + 2.7521557770526783e-6F;
		result = (result * x2) - 1.9840782426250314e-4F;
		result = (result * x2) + 8.3333303183525942e-3F;
		result = (result * x2) - 1.6666666601721269e-1F;
		result = (result * x2) + 1.0F;
		result = result * x;
		return result;
#endif
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Returns the cosine of the input angle.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline scalarf RTM_SIMD_CALL scalar_cos(scalarf_arg0 angle) RTM_NO_EXCEPT
	{
		// Use a degree 10 minimax approximation polynomial
		// See: GPGPU Programming for Games and Science (David H. Eberly)

		// Remap our input in the [-pi, pi] range
		__m128 quotient = _mm_mul_ss(angle.value, _mm_set_ps1(constants::one_div_two_pi()));
		quotient = scalar_round_bankers(scalarf{ quotient }).value;
		quotient = _mm_mul_ss(quotient, _mm_set_ps1(constants::two_pi()));
		__m128 x = _mm_sub_ss(angle.value, quotient);

		// Remap our input in the [-pi/2, pi/2] range
		const __m128 sign_mask = _mm_set_ps(-0.0F, -0.0F, -0.0F, -0.0F);
		__m128 x_sign = _mm_and_ps(x, sign_mask);
		__m128 reference = _mm_or_ps(x_sign, _mm_set_ps1(constants::pi()));
		const __m128 reflection = _mm_sub_ss(reference, x);

		const __m128i abs_mask = _mm_set_epi32(0x7FFFFFFFULL, 0x7FFFFFFFULL, 0x7FFFFFFFULL, 0x7FFFFFFFULL);
		__m128 x_abs = _mm_and_ps(x, _mm_castsi128_ps(abs_mask));
		__m128 is_less_equal_than_half_pi = _mm_cmple_ss(x_abs, _mm_set_ps1(constants::half_pi()));

		x = RTM_VECTOR4F_SELECT(is_less_equal_than_half_pi, x, reflection);

		// Calculate our value
		const float x2 = _mm_cvtss_f32(_mm_mul_ss(x, x));
		float result = (x2 * -2.6051615464872668e-7F) + 2.4760495088926859e-5F;
		result = (result * x2) - 1.3888377661039897e-3F;
		result = (result * x2) + 4.1666638865338612e-2F;
		result = (result * x2) - 4.9999999508695869e-1F;
		result = (result * x2) + 1.0F;

		// Remap into [-pi, pi]
		__m128 result_v = _mm_set_ps1(result);
		__m128 cosine = _mm_or_ps(result_v, _mm_andnot_ps(is_less_equal_than_half_pi, sign_mask));
		return scalarf{ cosine };
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Returns the cosine of the input angle.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline float RTM_SIMD_CALL scalar_cos(float angle) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return scalar_cast(scalar_cos(scalar_set(angle)));
#elif defined(RTM_NEON_INTRINSICS)
		return std::cos(angle);
#else
		// Use a degree 10 minimax approximation polynomial
		// See: GPGPU Programming for Games and Science (David H. Eberly)

		// Remap our input in the [-pi, pi] range
		float quotient = angle * constants::one_div_two_pi();
		quotient = scalar_round_bankers(quotient);
		quotient = quotient * constants::two_pi();
		float x = angle - quotient;

		// Remap our input in the [-pi/2, pi/2] range
		const float reference = rtm_impl::copysign(constants::pi(), x);
		const float reflection = reference - x;
		const float x_abs = scalar_abs(x);
		x = x_abs <= constants::half_pi() ? x : reflection;

		// Calculate our value
		const float x2 = x * x;
		float result = (x2 * -2.6051615464872668e-7F) + 2.4760495088926859e-5F;
		result = (result * x2) - 1.3888377661039897e-3F;
		result = (result * x2) + 4.1666638865338612e-2F;
		result = (result * x2) - 4.9999999508695869e-1F;
		result = (result * x2) + 1.0F;

		// Remap into [-pi, pi]
		if (x_abs <= constants::half_pi())
			return result;
		else
			return -result;
#endif
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Returns both sine and cosine of the input angle.
	// The result's [x] component contains sin(angle).
	// The result's [y] component contains cos(angle).
	// [zw] are undefined.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline vector4f RTM_SIMD_CALL scalar_sincos(scalarf_arg0 angle) RTM_NO_EXCEPT
	{
		scalarf sin_ = scalar_sin(angle);
		scalarf cos_ = scalar_cos(angle);

		return _mm_unpacklo_ps(sin_.value, cos_.value);
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Returns both sine and cosine of the input angle.
	// The result's [x] component contains sin(angle).
	// The result's [y] component contains cos(angle).
	// [zw] are undefined.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline vector4f RTM_SIMD_CALL scalar_sincos(float angle) RTM_NO_EXCEPT
	{
		scalarf angle_ = scalar_set(angle);
		scalarf sin_ = scalar_sin(angle_);
		scalarf cos_ = scalar_cos(angle_);

#if defined(RTM_SSE2_INTRINSICS)
		return _mm_unpacklo_ps(sin_.value, cos_.value);
#elif defined(RTM_NEON_INTRINSICS)
		return vsetq_lane_f32(cos_, vmovq_n_f32(sin_), 1);
#else
		return vector4f{ sin_, cos_, sin_, cos_ };
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns both sine and cosine of the input angle.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline void scalar_sincos(float angle, float& out_sin, float& out_cos) RTM_NO_EXCEPT
	{
		out_sin = scalar_sin(angle);
		out_cos = scalar_cos(angle);
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Returns the arc-sine of the input.
	// Input value must be in the range [-1.0, 1.0].
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline scalarf RTM_SIMD_CALL scalar_asin(scalarf_arg0 value) RTM_NO_EXCEPT
	{
		// Use a degree 7 minimax approximation polynomial
		// See: GPGPU Programming for Games and Science (David H. Eberly)

		// We first calculate our scale: sqrt(1.0 - abs(value))
		const __m128i abs_mask = _mm_set_epi32(0x7FFFFFFFULL, 0x7FFFFFFFULL, 0x7FFFFFFFULL, 0x7FFFFFFFULL);
		__m128 abs_value = _mm_and_ps(value.value, _mm_castsi128_ps(abs_mask));

		// Calculate our value
		const float x = _mm_cvtss_f32(abs_value);
		float result = (x * -1.2690614339589956e-3F) + 6.7072304676685235e-3F;
		result = (result * x) - 1.7162031184398074e-2F;
		result = (result * x) + 3.0961594977611639e-2F;
		result = (result * x) - 5.0207843052845647e-2F;
		result = (result * x) + 8.8986946573346160e-2F;
		result = (result * x) - 2.1459960076929829e-1F;
		result = (result * x) + 1.5707963267948966F;

		// Scale our result
		const __m128 scale = _mm_sqrt_ss(_mm_sub_ss(_mm_set_ps1(1.0F), abs_value));
		result = result * _mm_cvtss_f32(scale);

		// Handle negative values through reflection
		if (_mm_cvtss_f32(value.value) < 0.0F)
			result = constants::pi() - result;

		// Shift our final result
		const float offset = constants::half_pi();
		result = offset - result;
		return scalarf{ _mm_set_ps1(result) };
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Returns the arc-sine of the input.
	// Input value must be in the range [-1.0, 1.0].
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline float scalar_asin(float value) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return scalar_cast(scalar_asin(scalar_set(value)));
#elif defined(RTM_NEON_INTRINSICS)
		return std::asin(value);
#else
		// Use a degree 7 minimax approximation polynomial
		// See: GPGPU Programming for Games and Science (David H. Eberly)

		// We first calculate our scale: sqrt(1.0 - abs(value))
		const float abs_value = scalar_abs(value);

		// Calculate our value
		float result = (abs_value * -1.2690614339589956e-3F) + 6.7072304676685235e-3F;
		result = (result * abs_value) - 1.7162031184398074e-2F;
		result = (result * abs_value) + 3.0961594977611639e-2F;
		result = (result * abs_value) - 5.0207843052845647e-2F;
		result = (result * abs_value) + 8.8986946573346160e-2F;
		result = (result * abs_value) - 2.1459960076929829e-1F;
		result = (result * abs_value) + 1.5707963267948966F;

		// Scale our result
		const float scale = scalar_sqrt(1.0F - abs_value);
		result = result * scale;

		// Handle negative values through reflection
		if (value < 0.0F)
			result = constants::pi() - result;

		// Shift our final result
		const float offset = constants::half_pi();
		result = offset - result;
		return result;
#endif
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Returns the arc-cosine of the input.
	// Input value must be in the range [-1.0, 1.0].
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline scalarf RTM_SIMD_CALL scalar_acos(scalarf_arg0 value) RTM_NO_EXCEPT
	{
		// Use the identity: acos(value) + asin(value) = PI/2
		// This ends up being: acos(value) = PI/2 - asin(value)
		// Since asin(value) = PI/2 - sqrt(1.0 - polynomial(value))
		// Our end result is acos(value) = sqrt(1.0 - polynomial(value))
		// This means we can re-use the same polynomial as asin()
		// See: GPGPU Programming for Games and Science (David H. Eberly)

		// We first calculate our scale: sqrt(1.0 - abs(value))
		const __m128i abs_mask = _mm_set_epi32(0x7FFFFFFFULL, 0x7FFFFFFFULL, 0x7FFFFFFFULL, 0x7FFFFFFFULL);
		__m128 abs_value = _mm_and_ps(value.value, _mm_castsi128_ps(abs_mask));

		// Calculate our value
		const float x = _mm_cvtss_f32(abs_value);
		float result = (x * -1.2690614339589956e-3F) + 6.7072304676685235e-3F;
		result = (result * x) - 1.7162031184398074e-2F;
		result = (result * x) + 3.0961594977611639e-2F;
		result = (result * x) - 5.0207843052845647e-2F;
		result = (result * x) + 8.8986946573346160e-2F;
		result = (result * x) - 2.1459960076929829e-1F;
		result = (result * x) + 1.5707963267948966F;

		// Scale our result
		const __m128 scale = _mm_sqrt_ss(_mm_sub_ss(_mm_set_ps1(1.0F), abs_value));
		result = result * _mm_cvtss_f32(scale);

		// Handle negative values through reflection
		if (_mm_cvtss_f32(value.value) < 0.0F)
			result = constants::pi() - result;

		return scalarf{ _mm_set_ps1(result) };
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Returns the arc-cosine of the input.
	// Input value must be in the range [-1.0, 1.0].
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline float scalar_acos(float value) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return scalar_cast(scalar_acos(scalar_set(value)));
#elif defined(RTM_NEON_INTRINSICS)
		return std::acos(value);
#else
		// Use the identity: acos(value) + asin(value) = PI/2
		// This ends up being: acos(value) = PI/2 - asin(value)
		// Since asin(value) = PI/2 - sqrt(1.0 - polynomial(value))
		// Our end result is acos(value) = sqrt(1.0 - polynomial(value))
		// This means we can re-use the same polynomial as asin()
		// See: GPGPU Programming for Games and Science (David H. Eberly)

		// We first calculate our scale: sqrt(1.0 - abs(value))
		const float abs_value = scalar_abs(value);

		// Calculate our value
		float result = (abs_value * -1.2690614339589956e-3F) + 6.7072304676685235e-3F;
		result = (result * abs_value) - 1.7162031184398074e-2F;
		result = (result * abs_value) + 3.0961594977611639e-2F;
		result = (result * abs_value) - 5.0207843052845647e-2F;
		result = (result * abs_value) + 8.8986946573346160e-2F;
		result = (result * abs_value) - 2.1459960076929829e-1F;
		result = (result * abs_value) + 1.5707963267948966F;

		// Scale our result
		const float scale = scalar_sqrt(1.0F - abs_value);
		result = result * scale;

		// Handle negative values through reflection
		if (value < 0.0F)
			result = constants::pi() - result;

		return result;
#endif
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Returns the tangent of the input angle.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline scalarf RTM_SIMD_CALL scalar_tan(scalarf_arg0 angle) RTM_NO_EXCEPT
	{
		// Use the identity: tan(angle) = sin(angle) / cos(angle)
		scalarf sin_ = scalar_sin(angle);
		scalarf cos_ = scalar_cos(angle);
		if (scalar_cast(cos_) == 0.0F)
			return scalar_set(rtm_impl::copysign(std::numeric_limits<float>::infinity(), scalar_cast(angle)));

		return scalar_div(sin_, cos_);
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Returns the tangent of the input angle.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline float scalar_tan(float angle) RTM_NO_EXCEPT
	{
#if defined(RTM_NEON_INTRINSICS)
		return std::tan(angle);
#else
		// Use the identity: tan(angle) = sin(angle) / cos(angle)
		scalarf angle_ = scalar_set(angle);
		scalarf sin_ = scalar_sin(angle_);
		scalarf cos_ = scalar_cos(angle_);
		if (scalar_cast(cos_) == 0.0F)
			return rtm_impl::copysign(std::numeric_limits<float>::infinity(), angle);

		return scalar_cast(scalar_div(sin_, cos_));
#endif
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Returns the arc-tangent of the input.
	// Note that due to the sign ambiguity, atan cannot determine which quadrant
	// the value resides in. See scalar_atan2.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline scalarf RTM_SIMD_CALL scalar_atan(scalarf_arg0 value) RTM_NO_EXCEPT
	{
		// Use a degree 13 minimax approximation polynomial
		// See: GPGPU Programming for Games and Science (David H. Eberly)

		// Discard our sign, we'll restore it later
		const __m128i abs_mask = _mm_set_epi32(0x7FFFFFFFULL, 0x7FFFFFFFULL, 0x7FFFFFFFULL, 0x7FFFFFFFULL);
		__m128 abs_value = _mm_and_ps(value.value, _mm_castsi128_ps(abs_mask));

		// Compute our value
		const __m128 one = _mm_set_ps1(1.0F);
		__m128 is_larger_than_one = _mm_cmpgt_ss(abs_value, one);
		__m128 reciprocal = _mm_div_ss(one, abs_value);

		__m128 x = RTM_VECTOR4F_SELECT(is_larger_than_one, reciprocal, abs_value);

		float x_s = _mm_cvtss_f32(x);
		float x2 = x_s * x_s;

		float result = (x2 * 7.2128853633444123e-3F) - 3.5059680836411644e-2F;
		result = (result * x2) + 8.1675882859940430e-2F;
		result = (result * x2) - 1.3374657325451267e-1F;
		result = (result * x2) + 1.9856563505717162e-1F;
		result = (result * x2) - 3.3324998579202170e-1F;
		result = (result * x2) + 1.0F;
		result = result * x_s;

		__m128 result_s = _mm_set_ps1(result);
		__m128 remapped = _mm_sub_ss(_mm_set_ps1(constants::half_pi()), result_s);

		// pi/2 - result
		result_s = RTM_VECTOR4F_SELECT(is_larger_than_one, remapped, result_s);

		// Keep the original sign
		result_s = _mm_or_ps(result_s, _mm_and_ps(value.value, _mm_set_ps1(-0.0F)));

		return scalarf{ result_s };
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Returns the arc-tangent of the input.
	// Note that due to the sign ambiguity, atan cannot determine which quadrant
	// the value resides in. See scalar_atan2.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline float RTM_SIMD_CALL scalar_atan(float value) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return scalar_cast(scalar_atan(scalar_set(value)));
#elif defined(RTM_NEON_INTRINSICS)
		return std::atan(value);
#else
		// Use a degree 13 minimax approximation polynomial
		// See: GPGPU Programming for Games and Science (David H. Eberly)

		// Discard our sign, we'll restore it later
		float abs_value = scalar_abs(value);

		// Compute our value
		float x = abs_value > 1.0F ? scalar_reciprocal(abs_value) : abs_value;
		float x2 = x * x;

		float result = (x2 * 7.2128853633444123e-3F) - 3.5059680836411644e-2F;
		result = (result * x2) + 8.1675882859940430e-2F;
		result = (result * x2) - 1.3374657325451267e-1F;
		result = (result * x2) + 1.9856563505717162e-1F;
		result = (result * x2) - 3.3324998579202170e-1F;
		result = (result * x2) + 1.0F;
		result = result * x;

		if (abs_value > 1.0f)
			result = constants::half_pi() - result; // pi/2 - result

		// Keep the original sign
		result = value >= 0.0F ? result : -result;

		return result;
#endif
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Returns the arc-tangent of [y/x] using the sign of the arguments to
	// determine the correct quadrant.
	// Y represents the proportion of the y-coordinate.
	// X represents the proportion of the x-coordinate.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline scalarf RTM_SIMD_CALL scalar_atan2(scalarf y, scalarf x) RTM_NO_EXCEPT
	{
		// If X == 0.0 and Y != 0.0, we return PI/2 with the sign of Y
		// If X == 0.0 and Y == 0.0, we return 0.0
		// If X > 0.0, we return atan(y/x)
		// If X < 0.0, we return atan(y/x) + sign(Y) * PI
		// See: https://en.wikipedia.org/wiki/Atan2#Definition_and_computation

		const __m128 y_value = y.value;
		const __m128 x_value = x.value;

		const __m128 zero = _mm_setzero_ps();
		__m128 is_x_zero = _mm_cmpeq_ss(x_value, zero);
		__m128 is_y_zero = _mm_cmpeq_ss(y_value, zero);
		__m128 inputs_are_zero = _mm_and_ps(is_x_zero, is_y_zero);

		__m128 is_x_positive = _mm_cmpgt_ss(x_value, zero);

		const __m128 sign_mask = _mm_set_ps(-0.0F, -0.0F, -0.0F, -0.0F);
		__m128 y_sign = _mm_and_ps(y_value, sign_mask);

		// If X == 0.0, our offset is PI/2 otherwise it is PI both with the sign of Y
		__m128 half_pi = _mm_set_ps1(constants::half_pi());
		__m128 pi = _mm_set_ps1(constants::pi());
		__m128 offset = _mm_or_ps(_mm_and_ps(is_x_zero, half_pi), _mm_andnot_ps(is_x_zero, pi));
		offset = _mm_or_ps(offset, y_sign);

		// If X > 0.0, our offset is 0.0
		offset = _mm_andnot_ps(is_x_positive, offset);

		// If X == 0.0 and Y == 0.0, our offset is 0.0
		offset = _mm_andnot_ps(inputs_are_zero, offset);

		__m128 angle = _mm_div_ss(y_value, x_value);
		scalarf angle_s = scalarf{ angle };
		scalarf value_s = scalar_atan(angle_s);
		__m128 value = value_s.value;

		// If X == 0.0, our value is 0.0 otherwise it is atan(y/x)
		value = _mm_or_ps(_mm_and_ps(is_x_zero, zero), _mm_andnot_ps(is_x_zero, value));

		// If X == 0.0 and Y == 0.0, our value is 0.0
		value = _mm_andnot_ps(inputs_are_zero, value);

		__m128 result = _mm_add_ss(value, offset);
		return scalarf{ result };
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Returns the arc-tangent of [y/x] using the sign of the arguments to
	// determine the correct quadrant.
	// Y represents the proportion of the y-coordinate.
	// X represents the proportion of the x-coordinate.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline float scalar_atan2(float y, float x) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return scalar_cast(scalar_atan2(scalar_set(y), scalar_set(x)));
#elif defined(RTM_NEON_INTRINSICS)
		return std::atan2(y, x);
#else
		// If X == 0.0 and Y != 0.0, we return PI/2 with the sign of Y
		// If X == 0.0 and Y == 0.0, we return 0.0
		// If X > 0.0, we return atan(y/x)
		// If X < 0.0, we return atan(y/x) + sign(Y) * PI
		// See: https://en.wikipedia.org/wiki/Atan2#Definition_and_computation

		if (x == 0.0F)
		{
			if (y == 0.0F)
				return 0.0F;

			return rtm_impl::copysign(constants::half_pi(), y);
		}

		float value = scalar_atan(y / x);
		if (x > 0.0F)
			return value;

		float offset = rtm_impl::copysign(constants::pi(), y);
		return value + offset;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Converts degrees into radians.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr float scalar_deg_to_rad(float deg) RTM_NO_EXCEPT
	{
		return deg * constants::pi_div_one_eighty();
	}

	//////////////////////////////////////////////////////////////////////////
	// Converts radians into degrees.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr float scalar_rad_to_deg(float rad) RTM_NO_EXCEPT
	{
		return rad * constants::one_eighty_div_pi();
	}

	RTM_IMPL_VERSION_NAMESPACE_END
}

RTM_IMPL_FILE_PRAGMA_POP

```

`includes/rtm/type_traits.h`:

```h
#pragma once

////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2018 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include "rtm/types.h"
#include "rtm/version.h"
#include "rtm/impl/error.h"

namespace rtm
{
	RTM_IMPL_VERSION_NAMESPACE_BEGIN

#if defined(RTM_COMPILER_GCC)
	#pragma GCC diagnostic push
#if __GNUC__ > 5
	// GCC complains that the alignment attribute is ignored, we don't care with type traits
	#pragma GCC diagnostic ignored "-Wignored-attributes"
#endif
	// GCC complains that the type is deprecated when we specialize it, we don't care
	#pragma GCC diagnostic ignored "-Wdeprecated-declarations"
#endif

	//////////////////////////////////////////////////////////////////////////
	// Returns the proper types for a floating point type.
	//////////////////////////////////////////////////////////////////////////
	template<typename float_type>
	struct RTM_DEPRECATED("Use 'related_types' instead. To be removed in 2.4") float_traits {};

	template<>
	struct RTM_DEPRECATED("Use 'related_types' instead. To be removed in 2.4") float_traits<float>
	{
		using mask4 = mask4f;

		using scalar = scalarf;
		using vector4 = vector4f;
		using quat = quatf;

		using qv = qvf;
		using qvs = qvsf;
		using qvv = qvvf;

		using matrix3x3 = matrix3x3f;
		using matrix3x4 = matrix3x4f;
		using matrix4x4 = matrix4x4f;

		using float1 = float;
		using float2 = float2f;
		using float3 = float3f;
		using float4 = float4f;

		using int1 = uint32_t;
	};

	template<>
	struct RTM_DEPRECATED("Use 'related_types' instead. To be removed in 2.4") float_traits<double>
	{
		using mask4 = mask4d;

		using scalar = scalard;
		using vector4 = vector4d;
		using quat = quatd;

		using qv = qvd;
		using qvs = qvsd;
		using qvv = qvvd;

		using matrix3x3 = matrix3x3d;
		using matrix3x4 = matrix3x4d;
		using matrix4x4 = matrix4x4d;

		using float1 = double;
		using float2 = float2d;
		using float3 = float3d;
		using float4 = float4d;

		using int1 = uint64_t;
	};

	//////////////////////////////////////////////////////////////////////////
	// Specifies the related types given a source type.
	//////////////////////////////////////////////////////////////////////////
	template<typename source_type>
	struct related_types {};

	template<>
	struct related_types<float>
	{
		using mask4 = mask4f;

		using scalar = scalarf;
		using vector4 = vector4f;
		using quat = quatf;

		using qv = qvf;
		using qvs = qvsf;
		using qvv = qvvf;

		using matrix3x3 = matrix3x3f;
		using matrix3x4 = matrix3x4f;
		using matrix4x4 = matrix4x4f;

		using float1 = float;
		using float2 = float2f;
		using float3 = float3f;
		using float4 = float4f;

		using int1 = uint32_t;
	};

	template<>
	struct related_types<double>
	{
		using mask4 = mask4d;

		using scalar = scalard;
		using vector4 = vector4d;
		using quat = quatd;

		using qv = qvd;
		using qvs = qvsd;
		using qvv = qvvd;

		using matrix3x3 = matrix3x3d;
		using matrix3x4 = matrix3x4d;
		using matrix4x4 = matrix4x4d;

		using float1 = double;
		using float2 = float2d;
		using float3 = float3d;
		using float4 = float4d;

		using int1 = uint64_t;
	};

	// Alias all related float types
	template<> struct related_types<vector4f> : related_types<float> {};
	template<> struct related_types<qvf> : related_types<float> {};
	template<> struct related_types<qvsf> : related_types<float> {};
	template<> struct related_types<qvvf> : related_types<float> {};
	template<> struct related_types<matrix3x3f> : related_types<float> {};
	template<> struct related_types<matrix3x4f> : related_types<float> {};
	template<> struct related_types<matrix4x4f> : related_types<float> {};
	template<> struct related_types<float2f> : related_types<float> {};
	template<> struct related_types<float3f> : related_types<float> {};
	template<> struct related_types<float4f> : related_types<float> {};

#if defined(RTM_SSE2_INTRINSICS)
	// mask4f is an alias of vector4f
	// quatf is an alias of vector4f
	template<> struct related_types<scalarf> : related_types<float> {};
#elif defined(RTM_NEON_INTRINSICS)
	// scalarf is an alias of float
	// quatf is an alias of vector4f
#if !defined(RTM_COMPILER_MSVC)
	// MSVC aliases uint32x4_t and float32x4_t
	template<> struct related_types<mask4f> : related_types<float> {};
#endif
#else
	// scalarf is an alias of float
	template<> struct related_types<mask4f> : related_types<float> {};
	template<> struct related_types<quatf> : related_types<float> {};
#endif

	// Alias all related double types
	template<> struct related_types<mask4d> : related_types<double> {};
	template<> struct related_types<vector4d> : related_types<double> {};
	template<> struct related_types<quatd> : related_types<double> {};
	template<> struct related_types<qvd> : related_types<double> {};
	template<> struct related_types<qvsd> : related_types<double> {};
	template<> struct related_types<qvvd> : related_types<double> {};
	template<> struct related_types<matrix3x3d> : related_types<double> {};
	template<> struct related_types<matrix3x4d> : related_types<double> {};
	template<> struct related_types<matrix4x4d> : related_types<double> {};
	template<> struct related_types<float2d> : related_types<double> {};
	template<> struct related_types<float3d> : related_types<double> {};
	template<> struct related_types<float4d> : related_types<double> {};

#if defined(RTM_SSE2_INTRINSICS)
	template<> struct related_types<scalard> : related_types<double> {};
#elif defined(RTM_NEON_INTRINSICS)
	// scalard is an alias of double
#else
	// scalard is an alias of double
#endif

#if defined(RTM_COMPILER_GCC)
	#pragma GCC diagnostic pop
#endif

	RTM_IMPL_VERSION_NAMESPACE_END
}

```

`includes/rtm/types.h`:

```h
#pragma once

////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2017 Nicholas Frechette & Animation Compression Library contributors
// Copyright (c) 2018 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include "rtm/math.h"
#include "rtm/version.h"

#include <cstdint>

namespace rtm
{
	RTM_IMPL_VERSION_NAMESPACE_BEGIN

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// A quaternion (4D complex number) where the imaginary part is the [w] component.
	// It accurately represents a 3D rotation with no gimbal lock as long as it is kept normalized.
	//////////////////////////////////////////////////////////////////////////
	using quatf = __m128;

	//////////////////////////////////////////////////////////////////////////
	// A quaternion (4D complex number) where the imaginary part is the [w] component.
	// It accurately represents a 3D rotation with no gimbal lock as long as it is kept normalized.
	//////////////////////////////////////////////////////////////////////////
	struct quatd
	{
		__m128d xy;
		__m128d zw;
	};

	//////////////////////////////////////////////////////////////////////////
	// A 4D vector.
	//////////////////////////////////////////////////////////////////////////
	using vector4f = __m128;

	//////////////////////////////////////////////////////////////////////////
	// A 4D vector.
	//////////////////////////////////////////////////////////////////////////
	struct vector4d
	{
		__m128d xy;
		__m128d zw;
	};

	//////////////////////////////////////////////////////////////////////////
	// A 4x32 bit vector comparison mask for 32 bit floats: ~0 if true, 0 otherwise.
	//////////////////////////////////////////////////////////////////////////
	using mask4f = __m128;

	//////////////////////////////////////////////////////////////////////////
	// A 4x64 bit vector comparison mask for 64 bit floats: ~0 if true, 0 otherwise.
	//////////////////////////////////////////////////////////////////////////
	struct mask4d
	{
		__m128d xy;
		__m128d zw;
	};

	//////////////////////////////////////////////////////////////////////////
	// A 4x32 bit vector comparison mask: ~0 if true, 0 otherwise.
	//////////////////////////////////////////////////////////////////////////
	using mask4i = __m128i;

	//////////////////////////////////////////////////////////////////////////
	// A 4x64 bit vector comparison mask: ~0 if true, 0 otherwise.
	//////////////////////////////////////////////////////////////////////////
	struct mask4q
	{
		__m128i xy;
		__m128i zw;
	};
#elif defined(RTM_NEON_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// A quaternion (4D complex number) where the imaginary part is the [w] component.
	// It accurately represents a 3D rotation with no gimbal lock as long as it is kept normalized.
	//////////////////////////////////////////////////////////////////////////
	using quatf = float32x4_t;

	//////////////////////////////////////////////////////////////////////////
	// A quaternion (4D complex number) where the imaginary part is the [w] component.
	// It accurately represents a 3D rotation with no gimbal lock as long as it is kept normalized.
	//////////////////////////////////////////////////////////////////////////
	struct alignas(16) quatd
	{
		double x;
		double y;
		double z;
		double w;
	};

	//////////////////////////////////////////////////////////////////////////
	// A 4D vector.
	//////////////////////////////////////////////////////////////////////////
	using vector4f = float32x4_t;

	//////////////////////////////////////////////////////////////////////////
	// A 4D vector.
	//////////////////////////////////////////////////////////////////////////
	struct alignas(16) vector4d
	{
		double x;
		double y;
		double z;
		double w;
	};

	//////////////////////////////////////////////////////////////////////////
	// A 4x32 bit vector comparison mask for 32 bit floats: ~0 if true, 0 otherwise.
	//////////////////////////////////////////////////////////////////////////
	using mask4f = uint32x4_t;

	//////////////////////////////////////////////////////////////////////////
	// A 4x64 bit vector comparison mask for 64 bit floats: ~0 if true, 0 otherwise.
	//////////////////////////////////////////////////////////////////////////
	struct alignas(16) mask4d
	{
		uint64_t x;
		uint64_t y;
		uint64_t z;
		uint64_t w;
	};

	// MSVC uses a simple typedef to an identical underlying type for uint32x4_t and float32x4_t
	// We also want two different types for mask4f and mask4i and mask4f is more commonly used
	// To avoid issues of duplicate symbols, we introduce a concrete type for mask4i

	//////////////////////////////////////////////////////////////////////////
	// A 4x32 bit vector comparison mask for 32 bit integers: ~0 if true, 0 otherwise.
	//////////////////////////////////////////////////////////////////////////
	struct alignas(16) mask4i
	{
		uint32x4_t value;
	};

	// Helper macros to simplify usage
	#define RTM_IMPL_MASK4i_GET(mask) mask.value
	#define RTM_IMPL_MASK4i_SET(mask) mask4i{ mask }

	//////////////////////////////////////////////////////////////////////////
	// A 4x64 bit vector comparison mask for 64 bit integers: ~0 if true, 0 otherwise.
	//////////////////////////////////////////////////////////////////////////
	struct alignas(16) mask4q
	{
		uint64_t x;
		uint64_t y;
		uint64_t z;
		uint64_t w;
	};
#else
	//////////////////////////////////////////////////////////////////////////
	// A quaternion (4D complex number) where the imaginary part is the [w] component.
	// It accurately represents a 3D rotation with no gimbal lock as long as it is kept normalized.
	//////////////////////////////////////////////////////////////////////////
	struct alignas(16) quatf
	{
		float x;
		float y;
		float z;
		float w;
	};

	//////////////////////////////////////////////////////////////////////////
	// A quaternion (4D complex number) where the imaginary part is the [w] component.
	// It accurately represents a 3D rotation with no gimbal lock as long as it is kept normalized.
	//////////////////////////////////////////////////////////////////////////
	struct alignas(16) quatd
	{
		double x;
		double y;
		double z;
		double w;
	};

	//////////////////////////////////////////////////////////////////////////
	// A 4D vector.
	//////////////////////////////////////////////////////////////////////////
	struct alignas(16) vector4f
	{
		float x;
		float y;
		float z;
		float w;
	};

	//////////////////////////////////////////////////////////////////////////
	// A 4D vector.
	//////////////////////////////////////////////////////////////////////////
	struct alignas(16) vector4d
	{
		double x;
		double y;
		double z;
		double w;
	};

	//////////////////////////////////////////////////////////////////////////
	// A 4x32 bit vector comparison mask for 32 bit floats: ~0 if true, 0 otherwise.
	//////////////////////////////////////////////////////////////////////////
	struct alignas(16) mask4f
	{
		uint32_t x;
		uint32_t y;
		uint32_t z;
		uint32_t w;
	};

	//////////////////////////////////////////////////////////////////////////
	// A 4x64 bit vector comparison mask for 64 bit floats: ~0 if true, 0 otherwise.
	//////////////////////////////////////////////////////////////////////////
	struct alignas(16) mask4d
	{
		uint64_t x;
		uint64_t y;
		uint64_t z;
		uint64_t w;
	};

	//////////////////////////////////////////////////////////////////////////
	// A 4x32 bit vector comparison mask: ~0 if true, 0 otherwise.
	//////////////////////////////////////////////////////////////////////////
	struct alignas(16) mask4i
	{
		uint32_t x;
		uint32_t y;
		uint32_t z;
		uint32_t w;
	};

	//////////////////////////////////////////////////////////////////////////
	// A 4x64 bit vector comparison mask: ~0 if true, 0 otherwise.
	//////////////////////////////////////////////////////////////////////////
	struct alignas(16) mask4q
	{
		uint64_t x;
		uint64_t y;
		uint64_t z;
		uint64_t w;
	};
#endif

#if defined(RTM_SSE2_INTRINSICS)
	// With SSE2, we use a concrete type for scalarf/scalard unlike other platforms and other types
	// like vector4f and quatf. We don't use a concrete type when we can avoid it to help the compiler
	// optimize as much as possible. But we must be able to tell a scalar apart from a vector for
	// return type overloading and argument overloading.
	// For example, we want to support vector_mul(vec4, vec4) and vector_mul(vec4, scalar).
	// When scalarf is a 'float', the type is distinct and everything works as expected
	// but if we use __m128, the type is the same as vector4f and we won't be able to tell
	// them apart.
	// Another example is vector_dot where we want to support returning a float, a scalarf, and
	// a vector4f depending on what the user expects. We could always return a float/scalarf but
	// if we need a vector4f it is less efficient if _mm_dp_ps is used: we would have an extra
	// shuffle.
	// Using a concrete type here allows us to tell the types apart and properly overload them
	// when required. The compiler should still be able to optimize properly.

	//////////////////////////////////////////////////////////////////////////
	// A SIMD friendly scalar type. Different architectures have an easier or harder time
	// working with scalar floating point numbers. For example, older PowerPC processors
	// had to write to memory and reload from it to transfer from one register file into
	// another (e.g convert from a float to a SIMD vector). Modern processors handle
	// this much better but inefficiencies remain, especially with SSE. While it is
	// free to convert a SIMD scalar into a float with _mm_cvtss_f32(..) the reverse generally
	// requires the compiler to fill the unused SIMD lanes with known values (either zero or the same).
	// This introduces an extra instruction that isn't always required when only the first lane is used
	// such as with scalar_sqrt_reciprocal(..). By introducing a type for SIMD scalar values,
	// each platform is free to make an optimal choice.
	//////////////////////////////////////////////////////////////////////////
	struct scalarf
	{
		__m128 value;
	};

	//////////////////////////////////////////////////////////////////////////
	// A SIMD friendly scalar type. Different architectures have an easier or harder time
	// working with scalar floating point numbers. For example, older PowerPC processors
	// had to write to memory and reload from it to transfer from one register file into
	// another (e.g convert from a float to a SIMD vector). Modern processors handle
	// this much better but inefficiencies remain, especially with SSE. While it is
	// free to convert a SIMD scalar into a float with _mm_cvtss_f32(..) the reverse generally
	// requires the compiler to fill the unused SIMD lanes with known values (either zero or the same).
	// This introduces an extra instruction that isn't always required when only the first lane is used
	// such as with scalar_sqrt_reciprocal(..). By introducing a type for SIMD scalar values,
	// each platform is free to make an optimal choice.
	//////////////////////////////////////////////////////////////////////////
	struct scalard
	{
		__m128d value;
	};
#else
	//////////////////////////////////////////////////////////////////////////
	// A SIMD friendly scalar type. Different architectures have an easier or harder time
	// working with scalar floating point numbers. For example, older PowerPC processors
	// had to write to memory and reload from it to transfer from one register file into
	// another (e.g convert from a float to a SIMD vector). Modern processors handle
	// this much better but inefficiencies remain, especially with SSE. While it is
	// free to convert a SIMD scalar into a float with _mm_cvtss_f32(..) the reverse generally
	// requires the compiler to fill the unused SIMD lanes with known values (either zero or the same).
	// This introduces an extra instruction that isn't always required when only the first lane is used
	// such as with scalar_sqrt_reciprocal(..). By introducing a type for SIMD scalar values,
	// each platform is free to make an optimal choice.
	//////////////////////////////////////////////////////////////////////////
	using scalarf = float;

	//////////////////////////////////////////////////////////////////////////
	// A SIMD friendly scalar type. Different architectures have an easier or harder time
	// working with scalar floating point numbers. For example, older PowerPC processors
	// had to write to memory and reload from it to transfer from one register file into
	// another (e.g convert from a float to a SIMD vector). Modern processors handle
	// this much better but inefficiencies remain, especially with SSE. While it is
	// free to convert a SIMD scalar into a float with _mm_cvtss_f32(..) the reverse generally
	// requires the compiler to fill the unused SIMD lanes with known values (either zero or the same).
	// This introduces an extra instruction that isn't always required when only the first lane is used
	// such as with scalar_sqrt_reciprocal(..). By introducing a type for SIMD scalar values,
	// each platform is free to make an optimal choice.
	//////////////////////////////////////////////////////////////////////////
	using scalard = double;
#endif

	//////////////////////////////////////////////////////////////////////////
	// A QV transform represents a 3D rotation (quaternion) and a 3D translation (vector).
	// QV transforms form a group with a well defined multiplication and inverse.
	//////////////////////////////////////////////////////////////////////////
	struct qvf
	{
		quatf		rotation;
		vector4f	translation;	// [w] is undefined
	};

	//////////////////////////////////////////////////////////////////////////
	// A QV transform represents a 3D rotation (quaternion) and a 3D translation (vector).
	// QV transforms form a group with a well defined multiplication and inverse.
	//////////////////////////////////////////////////////////////////////////
	struct qvd
	{
		quatd		rotation;
		vector4d	translation;	// [w] is undefined
	};

	//////////////////////////////////////////////////////////////////////////
	// A QVS transform represents a 3D rotation (quaternion), a 3D translation (vector),
	// and a single scalar uniform scale value.
	// QVS transforms form a group with a well defined multiplication and inverse.
	//////////////////////////////////////////////////////////////////////////
	struct qvsf
	{
		quatf		rotation;
		vector4f	translation_scale;	// [xyz] for translation, [w] for scale
	};

	//////////////////////////////////////////////////////////////////////////
	// A QVS transform represents a 3D rotation (quaternion), a 3D translation (vector),
	// and a single scalar uniform scale value.
	// QVS transforms form a group with a well defined multiplication and inverse.
	//////////////////////////////////////////////////////////////////////////
	struct qvsd
	{
		quatd		rotation;
		vector4d	translation_scale;	// [xyz] for translation, [w] for scale
	};

	//////////////////////////////////////////////////////////////////////////
	// A QVV transform represents a 3D rotation (quaternion), 3D translation (vector), and 3D non-uniform scale (vector).
	// WARNING: QVV transforms do _NOT_ form a group. It has no well defined multiplication and inverse.
	// This is because when non-uniform scale is rotated (e.g. as the result of two QVV multiplying),
	// shear is introduced and there is no place to store it. As such, QVV discards the introduced shear
	// which is mathematically incorrect. As a result of this, QVV multiplication is _NOT_ assossiative and
	// the inverse is incorrect. USE AT YOUR OWN RISK.
	//////////////////////////////////////////////////////////////////////////
	struct qvvf
	{
		quatf		rotation;
		vector4f	translation;	// [w] is undefined
		vector4f	scale;			// [w] is undefined
	};

	//////////////////////////////////////////////////////////////////////////
	// A QVV transform represents a 3D rotation (quaternion), 3D translation (vector), and 3D non-uniform scale (vector).
	// WARNING: QVV transforms do _NOT_ form a group. It has no well defined multiplication and inverse.
	// This is because when non-uniform scale is rotated (e.g. as the result of two QVV multiplying),
	// shear is introduced and there is no place to store it. As such, QVV discards the introduced shear
	// which is mathematically incorrect. As a result of this, QVV multiplication is _NOT_ assossiative and
	// the inverse is incorrect. USE AT YOUR OWN RISK.
	//////////////////////////////////////////////////////////////////////////
	struct qvvd
	{
		quatd		rotation;
		vector4d	translation;	// [w] is undefined
		vector4d	scale;			// [w] is undefined
	};

	//////////////////////////////////////////////////////////////////////////
	// A generic 3x3 matrix.
	// Note: The [w] component of every column vector is undefined.
	//////////////////////////////////////////////////////////////////////////
	struct matrix3x3f
	{
		vector4f	x_axis;
		vector4f	y_axis;
		vector4f	z_axis;
	};

	//////////////////////////////////////////////////////////////////////////
	// A generic 3x3 matrix.
	// Note: The [w] component of every column vector is undefined.
	//////////////////////////////////////////////////////////////////////////
	struct matrix3x3d
	{
		vector4d	x_axis;
		vector4d	y_axis;
		vector4d	z_axis;
	};

	//////////////////////////////////////////////////////////////////////////
	// An 3x4 affine matrix represents a 3D rotation, 3D translation, and 3D scale.
	// It properly deals with skew/shear when present but once scale with mirroring is combined,
	// it cannot be safely extracted back.
	//
	// Affine matrices are 4x4 but have their last row always equal to [0, 0, 0, 1] which is why it is 3x4.
	// Note: We do not track the implicit last row and it is thus undefined.
	//
	// Left handed coordinate system:
	// X axis == forward
	// Y axis == right
	// Z axis == up
	//////////////////////////////////////////////////////////////////////////
	struct matrix3x4f
	{
		vector4f	x_axis;
		vector4f	y_axis;
		vector4f	z_axis;
		vector4f	w_axis;
	};

	//////////////////////////////////////////////////////////////////////////
	// An 3x4 affine matrix represents a 3D rotation, 3D translation, and 3D scale.
	// It properly deals with skew/shear when present but once scale with mirroring is combined,
	// it cannot be safely extracted back.
	//
	// Affine matrices are 4x4 but have their last row always equal to [0, 0, 0, 1] which is why it is 3x4.
	// Note: We do not track the implicit last row and it is thus undefined.
	//
	// Left handed coordinate system:
	// X axis == forward
	// Y axis == right
	// Z axis == up
	//////////////////////////////////////////////////////////////////////////
	struct matrix3x4d
	{
		vector4d	x_axis;
		vector4d	y_axis;
		vector4d	z_axis;
		vector4d	w_axis;
	};

	//////////////////////////////////////////////////////////////////////////
	// A generic 4x4 matrix.
	//////////////////////////////////////////////////////////////////////////
	struct matrix4x4f
	{
		vector4f	x_axis;
		vector4f	y_axis;
		vector4f	z_axis;
		vector4f	w_axis;
	};

	//////////////////////////////////////////////////////////////////////////
	// A generic 4x4 matrix.
	//////////////////////////////////////////////////////////////////////////
	struct matrix4x4d
	{
		vector4d	x_axis;
		vector4d	y_axis;
		vector4d	z_axis;
		vector4d	w_axis;
	};

	//////////////////////////////////////////////////////////////////////////
	// Represents a component when mixing/shuffling/permuting vectors.
	// [xyzw] are used to refer to the first input while [abcd] refer to the second input.
	//////////////////////////////////////////////////////////////////////////
	enum class mix4
	{
		x = 0,
		y = 1,
		z = 2,
		w = 3,

		a = 4,
		b = 5,
		c = 6,
		d = 7,
	};

	//////////////////////////////////////////////////////////////////////////
	// Represents an axis in 3D.
	//////////////////////////////////////////////////////////////////////////
	enum class axis3
	{
		x = 0,
		y = 1,
		z = 2,
	};

	//////////////////////////////////////////////////////////////////////////
	// Represents an axis in 4D.
	//////////////////////////////////////////////////////////////////////////
	enum class axis4
	{
		x = 0,
		y = 1,
		z = 2,
		w = 3,
	};

	//////////////////////////////////////////////////////////////////////////
	// Represents 2D vector components.
	//////////////////////////////////////////////////////////////////////////
	enum class component2
	{
		x = 0,
		y = 1,
	};

	//////////////////////////////////////////////////////////////////////////
	// Represents 3D vector components.
	//////////////////////////////////////////////////////////////////////////
	enum class component3
	{
		x = 0,
		y = 1,
		z = 2,
	};

	//////////////////////////////////////////////////////////////////////////
	// Represents 4D vector components.
	//////////////////////////////////////////////////////////////////////////
	enum class component4
	{
		x = 0,
		y = 1,
		z = 2,
		w = 3,
	};


	//////////////////////////////////////////////////////////////////////////
	// Various unaligned types suitable for interop. with GPUs, etc.
	//////////////////////////////////////////////////////////////////////////


	struct float2f
	{
		float x;
		float y;
	};

	struct float3f
	{
		float x;
		float y;
		float z;
	};

	struct float4f
	{
		float x;
		float y;
		float z;
		float w;
	};

	struct float2d
	{
		double x;
		double y;
	};

	struct float3d
	{
		double x;
		double y;
		double z;
	};

	struct float4d
	{
		double x;
		double y;
		double z;
		double w;
	};

	RTM_IMPL_VERSION_NAMESPACE_END
}

// Always include the register passing typedefs
#include "rtm/impl/type_args.h"

```

`includes/rtm/vector4d.h`:

```h
#pragma once

////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2017 Nicholas Frechette & Animation Compression Library contributors
// Copyright (c) 2018 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include "rtm/math.h"
#include "rtm/scalard.h"
#include "rtm/version.h"
#include "rtm/impl/bit_cast.impl.h"
#include "rtm/impl/compiler_utils.h"
#include "rtm/impl/memory_utils.h"
#include "rtm/impl/vector_common.h"

#include <cstring>
#include <limits>

RTM_IMPL_FILE_PRAGMA_PUSH

namespace rtm
{
	RTM_IMPL_VERSION_NAMESPACE_BEGIN

	//////////////////////////////////////////////////////////////////////////
	// Setters, getters, and casts
	//////////////////////////////////////////////////////////////////////////


	//////////////////////////////////////////////////////////////////////////
	// Loads an unaligned vector4 from memory.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4d RTM_SIMD_CALL vector_load(const double* input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return vector4d{ _mm_loadu_pd(input), _mm_loadu_pd(input + 2) };
#else
		return vector_set(input[0], input[1], input[2], input[3]);
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Loads an input scalar from memory into the [x] component and sets the [yzw] components to zero.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4d RTM_SIMD_CALL vector_load1(const double* input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		const __m128d zero = _mm_setzero_pd();
		return vector4d{ _mm_loadl_pd(zero, input), zero };
#else
		return vector_set(input[0], 0.0, 0.0, 0.0);
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Loads an unaligned vector2 from memory and sets the [zw] components to zero.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4d RTM_SIMD_CALL vector_load2(const double* input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return vector4d{ _mm_loadu_pd(input), _mm_setzero_pd() };
#else
		return vector_set(input[0], input[1], 0.0, 0.0);
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Loads an unaligned vector3 from memory and sets the [w] component to zero.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4d RTM_SIMD_CALL vector_load3(const double* input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		const __m128d zero = _mm_setzero_pd();
		return vector4d{ _mm_loadu_pd(input), _mm_loadl_pd(zero, input + 2) };
#else
		return vector_set(input[0], input[1], input[2], 0.0);
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Loads an unaligned vector4 from memory.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4d RTM_SIMD_CALL vector_load(const float4d* input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return vector4d{ _mm_loadu_pd(&input->x), _mm_loadu_pd(&input->z) };
#else
		return vector_set(input->x, input->y, input->z, input->w);
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Loads an unaligned vector2 from memory and sets the [zw] components to zero.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4d RTM_SIMD_CALL vector_load2(const float2d* input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return vector4d{ _mm_loadu_pd(&input->x), _mm_setzero_pd() };
#else
		return vector_set(input->x, input->y, 0.0, 0.0);
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Loads an unaligned vector3 from memory and sets the [w] component to zero.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4d RTM_SIMD_CALL vector_load3(const float3d* input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		const __m128d zero = _mm_setzero_pd();
		return vector4d{ _mm_loadu_pd(&input->x), _mm_loadl_pd(zero, &input->z) };
#else
		return vector_set(input->x, input->y, input->z, 0.0);
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Loads an input scalar from memory into the [xyzw] components.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4d RTM_SIMD_CALL vector_broadcast(const double* input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		const __m128d value = _mm_load1_pd(input);
		return vector4d{ value, value };
#else
		return vector_set(*input);
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Casts a quaternion to a vector4.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4d RTM_SIMD_CALL quat_to_vector(quatd_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return vector4d{ input.xy, input.zw };
#else
		return vector4d{ input.x, input.y, input.z, input.w };
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Casts a vector4 float32 variant to a float64 variant.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4d RTM_SIMD_CALL vector_cast(vector4f_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return vector4d{ _mm_cvtps_pd(input), _mm_cvtps_pd(_mm_shuffle_ps(input, input, _MM_SHUFFLE(3, 2, 3, 2))) };
#elif defined(RTM_NEON_INTRINSICS)
		return vector4d{ double(vgetq_lane_f32(input, 0)), double(vgetq_lane_f32(input, 1)), double(vgetq_lane_f32(input, 2)), double(vgetq_lane_f32(input, 3)) };
#else
		return vector4d{ double(input.x), double(input.y), double(input.z), double(input.w) };
#endif
	}

	namespace rtm_impl
	{
		//////////////////////////////////////////////////////////////////////////
		// This is a helper struct to allow a single consistent API between
		// various vector types when the semantics are identical but the return
		// type differs. Implicit coercion is used to return the desired value
		// at the call site.
		//////////////////////////////////////////////////////////////////////////
		struct vector4d_vector_get_x
		{
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator double() const RTM_NO_EXCEPT
			{
#if defined(RTM_SSE2_INTRINSICS)
				return _mm_cvtsd_f64(input.xy);
#else
				return input.x;
#endif
			}

#if defined(RTM_SSE2_INTRINSICS)
			RTM_DEPRECATED("Use 'as_scalar' suffix instead. To be removed in 2.4.")
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator scalard() const RTM_NO_EXCEPT
			{
				return scalard{ input.xy };
			}
#endif

			vector4d input;
		};
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the vector4 [x] component.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr rtm_impl::vector4d_vector_get_x RTM_SIMD_CALL vector_get_x(vector4d_arg0 input) RTM_NO_EXCEPT
	{
		return rtm_impl::vector4d_vector_get_x{ input };
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the vector4 [x] component.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE scalard RTM_SIMD_CALL vector_get_x_as_scalar(vector4d_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return scalard{ input.xy };
#else
		return vector_get_x(input);
#endif
	}

	namespace rtm_impl
	{
		//////////////////////////////////////////////////////////////////////////
		// This is a helper struct to allow a single consistent API between
		// various vector types when the semantics are identical but the return
		// type differs. Implicit coercion is used to return the desired value
		// at the call site.
		//////////////////////////////////////////////////////////////////////////
		struct vector4d_vector_get_y
		{
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator double() const RTM_NO_EXCEPT
			{
#if defined(RTM_SSE2_INTRINSICS)
				return _mm_cvtsd_f64(_mm_shuffle_pd(input.xy, input.xy, _MM_SHUFFLE2(0, 1)));
#else
				return input.y;
#endif
			}

#if defined(RTM_SSE2_INTRINSICS)
			RTM_DEPRECATED("Use 'as_scalar' suffix instead. To be removed in 2.4.")
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator scalard() const RTM_NO_EXCEPT
			{
				return scalard{ _mm_shuffle_pd(input.xy, input.xy, _MM_SHUFFLE2(0, 1)) };
			}
#endif

			vector4d input;
		};
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the vector4 [y] component.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr rtm_impl::vector4d_vector_get_y RTM_SIMD_CALL vector_get_y(vector4d_arg0 input) RTM_NO_EXCEPT
	{
		return rtm_impl::vector4d_vector_get_y{ input };
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the vector4 [y] component.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE scalard RTM_SIMD_CALL vector_get_y_as_scalar(vector4d_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return scalard{ _mm_shuffle_pd(input.xy, input.xy, _MM_SHUFFLE2(0, 1)) };
#else
		return vector_get_y(input);
#endif
	}

	namespace rtm_impl
	{
		//////////////////////////////////////////////////////////////////////////
		// This is a helper struct to allow a single consistent API between
		// various vector types when the semantics are identical but the return
		// type differs. Implicit coercion is used to return the desired value
		// at the call site.
		//////////////////////////////////////////////////////////////////////////
		struct vector4d_vector_get_z
		{
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator double() const RTM_NO_EXCEPT
			{
#if defined(RTM_SSE2_INTRINSICS)
				return _mm_cvtsd_f64(input.zw);
#else
				return input.z;
#endif
			}

#if defined(RTM_SSE2_INTRINSICS)
			RTM_DEPRECATED("Use 'as_scalar' suffix instead. To be removed in 2.4.")
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator scalard() const RTM_NO_EXCEPT
			{
				return scalard{ input.zw };
			}
#endif

			vector4d input;
		};
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the vector4 [z] component.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr rtm_impl::vector4d_vector_get_z RTM_SIMD_CALL vector_get_z(vector4d_arg0 input) RTM_NO_EXCEPT
	{
		return rtm_impl::vector4d_vector_get_z{ input };
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the vector4 [z] component.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE scalard RTM_SIMD_CALL vector_get_z_as_scalar(vector4d_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return scalard{ input.zw };
#else
		return vector_get_z(input);
#endif
	}

	namespace rtm_impl
	{
		//////////////////////////////////////////////////////////////////////////
		// This is a helper struct to allow a single consistent API between
		// various vector types when the semantics are identical but the return
		// type differs. Implicit coercion is used to return the desired value
		// at the call site.
		//////////////////////////////////////////////////////////////////////////
		struct vector4d_vector_get_w
		{
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator double() const RTM_NO_EXCEPT
			{
#if defined(RTM_SSE2_INTRINSICS)
				return _mm_cvtsd_f64(_mm_shuffle_pd(input.zw, input.zw, _MM_SHUFFLE2(0, 1)));
#else
				return input.w;
#endif
			}

#if defined(RTM_SSE2_INTRINSICS)
			RTM_DEPRECATED("Use 'as_scalar' suffix instead. To be removed in 2.4.")
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator scalard() const RTM_NO_EXCEPT
			{
				return scalard{ _mm_shuffle_pd(input.zw, input.zw, _MM_SHUFFLE2(0, 1)) };
			}
#endif

			vector4d input;
		};
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the vector4 [w] component.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr rtm_impl::vector4d_vector_get_w RTM_SIMD_CALL vector_get_w(vector4d_arg0 input) RTM_NO_EXCEPT
	{
		return rtm_impl::vector4d_vector_get_w{ input };
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the vector4 [w] component.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE scalard RTM_SIMD_CALL vector_get_w_as_scalar(vector4d_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return scalard{ _mm_shuffle_pd(input.zw, input.zw, _MM_SHUFFLE2(0, 1)) };
#else
		return vector_get_w(input);
#endif
	}

	namespace rtm_impl
	{
		//////////////////////////////////////////////////////////////////////////
		// This is a helper struct to allow a single consistent API between
		// various vector types when the semantics are identical but the return
		// type differs. Implicit coercion is used to return the desired value
		// at the call site.
		//////////////////////////////////////////////////////////////////////////
		template<component4 component>
		struct vector4d_vector_get_component_static
		{
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator double() const RTM_NO_EXCEPT
			{
				switch (component)
				{
					default:
					case component4::x:	return vector_get_x(input);
					case component4::y:	return vector_get_y(input);
					case component4::z:	return vector_get_z(input);
					case component4::w:	return vector_get_w(input);
				}
			}

#if defined(RTM_SSE2_INTRINSICS)
			RTM_DEPRECATED("Use 'as_scalar' suffix instead. To be removed in 2.4.")
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator scalard() const RTM_NO_EXCEPT
			{
				switch (component)
				{
					default:
					case component4::x:	return vector_get_x_as_scalar(input);
					case component4::y:	return vector_get_y_as_scalar(input);
					case component4::z:	return vector_get_z_as_scalar(input);
					case component4::w:	return vector_get_w_as_scalar(input);
				}
			}
#endif

			vector4d input;
		};
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the vector2 desired component.
	//////////////////////////////////////////////////////////////////////////
	template<component2 component, component4 component_ = static_cast<component4>(component)>
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr rtm_impl::vector4d_vector_get_component_static<component_> RTM_SIMD_CALL vector_get_component2(vector4d_arg0 input) RTM_NO_EXCEPT
	{
		return rtm_impl::vector4d_vector_get_component_static<component_>{ input };
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the vector2 desired component.
	//////////////////////////////////////////////////////////////////////////
	template<component2 component>
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE scalard RTM_SIMD_CALL vector_get_component2_as_scalar(vector4d_arg0 input) RTM_NO_EXCEPT
	{
		switch (component)
		{
			default:
			case component2::x:	return vector_get_x_as_scalar(input);
			case component2::y:	return vector_get_y_as_scalar(input);
		}
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the vector3 desired component.
	//////////////////////////////////////////////////////////////////////////
	template<component3 component, component4 component_ = static_cast<component4>(component)>
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr rtm_impl::vector4d_vector_get_component_static<component_> RTM_SIMD_CALL vector_get_component3(vector4d_arg0 input) RTM_NO_EXCEPT
	{
		return rtm_impl::vector4d_vector_get_component_static<component_>{ input };
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the vector3 desired component.
	//////////////////////////////////////////////////////////////////////////
	template<component3 component>
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE scalard RTM_SIMD_CALL vector_get_component3_as_scalar(vector4d_arg0 input) RTM_NO_EXCEPT
	{
		switch (component)
		{
			default:
			case component3::x:	return vector_get_x_as_scalar(input);
			case component3::y:	return vector_get_y_as_scalar(input);
			case component3::z:	return vector_get_z_as_scalar(input);
		}
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the vector4 desired component.
	//////////////////////////////////////////////////////////////////////////
	template<component4 component>
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr rtm_impl::vector4d_vector_get_component_static<component> RTM_SIMD_CALL vector_get_component(vector4d_arg0 input) RTM_NO_EXCEPT
	{
		return rtm_impl::vector4d_vector_get_component_static<component>{ input };
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the vector4 desired component.
	//////////////////////////////////////////////////////////////////////////
	template<component4 component>
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE scalard RTM_SIMD_CALL vector_get_component_as_scalar(vector4d_arg0 input) RTM_NO_EXCEPT
	{
		switch (component)
		{
			default:
			case component4::x:	return vector_get_x_as_scalar(input);
			case component4::y:	return vector_get_y_as_scalar(input);
			case component4::z:	return vector_get_z_as_scalar(input);
			case component4::w:	return vector_get_w_as_scalar(input);
		}
	}

	namespace rtm_impl
	{
		//////////////////////////////////////////////////////////////////////////
		// This is a helper struct to allow a single consistent API between
		// various vector types when the semantics are identical but the return
		// type differs. Implicit coercion is used to return the desired value
		// at the call site.
		//////////////////////////////////////////////////////////////////////////
		struct vector4d_vector_get_component
		{
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator double() const RTM_NO_EXCEPT
			{
				switch (component)
				{
					default:
					case component4::x:	return vector_get_x(input);
					case component4::y:	return vector_get_y(input);
					case component4::z:	return vector_get_z(input);
					case component4::w:	return vector_get_w(input);
				}
			}

#if defined(RTM_SSE2_INTRINSICS)
			RTM_DEPRECATED("Use 'as_scalar' suffix instead. To be removed in 2.4.")
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator scalard() const RTM_NO_EXCEPT
			{
				switch (component)
				{
					default:
					case component4::x:	return vector_get_x_as_scalar(input);
					case component4::y:	return vector_get_y_as_scalar(input);
					case component4::z:	return vector_get_z_as_scalar(input);
					case component4::w:	return vector_get_w_as_scalar(input);
				}
			}
#endif

			vector4d input;
			component4 component;
			int32_t padding[3];
		};
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the vector2 desired component.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr rtm_impl::vector4d_vector_get_component RTM_SIMD_CALL vector_get_component2(vector4d_arg0 input, component2 component) RTM_NO_EXCEPT
	{
		return rtm_impl::vector4d_vector_get_component{ input, static_cast<component4>(component), { 0 } };
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the vector2 desired component.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE scalard RTM_SIMD_CALL vector_get_component2_as_scalar(vector4d_arg0 input, component2 component) RTM_NO_EXCEPT
	{
		switch (component)
		{
			default:
			case component2::x:	return vector_get_x_as_scalar(input);
			case component2::y:	return vector_get_y_as_scalar(input);
		}
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the vector3 desired component.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr rtm_impl::vector4d_vector_get_component RTM_SIMD_CALL vector_get_component3(vector4d_arg0 input, component3 component) RTM_NO_EXCEPT
	{
		return rtm_impl::vector4d_vector_get_component{ input, static_cast<component4>(component), { 0 } };
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the vector3 desired component.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE scalard RTM_SIMD_CALL vector_get_component3_as_scalar(vector4d_arg0 input, component3 component) RTM_NO_EXCEPT
	{
		switch (component)
		{
			default:
			case component3::x:	return vector_get_x_as_scalar(input);
			case component3::y:	return vector_get_y_as_scalar(input);
			case component3::z:	return vector_get_z_as_scalar(input);
		}
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the vector4 desired component.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr rtm_impl::vector4d_vector_get_component RTM_SIMD_CALL vector_get_component(vector4d_arg0 input, component4 component) RTM_NO_EXCEPT
	{
		return rtm_impl::vector4d_vector_get_component{ input, component, { 0 } };
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the vector4 desired component.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE scalard RTM_SIMD_CALL vector_get_component_as_scalar(vector4d_arg0 input, component4 component) RTM_NO_EXCEPT
	{
		switch (component)
		{
			default:
			case component4::x:	return vector_get_x_as_scalar(input);
			case component4::y:	return vector_get_y_as_scalar(input);
			case component4::z:	return vector_get_z_as_scalar(input);
			case component4::w:	return vector_get_w_as_scalar(input);
		}
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the smallest component in the input vector as a scalar.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr rtm_impl::vector4d_get_min_component RTM_SIMD_CALL vector_get_min_component(vector4d_arg0 input) RTM_NO_EXCEPT
	{
		return rtm_impl::vector4d_get_min_component{ input };
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the smallest component in the input vector as a scalar.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE scalard RTM_SIMD_CALL vector_get_min_component_as_scalar(vector4d_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		__m128d xz_yw = _mm_min_pd(input.xy, input.zw);
		__m128d yw_yw = _mm_shuffle_pd(xz_yw, xz_yw, _MM_SHUFFLE2(0, 1));
		return scalard{ _mm_min_pd(xz_yw, yw_yw) };
#else
		return vector_get_min_component(input);
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the largest component in the input vector as a scalar.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr rtm_impl::vector4d_get_max_component RTM_SIMD_CALL vector_get_max_component(vector4d_arg0 input) RTM_NO_EXCEPT
	{
		return rtm_impl::vector4d_get_max_component{ input };
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the largest component in the input vector as a scalar.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE scalard RTM_SIMD_CALL vector_get_max_component_as_scalar(vector4d_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		__m128d xz_yw = _mm_max_pd(input.xy, input.zw);
		__m128d yw_yw = _mm_shuffle_pd(xz_yw, xz_yw, _MM_SHUFFLE2(0, 1));
		return scalard{ _mm_max_pd(xz_yw, yw_yw) };
#else
		return vector_get_max_component(input);
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Sets the vector4 [x] component and returns the new value.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4d RTM_SIMD_CALL vector_set_x(vector4d_arg0 input, double lane_value) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return vector4d{ _mm_move_sd(input.xy, _mm_set_sd(lane_value)), input.zw };
#else
		return vector4d{ lane_value, input.y, input.z, input.w };
#endif
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Sets the vector4 [x] component and returns the new value.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4d RTM_SIMD_CALL vector_set_x(vector4d_arg0 input, scalard_arg2 lane_value) RTM_NO_EXCEPT
	{
		return vector4d{ _mm_move_sd(input.xy, lane_value.value), input.zw };
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Sets the vector4 [y] component and returns the new value.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4d RTM_SIMD_CALL vector_set_y(vector4d_arg0 input, double lane_value) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return vector4d{ _mm_shuffle_pd(input.xy, _mm_set_sd(lane_value), _MM_SHUFFLE2(0, 0)), input.zw };
#else
		return vector4d{ input.x, lane_value, input.z, input.w };
#endif
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Sets the vector4 [y] component and returns the new value.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4d RTM_SIMD_CALL vector_set_y(vector4d_arg0 input, scalard_arg2 lane_value) RTM_NO_EXCEPT
	{
		return vector4d{ _mm_shuffle_pd(input.xy, lane_value.value, _MM_SHUFFLE2(0, 0)), input.zw };
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Sets the vector4 [z] component and returns the new value.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4d RTM_SIMD_CALL vector_set_z(vector4d_arg0 input, double lane_value) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return vector4d{ input.xy, _mm_move_sd(input.zw, _mm_set_sd(lane_value)) };
#else
		return vector4d{ input.x, input.y, lane_value, input.w };
#endif
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Sets the vector4 [z] component and returns the new value.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4d RTM_SIMD_CALL vector_set_z(vector4d_arg0 input, scalard_arg2 lane_value) RTM_NO_EXCEPT
	{
		return vector4d{ input.xy, _mm_move_sd(input.zw, lane_value.value) };
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Sets the vector4 [w] component and returns the new value.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4d RTM_SIMD_CALL vector_set_w(vector4d_arg0 input, double lane_value) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return vector4d{ input.xy, _mm_shuffle_pd(input.zw, _mm_set_sd(lane_value), _MM_SHUFFLE2(0, 0)) };
#else
		return vector4d{ input.x, input.y, input.z, lane_value };
#endif
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Sets the vector4 [w] component and returns the new value.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4d RTM_SIMD_CALL vector_set_w(vector4d_arg0 input, scalard_arg2 lane_value) RTM_NO_EXCEPT
	{
		return vector4d{ input.xy, _mm_shuffle_pd(input.zw, lane_value.value, _MM_SHUFFLE2(0, 0)) };
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Sets the desired vector2 component and returns the new value.
	//////////////////////////////////////////////////////////////////////////
	template<component2 component>
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4d RTM_SIMD_CALL vector_set_component2(vector4d_arg0 input, double lane_value) RTM_NO_EXCEPT
	{
		switch (component)
		{
			default:
			case component2::x:	return vector_set_x(input, lane_value);
			case component2::y:	return vector_set_y(input, lane_value);
		}
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Sets the desired vector2 component and returns the new value.
	//////////////////////////////////////////////////////////////////////////
	template<component2 component>
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4d RTM_SIMD_CALL vector_set_component2(vector4d_arg0 input, scalard_arg2 lane_value) RTM_NO_EXCEPT
	{
		switch (component)
		{
			default:
			case component2::x:	return vector_set_x(input, lane_value);
			case component2::y:	return vector_set_y(input, lane_value);
		}
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Sets the desired vector3 component and returns the new value.
	//////////////////////////////////////////////////////////////////////////
	template<component3 component>
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4d RTM_SIMD_CALL vector_set_component3(vector4d_arg0 input, double lane_value) RTM_NO_EXCEPT
	{
		switch (component)
		{
			default:
			case component3::x:	return vector_set_x(input, lane_value);
			case component3::y:	return vector_set_y(input, lane_value);
			case component3::z:	return vector_set_z(input, lane_value);
		}
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Sets the desired vector3 component and returns the new value.
	//////////////////////////////////////////////////////////////////////////
	template<component3 component>
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4d RTM_SIMD_CALL vector_set_component3(vector4d_arg0 input, scalard_arg2 lane_value) RTM_NO_EXCEPT
	{
		switch (component)
		{
			default:
			case component3::x:	return vector_set_x(input, lane_value);
			case component3::y:	return vector_set_y(input, lane_value);
			case component3::z:	return vector_set_z(input, lane_value);
		}
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Sets the desired vector4 component and returns the new value.
	//////////////////////////////////////////////////////////////////////////
	template<component4 component>
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4d RTM_SIMD_CALL vector_set_component(vector4d_arg0 input, double lane_value) RTM_NO_EXCEPT
	{
		switch (component)
		{
			default:
			case component4::x:	return vector_set_x(input, lane_value);
			case component4::y:	return vector_set_y(input, lane_value);
			case component4::z:	return vector_set_z(input, lane_value);
			case component4::w:	return vector_set_w(input, lane_value);
		}
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Sets the desired vector4 component and returns the new value.
	//////////////////////////////////////////////////////////////////////////
	template<component4 component>
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4d RTM_SIMD_CALL vector_set_component(vector4d_arg0 input, scalard_arg2 lane_value) RTM_NO_EXCEPT
	{
		switch (component)
		{
			default:
			case component4::x:	return vector_set_x(input, lane_value);
			case component4::y:	return vector_set_y(input, lane_value);
			case component4::z:	return vector_set_z(input, lane_value);
			case component4::w:	return vector_set_w(input, lane_value);
		}
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Sets the desired vector2 component and returns the new value.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4d RTM_SIMD_CALL vector_set_component2(vector4d_arg0 input, double lane_value, component2 component) RTM_NO_EXCEPT
	{
		switch (component)
		{
			default:
			case component2::x:	return vector_set_x(input, lane_value);
			case component2::y:	return vector_set_y(input, lane_value);
		}
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Sets the desired vector2 component and returns the new value.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4d RTM_SIMD_CALL vector_set_component2(vector4d_arg0 input, scalard_arg2 lane_value, component2 component) RTM_NO_EXCEPT
	{
		switch (component)
		{
			default:
			case component2::x:	return vector_set_x(input, lane_value);
			case component2::y:	return vector_set_y(input, lane_value);
		}
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Sets the desired vector3 component and returns the new value.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4d RTM_SIMD_CALL vector_set_component3(vector4d_arg0 input, double lane_value, component3 component) RTM_NO_EXCEPT
	{
		switch (component)
		{
			default:
			case component3::x:	return vector_set_x(input, lane_value);
			case component3::y:	return vector_set_y(input, lane_value);
			case component3::z:	return vector_set_z(input, lane_value);
		}
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Sets the desired vector3 component and returns the new value.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4d RTM_SIMD_CALL vector_set_component3(vector4d_arg0 input, scalard_arg2 lane_value, component3 component) RTM_NO_EXCEPT
	{
		switch (component)
		{
			default:
			case component3::x:	return vector_set_x(input, lane_value);
			case component3::y:	return vector_set_y(input, lane_value);
			case component3::z:	return vector_set_z(input, lane_value);
		}
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Sets the desired vector4 component and returns the new value.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4d RTM_SIMD_CALL vector_set_component(vector4d_arg0 input, double lane_value, component4 component) RTM_NO_EXCEPT
	{
		switch (component)
		{
			default:
			case component4::x:	return vector_set_x(input, lane_value);
			case component4::y:	return vector_set_y(input, lane_value);
			case component4::z:	return vector_set_z(input, lane_value);
			case component4::w:	return vector_set_w(input, lane_value);
		}
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Sets the desired vector4 component and returns the new value.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4d RTM_SIMD_CALL vector_set_component(vector4d_arg0 input, scalard_arg2 lane_value, component4 component) RTM_NO_EXCEPT
	{
		switch (component)
		{
			default:
			case component4::x:	return vector_set_x(input, lane_value);
			case component4::y:	return vector_set_y(input, lane_value);
			case component4::z:	return vector_set_z(input, lane_value);
			case component4::w:	return vector_set_w(input, lane_value);
		}
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Returns a floating point pointer to the vector4 data.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE const double* vector_to_pointer(const vector4d& input) RTM_NO_EXCEPT
	{
		return rtm_impl::bit_cast<const double*>(&input);
	}

	//////////////////////////////////////////////////////////////////////////
	// Writes a vector4 to unaligned memory.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE void RTM_SIMD_CALL vector_store(vector4d_arg0 input, double* output) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		_mm_storeu_pd(output, input.xy);
		_mm_storeu_pd(output + 2, input.zw);
#else
		output[0] = vector_get_x(input);
		output[1] = vector_get_y(input);
		output[2] = vector_get_z(input);
		output[3] = vector_get_w(input);
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Writes a vector1 to unaligned memory.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE void RTM_SIMD_CALL vector_store1(vector4d_arg0 input, double* output) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		_mm_store_sd(output, input.xy);
#else
		output[0] = vector_get_x(input);
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Writes a vector2 to unaligned memory.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE void RTM_SIMD_CALL vector_store2(vector4d_arg0 input, double* output) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		_mm_storeu_pd(output, input.xy);
#else
		output[0] = vector_get_x(input);
		output[1] = vector_get_y(input);
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Writes a vector3 to unaligned memory.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE void RTM_SIMD_CALL vector_store3(vector4d_arg0 input, double* output) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		_mm_storeu_pd(output, input.xy);
		_mm_store_sd(output + 2, input.zw);
#else
		output[0] = vector_get_x(input);
		output[1] = vector_get_y(input);
		output[2] = vector_get_z(input);
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Writes a vector4 to unaligned memory.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE void RTM_SIMD_CALL vector_store(vector4d_arg0 input, uint8_t* output) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		_mm_storeu_pd(rtm_impl::bit_cast<double*>(output), input.xy);
		_mm_storeu_pd(rtm_impl::bit_cast<double*>(output) + 2, input.zw);
#else
		std::memcpy(output, &input, sizeof(vector4d));
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Writes a vector1 to unaligned memory.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE void RTM_SIMD_CALL vector_store1(vector4d_arg0 input, uint8_t* output)
	{
#if defined(RTM_SSE2_INTRINSICS)
		_mm_store_sd(rtm_impl::bit_cast<double*>(output), input.xy);
#else
		std::memcpy(output, &input, sizeof(double) * 1);
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Writes a vector2 to unaligned memory.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE void RTM_SIMD_CALL vector_store2(vector4d_arg0 input, uint8_t* output)
	{
#if defined(RTM_SSE2_INTRINSICS)
		_mm_storeu_pd(rtm_impl::bit_cast<double*>(output), input.xy);
#else
		std::memcpy(output, &input, sizeof(double) * 2);
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Writes a vector3 to unaligned memory.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE void RTM_SIMD_CALL vector_store3(vector4d_arg0 input, uint8_t* output)
	{
#if defined(RTM_SSE2_INTRINSICS)
		_mm_storeu_pd(rtm_impl::bit_cast<double*>(output), input.xy);
		_mm_store_sd(rtm_impl::bit_cast<double*>(output) + 2, input.zw);
#else
		std::memcpy(output, &input, sizeof(double) * 3);
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Writes a vector4 to unaligned memory.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE void RTM_SIMD_CALL vector_store(vector4d_arg0 input, float4d* output) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		_mm_storeu_pd(&output->x, input.xy);
		_mm_storeu_pd(&output->z, input.zw);
#else
		output->x = vector_get_x(input);
		output->y = vector_get_y(input);
		output->z = vector_get_z(input);
		output->w = vector_get_w(input);
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Writes a vector2 to unaligned memory.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE void RTM_SIMD_CALL vector_store2(vector4d_arg0 input, float2d* output) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		_mm_storeu_pd(&output->x, input.xy);
#else
		output->x = vector_get_x(input);
		output->y = vector_get_y(input);
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Writes a vector3 to unaligned memory.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE void RTM_SIMD_CALL vector_store3(vector4d_arg0 input, float3d* output) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		_mm_storeu_pd(&output->x, input.xy);
		_mm_store_sd(&output->z, input.zw);
#else
		output->x = vector_get_x(input);
		output->y = vector_get_y(input);
		output->z = vector_get_z(input);
#endif
	}



	//////////////////////////////////////////////////////////////////////////
	// Arithmetic
	//////////////////////////////////////////////////////////////////////////


	//////////////////////////////////////////////////////////////////////////
	// Per component addition of the two inputs: lhs + rhs
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4d RTM_SIMD_CALL vector_add(vector4d_arg0 lhs, vector4d_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return vector4d{ _mm_add_pd(lhs.xy, rhs.xy), _mm_add_pd(lhs.zw, rhs.zw) };
#else
		return vector_set(lhs.x + rhs.x, lhs.y + rhs.y, lhs.z + rhs.z, lhs.w + rhs.w);
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Per component subtraction of the two inputs: lhs - rhs
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4d RTM_SIMD_CALL vector_sub(vector4d_arg0 lhs, vector4d_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return vector4d{ _mm_sub_pd(lhs.xy, rhs.xy), _mm_sub_pd(lhs.zw, rhs.zw) };
#else
		return vector_set(lhs.x - rhs.x, lhs.y - rhs.y, lhs.z - rhs.z, lhs.w - rhs.w);
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Per component multiplication of the two inputs: lhs * rhs
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4d RTM_SIMD_CALL vector_mul(vector4d_arg0 lhs, vector4d_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return vector4d{ _mm_mul_pd(lhs.xy, rhs.xy), _mm_mul_pd(lhs.zw, rhs.zw) };
#else
		return vector_set(lhs.x * rhs.x, lhs.y * rhs.y, lhs.z * rhs.z, lhs.w * rhs.w);
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Per component multiplication of the vector by a scalar: lhs * rhs
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4d RTM_SIMD_CALL vector_mul(vector4d_arg0 lhs, double rhs) RTM_NO_EXCEPT
	{
		return vector_mul(lhs, vector_set(rhs));
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Per component multiplication of the vector by a scalar: lhs * rhs
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4d RTM_SIMD_CALL vector_mul(vector4d_arg0 lhs, scalard_arg2 rhs) RTM_NO_EXCEPT
	{
		const __m128d rhs_xx = _mm_shuffle_pd(rhs.value, rhs.value, _MM_SHUFFLE2(0, 0));
		return vector4d{ _mm_mul_pd(lhs.xy, rhs_xx), _mm_mul_pd(lhs.zw, rhs_xx) };
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Per component division of the two inputs: lhs / rhs
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4d RTM_SIMD_CALL vector_div(vector4d_arg0 lhs, vector4d_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return vector4d{ _mm_div_pd(lhs.xy, rhs.xy), _mm_div_pd(lhs.zw, rhs.zw) };
#else
		return vector_set(lhs.x / rhs.x, lhs.y / rhs.y, lhs.z / rhs.z, lhs.w / rhs.w);
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Per component maximum of the two inputs: max(lhs, rhs)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4d RTM_SIMD_CALL vector_max(vector4d_arg0 lhs, vector4d_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return vector4d{ _mm_max_pd(lhs.xy, rhs.xy), _mm_max_pd(lhs.zw, rhs.zw) };
#else
		return vector_set(scalar_max(lhs.x, rhs.x), scalar_max(lhs.y, rhs.y), scalar_max(lhs.z, rhs.z), scalar_max(lhs.w, rhs.w));
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Per component minimum of the two inputs: min(lhs, rhs)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4d RTM_SIMD_CALL vector_min(vector4d_arg0 lhs, vector4d_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return vector4d{ _mm_min_pd(lhs.xy, rhs.xy), _mm_min_pd(lhs.zw, rhs.zw) };
#else
		return vector_set(scalar_min(lhs.x, rhs.x), scalar_min(lhs.y, rhs.y), scalar_min(lhs.z, rhs.z), scalar_min(lhs.w, rhs.w));
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Per component clamping of an input between a minimum and a maximum value: min(max_value, max(min_value, input))
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4d RTM_SIMD_CALL vector_clamp(vector4d_arg0 input, vector4d_arg1 min_value, vector4d_arg2 max_value) RTM_NO_EXCEPT
	{
		return vector_min(max_value, vector_max(min_value, input));
	}

	//////////////////////////////////////////////////////////////////////////
	// Per component absolute of the input: abs(input)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4d RTM_SIMD_CALL vector_abs(vector4d_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		const __m128d abs_mask = _mm_castsi128_pd(_mm_set_epi64x(0x7FFFFFFFFFFFFFFFULL, 0x7FFFFFFFFFFFFFFFULL));
		return vector4d{ _mm_and_pd(input.xy, abs_mask), _mm_and_pd(input.zw, abs_mask) };
#else
		return vector_set(scalar_abs(input.x), scalar_abs(input.y), scalar_abs(input.z), scalar_abs(input.w));
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Per component negation of the input: -input
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4d RTM_SIMD_CALL vector_neg(vector4d_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		constexpr __m128d signs = RTM_VECTOR2D_MAKE(-0.0, -0.0);
		return vector4d{ _mm_xor_pd(input.xy, signs), _mm_xor_pd(input.zw, signs) };
#else
		return vector_mul(input, -1.0);
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Per component negation of the input: -input
	// Each template argument controls whether a SIMD lane should be negated (true) or not (false).
	//////////////////////////////////////////////////////////////////////////
	template<bool x, bool y, bool z, bool w>
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4d RTM_SIMD_CALL vector_neg(vector4d_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		constexpr __m128d signs_xy = RTM_VECTOR2D_MAKE(x ? -0.0 : 0.0, y ? -0.0 : 0.0);
		constexpr __m128d signs_zw = RTM_VECTOR2D_MAKE(z ? -0.0 : 0.0, w ? -0.0 : 0.0);
		return vector4d{ _mm_xor_pd(input.xy, signs_xy), _mm_xor_pd(input.zw, signs_zw) };
#else
		const vector4d signs = vector_set(x ? -1.0 : 1.0, y ? -1.0 : 1.0, z ? -1.0 : 1.0, w ? -1.0 : 1.0);
		return vector_mul(input, signs);
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Per component reciprocal of the input: 1.0 / input
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4d RTM_SIMD_CALL vector_reciprocal(vector4d_arg0 input) RTM_NO_EXCEPT
	{
		// Performance note:
		// With modern out-of-order executing processors, it is typically faster to use
		// a full division instead of a reciprocal estimate + Newton-Raphson iterations
		// because the resulting code is more dense and is more likely to inline and
		// as it uses fewer instructions.
		return vector_div(vector_set(1.0), input);
	}

	//////////////////////////////////////////////////////////////////////////
	// Per component square root of the input: sqrt(input)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4d RTM_SIMD_CALL vector_sqrt(vector4d_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return vector4d{ _mm_sqrt_pd(input.xy), _mm_sqrt_pd(input.zw) };
#else
		scalard x = vector_get_x(input);
		scalard y = vector_get_y(input);
		scalard z = vector_get_z(input);
		scalard w = vector_get_w(input);
		return vector_set(scalar_sqrt(x), scalar_sqrt(y), scalar_sqrt(z), scalar_sqrt(w));
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Per component reciprocal square root of the input: 1.0 / sqrt(input)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4d RTM_SIMD_CALL vector_sqrt_reciprocal(vector4d_arg0 input) RTM_NO_EXCEPT
	{
		// Performance note:
		// With modern out-of-order executing processors, it is typically faster to use
		// a full division/square root instead of a reciprocal estimate + Newton-Raphson iterations
		// because the resulting code is more dense and is more likely to inline and
		// as it uses fewer instructions.
		return vector_reciprocal(vector_sqrt(input));
	}

	//////////////////////////////////////////////////////////////////////////
	// Per component returns the smallest integer value not less than the input (round towards positive infinity).
	// vector_ceil([1.8, 1.0, -1.8, -1.0]) = [2.0, 1.0, -1.0, -1.0]
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline vector4d RTM_SIMD_CALL vector_ceil(vector4d_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		// NaN, +- Infinity, and numbers larger or equal to 2^23 remain unchanged
		// since they have no fractional part.

		const __m128i abs_mask = _mm_set_epi64x(0x7FFFFFFFFFFFFFFFULL, 0x7FFFFFFFFFFFFFFFULL);
		const __m128d fractional_limit = _mm_set1_pd(4503599627370496.0); // 2^52

		// Build our mask, larger values that have no fractional part, and infinities will be true
		// Smaller values and NaN will be false
		__m128d abs_input_xy = _mm_and_pd(input.xy, _mm_castsi128_pd(abs_mask));
		__m128d abs_input_zw = _mm_and_pd(input.zw, _mm_castsi128_pd(abs_mask));
		__m128d is_input_large_xy = _mm_cmpge_pd(abs_input_xy, fractional_limit);
		__m128d is_input_large_zw = _mm_cmpge_pd(abs_input_zw, fractional_limit);

		// Test if our input is NaN with (value != value), it is only true for NaN
		__m128d is_nan_xy = _mm_cmpneq_pd(input.xy, input.xy);
		__m128d is_nan_zw = _mm_cmpneq_pd(input.zw, input.zw);

		// Combine our masks to determine if we should return the original value
		__m128d use_original_input_xy = _mm_or_pd(is_input_large_xy, is_nan_xy);
		__m128d use_original_input_zw = _mm_or_pd(is_input_large_zw, is_nan_zw);

		// Convert to an integer and back
		__m128d integer_part_xy = _mm_cvtepi32_pd(_mm_cvtpd_epi32(input.xy));
		__m128d integer_part_zw = _mm_cvtepi32_pd(_mm_cvtpd_epi32(input.zw));

		// Test if the returned value is smaller than the original.
		// A positive input will round towards zero and be lower when we need it to be greater.
		__m128d is_positive_xy = _mm_cmplt_pd(integer_part_xy, input.xy);
		__m128d is_positive_zw = _mm_cmplt_pd(integer_part_zw, input.zw);

		// Our mask output is 64 bit wide but to convert to a bias, we need 32 bit integers
		is_positive_xy = _mm_castps_pd(_mm_shuffle_ps(_mm_castpd_ps(is_positive_xy), _mm_castpd_ps(is_positive_xy), _MM_SHUFFLE(2, 0, 2, 0)));
		is_positive_zw = _mm_castps_pd(_mm_shuffle_ps(_mm_castpd_ps(is_positive_zw), _mm_castpd_ps(is_positive_zw), _MM_SHUFFLE(2, 0, 2, 0)));

		// Convert our mask to a float, ~0 yields -1.0 since it is a valid signed integer
		// Negative values will yield a 0.0 bias
		__m128d bias_xy = _mm_cvtepi32_pd(_mm_castpd_si128(is_positive_xy));
		__m128d bias_zw = _mm_cvtepi32_pd(_mm_castpd_si128(is_positive_zw));

		// Subtract our bias to properly handle positive values
		integer_part_xy = _mm_sub_pd(integer_part_xy, bias_xy);
		integer_part_zw = _mm_sub_pd(integer_part_zw, bias_zw);

		__m128d result_xy = _mm_or_pd(_mm_and_pd(use_original_input_xy, input.xy), _mm_andnot_pd(use_original_input_xy, integer_part_xy));
		__m128d result_zw = _mm_or_pd(_mm_and_pd(use_original_input_zw, input.zw), _mm_andnot_pd(use_original_input_zw, integer_part_zw));
		return vector4d{ result_xy, result_zw };
#else
		return vector_set(scalar_ceil(vector_get_x(input)), scalar_ceil(vector_get_y(input)), scalar_ceil(vector_get_z(input)), scalar_ceil(vector_get_w(input)));
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Per component returns the largest integer value not greater than the input (round towards negative infinity).
	// vector_floor([1.8, 1.0, -1.8, -1.0]) = [1.0, 1.0, -2.0, -1.0]
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline vector4d RTM_SIMD_CALL vector_floor(vector4d_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE4_INTRINSICS)
		return vector4d{ _mm_floor_pd(input.xy), _mm_floor_pd(input.zw) };
#elif defined(RTM_SSE2_INTRINSICS)
		// NaN, +- Infinity, and numbers larger or equal to 2^23 remain unchanged
		// since they have no fractional part.

		const __m128i abs_mask = _mm_set_epi64x(0x7FFFFFFFFFFFFFFFULL, 0x7FFFFFFFFFFFFFFFULL);
		const __m128d fractional_limit = _mm_set1_pd(4503599627370496.0); // 2^52

		// Build our mask, larger values that have no fractional part, and infinities will be true
		// Smaller values and NaN will be false
		__m128d abs_input_xy = _mm_and_pd(input.xy, _mm_castsi128_pd(abs_mask));
		__m128d abs_input_zw = _mm_and_pd(input.zw, _mm_castsi128_pd(abs_mask));
		__m128d is_input_large_xy = _mm_cmpge_pd(abs_input_xy, fractional_limit);
		__m128d is_input_large_zw = _mm_cmpge_pd(abs_input_zw, fractional_limit);

		// Test if our input is NaN with (value != value), it is only true for NaN
		__m128d is_nan_xy = _mm_cmpneq_pd(input.xy, input.xy);
		__m128d is_nan_zw = _mm_cmpneq_pd(input.zw, input.zw);

		// Combine our masks to determine if we should return the original value
		__m128d use_original_input_xy = _mm_or_pd(is_input_large_xy, is_nan_xy);
		__m128d use_original_input_zw = _mm_or_pd(is_input_large_zw, is_nan_zw);

		// Convert to an integer and back
		__m128d integer_part_xy = _mm_cvtepi32_pd(_mm_cvtpd_epi32(input.xy));
		__m128d integer_part_zw = _mm_cvtepi32_pd(_mm_cvtpd_epi32(input.zw));

		// Test if the returned value is greater than the original.
		// A negative input will round towards zero and be greater when we need it to be smaller.
		__m128d is_negative_xy = _mm_cmpgt_pd(integer_part_xy, input.xy);
		__m128d is_negative_zw = _mm_cmpgt_pd(integer_part_zw, input.zw);

		// Our mask output is 64 bit wide but to convert to a bias, we need 32 bit integers
		is_negative_xy = _mm_castps_pd(_mm_shuffle_ps(_mm_castpd_ps(is_negative_xy), _mm_castpd_ps(is_negative_xy), _MM_SHUFFLE(2, 0, 2, 0)));
		is_negative_zw = _mm_castps_pd(_mm_shuffle_ps(_mm_castpd_ps(is_negative_zw), _mm_castpd_ps(is_negative_zw), _MM_SHUFFLE(2, 0, 2, 0)));

		// Convert our mask to a float, ~0 yields -1.0 since it is a valid signed integer
		// Positive values will yield a 0.0 bias
		__m128d bias_xy = _mm_cvtepi32_pd(_mm_castpd_si128(is_negative_xy));
		__m128d bias_zw = _mm_cvtepi32_pd(_mm_castpd_si128(is_negative_zw));

		// Add our bias to properly handle negative values
		integer_part_xy = _mm_add_pd(integer_part_xy, bias_xy);
		integer_part_zw = _mm_add_pd(integer_part_zw, bias_zw);

		__m128d result_xy = _mm_or_pd(_mm_and_pd(use_original_input_xy, input.xy), _mm_andnot_pd(use_original_input_xy, integer_part_xy));
		__m128d result_zw = _mm_or_pd(_mm_and_pd(use_original_input_zw, input.zw), _mm_andnot_pd(use_original_input_zw, integer_part_zw));
		return vector4d{ result_xy, result_zw };
#else
		return vector_set(scalar_floor(vector_get_x(input)), scalar_floor(vector_get_y(input)), scalar_floor(vector_get_z(input)), scalar_floor(vector_get_w(input)));
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// 3D cross product: lhs x rhs
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4d RTM_SIMD_CALL vector_cross3(vector4d_arg0 lhs, vector4d_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		// cross(a, b).zxy = (a * b.yzx) - (a.yzx * b)
		__m128d lhs_yz = _mm_shuffle_pd(lhs.xy, lhs.zw, _MM_SHUFFLE2(0, 1));
		__m128d lhs_xy = lhs.xy;
		__m128d rhs_yz = _mm_shuffle_pd(rhs.xy, rhs.zw, _MM_SHUFFLE2(0, 1));
		__m128d rhs_xy = rhs.xy;

		__m128d tmp_zx = _mm_sub_pd(_mm_mul_pd(lhs_xy, rhs_yz), _mm_mul_pd(lhs_yz, rhs_xy));
		__m128d tmp_y = _mm_sub_pd(_mm_mul_pd(lhs.zw, rhs_xy), _mm_mul_pd(lhs_xy, rhs.zw));

		// cross(a, b) = ((a * b.yzx) - (a.yzx * b)).yzx
		__m128d result_xy = _mm_shuffle_pd(tmp_zx, tmp_y, _MM_SHUFFLE2(0, 1));
		__m128d result_z = tmp_zx;
		return vector4d{ result_xy, result_z };
#else
		// cross(a, b) = (a.yzx * b.zxy) - (a.zxy * b.yzx)
		const double lhs_x = vector_get_x(lhs);
		const double lhs_y = vector_get_y(lhs);
		const double lhs_z = vector_get_z(lhs);
		const double rhs_x = vector_get_x(rhs);
		const double rhs_y = vector_get_y(rhs);
		const double rhs_z = vector_get_z(rhs);
		return vector_set((lhs_y * rhs_z) - (lhs_z * rhs_y), (lhs_z * rhs_x) - (lhs_x * rhs_z), (lhs_x * rhs_y) - (lhs_y * rhs_x));
#endif
	}

	namespace rtm_impl
	{
		//////////////////////////////////////////////////////////////////////////
		// This is a helper struct to allow a single consistent API between
		// various vector types when the semantics are identical but the return
		// type differs. Implicit coercion is used to return the desired value
		// at the call site.
		//////////////////////////////////////////////////////////////////////////
		struct vector4d_vector_dot
		{
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator double() const RTM_NO_EXCEPT
			{
#if defined(RTM_SSE2_INTRINSICS)
				const __m128d x2_y2 = _mm_mul_pd(lhs.xy, rhs.xy);
				const __m128d z2_w2 = _mm_mul_pd(lhs.zw, rhs.zw);
				const __m128d x2z2_y2w2 = _mm_add_pd(x2_y2, z2_w2);
				const __m128d y2w2 = _mm_shuffle_pd(x2z2_y2w2, x2z2_y2w2, _MM_SHUFFLE2(1, 1));
				const __m128d x2y2z2w2 = _mm_add_pd(x2z2_y2w2, y2w2);
				return _mm_cvtsd_f64(x2y2z2w2);
#else
				const scalard lhs_x = vector_get_x_as_scalar(lhs);
				const scalard lhs_y = vector_get_y_as_scalar(lhs);
				const scalard lhs_z = vector_get_z_as_scalar(lhs);
				const scalard lhs_w = vector_get_w_as_scalar(lhs);
				const scalard rhs_x = vector_get_x_as_scalar(rhs);
				const scalard rhs_y = vector_get_y_as_scalar(rhs);
				const scalard rhs_z = vector_get_z_as_scalar(rhs);
				const scalard rhs_w = vector_get_w_as_scalar(rhs);
				const scalard xx = scalar_mul(lhs_x, rhs_x);
				const scalard yy = scalar_mul(lhs_y, rhs_y);
				const scalard zz = scalar_mul(lhs_z, rhs_z);
				const scalard ww = scalar_mul(lhs_w, rhs_w);
				return scalar_cast(scalar_add(scalar_add(xx, yy), scalar_add(zz, ww)));
#endif
			}

#if defined(RTM_SSE2_INTRINSICS)
			RTM_DEPRECATED("Use 'as_scalar' suffix instead. To be removed in 2.4.")
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator scalard() const RTM_NO_EXCEPT
			{
				const __m128d x2_y2 = _mm_mul_pd(lhs.xy, rhs.xy);
				const __m128d z2_w2 = _mm_mul_pd(lhs.zw, rhs.zw);
				const __m128d x2z2_y2w2 = _mm_add_pd(x2_y2, z2_w2);
				const __m128d y2w2 = _mm_shuffle_pd(x2z2_y2w2, x2z2_y2w2, _MM_SHUFFLE2(1, 1));
				const __m128d x2y2z2w2 = _mm_add_pd(x2z2_y2w2, y2w2);
				return scalard{ x2y2z2w2 };
			}
#endif

			RTM_DEPRECATED("Use 'as_vector' suffix instead. To be removed in 2.4.")
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator vector4d() const RTM_NO_EXCEPT
			{
				const double dot = *this;
				return vector_set(dot);
			}

			vector4d lhs;
			vector4d rhs;
		};
	}

	//////////////////////////////////////////////////////////////////////////
	// 4D dot product: lhs . rhs
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr rtm_impl::vector4d_vector_dot RTM_SIMD_CALL vector_dot(vector4d_arg0 lhs, vector4d_arg1 rhs) RTM_NO_EXCEPT
	{
		return rtm_impl::vector4d_vector_dot{ lhs, rhs };
	}

	//////////////////////////////////////////////////////////////////////////
	// 4D dot product: lhs . rhs
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE scalard RTM_SIMD_CALL vector_dot_as_scalar(vector4d_arg0 lhs, vector4d_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		const __m128d x2_y2 = _mm_mul_pd(lhs.xy, rhs.xy);
		const __m128d z2_w2 = _mm_mul_pd(lhs.zw, rhs.zw);
		const __m128d x2z2_y2w2 = _mm_add_pd(x2_y2, z2_w2);
		const __m128d y2w2 = _mm_shuffle_pd(x2z2_y2w2, x2z2_y2w2, _MM_SHUFFLE2(1, 1));
		const __m128d x2y2z2w2 = _mm_add_pd(x2z2_y2w2, y2w2);
		return scalard{ x2y2z2w2 };
#else
		const scalard lhs_x = vector_get_x_as_scalar(lhs);
		const scalard lhs_y = vector_get_y_as_scalar(lhs);
		const scalard lhs_z = vector_get_z_as_scalar(lhs);
		const scalard lhs_w = vector_get_w_as_scalar(lhs);
		const scalard rhs_x = vector_get_x_as_scalar(rhs);
		const scalard rhs_y = vector_get_y_as_scalar(rhs);
		const scalard rhs_z = vector_get_z_as_scalar(rhs);
		const scalard rhs_w = vector_get_w_as_scalar(rhs);
		const scalard xx = scalar_mul(lhs_x, rhs_x);
		const scalard yy = scalar_mul(lhs_y, rhs_y);
		const scalard zz = scalar_mul(lhs_z, rhs_z);
		const scalard ww = scalar_mul(lhs_w, rhs_w);
		return scalar_add(scalar_add(xx, yy), scalar_add(zz, ww));
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// 4D dot product: lhs . rhs
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4d RTM_SIMD_CALL vector_dot_as_vector(vector4d_arg0 lhs, vector4d_arg1 rhs) RTM_NO_EXCEPT
	{
		return vector_set(vector_dot_as_scalar(lhs, rhs));
	}

	namespace rtm_impl
	{
		//////////////////////////////////////////////////////////////////////////
		// This is a helper struct to allow a single consistent API between
		// various vector types when the semantics are identical but the return
		// type differs. Implicit coercion is used to return the desired value
		// at the call site.
		//////////////////////////////////////////////////////////////////////////
		struct vector4d_vector_dot2
		{
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator double() const RTM_NO_EXCEPT
			{
#if defined(RTM_SSE2_INTRINSICS)
				__m128d x2_y2 = _mm_mul_pd(lhs.xy, rhs.xy);
				__m128d y2 = _mm_shuffle_pd(x2_y2, x2_y2, _MM_SHUFFLE2(0, 1));
				return _mm_cvtsd_f64(_mm_add_sd(x2_y2, y2));
#else
				return (vector_get_x(lhs) * vector_get_x(rhs)) + (vector_get_y(lhs) * vector_get_y(rhs));
#endif
			}

#if defined(RTM_SSE2_INTRINSICS)
			RTM_DEPRECATED("Use 'as_scalar' suffix instead. To be removed in 2.4.")
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator scalard() const RTM_NO_EXCEPT
			{
				__m128d x2_y2 = _mm_mul_pd(lhs.xy, rhs.xy);
				__m128d y2 = _mm_shuffle_pd(x2_y2, x2_y2, _MM_SHUFFLE2(0, 1));
				return scalard{ _mm_add_sd(x2_y2, y2) };
			}
#endif

			RTM_DEPRECATED("Use 'as_vector' suffix instead. To be removed in 2.4.")
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator vector4d() const RTM_NO_EXCEPT
			{
				const double dot = *this;
				return vector_set(dot);
			}

			vector4d lhs;
			vector4d rhs;
		};
	}

	//////////////////////////////////////////////////////////////////////////
	// 2D dot product: lhs . rhs
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr rtm_impl::vector4d_vector_dot2 RTM_SIMD_CALL vector_dot2(vector4d_arg0 lhs, vector4d_arg1 rhs) RTM_NO_EXCEPT
	{
		return rtm_impl::vector4d_vector_dot2{ lhs, rhs };
	}

	//////////////////////////////////////////////////////////////////////////
	// 2D dot product: lhs . rhs
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE scalard RTM_SIMD_CALL vector_dot2_as_scalar(vector4d_arg0 lhs, vector4d_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		__m128d x2_y2 = _mm_mul_pd(lhs.xy, rhs.xy);
		__m128d y2 = _mm_shuffle_pd(x2_y2, x2_y2, _MM_SHUFFLE2(0, 1));
		return scalard{ _mm_add_sd(x2_y2, y2) };
#else
		return vector_dot2(lhs, rhs);
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// 2D dot product: lhs . rhs
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4d RTM_SIMD_CALL vector_dot2_as_vector(vector4d_arg0 lhs, vector4d_arg1 rhs) RTM_NO_EXCEPT
	{
		return vector_set(vector_dot2_as_scalar(lhs, rhs));
	}

	namespace rtm_impl
	{
		//////////////////////////////////////////////////////////////////////////
		// This is a helper struct to allow a single consistent API between
		// various vector types when the semantics are identical but the return
		// type differs. Implicit coercion is used to return the desired value
		// at the call site.
		//////////////////////////////////////////////////////////////////////////
		struct vector4d_vector_dot3
		{
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator double() const RTM_NO_EXCEPT
			{
#if defined(RTM_SSE2_INTRINSICS)
				__m128d x2_y2 = _mm_mul_pd(lhs.xy, rhs.xy);
				__m128d z2_w2 = _mm_mul_pd(lhs.zw, rhs.zw);
				__m128d y2 = _mm_shuffle_pd(x2_y2, x2_y2, _MM_SHUFFLE2(0, 1));
				__m128d x2y2 = _mm_add_sd(x2_y2, y2);
				return _mm_cvtsd_f64(_mm_add_sd(x2y2, z2_w2));
#else
				return (vector_get_x(lhs) * vector_get_x(rhs)) + (vector_get_y(lhs) * vector_get_y(rhs)) + (vector_get_z(lhs) * vector_get_z(rhs));
#endif
			}

#if defined(RTM_SSE2_INTRINSICS)
			RTM_DEPRECATED("Use 'as_scalar' suffix instead. To be removed in 2.4.")
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator scalard() const RTM_NO_EXCEPT
			{
				__m128d x2_y2 = _mm_mul_pd(lhs.xy, rhs.xy);
				__m128d z2_w2 = _mm_mul_pd(lhs.zw, rhs.zw);
				__m128d y2 = _mm_shuffle_pd(x2_y2, x2_y2, _MM_SHUFFLE2(0, 1));
				__m128d x2y2 = _mm_add_sd(x2_y2, y2);
				return scalard{ _mm_add_sd(x2y2, z2_w2) };
			}
#endif

			RTM_DEPRECATED("Use 'as_vector' suffix instead. To be removed in 2.4.")
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator vector4d() const RTM_NO_EXCEPT
			{
				const double dot = *this;
				return vector_set(dot);
			}

			vector4d lhs;
			vector4d rhs;
		};
	}

	//////////////////////////////////////////////////////////////////////////
	// 3D dot product: lhs . rhs
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr rtm_impl::vector4d_vector_dot3 RTM_SIMD_CALL vector_dot3(vector4d_arg0 lhs, vector4d_arg1 rhs) RTM_NO_EXCEPT
	{
		return rtm_impl::vector4d_vector_dot3{ lhs, rhs };
	}

	//////////////////////////////////////////////////////////////////////////
	// 3D dot product: lhs . rhs
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE scalard RTM_SIMD_CALL vector_dot3_as_scalar(vector4d_arg0 lhs, vector4d_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		__m128d x2_y2 = _mm_mul_pd(lhs.xy, rhs.xy);
		__m128d z2_w2 = _mm_mul_pd(lhs.zw, rhs.zw);
		__m128d y2 = _mm_shuffle_pd(x2_y2, x2_y2, _MM_SHUFFLE2(0, 1));
		__m128d x2y2 = _mm_add_sd(x2_y2, y2);
		return scalard{ _mm_add_sd(x2y2, z2_w2) };
#else
		return vector_dot3(lhs, rhs);
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// 3D dot product: lhs . rhs
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4d RTM_SIMD_CALL vector_dot3_as_vector(vector4d_arg0 lhs, vector4d_arg1 rhs) RTM_NO_EXCEPT
	{
		return vector_set(vector_dot3_as_scalar(lhs, rhs));
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the squared length/norm of the vector4.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr rtm_impl::vector4d_vector_dot RTM_SIMD_CALL vector_length_squared(vector4d_arg0 input) RTM_NO_EXCEPT
	{
		return rtm_impl::vector4d_vector_dot{ input, input };
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the squared length/norm of the vector4.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE scalard RTM_SIMD_CALL vector_length_squared_as_scalar(vector4d_arg0 input) RTM_NO_EXCEPT
	{
		return vector_dot_as_scalar(input, input);
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the squared length/norm of the vector4.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4d RTM_SIMD_CALL vector_length_squared_as_vector(vector4d_arg0 input) RTM_NO_EXCEPT
	{
		return vector_dot_as_vector(input, input);
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the squared length/norm of the vector2.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr rtm_impl::vector4d_vector_dot2 RTM_SIMD_CALL vector_length_squared2(vector4d_arg0 input) RTM_NO_EXCEPT
	{
		return rtm_impl::vector4d_vector_dot2{ input, input };
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the squared length/norm of the vector2.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE scalard RTM_SIMD_CALL vector_length_squared2_as_scalar(vector4d_arg0 input) RTM_NO_EXCEPT
	{
		return vector_dot2_as_scalar(input, input);
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the squared length/norm of the vector2.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4d RTM_SIMD_CALL vector_length_squared2_as_vector(vector4d_arg0 input) RTM_NO_EXCEPT
	{
		return vector_dot2_as_vector(input, input);
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the squared length/norm of the vector3.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr rtm_impl::vector4d_vector_dot3 RTM_SIMD_CALL vector_length_squared3(vector4d_arg0 input) RTM_NO_EXCEPT
	{
		return rtm_impl::vector4d_vector_dot3{ input, input };
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the squared length/norm of the vector3.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE scalard RTM_SIMD_CALL vector_length_squared3_as_scalar(vector4d_arg0 input) RTM_NO_EXCEPT
	{
		return vector_dot3_as_scalar(input, input);
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the squared length/norm of the vector3.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4d RTM_SIMD_CALL vector_length_squared3_as_vector(vector4d_arg0 input) RTM_NO_EXCEPT
	{
		return vector_dot3_as_vector(input, input);
	}

	namespace rtm_impl
	{
		//////////////////////////////////////////////////////////////////////////
		// This is a helper struct to allow a single consistent API between
		// various vector types when the semantics are identical but the return
		// type differs. Implicit coercion is used to return the desired value
		// at the call site.
		//////////////////////////////////////////////////////////////////////////
		struct vector4d_vector_length
		{
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator double() const RTM_NO_EXCEPT
			{
				const scalard len_sq = vector_length_squared_as_scalar(input);
				return scalar_cast(scalar_sqrt(len_sq));
			}

#if defined(RTM_SSE2_INTRINSICS)
			RTM_DEPRECATED("Use 'as_scalar' suffix instead. To be removed in 2.4.")
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator scalard() const RTM_NO_EXCEPT
			{
				const scalard len_sq = vector_length_squared_as_scalar(input);
				return scalar_sqrt(len_sq);
			}
#endif

			vector4d input;
		};
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the length/norm of the vector4.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr rtm_impl::vector4d_vector_length RTM_SIMD_CALL vector_length(vector4d_arg0 input) RTM_NO_EXCEPT
	{
		return rtm_impl::vector4d_vector_length{ input };
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the length/norm of the vector4.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE scalard RTM_SIMD_CALL vector_length_as_scalar(vector4d_arg0 input) RTM_NO_EXCEPT
	{
		const scalard len_sq = vector_length_squared_as_scalar(input);
		return scalar_sqrt(len_sq);
	}

	namespace rtm_impl
	{
		//////////////////////////////////////////////////////////////////////////
		// This is a helper struct to allow a single consistent API between
		// various vector types when the semantics are identical but the return
		// type differs. Implicit coercion is used to return the desired value
		// at the call site.
		//////////////////////////////////////////////////////////////////////////
		struct vector4d_vector_length3
		{
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator double() const RTM_NO_EXCEPT
			{
				const scalard len_sq = vector_length_squared3_as_scalar(input);
				return scalar_cast(scalar_sqrt(len_sq));
			}

#if defined(RTM_SSE2_INTRINSICS)
			RTM_DEPRECATED("Use 'as_scalar' suffix instead. To be removed in 2.4.")
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator scalard() const RTM_NO_EXCEPT
			{
				const scalard len_sq = vector_length_squared3_as_scalar(input);
				return scalar_sqrt(len_sq);
			}
#endif

			vector4d input;
		};
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the length/norm of the vector3.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr rtm_impl::vector4d_vector_length3 RTM_SIMD_CALL vector_length3(vector4d_arg0 input) RTM_NO_EXCEPT
	{
		return rtm_impl::vector4d_vector_length3{ input };
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the length/norm of the vector3.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE scalard RTM_SIMD_CALL vector_length3_as_scalar(vector4d_arg0 input) RTM_NO_EXCEPT
	{
		const scalard len_sq = vector_length_squared3_as_scalar(input);
		return scalar_sqrt(len_sq);
	}

	namespace rtm_impl
	{
		//////////////////////////////////////////////////////////////////////////
		// This is a helper struct to allow a single consistent API between
		// various vector types when the semantics are identical but the return
		// type differs. Implicit coercion is used to return the desired value
		// at the call site.
		//////////////////////////////////////////////////////////////////////////
		struct vector4d_vector_length_reciprocal
		{
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator double() const RTM_NO_EXCEPT
			{
				const scalard len_sq = vector_length_squared_as_scalar(input);
				return scalar_cast(scalar_sqrt_reciprocal(len_sq));
			}

#if defined(RTM_SSE2_INTRINSICS)
			RTM_DEPRECATED("Use 'as_scalar' suffix instead. To be removed in 2.4.")
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator scalard() const RTM_NO_EXCEPT
			{
				const scalard len_sq = vector_length_squared_as_scalar(input);
				return scalar_sqrt_reciprocal(len_sq);
			}
#endif

			vector4d input;
		};
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the reciprocal length/norm of the vector4.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr rtm_impl::vector4d_vector_length_reciprocal RTM_SIMD_CALL vector_length_reciprocal(vector4d_arg0 input) RTM_NO_EXCEPT
	{
		return rtm_impl::vector4d_vector_length_reciprocal{ input };
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the reciprocal length/norm of the vector4.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE scalard RTM_SIMD_CALL vector_length_reciprocal_as_scalar(vector4d_arg0 input) RTM_NO_EXCEPT
	{
		const scalard len_sq = vector_length_squared_as_scalar(input);
		return scalar_sqrt_reciprocal(len_sq);
	}

	namespace rtm_impl
	{
		//////////////////////////////////////////////////////////////////////////
		// This is a helper struct to allow a single consistent API between
		// various vector types when the semantics are identical but the return
		// type differs. Implicit coercion is used to return the desired value
		// at the call site.
		//////////////////////////////////////////////////////////////////////////
		struct vector4d_vector_length_reciprocal2
		{
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator double() const RTM_NO_EXCEPT
			{
				const scalard len_sq = vector_length_squared2_as_scalar(input);
				return scalar_cast(scalar_sqrt_reciprocal(len_sq));
			}

#if defined(RTM_SSE2_INTRINSICS)
			RTM_DEPRECATED("Use 'as_scalar' suffix instead. To be removed in 2.4.")
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator scalard() const RTM_NO_EXCEPT
			{
				const scalard len_sq = vector_length_squared2_as_scalar(input);
				return scalar_sqrt_reciprocal(len_sq);
			}
#endif

			vector4d input;
		};
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the reciprocal length/norm of the vector2.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr rtm_impl::vector4d_vector_length_reciprocal2 RTM_SIMD_CALL vector_length_reciprocal2(vector4d_arg0 input) RTM_NO_EXCEPT
	{
		return rtm_impl::vector4d_vector_length_reciprocal2{ input };
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the reciprocal length/norm of the vector2.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE scalard RTM_SIMD_CALL vector_length_reciprocal2_as_scalar(vector4d_arg0 input) RTM_NO_EXCEPT
	{
		const scalard len_sq = vector_length_squared2_as_scalar(input);
		return scalar_sqrt_reciprocal(len_sq);
	}

	namespace rtm_impl
	{
		//////////////////////////////////////////////////////////////////////////
		// This is a helper struct to allow a single consistent API between
		// various vector types when the semantics are identical but the return
		// type differs. Implicit coercion is used to return the desired value
		// at the call site.
		//////////////////////////////////////////////////////////////////////////
		struct vector4d_vector_length_reciprocal3
		{
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator double() const RTM_NO_EXCEPT
			{
				const scalard len_sq = vector_length_squared3_as_scalar(input);
				return scalar_cast(scalar_sqrt_reciprocal(len_sq));
			}

#if defined(RTM_SSE2_INTRINSICS)
			RTM_DEPRECATED("Use 'as_scalar' suffix instead. To be removed in 2.4.")
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator scalard() const RTM_NO_EXCEPT
			{
				const scalard len_sq = vector_length_squared3_as_scalar(input);
				return scalar_sqrt_reciprocal(len_sq);
			}
#endif

			vector4d input;
		};
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the reciprocal length/norm of the vector3.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr rtm_impl::vector4d_vector_length_reciprocal3 RTM_SIMD_CALL vector_length_reciprocal3(vector4d_arg0 input) RTM_NO_EXCEPT
	{
		return rtm_impl::vector4d_vector_length_reciprocal3{ input };
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the reciprocal length/norm of the vector3.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE scalard RTM_SIMD_CALL vector_length_reciprocal3_as_scalar(vector4d_arg0 input) RTM_NO_EXCEPT
	{
		const scalard len_sq = vector_length_squared3_as_scalar(input);
		return scalar_sqrt_reciprocal(len_sq);
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the distance between two 3D points.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE rtm_impl::vector4d_vector_length3 RTM_SIMD_CALL vector_distance3(vector4d_arg0 lhs, vector4d_arg1 rhs) RTM_NO_EXCEPT
	{
		const vector4d difference = vector_sub(lhs, rhs);
		return rtm_impl::vector4d_vector_length3{ difference };
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the distance between two 3D points.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE scalard RTM_SIMD_CALL vector_distance3_as_scalar(vector4d_arg0 lhs, vector4d_arg1 rhs) RTM_NO_EXCEPT
	{
		const vector4d difference = vector_sub(lhs, rhs);
		return vector_length3_as_scalar(difference);
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the squared distance between two 3D points.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE rtm_impl::vector4d_vector_dot3 RTM_SIMD_CALL vector_distance_squared3(vector4d_arg0 lhs, vector4d_arg1 rhs) RTM_NO_EXCEPT
	{
		const vector4d difference = vector_sub(lhs, rhs);
		return rtm_impl::vector4d_vector_dot3{ difference, difference };
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the squared distance between two 3D points.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE scalard RTM_SIMD_CALL vector_distance_squared3_as_scalar(vector4d_arg0 lhs, vector4d_arg1 rhs) RTM_NO_EXCEPT
	{
		const vector4d difference = vector_sub(lhs, rhs);
		return vector_length_squared3_as_scalar(difference);
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns a normalized vector2.
	// If the length of the input is not finite or zero, the result is undefined.
	// For a safe alternative, supply a fallback value and a threshold.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4d RTM_SIMD_CALL vector_normalize2(vector4d_arg0 input) RTM_NO_EXCEPT
	{
		// Reciprocal is more accurate to normalize with
		const scalard len_sq = vector_length_squared2_as_scalar(input);
		return vector_mul(input, scalar_sqrt_reciprocal(len_sq));
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns a normalized vector2.
	// If the length of the input is below the supplied threshold, the
	// fall back value is returned instead.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4d RTM_SIMD_CALL vector_normalize2(vector4d_arg0 input, vector4d_arg1 fallback, double threshold = 1.0E-8) RTM_NO_EXCEPT
	{
		// Reciprocal is more accurate to normalize with
		const scalard len_sq = vector_length_squared2_as_scalar(input);
		if (scalar_cast(len_sq) >= threshold)
			return vector_mul(input, scalar_sqrt_reciprocal(len_sq));
		else
			return fallback;
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns a normalized vector3.
	// If the length of the input is not finite or zero, the result is undefined.
	// For a safe alternative, supply a fallback value and a threshold.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4d RTM_SIMD_CALL vector_normalize3(vector4d_arg0 input) RTM_NO_EXCEPT
	{
		// Reciprocal is more accurate to normalize with
		const scalard len_sq = vector_length_squared3_as_scalar(input);
		return vector_mul(input, scalar_sqrt_reciprocal(len_sq));
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns a normalized vector3.
	// If the length of the input is below the supplied threshold, the
	// fall back value is returned instead.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4d RTM_SIMD_CALL vector_normalize3(vector4d_arg0 input, vector4d_arg1 fallback, double threshold = 1.0E-8) RTM_NO_EXCEPT
	{
		// Reciprocal is more accurate to normalize with
		const scalard len_sq = vector_length_squared3_as_scalar(input);
		if (scalar_cast(len_sq) >= threshold)
			return vector_mul(input, scalar_sqrt_reciprocal(len_sq));
		else
			return fallback;
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns a normalized vector4.
	// If the length of the input is not finite or zero, the result is undefined.
	// For a safe alternative, supply a fallback value and a threshold.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4d RTM_SIMD_CALL vector_normalize(vector4d_arg0 input) RTM_NO_EXCEPT
	{
		// Reciprocal is more accurate to normalize with
		const scalard len_sq = vector_length_squared_as_scalar(input);
		return vector_mul(input, scalar_sqrt_reciprocal(len_sq));
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns a normalized vector4.
	// If the length of the input is below the supplied threshold, the
	// fall back value is returned instead.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4d RTM_SIMD_CALL vector_normalize(vector4d_arg0 input, vector4d_arg1 fallback, double threshold = 1.0E-8) RTM_NO_EXCEPT
	{
		// Reciprocal is more accurate to normalize with
		const scalard len_sq = vector_length_squared_as_scalar(input);
		if (scalar_cast(len_sq) >= threshold)
			return vector_mul(input, scalar_sqrt_reciprocal(len_sq));
		else
			return fallback;
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns per component the fractional part of the input.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline vector4d RTM_SIMD_CALL vector_fraction(vector4d_arg0 input) RTM_NO_EXCEPT
	{
		return vector_set(scalar_fraction(vector_get_x(input)), scalar_fraction(vector_get_y(input)), scalar_fraction(vector_get_z(input)), scalar_fraction(vector_get_w(input)));
	}

	//////////////////////////////////////////////////////////////////////////
	// Per component multiplication/addition of the three inputs: v2 + (v0 * v1)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4d RTM_SIMD_CALL vector_mul_add(vector4d_arg0 v0, vector4d_arg1 v1, vector4d_arg2 v2) RTM_NO_EXCEPT
	{
		return vector_add(vector_mul(v0, v1), v2);
	}

	//////////////////////////////////////////////////////////////////////////
	// Per component multiplication/addition of the three inputs: v2 + (v0 * s1)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4d RTM_SIMD_CALL vector_mul_add(vector4d_arg0 v0, double s1, vector4d_arg2 v2) RTM_NO_EXCEPT
	{
		return vector_add(vector_mul(v0, s1), v2);
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Per component multiplication/addition of the three inputs: v2 + (v0 * s1)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4d RTM_SIMD_CALL vector_mul_add(vector4d_arg0 v0, scalard_arg2 s1, vector4d_arg2 v2) RTM_NO_EXCEPT
	{
		return vector_add(vector_mul(v0, s1), v2);
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Per component negative multiplication/subtraction of the three inputs: -((v0 * v1) - v2)
	// This is mathematically equivalent to: v2 - (v0 * v1)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4d RTM_SIMD_CALL vector_neg_mul_sub(vector4d_arg0 v0, vector4d_arg1 v1, vector4d_arg2 v2) RTM_NO_EXCEPT
	{
		return vector_sub(v2, vector_mul(v0, v1));
	}

	//////////////////////////////////////////////////////////////////////////
	// Per component negative multiplication/subtraction of the three inputs: -((v0 * s1) - v2)
	// This is mathematically equivalent to: v2 - (v0 * s1)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4d RTM_SIMD_CALL vector_neg_mul_sub(vector4d_arg0 v0, double s1, vector4d_arg2 v2) RTM_NO_EXCEPT
	{
		return vector_sub(v2, vector_mul(v0, s1));
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Per component negative multiplication/subtraction of the three inputs: -((v0 * s1) - v2)
	// This is mathematically equivalent to: v2 - (v0 * s1)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4d RTM_SIMD_CALL vector_neg_mul_sub(vector4d_arg0 v0, scalard_arg2 s1, vector4d_arg0 v2) RTM_NO_EXCEPT
	{
		return vector_sub(v2, vector_mul(v0, s1));
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Per component linear interpolation of the two inputs at the specified alpha.
	// The formula used is: ((1.0 - alpha) * start) + (alpha * end).
	// Interpolation is stable and will return 'start' when alpha is 0.0 and 'end' when it is 1.0.
	// This is the same instruction count when FMA is present but it might be slightly slower
	// due to the extra multiplication compared to: start + (alpha * (end - start)).
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4d RTM_SIMD_CALL vector_lerp(vector4d_arg0 start, vector4d_arg1 end, double alpha) RTM_NO_EXCEPT
	{
		// ((1.0 - alpha) * start) + (alpha * end) == (start - alpha * start) + (alpha * end)
		return vector_mul_add(end, alpha, vector_neg_mul_sub(start, alpha, start));
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Per component linear interpolation of the two inputs at the specified alpha.
	// The formula used is: ((1.0 - alpha) * start) + (alpha * end).
	// Interpolation is stable and will return 'start' when alpha is 0.0 and 'end' when it is 1.0.
	// This is the same instruction count when FMA is present but it might be slightly slower
	// due to the extra multiplication compared to: start + (alpha * (end - start)).
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4d RTM_SIMD_CALL vector_lerp(vector4d_arg0 start, vector4d_arg1 end, scalard_arg4 alpha) RTM_NO_EXCEPT
	{
		// ((1.0 - alpha) * start) + (alpha * end) == (start - alpha * start) + (alpha * end)
		const vector4d alpha_v = vector_set(alpha);
		return vector_mul_add(end, alpha_v, vector_neg_mul_sub(start, alpha_v, start));
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Per component linear interpolation of the two inputs at the specified alpha.
	// The formula used is: ((1.0 - alpha) * start) + (alpha * end).
	// Interpolation is stable and will return 'start' when alpha is 0.0 and 'end' when it is 1.0.
	// This is the same instruction count when FMA is present but it might be slightly slower
	// due to the extra multiplication compared to: start + (alpha * (end - start)).
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4d RTM_SIMD_CALL vector_lerp(vector4d_arg0 start, vector4d_arg1 end, vector4d_arg2 alpha) RTM_NO_EXCEPT
	{
		// ((1.0 - alpha) * start) + (alpha * end) == (start - alpha * start) + (alpha * end)
		return vector_mul_add(end, alpha, vector_neg_mul_sub(start, alpha, start));
	}



	//////////////////////////////////////////////////////////////////////////
	// Comparisons and masking
	//////////////////////////////////////////////////////////////////////////


	//////////////////////////////////////////////////////////////////////////
	// Returns per component ~0 if equal, otherwise 0: lhs == rhs ? ~0 : 0
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE mask4d RTM_SIMD_CALL vector_equal(vector4d_arg0 lhs, vector4d_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		__m128d xy_lt_pd = _mm_cmpeq_pd(lhs.xy, rhs.xy);
		__m128d zw_lt_pd = _mm_cmpeq_pd(lhs.zw, rhs.zw);
		return mask4d{ xy_lt_pd, zw_lt_pd };
#else
		return mask4d{ rtm_impl::get_mask_value(lhs.x == rhs.x), rtm_impl::get_mask_value(lhs.y == rhs.y), rtm_impl::get_mask_value(lhs.z == rhs.z), rtm_impl::get_mask_value(lhs.w == rhs.w) };
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns per component ~0 if not equal, otherwise 0: lhs != rhs ? ~0 : 0
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE mask4d RTM_SIMD_CALL vector_not_equal(vector4d_arg0 lhs, vector4d_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		__m128d xy_lt_pd = _mm_cmpneq_pd(lhs.xy, rhs.xy);
		__m128d zw_lt_pd = _mm_cmpneq_pd(lhs.zw, rhs.zw);
		return mask4d{ xy_lt_pd, zw_lt_pd };
#else
		return mask4d{ rtm_impl::get_mask_value(lhs.x != rhs.x), rtm_impl::get_mask_value(lhs.y != rhs.y), rtm_impl::get_mask_value(lhs.z != rhs.z), rtm_impl::get_mask_value(lhs.w != rhs.w) };
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns per component ~0 if less than, otherwise 0: lhs < rhs ? ~0 : 0
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE mask4d RTM_SIMD_CALL vector_less_than(vector4d_arg0 lhs, vector4d_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		__m128d xy_lt_pd = _mm_cmplt_pd(lhs.xy, rhs.xy);
		__m128d zw_lt_pd = _mm_cmplt_pd(lhs.zw, rhs.zw);
		return mask4d{xy_lt_pd, zw_lt_pd};
#else
		return mask4d{rtm_impl::get_mask_value(lhs.x < rhs.x), rtm_impl::get_mask_value(lhs.y < rhs.y), rtm_impl::get_mask_value(lhs.z < rhs.z), rtm_impl::get_mask_value(lhs.w < rhs.w)};
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns per component ~0 if less equal, otherwise 0: lhs <= rhs ? ~0 : 0
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE mask4d RTM_SIMD_CALL vector_less_equal(vector4d_arg0 lhs, vector4d_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		__m128d xy_lt_pd = _mm_cmple_pd(lhs.xy, rhs.xy);
		__m128d zw_lt_pd = _mm_cmple_pd(lhs.zw, rhs.zw);
		return mask4d{ xy_lt_pd, zw_lt_pd };
#else
		return mask4d{ rtm_impl::get_mask_value(lhs.x <= rhs.x), rtm_impl::get_mask_value(lhs.y <= rhs.y), rtm_impl::get_mask_value(lhs.z <= rhs.z), rtm_impl::get_mask_value(lhs.w <= rhs.w) };
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns per component ~0 if greater than, otherwise 0: lhs > rhs ? ~0 : 0
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE mask4d RTM_SIMD_CALL vector_greater_than(vector4d_arg0 lhs, vector4d_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		__m128d xy_ge_pd = _mm_cmpgt_pd(lhs.xy, rhs.xy);
		__m128d zw_ge_pd = _mm_cmpgt_pd(lhs.zw, rhs.zw);
		return mask4d{ xy_ge_pd, zw_ge_pd };
#else
		return mask4d{ rtm_impl::get_mask_value(lhs.x > rhs.x), rtm_impl::get_mask_value(lhs.y > rhs.y), rtm_impl::get_mask_value(lhs.z > rhs.z), rtm_impl::get_mask_value(lhs.w > rhs.w) };
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns per component ~0 if greater equal, otherwise 0: lhs >= rhs ? ~0 : 0
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE mask4d RTM_SIMD_CALL vector_greater_equal(vector4d_arg0 lhs, vector4d_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		__m128d xy_ge_pd = _mm_cmpge_pd(lhs.xy, rhs.xy);
		__m128d zw_ge_pd = _mm_cmpge_pd(lhs.zw, rhs.zw);
		return mask4d{ xy_ge_pd, zw_ge_pd };
#else
		return mask4d{ rtm_impl::get_mask_value(lhs.x >= rhs.x), rtm_impl::get_mask_value(lhs.y >= rhs.y), rtm_impl::get_mask_value(lhs.z >= rhs.z), rtm_impl::get_mask_value(lhs.w >= rhs.w) };
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if all 4 components are less than, otherwise false: all(lhs.xyzw < rhs.xyzw)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL vector_all_less_than(vector4d_arg0 lhs, vector4d_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		__m128d xy_lt_pd = _mm_cmplt_pd(lhs.xy, rhs.xy);
		__m128d zw_lt_pd = _mm_cmplt_pd(lhs.zw, rhs.zw);
		return (_mm_movemask_pd(xy_lt_pd) & _mm_movemask_pd(zw_lt_pd)) == 3;
#else
		return lhs.x < rhs.x && lhs.y < rhs.y && lhs.z < rhs.z && lhs.w < rhs.w;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if all [xy] components are less than, otherwise false: all(lhs.xy < rhs.xy)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL vector_all_less_than2(vector4d_arg0 lhs, vector4d_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		__m128d xy_lt_pd = _mm_cmplt_pd(lhs.xy, rhs.xy);
		return _mm_movemask_pd(xy_lt_pd) == 3;
#else
		return lhs.x < rhs.x && lhs.y < rhs.y;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if all [xyz] components are less than, otherwise false: all(lhs.xyz < rhs.xyz)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL vector_all_less_than3(vector4d_arg0 lhs, vector4d_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		__m128d xy_lt_pd = _mm_cmplt_pd(lhs.xy, rhs.xy);
		__m128d zw_lt_pd = _mm_cmplt_pd(lhs.zw, rhs.zw);
		return _mm_movemask_pd(xy_lt_pd) == 3 && (_mm_movemask_pd(zw_lt_pd) & 1) == 1;
#else
		return lhs.x < rhs.x && lhs.y < rhs.y && lhs.z < rhs.z;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if any 4 components are less than, otherwise false: any(lhs.xyzw < rhs.xyzw)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL vector_any_less_than(vector4d_arg0 lhs, vector4d_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		__m128d xy_lt_pd = _mm_cmplt_pd(lhs.xy, rhs.xy);
		__m128d zw_lt_pd = _mm_cmplt_pd(lhs.zw, rhs.zw);
		return (_mm_movemask_pd(xy_lt_pd) | _mm_movemask_pd(zw_lt_pd)) != 0;
#else
		return lhs.x < rhs.x || lhs.y < rhs.y || lhs.z < rhs.z || lhs.w < rhs.w;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if any [xy] components are less than, otherwise false: any(lhs.xy < rhs.xy)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL vector_any_less_than2(vector4d_arg0 lhs, vector4d_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		__m128d xy_lt_pd = _mm_cmplt_pd(lhs.xy, rhs.xy);
		return _mm_movemask_pd(xy_lt_pd) != 0;
#else
		return lhs.x < rhs.x || lhs.y < rhs.y;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if any [xyz] components are less than, otherwise false: any(lhs.xyz < rhs.xyz)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL vector_any_less_than3(vector4d_arg0 lhs, vector4d_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		__m128d xy_lt_pd = _mm_cmplt_pd(lhs.xy, rhs.xy);
		__m128d zw_lt_pd = _mm_cmplt_pd(lhs.zw, rhs.zw);
		return _mm_movemask_pd(xy_lt_pd) != 0 || (_mm_movemask_pd(zw_lt_pd) & 0x1) != 0;
#else
		return lhs.x < rhs.x || lhs.y < rhs.y || lhs.z < rhs.z;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if all 4 components are less equal, otherwise false: all(lhs.xyzw <= rhs.xyzw)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL vector_all_less_equal(vector4d_arg0 lhs, vector4d_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		__m128d xy_le_pd = _mm_cmple_pd(lhs.xy, rhs.xy);
		__m128d zw_le_pd = _mm_cmple_pd(lhs.zw, rhs.zw);
		return (_mm_movemask_pd(xy_le_pd) & _mm_movemask_pd(zw_le_pd)) == 3;
#else
		return lhs.x <= rhs.x && lhs.y <= rhs.y && lhs.z <= rhs.z && lhs.w <= rhs.w;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if all [xy] components are less equal, otherwise false: all(lhs.xy <= rhs.xy)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL vector_all_less_equal2(vector4d_arg0 lhs, vector4d_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		__m128d xy_le_pd = _mm_cmple_pd(lhs.xy, rhs.xy);
		return _mm_movemask_pd(xy_le_pd) == 3;
#else
		return lhs.x <= rhs.x && lhs.y <= rhs.y;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if all [xyz] components are less equal, otherwise false: all(lhs.xyz <= rhs.xyz)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL vector_all_less_equal3(vector4d_arg0 lhs, vector4d_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		__m128d xy_le_pd = _mm_cmple_pd(lhs.xy, rhs.xy);
		__m128d zw_le_pd = _mm_cmple_pd(lhs.zw, rhs.zw);
		return _mm_movemask_pd(xy_le_pd) == 3 && (_mm_movemask_pd(zw_le_pd) & 1) != 0;
#else
		return lhs.x <= rhs.x && lhs.y <= rhs.y && lhs.z <= rhs.z;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if any 4 components are less equal, otherwise false: any(lhs.xyzw <= rhs.xyzw)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL vector_any_less_equal(vector4d_arg0 lhs, vector4d_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		__m128d xy_le_pd = _mm_cmple_pd(lhs.xy, rhs.xy);
		__m128d zw_le_pd = _mm_cmple_pd(lhs.zw, rhs.zw);
		return (_mm_movemask_pd(xy_le_pd) | _mm_movemask_pd(zw_le_pd)) != 0;
#else
		return lhs.x <= rhs.x || lhs.y <= rhs.y || lhs.z <= rhs.z || lhs.w <= rhs.w;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if any [xy] components are less equal, otherwise false: any(lhs.xy <= rhs.xy)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL vector_any_less_equal2(vector4d_arg0 lhs, vector4d_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		__m128d xy_le_pd = _mm_cmple_pd(lhs.xy, rhs.xy);
		return _mm_movemask_pd(xy_le_pd) != 0;
#else
		return lhs.x <= rhs.x || lhs.y <= rhs.y;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if any [xyz] components are less equal, otherwise false: any(lhs.xyz <= rhs.xyz)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL vector_any_less_equal3(vector4d_arg0 lhs, vector4d_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		__m128d xy_le_pd = _mm_cmple_pd(lhs.xy, rhs.xy);
		__m128d zw_le_pd = _mm_cmple_pd(lhs.zw, rhs.zw);
		return _mm_movemask_pd(xy_le_pd) != 0 || (_mm_movemask_pd(zw_le_pd) & 1) != 0;
#else
		return lhs.x <= rhs.x || lhs.y <= rhs.y || lhs.z <= rhs.z;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if all 4 components are greater than, otherwise false: all(lhs.xyzw > rhs.xyzw)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL vector_all_greater_than(vector4d_arg0 lhs, vector4d_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		__m128d xy_ge_pd = _mm_cmpgt_pd(lhs.xy, rhs.xy);
		__m128d zw_ge_pd = _mm_cmpgt_pd(lhs.zw, rhs.zw);
		return (_mm_movemask_pd(xy_ge_pd) & _mm_movemask_pd(zw_ge_pd)) == 3;
#else
		return lhs.x > rhs.x && lhs.y > rhs.y && lhs.z > rhs.z && lhs.w > rhs.w;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if all [xy] components are greater than, otherwise false: all(lhs.xy > rhs.xy)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL vector_all_greater_than2(vector4d_arg0 lhs, vector4d_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		__m128d xy_ge_pd = _mm_cmpgt_pd(lhs.xy, rhs.xy);
		return _mm_movemask_pd(xy_ge_pd) == 3;
#else
		return lhs.x > rhs.x && lhs.y > rhs.y;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if all [xyz] components are greater than, otherwise false: all(lhs.xyz > rhs.xyz)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL vector_all_greater_than3(vector4d_arg0 lhs, vector4d_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		__m128d xy_ge_pd = _mm_cmpgt_pd(lhs.xy, rhs.xy);
		__m128d zw_ge_pd = _mm_cmpgt_pd(lhs.zw, rhs.zw);
		return _mm_movemask_pd(xy_ge_pd) == 3 && (_mm_movemask_pd(zw_ge_pd) & 1) != 0;
#else
		return lhs.x > rhs.x && lhs.y > rhs.y && lhs.z > rhs.z;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if any 4 components are greater than, otherwise false: any(lhs.xyzw > rhs.xyzw)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL vector_any_greater_than(vector4d_arg0 lhs, vector4d_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		__m128d xy_ge_pd = _mm_cmpgt_pd(lhs.xy, rhs.xy);
		__m128d zw_ge_pd = _mm_cmpgt_pd(lhs.zw, rhs.zw);
		return (_mm_movemask_pd(xy_ge_pd) | _mm_movemask_pd(zw_ge_pd)) != 0;
#else
		return lhs.x > rhs.x || lhs.y > rhs.y || lhs.z > rhs.z || lhs.w > rhs.w;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if any [xy] components are greater than, otherwise false: any(lhs.xy > rhs.xy)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL vector_any_greater_than2(vector4d_arg0 lhs, vector4d_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		__m128d xy_ge_pd = _mm_cmpgt_pd(lhs.xy, rhs.xy);
		return _mm_movemask_pd(xy_ge_pd) != 0;
#else
		return lhs.x > rhs.x || lhs.y > rhs.y;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if any [xyz] components are greater than, otherwise false: any(lhs.xyz > rhs.xyz)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL vector_any_greater_than3(vector4d_arg0 lhs, vector4d_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		__m128d xy_ge_pd = _mm_cmpgt_pd(lhs.xy, rhs.xy);
		__m128d zw_ge_pd = _mm_cmpgt_pd(lhs.zw, rhs.zw);
		return _mm_movemask_pd(xy_ge_pd) != 0 || (_mm_movemask_pd(zw_ge_pd) & 1) != 0;
#else
		return lhs.x > rhs.x || lhs.y > rhs.y || lhs.z > rhs.z;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if all 4 components are greater equal, otherwise false: all(lhs.xyzw >= rhs.xyzw)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL vector_all_greater_equal(vector4d_arg0 lhs, vector4d_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		__m128d xy_ge_pd = _mm_cmpge_pd(lhs.xy, rhs.xy);
		__m128d zw_ge_pd = _mm_cmpge_pd(lhs.zw, rhs.zw);
		return (_mm_movemask_pd(xy_ge_pd) & _mm_movemask_pd(zw_ge_pd)) == 3;
#else
		return lhs.x >= rhs.x && lhs.y >= rhs.y && lhs.z >= rhs.z && lhs.w >= rhs.w;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if all [xy] components are greater equal, otherwise false: all(lhs.xy >= rhs.xy)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL vector_all_greater_equal2(vector4d_arg0 lhs, vector4d_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		__m128d xy_ge_pd = _mm_cmpge_pd(lhs.xy, rhs.xy);
		return _mm_movemask_pd(xy_ge_pd) == 3;
#else
		return lhs.x >= rhs.x && lhs.y >= rhs.y;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if all [xyz] components are greater equal, otherwise false: all(lhs.xyz >= rhs.xyz)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL vector_all_greater_equal3(vector4d_arg0 lhs, vector4d_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		__m128d xy_ge_pd = _mm_cmpge_pd(lhs.xy, rhs.xy);
		__m128d zw_ge_pd = _mm_cmpge_pd(lhs.zw, rhs.zw);
		return _mm_movemask_pd(xy_ge_pd) == 3 && (_mm_movemask_pd(zw_ge_pd) & 1) != 0;
#else
		return lhs.x >= rhs.x && lhs.y >= rhs.y && lhs.z >= rhs.z;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if any 4 components are greater equal, otherwise false: any(lhs.xyzw >= rhs.xyzw)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL vector_any_greater_equal(vector4d_arg0 lhs, vector4d_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		__m128d xy_ge_pd = _mm_cmpge_pd(lhs.xy, rhs.xy);
		__m128d zw_ge_pd = _mm_cmpge_pd(lhs.zw, rhs.zw);
		return (_mm_movemask_pd(xy_ge_pd) | _mm_movemask_pd(zw_ge_pd)) != 0;
#else
		return lhs.x >= rhs.x || lhs.y >= rhs.y || lhs.z >= rhs.z || lhs.w >= rhs.w;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if any [xy] components are greater equal, otherwise false: any(lhs.xy >= rhs.xy)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL vector_any_greater_equal2(vector4d_arg0 lhs, vector4d_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		__m128d xy_ge_pd = _mm_cmpge_pd(lhs.xy, rhs.xy);
		return _mm_movemask_pd(xy_ge_pd) != 0;
#else
		return lhs.x >= rhs.x || lhs.y >= rhs.y;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if any [xyz] components are greater equal, otherwise false: any(lhs.xyz >= rhs.xyz)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL vector_any_greater_equal3(vector4d_arg0 lhs, vector4d_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		__m128d xy_ge_pd = _mm_cmpge_pd(lhs.xy, rhs.xy);
		__m128d zw_ge_pd = _mm_cmpge_pd(lhs.zw, rhs.zw);
		return _mm_movemask_pd(xy_ge_pd) != 0 || (_mm_movemask_pd(zw_ge_pd) & 1) != 0;
#else
		return lhs.x >= rhs.x || lhs.y >= rhs.y || lhs.z >= rhs.z;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if all [xyzw] components are equal, otherwise false: all(lhs.xyzw == rhs.xyzw)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL vector_all_equal(vector4d_arg0 lhs, vector4d_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		__m128d xy_eq_pd = _mm_cmpeq_pd(lhs.xy, rhs.xy);
		__m128d zw_eq_pd = _mm_cmpeq_pd(lhs.zw, rhs.zw);
		return (_mm_movemask_pd(xy_eq_pd) & _mm_movemask_pd(zw_eq_pd)) == 3;
#else
		return lhs.x == rhs.x && lhs.y == rhs.y && lhs.z == rhs.z && lhs.w == rhs.w;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if all [xy] components are equal, otherwise false: all(lhs.xy == rhs.xy)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL vector_all_equal2(vector4d_arg0 lhs, vector4d_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		__m128d xy_eq_pd = _mm_cmpeq_pd(lhs.xy, rhs.xy);
		return _mm_movemask_pd(xy_eq_pd) == 3;
#else
		return lhs.x == rhs.x && lhs.y == rhs.y;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if all [xyz] components are equal, otherwise false: all(lhs.xyz == rhs.xyz)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL vector_all_equal3(vector4d_arg0 lhs, vector4d_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		__m128d xy_eq_pd = _mm_cmpeq_pd(lhs.xy, rhs.xy);
		__m128d zw_eq_pd = _mm_cmpeq_pd(lhs.zw, rhs.zw);
		return _mm_movemask_pd(xy_eq_pd) == 3 && (_mm_movemask_pd(zw_eq_pd) & 1) != 0;
#else
		return lhs.x == rhs.x && lhs.y == rhs.y && lhs.z == rhs.z;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if any [xyzw] components are equal, otherwise false: any(lhs.xyzw == rhs.xyzw)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL vector_any_equal(vector4d_arg0 lhs, vector4d_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		__m128d xy_eq_pd = _mm_cmpeq_pd(lhs.xy, rhs.xy);
		__m128d zw_eq_pd = _mm_cmpeq_pd(lhs.zw, rhs.zw);
		return (_mm_movemask_pd(xy_eq_pd) | _mm_movemask_pd(zw_eq_pd)) != 0;
#else
		return lhs.x == rhs.x || lhs.y == rhs.y || lhs.z == rhs.z || lhs.w == rhs.w;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if any [xy] components are equal, otherwise false: any(lhs.xy == rhs.xy)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL vector_any_equal2(vector4d_arg0 lhs, vector4d_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		__m128d xy_eq_pd = _mm_cmpeq_pd(lhs.xy, rhs.xy);
		return _mm_movemask_pd(xy_eq_pd) != 0;
#else
		return lhs.x == rhs.x || lhs.y == rhs.y;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if any [xyz] components are equal, otherwise false: any(lhs.xyz == rhs.xyz)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL vector_any_equal3(vector4d_arg0 lhs, vector4d_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		__m128d xy_eq_pd = _mm_cmpeq_pd(lhs.xy, rhs.xy);
		__m128d zw_eq_pd = _mm_cmpeq_pd(lhs.zw, rhs.zw);
		return _mm_movemask_pd(xy_eq_pd) != 0 || (_mm_movemask_pd(zw_eq_pd) & 1) != 0;
#else
		return lhs.x == rhs.x || lhs.y == rhs.y || lhs.z == rhs.z;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if all [xyzw] components are not equal, otherwise false: all(lhs.xyzw != rhs.xyzw)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL vector_all_not_equal(vector4d_arg0 lhs, vector4d_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		__m128d xy_eq_pd = _mm_cmpneq_pd(lhs.xy, rhs.xy);
		__m128d zw_eq_pd = _mm_cmpneq_pd(lhs.zw, rhs.zw);
		return (_mm_movemask_pd(xy_eq_pd) & _mm_movemask_pd(zw_eq_pd)) == 3;
#else
		return lhs.x != rhs.x && lhs.y != rhs.y && lhs.z != rhs.z && lhs.w != rhs.w;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if all [xy] components are not equal, otherwise false: all(lhs.xy != rhs.xy)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL vector_all_not_equal2(vector4d_arg0 lhs, vector4d_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		__m128d xy_eq_pd = _mm_cmpneq_pd(lhs.xy, rhs.xy);
		return _mm_movemask_pd(xy_eq_pd) == 3;
#else
		return lhs.x != rhs.x && lhs.y != rhs.y;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if all [xyz] components are not equal, otherwise false: all(lhs.xyz != rhs.xyz)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL vector_all_not_equal3(vector4d_arg0 lhs, vector4d_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		__m128d xy_eq_pd = _mm_cmpneq_pd(lhs.xy, rhs.xy);
		__m128d zw_eq_pd = _mm_cmpneq_pd(lhs.zw, rhs.zw);
		return _mm_movemask_pd(xy_eq_pd) == 3 && (_mm_movemask_pd(zw_eq_pd) & 1) != 0;
#else
		return lhs.x != rhs.x && lhs.y != rhs.y && lhs.z != rhs.z;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if any [xyzw] components are not equal, otherwise false: any(lhs.xyzw != rhs.xyzw)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL vector_any_not_equal(vector4d_arg0 lhs, vector4d_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		__m128d xy_eq_pd = _mm_cmpneq_pd(lhs.xy, rhs.xy);
		__m128d zw_eq_pd = _mm_cmpneq_pd(lhs.zw, rhs.zw);
		return (_mm_movemask_pd(xy_eq_pd) | _mm_movemask_pd(zw_eq_pd)) != 0;
#else
		return lhs.x != rhs.x || lhs.y != rhs.y || lhs.z != rhs.z || lhs.w != rhs.w;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if any [xy] components are not equal, otherwise false: any(lhs.xy != rhs.xy)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL vector_any_not_equal2(vector4d_arg0 lhs, vector4d_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		__m128d xy_eq_pd = _mm_cmpneq_pd(lhs.xy, rhs.xy);
		return _mm_movemask_pd(xy_eq_pd) != 0;
#else
		return lhs.x != rhs.x || lhs.y != rhs.y;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if any [xyz] components are not equal, otherwise false: any(lhs.xyz != rhs.xyz)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL vector_any_not_equal3(vector4d_arg0 lhs, vector4d_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		__m128d xy_eq_pd = _mm_cmpneq_pd(lhs.xy, rhs.xy);
		__m128d zw_eq_pd = _mm_cmpneq_pd(lhs.zw, rhs.zw);
		return _mm_movemask_pd(xy_eq_pd) != 0 || (_mm_movemask_pd(zw_eq_pd) & 1) != 0;
#else
		return lhs.x != rhs.x || lhs.y != rhs.y || lhs.z != rhs.z;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if all 4 components are near equal, otherwise false: all(abs(lhs - rhs).xyzw <= threshold)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL vector_all_near_equal(vector4d_arg0 lhs, vector4d_arg1 rhs, double threshold = 0.00001) RTM_NO_EXCEPT
	{
		return vector_all_less_equal(vector_abs(vector_sub(lhs, rhs)), vector_set(threshold));
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if all [xy] components are near equal, otherwise false: all(abs(lhs - rhs).xy <= threshold)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL vector_all_near_equal2(vector4d_arg0 lhs, vector4d_arg1 rhs, double threshold = 0.00001) RTM_NO_EXCEPT
	{
		return vector_all_less_equal2(vector_abs(vector_sub(lhs, rhs)), vector_set(threshold));
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if all [xyz] components are near equal, otherwise false: all(abs(lhs - rhs).xyz <= threshold)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL vector_all_near_equal3(vector4d_arg0 lhs, vector4d_arg1 rhs, double threshold = 0.00001) RTM_NO_EXCEPT
	{
		return vector_all_less_equal3(vector_abs(vector_sub(lhs, rhs)), vector_set(threshold));
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if any 4 components are near equal, otherwise false: any(abs(lhs - rhs).xyzw <= threshold)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL vector_any_near_equal(vector4d_arg0 lhs, vector4d_arg1 rhs, double threshold = 0.00001) RTM_NO_EXCEPT
	{
		return vector_any_less_equal(vector_abs(vector_sub(lhs, rhs)), vector_set(threshold));
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if any [xy] components are near equal, otherwise false: any(abs(lhs - rhs).xy <= threshold)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL vector_any_near_equal2(vector4d_arg0 lhs, vector4d_arg1 rhs, double threshold = 0.00001) RTM_NO_EXCEPT
	{
		return vector_any_less_equal2(vector_abs(vector_sub(lhs, rhs)), vector_set(threshold));
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if any [xyz] components are near equal, otherwise false: any(abs(lhs - rhs).xyz <= threshold)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL vector_any_near_equal3(vector4d_arg0 lhs, vector4d_arg1 rhs, double threshold = 0.00001) RTM_NO_EXCEPT
	{
		return vector_any_less_equal3(vector_abs(vector_sub(lhs, rhs)), vector_set(threshold));
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns per component ~0 if input is finite, otherwise 0: finite(input) ? ~0 : 0
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE mask4d RTM_SIMD_CALL vector_finite(vector4d_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		const __m128i abs_mask = _mm_set_epi64x(0x7FFFFFFFFFFFFFFFULL, 0x7FFFFFFFFFFFFFFFULL);
		__m128d abs_input_xy = _mm_and_pd(input.xy, _mm_castsi128_pd(abs_mask));
		__m128d abs_input_zw = _mm_and_pd(input.zw, _mm_castsi128_pd(abs_mask));

		const __m128d infinity = _mm_set1_pd(std::numeric_limits<double>::infinity());
		__m128d is_not_infinity_xy = _mm_cmpneq_pd(abs_input_xy, infinity);
		__m128d is_not_infinity_zw = _mm_cmpneq_pd(abs_input_zw, infinity);

		__m128d is_nan_xy = _mm_cmpneq_pd(input.xy, input.xy);
		__m128d is_nan_zw = _mm_cmpneq_pd(input.zw, input.zw);

		__m128d is_finite_xy = _mm_andnot_pd(is_nan_xy, is_not_infinity_xy);
		__m128d is_finite_zw = _mm_andnot_pd(is_nan_zw, is_not_infinity_zw);
		return mask4d{ is_finite_xy, is_finite_zw };
#else
		return mask4d{
			rtm_impl::get_mask_value(scalar_is_finite(vector_get_x(input))),
			rtm_impl::get_mask_value(scalar_is_finite(vector_get_y(input))),
			rtm_impl::get_mask_value(scalar_is_finite(vector_get_z(input))),
			rtm_impl::get_mask_value(scalar_is_finite(vector_get_w(input)))
			};
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if all 4 components are finite (not NaN/Inf), otherwise false: all(finite(input))
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL vector_is_finite(vector4d_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		const __m128i abs_mask = _mm_set_epi64x(0x7FFFFFFFFFFFFFFFULL, 0x7FFFFFFFFFFFFFFFULL);
		__m128d abs_input_xy = _mm_and_pd(input.xy, _mm_castsi128_pd(abs_mask));
		__m128d abs_input_zw = _mm_and_pd(input.zw, _mm_castsi128_pd(abs_mask));

		const __m128d infinity = _mm_set1_pd(std::numeric_limits<double>::infinity());
		__m128d is_infinity_xy = _mm_cmpeq_pd(abs_input_xy, infinity);
		__m128d is_infinity_zw = _mm_cmpeq_pd(abs_input_zw, infinity);

		__m128d is_nan_xy = _mm_cmpneq_pd(input.xy, input.xy);
		__m128d is_nan_zw = _mm_cmpneq_pd(input.zw, input.zw);

		__m128d is_not_finite_xy = _mm_or_pd(is_infinity_xy, is_nan_xy);
		__m128d is_not_finite_zw = _mm_or_pd(is_infinity_zw, is_nan_zw);
		__m128d is_not_finite = _mm_or_pd(is_not_finite_xy, is_not_finite_zw);
		return _mm_movemask_pd(is_not_finite) == 0x0;
#else
		return scalar_is_finite(vector_get_x(input)) && scalar_is_finite(vector_get_y(input)) && scalar_is_finite(vector_get_z(input)) && scalar_is_finite(vector_get_w(input));
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if all [xy] components are finite (not NaN/Inf), otherwise false: all(finite(input))
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL vector_is_finite2(vector4d_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		const __m128i abs_mask = _mm_set_epi64x(0x7FFFFFFFFFFFFFFFULL, 0x7FFFFFFFFFFFFFFFULL);
		__m128d abs_input_xy = _mm_and_pd(input.xy, _mm_castsi128_pd(abs_mask));

		const __m128d infinity = _mm_set1_pd(std::numeric_limits<double>::infinity());
		__m128d is_infinity_xy = _mm_cmpeq_pd(abs_input_xy, infinity);

		__m128d is_nan_xy = _mm_cmpneq_pd(input.xy, input.xy);

		__m128d is_not_finite_xy = _mm_or_pd(is_infinity_xy, is_nan_xy);
		return _mm_movemask_pd(is_not_finite_xy) == 0x0;
#else
		return scalar_is_finite(vector_get_x(input)) && scalar_is_finite(vector_get_y(input));
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if all [xyz] components are finite (not NaN/Inf), otherwise false: all(finite(input))
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL vector_is_finite3(vector4d_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		const __m128i abs_mask = _mm_set_epi64x(0x7FFFFFFFFFFFFFFFULL, 0x7FFFFFFFFFFFFFFFULL);
		__m128d abs_input_xy = _mm_and_pd(input.xy, _mm_castsi128_pd(abs_mask));
		__m128d abs_input_zw = _mm_and_pd(input.zw, _mm_castsi128_pd(abs_mask));

		const __m128d infinity = _mm_set1_pd(std::numeric_limits<double>::infinity());
		__m128d is_infinity_xy = _mm_cmpeq_pd(abs_input_xy, infinity);
		__m128d is_infinity_zw = _mm_cmpeq_pd(abs_input_zw, infinity);

		__m128d is_nan_xy = _mm_cmpneq_pd(input.xy, input.xy);
		__m128d is_nan_zw = _mm_cmpneq_pd(input.zw, input.zw);

		__m128d is_not_finite_xy = _mm_or_pd(is_infinity_xy, is_nan_xy);
		__m128d is_not_finite_zw = _mm_or_pd(is_infinity_zw, is_nan_zw);
		return _mm_movemask_pd(is_not_finite_xy) == 0 && (_mm_movemask_pd(is_not_finite_zw) & 0x1) == 0;
#else
		return scalar_is_finite(vector_get_x(input)) && scalar_is_finite(vector_get_y(input)) && scalar_is_finite(vector_get_z(input));
#endif
	}



	//////////////////////////////////////////////////////////////////////////
	// Swizzling, permutations, and mixing
	//////////////////////////////////////////////////////////////////////////


	//////////////////////////////////////////////////////////////////////////
	// Per component selection depending on the mask: mask != 0 ? if_true : if_false
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4d RTM_SIMD_CALL vector_select(mask4d_arg0 mask, vector4d_arg1 if_true, vector4d_arg2 if_false) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		__m128d xy = RTM_VECTOR2D_SELECT(mask.xy, if_true.xy, if_false.xy);
		__m128d zw = RTM_VECTOR2D_SELECT(mask.zw, if_true.zw, if_false.zw);
		return vector4d{ xy, zw };
#else
		return vector4d{ rtm_impl::select(mask.x, if_true.x, if_false.x), rtm_impl::select(mask.y, if_true.y, if_false.y), rtm_impl::select(mask.z, if_true.z, if_false.z), rtm_impl::select(mask.w, if_true.w, if_false.w) };
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Mixes two inputs and returns the desired components.
	// [xyzw] indexes into the first input while [abcd] indexes in the second.
	//////////////////////////////////////////////////////////////////////////
	template<mix4 comp0, mix4 comp1, mix4 comp2, mix4 comp3>
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4d RTM_SIMD_CALL vector_mix(vector4d_arg0 input0, vector4d_arg1 input1) RTM_NO_EXCEPT
	{
		// Exactly input 0
		if (rtm_impl::static_condition<comp0 == mix4::x && comp1 == mix4::y && comp2 == mix4::z && comp3 == mix4::w>::test())
			return input0;

		// Exactly input 1
		if (rtm_impl::static_condition<comp0 == mix4::a && comp1 == mix4::b && comp2 == mix4::c && comp3 == mix4::d>::test())
			return input1;

#if defined(RTM_SSE2_INTRINSICS)
		// We do the same treatment for each half of the result pair

		__m128d result_xy;
		__m128d result_zw;

		if (rtm_impl::static_condition<comp0 == mix4::x && comp1 == mix4::y>::test())
		{
			result_xy = input0.xy;
		}

		else if (rtm_impl::static_condition<comp0 == mix4::z && comp1 == mix4::w>::test())
		{
			result_xy = input0.zw;
		}

		else if (rtm_impl::static_condition<comp0 == mix4::a && comp1 == mix4::b>::test())
		{
			result_xy = input1.xy;
		}

		else if (rtm_impl::static_condition<comp0 == mix4::c && comp1 == mix4::d>::test())
		{
			result_xy = input1.zw;
		}

	#if defined(RTM_SSE4_INTRINSICS)
		// xy0 xy0	duplicate
		// xy0 zw0
		else if (rtm_impl::static_condition<(comp0 == mix4::x || comp0 == mix4::z) && (comp1 == mix4::y || comp1 == mix4::w)>::test())
		{
			constexpr int mask = (comp0 == mix4::z ? 1 : 0) | (comp1 == mix4::w ? 2 : 0);
			result_xy = _mm_blend_pd(input0.xy, input0.zw, mask);
		}

		// xy0 xy1
		else if (rtm_impl::static_condition<(comp0 == mix4::x || comp0 == mix4::a) && (comp1 == mix4::y || comp1 == mix4::b)>::test())
		{
			constexpr int mask = (comp0 == mix4::a ? 1 : 0) | (comp1 == mix4::b ? 2 : 0);
			result_xy = _mm_blend_pd(input0.xy, input1.xy, mask);
		}

		// xy0 zw1
		else if (rtm_impl::static_condition<(comp0 == mix4::x || comp0 == mix4::c) && (comp1 == mix4::y || comp1 == mix4::d)>::test())
		{
			constexpr int mask = (comp0 == mix4::c ? 1 : 0) | (comp1 == mix4::d ? 2 : 0);
			result_xy = _mm_blend_pd(input0.xy, input1.zw, mask);
		}

		// xy1 xy0	duplicate
		// xy1 zw0
		else if (rtm_impl::static_condition<(comp0 == mix4::a || comp0 == mix4::z) && (comp1 == mix4::b || comp1 == mix4::w)>::test())
		{
			constexpr int mask = (comp0 == mix4::z ? 1 : 0) | (comp1 == mix4::w ? 2 : 0);
			result_xy = _mm_blend_pd(input1.xy, input0.zw, mask);
		}

		// xy1 xy1	duplicate
		// xy1 zw1
		else if (rtm_impl::static_condition<(comp0 == mix4::a || comp0 == mix4::c) && (comp1 == mix4::b || comp1 == mix4::d)>::test())
		{
			constexpr int mask = (comp0 == mix4::c ? 1 : 0) | (comp1 == mix4::d ? 2 : 0);
			result_xy = _mm_blend_pd(input1.xy, input1.zw, mask);
		}
	#endif // defined(RTM_SSE4_INTRINSICS)

	#if defined(RTM_SSE3_INTRINSICS)
		else if (rtm_impl::static_condition<comp0 == mix4::x && comp1 == mix4::x>::test())
			result_xy = _mm_movedup_pd(input0.xy);

		else if (rtm_impl::static_condition<comp0 == mix4::z && comp1 == mix4::z>::test())
			result_xy = _mm_movedup_pd(input0.zw);

		else if (rtm_impl::static_condition<comp0 == mix4::a && comp1 == mix4::a>::test())
			result_xy = _mm_movedup_pd(input1.xy);

		else if (rtm_impl::static_condition<comp0 == mix4::c && comp1 == mix4::c>::test())
			result_xy = _mm_movedup_pd(input1.zw);
	#endif // defined(RTM_SSE3_INTRINSICS)

		// xy0 xy0	duplicate
		// xy0 zw0
		else if (rtm_impl::static_condition<comp0 == mix4::z && comp1 == mix4::y>::test())
			result_xy = _mm_move_sd(input0.xy, input0.zw);

		// xy0 xy1
		else if (rtm_impl::static_condition<comp0 == mix4::a && comp1 == mix4::y>::test())
			result_xy = _mm_move_sd(input0.xy, input1.xy);

		// xy0 zw1
		else if (rtm_impl::static_condition<comp0 == mix4::c && comp1 == mix4::y>::test())
			result_xy = _mm_move_sd(input0.xy, input1.zw);

		// xy1 xy0
		else if (rtm_impl::static_condition<comp0 == mix4::x && comp1 == mix4::b>::test())
			result_xy = _mm_move_sd(input1.xy, input0.xy);

		// xy1 zw0
		else if (rtm_impl::static_condition<comp0 == mix4::z && comp1 == mix4::b>::test())
			result_xy = _mm_move_sd(input1.xy, input0.zw);

		// xy1 xy1	duplicate
		// xy1 zw1
		else if (rtm_impl::static_condition<comp0 == mix4::c && comp1 == mix4::b>::test())
			result_xy = _mm_move_sd(input1.xy, input1.zw);

		// zw0 xy0
		else if (rtm_impl::static_condition<comp0 == mix4::x && comp1 == mix4::w>::test())
			result_xy = _mm_move_sd(input0.zw, input0.xy);

		// zw0 zw0 duplicate
		// zw0 xy1
		else if (rtm_impl::static_condition<comp0 == mix4::a && comp1 == mix4::w>::test())
			result_xy = _mm_move_sd(input0.zw, input1.xy);

		// zw0 zw1
		else if (rtm_impl::static_condition<comp0 == mix4::c && comp1 == mix4::w>::test())
			result_xy = _mm_move_sd(input0.zw, input1.zw);

		// zw1 xy0
		else if (rtm_impl::static_condition<comp0 == mix4::x && comp1 == mix4::d>::test())
			result_xy = _mm_move_sd(input1.zw, input0.xy);

		// zw1 zw0
		else if (rtm_impl::static_condition<comp0 == mix4::z && comp1 == mix4::d>::test())
			result_xy = _mm_move_sd(input1.zw, input0.zw);

		// zw1 xy1
		// zw1 zw1 duplicate
		else if (rtm_impl::static_condition<comp0 == mix4::a && comp1 == mix4::d>::test())
			result_xy = _mm_move_sd(input1.zw, input1.xy);

		// xy0 xy0
		else if (rtm_impl::static_condition<comp0 == mix4::y && comp1 == mix4::y>::test())
			result_xy = _mm_unpackhi_pd(input0.xy, input0.xy);

		// xy0 zw0
		else if (rtm_impl::static_condition<comp0 == mix4::y && comp1 == mix4::w>::test())
			result_xy = _mm_unpackhi_pd(input0.xy, input0.zw);

		// xy0 xy1
		else if (rtm_impl::static_condition<comp0 == mix4::y && comp1 == mix4::b>::test())
			result_xy = _mm_unpackhi_pd(input0.xy, input1.xy);

		// xy0 zw1
		else if (rtm_impl::static_condition<comp0 == mix4::y && comp1 == mix4::d>::test())
			result_xy = _mm_unpackhi_pd(input0.xy, input1.zw);

		// xy1 xy0
		else if (rtm_impl::static_condition<comp0 == mix4::b && comp1 == mix4::y>::test())
			result_xy = _mm_unpackhi_pd(input1.xy, input0.xy);

		// xy1 zw0
		else if (rtm_impl::static_condition<comp0 == mix4::b && comp1 == mix4::w>::test())
			result_xy = _mm_unpackhi_pd(input1.xy, input0.zw);

		// xy1 xy1
		else if (rtm_impl::static_condition<comp0 == mix4::b && comp1 == mix4::b>::test())
			result_xy = _mm_unpackhi_pd(input1.xy, input1.xy);

		// xy1 zw1
		else if (rtm_impl::static_condition<comp0 == mix4::b && comp1 == mix4::d>::test())
			result_xy = _mm_unpackhi_pd(input1.xy, input1.zw);

		// zw0 xy0
		else if (rtm_impl::static_condition<comp0 == mix4::w && comp1 == mix4::y>::test())
			result_xy = _mm_unpackhi_pd(input0.zw, input0.xy);

		// zw0 zw0
		else if (rtm_impl::static_condition<comp0 == mix4::w && comp1 == mix4::w>::test())
			result_xy = _mm_unpackhi_pd(input0.zw, input0.zw);

		// zw0 xy1
		else if (rtm_impl::static_condition<comp0 == mix4::w && comp1 == mix4::b>::test())
			result_xy = _mm_unpackhi_pd(input0.zw, input1.xy);

		// zw0 zw1
		else if (rtm_impl::static_condition<comp0 == mix4::w && comp1 == mix4::d>::test())
			result_xy = _mm_unpackhi_pd(input0.zw, input1.zw);

		// zw1 xy0
		else if (rtm_impl::static_condition<comp0 == mix4::d && comp1 == mix4::y>::test())
			result_xy = _mm_unpackhi_pd(input1.zw, input0.xy);

		// zw1 zw0
		else if (rtm_impl::static_condition<comp0 == mix4::d && comp1 == mix4::w>::test())
			result_xy = _mm_unpackhi_pd(input1.zw, input0.zw);

		// zw1 xy1
		else if (rtm_impl::static_condition<comp0 == mix4::d && comp1 == mix4::b>::test())
			result_xy = _mm_unpackhi_pd(input1.zw, input1.xy);

		// zw1 zw1
		else if (rtm_impl::static_condition<comp0 == mix4::d && comp1 == mix4::d>::test())
			result_xy = _mm_unpackhi_pd(input1.zw, input1.zw);

		// xy0 xy0
		else if (rtm_impl::static_condition<comp0 == mix4::x && comp1 == mix4::x>::test())
			result_xy = _mm_unpacklo_pd(input0.xy, input0.xy);

		// xy0 zw0
		else if (rtm_impl::static_condition<comp0 == mix4::x && comp1 == mix4::z>::test())
			result_xy = _mm_unpacklo_pd(input0.xy, input0.zw);

		// xy0 xy1
		else if (rtm_impl::static_condition<comp0 == mix4::x && comp1 == mix4::a>::test())
			result_xy = _mm_unpacklo_pd(input0.xy, input1.xy);

		// xy0 zw1
		else if (rtm_impl::static_condition<comp0 == mix4::x && comp1 == mix4::c>::test())
			result_xy = _mm_unpacklo_pd(input0.xy, input1.zw);

		// xy1 xy0
		else if (rtm_impl::static_condition<comp0 == mix4::a && comp1 == mix4::x>::test())
			result_xy = _mm_unpacklo_pd(input1.xy, input0.xy);

		// xy1 zw0
		else if (rtm_impl::static_condition<comp0 == mix4::a && comp1 == mix4::z>::test())
			result_xy = _mm_unpacklo_pd(input1.xy, input0.zw);

		// xy1 xy1
		else if (rtm_impl::static_condition<comp0 == mix4::a && comp1 == mix4::a>::test())
			result_xy = _mm_unpacklo_pd(input1.xy, input1.xy);

		// xy1 zw1
		else if (rtm_impl::static_condition<comp0 == mix4::a && comp1 == mix4::c>::test())
			result_xy = _mm_unpacklo_pd(input1.xy, input1.zw);

		// zw0 xy0
		else if (rtm_impl::static_condition<comp0 == mix4::z && comp1 == mix4::x>::test())
			result_xy = _mm_unpacklo_pd(input0.zw, input0.xy);

		// zw0 zw0
		else if (rtm_impl::static_condition<comp0 == mix4::z && comp1 == mix4::z>::test())
			result_xy = _mm_unpacklo_pd(input0.zw, input0.zw);

		// zw0 xy1
		else if (rtm_impl::static_condition<comp0 == mix4::z && comp1 == mix4::a>::test())
			result_xy = _mm_unpacklo_pd(input0.zw, input1.xy);

		// zw0 zw1
		else if (rtm_impl::static_condition<comp0 == mix4::z && comp1 == mix4::c>::test())
			result_xy = _mm_unpacklo_pd(input0.zw, input1.zw);

		// zw1 xy0
		else if (rtm_impl::static_condition<comp0 == mix4::c && comp1 == mix4::x>::test())
			result_xy = _mm_unpacklo_pd(input1.zw, input0.xy);

		// zw1 zw0
		else if (rtm_impl::static_condition<comp0 == mix4::c && comp1 == mix4::z>::test())
			result_xy = _mm_unpacklo_pd(input1.zw, input0.zw);

		// zw1 xy1
		else if (rtm_impl::static_condition<comp0 == mix4::c && comp1 == mix4::a>::test())
			result_xy = _mm_unpacklo_pd(input1.zw, input1.xy);

		// zw1 zw1
		else if (rtm_impl::static_condition<comp0 == mix4::c && comp1 == mix4::c>::test())
			result_xy = _mm_unpacklo_pd(input1.zw, input1.zw);

		// xy0 xy0
		else if (rtm_impl::static_condition<(comp0 == mix4::x || comp0 == mix4::y) && (comp1 == mix4::x || comp1 == mix4::y)>::test())
			result_xy = _mm_shuffle_pd(input0.xy, input0.xy, _MM_SHUFFLE2(int(comp1) % 2, int(comp0) % 2));

		// xy0 zw0
		else if (rtm_impl::static_condition<(comp0 == mix4::x || comp0 == mix4::y) && (comp1 == mix4::z || comp1 == mix4::w)>::test())
			result_xy = _mm_shuffle_pd(input0.xy, input0.zw, _MM_SHUFFLE2(int(comp1) % 2, int(comp0) % 2));

		// xy0 xy1
		else if (rtm_impl::static_condition<(comp0 == mix4::x || comp0 == mix4::y) && (comp1 == mix4::a || comp1 == mix4::b)>::test())
			result_xy = _mm_shuffle_pd(input0.xy, input1.xy, _MM_SHUFFLE2(int(comp1) % 2, int(comp0) % 2));

		// xy0 zw1
		else if (rtm_impl::static_condition<(comp0 == mix4::x || comp0 == mix4::y) && (comp1 == mix4::c || comp1 == mix4::d)>::test())
			result_xy = _mm_shuffle_pd(input0.xy, input1.zw, _MM_SHUFFLE2(int(comp1) % 2, int(comp0) % 2));

		// xy1 xy0
		else if (rtm_impl::static_condition<(comp0 == mix4::a || comp0 == mix4::b) && (comp1 == mix4::x || comp1 == mix4::y)>::test())
			result_xy = _mm_shuffle_pd(input1.xy, input0.xy, _MM_SHUFFLE2(int(comp1) % 2, int(comp0) % 2));

		// xy1 zw0
		else if (rtm_impl::static_condition<(comp0 == mix4::a || comp0 == mix4::b) && (comp1 == mix4::z || comp1 == mix4::w)>::test())
			result_xy = _mm_shuffle_pd(input1.xy, input0.zw, _MM_SHUFFLE2(int(comp1) % 2, int(comp0) % 2));

		// xy1 xy1
		else if (rtm_impl::static_condition<(comp0 == mix4::a || comp0 == mix4::b) && (comp1 == mix4::a || comp1 == mix4::b)>::test())
			result_xy = _mm_shuffle_pd(input1.xy, input1.xy, _MM_SHUFFLE2(int(comp1) % 2, int(comp0) % 2));

		// xy1 zw1
		else if (rtm_impl::static_condition<(comp0 == mix4::a || comp0 == mix4::b) && (comp1 == mix4::c || comp1 == mix4::d)>::test())
			result_xy = _mm_shuffle_pd(input1.xy, input1.zw, _MM_SHUFFLE2(int(comp1) % 2, int(comp0) % 2));

		// zw0 xy0
		else if (rtm_impl::static_condition<(comp0 == mix4::z || comp0 == mix4::w) && (comp1 == mix4::x || comp1 == mix4::y)>::test())
			result_xy = _mm_shuffle_pd(input0.zw, input0.xy, _MM_SHUFFLE2(int(comp1) % 2, int(comp0) % 2));

		// zw0 zw0
		else if (rtm_impl::static_condition<(comp0 == mix4::z || comp0 == mix4::w) && (comp1 == mix4::z || comp1 == mix4::w)>::test())
			result_xy = _mm_shuffle_pd(input0.zw, input0.zw, _MM_SHUFFLE2(int(comp1) % 2, int(comp0) % 2));

		// zw0 xy1
		else if (rtm_impl::static_condition<(comp0 == mix4::z || comp0 == mix4::w) && (comp1 == mix4::a || comp1 == mix4::b)>::test())
			result_xy = _mm_shuffle_pd(input0.zw, input1.xy, _MM_SHUFFLE2(int(comp1) % 2, int(comp0) % 2));

		// zw0 zw1
		else if (rtm_impl::static_condition<(comp0 == mix4::z || comp0 == mix4::w) && (comp1 == mix4::c || comp1 == mix4::d)>::test())
			result_xy = _mm_shuffle_pd(input0.zw, input1.zw, _MM_SHUFFLE2(int(comp1) % 2, int(comp0) % 2));

		// zw1 xy0
		else if (rtm_impl::static_condition<(comp0 == mix4::c || comp0 == mix4::d) && (comp1 == mix4::x || comp1 == mix4::y)>::test())
			result_xy = _mm_shuffle_pd(input1.zw, input0.xy, _MM_SHUFFLE2(int(comp1) % 2, int(comp0) % 2));

		// zw1 zw0
		else if (rtm_impl::static_condition<(comp0 == mix4::c || comp0 == mix4::d) && (comp1 == mix4::z || comp1 == mix4::w)>::test())
			result_xy = _mm_shuffle_pd(input1.zw, input0.zw, _MM_SHUFFLE2(int(comp1) % 2, int(comp0) % 2));

		// zw1 xy1
		else if (rtm_impl::static_condition<(comp0 == mix4::c || comp0 == mix4::d) && (comp1 == mix4::a || comp1 == mix4::b)>::test())
			result_xy = _mm_shuffle_pd(input1.zw, input1.xy, _MM_SHUFFLE2(int(comp1) % 2, int(comp0) % 2));

		// zw1 zw1
		else if (rtm_impl::static_condition<(comp0 == mix4::c || comp0 == mix4::d) && (comp1 == mix4::c || comp1 == mix4::d)>::test())
			result_xy = _mm_shuffle_pd(input1.zw, input1.zw, _MM_SHUFFLE2(int(comp1) % 2, int(comp0) % 2));

		else
			result_xy = _mm_setzero_pd();	// Set a dummy value, never used

		if (rtm_impl::static_condition<comp2 == mix4::x && comp3 == mix4::y>::test())
		{
			result_zw = input0.xy;
		}

		else if (rtm_impl::static_condition<comp2 == mix4::z && comp3 == mix4::w>::test())
		{
			result_zw = input0.zw;
		}

		else if (rtm_impl::static_condition<comp2 == mix4::a && comp3 == mix4::b>::test())
		{
			result_zw = input1.xy;
		}

		else if (rtm_impl::static_condition<comp2 == mix4::c && comp3 == mix4::d>::test())
		{
			result_zw = input1.zw;
		}

	#if defined(RTM_SSE4_INTRINSICS)
		// xy0 xy0	duplicate
		// xy0 zw0
		else if (rtm_impl::static_condition<(comp2 == mix4::x || comp2 == mix4::z) && (comp3 == mix4::y || comp3 == mix4::w)>::test())
		{
			constexpr int mask = (comp2 == mix4::z ? 1 : 0) | (comp3 == mix4::w ? 2 : 0);
			result_zw = _mm_blend_pd(input0.xy, input0.zw, mask);
		}

		// xy0 xy1
		else if (rtm_impl::static_condition<(comp2 == mix4::x || comp2 == mix4::a) && (comp3 == mix4::y || comp3 == mix4::b)>::test())
		{
			constexpr int mask = (comp2 == mix4::a ? 1 : 0) | (comp3 == mix4::b ? 2 : 0);
			result_zw = _mm_blend_pd(input0.xy, input1.xy, mask);
		}

		// xy0 zw1
		else if (rtm_impl::static_condition<(comp2 == mix4::x || comp2 == mix4::c) && (comp3 == mix4::y || comp3 == mix4::d)>::test())
		{
			constexpr int mask = (comp2 == mix4::c ? 1 : 0) | (comp3 == mix4::d ? 2 : 0);
			result_zw = _mm_blend_pd(input0.xy, input1.zw, mask);
		}

		// xy1 xy0	duplicate
		// xy1 zw0
		else if (rtm_impl::static_condition<(comp2 == mix4::a || comp2 == mix4::z) && (comp3 == mix4::b || comp3 == mix4::w)>::test())
		{
			constexpr int mask = (comp2 == mix4::z ? 1 : 0) | (comp3 == mix4::w ? 2 : 0);
			result_zw = _mm_blend_pd(input1.xy, input0.zw, mask);
		}

		// xy1 xy1	duplicate
		// xy1 zw1
		else if (rtm_impl::static_condition<(comp2 == mix4::a || comp2 == mix4::c) && (comp3 == mix4::b || comp3 == mix4::d)>::test())
		{
			constexpr int mask = (comp2 == mix4::c ? 1 : 0) | (comp3 == mix4::d ? 2 : 0);
			result_zw = _mm_blend_pd(input1.xy, input1.zw, mask);
		}
	#endif // defined(RTM_SSE4_INTRINSICS)

	#if defined(RTM_SSE3_INTRINSICS)
		else if (rtm_impl::static_condition<comp2 == mix4::x && comp3 == mix4::x>::test())
			result_zw = _mm_movedup_pd(input0.xy);

		else if (rtm_impl::static_condition<comp2 == mix4::z && comp3 == mix4::z>::test())
			result_zw = _mm_movedup_pd(input0.zw);

		else if (rtm_impl::static_condition<comp2 == mix4::a && comp3 == mix4::a>::test())
			result_zw = _mm_movedup_pd(input1.xy);

		else if (rtm_impl::static_condition<comp2 == mix4::c && comp3 == mix4::c>::test())
			result_zw = _mm_movedup_pd(input1.zw);
	#endif // defined(RTM_SSE3_INTRINSICS)

		// xy0 xy0	duplicate
		// xy0 zw0
		else if (rtm_impl::static_condition<comp2 == mix4::z && comp3 == mix4::y>::test())
			result_zw = _mm_move_sd(input0.xy, input0.zw);

		// xy0 xy1
		else if (rtm_impl::static_condition<comp2 == mix4::a && comp3 == mix4::y>::test())
			result_zw = _mm_move_sd(input0.xy, input1.xy);

		// xy0 zw1
		else if (rtm_impl::static_condition<comp2 == mix4::c && comp3 == mix4::y>::test())
			result_zw = _mm_move_sd(input0.xy, input1.zw);

		// xy1 xy0
		else if (rtm_impl::static_condition<comp2 == mix4::x && comp3 == mix4::b>::test())
			result_zw = _mm_move_sd(input1.xy, input0.xy);

		// xy1 zw0
		else if (rtm_impl::static_condition<comp2 == mix4::z && comp3 == mix4::b>::test())
			result_zw = _mm_move_sd(input1.xy, input0.zw);

		// xy1 xy1	duplicate
		// xy1 zw1
		else if (rtm_impl::static_condition<comp2 == mix4::c && comp3 == mix4::b>::test())
			result_zw = _mm_move_sd(input1.xy, input1.zw);

		// zw0 xy0
		else if (rtm_impl::static_condition<comp2 == mix4::x && comp3 == mix4::w>::test())
			result_zw = _mm_move_sd(input0.zw, input0.xy);

		// zw0 zw0 duplicate
		// zw0 xy1
		else if (rtm_impl::static_condition<comp2 == mix4::a && comp3 == mix4::w>::test())
			result_zw = _mm_move_sd(input0.zw, input1.xy);

		// zw0 zw1
		else if (rtm_impl::static_condition<comp2 == mix4::c && comp3 == mix4::w>::test())
			result_zw = _mm_move_sd(input0.zw, input1.zw);

		// zw1 xy0
		else if (rtm_impl::static_condition<comp2 == mix4::x && comp3 == mix4::d>::test())
			result_zw = _mm_move_sd(input1.zw, input0.xy);

		// zw1 zw0
		else if (rtm_impl::static_condition<comp2 == mix4::z && comp3 == mix4::d>::test())
			result_zw = _mm_move_sd(input1.zw, input0.zw);

		// zw1 xy1
		// zw1 zw1 duplicate
		else if (rtm_impl::static_condition<comp2 == mix4::a && comp3 == mix4::d>::test())
			result_zw = _mm_move_sd(input1.zw, input1.xy);

		// xy0 xy0
		else if (rtm_impl::static_condition<comp2 == mix4::y && comp3 == mix4::y>::test())
			result_zw = _mm_unpackhi_pd(input0.xy, input0.xy);

		// xy0 zw0
		else if (rtm_impl::static_condition<comp2 == mix4::y && comp3 == mix4::w>::test())
			result_zw = _mm_unpackhi_pd(input0.xy, input0.zw);

		// xy0 xy1
		else if (rtm_impl::static_condition<comp2 == mix4::y && comp3 == mix4::b>::test())
			result_zw = _mm_unpackhi_pd(input0.xy, input1.xy);

		// xy0 zw1
		else if (rtm_impl::static_condition<comp2 == mix4::y && comp3 == mix4::d>::test())
			result_zw = _mm_unpackhi_pd(input0.xy, input1.zw);

		// xy1 xy0
		else if (rtm_impl::static_condition<comp2 == mix4::b && comp3 == mix4::y>::test())
			result_zw = _mm_unpackhi_pd(input1.xy, input0.xy);

		// xy1 zw0
		else if (rtm_impl::static_condition<comp2 == mix4::b && comp3 == mix4::w>::test())
			result_zw = _mm_unpackhi_pd(input1.xy, input0.zw);

		// xy1 xy1
		else if (rtm_impl::static_condition<comp2 == mix4::b && comp3 == mix4::b>::test())
			result_zw = _mm_unpackhi_pd(input1.xy, input1.xy);

		// xy1 zw1
		else if (rtm_impl::static_condition<comp2 == mix4::b && comp3 == mix4::d>::test())
			result_zw = _mm_unpackhi_pd(input1.xy, input1.zw);

		// zw0 xy0
		else if (rtm_impl::static_condition<comp2 == mix4::w && comp3 == mix4::y>::test())
			result_zw = _mm_unpackhi_pd(input0.zw, input0.xy);

		// zw0 zw0
		else if (rtm_impl::static_condition<comp2 == mix4::w && comp3 == mix4::w>::test())
			result_zw = _mm_unpackhi_pd(input0.zw, input0.zw);

		// zw0 xy1
		else if (rtm_impl::static_condition<comp2 == mix4::w && comp3 == mix4::b>::test())
			result_zw = _mm_unpackhi_pd(input0.zw, input1.xy);

		// zw0 zw1
		else if (rtm_impl::static_condition<comp2 == mix4::w && comp3 == mix4::d>::test())
			result_zw = _mm_unpackhi_pd(input0.zw, input1.zw);

		// zw1 xy0
		else if (rtm_impl::static_condition<comp2 == mix4::d && comp3 == mix4::y>::test())
			result_zw = _mm_unpackhi_pd(input1.zw, input0.xy);

		// zw1 zw0
		else if (rtm_impl::static_condition<comp2 == mix4::d && comp3 == mix4::w>::test())
			result_zw = _mm_unpackhi_pd(input1.zw, input0.zw);

		// zw1 xy1
		else if (rtm_impl::static_condition<comp2 == mix4::d && comp3 == mix4::b>::test())
			result_zw = _mm_unpackhi_pd(input1.zw, input1.xy);

		// zw1 zw1
		else if (rtm_impl::static_condition<comp2 == mix4::d && comp3 == mix4::d>::test())
			result_zw = _mm_unpackhi_pd(input1.zw, input1.zw);

		// xy0 xy0
		else if (rtm_impl::static_condition<comp2 == mix4::x && comp3 == mix4::x>::test())
			result_zw = _mm_unpacklo_pd(input0.xy, input0.xy);

		// xy0 zw0
		else if (rtm_impl::static_condition<comp2 == mix4::x && comp3 == mix4::z>::test())
			result_zw = _mm_unpacklo_pd(input0.xy, input0.zw);

		// xy0 xy1
		else if (rtm_impl::static_condition<comp2 == mix4::x && comp3 == mix4::a>::test())
			result_zw = _mm_unpacklo_pd(input0.xy, input1.xy);

		// xy0 zw1
		else if (rtm_impl::static_condition<comp2 == mix4::x && comp3 == mix4::c>::test())
			result_zw = _mm_unpacklo_pd(input0.xy, input1.zw);

		// xy1 xy0
		else if (rtm_impl::static_condition<comp2 == mix4::a && comp3 == mix4::x>::test())
			result_zw = _mm_unpacklo_pd(input1.xy, input0.xy);

		// xy1 zw0
		else if (rtm_impl::static_condition<comp2 == mix4::a && comp3 == mix4::z>::test())
			result_zw = _mm_unpacklo_pd(input1.xy, input0.zw);

		// xy1 xy1
		else if (rtm_impl::static_condition<comp2 == mix4::a && comp3 == mix4::a>::test())
			result_zw = _mm_unpacklo_pd(input1.xy, input1.xy);

		// xy1 zw1
		else if (rtm_impl::static_condition<comp2 == mix4::a && comp3 == mix4::c>::test())
			result_zw = _mm_unpacklo_pd(input1.xy, input1.zw);

		// zw0 xy0
		else if (rtm_impl::static_condition<comp2 == mix4::z && comp3 == mix4::x>::test())
			result_zw = _mm_unpacklo_pd(input0.zw, input0.xy);

		// zw0 zw0
		else if (rtm_impl::static_condition<comp2 == mix4::z && comp3 == mix4::z>::test())
			result_zw = _mm_unpacklo_pd(input0.zw, input0.zw);

		// zw0 xy1
		else if (rtm_impl::static_condition<comp2 == mix4::z && comp3 == mix4::a>::test())
			result_zw = _mm_unpacklo_pd(input0.zw, input1.xy);

		// zw0 zw1
		else if (rtm_impl::static_condition<comp2 == mix4::z && comp3 == mix4::c>::test())
			result_zw = _mm_unpacklo_pd(input0.zw, input1.zw);

		// zw1 xy0
		else if (rtm_impl::static_condition<comp2 == mix4::c && comp3 == mix4::x>::test())
			result_zw = _mm_unpacklo_pd(input1.zw, input0.xy);

		// zw1 zw0
		else if (rtm_impl::static_condition<comp2 == mix4::c && comp3 == mix4::z>::test())
			result_zw = _mm_unpacklo_pd(input1.zw, input0.zw);

		// zw1 xy1
		else if (rtm_impl::static_condition<comp2 == mix4::c && comp3 == mix4::a>::test())
			result_zw = _mm_unpacklo_pd(input1.zw, input1.xy);

		// zw1 zw1
		else if (rtm_impl::static_condition<comp2 == mix4::c && comp3 == mix4::c>::test())
			result_zw = _mm_unpacklo_pd(input1.zw, input1.zw);

		// xy0 xy0
		else if (rtm_impl::static_condition<(comp2 == mix4::x || comp2 == mix4::y) && (comp3 == mix4::x || comp3 == mix4::y)>::test())
			result_zw = _mm_shuffle_pd(input0.xy, input0.xy, _MM_SHUFFLE2(int(comp3) % 2, int(comp2) % 2));

		// xy0 zw0
		else if (rtm_impl::static_condition<(comp2 == mix4::x || comp2 == mix4::y) && (comp3 == mix4::z || comp3 == mix4::w)>::test())
			result_zw = _mm_shuffle_pd(input0.xy, input0.zw, _MM_SHUFFLE2(int(comp3) % 2, int(comp2) % 2));

		// xy0 xy1
		else if (rtm_impl::static_condition<(comp2 == mix4::x || comp2 == mix4::y) && (comp3 == mix4::a || comp3 == mix4::b)>::test())
			result_zw = _mm_shuffle_pd(input0.xy, input1.xy, _MM_SHUFFLE2(int(comp3) % 2, int(comp2) % 2));

		// xy0 zw1
		else if (rtm_impl::static_condition<(comp2 == mix4::x || comp2 == mix4::y) && (comp3 == mix4::c || comp3 == mix4::d)>::test())
			result_zw = _mm_shuffle_pd(input0.xy, input1.zw, _MM_SHUFFLE2(int(comp3) % 2, int(comp2) % 2));

		// xy1 xy0
		else if (rtm_impl::static_condition<(comp2 == mix4::a || comp2 == mix4::b) && (comp3 == mix4::x || comp3 == mix4::y)>::test())
			result_zw = _mm_shuffle_pd(input1.xy, input0.xy, _MM_SHUFFLE2(int(comp3) % 2, int(comp2) % 2));

		// xy1 zw0
		else if (rtm_impl::static_condition<(comp2 == mix4::a || comp2 == mix4::b) && (comp3 == mix4::z || comp3 == mix4::w)>::test())
			result_zw = _mm_shuffle_pd(input1.xy, input0.zw, _MM_SHUFFLE2(int(comp3) % 2, int(comp2) % 2));

		// xy1 xy1
		else if (rtm_impl::static_condition<(comp2 == mix4::a || comp2 == mix4::b) && (comp3 == mix4::a || comp3 == mix4::b)>::test())
			result_zw = _mm_shuffle_pd(input1.xy, input1.xy, _MM_SHUFFLE2(int(comp3) % 2, int(comp2) % 2));

		// xy1 zw1
		else if (rtm_impl::static_condition<(comp2 == mix4::a || comp2 == mix4::b) && (comp3 == mix4::c || comp3 == mix4::d)>::test())
			result_zw = _mm_shuffle_pd(input1.xy, input1.zw, _MM_SHUFFLE2(int(comp3) % 2, int(comp2) % 2));

		// zw0 xy0
		else if (rtm_impl::static_condition<(comp2 == mix4::z || comp2 == mix4::w) && (comp3 == mix4::x || comp3 == mix4::y)>::test())
			result_zw = _mm_shuffle_pd(input0.zw, input0.xy, _MM_SHUFFLE2(int(comp3) % 2, int(comp2) % 2));

		// zw0 zw0
		else if (rtm_impl::static_condition<(comp2 == mix4::z || comp2 == mix4::w) && (comp3 == mix4::z || comp3 == mix4::w)>::test())
			result_zw = _mm_shuffle_pd(input0.zw, input0.zw, _MM_SHUFFLE2(int(comp3) % 2, int(comp2) % 2));

		// zw0 xy1
		else if (rtm_impl::static_condition<(comp2 == mix4::z || comp2 == mix4::w) && (comp3 == mix4::a || comp3 == mix4::b)>::test())
			result_zw = _mm_shuffle_pd(input0.zw, input1.xy, _MM_SHUFFLE2(int(comp3) % 2, int(comp2) % 2));

		// zw0 zw1
		else if (rtm_impl::static_condition<(comp2 == mix4::z || comp2 == mix4::w) && (comp3 == mix4::c || comp3 == mix4::d)>::test())
			result_zw = _mm_shuffle_pd(input0.zw, input1.zw, _MM_SHUFFLE2(int(comp3) % 2, int(comp2) % 2));

		// zw1 xy0
		else if (rtm_impl::static_condition<(comp2 == mix4::c || comp2 == mix4::d) && (comp3 == mix4::x || comp3 == mix4::y)>::test())
			result_zw = _mm_shuffle_pd(input1.zw, input0.xy, _MM_SHUFFLE2(int(comp3) % 2, int(comp2) % 2));

		// zw1 zw0
		else if (rtm_impl::static_condition<(comp2 == mix4::c || comp2 == mix4::d) && (comp3 == mix4::z || comp3 == mix4::w)>::test())
			result_zw = _mm_shuffle_pd(input1.zw, input0.zw, _MM_SHUFFLE2(int(comp3) % 2, int(comp2) % 2));

		// zw1 xy1
		else if (rtm_impl::static_condition<(comp2 == mix4::c || comp2 == mix4::d) && (comp3 == mix4::a || comp3 == mix4::b)>::test())
			result_zw = _mm_shuffle_pd(input1.zw, input1.xy, _MM_SHUFFLE2(int(comp3) % 2, int(comp2) % 2));

		// zw1 zw1
		else if (rtm_impl::static_condition<(comp2 == mix4::c || comp2 == mix4::d) && (comp3 == mix4::c || comp3 == mix4::d)>::test())
			result_zw = _mm_shuffle_pd(input1.zw, input1.zw, _MM_SHUFFLE2(int(comp3) % 2, int(comp2) % 2));

		else
			result_zw = _mm_setzero_pd();	// Set a dummy value, never used


		return vector4d{ result_xy, result_zw };
#else
		// Non-simd variant
		constexpr component4 component0 = rtm_impl::mix_to_component(comp0);
		constexpr component4 component1 = rtm_impl::mix_to_component(comp1);
		constexpr component4 component2 = rtm_impl::mix_to_component(comp2);
		constexpr component4 component3 = rtm_impl::mix_to_component(comp3);

		const double x = rtm_impl::is_mix_xyzw(comp0) ? vector_get_component(input0, component0) : vector_get_component(input1, component0);
		const double y = rtm_impl::is_mix_xyzw(comp1) ? vector_get_component(input0, component1) : vector_get_component(input1, component1);
		const double z = rtm_impl::is_mix_xyzw(comp2) ? vector_get_component(input0, component2) : vector_get_component(input1, component2);
		const double w = rtm_impl::is_mix_xyzw(comp3) ? vector_get_component(input0, component3) : vector_get_component(input1, component3);

		return vector_set(x, y, z, w);
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Replicates the [x] component in all components.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4d RTM_SIMD_CALL vector_dup_x(vector4d_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		__m128d value = _mm_shuffle_pd(input.xy, input.xy, _MM_SHUFFLE2(0, 0));
		return vector4d{ value, value };
#else
		return vector4d{ input.x, input.x, input.x, input.x };
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Replicates the [y] component in all components.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4d RTM_SIMD_CALL vector_dup_y(vector4d_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		__m128d value = _mm_shuffle_pd(input.xy, input.xy, _MM_SHUFFLE2(1, 1));
		return vector4d{ value, value };
#else
		return vector4d{ input.y, input.y, input.y, input.y };
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Replicates the [z] component in all components.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4d RTM_SIMD_CALL vector_dup_z(vector4d_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		__m128d value = _mm_shuffle_pd(input.zw, input.zw, _MM_SHUFFLE2(0, 0));
		return vector4d{ value, value };
#else
		return vector4d{ input.z, input.z, input.z, input.z };
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Replicates the [w] component in all components.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4d RTM_SIMD_CALL vector_dup_w(vector4d_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		__m128d value = _mm_shuffle_pd(input.zw, input.zw, _MM_SHUFFLE2(1, 1));
		return vector4d{ value, value };
#else
		return vector4d{ input.w, input.w, input.w, input.w };
#endif
	}


	//////////////////////////////////////////////////////////////////////////
	// Logical
	//////////////////////////////////////////////////////////////////////////

	//////////////////////////////////////////////////////////////////////////
	// Per component logical AND between the inputs: input0 & input1
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline vector4d RTM_SIMD_CALL vector_and(vector4d_arg0 input0, vector4d_arg1 input1) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		__m128d xy = _mm_and_pd(input0.xy, input1.xy);
		__m128d zw = _mm_and_pd(input0.zw, input1.zw);
		return vector4d{ xy, zw };
#else
		const uint64_t* input0_ = rtm_impl::bit_cast<const uint64_t*>(&input0);
		const uint64_t* input1_ = rtm_impl::bit_cast<const uint64_t*>(&input1);

		vector4d result = input0;
		uint64_t* result_ = rtm_impl::bit_cast<uint64_t*>(&result);

		result_[0] = input0_[0] & input1_[0];
		result_[1] = input0_[1] & input1_[1];
		result_[2] = input0_[2] & input1_[2];
		result_[3] = input0_[3] & input1_[3];

		return result;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Per component logical OR between the inputs: input0 | input1
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline vector4d RTM_SIMD_CALL vector_or(vector4d_arg0 input0, vector4d_arg1 input1) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		__m128d xy = _mm_or_pd(input0.xy, input1.xy);
		__m128d zw = _mm_or_pd(input0.zw, input1.zw);
		return vector4d{ xy, zw };
#else
		const uint64_t* input0_ = rtm_impl::bit_cast<const uint64_t*>(&input0);
		const uint64_t* input1_ = rtm_impl::bit_cast<const uint64_t*>(&input1);

		vector4d result = input0;
		uint64_t* result_ = rtm_impl::bit_cast<uint64_t*>(&result);

		result_[0] = input0_[0] | input1_[0];
		result_[1] = input0_[1] | input1_[1];
		result_[2] = input0_[2] | input1_[2];
		result_[3] = input0_[3] | input1_[3];

		return result;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Per component logical XOR between the inputs: input0 ^ input1
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline vector4d RTM_SIMD_CALL vector_xor(vector4d_arg0 input0, vector4d_arg1 input1) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		__m128d xy = _mm_xor_pd(input0.xy, input1.xy);
		__m128d zw = _mm_xor_pd(input0.zw, input1.zw);
		return vector4d{ xy, zw };
#else
		const uint64_t* input0_ = rtm_impl::bit_cast<const uint64_t*>(&input0);
		const uint64_t* input1_ = rtm_impl::bit_cast<const uint64_t*>(&input1);

		vector4d result = input0;
		uint64_t* result_ = rtm_impl::bit_cast<uint64_t*>(&result);

		result_[0] = input0_[0] ^ input1_[0];
		result_[1] = input0_[1] ^ input1_[1];
		result_[2] = input0_[2] ^ input1_[2];
		result_[3] = input0_[3] ^ input1_[3];

		return result;
#endif
	}


	//////////////////////////////////////////////////////////////////////////
	// Miscellaneous
	//////////////////////////////////////////////////////////////////////////


	//////////////////////////////////////////////////////////////////////////
	// Returns per component the sign of the input vector: input >= 0.0 ? 1.0 : -1.0
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4d RTM_SIMD_CALL vector_sign(vector4d_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		const __m128d signs = _mm_set1_pd(-0.0);
		const __m128d one = _mm_set1_pd(1.0);
		// Mask out the sign bit
		const __m128d sign_bits_xy = _mm_and_pd(input.xy, signs);
		const __m128d sign_bits_zw = _mm_and_pd(input.zw, signs);
		// Copy the sign bit onto +-1.0f
		return vector4d{ _mm_or_pd(sign_bits_xy, one), _mm_or_pd(sign_bits_zw, one) };
#else
		const mask4d mask = vector_greater_equal(input, vector_zero());
		return vector_select(mask, vector_set(1.0), vector_set(-1.0));
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns per component the input with the sign of the control value.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4d RTM_SIMD_CALL vector_copy_sign(vector4d_arg0 input, vector4d_arg1 control_sign) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		const __m128d sign_bit = _mm_set1_pd(-0.0);
		__m128d signs_xy = _mm_and_pd(sign_bit, control_sign.xy);
		__m128d signs_zw = _mm_and_pd(sign_bit, control_sign.zw);
		__m128d abs_input_xy = _mm_andnot_pd(sign_bit, input.xy);
		__m128d abs_input_zw = _mm_andnot_pd(sign_bit, input.zw);
		__m128d xy = _mm_or_pd(abs_input_xy, signs_xy);
		__m128d zw = _mm_or_pd(abs_input_zw, signs_zw);
		return vector4d{ xy, zw };
#else
		double x = vector_get_x(input);
		double y = vector_get_y(input);
		double z = vector_get_z(input);
		double w = vector_get_w(input);

		double x_sign = vector_get_x(control_sign);
		double y_sign = vector_get_y(control_sign);
		double z_sign = vector_get_z(control_sign);
		double w_sign = vector_get_w(control_sign);

		return vector_set(rtm_impl::copysign(x, x_sign), rtm_impl::copysign(y, y_sign), rtm_impl::copysign(z, z_sign), rtm_impl::copysign(w, w_sign));
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns per component the rounded input using a symmetric algorithm.
	// vector_round_symmetric(1.5) = 2.0
	// vector_round_symmetric(1.2) = 1.0
	// vector_round_symmetric(-1.5) = -2.0
	// vector_round_symmetric(-1.2) = -1.0
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline vector4d RTM_SIMD_CALL vector_round_symmetric(vector4d_arg0 input) RTM_NO_EXCEPT
	{
		// NaN, +- Infinity, and numbers larger or equal to 2^23 remain unchanged
		// since they have no fractional part.

#if defined(RTM_SSE4_INTRINSICS)
		__m128d zero = _mm_setzero_pd();
		__m128d is_positive_xy = _mm_cmpge_pd(input.xy, zero);
		__m128d is_positive_zw = _mm_cmpge_pd(input.zw, zero);

		const __m128d sign_mask = _mm_set_pd(-0.0, -0.0);
		__m128d sign_xy = _mm_andnot_pd(is_positive_xy, sign_mask);
		__m128d sign_zw = _mm_andnot_pd(is_positive_zw, sign_mask);

		// For positive values, we add a bias of 0.5.
		// For negative values, we add a bias of -0.5.
		__m128d half = _mm_set1_pd(0.5);
		__m128d bias_xy = _mm_or_pd(sign_xy, half);
		__m128d bias_zw = _mm_or_pd(sign_zw, half);
		__m128d biased_input_xy = _mm_add_pd(input.xy, bias_xy);
		__m128d biased_input_zw = _mm_add_pd(input.zw, bias_zw);

		__m128d floored_xy = _mm_floor_pd(biased_input_xy);
		__m128d floored_zw = _mm_floor_pd(biased_input_zw);
		__m128d ceiled_xy = _mm_ceil_pd(biased_input_xy);
		__m128d ceiled_zw = _mm_ceil_pd(biased_input_zw);

		__m128d result_xy = RTM_VECTOR2D_SELECT(is_positive_xy, floored_xy, ceiled_xy);
		__m128d result_zw = RTM_VECTOR2D_SELECT(is_positive_zw, floored_zw, ceiled_zw);
		return vector4d{ result_xy, result_zw };
#elif defined(RTM_SSE2_INTRINSICS)
		const __m128i abs_mask = _mm_set_epi64x(0x7FFFFFFFFFFFFFFFULL, 0x7FFFFFFFFFFFFFFFULL);
		const __m128d fractional_limit = _mm_set1_pd(4503599627370496.0); // 2^52

		// Build our mask, larger values that have no fractional part, and infinities will be true
		// Smaller values and NaN will be false
		__m128d abs_input_xy = _mm_and_pd(input.xy, _mm_castsi128_pd(abs_mask));
		__m128d abs_input_zw = _mm_and_pd(input.zw, _mm_castsi128_pd(abs_mask));
		__m128d is_input_large_xy = _mm_cmpge_pd(abs_input_xy, fractional_limit);
		__m128d is_input_large_zw = _mm_cmpge_pd(abs_input_zw, fractional_limit);

		// Test if our input is NaN with (value != value), it is only true for NaN
		__m128d is_nan_xy = _mm_cmpneq_pd(input.xy, input.xy);
		__m128d is_nan_zw = _mm_cmpneq_pd(input.zw, input.zw);

		// Combine our masks to determine if we should return the original value
		__m128d use_original_input_xy = _mm_or_pd(is_input_large_xy, is_nan_xy);
		__m128d use_original_input_zw = _mm_or_pd(is_input_large_zw, is_nan_zw);

		const __m128d sign_mask = _mm_set_pd(-0.0, -0.0);
		__m128d sign_xy = _mm_and_pd(input.xy, sign_mask);
		__m128d sign_zw = _mm_and_pd(input.zw, sign_mask);

		// For positive values, we add a bias of 0.5.
		// For negative values, we add a bias of -0.5.
		__m128d half = _mm_set1_pd(0.5);
		__m128d bias_xy = _mm_or_pd(sign_xy, half);
		__m128d bias_zw = _mm_or_pd(sign_zw, half);
		__m128d biased_input_xy = _mm_add_pd(input.xy, bias_xy);
		__m128d biased_input_zw = _mm_add_pd(input.zw, bias_zw);

		// Convert to an integer with truncation and back, this rounds towards zero.
		__m128d integer_part_xy = _mm_cvtepi32_pd(_mm_cvttpd_epi32(biased_input_xy));
		__m128d integer_part_zw = _mm_cvtepi32_pd(_mm_cvttpd_epi32(biased_input_zw));

		__m128d result_xy = _mm_or_pd(_mm_and_pd(use_original_input_xy, input.xy), _mm_andnot_pd(use_original_input_xy, integer_part_xy));
		__m128d result_zw = _mm_or_pd(_mm_and_pd(use_original_input_zw, input.zw), _mm_andnot_pd(use_original_input_zw, integer_part_zw));

		return vector4d{ result_xy, result_zw };
#else
		const vector4d half = vector_set(0.5);
		const vector4d floored = vector_floor(vector_add(input, half));
		const vector4d ceiled = vector_ceil(vector_sub(input, half));
		const mask4d is_greater_equal = vector_greater_equal(input, vector_zero());
		return vector_select(is_greater_equal, floored, ceiled);
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns per component the rounded input using banker's rounding (half to even).
	// vector_round_bankers(2.5) = 2.0
	// vector_round_bankers(1.5) = 2.0
	// vector_round_bankers(1.2) = 1.0
	// vector_round_bankers(-2.5) = -2.0
	// vector_round_bankers(-1.5) = -2.0
	// vector_round_bankers(-1.2) = -1.0
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4d RTM_SIMD_CALL vector_round_bankers(vector4d_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE4_INTRINSICS)
		return vector4d{ _mm_round_pd(input.xy, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC), _mm_round_pd(input.zw, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC) };
#elif defined(RTM_SSE2_INTRINSICS)
		const __m128i abs_mask = _mm_set_epi64x(0x7FFFFFFFFFFFFFFFULL, 0x7FFFFFFFFFFFFFFFULL);

		const __m128d sign_mask = _mm_set_pd(-0.0, -0.0);
		__m128d sign_xy = _mm_and_pd(input.xy, sign_mask);
		__m128d sign_zw = _mm_and_pd(input.zw, sign_mask);

		// We add the largest integer that a 64 bit floating point number can represent and subtract it afterwards.
		// This relies on the fact that if we had a fractional part, the new value cannot be represented accurately
		// and IEEE 754 will perform rounding for us. The default rounding mode is Banker's rounding.
		// This has the effect of removing the fractional part while simultaneously rounding.
		// Use the same sign as the input value to make sure we handle positive and negative values.
		const __m128d fractional_limit = _mm_set1_pd(4503599627370496.0); // 2^52
		__m128d truncating_offset_xy = _mm_or_pd(sign_xy, fractional_limit);
		__m128d truncating_offset_zw = _mm_or_pd(sign_zw, fractional_limit);
		__m128d integer_part_xy = _mm_sub_pd(_mm_add_pd(input.xy, truncating_offset_xy), truncating_offset_xy);
		__m128d integer_part_zw = _mm_sub_pd(_mm_add_pd(input.zw, truncating_offset_zw), truncating_offset_zw);

		__m128d abs_input_xy = _mm_and_pd(input.xy, _mm_castsi128_pd(abs_mask));
		__m128d abs_input_zw = _mm_and_pd(input.zw, _mm_castsi128_pd(abs_mask));
		__m128d is_input_large_xy = _mm_cmpge_pd(abs_input_xy, fractional_limit);
		__m128d is_input_large_zw = _mm_cmpge_pd(abs_input_zw, fractional_limit);
		__m128d result_xy = _mm_or_pd(_mm_and_pd(is_input_large_xy, input.xy), _mm_andnot_pd(is_input_large_xy, integer_part_xy));
		__m128d result_zw = _mm_or_pd(_mm_and_pd(is_input_large_zw, input.zw), _mm_andnot_pd(is_input_large_zw, integer_part_zw));
		return vector4d{ result_xy, result_zw };
#else
		scalard x = scalar_round_bankers(scalard(vector_get_x(input)));
		scalard y = scalar_round_bankers(scalard(vector_get_y(input)));
		scalard z = scalar_round_bankers(scalard(vector_get_z(input)));
		scalard w = scalar_round_bankers(scalard(vector_get_w(input)));
		return vector_set(x, y, z, w);
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns per component the sine of the input angle.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline vector4d RTM_SIMD_CALL vector_sin(vector4d_arg0 input) RTM_NO_EXCEPT
	{
		scalard x = scalar_sin(vector_get_x_as_scalar(input));
		scalard y = scalar_sin(vector_get_y_as_scalar(input));
		scalard z = scalar_sin(vector_get_z_as_scalar(input));
		scalard w = scalar_sin(vector_get_w_as_scalar(input));
		return vector_set(x, y, z, w);
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns per component the arc-sine of the input.
	// Input value must be in the range [-1.0, 1.0].
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline vector4d RTM_SIMD_CALL vector_asin(vector4d_arg0 input) RTM_NO_EXCEPT
	{
		scalard x = scalar_asin(vector_get_x_as_scalar(input));
		scalard y = scalar_asin(vector_get_y_as_scalar(input));
		scalard z = scalar_asin(vector_get_z_as_scalar(input));
		scalard w = scalar_asin(vector_get_w_as_scalar(input));
		return vector_set(x, y, z, w);
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns per component the cosine of the input angle.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline vector4d RTM_SIMD_CALL vector_cos(vector4d_arg0 input) RTM_NO_EXCEPT
	{
		scalard x = scalar_cos(vector_get_x_as_scalar(input));
		scalard y = scalar_cos(vector_get_y_as_scalar(input));
		scalard z = scalar_cos(vector_get_z_as_scalar(input));
		scalard w = scalar_cos(vector_get_w_as_scalar(input));
		return vector_set(x, y, z, w);
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns per component the arc-cosine of the input.
	// Input value must be in the range [-1.0, 1.0].
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline vector4d RTM_SIMD_CALL vector_acos(vector4d_arg0 input) RTM_NO_EXCEPT
	{
		scalard x = scalar_acos(vector_get_x_as_scalar(input));
		scalard y = scalar_acos(vector_get_y_as_scalar(input));
		scalard z = scalar_acos(vector_get_z_as_scalar(input));
		scalard w = scalar_acos(vector_get_w_as_scalar(input));
		return vector_set(x, y, z, w);
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns per component the sine and cosine of the input angle.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline void RTM_SIMD_CALL vector_sincos(vector4d_arg0 input, vector4d& out_sine, vector4d& out_cosine) RTM_NO_EXCEPT
	{
		const vector4d x = scalar_sincos(vector_get_x_as_scalar(input));
		const vector4d y = scalar_sincos(vector_get_y_as_scalar(input));
		const vector4d z = scalar_sincos(vector_get_z_as_scalar(input));
		const vector4d w = scalar_sincos(vector_get_w_as_scalar(input));

		const vector4d cos_xy = vector_mix<mix4::y, mix4::b, mix4::y, mix4::b>(x, y);
		const vector4d cos_zw = vector_mix<mix4::y, mix4::b, mix4::y, mix4::b>(z, w);
		out_cosine = vector_mix<mix4::x, mix4::y, mix4::a, mix4::b>(cos_xy, cos_zw);

		const vector4d sin_xy = vector_mix<mix4::x, mix4::a, mix4::x, mix4::a>(x, y);
		const vector4d sin_zw = vector_mix<mix4::x, mix4::a, mix4::x, mix4::a>(z, w);
		out_sine = vector_mix<mix4::x, mix4::y, mix4::a, mix4::b>(sin_xy, sin_zw);
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns per component the tangent of the input angle.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline vector4d RTM_SIMD_CALL vector_tan(vector4d_arg0 angle) RTM_NO_EXCEPT
	{
		// Use the identity: tan(angle) = sin(angle) / cos(angle)
		vector4d sin_;
		vector4d cos_;
		vector_sincos(angle, sin_, cos_);

		const mask4d is_cos_zero = vector_equal(cos_, vector_zero());
		const vector4d signed_infinity = vector_copy_sign(vector_set(std::numeric_limits<double>::infinity()), angle);
		const vector4d result = vector_div(sin_, cos_);
		return vector_select(is_cos_zero, signed_infinity, result);
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns per component the arc-tangent of the input.
	// Note that due to the sign ambiguity, atan cannot determine which quadrant
	// the value resides in.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline vector4d RTM_SIMD_CALL vector_atan(vector4d_arg0 input) RTM_NO_EXCEPT
	{
		scalard x = scalar_atan(vector_get_x_as_scalar(input));
		scalard y = scalar_atan(vector_get_y_as_scalar(input));
		scalard z = scalar_atan(vector_get_z_as_scalar(input));
		scalard w = scalar_atan(vector_get_w_as_scalar(input));
		return vector_set(x, y, z, w);
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns per component the arc-tangent of [y/x] using the sign of the arguments to
	// determine the correct quadrant.
	// Y represents the proportion of the y-coordinate.
	// X represents the proportion of the x-coordinate.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline vector4d RTM_SIMD_CALL vector_atan2(vector4d_arg0 y, vector4d_arg1 x) RTM_NO_EXCEPT
	{
		scalard x_ = scalar_atan2(vector_get_x_as_scalar(y), vector_get_x_as_scalar(x));
		scalard y_ = scalar_atan2(vector_get_y_as_scalar(y), vector_get_y_as_scalar(x));
		scalard z_ = scalar_atan2(vector_get_z_as_scalar(y), vector_get_z_as_scalar(x));
		scalard w_ = scalar_atan2(vector_get_w_as_scalar(y), vector_get_w_as_scalar(x));
		return vector_set(x_, y_, z_, w_);
	}

	RTM_IMPL_VERSION_NAMESPACE_END
}

RTM_IMPL_FILE_PRAGMA_POP

```

`includes/rtm/vector4f.h`:

```h
#pragma once

////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2017 Nicholas Frechette & Animation Compression Library contributors
// Copyright (c) 2018 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include "rtm/macros.h"
#include "rtm/math.h"
#include "rtm/scalarf.h"
#include "rtm/version.h"
#include "rtm/impl/bit_cast.impl.h"
#include "rtm/impl/compiler_utils.h"
#include "rtm/impl/macros.mask4.impl.h"
#include "rtm/impl/memory_utils.h"
#include "rtm/impl/vector_common.h"

#include <cstring>
#include <limits>

RTM_IMPL_FILE_PRAGMA_PUSH

namespace rtm
{
	RTM_IMPL_VERSION_NAMESPACE_BEGIN

	//////////////////////////////////////////////////////////////////////////
	// Setters, getters, and casts
	//////////////////////////////////////////////////////////////////////////


	//////////////////////////////////////////////////////////////////////////
	// Loads an unaligned vector4 from memory.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4f RTM_SIMD_CALL vector_load(const float* input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return _mm_loadu_ps(input);
#elif defined(RTM_NEON_INTRINSICS)
		return vld1q_f32(input);
#else
		return vector_set(input[0], input[1], input[2], input[3]);
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Loads an input scalar from memory into the [x] component and sets the [yzw] components to zero.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4f RTM_SIMD_CALL vector_load1(const float* input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return _mm_load_ss(input);
#elif defined(RTM_NEON_INTRINSICS)
		return vld1q_lane_f32(input, vdupq_n_f32(0.0F), 0);
#else
		return vector_set(input[0], 0.0F, 0.0F, 0.0F);
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Loads an unaligned vector2 from memory and sets the [zw] components to zero.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4f RTM_SIMD_CALL vector_load2(const float* input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return _mm_castpd_ps(_mm_load_sd(rtm_impl::bit_cast<const double*>(input)));
#elif defined(RTM_NEON_INTRINSICS)
		const float32x2_t xy = vld1_f32(input);
		return vcombine_f32(xy, vdup_n_f32(0.0F));
#else
		return vector_set(input[0], input[1], 0.0F, 0.0F);
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Loads an unaligned vector3 from memory and sets the [w] component to zero.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4f RTM_SIMD_CALL vector_load3(const float* input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		const __m128 xy = _mm_castpd_ps(_mm_load_sd(rtm_impl::bit_cast<const double*>(input)));
		const __m128 z = _mm_load_ss(input + 2);
		return _mm_movelh_ps(xy, z);
#elif defined(RTM_NEON_INTRINSICS)
		const float32x2_t xy = vld1_f32(input);
		const float32x2_t z = vld1_lane_f32(input + 2, vdup_n_f32(0.0F), 0);
		return vcombine_f32(xy, z);
#else
		return vector_set(input[0], input[1], input[2], 0.0F);
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Loads an unaligned vector4 from memory.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4f RTM_SIMD_CALL vector_load(const float4f* input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return _mm_loadu_ps(&input->x);
#elif defined(RTM_NEON_INTRINSICS)
		return vld1q_f32(&input->x);
#else
		return vector_set(input->x, input->y, input->z, input->w);
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Loads an unaligned vector2 from memory and sets the [zw] components to zero.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4f RTM_SIMD_CALL vector_load2(const float2f* input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return _mm_castpd_ps(_mm_load_sd(rtm_impl::bit_cast<const double*>(&input->x)));
#elif defined(RTM_NEON_INTRINSICS)
		const float32x2_t xy = vld1_f32(&input->x);
		return vcombine_f32(xy, vdup_n_f32(0.0F));
#else
		return vector_set(input->x, input->y, 0.0F, 0.0F);
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Loads an unaligned vector3 from memory and sets the [w] component to zero.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4f RTM_SIMD_CALL vector_load3(const float3f* input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		const __m128 xy = _mm_castpd_ps(_mm_load_sd(rtm_impl::bit_cast<const double*>(&input->x)));
		const __m128 z = _mm_load_ss(&input->z);
		return _mm_movelh_ps(xy, z);
#elif defined(RTM_NEON_INTRINSICS)
		const float32x2_t xy = vld1_f32(&input->x);
		const float32x2_t z = vld1_lane_f32(&input->z, vdup_n_f32(0.0F), 0);
		return vcombine_f32(xy, z);
#else
		return vector_set(input->x, input->y, input->z, 0.0F);
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Loads an input scalar from memory into the [xyzw] components.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4f RTM_SIMD_CALL vector_broadcast(const float* input) RTM_NO_EXCEPT
	{
#if defined(RTM_AVX_INTRINSICS)
		return _mm_broadcast_ss(input);
#elif defined(RTM_SSE2_INTRINSICS)
		return _mm_load_ps1(input);
#elif defined(RTM_NEON_INTRINSICS)
		return vld1q_dup_f32(input);
#else
		return vector_set(*input);
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Casts a quaternion to a vector4.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4f RTM_SIMD_CALL quat_to_vector(quatf_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS) || defined(RTM_NEON_INTRINSICS)
		return input;
#else
		return vector4f{ input.x, input.y, input.z, input.w };
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Casts a vector4 float64 variant to a float32 variant.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4f RTM_SIMD_CALL vector_cast(const vector4d& input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return _mm_shuffle_ps(_mm_cvtpd_ps(input.xy), _mm_cvtpd_ps(input.zw), _MM_SHUFFLE(1, 0, 1, 0));
#else
		return vector_set(float(input.x), float(input.y), float(input.z), float(input.w));
#endif
	}

	namespace rtm_impl
	{
		//////////////////////////////////////////////////////////////////////////
		// This is a helper struct to allow a single consistent API between
		// various vector types when the semantics are identical but the return
		// type differs. Implicit coercion is used to return the desired value
		// at the call site.
		//////////////////////////////////////////////////////////////////////////
		struct vector4f_vector_get_x
		{
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator float() const RTM_NO_EXCEPT
			{
#if defined(RTM_SSE2_INTRINSICS)
				return _mm_cvtss_f32(input);
#elif defined(RTM_NEON_INTRINSICS)
				return vgetq_lane_f32(input, 0);
#else
				return input.x;
#endif
			}

#if defined(RTM_SSE2_INTRINSICS)
			RTM_DEPRECATED("Use 'as_scalar' suffix instead. To be removed in 2.4.")
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator scalarf() const RTM_NO_EXCEPT
			{
				return scalarf{ input };
			}
#endif

			vector4f input;
		};
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the vector4 [x] component.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr rtm_impl::vector4f_vector_get_x RTM_SIMD_CALL vector_get_x(vector4f_arg0 input) RTM_NO_EXCEPT
	{
		return rtm_impl::vector4f_vector_get_x{ input };
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the vector4 [x] component.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE scalarf RTM_SIMD_CALL vector_get_x_as_scalar(vector4f_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return scalarf{ input };
#else
		return vector_get_x(input);
#endif
	}

	namespace rtm_impl
	{
		//////////////////////////////////////////////////////////////////////////
		// This is a helper struct to allow a single consistent API between
		// various vector types when the semantics are identical but the return
		// type differs. Implicit coercion is used to return the desired value
		// at the call site.
		//////////////////////////////////////////////////////////////////////////
		struct vector4f_vector_get_y
		{
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator float() const RTM_NO_EXCEPT
			{
#if defined(RTM_SSE2_INTRINSICS)
				return _mm_cvtss_f32(_mm_shuffle_ps(input, input, _MM_SHUFFLE(1, 1, 1, 1)));
#elif defined(RTM_NEON_INTRINSICS)
				return vgetq_lane_f32(input, 1);
#else
				return input.y;
#endif
			}

#if defined(RTM_SSE2_INTRINSICS)
			RTM_DEPRECATED("Use 'as_scalar' suffix instead. To be removed in 2.4.")
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator scalarf() const RTM_NO_EXCEPT
			{
				return scalarf{ _mm_shuffle_ps(input, input, _MM_SHUFFLE(1, 1, 1, 1)) };
			}
#endif

			vector4f input;
		};
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the vector4 [y] component.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr rtm_impl::vector4f_vector_get_y RTM_SIMD_CALL vector_get_y(vector4f_arg0 input) RTM_NO_EXCEPT
	{
		return rtm_impl::vector4f_vector_get_y{ input };
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the vector4 [y] component.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE scalarf RTM_SIMD_CALL vector_get_y_as_scalar(vector4f_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return scalarf{ _mm_shuffle_ps(input, input, _MM_SHUFFLE(1, 1, 1, 1)) };
#else
		return vector_get_y(input);
#endif
	}

	namespace rtm_impl
	{
		//////////////////////////////////////////////////////////////////////////
		// This is a helper struct to allow a single consistent API between
		// various vector types when the semantics are identical but the return
		// type differs. Implicit coercion is used to return the desired value
		// at the call site.
		//////////////////////////////////////////////////////////////////////////
		struct vector4f_vector_get_z
		{
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator float() const RTM_NO_EXCEPT
			{
#if defined(RTM_SSE2_INTRINSICS)
				return _mm_cvtss_f32(_mm_shuffle_ps(input, input, _MM_SHUFFLE(2, 2, 2, 2)));
#elif defined(RTM_NEON_INTRINSICS)
				return vgetq_lane_f32(input, 2);
#else
				return input.z;
#endif
			}

#if defined(RTM_SSE2_INTRINSICS)
			RTM_DEPRECATED("Use 'as_scalar' suffix instead. To be removed in 2.4.")
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator scalarf() const RTM_NO_EXCEPT
			{
				return scalarf{ _mm_shuffle_ps(input, input, _MM_SHUFFLE(2, 2, 2, 2)) };
			}
#endif

			vector4f input;
		};
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the vector4 [z] component.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr rtm_impl::vector4f_vector_get_z RTM_SIMD_CALL vector_get_z(vector4f_arg0 input) RTM_NO_EXCEPT
	{
		return rtm_impl::vector4f_vector_get_z{ input };
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the vector4 [z] component.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE scalarf RTM_SIMD_CALL vector_get_z_as_scalar(vector4f_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return scalarf{ _mm_shuffle_ps(input, input, _MM_SHUFFLE(2, 2, 2, 2)) };
#else
		return vector_get_z(input);
#endif
	}

	namespace rtm_impl
	{
		//////////////////////////////////////////////////////////////////////////
		// This is a helper struct to allow a single consistent API between
		// various vector types when the semantics are identical but the return
		// type differs. Implicit coercion is used to return the desired value
		// at the call site.
		//////////////////////////////////////////////////////////////////////////
		struct vector4f_vector_get_w
		{
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator float() const RTM_NO_EXCEPT
			{
#if defined(RTM_SSE2_INTRINSICS)
				return _mm_cvtss_f32(_mm_shuffle_ps(input, input, _MM_SHUFFLE(3, 3, 3, 3)));
#elif defined(RTM_NEON_INTRINSICS)
				return vgetq_lane_f32(input, 3);
#else
				return input.w;
#endif
			}

#if defined(RTM_SSE2_INTRINSICS)
			RTM_DEPRECATED("Use 'as_scalar' suffix instead. To be removed in 2.4.")
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator scalarf() const RTM_NO_EXCEPT
			{
				return scalarf{ _mm_shuffle_ps(input, input, _MM_SHUFFLE(3, 3, 3, 3)) };
			}
#endif

			vector4f input;
		};
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the vector4 [w] component.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr rtm_impl::vector4f_vector_get_w RTM_SIMD_CALL vector_get_w(vector4f_arg0 input) RTM_NO_EXCEPT
	{
		return rtm_impl::vector4f_vector_get_w{ input };
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the vector4 [w] component.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE scalarf RTM_SIMD_CALL vector_get_w_as_scalar(vector4f_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return scalarf{ _mm_shuffle_ps(input, input, _MM_SHUFFLE(3, 3, 3, 3)) };
#else
		return vector_get_w(input);
#endif
	}

	namespace rtm_impl
	{
		//////////////////////////////////////////////////////////////////////////
		// This is a helper struct to allow a single consistent API between
		// various vector types when the semantics are identical but the return
		// type differs. Implicit coercion is used to return the desired value
		// at the call site.
		//////////////////////////////////////////////////////////////////////////
		template<component4 component>
		struct vector4f_vector_get_component_static
		{
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator float() const RTM_NO_EXCEPT
			{
				switch (component)
				{
					default:
					case component4::x:	return vector_get_x(input);
					case component4::y:	return vector_get_y(input);
					case component4::z:	return vector_get_z(input);
					case component4::w:	return vector_get_w(input);
				}
			}

#if defined(RTM_SSE2_INTRINSICS)
			RTM_DEPRECATED("Use 'as_scalar' suffix instead. To be removed in 2.4.")
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator scalarf() const RTM_NO_EXCEPT
			{
				switch (component)
				{
					default:
					case component4::x:	return vector_get_x_as_scalar(input);
					case component4::y:	return vector_get_y_as_scalar(input);
					case component4::z:	return vector_get_z_as_scalar(input);
					case component4::w:	return vector_get_w_as_scalar(input);
				}
			}
#endif

			vector4f input;
		};
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the vector2 desired component.
	//////////////////////////////////////////////////////////////////////////
	template<component2 component, component4 component_ = static_cast<component4>(component)>
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr rtm_impl::vector4f_vector_get_component_static<component_> RTM_SIMD_CALL vector_get_component2(vector4f_arg0 input) RTM_NO_EXCEPT
	{
		return rtm_impl::vector4f_vector_get_component_static<component_>{ input };
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the vector2 desired component.
	//////////////////////////////////////////////////////////////////////////
	template<component2 component>
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE scalarf RTM_SIMD_CALL vector_get_component2_as_scalar(vector4f_arg0 input) RTM_NO_EXCEPT
	{
		switch (component)
		{
			default:
			case component2::x:	return vector_get_x_as_scalar(input);
			case component2::y:	return vector_get_y_as_scalar(input);
		}
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the vector3 desired component.
	//////////////////////////////////////////////////////////////////////////
	template<component3 component, component4 component_ = static_cast<component4>(component)>
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr rtm_impl::vector4f_vector_get_component_static<component_> RTM_SIMD_CALL vector_get_component3(vector4f_arg0 input) RTM_NO_EXCEPT
	{
		return rtm_impl::vector4f_vector_get_component_static<component_>{ input };
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the vector3 desired component.
	//////////////////////////////////////////////////////////////////////////
	template<component3 component>
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE scalarf RTM_SIMD_CALL vector_get_component3_as_scalar(vector4f_arg0 input) RTM_NO_EXCEPT
	{
		switch (component)
		{
			default:
			case component3::x:	return vector_get_x_as_scalar(input);
			case component3::y:	return vector_get_y_as_scalar(input);
			case component3::z:	return vector_get_z_as_scalar(input);
		}
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the vector4 desired component.
	//////////////////////////////////////////////////////////////////////////
	template<component4 component>
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr rtm_impl::vector4f_vector_get_component_static<component> RTM_SIMD_CALL vector_get_component(vector4f_arg0 input) RTM_NO_EXCEPT
	{
		return rtm_impl::vector4f_vector_get_component_static<component>{ input };
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the vector4 desired component.
	//////////////////////////////////////////////////////////////////////////
	template<component4 component>
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE scalarf RTM_SIMD_CALL vector_get_component_as_scalar(vector4f_arg0 input) RTM_NO_EXCEPT
	{
		switch (component)
		{
			default:
			case component4::x:	return vector_get_x_as_scalar(input);
			case component4::y:	return vector_get_y_as_scalar(input);
			case component4::z:	return vector_get_z_as_scalar(input);
			case component4::w:	return vector_get_w_as_scalar(input);
		}
	}

	namespace rtm_impl
	{
		//////////////////////////////////////////////////////////////////////////
		// This is a helper struct to allow a single consistent API between
		// various vector types when the semantics are identical but the return
		// type differs. Implicit coercion is used to return the desired value
		// at the call site.
		//////////////////////////////////////////////////////////////////////////
		struct vector4f_vector_get_component
		{
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator float() const RTM_NO_EXCEPT
			{
				switch (component)
				{
					default:
					case component4::x:	return vector_get_x(input);
					case component4::y:	return vector_get_y(input);
					case component4::z:	return vector_get_z(input);
					case component4::w:	return vector_get_w(input);
				}
			}

#if defined(RTM_SSE2_INTRINSICS)
			RTM_DEPRECATED("Use 'as_scalar' suffix instead. To be removed in 2.4.")
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator scalarf() const RTM_NO_EXCEPT
			{
				switch (component)
				{
					default:
					case component4::x:	return vector_get_x_as_scalar(input);
					case component4::y:	return vector_get_y_as_scalar(input);
					case component4::z:	return vector_get_z_as_scalar(input);
					case component4::w:	return vector_get_w_as_scalar(input);
				}
			}
#endif

			vector4f input;
			component4 component;
			int32_t padding[3];
		};
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the vector2 desired component.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr rtm_impl::vector4f_vector_get_component RTM_SIMD_CALL vector_get_component2(vector4f_arg0 input, component2 component) RTM_NO_EXCEPT
	{
		return rtm_impl::vector4f_vector_get_component{ input, static_cast<component4>(component), { 0 } };
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the vector2 desired component.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE scalarf RTM_SIMD_CALL vector_get_component2_as_scalar(vector4f_arg0 input, component2 component) RTM_NO_EXCEPT
	{
		switch (component)
		{
			default:
			case component2::x:	return vector_get_x_as_scalar(input);
			case component2::y:	return vector_get_y_as_scalar(input);
		}
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the vector3 desired component.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr rtm_impl::vector4f_vector_get_component RTM_SIMD_CALL vector_get_component3(vector4f_arg0 input, component3 component) RTM_NO_EXCEPT
	{
		return rtm_impl::vector4f_vector_get_component{ input, static_cast<component4>(component), { 0 } };
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the vector3 desired component.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE scalarf RTM_SIMD_CALL vector_get_component3_as_scalar(vector4f_arg0 input, component3 component) RTM_NO_EXCEPT
	{
		switch (component)
		{
			default:
			case component3::x:	return vector_get_x_as_scalar(input);
			case component3::y:	return vector_get_y_as_scalar(input);
			case component3::z:	return vector_get_z_as_scalar(input);
		}
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the vector4 desired component.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr rtm_impl::vector4f_vector_get_component RTM_SIMD_CALL vector_get_component(vector4f_arg0 input, component4 component) RTM_NO_EXCEPT
	{
		return rtm_impl::vector4f_vector_get_component{ input, component, { 0 } };
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the vector4 desired component.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE scalarf RTM_SIMD_CALL vector_get_component_as_scalar(vector4f_arg0 input, component4 component) RTM_NO_EXCEPT
	{
		switch (component)
		{
			default:
			case component4::x:	return vector_get_x_as_scalar(input);
			case component4::y:	return vector_get_y_as_scalar(input);
			case component4::z:	return vector_get_z_as_scalar(input);
			case component4::w:	return vector_get_w_as_scalar(input);
		}
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the smallest component in the input vector as a scalar.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr rtm_impl::vector4f_get_min_component RTM_SIMD_CALL vector_get_min_component(vector4f_arg0 input) RTM_NO_EXCEPT
	{
		return rtm_impl::vector4f_get_min_component{ input };
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the smallest component in the input vector as a scalar.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE scalarf RTM_SIMD_CALL vector_get_min_component_as_scalar(vector4f_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		__m128 zwzw = _mm_movehl_ps(input, input);
		__m128 xz_yw_zz_ww = _mm_min_ps(input, zwzw);
		__m128 yw_yw_yw_yw = _mm_shuffle_ps(xz_yw_zz_ww, xz_yw_zz_ww, _MM_SHUFFLE(1, 1, 1, 1));
		return scalarf{ _mm_min_ps(xz_yw_zz_ww, yw_yw_yw_yw) };
#else
		return vector_get_min_component(input);
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the largest component in the input vector as a scalar.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr rtm_impl::vector4f_get_max_component RTM_SIMD_CALL vector_get_max_component(vector4f_arg0 input) RTM_NO_EXCEPT
	{
		return rtm_impl::vector4f_get_max_component{ input };
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the largest component in the input vector as a scalar.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE scalarf RTM_SIMD_CALL vector_get_max_component_as_scalar(vector4f_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		__m128 zwzw = _mm_movehl_ps(input, input);
		__m128 xz_yw_zz_ww = _mm_max_ps(input, zwzw);
		__m128 yw_yw_yw_yw = _mm_shuffle_ps(xz_yw_zz_ww, xz_yw_zz_ww, _MM_SHUFFLE(1, 1, 1, 1));
		return scalarf{ _mm_max_ps(xz_yw_zz_ww, yw_yw_yw_yw) };
#else
		return vector_get_max_component(input);
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Sets the vector4 [x] component and returns the new value.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4f RTM_SIMD_CALL vector_set_x(vector4f_arg0 input, float lane_value) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return _mm_move_ss(input, _mm_set_ss(lane_value));
#elif defined(RTM_NEON_INTRINSICS)
		return vsetq_lane_f32(lane_value, input, 0);
#else
		return vector4f{ lane_value, input.y, input.z, input.w };
#endif
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Sets the vector4 [x] component and returns the new value.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4f RTM_SIMD_CALL vector_set_x(vector4f_arg0 input, scalarf_arg1 lane_value) RTM_NO_EXCEPT
	{
		return _mm_move_ss(input, lane_value.value);
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Sets the vector4 [y] component and returns the new value.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4f RTM_SIMD_CALL vector_set_y(vector4f_arg0 input, float lane_value) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE4_INTRINSICS)
		return _mm_insert_ps(input, _mm_set_ss(lane_value), 0x10);
#elif defined(RTM_SSE2_INTRINSICS)
		const __m128 yxzw = _mm_shuffle_ps(input, input, _MM_SHUFFLE(3, 2, 0, 1));
		const __m128 vxzw = _mm_move_ss(yxzw, _mm_set_ss(lane_value));
		return _mm_shuffle_ps(vxzw, vxzw, _MM_SHUFFLE(3, 2, 0, 1));
#elif defined(RTM_NEON_INTRINSICS)
		return vsetq_lane_f32(lane_value, input, 1);
#else
		return vector4f{ input.x, lane_value, input.z, input.w };
#endif
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Sets the vector4 [y] component and returns the new value.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4f RTM_SIMD_CALL vector_set_y(vector4f_arg0 input, scalarf_arg1 lane_value) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE4_INTRINSICS)
		return _mm_insert_ps(input, lane_value.value, 0x10);
#else
		const __m128 yxzw = _mm_shuffle_ps(input, input, _MM_SHUFFLE(3, 2, 0, 1));
		const __m128 vxzw = _mm_move_ss(yxzw, lane_value.value);
		return _mm_shuffle_ps(vxzw, vxzw, _MM_SHUFFLE(3, 2, 0, 1));
#endif
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Sets the vector4 [z] component and returns the new value.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4f RTM_SIMD_CALL vector_set_z(vector4f_arg0 input, float lane_value) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE4_INTRINSICS)
		return _mm_insert_ps(input, _mm_set_ss(lane_value), 0x20);
#elif defined(RTM_SSE2_INTRINSICS)
		const __m128 zyxw = _mm_shuffle_ps(input, input, _MM_SHUFFLE(3, 0, 1, 2));
		const __m128 vyxw = _mm_move_ss(zyxw, _mm_set_ss(lane_value));
		return _mm_shuffle_ps(vyxw, vyxw, _MM_SHUFFLE(3, 0, 1, 2));
#elif defined(RTM_NEON_INTRINSICS)
		return vsetq_lane_f32(lane_value, input, 2);
#else
		return vector4f{ input.x, input.y, lane_value, input.w };
#endif
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Sets the vector4 [z] component and returns the new value.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4f RTM_SIMD_CALL vector_set_z(vector4f_arg0 input, scalarf_arg1 lane_value) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE4_INTRINSICS)
		return _mm_insert_ps(input, lane_value.value, 0x20);
#else
		const __m128 yxzw = _mm_shuffle_ps(input, input, _MM_SHUFFLE(3, 0, 1, 2));
		const __m128 vxzw = _mm_move_ss(yxzw, lane_value.value);
		return _mm_shuffle_ps(vxzw, vxzw, _MM_SHUFFLE(3, 0, 1, 2));
#endif
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Sets the vector4 [w] component and returns the new value.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4f RTM_SIMD_CALL vector_set_w(vector4f_arg0 input, float lane_value) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE4_INTRINSICS)
		return _mm_insert_ps(input, _mm_set_ss(lane_value), 0x30);
#elif defined(RTM_SSE2_INTRINSICS)
		const __m128 wyzx = _mm_shuffle_ps(input, input, _MM_SHUFFLE(0, 2, 1, 3));
		const __m128 vyzx = _mm_move_ss(wyzx, _mm_set_ss(lane_value));
		return _mm_shuffle_ps(vyzx, vyzx, _MM_SHUFFLE(0, 2, 1, 3));
#elif defined(RTM_NEON_INTRINSICS)
		return vsetq_lane_f32(lane_value, input, 3);
#else
		return vector4f{ input.x, input.y, input.z, lane_value };
#endif
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Sets the vector4 [w] component and returns the new value.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4f RTM_SIMD_CALL vector_set_w(vector4f_arg0 input, scalarf_arg1 lane_value) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE4_INTRINSICS)
		return _mm_insert_ps(input, lane_value.value, 0x30);
#else
		const __m128 yxzw = _mm_shuffle_ps(input, input, _MM_SHUFFLE(0, 2, 1, 3));
		const __m128 vxzw = _mm_move_ss(yxzw, lane_value.value);
		return _mm_shuffle_ps(vxzw, vxzw, _MM_SHUFFLE(0, 2, 1, 3));
#endif
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Sets the desired vector2 component and returns the new value.
	//////////////////////////////////////////////////////////////////////////
	template<component2 component>
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4f RTM_SIMD_CALL vector_set_component2(vector4f_arg0 input, float lane_value) RTM_NO_EXCEPT
	{
		switch (component)
		{
			default:
			case component2::x:	return vector_set_x(input, lane_value);
			case component2::y:	return vector_set_y(input, lane_value);
		}
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Sets the desired vector2 component and returns the new value.
	//////////////////////////////////////////////////////////////////////////
	template<component2 component>
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4f RTM_SIMD_CALL vector_set_component2(vector4f_arg0 input, scalarf_arg1 lane_value) RTM_NO_EXCEPT
	{
		switch (component)
		{
			default:
			case component2::x:	return vector_set_x(input, lane_value);
			case component2::y:	return vector_set_y(input, lane_value);
		}
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Sets the desired vector3 component and returns the new value.
	//////////////////////////////////////////////////////////////////////////
	template<component3 component>
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4f RTM_SIMD_CALL vector_set_component3(vector4f_arg0 input, float lane_value) RTM_NO_EXCEPT
	{
		switch (component)
		{
			default:
			case component3::x:	return vector_set_x(input, lane_value);
			case component3::y:	return vector_set_y(input, lane_value);
			case component3::z:	return vector_set_z(input, lane_value);
		}
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Sets the desired vector3 component and returns the new value.
	//////////////////////////////////////////////////////////////////////////
	template<component3 component>
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4f RTM_SIMD_CALL vector_set_component3(vector4f_arg0 input, scalarf_arg1 lane_value) RTM_NO_EXCEPT
	{
		switch (component)
		{
			default:
			case component3::x:	return vector_set_x(input, lane_value);
			case component3::y:	return vector_set_y(input, lane_value);
			case component3::z:	return vector_set_z(input, lane_value);
		}
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Sets the desired vector4 component and returns the new value.
	//////////////////////////////////////////////////////////////////////////
	template<component4 component>
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4f RTM_SIMD_CALL vector_set_component(vector4f_arg0 input, float lane_value) RTM_NO_EXCEPT
	{
		switch (component)
		{
			default:
			case component4::x:	return vector_set_x(input, lane_value);
			case component4::y:	return vector_set_y(input, lane_value);
			case component4::z:	return vector_set_z(input, lane_value);
			case component4::w:	return vector_set_w(input, lane_value);
		}
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Sets the desired vector4 component and returns the new value.
	//////////////////////////////////////////////////////////////////////////
	template<component4 component>
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4f RTM_SIMD_CALL vector_set_component(vector4f_arg0 input, scalarf_arg1 lane_value) RTM_NO_EXCEPT
	{
		switch (component)
		{
			default:
			case component4::x:	return vector_set_x(input, lane_value);
			case component4::y:	return vector_set_y(input, lane_value);
			case component4::z:	return vector_set_z(input, lane_value);
			case component4::w:	return vector_set_w(input, lane_value);
		}
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Sets the desired vector2 component and returns the new value.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4f RTM_SIMD_CALL vector_set_component2(vector4f_arg0 input, float lane_value, component2 component) RTM_NO_EXCEPT
	{
		switch (component)
		{
			default:
			case component2::x:	return vector_set_x(input, lane_value);
			case component2::y:	return vector_set_y(input, lane_value);
		}
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Sets the desired vector2 component and returns the new value.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4f RTM_SIMD_CALL vector_set_component2(vector4f_arg0 input, scalarf_arg1 lane_value, component2 component) RTM_NO_EXCEPT
	{
		switch (component)
		{
			default:
			case component2::x:	return vector_set_x(input, lane_value);
			case component2::y:	return vector_set_y(input, lane_value);
		}
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Sets the desired vector3 component and returns the new value.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4f RTM_SIMD_CALL vector_set_component3(vector4f_arg0 input, float lane_value, component3 component) RTM_NO_EXCEPT
	{
		switch (component)
		{
			default:
			case component3::x:	return vector_set_x(input, lane_value);
			case component3::y:	return vector_set_y(input, lane_value);
			case component3::z:	return vector_set_z(input, lane_value);
		}
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Sets the desired vector3 component and returns the new value.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4f RTM_SIMD_CALL vector_set_component3(vector4f_arg0 input, scalarf_arg1 lane_value, component3 component) RTM_NO_EXCEPT
	{
		switch (component)
		{
			default:
			case component3::x:	return vector_set_x(input, lane_value);
			case component3::y:	return vector_set_y(input, lane_value);
			case component3::z:	return vector_set_z(input, lane_value);
		}
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Sets the desired vector4 component and returns the new value.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4f RTM_SIMD_CALL vector_set_component(vector4f_arg0 input, float lane_value, component4 component) RTM_NO_EXCEPT
	{
		switch (component)
		{
			default:
			case component4::x:	return vector_set_x(input, lane_value);
			case component4::y:	return vector_set_y(input, lane_value);
			case component4::z:	return vector_set_z(input, lane_value);
			case component4::w:	return vector_set_w(input, lane_value);
		}
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Sets the desired vector4 component and returns the new value.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4f RTM_SIMD_CALL vector_set_component(vector4f_arg0 input, scalarf_arg1 lane_value, component4 component) RTM_NO_EXCEPT
	{
		switch (component)
		{
			default:
			case component4::x:	return vector_set_x(input, lane_value);
			case component4::y:	return vector_set_y(input, lane_value);
			case component4::z:	return vector_set_z(input, lane_value);
			case component4::w:	return vector_set_w(input, lane_value);
		}
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Returns a floating point pointer to the vector4 data.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE const float* RTM_SIMD_CALL vector_to_pointer(const vector4f& input) RTM_NO_EXCEPT
	{
		return rtm_impl::bit_cast<const float*>(&input);
	}

	//////////////////////////////////////////////////////////////////////////
	// Writes a vector4 to unaligned memory.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE void RTM_SIMD_CALL vector_store(vector4f_arg0 input, float* output) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		_mm_storeu_ps(output, input);
#else
		output[0] = vector_get_x(input);
		output[1] = vector_get_y(input);
		output[2] = vector_get_z(input);
		output[3] = vector_get_w(input);
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Writes a vector1 to unaligned memory.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE void RTM_SIMD_CALL vector_store1(vector4f_arg0 input, float* output) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		_mm_store_ss(output, input);
#else
		output[0] = vector_get_x(input);
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Writes a vector2 to unaligned memory.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE void RTM_SIMD_CALL vector_store2(vector4f_arg0 input, float* output) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		_mm_store_sd(rtm_impl::bit_cast<double*>(output), _mm_castps_pd(input));
#else
		output[0] = vector_get_x(input);
		output[1] = vector_get_y(input);
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Writes a vector3 to unaligned memory.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE void RTM_SIMD_CALL vector_store3(vector4f_arg0 input, float* output) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		_mm_store_sd(rtm_impl::bit_cast<double*>(output), _mm_castps_pd(input));
		_mm_store_ss(output + 2, _mm_shuffle_ps(input, input, _MM_SHUFFLE(2, 2, 2, 2)));
#else
		output[0] = vector_get_x(input);
		output[1] = vector_get_y(input);
		output[2] = vector_get_z(input);
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Writes a vector4 to unaligned memory.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE void RTM_SIMD_CALL vector_store(vector4f_arg0 input, uint8_t* output) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		_mm_storeu_ps(rtm_impl::bit_cast<float*>(output), input);
#else
		std::memcpy(output, &input, sizeof(vector4f));
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Writes a vector1 to unaligned memory.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE void RTM_SIMD_CALL vector_store1(vector4f_arg0 input, uint8_t* output) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		_mm_store_ss(rtm_impl::bit_cast<float*>(output), input);
#else
		std::memcpy(output, &input, sizeof(float) * 1);
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Writes a vector2 to unaligned memory.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE void RTM_SIMD_CALL vector_store2(vector4f_arg0 input, uint8_t* output) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		_mm_store_sd(rtm_impl::bit_cast<double*>(output), _mm_castps_pd(input));
#else
		std::memcpy(output, &input, sizeof(float) * 2);
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Writes a vector3 to unaligned memory.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE void RTM_SIMD_CALL vector_store3(vector4f_arg0 input, uint8_t* output) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		_mm_store_sd(rtm_impl::bit_cast<double*>(output), _mm_castps_pd(input));
		_mm_store_ss(rtm_impl::bit_cast<float*>(output) + 2, _mm_shuffle_ps(input, input, _MM_SHUFFLE(2, 2, 2, 2)));
#else
		std::memcpy(output, &input, sizeof(float) * 3);
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Writes a vector4 to unaligned memory.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE void RTM_SIMD_CALL vector_store(vector4f_arg0 input, float4f* output) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		_mm_storeu_ps(&output->x, input);
#else
		output->x = vector_get_x(input);
		output->y = vector_get_y(input);
		output->z = vector_get_z(input);
		output->w = vector_get_w(input);
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Writes a vector2 to unaligned memory.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE void RTM_SIMD_CALL vector_store2(vector4f_arg0 input, float2f* output) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		_mm_store_sd(rtm_impl::bit_cast<double*>(&output->x), _mm_castps_pd(input));
#else
		output->x = vector_get_x(input);
		output->y = vector_get_y(input);
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Writes a vector3 to unaligned memory.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE void RTM_SIMD_CALL vector_store3(vector4f_arg0 input, float3f* output) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		_mm_store_sd(rtm_impl::bit_cast<double*>(&output->x), _mm_castps_pd(input));
		_mm_store_ss(rtm_impl::bit_cast<float*>(&output->z), _mm_shuffle_ps(input, input, _MM_SHUFFLE(2, 2, 2, 2)));
#else
		output->x = vector_get_x(input);
		output->y = vector_get_y(input);
		output->z = vector_get_z(input);
#endif
	}



	//////////////////////////////////////////////////////////////////////////
	// Arithmetic
	//////////////////////////////////////////////////////////////////////////


	//////////////////////////////////////////////////////////////////////////
	// Per component addition of the two inputs: lhs + rhs
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4f RTM_SIMD_CALL vector_add(vector4f_arg0 lhs, vector4f_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return _mm_add_ps(lhs, rhs);
#elif defined(RTM_NEON_INTRINSICS)
		return vaddq_f32(lhs, rhs);
#else
		return vector_set(lhs.x + rhs.x, lhs.y + rhs.y, lhs.z + rhs.z, lhs.w + rhs.w);
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Per component subtraction of the two inputs: lhs - rhs
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4f RTM_SIMD_CALL vector_sub(vector4f_arg0 lhs, vector4f_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return _mm_sub_ps(lhs, rhs);
#elif defined(RTM_NEON_INTRINSICS)
		return vsubq_f32(lhs, rhs);
#else
		return vector_set(lhs.x - rhs.x, lhs.y - rhs.y, lhs.z - rhs.z, lhs.w - rhs.w);
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Per component multiplication of the two inputs: lhs * rhs
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4f RTM_SIMD_CALL vector_mul(vector4f_arg0 lhs, vector4f_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return _mm_mul_ps(lhs, rhs);
#elif defined(RTM_NEON_INTRINSICS)
		return vmulq_f32(lhs, rhs);
#else
		return vector_set(lhs.x * rhs.x, lhs.y * rhs.y, lhs.z * rhs.z, lhs.w * rhs.w);
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Per component multiplication of the vector by a scalar: lhs * rhs
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4f RTM_SIMD_CALL vector_mul(vector4f_arg0 lhs, float rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_NEON_INTRINSICS)
		return vmulq_n_f32(lhs, rhs);
#else
		return vector_mul(lhs, vector_set(rhs));
#endif
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Per component multiplication of the vector by a scalar: lhs * rhs
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4f RTM_SIMD_CALL vector_mul(vector4f_arg0 lhs, scalarf_arg1 rhs) RTM_NO_EXCEPT
	{
		return _mm_mul_ps(lhs, _mm_shuffle_ps(rhs.value, rhs.value, _MM_SHUFFLE(0, 0, 0, 0)));
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Per component division of the two inputs: lhs / rhs
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4f RTM_SIMD_CALL vector_div(vector4f_arg0 lhs, vector4f_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return _mm_div_ps(lhs, rhs);
#elif defined (RTM_NEON64_INTRINSICS)
		return vdivq_f32(lhs, rhs);
#elif defined(RTM_NEON_INTRINSICS)
		// Use scalar division on ARMv7, slow but accurate
		float x = vgetq_lane_f32(lhs, 0) / vgetq_lane_f32(rhs, 0);
		float y = vgetq_lane_f32(lhs, 1) / vgetq_lane_f32(rhs, 1);
		float z = vgetq_lane_f32(lhs, 2) / vgetq_lane_f32(rhs, 2);
		float w = vgetq_lane_f32(lhs, 3) / vgetq_lane_f32(rhs, 3);

		float32x4_t result = lhs;
		result = vsetq_lane_f32(x, result, 0);
		result = vsetq_lane_f32(y, result, 1);
		result = vsetq_lane_f32(z, result, 2);
		result = vsetq_lane_f32(w, result, 3);
		return result;
#else
		return vector_set(lhs.x / rhs.x, lhs.y / rhs.y, lhs.z / rhs.z, lhs.w / rhs.w);
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Per component maximum of the two inputs: max(lhs, rhs)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4f RTM_SIMD_CALL vector_max(vector4f_arg0 lhs, vector4f_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return _mm_max_ps(lhs, rhs);
#elif defined(RTM_NEON_INTRINSICS)
		return vmaxq_f32(lhs, rhs);
#else
		return vector_set(scalar_max(lhs.x, rhs.x), scalar_max(lhs.y, rhs.y), scalar_max(lhs.z, rhs.z), scalar_max(lhs.w, rhs.w));
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Per component minimum of the two inputs: min(lhs, rhs)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4f RTM_SIMD_CALL vector_min(vector4f_arg0 lhs, vector4f_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return _mm_min_ps(lhs, rhs);
#elif defined(RTM_NEON_INTRINSICS)
		return vminq_f32(lhs, rhs);
#else
		return vector_set(scalar_min(lhs.x, rhs.x), scalar_min(lhs.y, rhs.y), scalar_min(lhs.z, rhs.z), scalar_min(lhs.w, rhs.w));
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Per component clamping of an input between a minimum and a maximum value: min(max_value, max(min_value, input))
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4f RTM_SIMD_CALL vector_clamp(vector4f_arg0 input, vector4f_arg1 min_value, vector4f_arg2 max_value) RTM_NO_EXCEPT
	{
		return vector_min(max_value, vector_max(min_value, input));
	}

	//////////////////////////////////////////////////////////////////////////
	// Per component absolute of the input: abs(input)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4f RTM_SIMD_CALL vector_abs(vector4f_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		const __m128i abs_mask = _mm_set_epi32(0x7FFFFFFFULL, 0x7FFFFFFFULL, 0x7FFFFFFFULL, 0x7FFFFFFFULL);
		return _mm_and_ps(input, _mm_castsi128_ps(abs_mask));
#elif defined(RTM_NEON_INTRINSICS)
		return vabsq_f32(input);
#else
		return vector_set(scalar_abs(input.x), scalar_abs(input.y), scalar_abs(input.z), scalar_abs(input.w));
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Per component negation of the input: -input
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4f RTM_SIMD_CALL vector_neg(vector4f_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		constexpr __m128 signs = RTM_VECTOR4F_MAKE(-0.0F, -0.0F, -0.0F, -0.0F);
		return _mm_xor_ps(input, signs);
#elif defined(RTM_NEON_INTRINSICS)
		return vnegq_f32(input);
#else
		return vector_mul(input, -1.0f);
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Per component negation of the input: -input
	// Each template argument controls whether a SIMD lane should be negated (true) or not (false).
	//////////////////////////////////////////////////////////////////////////
	template<bool x, bool y, bool z, bool w>
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4f RTM_SIMD_CALL vector_neg(vector4f_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		constexpr __m128 signs = RTM_VECTOR4F_MAKE(x ? -0.0F : 0.0F, y ? -0.0F : 0.0F, z ? -0.0F : 0.0F, w ? -0.0F : 0.0F);
		return _mm_xor_ps(input, signs);
#elif defined(RTM_NEON_INTRINSICS)
		alignas(16) constexpr uint32_t sign_bit_i[4] =
		{
			x ? 0x80000000U : 0,
			y ? 0x80000000U : 0,
			z ? 0x80000000U : 0,
			w ? 0x80000000U : 0,
		};
		const uint32x4_t sign_bit = *rtm_impl::bit_cast<const uint32x4_t*>(&sign_bit_i[0]);
		return vreinterpretq_f32_u32(veorq_u32(vreinterpretq_u32_f32(input), sign_bit));
#else
		const vector4f signs = vector_set(x ? -1.0F : 1.0F, y ? -1.0F : 1.0F, z ? -1.0F : 1.0F, w ? -1.0F : 1.0F);
		return vector_mul(input, signs);
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Per component reciprocal of the input: 1.0 / input
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4f RTM_SIMD_CALL vector_reciprocal(vector4f_arg0 input) RTM_NO_EXCEPT
	{
		// Performance note:
		// With modern out-of-order executing processors, it is typically faster to use
		// a full division instead of a reciprocal estimate + Newton-Raphson iterations
		// because the resulting code is more dense and is more likely to inline and
		// as it uses fewer instructions.
#if defined(RTM_SSE2_INTRINSICS)
		return _mm_div_ps(_mm_set_ps1(1.0F), input);
#elif defined(RTM_NEON64_INTRINSICS)
		return vdivq_f32(vdupq_n_f32(1.0F), input);
#elif defined(RTM_NEON_INTRINSICS)
		// Perform two passes of Newton-Raphson iteration on the hardware estimate
		float32x4_t x0 = vrecpeq_f32(input);

		// First iteration
		float32x4_t x1 = vmulq_f32(x0, vrecpsq_f32(x0, input));

		// Second iteration
		float32x4_t x2 = vmulq_f32(x1, vrecpsq_f32(x1, input));
		return x2;
#else
		return vector_div(vector_set(1.0F), input);
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Per component square root of the input: sqrt(input)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4f RTM_SIMD_CALL vector_sqrt(vector4f_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return _mm_sqrt_ps(input);
#elif defined(RTM_NEON64_INTRINSICS) && defined(RTM_IMPL_VSQRT_SUPPORTED)
		return vsqrtq_f32(input);
#else
		scalarf x = vector_get_x(input);
		scalarf y = vector_get_y(input);
		scalarf z = vector_get_z(input);
		scalarf w = vector_get_w(input);
		return vector_set(scalar_sqrt(x), scalar_sqrt(y), scalar_sqrt(z), scalar_sqrt(w));
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Per component reciprocal square root of the input: 1.0 / sqrt(input)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4f RTM_SIMD_CALL vector_sqrt_reciprocal(vector4f_arg0 input) RTM_NO_EXCEPT
	{
		// Performance note:
		// With modern out-of-order executing processors, it is typically faster to use
		// a full division/square root instead of a reciprocal estimate + Newton-Raphson iterations
		// because the resulting code is more dense and is more likely to inline and
		// as it uses fewer instructions.
		return vector_reciprocal(vector_sqrt(input));
	}

	//////////////////////////////////////////////////////////////////////////
	// Per component returns the smallest integer value not less than the input (round towards positive infinity).
	// vector_ceil([1.8, 1.0, -1.8, -1.0]) = [2.0, 1.0, -1.0, -1.0]
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline vector4f RTM_SIMD_CALL vector_ceil(vector4f_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE4_INTRINSICS)
		return _mm_ceil_ps(input);
#elif defined(RTM_SSE2_INTRINSICS)
		// NaN, +- Infinity, and numbers larger or equal to 2^23 remain unchanged
		// since they have no fractional part.

		const __m128i abs_mask = _mm_set_epi32(0x7FFFFFFFULL, 0x7FFFFFFFULL, 0x7FFFFFFFULL, 0x7FFFFFFFULL);
		const __m128 fractional_limit = _mm_set_ps1(8388608.0F); // 2^23

		// Build our mask, larger values that have no fractional part, and infinities will be true
		// Smaller values and NaN will be false
		__m128 abs_input = _mm_and_ps(input, _mm_castsi128_ps(abs_mask));
		__m128 is_input_large = _mm_cmpge_ps(abs_input, fractional_limit);

		// Test if our input is NaN with (value != value), it is only true for NaN
		__m128 is_nan = _mm_cmpneq_ps(input, input);

		// Combine our masks to determine if we should return the original value
		__m128 use_original_input = _mm_or_ps(is_input_large, is_nan);

		// Convert to an integer and back. This does banker's rounding by default
		__m128 integer_part = _mm_cvtepi32_ps(_mm_cvtps_epi32(input));

		// Test if the returned value is smaller than the original.
		// A positive input will round towards zero and be lower when we need it to be greater.
		__m128 is_positive = _mm_cmplt_ps(integer_part, input);

		// Convert our mask to a float, ~0 yields -1.0 since it is a valid signed integer
		// Negative values will yield a 0.0 bias
		__m128 bias = _mm_cvtepi32_ps(_mm_castps_si128(is_positive));

		// Subtract our bias to properly handle positive values
		integer_part = _mm_sub_ps(integer_part, bias);

		return _mm_or_ps(_mm_and_ps(use_original_input, input), _mm_andnot_ps(use_original_input, integer_part));
#elif defined(RTM_NEON64_INTRINSICS)
		return vrndpq_f32(input);
#elif defined(RTM_NEON_INTRINSICS)
		// NaN, +- Infinity, and numbers larger or equal to 2^23 remain unchanged
		// since they have no fractional part.

		float32x4_t fractional_limit = vdupq_n_f32(8388608.0F); // 2^23

		// Build our mask, larger values that have no fractional part, and infinities will be true
		// Smaller values and NaN will be false
		uint32x4_t is_input_large = vcageq_f32(input, fractional_limit);

		// Test if our input is NaN with (value != value), it is only true for NaN
		uint32x4_t is_nan = vmvnq_u32(vceqq_f32(input, input));

		// Combine our masks to determine if we should return the original value
		uint32x4_t use_original_input = vorrq_u32(is_input_large, is_nan);

		// Convert to an integer and back. This does banker's rounding by default
		float32x4_t integer_part = vcvtq_f32_s32(vcvtq_s32_f32(input));

		// Test if the returned value is smaller than the original.
		// A positive input will round towards zero and be lower when we need it to be greater.
		uint32x4_t is_positive = vcltq_f32(integer_part, input);

		float32x4_t bias = vcvtq_f32_s32(is_positive);

		// Subtract our bias to properly handle positive values
		integer_part = vsubq_f32(integer_part, bias);

		return vbslq_f32(use_original_input, input, integer_part);
#else
		return vector_set(scalar_ceil(vector_get_x(input)), scalar_ceil(vector_get_y(input)), scalar_ceil(vector_get_z(input)), scalar_ceil(vector_get_w(input)));
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Per component returns the largest integer value not greater than the input (round towards negative infinity).
	// vector_floor([1.8, 1.0, -1.8, -1.0]) = [1.0, 1.0, -2.0, -1.0]
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline vector4f RTM_SIMD_CALL vector_floor(vector4f_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE4_INTRINSICS)
		return _mm_floor_ps(input);
#elif defined(RTM_SSE2_INTRINSICS)
		// NaN, +- Infinity, and numbers larger or equal to 2^23 remain unchanged
		// since they have no fractional part.

		const __m128i abs_mask = _mm_set_epi32(0x7FFFFFFFULL, 0x7FFFFFFFULL, 0x7FFFFFFFULL, 0x7FFFFFFFULL);
		const __m128 fractional_limit = _mm_set_ps1(8388608.0F); // 2^23

		// Build our mask, larger values that have no fractional part, and infinities will be true
		// Smaller values and NaN will be false
		__m128 abs_input = _mm_and_ps(input, _mm_castsi128_ps(abs_mask));
		__m128 is_input_large = _mm_cmpge_ps(abs_input, fractional_limit);

		// Test if our input is NaN with (value != value), it is only true for NaN
		__m128 is_nan = _mm_cmpneq_ps(input, input);

		// Combine our masks to determine if we should return the original value
		__m128 use_original_input = _mm_or_ps(is_input_large, is_nan);

		// Convert to an integer and back. This does banker's rounding by default
		__m128 integer_part = _mm_cvtepi32_ps(_mm_cvtps_epi32(input));

		// Test if the returned value is greater than the original.
		// A negative input will round towards zero and be greater when we need it to be smaller.
		__m128 is_negative = _mm_cmpgt_ps(integer_part, input);

		// Convert our mask to a float, ~0 yields -1.0 since it is a valid signed integer
		// Positive values will yield a 0.0 bias
		__m128 bias = _mm_cvtepi32_ps(_mm_castps_si128(is_negative));

		// Add our bias to properly handle negative values
		integer_part = _mm_add_ps(integer_part, bias);

		return _mm_or_ps(_mm_and_ps(use_original_input, input), _mm_andnot_ps(use_original_input, integer_part));
#elif defined(RTM_NEON64_INTRINSICS)
		return vrndmq_f32(input);
#elif defined(RTM_NEON_INTRINSICS)
		// NaN, +- Infinity, and numbers larger or equal to 2^23 remain unchanged
		// since they have no fractional part.

		float32x4_t fractional_limit = vdupq_n_f32(8388608.0F); // 2^23

		// Build our mask, larger values that have no fractional part, and infinities will be true
		// Smaller values and NaN will be false
		uint32x4_t is_input_large = vcageq_f32(input, fractional_limit);

		// Test if our input is NaN with (value != value), it is only true for NaN
		uint32x4_t is_nan = vmvnq_u32(vceqq_f32(input, input));

		// Combine our masks to determine if we should return the original value
		uint32x4_t use_original_input = vorrq_u32(is_input_large, is_nan);

		// Convert to an integer and back. This does banker's rounding by default
		float32x4_t integer_part = vcvtq_f32_s32(vcvtq_s32_f32(input));

		// Test if the returned value is greater than the original.
		// A negative input will round towards zero and be greater when we need it to be smaller.
		uint32x4_t is_negative = vcgtq_f32(integer_part, input);

		float32x4_t bias = vcvtq_f32_s32(is_negative);

		// Add our bias to properly handle negative values
		integer_part = vaddq_f32(integer_part, bias);

		return vbslq_f32(use_original_input, input, integer_part);
#else
		return vector_set(scalar_floor(vector_get_x(input)), scalar_floor(vector_get_y(input)), scalar_floor(vector_get_z(input)), scalar_floor(vector_get_w(input)));
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// 3D cross product: lhs x rhs
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4f RTM_SIMD_CALL vector_cross3(vector4f_arg0 lhs, vector4f_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		// cross(a, b).zxy = (a * b.yzx) - (a.yzx * b)
		__m128 lhs_yzx = _mm_shuffle_ps(lhs, lhs, _MM_SHUFFLE(3, 0, 2, 1));
		__m128 rhs_yzx = _mm_shuffle_ps(rhs, rhs, _MM_SHUFFLE(3, 0, 2, 1));
		__m128 tmp_zxy = _mm_sub_ps(_mm_mul_ps(lhs, rhs_yzx), _mm_mul_ps(lhs_yzx, rhs));

		// cross(a, b) = ((a * b.yzx) - (a.yzx * b)).yzx
		return _mm_shuffle_ps(tmp_zxy, tmp_zxy, _MM_SHUFFLE(3, 0, 2, 1));
#elif defined(RTM_NEON_INTRINSICS)
		// cross(a, b) = (a.yzx * b.zxy) - (a.zxy * b.yzx)
		float32x4_t lhs_yzwx = vextq_f32(lhs, lhs, 1);
		float32x4_t rhs_wxyz = vextq_f32(rhs, rhs, 3);

		float32x4_t lhs_yzx = vsetq_lane_f32(vgetq_lane_f32(lhs, 0), lhs_yzwx, 2);
		float32x4_t rhs_zxy = vsetq_lane_f32(vgetq_lane_f32(rhs, 2), rhs_wxyz, 0);

		// part_a = (a.yzx * b.zxy)
		float32x4_t part_a = vmulq_f32(lhs_yzx, rhs_zxy);

		float32x4_t lhs_wxyz = vextq_f32(lhs, lhs, 3);
		float32x4_t rhs_yzwx = vextq_f32(rhs, rhs, 1);
		float32x4_t lhs_zxy = vsetq_lane_f32(vgetq_lane_f32(lhs, 2), lhs_wxyz, 0);
		float32x4_t rhs_yzx = vsetq_lane_f32(vgetq_lane_f32(rhs, 0), rhs_yzwx, 2);

		return vmlsq_f32(part_a, lhs_zxy, rhs_yzx);
#else
		// cross(a, b) = (a.yzx * b.zxy) - (a.zxy * b.yzx)
		const float lhs_x = vector_get_x(lhs);
		const float lhs_y = vector_get_y(lhs);
		const float lhs_z = vector_get_z(lhs);
		const float rhs_x = vector_get_x(rhs);
		const float rhs_y = vector_get_y(rhs);
		const float rhs_z = vector_get_z(rhs);
		return vector_set((lhs_y * rhs_z) - (lhs_z * rhs_y), (lhs_z * rhs_x) - (lhs_x * rhs_z), (lhs_x * rhs_y) - (lhs_y * rhs_x));
#endif
	}

	namespace rtm_impl
	{
		//////////////////////////////////////////////////////////////////////////
		// This is a helper struct to allow a single consistent API between
		// various vector types when the semantics are identical but the return
		// type differs. Implicit coercion is used to return the desired value
		// at the call site.
		//////////////////////////////////////////////////////////////////////////
		struct vector4f_vector_dot
		{
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator float() const RTM_NO_EXCEPT
			{
#if defined(RTM_SSE4_INTRINSICS) && 0
				// SSE4 dot product instruction appears slower on Zen2, is it the case elsewhere as well?
				return _mm_cvtss_f32(_mm_dp_ps(lhs, rhs, 0xFF));
#elif defined(RTM_SSE2_INTRINSICS)
				__m128 x2_y2_z2_w2 = _mm_mul_ps(lhs, rhs);
				__m128 z2_w2_0_0 = _mm_shuffle_ps(x2_y2_z2_w2, x2_y2_z2_w2, _MM_SHUFFLE(0, 0, 3, 2));
				__m128 x2z2_y2w2_0_0 = _mm_add_ps(x2_y2_z2_w2, z2_w2_0_0);
				__m128 y2w2_0_0_0 = _mm_shuffle_ps(x2z2_y2w2_0_0, x2z2_y2w2_0_0, _MM_SHUFFLE(0, 0, 0, 1));
				__m128 x2y2z2w2_0_0_0 = _mm_add_ps(x2z2_y2w2_0_0, y2w2_0_0_0);
				return _mm_cvtss_f32(x2y2z2w2_0_0_0);
#elif defined(RTM_NEON64_INTRINSICS) && defined(RTM_IMPL_VADDVQ_SUPPORTED)
				float32x4_t x2_y2_z2_w2 = vmulq_f32(lhs, rhs);
				return vaddvq_f32(x2_y2_z2_w2);
#elif defined(RTM_NEON_INTRINSICS)
				float32x4_t x2_y2_z2_w2 = vmulq_f32(lhs, rhs);
				float32x2_t x2_y2 = vget_low_f32(x2_y2_z2_w2);
				float32x2_t z2_w2 = vget_high_f32(x2_y2_z2_w2);
				float32x2_t x2z2_y2w2 = vadd_f32(x2_y2, z2_w2);
				float32x2_t x2y2z2w2 = vpadd_f32(x2z2_y2w2, x2z2_y2w2);
				return vget_lane_f32(x2y2z2w2, 0);
#else
				return (vector_get_x(lhs) * vector_get_x(rhs)) + (vector_get_y(lhs) * vector_get_y(rhs)) + (vector_get_z(lhs) * vector_get_z(rhs)) + (vector_get_w(lhs) * vector_get_w(rhs));
#endif
			}

#if defined(RTM_SSE2_INTRINSICS)
			RTM_DEPRECATED("Use 'as_scalar' suffix instead. To be removed in 2.4.")
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator scalarf() const RTM_NO_EXCEPT
			{
#if defined(RTM_SSE4_INTRINSICS) && 0
				// SSE4 dot product instruction appears slower on Zen2, is it the case elsewhere as well?
				return scalarf{ _mm_cvtss_f32(_mm_dp_ps(lhs, rhs, 0xFF)) };
#else
				__m128 x2_y2_z2_w2 = _mm_mul_ps(lhs, rhs);
				__m128 z2_w2_0_0 = _mm_shuffle_ps(x2_y2_z2_w2, x2_y2_z2_w2, _MM_SHUFFLE(0, 0, 3, 2));
				__m128 x2z2_y2w2_0_0 = _mm_add_ps(x2_y2_z2_w2, z2_w2_0_0);
				__m128 y2w2_0_0_0 = _mm_shuffle_ps(x2z2_y2w2_0_0, x2z2_y2w2_0_0, _MM_SHUFFLE(0, 0, 0, 1));
				__m128 x2y2z2w2_0_0_0 = _mm_add_ps(x2z2_y2w2_0_0, y2w2_0_0_0);
				return scalarf{ x2y2z2w2_0_0_0 };
#endif
			}
#endif

			RTM_DEPRECATED("Use 'as_vector' suffix instead. To be removed in 2.4.")
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator vector4f() const RTM_NO_EXCEPT
			{
#if defined(RTM_SSE4_INTRINSICS) && 0
				// SSE4 dot product instruction appears slower on Zen2, is it the case elsewhere as well?
				return _mm_dp_ps(lhs, rhs, 0xFF);
#elif defined(RTM_SSE2_INTRINSICS)
				__m128 x2_y2_z2_w2 = _mm_mul_ps(lhs, rhs);
				__m128 z2_w2_0_0 = _mm_shuffle_ps(x2_y2_z2_w2, x2_y2_z2_w2, _MM_SHUFFLE(0, 0, 3, 2));
				__m128 x2z2_y2w2_0_0 = _mm_add_ps(x2_y2_z2_w2, z2_w2_0_0);
				__m128 y2w2_0_0_0 = _mm_shuffle_ps(x2z2_y2w2_0_0, x2z2_y2w2_0_0, _MM_SHUFFLE(0, 0, 0, 1));
				__m128 x2y2z2w2_0_0_0 = _mm_add_ps(x2z2_y2w2_0_0, y2w2_0_0_0);
				return _mm_shuffle_ps(x2y2z2w2_0_0_0, x2y2z2w2_0_0_0, _MM_SHUFFLE(0, 0, 0, 0));
#elif defined(RTM_NEON64_INTRINSICS)
				float32x4_t x2_y2_z2_w2 = vmulq_f32(lhs, rhs);
				float32x4_t x2y2_z2w2_x2y2_z2w2 = vpaddq_f32(x2_y2_z2_w2, x2_y2_z2_w2);
				float32x4_t x2y2z2w2_x2y2z2w2_x2y2z2w2_x2y2z2w2 = vpaddq_f32(x2y2_z2w2_x2y2_z2w2, x2y2_z2w2_x2y2_z2w2);
				return x2y2z2w2_x2y2z2w2_x2y2z2w2_x2y2z2w2;
#elif defined(RTM_NEON_INTRINSICS)
				float32x4_t x2_y2_z2_w2 = vmulq_f32(lhs, rhs);
				float32x2_t x2_y2 = vget_low_f32(x2_y2_z2_w2);
				float32x2_t z2_w2 = vget_high_f32(x2_y2_z2_w2);
				float32x2_t x2z2_y2w2 = vadd_f32(x2_y2, z2_w2);
				float32x2_t x2y2z2w2 = vpadd_f32(x2z2_y2w2, x2z2_y2w2);
				return vcombine_f32(x2y2z2w2, x2y2z2w2);
#else
				scalarf result = *this;
				return vector_set(result);
#endif
			}

			vector4f lhs;
			vector4f rhs;
		};
	}

	//////////////////////////////////////////////////////////////////////////
	// 4D dot product: lhs . rhs
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr rtm_impl::vector4f_vector_dot RTM_SIMD_CALL vector_dot(vector4f_arg0 lhs, vector4f_arg1 rhs) RTM_NO_EXCEPT
	{
		return rtm_impl::vector4f_vector_dot{ lhs, rhs };
	}

	//////////////////////////////////////////////////////////////////////////
	// 4D dot product: lhs . rhs
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE scalarf RTM_SIMD_CALL vector_dot_as_scalar(vector4f_arg0 lhs, vector4f_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
	#if defined(RTM_SSE4_INTRINSICS) && 0
		// SSE4 dot product instruction appears slower on Zen2, is it the case elsewhere as well?
		return scalarf{ _mm_cvtss_f32(_mm_dp_ps(lhs, rhs, 0xFF)) };
	#else
		__m128 x2_y2_z2_w2 = _mm_mul_ps(lhs, rhs);
		__m128 z2_w2_0_0 = _mm_shuffle_ps(x2_y2_z2_w2, x2_y2_z2_w2, _MM_SHUFFLE(0, 0, 3, 2));
		__m128 x2z2_y2w2_0_0 = _mm_add_ps(x2_y2_z2_w2, z2_w2_0_0);
		__m128 y2w2_0_0_0 = _mm_shuffle_ps(x2z2_y2w2_0_0, x2z2_y2w2_0_0, _MM_SHUFFLE(0, 0, 0, 1));
		__m128 x2y2z2w2_0_0_0 = _mm_add_ps(x2z2_y2w2_0_0, y2w2_0_0_0);
		return scalarf{ x2y2z2w2_0_0_0 };
	#endif
#else
	return vector_dot(lhs, rhs);
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// 4D dot product: lhs . rhs
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4f RTM_SIMD_CALL vector_dot_as_vector(vector4f_arg0 lhs, vector4f_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE4_INTRINSICS) && 0
		// SSE4 dot product instruction appears slower on Zen2, is it the case elsewhere as well?
		return _mm_dp_ps(lhs, rhs, 0xFF);
#elif defined(RTM_SSE2_INTRINSICS)
		__m128 x2_y2_z2_w2 = _mm_mul_ps(lhs, rhs);
		__m128 z2_w2_0_0 = _mm_shuffle_ps(x2_y2_z2_w2, x2_y2_z2_w2, _MM_SHUFFLE(0, 0, 3, 2));
		__m128 x2z2_y2w2_0_0 = _mm_add_ps(x2_y2_z2_w2, z2_w2_0_0);
		__m128 y2w2_0_0_0 = _mm_shuffle_ps(x2z2_y2w2_0_0, x2z2_y2w2_0_0, _MM_SHUFFLE(0, 0, 0, 1));
		__m128 x2y2z2w2_0_0_0 = _mm_add_ps(x2z2_y2w2_0_0, y2w2_0_0_0);
		return _mm_shuffle_ps(x2y2z2w2_0_0_0, x2y2z2w2_0_0_0, _MM_SHUFFLE(0, 0, 0, 0));
#elif defined(RTM_NEON64_INTRINSICS)
		float32x4_t x2_y2_z2_w2 = vmulq_f32(lhs, rhs);
		float32x4_t x2y2_z2w2_x2y2_z2w2 = vpaddq_f32(x2_y2_z2_w2, x2_y2_z2_w2);
		float32x4_t x2y2z2w2_x2y2z2w2_x2y2z2w2_x2y2z2w2 = vpaddq_f32(x2y2_z2w2_x2y2_z2w2, x2y2_z2w2_x2y2_z2w2);
		return x2y2z2w2_x2y2z2w2_x2y2z2w2_x2y2z2w2;
#elif defined(RTM_NEON_INTRINSICS)
		float32x4_t x2_y2_z2_w2 = vmulq_f32(lhs, rhs);
		float32x2_t x2_y2 = vget_low_f32(x2_y2_z2_w2);
		float32x2_t z2_w2 = vget_high_f32(x2_y2_z2_w2);
		float32x2_t x2z2_y2w2 = vadd_f32(x2_y2, z2_w2);
		float32x2_t x2y2z2w2 = vpadd_f32(x2z2_y2w2, x2z2_y2w2);
		return vcombine_f32(x2y2z2w2, x2y2z2w2);
#else
		return vector_set(vector_dot_as_scalar(lhs, rhs));
#endif
	}

	namespace rtm_impl
	{
		//////////////////////////////////////////////////////////////////////////
		// This is a helper struct to allow a single consistent API between
		// various vector types when the semantics are identical but the return
		// type differs. Implicit coercion is used to return the desired value
		// at the call site.
		//////////////////////////////////////////////////////////////////////////
		struct vector4f_vector_dot2
		{
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator float() const RTM_NO_EXCEPT
			{
#if defined(RTM_SSE4_INTRINSICS) && 0
				// SSE4 dot product instruction appears slower on Zen2, is it the case elsewhere as well?
				return _mm_cvtss_f32(_mm_dp_ps(lhs, rhs, 0x7F));
#elif defined(RTM_SSE2_INTRINSICS)
				__m128 x2_y2_z2_w2 = _mm_mul_ps(lhs, rhs);
				__m128 y2_0_0_0 = _mm_shuffle_ps(x2_y2_z2_w2, x2_y2_z2_w2, _MM_SHUFFLE(0, 0, 0, 1));
				__m128 x2y2_0_0_0 = _mm_add_ss(x2_y2_z2_w2, y2_0_0_0);
				return _mm_cvtss_f32(x2y2_0_0_0);
#elif defined(RTM_NEON_INTRINSICS)
				float32x4_t x2_y2_z2_w2 = vmulq_f32(lhs, rhs);
				float32x2_t x2_y2 = vget_low_f32(x2_y2_z2_w2);
				float32x2_t x2y2_x2y2 = vpadd_f32(x2_y2, x2_y2);
				return vget_lane_f32(x2y2_x2y2, 0);
#else
				return (vector_get_x(lhs) * vector_get_x(rhs)) + (vector_get_y(lhs) * vector_get_y(rhs));
#endif
			}

#if defined(RTM_SSE2_INTRINSICS)
			RTM_DEPRECATED("Use 'as_scalar' suffix instead. To be removed in 2.4.")
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator scalarf() const RTM_NO_EXCEPT
			{
				__m128 x2_y2_z2_w2 = _mm_mul_ps(lhs, rhs);
				__m128 y2_0_0_0 = _mm_shuffle_ps(x2_y2_z2_w2, x2_y2_z2_w2, _MM_SHUFFLE(0, 0, 0, 1));
				return scalarf{ _mm_add_ss(x2_y2_z2_w2, y2_0_0_0) };
			}
#endif

			RTM_DEPRECATED("Use 'as_vector' suffix instead. To be removed in 2.4.")
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator vector4f() const RTM_NO_EXCEPT
			{
#if defined(RTM_SSE4_INTRINSICS) && 0
				// SSE4 dot product instruction appears slower on Zen2, is it the case elsewhere as well?
				return _mm_cvtss_f32(_mm_dp_ps(lhs, rhs, 0xFF));
#elif defined(RTM_SSE2_INTRINSICS)
				__m128 x2_y2_z2_w2 = _mm_mul_ps(lhs, rhs);
				__m128 y2_0_0_0 = _mm_shuffle_ps(x2_y2_z2_w2, x2_y2_z2_w2, _MM_SHUFFLE(0, 0, 0, 1));
				__m128 x2y2_0_0_0 = _mm_add_ss(x2_y2_z2_w2, y2_0_0_0);
				return _mm_shuffle_ps(x2y2_0_0_0, x2y2_0_0_0, _MM_SHUFFLE(0, 0, 0, 0));
#elif defined(RTM_NEON_INTRINSICS)
				float32x4_t x2_y2_z2_w2 = vmulq_f32(lhs, rhs);
				float32x2_t x2_y2 = vget_low_f32(x2_y2_z2_w2);
				float32x2_t x2y2_x2y2 = vpadd_f32(x2_y2, x2_y2);
				return vcombine_f32(x2y2_x2y2, x2y2_x2y2);
#else
				scalarf result = *this;
				return vector_set(result);
#endif
			}

			vector4f lhs;
			vector4f rhs;
		};
	}

	//////////////////////////////////////////////////////////////////////////
	// 2D dot product: lhs . rhs
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr rtm_impl::vector4f_vector_dot2 RTM_SIMD_CALL vector_dot2(vector4f_arg0 lhs, vector4f_arg1 rhs) RTM_NO_EXCEPT
	{
		return rtm_impl::vector4f_vector_dot2{ lhs, rhs };
	}

	//////////////////////////////////////////////////////////////////////////
	// 2D dot product: lhs . rhs
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE scalarf RTM_SIMD_CALL vector_dot2_as_scalar(vector4f_arg0 lhs, vector4f_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		__m128 x2_y2_z2_w2 = _mm_mul_ps(lhs, rhs);
		__m128 y2_0_0_0 = _mm_shuffle_ps(x2_y2_z2_w2, x2_y2_z2_w2, _MM_SHUFFLE(0, 0, 0, 1));
		return scalarf{ _mm_add_ss(x2_y2_z2_w2, y2_0_0_0) };
#else
		return vector_dot2(lhs, rhs);
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// 2D dot product: lhs . rhs
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4f RTM_SIMD_CALL vector_dot2_as_vector(vector4f_arg0 lhs, vector4f_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE4_INTRINSICS) && 0
		// SSE4 dot product instruction appears slower on Zen2, is it the case elsewhere as well?
		return _mm_cvtss_f32(_mm_dp_ps(lhs, rhs, 0xFF));
#elif defined(RTM_SSE2_INTRINSICS)
		__m128 x2_y2_z2_w2 = _mm_mul_ps(lhs, rhs);
		__m128 y2_0_0_0 = _mm_shuffle_ps(x2_y2_z2_w2, x2_y2_z2_w2, _MM_SHUFFLE(0, 0, 0, 1));
		__m128 x2y2_0_0_0 = _mm_add_ss(x2_y2_z2_w2, y2_0_0_0);
		return _mm_shuffle_ps(x2y2_0_0_0, x2y2_0_0_0, _MM_SHUFFLE(0, 0, 0, 0));
#elif defined(RTM_NEON_INTRINSICS)
		float32x4_t x2_y2_z2_w2 = vmulq_f32(lhs, rhs);
		float32x2_t x2_y2 = vget_low_f32(x2_y2_z2_w2);
		float32x2_t x2y2_x2y2 = vpadd_f32(x2_y2, x2_y2);
		return vcombine_f32(x2y2_x2y2, x2y2_x2y2);
#else
		return vector_set(vector_dot2_as_scalar(lhs, rhs));
#endif
	}

	namespace rtm_impl
	{
		//////////////////////////////////////////////////////////////////////////
		// This is a helper struct to allow a single consistent API between
		// various vector types when the semantics are identical but the return
		// type differs. Implicit coercion is used to return the desired value
		// at the call site.
		//////////////////////////////////////////////////////////////////////////
		struct vector4f_vector_dot3
		{
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator float() const RTM_NO_EXCEPT
			{
#if defined(RTM_SSE4_INTRINSICS) && 0
				// SSE4 dot product instruction appears slower on Zen2, is it the case elsewhere as well?
				return _mm_cvtss_f32(_mm_dp_ps(lhs, rhs, 0x7F));
#elif defined(RTM_SSE2_INTRINSICS)
				__m128 x2_y2_z2_w2 = _mm_mul_ps(lhs, rhs);
				__m128 y2_0_0_0 = _mm_shuffle_ps(x2_y2_z2_w2, x2_y2_z2_w2, _MM_SHUFFLE(0, 0, 0, 1));
				__m128 x2y2_0_0_0 = _mm_add_ss(x2_y2_z2_w2, y2_0_0_0);
				__m128 z2_0_0_0 = _mm_shuffle_ps(x2_y2_z2_w2, x2_y2_z2_w2, _MM_SHUFFLE(0, 0, 0, 2));
				__m128 x2y2z2_0_0_0 = _mm_add_ss(x2y2_0_0_0, z2_0_0_0);
				return _mm_cvtss_f32(x2y2z2_0_0_0);
#elif defined(RTM_NEON_INTRINSICS)
				float32x4_t x2_y2_z2_w2 = vmulq_f32(lhs, rhs);
				float32x2_t x2_y2 = vget_low_f32(x2_y2_z2_w2);
				float32x2_t z2_w2 = vget_high_f32(x2_y2_z2_w2);
				float32x2_t x2y2_x2y2 = vpadd_f32(x2_y2, x2_y2);
				float32x2_t z2_z2 = vdup_lane_f32(z2_w2, 0);
				float32x2_t x2y2z2_x2y2z2 = vadd_f32(x2y2_x2y2, z2_z2);
				return vget_lane_f32(x2y2z2_x2y2z2, 0);
#else
				return (vector_get_x(lhs) * vector_get_x(rhs)) + (vector_get_y(lhs) * vector_get_y(rhs)) + (vector_get_z(lhs) * vector_get_z(rhs));
#endif
			}

#if defined(RTM_SSE2_INTRINSICS)
			RTM_DEPRECATED("Use 'as_scalar' suffix instead. To be removed in 2.4.")
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator scalarf() const RTM_NO_EXCEPT
			{
				__m128 x2_y2_z2_w2 = _mm_mul_ps(lhs, rhs);
				__m128 y2_0_0_0 = _mm_shuffle_ps(x2_y2_z2_w2, x2_y2_z2_w2, _MM_SHUFFLE(0, 0, 0, 1));
				__m128 x2y2_0_0_0 = _mm_add_ss(x2_y2_z2_w2, y2_0_0_0);
				__m128 z2_0_0_0 = _mm_shuffle_ps(x2_y2_z2_w2, x2_y2_z2_w2, _MM_SHUFFLE(0, 0, 0, 2));
				return scalarf{ _mm_add_ss(x2y2_0_0_0, z2_0_0_0) };
			}
#endif

			RTM_DEPRECATED("Use 'as_vector' suffix instead. To be removed in 2.4.")
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator vector4f() const RTM_NO_EXCEPT
			{
#if defined(RTM_SSE4_INTRINSICS) && 0
				// SSE4 dot product instruction appears slower on Zen2, is it the case elsewhere as well?
				return _mm_cvtss_f32(_mm_dp_ps(lhs, rhs, 0xFF));
#elif defined(RTM_SSE2_INTRINSICS)
				__m128 x2_y2_z2_w2 = _mm_mul_ps(lhs, rhs);
				__m128 y2_0_0_0 = _mm_shuffle_ps(x2_y2_z2_w2, x2_y2_z2_w2, _MM_SHUFFLE(0, 0, 0, 1));
				__m128 x2y2_0_0_0 = _mm_add_ss(x2_y2_z2_w2, y2_0_0_0);
				__m128 z2_0_0_0 = _mm_shuffle_ps(x2_y2_z2_w2, x2_y2_z2_w2, _MM_SHUFFLE(0, 0, 0, 2));
				__m128 x2y2z2_0_0_0 = _mm_add_ss(x2y2_0_0_0, z2_0_0_0);
				return _mm_shuffle_ps(x2y2z2_0_0_0, x2y2z2_0_0_0, _MM_SHUFFLE(0, 0, 0, 0));
#elif defined(RTM_NEON64_INTRINSICS)
				float32x4_t x2_y2_z2_w2 = vmulq_f32(lhs, rhs);
				float32x4_t x2_y2_z2 = vsetq_lane_f32(0.0F, x2_y2_z2_w2, 3);
				float32x4_t x2y2_z2_x2y2_z2 = vpaddq_f32(x2_y2_z2, x2_y2_z2);
				float32x4_t x2y2z2_x2y2z2_x2y2z2_x2y2z2 = vpaddq_f32(x2y2_z2_x2y2_z2, x2y2_z2_x2y2_z2);
				return x2y2z2_x2y2z2_x2y2z2_x2y2z2;
#elif defined(RTM_NEON_INTRINSICS)
				float32x4_t x2_y2_z2_w2 = vmulq_f32(lhs, rhs);
				float32x2_t x2_y2 = vget_low_f32(x2_y2_z2_w2);
				float32x2_t z2_w2 = vget_high_f32(x2_y2_z2_w2);
				float32x2_t x2y2_x2y2 = vpadd_f32(x2_y2, x2_y2);
				float32x2_t z2_z2 = vdup_lane_f32(z2_w2, 0);
				float32x2_t x2y2z2_x2y2z2 = vadd_f32(x2y2_x2y2, z2_z2);
				return vdupq_lane_f32(x2y2z2_x2y2z2, 0);
#else
				scalarf result = *this;
				return vector_set(result);
#endif
			}

			vector4f lhs;
			vector4f rhs;
		};
	}

	//////////////////////////////////////////////////////////////////////////
	// 3D dot product: lhs . rhs
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr rtm_impl::vector4f_vector_dot3 RTM_SIMD_CALL vector_dot3(vector4f_arg0 lhs, vector4f_arg1 rhs) RTM_NO_EXCEPT
	{
		return rtm_impl::vector4f_vector_dot3{ lhs, rhs };
	}

	//////////////////////////////////////////////////////////////////////////
	// 3D dot product: lhs . rhs
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE scalarf RTM_SIMD_CALL vector_dot3_as_scalar(vector4f_arg0 lhs, vector4f_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		__m128 x2_y2_z2_w2 = _mm_mul_ps(lhs, rhs);
		__m128 y2_0_0_0 = _mm_shuffle_ps(x2_y2_z2_w2, x2_y2_z2_w2, _MM_SHUFFLE(0, 0, 0, 1));
		__m128 x2y2_0_0_0 = _mm_add_ss(x2_y2_z2_w2, y2_0_0_0);
		__m128 z2_0_0_0 = _mm_shuffle_ps(x2_y2_z2_w2, x2_y2_z2_w2, _MM_SHUFFLE(0, 0, 0, 2));
		return scalarf{ _mm_add_ss(x2y2_0_0_0, z2_0_0_0) };
#else
		return vector_dot3(lhs, rhs);
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// 3D dot product: lhs . rhs
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4f RTM_SIMD_CALL vector_dot3_as_vector(vector4f_arg0 lhs, vector4f_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE4_INTRINSICS) && 0
		// SSE4 dot product instruction appears slower on Zen2, is it the case elsewhere as well?
		return _mm_cvtss_f32(_mm_dp_ps(lhs, rhs, 0xFF));
#elif defined(RTM_SSE2_INTRINSICS)
		__m128 x2_y2_z2_w2 = _mm_mul_ps(lhs, rhs);
		__m128 y2_0_0_0 = _mm_shuffle_ps(x2_y2_z2_w2, x2_y2_z2_w2, _MM_SHUFFLE(0, 0, 0, 1));
		__m128 x2y2_0_0_0 = _mm_add_ss(x2_y2_z2_w2, y2_0_0_0);
		__m128 z2_0_0_0 = _mm_shuffle_ps(x2_y2_z2_w2, x2_y2_z2_w2, _MM_SHUFFLE(0, 0, 0, 2));
		__m128 x2y2z2_0_0_0 = _mm_add_ss(x2y2_0_0_0, z2_0_0_0);
		return _mm_shuffle_ps(x2y2z2_0_0_0, x2y2z2_0_0_0, _MM_SHUFFLE(0, 0, 0, 0));
#elif defined(RTM_NEON64_INTRINSICS)
		float32x4_t x2_y2_z2_w2 = vmulq_f32(lhs, rhs);
		float32x4_t x2_y2_z2 = vsetq_lane_f32(0.0F, x2_y2_z2_w2, 3);
		float32x4_t x2y2_z2_x2y2_z2 = vpaddq_f32(x2_y2_z2, x2_y2_z2);
		float32x4_t x2y2z2_x2y2z2_x2y2z2_x2y2z2 = vpaddq_f32(x2y2_z2_x2y2_z2, x2y2_z2_x2y2_z2);
		return x2y2z2_x2y2z2_x2y2z2_x2y2z2;
#elif defined(RTM_NEON_INTRINSICS)
		float32x4_t x2_y2_z2_w2 = vmulq_f32(lhs, rhs);
		float32x2_t x2_y2 = vget_low_f32(x2_y2_z2_w2);
		float32x2_t z2_w2 = vget_high_f32(x2_y2_z2_w2);
		float32x2_t x2y2_x2y2 = vpadd_f32(x2_y2, x2_y2);
		float32x2_t z2_z2 = vdup_lane_f32(z2_w2, 0);
		float32x2_t x2y2z2_x2y2z2 = vadd_f32(x2y2_x2y2, z2_z2);
		return vdupq_lane_f32(x2y2z2_x2y2z2, 0);
#else
		return vector_set(vector_dot3_as_scalar(lhs, rhs));
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the squared length/norm of the vector4.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr rtm_impl::vector4f_vector_dot RTM_SIMD_CALL vector_length_squared(vector4f_arg0 input) RTM_NO_EXCEPT
	{
		return rtm_impl::vector4f_vector_dot{ input, input };
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the squared length/norm of the vector4.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE scalarf RTM_SIMD_CALL vector_length_squared_as_scalar(vector4f_arg0 input) RTM_NO_EXCEPT
	{
		return vector_dot_as_scalar(input, input);
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the squared length/norm of the vector4.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4f RTM_SIMD_CALL vector_length_squared_as_vector(vector4f_arg0 input) RTM_NO_EXCEPT
	{
		return vector_dot_as_vector(input, input);
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the squared length/norm of the vector2.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr rtm_impl::vector4f_vector_dot2 RTM_SIMD_CALL vector_length_squared2(vector4f_arg0 input) RTM_NO_EXCEPT
	{
		return rtm_impl::vector4f_vector_dot2{ input, input };
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the squared length/norm of the vector2.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE scalarf RTM_SIMD_CALL vector_length_squared2_as_scalar(vector4f_arg0 input) RTM_NO_EXCEPT
	{
		return vector_dot2_as_scalar(input, input);
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the squared length/norm of the vector2.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4f RTM_SIMD_CALL vector_length_squared2_as_vector(vector4f_arg0 input) RTM_NO_EXCEPT
	{
		return vector_dot2_as_vector(input, input);
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the squared length/norm of the vector3.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr rtm_impl::vector4f_vector_dot3 RTM_SIMD_CALL vector_length_squared3(vector4f_arg0 input) RTM_NO_EXCEPT
	{
		return rtm_impl::vector4f_vector_dot3{ input, input };
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the squared length/norm of the vector3.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE scalarf RTM_SIMD_CALL vector_length_squared3_as_scalar(vector4f_arg0 input) RTM_NO_EXCEPT
	{
		return vector_dot3_as_scalar(input, input);
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the squared length/norm of the vector3.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4f RTM_SIMD_CALL vector_length_squared3_as_vector(vector4f_arg0 input) RTM_NO_EXCEPT
	{
		return vector_dot3_as_vector(input, input);
	}

	namespace rtm_impl
	{
		//////////////////////////////////////////////////////////////////////////
		// This is a helper struct to allow a single consistent API between
		// various vector types when the semantics are identical but the return
		// type differs. Implicit coercion is used to return the desired value
		// at the call site.
		//////////////////////////////////////////////////////////////////////////
		struct vector4f_vector_length
		{
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator float() const RTM_NO_EXCEPT
			{
				const scalarf len_sq = vector_length_squared_as_scalar(input);
				return scalar_cast(scalar_sqrt(len_sq));
			}

#if defined(RTM_SSE2_INTRINSICS)
			RTM_DEPRECATED("Use 'as_scalar' suffix instead. To be removed in 2.4.")
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator scalarf() const RTM_NO_EXCEPT
			{
				const scalarf len_sq = vector_length_squared_as_scalar(input);
				return scalar_sqrt(len_sq);
			}
#endif

			vector4f input;
		};
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the length/norm of the vector4.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr rtm_impl::vector4f_vector_length RTM_SIMD_CALL vector_length(vector4f_arg0 input) RTM_NO_EXCEPT
	{
		return rtm_impl::vector4f_vector_length{ input };
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the length/norm of the vector4.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE scalarf RTM_SIMD_CALL vector_length_as_scalar(vector4f_arg0 input) RTM_NO_EXCEPT
	{
		const scalarf len_sq = vector_length_squared_as_scalar(input);
		return scalar_sqrt(len_sq);
	}

	namespace rtm_impl
	{
		//////////////////////////////////////////////////////////////////////////
		// This is a helper struct to allow a single consistent API between
		// various vector types when the semantics are identical but the return
		// type differs. Implicit coercion is used to return the desired value
		// at the call site.
		//////////////////////////////////////////////////////////////////////////
		struct vector4f_vector_length3
		{
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator float() const RTM_NO_EXCEPT
			{
				const scalarf len_sq = vector_length_squared3_as_scalar(input);
				return scalar_cast(scalar_sqrt(len_sq));
			}

#if defined(RTM_SSE2_INTRINSICS)
			RTM_DEPRECATED("Use 'as_scalar' suffix instead. To be removed in 2.4.")
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator scalarf() const RTM_NO_EXCEPT
			{
				const scalarf len_sq = vector_length_squared3_as_scalar(input);
				return scalar_sqrt(len_sq);
			}
#endif

			vector4f input;
		};
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the length/norm of the vector3.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr rtm_impl::vector4f_vector_length3 RTM_SIMD_CALL vector_length3(vector4f_arg0 input) RTM_NO_EXCEPT
	{
		return rtm_impl::vector4f_vector_length3{ input };
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the length/norm of the vector3.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE scalarf RTM_SIMD_CALL vector_length3_as_scalar(vector4f_arg0 input) RTM_NO_EXCEPT
	{
		const scalarf len_sq = vector_length_squared3_as_scalar(input);
		return scalar_sqrt(len_sq);
	}

	namespace rtm_impl
	{
		//////////////////////////////////////////////////////////////////////////
		// This is a helper struct to allow a single consistent API between
		// various vector types when the semantics are identical but the return
		// type differs. Implicit coercion is used to return the desired value
		// at the call site.
		//////////////////////////////////////////////////////////////////////////
		struct vector4f_vector_length_reciprocal
		{
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator float() const RTM_NO_EXCEPT
			{
				const scalarf len_sq = vector_length_squared_as_scalar(input);
				return scalar_cast(scalar_sqrt_reciprocal(len_sq));
			}

#if defined(RTM_SSE2_INTRINSICS)
			RTM_DEPRECATED("Use 'as_scalar' suffix instead. To be removed in 2.4.")
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator scalarf() const RTM_NO_EXCEPT
			{
				const scalarf len_sq = vector_length_squared_as_scalar(input);
				return scalar_sqrt_reciprocal(len_sq);
			}
#endif

			vector4f input;
		};
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the reciprocal length/norm of the vector4.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr rtm_impl::vector4f_vector_length_reciprocal RTM_SIMD_CALL vector_length_reciprocal(vector4f_arg0 input) RTM_NO_EXCEPT
	{
		return rtm_impl::vector4f_vector_length_reciprocal{ input };
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the reciprocal length/norm of the vector4.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE scalarf RTM_SIMD_CALL vector_length_reciprocal_as_scalar(vector4f_arg0 input) RTM_NO_EXCEPT
	{
		const scalarf len_sq = vector_length_squared_as_scalar(input);
		return scalar_sqrt_reciprocal(len_sq);
	}

	namespace rtm_impl
	{
		//////////////////////////////////////////////////////////////////////////
		// This is a helper struct to allow a single consistent API between
		// various vector types when the semantics are identical but the return
		// type differs. Implicit coercion is used to return the desired value
		// at the call site.
		//////////////////////////////////////////////////////////////////////////
		struct vector4f_vector_length_reciprocal2
		{
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator float() const RTM_NO_EXCEPT
			{
				const scalarf len_sq = vector_length_squared2_as_scalar(input);
				return scalar_cast(scalar_sqrt_reciprocal(len_sq));
			}

#if defined(RTM_SSE2_INTRINSICS)
			RTM_DEPRECATED("Use 'as_scalar' suffix instead. To be removed in 2.4.")
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator scalarf() const RTM_NO_EXCEPT
			{
				const scalarf len_sq = vector_length_squared2_as_scalar(input);
				return scalar_sqrt_reciprocal(len_sq);
			}
#endif

			vector4f input;
		};
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the reciprocal length/norm of the vector2.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr rtm_impl::vector4f_vector_length_reciprocal2 RTM_SIMD_CALL vector_length_reciprocal2(vector4f_arg0 input) RTM_NO_EXCEPT
	{
		return rtm_impl::vector4f_vector_length_reciprocal2{ input };
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the reciprocal length/norm of the vector2.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE scalarf RTM_SIMD_CALL vector_length_reciprocal2_as_scalar(vector4f_arg0 input) RTM_NO_EXCEPT
	{
		const scalarf len_sq = vector_length_squared2_as_scalar(input);
		return scalar_sqrt_reciprocal(len_sq);
	}

	namespace rtm_impl
	{
		//////////////////////////////////////////////////////////////////////////
		// This is a helper struct to allow a single consistent API between
		// various vector types when the semantics are identical but the return
		// type differs. Implicit coercion is used to return the desired value
		// at the call site.
		//////////////////////////////////////////////////////////////////////////
		struct vector4f_vector_length_reciprocal3
		{
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator float() const RTM_NO_EXCEPT
			{
				const scalarf len_sq = vector_length_squared3_as_scalar(input);
				return scalar_cast(scalar_sqrt_reciprocal(len_sq));
			}

#if defined(RTM_SSE2_INTRINSICS)
			RTM_DEPRECATED("Use 'as_scalar' suffix instead. To be removed in 2.4.")
			RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE RTM_SIMD_CALL operator scalarf() const RTM_NO_EXCEPT
			{
				const scalarf len_sq = vector_length_squared3_as_scalar(input);
				return scalar_sqrt_reciprocal(len_sq);
			}
#endif

			vector4f input;
		};
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the reciprocal length/norm of the vector3.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE constexpr rtm_impl::vector4f_vector_length_reciprocal3 RTM_SIMD_CALL vector_length_reciprocal3(vector4f_arg0 input) RTM_NO_EXCEPT
	{
		return rtm_impl::vector4f_vector_length_reciprocal3{ input };
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the reciprocal length/norm of the vector3.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE scalarf RTM_SIMD_CALL vector_length_reciprocal3_as_scalar(vector4f_arg0 input) RTM_NO_EXCEPT
	{
		const scalarf len_sq = vector_length_squared3_as_scalar(input);
		return scalar_sqrt_reciprocal(len_sq);
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the distance between two 3D points.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE rtm_impl::vector4f_vector_length3 RTM_SIMD_CALL vector_distance3(vector4f_arg0 lhs, vector4f_arg1 rhs) RTM_NO_EXCEPT
	{
		const vector4f difference = vector_sub(lhs, rhs);
		return rtm_impl::vector4f_vector_length3{ difference };
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the distance between two 3D points.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE scalarf RTM_SIMD_CALL vector_distance3_as_scalar(vector4f_arg0 lhs, vector4f_arg1 rhs) RTM_NO_EXCEPT
	{
		const vector4f difference = vector_sub(lhs, rhs);
		return vector_length3_as_scalar(difference);
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the squared distance between two 3D points.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE rtm_impl::vector4f_vector_dot3 RTM_SIMD_CALL vector_distance_squared3(vector4f_arg0 lhs, vector4f_arg1 rhs) RTM_NO_EXCEPT
	{
		const vector4f difference = vector_sub(lhs, rhs);
		return rtm_impl::vector4f_vector_dot3{ difference, difference };
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns the squared distance between two 3D points.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE scalarf RTM_SIMD_CALL vector_distance_squared3_as_scalar(vector4f_arg0 lhs, vector4f_arg1 rhs) RTM_NO_EXCEPT
	{
		const vector4f difference = vector_sub(lhs, rhs);
		return vector_length_squared3_as_scalar(difference);
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns a normalized vector2.
	// If the length of the input is not finite or zero, the result is undefined.
	// For a safe alternative, supply a fallback value and a threshold.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4f RTM_SIMD_CALL vector_normalize2(vector4f_arg0 input) RTM_NO_EXCEPT
	{
		// Reciprocal is more accurate to normalize with
		const scalarf len_sq = vector_length_squared2_as_scalar(input);
		return vector_mul(input, scalar_sqrt_reciprocal(len_sq));
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns a normalized vector2.
	// If the length of the input is below the supplied threshold, the
	// fall back value is returned instead.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4f RTM_SIMD_CALL vector_normalize2(vector4f_arg0 input, vector4f_arg1 fallback, float threshold = 1.0E-8F) RTM_NO_EXCEPT
	{
		// Reciprocal is more accurate to normalize with
		const scalarf len_sq = vector_length_squared2_as_scalar(input);
		if (scalar_cast(len_sq) >= threshold)
			return vector_mul(input, scalar_sqrt_reciprocal(len_sq));
		else
			return fallback;
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns a normalized vector3.
	// If the length of the input is not finite or zero, the result is undefined.
	// For a safe alternative, supply a fallback value and a threshold.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4f RTM_SIMD_CALL vector_normalize3(vector4f_arg0 input) RTM_NO_EXCEPT
	{
		// Reciprocal is more accurate to normalize with
		const scalarf len_sq = vector_length_squared3_as_scalar(input);
		return vector_mul(input, scalar_sqrt_reciprocal(len_sq));
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns a normalized vector3.
	// If the length of the input is below the supplied threshold, the
	// fall back value is returned instead.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4f RTM_SIMD_CALL vector_normalize3(vector4f_arg0 input, vector4f_arg1 fallback, float threshold = 1.0E-8F) RTM_NO_EXCEPT
	{
		// Reciprocal is more accurate to normalize with
		const scalarf len_sq = vector_length_squared3_as_scalar(input);
		if (scalar_cast(len_sq) >= threshold)
			return vector_mul(input, scalar_sqrt_reciprocal(len_sq));
		else
			return fallback;
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns a normalized vector4.
	// If the length of the input is not finite or zero, the result is undefined.
	// For a safe alternative, supply a fallback value and a threshold.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4f RTM_SIMD_CALL vector_normalize(vector4f_arg0 input) RTM_NO_EXCEPT
	{
		// Reciprocal is more accurate to normalize with
		const scalarf len_sq = vector_length_squared_as_scalar(input);
		return vector_mul(input, scalar_sqrt_reciprocal(len_sq));
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns a normalized vector4.
	// If the length of the input is below the supplied threshold, the
	// fall back value is returned instead.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4f RTM_SIMD_CALL vector_normalize(vector4f_arg0 input, vector4f_arg1 fallback, float threshold = 1.0E-8F) RTM_NO_EXCEPT
	{
		// Reciprocal is more accurate to normalize with
		const scalarf len_sq = vector_length_squared_as_scalar(input);
		if (scalar_cast(len_sq) >= threshold)
			return vector_mul(input, scalar_sqrt_reciprocal(len_sq));
		else
			return fallback;
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns per component the fractional part of the input.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline vector4f RTM_SIMD_CALL vector_fraction(vector4f_arg0 input) RTM_NO_EXCEPT
	{
		return vector_set(scalar_fraction(vector_get_x(input)), scalar_fraction(vector_get_y(input)), scalar_fraction(vector_get_z(input)), scalar_fraction(vector_get_w(input)));
	}

	//////////////////////////////////////////////////////////////////////////
	// Per component multiplication/addition of the three inputs: v2 + (v0 * v1)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4f RTM_SIMD_CALL vector_mul_add(vector4f_arg0 v0, vector4f_arg1 v1, vector4f_arg2 v2) RTM_NO_EXCEPT
	{
		return RTM_VECTOR4F_MULV_ADD(v0, v1, v2);
	}

	//////////////////////////////////////////////////////////////////////////
	// Per component multiplication/addition of the three inputs: v2 + (v0 * s1)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4f RTM_SIMD_CALL vector_mul_add(vector4f_arg0 v0, float s1, vector4f_arg2 v2) RTM_NO_EXCEPT
	{
		return RTM_VECTOR4F_MULS_ADD(v0, s1, v2);
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Per component multiplication/addition of the three inputs: v2 + (v0 * s1)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4f RTM_SIMD_CALL vector_mul_add(vector4f_arg0 v0, scalarf_arg1 s1, vector4f_arg2 v2) RTM_NO_EXCEPT
	{
		return vector_add(vector_mul(v0, s1), v2);
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Per component negative multiplication/subtraction of the three inputs: -((v0 * v1) - v2)
	// This is mathematically equivalent to: v2 - (v0 * v1)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4f RTM_SIMD_CALL vector_neg_mul_sub(vector4f_arg0 v0, vector4f_arg1 v1, vector4f_arg2 v2) RTM_NO_EXCEPT
	{
		return RTM_VECTOR4F_NEG_MULV_SUB(v0, v1, v2);
	}

	//////////////////////////////////////////////////////////////////////////
	// Per component negative multiplication/subtraction of the three inputs: -((v0 * s1) - v2)
	// This is mathematically equivalent to: v2 - (v0 * s1)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4f RTM_SIMD_CALL vector_neg_mul_sub(vector4f_arg0 v0, float s1, vector4f_arg2 v2) RTM_NO_EXCEPT
	{
		return RTM_VECTOR4F_NEG_MULS_SUB(v0, s1, v2);
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Per component negative multiplication/subtraction of the three inputs: -((v0 * s1) - v2)
	// This is mathematically equivalent to: v2 - (v0 * s1)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4f RTM_SIMD_CALL vector_neg_mul_sub(vector4f_arg0 v0, scalarf_arg1 s1, vector4f_arg2 v2) RTM_NO_EXCEPT
	{
		return vector_sub(v2, vector_mul(v0, s1));
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Per component linear interpolation of the two inputs at the specified alpha.
	// The formula used is: ((1.0 - alpha) * start) + (alpha * end).
	// Interpolation is stable and will return 'start' when alpha is 0.0 and 'end' when it is 1.0.
	// This is the same instruction count when FMA is present but it might be slightly slower
	// due to the extra multiplication compared to: start + (alpha * (end - start)).
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4f RTM_SIMD_CALL vector_lerp(vector4f_arg0 start, vector4f_arg1 end, float alpha) RTM_NO_EXCEPT
	{
		// ((1.0 - alpha) * start) + (alpha * end) == (start - alpha * start) + (alpha * end)
		return vector_mul_add(end, alpha, vector_neg_mul_sub(start, alpha, start));
	}

#if defined(RTM_SSE2_INTRINSICS)
	//////////////////////////////////////////////////////////////////////////
	// Per component linear interpolation of the two inputs at the specified alpha.
	// The formula used is: ((1.0 - alpha) * start) + (alpha * end).
	// Interpolation is stable and will return 'start' when alpha is 0.0 and 'end' when it is 1.0.
	// This is the same instruction count when FMA is present but it might be slightly slower
	// due to the extra multiplication compared to: start + (alpha * (end - start)).
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4f RTM_SIMD_CALL vector_lerp(vector4f_arg0 start, vector4f_arg1 end, scalarf_arg2 alpha) RTM_NO_EXCEPT
	{
		// ((1.0 - alpha) * start) + (alpha * end) == (start - alpha * start) + (alpha * end)
		const vector4f alpha_v = vector_set(alpha);
		return vector_mul_add(end, alpha_v, vector_neg_mul_sub(start, alpha_v, start));
	}
#endif

	//////////////////////////////////////////////////////////////////////////
	// Per component linear interpolation of the two inputs at the specified alpha.
	// The formula used is: ((1.0 - alpha) * start) + (alpha * end).
	// Interpolation is stable and will return 'start' when alpha is 0.0 and 'end' when it is 1.0.
	// This is the same instruction count when FMA is present but it might be slightly slower
	// due to the extra multiplication compared to: start + (alpha * (end - start)).
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4f RTM_SIMD_CALL vector_lerp(vector4f_arg0 start, vector4f_arg1 end, vector4f_arg2 alpha) RTM_NO_EXCEPT
	{
		// ((1.0 - alpha) * start) + (alpha * end) == (start - alpha * start) + (alpha * end)
		return vector_mul_add(end, alpha, vector_neg_mul_sub(start, alpha, start));
	}



	//////////////////////////////////////////////////////////////////////////
	// Comparisons and masking
	//////////////////////////////////////////////////////////////////////////


	//////////////////////////////////////////////////////////////////////////
	// Returns per component ~0 if equal, otherwise 0: lhs == rhs ? ~0 : 0
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE mask4f RTM_SIMD_CALL vector_equal(vector4f_arg0 lhs, vector4f_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return _mm_cmpeq_ps(lhs, rhs);
#elif defined(RTM_NEON_INTRINSICS)
		return vceqq_f32(lhs, rhs);
#else
		return mask4f{ rtm_impl::get_mask_value(lhs.x == rhs.x), rtm_impl::get_mask_value(lhs.y == rhs.y), rtm_impl::get_mask_value(lhs.z == rhs.z), rtm_impl::get_mask_value(lhs.w == rhs.w) };
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns per component ~0 if not equal, otherwise 0: lhs != rhs ? ~0 : 0
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE mask4f RTM_SIMD_CALL vector_not_equal(vector4f_arg0 lhs, vector4f_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return _mm_cmpneq_ps(lhs, rhs);
#elif defined(RTM_NEON_INTRINSICS)
		return vmvnq_u32(vceqq_f32(lhs, rhs));
#else
		return mask4f{ rtm_impl::get_mask_value(lhs.x != rhs.x), rtm_impl::get_mask_value(lhs.y != rhs.y), rtm_impl::get_mask_value(lhs.z != rhs.z), rtm_impl::get_mask_value(lhs.w != rhs.w) };
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns per component ~0 if less than, otherwise 0: lhs < rhs ? ~0 : 0
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE mask4f RTM_SIMD_CALL vector_less_than(vector4f_arg0 lhs, vector4f_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return _mm_cmplt_ps(lhs, rhs);
#elif defined(RTM_NEON_INTRINSICS)
		return vcltq_f32(lhs, rhs);
#else
		return mask4f{ rtm_impl::get_mask_value(lhs.x < rhs.x), rtm_impl::get_mask_value(lhs.y < rhs.y), rtm_impl::get_mask_value(lhs.z < rhs.z), rtm_impl::get_mask_value(lhs.w < rhs.w) };
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns per component ~0 if less equal, otherwise 0: lhs <= rhs ? ~0 : 0
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE mask4f RTM_SIMD_CALL vector_less_equal(vector4f_arg0 lhs, vector4f_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return _mm_cmple_ps(lhs, rhs);
#elif defined(RTM_NEON_INTRINSICS)
		return vcleq_f32(lhs, rhs);
#else
		return mask4f{ rtm_impl::get_mask_value(lhs.x <= rhs.x), rtm_impl::get_mask_value(lhs.y <= rhs.y), rtm_impl::get_mask_value(lhs.z <= rhs.z), rtm_impl::get_mask_value(lhs.w <= rhs.w) };
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns per component ~0 if greater than, otherwise 0: lhs > rhs ? ~0 : 0
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE mask4f RTM_SIMD_CALL vector_greater_than(vector4f_arg0 lhs, vector4f_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return _mm_cmpgt_ps(lhs, rhs);
#elif defined(RTM_NEON_INTRINSICS)
		return vcgtq_f32(lhs, rhs);
#else
		return mask4f{ rtm_impl::get_mask_value(lhs.x > rhs.x), rtm_impl::get_mask_value(lhs.y > rhs.y), rtm_impl::get_mask_value(lhs.z > rhs.z), rtm_impl::get_mask_value(lhs.w > rhs.w) };
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns per component ~0 if greater equal, otherwise 0: lhs >= rhs ? ~0 : 0
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE mask4f RTM_SIMD_CALL vector_greater_equal(vector4f_arg0 lhs, vector4f_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return _mm_cmpge_ps(lhs, rhs);
#elif defined(RTM_NEON_INTRINSICS)
		return vcgeq_f32(lhs, rhs);
#else
		return mask4f{ rtm_impl::get_mask_value(lhs.x >= rhs.x), rtm_impl::get_mask_value(lhs.y >= rhs.y), rtm_impl::get_mask_value(lhs.z >= rhs.z), rtm_impl::get_mask_value(lhs.w >= rhs.w) };
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if all 4 components are less than, otherwise false: all(lhs.xyzw < rhs.xyzw)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL vector_all_less_than(vector4f_arg0 lhs, vector4f_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		const __m128 mask = _mm_cmplt_ps(lhs, rhs);

		bool result;
		RTM_MASK4F_ALL_TRUE(mask, result);
		return result;
#elif defined(RTM_NEON_INTRINSICS)
		const uint32x4_t mask = vcltq_f32(lhs, rhs);

		bool result;
		RTM_MASK4F_ALL_TRUE(mask, result);
		return result;
#else
		return lhs.x < rhs.x && lhs.y < rhs.y && lhs.z < rhs.z && lhs.w < rhs.w;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if all [xy] components are less than, otherwise false: all(lhs.xy < rhs.xy)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL vector_all_less_than2(vector4f_arg0 lhs, vector4f_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		const __m128 mask = _mm_cmplt_ps(lhs, rhs);

		bool result;
		RTM_MASK4F_ALL_TRUE2(mask, result);
		return result;
#elif defined(RTM_NEON_INTRINSICS)
		const uint32x2_t mask = vclt_f32(vget_low_f32(lhs), vget_low_f32(rhs));

		bool result;
		RTM_MASK2F_ALL_TRUE(mask, result);
		return result;
#else
		return lhs.x < rhs.x && lhs.y < rhs.y;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if all [xyz] components are less than, otherwise false: all(lhs.xyz < rhs.xyz)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL vector_all_less_than3(vector4f_arg0 lhs, vector4f_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		const __m128 mask = _mm_cmplt_ps(lhs, rhs);

		bool result;
		RTM_MASK4F_ALL_TRUE3(mask, result);
		return result;
#elif defined(RTM_NEON_INTRINSICS)
		const uint32x4_t mask = vcltq_f32(lhs, rhs);

		bool result;
		RTM_MASK4F_ALL_TRUE3(mask, result);
		return result;
#else
		return lhs.x < rhs.x && lhs.y < rhs.y && lhs.z < rhs.z;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if any 4 components are less than, otherwise false: any(lhs.xyzw < rhs.xyzw)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL vector_any_less_than(vector4f_arg0 lhs, vector4f_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		const __m128 mask = _mm_cmplt_ps(lhs, rhs);

		bool result;
		RTM_MASK4F_ANY_TRUE(mask, result);
		return result;
#elif defined(RTM_NEON_INTRINSICS)
		const uint32x4_t mask = vcltq_f32(lhs, rhs);

		bool result;
		RTM_MASK4F_ANY_TRUE(mask, result);
		return result;
#else
		return lhs.x < rhs.x || lhs.y < rhs.y || lhs.z < rhs.z || lhs.w < rhs.w;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if any [xy] components are less than, otherwise false: any(lhs.xy < rhs.xy)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL vector_any_less_than2(vector4f_arg0 lhs, vector4f_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		const __m128 mask = _mm_cmplt_ps(lhs, rhs);

		bool result;
		RTM_MASK4F_ANY_TRUE2(mask, result);
		return result;
#elif defined(RTM_NEON_INTRINSICS)
		const uint32x2_t mask = vclt_f32(vget_low_f32(lhs), vget_low_f32(rhs));

		bool result;
		RTM_MASK2F_ANY_TRUE(mask, result);
		return result;
#else
		return lhs.x < rhs.x || lhs.y < rhs.y;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if any [xyz] components are less than, otherwise false: any(lhs.xyz < rhs.xyz)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL vector_any_less_than3(vector4f_arg0 lhs, vector4f_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		const __m128 mask = _mm_cmplt_ps(lhs, rhs);

		bool result;
		RTM_MASK4F_ANY_TRUE3(mask, result);
		return result;
#elif defined(RTM_NEON_INTRINSICS)
		const uint32x4_t mask = vcltq_f32(lhs, rhs);

		bool result;
		RTM_MASK4F_ANY_TRUE3(mask, result);
		return result;
#else
		return lhs.x < rhs.x || lhs.y < rhs.y || lhs.z < rhs.z;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if all 4 components are less equal, otherwise false: all(lhs.xyzw <= rhs.xyzw)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL vector_all_less_equal(vector4f_arg0 lhs, vector4f_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		const __m128 mask = _mm_cmple_ps(lhs, rhs);

		bool result;
		RTM_MASK4F_ALL_TRUE(mask, result);
		return result;
#elif defined(RTM_NEON_INTRINSICS)
		const uint32x4_t mask = vcleq_f32(lhs, rhs);

		bool result;
		RTM_MASK4F_ALL_TRUE(mask, result);
		return result;
#else
		return lhs.x <= rhs.x && lhs.y <= rhs.y && lhs.z <= rhs.z && lhs.w <= rhs.w;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if all [xy] components are less equal, otherwise false: all(lhs.xy <= rhs.xy)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL vector_all_less_equal2(vector4f_arg0 lhs, vector4f_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		const __m128 mask = _mm_cmple_ps(lhs, rhs);

		bool result;
		RTM_MASK4F_ALL_TRUE2(mask, result);
		return result;
#elif defined(RTM_NEON_INTRINSICS)
		const uint32x2_t mask = vcle_f32(vget_low_f32(lhs), vget_low_f32(rhs));

		bool result;
		RTM_MASK2F_ALL_TRUE(mask, result);
		return result;
#else
		return lhs.x <= rhs.x && lhs.y <= rhs.y;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if all [xyz] components are less equal, otherwise false: all(lhs.xyz <= rhs.xyz)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL vector_all_less_equal3(vector4f_arg0 lhs, vector4f_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		const __m128 mask = _mm_cmple_ps(lhs, rhs);

		bool result;
		RTM_MASK4F_ALL_TRUE3(mask, result);
		return result;
#elif defined(RTM_NEON_INTRINSICS)
		const uint32x4_t mask = vcleq_f32(lhs, rhs);

		bool result;
		RTM_MASK4F_ALL_TRUE3(mask, result);
		return result;
#else
		return lhs.x <= rhs.x && lhs.y <= rhs.y && lhs.z <= rhs.z;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if any 4 components are less equal, otherwise false: any(lhs.xyzw <= rhs.xyzw)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL vector_any_less_equal(vector4f_arg0 lhs, vector4f_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		const __m128 mask = _mm_cmple_ps(lhs, rhs);

		bool result;
		RTM_MASK4F_ANY_TRUE(mask, result);
		return result;
#elif defined(RTM_NEON_INTRINSICS)
		const uint32x4_t mask = vcleq_f32(lhs, rhs);

		bool result;
		RTM_MASK4F_ANY_TRUE(mask, result);
		return result;
#else
		return lhs.x <= rhs.x || lhs.y <= rhs.y || lhs.z <= rhs.z || lhs.w <= rhs.w;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if any [xy] components are less equal, otherwise false: any(lhs.xy <= rhs.xy)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL vector_any_less_equal2(vector4f_arg0 lhs, vector4f_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		const __m128 mask = _mm_cmple_ps(lhs, rhs);

		bool result;
		RTM_MASK4F_ANY_TRUE2(mask, result);
		return result;
#elif defined(RTM_NEON_INTRINSICS)
		const uint32x2_t mask = vcle_f32(vget_low_f32(lhs), vget_low_f32(rhs));

		bool result;
		RTM_MASK2F_ANY_TRUE(mask, result);
		return result;
#else
		return lhs.x <= rhs.x || lhs.y <= rhs.y;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if any [xyz] components are less equal, otherwise false: any(lhs.xyz <= rhs.xyz)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL vector_any_less_equal3(vector4f_arg0 lhs, vector4f_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		const __m128 mask = _mm_cmple_ps(lhs, rhs);

		bool result;
		RTM_MASK4F_ANY_TRUE3(mask, result);
		return result;
#elif defined(RTM_NEON_INTRINSICS)
		const uint32x4_t mask = vcleq_f32(lhs, rhs);

		bool result;
		RTM_MASK4F_ANY_TRUE3(mask, result);
		return result;
#else
		return lhs.x <= rhs.x || lhs.y <= rhs.y || lhs.z <= rhs.z;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if all 4 components are greater than, otherwise false: all(lhs.xyzw > rhs.xyzw)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL vector_all_greater_than(vector4f_arg0 lhs, vector4f_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		const __m128 mask = _mm_cmpgt_ps(lhs, rhs);

		bool result;
		RTM_MASK4F_ALL_TRUE(mask, result);
		return result;
#elif defined(RTM_NEON_INTRINSICS)
		const uint32x4_t mask = vcgtq_f32(lhs, rhs);

		bool result;
		RTM_MASK4F_ALL_TRUE(mask, result);
		return result;
#else
		return lhs.x > rhs.x && lhs.y > rhs.y && lhs.z > rhs.z && lhs.w > rhs.w;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if all [xy] components are greater than, otherwise false: all(lhs.xy > rhs.xy)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL vector_all_greater_than2(vector4f_arg0 lhs, vector4f_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		const __m128 mask = _mm_cmpgt_ps(lhs, rhs);

		bool result;
		RTM_MASK4F_ALL_TRUE2(mask, result);
		return result;
#elif defined(RTM_NEON_INTRINSICS)
		const uint32x2_t mask = vcgt_f32(vget_low_f32(lhs), vget_low_f32(rhs));

		bool result;
		RTM_MASK2F_ALL_TRUE(mask, result);
		return result;
#else
		return lhs.x > rhs.x && lhs.y > rhs.y;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if all [xyz] components are greater than, otherwise false: all(lhs.xyz > rhs.xyz)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL vector_all_greater_than3(vector4f_arg0 lhs, vector4f_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		const __m128 mask = _mm_cmpgt_ps(lhs, rhs);

		bool result;
		RTM_MASK4F_ALL_TRUE3(mask, result);
		return result;
#elif defined(RTM_NEON_INTRINSICS)
		const uint32x4_t mask = vcgtq_f32(lhs, rhs);

		bool result;
		RTM_MASK4F_ALL_TRUE3(mask, result);
		return result;
#else
		return lhs.x > rhs.x && lhs.y > rhs.y && lhs.z > rhs.z;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if any 4 components are greater than, otherwise false: any(lhs.xyzw > rhs.xyzw)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL vector_any_greater_than(vector4f_arg0 lhs, vector4f_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		const __m128 mask = _mm_cmpgt_ps(lhs, rhs);

		bool result;
		RTM_MASK4F_ANY_TRUE(mask, result);
		return result;
#elif defined(RTM_NEON_INTRINSICS)
		const uint32x4_t mask = vcgtq_f32(lhs, rhs);

		bool result;
		RTM_MASK4F_ANY_TRUE(mask, result);
		return result;
#else
		return lhs.x > rhs.x || lhs.y > rhs.y || lhs.z > rhs.z || lhs.w > rhs.w;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if any [xy] components are greater than, otherwise false: any(lhs.xy > rhs.xy)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL vector_any_greater_than2(vector4f_arg0 lhs, vector4f_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		const __m128 mask = _mm_cmpgt_ps(lhs, rhs);

		bool result;
		RTM_MASK4F_ANY_TRUE2(mask, result);
		return result;
#elif defined(RTM_NEON_INTRINSICS)
		const uint32x2_t mask = vcgt_f32(vget_low_f32(lhs), vget_low_f32(rhs));

		bool result;
		RTM_MASK2F_ANY_TRUE(mask, result);
		return result;
#else
		return lhs.x > rhs.x || lhs.y > rhs.y;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if any [xyz] components are greater than, otherwise false: any(lhs.xyz > rhs.xyz)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL vector_any_greater_than3(vector4f_arg0 lhs, vector4f_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		const __m128 mask = _mm_cmpgt_ps(lhs, rhs);

		bool result;
		RTM_MASK4F_ANY_TRUE3(mask, result);
		return result;
#elif defined(RTM_NEON_INTRINSICS)
		const uint32x4_t mask = vcgtq_f32(lhs, rhs);

		bool result;
		RTM_MASK4F_ANY_TRUE3(mask, result);
		return result;
#else
		return lhs.x > rhs.x || lhs.y > rhs.y || lhs.z > rhs.z;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if all 4 components are greater equal, otherwise false: all(lhs.xyzw >= rhs.xyzw)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL vector_all_greater_equal(vector4f_arg0 lhs, vector4f_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		const __m128 mask = _mm_cmpge_ps(lhs, rhs);

		bool result;
		RTM_MASK4F_ALL_TRUE(mask, result);
		return result;
#elif defined(RTM_NEON_INTRINSICS)
		const uint32x4_t mask = vcgeq_f32(lhs, rhs);

		bool result;
		RTM_MASK4F_ALL_TRUE(mask, result);
		return result;
#else
		return lhs.x >= rhs.x && lhs.y >= rhs.y && lhs.z >= rhs.z && lhs.w >= rhs.w;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if all [xy] components are greater equal, otherwise false: all(lhs.xy >= rhs.xy)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL vector_all_greater_equal2(vector4f_arg0 lhs, vector4f_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		const __m128 mask = _mm_cmpge_ps(lhs, rhs);

		bool result;
		RTM_MASK4F_ALL_TRUE2(mask, result);
		return result;
#elif defined(RTM_NEON_INTRINSICS)
		const uint32x2_t mask = vcge_f32(vget_low_f32(lhs), vget_low_f32(rhs));

		bool result;
		RTM_MASK2F_ALL_TRUE(mask, result);
		return result;
#else
		return lhs.x >= rhs.x && lhs.y >= rhs.y;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if all [xyz] components are greater equal, otherwise false: all(lhs.xyz >= rhs.xyz)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL vector_all_greater_equal3(vector4f_arg0 lhs, vector4f_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		const __m128 mask = _mm_cmpge_ps(lhs, rhs);

		bool result;
		RTM_MASK4F_ALL_TRUE3(mask, result);
		return result;
#elif defined(RTM_NEON_INTRINSICS)
		const uint32x4_t mask = vcgeq_f32(lhs, rhs);

		bool result;
		RTM_MASK4F_ALL_TRUE3(mask, result);
		return result;
#else
		return lhs.x >= rhs.x && lhs.y >= rhs.y && lhs.z >= rhs.z;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if any 4 components are greater equal, otherwise false: any(lhs.xyzw >= rhs.xyzw)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL vector_any_greater_equal(vector4f_arg0 lhs, vector4f_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		const __m128 mask = _mm_cmpge_ps(lhs, rhs);

		bool result;
		RTM_MASK4F_ANY_TRUE(mask, result);
		return result;
#elif defined(RTM_NEON_INTRINSICS)
		const uint32x4_t mask = vcgeq_f32(lhs, rhs);

		bool result;
		RTM_MASK4F_ANY_TRUE(mask, result);
		return result;
#else
		return lhs.x >= rhs.x || lhs.y >= rhs.y || lhs.z >= rhs.z || lhs.w >= rhs.w;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if any [xy] components are greater equal, otherwise false: any(lhs.xy >= rhs.xy)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL vector_any_greater_equal2(vector4f_arg0 lhs, vector4f_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		const __m128 mask = _mm_cmpge_ps(lhs, rhs);

		bool result;
		RTM_MASK4F_ANY_TRUE2(mask, result);
		return result;
#elif defined(RTM_NEON_INTRINSICS)
		const uint32x2_t mask = vcge_f32(vget_low_f32(lhs), vget_low_f32(rhs));

		bool result;
		RTM_MASK2F_ANY_TRUE(mask, result);
		return result;
#else
		return lhs.x >= rhs.x || lhs.y >= rhs.y;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if any [xyz] components are greater equal, otherwise false: any(lhs.xyz >= rhs.xyz)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL vector_any_greater_equal3(vector4f_arg0 lhs, vector4f_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		const __m128 mask = _mm_cmpge_ps(lhs, rhs);

		bool result;
		RTM_MASK4F_ANY_TRUE3(mask, result);
		return result;
#elif defined(RTM_NEON_INTRINSICS)
		const uint32x4_t mask = vcgeq_f32(lhs, rhs);

		bool result;
		RTM_MASK4F_ANY_TRUE3(mask, result);
		return result;
#else
		return lhs.x >= rhs.x || lhs.y >= rhs.y || lhs.z >= rhs.z;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if all [xyzw] components are equal, otherwise false: all(lhs.xyzw == rhs.xyzw)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL vector_all_equal(vector4f_arg0 lhs, vector4f_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		const __m128 mask = _mm_cmpeq_ps(lhs, rhs);

		bool result;
		RTM_MASK4F_ALL_TRUE(mask, result);
		return result;
#elif defined(RTM_NEON_INTRINSICS)
		const uint32x4_t mask = vceqq_f32(lhs, rhs);

		bool result;
		RTM_MASK4F_ALL_TRUE(mask, result);
		return result;
#else
		return lhs.x == rhs.x && lhs.y == rhs.y && lhs.z == rhs.z && lhs.w == rhs.w;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if all [xy] components are equal, otherwise false: all(lhs.xy == rhs.xy)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL vector_all_equal2(vector4f_arg0 lhs, vector4f_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		const __m128 mask = _mm_cmpeq_ps(lhs, rhs);

		bool result;
		RTM_MASK4F_ALL_TRUE2(mask, result);
		return result;
#elif defined(RTM_NEON_INTRINSICS)
		const uint32x2_t mask = vceq_f32(vget_low_f32(lhs), vget_low_f32(rhs));

		bool result;
		RTM_MASK2F_ALL_TRUE(mask, result);
		return result;
#else
		return lhs.x == rhs.x && lhs.y == rhs.y;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if all [xyz] components are equal, otherwise false: all(lhs.xyz == rhs.xyz)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL vector_all_equal3(vector4f_arg0 lhs, vector4f_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		const __m128 mask = _mm_cmpeq_ps(lhs, rhs);

		bool result;
		RTM_MASK4F_ALL_TRUE3(mask, result);
		return result;
#elif defined(RTM_NEON_INTRINSICS)
		const uint32x4_t mask = vceqq_f32(lhs, rhs);

		bool result;
		RTM_MASK4F_ALL_TRUE3(mask, result);
		return result;
#else
		return lhs.x == rhs.x && lhs.y == rhs.y && lhs.z == rhs.z;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if any [xyzw] components are equal, otherwise false: any(lhs.xyzw == rhs.xyzw)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL vector_any_equal(vector4f_arg0 lhs, vector4f_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		const __m128 mask = _mm_cmpeq_ps(lhs, rhs);

		bool result;
		RTM_MASK4F_ANY_TRUE(mask, result);
		return result;
#elif defined(RTM_NEON_INTRINSICS)
		const uint32x4_t mask = vceqq_f32(lhs, rhs);

		bool result;
		RTM_MASK4F_ANY_TRUE(mask, result);
		return result;
#else
		return lhs.x == rhs.x || lhs.y == rhs.y || lhs.z == rhs.z || lhs.w == rhs.w;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if any [xy] components are equal, otherwise false: any(lhs.xy == rhs.xy)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL vector_any_equal2(vector4f_arg0 lhs, vector4f_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		const __m128 mask = _mm_cmpeq_ps(lhs, rhs);

		bool result;
		RTM_MASK4F_ANY_TRUE2(mask, result);
		return result;
#elif defined(RTM_NEON_INTRINSICS)
		const uint32x2_t mask = vceq_f32(vget_low_f32(lhs), vget_low_f32(rhs));

		bool result;
		RTM_MASK2F_ANY_TRUE(mask, result);
		return result;
#else
		return lhs.x == rhs.x || lhs.y == rhs.y;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if any [xyz] components are equal, otherwise false: any(lhs.xyz == rhs.xyz)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL vector_any_equal3(vector4f_arg0 lhs, vector4f_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		const __m128 mask = _mm_cmpeq_ps(lhs, rhs);

		bool result;
		RTM_MASK4F_ANY_TRUE3(mask, result);
		return result;
#elif defined(RTM_NEON_INTRINSICS)
		const uint32x4_t mask = vceqq_f32(lhs, rhs);

		bool result;
		RTM_MASK4F_ANY_TRUE3(mask, result);
		return result;
#else
		return lhs.x == rhs.x || lhs.y == rhs.y || lhs.z == rhs.z;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if all [xyzw] components are not equal, otherwise false: all(lhs.xyzw != rhs.xyzw)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL vector_all_not_equal(vector4f_arg0 lhs, vector4f_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		const __m128 mask = _mm_cmpneq_ps(lhs, rhs);

		bool result;
		RTM_MASK4F_ALL_TRUE(mask, result);
		return result;
#elif defined(RTM_NEON_INTRINSICS)
		const uint32x4_t mask = vmvnq_u32(vceqq_f32(lhs, rhs));

		bool result;
		RTM_MASK4F_ALL_TRUE(mask, result);
		return result;
#else
		return lhs.x != rhs.x && lhs.y != rhs.y && lhs.z != rhs.z && lhs.w != rhs.w;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if all [xy] components are not equal, otherwise false: all(lhs.xy != rhs.xy)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL vector_all_not_equal2(vector4f_arg0 lhs, vector4f_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		const __m128 mask = _mm_cmpneq_ps(lhs, rhs);

		bool result;
		RTM_MASK4F_ALL_TRUE2(mask, result);
		return result;
#elif defined(RTM_NEON_INTRINSICS)
		const uint32x2_t mask = vmvn_u32(vceq_f32(vget_low_f32(lhs), vget_low_f32(rhs)));

		bool result;
		RTM_MASK2F_ALL_TRUE(mask, result);
		return result;
#else
		return lhs.x != rhs.x && lhs.y != rhs.y;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if all [xyz] components are not equal, otherwise false: all(lhs.xyz != rhs.xyz)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL vector_all_not_equal3(vector4f_arg0 lhs, vector4f_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		const __m128 mask = _mm_cmpneq_ps(lhs, rhs);

		bool result;
		RTM_MASK4F_ALL_TRUE3(mask, result);
		return result;
#elif defined(RTM_NEON_INTRINSICS)
		const uint32x4_t mask = vmvnq_u32(vceqq_f32(lhs, rhs));

		bool result;
		RTM_MASK4F_ALL_TRUE3(mask, result);
		return result;
#else
		return lhs.x != rhs.x && lhs.y != rhs.y && lhs.z != rhs.z;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if any [xyzw] components are not equal, otherwise false: any(lhs.xyzw != rhs.xyzw)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL vector_any_not_equal(vector4f_arg0 lhs, vector4f_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		const __m128 mask = _mm_cmpneq_ps(lhs, rhs);

		bool result;
		RTM_MASK4F_ANY_TRUE(mask, result);
		return result;
#elif defined(RTM_NEON_INTRINSICS)
		const uint32x4_t mask = vmvnq_u32(vceqq_f32(lhs, rhs));

		bool result;
		RTM_MASK4F_ANY_TRUE(mask, result);
		return result;
#else
		return lhs.x != rhs.x || lhs.y != rhs.y || lhs.z != rhs.z || lhs.w != rhs.w;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if any [xy] components are not equal, otherwise false: any(lhs.xy != rhs.xy)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL vector_any_not_equal2(vector4f_arg0 lhs, vector4f_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		const __m128 mask = _mm_cmpneq_ps(lhs, rhs);

		bool result;
		RTM_MASK4F_ANY_TRUE2(mask, result);
		return result;
#elif defined(RTM_NEON_INTRINSICS)
		const uint32x2_t mask = vmvn_u32(vceq_f32(vget_low_f32(lhs), vget_low_f32(rhs)));

		bool result;
		RTM_MASK2F_ANY_TRUE(mask, result);
		return result;
#else
		return lhs.x != rhs.x || lhs.y != rhs.y;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if any [xyz] components are not equal, otherwise false: any(lhs.xyz != rhs.xyz)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL vector_any_not_equal3(vector4f_arg0 lhs, vector4f_arg1 rhs) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		const __m128 mask = _mm_cmpneq_ps(lhs, rhs);

		bool result;
		RTM_MASK4F_ANY_TRUE3(mask, result);
		return result;
#elif defined(RTM_NEON_INTRINSICS)
		const uint32x4_t mask = vmvnq_u32(vceqq_f32(lhs, rhs));

		bool result;
		RTM_MASK4F_ANY_TRUE3(mask, result);
		return result;
#else
		return lhs.x != rhs.x || lhs.y != rhs.y || lhs.z != rhs.z;
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if all 4 components are near equal, otherwise false: all(abs(lhs - rhs).xyzw <= threshold)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL vector_all_near_equal(vector4f_arg0 lhs, vector4f_arg1 rhs, float threshold = 0.00001F) RTM_NO_EXCEPT
	{
		return vector_all_less_equal(vector_abs(vector_sub(lhs, rhs)), vector_set(threshold));
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if all [xy] components are near equal, otherwise false: all(abs(lhs - rhs).xy <= threshold)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL vector_all_near_equal2(vector4f_arg0 lhs, vector4f_arg1 rhs, float threshold = 0.00001F) RTM_NO_EXCEPT
	{
		return vector_all_less_equal2(vector_abs(vector_sub(lhs, rhs)), vector_set(threshold));
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if all [xyz] components are near equal, otherwise false: all(abs(lhs - rhs).xyz <= threshold)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL vector_all_near_equal3(vector4f_arg0 lhs, vector4f_arg1 rhs, float threshold = 0.00001F) RTM_NO_EXCEPT
	{
		return vector_all_less_equal3(vector_abs(vector_sub(lhs, rhs)), vector_set(threshold));
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if any 4 components are near equal, otherwise false: any(abs(lhs - rhs).xyzw <= threshold)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL vector_any_near_equal(vector4f_arg0 lhs, vector4f_arg1 rhs, float threshold = 0.00001F) RTM_NO_EXCEPT
	{
		return vector_any_less_equal(vector_abs(vector_sub(lhs, rhs)), vector_set(threshold));
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if any [xy] components are near equal, otherwise false: any(abs(lhs - rhs).xy <= threshold)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL vector_any_near_equal2(vector4f_arg0 lhs, vector4f_arg1 rhs, float threshold = 0.00001F) RTM_NO_EXCEPT
	{
		return vector_any_less_equal2(vector_abs(vector_sub(lhs, rhs)), vector_set(threshold));
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if any [xyz] components are near equal, otherwise false: any(abs(lhs - rhs).xyz <= threshold)
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL vector_any_near_equal3(vector4f_arg0 lhs, vector4f_arg1 rhs, float threshold = 0.00001F) RTM_NO_EXCEPT
	{
		return vector_any_less_equal3(vector_abs(vector_sub(lhs, rhs)), vector_set(threshold));
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns per component ~0 if input is finite, otherwise 0: finite(input) ? ~0 : 0
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE mask4f RTM_SIMD_CALL vector_finite(vector4f_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		const __m128i abs_mask = _mm_set_epi32(0x7FFFFFFFULL, 0x7FFFFFFFULL, 0x7FFFFFFFULL, 0x7FFFFFFFULL);
		__m128 abs_input = _mm_and_ps(input, _mm_castsi128_ps(abs_mask));

		const __m128 infinity = _mm_set_ps1(std::numeric_limits<float>::infinity());
		__m128 is_not_infinity = _mm_cmpneq_ps(abs_input, infinity);

		__m128 is_nan = _mm_cmpneq_ps(input, input);

		__m128 is_finite = _mm_andnot_ps(is_nan, is_not_infinity);
		return is_finite;
#elif defined(RTM_NEON_INTRINSICS)
		const float32x4_t abs_input = vabsq_f32(input);
		const float32x4_t infinity = vdupq_n_f32(std::numeric_limits<float>::infinity());
		const uint32x4_t is_not_infinity = vmvnq_u32(vceqq_f32(abs_input, infinity));
		const uint32x4_t is_not_nan = vceqq_f32(input, input);
		const uint32x4_t is_finite = vandq_u32(is_not_infinity, is_not_nan);
		return is_finite;
#else
		return mask4f{
			rtm_impl::get_mask_value(scalar_is_finite(vector_get_x(input))),
			rtm_impl::get_mask_value(scalar_is_finite(vector_get_y(input))),
			rtm_impl::get_mask_value(scalar_is_finite(vector_get_z(input))),
			rtm_impl::get_mask_value(scalar_is_finite(vector_get_w(input)))
			};
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if all 4 components are finite (not NaN/Inf), otherwise false: all(finite(input))
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL vector_is_finite(vector4f_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		const __m128i abs_mask = _mm_set_epi32(0x7FFFFFFFULL, 0x7FFFFFFFULL, 0x7FFFFFFFULL, 0x7FFFFFFFULL);
		__m128 abs_input = _mm_and_ps(input, _mm_castsi128_ps(abs_mask));

		const __m128 infinity = _mm_set_ps1(std::numeric_limits<float>::infinity());
		__m128 is_infinity = _mm_cmpeq_ps(abs_input, infinity);

		__m128 is_nan = _mm_cmpneq_ps(input, input);

		__m128 is_not_finite = _mm_or_ps(is_infinity, is_nan);
		return _mm_movemask_ps(is_not_finite) == 0;
#else
		return scalar_is_finite(vector_get_x(input)) && scalar_is_finite(vector_get_y(input)) && scalar_is_finite(vector_get_z(input)) && scalar_is_finite(vector_get_w(input));
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if all [xy] components are finite (not NaN/Inf), otherwise false: all(finite(input))
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL vector_is_finite2(vector4f_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		const __m128i abs_mask = _mm_set_epi32(0x7FFFFFFFULL, 0x7FFFFFFFULL, 0x7FFFFFFFULL, 0x7FFFFFFFULL);
		__m128 abs_input = _mm_and_ps(input, _mm_castsi128_ps(abs_mask));

		const __m128 infinity = _mm_set_ps1(std::numeric_limits<float>::infinity());
		__m128 is_infinity = _mm_cmpeq_ps(abs_input, infinity);

		__m128 is_nan = _mm_cmpneq_ps(input, input);

		__m128 is_not_finite = _mm_or_ps(is_infinity, is_nan);
		return (_mm_movemask_ps(is_not_finite) & 0x3) == 0;
#else
		return scalar_is_finite(vector_get_x(input)) && scalar_is_finite(vector_get_y(input));
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns true if all [xyz] components are finite (not NaN/Inf), otherwise false: all(finite(input))
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE bool RTM_SIMD_CALL vector_is_finite3(vector4f_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		const __m128i abs_mask = _mm_set_epi32(0x7FFFFFFFULL, 0x7FFFFFFFULL, 0x7FFFFFFFULL, 0x7FFFFFFFULL);
		__m128 abs_input = _mm_and_ps(input, _mm_castsi128_ps(abs_mask));

		const __m128 infinity = _mm_set_ps1(std::numeric_limits<float>::infinity());
		__m128 is_infinity = _mm_cmpeq_ps(abs_input, infinity);

		__m128 is_nan = _mm_cmpneq_ps(input, input);

		__m128 is_not_finite = _mm_or_ps(is_infinity, is_nan);
		return (_mm_movemask_ps(is_not_finite) & 0x7) == 0;
#else
		return scalar_is_finite(vector_get_x(input)) && scalar_is_finite(vector_get_y(input)) && scalar_is_finite(vector_get_z(input));
#endif
	}



	//////////////////////////////////////////////////////////////////////////
	// Swizzling, permutations, and mixing
	//////////////////////////////////////////////////////////////////////////


	//////////////////////////////////////////////////////////////////////////
	// Per component selection depending on the mask: mask != 0 ? if_true : if_false
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4f RTM_SIMD_CALL vector_select(mask4f_arg0 mask, vector4f_arg1 if_true, vector4f_arg2 if_false) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS) || defined(RTM_NEON_INTRINSICS)
		return RTM_VECTOR4F_SELECT(mask, if_true, if_false);
#else
		return vector4f{ rtm_impl::select(mask.x, if_true.x, if_false.x), rtm_impl::select(mask.y, if_true.y, if_false.y), rtm_impl::select(mask.z, if_true.z, if_false.z), rtm_impl::select(mask.w, if_true.w, if_false.w) };
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Mixes two inputs and returns the desired components.
	// [xyzw] indexes into the first input while [abcd] indexes in the second.
	//////////////////////////////////////////////////////////////////////////
	template<mix4 comp0, mix4 comp1, mix4 comp2, mix4 comp3>
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4f RTM_SIMD_CALL vector_mix(vector4f_arg0 input0, vector4f_arg1 input1) RTM_NO_EXCEPT
	{
		// Exactly input 0
		if (rtm_impl::static_condition<comp0 == mix4::x && comp1 == mix4::y && comp2 == mix4::z && comp3 == mix4::w>::test())
			return input0;

		// Exactly input 1
		if (rtm_impl::static_condition<comp0 == mix4::a && comp1 == mix4::b && comp2 == mix4::c && comp3 == mix4::d>::test())
			return input1;

#if defined(RTM_SSE2_INTRINSICS)
		// Instruction costs taken from Agner's instruction tables
		// Cost:	ops, latency, throughput, execution port

	#if defined(RTM_COMPILER_CLANG) || defined(RTM_COMPILER_GCC)
		#if RTM_HAS_BUILTIN(__builtin_shufflevector)
		// GCC/Clang offer a builtin intrinsic to generate optimal assembly for this
		return __builtin_shufflevector(input0, input1, int(comp0), int(comp1), int(comp2), int(comp3));
		#endif
	#endif

	#if defined(RTM_AVX_INTRINSICS)
		// TODO: AVX introduced the permute instruction (_mm_permute_ps) but on some AMD processors like Zen2
		// the instruction has much higher latency and a lower reciprocal throughput which makes it a poor fit
		// On the up side, it allows to load the input directly from memory in a single instruction
		// but for a function like vector_mix, we cannot assume whether any input is in memory or register
		// To make a smart choice for this, we would need to know at compile time what processor we are targeting
		// and this is not available to us. For that reason, I have opted to not implement its support for the
		// time being.
	#endif // defined(RTM_AVX_INTRINSICS)

	#if defined(RTM_SSE4_INTRINSICS)
		// blendps is generally always faster than other instructions and/or it can execute over more ports
		// AMD Jaguar:	???
		// AMD Zen 2:	1, 1, 0.33, P013
		// AMD Zen 4:	1, 1, 0.25, P0123

        // Each component comes from the respective position of input 0 or input 1
        if (rtm_impl::static_condition<(comp0 == mix4::a || comp0 == mix4::x) && (comp1 == mix4::b || comp1 == mix4::y) &&
                                       (comp2 == mix4::c || comp2 == mix4::z) && (comp3 == mix4::d || comp3 == mix4::w)>::test())
        {
            constexpr int mask = (comp0 == mix4::a ? 1 : 0) | (comp1 == mix4::b ? 2 : 0) |
                                 (comp2 == mix4::c ? 4 : 0) | (comp3 == mix4::d ? 8 : 0);
            return _mm_blend_ps(input0, input1, mask);
        }

		// insertps would otherwise require 2 shuffle instructions
		// AMD Jaguar:	1, ?, 0.5, FP01
		// AMD Zen 2:	1, 1, 0.5, P12
		// AMD Zen 4:	1, 1, 0.33, P123

        // First component comes from input 1, others come from the respective positions of input 0
        if (rtm_impl::static_condition<rtm_impl::is_mix_abcd(comp0) && comp1 == mix4::y && comp2 == mix4::z && comp3 == mix4::w>::test())
            return _mm_insert_ps(input0, input1, (int(comp0) % 4) << 6);

        // Second component comes from input 1, others come from the respective positions of input 0
        if (rtm_impl::static_condition<comp0 == mix4::x && rtm_impl::is_mix_abcd(comp1) && comp2 == mix4::z && comp3 == mix4::w>::test())
            return _mm_insert_ps(input0, input1, ((int(comp1) % 4) << 6) | (1 << 4));

        // Third component comes from input 1, others come from the respective positions of input 0
        if (rtm_impl::static_condition<comp0 == mix4::x && comp1 == mix4::y && rtm_impl::is_mix_abcd(comp2) && comp3 == mix4::w>::test())
            return _mm_insert_ps(input0, input1, ((int(comp2) % 4) << 6) | (2 << 4));

        // Fourth component comes from input 1, others come from the respective positions of input 0
        if (rtm_impl::static_condition<comp0 == mix4::x && comp1 == mix4::y && comp2 == mix4::z && rtm_impl::is_mix_abcd(comp3)>::test())
            return _mm_insert_ps(input0, input1, ((int(comp3) % 4) << 6) | (3 << 4));

        // First component comes from input 0, others come from the respective positions of input 1
        if (rtm_impl::static_condition<rtm_impl::is_mix_xyzw(comp0) && comp1 == mix4::b && comp2 == mix4::c && comp3 == mix4::d>::test())
            return _mm_insert_ps(input1, input0, (int(comp0) % 4) << 6);

        // Second component comes from input 0, others come from the respective positions of input 1
        if (rtm_impl::static_condition<comp0 == mix4::a && rtm_impl::is_mix_xyzw(comp1) && comp2 == mix4::c && comp3 == mix4::d>::test())
            return _mm_insert_ps(input1, input0, ((int(comp1) % 4) << 6) | (1 << 4));

        // Third component comes from input 0, others come from the respective positions of input 1
        if (rtm_impl::static_condition<comp0 == mix4::a && comp1 == mix4::b && rtm_impl::is_mix_xyzw(comp2) && comp3 == mix4::d>::test())
            return _mm_insert_ps(input1, input0, ((int(comp2) % 4) << 6) | (2 << 4));

        // Fourth component comes from input 0, others come from the respective positions of input 1
        if (rtm_impl::static_condition<comp0 == mix4::a && comp1 == mix4::b && comp2 == mix4::c && rtm_impl::is_mix_xyzw(comp3)>::test())
            return _mm_insert_ps(input1, input0, ((int(comp3) % 4) << 6) | (3 << 4));
	#endif // defined(RTM_SSE4_INTRINSICS)

	#if defined(RTM_SSE3_INTRINSICS)
		// movs[lh]dup is typically as fast or faster than shufps
		// AMD Jaguar:	1, 1, 0.5, FP01
		// AMD Zen 2:	1, 1, 0.5, P12
		// AMD Zen 4:	1, 1, 0.33, P123

		// Duplicate low elements of input 0
		if (rtm_impl::static_condition<comp0 == mix4::x && comp1 == mix4::x && comp2 == mix4::z && comp3 == mix4::z>::test())
			return _mm_moveldup_ps(input0);

		// Duplicate low elements of input 1
		if (rtm_impl::static_condition<comp0 == mix4::a && comp1 == mix4::a && comp2 == mix4::c && comp3 == mix4::c>::test())
			return _mm_moveldup_ps(input1);

		// Duplicate high elements of input 0
		if (rtm_impl::static_condition<comp0 == mix4::y && comp1 == mix4::y && comp2 == mix4::w && comp3 == mix4::w>::test())
			return _mm_movehdup_ps(input0);

		// Duplicate high elements of input 1
		if (rtm_impl::static_condition<comp0 == mix4::b && comp1 == mix4::b && comp2 == mix4::d && comp3 == mix4::d>::test())
			return _mm_movehdup_ps(input1);
	#endif // defined(RTM_SSE3_INTRINSICS)

		// unpck[hl]ps is typically as fast as shufps
		// AMD Jaguar:	1, 2, 0.5, FP01
		// AMD Zen 2:	1, 1, 0.5, P12
		// AMD Zen 4:	1, 1, 0.33, P123

		// Low words from both inputs are interleaved
		if (rtm_impl::static_condition<comp0 == mix4::x && comp1 == mix4::a && comp2 == mix4::y && comp3 == mix4::b>::test())
			return _mm_unpacklo_ps(input0, input1);

		// Low words from both inputs are interleaved
		if (rtm_impl::static_condition<comp0 == mix4::a && comp1 == mix4::x && comp2 == mix4::b && comp3 == mix4::y>::test())
			return _mm_unpacklo_ps(input1, input0);

		// Low word from input 0 is interleaved
		if (rtm_impl::static_condition<comp0 == mix4::x && comp1 == mix4::x && comp2 == mix4::y && comp3 == mix4::y>::test())
			return _mm_unpacklo_ps(input0, input0);

		// Low word from input 1 is interleaved
		if (rtm_impl::static_condition<comp0 == mix4::a && comp1 == mix4::a && comp2 == mix4::b && comp3 == mix4::b>::test())
			return _mm_unpacklo_ps(input1, input1);

		// High words from both inputs are interleaved
		if (rtm_impl::static_condition<comp0 == mix4::z && comp1 == mix4::c && comp2 == mix4::w && comp3 == mix4::d>::test())
			return _mm_unpackhi_ps(input0, input1);

		// High words from both inputs are interleaved
		if (rtm_impl::static_condition<comp0 == mix4::c && comp1 == mix4::z && comp2 == mix4::d && comp3 == mix4::w>::test())
			return _mm_unpackhi_ps(input1, input0);

		// High word from input 0 is interleaved
		if (rtm_impl::static_condition<comp0 == mix4::z && comp1 == mix4::z && comp2 == mix4::w && comp3 == mix4::w>::test())
			return _mm_unpackhi_ps(input0, input0);

		// High word from input 1 is interleaved
		if (rtm_impl::static_condition<comp0 == mix4::c && comp1 == mix4::c && comp2 == mix4::d && comp3 == mix4::d>::test())
			return _mm_unpackhi_ps(input1, input1);

#if 0
		// mov[lh/hl]ps is at times slower than shufps and it can execute on fewer ports
		// Its usage can be replicated using a single shufps instruction.
		// For these reasons, we do not use it. Code is left here to document that it was considered.
		// AMD Jaguar:	1, 2, 2, FP01
		// AMD Zen 2:	1, 1, 0.5, P12
		// AMD Zen 4:	1, 1, 0.5, P12

		// Duplicate bottom half of input 0
		if (rtm_impl::static_condition<comp0 == mix4::x && comp1 == mix4::y && comp2 == mix4::x && comp3 == mix4::y>::test())
			return _mm_movelh_ps(input0, input0);

		// Duplicate bottom half of input 1
		if (rtm_impl::static_condition<comp0 == mix4::a && comp1 == mix4::b && comp2 == mix4::a && comp3 == mix4::b>::test())
			return _mm_movelh_ps(input1, input1);

		// Move bottom half of input 1 to top of input 0
		if (rtm_impl::static_condition<comp0 == mix4::x && comp1 == mix4::y && comp2 == mix4::a && comp3 == mix4::b>::test())
			return _mm_movelh_ps(input0, input1);

		// Move bottom half of input 0 to top of input 1
		if (rtm_impl::static_condition<comp0 == mix4::a && comp1 == mix4::b && comp2 == mix4::x && comp3 == mix4::y>::test())
			return _mm_movelh_ps(input1, input0);

		// Duplicate top half of input 0
		if (rtm_impl::static_condition<comp0 == mix4::z && comp1 == mix4::w && comp2 == mix4::z && comp3 == mix4::w>::test())
			return _mm_movehl_ps(input0, input0);

		// Duplicate top half of input 1
		if (rtm_impl::static_condition<comp0 == mix4::c && comp1 == mix4::d && comp2 == mix4::c && comp3 == mix4::d>::test())
			return _mm_movehl_ps(input1, input1);

		// Move top half of input 1 to top of input 0
		if (rtm_impl::static_condition<comp0 == mix4::z && comp1 == mix4::w && comp2 == mix4::c && comp3 == mix4::d>::test())
			return _mm_movehl_ps(input1, input0);

		// Move top half of input 0 to top of input 1
		if (rtm_impl::static_condition<comp0 == mix4::c && comp1 == mix4::d && comp2 == mix4::z && comp3 == mix4::w>::test())
			return _mm_movehl_ps(input0, input1);
#endif

		// shufps is the baseline instruction for vector_mix
		// AMD Jaguar:	1, 2, 0.5, FP01
		// AMD Zen 2:	1, 1, 0.5, P12
		// AMD Zen 4:	1, 1, 0.33, P123

		// All four components come from input 0
		if (rtm_impl::static_condition<rtm_impl::is_mix_xyzw(comp0) && rtm_impl::is_mix_xyzw(comp1) && rtm_impl::is_mix_xyzw(comp2) && rtm_impl::is_mix_xyzw(comp3)>::test())
			return _mm_shuffle_ps(input0, input0, _MM_SHUFFLE(int(comp3) % 4, int(comp2) % 4, int(comp1) % 4, int(comp0) % 4));

		// All four components come from input 1
		if (rtm_impl::static_condition<rtm_impl::is_mix_abcd(comp0) && rtm_impl::is_mix_abcd(comp1) && rtm_impl::is_mix_abcd(comp2) && rtm_impl::is_mix_abcd(comp3)>::test())
			return _mm_shuffle_ps(input1, input1, _MM_SHUFFLE(int(comp3) % 4, int(comp2) % 4, int(comp1) % 4, int(comp0) % 4));

		// First two components come from input 0, second two come from input 1
		if (rtm_impl::static_condition<rtm_impl::is_mix_xyzw(comp0) && rtm_impl::is_mix_xyzw(comp1) && rtm_impl::is_mix_abcd(comp2) && rtm_impl::is_mix_abcd(comp3)>::test())
			return _mm_shuffle_ps(input0, input1, _MM_SHUFFLE(int(comp3) % 4, int(comp2) % 4, int(comp1) % 4, int(comp0) % 4));

		// First two components come from input 1, second two come from input 0
		if (rtm_impl::static_condition<rtm_impl::is_mix_abcd(comp0) && rtm_impl::is_mix_abcd(comp1) && rtm_impl::is_mix_xyzw(comp2) && rtm_impl::is_mix_xyzw(comp3)>::test())
			return _mm_shuffle_ps(input1, input0, _MM_SHUFFLE(int(comp3) % 4, int(comp2) % 4, int(comp1) % 4, int(comp0) % 4));

		// Mixed components
		if (rtm_impl::static_condition<rtm_impl::is_mix_xyzw(comp0) && rtm_impl::is_mix_xyzw(comp1) && rtm_impl::is_mix_xyzw(comp2) && rtm_impl::is_mix_abcd(comp3)>::test())
		{
			const __m128 z0z0w1w1 = _mm_shuffle_ps(input0, input1, _MM_SHUFFLE(int(comp3) % 4, int(comp3) % 4, int(comp2) % 4, int(comp2) % 4));
			return _mm_shuffle_ps(input0, z0z0w1w1, _MM_SHUFFLE(2, 0, int(comp1) % 4, int(comp0) % 4));
		}

		// Mixed components
		if (rtm_impl::static_condition<rtm_impl::is_mix_xyzw(comp0) && rtm_impl::is_mix_xyzw(comp1) && rtm_impl::is_mix_abcd(comp2) && rtm_impl::is_mix_xyzw(comp3)>::test())
		{
			const __m128 z1z1w0w0 = _mm_shuffle_ps(input1, input0, _MM_SHUFFLE(int(comp3) % 4, int(comp3) % 4, int(comp2) % 4, int(comp2) % 4));
			return _mm_shuffle_ps(input0, z1z1w0w0, _MM_SHUFFLE(2, 0, int(comp1) % 4, int(comp0) % 4));
		}

		// Mixed components
		if (rtm_impl::static_condition<rtm_impl::is_mix_xyzw(comp0) && rtm_impl::is_mix_abcd(comp1) && rtm_impl::is_mix_xyzw(comp2) && rtm_impl::is_mix_xyzw(comp3)>::test())
		{
			const __m128 x0x0y1y1 = _mm_shuffle_ps(input0, input1, _MM_SHUFFLE(int(comp1) % 4, int(comp1) % 4, int(comp0) % 4, int(comp0) % 4));
			return _mm_shuffle_ps(x0x0y1y1, input0, _MM_SHUFFLE(int(comp3) % 4, int(comp2) % 4, 2, 0));
		}

		// Mixed components
		if (rtm_impl::static_condition<rtm_impl::is_mix_abcd(comp0) && rtm_impl::is_mix_xyzw(comp1) && rtm_impl::is_mix_xyzw(comp2) && rtm_impl::is_mix_xyzw(comp3)>::test())
		{
			const __m128 x1x1y0y0 = _mm_shuffle_ps(input1, input0, _MM_SHUFFLE(int(comp1) % 4, int(comp1) % 4, int(comp0) % 4, int(comp0) % 4));
			return _mm_shuffle_ps(x1x1y0y0, input0, _MM_SHUFFLE(int(comp3) % 4, int(comp2) % 4, 2, 0));
		}

		// Mixed components
		if (rtm_impl::static_condition<rtm_impl::is_mix_abcd(comp0) && rtm_impl::is_mix_abcd(comp1) && rtm_impl::is_mix_abcd(comp2) && rtm_impl::is_mix_xyzw(comp3)>::test())
		{
			const __m128 z1z1w0w0 = _mm_shuffle_ps(input1, input0, _MM_SHUFFLE(int(comp3) % 4, int(comp3) % 4, int(comp2) % 4, int(comp2) % 4));
			return _mm_shuffle_ps(input1, z1z1w0w0, _MM_SHUFFLE(2, 0, int(comp1) % 4, int(comp0) % 4));
		}

		// Mixed components
		if (rtm_impl::static_condition<rtm_impl::is_mix_abcd(comp0) && rtm_impl::is_mix_abcd(comp1) && rtm_impl::is_mix_xyzw(comp2) && rtm_impl::is_mix_abcd(comp3)>::test())
		{
			const __m128 z0z0w1w1 = _mm_shuffle_ps(input0, input1, _MM_SHUFFLE(int(comp3) % 4, int(comp3) % 4, int(comp2) % 4, int(comp2) % 4));
			return _mm_shuffle_ps(input1, z0z0w1w1, _MM_SHUFFLE(2, 0, int(comp1) % 4, int(comp0) % 4));
		}

		// Mixed components
		if (rtm_impl::static_condition<rtm_impl::is_mix_abcd(comp0) && rtm_impl::is_mix_xyzw(comp1) && rtm_impl::is_mix_abcd(comp2) && rtm_impl::is_mix_abcd(comp3)>::test())
		{
			const __m128 x1x1y0y0 = _mm_shuffle_ps(input1, input0, _MM_SHUFFLE(int(comp1) % 4, int(comp1) % 4, int(comp0) % 4, int(comp0) % 4));
			return _mm_shuffle_ps(x1x1y0y0, input1, _MM_SHUFFLE(int(comp3) % 4, int(comp2) % 4, 2, 0));
		}

		// Mixed components
		if (rtm_impl::static_condition<rtm_impl::is_mix_xyzw(comp0) && rtm_impl::is_mix_abcd(comp1) && rtm_impl::is_mix_abcd(comp2) && rtm_impl::is_mix_abcd(comp3)>::test())
		{
			const __m128 x0x0y1y1 = _mm_shuffle_ps(input0, input1, _MM_SHUFFLE(int(comp1) % 4, int(comp1) % 4, int(comp0) % 4, int(comp0) % 4));
			return _mm_shuffle_ps(x0x0y1y1, input1, _MM_SHUFFLE(int(comp3) % 4, int(comp2) % 4, 2, 0));
		}

		// Mixed components
		if (rtm_impl::static_condition<rtm_impl::is_mix_xyzw(comp0) && rtm_impl::is_mix_abcd(comp1) && rtm_impl::is_mix_xyzw(comp2) && rtm_impl::is_mix_abcd(comp3)>::test())
		{
			const __m128 x0x0y1y1 = _mm_shuffle_ps(input0, input1, _MM_SHUFFLE(int(comp1) % 4, int(comp1) % 4, int(comp0) % 4, int(comp0) % 4));
			const __m128 z0z0w1w1 = _mm_shuffle_ps(input0, input1, _MM_SHUFFLE(int(comp3) % 4, int(comp3) % 4, int(comp2) % 4, int(comp2) % 4));
			return _mm_shuffle_ps(x0x0y1y1, z0z0w1w1, _MM_SHUFFLE(2, 0, 2, 0));
		}

		// Mixed components
		if (rtm_impl::static_condition<rtm_impl::is_mix_abcd(comp0) && rtm_impl::is_mix_xyzw(comp1) && rtm_impl::is_mix_abcd(comp2) && rtm_impl::is_mix_xyzw(comp3)>::test())
		{
			const __m128 x1x1y0y0 = _mm_shuffle_ps(input1, input0, _MM_SHUFFLE(int(comp1) % 4, int(comp1) % 4, int(comp0) % 4, int(comp0) % 4));
			const __m128 z1z1w0w0 = _mm_shuffle_ps(input1, input0, _MM_SHUFFLE(int(comp3) % 4, int(comp3) % 4, int(comp2) % 4, int(comp2) % 4));
			return _mm_shuffle_ps(x1x1y0y0, z1z1w0w0, _MM_SHUFFLE(2, 0, 2, 0));
		}

		// Mixed components
		if (rtm_impl::static_condition<rtm_impl::is_mix_xyzw(comp0) && rtm_impl::is_mix_abcd(comp1) && rtm_impl::is_mix_abcd(comp2) && rtm_impl::is_mix_xyzw(comp3)>::test())
		{
			const __m128 x0x0y1y1 = _mm_shuffle_ps(input0, input1, _MM_SHUFFLE(int(comp1) % 4, int(comp1) % 4, int(comp0) % 4, int(comp0) % 4));
			const __m128 z1z1w0w0 = _mm_shuffle_ps(input1, input0, _MM_SHUFFLE(int(comp3) % 4, int(comp3) % 4, int(comp2) % 4, int(comp2) % 4));
			return _mm_shuffle_ps(x0x0y1y1, z1z1w0w0, _MM_SHUFFLE(2, 0, 2, 0));
		}

		// Mixed components
		if (rtm_impl::static_condition<rtm_impl::is_mix_abcd(comp0) && rtm_impl::is_mix_xyzw(comp1) && rtm_impl::is_mix_xyzw(comp2) && rtm_impl::is_mix_abcd(comp3)>::test())
		{
			const __m128 x1x1y0y0 = _mm_shuffle_ps(input1, input0, _MM_SHUFFLE(int(comp1) % 4, int(comp1) % 4, int(comp0) % 4, int(comp0) % 4));
			const __m128 z0z0w1w1 = _mm_shuffle_ps(input0, input1, _MM_SHUFFLE(int(comp3) % 4, int(comp3) % 4, int(comp2) % 4, int(comp2) % 4));
			return _mm_shuffle_ps(x1x1y0y0, z0z0w1w1, _MM_SHUFFLE(2, 0, 2, 0));
		}

		// All vector_mix permutations are handled above, return a dummy value to avoid warnings
		return _mm_setzero_ps();
#elif defined(RTM_NEON_INTRINSICS)
	#if defined(RTM_COMPILER_CLANG) || defined(RTM_COMPILER_GCC)
		#if RTM_HAS_BUILTIN(__builtin_shufflevector)
			// GCC/Clang offer a builtin intrinsic to generate optimal assembly for this
			return __builtin_shufflevector(input0, input1, int(comp0), int(comp1), int(comp2), int(comp3));
		#endif
	#endif

	#if defined(RTM_NEON64_INTRINSICS)
		// Duplicate low words of input 0
		if (rtm_impl::static_condition<comp0 == mix4::x && comp1 == mix4::x && comp2 == mix4::y && comp3 == mix4::y>::test())
			return vzip1q_f32(input0, input0);

		// Duplicate low words of input 1
		if (rtm_impl::static_condition<comp0 == mix4::a && comp1 == mix4::a && comp2 == mix4::b && comp3 == mix4::b>::test())
			return vzip1q_f32(input1, input1);

		// Low words from both inputs are interleaved
		if (rtm_impl::static_condition<comp0 == mix4::x && comp1 == mix4::a && comp2 == mix4::y && comp3 == mix4::b>::test())
			return vzip1q_f32(input0, input1);

		// Low words from both inputs are interleaved
		if (rtm_impl::static_condition<comp0 == mix4::a && comp1 == mix4::x && comp2 == mix4::b && comp3 == mix4::y>::test())
			return vzip1q_f32(input1, input0);

		// Duplicate high words of input 0
		if (rtm_impl::static_condition<comp0 == mix4::z && comp1 == mix4::z && comp2 == mix4::w && comp3 == mix4::w>::test())
			return vzip2q_f32(input0, input0);

		// Duplicate high words of input 1
		if (rtm_impl::static_condition<comp0 == mix4::c && comp1 == mix4::c && comp2 == mix4::d && comp3 == mix4::d>::test())
			return vzip2q_f32(input1, input1);

		// High words from both inputs are interleaved
		if (rtm_impl::static_condition<comp0 == mix4::z && comp1 == mix4::c && comp2 == mix4::w && comp3 == mix4::d>::test())
			return vzip2q_f32(input0, input1);

		// High words from both inputs are interleaved
		if (rtm_impl::static_condition<comp0 == mix4::c && comp1 == mix4::z && comp2 == mix4::d && comp3 == mix4::w>::test())
			return vzip2q_f32(input1, input0);

		// Even-numbered vector elements, interleaved
		if (rtm_impl::static_condition<comp0 == mix4::x && comp1 == mix4::z && comp2 == mix4::x && comp3 == mix4::z>::test())
			return vuzp1q_f32(input0, input0);

		// Even-numbered vector elements, interleaved
		if (rtm_impl::static_condition<comp0 == mix4::a && comp1 == mix4::c && comp2 == mix4::a && comp3 == mix4::c>::test())
			return vuzp1q_f32(input1, input1);

		// Even-numbered vector elements, consecutively
		if (rtm_impl::static_condition<comp0 == mix4::x && comp1 == mix4::z && comp2 == mix4::a && comp3 == mix4::c>::test())
			return vuzp1q_f32(input0, input1);

		// Even-numbered vector elements, consecutively
		if (rtm_impl::static_condition<comp0 == mix4::a && comp1 == mix4::c && comp2 == mix4::x && comp3 == mix4::z>::test())
			return vuzp1q_f32(input1, input0);

		// Odd-numbered vector elements, interleaved
		if (rtm_impl::static_condition<comp0 == mix4::y && comp1 == mix4::w && comp2 == mix4::y && comp3 == mix4::w>::test())
			return vuzp2q_f32(input0, input0);

		// Odd-numbered vector elements, interleaved
		if (rtm_impl::static_condition<comp0 == mix4::b && comp1 == mix4::d && comp2 == mix4::b && comp3 == mix4::d>::test())
			return vuzp2q_f32(input1, input1);

		// Odd-numbered vector elements, consecutively
		if (rtm_impl::static_condition<comp0 == mix4::y && comp1 == mix4::w && comp2 == mix4::b && comp3 == mix4::d>::test())
			return vuzp2q_f32(input0, input1);

		// Odd-numbered vector elements, consecutively
		if (rtm_impl::static_condition<comp0 == mix4::b && comp1 == mix4::d && comp2 == mix4::y && comp3 == mix4::w>::test())
			return vuzp2q_f32(input1, input0);

		// Duplicate even-numbered vector elements, consecutively
		if (rtm_impl::static_condition<comp0 == mix4::x && comp1 == mix4::x && comp2 == mix4::z && comp3 == mix4::z>::test())
			return vtrn1q_f32(input0, input0);

		// Duplicate even-numbered vector elements, consecutively
		if (rtm_impl::static_condition<comp0 == mix4::a && comp1 == mix4::a && comp2 == mix4::c && comp3 == mix4::c>::test())
			return vtrn1q_f32(input1, input1);

		// Even-numbered vector elements, interleaved
		if (rtm_impl::static_condition<comp0 == mix4::x && comp1 == mix4::a && comp2 == mix4::z && comp3 == mix4::c>::test())
			return vtrn1q_f32(input0, input1);

		// Even-numbered vector elements, interleaved
		if (rtm_impl::static_condition<comp0 == mix4::a && comp1 == mix4::x && comp2 == mix4::c && comp3 == mix4::z>::test())
			return vtrn1q_f32(input1, input0);

		// Duplicate odd-numbered vector elements, consecutively
		if (rtm_impl::static_condition<comp0 == mix4::y && comp1 == mix4::y && comp2 == mix4::w && comp3 == mix4::w>::test())
			return vtrn2q_f32(input0, input0);

		// Duplicate odd-numbered vector elements, consecutively
		if (rtm_impl::static_condition<comp0 == mix4::b && comp1 == mix4::b && comp2 == mix4::d && comp3 == mix4::d>::test())
			return vtrn2q_f32(input1, input1);

		// Odd-numbered vector elements, interleaved
		if (rtm_impl::static_condition<comp0 == mix4::y && comp1 == mix4::b && comp2 == mix4::w && comp3 == mix4::d>::test())
			return vtrn2q_f32(input0, input1);

		// Odd-numbered vector elements, interleaved
		if (rtm_impl::static_condition<comp0 == mix4::b && comp1 == mix4::y && comp2 == mix4::d && comp3 == mix4::w>::test())
			return vtrn2q_f32(input1, input0);
	#endif // defined(RTM_NEON64_INTRINSICS)

		// The highest vector elements from input 0 and the lowest vector elements from input 1, consecutively
		if (rtm_impl::static_condition<rtm_impl::is_mix_xyzw(comp0) &&
									int(comp0) + 1 == int(comp1) &&
									int(comp1) + 1 == int(comp2) &&
									int(comp2) + 1 == int(comp3)>::test())
			return vextq_f32(input0, input1, int(comp0) % 4);

		// The highest vector elements from input 1 and the lowest vector elements from input 0, consecutively
		if (rtm_impl::static_condition<rtm_impl::is_mix_abcd(comp0) &&
									(int(comp0) + 1) % 8 == int(comp1) &&
									(int(comp1) + 1) % 8 == int(comp2) &&
									(int(comp2) + 1) % 8 == int(comp3)>::test())
			return vextq_f32(input1, input0, int(comp0) % 4);

		// All four components come from input 0, reversed order in each doubleword
		if (rtm_impl::static_condition<comp0 == mix4::y && comp1 == mix4::x && comp2 == mix4::w && comp3 == mix4::z>::test())
			return vrev64q_f32(input0);

		// All four components come from input 1, reversed order in each doubleword
		if (rtm_impl::static_condition<comp0 == mix4::b && comp1 == mix4::a && comp2 == mix4::d && comp3 == mix4::c>::test())
			return vrev64q_f32(input1);

		// First component comes from input 1, others come from the respective positions of input 0
		if (rtm_impl::static_condition<rtm_impl::is_mix_abcd(comp0) && comp1 == mix4::y && comp2 == mix4::z && comp3 == mix4::w>::test())
			return vsetq_lane_f32(vgetq_lane_f32(input1, int(comp0) % 4), input0, 0);

		// Second component comes from input 1, others come from the respective positions of input 0
		if (rtm_impl::static_condition<comp0 == mix4::x && rtm_impl::is_mix_abcd(comp1) && comp2 == mix4::z && comp3 == mix4::w>::test())
			return vsetq_lane_f32(vgetq_lane_f32(input1, int(comp1) % 4), input0, 1);

		// Third component comes from input 1, others come from the respective positions of input 0
		if (rtm_impl::static_condition<comp0 == mix4::x && comp1 == mix4::y && rtm_impl::is_mix_abcd(comp2) && comp3 == mix4::w>::test())
			return vsetq_lane_f32(vgetq_lane_f32(input1, int(comp2) % 4), input0, 2);

		// Fourth component comes from input 1, others come from the respective positions of input 0
		if (rtm_impl::static_condition<comp0 == mix4::x && comp1 == mix4::y && comp2 == mix4::z && rtm_impl::is_mix_abcd(comp3)>::test())
			return vsetq_lane_f32(vgetq_lane_f32(input1, int(comp3) % 4), input0, 3);

		// First component comes from input 0, others come from the respective positions of input 1
		if (rtm_impl::static_condition<rtm_impl::is_mix_xyzw(comp0) && comp1 == mix4::b && comp2 == mix4::c && comp3 == mix4::d>::test())
			return vsetq_lane_f32(vgetq_lane_f32(input0, int(comp0) % 4), input1, 0);

		// Second component comes from input 0, others come from the respective positions of input 1
		if (rtm_impl::static_condition<comp0 == mix4::a && rtm_impl::is_mix_xyzw(comp1) && comp2 == mix4::c && comp3 == mix4::d>::test())
			return vsetq_lane_f32(vgetq_lane_f32(input0, int(comp1) % 4), input1, 1);

		// Third component comes from input 0, others come from the respective positions of input 1
		if (rtm_impl::static_condition<comp0 == mix4::a && comp1 == mix4::b && rtm_impl::is_mix_xyzw(comp2) && comp3 == mix4::d>::test())
			return vsetq_lane_f32(vgetq_lane_f32(input0, int(comp2) % 4), input1, 2);

		// Fourth component comes from input 0, others come from the respective positions of input 1
		if (rtm_impl::static_condition<comp0 == mix4::a && comp1 == mix4::b && comp2 == mix4::c && rtm_impl::is_mix_xyzw(comp3)>::test())
			return vsetq_lane_f32(vgetq_lane_f32(input0, int(comp3) % 4), input1, 3);

		// All comes from the same position
		if (rtm_impl::static_condition<comp0 == comp1 && comp0 == comp2 && comp0 == comp3>::test())
		{
			// All comes from the same position of input0
			if (rtm_impl::static_condition<rtm_impl::is_mix_xyzw(comp0)>::test())
				return vmovq_n_f32(vgetq_lane_f32(input0, int(comp0) % 4));

			// All comes from the same position of input1
			if (rtm_impl::static_condition<rtm_impl::is_mix_abcd(comp0)>::test())
				return vmovq_n_f32(vgetq_lane_f32(input1, int(comp0) % 4));
		}

		// Fallback to handle remaining cases
		const float x = vgetq_lane_f32(rtm_impl::is_mix_xyzw(comp0) ? input0 : input1, int(comp0) % 4);
		const float y = vgetq_lane_f32(rtm_impl::is_mix_xyzw(comp1) ? input0 : input1, int(comp1) % 4);
		const float z = vgetq_lane_f32(rtm_impl::is_mix_xyzw(comp2) ? input0 : input1, int(comp2) % 4);
		const float w = vgetq_lane_f32(rtm_impl::is_mix_xyzw(comp3) ? input0 : input1, int(comp3) % 4);
		const float32x2_t xy = vset_lane_f32(y, vmov_n_f32(x), 1);
		const float32x2_t zw = vset_lane_f32(w, vmov_n_f32(z), 1);
		return vcombine_f32(xy, zw);
#else
		// Non-simd variant
		constexpr component4 component0 = rtm_impl::mix_to_component(comp0);
		constexpr component4 component1 = rtm_impl::mix_to_component(comp1);
		constexpr component4 component2 = rtm_impl::mix_to_component(comp2);
		constexpr component4 component3 = rtm_impl::mix_to_component(comp3);

		const float x0 = vector_get_component(input0, component0);
		const float x1 = vector_get_component(input1, component0);
		const float x = rtm_impl::is_mix_xyzw(comp0) ? x0 : x1;

		const float y0 = vector_get_component(input0, component1);
		const float y1 = vector_get_component(input1, component1);
		const float y = rtm_impl::is_mix_xyzw(comp1) ? y0 : y1;

		const float z0 = vector_get_component(input0, component2);
		const float z1 = vector_get_component(input1, component2);
		const float z = rtm_impl::is_mix_xyzw(comp2) ? z0 : z1;

		const float w0 = vector_get_component(input0, component3);
		const float w1 = vector_get_component(input1, component3);
		const float w = rtm_impl::is_mix_xyzw(comp3) ? w0 : w1;

		return vector_set(x, y, z, w);
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Replicates the [x] component in all components.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4f RTM_SIMD_CALL vector_dup_x(vector4f_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return _mm_shuffle_ps(input, input, _MM_SHUFFLE(0, 0, 0, 0));
#elif defined(RTM_NEON_INTRINSICS)
		return vmovq_n_f32(vgetq_lane_f32(input, 0));
#else
		return vector4f{ input.x, input.x, input.x, input.x };
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Replicates the [y] component in all components.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4f RTM_SIMD_CALL vector_dup_y(vector4f_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return _mm_shuffle_ps(input, input, _MM_SHUFFLE(1, 1, 1, 1));
#elif defined(RTM_NEON_INTRINSICS)
		return vmovq_n_f32(vgetq_lane_f32(input, 1));
#else
		return vector4f{ input.y, input.y, input.y, input.y };
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Replicates the [z] component in all components.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4f RTM_SIMD_CALL vector_dup_z(vector4f_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return _mm_shuffle_ps(input, input, _MM_SHUFFLE(2, 2, 2, 2));
#elif defined(RTM_NEON_INTRINSICS)
		return vmovq_n_f32(vgetq_lane_f32(input, 2));
#else
		return vector4f{ input.z, input.z, input.z, input.z };
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Replicates the [w] component in all components.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4f RTM_SIMD_CALL vector_dup_w(vector4f_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return _mm_shuffle_ps(input, input, _MM_SHUFFLE(3, 3, 3, 3));
#elif defined(RTM_NEON_INTRINSICS)
		return vmovq_n_f32(vgetq_lane_f32(input, 3));
#else
		return vector4f{ input.w, input.w, input.w, input.w };
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Logical
	//////////////////////////////////////////////////////////////////////////

	//////////////////////////////////////////////////////////////////////////
	// Per component logical AND between the inputs: input0 & input1
	//////////////////////////////////////////////////////////////////////////
	inline vector4f RTM_SIMD_CALL vector_and(vector4f_arg0 input0, vector4f_arg1 input1) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return _mm_and_ps(input0, input1);
#elif defined(RTM_NEON_INTRINSICS)
		return vreinterpretq_f32_u32(vandq_u32(vreinterpretq_u32_f32(input0), vreinterpretq_u32_f32(input1)));
#else

#if defined(RTM_COMPILER_GCC)
	// GCC complains 'result' is used uninitialized but that is not true, ignore it
	#pragma GCC diagnostic push
	#pragma GCC diagnostic ignored "-Wuninitialized"
#endif

		const uint32_t* input0_ = rtm_impl::bit_cast<const uint32_t*>(&input0);
		const uint32_t* input1_ = rtm_impl::bit_cast<const uint32_t*>(&input1);

		vector4f result;
		uint32_t* result_ = rtm_impl::bit_cast<uint32_t*>(&result);

		result_[0] = input0_[0] & input1_[0];
		result_[1] = input0_[1] & input1_[1];
		result_[2] = input0_[2] & input1_[2];
		result_[3] = input0_[3] & input1_[3];

		return result;

#if defined(RTM_COMPILER_GCC)
	#pragma GCC diagnostic pop
#endif
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Per component logical OR between the inputs: input0 | input1
	//////////////////////////////////////////////////////////////////////////
	inline vector4f RTM_SIMD_CALL vector_or(vector4f_arg0 input0, vector4f_arg1 input1) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return _mm_or_ps(input0, input1);
#elif defined(RTM_NEON_INTRINSICS)
		return vreinterpretq_f32_u32(vorrq_u32(vreinterpretq_u32_f32(input0), vreinterpretq_u32_f32(input1)));
#else

#if defined(RTM_COMPILER_GCC)
	// GCC complains 'result' is used uninitialized but that is not true, ignore it
	#pragma GCC diagnostic push
	#pragma GCC diagnostic ignored "-Wuninitialized"
#endif

		const uint32_t* input0_ = rtm_impl::bit_cast<const uint32_t*>(&input0);
		const uint32_t* input1_ = rtm_impl::bit_cast<const uint32_t*>(&input1);

		vector4f result;
		uint32_t* result_ = rtm_impl::bit_cast<uint32_t*>(&result);

		result_[0] = input0_[0] | input1_[0];
		result_[1] = input0_[1] | input1_[1];
		result_[2] = input0_[2] | input1_[2];
		result_[3] = input0_[3] | input1_[3];

		return result;

#if defined(RTM_COMPILER_GCC)
	#pragma GCC diagnostic pop
#endif
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Per component logical XOR between the inputs: input0 ^ input1
	//////////////////////////////////////////////////////////////////////////
	inline vector4f RTM_SIMD_CALL vector_xor(vector4f_arg0 input0, vector4f_arg1 input1) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		return _mm_xor_ps(input0, input1);
#elif defined(RTM_NEON_INTRINSICS)
		return vreinterpretq_f32_u32(veorq_u32(vreinterpretq_u32_f32(input0), vreinterpretq_u32_f32(input1)));
#else

#if defined(RTM_COMPILER_GCC)
	// GCC complains 'result' is used uninitialized but that is not true, ignore it
	#pragma GCC diagnostic push
	#pragma GCC diagnostic ignored "-Wuninitialized"
#endif

		const uint32_t* input0_ = rtm_impl::bit_cast<const uint32_t*>(&input0);
		const uint32_t* input1_ = rtm_impl::bit_cast<const uint32_t*>(&input1);

		vector4f result;
		uint32_t* result_ = rtm_impl::bit_cast<uint32_t*>(&result);

		result_[0] = input0_[0] ^ input1_[0];
		result_[1] = input0_[1] ^ input1_[1];
		result_[2] = input0_[2] ^ input1_[2];
		result_[3] = input0_[3] ^ input1_[3];

		return result;

#if defined(RTM_COMPILER_GCC)
	#pragma GCC diagnostic pop
#endif
#endif
	}


	//////////////////////////////////////////////////////////////////////////
	// Miscellaneous
	//////////////////////////////////////////////////////////////////////////


	//////////////////////////////////////////////////////////////////////////
	// Returns per component the sign of the input vector: input >= 0.0 ? 1.0 : -1.0
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4f RTM_SIMD_CALL vector_sign(vector4f_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		constexpr __m128 signs = RTM_VECTOR4F_MAKE(-0.0F, -0.0F, -0.0F, -0.0F);
		constexpr __m128 one = RTM_VECTOR4F_MAKE(1.0F, 1.0F, 1.0F, 1.0F);
		const __m128 sign_bits = _mm_and_ps(input, signs);	// Mask out the sign bit
		return _mm_or_ps(sign_bits, one);					// Copy the sign bit onto +-1.0f
#else
		const mask4f mask = vector_greater_equal(input, vector_zero());
		return vector_select(mask, vector_set(1.0F), vector_set(-1.0F));
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns per component the input with the sign of the control value.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4f RTM_SIMD_CALL vector_copy_sign(vector4f_arg0 input, vector4f_arg1 control_sign) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		const __m128 sign_bit = _mm_set_ps1(-0.0F);
		__m128 signs = _mm_and_ps(sign_bit, control_sign);
		__m128 abs_input = _mm_andnot_ps(sign_bit, input);
		return _mm_or_ps(abs_input, signs);
#else
		float x = vector_get_x(input);
		float y = vector_get_y(input);
		float z = vector_get_z(input);
		float w = vector_get_w(input);

		float x_sign = vector_get_x(control_sign);
		float y_sign = vector_get_y(control_sign);
		float z_sign = vector_get_z(control_sign);
		float w_sign = vector_get_w(control_sign);

		return vector_set(rtm_impl::copysign(x, x_sign), rtm_impl::copysign(y, y_sign), rtm_impl::copysign(z, z_sign), rtm_impl::copysign(w, w_sign));
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns per component the rounded input using a symmetric algorithm.
	// vector_round_symmetric(1.5) = 2.0
	// vector_round_symmetric(1.2) = 1.0
	// vector_round_symmetric(-1.5) = -2.0
	// vector_round_symmetric(-1.2) = -1.0
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4f RTM_SIMD_CALL vector_round_symmetric(vector4f_arg0 input) RTM_NO_EXCEPT
	{
		// NaN, +- Infinity, and numbers larger or equal to 2^23 remain unchanged
		// since they have no fractional part.

#if defined(RTM_SSE4_INTRINSICS)
		__m128 is_positive = _mm_cmpge_ps(input, _mm_setzero_ps());

		const __m128 sign_mask = _mm_set_ps(-0.0F, -0.0F, -0.0F, -0.0F);
		__m128 sign = _mm_andnot_ps(is_positive, sign_mask);

		// For positive values, we add a bias of 0.5.
		// For negative values, we add a bias of -0.5.
		__m128 bias = _mm_or_ps(sign, _mm_set_ps1(0.5F));
		__m128 biased_input = _mm_add_ps(input, bias);

		__m128 floored = _mm_floor_ps(biased_input);
		__m128 ceiled = _mm_ceil_ps(biased_input);

		return vector_select(is_positive, floored, ceiled);
#elif defined(RTM_SSE2_INTRINSICS)
		const __m128i abs_mask = _mm_set_epi32(0x7FFFFFFFULL, 0x7FFFFFFFULL, 0x7FFFFFFFULL, 0x7FFFFFFFULL);
		const __m128 fractional_limit = _mm_set_ps1(8388608.0F); // 2^23

		// Build our mask, larger values that have no fractional part, and infinities will be true
		// Smaller values and NaN will be false
		__m128 abs_input = _mm_and_ps(input, _mm_castsi128_ps(abs_mask));
		__m128 is_input_large = _mm_cmpge_ps(abs_input, fractional_limit);

		// Test if our input is NaN with (value != value), it is only true for NaN
		__m128 is_nan = _mm_cmpneq_ps(input, input);

		// Combine our masks to determine if we should return the original value
		__m128 use_original_input = _mm_or_ps(is_input_large, is_nan);

		const __m128 sign_mask = _mm_set_ps(-0.0F, -0.0F, -0.0F, -0.0F);
		__m128 sign = _mm_and_ps(input, sign_mask);

		// For positive values, we add a bias of 0.5.
		// For negative values, we add a bias of -0.5.
		__m128 bias = _mm_or_ps(sign, _mm_set_ps1(0.5F));
		__m128 biased_input = _mm_add_ps(input, bias);

		// Convert to an integer with truncation and back, this rounds towards zero.
		__m128 integer_part = _mm_cvtepi32_ps(_mm_cvttps_epi32(biased_input));

		return _mm_or_ps(_mm_and_ps(use_original_input, input), _mm_andnot_ps(use_original_input, integer_part));
#elif defined(RTM_NEON_INTRINSICS) && !defined(RTM_NEON64_INTRINSICS) // arm64 is faster with scalar code
		// NaN, +- Infinity, and numbers larger or equal to 2^23 remain unchanged
		// since they have no fractional part.

		float32x4_t fractional_limit = vdupq_n_f32(8388608.0F); // 2^23

		// Build our mask, larger values that have no fractional part, and infinities will be true
		// Smaller values and NaN will be false
		uint32x4_t is_input_large = vcageq_f32(input, fractional_limit);

		// Test if our input is NaN with (value != value), it is only true for NaN
		uint32x4_t is_nan = vmvnq_u32(vceqq_f32(input, input));

		// Combine our masks to determine if we should return the original value
		uint32x4_t use_original_input = vorrq_u32(is_input_large, is_nan);

        uint32x4_t sign_mask = vreinterpretq_u32_f32(vdupq_n_f32(-0.0F));
		uint32x4_t sign = vandq_u32(vreinterpretq_u32_f32(input), sign_mask);

		// For positive values, we add a bias of 0.5.
		// For negative values, we add a bias of -0.5.
		float32x4_t bias = vreinterpretq_f32_u32(vorrq_u32(sign, vreinterpretq_u32_f32(vdupq_n_f32(0.5F))));
		float32x4_t biased_input = vaddq_f32(input, bias);

		// Convert to an integer and back. This does banker's rounding by default
		float32x4_t integer_part = vcvtq_f32_s32(vcvtq_s32_f32(biased_input));

		return vbslq_f32(use_original_input, input, integer_part);
#else
		const vector4f half = vector_set(0.5F);
		const vector4f floored = vector_floor(vector_add(input, half));
		const vector4f ceiled = vector_ceil(vector_sub(input, half));
		const mask4f is_greater_equal = vector_greater_equal(input, vector_zero());
		return vector_select(is_greater_equal, floored, ceiled);
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns per component the rounded input using banker's rounding (half to even).
	// vector_round_bankers(2.5) = 2.0
	// vector_round_bankers(1.5) = 2.0
	// vector_round_bankers(1.2) = 1.0
	// vector_round_bankers(-2.5) = -2.0
	// vector_round_bankers(-1.5) = -2.0
	// vector_round_bankers(-1.2) = -1.0
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4f RTM_SIMD_CALL vector_round_bankers(vector4f_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE4_INTRINSICS)
		return _mm_round_ps(input, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);
#elif defined(RTM_SSE2_INTRINSICS)
		const __m128 sign_mask = _mm_set_ps(-0.0F, -0.0F, -0.0F, -0.0F);
		__m128 sign = _mm_and_ps(input, sign_mask);

		// We add the largest integer that a 32 bit floating point number can represent and subtract it afterwards.
		// This relies on the fact that if we had a fractional part, the new value cannot be represented accurately
		// and IEEE 754 will perform rounding for us. The default rounding mode is Banker's rounding.
		// This has the effect of removing the fractional part while simultaneously rounding.
		// Use the same sign as the input value to make sure we handle positive and negative values.
		const __m128 fractional_limit = _mm_set_ps1(8388608.0F); // 2^23
		__m128 truncating_offset = _mm_or_ps(sign, fractional_limit);
		__m128 integer_part = _mm_sub_ps(_mm_add_ps(input, truncating_offset), truncating_offset);

		// If our input was so large that it had no fractional part, return it unchanged
		// Otherwise return our integer part
		const __m128i abs_mask = _mm_set_epi32(0x7FFFFFFFULL, 0x7FFFFFFFULL, 0x7FFFFFFFULL, 0x7FFFFFFFULL);
		__m128 abs_input = _mm_and_ps(input, _mm_castsi128_ps(abs_mask));
		__m128 is_input_large = _mm_cmpge_ps(abs_input, fractional_limit);
		return _mm_or_ps(_mm_and_ps(is_input_large, input), _mm_andnot_ps(is_input_large, integer_part));
#elif defined(RTM_NEON64_INTRINSICS)
		return vrndnq_f32(input);
#elif defined(RTM_NEON_INTRINSICS)
        uint32x4_t sign_mask = vreinterpretq_u32_f32(vdupq_n_f32(-0.0F));
		uint32x4_t sign = vandq_u32(vreinterpretq_u32_f32(input), sign_mask);

		// We add the largest integer that a 32 bit floating point number can represent and subtract it afterwards.
		// This relies on the fact that if we had a fractional part, the new value cannot be represented accurately
		// and IEEE 754 will perform rounding for us. The default rounding mode is Banker's rounding.
		// This has the effect of removing the fractional part while simultaneously rounding.
		// Use the same sign as the input value to make sure we handle positive and negative values.
		float32x4_t fractional_limit = vdupq_n_f32(8388608.0F); // 2^23
		float32x4_t truncating_offset = vreinterpretq_f32_u32(vorrq_u32(sign, vreinterpretq_u32_f32(fractional_limit)));
		float32x4_t integer_part = vsubq_f32(vaddq_f32(input, truncating_offset), truncating_offset);

		// If our input was so large that it had no fractional part, return it unchanged
		// Otherwise return our integer part
		uint32x4_t is_input_large = vcageq_f32(input, fractional_limit);
		return vbslq_f32(is_input_large, input, integer_part);
#else
		scalarf x = scalar_round_bankers(scalarf(vector_get_x(input)));
		scalarf y = scalar_round_bankers(scalarf(vector_get_y(input)));
		scalarf z = scalar_round_bankers(scalarf(vector_get_z(input)));
		scalarf w = scalar_round_bankers(scalarf(vector_get_w(input)));
		return vector_set(x, y, z, w);
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns per component the sine of the input angle.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline vector4f RTM_SIMD_CALL vector_sin(vector4f_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		// Use a degree 11 minimax approximation polynomial
		// See: GPGPU Programming for Games and Science (David H. Eberly)

		// Remap our input in the [-pi, pi] range
		__m128 quotient = _mm_mul_ps(input, _mm_set_ps1(constants::one_div_two_pi()));
		quotient = vector_round_bankers(quotient);
		quotient = _mm_mul_ps(quotient, _mm_set_ps1(constants::two_pi()));
		__m128 x = _mm_sub_ps(input, quotient);

		// Remap our input in the [-pi/2, pi/2] range
		const __m128 sign_mask = _mm_set_ps(-0.0F, -0.0F, -0.0F, -0.0F);
		__m128 sign = _mm_and_ps(x, sign_mask);
		__m128 reference = _mm_or_ps(sign, _mm_set_ps1(constants::pi()));

		const __m128 reflection = _mm_sub_ps(reference, x);
		const __m128i abs_mask = _mm_set_epi32(0x7FFFFFFFULL, 0x7FFFFFFFULL, 0x7FFFFFFFULL, 0x7FFFFFFFULL);
		const __m128 x_abs = _mm_and_ps(x, _mm_castsi128_ps(abs_mask));

		__m128 is_less_equal_than_half_pi = _mm_cmple_ps(x_abs, _mm_set_ps1(constants::half_pi()));

		x = RTM_VECTOR4F_SELECT(is_less_equal_than_half_pi, x, reflection);

		// Calculate our value
		const __m128 x2 = _mm_mul_ps(x, x);
		__m128 result = _mm_add_ps(_mm_mul_ps(x2, _mm_set_ps1(-2.3828544692960918e-8F)), _mm_set_ps1(2.7521557770526783e-6F));
		result = _mm_add_ps(_mm_mul_ps(result, x2), _mm_set_ps1(-1.9840782426250314e-4F));
		result = _mm_add_ps(_mm_mul_ps(result, x2), _mm_set_ps1(8.3333303183525942e-3F));
		result = _mm_add_ps(_mm_mul_ps(result, x2), _mm_set_ps1(-1.6666666601721269e-1F));
		result = _mm_add_ps(_mm_mul_ps(result, x2), _mm_set_ps1(1.0F));
		result = _mm_mul_ps(result, x);
		return result;
#elif defined(RTM_NEON_INTRINSICS)
		// Use a degree 11 minimax approximation polynomial
		// See: GPGPU Programming for Games and Science (David H. Eberly)

		// Remap our input in the [-pi, pi] range
		float32x4_t quotient = vmulq_n_f32(input, constants::one_div_two_pi());
		quotient = vector_round_bankers(quotient);
		quotient = vmulq_n_f32(quotient, constants::two_pi());
		float32x4_t x = vsubq_f32(input, quotient);

		// Remap our input in the [-pi/2, pi/2] range
		uint32x4_t sign_mask = vreinterpretq_u32_f32(vdupq_n_f32(-0.0F));
		uint32x4_t sign = vandq_u32(vreinterpretq_u32_f32(x), sign_mask);
		float32x4_t reference = vreinterpretq_f32_u32(vorrq_u32(sign, vreinterpretq_u32_f32(vdupq_n_f32(constants::pi()))));

		float32x4_t reflection = vsubq_f32(reference, x);
#if !defined(RTM_IMPL_VCA_SUPPORTED)
		uint32x4_t is_less_equal_than_half_pi = vcleq_f32(vabsq_f32(x), vdupq_n_f32(constants::half_pi()));
#else
		uint32x4_t is_less_equal_than_half_pi = vcaleq_f32(x, vdupq_n_f32(constants::half_pi()));
#endif
		x = vbslq_f32(is_less_equal_than_half_pi, x, reflection);

		// Calculate our value
		float32x4_t x2 = vmulq_f32(x, x);

		float32x4_t result = RTM_VECTOR4F_MULS_ADD(x2, -2.3828544692960918e-8F, vdupq_n_f32(2.7521557770526783e-6F));
		result = RTM_VECTOR4F_MULV_ADD(result, x2, vdupq_n_f32(-1.9840782426250314e-4F));
		result = RTM_VECTOR4F_MULV_ADD(result, x2, vdupq_n_f32(8.3333303183525942e-3F));
		result = RTM_VECTOR4F_MULV_ADD(result, x2, vdupq_n_f32(-1.6666666601721269e-1F));
		result = RTM_VECTOR4F_MULV_ADD(result, x2, vdupq_n_f32(1.0F));

		result = vmulq_f32(result, x);
		return result;
#else
		scalarf x = scalar_sin(scalarf(vector_get_x(input)));
		scalarf y = scalar_sin(scalarf(vector_get_y(input)));
		scalarf z = scalar_sin(scalarf(vector_get_z(input)));
		scalarf w = scalar_sin(scalarf(vector_get_w(input)));
		return vector_set(x, y, z, w);
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns per component the arc-sine of the input.
	// Input value must be in the range [-1.0, 1.0].
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline vector4f RTM_SIMD_CALL vector_asin(vector4f_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		// Use a degree 7 minimax approximation polynomial
		// See: GPGPU Programming for Games and Science (David H. Eberly)

		// We first calculate our scale: sqrt(1.0 - abs(value))
		// Use the sign bit to generate our absolute value since we'll re-use that constant
		const __m128 sign_bit = _mm_set_ps1(-0.0F);
		__m128 abs_value = _mm_andnot_ps(sign_bit, input);

		// Calculate our value
		__m128 result = _mm_add_ps(_mm_mul_ps(abs_value, _mm_set_ps1(-1.2690614339589956e-3F)), _mm_set_ps1(6.7072304676685235e-3F));
		result = _mm_add_ps(_mm_mul_ps(result, abs_value), _mm_set_ps1(-1.7162031184398074e-2F));
		result = _mm_add_ps(_mm_mul_ps(result, abs_value), _mm_set_ps1(3.0961594977611639e-2F));
		result = _mm_add_ps(_mm_mul_ps(result, abs_value), _mm_set_ps1(-5.0207843052845647e-2F));
		result = _mm_add_ps(_mm_mul_ps(result, abs_value), _mm_set_ps1(8.8986946573346160e-2F));
		result = _mm_add_ps(_mm_mul_ps(result, abs_value), _mm_set_ps1(-2.1459960076929829e-1F));
		result = _mm_add_ps(_mm_mul_ps(result, abs_value), _mm_set_ps1(1.5707963267948966F));

		// Scale our result
		__m128 scale = _mm_sqrt_ps(_mm_sub_ps(_mm_set_ps1(1.0F), abs_value));
		result = _mm_mul_ps(result, scale);

		// Normally the math is as follow:
		// If input is positive: PI/2 - result
		// If input is negative: PI/2 - (PI - result) = PI/2 - PI + result = -PI/2 + result

		// As such, the offset is PI/2 and it takes the sign of the input
		// This allows us to load a single constant from memory directly
		__m128 input_sign = _mm_and_ps(input, sign_bit);
		__m128 offset = _mm_or_ps(input_sign, _mm_set_ps1(constants::half_pi()));

		// And our result has the opposite sign of the input
		result = _mm_xor_ps(result, _mm_xor_ps(input_sign, sign_bit));
		return _mm_add_ps(result, offset);
#else
		scalarf x = scalar_asin(scalarf(vector_get_x(input)));
		scalarf y = scalar_asin(scalarf(vector_get_y(input)));
		scalarf z = scalar_asin(scalarf(vector_get_z(input)));
		scalarf w = scalar_asin(scalarf(vector_get_w(input)));
		return vector_set(x, y, z, w);
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns per component the cosine of the input angle.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline vector4f RTM_SIMD_CALL vector_cos(vector4f_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		// Use a degree 10 minimax approximation polynomial
		// See: GPGPU Programming for Games and Science (David H. Eberly)

		// Remap our input in the [-pi, pi] range
		__m128 quotient = _mm_mul_ps(input, _mm_set_ps1(constants::one_div_two_pi()));
		quotient = vector_round_bankers(quotient);
		quotient = _mm_mul_ps(quotient, _mm_set_ps1(constants::two_pi()));
		__m128 x = _mm_sub_ps(input, quotient);

		// Remap our input in the [-pi/2, pi/2] range
		const __m128 sign_mask = _mm_set_ps(-0.0F, -0.0F, -0.0F, -0.0F);
		__m128 x_sign = _mm_and_ps(x, sign_mask);
		__m128 reference = _mm_or_ps(x_sign, _mm_set_ps1(constants::pi()));
		const __m128 reflection = _mm_sub_ps(reference, x);

		const __m128i abs_mask = _mm_set_epi32(0x7FFFFFFFULL, 0x7FFFFFFFULL, 0x7FFFFFFFULL, 0x7FFFFFFFULL);
		__m128 x_abs = _mm_and_ps(x, _mm_castsi128_ps(abs_mask));
		__m128 is_less_equal_than_half_pi = _mm_cmple_ps(x_abs, _mm_set_ps1(constants::half_pi()));

		x = RTM_VECTOR4F_SELECT(is_less_equal_than_half_pi, x, reflection);

		// Calculate our value
		const __m128 x2 = _mm_mul_ps(x, x);
		__m128 result = _mm_add_ps(_mm_mul_ps(x2, _mm_set_ps1(-2.6051615464872668e-7F)), _mm_set_ps1(2.4760495088926859e-5F));
		result = _mm_add_ps(_mm_mul_ps(result, x2), _mm_set_ps1(-1.3888377661039897e-3F));
		result = _mm_add_ps(_mm_mul_ps(result, x2), _mm_set_ps1(4.1666638865338612e-2F));
		result = _mm_add_ps(_mm_mul_ps(result, x2), _mm_set_ps1(-4.9999999508695869e-1F));
		result = _mm_add_ps(_mm_mul_ps(result, x2), _mm_set_ps1(1.0F));

		// Remap into [-pi, pi]
		return _mm_or_ps(result, _mm_andnot_ps(is_less_equal_than_half_pi, sign_mask));
#elif defined(RTM_NEON_INTRINSICS)
		// Use a degree 10 minimax approximation polynomial
		// See: GPGPU Programming for Games and Science (David H. Eberly)

		// Remap our input in the [-pi, pi] range
		float32x4_t quotient = vmulq_n_f32(input, constants::one_div_two_pi());
		quotient = vector_round_bankers(quotient);
		quotient = vmulq_n_f32(quotient, constants::two_pi());
		float32x4_t x = vsubq_f32(input, quotient);

		// Remap our input in the [-pi/2, pi/2] range
		uint32x4_t sign_mask = vreinterpretq_u32_f32(vdupq_n_f32(-0.0F));
		uint32x4_t sign = vandq_u32(vreinterpretq_u32_f32(x), sign_mask);
		float32x4_t reference = vreinterpretq_f32_u32(vorrq_u32(sign, vreinterpretq_u32_f32(vdupq_n_f32(constants::pi()))));

		float32x4_t reflection = vsubq_f32(reference, x);
#if !defined(RTM_IMPL_VCA_SUPPORTED)
		uint32x4_t is_less_equal_than_half_pi = vcleq_f32(vabsq_f32(x), vdupq_n_f32(constants::half_pi()));
#else
		uint32x4_t is_less_equal_than_half_pi = vcaleq_f32(x, vdupq_n_f32(constants::half_pi()));
#endif
		x = vbslq_f32(is_less_equal_than_half_pi, x, reflection);

		// Calculate our value
		float32x4_t x2 = vmulq_f32(x, x);

		float32x4_t result = RTM_VECTOR4F_MULS_ADD(x2, -2.6051615464872668e-7F, vdupq_n_f32(2.4760495088926859e-5F));
		result = RTM_VECTOR4F_MULV_ADD(result, x2, vdupq_n_f32(-1.3888377661039897e-3F));
		result = RTM_VECTOR4F_MULV_ADD(result, x2, vdupq_n_f32(4.1666638865338612e-2F));
		result = RTM_VECTOR4F_MULV_ADD(result, x2, vdupq_n_f32(-4.9999999508695869e-1F));
		result = RTM_VECTOR4F_MULV_ADD(result, x2, vdupq_n_f32(1.0F));

		// Remap into [-pi, pi]
		return vbslq_f32(is_less_equal_than_half_pi, result, vnegq_f32(result));
#else
		scalarf x = scalar_cos(scalarf(vector_get_x(input)));
		scalarf y = scalar_cos(scalarf(vector_get_y(input)));
		scalarf z = scalar_cos(scalarf(vector_get_z(input)));
		scalarf w = scalar_cos(scalarf(vector_get_w(input)));
		return vector_set(x, y, z, w);
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns per component the arc-cosine of the input.
	// Input value must be in the range [-1.0, 1.0].
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline vector4f RTM_SIMD_CALL vector_acos(vector4f_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		// Use the identity: acos(value) + asin(value) = PI/2
		// This ends up being: acos(value) = PI/2 - asin(value)
		// Since asin(value) = PI/2 - sqrt(1.0 - polynomial(value))
		// Our end result is acos(value) = sqrt(1.0 - polynomial(value))
		// This means we can re-use the same polynomial as asin()
		// See: GPGPU Programming for Games and Science (David H. Eberly)

		// We first calculate our scale: sqrt(1.0 - abs(value))
		// Use the sign bit to generate our absolute value since we'll re-use that constant
		const __m128 sign_bit = _mm_set_ps1(-0.0F);
		__m128 abs_value = _mm_andnot_ps(sign_bit, input);

		// Calculate our value
		__m128 result = _mm_add_ps(_mm_mul_ps(abs_value, _mm_set_ps1(-1.2690614339589956e-3F)), _mm_set_ps1(6.7072304676685235e-3F));
		result = _mm_add_ps(_mm_mul_ps(result, abs_value), _mm_set_ps1(-1.7162031184398074e-2F));
		result = _mm_add_ps(_mm_mul_ps(result, abs_value), _mm_set_ps1(3.0961594977611639e-2F));
		result = _mm_add_ps(_mm_mul_ps(result, abs_value), _mm_set_ps1(-5.0207843052845647e-2F));
		result = _mm_add_ps(_mm_mul_ps(result, abs_value), _mm_set_ps1(8.8986946573346160e-2F));
		result = _mm_add_ps(_mm_mul_ps(result, abs_value), _mm_set_ps1(-2.1459960076929829e-1F));
		result = _mm_add_ps(_mm_mul_ps(result, abs_value), _mm_set_ps1(1.5707963267948966F));

		// Scale our result
		__m128 scale = _mm_sqrt_ps(_mm_sub_ps(_mm_set_ps1(1.0F), abs_value));
		result = _mm_mul_ps(result, scale);

		// Normally the math is as follow:
		// If input is positive: result
		// If input is negative: PI - result = -result + PI

		// As such, the offset is 0.0 when the input is positive and PI when negative
		__m128 is_input_negative = _mm_cmplt_ps(input, _mm_setzero_ps());
		__m128 offset = _mm_and_ps(is_input_negative, _mm_set_ps1(constants::pi()));

		// And our result has the same sign of the input
		__m128 input_sign = _mm_and_ps(input, sign_bit);
		result = _mm_or_ps(result, input_sign);
		return _mm_add_ps(result, offset);
#else
		scalarf x = scalar_acos(scalarf(vector_get_x(input)));
		scalarf y = scalar_acos(scalarf(vector_get_y(input)));
		scalarf z = scalar_acos(scalarf(vector_get_z(input)));
		scalarf w = scalar_acos(scalarf(vector_get_w(input)));
		return vector_set(x, y, z, w);
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns per component the sine and cosine of the input angle.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline void RTM_SIMD_CALL vector_sincos(vector4f_arg0 input, vector4f& out_sine, vector4f& out_cosine) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		// Use a degree 10 minimax approximation polynomial
		// See: GPGPU Programming for Games and Science (David H. Eberly)

		// Remap our input in the [-pi, pi] range
		__m128 quotient = _mm_mul_ps(input, _mm_set_ps1(constants::one_div_two_pi()));
		quotient = vector_round_bankers(quotient);
		quotient = _mm_mul_ps(quotient, _mm_set_ps1(constants::two_pi()));
		__m128 x = _mm_sub_ps(input, quotient);

		// Remap our input in the [-pi/2, pi/2] range
		const __m128 sign_mask = _mm_set_ps(-0.0F, -0.0F, -0.0F, -0.0F);
		__m128 x_sign = _mm_and_ps(x, sign_mask);
		__m128 reference = _mm_or_ps(x_sign, _mm_set_ps1(constants::pi()));
		const __m128 reflection = _mm_sub_ps(reference, x);

		const __m128i abs_mask = _mm_set_epi32(0x7FFFFFFFULL, 0x7FFFFFFFULL, 0x7FFFFFFFULL, 0x7FFFFFFFULL);
		__m128 x_abs = _mm_and_ps(x, _mm_castsi128_ps(abs_mask));
		__m128 is_less_equal_than_half_pi = _mm_cmple_ps(x_abs, _mm_set_ps1(constants::half_pi()));

		x = RTM_VECTOR4F_SELECT(is_less_equal_than_half_pi, x, reflection);
		const __m128 x2 = _mm_mul_ps(x, x);

		// Calculate our cosine value
		__m128 cosine_result = _mm_add_ps(_mm_mul_ps(x2, _mm_set_ps1(-2.6051615464872668e-7F)), _mm_set_ps1(2.4760495088926859e-5F));
		cosine_result = _mm_add_ps(_mm_mul_ps(cosine_result, x2), _mm_set_ps1(-1.3888377661039897e-3F));
		cosine_result = _mm_add_ps(_mm_mul_ps(cosine_result, x2), _mm_set_ps1(4.1666638865338612e-2F));
		cosine_result = _mm_add_ps(_mm_mul_ps(cosine_result, x2), _mm_set_ps1(-4.9999999508695869e-1F));
		cosine_result = _mm_add_ps(_mm_mul_ps(cosine_result, x2), _mm_set_ps1(1.0F));

		// Remap into [-pi, pi]
		out_cosine = _mm_or_ps(cosine_result, _mm_andnot_ps(is_less_equal_than_half_pi, sign_mask));

		// Calculate our sine value
		__m128 sine_result = _mm_add_ps(_mm_mul_ps(x2, _mm_set_ps1(-2.3828544692960918e-8F)), _mm_set_ps1(2.7521557770526783e-6F));
		sine_result = _mm_add_ps(_mm_mul_ps(sine_result, x2), _mm_set_ps1(-1.9840782426250314e-4F));
		sine_result = _mm_add_ps(_mm_mul_ps(sine_result, x2), _mm_set_ps1(8.3333303183525942e-3F));
		sine_result = _mm_add_ps(_mm_mul_ps(sine_result, x2), _mm_set_ps1(-1.6666666601721269e-1F));
		sine_result = _mm_add_ps(_mm_mul_ps(sine_result, x2), _mm_set_ps1(1.0F));
		out_sine = _mm_mul_ps(sine_result, x);
#elif defined(RTM_NEON_INTRINSICS)
		// Use a degree 10 minimax approximation polynomial
		// See: GPGPU Programming for Games and Science (David H. Eberly)

		// Remap our input in the [-pi, pi] range
		float32x4_t quotient = vmulq_n_f32(input, constants::one_div_two_pi());
		quotient = vector_round_bankers(quotient);
		quotient = vmulq_n_f32(quotient, constants::two_pi());
		float32x4_t x = vsubq_f32(input, quotient);

		// Remap our input in the [-pi/2, pi/2] range
		const uint32x4_t sign_mask = vreinterpretq_u32_f32(vdupq_n_f32(-0.0F));
		const uint32x4_t sign = vandq_u32(vreinterpretq_u32_f32(x), sign_mask);
		const float32x4_t reference = vreinterpretq_f32_u32(vorrq_u32(sign, vreinterpretq_u32_f32(vdupq_n_f32(constants::pi()))));

		const float32x4_t reflection = vsubq_f32(reference, x);
#if !defined(RTM_IMPL_VCA_SUPPORTED)
		const uint32x4_t is_less_equal_than_half_pi = vcleq_f32(vabsq_f32(x), vdupq_n_f32(constants::half_pi()));
#else
		const uint32x4_t is_less_equal_than_half_pi = vcaleq_f32(x, vdupq_n_f32(constants::half_pi()));
#endif
		x = vbslq_f32(is_less_equal_than_half_pi, x, reflection);
		const float32x4_t x2 = vmulq_f32(x, x);

		// Calculate our cosine value
		float32x4_t cosine_result = RTM_VECTOR4F_MULS_ADD(x2, -2.6051615464872668e-7F, vdupq_n_f32(2.4760495088926859e-5F));
		cosine_result = RTM_VECTOR4F_MULV_ADD(cosine_result, x2, vdupq_n_f32(-1.3888377661039897e-3F));
		cosine_result = RTM_VECTOR4F_MULV_ADD(cosine_result, x2, vdupq_n_f32(4.1666638865338612e-2F));
		cosine_result = RTM_VECTOR4F_MULV_ADD(cosine_result, x2, vdupq_n_f32(-4.9999999508695869e-1F));
		cosine_result = RTM_VECTOR4F_MULV_ADD(cosine_result, x2, vdupq_n_f32(1.0F));

		// Remap into [-pi, pi]
		out_cosine = vbslq_f32(is_less_equal_than_half_pi, cosine_result, vnegq_f32(cosine_result));

		// Calculate our sine value
		float32x4_t sine_result = RTM_VECTOR4F_MULS_ADD(x2, -2.3828544692960918e-8F, vdupq_n_f32(2.7521557770526783e-6F));
		sine_result = RTM_VECTOR4F_MULV_ADD(sine_result, x2, vdupq_n_f32(-1.9840782426250314e-4F));
		sine_result = RTM_VECTOR4F_MULV_ADD(sine_result, x2, vdupq_n_f32(8.3333303183525942e-3F));
		sine_result = RTM_VECTOR4F_MULV_ADD(sine_result, x2, vdupq_n_f32(-1.6666666601721269e-1F));
		sine_result = RTM_VECTOR4F_MULV_ADD(sine_result, x2, vdupq_n_f32(1.0F));

		out_sine = vmulq_f32(sine_result, x);
#else
		const vector4f x = scalar_sincos(scalarf(vector_get_x(input)));
		const vector4f y = scalar_sincos(scalarf(vector_get_y(input)));
		const vector4f z = scalar_sincos(scalarf(vector_get_z(input)));
		const vector4f w = scalar_sincos(scalarf(vector_get_w(input)));

		out_cosine = vector4f{ x.y, y.y, z.y, w.y };
		out_sine = vector4f{ x.x, y.x, z.x, w.x };
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns per component the tangent of the input angle.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline vector4f RTM_SIMD_CALL vector_tan(vector4f_arg0 angle) RTM_NO_EXCEPT
	{
		// Use the identity: tan(angle) = sin(angle) / cos(angle)
		vector4f sin_;
		vector4f cos_;
		vector_sincos(angle, sin_, cos_);

		const mask4f is_cos_zero = vector_equal(cos_, vector_zero());
		const vector4f signed_infinity = vector_copy_sign(vector_set(std::numeric_limits<float>::infinity()), angle);
		const vector4f result = vector_div(sin_, cos_);
		return vector_select(is_cos_zero, signed_infinity, result);
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns per component the arc-tangent of the input.
	// Note that due to the sign ambiguity, atan cannot determine which quadrant
	// the value resides in.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline vector4f RTM_SIMD_CALL vector_atan(vector4f_arg0 input) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		// Use a degree 13 minimax approximation polynomial
		// See: GPGPU Programming for Games and Science (David H. Eberly)

		// Discard our sign, we'll restore it later
		const __m128i abs_mask = _mm_set_epi32(0x7FFFFFFFULL, 0x7FFFFFFFULL, 0x7FFFFFFFULL, 0x7FFFFFFFULL);
		__m128 abs_value = _mm_and_ps(input, _mm_castsi128_ps(abs_mask));

		// Compute our value
		__m128 is_larger_than_one = _mm_cmpgt_ps(abs_value, _mm_set_ps1(1.0F));
		__m128 reciprocal = vector_reciprocal(abs_value);

		__m128 x = vector_select(is_larger_than_one, reciprocal, abs_value);

		__m128 x2 = _mm_mul_ps(x, x);

		__m128 result = _mm_add_ps(_mm_mul_ps(x2, _mm_set_ps1(7.2128853633444123e-3F)), _mm_set_ps1(-3.5059680836411644e-2F));
		result = _mm_add_ps(_mm_mul_ps(result, x2), _mm_set_ps1(8.1675882859940430e-2F));
		result = _mm_add_ps(_mm_mul_ps(result, x2), _mm_set_ps1(-1.3374657325451267e-1F));
		result = _mm_add_ps(_mm_mul_ps(result, x2), _mm_set_ps1(1.9856563505717162e-1F));
		result = _mm_add_ps(_mm_mul_ps(result, x2), _mm_set_ps1(-3.3324998579202170e-1F));
		result = _mm_add_ps(_mm_mul_ps(result, x2), _mm_set_ps1(1.0F));
		result = _mm_mul_ps(result, x);

		__m128 remapped = _mm_sub_ps(_mm_set_ps1(constants::half_pi()), result);

		// pi/2 - result
		result = vector_select(is_larger_than_one, remapped, result);

		// Keep the original sign
		return _mm_or_ps(result, _mm_and_ps(input, _mm_set_ps1(-0.0F)));
#elif defined(RTM_NEON_INTRINSICS)
		// Use a degree 13 minimax approximation polynomial
		// See: GPGPU Programming for Games and Science (David H. Eberly)

		// Discard our sign, we'll restore it later
		float32x4_t abs_value = vabsq_f32(input);

		// Compute our value
#if !defined(RTM_IMPL_VCA_SUPPORTED)
		uint32x4_t is_larger_than_one = vcgtq_f32(vabsq_f32(input), vdupq_n_f32(1.0F));
#else
		uint32x4_t is_larger_than_one = vcagtq_f32(input, vdupq_n_f32(1.0F));
#endif
		float32x4_t reciprocal = vector_reciprocal(abs_value);

		float32x4_t x = vbslq_f32(is_larger_than_one, reciprocal, abs_value);

		float32x4_t x2 = vmulq_f32(x, x);

		float32x4_t result = RTM_VECTOR4F_MULS_ADD(x2, 7.2128853633444123e-3F, vdupq_n_f32(-3.5059680836411644e-2F));
		result = RTM_VECTOR4F_MULV_ADD(result, x2, vdupq_n_f32(8.1675882859940430e-2F));
		result = RTM_VECTOR4F_MULV_ADD(result, x2, vdupq_n_f32(-1.3374657325451267e-1F));
		result = RTM_VECTOR4F_MULV_ADD(result, x2, vdupq_n_f32(1.9856563505717162e-1F));
		result = RTM_VECTOR4F_MULV_ADD(result, x2, vdupq_n_f32(-3.3324998579202170e-1F));
		result = RTM_VECTOR4F_MULV_ADD(result, x2, vdupq_n_f32(1.0F));

		result = vmulq_f32(result, x);

		float32x4_t remapped = vsubq_f32(vdupq_n_f32(constants::half_pi()), result);

		// pi/2 - result
		result = vbslq_f32(is_larger_than_one, remapped, result);

		// Keep the original sign
		return vreinterpretq_f32_u32(vorrq_u32(vreinterpretq_u32_f32(result), vandq_u32(vreinterpretq_u32_f32(input), vreinterpretq_u32_f32(vdupq_n_f32(-0.0F)))));
#else
		scalarf x = scalar_atan(scalarf(vector_get_x(input)));
		scalarf y = scalar_atan(scalarf(vector_get_y(input)));
		scalarf z = scalar_atan(scalarf(vector_get_z(input)));
		scalarf w = scalar_atan(scalarf(vector_get_w(input)));
		return vector_set(x, y, z, w);
#endif
	}

	//////////////////////////////////////////////////////////////////////////
	// Returns per component the arc-tangent of [y/x] using the sign of the arguments to
	// determine the correct quadrant.
	// Y represents the proportion of the y-coordinate.
	// X represents the proportion of the x-coordinate.
	//////////////////////////////////////////////////////////////////////////
	RTM_DISABLE_SECURITY_COOKIE_CHECK inline vector4f RTM_SIMD_CALL vector_atan2(vector4f_arg0 y, vector4f_arg1 x) RTM_NO_EXCEPT
	{
#if defined(RTM_SSE2_INTRINSICS)
		// If X == 0.0 and Y != 0.0, we return PI/2 with the sign of Y
		// If X == 0.0 and Y == 0.0, we return 0.0
		// If X > 0.0, we return atan(y/x)
		// If X < 0.0, we return atan(y/x) + sign(Y) * PI
		// See: https://en.wikipedia.org/wiki/Atan2#Definition_and_computation

		const __m128 zero = _mm_setzero_ps();
		__m128 is_x_zero = _mm_cmpeq_ps(x, zero);
		__m128 is_y_zero = _mm_cmpeq_ps(y, zero);
		__m128 inputs_are_zero = _mm_and_ps(is_x_zero, is_y_zero);

		__m128 is_x_positive = _mm_cmpgt_ps(x, zero);

		const __m128 sign_mask = _mm_set_ps(-0.0F, -0.0F, -0.0F, -0.0F);
		__m128 y_sign = _mm_and_ps(y, sign_mask);

		// If X == 0.0, our offset is PI/2 otherwise it is PI both with the sign of Y
		__m128 half_pi = _mm_set_ps1(constants::half_pi());
		__m128 pi = _mm_set_ps1(constants::pi());
		__m128 offset = _mm_or_ps(_mm_and_ps(is_x_zero, half_pi), _mm_andnot_ps(is_x_zero, pi));
		offset = _mm_or_ps(offset, y_sign);

		// If X > 0.0, our offset is 0.0
		offset = _mm_andnot_ps(is_x_positive, offset);

		// If X == 0.0 and Y == 0.0, our offset is 0.0
		offset = _mm_andnot_ps(inputs_are_zero, offset);

		__m128 angle = _mm_div_ps(y, x);
		__m128 value = vector_atan(angle);

		// If X == 0.0, our value is 0.0 otherwise it is atan(y/x)
		value = _mm_andnot_ps(is_x_zero, value);

		// If X == 0.0 and Y == 0.0, our value is 0.0
		value = _mm_andnot_ps(inputs_are_zero, value);

		return _mm_add_ps(value, offset);
#elif defined(RTM_NEON64_INTRINSICS)
		// If X == 0.0 and Y != 0.0, we return PI/2 with the sign of Y
		// If X == 0.0 and Y == 0.0, we return 0.0
		// If X > 0.0, we return atan(y/x)
		// If X < 0.0, we return atan(y/x) + sign(Y) * PI
		// See: https://en.wikipedia.org/wiki/Atan2#Definition_and_computation

#if !defined(RTM_IMPL_VCZ_SUPPORTED)
		float32x4_t zero = vdupq_n_f32(0.0F);
		uint32x4_t is_x_zero = vceqq_f32(x, zero);
		uint32x4_t is_y_zero = vceqq_f32(y, zero);
		uint32x4_t inputs_are_zero = vandq_u32(is_x_zero, is_y_zero);

		uint32x4_t is_x_positive = vcgtq_f32(x, zero);
#else
		uint32x4_t is_x_zero = vceqzq_f32(x);
		uint32x4_t is_y_zero = vceqzq_f32(y);
		uint32x4_t inputs_are_zero = vandq_u32(is_x_zero, is_y_zero);

		uint32x4_t is_x_positive = vcgtzq_f32(x);
#endif

		uint32x4_t y_sign = vandq_u32(vreinterpretq_u32_f32(y), vreinterpretq_u32_f32(vdupq_n_f32(-0.0F)));

		// If X == 0.0, our offset is PI/2 otherwise it is PI both with the sign of Y
		float32x4_t half_pi = vdupq_n_f32(constants::half_pi());
		float32x4_t pi = vdupq_n_f32(constants::pi());
		float32x4_t offset = vreinterpretq_f32_u32(vorrq_u32(vandq_u32(is_x_zero, vreinterpretq_u32_f32(half_pi)), vandq_u32(vmvnq_u32(is_x_zero), vreinterpretq_u32_f32(pi))));
		offset = vreinterpretq_f32_u32(vorrq_u32(vreinterpretq_u32_f32(offset), y_sign));

		// If X > 0.0, our offset is 0.0
		offset = vreinterpretq_f32_u32(vandq_u32(vmvnq_u32(is_x_positive), vreinterpretq_u32_f32(offset)));

		// If X == 0.0 and Y == 0.0, our offset is 0.0
		offset = vreinterpretq_f32_u32(vandq_u32(vmvnq_u32(inputs_are_zero), vreinterpretq_u32_f32(offset)));

		float32x4_t angle = vector_div(y, x);
		float32x4_t value = vector_atan(angle);

		// If X == 0.0, our value is 0.0 otherwise it is atan(y/x)
		value = vreinterpretq_f32_u32(vandq_u32(vmvnq_u32(is_x_zero), vreinterpretq_u32_f32(value)));

		// If X == 0.0 and Y == 0.0, our value is 0.0
		value = vreinterpretq_f32_u32(vandq_u32(vmvnq_u32(inputs_are_zero), vreinterpretq_u32_f32(value)));

		return vaddq_f32(value, offset);
#else
		scalarf x_ = scalar_atan2(scalarf(vector_get_x(y)), scalarf(vector_get_x(x)));
		scalarf y_ = scalar_atan2(scalarf(vector_get_y(y)), scalarf(vector_get_y(x)));
		scalarf z_ = scalar_atan2(scalarf(vector_get_z(y)), scalarf(vector_get_z(x)));
		scalarf w_ = scalar_atan2(scalarf(vector_get_w(y)), scalarf(vector_get_w(x)));
		return vector_set(x_, y_, z_, w_);
#endif
	}

	RTM_IMPL_VERSION_NAMESPACE_END
}

RTM_IMPL_FILE_PRAGMA_POP

```

`includes/rtm/version.h`:

```h
#pragma once

////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2022 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include "rtm/impl/detect_compiler.h"

////////////////////////////////////////////////////////////////////////////////
// Macros to detect the Realtime Math version
////////////////////////////////////////////////////////////////////////////////

#define RTM_VERSION_MAJOR 2
#define RTM_VERSION_MINOR 3
#define RTM_VERSION_PATCH 99

////////////////////////////////////////////////////////////////////////////////
// In order to allow multiple versions of this library to coexist side by side
// within the same executable/library, the symbols have to be unique per version.
// We achieve this by using a versioned namespace that we optionally inline.
// To disable namespace inlining, define RTM_NO_INLINE_NAMESPACE before including
// any Realtime Math header. To disable the versioned namespace altogether,
// define RTM_NO_VERSION_NAMESPACE before including any Realtime Math header.
////////////////////////////////////////////////////////////////////////////////

#if !defined(RTM_NO_VERSION_NAMESPACE)
	#if defined(RTM_COMPILER_MSVC) && RTM_COMPILER_MSVC == RTM_COMPILER_MSVC_2015
		// VS2015 struggles with type resolution when inline namespaces are used
		// For that reason, we disable it explicitly
		#define RTM_NO_VERSION_NAMESPACE
	#endif
#endif

// Force macro expansion to concatenate namespace identifier
#define RTM_IMPL_VERSION_CONCAT_IMPL(prefix, major, minor, patch) prefix ## major ## minor ## patch
#define RTM_IMPL_VERSION_CONCAT(prefix, major, minor, patch) RTM_IMPL_VERSION_CONCAT_IMPL(prefix, major, minor, patch)

// Name of the namespace, e.g. v214
#define RTM_IMPL_VERSION_NAMESPACE_NAME RTM_IMPL_VERSION_CONCAT(v, RTM_VERSION_MAJOR, RTM_VERSION_MINOR, RTM_VERSION_PATCH)

// Because this is being introduced in a patch release, as caution, it is disabled
// by default. It does break ABI if host runtimes forward declare types but that
// is something they shouldn't do with a 3rd party library. Now, we offer forward
// declaration headers to help prepare the migration in the next minor release.
#if defined(RTM_NO_VERSION_NAMESPACE) || !defined(RTM_ENABLE_VERSION_NAMESPACE)
	// Namespace is inlined, its usage does not need to be qualified with the
	// full version everywhere
	#define RTM_IMPL_NAMESPACE rtm

	#define RTM_IMPL_VERSION_NAMESPACE_BEGIN
	#define RTM_IMPL_VERSION_NAMESPACE_END
#elif defined(RTM_NO_INLINE_NAMESPACE)
	// Namespace won't be inlined, its usage will have to be qualified with the
	// full version everywhere
	#define RTM_IMPL_NAMESPACE rtm::RTM_IMPL_VERSION_NAMESPACE_NAME

	#define RTM_IMPL_VERSION_NAMESPACE_BEGIN \
		namespace RTM_IMPL_VERSION_NAMESPACE_NAME \
		{

	#define RTM_IMPL_VERSION_NAMESPACE_END \
		}
#else
	// Namespace is inlined, its usage does not need to be qualified with the
	// full version everywhere
	#define RTM_IMPL_NAMESPACE rtm

	#define RTM_IMPL_VERSION_NAMESPACE_BEGIN \
		inline namespace RTM_IMPL_VERSION_NAMESPACE_NAME \
		{

	#define RTM_IMPL_VERSION_NAMESPACE_END \
		}
#endif

```

`make.py`:

```py
import argparse
import multiprocessing
import os
import platform
import shutil
import subprocess
import sys

def parse_argv():
	parser = argparse.ArgumentParser(add_help=False)

	actions = parser.add_argument_group(title='Actions', description='If no action is specified, on Windows, OS X, and Linux the solution/make files are generated.  Multiple actions can be used simultaneously.')
	actions.add_argument('-build', action='store_true')
	actions.add_argument('-clean', action='store_true')
	actions.add_argument('-clean_only', action='store_true')
	actions.add_argument('-unit_test', action='store_true')
	actions.add_argument('-bench', action='store_true')
	actions.add_argument('-run_bench', action='store_true')
	actions.add_argument('-pull_bench', action='store_true')	# Android only

	target = parser.add_argument_group(title='Target')
	target.add_argument('-compiler', choices=['vs2015', 'vs2017', 'vs2019', 'vs2019-clang', 'vs2022', 'vs2022-clang', 'android', 'clang4', 'clang5', 'clang6', 'clang7', 'clang8', 'clang9', 'clang10', 'clang11', 'clang12', 'clang13', 'clang14', 'clang15', 'clang16', 'clang17', 'clang18', 'gcc5', 'gcc6', 'gcc7', 'gcc8', 'gcc9', 'gcc10', 'gcc11', 'gcc12', 'gcc13', 'osx', 'ios', 'emscripten'], help='Defaults to the host system\'s default compiler')
	target.add_argument('-config', choices=['Debug', 'Release'], type=str.capitalize)
	target.add_argument('-cpu', choices=['x86', 'x64', 'armv7', 'arm64', 'arm64ec', 'wasm'], help='Defaults to the host system\'s architecture')
	target.add_argument('-cpp_version', choices=['11', '14', '17', '20'], help='Defaults to C++11')

	misc = parser.add_argument_group(title='Miscellaneous')
	misc.add_argument('-avx', dest='use_avx', action='store_true', help='Compile using AVX instructions on Windows, OS X, and Linux')
	misc.add_argument('-avx2', dest='use_avx2', action='store_true', help='Compile using AVX2 instructions on Windows, OS X, and Linux')
	misc.add_argument('-nosimd', dest='use_simd', action='store_false', help='Compile without SIMD instructions')
	misc.add_argument('-simd', dest='use_simd', action='store_true', help='Compile with default SIMD instructions')
	misc.add_argument('-num_threads', help='No. to use while compiling and regressing')
	misc.add_argument('-tests_matching', help='Only run tests whose names match this regex')
	misc.add_argument('-vector_mix_test', action='store_true', help='Enable the vector_mix unit tests')
	misc.add_argument('-ci', action='store_true', help='Whether or not this is a Continuous Integration build')
	misc.add_argument('-help', action='help', help='Display this usage information')

	num_threads = multiprocessing.cpu_count()
	if platform.system() == 'Linux' and sys.version_info >= (3, 4):
		num_threads = len(os.sched_getaffinity(0))
	if not num_threads or num_threads == 0:
		num_threads = 4

	parser.set_defaults(build=False, clean=False, clean_only=False, unit_test=False,
		compiler=None, config='Release', cpu=None, cpp_version='11', use_avx=False, use_avx2=False, use_simd=True, num_threads=num_threads, tests_matching='', vector_mix_test=False,
		bench=False, run_bench=False, pull_bench=False)

	args = parser.parse_args()

	is_arm64_cpu = False
	if platform.machine() == 'arm64' or platform.machine() == 'aarch64':
		is_arm64_cpu = True

	# Sanitize and validate our options
	if (args.use_avx or args.use_avx2) and not args.use_simd:
		print('SIMD is explicitly disabled, AVX and AVX2 will not be used')
		args.use_avx = False
		args.use_avx2 = False

	if args.compiler == 'android':
		if not args.cpu:
			args.cpu = 'arm64'

		if not platform.system() == 'Windows':
			print('Android is only supported on Windows')
			sys.exit(1)

		if args.use_avx or args.use_avx2:
			print('AVX and AVX2 are not supported on Android')
			sys.exit(1)

		if not args.cpu in ['armv7', 'arm64']:
			print('{} cpu architecture not in supported list [armv7, arm64] for Android'.format(args.cpu))
			sys.exit(1)
	elif args.compiler == 'ios':
		if not args.cpu:
			args.cpu = 'arm64'

		if not platform.system() == 'Darwin':
			print('iOS is only supported on OS X')
			sys.exit(1)

		if args.use_avx or args.use_avx2:
			print('AVX and AVX2 are not supported on iOS')
			sys.exit(1)

		if args.unit_test:
			print('Unit tests cannot run from the command line on iOS')
			sys.exit(1)

		if not args.cpu in ['arm64']:
			print('{} cpu architecture not in supported list [arm64] for iOS'.format(args.cpu))
			sys.exit(1)
	elif args.compiler == 'emscripten':
		if not args.cpu:
			args.cpu = 'wasm'

		if not platform.system() == 'Darwin' and not platform.system() == 'Linux':
			print('Emscripten is only supported on OS X and Linux')
			sys.exit(1)

		if args.use_avx or args.use_avx2:
			print('AVX and AVX2 are not supported on Emscripten')
			sys.exit(1)

		if not args.cpu in ['wasm']:
			print('{} cpu architecture not in supported list [wasm] for Emscripten'.format(args.cpu))
			sys.exit(1)
	else:
		if not args.cpu:
			if is_arm64_cpu:
				args.cpu = 'arm64'
			else:
				args.cpu = 'x64'

	if args.cpu == 'arm64':
		is_arm_supported = False

		# Cross compilation
		if args.compiler in ['vs2017', 'vs2019', 'vs2022', 'ios', 'android']:
			is_arm_supported = True

		# Native compilation
		if platform.system() == 'Darwin' and is_arm64_cpu:
			is_arm_supported = True
		elif platform.system() == 'Linux' and is_arm64_cpu:
			is_arm_supported = True

		if not is_arm_supported:
			print('arm64 is only supported with VS2017, VS2019, VS2022, OS X (M* processors), Linux, Android, and iOS')
			sys.exit(1)
	elif args.cpu == 'arm64ec':
		if not args.compiler in ['vs2019', 'vs2022']:
			print('arm64ec is only supported with VS2019 and VS2022')
			sys.exit(1)
	elif args.cpu == 'armv7':
		if not args.compiler == 'android':
			print('armv7 is only supported with Android')
			sys.exit(1)
	elif args.cpu == 'wasm':
		if not args.compiler == 'emscripten':
			print('wasm is only supported with Emscripten')
			sys.exit(1)

	if platform.system() == 'Darwin' and args.cpu == 'x86':
		result = subprocess.check_output(['xcodebuild', '-version']).decode("utf-8")
		if 'Xcode 11' in result:
			print('Versions of Xcode 11 and up no longer support x86')
			sys.exit(1)

	return args

def get_generator(compiler, cpu):
	if compiler == None:
		return None

	if platform.system() == 'Windows':
		if compiler == 'vs2015':
			if cpu == 'x86':
				return 'Visual Studio 14'
			elif cpu == 'x64':
				return 'Visual Studio 14 Win64'
		elif compiler == 'vs2017':
			if cpu == 'x86':
				return 'Visual Studio 15'
			elif cpu == 'x64':
				return 'Visual Studio 15 Win64'
			elif cpu == 'arm64':
				# VS2017 ARM/ARM64 support only works with cmake 3.13 and up and the architecture must be specified with
				# the -A cmake switch
				return 'Visual Studio 15 2017'
		elif compiler == 'vs2019' or compiler == 'vs2019-clang':
			return 'Visual Studio 16 2019'
		elif compiler == 'vs2022' or compiler == 'vs2022-clang':
			return 'Visual Studio 17 2022'
		elif compiler == 'android':
			# For Android, we use the default generator since we don't build with CMake
			return None
	elif platform.system() == 'Darwin':
		if compiler == 'osx' or compiler == 'ios':
			return 'Xcode'
		elif compiler == 'emscripten':
			# Emscripten uses the default generator
			return None
	elif platform.system() == 'Linux':
		if compiler == 'emscripten':
			# Emscripten uses the default generator
			return None

		return 'Unix Makefiles'

	print('Unknown compiler: {}'.format(compiler))
	print('See help with: python make.py -help')
	sys.exit(1)

def get_architecture(compiler, cpu):
	if compiler == None:
		return None

	if platform.system() == 'Windows':
		if compiler == 'vs2017':
			if cpu == 'arm64':
				return 'ARM64'

		is_modern_vs = False
		if compiler == 'vs2019' or compiler == 'vs2019-clang':
			is_modern_vs = True
		elif compiler == 'vs2022' or compiler == 'vs2022-clang':
			is_modern_vs = True

		if is_modern_vs:
			if cpu == 'x86':
				return 'Win32'
			else:
				return cpu

	# This compiler/cpu pair does not need the architecture switch
	return None

def get_toolchain(compiler, cmake_script_dir):
	if platform.system() == 'Windows' and compiler == 'android':
		return os.path.join(cmake_script_dir, 'Toolchain-Android.cmake')
	elif platform.system() == 'Darwin' and compiler == 'ios':
		return os.path.join(cmake_script_dir, 'Toolchain-iOS.cmake')

	# No toolchain
	return None

def set_compiler_env(compiler, args):
	if platform.system() == 'Linux':
		if compiler == 'clang4':
			os.environ['CC'] = 'clang-4.0'
			os.environ['CXX'] = 'clang++-4.0'
		elif compiler == 'clang5':
			os.environ['CC'] = 'clang-5.0'
			os.environ['CXX'] = 'clang++-5.0'
		elif compiler == 'clang6':
			os.environ['CC'] = 'clang-6.0'
			os.environ['CXX'] = 'clang++-6.0'
		elif compiler == 'clang7':
			os.environ['CC'] = 'clang-7'
			os.environ['CXX'] = 'clang++-7'
		elif compiler == 'clang8':
			os.environ['CC'] = 'clang-8'
			os.environ['CXX'] = 'clang++-8'
		elif compiler == 'clang9':
			os.environ['CC'] = 'clang-9'
			os.environ['CXX'] = 'clang++-9'
		elif compiler == 'clang10':
			os.environ['CC'] = 'clang-10'
			os.environ['CXX'] = 'clang++-10'
		elif compiler == 'clang11':
			os.environ['CC'] = 'clang-11'
			os.environ['CXX'] = 'clang++-11'
		elif compiler == 'clang12':
			os.environ['CC'] = 'clang-12'
			os.environ['CXX'] = 'clang++-12'
		elif compiler == 'clang13':
			os.environ['CC'] = 'clang-13'
			os.environ['CXX'] = 'clang++-13'
		elif compiler == 'clang14':
			os.environ['CC'] = 'clang-14'
			os.environ['CXX'] = 'clang++-14'
		elif compiler == 'clang15':
			os.environ['CC'] = 'clang-15'
			os.environ['CXX'] = 'clang++-15'
		elif compiler == 'clang16':
			os.environ['CC'] = 'clang-16'
			os.environ['CXX'] = 'clang++-16'
		elif compiler == 'clang17':
			os.environ['CC'] = 'clang-17'
			os.environ['CXX'] = 'clang++-17'
		elif compiler == 'clang18':
			os.environ['CC'] = 'clang-18'
			os.environ['CXX'] = 'clang++-18'
		elif compiler == 'gcc5':
			os.environ['CC'] = 'gcc-5'
			os.environ['CXX'] = 'g++-5'
		elif compiler == 'gcc6':
			os.environ['CC'] = 'gcc-6'
			os.environ['CXX'] = 'g++-6'
		elif compiler == 'gcc7':
			os.environ['CC'] = 'gcc-7'
			os.environ['CXX'] = 'g++-7'
		elif compiler == 'gcc8':
			os.environ['CC'] = 'gcc-8'
			os.environ['CXX'] = 'g++-8'
		elif compiler == 'gcc9':
			os.environ['CC'] = 'gcc-9'
			os.environ['CXX'] = 'g++-9'
		elif compiler == 'gcc10':
			os.environ['CC'] = 'gcc-10'
			os.environ['CXX'] = 'g++-10'
		elif compiler == 'gcc11':
			os.environ['CC'] = 'gcc-11'
			os.environ['CXX'] = 'g++-11'
		elif compiler == 'gcc12':
			os.environ['CC'] = 'gcc-12'
			os.environ['CXX'] = 'g++-12'
		elif compiler == 'gcc13':
			os.environ['CC'] = 'gcc-13'
			os.environ['CXX'] = 'g++-13'
		elif compiler == 'emscripten':
			# Nothing to do for Emscripten
			return
		else:
			print('Unknown compiler: {}'.format(compiler))
			print('See help with: python make.py -help')
			sys.exit(1)

def do_generate_solution(build_dir, cmake_script_dir, args):
	compiler = args.compiler
	cpu = args.cpu
	config = args.config

	is_arm64_cpu = False
	if platform.machine() == 'arm64' or platform.machine() == 'aarch64':
		is_arm64_cpu = True

	if compiler:
		set_compiler_env(compiler, args)

	extra_switches = ['--no-warn-unused-cli']
	extra_switches.append('-DCPU_INSTRUCTION_SET:STRING={}'.format(cpu))
	extra_switches.append('-DCMAKE_CXX_STANDARD:STRING={}'.format(args.cpp_version))

	if platform.system() == 'Windows' and not is_arm64_cpu:
		if cpu == 'arm64' or cpu == 'arm64ec':
			extra_switches.append('-DIS_CROSS_COMPILING:BOOL=true')

	if args.use_avx:
		print('Enabling AVX usage')
		extra_switches.append('-DUSE_AVX_INSTRUCTIONS:BOOL=true')

	if args.use_avx2:
		print('Enabling AVX2 usage')
		extra_switches.append('-DUSE_AVX2_INSTRUCTIONS:BOOL=true')

	if not args.use_simd:
		print('Disabling SIMD instruction usage')
		extra_switches.append('-DUSE_SIMD_INSTRUCTIONS:BOOL=false')

	if args.vector_mix_test:
		print('Enabling vector_mix unit tests')
		extra_switches.append('-DWITH_VECTOR_MIX_TESTS:BOOL=true')

	if args.bench:
		extra_switches.append('-DBUILD_BENCHMARK_EXE:BOOL=true')

	if platform.system() == 'Windows':
		if os.path.sep == '\\':
			# Native Windows
			extra_switches
		else:
			# MSYS2 or Cygwin
			extra_switches.append('-DCMAKE_BUILD_TYPE={}'.format(config.upper()))
	else:
		extra_switches.append('-DCMAKE_BUILD_TYPE={}'.format(config.upper()))

	if platform.system() == 'Darwin' and compiler == 'ios' and args.ci:
		# Disable code signing for CI iOS builds since we just test compilation
		extra_switches.append('-DCMAKE_XCODE_ATTRIBUTE_CODE_SIGNING_REQUIRED=NO')
		extra_switches.append('-DCMAKE_XCODE_ATTRIBUTE_CODE_SIGNING_ALLOWED=NO')

	toolchain = get_toolchain(compiler, cmake_script_dir)
	if toolchain:
		extra_switches.append('-DCMAKE_TOOLCHAIN_FILE={}'.format(toolchain))

	# Generate IDE solution
	print('Generating build files ...')
	if compiler == 'emscripten':
		cmake_cmd = 'emcmake cmake .. -DCMAKE_INSTALL_PREFIX="{}" {}'.format(build_dir, ' '.join(extra_switches))
	else:
		cmake_generator = get_generator(compiler, cpu)
		if not cmake_generator:
			print('Using default generator')
		else:
			generator_suffix = ''
			if compiler == 'vs2019-clang' or compiler == 'vs2022-clang':
				extra_switches.append('-T ClangCL')
				generator_suffix = 'Clang CL'

			print('Using generator: {} {}'.format(cmake_generator, generator_suffix))
			extra_switches.append('-G "{}"'.format(cmake_generator))

		cmake_arch = get_architecture(compiler, cpu)
		if cmake_arch:
			print('Using architecture: {}'.format(cmake_arch))
			extra_switches.append('-A {}'.format(cmake_arch))

		cmake_cmd = 'cmake .. -DCMAKE_INSTALL_PREFIX="{}" {}'.format(build_dir, ' '.join(extra_switches))

	result = subprocess.call(cmake_cmd, shell=True)
	if result != 0:
		sys.exit(result)

def do_build(args):
	config = args.config

	print('Building ...')
	cmake_cmd = 'cmake --build .'
	if platform.system() == 'Windows':
		if args.compiler == 'android':
			cmake_cmd += ' --config {}'.format(config)
		else:
			if os.path.sep == '\\':
				# Native Windows
				cmake_cmd += ' --config {} --target INSTALL'.format(config)
			else:
				# MSYS2 or Cygwin
				cmake_cmd += ' --config {} --target install'.format(config)
	elif platform.system() == 'Darwin':
		if args.compiler == 'ios':
			cmake_cmd += ' --config {}'.format(config)
		else:
			cmake_cmd += ' --config {} --target install'.format(config)
	else:
		cmake_cmd += ' --target install'

	result = subprocess.call(cmake_cmd, shell=True)
	if result != 0:
		sys.exit(result)

def do_tests_android(build_dir, args):
	# Switch our working directory to where we built everything
	working_dir = os.path.join(build_dir, 'tests', 'main_android')
	os.chdir(working_dir)

	gradlew_exe = os.path.join(working_dir, 'gradlew.bat')

	# We uninstall first and then install
	if args.config == 'Debug':
		install_cmd = 'uninstallAll installDebug'
	elif args.config == 'Release':
		install_cmd = 'uninstallAll installRelease'

	# Install our app
	test_cmd = '"{}" {}'.format(gradlew_exe, install_cmd)
	result = subprocess.call(test_cmd, shell=True)
	if result != 0:
		sys.exit(result)

	# Execute through ADB
	run_cmd = 'adb shell am start -n "com.rtm.unit_tests/com.rtm.unit_tests.MainActivity" -a android.intent.action.MAIN -c android.intent.category.LAUNCHER'
	result = subprocess.call(run_cmd, shell=True)
	if result != 0:
		sys.exit(result)

	# Restore working directory
	os.chdir(build_dir)

def do_tests_cmake(args):
	ctest_cmd = 'ctest --output-on-failure --parallel {}'.format(args.num_threads)

	if platform.system() == 'Windows' or platform.system() == 'Darwin':
		ctest_cmd += ' -C {}'.format(args.config)
	if args.tests_matching:
		ctest_cmd += ' --tests-regex {}'.format(args.tests_matching)

	result = subprocess.call(ctest_cmd, shell=True)
	if result != 0:
		sys.exit(result)

def do_tests(build_dir, args):
	print('Running unit tests ...')

	if args.compiler == 'android':
		do_tests_android(build_dir, args)
	else:
		do_tests_cmake(args)

def do_run_bench_android(build_dir, args):
	# Switch our working directory to where we built everything
	working_dir = os.path.join(build_dir, 'tools', 'bench', 'main_android')
	os.chdir(working_dir)

	gradlew_exe = os.path.join(working_dir, 'gradlew.bat')

	# We uninstall first and then install
	if args.config == 'Debug':
		install_cmd = 'uninstallAll installDebug'
	elif args.config == 'Release':
		install_cmd = 'uninstallAll installRelease'

	# Install our app
	test_cmd = '"{}" {}'.format(gradlew_exe, install_cmd)
	result = subprocess.call(test_cmd, shell=True)
	if result != 0:
		sys.exit(result)

	# Execute through ADB
	run_cmd = 'adb shell am start -n "com.rtm.benchmark/com.rtm.benchmark.MainActivity" -a android.intent.action.MAIN -c android.intent.category.LAUNCHER'
	result = subprocess.call(run_cmd, shell=True)
	if result != 0:
		sys.exit(result)

	# Restore working directory
	os.chdir(build_dir)

def do_pull_bench_android(build_dir):
	# Grab the android directory we wrote the results to
	output = str(subprocess.check_output('adb logcat -s acl -e "Benchmark results will be written to:" -m 1 -d'))
	matches = re.search('Benchmark results will be written to: ([/\.\w]+)', output)
	if matches == None:
		print('Failed to find Android source directory from ADB')
		android_src_dir = '/storage/emulated/0/Android/data/com.rtm.benchmark/files'
		print('{} will be used instead'.format(android_src_dir))
	else:
		android_src_dir = matches.group(1)

	# Grab the benchmark results from the android device
	dst_filename = os.path.join(build_dir, 'benchmark_results.json')
	src_filename = '{}/benchmark_results.json'.format(android_src_dir)
	cmd = 'adb pull "{}" "{}"'.format(src_filename, dst_filename)
	os.system(cmd)

def do_run_bench_native(build_dir):
	if platform.system() == 'Windows':
		bench_exe = os.path.join(os.getcwd(), 'bin/rtm_bench.exe')
	else:
		bench_exe = os.path.join(os.getcwd(), 'bin/rtm_bench')

	benchmark_output_filename = os.path.join(build_dir, 'benchmark_results.json')
	bench_cmd = '{} --benchmark_out={} --benchmark_out_format=json'.format(bench_exe, benchmark_output_filename)

	result = subprocess.call(bench_cmd, shell=True)
	if result != 0:
		sys.exit(result)

def do_run_bench(build_dir, args):
	if args.compiler == 'ios':
		return	# Not supported on iOS

	print('Running benchmark ...')

	if args.compiler == 'android':
		do_run_bench_android(build_dir, args)
	else:
		do_run_bench_native(build_dir)

if __name__ == "__main__":
	args = parse_argv()

	build_dir = os.path.join(os.getcwd(), 'build')
	cmake_script_dir = os.path.join(os.getcwd(), 'cmake')

	is_clean_requested = args.clean or args.clean_only
	if is_clean_requested and os.path.exists(build_dir):
		print('Cleaning previous build ...')
		shutil.rmtree(build_dir)

	if args.clean_only:
		sys.exit(0)

	if not os.path.exists(build_dir):
		os.makedirs(build_dir)

	os.chdir(build_dir)

	print('Using config: {}'.format(args.config))
	print('Using cpu: {}'.format(args.cpu))
	if args.compiler:
		print('Using compiler: {}'.format(args.compiler))
	print('Using C++-{}'.format(args.cpp_version))
	print('Using {} threads'.format(args.num_threads))

	# Make sure 'make' runs with all available cores
	os.environ['MAKEFLAGS'] = '-j{}'.format(args.num_threads)

	do_generate_solution(build_dir, cmake_script_dir, args)

	if args.build:
		do_build(args)

	if args.unit_test:
		do_tests(build_dir, args)

	if args.run_bench:
		do_run_bench(build_dir, args)

	if args.pull_bench:
		do_pull_bench_android(build_dir)

	sys.exit(0)

```

`sonar-project.properties`:

```properties
sonar.organization=nfrechette-github

sonar.projectKey=nfrechette_rtm
sonar.projectName=Realtime Math
sonar.projectVersion=2.3.99

sonar.sources=includes,tests/sources,tests/main_generic

sonar.cfamily.build-wrapper-output=bw_output
sonar.cfamily.threads=4

```

`tests/CMakeLists.txt`:

```txt
project(rtm_unit_tests_root NONE)

if(PLATFORM_ANDROID)
	add_subdirectory("${PROJECT_SOURCE_DIR}/main_android")
elseif(PLATFORM_IOS)
	add_subdirectory("${PROJECT_SOURCE_DIR}/main_ios")
elseif(PLATFORM_EMSCRIPTEN)
	add_subdirectory("${PROJECT_SOURCE_DIR}/main_emscripten")
else()
	add_subdirectory("${PROJECT_SOURCE_DIR}/main_generic")
	add_subdirectory("${PROJECT_SOURCE_DIR}/validate_includes")
endif()

```

`tests/main_android/CMakeLists.txt`:

```txt
project(rtm_unit_tests_gradle_shim NONE)

# Set our project root since our gradle files used to build live in the binary output directory
# but the actual source files live in the source/current directory.
set(RTM_PROJECT_ROOT ${CMAKE_CURRENT_BINARY_DIR}/app)
file(RELATIVE_PATH RTM_PROJECT_ROOT ${RTM_PROJECT_ROOT} ${PROJECT_SOURCE_DIR}/app)

# Setup our state
set(RTM_ANDROID_PROJECT_NAME "RTM Unit Tests")
set(RTM_ANDROID_PROJECT_PACKAGE_NAME "com.rtm.unit_tests")

# Configure gradle for our build configuration
configure_file(${RTM_ANDROID_MISC_DIR}/app/build.gradle.in app/build.gradle @ONLY)
configure_file(${RTM_ANDROID_MISC_DIR}/app/src/main/AndroidManifest.xml.in app/src/main/AndroidManifest.xml @ONLY)
configure_file(${RTM_ANDROID_MISC_DIR}/app/src/main/res/values/strings.xml.in app/src/main/res/values/strings.xml @ONLY)
configure_file(${RTM_ANDROID_MISC_DIR}/settings.gradle.in settings.gradle @ONLY)

# Copy gradle related files
file(COPY
	${RTM_ANDROID_MISC_DIR}/build.gradle
	${RTM_ANDROID_MISC_DIR}/gradle.properties
	${RTM_ANDROID_MISC_DIR}/gradle
	${RTM_ANDROID_MISC_DIR}/gradlew
	${RTM_ANDROID_MISC_DIR}/gradlew.bat
	DESTINATION .)

file(COPY
	${RTM_ANDROID_MISC_DIR}/app/proguard-rules.pro
	DESTINATION app)

add_custom_target(${PROJECT_NAME} ALL
	COMMAND gradlew.bat
	# Decide whether we should build Debug or Release
	$<$<CONFIG:Debug>:assembleDebug>
	$<$<CONFIG:Release>:assembleRelease>)

```

`tests/main_android/app/src/main/cpp/CMakeLists.txt`:

```txt
cmake_minimum_required(VERSION 3.2...3.25)
project(rtm_unit_tests CXX)

# Project root is <rtm-dir>\tests\main_android
set(PROJECT_ROOT_DIR "${PROJECT_SOURCE_DIR}/../../../..")

include_directories("${PROJECT_ROOT_DIR}/../../includes")
include_directories("${PROJECT_ROOT_DIR}/../../external/catch2/single_include")
include_directories("${PROJECT_ROOT_DIR}/../sources")

# Grab all of our test source files
file(GLOB_RECURSE ALL_TEST_SOURCE_FILES LIST_DIRECTORIES false
	${PROJECT_ROOT_DIR}/../sources/*.h
	${PROJECT_ROOT_DIR}/../sources/*.cpp)

# Grab all of our main source files
file(GLOB_RECURSE ALL_MAIN_SOURCE_FILES LIST_DIRECTORIES false
	${PROJECT_SOURCE_DIR}/*.cpp)

add_library(${PROJECT_NAME} SHARED ${ALL_TEST_SOURCE_FILES} ${ALL_MAIN_SOURCE_FILES})

# Enable exceptions
target_compile_options(${PROJECT_NAME} PRIVATE -fexceptions)

# Enable debug symbols
target_compile_options(${PROJECT_NAME} PRIVATE -g)

# Throw on failure to allow us to catch them and recover
add_definitions(-DRTM_ON_ASSERT_THROW)

# Disable SIMD if not needed
if(NOT USE_SIMD_INSTRUCTIONS)
	add_definitions(-DRTM_NO_INTRINSICS)
endif()

if(WITH_VECTOR_MIX_TESTS)
	# Enable vector_mix unit tests
	add_definitions(-DRTM_IMPL_WITH_VECTOR_MIX_TESTS)
endif()

target_include_directories(${PROJECT_NAME} PUBLIC jni)

target_link_libraries(${PROJECT_NAME} m log)

```

`tests/main_android/app/src/main/cpp/main.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2018 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#define CATCH_CONFIG_RUNNER
#include "catch2.impl.h"

#include <jni.h>

extern "C" jint Java_com_rtm_unit_1tests_MainActivity_getNumUnitTestCases(JNIEnv* env, jobject caller)
{
	return Catch::getRegistryHub().getTestCaseRegistry().getAllTests().size();
}

extern "C" jint Java_com_rtm_unit_1tests_MainActivity_runUnitTests(JNIEnv* env, jobject caller)
{
	int result = Catch::Session().run();

	return (result < 0xff ? result : 0xff);
}

```

`tests/main_android/app/src/main/java/com/rtm/unit_tests/MainActivity.java`:

```java
package com.rtm.unit_tests;

import android.app.Activity;
import android.widget.TextView;
import android.os.Bundle;

public class MainActivity extends Activity {
	static {
		System.loadLibrary("rtm_unit_tests");
	}

	@Override
	public void onCreate(Bundle savedInstanceState) {
		super.onCreate(savedInstanceState);

		TextView resultTextView = new TextView(this);

		int numUnitTestCases = getNumUnitTestCases();
		int numFailed = runUnitTests();

		if (numFailed == 0)
			resultTextView.setText("All " + numUnitTestCases + " test cases ran successfully!");
		else
			resultTextView.setText(numFailed + " test cases failed!");

		setContentView(resultTextView);
	}

	public native int getNumUnitTestCases();
	public native int runUnitTests();
}

```

`tests/main_emscripten/CMakeLists.txt`:

```txt
project(rtm_unit_tests CXX)

include_directories("${PROJECT_SOURCE_DIR}/../../includes")
include_directories("${PROJECT_SOURCE_DIR}/../../external/catch2/single_include")
include_directories("${PROJECT_SOURCE_DIR}/../sources")

# Grab all of our test source files
file(GLOB_RECURSE ALL_TEST_SOURCE_FILES LIST_DIRECTORIES false
	${PROJECT_SOURCE_DIR}/../sources/*.h
	${PROJECT_SOURCE_DIR}/../sources/*.cpp)

# Grab all of our main source files
file(GLOB_RECURSE ALL_MAIN_SOURCE_FILES LIST_DIRECTORIES false
	${PROJECT_SOURCE_DIR}/*.cpp)

add_executable(${PROJECT_NAME} ${ALL_TEST_SOURCE_FILES} ${ALL_MAIN_SOURCE_FILES})

# Throw on failure to allow us to catch them and recover
add_definitions(-DRTM_ON_ASSERT_THROW)

if(WITH_VECTOR_MIX_TESTS)
	# Enable vector_mix unit tests
	add_definitions(-DRTM_IMPL_WITH_VECTOR_MIX_TESTS)
endif()

target_compile_options(${PROJECT_NAME} PRIVATE -Wall -Wextra)		# Enable all warnings
target_compile_options(${PROJECT_NAME} PRIVATE -Wshadow)			# Enable shadowing warnings
target_compile_options(${PROJECT_NAME} PRIVATE -Werror)				# Treat warnings as errors

# Exceptions are not enabled by default, enable them
target_compile_options(${PROJECT_NAME} PRIVATE -fexceptions)
target_link_libraries(${PROJECT_NAME} PRIVATE "-s DISABLE_EXCEPTION_CATCHING=0")

target_link_libraries(${PROJECT_NAME} PRIVATE "-s NODERAWFS=1")				# Enable the raw node file system
target_link_libraries(${PROJECT_NAME} PRIVATE -lnodefs.js)					# Link the node file system

target_link_libraries(${PROJECT_NAME} PRIVATE "-s ENVIRONMENT=node")		# Force the environment to node

target_link_libraries(${PROJECT_NAME} PRIVATE "-s ALLOW_MEMORY_GROWTH=1")	# Allow dynamic memory allocation

# Setup Catch2 so we can find and execute the unit tests with CTest
set(OptionalCatchTestLauncher node)
list(APPEND CMAKE_MODULE_PATH "${PROJECT_SOURCE_DIR}/../../external/catch2/contrib")
include(CTest)
include(ParseAndAddCatchTests)
ParseAndAddCatchTests(${PROJECT_NAME})

install(FILES
	${CMAKE_CURRENT_BINARY_DIR}/${PROJECT_NAME}.js
	${CMAKE_CURRENT_BINARY_DIR}/${PROJECT_NAME}.wasm
	DESTINATION bin)

```

`tests/main_emscripten/main.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#define CATCH_CONFIG_RUNNER
#include "catch2.impl.h"

int main(int argc, char* argv[])
{
	int result = Catch::Session().run(argc, argv);
	return (result < 0xff ? result : 0xff);
}

```

`tests/main_generic/CMakeLists.txt`:

```txt
project(rtm_unit_tests CXX)

include_directories("${PROJECT_SOURCE_DIR}/../../includes")
include_directories("${PROJECT_SOURCE_DIR}/../../external/catch2/single_include")
include_directories("${PROJECT_SOURCE_DIR}/../sources")

# Grab all of our test source files
file(GLOB_RECURSE ALL_TEST_SOURCE_FILES LIST_DIRECTORIES false
	${PROJECT_SOURCE_DIR}/../sources/*.h
	${PROJECT_SOURCE_DIR}/../sources/*.cpp)

create_source_groups("${ALL_TEST_SOURCE_FILES}" ${PROJECT_SOURCE_DIR}/..)

# Grab all of our main source files
file(GLOB_RECURSE ALL_MAIN_SOURCE_FILES LIST_DIRECTORIES false
	${PROJECT_SOURCE_DIR}/*.cpp)

create_source_groups("${ALL_MAIN_SOURCE_FILES}" ${PROJECT_SOURCE_DIR})

add_executable(${PROJECT_NAME} ${ALL_TEST_SOURCE_FILES} ${ALL_MAIN_SOURCE_FILES})

if(NOT IS_CROSS_COMPILING)
	# Don't attempt to detect unit tests if we are cross-compiling
	list(APPEND CMAKE_MODULE_PATH "${PROJECT_SOURCE_DIR}/../../external/catch2/contrib")
	include(CTest)
	include(Catch)
	catch_discover_tests(${PROJECT_NAME})
endif()

setup_default_compiler_flags(${PROJECT_NAME})

if(MSVC)
	if(CPU_INSTRUCTION_SET MATCHES "arm64" OR CMAKE_CXX_COMPILER_ID MATCHES "Clang")
		# Exceptions are not enabled by default for ARM targets and clang-cl, enable them
		target_compile_options(${PROJECT_NAME} PRIVATE /EHsc)
	endif()
endif()

# Throw on failure to allow us to catch them and recover
add_definitions(-DRTM_ON_ASSERT_THROW)

if(WITH_VECTOR_MIX_TESTS)
	# Enable vector_mix unit tests
	add_definitions(-DRTM_IMPL_WITH_VECTOR_MIX_TESTS)
endif()

install(TARGETS ${PROJECT_NAME} RUNTIME DESTINATION bin)

```

`tests/main_generic/main.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2018 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#define CATCH_CONFIG_RUNNER
#include "catch2.impl.h"

#ifdef _WIN32
#include <conio.h>
#endif

int main(int argc, char* argv[])
{
	int result = Catch::Session().run(argc, argv);

#ifdef _WIN32
	if (IsDebuggerPresent())
	{
		printf("Press any key to continue...\n");
		while (_kbhit() == 0);
	}
#endif

	return (result < 0xff ? result : 0xff);
}

```

`tests/main_ios/CMakeLists.txt`:

```txt
project(rtm_unit_tests)

# iOS cmake toolchain does not support CMAKE_CXX_STANDARD
set(CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} -std=c++${CMAKE_CXX_STANDARD}")

# Force enable debug symbols
set(CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} -g")

# Enable optimizations in Release
set(CMAKE_CXX_FLAGS_RELEASE "${CMAKE_CXX_FLAGS_RELEASE} -O3")

set(MACOSX_BUNDLE_EXECUTABLE_NAME ${PROJECT_NAME})
set(MACOSX_BUNDLE_INFO_STRING "com.rtm.rtm-unit-tests")
set(MACOSX_BUNDLE_GUI_IDENTIFIER "com.rtm.rtm-unit-tests")
set(MACOSX_BUNDLE_BUNDLE_NAME "rtm-unit-tests")

include_directories("${PROJECT_SOURCE_DIR}/../../includes")
include_directories("${PROJECT_SOURCE_DIR}/../../external/catch2/single_include")
include_directories("${PROJECT_SOURCE_DIR}/../sources")

# Grab all of our test source files
file(GLOB_RECURSE ALL_TEST_SOURCE_FILES LIST_DIRECTORIES false
	${PROJECT_SOURCE_DIR}/../sources/*.h
	${PROJECT_SOURCE_DIR}/../sources/*.cpp)

create_source_groups("${ALL_TEST_SOURCE_FILES}" ${PROJECT_SOURCE_DIR}/..)

# Grab all of our main source files
file(GLOB_RECURSE ALL_MAIN_SOURCE_FILES LIST_DIRECTORIES false
	${PROJECT_SOURCE_DIR}/*.cpp)

create_source_groups("${ALL_MAIN_SOURCE_FILES}" ${PROJECT_SOURCE_DIR})

add_executable(${PROJECT_NAME} MACOSX_BUNDLE ${ALL_TEST_SOURCE_FILES} ${ALL_MAIN_SOURCE_FILES})

# Throw on failure to allow us to catch them and recover
add_definitions(-DRTM_ON_ASSERT_THROW)

# Disable SIMD if not needed
if(NOT USE_SIMD_INSTRUCTIONS)
	add_definitions(-DRTM_NO_INTRINSICS)
endif()

if(WITH_VECTOR_MIX_TESTS)
	# Enable vector_mix unit tests
	add_definitions(-DRTM_IMPL_WITH_VECTOR_MIX_TESTS)
endif()

# Set XCode properties
set_property(TARGET ${PROJECT_NAME} PROPERTY XCODE_ATTRIBUTE_PRODUCT_BUNDLE_IDENTIFIER "com.rtm.rtm-unit-tests")

```

`tests/main_ios/main.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2018 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#define CATCH_CONFIG_RUNNER
#include "catch2.impl.h"

int main(int argc, char* argv[])
{
	int result = Catch::Session().run(argc, argv);

	return (result < 0xff ? result : 0xff);
}

```

`tests/sources/catch2.impl.h`:

```h
#pragma once

////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2023 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

// We disable various informational warnings with using /Wall with MSVC caused by Catch2

#if defined(_MSC_VER) && !defined(__clang__)
	#pragma warning(push)
	#pragma warning(disable : 4365)	// Signed/unsigned mismatch
	#pragma warning(disable : 4388)	// Signed/unsigned comparison
	#pragma warning(disable : 4583)	// Destructor is not implicitly called
	#pragma warning(disable : 4623)	// Default constructor implicitly deleted
	#pragma warning(disable : 4625)	// Copy constructor implicitly deleted
	#pragma warning(disable : 4626)	// Copy assignment operator implicitly deleted
	#pragma warning(disable : 4868)	// May not enforce left to right order in initializer
	#pragma warning(disable : 5026)	// Move constructor implicitly deleted
	#pragma warning(disable : 5027)	// Move assignment operator implicitly deleted
	#pragma warning(disable : 5039)	// Pointer to potentially throwing function passed to extern C
	#pragma warning(disable : 5204)	// Class has virtual functions but no virtual destructor
	#pragma warning(disable : 5219)	// Implicit conversion, possible loss of data
	#pragma warning(disable : 5267)	// Implicit copy constructor deprecated due to user destructor
#endif

// Include Windows.h explicitly
// Below is a common configuration which ends up defining min/max macros by default
// We wish to support this and catch improper usage during compilation of the unit tests
#if defined(_WIN32)
	// The below excludes some other unused services from the windows headers -- see windows.h for details.
	#define NOGDICAPMASKS            // CC_*, LC_*, PC_*, CP_*, TC_*, RC_
	#define NOVIRTUALKEYCODES        // VK_*
	#define NOWINMESSAGES            // WM_*, EM_*, LB_*, CB_*
	#define NOWINSTYLES                // WS_*, CS_*, ES_*, LBS_*, SBS_*, CBS_*
	#define NOSYSMETRICS            // SM_*
	#define NOMENUS                    // MF_*
	#define NOICONS                    // IDI_*
	#define NOKEYSTATES                // MK_*
	#define NOSYSCOMMANDS            // SC_*
	#define NORASTEROPS                // Binary and Tertiary raster ops
	#define NOSHOWWINDOW            // SW_*
	#define OEMRESOURCE                // OEM Resource values
	#define NOATOM                    // Atom Manager routines
	#define NOCLIPBOARD                // Clipboard routines
	#define NOCOLOR                    // Screen colors
	#define NOCTLMGR                // Control and Dialog routines
	#define NODRAWTEXT                // DrawText() and DT_*
	#define NOGDI                    // All GDI #defines and routines
	#define NOKERNEL                // All KERNEL #defines and routines
	#define NOUSER                    // All USER #defines and routines
	#define NONLS                    // All NLS #defines and routines
	#define NOMB                    // MB_* and MessageBox()
	#define NOMEMMGR                // GMEM_*, LMEM_*, GHND, LHND, associated routines
	#define NOMETAFILE                // typedef METAFILEPICT
	#if !defined(NOMINMAX) && 0			// We leave MIN/MAX macros enabled, they are defined by default and we must handle it
	#define NOMINMAX                // Macros min(a,b) and max(a,b)
	#endif
	#define NOMSG                    // typedef MSG and associated routines
	#define NOOPENFILE                // OpenFile(), OemToAnsi, AnsiToOem, and OF_*
	#define NOSCROLL                // SB_* and scrolling routines
	#define NOSERVICE                // All Service Controller routines, SERVICE_ equates, etc.
	#define NOSOUND                    // Sound driver routines
	#define NOTEXTMETRIC            // typedef TEXTMETRIC and associated routines
	#define NOWH                    // SetWindowsHook and WH_*
	#define NOWINOFFSETS            // GWL_*, GCL_*, associated routines
	#define NOCOMM                    // COMM driver routines
	#define NOKANJI                    // Kanji support stuff.
	#define NOHELP                    // Help engine interface.
	#define NOPROFILER                // Profiler interface.
	#define NODEFERWINDOWPOS        // DeferWindowPos routines
	#define NOMCX                    // Modem Configuration Extensions
	#define NOCRYPT
	#define NOTAPE
	#define NOIMAGE
	#define NOPROXYSTUB
	#define NORPC

	#include <windows.h>
#endif

#include <catch2/catch.hpp>

#if defined(_MSC_VER) && !defined(__clang__)
	#pragma warning(pop)
#endif

```

`tests/sources/test_constants.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2019 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include "catch2.impl.h"

#include <rtm/constants.h>
#include <rtm/scalarf.h>
#include <rtm/scalard.h>

using namespace rtm;

TEST_CASE("constants", "[math][constants]")
{
	// Float
	CHECK(scalar_near_equal(float(constants::pi()), 3.141592653589793238462643383279502884F, 1.0e-6F));
	CHECK(scalar_near_equal(float(-constants::pi()), -3.141592653589793238462643383279502884F, 1.0e-6F));
	CHECK(scalar_near_equal(float(+constants::pi()), +3.141592653589793238462643383279502884F, 1.0e-6F));
	CHECK(scalar_near_equal(float(constants::pi()) * 2.0F, 3.141592653589793238462643383279502884F * 2.0F, 1.0e-6F));
	CHECK(scalar_near_equal(float(2.0F * constants::pi()), 2.0F * 3.141592653589793238462643383279502884F, 1.0e-6F));
	CHECK(scalar_near_equal(float(constants::pi()) / 2.0F, 3.141592653589793238462643383279502884F / 2.0F, 1.0e-6F));
	CHECK(scalar_near_equal(float(2.0F / constants::pi()), 2.0F / 3.141592653589793238462643383279502884F, 1.0e-6F));
	CHECK(scalar_near_equal(float(constants::pi()) + 1.0F, 3.141592653589793238462643383279502884F + 1.0F, 1.0e-6F));
	CHECK(scalar_near_equal(float(1.0F + constants::pi()), 1.0F + 3.141592653589793238462643383279502884F, 1.0e-6F));
	CHECK(scalar_near_equal(float(constants::pi()) - 1.0F, 3.141592653589793238462643383279502884F - 1.0F, 1.0e-6F));
	CHECK(scalar_near_equal(float(1.0F - constants::pi()), 1.0F - 3.141592653589793238462643383279502884F, 1.0e-6F));

	// Double
	CHECK(scalar_near_equal(double(constants::pi()), 3.141592653589793238462643383279502884, 1.0e-6));
	CHECK(scalar_near_equal(double(-constants::pi()), -3.141592653589793238462643383279502884, 1.0e-6));
	CHECK(scalar_near_equal(double(+constants::pi()), +3.141592653589793238462643383279502884, 1.0e-6));
	CHECK(scalar_near_equal(double(constants::pi() * 2.0), 3.141592653589793238462643383279502884 * 2.0, 1.0e-6));
	CHECK(scalar_near_equal(double(2.0 * constants::pi()), 2.0 * 3.141592653589793238462643383279502884, 1.0e-6));
	CHECK(scalar_near_equal(double(constants::pi() / 2.0), 3.141592653589793238462643383279502884 / 2.0, 1.0e-6));
	CHECK(scalar_near_equal(double(2.0 / constants::pi()), 2.0 / 3.141592653589793238462643383279502884, 1.0e-6));
	CHECK(scalar_near_equal(double(constants::pi() + 1.0), 3.141592653589793238462643383279502884 + 1.0, 1.0e-6));
	CHECK(scalar_near_equal(double(1.0 + constants::pi()), 1.0 + 3.141592653589793238462643383279502884, 1.0e-6));
	CHECK(scalar_near_equal(double(constants::pi() - 1.0), 3.141592653589793238462643383279502884 - 1.0, 1.0e-6));
	CHECK(scalar_near_equal(double(1.0 - constants::pi()), 1.0 - 3.141592653589793238462643383279502884, 1.0e-6));
}

```

`tests/sources/test_header_fwd.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2022 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/fwd.h>

```

`tests/sources/test_macros_matrix.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2021 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include "catch2.impl.h"

#include <rtm/macros.h>
#include <rtm/vector4d.h>
#include <rtm/vector4f.h>

TEST_CASE("macros matrixf", "[math][macros][matrix]")
{
	const float threshold = 0.0F;	// Result must be binary exact!

	{
		rtm::vector4f xy0 = rtm::vector_set(1.0F, 2.0F, 3.0F);
		rtm::vector4f xy1 = rtm::vector_set(4.0F, 5.0F, 6.0F);

		rtm::vector4f xx;
		rtm::vector4f yy;
		RTM_MATRIXF_TRANSPOSE_2X2(xy0, xy1, xx, yy);

		CHECK(rtm::vector_all_near_equal2(rtm::vector_set(1.0F, 4.0F, 7.0F), xx, threshold));
		CHECK(rtm::vector_all_near_equal2(rtm::vector_set(2.0F, 5.0F, 8.0F), yy, threshold));

		// Test when input == output
		RTM_MATRIXF_TRANSPOSE_2X2(xy0, xy1, xy0, xy1);

		CHECK(rtm::vector_all_near_equal2(rtm::vector_set(1.0F, 4.0F, 7.0F), xy0, threshold));
		CHECK(rtm::vector_all_near_equal2(rtm::vector_set(2.0F, 5.0F, 8.0F), xy1, threshold));
	}

	{
		rtm::vector4f xyz0 = rtm::vector_set(1.0F, 2.0F, 3.0F);
		rtm::vector4f xyz1 = rtm::vector_set(4.0F, 5.0F, 6.0F);
		rtm::vector4f xyz2 = rtm::vector_set(7.0F, 8.0F, 9.0F);

		rtm::vector4f xxx;
		rtm::vector4f yyy;
		rtm::vector4f zzz;
		RTM_MATRIXF_TRANSPOSE_3X3(xyz0, xyz1, xyz2, xxx, yyy, zzz);

		CHECK(rtm::vector_all_near_equal3(rtm::vector_set(1.0F, 4.0F, 7.0F), xxx, threshold));
		CHECK(rtm::vector_all_near_equal3(rtm::vector_set(2.0F, 5.0F, 8.0F), yyy, threshold));
		CHECK(rtm::vector_all_near_equal3(rtm::vector_set(3.0F, 6.0F, 9.0F), zzz, threshold));

		// Test when input == output
		RTM_MATRIXF_TRANSPOSE_3X3(xyz0, xyz1, xyz2, xyz0, xyz1, xyz2);

		CHECK(rtm::vector_all_near_equal3(rtm::vector_set(1.0F, 4.0F, 7.0F), xyz0, threshold));
		CHECK(rtm::vector_all_near_equal3(rtm::vector_set(2.0F, 5.0F, 8.0F), xyz1, threshold));
		CHECK(rtm::vector_all_near_equal3(rtm::vector_set(3.0F, 6.0F, 9.0F), xyz2, threshold));
	}

	{
		rtm::vector4f xyzw0 = rtm::vector_set(1.0F, 2.0F, 3.0F, 20.0F);
		rtm::vector4f xyzw1 = rtm::vector_set(4.0F, 5.0F, 6.0F, 21.0F);
		rtm::vector4f xyzw2 = rtm::vector_set(7.0F, 8.0F, 9.0F, 22.0F);
		rtm::vector4f xyzw3 = rtm::vector_set(10.0F, 11.0F, 12.0F, 23.0F);

		rtm::vector4f xxxx;
		rtm::vector4f yyyy;
		rtm::vector4f zzzz;
		rtm::vector4f wwww;
		RTM_MATRIXF_TRANSPOSE_4X4(xyzw0, xyzw1, xyzw2, xyzw3, xxxx, yyyy, zzzz, wwww);

		CHECK(rtm::vector_all_near_equal(rtm::vector_set(1.0F, 4.0F, 7.0F, 10.0F), xxxx, threshold));
		CHECK(rtm::vector_all_near_equal(rtm::vector_set(2.0F, 5.0F, 8.0F, 11.0F), yyyy, threshold));
		CHECK(rtm::vector_all_near_equal(rtm::vector_set(3.0F, 6.0F, 9.0F, 12.0F), zzzz, threshold));
		CHECK(rtm::vector_all_near_equal(rtm::vector_set(20.0F, 21.0F, 22.0F, 23.0F), wwww, threshold));

		// Test when input == output
		RTM_MATRIXF_TRANSPOSE_4X4(xyzw0, xyzw1, xyzw2, xyzw3, xyzw0, xyzw1, xyzw2, xyzw3);

		CHECK(rtm::vector_all_near_equal(rtm::vector_set(1.0F, 4.0F, 7.0F, 10.0F), xyzw0, threshold));
		CHECK(rtm::vector_all_near_equal(rtm::vector_set(2.0F, 5.0F, 8.0F, 11.0F), xyzw1, threshold));
		CHECK(rtm::vector_all_near_equal(rtm::vector_set(3.0F, 6.0F, 9.0F, 12.0F), xyzw2, threshold));
		CHECK(rtm::vector_all_near_equal(rtm::vector_set(20.0F, 21.0F, 22.0F, 23.0F), xyzw3, threshold));
	}

	{
		rtm::vector4f xyz0 = rtm::vector_set(1.0F, 2.0F, 3.0F, 20.0F);
		rtm::vector4f xyz1 = rtm::vector_set(4.0F, 5.0F, 6.0F, 21.0F);
		rtm::vector4f xyz2 = rtm::vector_set(7.0F, 8.0F, 9.0F, 22.0F);
		rtm::vector4f xyz3 = rtm::vector_set(10.0F, 11.0F, 12.0F, 23.0F);

		rtm::vector4f xxxx;
		rtm::vector4f yyyy;
		rtm::vector4f zzzz;
		RTM_MATRIXF_TRANSPOSE_4X3(xyz0, xyz1, xyz2, xyz3, xxxx, yyyy, zzzz);

		CHECK(rtm::vector_all_near_equal(rtm::vector_set(1.0F, 4.0F, 7.0F, 10.0F), xxxx, threshold));
		CHECK(rtm::vector_all_near_equal(rtm::vector_set(2.0F, 5.0F, 8.0F, 11.0F), yyyy, threshold));
		CHECK(rtm::vector_all_near_equal(rtm::vector_set(3.0F, 6.0F, 9.0F, 12.0F), zzzz, threshold));

		// Test when input == output
		RTM_MATRIXF_TRANSPOSE_4X3(xyz0, xyz1, xyz2, xyz3, xyz0, xyz1, xyz2);

		CHECK(rtm::vector_all_near_equal(rtm::vector_set(1.0F, 4.0F, 7.0F, 10.0F), xyz0, threshold));
		CHECK(rtm::vector_all_near_equal(rtm::vector_set(2.0F, 5.0F, 8.0F, 11.0F), xyz1, threshold));
		CHECK(rtm::vector_all_near_equal(rtm::vector_set(3.0F, 6.0F, 9.0F, 12.0F), xyz2, threshold));
	}

	{
		rtm::vector4f xyzw0 = rtm::vector_set(1.0F, 2.0F, 3.0F, 20.0F);
		rtm::vector4f xyzw1 = rtm::vector_set(4.0F, 5.0F, 6.0F, 21.0F);
		rtm::vector4f xyzw2 = rtm::vector_set(7.0F, 8.0F, 9.0F, 22.0F);

		rtm::vector4f xxx;
		rtm::vector4f yyy;
		rtm::vector4f zzz;
		rtm::vector4f www;
		RTM_MATRIXF_TRANSPOSE_3X4(xyzw0, xyzw1, xyzw2, xxx, yyy, zzz, www);

		CHECK(rtm::vector_all_near_equal3(rtm::vector_set(1.0F, 4.0F, 7.0F), xxx, threshold));
		CHECK(rtm::vector_all_near_equal3(rtm::vector_set(2.0F, 5.0F, 8.0F), yyy, threshold));
		CHECK(rtm::vector_all_near_equal3(rtm::vector_set(3.0F, 6.0F, 9.0F), zzz, threshold));
		CHECK(rtm::vector_all_near_equal3(rtm::vector_set(20.0F, 21.0F, 22.0F), www, threshold));

		// Test when input == output
		RTM_MATRIXF_TRANSPOSE_3X4(xyzw0, xyzw1, xyzw2, xyzw0, xyzw1, xyzw2, xxx);

		CHECK(rtm::vector_all_near_equal3(rtm::vector_set(1.0F, 4.0F, 7.0F), xyzw0, threshold));
		CHECK(rtm::vector_all_near_equal3(rtm::vector_set(2.0F, 5.0F, 8.0F), xyzw1, threshold));
		CHECK(rtm::vector_all_near_equal3(rtm::vector_set(3.0F, 6.0F, 9.0F), xyzw2, threshold));
		CHECK(rtm::vector_all_near_equal3(rtm::vector_set(20.0F, 21.0F, 22.0F), xxx, threshold));
	}
}

TEST_CASE("macros matrixd", "[math][macros][matrix]")
{
	const double threshold = 0.0;	// Result must be binary exact!

	{
		rtm::vector4d xy0 = rtm::vector_set(1.0, 2.0, 3.0);
		rtm::vector4d xy1 = rtm::vector_set(4.0, 5.0, 6.0);

		rtm::vector4d xx;
		rtm::vector4d yy;
		RTM_MATRIXD_TRANSPOSE_2X2(xy0, xy1, xx, yy);

		CHECK(rtm::vector_all_near_equal2(rtm::vector_set(1.0, 4.0, 7.0), xx, threshold));
		CHECK(rtm::vector_all_near_equal2(rtm::vector_set(2.0, 5.0, 8.0), yy, threshold));

		// Test when input == output
		RTM_MATRIXD_TRANSPOSE_2X2(xy0, xy1, xy0, xy1);

		CHECK(rtm::vector_all_near_equal2(rtm::vector_set(1.0, 4.0, 7.0), xy0, threshold));
		CHECK(rtm::vector_all_near_equal2(rtm::vector_set(2.0, 5.0, 8.0), xy1, threshold));
	}

	{
		rtm::vector4d xyz0 = rtm::vector_set(1.0, 2.0, 3.0);
		rtm::vector4d xyz1 = rtm::vector_set(4.0, 5.0, 6.0);
		rtm::vector4d xyz2 = rtm::vector_set(7.0, 8.0, 9.0);

		rtm::vector4d xxx;
		rtm::vector4d yyy;
		rtm::vector4d zzz;
		RTM_MATRIXD_TRANSPOSE_3X3(xyz0, xyz1, xyz2, xxx, yyy, zzz);

		CHECK(rtm::vector_all_near_equal3(rtm::vector_set(1.0, 4.0, 7.0), xxx, threshold));
		CHECK(rtm::vector_all_near_equal3(rtm::vector_set(2.0, 5.0, 8.0), yyy, threshold));
		CHECK(rtm::vector_all_near_equal3(rtm::vector_set(3.0, 6.0, 9.0), zzz, threshold));

		// Test when input == output
		RTM_MATRIXD_TRANSPOSE_3X3(xyz0, xyz1, xyz2, xyz0, xyz1, xyz2);

		CHECK(rtm::vector_all_near_equal3(rtm::vector_set(1.0, 4.0, 7.0), xyz0, threshold));
		CHECK(rtm::vector_all_near_equal3(rtm::vector_set(2.0, 5.0, 8.0), xyz1, threshold));
		CHECK(rtm::vector_all_near_equal3(rtm::vector_set(3.0, 6.0, 9.0), xyz2, threshold));
	}

	{
		rtm::vector4d xyzw0 = rtm::vector_set(1.0, 2.0, 3.0, 20.0);
		rtm::vector4d xyzw1 = rtm::vector_set(4.0, 5.0, 6.0, 21.0);
		rtm::vector4d xyzw2 = rtm::vector_set(7.0, 8.0, 9.0, 22.0);
		rtm::vector4d xyzw3 = rtm::vector_set(10.0, 11.0, 12.0, 23.0);

		rtm::vector4d xxxx;
		rtm::vector4d yyyy;
		rtm::vector4d zzzz;
		rtm::vector4d wwww;
		RTM_MATRIXD_TRANSPOSE_4X4(xyzw0, xyzw1, xyzw2, xyzw3, xxxx, yyyy, zzzz, wwww);

		CHECK(rtm::vector_all_near_equal(rtm::vector_set(1.0, 4.0, 7.0, 10.0), xxxx, threshold));
		CHECK(rtm::vector_all_near_equal(rtm::vector_set(2.0, 5.0, 8.0, 11.0), yyyy, threshold));
		CHECK(rtm::vector_all_near_equal(rtm::vector_set(3.0, 6.0, 9.0, 12.0), zzzz, threshold));
		CHECK(rtm::vector_all_near_equal(rtm::vector_set(20.0, 21.0, 22.0, 23.0), wwww, threshold));

		// Test when input == output
		RTM_MATRIXD_TRANSPOSE_4X4(xyzw0, xyzw1, xyzw2, xyzw3, xyzw0, xyzw1, xyzw2, xyzw3);

		CHECK(rtm::vector_all_near_equal(rtm::vector_set(1.0, 4.0, 7.0, 10.0), xyzw0, threshold));
		CHECK(rtm::vector_all_near_equal(rtm::vector_set(2.0, 5.0, 8.0, 11.0), xyzw1, threshold));
		CHECK(rtm::vector_all_near_equal(rtm::vector_set(3.0, 6.0, 9.0, 12.0), xyzw2, threshold));
		CHECK(rtm::vector_all_near_equal(rtm::vector_set(20.0, 21.0, 22.0, 23.0), xyzw3, threshold));
	}

	{
		rtm::vector4d xyz0 = rtm::vector_set(1.0, 2.0, 3.0, 20.0);
		rtm::vector4d xyz1 = rtm::vector_set(4.0, 5.0, 6.0, 21.0);
		rtm::vector4d xyz2 = rtm::vector_set(7.0, 8.0, 9.0, 22.0);
		rtm::vector4d xyz3 = rtm::vector_set(10.0, 11.0, 12.0, 23.0);

		rtm::vector4d xxxx;
		rtm::vector4d yyyy;
		rtm::vector4d zzzz;
		RTM_MATRIXD_TRANSPOSE_4X3(xyz0, xyz1, xyz2, xyz3, xxxx, yyyy, zzzz);

		CHECK(rtm::vector_all_near_equal(rtm::vector_set(1.0, 4.0, 7.0, 10.0), xxxx, threshold));
		CHECK(rtm::vector_all_near_equal(rtm::vector_set(2.0, 5.0, 8.0, 11.0), yyyy, threshold));
		CHECK(rtm::vector_all_near_equal(rtm::vector_set(3.0, 6.0, 9.0, 12.0), zzzz, threshold));

		// Test when input == output
		RTM_MATRIXD_TRANSPOSE_4X3(xyz0, xyz1, xyz2, xyz3, xyz0, xyz1, xyz2);

		CHECK(rtm::vector_all_near_equal(rtm::vector_set(1.0, 4.0, 7.0, 10.0), xyz0, threshold));
		CHECK(rtm::vector_all_near_equal(rtm::vector_set(2.0, 5.0, 8.0, 11.0), xyz1, threshold));
		CHECK(rtm::vector_all_near_equal(rtm::vector_set(3.0, 6.0, 9.0, 12.0), xyz2, threshold));
	}

	{
		rtm::vector4d xyzw0 = rtm::vector_set(1.0, 2.0, 3.0, 20.0);
		rtm::vector4d xyzw1 = rtm::vector_set(4.0, 5.0, 6.0, 21.0);
		rtm::vector4d xyzw2 = rtm::vector_set(7.0, 8.0, 9.0, 22.0);

		rtm::vector4d xxx;
		rtm::vector4d yyy;
		rtm::vector4d zzz;
		rtm::vector4d www;
		RTM_MATRIXD_TRANSPOSE_3X4(xyzw0, xyzw1, xyzw2, xxx, yyy, zzz, www);

		CHECK(rtm::vector_all_near_equal3(rtm::vector_set(1.0, 4.0, 7.0), xxx, threshold));
		CHECK(rtm::vector_all_near_equal3(rtm::vector_set(2.0, 5.0, 8.0), yyy, threshold));
		CHECK(rtm::vector_all_near_equal3(rtm::vector_set(3.0, 6.0, 9.0), zzz, threshold));
		CHECK(rtm::vector_all_near_equal3(rtm::vector_set(20.0, 21.0, 22.0), www, threshold));

		// Test when input == output
		RTM_MATRIXD_TRANSPOSE_3X4(xyzw0, xyzw1, xyzw2, xyzw0, xyzw1, xyzw2, xxx);

		CHECK(rtm::vector_all_near_equal3(rtm::vector_set(1.0, 4.0, 7.0), xyzw0, threshold));
		CHECK(rtm::vector_all_near_equal3(rtm::vector_set(2.0, 5.0, 8.0), xyzw1, threshold));
		CHECK(rtm::vector_all_near_equal3(rtm::vector_set(3.0, 6.0, 9.0), xyzw2, threshold));
		CHECK(rtm::vector_all_near_equal3(rtm::vector_set(20.0, 21.0, 22.0), xxx, threshold));
	}
}

```

`tests/sources/test_mask4.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2019 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include "catch2.impl.h"

#include <rtm/type_traits.h>
#include <rtm/mask4d.h>
#include <rtm/mask4f.h>
#include <rtm/mask4i.h>
#include <rtm/mask4q.h>

#include <cstring>

using namespace rtm;

template<typename IntType, typename Mask4Type>
inline Mask4Type reference_mask_and(const Mask4Type& input0, const Mask4Type& input1)
{
	IntType input0_[4];
	IntType input1_[4];

	static_assert(sizeof(Mask4Type) == sizeof(input0_), "Unexpected size");
	std::memcpy(&input0_[0], &input0, sizeof(Mask4Type));
	std::memcpy(&input1_[0], &input1, sizeof(Mask4Type));

	IntType result_[4];
	result_[0] = input0_[0] & input1_[0];
	result_[1] = input0_[1] & input1_[1];
	result_[2] = input0_[2] & input1_[2];
	result_[3] = input0_[3] & input1_[3];

	Mask4Type result;
	std::memcpy(&result, &result_[0], sizeof(Mask4Type));

	return result;
}

template<typename IntType, typename Mask4Type>
inline Mask4Type reference_mask_or(const Mask4Type& input0, const Mask4Type& input1)
{
	IntType input0_[4];
	IntType input1_[4];

	static_assert(sizeof(Mask4Type) == sizeof(input0_), "Unexpected size");
	std::memcpy(&input0_[0], &input0, sizeof(Mask4Type));
	std::memcpy(&input1_[0], &input1, sizeof(Mask4Type));

	IntType result_[4];
	result_[0] = input0_[0] | input1_[0];
	result_[1] = input0_[1] | input1_[1];
	result_[2] = input0_[2] | input1_[2];
	result_[3] = input0_[3] | input1_[3];

	Mask4Type result;
	std::memcpy(&result, &result_[0], sizeof(Mask4Type));

	return result;
}

template<typename IntType, typename Mask4Type>
inline Mask4Type reference_mask_xor(const Mask4Type& input0, const Mask4Type& input1)
{
	IntType input0_[4];
	IntType input1_[4];

	static_assert(sizeof(Mask4Type) == sizeof(input0_), "Unexpected size");
	std::memcpy(&input0_[0], &input0, sizeof(Mask4Type));
	std::memcpy(&input1_[0], &input1, sizeof(Mask4Type));

	IntType result_[4];
	result_[0] = input0_[0] ^ input1_[0];
	result_[1] = input0_[1] ^ input1_[1];
	result_[2] = input0_[2] ^ input1_[2];
	result_[3] = input0_[3] ^ input1_[3];

	Mask4Type result;
	std::memcpy(&result, &result_[0], sizeof(Mask4Type));

	return result;
}

template<typename IntType, typename Mask4Type>
inline Mask4Type reference_mask_not(const Mask4Type& input)
{
	IntType input_[4];

	static_assert(sizeof(Mask4Type) == sizeof(input_), "Unexpected size");
	std::memcpy(&input_[0], &input, sizeof(Mask4Type));

	IntType result_[4];
	result_[0] = ~input_[0];
	result_[1] = ~input_[1];
	result_[2] = ~input_[2];
	result_[3] = ~input_[3];

	Mask4Type result;
	std::memcpy(&result, &result_[0], sizeof(Mask4Type));

	return result;
}

template<typename MaskType, typename IntType>
static void test_mask_impl()
{
	{
		const MaskType mask = mask_set(IntType(0), ~IntType(0), IntType(0), ~IntType(0));
		CHECK(mask_get_x(mask) == IntType(0));
		CHECK(mask_get_y(mask) == ~IntType(0));
		CHECK(mask_get_z(mask) == IntType(0));
		CHECK(mask_get_w(mask) == ~IntType(0));
	}

	{
		const MaskType mask = mask_set(false, true, false, true);
		CHECK(mask_get_x(mask) == IntType(0));
		CHECK(mask_get_y(mask) == ~IntType(0));
		CHECK(mask_get_z(mask) == IntType(0));
		CHECK(mask_get_w(mask) == ~IntType(0));
	}

	{
		MaskType mask = mask_true();
		CHECK(mask_get_x(mask) == ~IntType(0));
		CHECK(mask_get_y(mask) == ~IntType(0));
		CHECK(mask_get_z(mask) == ~IntType(0));
		CHECK(mask_get_w(mask) == ~IntType(0));

		mask = mask_false();
		CHECK(mask_get_x(mask) == IntType(0));
		CHECK(mask_get_y(mask) == IntType(0));
		CHECK(mask_get_z(mask) == IntType(0));
		CHECK(mask_get_w(mask) == IntType(0));
	}

	{
		const MaskType mask0 = mask_set(IntType(0), ~IntType(0), IntType(0), ~IntType(0));
		const MaskType mask1 = mask_set(IntType(0), IntType(0), IntType(0), IntType(0));
		const MaskType mask2 = mask_set(IntType(0), IntType(0), IntType(0), ~IntType(0));
		const MaskType mask3 = mask_set(IntType(0), IntType(0), ~IntType(0), ~IntType(0));
		const MaskType mask4 = mask_set(IntType(0), ~IntType(0), ~IntType(0), ~IntType(0));
		const MaskType mask5 = mask_set(~IntType(0), ~IntType(0), ~IntType(0), ~IntType(0));
		const MaskType mask6 = mask_set(~IntType(0), IntType(0), IntType(0), IntType(0));
		const MaskType mask7 = mask_set(~IntType(0), ~IntType(0), IntType(0), IntType(0));
		const MaskType mask8 = mask_set(~IntType(0), ~IntType(0), ~IntType(0), IntType(0));

		CHECK(mask_all_true(mask0) == false);
		CHECK(mask_all_true(mask1) == false);
		CHECK(mask_all_true(mask2) == false);
		CHECK(mask_all_true(mask3) == false);
		CHECK(mask_all_true(mask4) == false);
		CHECK(mask_all_true(mask5) == true);
		CHECK(mask_all_true(mask6) == false);
		CHECK(mask_all_true(mask7) == false);
		CHECK(mask_all_true(mask8) == false);

		CHECK(mask_all_true2(mask0) == false);
		CHECK(mask_all_true2(mask1) == false);
		CHECK(mask_all_true2(mask2) == false);
		CHECK(mask_all_true2(mask3) == false);
		CHECK(mask_all_true2(mask4) == false);
		CHECK(mask_all_true2(mask5) == true);
		CHECK(mask_all_true2(mask6) == false);
		CHECK(mask_all_true2(mask7) == true);
		CHECK(mask_all_true2(mask8) == true);

		CHECK(mask_all_true3(mask0) == false);
		CHECK(mask_all_true3(mask1) == false);
		CHECK(mask_all_true3(mask2) == false);
		CHECK(mask_all_true3(mask3) == false);
		CHECK(mask_all_true3(mask4) == false);
		CHECK(mask_all_true3(mask5) == true);
		CHECK(mask_all_true3(mask6) == false);
		CHECK(mask_all_true3(mask7) == false);
		CHECK(mask_all_true3(mask8) == true);

		CHECK(mask_any_true(mask0) == true);
		CHECK(mask_any_true(mask1) == false);
		CHECK(mask_any_true(mask2) == true);
		CHECK(mask_any_true(mask3) == true);
		CHECK(mask_any_true(mask4) == true);
		CHECK(mask_any_true(mask5) == true);
		CHECK(mask_any_true(mask6) == true);
		CHECK(mask_any_true(mask7) == true);
		CHECK(mask_any_true(mask8) == true);

		CHECK(mask_any_true2(mask0) == true);
		CHECK(mask_any_true2(mask1) == false);
		CHECK(mask_any_true2(mask2) == false);
		CHECK(mask_any_true2(mask3) == false);
		CHECK(mask_any_true2(mask4) == true);
		CHECK(mask_any_true2(mask5) == true);
		CHECK(mask_any_true2(mask6) == true);
		CHECK(mask_any_true2(mask7) == true);
		CHECK(mask_any_true2(mask8) == true);

		CHECK(mask_any_true3(mask0) == true);
		CHECK(mask_any_true3(mask1) == false);
		CHECK(mask_any_true3(mask2) == false);
		CHECK(mask_any_true3(mask3) == true);
		CHECK(mask_any_true3(mask4) == true);
		CHECK(mask_any_true3(mask5) == true);
		CHECK(mask_any_true3(mask6) == true);
		CHECK(mask_any_true3(mask7) == true);
		CHECK(mask_any_true3(mask8) == true);
	}

	{
		const MaskType all_true = mask_set(true, true, true, true);
		const MaskType all_false = mask_set(false, false, false, false);

		CHECK(!mask_all_equal2(all_true, mask_set(false, false, false, false)));
		CHECK(!mask_all_equal2(all_true, mask_set(true, false, false, false)));
		CHECK(mask_all_equal2(all_true, mask_set(true, true, false, false)));
		CHECK(mask_all_equal2(all_true, mask_set(true, true, true, false)));
		CHECK(mask_all_equal2(all_true, mask_set(true, true, true, true)));

		CHECK(mask_all_equal2(all_false, mask_set(false, false, false, false)));
		CHECK(mask_all_equal2(all_false, mask_set(false, false, false, true)));
		CHECK(mask_all_equal2(all_false, mask_set(false, false, true, true)));
		CHECK(!mask_all_equal2(all_false, mask_set(false, true, true, true)));
		CHECK(!mask_all_equal2(all_false, mask_set(true, true, true, true)));

		CHECK(!mask_all_equal3(all_true, mask_set(false, false, false, false)));
		CHECK(!mask_all_equal3(all_true, mask_set(true, false, false, false)));
		CHECK(!mask_all_equal3(all_true, mask_set(true, true, false, false)));
		CHECK(mask_all_equal3(all_true, mask_set(true, true, true, false)));
		CHECK(mask_all_equal3(all_true, mask_set(true, true, true, true)));

		CHECK(mask_all_equal3(all_false, mask_set(false, false, false, false)));
		CHECK(mask_all_equal3(all_false, mask_set(false, false, false, true)));
		CHECK(!mask_all_equal3(all_false, mask_set(false, false, true, true)));
		CHECK(!mask_all_equal3(all_false, mask_set(false, true, true, true)));
		CHECK(!mask_all_equal3(all_false, mask_set(true, true, true, true)));

		CHECK(!mask_all_equal(all_true, mask_set(false, false, false, false)));
		CHECK(!mask_all_equal(all_true, mask_set(true, false, false, false)));
		CHECK(!mask_all_equal(all_true, mask_set(true, true, false, false)));
		CHECK(!mask_all_equal(all_true, mask_set(true, true, true, false)));
		CHECK(mask_all_equal(all_true, mask_set(true, true, true, true)));

		CHECK(mask_all_equal(all_false, mask_set(false, false, false, false)));
		CHECK(!mask_all_equal(all_false, mask_set(false, false, false, true)));
		CHECK(!mask_all_equal(all_false, mask_set(false, false, true, true)));
		CHECK(!mask_all_equal(all_false, mask_set(false, true, true, true)));
		CHECK(!mask_all_equal(all_false, mask_set(true, true, true, true)));

		CHECK(!mask_any_equal2(all_true, mask_set(false, false, false, false)));
		CHECK(mask_any_equal2(all_true, mask_set(true, false, false, false)));
		CHECK(mask_any_equal2(all_true, mask_set(false, true, false, false)));
		CHECK(mask_any_equal2(all_true, mask_set(true, true, true, false)));
		CHECK(mask_any_equal2(all_true, mask_set(true, true, true, true)));

		CHECK(mask_any_equal2(all_false, mask_set(false, false, false, false)));
		CHECK(mask_any_equal2(all_false, mask_set(false, false, false, true)));
		CHECK(mask_any_equal2(all_false, mask_set(true, false, true, true)));
		CHECK(mask_any_equal2(all_false, mask_set(false, true, true, true)));
		CHECK(!mask_any_equal2(all_false, mask_set(true, true, true, true)));

		CHECK(!mask_any_equal3(all_true, mask_set(false, false, false, false)));
		CHECK(mask_any_equal3(all_true, mask_set(true, false, false, false)));
		CHECK(mask_any_equal3(all_true, mask_set(true, true, false, false)));
		CHECK(mask_any_equal3(all_true, mask_set(true, true, true, false)));
		CHECK(mask_any_equal3(all_true, mask_set(true, true, true, true)));

		CHECK(mask_any_equal3(all_false, mask_set(false, false, false, false)));
		CHECK(mask_any_equal3(all_false, mask_set(false, false, false, true)));
		CHECK(mask_any_equal3(all_false, mask_set(false, false, true, true)));
		CHECK(mask_any_equal3(all_false, mask_set(false, true, true, true)));
		CHECK(!mask_any_equal3(all_false, mask_set(true, true, true, true)));

		CHECK(!mask_any_equal(all_true, mask_set(false, false, false, false)));
		CHECK(mask_any_equal(all_true, mask_set(true, false, false, false)));
		CHECK(mask_any_equal(all_true, mask_set(true, true, false, false)));
		CHECK(mask_any_equal(all_true, mask_set(true, true, true, false)));
		CHECK(mask_any_equal(all_true, mask_set(true, true, true, true)));

		CHECK(mask_any_equal(all_false, mask_set(false, false, false, false)));
		CHECK(mask_any_equal(all_false, mask_set(false, false, false, true)));
		CHECK(mask_any_equal(all_false, mask_set(false, false, true, true)));
		CHECK(mask_any_equal(all_false, mask_set(false, true, true, true)));
		CHECK(!mask_any_equal(all_false, mask_set(true, true, true, true)));
	}

	{
		const MaskType mask0 = mask_set(true, false, true, false);
		const MaskType mask1 = mask_set(true, true, false, false);
		const MaskType mask2 = mask_set(false, true, false, true);

		CHECK(mask_all_equal(mask_and(mask0, mask1), reference_mask_and<IntType>(mask0, mask1)));
		CHECK(mask_all_equal(mask_and(mask0, mask2), reference_mask_and<IntType>(mask0, mask2)));
		CHECK(mask_all_equal(mask_and(mask1, mask2), reference_mask_and<IntType>(mask1, mask2)));

		CHECK(mask_all_equal(mask_or(mask0, mask1), reference_mask_or<IntType>(mask0, mask1)));
		CHECK(mask_all_equal(mask_or(mask0, mask2), reference_mask_or<IntType>(mask0, mask2)));
		CHECK(mask_all_equal(mask_or(mask1, mask2), reference_mask_or<IntType>(mask1, mask2)));

		CHECK(mask_all_equal(mask_xor(mask0, mask1), reference_mask_xor<IntType>(mask0, mask1)));
		CHECK(mask_all_equal(mask_xor(mask0, mask2), reference_mask_xor<IntType>(mask0, mask2)));
		CHECK(mask_all_equal(mask_xor(mask1, mask2), reference_mask_xor<IntType>(mask1, mask2)));

		CHECK(mask_all_equal(mask_not(mask0), reference_mask_not<IntType>(mask0)));
		CHECK(mask_all_equal(mask_not(mask1), reference_mask_not<IntType>(mask1)));
		CHECK(mask_all_equal(mask_not(mask2), reference_mask_not<IntType>(mask2)));
	}
}

TEST_CASE("mask4f math", "[math][mask]")
{
	test_mask_impl<mask4f, uint32_t>();
}

TEST_CASE("mask4d math", "[math][mask]")
{
	test_mask_impl<mask4d, uint64_t>();
}

TEST_CASE("mask4i math", "[math][mask]")
{
	test_mask_impl<mask4i, uint32_t>();
}

TEST_CASE("mask4q math", "[math][mask]")
{
	test_mask_impl<mask4q, uint64_t>();
}

```

`tests/sources/test_matrix3x3_impl.h`:

```h
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2018 Nicholas Frechette & Animation Compression Library contributors
// Copyright (c) 2018 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include "catch2.impl.h"

#include <rtm/type_traits.h>
#include <rtm/matrix3x3f.h>
#include <rtm/matrix3x3d.h>
#include <rtm/matrix3x4f.h>
#include <rtm/matrix3x4d.h>

using namespace rtm;

template<typename FloatType>
static void test_matrix3x3_setters(const FloatType threshold)
{
	using QuatType = typename related_types<FloatType>::quat;
	using Vector4Type = typename related_types<FloatType>::vector4;
	using Matrix3x3Type = typename related_types<FloatType>::matrix3x3;

	const Matrix3x3Type identity = matrix_identity();

	{
		Vector4Type x_axis = vector_set(FloatType(1.0), FloatType(2.0), FloatType(3.0), FloatType(0.0));
		Vector4Type y_axis = vector_set(FloatType(4.0), FloatType(5.0), FloatType(6.0), FloatType(0.0));
		Vector4Type z_axis = vector_set(FloatType(7.0), FloatType(8.0), FloatType(9.0), FloatType(0.0));
		Matrix3x3Type mtx = matrix_set(x_axis, y_axis, z_axis);
		CHECK(vector_all_near_equal3(x_axis, mtx.x_axis, threshold));
		CHECK(vector_all_near_equal3(y_axis, mtx.y_axis, threshold));
		CHECK(vector_all_near_equal3(z_axis, mtx.z_axis, threshold));
	}

	{
		CHECK(vector_all_near_equal3(vector_set(FloatType(1.0), FloatType(0.0), FloatType(0.0)), identity.x_axis, threshold));
		CHECK(vector_all_near_equal3(vector_set(FloatType(0.0), FloatType(1.0), FloatType(0.0)), identity.y_axis, threshold));
		CHECK(vector_all_near_equal3(vector_set(FloatType(0.0), FloatType(0.0), FloatType(1.0)), identity.z_axis, threshold));
	}

	{
		QuatType rotation_around_z = quat_from_euler(scalar_deg_to_rad(FloatType(0.0)), scalar_deg_to_rad(FloatType(90.0)), scalar_deg_to_rad(FloatType(0.0)));
		Matrix3x3Type mtx = matrix_from_quat(rotation_around_z);
		CHECK(vector_all_near_equal3(vector_set(FloatType(0.0), FloatType(1.0), FloatType(0.0)), mtx.x_axis, threshold));
		CHECK(vector_all_near_equal3(vector_set(FloatType(-1.0), FloatType(0.0), FloatType(0.0)), mtx.y_axis, threshold));
		CHECK(vector_all_near_equal3(vector_set(FloatType(0.0), FloatType(0.0), FloatType(1.0)), mtx.z_axis, threshold));
	}

	{
		QuatType rotation_around_z = quat_from_euler(scalar_deg_to_rad(FloatType(0.0)), scalar_deg_to_rad(FloatType(90.0)), scalar_deg_to_rad(FloatType(0.0)));
		Matrix3x3Type mtx = matrix_from_rotation(rotation_around_z);
		CHECK(vector_all_near_equal3(vector_set(FloatType(0.0), FloatType(1.0), FloatType(0.0)), mtx.x_axis, threshold));
		CHECK(vector_all_near_equal3(vector_set(FloatType(-1.0), FloatType(0.0), FloatType(0.0)), mtx.y_axis, threshold));
		CHECK(vector_all_near_equal3(vector_set(FloatType(0.0), FloatType(0.0), FloatType(1.0)), mtx.z_axis, threshold));
	}

	{
		Matrix3x3Type mtx = matrix_from_scale(vector_set(FloatType(4.0), FloatType(5.0), FloatType(6.0)));
		CHECK(vector_all_near_equal3(vector_set(FloatType(4.0), FloatType(0.0), FloatType(0.0)), mtx.x_axis, threshold));
		CHECK(vector_all_near_equal3(vector_set(FloatType(0.0), FloatType(5.0), FloatType(0.0)), mtx.y_axis, threshold));
		CHECK(vector_all_near_equal3(vector_set(FloatType(0.0), FloatType(0.0), FloatType(6.0)), mtx.z_axis, threshold));
	}

	{
		QuatType rotation_around_z = quat_from_euler(scalar_deg_to_rad(FloatType(0.0)), scalar_deg_to_rad(FloatType(90.0)), scalar_deg_to_rad(FloatType(0.0)));
		Matrix3x3Type mtx = matrix_from_rotation(rotation_around_z);
		CHECK(vector_all_near_equal3(matrix_get_axis(mtx, axis3::x), mtx.x_axis, threshold));
		CHECK(vector_all_near_equal3(matrix_get_axis(mtx, axis3::y), mtx.y_axis, threshold));
		CHECK(vector_all_near_equal3(matrix_get_axis(mtx, axis3::z), mtx.z_axis, threshold));
	}

	{
		QuatType rotation_around_z = quat_from_euler(scalar_deg_to_rad(FloatType(0.0)), scalar_deg_to_rad(FloatType(90.0)), scalar_deg_to_rad(FloatType(0.0)));
		Matrix3x3Type mtx = matrix_from_rotation(rotation_around_z);

		Matrix3x3Type mtx2 = identity;
		mtx2 = matrix_set_axis(mtx2, mtx.x_axis, axis3::x);
		mtx2 = matrix_set_axis(mtx2, mtx.y_axis, axis3::y);
		mtx2 = matrix_set_axis(mtx2, mtx.z_axis, axis3::z);
		CHECK(vector_all_near_equal3(mtx2.x_axis, mtx.x_axis, threshold));
		CHECK(vector_all_near_equal3(mtx2.y_axis, mtx.y_axis, threshold));
		CHECK(vector_all_near_equal3(mtx2.z_axis, mtx.z_axis, threshold));
	}

	{
		Vector4Type x_axis = vector_set(FloatType(1.0), FloatType(2.0), FloatType(3.0), FloatType(0.0));
		Vector4Type y_axis = vector_set(FloatType(4.0), FloatType(5.0), FloatType(6.0), FloatType(0.0));
		Vector4Type z_axis = vector_set(FloatType(7.0), FloatType(8.0), FloatType(9.0), FloatType(0.0));
		Matrix3x3Type mtx = matrix_set(x_axis, y_axis, z_axis);
		CHECK(FloatType(matrix_get_component(mtx, axis3::x, component3::x)) == FloatType(1.0));
		CHECK(FloatType(matrix_get_component(mtx, axis3::x, component3::y)) == FloatType(2.0));
		CHECK(FloatType(matrix_get_component(mtx, axis3::x, component3::z)) == FloatType(3.0));
		CHECK(FloatType(matrix_get_component(mtx, axis3::y, component3::x)) == FloatType(4.0));
		CHECK(FloatType(matrix_get_component(mtx, axis3::y, component3::y)) == FloatType(5.0));
		CHECK(FloatType(matrix_get_component(mtx, axis3::y, component3::z)) == FloatType(6.0));
		CHECK(FloatType(matrix_get_component(mtx, axis3::z, component3::x)) == FloatType(7.0));
		CHECK(FloatType(matrix_get_component(mtx, axis3::z, component3::y)) == FloatType(8.0));
		CHECK(FloatType(matrix_get_component(mtx, axis3::z, component3::z)) == FloatType(9.0));

		CHECK(scalar_cast(matrix_get_component_as_scalar(mtx, axis3::x, component3::x)) == FloatType(1.0));
		CHECK(scalar_cast(matrix_get_component_as_scalar(mtx, axis3::x, component3::y)) == FloatType(2.0));
		CHECK(scalar_cast(matrix_get_component_as_scalar(mtx, axis3::x, component3::z)) == FloatType(3.0));
		CHECK(scalar_cast(matrix_get_component_as_scalar(mtx, axis3::y, component3::x)) == FloatType(4.0));
		CHECK(scalar_cast(matrix_get_component_as_scalar(mtx, axis3::y, component3::y)) == FloatType(5.0));
		CHECK(scalar_cast(matrix_get_component_as_scalar(mtx, axis3::y, component3::z)) == FloatType(6.0));
		CHECK(scalar_cast(matrix_get_component_as_scalar(mtx, axis3::z, component3::x)) == FloatType(7.0));
		CHECK(scalar_cast(matrix_get_component_as_scalar(mtx, axis3::z, component3::y)) == FloatType(8.0));
		CHECK(scalar_cast(matrix_get_component_as_scalar(mtx, axis3::z, component3::z)) == FloatType(9.0));
	}

	{
		Vector4Type x_axis = vector_set(FloatType(1.0), FloatType(2.0), FloatType(3.0), FloatType(0.0));
		Vector4Type y_axis = vector_set(FloatType(4.0), FloatType(5.0), FloatType(6.0), FloatType(0.0));
		Vector4Type z_axis = vector_set(FloatType(7.0), FloatType(8.0), FloatType(9.0), FloatType(0.0));
		Matrix3x3Type mtx = matrix_set(x_axis, y_axis, z_axis);

		Matrix3x3Type mtx2 = identity;
		mtx2 = matrix_set_component(mtx2, FloatType(1.0), axis3::x, component3::x);
		mtx2 = matrix_set_component(mtx2, FloatType(2.0), axis3::x, component3::y);
		mtx2 = matrix_set_component(mtx2, FloatType(3.0), axis3::x, component3::z);
		mtx2 = matrix_set_component(mtx2, FloatType(4.0), axis3::y, component3::x);
		mtx2 = matrix_set_component(mtx2, FloatType(5.0), axis3::y, component3::y);
		mtx2 = matrix_set_component(mtx2, FloatType(6.0), axis3::y, component3::z);
		mtx2 = matrix_set_component(mtx2, FloatType(7.0), axis3::z, component3::x);
		mtx2 = matrix_set_component(mtx2, FloatType(8.0), axis3::z, component3::y);
		mtx2 = matrix_set_component(mtx2, FloatType(9.0), axis3::z, component3::z);
		CHECK(vector_all_near_equal3(mtx2.x_axis, mtx.x_axis, threshold));
		CHECK(vector_all_near_equal3(mtx2.y_axis, mtx.y_axis, threshold));
		CHECK(vector_all_near_equal3(mtx2.z_axis, mtx.z_axis, threshold));

		mtx2 = identity;
		mtx2 = matrix_set_component(mtx2, scalar_set(FloatType(1.0)), axis3::x, component3::x);
		mtx2 = matrix_set_component(mtx2, scalar_set(FloatType(2.0)), axis3::x, component3::y);
		mtx2 = matrix_set_component(mtx2, scalar_set(FloatType(3.0)), axis3::x, component3::z);
		mtx2 = matrix_set_component(mtx2, scalar_set(FloatType(4.0)), axis3::y, component3::x);
		mtx2 = matrix_set_component(mtx2, scalar_set(FloatType(5.0)), axis3::y, component3::y);
		mtx2 = matrix_set_component(mtx2, scalar_set(FloatType(6.0)), axis3::y, component3::z);
		mtx2 = matrix_set_component(mtx2, scalar_set(FloatType(7.0)), axis3::z, component3::x);
		mtx2 = matrix_set_component(mtx2, scalar_set(FloatType(8.0)), axis3::z, component3::y);
		mtx2 = matrix_set_component(mtx2, scalar_set(FloatType(9.0)), axis3::z, component3::z);
		CHECK(vector_all_near_equal3(mtx2.x_axis, mtx.x_axis, threshold));
		CHECK(vector_all_near_equal3(mtx2.y_axis, mtx.y_axis, threshold));
		CHECK(vector_all_near_equal3(mtx2.z_axis, mtx.z_axis, threshold));
	}
}

template<typename FloatType>
static void test_matrix3x3_arithmetic(const FloatType threshold)
{
	using QuatType = typename related_types<FloatType>::quat;
	using Vector4Type = typename related_types<FloatType>::vector4;
	using Matrix3x3Type = typename related_types<FloatType>::matrix3x3;

	{
		Vector4Type x_axis = vector_set(FloatType(1.0), FloatType(0.0), FloatType(0.0));
		Vector4Type y_axis = vector_set(FloatType(0.0), FloatType(1.0), FloatType(0.0));

		QuatType rotation_around_z = quat_from_euler(scalar_deg_to_rad(FloatType(0.0)), scalar_deg_to_rad(FloatType(90.0)), scalar_deg_to_rad(FloatType(0.0)));
		Matrix3x3Type mtx_a = matrix_from_quat(rotation_around_z);
		Vector4Type result = matrix_mul_vector3(x_axis, mtx_a);
		CHECK(vector_all_near_equal3(result, vector_set(FloatType(0.0), FloatType(1.0), FloatType(0.0)), threshold));
		result = matrix_mul_vector3(y_axis, mtx_a);
		CHECK(vector_all_near_equal3(result, vector_set(FloatType(-1.0), FloatType(0.0), FloatType(0.0)), threshold));

		QuatType rotation_around_x = quat_from_euler(scalar_deg_to_rad(FloatType(0.0)), scalar_deg_to_rad(FloatType(0.0)), scalar_deg_to_rad(FloatType(90.0)));
		Matrix3x3Type mtx_b = matrix_from_quat(rotation_around_x);
		result = matrix_mul_vector3(x_axis, mtx_b);
		CHECK(vector_all_near_equal3(result, vector_set(FloatType(1.0), FloatType(0.0), FloatType(0.0)), threshold));
		result = matrix_mul_vector3(y_axis, mtx_b);
		CHECK(vector_all_near_equal3(result, vector_set(FloatType(0.0), FloatType(0.0), FloatType(-1.0)), threshold));

		Matrix3x3Type mtx_ab = matrix_mul(mtx_a, mtx_b);
		Matrix3x3Type mtx_ba = matrix_mul(mtx_b, mtx_a);
		result = matrix_mul_vector3(x_axis, mtx_ab);
		CHECK(vector_all_near_equal3(result, vector_set(FloatType(0.0), FloatType(0.0), FloatType(-1.0)), threshold));
		CHECK(vector_all_near_equal3(result, matrix_mul_vector3(matrix_mul_vector3(x_axis, mtx_a), mtx_b), threshold));
		result = matrix_mul_vector3(y_axis, mtx_ab);
		CHECK(vector_all_near_equal3(result, vector_set(FloatType(-1.0), FloatType(0.0), FloatType(0.0)), threshold));
		CHECK(vector_all_near_equal3(result, matrix_mul_vector3(matrix_mul_vector3(y_axis, mtx_a), mtx_b), threshold));
		result = matrix_mul_vector3(x_axis, mtx_ba);
		CHECK(vector_all_near_equal3(result, vector_set(FloatType(0.0), FloatType(1.0), FloatType(0.0)), threshold));
		CHECK(vector_all_near_equal3(result, matrix_mul_vector3(matrix_mul_vector3(x_axis, mtx_b), mtx_a), threshold));
		result = matrix_mul_vector3(y_axis, mtx_ba);
		CHECK(vector_all_near_equal3(result, vector_set(FloatType(0.0), FloatType(0.0), FloatType(-1.0)), threshold));
		CHECK(vector_all_near_equal3(result, matrix_mul_vector3(matrix_mul_vector3(y_axis, mtx_b), mtx_a), threshold));
	}
}

template<typename FloatType>
static void test_matrix3x3_transformations(const FloatType threshold)
{
	using QuatType = typename related_types<FloatType>::quat;
	using Vector4Type = typename related_types<FloatType>::vector4;
	using Matrix3x3Type = typename related_types<FloatType>::matrix3x3;

	const Matrix3x3Type identity = matrix_identity();
	const Vector4Type zero = vector_zero();

	{
		Vector4Type x_axis = vector_set(FloatType(1.0), FloatType(2.0), FloatType(3.0), FloatType(0.0));
		Vector4Type y_axis = vector_set(FloatType(4.0), FloatType(5.0), FloatType(6.0), FloatType(0.0));
		Vector4Type z_axis = vector_set(FloatType(7.0), FloatType(8.0), FloatType(9.0), FloatType(0.0));
		Matrix3x3Type mtx0 = matrix_set(x_axis, y_axis, z_axis);
		Matrix3x3Type mtx1 = matrix_transpose(mtx0);
		CHECK(vector_all_near_equal3(vector_set(FloatType(1.0), FloatType(4.0), FloatType(7.0)), mtx1.x_axis, threshold));
		CHECK(vector_all_near_equal3(vector_set(FloatType(2.0), FloatType(5.0), FloatType(8.0)), mtx1.y_axis, threshold));
		CHECK(vector_all_near_equal3(vector_set(FloatType(3.0), FloatType(6.0), FloatType(9.0)), mtx1.z_axis, threshold));
	}

	{
		QuatType rotation_around_z = quat_from_euler(scalar_deg_to_rad(FloatType(0.0)), scalar_deg_to_rad(FloatType(90.0)), scalar_deg_to_rad(FloatType(0.0)));
		Matrix3x3Type mtx = matrix_from_quat(rotation_around_z);
		Matrix3x3Type inv_mtx = matrix_inverse(mtx);
		Matrix3x3Type result = matrix_mul(mtx, inv_mtx);
		CHECK(vector_all_near_equal3(vector_set(FloatType(1.0), FloatType(0.0), FloatType(0.0)), result.x_axis, threshold));
		CHECK(vector_all_near_equal3(vector_set(FloatType(0.0), FloatType(1.0), FloatType(0.0)), result.y_axis, threshold));
		CHECK(vector_all_near_equal3(vector_set(FloatType(0.0), FloatType(0.0), FloatType(1.0)), result.z_axis, threshold));
	}

	{
		QuatType rotation = quat_from_euler(scalar_deg_to_rad(FloatType(12.3)), scalar_deg_to_rad(FloatType(42.8)), scalar_deg_to_rad(FloatType(33.41)));
		Matrix3x3Type mtx = matrix_from_quat(rotation);
		Matrix3x3Type inv_mtx = matrix_inverse(mtx);
		Matrix3x3Type result = matrix_mul(mtx, inv_mtx);
		CHECK(vector_all_near_equal3(vector_set(FloatType(1.0), FloatType(0.0), FloatType(0.0)), result.x_axis, threshold));
		CHECK(vector_all_near_equal3(vector_set(FloatType(0.0), FloatType(1.0), FloatType(0.0)), result.y_axis, threshold));
		CHECK(vector_all_near_equal3(vector_set(FloatType(0.0), FloatType(0.0), FloatType(1.0)), result.z_axis, threshold));
	}

	{
		QuatType rotation = quat_from_euler(scalar_deg_to_rad(FloatType(12.3)), scalar_deg_to_rad(FloatType(42.8)), scalar_deg_to_rad(FloatType(33.41)));
		Matrix3x3Type mtx = matrix_from_quat(rotation);
		Matrix3x3Type inv_mtx = matrix_inverse(mtx, mtx);
		Matrix3x3Type result = matrix_mul(mtx, inv_mtx);
		CHECK(vector_all_near_equal3(vector_set(FloatType(1.0), FloatType(0.0), FloatType(0.0)), result.x_axis, threshold));
		CHECK(vector_all_near_equal3(vector_set(FloatType(0.0), FloatType(1.0), FloatType(0.0)), result.y_axis, threshold));
		CHECK(vector_all_near_equal3(vector_set(FloatType(0.0), FloatType(0.0), FloatType(1.0)), result.z_axis, threshold));
	}

	{
		Matrix3x3Type mtx = matrix_set(zero, zero, zero);
		Matrix3x3Type inv_mtx = matrix_inverse(mtx, identity);
		CHECK(vector_all_near_equal(identity.x_axis, inv_mtx.x_axis, threshold));
		CHECK(vector_all_near_equal(identity.y_axis, inv_mtx.y_axis, threshold));
		CHECK(vector_all_near_equal(identity.z_axis, inv_mtx.z_axis, threshold));
	}
}

template<typename FloatType>
static void test_matrix3x3_misc(const FloatType threshold)
{
	using QuatType = typename related_types<FloatType>::quat;
	using Vector4Type = typename related_types<FloatType>::vector4;
	using Matrix3x3Type = typename related_types<FloatType>::matrix3x3;
	using Matrix3x4Type = typename related_types<FloatType>::matrix3x4;

	{
		QuatType rotation_around_z = quat_from_euler(scalar_deg_to_rad(FloatType(0.0)), scalar_deg_to_rad(FloatType(90.0)), scalar_deg_to_rad(FloatType(0.0)));
		Matrix3x3Type mtx = matrix_from_quat(rotation_around_z);
		QuatType rotation = quat_from_matrix(mtx);
		CHECK(quat_near_equal(rotation_around_z, rotation, threshold));
	}

	{
		QuatType rotation_around_z = quat_from_euler(scalar_deg_to_rad(FloatType(0.0)), scalar_deg_to_rad(FloatType(90.0)), scalar_deg_to_rad(FloatType(0.0)));
		Vector4Type translation = vector_set(FloatType(1.0), FloatType(2.0), FloatType(3.0));
		Matrix3x4Type mtx0_3x4 = matrix_from_qv(rotation_around_z, translation);
		Matrix3x3Type mtx0 = matrix_cast(mtx0_3x4);
		CHECK(vector_all_near_equal3(vector_set(FloatType(0.0), FloatType(1.0), FloatType(0.0)), mtx0.x_axis, threshold));
		CHECK(vector_all_near_equal3(vector_set(FloatType(-1.0), FloatType(0.0), FloatType(0.0)), mtx0.y_axis, threshold));
		CHECK(vector_all_near_equal3(vector_set(FloatType(0.0), FloatType(0.0), FloatType(1.0)), mtx0.z_axis, threshold));
	}

	{
		QuatType rotation_around_z = quat_from_euler(scalar_deg_to_rad(FloatType(0.0)), scalar_deg_to_rad(FloatType(90.0)), scalar_deg_to_rad(FloatType(0.0)));
		Vector4Type translation = vector_set(FloatType(1.0), FloatType(2.0), FloatType(3.0));
		FloatType scale = FloatType(4.0);
		Matrix3x4Type mtx0_3x4 = matrix_from_qvs(rotation_around_z, translation, scale);
		Matrix3x3Type mtx0 = matrix_cast(mtx0_3x4);
		Matrix3x3Type mtx0_no_scale = matrix_remove_scale(mtx0);
		CHECK(vector_all_near_equal3(vector_set(FloatType(0.0), FloatType(1.0), FloatType(0.0)), mtx0_no_scale.x_axis, threshold));
		CHECK(vector_all_near_equal3(vector_set(FloatType(-1.0), FloatType(0.0), FloatType(0.0)), mtx0_no_scale.y_axis, threshold));
		CHECK(vector_all_near_equal3(vector_set(FloatType(0.0), FloatType(0.0), FloatType(1.0)), mtx0_no_scale.z_axis, threshold));
	}

	{
		QuatType rotation_around_z = quat_from_euler(scalar_deg_to_rad(FloatType(0.0)), scalar_deg_to_rad(FloatType(90.0)), scalar_deg_to_rad(FloatType(0.0)));
		Vector4Type translation = vector_set(FloatType(1.0), FloatType(2.0), FloatType(3.0));
		Vector4Type scale = vector_set(FloatType(4.0), FloatType(5.0), FloatType(6.0));
		Matrix3x4Type mtx0_3x4 = matrix_from_qvv(rotation_around_z, translation, scale);
		Matrix3x3Type mtx0 = matrix_cast(mtx0_3x4);
		Matrix3x3Type mtx0_no_scale = matrix_remove_scale(mtx0);
		CHECK(vector_all_near_equal3(vector_set(FloatType(0.0), FloatType(1.0), FloatType(0.0)), mtx0_no_scale.x_axis, threshold));
		CHECK(vector_all_near_equal3(vector_set(FloatType(-1.0), FloatType(0.0), FloatType(0.0)), mtx0_no_scale.y_axis, threshold));
		CHECK(vector_all_near_equal3(vector_set(FloatType(0.0), FloatType(0.0), FloatType(1.0)), mtx0_no_scale.z_axis, threshold));
	}

	{
		Vector4Type x_axis = vector_set(FloatType(5.62565), FloatType(7.90751), FloatType(4.37048), FloatType(99999.9999));
		Vector4Type y_axis = vector_set(FloatType(0.36345), FloatType(7.87300), FloatType(7.23000), FloatType(99999.9999));
		Vector4Type z_axis = vector_set(FloatType(8.06413), FloatType(3.91970), FloatType(8.48928), FloatType(99999.9999));
		Matrix3x3Type mtx = matrix_set(x_axis, y_axis, z_axis);
		FloatType det = scalar_cast(matrix_determinant(mtx));
		CHECK(scalar_near_equal(det, FloatType(381.95681179092484), threshold));
	}

	{
		Vector4Type x_axis = vector_set(FloatType(5.62565), FloatType(7.90751), FloatType(4.37048), FloatType(99999.9999));
		Vector4Type y_axis = vector_set(FloatType(0.36345), FloatType(7.87300), FloatType(7.23000), FloatType(99999.9999));
		Vector4Type z_axis = vector_set(FloatType(8.06413), FloatType(3.91970), FloatType(8.48928), FloatType(99999.9999));
		Matrix3x3Type mtx = matrix_set(x_axis, y_axis, z_axis);

		FloatType det_xx = scalar_cast(matrix_minor(mtx, axis3::x, axis3::x));
		CHECK(scalar_near_equal(det_xx, FloatType(38.4967), threshold));
		FloatType det_xy = scalar_cast(matrix_minor(mtx, axis3::x, axis3::y));
		CHECK(scalar_near_equal(det_xy, FloatType(-55.2182), threshold));
		FloatType det_xz = scalar_cast(matrix_minor(mtx, axis3::x, axis3::z));
		CHECK(scalar_near_equal(det_xz, FloatType(-62.0643), threshold));

		FloatType det_yx = scalar_cast(matrix_minor(mtx, axis3::y, axis3::x));
		CHECK(scalar_near_equal(det_yx, FloatType(49.9981), threshold));
		FloatType det_yy = scalar_cast(matrix_minor(mtx, axis3::y, axis3::y));
		CHECK(scalar_near_equal(det_yy, FloatType(12.5136), threshold));
		FloatType det_yz = scalar_cast(matrix_minor(mtx, axis3::y, axis3::z));
		CHECK(scalar_near_equal(det_yz, FloatType(-41.7163), threshold));

		FloatType det_zx = scalar_cast(matrix_minor(mtx, axis3::z, axis3::x));
		CHECK(scalar_near_equal(det_zx, FloatType(22.7625), threshold));
		FloatType det_zy = scalar_cast(matrix_minor(mtx, axis3::z, axis3::y));
		CHECK(scalar_near_equal(det_zy, FloatType(39.085), threshold));
		FloatType det_zz = scalar_cast(matrix_minor(mtx, axis3::z, axis3::z));
		CHECK(scalar_near_equal(det_zz, FloatType(41.4168), threshold));
	}

	{
		Vector4Type x_axis = vector_set(FloatType(5.62565), FloatType(7.90751), FloatType(4.37048), FloatType(99999.9999));
		Vector4Type y_axis = vector_set(FloatType(0.36345), FloatType(7.87300), FloatType(7.23000), FloatType(99999.9999));
		Vector4Type z_axis = vector_set(FloatType(8.06413), FloatType(3.91970), FloatType(8.48928), FloatType(99999.9999));
		Matrix3x4Type mtx = matrix_set(x_axis, y_axis, z_axis, x_axis);
		Matrix3x3Type cof = matrix_cofactor(mtx);

		FloatType cof_xx = scalar_cast(vector_get_x_as_scalar(cof.x_axis));
		CHECK(scalar_near_equal(cof_xx, FloatType(38.4967), threshold));
		FloatType cof_xy = scalar_cast(vector_get_y_as_scalar(cof.x_axis));
		CHECK(scalar_near_equal(cof_xy, FloatType(55.2182), threshold));
		FloatType cof_xz = scalar_cast(vector_get_z_as_scalar(cof.x_axis));
		CHECK(scalar_near_equal(cof_xz, FloatType(-62.0643), threshold));

		FloatType cof_yx = scalar_cast(vector_get_x_as_scalar(cof.y_axis));
		CHECK(scalar_near_equal(cof_yx, FloatType(-49.9981), threshold));
		FloatType cof_yy = scalar_cast(vector_get_y_as_scalar(cof.y_axis));
		CHECK(scalar_near_equal(cof_yy, FloatType(12.5136), threshold));
		FloatType cof_yz = scalar_cast(vector_get_z_as_scalar(cof.y_axis));
		CHECK(scalar_near_equal(cof_yz, FloatType(41.7163), threshold));

		FloatType cof_zx = scalar_cast(vector_get_x_as_scalar(cof.z_axis));
		CHECK(scalar_near_equal(cof_zx, FloatType(22.7625), threshold));
		FloatType cof_zy = scalar_cast(vector_get_y_as_scalar(cof.z_axis));
		CHECK(scalar_near_equal(cof_zy, FloatType(-39.085), threshold));
		FloatType cof_zz = scalar_cast(vector_get_z_as_scalar(cof.z_axis));
		CHECK(scalar_near_equal(cof_zz, FloatType(41.4168), threshold));
	}

	{
		Vector4Type x_axis = vector_set(FloatType(5.62565), FloatType(7.90751), FloatType(4.37048), FloatType(99999.9999));
		Vector4Type y_axis = vector_set(FloatType(0.36345), FloatType(7.87300), FloatType(7.23000), FloatType(99999.9999));
		Vector4Type z_axis = vector_set(FloatType(8.06413), FloatType(3.91970), FloatType(8.48928), FloatType(99999.9999));
		Matrix3x4Type mtx = matrix_set(x_axis, y_axis, z_axis, x_axis);
		Matrix3x3Type adj = matrix_adjugate(mtx);

		FloatType adj_xx = scalar_cast(vector_get_x_as_scalar(adj.x_axis));
		CHECK(scalar_near_equal(adj_xx, FloatType(38.4967), threshold));
		FloatType adj_xy = scalar_cast(vector_get_y_as_scalar(adj.x_axis));
		CHECK(scalar_near_equal(adj_xy, FloatType(-49.9981), threshold));
		FloatType adj_xz = scalar_cast(vector_get_z_as_scalar(adj.x_axis));
		CHECK(scalar_near_equal(adj_xz, FloatType(22.7625), threshold));

		FloatType adj_yx = scalar_cast(vector_get_x_as_scalar(adj.y_axis));
		CHECK(scalar_near_equal(adj_yx, FloatType(55.2182), threshold));
		FloatType adj_yy = scalar_cast(vector_get_y_as_scalar(adj.y_axis));
		CHECK(scalar_near_equal(adj_yy, FloatType(12.5136), threshold));
		FloatType adj_yz = scalar_cast(vector_get_z_as_scalar(adj.y_axis));
		CHECK(scalar_near_equal(adj_yz, FloatType(-39.085), threshold));

		FloatType adj_zx = scalar_cast(vector_get_x_as_scalar(adj.z_axis));
		CHECK(scalar_near_equal(adj_zx, FloatType(-62.0643), threshold));
		FloatType adj_zy = scalar_cast(vector_get_y_as_scalar(adj.z_axis));
		CHECK(scalar_near_equal(adj_zy, FloatType(41.7163), threshold));
		FloatType adj_zz = scalar_cast(vector_get_z_as_scalar(adj.z_axis));
		CHECK(scalar_near_equal(adj_zz, FloatType(41.4168), threshold));
	}
}

```

`tests/sources/test_matrix3x3d.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include "test_matrix3x3_impl.h"

TEST_CASE("matrix3x3d math get/set", "[math][matrix3x3]")
{
	test_matrix3x3_setters<double>(1.0E-4);
}

TEST_CASE("matrix3x3d math arithmetic", "[math][matrix3x3]")
{
	test_matrix3x3_arithmetic<double>(1.0E-4);
}

TEST_CASE("matrix3x3d math transformations", "[math][matrix3x3]")
{
	test_matrix3x3_transformations<double>(1.0E-4);
}

TEST_CASE("matrix3x3d math misc", "[math][matrix3x3]")
{
	test_matrix3x3_misc<double>(1.0E-4);

	{
		quatd rotation_around_z = quat_from_euler(scalar_deg_to_rad(0.0), scalar_deg_to_rad(90.0), scalar_deg_to_rad(0.0));
		matrix3x3d src = matrix_from_quat(rotation_around_z);
		matrix3x3f dst = matrix_cast(src);
		CHECK(vector_all_near_equal3(vector_cast(src.x_axis), dst.x_axis, 1.0E-4F));
		CHECK(vector_all_near_equal3(vector_cast(src.y_axis), dst.y_axis, 1.0E-4F));
		CHECK(vector_all_near_equal3(vector_cast(src.z_axis), dst.z_axis, 1.0E-4F));
	}
}

```

`tests/sources/test_matrix3x3f.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include "test_matrix3x3_impl.h"

TEST_CASE("matrix3x3f math get/set", "[math][matrix3x3]")
{
	test_matrix3x3_setters<float>(1.0E-4F);
}

TEST_CASE("matrix3x3f math arithmetic", "[math][matrix3x3]")
{
	test_matrix3x3_arithmetic<float>(1.0E-4F);
}

TEST_CASE("matrix3x3f math transformations", "[math][matrix3x3]")
{
	test_matrix3x3_transformations<float>(1.0E-4F);
}

TEST_CASE("matrix3x3f math misc", "[math][matrix3x3]")
{
	test_matrix3x3_misc<float>(1.0E-4F);

	{
		quatf rotation_around_z = quat_from_euler(scalar_deg_to_rad(0.0F), scalar_deg_to_rad(90.0F), scalar_deg_to_rad(0.0F));
		matrix3x3f src = matrix_from_quat(rotation_around_z);

		// Work around clang5 compiler crash by marking this variable as static
		static matrix3x3d dst = matrix_cast(src);
		CHECK(vector_all_near_equal3(vector_cast(src.x_axis), dst.x_axis, 1.0E-4));
		CHECK(vector_all_near_equal3(vector_cast(src.y_axis), dst.y_axis, 1.0E-4));
		CHECK(vector_all_near_equal3(vector_cast(src.z_axis), dst.z_axis, 1.0E-4));
	}
}

```

`tests/sources/test_matrix3x4.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2018 Nicholas Frechette & Animation Compression Library contributors
// Copyright (c) 2018 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include "catch2.impl.h"

#include <rtm/type_traits.h>
#include <rtm/matrix3x4f.h>
#include <rtm/matrix3x4d.h>
#include <rtm/qvf.h>
#include <rtm/qvd.h>
#include <rtm/qvsf.h>
#include <rtm/qvsd.h>
#include <rtm/qvvf.h>
#include <rtm/qvvd.h>

using namespace rtm;

template<typename FloatType>
static void test_affine_matrix_setters(const FloatType threshold)
{
	using QuatType = typename related_types<FloatType>::quat;
	using Vector4Type = typename related_types<FloatType>::vector4;
	using QVType = typename related_types<FloatType>::qv;
	using QVSType = typename related_types<FloatType>::qvs;
	using QVVType = typename related_types<FloatType>::qvv;
	using Matrix3x3Type = typename related_types<FloatType>::matrix3x3;
	using Matrix3x4Type = typename related_types<FloatType>::matrix3x4;

	const Matrix3x4Type identity = matrix_identity();

	{
		Vector4Type x_axis = vector_set(FloatType(1.0), FloatType(2.0), FloatType(3.0), FloatType(0.0));
		Vector4Type y_axis = vector_set(FloatType(4.0), FloatType(5.0), FloatType(6.0), FloatType(0.0));
		Vector4Type z_axis = vector_set(FloatType(7.0), FloatType(8.0), FloatType(9.0), FloatType(0.0));
		Vector4Type w_axis = vector_set(FloatType(10.0), FloatType(11.0), FloatType(12.0), FloatType(1.0));
		Matrix3x4Type mtx = matrix_set(x_axis, y_axis, z_axis, w_axis);
		CHECK(vector_all_near_equal3(x_axis, mtx.x_axis, threshold));
		CHECK(vector_all_near_equal3(y_axis, mtx.y_axis, threshold));
		CHECK(vector_all_near_equal3(z_axis, mtx.z_axis, threshold));
		CHECK(vector_all_near_equal3(w_axis, mtx.w_axis, threshold));
	}

	{
		CHECK(vector_all_near_equal3(vector_set(FloatType(1.0), FloatType(0.0), FloatType(0.0), FloatType(0.0)), identity.x_axis, threshold));
		CHECK(vector_all_near_equal3(vector_set(FloatType(0.0), FloatType(1.0), FloatType(0.0), FloatType(0.0)), identity.y_axis, threshold));
		CHECK(vector_all_near_equal3(vector_set(FloatType(0.0), FloatType(0.0), FloatType(1.0), FloatType(0.0)), identity.z_axis, threshold));
		CHECK(vector_all_near_equal3(vector_set(FloatType(0.0), FloatType(0.0), FloatType(0.0), FloatType(1.0)), identity.w_axis, threshold));
	}

	{
		QuatType rotation_around_z = quat_from_euler(scalar_deg_to_rad(FloatType(0.0)), scalar_deg_to_rad(FloatType(90.0)), scalar_deg_to_rad(FloatType(0.0)));
		Vector4Type translation = vector_set(FloatType(1.0), FloatType(2.0), FloatType(3.0));
		Matrix3x4Type mtx = matrix_from_qv(rotation_around_z, translation);
		CHECK(vector_all_near_equal3(vector_set(FloatType(0.0), FloatType(1.0), FloatType(0.0), FloatType(0.0)), mtx.x_axis, threshold));
		CHECK(vector_all_near_equal3(vector_set(FloatType(-1.0), FloatType(0.0), FloatType(0.0), FloatType(0.0)), mtx.y_axis, threshold));
		CHECK(vector_all_near_equal3(vector_set(FloatType(0.0), FloatType(0.0), FloatType(1.0), FloatType(0.0)), mtx.z_axis, threshold));
		CHECK(vector_all_near_equal3(vector_set(FloatType(1.0), FloatType(2.0), FloatType(3.0), FloatType(1.0)), mtx.w_axis, threshold));
	}

	{
		QuatType rotation_around_z = quat_from_euler(scalar_deg_to_rad(FloatType(0.0)), scalar_deg_to_rad(FloatType(90.0)), scalar_deg_to_rad(FloatType(0.0)));
		Vector4Type translation = vector_set(FloatType(1.0), FloatType(2.0), FloatType(3.0));
		Matrix3x4Type mtx = matrix_from_qvs(rotation_around_z, translation, FloatType(1.0));
		CHECK(vector_all_near_equal3(vector_set(FloatType(0.0), FloatType(1.0), FloatType(0.0), FloatType(0.0)), mtx.x_axis, threshold));
		CHECK(vector_all_near_equal3(vector_set(FloatType(-1.0), FloatType(0.0), FloatType(0.0), FloatType(0.0)), mtx.y_axis, threshold));
		CHECK(vector_all_near_equal3(vector_set(FloatType(0.0), FloatType(0.0), FloatType(1.0), FloatType(0.0)), mtx.z_axis, threshold));
		CHECK(vector_all_near_equal3(vector_set(FloatType(1.0), FloatType(2.0), FloatType(3.0), FloatType(1.0)), mtx.w_axis, threshold));

		FloatType scale = FloatType(4.0);
		mtx = matrix_from_qvs(rotation_around_z, translation, scale);
		CHECK(vector_all_near_equal3(vector_set(FloatType(0.0), FloatType(4.0), FloatType(0.0), FloatType(0.0)), mtx.x_axis, threshold));
		CHECK(vector_all_near_equal3(vector_set(FloatType(-4.0), FloatType(0.0), FloatType(0.0), FloatType(0.0)), mtx.y_axis, threshold));
		CHECK(vector_all_near_equal3(vector_set(FloatType(0.0), FloatType(0.0), FloatType(4.0), FloatType(0.0)), mtx.z_axis, threshold));
		CHECK(vector_all_near_equal3(vector_set(FloatType(1.0), FloatType(2.0), FloatType(3.0), FloatType(1.0)), mtx.w_axis, threshold));
	}

	{
		QuatType rotation_around_z = quat_from_euler(scalar_deg_to_rad(FloatType(0.0)), scalar_deg_to_rad(FloatType(90.0)), scalar_deg_to_rad(FloatType(0.0)));
		Vector4Type translation = vector_set(FloatType(1.0), FloatType(2.0), FloatType(3.0));
		Matrix3x4Type mtx = matrix_from_qvv(rotation_around_z, translation, vector_set(FloatType(1.0)));
		CHECK(vector_all_near_equal3(vector_set(FloatType(0.0), FloatType(1.0), FloatType(0.0), FloatType(0.0)), mtx.x_axis, threshold));
		CHECK(vector_all_near_equal3(vector_set(FloatType(-1.0), FloatType(0.0), FloatType(0.0), FloatType(0.0)), mtx.y_axis, threshold));
		CHECK(vector_all_near_equal3(vector_set(FloatType(0.0), FloatType(0.0), FloatType(1.0), FloatType(0.0)), mtx.z_axis, threshold));
		CHECK(vector_all_near_equal3(vector_set(FloatType(1.0), FloatType(2.0), FloatType(3.0), FloatType(1.0)), mtx.w_axis, threshold));

		Vector4Type scale = vector_set(FloatType(4.0), FloatType(5.0), FloatType(6.0));
		mtx = matrix_from_qvv(rotation_around_z, translation, scale);
		CHECK(vector_all_near_equal3(vector_set(FloatType(0.0), FloatType(4.0), FloatType(0.0), FloatType(0.0)), mtx.x_axis, threshold));
		CHECK(vector_all_near_equal3(vector_set(FloatType(-5.0), FloatType(0.0), FloatType(0.0), FloatType(0.0)), mtx.y_axis, threshold));
		CHECK(vector_all_near_equal3(vector_set(FloatType(0.0), FloatType(0.0), FloatType(6.0), FloatType(0.0)), mtx.z_axis, threshold));
		CHECK(vector_all_near_equal3(vector_set(FloatType(1.0), FloatType(2.0), FloatType(3.0), FloatType(1.0)), mtx.w_axis, threshold));
	}

	{
		QuatType rotation_around_z = quat_from_euler(scalar_deg_to_rad(FloatType(0.0)), scalar_deg_to_rad(FloatType(90.0)), scalar_deg_to_rad(FloatType(0.0)));
		Matrix3x4Type mtx = matrix_from_quat(rotation_around_z);
		CHECK(vector_all_near_equal3(vector_set(FloatType(0.0), FloatType(1.0), FloatType(0.0), FloatType(0.0)), mtx.x_axis, threshold));
		CHECK(vector_all_near_equal3(vector_set(FloatType(-1.0), FloatType(0.0), FloatType(0.0), FloatType(0.0)), mtx.y_axis, threshold));
		CHECK(vector_all_near_equal3(vector_set(FloatType(0.0), FloatType(0.0), FloatType(1.0), FloatType(0.0)), mtx.z_axis, threshold));
		CHECK(vector_all_near_equal3(vector_set(FloatType(0.0), FloatType(0.0), FloatType(0.0), FloatType(1.0)), mtx.w_axis, threshold));
	}

	{
		QuatType rotation_around_z = quat_from_euler(scalar_deg_to_rad(FloatType(0.0)), scalar_deg_to_rad(FloatType(90.0)), scalar_deg_to_rad(FloatType(0.0)));
		Matrix3x4Type mtx = matrix_from_rotation(rotation_around_z);
		CHECK(vector_all_near_equal3(vector_set(FloatType(0.0), FloatType(1.0), FloatType(0.0), FloatType(0.0)), mtx.x_axis, threshold));
		CHECK(vector_all_near_equal3(vector_set(FloatType(-1.0), FloatType(0.0), FloatType(0.0), FloatType(0.0)), mtx.y_axis, threshold));
		CHECK(vector_all_near_equal3(vector_set(FloatType(0.0), FloatType(0.0), FloatType(1.0), FloatType(0.0)), mtx.z_axis, threshold));
		CHECK(vector_all_near_equal3(vector_set(FloatType(0.0), FloatType(0.0), FloatType(0.0), FloatType(1.0)), mtx.w_axis, threshold));
	}

	{
		QuatType rotation_around_z = quat_from_euler(scalar_deg_to_rad(FloatType(0.0)), scalar_deg_to_rad(FloatType(90.0)), scalar_deg_to_rad(FloatType(0.0)));
		Matrix3x3Type rotation_mtx = matrix_from_quat(rotation_around_z);
		Matrix3x4Type mtx = matrix_from_rotation(rotation_mtx);
		CHECK(vector_all_near_equal3(vector_set(FloatType(0.0), FloatType(1.0), FloatType(0.0), FloatType(0.0)), mtx.x_axis, threshold));
		CHECK(vector_all_near_equal3(vector_set(FloatType(-1.0), FloatType(0.0), FloatType(0.0), FloatType(0.0)), mtx.y_axis, threshold));
		CHECK(vector_all_near_equal3(vector_set(FloatType(0.0), FloatType(0.0), FloatType(1.0), FloatType(0.0)), mtx.z_axis, threshold));
		CHECK(vector_all_near_equal3(vector_set(FloatType(0.0), FloatType(0.0), FloatType(0.0), FloatType(1.0)), mtx.w_axis, threshold));
	}

	{
		Matrix3x4Type mtx = matrix_from_translation(vector_set(FloatType(1.0), FloatType(2.0), FloatType(3.0)));
		CHECK(vector_all_near_equal3(vector_set(FloatType(1.0), FloatType(0.0), FloatType(0.0), FloatType(0.0)), mtx.x_axis, threshold));
		CHECK(vector_all_near_equal3(vector_set(FloatType(0.0), FloatType(1.0), FloatType(0.0), FloatType(0.0)), mtx.y_axis, threshold));
		CHECK(vector_all_near_equal3(vector_set(FloatType(0.0), FloatType(0.0), FloatType(1.0), FloatType(0.0)), mtx.z_axis, threshold));
		CHECK(vector_all_near_equal3(vector_set(FloatType(1.0), FloatType(2.0), FloatType(3.0), FloatType(1.0)), mtx.w_axis, threshold));
	}

	{
		Matrix3x4Type mtx = matrix_from_scale(vector_set(FloatType(4.0), FloatType(5.0), FloatType(6.0)));
		CHECK(vector_all_near_equal3(vector_set(FloatType(4.0), FloatType(0.0), FloatType(0.0), FloatType(0.0)), mtx.x_axis, threshold));
		CHECK(vector_all_near_equal3(vector_set(FloatType(0.0), FloatType(5.0), FloatType(0.0), FloatType(0.0)), mtx.y_axis, threshold));
		CHECK(vector_all_near_equal3(vector_set(FloatType(0.0), FloatType(0.0), FloatType(6.0), FloatType(0.0)), mtx.z_axis, threshold));
		CHECK(vector_all_near_equal3(vector_set(FloatType(0.0), FloatType(0.0), FloatType(0.0), FloatType(1.0)), mtx.w_axis, threshold));
	}

	{
		QuatType rotation_around_z = quat_from_euler(scalar_deg_to_rad(FloatType(0.0)), scalar_deg_to_rad(FloatType(90.0)), scalar_deg_to_rad(FloatType(0.0)));
		Vector4Type translation = vector_set(FloatType(1.0), FloatType(2.0), FloatType(3.0));
		QVType transform = qv_set(rotation_around_z, translation);
		Matrix3x4Type mtx = matrix_from_qv(transform);
		CHECK(vector_all_near_equal3(vector_set(FloatType(0.0), FloatType(1.0), FloatType(0.0), FloatType(0.0)), mtx.x_axis, threshold));
		CHECK(vector_all_near_equal3(vector_set(FloatType(-1.0), FloatType(0.0), FloatType(0.0), FloatType(0.0)), mtx.y_axis, threshold));
		CHECK(vector_all_near_equal3(vector_set(FloatType(0.0), FloatType(0.0), FloatType(1.0), FloatType(0.0)), mtx.z_axis, threshold));
		CHECK(vector_all_near_equal3(vector_set(FloatType(1.0), FloatType(2.0), FloatType(3.0), FloatType(1.0)), mtx.w_axis, threshold));
	}

	{
		QuatType rotation_around_z = quat_from_euler(scalar_deg_to_rad(FloatType(0.0)), scalar_deg_to_rad(FloatType(90.0)), scalar_deg_to_rad(FloatType(0.0)));
		Vector4Type translation = vector_set(FloatType(1.0), FloatType(2.0), FloatType(3.0));
		FloatType scale = FloatType(4.0);
		QVSType transform = qvs_set(rotation_around_z, translation, scale);
		Matrix3x4Type mtx = matrix_from_qvs(transform);
		CHECK(vector_all_near_equal3(vector_set(FloatType(0.0), FloatType(4.0), FloatType(0.0), FloatType(0.0)), mtx.x_axis, threshold));
		CHECK(vector_all_near_equal3(vector_set(FloatType(-4.0), FloatType(0.0), FloatType(0.0), FloatType(0.0)), mtx.y_axis, threshold));
		CHECK(vector_all_near_equal3(vector_set(FloatType(0.0), FloatType(0.0), FloatType(4.0), FloatType(0.0)), mtx.z_axis, threshold));
		CHECK(vector_all_near_equal3(vector_set(FloatType(1.0), FloatType(2.0), FloatType(3.0), FloatType(1.0)), mtx.w_axis, threshold));
	}

	{
		QuatType rotation_around_z = quat_from_euler(scalar_deg_to_rad(FloatType(0.0)), scalar_deg_to_rad(FloatType(90.0)), scalar_deg_to_rad(FloatType(0.0)));
		Vector4Type translation = vector_set(FloatType(1.0), FloatType(2.0), FloatType(3.0));
		Vector4Type scale = vector_set(FloatType(4.0), FloatType(5.0), FloatType(6.0));
		QVVType transform = qvv_set(rotation_around_z, translation, scale);
		Matrix3x4Type mtx = matrix_from_qvv(transform);
		CHECK(vector_all_near_equal3(vector_set(FloatType(0.0), FloatType(4.0), FloatType(0.0), FloatType(0.0)), mtx.x_axis, threshold));
		CHECK(vector_all_near_equal3(vector_set(FloatType(-5.0), FloatType(0.0), FloatType(0.0), FloatType(0.0)), mtx.y_axis, threshold));
		CHECK(vector_all_near_equal3(vector_set(FloatType(0.0), FloatType(0.0), FloatType(6.0), FloatType(0.0)), mtx.z_axis, threshold));
		CHECK(vector_all_near_equal3(vector_set(FloatType(1.0), FloatType(2.0), FloatType(3.0), FloatType(1.0)), mtx.w_axis, threshold));
	}

	{
		QuatType rotation_around_z = quat_from_euler(scalar_deg_to_rad(FloatType(0.0)), scalar_deg_to_rad(FloatType(90.0)), scalar_deg_to_rad(FloatType(0.0)));
		Vector4Type translation = vector_set(FloatType(1.0), FloatType(2.0), FloatType(3.0));
		Vector4Type scale = vector_set(FloatType(4.0), FloatType(5.0), FloatType(6.0));
		Matrix3x4Type mtx = matrix_from_qvv(rotation_around_z, translation, scale);
		CHECK(vector_all_near_equal3(matrix_get_axis(mtx, axis4::x), mtx.x_axis, threshold));
		CHECK(vector_all_near_equal3(matrix_get_axis(mtx, axis4::y), mtx.y_axis, threshold));
		CHECK(vector_all_near_equal3(matrix_get_axis(mtx, axis4::z), mtx.z_axis, threshold));
		CHECK(vector_all_near_equal3(matrix_get_axis(mtx, axis4::w), mtx.w_axis, threshold));
	}

	{
		QuatType rotation_around_z = quat_from_euler(scalar_deg_to_rad(FloatType(0.0)), scalar_deg_to_rad(FloatType(90.0)), scalar_deg_to_rad(FloatType(0.0)));
		Vector4Type translation = vector_set(FloatType(1.0), FloatType(2.0), FloatType(3.0));
		Vector4Type scale = vector_set(FloatType(4.0), FloatType(5.0), FloatType(6.0));
		Matrix3x4Type mtx = matrix_from_qvv(rotation_around_z, translation, scale);

		Matrix3x4Type mtx2 = identity;
		mtx2 = matrix_set_axis(mtx2, mtx.x_axis, axis4::x);
		mtx2 = matrix_set_axis(mtx2, mtx.y_axis, axis4::y);
		mtx2 = matrix_set_axis(mtx2, mtx.z_axis, axis4::z);
		mtx2 = matrix_set_axis(mtx2, mtx.w_axis, axis4::w);
		CHECK(vector_all_near_equal3(mtx2.x_axis, mtx.x_axis, threshold));
		CHECK(vector_all_near_equal3(mtx2.y_axis, mtx.y_axis, threshold));
		CHECK(vector_all_near_equal3(mtx2.z_axis, mtx.z_axis, threshold));
		CHECK(vector_all_near_equal3(mtx2.w_axis, mtx.w_axis, threshold));
	}

	{
		Vector4Type x_axis = vector_set(FloatType(1.0), FloatType(2.0), FloatType(3.0), FloatType(0.0));
		Vector4Type y_axis = vector_set(FloatType(4.0), FloatType(5.0), FloatType(6.0), FloatType(0.0));
		Vector4Type z_axis = vector_set(FloatType(7.0), FloatType(8.0), FloatType(9.0), FloatType(0.0));
		Vector4Type w_axis = vector_set(FloatType(10.0), FloatType(11.0), FloatType(12.0), FloatType(0.0));
		Matrix3x4Type mtx = matrix_set(x_axis, y_axis, z_axis, w_axis);
		CHECK(FloatType(matrix_get_component(mtx, axis4::x, component3::x)) == FloatType(1.0));
		CHECK(FloatType(matrix_get_component(mtx, axis4::x, component3::y)) == FloatType(2.0));
		CHECK(FloatType(matrix_get_component(mtx, axis4::x, component3::z)) == FloatType(3.0));
		CHECK(FloatType(matrix_get_component(mtx, axis4::y, component3::x)) == FloatType(4.0));
		CHECK(FloatType(matrix_get_component(mtx, axis4::y, component3::y)) == FloatType(5.0));
		CHECK(FloatType(matrix_get_component(mtx, axis4::y, component3::z)) == FloatType(6.0));
		CHECK(FloatType(matrix_get_component(mtx, axis4::z, component3::x)) == FloatType(7.0));
		CHECK(FloatType(matrix_get_component(mtx, axis4::z, component3::y)) == FloatType(8.0));
		CHECK(FloatType(matrix_get_component(mtx, axis4::z, component3::z)) == FloatType(9.0));
		CHECK(FloatType(matrix_get_component(mtx, axis4::w, component3::x)) == FloatType(10.0));
		CHECK(FloatType(matrix_get_component(mtx, axis4::w, component3::y)) == FloatType(11.0));
		CHECK(FloatType(matrix_get_component(mtx, axis4::w, component3::z)) == FloatType(12.0));

		CHECK(scalar_cast(matrix_get_component_as_scalar(mtx, axis4::x, component3::x)) == FloatType(1.0));
		CHECK(scalar_cast(matrix_get_component_as_scalar(mtx, axis4::x, component3::y)) == FloatType(2.0));
		CHECK(scalar_cast(matrix_get_component_as_scalar(mtx, axis4::x, component3::z)) == FloatType(3.0));
		CHECK(scalar_cast(matrix_get_component_as_scalar(mtx, axis4::y, component3::x)) == FloatType(4.0));
		CHECK(scalar_cast(matrix_get_component_as_scalar(mtx, axis4::y, component3::y)) == FloatType(5.0));
		CHECK(scalar_cast(matrix_get_component_as_scalar(mtx, axis4::y, component3::z)) == FloatType(6.0));
		CHECK(scalar_cast(matrix_get_component_as_scalar(mtx, axis4::z, component3::x)) == FloatType(7.0));
		CHECK(scalar_cast(matrix_get_component_as_scalar(mtx, axis4::z, component3::y)) == FloatType(8.0));
		CHECK(scalar_cast(matrix_get_component_as_scalar(mtx, axis4::z, component3::z)) == FloatType(9.0));
		CHECK(scalar_cast(matrix_get_component_as_scalar(mtx, axis4::w, component3::x)) == FloatType(10.0));
		CHECK(scalar_cast(matrix_get_component_as_scalar(mtx, axis4::w, component3::y)) == FloatType(11.0));
		CHECK(scalar_cast(matrix_get_component_as_scalar(mtx, axis4::w, component3::z)) == FloatType(12.0));
	}

	{
		Vector4Type x_axis = vector_set(FloatType(1.0), FloatType(2.0), FloatType(3.0), FloatType(0.0));
		Vector4Type y_axis = vector_set(FloatType(4.0), FloatType(5.0), FloatType(6.0), FloatType(0.0));
		Vector4Type z_axis = vector_set(FloatType(7.0), FloatType(8.0), FloatType(9.0), FloatType(0.0));
		Vector4Type w_axis = vector_set(FloatType(10.0), FloatType(11.0), FloatType(12.0), FloatType(0.0));
		Matrix3x4Type mtx = matrix_set(x_axis, y_axis, z_axis, w_axis);

		Matrix3x4Type mtx2 = identity;
		mtx2 = matrix_set_component(mtx2, FloatType(1.0), axis4::x, component3::x);
		mtx2 = matrix_set_component(mtx2, FloatType(2.0), axis4::x, component3::y);
		mtx2 = matrix_set_component(mtx2, FloatType(3.0), axis4::x, component3::z);
		mtx2 = matrix_set_component(mtx2, FloatType(4.0), axis4::y, component3::x);
		mtx2 = matrix_set_component(mtx2, FloatType(5.0), axis4::y, component3::y);
		mtx2 = matrix_set_component(mtx2, FloatType(6.0), axis4::y, component3::z);
		mtx2 = matrix_set_component(mtx2, FloatType(7.0), axis4::z, component3::x);
		mtx2 = matrix_set_component(mtx2, FloatType(8.0), axis4::z, component3::y);
		mtx2 = matrix_set_component(mtx2, FloatType(9.0), axis4::z, component3::z);
		mtx2 = matrix_set_component(mtx2, FloatType(10.0), axis4::w, component3::x);
		mtx2 = matrix_set_component(mtx2, FloatType(11.0), axis4::w, component3::y);
		mtx2 = matrix_set_component(mtx2, FloatType(12.0), axis4::w, component3::z);
		CHECK(vector_all_near_equal3(mtx2.x_axis, mtx.x_axis, threshold));
		CHECK(vector_all_near_equal3(mtx2.y_axis, mtx.y_axis, threshold));
		CHECK(vector_all_near_equal3(mtx2.z_axis, mtx.z_axis, threshold));
		CHECK(vector_all_near_equal3(mtx2.w_axis, mtx.w_axis, threshold));

		mtx2 = identity;
		mtx2 = matrix_set_component(mtx2, scalar_set(FloatType(1.0)), axis4::x, component3::x);
		mtx2 = matrix_set_component(mtx2, scalar_set(FloatType(2.0)), axis4::x, component3::y);
		mtx2 = matrix_set_component(mtx2, scalar_set(FloatType(3.0)), axis4::x, component3::z);
		mtx2 = matrix_set_component(mtx2, scalar_set(FloatType(4.0)), axis4::y, component3::x);
		mtx2 = matrix_set_component(mtx2, scalar_set(FloatType(5.0)), axis4::y, component3::y);
		mtx2 = matrix_set_component(mtx2, scalar_set(FloatType(6.0)), axis4::y, component3::z);
		mtx2 = matrix_set_component(mtx2, scalar_set(FloatType(7.0)), axis4::z, component3::x);
		mtx2 = matrix_set_component(mtx2, scalar_set(FloatType(8.0)), axis4::z, component3::y);
		mtx2 = matrix_set_component(mtx2, scalar_set(FloatType(9.0)), axis4::z, component3::z);
		mtx2 = matrix_set_component(mtx2, scalar_set(FloatType(10.0)), axis4::w, component3::x);
		mtx2 = matrix_set_component(mtx2, scalar_set(FloatType(11.0)), axis4::w, component3::y);
		mtx2 = matrix_set_component(mtx2, scalar_set(FloatType(12.0)), axis4::w, component3::z);
		CHECK(vector_all_near_equal3(mtx2.x_axis, mtx.x_axis, threshold));
		CHECK(vector_all_near_equal3(mtx2.y_axis, mtx.y_axis, threshold));
		CHECK(vector_all_near_equal3(mtx2.z_axis, mtx.z_axis, threshold));
		CHECK(vector_all_near_equal3(mtx2.w_axis, mtx.w_axis, threshold));
	}
}

template<typename FloatType>
static void test_affine_matrix_arithmetic(const FloatType threshold)
{
	using QuatType = typename related_types<FloatType>::quat;
	using Vector4Type = typename related_types<FloatType>::vector4;
	using Matrix3x4Type = typename related_types<FloatType>::matrix3x4;

	{
		Vector4Type x_axis = vector_set(FloatType(1.0), FloatType(0.0), FloatType(0.0));
		Vector4Type y_axis = vector_set(FloatType(0.0), FloatType(1.0), FloatType(0.0));

		QuatType rotation_around_z = quat_from_euler(scalar_deg_to_rad(FloatType(0.0)), scalar_deg_to_rad(FloatType(90.0)), scalar_deg_to_rad(FloatType(0.0)));
		Matrix3x4Type mtx_a = matrix_from_qvv(rotation_around_z, x_axis, vector_set(FloatType(1.0)));
		Vector4Type result = matrix_mul_point3(x_axis, mtx_a);
		CHECK(vector_all_near_equal3(result, vector_set(FloatType(1.0), FloatType(1.0), FloatType(0.0)), threshold));
		result = matrix_mul_point3(y_axis, mtx_a);
		CHECK(vector_all_near_equal3(result, vector_set(FloatType(0.0), FloatType(0.0), FloatType(0.0)), threshold));

		QuatType rotation_around_x = quat_from_euler(scalar_deg_to_rad(FloatType(0.0)), scalar_deg_to_rad(FloatType(0.0)), scalar_deg_to_rad(FloatType(90.0)));
		Matrix3x4Type mtx_b = matrix_from_qvv(rotation_around_x, y_axis, vector_set(FloatType(1.0)));
		result = matrix_mul_point3(x_axis, mtx_b);
		CHECK(vector_all_near_equal3(result, vector_set(FloatType(1.0), FloatType(1.0), FloatType(0.0)), threshold));
		result = matrix_mul_point3(y_axis, mtx_b);
		CHECK(vector_all_near_equal3(result, vector_set(FloatType(0.0), FloatType(1.0), FloatType(-1.0)), threshold));

		Matrix3x4Type mtx_ab = matrix_mul(mtx_a, mtx_b);
		Matrix3x4Type mtx_ba = matrix_mul(mtx_b, mtx_a);
		result = matrix_mul_point3(x_axis, mtx_ab);
		CHECK(vector_all_near_equal3(result, vector_set(FloatType(1.0), FloatType(1.0), FloatType(-1.0)), threshold));
		CHECK(vector_all_near_equal3(result, matrix_mul_point3(matrix_mul_point3(x_axis, mtx_a), mtx_b), threshold));
		result = matrix_mul_point3(y_axis, mtx_ab);
		CHECK(vector_all_near_equal3(result, vector_set(FloatType(0.0), FloatType(1.0), FloatType(0.0)), threshold));
		CHECK(vector_all_near_equal3(result, matrix_mul_point3(matrix_mul_point3(y_axis, mtx_a), mtx_b), threshold));
		result = matrix_mul_point3(x_axis, mtx_ba);
		CHECK(vector_all_near_equal3(result, vector_set(FloatType(0.0), FloatType(1.0), FloatType(0.0)), threshold));
		CHECK(vector_all_near_equal3(result, matrix_mul_point3(matrix_mul_point3(x_axis, mtx_b), mtx_a), threshold));
		result = matrix_mul_point3(y_axis, mtx_ba);
		CHECK(vector_all_near_equal3(result, vector_set(FloatType(0.0), FloatType(0.0), FloatType(-1.0)), threshold));
		CHECK(vector_all_near_equal3(result, matrix_mul_point3(matrix_mul_point3(y_axis, mtx_b), mtx_a), threshold));
	}

	{
		Vector4Type x_axis = vector_set(FloatType(1.0), FloatType(0.0), FloatType(0.0));
		Vector4Type y_axis = vector_set(FloatType(0.0), FloatType(1.0), FloatType(0.0));

		QuatType rotation_around_z = quat_from_euler(scalar_deg_to_rad(FloatType(0.0)), scalar_deg_to_rad(FloatType(90.0)), scalar_deg_to_rad(FloatType(0.0)));
		Matrix3x4Type mtx_a = matrix_from_qvv(rotation_around_z, x_axis, vector_set(FloatType(1.0)));
		Vector4Type result = matrix_mul_vector3(x_axis, mtx_a);
		CHECK(vector_all_near_equal3(result, vector_set(FloatType(0.0), FloatType(1.0), FloatType(0.0)), threshold));
		result = matrix_mul_vector3(y_axis, mtx_a);
		CHECK(vector_all_near_equal3(result, vector_set(FloatType(-1.0), FloatType(0.0), FloatType(0.0)), threshold));

		QuatType rotation_around_x = quat_from_euler(scalar_deg_to_rad(FloatType(0.0)), scalar_deg_to_rad(FloatType(0.0)), scalar_deg_to_rad(FloatType(90.0)));
		Matrix3x4Type mtx_b = matrix_from_qvv(rotation_around_x, y_axis, vector_set(FloatType(1.0)));
		result = matrix_mul_vector3(x_axis, mtx_b);
		CHECK(vector_all_near_equal3(result, vector_set(FloatType(1.0), FloatType(0.0), FloatType(0.0)), threshold));
		result = matrix_mul_vector3(y_axis, mtx_b);
		CHECK(vector_all_near_equal3(result, vector_set(FloatType(0.0), FloatType(0.0), FloatType(-1.0)), threshold));

		Matrix3x4Type mtx_ab = matrix_mul(mtx_a, mtx_b);
		Matrix3x4Type mtx_ba = matrix_mul(mtx_b, mtx_a);
		result = matrix_mul_vector3(x_axis, mtx_ab);
		CHECK(vector_all_near_equal3(result, vector_set(FloatType(0.0), FloatType(0.0), FloatType(-1.0)), threshold));
		CHECK(vector_all_near_equal3(result, matrix_mul_vector3(matrix_mul_vector3(x_axis, mtx_a), mtx_b), threshold));
		result = matrix_mul_vector3(y_axis, mtx_ab);
		CHECK(vector_all_near_equal3(result, vector_set(FloatType(-1.0), FloatType(0.0), FloatType(0.0)), threshold));
		CHECK(vector_all_near_equal3(result, matrix_mul_vector3(matrix_mul_vector3(y_axis, mtx_a), mtx_b), threshold));
		result = matrix_mul_vector3(x_axis, mtx_ba);
		CHECK(vector_all_near_equal3(result, vector_set(FloatType(0.0), FloatType(1.0), FloatType(0.0)), threshold));
		CHECK(vector_all_near_equal3(result, matrix_mul_vector3(matrix_mul_vector3(x_axis, mtx_b), mtx_a), threshold));
		result = matrix_mul_vector3(y_axis, mtx_ba);
		CHECK(vector_all_near_equal3(result, vector_set(FloatType(0.0), FloatType(0.0), FloatType(-1.0)), threshold));
		CHECK(vector_all_near_equal3(result, matrix_mul_vector3(matrix_mul_vector3(y_axis, mtx_b), mtx_a), threshold));
	}
}

template<typename FloatType>
static void test_affine_matrix_transformations(const FloatType threshold)
{
	using QuatType = typename related_types<FloatType>::quat;
	using Vector4Type = typename related_types<FloatType>::vector4;
	using Matrix3x3Type = typename related_types<FloatType>::matrix3x3;
	using Matrix3x4Type = typename related_types<FloatType>::matrix3x4;

	const Matrix3x4Type identity = matrix_identity();
	const Vector4Type zero = vector_zero();

	{
		Vector4Type x_axis = vector_set(FloatType(1.0), FloatType(2.0), FloatType(3.0), FloatType(0.0));
		Vector4Type y_axis = vector_set(FloatType(4.0), FloatType(5.0), FloatType(6.0), FloatType(0.0));
		Vector4Type z_axis = vector_set(FloatType(7.0), FloatType(8.0), FloatType(9.0), FloatType(0.0));
		Vector4Type w_axis = vector_set(FloatType(10.0), FloatType(11.0), FloatType(12.0), FloatType(1.0));
		Matrix3x4Type mtx0 = matrix_set(x_axis, y_axis, z_axis, w_axis);
		Matrix3x3Type mtx1 = matrix_transpose(mtx0);
		CHECK(vector_all_near_equal3(vector_set(FloatType(1.0), FloatType(4.0), FloatType(7.0), FloatType(10.0)), mtx1.x_axis, threshold));
		CHECK(vector_all_near_equal3(vector_set(FloatType(2.0), FloatType(5.0), FloatType(8.0), FloatType(11.0)), mtx1.y_axis, threshold));
		CHECK(vector_all_near_equal3(vector_set(FloatType(3.0), FloatType(6.0), FloatType(9.0), FloatType(12.0)), mtx1.z_axis, threshold));
	}

	{
		QuatType rotation_around_z = quat_from_euler(scalar_deg_to_rad(FloatType(0.0)), scalar_deg_to_rad(FloatType(90.0)), scalar_deg_to_rad(FloatType(0.0)));
		Vector4Type translation = vector_set(FloatType(1.0), FloatType(2.0), FloatType(3.0));
		Vector4Type scale = vector_set(FloatType(4.0), FloatType(5.0), FloatType(6.0));
		Matrix3x4Type mtx = matrix_from_qvv(rotation_around_z, translation, scale);
		Matrix3x4Type inv_mtx = matrix_inverse(mtx);
		Matrix3x4Type result = matrix_mul(mtx, inv_mtx);
		CHECK(vector_all_near_equal3(vector_set(FloatType(1.0), FloatType(0.0), FloatType(0.0), FloatType(0.0)), result.x_axis, threshold));
		CHECK(vector_all_near_equal3(vector_set(FloatType(0.0), FloatType(1.0), FloatType(0.0), FloatType(0.0)), result.y_axis, threshold));
		CHECK(vector_all_near_equal3(vector_set(FloatType(0.0), FloatType(0.0), FloatType(1.0), FloatType(0.0)), result.z_axis, threshold));
		CHECK(vector_all_near_equal3(vector_set(FloatType(0.0), FloatType(0.0), FloatType(0.0), FloatType(1.0)), result.w_axis, threshold));
	}

	{
		QuatType rotation_around_z = quat_from_euler(scalar_deg_to_rad(FloatType(0.0)), scalar_deg_to_rad(FloatType(90.0)), scalar_deg_to_rad(FloatType(0.0)));
		Vector4Type translation = vector_set(FloatType(1.0), FloatType(2.0), FloatType(3.0));
		Vector4Type scale = vector_set(FloatType(4.0), FloatType(5.0), FloatType(6.0));
		Matrix3x4Type mtx = matrix_from_qvv(rotation_around_z, translation, scale);
		Matrix3x4Type inv_mtx = matrix_inverse(mtx, mtx);
		Matrix3x4Type result = matrix_mul(mtx, inv_mtx);
		CHECK(vector_all_near_equal3(vector_set(FloatType(1.0), FloatType(0.0), FloatType(0.0), FloatType(0.0)), result.x_axis, threshold));
		CHECK(vector_all_near_equal3(vector_set(FloatType(0.0), FloatType(1.0), FloatType(0.0), FloatType(0.0)), result.y_axis, threshold));
		CHECK(vector_all_near_equal3(vector_set(FloatType(0.0), FloatType(0.0), FloatType(1.0), FloatType(0.0)), result.z_axis, threshold));
		CHECK(vector_all_near_equal3(vector_set(FloatType(0.0), FloatType(0.0), FloatType(0.0), FloatType(1.0)), result.w_axis, threshold));
	}

	{
		Matrix3x4Type mtx = matrix_set(zero, zero, zero, zero);
		Matrix3x4Type inv_mtx = matrix_inverse(mtx, identity);
		CHECK(vector_all_near_equal(identity.x_axis, inv_mtx.x_axis, threshold));
		CHECK(vector_all_near_equal(identity.y_axis, inv_mtx.y_axis, threshold));
		CHECK(vector_all_near_equal(identity.z_axis, inv_mtx.z_axis, threshold));
		CHECK(vector_all_near_equal(identity.w_axis, inv_mtx.w_axis, threshold));
	}

	{
		QuatType rotation = quat_from_euler(scalar_deg_to_rad(FloatType(12.3)), scalar_deg_to_rad(FloatType(42.8)), scalar_deg_to_rad(FloatType(33.41)));
		Vector4Type translation = vector_set(FloatType(1.0), FloatType(2.0), FloatType(3.0));
		Vector4Type scale = vector_set(FloatType(4.0), FloatType(5.0), FloatType(6.0));
		Matrix3x4Type mtx = matrix_from_qvv(rotation, translation, scale);
		Matrix3x4Type inv_mtx = matrix_inverse(mtx);
		Matrix3x4Type result = matrix_mul(mtx, inv_mtx);
		CHECK(vector_all_near_equal3(vector_set(FloatType(1.0), FloatType(0.0), FloatType(0.0), FloatType(0.0)), result.x_axis, threshold));
		CHECK(vector_all_near_equal3(vector_set(FloatType(0.0), FloatType(1.0), FloatType(0.0), FloatType(0.0)), result.y_axis, threshold));
		CHECK(vector_all_near_equal3(vector_set(FloatType(0.0), FloatType(0.0), FloatType(1.0), FloatType(0.0)), result.z_axis, threshold));
		CHECK(vector_all_near_equal3(vector_set(FloatType(0.0), FloatType(0.0), FloatType(0.0), FloatType(1.0)), result.w_axis, threshold));
	}
}

template<typename FloatType>
static void test_affine_matrix_misc(const FloatType threshold)
{
	using QuatType = typename related_types<FloatType>::quat;
	using Vector4Type = typename related_types<FloatType>::vector4;
	using Matrix3x3Type = typename related_types<FloatType>::matrix3x3;
	using Matrix3x4Type = typename related_types<FloatType>::matrix3x4;

	{
		QuatType rotation_around_z = quat_from_euler(scalar_deg_to_rad(FloatType(0.0)), scalar_deg_to_rad(FloatType(90.0)), scalar_deg_to_rad(FloatType(0.0)));
		Matrix3x4Type mtx = matrix_from_quat(rotation_around_z);
		QuatType rotation = quat_from_matrix(mtx);
		CHECK(quat_near_equal(rotation_around_z, rotation, threshold));
	}

	{
		QuatType rotation_around_z = quat_from_euler(scalar_deg_to_rad(FloatType(0.0)), scalar_deg_to_rad(FloatType(90.0)), scalar_deg_to_rad(FloatType(0.0)));
		Vector4Type translation = vector_set(FloatType(1.0), FloatType(2.0), FloatType(3.0));
		Vector4Type scale = vector_set(FloatType(4.0), FloatType(5.0), FloatType(6.0));
		Matrix3x4Type mtx0 = matrix_from_qvv(rotation_around_z, translation, scale);
		Matrix3x4Type mtx0_no_scale = matrix_remove_scale(mtx0);
		CHECK(vector_all_near_equal3(vector_set(FloatType(0.0), FloatType(1.0), FloatType(0.0), FloatType(0.0)), mtx0_no_scale.x_axis, threshold));
		CHECK(vector_all_near_equal3(vector_set(FloatType(-1.0), FloatType(0.0), FloatType(0.0), FloatType(0.0)), mtx0_no_scale.y_axis, threshold));
		CHECK(vector_all_near_equal3(vector_set(FloatType(0.0), FloatType(0.0), FloatType(1.0), FloatType(0.0)), mtx0_no_scale.z_axis, threshold));
		CHECK(vector_all_near_equal3(vector_set(FloatType(1.0), FloatType(2.0), FloatType(3.0), FloatType(1.0)), mtx0_no_scale.w_axis, threshold));
	}

	{
		Vector4Type x_axis = vector_set(FloatType(5.62565), FloatType(7.90751), FloatType(4.37048), FloatType(99999.9999));
		Vector4Type y_axis = vector_set(FloatType(0.36345), FloatType(7.87300), FloatType(7.23000), FloatType(99999.9999));
		Vector4Type z_axis = vector_set(FloatType(8.06413), FloatType(3.91970), FloatType(8.48928), FloatType(99999.9999));
		Matrix3x4Type mtx = matrix_set(x_axis, y_axis, z_axis, x_axis);
		FloatType det = scalar_cast(matrix_determinant(mtx));
		CHECK(scalar_near_equal(det, FloatType(381.95681179092484), threshold));
	}

	{
		Vector4Type x_axis = vector_set(FloatType(5.62565), FloatType(7.90751), FloatType(4.37048), FloatType(99999.9999));
		Vector4Type y_axis = vector_set(FloatType(0.36345), FloatType(7.87300), FloatType(7.23000), FloatType(99999.9999));
		Vector4Type z_axis = vector_set(FloatType(8.06413), FloatType(3.91970), FloatType(8.48928), FloatType(99999.9999));
		Matrix3x4Type mtx = matrix_set(x_axis, y_axis, z_axis, x_axis);

		FloatType det_xx = scalar_cast(matrix_minor(mtx, axis3::x, axis3::x));
		CHECK(scalar_near_equal(det_xx, FloatType(38.4967), threshold));
		FloatType det_xy = scalar_cast(matrix_minor(mtx, axis3::x, axis3::y));
		CHECK(scalar_near_equal(det_xy, FloatType(-55.2182), threshold));
		FloatType det_xz = scalar_cast(matrix_minor(mtx, axis3::x, axis3::z));
		CHECK(scalar_near_equal(det_xz, FloatType(-62.0643), threshold));

		FloatType det_yx = scalar_cast(matrix_minor(mtx, axis3::y, axis3::x));
		CHECK(scalar_near_equal(det_yx, FloatType(49.9981), threshold));
		FloatType det_yy = scalar_cast(matrix_minor(mtx, axis3::y, axis3::y));
		CHECK(scalar_near_equal(det_yy, FloatType(12.5136), threshold));
		FloatType det_yz = scalar_cast(matrix_minor(mtx, axis3::y, axis3::z));
		CHECK(scalar_near_equal(det_yz, FloatType(-41.7163), threshold));

		FloatType det_zx = scalar_cast(matrix_minor(mtx, axis3::z, axis3::x));
		CHECK(scalar_near_equal(det_zx, FloatType(22.7625), threshold));
		FloatType det_zy = scalar_cast(matrix_minor(mtx, axis3::z, axis3::y));
		CHECK(scalar_near_equal(det_zy, FloatType(39.085), threshold));
		FloatType det_zz = scalar_cast(matrix_minor(mtx, axis3::z, axis3::z));
		CHECK(scalar_near_equal(det_zz, FloatType(41.4168), threshold));
	}

	{
		Vector4Type x_axis = vector_set(FloatType(5.62565), FloatType(7.90751), FloatType(4.37048), FloatType(99999.9999));
		Vector4Type y_axis = vector_set(FloatType(0.36345), FloatType(7.87300), FloatType(7.23000), FloatType(99999.9999));
		Vector4Type z_axis = vector_set(FloatType(8.06413), FloatType(3.91970), FloatType(8.48928), FloatType(99999.9999));
		Matrix3x4Type mtx = matrix_set(x_axis, y_axis, z_axis, x_axis);
		Matrix3x3Type cof = matrix_cofactor(mtx);

		FloatType cof_xx = scalar_cast(vector_get_x_as_scalar(cof.x_axis));
		CHECK(scalar_near_equal(cof_xx, FloatType(38.4967), threshold));
		FloatType cof_xy = scalar_cast(vector_get_y_as_scalar(cof.x_axis));
		CHECK(scalar_near_equal(cof_xy, FloatType(55.2182), threshold));
		FloatType cof_xz = scalar_cast(vector_get_z_as_scalar(cof.x_axis));
		CHECK(scalar_near_equal(cof_xz, FloatType(-62.0643), threshold));

		FloatType cof_yx = scalar_cast(vector_get_x_as_scalar(cof.y_axis));
		CHECK(scalar_near_equal(cof_yx, FloatType(-49.9981), threshold));
		FloatType cof_yy = scalar_cast(vector_get_y_as_scalar(cof.y_axis));
		CHECK(scalar_near_equal(cof_yy, FloatType(12.5136), threshold));
		FloatType cof_yz = scalar_cast(vector_get_z_as_scalar(cof.y_axis));
		CHECK(scalar_near_equal(cof_yz, FloatType(41.7163), threshold));

		FloatType cof_zx = scalar_cast(vector_get_x_as_scalar(cof.z_axis));
		CHECK(scalar_near_equal(cof_zx, FloatType(22.7625), threshold));
		FloatType cof_zy = scalar_cast(vector_get_y_as_scalar(cof.z_axis));
		CHECK(scalar_near_equal(cof_zy, FloatType(-39.085), threshold));
		FloatType cof_zz = scalar_cast(vector_get_z_as_scalar(cof.z_axis));
		CHECK(scalar_near_equal(cof_zz, FloatType(41.4168), threshold));
	}

	{
		Vector4Type x_axis = vector_set(FloatType(5.62565), FloatType(7.90751), FloatType(4.37048), FloatType(99999.9999));
		Vector4Type y_axis = vector_set(FloatType(0.36345), FloatType(7.87300), FloatType(7.23000), FloatType(99999.9999));
		Vector4Type z_axis = vector_set(FloatType(8.06413), FloatType(3.91970), FloatType(8.48928), FloatType(99999.9999));
		Matrix3x4Type mtx = matrix_set(x_axis, y_axis, z_axis, x_axis);
		Matrix3x3Type adj = matrix_adjugate(mtx);

		FloatType adj_xx = scalar_cast(vector_get_x_as_scalar(adj.x_axis));
		CHECK(scalar_near_equal(adj_xx, FloatType(38.4967), threshold));
		FloatType adj_xy = scalar_cast(vector_get_y_as_scalar(adj.x_axis));
		CHECK(scalar_near_equal(adj_xy, FloatType(-49.9981), threshold));
		FloatType adj_xz = scalar_cast(vector_get_z_as_scalar(adj.x_axis));
		CHECK(scalar_near_equal(adj_xz, FloatType(22.7625), threshold));

		FloatType adj_yx = scalar_cast(vector_get_x_as_scalar(adj.y_axis));
		CHECK(scalar_near_equal(adj_yx, FloatType(55.2182), threshold));
		FloatType adj_yy = scalar_cast(vector_get_y_as_scalar(adj.y_axis));
		CHECK(scalar_near_equal(adj_yy, FloatType(12.5136), threshold));
		FloatType adj_yz = scalar_cast(vector_get_z_as_scalar(adj.y_axis));
		CHECK(scalar_near_equal(adj_yz, FloatType(-39.085), threshold));

		FloatType adj_zx = scalar_cast(vector_get_x_as_scalar(adj.z_axis));
		CHECK(scalar_near_equal(adj_zx, FloatType(-62.0643), threshold));
		FloatType adj_zy = scalar_cast(vector_get_y_as_scalar(adj.z_axis));
		CHECK(scalar_near_equal(adj_zy, FloatType(41.7163), threshold));
		FloatType adj_zz = scalar_cast(vector_get_z_as_scalar(adj.z_axis));
		CHECK(scalar_near_equal(adj_zz, FloatType(41.4168), threshold));
	}
}

TEST_CASE("matrix3x4f math", "[math][matrix3x4]")
{
	test_affine_matrix_setters<float>(1.0E-4F);
	test_affine_matrix_arithmetic<float>(1.0E-4F);
	test_affine_matrix_transformations<float>(1.0E-4F);
	test_affine_matrix_misc<float>(1.0E-4F);

	{
		quatf rotation_around_z = quat_from_euler(scalar_deg_to_rad(0.0F), scalar_deg_to_rad(90.0F), scalar_deg_to_rad(0.0F));
		vector4f translation = vector_set(1.0F, 2.0F, 3.0F);
		vector4f scale = vector_set(4.0F, 5.0F, 6.0F);
		matrix3x4f src = matrix_from_qvv(rotation_around_z, translation, scale);
		matrix3x4d dst = matrix_cast(src);
		CHECK(vector_all_near_equal3(vector_cast(src.x_axis), dst.x_axis, 1.0E-4));
		CHECK(vector_all_near_equal3(vector_cast(src.y_axis), dst.y_axis, 1.0E-4));
		CHECK(vector_all_near_equal3(vector_cast(src.z_axis), dst.z_axis, 1.0E-4));
		CHECK(vector_all_near_equal3(vector_cast(src.w_axis), dst.w_axis, 1.0E-4));
	}
}

TEST_CASE("matrix3x4d math", "[math][matrix3x4]")
{
	test_affine_matrix_setters<double>(1.0E-4);
	test_affine_matrix_arithmetic<double>(1.0E-4F);
	test_affine_matrix_transformations<double>(1.0E-4F);
	test_affine_matrix_misc<double>(1.0E-4F);

	{
		quatd rotation_around_z = quat_from_euler(scalar_deg_to_rad(0.0), scalar_deg_to_rad(90.0), scalar_deg_to_rad(0.0));
		vector4d translation = vector_set(1.0, 2.0, 3.0);
		vector4d scale = vector_set(4.0, 5.0, 6.0);
		matrix3x4d src = matrix_from_qvv(rotation_around_z, translation, scale);
		matrix3x4f dst = matrix_cast(src);
		CHECK(vector_all_near_equal3(vector_cast(src.x_axis), dst.x_axis, 1.0E-4F));
		CHECK(vector_all_near_equal3(vector_cast(src.y_axis), dst.y_axis, 1.0E-4F));
		CHECK(vector_all_near_equal3(vector_cast(src.z_axis), dst.z_axis, 1.0E-4F));
		CHECK(vector_all_near_equal3(vector_cast(src.w_axis), dst.w_axis, 1.0E-4F));
	}
}

```

`tests/sources/test_matrix4x4.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2018 Nicholas Frechette & Animation Compression Library contributors
// Copyright (c) 2018 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include "catch2.impl.h"

#include <rtm/type_traits.h>
#include <rtm/matrix3x4f.h>
#include <rtm/matrix3x4d.h>
#include <rtm/matrix4x4f.h>
#include <rtm/matrix4x4d.h>

using namespace rtm;

template<typename FloatType>
static void test_matrix4x4_setters(const FloatType threshold)
{
	using Vector4Type = typename related_types<FloatType>::vector4;
	using Matrix4x4Type = typename related_types<FloatType>::matrix4x4;

	const Matrix4x4Type identity = matrix_identity();

	{
		Vector4Type x_axis = vector_set(FloatType(1.0), FloatType(2.0), FloatType(3.0), FloatType(0.0));
		Vector4Type y_axis = vector_set(FloatType(4.0), FloatType(5.0), FloatType(6.0), FloatType(0.0));
		Vector4Type z_axis = vector_set(FloatType(7.0), FloatType(8.0), FloatType(9.0), FloatType(0.0));
		Vector4Type w_axis = vector_set(FloatType(10.0), FloatType(11.0), FloatType(12.0), FloatType(1.0));
		Matrix4x4Type mtx = matrix_set(x_axis, y_axis, z_axis, w_axis);
		CHECK(vector_all_near_equal(x_axis, mtx.x_axis, threshold));
		CHECK(vector_all_near_equal(y_axis, mtx.y_axis, threshold));
		CHECK(vector_all_near_equal(z_axis, mtx.z_axis, threshold));
		CHECK(vector_all_near_equal(w_axis, mtx.w_axis, threshold));
	}

	{
		CHECK(vector_all_near_equal(vector_set(FloatType(1.0), FloatType(0.0), FloatType(0.0), FloatType(0.0)), identity.x_axis, threshold));
		CHECK(vector_all_near_equal(vector_set(FloatType(0.0), FloatType(1.0), FloatType(0.0), FloatType(0.0)), identity.y_axis, threshold));
		CHECK(vector_all_near_equal(vector_set(FloatType(0.0), FloatType(0.0), FloatType(1.0), FloatType(0.0)), identity.z_axis, threshold));
		CHECK(vector_all_near_equal(vector_set(FloatType(0.0), FloatType(0.0), FloatType(0.0), FloatType(1.0)), identity.w_axis, threshold));
	}

	{
		Vector4Type x_axis = vector_set(FloatType(1.0), FloatType(2.0), FloatType(3.0), FloatType(0.0));
		Vector4Type y_axis = vector_set(FloatType(4.0), FloatType(5.0), FloatType(6.0), FloatType(0.0));
		Vector4Type z_axis = vector_set(FloatType(7.0), FloatType(8.0), FloatType(9.0), FloatType(0.0));
		Vector4Type w_axis = vector_set(FloatType(10.0), FloatType(11.0), FloatType(12.0), FloatType(1.0));
		Matrix4x4Type mtx = matrix_set(x_axis, y_axis, z_axis, w_axis);
		CHECK(vector_all_near_equal(matrix_get_axis(mtx, axis4::x), mtx.x_axis, threshold));
		CHECK(vector_all_near_equal(matrix_get_axis(mtx, axis4::y), mtx.y_axis, threshold));
		CHECK(vector_all_near_equal(matrix_get_axis(mtx, axis4::z), mtx.z_axis, threshold));
		CHECK(vector_all_near_equal(matrix_get_axis(mtx, axis4::w), mtx.w_axis, threshold));
	}

	{
		Vector4Type x_axis = vector_set(FloatType(1.0), FloatType(2.0), FloatType(3.0), FloatType(0.0));
		Vector4Type y_axis = vector_set(FloatType(4.0), FloatType(5.0), FloatType(6.0), FloatType(0.0));
		Vector4Type z_axis = vector_set(FloatType(7.0), FloatType(8.0), FloatType(9.0), FloatType(0.0));
		Vector4Type w_axis = vector_set(FloatType(10.0), FloatType(11.0), FloatType(12.0), FloatType(1.0));
		Matrix4x4Type mtx = matrix_set(x_axis, y_axis, z_axis, w_axis);

		Matrix4x4Type mtx2 = identity;
		mtx2 = matrix_set_axis(mtx2, mtx.x_axis, axis4::x);
		mtx2 = matrix_set_axis(mtx2, mtx.y_axis, axis4::y);
		mtx2 = matrix_set_axis(mtx2, mtx.z_axis, axis4::z);
		mtx2 = matrix_set_axis(mtx2, mtx.w_axis, axis4::w);
		CHECK(vector_all_near_equal(mtx2.x_axis, mtx.x_axis, threshold));
		CHECK(vector_all_near_equal(mtx2.y_axis, mtx.y_axis, threshold));
		CHECK(vector_all_near_equal(mtx2.z_axis, mtx.z_axis, threshold));
		CHECK(vector_all_near_equal(mtx2.w_axis, mtx.w_axis, threshold));
	}

	{
		Vector4Type x_axis = vector_set(FloatType(1.0), FloatType(2.0), FloatType(3.0), FloatType(13.0));
		Vector4Type y_axis = vector_set(FloatType(4.0), FloatType(5.0), FloatType(6.0), FloatType(14.0));
		Vector4Type z_axis = vector_set(FloatType(7.0), FloatType(8.0), FloatType(9.0), FloatType(15.0));
		Vector4Type w_axis = vector_set(FloatType(10.0), FloatType(11.0), FloatType(12.0), FloatType(16.0));
		Matrix4x4Type mtx = matrix_set(x_axis, y_axis, z_axis, w_axis);
		CHECK(FloatType(matrix_get_component(mtx, axis4::x, component4::x)) == FloatType(1.0));
		CHECK(FloatType(matrix_get_component(mtx, axis4::x, component4::y)) == FloatType(2.0));
		CHECK(FloatType(matrix_get_component(mtx, axis4::x, component4::z)) == FloatType(3.0));
		CHECK(FloatType(matrix_get_component(mtx, axis4::x, component4::w)) == FloatType(13.0));
		CHECK(FloatType(matrix_get_component(mtx, axis4::y, component4::x)) == FloatType(4.0));
		CHECK(FloatType(matrix_get_component(mtx, axis4::y, component4::y)) == FloatType(5.0));
		CHECK(FloatType(matrix_get_component(mtx, axis4::y, component4::z)) == FloatType(6.0));
		CHECK(FloatType(matrix_get_component(mtx, axis4::y, component4::w)) == FloatType(14.0));
		CHECK(FloatType(matrix_get_component(mtx, axis4::z, component4::x)) == FloatType(7.0));
		CHECK(FloatType(matrix_get_component(mtx, axis4::z, component4::y)) == FloatType(8.0));
		CHECK(FloatType(matrix_get_component(mtx, axis4::z, component4::z)) == FloatType(9.0));
		CHECK(FloatType(matrix_get_component(mtx, axis4::z, component4::w)) == FloatType(15.0));
		CHECK(FloatType(matrix_get_component(mtx, axis4::w, component4::x)) == FloatType(10.0));
		CHECK(FloatType(matrix_get_component(mtx, axis4::w, component4::y)) == FloatType(11.0));
		CHECK(FloatType(matrix_get_component(mtx, axis4::w, component4::z)) == FloatType(12.0));
		CHECK(FloatType(matrix_get_component(mtx, axis4::w, component4::w)) == FloatType(16.0));

		CHECK(scalar_cast(matrix_get_component_as_scalar(mtx, axis4::x, component4::x)) == FloatType(1.0));
		CHECK(scalar_cast(matrix_get_component_as_scalar(mtx, axis4::x, component4::y)) == FloatType(2.0));
		CHECK(scalar_cast(matrix_get_component_as_scalar(mtx, axis4::x, component4::z)) == FloatType(3.0));
		CHECK(scalar_cast(matrix_get_component_as_scalar(mtx, axis4::x, component4::w)) == FloatType(13.0));
		CHECK(scalar_cast(matrix_get_component_as_scalar(mtx, axis4::y, component4::x)) == FloatType(4.0));
		CHECK(scalar_cast(matrix_get_component_as_scalar(mtx, axis4::y, component4::y)) == FloatType(5.0));
		CHECK(scalar_cast(matrix_get_component_as_scalar(mtx, axis4::y, component4::z)) == FloatType(6.0));
		CHECK(scalar_cast(matrix_get_component_as_scalar(mtx, axis4::y, component4::w)) == FloatType(14.0));
		CHECK(scalar_cast(matrix_get_component_as_scalar(mtx, axis4::z, component4::x)) == FloatType(7.0));
		CHECK(scalar_cast(matrix_get_component_as_scalar(mtx, axis4::z, component4::y)) == FloatType(8.0));
		CHECK(scalar_cast(matrix_get_component_as_scalar(mtx, axis4::z, component4::z)) == FloatType(9.0));
		CHECK(scalar_cast(matrix_get_component_as_scalar(mtx, axis4::z, component4::w)) == FloatType(15.0));
		CHECK(scalar_cast(matrix_get_component_as_scalar(mtx, axis4::w, component4::x)) == FloatType(10.0));
		CHECK(scalar_cast(matrix_get_component_as_scalar(mtx, axis4::w, component4::y)) == FloatType(11.0));
		CHECK(scalar_cast(matrix_get_component_as_scalar(mtx, axis4::w, component4::z)) == FloatType(12.0));
		CHECK(scalar_cast(matrix_get_component_as_scalar(mtx, axis4::w, component4::w)) == FloatType(16.0));
	}

	{
		Vector4Type x_axis = vector_set(FloatType(1.0), FloatType(2.0), FloatType(3.0), FloatType(13.0));
		Vector4Type y_axis = vector_set(FloatType(4.0), FloatType(5.0), FloatType(6.0), FloatType(14.0));
		Vector4Type z_axis = vector_set(FloatType(7.0), FloatType(8.0), FloatType(9.0), FloatType(15.0));
		Vector4Type w_axis = vector_set(FloatType(10.0), FloatType(11.0), FloatType(12.0), FloatType(16.0));
		Matrix4x4Type mtx = matrix_set(x_axis, y_axis, z_axis, w_axis);

		Matrix4x4Type mtx2 = identity;
		mtx2 = matrix_set_component(mtx2, FloatType(1.0), axis4::x, component4::x);
		mtx2 = matrix_set_component(mtx2, FloatType(2.0), axis4::x, component4::y);
		mtx2 = matrix_set_component(mtx2, FloatType(3.0), axis4::x, component4::z);
		mtx2 = matrix_set_component(mtx2, FloatType(13.0), axis4::x, component4::w);
		mtx2 = matrix_set_component(mtx2, FloatType(4.0), axis4::y, component4::x);
		mtx2 = matrix_set_component(mtx2, FloatType(5.0), axis4::y, component4::y);
		mtx2 = matrix_set_component(mtx2, FloatType(6.0), axis4::y, component4::z);
		mtx2 = matrix_set_component(mtx2, FloatType(14.0), axis4::y, component4::w);
		mtx2 = matrix_set_component(mtx2, FloatType(7.0), axis4::z, component4::x);
		mtx2 = matrix_set_component(mtx2, FloatType(8.0), axis4::z, component4::y);
		mtx2 = matrix_set_component(mtx2, FloatType(9.0), axis4::z, component4::z);
		mtx2 = matrix_set_component(mtx2, FloatType(15.0), axis4::z, component4::w);
		mtx2 = matrix_set_component(mtx2, FloatType(10.0), axis4::w, component4::x);
		mtx2 = matrix_set_component(mtx2, FloatType(11.0), axis4::w, component4::y);
		mtx2 = matrix_set_component(mtx2, FloatType(12.0), axis4::w, component4::z);
		mtx2 = matrix_set_component(mtx2, FloatType(16.0), axis4::w, component4::w);
		CHECK(vector_all_near_equal(mtx2.x_axis, mtx.x_axis, threshold));
		CHECK(vector_all_near_equal(mtx2.y_axis, mtx.y_axis, threshold));
		CHECK(vector_all_near_equal(mtx2.z_axis, mtx.z_axis, threshold));
		CHECK(vector_all_near_equal(mtx2.w_axis, mtx.w_axis, threshold));

		mtx2 = identity;
		mtx2 = matrix_set_component(mtx2, scalar_set(FloatType(1.0)), axis4::x, component4::x);
		mtx2 = matrix_set_component(mtx2, scalar_set(FloatType(2.0)), axis4::x, component4::y);
		mtx2 = matrix_set_component(mtx2, scalar_set(FloatType(3.0)), axis4::x, component4::z);
		mtx2 = matrix_set_component(mtx2, scalar_set(FloatType(13.0)), axis4::x, component4::w);
		mtx2 = matrix_set_component(mtx2, scalar_set(FloatType(4.0)), axis4::y, component4::x);
		mtx2 = matrix_set_component(mtx2, scalar_set(FloatType(5.0)), axis4::y, component4::y);
		mtx2 = matrix_set_component(mtx2, scalar_set(FloatType(6.0)), axis4::y, component4::z);
		mtx2 = matrix_set_component(mtx2, scalar_set(FloatType(14.0)), axis4::y, component4::w);
		mtx2 = matrix_set_component(mtx2, scalar_set(FloatType(7.0)), axis4::z, component4::x);
		mtx2 = matrix_set_component(mtx2, scalar_set(FloatType(8.0)), axis4::z, component4::y);
		mtx2 = matrix_set_component(mtx2, scalar_set(FloatType(9.0)), axis4::z, component4::z);
		mtx2 = matrix_set_component(mtx2, scalar_set(FloatType(15.0)), axis4::z, component4::w);
		mtx2 = matrix_set_component(mtx2, scalar_set(FloatType(10.0)), axis4::w, component4::x);
		mtx2 = matrix_set_component(mtx2, scalar_set(FloatType(11.0)), axis4::w, component4::y);
		mtx2 = matrix_set_component(mtx2, scalar_set(FloatType(12.0)), axis4::w, component4::z);
		mtx2 = matrix_set_component(mtx2, scalar_set(FloatType(16.0)), axis4::w, component4::w);
		CHECK(vector_all_near_equal(mtx2.x_axis, mtx.x_axis, threshold));
		CHECK(vector_all_near_equal(mtx2.y_axis, mtx.y_axis, threshold));
		CHECK(vector_all_near_equal(mtx2.z_axis, mtx.z_axis, threshold));
		CHECK(vector_all_near_equal(mtx2.w_axis, mtx.w_axis, threshold));
	}
}

template<typename FloatType>
static void test_matrix4x4_arithmetic(const FloatType threshold)
{
	using QuatType = typename related_types<FloatType>::quat;
	using Vector4Type = typename related_types<FloatType>::vector4;
	using Matrix3x4Type = typename related_types<FloatType>::matrix3x4;
	using Matrix4x4Type = typename related_types<FloatType>::matrix4x4;

	{
		Vector4Type x_axis = vector_set(FloatType(1.0), FloatType(0.0), FloatType(0.0));
		Vector4Type y_axis = vector_set(FloatType(0.0), FloatType(1.0), FloatType(0.0));

		QuatType rotation_around_z = quat_from_euler(scalar_deg_to_rad(FloatType(0.0)), scalar_deg_to_rad(FloatType(90.0)), scalar_deg_to_rad(FloatType(0.0)));
		Matrix3x4Type mtx_a3x4 = matrix_from_qvv(rotation_around_z, x_axis, vector_set(FloatType(1.0)));
		Matrix4x4Type mtx_a = matrix_cast(mtx_a3x4);
		Vector4Type result = matrix_mul_vector(x_axis, mtx_a);
		CHECK(vector_all_near_equal3(result, vector_set(FloatType(0.0), FloatType(1.0), FloatType(0.0), FloatType(0.0)), threshold));
		result = matrix_mul_vector(y_axis, mtx_a);
		CHECK(vector_all_near_equal3(result, vector_set(FloatType(-1.0), FloatType(0.0), FloatType(0.0), FloatType(0.0)), threshold));

		QuatType rotation_around_x = quat_from_euler(scalar_deg_to_rad(FloatType(0.0)), scalar_deg_to_rad(FloatType(0.0)), scalar_deg_to_rad(FloatType(90.0)));
		Matrix3x4Type mtx_b3x4 = matrix_from_qvv(rotation_around_x, y_axis, vector_set(FloatType(1.0)));
		Matrix4x4Type mtx_b = matrix_cast(mtx_b3x4);
		result = matrix_mul_vector(x_axis, mtx_b);
		CHECK(vector_all_near_equal3(result, vector_set(FloatType(1.0), FloatType(0.0), FloatType(0.0), FloatType(0.0)), threshold));
		result = matrix_mul_vector(y_axis, mtx_b);
		CHECK(vector_all_near_equal3(result, vector_set(FloatType(0.0), FloatType(0.0), FloatType(-1.0), FloatType(0.0)), threshold));

		Matrix4x4Type mtx_ab = matrix_mul(mtx_a, mtx_b);
		Matrix4x4Type mtx_ba = matrix_mul(mtx_b, mtx_a);
		result = matrix_mul_vector(x_axis, mtx_ab);
		CHECK(vector_all_near_equal3(result, vector_set(FloatType(0.0), FloatType(0.0), FloatType(-1.0), FloatType(0.0)), threshold));
		CHECK(vector_all_near_equal3(result, matrix_mul_vector(matrix_mul_vector(x_axis, mtx_a), mtx_b), threshold));
		result = matrix_mul_vector(y_axis, mtx_ab);
		CHECK(vector_all_near_equal3(result, vector_set(FloatType(-1.0), FloatType(0.0), FloatType(0.0), FloatType(0.0)), threshold));
		CHECK(vector_all_near_equal3(result, matrix_mul_vector(matrix_mul_vector(y_axis, mtx_a), mtx_b), threshold));
		result = matrix_mul_vector(x_axis, mtx_ba);
		CHECK(vector_all_near_equal3(result, vector_set(FloatType(0.0), FloatType(1.0), FloatType(0.0), FloatType(0.0)), threshold));
		CHECK(vector_all_near_equal3(result, matrix_mul_vector(matrix_mul_vector(x_axis, mtx_b), mtx_a), threshold));
		result = matrix_mul_vector(y_axis, mtx_ba);
		CHECK(vector_all_near_equal3(result, vector_set(FloatType(0.0), FloatType(0.0), FloatType(-1.0), FloatType(0.0)), threshold));
		CHECK(vector_all_near_equal3(result, matrix_mul_vector(matrix_mul_vector(y_axis, mtx_b), mtx_a), threshold));
	}

	{
		Vector4Type x_axis = vector_set(FloatType(1.0), FloatType(2.0), FloatType(3.0), FloatType(0.2));
		Vector4Type y_axis = vector_set(FloatType(4.0), FloatType(5.0), FloatType(6.0), FloatType(0.5));
		Vector4Type z_axis = vector_set(FloatType(7.0), FloatType(8.0), FloatType(9.0), FloatType(0.8));
		Vector4Type w_axis = vector_set(FloatType(10.0), FloatType(11.0), FloatType(12.0), FloatType(1.2));
		Matrix4x4Type mtx = matrix_set(x_axis, y_axis, z_axis, w_axis);
		Matrix4x4Type mtx_squared = matrix_mul(mtx, mtx);
		CHECK(vector_all_near_equal(vector_set(FloatType(32.0), FloatType(38.2), FloatType(44.4), FloatType(3.84)), mtx_squared.x_axis, threshold));
		CHECK(vector_all_near_equal(vector_set(FloatType(71.0), FloatType(86.5), FloatType(102.0), FloatType(8.7)), mtx_squared.y_axis, threshold));
		CHECK(vector_all_near_equal(vector_set(FloatType(110.0), FloatType(134.8), FloatType(159.6), FloatType(13.56)), mtx_squared.z_axis, threshold));
		CHECK(vector_all_near_equal(vector_set(FloatType(150.0), FloatType(184.2), FloatType(218.4), FloatType(18.54)), mtx_squared.w_axis, threshold));

		Vector4Type some_vector = vector_set(FloatType(0.8), FloatType(2.4), FloatType(1.5), FloatType(10.24));
		Vector4Type result = matrix_mul_vector(some_vector, mtx);
		CHECK(vector_all_near_equal(vector_set(FloatType(123.3), FloatType(138.24), FloatType(153.18), FloatType(14.848)), result, threshold));
	}
}

template<typename FloatType>
static void test_matrix4x4_transformations(const FloatType threshold)
{
	using QuatType = typename related_types<FloatType>::quat;
	using Vector4Type = typename related_types<FloatType>::vector4;
	using Matrix3x4Type = typename related_types<FloatType>::matrix3x4;
	using Matrix4x4Type = typename related_types<FloatType>::matrix4x4;

	const Matrix4x4Type identity = matrix_identity();
	const Vector4Type zero = vector_zero();

	{
		Vector4Type x_axis = vector_set(FloatType(1.0), FloatType(2.0), FloatType(3.0), FloatType(0.0));
		Vector4Type y_axis = vector_set(FloatType(4.0), FloatType(5.0), FloatType(6.0), FloatType(0.0));
		Vector4Type z_axis = vector_set(FloatType(7.0), FloatType(8.0), FloatType(9.0), FloatType(0.0));
		Vector4Type w_axis = vector_set(FloatType(10.0), FloatType(11.0), FloatType(12.0), FloatType(1.0));
		Matrix4x4Type mtx0 = matrix_set(x_axis, y_axis, z_axis, w_axis);
		Matrix4x4Type mtx1 = matrix_transpose(mtx0);
		CHECK(vector_all_near_equal(vector_set(FloatType(1.0), FloatType(4.0), FloatType(7.0), FloatType(10.0)), mtx1.x_axis, threshold));
		CHECK(vector_all_near_equal(vector_set(FloatType(2.0), FloatType(5.0), FloatType(8.0), FloatType(11.0)), mtx1.y_axis, threshold));
		CHECK(vector_all_near_equal(vector_set(FloatType(3.0), FloatType(6.0), FloatType(9.0), FloatType(12.0)), mtx1.z_axis, threshold));
		CHECK(vector_all_near_equal(vector_set(FloatType(0.0), FloatType(0.0), FloatType(0.0), FloatType(1.0)), mtx1.w_axis, threshold));
	}

	{
		QuatType rotation_around_z = quat_from_euler(scalar_deg_to_rad(FloatType(0.0)), scalar_deg_to_rad(FloatType(90.0)), scalar_deg_to_rad(FloatType(0.0)));
		Vector4Type translation = vector_set(FloatType(1.0), FloatType(2.0), FloatType(3.0));
		Vector4Type scale = vector_set(FloatType(4.0), FloatType(5.0), FloatType(6.0));
		Matrix3x4Type mtx3x4 = matrix_from_qvv(rotation_around_z, translation, scale);
		mtx3x4.x_axis = vector_set(vector_get_x(mtx3x4.x_axis), vector_get_y(mtx3x4.x_axis), vector_get_z(mtx3x4.x_axis), FloatType(0.0));
		mtx3x4.y_axis = vector_set(vector_get_x(mtx3x4.y_axis), vector_get_y(mtx3x4.y_axis), vector_get_z(mtx3x4.y_axis), FloatType(0.0));
		mtx3x4.z_axis = vector_set(vector_get_x(mtx3x4.z_axis), vector_get_y(mtx3x4.z_axis), vector_get_z(mtx3x4.z_axis), FloatType(0.0));
		mtx3x4.w_axis = vector_set(vector_get_x(mtx3x4.w_axis), vector_get_y(mtx3x4.w_axis), vector_get_z(mtx3x4.w_axis), FloatType(1.0));
		Matrix4x4Type mtx = matrix_cast(mtx3x4);
		Matrix4x4Type inv_mtx = matrix_inverse(mtx);
		Matrix4x4Type result = matrix_mul(mtx, inv_mtx);
		CHECK(vector_all_near_equal(vector_set(FloatType(1.0), FloatType(0.0), FloatType(0.0), FloatType(0.0)), result.x_axis, threshold));
		CHECK(vector_all_near_equal(vector_set(FloatType(0.0), FloatType(1.0), FloatType(0.0), FloatType(0.0)), result.y_axis, threshold));
		CHECK(vector_all_near_equal(vector_set(FloatType(0.0), FloatType(0.0), FloatType(1.0), FloatType(0.0)), result.z_axis, threshold));
		CHECK(vector_all_near_equal(vector_set(FloatType(0.0), FloatType(0.0), FloatType(0.0), FloatType(1.0)), result.w_axis, threshold));
	}

	{
		QuatType rotation_around_z = quat_from_euler(scalar_deg_to_rad(FloatType(0.0)), scalar_deg_to_rad(FloatType(90.0)), scalar_deg_to_rad(FloatType(0.0)));
		Vector4Type translation = vector_set(FloatType(1.0), FloatType(2.0), FloatType(3.0));
		Vector4Type scale = vector_set(FloatType(4.0), FloatType(5.0), FloatType(6.0));
		Matrix3x4Type mtx3x4 = matrix_from_qvv(rotation_around_z, translation, scale);
		mtx3x4.x_axis = vector_set(vector_get_x(mtx3x4.x_axis), vector_get_y(mtx3x4.x_axis), vector_get_z(mtx3x4.x_axis), FloatType(0.0));
		mtx3x4.y_axis = vector_set(vector_get_x(mtx3x4.y_axis), vector_get_y(mtx3x4.y_axis), vector_get_z(mtx3x4.y_axis), FloatType(0.0));
		mtx3x4.z_axis = vector_set(vector_get_x(mtx3x4.z_axis), vector_get_y(mtx3x4.z_axis), vector_get_z(mtx3x4.z_axis), FloatType(0.0));
		mtx3x4.w_axis = vector_set(vector_get_x(mtx3x4.w_axis), vector_get_y(mtx3x4.w_axis), vector_get_z(mtx3x4.w_axis), FloatType(1.0));
		Matrix4x4Type mtx = matrix_cast(mtx3x4);
		Matrix4x4Type inv_mtx = matrix_inverse(mtx, mtx);
		Matrix4x4Type result = matrix_mul(mtx, inv_mtx);
		CHECK(vector_all_near_equal(vector_set(FloatType(1.0), FloatType(0.0), FloatType(0.0), FloatType(0.0)), result.x_axis, threshold));
		CHECK(vector_all_near_equal(vector_set(FloatType(0.0), FloatType(1.0), FloatType(0.0), FloatType(0.0)), result.y_axis, threshold));
		CHECK(vector_all_near_equal(vector_set(FloatType(0.0), FloatType(0.0), FloatType(1.0), FloatType(0.0)), result.z_axis, threshold));
		CHECK(vector_all_near_equal(vector_set(FloatType(0.0), FloatType(0.0), FloatType(0.0), FloatType(1.0)), result.w_axis, threshold));
	}

	{
		Vector4Type x_axis = vector_set(FloatType(0.1), FloatType(0.6), FloatType(0.123), FloatType(0.23));
		Vector4Type y_axis = vector_set(FloatType(0.512), FloatType(0.661), FloatType(0.133), FloatType(0.5));
		Vector4Type z_axis = vector_set(FloatType(0.14), FloatType(0.88), FloatType(0.193), FloatType(0.8));
		Vector4Type w_axis = vector_set(FloatType(0.2), FloatType(1.1), FloatType(2.4), FloatType(1.2));
		Matrix4x4Type mtx = matrix_set(x_axis, y_axis, z_axis, w_axis);
		Matrix4x4Type inv_mtx = matrix_inverse(mtx);
		CHECK(vector_all_near_equal(vector_set(FloatType(-0.782157), FloatType(2.46605), FloatType(-1.33225), FloatType(0.0105605)), inv_mtx.x_axis, threshold));
		CHECK(vector_all_near_equal(vector_set(FloatType(2.95727), FloatType(-0.418418), FloatType(-0.450484), FloatType(-0.0921463)), inv_mtx.y_axis, threshold));
		CHECK(vector_all_near_equal(vector_set(FloatType(0.304562), FloatType(-0.0319315), FloatType(-0.764003), FloatType(0.464266)), inv_mtx.z_axis, threshold));
		CHECK(vector_all_near_equal(vector_set(FloatType(-3.18959), FloatType(0.0364035), FloatType(2.16299), FloatType(-0.0124914)), inv_mtx.w_axis, threshold));
	}

	{
		Matrix4x4Type mtx = matrix_set(zero, zero, zero, zero);
		Matrix4x4Type inv_mtx = matrix_inverse(mtx, identity);
		CHECK(vector_all_near_equal(identity.x_axis, inv_mtx.x_axis, threshold));
		CHECK(vector_all_near_equal(identity.y_axis, inv_mtx.y_axis, threshold));
		CHECK(vector_all_near_equal(identity.z_axis, inv_mtx.z_axis, threshold));
		CHECK(vector_all_near_equal(identity.w_axis, inv_mtx.w_axis, threshold));
	}
}

template<typename FloatType>
static void test_matrix4x4_misc(const FloatType threshold)
{
	using Vector4Type = typename related_types<FloatType>::vector4;
	using Matrix4x4Type = typename related_types<FloatType>::matrix4x4;

	{
		Vector4Type x_axis = vector_set(FloatType(1.65424), FloatType(0.22921), FloatType(5.73038), FloatType(4.46541));
		Vector4Type y_axis = vector_set(FloatType(1.90220), FloatType(0.82590), FloatType(6.61556), FloatType(4.46383));
		Vector4Type z_axis = vector_set(FloatType(7.36288), FloatType(7.09841), FloatType(0.33519), FloatType(7.43985));
		Vector4Type w_axis = vector_set(FloatType(4.42391), FloatType(4.03858), FloatType(2.49537), FloatType(0.11255));
		Matrix4x4Type mtx = matrix_set(x_axis, y_axis, z_axis, w_axis);
		FloatType det = scalar_cast(matrix_determinant(mtx));
		CHECK(scalar_near_equal(det, FloatType(120.68779956246105324363), threshold));
	}

	{
		FloatType threshold2 = FloatType(1.0E-3F);
		Vector4Type x_axis = vector_set(FloatType(1.65424), FloatType(0.22921), FloatType(5.73038), FloatType(4.46541));
		Vector4Type y_axis = vector_set(FloatType(1.90220), FloatType(0.82590), FloatType(6.61556), FloatType(4.46383));
		Vector4Type z_axis = vector_set(FloatType(7.36288), FloatType(7.09841), FloatType(0.33519), FloatType(7.43985));
		Vector4Type w_axis = vector_set(FloatType(4.42391), FloatType(4.03858), FloatType(2.49537), FloatType(0.11255));
		Matrix4x4Type mtx = matrix_set(x_axis, y_axis, z_axis, w_axis);

		FloatType det_xx = scalar_cast(matrix_minor(mtx, axis4::x, axis4::x));
		CHECK(scalar_near_equal(det_xx, FloatType(251.213), threshold2));
		FloatType det_xy = scalar_cast(matrix_minor(mtx, axis4::x, axis4::y));
		CHECK(scalar_near_equal(det_xy, FloatType(252.409), threshold2));
		FloatType det_xz = scalar_cast(matrix_minor(mtx, axis4::x, axis4::z));
		CHECK(scalar_near_equal(det_xz, FloatType(-36.5778), threshold2));
		FloatType det_xw = scalar_cast(matrix_minor(mtx, axis4::x, axis4::w));
		CHECK(scalar_near_equal(det_xw, FloatType(6.1402), threshold2));

		FloatType det_yx = scalar_cast(matrix_minor(mtx, axis4::y, axis4::x));
		CHECK(scalar_near_equal(det_yx, FloatType(236.404), threshold2));
		FloatType det_yy = scalar_cast(matrix_minor(mtx, axis4::y, axis4::y));
		CHECK(scalar_near_equal(det_yy, FloatType(228.63), threshold2));
		FloatType det_yz = scalar_cast(matrix_minor(mtx, axis4::y, axis4::z));
		CHECK(scalar_near_equal(det_yz, FloatType(-48.4728), threshold2));
		FloatType det_yw = scalar_cast(matrix_minor(mtx, axis4::y, axis4::w));
		CHECK(scalar_near_equal(det_yw, FloatType(13.6377), threshold2));

		FloatType det_zx = scalar_cast(matrix_minor(mtx, axis4::z, axis4::x));
		CHECK(scalar_near_equal(det_zx, FloatType(-9.7121), threshold2));
		FloatType det_zy = scalar_cast(matrix_minor(mtx, axis4::z, axis4::y));
		CHECK(scalar_near_equal(det_zy, FloatType(-14.752), threshold2));
		FloatType det_zz = scalar_cast(matrix_minor(mtx, axis4::z, axis4::z));
		CHECK(scalar_near_equal(det_zz, FloatType(-7.20201), threshold2));
		FloatType det_zw = scalar_cast(matrix_minor(mtx, axis4::z, axis4::w));
		CHECK(scalar_near_equal(det_zw, FloatType(-12.0829), threshold2));

		FloatType det_wx = scalar_cast(matrix_minor(mtx, axis4::w, axis4::x));
		CHECK(scalar_near_equal(det_wx, FloatType(-51.1582), threshold2));
		FloatType det_wy = scalar_cast(matrix_minor(mtx, axis4::w, axis4::y));
		CHECK(scalar_near_equal(det_wy, FloatType(-28.475), threshold2));
		FloatType det_wz = scalar_cast(matrix_minor(mtx, axis4::w, axis4::z));
		CHECK(scalar_near_equal(det_wz, FloatType(-4.82179), threshold2));
		FloatType det_ww = scalar_cast(matrix_minor(mtx, axis4::w, axis4::w));
		CHECK(scalar_near_equal(det_ww, FloatType(-23.678), threshold2));
	}

	{
		FloatType threshold2 = FloatType(1.0E-3F);
		Vector4Type x_axis = vector_set(FloatType(1.65424), FloatType(0.22921), FloatType(5.73038), FloatType(4.46541));
		Vector4Type y_axis = vector_set(FloatType(1.90220), FloatType(0.82590), FloatType(6.61556), FloatType(4.46383));
		Vector4Type z_axis = vector_set(FloatType(7.36288), FloatType(7.09841), FloatType(0.33519), FloatType(7.43985));
		Vector4Type w_axis = vector_set(FloatType(4.42391), FloatType(4.03858), FloatType(2.49537), FloatType(0.11255));
		Matrix4x4Type mtx = matrix_set(x_axis, y_axis, z_axis, w_axis);
		Matrix4x4Type cof = matrix_cofactor(mtx);

		FloatType cof_xx = scalar_cast(vector_get_x_as_scalar(cof.x_axis));
		CHECK(scalar_near_equal(cof_xx, FloatType(251.213), threshold2));
		FloatType cof_xy = scalar_cast(vector_get_y_as_scalar(cof.x_axis));
		CHECK(scalar_near_equal(cof_xy, FloatType(-252.409), threshold2));
		FloatType cof_xz = scalar_cast(vector_get_z_as_scalar(cof.x_axis));
		CHECK(scalar_near_equal(cof_xz, FloatType(-36.5778), threshold2));
		FloatType cof_xw = scalar_cast(vector_get_w_as_scalar(cof.x_axis));
		CHECK(scalar_near_equal(cof_xw, FloatType(-6.1402), threshold2));

		FloatType cof_yx = scalar_cast(vector_get_x_as_scalar(cof.y_axis));
		CHECK(scalar_near_equal(cof_yx, FloatType(-236.404), threshold2));
		FloatType cof_yy = scalar_cast(vector_get_y_as_scalar(cof.y_axis));
		CHECK(scalar_near_equal(cof_yy, FloatType(228.63), threshold2));
		FloatType cof_yz = scalar_cast(vector_get_z_as_scalar(cof.y_axis));
		CHECK(scalar_near_equal(cof_yz, FloatType(48.4728), threshold2));
		FloatType cof_yw = scalar_cast(vector_get_w_as_scalar(cof.y_axis));
		CHECK(scalar_near_equal(cof_yw, FloatType(13.6377), threshold2));

		FloatType cof_zx = scalar_cast(vector_get_x_as_scalar(cof.z_axis));
		CHECK(scalar_near_equal(cof_zx, FloatType(-9.7121), threshold2));
		FloatType cof_zy = scalar_cast(vector_get_y_as_scalar(cof.z_axis));
		CHECK(scalar_near_equal(cof_zy, FloatType(14.752), threshold2));
		FloatType cof_zz = scalar_cast(vector_get_z_as_scalar(cof.z_axis));
		CHECK(scalar_near_equal(cof_zz, FloatType(-7.20201), threshold2));
		FloatType cof_zw = scalar_cast(vector_get_w_as_scalar(cof.z_axis));
		CHECK(scalar_near_equal(cof_zw, FloatType(12.0829), threshold2));

		FloatType cof_wx = scalar_cast(vector_get_x_as_scalar(cof.w_axis));
		CHECK(scalar_near_equal(cof_wx, FloatType(51.1582), threshold2));
		FloatType cof_wy = scalar_cast(vector_get_y_as_scalar(cof.w_axis));
		CHECK(scalar_near_equal(cof_wy, FloatType(-28.475), threshold2));
		FloatType cof_wz = scalar_cast(vector_get_z_as_scalar(cof.w_axis));
		CHECK(scalar_near_equal(cof_wz, FloatType(4.82179), threshold2));
		FloatType cof_ww = scalar_cast(vector_get_w_as_scalar(cof.w_axis));
		CHECK(scalar_near_equal(cof_ww, FloatType(-23.678), threshold2));
	}

	{
		FloatType threshold2 = FloatType(1.0E-3F);
		Vector4Type x_axis = vector_set(FloatType(1.65424), FloatType(0.22921), FloatType(5.73038), FloatType(4.46541));
		Vector4Type y_axis = vector_set(FloatType(1.90220), FloatType(0.82590), FloatType(6.61556), FloatType(4.46383));
		Vector4Type z_axis = vector_set(FloatType(7.36288), FloatType(7.09841), FloatType(0.33519), FloatType(7.43985));
		Vector4Type w_axis = vector_set(FloatType(4.42391), FloatType(4.03858), FloatType(2.49537), FloatType(0.11255));
		Matrix4x4Type mtx = matrix_set(x_axis, y_axis, z_axis, w_axis);
		Matrix4x4Type adj = matrix_adjugate(mtx);

		FloatType adj_xx = scalar_cast(vector_get_x_as_scalar(adj.x_axis));
		CHECK(scalar_near_equal(adj_xx, FloatType(251.213), threshold2));
		FloatType adj_xy = scalar_cast(vector_get_y_as_scalar(adj.x_axis));
		CHECK(scalar_near_equal(adj_xy, FloatType(-236.404), threshold2));
		FloatType adj_xz = scalar_cast(vector_get_z_as_scalar(adj.x_axis));
		CHECK(scalar_near_equal(adj_xz, FloatType(-9.7121), threshold2));
		FloatType adj_xw = scalar_cast(vector_get_w_as_scalar(adj.x_axis));
		CHECK(scalar_near_equal(adj_xw, FloatType(51.1582), threshold2));

		FloatType adj_yx = scalar_cast(vector_get_x_as_scalar(adj.y_axis));
		CHECK(scalar_near_equal(adj_yx, FloatType(-252.409), threshold2));
		FloatType adj_yy = scalar_cast(vector_get_y_as_scalar(adj.y_axis));
		CHECK(scalar_near_equal(adj_yy, FloatType(228.63), threshold2));
		FloatType adj_yz = scalar_cast(vector_get_z_as_scalar(adj.y_axis));
		CHECK(scalar_near_equal(adj_yz, FloatType(14.752), threshold2));
		FloatType adj_yw = scalar_cast(vector_get_w_as_scalar(adj.y_axis));
		CHECK(scalar_near_equal(adj_yw, FloatType(-28.475), threshold2));

		FloatType adj_zx = scalar_cast(vector_get_x_as_scalar(adj.z_axis));
		CHECK(scalar_near_equal(adj_zx, FloatType(-36.5778), threshold2));
		FloatType adj_zy = scalar_cast(vector_get_y_as_scalar(adj.z_axis));
		CHECK(scalar_near_equal(adj_zy, FloatType(48.4728), threshold2));
		FloatType adj_zz = scalar_cast(vector_get_z_as_scalar(adj.z_axis));
		CHECK(scalar_near_equal(adj_zz, FloatType(-7.20201), threshold2));
		FloatType adj_zw = scalar_cast(vector_get_w_as_scalar(adj.z_axis));
		CHECK(scalar_near_equal(adj_zw, FloatType(4.82179), threshold2));

		FloatType adj_wx = scalar_cast(vector_get_x_as_scalar(adj.w_axis));
		CHECK(scalar_near_equal(adj_wx, FloatType(-6.1402), threshold2));
		FloatType adj_wy = scalar_cast(vector_get_y_as_scalar(adj.w_axis));
		CHECK(scalar_near_equal(adj_wy, FloatType(13.6377), threshold2));
		FloatType adj_wz = scalar_cast(vector_get_z_as_scalar(adj.w_axis));
		CHECK(scalar_near_equal(adj_wz, FloatType(12.0829), threshold2));
		FloatType adj_ww = scalar_cast(vector_get_w_as_scalar(adj.w_axis));
		CHECK(scalar_near_equal(adj_ww, FloatType(-23.678), threshold2));
	}
}

TEST_CASE("matrix4x4f math", "[math][matrix4x4]")
{
	test_matrix4x4_setters<float>(1.0E-4F);
	test_matrix4x4_arithmetic<float>(1.0E-4F);
	test_matrix4x4_transformations<float>(1.0E-4F);
	test_matrix4x4_misc<float>(1.0E-4F);

	{
		quatf rotation_around_z = quat_from_euler(scalar_deg_to_rad(0.0F), scalar_deg_to_rad(90.0F), scalar_deg_to_rad(0.0F));
		vector4f translation = vector_set(1.0F, 2.0F, 3.0F);
		vector4f scale = vector_set(4.0F, 5.0F, 6.0F);
		matrix3x4f src3x4 = matrix_from_qvv(rotation_around_z, translation, scale);

		matrix4x4f src = matrix_cast(src3x4);
		CHECK((float)vector_get_w(src.x_axis) == 0.0F);
		CHECK((float)vector_get_w(src.y_axis) == 0.0F);
		CHECK((float)vector_get_w(src.z_axis) == 0.0F);
		CHECK((float)vector_get_w(src.w_axis) == 1.0F);

		matrix4x4d dst = matrix_cast(src);
		CHECK(vector_all_near_equal(vector_cast(src.x_axis), dst.x_axis, 1.0E-4));
		CHECK(vector_all_near_equal(vector_cast(src.y_axis), dst.y_axis, 1.0E-4));
		CHECK(vector_all_near_equal(vector_cast(src.z_axis), dst.z_axis, 1.0E-4));
		CHECK(vector_all_near_equal(vector_cast(src.w_axis), dst.w_axis, 1.0E-4));
	}
}

TEST_CASE("matrix4x4d math", "[math][matrix4x4]")
{
	test_matrix4x4_setters<double>(1.0E-4);
	test_matrix4x4_arithmetic<double>(1.0E-4);
	test_matrix4x4_transformations<double>(1.0E-4);
	test_matrix4x4_misc<double>(1.0E-4);

	{
		quatd rotation_around_z = quat_from_euler(scalar_deg_to_rad(0.0), scalar_deg_to_rad(90.0), scalar_deg_to_rad(0.0));
		vector4d translation = vector_set(1.0, 2.0, 3.0);
		vector4d scale = vector_set(4.0, 5.0, 6.0);
		matrix3x4d src3x4 = matrix_from_qvv(rotation_around_z, translation, scale);

		matrix4x4d src = matrix_cast(src3x4);
		CHECK((double)vector_get_w(src.x_axis) == 0.0);
		CHECK((double)vector_get_w(src.y_axis) == 0.0);
		CHECK((double)vector_get_w(src.z_axis) == 0.0);
		CHECK((double)vector_get_w(src.w_axis) == 1.0);

		matrix4x4f dst = matrix_cast(src);
		CHECK(vector_all_near_equal(vector_cast(src.x_axis), dst.x_axis, 1.0E-4F));
		CHECK(vector_all_near_equal(vector_cast(src.y_axis), dst.y_axis, 1.0E-4F));
		CHECK(vector_all_near_equal(vector_cast(src.z_axis), dst.z_axis, 1.0E-4F));
		CHECK(vector_all_near_equal(vector_cast(src.w_axis), dst.w_axis, 1.0E-4F));
	}
}

```

`tests/sources/test_memory_utils.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2017 Nicholas Frechette & Animation Compression Library contributors
// Copyright (c) 2018 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include "catch2.impl.h"

#include <rtm/impl/memory_utils.h>

#include <cstdint>
#include <cstring>
#include <limits>

using namespace rtm;
using namespace rtm::rtm_impl;

TEST_CASE("misc tests", "[core][memory]")
{
	size_t num_powers_of_two = 0;
	for (size_t i = 0; i <= 65536; ++i)
	{
		if (is_power_of_two(i))
			num_powers_of_two++;
	}

	CHECK(num_powers_of_two == 17);
	CHECK(is_power_of_two(1) == true);
	CHECK(is_power_of_two(2) == true);
	CHECK(is_power_of_two(4) == true);
	CHECK(is_power_of_two(8) == true);
	CHECK(is_power_of_two(16) == true);
	CHECK(is_power_of_two(32) == true);
	CHECK(is_power_of_two(64) == true);
	CHECK(is_power_of_two(128) == true);
	CHECK(is_power_of_two(256) == true);
	CHECK(is_power_of_two(512) == true);
	CHECK(is_power_of_two(1024) == true);
	CHECK(is_power_of_two(2048) == true);
	CHECK(is_power_of_two(4096) == true);
	CHECK(is_power_of_two(8192) == true);
	CHECK(is_power_of_two(16384) == true);
	CHECK(is_power_of_two(32768) == true);
	CHECK(is_power_of_two(65536) == true);

	CHECK(is_alignment_valid<int32_t>(0) == false);
	CHECK(is_alignment_valid<int32_t>(4) == true);
	CHECK(is_alignment_valid<int32_t>(8) == true);
	CHECK(is_alignment_valid<int32_t>(2) == false);
	CHECK(is_alignment_valid<int32_t>(5) == false);
	CHECK(is_alignment_valid<int64_t>(8) == true);
	CHECK(is_alignment_valid<int64_t>(16) == true);

	struct alignas(8) Tmp
	{
		int32_t padding;	// Aligned to 8 bytes
		int32_t value;		// Aligned to 4 bytes
	};
	Tmp tmp;
	CHECK(is_aligned_to(&tmp.padding, 8) == true);
	CHECK(is_aligned_to(&tmp.value, 4) == true);
	CHECK(is_aligned_to(&tmp.value, 2) == true);
	CHECK(is_aligned_to(&tmp.value, 1) == true);
	CHECK(is_aligned_to(&tmp.value, 8) == false);

	CHECK(is_aligned_to(4, 4) == true);
	CHECK(is_aligned_to(4, 2) == true);
	CHECK(is_aligned_to(4, 1) == true);
	CHECK(is_aligned_to(4, 8) == false);
	CHECK(is_aligned_to(6, 4) == false);
	CHECK(is_aligned_to(6, 2) == true);
	CHECK(is_aligned_to(6, 1) == true);

	CHECK(is_aligned_to(align_to(5, 4), 4) == true);
	CHECK(align_to(5, 4) == 8);
	CHECK(is_aligned_to(align_to(8, 4), 4) == true);
	CHECK(align_to(8, 4) == 8);

	void* ptr = (void*)0x00000000;
	CHECK(align_to(ptr, 4) == (void*)0x00000000);
	CHECK(align_to(ptr, 8) == (void*)0x00000000);
	ptr = (void*)0x00000001;
	CHECK(align_to(ptr, 4) == (void*)0x00000004);
	CHECK(align_to(ptr, 8) == (void*)0x00000008);
	ptr = (void*)0x00000004;
	CHECK(align_to(ptr, 4) == (void*)0x00000004);
	CHECK(align_to(ptr, 8) == (void*)0x00000008);

	int32_t array[8];
	CHECK(get_array_size(array) == (sizeof(array) / sizeof(array[0])));
}

TEST_CASE("raw memory support", "[core][memory]")
{
	uint32_t value32 = 0xAB78FE04;
	uint8_t unaligned_value_buffer[5] = { 0x00, 0x00, 0x00, 0x00, 0x00 };
	std::memcpy(&unaligned_value_buffer[1], &value32, sizeof(uint32_t));
	CHECK(unaligned_read<uint32_t>(&unaligned_value_buffer[1]) == value32);
}

enum class UnsignedEnum : uint32_t
{
	ZERO = 0,
	U16_MAX = (std::numeric_limits<uint16_t>::max)(),
	U32_MAX = (std::numeric_limits<uint32_t>::max)(),
};

enum class SignedEnum : int32_t
{
	I32_MIN = (std::numeric_limits<int32_t>::min)(),
	I16_MIN = (std::numeric_limits<int16_t>::min)(),
	I16_MAX = (std::numeric_limits<int16_t>::max)(),
	I32_MAX = (std::numeric_limits<int32_t>::max)(),
};

TEST_CASE("safe_static_cast from unsigned enum", "[core][memory]")
{
	CHECK(safe_static_cast<uint32_t>(UnsignedEnum::ZERO) == 0);
	CHECK(safe_static_cast<uint32_t>(UnsignedEnum::U16_MAX) == (std::numeric_limits<uint16_t>::max)());
	CHECK(safe_static_cast<uint32_t>(UnsignedEnum::U32_MAX) == (std::numeric_limits<uint32_t>::max)());

	CHECK(safe_static_cast<int32_t>(UnsignedEnum::ZERO) == 0);
	CHECK(safe_static_cast<int32_t>(UnsignedEnum::U16_MAX));
	CHECK_THROWS(safe_static_cast<int32_t>(UnsignedEnum::U32_MAX));

	CHECK(safe_static_cast<uint16_t>(UnsignedEnum::ZERO) == 0);
	CHECK(safe_static_cast<uint16_t>(UnsignedEnum::U16_MAX) == (std::numeric_limits<uint16_t>::max)());
	CHECK_THROWS(safe_static_cast<uint16_t>(UnsignedEnum::U32_MAX));

	CHECK(safe_static_cast<int16_t>(UnsignedEnum::ZERO) == 0);
	CHECK_THROWS(safe_static_cast<int16_t>(UnsignedEnum::U16_MAX));
	CHECK_THROWS(safe_static_cast<int16_t>(UnsignedEnum::U32_MAX));
}

TEST_CASE("safe_static_cast from signed enum", "[core][memory]")
{
	CHECK_THROWS(safe_static_cast<uint32_t>(SignedEnum::I32_MIN));
	CHECK_THROWS(safe_static_cast<uint32_t>(SignedEnum::I16_MIN));
	CHECK(safe_static_cast<uint32_t>(SignedEnum::I16_MAX) == safe_static_cast<uint32_t>((std::numeric_limits<int16_t>::max)()));
	CHECK(safe_static_cast<uint32_t>(SignedEnum::I32_MAX) == safe_static_cast<uint32_t>((std::numeric_limits<int32_t>::max)()));

	CHECK(safe_static_cast<int32_t>(SignedEnum::I32_MIN) == (std::numeric_limits<int32_t>::min)());
	CHECK(safe_static_cast<int32_t>(SignedEnum::I16_MIN) == (std::numeric_limits<int16_t>::min)());
	CHECK(safe_static_cast<int32_t>(SignedEnum::I16_MAX) == (std::numeric_limits<int16_t>::max)());
	CHECK(safe_static_cast<int32_t>(SignedEnum::I32_MAX) == (std::numeric_limits<int32_t>::max)());

	CHECK_THROWS(safe_static_cast<uint16_t>(SignedEnum::I32_MIN));
	CHECK_THROWS(safe_static_cast<uint16_t>(SignedEnum::I16_MIN));
	CHECK(safe_static_cast<uint16_t>(SignedEnum::I16_MAX) == (std::numeric_limits<int16_t>::max)());
	CHECK_THROWS(safe_static_cast<uint16_t>(SignedEnum::I32_MAX));

	CHECK_THROWS(safe_static_cast<int16_t>(SignedEnum::I32_MIN));
	CHECK(safe_static_cast<int16_t>(SignedEnum::I16_MIN) == (std::numeric_limits<int16_t>::min)());
	CHECK(safe_static_cast<int16_t>(SignedEnum::I16_MAX) == (std::numeric_limits<int16_t>::max)());
	CHECK_THROWS(safe_static_cast<int16_t>(SignedEnum::I32_MAX));
}

TEST_CASE("safe_static_cast from signed int", "[core][memory]")
{
	CHECK_THROWS(safe_static_cast<uint32_t>((std::numeric_limits<int32_t>::min)()));
	CHECK_THROWS(safe_static_cast<uint32_t>((std::numeric_limits<int16_t>::min)()));
	CHECK(safe_static_cast<uint32_t>((std::numeric_limits<int16_t>::max)()) == safe_static_cast<uint32_t>((std::numeric_limits<int16_t>::max)()));
	CHECK(safe_static_cast<uint32_t>((std::numeric_limits<int32_t>::max)()) == safe_static_cast<uint32_t>((std::numeric_limits<int32_t>::max)()));

	CHECK(safe_static_cast<int32_t>((std::numeric_limits<int32_t>::min)()) == (std::numeric_limits<int32_t>::min)());
	CHECK(safe_static_cast<int32_t>((std::numeric_limits<int16_t>::min)()) == (std::numeric_limits<int16_t>::min)());
	CHECK(safe_static_cast<int32_t>((std::numeric_limits<int16_t>::max)()) == (std::numeric_limits<int16_t>::max)());
	CHECK(safe_static_cast<int32_t>((std::numeric_limits<int32_t>::max)()) == (std::numeric_limits<int32_t>::max)());

	CHECK_THROWS(safe_static_cast<uint16_t>((std::numeric_limits<int32_t>::min)()));
	CHECK_THROWS(safe_static_cast<uint16_t>((std::numeric_limits<int16_t>::min)()));
	CHECK(safe_static_cast<uint16_t>((std::numeric_limits<int16_t>::max)()) == (std::numeric_limits<int16_t>::max)());
	CHECK_THROWS(safe_static_cast<uint16_t>((std::numeric_limits<int32_t>::max)()));

	CHECK_THROWS(safe_static_cast<int16_t>((std::numeric_limits<int32_t>::min)()));
	CHECK(safe_static_cast<int16_t>((std::numeric_limits<int16_t>::min)()) == (std::numeric_limits<int16_t>::min)());
	CHECK(safe_static_cast<int16_t>((std::numeric_limits<int16_t>::max)()) == (std::numeric_limits<int16_t>::max)());
	CHECK_THROWS(safe_static_cast<int16_t>((std::numeric_limits<int32_t>::max)()));
}

TEST_CASE("safe_static_cast from unsigned int", "[core][memory]")
{
	CHECK(safe_static_cast<uint32_t>(0U) == 0);
	CHECK(safe_static_cast<uint32_t>((std::numeric_limits<uint16_t>::max)()) == (std::numeric_limits<uint16_t>::max)());
	CHECK(safe_static_cast<uint32_t>((std::numeric_limits<uint32_t>::max)()) == (std::numeric_limits<uint32_t>::max)());

	CHECK(safe_static_cast<int32_t>(0U) == 0);
	CHECK(safe_static_cast<int32_t>((std::numeric_limits<uint16_t>::max)()) == (std::numeric_limits<uint16_t>::max)());
	CHECK_THROWS(safe_static_cast<int32_t>((std::numeric_limits<uint32_t>::max)()));

	CHECK(safe_static_cast<uint16_t>(0U) == 0);
	CHECK(safe_static_cast<uint16_t>((std::numeric_limits<uint16_t>::max)()) == (std::numeric_limits<uint16_t>::max)());
	CHECK_THROWS(safe_static_cast<uint16_t>((std::numeric_limits<uint32_t>::max)()));

	CHECK(safe_static_cast<int16_t>(0U) == 0);
	CHECK_THROWS(safe_static_cast<int16_t>((std::numeric_limits<uint16_t>::max)()));
	CHECK_THROWS(safe_static_cast<int16_t>((std::numeric_limits<uint32_t>::max)()));
}

TEST_CASE("safe_static_cast from double", "[core][memory]")
{
	// Use volatile to avoid a constant being truncated warning
	volatile double value = -(std::numeric_limits<double>::max)();
	CHECK_THROWS(safe_static_cast<float>(value));
	value = (std::numeric_limits<double>::max)();
	CHECK_THROWS(safe_static_cast<float>(value));

	CHECK(safe_static_cast<float>(-(std::numeric_limits<float>::max)()) == -(std::numeric_limits<float>::max)());
	CHECK(safe_static_cast<float>((std::numeric_limits<float>::max)()) == (std::numeric_limits<float>::max)());
}

```

`tests/sources/test_packing_quat.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2019 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include "catch2.impl.h"

#include <rtm/type_traits.h>
#include <rtm/packing/quatf.h>
#include <rtm/packing/quatd.h>

using namespace rtm;

template<typename FloatType>
static void test_quat_impl(const FloatType threshold)
{
	using QuatType = typename related_types<FloatType>::quat;
	using Vector4Type = typename related_types<FloatType>::vector4;

	{
		QuatType quat0 = quat_set(FloatType(0.39564531008956383), FloatType(0.044254239301713752), FloatType(0.22768840967675355), FloatType(-0.88863059760894492));
		QuatType quat1 = quat_ensure_positive_w(quat0);
		QuatType quat2 = quat_ensure_positive_w(quat1);

		CHECK(FloatType(quat_get_x(quat0)) == -FloatType(quat_get_x(quat1)));
		CHECK(FloatType(quat_get_y(quat0)) == -FloatType(quat_get_y(quat1)));
		CHECK(FloatType(quat_get_z(quat0)) == -FloatType(quat_get_z(quat1)));
		CHECK(FloatType(quat_get_w(quat0)) == -FloatType(quat_get_w(quat1)));

		CHECK(FloatType(quat_get_x(quat2)) == FloatType(quat_get_x(quat1)));
		CHECK(FloatType(quat_get_y(quat2)) == FloatType(quat_get_y(quat1)));
		CHECK(FloatType(quat_get_z(quat2)) == FloatType(quat_get_z(quat1)));
		CHECK(FloatType(quat_get_w(quat2)) == FloatType(quat_get_w(quat1)));

		Vector4Type vec1 = quat_to_vector(quat1);
		QuatType quat3 = quat_from_positive_w(vec1);
		CHECK(FloatType(quat_get_x(quat1)) == FloatType(quat_get_x(quat3)));
		CHECK(FloatType(quat_get_y(quat1)) == FloatType(quat_get_y(quat3)));
		CHECK(FloatType(quat_get_z(quat1)) == FloatType(quat_get_z(quat3)));
		CHECK(scalar_near_equal(FloatType(quat_get_w(quat1)), FloatType(quat_get_w(quat3)), threshold));
	}
}

TEST_CASE("quatf packing math", "[math][quat][packing]")
{
	test_quat_impl<float>(1.0E-4F);
}

TEST_CASE("quatd packing math", "[math][quat][packing]")
{
	test_quat_impl<double>(1.0E-6);
}

```

`tests/sources/test_quat.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2018 Nicholas Frechette & Animation Compression Library contributors
// Copyright (c) 2018 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include "catch2.impl.h"

#include <rtm/type_traits.h>
#include <rtm/mask4f.h>
#include <rtm/mask4d.h>
#include <rtm/quatf.h>
#include <rtm/quatd.h>
#include <rtm/vector4f.h>
#include <rtm/vector4d.h>
#include <rtm/impl/bit_cast.impl.h>

#include <array>
#include <limits>

using namespace rtm;

template<typename QuatType, typename Vector4Type, typename FloatType>
static Vector4Type quat_rotate_scalar(const Vector4Type& vector, const QuatType& rotation)
{
	// (q.W*q.W-qv.qv)v + 2(qv.v)qv + 2 q.W (qv x v)
	FloatType rotation_x = quat_get_x(rotation);
	FloatType rotation_y = quat_get_y(rotation);
	FloatType rotation_z = quat_get_z(rotation);
	Vector4Type qv = vector_set(rotation_x, rotation_y, rotation_z);
	Vector4Type vOut = vector_mul(vector_cross3(qv, vector), FloatType(2.0) * quat_get_w(rotation));
	vOut = vector_add(vOut, vector_mul(vector, (quat_get_w(rotation) * quat_get_w(rotation)) - vector_dot(qv, qv)));
	vOut = vector_add(vOut, vector_mul(qv, FloatType(2.0) * vector_dot(qv, vector)));
	return vOut;
}

template<typename QuatType, typename FloatType>
static QuatType quat_add_scalar(const QuatType& lhs, const QuatType& rhs)
{
	FloatType lhs_raw[4] = { quat_get_x(lhs), quat_get_y(lhs), quat_get_z(lhs), quat_get_w(lhs) };
	FloatType rhs_raw[4] = { quat_get_x(rhs), quat_get_y(rhs), quat_get_z(rhs), quat_get_w(rhs) };

	FloatType x = lhs_raw[0] + rhs_raw[0];
	FloatType y = lhs_raw[1] + rhs_raw[1];
	FloatType z = lhs_raw[2] + rhs_raw[2];
	FloatType w = lhs_raw[3] + rhs_raw[3];

	return quat_set(x, y, z, w);
}

template<typename QuatType, typename FloatType>
static QuatType quat_mul_scalar(const QuatType& lhs, const QuatType& rhs)
{
	FloatType lhs_raw[4] = { quat_get_x(lhs), quat_get_y(lhs), quat_get_z(lhs), quat_get_w(lhs) };
	FloatType rhs_raw[4] = { quat_get_x(rhs), quat_get_y(rhs), quat_get_z(rhs), quat_get_w(rhs) };

	FloatType x = (rhs_raw[3] * lhs_raw[0]) + (rhs_raw[0] * lhs_raw[3]) + (rhs_raw[1] * lhs_raw[2]) - (rhs_raw[2] * lhs_raw[1]);
	FloatType y = (rhs_raw[3] * lhs_raw[1]) - (rhs_raw[0] * lhs_raw[2]) + (rhs_raw[1] * lhs_raw[3]) + (rhs_raw[2] * lhs_raw[0]);
	FloatType z = (rhs_raw[3] * lhs_raw[2]) + (rhs_raw[0] * lhs_raw[1]) - (rhs_raw[1] * lhs_raw[0]) + (rhs_raw[2] * lhs_raw[3]);
	FloatType w = (rhs_raw[3] * lhs_raw[3]) - (rhs_raw[0] * lhs_raw[0]) - (rhs_raw[1] * lhs_raw[1]) - (rhs_raw[2] * lhs_raw[2]);

	return quat_set(x, y, z, w);
}

template<typename QuatType, typename FloatType>
static QuatType quat_mul_scalar(const QuatType& lhs, FloatType rhs)
{
	FloatType lhs_raw[4] = { quat_get_x(lhs), quat_get_y(lhs), quat_get_z(lhs), quat_get_w(lhs) };

	FloatType x = lhs_raw[0] * rhs;
	FloatType y = lhs_raw[1] * rhs;
	FloatType z = lhs_raw[2] * rhs;
	FloatType w = lhs_raw[3] * rhs;

	return quat_set(x, y, z, w);
}

template<typename QuatType, typename FloatType>
static FloatType scalar_dot(const QuatType& lhs, const QuatType& rhs)
{
	return (quat_get_x(lhs) * quat_get_x(rhs)) + (quat_get_y(lhs) * quat_get_y(rhs)) + (quat_get_z(lhs) * quat_get_z(rhs)) + (quat_get_w(lhs) * quat_get_w(rhs));
}

template<typename QuatType, typename FloatType>
static QuatType scalar_normalize(const QuatType& input)
{
	FloatType inv_len = FloatType(1.0) / rtm::scalar_sqrt(scalar_dot<QuatType, FloatType>(input, input));
	return quat_set(quat_get_x(input) * inv_len, quat_get_y(input) * inv_len, quat_get_z(input) * inv_len, quat_get_w(input) * inv_len);
}

template<typename QuatType, typename FloatType>
static QuatType scalar_lerp(const QuatType& start, const QuatType& end, FloatType alpha)
{
	FloatType dot = scalar_dot<QuatType, FloatType>(start, end);
	FloatType bias = dot >= FloatType(0.0) ? FloatType(1.0) : FloatType(-1.0);
	FloatType x = quat_get_x(start) + ((quat_get_x(end) * bias) - quat_get_x(start)) * alpha;
	FloatType y = quat_get_y(start) + ((quat_get_y(end) * bias) - quat_get_y(start)) * alpha;
	FloatType z = quat_get_z(start) + ((quat_get_z(end) * bias) - quat_get_z(start)) * alpha;
	FloatType w = quat_get_w(start) + ((quat_get_w(end) * bias) - quat_get_w(start)) * alpha;
	return quat_normalize(quat_set(x, y, z, w));
}

template<typename FloatType>
static void test_quat_impl(const FloatType threshold)
{
	using QuatType = typename related_types<FloatType>::quat;
	using Vector4Type = typename related_types<FloatType>::vector4;
	using Float4Type = typename related_types<FloatType>::float4;
	using ScalarType = typename related_types<FloatType>::scalar;
	using MaskType = typename related_types<FloatType>::mask4;

	const Vector4Type zero = vector_zero();
	const QuatType identity = quat_identity();

	//////////////////////////////////////////////////////////////////////////
	// Setters, getters, and casts

	CHECK(FloatType(quat_get_x(quat_set(FloatType(0.0), FloatType(2.34), FloatType(-3.12), FloatType(10000.0)))) == FloatType(0.0));
	CHECK(FloatType(quat_get_y(quat_set(FloatType(0.0), FloatType(2.34), FloatType(-3.12), FloatType(10000.0)))) == FloatType(2.34));
	CHECK(FloatType(quat_get_z(quat_set(FloatType(0.0), FloatType(2.34), FloatType(-3.12), FloatType(10000.0)))) == FloatType(-3.12));
	CHECK(FloatType(quat_get_w(quat_set(FloatType(0.0), FloatType(2.34), FloatType(-3.12), FloatType(10000.0)))) == FloatType(10000.0));

	CHECK(scalar_cast(quat_get_x_as_scalar(quat_set(FloatType(0.0), FloatType(2.34), FloatType(-3.12), FloatType(10000.0)))) == FloatType(0.0));
	CHECK(scalar_cast(quat_get_y_as_scalar(quat_set(FloatType(0.0), FloatType(2.34), FloatType(-3.12), FloatType(10000.0)))) == FloatType(2.34));
	CHECK(scalar_cast(quat_get_z_as_scalar(quat_set(FloatType(0.0), FloatType(2.34), FloatType(-3.12), FloatType(10000.0)))) == FloatType(-3.12));
	CHECK(scalar_cast(quat_get_w_as_scalar(quat_set(FloatType(0.0), FloatType(2.34), FloatType(-3.12), FloatType(10000.0)))) == FloatType(10000.0));

	CHECK(FloatType(quat_get_x(identity)) == FloatType(0.0));
	CHECK(FloatType(quat_get_y(identity)) == FloatType(0.0));
	CHECK(FloatType(quat_get_z(identity)) == FloatType(0.0));
	CHECK(FloatType(quat_get_w(identity)) == FloatType(1.0));

	CHECK(quat_near_equal(quat_set_x(identity, FloatType(4.0)), quat_set(FloatType(4.0), FloatType(0.0), FloatType(0.0), FloatType(1.0)), FloatType(0.0)));
	CHECK(quat_near_equal(quat_set_y(identity, FloatType(4.0)), quat_set(FloatType(0.0), FloatType(4.0), FloatType(0.0), FloatType(1.0)), FloatType(0.0)));
	CHECK(quat_near_equal(quat_set_z(identity, FloatType(4.0)), quat_set(FloatType(0.0), FloatType(0.0), FloatType(4.0), FloatType(1.0)), FloatType(0.0)));
	CHECK(quat_near_equal(quat_set_w(identity, FloatType(4.0)), quat_set(FloatType(0.0), FloatType(0.0), FloatType(0.0), FloatType(4.0)), FloatType(0.0)));

	CHECK(quat_near_equal(quat_set_x(identity, scalar_set(FloatType(4.0))), quat_set(FloatType(4.0), FloatType(0.0), FloatType(0.0), FloatType(1.0)), FloatType(0.0)));
	CHECK(quat_near_equal(quat_set_y(identity, scalar_set(FloatType(4.0))), quat_set(FloatType(0.0), FloatType(4.0), FloatType(0.0), FloatType(1.0)), FloatType(0.0)));
	CHECK(quat_near_equal(quat_set_z(identity, scalar_set(FloatType(4.0))), quat_set(FloatType(0.0), FloatType(0.0), FloatType(4.0), FloatType(1.0)), FloatType(0.0)));
	CHECK(quat_near_equal(quat_set_w(identity, scalar_set(FloatType(4.0))), quat_set(FloatType(0.0), FloatType(0.0), FloatType(0.0), FloatType(4.0)), FloatType(0.0)));

	{
		struct alignas(16) Tmp
		{
			uint8_t padding0[8];	//  8 |  8
			FloatType values[4];	// 24 | 40
			uint8_t padding1[8];	// 32 | 48
		};

		Tmp tmp = { { 0 }, { FloatType(0.0), FloatType(2.34), FloatType(-3.12), FloatType(10000.0) }, {} };
		CHECK(FloatType(quat_get_x(quat_load(&tmp.values[0]))) == tmp.values[0]);
		CHECK(FloatType(quat_get_y(quat_load(&tmp.values[0]))) == tmp.values[1]);
		CHECK(FloatType(quat_get_z(quat_load(&tmp.values[0]))) == tmp.values[2]);
		CHECK(FloatType(quat_get_w(quat_load(&tmp.values[0]))) == tmp.values[3]);
	}

	{
		Float4Type tmpf4 = { FloatType(0.0), FloatType(2.34), FloatType(-3.12), FloatType(10000.0) };
		CHECK(FloatType(quat_get_x(quat_load(&tmpf4))) == tmpf4.x);
		CHECK(FloatType(quat_get_y(quat_load(&tmpf4))) == tmpf4.y);
		CHECK(FloatType(quat_get_z(quat_load(&tmpf4))) == tmpf4.z);
		CHECK(FloatType(quat_get_w(quat_load(&tmpf4))) == tmpf4.w);
	}

	{
		const Vector4Type vec = vector_set(FloatType(0.0), FloatType(2.34), FloatType(-3.12), FloatType(10000.0));
		CHECK(FloatType(quat_get_x(vector_to_quat(vec))) == FloatType(vector_get_x(vec)));
		CHECK(FloatType(quat_get_y(vector_to_quat(vec))) == FloatType(vector_get_y(vec)));
		CHECK(FloatType(quat_get_z(vector_to_quat(vec))) == FloatType(vector_get_z(vec)));
		CHECK(FloatType(quat_get_w(vector_to_quat(vec))) == FloatType(vector_get_w(vec)));
	}

	{
		struct alignas(16) Tmp
		{
			uint8_t padding0[8];	//  8 |  8
			FloatType values[4];	// 24 | 40
			uint8_t padding1[8];	// 32 | 48
		};

		Tmp tmp = { { 0 }, { FloatType(0.0), FloatType(2.34), FloatType(-3.12), FloatType(10000.0) }, {} };
		quat_store(quat_set(FloatType(0.0), FloatType(2.34), FloatType(-3.12), FloatType(10000.0)), &tmp.values[0]);
		CHECK(tmp.values[0] == FloatType(0.0));
		CHECK(tmp.values[1] == FloatType(2.34));
		CHECK(tmp.values[2] == FloatType(-3.12));
		CHECK(tmp.values[3] == FloatType(10000.0));
	}

	{
		QuatType quat = quat_set(FloatType(0.0), FloatType(2.34), FloatType(-3.12), FloatType(10000.0));
		Float4Type tmpf4 = { FloatType(0.0), FloatType(0.0), FloatType(0.0), FloatType(0.0) };

		quat_store(quat, &tmpf4);
		CHECK(FloatType(quat_get_x(quat)) == tmpf4.x);
		CHECK(FloatType(quat_get_y(quat)) == tmpf4.y);
		CHECK(FloatType(quat_get_z(quat)) == tmpf4.z);
		CHECK(FloatType(quat_get_w(quat)) == tmpf4.w);
	}

	//////////////////////////////////////////////////////////////////////////
	// Arithmetic

	{
		QuatType quat = quat_from_euler(scalar_deg_to_rad(FloatType(30.0)), scalar_deg_to_rad(FloatType(-45.0)), scalar_deg_to_rad(FloatType(90.0)));
		QuatType quat_conj = quat_conjugate(quat);
		QuatType quat_conj_scalar = quat_set(-FloatType(quat_get_x(quat)), -FloatType(quat_get_y(quat)), -FloatType(quat_get_z(quat)), FloatType(quat_get_w(quat)));
		CHECK(quat_near_equal(quat_conj, quat_conj_scalar, threshold));
	}

	{
		QuatType quat0 = quat_from_euler(scalar_deg_to_rad(FloatType(30.0)), scalar_deg_to_rad(FloatType(-45.0)), scalar_deg_to_rad(FloatType(90.0)));
		QuatType quat1 = quat_from_euler(scalar_deg_to_rad(FloatType(45.0)), scalar_deg_to_rad(FloatType(60.0)), scalar_deg_to_rad(FloatType(120.0)));
		QuatType result = quat_add(quat0, quat1);
		QuatType result_ref = quat_add_scalar<QuatType, FloatType>(quat0, quat1);
		CHECK(quat_near_equal(result, result_ref, threshold));
	}

	{
		QuatType quat0 = quat_from_euler(scalar_deg_to_rad(FloatType(30.0)), scalar_deg_to_rad(FloatType(-45.0)), scalar_deg_to_rad(FloatType(90.0)));
		QuatType quat1 = quat_from_euler(scalar_deg_to_rad(FloatType(45.0)), scalar_deg_to_rad(FloatType(60.0)), scalar_deg_to_rad(FloatType(120.0)));
		QuatType result = quat_mul(quat0, quat1);
		QuatType result_ref = quat_mul_scalar<QuatType, FloatType>(quat0, quat1);
		CHECK(quat_near_equal(result, result_ref, threshold));

		quat0 = quat_set(FloatType(0.39564531008956383), FloatType(0.044254239301713752), FloatType(0.22768840967675355), FloatType(0.88863059760894492));
		quat1 = quat_set(FloatType(1.0), FloatType(0.0), FloatType(0.0), FloatType(0.0));
		result = quat_mul(quat0, quat1);
		result_ref = quat_mul_scalar<QuatType, FloatType>(quat0, quat1);
		CHECK(quat_near_equal(result, result_ref, threshold));
	}

	{
		QuatType quat0 = quat_from_euler(scalar_deg_to_rad(FloatType(30.0)), scalar_deg_to_rad(FloatType(-45.0)), scalar_deg_to_rad(FloatType(90.0)));
		QuatType result = quat_mul(quat0, FloatType(123.13));
		QuatType result_ref = quat_mul_scalar<QuatType, FloatType>(quat0, FloatType(123.13));
		CHECK(quat_near_equal(result, result_ref, threshold));

		result = quat_mul(quat0, scalar_set(FloatType(123.13)));
		CHECK(quat_near_equal(result, result_ref, threshold));
	}

	{
		Vector4Type x_axis = vector_set(FloatType(1.0), FloatType(0.0), FloatType(0.0));
		Vector4Type y_axis = vector_set(FloatType(0.0), FloatType(1.0), FloatType(0.0));

		QuatType rotation_around_z = quat_from_euler(scalar_deg_to_rad(FloatType(0.0)), scalar_deg_to_rad(FloatType(90.0)), scalar_deg_to_rad(FloatType(0.0)));
		Vector4Type result = quat_mul_vector3(x_axis, rotation_around_z);
		CHECK(vector_all_near_equal3(result, vector_set(FloatType(0.0), FloatType(1.0), FloatType(0.0)), threshold));
		result = quat_mul_vector3(y_axis, rotation_around_z);
		CHECK(vector_all_near_equal3(result, vector_set(FloatType(-1.0), FloatType(0.0), FloatType(0.0)), threshold));

		QuatType rotation_around_x = quat_from_euler(scalar_deg_to_rad(FloatType(0.0)), scalar_deg_to_rad(FloatType(0.0)), scalar_deg_to_rad(FloatType(90.0)));
		result = quat_mul_vector3(x_axis, rotation_around_x);
		CHECK(vector_all_near_equal3(result, vector_set(FloatType(1.0), FloatType(0.0), FloatType(0.0)), threshold));
		result = quat_mul_vector3(y_axis, rotation_around_x);
		CHECK(vector_all_near_equal3(result, vector_set(FloatType(0.0), FloatType(0.0), FloatType(-1.0)), threshold));

		QuatType rotation_xz = quat_mul(rotation_around_x, rotation_around_z);
		QuatType rotation_zx = quat_mul(rotation_around_z, rotation_around_x);
		result = quat_mul_vector3(x_axis, rotation_xz);
		CHECK(vector_all_near_equal3(result, vector_set(FloatType(0.0), FloatType(1.0), FloatType(0.0)), threshold));
		result = quat_mul_vector3(y_axis, rotation_xz);
		CHECK(vector_all_near_equal3(result, vector_set(FloatType(0.0), FloatType(0.0), FloatType(-1.0)), threshold));
		result = quat_mul_vector3(x_axis, rotation_zx);
		CHECK(vector_all_near_equal3(result, vector_set(FloatType(0.0), FloatType(0.0), FloatType(-1.0)), threshold));
		result = quat_mul_vector3(y_axis, rotation_zx);
		CHECK(vector_all_near_equal3(result, vector_set(FloatType(-1.0), FloatType(0.0), FloatType(0.0)), threshold));
	}

	{
		const QuatType test_rotations[] = {
			identity,
			quat_from_euler(scalar_deg_to_rad(FloatType(30.0)), scalar_deg_to_rad(FloatType(-45.0)), scalar_deg_to_rad(FloatType(90.0))),
			quat_from_euler(scalar_deg_to_rad(FloatType(45.0)), scalar_deg_to_rad(FloatType(60.0)), scalar_deg_to_rad(FloatType(120.0))),
			quat_from_euler(scalar_deg_to_rad(FloatType(0.0)), scalar_deg_to_rad(FloatType(180.0)), scalar_deg_to_rad(FloatType(45.0))),
			quat_from_euler(scalar_deg_to_rad(FloatType(-120.0)), scalar_deg_to_rad(FloatType(-90.0)), scalar_deg_to_rad(FloatType(0.0))),
			quat_from_euler(scalar_deg_to_rad(FloatType(-0.01)), scalar_deg_to_rad(FloatType(0.02)), scalar_deg_to_rad(FloatType(-0.03))),
		};

		const Vector4Type test_vectors[] = {
			zero,
			vector_set(FloatType(1.0), FloatType(0.0), FloatType(0.0)),
			vector_set(FloatType(0.0), FloatType(1.0), FloatType(0.0)),
			vector_set(FloatType(0.0), FloatType(0.0), FloatType(1.0)),
			vector_set(FloatType(45.0), FloatType(-60.0), FloatType(120.0)),
			vector_set(FloatType(-45.0), FloatType(60.0), FloatType(-120.0)),
			vector_set(FloatType(0.57735026918962576451), FloatType(0.57735026918962576451), FloatType(0.57735026918962576451)),
			vector_set(FloatType(-1.0), FloatType(0.0), FloatType(0.0)),
		};

		for (size_t quat_index = 0; quat_index < rtm_impl::get_array_size(test_rotations); ++quat_index)
		{
			const QuatType& rotation = test_rotations[quat_index];
			for (size_t vector_index = 0; vector_index < rtm_impl::get_array_size(test_vectors); ++vector_index)
			{
				const Vector4Type& vector = test_vectors[vector_index];
				Vector4Type result = quat_mul_vector3(vector, rotation);
				Vector4Type result_ref = quat_rotate_scalar<QuatType, Vector4Type, FloatType>(vector, rotation);
				CHECK(vector_all_near_equal3(result, result_ref, threshold));
			}
		}
	}

	{
		const FloatType test_value10_flt[4] = { FloatType(-0.001138), FloatType(0.91623), FloatType(-1.624598), FloatType(0.715671) };
		const FloatType test_value11_flt[4] = { FloatType(0.1138), FloatType(-0.623), FloatType(1.4598), FloatType(-0.5671) };
		const QuatType test_value10 = quat_set(test_value10_flt[0], test_value10_flt[1], test_value10_flt[2], test_value10_flt[3]);
		const QuatType test_value11 = quat_set(test_value11_flt[0], test_value11_flt[1], test_value11_flt[2], test_value11_flt[3]);
		const FloatType scalar_dot_result = scalar_dot<QuatType, FloatType>(test_value10, test_value11);
		const FloatType quat_dot_result = quat_dot(test_value10, test_value11);
		CHECK(scalar_near_equal(quat_dot_result, scalar_dot_result, threshold));
		const ScalarType quat_sdot_result = quat_dot_as_scalar(test_value10, test_value11);
		CHECK(scalar_near_equal(scalar_cast(quat_sdot_result), scalar_dot_result, threshold));
	}

	{
		QuatType quat = quat_from_euler(scalar_deg_to_rad(FloatType(30.0)), scalar_deg_to_rad(FloatType(-45.0)), scalar_deg_to_rad(FloatType(90.0)));
		Vector4Type vec = quat_to_vector(quat);

		CHECK(scalar_near_equal(FloatType(quat_length_squared(quat)), FloatType(vector_length_squared(vec)), threshold));
		CHECK(scalar_near_equal(scalar_cast(quat_length_squared_as_scalar(quat)), scalar_cast(vector_length_squared_as_scalar(vec)), threshold));
		CHECK(scalar_near_equal(FloatType(quat_length(quat)), FloatType(vector_length(vec)), threshold));
		CHECK(scalar_near_equal(scalar_cast(quat_length_as_scalar(quat)), scalar_cast(vector_length_as_scalar(vec)), threshold));
		CHECK(scalar_near_equal(FloatType(quat_length_reciprocal(quat)), FloatType(vector_length_reciprocal(vec)), threshold));
		CHECK(scalar_near_equal(scalar_cast(quat_length_reciprocal_as_scalar(quat)), scalar_cast(vector_length_reciprocal_as_scalar(vec)), threshold));
	}

	{
		QuatType quat = quat_set(FloatType(-0.001138), FloatType(0.91623), FloatType(-1.624598), FloatType(0.715671));
		const QuatType scalar_normalize_result = scalar_normalize<QuatType, FloatType>(quat);
		const QuatType quat_normalize_result = quat_normalize(quat);
		CHECK(scalar_near_equal(FloatType(quat_get_x(quat_normalize_result)), FloatType(quat_get_x(scalar_normalize_result)), threshold));
		CHECK(scalar_near_equal(FloatType(quat_get_y(quat_normalize_result)), FloatType(quat_get_y(scalar_normalize_result)), threshold));
		CHECK(scalar_near_equal(FloatType(quat_get_z(quat_normalize_result)), FloatType(quat_get_z(scalar_normalize_result)), threshold));
		CHECK(scalar_near_equal(FloatType(quat_get_w(quat_normalize_result)), FloatType(quat_get_w(scalar_normalize_result)), threshold));
	}

	{
		// Stored as doubles: quat_set(-0.001138, 0.91623, -1.624598, 0.715671)
		const std::array<uint64_t, 4> raw_quat = { { 0xBF52A51E321A2E7FULL, 0x3FED51C193B3A68BULL, 0xBFF9FE5A78F25A25ULL, 0x3FE6E6C6DE76427CULL } };
		const double* raw_quat_ = rtm_impl::bit_cast<const double*>(raw_quat.data());
		QuatType quat = quat_set(FloatType(raw_quat_[0]), FloatType(raw_quat_[1]), FloatType(raw_quat_[2]), FloatType(raw_quat_[3]));

		const std::array<uint64_t, 4> raw_result32 = { { 0xBA15540FULL, 0x3EEAD1D9ULL, 0xBF502EF0ULL, 0x3EB76B2CULL } };
		const std::array<uint64_t, 4> raw_result64 = { { 0xBF42AA81E1FF0E34ULL, 0x3FDD5A3B249ED450ULL, 0xBFEA05DDEFFE89CAULL, 0x3FD6ED6586A3C455ULL } };
		const std::array<uint64_t, 4>& raw_expected_result = sizeof(FloatType) == 4 ? raw_result32 : raw_result64;

		const QuatType quat_normalize_result = quat_normalize_deterministic(quat);
		CHECK(quat_is_normalized(quat_normalize_result));

		std::array<uint64_t, 4> result = { { 0, 0, 0, 0 } };
		std::memcpy(&result[0], rtm_impl::bit_cast<const uint8_t*>(&quat_normalize_result) + (0 * sizeof(FloatType)), sizeof(FloatType));
		std::memcpy(&result[1], rtm_impl::bit_cast<const uint8_t*>(&quat_normalize_result) + (1 * sizeof(FloatType)), sizeof(FloatType));
		std::memcpy(&result[2], rtm_impl::bit_cast<const uint8_t*>(&quat_normalize_result) + (2 * sizeof(FloatType)), sizeof(FloatType));
		std::memcpy(&result[3], rtm_impl::bit_cast<const uint8_t*>(&quat_normalize_result) + (3 * sizeof(FloatType)), sizeof(FloatType));

		CHECK(result[0] == raw_expected_result[0]);
		CHECK(result[1] == raw_expected_result[1]);
		CHECK(result[2] == raw_expected_result[2]);
		CHECK(result[3] == raw_expected_result[3]);
	}

	{
		QuatType quat0 = quat_normalize(quat_from_euler(scalar_deg_to_rad(FloatType(30.0)), scalar_deg_to_rad(FloatType(-45.0)), scalar_deg_to_rad(FloatType(90.0))));
		QuatType quat1 = quat_normalize(quat_from_euler(scalar_deg_to_rad(FloatType(45.0)), scalar_deg_to_rad(FloatType(60.0)), scalar_deg_to_rad(FloatType(120.0))));

		QuatType scalar_result = scalar_lerp<QuatType, FloatType>(quat0, quat1, FloatType(0.33));

		CHECK(scalar_near_equal(FloatType(quat_get_x(quat_lerp(quat0, quat1, FloatType(0.33)))), FloatType(quat_get_x(scalar_result)), threshold));
		CHECK(scalar_near_equal(FloatType(quat_get_y(quat_lerp(quat0, quat1, FloatType(0.33)))), FloatType(quat_get_y(scalar_result)), threshold));
		CHECK(scalar_near_equal(FloatType(quat_get_z(quat_lerp(quat0, quat1, FloatType(0.33)))), FloatType(quat_get_z(scalar_result)), threshold));
		CHECK(scalar_near_equal(FloatType(quat_get_w(quat_lerp(quat0, quat1, FloatType(0.33)))), FloatType(quat_get_w(scalar_result)), threshold));

		CHECK(scalar_near_equal(FloatType(quat_get_x(quat_lerp(quat0, quat1, scalar_set(FloatType(0.33))))), FloatType(quat_get_x(scalar_result)), threshold));
		CHECK(scalar_near_equal(FloatType(quat_get_y(quat_lerp(quat0, quat1, scalar_set(FloatType(0.33))))), FloatType(quat_get_y(scalar_result)), threshold));
		CHECK(scalar_near_equal(FloatType(quat_get_z(quat_lerp(quat0, quat1, scalar_set(FloatType(0.33))))), FloatType(quat_get_z(scalar_result)), threshold));
		CHECK(scalar_near_equal(FloatType(quat_get_w(quat_lerp(quat0, quat1, scalar_set(FloatType(0.33))))), FloatType(quat_get_w(scalar_result)), threshold));

		quat1 = quat_neg(quat1);
		CHECK(scalar_near_equal(FloatType(quat_get_x(quat_lerp(quat0, quat1, FloatType(0.33)))), FloatType(quat_get_x(scalar_result)), threshold));
		CHECK(scalar_near_equal(FloatType(quat_get_y(quat_lerp(quat0, quat1, FloatType(0.33)))), FloatType(quat_get_y(scalar_result)), threshold));
		CHECK(scalar_near_equal(FloatType(quat_get_z(quat_lerp(quat0, quat1, FloatType(0.33)))), FloatType(quat_get_z(scalar_result)), threshold));
		CHECK(scalar_near_equal(FloatType(quat_get_w(quat_lerp(quat0, quat1, FloatType(0.33)))), FloatType(quat_get_w(scalar_result)), threshold));

		CHECK(scalar_near_equal(FloatType(quat_get_x(quat_lerp(quat0, quat1, scalar_set(FloatType(0.33))))), FloatType(quat_get_x(scalar_result)), threshold));
		CHECK(scalar_near_equal(FloatType(quat_get_y(quat_lerp(quat0, quat1, scalar_set(FloatType(0.33))))), FloatType(quat_get_y(scalar_result)), threshold));
		CHECK(scalar_near_equal(FloatType(quat_get_z(quat_lerp(quat0, quat1, scalar_set(FloatType(0.33))))), FloatType(quat_get_z(scalar_result)), threshold));
		CHECK(scalar_near_equal(FloatType(quat_get_w(quat_lerp(quat0, quat1, scalar_set(FloatType(0.33))))), FloatType(quat_get_w(scalar_result)), threshold));

		CHECK(vector_all_near_equal(quat_to_vector(quat_lerp(quat0, quat1, FloatType(0.0))), quat_to_vector(quat0), threshold));
		CHECK(vector_all_near_equal(quat_to_vector(quat_lerp(quat0, quat1, FloatType(1.0))), quat_to_vector(quat_neg(quat1)), threshold));
		CHECK(vector_all_near_equal(quat_to_vector(quat_lerp(quat0, quat1, scalar_set(FloatType(0.0)))), quat_to_vector(quat0), threshold));
		CHECK(vector_all_near_equal(quat_to_vector(quat_lerp(quat0, quat1, scalar_set(FloatType(1.0)))), quat_to_vector(quat_neg(quat1)), threshold));
	}

	{
		QuatType quat0 = quat_normalize(quat_from_euler(scalar_deg_to_rad(FloatType(30.0)), scalar_deg_to_rad(FloatType(-45.0)), scalar_deg_to_rad(FloatType(90.0))));
		QuatType quat1 = quat_normalize(quat_from_euler(scalar_deg_to_rad(FloatType(45.0)), scalar_deg_to_rad(FloatType(60.0)), scalar_deg_to_rad(FloatType(120.0))));

		CHECK(vector_all_near_equal(quat_to_vector(quat_slerp(quat0, quat1, FloatType(0.0))), quat_to_vector(quat0), threshold));
		CHECK(vector_all_near_equal(quat_to_vector(quat_slerp(quat0, quat1, FloatType(1.0))), quat_to_vector(quat1), threshold));

		// Remove quat0 from quat1
		QuatType delta_0_to_1 = quat_normalize(quat_mul(quat1, quat_conjugate(quat0)));

		Vector4Type axis;
		FloatType angle;
		quat_to_axis_angle(delta_0_to_1, axis, angle);

		FloatType alpha = FloatType(0.33);
		QuatType interp_delta = quat_from_axis_angle(axis, angle * alpha);

		QuatType result_ref = quat_mul(interp_delta, quat0);
		QuatType result0 = quat_slerp(quat0, quat1, alpha);
		QuatType result0_s = quat_slerp(quat0, quat1, scalar_set(alpha));

		CHECK(vector_all_near_equal(quat_to_vector(result0), quat_to_vector(result_ref), threshold));
		CHECK(vector_all_near_equal(quat_to_vector(result0_s), quat_to_vector(result_ref), threshold));

		quat1 = quat_neg(quat1);
		QuatType result1 = quat_slerp(quat0, quat1, alpha);
		QuatType result1_s = quat_slerp(quat0, quat1, scalar_set(alpha));

		CHECK(vector_all_near_equal(quat_to_vector(result1), quat_to_vector(result_ref), threshold));
		CHECK(vector_all_near_equal(quat_to_vector(result1_s), quat_to_vector(result_ref), threshold));
	}

	{
		QuatType quat0 = quat_from_euler(scalar_deg_to_rad(FloatType(30.0)), scalar_deg_to_rad(FloatType(-45.0)), scalar_deg_to_rad(FloatType(90.0)));
		QuatType quat1 = quat_neg(quat0);

		CHECK(FloatType(quat_get_x(quat0)) == -FloatType(quat_get_x(quat1)));
		CHECK(FloatType(quat_get_y(quat0)) == -FloatType(quat_get_y(quat1)));
		CHECK(FloatType(quat_get_z(quat0)) == -FloatType(quat_get_z(quat1)));
		CHECK(FloatType(quat_get_w(quat0)) == -FloatType(quat_get_w(quat1)));
	}

	{
		QuatType quat0 = quat_from_euler(scalar_deg_to_rad(FloatType(30.0)), scalar_deg_to_rad(FloatType(-45.0)), scalar_deg_to_rad(FloatType(90.0)));
		QuatType quat_log = quat_rotation_log(quat0);
		QuatType quat_exp = quat_rotation_exp(quat_log);
		QuatType quat_log_identity = quat_rotation_log(identity);
		QuatType quat_exp_identity = quat_rotation_exp(quat_log_identity);

		CHECK(quat_near_equal(quat0, quat_exp, threshold));
		CHECK(vector_all_near_equal(quat_to_vector(quat_log_identity), zero, threshold));
		CHECK(quat_near_equal(identity, quat_exp_identity, threshold));
	}

	//////////////////////////////////////////////////////////////////////////
	// Conversion to/from axis/angle/euler

	{
		QuatType rotation = quat_from_euler(scalar_deg_to_rad(FloatType(0.0)), scalar_deg_to_rad(FloatType(90.0)), scalar_deg_to_rad(FloatType(0.0)));
		Vector4Type axis;
		FloatType angle;
		quat_to_axis_angle(rotation, axis, angle);
		CHECK(vector_all_near_equal3(axis, vector_set(FloatType(0.0), FloatType(0.0), FloatType(1.0)), threshold));
		CHECK(vector_all_near_equal3(quat_get_axis(rotation), vector_set(FloatType(0.0), FloatType(0.0), FloatType(1.0)), threshold));
		CHECK(scalar_near_equal(quat_get_angle(rotation), scalar_deg_to_rad(FloatType(90.0)), threshold));
	}

	{
		QuatType rotation = quat_from_euler(scalar_deg_to_rad(FloatType(0.0)), scalar_deg_to_rad(FloatType(90.0)), scalar_deg_to_rad(FloatType(0.0)));
		Vector4Type axis;
		FloatType angle;
		quat_to_axis_angle(rotation, axis, angle);
		QuatType rotation_new = quat_from_axis_angle(axis, angle);
		CHECK(quat_near_equal(rotation, rotation_new, threshold));
	}

	{
		QuatType rotation = quat_set(FloatType(0.39564531008956383), FloatType(0.044254239301713752), FloatType(0.22768840967675355), FloatType(0.88863059760894492));
		Vector4Type axis_ref = vector_set(FloatType(1.0), FloatType(0.0), FloatType(0.0));
		axis_ref = quat_mul_vector3(axis_ref, rotation);
		FloatType angle_ref = scalar_deg_to_rad(FloatType(57.0));
		QuatType result = quat_from_axis_angle(axis_ref, angle_ref);
		Vector4Type axis;
		FloatType angle;
		quat_to_axis_angle(result, axis, angle);
		CHECK(vector_all_near_equal3(axis, axis_ref, threshold));
		CHECK(scalar_near_equal(angle, angle_ref, threshold));
	}

	//////////////////////////////////////////////////////////////////////////
	// Comparisons and masking

	{
		const FloatType inf = std::numeric_limits<FloatType>::infinity();
		const FloatType nan = std::numeric_limits<FloatType>::quiet_NaN();
		CHECK(quat_is_finite(identity) == true);
		CHECK(quat_is_finite(quat_set(inf, inf, inf, inf)) == false);
		CHECK(quat_is_finite(quat_set(inf, FloatType(1.0), FloatType(1.0), FloatType(1.0))) == false);
		CHECK(quat_is_finite(quat_set(FloatType(1.0), FloatType(inf), FloatType(1.0), FloatType(1.0))) == false);
		CHECK(quat_is_finite(quat_set(FloatType(1.0), FloatType(1.0), FloatType(inf), FloatType(1.0))) == false);
		CHECK(quat_is_finite(quat_set(FloatType(1.0), FloatType(1.0), FloatType(1.0), FloatType(inf))) == false);
		CHECK(quat_is_finite(quat_set(nan, nan, nan, nan)) == false);
		CHECK(quat_is_finite(quat_set(nan, FloatType(1.0), FloatType(1.0), FloatType(1.0))) == false);
		CHECK(quat_is_finite(quat_set(FloatType(1.0), FloatType(nan), FloatType(1.0), FloatType(1.0))) == false);
		CHECK(quat_is_finite(quat_set(FloatType(1.0), FloatType(1.0), FloatType(nan), FloatType(1.0))) == false);
		CHECK(quat_is_finite(quat_set(FloatType(1.0), FloatType(1.0), FloatType(1.0), FloatType(nan))) == false);
	}

	{
		QuatType quat0 = quat_set(FloatType(0.39564531008956383), FloatType(0.044254239301713752), FloatType(0.22768840967675355), FloatType(0.88863059760894492));
		FloatType quat_len = quat_length(quat0);
		CHECK(scalar_near_equal(quat_len, FloatType(1.0), threshold));
		CHECK(quat_is_normalized(quat0) == true);
		CHECK(quat_is_normalized(identity, FloatType(0.0)) == true);

		QuatType quat1 = vector_to_quat(vector_mul(quat_to_vector(quat0), FloatType(1.1)));
		CHECK(quat_is_normalized(quat1) == false);
	}

	{
		CHECK(quat_are_equal(identity, identity) == true);
		CHECK(quat_are_equal(identity, quat_set(FloatType(0.0), FloatType(0.0), FloatType(0.0), FloatType(1.000001))) == false);
		CHECK(quat_are_equal(identity, quat_set(FloatType(0.0), FloatType(0.0), FloatType(0.000001), FloatType(1.0))) == false);
		CHECK(quat_are_equal(identity, quat_set(FloatType(0.0), FloatType(0.000001), FloatType(0.0), FloatType(1.0))) == false);
		CHECK(quat_are_equal(identity, quat_set(FloatType(0.000001), FloatType(0.0), FloatType(0.0), FloatType(1.0))) == false);
	}

	{
		CHECK(quat_near_equal(identity, identity, FloatType(0.0)) == true);
		CHECK(quat_near_equal(identity, quat_set(FloatType(0.0), FloatType(0.0), FloatType(0.0), FloatType(2.0)), FloatType(1.0001)) == true);
		CHECK(quat_near_equal(identity, quat_set(FloatType(0.0), FloatType(0.0), FloatType(0.0), FloatType(2.0)), FloatType(1.0)) == true);
		CHECK(quat_near_equal(identity, quat_set(FloatType(0.0), FloatType(0.0), FloatType(0.0), FloatType(2.0)), FloatType(0.9999)) == false);
	}

	{
		CHECK(quat_near_identity(identity, FloatType(0.0)) == true);
		CHECK(quat_near_identity(quat_set(FloatType(0.0), FloatType(0.0), FloatType(0.0), FloatType(0.9999999)), FloatType(0.001)) == true);
		CHECK(quat_near_identity(quat_set(FloatType(0.0), FloatType(0.0), FloatType(0.0), FloatType(0.98)), FloatType(0.001)) == false);
	}

	{
		QuatType quat0 = quat_set(FloatType(0.39564531008956383), FloatType(0.044254239301713752), FloatType(0.22768840967675355), FloatType(0.88863059760894492));
		QuatType quat1 = identity;

		MaskType all_true = mask_true();
		MaskType all_false = mask_false();

		CHECK(quat_are_equal(quat_select(all_true, quat0, quat1), quat0));
		CHECK(quat_are_equal(quat_select(all_false, quat0, quat1), quat1));
	}
}

TEST_CASE("quatf math", "[math][quat]")
{
	test_quat_impl<float>(1.0E-4F);

	const quatf src = quat_set(0.39564531008956383F, 0.044254239301713752F, 0.22768840967675355F, 0.88863059760894492F);
	const quatd dst = quat_cast(src);
	CHECK(scalar_near_equal(double(quat_get_x(dst)), 0.39564531008956383, 1.0E-6));
	CHECK(scalar_near_equal(double(quat_get_y(dst)), 0.044254239301713752, 1.0E-6));
	CHECK(scalar_near_equal(double(quat_get_z(dst)), 0.22768840967675355, 1.0E-6));
	CHECK(scalar_near_equal(double(quat_get_w(dst)), 0.88863059760894492, 1.0E-6));
}

TEST_CASE("quatd math", "[math][quat]")
{
	test_quat_impl<double>(1.0E-6);

	const quatd src = quat_set(0.39564531008956383, 0.044254239301713752, 0.22768840967675355, 0.88863059760894492);
	const quatf dst = quat_cast(src);
	CHECK(scalar_near_equal(float(quat_get_x(dst)), 0.39564531008956383F, 1.0E-6F));
	CHECK(scalar_near_equal(float(quat_get_y(dst)), 0.044254239301713752F, 1.0E-6F));
	CHECK(scalar_near_equal(float(quat_get_z(dst)), 0.22768840967675355F, 1.0E-6F));
	CHECK(scalar_near_equal(float(quat_get_w(dst)), 0.88863059760894492F, 1.0E-6F));
}

```

`tests/sources/test_qv.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2023 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include "catch2.impl.h"

#include <rtm/qvf.h>
#include <rtm/qvd.h>

using namespace rtm;

template<typename TransformType, typename FloatType>
static void test_qv_impl(const TransformType& identity, const FloatType threshold)
{
	using QuatType = decltype(TransformType::rotation);
	using Vector4Type = decltype(TransformType::translation);
	using ScalarType = typename related_types<FloatType>::scalar;
	using Matrix3x3Type = typename related_types<FloatType>::matrix3x3;
	using Matrix3x4Type = typename related_types<FloatType>::matrix3x4;

	{
		Vector4Type zero = vector_set(FloatType(0.0));
		QuatType q_identity = quat_set(FloatType(0.0), FloatType(0.0), FloatType(0.0), FloatType(1.0));
		TransformType tmp = qv_set(q_identity, zero);
		CHECK(quat_near_equal(identity.rotation, tmp.rotation, threshold));
		CHECK(vector_all_near_equal3(identity.translation, tmp.translation, threshold));
		CHECK(quat_near_equal(q_identity, tmp.rotation, threshold));
		CHECK(vector_all_near_equal3(zero, tmp.translation, threshold));
	}

	{
		QuatType rotation_around_z = quat_from_euler(scalar_deg_to_rad(FloatType(0.0)), scalar_deg_to_rad(FloatType(90.0)), scalar_deg_to_rad(FloatType(0.0)));
		Matrix3x3Type mtx = matrix_from_quat(rotation_around_z);
		TransformType transform = qv_from_matrix(mtx);
		CHECK(quat_near_equal(rotation_around_z, transform.rotation, threshold));
		CHECK(vector_all_near_equal3(identity.translation, transform.translation, threshold));
	}

	{
		QuatType rotation_around_z = quat_from_euler(scalar_deg_to_rad(FloatType(0.0)), scalar_deg_to_rad(FloatType(90.0)), scalar_deg_to_rad(FloatType(0.0)));
		Vector4Type translation = vector_set(FloatType(1.0), FloatType(2.0), FloatType(3.0));
		Matrix3x4Type mtx = matrix_from_qv(rotation_around_z, translation);
		TransformType transform = qv_from_matrix(mtx);
		CHECK(quat_near_equal(rotation_around_z, transform.rotation, threshold));
		CHECK(vector_all_near_equal3(translation, transform.translation, threshold));
	}

	{
		Vector4Type x_axis = vector_set(FloatType(1.0), FloatType(0.0), FloatType(0.0));
		Vector4Type y_axis = vector_set(FloatType(0.0), FloatType(1.0), FloatType(0.0));

		QuatType rotation_around_z = quat_from_euler(scalar_deg_to_rad(FloatType(0.0)), scalar_deg_to_rad(FloatType(90.0)), scalar_deg_to_rad(FloatType(0.0)));
		TransformType transform_a = qv_set(rotation_around_z, x_axis);
		Vector4Type result = qv_mul_point3(x_axis, transform_a);
		CHECK(vector_all_near_equal3(result, vector_set(FloatType(1.0), FloatType(1.0), FloatType(0.0)), threshold));
		result = qv_mul_point3(y_axis, transform_a);
		CHECK(vector_all_near_equal3(result, vector_set(FloatType(0.0), FloatType(0.0), FloatType(0.0)), threshold));

		QuatType rotation_around_x = quat_from_euler(scalar_deg_to_rad(FloatType(0.0)), scalar_deg_to_rad(FloatType(0.0)), scalar_deg_to_rad(FloatType(90.0)));
		TransformType transform_b = qv_set(rotation_around_x, y_axis);
		result = qv_mul_point3(x_axis, transform_b);
		CHECK(vector_all_near_equal3(result, vector_set(FloatType(1.0), FloatType(1.0), FloatType(0.0)), threshold));
		result = qv_mul_point3(y_axis, transform_b);
		CHECK(vector_all_near_equal3(result, vector_set(FloatType(0.0), FloatType(1.0), FloatType(-1.0)), threshold));

		TransformType transform_ab = qv_mul(transform_a, transform_b);
		TransformType transform_ba = qv_mul(transform_b, transform_a);
		result = qv_mul_point3(x_axis, transform_ab);
		CHECK(vector_all_near_equal3(result, vector_set(FloatType(1.0), FloatType(1.0), FloatType(-1.0)), threshold));
		CHECK(vector_all_near_equal3(result, qv_mul_point3(qv_mul_point3(x_axis, transform_a), transform_b), threshold));
		result = qv_mul_point3(y_axis, transform_ab);
		CHECK(vector_all_near_equal3(result, vector_set(FloatType(0.0), FloatType(1.0), FloatType(0.0)), threshold));
		CHECK(vector_all_near_equal3(result, qv_mul_point3(qv_mul_point3(y_axis, transform_a), transform_b), threshold));
		result = qv_mul_point3(x_axis, transform_ba);
		CHECK(vector_all_near_equal3(result, vector_set(FloatType(0.0), FloatType(1.0), FloatType(0.0)), threshold));
		CHECK(vector_all_near_equal3(result, qv_mul_point3(qv_mul_point3(x_axis, transform_b), transform_a), threshold));
		result = qv_mul_point3(y_axis, transform_ba);
		CHECK(vector_all_near_equal3(result, vector_set(FloatType(0.0), FloatType(0.0), FloatType(-1.0)), threshold));
		CHECK(vector_all_near_equal3(result, qv_mul_point3(qv_mul_point3(y_axis, transform_b), transform_a), threshold));
	}

	{
		Vector4Type x_axis = vector_set(FloatType(1.0), FloatType(0.0), FloatType(0.0));
		Vector4Type y_axis = vector_set(FloatType(0.0), FloatType(1.0), FloatType(0.0));

		QuatType rotation_around_z = quat_from_euler(scalar_deg_to_rad(FloatType(0.0)), scalar_deg_to_rad(FloatType(90.0)), scalar_deg_to_rad(FloatType(0.0)));
		TransformType transform_a = qv_set(rotation_around_z, x_axis);
		Vector4Type result = qv_mul_point3(x_axis, transform_a);
		CHECK(vector_all_near_equal3(result, vector_set(FloatType(1.0), FloatType(1.0), FloatType(0.0)), threshold));
		result = qv_mul_point3(y_axis, transform_a);
		CHECK(vector_all_near_equal3(result, vector_set(FloatType(0.0), FloatType(0.0), FloatType(0.0)), threshold));

		QuatType rotation_around_x = quat_from_euler(scalar_deg_to_rad(FloatType(0.0)), scalar_deg_to_rad(FloatType(0.0)), scalar_deg_to_rad(FloatType(90.0)));
		TransformType transform_b = qv_set(rotation_around_x, y_axis);
		result = qv_mul_point3(x_axis, transform_b);
		CHECK(vector_all_near_equal3(result, vector_set(FloatType(1.0), FloatType(1.0), FloatType(0.0)), threshold));
		result = qv_mul_point3(y_axis, transform_b);
		CHECK(vector_all_near_equal3(result, vector_set(FloatType(0.0), FloatType(1.0), FloatType(-1.0)), threshold));

		TransformType transform_ab = qv_mul(transform_a, transform_b);
		TransformType transform_ba = qv_mul(transform_b, transform_a);
		result = qv_mul_point3(x_axis, transform_ab);
		CHECK(vector_all_near_equal3(result, vector_set(FloatType(1.0), FloatType(1.0), FloatType(-1.0)), threshold));
		CHECK(vector_all_near_equal3(result, qv_mul_point3(qv_mul_point3(x_axis, transform_a), transform_b), threshold));
		result = qv_mul_point3(y_axis, transform_ab);
		CHECK(vector_all_near_equal3(result, vector_set(FloatType(0.0), FloatType(1.0), FloatType(0.0)), threshold));
		CHECK(vector_all_near_equal3(result, qv_mul_point3(qv_mul_point3(y_axis, transform_a), transform_b), threshold));
		result = qv_mul_point3(x_axis, transform_ba);
		CHECK(vector_all_near_equal3(result, vector_set(FloatType(0.0), FloatType(1.0), FloatType(0.0)), threshold));
		CHECK(vector_all_near_equal3(result, qv_mul_point3(qv_mul_point3(x_axis, transform_b), transform_a), threshold));
		result = qv_mul_point3(y_axis, transform_ba);
		CHECK(vector_all_near_equal3(result, vector_set(FloatType(0.0), FloatType(0.0), FloatType(-1.0)), threshold));
		CHECK(vector_all_near_equal3(result, qv_mul_point3(qv_mul_point3(y_axis, transform_b), transform_a), threshold));
	}

	{
		Vector4Type x_axis = vector_set(FloatType(1.0), FloatType(0.0), FloatType(0.0));

		QuatType rotation_around_z = quat_from_euler(scalar_deg_to_rad(FloatType(0.0)), scalar_deg_to_rad(FloatType(90.0)), scalar_deg_to_rad(FloatType(0.0)));
		TransformType transform_a = qv_set(rotation_around_z, x_axis);
		TransformType transform_b = qv_inverse(transform_a);
		TransformType transform_ab = qv_mul(transform_a, transform_b);
		CHECK(quat_near_equal(identity.rotation, transform_ab.rotation, threshold));
		CHECK(vector_all_near_equal3(identity.translation, transform_ab.translation, threshold));

		transform_a = qv_set(rotation_around_z, x_axis);
		transform_b = qv_inverse(transform_a);
		transform_ab = qv_mul(transform_a, transform_b);
		CHECK(quat_near_equal(identity.rotation, transform_ab.rotation, threshold));
		CHECK(vector_all_near_equal3(identity.translation, transform_ab.translation, threshold));
	}

	{
		Vector4Type x_axis = vector_set(FloatType(1.0), FloatType(0.0), FloatType(0.0));

		QuatType rotation_around_z = quat_from_euler(scalar_deg_to_rad(FloatType(0.0)), scalar_deg_to_rad(FloatType(90.0)), scalar_deg_to_rad(FloatType(0.0)));
		TransformType transform_a = qv_set(rotation_around_z, x_axis);
		TransformType transform_b = qv_inverse(transform_a);
		TransformType transform_ab = qv_mul(transform_a, transform_b);
		CHECK(quat_near_equal(identity.rotation, transform_ab.rotation, threshold));
		CHECK(vector_all_near_equal3(identity.translation, transform_ab.translation, threshold));
	}

	{
		Vector4Type x_axis = vector_set(FloatType(1.0), FloatType(0.0), FloatType(0.0));
		QuatType rotation_around_z = quat_from_euler(scalar_deg_to_rad(FloatType(0.0)), scalar_deg_to_rad(FloatType(90.0)), scalar_deg_to_rad(FloatType(0.0)));
		TransformType transform_a = qv_set(rotation_around_z, x_axis);
		CHECK(quat_is_normalized(qv_normalize(transform_a).rotation, threshold));

		QuatType quat = quat_set(FloatType(-0.001138), FloatType(0.91623), FloatType(-1.624598), FloatType(0.715671));
		TransformType transform_b = qv_set(quat, x_axis);
		CHECK(!quat_is_normalized(transform_b.rotation, threshold));
		CHECK(quat_is_normalized(qv_normalize(transform_b).rotation, threshold));
	}

	{
		FloatType alpha = FloatType(0.33);
		ScalarType alpha_s = scalar_set(alpha);
		QuatType quat0 = quat_normalize(quat_from_euler(scalar_deg_to_rad(FloatType(30.0)), scalar_deg_to_rad(FloatType(-45.0)), scalar_deg_to_rad(FloatType(90.0))));
		QuatType quat1 = quat_normalize(quat_from_euler(scalar_deg_to_rad(FloatType(45.0)), scalar_deg_to_rad(FloatType(60.0)), scalar_deg_to_rad(FloatType(120.0))));

		QuatType quat_ref_lerp = quat_lerp(quat0, quat1, alpha);
		QuatType quat_ref_lerp_s = quat_lerp(quat0, quat1, alpha_s);
		QuatType quat_ref_slerp = quat_slerp(quat0, quat1, alpha);
		QuatType quat_ref_slerp_s = quat_slerp(quat0, quat1, alpha_s);

		Vector4Type trans0 = vector_set(FloatType(-0.001138), FloatType(0.91623), FloatType(-1.624598));
		Vector4Type trans1 = vector_set(FloatType(-0.001138), FloatType(0.91623), FloatType(-1.624598));

		Vector4Type trans_ref = vector_lerp(trans0, trans1, alpha);
		Vector4Type trans_ref_s = vector_lerp(trans0, trans1, alpha_s);

		TransformType transform0 = qv_set(quat0, trans0);
		TransformType transform1 = qv_set(quat1, trans1);

		TransformType transform_ref_lerp = qv_set(quat_ref_lerp, trans_ref);
		TransformType transform_ref_slerp = qv_set(quat_ref_slerp, trans_ref);

		TransformType transform_ref_lerp_s = qv_set(quat_ref_lerp_s, trans_ref_s);
		TransformType transform_ref_slerp_s = qv_set(quat_ref_slerp_s, trans_ref_s);

		TransformType transform_lerp = qv_lerp(transform0, transform1, alpha);
		TransformType transform_lerp_s = qv_lerp(transform0, transform1, alpha_s);

		TransformType transform_slerp = qv_slerp(transform0, transform1, alpha);
		TransformType transform_slerp_s = qv_slerp(transform0, transform1, alpha_s);

		CHECK(quat_near_equal(transform_lerp.rotation, transform_ref_lerp.rotation, threshold));
		CHECK(quat_near_equal(transform_lerp_s.rotation, transform_ref_lerp_s.rotation, threshold));
		CHECK(vector_all_near_equal3(transform_lerp.translation, transform_ref_lerp.translation, threshold));
		CHECK(vector_all_near_equal3(transform_lerp_s.translation, transform_ref_lerp_s.translation, threshold));

		CHECK(quat_near_equal(transform_slerp.rotation, transform_ref_slerp.rotation, threshold));
		CHECK(quat_near_equal(transform_slerp_s.rotation, transform_ref_slerp_s.rotation, threshold));
		CHECK(vector_all_near_equal3(transform_slerp.translation, transform_ref_slerp.translation, threshold));
		CHECK(vector_all_near_equal3(transform_slerp_s.translation, transform_ref_slerp_s.translation, threshold));
	}

	{
		const FloatType inf = std::numeric_limits<FloatType>::infinity();
		const FloatType nan = std::numeric_limits<FloatType>::quiet_NaN();

		const QuatType quat0 = quat_normalize(quat_from_euler(scalar_deg_to_rad(FloatType(30.0)), scalar_deg_to_rad(FloatType(-45.0)), scalar_deg_to_rad(FloatType(90.0))));
		const QuatType quat_inf = quat_set(inf, inf, inf, inf);
		const QuatType quat_nan = quat_set(nan, nan, nan, nan);

		const Vector4Type trans0 = vector_set(FloatType(-0.001138), FloatType(0.91623), FloatType(-1.624598));
		const Vector4Type vec_inf = vector_set(inf, inf, inf, inf);
		const Vector4Type vec_nan = vector_set(nan, nan, nan, nan);

		CHECK(qv_is_finite(identity) == true);
		CHECK(qv_is_finite(qv_set(quat0, trans0)) == true);
		CHECK(qv_is_finite(qv_set(quat_inf, trans0)) == false);
		CHECK(qv_is_finite(qv_set(quat_nan, trans0)) == false);
		CHECK(qv_is_finite(qv_set(quat0, vec_inf)) == false);
		CHECK(qv_is_finite(qv_set(quat0, vec_nan)) == false);
	}
}

TEST_CASE("qvf math", "[math][qv]")
{
	test_qv_impl<qvf, float>(qv_identity(), 1.0E-4F);

	const quatf src_rotation = quat_set(0.39564531008956383F, 0.044254239301713752F, 0.22768840967675355F, 0.88863059760894492F);
	const vector4f src_translation = vector_set(-2.65F, 2.996113F, 0.68123521F);
	const qvf src = qv_set(src_rotation, src_translation);
	const qvd dst = qv_cast(src);
	CHECK(quat_near_equal(src.rotation, quat_cast(dst.rotation), 1.0E-6F));
	CHECK(vector_all_near_equal3(src.translation, vector_cast(dst.translation), 1.0E-6F));
}

TEST_CASE("qvd math", "[math][qv]")
{
	test_qv_impl<qvd, double>(qv_identity(), 1.0E-6);

	const quatd src_rotation = quat_set(0.39564531008956383, 0.044254239301713752, 0.22768840967675355, 0.88863059760894492);
	const vector4d src_translation = vector_set(-2.65, 2.996113, 0.68123521);
	const qvd src = qv_set(src_rotation, src_translation);
	const qvf dst = qv_cast(src);
	CHECK(quat_near_equal(src.rotation, quat_cast(dst.rotation), 1.0E-6));
	CHECK(vector_all_near_equal3(src.translation, vector_cast(dst.translation), 1.0E-6));
}

```

`tests/sources/test_qvs.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2023 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include "catch2.impl.h"

#include <rtm/qvsf.h>
#include <rtm/qvsd.h>
#include <rtm/type_traits.h>
#include <rtm/matrix3x3f.h>
#include <rtm/matrix3x3d.h>
#include <rtm/matrix3x4f.h>
#include <rtm/matrix3x4d.h>

using namespace rtm;

template<typename TransformType, typename FloatType>
static void test_qvs_interpolation(const FloatType threshold)
{
	using QuatType = typename related_types<FloatType>::quat;
	using Vector4Type = typename related_types<FloatType>::vector4;
	using ScalarType = typename related_types<FloatType>::scalar;

	FloatType alpha = FloatType(0.33);
	ScalarType alpha_s = scalar_set(alpha);
	QuatType quat0 = quat_normalize(quat_from_euler(scalar_deg_to_rad(FloatType(30.0)), scalar_deg_to_rad(FloatType(-45.0)), scalar_deg_to_rad(FloatType(90.0))));
	QuatType quat1 = quat_normalize(quat_from_euler(scalar_deg_to_rad(FloatType(45.0)), scalar_deg_to_rad(FloatType(60.0)), scalar_deg_to_rad(FloatType(120.0))));

	QuatType quat_ref_lerp = quat_lerp(quat0, quat1, alpha);
	QuatType quat_ref_lerp_s = quat_lerp(quat0, quat1, alpha_s);
	QuatType quat_ref_slerp = quat_slerp(quat0, quat1, alpha);
	QuatType quat_ref_slerp_s = quat_slerp(quat0, quat1, alpha_s);

	Vector4Type trans0 = vector_set(FloatType(-0.001138), FloatType(0.91623), FloatType(-1.624598));
	Vector4Type trans1 = vector_set(FloatType(-0.001138), FloatType(0.91623), FloatType(-1.624598));

	Vector4Type trans_ref = vector_lerp(trans0, trans1, alpha);
	Vector4Type trans_ref_s = vector_lerp(trans0, trans1, alpha_s);

	FloatType scale0 = FloatType(-1.915);
	FloatType scale1 = FloatType(-0.2113);

	FloatType scale_ref = scalar_lerp(scale0, scale1, alpha);
	ScalarType scale_ref_s = scalar_lerp(scalar_set(scale0), scalar_set(scale1), alpha_s);

	TransformType transform0 = qvs_set(quat0, trans0, scale0);
	TransformType transform1 = qvs_set(quat1, trans1, scale1);

	TransformType transform_ref_lerp = qvs_set(quat_ref_lerp, trans_ref, scale_ref);
	TransformType transform_ref_slerp = qvs_set(quat_ref_slerp, trans_ref, scale_ref);

	TransformType transform_ref_lerp_s = qvs_set(quat_ref_lerp_s, trans_ref_s, scale_ref_s);
	TransformType transform_ref_slerp_s = qvs_set(quat_ref_slerp_s, trans_ref_s, scale_ref_s);

	TransformType transform_lerp = qvs_lerp(transform0, transform1, alpha);
	TransformType transform_lerp_s = qvs_lerp(transform0, transform1, alpha_s);

	TransformType transform_slerp = qvs_slerp(transform0, transform1, alpha);
	TransformType transform_slerp_s = qvs_slerp(transform0, transform1, alpha_s);

	TransformType transform_lerp_no_scale = qvs_lerp_no_scale(transform0, transform1, alpha);
	TransformType transform_lerp_no_scale_s = qvs_lerp_no_scale(transform0, transform1, alpha_s);

	TransformType transform_slerp_no_scale = qvs_slerp_no_scale(transform0, transform1, alpha);
	TransformType transform_slerp_no_scale_s = qvs_slerp_no_scale(transform0, transform1, alpha_s);

	CHECK(quat_near_equal(transform_lerp.rotation, transform_ref_lerp.rotation, threshold));
	CHECK(quat_near_equal(transform_lerp_s.rotation, transform_ref_lerp_s.rotation, threshold));
	CHECK(vector_all_near_equal3(qvs_get_translation(transform_lerp), qvs_get_translation(transform_ref_lerp), threshold));
	CHECK(vector_all_near_equal3(qvs_get_translation(transform_lerp_s), qvs_get_translation(transform_ref_lerp_s), threshold));
	CHECK(scalar_near_equal(qvs_get_scale(transform_lerp), qvs_get_scale(transform_ref_lerp), threshold));
	CHECK(scalar_near_equal(qvs_get_scale(transform_lerp_s), qvs_get_scale(transform_ref_lerp_s), threshold));

	CHECK(quat_near_equal(transform_slerp.rotation, transform_ref_slerp.rotation, threshold));
	CHECK(quat_near_equal(transform_slerp_s.rotation, transform_ref_slerp_s.rotation, threshold));
	CHECK(vector_all_near_equal3(qvs_get_translation(transform_slerp), qvs_get_translation(transform_ref_slerp), threshold));
	CHECK(vector_all_near_equal3(qvs_get_translation(transform_slerp_s), qvs_get_translation(transform_ref_slerp_s), threshold));
	CHECK(scalar_near_equal(qvs_get_scale(transform_slerp), qvs_get_scale(transform_ref_slerp), threshold));
	CHECK(scalar_near_equal(qvs_get_scale(transform_slerp_s), qvs_get_scale(transform_ref_slerp_s), threshold));

	CHECK(quat_near_equal(transform_lerp_no_scale.rotation, transform_ref_lerp.rotation, threshold));
	CHECK(quat_near_equal(transform_lerp_no_scale_s.rotation, transform_ref_lerp_s.rotation, threshold));
	CHECK(vector_all_near_equal3(qvs_get_translation(transform_lerp_no_scale), qvs_get_translation(transform_ref_lerp), threshold));
	CHECK(vector_all_near_equal3(qvs_get_translation(transform_lerp_no_scale_s), qvs_get_translation(transform_ref_lerp_s), threshold));
	CHECK(scalar_near_equal(qvs_get_scale(transform_lerp_no_scale), qvs_get_scale(transform0), threshold));
	CHECK(scalar_near_equal(qvs_get_scale(transform_lerp_no_scale_s), qvs_get_scale(transform0), threshold));

	CHECK(quat_near_equal(transform_slerp_no_scale.rotation, transform_ref_slerp.rotation, threshold));
	CHECK(quat_near_equal(transform_slerp_no_scale_s.rotation, transform_ref_slerp_s.rotation, threshold));
	CHECK(vector_all_near_equal3(qvs_get_translation(transform_slerp_no_scale), qvs_get_translation(transform_ref_slerp), threshold));
	CHECK(vector_all_near_equal3(qvs_get_translation(transform_slerp_no_scale_s), qvs_get_translation(transform_ref_slerp_s), threshold));
	CHECK(scalar_near_equal(qvs_get_scale(transform_slerp_no_scale), qvs_get_scale(transform0), threshold));
	CHECK(scalar_near_equal(qvs_get_scale(transform_slerp_no_scale_s), qvs_get_scale(transform0), threshold));
}

template<typename TransformType, typename FloatType>
static void test_qvs_impl(const TransformType& identity, const FloatType threshold)
{
	using QuatType = typename related_types<FloatType>::quat;
	using Vector4Type = typename related_types<FloatType>::vector4;
	using Matrix3x3Type = typename related_types<FloatType>::matrix3x3;
	using Matrix3x4Type = typename related_types<FloatType>::matrix3x4;

	{
		Vector4Type zero = vector_set(FloatType(0.0));
		FloatType one = FloatType(1.0);
		QuatType q_identity = quat_set(FloatType(0.0), FloatType(0.0), FloatType(0.0), FloatType(1.0));
		TransformType tmp = qvs_set(q_identity, zero, one);
		CHECK(quat_near_equal(identity.rotation, tmp.rotation, threshold));
		CHECK(vector_all_near_equal(identity.translation_scale, tmp.translation_scale, threshold));
		CHECK(quat_near_equal(q_identity, qvs_get_rotation(tmp), threshold));
		CHECK(vector_all_near_equal3(zero, qvs_get_translation(tmp), threshold));
		CHECK(scalar_near_equal(one, qvs_get_scale(tmp), threshold));
	}

	{
		QuatType rotation_around_z = quat_from_euler(scalar_deg_to_rad(FloatType(0.0)), scalar_deg_to_rad(FloatType(90.0)), scalar_deg_to_rad(FloatType(0.0)));
		FloatType one = FloatType(1.0);
		Matrix3x3Type mtx = matrix_from_quat(rotation_around_z);
		TransformType transform = qvs_from_matrix(mtx);
		CHECK(quat_near_equal(rotation_around_z, transform.rotation, threshold));
		CHECK(vector_all_near_equal3(qvs_get_translation(identity), qvs_get_translation(transform), threshold));
		CHECK(scalar_near_equal(one, qvs_get_scale(transform), threshold));
	}

	{
		QuatType rotation_around_z = quat_from_euler(scalar_deg_to_rad(FloatType(0.0)), scalar_deg_to_rad(FloatType(90.0)), scalar_deg_to_rad(FloatType(0.0)));
		Vector4Type translation = vector_set(FloatType(1.0), FloatType(2.0), FloatType(3.0));
		FloatType one = FloatType(1.0);

		// Default scale
		Matrix3x4Type mtx = matrix_from_qv(rotation_around_z, translation);
		TransformType transform = qvs_from_matrix(mtx);
		CHECK(quat_near_equal(rotation_around_z, transform.rotation, threshold));
		CHECK(vector_all_near_equal3(translation, qvs_get_translation(transform), threshold));
		CHECK(scalar_near_equal(one, qvs_get_scale(transform), threshold));

		// Positive scale
		FloatType scale = FloatType(1.6);
		mtx = matrix_from_qvs(rotation_around_z, translation, scale);
		transform = qvs_from_matrix(mtx);
		CHECK(quat_near_equal(rotation_around_z, transform.rotation, threshold));
		CHECK(vector_all_near_equal3(translation, qvs_get_translation(transform), threshold));
		CHECK(scalar_near_equal(scale, qvs_get_scale(transform), threshold));

		// Zero scale
		scale = FloatType(0.0);
		mtx = matrix_from_qvs(rotation_around_z, translation, scale);
		transform = qvs_from_matrix(mtx);
		Matrix3x4Type mtx2 = matrix_from_qvs(transform);
		CHECK(quat_near_equal(identity.rotation, transform.rotation, threshold));
		CHECK(vector_all_near_equal3(translation, qvs_get_translation(transform), threshold));
		CHECK(scalar_near_equal(scale, qvs_get_scale(transform), threshold));
		CHECK(vector_all_near_equal3(mtx.x_axis, mtx2.x_axis, threshold));
		CHECK(vector_all_near_equal3(mtx.y_axis, mtx2.y_axis, threshold));
		CHECK(vector_all_near_equal3(mtx.z_axis, mtx2.z_axis, threshold));
		CHECK(vector_all_near_equal3(mtx.w_axis, mtx2.w_axis, threshold));

		// Negative scale
		scale = FloatType(-1.6);
		mtx = matrix_from_qvs(rotation_around_z, translation, scale);
		transform = qvs_from_matrix(mtx);
		mtx2 = matrix_from_qvs(transform);
		CHECK(quat_near_equal(rotation_around_z, transform.rotation, threshold));
		CHECK(vector_all_near_equal3(translation, qvs_get_translation(transform), threshold));
		CHECK(scalar_near_equal(scale, qvs_get_scale(transform), threshold));
		CHECK(vector_all_near_equal3(mtx.x_axis, mtx2.x_axis, threshold));
		CHECK(vector_all_near_equal3(mtx.y_axis, mtx2.y_axis, threshold));
		CHECK(vector_all_near_equal3(mtx.z_axis, mtx2.z_axis, threshold));
		CHECK(vector_all_near_equal3(mtx.w_axis, mtx2.w_axis, threshold));
	}

	{
		Vector4Type x_axis = vector_set(FloatType(1.0), FloatType(0.0), FloatType(0.0));
		Vector4Type y_axis = vector_set(FloatType(0.0), FloatType(1.0), FloatType(0.0));
		FloatType test_scale = FloatType(1.2);

		QuatType rotation_around_z = quat_from_euler(scalar_deg_to_rad(FloatType(0.0)), scalar_deg_to_rad(FloatType(90.0)), scalar_deg_to_rad(FloatType(0.0)));
		TransformType transform_a = qvs_set(rotation_around_z, x_axis, test_scale);
		Vector4Type result = qvs_mul_point3(x_axis, transform_a);
		CHECK(vector_all_near_equal3(result, vector_set(FloatType(1.0), FloatType(1.2), FloatType(0.0)), threshold));
		result = qvs_mul_point3(y_axis, transform_a);
		CHECK(vector_all_near_equal3(result, vector_set(FloatType(-0.2), FloatType(0.0), FloatType(0.0)), threshold));

		QuatType rotation_around_x = quat_from_euler(scalar_deg_to_rad(FloatType(0.0)), scalar_deg_to_rad(FloatType(0.0)), scalar_deg_to_rad(FloatType(90.0)));
		TransformType transform_b = qvs_set(rotation_around_x, y_axis, test_scale);
		result = qvs_mul_point3(x_axis, transform_b);
		CHECK(vector_all_near_equal3(result, vector_set(FloatType(1.2), FloatType(1.0), FloatType(0.0)), threshold));
		result = qvs_mul_point3(y_axis, transform_b);
		CHECK(vector_all_near_equal3(result, vector_set(FloatType(0.0), FloatType(1.0), FloatType(-1.2)), threshold));

		TransformType transform_ab = qvs_mul(transform_a, transform_b);
		TransformType transform_ba = qvs_mul(transform_b, transform_a);
		result = qvs_mul_point3(x_axis, transform_ab);
		CHECK(vector_all_near_equal3(result, vector_set(FloatType(1.2), FloatType(1.0), FloatType(-1.44)), threshold));
		CHECK(vector_all_near_equal3(result, qvs_mul_point3(qvs_mul_point3(x_axis, transform_a), transform_b), threshold));
		result = qvs_mul_point3(y_axis, transform_ab);
		CHECK(vector_all_near_equal3(result, vector_set(FloatType(-0.24), FloatType(1.0), FloatType(0.0)), threshold));
		CHECK(vector_all_near_equal3(result, qvs_mul_point3(qvs_mul_point3(y_axis, transform_a), transform_b), threshold));
		result = qvs_mul_point3(x_axis, transform_ba);
		CHECK(vector_all_near_equal3(result, vector_set(FloatType(-0.2), FloatType(1.44), FloatType(0.0)), threshold));
		CHECK(vector_all_near_equal3(result, qvs_mul_point3(qvs_mul_point3(x_axis, transform_b), transform_a), threshold));
		result = qvs_mul_point3(y_axis, transform_ba);
		CHECK(vector_all_near_equal3(result, vector_set(FloatType(-0.2), FloatType(0.0), FloatType(-1.44)), threshold));
		CHECK(vector_all_near_equal3(result, qvs_mul_point3(qvs_mul_point3(y_axis, transform_b), transform_a), threshold));
	}

	{
		Vector4Type x_axis = vector_set(FloatType(1.0), FloatType(0.0), FloatType(0.0));
		Vector4Type y_axis = vector_set(FloatType(0.0), FloatType(1.0), FloatType(0.0));

		QuatType rotation_around_z = quat_from_euler(scalar_deg_to_rad(FloatType(0.0)), scalar_deg_to_rad(FloatType(90.0)), scalar_deg_to_rad(FloatType(0.0)));
		TransformType transform_a = qvs_set(rotation_around_z, x_axis, FloatType(1.0));
		Vector4Type result = qvs_mul_point3_no_scale(x_axis, transform_a);
		CHECK(vector_all_near_equal3(result, vector_set(FloatType(1.0), FloatType(1.0), FloatType(0.0)), threshold));
		result = qvs_mul_point3_no_scale(y_axis, transform_a);
		CHECK(vector_all_near_equal3(result, vector_set(FloatType(0.0), FloatType(0.0), FloatType(0.0)), threshold));

		QuatType rotation_around_x = quat_from_euler(scalar_deg_to_rad(FloatType(0.0)), scalar_deg_to_rad(FloatType(0.0)), scalar_deg_to_rad(FloatType(90.0)));
		TransformType transform_b = qvs_set(rotation_around_x, y_axis, FloatType(1.0));
		result = qvs_mul_point3_no_scale(x_axis, transform_b);
		CHECK(vector_all_near_equal3(result, vector_set(FloatType(1.0), FloatType(1.0), FloatType(0.0)), threshold));
		result = qvs_mul_point3_no_scale(y_axis, transform_b);
		CHECK(vector_all_near_equal3(result, vector_set(FloatType(0.0), FloatType(1.0), FloatType(-1.0)), threshold));

		TransformType transform_ab = qvs_mul_no_scale(transform_a, transform_b);
		TransformType transform_ba = qvs_mul_no_scale(transform_b, transform_a);
		result = qvs_mul_point3_no_scale(x_axis, transform_ab);
		CHECK(vector_all_near_equal3(result, vector_set(FloatType(1.0), FloatType(1.0), FloatType(-1.0)), threshold));
		CHECK(vector_all_near_equal3(result, qvs_mul_point3_no_scale(qvs_mul_point3_no_scale(x_axis, transform_a), transform_b), threshold));
		result = qvs_mul_point3_no_scale(y_axis, transform_ab);
		CHECK(vector_all_near_equal3(result, vector_set(FloatType(0.0), FloatType(1.0), FloatType(0.0)), threshold));
		CHECK(vector_all_near_equal3(result, qvs_mul_point3_no_scale(qvs_mul_point3_no_scale(y_axis, transform_a), transform_b), threshold));
		result = qvs_mul_point3_no_scale(x_axis, transform_ba);
		CHECK(vector_all_near_equal3(result, vector_set(FloatType(0.0), FloatType(1.0), FloatType(0.0)), threshold));
		CHECK(vector_all_near_equal3(result, qvs_mul_point3_no_scale(qvs_mul_point3_no_scale(x_axis, transform_b), transform_a), threshold));
		result = qvs_mul_point3_no_scale(y_axis, transform_ba);
		CHECK(vector_all_near_equal3(result, vector_set(FloatType(0.0), FloatType(0.0), FloatType(-1.0)), threshold));
		CHECK(vector_all_near_equal3(result, qvs_mul_point3_no_scale(qvs_mul_point3_no_scale(y_axis, transform_b), transform_a), threshold));
	}

	{
		Vector4Type x_axis = vector_set(FloatType(1.0), FloatType(0.0), FloatType(0.0));
		FloatType test_scale1 = FloatType(1.2);
		FloatType test_scale2 = FloatType(-1.2);
		FloatType test_scale3 = FloatType(0.0);

		QuatType rotation_around_z = quat_from_euler(scalar_deg_to_rad(FloatType(0.0)), scalar_deg_to_rad(FloatType(90.0)), scalar_deg_to_rad(FloatType(0.0)));
		TransformType transform_a = qvs_set(rotation_around_z, x_axis, test_scale1);
		TransformType transform_b = qvs_inverse(transform_a);
		TransformType transform_ab = qvs_mul(transform_a, transform_b);
		CHECK(quat_near_equal(identity.rotation, transform_ab.rotation, threshold));
		CHECK(vector_all_near_equal3(qvs_get_translation(identity), qvs_get_translation(transform_ab), threshold));
		CHECK(scalar_near_equal(qvs_get_scale(identity), qvs_get_scale(transform_ab), threshold));

		transform_a = qvs_set(rotation_around_z, x_axis, test_scale2);
		transform_b = qvs_inverse(transform_a);
		transform_ab = qvs_mul(transform_a, transform_b);
		CHECK(quat_near_equal(identity.rotation, transform_ab.rotation, threshold));
		CHECK(vector_all_near_equal3(qvs_get_translation(identity), qvs_get_translation(transform_ab), threshold));
		CHECK(scalar_near_equal(qvs_get_scale(identity), qvs_get_scale(transform_ab), threshold));

		transform_a = qvs_set(rotation_around_z, x_axis, test_scale3);
		transform_b = qvs_inverse(transform_a, FloatType(1.0));
		CHECK(qvs_is_finite(transform_b));
	}

	{
		Vector4Type x_axis = vector_set(FloatType(1.0), FloatType(0.0), FloatType(0.0));

		QuatType rotation_around_z = quat_from_euler(scalar_deg_to_rad(FloatType(0.0)), scalar_deg_to_rad(FloatType(90.0)), scalar_deg_to_rad(FloatType(0.0)));
		TransformType transform_a = qvs_set(rotation_around_z, x_axis, FloatType(1.0));
		TransformType transform_b = qvs_inverse_no_scale(transform_a);
		TransformType transform_ab = qvs_mul_no_scale(transform_a, transform_b);
		CHECK(quat_near_equal(identity.rotation, transform_ab.rotation, threshold));
		CHECK(vector_all_near_equal3(qvs_get_translation(identity), qvs_get_translation(transform_ab), threshold));
		CHECK(scalar_near_equal(qvs_get_scale(identity), qvs_get_scale(transform_ab), threshold));
	}

	{
		Vector4Type x_axis = vector_set(FloatType(1.0), FloatType(0.0), FloatType(0.0));
		QuatType rotation_around_z = quat_from_euler(scalar_deg_to_rad(FloatType(0.0)), scalar_deg_to_rad(FloatType(90.0)), scalar_deg_to_rad(FloatType(0.0)));
		TransformType transform_a = qvs_set(rotation_around_z, x_axis, FloatType(1.0));
		transform_a = qvs_normalize(transform_a);
		CHECK(quat_is_normalized(transform_a.rotation, threshold));

		QuatType quat = quat_set(FloatType(-0.001138), FloatType(0.91623), FloatType(-1.624598), FloatType(0.715671));
		TransformType transform_b = qvs_set(quat, x_axis, FloatType(1.0));
		CHECK(!quat_is_normalized(transform_b.rotation, threshold));
		transform_b = qvs_normalize(transform_b);
		CHECK(quat_is_normalized(transform_b.rotation, threshold));
	}

	test_qvs_interpolation<TransformType>(threshold);

	{
		const FloatType inf = std::numeric_limits<FloatType>::infinity();
		const FloatType nan = std::numeric_limits<FloatType>::quiet_NaN();

		const QuatType quat0 = quat_normalize(quat_from_euler(scalar_deg_to_rad(FloatType(30.0)), scalar_deg_to_rad(FloatType(-45.0)), scalar_deg_to_rad(FloatType(90.0))));
		const QuatType quat_inf = quat_set(inf, inf, inf, inf);
		const QuatType quat_nan = quat_set(nan, nan, nan, nan);

		const Vector4Type trans0 = vector_set(FloatType(-0.001138), FloatType(0.91623), FloatType(-1.624598));
		const FloatType scale0 = FloatType(-1.915);
		const Vector4Type vec_inf = vector_set(inf, inf, inf, inf);
		const Vector4Type vec_nan = vector_set(nan, nan, nan, nan);

		CHECK(qvs_is_finite(identity) == true);
		CHECK(qvs_is_finite(qvs_set(quat0, trans0, scale0)) == true);
		CHECK(qvs_is_finite(qvs_set(quat_inf, trans0, scale0)) == false);
		CHECK(qvs_is_finite(qvs_set(quat_nan, trans0, scale0)) == false);
		CHECK(qvs_is_finite(qvs_set(quat0, vec_inf, scale0)) == false);
		CHECK(qvs_is_finite(qvs_set(quat0, vec_nan, scale0)) == false);
		CHECK(qvs_is_finite(qvs_set(quat0, trans0, inf)) == false);
		CHECK(qvs_is_finite(qvs_set(quat0, trans0, nan)) == false);
		CHECK(qvs_is_finite(qvs_set(quat_inf, vec_inf, inf)) == false);
		CHECK(qvs_is_finite(qvs_set(quat_nan, vec_nan, nan)) == false);
	}
}

TEST_CASE("qvsf math", "[math][qvs]")
{
	test_qvs_impl<qvsf, float>(qvs_identity(), 1.0E-4F);

	const quatf src_rotation = quat_set(0.39564531008956383F, 0.044254239301713752F, 0.22768840967675355F, 0.88863059760894492F);
	const vector4f src_translation = vector_set(-2.65F, 2.996113F, 0.68123521F);
	const float src_scale = 1.2F;
	const qvsf src = qvs_set(src_rotation, src_translation, src_scale);
	const qvsd dst = qvs_cast(src);
	CHECK(quat_near_equal(src.rotation, quat_cast(dst.rotation), 1.0E-6F));
	CHECK(vector_all_near_equal(src.translation_scale, vector_cast(dst.translation_scale), 1.0E-6F));
}

TEST_CASE("qvsd math", "[math][qvs]")
{
	test_qvs_impl<qvsd, double>(qvs_identity(), 1.0E-6);

	const quatd src_rotation = quat_set(0.39564531008956383, 0.044254239301713752, 0.22768840967675355, 0.88863059760894492);
	const vector4d src_translation = vector_set(-2.65, 2.996113, 0.68123521);
	const double src_scale = 1.2;
	const qvsd src = qvs_set(src_rotation, src_translation, src_scale);
	const qvsf dst = qvs_cast(src);
	CHECK(quat_near_equal(src.rotation, quat_cast(dst.rotation), 1.0E-6));
	CHECK(vector_all_near_equal(src.translation_scale, vector_cast(dst.translation_scale), 1.0E-6));
}

```

`tests/sources/test_qvv.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2018 Nicholas Frechette & Animation Compression Library contributors
// Copyright (c) 2018 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include "catch2.impl.h"

#include <rtm/qvvf.h>
#include <rtm/qvvd.h>

using namespace rtm;

template<typename TransformType, typename FloatType>
static void test_qvv_interpolation(const FloatType threshold)
{
	using QuatType = decltype(TransformType::rotation);
	using Vector4Type = decltype(TransformType::translation);
	using ScalarType = typename related_types<FloatType>::scalar;

	FloatType alpha = FloatType(0.33);
	ScalarType alpha_s = scalar_set(alpha);
	QuatType quat0 = quat_normalize(quat_from_euler(scalar_deg_to_rad(FloatType(30.0)), scalar_deg_to_rad(FloatType(-45.0)), scalar_deg_to_rad(FloatType(90.0))));
	QuatType quat1 = quat_normalize(quat_from_euler(scalar_deg_to_rad(FloatType(45.0)), scalar_deg_to_rad(FloatType(60.0)), scalar_deg_to_rad(FloatType(120.0))));

	QuatType quat_ref_lerp = quat_lerp(quat0, quat1, alpha);
	QuatType quat_ref_lerp_s = quat_lerp(quat0, quat1, alpha_s);
	QuatType quat_ref_slerp = quat_slerp(quat0, quat1, alpha);
	QuatType quat_ref_slerp_s = quat_slerp(quat0, quat1, alpha_s);

	Vector4Type trans0 = vector_set(FloatType(-0.001138), FloatType(0.91623), FloatType(-1.624598));
	Vector4Type trans1 = vector_set(FloatType(-0.001138), FloatType(0.91623), FloatType(-1.624598));

	Vector4Type trans_ref = vector_lerp(trans0, trans1, alpha);
	Vector4Type trans_ref_s = vector_lerp(trans0, trans1, alpha_s);

	Vector4Type scale0 = vector_set(FloatType(-1.915), FloatType(0.23656), FloatType(-3.7811));
	Vector4Type scale1 = vector_set(FloatType(-0.2113), FloatType(12.22335), FloatType(-1.7261));

	Vector4Type scale_ref = vector_lerp(scale0, scale1, alpha);
	Vector4Type scale_ref_s = vector_lerp(scale0, scale1, alpha_s);

	TransformType transform0 = qvv_set(quat0, trans0, scale0);
	TransformType transform1 = qvv_set(quat1, trans1, scale1);

	TransformType transform_ref_lerp = qvv_set(quat_ref_lerp, trans_ref, scale_ref);
	TransformType transform_ref_slerp = qvv_set(quat_ref_slerp, trans_ref, scale_ref);

	TransformType transform_ref_lerp_s = qvv_set(quat_ref_lerp_s, trans_ref_s, scale_ref_s);
	TransformType transform_ref_slerp_s = qvv_set(quat_ref_slerp_s, trans_ref_s, scale_ref_s);

	TransformType transform_lerp = qvv_lerp(transform0, transform1, alpha);
	TransformType transform_lerp_s = qvv_lerp(transform0, transform1, alpha_s);

	TransformType transform_slerp = qvv_slerp(transform0, transform1, alpha);
	TransformType transform_slerp_s = qvv_slerp(transform0, transform1, alpha_s);

	TransformType transform_lerp_no_scale = qvv_lerp_no_scale(transform0, transform1, alpha);
	TransformType transform_lerp_no_scale_s = qvv_lerp_no_scale(transform0, transform1, alpha_s);

	TransformType transform_slerp_no_scale = qvv_slerp_no_scale(transform0, transform1, alpha);
	TransformType transform_slerp_no_scale_s = qvv_slerp_no_scale(transform0, transform1, alpha_s);

	CHECK(quat_near_equal(transform_lerp.rotation, transform_ref_lerp.rotation, threshold));
	CHECK(quat_near_equal(transform_lerp_s.rotation, transform_ref_lerp_s.rotation, threshold));
	CHECK(vector_all_near_equal3(transform_lerp.translation, transform_ref_lerp.translation, threshold));
	CHECK(vector_all_near_equal3(transform_lerp_s.translation, transform_ref_lerp_s.translation, threshold));
	CHECK(vector_all_near_equal3(transform_lerp.scale, transform_ref_lerp.scale, threshold));
	CHECK(vector_all_near_equal3(transform_lerp_s.scale, transform_ref_lerp_s.scale, threshold));

	CHECK(quat_near_equal(transform_slerp.rotation, transform_ref_slerp.rotation, threshold));
	CHECK(quat_near_equal(transform_slerp_s.rotation, transform_ref_slerp_s.rotation, threshold));
	CHECK(vector_all_near_equal3(transform_slerp.translation, transform_ref_slerp.translation, threshold));
	CHECK(vector_all_near_equal3(transform_slerp_s.translation, transform_ref_slerp_s.translation, threshold));
	CHECK(vector_all_near_equal3(transform_slerp.scale, transform_ref_slerp.scale, threshold));
	CHECK(vector_all_near_equal3(transform_slerp_s.scale, transform_ref_slerp_s.scale, threshold));

	CHECK(quat_near_equal(transform_lerp_no_scale.rotation, transform_ref_lerp.rotation, threshold));
	CHECK(quat_near_equal(transform_lerp_no_scale_s.rotation, transform_ref_lerp_s.rotation, threshold));
	CHECK(vector_all_near_equal3(transform_lerp_no_scale.translation, transform_ref_lerp.translation, threshold));
	CHECK(vector_all_near_equal3(transform_lerp_no_scale_s.translation, transform_ref_lerp_s.translation, threshold));
	CHECK(vector_all_near_equal3(transform_lerp_no_scale.scale, transform0.scale, threshold));
	CHECK(vector_all_near_equal3(transform_lerp_no_scale_s.scale, transform0.scale, threshold));

	CHECK(quat_near_equal(transform_slerp_no_scale.rotation, transform_ref_slerp.rotation, threshold));
	CHECK(quat_near_equal(transform_slerp_no_scale_s.rotation, transform_ref_slerp_s.rotation, threshold));
	CHECK(vector_all_near_equal3(transform_slerp_no_scale.translation, transform_ref_slerp.translation, threshold));
	CHECK(vector_all_near_equal3(transform_slerp_no_scale_s.translation, transform_ref_slerp_s.translation, threshold));
	CHECK(vector_all_near_equal3(transform_slerp_no_scale.scale, transform0.scale, threshold));
	CHECK(vector_all_near_equal3(transform_slerp_no_scale_s.scale, transform0.scale, threshold));
}

template<typename TransformType, typename FloatType>
static void test_qvv_impl(const TransformType& identity, const FloatType threshold)
{
	using QuatType = decltype(TransformType::rotation);
	using Vector4Type = decltype(TransformType::translation);
	using Matrix3x3Type = typename related_types<FloatType>::matrix3x3;
	using Matrix3x4Type = typename related_types<FloatType>::matrix3x4;

	{
		Vector4Type zero = vector_set(FloatType(0.0));
		Vector4Type one = vector_set(FloatType(1.0));
		QuatType q_identity = quat_set(FloatType(0.0), FloatType(0.0), FloatType(0.0), FloatType(1.0));
		TransformType tmp = qvv_set(q_identity, zero, one);
		CHECK(quat_near_equal(identity.rotation, tmp.rotation, threshold));
		CHECK(vector_all_near_equal3(identity.translation, tmp.translation, threshold));
		CHECK(vector_all_near_equal3(identity.scale, tmp.scale, threshold));
		CHECK(quat_near_equal(q_identity, tmp.rotation, threshold));
		CHECK(vector_all_near_equal3(zero, tmp.translation, threshold));
		CHECK(vector_all_near_equal3(one, tmp.scale, threshold));
	}

	{
		QuatType rotation_around_z = quat_from_euler(scalar_deg_to_rad(FloatType(0.0)), scalar_deg_to_rad(FloatType(90.0)), scalar_deg_to_rad(FloatType(0.0)));
		Matrix3x3Type mtx = matrix_from_quat(rotation_around_z);
		TransformType transform = qvv_from_matrix(mtx);
		CHECK(quat_near_equal(rotation_around_z, transform.rotation, threshold));
		CHECK(vector_all_near_equal3(identity.translation, transform.translation, threshold));
		CHECK(vector_all_near_equal3(identity.scale, transform.scale, threshold));
	}

	{
		QuatType rotation_around_z = quat_from_euler(scalar_deg_to_rad(FloatType(0.0)), scalar_deg_to_rad(FloatType(90.0)), scalar_deg_to_rad(FloatType(0.0)));
		Vector4Type translation = vector_set(FloatType(1.0), FloatType(2.0), FloatType(3.0));

		// Default scale
		Matrix3x4Type mtx = matrix_from_qv(rotation_around_z, translation);
		TransformType transform = qvv_from_matrix(mtx);
		CHECK(quat_near_equal(rotation_around_z, transform.rotation, threshold));
		CHECK(vector_all_near_equal3(translation, transform.translation, threshold));
		CHECK(vector_all_near_equal3(identity.scale, transform.scale, threshold));

		// All positive scale
		Vector4Type scale = vector_set(FloatType(1.2), FloatType(0.2), FloatType(1.6));
		mtx = matrix_from_qvv(rotation_around_z, translation, scale);
		transform = qvv_from_matrix(mtx);
		CHECK(quat_near_equal(rotation_around_z, transform.rotation, threshold));
		CHECK(vector_all_near_equal3(translation, transform.translation, threshold));
		CHECK(vector_all_near_equal3(scale, transform.scale, threshold));

		// X has zero scale
		scale = vector_set(FloatType(0.0), FloatType(0.2), FloatType(1.6));
		mtx = matrix_from_qvv(rotation_around_z, translation, scale);
		transform = qvv_from_matrix(mtx);
		Matrix3x4Type mtx2 = matrix_from_qvv(transform);
		CHECK(vector_all_near_equal3(mtx.x_axis, mtx2.x_axis, threshold));
		CHECK(vector_all_near_equal3(mtx.y_axis, mtx2.y_axis, threshold));
		CHECK(vector_all_near_equal3(mtx.z_axis, mtx2.z_axis, threshold));
		CHECK(vector_all_near_equal3(mtx.w_axis, mtx2.w_axis, threshold));

		// Y has zero scale
		scale = vector_set(FloatType(1.2), FloatType(0.0), FloatType(1.6));
		mtx = matrix_from_qvv(rotation_around_z, translation, scale);
		transform = qvv_from_matrix(mtx);
		mtx2 = matrix_from_qvv(transform);
		CHECK(vector_all_near_equal3(mtx.x_axis, mtx2.x_axis, threshold));
		CHECK(vector_all_near_equal3(mtx.y_axis, mtx2.y_axis, threshold));
		CHECK(vector_all_near_equal3(mtx.z_axis, mtx2.z_axis, threshold));
		CHECK(vector_all_near_equal3(mtx.w_axis, mtx2.w_axis, threshold));

		// Z has zero scale
		scale = vector_set(FloatType(1.2), FloatType(0.2), FloatType(0.0));
		mtx = matrix_from_qvv(rotation_around_z, translation, scale);
		transform = qvv_from_matrix(mtx);
		mtx2 = matrix_from_qvv(transform);
		CHECK(vector_all_near_equal3(mtx.x_axis, mtx2.x_axis, threshold));
		CHECK(vector_all_near_equal3(mtx.y_axis, mtx2.y_axis, threshold));
		CHECK(vector_all_near_equal3(mtx.z_axis, mtx2.z_axis, threshold));
		CHECK(vector_all_near_equal3(mtx.w_axis, mtx2.w_axis, threshold));

		// XY have zero scale
		scale = vector_set(FloatType(0.0), FloatType(0.0), FloatType(1.6));
		mtx = matrix_from_qvv(rotation_around_z, translation, scale);
		transform = qvv_from_matrix(mtx);
		mtx2 = matrix_from_qvv(transform);
		CHECK(vector_all_near_equal3(mtx.x_axis, mtx2.x_axis, threshold));
		CHECK(vector_all_near_equal3(mtx.y_axis, mtx2.y_axis, threshold));
		CHECK(vector_all_near_equal3(mtx.z_axis, mtx2.z_axis, threshold));
		CHECK(vector_all_near_equal3(mtx.w_axis, mtx2.w_axis, threshold));

		// XZ have zero scale
		scale = vector_set(FloatType(0.0), FloatType(0.2), FloatType(0.0));
		mtx = matrix_from_qvv(rotation_around_z, translation, scale);
		transform = qvv_from_matrix(mtx);
		mtx2 = matrix_from_qvv(transform);
		CHECK(vector_all_near_equal3(mtx.x_axis, mtx2.x_axis, threshold));
		CHECK(vector_all_near_equal3(mtx.y_axis, mtx2.y_axis, threshold));
		CHECK(vector_all_near_equal3(mtx.z_axis, mtx2.z_axis, threshold));
		CHECK(vector_all_near_equal3(mtx.w_axis, mtx2.w_axis, threshold));

		// YZ have zero scale
		scale = vector_set(FloatType(1.2), FloatType(0.0), FloatType(0.0));
		mtx = matrix_from_qvv(rotation_around_z, translation, scale);
		transform = qvv_from_matrix(mtx);
		mtx2 = matrix_from_qvv(transform);
		CHECK(vector_all_near_equal3(mtx.x_axis, mtx2.x_axis, threshold));
		CHECK(vector_all_near_equal3(mtx.y_axis, mtx2.y_axis, threshold));
		CHECK(vector_all_near_equal3(mtx.z_axis, mtx2.z_axis, threshold));
		CHECK(vector_all_near_equal3(mtx.w_axis, mtx2.w_axis, threshold));

		// XYZ have zero scale
		scale = vector_set(FloatType(0.0), FloatType(0.0), FloatType(0.0));
		mtx = matrix_from_qvv(rotation_around_z, translation, scale);
		transform = qvv_from_matrix(mtx);
		mtx2 = matrix_from_qvv(transform);
		CHECK(quat_near_equal(identity.rotation, transform.rotation, threshold));
		CHECK(vector_all_near_equal3(mtx.x_axis, mtx2.x_axis, threshold));
		CHECK(vector_all_near_equal3(mtx.y_axis, mtx2.y_axis, threshold));
		CHECK(vector_all_near_equal3(mtx.z_axis, mtx2.z_axis, threshold));
		CHECK(vector_all_near_equal3(mtx.w_axis, mtx2.w_axis, threshold));

		// X has negative scale
		scale = vector_set(FloatType(-1.2), FloatType(0.2), FloatType(1.6));
		mtx = matrix_from_qvv(rotation_around_z, translation, scale);
		transform = qvv_from_matrix(mtx);
		mtx2 = matrix_from_qvv(transform);
		CHECK(vector_all_near_equal3(mtx.x_axis, mtx2.x_axis, threshold));
		CHECK(vector_all_near_equal3(mtx.y_axis, mtx2.y_axis, threshold));
		CHECK(vector_all_near_equal3(mtx.z_axis, mtx2.z_axis, threshold));
		CHECK(vector_all_near_equal3(mtx.w_axis, mtx2.w_axis, threshold));

		// Y has negative scale
		scale = vector_set(FloatType(1.2), FloatType(-0.2), FloatType(1.6));
		mtx = matrix_from_qvv(rotation_around_z, translation, scale);
		transform = qvv_from_matrix(mtx);
		mtx2 = matrix_from_qvv(transform);
		CHECK(vector_all_near_equal3(mtx.x_axis, mtx2.x_axis, threshold));
		CHECK(vector_all_near_equal3(mtx.y_axis, mtx2.y_axis, threshold));
		CHECK(vector_all_near_equal3(mtx.z_axis, mtx2.z_axis, threshold));
		CHECK(vector_all_near_equal3(mtx.w_axis, mtx2.w_axis, threshold));

		// Z has negative scale
		scale = vector_set(FloatType(1.2), FloatType(0.2), FloatType(-1.6));
		mtx = matrix_from_qvv(rotation_around_z, translation, scale);
		transform = qvv_from_matrix(mtx);
		mtx2 = matrix_from_qvv(transform);
		CHECK(vector_all_near_equal3(mtx.x_axis, mtx2.x_axis, threshold));
		CHECK(vector_all_near_equal3(mtx.y_axis, mtx2.y_axis, threshold));
		CHECK(vector_all_near_equal3(mtx.z_axis, mtx2.z_axis, threshold));
		CHECK(vector_all_near_equal3(mtx.w_axis, mtx2.w_axis, threshold));

		// XY have negative scale
		scale = vector_set(FloatType(-1.2), FloatType(-0.2), FloatType(1.6));
		mtx = matrix_from_qvv(rotation_around_z, translation, scale);
		transform = qvv_from_matrix(mtx);
		mtx2 = matrix_from_qvv(transform);
		CHECK(vector_all_near_equal3(mtx.x_axis, mtx2.x_axis, threshold));
		CHECK(vector_all_near_equal3(mtx.y_axis, mtx2.y_axis, threshold));
		CHECK(vector_all_near_equal3(mtx.z_axis, mtx2.z_axis, threshold));
		CHECK(vector_all_near_equal3(mtx.w_axis, mtx2.w_axis, threshold));

		// XZ have negative scale
		scale = vector_set(FloatType(-1.2), FloatType(0.2), FloatType(-1.6));
		mtx = matrix_from_qvv(rotation_around_z, translation, scale);
		transform = qvv_from_matrix(mtx);
		mtx2 = matrix_from_qvv(transform);
		CHECK(vector_all_near_equal3(mtx.x_axis, mtx2.x_axis, threshold));
		CHECK(vector_all_near_equal3(mtx.y_axis, mtx2.y_axis, threshold));
		CHECK(vector_all_near_equal3(mtx.z_axis, mtx2.z_axis, threshold));
		CHECK(vector_all_near_equal3(mtx.w_axis, mtx2.w_axis, threshold));

		// YZ have negative scale
		scale = vector_set(FloatType(1.2), FloatType(-0.2), FloatType(-1.6));
		mtx = matrix_from_qvv(rotation_around_z, translation, scale);
		transform = qvv_from_matrix(mtx);
		mtx2 = matrix_from_qvv(transform);
		CHECK(vector_all_near_equal3(mtx.x_axis, mtx2.x_axis, threshold));
		CHECK(vector_all_near_equal3(mtx.y_axis, mtx2.y_axis, threshold));
		CHECK(vector_all_near_equal3(mtx.z_axis, mtx2.z_axis, threshold));
		CHECK(vector_all_near_equal3(mtx.w_axis, mtx2.w_axis, threshold));

		// XYZ have negative scale
		scale = vector_set(FloatType(-1.2), FloatType(-0.2), FloatType(-1.6));
		mtx = matrix_from_qvv(rotation_around_z, translation, scale);
		transform = qvv_from_matrix(mtx);
		mtx2 = matrix_from_qvv(transform);
		CHECK(vector_all_near_equal3(mtx.x_axis, mtx2.x_axis, threshold));
		CHECK(vector_all_near_equal3(mtx.y_axis, mtx2.y_axis, threshold));
		CHECK(vector_all_near_equal3(mtx.z_axis, mtx2.z_axis, threshold));
		CHECK(vector_all_near_equal3(mtx.w_axis, mtx2.w_axis, threshold));
	}

	{
		Vector4Type x_axis = vector_set(FloatType(1.0), FloatType(0.0), FloatType(0.0));
		Vector4Type y_axis = vector_set(FloatType(0.0), FloatType(1.0), FloatType(0.0));
		Vector4Type test_scale = vector_set(FloatType(1.2));

		QuatType rotation_around_z = quat_from_euler(scalar_deg_to_rad(FloatType(0.0)), scalar_deg_to_rad(FloatType(90.0)), scalar_deg_to_rad(FloatType(0.0)));
		TransformType transform_a = qvv_set(rotation_around_z, x_axis, test_scale);
		Vector4Type result = qvv_mul_point3(x_axis, transform_a);
		CHECK(vector_all_near_equal3(result, vector_set(FloatType(1.0), FloatType(1.2), FloatType(0.0)), threshold));
		result = qvv_mul_point3(y_axis, transform_a);
		CHECK(vector_all_near_equal3(result, vector_set(FloatType(-0.2), FloatType(0.0), FloatType(0.0)), threshold));

		QuatType rotation_around_x = quat_from_euler(scalar_deg_to_rad(FloatType(0.0)), scalar_deg_to_rad(FloatType(0.0)), scalar_deg_to_rad(FloatType(90.0)));
		TransformType transform_b = qvv_set(rotation_around_x, y_axis, test_scale);
		result = qvv_mul_point3(x_axis, transform_b);
		CHECK(vector_all_near_equal3(result, vector_set(FloatType(1.2), FloatType(1.0), FloatType(0.0)), threshold));
		result = qvv_mul_point3(y_axis, transform_b);
		CHECK(vector_all_near_equal3(result, vector_set(FloatType(0.0), FloatType(1.0), FloatType(-1.2)), threshold));

		TransformType transform_ab = qvv_mul(transform_a, transform_b);
		TransformType transform_ba = qvv_mul(transform_b, transform_a);
		result = qvv_mul_point3(x_axis, transform_ab);
		CHECK(vector_all_near_equal3(result, vector_set(FloatType(1.2), FloatType(1.0), FloatType(-1.44)), threshold));
		CHECK(vector_all_near_equal3(result, qvv_mul_point3(qvv_mul_point3(x_axis, transform_a), transform_b), threshold));
		result = qvv_mul_point3(y_axis, transform_ab);
		CHECK(vector_all_near_equal3(result, vector_set(FloatType(-0.24), FloatType(1.0), FloatType(0.0)), threshold));
		CHECK(vector_all_near_equal3(result, qvv_mul_point3(qvv_mul_point3(y_axis, transform_a), transform_b), threshold));
		result = qvv_mul_point3(x_axis, transform_ba);
		CHECK(vector_all_near_equal3(result, vector_set(FloatType(-0.2), FloatType(1.44), FloatType(0.0)), threshold));
		CHECK(vector_all_near_equal3(result, qvv_mul_point3(qvv_mul_point3(x_axis, transform_b), transform_a), threshold));
		result = qvv_mul_point3(y_axis, transform_ba);
		CHECK(vector_all_near_equal3(result, vector_set(FloatType(-0.2), FloatType(0.0), FloatType(-1.44)), threshold));
		CHECK(vector_all_near_equal3(result, qvv_mul_point3(qvv_mul_point3(y_axis, transform_b), transform_a), threshold));
	}

	{
		Vector4Type x_axis = vector_set(FloatType(1.0), FloatType(0.0), FloatType(0.0));
		Vector4Type y_axis = vector_set(FloatType(0.0), FloatType(1.0), FloatType(0.0));

		QuatType rotation_around_z = quat_from_euler(scalar_deg_to_rad(FloatType(0.0)), scalar_deg_to_rad(FloatType(90.0)), scalar_deg_to_rad(FloatType(0.0)));
		TransformType transform_a = qvv_set(rotation_around_z, x_axis, vector_set(FloatType(1.0)));
		Vector4Type result = qvv_mul_point3_no_scale(x_axis, transform_a);
		CHECK(vector_all_near_equal3(result, vector_set(FloatType(1.0), FloatType(1.0), FloatType(0.0)), threshold));
		result = qvv_mul_point3_no_scale(y_axis, transform_a);
		CHECK(vector_all_near_equal3(result, vector_set(FloatType(0.0), FloatType(0.0), FloatType(0.0)), threshold));

		QuatType rotation_around_x = quat_from_euler(scalar_deg_to_rad(FloatType(0.0)), scalar_deg_to_rad(FloatType(0.0)), scalar_deg_to_rad(FloatType(90.0)));
		TransformType transform_b = qvv_set(rotation_around_x, y_axis, vector_set(FloatType(1.0)));
		result = qvv_mul_point3_no_scale(x_axis, transform_b);
		CHECK(vector_all_near_equal3(result, vector_set(FloatType(1.0), FloatType(1.0), FloatType(0.0)), threshold));
		result = qvv_mul_point3_no_scale(y_axis, transform_b);
		CHECK(vector_all_near_equal3(result, vector_set(FloatType(0.0), FloatType(1.0), FloatType(-1.0)), threshold));

		TransformType transform_ab = qvv_mul_no_scale(transform_a, transform_b);
		TransformType transform_ba = qvv_mul_no_scale(transform_b, transform_a);
		result = qvv_mul_point3_no_scale(x_axis, transform_ab);
		CHECK(vector_all_near_equal3(result, vector_set(FloatType(1.0), FloatType(1.0), FloatType(-1.0)), threshold));
		CHECK(vector_all_near_equal3(result, qvv_mul_point3_no_scale(qvv_mul_point3_no_scale(x_axis, transform_a), transform_b), threshold));
		result = qvv_mul_point3_no_scale(y_axis, transform_ab);
		CHECK(vector_all_near_equal3(result, vector_set(FloatType(0.0), FloatType(1.0), FloatType(0.0)), threshold));
		CHECK(vector_all_near_equal3(result, qvv_mul_point3_no_scale(qvv_mul_point3_no_scale(y_axis, transform_a), transform_b), threshold));
		result = qvv_mul_point3_no_scale(x_axis, transform_ba);
		CHECK(vector_all_near_equal3(result, vector_set(FloatType(0.0), FloatType(1.0), FloatType(0.0)), threshold));
		CHECK(vector_all_near_equal3(result, qvv_mul_point3_no_scale(qvv_mul_point3_no_scale(x_axis, transform_b), transform_a), threshold));
		result = qvv_mul_point3_no_scale(y_axis, transform_ba);
		CHECK(vector_all_near_equal3(result, vector_set(FloatType(0.0), FloatType(0.0), FloatType(-1.0)), threshold));
		CHECK(vector_all_near_equal3(result, qvv_mul_point3_no_scale(qvv_mul_point3_no_scale(y_axis, transform_b), transform_a), threshold));
	}

	{
		Vector4Type x_axis = vector_set(FloatType(1.0), FloatType(0.0), FloatType(0.0));
		Vector4Type test_scale1 = vector_set(FloatType(1.2));
		Vector4Type test_scale2 = vector_set(FloatType(-1.2));
		Vector4Type test_scale3 = vector_set(FloatType(1.2), FloatType(0.0), FloatType(-1.2));

		QuatType rotation_around_z = quat_from_euler(scalar_deg_to_rad(FloatType(0.0)), scalar_deg_to_rad(FloatType(90.0)), scalar_deg_to_rad(FloatType(0.0)));
		TransformType transform_a = qvv_set(rotation_around_z, x_axis, test_scale1);
		TransformType transform_b = qvv_inverse(transform_a);
		TransformType transform_ab = qvv_mul(transform_a, transform_b);
		CHECK(quat_near_equal(identity.rotation, transform_ab.rotation, threshold));
		CHECK(vector_all_near_equal3(identity.translation, transform_ab.translation, threshold));
		CHECK(vector_all_near_equal3(identity.scale, transform_ab.scale, threshold));

		transform_a = qvv_set(rotation_around_z, x_axis, test_scale2);
		transform_b = qvv_inverse(transform_a);
		transform_ab = qvv_mul(transform_a, transform_b);
		CHECK(quat_near_equal(identity.rotation, transform_ab.rotation, threshold));
		CHECK(vector_all_near_equal3(identity.translation, transform_ab.translation, threshold));
		CHECK(vector_all_near_equal3(identity.scale, transform_ab.scale, threshold));

		transform_a = qvv_set(rotation_around_z, x_axis, test_scale3);
		transform_b = qvv_inverse(transform_a, vector_set(FloatType(1.0)));
		CHECK(qvv_is_finite(transform_b));
	}

	{
		Vector4Type x_axis = vector_set(FloatType(1.0), FloatType(0.0), FloatType(0.0));

		QuatType rotation_around_z = quat_from_euler(scalar_deg_to_rad(FloatType(0.0)), scalar_deg_to_rad(FloatType(90.0)), scalar_deg_to_rad(FloatType(0.0)));
		TransformType transform_a = qvv_set(rotation_around_z, x_axis, vector_set(FloatType(1.0)));
		TransformType transform_b = qvv_inverse_no_scale(transform_a);
		TransformType transform_ab = qvv_mul_no_scale(transform_a, transform_b);
		CHECK(quat_near_equal(identity.rotation, transform_ab.rotation, threshold));
		CHECK(vector_all_near_equal3(identity.translation, transform_ab.translation, threshold));
		CHECK(vector_all_near_equal3(identity.scale, transform_ab.scale, threshold));
	}

	{
		Vector4Type x_axis = vector_set(FloatType(1.0), FloatType(0.0), FloatType(0.0));
		QuatType rotation_around_z = quat_from_euler(scalar_deg_to_rad(FloatType(0.0)), scalar_deg_to_rad(FloatType(90.0)), scalar_deg_to_rad(FloatType(0.0)));
		TransformType transform_a = qvv_set(rotation_around_z, x_axis, vector_set(FloatType(1.0)));
		CHECK(quat_is_normalized(qvv_normalize(transform_a).rotation, threshold));

		QuatType quat = quat_set(FloatType(-0.001138), FloatType(0.91623), FloatType(-1.624598), FloatType(0.715671));
		TransformType transform_b = qvv_set(quat, x_axis, vector_set(FloatType(1.0)));
		CHECK(!quat_is_normalized(transform_b.rotation, threshold));
		CHECK(quat_is_normalized(qvv_normalize(transform_b).rotation, threshold));
	}

	test_qvv_interpolation<TransformType>(threshold);

	{
		const FloatType inf = std::numeric_limits<FloatType>::infinity();
		const FloatType nan = std::numeric_limits<FloatType>::quiet_NaN();

		const QuatType quat0 = quat_normalize(quat_from_euler(scalar_deg_to_rad(FloatType(30.0)), scalar_deg_to_rad(FloatType(-45.0)), scalar_deg_to_rad(FloatType(90.0))));
		const QuatType quat_inf = quat_set(inf, inf, inf, inf);
		const QuatType quat_nan = quat_set(nan, nan, nan, nan);

		const Vector4Type trans0 = vector_set(FloatType(-0.001138), FloatType(0.91623), FloatType(-1.624598));
		const Vector4Type scale0 = vector_set(FloatType(-1.915), FloatType(0.23656), FloatType(-3.7811));
		const Vector4Type vec_inf = vector_set(inf, inf, inf, inf);
		const Vector4Type vec_nan = vector_set(nan, nan, nan, nan);

		CHECK(qvv_is_finite(identity) == true);
		CHECK(qvv_is_finite(qvv_set(quat0, trans0, scale0)) == true);
		CHECK(qvv_is_finite(qvv_set(quat_inf, trans0, scale0)) == false);
		CHECK(qvv_is_finite(qvv_set(quat_nan, trans0, scale0)) == false);
		CHECK(qvv_is_finite(qvv_set(quat0, vec_inf, scale0)) == false);
		CHECK(qvv_is_finite(qvv_set(quat0, vec_nan, scale0)) == false);
		CHECK(qvv_is_finite(qvv_set(quat0, trans0, vec_inf)) == false);
		CHECK(qvv_is_finite(qvv_set(quat0, trans0, vec_nan)) == false);
		CHECK(qvv_is_finite(qvv_set(quat_inf, vec_inf, vec_inf)) == false);
		CHECK(qvv_is_finite(qvv_set(quat_nan, vec_nan, vec_nan)) == false);
	}
}

TEST_CASE("qvvf math", "[math][qvv]")
{
	test_qvv_impl<qvvf, float>(qvv_identity(), 1.0E-4F);

	const quatf src_rotation = quat_set(0.39564531008956383F, 0.044254239301713752F, 0.22768840967675355F, 0.88863059760894492F);
	const vector4f src_translation = vector_set(-2.65F, 2.996113F, 0.68123521F);
	const vector4f src_scale = vector_set(1.2F, 0.8F, 2.1F);
	const qvvf src = qvv_set(src_rotation, src_translation, src_scale);
	const qvvd dst = qvv_cast(src);
	CHECK(quat_near_equal(src.rotation, quat_cast(dst.rotation), 1.0E-6F));
	CHECK(vector_all_near_equal3(src.translation, vector_cast(dst.translation), 1.0E-6F));
	CHECK(vector_all_near_equal3(src.scale, vector_cast(dst.scale), 1.0E-6F));
}

TEST_CASE("qvvd math", "[math][qvv]")
{
	test_qvv_impl<qvvd, double>(qvv_identity(), 1.0E-6);

	const quatd src_rotation = quat_set(0.39564531008956383, 0.044254239301713752, 0.22768840967675355, 0.88863059760894492);
	const vector4d src_translation = vector_set(-2.65, 2.996113, 0.68123521);
	const vector4d src_scale = vector_set(1.2, 0.8, 2.1);
	const qvvd src = qvv_set(src_rotation, src_translation, src_scale);
	const qvvf dst = qvv_cast(src);
	CHECK(quat_near_equal(src.rotation, quat_cast(dst.rotation), 1.0E-6));
	CHECK(vector_all_near_equal3(src.translation, vector_cast(dst.translation), 1.0E-6));
	CHECK(vector_all_near_equal3(src.scale, vector_cast(dst.scale), 1.0E-6));
}

```

`tests/sources/test_scalar.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2018 Nicholas Frechette & Animation Compression Library contributors
// Copyright (c) 2018 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include "catch2.impl.h"

#include <rtm/scalarf.h>
#include <rtm/scalard.h>
#include <rtm/type_traits.h>
#include <rtm/vector4f.h>
#include <rtm/vector4d.h>

#include <limits>

using namespace rtm;

template<typename FloatType>
static void test_scalar_impl(const FloatType threshold, const FloatType trig_threshold)
{
	const FloatType infinity = std::numeric_limits<FloatType>::infinity();
	const FloatType nan = std::numeric_limits<FloatType>::quiet_NaN();

	CHECK(scalar_cast(scalar_set(FloatType(0.2345))) == FloatType(0.2345));

	const FloatType value = FloatType(0.2345);
	CHECK(scalar_equal(scalar_load_as_scalar(&value), scalar_set(value)));
	CHECK(scalar_load(&value) == FloatType(0.2345));

	FloatType tmp = FloatType(0.0);
	scalar_store(FloatType(16.5), &tmp);
	CHECK(tmp == FloatType(16.5));
	scalar_store(scalar_set(FloatType(4.5)), &tmp);
	CHECK(tmp == FloatType(4.5));

	CHECK(scalar_floor(FloatType(0.0)) == FloatType(0.0));
	CHECK(scalar_floor(FloatType(-0.0)) == FloatType(0.0));
	CHECK(scalar_floor(FloatType(0.5)) == FloatType(0.0));
	CHECK(scalar_floor(FloatType(2.5)) == FloatType(2.0));
	CHECK(scalar_floor(FloatType(3.0)) == FloatType(3.0));
	CHECK(scalar_floor(FloatType(-0.5)) == FloatType(-1.0));
	CHECK(scalar_floor(FloatType(-2.5)) == FloatType(-3.0));
	CHECK(scalar_floor(FloatType(-3.0)) == FloatType(-3.0));
	CHECK(scalar_floor(FloatType(infinity)) == FloatType(infinity));
	CHECK(scalar_floor(FloatType(-infinity)) == FloatType(-infinity));
	CHECK(std::isnan(scalar_floor(FloatType(nan))));

	CHECK(scalar_cast(scalar_floor(scalar_set(FloatType(0.0)))) == FloatType(0.0));
	CHECK(scalar_cast(scalar_floor(scalar_set(FloatType(-0.0)))) == FloatType(0.0));
	CHECK(scalar_cast(scalar_floor(scalar_set(FloatType(0.5)))) == FloatType(0.0));
	CHECK(scalar_cast(scalar_floor(scalar_set(FloatType(2.5)))) == FloatType(2.0));
	CHECK(scalar_cast(scalar_floor(scalar_set(FloatType(3.0)))) == FloatType(3.0));
	CHECK(scalar_cast(scalar_floor(scalar_set(FloatType(-0.5)))) == FloatType(-1.0));
	CHECK(scalar_cast(scalar_floor(scalar_set(FloatType(-2.5)))) == FloatType(-3.0));
	CHECK(scalar_cast(scalar_floor(scalar_set(FloatType(-3.0)))) == FloatType(-3.0));
	CHECK(scalar_cast(scalar_floor(scalar_set(FloatType(infinity)))) == FloatType(infinity));
	CHECK(scalar_cast(scalar_floor(scalar_set(FloatType(-infinity)))) == FloatType(-infinity));
	CHECK(std::isnan(scalar_cast(scalar_floor(scalar_set(FloatType(nan))))));

	CHECK(scalar_ceil(FloatType(0.0)) == FloatType(0.0));
	CHECK(scalar_ceil(FloatType(-0.0)) == FloatType(0.0));
	CHECK(scalar_ceil(FloatType(0.5)) == FloatType(1.0));
	CHECK(scalar_ceil(FloatType(2.5)) == FloatType(3.0));
	CHECK(scalar_ceil(FloatType(3.0)) == FloatType(3.0));
	CHECK(scalar_ceil(FloatType(-0.5)) == FloatType(0.0));
	CHECK(scalar_ceil(FloatType(-2.5)) == FloatType(-2.0));
	CHECK(scalar_ceil(FloatType(-3.0)) == FloatType(-3.0));
	CHECK(scalar_ceil(FloatType(infinity)) == FloatType(infinity));
	CHECK(scalar_ceil(FloatType(-infinity)) == FloatType(-infinity));
	CHECK(std::isnan(scalar_ceil(FloatType(nan))));

	CHECK(scalar_cast(scalar_ceil(scalar_set(FloatType(0.0)))) == FloatType(0.0));
	CHECK(scalar_cast(scalar_ceil(scalar_set(FloatType(-0.0)))) == FloatType(0.0));
	CHECK(scalar_cast(scalar_ceil(scalar_set(FloatType(0.5)))) == FloatType(1.0));
	CHECK(scalar_cast(scalar_ceil(scalar_set(FloatType(2.5)))) == FloatType(3.0));
	CHECK(scalar_cast(scalar_ceil(scalar_set(FloatType(3.0)))) == FloatType(3.0));
	CHECK(scalar_cast(scalar_ceil(scalar_set(FloatType(-0.5)))) == FloatType(0.0));
	CHECK(scalar_cast(scalar_ceil(scalar_set(FloatType(-2.5)))) == FloatType(-2.0));
	CHECK(scalar_cast(scalar_ceil(scalar_set(FloatType(-3.0)))) == FloatType(-3.0));
	CHECK(scalar_cast(scalar_ceil(scalar_set(FloatType(infinity)))) == FloatType(infinity));
	CHECK(scalar_cast(scalar_ceil(scalar_set(FloatType(-infinity)))) == FloatType(-infinity));
	CHECK(std::isnan(scalar_cast(scalar_ceil(scalar_set(FloatType(nan))))));

	CHECK(scalar_clamp(FloatType(0.5), FloatType(0.0), FloatType(1.0)) == FloatType(0.5));
	CHECK(scalar_clamp(FloatType(-0.5), FloatType(0.0), FloatType(1.0)) == FloatType(0.0));
	CHECK(scalar_clamp(FloatType(1.5), FloatType(0.0), FloatType(1.0)) == FloatType(1.0));

	CHECK(scalar_cast(scalar_clamp(scalar_set(FloatType(0.5)), scalar_set(FloatType(0.0)), scalar_set(FloatType(1.0)))) == FloatType(0.5));
	CHECK(scalar_cast(scalar_clamp(scalar_set(FloatType(-0.5)), scalar_set(FloatType(0.0)), scalar_set(FloatType(1.0)))) == FloatType(0.0));
	CHECK(scalar_cast(scalar_clamp(scalar_set(FloatType(1.5)), scalar_set(FloatType(0.0)), scalar_set(FloatType(1.0)))) == FloatType(1.0));

	CHECK(scalar_abs(FloatType(0.0)) == FloatType(0.0));
	CHECK(scalar_abs(FloatType(2.0)) == FloatType(2.0));
	CHECK(scalar_abs(FloatType(-2.0)) == FloatType(2.0));

	CHECK(scalar_equal(scalar_abs(scalar_set(FloatType(0.0))), scalar_set(FloatType(0.0))));
	CHECK(scalar_equal(scalar_abs(scalar_set(FloatType(2.0))), scalar_set(FloatType(2.0))));
	CHECK(scalar_equal(scalar_abs(scalar_set(FloatType(-2.0))), scalar_set(FloatType(2.0))));

	CHECK(scalar_equal(FloatType(1.123), FloatType(1.123)) == true);
	CHECK(scalar_equal(FloatType(1.123), FloatType(1.124)) == false);
	CHECK(scalar_equal(scalar_set(FloatType(1.123)), scalar_set(FloatType(1.123))) == true);
	CHECK(scalar_equal(scalar_set(FloatType(1.123)), scalar_set(FloatType(1.124))) == false);

	CHECK(scalar_near_equal(FloatType(1.0), FloatType(1.0), FloatType(0.00001)) == true);
	CHECK(scalar_near_equal(FloatType(1.0), FloatType(1.000001), FloatType(0.00001)) == true);
	CHECK(scalar_near_equal(FloatType(1.0), FloatType(0.999999), FloatType(0.00001)) == true);
	CHECK(scalar_near_equal(FloatType(1.0), FloatType(1.001), FloatType(0.00001)) == false);
	CHECK(scalar_near_equal(FloatType(1.0), FloatType(0.999), FloatType(0.00001)) == false);

	CHECK(scalar_sqrt(FloatType(0.0)) == FloatType(0.0));
	CHECK(scalar_near_equal(scalar_sqrt(FloatType(0.5)), std::sqrt(FloatType(0.5)), threshold));
	CHECK(scalar_near_equal(scalar_sqrt(FloatType(32.5)), std::sqrt(FloatType(32.5)), threshold));
	CHECK(scalar_equal(scalar_sqrt(FloatType(32.5)), scalar_cast(scalar_sqrt(scalar_set(FloatType(32.5))))));

	CHECK(scalar_lower_than(FloatType(1.123), FloatType(1.123)) == false);
	CHECK(scalar_lower_than(FloatType(1.123), FloatType(1.124)) == true);
	CHECK(scalar_lower_than(FloatType(1.124), FloatType(1.123)) == false);
	CHECK(scalar_lower_than(scalar_set(FloatType(1.123)), scalar_set(FloatType(1.123))) == false);
	CHECK(scalar_lower_than(scalar_set(FloatType(1.123)), scalar_set(FloatType(1.124))) == true);
	CHECK(scalar_lower_than(scalar_set(FloatType(1.124)), scalar_set(FloatType(1.123))) == false);

	CHECK(scalar_lower_equal(FloatType(1.123), FloatType(1.123)) == true);
	CHECK(scalar_lower_equal(FloatType(1.123), FloatType(1.124)) == true);
	CHECK(scalar_lower_equal(FloatType(1.124), FloatType(1.123)) == false);
	CHECK(scalar_lower_equal(scalar_set(FloatType(1.123)), scalar_set(FloatType(1.123))) == true);
	CHECK(scalar_lower_equal(scalar_set(FloatType(1.123)), scalar_set(FloatType(1.124))) == true);
	CHECK(scalar_lower_equal(scalar_set(FloatType(1.124)), scalar_set(FloatType(1.123))) == false);

	CHECK(scalar_greater_than(FloatType(1.123), FloatType(1.123)) == false);
	CHECK(scalar_greater_than(FloatType(1.123), FloatType(1.124)) == false);
	CHECK(scalar_greater_than(FloatType(1.124), FloatType(1.123)) == true);
	CHECK(scalar_greater_than(scalar_set(FloatType(1.123)), scalar_set(FloatType(1.123))) == false);
	CHECK(scalar_greater_than(scalar_set(FloatType(1.123)), scalar_set(FloatType(1.124))) == false);
	CHECK(scalar_greater_than(scalar_set(FloatType(1.124)), scalar_set(FloatType(1.123))) == true);

	CHECK(scalar_greater_equal(FloatType(1.123), FloatType(1.123)) == true);
	CHECK(scalar_greater_equal(FloatType(1.123), FloatType(1.124)) == false);
	CHECK(scalar_greater_equal(FloatType(1.124), FloatType(1.123)) == true);
	CHECK(scalar_greater_equal(scalar_set(FloatType(1.123)), scalar_set(FloatType(1.123))) == true);
	CHECK(scalar_greater_equal(scalar_set(FloatType(1.123)), scalar_set(FloatType(1.124))) == false);
	CHECK(scalar_greater_equal(scalar_set(FloatType(1.124)), scalar_set(FloatType(1.123))) == true);

	CHECK(scalar_near_equal(scalar_sqrt_reciprocal(FloatType(0.5)), FloatType(1.0) / std::sqrt(FloatType(0.5)), threshold));
	CHECK(scalar_near_equal(scalar_sqrt_reciprocal(FloatType(32.5)), FloatType(1.0) / std::sqrt(FloatType(32.5)), threshold));

	CHECK(scalar_near_equal(scalar_cast(scalar_sqrt_reciprocal(scalar_set(FloatType(0.5)))), FloatType(1.0) / std::sqrt(FloatType(0.5)), threshold));
	CHECK(scalar_near_equal(scalar_cast(scalar_sqrt_reciprocal(scalar_set(FloatType(32.5)))), FloatType(1.0) / std::sqrt(FloatType(32.5)), threshold));

	CHECK(scalar_near_equal(scalar_reciprocal(FloatType(0.5)), FloatType(1.0 / 0.5), threshold));
	CHECK(scalar_near_equal(scalar_reciprocal(FloatType(32.5)), FloatType(1.0 / 32.5), threshold));
	CHECK(scalar_near_equal(scalar_reciprocal(FloatType(-0.5)), FloatType(1.0 / -0.5), threshold));
	CHECK(scalar_near_equal(scalar_reciprocal(FloatType(-32.5)), FloatType(1.0 / -32.5), threshold));

	CHECK(scalar_near_equal(scalar_cast(scalar_reciprocal(scalar_set(FloatType(0.5)))), FloatType(1.0 / 0.5), threshold));
	CHECK(scalar_near_equal(scalar_cast(scalar_reciprocal(scalar_set(FloatType(32.5)))), FloatType(1.0 / 32.5), threshold));
	CHECK(scalar_near_equal(scalar_cast(scalar_reciprocal(scalar_set(FloatType(-0.5)))), FloatType(1.0 / -0.5), threshold));
	CHECK(scalar_near_equal(scalar_cast(scalar_reciprocal(scalar_set(FloatType(-32.5)))), FloatType(1.0 / -32.5), threshold));

	CHECK(scalar_near_equal(scalar_add(FloatType(-0.5), FloatType(1.0)), FloatType(-0.5) + FloatType(1.0), threshold));
	CHECK(scalar_near_equal(scalar_add(FloatType(1.0), FloatType(-0.5)), FloatType(1.0) + FloatType(-0.5), threshold));
	CHECK(scalar_near_equal(scalar_add(FloatType(1.0), FloatType(1.0)), FloatType(1.0) + FloatType(1.0), threshold));

	CHECK(scalar_near_equal(scalar_cast(scalar_add(scalar_set(FloatType(-0.5)), scalar_set(FloatType(1.0)))), FloatType(-0.5) + FloatType(1.0), threshold));
	CHECK(scalar_near_equal(scalar_cast(scalar_add(scalar_set(FloatType(1.0)), scalar_set(FloatType(-0.5)))), FloatType(1.0) + FloatType(-0.5), threshold));
	CHECK(scalar_near_equal(scalar_cast(scalar_add(scalar_set(FloatType(1.0)), scalar_set(FloatType(1.0)))), FloatType(1.0) + FloatType(1.0), threshold));

	CHECK(scalar_near_equal(scalar_sub(FloatType(-0.5), FloatType(1.0)), FloatType(-0.5) - FloatType(1.0), threshold));
	CHECK(scalar_near_equal(scalar_sub(FloatType(1.0), FloatType(-0.5)), FloatType(1.0) - FloatType(-0.5), threshold));
	CHECK(scalar_near_equal(scalar_sub(FloatType(1.0), FloatType(1.0)), FloatType(1.0) - FloatType(1.0), threshold));

	CHECK(scalar_near_equal(scalar_cast(scalar_sub(scalar_set(FloatType(-0.5)), scalar_set(FloatType(1.0)))), FloatType(-0.5) - FloatType(1.0), threshold));
	CHECK(scalar_near_equal(scalar_cast(scalar_sub(scalar_set(FloatType(1.0)), scalar_set(FloatType(-0.5)))), FloatType(1.0) - FloatType(-0.5), threshold));
	CHECK(scalar_near_equal(scalar_cast(scalar_sub(scalar_set(FloatType(1.0)), scalar_set(FloatType(1.0)))), FloatType(1.0) - FloatType(1.0), threshold));

	CHECK(scalar_near_equal(scalar_mul(FloatType(-0.5), FloatType(1.0)), FloatType(-0.5) * FloatType(1.0), threshold));
	CHECK(scalar_near_equal(scalar_mul(FloatType(1.0), FloatType(-0.5)), FloatType(1.0) * FloatType(-0.5), threshold));
	CHECK(scalar_near_equal(scalar_mul(FloatType(1.0), FloatType(1.0)), FloatType(1.0) * FloatType(1.0), threshold));

	CHECK(scalar_near_equal(scalar_cast(scalar_mul(scalar_set(FloatType(-0.5)), scalar_set(FloatType(1.0)))), FloatType(-0.5) * FloatType(1.0), threshold));
	CHECK(scalar_near_equal(scalar_cast(scalar_mul(scalar_set(FloatType(1.0)), scalar_set(FloatType(-0.5)))), FloatType(1.0) * FloatType(-0.5), threshold));
	CHECK(scalar_near_equal(scalar_cast(scalar_mul(scalar_set(FloatType(1.0)), scalar_set(FloatType(1.0)))), FloatType(1.0) * FloatType(1.0), threshold));

	CHECK(scalar_near_equal(scalar_div(FloatType(-0.5), FloatType(1.0)), FloatType(-0.5) / FloatType(1.0), threshold));
	CHECK(scalar_near_equal(scalar_div(FloatType(1.0), FloatType(-0.5)), FloatType(1.0) / FloatType(-0.5), threshold));
	CHECK(scalar_near_equal(scalar_div(FloatType(1.0), FloatType(1.0)), FloatType(1.0) / FloatType(1.0), threshold));

	CHECK(scalar_near_equal(scalar_cast(scalar_div(scalar_set(FloatType(-0.5)), scalar_set(FloatType(1.0)))), FloatType(-0.5) / FloatType(1.0), threshold));
	CHECK(scalar_near_equal(scalar_cast(scalar_div(scalar_set(FloatType(1.0)), scalar_set(FloatType(-0.5)))), FloatType(1.0) / FloatType(-0.5), threshold));
	CHECK(scalar_near_equal(scalar_cast(scalar_div(scalar_set(FloatType(1.0)), scalar_set(FloatType(1.0)))), FloatType(1.0) / FloatType(1.0), threshold));

	const FloatType values[] = { FloatType(-1.0 / 3.0), FloatType(0.0341), FloatType(-0.54132) };
	CHECK(scalar_near_equal(scalar_mul_add(values[0], values[1], values[2]), (values[0] * values[1]) + values[2], threshold));
	CHECK(scalar_near_equal(scalar_cast(scalar_mul_add(scalar_set(values[0]), scalar_set(values[1]), scalar_set(values[2]))), (values[0] * values[1]) + values[2], threshold));

	CHECK(scalar_near_equal(scalar_neg_mul_sub(values[0], values[1], values[2]), values[2] - (values[0] * values[1]), threshold));
	CHECK(scalar_near_equal(scalar_cast(scalar_neg_mul_sub(scalar_set(values[0]), scalar_set(values[1]), scalar_set(values[2]))), values[2] - (values[0] * values[1]), threshold));

	CHECK(scalar_near_equal(scalar_lerp(values[0], values[1], values[2]), ((values[1] - values[0]) * values[2]) + values[0], threshold));
	CHECK(scalar_near_equal(scalar_lerp(scalar_set(values[0]), scalar_set(values[1]), scalar_set(values[2])), scalar_set(((values[1] - values[0]) * values[2]) + values[0]), scalar_set(threshold)));

	// Lerp must be stable and return exactly the start when the interpolation alpha is 0.0 and exactly the end when 1.0
	CHECK(scalar_near_equal(scalar_lerp(values[0], values[1], FloatType(0.0)), values[0], FloatType(0.0)));
	CHECK(scalar_near_equal(scalar_lerp(values[0], values[1], FloatType(1.0)), values[1], FloatType(0.0)));
	CHECK(scalar_near_equal(scalar_lerp(scalar_set(values[0]), scalar_set(values[1]), scalar_set(FloatType(0.0))), scalar_set(values[0]), scalar_set(FloatType(0.0))));
	CHECK(scalar_near_equal(scalar_lerp(scalar_set(values[0]), scalar_set(values[1]), scalar_set(FloatType(1.0))), scalar_set(values[1]), scalar_set(FloatType(0.0))));

	using Vector4Type = typename related_types<FloatType>::vector4;

	const FloatType half_pi = FloatType(rtm::constants::half_pi());
	const FloatType pi = FloatType(rtm::constants::pi());

	const FloatType angles[] =
	{
		FloatType(0.0), FloatType(-0.0),
		pi, -pi,
		half_pi, -half_pi,
		half_pi * FloatType(0.5), -half_pi * FloatType(0.5),
		half_pi * FloatType(0.25), -half_pi * FloatType(0.25),
		FloatType(0.5), FloatType(-0.5),
		FloatType(32.5), FloatType(-32.5),
	};

	for (const FloatType angle : angles)
	{
		INFO("The angle is " << angle);

		{
			const FloatType ref_sin = std::sin(angle);
			const FloatType ref_cos = std::cos(angle);

			INFO("The reference sin(angle) is " << ref_sin);
			INFO("The reference cos(angle) is " << ref_cos);

			CHECK(scalar_near_equal(scalar_sin(angle), ref_sin, trig_threshold));
			CHECK(scalar_near_equal(scalar_cast(scalar_sin(scalar_set(angle))), ref_sin, trig_threshold));
			CHECK(scalar_near_equal(scalar_cos(angle), ref_cos, trig_threshold));
			CHECK(scalar_near_equal(scalar_cast(scalar_cos(scalar_set(angle))), ref_cos, trig_threshold));

			FloatType sin_result;
			FloatType cos_result;
			scalar_sincos(angle, sin_result, cos_result);
			CHECK(scalar_near_equal(sin_result, ref_sin, trig_threshold));
			CHECK(scalar_near_equal(cos_result, ref_cos, trig_threshold));

			Vector4Type sincos0 = scalar_sincos(angle);
			CHECK(scalar_near_equal(vector_get_x(sincos0), ref_sin, trig_threshold));
			CHECK(scalar_near_equal(vector_get_y(sincos0), ref_cos, trig_threshold));

			Vector4Type sincos1 = scalar_sincos(scalar_set(angle));
			CHECK(scalar_near_equal(vector_get_x(sincos1), ref_sin, trig_threshold));
			CHECK(scalar_near_equal(vector_get_y(sincos1), ref_cos, trig_threshold));

			CHECK(scalar_near_equal(scalar_asin(ref_sin), std::asin(ref_sin), trig_threshold));
			CHECK(scalar_near_equal(scalar_cast(scalar_asin(scalar_set(ref_sin))), std::asin(ref_sin), trig_threshold));
			CHECK(scalar_near_equal(scalar_acos(ref_cos), std::acos(ref_cos), trig_threshold));
			CHECK(scalar_near_equal(scalar_cast(scalar_acos(scalar_set(ref_cos))), std::acos(ref_cos), trig_threshold));
		}

		{
			const FloatType ref_tan = std::tan(angle);
			const FloatType rtm_tan = scalar_tan(angle);

			INFO("The reference tan(angle) is " << ref_tan);
			INFO("The RTM tan(angle) is " << rtm_tan);

			// For +-PI/2, we only test that the value is really large or really small
			if (scalar_abs(angle) == FloatType(half_pi))
			{
				CHECK(scalar_greater_than(scalar_abs(rtm_tan), FloatType(1.0e6)));
				CHECK(scalar_greater_than(scalar_cast(scalar_abs(scalar_tan(scalar_set(angle)))), FloatType(1.0e6)));
			}
			else
			{
				CHECK(scalar_near_equal(rtm_tan, ref_tan, trig_threshold));
				CHECK(scalar_near_equal(scalar_cast(scalar_tan(scalar_set(angle))), ref_tan, trig_threshold));
			}
		}
	}

	const FloatType angles_acos[] = { FloatType(-1.0), FloatType(-0.75), FloatType(-0.5), FloatType(-0.25), FloatType(0.0), FloatType(0.25), FloatType(0.5), FloatType(0.75), FloatType(1.0) };
	for (const FloatType angle : angles_acos)
	{
		CHECK(scalar_near_equal(scalar_acos(angle), std::acos(angle), trig_threshold));
	}

	const FloatType angles_atan[] = { FloatType(-10.0), FloatType(-5.0), FloatType(-0.5), FloatType(-0.25), FloatType(0.0), FloatType(0.25), FloatType(0.5), FloatType(0.75), FloatType(81.0) };
	for (const FloatType angle : angles_atan)
	{
		CHECK(scalar_near_equal(scalar_atan(angle), std::atan(angle), trig_threshold));
		CHECK(scalar_near_equal(scalar_cast(scalar_atan(scalar_set(angle))), std::atan(angle), trig_threshold));
	}

	CHECK(scalar_near_equal(scalar_atan2(FloatType(-2.0), FloatType(-2.0)), std::atan2(FloatType(-2.0), FloatType(-2.0)), trig_threshold));
	CHECK(scalar_near_equal(scalar_atan2(FloatType(-1.0), FloatType(-2.0)), std::atan2(FloatType(-1.0), FloatType(-2.0)), trig_threshold));
	CHECK(scalar_near_equal(scalar_atan2(FloatType(-2.0), FloatType(-1.0)), std::atan2(FloatType(-2.0), FloatType(-1.0)), trig_threshold));
	CHECK(scalar_near_equal(scalar_atan2(FloatType(2.0), FloatType(2.0)), std::atan2(FloatType(2.0), FloatType(2.0)), trig_threshold));
	CHECK(scalar_near_equal(scalar_atan2(FloatType(1.0), FloatType(2.0)), std::atan2(FloatType(1.0), FloatType(2.0)), trig_threshold));
	CHECK(scalar_near_equal(scalar_atan2(FloatType(2.0), FloatType(1.0)), std::atan2(FloatType(2.0), FloatType(1.0)), trig_threshold));
	CHECK(scalar_near_equal(scalar_atan2(FloatType(2.0), FloatType(0.0)), std::atan2(FloatType(2.0), FloatType(0.0)), trig_threshold));
	CHECK(scalar_near_equal(scalar_atan2(FloatType(-2.0), FloatType(0.0)), std::atan2(FloatType(-2.0), FloatType(0.0)), trig_threshold));
	CHECK(scalar_near_equal(scalar_atan2(FloatType(2.0), FloatType(-0.0)), std::atan2(FloatType(2.0), FloatType(-0.0)), trig_threshold));
	CHECK(scalar_near_equal(scalar_atan2(FloatType(-2.0), FloatType(-0.0)), std::atan2(FloatType(-2.0), FloatType(-0.0)), trig_threshold));
	CHECK(scalar_near_equal(scalar_atan2(FloatType(0.0), FloatType(2.0)), std::atan2(FloatType(0.0), FloatType(2.0)), trig_threshold));
	CHECK(scalar_near_equal(scalar_atan2(FloatType(0.0), FloatType(-2.0)), std::atan2(FloatType(0.0), FloatType(-2.0)), trig_threshold));
	CHECK(scalar_near_equal(scalar_atan2(FloatType(-0.0), FloatType(2.0)), std::atan2(FloatType(-0.0), FloatType(2.0)), trig_threshold));
	CHECK(scalar_near_equal(scalar_atan2(FloatType(-0.0), FloatType(-2.0)), std::atan2(FloatType(-0.0), FloatType(-2.0)), trig_threshold));
	CHECK(scalar_atan2(FloatType(0.0), FloatType(0.0)) == FloatType(0.0));

	CHECK(scalar_near_equal(scalar_cast(scalar_atan2(scalar_set(FloatType(-2.0)), scalar_set(FloatType(-2.0)))), std::atan2(FloatType(-2.0), FloatType(-2.0)), trig_threshold));
	CHECK(scalar_near_equal(scalar_cast(scalar_atan2(scalar_set(FloatType(-1.0)), scalar_set(FloatType(-2.0)))), std::atan2(FloatType(-1.0), FloatType(-2.0)), trig_threshold));
	CHECK(scalar_near_equal(scalar_cast(scalar_atan2(scalar_set(FloatType(-2.0)), scalar_set(FloatType(-1.0)))), std::atan2(FloatType(-2.0), FloatType(-1.0)), trig_threshold));
	CHECK(scalar_near_equal(scalar_cast(scalar_atan2(scalar_set(FloatType(2.0)), scalar_set(FloatType(2.0)))), std::atan2(FloatType(2.0), FloatType(2.0)), trig_threshold));
	CHECK(scalar_near_equal(scalar_cast(scalar_atan2(scalar_set(FloatType(1.0)), scalar_set(FloatType(2.0)))), std::atan2(FloatType(1.0), FloatType(2.0)), trig_threshold));
	CHECK(scalar_near_equal(scalar_cast(scalar_atan2(scalar_set(FloatType(2.0)), scalar_set(FloatType(1.0)))), std::atan2(FloatType(2.0), FloatType(1.0)), trig_threshold));
	CHECK(scalar_near_equal(scalar_cast(scalar_atan2(scalar_set(FloatType(2.0)), scalar_set(FloatType(0.0)))), std::atan2(FloatType(2.0), FloatType(0.0)), trig_threshold));
	CHECK(scalar_near_equal(scalar_cast(scalar_atan2(scalar_set(FloatType(-2.0)), scalar_set(FloatType(0.0)))), std::atan2(FloatType(-2.0), FloatType(0.0)), trig_threshold));
	CHECK(scalar_near_equal(scalar_cast(scalar_atan2(scalar_set(FloatType(2.0)), scalar_set(FloatType(-0.0)))), std::atan2(FloatType(2.0), FloatType(-0.0)), trig_threshold));
	CHECK(scalar_near_equal(scalar_cast(scalar_atan2(scalar_set(FloatType(-2.0)), scalar_set(FloatType(-0.0)))), std::atan2(FloatType(-2.0), FloatType(-0.0)), trig_threshold));
	CHECK(scalar_near_equal(scalar_cast(scalar_atan2(scalar_set(FloatType(0.0)), scalar_set(FloatType(2.0)))), std::atan2(FloatType(0.0), FloatType(2.0)), trig_threshold));
	CHECK(scalar_near_equal(scalar_cast(scalar_atan2(scalar_set(FloatType(0.0)), scalar_set(FloatType(-2.0)))), std::atan2(FloatType(0.0), FloatType(-2.0)), trig_threshold));
	CHECK(scalar_near_equal(scalar_cast(scalar_atan2(scalar_set(FloatType(-0.0)), scalar_set(FloatType(2.0)))), std::atan2(FloatType(-0.0), FloatType(2.0)), trig_threshold));
	CHECK(scalar_near_equal(scalar_cast(scalar_atan2(scalar_set(FloatType(-0.0)), scalar_set(FloatType(-2.0)))), std::atan2(FloatType(-0.0), FloatType(-2.0)), trig_threshold));
	CHECK(scalar_cast(scalar_atan2(scalar_set(FloatType(0.0)), scalar_set(FloatType(0.0)))) == FloatType(0.0));

	CHECK(scalar_min(FloatType(-0.5), FloatType(1.0)) == FloatType(-0.5));
	CHECK(scalar_min(FloatType(1.0), FloatType(-0.5)) == FloatType(-0.5));
	CHECK(scalar_min(FloatType(1.0), FloatType(1.0)) == FloatType(1.0));

	CHECK(scalar_equal(scalar_min(scalar_set(FloatType(-0.5)), scalar_set(FloatType(1.0))), scalar_set(FloatType(-0.5))));
	CHECK(scalar_equal(scalar_min(scalar_set(FloatType(1.0)), scalar_set(FloatType(-0.5))), scalar_set(FloatType(-0.5))));
	CHECK(scalar_equal(scalar_min(scalar_set(FloatType(1.0)), scalar_set(FloatType(1.0))), scalar_set(FloatType(1.0))));

	CHECK(scalar_max(FloatType(-0.5), FloatType(1.0)) == FloatType(1.0));
	CHECK(scalar_max(FloatType(1.0), FloatType(-0.5)) == FloatType(1.0));
	CHECK(scalar_max(FloatType(1.0), FloatType(1.0)) == FloatType(1.0));

	CHECK(scalar_equal(scalar_max(scalar_set(FloatType(-0.5)), scalar_set(FloatType(1.0))), scalar_set(FloatType(1.0))));
	CHECK(scalar_equal(scalar_max(scalar_set(FloatType(1.0)), scalar_set(FloatType(-0.5))), scalar_set(FloatType(1.0))));
	CHECK(scalar_equal(scalar_max(scalar_set(FloatType(1.0)), scalar_set(FloatType(1.0))), scalar_set(FloatType(1.0))));

	CHECK(scalar_is_finite(FloatType(0.0)) == true);
	CHECK(scalar_is_finite(FloatType(32.0)) == true);
	CHECK(scalar_is_finite(FloatType(-32.0)) == true);
	CHECK(scalar_is_finite(std::numeric_limits<FloatType>::infinity()) == false);
	CHECK(scalar_is_finite(std::numeric_limits<FloatType>::quiet_NaN()) == false);
	CHECK(scalar_is_finite(std::numeric_limits<FloatType>::signaling_NaN()) == false);

	CHECK(scalar_is_finite(scalar_set(FloatType(0.0))) == true);
	CHECK(scalar_is_finite(scalar_set(FloatType(32.0))) == true);
	CHECK(scalar_is_finite(scalar_set(FloatType(-32.0))) == true);
	CHECK(scalar_is_finite(scalar_set(std::numeric_limits<FloatType>::infinity())) == false);
	CHECK(scalar_is_finite(scalar_set(std::numeric_limits<FloatType>::quiet_NaN())) == false);
	CHECK(scalar_is_finite(scalar_set(std::numeric_limits<FloatType>::signaling_NaN())) == false);

	CHECK(scalar_round_symmetric(FloatType(-1.75)) == FloatType(-2.0));
	CHECK(scalar_round_symmetric(FloatType(-1.5)) == FloatType(-2.0));
	CHECK(scalar_round_symmetric(FloatType(-1.4999)) == FloatType(-1.0));
	CHECK(scalar_round_symmetric(FloatType(-0.5)) == FloatType(-1.0));
	CHECK(scalar_round_symmetric(FloatType(-0.4999)) == FloatType(0.0));
	CHECK(scalar_round_symmetric(FloatType(0.0)) == FloatType(0.0));
	CHECK(scalar_round_symmetric(FloatType(-0.0)) == FloatType(0.0));
	CHECK(scalar_round_symmetric(FloatType(0.4999)) == FloatType(0.0));
	CHECK(scalar_round_symmetric(FloatType(0.5)) == FloatType(1.0));
	CHECK(scalar_round_symmetric(FloatType(1.4999)) == FloatType(1.0));
	CHECK(scalar_round_symmetric(FloatType(1.5)) == FloatType(2.0));
	CHECK(scalar_round_symmetric(FloatType(1.75)) == FloatType(2.0));
	CHECK(scalar_round_symmetric(infinity) == FloatType(infinity));
	CHECK(scalar_round_symmetric(-infinity) == FloatType(-infinity));
	CHECK(std::isnan(scalar_round_symmetric(nan)));

	CHECK(scalar_cast(scalar_round_symmetric(scalar_set(FloatType(-1.75)))) == FloatType(-2.0));
	CHECK(scalar_cast(scalar_round_symmetric(scalar_set(FloatType(-1.5)))) == FloatType(-2.0));
	CHECK(scalar_cast(scalar_round_symmetric(scalar_set(FloatType(-1.4999)))) == FloatType(-1.0));
	CHECK(scalar_cast(scalar_round_symmetric(scalar_set(FloatType(-0.5)))) == FloatType(-1.0));
	CHECK(scalar_cast(scalar_round_symmetric(scalar_set(FloatType(-0.4999)))) == FloatType(0.0));
	CHECK(scalar_cast(scalar_round_symmetric(scalar_set(FloatType(0.0)))) == FloatType(0.0));
	CHECK(scalar_cast(scalar_round_symmetric(scalar_set(FloatType(-0.0)))) == FloatType(0.0));
	CHECK(scalar_cast(scalar_round_symmetric(scalar_set(FloatType(0.4999)))) == FloatType(0.0));
	CHECK(scalar_cast(scalar_round_symmetric(scalar_set(FloatType(0.5)))) == FloatType(1.0));
	CHECK(scalar_cast(scalar_round_symmetric(scalar_set(FloatType(1.4999)))) == FloatType(1.0));
	CHECK(scalar_cast(scalar_round_symmetric(scalar_set(FloatType(1.5)))) == FloatType(2.0));
	CHECK(scalar_cast(scalar_round_symmetric(scalar_set(FloatType(1.75)))) == FloatType(2.0));
	CHECK(scalar_cast(scalar_round_symmetric(scalar_set(infinity))) == infinity);
	CHECK(scalar_cast(scalar_round_symmetric(scalar_set(-infinity))) == -infinity);
	CHECK(std::isnan(scalar_cast(scalar_round_symmetric(scalar_set(nan)))));

	CHECK(scalar_round_bankers(FloatType(-2.5)) == FloatType(-2.0));
	CHECK(scalar_round_bankers(FloatType(-1.75)) == FloatType(-2.0));
	CHECK(scalar_round_bankers(FloatType(-1.5)) == FloatType(-2.0));
	CHECK(scalar_round_bankers(FloatType(-1.4999)) == FloatType(-1.0));
	CHECK(scalar_round_bankers(FloatType(-0.5)) == FloatType(0.0));
	CHECK(scalar_round_bankers(FloatType(-0.4999)) == FloatType(0.0));
	CHECK(scalar_round_bankers(FloatType(0.0)) == FloatType(0.0));
	CHECK(scalar_round_bankers(FloatType(-0.0)) == FloatType(0.0));
	CHECK(scalar_round_bankers(FloatType(0.4999)) == FloatType(0.0));
	CHECK(scalar_round_bankers(FloatType(0.5)) == FloatType(0.0));
	CHECK(scalar_round_bankers(FloatType(1.4999)) == FloatType(1.0));
	CHECK(scalar_round_bankers(FloatType(1.5)) == FloatType(2.0));
	CHECK(scalar_round_bankers(FloatType(1.75)) == FloatType(2.0));
	CHECK(scalar_round_bankers(FloatType(2.5)) == FloatType(2.0));

	CHECK(scalar_cast(scalar_round_bankers(scalar_set(FloatType(-2.5)))) == FloatType(-2.0));
	CHECK(scalar_cast(scalar_round_bankers(scalar_set(FloatType(-1.75)))) == FloatType(-2.0));
	CHECK(scalar_cast(scalar_round_bankers(scalar_set(FloatType(-1.5)))) == FloatType(-2.0));
	CHECK(scalar_cast(scalar_round_bankers(scalar_set(FloatType(-1.4999)))) == FloatType(-1.0));
	CHECK(scalar_cast(scalar_round_bankers(scalar_set(FloatType(-0.5)))) == FloatType(0.0));
	CHECK(scalar_cast(scalar_round_bankers(scalar_set(FloatType(-0.4999)))) == FloatType(0.0));
	CHECK(scalar_cast(scalar_round_bankers(scalar_set(FloatType(0.0)))) == FloatType(0.0));
	CHECK(scalar_cast(scalar_round_bankers(scalar_set(FloatType(-0.0)))) == FloatType(0.0));
	CHECK(scalar_cast(scalar_round_bankers(scalar_set(FloatType(0.4999)))) == FloatType(0.0));
	CHECK(scalar_cast(scalar_round_bankers(scalar_set(FloatType(0.5)))) == FloatType(0.0));
	CHECK(scalar_cast(scalar_round_bankers(scalar_set(FloatType(1.4999)))) == FloatType(1.0));
	CHECK(scalar_cast(scalar_round_bankers(scalar_set(FloatType(1.5)))) == FloatType(2.0));
	CHECK(scalar_cast(scalar_round_bankers(scalar_set(FloatType(1.75)))) == FloatType(2.0));
	CHECK(scalar_cast(scalar_round_bankers(scalar_set(FloatType(2.5)))) == FloatType(2.0));

	CHECK(scalar_fraction(FloatType(0.0)) == FloatType(0.0));
	CHECK(scalar_fraction(FloatType(-0.0)) == FloatType(0.0));
	CHECK(scalar_fraction(FloatType(1.0)) == FloatType(0.0));
	CHECK(scalar_fraction(FloatType(-1.0)) == FloatType(0.0));
	CHECK(scalar_near_equal(scalar_fraction(FloatType(0.25)), FloatType(0.25), threshold));
	CHECK(scalar_near_equal(scalar_fraction(FloatType(0.5)), FloatType(0.5), threshold));
	CHECK(scalar_near_equal(scalar_fraction(FloatType(0.75)), FloatType(0.75), threshold));

	CHECK(scalar_deg_to_rad(FloatType(0.0)) == FloatType(0.0));
	CHECK(scalar_near_equal(scalar_deg_to_rad(FloatType(90.0)), FloatType(rtm::constants::half_pi()), threshold));
	CHECK(scalar_near_equal(scalar_deg_to_rad(FloatType(-90.0)), FloatType(-rtm::constants::half_pi()), threshold));
	CHECK(scalar_near_equal(scalar_deg_to_rad(FloatType(180.0)), FloatType(rtm::constants::pi()), threshold));
	CHECK(scalar_near_equal(scalar_deg_to_rad(FloatType(-180.0)), FloatType(-rtm::constants::pi()), threshold));
	CHECK(scalar_near_equal(scalar_deg_to_rad(FloatType(360.0)), FloatType(rtm::constants::two_pi()), threshold));
	CHECK(scalar_near_equal(scalar_deg_to_rad(FloatType(-360.0)), FloatType(-rtm::constants::two_pi()), threshold));

	CHECK(scalar_rad_to_deg(FloatType(0.0)) == FloatType(0.0));
	CHECK(scalar_near_equal(scalar_rad_to_deg(FloatType(rtm::constants::half_pi())), FloatType(90.0), threshold));
	CHECK(scalar_near_equal(scalar_rad_to_deg(FloatType(-rtm::constants::half_pi())), FloatType(-90.0), threshold));
	CHECK(scalar_near_equal(scalar_rad_to_deg(FloatType(rtm::constants::pi())), FloatType(180.0), threshold));
	CHECK(scalar_near_equal(scalar_rad_to_deg(FloatType(-rtm::constants::pi())), FloatType(-180.0), threshold));
	CHECK(scalar_near_equal(scalar_rad_to_deg(FloatType(rtm::constants::two_pi())), FloatType(360.0), threshold));
	CHECK(scalar_near_equal(scalar_rad_to_deg(FloatType(-rtm::constants::two_pi())), FloatType(-360.0), threshold));
}

TEST_CASE("scalarf math", "[math][scalar]")
{
	test_scalar_impl<float>(1.0E-6F, 1.0E-5F);

	CHECK(scalar_floor(1073741824.5F) == 1073741824.0F);
	CHECK(scalar_floor(-1073741824.5F) == -1073741824.0F);
	CHECK(scalar_cast(scalar_floor(scalar_set(1073741824.5F))) == 1073741824.0F);
	CHECK(scalar_cast(scalar_floor(scalar_set(-1073741824.5F))) == -1073741824.0F);

	CHECK(scalar_ceil(1073741824.5F) == 1073741824.0F);
	CHECK(scalar_ceil(-1073741824.5F) == -1073741824.0F);
	CHECK(scalar_cast(scalar_ceil(scalar_set(1073741824.5F))) == 1073741824.0F);
	CHECK(scalar_cast(scalar_ceil(scalar_set(-1073741824.5F))) == -1073741824.0F);

	CHECK(scalar_round_symmetric(1073741824.5F) == 1073741824.0F);
	CHECK(scalar_round_symmetric(-1073741824.5F) == -1073741824.0F);
	CHECK(scalar_cast(scalar_round_symmetric(scalar_set(1073741824.5F))) == 1073741824.0F);
	CHECK(scalar_cast(scalar_round_symmetric(scalar_set(-1073741824.5F))) == -1073741824.0F);

	CHECK(scalar_round_bankers(1073741824.5F) == 1073741824.0F);
	CHECK(scalar_round_bankers(-1073741824.5F) == -1073741824.0F);
	CHECK(scalar_cast(scalar_round_bankers(scalar_set(1073741824.5F))) == 1073741824.0F);
	CHECK(scalar_cast(scalar_round_bankers(scalar_set(-1073741824.5F))) == -1073741824.0F);
}

TEST_CASE("scalard math", "[math][scalar]")
{
	test_scalar_impl<double>(1.0E-9, 1.0E-9);

	CHECK(scalar_floor(36028797018963968.5) == 36028797018963968.5);
	CHECK(scalar_floor(-36028797018963968.5) == -36028797018963968.5);
	CHECK(scalar_cast(scalar_floor(scalar_set(36028797018963968.5))) == 36028797018963968.5);
	CHECK(scalar_cast(scalar_floor(scalar_set(-36028797018963968.5))) == -36028797018963968.5);

	CHECK(scalar_ceil(36028797018963968.5) == 36028797018963968.5);
	CHECK(scalar_ceil(-36028797018963968.5) == -36028797018963968.5);
	CHECK(scalar_cast(scalar_ceil(scalar_set(36028797018963968.5))) == 36028797018963968.5);
	CHECK(scalar_cast(scalar_ceil(scalar_set(-36028797018963968.5))) == -36028797018963968.5);

	CHECK(scalar_round_symmetric(36028797018963968.5) == 36028797018963968.5);
	CHECK(scalar_round_symmetric(-36028797018963968.5) == -36028797018963968.5);
	CHECK(scalar_cast(scalar_round_symmetric(scalar_set(36028797018963968.5))) == 36028797018963968.5);
	CHECK(scalar_cast(scalar_round_symmetric(scalar_set(-36028797018963968.5))) == -36028797018963968.5);

	CHECK(scalar_round_bankers(36028797018963968.5) == 36028797018963968.5);
	CHECK(scalar_round_bankers(-36028797018963968.5) == -36028797018963968.5);
	CHECK(scalar_cast(scalar_round_bankers(scalar_set(36028797018963968.5))) == 36028797018963968.5);
	CHECK(scalar_cast(scalar_round_bankers(scalar_set(-36028797018963968.5))) == -36028797018963968.5);
}

```

`tests/sources/test_vector4_cast.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2025 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include "catch2.impl.h"

#include <rtm/scalard.h>
#include <rtm/scalarf.h>
#include <rtm/vector4d.h>
#include <rtm/vector4f.h>

using namespace rtm;

TEST_CASE("vector4f math cast", "[math][vector4]")
{
	const vector4f src = vector_set(-2.65F, 2.996113F, 0.68123521F, -5.9182F);
	const vector4d dst = vector_cast(src);
	CHECK(scalar_near_equal(vector_get_x(dst), -2.65, 1.0E-6));
	CHECK(scalar_near_equal(vector_get_y(dst), 2.996113, 1.0E-6));
	CHECK(scalar_near_equal(vector_get_z(dst), 0.68123521, 1.0E-6));
	CHECK(scalar_near_equal(vector_get_w(dst), -5.9182, 1.0E-6));

	const vector4f large_values = vector_set(1073741824.5F, 1073741824.5F, -1073741824.5F, -1073741824.5F);
	CHECK(float(vector_get_x(vector_floor(large_values))) == scalar_floor(float(vector_get_x(large_values))));
	CHECK(float(vector_get_y(vector_floor(large_values))) == scalar_floor(float(vector_get_y(large_values))));
	CHECK(float(vector_get_z(vector_floor(large_values))) == scalar_floor(float(vector_get_z(large_values))));
	CHECK(float(vector_get_w(vector_floor(large_values))) == scalar_floor(float(vector_get_w(large_values))));

	CHECK(float(vector_get_x(vector_ceil(large_values))) == scalar_ceil(float(vector_get_x(large_values))));
	CHECK(float(vector_get_y(vector_ceil(large_values))) == scalar_ceil(float(vector_get_y(large_values))));
	CHECK(float(vector_get_z(vector_ceil(large_values))) == scalar_ceil(float(vector_get_z(large_values))));
	CHECK(float(vector_get_w(vector_ceil(large_values))) == scalar_ceil(float(vector_get_w(large_values))));
}

TEST_CASE("vector4d math cast", "[math][vector4]")
{
	const vector4d src = vector_set(-2.65, 2.996113, 0.68123521, -5.9182);
	const vector4f dst = vector_cast(src);
	CHECK(scalar_near_equal(vector_get_x(dst), -2.65F, 1.0E-6F));
	CHECK(scalar_near_equal(vector_get_y(dst), 2.996113F, 1.0E-6F));
	CHECK(scalar_near_equal(vector_get_z(dst), 0.68123521F, 1.0E-6F));
	CHECK(scalar_near_equal(vector_get_w(dst), -5.9182F, 1.0E-6F));

	const vector4d large_values = vector_set(36028797018963968.5, 36028797018963968.5, -36028797018963968.5, -36028797018963968.5);
	CHECK(double(vector_get_x(vector_floor(large_values))) == scalar_floor(double(vector_get_x(large_values))));
	CHECK(double(vector_get_y(vector_floor(large_values))) == scalar_floor(double(vector_get_y(large_values))));
	CHECK(double(vector_get_z(vector_floor(large_values))) == scalar_floor(double(vector_get_z(large_values))));
	CHECK(double(vector_get_w(vector_floor(large_values))) == scalar_floor(double(vector_get_w(large_values))));

	CHECK(double(vector_get_x(vector_ceil(large_values))) == scalar_ceil(double(vector_get_x(large_values))));
	CHECK(double(vector_get_y(vector_ceil(large_values))) == scalar_ceil(double(vector_get_y(large_values))));
	CHECK(double(vector_get_z(vector_ceil(large_values))) == scalar_ceil(double(vector_get_z(large_values))));
	CHECK(double(vector_get_w(vector_ceil(large_values))) == scalar_ceil(double(vector_get_w(large_values))));
}

```

`tests/sources/test_vector4_impl.h`:

```h
#pragma once

////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2018 Nicholas Frechette & Animation Compression Library contributors
// Copyright (c) 2018 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/type_traits.h>

// cpp files that include this header must include the vector header they wish to use
//#include <rtm/vector4f.h>
//#include <rtm/vector4d.h>

#include <cstring>
#include <limits>
#include <utility>

using namespace rtm;

template<typename Vector4Type, typename FloatType>
inline const FloatType* vector_as_float_ptr_raw(const Vector4Type& input)
{
	return vector_to_pointer(input);
}

template<typename Vector4Type, typename FloatType>
inline Vector4Type scalar_cross3(const Vector4Type& lhs, const Vector4Type& rhs)
{
	const FloatType lhs_x = vector_get_x(lhs);
	const FloatType lhs_y = vector_get_y(lhs);
	const FloatType lhs_z = vector_get_z(lhs);
	const FloatType rhs_x = vector_get_x(rhs);
	const FloatType rhs_y = vector_get_y(rhs);
	const FloatType rhs_z = vector_get_z(rhs);
	return vector_set((lhs_y * rhs_z) - (lhs_z * rhs_y), (lhs_z * rhs_x) - (lhs_x * rhs_z), (lhs_x * rhs_y) - (lhs_y * rhs_x));
}

template<typename Vector4Type, typename FloatType>
inline FloatType scalar_dot4(const Vector4Type& lhs, const Vector4Type& rhs)
{
	const FloatType lhs_x = vector_get_x(lhs);
	const FloatType lhs_y = vector_get_y(lhs);
	const FloatType lhs_z = vector_get_z(lhs);
	const FloatType lhs_w = vector_get_w(lhs);
	const FloatType rhs_x = vector_get_x(rhs);
	const FloatType rhs_y = vector_get_y(rhs);
	const FloatType rhs_z = vector_get_z(rhs);
	const FloatType rhs_w = vector_get_w(rhs);
	return (lhs_x * rhs_x) + (lhs_y * rhs_y) + (lhs_z * rhs_z) + (lhs_w * rhs_w);
}

template<typename Vector4Type, typename FloatType>
inline FloatType scalar_dot3(const Vector4Type& lhs, const Vector4Type& rhs)
{
	const FloatType lhs_x = vector_get_x(lhs);
	const FloatType lhs_y = vector_get_y(lhs);
	const FloatType lhs_z = vector_get_z(lhs);
	const FloatType rhs_x = vector_get_x(rhs);
	const FloatType rhs_y = vector_get_y(rhs);
	const FloatType rhs_z = vector_get_z(rhs);
	return (lhs_x * rhs_x) + (lhs_y * rhs_y) + (lhs_z * rhs_z);
}

template<typename Vector4Type, typename FloatType>
inline FloatType scalar_dot2(const Vector4Type& lhs, const Vector4Type& rhs)
{
	const FloatType lhs_x = vector_get_x(lhs);
	const FloatType lhs_y = vector_get_y(lhs);
	const FloatType rhs_x = vector_get_x(rhs);
	const FloatType rhs_y = vector_get_y(rhs);
	return (lhs_x * rhs_x) + (lhs_y * rhs_y);
}

template<typename Vector4Type, typename FloatType>
inline Vector4Type scalar_normalize2(const Vector4Type& input, const Vector4Type& fallback, FloatType threshold)
{
	FloatType len_sq = scalar_dot2<Vector4Type, FloatType>(input, input);
	if (len_sq >= threshold)
	{
		FloatType inv_len = rtm::scalar_sqrt_reciprocal(len_sq);
		return vector_set(vector_get_x(input) * inv_len, vector_get_y(input) * inv_len, (FloatType)0.0);
	}
	else
		return fallback;
}

template<typename Vector4Type, typename FloatType>
inline Vector4Type scalar_normalize3(const Vector4Type& input, const Vector4Type& fallback, FloatType threshold)
{
	FloatType len_sq = scalar_dot3<Vector4Type, FloatType>(input, input);
	if (len_sq >= threshold)
	{
		FloatType inv_len = rtm::scalar_sqrt_reciprocal(len_sq);
		return vector_set(vector_get_x(input) * inv_len, vector_get_y(input) * inv_len, vector_get_z(input) * inv_len);
	}
	else
		return fallback;
}

template<typename Vector4Type, typename FloatType>
inline Vector4Type scalar_normalize4(const Vector4Type& input, const Vector4Type& fallback, FloatType threshold)
{
	FloatType len_sq = scalar_dot4<Vector4Type, FloatType>(input, input);
	if (len_sq >= threshold)
	{
		FloatType inv_len = rtm::scalar_sqrt_reciprocal(len_sq);
		return vector_set(vector_get_x(input) * inv_len, vector_get_y(input) * inv_len, vector_get_z(input) * inv_len, vector_get_w(input) * inv_len);
	}
	else
		return fallback;
}

template<typename FloatType>
void test_vector4_getset_impl()
{
	using QuatType = typename related_types<FloatType>::quat;
	using Vector4Type = typename related_types<FloatType>::vector4;
	using ScalarType = typename related_types<FloatType>::scalar;
	using Float2Type = typename related_types<FloatType>::float2;
	using Float3Type = typename related_types<FloatType>::float3;
	using Float4Type = typename related_types<FloatType>::float4;

	const Vector4Type zero = vector_zero();
	const QuatType identity = quat_identity();

	struct alignas(16) Tmp
	{
		uint8_t padding0[8];	//  8 |  8
		FloatType values[4];	// 24 | 40
		uint8_t padding1[8];	// 32 | 48
	};

	Tmp tmp = { { 0 }, { FloatType(0.0), FloatType(2.34), FloatType(-3.12), FloatType(10000.0) }, {} };
	alignas(16) uint8_t buffer[64];

	const FloatType test_value0_flt[4] = { FloatType(2.0), FloatType(9.34), FloatType(-54.12), FloatType(6000.0) };
	const FloatType test_value1_flt[4] = { FloatType(0.75), FloatType(-4.52), FloatType(44.68), FloatType(-54225.0) };
	const FloatType test_value3_flt[4] = { FloatType(2.0), FloatType(-9.34), FloatType(54.12), FloatType(6000.1) };
	const Vector4Type test_value0 = vector_set(test_value0_flt[0], test_value0_flt[1], test_value0_flt[2], test_value0_flt[3]);
	const Vector4Type test_value1 = vector_set(test_value1_flt[0], test_value1_flt[1], test_value1_flt[2], test_value1_flt[3]);
	const Vector4Type test_value3 = vector_set(test_value3_flt[0], test_value3_flt[1], test_value3_flt[2], test_value3_flt[3]);

	//////////////////////////////////////////////////////////////////////////
	// Setters, getters, and casts

	CHECK(FloatType(vector_get_x(vector_set(FloatType(0.0), FloatType(2.34), FloatType(-3.12), FloatType(10000.0)))) == FloatType(0.0));
	CHECK(FloatType(vector_get_y(vector_set(FloatType(0.0), FloatType(2.34), FloatType(-3.12), FloatType(10000.0)))) == FloatType(2.34));
	CHECK(FloatType(vector_get_z(vector_set(FloatType(0.0), FloatType(2.34), FloatType(-3.12), FloatType(10000.0)))) == FloatType(-3.12));
	CHECK(FloatType(vector_get_w(vector_set(FloatType(0.0), FloatType(2.34), FloatType(-3.12), FloatType(10000.0)))) == FloatType(10000.0));

	CHECK(scalar_cast(vector_get_x_as_scalar(vector_set(FloatType(0.0), FloatType(2.34), FloatType(-3.12), FloatType(10000.0)))) == FloatType(0.0));
	CHECK(scalar_cast(vector_get_y_as_scalar(vector_set(FloatType(0.0), FloatType(2.34), FloatType(-3.12), FloatType(10000.0)))) == FloatType(2.34));
	CHECK(scalar_cast(vector_get_z_as_scalar(vector_set(FloatType(0.0), FloatType(2.34), FloatType(-3.12), FloatType(10000.0)))) == FloatType(-3.12));
	CHECK(scalar_cast(vector_get_w_as_scalar(vector_set(FloatType(0.0), FloatType(2.34), FloatType(-3.12), FloatType(10000.0)))) == FloatType(10000.0));

	CHECK(FloatType(vector_get_x(vector_set(scalar_set(FloatType(0.0)), scalar_set(FloatType(2.34)), scalar_set(FloatType(-3.12)), scalar_set(FloatType(10000.0))))) == FloatType(0.0));
	CHECK(FloatType(vector_get_y(vector_set(scalar_set(FloatType(0.0)), scalar_set(FloatType(2.34)), scalar_set(FloatType(-3.12)), scalar_set(FloatType(10000.0))))) == FloatType(2.34));
	CHECK(FloatType(vector_get_z(vector_set(scalar_set(FloatType(0.0)), scalar_set(FloatType(2.34)), scalar_set(FloatType(-3.12)), scalar_set(FloatType(10000.0))))) == FloatType(-3.12));
	CHECK(FloatType(vector_get_w(vector_set(scalar_set(FloatType(0.0)), scalar_set(FloatType(2.34)), scalar_set(FloatType(-3.12)), scalar_set(FloatType(10000.0))))) == FloatType(10000.0));

	CHECK(FloatType(vector_get_x(vector_set(FloatType(0.0), FloatType(2.34), FloatType(-3.12)))) == FloatType(0.0));
	CHECK(FloatType(vector_get_y(vector_set(FloatType(0.0), FloatType(2.34), FloatType(-3.12)))) == FloatType(2.34));
	CHECK(FloatType(vector_get_z(vector_set(FloatType(0.0), FloatType(2.34), FloatType(-3.12)))) == FloatType(-3.12));
	CHECK(FloatType(vector_get_w(vector_set(FloatType(0.0), FloatType(2.34), FloatType(-3.12)))) == FloatType(0.0));

	CHECK(FloatType(vector_get_x(vector_set(scalar_set(FloatType(0.0)), scalar_set(FloatType(2.34)), scalar_set(FloatType(-3.12))))) == FloatType(0.0));
	CHECK(FloatType(vector_get_y(vector_set(scalar_set(FloatType(0.0)), scalar_set(FloatType(2.34)), scalar_set(FloatType(-3.12))))) == FloatType(2.34));
	CHECK(FloatType(vector_get_z(vector_set(scalar_set(FloatType(0.0)), scalar_set(FloatType(2.34)), scalar_set(FloatType(-3.12))))) == FloatType(-3.12));
	CHECK(FloatType(vector_get_w(vector_set(scalar_set(FloatType(0.0)), scalar_set(FloatType(2.34)), scalar_set(FloatType(-3.12))))) == FloatType(0.0));

	CHECK(FloatType(vector_get_x(vector_set(FloatType(-3.12)))) == FloatType(-3.12));
	CHECK(FloatType(vector_get_y(vector_set(FloatType(-3.12)))) == FloatType(-3.12));
	CHECK(FloatType(vector_get_z(vector_set(FloatType(-3.12)))) == FloatType(-3.12));
	CHECK(FloatType(vector_get_w(vector_set(FloatType(-3.12)))) == FloatType(-3.12));

	CHECK(FloatType(vector_get_x(vector_set(scalar_set(FloatType(-3.12))))) == FloatType(-3.12));
	CHECK(FloatType(vector_get_y(vector_set(scalar_set(FloatType(-3.12))))) == FloatType(-3.12));
	CHECK(FloatType(vector_get_z(vector_set(scalar_set(FloatType(-3.12))))) == FloatType(-3.12));
	CHECK(FloatType(vector_get_w(vector_set(scalar_set(FloatType(-3.12))))) == FloatType(-3.12));

	CHECK(FloatType(vector_get_x(zero)) == FloatType(0.0));
	CHECK(FloatType(vector_get_y(zero)) == FloatType(0.0));
	CHECK(FloatType(vector_get_z(zero)) == FloatType(0.0));
	CHECK(FloatType(vector_get_w(zero)) == FloatType(0.0));

	CHECK(FloatType(vector_get_x((Vector4Type)vector_coord_forward())) == FloatType(0.0));
	CHECK(FloatType(vector_get_y((Vector4Type)vector_coord_forward())) == FloatType(0.0));
	CHECK(FloatType(vector_get_z((Vector4Type)vector_coord_forward())) == FloatType(1.0));
	CHECK(FloatType(vector_get_w((Vector4Type)vector_coord_forward())) == FloatType(0.0));

	CHECK(FloatType(vector_get_x((Vector4Type)vector_coord_up())) == FloatType(0.0));
	CHECK(FloatType(vector_get_y((Vector4Type)vector_coord_up())) == FloatType(1.0));
	CHECK(FloatType(vector_get_z((Vector4Type)vector_coord_up())) == FloatType(0.0));
	CHECK(FloatType(vector_get_w((Vector4Type)vector_coord_up())) == FloatType(0.0));

	CHECK(FloatType(vector_get_x((Vector4Type)vector_coord_cross())) == FloatType(1.0));
	CHECK(FloatType(vector_get_y((Vector4Type)vector_coord_cross())) == FloatType(0.0));
	CHECK(FloatType(vector_get_z((Vector4Type)vector_coord_cross())) == FloatType(0.0));
	CHECK(FloatType(vector_get_w((Vector4Type)vector_coord_cross())) == FloatType(0.0));

	CHECK(FloatType(vector_get_x((Vector4Type)vector_load(&tmp.values[0]))) == tmp.values[0]);
	CHECK(FloatType(vector_get_y((Vector4Type)vector_load(&tmp.values[0]))) == tmp.values[1]);
	CHECK(FloatType(vector_get_z((Vector4Type)vector_load(&tmp.values[0]))) == tmp.values[2]);
	CHECK(FloatType(vector_get_w((Vector4Type)vector_load(&tmp.values[0]))) == tmp.values[3]);

	CHECK(FloatType(vector_get_x((Vector4Type)vector_load1(&tmp.values[1]))) == tmp.values[1]);
	CHECK(FloatType(vector_get_y((Vector4Type)vector_load1(&tmp.values[1]))) == FloatType(0.0));
	CHECK(FloatType(vector_get_z((Vector4Type)vector_load1(&tmp.values[1]))) == FloatType(0.0));
	CHECK(FloatType(vector_get_w((Vector4Type)vector_load1(&tmp.values[1]))) == FloatType(0.0));

	CHECK(FloatType(vector_get_x((Vector4Type)vector_load2(&tmp.values[1]))) == tmp.values[1]);
	CHECK(FloatType(vector_get_y((Vector4Type)vector_load2(&tmp.values[1]))) == tmp.values[2]);
	CHECK(FloatType(vector_get_z((Vector4Type)vector_load2(&tmp.values[1]))) == FloatType(0.0));
	CHECK(FloatType(vector_get_w((Vector4Type)vector_load2(&tmp.values[1]))) == FloatType(0.0));

	CHECK(FloatType(vector_get_x((Vector4Type)vector_load3(&tmp.values[1]))) == tmp.values[1]);
	CHECK(FloatType(vector_get_y((Vector4Type)vector_load3(&tmp.values[1]))) == tmp.values[2]);
	CHECK(FloatType(vector_get_z((Vector4Type)vector_load3(&tmp.values[1]))) == tmp.values[3]);
	CHECK(FloatType(vector_get_w((Vector4Type)vector_load3(&tmp.values[1]))) == FloatType(0.0));

	Float2Type tmpf2 = { tmp.values[0], tmp.values[1] };
	Float3Type tmpf3 = { tmp.values[0], tmp.values[1], tmp.values[2] };
	Float4Type tmpf4 = { tmp.values[0], tmp.values[1], tmp.values[2], tmp.values[3] };

	CHECK(FloatType(vector_get_x((Vector4Type)vector_load2(&tmpf2))) == tmpf2.x);
	CHECK(FloatType(vector_get_y((Vector4Type)vector_load2(&tmpf2))) == tmpf2.y);
	CHECK(FloatType(vector_get_z((Vector4Type)vector_load2(&tmpf2))) == FloatType(0.0));
	CHECK(FloatType(vector_get_w((Vector4Type)vector_load2(&tmpf2))) == FloatType(0.0));

	CHECK(FloatType(vector_get_x((Vector4Type)vector_load3(&tmpf3))) == tmpf3.x);
	CHECK(FloatType(vector_get_y((Vector4Type)vector_load3(&tmpf3))) == tmpf3.y);
	CHECK(FloatType(vector_get_z((Vector4Type)vector_load3(&tmpf3))) == tmpf3.z);
	CHECK(FloatType(vector_get_w((Vector4Type)vector_load3(&tmpf3))) == FloatType(0.0));

	CHECK(FloatType(vector_get_x((Vector4Type)vector_load(&tmpf4))) == tmpf4.x);
	CHECK(FloatType(vector_get_y((Vector4Type)vector_load(&tmpf4))) == tmpf4.y);
	CHECK(FloatType(vector_get_z((Vector4Type)vector_load(&tmpf4))) == tmpf4.z);
	CHECK(FloatType(vector_get_w((Vector4Type)vector_load(&tmpf4))) == tmpf4.w);

	std::memcpy(&buffer[1], &tmp.values[0], sizeof(tmp.values));
	CHECK(FloatType(vector_get_x((Vector4Type)vector_load(&buffer[1]))) == tmp.values[0]);
	CHECK(FloatType(vector_get_y((Vector4Type)vector_load(&buffer[1]))) == tmp.values[1]);
	CHECK(FloatType(vector_get_z((Vector4Type)vector_load(&buffer[1]))) == tmp.values[2]);
	CHECK(FloatType(vector_get_w((Vector4Type)vector_load(&buffer[1]))) == tmp.values[3]);

	CHECK(FloatType(vector_get_x((Vector4Type)vector_load1(&buffer[1 + sizeof(FloatType)]))) == tmp.values[1]);
	CHECK(FloatType(vector_get_y((Vector4Type)vector_load1(&buffer[1 + sizeof(FloatType)]))) == FloatType(0.0));
	CHECK(FloatType(vector_get_z((Vector4Type)vector_load1(&buffer[1 + sizeof(FloatType)]))) == FloatType(0.0));
	CHECK(FloatType(vector_get_w((Vector4Type)vector_load1(&buffer[1 + sizeof(FloatType)]))) == FloatType(0.0));

	CHECK(FloatType(vector_get_x((Vector4Type)vector_load2(&buffer[1 + sizeof(FloatType)]))) == tmp.values[1]);
	CHECK(FloatType(vector_get_y((Vector4Type)vector_load2(&buffer[1 + sizeof(FloatType)]))) == tmp.values[2]);
	CHECK(FloatType(vector_get_z((Vector4Type)vector_load2(&buffer[1 + sizeof(FloatType)]))) == FloatType(0.0));
	CHECK(FloatType(vector_get_w((Vector4Type)vector_load2(&buffer[1 + sizeof(FloatType)]))) == FloatType(0.0));

	CHECK(FloatType(vector_get_x((Vector4Type)vector_load3(&buffer[1 + sizeof(FloatType)]))) == tmp.values[1]);
	CHECK(FloatType(vector_get_y((Vector4Type)vector_load3(&buffer[1 + sizeof(FloatType)]))) == tmp.values[2]);
	CHECK(FloatType(vector_get_z((Vector4Type)vector_load3(&buffer[1 + sizeof(FloatType)]))) == tmp.values[3]);
	CHECK(FloatType(vector_get_w((Vector4Type)vector_load3(&buffer[1 + sizeof(FloatType)]))) == FloatType(0.0));

	CHECK(FloatType(vector_get_x((Vector4Type)vector_broadcast(&tmp.values[0]))) == tmp.values[0]);
	CHECK(FloatType(vector_get_y((Vector4Type)vector_broadcast(&tmp.values[0]))) == tmp.values[0]);
	CHECK(FloatType(vector_get_z((Vector4Type)vector_broadcast(&tmp.values[0]))) == tmp.values[0]);
	CHECK(FloatType(vector_get_w((Vector4Type)vector_broadcast(&tmp.values[0]))) == tmp.values[0]);

	CHECK(FloatType(vector_get_x(quat_to_vector(identity))) == FloatType(quat_get_x(identity)));
	CHECK(FloatType(vector_get_y(quat_to_vector(identity))) == FloatType(quat_get_y(identity)));
	CHECK(FloatType(vector_get_z(quat_to_vector(identity))) == FloatType(quat_get_z(identity)));
	CHECK(FloatType(vector_get_w(quat_to_vector(identity))) == FloatType(quat_get_w(identity)));

	CHECK(FloatType(vector_get_component2<component2::x>(vector_set(FloatType(0.0), FloatType(2.34), FloatType(-3.12), FloatType(10000.0)))) == FloatType(0.0));
	CHECK(FloatType(vector_get_component2<component2::y>(vector_set(FloatType(0.0), FloatType(2.34), FloatType(-3.12), FloatType(10000.0)))) == FloatType(2.34));

	CHECK(FloatType(vector_get_component3<component3::x>(vector_set(FloatType(0.0), FloatType(2.34), FloatType(-3.12), FloatType(10000.0)))) == FloatType(0.0));
	CHECK(FloatType(vector_get_component3<component3::y>(vector_set(FloatType(0.0), FloatType(2.34), FloatType(-3.12), FloatType(10000.0)))) == FloatType(2.34));
	CHECK(FloatType(vector_get_component3<component3::z>(vector_set(FloatType(0.0), FloatType(2.34), FloatType(-3.12), FloatType(10000.0)))) == FloatType(-3.12));

	CHECK(FloatType(vector_get_component<component4::x>(vector_set(FloatType(0.0), FloatType(2.34), FloatType(-3.12), FloatType(10000.0)))) == FloatType(0.0));
	CHECK(FloatType(vector_get_component<component4::y>(vector_set(FloatType(0.0), FloatType(2.34), FloatType(-3.12), FloatType(10000.0)))) == FloatType(2.34));
	CHECK(FloatType(vector_get_component<component4::z>(vector_set(FloatType(0.0), FloatType(2.34), FloatType(-3.12), FloatType(10000.0)))) == FloatType(-3.12));
	CHECK(FloatType(vector_get_component<component4::w>(vector_set(FloatType(0.0), FloatType(2.34), FloatType(-3.12), FloatType(10000.0)))) == FloatType(10000.0));

	CHECK(scalar_cast(vector_get_component2_as_scalar<component2::x>(vector_set(FloatType(0.0), FloatType(2.34), FloatType(-3.12), FloatType(10000.0)))) == FloatType(0.0));
	CHECK(scalar_cast(vector_get_component2_as_scalar<component2::y>(vector_set(FloatType(0.0), FloatType(2.34), FloatType(-3.12), FloatType(10000.0)))) == FloatType(2.34));

	CHECK(scalar_cast(vector_get_component3_as_scalar<component3::x>(vector_set(FloatType(0.0), FloatType(2.34), FloatType(-3.12), FloatType(10000.0)))) == FloatType(0.0));
	CHECK(scalar_cast(vector_get_component3_as_scalar<component3::y>(vector_set(FloatType(0.0), FloatType(2.34), FloatType(-3.12), FloatType(10000.0)))) == FloatType(2.34));
	CHECK(scalar_cast(vector_get_component3_as_scalar<component3::z>(vector_set(FloatType(0.0), FloatType(2.34), FloatType(-3.12), FloatType(10000.0)))) == FloatType(-3.12));

	CHECK(scalar_cast(vector_get_component_as_scalar<component4::x>(vector_set(FloatType(0.0), FloatType(2.34), FloatType(-3.12), FloatType(10000.0)))) == FloatType(0.0));
	CHECK(scalar_cast(vector_get_component_as_scalar<component4::y>(vector_set(FloatType(0.0), FloatType(2.34), FloatType(-3.12), FloatType(10000.0)))) == FloatType(2.34));
	CHECK(scalar_cast(vector_get_component_as_scalar<component4::z>(vector_set(FloatType(0.0), FloatType(2.34), FloatType(-3.12), FloatType(10000.0)))) == FloatType(-3.12));
	CHECK(scalar_cast(vector_get_component_as_scalar<component4::w>(vector_set(FloatType(0.0), FloatType(2.34), FloatType(-3.12), FloatType(10000.0)))) == FloatType(10000.0));

	CHECK(FloatType(vector_get_component2(vector_set(FloatType(0.0), FloatType(2.34), FloatType(-3.12), FloatType(10000.0)), component2::x)) == FloatType(0.0));
	CHECK(FloatType(vector_get_component2(vector_set(FloatType(0.0), FloatType(2.34), FloatType(-3.12), FloatType(10000.0)), component2::y)) == FloatType(2.34));

	CHECK(FloatType(vector_get_component3(vector_set(FloatType(0.0), FloatType(2.34), FloatType(-3.12), FloatType(10000.0)), component3::x)) == FloatType(0.0));
	CHECK(FloatType(vector_get_component3(vector_set(FloatType(0.0), FloatType(2.34), FloatType(-3.12), FloatType(10000.0)), component3::y)) == FloatType(2.34));
	CHECK(FloatType(vector_get_component3(vector_set(FloatType(0.0), FloatType(2.34), FloatType(-3.12), FloatType(10000.0)), component3::z)) == FloatType(-3.12));

	CHECK(FloatType(vector_get_component(vector_set(FloatType(0.0), FloatType(2.34), FloatType(-3.12), FloatType(10000.0)), component4::x)) == FloatType(0.0));
	CHECK(FloatType(vector_get_component(vector_set(FloatType(0.0), FloatType(2.34), FloatType(-3.12), FloatType(10000.0)), component4::y)) == FloatType(2.34));
	CHECK(FloatType(vector_get_component(vector_set(FloatType(0.0), FloatType(2.34), FloatType(-3.12), FloatType(10000.0)), component4::z)) == FloatType(-3.12));
	CHECK(FloatType(vector_get_component(vector_set(FloatType(0.0), FloatType(2.34), FloatType(-3.12), FloatType(10000.0)), component4::w)) == FloatType(10000.0));

	CHECK(scalar_cast(vector_get_component2_as_scalar(vector_set(FloatType(0.0), FloatType(2.34), FloatType(-3.12), FloatType(10000.0)), component2::x)) == FloatType(0.0));
	CHECK(scalar_cast(vector_get_component2_as_scalar(vector_set(FloatType(0.0), FloatType(2.34), FloatType(-3.12), FloatType(10000.0)), component2::y)) == FloatType(2.34));

	CHECK(scalar_cast(vector_get_component3_as_scalar(vector_set(FloatType(0.0), FloatType(2.34), FloatType(-3.12), FloatType(10000.0)), component3::x)) == FloatType(0.0));
	CHECK(scalar_cast(vector_get_component3_as_scalar(vector_set(FloatType(0.0), FloatType(2.34), FloatType(-3.12), FloatType(10000.0)), component3::y)) == FloatType(2.34));
	CHECK(scalar_cast(vector_get_component3_as_scalar(vector_set(FloatType(0.0), FloatType(2.34), FloatType(-3.12), FloatType(10000.0)), component3::z)) == FloatType(-3.12));

	CHECK(scalar_cast(vector_get_component_as_scalar(vector_set(FloatType(0.0), FloatType(2.34), FloatType(-3.12), FloatType(10000.0)), component4::x)) == FloatType(0.0));
	CHECK(scalar_cast(vector_get_component_as_scalar(vector_set(FloatType(0.0), FloatType(2.34), FloatType(-3.12), FloatType(10000.0)), component4::y)) == FloatType(2.34));
	CHECK(scalar_cast(vector_get_component_as_scalar(vector_set(FloatType(0.0), FloatType(2.34), FloatType(-3.12), FloatType(10000.0)), component4::z)) == FloatType(-3.12));
	CHECK(scalar_cast(vector_get_component_as_scalar(vector_set(FloatType(0.0), FloatType(2.34), FloatType(-3.12), FloatType(10000.0)), component4::w)) == FloatType(10000.0));

	CHECK((vector_as_float_ptr_raw<Vector4Type, FloatType>(vector_load(&tmp.values[0]))[0] == tmp.values[0]));
	CHECK((vector_as_float_ptr_raw<Vector4Type, FloatType>(vector_load(&tmp.values[0]))[1] == tmp.values[1]));
	CHECK((vector_as_float_ptr_raw<Vector4Type, FloatType>(vector_load(&tmp.values[0]))[2] == tmp.values[2]));
	CHECK((vector_as_float_ptr_raw<Vector4Type, FloatType>(vector_load(&tmp.values[0]))[3] == tmp.values[3]));

	vector_store(test_value0, &tmp.values[0]);
	CHECK(FloatType(vector_get_x(test_value0)) == tmp.values[0]);
	CHECK(FloatType(vector_get_y(test_value0)) == tmp.values[1]);
	CHECK(FloatType(vector_get_z(test_value0)) == tmp.values[2]);
	CHECK(FloatType(vector_get_w(test_value0)) == tmp.values[3]);

	vector_store1(test_value0, &tmp.values[0]);
	CHECK(FloatType(vector_get_x(test_value0)) == tmp.values[0]);

	vector_store2(test_value0, &tmp.values[0]);
	CHECK(FloatType(vector_get_x(test_value0)) == tmp.values[0]);
	CHECK(FloatType(vector_get_y(test_value0)) == tmp.values[1]);

	vector_store3(test_value1, &tmp.values[0]);
	CHECK(FloatType(vector_get_x(test_value1)) == tmp.values[0]);
	CHECK(FloatType(vector_get_y(test_value1)) == tmp.values[1]);
	CHECK(FloatType(vector_get_z(test_value1)) == tmp.values[2]);
	CHECK(FloatType(vector_get_w(test_value0)) == tmp.values[3]);

	vector_store(test_value1, &buffer[1]);
	CHECK(FloatType(vector_get_x(test_value1)) == FloatType(vector_get_x((Vector4Type)vector_load(&buffer[1]))));
	CHECK(FloatType(vector_get_y(test_value1)) == FloatType(vector_get_y((Vector4Type)vector_load(&buffer[1]))));
	CHECK(FloatType(vector_get_z(test_value1)) == FloatType(vector_get_z((Vector4Type)vector_load(&buffer[1]))));
	CHECK(FloatType(vector_get_w(test_value1)) == FloatType(vector_get_w((Vector4Type)vector_load(&buffer[1]))));

	vector_store1(test_value1, &buffer[1]);
	CHECK(FloatType(vector_get_x(test_value1)) == FloatType(vector_get_x((Vector4Type)vector_load1(&buffer[1]))));

	vector_store2(test_value1, &buffer[1]);
	CHECK(FloatType(vector_get_x(test_value1)) == FloatType(vector_get_x((Vector4Type)vector_load2(&buffer[1]))));

	vector_store3(test_value1, &buffer[1]);
	CHECK(FloatType(vector_get_x(test_value1)) == FloatType(vector_get_x((Vector4Type)vector_load3(&buffer[1]))));
	CHECK(FloatType(vector_get_y(test_value1)) == FloatType(vector_get_y((Vector4Type)vector_load3(&buffer[1]))));
	CHECK(FloatType(vector_get_z(test_value1)) == FloatType(vector_get_z((Vector4Type)vector_load3(&buffer[1]))));

	vector_store(test_value1, &tmpf4);
	CHECK(FloatType(vector_get_x(test_value1)) == tmpf4.x);
	CHECK(FloatType(vector_get_y(test_value1)) == tmpf4.y);
	CHECK(FloatType(vector_get_z(test_value1)) == tmpf4.z);
	CHECK(FloatType(vector_get_w(test_value1)) == tmpf4.w);

	vector_store2(test_value1, &tmpf2);
	CHECK(FloatType(vector_get_x(test_value1)) == tmpf2.x);
	CHECK(FloatType(vector_get_y(test_value1)) == tmpf2.y);

	vector_store3(test_value1, &tmpf3);
	CHECK(FloatType(vector_get_x(test_value1)) == tmpf3.x);
	CHECK(FloatType(vector_get_y(test_value1)) == tmpf3.y);
	CHECK(FloatType(vector_get_z(test_value1)) == tmpf3.z);

	CHECK((FloatType)vector_get_min_component(test_value0) == FloatType(vector_get_z(test_value0)));
	CHECK((FloatType)vector_get_min_component(test_value3) == FloatType(vector_get_y(test_value3)));
	CHECK((FloatType)vector_get_max_component(test_value0) == FloatType(vector_get_w(test_value0)));
	CHECK(scalar_equal(vector_get_min_component_as_scalar(test_value0), (ScalarType)vector_as_scalar(vector_dup_z(test_value0))));
	CHECK(scalar_equal(vector_get_min_component_as_scalar(test_value3), (ScalarType)vector_as_scalar(vector_dup_y(test_value3))));
	CHECK(scalar_equal(vector_get_max_component_as_scalar(test_value0), (ScalarType)vector_as_scalar(vector_dup_w(test_value0))));

	CHECK(vector_all_near_equal(vector_set_x(zero, FloatType(4.0)), vector_set(FloatType(4.0), FloatType(0.0), FloatType(0.0), FloatType(0.0)), FloatType(0.0)));
	CHECK(vector_all_near_equal(vector_set_y(zero, FloatType(4.0)), vector_set(FloatType(0.0), FloatType(4.0), FloatType(0.0), FloatType(0.0)), FloatType(0.0)));
	CHECK(vector_all_near_equal(vector_set_z(zero, FloatType(4.0)), vector_set(FloatType(0.0), FloatType(0.0), FloatType(4.0), FloatType(0.0)), FloatType(0.0)));
	CHECK(vector_all_near_equal(vector_set_w(zero, FloatType(4.0)), vector_set(FloatType(0.0), FloatType(0.0), FloatType(0.0), FloatType(4.0)), FloatType(0.0)));

	CHECK(vector_all_near_equal(vector_set_x(zero, scalar_set(FloatType(4.0))), vector_set(FloatType(4.0), FloatType(0.0), FloatType(0.0), FloatType(0.0)), FloatType(0.0)));
	CHECK(vector_all_near_equal(vector_set_y(zero, scalar_set(FloatType(4.0))), vector_set(FloatType(0.0), FloatType(4.0), FloatType(0.0), FloatType(0.0)), FloatType(0.0)));
	CHECK(vector_all_near_equal(vector_set_z(zero, scalar_set(FloatType(4.0))), vector_set(FloatType(0.0), FloatType(0.0), FloatType(4.0), FloatType(0.0)), FloatType(0.0)));
	CHECK(vector_all_near_equal(vector_set_w(zero, scalar_set(FloatType(4.0))), vector_set(FloatType(0.0), FloatType(0.0), FloatType(0.0), FloatType(4.0)), FloatType(0.0)));

	CHECK(vector_all_near_equal(vector_set_component2<component2::x>(zero, FloatType(4.0)), vector_set(FloatType(4.0), FloatType(0.0), FloatType(0.0), FloatType(0.0)), FloatType(0.0)));
	CHECK(vector_all_near_equal(vector_set_component2<component2::y>(zero, FloatType(4.0)), vector_set(FloatType(0.0), FloatType(4.0), FloatType(0.0), FloatType(0.0)), FloatType(0.0)));

	CHECK(vector_all_near_equal(vector_set_component3<component3::x>(zero, FloatType(4.0)), vector_set(FloatType(4.0), FloatType(0.0), FloatType(0.0), FloatType(0.0)), FloatType(0.0)));
	CHECK(vector_all_near_equal(vector_set_component3<component3::y>(zero, FloatType(4.0)), vector_set(FloatType(0.0), FloatType(4.0), FloatType(0.0), FloatType(0.0)), FloatType(0.0)));
	CHECK(vector_all_near_equal(vector_set_component3<component3::z>(zero, FloatType(4.0)), vector_set(FloatType(0.0), FloatType(0.0), FloatType(4.0), FloatType(0.0)), FloatType(0.0)));

	CHECK(vector_all_near_equal(vector_set_component<component4::x>(zero, FloatType(4.0)), vector_set(FloatType(4.0), FloatType(0.0), FloatType(0.0), FloatType(0.0)), FloatType(0.0)));
	CHECK(vector_all_near_equal(vector_set_component<component4::y>(zero, FloatType(4.0)), vector_set(FloatType(0.0), FloatType(4.0), FloatType(0.0), FloatType(0.0)), FloatType(0.0)));
	CHECK(vector_all_near_equal(vector_set_component<component4::z>(zero, FloatType(4.0)), vector_set(FloatType(0.0), FloatType(0.0), FloatType(4.0), FloatType(0.0)), FloatType(0.0)));
	CHECK(vector_all_near_equal(vector_set_component<component4::w>(zero, FloatType(4.0)), vector_set(FloatType(0.0), FloatType(0.0), FloatType(0.0), FloatType(4.0)), FloatType(0.0)));

	CHECK(vector_all_near_equal(vector_set_component2<component2::x>(zero, scalar_set(FloatType(4.0))), vector_set(FloatType(4.0), FloatType(0.0), FloatType(0.0), FloatType(0.0)), FloatType(0.0)));
	CHECK(vector_all_near_equal(vector_set_component2<component2::y>(zero, scalar_set(FloatType(4.0))), vector_set(FloatType(0.0), FloatType(4.0), FloatType(0.0), FloatType(0.0)), FloatType(0.0)));

	CHECK(vector_all_near_equal(vector_set_component3<component3::x>(zero, scalar_set(FloatType(4.0))), vector_set(FloatType(4.0), FloatType(0.0), FloatType(0.0), FloatType(0.0)), FloatType(0.0)));
	CHECK(vector_all_near_equal(vector_set_component3<component3::y>(zero, scalar_set(FloatType(4.0))), vector_set(FloatType(0.0), FloatType(4.0), FloatType(0.0), FloatType(0.0)), FloatType(0.0)));
	CHECK(vector_all_near_equal(vector_set_component3<component3::z>(zero, scalar_set(FloatType(4.0))), vector_set(FloatType(0.0), FloatType(0.0), FloatType(4.0), FloatType(0.0)), FloatType(0.0)));

	CHECK(vector_all_near_equal(vector_set_component<component4::x>(zero, scalar_set(FloatType(4.0))), vector_set(FloatType(4.0), FloatType(0.0), FloatType(0.0), FloatType(0.0)), FloatType(0.0)));
	CHECK(vector_all_near_equal(vector_set_component<component4::y>(zero, scalar_set(FloatType(4.0))), vector_set(FloatType(0.0), FloatType(4.0), FloatType(0.0), FloatType(0.0)), FloatType(0.0)));
	CHECK(vector_all_near_equal(vector_set_component<component4::z>(zero, scalar_set(FloatType(4.0))), vector_set(FloatType(0.0), FloatType(0.0), FloatType(4.0), FloatType(0.0)), FloatType(0.0)));
	CHECK(vector_all_near_equal(vector_set_component<component4::w>(zero, scalar_set(FloatType(4.0))), vector_set(FloatType(0.0), FloatType(0.0), FloatType(0.0), FloatType(4.0)), FloatType(0.0)));

	CHECK(vector_all_near_equal(vector_set_component2(zero, FloatType(4.0), component2::x), vector_set(FloatType(4.0), FloatType(0.0), FloatType(0.0), FloatType(0.0)), FloatType(0.0)));
	CHECK(vector_all_near_equal(vector_set_component2(zero, FloatType(4.0), component2::y), vector_set(FloatType(0.0), FloatType(4.0), FloatType(0.0), FloatType(0.0)), FloatType(0.0)));

	CHECK(vector_all_near_equal(vector_set_component3(zero, FloatType(4.0), component3::x), vector_set(FloatType(4.0), FloatType(0.0), FloatType(0.0), FloatType(0.0)), FloatType(0.0)));
	CHECK(vector_all_near_equal(vector_set_component3(zero, FloatType(4.0), component3::y), vector_set(FloatType(0.0), FloatType(4.0), FloatType(0.0), FloatType(0.0)), FloatType(0.0)));
	CHECK(vector_all_near_equal(vector_set_component3(zero, FloatType(4.0), component3::z), vector_set(FloatType(0.0), FloatType(0.0), FloatType(4.0), FloatType(0.0)), FloatType(0.0)));

	CHECK(vector_all_near_equal(vector_set_component(zero, FloatType(4.0), component4::x), vector_set(FloatType(4.0), FloatType(0.0), FloatType(0.0), FloatType(0.0)), FloatType(0.0)));
	CHECK(vector_all_near_equal(vector_set_component(zero, FloatType(4.0), component4::y), vector_set(FloatType(0.0), FloatType(4.0), FloatType(0.0), FloatType(0.0)), FloatType(0.0)));
	CHECK(vector_all_near_equal(vector_set_component(zero, FloatType(4.0), component4::z), vector_set(FloatType(0.0), FloatType(0.0), FloatType(4.0), FloatType(0.0)), FloatType(0.0)));
	CHECK(vector_all_near_equal(vector_set_component(zero, FloatType(4.0), component4::w), vector_set(FloatType(0.0), FloatType(0.0), FloatType(0.0), FloatType(4.0)), FloatType(0.0)));

	CHECK(vector_all_near_equal(vector_set_component2(zero, scalar_set(FloatType(4.0)), component2::x), vector_set(FloatType(4.0), FloatType(0.0), FloatType(0.0), FloatType(0.0)), FloatType(0.0)));
	CHECK(vector_all_near_equal(vector_set_component2(zero, scalar_set(FloatType(4.0)), component2::y), vector_set(FloatType(0.0), FloatType(4.0), FloatType(0.0), FloatType(0.0)), FloatType(0.0)));

	CHECK(vector_all_near_equal(vector_set_component3(zero, scalar_set(FloatType(4.0)), component3::x), vector_set(FloatType(4.0), FloatType(0.0), FloatType(0.0), FloatType(0.0)), FloatType(0.0)));
	CHECK(vector_all_near_equal(vector_set_component3(zero, scalar_set(FloatType(4.0)), component3::y), vector_set(FloatType(0.0), FloatType(4.0), FloatType(0.0), FloatType(0.0)), FloatType(0.0)));
	CHECK(vector_all_near_equal(vector_set_component3(zero, scalar_set(FloatType(4.0)), component3::z), vector_set(FloatType(0.0), FloatType(0.0), FloatType(4.0), FloatType(0.0)), FloatType(0.0)));

	CHECK(vector_all_near_equal(vector_set_component(zero, scalar_set(FloatType(4.0)), component4::x), vector_set(FloatType(4.0), FloatType(0.0), FloatType(0.0), FloatType(0.0)), FloatType(0.0)));
	CHECK(vector_all_near_equal(vector_set_component(zero, scalar_set(FloatType(4.0)), component4::y), vector_set(FloatType(0.0), FloatType(4.0), FloatType(0.0), FloatType(0.0)), FloatType(0.0)));
	CHECK(vector_all_near_equal(vector_set_component(zero, scalar_set(FloatType(4.0)), component4::z), vector_set(FloatType(0.0), FloatType(0.0), FloatType(4.0), FloatType(0.0)), FloatType(0.0)));
	CHECK(vector_all_near_equal(vector_set_component(zero, scalar_set(FloatType(4.0)), component4::w), vector_set(FloatType(0.0), FloatType(0.0), FloatType(0.0), FloatType(4.0)), FloatType(0.0)));

	CHECK((FloatType)vector_as_scalar(test_value1) == FloatType(vector_get_x(test_value1)));
	CHECK(scalar_equal(vector_as_scalar(test_value1), scalar_set(vector_get_x(test_value1))));
}

template<typename FloatType>
void test_vector4_arithmetic_impl(const FloatType threshold)
{
	using Vector4Type = typename related_types<FloatType>::vector4;
	using ScalarType = typename related_types<FloatType>::scalar;

	const Vector4Type zero = vector_zero();
	const Vector4Type infinity = vector_set(std::numeric_limits<FloatType>::infinity());
	const Vector4Type nan = vector_set(std::numeric_limits<FloatType>::quiet_NaN());

	const FloatType test_value0_flt[4] = { FloatType(2.0), FloatType(9.34), FloatType(-54.12), FloatType(6000.0) };
	const FloatType test_value1_flt[4] = { FloatType(0.75), FloatType(-4.52), FloatType(44.68), FloatType(-54225.0) };
	const FloatType test_value2_flt[4] = { FloatType(-2.65), FloatType(2.996113), FloatType(0.68123521), FloatType(-5.9182) };
	const Vector4Type test_value0 = vector_set(test_value0_flt[0], test_value0_flt[1], test_value0_flt[2], test_value0_flt[3]);
	const Vector4Type test_value1 = vector_set(test_value1_flt[0], test_value1_flt[1], test_value1_flt[2], test_value1_flt[3]);
	const Vector4Type test_value2 = vector_set(test_value2_flt[0], test_value2_flt[1], test_value2_flt[2], test_value2_flt[3]);

	//////////////////////////////////////////////////////////////////////////
	// Arithmetic

	CHECK(scalar_near_equal(vector_get_x(vector_add(test_value0, test_value1)), test_value0_flt[0] + test_value1_flt[0], threshold));
	CHECK(scalar_near_equal(vector_get_y(vector_add(test_value0, test_value1)), test_value0_flt[1] + test_value1_flt[1], threshold));
	CHECK(scalar_near_equal(vector_get_z(vector_add(test_value0, test_value1)), test_value0_flt[2] + test_value1_flt[2], threshold));
	CHECK(scalar_near_equal(vector_get_w(vector_add(test_value0, test_value1)), test_value0_flt[3] + test_value1_flt[3], threshold));

	CHECK(scalar_near_equal(vector_get_x(vector_sub(test_value0, test_value1)), test_value0_flt[0] - test_value1_flt[0], threshold));
	CHECK(scalar_near_equal(vector_get_y(vector_sub(test_value0, test_value1)), test_value0_flt[1] - test_value1_flt[1], threshold));
	CHECK(scalar_near_equal(vector_get_z(vector_sub(test_value0, test_value1)), test_value0_flt[2] - test_value1_flt[2], threshold));
	CHECK(scalar_near_equal(vector_get_w(vector_sub(test_value0, test_value1)), test_value0_flt[3] - test_value1_flt[3], threshold));

	CHECK(scalar_near_equal(vector_get_x(vector_mul(test_value0, test_value1)), test_value0_flt[0] * test_value1_flt[0], threshold));
	CHECK(scalar_near_equal(vector_get_y(vector_mul(test_value0, test_value1)), test_value0_flt[1] * test_value1_flt[1], threshold));
	CHECK(scalar_near_equal(vector_get_z(vector_mul(test_value0, test_value1)), test_value0_flt[2] * test_value1_flt[2], threshold));
	// We have a strange codegen bug with gcc5, use the Catch near equal impl instead
	CHECK(scalar_cast(vector_get_w_as_scalar(vector_mul(test_value0, test_value1))) == Approx(test_value0_flt[3] * test_value1_flt[3]).margin(threshold));

	CHECK(scalar_near_equal(vector_get_x(vector_mul(test_value0, FloatType(2.34))), test_value0_flt[0] * FloatType(2.34), threshold));
	CHECK(scalar_near_equal(vector_get_y(vector_mul(test_value0, FloatType(2.34))), test_value0_flt[1] * FloatType(2.34), threshold));
	CHECK(scalar_near_equal(vector_get_z(vector_mul(test_value0, FloatType(2.34))), test_value0_flt[2] * FloatType(2.34), threshold));
	CHECK(scalar_near_equal(vector_get_w(vector_mul(test_value0, FloatType(2.34))), test_value0_flt[3] * FloatType(2.34), threshold));

	CHECK(scalar_near_equal(vector_get_x(vector_mul(test_value0, scalar_set(FloatType(2.34)))), test_value0_flt[0] * FloatType(2.34), threshold));
	CHECK(scalar_near_equal(vector_get_y(vector_mul(test_value0, scalar_set(FloatType(2.34)))), test_value0_flt[1] * FloatType(2.34), threshold));
	CHECK(scalar_near_equal(vector_get_z(vector_mul(test_value0, scalar_set(FloatType(2.34)))), test_value0_flt[2] * FloatType(2.34), threshold));
	CHECK(scalar_near_equal(vector_get_w(vector_mul(test_value0, scalar_set(FloatType(2.34)))), test_value0_flt[3] * FloatType(2.34), threshold));

	CHECK(scalar_near_equal(vector_get_x(vector_div(test_value0, test_value1)), test_value0_flt[0] / test_value1_flt[0], threshold));
	CHECK(scalar_near_equal(vector_get_y(vector_div(test_value0, test_value1)), test_value0_flt[1] / test_value1_flt[1], threshold));
	CHECK(scalar_near_equal(vector_get_z(vector_div(test_value0, test_value1)), test_value0_flt[2] / test_value1_flt[2], threshold));
	CHECK(scalar_near_equal(vector_get_w(vector_div(test_value0, test_value1)), test_value0_flt[3] / test_value1_flt[3], threshold));

	CHECK(scalar_near_equal(vector_get_x(vector_max(test_value0, test_value1)), scalar_max(test_value0_flt[0], test_value1_flt[0]), threshold));
	CHECK(scalar_near_equal(vector_get_y(vector_max(test_value0, test_value1)), scalar_max(test_value0_flt[1], test_value1_flt[1]), threshold));
	CHECK(scalar_near_equal(vector_get_z(vector_max(test_value0, test_value1)), scalar_max(test_value0_flt[2], test_value1_flt[2]), threshold));
	CHECK(scalar_near_equal(vector_get_w(vector_max(test_value0, test_value1)), scalar_max(test_value0_flt[3], test_value1_flt[3]), threshold));

	CHECK(scalar_near_equal(vector_get_x(vector_min(test_value0, test_value1)), scalar_min(test_value0_flt[0], test_value1_flt[0]), threshold));
	CHECK(scalar_near_equal(vector_get_y(vector_min(test_value0, test_value1)), scalar_min(test_value0_flt[1], test_value1_flt[1]), threshold));
	CHECK(scalar_near_equal(vector_get_z(vector_min(test_value0, test_value1)), scalar_min(test_value0_flt[2], test_value1_flt[2]), threshold));
	CHECK(scalar_near_equal(vector_get_w(vector_min(test_value0, test_value1)), scalar_min(test_value0_flt[3], test_value1_flt[3]), threshold));

	CHECK(scalar_near_equal(vector_get_x(vector_clamp(test_value0, test_value1, test_value2)), scalar_clamp(test_value0_flt[0], test_value1_flt[0], test_value2_flt[0]), threshold));
	CHECK(scalar_near_equal(vector_get_y(vector_clamp(test_value0, test_value1, test_value2)), scalar_clamp(test_value0_flt[1], test_value1_flt[1], test_value2_flt[1]), threshold));
	CHECK(scalar_near_equal(vector_get_z(vector_clamp(test_value0, test_value1, test_value2)), scalar_clamp(test_value0_flt[2], test_value1_flt[2], test_value2_flt[2]), threshold));
	CHECK(scalar_near_equal(vector_get_w(vector_clamp(test_value0, test_value1, test_value2)), scalar_clamp(test_value0_flt[3], test_value1_flt[3], test_value2_flt[3]), threshold));

	CHECK(scalar_near_equal(vector_get_x(vector_abs(test_value0)), scalar_abs(test_value0_flt[0]), threshold));
	CHECK(scalar_near_equal(vector_get_y(vector_abs(test_value0)), scalar_abs(test_value0_flt[1]), threshold));
	CHECK(scalar_near_equal(vector_get_z(vector_abs(test_value0)), scalar_abs(test_value0_flt[2]), threshold));
	CHECK(scalar_near_equal(vector_get_w(vector_abs(test_value0)), scalar_abs(test_value0_flt[3]), threshold));

	CHECK(scalar_near_equal(vector_get_x(vector_neg(test_value0)), -test_value0_flt[0], threshold));
	CHECK(scalar_near_equal(vector_get_y(vector_neg(test_value0)), -test_value0_flt[1], threshold));
	CHECK(scalar_near_equal(vector_get_z(vector_neg(test_value0)), -test_value0_flt[2], threshold));
	CHECK(scalar_near_equal(vector_get_w(vector_neg(test_value0)), -test_value0_flt[3], threshold));

	CHECK(scalar_near_equal(vector_get_x(vector_neg<true, false, false, false>(test_value0)), -test_value0_flt[0], threshold));
	CHECK(scalar_near_equal(vector_get_y(vector_neg<true, false, false, false>(test_value0)), test_value0_flt[1], threshold));
	CHECK(scalar_near_equal(vector_get_z(vector_neg<true, false, false, false>(test_value0)), test_value0_flt[2], threshold));
	CHECK(scalar_near_equal(vector_get_w(vector_neg<true, false, false, false>(test_value0)), test_value0_flt[3], threshold));

	CHECK(scalar_near_equal(vector_get_x(vector_neg<false, true, false, false>(test_value0)), test_value0_flt[0], threshold));
	CHECK(scalar_near_equal(vector_get_y(vector_neg<false, true, false, false>(test_value0)), -test_value0_flt[1], threshold));
	CHECK(scalar_near_equal(vector_get_z(vector_neg<false, true, false, false>(test_value0)), test_value0_flt[2], threshold));
	CHECK(scalar_near_equal(vector_get_w(vector_neg<false, true, false, false>(test_value0)), test_value0_flt[3], threshold));

	CHECK(scalar_near_equal(vector_get_x(vector_neg<false, false, true, false>(test_value0)), test_value0_flt[0], threshold));
	CHECK(scalar_near_equal(vector_get_y(vector_neg<false, false, true, false>(test_value0)), test_value0_flt[1], threshold));
	CHECK(scalar_near_equal(vector_get_z(vector_neg<false, false, true, false>(test_value0)), -test_value0_flt[2], threshold));
	CHECK(scalar_near_equal(vector_get_w(vector_neg<false, false, true, false>(test_value0)), test_value0_flt[3], threshold));

	CHECK(scalar_near_equal(vector_get_x(vector_neg<false, false, false, true>(test_value0)), test_value0_flt[0], threshold));
	CHECK(scalar_near_equal(vector_get_y(vector_neg<false, false, false, true>(test_value0)), test_value0_flt[1], threshold));
	CHECK(scalar_near_equal(vector_get_z(vector_neg<false, false, false, true>(test_value0)), test_value0_flt[2], threshold));
	CHECK(scalar_near_equal(vector_get_w(vector_neg<false, false, false, true>(test_value0)), -test_value0_flt[3], threshold));

	CHECK(scalar_near_equal(vector_get_x(vector_neg<true, true, true, true>(test_value0)), -test_value0_flt[0], threshold));
	CHECK(scalar_near_equal(vector_get_y(vector_neg<true, true, true, true>(test_value0)), -test_value0_flt[1], threshold));
	CHECK(scalar_near_equal(vector_get_z(vector_neg<true, true, true, true>(test_value0)), -test_value0_flt[2], threshold));
	CHECK(scalar_near_equal(vector_get_w(vector_neg<true, true, true, true>(test_value0)), -test_value0_flt[3], threshold));

	CHECK(scalar_near_equal(vector_get_x(vector_reciprocal(test_value0)), scalar_reciprocal(test_value0_flt[0]), threshold));
	CHECK(scalar_near_equal(vector_get_y(vector_reciprocal(test_value0)), scalar_reciprocal(test_value0_flt[1]), threshold));
	CHECK(scalar_near_equal(vector_get_z(vector_reciprocal(test_value0)), scalar_reciprocal(test_value0_flt[2]), threshold));
	CHECK(scalar_near_equal(vector_get_w(vector_reciprocal(test_value0)), scalar_reciprocal(test_value0_flt[3]), threshold));

	CHECK(scalar_near_equal(vector_get_x(vector_sqrt(vector_abs(test_value0))), scalar_sqrt(scalar_abs(test_value0_flt[0])), threshold));
	CHECK(scalar_near_equal(vector_get_y(vector_sqrt(vector_abs(test_value0))), scalar_sqrt(scalar_abs(test_value0_flt[1])), threshold));
	CHECK(scalar_near_equal(vector_get_z(vector_sqrt(vector_abs(test_value0))), scalar_sqrt(scalar_abs(test_value0_flt[2])), threshold));
	CHECK(scalar_near_equal(vector_get_w(vector_sqrt(vector_abs(test_value0))), scalar_sqrt(scalar_abs(test_value0_flt[3])), threshold));

	CHECK(scalar_near_equal(vector_get_x(vector_sqrt_reciprocal(vector_abs(test_value0))), scalar_reciprocal(scalar_sqrt(scalar_abs(test_value0_flt[0]))), threshold));
	CHECK(scalar_near_equal(vector_get_y(vector_sqrt_reciprocal(vector_abs(test_value0))), scalar_reciprocal(scalar_sqrt(scalar_abs(test_value0_flt[1]))), threshold));
	CHECK(scalar_near_equal(vector_get_z(vector_sqrt_reciprocal(vector_abs(test_value0))), scalar_reciprocal(scalar_sqrt(scalar_abs(test_value0_flt[2]))), threshold));
	CHECK(scalar_near_equal(vector_get_w(vector_sqrt_reciprocal(vector_abs(test_value0))), scalar_reciprocal(scalar_sqrt(scalar_abs(test_value0_flt[3]))), threshold));

	const Vector4Type neg_zero = vector_set(FloatType(-0.0));
	CHECK(FloatType(vector_get_x(vector_floor(neg_zero))) == scalar_floor(FloatType(-0.0)));
	CHECK(FloatType(vector_get_x(vector_floor(test_value0))) == scalar_floor(test_value0_flt[0]));
	CHECK(FloatType(vector_get_y(vector_floor(test_value0))) == scalar_floor(test_value0_flt[1]));
	CHECK(FloatType(vector_get_z(vector_floor(test_value0))) == scalar_floor(test_value0_flt[2]));
	CHECK(FloatType(vector_get_w(vector_floor(test_value0))) == scalar_floor(test_value0_flt[3]));
	CHECK(FloatType(vector_get_x(vector_floor(infinity))) == scalar_floor(FloatType(vector_get_x(infinity))));
	CHECK(FloatType(vector_get_y(vector_floor(infinity))) == scalar_floor(FloatType(vector_get_y(infinity))));
	CHECK(FloatType(vector_get_z(vector_floor(infinity))) == scalar_floor(FloatType(vector_get_z(infinity))));
	CHECK(FloatType(vector_get_w(vector_floor(infinity))) == scalar_floor(FloatType(vector_get_w(infinity))));
	CHECK(FloatType(vector_get_x(vector_floor(vector_neg(infinity)))) == scalar_floor(-FloatType(vector_get_x(infinity))));
	CHECK(FloatType(vector_get_y(vector_floor(vector_neg(infinity)))) == scalar_floor(-FloatType(vector_get_y(infinity))));
	CHECK(FloatType(vector_get_z(vector_floor(vector_neg(infinity)))) == scalar_floor(-FloatType(vector_get_z(infinity))));
	CHECK(FloatType(vector_get_w(vector_floor(vector_neg(infinity)))) == scalar_floor(-FloatType(vector_get_w(infinity))));
	CHECK(std::isnan(FloatType(vector_get_x(vector_floor(nan)))));
	CHECK(std::isnan(FloatType(vector_get_y(vector_floor(nan)))));
	CHECK(std::isnan(FloatType(vector_get_z(vector_floor(nan)))));
	CHECK(std::isnan(FloatType(vector_get_w(vector_floor(nan)))));

	CHECK(FloatType(vector_get_x(vector_ceil(neg_zero))) == scalar_ceil(FloatType(-0.0)));
	CHECK(FloatType(vector_get_x(vector_ceil(test_value0)) == scalar_ceil(test_value0_flt[0])));
	CHECK(FloatType(vector_get_y(vector_ceil(test_value0)) == scalar_ceil(test_value0_flt[1])));
	CHECK(FloatType(vector_get_z(vector_ceil(test_value0)) == scalar_ceil(test_value0_flt[2])));
	CHECK(FloatType(vector_get_w(vector_ceil(test_value0)) == scalar_ceil(test_value0_flt[3])));
	CHECK(FloatType(vector_get_x(vector_ceil(infinity))) == scalar_ceil(FloatType(vector_get_x(infinity))));
	CHECK(FloatType(vector_get_y(vector_ceil(infinity))) == scalar_ceil(FloatType(vector_get_y(infinity))));
	CHECK(FloatType(vector_get_z(vector_ceil(infinity))) == scalar_ceil(FloatType(vector_get_z(infinity))));
	CHECK(FloatType(vector_get_w(vector_ceil(infinity))) == scalar_ceil(FloatType(vector_get_w(infinity))));
	CHECK(FloatType(vector_get_x(vector_ceil(vector_neg(infinity)))) == scalar_ceil(-FloatType(vector_get_x(infinity))));
	CHECK(FloatType(vector_get_y(vector_ceil(vector_neg(infinity)))) == scalar_ceil(-FloatType(vector_get_y(infinity))));
	CHECK(FloatType(vector_get_z(vector_ceil(vector_neg(infinity)))) == scalar_ceil(-FloatType(vector_get_z(infinity))));
	CHECK(FloatType(vector_get_w(vector_ceil(vector_neg(infinity)))) == scalar_ceil(-FloatType(vector_get_w(infinity))));
	CHECK(std::isnan(FloatType(vector_get_x(vector_ceil(nan)))));
	CHECK(std::isnan(FloatType(vector_get_y(vector_ceil(nan)))));
	CHECK(std::isnan(FloatType(vector_get_z(vector_ceil(nan)))));
	CHECK(std::isnan(FloatType(vector_get_w(vector_ceil(nan)))));

	const Vector4Type scalar_cross3_result = scalar_cross3<Vector4Type, FloatType>(test_value0, test_value1);
	const Vector4Type vector_cross3_result = vector_cross3(test_value0, test_value1);
	// We have a strange codegen bug with gcc, use the Catch near equal impl instead
	CHECK(FloatType(vector_get_x(vector_cross3_result)) == Approx(FloatType(vector_get_x(scalar_cross3_result))).margin(threshold));
	CHECK(scalar_near_equal(FloatType(vector_get_y(vector_cross3_result)), FloatType(vector_get_y(scalar_cross3_result)), threshold));
	CHECK(scalar_near_equal(FloatType(vector_get_z(vector_cross3_result)), FloatType(vector_get_z(scalar_cross3_result)), threshold));

	const FloatType test_value10_flt[4] = { FloatType(-0.001138), FloatType(0.91623), FloatType(-1.624598), FloatType(0.715671) };
	const FloatType test_value11_flt[4] = { FloatType(0.1138), FloatType(-0.623), FloatType(1.4598), FloatType(-0.5671) };
	const Vector4Type test_value10 = vector_set(test_value10_flt[0], test_value10_flt[1], test_value10_flt[2], test_value10_flt[3]);
	const Vector4Type test_value11 = vector_set(test_value11_flt[0], test_value11_flt[1], test_value11_flt[2], test_value11_flt[3]);
	const FloatType scalar_dot_result = scalar_dot4<Vector4Type, FloatType>(test_value10, test_value11);
	const FloatType vector_dot_result = vector_dot(test_value10, test_value11);
	CHECK(scalar_near_equal(vector_dot_result, scalar_dot_result, threshold));

	const FloatType scalar_dot3_result = scalar_dot3<Vector4Type, FloatType>(test_value10, test_value11);
	const FloatType vector_dot3_result = vector_dot3(test_value10, test_value11);
	CHECK(scalar_near_equal(vector_dot3_result, scalar_dot3_result, threshold));
	const ScalarType vector_dot3_result_scalar = vector_dot3_as_scalar(test_value10, test_value11);
	CHECK(scalar_equal(vector_dot3_result, scalar_cast(vector_dot3_result_scalar)));
	const Vector4Type vector_dot3_result_vec = vector_dot3_as_vector(test_value10, test_value11);
	CHECK(scalar_equal(vector_dot3_result, (FloatType)vector_get_x(vector_dot3_result_vec)));
	CHECK(scalar_equal(vector_dot3_result, (FloatType)vector_get_y(vector_dot3_result_vec)));
	CHECK(scalar_equal(vector_dot3_result, (FloatType)vector_get_z(vector_dot3_result_vec)));
	CHECK(scalar_equal(vector_dot3_result, (FloatType)vector_get_w(vector_dot3_result_vec)));

	const FloatType scalar_dot2_result = scalar_dot2<Vector4Type, FloatType>(test_value10, test_value11);
	const FloatType vector_dot2_result = vector_dot2(test_value10, test_value11);
	CHECK(scalar_near_equal(vector_dot2_result, scalar_dot2_result, threshold));
	const ScalarType vector_dot2_result_scalar = vector_dot2_as_scalar(test_value10, test_value11);
	CHECK(scalar_equal(vector_dot2_result, scalar_cast(vector_dot2_result_scalar)));
	const Vector4Type vector_dot2_result_vec = vector_dot2_as_vector(test_value10, test_value11);
	CHECK(scalar_equal(vector_dot2_result, (FloatType)vector_get_x(vector_dot2_result_vec)));
	CHECK(scalar_equal(vector_dot2_result, (FloatType)vector_get_y(vector_dot2_result_vec)));
	CHECK(scalar_equal(vector_dot2_result, (FloatType)vector_get_z(vector_dot2_result_vec)));
	CHECK(scalar_equal(vector_dot2_result, (FloatType)vector_get_w(vector_dot2_result_vec)));

	const ScalarType vector_sdot_result = vector_dot_as_scalar(test_value10, test_value11);
	CHECK(scalar_near_equal(scalar_cast(vector_sdot_result), scalar_dot_result, threshold));

	const Vector4Type vector_vdot_result = vector_dot_as_vector(test_value10, test_value11);
	CHECK(scalar_near_equal((FloatType)vector_get_x(vector_vdot_result), scalar_dot_result, threshold));
	CHECK(scalar_near_equal((FloatType)vector_get_y(vector_vdot_result), scalar_dot_result, threshold));
	CHECK(scalar_near_equal((FloatType)vector_get_z(vector_vdot_result), scalar_dot_result, threshold));
	CHECK(scalar_near_equal((FloatType)vector_get_w(vector_vdot_result), scalar_dot_result, threshold));

	const FloatType length_squared_threshold = FloatType(30.0);	// Input W has large values
	CHECK(scalar_near_equal(scalar_dot4<Vector4Type, FloatType>(test_value0, test_value0), vector_length_squared(test_value0), length_squared_threshold));
	CHECK(scalar_near_equal(scalar_dot4<Vector4Type, FloatType>(test_value0, test_value0), scalar_cast(vector_length_squared_as_scalar(test_value0)), length_squared_threshold));
	const FloatType vector_length_squared3_ref = scalar_dot3<Vector4Type, FloatType>(test_value0, test_value0);
	const FloatType vector_length_squared3_result = vector_length_squared3(test_value0);
	CHECK(scalar_near_equal(vector_length_squared3_ref, vector_length_squared3_result, threshold));
	const ScalarType vector_length_squared3_result_scalar = vector_length_squared3_as_scalar(test_value0);
	CHECK(scalar_equal(vector_length_squared3_result, scalar_cast(vector_length_squared3_result_scalar)));

	CHECK(scalar_near_equal(rtm::scalar_sqrt(scalar_dot4<Vector4Type, FloatType>(test_value0, test_value0)), vector_length(test_value0), threshold));
	CHECK(scalar_near_equal(rtm::scalar_sqrt(scalar_dot4<Vector4Type, FloatType>(test_value0, test_value0)), scalar_cast(vector_length_as_scalar(test_value0)), threshold));
	const FloatType vector_length3_result = vector_length3(test_value0);
	CHECK(scalar_near_equal(rtm::scalar_sqrt(scalar_dot3<Vector4Type, FloatType>(test_value0, test_value0)), vector_length3_result, threshold));
	const ScalarType vector_length3_result_scalar = vector_length3_as_scalar(test_value0);
	CHECK(scalar_equal(vector_length3_result, scalar_cast(vector_length3_result_scalar)));

	CHECK(scalar_near_equal(rtm::scalar_sqrt_reciprocal(scalar_dot4<Vector4Type, FloatType>(test_value0, test_value0)), vector_length_reciprocal(test_value0), threshold));
	CHECK(scalar_near_equal(rtm::scalar_sqrt_reciprocal(scalar_dot4<Vector4Type, FloatType>(test_value0, test_value0)), scalar_cast(vector_length_reciprocal_as_scalar(test_value0)), threshold));
	CHECK(scalar_near_equal(rtm::scalar_sqrt_reciprocal(scalar_dot3<Vector4Type, FloatType>(test_value0, test_value0)), vector_length_reciprocal3(test_value0), threshold));
	CHECK(scalar_near_equal(rtm::scalar_sqrt_reciprocal(scalar_dot3<Vector4Type, FloatType>(test_value0, test_value0)), scalar_cast(vector_length_reciprocal3_as_scalar(test_value0)), threshold));
	CHECK(scalar_near_equal(rtm::scalar_sqrt_reciprocal(scalar_dot2<Vector4Type, FloatType>(test_value0, test_value0)), vector_length_reciprocal2(test_value0), threshold));
	CHECK(scalar_near_equal(rtm::scalar_sqrt_reciprocal(scalar_dot2<Vector4Type, FloatType>(test_value0, test_value0)), scalar_cast(vector_length_reciprocal2_as_scalar(test_value0)), threshold));

	const Vector4Type test_value_diff = vector_sub(test_value0, test_value1);
	const FloatType vector_distance3_result = vector_distance3(test_value0, test_value1);
	CHECK(scalar_near_equal(rtm::scalar_sqrt(scalar_dot3<Vector4Type, FloatType>(test_value_diff, test_value_diff)), vector_distance3_result, threshold));
	const ScalarType vector_distance3_result_scalar = vector_distance3_as_scalar(test_value0, test_value1);
	CHECK(scalar_equal(vector_distance3_result, scalar_cast(vector_distance3_result_scalar)));

	const FloatType vector_distance_squared3_result = vector_distance_squared3(test_value0, test_value1);
	CHECK(scalar_near_equal(scalar_dot3<Vector4Type, FloatType>(test_value_diff, test_value_diff), vector_distance_squared3_result, scalar_sqrt(threshold)));
	const ScalarType vector_distance_squared3_result_scalar = vector_distance_squared3_as_scalar(test_value0, test_value1);
	CHECK(scalar_equal(vector_distance_squared3_result, scalar_cast(vector_distance_squared3_result_scalar)));

	const Vector4Type scalar_normalize2_result = scalar_normalize2<Vector4Type, FloatType>(test_value0, zero, threshold);
	const Vector4Type vector_normalize2_result = vector_normalize2(test_value0);
	CHECK(scalar_near_equal(vector_get_x(vector_normalize2_result), vector_get_x(scalar_normalize2_result), threshold));
	CHECK(scalar_near_equal(vector_get_y(vector_normalize2_result), vector_get_y(scalar_normalize2_result), threshold));

	const Vector4Type vector_normalize2_result_safe = vector_normalize2(test_value0, zero, threshold);
	CHECK(scalar_near_equal(vector_get_x(vector_normalize2_result_safe), vector_get_x(scalar_normalize2_result), threshold));
	CHECK(scalar_near_equal(vector_get_y(vector_normalize2_result_safe), vector_get_y(scalar_normalize2_result), threshold));

	const Vector4Type scalar_normalize2_result0 = scalar_normalize2<Vector4Type, FloatType>(zero, zero, threshold);
	const Vector4Type vector_normalize2_result0 = vector_normalize2(zero, zero, threshold);
	CHECK(scalar_near_equal(vector_get_x(vector_normalize2_result0), vector_get_x(scalar_normalize2_result0), threshold));
	CHECK(scalar_near_equal(vector_get_y(vector_normalize2_result0), vector_get_y(scalar_normalize2_result0), threshold));

	const Vector4Type scalar_normalize3_result = scalar_normalize3<Vector4Type, FloatType>(test_value0, zero, threshold);
	const Vector4Type vector_normalize3_result = vector_normalize3(test_value0);
	CHECK(scalar_near_equal(vector_get_x(vector_normalize3_result), vector_get_x(scalar_normalize3_result), threshold));
	CHECK(scalar_near_equal(vector_get_y(vector_normalize3_result), vector_get_y(scalar_normalize3_result), threshold));
	CHECK(scalar_near_equal(vector_get_z(vector_normalize3_result), vector_get_z(scalar_normalize3_result), threshold));

	const Vector4Type vector_normalize3_result_safe = vector_normalize3(test_value0, zero, threshold);
	CHECK(scalar_near_equal(vector_get_x(vector_normalize3_result_safe), vector_get_x(scalar_normalize3_result), threshold));
	CHECK(scalar_near_equal(vector_get_y(vector_normalize3_result_safe), vector_get_y(scalar_normalize3_result), threshold));
	CHECK(scalar_near_equal(vector_get_z(vector_normalize3_result_safe), vector_get_z(scalar_normalize3_result), threshold));

	const Vector4Type scalar_normalize3_result0 = scalar_normalize3<Vector4Type, FloatType>(zero, zero, threshold);
	const Vector4Type vector_normalize3_result0 = vector_normalize3(zero, zero, threshold);
	CHECK(scalar_near_equal(vector_get_x(vector_normalize3_result0), vector_get_x(scalar_normalize3_result0), threshold));
	CHECK(scalar_near_equal(vector_get_y(vector_normalize3_result0), vector_get_y(scalar_normalize3_result0), threshold));
	CHECK(scalar_near_equal(vector_get_z(vector_normalize3_result0), vector_get_z(scalar_normalize3_result0), threshold));

	const Vector4Type scalar_normalize4_result = scalar_normalize4<Vector4Type, FloatType>(test_value0, zero, threshold);
	const Vector4Type vector_normalize4_result = vector_normalize(test_value0);
	CHECK(scalar_near_equal(vector_get_x(vector_normalize4_result), vector_get_x(scalar_normalize4_result), threshold));
	CHECK(scalar_near_equal(vector_get_y(vector_normalize4_result), vector_get_y(scalar_normalize4_result), threshold));
	CHECK(scalar_near_equal(vector_get_z(vector_normalize4_result), vector_get_z(scalar_normalize4_result), threshold));
	CHECK(scalar_near_equal(vector_get_w(vector_normalize4_result), vector_get_w(scalar_normalize4_result), threshold));

	const Vector4Type vector_normalize4_result_safe = vector_normalize(test_value0, zero, threshold);
	CHECK(scalar_near_equal(vector_get_x(vector_normalize4_result_safe), vector_get_x(scalar_normalize4_result), threshold));
	CHECK(scalar_near_equal(vector_get_y(vector_normalize4_result_safe), vector_get_y(scalar_normalize4_result), threshold));
	CHECK(scalar_near_equal(vector_get_z(vector_normalize4_result_safe), vector_get_z(scalar_normalize4_result), threshold));
	CHECK(scalar_near_equal(vector_get_w(vector_normalize4_result_safe), vector_get_w(scalar_normalize4_result), threshold));

	const Vector4Type scalar_normalize4_result0 = scalar_normalize4<Vector4Type, FloatType>(zero, zero, threshold);
	const Vector4Type vector_normalize4_result0 = vector_normalize(zero, zero, threshold);
	CHECK(scalar_near_equal(vector_get_x(vector_normalize4_result0), vector_get_x(scalar_normalize4_result0), threshold));
	CHECK(scalar_near_equal(vector_get_y(vector_normalize4_result0), vector_get_y(scalar_normalize4_result0), threshold));
	CHECK(scalar_near_equal(vector_get_z(vector_normalize4_result0), vector_get_z(scalar_normalize4_result0), threshold));
	CHECK(scalar_near_equal(vector_get_w(vector_normalize4_result0), vector_get_w(scalar_normalize4_result0), threshold));

	CHECK(scalar_near_equal(vector_get_x(vector_lerp(test_value10, test_value11, FloatType(0.33))), ((test_value11_flt[0] - test_value10_flt[0]) * FloatType(0.33)) + test_value10_flt[0], threshold));
	CHECK(scalar_near_equal(vector_get_y(vector_lerp(test_value10, test_value11, FloatType(0.33))), ((test_value11_flt[1] - test_value10_flt[1]) * FloatType(0.33)) + test_value10_flt[1], threshold));
	CHECK(scalar_near_equal(vector_get_z(vector_lerp(test_value10, test_value11, FloatType(0.33))), ((test_value11_flt[2] - test_value10_flt[2]) * FloatType(0.33)) + test_value10_flt[2], threshold));
	CHECK(scalar_near_equal(vector_get_w(vector_lerp(test_value10, test_value11, FloatType(0.33))), ((test_value11_flt[3] - test_value10_flt[3]) * FloatType(0.33)) + test_value10_flt[3], threshold));

	CHECK(scalar_near_equal(vector_get_x(vector_lerp(test_value10, test_value11, scalar_set(FloatType(0.33)))), ((test_value11_flt[0] - test_value10_flt[0]) * FloatType(0.33)) + test_value10_flt[0], threshold));
	CHECK(scalar_near_equal(vector_get_y(vector_lerp(test_value10, test_value11, scalar_set(FloatType(0.33)))), ((test_value11_flt[1] - test_value10_flt[1]) * FloatType(0.33)) + test_value10_flt[1], threshold));
	CHECK(scalar_near_equal(vector_get_z(vector_lerp(test_value10, test_value11, scalar_set(FloatType(0.33)))), ((test_value11_flt[2] - test_value10_flt[2]) * FloatType(0.33)) + test_value10_flt[2], threshold));
	CHECK(scalar_near_equal(vector_get_w(vector_lerp(test_value10, test_value11, scalar_set(FloatType(0.33)))), ((test_value11_flt[3] - test_value10_flt[3]) * FloatType(0.33)) + test_value10_flt[3], threshold));

	CHECK(scalar_near_equal(vector_get_x(vector_lerp(test_value10, test_value11, vector_set(FloatType(0.33)))), ((test_value11_flt[0] - test_value10_flt[0]) * FloatType(0.33)) + test_value10_flt[0], threshold));
	CHECK(scalar_near_equal(vector_get_y(vector_lerp(test_value10, test_value11, vector_set(FloatType(0.33)))), ((test_value11_flt[1] - test_value10_flt[1]) * FloatType(0.33)) + test_value10_flt[1], threshold));
	CHECK(scalar_near_equal(vector_get_z(vector_lerp(test_value10, test_value11, vector_set(FloatType(0.33)))), ((test_value11_flt[2] - test_value10_flt[2]) * FloatType(0.33)) + test_value10_flt[2], threshold));
	CHECK(scalar_near_equal(vector_get_w(vector_lerp(test_value10, test_value11, vector_set(FloatType(0.33)))), ((test_value11_flt[3] - test_value10_flt[3]) * FloatType(0.33)) + test_value10_flt[3], threshold));

	// Lerp must be stable and return exactly the start when the interpolation alpha is 0.0 and exactly the end when 1.0
	CHECK(vector_all_near_equal(vector_lerp(test_value10, test_value11, FloatType(0.0)), test_value10, FloatType(0.0)));
	CHECK(vector_all_near_equal(vector_lerp(test_value10, test_value11, FloatType(1.0)), test_value11, FloatType(0.0)));
	CHECK(vector_all_near_equal(vector_lerp(test_value10, test_value11, scalar_set(FloatType(0.0))), test_value10, FloatType(0.0)));
	CHECK(vector_all_near_equal(vector_lerp(test_value10, test_value11, scalar_set(FloatType(1.0))), test_value11, FloatType(0.0)));
	CHECK(vector_all_near_equal(vector_lerp(test_value10, test_value11, vector_set(FloatType(0.0))), test_value10, FloatType(0.0)));
	CHECK(vector_all_near_equal(vector_lerp(test_value10, test_value11, vector_set(FloatType(1.0))), test_value11, FloatType(0.0)));

	CHECK(FloatType(vector_get_x(vector_fraction(neg_zero))) == scalar_fraction(FloatType(-0.0)));
	CHECK(scalar_near_equal(vector_get_x(vector_fraction(test_value0)), scalar_fraction(test_value0_flt[0]), threshold));
	CHECK(scalar_near_equal(vector_get_y(vector_fraction(test_value0)), scalar_fraction(test_value0_flt[1]), threshold));
	CHECK(scalar_near_equal(vector_get_z(vector_fraction(test_value0)), scalar_fraction(test_value0_flt[2]), threshold));
	CHECK(scalar_near_equal(vector_get_w(vector_fraction(test_value0)), scalar_fraction(test_value0_flt[3]), threshold));

	CHECK(scalar_near_equal(vector_get_x(vector_mul_add(test_value10, test_value11, test_value2)), (test_value10_flt[0] * test_value11_flt[0]) + test_value2_flt[0], threshold));
	CHECK(scalar_near_equal(vector_get_y(vector_mul_add(test_value10, test_value11, test_value2)), (test_value10_flt[1] * test_value11_flt[1]) + test_value2_flt[1], threshold));
	CHECK(scalar_near_equal(vector_get_z(vector_mul_add(test_value10, test_value11, test_value2)), (test_value10_flt[2] * test_value11_flt[2]) + test_value2_flt[2], threshold));
	CHECK(scalar_near_equal(vector_get_w(vector_mul_add(test_value10, test_value11, test_value2)), (test_value10_flt[3] * test_value11_flt[3]) + test_value2_flt[3], threshold));

	CHECK(scalar_near_equal(vector_get_x(vector_mul_add(test_value10, test_value11_flt[0], test_value2)), (test_value10_flt[0] * test_value11_flt[0]) + test_value2_flt[0], threshold));
	CHECK(scalar_near_equal(vector_get_y(vector_mul_add(test_value10, test_value11_flt[0], test_value2)), (test_value10_flt[1] * test_value11_flt[0]) + test_value2_flt[1], threshold));
	CHECK(scalar_near_equal(vector_get_z(vector_mul_add(test_value10, test_value11_flt[0], test_value2)), (test_value10_flt[2] * test_value11_flt[0]) + test_value2_flt[2], threshold));
	CHECK(scalar_near_equal(vector_get_w(vector_mul_add(test_value10, test_value11_flt[0], test_value2)), (test_value10_flt[3] * test_value11_flt[0]) + test_value2_flt[3], threshold));

	CHECK(scalar_near_equal(vector_get_x(vector_mul_add(test_value10, scalar_set(test_value11_flt[0]), test_value2)), (test_value10_flt[0] * test_value11_flt[0]) + test_value2_flt[0], threshold));
	CHECK(scalar_near_equal(vector_get_y(vector_mul_add(test_value10, scalar_set(test_value11_flt[0]), test_value2)), (test_value10_flt[1] * test_value11_flt[0]) + test_value2_flt[1], threshold));
	CHECK(scalar_near_equal(vector_get_z(vector_mul_add(test_value10, scalar_set(test_value11_flt[0]), test_value2)), (test_value10_flt[2] * test_value11_flt[0]) + test_value2_flt[2], threshold));
	CHECK(scalar_near_equal(vector_get_w(vector_mul_add(test_value10, scalar_set(test_value11_flt[0]), test_value2)), (test_value10_flt[3] * test_value11_flt[0]) + test_value2_flt[3], threshold));

	CHECK(scalar_near_equal(vector_get_x(vector_neg_mul_sub(test_value10, test_value11, test_value2)), (test_value10_flt[0] * -test_value11_flt[0]) + test_value2_flt[0], threshold));
	CHECK(scalar_near_equal(vector_get_y(vector_neg_mul_sub(test_value10, test_value11, test_value2)), (test_value10_flt[1] * -test_value11_flt[1]) + test_value2_flt[1], threshold));
	CHECK(scalar_near_equal(vector_get_z(vector_neg_mul_sub(test_value10, test_value11, test_value2)), (test_value10_flt[2] * -test_value11_flt[2]) + test_value2_flt[2], threshold));
	CHECK(scalar_near_equal(vector_get_w(vector_neg_mul_sub(test_value10, test_value11, test_value2)), (test_value10_flt[3] * -test_value11_flt[3]) + test_value2_flt[3], threshold));

	CHECK(scalar_near_equal(vector_get_x(vector_neg_mul_sub(test_value10, test_value11_flt[0], test_value2)), (test_value10_flt[0] * -test_value11_flt[0]) + test_value2_flt[0], threshold));
	CHECK(scalar_near_equal(vector_get_y(vector_neg_mul_sub(test_value10, test_value11_flt[0], test_value2)), (test_value10_flt[1] * -test_value11_flt[0]) + test_value2_flt[1], threshold));
	CHECK(scalar_near_equal(vector_get_z(vector_neg_mul_sub(test_value10, test_value11_flt[0], test_value2)), (test_value10_flt[2] * -test_value11_flt[0]) + test_value2_flt[2], threshold));
	CHECK(scalar_near_equal(vector_get_w(vector_neg_mul_sub(test_value10, test_value11_flt[0], test_value2)), (test_value10_flt[3] * -test_value11_flt[0]) + test_value2_flt[3], threshold));

	CHECK(scalar_near_equal(vector_get_x(vector_neg_mul_sub(test_value10, scalar_set(test_value11_flt[0]), test_value2)), (test_value10_flt[0] * -test_value11_flt[0]) + test_value2_flt[0], threshold));
	CHECK(scalar_near_equal(vector_get_y(vector_neg_mul_sub(test_value10, scalar_set(test_value11_flt[0]), test_value2)), (test_value10_flt[1] * -test_value11_flt[0]) + test_value2_flt[1], threshold));
	CHECK(scalar_near_equal(vector_get_z(vector_neg_mul_sub(test_value10, scalar_set(test_value11_flt[0]), test_value2)), (test_value10_flt[2] * -test_value11_flt[0]) + test_value2_flt[2], threshold));
	CHECK(scalar_near_equal(vector_get_w(vector_neg_mul_sub(test_value10, scalar_set(test_value11_flt[0]), test_value2)), (test_value10_flt[3] * -test_value11_flt[0]) + test_value2_flt[3], threshold));
}

template<typename FloatType>
void test_vector4_relational_impl(const FloatType threshold)
{
	using Vector4Type = typename related_types<FloatType>::vector4;

	const Vector4Type zero = vector_zero();

	const FloatType test_value0_flt[4] = { FloatType(2.0), FloatType(9.34), FloatType(-54.12), FloatType(6000.0) };
	const FloatType test_value1_flt[4] = { FloatType(0.75), FloatType(-4.52), FloatType(44.68), FloatType(-54225.0) };
	const FloatType test_value3_flt[4] = { FloatType(2.0), FloatType(-9.34), FloatType(54.12), FloatType(6000.1) };
	const Vector4Type test_value0 = vector_set(test_value0_flt[0], test_value0_flt[1], test_value0_flt[2], test_value0_flt[3]);
	const Vector4Type test_value1 = vector_set(test_value1_flt[0], test_value1_flt[1], test_value1_flt[2], test_value1_flt[3]);
	const Vector4Type test_value3 = vector_set(test_value3_flt[0], test_value3_flt[1], test_value3_flt[2], test_value3_flt[3]);

	//////////////////////////////////////////////////////////////////////////
	// Comparisons and masking

	CHECK((mask_get_x(vector_equal(test_value0, test_value1)) != 0) == (test_value0_flt[0] == test_value1_flt[0]));
	CHECK((mask_get_y(vector_equal(test_value0, test_value1)) != 0) == (test_value0_flt[1] == test_value1_flt[1]));
	CHECK((mask_get_z(vector_equal(test_value0, test_value1)) != 0) == (test_value0_flt[2] == test_value1_flt[2]));
	CHECK((mask_get_w(vector_equal(test_value0, test_value1)) != 0) == (test_value0_flt[3] == test_value1_flt[3]));

	CHECK((mask_get_x(vector_equal(test_value0, test_value0)) != 0) == (test_value0_flt[0] == test_value0_flt[0]));
	CHECK((mask_get_y(vector_equal(test_value0, test_value0)) != 0) == (test_value0_flt[1] == test_value0_flt[1]));
	CHECK((mask_get_z(vector_equal(test_value0, test_value0)) != 0) == (test_value0_flt[2] == test_value0_flt[2]));
	CHECK((mask_get_w(vector_equal(test_value0, test_value0)) != 0) == (test_value0_flt[3] == test_value0_flt[3]));

	CHECK((mask_get_x(vector_not_equal(test_value0, test_value1)) != 0) != (test_value0_flt[0] == test_value1_flt[0]));
	CHECK((mask_get_y(vector_not_equal(test_value0, test_value1)) != 0) != (test_value0_flt[1] == test_value1_flt[1]));
	CHECK((mask_get_z(vector_not_equal(test_value0, test_value1)) != 0) != (test_value0_flt[2] == test_value1_flt[2]));
	CHECK((mask_get_w(vector_not_equal(test_value0, test_value1)) != 0) != (test_value0_flt[3] == test_value1_flt[3]));

	CHECK((mask_get_x(vector_not_equal(test_value0, test_value0)) != 0) != (test_value0_flt[0] == test_value0_flt[0]));
	CHECK((mask_get_y(vector_not_equal(test_value0, test_value0)) != 0) != (test_value0_flt[1] == test_value0_flt[1]));
	CHECK((mask_get_z(vector_not_equal(test_value0, test_value0)) != 0) != (test_value0_flt[2] == test_value0_flt[2]));
	CHECK((mask_get_w(vector_not_equal(test_value0, test_value0)) != 0) != (test_value0_flt[3] == test_value0_flt[3]));

	CHECK((mask_get_x(vector_less_than(test_value0, test_value1)) != 0) == (test_value0_flt[0] < test_value1_flt[0]));
	CHECK((mask_get_y(vector_less_than(test_value0, test_value1)) != 0) == (test_value0_flt[1] < test_value1_flt[1]));
	CHECK((mask_get_z(vector_less_than(test_value0, test_value1)) != 0) == (test_value0_flt[2] < test_value1_flt[2]));
	CHECK((mask_get_w(vector_less_than(test_value0, test_value1)) != 0) == (test_value0_flt[3] < test_value1_flt[3]));

	CHECK((mask_get_x(vector_less_equal(test_value0, test_value3)) != 0) == (test_value0_flt[0] <= test_value3_flt[0]));
	CHECK((mask_get_y(vector_less_equal(test_value0, test_value3)) != 0) == (test_value0_flt[1] <= test_value3_flt[1]));
	CHECK((mask_get_z(vector_less_equal(test_value0, test_value3)) != 0) == (test_value0_flt[2] <= test_value3_flt[2]));
	CHECK((mask_get_w(vector_less_equal(test_value0, test_value3)) != 0) == (test_value0_flt[3] <= test_value3_flt[3]));

	CHECK((mask_get_x(vector_greater_than(test_value0, test_value1)) != 0) == (test_value0_flt[0] > test_value1_flt[0]));
	CHECK((mask_get_y(vector_greater_than(test_value0, test_value1)) != 0) == (test_value0_flt[1] > test_value1_flt[1]));
	CHECK((mask_get_z(vector_greater_than(test_value0, test_value1)) != 0) == (test_value0_flt[2] > test_value1_flt[2]));
	CHECK((mask_get_w(vector_greater_than(test_value0, test_value1)) != 0) == (test_value0_flt[3] > test_value1_flt[3]));

	CHECK((mask_get_x(vector_greater_equal(test_value0, test_value1)) != 0) == (test_value0_flt[0] >= test_value1_flt[0]));
	CHECK((mask_get_y(vector_greater_equal(test_value0, test_value1)) != 0) == (test_value0_flt[1] >= test_value1_flt[1]));
	CHECK((mask_get_z(vector_greater_equal(test_value0, test_value1)) != 0) == (test_value0_flt[2] >= test_value1_flt[2]));
	CHECK((mask_get_w(vector_greater_equal(test_value0, test_value1)) != 0) == (test_value0_flt[3] >= test_value1_flt[3]));

	CHECK(vector_all_less_than(zero, vector_set(FloatType(1.0), FloatType(1.0), FloatType(1.0), FloatType(1.0))) == true);
	CHECK(vector_all_less_than(zero, vector_set(FloatType(1.0), FloatType(0.0), FloatType(0.0), FloatType(0.0))) == false);
	CHECK(vector_all_less_than(zero, vector_set(FloatType(0.0), FloatType(1.0), FloatType(0.0), FloatType(0.0))) == false);
	CHECK(vector_all_less_than(zero, vector_set(FloatType(0.0), FloatType(0.0), FloatType(1.0), FloatType(0.0))) == false);
	CHECK(vector_all_less_than(zero, vector_set(FloatType(0.0), FloatType(0.0), FloatType(0.0), FloatType(1.0))) == false);
	CHECK(vector_all_less_than(zero, zero) == false);

	CHECK(vector_all_less_than2(zero, vector_set(FloatType(1.0), FloatType(1.0), FloatType(1.0), FloatType(0.0))) == true);
	CHECK(vector_all_less_than2(zero, vector_set(FloatType(1.0), FloatType(0.0), FloatType(0.0), FloatType(0.0))) == false);
	CHECK(vector_all_less_than2(zero, vector_set(FloatType(0.0), FloatType(1.0), FloatType(0.0), FloatType(0.0))) == false);
	CHECK(vector_all_less_than2(zero, vector_set(FloatType(0.0), FloatType(0.0), FloatType(1.0), FloatType(0.0))) == false);
	CHECK(vector_all_less_than2(zero, zero) == false);

	CHECK(vector_all_less_than3(zero, vector_set(FloatType(1.0), FloatType(1.0), FloatType(1.0), FloatType(0.0))) == true);
	CHECK(vector_all_less_than3(zero, vector_set(FloatType(1.0), FloatType(0.0), FloatType(0.0), FloatType(0.0))) == false);
	CHECK(vector_all_less_than3(zero, vector_set(FloatType(0.0), FloatType(1.0), FloatType(0.0), FloatType(0.0))) == false);
	CHECK(vector_all_less_than3(zero, vector_set(FloatType(0.0), FloatType(0.0), FloatType(1.0), FloatType(0.0))) == false);
	CHECK(vector_all_less_than3(zero, zero) == false);

	CHECK(vector_any_less_than(zero, vector_set(FloatType(1.0), FloatType(1.0), FloatType(1.0), FloatType(1.0))) == true);
	CHECK(vector_any_less_than(zero, vector_set(FloatType(1.0), FloatType(0.0), FloatType(0.0), FloatType(0.0))) == true);
	CHECK(vector_any_less_than(zero, vector_set(FloatType(0.0), FloatType(1.0), FloatType(0.0), FloatType(0.0))) == true);
	CHECK(vector_any_less_than(zero, vector_set(FloatType(0.0), FloatType(0.0), FloatType(1.0), FloatType(0.0))) == true);
	CHECK(vector_any_less_than(zero, vector_set(FloatType(0.0), FloatType(0.0), FloatType(0.0), FloatType(1.0))) == true);
	CHECK(vector_any_less_than(zero, zero) == false);

	CHECK(vector_any_less_than2(zero, vector_set(FloatType(1.0), FloatType(1.0), FloatType(1.0), FloatType(0.0))) == true);
	CHECK(vector_any_less_than2(zero, vector_set(FloatType(1.0), FloatType(0.0), FloatType(0.0), FloatType(0.0))) == true);
	CHECK(vector_any_less_than2(zero, vector_set(FloatType(0.0), FloatType(1.0), FloatType(0.0), FloatType(0.0))) == true);
	CHECK(vector_any_less_than2(zero, vector_set(FloatType(0.0), FloatType(0.0), FloatType(1.0), FloatType(0.0))) == false);
	CHECK(vector_any_less_than2(zero, zero) == false);

	CHECK(vector_any_less_than3(zero, vector_set(FloatType(1.0), FloatType(1.0), FloatType(1.0), FloatType(0.0))) == true);
	CHECK(vector_any_less_than3(zero, vector_set(FloatType(1.0), FloatType(0.0), FloatType(0.0), FloatType(0.0))) == true);
	CHECK(vector_any_less_than3(zero, vector_set(FloatType(0.0), FloatType(1.0), FloatType(0.0), FloatType(0.0))) == true);
	CHECK(vector_any_less_than3(zero, vector_set(FloatType(0.0), FloatType(0.0), FloatType(1.0), FloatType(0.0))) == true);
	CHECK(vector_any_less_than3(zero, zero) == false);

	CHECK(vector_all_less_equal(zero, vector_set(FloatType(1.0), FloatType(1.0), FloatType(1.0), FloatType(1.0))) == true);
	CHECK(vector_all_less_equal(zero, vector_set(FloatType(1.0), FloatType(0.0), FloatType(0.0), FloatType(0.0))) == true);
	CHECK(vector_all_less_equal(zero, vector_set(FloatType(0.0), FloatType(1.0), FloatType(0.0), FloatType(0.0))) == true);
	CHECK(vector_all_less_equal(zero, vector_set(FloatType(0.0), FloatType(0.0), FloatType(1.0), FloatType(0.0))) == true);
	CHECK(vector_all_less_equal(zero, vector_set(FloatType(0.0), FloatType(0.0), FloatType(0.0), FloatType(1.0))) == true);
	CHECK(vector_all_less_equal(zero, vector_set(FloatType(-1.0), FloatType(0.0), FloatType(0.0), FloatType(0.0))) == false);
	CHECK(vector_all_less_equal(zero, vector_set(FloatType(0.0), FloatType(-1.0), FloatType(0.0), FloatType(0.0))) == false);
	CHECK(vector_all_less_equal(zero, vector_set(FloatType(0.0), FloatType(0.0), FloatType(-1.0), FloatType(0.0))) == false);
	CHECK(vector_all_less_equal(zero, vector_set(FloatType(0.0), FloatType(0.0), FloatType(0.0), FloatType(-1.0))) == false);
	CHECK(vector_all_less_equal(zero, zero) == true);

	CHECK(vector_all_less_equal2(zero, vector_set(FloatType(1.0), FloatType(1.0), FloatType(1.0), FloatType(0.0))) == true);
	CHECK(vector_all_less_equal2(zero, vector_set(FloatType(1.0), FloatType(0.0), FloatType(0.0), FloatType(0.0))) == true);
	CHECK(vector_all_less_equal2(zero, vector_set(FloatType(0.0), FloatType(1.0), FloatType(0.0), FloatType(0.0))) == true);
	CHECK(vector_all_less_equal2(zero, vector_set(FloatType(0.0), FloatType(0.0), FloatType(1.0), FloatType(0.0))) == true);
	CHECK(vector_all_less_equal2(zero, vector_set(FloatType(-1.0), FloatType(0.0), FloatType(0.0), FloatType(0.0))) == false);
	CHECK(vector_all_less_equal2(zero, vector_set(FloatType(0.0), FloatType(-1.0), FloatType(0.0), FloatType(0.0))) == false);
	CHECK(vector_all_less_equal2(zero, vector_set(FloatType(0.0), FloatType(0.0), FloatType(-1.0), FloatType(0.0))) == true);
	CHECK(vector_all_less_equal2(zero, zero) == true);

	CHECK(vector_all_less_equal3(zero, vector_set(FloatType(1.0), FloatType(1.0), FloatType(1.0), FloatType(0.0))) == true);
	CHECK(vector_all_less_equal3(zero, vector_set(FloatType(1.0), FloatType(0.0), FloatType(0.0), FloatType(0.0))) == true);
	CHECK(vector_all_less_equal3(zero, vector_set(FloatType(0.0), FloatType(1.0), FloatType(0.0), FloatType(0.0))) == true);
	CHECK(vector_all_less_equal3(zero, vector_set(FloatType(0.0), FloatType(0.0), FloatType(1.0), FloatType(0.0))) == true);
	CHECK(vector_all_less_equal3(zero, vector_set(FloatType(-1.0), FloatType(0.0), FloatType(0.0), FloatType(0.0))) == false);
	CHECK(vector_all_less_equal3(zero, vector_set(FloatType(0.0), FloatType(-1.0), FloatType(0.0), FloatType(0.0))) == false);
	CHECK(vector_all_less_equal3(zero, vector_set(FloatType(0.0), FloatType(0.0), FloatType(-1.0), FloatType(0.0))) == false);
	CHECK(vector_all_less_equal3(zero, zero) == true);

	CHECK(vector_any_less_equal(zero, vector_set(FloatType(1.0), FloatType(1.0), FloatType(1.0), FloatType(1.0))) == true);
	CHECK(vector_any_less_equal(zero, vector_set(FloatType(1.0), FloatType(-1.0), FloatType(-1.0), FloatType(-1.0))) == true);
	CHECK(vector_any_less_equal(zero, vector_set(FloatType(-1.0), FloatType(1.0), FloatType(-1.0), FloatType(-1.0))) == true);
	CHECK(vector_any_less_equal(zero, vector_set(FloatType(-1.0), FloatType(-1.0), FloatType(1.0), FloatType(-1.0))) == true);
	CHECK(vector_any_less_equal(zero, vector_set(FloatType(-1.0), FloatType(-1.0), FloatType(-1.0), FloatType(1.0))) == true);
	CHECK(vector_any_less_equal(zero, vector_set(FloatType(-1.0), FloatType(-1.0), FloatType(-1.0), FloatType(-1.0))) == false);
	CHECK(vector_any_less_equal(zero, zero) == true);

	CHECK(vector_any_less_equal2(zero, vector_set(FloatType(1.0), FloatType(1.0), FloatType(1.0), FloatType(0.0))) == true);
	CHECK(vector_any_less_equal2(zero, vector_set(FloatType(1.0), FloatType(-1.0), FloatType(-1.0), FloatType(0.0))) == true);
	CHECK(vector_any_less_equal2(zero, vector_set(FloatType(-1.0), FloatType(1.0), FloatType(-1.0), FloatType(0.0))) == true);
	CHECK(vector_any_less_equal2(zero, vector_set(FloatType(-1.0), FloatType(-1.0), FloatType(1.0), FloatType(0.0))) == false);
	CHECK(vector_any_less_equal2(zero, vector_set(FloatType(-1.0), FloatType(-1.0), FloatType(-1.0), FloatType(0.0))) == false);
	CHECK(vector_any_less_equal2(zero, zero) == true);

	CHECK(vector_any_less_equal3(zero, vector_set(FloatType(1.0), FloatType(1.0), FloatType(1.0), FloatType(0.0))) == true);
	CHECK(vector_any_less_equal3(zero, vector_set(FloatType(1.0), FloatType(-1.0), FloatType(-1.0), FloatType(0.0))) == true);
	CHECK(vector_any_less_equal3(zero, vector_set(FloatType(-1.0), FloatType(1.0), FloatType(-1.0), FloatType(0.0))) == true);
	CHECK(vector_any_less_equal3(zero, vector_set(FloatType(-1.0), FloatType(-1.0), FloatType(1.0), FloatType(0.0))) == true);
	CHECK(vector_any_less_equal3(zero, vector_set(FloatType(-1.0), FloatType(-1.0), FloatType(-1.0), FloatType(0.0))) == false);
	CHECK(vector_any_less_equal3(zero, zero) == true);

	CHECK(vector_all_greater_than(vector_set(FloatType(1.0), FloatType(1.0), FloatType(1.0), FloatType(1.0)), zero) == true);
	CHECK(vector_all_greater_than(vector_set(FloatType(1.0), FloatType(-1.0), FloatType(-1.0), FloatType(-1.0)), zero) == false);
	CHECK(vector_all_greater_than(vector_set(FloatType(-1.0), FloatType(1.0), FloatType(-1.0), FloatType(-1.0)), zero) == false);
	CHECK(vector_all_greater_than(vector_set(FloatType(-1.0), FloatType(-1.0), FloatType(1.0), FloatType(-1.0)), zero) == false);
	CHECK(vector_all_greater_than(vector_set(FloatType(-1.0), FloatType(-1.0), FloatType(-1.0), FloatType(1.0)), zero) == false);
	CHECK(vector_all_greater_than(vector_set(FloatType(0.0), FloatType(-1.0), FloatType(-1.0), FloatType(-1.0)), zero) == false);
	CHECK(vector_all_greater_than(vector_set(FloatType(-1.0), FloatType(0.0), FloatType(-1.0), FloatType(-1.0)), zero) == false);
	CHECK(vector_all_greater_than(vector_set(FloatType(-1.0), FloatType(-1.0), FloatType(0.0), FloatType(-1.0)), zero) == false);
	CHECK(vector_all_greater_than(vector_set(FloatType(-1.0), FloatType(-1.0), FloatType(-1.0), FloatType(0.0)), zero) == false);
	CHECK(vector_all_greater_than(vector_set(FloatType(-1.0), FloatType(-1.0), FloatType(-1.0), FloatType(-1.0)), zero) == false);
	CHECK(vector_all_greater_than(zero, zero) == false);

	CHECK(vector_all_greater_than2(vector_set(FloatType(1.0), FloatType(1.0), FloatType(1.0), FloatType(0.0)), zero) == true);
	CHECK(vector_all_greater_than2(vector_set(FloatType(1.0), FloatType(-1.0), FloatType(-1.0), FloatType(0.0)), zero) == false);
	CHECK(vector_all_greater_than2(vector_set(FloatType(-1.0), FloatType(1.0), FloatType(-1.0), FloatType(0.0)), zero) == false);
	CHECK(vector_all_greater_than2(vector_set(FloatType(-1.0), FloatType(-1.0), FloatType(1.0), FloatType(0.0)), zero) == false);
	CHECK(vector_all_greater_than2(vector_set(FloatType(0.0), FloatType(-1.0), FloatType(-1.0), FloatType(0.0)), zero) == false);
	CHECK(vector_all_greater_than2(vector_set(FloatType(-1.0), FloatType(0.0), FloatType(-1.0), FloatType(0.0)), zero) == false);
	CHECK(vector_all_greater_than2(vector_set(FloatType(-1.0), FloatType(-1.0), FloatType(0.0), FloatType(0.0)), zero) == false);
	CHECK(vector_all_greater_than2(vector_set(FloatType(-1.0), FloatType(-1.0), FloatType(-1.0), FloatType(0.0)), zero) == false);
	CHECK(vector_all_greater_than2(zero, zero) == false);

	CHECK(vector_all_greater_than3(vector_set(FloatType(1.0), FloatType(1.0), FloatType(1.0), FloatType(0.0)), zero) == true);
	CHECK(vector_all_greater_than3(vector_set(FloatType(1.0), FloatType(-1.0), FloatType(-1.0), FloatType(0.0)), zero) == false);
	CHECK(vector_all_greater_than3(vector_set(FloatType(-1.0), FloatType(1.0), FloatType(-1.0), FloatType(0.0)), zero) == false);
	CHECK(vector_all_greater_than3(vector_set(FloatType(-1.0), FloatType(-1.0), FloatType(1.0), FloatType(0.0)), zero) == false);
	CHECK(vector_all_greater_than3(vector_set(FloatType(0.0), FloatType(-1.0), FloatType(-1.0), FloatType(0.0)), zero) == false);
	CHECK(vector_all_greater_than3(vector_set(FloatType(-1.0), FloatType(0.0), FloatType(-1.0), FloatType(0.0)), zero) == false);
	CHECK(vector_all_greater_than3(vector_set(FloatType(-1.0), FloatType(-1.0), FloatType(0.0), FloatType(0.0)), zero) == false);
	CHECK(vector_all_greater_than3(vector_set(FloatType(-1.0), FloatType(-1.0), FloatType(-1.0), FloatType(0.0)), zero) == false);
	CHECK(vector_all_greater_than3(zero, zero) == false);

	CHECK(vector_any_greater_than(vector_set(FloatType(1.0), FloatType(1.0), FloatType(1.0), FloatType(1.0)), zero) == true);
	CHECK(vector_any_greater_than(vector_set(FloatType(1.0), FloatType(-1.0), FloatType(-1.0), FloatType(-1.0)), zero) == true);
	CHECK(vector_any_greater_than(vector_set(FloatType(-1.0), FloatType(1.0), FloatType(-1.0), FloatType(-1.0)), zero) == true);
	CHECK(vector_any_greater_than(vector_set(FloatType(-1.0), FloatType(-1.0), FloatType(1.0), FloatType(-1.0)), zero) == true);
	CHECK(vector_any_greater_than(vector_set(FloatType(-1.0), FloatType(-1.0), FloatType(-1.0), FloatType(1.0)), zero) == true);
	CHECK(vector_any_greater_than(vector_set(FloatType(0.0), FloatType(-1.0), FloatType(-1.0), FloatType(-1.0)), zero) == false);
	CHECK(vector_any_greater_than(vector_set(FloatType(-1.0), FloatType(0.0), FloatType(-1.0), FloatType(-1.0)), zero) == false);
	CHECK(vector_any_greater_than(vector_set(FloatType(-1.0), FloatType(-1.0), FloatType(0.0), FloatType(-1.0)), zero) == false);
	CHECK(vector_any_greater_than(vector_set(FloatType(-1.0), FloatType(-1.0), FloatType(-1.0), FloatType(0.0)), zero) == false);
	CHECK(vector_any_greater_than(vector_set(FloatType(-1.0), FloatType(-1.0), FloatType(-1.0), FloatType(-1.0)), zero) == false);
	CHECK(vector_any_greater_than(zero, zero) == false);

	CHECK(vector_any_greater_than2(vector_set(FloatType(1.0), FloatType(1.0), FloatType(1.0), FloatType(0.0)), zero) == true);
	CHECK(vector_any_greater_than2(vector_set(FloatType(1.0), FloatType(-1.0), FloatType(-1.0), FloatType(0.0)), zero) == true);
	CHECK(vector_any_greater_than2(vector_set(FloatType(-1.0), FloatType(1.0), FloatType(-1.0), FloatType(0.0)), zero) == true);
	CHECK(vector_any_greater_than2(vector_set(FloatType(-1.0), FloatType(-1.0), FloatType(1.0), FloatType(0.0)), zero) == false);
	CHECK(vector_any_greater_than2(vector_set(FloatType(0.0), FloatType(-1.0), FloatType(-1.0), FloatType(0.0)), zero) == false);
	CHECK(vector_any_greater_than2(vector_set(FloatType(-1.0), FloatType(0.0), FloatType(-1.0), FloatType(0.0)), zero) == false);
	CHECK(vector_any_greater_than2(vector_set(FloatType(-1.0), FloatType(-1.0), FloatType(0.0), FloatType(0.0)), zero) == false);
	CHECK(vector_any_greater_than2(vector_set(FloatType(-1.0), FloatType(-1.0), FloatType(-1.0), FloatType(0.0)), zero) == false);
	CHECK(vector_any_greater_than2(zero, zero) == false);

	CHECK(vector_any_greater_than3(vector_set(FloatType(1.0), FloatType(1.0), FloatType(1.0), FloatType(0.0)), zero) == true);
	CHECK(vector_any_greater_than3(vector_set(FloatType(1.0), FloatType(-1.0), FloatType(-1.0), FloatType(0.0)), zero) == true);
	CHECK(vector_any_greater_than3(vector_set(FloatType(-1.0), FloatType(1.0), FloatType(-1.0), FloatType(0.0)), zero) == true);
	CHECK(vector_any_greater_than3(vector_set(FloatType(-1.0), FloatType(-1.0), FloatType(1.0), FloatType(0.0)), zero) == true);
	CHECK(vector_any_greater_than3(vector_set(FloatType(0.0), FloatType(-1.0), FloatType(-1.0), FloatType(0.0)), zero) == false);
	CHECK(vector_any_greater_than3(vector_set(FloatType(-1.0), FloatType(0.0), FloatType(-1.0), FloatType(0.0)), zero) == false);
	CHECK(vector_any_greater_than3(vector_set(FloatType(-1.0), FloatType(-1.0), FloatType(0.0), FloatType(0.0)), zero) == false);
	CHECK(vector_any_greater_than3(vector_set(FloatType(-1.0), FloatType(-1.0), FloatType(-1.0), FloatType(0.0)), zero) == false);
	CHECK(vector_any_greater_than3(zero, zero) == false);

	CHECK(vector_all_greater_equal(vector_set(FloatType(1.0), FloatType(1.0), FloatType(1.0), FloatType(1.0)), zero) == true);
	CHECK(vector_all_greater_equal(vector_set(FloatType(1.0), FloatType(-1.0), FloatType(-1.0), FloatType(-1.0)), zero) == false);
	CHECK(vector_all_greater_equal(vector_set(FloatType(-1.0), FloatType(1.0), FloatType(-1.0), FloatType(-1.0)), zero) == false);
	CHECK(vector_all_greater_equal(vector_set(FloatType(-1.0), FloatType(-1.0), FloatType(1.0), FloatType(-1.0)), zero) == false);
	CHECK(vector_all_greater_equal(vector_set(FloatType(-1.0), FloatType(-1.0), FloatType(-1.0), FloatType(1.0)), zero) == false);
	CHECK(vector_all_greater_equal(vector_set(FloatType(0.0), FloatType(-1.0), FloatType(-1.0), FloatType(-1.0)), zero) == false);
	CHECK(vector_all_greater_equal(vector_set(FloatType(-1.0), FloatType(0.0), FloatType(-1.0), FloatType(-1.0)), zero) == false);
	CHECK(vector_all_greater_equal(vector_set(FloatType(-1.0), FloatType(-1.0), FloatType(0.0), FloatType(-1.0)), zero) == false);
	CHECK(vector_all_greater_equal(vector_set(FloatType(-1.0), FloatType(-1.0), FloatType(-1.0), FloatType(0.0)), zero) == false);
	CHECK(vector_all_greater_equal(vector_set(FloatType(-1.0), FloatType(-1.0), FloatType(-1.0), FloatType(-1.0)), zero) == false);
	CHECK(vector_all_greater_equal(zero, zero) == true);

	CHECK(vector_all_greater_equal2(vector_set(FloatType(1.0), FloatType(1.0), FloatType(1.0), FloatType(0.0)), zero) == true);
	CHECK(vector_all_greater_equal2(vector_set(FloatType(1.0), FloatType(-1.0), FloatType(-1.0), FloatType(0.0)), zero) == false);
	CHECK(vector_all_greater_equal2(vector_set(FloatType(-1.0), FloatType(1.0), FloatType(-1.0), FloatType(0.0)), zero) == false);
	CHECK(vector_all_greater_equal2(vector_set(FloatType(-1.0), FloatType(-1.0), FloatType(1.0), FloatType(0.0)), zero) == false);
	CHECK(vector_all_greater_equal2(vector_set(FloatType(0.0), FloatType(-1.0), FloatType(-1.0), FloatType(0.0)), zero) == false);
	CHECK(vector_all_greater_equal2(vector_set(FloatType(-1.0), FloatType(0.0), FloatType(-1.0), FloatType(0.0)), zero) == false);
	CHECK(vector_all_greater_equal2(vector_set(FloatType(-1.0), FloatType(-1.0), FloatType(0.0), FloatType(0.0)), zero) == false);
	CHECK(vector_all_greater_equal2(vector_set(FloatType(-1.0), FloatType(-1.0), FloatType(-1.0), FloatType(0.0)), zero) == false);
	CHECK(vector_all_greater_equal2(zero, zero) == true);

	CHECK(vector_all_greater_equal3(vector_set(FloatType(1.0), FloatType(1.0), FloatType(1.0), FloatType(0.0)), zero) == true);
	CHECK(vector_all_greater_equal3(vector_set(FloatType(1.0), FloatType(-1.0), FloatType(-1.0), FloatType(0.0)), zero) == false);
	CHECK(vector_all_greater_equal3(vector_set(FloatType(-1.0), FloatType(1.0), FloatType(-1.0), FloatType(0.0)), zero) == false);
	CHECK(vector_all_greater_equal3(vector_set(FloatType(-1.0), FloatType(-1.0), FloatType(1.0), FloatType(0.0)), zero) == false);
	CHECK(vector_all_greater_equal3(vector_set(FloatType(0.0), FloatType(-1.0), FloatType(-1.0), FloatType(0.0)), zero) == false);
	CHECK(vector_all_greater_equal3(vector_set(FloatType(-1.0), FloatType(0.0), FloatType(-1.0), FloatType(0.0)), zero) == false);
	CHECK(vector_all_greater_equal3(vector_set(FloatType(-1.0), FloatType(-1.0), FloatType(0.0), FloatType(0.0)), zero) == false);
	CHECK(vector_all_greater_equal3(vector_set(FloatType(-1.0), FloatType(-1.0), FloatType(-1.0), FloatType(0.0)), zero) == false);
	CHECK(vector_all_greater_equal3(zero, zero) == true);

	CHECK(vector_any_greater_equal(vector_set(FloatType(1.0), FloatType(1.0), FloatType(1.0), FloatType(1.0)), zero) == true);
	CHECK(vector_any_greater_equal(vector_set(FloatType(1.0), FloatType(-1.0), FloatType(-1.0), FloatType(-1.0)), zero) == true);
	CHECK(vector_any_greater_equal(vector_set(FloatType(-1.0), FloatType(1.0), FloatType(-1.0), FloatType(-1.0)), zero) == true);
	CHECK(vector_any_greater_equal(vector_set(FloatType(-1.0), FloatType(-1.0), FloatType(1.0), FloatType(-1.0)), zero) == true);
	CHECK(vector_any_greater_equal(vector_set(FloatType(-1.0), FloatType(-1.0), FloatType(-1.0), FloatType(1.0)), zero) == true);
	CHECK(vector_any_greater_equal(vector_set(FloatType(0.0), FloatType(-1.0), FloatType(-1.0), FloatType(-1.0)), zero) == true);
	CHECK(vector_any_greater_equal(vector_set(FloatType(-1.0), FloatType(0.0), FloatType(-1.0), FloatType(-1.0)), zero) == true);
	CHECK(vector_any_greater_equal(vector_set(FloatType(-1.0), FloatType(-1.0), FloatType(0.0), FloatType(-1.0)), zero) == true);
	CHECK(vector_any_greater_equal(vector_set(FloatType(-1.0), FloatType(-1.0), FloatType(-1.0), FloatType(0.0)), zero) == true);
	CHECK(vector_any_greater_equal(vector_set(FloatType(-1.0), FloatType(-1.0), FloatType(-1.0), FloatType(-1.0)), zero) == false);
	CHECK(vector_any_greater_equal(zero, zero) == true);

	CHECK(vector_any_greater_equal2(vector_set(FloatType(1.0), FloatType(1.0), FloatType(1.0), FloatType(0.0)), zero) == true);
	CHECK(vector_any_greater_equal2(vector_set(FloatType(1.0), FloatType(-1.0), FloatType(-1.0), FloatType(0.0)), zero) == true);
	CHECK(vector_any_greater_equal2(vector_set(FloatType(-1.0), FloatType(1.0), FloatType(-1.0), FloatType(0.0)), zero) == true);
	CHECK(vector_any_greater_equal2(vector_set(FloatType(-1.0), FloatType(-1.0), FloatType(1.0), FloatType(0.0)), zero) == false);
	CHECK(vector_any_greater_equal2(vector_set(FloatType(0.0), FloatType(-1.0), FloatType(-1.0), FloatType(0.0)), zero) == true);
	CHECK(vector_any_greater_equal2(vector_set(FloatType(-1.0), FloatType(0.0), FloatType(-1.0), FloatType(0.0)), zero) == true);
	CHECK(vector_any_greater_equal2(vector_set(FloatType(-1.0), FloatType(-1.0), FloatType(0.0), FloatType(0.0)), zero) == false);
	CHECK(vector_any_greater_equal2(vector_set(FloatType(-1.0), FloatType(-1.0), FloatType(-1.0), FloatType(0.0)), zero) == false);
	CHECK(vector_any_greater_equal2(zero, zero) == true);

	CHECK(vector_any_greater_equal3(vector_set(FloatType(1.0), FloatType(1.0), FloatType(1.0), FloatType(0.0)), zero) == true);
	CHECK(vector_any_greater_equal3(vector_set(FloatType(1.0), FloatType(-1.0), FloatType(-1.0), FloatType(0.0)), zero) == true);
	CHECK(vector_any_greater_equal3(vector_set(FloatType(-1.0), FloatType(1.0), FloatType(-1.0), FloatType(0.0)), zero) == true);
	CHECK(vector_any_greater_equal3(vector_set(FloatType(-1.0), FloatType(-1.0), FloatType(1.0), FloatType(0.0)), zero) == true);
	CHECK(vector_any_greater_equal3(vector_set(FloatType(0.0), FloatType(-1.0), FloatType(-1.0), FloatType(0.0)), zero) == true);
	CHECK(vector_any_greater_equal3(vector_set(FloatType(-1.0), FloatType(0.0), FloatType(-1.0), FloatType(0.0)), zero) == true);
	CHECK(vector_any_greater_equal3(vector_set(FloatType(-1.0), FloatType(-1.0), FloatType(0.0), FloatType(0.0)), zero) == true);
	CHECK(vector_any_greater_equal3(vector_set(FloatType(-1.0), FloatType(-1.0), FloatType(-1.0), FloatType(0.0)), zero) == false);
	CHECK(vector_any_greater_equal3(zero, zero) == true);

	CHECK(vector_all_equal(vector_set(FloatType(1.0), FloatType(1.0), FloatType(1.0), FloatType(1.0)), zero) == false);
	CHECK(vector_all_equal(vector_set(FloatType(1.0), FloatType(-1.0), FloatType(-1.0), FloatType(-1.0)), zero) == false);
	CHECK(vector_all_equal(vector_set(FloatType(-1.0), FloatType(1.0), FloatType(-1.0), FloatType(-1.0)), zero) == false);
	CHECK(vector_all_equal(vector_set(FloatType(-1.0), FloatType(-1.0), FloatType(1.0), FloatType(-1.0)), zero) == false);
	CHECK(vector_all_equal(vector_set(FloatType(-1.0), FloatType(-1.0), FloatType(-1.0), FloatType(1.0)), zero) == false);
	CHECK(vector_all_equal(vector_set(FloatType(0.0), FloatType(0.0), FloatType(-1.0), FloatType(-1.0)), zero) == false);
	CHECK(vector_all_equal(vector_set(FloatType(-1.0), FloatType(0.0), FloatType(-1.0), FloatType(-1.0)), zero) == false);
	CHECK(vector_all_equal(vector_set(FloatType(-1.0), FloatType(-1.0), FloatType(0.0), FloatType(-1.0)), zero) == false);
	CHECK(vector_all_equal(vector_set(FloatType(-1.0), FloatType(-1.0), FloatType(-1.0), FloatType(0.0)), zero) == false);
	CHECK(vector_all_equal(vector_set(FloatType(-1.0), FloatType(-1.0), FloatType(-1.0), FloatType(-1.0)), zero) == false);
	CHECK(vector_all_equal(zero, zero) == true);

	CHECK(vector_all_equal2(vector_set(FloatType(1.0), FloatType(1.0), FloatType(1.0), FloatType(0.0)), zero) == false);
	CHECK(vector_all_equal2(vector_set(FloatType(1.0), FloatType(-1.0), FloatType(-1.0), FloatType(0.0)), zero) == false);
	CHECK(vector_all_equal2(vector_set(FloatType(-1.0), FloatType(1.0), FloatType(-1.0), FloatType(0.0)), zero) == false);
	CHECK(vector_all_equal2(vector_set(FloatType(-1.0), FloatType(-1.0), FloatType(1.0), FloatType(0.0)), zero) == false);
	CHECK(vector_all_equal2(vector_set(FloatType(0.0), FloatType(0.0), FloatType(-1.0), FloatType(0.0)), zero) == true);
	CHECK(vector_all_equal2(vector_set(FloatType(-1.0), FloatType(0.0), FloatType(-1.0), FloatType(0.0)), zero) == false);
	CHECK(vector_all_equal2(vector_set(FloatType(-1.0), FloatType(-1.0), FloatType(0.0), FloatType(0.0)), zero) == false);
	CHECK(vector_all_equal2(vector_set(FloatType(-1.0), FloatType(-1.0), FloatType(-1.0), FloatType(0.0)), zero) == false);
	CHECK(vector_all_equal2(zero, zero) == true);

	CHECK(vector_all_equal3(vector_set(FloatType(1.0), FloatType(1.0), FloatType(1.0), FloatType(0.0)), zero) == false);
	CHECK(vector_all_equal3(vector_set(FloatType(1.0), FloatType(-1.0), FloatType(-1.0), FloatType(0.0)), zero) == false);
	CHECK(vector_all_equal3(vector_set(FloatType(-1.0), FloatType(1.0), FloatType(-1.0), FloatType(0.0)), zero) == false);
	CHECK(vector_all_equal3(vector_set(FloatType(-1.0), FloatType(-1.0), FloatType(1.0), FloatType(0.0)), zero) == false);
	CHECK(vector_all_equal3(vector_set(FloatType(0.0), FloatType(0.0), FloatType(-1.0), FloatType(0.0)), zero) == false);
	CHECK(vector_all_equal3(vector_set(FloatType(-1.0), FloatType(0.0), FloatType(-1.0), FloatType(0.0)), zero) == false);
	CHECK(vector_all_equal3(vector_set(FloatType(-1.0), FloatType(-1.0), FloatType(0.0), FloatType(0.0)), zero) == false);
	CHECK(vector_all_equal3(vector_set(FloatType(-1.0), FloatType(-1.0), FloatType(-1.0), FloatType(0.0)), zero) == false);
	CHECK(vector_all_equal3(zero, zero) == true);

	CHECK(vector_any_equal(vector_set(FloatType(1.0), FloatType(1.0), FloatType(1.0), FloatType(1.0)), zero) == false);
	CHECK(vector_any_equal(vector_set(FloatType(1.0), FloatType(-1.0), FloatType(-1.0), FloatType(-1.0)), zero) == false);
	CHECK(vector_any_equal(vector_set(FloatType(-1.0), FloatType(1.0), FloatType(-1.0), FloatType(-1.0)), zero) == false);
	CHECK(vector_any_equal(vector_set(FloatType(-1.0), FloatType(-1.0), FloatType(1.0), FloatType(-1.0)), zero) == false);
	CHECK(vector_any_equal(vector_set(FloatType(-1.0), FloatType(-1.0), FloatType(-1.0), FloatType(1.0)), zero) == false);
	CHECK(vector_any_equal(vector_set(FloatType(0.0), FloatType(-1.0), FloatType(-1.0), FloatType(-1.0)), zero) == true);
	CHECK(vector_any_equal(vector_set(FloatType(-1.0), FloatType(0.0), FloatType(-1.0), FloatType(-1.0)), zero) == true);
	CHECK(vector_any_equal(vector_set(FloatType(-1.0), FloatType(-1.0), FloatType(0.0), FloatType(-1.0)), zero) == true);
	CHECK(vector_any_equal(vector_set(FloatType(-1.0), FloatType(-1.0), FloatType(-1.0), FloatType(0.0)), zero) == true);
	CHECK(vector_any_equal(vector_set(FloatType(-1.0), FloatType(-1.0), FloatType(-1.0), FloatType(-1.0)), zero) == false);
	CHECK(vector_any_equal(zero, zero) == true);

	CHECK(vector_any_equal2(vector_set(FloatType(1.0), FloatType(1.0), FloatType(1.0), FloatType(0.0)), zero) == false);
	CHECK(vector_any_equal2(vector_set(FloatType(1.0), FloatType(-1.0), FloatType(-1.0), FloatType(0.0)), zero) == false);
	CHECK(vector_any_equal2(vector_set(FloatType(-1.0), FloatType(1.0), FloatType(-1.0), FloatType(0.0)), zero) == false);
	CHECK(vector_any_equal2(vector_set(FloatType(-1.0), FloatType(-1.0), FloatType(1.0), FloatType(0.0)), zero) == false);
	CHECK(vector_any_equal2(vector_set(FloatType(0.0), FloatType(-1.0), FloatType(-1.0), FloatType(0.0)), zero) == true);
	CHECK(vector_any_equal2(vector_set(FloatType(-1.0), FloatType(0.0), FloatType(-1.0), FloatType(0.0)), zero) == true);
	CHECK(vector_any_equal2(vector_set(FloatType(-1.0), FloatType(-1.0), FloatType(0.0), FloatType(0.0)), zero) == false);
	CHECK(vector_any_equal2(vector_set(FloatType(-1.0), FloatType(-1.0), FloatType(-1.0), FloatType(0.0)), zero) == false);
	CHECK(vector_any_equal2(zero, zero) == true);

	CHECK(vector_any_equal3(vector_set(FloatType(1.0), FloatType(1.0), FloatType(1.0), FloatType(0.0)), zero) == false);
	CHECK(vector_any_equal3(vector_set(FloatType(1.0), FloatType(-1.0), FloatType(-1.0), FloatType(0.0)), zero) == false);
	CHECK(vector_any_equal3(vector_set(FloatType(-1.0), FloatType(1.0), FloatType(-1.0), FloatType(0.0)), zero) == false);
	CHECK(vector_any_equal3(vector_set(FloatType(-1.0), FloatType(-1.0), FloatType(1.0), FloatType(0.0)), zero) == false);
	CHECK(vector_any_equal3(vector_set(FloatType(0.0), FloatType(-1.0), FloatType(-1.0), FloatType(0.0)), zero) == true);
	CHECK(vector_any_equal3(vector_set(FloatType(-1.0), FloatType(0.0), FloatType(-1.0), FloatType(0.0)), zero) == true);
	CHECK(vector_any_equal3(vector_set(FloatType(-1.0), FloatType(-1.0), FloatType(0.0), FloatType(0.0)), zero) == true);
	CHECK(vector_any_equal3(vector_set(FloatType(-1.0), FloatType(-1.0), FloatType(-1.0), FloatType(0.0)), zero) == false);
	CHECK(vector_any_equal3(zero, zero) == true);


	CHECK(vector_all_not_equal(vector_set(FloatType(1.0), FloatType(1.0), FloatType(1.0), FloatType(1.0)), zero) != false);
	CHECK(vector_all_not_equal(vector_set(FloatType(1.0), FloatType(-1.0), FloatType(-1.0), FloatType(-1.0)), zero) != false);
	CHECK(vector_all_not_equal(vector_set(FloatType(-1.0), FloatType(1.0), FloatType(-1.0), FloatType(-1.0)), zero) != false);
	CHECK(vector_all_not_equal(vector_set(FloatType(-1.0), FloatType(-1.0), FloatType(1.0), FloatType(-1.0)), zero) != false);
	CHECK(vector_all_not_equal(vector_set(FloatType(-1.0), FloatType(-1.0), FloatType(-1.0), FloatType(1.0)), zero) != false);
	CHECK(vector_all_not_equal(vector_set(FloatType(0.0), FloatType(0.0), FloatType(-1.0), FloatType(-1.0)), zero) == false);
	CHECK(vector_all_not_equal(vector_set(FloatType(-1.0), FloatType(0.0), FloatType(-1.0), FloatType(-1.0)), zero) == false);
	CHECK(vector_all_not_equal(vector_set(FloatType(-1.0), FloatType(-1.0), FloatType(0.0), FloatType(-1.0)), zero) == false);
	CHECK(vector_all_not_equal(vector_set(FloatType(-1.0), FloatType(-1.0), FloatType(-1.0), FloatType(0.0)), zero) == false);
	CHECK(vector_all_not_equal(vector_set(FloatType(-1.0), FloatType(-1.0), FloatType(-1.0), FloatType(-1.0)), zero) != false);
	CHECK(vector_all_not_equal(zero, zero) != true);

	CHECK(vector_all_not_equal2(vector_set(FloatType(1.0), FloatType(1.0), FloatType(1.0), FloatType(0.0)), zero) != false);
	CHECK(vector_all_not_equal2(vector_set(FloatType(1.0), FloatType(-1.0), FloatType(-1.0), FloatType(0.0)), zero) != false);
	CHECK(vector_all_not_equal2(vector_set(FloatType(-1.0), FloatType(1.0), FloatType(-1.0), FloatType(0.0)), zero) != false);
	CHECK(vector_all_not_equal2(vector_set(FloatType(-1.0), FloatType(-1.0), FloatType(1.0), FloatType(0.0)), zero) != false);
	CHECK(vector_all_not_equal2(vector_set(FloatType(0.0), FloatType(0.0), FloatType(-1.0), FloatType(0.0)), zero) != true);
	CHECK(vector_all_not_equal2(vector_set(FloatType(-1.0), FloatType(0.0), FloatType(-1.0), FloatType(0.0)), zero) == false);
	CHECK(vector_all_not_equal2(vector_set(FloatType(-1.0), FloatType(-1.0), FloatType(0.0), FloatType(0.0)), zero) != false);
	CHECK(vector_all_not_equal2(vector_set(FloatType(-1.0), FloatType(-1.0), FloatType(-1.0), FloatType(0.0)), zero) != false);
	CHECK(vector_all_not_equal2(zero, zero) != true);

	CHECK(vector_all_not_equal3(vector_set(FloatType(1.0), FloatType(1.0), FloatType(1.0), FloatType(0.0)), zero) != false);
	CHECK(vector_all_not_equal3(vector_set(FloatType(1.0), FloatType(-1.0), FloatType(-1.0), FloatType(0.0)), zero) != false);
	CHECK(vector_all_not_equal3(vector_set(FloatType(-1.0), FloatType(1.0), FloatType(-1.0), FloatType(0.0)), zero) != false);
	CHECK(vector_all_not_equal3(vector_set(FloatType(-1.0), FloatType(-1.0), FloatType(1.0), FloatType(0.0)), zero) != false);
	CHECK(vector_all_not_equal3(vector_set(FloatType(0.0), FloatType(0.0), FloatType(-1.0), FloatType(0.0)), zero) == false);
	CHECK(vector_all_not_equal3(vector_set(FloatType(-1.0), FloatType(0.0), FloatType(-1.0), FloatType(0.0)), zero) == false);
	CHECK(vector_all_not_equal3(vector_set(FloatType(-1.0), FloatType(-1.0), FloatType(0.0), FloatType(0.0)), zero) == false);
	CHECK(vector_all_not_equal3(vector_set(FloatType(-1.0), FloatType(-1.0), FloatType(-1.0), FloatType(0.0)), zero) != false);
	CHECK(vector_all_not_equal3(zero, zero) != true);

	CHECK(vector_any_not_equal(vector_set(FloatType(1.0), FloatType(1.0), FloatType(1.0), FloatType(1.0)), zero) != false);
	CHECK(vector_any_not_equal(vector_set(FloatType(1.0), FloatType(-1.0), FloatType(-1.0), FloatType(-1.0)), zero) != false);
	CHECK(vector_any_not_equal(vector_set(FloatType(-1.0), FloatType(1.0), FloatType(-1.0), FloatType(-1.0)), zero) != false);
	CHECK(vector_any_not_equal(vector_set(FloatType(-1.0), FloatType(-1.0), FloatType(1.0), FloatType(-1.0)), zero) != false);
	CHECK(vector_any_not_equal(vector_set(FloatType(-1.0), FloatType(-1.0), FloatType(-1.0), FloatType(1.0)), zero) != false);
	CHECK(vector_any_not_equal(vector_set(FloatType(0.0), FloatType(-1.0), FloatType(-1.0), FloatType(-1.0)), zero) == true);
	CHECK(vector_any_not_equal(vector_set(FloatType(-1.0), FloatType(0.0), FloatType(-1.0), FloatType(-1.0)), zero) == true);
	CHECK(vector_any_not_equal(vector_set(FloatType(-1.0), FloatType(-1.0), FloatType(0.0), FloatType(-1.0)), zero) == true);
	CHECK(vector_any_not_equal(vector_set(FloatType(-1.0), FloatType(-1.0), FloatType(-1.0), FloatType(0.0)), zero) == true);
	CHECK(vector_any_not_equal(vector_set(FloatType(-1.0), FloatType(-1.0), FloatType(-1.0), FloatType(-1.0)), zero) != false);
	CHECK(vector_any_not_equal(zero, zero) != true);

	CHECK(vector_any_not_equal2(vector_set(FloatType(1.0), FloatType(1.0), FloatType(1.0), FloatType(0.0)), zero) != false);
	CHECK(vector_any_not_equal2(vector_set(FloatType(1.0), FloatType(-1.0), FloatType(-1.0), FloatType(0.0)), zero) != false);
	CHECK(vector_any_not_equal2(vector_set(FloatType(-1.0), FloatType(1.0), FloatType(-1.0), FloatType(0.0)), zero) != false);
	CHECK(vector_any_not_equal2(vector_set(FloatType(-1.0), FloatType(-1.0), FloatType(1.0), FloatType(0.0)), zero) != false);
	CHECK(vector_any_not_equal2(vector_set(FloatType(0.0), FloatType(-1.0), FloatType(-1.0), FloatType(0.0)), zero) == true);
	CHECK(vector_any_not_equal2(vector_set(FloatType(-1.0), FloatType(0.0), FloatType(-1.0), FloatType(0.0)), zero) == true);
	CHECK(vector_any_not_equal2(vector_set(FloatType(-1.0), FloatType(-1.0), FloatType(0.0), FloatType(0.0)), zero) != false);
	CHECK(vector_any_not_equal2(vector_set(FloatType(-1.0), FloatType(-1.0), FloatType(-1.0), FloatType(0.0)), zero) != false);
	CHECK(vector_any_not_equal2(zero, zero) != true);

	CHECK(vector_any_not_equal3(vector_set(FloatType(1.0), FloatType(1.0), FloatType(1.0), FloatType(0.0)), zero) != false);
	CHECK(vector_any_not_equal3(vector_set(FloatType(1.0), FloatType(-1.0), FloatType(-1.0), FloatType(0.0)), zero) != false);
	CHECK(vector_any_not_equal3(vector_set(FloatType(-1.0), FloatType(1.0), FloatType(-1.0), FloatType(0.0)), zero) != false);
	CHECK(vector_any_not_equal3(vector_set(FloatType(-1.0), FloatType(-1.0), FloatType(1.0), FloatType(0.0)), zero) != false);
	CHECK(vector_any_not_equal3(vector_set(FloatType(0.0), FloatType(-1.0), FloatType(-1.0), FloatType(0.0)), zero) == true);
	CHECK(vector_any_not_equal3(vector_set(FloatType(-1.0), FloatType(0.0), FloatType(-1.0), FloatType(0.0)), zero) == true);
	CHECK(vector_any_not_equal3(vector_set(FloatType(-1.0), FloatType(-1.0), FloatType(0.0), FloatType(0.0)), zero) == true);
	CHECK(vector_any_not_equal3(vector_set(FloatType(-1.0), FloatType(-1.0), FloatType(-1.0), FloatType(0.0)), zero) != false);
	CHECK(vector_any_not_equal3(zero, zero) != true);

	CHECK(vector_all_near_equal(zero, zero, threshold) == true);
	CHECK(vector_all_near_equal(zero, vector_set(FloatType(1.0), FloatType(1.0), FloatType(1.0), FloatType(1.0)), FloatType(1.0001)) == true);
	CHECK(vector_all_near_equal(zero, vector_set(FloatType(1.0), FloatType(1.0), FloatType(1.0), FloatType(1.0)), FloatType(1.0)) == true);
	CHECK(vector_all_near_equal(zero, vector_set(FloatType(1.0), FloatType(1.0), FloatType(1.0), FloatType(1.0)), FloatType(0.9999)) == false);

	CHECK(vector_all_near_equal2(zero, zero, threshold) == true);
	CHECK(vector_all_near_equal2(zero, vector_set(FloatType(1.0), FloatType(1.0), FloatType(1.0), FloatType(2.0)), FloatType(1.0001)) == true);
	CHECK(vector_all_near_equal2(zero, vector_set(FloatType(1.0), FloatType(1.0), FloatType(1.0), FloatType(2.0)), FloatType(1.0)) == true);
	CHECK(vector_all_near_equal2(zero, vector_set(FloatType(1.0), FloatType(1.0), FloatType(1.0), FloatType(2.0)), FloatType(0.9999)) == false);

	CHECK(vector_all_near_equal3(zero, zero, threshold) == true);
	CHECK(vector_all_near_equal3(zero, vector_set(FloatType(1.0), FloatType(1.0), FloatType(1.0), FloatType(2.0)), FloatType(1.0001)) == true);
	CHECK(vector_all_near_equal3(zero, vector_set(FloatType(1.0), FloatType(1.0), FloatType(1.0), FloatType(2.0)), FloatType(1.0)) == true);
	CHECK(vector_all_near_equal3(zero, vector_set(FloatType(1.0), FloatType(1.0), FloatType(1.0), FloatType(2.0)), FloatType(0.9999)) == false);

	CHECK(vector_any_near_equal(zero, zero, threshold) == true);
	CHECK(vector_any_near_equal(zero, vector_set(FloatType(1.0), FloatType(2.0), FloatType(2.0), FloatType(2.0)), FloatType(1.0001)) == true);
	CHECK(vector_any_near_equal(zero, vector_set(FloatType(2.0), FloatType(1.0), FloatType(2.0), FloatType(2.0)), FloatType(1.0001)) == true);
	CHECK(vector_any_near_equal(zero, vector_set(FloatType(2.0), FloatType(2.0), FloatType(1.0), FloatType(2.0)), FloatType(1.0001)) == true);
	CHECK(vector_any_near_equal(zero, vector_set(FloatType(2.0), FloatType(2.0), FloatType(2.0), FloatType(1.0)), FloatType(1.0001)) == true);
	CHECK(vector_any_near_equal(zero, vector_set(FloatType(1.0), FloatType(2.0), FloatType(2.0), FloatType(2.0)), FloatType(1.0)) == true);
	CHECK(vector_any_near_equal(zero, vector_set(FloatType(2.0), FloatType(1.0), FloatType(2.0), FloatType(2.0)), FloatType(1.0)) == true);
	CHECK(vector_any_near_equal(zero, vector_set(FloatType(2.0), FloatType(2.0), FloatType(1.0), FloatType(2.0)), FloatType(1.0)) == true);
	CHECK(vector_any_near_equal(zero, vector_set(FloatType(2.0), FloatType(2.0), FloatType(2.0), FloatType(1.0)), FloatType(1.0)) == true);
	CHECK(vector_any_near_equal(zero, vector_set(FloatType(1.0), FloatType(1.0), FloatType(1.0), FloatType(1.0)), FloatType(0.9999)) == false);

	CHECK(vector_any_near_equal2(zero, zero, threshold) == true);
	CHECK(vector_any_near_equal2(zero, vector_set(FloatType(1.0), FloatType(2.0), FloatType(2.0), FloatType(2.0)), FloatType(1.0001)) == true);
	CHECK(vector_any_near_equal2(zero, vector_set(FloatType(2.0), FloatType(1.0), FloatType(2.0), FloatType(2.0)), FloatType(1.0001)) == true);
	CHECK(vector_any_near_equal2(zero, vector_set(FloatType(2.0), FloatType(2.0), FloatType(1.0), FloatType(2.0)), FloatType(1.0001)) == false);
	CHECK(vector_any_near_equal2(zero, vector_set(FloatType(1.0), FloatType(2.0), FloatType(2.0), FloatType(2.0)), FloatType(1.0)) == true);
	CHECK(vector_any_near_equal2(zero, vector_set(FloatType(2.0), FloatType(1.0), FloatType(2.0), FloatType(2.0)), FloatType(1.0)) == true);
	CHECK(vector_any_near_equal2(zero, vector_set(FloatType(2.0), FloatType(2.0), FloatType(1.0), FloatType(2.0)), FloatType(1.0)) == false);
	CHECK(vector_any_near_equal2(zero, vector_set(FloatType(1.0), FloatType(1.0), FloatType(1.0), FloatType(2.0)), FloatType(0.9999)) == false);

	CHECK(vector_any_near_equal3(zero, zero, threshold) == true);
	CHECK(vector_any_near_equal3(zero, vector_set(FloatType(1.0), FloatType(2.0), FloatType(2.0), FloatType(2.0)), FloatType(1.0001)) == true);
	CHECK(vector_any_near_equal3(zero, vector_set(FloatType(2.0), FloatType(1.0), FloatType(2.0), FloatType(2.0)), FloatType(1.0001)) == true);
	CHECK(vector_any_near_equal3(zero, vector_set(FloatType(2.0), FloatType(2.0), FloatType(1.0), FloatType(2.0)), FloatType(1.0001)) == true);
	CHECK(vector_any_near_equal3(zero, vector_set(FloatType(1.0), FloatType(2.0), FloatType(2.0), FloatType(2.0)), FloatType(1.0)) == true);
	CHECK(vector_any_near_equal3(zero, vector_set(FloatType(2.0), FloatType(1.0), FloatType(2.0), FloatType(2.0)), FloatType(1.0)) == true);
	CHECK(vector_any_near_equal3(zero, vector_set(FloatType(2.0), FloatType(2.0), FloatType(1.0), FloatType(2.0)), FloatType(1.0)) == true);
	CHECK(vector_any_near_equal3(zero, vector_set(FloatType(1.0), FloatType(1.0), FloatType(1.0), FloatType(2.0)), FloatType(0.9999)) == false);
}

template<typename FloatType, typename Vector4Type>
inline Vector4Type reference_vector_and(const Vector4Type& input0, const Vector4Type& input1)
{
	using Int1Type = typename related_types<FloatType>::int1;

	Int1Type input0_[4];
	Int1Type input1_[4];

	static_assert(sizeof(Vector4Type) == sizeof(input0_), "Unexpected size");
	std::memcpy(&input0_[0], &input0, sizeof(Vector4Type));
	std::memcpy(&input1_[0], &input1, sizeof(Vector4Type));

	Int1Type result_[4];
	result_[0] = input0_[0] & input1_[0];
	result_[1] = input0_[1] & input1_[1];
	result_[2] = input0_[2] & input1_[2];
	result_[3] = input0_[3] & input1_[3];

	Vector4Type result;
	std::memcpy(&result, &result_[0], sizeof(Vector4Type));

	return result;
}

template<typename FloatType, typename Vector4Type>
inline Vector4Type reference_vector_or(const Vector4Type& input0, const Vector4Type& input1)
{
	using Int1Type = typename related_types<FloatType>::int1;

	Int1Type input0_[4];
	Int1Type input1_[4];

	static_assert(sizeof(Vector4Type) == sizeof(input0_), "Unexpected size");
	std::memcpy(&input0_[0], &input0, sizeof(Vector4Type));
	std::memcpy(&input1_[0], &input1, sizeof(Vector4Type));

	Int1Type result_[4];
	result_[0] = input0_[0] | input1_[0];
	result_[1] = input0_[1] | input1_[1];
	result_[2] = input0_[2] | input1_[2];
	result_[3] = input0_[3] | input1_[3];

	Vector4Type result;
	std::memcpy(&result, &result_[0], sizeof(Vector4Type));

	return result;
}

template<typename FloatType, typename Vector4Type>
inline Vector4Type reference_vector_xor(const Vector4Type& input0, const Vector4Type& input1)
{
	using Int1Type = typename related_types<FloatType>::int1;

	Int1Type input0_[4];
	Int1Type input1_[4];

	static_assert(sizeof(Vector4Type) == sizeof(input0_), "Unexpected size");
	std::memcpy(&input0_[0], &input0, sizeof(Vector4Type));
	std::memcpy(&input1_[0], &input1, sizeof(Vector4Type));

	Int1Type result_[4];
	result_[0] = input0_[0] ^ input1_[0];
	result_[1] = input0_[1] ^ input1_[1];
	result_[2] = input0_[2] ^ input1_[2];
	result_[3] = input0_[3] ^ input1_[3];

	Vector4Type result;
	std::memcpy(&result, &result_[0], sizeof(Vector4Type));

	return result;
}

template<typename FloatType>
void test_vector4_logical_impl()
{
	using Vector4Type = typename related_types<FloatType>::vector4;

	const FloatType test_value0_flt[4] = { FloatType(2.0), FloatType(9.34), FloatType(-54.12), FloatType(6000.0) };
	const FloatType test_value1_flt[4] = { FloatType(0.75), FloatType(-4.52), FloatType(44.68), FloatType(-54225.0) };
	const FloatType test_value2_flt[4] = { FloatType(2.0), FloatType(-9.34), FloatType(54.12), FloatType(6000.1) };
	const Vector4Type test_value0 = vector_set(test_value0_flt[0], test_value0_flt[1], test_value0_flt[2], test_value0_flt[3]);
	const Vector4Type test_value1 = vector_set(test_value1_flt[0], test_value1_flt[1], test_value1_flt[2], test_value1_flt[3]);
	const Vector4Type test_value2 = vector_set(test_value2_flt[0], test_value2_flt[1], test_value2_flt[2], test_value2_flt[3]);

	CHECK(vector_all_near_equal(vector_and(test_value0, test_value1), reference_vector_and<FloatType>(test_value0, test_value1), FloatType(0.0)));
	CHECK(vector_all_near_equal(vector_and(test_value1, test_value2), reference_vector_and<FloatType>(test_value1, test_value2), FloatType(0.0)));
	CHECK(vector_all_near_equal(vector_and(test_value0, test_value2), reference_vector_and<FloatType>(test_value0, test_value2), FloatType(0.0)));

	CHECK(vector_all_near_equal(vector_or(test_value0, test_value1), reference_vector_or<FloatType>(test_value0, test_value1), FloatType(0.0)));
	CHECK(vector_all_near_equal(vector_or(test_value1, test_value2), reference_vector_or<FloatType>(test_value1, test_value2), FloatType(0.0)));
	CHECK(vector_all_near_equal(vector_or(test_value0, test_value2), reference_vector_or<FloatType>(test_value0, test_value2), FloatType(0.0)));

	CHECK(vector_all_near_equal(vector_xor(test_value0, test_value1), reference_vector_xor<FloatType>(test_value0, test_value1), FloatType(0.0)));
	CHECK(vector_all_near_equal(vector_xor(test_value1, test_value2), reference_vector_xor<FloatType>(test_value1, test_value2), FloatType(0.0)));
	CHECK(vector_all_near_equal(vector_xor(test_value0, test_value2), reference_vector_xor<FloatType>(test_value0, test_value2), FloatType(0.0)));
}

template<typename FloatType>
void test_vector4_impl(const FloatType threshold)
{
	using Vector4Type = typename related_types<FloatType>::vector4;

	const Vector4Type zero = vector_zero();

	const FloatType test_value0_flt[4] = { FloatType(2.0), FloatType(9.34), FloatType(-54.12), FloatType(6000.0) };
	const FloatType test_value1_flt[4] = { FloatType(0.75), FloatType(-4.52), FloatType(44.68), FloatType(-54225.0) };
	const Vector4Type test_value0 = vector_set(test_value0_flt[0], test_value0_flt[1], test_value0_flt[2], test_value0_flt[3]);
	const Vector4Type test_value1 = vector_set(test_value1_flt[0], test_value1_flt[1], test_value1_flt[2], test_value1_flt[3]);

	const FloatType inf = std::numeric_limits<FloatType>::infinity();
	const FloatType nan = std::numeric_limits<FloatType>::quiet_NaN();

	CHECK((mask_get_x(vector_finite(vector_set(FloatType(1.0), inf, nan, -inf))) != 0) == scalar_is_finite(FloatType(1.0)));
	CHECK((mask_get_y(vector_finite(vector_set(FloatType(1.0), inf, nan, -inf))) != 0) == scalar_is_finite(inf));
	CHECK((mask_get_z(vector_finite(vector_set(FloatType(1.0), inf, nan, -inf))) != 0) == scalar_is_finite(nan));
	CHECK((mask_get_w(vector_finite(vector_set(FloatType(1.0), inf, nan, -inf))) != 0) == scalar_is_finite(-inf));

	CHECK(vector_is_finite(zero) == true);
	CHECK(vector_is_finite(vector_set(inf, inf, inf, inf)) == false);
	CHECK(vector_is_finite(vector_set(-inf, -inf, -inf, -inf)) == false);
	CHECK(vector_is_finite(vector_set(inf, FloatType(1.0), FloatType(1.0), FloatType(1.0))) == false);
	CHECK(vector_is_finite(vector_set(FloatType(1.0), FloatType(inf), FloatType(1.0), FloatType(1.0))) == false);
	CHECK(vector_is_finite(vector_set(FloatType(1.0), FloatType(1.0), FloatType(inf), FloatType(1.0))) == false);
	CHECK(vector_is_finite(vector_set(FloatType(1.0), FloatType(1.0), FloatType(1.0), FloatType(inf))) == false);
	CHECK(vector_is_finite(vector_set(nan, nan, nan, nan)) == false);
	CHECK(vector_is_finite(vector_set(nan, FloatType(1.0), FloatType(1.0), FloatType(1.0))) == false);
	CHECK(vector_is_finite(vector_set(FloatType(1.0), FloatType(nan), FloatType(1.0), FloatType(1.0))) == false);
	CHECK(vector_is_finite(vector_set(FloatType(1.0), FloatType(1.0), FloatType(nan), FloatType(1.0))) == false);
	CHECK(vector_is_finite(vector_set(FloatType(1.0), FloatType(1.0), FloatType(1.0), FloatType(nan))) == false);

	CHECK(vector_is_finite2(zero) == true);
	CHECK(vector_is_finite2(vector_set(inf, inf, inf, inf)) == false);
	CHECK(vector_is_finite2(vector_set(-inf, -inf, -inf, -inf)) == false);
	CHECK(vector_is_finite2(vector_set(inf, FloatType(1.0), FloatType(1.0), FloatType(1.0))) == false);
	CHECK(vector_is_finite2(vector_set(FloatType(1.0), FloatType(inf), FloatType(1.0), FloatType(1.0))) == false);
	CHECK(vector_is_finite2(vector_set(FloatType(1.0), FloatType(1.0), FloatType(inf), FloatType(1.0))) == true);
	CHECK(vector_is_finite2(vector_set(FloatType(1.0), FloatType(1.0), FloatType(1.0), FloatType(inf))) == true);
	CHECK(vector_is_finite2(vector_set(nan, nan, nan, nan)) == false);
	CHECK(vector_is_finite2(vector_set(nan, FloatType(1.0), FloatType(1.0), FloatType(1.0))) == false);
	CHECK(vector_is_finite2(vector_set(FloatType(1.0), FloatType(nan), FloatType(1.0), FloatType(1.0))) == false);
	CHECK(vector_is_finite2(vector_set(FloatType(1.0), FloatType(1.0), FloatType(nan), FloatType(1.0))) == true);
	CHECK(vector_is_finite2(vector_set(FloatType(1.0), FloatType(1.0), FloatType(1.0), FloatType(nan))) == true);

	CHECK(vector_is_finite3(zero) == true);
	CHECK(vector_is_finite3(vector_set(inf, inf, inf, inf)) == false);
	CHECK(vector_is_finite3(vector_set(-inf, -inf, -inf, -inf)) == false);
	CHECK(vector_is_finite3(vector_set(inf, FloatType(1.0), FloatType(1.0), FloatType(1.0))) == false);
	CHECK(vector_is_finite3(vector_set(FloatType(1.0), FloatType(inf), FloatType(1.0), FloatType(1.0))) == false);
	CHECK(vector_is_finite3(vector_set(FloatType(1.0), FloatType(1.0), FloatType(inf), FloatType(1.0))) == false);
	CHECK(vector_is_finite3(vector_set(FloatType(1.0), FloatType(1.0), FloatType(1.0), FloatType(inf))) == true);
	CHECK(vector_is_finite3(vector_set(nan, nan, nan, nan)) == false);
	CHECK(vector_is_finite3(vector_set(nan, FloatType(1.0), FloatType(1.0), FloatType(1.0))) == false);
	CHECK(vector_is_finite3(vector_set(FloatType(1.0), FloatType(nan), FloatType(1.0), FloatType(1.0))) == false);
	CHECK(vector_is_finite3(vector_set(FloatType(1.0), FloatType(1.0), FloatType(nan), FloatType(1.0))) == false);
	CHECK(vector_is_finite3(vector_set(FloatType(1.0), FloatType(1.0), FloatType(1.0), FloatType(nan))) == true);

	//////////////////////////////////////////////////////////////////////////
	// Swizzling, permutations, and mixing

	CHECK(scalar_near_equal(vector_get_x(vector_select(vector_less_than(zero, vector_set(FloatType(1.0))), test_value0, test_value1)), test_value0_flt[0], threshold));
	CHECK(scalar_near_equal(vector_get_y(vector_select(vector_less_than(zero, vector_set(FloatType(1.0))), test_value0, test_value1)), test_value0_flt[1], threshold));
	CHECK(scalar_near_equal(vector_get_z(vector_select(vector_less_than(zero, vector_set(FloatType(1.0))), test_value0, test_value1)), test_value0_flt[2], threshold));
	CHECK(scalar_near_equal(vector_get_w(vector_select(vector_less_than(zero, vector_set(FloatType(1.0))), test_value0, test_value1)), test_value0_flt[3], threshold));
	CHECK(scalar_near_equal(vector_get_x(vector_select(vector_less_than(vector_set(FloatType(1.0)), zero), test_value0, test_value1)), test_value1_flt[0], threshold));
	CHECK(scalar_near_equal(vector_get_y(vector_select(vector_less_than(vector_set(FloatType(1.0)), zero), test_value0, test_value1)), test_value1_flt[1], threshold));
	CHECK(scalar_near_equal(vector_get_z(vector_select(vector_less_than(vector_set(FloatType(1.0)), zero), test_value0, test_value1)), test_value1_flt[2], threshold));
	CHECK(scalar_near_equal(vector_get_w(vector_select(vector_less_than(vector_set(FloatType(1.0)), zero), test_value0, test_value1)), test_value1_flt[3], threshold));

	//////////////////////////////////////////////////////////////////////////
	// Misc

	auto scalar_sign = [](FloatType value) { return value >= FloatType(0.0) ? FloatType(1.0) : FloatType(-1.0); };
	CHECK(FloatType(vector_get_x(vector_sign(test_value0))) == scalar_sign(test_value0_flt[0]));
	CHECK(FloatType(vector_get_y(vector_sign(test_value0))) == scalar_sign(test_value0_flt[1]));
	CHECK(FloatType(vector_get_z(vector_sign(test_value0))) == scalar_sign(test_value0_flt[2]));
	CHECK(FloatType(vector_get_w(vector_sign(test_value0))) == scalar_sign(test_value0_flt[3]));

	CHECK(FloatType(vector_get_x(vector_copy_sign(test_value0, test_value1))) == rtm_impl::copysign(test_value0_flt[0], test_value1_flt[0]));
	CHECK(FloatType(vector_get_y(vector_copy_sign(test_value0, test_value1))) == rtm_impl::copysign(test_value0_flt[1], test_value1_flt[1]));
	CHECK(FloatType(vector_get_z(vector_copy_sign(test_value0, test_value1))) == rtm_impl::copysign(test_value0_flt[2], test_value1_flt[2]));
	CHECK(FloatType(vector_get_w(vector_copy_sign(test_value0, test_value1))) == rtm_impl::copysign(test_value0_flt[3], test_value1_flt[3]));

	{
		const Vector4Type infinity_v = vector_set(std::numeric_limits<FloatType>::infinity());
		const Vector4Type nan_v = vector_set(std::numeric_limits<FloatType>::quiet_NaN());

		const Vector4Type input0 = vector_set(FloatType(-1.75), FloatType(-1.5), FloatType(-1.4999), FloatType(-0.5));
		const Vector4Type input1 = vector_set(FloatType(-0.4999), FloatType(0.0), FloatType(0.4999), FloatType(0.5));
		const Vector4Type input2 = vector_set(FloatType(1.4999), FloatType(1.5), FloatType(1.75), FloatType(0.0));
		const Vector4Type input3 = infinity_v;
		const Vector4Type input4 = vector_neg(infinity_v);
		const Vector4Type input5 = nan_v;

		const Vector4Type result0 = vector_round_symmetric(input0);
		const Vector4Type result1 = vector_round_symmetric(input1);
		const Vector4Type result2 = vector_round_symmetric(input2);
		const Vector4Type result3 = vector_round_symmetric(input3);
		const Vector4Type result4 = vector_round_symmetric(input4);
		const Vector4Type result5 = vector_round_symmetric(input5);
		const Vector4Type result6 = vector_round_symmetric(vector_set(FloatType(-0.0)));

		CHECK(FloatType(vector_get_x(result0)) == scalar_round_symmetric(FloatType(vector_get_x(input0))));
		CHECK(FloatType(vector_get_y(result0)) == scalar_round_symmetric(FloatType(vector_get_y(input0))));
		CHECK(FloatType(vector_get_z(result0)) == scalar_round_symmetric(FloatType(vector_get_z(input0))));
		CHECK(FloatType(vector_get_w(result0)) == scalar_round_symmetric(FloatType(vector_get_w(input0))));

		CHECK(FloatType(vector_get_x(result1)) == scalar_round_symmetric(FloatType(vector_get_x(input1))));
		CHECK(FloatType(vector_get_y(result1)) == scalar_round_symmetric(FloatType(vector_get_y(input1))));
		CHECK(FloatType(vector_get_z(result1)) == scalar_round_symmetric(FloatType(vector_get_z(input1))));
		CHECK(FloatType(vector_get_w(result1)) == scalar_round_symmetric(FloatType(vector_get_w(input1))));

		CHECK(FloatType(vector_get_x(result2)) == scalar_round_symmetric(FloatType(vector_get_x(input2))));
		CHECK(FloatType(vector_get_y(result2)) == scalar_round_symmetric(FloatType(vector_get_y(input2))));
		CHECK(FloatType(vector_get_z(result2)) == scalar_round_symmetric(FloatType(vector_get_z(input2))));
		CHECK(FloatType(vector_get_w(result2)) == scalar_round_symmetric(FloatType(vector_get_w(input2))));

		CHECK(FloatType(vector_get_x(result3)) == scalar_round_symmetric(FloatType(vector_get_x(input3))));
		CHECK(FloatType(vector_get_y(result3)) == scalar_round_symmetric(FloatType(vector_get_y(input3))));
		CHECK(FloatType(vector_get_z(result3)) == scalar_round_symmetric(FloatType(vector_get_z(input3))));
		CHECK(FloatType(vector_get_w(result3)) == scalar_round_symmetric(FloatType(vector_get_w(input3))));

		CHECK(FloatType(vector_get_x(result4)) == scalar_round_symmetric(FloatType(vector_get_x(input4))));
		CHECK(FloatType(vector_get_y(result4)) == scalar_round_symmetric(FloatType(vector_get_y(input4))));
		CHECK(FloatType(vector_get_z(result4)) == scalar_round_symmetric(FloatType(vector_get_z(input4))));
		CHECK(FloatType(vector_get_w(result4)) == scalar_round_symmetric(FloatType(vector_get_w(input4))));

		CHECK(FloatType(vector_get_x(result6)) == scalar_round_symmetric(FloatType(-0.0)));

		CHECK(std::isnan(FloatType(vector_get_x(result5))));
		CHECK(std::isnan(FloatType(vector_get_y(result5))));
		CHECK(std::isnan(FloatType(vector_get_z(result5))));
		CHECK(std::isnan(FloatType(vector_get_w(result5))));
	}

	{
		const Vector4Type infinity_v = vector_set(std::numeric_limits<FloatType>::infinity());
		const Vector4Type nan_v = vector_set(std::numeric_limits<FloatType>::quiet_NaN());

		const Vector4Type input0 = vector_set(FloatType(-1.75), FloatType(-1.5), FloatType(-1.4999), FloatType(-0.5));
		const Vector4Type input1 = vector_set(FloatType(-0.4999), FloatType(0.0), FloatType(0.4999), FloatType(0.5));
		const Vector4Type input2 = vector_set(FloatType(1.4999), FloatType(1.5), FloatType(1.75), FloatType(0.0));
		const Vector4Type input3 = infinity_v;
		const Vector4Type input4 = vector_neg(infinity_v);
		const Vector4Type input5 = nan_v;
		const Vector4Type input6 = vector_set(FloatType(2.5), FloatType(-2.5), FloatType(2.75), FloatType(-2.75));

		const Vector4Type result0 = vector_round_bankers(input0);
		const Vector4Type result1 = vector_round_bankers(input1);
		const Vector4Type result2 = vector_round_bankers(input2);
		const Vector4Type result3 = vector_round_bankers(input3);
		const Vector4Type result4 = vector_round_bankers(input4);
		const Vector4Type result5 = vector_round_bankers(input5);
		const Vector4Type result6 = vector_round_bankers(input6);
		const Vector4Type result7 = vector_round_symmetric(vector_set(FloatType(-0.0)));

		CHECK(FloatType(vector_get_x(result0)) == scalar_round_bankers(FloatType(vector_get_x(input0))));
		CHECK(FloatType(vector_get_y(result0)) == scalar_round_bankers(FloatType(vector_get_y(input0))));
		CHECK(FloatType(vector_get_z(result0)) == scalar_round_bankers(FloatType(vector_get_z(input0))));
		CHECK(FloatType(vector_get_w(result0)) == scalar_round_bankers(FloatType(vector_get_w(input0))));

		CHECK(FloatType(vector_get_x(result1)) == scalar_round_bankers(FloatType(vector_get_x(input1))));
		CHECK(FloatType(vector_get_y(result1)) == scalar_round_bankers(FloatType(vector_get_y(input1))));
		CHECK(FloatType(vector_get_z(result1)) == scalar_round_bankers(FloatType(vector_get_z(input1))));
		CHECK(FloatType(vector_get_w(result1)) == scalar_round_bankers(FloatType(vector_get_w(input1))));

		CHECK(FloatType(vector_get_x(result2)) == scalar_round_bankers(FloatType(vector_get_x(input2))));
		CHECK(FloatType(vector_get_y(result2)) == scalar_round_bankers(FloatType(vector_get_y(input2))));
		CHECK(FloatType(vector_get_z(result2)) == scalar_round_bankers(FloatType(vector_get_z(input2))));
		CHECK(FloatType(vector_get_w(result2)) == scalar_round_bankers(FloatType(vector_get_w(input2))));

		CHECK(FloatType(vector_get_x(result3)) == scalar_round_bankers(FloatType(vector_get_x(input3))));
		CHECK(FloatType(vector_get_y(result3)) == scalar_round_bankers(FloatType(vector_get_y(input3))));
		CHECK(FloatType(vector_get_z(result3)) == scalar_round_bankers(FloatType(vector_get_z(input3))));
		CHECK(FloatType(vector_get_w(result3)) == scalar_round_bankers(FloatType(vector_get_w(input3))));

		CHECK(FloatType(vector_get_x(result4)) == scalar_round_bankers(FloatType(vector_get_x(input4))));
		CHECK(FloatType(vector_get_y(result4)) == scalar_round_bankers(FloatType(vector_get_y(input4))));
		CHECK(FloatType(vector_get_z(result4)) == scalar_round_bankers(FloatType(vector_get_z(input4))));
		CHECK(FloatType(vector_get_w(result4)) == scalar_round_bankers(FloatType(vector_get_w(input4))));

		CHECK(std::isnan(FloatType(vector_get_x(result5))));
		CHECK(std::isnan(FloatType(vector_get_y(result5))));
		CHECK(std::isnan(FloatType(vector_get_z(result5))));
		CHECK(std::isnan(FloatType(vector_get_w(result5))));

		CHECK(FloatType(vector_get_x(result6)) == scalar_round_bankers(FloatType(vector_get_x(input6))));
		CHECK(FloatType(vector_get_y(result6)) == scalar_round_bankers(FloatType(vector_get_y(input6))));
		CHECK(FloatType(vector_get_z(result6)) == scalar_round_bankers(FloatType(vector_get_z(input6))));
		CHECK(FloatType(vector_get_w(result6)) == scalar_round_bankers(FloatType(vector_get_w(input6))));

		CHECK(FloatType(vector_get_x(result7)) == scalar_round_symmetric(FloatType(-0.0)));
	}

	{
		const FloatType half_pi = FloatType(rtm::constants::half_pi());
		const FloatType pi = FloatType(rtm::constants::pi());

		const FloatType angles[] = { FloatType(0.0), pi, -pi, half_pi, -half_pi, FloatType(0.5), FloatType(32.5), FloatType(-0.5), FloatType(-32.5) };

		for (const FloatType angle : angles)
		{
			INFO("angle: " << angle);

			const Vector4Type angle_v = vector_set(angle);

			const FloatType ref_sin = scalar_sin(angle);
			const FloatType ref_cos = scalar_cos(angle);
			const FloatType ref_tan = scalar_tan(angle);
			const Vector4Type ref_sincos = scalar_sincos(angle);
			const FloatType ref_asin = scalar_asin(ref_sin);
			const FloatType ref_acos = scalar_acos(ref_cos);

			const Vector4Type rtm_sin = vector_sin(angle_v);
			const Vector4Type rtm_cos = vector_cos(angle_v);
			Vector4Type rtm_sin2;
			Vector4Type rtm_cos2;
			vector_sincos(angle_v, rtm_sin2, rtm_cos2);
			const Vector4Type rtm_tan = vector_tan(angle_v);
			const Vector4Type rtm_asin = vector_asin(vector_set(ref_sin));
			const Vector4Type rtm_acos = vector_acos(vector_set(ref_cos));

			CHECK(scalar_near_equal(FloatType(vector_get_x(rtm_sin)), ref_sin, threshold));
			CHECK(scalar_near_equal(FloatType(vector_get_y(rtm_sin)), ref_sin, threshold));
			CHECK(scalar_near_equal(FloatType(vector_get_z(rtm_sin)), ref_sin, threshold));
			CHECK(scalar_near_equal(FloatType(vector_get_w(rtm_sin)), ref_sin, threshold));

			CHECK(scalar_near_equal(FloatType(vector_get_x(rtm_asin)), ref_asin, threshold));
			CHECK(scalar_near_equal(FloatType(vector_get_y(rtm_asin)), ref_asin, threshold));
			CHECK(scalar_near_equal(FloatType(vector_get_z(rtm_asin)), ref_asin, threshold));
			CHECK(scalar_near_equal(FloatType(vector_get_w(rtm_asin)), ref_asin, threshold));

			CHECK(scalar_near_equal(FloatType(vector_get_x(rtm_cos)), ref_cos, threshold));
			CHECK(scalar_near_equal(FloatType(vector_get_y(rtm_cos)), ref_cos, threshold));
			CHECK(scalar_near_equal(FloatType(vector_get_z(rtm_cos)), ref_cos, threshold));
			CHECK(scalar_near_equal(FloatType(vector_get_w(rtm_cos)), ref_cos, threshold));

			CHECK(scalar_near_equal(FloatType(vector_get_x(rtm_acos)), ref_acos, threshold));
			CHECK(scalar_near_equal(FloatType(vector_get_y(rtm_acos)), ref_acos, threshold));
			CHECK(scalar_near_equal(FloatType(vector_get_z(rtm_acos)), ref_acos, threshold));
			CHECK(scalar_near_equal(FloatType(vector_get_w(rtm_acos)), ref_acos, threshold));

			CHECK(scalar_near_equal(FloatType(vector_get_x(rtm_sin2)), (FloatType)vector_get_x(ref_sincos), threshold));
			CHECK(scalar_near_equal(FloatType(vector_get_y(rtm_sin2)), (FloatType)vector_get_x(ref_sincos), threshold));
			CHECK(scalar_near_equal(FloatType(vector_get_z(rtm_sin2)), (FloatType)vector_get_x(ref_sincos), threshold));
			CHECK(scalar_near_equal(FloatType(vector_get_w(rtm_sin2)), (FloatType)vector_get_x(ref_sincos), threshold));
			CHECK(scalar_near_equal(FloatType(vector_get_x(rtm_cos2)), (FloatType)vector_get_y(ref_sincos), threshold));
			CHECK(scalar_near_equal(FloatType(vector_get_y(rtm_cos2)), (FloatType)vector_get_y(ref_sincos), threshold));
			CHECK(scalar_near_equal(FloatType(vector_get_z(rtm_cos2)), (FloatType)vector_get_y(ref_sincos), threshold));
			CHECK(scalar_near_equal(FloatType(vector_get_w(rtm_cos2)), (FloatType)vector_get_y(ref_sincos), threshold));

			// For +-PI/2, we only test that the value is really large or really small
			if (scalar_abs(angle) == half_pi)
			{
				CHECK(vector_all_greater_equal(vector_abs(rtm_tan), vector_set(FloatType(1.0e6))));
			}
			else
			{
				CHECK(scalar_near_equal(FloatType(vector_get_x(rtm_tan)), ref_tan, threshold));
				CHECK(scalar_near_equal(FloatType(vector_get_y(rtm_tan)), ref_tan, threshold));
				CHECK(scalar_near_equal(FloatType(vector_get_z(rtm_tan)), ref_tan, threshold));
				CHECK(scalar_near_equal(FloatType(vector_get_w(rtm_tan)), ref_tan, threshold));
			}
		}

		const FloatType angles_atan[] = { FloatType(-10.0), FloatType(-5.0), FloatType(-0.5), FloatType(-0.25), FloatType(0.0), FloatType(0.25), FloatType(0.5), FloatType(0.75), FloatType(81.0) };
		for (const FloatType angle : angles_atan)
		{
			const Vector4Type angle_v = vector_set(angle);

			CHECK(scalar_near_equal(FloatType(vector_get_x(vector_atan(angle_v))), scalar_atan(angle), threshold));
			CHECK(scalar_near_equal(FloatType(vector_get_y(vector_atan(angle_v))), scalar_atan(angle), threshold));
			CHECK(scalar_near_equal(FloatType(vector_get_z(vector_atan(angle_v))), scalar_atan(angle), threshold));
			CHECK(scalar_near_equal(FloatType(vector_get_w(vector_atan(angle_v))), scalar_atan(angle), threshold));
		}

		const std::pair<FloatType, FloatType> angles_atan2[] =
		{
			{ FloatType(-2.0), FloatType(-2.0) },
			{ FloatType(-1.0), FloatType(-2.0) },
			{ FloatType(-2.0), FloatType(-1.0) },
			{ FloatType(2.0), FloatType(2.0) },
			{ FloatType(1.0), FloatType(2.0) },
			{ FloatType(2.0), FloatType(1.0) },
			{ FloatType(2.0), FloatType(0.0) },
			{ FloatType(-2.0), FloatType(0.0) },
			{ FloatType(2.0), FloatType(-0.0) },
			{ FloatType(-2.0), FloatType(-0.0) },
			{ FloatType(0.0), FloatType(2.0) },
			{ FloatType(0.0), FloatType(-2.0) },
			{ FloatType(-0.0), FloatType(2.0) },
			{ FloatType(-0.0), FloatType(-2.0) },
		};
		for (const std::pair<FloatType, FloatType>& angles_ : angles_atan2)
		{
			const Vector4Type angle_y = vector_set(angles_.first);
			const Vector4Type angle_x = vector_set(angles_.second);

			CHECK(scalar_near_equal(FloatType(vector_get_x(vector_atan2(angle_y, angle_x))), scalar_atan2(angles_.first, angles_.second), threshold));
			CHECK(scalar_near_equal(FloatType(vector_get_y(vector_atan2(angle_y, angle_x))), scalar_atan2(angles_.first, angles_.second), threshold));
			CHECK(scalar_near_equal(FloatType(vector_get_z(vector_atan2(angle_y, angle_x))), scalar_atan2(angles_.first, angles_.second), threshold));
			CHECK(scalar_near_equal(FloatType(vector_get_w(vector_atan2(angle_y, angle_x))), scalar_atan2(angles_.first, angles_.second), threshold));
		}
	}
}

```

`tests/sources/test_vector4d.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2018 Nicholas Frechette & Animation Compression Library contributors
// Copyright (c) 2018 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include "catch2.impl.h"

#include <rtm/scalard.h>
#include <rtm/vector4d.h>
#include <rtm/quatd.h>
#include <rtm/mask4d.h>

#include "test_vector4_impl.h"

TEST_CASE("vector4d math get/set", "[math][vector4]")
{
	test_vector4_getset_impl<double>();
}

TEST_CASE("vector4d math arithmetic", "[math][vector4]")
{
	test_vector4_arithmetic_impl<double>(1.0E-9);
}

TEST_CASE("vector4d math relational", "[math][vector4]")
{
	test_vector4_relational_impl<double>(1.0E-9);
}

TEST_CASE("vector4d math logical", "[math][vector4]")
{
	test_vector4_logical_impl<double>();
}

TEST_CASE("vector4d math misc", "[math][vector4]")
{
	test_vector4_impl<double>(1.0E-9);
}

```

`tests/sources/test_vector4f.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2018 Nicholas Frechette & Animation Compression Library contributors
// Copyright (c) 2018 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include "catch2.impl.h"

#include <rtm/scalarf.h>
#include <rtm/vector4f.h>
#include <rtm/quatf.h>
#include <rtm/mask4f.h>

#include "test_vector4_impl.h"

TEST_CASE("vector4f math get/set", "[math][vector4]")
{
	test_vector4_getset_impl<float>();
}

TEST_CASE("vector4f math arithmetic", "[math][vector4]")
{
#if defined(RTM_NO_INTRINSICS)
	const float threshold = 1.0E-4F;
#else
	const float threshold = 1.0E-5F;
#endif

	test_vector4_arithmetic_impl<float>(threshold);
}

TEST_CASE("vector4f math relational", "[math][vector4]")
{
#if defined(RTM_NO_INTRINSICS)
	const float threshold = 1.0E-4F;
#else
	const float threshold = 1.0E-5F;
#endif

	test_vector4_relational_impl<float>(threshold);
}

TEST_CASE("vector4f math logical", "[math][vector4]")
{
	test_vector4_logical_impl<float>();
}

TEST_CASE("vector4f math misc", "[math][vector4]")
{
#if defined(RTM_NO_INTRINSICS)
	const float threshold = 1.0E-4F;
#else
	const float threshold = 1.0E-5F;
#endif

	test_vector4_impl<float>(threshold);
}

```

`tests/sources/test_vqm.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2024 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include "catch2.impl.h"

#include <rtm/matrix3x4f.h>
#include <rtm/matrix3x4d.h>
#include <rtm/type_traits.h>
#include <rtm/experimental/vqmf.h>
#include <rtm/experimental/vqmd.h>

using namespace rtm;

template<typename TransformType, typename FloatType>
static void test_vqm_impl(const FloatType threshold)
{
	using QuatType = typename related_types<FloatType>::quat;
	using Vector4Type = typename related_types<FloatType>::vector4;
	using Matrix3x4Type = typename related_types<FloatType>::matrix3x4;

	// Identity validation
	{
		Vector4Type point = vector_set(FloatType(12.0), FloatType(0.0), FloatType(-130.033));

		TransformType identity = vqm_identity();

		Vector4Type mul_point_result = vqm_mul_point3(point, identity);
		CHECK(vector_all_near_equal3(mul_point_result, point, threshold));

		TransformType mul_itself_result = vqm_mul(identity, identity);
		CHECK(quat_near_equal(mul_itself_result.rotation, identity.rotation, threshold));
		CHECK(vector_all_near_equal3(mul_itself_result.x_axis, identity.x_axis, threshold));
		CHECK(vector_all_near_equal3(mul_itself_result.y_axis, identity.y_axis, threshold));
		CHECK(vector_all_near_equal3(mul_itself_result.z_axis, identity.z_axis, threshold));
		CHECK(vector_all_near_equal3(mul_itself_result.translation, identity.translation, threshold));

		TransformType inverse_result = vqm_inverse(identity);
		CHECK(quat_near_equal(inverse_result.rotation, identity.rotation, threshold));
		CHECK(vector_all_near_equal3(inverse_result.x_axis, identity.x_axis, threshold));
		CHECK(vector_all_near_equal3(inverse_result.y_axis, identity.y_axis, threshold));
		CHECK(vector_all_near_equal3(inverse_result.z_axis, identity.z_axis, threshold));
		CHECK(vector_all_near_equal3(inverse_result.translation, identity.translation, threshold));
	}

	// Getters and setters
	{
		QuatType rotation = quat_from_euler(scalar_deg_to_rad(FloatType(10.1)), scalar_deg_to_rad(FloatType(41.6)), scalar_deg_to_rad(FloatType(-12.7)));
		Vector4Type translation = vector_set(FloatType(1.0), FloatType(2.0), FloatType(3.0));
		Vector4Type scale = vector_set(FloatType(4.0), FloatType(5.0), FloatType(6.0));

		TransformType identity = vqm_identity();
		TransformType tx = vqm_set_rotation(identity, rotation);
		CHECK(quat_near_equal(vqm_get_rotation(tx), rotation, threshold));
		CHECK(vector_all_near_equal3(tx.x_axis, identity.x_axis, threshold));
		CHECK(vector_all_near_equal3(tx.y_axis, identity.y_axis, threshold));
		CHECK(vector_all_near_equal3(tx.z_axis, identity.z_axis, threshold));
		CHECK(vector_all_near_equal3(tx.translation, identity.translation, threshold));

		tx = vqm_set_translation(tx, translation);
		CHECK(quat_near_equal(vqm_get_rotation(tx), rotation, threshold));
		CHECK(vector_all_near_equal3(tx.x_axis, identity.x_axis, threshold));
		CHECK(vector_all_near_equal3(tx.y_axis, identity.y_axis, threshold));
		CHECK(vector_all_near_equal3(tx.z_axis, identity.z_axis, threshold));
		CHECK(vector_all_near_equal3(vqm_get_translation(tx), translation, threshold));

		tx = vqm_set_scale(tx, scale);
		CHECK(quat_near_equal(vqm_get_rotation(tx), rotation, threshold));
		CHECK(vector_all_near_equal3(vqm_get_scale(tx), scale, threshold));
		CHECK(vector_all_near_equal3(vqm_get_translation(tx), translation, threshold));
	}

	// Matrix conversion validation
	{
		QuatType rotation = quat_from_euler(scalar_deg_to_rad(FloatType(10.1)), scalar_deg_to_rad(FloatType(41.6)), scalar_deg_to_rad(FloatType(-12.7)));
		Vector4Type translation = vector_set(FloatType(1.0), FloatType(2.0), FloatType(3.0));
		Vector4Type scale = vector_set(FloatType(4.0), FloatType(5.0), FloatType(6.0));

		Matrix3x4Type src_mtx = matrix_from_qvv(rotation, translation, scale);
		TransformType dst_tx = vqm_set(translation, rotation, scale);
		Matrix3x4Type dst_mtx = vqm_to_matrix(dst_tx);
		CHECK(vector_all_near_equal3(src_mtx.x_axis, dst_mtx.x_axis, threshold));
		CHECK(vector_all_near_equal3(src_mtx.y_axis, dst_mtx.y_axis, threshold));
		CHECK(vector_all_near_equal3(src_mtx.z_axis, dst_mtx.z_axis, threshold));
		CHECK(vector_all_near_equal3(src_mtx.w_axis, dst_mtx.w_axis, threshold));
	}

	// VQM + VQM validation
	{
		// TODO
	}

	// VQM * VQM validation
	{
		QuatType rotation = quat_from_euler(scalar_deg_to_rad(FloatType(10.1)), scalar_deg_to_rad(FloatType(41.6)), scalar_deg_to_rad(FloatType(-12.7)));
		Vector4Type translation = vector_set(FloatType(1.0), FloatType(2.0), FloatType(3.0));

		// All positive scale
		Vector4Type scale = vector_set(FloatType(4.0), FloatType(5.0), FloatType(6.0));

		Matrix3x4Type src_mtx = matrix_from_qvv(rotation, translation, scale);
		src_mtx = matrix_mul(src_mtx, src_mtx);

		TransformType dst_tx = vqm_set(translation, rotation, scale);
		dst_tx = vqm_mul(dst_tx, dst_tx);
		Matrix3x4Type dst_mtx = vqm_to_matrix(dst_tx);
		CHECK(vector_all_near_equal3(src_mtx.x_axis, dst_mtx.x_axis, threshold));
		CHECK(vector_all_near_equal3(src_mtx.y_axis, dst_mtx.y_axis, threshold));
		CHECK(vector_all_near_equal3(src_mtx.z_axis, dst_mtx.z_axis, threshold));
		CHECK(vector_all_near_equal3(src_mtx.w_axis, dst_mtx.w_axis, threshold));

		// One negative scale
		scale = vector_set(FloatType(-4.0), FloatType(5.0), FloatType(6.0));

		src_mtx = matrix_from_qvv(rotation, translation, scale);
		src_mtx = matrix_mul(src_mtx, src_mtx);

		dst_tx = vqm_set(translation, rotation, scale);
		dst_tx = vqm_mul(dst_tx, dst_tx);
		dst_mtx = vqm_to_matrix(dst_tx);
		CHECK(vector_all_near_equal3(src_mtx.x_axis, dst_mtx.x_axis, threshold));
		CHECK(vector_all_near_equal3(src_mtx.y_axis, dst_mtx.y_axis, threshold));
		CHECK(vector_all_near_equal3(src_mtx.z_axis, dst_mtx.z_axis, threshold));
		CHECK(vector_all_near_equal3(src_mtx.w_axis, dst_mtx.w_axis, threshold));

		// Two negative scale
		scale = vector_set(FloatType(-4.0), FloatType(-5.0), FloatType(6.0));

		src_mtx = matrix_from_qvv(rotation, translation, scale);
		src_mtx = matrix_mul(src_mtx, src_mtx);

		dst_tx = vqm_set(translation, rotation, scale);
		dst_tx = vqm_mul(dst_tx, dst_tx);
		dst_mtx = vqm_to_matrix(dst_tx);
		CHECK(vector_all_near_equal3(src_mtx.x_axis, dst_mtx.x_axis, threshold));
		CHECK(vector_all_near_equal3(src_mtx.y_axis, dst_mtx.y_axis, threshold));
		CHECK(vector_all_near_equal3(src_mtx.z_axis, dst_mtx.z_axis, threshold));
		CHECK(vector_all_near_equal3(src_mtx.w_axis, dst_mtx.w_axis, threshold));

		// Three negative scale
		scale = vector_set(FloatType(-4.0), FloatType(-5.0), FloatType(-6.0));

		src_mtx = matrix_from_qvv(rotation, translation, scale);
		src_mtx = matrix_mul(src_mtx, src_mtx);

		dst_tx = vqm_set(translation, rotation, scale);
		dst_tx = vqm_mul(dst_tx, dst_tx);
		dst_mtx = vqm_to_matrix(dst_tx);
		CHECK(vector_all_near_equal3(src_mtx.x_axis, dst_mtx.x_axis, threshold));
		CHECK(vector_all_near_equal3(src_mtx.y_axis, dst_mtx.y_axis, threshold));
		CHECK(vector_all_near_equal3(src_mtx.z_axis, dst_mtx.z_axis, threshold));
		CHECK(vector_all_near_equal3(src_mtx.w_axis, dst_mtx.w_axis, threshold));

		// One zero scale
		scale = vector_set(FloatType(0.0), FloatType(5.0), FloatType(6.0));

		src_mtx = matrix_from_qvv(rotation, translation, scale);
		src_mtx = matrix_mul(src_mtx, src_mtx);

		dst_tx = vqm_set(translation, rotation, scale);
		dst_tx = vqm_mul(dst_tx, dst_tx);
		dst_mtx = vqm_to_matrix(dst_tx);
		CHECK(vector_all_near_equal3(src_mtx.x_axis, dst_mtx.x_axis, threshold));
		CHECK(vector_all_near_equal3(src_mtx.y_axis, dst_mtx.y_axis, threshold));
		CHECK(vector_all_near_equal3(src_mtx.z_axis, dst_mtx.z_axis, threshold));
		CHECK(vector_all_near_equal3(src_mtx.w_axis, dst_mtx.w_axis, threshold));

		// Two zero scale
		scale = vector_set(FloatType(0.0), FloatType(0.0), FloatType(6.0));

		src_mtx = matrix_from_qvv(rotation, translation, scale);
		src_mtx = matrix_mul(src_mtx, src_mtx);

		dst_tx = vqm_set(translation, rotation, scale);
		dst_tx = vqm_mul(dst_tx, dst_tx);
		dst_mtx = vqm_to_matrix(dst_tx);
		CHECK(vector_all_near_equal3(src_mtx.x_axis, dst_mtx.x_axis, threshold));
		CHECK(vector_all_near_equal3(src_mtx.y_axis, dst_mtx.y_axis, threshold));
		CHECK(vector_all_near_equal3(src_mtx.z_axis, dst_mtx.z_axis, threshold));
		CHECK(vector_all_near_equal3(src_mtx.w_axis, dst_mtx.w_axis, threshold));

		// Three zero scale
		scale = vector_set(FloatType(0.0), FloatType(0.0), FloatType(0.0));

		src_mtx = matrix_from_qvv(rotation, translation, scale);
		src_mtx = matrix_mul(src_mtx, src_mtx);

		dst_tx = vqm_set(translation, rotation, scale);
		dst_tx = vqm_mul(dst_tx, dst_tx);
		dst_mtx = vqm_to_matrix(dst_tx);
		CHECK(vector_all_near_equal3(src_mtx.x_axis, dst_mtx.x_axis, threshold));
		CHECK(vector_all_near_equal3(src_mtx.y_axis, dst_mtx.y_axis, threshold));
		CHECK(vector_all_near_equal3(src_mtx.z_axis, dst_mtx.z_axis, threshold));
		CHECK(vector_all_near_equal3(src_mtx.w_axis, dst_mtx.w_axis, threshold));
	}

	// VQM * scalar validation
	{
		// TODO
	}

	// point/vec3 * VQM validation
	{
		Vector4Type point = vector_set(FloatType(12.0), FloatType(0.0), FloatType(-130.033));

		QuatType rotation = quat_from_euler(scalar_deg_to_rad(FloatType(10.1)), scalar_deg_to_rad(FloatType(41.6)), scalar_deg_to_rad(FloatType(-12.7)));
		Vector4Type translation = vector_set(FloatType(1.0), FloatType(2.0), FloatType(3.0));

		// All positive scale
		Vector4Type scale = vector_set(FloatType(4.0), FloatType(5.0), FloatType(6.0));

		Matrix3x4Type src_mtx = matrix_from_qvv(rotation, translation, scale);
		Vector4Type src_point = matrix_mul_point3(point, src_mtx);

		TransformType dst_tx = vqm_set(translation, rotation, scale);
		Vector4Type dst_point = vqm_mul_point3(point, dst_tx);
		CHECK(vector_all_near_equal3(src_point, dst_point, threshold));

		src_point = matrix_mul_vector3(point, src_mtx);
		dst_point = vqm_mul_vector3(point, dst_tx);
		CHECK(vector_all_near_equal3(src_point, dst_point, threshold));

		// One negative scale
		scale = vector_set(FloatType(-4.0), FloatType(5.0), FloatType(6.0));

		src_mtx = matrix_from_qvv(rotation, translation, scale);
		src_point = matrix_mul_point3(point, src_mtx);

		dst_tx = vqm_set(translation, rotation, scale);
		dst_point = vqm_mul_point3(point, dst_tx);
		CHECK(vector_all_near_equal3(src_point, dst_point, threshold));

		src_point = matrix_mul_vector3(point, src_mtx);
		dst_point = vqm_mul_vector3(point, dst_tx);
		CHECK(vector_all_near_equal3(src_point, dst_point, threshold));

		// Two negative scale
		scale = vector_set(FloatType(-4.0), FloatType(-5.0), FloatType(6.0));

		src_mtx = matrix_from_qvv(rotation, translation, scale);
		src_point = matrix_mul_point3(point, src_mtx);

		dst_tx = vqm_set(translation, rotation, scale);
		dst_point = vqm_mul_point3(point, dst_tx);
		CHECK(vector_all_near_equal3(src_point, dst_point, threshold));

		src_point = matrix_mul_vector3(point, src_mtx);
		dst_point = vqm_mul_vector3(point, dst_tx);
		CHECK(vector_all_near_equal3(src_point, dst_point, threshold));

		// Three negative scale
		scale = vector_set(FloatType(-4.0), FloatType(-5.0), FloatType(-6.0));

		src_mtx = matrix_from_qvv(rotation, translation, scale);
		src_point = matrix_mul_point3(point, src_mtx);

		dst_tx = vqm_set(translation, rotation, scale);
		dst_point = vqm_mul_point3(point, dst_tx);
		CHECK(vector_all_near_equal3(src_point, dst_point, threshold));

		src_point = matrix_mul_vector3(point, src_mtx);
		dst_point = vqm_mul_vector3(point, dst_tx);
		CHECK(vector_all_near_equal3(src_point, dst_point, threshold));

		// One zero scale
		scale = vector_set(FloatType(0.0), FloatType(5.0), FloatType(6.0));

		src_mtx = matrix_from_qvv(rotation, translation, scale);
		src_point = matrix_mul_point3(point, src_mtx);

		dst_tx = vqm_set(translation, rotation, scale);
		dst_point = vqm_mul_point3(point, dst_tx);
		CHECK(vector_all_near_equal3(src_point, dst_point, threshold));

		src_point = matrix_mul_vector3(point, src_mtx);
		dst_point = vqm_mul_vector3(point, dst_tx);
		CHECK(vector_all_near_equal3(src_point, dst_point, threshold));

		// Two zero scale
		scale = vector_set(FloatType(0.0), FloatType(0.0), FloatType(6.0));

		src_mtx = matrix_from_qvv(rotation, translation, scale);
		src_point = matrix_mul_point3(point, src_mtx);

		dst_tx = vqm_set(translation, rotation, scale);
		dst_point = vqm_mul_point3(point, dst_tx);
		CHECK(vector_all_near_equal3(src_point, dst_point, threshold));

		src_point = matrix_mul_vector3(point, src_mtx);
		dst_point = vqm_mul_vector3(point, dst_tx);
		CHECK(vector_all_near_equal3(src_point, dst_point, threshold));

		// Three zero scale
		scale = vector_set(FloatType(0.0), FloatType(0.0), FloatType(0.0));

		src_mtx = matrix_from_qvv(rotation, translation, scale);
		src_point = matrix_mul_point3(point, src_mtx);

		dst_tx = vqm_set(translation, rotation, scale);
		dst_point = vqm_mul_point3(point, dst_tx);
		CHECK(vector_all_near_equal3(src_point, dst_point, threshold));

		src_point = matrix_mul_vector3(point, src_mtx);
		dst_point = vqm_mul_vector3(point, dst_tx);
		CHECK(vector_all_near_equal3(src_point, dst_point, threshold));
	}

	// VQM inverse validation
	{
		QuatType rotation = quat_from_euler(scalar_deg_to_rad(FloatType(10.1)), scalar_deg_to_rad(FloatType(41.6)), scalar_deg_to_rad(FloatType(-12.7)));
		Vector4Type translation = vector_set(FloatType(1.0), FloatType(2.0), FloatType(3.0));

		// All positive scale
		Vector4Type scale = vector_set(FloatType(4.0), FloatType(5.0), FloatType(6.0));

		Matrix3x4Type src_mtx = matrix_from_qvv(rotation, translation, scale);
		Matrix3x4Type inv_src_mtx = matrix_inverse(src_mtx);

		TransformType dst_tx = vqm_set(translation, rotation, scale);
		TransformType inv_dst_tx = vqm_inverse(dst_tx);

		Matrix3x4Type inv_dst_mtx = vqm_to_matrix(inv_dst_tx);
		CHECK(vector_all_near_equal3(inv_src_mtx.x_axis, inv_dst_mtx.x_axis, threshold));
		CHECK(vector_all_near_equal3(inv_src_mtx.y_axis, inv_dst_mtx.y_axis, threshold));
		CHECK(vector_all_near_equal3(inv_src_mtx.z_axis, inv_dst_mtx.z_axis, threshold));
		CHECK(vector_all_near_equal3(inv_src_mtx.w_axis, inv_dst_mtx.w_axis, threshold));

		// T * T^-1 = identity
		TransformType identity = vqm_identity();
		TransformType inverse_mul_result = vqm_mul(dst_tx, vqm_inverse(dst_tx));
		CHECK(quat_near_equal(inverse_mul_result.rotation, identity.rotation, threshold));
		CHECK(vector_all_near_equal3(inverse_mul_result.x_axis, identity.x_axis, threshold));
		CHECK(vector_all_near_equal3(inverse_mul_result.y_axis, identity.y_axis, threshold));
		CHECK(vector_all_near_equal3(inverse_mul_result.z_axis, identity.z_axis, threshold));
		CHECK(vector_all_near_equal3(inverse_mul_result.translation, identity.translation, threshold));

		// T^-1 * T = identity
		inverse_mul_result = vqm_mul(vqm_inverse(dst_tx), dst_tx);
		CHECK(quat_near_equal(inverse_mul_result.rotation, identity.rotation, threshold));
		CHECK(vector_all_near_equal3(inverse_mul_result.x_axis, identity.x_axis, threshold));
		CHECK(vector_all_near_equal3(inverse_mul_result.y_axis, identity.y_axis, threshold));
		CHECK(vector_all_near_equal3(inverse_mul_result.z_axis, identity.z_axis, threshold));
		CHECK(vector_all_near_equal3(inverse_mul_result.translation, identity.translation, threshold));

		// One negative scale
		scale = vector_set(FloatType(-4.0), FloatType(5.0), FloatType(6.0));

		src_mtx = matrix_from_qvv(rotation, translation, scale);
		inv_src_mtx = matrix_inverse(src_mtx);

		dst_tx = vqm_set(translation, rotation, scale);
		inv_dst_tx = vqm_inverse(dst_tx);

		inv_dst_mtx = vqm_to_matrix(inv_dst_tx);
		CHECK(vector_all_near_equal3(inv_src_mtx.x_axis, inv_dst_mtx.x_axis, threshold));
		CHECK(vector_all_near_equal3(inv_src_mtx.y_axis, inv_dst_mtx.y_axis, threshold));
		CHECK(vector_all_near_equal3(inv_src_mtx.z_axis, inv_dst_mtx.z_axis, threshold));
		CHECK(vector_all_near_equal3(inv_src_mtx.w_axis, inv_dst_mtx.w_axis, threshold));

		// T * T^-1 = identity
		identity = vqm_identity();
		inverse_mul_result = vqm_mul(dst_tx, vqm_inverse(dst_tx));
		CHECK(quat_near_equal(inverse_mul_result.rotation, identity.rotation, threshold));
		CHECK(vector_all_near_equal3(inverse_mul_result.x_axis, identity.x_axis, threshold));
		CHECK(vector_all_near_equal3(inverse_mul_result.y_axis, identity.y_axis, threshold));
		CHECK(vector_all_near_equal3(inverse_mul_result.z_axis, identity.z_axis, threshold));
		CHECK(vector_all_near_equal3(inverse_mul_result.translation, identity.translation, threshold));

		// T^-1 * T = identity
		inverse_mul_result = vqm_mul(vqm_inverse(dst_tx), dst_tx);
		CHECK(quat_near_equal(inverse_mul_result.rotation, identity.rotation, threshold));
		CHECK(vector_all_near_equal3(inverse_mul_result.x_axis, identity.x_axis, threshold));
		CHECK(vector_all_near_equal3(inverse_mul_result.y_axis, identity.y_axis, threshold));
		CHECK(vector_all_near_equal3(inverse_mul_result.z_axis, identity.z_axis, threshold));
		CHECK(vector_all_near_equal3(inverse_mul_result.translation, identity.translation, threshold));

		// Two negative scale
		scale = vector_set(FloatType(-4.0), FloatType(-5.0), FloatType(6.0));

		src_mtx = matrix_from_qvv(rotation, translation, scale);
		inv_src_mtx = matrix_inverse(src_mtx);

		dst_tx = vqm_set(translation, rotation, scale);
		inv_dst_tx = vqm_inverse(dst_tx);

		inv_dst_mtx = vqm_to_matrix(inv_dst_tx);
		CHECK(vector_all_near_equal3(inv_src_mtx.x_axis, inv_dst_mtx.x_axis, threshold));
		CHECK(vector_all_near_equal3(inv_src_mtx.y_axis, inv_dst_mtx.y_axis, threshold));
		CHECK(vector_all_near_equal3(inv_src_mtx.z_axis, inv_dst_mtx.z_axis, threshold));
		CHECK(vector_all_near_equal3(inv_src_mtx.w_axis, inv_dst_mtx.w_axis, threshold));

		// T * T^-1 = identity
		identity = vqm_identity();
		inverse_mul_result = vqm_mul(dst_tx, vqm_inverse(dst_tx));
		CHECK(quat_near_equal(inverse_mul_result.rotation, identity.rotation, threshold));
		CHECK(vector_all_near_equal3(inverse_mul_result.x_axis, identity.x_axis, threshold));
		CHECK(vector_all_near_equal3(inverse_mul_result.y_axis, identity.y_axis, threshold));
		CHECK(vector_all_near_equal3(inverse_mul_result.z_axis, identity.z_axis, threshold));
		CHECK(vector_all_near_equal3(inverse_mul_result.translation, identity.translation, threshold));

		// T^-1 * T = identity
		inverse_mul_result = vqm_mul(vqm_inverse(dst_tx), dst_tx);
		CHECK(quat_near_equal(inverse_mul_result.rotation, identity.rotation, threshold));
		CHECK(vector_all_near_equal3(inverse_mul_result.x_axis, identity.x_axis, threshold));
		CHECK(vector_all_near_equal3(inverse_mul_result.y_axis, identity.y_axis, threshold));
		CHECK(vector_all_near_equal3(inverse_mul_result.z_axis, identity.z_axis, threshold));
		CHECK(vector_all_near_equal3(inverse_mul_result.translation, identity.translation, threshold));

		// Three negative scale
		scale = vector_set(FloatType(-4.0), FloatType(-5.0), FloatType(-6.0));

		src_mtx = matrix_from_qvv(rotation, translation, scale);
		inv_src_mtx = matrix_inverse(src_mtx);

		dst_tx = vqm_set(translation, rotation, scale);
		inv_dst_tx = vqm_inverse(dst_tx);

		inv_dst_mtx = vqm_to_matrix(inv_dst_tx);
		CHECK(vector_all_near_equal3(inv_src_mtx.x_axis, inv_dst_mtx.x_axis, threshold));
		CHECK(vector_all_near_equal3(inv_src_mtx.y_axis, inv_dst_mtx.y_axis, threshold));
		CHECK(vector_all_near_equal3(inv_src_mtx.z_axis, inv_dst_mtx.z_axis, threshold));
		CHECK(vector_all_near_equal3(inv_src_mtx.w_axis, inv_dst_mtx.w_axis, threshold));

		// T * T^-1 = identity
		identity = vqm_identity();
		inverse_mul_result = vqm_mul(dst_tx, vqm_inverse(dst_tx));
		CHECK(quat_near_equal(inverse_mul_result.rotation, identity.rotation, threshold));
		CHECK(vector_all_near_equal3(inverse_mul_result.x_axis, identity.x_axis, threshold));
		CHECK(vector_all_near_equal3(inverse_mul_result.y_axis, identity.y_axis, threshold));
		CHECK(vector_all_near_equal3(inverse_mul_result.z_axis, identity.z_axis, threshold));
		CHECK(vector_all_near_equal3(inverse_mul_result.translation, identity.translation, threshold));

		// T^-1 * T = identity
		inverse_mul_result = vqm_mul(vqm_inverse(dst_tx), dst_tx);
		CHECK(quat_near_equal(inverse_mul_result.rotation, identity.rotation, threshold));
		CHECK(vector_all_near_equal3(inverse_mul_result.x_axis, identity.x_axis, threshold));
		CHECK(vector_all_near_equal3(inverse_mul_result.y_axis, identity.y_axis, threshold));
		CHECK(vector_all_near_equal3(inverse_mul_result.z_axis, identity.z_axis, threshold));
		CHECK(vector_all_near_equal3(inverse_mul_result.translation, identity.translation, threshold));
	}
}

TEST_CASE("vqmf math", "[math][vqm]")
{
	test_vqm_impl<vqmf, float>(1.0E-3F);
}

TEST_CASE("vqmd math", "[math][vqm]")
{
	test_vqm_impl<vqmd, double>(1.0E-8);
}

```

`tests/sources/vector_mix/test_vector4_mix_impl.h`:

```h
#pragma once

////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2019 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include "catch2.impl.h"

// cpp files that include this header must include the vector header they wish to use
//#include <rtm/vector4f.h>
//#include <rtm/vector4d.h>

using namespace rtm;

template<typename Vector4Type, typename FloatType, mix4 XArg, mix4 YArg>
void test_vector_mix_impl(const FloatType threshold)
{
	(void)threshold;

#if defined(RTM_IMPL_WITH_VECTOR_MIX_TESTS)
	const FloatType test_value0_flt[4] = { FloatType(2.0), FloatType(9.34), FloatType(-54.12), FloatType(6000.0) };
	const FloatType test_value1_flt[4] = { FloatType(0.75), FloatType(-4.52), FloatType(44.68), FloatType(-54225.0) };

	const Vector4Type test_value0 = vector_set(test_value0_flt[0], test_value0_flt[1], test_value0_flt[2], test_value0_flt[3]);
	const Vector4Type test_value1 = vector_set(test_value1_flt[0], test_value1_flt[1], test_value1_flt[2], test_value1_flt[3]);

	Vector4Type results[8 * 8];
	uint32_t index = 0;

#define RTM_TEST_MIX_XYZ(comp0, comp1, comp2) \
	results[index++] = vector_mix<comp0, comp1, comp2, mix4::x>(test_value0, test_value1); \
	results[index++] = vector_mix<comp0, comp1, comp2, mix4::y>(test_value0, test_value1); \
	results[index++] = vector_mix<comp0, comp1, comp2, mix4::z>(test_value0, test_value1); \
	results[index++] = vector_mix<comp0, comp1, comp2, mix4::w>(test_value0, test_value1); \
	results[index++] = vector_mix<comp0, comp1, comp2, mix4::a>(test_value0, test_value1); \
	results[index++] = vector_mix<comp0, comp1, comp2, mix4::b>(test_value0, test_value1); \
	results[index++] = vector_mix<comp0, comp1, comp2, mix4::c>(test_value0, test_value1); \
	results[index++] = vector_mix<comp0, comp1, comp2, mix4::d>(test_value0, test_value1)

#define RTM_TEST_MIX_XY(comp0, comp1) \
	RTM_TEST_MIX_XYZ(comp0, comp1, mix4::x); \
	RTM_TEST_MIX_XYZ(comp0, comp1, mix4::y); \
	RTM_TEST_MIX_XYZ(comp0, comp1, mix4::z); \
	RTM_TEST_MIX_XYZ(comp0, comp1, mix4::w); \
	RTM_TEST_MIX_XYZ(comp0, comp1, mix4::a); \
	RTM_TEST_MIX_XYZ(comp0, comp1, mix4::b); \
	RTM_TEST_MIX_XYZ(comp0, comp1, mix4::c); \
	RTM_TEST_MIX_XYZ(comp0, comp1, mix4::d)

	// This generates 8*8 = 64 unit tests... it takes a while to compile and uses a lot of stack space
	RTM_TEST_MIX_XY(XArg, YArg);

	index = 0;

	const int comp0 = (int)XArg;
	const int comp1 = (int)YArg;
	for (int comp2 = 0; comp2 < 8; ++comp2)
	{
		for (int comp3 = 0; comp3 < 8; ++comp3)
		{
			INFO("vector_mix<" << comp0 << ", " << comp1 << ", " << comp2 << ", " << comp3 << ">");

			const Vector4Type expected = vector_set(
				rtm_impl::is_mix_xyzw((mix4)comp0) ? test_value0_flt[comp0 - (int)mix4::x] : test_value1_flt[comp0 - (int)mix4::a],
				rtm_impl::is_mix_xyzw((mix4)comp1) ? test_value0_flt[comp1 - (int)mix4::x] : test_value1_flt[comp1 - (int)mix4::a],
				rtm_impl::is_mix_xyzw((mix4)comp2) ? test_value0_flt[comp2 - (int)mix4::x] : test_value1_flt[comp2 - (int)mix4::a],
				rtm_impl::is_mix_xyzw((mix4)comp3) ? test_value0_flt[comp3 - (int)mix4::x] : test_value1_flt[comp3 - (int)mix4::a]);

			CHECK(vector_all_near_equal(expected, results[index], threshold));

			++index;
		}
	}

#undef RTM_TEST_MIX_XY
#undef RTM_TEST_MIX_XYZ
#endif	// defined(RTM_IMPL_WITH_VECTOR_MIX_TESTS)
}

```

`tests/sources/vector_mix/test_vector4d_mix_aa.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4d.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4d vector_mix<a a * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4d, double, mix4::a, mix4::a>(1.0E-6);
}

```

`tests/sources/vector_mix/test_vector4d_mix_ab.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4d.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4d vector_mix<a b * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4d, double, mix4::a, mix4::b>(1.0E-6);
}

```

`tests/sources/vector_mix/test_vector4d_mix_ac.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4d.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4d vector_mix<a c * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4d, double, mix4::a, mix4::c>(1.0E-6);
}

```

`tests/sources/vector_mix/test_vector4d_mix_ad.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4d.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4d vector_mix<a d * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4d, double, mix4::a, mix4::d>(1.0E-6);
}

```

`tests/sources/vector_mix/test_vector4d_mix_aw.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4d.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4d vector_mix<a w * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4d, double, mix4::a, mix4::w>(1.0E-6);
}

```

`tests/sources/vector_mix/test_vector4d_mix_ax.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4d.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4d vector_mix<a x * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4d, double, mix4::a, mix4::x>(1.0E-6);
}

```

`tests/sources/vector_mix/test_vector4d_mix_ay.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4d.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4d vector_mix<a y * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4d, double, mix4::a, mix4::y>(1.0E-6);
}

```

`tests/sources/vector_mix/test_vector4d_mix_az.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4d.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4d vector_mix<a z * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4d, double, mix4::a, mix4::z>(1.0E-6);
}

```

`tests/sources/vector_mix/test_vector4d_mix_ba.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4d.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4d vector_mix<b a * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4d, double, mix4::b, mix4::a>(1.0E-6);
}

```

`tests/sources/vector_mix/test_vector4d_mix_bb.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4d.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4d vector_mix<b b * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4d, double, mix4::b, mix4::b>(1.0E-6);
}

```

`tests/sources/vector_mix/test_vector4d_mix_bc.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4d.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4d vector_mix<b c * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4d, double, mix4::b, mix4::c>(1.0E-6);
}

```

`tests/sources/vector_mix/test_vector4d_mix_bd.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4d.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4d vector_mix<b d * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4d, double, mix4::b, mix4::d>(1.0E-6);
}

```

`tests/sources/vector_mix/test_vector4d_mix_bw.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4d.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4d vector_mix<b w * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4d, double, mix4::b, mix4::w>(1.0E-6);
}

```

`tests/sources/vector_mix/test_vector4d_mix_bx.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4d.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4d vector_mix<b x * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4d, double, mix4::b, mix4::x>(1.0E-6);
}

```

`tests/sources/vector_mix/test_vector4d_mix_by.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4d.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4d vector_mix<b y * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4d, double, mix4::b, mix4::y>(1.0E-6);
}

```

`tests/sources/vector_mix/test_vector4d_mix_bz.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4d.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4d vector_mix<b z * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4d, double, mix4::b, mix4::z>(1.0E-6);
}

```

`tests/sources/vector_mix/test_vector4d_mix_ca.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4d.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4d vector_mix<c a * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4d, double, mix4::c, mix4::a>(1.0E-6);
}

```

`tests/sources/vector_mix/test_vector4d_mix_cb.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4d.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4d vector_mix<c b * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4d, double, mix4::c, mix4::b>(1.0E-6);
}

```

`tests/sources/vector_mix/test_vector4d_mix_cc.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4d.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4d vector_mix<c c * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4d, double, mix4::c, mix4::c>(1.0E-6);
}

```

`tests/sources/vector_mix/test_vector4d_mix_cd.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4d.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4d vector_mix<c d * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4d, double, mix4::c, mix4::d>(1.0E-6);
}

```

`tests/sources/vector_mix/test_vector4d_mix_cw.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4d.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4d vector_mix<c w * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4d, double, mix4::c, mix4::w>(1.0E-6);
}

```

`tests/sources/vector_mix/test_vector4d_mix_cx.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4d.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4d vector_mix<c x * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4d, double, mix4::c, mix4::x>(1.0E-6);
}

```

`tests/sources/vector_mix/test_vector4d_mix_cy.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4d.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4d vector_mix<c y * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4d, double, mix4::c, mix4::y>(1.0E-6);
}

```

`tests/sources/vector_mix/test_vector4d_mix_cz.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4d.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4d vector_mix<c z * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4d, double, mix4::c, mix4::z>(1.0E-6);
}

```

`tests/sources/vector_mix/test_vector4d_mix_da.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4d.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4d vector_mix<d a * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4d, double, mix4::d, mix4::a>(1.0E-6);
}

```

`tests/sources/vector_mix/test_vector4d_mix_db.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4d.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4d vector_mix<d b * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4d, double, mix4::d, mix4::b>(1.0E-6);
}

```

`tests/sources/vector_mix/test_vector4d_mix_dc.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4d.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4d vector_mix<d c * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4d, double, mix4::d, mix4::c>(1.0E-6);
}

```

`tests/sources/vector_mix/test_vector4d_mix_dd.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4d.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4d vector_mix<d d * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4d, double, mix4::d, mix4::d>(1.0E-6);
}

```

`tests/sources/vector_mix/test_vector4d_mix_dw.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4d.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4d vector_mix<d w * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4d, double, mix4::d, mix4::w>(1.0E-6);
}

```

`tests/sources/vector_mix/test_vector4d_mix_dx.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4d.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4d vector_mix<d x * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4d, double, mix4::d, mix4::x>(1.0E-6);
}

```

`tests/sources/vector_mix/test_vector4d_mix_dy.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4d.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4d vector_mix<d y * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4d, double, mix4::d, mix4::y>(1.0E-6);
}

```

`tests/sources/vector_mix/test_vector4d_mix_dz.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4d.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4d vector_mix<d z * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4d, double, mix4::d, mix4::z>(1.0E-6);
}

```

`tests/sources/vector_mix/test_vector4d_mix_wa.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4d.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4d vector_mix<w a * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4d, double, mix4::w, mix4::a>(1.0E-6);
}

```

`tests/sources/vector_mix/test_vector4d_mix_wb.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4d.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4d vector_mix<w b * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4d, double, mix4::w, mix4::b>(1.0E-6);
}

```

`tests/sources/vector_mix/test_vector4d_mix_wc.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4d.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4d vector_mix<w c * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4d, double, mix4::w, mix4::c>(1.0E-6);
}

```

`tests/sources/vector_mix/test_vector4d_mix_wd.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4d.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4d vector_mix<w d * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4d, double, mix4::w, mix4::d>(1.0E-6);
}

```

`tests/sources/vector_mix/test_vector4d_mix_ww.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4d.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4d vector_mix<w w * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4d, double, mix4::w, mix4::w>(1.0E-6);
}

```

`tests/sources/vector_mix/test_vector4d_mix_wx.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4d.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4d vector_mix<w x * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4d, double, mix4::w, mix4::x>(1.0E-6);
}

```

`tests/sources/vector_mix/test_vector4d_mix_wy.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4d.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4d vector_mix<w y * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4d, double, mix4::w, mix4::y>(1.0E-6);
}

```

`tests/sources/vector_mix/test_vector4d_mix_wz.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4d.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4d vector_mix<w z * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4d, double, mix4::w, mix4::z>(1.0E-6);
}

```

`tests/sources/vector_mix/test_vector4d_mix_xa.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4d.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4d vector_mix<x a * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4d, double, mix4::x, mix4::a>(1.0E-6);
}

```

`tests/sources/vector_mix/test_vector4d_mix_xb.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4d.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4d vector_mix<x b * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4d, double, mix4::x, mix4::b>(1.0E-6);
}

```

`tests/sources/vector_mix/test_vector4d_mix_xc.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4d.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4d vector_mix<x c * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4d, double, mix4::x, mix4::c>(1.0E-6);
}

```

`tests/sources/vector_mix/test_vector4d_mix_xd.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4d.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4d vector_mix<x d * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4d, double, mix4::x, mix4::d>(1.0E-6);
}

```

`tests/sources/vector_mix/test_vector4d_mix_xw.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4d.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4d vector_mix<x w * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4d, double, mix4::x, mix4::w>(1.0E-6);
}

```

`tests/sources/vector_mix/test_vector4d_mix_xx.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4d.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4d vector_mix<x x * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4d, double, mix4::x, mix4::x>(1.0E-6);
}

```

`tests/sources/vector_mix/test_vector4d_mix_xy.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4d.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4d vector_mix<x y * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4d, double, mix4::x, mix4::y>(1.0E-6);
}

```

`tests/sources/vector_mix/test_vector4d_mix_xz.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4d.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4d vector_mix<x z * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4d, double, mix4::x, mix4::z>(1.0E-6);
}

```

`tests/sources/vector_mix/test_vector4d_mix_ya.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4d.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4d vector_mix<y a * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4d, double, mix4::y, mix4::a>(1.0E-6);
}

```

`tests/sources/vector_mix/test_vector4d_mix_yb.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4d.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4d vector_mix<y b * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4d, double, mix4::y, mix4::b>(1.0E-6);
}

```

`tests/sources/vector_mix/test_vector4d_mix_yc.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4d.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4d vector_mix<y c * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4d, double, mix4::y, mix4::c>(1.0E-6);
}

```

`tests/sources/vector_mix/test_vector4d_mix_yd.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4d.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4d vector_mix<y d * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4d, double, mix4::y, mix4::d>(1.0E-6);
}

```

`tests/sources/vector_mix/test_vector4d_mix_yw.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4d.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4d vector_mix<y w * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4d, double, mix4::y, mix4::w>(1.0E-6);
}

```

`tests/sources/vector_mix/test_vector4d_mix_yx.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4d.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4d vector_mix<y x * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4d, double, mix4::y, mix4::x>(1.0E-6);
}

```

`tests/sources/vector_mix/test_vector4d_mix_yy.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4d.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4d vector_mix<y y * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4d, double, mix4::y, mix4::y>(1.0E-6);
}

```

`tests/sources/vector_mix/test_vector4d_mix_yz.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4d.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4d vector_mix<y z * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4d, double, mix4::y, mix4::z>(1.0E-6);
}

```

`tests/sources/vector_mix/test_vector4d_mix_za.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4d.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4d vector_mix<z a * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4d, double, mix4::z, mix4::a>(1.0E-6);
}

```

`tests/sources/vector_mix/test_vector4d_mix_zb.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4d.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4d vector_mix<z b * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4d, double, mix4::z, mix4::b>(1.0E-6);
}

```

`tests/sources/vector_mix/test_vector4d_mix_zc.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4d.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4d vector_mix<z c * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4d, double, mix4::z, mix4::c>(1.0E-6);
}

```

`tests/sources/vector_mix/test_vector4d_mix_zd.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4d.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4d vector_mix<z d * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4d, double, mix4::z, mix4::d>(1.0E-6);
}

```

`tests/sources/vector_mix/test_vector4d_mix_zw.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4d.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4d vector_mix<z w * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4d, double, mix4::z, mix4::w>(1.0E-6);
}

```

`tests/sources/vector_mix/test_vector4d_mix_zx.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4d.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4d vector_mix<z x * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4d, double, mix4::z, mix4::x>(1.0E-6);
}

```

`tests/sources/vector_mix/test_vector4d_mix_zy.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4d.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4d vector_mix<z y * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4d, double, mix4::z, mix4::y>(1.0E-6);
}

```

`tests/sources/vector_mix/test_vector4d_mix_zz.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4d.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4d vector_mix<z z * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4d, double, mix4::z, mix4::z>(1.0E-6);
}

```

`tests/sources/vector_mix/test_vector4f_mix_aa.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4f.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4f vector_mix<a a * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4f, float, mix4::a, mix4::a>(1.0E-6F);
}

```

`tests/sources/vector_mix/test_vector4f_mix_ab.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4f.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4f vector_mix<a b * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4f, float, mix4::a, mix4::b>(1.0E-6F);
}

```

`tests/sources/vector_mix/test_vector4f_mix_ac.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4f.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4f vector_mix<a c * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4f, float, mix4::a, mix4::c>(1.0E-6F);
}

```

`tests/sources/vector_mix/test_vector4f_mix_ad.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4f.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4f vector_mix<a d * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4f, float, mix4::a, mix4::d>(1.0E-6F);
}

```

`tests/sources/vector_mix/test_vector4f_mix_aw.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4f.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4f vector_mix<a w * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4f, float, mix4::a, mix4::w>(1.0E-6F);
}

```

`tests/sources/vector_mix/test_vector4f_mix_ax.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4f.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4f vector_mix<a x * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4f, float, mix4::a, mix4::x>(1.0E-6F);
}

```

`tests/sources/vector_mix/test_vector4f_mix_ay.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4f.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4f vector_mix<a y * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4f, float, mix4::a, mix4::y>(1.0E-6F);
}

```

`tests/sources/vector_mix/test_vector4f_mix_az.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4f.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4f vector_mix<a z * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4f, float, mix4::a, mix4::z>(1.0E-6F);
}

```

`tests/sources/vector_mix/test_vector4f_mix_ba.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4f.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4f vector_mix<b a * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4f, float, mix4::b, mix4::a>(1.0E-6F);
}

```

`tests/sources/vector_mix/test_vector4f_mix_bb.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4f.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4f vector_mix<b b * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4f, float, mix4::b, mix4::b>(1.0E-6F);
}

```

`tests/sources/vector_mix/test_vector4f_mix_bc.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4f.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4f vector_mix<b c * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4f, float, mix4::b, mix4::c>(1.0E-6F);
}

```

`tests/sources/vector_mix/test_vector4f_mix_bd.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4f.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4f vector_mix<b d * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4f, float, mix4::b, mix4::d>(1.0E-6F);
}

```

`tests/sources/vector_mix/test_vector4f_mix_bw.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4f.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4f vector_mix<b w * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4f, float, mix4::b, mix4::w>(1.0E-6F);
}

```

`tests/sources/vector_mix/test_vector4f_mix_bx.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4f.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4f vector_mix<b x * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4f, float, mix4::b, mix4::x>(1.0E-6F);
}

```

`tests/sources/vector_mix/test_vector4f_mix_by.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4f.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4f vector_mix<b y * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4f, float, mix4::b, mix4::y>(1.0E-6F);
}

```

`tests/sources/vector_mix/test_vector4f_mix_bz.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4f.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4f vector_mix<b z * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4f, float, mix4::b, mix4::z>(1.0E-6F);
}

```

`tests/sources/vector_mix/test_vector4f_mix_ca.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4f.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4f vector_mix<c a * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4f, float, mix4::c, mix4::a>(1.0E-6F);
}

```

`tests/sources/vector_mix/test_vector4f_mix_cb.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4f.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4f vector_mix<c b * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4f, float, mix4::c, mix4::b>(1.0E-6F);
}

```

`tests/sources/vector_mix/test_vector4f_mix_cc.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4f.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4f vector_mix<c c * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4f, float, mix4::c, mix4::c>(1.0E-6F);
}

```

`tests/sources/vector_mix/test_vector4f_mix_cd.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4f.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4f vector_mix<c d * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4f, float, mix4::c, mix4::d>(1.0E-6F);
}

```

`tests/sources/vector_mix/test_vector4f_mix_cw.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4f.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4f vector_mix<c w * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4f, float, mix4::c, mix4::w>(1.0E-6F);
}

```

`tests/sources/vector_mix/test_vector4f_mix_cx.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4f.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4f vector_mix<c x * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4f, float, mix4::c, mix4::x>(1.0E-6F);
}

```

`tests/sources/vector_mix/test_vector4f_mix_cy.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4f.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4f vector_mix<c y * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4f, float, mix4::c, mix4::y>(1.0E-6F);
}

```

`tests/sources/vector_mix/test_vector4f_mix_cz.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4f.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4f vector_mix<c z * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4f, float, mix4::c, mix4::z>(1.0E-6F);
}

```

`tests/sources/vector_mix/test_vector4f_mix_da.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4f.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4f vector_mix<d a * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4f, float, mix4::d, mix4::a>(1.0E-6F);
}

```

`tests/sources/vector_mix/test_vector4f_mix_db.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4f.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4f vector_mix<d b * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4f, float, mix4::d, mix4::b>(1.0E-6F);
}

```

`tests/sources/vector_mix/test_vector4f_mix_dc.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4f.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4f vector_mix<d c * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4f, float, mix4::d, mix4::c>(1.0E-6F);
}

```

`tests/sources/vector_mix/test_vector4f_mix_dd.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4f.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4f vector_mix<d d * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4f, float, mix4::d, mix4::d>(1.0E-6F);
}

```

`tests/sources/vector_mix/test_vector4f_mix_dw.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4f.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4f vector_mix<d w * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4f, float, mix4::d, mix4::w>(1.0E-6F);
}

```

`tests/sources/vector_mix/test_vector4f_mix_dx.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4f.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4f vector_mix<d x * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4f, float, mix4::d, mix4::x>(1.0E-6F);
}

```

`tests/sources/vector_mix/test_vector4f_mix_dy.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4f.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4f vector_mix<d y * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4f, float, mix4::d, mix4::y>(1.0E-6F);
}

```

`tests/sources/vector_mix/test_vector4f_mix_dz.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4f.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4f vector_mix<d z * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4f, float, mix4::d, mix4::z>(1.0E-6F);
}

```

`tests/sources/vector_mix/test_vector4f_mix_wa.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4f.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4f vector_mix<w a * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4f, float, mix4::w, mix4::a>(1.0E-6F);
}

```

`tests/sources/vector_mix/test_vector4f_mix_wb.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4f.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4f vector_mix<w b * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4f, float, mix4::w, mix4::b>(1.0E-6F);
}

```

`tests/sources/vector_mix/test_vector4f_mix_wc.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4f.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4f vector_mix<w c * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4f, float, mix4::w, mix4::c>(1.0E-6F);
}

```

`tests/sources/vector_mix/test_vector4f_mix_wd.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4f.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4f vector_mix<w d * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4f, float, mix4::w, mix4::d>(1.0E-6F);
}

```

`tests/sources/vector_mix/test_vector4f_mix_ww.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4f.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4f vector_mix<w w * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4f, float, mix4::w, mix4::w>(1.0E-6F);
}

```

`tests/sources/vector_mix/test_vector4f_mix_wx.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4f.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4f vector_mix<w x * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4f, float, mix4::w, mix4::x>(1.0E-6F);
}

```

`tests/sources/vector_mix/test_vector4f_mix_wy.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4f.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4f vector_mix<w y * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4f, float, mix4::w, mix4::y>(1.0E-6F);
}

```

`tests/sources/vector_mix/test_vector4f_mix_wz.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4f.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4f vector_mix<w z * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4f, float, mix4::w, mix4::z>(1.0E-6F);
}

```

`tests/sources/vector_mix/test_vector4f_mix_xa.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4f.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4f vector_mix<x a * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4f, float, mix4::x, mix4::a>(1.0E-6F);
}

```

`tests/sources/vector_mix/test_vector4f_mix_xb.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4f.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4f vector_mix<x b * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4f, float, mix4::x, mix4::b>(1.0E-6F);
}

```

`tests/sources/vector_mix/test_vector4f_mix_xc.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4f.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4f vector_mix<x c * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4f, float, mix4::x, mix4::c>(1.0E-6F);
}

```

`tests/sources/vector_mix/test_vector4f_mix_xd.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4f.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4f vector_mix<x d * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4f, float, mix4::x, mix4::d>(1.0E-6F);
}

```

`tests/sources/vector_mix/test_vector4f_mix_xw.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4f.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4f vector_mix<x w * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4f, float, mix4::x, mix4::w>(1.0E-6F);
}

```

`tests/sources/vector_mix/test_vector4f_mix_xx.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4f.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4f vector_mix<x x * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4f, float, mix4::x, mix4::x>(1.0E-6F);
}

```

`tests/sources/vector_mix/test_vector4f_mix_xy.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4f.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4f vector_mix<x y * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4f, float, mix4::x, mix4::y>(1.0E-6F);
}

```

`tests/sources/vector_mix/test_vector4f_mix_xz.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4f.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4f vector_mix<x z * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4f, float, mix4::x, mix4::z>(1.0E-6F);
}

```

`tests/sources/vector_mix/test_vector4f_mix_ya.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4f.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4f vector_mix<y a * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4f, float, mix4::y, mix4::a>(1.0E-6F);
}

```

`tests/sources/vector_mix/test_vector4f_mix_yb.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4f.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4f vector_mix<y b * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4f, float, mix4::y, mix4::b>(1.0E-6F);
}

```

`tests/sources/vector_mix/test_vector4f_mix_yc.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4f.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4f vector_mix<y c * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4f, float, mix4::y, mix4::c>(1.0E-6F);
}

```

`tests/sources/vector_mix/test_vector4f_mix_yd.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4f.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4f vector_mix<y d * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4f, float, mix4::y, mix4::d>(1.0E-6F);
}

```

`tests/sources/vector_mix/test_vector4f_mix_yw.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4f.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4f vector_mix<y w * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4f, float, mix4::y, mix4::w>(1.0E-6F);
}

```

`tests/sources/vector_mix/test_vector4f_mix_yx.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4f.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4f vector_mix<y x * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4f, float, mix4::y, mix4::x>(1.0E-6F);
}

```

`tests/sources/vector_mix/test_vector4f_mix_yy.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4f.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4f vector_mix<y y * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4f, float, mix4::y, mix4::y>(1.0E-6F);
}

```

`tests/sources/vector_mix/test_vector4f_mix_yz.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4f.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4f vector_mix<y z * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4f, float, mix4::y, mix4::z>(1.0E-6F);
}

```

`tests/sources/vector_mix/test_vector4f_mix_za.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4f.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4f vector_mix<z a * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4f, float, mix4::z, mix4::a>(1.0E-6F);
}

```

`tests/sources/vector_mix/test_vector4f_mix_zb.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4f.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4f vector_mix<z b * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4f, float, mix4::z, mix4::b>(1.0E-6F);
}

```

`tests/sources/vector_mix/test_vector4f_mix_zc.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4f.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4f vector_mix<z c * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4f, float, mix4::z, mix4::c>(1.0E-6F);
}

```

`tests/sources/vector_mix/test_vector4f_mix_zd.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4f.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4f vector_mix<z d * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4f, float, mix4::z, mix4::d>(1.0E-6F);
}

```

`tests/sources/vector_mix/test_vector4f_mix_zw.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4f.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4f vector_mix<z w * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4f, float, mix4::z, mix4::w>(1.0E-6F);
}

```

`tests/sources/vector_mix/test_vector4f_mix_zx.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4f.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4f vector_mix<z x * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4f, float, mix4::z, mix4::x>(1.0E-6F);
}

```

`tests/sources/vector_mix/test_vector4f_mix_zy.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4f.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4f vector_mix<z y * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4f, float, mix4::z, mix4::y>(1.0E-6F);
}

```

`tests/sources/vector_mix/test_vector4f_mix_zz.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <rtm/vector4f.h>

#include "test_vector4_mix_impl.h"

TEST_CASE("vector4f vector_mix<z z * *>", "[math][vector4]")
{
	test_vector_mix_impl<vector4f, float, mix4::z, mix4::z>(1.0E-6F);
}

```

`tests/validate_includes/CMakeLists.txt`:

```txt
project(rtm_validate_includes CXX)

# The goal of this project is to generate a single cpp file for every public header
# This will allow us to detect if we are missing an include file during development

include_directories("${PROJECT_SOURCE_DIR}/../../includes")

# Grab all of our public header files
file(GLOB ALL_PUBLIC_HEADER_FILES LIST_DIRECTORIES false
	${PROJECT_SOURCE_DIR}/../../includes/rtm/*.h
	${PROJECT_SOURCE_DIR}/../../includes/rtm/packing/*.h)

# Generate the single include cpp files
foreach(HEADER_FILE ${ALL_PUBLIC_HEADER_FILES})
	# Find the root include directory position
	string(FIND ${HEADER_FILE} "rtm" HEADER_FILEN_RTM_POS REVERSE)

	# Strip the root of the include path
	string(SUBSTRING ${HEADER_FILE} ${HEADER_FILEN_RTM_POS} -1 HEADER_INCLUDE_PATH)

	# Configure our cpp file content
	set(RTM_SINGLE_INCLUDE_NAME ${HEADER_INCLUDE_PATH})

	# Sanitize our filename so we can generate a unique cpp file for it
	string(REPLACE "/" "_" HEADER_SANITIZED_FILENAME ${HEADER_INCLUDE_PATH})
	string(REPLACE "\\" "_" HEADER_SANITIZED_FILENAME ${HEADER_SANITIZED_FILENAME})

	# Generate our single include cpp file
	configure_file(${PROJECT_SOURCE_DIR}/single_include.cpp.in single_include_${HEADER_SANITIZED_FILENAME}.cpp @ONLY)
endforeach(HEADER_FILE)

# Grab all of our main source files
file(GLOB_RECURSE ALL_MAIN_SOURCE_FILES LIST_DIRECTORIES false
	${PROJECT_SOURCE_DIR}/*.cpp
	${PROJECT_BINARY_DIR}/*.cpp)

create_source_groups("${ALL_MAIN_SOURCE_FILES}" ${PROJECT_SOURCE_DIR})

add_library(${PROJECT_NAME} STATIC ${ALL_MAIN_SOURCE_FILES})

setup_default_compiler_flags(${PROJECT_NAME})

```

`tests/validate_includes/dummy.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2024 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

// We define a single symbol here to avoid a warning when we build our static library
// to validate includes
void some_function() {}

```

`tests/validate_includes/single_include.cpp.in`:

```in
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2024 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <@RTM_SINGLE_INCLUDE_NAME@>

```

`tools/android_misc/README.md`:

```md
# Android Misc

This directory contains various Android related files that are shared between the various projects.

```

`tools/android_misc/app/build.gradle.in`:

```in
apply plugin: 'com.android.application'

android {
    ndkVersion "20.1.5948944"
    compileSdkVersion 28
    defaultConfig {
        applicationId "@RTM_ANDROID_PROJECT_PACKAGE_NAME@"
        minSdkVersion 24
        targetSdkVersion 26
        versionCode 1
        versionName "1.0"
        testInstrumentationRunner "androidx.test.runner.AndroidJUnitRunner"
        externalNativeBuild {
            cmake {
                arguments '-DUSE_SIMD_INSTRUCTIONS=@USE_SIMD_INSTRUCTIONS@',
                          '-DWITH_VECTOR_MIX_TESTS=@WITH_VECTOR_MIX_TESTS@',
                          '-DCMAKE_CXX_STANDARD=@CMAKE_CXX_STANDARD@'
            }
        }
        ndk {
            abiFilters '@CPU_INSTRUCTION_SET@'
        }

    }
    buildTypes {
        release {
            minifyEnabled false
            proguardFiles getDefaultProguardFile('proguard-android-optimize.txt'), 'proguard-rules.pro'
            debuggable = true
            signingConfig signingConfigs.debug
            externalNativeBuild {
                cmake {
                    // Because we want to debug 'release' builds, the 'debuggable' property above
                    // forces the build configuration to 'Debug' which disables optimizations.
                    // Force a release configutation anyway.
                    arguments '-DCMAKE_BUILD_TYPE=Release'
                }
            }
        }
    }
    externalNativeBuild {
        cmake {
            path '@RTM_PROJECT_ROOT@/src/main/cpp/CMakeLists.txt'
        }
    }
    sourceSets {
        main {
            manifest.srcFile './src/main/AndroidManifest.xml'
            java.srcDirs = ['@RTM_PROJECT_ROOT@/src/main/java']
            res.srcDirs = ['./src/main/res']
        }
    }
}

dependencies {
    implementation fileTree(dir: 'libs', include: ['*.jar'])
    implementation 'androidx.appcompat:appcompat:1.0.2'
    implementation 'androidx.constraintlayout:constraintlayout:1.1.3'
}

```

`tools/android_misc/app/proguard-rules.pro`:

```pro
# Add project specific ProGuard rules here.
# You can control the set of applied configuration files using the
# proguardFiles setting in build.gradle.
#
# For more details, see
#   http://developer.android.com/guide/developing/tools/proguard.html

# If your project uses WebView with JS, uncomment the following
# and specify the fully qualified class name to the JavaScript interface
# class:
#-keepclassmembers class fqcn.of.javascript.interface.for.webview {
#   public *;
#}

# Uncomment this to preserve the line number information for
# debugging stack traces.
#-keepattributes SourceFile,LineNumberTable

# If you keep the line number information, uncomment this to
# hide the original source file name.
#-renamesourcefileattribute SourceFile

```

`tools/android_misc/app/src/main/AndroidManifest.xml.in`:

```in
<?xml version="1.0" encoding="utf-8"?>
<manifest xmlns:android="http://schemas.android.com/apk/res/android" package="@RTM_ANDROID_PROJECT_PACKAGE_NAME@">
    <application android:label="@string/app_name">
        <activity android:name=".MainActivity">
            <intent-filter>
                <action android:name="android.intent.action.MAIN" />
                <category android:name="android.intent.category.LAUNCHER" />
            </intent-filter>
        </activity>
    </application>
</manifest>

```

`tools/android_misc/app/src/main/res/values/strings.xml.in`:

```in
<?xml version="1.0" encoding="utf-8"?>
<resources>
    <string name="app_name">@RTM_ANDROID_PROJECT_NAME@</string>
</resources>

```

`tools/android_misc/build.gradle`:

```gradle
// Top-level build file where you can add configuration options common to all sub-projects/modules.

buildscript {
    repositories {
        google()
        jcenter()
    }
    dependencies {
        classpath 'com.android.tools.build:gradle:3.5.3'

        // NOTE: Do not place your application dependencies here; they belong
        // in the individual module build.gradle files
    }
}

allprojects {
    repositories {
        google()
        jcenter()
    }
}

task clean(type: Delete) {
    delete rootProject.buildDir
}

```

`tools/android_misc/gradle.properties`:

```properties
# Project-wide Gradle settings.
# IDE (e.g. Android Studio) users:
# Gradle settings configured through the IDE *will override*
# any settings specified in this file.
# For more details on how to configure your build environment visit
# http://www.gradle.org/docs/current/userguide/build_environment.html
# Specifies the JVM arguments used for the daemon process.
# The setting is particularly useful for tweaking memory settings.
org.gradle.jvmargs=-Xmx1536m
# When configured, Gradle will run in incubating parallel mode.
# This option should only be used with decoupled projects. More details, visit
# http://www.gradle.org/docs/current/userguide/multi_project_builds.html#sec:decoupled_projects
# org.gradle.parallel=true
# AndroidX package structure to make it clearer which packages are bundled with the
# Android operating system, and which are packaged with your app's APK
# https://developer.android.com/topic/libraries/support-library/androidx-rn
android.useAndroidX=true
# Automatically convert third-party libraries to use AndroidX
android.enableJetifier=true

# Run tasks in parallel
org.gradle.parallel=true

```

`tools/android_misc/gradle/wrapper/gradle-wrapper.properties`:

```properties
#Fri Mar 27 21:02:15 EDT 2020
distributionBase=GRADLE_USER_HOME
distributionPath=wrapper/dists
zipStoreBase=GRADLE_USER_HOME
zipStorePath=wrapper/dists
distributionUrl=https\://services.gradle.org/distributions/gradle-5.4.1-all.zip

```

`tools/android_misc/gradlew`:

```
#!/usr/bin/env sh

##############################################################################
##
##  Gradle start up script for UN*X
##
##############################################################################

# Attempt to set APP_HOME
# Resolve links: $0 may be a link
PRG="$0"
# Need this for relative symlinks.
while [ -h "$PRG" ] ; do
    ls=`ls -ld "$PRG"`
    link=`expr "$ls" : '.*-> \(.*\)$'`
    if expr "$link" : '/.*' > /dev/null; then
        PRG="$link"
    else
        PRG=`dirname "$PRG"`"/$link"
    fi
done
SAVED="`pwd`"
cd "`dirname \"$PRG\"`/" >/dev/null
APP_HOME="`pwd -P`"
cd "$SAVED" >/dev/null

APP_NAME="Gradle"
APP_BASE_NAME=`basename "$0"`

# Add default JVM options here. You can also use JAVA_OPTS and GRADLE_OPTS to pass JVM options to this script.
DEFAULT_JVM_OPTS=""

# Use the maximum available, or set MAX_FD != -1 to use that value.
MAX_FD="maximum"

warn () {
    echo "$*"
}

die () {
    echo
    echo "$*"
    echo
    exit 1
}

# OS specific support (must be 'true' or 'false').
cygwin=false
msys=false
darwin=false
nonstop=false
case "`uname`" in
  CYGWIN* )
    cygwin=true
    ;;
  Darwin* )
    darwin=true
    ;;
  MINGW* )
    msys=true
    ;;
  NONSTOP* )
    nonstop=true
    ;;
esac

CLASSPATH=$APP_HOME/gradle/wrapper/gradle-wrapper.jar

# Determine the Java command to use to start the JVM.
if [ -n "$JAVA_HOME" ] ; then
    if [ -x "$JAVA_HOME/jre/sh/java" ] ; then
        # IBM's JDK on AIX uses strange locations for the executables
        JAVACMD="$JAVA_HOME/jre/sh/java"
    else
        JAVACMD="$JAVA_HOME/bin/java"
    fi
    if [ ! -x "$JAVACMD" ] ; then
        die "ERROR: JAVA_HOME is set to an invalid directory: $JAVA_HOME

Please set the JAVA_HOME variable in your environment to match the
location of your Java installation."
    fi
else
    JAVACMD="java"
    which java >/dev/null 2>&1 || die "ERROR: JAVA_HOME is not set and no 'java' command could be found in your PATH.

Please set the JAVA_HOME variable in your environment to match the
location of your Java installation."
fi

# Increase the maximum file descriptors if we can.
if [ "$cygwin" = "false" -a "$darwin" = "false" -a "$nonstop" = "false" ] ; then
    MAX_FD_LIMIT=`ulimit -H -n`
    if [ $? -eq 0 ] ; then
        if [ "$MAX_FD" = "maximum" -o "$MAX_FD" = "max" ] ; then
            MAX_FD="$MAX_FD_LIMIT"
        fi
        ulimit -n $MAX_FD
        if [ $? -ne 0 ] ; then
            warn "Could not set maximum file descriptor limit: $MAX_FD"
        fi
    else
        warn "Could not query maximum file descriptor limit: $MAX_FD_LIMIT"
    fi
fi

# For Darwin, add options to specify how the application appears in the dock
if $darwin; then
    GRADLE_OPTS="$GRADLE_OPTS \"-Xdock:name=$APP_NAME\" \"-Xdock:icon=$APP_HOME/media/gradle.icns\""
fi

# For Cygwin, switch paths to Windows format before running java
if $cygwin ; then
    APP_HOME=`cygpath --path --mixed "$APP_HOME"`
    CLASSPATH=`cygpath --path --mixed "$CLASSPATH"`
    JAVACMD=`cygpath --unix "$JAVACMD"`

    # We build the pattern for arguments to be converted via cygpath
    ROOTDIRSRAW=`find -L / -maxdepth 1 -mindepth 1 -type d 2>/dev/null`
    SEP=""
    for dir in $ROOTDIRSRAW ; do
        ROOTDIRS="$ROOTDIRS$SEP$dir"
        SEP="|"
    done
    OURCYGPATTERN="(^($ROOTDIRS))"
    # Add a user-defined pattern to the cygpath arguments
    if [ "$GRADLE_CYGPATTERN" != "" ] ; then
        OURCYGPATTERN="$OURCYGPATTERN|($GRADLE_CYGPATTERN)"
    fi
    # Now convert the arguments - kludge to limit ourselves to /bin/sh
    i=0
    for arg in "$@" ; do
        CHECK=`echo "$arg"|egrep -c "$OURCYGPATTERN" -`
        CHECK2=`echo "$arg"|egrep -c "^-"`                                 ### Determine if an option

        if [ $CHECK -ne 0 ] && [ $CHECK2 -eq 0 ] ; then                    ### Added a condition
            eval `echo args$i`=`cygpath --path --ignore --mixed "$arg"`
        else
            eval `echo args$i`="\"$arg\""
        fi
        i=$((i+1))
    done
    case $i in
        (0) set -- ;;
        (1) set -- "$args0" ;;
        (2) set -- "$args0" "$args1" ;;
        (3) set -- "$args0" "$args1" "$args2" ;;
        (4) set -- "$args0" "$args1" "$args2" "$args3" ;;
        (5) set -- "$args0" "$args1" "$args2" "$args3" "$args4" ;;
        (6) set -- "$args0" "$args1" "$args2" "$args3" "$args4" "$args5" ;;
        (7) set -- "$args0" "$args1" "$args2" "$args3" "$args4" "$args5" "$args6" ;;
        (8) set -- "$args0" "$args1" "$args2" "$args3" "$args4" "$args5" "$args6" "$args7" ;;
        (9) set -- "$args0" "$args1" "$args2" "$args3" "$args4" "$args5" "$args6" "$args7" "$args8" ;;
    esac
fi

# Escape application args
save () {
    for i do printf %s\\n "$i" | sed "s/'/'\\\\''/g;1s/^/'/;\$s/\$/' \\\\/" ; done
    echo " "
}
APP_ARGS=$(save "$@")

# Collect all arguments for the java command, following the shell quoting and substitution rules
eval set -- $DEFAULT_JVM_OPTS $JAVA_OPTS $GRADLE_OPTS "\"-Dorg.gradle.appname=$APP_BASE_NAME\"" -classpath "\"$CLASSPATH\"" org.gradle.wrapper.GradleWrapperMain "$APP_ARGS"

# by default we should be in the correct project dir, but when run from Finder on Mac, the cwd is wrong
if [ "$(uname)" = "Darwin" ] && [ "$HOME" = "$PWD" ]; then
  cd "$(dirname "$0")"
fi

exec "$JAVACMD" "$@"

```

`tools/android_misc/gradlew.bat`:

```bat
@if "%DEBUG%" == "" @echo off
@rem ##########################################################################
@rem
@rem  Gradle startup script for Windows
@rem
@rem ##########################################################################

@rem Set local scope for the variables with windows NT shell
if "%OS%"=="Windows_NT" setlocal

set DIRNAME=%~dp0
if "%DIRNAME%" == "" set DIRNAME=.
set APP_BASE_NAME=%~n0
set APP_HOME=%DIRNAME%

@rem Add default JVM options here. You can also use JAVA_OPTS and GRADLE_OPTS to pass JVM options to this script.
set DEFAULT_JVM_OPTS=

@rem Find java.exe
if defined JAVA_HOME goto findJavaFromJavaHome

set JAVA_EXE=java.exe
%JAVA_EXE% -version >NUL 2>&1
if "%ERRORLEVEL%" == "0" goto init

echo.
echo ERROR: JAVA_HOME is not set and no 'java' command could be found in your PATH.
echo.
echo Please set the JAVA_HOME variable in your environment to match the
echo location of your Java installation.

goto fail

:findJavaFromJavaHome
set JAVA_HOME=%JAVA_HOME:"=%
set JAVA_EXE=%JAVA_HOME%/bin/java.exe

if exist "%JAVA_EXE%" goto init

echo.
echo ERROR: JAVA_HOME is set to an invalid directory: %JAVA_HOME%
echo.
echo Please set the JAVA_HOME variable in your environment to match the
echo location of your Java installation.

goto fail

:init
@rem Get command-line arguments, handling Windows variants

if not "%OS%" == "Windows_NT" goto win9xME_args

:win9xME_args
@rem Slurp the command line arguments.
set CMD_LINE_ARGS=
set _SKIP=2

:win9xME_args_slurp
if "x%~1" == "x" goto execute

set CMD_LINE_ARGS=%*

:execute
@rem Setup the command line

set CLASSPATH=%APP_HOME%\gradle\wrapper\gradle-wrapper.jar

@rem Execute Gradle
"%JAVA_EXE%" %DEFAULT_JVM_OPTS% %JAVA_OPTS% %GRADLE_OPTS% "-Dorg.gradle.appname=%APP_BASE_NAME%" -classpath "%CLASSPATH%" org.gradle.wrapper.GradleWrapperMain %CMD_LINE_ARGS%

:end
@rem End local scope for the variables with windows NT shell
if "%ERRORLEVEL%"=="0" goto mainEnd

:fail
rem Set variable GRADLE_EXIT_CONSOLE if you need the _script_ return code instead of
rem the _cmd.exe /c_ return code!
if  not "" == "%GRADLE_EXIT_CONSOLE%" exit 1
exit /b 1

:mainEnd
if "%OS%"=="Windows_NT" endlocal

:omega

```

`tools/android_misc/settings.gradle.in`:

```in
include ':app'
rootProject.name='@RTM_ANDROID_PROJECT_NAME@'

```

`tools/appveyor_ci.bat`:

```bat
@echo off

REM Unpack arguments
SET WORKER_IMAGE=%1
SET PLATFORM=%2
SET CONFIG=%3
SET TOOLCHAIN=%4
SET PYTHON_PATH=%5

echo Worker image: %WORKER_IMAGE%
echo Platform: %PLATFORM%
echo Config: %CONFIG%
echo Toolchain: %TOOLCHAIN%
echo Python path: %PYTHON_PATH%

REM Convert the build image and toolchain into our compiler string
IF /i %TOOLCHAIN%==msvc GOTO :msvc
IF /i %TOOLCHAIN%==clang GOTO :clang

echo Unknown toolchain: %TOOLCHAIN%
exit /B 1

:msvc
IF /i %WORKER_IMAGE%=="Visual Studio 2015" SET COMPILER=vs2015
IF /i %WORKER_IMAGE%=="Visual Studio 2017" SET COMPILER=vs2017
IF /i %WORKER_IMAGE%=="Visual Studio 2019" SET COMPILER=vs2019
IF /i %WORKER_IMAGE%=="Previous Visual Studio 2019" SET COMPILER=vs2019
GOTO :next

:clang
IF /i %WORKER_IMAGE%=="Visual Studio 2019" SET COMPILER=vs2019-clang
IF /i %WORKER_IMAGE%=="Previous Visual Studio 2019" SET COMPILER=vs2019-clang

REM HACK!!! Disable clang build for now with appveyor since vcpkg breaks the compiler detection of cmake
REM Fake build success
exit /B 0

GOTO :next

:next
REM Set our switch if we need to run unit tests
SET UNIT_TEST_FLAG=-unit_test
IF /i %PLATFORM%==arm64 SET UNIT_TEST_FLAG=

REM If PYTHON_PATH isn't set, assume it is in PATH
IF NOT DEFINED PYTHON_PATH SET PYTHON_PATH=python.exe

REM Build and run unit tests
%PYTHON_PATH% make.py -build %UNIT_TEST_FLAG% -compiler %COMPILER% -config %CONFIG% -cpu %PLATFORM% -clean
IF NOT %ERRORLEVEL% EQU 0 GOTO :build_failure

%PYTHON_PATH% make.py -build %UNIT_TEST_FLAG% -compiler %COMPILER% -config %CONFIG% -cpu %PLATFORM% -nosimd
IF NOT %ERRORLEVEL% EQU 0 GOTO :build_failure

REM Done!
exit /B 0

:build_failure
echo Build failed!
exit /B 1

```

`tools/bench/CMakeLists.txt`:

```txt
project(rtm_bench_root NONE)

if(PLATFORM_ANDROID)
	add_subdirectory("${PROJECT_SOURCE_DIR}/main_android")
elseif(PLATFORM_IOS)
	add_subdirectory("${PROJECT_SOURCE_DIR}/main_ios")
else()
	add_subdirectory("${PROJECT_SOURCE_DIR}/main_generic")
endif()

```

`tools/bench/main_android/CMakeLists.txt`:

```txt
project(rtm_bench_gradle_shim NONE)

# Set our project root since our gradle files used to build live in the binary output directory
# but the actual source files live in the source/current directory.
set(RTM_PROJECT_ROOT ${CMAKE_CURRENT_BINARY_DIR}/app)
file(RELATIVE_PATH RTM_PROJECT_ROOT ${RTM_PROJECT_ROOT} ${PROJECT_SOURCE_DIR}/app)

# Setup our state
set(RTM_ANDROID_PROJECT_NAME "RTM Benchmark")
set(RTM_ANDROID_PROJECT_PACKAGE_NAME "com.rtm.benchmark")

# Configure gradle for our build configuration
configure_file(${RTM_ANDROID_MISC_DIR}/app/build.gradle.in app/build.gradle @ONLY)
configure_file(${RTM_ANDROID_MISC_DIR}/app/src/main/AndroidManifest.xml.in app/src/main/AndroidManifest.xml @ONLY)
configure_file(${RTM_ANDROID_MISC_DIR}/app/src/main/res/values/strings.xml.in app/src/main/res/values/strings.xml @ONLY)
configure_file(${RTM_ANDROID_MISC_DIR}/settings.gradle.in settings.gradle @ONLY)

# Copy gradle related files
file(COPY
	${RTM_ANDROID_MISC_DIR}/build.gradle
	${RTM_ANDROID_MISC_DIR}/gradle.properties
	${RTM_ANDROID_MISC_DIR}/gradle
	${RTM_ANDROID_MISC_DIR}/gradlew
	${RTM_ANDROID_MISC_DIR}/gradlew.bat
	DESTINATION .)

file(COPY
	${RTM_ANDROID_MISC_DIR}/app/proguard-rules.pro
	DESTINATION app)

add_custom_target(${PROJECT_NAME} ALL
	COMMAND gradlew.bat
	# Decide whether we should build Debug or Release
	$<$<CONFIG:Debug>:assembleDebug>
	$<$<CONFIG:Release>:assembleRelease>)

```

`tools/bench/main_android/app/src/main/cpp/CMakeLists.txt`:

```txt
cmake_minimum_required(VERSION 3.2...3.25)
project(rtm_bench CXX)

# Project root is <rtm-dir>\tools\bench\main_android
set(PROJECT_ROOT_DIR "${PROJECT_SOURCE_DIR}/../../../..")

# Google Benchmark
set(BENCHMARK_ENABLE_TESTING OFF CACHE BOOL "No need to run benchmark's tests" FORCE)
add_subdirectory("${PROJECT_ROOT_DIR}/../../../external/benchmark" google_benchmark)

include_directories("${PROJECT_ROOT_DIR}/../../../includes")
include_directories("${PROJECT_ROOT_DIR}/../../../external/benchmark/include")

# Grab all of our test source files
file(GLOB_RECURSE ALL_BENCH_SOURCE_FILES LIST_DIRECTORIES false
	${PROJECT_ROOT_DIR}/../sources/*.h
	${PROJECT_ROOT_DIR}/../sources/*.cpp)

# Grab all of our main source files
file(GLOB_RECURSE ALL_MAIN_SOURCE_FILES LIST_DIRECTORIES false
	${PROJECT_SOURCE_DIR}/*.cpp)

add_library(${PROJECT_NAME} SHARED ${ALL_BENCH_SOURCE_FILES} ${ALL_MAIN_SOURCE_FILES})

# Enable debug symbols
target_compile_options(${PROJECT_NAME} PRIVATE -g)

# Disable SIMD if not needed
if(NOT USE_SIMD_INSTRUCTIONS)
	add_definitions(-DRTM_NO_INTRINSICS)
endif()

target_include_directories(${PROJECT_NAME} PUBLIC jni)

target_link_libraries(${PROJECT_NAME} m log)
target_link_libraries(${PROJECT_NAME} benchmark)

```

`tools/bench/main_android/app/src/main/cpp/main.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2019 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <benchmark/benchmark.h>

#include <android/log.h>
#include <iostream>
#include <jni.h>
#include <streambuf>

// Inspired from https://stackoverflow.com/questions/8870174/is-stdcout-usable-in-android-ndk
class androidbuf final : public std::streambuf
{
public:
	enum { bufsize = 4096 };
	androidbuf()
	{
		this->setp(buffer, buffer + bufsize - 1);
	}

private:
	int overflow(int c)
	{
		if (c == traits_type::eof())
		{
			*this->pptr() = traits_type::to_char_type(c);
			this->sbumpc();
		}
		return this->sync() ? traits_type::eof() : traits_type::not_eof(c);
	}

	int sync()
	{
		int rc = 0;
		if (this->pbase() != this->pptr())
		{
			char writebuf[bufsize + 1];
			memcpy(writebuf, this->pbase(), this->pptr() - this->pbase());
			writebuf[this->pptr() - this->pbase()] = '\0';

			rc = __android_log_write(ANDROID_LOG_INFO, "rtm", writebuf) > 0;
			this->setp(buffer, buffer + bufsize - 1);
		}
		return rc;
	}

	char buffer[bufsize];
};

extern "C" jint Java_com_rtm_benchmark_MainActivity_runBenchmark(JNIEnv* env, jobject caller)
{
	std::cout.rdbuf(new androidbuf());

	int argc = 0;
	benchmark::Initialize(&argc, nullptr);

	benchmark::RunSpecifiedBenchmarks();

	return 0;
}

```

`tools/bench/main_android/app/src/main/java/com/rtm/benchmark/MainActivity.java`:

```java
package com.rtm.benchmark;

////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2019 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

import android.app.Activity;
import android.widget.TextView;
import android.os.Bundle;

public class MainActivity extends Activity {
	static {
		System.loadLibrary("rtm_bench");
	}

	@Override
	public void onCreate(Bundle savedInstanceState) {
		super.onCreate(savedInstanceState);

		TextView resultTextView = new TextView(this);

		int result = runBenchmark();

		if (result == 0)
			resultTextView.setText("All benchmarks ran successfully!");

		setContentView(resultTextView);
	}

	public native int runBenchmark();
}

```

`tools/bench/main_generic/CMakeLists.txt`:

```txt
project(rtm_bench CXX)

# Google Benchmark
set(BENCHMARK_ENABLE_TESTING OFF CACHE BOOL "No need to run benchmark's tests" FORCE)
add_subdirectory("${PROJECT_SOURCE_DIR}/../../../external/benchmark" google_benchmark)

include_directories("${PROJECT_SOURCE_DIR}/../../../includes")
include_directories("${PROJECT_SOURCE_DIR}/../../../external/benchmark/include")

# Grab all of our benchmark source files
file(GLOB_RECURSE ALL_BENCH_SOURCE_FILES LIST_DIRECTORIES false
	${PROJECT_SOURCE_DIR}/../sources/*.h
	${PROJECT_SOURCE_DIR}/../sources/*.cpp)

create_source_groups("${ALL_BENCH_SOURCE_FILES}" ${PROJECT_SOURCE_DIR}/..)

# Grab all of our main source files
file(GLOB_RECURSE ALL_MAIN_SOURCE_FILES LIST_DIRECTORIES false
	${PROJECT_SOURCE_DIR}/*.cpp)

create_source_groups("${ALL_MAIN_SOURCE_FILES}" ${PROJECT_SOURCE_DIR})

add_executable(${PROJECT_NAME} ${ALL_BENCH_SOURCE_FILES} ${ALL_MAIN_SOURCE_FILES})

setup_default_compiler_flags(${PROJECT_NAME})
target_link_libraries(${PROJECT_NAME} PRIVATE benchmark)    # Link Google Benchmark

install(TARGETS ${PROJECT_NAME} RUNTIME DESTINATION bin)

```

`tools/bench/main_generic/main.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2019 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <benchmark/benchmark.h>

#ifdef _WIN32
#include <conio.h>

extern "C" __declspec(dllimport) int __stdcall IsDebuggerPresent();
#endif

int main(int argc, char* argv[])
{
	benchmark::Initialize(&argc, argv);

	benchmark::RunSpecifiedBenchmarks();

#ifdef _WIN32
	if (IsDebuggerPresent())
	{
		printf("Press any key to continue...\n");
		while (_kbhit() == 0);
	}
#endif

	return 0;
}

```

`tools/bench/main_ios/CMakeLists.txt`:

```txt
project(rtm_bench)

# Google Benchmark
set(BENCHMARK_ENABLE_TESTING OFF CACHE BOOL "No need to run benchmark's tests" FORCE)
add_subdirectory("${PROJECT_SOURCE_DIR}/../../../external/benchmark" google_benchmark)

# iOS cmake toolchain does not support CMAKE_CXX_STANDARD
set(CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} -std=c++${CMAKE_CXX_STANDARD}")

# Force enable debug symbols
set(CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} -g")

# Enable optimizations in Release
set(CMAKE_CXX_FLAGS_RELEASE "${CMAKE_CXX_FLAGS_RELEASE} -O3")

set(MACOSX_BUNDLE_EXECUTABLE_NAME ${PROJECT_NAME})
set(MACOSX_BUNDLE_INFO_STRING "com.rtm.rtm-bench")
set(MACOSX_BUNDLE_GUI_IDENTIFIER "com.rtm.rtm-bench")
set(MACOSX_BUNDLE_BUNDLE_NAME "rtm-bench")

include_directories("${PROJECT_SOURCE_DIR}/../../../includes")
include_directories("${PROJECT_SOURCE_DIR}/../../../external/benchmark/include")

# Grab all of our benchmark source files
file(GLOB_RECURSE ALL_BENCH_SOURCE_FILES LIST_DIRECTORIES false
	${PROJECT_SOURCE_DIR}/../sources/*.h
	${PROJECT_SOURCE_DIR}/../sources/*.cpp)

create_source_groups("${ALL_BENCH_SOURCE_FILES}" ${PROJECT_SOURCE_DIR}/..)

# Grab all of our main source files
file(GLOB_RECURSE ALL_MAIN_SOURCE_FILES LIST_DIRECTORIES false
	${PROJECT_SOURCE_DIR}/*.cpp)

create_source_groups("${ALL_MAIN_SOURCE_FILES}" ${PROJECT_SOURCE_DIR})

add_executable(${PROJECT_NAME} MACOSX_BUNDLE ${ALL_BENCH_SOURCE_FILES} ${ALL_MAIN_SOURCE_FILES})

target_link_libraries(${PROJECT_NAME} PRIVATE benchmark)    # Link Google Benchmark

# Disable SIMD if not needed
if(NOT USE_SIMD_INSTRUCTIONS)
	add_definitions(-DRTM_NO_INTRINSICS)
endif()

# Set XCode properties
set_property(TARGET ${PROJECT_NAME} PROPERTY XCODE_ATTRIBUTE_PRODUCT_BUNDLE_IDENTIFIER "com.rtm.rtm-bench")

```

`tools/bench/main_ios/main.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2019 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <benchmark/benchmark.h>

int main(int argc, char* argv[])
{
	benchmark::Initialize(&argc, argv);

	benchmark::RunSpecifiedBenchmarks();

	return 0;
}

```

`tools/bench/sources/bench_mask_all_equal.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2022 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <benchmark/benchmark.h>

#include <rtm/mask4f.h>

#include <cstring>

using namespace rtm;

RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_NOINLINE bool RTM_SIMD_CALL mask_all_equal_scalar4f(mask4f_arg0 lhs, mask4f_arg1 rhs) RTM_NO_EXCEPT
{
	return mask_get_x(lhs) == mask_get_x(rhs) &&
		mask_get_y(lhs) == mask_get_y(rhs) &&
		mask_get_z(lhs) == mask_get_z(rhs) &&
		mask_get_w(lhs) == mask_get_w(rhs);
}

RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_NOINLINE bool RTM_SIMD_CALL mask_all_equal_memcmp4f(mask4f_arg0 lhs, mask4f_arg1 rhs) RTM_NO_EXCEPT
{
	return std::memcmp(&lhs, &rhs, sizeof(uint32_t) * 4) == 0;
}

#if defined(RTM_SSE2_INTRINSICS) || defined(RTM_NEON_INTRINSICS)
RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_NOINLINE bool RTM_SIMD_CALL mask_all_equal_cmp4f(mask4f_arg0 lhs, mask4f_arg1 rhs) RTM_NO_EXCEPT
{
#if defined(RTM_SSE2_INTRINSICS)
	// Assembly for this is 16 bytes with SSE2/AVX: pcmpeqd, pmovmskb, cmp, sete, ret
	return _mm_movemask_epi8(_mm_cmpeq_epi32(_mm_castps_si128(lhs), _mm_castps_si128(rhs))) == 0xFFFF;
#elif defined(RTM_NEON_INTRINSICS)
	uint8x16_t mask = vreinterpretq_u8_u32(vceqq_u32(vreinterpretq_u32_f32(lhs), vreinterpretq_u32_f32(rhs)));
	uint8x8x2_t mask_0_8_1_9_2_10_3_11_4_12_5_13_6_14_7_15 = vzip_u8(vget_low_u8(mask), vget_high_u8(mask));
	uint16x4x2_t mask_0_8_4_12_1_9_5_13_2_10_6_14_3_11_7_15 = vzip_u16(vreinterpret_u16_u8(mask_0_8_1_9_2_10_3_11_4_12_5_13_6_14_7_15.val[0]), vreinterpret_u16_u8(mask_0_8_1_9_2_10_3_11_4_12_5_13_6_14_7_15.val[1]));
	return vget_lane_u32(vreinterpret_u32_u16(mask_0_8_4_12_1_9_5_13_2_10_6_14_3_11_7_15.val[0]), 0) == 0xFFFFFFFFU;
#endif
}

// Wins with SSE2 on Ryzen 2990X, same performance as cmp4f but shorter assembly
// With AVX, cmp4f is a bit faster, reason unclear. Assembly remains shorter and xor
// can execute on more ports than pcmpeqd and reciprocal throughput is faster and
// it should avoid a domain change.
RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_NOINLINE bool RTM_SIMD_CALL mask_all_equal_xor4f(mask4f_arg0 lhs, mask4f_arg1 rhs) RTM_NO_EXCEPT
{
#if defined(RTM_SSE2_INTRINSICS)
	// Assembly for this is 11 bytes with SSE2 (13 bytes with AVX): xorps, movmskps, test, sete, ret
	return _mm_movemask_ps(_mm_xor_ps(lhs, rhs)) == 0;
#elif defined(RTM_NEON_INTRINSICS)
	// Generates the following assembly:
	// eor.16b v0, v1, v0
    // ext.16b v1, v0, v0, #0x8
    // zip1.8b v2, v0, v1
    // zip2.8b v0, v0, v1
    // zip1.4h v0, v2, v0
    // fmov   w8, s0
    // cmp    w8, #0x0
    // cset   w0, eq
    // ret

	uint8x16_t mask = vreinterpretq_u8_u32(veorq_u32(vreinterpretq_u32_f32(lhs), vreinterpretq_u32_f32(rhs)));
	uint8x8x2_t mask_0_8_1_9_2_10_3_11_4_12_5_13_6_14_7_15 = vzip_u8(vget_low_u8(mask), vget_high_u8(mask));
	uint16x4x2_t mask_0_8_4_12_1_9_5_13_2_10_6_14_3_11_7_15 = vzip_u16(vreinterpret_u16_u8(mask_0_8_1_9_2_10_3_11_4_12_5_13_6_14_7_15.val[0]), vreinterpret_u16_u8(mask_0_8_1_9_2_10_3_11_4_12_5_13_6_14_7_15.val[1]));
	return vget_lane_u32(vreinterpret_u32_u16(mask_0_8_4_12_1_9_5_13_2_10_6_14_3_11_7_15.val[0]), 0) == 0;
#endif
}

#if defined(RTM_NEON_INTRINSICS)
// On an Apple M1, both mask_all_equal_xor4f and this variant have the same performance
// but this version has much short assembly
RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_NOINLINE bool RTM_SIMD_CALL mask_all_equal_xor_movn(mask4f_arg0 lhs, mask4f_arg1 rhs) RTM_NO_EXCEPT
{
	// Generates the following assembly:
	// eor.16b v0, v1, v0
	// xtn.4h v0, v0
	// fmov   x8, d0
	// cmp    x8, #0x0
	// cset   w0, eq
	// ret

	uint32x4_t mask = veorq_u32(vreinterpretq_u32_f32(lhs), vreinterpretq_u32_f32(rhs));
	uint16x4_t truncated_mask = vmovn_u32(mask);
	return vget_lane_u64(vreinterpret_u64_u16(truncated_mask), 0) == 0;
}
#endif
#endif

static void bm_mask_all_equal_scalar4f(benchmark::State& state)
{
	mask4f m0 = mask_set(true, false, true, false);
	mask4f m1 = mask_set(true, true, false, false);
	mask4f m2 = mask_set(false, true, false, true);
	mask4f m3 = mask_set(false, false, false, false);

	volatile bool r0 = false;
	volatile bool r1 = false;
	volatile bool r2 = false;
	volatile bool r3 = false;

	for (auto _ : state)
	{
		r0 ^= mask_all_equal_scalar4f(m0, m1);
		r1 ^= mask_all_equal_scalar4f(m1, m2);
		r2 ^= mask_all_equal_scalar4f(m2, m3);
		r3 ^= mask_all_equal_scalar4f(m3, m0);
	}

	benchmark::DoNotOptimize(r0);
	benchmark::DoNotOptimize(r1);
	benchmark::DoNotOptimize(r2);
	benchmark::DoNotOptimize(r3);
}

BENCHMARK(bm_mask_all_equal_scalar4f);

static void bm_mask_all_equal_memcmp4f(benchmark::State& state)
{
	mask4f m0 = mask_set(true, false, true, false);
	mask4f m1 = mask_set(true, true, false, false);
	mask4f m2 = mask_set(false, true, false, true);
	mask4f m3 = mask_set(false, false, false, false);

	volatile bool r0 = false;
	volatile bool r1 = false;
	volatile bool r2 = false;
	volatile bool r3 = false;

	for (auto _ : state)
	{
		r0 ^= mask_all_equal_memcmp4f(m0, m1);
		r1 ^= mask_all_equal_memcmp4f(m1, m2);
		r2 ^= mask_all_equal_memcmp4f(m2, m3);
		r3 ^= mask_all_equal_memcmp4f(m3, m0);
	}

	benchmark::DoNotOptimize(r0);
	benchmark::DoNotOptimize(r1);
	benchmark::DoNotOptimize(r2);
	benchmark::DoNotOptimize(r3);
}

BENCHMARK(bm_mask_all_equal_memcmp4f);

#if defined(RTM_SSE2_INTRINSICS) || defined(RTM_NEON_INTRINSICS)
static void bm_mask_all_equal_cmp4f(benchmark::State& state)
{
	mask4f m0 = mask_set(true, false, true, false);
	mask4f m1 = mask_set(true, true, false, false);
	mask4f m2 = mask_set(false, true, false, true);
	mask4f m3 = mask_set(false, false, false, false);

	volatile bool r0 = false;
	volatile bool r1 = false;
	volatile bool r2 = false;
	volatile bool r3 = false;

	for (auto _ : state)
	{
		r0 ^= mask_all_equal_cmp4f(m0, m1);
		r1 ^= mask_all_equal_cmp4f(m1, m2);
		r2 ^= mask_all_equal_cmp4f(m2, m3);
		r3 ^= mask_all_equal_cmp4f(m3, m0);
	}

	benchmark::DoNotOptimize(r0);
	benchmark::DoNotOptimize(r1);
	benchmark::DoNotOptimize(r2);
	benchmark::DoNotOptimize(r3);
}

BENCHMARK(bm_mask_all_equal_cmp4f);

static void bm_mask_all_equal_xor4f(benchmark::State& state)
{
	mask4f m0 = mask_set(true, false, true, false);
	mask4f m1 = mask_set(true, true, false, false);
	mask4f m2 = mask_set(false, true, false, true);
	mask4f m3 = mask_set(false, false, false, false);

	volatile bool r0 = false;
	volatile bool r1 = false;
	volatile bool r2 = false;
	volatile bool r3 = false;

	for (auto _ : state)
	{
		r0 ^= mask_all_equal_xor4f(m0, m1);
		r1 ^= mask_all_equal_xor4f(m1, m2);
		r2 ^= mask_all_equal_xor4f(m2, m3);
		r3 ^= mask_all_equal_xor4f(m3, m0);
	}

	benchmark::DoNotOptimize(r0);
	benchmark::DoNotOptimize(r1);
	benchmark::DoNotOptimize(r2);
	benchmark::DoNotOptimize(r3);
}

BENCHMARK(bm_mask_all_equal_xor4f);

#if defined(RTM_NEON_INTRINSICS)
static void bm_mask_all_equal_xor_movn(benchmark::State& state)
{
	mask4f m0 = mask_set(true, false, true, false);
	mask4f m1 = mask_set(true, true, false, false);
	mask4f m2 = mask_set(false, true, false, true);
	mask4f m3 = mask_set(false, false, false, false);

	volatile bool r0 = false;
	volatile bool r1 = false;
	volatile bool r2 = false;
	volatile bool r3 = false;

	for (auto _ : state)
	{
		r0 ^= mask_all_equal_xor_movn(m0, m1);
		r1 ^= mask_all_equal_xor_movn(m1, m2);
		r2 ^= mask_all_equal_xor_movn(m2, m3);
		r3 ^= mask_all_equal_xor_movn(m3, m0);
	}

	benchmark::DoNotOptimize(r0);
	benchmark::DoNotOptimize(r1);
	benchmark::DoNotOptimize(r2);
	benchmark::DoNotOptimize(r3);
}

BENCHMARK(bm_mask_all_equal_xor_movn);
#endif
#endif

```

`tools/bench/sources/bench_mask_any_equal3.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2022 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <benchmark/benchmark.h>

#include <rtm/mask4f.h>

#include <cstring>

using namespace rtm;

RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_NOINLINE bool RTM_SIMD_CALL mask_any_equal3_scalar4f(mask4f_arg0 lhs, mask4f_arg1 rhs) RTM_NO_EXCEPT
{
	return mask_get_x(lhs) == mask_get_x(rhs) &&
		mask_get_y(lhs) == mask_get_y(rhs) &&
		mask_get_z(lhs) == mask_get_z(rhs);
}

#if defined(RTM_NEON_INTRINSICS)
RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_NOINLINE bool RTM_SIMD_CALL mask_any_equal3_ref(mask4f_arg0 lhs, mask4f_arg1 rhs) RTM_NO_EXCEPT
{
	// Generates the following assembly:
	// eor.16b v0, v1, v0
    // xtn.4h v0, v0
    // fmov   x8, d0
    // mov    x9, #0xffffffffffff
    // bics   xzr, x9, x8
    // cset   w0, ne
    // ret

	uint32x4_t mask = veorq_u32(vreinterpretq_u32_f32(lhs), vreinterpretq_u32_f32(rhs));
	uint16x4_t truncated_mask = vmovn_u32(mask);
	return (vget_lane_u64(vreinterpret_u64_u16(truncated_mask), 0) & 0x0000FFFFFFFFFFFFULL) != 0x0000FFFFFFFFFFFFULL;
}

RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_NOINLINE bool RTM_SIMD_CALL mask_any_equal3_shl(mask4f_arg0 lhs, mask4f_arg1 rhs) RTM_NO_EXCEPT
{
	// Generates the following assembly:
	// eor.16b v0, v1, v0
    // xtn.4h v0, v0
    // fmov   x8, d0
    // mov    x9, #0xffffffffffff
    // bics   xzr, x9, x8
    // cset   w0, ne
    // ret

	uint32x4_t mask = veorq_u32(vreinterpretq_u32_f32(lhs), vreinterpretq_u32_f32(rhs));
	uint16x4_t truncated_mask = vmovn_u32(mask);
	return (~vget_lane_u64(vreinterpret_u64_u16(truncated_mask), 0) << 16) != 0;
}
#endif

static void bm_mask_any_equal3_scalar4f(benchmark::State& state)
{
	mask4f m0 = mask_set(true, false, true, false);
	mask4f m1 = mask_set(true, true, false, false);
	mask4f m2 = mask_set(false, true, false, true);
	mask4f m3 = mask_set(false, false, false, false);

	volatile bool r0 = false;
	volatile bool r1 = false;
	volatile bool r2 = false;
	volatile bool r3 = false;

	for (auto _ : state)
	{
		r0 ^= mask_any_equal3_scalar4f(m0, m1);
		r1 ^= mask_any_equal3_scalar4f(m1, m2);
		r2 ^= mask_any_equal3_scalar4f(m2, m3);
		r3 ^= mask_any_equal3_scalar4f(m3, m0);
	}

	benchmark::DoNotOptimize(r0);
	benchmark::DoNotOptimize(r1);
	benchmark::DoNotOptimize(r2);
	benchmark::DoNotOptimize(r3);
}

BENCHMARK(bm_mask_any_equal3_scalar4f);

#if defined(RTM_NEON_INTRINSICS)
static void bm_mask_any_equal3_ref(benchmark::State& state)
{
	mask4f m0 = mask_set(true, false, true, false);
	mask4f m1 = mask_set(true, true, false, false);
	mask4f m2 = mask_set(false, true, false, true);
	mask4f m3 = mask_set(false, false, false, false);

	volatile bool r0 = false;
	volatile bool r1 = false;
	volatile bool r2 = false;
	volatile bool r3 = false;

	for (auto _ : state)
	{
		r0 ^= mask_any_equal3_ref(m0, m1);
		r1 ^= mask_any_equal3_ref(m1, m2);
		r2 ^= mask_any_equal3_ref(m2, m3);
		r3 ^= mask_any_equal3_ref(m3, m0);
	}

	benchmark::DoNotOptimize(r0);
	benchmark::DoNotOptimize(r1);
	benchmark::DoNotOptimize(r2);
	benchmark::DoNotOptimize(r3);
}

BENCHMARK(bm_mask_any_equal3_ref);

static void bm_mask_any_equal3_shl(benchmark::State& state)
{
	mask4f m0 = mask_set(true, false, true, false);
	mask4f m1 = mask_set(true, true, false, false);
	mask4f m2 = mask_set(false, true, false, true);
	mask4f m3 = mask_set(false, false, false, false);

	volatile bool r0 = false;
	volatile bool r1 = false;
	volatile bool r2 = false;
	volatile bool r3 = false;

	for (auto _ : state)
	{
		r0 ^= mask_any_equal3_shl(m0, m1);
		r1 ^= mask_any_equal3_shl(m1, m2);
		r2 ^= mask_any_equal3_shl(m2, m3);
		r3 ^= mask_any_equal3_shl(m3, m0);
	}

	benchmark::DoNotOptimize(r0);
	benchmark::DoNotOptimize(r1);
	benchmark::DoNotOptimize(r2);
	benchmark::DoNotOptimize(r3);
}

BENCHMARK(bm_mask_any_equal3_shl);
#endif

```

`tools/bench/sources/bench_matrix3x3d_arg_passing.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2024 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <benchmark/benchmark.h>

#include <rtm/matrix3x3d.h>
#include <rtm/quatd.h>

using namespace rtm;

RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_NOINLINE matrix3x3d RTM_SIMD_CALL matrix_mul_passing_current(matrix3x3d_arg0 lhs, matrix3x3d_arg1 rhs) RTM_NO_EXCEPT
{
	vector4d tmp = vector_mul(vector_dup_x(lhs.x_axis), rhs.x_axis);
	tmp = vector_mul_add(vector_dup_y(lhs.x_axis), rhs.y_axis, tmp);
	tmp = vector_mul_add(vector_dup_z(lhs.x_axis), rhs.z_axis, tmp);
	vector4d x_axis = tmp;

	tmp = vector_mul(vector_dup_x(lhs.y_axis), rhs.x_axis);
	tmp = vector_mul_add(vector_dup_y(lhs.y_axis), rhs.y_axis, tmp);
	tmp = vector_mul_add(vector_dup_z(lhs.y_axis), rhs.z_axis, tmp);
	vector4d y_axis = tmp;

	tmp = vector_mul(vector_dup_x(lhs.z_axis), rhs.x_axis);
	tmp = vector_mul_add(vector_dup_y(lhs.z_axis), rhs.y_axis, tmp);
	tmp = vector_mul_add(vector_dup_z(lhs.z_axis), rhs.z_axis, tmp);
	vector4d z_axis = tmp;

	return matrix3x3d{ x_axis, y_axis, z_axis };
}

RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_NOINLINE void RTM_SIMD_CALL matrix_mul_passing_ref(const matrix3x3d& lhs, const matrix3x3d& rhs, matrix3x3d& out_result) RTM_NO_EXCEPT
{
	vector4d tmp = vector_mul(vector_dup_x(lhs.x_axis), rhs.x_axis);
	tmp = vector_mul_add(vector_dup_y(lhs.x_axis), rhs.y_axis, tmp);
	tmp = vector_mul_add(vector_dup_z(lhs.x_axis), rhs.z_axis, tmp);
	vector4d x_axis = tmp;

	tmp = vector_mul(vector_dup_x(lhs.y_axis), rhs.x_axis);
	tmp = vector_mul_add(vector_dup_y(lhs.y_axis), rhs.y_axis, tmp);
	tmp = vector_mul_add(vector_dup_z(lhs.y_axis), rhs.z_axis, tmp);
	vector4d y_axis = tmp;

	tmp = vector_mul(vector_dup_x(lhs.z_axis), rhs.x_axis);
	tmp = vector_mul_add(vector_dup_y(lhs.z_axis), rhs.y_axis, tmp);
	tmp = vector_mul_add(vector_dup_z(lhs.z_axis), rhs.z_axis, tmp);
	vector4d z_axis = tmp;

	out_result = matrix3x3d{ x_axis, y_axis, z_axis };
}

RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_NOINLINE matrix3x3d RTM_SIMD_CALL matrix_mul_passing_value(const matrix3x3d lhs, const matrix3x3d rhs) RTM_NO_EXCEPT
{
	vector4d tmp = vector_mul(vector_dup_x(lhs.x_axis), rhs.x_axis);
	tmp = vector_mul_add(vector_dup_y(lhs.x_axis), rhs.y_axis, tmp);
	tmp = vector_mul_add(vector_dup_z(lhs.x_axis), rhs.z_axis, tmp);
	vector4d x_axis = tmp;

	tmp = vector_mul(vector_dup_x(lhs.y_axis), rhs.x_axis);
	tmp = vector_mul_add(vector_dup_y(lhs.y_axis), rhs.y_axis, tmp);
	tmp = vector_mul_add(vector_dup_z(lhs.y_axis), rhs.z_axis, tmp);
	vector4d y_axis = tmp;

	tmp = vector_mul(vector_dup_x(lhs.z_axis), rhs.x_axis);
	tmp = vector_mul_add(vector_dup_y(lhs.z_axis), rhs.y_axis, tmp);
	tmp = vector_mul_add(vector_dup_z(lhs.z_axis), rhs.z_axis, tmp);
	vector4d z_axis = tmp;

	return matrix3x3d{ x_axis, y_axis, z_axis };
}

static void bm_matrix3x3d_arg_passing_current(benchmark::State& state)
{
	quatd rotation_around_z = quat_from_euler(scalar_deg_to_rad(0.0), scalar_deg_to_rad(90.0), scalar_deg_to_rad(0.0));
	matrix3x3d m0 = matrix_from_quat(rotation_around_z);

	for (auto _ : state)
	{
		// We use the same matrix for input/output to simulate the worst case scenario
		// where we might need store-forwarding to load our inputs
		// In practice, when the function is called, we don't know what produced the inputs
		m0 = matrix_mul_passing_current(m0, m0);
		m0 = matrix_mul_passing_current(m0, m0);
		m0 = matrix_mul_passing_current(m0, m0);
		m0 = matrix_mul_passing_current(m0, m0);
	}

	benchmark::DoNotOptimize(m0);
}

BENCHMARK(bm_matrix3x3d_arg_passing_current);

static void bm_matrix3x3d_arg_passing_ref(benchmark::State& state)
{
	quatd rotation_around_z = quat_from_euler(scalar_deg_to_rad(0.0), scalar_deg_to_rad(90.0), scalar_deg_to_rad(0.0));
	matrix3x3d m0 = matrix_from_quat(rotation_around_z);

	for (auto _ : state)
	{
		// We use the same matrix for input/output to simulate the worst case scenario
		// where we might need store-forwarding to load our inputs
		// In practice, when the function is called, we don't know what produced the inputs
		// Here, we'll populate the input registers with the desired memory addresses which is
		// very cheap but we'll incur memory round-trips and store-forwarding
		matrix_mul_passing_ref(m0, m0, m0);
		matrix_mul_passing_ref(m0, m0, m0);
		matrix_mul_passing_ref(m0, m0, m0);
		matrix_mul_passing_ref(m0, m0, m0);
	}

	benchmark::DoNotOptimize(m0);
}

BENCHMARK(bm_matrix3x3d_arg_passing_ref);

static void bm_matrix3x3d_arg_passing_value(benchmark::State& state)
{
	quatd rotation_around_z = quat_from_euler(scalar_deg_to_rad(0.0), scalar_deg_to_rad(90.0), scalar_deg_to_rad(0.0));
	matrix3x3d m0 = matrix_from_quat(rotation_around_z);

	for (auto _ : state)
	{
		// We use the same matrix for input/output to simulate the worst case scenario
		// where we might need to duplicate input register values
		// In practice, when the function is called, we don't know what produced the inputs
		// Here, we'll populate the input registers with the output register values with 'mov'
		// instructions which is very cheap and we avoid touching memory
		m0 = matrix_mul_passing_value(m0, m0);
		m0 = matrix_mul_passing_value(m0, m0);
		m0 = matrix_mul_passing_value(m0, m0);
		m0 = matrix_mul_passing_value(m0, m0);
	}

	benchmark::DoNotOptimize(m0);
}

BENCHMARK(bm_matrix3x3d_arg_passing_value);

```

`tools/bench/sources/bench_matrix3x3f_arg_passing.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2024 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <benchmark/benchmark.h>

#include <rtm/matrix3x3f.h>
#include <rtm/quatf.h>

using namespace rtm;

RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_NOINLINE matrix3x3f RTM_SIMD_CALL matrix_mul_passing_current(matrix3x3f_arg0 lhs, matrix3x3f_arg1 rhs) RTM_NO_EXCEPT
{
	vector4f tmp = vector_mul(vector_dup_x(lhs.x_axis), rhs.x_axis);
	tmp = vector_mul_add(vector_dup_y(lhs.x_axis), rhs.y_axis, tmp);
	tmp = vector_mul_add(vector_dup_z(lhs.x_axis), rhs.z_axis, tmp);
	vector4f x_axis = tmp;

	tmp = vector_mul(vector_dup_x(lhs.y_axis), rhs.x_axis);
	tmp = vector_mul_add(vector_dup_y(lhs.y_axis), rhs.y_axis, tmp);
	tmp = vector_mul_add(vector_dup_z(lhs.y_axis), rhs.z_axis, tmp);
	vector4f y_axis = tmp;

	tmp = vector_mul(vector_dup_x(lhs.z_axis), rhs.x_axis);
	tmp = vector_mul_add(vector_dup_y(lhs.z_axis), rhs.y_axis, tmp);
	tmp = vector_mul_add(vector_dup_z(lhs.z_axis), rhs.z_axis, tmp);
	vector4f z_axis = tmp;

	return matrix3x3f{ x_axis, y_axis, z_axis };
}

// On ARM64 (Apple clang), the caller places the 3 addresses into registers x0, x1, and x2
// ldp    q0, q1, [x1]
// ldp    q2, q3, [x0]
// fmul.4s v4, v0, v2[0]
// fmla.4s v4, v1, v2[1]
// ldr    q5, [x1, #0x20]
// fmla.4s v4, v5, v2[2]
// fmul.4s v2, v0, v3[0]
// fmla.4s v2, v1, v3[1]
// fmla.4s v2, v5, v3[2]
// ldr    q3, [x0, #0x20]
// fmul.4s v0, v0, v3[0]
// fmla.4s v0, v1, v3[1]
// fmla.4s v0, v5, v3[2]
// stp    q4, q2, [x2]
// str    q0, [x2, #0x20]
// ret
RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_NOINLINE void RTM_SIMD_CALL matrix_mul_passing_ref(const matrix3x3f& lhs, const matrix3x3f& rhs, matrix3x3f& out_result) RTM_NO_EXCEPT
{
	vector4f tmp = vector_mul(vector_dup_x(lhs.x_axis), rhs.x_axis);
	tmp = vector_mul_add(vector_dup_y(lhs.x_axis), rhs.y_axis, tmp);
	tmp = vector_mul_add(vector_dup_z(lhs.x_axis), rhs.z_axis, tmp);
	vector4f x_axis = tmp;

	tmp = vector_mul(vector_dup_x(lhs.y_axis), rhs.x_axis);
	tmp = vector_mul_add(vector_dup_y(lhs.y_axis), rhs.y_axis, tmp);
	tmp = vector_mul_add(vector_dup_z(lhs.y_axis), rhs.z_axis, tmp);
	vector4f y_axis = tmp;

	tmp = vector_mul(vector_dup_x(lhs.z_axis), rhs.x_axis);
	tmp = vector_mul_add(vector_dup_y(lhs.z_axis), rhs.y_axis, tmp);
	tmp = vector_mul_add(vector_dup_z(lhs.z_axis), rhs.z_axis, tmp);
	vector4f z_axis = tmp;

	out_result = matrix3x3f{ x_axis, y_axis, z_axis };
}

// On ARM64 (Apple clang), the caller places the vector values in registers v0, v1, v2, v3, v4, v5 and the result is returned in v0, v1, v2
// fmul.4s v6, v3, v0[0]
// fmla.4s v6, v4, v0[1]
// fmla.4s v6, v5, v0[2]
// fmul.4s v7, v3, v1[0]
// fmla.4s v7, v4, v1[1]
// fmla.4s v7, v5, v1[2]
// fmul.4s v3, v3, v2[0]
// fmla.4s v3, v4, v2[1]
// fmla.4s v3, v5, v2[2]
// mov.16b v0, v6
// mov.16b v1, v7
// mov.16b v2, v3
// ret
RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_NOINLINE matrix3x3f RTM_SIMD_CALL matrix_mul_passing_value(const matrix3x3f lhs, const matrix3x3f rhs) RTM_NO_EXCEPT
{
	vector4f tmp = vector_mul(vector_dup_x(lhs.x_axis), rhs.x_axis);
	tmp = vector_mul_add(vector_dup_y(lhs.x_axis), rhs.y_axis, tmp);
	tmp = vector_mul_add(vector_dup_z(lhs.x_axis), rhs.z_axis, tmp);
	vector4f x_axis = tmp;

	tmp = vector_mul(vector_dup_x(lhs.y_axis), rhs.x_axis);
	tmp = vector_mul_add(vector_dup_y(lhs.y_axis), rhs.y_axis, tmp);
	tmp = vector_mul_add(vector_dup_z(lhs.y_axis), rhs.z_axis, tmp);
	vector4f y_axis = tmp;

	tmp = vector_mul(vector_dup_x(lhs.z_axis), rhs.x_axis);
	tmp = vector_mul_add(vector_dup_y(lhs.z_axis), rhs.y_axis, tmp);
	tmp = vector_mul_add(vector_dup_z(lhs.z_axis), rhs.z_axis, tmp);
	vector4f z_axis = tmp;

	return matrix3x3f{ x_axis, y_axis, z_axis };
}

static void bm_matrix3x3f_arg_passing_current(benchmark::State& state)
{
	quatf rotation_around_z = quat_from_euler(scalar_deg_to_rad(0.0F), scalar_deg_to_rad(90.0F), scalar_deg_to_rad(0.0F));
	matrix3x3f m0 = matrix_from_quat(rotation_around_z);

	for (auto _ : state)
	{
		// We use the same matrix for input/output to simulate the worst case scenario
		// where we might need store-forwarding to load our inputs
		// In practice, when the function is called, we don't know what produced the inputs
		m0 = matrix_mul_passing_current(m0, m0);
		m0 = matrix_mul_passing_current(m0, m0);
		m0 = matrix_mul_passing_current(m0, m0);
		m0 = matrix_mul_passing_current(m0, m0);
	}

	benchmark::DoNotOptimize(m0);
}

BENCHMARK(bm_matrix3x3f_arg_passing_current);

static void bm_matrix3x3f_arg_passing_ref(benchmark::State& state)
{
	quatf rotation_around_z = quat_from_euler(scalar_deg_to_rad(0.0F), scalar_deg_to_rad(90.0F), scalar_deg_to_rad(0.0F));
	matrix3x3f m0 = matrix_from_quat(rotation_around_z);

	for (auto _ : state)
	{
		// We use the same matrix for input/output to simulate the worst case scenario
		// where we might need store-forwarding to load our inputs
		// In practice, when the function is called, we don't know what produced the inputs
		// Here, we'll populate the input registers with the desired memory addresses which is
		// very cheap but we'll incur memory round-trips and store-forwarding
		matrix_mul_passing_ref(m0, m0, m0);
		matrix_mul_passing_ref(m0, m0, m0);
		matrix_mul_passing_ref(m0, m0, m0);
		matrix_mul_passing_ref(m0, m0, m0);
	}

	benchmark::DoNotOptimize(m0);
}

BENCHMARK(bm_matrix3x3f_arg_passing_ref);

static void bm_matrix3x3f_arg_passing_value(benchmark::State& state)
{
	quatf rotation_around_z = quat_from_euler(scalar_deg_to_rad(0.0F), scalar_deg_to_rad(90.0F), scalar_deg_to_rad(0.0F));
	matrix3x3f m0 = matrix_from_quat(rotation_around_z);

	for (auto _ : state)
	{
		// We use the same matrix for input/output to simulate the worst case scenario
		// where we might need to duplicate input register values
		// In practice, when the function is called, we don't know what produced the inputs
		// Here, we'll populate the input registers with the output register values with 'mov'
		// instructions which is very cheap and we avoid touching memory
		m0 = matrix_mul_passing_value(m0, m0);
		m0 = matrix_mul_passing_value(m0, m0);
		m0 = matrix_mul_passing_value(m0, m0);
		m0 = matrix_mul_passing_value(m0, m0);
	}

	benchmark::DoNotOptimize(m0);
}

BENCHMARK(bm_matrix3x3f_arg_passing_value);

```

`tools/bench/sources/bench_quat_conjugate.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2019 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <benchmark/benchmark.h>

#include <rtm/quatf.h>
#include <rtm/impl/bit_cast.impl.h>

using namespace rtm;

// Wins on Ryzen 2990X desktop VS2017 x64 AVX
// Very odd but consistent result. mul/xor have about the same performance in this profile
// but xor should be faster.
RTM_FORCE_NOINLINE quatf RTM_SIMD_CALL quat_conjugate_scalar(quatf_arg0 input) RTM_NO_EXCEPT
{
	return quat_set(-quat_get_x(input), -quat_get_y(input), -quat_get_z(input), quat_get_w(input));
}

// Wins on Haswell laptop x64 AVX
// It seems that on haswell, xor incurs a domain switch penalty and is slower.
RTM_FORCE_NOINLINE quatf RTM_SIMD_CALL quat_conjugate_mul(quatf_arg0 input) RTM_NO_EXCEPT
{
#if defined(RTM_SSE2_INTRINSICS)
	constexpr __m128 signs = { -1.0f, -1.0f, -1.0f, 1.0f };
	return _mm_mul_ps(input, signs);
#elif defined(RTM_NEON_INTRINSICS)
	alignas(16) constexpr float signs_f[4] = { -1.0f, -1.0f, -1.0f, 1.0f };
	const float32x4_t signs = *rtm_impl::bit_cast<const float32x4_t*>(&signs_f[0]);
	return vmulq_f32(input, signs);
#else
	return quat_set(quat_get_x(input) * -1.0f, quat_get_y(input) * -1.0f, quat_get_z(input) * -1.0f, quat_get_w(input));
#endif
}

#if defined(RTM_SSE2_INTRINSICS) || defined(RTM_NEON_INTRINSICS)
// Wins on iPad Pro ARM64
// Wins on Pixel 3 ARM64
// mul/XOR/neg all end up taking 3 instructions, all dependent but XOR is fastest.
// Wins on Ryzen 2990X desktop clang9 x64 AVX
// Performance is about the same as mul.
// Wins on Samsung S8 ARM64
// Same performance as mul/neg
RTM_FORCE_NOINLINE quatf RTM_SIMD_CALL quat_conjugate_xor(quatf_arg0 input) RTM_NO_EXCEPT
{
#if defined(RTM_SSE2_INTRINSICS)
	constexpr __m128 signs = { -0.0f, -0.0f, -0.0f, 0.0f };
	return _mm_xor_ps(input, signs);
#elif defined(RTM_NEON_INTRINSICS)
	alignas(16) constexpr uint32_t signs_u[4] = { 0x80000000U, 0x80000000U, 0x80000000U, 0 };
	const uint32x4_t signs = *rtm_impl::bit_cast<const uint32x4_t*>(&signs_u[0]);
	return vreinterpretq_f32_u32(veorq_u32(vreinterpretq_u32_f32(input), signs));
#else
	#error not implemented
#endif
}
#endif

#if defined(RTM_NEON_INTRINSICS)
// Wins on Pixel 3 ARMv7
// mul/XOR/neg all end up taking 3 instructions, all dependent but neg is fastest.
// Wins on Samsung S8 ARMv7
// XOR/neg/scalar all have the same performance
RTM_FORCE_NOINLINE quatf RTM_SIMD_CALL quat_conjugate_neg(quatf_arg0 input) RTM_NO_EXCEPT
{
	const float32x4_t neg_input = vnegq_f32(input);
	return vsetq_lane_f32(vgetq_lane_f32(input, 3), neg_input, 3);
}
#endif

static void bm_quat_conjugate_scalar(benchmark::State& state)
{
	quatf q0 = quat_identity();
	quatf q1 = quat_identity();
	quatf q2 = quat_identity();
	quatf q3 = quat_identity();

	for (auto _ : state)
	{
		q0 = quat_conjugate_scalar(q0);
		q1 = quat_conjugate_scalar(q1);
		q2 = quat_conjugate_scalar(q2);
		q3 = quat_conjugate_scalar(q3);
	}

	benchmark::DoNotOptimize(q0);
	benchmark::DoNotOptimize(q1);
	benchmark::DoNotOptimize(q2);
	benchmark::DoNotOptimize(q3);
}

BENCHMARK(bm_quat_conjugate_scalar);

static void bm_quat_conjugate_mul(benchmark::State& state)
{
	quatf q0 = quat_identity();
	quatf q1 = quat_identity();
	quatf q2 = quat_identity();
	quatf q3 = quat_identity();

	for (auto _ : state)
	{
		q0 = quat_conjugate_mul(q0);
		q1 = quat_conjugate_mul(q1);
		q2 = quat_conjugate_mul(q2);
		q3 = quat_conjugate_mul(q3);
	}

	benchmark::DoNotOptimize(q0);
	benchmark::DoNotOptimize(q1);
	benchmark::DoNotOptimize(q2);
	benchmark::DoNotOptimize(q3);
}

BENCHMARK(bm_quat_conjugate_mul);

#if defined(RTM_SSE2_INTRINSICS) || defined(RTM_NEON_INTRINSICS)
static void bm_quat_conjugate_xor(benchmark::State& state)
{
	quatf q0 = quat_identity();
	quatf q1 = quat_identity();
	quatf q2 = quat_identity();
	quatf q3 = quat_identity();

	for (auto _ : state)
	{
		q0 = quat_conjugate_xor(q0);
		q1 = quat_conjugate_xor(q1);
		q2 = quat_conjugate_xor(q2);
		q3 = quat_conjugate_xor(q3);
	}

	benchmark::DoNotOptimize(q0);
	benchmark::DoNotOptimize(q1);
	benchmark::DoNotOptimize(q2);
	benchmark::DoNotOptimize(q3);
}

BENCHMARK(bm_quat_conjugate_xor);
#endif

#if defined(RTM_NEON_INTRINSICS)
static void bm_quat_conjugate_neg(benchmark::State& state)
{
	quatf q0 = quat_identity();
	quatf q1 = quat_identity();
	quatf q2 = quat_identity();
	quatf q3 = quat_identity();

	for (auto _ : state)
	{
		q0 = quat_conjugate_neg(q0);
		q1 = quat_conjugate_neg(q1);
		q2 = quat_conjugate_neg(q2);
		q3 = quat_conjugate_neg(q3);
	}

	benchmark::DoNotOptimize(q0);
	benchmark::DoNotOptimize(q1);
	benchmark::DoNotOptimize(q2);
	benchmark::DoNotOptimize(q3);
}

BENCHMARK(bm_quat_conjugate_neg);
#endif

```

`tools/bench/sources/bench_quat_ensure_positive_w.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2022 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <benchmark/benchmark.h>

#include <rtm/packing/quatf.h>
#include <rtm/impl/bit_cast.impl.h>

using namespace rtm;

RTM_FORCE_NOINLINE quatf RTM_SIMD_CALL quat_ensure_positive_w_scalar(quatf_arg0 input) RTM_NO_EXCEPT
{
	return quat_get_w(input) >= 0.f ? input : quat_neg(input);
}

#if defined(RTM_SSE2_INTRINSICS)
RTM_FORCE_NOINLINE quatf RTM_SIMD_CALL quat_ensure_positive_w_sse2(quatf_arg0 input) RTM_NO_EXCEPT
{
	constexpr __m128 sign_bit = { -0.0F, -0.0F, -0.0F, -0.0F };
	const __m128 input_sign = _mm_and_ps(input, sign_bit);
	const __m128 bias = _mm_shuffle_ps(input_sign, input_sign, _MM_SHUFFLE(3, 3, 3, 3));
	return _mm_xor_ps(input, bias);
}
#endif

#if defined(RTM_NEON_INTRINSICS)
RTM_FORCE_NOINLINE quatf RTM_SIMD_CALL quat_ensure_positive_w_neon(quatf_arg0 input) RTM_NO_EXCEPT
{
	alignas(16) constexpr uint32_t sign_bit_i[4] = { 0x80000000U, 0x80000000U, 0x80000000U, 0x80000000U };
	const uint32x4_t sign_bit = *rtm_impl::bit_cast<const uint32x4_t*>(&sign_bit_i[0]);
	const uint32x4_t input_u32 = vreinterpretq_u32_f32(input);
	const uint32x4_t input_sign = vandq_u32(input_u32, sign_bit);
	const uint32x4_t bias = vmovq_n_u32(vgetq_lane_u32(input_sign, 3));
	return vreinterpretq_f32_u32(veorq_u32(input_u32, bias));
}
#endif

static void bm_quat_ensure_positive_w_scalar(benchmark::State& state)
{
	quatf q0 = quat_identity();
	quatf q1 = quat_identity();
	quatf q2 = quat_identity();
	quatf q3 = quat_identity();

	for (auto _ : state)
	{
		q0 = quat_ensure_positive_w_scalar(q0);
		q1 = quat_ensure_positive_w_scalar(q1);
		q2 = quat_ensure_positive_w_scalar(q2);
		q3 = quat_ensure_positive_w_scalar(q3);
	}

	benchmark::DoNotOptimize(q0);
	benchmark::DoNotOptimize(q1);
	benchmark::DoNotOptimize(q2);
	benchmark::DoNotOptimize(q3);
}

BENCHMARK(bm_quat_ensure_positive_w_scalar);

#if defined(RTM_SSE2_INTRINSICS)
static void bm_quat_ensure_positive_w_sse2(benchmark::State& state)
{
	quatf q0 = quat_identity();
	quatf q1 = quat_identity();
	quatf q2 = quat_identity();
	quatf q3 = quat_identity();

	for (auto _ : state)
	{
		q0 = quat_ensure_positive_w_sse2(q0);
		q1 = quat_ensure_positive_w_sse2(q1);
		q2 = quat_ensure_positive_w_sse2(q2);
		q3 = quat_ensure_positive_w_sse2(q3);
	}

	benchmark::DoNotOptimize(q0);
	benchmark::DoNotOptimize(q1);
	benchmark::DoNotOptimize(q2);
	benchmark::DoNotOptimize(q3);
}

BENCHMARK(bm_quat_ensure_positive_w_sse2);
#endif

#if defined(RTM_NEON_INTRINSICS)
static void bm_quat_ensure_positive_w_neon(benchmark::State& state)
{
	quatf q0 = quat_identity();
	quatf q1 = quat_identity();
	quatf q2 = quat_identity();
	quatf q3 = quat_identity();

	for (auto _ : state)
	{
		q0 = quat_ensure_positive_w_neon(q0);
		q1 = quat_ensure_positive_w_neon(q1);
		q2 = quat_ensure_positive_w_neon(q2);
		q3 = quat_ensure_positive_w_neon(q3);
	}

	benchmark::DoNotOptimize(q0);
	benchmark::DoNotOptimize(q1);
	benchmark::DoNotOptimize(q2);
	benchmark::DoNotOptimize(q3);
}

BENCHMARK(bm_quat_ensure_positive_w_neon);
#endif

```

`tools/bench/sources/bench_quat_from_positive_w.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2019 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <benchmark/benchmark.h>

#include <rtm/packing/quatf.h>

using namespace rtm;

// Wins on Samsung S8 ARMv7
// Just a bit faster than neon
RTM_FORCE_NOINLINE quatf RTM_SIMD_CALL quat_from_positive_w_scalar(vector4f_arg0 input) RTM_NO_EXCEPT
{
	// Operation order is important here, due to rounding, ((1.0 - (X*X)) - Y*Y) - Z*Z is more accurate than 1.0 - dot3(xyz, xyz)
	float w_squared = ((1.0F - vector_get_x(input) * vector_get_x(input)) - vector_get_y(input) * vector_get_y(input)) - vector_get_z(input) * vector_get_z(input);
	// w_squared can be negative either due to rounding or due to quantization imprecision, we take the absolute value
	// to ensure the resulting quaternion is always normalized with a positive W component
	float w = scalar_sqrt(scalar_abs(w_squared));
	return quat_set_w(vector_to_quat(input), w);
}

#if defined(RTM_SSE4_INTRINSICS)
RTM_FORCE_NOINLINE quatf RTM_SIMD_CALL quat_from_positive_w_sse4_andnot(vector4f_arg0 input) RTM_NO_EXCEPT
{
	__m128 x2y2z2 = _mm_mul_ps(input, input);
	__m128 one = _mm_set_ss(1.0F);
	__m128 w_squared = _mm_sub_ss(_mm_sub_ss(_mm_sub_ss(one, x2y2z2), _mm_shuffle_ps(x2y2z2, x2y2z2, _MM_SHUFFLE(1, 1, 1, 1))), _mm_shuffle_ps(x2y2z2, x2y2z2, _MM_SHUFFLE(2, 2, 2, 2)));
	w_squared = _mm_andnot_ps(_mm_set_ss(-0.0F), w_squared);
	__m128 w = _mm_sqrt_ss(w_squared);
	return _mm_insert_ps(input, w, 0x30);
}

// Wins on Ryzen 2990X desktop clang9 x64 AVX
RTM_FORCE_NOINLINE quatf RTM_SIMD_CALL quat_from_positive_w_sse4_and(vector4f_arg0 input) RTM_NO_EXCEPT
{
	const __m128i abs_mask = _mm_set_epi32(0x7FFFFFFFULL, 0x7FFFFFFFULL, 0x7FFFFFFFULL, 0x7FFFFFFFULL);

	__m128 x2y2z2 = _mm_mul_ps(input, input);
	__m128 one = _mm_set_ss(1.0F);
	__m128 w_squared = _mm_sub_ss(_mm_sub_ss(_mm_sub_ss(one, x2y2z2), _mm_shuffle_ps(x2y2z2, x2y2z2, _MM_SHUFFLE(1, 1, 1, 1))), _mm_shuffle_ps(x2y2z2, x2y2z2, _MM_SHUFFLE(2, 2, 2, 2)));
	w_squared = _mm_and_ps(w_squared, _mm_castsi128_ps(abs_mask));
	__m128 w = _mm_sqrt_ss(w_squared);
	return _mm_insert_ps(input, w, 0x30);
}
#endif

#if defined(RTM_SSE2_INTRINSICS)
// Wins on Haswell laptop x64 AVX
// The various variants are almost all the same.
// Wins on Ryzen 2990X desktop VS2017 x64 AVX
// By all accounts, quat_from_positive_w_sse4_and should be faster since it uses nearly the same code
// except that it needs 1 instruction instead of 2 to set the W component. It ends up being dramatically
// slower because VS2017 decides to use XMM6 and spills it on the stack instead of using the volatile XMM5
// register which is unused and doesn't need spilling...
RTM_FORCE_NOINLINE quatf RTM_SIMD_CALL quat_from_positive_w_sse2_and(vector4f_arg0 input) RTM_NO_EXCEPT
{
	const __m128i abs_mask = _mm_set_epi32(0x7FFFFFFFULL, 0x7FFFFFFFULL, 0x7FFFFFFFULL, 0x7FFFFFFFULL);

	__m128 x2y2z2 = _mm_mul_ps(input, input);
	__m128 one = _mm_set_ss(1.0F);
	__m128 w_squared = _mm_sub_ss(_mm_sub_ss(_mm_sub_ss(one, x2y2z2), _mm_shuffle_ps(x2y2z2, x2y2z2, _MM_SHUFFLE(1, 1, 1, 1))), _mm_shuffle_ps(x2y2z2, x2y2z2, _MM_SHUFFLE(2, 2, 2, 2)));
	w_squared = _mm_and_ps(w_squared, _mm_castsi128_ps(abs_mask));
	__m128 w = _mm_sqrt_ss(w_squared);
	__m128 input_wyzx = _mm_shuffle_ps(input, input, _MM_SHUFFLE(0, 2, 1, 3));
	__m128 result_wyzx = _mm_move_ss(input_wyzx, w);
	return _mm_shuffle_ps(result_wyzx, result_wyzx, _MM_SHUFFLE(0, 2, 1, 3));
}

// With VS2019, slower on Ryzen 2990X with SSE2/AVX most likely because it uses more registers
// The cmpeq ends up converted into a constant load with 0xffffffff no matter what
RTM_FORCE_NOINLINE quatf RTM_SIMD_CALL quat_from_positive_w_sse2_and_synt(vector4f_arg0 input) RTM_NO_EXCEPT
{
	const __m128i zero = _mm_setzero_si128();
	const __m128i true_mask = _mm_cmpeq_epi32(zero, zero);
	//const __m128i true_mask = _mm_cmpeq_epi32(_mm_castps_si128(input), _mm_castps_si128(input));
	const __m128i abs_mask = _mm_srli_epi32(true_mask, 1);

	__m128 x2y2z2 = _mm_mul_ps(input, input);
	__m128 one = _mm_castsi128_ps(_mm_srli_epi32(_mm_slli_epi32(true_mask, 25), 2));
	__m128 w_squared = _mm_sub_ss(_mm_sub_ss(_mm_sub_ss(one, x2y2z2), _mm_shuffle_ps(x2y2z2, x2y2z2, _MM_SHUFFLE(1, 1, 1, 1))), _mm_shuffle_ps(x2y2z2, x2y2z2, _MM_SHUFFLE(2, 2, 2, 2)));
	w_squared = _mm_and_ps(w_squared, _mm_castsi128_ps(abs_mask));
	__m128 w = _mm_sqrt_ss(w_squared);
	__m128 input_wyzx = _mm_shuffle_ps(input, input, _MM_SHUFFLE(0, 2, 1, 3));
	__m128 result_wyzx = _mm_move_ss(input_wyzx, w);
	return _mm_shuffle_ps(result_wyzx, result_wyzx, _MM_SHUFFLE(0, 2, 1, 3));
}

RTM_FORCE_NOINLINE quatf RTM_SIMD_CALL quat_from_positive_w_sse2_and2(vector4f_arg0 input) RTM_NO_EXCEPT
{
	const __m128i abs_mask = _mm_set_epi32(0x7FFFFFFFULL, 0x7FFFFFFFULL, 0x7FFFFFFFULL, 0x7FFFFFFFULL);

	__m128 x2y2z2 = _mm_mul_ps(input, input);
	__m128 one = _mm_set_ss(1.0F);
	__m128 w_squared = _mm_sub_ss(_mm_sub_ss(_mm_sub_ss(one, x2y2z2), _mm_shuffle_ps(x2y2z2, x2y2z2, _MM_SHUFFLE(1, 1, 1, 1))), _mm_shuffle_ps(x2y2z2, x2y2z2, _MM_SHUFFLE(2, 2, 2, 2)));
	w_squared = _mm_and_ps(w_squared, _mm_castsi128_ps(abs_mask));
	__m128 w = _mm_sqrt_ss(w_squared);
	__m128 zzww = _mm_shuffle_ps(input, w, _MM_SHUFFLE(0, 0, 2, 2));
	return _mm_shuffle_ps(input, zzww, _MM_SHUFFLE(2, 0, 1, 0));
}
#endif

#if defined(RTM_NEON_INTRINSICS)
// Wins on iPad Pro ARM64
// Same perf as ref
// Wins on Pixel 3 ARM64
// Wins on Pixel 3 ARMv7
// Wins on Samsung S8 ARM64
RTM_FORCE_NOINLINE quatf RTM_SIMD_CALL quat_from_positive_w_neon(vector4f_arg0 input) RTM_NO_EXCEPT
{
	float32x4_t x2y2z2 = vmulq_f32(input, input);
	float w_squared = ((1.0F - vgetq_lane_f32(x2y2z2, 0)) - vgetq_lane_f32(x2y2z2, 1)) - vgetq_lane_f32(x2y2z2, 2);
	float w = rtm::scalar_sqrt(rtm::scalar_abs(w_squared));
	return vsetq_lane_f32(w, input, 3);
}

// As it turns out, on ARM64, fmov can be used to load certain kind of constants
// using an immediate value. This works for 1.0 and the float sign bit.
// See: https://stackoverflow.com/questions/64608307/how-do-i-move-a-floating-point-constant-into-an-fp-register
RTM_FORCE_NOINLINE quatf RTM_SIMD_CALL quat_from_positive_w_neon_synt(vector4f_arg0 input) RTM_NO_EXCEPT
{
	float32x4_t x2y2z2 = vmulq_f32(input, input);
	float one = vget_lane_f32(vreinterpret_f32_u32(vshr_n_u32(vshl_n_u32(vceq_u32(vreinterpret_u32_f32(vget_low_f32(input)), vreinterpret_u32_f32(vget_low_f32(input))), 25), 2)), 0);
	float w_squared = ((one - vgetq_lane_f32(x2y2z2, 0)) - vgetq_lane_f32(x2y2z2, 1)) - vgetq_lane_f32(x2y2z2, 2);
	float w = rtm::scalar_sqrt(rtm::scalar_abs(w_squared));
	return vsetq_lane_f32(w, input, 3);
}

#if defined(RTM_IMPL_VFMSS_SUPPORTED)
RTM_FORCE_NOINLINE quatf RTM_SIMD_CALL quat_from_positive_w_neon_vfmss(vector4f_arg0 input) RTM_NO_EXCEPT
{
	// 1.0 - (x * x)
	float result = vfmss_laneq_f32(1.0F, vgetq_lane_f32(input, 0), input, 0);
	// result - (y * y)
	result = vfmss_laneq_f32(result, vgetq_lane_f32(input, 1), input, 1);
	// result - (z * z)
	float w_squared = vfmss_laneq_f32(result, vgetq_lane_f32(input, 2), input, 2);
	float w = scalar_sqrt(scalar_abs(w_squared));
	return vsetq_lane_f32(w, input, 3);
}

RTM_FORCE_NOINLINE quatf RTM_SIMD_CALL quat_from_positive_w_neon_synt_vfmss(vector4f_arg0 input) RTM_NO_EXCEPT
{
	float one = vget_lane_f32(vreinterpret_f32_u32(vshr_n_u32(vshl_n_u32(vceq_u32(vreinterpret_u32_f32(vget_low_f32(input)), vreinterpret_u32_f32(vget_low_f32(input))), 25), 2)), 0);
	// 1.0 - (x * x)
	float result = vfmss_laneq_f32(one, vgetq_lane_f32(input, 0), input, 0);
	// result - (y * y)
	result = vfmss_laneq_f32(result, vgetq_lane_f32(input, 1), input, 1);
	// result - (z * z)
	float w_squared = vfmss_laneq_f32(result, vgetq_lane_f32(input, 2), input, 2);
	float w = scalar_sqrt(scalar_abs(w_squared));
	return vsetq_lane_f32(w, input, 3);
}
#endif
#endif

static void bm_quat_from_positive_w_scalar(benchmark::State& state)
{
	quatf q0 = quat_identity();
	quatf q1 = quat_identity();
	quatf q2 = quat_identity();
	quatf q3 = quat_identity();

	for (auto _ : state)
	{
		q0 = quat_from_positive_w_scalar(quat_to_vector(q0));
		q1 = quat_from_positive_w_scalar(quat_to_vector(q1));
		q2 = quat_from_positive_w_scalar(quat_to_vector(q2));
		q3 = quat_from_positive_w_scalar(quat_to_vector(q3));
	}

	benchmark::DoNotOptimize(q0);
	benchmark::DoNotOptimize(q1);
	benchmark::DoNotOptimize(q2);
	benchmark::DoNotOptimize(q3);
}

BENCHMARK(bm_quat_from_positive_w_scalar);

#if defined(RTM_SSE4_INTRINSICS)
static void bm_quat_from_positive_w_sse4_andnot(benchmark::State& state)
{
	quatf q0 = quat_identity();
	quatf q1 = quat_identity();
	quatf q2 = quat_identity();
	quatf q3 = quat_identity();

	for (auto _ : state)
	{
		q0 = quat_from_positive_w_sse4_andnot(q0);
		q1 = quat_from_positive_w_sse4_andnot(q1);
		q2 = quat_from_positive_w_sse4_andnot(q2);
		q3 = quat_from_positive_w_sse4_andnot(q3);
	}

	benchmark::DoNotOptimize(q0);
	benchmark::DoNotOptimize(q1);
	benchmark::DoNotOptimize(q2);
	benchmark::DoNotOptimize(q3);
}

BENCHMARK(bm_quat_from_positive_w_sse4_andnot);

static void bm_quat_from_positive_w_sse4_and(benchmark::State& state)
{
	quatf q0 = quat_identity();
	quatf q1 = quat_identity();
	quatf q2 = quat_identity();
	quatf q3 = quat_identity();

	for (auto _ : state)
	{
		q0 = quat_from_positive_w_sse4_and(q0);
		q1 = quat_from_positive_w_sse4_and(q1);
		q2 = quat_from_positive_w_sse4_and(q2);
		q3 = quat_from_positive_w_sse4_and(q3);
	}

	benchmark::DoNotOptimize(q0);
	benchmark::DoNotOptimize(q1);
	benchmark::DoNotOptimize(q2);
	benchmark::DoNotOptimize(q3);
}

BENCHMARK(bm_quat_from_positive_w_sse4_and);
#endif

#if defined(RTM_SSE2_INTRINSICS)
static void bm_quat_from_positive_w_sse2_and(benchmark::State& state)
{
	quatf q0 = quat_identity();
	quatf q1 = quat_identity();
	quatf q2 = quat_identity();
	quatf q3 = quat_identity();

	for (auto _ : state)
	{
		q0 = quat_from_positive_w_sse2_and(q0);
		q1 = quat_from_positive_w_sse2_and(q1);
		q2 = quat_from_positive_w_sse2_and(q2);
		q3 = quat_from_positive_w_sse2_and(q3);
	}

	benchmark::DoNotOptimize(q0);
	benchmark::DoNotOptimize(q1);
	benchmark::DoNotOptimize(q2);
	benchmark::DoNotOptimize(q3);
}

BENCHMARK(bm_quat_from_positive_w_sse2_and);

static void bm_quat_from_positive_w_sse2_and_synt(benchmark::State& state)
{
	quatf q0 = quat_identity();
	quatf q1 = quat_identity();
	quatf q2 = quat_identity();
	quatf q3 = quat_identity();

	for (auto _ : state)
	{
		q0 = quat_from_positive_w_sse2_and_synt(q0);
		q1 = quat_from_positive_w_sse2_and_synt(q1);
		q2 = quat_from_positive_w_sse2_and_synt(q2);
		q3 = quat_from_positive_w_sse2_and_synt(q3);
	}

	benchmark::DoNotOptimize(q0);
	benchmark::DoNotOptimize(q1);
	benchmark::DoNotOptimize(q2);
	benchmark::DoNotOptimize(q3);
}

BENCHMARK(bm_quat_from_positive_w_sse2_and_synt);

static void bm_quat_from_positive_w_sse2_and2(benchmark::State& state)
{
	quatf q0 = quat_identity();
	quatf q1 = quat_identity();
	quatf q2 = quat_identity();
	quatf q3 = quat_identity();

	for (auto _ : state)
	{
		q0 = quat_from_positive_w_sse2_and2(q0);
		q1 = quat_from_positive_w_sse2_and2(q1);
		q2 = quat_from_positive_w_sse2_and2(q2);
		q3 = quat_from_positive_w_sse2_and2(q3);
	}

	benchmark::DoNotOptimize(q0);
	benchmark::DoNotOptimize(q1);
	benchmark::DoNotOptimize(q2);
	benchmark::DoNotOptimize(q3);
}

BENCHMARK(bm_quat_from_positive_w_sse2_and2);
#endif

#if defined(RTM_NEON_INTRINSICS)
static void bm_quat_from_positive_w_neon(benchmark::State& state)
{
	quatf q0 = quat_identity();
	quatf q1 = quat_identity();
	quatf q2 = quat_identity();
	quatf q3 = quat_identity();

	for (auto _ : state)
	{
		q0 = quat_from_positive_w_neon(q0);
		q1 = quat_from_positive_w_neon(q1);
		q2 = quat_from_positive_w_neon(q2);
		q3 = quat_from_positive_w_neon(q3);
	}

	benchmark::DoNotOptimize(q0);
	benchmark::DoNotOptimize(q1);
	benchmark::DoNotOptimize(q2);
	benchmark::DoNotOptimize(q3);
}

BENCHMARK(bm_quat_from_positive_w_neon);

static void bm_quat_from_positive_w_neon_synt(benchmark::State& state)
{
	quatf q0 = quat_identity();
	quatf q1 = quat_identity();
	quatf q2 = quat_identity();
	quatf q3 = quat_identity();

	for (auto _ : state)
	{
		q0 = quat_from_positive_w_neon_synt(q0);
		q1 = quat_from_positive_w_neon_synt(q1);
		q2 = quat_from_positive_w_neon_synt(q2);
		q3 = quat_from_positive_w_neon_synt(q3);
	}

	benchmark::DoNotOptimize(q0);
	benchmark::DoNotOptimize(q1);
	benchmark::DoNotOptimize(q2);
	benchmark::DoNotOptimize(q3);
}

BENCHMARK(bm_quat_from_positive_w_neon_synt);

#if defined(RTM_IMPL_VFMSS_SUPPORTED)
static void bm_quat_from_positive_w_neon_vfmss(benchmark::State& state)
{
	quatf q0 = quat_identity();
	quatf q1 = quat_identity();
	quatf q2 = quat_identity();
	quatf q3 = quat_identity();

	for (auto _ : state)
	{
		q0 = quat_from_positive_w_neon_vfmss(q0);
		q1 = quat_from_positive_w_neon_vfmss(q1);
		q2 = quat_from_positive_w_neon_vfmss(q2);
		q3 = quat_from_positive_w_neon_vfmss(q3);
	}

	benchmark::DoNotOptimize(q0);
	benchmark::DoNotOptimize(q1);
	benchmark::DoNotOptimize(q2);
	benchmark::DoNotOptimize(q3);
}

BENCHMARK(bm_quat_from_positive_w_neon_vfmss);

static void bm_quat_from_positive_w_neon_synt_vfmss(benchmark::State& state)
{
	quatf q0 = quat_identity();
	quatf q1 = quat_identity();
	quatf q2 = quat_identity();
	quatf q3 = quat_identity();

	for (auto _ : state)
	{
		q0 = quat_from_positive_w_neon_synt_vfmss(q0);
		q1 = quat_from_positive_w_neon_synt_vfmss(q1);
		q2 = quat_from_positive_w_neon_synt_vfmss(q2);
		q3 = quat_from_positive_w_neon_synt_vfmss(q3);
	}

	benchmark::DoNotOptimize(q0);
	benchmark::DoNotOptimize(q1);
	benchmark::DoNotOptimize(q2);
	benchmark::DoNotOptimize(q3);
}

BENCHMARK(bm_quat_from_positive_w_neon_synt_vfmss);
#endif
#endif

```

`tools/bench/sources/bench_quat_mul.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2019 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <benchmark/benchmark.h>

#include <rtm/quatf.h>
#include <rtm/impl/bit_cast.impl.h>

using namespace rtm;

RTM_FORCE_NOINLINE quatf RTM_SIMD_CALL quat_mul_scalar(quatf_arg0 lhs, quatf_arg1 rhs) RTM_NO_EXCEPT
{
	const float lhs_x = quat_get_x(lhs);
	const float lhs_y = quat_get_y(lhs);
	const float lhs_z = quat_get_z(lhs);
	const float lhs_w = quat_get_w(lhs);

	const float rhs_x = quat_get_x(rhs);
	const float rhs_y = quat_get_y(rhs);
	const float rhs_z = quat_get_z(rhs);
	const float rhs_w = quat_get_w(rhs);

	const float x = (rhs_w * lhs_x) + (rhs_x * lhs_w) + (rhs_y * lhs_z) - (rhs_z * lhs_y);
	const float y = (rhs_w * lhs_y) - (rhs_x * lhs_z) + (rhs_y * lhs_w) + (rhs_z * lhs_x);
	const float z = (rhs_w * lhs_z) + (rhs_x * lhs_y) - (rhs_y * lhs_x) + (rhs_z * lhs_w);
	const float w = (rhs_w * lhs_w) - (rhs_x * lhs_x) - (rhs_y * lhs_y) - (rhs_z * lhs_z);

	return quat_set(x, y, z, w);
}

#if defined(RTM_FMA_INTRINSICS)
RTM_FORCE_NOINLINE quatf RTM_SIMD_CALL quat_mul_fma_mul(quatf_arg0 lhs, quatf_arg1 rhs) RTM_NO_EXCEPT
{
	constexpr __m128 control_wzyx = { 1.0f,-1.0f, 1.0f,-1.0f };
	constexpr __m128 control_zwxy = { 1.0f, 1.0f,-1.0f,-1.0f };
	constexpr __m128 control_yxwz = { -1.0f, 1.0f, 1.0f,-1.0f };

	const __m128 r_xxxx = _mm_shuffle_ps(rhs, rhs, _MM_SHUFFLE(0, 0, 0, 0));
	const __m128 r_yyyy = _mm_shuffle_ps(rhs, rhs, _MM_SHUFFLE(1, 1, 1, 1));
	const __m128 r_zzzz = _mm_shuffle_ps(rhs, rhs, _MM_SHUFFLE(2, 2, 2, 2));
	const __m128 r_wwww = _mm_shuffle_ps(rhs, rhs, _MM_SHUFFLE(3, 3, 3, 3));

	const __m128 lxrw_lyrw_lzrw_lwrw = _mm_mul_ps(r_wwww, lhs);
	const __m128 l_wzyx = _mm_shuffle_ps(lhs, lhs, _MM_SHUFFLE(0, 1, 2, 3));

	const __m128 lwrx_lzrx_lyrx_lxrx = _mm_mul_ps(r_xxxx, l_wzyx);
	const __m128 l_zwxy = _mm_shuffle_ps(l_wzyx, l_wzyx, _MM_SHUFFLE(2, 3, 0, 1));

	const __m128 lzry_lwry_lxry_lyry = _mm_mul_ps(r_yyyy, l_zwxy);
	const __m128 l_yxwz = _mm_shuffle_ps(l_zwxy, l_zwxy, _MM_SHUFFLE(0, 1, 2, 3));

	const __m128 lyrz_lxrz_lwrz_lzrz = _mm_mul_ps(r_zzzz, l_yxwz);
	const __m128 result0 = _mm_fmadd_ps(lwrx_lzrx_lyrx_lxrx, control_wzyx, lxrw_lyrw_lzrw_lwrw);
	const __m128 result1 = _mm_fmadd_ps(lzry_lwry_lxry_lyry, control_zwxy, result0);
	return _mm_fmadd_ps(lyrz_lxrz_lwrz_lzrz, control_yxwz, result1);
}

RTM_FORCE_NOINLINE quatf RTM_SIMD_CALL quat_mul_fma_xor(quatf_arg0 lhs, quatf_arg1 rhs) RTM_NO_EXCEPT
{
	constexpr __m128 control_wzyx = { 0.0f,-0.0f, 0.0f,-0.0f };
	constexpr __m128 control_zwxy = { 0.0f, 0.0f,-0.0f,-0.0f };
	constexpr __m128 control_yxwz = { -0.0f, 0.0f, 0.0f,-0.0f };

	const __m128 r_xxxx = _mm_shuffle_ps(rhs, rhs, _MM_SHUFFLE(0, 0, 0, 0));
	const __m128 r_yyyy = _mm_shuffle_ps(rhs, rhs, _MM_SHUFFLE(1, 1, 1, 1));
	const __m128 r_zzzz = _mm_shuffle_ps(rhs, rhs, _MM_SHUFFLE(2, 2, 2, 2));
	const __m128 r_wwww = _mm_shuffle_ps(rhs, rhs, _MM_SHUFFLE(3, 3, 3, 3));

	const __m128 l_wzyx = _mm_shuffle_ps(lhs, lhs, _MM_SHUFFLE(0, 1, 2, 3));

	const __m128 lwrx_lzrx_lyrx_lxrx = _mm_mul_ps(r_xxxx, l_wzyx);
	const __m128 l_zwxy = _mm_shuffle_ps(l_wzyx, l_wzyx, _MM_SHUFFLE(2, 3, 0, 1));

	const __m128 lwrx_nlzrx_lyrx_nlxrx = _mm_xor_ps(lwrx_lzrx_lyrx_lxrx, control_wzyx);

	const __m128 lzry_lwry_lxry_lyry = _mm_mul_ps(r_yyyy, l_zwxy);
	const __m128 l_yxwz = _mm_shuffle_ps(l_zwxy, l_zwxy, _MM_SHUFFLE(0, 1, 2, 3));

	const __m128 lzry_lwry_nlxry_nlyry = _mm_xor_ps(lzry_lwry_lxry_lyry, control_zwxy);

	const __m128 lyrz_lxrz_lwrz_lzrz = _mm_mul_ps(r_zzzz, l_yxwz);
	const __m128 result0 = _mm_fmadd_ps(r_wwww, lhs, lwrx_nlzrx_lyrx_nlxrx);

	const __m128 nlyrz_lxrz_lwrz_wlzrz = _mm_xor_ps(lyrz_lxrz_lwrz_lzrz, control_yxwz);
	const __m128 result1 = _mm_add_ps(lzry_lwry_nlxry_nlyry, nlyrz_lxrz_lwrz_wlzrz);
	return _mm_add_ps(result0, result1);
}
#endif

#if defined(RTM_SSE2_INTRINSICS)
// Wins on Haswell laptop x64 AVX
// It seems that on haswell, xor incurs a domain switch penalty and is slower.
RTM_FORCE_NOINLINE quatf RTM_SIMD_CALL quat_mul_sse_mul(quatf_arg0 lhs, quatf_arg1 rhs) RTM_NO_EXCEPT
{
	constexpr __m128 control_wzyx = { 1.0f,-1.0f, 1.0f,-1.0f };
	constexpr __m128 control_zwxy = { 1.0f, 1.0f,-1.0f,-1.0f };
	constexpr __m128 control_yxwz = { -1.0f, 1.0f, 1.0f,-1.0f };

	const __m128 r_xxxx = _mm_shuffle_ps(rhs, rhs, _MM_SHUFFLE(0, 0, 0, 0));
	const __m128 r_yyyy = _mm_shuffle_ps(rhs, rhs, _MM_SHUFFLE(1, 1, 1, 1));
	const __m128 r_zzzz = _mm_shuffle_ps(rhs, rhs, _MM_SHUFFLE(2, 2, 2, 2));
	const __m128 r_wwww = _mm_shuffle_ps(rhs, rhs, _MM_SHUFFLE(3, 3, 3, 3));

	const __m128 lxrw_lyrw_lzrw_lwrw = _mm_mul_ps(r_wwww, lhs);
	const __m128 l_wzyx = _mm_shuffle_ps(lhs, lhs, _MM_SHUFFLE(0, 1, 2, 3));

	const __m128 lwrx_lzrx_lyrx_lxrx = _mm_mul_ps(r_xxxx, l_wzyx);
	const __m128 l_zwxy = _mm_shuffle_ps(l_wzyx, l_wzyx, _MM_SHUFFLE(2, 3, 0, 1));

	const __m128 lwrx_nlzrx_lyrx_nlxrx = _mm_mul_ps(lwrx_lzrx_lyrx_lxrx, control_wzyx);

	const __m128 lzry_lwry_lxry_lyry = _mm_mul_ps(r_yyyy, l_zwxy);
	const __m128 l_yxwz = _mm_shuffle_ps(l_zwxy, l_zwxy, _MM_SHUFFLE(0, 1, 2, 3));

	const __m128 lzry_lwry_nlxry_nlyry = _mm_mul_ps(lzry_lwry_lxry_lyry, control_zwxy);

	const __m128 lyrz_lxrz_lwrz_lzrz = _mm_mul_ps(r_zzzz, l_yxwz);
	const __m128 result0 = _mm_add_ps(lxrw_lyrw_lzrw_lwrw, lwrx_nlzrx_lyrx_nlxrx);

	const __m128 nlyrz_lxrz_lwrz_wlzrz = _mm_mul_ps(lyrz_lxrz_lwrz_lzrz, control_yxwz);
	const __m128 result1 = _mm_add_ps(lzry_lwry_nlxry_nlyry, nlyrz_lxrz_lwrz_wlzrz);
	return _mm_add_ps(result0, result1);
}

// Wins on Ryzen 2990X desktop VS2017 x64 AVX
// Wins on Ryzen 2990X desktop clang9 x64 AVX
RTM_FORCE_NOINLINE quatf RTM_SIMD_CALL quat_mul_sse_xor(quatf_arg0 lhs, quatf_arg1 rhs) RTM_NO_EXCEPT
{
	constexpr __m128 control_wzyx = { 0.0f,-0.0f, 0.0f,-0.0f };
	constexpr __m128 control_zwxy = { 0.0f, 0.0f,-0.0f,-0.0f };
	constexpr __m128 control_yxwz = { -0.0f, 0.0f, 0.0f,-0.0f };

	const __m128 r_xxxx = _mm_shuffle_ps(rhs, rhs, _MM_SHUFFLE(0, 0, 0, 0));
	const __m128 r_yyyy = _mm_shuffle_ps(rhs, rhs, _MM_SHUFFLE(1, 1, 1, 1));
	const __m128 r_zzzz = _mm_shuffle_ps(rhs, rhs, _MM_SHUFFLE(2, 2, 2, 2));
	const __m128 r_wwww = _mm_shuffle_ps(rhs, rhs, _MM_SHUFFLE(3, 3, 3, 3));

	const __m128 lxrw_lyrw_lzrw_lwrw = _mm_mul_ps(r_wwww, lhs);
	const __m128 l_wzyx = _mm_shuffle_ps(lhs, lhs, _MM_SHUFFLE(0, 1, 2, 3));

	const __m128 lwrx_lzrx_lyrx_lxrx = _mm_mul_ps(r_xxxx, l_wzyx);
	const __m128 l_zwxy = _mm_shuffle_ps(l_wzyx, l_wzyx, _MM_SHUFFLE(2, 3, 0, 1));

	const __m128 lwrx_nlzrx_lyrx_nlxrx = _mm_xor_ps(lwrx_lzrx_lyrx_lxrx, control_wzyx);

	const __m128 lzry_lwry_lxry_lyry = _mm_mul_ps(r_yyyy, l_zwxy);
	const __m128 l_yxwz = _mm_shuffle_ps(l_zwxy, l_zwxy, _MM_SHUFFLE(0, 1, 2, 3));

	const __m128 lzry_lwry_nlxry_nlyry = _mm_xor_ps(lzry_lwry_lxry_lyry, control_zwxy);

	const __m128 lyrz_lxrz_lwrz_lzrz = _mm_mul_ps(r_zzzz, l_yxwz);
	const __m128 result0 = _mm_add_ps(lxrw_lyrw_lzrw_lwrw, lwrx_nlzrx_lyrx_nlxrx);

	const __m128 nlyrz_lxrz_lwrz_wlzrz = _mm_xor_ps(lyrz_lxrz_lwrz_lzrz, control_yxwz);
	const __m128 result1 = _mm_add_ps(lzry_lwry_nlxry_nlyry, nlyrz_lxrz_lwrz_wlzrz);
	return _mm_add_ps(result0, result1);
}
#endif

#if defined(RTM_NEON_INTRINSICS)
// Wins on iPad Pro ARM64
// Wins on Pixel 3 ARM64
// XOR is slower a bit likely due to pipeline stalls and with ARMV7 the zipping variant doesn't
// reduce the number of instructions by as much as ARM64.
RTM_FORCE_NOINLINE quatf RTM_SIMD_CALL quat_mul_neon_mul(quatf_arg0 lhs, quatf_arg1 rhs) RTM_NO_EXCEPT
{
	alignas(16) constexpr float control_wzyx_f[4] = { 1.0f, -1.0f, 1.0f, -1.0f };
	alignas(16) constexpr float control_zwxy_f[4] = { 1.0f, 1.0f, -1.0f, -1.0f };
	alignas(16) constexpr float control_yxwz_f[4] = { -1.0f, 1.0f, 1.0f, -1.0f };

	const float32x4_t control_wzyx = *rtm_impl::bit_cast<const float32x4_t*>(&control_wzyx_f[0]);
	const float32x4_t control_zwxy = *rtm_impl::bit_cast<const float32x4_t*>(&control_zwxy_f[0]);
	const float32x4_t control_yxwz = *rtm_impl::bit_cast<const float32x4_t*>(&control_yxwz_f[0]);

	const float32x2_t r_xy = vget_low_f32(rhs);
	const float32x2_t r_zw = vget_high_f32(rhs);

	const float32x4_t lxrw_lyrw_lzrw_lwrw = vmulq_lane_f32(lhs, r_zw, 1);

	const float32x4_t l_yxwz = vrev64q_f32(lhs);
	const float32x4_t l_wzyx = vcombine_f32(vget_high_f32(l_yxwz), vget_low_f32(l_yxwz));
	const float32x4_t l_zwxy = vrev64q_f32(l_wzyx);

	const float32x4_t lwrx_lzrx_lyrx_lxrx = vmulq_lane_f32(l_wzyx, r_xy, 0);
	const float32x4_t lzry_lwry_lxry_lyry = vmulq_lane_f32(l_zwxy, r_xy, 1);
	const float32x4_t lyrz_lxrz_lwrz_lzrz = vmulq_lane_f32(l_yxwz, r_zw, 0);

#if defined(RTM_NEON64_INTRINSICS)
	const float32x4_t result0 = vfmaq_f32(lxrw_lyrw_lzrw_lwrw, lwrx_lzrx_lyrx_lxrx, control_wzyx);
	const float32x4_t result1 = vfmaq_f32(result0, lzry_lwry_lxry_lyry, control_zwxy);
	return vfmaq_f32(result1, lyrz_lxrz_lwrz_lzrz, control_yxwz);
#else
	const float32x4_t result0 = vmlaq_f32(lxrw_lyrw_lzrw_lwrw, lwrx_lzrx_lyrx_lxrx, control_wzyx);
	const float32x4_t result1 = vmlaq_f32(result0, lzry_lwry_lxry_lyry, control_zwxy);
	return vmlaq_f32(result1, lyrz_lxrz_lwrz_lzrz, control_yxwz);
#endif
}

// Wins on Samsung S8 ARMv7
// XOR is a bit faster than scalar
RTM_FORCE_NOINLINE quatf RTM_SIMD_CALL quat_mul_neon_xor(quatf_arg0 lhs, quatf_arg1 rhs) RTM_NO_EXCEPT
{
	alignas(16) constexpr uint32x4_t control_wzyx_f[4] = { 0, 0x80000000U, 0, 0x80000000U };
	alignas(16) constexpr uint32x4_t control_zwxy_f[4] = { 0, 0, 0x80000000U, 0x80000000U };
	alignas(16) constexpr uint32x4_t control_yxwz_f[4] = { 0x80000000U, 0, 0, 0x80000000U };

	const uint32x4_t control_wzyx = *rtm_impl::bit_cast<const uint32x4_t*>(&control_wzyx_f[0]);
	const uint32x4_t control_zwxy = *rtm_impl::bit_cast<const uint32x4_t*>(&control_zwxy_f[0]);
	const uint32x4_t control_yxwz = *rtm_impl::bit_cast<const uint32x4_t*>(&control_yxwz_f[0]);

	float32x2_t r_xy = vget_low_f32(rhs);
	float32x2_t r_zw = vget_high_f32(rhs);

	float32x4_t lxrw_lyrw_lzrw_lwrw = vmulq_lane_f32(lhs, r_zw, 1);

	float32x4_t l_yxwz = vrev64q_f32(lhs);
	float32x4_t l_wzyx = vcombine_f32(vget_high_f32(l_yxwz), vget_low_f32(l_yxwz));
	float32x4_t lwrx_lzrx_lyrx_lxrx = vmulq_lane_f32(l_wzyx, r_xy, 0);

	float32x4_t result0 = vaddq_f32(vreinterpretq_f32_u32(veorq_u32(vreinterpretq_u32_f32(lwrx_lzrx_lyrx_lxrx), control_wzyx)), lxrw_lyrw_lzrw_lwrw);

	float32x4_t l_zwxy = vrev64q_f32(l_wzyx);
	float32x4_t lzry_lwry_lxry_lyry = vmulq_lane_f32(l_zwxy, r_xy, 1);

	float32x4_t result1 = vaddq_f32(vreinterpretq_f32_u32(veorq_u32(vreinterpretq_u32_f32(lzry_lwry_lxry_lyry), control_zwxy)), result0);

	float32x4_t lyrz_lxrz_lwrz_lzrz = vmulq_lane_f32(l_yxwz, r_zw, 0);

	return vaddq_f32(vreinterpretq_f32_u32(veorq_u32(vreinterpretq_u32_f32(lyrz_lxrz_lwrz_lzrz), control_yxwz)), result1);
}

// Wins on iPad Pro ARM64
RTM_FORCE_NOINLINE quatf RTM_SIMD_CALL quat_mul_neon_neg(quatf_arg0 lhs, quatf_arg1 rhs) RTM_NO_EXCEPT
{
	// Use shuffles and negation instead of loading constants and doing mul/xor.
	// On ARM64, neg and shuffles are usually 2 cycles while xor is still 3 cycles.
	// We have to shuffle things anyway, might as well leverage everything we can.

	// Dispatch rev first, if we can't dual dispatch with neg below, we won't stall it
	// [l.y, l.x, l.w, l.z]
	const float32x4_t y_x_w_z = vrev64q_f32(lhs);

	// [-l.x, -l.y, -l.z, -l.w]
	const float32x4_t neg_lhs = vnegq_f32(lhs);

	// trn([l.y, l.x, l.w, l.z], [-l.x, -l.y, -l.z, -l.w]) = [l.y, -l.x, l.w, -l.z], [l.x, -l.y, l.z, -l.w]
	float32x4x2_t y_nx_w_nz__x_ny_z_nw = vtrnq_f32(y_x_w_z, neg_lhs);

	// [l.w, -l.z, l.y, -l.x]
	float32x4_t l_wzyx = vcombine_f32(vget_high_f32(y_nx_w_nz__x_ny_z_nw.val[0]), vget_low_f32(y_nx_w_nz__x_ny_z_nw.val[0]));

	// [l.z, l.w, -l.x, -l.y]
	float32x4_t l_zwxy = vcombine_f32(vget_high_f32(lhs), vget_low_f32(neg_lhs));

	// neg([l.w, -l.z, l.y, -l.x]) = [-l.w, l.z, -l.y, l.x]
	float32x4_t nw_z_ny_x = vnegq_f32(l_wzyx);

	// [-l.y, l.x, l.w, -l.z]
	float32x4_t l_yxwz = vcombine_f32(vget_high_f32(nw_z_ny_x), vget_low_f32(l_wzyx));

	const float32x2_t r_xy = vget_low_f32(rhs);
	const float32x2_t r_zw = vget_high_f32(rhs);

	const float32x4_t lxrw_lyrw_lzrw_lwrw = vmulq_lane_f32(lhs, r_zw, 1);

#if defined(RTM_NEON64_INTRINSICS)
	const float32x4_t result0 = vfmaq_lane_f32(lxrw_lyrw_lzrw_lwrw, l_wzyx, r_xy, 0);
	const float32x4_t result1 = vfmaq_lane_f32(result0, l_zwxy, r_xy, 1);
	return vfmaq_lane_f32(result1, l_yxwz, r_zw, 0);
#else
	const float32x4_t result0 = vmlaq_lane_f32(lxrw_lyrw_lzrw_lwrw, l_wzyx, r_xy, 0);
	const float32x4_t result1 = vmlaq_lane_f32(result0, l_zwxy, r_xy, 1);
	return vmlaq_lane_f32(result1, l_yxwz, r_zw, 0);
#endif
}
#endif

static void bm_quat_mul_scalar(benchmark::State& state)
{
	quatf q0 = quat_identity();
	quatf q1 = quat_identity();
	quatf q2 = quat_identity();
	quatf q3 = quat_identity();

	for (auto _ : state)
	{
		q0 = quat_mul_scalar(q0, q0);
		q1 = quat_mul_scalar(q1, q1);
		q2 = quat_mul_scalar(q2, q2);
		q3 = quat_mul_scalar(q3, q3);
	}

	benchmark::DoNotOptimize(q0);
	benchmark::DoNotOptimize(q1);
	benchmark::DoNotOptimize(q2);
	benchmark::DoNotOptimize(q3);
}

BENCHMARK(bm_quat_mul_scalar);

#if defined(RTM_FMA_INTRINSICS)
static void bm_quat_mul_fma_mul(benchmark::State& state)
{
	quatf q0 = quat_identity();
	quatf q1 = quat_identity();
	quatf q2 = quat_identity();
	quatf q3 = quat_identity();

	for (auto _ : state)
	{
		q0 = quat_mul_fma_mul(q0, q0);
		q1 = quat_mul_fma_mul(q1, q1);
		q2 = quat_mul_fma_mul(q2, q2);
		q3 = quat_mul_fma_mul(q3, q3);
	}

	benchmark::DoNotOptimize(q0);
	benchmark::DoNotOptimize(q1);
	benchmark::DoNotOptimize(q2);
	benchmark::DoNotOptimize(q3);
}

BENCHMARK(bm_quat_mul_fma_mul);

static void bm_quat_mul_fma_xor(benchmark::State& state)
{
	quatf q0 = quat_identity();
	quatf q1 = quat_identity();
	quatf q2 = quat_identity();
	quatf q3 = quat_identity();

	for (auto _ : state)
	{
		q0 = quat_mul_fma_xor(q0, q0);
		q1 = quat_mul_fma_xor(q1, q1);
		q2 = quat_mul_fma_xor(q2, q2);
		q3 = quat_mul_fma_xor(q3, q3);
	}

	benchmark::DoNotOptimize(q0);
	benchmark::DoNotOptimize(q1);
	benchmark::DoNotOptimize(q2);
	benchmark::DoNotOptimize(q3);
}

BENCHMARK(bm_quat_mul_fma_xor);
#endif

#if defined(RTM_SSE2_INTRINSICS)
static void bm_quat_mul_sse_mul(benchmark::State& state)
{
	quatf q0 = quat_identity();
	quatf q1 = quat_identity();
	quatf q2 = quat_identity();
	quatf q3 = quat_identity();

	for (auto _ : state)
	{
		q0 = quat_mul_sse_mul(q0, q0);
		q1 = quat_mul_sse_mul(q1, q1);
		q2 = quat_mul_sse_mul(q2, q2);
		q3 = quat_mul_sse_mul(q3, q3);
	}

	benchmark::DoNotOptimize(q0);
	benchmark::DoNotOptimize(q1);
	benchmark::DoNotOptimize(q2);
	benchmark::DoNotOptimize(q3);
}

BENCHMARK(bm_quat_mul_sse_mul);

static void bm_quat_mul_sse_xor(benchmark::State& state)
{
	quatf q0 = quat_identity();
	quatf q1 = quat_identity();
	quatf q2 = quat_identity();
	quatf q3 = quat_identity();

	for (auto _ : state)
	{
		q0 = quat_mul_sse_xor(q0, q0);
		q1 = quat_mul_sse_xor(q1, q1);
		q2 = quat_mul_sse_xor(q2, q2);
		q3 = quat_mul_sse_xor(q3, q3);
	}

	benchmark::DoNotOptimize(q0);
	benchmark::DoNotOptimize(q1);
	benchmark::DoNotOptimize(q2);
	benchmark::DoNotOptimize(q3);
}

BENCHMARK(bm_quat_mul_sse_xor);
#endif

#if defined(RTM_NEON_INTRINSICS)
static void bm_quat_mul_neon_mul(benchmark::State& state)
{
	quatf q0 = quat_identity();
	quatf q1 = quat_identity();
	quatf q2 = quat_identity();
	quatf q3 = quat_identity();

	for (auto _ : state)
	{
		q0 = quat_mul_neon_mul(q0, q0);
		q1 = quat_mul_neon_mul(q1, q1);
		q2 = quat_mul_neon_mul(q2, q2);
		q3 = quat_mul_neon_mul(q3, q3);
	}

	benchmark::DoNotOptimize(q0);
	benchmark::DoNotOptimize(q1);
	benchmark::DoNotOptimize(q2);
	benchmark::DoNotOptimize(q3);
}

BENCHMARK(bm_quat_mul_neon_mul);

static void bm_quat_mul_neon_xor(benchmark::State& state)
{
	quatf q0 = quat_identity();
	quatf q1 = quat_identity();
	quatf q2 = quat_identity();
	quatf q3 = quat_identity();

	for (auto _ : state)
	{
		q0 = quat_mul_neon_xor(q0, q0);
		q1 = quat_mul_neon_xor(q1, q1);
		q2 = quat_mul_neon_xor(q2, q2);
		q3 = quat_mul_neon_xor(q3, q3);
	}

	benchmark::DoNotOptimize(q0);
	benchmark::DoNotOptimize(q1);
	benchmark::DoNotOptimize(q2);
	benchmark::DoNotOptimize(q3);
}

BENCHMARK(bm_quat_mul_neon_xor);

static void bm_quat_mul_neon_neg(benchmark::State& state)
{
	quatf q0 = quat_identity();
	quatf q1 = quat_identity();
	quatf q2 = quat_identity();
	quatf q3 = quat_identity();

	for (auto _ : state)
	{
		q0 = quat_mul_neon_neg(q0, q0);
		q1 = quat_mul_neon_neg(q1, q1);
		q2 = quat_mul_neon_neg(q2, q2);
		q3 = quat_mul_neon_neg(q3, q3);
	}

	benchmark::DoNotOptimize(q0);
	benchmark::DoNotOptimize(q1);
	benchmark::DoNotOptimize(q2);
	benchmark::DoNotOptimize(q3);
}

BENCHMARK(bm_quat_mul_neon_neg);
#endif

```

`tools/bench/sources/bench_quat_mul_vector3.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2019 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <benchmark/benchmark.h>

#include <rtm/quatf.h>
#include <rtm/impl/bit_cast.impl.h>

using namespace rtm;

RTM_FORCE_NOINLINE vector4f RTM_SIMD_CALL quat_mul_vector3_ref(vector4f_arg0 vector, quatf_arg1 rotation) RTM_NO_EXCEPT
{
	quatf vector_quat = quat_set_w(vector_to_quat(vector), 0.0f);
	quatf inv_rotation = quat_conjugate(rotation);
	return quat_to_vector(quat_mul(quat_mul(inv_rotation, vector_quat), rotation));
}

#if defined(RTM_FMA_INTRINSICS)
RTM_FORCE_NOINLINE vector4f RTM_SIMD_CALL quat_mul_vector3_fma(vector4f_arg0 vector, quatf_arg1 rotation) RTM_NO_EXCEPT
{
	const __m128 inv_rotation = quat_conjugate(rotation);

	// Normally when we multiply our inverse rotation quaternion with the input vector as a quaternion with W = 0.0.
	// As a result, we can strip the whole part that uses W saving a few instructions.
	// Because we have the rotation and its inverse, we also can use them to avoid flipping the signs
	// when lining up our SIMD additions. For the first quaternion multiplication, we can avoid 3 XORs by
	// doing 2 shuffles instead. The same trick can also be used with the second quaternion multiplication.
	// We also don't care about the result W lane but it comes for free.

	// temp = quat_mul(inv_rotation, vector_quat)
	__m128 temp;
	{
		const __m128 rotation_tmp0 = _mm_shuffle_ps(rotation, inv_rotation, _MM_SHUFFLE(3, 0, 2, 1));		// r.y, r.z, -r.x, -r.w
		const __m128 rotation_tmp1 = _mm_shuffle_ps(rotation, inv_rotation, _MM_SHUFFLE(3, 1, 2, 0));		// r.x, r.z, -r.y, -r.w

		const __m128 v_xxxx = _mm_shuffle_ps(vector, vector, _MM_SHUFFLE(0, 0, 0, 0));
		const __m128 v_yyyy = _mm_shuffle_ps(vector, vector, _MM_SHUFFLE(1, 1, 1, 1));
		const __m128 v_zzzz = _mm_shuffle_ps(vector, vector, _MM_SHUFFLE(2, 2, 2, 2));

		const __m128 rotation_tmp2 = _mm_shuffle_ps(rotation_tmp0, rotation_tmp1, _MM_SHUFFLE(0, 2, 1, 3));	// -r.w, r.z, -r.y, r.x
		const __m128 lwrx_lzrx_lyrx_lxrx = _mm_mul_ps(v_xxxx, rotation_tmp2);

		const __m128 rotation_tmp3 = _mm_shuffle_ps(inv_rotation, rotation, _MM_SHUFFLE(1, 0, 3, 2));		// -r.z, -r.w, r.x, r.y
		const __m128 rotation_tmp4 = _mm_shuffle_ps(rotation_tmp0, rotation_tmp1, _MM_SHUFFLE(1, 3, 2, 0));	// r.y, -r.x, -r.w, r.z

		temp = _mm_fmadd_ps(v_zzzz, rotation_tmp4, _mm_fmadd_ps(v_yyyy, rotation_tmp3, lwrx_lzrx_lyrx_lxrx));
	}

	// result = quat_mul(temp, rotation)
	{
		const __m128 rotation_tmp0 = _mm_shuffle_ps(rotation, inv_rotation, _MM_SHUFFLE(2, 0, 2, 0));		// r.x, r.z, -r.x, -r.z

		__m128 r_xxxx = _mm_shuffle_ps(rotation_tmp0, rotation_tmp0, _MM_SHUFFLE(2, 0, 2, 0));				// r.x, -r.x, r.x, -r.x
		__m128 r_yyyy = _mm_shuffle_ps(rotation, inv_rotation, _MM_SHUFFLE(1, 1, 1, 1));					// r.y, r.y, -r.y, -r.y
		__m128 r_zzzz = _mm_shuffle_ps(rotation_tmp0, rotation_tmp0, _MM_SHUFFLE(3, 1, 1, 3));				// -r.z, r.z, r.z, -r.z
		__m128 r_wwww = _mm_shuffle_ps(rotation, rotation, _MM_SHUFFLE(3, 3, 3, 3));						// r.w, r.w, r.w, r.w

		__m128 lxrw_lyrw_lzrw_lwrw = _mm_mul_ps(r_wwww, temp);

		__m128 t_wzyx = _mm_shuffle_ps(temp, temp, _MM_SHUFFLE(0, 1, 2, 3));
		__m128 t_zwxy = _mm_shuffle_ps(t_wzyx, t_wzyx, _MM_SHUFFLE(2, 3, 0, 1));
		__m128 t_yxwz = _mm_shuffle_ps(t_zwxy, t_zwxy, _MM_SHUFFLE(0, 1, 2, 3));

		__m128 result0 = _mm_fmadd_ps(r_xxxx, t_wzyx, lxrw_lyrw_lzrw_lwrw);
		__m128 result1 = _mm_fmadd_ps(r_yyyy, t_zwxy, result0);
		return _mm_fmadd_ps(r_zzzz, t_yxwz, result1);
	}
}
#endif

#if defined(RTM_SSE2_INTRINSICS)
// Wins on Haswell laptop x64 AVX
// Wins on Ryzen 2990X desktop VS2017 x64 AVX
// Wins on Ryzen 2990X desktop clang9 x64 AVX
RTM_FORCE_NOINLINE vector4f RTM_SIMD_CALL quat_mul_vector3_sse2(vector4f_arg0 vector, quatf_arg1 rotation) RTM_NO_EXCEPT
{
	const __m128 inv_rotation = quat_conjugate(rotation);

	// Normally when we multiply our inverse rotation quaternion with the input vector as a quaternion with W = 0.0.
	// As a result, we can strip the whole part that uses W saving a few instructions.
	// Because we have the rotation and its inverse, we also can use them to avoid flipping the signs
	// when lining up our SIMD additions. For the first quaternion multiplication, we can avoid 3 XORs by
	// doing 2 shuffles instead. The same trick can also be used with the second quaternion multiplication.
	// We also don't care about the result W lane but it comes for free.

	// temp = quat_mul(inv_rotation, vector_quat)
	__m128 temp;
	{
		const __m128 rotation_tmp0 = _mm_shuffle_ps(rotation, inv_rotation, _MM_SHUFFLE(3, 0, 2, 1));		// r.y, r.z, -r.x, -r.w
		const __m128 rotation_tmp1 = _mm_shuffle_ps(rotation, inv_rotation, _MM_SHUFFLE(3, 1, 2, 0));		// r.x, r.z, -r.y, -r.w

		const __m128 v_xxxx = _mm_shuffle_ps(vector, vector, _MM_SHUFFLE(0, 0, 0, 0));
		const __m128 v_yyyy = _mm_shuffle_ps(vector, vector, _MM_SHUFFLE(1, 1, 1, 1));
		const __m128 v_zzzz = _mm_shuffle_ps(vector, vector, _MM_SHUFFLE(2, 2, 2, 2));

		const __m128 rotation_tmp2 = _mm_shuffle_ps(rotation_tmp0, rotation_tmp1, _MM_SHUFFLE(0, 2, 1, 3));	// -r.w, r.z, -r.y, r.x
		const __m128 lwrx_lzrx_lyrx_lxrx = _mm_mul_ps(v_xxxx, rotation_tmp2);

		const __m128 rotation_tmp3 = _mm_shuffle_ps(inv_rotation, rotation, _MM_SHUFFLE(1, 0, 3, 2));		// -r.z, -r.w, r.x, r.y
		const __m128 lzry_lwry_lxry_lyry = _mm_mul_ps(v_yyyy, rotation_tmp3);

		const __m128 rotation_tmp4 = _mm_shuffle_ps(rotation_tmp0, rotation_tmp1, _MM_SHUFFLE(1, 3, 2, 0));	// r.y, -r.x, -r.w, r.z
		const __m128 lyrz_lxrz_lwrz_lzrz = _mm_mul_ps(v_zzzz, rotation_tmp4);

		temp = _mm_add_ps(_mm_add_ps(lwrx_lzrx_lyrx_lxrx, lzry_lwry_lxry_lyry), lyrz_lxrz_lwrz_lzrz);
	}

	// result = quat_mul(temp, rotation)
	{
		const __m128 rotation_tmp0 = _mm_shuffle_ps(rotation, inv_rotation, _MM_SHUFFLE(2, 0, 2, 0));		// r.x, r.z, -r.x, -r.z

		__m128 r_xxxx = _mm_shuffle_ps(rotation_tmp0, rotation_tmp0, _MM_SHUFFLE(2, 0, 2, 0));				// r.x, -r.x, r.x, -r.x
		__m128 r_yyyy = _mm_shuffle_ps(rotation, inv_rotation, _MM_SHUFFLE(1, 1, 1, 1));					// r.y, r.y, -r.y, -r.y
		__m128 r_zzzz = _mm_shuffle_ps(rotation_tmp0, rotation_tmp0, _MM_SHUFFLE(3, 1, 1, 3));				// -r.z, r.z, r.z, -r.z
		__m128 r_wwww = _mm_shuffle_ps(rotation, rotation, _MM_SHUFFLE(3, 3, 3, 3));						// r.w, r.w, r.w, r.w

		__m128 lxrw_lyrw_lzrw_lwrw = _mm_mul_ps(r_wwww, temp);

		__m128 t_wzyx = _mm_shuffle_ps(temp, temp, _MM_SHUFFLE(0, 1, 2, 3));
		__m128 lwrx_lzrx_lyrx_lxrx = _mm_mul_ps(r_xxxx, t_wzyx);

		__m128 t_zwxy = _mm_shuffle_ps(t_wzyx, t_wzyx, _MM_SHUFFLE(2, 3, 0, 1));
		__m128 lzry_lwry_lxry_lyry = _mm_mul_ps(r_yyyy, t_zwxy);

		__m128 t_yxwz = _mm_shuffle_ps(t_zwxy, t_zwxy, _MM_SHUFFLE(0, 1, 2, 3));
		__m128 lyrz_lxrz_lwrz_lzrz = _mm_mul_ps(r_zzzz, t_yxwz);

		__m128 result0 = _mm_add_ps(lxrw_lyrw_lzrw_lwrw, lwrx_lzrx_lyrx_lxrx);
		__m128 result1 = _mm_add_ps(lzry_lwry_lxry_lyry, lyrz_lxrz_lwrz_lzrz);
		return _mm_add_ps(result0, result1);
	}
}
#endif

// Wins on Pixel 3 ARMv7
// Wins on Pixel 3 ARM64
// Scalar is much faster. The zipping impl isn't faster here unlike quat_mul for ARM64, it doesn't reduce the instruction
// count by as much.
// Wins on Samsung S8 ARMv7
// Dramatically faster
// Wins on Samsung S8 ARM64
// Much faster
RTM_FORCE_NOINLINE vector4f RTM_SIMD_CALL quat_mul_vector3_scalar(vector4f_arg0 vector, quatf_arg1 rotation) RTM_NO_EXCEPT
{
#if defined(RTM_NEON_INTRINSICS)
	const float32x4_t n_rotation = vnegq_f32(rotation);
#else
	const quatf n_rotation = quat_conjugate(rotation);
#endif

	// temp = quat_mul(inv_rotation, vector_quat)
	float temp_x;
	float temp_y;
	float temp_z;
	float temp_w;
	{
		const float lhs_x = quat_get_x(n_rotation);
		const float lhs_y = quat_get_y(n_rotation);
		const float lhs_z = quat_get_z(n_rotation);
		const float lhs_w = quat_get_w(rotation);

		const float rhs_x = vector_get_x(vector);
		const float rhs_y = vector_get_y(vector);
		const float rhs_z = vector_get_z(vector);

		temp_x = (rhs_x * lhs_w) + (rhs_y * lhs_z) - (rhs_z * lhs_y);
		temp_y =  -(rhs_x * lhs_z) + (rhs_y * lhs_w) + (rhs_z * lhs_x);
		temp_z = (rhs_x * lhs_y) - (rhs_y * lhs_x) + (rhs_z * lhs_w);
		temp_w =  -(rhs_x * lhs_x) - (rhs_y * lhs_y) - (rhs_z * lhs_z);
	}

	// result = quat_mul(temp, rotation)
	{
		const float lhs_x = temp_x;
		const float lhs_y = temp_y;
		const float lhs_z = temp_z;
		const float lhs_w = temp_w;

		const float rhs_x = quat_get_x(rotation);
		const float rhs_y = quat_get_y(rotation);
		const float rhs_z = quat_get_z(rotation);
		const float rhs_w = quat_get_w(rotation);

		const float x = (rhs_w * lhs_x) + (rhs_x * lhs_w) + (rhs_y * lhs_z) - (rhs_z * lhs_y);
		const float y = (rhs_w * lhs_y) - (rhs_x * lhs_z) + (rhs_y * lhs_w) + (rhs_z * lhs_x);
		const float z = (rhs_w * lhs_z) + (rhs_x * lhs_y) - (rhs_y * lhs_x) + (rhs_z * lhs_w);

		return vector_set(x, y, z, z);
	}
}

#if defined(RTM_NEON_INTRINSICS)
RTM_FORCE_NOINLINE vector4f RTM_SIMD_CALL quat_mul_vector3_neon_neg(vector4f_arg0 vector, quatf_arg1 rotation) RTM_NO_EXCEPT
{
	// Normally when we multiply our inverse rotation quaternion with the input vector as a quaternion with W = 0.0.
	// As a result, we can strip the whole part that uses W saving a few instructions.
	// Use the same tricks as quat_mul

	// temp = quat_mul(inv_rotation, vector_quat)
	float32x4_t temp;
	{
		float32x4_t lhs = quat_conjugate(rotation);
		float32x4_t rhs = vector;

		// Dispatch rev first, if we can't dual dispatch with neg below, we won't stall it
		// [l.y, l.x, l.w, l.z]
		const float32x4_t y_x_w_z = vrev64q_f32(lhs);

		// [-l.x, -l.y, -l.z, -l.w]
		const float32x4_t neg_lhs = vnegq_f32(lhs);

		// trn([l.y, l.x, l.w, l.z], [-l.x, -l.y, -l.z, -l.w]) = [l.y, -l.x, l.w, -l.z], [l.x, -l.y, l.z, -l.w]
		float32x4x2_t y_nx_w_nz__x_ny_z_nw = vtrnq_f32(y_x_w_z, neg_lhs);

		// [l.w, -l.z, l.y, -l.x]
		float32x4_t l_wzyx = vcombine_f32(vget_high_f32(y_nx_w_nz__x_ny_z_nw.val[0]), vget_low_f32(y_nx_w_nz__x_ny_z_nw.val[0]));

		// [l.z, l.w, -l.x, -l.y]
		float32x4_t l_zwxy = vcombine_f32(vget_high_f32(lhs), vget_low_f32(neg_lhs));

		// neg([l.w, -l.z, l.y, -l.x]) = [-l.w, l.z, -l.y, l.x]
		float32x4_t nw_z_ny_x = vnegq_f32(l_wzyx);

		// [-l.y, l.x, l.w, -l.z]
		float32x4_t l_yxwz = vcombine_f32(vget_high_f32(nw_z_ny_x), vget_low_f32(l_wzyx));

		const float32x2_t r_xy = vget_low_f32(rhs);
		const float32x2_t r_zw = vget_high_f32(rhs);

		const float32x4_t result0 = vmulq_lane_f32(l_wzyx, r_xy, 0);

	#if defined(RTM_NEON64_INTRINSICS)
		const float32x4_t result1 = vfmaq_lane_f32(result0, l_zwxy, r_xy, 1);
		temp = vfmaq_lane_f32(result1, l_yxwz, r_zw, 0);
	#else
		const float32x4_t result1 = vmlaq_lane_f32(result0, l_zwxy, r_xy, 1);
		temp = vmlaq_lane_f32(result1, l_yxwz, r_zw, 0);
	#endif
	}

	// result = quat_mul(temp, rotation)
	{
		float32x4_t lhs = temp;
		float32x4_t rhs = rotation;

		// Dispatch rev first, if we can't dual dispatch with neg below, we won't stall it
		// [l.y, l.x, l.w, l.z]
		const float32x4_t y_x_w_z = vrev64q_f32(lhs);

		// [-l.x, -l.y, -l.z, -l.w]
		const float32x4_t neg_lhs = vnegq_f32(lhs);

		// trn([l.y, l.x, l.w, l.z], [-l.x, -l.y, -l.z, -l.w]) = [l.y, -l.x, l.w, -l.z], [l.x, -l.y, l.z, -l.w]
		float32x4x2_t y_nx_w_nz__x_ny_z_nw = vtrnq_f32(y_x_w_z, neg_lhs);

		// [l.w, -l.z, l.y, -l.x]
		float32x4_t l_wzyx = vcombine_f32(vget_high_f32(y_nx_w_nz__x_ny_z_nw.val[0]), vget_low_f32(y_nx_w_nz__x_ny_z_nw.val[0]));

		// [l.z, l.w, -l.x, -l.y]
		float32x4_t l_zwxy = vcombine_f32(vget_high_f32(lhs), vget_low_f32(neg_lhs));

		// neg([l.w, -l.z, l.y, -l.x]) = [-l.w, l.z, -l.y, l.x]
		float32x4_t nw_z_ny_x = vnegq_f32(l_wzyx);

		// [-l.y, l.x, l.w, -l.z]
		float32x4_t l_yxwz = vcombine_f32(vget_high_f32(nw_z_ny_x), vget_low_f32(l_wzyx));

		const float32x2_t r_xy = vget_low_f32(rhs);
		const float32x2_t r_zw = vget_high_f32(rhs);

		const float32x4_t lxrw_lyrw_lzrw_lwrw = vmulq_lane_f32(temp, r_zw, 1);

	#if defined(RTM_NEON64_INTRINSICS)
		const float32x4_t result0 = vfmaq_lane_f32(lxrw_lyrw_lzrw_lwrw, l_wzyx, r_xy, 0);
		const float32x4_t result1 = vfmaq_lane_f32(result0, l_zwxy, r_xy, 1);
		return vfmaq_lane_f32(result1, l_yxwz, r_zw, 0);
	#else
		const float32x4_t result0 = vmlaq_lane_f32(lxrw_lyrw_lzrw_lwrw, l_wzyx, r_xy, 0);
		const float32x4_t result1 = vmlaq_lane_f32(result0, l_zwxy, r_xy, 1);
		return vmlaq_lane_f32(result1, l_yxwz, r_zw, 0);
	#endif
	}
}

// Wins on iPad Pro ARM64
RTM_FORCE_NOINLINE vector4f RTM_SIMD_CALL quat_mul_vector3_neon_mul(vector4f_arg0 vector, quatf_arg1 rotation) RTM_NO_EXCEPT
{
	alignas(16) constexpr float control_wzyx_f[4] = { 1.0f, -1.0f, 1.0f, -1.0f };
	alignas(16) constexpr float control_zwxy_f[4] = { 1.0f, 1.0f, -1.0f, -1.0f };
	alignas(16) constexpr float control_yxwz_f[4] = { -1.0f, 1.0f, 1.0f, -1.0f };

	const float32x4_t control_wzyx = *rtm_impl::bit_cast<const float32x4_t*>(&control_wzyx_f[0]);
	const float32x4_t control_zwxy = *rtm_impl::bit_cast<const float32x4_t*>(&control_zwxy_f[0]);
	const float32x4_t control_yxwz = *rtm_impl::bit_cast<const float32x4_t*>(&control_yxwz_f[0]);

	const float32x4_t inv_rotation = quat_conjugate(rotation);

	// Normally when we multiply our inverse rotation quaternion with the input vector as a quaternion with W = 0.0.
	// As a result, we can strip the whole part that uses W saving a few instructions.
	// We also don't care about the result W lane but it comes for free.
	// With ARMv7, it seems that floating point instructions aren't the bottleneck and using shuffles to move things
	// around to take advantage of sign flipping like the other implementations does not benefit.

	// temp = quat_mul(inv_rotation, vector_quat)
	float32x4_t temp;
	{
		const float32x2_t r_xy = vget_low_f32(vector);
		const float32x2_t r_zw = vget_high_f32(vector);

		const float32x4_t l_yxwz = vrev64q_f32(inv_rotation);
		const float32x4_t l_wzyx = vcombine_f32(vget_high_f32(l_yxwz), vget_low_f32(l_yxwz));
		const float32x4_t l_zwxy = vrev64q_f32(l_wzyx);

		const float32x4_t lwrx_lzrx_lyrx_lxrx = vmulq_lane_f32(l_wzyx, r_xy, 0);
		const float32x4_t lzry_lwry_lxry_lyry = vmulq_lane_f32(l_zwxy, r_xy, 1);
		const float32x4_t lyrz_lxrz_lwrz_lzrz = vmulq_lane_f32(l_yxwz, r_zw, 0);

		const float32x4_t result0 = vmulq_f32(lwrx_lzrx_lyrx_lxrx, control_wzyx);

#if defined(RTM_NEON64_INTRINSICS)
		const float32x4_t result1 = vfmaq_f32(result0, lzry_lwry_lxry_lyry, control_zwxy);
		temp = vfmaq_f32(result1, lyrz_lxrz_lwrz_lzrz, control_yxwz);
#else
		const float32x4_t result1 = vmlaq_f32(result0, lzry_lwry_lxry_lyry, control_zwxy);
		temp = vmlaq_f32(result1, lyrz_lxrz_lwrz_lzrz, control_yxwz);
#endif
	}

	// result = quat_mul(temp, rotation)
	{
		const float32x2_t r_xy = vget_low_f32(rotation);
		const float32x2_t r_zw = vget_high_f32(rotation);

		const float32x4_t lxrw_lyrw_lzrw_lwrw = vmulq_lane_f32(temp, r_zw, 1);

		const float32x4_t l_yxwz = vrev64q_f32(temp);
		const float32x4_t l_wzyx = vcombine_f32(vget_high_f32(l_yxwz), vget_low_f32(l_yxwz));
		const float32x4_t l_zwxy = vrev64q_f32(l_wzyx);

		const float32x4_t lwrx_lzrx_lyrx_lxrx = vmulq_lane_f32(l_wzyx, r_xy, 0);
		const float32x4_t lzry_lwry_lxry_lyry = vmulq_lane_f32(l_zwxy, r_xy, 1);
		const float32x4_t lyrz_lxrz_lwrz_lzrz = vmulq_lane_f32(l_yxwz, r_zw, 0);

#if defined(RTM_NEON64_INTRINSICS)
		const float32x4_t result0 = vfmaq_f32(lxrw_lyrw_lzrw_lwrw, lwrx_lzrx_lyrx_lxrx, control_wzyx);
		const float32x4_t result1 = vfmaq_f32(result0, lzry_lwry_lxry_lyry, control_zwxy);
		return vfmaq_f32(result1, lyrz_lxrz_lwrz_lzrz, control_yxwz);
#else
		const float32x4_t result0 = vmlaq_f32(lxrw_lyrw_lzrw_lwrw, lwrx_lzrx_lyrx_lxrx, control_wzyx);
		const float32x4_t result1 = vmlaq_f32(result0, lzry_lwry_lxry_lyry, control_zwxy);
		return vmlaq_f32(result1, lyrz_lxrz_lwrz_lzrz, control_yxwz);
#endif
	}
}
#endif

static void bm_quat_mul_vector3_ref(benchmark::State& state)
{
	vector4f v0 = vector_set(12.0f, 32.0f, -2.0f);
	vector4f v1 = vector_set(12.0f, 32.0f, -2.0f);
	vector4f v2 = vector_set(12.0f, 32.0f, -2.0f);
	vector4f v3 = vector_set(12.0f, 32.0f, -2.0f);
	quatf q0 = quat_identity();
	quatf q1 = quat_identity();
	quatf q2 = quat_identity();
	quatf q3 = quat_identity();

	for (auto _ : state)
	{
		v0 = quat_mul_vector3_ref(v0, q0);
		v1 = quat_mul_vector3_ref(v1, q1);
		v2 = quat_mul_vector3_ref(v2, q2);
		v3 = quat_mul_vector3_ref(v3, q3);
	}

	benchmark::DoNotOptimize(q0);
	benchmark::DoNotOptimize(q1);
	benchmark::DoNotOptimize(q2);
	benchmark::DoNotOptimize(q3);
	benchmark::DoNotOptimize(v0);
	benchmark::DoNotOptimize(v1);
	benchmark::DoNotOptimize(v2);
	benchmark::DoNotOptimize(v3);
}

BENCHMARK(bm_quat_mul_vector3_ref);

static void bm_quat_mul_vector3_scalar(benchmark::State& state)
{
	vector4f v0 = vector_set(12.0f, 32.0f, -2.0f);
	vector4f v1 = vector_set(12.0f, 32.0f, -2.0f);
	vector4f v2 = vector_set(12.0f, 32.0f, -2.0f);
	vector4f v3 = vector_set(12.0f, 32.0f, -2.0f);
	quatf q0 = quat_identity();
	quatf q1 = quat_identity();
	quatf q2 = quat_identity();
	quatf q3 = quat_identity();

	for (auto _ : state)
	{
		v0 = quat_mul_vector3_scalar(v0, q0);
		v1 = quat_mul_vector3_scalar(v1, q1);
		v2 = quat_mul_vector3_scalar(v2, q2);
		v3 = quat_mul_vector3_scalar(v3, q3);
	}

	benchmark::DoNotOptimize(q0);
	benchmark::DoNotOptimize(q1);
	benchmark::DoNotOptimize(q2);
	benchmark::DoNotOptimize(q3);
	benchmark::DoNotOptimize(v0);
	benchmark::DoNotOptimize(v1);
	benchmark::DoNotOptimize(v2);
	benchmark::DoNotOptimize(v3);
}

BENCHMARK(bm_quat_mul_vector3_scalar);

#if defined(RTM_FMA_INTRINSICS)
static void bm_quat_mul_vector3_fma(benchmark::State& state)
{
	vector4f v0 = vector_set(12.0f, 32.0f, -2.0f);
	vector4f v1 = vector_set(12.0f, 32.0f, -2.0f);
	vector4f v2 = vector_set(12.0f, 32.0f, -2.0f);
	vector4f v3 = vector_set(12.0f, 32.0f, -2.0f);
	quatf q0 = quat_identity();
	quatf q1 = quat_identity();
	quatf q2 = quat_identity();
	quatf q3 = quat_identity();

	for (auto _ : state)
	{
		v0 = quat_mul_vector3_fma(v0, q0);
		v1 = quat_mul_vector3_fma(v1, q1);
		v2 = quat_mul_vector3_fma(v2, q2);
		v3 = quat_mul_vector3_fma(v3, q3);
	}

	benchmark::DoNotOptimize(q0);
	benchmark::DoNotOptimize(q1);
	benchmark::DoNotOptimize(q2);
	benchmark::DoNotOptimize(q3);
	benchmark::DoNotOptimize(v0);
	benchmark::DoNotOptimize(v1);
	benchmark::DoNotOptimize(v2);
	benchmark::DoNotOptimize(v3);
}

BENCHMARK(bm_quat_mul_vector3_fma);
#endif

#if defined(RTM_SSE2_INTRINSICS)
static void bm_quat_mul_vector3_sse2(benchmark::State& state)
{
	vector4f v0 = vector_set(12.0f, 32.0f, -2.0f);
	vector4f v1 = vector_set(12.0f, 32.0f, -2.0f);
	vector4f v2 = vector_set(12.0f, 32.0f, -2.0f);
	vector4f v3 = vector_set(12.0f, 32.0f, -2.0f);
	quatf q0 = quat_identity();
	quatf q1 = quat_identity();
	quatf q2 = quat_identity();
	quatf q3 = quat_identity();

	for (auto _ : state)
	{
		v0 = quat_mul_vector3_sse2(v0, q0);
		v1 = quat_mul_vector3_sse2(v1, q1);
		v2 = quat_mul_vector3_sse2(v2, q2);
		v3 = quat_mul_vector3_sse2(v3, q3);
	}

	benchmark::DoNotOptimize(q0);
	benchmark::DoNotOptimize(q1);
	benchmark::DoNotOptimize(q2);
	benchmark::DoNotOptimize(q3);
	benchmark::DoNotOptimize(v0);
	benchmark::DoNotOptimize(v1);
	benchmark::DoNotOptimize(v2);
	benchmark::DoNotOptimize(v3);
}

BENCHMARK(bm_quat_mul_vector3_sse2);
#endif

#if defined(RTM_NEON_INTRINSICS)
static void bm_quat_mul_vector3_neon_neg(benchmark::State& state)
{
	vector4f v0 = vector_set(12.0f, 32.0f, -2.0f);
	vector4f v1 = vector_set(12.0f, 32.0f, -2.0f);
	vector4f v2 = vector_set(12.0f, 32.0f, -2.0f);
	vector4f v3 = vector_set(12.0f, 32.0f, -2.0f);
	quatf q0 = quat_identity();
	quatf q1 = quat_identity();
	quatf q2 = quat_identity();
	quatf q3 = quat_identity();

	for (auto _ : state)
	{
		v0 = quat_mul_vector3_neon_neg(v0, q0);
		v1 = quat_mul_vector3_neon_neg(v1, q1);
		v2 = quat_mul_vector3_neon_neg(v2, q2);
		v3 = quat_mul_vector3_neon_neg(v3, q3);
	}

	benchmark::DoNotOptimize(q0);
	benchmark::DoNotOptimize(q1);
	benchmark::DoNotOptimize(q2);
	benchmark::DoNotOptimize(q3);
	benchmark::DoNotOptimize(v0);
	benchmark::DoNotOptimize(v1);
	benchmark::DoNotOptimize(v2);
	benchmark::DoNotOptimize(v3);
}

BENCHMARK(bm_quat_mul_vector3_neon_neg);

static void bm_quat_mul_vector3_neon_mul(benchmark::State& state)
{
	vector4f v0 = vector_set(12.0f, 32.0f, -2.0f);
	vector4f v1 = vector_set(12.0f, 32.0f, -2.0f);
	vector4f v2 = vector_set(12.0f, 32.0f, -2.0f);
	vector4f v3 = vector_set(12.0f, 32.0f, -2.0f);
	quatf q0 = quat_identity();
	quatf q1 = quat_identity();
	quatf q2 = quat_identity();
	quatf q3 = quat_identity();

	for (auto _ : state)
	{
		v0 = quat_mul_vector3_neon_mul(v0, q0);
		v1 = quat_mul_vector3_neon_mul(v1, q1);
		v2 = quat_mul_vector3_neon_mul(v2, q2);
		v3 = quat_mul_vector3_neon_mul(v3, q3);
	}

	benchmark::DoNotOptimize(q0);
	benchmark::DoNotOptimize(q1);
	benchmark::DoNotOptimize(q2);
	benchmark::DoNotOptimize(q3);
	benchmark::DoNotOptimize(v0);
	benchmark::DoNotOptimize(v1);
	benchmark::DoNotOptimize(v2);
	benchmark::DoNotOptimize(v3);
}

BENCHMARK(bm_quat_mul_vector3_neon_mul);
#endif

```

`tools/bench/sources/bench_qvv_mul.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2019 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <benchmark/benchmark.h>

#include <rtm/qvvf.h>

using namespace rtm;

RTM_FORCE_NOINLINE qvvf RTM_SIMD_CALL qvv_mul_ref(qvvf_arg0 lhs, qvvf_arg1 rhs) RTM_NO_EXCEPT
{
	const vector4f min_scale = vector_min(lhs.scale, rhs.scale);
	const vector4f scale = vector_mul(lhs.scale, rhs.scale);

	if (vector_any_less_than3(min_scale, vector_zero()))
	{
		// If we have negative scale, we go through a matrix
		const matrix3x4f lhs_mtx = matrix_from_qvv(lhs);
		const matrix3x4f rhs_mtx = matrix_from_qvv(rhs);
		matrix3x4f result_mtx = matrix_mul(lhs_mtx, rhs_mtx);
		result_mtx = matrix_remove_scale(result_mtx);

		const vector4f sign = vector_sign(scale);
		result_mtx.x_axis = vector_mul(result_mtx.x_axis, vector_dup_x(sign));
		result_mtx.y_axis = vector_mul(result_mtx.y_axis, vector_dup_y(sign));
		result_mtx.z_axis = vector_mul(result_mtx.z_axis, vector_dup_z(sign));

		const quatf rotation = quat_from_matrix(result_mtx);
		const vector4f translation = result_mtx.w_axis;
		return qvv_set(rotation, translation, scale);
	}
	else
	{
		const quatf rotation = quat_mul(lhs.rotation, rhs.rotation);
		const vector4f translation = vector_add(quat_mul_vector3(vector_mul(lhs.translation, rhs.scale), rhs.rotation), rhs.translation);
		return qvv_set(rotation, translation, scale);
	}
}

#if defined(RTM_SSE2_INTRINSICS)
// Wins on Haswell laptop x64 AVX
// Wins on Ryzen 2990X desktop VS2017 x64 AVX
// Wins on Ryzen 2990X desktop clang9 x64 AVX
RTM_FORCE_NOINLINE qvvf RTM_SIMD_CALL qvv_mul_sse2(qvvf_arg0 lhs, qvvf_arg1 rhs) RTM_NO_EXCEPT
{
	const vector4f min_scale = vector_min(lhs.scale, rhs.scale);
	const vector4f scale = vector_mul(lhs.scale, rhs.scale);

	if (vector_any_less_than3(min_scale, vector_zero()))
	{
		// If we have negative scale, we go through a matrix
		const matrix3x4f lhs_mtx = matrix_from_qvv(lhs);
		const matrix3x4f rhs_mtx = matrix_from_qvv(rhs);
		matrix3x4f result_mtx = matrix_mul(lhs_mtx, rhs_mtx);
		result_mtx = matrix_remove_scale(result_mtx);

		constexpr __m128 signs = { -0.0f, -0.0f, -0.0f, -0.0f };
		const __m128 sign_bits = _mm_and_ps(scale, signs);	// Mask out the sign bit

		result_mtx.x_axis = _mm_xor_ps(result_mtx.x_axis, _mm_shuffle_ps(sign_bits, sign_bits, _MM_SHUFFLE(0, 0, 0, 0)));
		result_mtx.y_axis = _mm_xor_ps(result_mtx.y_axis, _mm_shuffle_ps(sign_bits, sign_bits, _MM_SHUFFLE(1, 1, 1, 1)));
		result_mtx.z_axis = _mm_xor_ps(result_mtx.z_axis, _mm_shuffle_ps(sign_bits, sign_bits, _MM_SHUFFLE(2, 2, 2, 2)));

		const quatf rotation = quat_from_matrix(result_mtx);
		const vector4f translation = result_mtx.w_axis;
		return qvv_set(rotation, translation, scale);
	}
	else
	{
		const quatf rotation = quat_mul(lhs.rotation, rhs.rotation);
		const vector4f translation = vector_add(quat_mul_vector3(vector_mul(lhs.translation, rhs.scale), rhs.rotation), rhs.translation);
		return qvv_set(rotation, translation, scale);
	}
}
#endif

static void bm_qvv_mul_ref(benchmark::State& state)
{
	qvvf t0 = qvv_identity();
	qvvf t1 = qvv_set(quat_identity(), vector_zero(), vector_set(-1.0f));
	qvvf t2 = qvv_identity();
	qvvf t3 = qvv_set(quat_identity(), vector_zero(), vector_set(-1.0f));

	for (auto _ : state)
	{
		t0 = qvv_mul_ref(t0, t1);
		t2 = qvv_mul_ref(t2, t3);
	}

	benchmark::DoNotOptimize(t0);
	benchmark::DoNotOptimize(t1);
	benchmark::DoNotOptimize(t2);
	benchmark::DoNotOptimize(t3);
}

BENCHMARK(bm_qvv_mul_ref);

#if defined(RTM_SSE2_INTRINSICS)
static void bm_qvv_mul_sse2(benchmark::State& state)
{
	qvvf t0 = qvv_identity();
	qvvf t1 = qvv_set(quat_identity(), vector_zero(), vector_set(-1.0f));
	qvvf t2 = qvv_identity();
	qvvf t3 = qvv_set(quat_identity(), vector_zero(), vector_set(-1.0f));

	for (auto _ : state)
	{
		t0 = qvv_mul_sse2(t0, t1);
		t2 = qvv_mul_sse2(t2, t3);
	}

	benchmark::DoNotOptimize(t0);
	benchmark::DoNotOptimize(t1);
	benchmark::DoNotOptimize(t2);
	benchmark::DoNotOptimize(t3);
}

BENCHMARK(bm_qvv_mul_sse2);
#endif

```

`tools/bench/sources/bench_scalar_abs.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2019 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <benchmark/benchmark.h>

#include <rtm/scalarf.h>

using namespace rtm;

// Wins on Ryzen 2990X desktop VS2017 x64 AVX
// Oddly consistently faster despite the fact that the scalar impl is 3 instructions
// and converts to/from double while sse2_and is 2 instructions (a needless shuffle + and).
// It seems the functions are so short that the timing is dominated by something else.
RTM_FORCE_NOINLINE float RTM_SIMD_CALL scalar_abs_scalar(float input) RTM_NO_EXCEPT
{
	return std::fabs(input);
}

#if defined(RTM_SSE2_INTRINSICS)
// Wins on Haswell laptop x64 AVX
// Both impl are about the same.
// Wins on Ryzen 2990X desktop clang9 x64 AVX
RTM_FORCE_NOINLINE float RTM_SIMD_CALL scalar_abs_sse2_and(float input) RTM_NO_EXCEPT
{
	const __m128i abs_mask = _mm_set_epi32(0x7FFFFFFFULL, 0x7FFFFFFFULL, 0x7FFFFFFFULL, 0x7FFFFFFFULL);
	return _mm_cvtss_f32(_mm_and_ps(_mm_set_ps1(input), _mm_castsi128_ps(abs_mask)));
}
#endif

static void bm_scalar_abs_scalar(benchmark::State& state)
{
	float f0 = -123.134f;
	float f1 = -123.134f;
	float f2 = -123.134f;
	float f3 = -123.134f;
	float f4 = -123.134f;
	float f5 = -123.134f;
	float f6 = -123.134f;
	float f7 = -123.134f;

	for (auto _ : state)
	{
		f0 = scalar_abs_scalar(f0);
		f1 = scalar_abs_scalar(f1);
		f2 = scalar_abs_scalar(f2);
		f3 = scalar_abs_scalar(f3);
		f4 = scalar_abs_scalar(f4);
		f5 = scalar_abs_scalar(f5);
		f6 = scalar_abs_scalar(f6);
		f7 = scalar_abs_scalar(f7);
	}

	benchmark::DoNotOptimize(f0);
	benchmark::DoNotOptimize(f1);
	benchmark::DoNotOptimize(f2);
	benchmark::DoNotOptimize(f3);
	benchmark::DoNotOptimize(f4);
	benchmark::DoNotOptimize(f5);
	benchmark::DoNotOptimize(f6);
	benchmark::DoNotOptimize(f7);
}

BENCHMARK(bm_scalar_abs_scalar);

#if defined(RTM_SSE2_INTRINSICS)
static void bm_scalar_abs_sse2_and(benchmark::State& state)
{
	float f0 = -123.134f;
	float f1 = -123.134f;
	float f2 = -123.134f;
	float f3 = -123.134f;
	float f4 = -123.134f;
	float f5 = -123.134f;
	float f6 = -123.134f;
	float f7 = -123.134f;

	for (auto _ : state)
	{
		f0 = scalar_abs_sse2_and(f0);
		f1 = scalar_abs_sse2_and(f1);
		f2 = scalar_abs_sse2_and(f2);
		f3 = scalar_abs_sse2_and(f3);
		f4 = scalar_abs_sse2_and(f4);
		f5 = scalar_abs_sse2_and(f5);
		f6 = scalar_abs_sse2_and(f6);
		f7 = scalar_abs_sse2_and(f7);
	}

	benchmark::DoNotOptimize(f0);
	benchmark::DoNotOptimize(f1);
	benchmark::DoNotOptimize(f2);
	benchmark::DoNotOptimize(f3);
	benchmark::DoNotOptimize(f4);
	benchmark::DoNotOptimize(f5);
	benchmark::DoNotOptimize(f6);
	benchmark::DoNotOptimize(f7);
}

BENCHMARK(bm_scalar_abs_sse2_and);
#endif

```

`tools/bench/sources/bench_scalar_ceil.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <benchmark/benchmark.h>

#include <rtm/scalarf.h>

using namespace rtm;

RTM_FORCE_NOINLINE float RTM_SIMD_CALL scalar_ceil_scalar(float input) RTM_NO_EXCEPT
{
	return std::ceil(input);
}

#if defined(RTM_SSE2_INTRINSICS)
RTM_FORCE_NOINLINE float RTM_SIMD_CALL scalar_ceil_sse2(float input) RTM_NO_EXCEPT
{
	__m128 input_s = _mm_set_ps1(input);

	// NaN, +- Infinity, and numbers larger or equal to 2^23 remain unchanged
	// since they have no fractional part.

	const __m128i abs_mask = _mm_set_epi32(0x7FFFFFFFULL, 0x7FFFFFFFULL, 0x7FFFFFFFULL, 0x7FFFFFFFULL);
	const __m128 fractional_limit = _mm_set_ps1(8388608.0F); // 2^23

	// Build our mask, larger values that have no fractional part, and infinities will be true
	// Smaller values and NaN will be false
	__m128 abs_input = _mm_and_ps(input_s, _mm_castsi128_ps(abs_mask));
	__m128 is_input_large = _mm_cmpge_ss(abs_input, fractional_limit);

	// Test if our input is NaN with (value != value), it is only true for NaN
	__m128 is_nan = _mm_cmpneq_ss(input_s, input_s);

	// Combine our masks to determine if we should return the original value
	__m128 use_original_input = _mm_or_ps(is_input_large, is_nan);

	// Convert to an integer and back. This does banker's rounding by default
	__m128 integer_part = _mm_cvtepi32_ps(_mm_cvtps_epi32(input_s));

	// Test if the returned value is smaller than the original.
	// A positive input will round towards zero and be lower when we need it to be greater.
	__m128 is_positive = _mm_cmplt_ss(integer_part, input_s);

	// Convert our mask to a float, ~0 yields -1.0 since it is a valid signed integer
	// Negative values will yield a 0.0 bias
	__m128 bias = _mm_cvtepi32_ps(_mm_castps_si128(is_positive));

	// Subtract our bias to properly handle positive values
	integer_part = _mm_sub_ss(integer_part, bias);

	__m128 result = _mm_or_ps(_mm_and_ps(use_original_input, input_s), _mm_andnot_ps(use_original_input, integer_part));
	return _mm_cvtss_f32(result);
}
#endif

#if defined(RTM_NEON_INTRINSICS)
RTM_FORCE_NOINLINE float RTM_SIMD_CALL scalar_ceil_neon(float input) RTM_NO_EXCEPT
{
	// NaN, +- Infinity, and numbers larger or equal to 2^23 remain unchanged
	// since they have no fractional part.

	const float fractional_limit = 8388608.0F; // 2^23

	// Build our mask, larger values that have no fractional part, and infinities will be true
	// Smaller values and NaN will be false
	float abs_input = std::abs(input);
	bool is_input_large = abs_input >= fractional_limit;

	// Test if our input is NaN with (value != value), it is only true for NaN
	bool is_nan = input != input;

	// Combine our masks to determine if we should return the original value
	bool use_original_input = is_input_large | is_nan;

	// Convert to an integer and back. This does banker's rounding by default
	float integer_part = static_cast<float>(static_cast<int32_t>(input));

	// Test if the returned value is smaller than the original.
	// A positive input will round towards zero and be lower when we need it to be greater.
	bool is_positive = integer_part < input;

	// Add our bias to properly handle positive values
	integer_part = is_positive ? (integer_part + 1.0F) : integer_part;

	return use_original_input ? input : integer_part;
}
#endif

static void bm_scalar_ceil_scalar(benchmark::State& state)
{
	float f0 = -123.134f;
	float f1 = 123.134f;
	float f2 = -123.134f;
	float f3 = 123.134f;
	float f4 = -123.134f;
	float f5 = 123.134f;
	float f6 = -123.134f;
	float f7 = 123.134f;

	for (auto _ : state)
	{
		f0 = scalar_ceil_scalar(f0);
		f1 = scalar_ceil_scalar(f1);
		f2 = scalar_ceil_scalar(f2);
		f3 = scalar_ceil_scalar(f3);
		f4 = scalar_ceil_scalar(f4);
		f5 = scalar_ceil_scalar(f5);
		f6 = scalar_ceil_scalar(f6);
		f7 = scalar_ceil_scalar(f7);
	}

	benchmark::DoNotOptimize(f0);
	benchmark::DoNotOptimize(f1);
	benchmark::DoNotOptimize(f2);
	benchmark::DoNotOptimize(f3);
	benchmark::DoNotOptimize(f4);
	benchmark::DoNotOptimize(f5);
	benchmark::DoNotOptimize(f6);
	benchmark::DoNotOptimize(f7);
}

BENCHMARK(bm_scalar_ceil_scalar);

#if defined(RTM_SSE2_INTRINSICS)
static void bm_scalar_ceil_sse2(benchmark::State& state)
{
	float f0 = -123.134f;
	float f1 = 123.134f;
	float f2 = -123.134f;
	float f3 = 123.134f;
	float f4 = -123.134f;
	float f5 = 123.134f;
	float f6 = -123.134f;
	float f7 = 123.134f;

	for (auto _ : state)
	{
		f0 = scalar_ceil_sse2(f0);
		f1 = scalar_ceil_sse2(f1);
		f2 = scalar_ceil_sse2(f2);
		f3 = scalar_ceil_sse2(f3);
		f4 = scalar_ceil_sse2(f4);
		f5 = scalar_ceil_sse2(f5);
		f6 = scalar_ceil_sse2(f6);
		f7 = scalar_ceil_sse2(f7);
	}

	benchmark::DoNotOptimize(f0);
	benchmark::DoNotOptimize(f1);
	benchmark::DoNotOptimize(f2);
	benchmark::DoNotOptimize(f3);
	benchmark::DoNotOptimize(f4);
	benchmark::DoNotOptimize(f5);
	benchmark::DoNotOptimize(f6);
	benchmark::DoNotOptimize(f7);
}

BENCHMARK(bm_scalar_ceil_sse2);
#endif

#if defined(RTM_NEON_INTRINSICS)
static void bm_scalar_ceil_neon(benchmark::State& state)
{
	float f0 = -123.134f;
	float f1 = 123.134f;
	float f2 = -123.134f;
	float f3 = 123.134f;
	float f4 = -123.134f;
	float f5 = 123.134f;
	float f6 = -123.134f;
	float f7 = 123.134f;

	for (auto _ : state)
	{
		f0 = scalar_ceil_neon(f0);
		f1 = scalar_ceil_neon(f1);
		f2 = scalar_ceil_neon(f2);
		f3 = scalar_ceil_neon(f3);
		f4 = scalar_ceil_neon(f4);
		f5 = scalar_ceil_neon(f5);
		f6 = scalar_ceil_neon(f6);
		f7 = scalar_ceil_neon(f7);
	}

	benchmark::DoNotOptimize(f0);
	benchmark::DoNotOptimize(f1);
	benchmark::DoNotOptimize(f2);
	benchmark::DoNotOptimize(f3);
	benchmark::DoNotOptimize(f4);
	benchmark::DoNotOptimize(f5);
	benchmark::DoNotOptimize(f6);
	benchmark::DoNotOptimize(f7);
}

BENCHMARK(bm_scalar_ceil_neon);
#endif

```

`tools/bench/sources/bench_scalar_floor.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <benchmark/benchmark.h>

#include <rtm/scalarf.h>

using namespace rtm;

RTM_FORCE_NOINLINE float RTM_SIMD_CALL scalar_floor_scalar(float input) RTM_NO_EXCEPT
{
	return std::floor(input);
}

#if defined(RTM_SSE2_INTRINSICS)
RTM_FORCE_NOINLINE float RTM_SIMD_CALL scalar_floor_sse2(float input) RTM_NO_EXCEPT
{
	__m128 input_s = _mm_set_ps1(input);

	// NaN, +- Infinity, and numbers larger or equal to 2^23 remain unchanged
	// since they have no fractional part.

	const __m128i abs_mask = _mm_set_epi32(0x7FFFFFFFULL, 0x7FFFFFFFULL, 0x7FFFFFFFULL, 0x7FFFFFFFULL);
	const __m128 fractional_limit = _mm_set_ps1(8388608.0F); // 2^23

	// Build our mask, larger values that have no fractional part, and infinities will be true
	// Smaller values and NaN will be false
	__m128 abs_input = _mm_and_ps(input_s, _mm_castsi128_ps(abs_mask));
	__m128 is_input_large = _mm_cmpge_ss(abs_input, fractional_limit);

	// Test if our input is NaN with (value != value), it is only true for NaN
	__m128 is_nan = _mm_cmpneq_ss(input_s, input_s);

	// Combine our masks to determine if we should return the original value
	__m128 use_original_input = _mm_or_ps(is_input_large, is_nan);

	// Convert to an integer and back. This does banker's rounding by default
	__m128 integer_part = _mm_cvtepi32_ps(_mm_cvtps_epi32(input_s));

	// Test if the returned value is greater than the original.
	// A negative input will round towards zero and be greater when we need it to be smaller.
	__m128 is_negative = _mm_cmpgt_ss(integer_part, input_s);

	// Convert our mask to a float, ~0 yields -1.0 since it is a valid signed integer
	// Positive values will yield a 0.0 bias
	__m128 bias = _mm_cvtepi32_ps(_mm_castps_si128(is_negative));

	// Add our bias to properly handle negative values
	integer_part = _mm_add_ss(integer_part, bias);

	__m128 result = _mm_or_ps(_mm_and_ps(use_original_input, input_s), _mm_andnot_ps(use_original_input, integer_part));
	return _mm_cvtss_f32(result);
}
#endif

#if defined(RTM_NEON_INTRINSICS)
RTM_FORCE_NOINLINE float RTM_SIMD_CALL scalar_floor_neon(float input) RTM_NO_EXCEPT
{
	// NaN, +- Infinity, and numbers larger or equal to 2^23 remain unchanged
	// since they have no fractional part.

	const float fractional_limit = 8388608.0F; // 2^23

	// Build our mask, larger values that have no fractional part, and infinities will be true
	// Smaller values and NaN will be false
	float abs_input = std::abs(input);
	bool is_input_large = abs_input >= fractional_limit;

	// Test if our input is NaN with (value != value), it is only true for NaN
	bool is_nan = input != input;

	// Combine our masks to determine if we should return the original value
	bool use_original_input = is_input_large | is_nan;

	// Convert to an integer and back. This does banker's rounding by default
	float integer_part = static_cast<float>(static_cast<int32_t>(input));

	// Test if the returned value is greater than the original.
	// A negative input will round towards zero and be greater when we need it to be smaller.
	bool is_negative = integer_part > input;

	// Subtract our bias to properly handle negative values
	integer_part = is_negative ? (integer_part - 1.0F) : integer_part;

	return use_original_input ? input : integer_part;
}
#endif

static void bm_scalar_floor_scalar(benchmark::State& state)
{
	float f0 = -123.134f;
	float f1 = 123.134f;
	float f2 = -123.134f;
	float f3 = 123.134f;
	float f4 = -123.134f;
	float f5 = 123.134f;
	float f6 = -123.134f;
	float f7 = 123.134f;

	for (auto _ : state)
	{
		f0 = scalar_floor_scalar(f0);
		f1 = scalar_floor_scalar(f1);
		f2 = scalar_floor_scalar(f2);
		f3 = scalar_floor_scalar(f3);
		f4 = scalar_floor_scalar(f4);
		f5 = scalar_floor_scalar(f5);
		f6 = scalar_floor_scalar(f6);
		f7 = scalar_floor_scalar(f7);
	}

	benchmark::DoNotOptimize(f0);
	benchmark::DoNotOptimize(f1);
	benchmark::DoNotOptimize(f2);
	benchmark::DoNotOptimize(f3);
	benchmark::DoNotOptimize(f4);
	benchmark::DoNotOptimize(f5);
	benchmark::DoNotOptimize(f6);
	benchmark::DoNotOptimize(f7);
}

BENCHMARK(bm_scalar_floor_scalar);

#if defined(RTM_SSE2_INTRINSICS)
static void bm_scalar_floor_sse2(benchmark::State& state)
{
	float f0 = -123.134f;
	float f1 = 123.134f;
	float f2 = -123.134f;
	float f3 = 123.134f;
	float f4 = -123.134f;
	float f5 = 123.134f;
	float f6 = -123.134f;
	float f7 = 123.134f;

	for (auto _ : state)
	{
		f0 = scalar_floor_sse2(f0);
		f1 = scalar_floor_sse2(f1);
		f2 = scalar_floor_sse2(f2);
		f3 = scalar_floor_sse2(f3);
		f4 = scalar_floor_sse2(f4);
		f5 = scalar_floor_sse2(f5);
		f6 = scalar_floor_sse2(f6);
		f7 = scalar_floor_sse2(f7);
	}

	benchmark::DoNotOptimize(f0);
	benchmark::DoNotOptimize(f1);
	benchmark::DoNotOptimize(f2);
	benchmark::DoNotOptimize(f3);
	benchmark::DoNotOptimize(f4);
	benchmark::DoNotOptimize(f5);
	benchmark::DoNotOptimize(f6);
	benchmark::DoNotOptimize(f7);
}

BENCHMARK(bm_scalar_floor_sse2);
#endif

#if defined(RTM_NEON_INTRINSICS)
static void bm_scalar_floor_neon(benchmark::State& state)
{
	float f0 = -123.134f;
	float f1 = 123.134f;
	float f2 = -123.134f;
	float f3 = 123.134f;
	float f4 = -123.134f;
	float f5 = 123.134f;
	float f6 = -123.134f;
	float f7 = 123.134f;

	for (auto _ : state)
	{
		f0 = scalar_floor_neon(f0);
		f1 = scalar_floor_neon(f1);
		f2 = scalar_floor_neon(f2);
		f3 = scalar_floor_neon(f3);
		f4 = scalar_floor_neon(f4);
		f5 = scalar_floor_neon(f5);
		f6 = scalar_floor_neon(f6);
		f7 = scalar_floor_neon(f7);
	}

	benchmark::DoNotOptimize(f0);
	benchmark::DoNotOptimize(f1);
	benchmark::DoNotOptimize(f2);
	benchmark::DoNotOptimize(f3);
	benchmark::DoNotOptimize(f4);
	benchmark::DoNotOptimize(f5);
	benchmark::DoNotOptimize(f6);
	benchmark::DoNotOptimize(f7);
}

BENCHMARK(bm_scalar_floor_neon);
#endif

```

`tools/bench/sources/bench_scalar_reciprocal.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <benchmark/benchmark.h>

#include <rtm/scalarf.h>

using namespace rtm;

RTM_FORCE_NOINLINE float RTM_SIMD_CALL scalar_reciprocal_scalar(float input) RTM_NO_EXCEPT
{
	return 1.0f / input;
}

#if defined(RTM_SSE2_INTRINSICS)
RTM_FORCE_NOINLINE float RTM_SIMD_CALL scalar_reciprocal_sse2(float input) RTM_NO_EXCEPT
{
	__m128 input_s = _mm_set_ps1(input);

	// Perform two passes of Newton-Raphson iteration on the hardware estimate
	__m128 x0 = _mm_rcp_ss(input_s);

	// First iteration
	__m128 x1 = _mm_sub_ss(_mm_add_ss(x0, x0), _mm_mul_ss(input_s, _mm_mul_ss(x0, x0)));

	// Second iteration
	__m128 x2 = _mm_sub_ss(_mm_add_ss(x1, x1), _mm_mul_ss(input_s, _mm_mul_ss(x1, x1)));
	return _mm_cvtss_f32(x2);
}
#endif

#if defined(RTM_NEON64_INTRINSICS)
RTM_FORCE_NOINLINE float RTM_SIMD_CALL scalar_reciprocal_neon64(float input) RTM_NO_EXCEPT
{
	// Perform two passes of Newton-Raphson iteration on the hardware estimate
	float x0 = vrecpes_f32(input);

	// First iteration
	float x1 = x0 * vrecpss_f32(x0, input);

	// Second iteration
	float x2 = x1 * vrecpss_f32(x1, input);
	return x2;
}
#endif

#if defined(RTM_NEON_INTRINSICS)
RTM_FORCE_NOINLINE float RTM_SIMD_CALL scalar_reciprocal_neon(float input) RTM_NO_EXCEPT
{
	float32x2_t input_v = vdup_n_f32(input);

	// Perform two passes of Newton-Raphson iteration on the hardware estimate
	float32x2_t x0 = vrecpe_f32(input_v);

	// First iteration
	float32x2_t x1 = vmul_f32(x0, vrecps_f32(x0, input_v));

	// Second iteration
	float32x2_t x2 = vmul_f32(x1, vrecps_f32(x1, input_v));
	return vget_lane_f32(x2, 0);
}
#endif

static void bm_scalar_reciprocal_scalar(benchmark::State& state)
{
	float f0 = -123.134f;
	float f1 = 123.134f;
	float f2 = -123.134f;
	float f3 = 123.134f;
	float f4 = -123.134f;
	float f5 = 123.134f;
	float f6 = -123.134f;
	float f7 = 123.134f;

	for (auto _ : state)
	{
		f0 = scalar_reciprocal_scalar(f0);
		f1 = scalar_reciprocal_scalar(f1);
		f2 = scalar_reciprocal_scalar(f2);
		f3 = scalar_reciprocal_scalar(f3);
		f4 = scalar_reciprocal_scalar(f4);
		f5 = scalar_reciprocal_scalar(f5);
		f6 = scalar_reciprocal_scalar(f6);
		f7 = scalar_reciprocal_scalar(f7);
	}

	benchmark::DoNotOptimize(f0);
	benchmark::DoNotOptimize(f1);
	benchmark::DoNotOptimize(f2);
	benchmark::DoNotOptimize(f3);
	benchmark::DoNotOptimize(f4);
	benchmark::DoNotOptimize(f5);
	benchmark::DoNotOptimize(f6);
	benchmark::DoNotOptimize(f7);
}

BENCHMARK(bm_scalar_reciprocal_scalar);

#if defined(RTM_SSE2_INTRINSICS)
static void bm_scalar_reciprocal_sse2(benchmark::State& state)
{
	float f0 = -123.134f;
	float f1 = 123.134f;
	float f2 = -123.134f;
	float f3 = 123.134f;
	float f4 = -123.134f;
	float f5 = 123.134f;
	float f6 = -123.134f;
	float f7 = 123.134f;

	for (auto _ : state)
	{
		f0 = scalar_reciprocal_sse2(f0);
		f1 = scalar_reciprocal_sse2(f1);
		f2 = scalar_reciprocal_sse2(f2);
		f3 = scalar_reciprocal_sse2(f3);
		f4 = scalar_reciprocal_sse2(f4);
		f5 = scalar_reciprocal_sse2(f5);
		f6 = scalar_reciprocal_sse2(f6);
		f7 = scalar_reciprocal_sse2(f7);
	}

	benchmark::DoNotOptimize(f0);
	benchmark::DoNotOptimize(f1);
	benchmark::DoNotOptimize(f2);
	benchmark::DoNotOptimize(f3);
	benchmark::DoNotOptimize(f4);
	benchmark::DoNotOptimize(f5);
	benchmark::DoNotOptimize(f6);
	benchmark::DoNotOptimize(f7);
}

BENCHMARK(bm_scalar_reciprocal_sse2);
#endif

#if defined(RTM_NEON64_INTRINSICS)
static void bm_scalar_reciprocal_neon64(benchmark::State& state)
{
	float f0 = -123.134f;
	float f1 = 123.134f;
	float f2 = -123.134f;
	float f3 = 123.134f;
	float f4 = -123.134f;
	float f5 = 123.134f;
	float f6 = -123.134f;
	float f7 = 123.134f;

	for (auto _ : state)
	{
		f0 = scalar_reciprocal_neon64(f0);
		f1 = scalar_reciprocal_neon64(f1);
		f2 = scalar_reciprocal_neon64(f2);
		f3 = scalar_reciprocal_neon64(f3);
		f4 = scalar_reciprocal_neon64(f4);
		f5 = scalar_reciprocal_neon64(f5);
		f6 = scalar_reciprocal_neon64(f6);
		f7 = scalar_reciprocal_neon64(f7);
	}

	benchmark::DoNotOptimize(f0);
	benchmark::DoNotOptimize(f1);
	benchmark::DoNotOptimize(f2);
	benchmark::DoNotOptimize(f3);
	benchmark::DoNotOptimize(f4);
	benchmark::DoNotOptimize(f5);
	benchmark::DoNotOptimize(f6);
	benchmark::DoNotOptimize(f7);
}

BENCHMARK(bm_scalar_reciprocal_neon64);
#endif

#if defined(RTM_NEON_INTRINSICS)
static void bm_scalar_reciprocal_neon(benchmark::State& state)
{
	float f0 = -123.134f;
	float f1 = 123.134f;
	float f2 = -123.134f;
	float f3 = 123.134f;
	float f4 = -123.134f;
	float f5 = 123.134f;
	float f6 = -123.134f;
	float f7 = 123.134f;

	for (auto _ : state)
	{
		f0 = scalar_reciprocal_neon(f0);
		f1 = scalar_reciprocal_neon(f1);
		f2 = scalar_reciprocal_neon(f2);
		f3 = scalar_reciprocal_neon(f3);
		f4 = scalar_reciprocal_neon(f4);
		f5 = scalar_reciprocal_neon(f5);
		f6 = scalar_reciprocal_neon(f6);
		f7 = scalar_reciprocal_neon(f7);
	}

	benchmark::DoNotOptimize(f0);
	benchmark::DoNotOptimize(f1);
	benchmark::DoNotOptimize(f2);
	benchmark::DoNotOptimize(f3);
	benchmark::DoNotOptimize(f4);
	benchmark::DoNotOptimize(f5);
	benchmark::DoNotOptimize(f6);
	benchmark::DoNotOptimize(f7);
}

BENCHMARK(bm_scalar_reciprocal_neon);
#endif

```

`tools/bench/sources/bench_scalar_round_bankers.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <benchmark/benchmark.h>

#include <rtm/scalarf.h>

using namespace rtm;

RTM_FORCE_NOINLINE float RTM_SIMD_CALL scalar_round_bankers_scalar(float input) RTM_NO_EXCEPT
{
	if (!scalar_is_finite(input))
		return input;

	int32_t whole = static_cast<int32_t>(input);
	float whole_f = static_cast<float>(whole);
	float remainder = scalar_abs(input - whole_f);
	if (remainder < 0.5F)
		return whole_f;
	if (remainder > 0.5F)
		return input >= 0.0F ? (whole_f + 1.0F) : (whole_f - 1.0F);

	if ((whole % 2) == 0)
		return whole_f;
	else
		return input >= 0.0F ? (whole_f + 1.0F) : (whole_f - 1.0F);
}

#if defined(RTM_SSE2_INTRINSICS)
RTM_FORCE_NOINLINE float RTM_SIMD_CALL scalar_round_bankers_sse2(float input) RTM_NO_EXCEPT
{
	__m128 input_s = _mm_set_ps1(input);

	const __m128 sign_mask = _mm_set_ps(-0.0F, -0.0F, -0.0F, -0.0F);
	__m128 sign = _mm_and_ps(input_s, sign_mask);

	// We add the largest integer that a 32 bit floating point number can represent and subtract it afterwards.
	// This relies on the fact that if we had a fractional part, the new value cannot be represented accurately
	// and IEEE 754 will perform rounding for us. The default rounding mode is Banker's rounding.
	// This has the effect of removing the fractional part while simultaneously rounding.
	// Use the same sign as the input value to make sure we handle positive and negative values.
	const __m128 fractional_limit = _mm_set_ps1(8388608.0F); // 2^23
	__m128 truncating_offset = _mm_or_ps(sign, fractional_limit);
	__m128 integer_part = _mm_sub_ss(_mm_add_ss(input_s, truncating_offset), truncating_offset);

	// If our input was so large that it had no fractional part, return it unchanged
	// Otherwise return our integer part
	const __m128i abs_mask = _mm_set_epi32(0x7FFFFFFFULL, 0x7FFFFFFFULL, 0x7FFFFFFFULL, 0x7FFFFFFFULL);
	__m128 abs_input = _mm_and_ps(input_s, _mm_castsi128_ps(abs_mask));
	__m128 is_input_large = _mm_cmpge_ss(abs_input, fractional_limit);
	__m128 result = _mm_or_ps(_mm_and_ps(is_input_large, input_s), _mm_andnot_ps(is_input_large, integer_part));
	return _mm_cvtss_f32(result);
}
#endif

#if defined(RTM_NEON64_INTRINSICS) && defined(__ARM_FEATURE_DIRECTED_ROUNDING)
RTM_FORCE_NOINLINE float RTM_SIMD_CALL scalar_round_bankers_neon64(float input) RTM_NO_EXCEPT
{
	return vrndns_f32(input);
}
#endif

static void bm_scalar_round_bankers_scalar(benchmark::State& state)
{
	float f0 = -123.134f;
	float f1 = 123.134f;
	float f2 = -123.134f;
	float f3 = 123.134f;
	float f4 = -123.134f;
	float f5 = 123.134f;
	float f6 = -123.134f;
	float f7 = 123.134f;

	for (auto _ : state)
	{
		f0 = scalar_round_bankers_scalar(f0);
		f1 = scalar_round_bankers_scalar(f1);
		f2 = scalar_round_bankers_scalar(f2);
		f3 = scalar_round_bankers_scalar(f3);
		f4 = scalar_round_bankers_scalar(f4);
		f5 = scalar_round_bankers_scalar(f5);
		f6 = scalar_round_bankers_scalar(f6);
		f7 = scalar_round_bankers_scalar(f7);
	}

	benchmark::DoNotOptimize(f0);
	benchmark::DoNotOptimize(f1);
	benchmark::DoNotOptimize(f2);
	benchmark::DoNotOptimize(f3);
	benchmark::DoNotOptimize(f4);
	benchmark::DoNotOptimize(f5);
	benchmark::DoNotOptimize(f6);
	benchmark::DoNotOptimize(f7);
}

BENCHMARK(bm_scalar_round_bankers_scalar);

#if defined(RTM_SSE2_INTRINSICS)
static void bm_scalar_round_bankers_sse2(benchmark::State& state)
{
	float f0 = -123.134f;
	float f1 = 123.134f;
	float f2 = -123.134f;
	float f3 = 123.134f;
	float f4 = -123.134f;
	float f5 = 123.134f;
	float f6 = -123.134f;
	float f7 = 123.134f;

	for (auto _ : state)
	{
		f0 = scalar_round_bankers_sse2(f0);
		f1 = scalar_round_bankers_sse2(f1);
		f2 = scalar_round_bankers_sse2(f2);
		f3 = scalar_round_bankers_sse2(f3);
		f4 = scalar_round_bankers_sse2(f4);
		f5 = scalar_round_bankers_sse2(f5);
		f6 = scalar_round_bankers_sse2(f6);
		f7 = scalar_round_bankers_sse2(f7);
	}

	benchmark::DoNotOptimize(f0);
	benchmark::DoNotOptimize(f1);
	benchmark::DoNotOptimize(f2);
	benchmark::DoNotOptimize(f3);
	benchmark::DoNotOptimize(f4);
	benchmark::DoNotOptimize(f5);
	benchmark::DoNotOptimize(f6);
	benchmark::DoNotOptimize(f7);
}

BENCHMARK(bm_scalar_round_bankers_sse2);
#endif

#if defined(RTM_NEON64_INTRINSICS) && defined(__ARM_FEATURE_DIRECTED_ROUNDING)
static void bm_scalar_round_bankers_neon64(benchmark::State& state)
{
	float f0 = -123.134f;
	float f1 = 123.134f;
	float f2 = -123.134f;
	float f3 = 123.134f;
	float f4 = -123.134f;
	float f5 = 123.134f;
	float f6 = -123.134f;
	float f7 = 123.134f;

	for (auto _ : state)
	{
		f0 = scalar_round_bankers_neon64(f0);
		f1 = scalar_round_bankers_neon64(f1);
		f2 = scalar_round_bankers_neon64(f2);
		f3 = scalar_round_bankers_neon64(f3);
		f4 = scalar_round_bankers_neon64(f4);
		f5 = scalar_round_bankers_neon64(f5);
		f6 = scalar_round_bankers_neon64(f6);
		f7 = scalar_round_bankers_neon64(f7);
	}

	benchmark::DoNotOptimize(f0);
	benchmark::DoNotOptimize(f1);
	benchmark::DoNotOptimize(f2);
	benchmark::DoNotOptimize(f3);
	benchmark::DoNotOptimize(f4);
	benchmark::DoNotOptimize(f5);
	benchmark::DoNotOptimize(f6);
	benchmark::DoNotOptimize(f7);
}

BENCHMARK(bm_scalar_round_bankers_neon64);
#endif

```

`tools/bench/sources/bench_scalar_round_symmetric.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <benchmark/benchmark.h>

#include <rtm/scalarf.h>

using namespace rtm;

RTM_FORCE_NOINLINE float RTM_SIMD_CALL scalar_round_symmetric_scalar(float input) RTM_NO_EXCEPT
{
	return input >= 0.0F ? scalar_floor(input + 0.5F) : scalar_ceil(input - 0.5F);
}

#if defined(RTM_SSE2_INTRINSICS)
RTM_FORCE_NOINLINE float RTM_SIMD_CALL scalar_round_symmetric_sse2(float input) RTM_NO_EXCEPT
{
	__m128 input_s = _mm_set_ps1(input);

	const __m128i abs_mask = _mm_set_epi32(0x7FFFFFFFULL, 0x7FFFFFFFULL, 0x7FFFFFFFULL, 0x7FFFFFFFULL);
	const __m128 fractional_limit = _mm_set_ps1(8388608.0F); // 2^23

	// Build our mask, larger values that have no fractional part, and infinities will be true
	// Smaller values and NaN will be false
	__m128 abs_input = _mm_and_ps(input_s, _mm_castsi128_ps(abs_mask));
	__m128 is_input_large = _mm_cmpge_ss(abs_input, fractional_limit);

	// Test if our input is NaN with (value != value), it is only true for NaN
	__m128 is_nan = _mm_cmpneq_ss(input_s, input_s);

	// Combine our masks to determine if we should return the original value
	__m128 use_original_input = _mm_or_ps(is_input_large, is_nan);

	const __m128 sign_mask = _mm_set_ps(-0.0F, -0.0F, -0.0F, -0.0F);
	__m128 sign = _mm_and_ps(input_s, sign_mask);

	// For positive values, we add a bias of 0.5.
	// For negative values, we add a bias of -0.5.
	__m128 bias = _mm_or_ps(sign, _mm_set_ps1(0.5F));
	__m128 biased_input = _mm_add_ss(input_s, bias);

	// Convert to an integer with truncation and back, this rounds towards zero.
	__m128 integer_part = _mm_cvtepi32_ps(_mm_cvttps_epi32(biased_input));

	__m128 result = _mm_or_ps(_mm_and_ps(use_original_input, input_s), _mm_andnot_ps(use_original_input, integer_part));
	return _mm_cvtss_f32(result);
}
#endif

RTM_FORCE_NOINLINE float RTM_SIMD_CALL scalar_round_symmetric_2(float input) RTM_NO_EXCEPT
{
	// NaN, +- Infinity, and numbers larger or equal to 2^23 remain unchanged
	// since they have no fractional part.

	const float fractional_limit = 8388608.0F; // 2^23

	// Build our mask, larger values that have no fractional part, and infinities will be true
	// Smaller values and NaN will be false
	float abs_input = std::abs(input);
	bool is_input_large = abs_input >= fractional_limit;

	// Test if our input is NaN with (value != value), it is only true for NaN
	bool is_nan = input != input;

	// Combine our masks to determine if we should return the original value
	bool use_original_input = is_input_large | is_nan;

	// For positive values, we add a bias of 0.5.
	// For negative values, we add a bias of -0.5.
	float bias = input >= 0.0F ? 0.5F : -0.5F;
	float biased_input = input + bias;

	// Convert to an integer with truncation and back, this rounds towards zero.
	float integer_part = static_cast<float>(static_cast<int32_t>(biased_input));

	return use_original_input ? input : integer_part;
}

static void bm_scalar_round_symmetric_scalar(benchmark::State& state)
{
	float f0 = -123.134f;
	float f1 = 123.134f;
	float f2 = -123.134f;
	float f3 = 123.134f;
	float f4 = -123.134f;
	float f5 = 123.134f;
	float f6 = -123.134f;
	float f7 = 123.134f;

	for (auto _ : state)
	{
		f0 = scalar_round_symmetric_scalar(f0);
		f1 = scalar_round_symmetric_scalar(f1);
		f2 = scalar_round_symmetric_scalar(f2);
		f3 = scalar_round_symmetric_scalar(f3);
		f4 = scalar_round_symmetric_scalar(f4);
		f5 = scalar_round_symmetric_scalar(f5);
		f6 = scalar_round_symmetric_scalar(f6);
		f7 = scalar_round_symmetric_scalar(f7);
	}

	benchmark::DoNotOptimize(f0);
	benchmark::DoNotOptimize(f1);
	benchmark::DoNotOptimize(f2);
	benchmark::DoNotOptimize(f3);
	benchmark::DoNotOptimize(f4);
	benchmark::DoNotOptimize(f5);
	benchmark::DoNotOptimize(f6);
	benchmark::DoNotOptimize(f7);
}

BENCHMARK(bm_scalar_round_symmetric_scalar);

#if defined(RTM_SSE2_INTRINSICS)
static void bm_scalar_round_symmetric_sse2(benchmark::State& state)
{
	float f0 = -123.134f;
	float f1 = 123.134f;
	float f2 = -123.134f;
	float f3 = 123.134f;
	float f4 = -123.134f;
	float f5 = 123.134f;
	float f6 = -123.134f;
	float f7 = 123.134f;

	for (auto _ : state)
	{
		f0 = scalar_round_symmetric_sse2(f0);
		f1 = scalar_round_symmetric_sse2(f1);
		f2 = scalar_round_symmetric_sse2(f2);
		f3 = scalar_round_symmetric_sse2(f3);
		f4 = scalar_round_symmetric_sse2(f4);
		f5 = scalar_round_symmetric_sse2(f5);
		f6 = scalar_round_symmetric_sse2(f6);
		f7 = scalar_round_symmetric_sse2(f7);
	}

	benchmark::DoNotOptimize(f0);
	benchmark::DoNotOptimize(f1);
	benchmark::DoNotOptimize(f2);
	benchmark::DoNotOptimize(f3);
	benchmark::DoNotOptimize(f4);
	benchmark::DoNotOptimize(f5);
	benchmark::DoNotOptimize(f6);
	benchmark::DoNotOptimize(f7);
}

BENCHMARK(bm_scalar_round_symmetric_sse2);
#endif

static void bm_scalar_round_symmetric_2(benchmark::State& state)
{
	float f0 = -123.134f;
	float f1 = 123.134f;
	float f2 = -123.134f;
	float f3 = 123.134f;
	float f4 = -123.134f;
	float f5 = 123.134f;
	float f6 = -123.134f;
	float f7 = 123.134f;

	for (auto _ : state)
	{
		f0 = scalar_round_symmetric_2(f0);
		f1 = scalar_round_symmetric_2(f1);
		f2 = scalar_round_symmetric_2(f2);
		f3 = scalar_round_symmetric_2(f3);
		f4 = scalar_round_symmetric_2(f4);
		f5 = scalar_round_symmetric_2(f5);
		f6 = scalar_round_symmetric_2(f6);
		f7 = scalar_round_symmetric_2(f7);
	}

	benchmark::DoNotOptimize(f0);
	benchmark::DoNotOptimize(f1);
	benchmark::DoNotOptimize(f2);
	benchmark::DoNotOptimize(f3);
	benchmark::DoNotOptimize(f4);
	benchmark::DoNotOptimize(f5);
	benchmark::DoNotOptimize(f6);
	benchmark::DoNotOptimize(f7);
}

BENCHMARK(bm_scalar_round_symmetric_2);

```

`tools/bench/sources/bench_scalar_sin.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <benchmark/benchmark.h>

#include <rtm/scalarf.h>

using namespace rtm;

RTM_FORCE_NOINLINE float RTM_SIMD_CALL scalar_sin_scalar(float input) RTM_NO_EXCEPT
{
	// Use a degree 11 minimax approximation polynomial
	// See: GPGPU Programming for Games and Science (David H. Eberly)

	// Remap our input in the [-pi, pi] range
	float quotient = input * rtm::constants::one_div_two_pi();
	quotient = scalar_round_bankers(quotient);
	quotient = quotient * rtm::constants::two_pi();
	float x = input - quotient;

	// Remap our input in the [-pi/2, pi/2] range
	const float reference = rtm_impl::copysign(rtm::constants::pi(), x);
	const float reflection = reference - x;
	const float x_abs = scalar_abs(x);
	x = x_abs <= rtm::constants::half_pi() ? x : reflection;

	// Calculate our value
	const float x2 = x * x;
	float result = (x2 * -2.3828544692960918e-8F) + 2.7521557770526783e-6F;
	result = (result * x2) - 1.9840782426250314e-4F;
	result = (result * x2) + 8.3333303183525942e-3F;
	result = (result * x2) - 1.6666666601721269e-1F;
	result = (result * x2) + 1.0F;
	result = result * x;
	return result;
}

RTM_FORCE_NOINLINE float RTM_SIMD_CALL scalar_sin_std(float input) RTM_NO_EXCEPT
{
	return std::sin(input);
}

#if defined(RTM_SSE2_INTRINSICS)
RTM_FORCE_NOINLINE float RTM_SIMD_CALL scalar_sin_sse2(float input) RTM_NO_EXCEPT
{
	__m128 input_s = _mm_set_ps1(input);

	// Use a degree 11 minimax approximation polynomial
	// See: GPGPU Programming for Games and Science (David H. Eberly)

	// Remap our input in the [-pi, pi] range
	__m128 quotient = _mm_mul_ss(input_s, _mm_set_ps1(rtm::constants::one_div_two_pi()));
	quotient = scalar_round_bankers(scalarf{ quotient }).value;
	quotient = _mm_mul_ss(quotient, _mm_set_ps1(rtm::constants::two_pi()));
	__m128 x = _mm_sub_ss(input_s, quotient);

	// Remap our input in the [-pi/2, pi/2] range
	const __m128 sign_mask = _mm_set_ps(-0.0F, -0.0F, -0.0F, -0.0F);
	__m128 sign = _mm_and_ps(x, sign_mask);
	__m128 reference = _mm_or_ps(sign, _mm_set_ps1(rtm::constants::pi()));

	const __m128 reflection = _mm_sub_ss(reference, x);
	const __m128i abs_mask = _mm_set_epi32(0x7FFFFFFFULL, 0x7FFFFFFFULL, 0x7FFFFFFFULL, 0x7FFFFFFFULL);
	const __m128 x_abs = _mm_and_ps(x, _mm_castsi128_ps(abs_mask));

	__m128 is_less_equal_than_half_pi = _mm_cmple_ss(x_abs, _mm_set_ps1(rtm::constants::half_pi()));

	x = RTM_VECTOR4F_SELECT(is_less_equal_than_half_pi, x, reflection);

	// Calculate our value
	const float x2 = _mm_cvtss_f32(_mm_mul_ss(x, x));
	float result = (x2 * -2.3828544692960918e-8F) + 2.7521557770526783e-6F;
	result = (result * x2) - 1.9840782426250314e-4F;
	result = (result * x2) + 8.3333303183525942e-3F;
	result = (result * x2) - 1.6666666601721269e-1F;
	result = (result * x2) + 1.0F;
	result = result * _mm_cvtss_f32(x);
	return result;
}
#endif

#if defined(RTM_NEON64_INTRINSICS)
alignas(64) static constexpr float constants3_[12] =
{
	2.7521557770526783e-6F, rtm::constants::one_div_two_pi(),
	-2.3828544692960918e-8F, rtm::constants::two_pi(),
	-1.9840782426250314e-4F, rtm::constants::pi(),
	8.3333303183525942e-3F, -rtm::constants::pi(),

	-1.6666666601721269e-1F, rtm::constants::half_pi(),
	1.0F, 1.0F,
};

RTM_FORCE_NOINLINE float RTM_SIMD_CALL scalar_sin_neon64(float input) RTM_NO_EXCEPT
{
	// Use a degree 11 minimax approximation polynomial
	// See: GPGPU Programming for Games and Science (David H. Eberly)

	const float32x2x4_t constants0 = vld4_f32(&constants3_[0]);

	float32x2_t input_v = vdup_n_f32(input);
	const float32x2x2_t constants1 = vld2_f32(&constants3_[8]);

	// Remap our input in the [-pi, pi] range
	float32x2_t quotient = vmul_n_f32(constants0.val[0], input);	// [y] SIMD lane
	bool is_x_positive = input >= 0.0F;
	quotient = vrndn_f32(quotient);
	float32x2_t reference_v = is_x_positive ? constants0.val[2] : constants0.val[3];	// [y] SIMD lane

#if defined(RTM_NEON64_INTRINSICS)
	float32x2_t x_v = vfms_lane_f32(input_v, quotient, constants0.val[1], 1);	// [y] SIMD lane
#else
	float32x2_t x_v = vmls_lane_f32(input_v, quotient, constants0.val[1], 1);	// [y] SIMD lane
#endif

	float32x2_t reflection_v = vsub_f32(reference_v, x_v);

	// Remap our input in the [-pi/2, pi/2] range
	uint32x2_t is_less_equal_than_half_pi = vcale_f32(x_v, constants1.val[0]);
	x_v = vbsl_f32(is_less_equal_than_half_pi, x_v, reflection_v);	// [y] SIMD lane

	// Calculate our value, we only care about the [x] SIMD lane
	float32x2_t x2 = vmul_lane_f32(x_v, x_v, 1);	// [y] SIMD lane

#if defined(RTM_NEON64_INTRINSICS)
	float32x2_t result = vfma_lane_f32(constants0.val[0], constants0.val[1], x2, 1);
	result = vfma_lane_f32(constants0.val[2], result, x2, 1);
	result = vfma_lane_f32(constants0.val[3], result, x2, 1);
	result = vfma_lane_f32(constants1.val[0], result, x2, 1);
	result = vfma_lane_f32(constants1.val[1], result, x2, 1);
#else
	float32x2_t result = vmla_lane_f32(constants0.val[0], constants0.val[1], x2, 1);
	result = vmla_lane_f32(constants0.val[2], result, x2, 1);
	result = vmla_lane_f32(constants0.val[3], result, x2, 1);
	result = vmla_lane_f32(constants1.val[0], result, x2, 1);
	result = vmla_lane_f32(constants1.val[1], result, x2, 1);
#endif

	result = vmul_lane_f32(result, x_v, 1);
	return vget_lane_f32(result, 0);
}
#endif

static void bm_scalar_sin_scalar(benchmark::State& state)
{
	float f0 = -123.134f;
	float f1 = 123.134f;
	float f2 = -123.134f;
	float f3 = 123.134f;
	float f4 = -123.134f;
	float f5 = 123.134f;
	float f6 = -123.134f;
	float f7 = 123.134f;

	for (auto _ : state)
	{
		f0 = scalar_sin_scalar(f0) * 100.0F;
		f1 = scalar_sin_scalar(f1) * 100.0F;
		f2 = scalar_sin_scalar(f2) * 100.0F;
		f3 = scalar_sin_scalar(f3) * 100.0F;
		f4 = scalar_sin_scalar(f4) * 100.0F;
		f5 = scalar_sin_scalar(f5) * 100.0F;
		f6 = scalar_sin_scalar(f6) * 100.0F;
		f7 = scalar_sin_scalar(f7) * 100.0F;
	}

	benchmark::DoNotOptimize(f0);
	benchmark::DoNotOptimize(f1);
	benchmark::DoNotOptimize(f2);
	benchmark::DoNotOptimize(f3);
	benchmark::DoNotOptimize(f4);
	benchmark::DoNotOptimize(f5);
	benchmark::DoNotOptimize(f6);
	benchmark::DoNotOptimize(f7);
}

BENCHMARK(bm_scalar_sin_scalar);

static void bm_scalar_sin_std(benchmark::State& state)
{
	float f0 = -123.134f;
	float f1 = 123.134f;
	float f2 = -123.134f;
	float f3 = 123.134f;
	float f4 = -123.134f;
	float f5 = 123.134f;
	float f6 = -123.134f;
	float f7 = 123.134f;

	for (auto _ : state)
	{
		f0 = scalar_sin_std(f0) * 100.0F;
		f1 = scalar_sin_std(f1) * 100.0F;
		f2 = scalar_sin_std(f2) * 100.0F;
		f3 = scalar_sin_std(f3) * 100.0F;
		f4 = scalar_sin_std(f4) * 100.0F;
		f5 = scalar_sin_std(f5) * 100.0F;
		f6 = scalar_sin_std(f6) * 100.0F;
		f7 = scalar_sin_std(f7) * 100.0F;
	}

	benchmark::DoNotOptimize(f0);
	benchmark::DoNotOptimize(f1);
	benchmark::DoNotOptimize(f2);
	benchmark::DoNotOptimize(f3);
	benchmark::DoNotOptimize(f4);
	benchmark::DoNotOptimize(f5);
	benchmark::DoNotOptimize(f6);
	benchmark::DoNotOptimize(f7);
}

BENCHMARK(bm_scalar_sin_std);

#if defined(RTM_SSE2_INTRINSICS)
static void bm_scalar_sin_sse2(benchmark::State& state)
{
	float f0 = -123.134f;
	float f1 = 123.134f;
	float f2 = -123.134f;
	float f3 = 123.134f;
	float f4 = -123.134f;
	float f5 = 123.134f;
	float f6 = -123.134f;
	float f7 = 123.134f;

	for (auto _ : state)
	{
		f0 = scalar_sin_sse2(f0) * 100.0F;
		f1 = scalar_sin_sse2(f1) * 100.0F;
		f2 = scalar_sin_sse2(f2) * 100.0F;
		f3 = scalar_sin_sse2(f3) * 100.0F;
		f4 = scalar_sin_sse2(f4) * 100.0F;
		f5 = scalar_sin_sse2(f5) * 100.0F;
		f6 = scalar_sin_sse2(f6) * 100.0F;
		f7 = scalar_sin_sse2(f7) * 100.0F;
	}

	benchmark::DoNotOptimize(f0);
	benchmark::DoNotOptimize(f1);
	benchmark::DoNotOptimize(f2);
	benchmark::DoNotOptimize(f3);
	benchmark::DoNotOptimize(f4);
	benchmark::DoNotOptimize(f5);
	benchmark::DoNotOptimize(f6);
	benchmark::DoNotOptimize(f7);
}

BENCHMARK(bm_scalar_sin_sse2);
#endif

#if defined(RTM_NEON64_INTRINSICS)
static void bm_scalar_sin_neon64(benchmark::State& state)
{
	float f0 = -123.134f;
	float f1 = 123.134f;
	float f2 = -123.134f;
	float f3 = 123.134f;
	float f4 = -123.134f;
	float f5 = 123.134f;
	float f6 = -123.134f;
	float f7 = 123.134f;

	for (auto _ : state)
	{
		f0 = scalar_sin_neon64(f0) * 100.0F;
		f1 = scalar_sin_neon64(f1) * 100.0F;
		f2 = scalar_sin_neon64(f2) * 100.0F;
		f3 = scalar_sin_neon64(f3) * 100.0F;
		f4 = scalar_sin_neon64(f4) * 100.0F;
		f5 = scalar_sin_neon64(f5) * 100.0F;
		f6 = scalar_sin_neon64(f6) * 100.0F;
		f7 = scalar_sin_neon64(f7) * 100.0F;
	}

	benchmark::DoNotOptimize(f0);
	benchmark::DoNotOptimize(f1);
	benchmark::DoNotOptimize(f2);
	benchmark::DoNotOptimize(f3);
	benchmark::DoNotOptimize(f4);
	benchmark::DoNotOptimize(f5);
	benchmark::DoNotOptimize(f6);
	benchmark::DoNotOptimize(f7);
}

BENCHMARK(bm_scalar_sin_neon64);
#endif

```

`tools/bench/sources/bench_vector_abs.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2019 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <benchmark/benchmark.h>

#include <rtm/vector4f.h>

using namespace rtm;

RTM_FORCE_NOINLINE vector4f RTM_SIMD_CALL vector_abs_scalar(vector4f_arg0 input) RTM_NO_EXCEPT
{
	scalarf x = vector_get_x_as_scalar(input);
	scalarf y = vector_get_y_as_scalar(input);
	scalarf z = vector_get_z_as_scalar(input);
	scalarf w = vector_get_w_as_scalar(input);
	return vector_set(scalar_abs(x), scalar_abs(y), scalar_abs(z), scalar_abs(w));
}

#if defined(RTM_SSE2_INTRINSICS)
RTM_FORCE_NOINLINE vector4f RTM_SIMD_CALL vector_abs_sse2_maxsub(vector4f_arg0 input) RTM_NO_EXCEPT
{
	return vector_max(vector_sub(_mm_setzero_ps(), input), input);
}

// Wins on Haswell laptop x64 AVX
// Wins on Ryzen 2990X desktop VS2017 x64 AVX
// Wins on Ryzen 2990X desktop clang9 x64 AVX
RTM_FORCE_NOINLINE vector4f RTM_SIMD_CALL vector_abs_sse2_and(vector4f_arg0 input) RTM_NO_EXCEPT
{
	const __m128i abs_mask = _mm_set_epi32(0x7FFFFFFFULL, 0x7FFFFFFFULL, 0x7FFFFFFFULL, 0x7FFFFFFFULL);
	return _mm_and_ps(input, _mm_castsi128_ps(abs_mask));
}
#endif

static void bm_vector_abs_scalar(benchmark::State& state)
{
	vector4f v0 = vector_set(-1.0f, 1.0f, -2.0f, -123.134f);
	vector4f v1 = vector_set(-1.0f, 1.0f, -2.0f, -123.134f);
	vector4f v2 = vector_set(-1.0f, 1.0f, -2.0f, -123.134f);
	vector4f v3 = vector_set(-1.0f, 1.0f, -2.0f, -123.134f);
	vector4f v4 = vector_set(-1.0f, 1.0f, -2.0f, -123.134f);
	vector4f v5 = vector_set(-1.0f, 1.0f, -2.0f, -123.134f);
	vector4f v6 = vector_set(-1.0f, 1.0f, -2.0f, -123.134f);
	vector4f v7 = vector_set(-1.0f, 1.0f, -2.0f, -123.134f);

	for (auto _ : state)
	{
		v0 = vector_abs_scalar(v0);
		v1 = vector_abs_scalar(v1);
		v2 = vector_abs_scalar(v2);
		v3 = vector_abs_scalar(v3);
		v4 = vector_abs_scalar(v4);
		v5 = vector_abs_scalar(v5);
		v6 = vector_abs_scalar(v6);
		v7 = vector_abs_scalar(v7);
	}

	benchmark::DoNotOptimize(v0);
	benchmark::DoNotOptimize(v1);
	benchmark::DoNotOptimize(v2);
	benchmark::DoNotOptimize(v3);
	benchmark::DoNotOptimize(v4);
	benchmark::DoNotOptimize(v5);
	benchmark::DoNotOptimize(v6);
	benchmark::DoNotOptimize(v7);
}

BENCHMARK(bm_vector_abs_scalar);

#if defined(RTM_SSE2_INTRINSICS)
// Wins on Ryzen 2990X desktop VS2017 x64 AVX
static void bm_vector_abs_sse2_maxsub(benchmark::State& state)
{
	vector4f v0 = vector_set(-1.0f, 1.0f, -2.0f, -123.134f);
	vector4f v1 = vector_set(-1.0f, 1.0f, -2.0f, -123.134f);
	vector4f v2 = vector_set(-1.0f, 1.0f, -2.0f, -123.134f);
	vector4f v3 = vector_set(-1.0f, 1.0f, -2.0f, -123.134f);
	vector4f v4 = vector_set(-1.0f, 1.0f, -2.0f, -123.134f);
	vector4f v5 = vector_set(-1.0f, 1.0f, -2.0f, -123.134f);
	vector4f v6 = vector_set(-1.0f, 1.0f, -2.0f, -123.134f);
	vector4f v7 = vector_set(-1.0f, 1.0f, -2.0f, -123.134f);

	for (auto _ : state)
	{
		v0 = vector_abs_sse2_maxsub(v0);
		v1 = vector_abs_sse2_maxsub(v1);
		v2 = vector_abs_sse2_maxsub(v2);
		v3 = vector_abs_sse2_maxsub(v3);
		v4 = vector_abs_sse2_maxsub(v4);
		v5 = vector_abs_sse2_maxsub(v5);
		v6 = vector_abs_sse2_maxsub(v6);
		v7 = vector_abs_sse2_maxsub(v7);
	}

	benchmark::DoNotOptimize(v0);
	benchmark::DoNotOptimize(v1);
	benchmark::DoNotOptimize(v2);
	benchmark::DoNotOptimize(v3);
	benchmark::DoNotOptimize(v4);
	benchmark::DoNotOptimize(v5);
	benchmark::DoNotOptimize(v6);
	benchmark::DoNotOptimize(v7);
}

BENCHMARK(bm_vector_abs_sse2_maxsub);

static void bm_vector_abs_sse2_and(benchmark::State& state)
{
	vector4f v0 = vector_set(-1.0f, 1.0f, -2.0f, -123.134f);
	vector4f v1 = vector_set(-1.0f, 1.0f, -2.0f, -123.134f);
	vector4f v2 = vector_set(-1.0f, 1.0f, -2.0f, -123.134f);
	vector4f v3 = vector_set(-1.0f, 1.0f, -2.0f, -123.134f);
	vector4f v4 = vector_set(-1.0f, 1.0f, -2.0f, -123.134f);
	vector4f v5 = vector_set(-1.0f, 1.0f, -2.0f, -123.134f);
	vector4f v6 = vector_set(-1.0f, 1.0f, -2.0f, -123.134f);
	vector4f v7 = vector_set(-1.0f, 1.0f, -2.0f, -123.134f);

	for (auto _ : state)
	{
		v0 = vector_abs_sse2_and(v0);
		v1 = vector_abs_sse2_and(v1);
		v2 = vector_abs_sse2_and(v2);
		v3 = vector_abs_sse2_and(v3);
		v4 = vector_abs_sse2_and(v4);
		v5 = vector_abs_sse2_and(v5);
		v6 = vector_abs_sse2_and(v6);
		v7 = vector_abs_sse2_and(v7);
	}

	benchmark::DoNotOptimize(v0);
	benchmark::DoNotOptimize(v1);
	benchmark::DoNotOptimize(v2);
	benchmark::DoNotOptimize(v3);
	benchmark::DoNotOptimize(v4);
	benchmark::DoNotOptimize(v5);
	benchmark::DoNotOptimize(v6);
	benchmark::DoNotOptimize(v7);
}

BENCHMARK(bm_vector_abs_sse2_and);
#endif

```

`tools/bench/sources/bench_vector_acos.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <benchmark/benchmark.h>

#include <rtm/vector4f.h>

using namespace rtm;

RTM_FORCE_NOINLINE vector4f RTM_SIMD_CALL vector_acos_scalar(vector4f_arg0 input) RTM_NO_EXCEPT
{
	scalarf x = scalar_acos(vector_get_x_as_scalar(input));
	scalarf y = scalar_acos(vector_get_y_as_scalar(input));
	scalarf z = scalar_acos(vector_get_z_as_scalar(input));
	scalarf w = scalar_acos(vector_get_w_as_scalar(input));
	return vector_set(x, y, z, w);
}

#if defined(RTM_SSE2_INTRINSICS)
RTM_FORCE_NOINLINE vector4f RTM_SIMD_CALL vector_acos_sse2(vector4f_arg0 input) RTM_NO_EXCEPT
{
	// Use the identity: acos(value) + asin(value) = PI/2
	// This ends up being: acos(value) = PI/2 - asin(value)
	// Since asin(value) = PI/2 - sqrt(1.0 - polynomial(value))
	// Our end result is acos(value) = sqrt(1.0 - polynomial(value))
	// This means we can re-use the same polynomial as asin()
	// See: GPGPU Programming for Games and Science (David H. Eberly)

	// We first calculate our scale: sqrt(1.0 - abs(value))
	// Use the sign bit to generate our absolute value since we'll re-use that constant
	const __m128 sign_bit = _mm_set_ps1(-0.0F);
	__m128 abs_value = _mm_andnot_ps(sign_bit, input);

	// Calculate our value
	__m128 result = _mm_add_ps(_mm_mul_ps(abs_value, _mm_set_ps1(-1.2690614339589956e-3F)), _mm_set_ps1(6.7072304676685235e-3F));
	result = _mm_add_ps(_mm_mul_ps(result, abs_value), _mm_set_ps1(-1.7162031184398074e-2F));
	result = _mm_add_ps(_mm_mul_ps(result, abs_value), _mm_set_ps1(3.0961594977611639e-2F));
	result = _mm_add_ps(_mm_mul_ps(result, abs_value), _mm_set_ps1(-5.0207843052845647e-2F));
	result = _mm_add_ps(_mm_mul_ps(result, abs_value), _mm_set_ps1(8.8986946573346160e-2F));
	result = _mm_add_ps(_mm_mul_ps(result, abs_value), _mm_set_ps1(-2.1459960076929829e-1F));
	result = _mm_add_ps(_mm_mul_ps(result, abs_value), _mm_set_ps1(1.5707963267948966F));

	// Scale our result
	__m128 scale = _mm_sqrt_ps(_mm_sub_ps(_mm_set_ps1(1.0F), abs_value));
	result = _mm_mul_ps(result, scale);

	// Normally the math is as follow:
	// If input is positive: result
	// If input is negative: PI - result = -result + PI

	// As such, the offset is 0.0 when the input is positive and PI when negative
	__m128 is_input_negative = _mm_cmplt_ps(input, _mm_setzero_ps());
	__m128 offset = _mm_and_ps(is_input_negative, _mm_set_ps1(rtm::constants::pi()));

	// And our result has the same sign of the input
	__m128 input_sign = _mm_and_ps(input, sign_bit);
	result = _mm_or_ps(result, input_sign);
	return _mm_add_ps(result, offset);
}
#endif

#if defined(RTM_NEON_INTRINSICS)
RTM_FORCE_NOINLINE vector4f RTM_SIMD_CALL vector_acos_neon(vector4f_arg0 input) RTM_NO_EXCEPT
{
	// Use the identity: acos(value) + asin(value) = PI/2
	// This ends up being: acos(value) = PI/2 - asin(value)
	// Since asin(value) = PI/2 - sqrt(1.0 - polynomial(value))
	// Our end result is acos(value) = sqrt(1.0 - polynomial(value))
	// This means we can re-use the same polynomial as asin()
	// See: GPGPU Programming for Games and Science (David H. Eberly)

	// We first calculate our scale: sqrt(1.0 - abs(value))
	// Use the sign bit to generate our absolute value since we'll re-use that constant
	float32x4_t abs_value = vabsq_f32(input);

	// Calculate our value
	float32x4_t result = vmlaq_n_f32(vdupq_n_f32(6.7072304676685235e-3F), abs_value, -1.2690614339589956e-3F);
	result = vmlaq_f32(vdupq_n_f32(-1.7162031184398074e-2F), result, abs_value);
	result = vmlaq_f32(vdupq_n_f32(3.0961594977611639e-2F), result, abs_value);
	result = vmlaq_f32(vdupq_n_f32(-5.0207843052845647e-2F), result, abs_value);
	result = vmlaq_f32(vdupq_n_f32(8.8986946573346160e-2F), result, abs_value);
	result = vmlaq_f32(vdupq_n_f32(-2.1459960076929829e-1F), result, abs_value);
	result = vmlaq_f32(vdupq_n_f32(1.5707963267948966F), result, abs_value);

	// Scale our result
	float32x4_t scale = vector_sqrt(vsubq_f32(vdupq_n_f32(1.0F), abs_value));
	result = vmulq_f32(result, scale);

	// Normally the math is as follow:
	// If input is positive: result
	// If input is negative: PI - result = -result + PI

	// As such, the offset is 0.0 when the input is positive and PI when negative
	uint32x4_t is_input_negative = vcltq_f32(input, vdupq_n_f32(0.0F));
	float32x4_t offset = vreinterpretq_f32_u32(vandq_u32(is_input_negative, vreinterpretq_u32_f32(vdupq_n_f32(rtm::constants::pi()))));

	// And our result has the same sign of the input
	uint32x4_t sign_mask = vreinterpretq_u32_f32(vdupq_n_f32(-0.0F));
	uint32x4_t input_sign = vandq_u32(vreinterpretq_u32_f32(input), sign_mask);

	result = vreinterpretq_f32_u32(vorrq_u32(vreinterpretq_u32_f32(result), input_sign));
	return vaddq_f32(result, offset);
}
#endif

static void bm_vector_acos_scalar(benchmark::State& state)
{
	vector4f v0 = vector_set(-0.134f);
	vector4f v1 = vector_set(0.134f);
	vector4f v2 = vector_set(-0.134f);
	vector4f v3 = vector_set(0.134f);
	vector4f v4 = vector_set(-0.134f);
	vector4f v5 = vector_set(0.134f);
	vector4f v6 = vector_set(-0.134f);
	vector4f v7 = vector_set(0.134f);

	for (auto _ : state)
	{
		v0 = vector_acos_scalar(v0);
		v1 = vector_acos_scalar(v1);
		v2 = vector_acos_scalar(v2);
		v3 = vector_acos_scalar(v3);
		v4 = vector_acos_scalar(v4);
		v5 = vector_acos_scalar(v5);
		v6 = vector_acos_scalar(v6);
		v7 = vector_acos_scalar(v7);
	}

	benchmark::DoNotOptimize(v0);
	benchmark::DoNotOptimize(v1);
	benchmark::DoNotOptimize(v2);
	benchmark::DoNotOptimize(v3);
	benchmark::DoNotOptimize(v4);
	benchmark::DoNotOptimize(v5);
	benchmark::DoNotOptimize(v6);
	benchmark::DoNotOptimize(v7);
}

BENCHMARK(bm_vector_acos_scalar);

#if defined(RTM_SSE2_INTRINSICS)
static void bm_vector_acos_sse2(benchmark::State& state)
{
	vector4f v0 = vector_set(-0.134f);
	vector4f v1 = vector_set(0.134f);
	vector4f v2 = vector_set(-0.134f);
	vector4f v3 = vector_set(0.134f);
	vector4f v4 = vector_set(-0.134f);
	vector4f v5 = vector_set(0.134f);
	vector4f v6 = vector_set(-0.134f);
	vector4f v7 = vector_set(0.134f);

	for (auto _ : state)
	{
		v0 = vector_acos_sse2(v0);
		v1 = vector_acos_sse2(v1);
		v2 = vector_acos_sse2(v2);
		v3 = vector_acos_sse2(v3);
		v4 = vector_acos_sse2(v4);
		v5 = vector_acos_sse2(v5);
		v6 = vector_acos_sse2(v6);
		v7 = vector_acos_sse2(v7);
	}

	benchmark::DoNotOptimize(v0);
	benchmark::DoNotOptimize(v1);
	benchmark::DoNotOptimize(v2);
	benchmark::DoNotOptimize(v3);
	benchmark::DoNotOptimize(v4);
	benchmark::DoNotOptimize(v5);
	benchmark::DoNotOptimize(v6);
	benchmark::DoNotOptimize(v7);
}

BENCHMARK(bm_vector_acos_sse2);
#endif

#if defined(RTM_NEON_INTRINSICS)
static void bm_vector_acos_neon(benchmark::State& state)
{
	vector4f v0 = vector_set(-0.134f);
	vector4f v1 = vector_set(0.134f);
	vector4f v2 = vector_set(-0.134f);
	vector4f v3 = vector_set(0.134f);
	vector4f v4 = vector_set(-0.134f);
	vector4f v5 = vector_set(0.134f);
	vector4f v6 = vector_set(-0.134f);
	vector4f v7 = vector_set(0.134f);

	for (auto _ : state)
	{
		v0 = vector_acos_neon(v0);
		v1 = vector_acos_neon(v1);
		v2 = vector_acos_neon(v2);
		v3 = vector_acos_neon(v3);
		v4 = vector_acos_neon(v4);
		v5 = vector_acos_neon(v5);
		v6 = vector_acos_neon(v6);
		v7 = vector_acos_neon(v7);
	}

	benchmark::DoNotOptimize(v0);
	benchmark::DoNotOptimize(v1);
	benchmark::DoNotOptimize(v2);
	benchmark::DoNotOptimize(v3);
	benchmark::DoNotOptimize(v4);
	benchmark::DoNotOptimize(v5);
	benchmark::DoNotOptimize(v6);
	benchmark::DoNotOptimize(v7);
}

BENCHMARK(bm_vector_acos_neon);
#endif

```

`tools/bench/sources/bench_vector_asin.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <benchmark/benchmark.h>

#include <rtm/vector4f.h>

using namespace rtm;

RTM_FORCE_NOINLINE vector4f RTM_SIMD_CALL vector_asin_scalar(vector4f_arg0 input) RTM_NO_EXCEPT
{
	scalarf x = scalar_asin(vector_get_x_as_scalar(input));
	scalarf y = scalar_asin(vector_get_y_as_scalar(input));
	scalarf z = scalar_asin(vector_get_z_as_scalar(input));
	scalarf w = scalar_asin(vector_get_w_as_scalar(input));
	return vector_set(x, y, z, w);
}

#if defined(RTM_SSE2_INTRINSICS)
RTM_FORCE_NOINLINE vector4f RTM_SIMD_CALL vector_asin_sse2(vector4f_arg0 input) RTM_NO_EXCEPT
{
	// Use a degree 7 minimax approximation polynomial
	// See: GPGPU Programming for Games and Science (David H. Eberly)

	// We first calculate our scale: sqrt(1.0 - abs(value))
	// Use the sign bit to generate our absolute value since we'll re-use that constant
	const __m128 sign_bit = _mm_set_ps1(-0.0F);
	__m128 abs_value = _mm_andnot_ps(sign_bit, input);

	// Calculate our value
	__m128 result = _mm_add_ps(_mm_mul_ps(abs_value, _mm_set_ps1(-1.2690614339589956e-3F)), _mm_set_ps1(6.7072304676685235e-3F));
	result = _mm_add_ps(_mm_mul_ps(result, abs_value), _mm_set_ps1(-1.7162031184398074e-2F));
	result = _mm_add_ps(_mm_mul_ps(result, abs_value), _mm_set_ps1(3.0961594977611639e-2F));
	result = _mm_add_ps(_mm_mul_ps(result, abs_value), _mm_set_ps1(-5.0207843052845647e-2F));
	result = _mm_add_ps(_mm_mul_ps(result, abs_value), _mm_set_ps1(8.8986946573346160e-2F));
	result = _mm_add_ps(_mm_mul_ps(result, abs_value), _mm_set_ps1(-2.1459960076929829e-1F));
	result = _mm_add_ps(_mm_mul_ps(result, abs_value), _mm_set_ps1(1.5707963267948966F));

	// Scale our result
	__m128 scale = _mm_sqrt_ps(_mm_sub_ps(_mm_set_ps1(1.0F), abs_value));
	result = _mm_mul_ps(result, scale);

	// Normally the math is as follow:
	// If input is positive: PI/2 - result
	// If input is negative: PI/2 - (PI - result) = PI/2 - PI + result = -PI/2 + result

	// As such, the offset is PI/2 and it takes the sign of the input
	// This allows us to load a single constant from memory directly
	__m128 input_sign = _mm_and_ps(input, sign_bit);
	__m128 offset = _mm_or_ps(input_sign, _mm_set_ps1(rtm::constants::half_pi()));

	// And our result has the opposite sign of the input
	result = _mm_xor_ps(result, _mm_xor_ps(input_sign, sign_bit));
	return _mm_add_ps(result, offset);
}
#endif

#if defined(RTM_NEON_INTRINSICS)
RTM_FORCE_NOINLINE vector4f RTM_SIMD_CALL vector_asin_neon(vector4f_arg0 input) RTM_NO_EXCEPT
{
	// Use a degree 7 minimax approximation polynomial
	// See: GPGPU Programming for Games and Science (David H. Eberly)

	// We first calculate our scale: sqrt(1.0 - abs(value))
	// Use the sign bit to generate our absolute value since we'll re-use that constant
	float32x4_t abs_value = vabsq_f32(input);

	// Calculate our value
	float32x4_t result = vmlaq_n_f32(vdupq_n_f32(6.7072304676685235e-3F), abs_value, -1.2690614339589956e-3F);
	result = vmlaq_f32(vdupq_n_f32(-1.7162031184398074e-2F), result, abs_value);
	result = vmlaq_f32(vdupq_n_f32(3.0961594977611639e-2F), result, abs_value);
	result = vmlaq_f32(vdupq_n_f32(-5.0207843052845647e-2F), result, abs_value);
	result = vmlaq_f32(vdupq_n_f32(8.8986946573346160e-2F), result, abs_value);
	result = vmlaq_f32(vdupq_n_f32(-2.1459960076929829e-1F), result, abs_value);
	result = vmlaq_f32(vdupq_n_f32(1.5707963267948966F), result, abs_value);

	// Scale our result
	float32x4_t scale = vector_sqrt(vsubq_f32(vdupq_n_f32(1.0F), abs_value));
	result = vmulq_f32(result, scale);

	// Normally the math is as follow:
	// If input is positive: PI/2 - result
	// If input is negative: PI/2 - (PI - result) = PI/2 - PI + result = -PI/2 + result

	// As such, the offset is PI/2 and it takes the sign of the input
	// This allows us to load a single constant from memory directly
	uint32x4_t sign_mask = vreinterpretq_u32_f32(vdupq_n_f32(-0.0F));
	uint32x4_t input_sign = vandq_u32(vreinterpretq_u32_f32(input), sign_mask);
	float32x4_t offset = vreinterpretq_f32_u32(vorrq_u32(input_sign, vreinterpretq_u32_f32(vdupq_n_f32(rtm::constants::half_pi()))));

	// And our result has the opposite sign of the input
	result = vreinterpretq_f32_u32(veorq_u32(vreinterpretq_u32_f32(result), veorq_u32(input_sign, sign_mask)));
	return vaddq_f32(result, offset);
}
#endif

static void bm_vector_asin_scalar(benchmark::State& state)
{
	vector4f v0 = vector_set(-0.134f);
	vector4f v1 = vector_set(0.134f);
	vector4f v2 = vector_set(-0.134f);
	vector4f v3 = vector_set(0.134f);
	vector4f v4 = vector_set(-0.134f);
	vector4f v5 = vector_set(0.134f);
	vector4f v6 = vector_set(-0.134f);
	vector4f v7 = vector_set(0.134f);

	for (auto _ : state)
	{
		v0 = vector_asin_scalar(v0);
		v1 = vector_asin_scalar(v1);
		v2 = vector_asin_scalar(v2);
		v3 = vector_asin_scalar(v3);
		v4 = vector_asin_scalar(v4);
		v5 = vector_asin_scalar(v5);
		v6 = vector_asin_scalar(v6);
		v7 = vector_asin_scalar(v7);
	}

	benchmark::DoNotOptimize(v0);
	benchmark::DoNotOptimize(v1);
	benchmark::DoNotOptimize(v2);
	benchmark::DoNotOptimize(v3);
	benchmark::DoNotOptimize(v4);
	benchmark::DoNotOptimize(v5);
	benchmark::DoNotOptimize(v6);
	benchmark::DoNotOptimize(v7);
}

BENCHMARK(bm_vector_asin_scalar);

#if defined(RTM_SSE2_INTRINSICS)
static void bm_vector_asin_sse2(benchmark::State& state)
{
	vector4f v0 = vector_set(-0.134f);
	vector4f v1 = vector_set(0.134f);
	vector4f v2 = vector_set(-0.134f);
	vector4f v3 = vector_set(0.134f);
	vector4f v4 = vector_set(-0.134f);
	vector4f v5 = vector_set(0.134f);
	vector4f v6 = vector_set(-0.134f);
	vector4f v7 = vector_set(0.134f);

	for (auto _ : state)
	{
		v0 = vector_asin_sse2(v0);
		v1 = vector_asin_sse2(v1);
		v2 = vector_asin_sse2(v2);
		v3 = vector_asin_sse2(v3);
		v4 = vector_asin_sse2(v4);
		v5 = vector_asin_sse2(v5);
		v6 = vector_asin_sse2(v6);
		v7 = vector_asin_sse2(v7);
	}

	benchmark::DoNotOptimize(v0);
	benchmark::DoNotOptimize(v1);
	benchmark::DoNotOptimize(v2);
	benchmark::DoNotOptimize(v3);
	benchmark::DoNotOptimize(v4);
	benchmark::DoNotOptimize(v5);
	benchmark::DoNotOptimize(v6);
	benchmark::DoNotOptimize(v7);
}

BENCHMARK(bm_vector_asin_sse2);
#endif

#if defined(RTM_NEON_INTRINSICS)
static void bm_vector_asin_neon(benchmark::State& state)
{
	vector4f v0 = vector_set(-0.134f);
	vector4f v1 = vector_set(0.134f);
	vector4f v2 = vector_set(-0.134f);
	vector4f v3 = vector_set(0.134f);
	vector4f v4 = vector_set(-0.134f);
	vector4f v5 = vector_set(0.134f);
	vector4f v6 = vector_set(-0.134f);
	vector4f v7 = vector_set(0.134f);

	for (auto _ : state)
	{
		v0 = vector_asin_neon(v0);
		v1 = vector_asin_neon(v1);
		v2 = vector_asin_neon(v2);
		v3 = vector_asin_neon(v3);
		v4 = vector_asin_neon(v4);
		v5 = vector_asin_neon(v5);
		v6 = vector_asin_neon(v6);
		v7 = vector_asin_neon(v7);
	}

	benchmark::DoNotOptimize(v0);
	benchmark::DoNotOptimize(v1);
	benchmark::DoNotOptimize(v2);
	benchmark::DoNotOptimize(v3);
	benchmark::DoNotOptimize(v4);
	benchmark::DoNotOptimize(v5);
	benchmark::DoNotOptimize(v6);
	benchmark::DoNotOptimize(v7);
}

BENCHMARK(bm_vector_asin_neon);
#endif

```

`tools/bench/sources/bench_vector_atan.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <benchmark/benchmark.h>

#include <rtm/vector4f.h>

using namespace rtm;

RTM_FORCE_NOINLINE vector4f RTM_SIMD_CALL vector_atan_scalar(vector4f_arg0 input) RTM_NO_EXCEPT
{
	scalarf x = scalar_atan(vector_get_x_as_scalar(input));
	scalarf y = scalar_atan(vector_get_y_as_scalar(input));
	scalarf z = scalar_atan(vector_get_z_as_scalar(input));
	scalarf w = scalar_atan(vector_get_w_as_scalar(input));
	return vector_set(x, y, z, w);
}

#if defined(RTM_SSE2_INTRINSICS)
RTM_FORCE_NOINLINE vector4f RTM_SIMD_CALL vector_atan_sse2(vector4f_arg0 input) RTM_NO_EXCEPT
{
	// Use a degree 13 minimax approximation polynomial
	// See: GPGPU Programming for Games and Science (David H. Eberly)

	// Discard our sign, we'll restore it later
	const __m128i abs_mask = _mm_set_epi32(0x7FFFFFFFULL, 0x7FFFFFFFULL, 0x7FFFFFFFULL, 0x7FFFFFFFULL);
	__m128 abs_value = _mm_and_ps(input, _mm_castsi128_ps(abs_mask));

	// Compute our value
	__m128 is_larger_than_one = _mm_cmpgt_ps(abs_value, _mm_set_ps1(1.0F));
	__m128 reciprocal = vector_reciprocal(abs_value);

	__m128 x = vector_select(is_larger_than_one, reciprocal, abs_value);

	__m128 x2 = _mm_mul_ps(x, x);

	__m128 result = _mm_add_ps(_mm_mul_ps(x2, _mm_set_ps1(7.2128853633444123e-3F)), _mm_set_ps1(-3.5059680836411644e-2F));
	result = _mm_add_ps(_mm_mul_ps(result, x2), _mm_set_ps1(8.1675882859940430e-2F));
	result = _mm_add_ps(_mm_mul_ps(result, x2), _mm_set_ps1(-1.3374657325451267e-1F));
	result = _mm_add_ps(_mm_mul_ps(result, x2), _mm_set_ps1(1.9856563505717162e-1F));
	result = _mm_add_ps(_mm_mul_ps(result, x2), _mm_set_ps1(-3.3324998579202170e-1F));
	result = _mm_add_ps(_mm_mul_ps(result, x2), _mm_set_ps1(1.0F));
	result = _mm_mul_ps(result, x);

	__m128 remapped = _mm_sub_ps(_mm_set_ps1(rtm::constants::half_pi()), result);

	// pi/2 - result
	result = vector_select(is_larger_than_one, remapped, result);

	// Keep the original sign
	return _mm_or_ps(result, _mm_and_ps(input, _mm_set_ps1(-0.0F)));
}
#endif

#if defined(RTM_NEON_INTRINSICS)
RTM_FORCE_NOINLINE vector4f RTM_SIMD_CALL vector_atan_neon(vector4f_arg0 input) RTM_NO_EXCEPT
{
	// Use a degree 13 minimax approximation polynomial
	// See: GPGPU Programming for Games and Science (David H. Eberly)

	// Discard our sign, we'll restore it later
	float32x4_t abs_value = vabsq_f32(input);

	// Compute our value
	uint32x4_t is_larger_than_one = vcagtq_f32(input, vdupq_n_f32(1.0F));
	float32x4_t reciprocal = vector_reciprocal(abs_value);

	float32x4_t x = vector_select(is_larger_than_one, reciprocal, abs_value);

	float32x4_t x2 = vmulq_f32(x, x);

#if defined(RTM_NEON64_INTRINSICS)
	float32x4_t result = vfmaq_n_f32(vdupq_n_f32(-3.5059680836411644e-2F), x2, 7.2128853633444123e-3F);
	result = vfmaq_f32(vdupq_n_f32(8.1675882859940430e-2F), result, x2);
	result = vfmaq_f32(vdupq_n_f32(-1.3374657325451267e-1F), result, x2);
	result = vfmaq_f32(vdupq_n_f32(1.9856563505717162e-1F), result, x2);
	result = vfmaq_f32(vdupq_n_f32(-3.3324998579202170e-1F), result, x2);
	result = vfmaq_f32(vdupq_n_f32(1.0F), result, x2);
#else
	float32x4_t result = vmlaq_n_f32(vdupq_n_f32(-3.5059680836411644e-2F), x2, 7.2128853633444123e-3F);
	result = vmlaq_f32(vdupq_n_f32(8.1675882859940430e-2F), result, x2);
	result = vmlaq_f32(vdupq_n_f32(-1.3374657325451267e-1F), result, x2);
	result = vmlaq_f32(vdupq_n_f32(1.9856563505717162e-1F), result, x2);
	result = vmlaq_f32(vdupq_n_f32(-3.3324998579202170e-1F), result, x2);
	result = vmlaq_f32(vdupq_n_f32(1.0F), result, x2);
#endif

	result = vmulq_f32(result, x);

	float32x4_t remapped = vsubq_f32(vdupq_n_f32(rtm::constants::half_pi()), result);

	// pi/2 - result
	result = vector_select(is_larger_than_one, remapped, result);

	// Keep the original sign
	return vreinterpretq_f32_u32(vorrq_u32(vreinterpretq_u32_f32(result), vandq_u32(vreinterpretq_u32_f32(input), vreinterpretq_u32_f32(vdupq_n_f32(-0.0F)))));
}
#endif

static void bm_vector_atan_scalar(benchmark::State& state)
{
	vector4f v0 = vector_set(-0.134f);
	vector4f v1 = vector_set(0.134f);
	vector4f v2 = vector_set(-0.134f);
	vector4f v3 = vector_set(0.134f);
	vector4f v4 = vector_set(-0.134f);
	vector4f v5 = vector_set(0.134f);
	vector4f v6 = vector_set(-0.134f);
	vector4f v7 = vector_set(0.134f);

	for (auto _ : state)
	{
		v0 = vector_atan_scalar(v0);
		v1 = vector_atan_scalar(v1);
		v2 = vector_atan_scalar(v2);
		v3 = vector_atan_scalar(v3);
		v4 = vector_atan_scalar(v4);
		v5 = vector_atan_scalar(v5);
		v6 = vector_atan_scalar(v6);
		v7 = vector_atan_scalar(v7);
	}

	benchmark::DoNotOptimize(v0);
	benchmark::DoNotOptimize(v1);
	benchmark::DoNotOptimize(v2);
	benchmark::DoNotOptimize(v3);
	benchmark::DoNotOptimize(v4);
	benchmark::DoNotOptimize(v5);
	benchmark::DoNotOptimize(v6);
	benchmark::DoNotOptimize(v7);
}

BENCHMARK(bm_vector_atan_scalar);

#if defined(RTM_SSE2_INTRINSICS)
static void bm_vector_atan_sse2(benchmark::State& state)
{
	vector4f v0 = vector_set(-0.134f);
	vector4f v1 = vector_set(0.134f);
	vector4f v2 = vector_set(-0.134f);
	vector4f v3 = vector_set(0.134f);
	vector4f v4 = vector_set(-0.134f);
	vector4f v5 = vector_set(0.134f);
	vector4f v6 = vector_set(-0.134f);
	vector4f v7 = vector_set(0.134f);

	for (auto _ : state)
	{
		v0 = vector_atan_sse2(v0);
		v1 = vector_atan_sse2(v1);
		v2 = vector_atan_sse2(v2);
		v3 = vector_atan_sse2(v3);
		v4 = vector_atan_sse2(v4);
		v5 = vector_atan_sse2(v5);
		v6 = vector_atan_sse2(v6);
		v7 = vector_atan_sse2(v7);
	}

	benchmark::DoNotOptimize(v0);
	benchmark::DoNotOptimize(v1);
	benchmark::DoNotOptimize(v2);
	benchmark::DoNotOptimize(v3);
	benchmark::DoNotOptimize(v4);
	benchmark::DoNotOptimize(v5);
	benchmark::DoNotOptimize(v6);
	benchmark::DoNotOptimize(v7);
}

BENCHMARK(bm_vector_atan_sse2);
#endif

#if defined(RTM_NEON_INTRINSICS)
static void bm_vector_atan_neon(benchmark::State& state)
{
	vector4f v0 = vector_set(-0.134f);
	vector4f v1 = vector_set(0.134f);
	vector4f v2 = vector_set(-0.134f);
	vector4f v3 = vector_set(0.134f);
	vector4f v4 = vector_set(-0.134f);
	vector4f v5 = vector_set(0.134f);
	vector4f v6 = vector_set(-0.134f);
	vector4f v7 = vector_set(0.134f);

	for (auto _ : state)
	{
		v0 = vector_atan_neon(v0);
		v1 = vector_atan_neon(v1);
		v2 = vector_atan_neon(v2);
		v3 = vector_atan_neon(v3);
		v4 = vector_atan_neon(v4);
		v5 = vector_atan_neon(v5);
		v6 = vector_atan_neon(v6);
		v7 = vector_atan_neon(v7);
	}

	benchmark::DoNotOptimize(v0);
	benchmark::DoNotOptimize(v1);
	benchmark::DoNotOptimize(v2);
	benchmark::DoNotOptimize(v3);
	benchmark::DoNotOptimize(v4);
	benchmark::DoNotOptimize(v5);
	benchmark::DoNotOptimize(v6);
	benchmark::DoNotOptimize(v7);
}

BENCHMARK(bm_vector_atan_neon);
#endif

```

`tools/bench/sources/bench_vector_atan2.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <benchmark/benchmark.h>

#include <rtm/vector4f.h>

using namespace rtm;

RTM_FORCE_NOINLINE vector4f RTM_SIMD_CALL vector_atan2_scalar(vector4f_arg0 y, vector4f_arg1 x) RTM_NO_EXCEPT
{
	scalarf x_ = scalar_atan2(vector_get_x_as_scalar(y), vector_get_x_as_scalar(x));
	scalarf y_ = scalar_atan2(vector_get_y_as_scalar(y), vector_get_y_as_scalar(x));
	scalarf z = scalar_atan2(vector_get_z_as_scalar(y), vector_get_z_as_scalar(x));
	scalarf w = scalar_atan2(vector_get_w_as_scalar(y), vector_get_w_as_scalar(x));
	return vector_set(x_, y_, z, w);
}

#if defined(RTM_SSE2_INTRINSICS)
RTM_FORCE_NOINLINE vector4f RTM_SIMD_CALL vector_atan2_sse2(vector4f_arg0 y, vector4f_arg1 x) RTM_NO_EXCEPT
{
	// If X == 0.0 and Y != 0.0, we return PI/2 with the sign of Y
	// If X == 0.0 and Y == 0.0, we return 0.0
	// If X > 0.0, we return atan(y/x)
	// If X < 0.0, we return atan(y/x) + sign(Y) * PI
	// See: https://en.wikipedia.org/wiki/Atan2#Definition_and_computation

	const __m128 zero = _mm_setzero_ps();
	__m128 is_x_zero = _mm_cmpeq_ps(x, zero);
	__m128 is_y_zero = _mm_cmpeq_ps(y, zero);
	__m128 inputs_are_zero = _mm_and_ps(is_x_zero, is_y_zero);

	__m128 is_x_positive = _mm_cmpgt_ps(x, zero);

	const __m128 sign_mask = _mm_set_ps(-0.0F, -0.0F, -0.0F, -0.0F);
	__m128 y_sign = _mm_and_ps(y, sign_mask);

	// If X == 0.0, our offset is PI/2 otherwise it is PI both with the sign of Y
	__m128 half_pi = _mm_set_ps1(rtm::constants::half_pi());
	__m128 pi = _mm_set_ps1(rtm::constants::pi());
	__m128 offset = _mm_or_ps(_mm_and_ps(is_x_zero, half_pi), _mm_andnot_ps(is_x_zero, pi));
	offset = _mm_or_ps(offset, y_sign);

	// If X > 0.0, our offset is 0.0
	offset = _mm_andnot_ps(is_x_positive, offset);

	// If X == 0.0 and Y == 0.0, our offset is 0.0
	offset = _mm_andnot_ps(inputs_are_zero, offset);

	__m128 angle = _mm_div_ps(y, x);
	__m128 value = vector_atan(angle);

	// If X == 0.0, our value is 0.0 otherwise it is atan(y/x)
	value = _mm_andnot_ps(is_x_zero, value);

	// If X == 0.0 and Y == 0.0, our value is 0.0
	value = _mm_andnot_ps(inputs_are_zero, value);

	return _mm_add_ps(value, offset);
}
#endif

#if defined(RTM_NEON_INTRINSICS)
RTM_FORCE_NOINLINE vector4f RTM_SIMD_CALL vector_atan2_neon(vector4f_arg0 y, vector4f_arg1 x) RTM_NO_EXCEPT
{
	// If X == 0.0 and Y != 0.0, we return PI/2 with the sign of Y
	// If X == 0.0 and Y == 0.0, we return 0.0
	// If X > 0.0, we return atan(y/x)
	// If X < 0.0, we return atan(y/x) + sign(Y) * PI
	// See: https://en.wikipedia.org/wiki/Atan2#Definition_and_computation

#if defined(RTM_NEON64_INTRINSICS)
	uint32x4_t is_x_zero = vceqzq_f32(x);
	uint32x4_t is_y_zero = vceqzq_f32(y);
	uint32x4_t inputs_are_zero = vandq_u32(is_x_zero, is_y_zero);

	uint32x4_t is_x_positive = vcgtzq_f32(x);
#else
	float32x4_t zero = vdupq_n_f32(0.0F);
	uint32x4_t is_x_zero = vceqq_f32(x, zero);
	uint32x4_t is_y_zero = vceqq_f32(y, zero);
	uint32x4_t inputs_are_zero = vandq_u32(is_x_zero, is_y_zero);

	uint32x4_t is_x_positive = vcgtq_f32(x, zero);
#endif

	uint32x4_t y_sign = vandq_u32(vreinterpretq_u32_f32(y), vreinterpretq_u32_f32(vdupq_n_f32(-0.0F)));

	// If X == 0.0, our offset is PI/2 otherwise it is PI both with the sign of Y
	float32x4_t half_pi = vdupq_n_f32(rtm::constants::half_pi());
	float32x4_t pi = vdupq_n_f32(rtm::constants::pi());
	float32x4_t offset = vreinterpretq_f32_u32(vorrq_u32(vandq_u32(is_x_zero, vreinterpretq_u32_f32(half_pi)), vandq_u32(vmvnq_u32(is_x_zero), vreinterpretq_u32_f32(pi))));
	offset = vreinterpretq_f32_u32(vorrq_u32(vreinterpretq_u32_f32(offset), y_sign));

	// If X > 0.0, our offset is 0.0
	offset = vreinterpretq_f32_u32(vandq_u32(vmvnq_u32(is_x_positive), vreinterpretq_u32_f32(offset)));

	// If X == 0.0 and Y == 0.0, our offset is 0.0
	offset = vreinterpretq_f32_u32(vandq_u32(vmvnq_u32(inputs_are_zero), vreinterpretq_u32_f32(offset)));

	float32x4_t angle = vector_div(y, x);
	float32x4_t value = vector_atan(angle);

	// If X == 0.0, our value is 0.0 otherwise it is atan(y/x)
	value = vreinterpretq_f32_u32(vandq_u32(vmvnq_u32(is_x_zero), vreinterpretq_u32_f32(value)));

	// If X == 0.0 and Y == 0.0, our value is 0.0
	value = vreinterpretq_f32_u32(vandq_u32(vmvnq_u32(inputs_are_zero), vreinterpretq_u32_f32(value)));

	return vaddq_f32(value, offset);
}
#endif

static void bm_vector_atan2_scalar(benchmark::State& state)
{
	vector4f v0 = vector_set(-0.134f);
	vector4f v1 = vector_set(0.134f);
	vector4f v2 = vector_set(-0.134f);
	vector4f v3 = vector_set(0.134f);
	vector4f v4 = vector_set(-0.134f);
	vector4f v5 = vector_set(0.134f);
	vector4f v6 = vector_set(-0.134f);
	vector4f v7 = vector_set(0.134f);

	for (auto _ : state)
	{
		v0 = vector_atan2_scalar(v0, v1);
		v1 = vector_atan2_scalar(v1, v2);
		v2 = vector_atan2_scalar(v2, v3);
		v3 = vector_atan2_scalar(v3, v4);
		v4 = vector_atan2_scalar(v4, v5);
		v5 = vector_atan2_scalar(v5, v6);
		v6 = vector_atan2_scalar(v6, v7);
		v7 = vector_atan2_scalar(v7, v0);
	}

	benchmark::DoNotOptimize(v0);
	benchmark::DoNotOptimize(v1);
	benchmark::DoNotOptimize(v2);
	benchmark::DoNotOptimize(v3);
	benchmark::DoNotOptimize(v4);
	benchmark::DoNotOptimize(v5);
	benchmark::DoNotOptimize(v6);
	benchmark::DoNotOptimize(v7);
}

BENCHMARK(bm_vector_atan2_scalar);

#if defined(RTM_SSE2_INTRINSICS)
static void bm_vector_atan2_sse2(benchmark::State& state)
{
	vector4f v0 = vector_set(-0.134f);
	vector4f v1 = vector_set(0.134f);
	vector4f v2 = vector_set(-0.134f);
	vector4f v3 = vector_set(0.134f);
	vector4f v4 = vector_set(-0.134f);
	vector4f v5 = vector_set(0.134f);
	vector4f v6 = vector_set(-0.134f);
	vector4f v7 = vector_set(0.134f);

	for (auto _ : state)
	{
		v0 = vector_atan2_sse2(v0, v1);
		v1 = vector_atan2_sse2(v1, v2);
		v2 = vector_atan2_sse2(v2, v3);
		v3 = vector_atan2_sse2(v3, v4);
		v4 = vector_atan2_sse2(v4, v5);
		v5 = vector_atan2_sse2(v5, v6);
		v6 = vector_atan2_sse2(v6, v7);
		v7 = vector_atan2_sse2(v7, v0);
	}

	benchmark::DoNotOptimize(v0);
	benchmark::DoNotOptimize(v1);
	benchmark::DoNotOptimize(v2);
	benchmark::DoNotOptimize(v3);
	benchmark::DoNotOptimize(v4);
	benchmark::DoNotOptimize(v5);
	benchmark::DoNotOptimize(v6);
	benchmark::DoNotOptimize(v7);
}

BENCHMARK(bm_vector_atan2_sse2);
#endif

#if defined(RTM_NEON_INTRINSICS)
static void bm_vector_atan2_neon(benchmark::State& state)
{
	vector4f v0 = vector_set(-0.134f);
	vector4f v1 = vector_set(0.134f);
	vector4f v2 = vector_set(-0.134f);
	vector4f v3 = vector_set(0.134f);
	vector4f v4 = vector_set(-0.134f);
	vector4f v5 = vector_set(0.134f);
	vector4f v6 = vector_set(-0.134f);
	vector4f v7 = vector_set(0.134f);

	for (auto _ : state)
	{
		v0 = vector_atan2_neon(v0, v1);
		v1 = vector_atan2_neon(v1, v2);
		v2 = vector_atan2_neon(v2, v3);
		v3 = vector_atan2_neon(v3, v4);
		v4 = vector_atan2_neon(v4, v5);
		v5 = vector_atan2_neon(v5, v6);
		v6 = vector_atan2_neon(v6, v7);
		v7 = vector_atan2_neon(v7, v0);
	}

	benchmark::DoNotOptimize(v0);
	benchmark::DoNotOptimize(v1);
	benchmark::DoNotOptimize(v2);
	benchmark::DoNotOptimize(v3);
	benchmark::DoNotOptimize(v4);
	benchmark::DoNotOptimize(v5);
	benchmark::DoNotOptimize(v6);
	benchmark::DoNotOptimize(v7);
}

BENCHMARK(bm_vector_atan2_neon);
#endif

```

`tools/bench/sources/bench_vector_ceil.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <benchmark/benchmark.h>

#include <rtm/vector4f.h>

using namespace rtm;

RTM_FORCE_NOINLINE vector4f RTM_SIMD_CALL vector_ceil_scalar(vector4f_arg0 input) RTM_NO_EXCEPT
{
	scalarf x = vector_get_x_as_scalar(input);
	scalarf y = vector_get_y_as_scalar(input);
	scalarf z = vector_get_z_as_scalar(input);
	scalarf w = vector_get_w_as_scalar(input);
	return vector_set(scalar_ceil(x), scalar_ceil(y), scalar_ceil(z), scalar_ceil(w));
}

#if defined(RTM_SSE2_INTRINSICS)
RTM_FORCE_NOINLINE vector4f RTM_SIMD_CALL vector_ceil_sse2(vector4f_arg0 input) RTM_NO_EXCEPT
{
	// NaN, +- Infinity, and numbers larger or equal to 2^23 remain unchanged
	// since they have no fractional part.

	const __m128i abs_mask = _mm_set_epi32(0x7FFFFFFFULL, 0x7FFFFFFFULL, 0x7FFFFFFFULL, 0x7FFFFFFFULL);
	const __m128 fractional_limit = _mm_set_ps1(8388608.0F); // 2^23

	// Build our mask, larger values that have no fractional part, and infinities will be true
	// Smaller values and NaN will be false
	__m128 abs_input = _mm_and_ps(input, _mm_castsi128_ps(abs_mask));
	__m128 is_input_large = _mm_cmpge_ps(abs_input, fractional_limit);

	// Test if our input is NaN with (value != value), it is only true for NaN
	__m128 is_nan = _mm_cmpneq_ps(input, input);

	// Combine our masks to determine if we should return the original value
	__m128 use_original_input = _mm_or_ps(is_input_large, is_nan);

	// Convert to an integer and back. This does banker's rounding by default
	__m128 integer_part = _mm_cvtepi32_ps(_mm_cvtps_epi32(input));

	// Test if the returned value is smaller than the original.
	// A positive input will round towards zero and be lower when we need it to be greater.
	__m128 is_positive = _mm_cmplt_ps(integer_part, input);

	// Convert our mask to a float, ~0 yields -1.0 since it is a valid signed integer
	// Negative values will yield a 0.0 bias
	__m128 bias = _mm_cvtepi32_ps(_mm_castps_si128(is_positive));

	// Subtract our bias to properly handle positive values
	integer_part = _mm_sub_ps(integer_part, bias);

	return _mm_or_ps(_mm_and_ps(use_original_input, input), _mm_andnot_ps(use_original_input, integer_part));
}
#endif

#if defined(RTM_NEON_INTRINSICS)
RTM_FORCE_NOINLINE vector4f RTM_SIMD_CALL vector_ceil_neon(vector4f_arg0 input) RTM_NO_EXCEPT
{
	// NaN, +- Infinity, and numbers larger or equal to 2^23 remain unchanged
	// since they have no fractional part.

	float32x4_t fractional_limit = vdupq_n_f32(8388608.0F); // 2^23

	// Build our mask, larger values that have no fractional part, and infinities will be true
	// Smaller values and NaN will be false
	uint32x4_t is_input_large = vcageq_f32(input, fractional_limit);

	// Test if our input is NaN with (value != value), it is only true for NaN
	uint32x4_t is_nan = vmvnq_u32(vceqq_f32(input, input));

	// Combine our masks to determine if we should return the original value
	uint32x4_t use_original_input = vorrq_u32(is_input_large, is_nan);

	// Convert to an integer and back. This does banker's rounding by default
	float32x4_t integer_part = vcvtq_f32_s32(vcvtq_s32_f32(input));

	// Test if the returned value is smaller than the original.
	// A positive input will round towards zero and be lower when we need it to be greater.
	uint32x4_t is_positive = vcltq_f32(integer_part, input);

	float32x4_t bias = vcvtq_f32_s32(is_positive);

	// Subtract our bias to properly handle positive values
	integer_part = vsubq_f32(integer_part, bias);

	return vbslq_f32(use_original_input, input, integer_part);
}
#endif

static void bm_vector_ceil_scalar(benchmark::State& state)
{
	vector4f v0 = vector_set(-123.134f);
	vector4f v1 = vector_set(123.134f);
	vector4f v2 = vector_set(-123.134f);
	vector4f v3 = vector_set(123.134f);
	vector4f v4 = vector_set(-123.134f);
	vector4f v5 = vector_set(123.134f);
	vector4f v6 = vector_set(-123.134f);
	vector4f v7 = vector_set(123.134f);

	for (auto _ : state)
	{
		v0 = vector_ceil_scalar(v0);
		v1 = vector_ceil_scalar(v1);
		v2 = vector_ceil_scalar(v2);
		v3 = vector_ceil_scalar(v3);
		v4 = vector_ceil_scalar(v4);
		v5 = vector_ceil_scalar(v5);
		v6 = vector_ceil_scalar(v6);
		v7 = vector_ceil_scalar(v7);
	}

	benchmark::DoNotOptimize(v0);
	benchmark::DoNotOptimize(v1);
	benchmark::DoNotOptimize(v2);
	benchmark::DoNotOptimize(v3);
	benchmark::DoNotOptimize(v4);
	benchmark::DoNotOptimize(v5);
	benchmark::DoNotOptimize(v6);
	benchmark::DoNotOptimize(v7);
}

BENCHMARK(bm_vector_ceil_scalar);

#if defined(RTM_SSE2_INTRINSICS)
static void bm_vector_ceil_sse2(benchmark::State& state)
{
	vector4f v0 = vector_set(-123.134f);
	vector4f v1 = vector_set(123.134f);
	vector4f v2 = vector_set(-123.134f);
	vector4f v3 = vector_set(123.134f);
	vector4f v4 = vector_set(-123.134f);
	vector4f v5 = vector_set(123.134f);
	vector4f v6 = vector_set(-123.134f);
	vector4f v7 = vector_set(123.134f);

	for (auto _ : state)
	{
		v0 = vector_ceil_sse2(v0);
		v1 = vector_ceil_sse2(v1);
		v2 = vector_ceil_sse2(v2);
		v3 = vector_ceil_sse2(v3);
		v4 = vector_ceil_sse2(v4);
		v5 = vector_ceil_sse2(v5);
		v6 = vector_ceil_sse2(v6);
		v7 = vector_ceil_sse2(v7);
	}

	benchmark::DoNotOptimize(v0);
	benchmark::DoNotOptimize(v1);
	benchmark::DoNotOptimize(v2);
	benchmark::DoNotOptimize(v3);
	benchmark::DoNotOptimize(v4);
	benchmark::DoNotOptimize(v5);
	benchmark::DoNotOptimize(v6);
	benchmark::DoNotOptimize(v7);
}

BENCHMARK(bm_vector_ceil_sse2);
#endif

#if defined(RTM_NEON_INTRINSICS)
static void bm_vector_ceil_neon(benchmark::State& state)
{
	vector4f v0 = vector_set(-123.134f);
	vector4f v1 = vector_set(123.134f);
	vector4f v2 = vector_set(-123.134f);
	vector4f v3 = vector_set(123.134f);
	vector4f v4 = vector_set(-123.134f);
	vector4f v5 = vector_set(123.134f);
	vector4f v6 = vector_set(-123.134f);
	vector4f v7 = vector_set(123.134f);

	for (auto _ : state)
	{
		v0 = vector_ceil_neon(v0);
		v1 = vector_ceil_neon(v1);
		v2 = vector_ceil_neon(v2);
		v3 = vector_ceil_neon(v3);
		v4 = vector_ceil_neon(v4);
		v5 = vector_ceil_neon(v5);
		v6 = vector_ceil_neon(v6);
		v7 = vector_ceil_neon(v7);
	}

	benchmark::DoNotOptimize(v0);
	benchmark::DoNotOptimize(v1);
	benchmark::DoNotOptimize(v2);
	benchmark::DoNotOptimize(v3);
	benchmark::DoNotOptimize(v4);
	benchmark::DoNotOptimize(v5);
	benchmark::DoNotOptimize(v6);
	benchmark::DoNotOptimize(v7);
}

BENCHMARK(bm_vector_ceil_neon);
#endif

```

`tools/bench/sources/bench_vector_cos.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <benchmark/benchmark.h>

#include <rtm/vector4f.h>

using namespace rtm;

RTM_FORCE_NOINLINE vector4f RTM_SIMD_CALL vector_cos_scalar(vector4f_arg0 input) RTM_NO_EXCEPT
{
	scalarf x = scalar_cos(vector_get_x_as_scalar(input));
	scalarf y = scalar_cos(vector_get_y_as_scalar(input));
	scalarf z = scalar_cos(vector_get_z_as_scalar(input));
	scalarf w = scalar_cos(vector_get_w_as_scalar(input));
	return vector_set(x, y, z, w);
}

#if defined(RTM_SSE2_INTRINSICS)
RTM_FORCE_NOINLINE vector4f RTM_SIMD_CALL vector_cos_sse2(vector4f_arg0 input) RTM_NO_EXCEPT
{
	// Use a degree 10 minimax approximation polynomial
	// See: GPGPU Programming for Games and Science (David H. Eberly)

	// Remap our input in the [-pi, pi] range
	__m128 quotient = _mm_mul_ps(input, _mm_set_ps1(rtm::constants::one_div_two_pi()));
	quotient = vector_round_bankers(quotient);
	quotient = _mm_mul_ps(quotient, _mm_set_ps1(rtm::constants::two_pi()));
	__m128 x = _mm_sub_ps(input, quotient);

	// Remap our input in the [-pi/2, pi/2] range
	const __m128 sign_mask = _mm_set_ps(-0.0F, -0.0F, -0.0F, -0.0F);
	__m128 x_sign = _mm_and_ps(x, sign_mask);
	__m128 reference = _mm_or_ps(x_sign, _mm_set_ps1(rtm::constants::pi()));
	const __m128 reflection = _mm_sub_ps(reference, x);

	const __m128i abs_mask = _mm_set_epi32(0x7FFFFFFFULL, 0x7FFFFFFFULL, 0x7FFFFFFFULL, 0x7FFFFFFFULL);
	__m128 x_abs = _mm_and_ps(x, _mm_castsi128_ps(abs_mask));
	__m128 is_less_equal_than_half_pi = _mm_cmple_ps(x_abs, _mm_set_ps1(rtm::constants::half_pi()));

	x = RTM_VECTOR4F_SELECT(is_less_equal_than_half_pi, x, reflection);

	// Calculate our value
	const __m128 x2 = _mm_mul_ps(x, x);
	__m128 result = _mm_add_ps(_mm_mul_ps(x2, _mm_set_ps1(-2.6051615464872668e-7F)), _mm_set_ps1(2.4760495088926859e-5F));
	result = _mm_add_ps(_mm_mul_ps(result, x2), _mm_set_ps1(-1.3888377661039897e-3F));
	result = _mm_add_ps(_mm_mul_ps(result, x2), _mm_set_ps1(4.1666638865338612e-2F));
	result = _mm_add_ps(_mm_mul_ps(result, x2), _mm_set_ps1(-4.9999999508695869e-1F));
	result = _mm_add_ps(_mm_mul_ps(result, x2), _mm_set_ps1(1.0F));

	// Remap into [-pi, pi]
	return _mm_or_ps(result, _mm_andnot_ps(is_less_equal_than_half_pi, sign_mask));
}
#endif

#if defined(RTM_NEON_INTRINSICS)
RTM_FORCE_NOINLINE vector4f RTM_SIMD_CALL vector_cos_neon(vector4f_arg0 input) RTM_NO_EXCEPT
{
	// Use a degree 10 minimax approximation polynomial
	// See: GPGPU Programming for Games and Science (David H. Eberly)

	// Remap our input in the [-pi, pi] range
	float32x4_t quotient = vmulq_n_f32(input, rtm::constants::one_div_two_pi());
	quotient = vector_round_bankers(quotient);
	quotient = vmulq_n_f32(quotient, rtm::constants::two_pi());
	float32x4_t x = vsubq_f32(input, quotient);

	// Remap our input in the [-pi/2, pi/2] range
	uint32x4_t sign_mask = vreinterpretq_u32_f32(vdupq_n_f32(-0.0F));
	uint32x4_t sign = vandq_u32(vreinterpretq_u32_f32(x), sign_mask);
	float32x4_t reference = vreinterpretq_f32_u32(vorrq_u32(sign, vreinterpretq_u32_f32(vdupq_n_f32(rtm::constants::pi()))));

	float32x4_t reflection = vsubq_f32(reference, x);
	float32x4_t is_less_equal_than_half_pi = vcaleq_f32(x, vdupq_n_f32(rtm::constants::half_pi()));
	x = vbslq_f32(is_less_equal_than_half_pi, x, reflection);

	// Calculate our value
	float32x4_t x2 = vmulq_f32(x, x);

#if defined(RTM_NEON64_INTRINSICS)
	float32x4_t result = vfmaq_n_f32(vdupq_n_f32(2.4760495088926859e-5F), x2, -2.6051615464872668e-7F);
	result = vfmaq_f32(vdupq_n_f32(-1.3888377661039897e-3F), result, x2);
	result = vfmaq_f32(vdupq_n_f32(4.1666638865338612e-2F), result, x2);
	result = vfmaq_f32(vdupq_n_f32(-4.9999999508695869e-1F), result, x2);
	result = vfmaq_f32(vdupq_n_f32(1.0F), result, x2);
#else
	float32x4_t result = vmlaq_n_f32(vdupq_n_f32(2.4760495088926859e-5F), x2, -2.6051615464872668e-7F);
	result = vmlaq_f32(vdupq_n_f32(-1.3888377661039897e-3F), result, x2);
	result = vmlaq_f32(vdupq_n_f32(4.1666638865338612e-2F), result, x2);
	result = vmlaq_f32(vdupq_n_f32(-4.9999999508695869e-1F), result, x2);
	result = vmlaq_f32(vdupq_n_f32(1.0F), result, x2);
#endif

	// Remap into [-pi, pi]
	return vbslq_f32(is_less_equal_than_half_pi, result, vnegq_f32(result));
}
#endif

static void bm_vector_cos_scalar(benchmark::State& state)
{
	vector4f v0 = vector_set(-123.134f);
	vector4f v1 = vector_set(123.134f);
	vector4f v2 = vector_set(-123.134f);
	vector4f v3 = vector_set(123.134f);
	vector4f v4 = vector_set(-123.134f);
	vector4f v5 = vector_set(123.134f);
	vector4f v6 = vector_set(-123.134f);
	vector4f v7 = vector_set(123.134f);

	vector4f scale = vector_set(100.0F);

	for (auto _ : state)
	{
		v0 = vector_mul(vector_cos_scalar(v0), scale);
		v1 = vector_mul(vector_cos_scalar(v1), scale);
		v2 = vector_mul(vector_cos_scalar(v2), scale);
		v3 = vector_mul(vector_cos_scalar(v3), scale);
		v4 = vector_mul(vector_cos_scalar(v4), scale);
		v5 = vector_mul(vector_cos_scalar(v5), scale);
		v6 = vector_mul(vector_cos_scalar(v6), scale);
		v7 = vector_mul(vector_cos_scalar(v7), scale);
	}

	benchmark::DoNotOptimize(v0);
	benchmark::DoNotOptimize(v1);
	benchmark::DoNotOptimize(v2);
	benchmark::DoNotOptimize(v3);
	benchmark::DoNotOptimize(v4);
	benchmark::DoNotOptimize(v5);
	benchmark::DoNotOptimize(v6);
	benchmark::DoNotOptimize(v7);
}

BENCHMARK(bm_vector_cos_scalar);

#if defined(RTM_SSE2_INTRINSICS)
static void bm_vector_cos_sse2(benchmark::State& state)
{
	vector4f v0 = vector_set(-123.134f);
	vector4f v1 = vector_set(123.134f);
	vector4f v2 = vector_set(-123.134f);
	vector4f v3 = vector_set(123.134f);
	vector4f v4 = vector_set(-123.134f);
	vector4f v5 = vector_set(123.134f);
	vector4f v6 = vector_set(-123.134f);
	vector4f v7 = vector_set(123.134f);

	vector4f scale = vector_set(100.0F);

	for (auto _ : state)
	{
		v0 = vector_mul(vector_cos_sse2(v0), scale);
		v1 = vector_mul(vector_cos_sse2(v1), scale);
		v2 = vector_mul(vector_cos_sse2(v2), scale);
		v3 = vector_mul(vector_cos_sse2(v3), scale);
		v4 = vector_mul(vector_cos_sse2(v4), scale);
		v5 = vector_mul(vector_cos_sse2(v5), scale);
		v6 = vector_mul(vector_cos_sse2(v6), scale);
		v7 = vector_mul(vector_cos_sse2(v7), scale);
	}

	benchmark::DoNotOptimize(v0);
	benchmark::DoNotOptimize(v1);
	benchmark::DoNotOptimize(v2);
	benchmark::DoNotOptimize(v3);
	benchmark::DoNotOptimize(v4);
	benchmark::DoNotOptimize(v5);
	benchmark::DoNotOptimize(v6);
	benchmark::DoNotOptimize(v7);
}

BENCHMARK(bm_vector_cos_sse2);
#endif

#if defined(RTM_NEON_INTRINSICS)
static void bm_vector_cos_neon(benchmark::State& state)
{
	vector4f v0 = vector_set(-123.134f);
	vector4f v1 = vector_set(123.134f);
	vector4f v2 = vector_set(-123.134f);
	vector4f v3 = vector_set(123.134f);
	vector4f v4 = vector_set(-123.134f);
	vector4f v5 = vector_set(123.134f);
	vector4f v6 = vector_set(-123.134f);
	vector4f v7 = vector_set(123.134f);

	vector4f scale = vector_set(100.0F);

	for (auto _ : state)
	{
		v0 = vector_mul(vector_cos_neon(v0), scale);
		v1 = vector_mul(vector_cos_neon(v1), scale);
		v2 = vector_mul(vector_cos_neon(v2), scale);
		v3 = vector_mul(vector_cos_neon(v3), scale);
		v4 = vector_mul(vector_cos_neon(v4), scale);
		v5 = vector_mul(vector_cos_neon(v5), scale);
		v6 = vector_mul(vector_cos_neon(v6), scale);
		v7 = vector_mul(vector_cos_neon(v7), scale);
	}

	benchmark::DoNotOptimize(v0);
	benchmark::DoNotOptimize(v1);
	benchmark::DoNotOptimize(v2);
	benchmark::DoNotOptimize(v3);
	benchmark::DoNotOptimize(v4);
	benchmark::DoNotOptimize(v5);
	benchmark::DoNotOptimize(v6);
	benchmark::DoNotOptimize(v7);
}

BENCHMARK(bm_vector_cos_neon);
#endif

```

`tools/bench/sources/bench_vector_cross3.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2024 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <benchmark/benchmark.h>

#include <rtm/vector4f.h>

using namespace rtm;

RTM_FORCE_NOINLINE vector4f RTM_SIMD_CALL vector_cross3_scalar(vector4f_arg0 lhs, vector4f_arg1 rhs) RTM_NO_EXCEPT
{
	// cross(a, b) =
	//    (a.y * b.z) - (a.z * b.y)
	//    (a.z * b.x) - (a.x * b.z)
	//    (a.x * b.y) - (a.y * b.x)
	//
	// cross(a, b) = (a.yzx * b.zxy) - (a.zxy * b.yzx)

	// Compiles down to this with ARM64:
	// mov    s3, v0[1]
	// mov    s4, v1[1]
	// ext.16b v2, v0, v0, #0x8
	// ext.8b v5, v0, v2, #0x4
	// mov.s  v2[1], v0[0]
	// fneg.2s v2, v2
	// ext.16b v6, v1, v1, #0x8
	// ext.8b v7, v1, v6, #0x4
	// fmul.2s v2, v7, v2
	// mov.s  v6[1], v1[0]
	// fmla.2s v2, v6, v5
	// fneg   s3, s3
	// fmul.s s1, s3, v1[0]
	// fmla.s s1, s4, v0[0]
	// movi   d0, #0000000000000000
	// mov.s  v0[0], v1[0]
	// mov.d  v2[1], v0[0]
	// mov.16b v0, v2

	const float lhs_x = vector_get_x(lhs);
	const float lhs_y = vector_get_y(lhs);
	const float lhs_z = vector_get_z(lhs);
	const float rhs_x = vector_get_x(rhs);
	const float rhs_y = vector_get_y(rhs);
	const float rhs_z = vector_get_z(rhs);
	return vector_set((lhs_y * rhs_z) - (lhs_z * rhs_y), (lhs_z * rhs_x) - (lhs_x * rhs_z), (lhs_x * rhs_y) - (lhs_y * rhs_x));
}

#if defined(RTM_NEON_INTRINSICS)
RTM_FORCE_NOINLINE vector4f RTM_SIMD_CALL vector_cross3_neon(vector4f_arg0 lhs, vector4f_arg1 rhs) RTM_NO_EXCEPT
{
	// cross(a, b) =
	//    (a.y * b.z) - (a.z * b.y)
	//    (a.z * b.x) - (a.x * b.z)
	//    (a.x * b.y) - (a.y * b.x)
	//  or ..
	//    (a.yzx * b.zxy) - (a.zxy * b.yzx)

	// Compiles down to this with ARM64:
	// ext.16b v2, v0, v0, #0x4
	// mov.s  v2[2], v0[0]
	// ext.16b v3, v0, v1, #0xc
	// mov.s  v3[0], v1[2]
	// fmul.4s v2, v2, v3
	// ext.16b v3, v0, v0, #0xc
	// ext.16b v4, v1, v1, #0x4
	// mov.s  v3[0], v0[2]
	// mov.s  v4[2], v1[0]
	// fmls.4s v2, v4, v3
	// mov.16b v0, v2

	float32x4_t lhs_yzwx = vextq_f32(lhs, lhs, 1);
	float32x4_t rhs_wxyz = vextq_f32(rhs, rhs, 3);

	float32x4_t lhs_yzx = vsetq_lane_f32(vgetq_lane_f32(lhs, 0), lhs_yzwx, 2);
	float32x4_t rhs_zxy = vsetq_lane_f32(vgetq_lane_f32(rhs, 2), rhs_wxyz, 0);

	// part_a = (a.yzx * b.zxy)
	float32x4_t part_a = vmulq_f32(lhs_yzx, rhs_zxy);

	float32x4_t lhs_wxyz = vextq_f32(lhs, lhs, 3);
	float32x4_t rhs_yzwx = vextq_f32(rhs, rhs, 1);
	float32x4_t lhs_zxy = vsetq_lane_f32(vgetq_lane_f32(lhs, 2), lhs_wxyz, 0);
	float32x4_t rhs_yzx = vsetq_lane_f32(vgetq_lane_f32(rhs, 0), rhs_yzwx, 2);

	return vmlsq_f32(part_a, lhs_zxy, rhs_yzx);
}
#endif

#if defined(RTM_SSE2_INTRINSICS)
RTM_FORCE_NOINLINE vector4f RTM_SIMD_CALL vector_cross3_sse2(vector4f_arg0 lhs, vector4f_arg1 rhs) RTM_NO_EXCEPT
{
	// cross(a, b).zxy = (a * b.yzx) - (a.yzx * b)
	__m128 lhs_yzx = _mm_shuffle_ps(lhs, lhs, _MM_SHUFFLE(3, 0, 2, 1));
	__m128 rhs_yzx = _mm_shuffle_ps(rhs, rhs, _MM_SHUFFLE(3, 0, 2, 1));
	__m128 tmp_zxy = _mm_sub_ps(_mm_mul_ps(lhs, rhs_yzx), _mm_mul_ps(lhs_yzx, rhs));

	// cross(a, b) = ((a * b.yzx) - (a.yzx * b)).yzx
	return _mm_shuffle_ps(tmp_zxy, tmp_zxy, _MM_SHUFFLE(3, 0, 2, 1));
}
#endif

static void bm_vector_cross3_scalar(benchmark::State& state)
{
	vector4f v0 = vector_set(-1.0F, 1.0F, -2.0F, -123.134F);
	vector4f v1 = vector_set(-1.021F, 1.120F, -2.0331F, -1231.134F);
	vector4f v2 = vector_set(-11.0F, 1.330F, -21.50F, -1253.134F);
	vector4f v3 = vector_set(0.15F, 115.60F, 21.0221F, 123.13134F);

	for (auto _ : state)
	{
		v0 = vector_cross3_scalar(v0, v1);
		v1 = vector_cross3_scalar(v1, v2);
		v2 = vector_cross3_scalar(v2, v3);
		v3 = vector_cross3_scalar(v3, v0);
	}

	benchmark::DoNotOptimize(v0);
	benchmark::DoNotOptimize(v1);
	benchmark::DoNotOptimize(v2);
	benchmark::DoNotOptimize(v3);
}

BENCHMARK(bm_vector_cross3_scalar);

#if defined(RTM_NEON_INTRINSICS)
static void bm_vector_cross3_neon(benchmark::State& state)
{
	vector4f v0 = vector_set(-1.0F, 1.0F, -2.0F, -123.134F);
	vector4f v1 = vector_set(-1.021F, 1.120F, -2.0331F, -1231.134F);
	vector4f v2 = vector_set(-11.0F, 1.330F, -21.50F, -1253.134F);
	vector4f v3 = vector_set(0.15F, 115.60F, 21.0221F, 123.13134F);

	for (auto _ : state)
	{
		v0 = vector_cross3_neon(v0, v1);
		v1 = vector_cross3_neon(v1, v2);
		v2 = vector_cross3_neon(v2, v3);
		v3 = vector_cross3_neon(v3, v0);
	}

	benchmark::DoNotOptimize(v0);
	benchmark::DoNotOptimize(v1);
	benchmark::DoNotOptimize(v2);
	benchmark::DoNotOptimize(v3);
}

BENCHMARK(bm_vector_cross3_neon);
#endif

#if defined(RTM_SSE2_INTRINSICS)
static void bm_vector_cross3_sse2(benchmark::State& state)
{
	vector4f v0 = vector_set(-1.0F, 1.0F, -2.0F, -123.134F);
	vector4f v1 = vector_set(-1.021F, 1.120F, -2.0331F, -1231.134F);
	vector4f v2 = vector_set(-11.0F, 1.330F, -21.50F, -1253.134F);
	vector4f v3 = vector_set(0.15F, 115.60F, 21.0221F, 123.13134F);

	for (auto _ : state)
	{
		v0 = vector_cross3_sse2(v0, v1);
		v1 = vector_cross3_sse2(v1, v2);
		v2 = vector_cross3_sse2(v2, v3);
		v3 = vector_cross3_sse2(v3, v0);
	}

	benchmark::DoNotOptimize(v0);
	benchmark::DoNotOptimize(v1);
	benchmark::DoNotOptimize(v2);
	benchmark::DoNotOptimize(v3);
}

BENCHMARK(bm_vector_cross3_sse2);
#endif

```

`tools/bench/sources/bench_vector_dot.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2022 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <benchmark/benchmark.h>

#include <rtm/vector4f.h>

using namespace rtm;

RTM_FORCE_NOINLINE float RTM_SIMD_CALL vector_dot_scalar(vector4f_arg0 lhs, vector4f_arg1 rhs) RTM_NO_EXCEPT
{
	float lhs_x = vector_get_x(lhs);
	float lhs_y = vector_get_y(lhs);
	float lhs_z = vector_get_z(lhs);
	float lhs_w = vector_get_w(lhs);

	float rhs_x = vector_get_x(rhs);
	float rhs_y = vector_get_y(rhs);
	float rhs_z = vector_get_z(rhs);
	float rhs_w = vector_get_w(rhs);

	return (lhs_x * rhs_x) + (lhs_y * rhs_y) + (lhs_z * rhs_z) + (lhs_w * rhs_w);
}

#if defined(RTM_NEON_INTRINSICS)
RTM_FORCE_NOINLINE float RTM_SIMD_CALL vector_dot_neon(vector4f_arg0 lhs, vector4f_arg1 rhs) RTM_NO_EXCEPT
{
	// Compiles down to this with ARM64:
	// fmul.4s v0, v0, v1
	// ext.16b v1, v0, v0, #0x8
	// fadd.2s v0, v0, v1
	// faddp.2s v0, v0, v0
	float32x4_t x2_y2_z2_w2 = vmulq_f32(lhs, rhs);
	float32x2_t x2_y2 = vget_low_f32(x2_y2_z2_w2);
	float32x2_t z2_w2 = vget_high_f32(x2_y2_z2_w2);
	float32x2_t x2z2_y2w2 = vadd_f32(x2_y2, z2_w2);
	float32x2_t x2y2z2w2 = vpadd_f32(x2z2_y2w2, x2z2_y2w2);
	return vget_lane_f32(x2y2z2w2, 0);
}
#endif

#if defined(RTM_NEON64_INTRINSICS)
RTM_FORCE_NOINLINE float RTM_SIMD_CALL vector_dot_neon64(vector4f_arg0 lhs, vector4f_arg1 rhs) RTM_NO_EXCEPT
{
#if defined(RTM_IMPL_VADDVQ_SUPPORTED)
	// Compiles down to:
	// fmul.4s v0, v0, v1
	// faddp.4s v0, v0, v0
	// faddp.2s s0, v0
	float32x4_t x2_y2_z2_w2 = vmulq_f32(lhs, rhs);
	return vaddvq_f32(x2_y2_z2_w2);
#else
	(void)lhs;
	(void)rhs;
	return 0.0F;
#endif
}
#endif

#if defined(RTM_SSE2_INTRINSICS)
RTM_FORCE_NOINLINE float RTM_SIMD_CALL vector_dot_sse2(vector4f_arg0 lhs, vector4f_arg1 rhs) RTM_NO_EXCEPT
{
	__m128 x2_y2_z2_w2 = _mm_mul_ps(lhs, rhs);
	__m128 z2_w2_0_0 = _mm_shuffle_ps(x2_y2_z2_w2, x2_y2_z2_w2, _MM_SHUFFLE(0, 0, 3, 2));
	__m128 x2z2_y2w2_0_0 = _mm_add_ps(x2_y2_z2_w2, z2_w2_0_0);
	__m128 y2w2_0_0_0 = _mm_shuffle_ps(x2z2_y2w2_0_0, x2z2_y2w2_0_0, _MM_SHUFFLE(0, 0, 0, 1));
	__m128 x2y2z2w2_0_0_0 = _mm_add_ps(x2z2_y2w2_0_0, y2w2_0_0_0);
	return _mm_cvtss_f32(x2y2z2w2_0_0_0);
}
#endif

#if defined(RTM_SSE4_INTRINSICS)
// It appears that dpps is slower on Zen2 compared to the SSE2 implementation
// This could be because despite having more instructions, the SSE2 impl manages
// to pipeline them better.
RTM_FORCE_NOINLINE float RTM_SIMD_CALL vector_dot_sse4(vector4f_arg0 lhs, vector4f_arg1 rhs) RTM_NO_EXCEPT
{
	return _mm_cvtss_f32(_mm_dp_ps(lhs, rhs, 0xFF));
}
#endif

static void bm_vector_dot_scalar(benchmark::State& state)
{
	vector4f v0 = vector_set(-1.0F, 1.0F, -2.0F, -123.134F);
	vector4f v1 = vector_set(-1.021F, 1.120F, -2.0331F, -1231.134F);
	vector4f v2 = vector_set(-11.0F, 1.330F, -21.50F, -1253.134F);
	vector4f v3 = vector_set(0.15F, 115.60F, 21.0221F, 123.13134F);

	float sum0 = 0.0F;
	float sum1 = 0.0F;
	float sum2 = 0.0F;
	float sum3 = 0.0F;

	for (auto _ : state)
	{
		sum0 += vector_dot_scalar(v0, v1);
		sum1 += vector_dot_scalar(v1, v2);
		sum2 += vector_dot_scalar(v2, v3);
		sum3 += vector_dot_scalar(v3, v0);
	}

	benchmark::DoNotOptimize(sum0);
	benchmark::DoNotOptimize(sum1);
	benchmark::DoNotOptimize(sum2);
	benchmark::DoNotOptimize(sum3);
}

BENCHMARK(bm_vector_dot_scalar);

#if defined(RTM_NEON_INTRINSICS)
static void bm_vector_dot_neon(benchmark::State& state)
{
	vector4f v0 = vector_set(-1.0F, 1.0F, -2.0F, -123.134F);
	vector4f v1 = vector_set(-1.021F, 1.120F, -2.0331F, -1231.134F);
	vector4f v2 = vector_set(-11.0F, 1.330F, -21.50F, -1253.134F);
	vector4f v3 = vector_set(0.15F, 115.60F, 21.0221F, 123.13134F);

	float sum0 = 0.0F;
	float sum1 = 0.0F;
	float sum2 = 0.0F;
	float sum3 = 0.0F;

	for (auto _ : state)
	{
		sum0 += vector_dot_neon(v0, v1);
		sum1 += vector_dot_neon(v1, v2);
		sum2 += vector_dot_neon(v2, v3);
		sum3 += vector_dot_neon(v3, v0);
	}

	benchmark::DoNotOptimize(sum0);
	benchmark::DoNotOptimize(sum1);
	benchmark::DoNotOptimize(sum2);
	benchmark::DoNotOptimize(sum3);
}

BENCHMARK(bm_vector_dot_neon);
#endif

#if defined(RTM_NEON64_INTRINSICS)
static void bm_vector_dot_neon64(benchmark::State& state)
{
	vector4f v0 = vector_set(-1.0F, 1.0F, -2.0F, -123.134F);
	vector4f v1 = vector_set(-1.021F, 1.120F, -2.0331F, -1231.134F);
	vector4f v2 = vector_set(-11.0F, 1.330F, -21.50F, -1253.134F);
	vector4f v3 = vector_set(0.15F, 115.60F, 21.0221F, 123.13134F);

	float sum0 = 0.0F;
	float sum1 = 0.0F;
	float sum2 = 0.0F;
	float sum3 = 0.0F;

	for (auto _ : state)
	{
		sum0 += vector_dot_neon64(v0, v1);
		sum1 += vector_dot_neon64(v1, v2);
		sum2 += vector_dot_neon64(v2, v3);
		sum3 += vector_dot_neon64(v3, v0);
	}

	benchmark::DoNotOptimize(sum0);
	benchmark::DoNotOptimize(sum1);
	benchmark::DoNotOptimize(sum2);
	benchmark::DoNotOptimize(sum3);
}

BENCHMARK(bm_vector_dot_neon64);
#endif

#if defined(RTM_SSE2_INTRINSICS)
static void bm_vector_dot_sse2(benchmark::State& state)
{
	vector4f v0 = vector_set(-1.0F, 1.0F, -2.0F, -123.134F);
	vector4f v1 = vector_set(-1.021F, 1.120F, -2.0331F, -1231.134F);
	vector4f v2 = vector_set(-11.0F, 1.330F, -21.50F, -1253.134F);
	vector4f v3 = vector_set(0.15F, 115.60F, 21.0221F, 123.13134F);

	float sum0 = 0.0F;
	float sum1 = 0.0F;
	float sum2 = 0.0F;
	float sum3 = 0.0F;

	for (auto _ : state)
	{
		sum0 += vector_dot_sse2(v0, v1);
		sum1 += vector_dot_sse2(v1, v2);
		sum2 += vector_dot_sse2(v2, v3);
		sum3 += vector_dot_sse2(v3, v0);
	}

	benchmark::DoNotOptimize(sum0);
	benchmark::DoNotOptimize(sum1);
	benchmark::DoNotOptimize(sum2);
	benchmark::DoNotOptimize(sum3);
}

BENCHMARK(bm_vector_dot_sse2);
#endif

#if defined(RTM_SSE4_INTRINSICS)
static void bm_vector_dot_sse4(benchmark::State& state)
{
	vector4f v0 = vector_set(-1.0F, 1.0F, -2.0F, -123.134F);
	vector4f v1 = vector_set(-1.021F, 1.120F, -2.0331F, -1231.134F);
	vector4f v2 = vector_set(-11.0F, 1.330F, -21.50F, -1253.134F);
	vector4f v3 = vector_set(0.15F, 115.60F, 21.0221F, 123.13134F);

	float sum0 = 0.0F;
	float sum1 = 0.0F;
	float sum2 = 0.0F;
	float sum3 = 0.0F;

	for (auto _ : state)
	{
		sum0 += vector_dot_sse4(v0, v1);
		sum1 += vector_dot_sse4(v1, v2);
		sum2 += vector_dot_sse4(v2, v3);
		sum3 += vector_dot_sse4(v3, v0);
	}

	benchmark::DoNotOptimize(sum0);
	benchmark::DoNotOptimize(sum1);
	benchmark::DoNotOptimize(sum2);
	benchmark::DoNotOptimize(sum3);
}

BENCHMARK(bm_vector_dot_sse4);
#endif

```

`tools/bench/sources/bench_vector_dot3.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2023 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <benchmark/benchmark.h>

#include <rtm/vector4f.h>

using namespace rtm;

RTM_FORCE_NOINLINE float RTM_SIMD_CALL vector_dot3_scalar(vector4f_arg0 lhs, vector4f_arg1 rhs) RTM_NO_EXCEPT
{
	// Compiles down to this with ARM64:
	// fmul.4s v0, v0, v1
	// dup.4s v1, v0[1]
	// fadd.4s v1, v0, v1
	// dup.4s v0, v0[2]
	// fadd.4s v0, v0, v1
	float lhs_x = vector_get_x(lhs);
	float lhs_y = vector_get_y(lhs);
	float lhs_z = vector_get_z(lhs);

	float rhs_x = vector_get_x(rhs);
	float rhs_y = vector_get_y(rhs);
	float rhs_z = vector_get_z(rhs);

	return (lhs_x * rhs_x) + (lhs_y * rhs_y) + (lhs_z * rhs_z);
}

#if defined(RTM_NEON_INTRINSICS)
RTM_FORCE_NOINLINE float RTM_SIMD_CALL vector_dot3_neon(vector4f_arg0 lhs, vector4f_arg1 rhs) RTM_NO_EXCEPT
{
	// Compiles down to this with ARM64:
	// fmul.4s v0, v0, v1
	// faddp.2s v1, v0, v0
	// ext.16b v0, v0, v0, #0x8
	// fadd.2s v0, v0, v1
	float32x4_t x2_y2_z2_w2 = vmulq_f32(lhs, rhs);
	float32x2_t x2_y2 = vget_low_f32(x2_y2_z2_w2);
	float32x2_t z2_w2 = vget_high_f32(x2_y2_z2_w2);
	float32x2_t x2y2_x2y2 = vpadd_f32(x2_y2, x2_y2);
	float32x2_t z2_z2 = vdup_lane_f32(z2_w2, 0);
	float32x2_t x2y2z2_x2y2z2 = vadd_f32(x2y2_x2y2, z2_z2);
	return vget_lane_f32(x2y2z2_x2y2z2, 0);
}
#endif

#if defined(RTM_NEON64_INTRINSICS)
RTM_FORCE_NOINLINE float RTM_SIMD_CALL vector_dot3_neon64(vector4f_arg0 lhs, vector4f_arg1 rhs) RTM_NO_EXCEPT
{
#if defined(RTM_IMPL_VADDVQ_SUPPORTED)
	// Compiles down to:
	// fmul.4s v0, v0, v1
	// mov.s  v0[3], wzr
	// faddp.4s v0, v0, v0
	// faddp.2s s0, v0
	float32x4_t x2_y2_z2_w2 = vmulq_f32(lhs, rhs);
	float32x4_t x2_y2_z2 = vsetq_lane_f32(0.0F, x2_y2_z2_w2, 3);
	return vaddvq_f32(x2_y2_z2);
#else
	(void)lhs;
	(void)rhs;
	return 0.0F;
#endif
}
#endif

#if defined(RTM_SSE2_INTRINSICS)
RTM_FORCE_NOINLINE float RTM_SIMD_CALL vector_dot3_sse2(vector4f_arg0 lhs, vector4f_arg1 rhs) RTM_NO_EXCEPT
{
	__m128 x2_y2_z2_w2 = _mm_mul_ps(lhs, rhs);
	__m128 y2_0_0_0 = _mm_shuffle_ps(x2_y2_z2_w2, x2_y2_z2_w2, _MM_SHUFFLE(0, 0, 0, 1));
	__m128 x2y2_0_0_0 = _mm_add_ss(x2_y2_z2_w2, y2_0_0_0);
	__m128 z2_0_0_0 = _mm_shuffle_ps(x2_y2_z2_w2, x2_y2_z2_w2, _MM_SHUFFLE(0, 0, 0, 2));
	__m128 x2y2z2_0_0_0 = _mm_add_ss(x2y2_0_0_0, z2_0_0_0);
	return _mm_cvtss_f32(x2y2z2_0_0_0);
}
#endif

#if defined(RTM_SSE4_INTRINSICS)
RTM_FORCE_NOINLINE float RTM_SIMD_CALL vector_dot3_sse4(vector4f_arg0 lhs, vector4f_arg1 rhs) RTM_NO_EXCEPT
{
	return _mm_cvtss_f32(_mm_dp_ps(lhs, rhs, 0x7F));
}
#endif

static void bm_vector_dot3_scalar(benchmark::State& state)
{
	vector4f v0 = vector_set(-1.0F, 1.0F, -2.0F, -123.134F);
	vector4f v1 = vector_set(-1.021F, 1.120F, -2.0331F, -1231.134F);
	vector4f v2 = vector_set(-11.0F, 1.330F, -21.50F, -1253.134F);
	vector4f v3 = vector_set(0.15F, 115.60F, 21.0221F, 123.13134F);

	float sum0 = 0.0F;
	float sum1 = 0.0F;
	float sum2 = 0.0F;
	float sum3 = 0.0F;

	for (auto _ : state)
	{
		sum0 += vector_dot3_scalar(v0, v1);
		sum1 += vector_dot3_scalar(v1, v2);
		sum2 += vector_dot3_scalar(v2, v3);
		sum3 += vector_dot3_scalar(v3, v0);
	}

	benchmark::DoNotOptimize(sum0);
	benchmark::DoNotOptimize(sum1);
	benchmark::DoNotOptimize(sum2);
	benchmark::DoNotOptimize(sum3);
}

BENCHMARK(bm_vector_dot3_scalar);

#if defined(RTM_NEON_INTRINSICS)
static void bm_vector_dot3_neon(benchmark::State& state)
{
	vector4f v0 = vector_set(-1.0F, 1.0F, -2.0F, -123.134F);
	vector4f v1 = vector_set(-1.021F, 1.120F, -2.0331F, -1231.134F);
	vector4f v2 = vector_set(-11.0F, 1.330F, -21.50F, -1253.134F);
	vector4f v3 = vector_set(0.15F, 115.60F, 21.0221F, 123.13134F);

	float sum0 = 0.0F;
	float sum1 = 0.0F;
	float sum2 = 0.0F;
	float sum3 = 0.0F;

	for (auto _ : state)
	{
		sum0 += vector_dot3_neon(v0, v1);
		sum1 += vector_dot3_neon(v1, v2);
		sum2 += vector_dot3_neon(v2, v3);
		sum3 += vector_dot3_neon(v3, v0);
	}

	benchmark::DoNotOptimize(sum0);
	benchmark::DoNotOptimize(sum1);
	benchmark::DoNotOptimize(sum2);
	benchmark::DoNotOptimize(sum3);
}

BENCHMARK(bm_vector_dot3_neon);
#endif

#if defined(RTM_NEON64_INTRINSICS)
static void bm_vector_dot3_neon64(benchmark::State& state)
{
	vector4f v0 = vector_set(-1.0F, 1.0F, -2.0F, -123.134F);
	vector4f v1 = vector_set(-1.021F, 1.120F, -2.0331F, -1231.134F);
	vector4f v2 = vector_set(-11.0F, 1.330F, -21.50F, -1253.134F);
	vector4f v3 = vector_set(0.15F, 115.60F, 21.0221F, 123.13134F);

	float sum0 = 0.0F;
	float sum1 = 0.0F;
	float sum2 = 0.0F;
	float sum3 = 0.0F;

	for (auto _ : state)
	{
		sum0 += vector_dot3_neon64(v0, v1);
		sum1 += vector_dot3_neon64(v1, v2);
		sum2 += vector_dot3_neon64(v2, v3);
		sum3 += vector_dot3_neon64(v3, v0);
	}

	benchmark::DoNotOptimize(sum0);
	benchmark::DoNotOptimize(sum1);
	benchmark::DoNotOptimize(sum2);
	benchmark::DoNotOptimize(sum3);
}

BENCHMARK(bm_vector_dot3_neon64);
#endif

#if defined(RTM_SSE2_INTRINSICS)
static void bm_vector_dot3_sse2(benchmark::State& state)
{
	vector4f v0 = vector_set(-1.0F, 1.0F, -2.0F, -123.134F);
	vector4f v1 = vector_set(-1.021F, 1.120F, -2.0331F, -1231.134F);
	vector4f v2 = vector_set(-11.0F, 1.330F, -21.50F, -1253.134F);
	vector4f v3 = vector_set(0.15F, 115.60F, 21.0221F, 123.13134F);

	float sum0 = 0.0F;
	float sum1 = 0.0F;
	float sum2 = 0.0F;
	float sum3 = 0.0F;

	for (auto _ : state)
	{
		sum0 += vector_dot3_sse2(v0, v1);
		sum1 += vector_dot3_sse2(v1, v2);
		sum2 += vector_dot3_sse2(v2, v3);
		sum3 += vector_dot3_sse2(v3, v0);
	}

	benchmark::DoNotOptimize(sum0);
	benchmark::DoNotOptimize(sum1);
	benchmark::DoNotOptimize(sum2);
	benchmark::DoNotOptimize(sum3);
}

BENCHMARK(bm_vector_dot3_sse2);
#endif

#if defined(RTM_SSE4_INTRINSICS)
static void bm_vector_dot3_sse4(benchmark::State& state)
{
	vector4f v0 = vector_set(-1.0F, 1.0F, -2.0F, -123.134F);
	vector4f v1 = vector_set(-1.021F, 1.120F, -2.0331F, -1231.134F);
	vector4f v2 = vector_set(-11.0F, 1.330F, -21.50F, -1253.134F);
	vector4f v3 = vector_set(0.15F, 115.60F, 21.0221F, 123.13134F);

	float sum0 = 0.0F;
	float sum1 = 0.0F;
	float sum2 = 0.0F;
	float sum3 = 0.0F;

	for (auto _ : state)
	{
		sum0 += vector_dot3_sse4(v0, v1);
		sum1 += vector_dot3_sse4(v1, v2);
		sum2 += vector_dot3_sse4(v2, v3);
		sum3 += vector_dot3_sse4(v3, v0);
	}

	benchmark::DoNotOptimize(sum0);
	benchmark::DoNotOptimize(sum1);
	benchmark::DoNotOptimize(sum2);
	benchmark::DoNotOptimize(sum3);
}

BENCHMARK(bm_vector_dot3_sse4);
#endif

```

`tools/bench/sources/bench_vector_dot3_v.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2023 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <benchmark/benchmark.h>

#include <rtm/vector4f.h>

using namespace rtm;

RTM_FORCE_NOINLINE vector4f RTM_SIMD_CALL vector_dot3_scalar_v(vector4f_arg0 lhs, vector4f_arg1 rhs) RTM_NO_EXCEPT
{
	// Compiles down to this with ARM64:
	// fmul.4s v0, v0, v1
	// dup.4s v1, v0[1]
	// fadd.4s v1, v0, v1
	// dup.4s v0, v0[2]
	// fadd.4s v0, v0, v1
	// dup.4s v0, v0[0]
	float lhs_x = vector_get_x(lhs);
	float lhs_y = vector_get_y(lhs);
	float lhs_z = vector_get_z(lhs);

	float rhs_x = vector_get_x(rhs);
	float rhs_y = vector_get_y(rhs);
	float rhs_z = vector_get_z(rhs);

	return vector_set((lhs_x * rhs_x) + (lhs_y * rhs_y) + (lhs_z * rhs_z));
}

#if defined(RTM_NEON_INTRINSICS)
RTM_FORCE_NOINLINE vector4f RTM_SIMD_CALL vector_dot3_neon_v(vector4f_arg0 lhs, vector4f_arg1 rhs) RTM_NO_EXCEPT
{
	// Compiles down to this with ARM64:
	// fmul.4s v0, v0, v1
	// faddp.2s v1, v0, v0
	// ext.16b v0, v0, v0, #0x8
	// fadd.2s v0, v0, v1
	// dup.4s v0, v0[0]
	float32x4_t x2_y2_z2_w2 = vmulq_f32(lhs, rhs);
	float32x2_t x2_y2 = vget_low_f32(x2_y2_z2_w2);
	float32x2_t z2_w2 = vget_high_f32(x2_y2_z2_w2);
	float32x2_t x2y2_x2y2 = vpadd_f32(x2_y2, x2_y2);
	float32x2_t z2_z2 = vdup_lane_f32(z2_w2, 0);
	float32x2_t x2y2z2_x2y2z2 = vadd_f32(x2y2_x2y2, z2_z2);
	return vdupq_lane_f32(x2y2z2_x2y2z2, 0);
}
#endif

#if defined(RTM_NEON64_INTRINSICS)
RTM_FORCE_NOINLINE vector4f RTM_SIMD_CALL vector_dot3_neon64_v(vector4f_arg0 lhs, vector4f_arg1 rhs) RTM_NO_EXCEPT
{
#if defined(RTM_IMPL_VADDVQ_SUPPORTED)
	// Compiles down to:
	// fmul.4s v0, v0, v1
	// mov.s  v0[3], wzr
	// faddp.4s v0, v0, v0
	// faddp.2s s0, v0
	// dup.4s v0, v0[0]
	float32x4_t x2_y2_z2_w2 = vmulq_f32(lhs, rhs);
	float32x4_t x2_y2_z2 = vsetq_lane_f32(0.0F, x2_y2_z2_w2, 3);
	return vdupq_n_f32(vaddvq_f32(x2_y2_z2));
#else
	(void)rhs;
	return lhs;
#endif
}

RTM_FORCE_NOINLINE vector4f RTM_SIMD_CALL vector_dot3_neon64_paddq_v(vector4f_arg0 lhs, vector4f_arg1 rhs) RTM_NO_EXCEPT
{
	// Compiles down to:
	// fmul.4s v0, v0, v1
	// mov.s  v0[3], wzr
	// faddp.4s v0, v0, v0
	// faddp.4s v0, v0, v0
	float32x4_t x2_y2_z2_w2 = vmulq_f32(lhs, rhs);
	float32x4_t x2_y2_z2 = vsetq_lane_f32(0.0F, x2_y2_z2_w2, 3);
	float32x4_t x2y2_z2_x2y2_z2 = vpaddq_f32(x2_y2_z2, x2_y2_z2);
	float32x4_t x2y2z2_x2y2z2_x2y2z2_x2y2z2 = vpaddq_f32(x2y2_z2_x2y2_z2, x2y2_z2_x2y2_z2);
	return x2y2z2_x2y2z2_x2y2z2_x2y2z2;
}
#endif

#if defined(RTM_SSE2_INTRINSICS)
RTM_FORCE_NOINLINE vector4f RTM_SIMD_CALL vector_dot3_sse2_v(vector4f_arg0 lhs, vector4f_arg1 rhs) RTM_NO_EXCEPT
{
	__m128 x2_y2_z2_w2 = _mm_mul_ps(lhs, rhs);
	__m128 y2_0_0_0 = _mm_shuffle_ps(x2_y2_z2_w2, x2_y2_z2_w2, _MM_SHUFFLE(0, 0, 0, 1));
	__m128 x2y2_0_0_0 = _mm_add_ss(x2_y2_z2_w2, y2_0_0_0);
	__m128 z2_0_0_0 = _mm_shuffle_ps(x2_y2_z2_w2, x2_y2_z2_w2, _MM_SHUFFLE(0, 0, 0, 2));
	__m128 x2y2z2_0_0_0 = _mm_add_ss(x2y2_0_0_0, z2_0_0_0);
	return _mm_shuffle_ps(x2y2z2_0_0_0, x2y2z2_0_0_0, _MM_SHUFFLE(0, 0, 0, 0));
}
#endif

#if defined(RTM_SSE4_INTRINSICS)
RTM_FORCE_NOINLINE vector4f RTM_SIMD_CALL vector_dot3_sse4_v(vector4f_arg0 lhs, vector4f_arg1 rhs) RTM_NO_EXCEPT
{
	return _mm_dp_ps(lhs, rhs, 0xFF);
}
#endif

static void bm_vector_dot3_scalar_v(benchmark::State& state)
{
	vector4f v0 = vector_set(-1.0F, 1.0F, -2.0F, -123.134F);
	vector4f v1 = vector_set(-1.021F, 1.120F, -2.0331F, -1231.134F);
	vector4f v2 = vector_set(-11.0F, 1.330F, -21.50F, -1253.134F);
	vector4f v3 = vector_set(0.15F, 115.60F, 21.0221F, 123.13134F);

	vector4f sum0 = vector_zero();
	vector4f sum1 = vector_zero();
	vector4f sum2 = vector_zero();
	vector4f sum3 = vector_zero();

	for (auto _ : state)
	{
		sum0 = vector_add(sum0, vector_dot3_scalar_v(v0, v1));
		sum1 = vector_add(sum1, vector_dot3_scalar_v(v1, v2));
		sum2 = vector_add(sum2, vector_dot3_scalar_v(v2, v3));
		sum3 = vector_add(sum3, vector_dot3_scalar_v(v3, v0));
	}

	benchmark::DoNotOptimize(sum0);
	benchmark::DoNotOptimize(sum1);
	benchmark::DoNotOptimize(sum2);
	benchmark::DoNotOptimize(sum3);
}

BENCHMARK(bm_vector_dot3_scalar_v);

#if defined(RTM_NEON_INTRINSICS)
static void bm_vector_dot3_neon_v(benchmark::State& state)
{
	vector4f v0 = vector_set(-1.0F, 1.0F, -2.0F, -123.134F);
	vector4f v1 = vector_set(-1.021F, 1.120F, -2.0331F, -1231.134F);
	vector4f v2 = vector_set(-11.0F, 1.330F, -21.50F, -1253.134F);
	vector4f v3 = vector_set(0.15F, 115.60F, 21.0221F, 123.13134F);

	vector4f sum0 = vector_zero();
	vector4f sum1 = vector_zero();
	vector4f sum2 = vector_zero();
	vector4f sum3 = vector_zero();

	for (auto _ : state)
	{
		sum0 = vector_add(sum0, vector_dot3_neon_v(v0, v1));
		sum1 = vector_add(sum1, vector_dot3_neon_v(v1, v2));
		sum2 = vector_add(sum2, vector_dot3_neon_v(v2, v3));
		sum3 = vector_add(sum3, vector_dot3_neon_v(v3, v0));
	}

	benchmark::DoNotOptimize(sum0);
	benchmark::DoNotOptimize(sum1);
	benchmark::DoNotOptimize(sum2);
	benchmark::DoNotOptimize(sum3);
}

BENCHMARK(bm_vector_dot3_neon_v);
#endif

#if defined(RTM_NEON64_INTRINSICS)
static void bm_vector_dot3_neon64_v(benchmark::State& state)
{
	vector4f v0 = vector_set(-1.0F, 1.0F, -2.0F, -123.134F);
	vector4f v1 = vector_set(-1.021F, 1.120F, -2.0331F, -1231.134F);
	vector4f v2 = vector_set(-11.0F, 1.330F, -21.50F, -1253.134F);
	vector4f v3 = vector_set(0.15F, 115.60F, 21.0221F, 123.13134F);

	vector4f sum0 = vector_zero();
	vector4f sum1 = vector_zero();
	vector4f sum2 = vector_zero();
	vector4f sum3 = vector_zero();

	for (auto _ : state)
	{
		sum0 = vector_add(sum0, vector_dot3_neon64_v(v0, v1));
		sum1 = vector_add(sum1, vector_dot3_neon64_v(v1, v2));
		sum2 = vector_add(sum2, vector_dot3_neon64_v(v2, v3));
		sum3 = vector_add(sum3, vector_dot3_neon64_v(v3, v0));
	}

	benchmark::DoNotOptimize(sum0);
	benchmark::DoNotOptimize(sum1);
	benchmark::DoNotOptimize(sum2);
	benchmark::DoNotOptimize(sum3);
}

BENCHMARK(bm_vector_dot3_neon64_v);

static void bm_vector_dot3_neon64_paddq_v(benchmark::State& state)
{
	vector4f v0 = vector_set(-1.0F, 1.0F, -2.0F, -123.134F);
	vector4f v1 = vector_set(-1.021F, 1.120F, -2.0331F, -1231.134F);
	vector4f v2 = vector_set(-11.0F, 1.330F, -21.50F, -1253.134F);
	vector4f v3 = vector_set(0.15F, 115.60F, 21.0221F, 123.13134F);

	vector4f sum0 = vector_zero();
	vector4f sum1 = vector_zero();
	vector4f sum2 = vector_zero();
	vector4f sum3 = vector_zero();

	for (auto _ : state)
	{
		sum0 = vector_add(sum0, vector_dot3_neon64_paddq_v(v0, v1));
		sum1 = vector_add(sum1, vector_dot3_neon64_paddq_v(v1, v2));
		sum2 = vector_add(sum2, vector_dot3_neon64_paddq_v(v2, v3));
		sum3 = vector_add(sum3, vector_dot3_neon64_paddq_v(v3, v0));
	}

	benchmark::DoNotOptimize(sum0);
	benchmark::DoNotOptimize(sum1);
	benchmark::DoNotOptimize(sum2);
	benchmark::DoNotOptimize(sum3);
}

BENCHMARK(bm_vector_dot3_neon64_paddq_v);
#endif

#if defined(RTM_SSE2_INTRINSICS)
static void bm_vector_dot3_sse2_v(benchmark::State& state)
{
	vector4f v0 = vector_set(-1.0F, 1.0F, -2.0F, -123.134F);
	vector4f v1 = vector_set(-1.021F, 1.120F, -2.0331F, -1231.134F);
	vector4f v2 = vector_set(-11.0F, 1.330F, -21.50F, -1253.134F);
	vector4f v3 = vector_set(0.15F, 115.60F, 21.0221F, 123.13134F);

	vector4f sum0 = vector_zero();
	vector4f sum1 = vector_zero();
	vector4f sum2 = vector_zero();
	vector4f sum3 = vector_zero();

	for (auto _ : state)
	{
		sum0 = vector_add(sum0, vector_dot3_sse2_v(v0, v1));
		sum1 = vector_add(sum1, vector_dot3_sse2_v(v1, v2));
		sum2 = vector_add(sum2, vector_dot3_sse2_v(v2, v3));
		sum3 = vector_add(sum3, vector_dot3_sse2_v(v3, v0));
	}

	benchmark::DoNotOptimize(sum0);
	benchmark::DoNotOptimize(sum1);
	benchmark::DoNotOptimize(sum2);
	benchmark::DoNotOptimize(sum3);
}

BENCHMARK(bm_vector_dot3_sse2_v);
#endif

#if defined(RTM_SSE4_INTRINSICS)
static void bm_vector_dot3_sse4_v(benchmark::State& state)
{
	vector4f v0 = vector_set(-1.0F, 1.0F, -2.0F, -123.134F);
	vector4f v1 = vector_set(-1.021F, 1.120F, -2.0331F, -1231.134F);
	vector4f v2 = vector_set(-11.0F, 1.330F, -21.50F, -1253.134F);
	vector4f v3 = vector_set(0.15F, 115.60F, 21.0221F, 123.13134F);

	vector4f sum0 = vector_zero();
	vector4f sum1 = vector_zero();
	vector4f sum2 = vector_zero();
	vector4f sum3 = vector_zero();

	for (auto _ : state)
	{
		sum0 = vector_add(sum0, vector_dot3_sse4_v(v0, v1));
		sum1 = vector_add(sum1, vector_dot3_sse4_v(v1, v2));
		sum2 = vector_add(sum2, vector_dot3_sse4_v(v2, v3));
		sum3 = vector_add(sum3, vector_dot3_sse4_v(v3, v0));
	}

	benchmark::DoNotOptimize(sum0);
	benchmark::DoNotOptimize(sum1);
	benchmark::DoNotOptimize(sum2);
	benchmark::DoNotOptimize(sum3);
}

BENCHMARK(bm_vector_dot3_sse4_v);
#endif

```

`tools/bench/sources/bench_vector_dot_v.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2023 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <benchmark/benchmark.h>

#include <rtm/vector4f.h>

using namespace rtm;

RTM_FORCE_NOINLINE vector4f RTM_SIMD_CALL vector_dot_scalar_v(vector4f_arg0 lhs, vector4f_arg1 rhs) RTM_NO_EXCEPT
{
	float lhs_x = vector_get_x(lhs);
	float lhs_y = vector_get_y(lhs);
	float lhs_z = vector_get_z(lhs);
	float lhs_w = vector_get_w(lhs);

	float rhs_x = vector_get_x(rhs);
	float rhs_y = vector_get_y(rhs);
	float rhs_z = vector_get_z(rhs);
	float rhs_w = vector_get_w(rhs);

	return vector_set((lhs_x * rhs_x) + (lhs_y * rhs_y) + (lhs_z * rhs_z) + (lhs_w * rhs_w));
}

#if defined(RTM_NEON_INTRINSICS)
RTM_FORCE_NOINLINE vector4f RTM_SIMD_CALL vector_dot_neon_v(vector4f_arg0 lhs, vector4f_arg1 rhs) RTM_NO_EXCEPT
{
	// Compiles down to this with ARM64:
	// fmul.4s v0, v0, v1
	// ext.16b v1, v0, v0, #0x8
	// fadd.2s v0, v0, v1
	// faddp.2s v0, v0, v0
	// mov.d  v0[1], v0[0]
	float32x4_t x2_y2_z2_w2 = vmulq_f32(lhs, rhs);
	float32x2_t x2_y2 = vget_low_f32(x2_y2_z2_w2);
	float32x2_t z2_w2 = vget_high_f32(x2_y2_z2_w2);
	float32x2_t x2z2_y2w2 = vadd_f32(x2_y2, z2_w2);
	float32x2_t x2y2z2w2 = vpadd_f32(x2z2_y2w2, x2z2_y2w2);
	return vcombine_f32(x2y2z2w2, x2y2z2w2);
}

RTM_FORCE_NOINLINE vector4f RTM_SIMD_CALL vector_dot_neon_fma_v(vector4f_arg0 lhs, vector4f_arg1 rhs) RTM_NO_EXCEPT
{
	// Compiles down to this with ARM64:
	// ext.16b v2, v0, v0, #0x8
	// ext.16b v3, v1, v1, #0x8
	// fmul.2s v0, v0, v1
	// fmla.2s v0, v3, v2
	// faddp.2s v0, v0, v0
	// mov.d  v0[1], v0[0]
	float32x2_t lhs_x_y = vget_low_f32(lhs);
	float32x2_t lhs_z_w = vget_high_f32(lhs);
	float32x2_t rhs_x_y = vget_low_f32(rhs);
	float32x2_t rhs_z_w = vget_high_f32(rhs);
	float32x2_t x2_y2 = vmul_f32(lhs_x_y, rhs_x_y);
	float32x2_t x2z2_y2w2 = RTM_VECTOR2F_MULV_ADD(x2_y2, lhs_z_w, rhs_z_w);
	float32x2_t x2y2z2w2 = vpadd_f32(x2z2_y2w2, x2z2_y2w2);
	return vcombine_f32(x2y2z2w2, x2y2z2w2);
}
#endif

#if defined(RTM_NEON64_INTRINSICS)
RTM_FORCE_NOINLINE vector4f RTM_SIMD_CALL vector_dot_neon64_v(vector4f_arg0 lhs, vector4f_arg1 rhs) RTM_NO_EXCEPT
{
#if defined(RTM_IMPL_VADDVQ_SUPPORTED)
	// Compiles down to:
	// fmul.4s v0, v0, v1
	// faddp.4s v0, v0, v0
	// faddp.2s s0, v0
	// dup.4s v0, v0[0]
	float32x4_t x2_y2_z2_w2 = vmulq_f32(lhs, rhs);
	return vdupq_n_f32(vaddvq_f32(x2_y2_z2_w2));
#else
	(void)rhs;
	return lhs;
#endif
}

RTM_FORCE_NOINLINE vector4f RTM_SIMD_CALL vector_dot_neon64_paddq_v(vector4f_arg0 lhs, vector4f_arg1 rhs) RTM_NO_EXCEPT
{
	// Compiles down to:
	// fmul.4s v0, v0, v1
	// faddp.4s v0, v0, v0
	// faddp.4s v0, v0, v0
	float32x4_t x2_y2_z2_w2 = vmulq_f32(lhs, rhs);
	float32x4_t x2y2_z2w2_x2y2_z2w2 = vpaddq_f32(x2_y2_z2_w2, x2_y2_z2_w2);
	float32x4_t x2y2z2w2_x2y2z2w2_x2y2z2w2_x2y2z2w2 = vpaddq_f32(x2y2_z2w2_x2y2_z2w2, x2y2_z2w2_x2y2_z2w2);
	return x2y2z2w2_x2y2z2w2_x2y2z2w2_x2y2z2w2;
}
#endif

#if defined(RTM_SSE2_INTRINSICS)
RTM_FORCE_NOINLINE vector4f RTM_SIMD_CALL vector_dot_sse2_v(vector4f_arg0 lhs, vector4f_arg1 rhs) RTM_NO_EXCEPT
{
	__m128 x2_y2_z2_w2 = _mm_mul_ps(lhs, rhs);
	__m128 z2_w2_0_0 = _mm_shuffle_ps(x2_y2_z2_w2, x2_y2_z2_w2, _MM_SHUFFLE(0, 0, 3, 2));
	__m128 x2z2_y2w2_0_0 = _mm_add_ps(x2_y2_z2_w2, z2_w2_0_0);
	__m128 y2w2_0_0_0 = _mm_shuffle_ps(x2z2_y2w2_0_0, x2z2_y2w2_0_0, _MM_SHUFFLE(0, 0, 0, 1));
	__m128 x2y2z2w2_0_0_0 = _mm_add_ps(x2z2_y2w2_0_0, y2w2_0_0_0);
	return _mm_shuffle_ps(x2y2z2w2_0_0_0, x2y2z2w2_0_0_0, _MM_SHUFFLE(0, 0, 0, 0));
}
#endif

#if defined(RTM_SSE4_INTRINSICS)
// SSE4 dot product instruction appears slower on Zen2
// On the hardware Github Actions run on, performance is identical with clang14
// Need to measure on Zen3 and Intel, if same speed as well, we should use dpps instruction
// since it leads to smaller assembly and improved inlining
RTM_FORCE_NOINLINE vector4f RTM_SIMD_CALL vector_dot_sse4_v(vector4f_arg0 lhs, vector4f_arg1 rhs) RTM_NO_EXCEPT
{
	return _mm_dp_ps(lhs, rhs, 0xFF);
}
#endif

static void bm_vector_dot_scalar_v(benchmark::State& state)
{
	vector4f v0 = vector_set(-1.0F, 1.0F, -2.0F, -123.134F);
	vector4f v1 = vector_set(-1.021F, 1.120F, -2.0331F, -1231.134F);
	vector4f v2 = vector_set(-11.0F, 1.330F, -21.50F, -1253.134F);
	vector4f v3 = vector_set(0.15F, 115.60F, 21.0221F, 123.13134F);

	vector4f sum0 = vector_zero();
	vector4f sum1 = vector_zero();
	vector4f sum2 = vector_zero();
	vector4f sum3 = vector_zero();

	for (auto _ : state)
	{
		sum0 = vector_add(sum0, vector_dot_scalar_v(v0, v1));
		sum1 = vector_add(sum1, vector_dot_scalar_v(v1, v2));
		sum2 = vector_add(sum2, vector_dot_scalar_v(v2, v3));
		sum3 = vector_add(sum3, vector_dot_scalar_v(v3, v0));
	}

	benchmark::DoNotOptimize(sum0);
	benchmark::DoNotOptimize(sum1);
	benchmark::DoNotOptimize(sum2);
	benchmark::DoNotOptimize(sum3);
}

BENCHMARK(bm_vector_dot_scalar_v);

#if defined(RTM_NEON_INTRINSICS)
static void bm_vector_dot_neon_v(benchmark::State& state)
{
	vector4f v0 = vector_set(-1.0F, 1.0F, -2.0F, -123.134F);
	vector4f v1 = vector_set(-1.021F, 1.120F, -2.0331F, -1231.134F);
	vector4f v2 = vector_set(-11.0F, 1.330F, -21.50F, -1253.134F);
	vector4f v3 = vector_set(0.15F, 115.60F, 21.0221F, 123.13134F);

	vector4f sum0 = vector_zero();
	vector4f sum1 = vector_zero();
	vector4f sum2 = vector_zero();
	vector4f sum3 = vector_zero();

	for (auto _ : state)
	{
		sum0 = vector_add(sum0, vector_dot_neon_v(v0, v1));
		sum1 = vector_add(sum1, vector_dot_neon_v(v1, v2));
		sum2 = vector_add(sum2, vector_dot_neon_v(v2, v3));
		sum3 = vector_add(sum3, vector_dot_neon_v(v3, v0));
	}

	benchmark::DoNotOptimize(sum0);
	benchmark::DoNotOptimize(sum1);
	benchmark::DoNotOptimize(sum2);
	benchmark::DoNotOptimize(sum3);
}

BENCHMARK(bm_vector_dot_neon_v);

static void bm_vector_dot_neon_fma_v(benchmark::State& state)
{
	vector4f v0 = vector_set(-1.0F, 1.0F, -2.0F, -123.134F);
	vector4f v1 = vector_set(-1.021F, 1.120F, -2.0331F, -1231.134F);
	vector4f v2 = vector_set(-11.0F, 1.330F, -21.50F, -1253.134F);
	vector4f v3 = vector_set(0.15F, 115.60F, 21.0221F, 123.13134F);

	vector4f sum0 = vector_zero();
	vector4f sum1 = vector_zero();
	vector4f sum2 = vector_zero();
	vector4f sum3 = vector_zero();

	for (auto _ : state)
	{
		sum0 = vector_add(sum0, vector_dot_neon_fma_v(v0, v1));
		sum1 = vector_add(sum1, vector_dot_neon_fma_v(v1, v2));
		sum2 = vector_add(sum2, vector_dot_neon_fma_v(v2, v3));
		sum3 = vector_add(sum3, vector_dot_neon_fma_v(v3, v0));
	}

	benchmark::DoNotOptimize(sum0);
	benchmark::DoNotOptimize(sum1);
	benchmark::DoNotOptimize(sum2);
	benchmark::DoNotOptimize(sum3);
}

BENCHMARK(bm_vector_dot_neon_fma_v);
#endif

#if defined(RTM_NEON64_INTRINSICS)
static void bm_vector_dot_neon64_v(benchmark::State& state)
{
	vector4f v0 = vector_set(-1.0F, 1.0F, -2.0F, -123.134F);
	vector4f v1 = vector_set(-1.021F, 1.120F, -2.0331F, -1231.134F);
	vector4f v2 = vector_set(-11.0F, 1.330F, -21.50F, -1253.134F);
	vector4f v3 = vector_set(0.15F, 115.60F, 21.0221F, 123.13134F);

	vector4f sum0 = vector_zero();
	vector4f sum1 = vector_zero();
	vector4f sum2 = vector_zero();
	vector4f sum3 = vector_zero();

	for (auto _ : state)
	{
		sum0 = vector_add(sum0, vector_dot_neon64_v(v0, v1));
		sum1 = vector_add(sum1, vector_dot_neon64_v(v1, v2));
		sum2 = vector_add(sum2, vector_dot_neon64_v(v2, v3));
		sum3 = vector_add(sum3, vector_dot_neon64_v(v3, v0));
	}

	benchmark::DoNotOptimize(sum0);
	benchmark::DoNotOptimize(sum1);
	benchmark::DoNotOptimize(sum2);
	benchmark::DoNotOptimize(sum3);
}

BENCHMARK(bm_vector_dot_neon64_v);

static void bm_vector_dot_neon64_paddq_v(benchmark::State& state)
{
	vector4f v0 = vector_set(-1.0F, 1.0F, -2.0F, -123.134F);
	vector4f v1 = vector_set(-1.021F, 1.120F, -2.0331F, -1231.134F);
	vector4f v2 = vector_set(-11.0F, 1.330F, -21.50F, -1253.134F);
	vector4f v3 = vector_set(0.15F, 115.60F, 21.0221F, 123.13134F);

	vector4f sum0 = vector_zero();
	vector4f sum1 = vector_zero();
	vector4f sum2 = vector_zero();
	vector4f sum3 = vector_zero();

	for (auto _ : state)
	{
		sum0 = vector_add(sum0, vector_dot_neon64_paddq_v(v0, v1));
		sum1 = vector_add(sum1, vector_dot_neon64_paddq_v(v1, v2));
		sum2 = vector_add(sum2, vector_dot_neon64_paddq_v(v2, v3));
		sum3 = vector_add(sum3, vector_dot_neon64_paddq_v(v3, v0));
	}

	benchmark::DoNotOptimize(sum0);
	benchmark::DoNotOptimize(sum1);
	benchmark::DoNotOptimize(sum2);
	benchmark::DoNotOptimize(sum3);
}

BENCHMARK(bm_vector_dot_neon64_paddq_v);
#endif

#if defined(RTM_SSE2_INTRINSICS)
static void bm_vector_dot_sse2_v(benchmark::State& state)
{
	vector4f v0 = vector_set(-1.0F, 1.0F, -2.0F, -123.134F);
	vector4f v1 = vector_set(-1.021F, 1.120F, -2.0331F, -1231.134F);
	vector4f v2 = vector_set(-11.0F, 1.330F, -21.50F, -1253.134F);
	vector4f v3 = vector_set(0.15F, 115.60F, 21.0221F, 123.13134F);

	vector4f sum0 = vector_zero();
	vector4f sum1 = vector_zero();
	vector4f sum2 = vector_zero();
	vector4f sum3 = vector_zero();

	for (auto _ : state)
	{
		sum0 = vector_add(sum0, vector_dot_sse2_v(v0, v1));
		sum1 = vector_add(sum1, vector_dot_sse2_v(v1, v2));
		sum2 = vector_add(sum2, vector_dot_sse2_v(v2, v3));
		sum3 = vector_add(sum3, vector_dot_sse2_v(v3, v0));
	}

	benchmark::DoNotOptimize(sum0);
	benchmark::DoNotOptimize(sum1);
	benchmark::DoNotOptimize(sum2);
	benchmark::DoNotOptimize(sum3);
}

BENCHMARK(bm_vector_dot_sse2_v);
#endif

#if defined(RTM_SSE4_INTRINSICS)
static void bm_vector_dot_sse4_v(benchmark::State& state)
{
	vector4f v0 = vector_set(-1.0F, 1.0F, -2.0F, -123.134F);
	vector4f v1 = vector_set(-1.021F, 1.120F, -2.0331F, -1231.134F);
	vector4f v2 = vector_set(-11.0F, 1.330F, -21.50F, -1253.134F);
	vector4f v3 = vector_set(0.15F, 115.60F, 21.0221F, 123.13134F);

	vector4f sum0 = vector_zero();
	vector4f sum1 = vector_zero();
	vector4f sum2 = vector_zero();
	vector4f sum3 = vector_zero();

	for (auto _ : state)
	{
		sum0 = vector_add(sum0, vector_dot_sse4_v(v0, v1));
		sum1 = vector_add(sum1, vector_dot_sse4_v(v1, v2));
		sum2 = vector_add(sum2, vector_dot_sse4_v(v2, v3));
		sum3 = vector_add(sum3, vector_dot_sse4_v(v3, v0));
	}

	benchmark::DoNotOptimize(sum0);
	benchmark::DoNotOptimize(sum1);
	benchmark::DoNotOptimize(sum2);
	benchmark::DoNotOptimize(sum3);
}

BENCHMARK(bm_vector_dot_sse4_v);
#endif

```

`tools/bench/sources/bench_vector_floor.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <benchmark/benchmark.h>

#include <rtm/vector4f.h>

using namespace rtm;

RTM_FORCE_NOINLINE vector4f RTM_SIMD_CALL vector_floor_scalar(vector4f_arg0 input) RTM_NO_EXCEPT
{
	scalarf x = vector_get_x_as_scalar(input);
	scalarf y = vector_get_y_as_scalar(input);
	scalarf z = vector_get_z_as_scalar(input);
	scalarf w = vector_get_w_as_scalar(input);
	return vector_set(scalar_floor(x), scalar_floor(y), scalar_floor(z), scalar_floor(w));
}

#if defined(RTM_SSE2_INTRINSICS)
RTM_FORCE_NOINLINE vector4f RTM_SIMD_CALL vector_floor_sse2(vector4f_arg0 input) RTM_NO_EXCEPT
{
	// NaN, +- Infinity, and numbers larger or equal to 2^23 remain unchanged
	// since they have no fractional part.

	const __m128i abs_mask = _mm_set_epi32(0x7FFFFFFFULL, 0x7FFFFFFFULL, 0x7FFFFFFFULL, 0x7FFFFFFFULL);
	const __m128 fractional_limit = _mm_set_ps1(8388608.0F); // 2^23

	// Build our mask, larger values that have no fractional part, and infinities will be true
	// Smaller values and NaN will be false
	__m128 abs_input = _mm_and_ps(input, _mm_castsi128_ps(abs_mask));
	__m128 is_input_large = _mm_cmpge_ps(abs_input, fractional_limit);

	// Test if our input is NaN with (value != value), it is only true for NaN
	__m128 is_nan = _mm_cmpneq_ps(input, input);

	// Combine our masks to determine if we should return the original value
	__m128 use_original_input = _mm_or_ps(is_input_large, is_nan);

	// Convert to an integer and back. This does banker's rounding by default
	__m128 integer_part = _mm_cvtepi32_ps(_mm_cvtps_epi32(input));

	// Test if the returned value is greater than the original.
	// A negative input will round towards zero and be greater when we need it to be smaller.
	__m128 is_negative = _mm_cmpgt_ps(integer_part, input);

	// Convert our mask to a float, ~0 yields -1.0 since it is a valid signed integer
	// Positive values will yield a 0.0 bias
	__m128 bias = _mm_cvtepi32_ps(_mm_castps_si128(is_negative));

	// Add our bias to properly handle negative values
	integer_part = _mm_add_ps(integer_part, bias);

	return _mm_or_ps(_mm_and_ps(use_original_input, input), _mm_andnot_ps(use_original_input, integer_part));
}
#endif

#if defined(RTM_NEON_INTRINSICS)
RTM_FORCE_NOINLINE vector4f RTM_SIMD_CALL vector_floor_neon(vector4f_arg0 input) RTM_NO_EXCEPT
{
	// NaN, +- Infinity, and numbers larger or equal to 2^23 remain unchanged
	// since they have no fractional part.

	float32x4_t fractional_limit = vdupq_n_f32(8388608.0F); // 2^23

	// Build our mask, larger values that have no fractional part, and infinities will be true
	// Smaller values and NaN will be false
	float32x4_t abs_input = vabsq_f32(input);
	uint32x4_t is_input_large = vcgeq_f32(abs_input, fractional_limit);

	// Test if our input is NaN with (value != value), it is only true for NaN
	uint32x4_t is_nan = vmvnq_u32(vceqq_f32(input, input));

	// Combine our masks to determine if we should return the original value
	uint32x4_t use_original_input = vorrq_u32(is_input_large, is_nan);

	// Convert to an integer and back. This does banker's rounding by default
	float32x4_t integer_part = vcvtq_f32_s32(vcvtq_s32_f32(input));

	// Test if the returned value is greater than the original.
	// A negative input will round towards zero and be greater when we need it to be smaller.
	uint32x4_t is_negative = vcgtq_f32(integer_part, input);

	float32x4_t bias = vcvtq_f32_s32(is_negative);

	// Add our bias to properly handle negative values
	integer_part = vaddq_f32(integer_part, bias);

	return vbslq_f32(use_original_input, input, integer_part);
}

RTM_FORCE_NOINLINE vector4f RTM_SIMD_CALL vector_floor_neon_abs_cmp(vector4f_arg0 input) RTM_NO_EXCEPT
{
	// NaN, +- Infinity, and numbers larger or equal to 2^23 remain unchanged
	// since they have no fractional part.

	float32x4_t fractional_limit = vdupq_n_f32(8388608.0F); // 2^23

	// Build our mask, larger values that have no fractional part, and infinities will be true
	// Smaller values and NaN will be false
	uint32x4_t is_input_large = vcageq_f32(input, fractional_limit);

	// Test if our input is NaN with (value != value), it is only true for NaN
	uint32x4_t is_nan = vmvnq_u32(vceqq_f32(input, input));

	// Combine our masks to determine if we should return the original value
	uint32x4_t use_original_input = vorrq_u32(is_input_large, is_nan);

	// Convert to an integer and back. This does banker's rounding by default
	float32x4_t integer_part = vcvtq_f32_s32(vcvtq_s32_f32(input));

	// Test if the returned value is greater than the original.
	// A negative input will round towards zero and be greater when we need it to be smaller.
	uint32x4_t is_negative = vcgtq_f32(integer_part, input);

	float32x4_t bias = vcvtq_f32_s32(is_negative);

	// Add our bias to properly handle negative values
	integer_part = vaddq_f32(integer_part, bias);

	return vbslq_f32(use_original_input, input, integer_part);
}
#endif

static void bm_vector_floor_scalar(benchmark::State& state)
{
	vector4f v0 = vector_set(-123.134f);
	vector4f v1 = vector_set(123.134f);
	vector4f v2 = vector_set(-123.134f);
	vector4f v3 = vector_set(123.134f);
	vector4f v4 = vector_set(-123.134f);
	vector4f v5 = vector_set(123.134f);
	vector4f v6 = vector_set(-123.134f);
	vector4f v7 = vector_set(123.134f);

	for (auto _ : state)
	{
		v0 = vector_floor_scalar(v0);
		v1 = vector_floor_scalar(v1);
		v2 = vector_floor_scalar(v2);
		v3 = vector_floor_scalar(v3);
		v4 = vector_floor_scalar(v4);
		v5 = vector_floor_scalar(v5);
		v6 = vector_floor_scalar(v6);
		v7 = vector_floor_scalar(v7);
	}

	benchmark::DoNotOptimize(v0);
	benchmark::DoNotOptimize(v1);
	benchmark::DoNotOptimize(v2);
	benchmark::DoNotOptimize(v3);
	benchmark::DoNotOptimize(v4);
	benchmark::DoNotOptimize(v5);
	benchmark::DoNotOptimize(v6);
	benchmark::DoNotOptimize(v7);
}

BENCHMARK(bm_vector_floor_scalar);

#if defined(RTM_SSE2_INTRINSICS)
static void bm_vector_floor_sse2(benchmark::State& state)
{
	vector4f v0 = vector_set(-123.134f);
	vector4f v1 = vector_set(123.134f);
	vector4f v2 = vector_set(-123.134f);
	vector4f v3 = vector_set(123.134f);
	vector4f v4 = vector_set(-123.134f);
	vector4f v5 = vector_set(123.134f);
	vector4f v6 = vector_set(-123.134f);
	vector4f v7 = vector_set(123.134f);

	for (auto _ : state)
	{
		v0 = vector_floor_sse2(v0);
		v1 = vector_floor_sse2(v1);
		v2 = vector_floor_sse2(v2);
		v3 = vector_floor_sse2(v3);
		v4 = vector_floor_sse2(v4);
		v5 = vector_floor_sse2(v5);
		v6 = vector_floor_sse2(v6);
		v7 = vector_floor_sse2(v7);
	}

	benchmark::DoNotOptimize(v0);
	benchmark::DoNotOptimize(v1);
	benchmark::DoNotOptimize(v2);
	benchmark::DoNotOptimize(v3);
	benchmark::DoNotOptimize(v4);
	benchmark::DoNotOptimize(v5);
	benchmark::DoNotOptimize(v6);
	benchmark::DoNotOptimize(v7);
}

BENCHMARK(bm_vector_floor_sse2);
#endif

#if defined(RTM_NEON_INTRINSICS)
static void bm_vector_floor_neon(benchmark::State& state)
{
	vector4f v0 = vector_set(-123.134f);
	vector4f v1 = vector_set(123.134f);
	vector4f v2 = vector_set(-123.134f);
	vector4f v3 = vector_set(123.134f);
	vector4f v4 = vector_set(-123.134f);
	vector4f v5 = vector_set(123.134f);
	vector4f v6 = vector_set(-123.134f);
	vector4f v7 = vector_set(123.134f);

	for (auto _ : state)
	{
		v0 = vector_floor_neon(v0);
		v1 = vector_floor_neon(v1);
		v2 = vector_floor_neon(v2);
		v3 = vector_floor_neon(v3);
		v4 = vector_floor_neon(v4);
		v5 = vector_floor_neon(v5);
		v6 = vector_floor_neon(v6);
		v7 = vector_floor_neon(v7);
	}

	benchmark::DoNotOptimize(v0);
	benchmark::DoNotOptimize(v1);
	benchmark::DoNotOptimize(v2);
	benchmark::DoNotOptimize(v3);
	benchmark::DoNotOptimize(v4);
	benchmark::DoNotOptimize(v5);
	benchmark::DoNotOptimize(v6);
	benchmark::DoNotOptimize(v7);
}

BENCHMARK(bm_vector_floor_neon);

static void bm_vector_floor_neon_abs_cmp(benchmark::State& state)
{
	vector4f v0 = vector_set(-123.134f);
	vector4f v1 = vector_set(123.134f);
	vector4f v2 = vector_set(-123.134f);
	vector4f v3 = vector_set(123.134f);
	vector4f v4 = vector_set(-123.134f);
	vector4f v5 = vector_set(123.134f);
	vector4f v6 = vector_set(-123.134f);
	vector4f v7 = vector_set(123.134f);

	for (auto _ : state)
	{
		v0 = vector_floor_neon_abs_cmp(v0);
		v1 = vector_floor_neon_abs_cmp(v1);
		v2 = vector_floor_neon_abs_cmp(v2);
		v3 = vector_floor_neon_abs_cmp(v3);
		v4 = vector_floor_neon_abs_cmp(v4);
		v5 = vector_floor_neon_abs_cmp(v5);
		v6 = vector_floor_neon_abs_cmp(v6);
		v7 = vector_floor_neon_abs_cmp(v7);
	}

	benchmark::DoNotOptimize(v0);
	benchmark::DoNotOptimize(v1);
	benchmark::DoNotOptimize(v2);
	benchmark::DoNotOptimize(v3);
	benchmark::DoNotOptimize(v4);
	benchmark::DoNotOptimize(v5);
	benchmark::DoNotOptimize(v6);
	benchmark::DoNotOptimize(v7);
}

BENCHMARK(bm_vector_floor_neon_abs_cmp);
#endif

```

`tools/bench/sources/bench_vector_reciprocal.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <benchmark/benchmark.h>

#include <rtm/vector4f.h>

using namespace rtm;

RTM_FORCE_NOINLINE vector4f RTM_SIMD_CALL vector_reciprocal_scalar(vector4f_arg0 input) RTM_NO_EXCEPT
{
	return vector_div(vector_set(1.0F), input);
}

#if defined(RTM_SSE2_INTRINSICS)
RTM_FORCE_NOINLINE vector4f RTM_SIMD_CALL vector_reciprocal_sse2(vector4f_arg0 input) RTM_NO_EXCEPT
{
	// Perform two passes of Newton-Raphson iteration on the hardware estimate
	__m128 x0 = _mm_rcp_ps(input);

	// First iteration
	__m128 x1 = _mm_sub_ps(_mm_add_ps(x0, x0), _mm_mul_ps(input, _mm_mul_ps(x0, x0)));

	// Second iteration
	__m128 x2 = _mm_sub_ps(_mm_add_ps(x1, x1), _mm_mul_ps(input, _mm_mul_ps(x1, x1)));
	return x2;
}
#endif

#if defined(RTM_NEON_INTRINSICS)
RTM_FORCE_NOINLINE vector4f RTM_SIMD_CALL vector_reciprocal_neon(vector4f_arg0 input) RTM_NO_EXCEPT
{
	// Perform two passes of Newton-Raphson iteration on the hardware estimate
	float32x4_t x0 = vrecpeq_f32(input);

	// First iteration
	float32x4_t x1 = vmulq_f32(x0, vrecpsq_f32(x0, input));

	// Second iteration
	float32x4_t x2 = vmulq_f32(x1, vrecpsq_f32(x1, input));
	return x2;
}
#endif

static void bm_vector_reciprocal_scalar(benchmark::State& state)
{
	vector4f v0 = vector_set(-123.134f);
	vector4f v1 = vector_set(123.134f);
	vector4f v2 = vector_set(-123.134f);
	vector4f v3 = vector_set(123.134f);
	vector4f v4 = vector_set(-123.134f);
	vector4f v5 = vector_set(123.134f);
	vector4f v6 = vector_set(-123.134f);
	vector4f v7 = vector_set(123.134f);

	for (auto _ : state)
	{
		v0 = vector_reciprocal_scalar(v0);
		v1 = vector_reciprocal_scalar(v1);
		v2 = vector_reciprocal_scalar(v2);
		v3 = vector_reciprocal_scalar(v3);
		v4 = vector_reciprocal_scalar(v4);
		v5 = vector_reciprocal_scalar(v5);
		v6 = vector_reciprocal_scalar(v6);
		v7 = vector_reciprocal_scalar(v7);
	}

	benchmark::DoNotOptimize(v0);
	benchmark::DoNotOptimize(v1);
	benchmark::DoNotOptimize(v2);
	benchmark::DoNotOptimize(v3);
	benchmark::DoNotOptimize(v4);
	benchmark::DoNotOptimize(v5);
	benchmark::DoNotOptimize(v6);
	benchmark::DoNotOptimize(v7);
}

BENCHMARK(bm_vector_reciprocal_scalar);

#if defined(RTM_SSE2_INTRINSICS)
static void bm_vector_reciprocal_sse2(benchmark::State& state)
{
	vector4f v0 = vector_set(-123.134f);
	vector4f v1 = vector_set(123.134f);
	vector4f v2 = vector_set(-123.134f);
	vector4f v3 = vector_set(123.134f);
	vector4f v4 = vector_set(-123.134f);
	vector4f v5 = vector_set(123.134f);
	vector4f v6 = vector_set(-123.134f);
	vector4f v7 = vector_set(123.134f);

	for (auto _ : state)
	{
		v0 = vector_reciprocal_sse2(v0);
		v1 = vector_reciprocal_sse2(v1);
		v2 = vector_reciprocal_sse2(v2);
		v3 = vector_reciprocal_sse2(v3);
		v4 = vector_reciprocal_sse2(v4);
		v5 = vector_reciprocal_sse2(v5);
		v6 = vector_reciprocal_sse2(v6);
		v7 = vector_reciprocal_sse2(v7);
	}

	benchmark::DoNotOptimize(v0);
	benchmark::DoNotOptimize(v1);
	benchmark::DoNotOptimize(v2);
	benchmark::DoNotOptimize(v3);
	benchmark::DoNotOptimize(v4);
	benchmark::DoNotOptimize(v5);
	benchmark::DoNotOptimize(v6);
	benchmark::DoNotOptimize(v7);
}

BENCHMARK(bm_vector_reciprocal_sse2);
#endif

#if defined(RTM_NEON_INTRINSICS)
static void bm_vector_reciprocal_neon(benchmark::State& state)
{
	vector4f v0 = vector_set(-123.134f);
	vector4f v1 = vector_set(123.134f);
	vector4f v2 = vector_set(-123.134f);
	vector4f v3 = vector_set(123.134f);
	vector4f v4 = vector_set(-123.134f);
	vector4f v5 = vector_set(123.134f);
	vector4f v6 = vector_set(-123.134f);
	vector4f v7 = vector_set(123.134f);

	for (auto _ : state)
	{
		v0 = vector_reciprocal_neon(v0);
		v1 = vector_reciprocal_neon(v1);
		v2 = vector_reciprocal_neon(v2);
		v3 = vector_reciprocal_neon(v3);
		v4 = vector_reciprocal_neon(v4);
		v5 = vector_reciprocal_neon(v5);
		v6 = vector_reciprocal_neon(v6);
		v7 = vector_reciprocal_neon(v7);
	}

	benchmark::DoNotOptimize(v0);
	benchmark::DoNotOptimize(v1);
	benchmark::DoNotOptimize(v2);
	benchmark::DoNotOptimize(v3);
	benchmark::DoNotOptimize(v4);
	benchmark::DoNotOptimize(v5);
	benchmark::DoNotOptimize(v6);
	benchmark::DoNotOptimize(v7);
}

BENCHMARK(bm_vector_reciprocal_neon);
#endif

```

`tools/bench/sources/bench_vector_round_bankers.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <benchmark/benchmark.h>

#include <rtm/vector4f.h>

using namespace rtm;

RTM_FORCE_NOINLINE vector4f RTM_SIMD_CALL vector_round_bankers_scalar(vector4f_arg0 input) RTM_NO_EXCEPT
{
	scalarf x = scalar_round_bankers(vector_get_x_as_scalar(input));
	scalarf y = scalar_round_bankers(vector_get_y_as_scalar(input));
	scalarf z = scalar_round_bankers(vector_get_z_as_scalar(input));
	scalarf w = scalar_round_bankers(vector_get_w_as_scalar(input));
	return vector_set(x, y, z, w);
}

#if defined(RTM_SSE2_INTRINSICS)
RTM_FORCE_NOINLINE vector4f RTM_SIMD_CALL vector_round_bankers_sse2(vector4f_arg0 input) RTM_NO_EXCEPT
{
	const __m128 sign_mask = _mm_set_ps(-0.0F, -0.0F, -0.0F, -0.0F);
	__m128 sign = _mm_and_ps(input, sign_mask);

	// We add the largest integer that a 32 bit floating point number can represent and subtract it afterwards.
	// This relies on the fact that if we had a fractional part, the new value cannot be represented accurately
	// and IEEE 754 will perform rounding for us. The default rounding mode is Banker's rounding.
	// This has the effect of removing the fractional part while simultaneously rounding.
	// Use the same sign as the input value to make sure we handle positive and negative values.
	const __m128 fractional_limit = _mm_set_ps1(8388608.0F); // 2^23
	__m128 truncating_offset = _mm_or_ps(sign, fractional_limit);
	__m128 integer_part = _mm_sub_ps(_mm_add_ps(input, truncating_offset), truncating_offset);

	// If our input was so large that it had no fractional part, return it unchanged
	// Otherwise return our integer part
	const __m128i abs_mask = _mm_set_epi32(0x7FFFFFFFULL, 0x7FFFFFFFULL, 0x7FFFFFFFULL, 0x7FFFFFFFULL);
	__m128 abs_input = _mm_and_ps(input, _mm_castsi128_ps(abs_mask));
	__m128 is_input_large = _mm_cmpge_ps(abs_input, fractional_limit);
	return _mm_or_ps(_mm_and_ps(is_input_large, input), _mm_andnot_ps(is_input_large, integer_part));
}
#endif

#if defined(RTM_NEON64_INTRINSICS)
RTM_FORCE_NOINLINE vector4f RTM_SIMD_CALL vector_round_bankers_neon64(vector4f_arg0 input) RTM_NO_EXCEPT
{
	return vrndnq_f32(input);
}
#endif

#if defined(RTM_NEON_INTRINSICS)
RTM_FORCE_NOINLINE vector4f RTM_SIMD_CALL vector_round_bankers_neon(vector4f_arg0 input) RTM_NO_EXCEPT
{
    uint32x4_t sign_mask = vreinterpretq_u32_f32(vdupq_n_f32(-0.0F));
	uint32x4_t sign = vandq_u32(vreinterpretq_u32_f32(input), sign_mask);

	// We add the largest integer that a 32 bit floating point number can represent and subtract it afterwards.
	// This relies on the fact that if we had a fractional part, the new value cannot be represented accurately
	// and IEEE 754 will perform rounding for us. The default rounding mode is Banker's rounding.
	// This has the effect of removing the fractional part while simultaneously rounding.
	// Use the same sign as the input value to make sure we handle positive and negative values.
	float32x4_t fractional_limit = vdupq_n_f32(8388608.0F); // 2^23
	float32x4_t truncating_offset = vreinterpretq_f32_u32(vorrq_u32(sign, vreinterpretq_u32_f32(fractional_limit)));
	float32x4_t integer_part = vsubq_f32(vaddq_f32(input, truncating_offset), truncating_offset);

	// If our input was so large that it had no fractional part, return it unchanged
	// Otherwise return our integer part
	uint32x4_t is_input_large = vcageq_f32(input, fractional_limit);
	return vbslq_f32(is_input_large, input, integer_part);
}
#endif

static void bm_vector_round_bankers_scalar(benchmark::State& state)
{
	vector4f v0 = vector_set(-123.134f);
	vector4f v1 = vector_set(123.134f);
	vector4f v2 = vector_set(-123.134f);
	vector4f v3 = vector_set(123.134f);
	vector4f v4 = vector_set(-123.134f);
	vector4f v5 = vector_set(123.134f);
	vector4f v6 = vector_set(-123.134f);
	vector4f v7 = vector_set(123.134f);

	for (auto _ : state)
	{
		v0 = vector_round_bankers_scalar(v0);
		v1 = vector_round_bankers_scalar(v1);
		v2 = vector_round_bankers_scalar(v2);
		v3 = vector_round_bankers_scalar(v3);
		v4 = vector_round_bankers_scalar(v4);
		v5 = vector_round_bankers_scalar(v5);
		v6 = vector_round_bankers_scalar(v6);
		v7 = vector_round_bankers_scalar(v7);
	}

	benchmark::DoNotOptimize(v0);
	benchmark::DoNotOptimize(v1);
	benchmark::DoNotOptimize(v2);
	benchmark::DoNotOptimize(v3);
	benchmark::DoNotOptimize(v4);
	benchmark::DoNotOptimize(v5);
	benchmark::DoNotOptimize(v6);
	benchmark::DoNotOptimize(v7);
}

BENCHMARK(bm_vector_round_bankers_scalar);

#if defined(RTM_SSE2_INTRINSICS)
static void bm_vector_round_bankers_sse2(benchmark::State& state)
{
	vector4f v0 = vector_set(-123.134f);
	vector4f v1 = vector_set(123.134f);
	vector4f v2 = vector_set(-123.134f);
	vector4f v3 = vector_set(123.134f);
	vector4f v4 = vector_set(-123.134f);
	vector4f v5 = vector_set(123.134f);
	vector4f v6 = vector_set(-123.134f);
	vector4f v7 = vector_set(123.134f);

	for (auto _ : state)
	{
		v0 = vector_round_bankers_sse2(v0);
		v1 = vector_round_bankers_sse2(v1);
		v2 = vector_round_bankers_sse2(v2);
		v3 = vector_round_bankers_sse2(v3);
		v4 = vector_round_bankers_sse2(v4);
		v5 = vector_round_bankers_sse2(v5);
		v6 = vector_round_bankers_sse2(v6);
		v7 = vector_round_bankers_sse2(v7);
	}

	benchmark::DoNotOptimize(v0);
	benchmark::DoNotOptimize(v1);
	benchmark::DoNotOptimize(v2);
	benchmark::DoNotOptimize(v3);
	benchmark::DoNotOptimize(v4);
	benchmark::DoNotOptimize(v5);
	benchmark::DoNotOptimize(v6);
	benchmark::DoNotOptimize(v7);
}

BENCHMARK(bm_vector_round_bankers_sse2);
#endif

#if defined(RTM_NEON64_INTRINSICS)
static void bm_vector_round_bankers_neon64(benchmark::State& state)
{
	vector4f v0 = vector_set(-123.134f);
	vector4f v1 = vector_set(123.134f);
	vector4f v2 = vector_set(-123.134f);
	vector4f v3 = vector_set(123.134f);
	vector4f v4 = vector_set(-123.134f);
	vector4f v5 = vector_set(123.134f);
	vector4f v6 = vector_set(-123.134f);
	vector4f v7 = vector_set(123.134f);

	for (auto _ : state)
	{
		v0 = vector_round_bankers_neon64(v0);
		v1 = vector_round_bankers_neon64(v1);
		v2 = vector_round_bankers_neon64(v2);
		v3 = vector_round_bankers_neon64(v3);
		v4 = vector_round_bankers_neon64(v4);
		v5 = vector_round_bankers_neon64(v5);
		v6 = vector_round_bankers_neon64(v6);
		v7 = vector_round_bankers_neon64(v7);
	}

	benchmark::DoNotOptimize(v0);
	benchmark::DoNotOptimize(v1);
	benchmark::DoNotOptimize(v2);
	benchmark::DoNotOptimize(v3);
	benchmark::DoNotOptimize(v4);
	benchmark::DoNotOptimize(v5);
	benchmark::DoNotOptimize(v6);
	benchmark::DoNotOptimize(v7);
}

BENCHMARK(bm_vector_round_bankers_neon64);
#endif

#if defined(RTM_NEON_INTRINSICS)
static void bm_vector_round_bankers_neon(benchmark::State& state)
{
	vector4f v0 = vector_set(-123.134f);
	vector4f v1 = vector_set(123.134f);
	vector4f v2 = vector_set(-123.134f);
	vector4f v3 = vector_set(123.134f);
	vector4f v4 = vector_set(-123.134f);
	vector4f v5 = vector_set(123.134f);
	vector4f v6 = vector_set(-123.134f);
	vector4f v7 = vector_set(123.134f);

	for (auto _ : state)
	{
		v0 = vector_round_bankers_neon(v0);
		v1 = vector_round_bankers_neon(v1);
		v2 = vector_round_bankers_neon(v2);
		v3 = vector_round_bankers_neon(v3);
		v4 = vector_round_bankers_neon(v4);
		v5 = vector_round_bankers_neon(v5);
		v6 = vector_round_bankers_neon(v6);
		v7 = vector_round_bankers_neon(v7);
	}

	benchmark::DoNotOptimize(v0);
	benchmark::DoNotOptimize(v1);
	benchmark::DoNotOptimize(v2);
	benchmark::DoNotOptimize(v3);
	benchmark::DoNotOptimize(v4);
	benchmark::DoNotOptimize(v5);
	benchmark::DoNotOptimize(v6);
	benchmark::DoNotOptimize(v7);
}

BENCHMARK(bm_vector_round_bankers_neon);
#endif

```

`tools/bench/sources/bench_vector_round_symmetric.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <benchmark/benchmark.h>

#include <rtm/vector4f.h>

using namespace rtm;

RTM_FORCE_NOINLINE vector4f RTM_SIMD_CALL vector_round_symmetric_scalar(vector4f_arg0 input) RTM_NO_EXCEPT
{
	const vector4f half = vector_set(0.5F);
	const vector4f floored = vector_floor(vector_add(input, half));
	const vector4f ceiled = vector_ceil(vector_sub(input, half));
	const mask4f is_greater_equal = vector_greater_equal(input, vector_zero());
	return vector_select(is_greater_equal, floored, ceiled);
}

#if defined(RTM_SSE2_INTRINSICS)
RTM_FORCE_NOINLINE vector4f RTM_SIMD_CALL vector_round_symmetric_sse2(vector4f_arg0 input) RTM_NO_EXCEPT
{
	const __m128i abs_mask = _mm_set_epi32(0x7FFFFFFFULL, 0x7FFFFFFFULL, 0x7FFFFFFFULL, 0x7FFFFFFFULL);
	const __m128 fractional_limit = _mm_set_ps1(8388608.0F); // 2^23

	// Build our mask, larger values that have no fractional part, and infinities will be true
	// Smaller values and NaN will be false
	__m128 abs_input = _mm_and_ps(input, _mm_castsi128_ps(abs_mask));
	__m128 is_input_large = _mm_cmpge_ps(abs_input, fractional_limit);

	// Test if our input is NaN with (value != value), it is only true for NaN
	__m128 is_nan = _mm_cmpneq_ps(input, input);

	// Combine our masks to determine if we should return the original value
	__m128 use_original_input = _mm_or_ps(is_input_large, is_nan);

	const __m128 sign_mask = _mm_set_ps(-0.0F, -0.0F, -0.0F, -0.0F);
	__m128 sign = _mm_and_ps(input, sign_mask);

	// For positive values, we add a bias of 0.5.
	// For negative values, we add a bias of -0.5.
	__m128 bias = _mm_or_ps(sign, _mm_set_ps1(0.5F));
	__m128 biased_input = _mm_add_ps(input, bias);

	// Convert to an integer with truncation and back, this rounds towards zero.
	__m128 integer_part = _mm_cvtepi32_ps(_mm_cvttps_epi32(biased_input));

	return _mm_or_ps(_mm_and_ps(use_original_input, input), _mm_andnot_ps(use_original_input, integer_part));
}
#endif

#if defined(RTM_NEON_INTRINSICS)
RTM_FORCE_NOINLINE vector4f RTM_SIMD_CALL vector_round_symmetric_neon(vector4f_arg0 input) RTM_NO_EXCEPT
{
	// NaN, +- Infinity, and numbers larger or equal to 2^23 remain unchanged
	// since they have no fractional part.

	float32x4_t fractional_limit = vdupq_n_f32(8388608.0F); // 2^23

	// Build our mask, larger values that have no fractional part, and infinities will be true
	// Smaller values and NaN will be false
	uint32x4_t is_input_large = vcageq_f32(input, fractional_limit);

	// Test if our input is NaN with (value != value), it is only true for NaN
	uint32x4_t is_nan = vmvnq_u32(vceqq_f32(input, input));

	// Combine our masks to determine if we should return the original value
	uint32x4_t use_original_input = vorrq_u32(is_input_large, is_nan);

    uint32x4_t sign_mask = vreinterpretq_u32_f32(vdupq_n_f32(-0.0F));
	uint32x4_t sign = vandq_u32(vreinterpretq_u32_f32(input), sign_mask);

	// For positive values, we add a bias of 0.5.
	// For negative values, we add a bias of -0.5.
	float32x4_t bias = vreinterpretq_f32_u32(vorrq_u32(sign, vreinterpretq_u32_f32(vdupq_n_f32(0.5F))));
	float32x4_t biased_input = vaddq_f32(input, bias);

	// Convert to an integer and back. This does banker's rounding by default
	float32x4_t integer_part = vcvtq_f32_s32(vcvtq_s32_f32(biased_input));

	return vbslq_f32(use_original_input, input, integer_part);
}
#endif

static void bm_vector_round_symmetric_scalar(benchmark::State& state)
{
	vector4f v0 = vector_set(-123.134f);
	vector4f v1 = vector_set(123.134f);
	vector4f v2 = vector_set(-123.134f);
	vector4f v3 = vector_set(123.134f);
	vector4f v4 = vector_set(-123.134f);
	vector4f v5 = vector_set(123.134f);
	vector4f v6 = vector_set(-123.134f);
	vector4f v7 = vector_set(123.134f);

	for (auto _ : state)
	{
		v0 = vector_round_symmetric_scalar(v0);
		v1 = vector_round_symmetric_scalar(v1);
		v2 = vector_round_symmetric_scalar(v2);
		v3 = vector_round_symmetric_scalar(v3);
		v4 = vector_round_symmetric_scalar(v4);
		v5 = vector_round_symmetric_scalar(v5);
		v6 = vector_round_symmetric_scalar(v6);
		v7 = vector_round_symmetric_scalar(v7);
	}

	benchmark::DoNotOptimize(v0);
	benchmark::DoNotOptimize(v1);
	benchmark::DoNotOptimize(v2);
	benchmark::DoNotOptimize(v3);
	benchmark::DoNotOptimize(v4);
	benchmark::DoNotOptimize(v5);
	benchmark::DoNotOptimize(v6);
	benchmark::DoNotOptimize(v7);
}

BENCHMARK(bm_vector_round_symmetric_scalar);

#if defined(RTM_SSE2_INTRINSICS)
static void bm_vector_round_symmetric_sse2(benchmark::State& state)
{
	vector4f v0 = vector_set(-123.134f);
	vector4f v1 = vector_set(123.134f);
	vector4f v2 = vector_set(-123.134f);
	vector4f v3 = vector_set(123.134f);
	vector4f v4 = vector_set(-123.134f);
	vector4f v5 = vector_set(123.134f);
	vector4f v6 = vector_set(-123.134f);
	vector4f v7 = vector_set(123.134f);

	for (auto _ : state)
	{
		v0 = vector_round_symmetric_sse2(v0);
		v1 = vector_round_symmetric_sse2(v1);
		v2 = vector_round_symmetric_sse2(v2);
		v3 = vector_round_symmetric_sse2(v3);
		v4 = vector_round_symmetric_sse2(v4);
		v5 = vector_round_symmetric_sse2(v5);
		v6 = vector_round_symmetric_sse2(v6);
		v7 = vector_round_symmetric_sse2(v7);
	}

	benchmark::DoNotOptimize(v0);
	benchmark::DoNotOptimize(v1);
	benchmark::DoNotOptimize(v2);
	benchmark::DoNotOptimize(v3);
	benchmark::DoNotOptimize(v4);
	benchmark::DoNotOptimize(v5);
	benchmark::DoNotOptimize(v6);
	benchmark::DoNotOptimize(v7);
}

BENCHMARK(bm_vector_round_symmetric_sse2);
#endif

#if defined(RTM_NEON_INTRINSICS)
static void bm_vector_round_symmetric_neon(benchmark::State& state)
{
	vector4f v0 = vector_set(-123.134f);
	vector4f v1 = vector_set(123.134f);
	vector4f v2 = vector_set(-123.134f);
	vector4f v3 = vector_set(123.134f);
	vector4f v4 = vector_set(-123.134f);
	vector4f v5 = vector_set(123.134f);
	vector4f v6 = vector_set(-123.134f);
	vector4f v7 = vector_set(123.134f);

	for (auto _ : state)
	{
		v0 = vector_round_symmetric_neon(v0);
		v1 = vector_round_symmetric_neon(v1);
		v2 = vector_round_symmetric_neon(v2);
		v3 = vector_round_symmetric_neon(v3);
		v4 = vector_round_symmetric_neon(v4);
		v5 = vector_round_symmetric_neon(v5);
		v6 = vector_round_symmetric_neon(v6);
		v7 = vector_round_symmetric_neon(v7);
	}

	benchmark::DoNotOptimize(v0);
	benchmark::DoNotOptimize(v1);
	benchmark::DoNotOptimize(v2);
	benchmark::DoNotOptimize(v3);
	benchmark::DoNotOptimize(v4);
	benchmark::DoNotOptimize(v5);
	benchmark::DoNotOptimize(v6);
	benchmark::DoNotOptimize(v7);
}

BENCHMARK(bm_vector_round_symmetric_neon);
#endif

```

`tools/bench/sources/bench_vector_sign.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2019 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <benchmark/benchmark.h>

#include <rtm/vector4f.h>

using namespace rtm;

// Wins on Ryzen 2990X desktop VS2017 x64 AVX
// Despite taking 5 instructions unlike sse2 which needs 2, this is consistently faster as well.
RTM_FORCE_NOINLINE vector4f RTM_SIMD_CALL vector_sign_ref(vector4f_arg0 input) RTM_NO_EXCEPT
{
	const mask4f mask = vector_greater_equal(input, vector_zero());
	return vector_select(mask, vector_set(1.0f), vector_set(-1.0f));
}

#if defined(RTM_SSE2_INTRINSICS)
// Wins on Haswell laptop x64 AVX
// Same performance as ref.
// Wins on Ryzen 2990X desktop clang9 x64 AVX
RTM_FORCE_NOINLINE vector4f RTM_SIMD_CALL vector_sign_sse2(vector4f_arg0 input) RTM_NO_EXCEPT
{
	constexpr __m128 signs = { -0.0f, -0.0f, -0.0f, -0.0f };
	constexpr __m128 one = { 1.0f, 1.0f, 1.0f, 1.0f };
	const __m128 sign_bits = _mm_and_ps(input, signs);	// Mask out the sign bit
	return _mm_or_ps(sign_bits, one);					// Copy the sign bit onto +-1.0f
}
#endif

static void bm_vector_sign_ref(benchmark::State& state)
{
	vector4f v0 = vector_set(-1.0f, 1.0f, -2.0f, -123.134f);
	vector4f v1 = vector_set(-1.0f, 1.0f, -2.0f, -123.134f);
	vector4f v2 = vector_set(-1.0f, 1.0f, -2.0f, -123.134f);
	vector4f v3 = vector_set(-1.0f, 1.0f, -2.0f, -123.134f);
	vector4f v4 = vector_set(-1.0f, 1.0f, -2.0f, -123.134f);
	vector4f v5 = vector_set(-1.0f, 1.0f, -2.0f, -123.134f);
	vector4f v6 = vector_set(-1.0f, 1.0f, -2.0f, -123.134f);
	vector4f v7 = vector_set(-1.0f, 1.0f, -2.0f, -123.134f);

	for (auto _ : state)
	{
		v0 = vector_sign_ref(v0);
		v1 = vector_sign_ref(v1);
		v2 = vector_sign_ref(v2);
		v3 = vector_sign_ref(v3);
		v4 = vector_sign_ref(v4);
		v5 = vector_sign_ref(v5);
		v6 = vector_sign_ref(v6);
		v7 = vector_sign_ref(v7);
	}

	benchmark::DoNotOptimize(v0);
	benchmark::DoNotOptimize(v1);
	benchmark::DoNotOptimize(v2);
	benchmark::DoNotOptimize(v3);
	benchmark::DoNotOptimize(v4);
	benchmark::DoNotOptimize(v5);
	benchmark::DoNotOptimize(v6);
	benchmark::DoNotOptimize(v7);
}

BENCHMARK(bm_vector_sign_ref);

#if defined(RTM_SSE2_INTRINSICS)
static void bm_vector_sign_sse2(benchmark::State& state)
{
	vector4f v0 = vector_set(-1.0f, 1.0f, -2.0f, -123.134f);
	vector4f v1 = vector_set(-1.0f, 1.0f, -2.0f, -123.134f);
	vector4f v2 = vector_set(-1.0f, 1.0f, -2.0f, -123.134f);
	vector4f v3 = vector_set(-1.0f, 1.0f, -2.0f, -123.134f);
	vector4f v4 = vector_set(-1.0f, 1.0f, -2.0f, -123.134f);
	vector4f v5 = vector_set(-1.0f, 1.0f, -2.0f, -123.134f);
	vector4f v6 = vector_set(-1.0f, 1.0f, -2.0f, -123.134f);
	vector4f v7 = vector_set(-1.0f, 1.0f, -2.0f, -123.134f);

	for (auto _ : state)
	{
		v0 = vector_sign_sse2(v0);
		v1 = vector_sign_sse2(v1);
		v2 = vector_sign_sse2(v2);
		v3 = vector_sign_sse2(v3);
		v4 = vector_sign_sse2(v4);
		v5 = vector_sign_sse2(v5);
		v6 = vector_sign_sse2(v6);
		v7 = vector_sign_sse2(v7);
	}

	benchmark::DoNotOptimize(v0);
	benchmark::DoNotOptimize(v1);
	benchmark::DoNotOptimize(v2);
	benchmark::DoNotOptimize(v3);
	benchmark::DoNotOptimize(v4);
	benchmark::DoNotOptimize(v5);
	benchmark::DoNotOptimize(v6);
	benchmark::DoNotOptimize(v7);
}

BENCHMARK(bm_vector_sign_sse2);
#endif

```

`tools/bench/sources/bench_vector_sin.cpp`:

```cpp
////////////////////////////////////////////////////////////////////////////////
// The MIT License (MIT)
//
// Copyright (c) 2020 Nicholas Frechette & Realtime Math contributors
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in all
// copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.
////////////////////////////////////////////////////////////////////////////////

#include <benchmark/benchmark.h>

#include <rtm/vector4f.h>

using namespace rtm;

RTM_FORCE_NOINLINE vector4f RTM_SIMD_CALL vector_sin_scalar(vector4f_arg0 input) RTM_NO_EXCEPT
{
	scalarf x = scalar_sin(vector_get_x_as_scalar(input));
	scalarf y = scalar_sin(vector_get_y_as_scalar(input));
	scalarf z = scalar_sin(vector_get_z_as_scalar(input));
	scalarf w = scalar_sin(vector_get_w_as_scalar(input));
	return vector_set(x, y, z, w);
}

#if defined(RTM_SSE2_INTRINSICS)
RTM_FORCE_NOINLINE vector4f RTM_SIMD_CALL vector_sin_sse2(vector4f_arg0 input) RTM_NO_EXCEPT
{
	// Use a degree 11 minimax approximation polynomial
	// See: GPGPU Programming for Games and Science (David H. Eberly)

	// Remap our input in the [-pi, pi] range
	__m128 quotient = _mm_mul_ps(input, _mm_set_ps1(rtm::constants::one_div_two_pi()));
	quotient = vector_round_bankers(quotient);
	quotient = _mm_mul_ps(quotient, _mm_set_ps1(rtm::constants::two_pi()));
	__m128 x = _mm_sub_ps(input, quotient);

	// Remap our input in the [-pi/2, pi/2] range
	const __m128 sign_mask = _mm_set_ps(-0.0F, -0.0F, -0.0F, -0.0F);
	__m128 sign = _mm_and_ps(x, sign_mask);
	__m128 reference = _mm_or_ps(sign, _mm_set_ps1(rtm::constants::pi()));

	const __m128 reflection = _mm_sub_ps(reference, x);
	const __m128i abs_mask = _mm_set_epi32(0x7FFFFFFFULL, 0x7FFFFFFFULL, 0x7FFFFFFFULL, 0x7FFFFFFFULL);
	const __m128 x_abs = _mm_and_ps(x, _mm_castsi128_ps(abs_mask));

	__m128 is_less_equal_than_half_pi = _mm_cmple_ps(x_abs, _mm_set_ps1(rtm::constants::half_pi()));

	x = RTM_VECTOR4F_SELECT(is_less_equal_than_half_pi, x, reflection);

	// Calculate our value
	const __m128 x2 = _mm_mul_ps(x, x);
	__m128 result = _mm_add_ps(_mm_mul_ps(x2, _mm_set_ps1(-2.3828544692960918e-8F)), _mm_set_ps1(2.7521557770526783e-6F));
	result = _mm_add_ps(_mm_mul_ps(result, x2), _mm_set_ps1(-1.9840782426250314e-4F));
	result = _mm_add_ps(_mm_mul_ps(result, x2), _mm_set_ps1(8.3333303183525942e-3F));
	result = _mm_add_ps(_mm_mul_ps(result, x2), _mm_set_ps1(-1.6666666601721269e-1F));
	result = _mm_add_ps(_mm_mul_ps(result, x2), _mm_set_ps1(1.0F));
	result = _mm_mul_ps(result, x);
	return result;
}
#endif

#if defined(RTM_NEON_INTRINSICS)
RTM_FORCE_NOINLINE vector4f RTM_SIMD_CALL vector_sin_neon(vector4f_arg0 input) RTM_NO_EXCEPT
{
	// Use a degree 11 minimax approximation polynomial
	// See: GPGPU Programming for Games and Science (David H. Eberly)

	// Remap our input in the [-pi, pi] range
	float32x4_t quotient = vmulq_n_f32(input, rtm::constants::one_div_two_pi());
	quotient = vector_round_bankers(quotient);
	quotient = vmulq_n_f32(quotient, rtm::constants::two_pi());
	float32x4_t x = vsubq_f32(input, quotient);

	// Remap our input in the [-pi/2, pi/2] range
	uint32x4_t sign_mask = vreinterpretq_u32_f32(vdupq_n_f32(-0.0F));
	uint32x4_t sign = vandq_u32(vreinterpretq_u32_f32(x), sign_mask);
	float32x4_t reference = vreinterpretq_f32_u32(vorrq_u32(sign, vreinterpretq_u32_f32(vdupq_n_f32(rtm::constants::pi()))));

	float32x4_t reflection = vsubq_f32(reference, x);
	float32x4_t is_less_equal_than_half_pi = vcaleq_f32(x, vdupq_n_f32(rtm::constants::half_pi()));
	x = vbslq_f32(is_less_equal_than_half_pi, x, reflection);

	// Calculate our value
	float32x4_t x2 = vmulq_f32(x, x);

#if defined(RTM_NEON64_INTRINSICS)
	float32x4_t result = vfmaq_n_f32(vdupq_n_f32(2.7521557770526783e-6F), x2, -2.3828544692960918e-8F);
	result = vfmaq_f32(vdupq_n_f32(-1.9840782426250314e-4F), result, x2);
	result = vfmaq_f32(vdupq_n_f32(8.3333303183525942e-3F), result, x2);
	result = vfmaq_f32(vdupq_n_f32(-1.6666666601721269e-1F), result, x2);
	result = vfmaq_f32(vdupq_n_f32(1.0F), result, x2);
#else
	float32x4_t result = vmlaq_n_f32(vdupq_n_f32(2.7521557770526783e-6F), x2, -2.3828544692960918e-8F);
	result = vmlaq_f32(vdupq_n_f32(-1.9840782426250314e-4F), result, x2);
	result = vmlaq_f32(vdupq_n_f32(8.3333303183525942e-3F), result, x2);
	result = vmlaq_f32(vdupq_n_f32(-1.6666666601721269e-1F), result, x2);
	result = vmlaq_f32(vdupq_n_f32(1.0F), result, x2);
#endif

	result = vmulq_f32(result, x);
	return result;
}
#endif

static void bm_vector_sin_scalar(benchmark::State& state)
{
	vector4f v0 = vector_set(-123.134f);
	vector4f v1 = vector_set(123.134f);
	vector4f v2 = vector_set(-123.134f);
	vector4f v3 = vector_set(123.134f);
	vector4f v4 = vector_set(-123.134f);
	vector4f v5 = vector_set(123.134f);
	vector4f v6 = vector_set(-123.134f);
	vector4f v7 = vector_set(123.134f);

	vector4f scale = vector_set(100.0F);

	for (auto _ : state)
	{
		v0 = vector_mul(vector_sin_scalar(v0), scale);
		v1 = vector_mul(vector_sin_scalar(v1), scale);
		v2 = vector_mul(vector_sin_scalar(v2), scale);
		v3 = vector_mul(vector_sin_scalar(v3), scale);
		v4 = vector_mul(vector_sin_scalar(v4), scale);
		v5 = vector_mul(vector_sin_scalar(v5), scale);
		v6 = vector_mul(vector_sin_scalar(v6), scale);
		v7 = vector_mul(vector_sin_scalar(v7), scale);
	}

	benchmark::DoNotOptimize(v0);
	benchmark::DoNotOptimize(v1);
	benchmark::DoNotOptimize(v2);
	benchmark::DoNotOptimize(v3);
	benchmark::DoNotOptimize(v4);
	benchmark::DoNotOptimize(v5);
	benchmark::DoNotOptimize(v6);
	benchmark::DoNotOptimize(v7);
}

BENCHMARK(bm_vector_sin_scalar);

#if defined(RTM_SSE2_INTRINSICS)
static void bm_vector_sin_sse2(benchmark::State& state)
{
	vector4f v0 = vector_set(-123.134f);
	vector4f v1 = vector_set(123.134f);
	vector4f v2 = vector_set(-123.134f);
	vector4f v3 = vector_set(123.134f);
	vector4f v4 = vector_set(-123.134f);
	vector4f v5 = vector_set(123.134f);
	vector4f v6 = vector_set(-123.134f);
	vector4f v7 = vector_set(123.134f);

	vector4f scale = vector_set(100.0F);

	for (auto _ : state)
	{
		v0 = vector_mul(vector_sin_sse2(v0), scale);
		v1 = vector_mul(vector_sin_sse2(v1), scale);
		v2 = vector_mul(vector_sin_sse2(v2), scale);
		v3 = vector_mul(vector_sin_sse2(v3), scale);
		v4 = vector_mul(vector_sin_sse2(v4), scale);
		v5 = vector_mul(vector_sin_sse2(v5), scale);
		v6 = vector_mul(vector_sin_sse2(v6), scale);
		v7 = vector_mul(vector_sin_sse2(v7), scale);
	}

	benchmark::DoNotOptimize(v0);
	benchmark::DoNotOptimize(v1);
	benchmark::DoNotOptimize(v2);
	benchmark::DoNotOptimize(v3);
	benchmark::DoNotOptimize(v4);
	benchmark::DoNotOptimize(v5);
	benchmark::DoNotOptimize(v6);
	benchmark::DoNotOptimize(v7);
}

BENCHMARK(bm_vector_sin_sse2);
#endif

#if defined(RTM_NEON_INTRINSICS)
static void bm_vector_sin_neon(benchmark::State& state)
{
	vector4f v0 = vector_set(-123.134f);
	vector4f v1 = vector_set(123.134f);
	vector4f v2 = vector_set(-123.134f);
	vector4f v3 = vector_set(123.134f);
	vector4f v4 = vector_set(-123.134f);
	vector4f v5 = vector_set(123.134f);
	vector4f v6 = vector_set(-123.134f);
	vector4f v7 = vector_set(123.134f);

	vector4f scale = vector_set(100.0F);

	for (auto _ : state)
	{
		v0 = vector_mul(vector_sin_neon(v0), scale);
		v1 = vector_mul(vector_sin_neon(v1), scale);
		v2 = vector_mul(vector_sin_neon(v2), scale);
		v3 = vector_mul(vector_sin_neon(v3), scale);
		v4 = vector_mul(vector_sin_neon(v4), scale);
		v5 = vector_mul(vector_sin_neon(v5), scale);
		v6 = vector_mul(vector_sin_neon(v6), scale);
		v7 = vector_mul(vector_sin_neon(v7), scale);
	}

	benchmark::DoNotOptimize(v0);
	benchmark::DoNotOptimize(v1);
	benchmark::DoNotOptimize(v2);
	benchmark::DoNotOptimize(v3);
	benchmark::DoNotOptimize(v4);
	benchmark::DoNotOptimize(v5);
	benchmark::DoNotOptimize(v6);
	benchmark::DoNotOptimize(v7);
}

BENCHMARK(bm_vector_sin_neon);
#endif

```

`tools/gen_vector_mix_sse.py`:

```py
import sys

# This script is used to generate the vector_mix template specialization

# Note:
# I attempted to use this script to exhaustively specialize every template
# for vector_mix and while it worked fine, compilation was quite dramatically slower
# and as a result, this is not currently used

def idx_to_mix(idx):
	if idx == 0:
		return 'mix4::x'
	elif idx == 1:
		return 'mix4::y'
	elif idx == 2:
		return 'mix4::z'
	elif idx == 3:
		return 'mix4::w'
	elif idx == 4:
		return 'mix4::a'
	elif idx == 5:
		return 'mix4::b'
	elif idx == 6:
		return 'mix4::c'
	elif idx == 7:
		return 'mix4::d'

def idx_to_mix_short(idx):
	if idx == 0:
		return 'x'
	elif idx == 1:
		return 'y'
	elif idx == 2:
		return 'z'
	elif idx == 3:
		return 'w'
	elif idx == 4:
		return 'a'
	elif idx == 5:
		return 'b'
	elif idx == 6:
		return 'c'
	elif idx == 7:
		return 'd'

def is_mix_xyzw(idx):
	return idx >= 0 and idx <= 3

def is_mix_abcd(idx):
	return idx >= 4 and idx <= 7

def get_selector(idx):
	if (idx % 4) == 0:
		return 'x'
	elif (idx % 4) == 1:
		return 'y'
	elif (idx % 4) == 2:
		return 'z'
	elif (idx % 4) == 3:
		return 'w'

# Rule formats are tuples of the form: (inner txt, cpp macro guard)
# The cpp macro guard can be None
# Rule functions return a matching rule format on success, or None on failure
# Rules will match in the order defined in the rules list
# When a rule matches, no other rule with the same cpp macro guard can match afterwards
# A rule must always match, as such a fallback must be implemented for every architecture

def rule_self0(x, y, z, w):
	if x == 0 and y == 1 and z == 2 and w == 3:
		inner_txt='''		(void)input1;
		return input0;'''
		return (inner_txt, None)

def rule_self1(x, y, z, w):
	if x == 4 and y == 5 and z == 6 and w == 7:
		inner_txt='''		(void)input0;
		return input1;'''
		return (inner_txt, None)

def rule_sse4_blend(x, y, z, w):
	if (x % 4) == 0 and (y % 4) == 1 and (z % 4) == 2 and (w % 4) == 3:
		mask_x = 0 if x == 0 else 1
		mask_y = 0 if y == 1 else 2
		mask_z = 0 if z == 2 else 4
		mask_w = 0 if w == 3 else 8
		inner_txt='''		return _mm_blend_ps(input0, input1, {} | {} | {} | {});'''.format(mask_x, mask_y, mask_z, mask_w)
		return (inner_txt, 'RTM_SSE4_INTRINSICS')

def rule_sse4_insert(x, y, z, w):
	if is_mix_abcd(x) and y == 1 and z == 2 and w == 3:
		inner_txt='''		return _mm_insert_ps(input0, input1, ({} << 6) | (0 << 4));'''.format(x % 4)
		return (inner_txt, 'RTM_SSE4_INTRINSICS')
	if x == 0 and is_mix_abcd(y) and z == 2 and w == 3:
		inner_txt='''		return _mm_insert_ps(input0, input1, ({} << 6) | (1 << 4));'''.format(y % 4)
		return (inner_txt, 'RTM_SSE4_INTRINSICS')
	if x == 0 and y == 1 and is_mix_abcd(z) and w == 3:
		inner_txt='''		return _mm_insert_ps(input0, input1, ({} << 6) | (2 << 4));'''.format(z % 4)
		return (inner_txt, 'RTM_SSE4_INTRINSICS')
	if x == 0 and y == 1 and z == 2 and is_mix_abcd(w):
		inner_txt='''		return _mm_insert_ps(input0, input1, ({} << 6) | (3 << 4));'''.format(w % 4)
		return (inner_txt, 'RTM_SSE4_INTRINSICS')
	if is_mix_xyzw(x) and y == 5 and z == 6 and w == 7:
		inner_txt='''		return _mm_insert_ps(input1, input0, ({} << 6) | (0 << 4));'''.format(x % 4)
		return (inner_txt, 'RTM_SSE4_INTRINSICS')
	if x == 4 and is_mix_xyzw(y) and z == 6 and w == 7:
		inner_txt='''		return _mm_insert_ps(input1, input0, ({} << 6) | (1 << 4));'''.format(y % 4)
		return (inner_txt, 'RTM_SSE4_INTRINSICS')
	if x == 4 and y == 5 and is_mix_xyzw(z) and w == 7:
		inner_txt='''		return _mm_insert_ps(input1, input0, ({} << 6) | (2 << 4));'''.format(z % 4)
		return (inner_txt, 'RTM_SSE4_INTRINSICS')
	if x == 4 and y == 5 and z == 6 and is_mix_xyzw(w):
		inner_txt='''		return _mm_insert_ps(input1, input0, ({} << 6) | (3 << 4));'''.format(w % 4)
		return (inner_txt, 'RTM_SSE4_INTRINSICS')

def rule_sse3_moveldup(x, y, z, w):
	if x == 0 and y == 0 and z == 2 and w == 2:
		inner_txt='''		(void)input1;
		return _mm_moveldup_ps(input0);'''
		return (inner_txt, 'RTM_SSE3_INTRINSICS')
	if x == 4 and y == 4 and z == 6 and w == 6:
		inner_txt='''		(void)input0;
		return _mm_moveldup_ps(input1);'''
		return (inner_txt, 'RTM_SSE3_INTRINSICS')

def rule_sse3_movehdup(x, y, z, w):
	if x == 1 and y == 1 and z == 3 and w == 3:
		inner_txt='''		(void)input1;
		return _mm_movehdup_ps(input0);'''
		return (inner_txt, 'RTM_SSE3_INTRINSICS')
	if x == 5 and y == 5 and z == 7 and w == 7:
		inner_txt='''		(void)input0;
		return _mm_movehdup_ps(input1);'''
		return (inner_txt, 'RTM_SSE3_INTRINSICS')

def rule_sse2_unpacklo(x, y, z, w):
	if x == 0 and y == 4 and z == 1 and w == 5:
		inner_txt='''		return _mm_unpacklo_ps(input0, input1);'''
		return (inner_txt, 'RTM_SSE2_INTRINSICS')
	if x == 4 and y == 0 and z == 5 and w == 1:
		inner_txt='''		return _mm_unpacklo_ps(input1, input0);'''
		return (inner_txt, 'RTM_SSE2_INTRINSICS')
	if x == 0 and y == 0 and z == 1 and w == 1:
		inner_txt='''		(void)input1;
		return _mm_unpacklo_ps(input0, input0);'''
		return (inner_txt, 'RTM_SSE2_INTRINSICS')
	if x == 4 and y == 4 and z == 5 and w == 5:
		inner_txt='''		(void)input0;
		return _mm_unpacklo_ps(input1, input1);'''
		return (inner_txt, 'RTM_SSE2_INTRINSICS')

def rule_sse2_unpackhi(x, y, z, w):
	if x == 2 and y == 6 and z == 3 and w == 7:
		inner_txt='''		return _mm_unpackhi_ps(input0, input1);'''
		return (inner_txt, 'RTM_SSE2_INTRINSICS')
	if x == 6 and y == 2 and z == 7 and w == 3:
		inner_txt='''		return _mm_unpackhi_ps(input1, input0);'''
		return (inner_txt, 'RTM_SSE2_INTRINSICS')
	if x == 2 and y == 2 and z == 3 and w == 3:
		inner_txt='''		(void)input1;
		return _mm_unpackhi_ps(input0, input0);'''
		return (inner_txt, 'RTM_SSE2_INTRINSICS')
	if x == 6 and y == 6 and z == 7 and w == 7:
		inner_txt='''		(void)input0;
		return _mm_unpackhi_ps(input1, input1);'''
		return (inner_txt, 'RTM_SSE2_INTRINSICS')

def rule_sse2_movelh(x, y, z, w):
	if x == 0 and y == 1 and z == 0 and w == 1:
		inner_txt='''		(void)input1;
		return _mm_movelh_ps(input0, input0);'''
		return (inner_txt, 'RTM_SSE2_INTRINSICS')
	if x == 4 and y == 5 and z == 4 and w == 5:
		inner_txt='''		(void)input0;
		return _mm_movelh_ps(input1, input1);'''
		return (inner_txt, 'RTM_SSE2_INTRINSICS')

def rule_sse2_movehl(x, y, z, w):
	if x == 2 and y == 3 and z == 2 and w == 3:
		inner_txt='''		(void)input1;
		return _mm_movehl_ps(input0, input0);'''
		return (inner_txt, 'RTM_SSE2_INTRINSICS')
	if x == 6 and y == 7 and z == 6 and w == 7:
		inner_txt='''		(void)input0;
		return _mm_movehl_ps(input1, input1);'''
		return (inner_txt, 'RTM_SSE2_INTRINSICS')

def rule_sse2_shuffle(x, y, z, w):
	if is_mix_xyzw(x) and is_mix_xyzw(y) and is_mix_xyzw(z) and is_mix_xyzw(w):
		inner_txt='''		(void)input1;
		return _mm_shuffle_ps(input0, input0, _MM_SHUFFLE({}, {}, {}, {}));'''.format(w % 4, z % 4, y % 4, x % 4)
		return (inner_txt, 'RTM_SSE2_INTRINSICS')
	if is_mix_xyzw(x) and is_mix_xyzw(y) and is_mix_abcd(z) and is_mix_abcd(w):
		inner_txt='''		return _mm_shuffle_ps(input0, input1, _MM_SHUFFLE({}, {}, {}, {}));'''.format(w % 4, z % 4, y % 4, x % 4)
		return (inner_txt, 'RTM_SSE2_INTRINSICS')
	if is_mix_abcd(x) and is_mix_abcd(y) and is_mix_abcd(z) and is_mix_abcd(w):
		inner_txt='''		(void)input0;
		return _mm_shuffle_ps(input1, input1, _MM_SHUFFLE({}, {}, {}, {}));'''.format(w % 4, z % 4, y % 4, x % 4)
		return (inner_txt, 'RTM_SSE2_INTRINSICS')
	if is_mix_abcd(x) and is_mix_abcd(y) and is_mix_xyzw(z) and is_mix_xyzw(w):
		inner_txt='''		return _mm_shuffle_ps(input1, input0, _MM_SHUFFLE({}, {}, {}, {}));'''.format(w % 4, z % 4, y % 4, x % 4)
		return (inner_txt, 'RTM_SSE2_INTRINSICS')

	if is_mix_xyzw(x) and is_mix_xyzw(y) and is_mix_xyzw(z) and is_mix_abcd(w):
		inner_txt='''		const __m128 z0z0w1w1 = _mm_shuffle_ps(input0, input1, _MM_SHUFFLE({}, {}, {}, {}));
		return _mm_shuffle_ps(input0, z0z0w1w1, _MM_SHUFFLE({}, {}, {}, {}));'''.format(w % 4, w % 4, z % 4, z % 4, 2, 0, y % 4, x % 4)
		return (inner_txt, 'RTM_SSE2_INTRINSICS')
	if is_mix_xyzw(x) and is_mix_xyzw(y) and is_mix_abcd(z) and is_mix_xyzw(w):
		inner_txt='''		const __m128 z1z1w0w0 = _mm_shuffle_ps(input1, input0, _MM_SHUFFLE({}, {}, {}, {}));
		return _mm_shuffle_ps(input0, z1z1w0w0, _MM_SHUFFLE({}, {}, {}, {}));'''.format(w % 4, w % 4, z % 4, z % 4, 2, 0, y % 4, x % 4)
		return (inner_txt, 'RTM_SSE2_INTRINSICS')
	if is_mix_xyzw(x) and is_mix_abcd(y) and is_mix_xyzw(z) and is_mix_xyzw(w):
		inner_txt='''		const __m128 x0x0y1y1 = _mm_shuffle_ps(input0, input1, _MM_SHUFFLE({}, {}, {}, {}));
		return _mm_shuffle_ps(x0x0y1y1, input0, _MM_SHUFFLE({}, {}, {}, {}));'''.format(y % 4, y % 4, x % 4, x % 4, w % 4, z % 4, 2, 0)
		return (inner_txt, 'RTM_SSE2_INTRINSICS')
	if is_mix_abcd(x) and is_mix_xyzw(y) and is_mix_xyzw(z) and is_mix_xyzw(w):
		inner_txt='''		const __m128 x1x1y0y0 = _mm_shuffle_ps(input1, input0, _MM_SHUFFLE({}, {}, {}, {}));
		return _mm_shuffle_ps(x1x1y0y0, input0, _MM_SHUFFLE({}, {}, {}, {}));'''.format(y % 4, y % 4, x % 4, x % 4, w % 4, z % 4, 2, 0)
		return (inner_txt, 'RTM_SSE2_INTRINSICS')

	if is_mix_abcd(x) and is_mix_abcd(y) and is_mix_abcd(z) and is_mix_xyzw(w):
		inner_txt='''		const __m128 z1z1w0w0 = _mm_shuffle_ps(input1, input0, _MM_SHUFFLE({}, {}, {}, {}));
		return _mm_shuffle_ps(input1, z1z1w0w0, _MM_SHUFFLE({}, {}, {}, {}));'''.format(w % 4, w % 4, z % 4, z % 4, 2, 0, y % 4, x % 4)
		return (inner_txt, 'RTM_SSE2_INTRINSICS')
	if is_mix_abcd(x) and is_mix_abcd(y) and is_mix_xyzw(z) and is_mix_abcd(w):
		inner_txt='''		const __m128 z0z0w1w1 = _mm_shuffle_ps(input0, input1, _MM_SHUFFLE({}, {}, {}, {}));
		return _mm_shuffle_ps(input1, z0z0w1w1, _MM_SHUFFLE({}, {}, {}, {}));'''.format(w % 4, w % 4, z % 4, z % 4, 2, 0, y % 4, x % 4)
		return (inner_txt, 'RTM_SSE2_INTRINSICS')
	if is_mix_abcd(x) and is_mix_xyzw(y) and is_mix_abcd(z) and is_mix_abcd(w):
		inner_txt='''		const __m128 x1x1y0y0 = _mm_shuffle_ps(input1, input0, _MM_SHUFFLE({}, {}, {}, {}));
		return _mm_shuffle_ps(x1x1y0y0, input1, _MM_SHUFFLE({}, {}, {}, {}));'''.format(y % 4, y % 4, x % 4, x % 4, w % 4, z % 4, 2, 0)
		return (inner_txt, 'RTM_SSE2_INTRINSICS')
	if is_mix_xyzw(x) and is_mix_abcd(y) and is_mix_abcd(z) and is_mix_abcd(w):
		inner_txt='''		const __m128 x0x0y1y1 = _mm_shuffle_ps(input0, input1, _MM_SHUFFLE({}, {}, {}, {}));
		return _mm_shuffle_ps(x0x0y1y1, input1, _MM_SHUFFLE({}, {}, {}, {}));'''.format(y % 4, y % 4, x % 4, x % 4, w % 4, z % 4, 2, 0)
		return (inner_txt, 'RTM_SSE2_INTRINSICS')

	if is_mix_xyzw(x) and is_mix_abcd(y) and is_mix_xyzw(z) and is_mix_abcd(w):
		inner_txt='''		const __m128 x0x0y1y1 = _mm_shuffle_ps(input0, input1, _MM_SHUFFLE({}, {}, {}, {}));
		const __m128 z0z0w1w1 = _mm_shuffle_ps(input0, input1, _MM_SHUFFLE({}, {}, {}, {}));
		return _mm_shuffle_ps(x0x0y1y1, z0z0w1w1, _MM_SHUFFLE({}, {}, {}, {}));'''.format(y % 4, y % 4, x % 4, x % 4, w % 4, w % 4, z % 4, z % 4, 2, 0, 2, 0)
		return (inner_txt, 'RTM_SSE2_INTRINSICS')
	if is_mix_abcd(x) and is_mix_xyzw(y) and is_mix_abcd(z) and is_mix_xyzw(w):
		inner_txt='''		const __m128 x1x1y0y0 = _mm_shuffle_ps(input1, input0, _MM_SHUFFLE({}, {}, {}, {}));
		const __m128 z1z1w0w0 = _mm_shuffle_ps(input1, input0, _MM_SHUFFLE({}, {}, {}, {}));
		return _mm_shuffle_ps(x1x1y0y0, z1z1w0w0, _MM_SHUFFLE({}, {}, {}, {}));'''.format(y % 4, y % 4, x % 4, x % 4, w % 4, w % 4, z % 4, z % 4, 2, 0, 2, 0)
		return (inner_txt, 'RTM_SSE2_INTRINSICS')
	if is_mix_xyzw(x) and is_mix_abcd(y) and is_mix_abcd(z) and is_mix_xyzw(w):
		inner_txt='''		const __m128 x0x0y1y1 = _mm_shuffle_ps(input0, input1, _MM_SHUFFLE({}, {}, {}, {}));
		const __m128 z1z1w0w0 = _mm_shuffle_ps(input1, input0, _MM_SHUFFLE({}, {}, {}, {}));
		return _mm_shuffle_ps(x0x0y1y1, z1z1w0w0, _MM_SHUFFLE({}, {}, {}, {}));'''.format(y % 4, y % 4, x % 4, x % 4, w % 4, w % 4, z % 4, z % 4, 2, 0, 2, 0)
		return (inner_txt, 'RTM_SSE2_INTRINSICS')
	if is_mix_abcd(x) and is_mix_xyzw(y) and is_mix_xyzw(z) and is_mix_abcd(w):
		inner_txt='''		const __m128 x1x1y0y0 = _mm_shuffle_ps(input1, input0, _MM_SHUFFLE({}, {}, {}, {}));
		const __m128 z0z0w1w1 = _mm_shuffle_ps(input0, input1, _MM_SHUFFLE({}, {}, {}, {}));
		return _mm_shuffle_ps(x1x1y0y0, z0z0w1w1, _MM_SHUFFLE({}, {}, {}, {}));'''.format(y % 4, y % 4, x % 4, x % 4, w % 4, w % 4, z % 4, z % 4, 2, 0, 2, 0)
		return (inner_txt, 'RTM_SSE2_INTRINSICS')

def rule_neon64_zip1(x, y, z, w):
	if x == 0 and y == 4 and z == 1 and w == 5:
		inner_txt='''		return vzip1q_f32(input0, input1);'''
		return (inner_txt, 'RTM_NEON64_INTRINSICS')
	if x == 4 and y == 0 and z == 5 and w == 1:
		inner_txt='''		return vzip1q_f32(input1, input0);'''
		return (inner_txt, 'RTM_NEON64_INTRINSICS')

def rule_neon64_zip2(x, y, z, w):
	if x == 2 and y == 6 and z == 3 and w == 7:
		inner_txt='''		return vzip2q_f32(input0, input1);'''
		return (inner_txt, 'RTM_NEON64_INTRINSICS')
	if x == 6 and y == 2 and z == 7 and w == 3:
		inner_txt='''		return vzip2q_f32(input1, input0);'''
		return (inner_txt, 'RTM_NEON64_INTRINSICS')

def rule_neon64_uzp1(x, y, z, w):
	if x == 0 and y == 2 and z == 4 and w == 6:
		inner_txt='''		return vuzp1q_f32(input0, input1);'''
		return (inner_txt, 'RTM_NEON64_INTRINSICS')
	if x == 4 and y == 6 and z == 0 and w == 2:
		inner_txt='''		return vuzp1q_f32(input1, input0);'''
		return (inner_txt, 'RTM_NEON64_INTRINSICS')

def rule_neon64_uzp2(x, y, z, w):
	if x == 1 and y == 3 and z == 5 and w == 7:
		inner_txt='''		return vuzp2q_f32(input0, input1);'''
		return (inner_txt, 'RTM_NEON64_INTRINSICS')
	if x == 5 and y == 7 and z == 1 and w == 3:
		inner_txt='''		return vuzp2q_f32(input1, input0);'''
		return (inner_txt, 'RTM_NEON64_INTRINSICS')

def rule_neon64_trn1(x, y, z, w):
	if x == 0 and y == 4 and z == 2 and w == 6:
		inner_txt='''		return vtrn1q_f32(input0, input1);'''
		return (inner_txt, 'RTM_NEON64_INTRINSICS')
	if x == 4 and y == 0 and z == 6 and w == 2:
		inner_txt='''		return vtrn1q_f32(input1, input0);'''
		return (inner_txt, 'RTM_NEON64_INTRINSICS')

def rule_neon64_trn2(x, y, z, w):
	if x == 1 and y == 5 and z == 3 and w == 7:
		inner_txt='''		return vtrn2q_f32(input0, input1);'''
		return (inner_txt, 'RTM_NEON64_INTRINSICS')
	if x == 5 and y == 1 and z == 7 and w == 3:
		inner_txt='''		return vtrn2q_f32(input1, input0);'''
		return (inner_txt, 'RTM_NEON64_INTRINSICS')

def rule_neon_ext(x, y, z, w):
	if is_mix_xyzw(x) and (x + 1) == y and (y + 1) == z and (z + 1) == w:
		inner_txt='''		return vextq_f32(input0, input1, {});'''.format(x % 4)
		return (inner_txt, 'RTM_NEON_INTRINSICS')
	if is_mix_abcd(x) and ((x + 1) % 8 == y) and ((y + 1) % 8) == z and ((z + 1) % 8) == w:
		inner_txt='''		return vextq_f32(input1, input0, {});'''.format(x % 4)
		return (inner_txt, 'RTM_NEON_INTRINSICS')

def rule_neon_rev64(x, y, z, w):
	if x == 1 and y == 0 and z == 3 and w == 2:
		inner_txt='''		(void)input1;
		return vrev64q_f32(input0);'''
		return (inner_txt, 'RTM_NEON_INTRINSICS')
	if x == 5 and y == 4 and z == 7 and w == 6:
		inner_txt='''		(void)input0;
		return vrev64q_f32(input1);'''
		return (inner_txt, 'RTM_NEON_INTRINSICS')

def rule_neon_movn(x, y, z, w):
	if x == y and x == z and x == w and is_mix_xyzw(x):
		inner_txt='''		(void)input1;
		return vmovq_n_f32(vgetq_lane_f32(input0, {}));'''.format(x % 4)
		return (inner_txt, 'RTM_NEON_INTRINSICS')
	if x == y and x == z and x == w and is_mix_abcd(x):
		inner_txt='''		(void)input0;
		return vmovq_n_f32(vgetq_lane_f32(input1, {}));'''.format(x % 4)
		return (inner_txt, 'RTM_NEON_INTRINSICS')

def rule_neon_getset(x, y, z, w):
	if is_mix_abcd(x) and y == 1 and z == 2 and w == 3:
		inner_txt='''		return vsetq_lane_f32(vgetq_lane_f32(input1, {}), input0, 0);'''.format(x % 4)
		return (inner_txt, 'RTM_NEON_INTRINSICS')
	if x == 0 and is_mix_abcd(y) and z == 2 and w == 3:
		inner_txt='''		return vsetq_lane_f32(vgetq_lane_f32(input1, {}), input0, 1);'''.format(y % 4)
		return (inner_txt, 'RTM_NEON_INTRINSICS')
	if x == 0 and y == 1 and is_mix_abcd(z) and w == 3:
		inner_txt='''		return vsetq_lane_f32(vgetq_lane_f32(input1, {}), input0, 2);'''.format(z % 4)
		return (inner_txt, 'RTM_NEON_INTRINSICS')
	if x == 0 and y == 1 and z == 2 and is_mix_abcd(w):
		inner_txt='''		return vsetq_lane_f32(vgetq_lane_f32(input1, {}), input0, 3);'''.format(w % 4)
		return (inner_txt, 'RTM_NEON_INTRINSICS')

	if is_mix_xyzw(x) and y == 5 and z == 6 and w == 7:
		inner_txt='''		return vsetq_lane_f32(vgetq_lane_f32(input0, {}), input1, 0);'''.format(x % 4)
		return (inner_txt, 'RTM_NEON_INTRINSICS')
	if x == 4 and is_mix_xyzw(y) and z == 6 and w == 7:
		inner_txt='''		return vsetq_lane_f32(vgetq_lane_f32(input0, {}), input1, 1);'''.format(y % 4)
		return (inner_txt, 'RTM_NEON_INTRINSICS')
	if x == 4 and y == 5 and is_mix_xyzw(z) and w == 7:
		inner_txt='''		return vsetq_lane_f32(vgetq_lane_f32(input0, {}), input1, 2);'''.format(z % 4)
		return (inner_txt, 'RTM_NEON_INTRINSICS')
	if x == 4 and y == 5 and z == 6 and is_mix_xyzw(w):
		inner_txt='''		return vsetq_lane_f32(vgetq_lane_f32(input0, {}), input1, 3);'''.format(w % 4)
		return (inner_txt, 'RTM_NEON_INTRINSICS')

	if is_mix_xyzw(x) and y == 1 and z == 2 and w == 3:
		inner_txt='''		(void)input1;
		return vsetq_lane_f32(vgetq_lane_f32(input0, {}), input0, 0);'''.format(x % 4)
		return (inner_txt, 'RTM_NEON_INTRINSICS')
	if x == 0 and is_mix_xyzw(y) and z == 2 and w == 3:
		inner_txt='''		(void)input1;
		return vsetq_lane_f32(vgetq_lane_f32(input0, {}), input0, 1);'''.format(y % 4)
		return (inner_txt, 'RTM_NEON_INTRINSICS')
	if x == 0 and y == 1 and is_mix_xyzw(z) and w == 3:
		inner_txt='''		(void)input1;
		return vsetq_lane_f32(vgetq_lane_f32(input0, {}), input0, 2);'''.format(z % 4)
		return (inner_txt, 'RTM_NEON_INTRINSICS')
	if x == 0 and y == 1 and z == 2 and is_mix_xyzw(w):
		inner_txt='''		(void)input1;
		return vsetq_lane_f32(vgetq_lane_f32(input0, {}), input0, 3);'''.format(w % 4)
		return (inner_txt, 'RTM_NEON_INTRINSICS')

	if is_mix_abcd(x) and y == 5 and z == 6 and w == 7:
		inner_txt='''		(void)input0;
		return vsetq_lane_f32(vgetq_lane_f32(input1, {}), input1, 0);'''.format(x % 4)
		return (inner_txt, 'RTM_NEON_INTRINSICS')
	if x == 4 and is_mix_abcd(y) and z == 6 and w == 7:
		inner_txt='''		(void)input0;
		return vsetq_lane_f32(vgetq_lane_f32(input1, {}), input1, 1);'''.format(y % 4)
		return (inner_txt, 'RTM_NEON_INTRINSICS')
	if x == 4 and y == 5 and is_mix_abcd(z) and w == 7:
		inner_txt='''		(void)input0;
		return vsetq_lane_f32(vgetq_lane_f32(input1, {}), input1, 2);'''.format(z % 4)
		return (inner_txt, 'RTM_NEON_INTRINSICS')
	if x == 4 and y == 5 and z == 6 and is_mix_abcd(w):
		inner_txt='''		(void)input0;
		return vsetq_lane_f32(vgetq_lane_f32(input1, {}), input1, 3);'''.format(w % 4)
		return (inner_txt, 'RTM_NEON_INTRINSICS')

	inner_x = 'input0' if is_mix_xyzw(x) else 'input1'
	inner_y = 'input0' if is_mix_xyzw(y) else 'input1'
	inner_z = 'input0' if is_mix_xyzw(z) else 'input1'
	inner_w = 'input0' if is_mix_xyzw(w) else 'input1'

	inner_txt='''		(void)input0; (void)input1;
		const float x = vgetq_lane_f32({}, {});
		const float y = vgetq_lane_f32({}, {});
		const float z = vgetq_lane_f32({}, {});
		const float w = vgetq_lane_f32({}, {});
		return vector_set(x, y, z, w);'''.format(inner_x, x % 4, inner_y, y % 4, inner_z, z % 4, inner_w, w % 4)
	return (inner_txt, 'RTM_NEON_INTRINSICS')

rules = []
rules.append(rule_self0)
rules.append(rule_self1)
rules.append(rule_sse4_blend)
rules.append(rule_sse4_insert)
rules.append(rule_sse3_moveldup)
rules.append(rule_sse3_movehdup)
rules.append(rule_sse2_unpacklo)
rules.append(rule_sse2_unpackhi)
rules.append(rule_sse2_movelh)
rules.append(rule_sse2_movehl)
rules.append(rule_sse2_shuffle)
rules.append(rule_neon64_zip1)
rules.append(rule_neon64_zip2)
rules.append(rule_neon64_uzp1)
rules.append(rule_neon64_uzp2)
rules.append(rule_neon64_trn1)
rules.append(rule_neon64_trn2)
rules.append(rule_neon_ext)
rules.append(rule_neon_rev64)
rules.append(rule_neon_movn)
rules.append(rule_neon_getset)

def print_xyzw(x, y, z, w):
	needs_format = True
	inner_txt = ''

	matching_results = []
	is_match_exclusive = False
	matching_cpp_guards = set()

	for rule in rules:
		result = rule(x, y, z, w)
		if result:
			inner_txt, cpp_guard = result

			if cpp_guard in matching_cpp_guards:
				# We already got a match for this cpp guard
				continue

			if cpp_guard:
				continuation = 'el' if matching_results else ''
				guard_txt = '''\
#{}if defined({})
{}
'''
				inner_txt = guard_txt.format(continuation, cpp_guard, inner_txt)

			matching_results.append(inner_txt)
			matching_cpp_guards.add(cpp_guard)

			if not cpp_guard:
				# A trivial rule without a cpp guard has matched, these are exclusive
				is_match_exclusive = True
				break

	if not matching_results:
		print('No matching rule found for: {}, {}, {}, {}'.format(x, y, z, w))
		sys.exit(1)

	if not is_match_exclusive:
		# Add non-simd fallback
		inner_txt = '''\
#else
		(void)input0; (void)input1;
		return vector4f{{ {}.{}, {}.{}, {}.{}, {}.{} }};
#endif'''

		inner_x = 'input0' if is_mix_xyzw(x) else 'input1'
		inner_y = 'input0' if is_mix_xyzw(y) else 'input1'
		inner_z = 'input0' if is_mix_xyzw(z) else 'input1'
		inner_w = 'input0' if is_mix_xyzw(w) else 'input1'

		selector_x = get_selector(x)
		selector_y = get_selector(y)
		selector_z = get_selector(z)
		selector_w = get_selector(w)

		inner_txt = inner_txt.format(inner_x, selector_x, inner_y, selector_y, inner_z, selector_z, inner_w, selector_w)

		matching_results.append(inner_txt)

	vector_mix_inner_txt = ''.join(matching_results)

	txt ='''	template<>
	RTM_DISABLE_SECURITY_COOKIE_CHECK RTM_FORCE_INLINE vector4f RTM_SIMD_CALL
		vector_mix<{}, {}, {}, {}>(vector4f_arg0 input0, vector4f_arg1 input1) RTM_NO_EXCEPT
	{{
		// [{}{}{}{}]
{}
	}}
	'''

	print(txt.format(idx_to_mix(x), idx_to_mix(y), idx_to_mix(z), idx_to_mix(w), idx_to_mix_short(x), idx_to_mix_short(y), idx_to_mix_short(z), idx_to_mix_short(w), vector_mix_inner_txt))

def print_xyz_(x, y, z):
	print_xyzw(x, y, z, 0)
	print_xyzw(x, y, z, 1)
	print_xyzw(x, y, z, 2)
	print_xyzw(x, y, z, 3)
	print_xyzw(x, y, z, 4)
	print_xyzw(x, y, z, 5)
	print_xyzw(x, y, z, 6)
	print_xyzw(x, y, z, 7)

def print_xy__(x, y):
	print_xyz_(x, y, 0)
	print_xyz_(x, y, 1)
	print_xyz_(x, y, 2)
	print_xyz_(x, y, 3)
	print_xyz_(x, y, 4)
	print_xyz_(x, y, 5)
	print_xyz_(x, y, 6)
	print_xyz_(x, y, 7)

def print_x___(x):
	print_xy__(x, 0)
	print_xy__(x, 1)
	print_xy__(x, 2)
	print_xy__(x, 3)
	print_xy__(x, 4)
	print_xy__(x, 5)
	print_xy__(x, 6)
	print_xy__(x, 7)

if __name__ == "__main__":
	print_x___(0)
	print_x___(1)
	print_x___(2)
	print_x___(3)
	print_x___(4)
	print_x___(5)
	print_x___(6)
	print_x___(7)

```

`tools/release_scripts/README.md`:

```md
# Release scripts

Validating a release requires a lot of work and to that end, some scripts were written to automate the process as much as possible.

## test_everything.py

This script runs the unit tests on every platform except iOS and Android. It will run every permutation possible.

```

`tools/release_scripts/test_everything.py`:

```py
import os
import platform
import shutil
import subprocess
import sys

def get_platform_compilers():
	if platform.system() == 'Windows':
		return [ 'vs2015', 'vs2017', 'vs2019', 'vs2019-clang' ]
	elif platform.system() == 'Linux':
		compilers = []
		if shutil.which('g++-5'):
			compilers.append('gcc5')
		if shutil.which('g++-6'):
			compilers.append('gcc6')
		if shutil.which('g++-7'):
			compilers.append('gcc7')
		if shutil.which('g++-8'):
			compilers.append('gcc8')
		if shutil.which('g++-9'):
			compilers.append('gcc9')
		if shutil.which('g++-10'):
			compilers.append('gcc10')

		if shutil.which('clang++-4.0'):
			compilers.append('clang4')
		if shutil.which('clang++-5.0'):
			compilers.append('clang5')
		if shutil.which('clang++-6.0'):
			compilers.append('clang6')
		if shutil.which('clang++-7'):
			compilers.append('clang7')
		if shutil.which('clang++-8'):
			compilers.append('clang8')
		if shutil.which('clang++-9'):
			compilers.append('clang9')
		if shutil.which('clang++-10'):
			compilers.append('clang10')
		if shutil.which('clang++-11'):
			compilers.append('clang11')

		return compilers
	elif platform.system() == 'Darwin':
		return [ 'osx' ]
	else:
		print('Unknown platform!')
		sys.exit(1)

def get_python_exe_name():
	if platform.system() == 'Windows':
		return 'python'
	else:
		return 'python3'

if __name__ == "__main__":
	os.environ['PYTHONIOENCODING'] = 'utf_8'

	configs = [ 'debug', 'release' ]
	archs = [ 'x86', 'x64' ]
	compilers = get_platform_compilers()
	simd_opts = [ '', '-avx', '-avx2', '-nosimd' ]
	python_exe = get_python_exe_name()

	if platform.system() == 'Darwin':
		result = subprocess.check_output(['xcodebuild', '-version']).decode("utf-8")
		if 'Xcode 11' in result:
			archs.remove('x86')

	cmd_args = []
	for config in configs:
		for arch in archs:
			for compiler in compilers:
				for simd in simd_opts:
					if compiler == 'clang7' and simd == '-nosimd':
						continue	# Hack to avoid compiler issue
					if compiler == 'clang8' and simd == '-nosimd':
						continue	# Hack to avoid compiler issue
					args = [python_exe, 'make.py', '-compiler', compiler, '-cpu', arch, '-config', config, simd, '-build', '-unit_test', '-vector_mix_test', '-clean']
					cmd_args.append([x for x in args if x])

	if platform.system() == 'Windows':
		for config in configs:
			# Windows ARM
			args = [python_exe, 'make.py', '-compiler', 'vs2017', '-cpu', 'arm64', '-config', config, '-build', '-vector_mix_test', '-clean']
			cmd_args.append([x for x in args if x])
			args = [python_exe, 'make.py', '-compiler', 'vs2017', '-cpu', 'arm64', '-config', config, '-build', '-vector_mix_test', '-clean', '-nosimd']
			cmd_args.append([x for x in args if x])
			args = [python_exe, 'make.py', '-compiler', 'vs2019', '-cpu', 'arm64', '-config', config, '-build', '-vector_mix_test', '-clean']
			cmd_args.append([x for x in args if x])
			args = [python_exe, 'make.py', '-compiler', 'vs2019', '-cpu', 'arm64', '-config', config, '-build', '-vector_mix_test', '-clean', '-nosimd']
			cmd_args.append([x for x in args if x])

			# Android
			args = [python_exe, 'make.py', '-compiler', 'android', '-cpu', 'armv7', '-config', config, '-build', '-vector_mix_test', '-clean']
			cmd_args.append([x for x in args if x])
			args = [python_exe, 'make.py', '-compiler', 'android', '-cpu', 'armv7', '-config', config, '-build', '-vector_mix_test', '-clean', '-nosimd']
			cmd_args.append([x for x in args if x])
			args = [python_exe, 'make.py', '-compiler', 'android', '-cpu', 'arm64', '-config', config, '-build', '-vector_mix_test', '-clean']
			cmd_args.append([x for x in args if x])
			args = [python_exe, 'make.py', '-compiler', 'android', '-cpu', 'arm64', '-config', config, '-build', '-vector_mix_test', '-clean', '-nosimd']
			cmd_args.append([x for x in args if x])
	elif platform.system() == 'Darwin':
		for config in configs:
			# iOS
			args = [python_exe, 'make.py', '-compiler', 'ios', '-config', config, '-build', '-vector_mix_test', '-clean']
			cmd_args.append([x for x in args if x])
			args = [python_exe, 'make.py', '-compiler', 'ios', '-config', config, '-build', '-vector_mix_test', '-clean', '-nosimd']
			cmd_args.append([x for x in args if x])

	if platform.system() == 'Darwin' or platform.system() == 'Linux':
		# Emscripten
		args = [python_exe, 'make.py', '-compiler', 'emscripten', '-config', 'debug', '-build', '-vector_mix_test', '-clean']
		cmd_args.append([x for x in args if x])
		args = [python_exe, 'make.py', '-compiler', 'emscripten', '-config', 'release', '-build', '-unit_test', '-vector_mix_test', '-clean']
		cmd_args.append([x for x in args if x])

	root_dir = os.path.join(os.getcwd(), '../..')
	os.chdir(root_dir)

	for args in cmd_args:
		cmd = " ".join(args)
		print('Running command: "{}" ...'.format(cmd))
		try:
			if 'android' in args:
				subprocess.check_call(args)
			else:
				subprocess.check_output(args)
		except subprocess.CalledProcessError as e:
			print('Failed command: {}'.format(cmd))
			print(e.output.decode(sys.stdout.encoding))
			sys.exit(1)

	print('Done!')
	sys.exit(0)

```

`tools/setup_linux_compiler.sh`:

```sh
#!/usr/bin/env bash

# Extract our command line arguments
COMPILER=$1

# Convert our GCC compiler into a list of packages it needs
if [[ $COMPILER == gcc5 ]]; then
    PACKAGES="g++-5 g++-5-multilib g++-multilib"
elif [[ $COMPILER == gcc6 ]]; then
    PACKAGES="g++-6 g++-6-multilib g++-multilib"
elif [[ $COMPILER == gcc7 ]]; then
    PACKAGES="g++-7 g++-7-multilib g++-multilib"
elif [[ $COMPILER == gcc8 ]]; then
    PACKAGES="g++-8 g++-8-multilib g++-multilib"
elif [[ $COMPILER == gcc9 ]]; then
    PACKAGES="g++-9 g++-9-multilib g++-multilib"
elif [[ $COMPILER == gcc10 ]]; then
    PACKAGES="g++-10 g++-10-multilib g++-multilib"
fi

# If using clang, add our apt source key
if [[ $COMPILER == clang* ]]; then
    curl -sSL "http://apt.llvm.org/llvm-snapshot.gpg.key" | sudo -E apt-key add - ;
fi

# Convert our clang compiler into a list of packages it needs and its source
if [[ $COMPILER == clang4 ]]; then
    # clang4 isn't available after xenial
    PACKAGES="clang-4.0 libstdc++-5-dev libc6-dev-i386 g++-5-multilib g++-multilib"
    echo "deb http://apt.llvm.org/xenial/ llvm-toolchain-xenial-4.0 main" | sudo tee -a /etc/apt/sources.list > /dev/null ;
elif [[ $COMPILER == clang5 ]]; then
    PACKAGES="clang-5.0 libstdc++-5-dev libc6-dev-i386 g++-5-multilib g++-multilib"
    echo "deb http://apt.llvm.org/bionic/ llvm-toolchain-bionic-5.0 main" | sudo tee -a /etc/apt/sources.list > /dev/null ;
elif [[ $COMPILER == clang6 ]]; then
    PACKAGES="clang-6.0 libstdc++-5-dev libc6-dev-i386 g++-5-multilib g++-multilib"
    echo "deb http://apt.llvm.org/bionic/ llvm-toolchain-bionic-6.0 main" | sudo tee -a /etc/apt/sources.list > /dev/null ;
elif [[ $COMPILER == clang7 ]]; then
    PACKAGES="clang-7 libstdc++-5-dev libc6-dev-i386 g++-5-multilib g++-multilib"
    echo "deb http://apt.llvm.org/bionic/ llvm-toolchain-bionic-7 main" | sudo tee -a /etc/apt/sources.list > /dev/null ;
elif [[ $COMPILER == clang8 ]]; then
    PACKAGES="clang-8 libstdc++-5-dev libc6-dev-i386 g++-5-multilib g++-multilib"
    echo "deb http://apt.llvm.org/bionic/ llvm-toolchain-bionic-8 main" | sudo tee -a /etc/apt/sources.list > /dev/null ;
elif [[ $COMPILER == clang9 ]]; then
    PACKAGES="clang-9 libstdc++-5-dev libc6-dev-i386 g++-5-multilib g++-multilib"
    echo "deb http://apt.llvm.org/bionic/ llvm-toolchain-bionic-9 main" | sudo tee -a /etc/apt/sources.list > /dev/null ;
elif [[ $COMPILER == clang10 ]]; then
    PACKAGES="clang-10 libstdc++-5-dev libc6-dev-i386 g++-5-multilib g++-multilib"
    echo "deb http://apt.llvm.org/bionic/ llvm-toolchain-bionic-10 main" | sudo tee -a /etc/apt/sources.list > /dev/null ;
elif [[ $COMPILER == clang11 ]]; then
    PACKAGES="clang-11 libstdc++-5-dev libc6-dev-i386 g++-5-multilib g++-multilib"
    echo "deb http://apt.llvm.org/bionic/ llvm-toolchain-bionic-11 main" | sudo tee -a /etc/apt/sources.list > /dev/null ;
fi

# Install the packages we need
sudo -E apt-add-repository -y "ppa:ubuntu-toolchain-r/test";
sudo -E apt-get -yq update;
sudo -E apt-get -yq --no-install-suggests --no-install-recommends --force-yes install $PACKAGES;

```

`tools/setup_osx_compiler.sh`:

```sh
#!/usr/bin/env bash

# Extract our command line arguments
COMPILER=$1

# See Github hosted runners:
# macos-14: https://github.com/actions/runner-images/blob/main/images/macos/macos-14-arm64-Readme.md
#   xcode 15.0.1, 15.1, 15.2, 15.3, 15.4, 16.1, 16.2
# macos-13: https://github.com/actions/runner-images/blob/main/images/macos/macos-13-Readme.md
#   xcode 14.1, 14.2, 14.3.1, 15.0.1, 15.1, 15.2
# macos-12: https://github.com/actions/runner-images/blob/main/images/macos/macos-12-Readme.md
#   xcode 13.1, 12.2.1, 13.3.1, 13.4.1, 14.1, 14.2
# maxos-11: https://github.com/actions/runner-images/blob/main/images/macos/macos-11-Readme.md
#   xcode 11.7, 12.4, 12.5.1, 13.0, 13.1, 13.2.1
# maxos-10.15: https://github.com/actions/runner-images/blob/main/images/macos/macos-10.15-Readme.md
#   xcode 10.3, 11.2.1, 11.3.1, 11.4.1, 11.5, 11.6, 11.7, 12, 12.1, 12.1.1, 12.2, 12.3, 12.4

# Convert our compiler string into our XCode path
# Paths must match Github Action virtual images
if [[ $COMPILER == xcode10 ]]; then
    XCODE_PATH="/Applications/Xcode_10.3.app"
elif [[ $COMPILER == xcode11 ]]; then
    XCODE_PATH="/Applications/Xcode_11.7.app"
elif [[ $COMPILER == xcode12 ]]; then
    XCODE_PATH="/Applications/Xcode_12.5.1.app"
elif [[ $COMPILER == xcode13 ]]; then
    XCODE_PATH="/Applications/Xcode_13.2.1.app"
elif [[ $COMPILER == xcode14 ]]; then
    XCODE_PATH="/Applications/Xcode_14.3.1.app"
elif [[ $COMPILER == xcode15 ]]; then
    XCODE_PATH="/Applications/Xcode_15.4.app"
elif [[ $COMPILER == xcode16 ]]; then
    XCODE_PATH="/Applications/Xcode_16.2.app"
fi

# Select our XCode version
sudo xcode-select -s $XCODE_PATH/Contents/Developer;

```

`tools/vs_visualizers/rtm.natvis`:

```natvis
<?xml version="1.0" encoding="utf-8"?>

<AutoVisualizer xmlns="http://schemas.microsoft.com/vstudio/debugger/natvis/2010">

	<Type Name="rtm::vector4f" Priority="MediumLow">
		<DisplayString>({x}, {y}, {z}, {w})</DisplayString>
	</Type>

	<Type Name="rtm::quatf" Priority="MediumLow">
		<DisplayString>({x}, {y}, {z}, {w})</DisplayString>
	</Type>

	<Type Name="__m128" Priority="Low">
		<DisplayString>({m128_f32[0]}, {m128_f32[1]}, {m128_f32[2]}, {m128_f32[3]})</DisplayString>
	</Type>

	<Type Name="__m128i" Priority="Low">
		<DisplayString>({m128i_u32[0]}, {m128i_u32[1]}, {m128i_u32[2]}, {m128i_u32[3]})</DisplayString>
	</Type>

	<Type Name="__m128d" Priority="Low">
		<DisplayString>({m128d_f64[0]}, {m128d_f64[1]})</DisplayString>
	</Type>

	<Type Name="rtm::vector4d" Priority="MediumLow">
		<DisplayString>({x}, {y}, {z}, {w})</DisplayString>
	</Type>

	<Type Name="rtm::vector4d">
		<DisplayString>({xy.m128d_f64[0]}, {xy.m128d_f64[1]}, {zw.m128d_f64[0]}, {zw.m128d_f64[1]})</DisplayString>
	</Type>

	<Type Name="rtm::quatd" Priority="MediumLow">
		<DisplayString>({x}, {y}, {z}, {w})</DisplayString>
	</Type>

	<Type Name="rtm::quatd">
		<DisplayString>({xy.m128d_f64[0]}, {xy.m128d_f64[1]}, {zw.m128d_f64[0]}, {zw.m128d_f64[1]})</DisplayString>
	</Type>

	<Type Name="rtm::rtm_impl::angle_constant">
		<DisplayString>Degrees: {dbl * (180.0 / 3.14159265358979323846)}</DisplayString>
	</Type>

	<Type Name="rtm::float2f">
		<DisplayString>({x}, {y})</DisplayString>
	</Type>

	<Type Name="rtm::float3f">
		<DisplayString>({x}, {y}, {z})</DisplayString>
	</Type>

	<Type Name="rtm::float4f">
		<DisplayString>({x}, {y}, {z}, {w})</DisplayString>
	</Type>

	<Type Name="rtm::float2d">
		<DisplayString>({x}, {y})</DisplayString>
	</Type>

	<Type Name="rtm::float3d">
		<DisplayString>({x}, {y}, {z})</DisplayString>
	</Type>

	<Type Name="rtm::float4d">
		<DisplayString>({x}, {y}, {z}, {w})</DisplayString>
	</Type>

</AutoVisualizer>

```