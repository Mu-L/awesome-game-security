Project Path: arc_ck0i_Kernelcloak_z3_q724p

Source Tree:

```txt
arc_ck0i_Kernelcloak_z3_q724p
├── LICENSE
├── README.md
├── config.h
├── core
│   ├── array.h
│   ├── memory.h
│   ├── random.h
│   ├── string_utils.h
│   ├── sync.h
│   └── types.h
├── crypto
│   ├── hash.h
│   ├── xor_cipher.h
│   └── xtea.h
├── kernelcloak.h
├── obfuscation
│   ├── boolean.h
│   ├── cfg_flatten.h
│   ├── cfg_protect.h
│   ├── compare.h
│   ├── control_flow.h
│   ├── mba.h
│   └── value.h
├── security
│   ├── anti_debug.h
│   ├── anti_vm.h
│   ├── import_hiding.h
│   ├── integrity.h
│   └── pe_erase.h
└── strings
    ├── encrypted_string.h
    ├── encrypted_wstring.h
    ├── layered_string.h
    └── stack_string.h

```

`LICENSE`:

```
MIT License

Copyright (c) 2026

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

```

`README.md`:

```md
# KernelCloak

Header-only C++17 obfuscation library designed exclusively for Windows kernel-mode drivers (WDM/KMDF/WDF).

A spiritual successor to [Cloakwork](https://github.com/ck0i/Cloakwork), but where Cloakwork's kernel support is mostly no-ops, KernelCloak makes **every feature fully functional at IRQL-aware ring 0**.

## Features

### String Encryption
- `KC_STR("text")` - compile-time encrypted, stack-decrypted strings with no static destructors
- `KC_WSTR(L"text")` - wide string variant
- `KC_STACK_STR(name, 'h','e','l','l','o','\0')` - char-by-char construction, zero string literals
- `KC_STR_LAYERED("text")` - triple-layer encryption (XOR + XTEA + byte shuffle) with runtime re-keying

### Value Obfuscation
- `KC_INT(x)` - XOR-encoded integer/pointer storage with volatile decode
- `KC_ADD/SUB/AND/OR/XOR/NEG` - Mixed Boolean Arithmetic operations
- `KC_EQ/NE/LT/GT/LE/GE` - obfuscated comparisons via MBA

### Control Flow
- `KC_IF/KC_ELSE/KC_ENDIF` - conditionals with opaque predicate injection
- `KC_JUNK()` / `KC_JUNK_FLOW()` - dead code insertion
- `KC_FLAT_FUNC/BLOCK/GOTO/IF/RETURN/END` - full CFG flattening with XOR-encrypted state transitions
- `KC_PROTECT(ret_type, body)` - lightweight CFG protection wrapper
- `KC_TRUE/KC_FALSE/KC_BOOL` - opaque predicates sourced from `__rdtsc`, stack entropy, kernel APIs

### Cryptographic Primitives
- XTEA block cipher - fully constexpr encrypt, runtime decrypt
- Rolling XOR cipher - lightweight fast-path encryption
- FNV-1a hash - 32/64-bit, case-insensitive, wide string variants

### Security
- **Anti-Debug** - `KdDebuggerEnabled`, hardware breakpoint detection, RDTSC timing checks
- **Anti-VM** - CPUID hypervisor detection, MSR checks, registry artifact scanning
- **Integrity** - hook detection, function prologue hashing, .text section self-checksum
- **PE Erase** - zero DOS/NT headers and section table after load
- **Import Hiding** - PsLoadedModuleList walking with hash-based export resolution, forwarded export support

### Core Primitives
- `KernelBuffer<T>` - RAII pool allocation with move semantics
- `KSpinLock` - RAII spinlock with IRQL save/restore guards
- `KernelAtomic<T>` - lock-free atomics via Interlocked* intrinsics
- `KArray<T, N>` - stack-allocated constexpr array
- Compile-time and runtime PRNG (xorshift32 + lock-free splitmix64, seeded from kernel entropy)

## Quick Start

```cpp
#include "kernelcloak.h"

NTSTATUS DriverEntry(PDRIVER_OBJECT DriverObject, PUNICODE_STRING RegistryPath) {
    // encrypted strings - plaintext never in binary
    DbgPrint("%s\n", KC_STR("driver loaded successfully"));

    // obfuscated values - constants not visible in disassembly
    auto magic = KC_INT(0xDEADBEEF);
    auto result = KC_ADD(magic, KC_INT(0x1000));

    // hidden imports - functions not in IAT
    auto fn = reinterpret_cast<decltype(&MmGetSystemRoutineAddress)>(
        KC_IMPORT("ntoskrnl.exe", "MmGetSystemRoutineAddress")
    );

    // anti-debug/anti-vm
    if (KC_IS_DEBUGGED()) {
        return STATUS_ACCESS_DENIED;
    }

    // PE header erasure
    KC_ERASE_PE_HEADER();

    return STATUS_SUCCESS;
}
```

## Usage

Single header include:

```cpp
#include "kernelcloak.h"
```

Or include only what you need:

```cpp
#include "strings/encrypted_string.h"
#include "obfuscation/value.h"
```

## Configuration

Define before including to customize:

```cpp
#define KC_ENABLE_ALL 0                    // disable everything by default
#define KC_ENABLE_STRING_ENCRYPTION 1      // enable only what you need
#define KC_ENABLE_VALUE_OBFUSCATION 1
#define KC_POOL_TAG 'myTg'                 // custom pool tag
#define KC_ANTI_DEBUG_RESPONSE 0           // 0=ignore, 1=bugcheck, 2=corrupt
#define KC_ANTI_VM_RESPONSE 0              // independent anti-vm response (defaults to anti-debug response)
#include "kernelcloak.h"
```

When a feature is disabled, its macros compile to passthrough/no-ops with zero overhead.

## Requirements

- Windows Driver Kit (WDK) 10+
- Visual Studio 2019+ with C++17 support
- Windows 10/11 x64 target

## Design Principles

1. **Header-only** - no .lib or .cpp files to compile
2. **C++17** - no C++20 features (concepts, consteval, ranges)
3. **IRQL-aware** - PASSIVE-only paths are documented/gated where relevant
4. **Zero dependencies** - only WDK headers + compiler intrinsics
5. **Unique per build** - compile-time PRNG seeded from `__TIME__`, `__COUNTER__`, `__LINE__`
6. **No unintended BSOD** - failure paths return false/null; anti-debug/anti-vm responses are configurable
7. **No CRT** - no static destructors, no atexit, no STL

## What Makes This Different From Cloakwork

| Cloakwork Kernel Gap | KernelCloak Solution |
|---|---|
| `CW_STR()` - no-op (needs atexit) | Stack-decrypt pattern, no static destructors |
| `CW_INT()` - no-op (needs C++20 concepts) | SFINAE-based C++17 implementation |
| `CW_IF()` - regular if (depends on MBA + concepts) | Standalone C++17 MBA + opaque predicates |
| `CW_IMPORT()` - unavailable (PEB walking) | PsLoadedModuleList + export directory parsing |
| `CW_ANTI_VM()` - no-op (usermode APIs) | Kernel-native CPUID/MSR/registry/SMBIOS |
| Integrity checks unavailable (VirtualQuery) | Direct PE section parsing from driver base |

## Project Structure

```
Kernelcloak/
│   ├── kernelcloak.h       # master header
│   ├── config.h            # feature toggles
│   ├── core/               # kernel-safe primitives
│   ├── crypto/             # encryption primitives
│   ├── strings/            # string obfuscation
│   ├── obfuscation/        # value/control flow
│   └── security/           # anti-debug/vm/integrity
```

## License

MIT

```

`config.h`:

```h
#pragma once

// KernelCloak Configuration
// Define these before including kernelcloak.h to customize behavior.

// master switch
#ifndef KC_ENABLE_ALL
#define KC_ENABLE_ALL 1
#endif

// feature toggles (all default to KC_ENABLE_ALL)
#ifndef KC_ENABLE_STRING_ENCRYPTION
#define KC_ENABLE_STRING_ENCRYPTION KC_ENABLE_ALL
#endif

#ifndef KC_ENABLE_VALUE_OBFUSCATION
#define KC_ENABLE_VALUE_OBFUSCATION KC_ENABLE_ALL
#endif

#ifndef KC_ENABLE_CONTROL_FLOW
#define KC_ENABLE_CONTROL_FLOW KC_ENABLE_ALL
#endif

#ifndef KC_ENABLE_ANTI_DEBUG
#define KC_ENABLE_ANTI_DEBUG KC_ENABLE_ALL
#endif

#ifndef KC_ENABLE_ANTI_VM
#define KC_ENABLE_ANTI_VM KC_ENABLE_ALL
#endif

#ifndef KC_ENABLE_IMPORT_HIDING
#define KC_ENABLE_IMPORT_HIDING KC_ENABLE_ALL
#endif

#ifndef KC_ENABLE_INTEGRITY
#define KC_ENABLE_INTEGRITY KC_ENABLE_ALL
#endif

#ifndef KC_ENABLE_PE_ERASE
#define KC_ENABLE_PE_ERASE KC_ENABLE_ALL
#endif

#ifndef KC_ENABLE_CFG_FLATTEN
#define KC_ENABLE_CFG_FLATTEN KC_ENABLE_ALL
#endif

#ifndef KC_ENABLE_MBA
#define KC_ENABLE_MBA KC_ENABLE_ALL
#endif

#ifndef KC_ENABLE_BOOLEAN_OBFUSCATION
#define KC_ENABLE_BOOLEAN_OBFUSCATION KC_ENABLE_ALL
#endif

// behavioral config
#ifndef KC_IMPORT_HIDING_LOCK_MODULE_LIST
#define KC_IMPORT_HIDING_LOCK_MODULE_LIST 1
#endif

#ifndef KC_POOL_TAG
#define KC_POOL_TAG 'kcLK'
#endif

#ifndef KC_ANTI_DEBUG_RESPONSE
#define KC_ANTI_DEBUG_RESPONSE 1  // 0=ignore, 1=KeBugCheck, 2=corrupt state
#endif

// anti-vm can (optionally) share the same response as anti-debug, but keep it configurable
#ifndef KC_ANTI_VM_RESPONSE
#define KC_ANTI_VM_RESPONSE KC_ANTI_DEBUG_RESPONSE
#endif

#ifndef KC_LAYERED_REKEY_INTERVAL
#define KC_LAYERED_REKEY_INTERVAL 1000
#endif

#ifndef KC_XTEA_ROUNDS
#define KC_XTEA_ROUNDS 32
#endif

// compiler intrinsics and annotations
#ifdef _MSC_VER
#define KC_FORCEINLINE __forceinline
#define KC_NOINLINE __declspec(noinline)
#define KC_NOVTABLE __declspec(novtable)
#define KC_SELECTANY __declspec(selectany)
#else
#define KC_FORCEINLINE inline __attribute__((always_inline))
#define KC_NOINLINE __attribute__((noinline))
#define KC_NOVTABLE
#define KC_SELECTANY __attribute__((weak))
#endif

// suppress common WDK warnings
#pragma warning(disable: 4201) // nameless struct/union
#pragma warning(disable: 4324) // structure padded due to alignment
#pragma warning(disable: 4471) // forward declaration of unscoped enum

```

`core/array.h`:

```h
#pragma once
#include "../config.h"
#include "types.h"

namespace kernelcloak {
namespace core {

// stack-allocated fixed-size array, constexpr-friendly, no heap
// aggregate type when possible for brace-init
template<typename T, size_t N>
struct KArray {
    static_assert(N > 0, "KArray size must be > 0");

    // public data member for aggregate initialization
    T m_data[N];

    // element access
    KC_FORCEINLINE constexpr T& operator[](size_t i) noexcept { return m_data[i]; }
    KC_FORCEINLINE constexpr const T& operator[](size_t i) const noexcept { return m_data[i]; }

    KC_FORCEINLINE constexpr T& at(size_t i) noexcept { return m_data[i]; }
    KC_FORCEINLINE constexpr const T& at(size_t i) const noexcept { return m_data[i]; }

    KC_FORCEINLINE constexpr T& front() noexcept { return m_data[0]; }
    KC_FORCEINLINE constexpr const T& front() const noexcept { return m_data[0]; }

    KC_FORCEINLINE constexpr T& back() noexcept { return m_data[N - 1]; }
    KC_FORCEINLINE constexpr const T& back() const noexcept { return m_data[N - 1]; }

    // capacity
    KC_FORCEINLINE static constexpr size_t size() noexcept { return N; }
    KC_FORCEINLINE static constexpr bool empty() noexcept { return false; }
    KC_FORCEINLINE static constexpr size_t max_size() noexcept { return N; }

    // data access
    KC_FORCEINLINE constexpr T* data() noexcept { return m_data; }
    KC_FORCEINLINE constexpr const T* data() const noexcept { return m_data; }

    // iterators
    KC_FORCEINLINE constexpr T* begin() noexcept { return m_data; }
    KC_FORCEINLINE constexpr const T* begin() const noexcept { return m_data; }
    KC_FORCEINLINE constexpr T* end() noexcept { return m_data + N; }
    KC_FORCEINLINE constexpr const T* end() const noexcept { return m_data + N; }

    // fill
    KC_FORCEINLINE constexpr void fill(const T& value) noexcept {
        for (size_t i = 0; i < N; ++i) {
            m_data[i] = value;
        }
    }

    // swap
    KC_FORCEINLINE constexpr void swap(KArray& other) noexcept {
        for (size_t i = 0; i < N; ++i) {
            T tmp = detail::kc_move(m_data[i]);
            m_data[i] = detail::kc_move(other.m_data[i]);
            other.m_data[i] = detail::kc_move(tmp);
        }
    }
};

// deduction guide
template<typename T, typename... U>
KArray(T, U...) -> KArray<T, 1 + sizeof...(U)>;

// zero-size specialization - still valid type, just empty
template<typename T>
struct KArray<T, 0> {
    KC_FORCEINLINE static constexpr size_t size() noexcept { return 0; }
    KC_FORCEINLINE static constexpr bool empty() noexcept { return true; }
    KC_FORCEINLINE static constexpr size_t max_size() noexcept { return 0; }
    KC_FORCEINLINE constexpr T* data() noexcept { return nullptr; }
    KC_FORCEINLINE constexpr const T* data() const noexcept { return nullptr; }
    KC_FORCEINLINE constexpr T* begin() noexcept { return nullptr; }
    KC_FORCEINLINE constexpr const T* begin() const noexcept { return nullptr; }
    KC_FORCEINLINE constexpr T* end() noexcept { return nullptr; }
    KC_FORCEINLINE constexpr const T* end() const noexcept { return nullptr; }
    KC_FORCEINLINE constexpr void fill(const T&) noexcept {}
    KC_FORCEINLINE constexpr void swap(KArray&) noexcept {}
};

} // namespace core
} // namespace kernelcloak

```

`core/memory.h`:

```h
#pragma once
#include "../config.h"
#include "types.h"

#pragma warning(push)
#pragma warning(disable: 4471)  // forward decl of unscoped enum (POOL_TYPE)
#pragma warning(disable: 4005)  // macro redefinition (ntddk vs wdm overlap)

extern "C" {
    // pool allocation - actual ntoskrnl exports
#if !defined(_NTDDK_) && !defined(_WDMDDK_)
    void* __stdcall ExAllocatePool2(
        unsigned __int64 Flags,
        kernelcloak::size_t NumberOfBytes,
        unsigned long Tag
    );

    void __stdcall ExFreePoolWithTag(
        void* P,
        unsigned long Tag
    );
#endif

    // kernel memcpy/memset/memmove - available as compiler intrinsics and/or kernel exports.
    // note: Rtl*Memory helpers are macros in WDK headers; avoid declaring them here.
    void* __cdecl memcpy(void* dst, const void* src, kernelcloak::size_t size);
    void* __cdecl memset(void* dst, int val, kernelcloak::size_t size);
    void* __cdecl memmove(void* dst, const void* src, kernelcloak::size_t size);
}

#ifndef POOL_FLAG_NON_PAGED
#define POOL_FLAG_NON_PAGED 0x0000000000000040ULL
#endif

#ifndef POOL_FLAG_PAGED
#define POOL_FLAG_PAGED 0x0000000000000100ULL
#endif

// placement new - kernel mode builds often don't include <new>. avoid redefining if <new> was included.
#if !defined(_KC_PLACEMENT_NEW_DEFINED) && !defined(_NEW_) && !defined(_INC_NEW)
#define _KC_PLACEMENT_NEW_DEFINED
inline void* operator new(kernelcloak::size_t, void* p) noexcept { return p; }
inline void* operator new[](kernelcloak::size_t, void* p) noexcept { return p; }
inline void operator delete(void*, void*) noexcept {}
inline void operator delete[](void*, void*) noexcept {}
#endif

namespace kernelcloak {
namespace core {

KC_FORCEINLINE void* kc_memcpy(void* dst, const void* src, size_t n) {
    memcpy(dst, src, n);
    return dst;
}

KC_FORCEINLINE void* kc_memset(void* dst, int val, size_t n) {
    memset(dst, val, n);
    return dst;
}

KC_FORCEINLINE void* kc_memmove(void* dst, const void* src, size_t n) {
    memmove(dst, src, n);
    return dst;
}

KC_FORCEINLINE void kc_memzero(void* dst, size_t n) {
    memset(dst, 0, n);
}

KC_FORCEINLINE void* kc_pool_alloc(size_t size, uint64_t flags = POOL_FLAG_NON_PAGED, uint32_t tag = KC_POOL_TAG) {
    return ExAllocatePool2(flags, size, tag);
}

KC_FORCEINLINE void kc_pool_free(void* ptr, uint32_t tag = KC_POOL_TAG) {
    if (ptr) {
        ExFreePoolWithTag(ptr, tag);
    }
}

// RAII kernel pool buffer
// non-paged by default (safe at DISPATCH_LEVEL), supports move, deleted copy
template<typename T>
class KernelBuffer {
    T* m_ptr = nullptr;
    size_t m_size = 0;
    uint32_t m_tag = KC_POOL_TAG;

public:
    KernelBuffer() noexcept = default;

    explicit KernelBuffer(size_t count, uint64_t flags = POOL_FLAG_NON_PAGED, uint32_t tag = KC_POOL_TAG) noexcept
        : m_tag(tag)
    {
        if (count == 0)
            return;

        constexpr size_t kc_size_max = ~static_cast<size_t>(0);
        if (count > (kc_size_max / sizeof(T)))
            return;

        m_size = count * sizeof(T);
        m_ptr = static_cast<T*>(ExAllocatePool2(flags, m_size, m_tag));
        if (m_ptr) {
            kc_memzero(m_ptr, m_size);
        } else {
            m_size = 0;
        }
    }

    ~KernelBuffer() noexcept {
        reset();
    }

    KernelBuffer(KernelBuffer&& other) noexcept
        : m_ptr(detail::kc_exchange(other.m_ptr, nullptr))
        , m_size(detail::kc_exchange(other.m_size, static_cast<size_t>(0)))
        , m_tag(other.m_tag)
    {}

    KernelBuffer& operator=(KernelBuffer&& other) noexcept {
        if (this != &other) {
            reset();
            m_ptr = detail::kc_exchange(other.m_ptr, nullptr);
            m_size = detail::kc_exchange(other.m_size, static_cast<size_t>(0));
            m_tag = other.m_tag;
        }
        return *this;
    }

    KernelBuffer(const KernelBuffer&) = delete;
    KernelBuffer& operator=(const KernelBuffer&) = delete;

    void reset() noexcept {
        if (auto* p = detail::kc_exchange(m_ptr, nullptr)) {
            ExFreePoolWithTag(p, m_tag);
        }
        m_size = 0;
    }

    KC_FORCEINLINE T* get() noexcept { return m_ptr; }
    KC_FORCEINLINE const T* get() const noexcept { return m_ptr; }
    KC_FORCEINLINE size_t size_bytes() const noexcept { return m_size; }
    KC_FORCEINLINE size_t count() const noexcept { return m_size / sizeof(T); }
    KC_FORCEINLINE explicit operator bool() const noexcept { return m_ptr != nullptr; }

    KC_FORCEINLINE T& operator[](size_t i) noexcept { return m_ptr[i]; }
    KC_FORCEINLINE const T& operator[](size_t i) const noexcept { return m_ptr[i]; }

    KC_FORCEINLINE T* operator->() noexcept { return m_ptr; }
    KC_FORCEINLINE const T* operator->() const noexcept { return m_ptr; }

    KC_FORCEINLINE T& operator*() noexcept { return *m_ptr; }
    KC_FORCEINLINE const T& operator*() const noexcept { return *m_ptr; }

    KC_FORCEINLINE T* release() noexcept {
        m_size = 0;
        return detail::kc_exchange(m_ptr, nullptr);
    }

    KC_FORCEINLINE T* begin() noexcept { return m_ptr; }
    KC_FORCEINLINE T* end() noexcept { return m_ptr + count(); }
    KC_FORCEINLINE const T* begin() const noexcept { return m_ptr; }
    KC_FORCEINLINE const T* end() const noexcept { return m_ptr + count(); }
};

template<typename T>
KC_FORCEINLINE KernelBuffer<T> make_kernel_buffer(size_t count, uint64_t flags = POOL_FLAG_NON_PAGED) {
    return KernelBuffer<T>(count, flags);
}

} // namespace core
} // namespace kernelcloak

#pragma warning(pop)

```

`core/random.h`:

```h
#pragma once
#include "../config.h"
#include "types.h"

#pragma warning(push)
#pragma warning(disable: 4307) // integral constant overflow (intentional in hashing)

extern "C" {
    unsigned __int64 __rdtsc();

    __int64 _InterlockedExchangeAdd64(__int64 volatile*, __int64);
    __int64 _InterlockedCompareExchange64(__int64 volatile*, __int64, __int64);
    __int64 _InterlockedExchange64(__int64 volatile*, __int64);
}

#pragma intrinsic(__rdtsc)
#pragma intrinsic(_InterlockedExchangeAdd64)
#pragma intrinsic(_InterlockedCompareExchange64)
#pragma intrinsic(_InterlockedExchange64)

#if !defined(_NTDDK_) && !defined(_WDMDDK_)
// LARGE_INTEGER may already be defined by ntddk.h/wdm.h
#ifndef _LARGE_INTEGER_DEFINED
#define _LARGE_INTEGER_DEFINED
extern "C" {
    typedef union _LARGE_INTEGER {
        struct { unsigned long LowPart; long HighPart; };
        __int64 QuadPart;
    } LARGE_INTEGER, *PLARGE_INTEGER;
}
#endif

extern "C" {
    LARGE_INTEGER __stdcall KeQueryPerformanceCounter(LARGE_INTEGER* PerformanceFrequency);
    unsigned __int64 __stdcall KeQueryInterruptTime();
    void* __stdcall PsGetCurrentProcessId();
    void* __stdcall PsGetCurrentThreadId();
}
#endif

namespace kernelcloak {
namespace detail {

// compile-time seed generation from __TIME__, __COUNTER__, __LINE__
// each macro instantiation site gets different entropy
constexpr uint32_t ct_hash_char(uint32_t hash, char c) {
    return (hash ^ static_cast<uint32_t>(c)) * 0x01000193u;
}

constexpr uint32_t ct_hash_string(const char* str, uint32_t hash = 0x811c9dc5u) {
    return (*str == '\0') ? hash : ct_hash_string(str + 1, ct_hash_char(hash, *str));
}

// xorshift32 - good distribution, fast, small state
constexpr uint32_t xorshift32(uint32_t state) {
    state ^= state << 13;
    state ^= state >> 17;
    state ^= state << 5;
    return state;
}

// advance the generator N times for better diffusion from initial seed
constexpr uint32_t ct_advance(uint32_t seed, uint32_t rounds) {
    for (uint32_t i = 0; i < rounds; ++i) {
        seed = xorshift32(seed);
    }
    return seed;
}

// seed combiner - mixes __TIME__, __COUNTER__, __LINE__ into a single seed
// every call site gets unique entropy because __COUNTER__ increments per TU
// and __LINE__ differs per call site
constexpr uint32_t ct_make_seed(const char* time, uint32_t counter, uint32_t line) {
    uint32_t h = ct_hash_string(time);
    h ^= counter * 0x9e3779b9u;        // golden ratio mixing
    h = xorshift32(h);
    h ^= line * 0x517cc1b7u;           // another prime
    h = xorshift32(h);
    // ensure non-zero (xorshift needs nonzero state)
    return h ? h : 0xdeadbeef;
}

// compile-time random with parameterized generation count
// each call uses __COUNTER__ to produce a different sequence position
constexpr uint32_t ct_random_impl(uint32_t seed, uint32_t generation) {
    return ct_advance(seed, generation + 3);
}

// range clamping for compile-time
constexpr uint32_t ct_range(uint32_t val, uint32_t min_val, uint32_t max_val) {
    return min_val + (val % (max_val - min_val + 1));
}

// fallback seed generation without __TIME__ (WDK builds may undefine it)
constexpr uint32_t ct_make_seed(uint32_t counter, uint32_t line) {
    uint32_t h = 0x811c9dc5u;
    h ^= counter * 0x9e3779b9u;
    h = xorshift32(h);
    h ^= line * 0x517cc1b7u;
    h = xorshift32(h);
    return h ? h : 0xdeadbeef;
}

KC_FORCEINLINE uint64_t splitmix64(uint64_t x) noexcept {
    x += 0x9E3779B97F4A7C15ull;
    x = (x ^ (x >> 30)) * 0xBF58476D1CE4E5B9ull;
    x = (x ^ (x >> 27)) * 0x94D049BB133111EBull;
    return x ^ (x >> 31);
}

} // namespace detail

namespace core {

// compile-time PRNG - constexpr-capable wrapper around xorshift32
// used by crypto primitives for key generation at compile time
struct ct_random {
    uint32_t state;

    constexpr explicit ct_random(uint32_t seed) noexcept
        : state(seed ? seed : 0xdeadbeef) {}

    constexpr uint32_t next() noexcept {
        state = ::kernelcloak::detail::xorshift32(state);
        return state;
    }
};

// runtime PRNG state - lock-free splitmix64 backed by an interlocked counter.
// seeded from multiple hardware entropy sources, and lazily self-seeds if
// kc_random_init() wasn't called.
struct RuntimePrng {
    volatile __int64 state;

    KC_NOINLINE void seed() noexcept {
        // gather entropy from multiple uncorrelated sources
        uint64_t tsc = __rdtsc();

        LARGE_INTEGER freq;
        LARGE_INTEGER perf = KeQueryPerformanceCounter(&freq);

        uint64_t interrupt_time = KeQueryInterruptTime();

        auto pid = reinterpret_cast<uint64_t>(PsGetCurrentProcessId());
        auto tid = reinterpret_cast<uint64_t>(PsGetCurrentThreadId());

        // stack/pool KASLR entropy - address of local variable
        uint64_t stack_entropy = reinterpret_cast<uint64_t>(&tsc);

        // mix all sources into a single 64-bit seed
        uint64_t seed = tsc ^ (perf.QuadPart << 17) ^ interrupt_time;
        seed ^= (pid << 32) | tid;
        seed ^= stack_entropy;
        seed ^= static_cast<uint64_t>(freq.QuadPart) << 23;

        if (!seed) {
            seed = 0x853c49e6748fea9bull;
        }

        _InterlockedExchange64(&state, static_cast<__int64>(seed));
    }

    KC_FORCEINLINE uint64_t next64() noexcept {
        // lazy seed - avoid deterministic all-zero startup
        if (_InterlockedCompareExchange64(&state, 0, 0) == 0) {
            seed();
        }

        constexpr uint64_t inc = 0x9E3779B97F4A7C15ull;
        uint64_t x = static_cast<uint64_t>(_InterlockedExchangeAdd64(
            &state, static_cast<__int64>(inc)));
        return ::kernelcloak::detail::splitmix64(x);
    }

    KC_FORCEINLINE uint32_t next32() noexcept {
        return static_cast<uint32_t>(next64());
    }

    KC_FORCEINLINE uint32_t range(uint32_t min_val, uint32_t max_val) noexcept {
        if (min_val >= max_val) return min_val;
        return min_val + (next32() % (max_val - min_val + 1));
    }
};

// global runtime PRNG instance
// callers can call kc_random_init() once early for stronger entropy, but
// kc_random_rt() will lazily seed on first use if needed.
inline RuntimePrng& kc_global_prng() noexcept {
    // non-paged static - safe at DISPATCH_LEVEL
    static RuntimePrng instance = {};
    return instance;
}

KC_FORCEINLINE void kc_random_init() noexcept {
    kc_global_prng().seed();
}

KC_FORCEINLINE uint32_t kc_random_rt() noexcept {
    return kc_global_prng().next32();
}

KC_FORCEINLINE uint32_t kc_random_rt_range(uint32_t min_val, uint32_t max_val) noexcept {
    return kc_global_prng().range(min_val, max_val);
}

} // namespace core
} // namespace kernelcloak

// compile-time random macros
// each invocation gets unique entropy from __COUNTER__ and __LINE__
// the seed is built from __TIME__ (per-TU), __COUNTER__ (per-macro-expansion), __LINE__ (per-line)
// __TIME__ may be undefined in WDK/deterministic builds
#ifdef __TIME__
#define KC_CT_SEED_() \
    ::kernelcloak::detail::ct_make_seed(__TIME__, __COUNTER__, __LINE__)
#else
#define KC_CT_SEED_() \
    ::kernelcloak::detail::ct_make_seed(__COUNTER__, __LINE__)
#endif

// primary compile-time random value
#define KC_RANDOM_CT() \
    (::kernelcloak::detail::ct_random_impl(KC_CT_SEED_(), __COUNTER__))

// compile-time random in [min, max] range
#define KC_RAND_CT(min_val, max_val) \
    (::kernelcloak::detail::ct_range(KC_RANDOM_CT(), \
        static_cast<::kernelcloak::uint32_t>(min_val), \
        static_cast<::kernelcloak::uint32_t>(max_val)))

// runtime random
#define KC_RANDOM_RT() \
    (::kernelcloak::core::kc_random_rt())

#define KC_RAND_RT(min_val, max_val) \
    (::kernelcloak::core::kc_random_rt_range( \
        static_cast<::kernelcloak::uint32_t>(min_val), \
        static_cast<::kernelcloak::uint32_t>(max_val)))

#pragma warning(pop)

```

`core/string_utils.h`:

```h
#pragma once
#include "../config.h"
#include "types.h"

#pragma warning(push)
#pragma warning(disable: 4201) // nameless struct/union

// NTSTATUS typedef - only when kernel headers aren't already included
#if !defined(_NTDDK_) && !defined(_WDMDDK_)
extern "C" {
#ifndef _NTSTATUS_DEFINED
#define _NTSTATUS_DEFINED
    typedef long NTSTATUS;
#endif

    // ntstrsafe string functions
    NTSTATUS __stdcall RtlStringCbCopyA(char* pszDest, unsigned __int64 cbDest, const char* pszSrc);
    NTSTATUS __stdcall RtlStringCbCatA(char* pszDest, unsigned __int64 cbDest, const char* pszSrc);
    NTSTATUS __stdcall RtlStringCbLengthA(const char* psz, unsigned __int64 cbMax, unsigned __int64* pcbLength);
    NTSTATUS __stdcall RtlStringCbCopyW(wchar_t* pszDest, unsigned __int64 cbDest, const wchar_t* pszSrc);
    NTSTATUS __stdcall RtlStringCbCatW(wchar_t* pszDest, unsigned __int64 cbDest, const wchar_t* pszSrc);
    NTSTATUS __stdcall RtlStringCbLengthW(const wchar_t* psz, unsigned __int64 cbMax, unsigned __int64* pcbLength);
    NTSTATUS __stdcall RtlStringCbPrintfA(char* pszDest, unsigned __int64 cbDest, const char* pszFormat, ...);
    NTSTATUS __stdcall RtlStringCbPrintfW(wchar_t* pszDest, unsigned __int64 cbDest, const wchar_t* pszFormat, ...);

    // safe max length for ntstrsafe (NTSTRSAFE_MAX_CCH * sizeof(char))
    // ntstrsafe.h defines NTSTRSAFE_MAX_CCH as 2147483647 (INT_MAX)
}
#endif

#define KC_NT_SUCCESS(status) (((NTSTATUS)(status)) >= 0)

// ntstrsafe max buffer size
#ifndef KC_STRSAFE_MAX_CB
#define KC_STRSAFE_MAX_CB (2147483647ULL)
#endif

namespace kernelcloak {
namespace core {

// compile-time string length
template<typename CharT>
constexpr size_t kc_strlen(const CharT* str) noexcept {
    size_t len = 0;
    while (str[len] != CharT(0)) {
        ++len;
    }
    return len;
}

// compile-time string compare
constexpr int kc_strcmp(const char* a, const char* b) noexcept {
    while (*a && (*a == *b)) {
        ++a;
        ++b;
    }
    return static_cast<unsigned char>(*a) - static_cast<unsigned char>(*b);
}

constexpr int kc_wcscmp(const wchar_t* a, const wchar_t* b) noexcept {
    while (*a && (*a == *b)) {
        ++a;
        ++b;
    }
    return (*a > *b) ? 1 : ((*a < *b) ? -1 : 0);
}

// compile-time case-insensitive compare
constexpr int kc_stricmp(const char* a, const char* b) noexcept {
    while (*a && *b) {
        char ca = (*a >= 'A' && *a <= 'Z') ? (*a + ('a' - 'A')) : *a;
        char cb = (*b >= 'A' && *b <= 'Z') ? (*b + ('a' - 'A')) : *b;
        if (ca != cb) {
            return static_cast<unsigned char>(ca) - static_cast<unsigned char>(cb);
        }
        ++a;
        ++b;
    }
    char ca = (*a >= 'A' && *a <= 'Z') ? (*a + ('a' - 'A')) : *a;
    char cb = (*b >= 'A' && *b <= 'Z') ? (*b + ('a' - 'A')) : *b;
    return static_cast<unsigned char>(ca) - static_cast<unsigned char>(cb);
}

// compile-time n-bounded compare
constexpr int kc_strncmp(const char* a, const char* b, size_t n) noexcept {
    for (size_t i = 0; i < n; ++i) {
        if (a[i] != b[i]) {
            return static_cast<unsigned char>(a[i]) - static_cast<unsigned char>(b[i]);
        }
        if (a[i] == '\0') {
            return 0;
        }
    }
    return 0;
}

// compile-time tolower/toupper
constexpr char kc_tolower(char c) noexcept {
    return (c >= 'A' && c <= 'Z') ? (c + ('a' - 'A')) : c;
}

constexpr char kc_toupper(char c) noexcept {
    return (c >= 'a' && c <= 'z') ? (c - ('a' - 'A')) : c;
}

constexpr wchar_t kc_towlower(wchar_t c) noexcept {
    return (c >= L'A' && c <= L'Z') ? (c + (L'a' - L'A')) : c;
}

constexpr wchar_t kc_towupper(wchar_t c) noexcept {
    return (c >= L'a' && c <= L'z') ? (c - (L'a' - L'A')) : c;
}

// compile-time string search
constexpr const char* kc_strstr(const char* haystack, const char* needle) noexcept {
    if (*needle == '\0') return haystack;
    for (; *haystack; ++haystack) {
        const char* h = haystack;
        const char* n = needle;
        while (*h && *n && (*h == *n)) {
            ++h;
            ++n;
        }
        if (*n == '\0') return haystack;
    }
    return nullptr;
}

// compile-time char search
constexpr const char* kc_strchr(const char* str, char c) noexcept {
    while (*str) {
        if (*str == c) return str;
        ++str;
    }
    return (c == '\0') ? str : nullptr;
}

// compile-time string copy (bounded)
constexpr void kc_strncpy(char* dst, const char* src, size_t n) noexcept {
    size_t i = 0;
    for (; i < n && src[i] != '\0'; ++i) {
        dst[i] = src[i];
    }
    for (; i < n; ++i) {
        dst[i] = '\0';
    }
}

// runtime safe string operations (wrapping ntstrsafe)
// these are NOT constexpr as they call external kernel functions

struct StringResult {
    NTSTATUS status;
    KC_FORCEINLINE bool ok() const noexcept { return KC_NT_SUCCESS(status); }
    KC_FORCEINLINE operator bool() const noexcept { return ok(); }
};

KC_FORCEINLINE StringResult kc_safe_copy(char* dst, size_t dst_size, const char* src) noexcept {
    return { RtlStringCbCopyA(dst, dst_size, src) };
}

KC_FORCEINLINE StringResult kc_safe_copy(wchar_t* dst, size_t dst_size, const wchar_t* src) noexcept {
    return { RtlStringCbCopyW(dst, dst_size, src) };
}

KC_FORCEINLINE StringResult kc_safe_cat(char* dst, size_t dst_size, const char* src) noexcept {
    return { RtlStringCbCatA(dst, dst_size, src) };
}

KC_FORCEINLINE StringResult kc_safe_cat(wchar_t* dst, size_t dst_size, const wchar_t* src) noexcept {
    return { RtlStringCbCatW(dst, dst_size, src) };
}

KC_FORCEINLINE StringResult kc_safe_length(const char* str, size_t max_size, size_t* out_length) noexcept {
    unsigned __int64 len = 0;
    auto status = RtlStringCbLengthA(str, max_size, &len);
    *out_length = static_cast<size_t>(len);
    return { status };
}

KC_FORCEINLINE StringResult kc_safe_length(const wchar_t* str, size_t max_size, size_t* out_length) noexcept {
    unsigned __int64 len = 0;
    auto status = RtlStringCbLengthW(str, max_size, &len);
    *out_length = static_cast<size_t>(len);
    return { status };
}

// compile-time FNV-1a hash for string comparison without exposing plaintext
constexpr uint32_t kc_hash32(const char* str) noexcept {
    uint32_t hash = 0x811c9dc5u;
    while (*str) {
        hash ^= static_cast<uint32_t>(static_cast<unsigned char>(*str));
        hash *= 0x01000193u;
        ++str;
    }
    return hash;
}

constexpr uint64_t kc_hash64(const char* str) noexcept {
    uint64_t hash = 0xcbf29ce484222325ull;
    while (*str) {
        hash ^= static_cast<uint64_t>(static_cast<unsigned char>(*str));
        hash *= 0x100000001b3ull;
        ++str;
    }
    return hash;
}

// wide char hashing
constexpr uint32_t kc_hash32(const wchar_t* str) noexcept {
    uint32_t hash = 0x811c9dc5u;
    while (*str) {
        hash ^= static_cast<uint32_t>(*str) & 0xFF;
        hash *= 0x01000193u;
        hash ^= (static_cast<uint32_t>(*str) >> 8) & 0xFF;
        hash *= 0x01000193u;
        ++str;
    }
    return hash;
}

// case-insensitive hash
constexpr uint32_t kc_hash32_i(const char* str) noexcept {
    uint32_t hash = 0x811c9dc5u;
    while (*str) {
        char c = (*str >= 'A' && *str <= 'Z') ? (*str + ('a' - 'A')) : *str;
        hash ^= static_cast<uint32_t>(static_cast<unsigned char>(c));
        hash *= 0x01000193u;
        ++str;
    }
    return hash;
}

} // namespace core
} // namespace kernelcloak

#pragma warning(pop)

```

`core/sync.h`:

```h
#pragma once
#include "../config.h"
#include "types.h"

#pragma warning(push)
#pragma warning(disable: 4201) // nameless struct/union
#pragma warning(disable: 4324) // structure padded due to alignment

#if !defined(_NTDDK_) && !defined(_WDMDDK_)
extern "C" {
    // avoid defining WDK typedef names (KSPIN_LOCK/KIRQL) so include order doesn't break builds.
    // these signatures match the WDK on both x86/x64 (KSPIN_LOCK == ULONG_PTR, KIRQL == UCHAR).
    void __stdcall KeInitializeSpinLock(kernelcloak::uintptr_t* SpinLock);
    void __stdcall KeAcquireSpinLock(kernelcloak::uintptr_t* SpinLock, unsigned char* OldIrql);
    void __stdcall KeReleaseSpinLock(kernelcloak::uintptr_t* SpinLock, unsigned char NewIrql);
    void __stdcall KeAcquireSpinLockAtDpcLevel(kernelcloak::uintptr_t* SpinLock);
    void __stdcall KeReleaseSpinLockFromDpcLevel(kernelcloak::uintptr_t* SpinLock);
    unsigned char __stdcall KeGetCurrentIrql();
}
#endif

extern "C" {
    // interlocked intrinsics - compiler builtins, always need declaration
    long _InterlockedIncrement(long volatile*);
    long _InterlockedDecrement(long volatile*);
    long _InterlockedExchange(long volatile*, long);
    long _InterlockedCompareExchange(long volatile*, long, long);
    long _InterlockedExchangeAdd(long volatile*, long);
    long _InterlockedOr(long volatile*, long);
    long _InterlockedAnd(long volatile*, long);
    long _InterlockedXor(long volatile*, long);

    __int64 _InterlockedIncrement64(__int64 volatile*);
    __int64 _InterlockedDecrement64(__int64 volatile*);
    __int64 _InterlockedExchange64(__int64 volatile*, __int64);
    __int64 _InterlockedCompareExchange64(__int64 volatile*, __int64, __int64);
    __int64 _InterlockedExchangeAdd64(__int64 volatile*, __int64);
    __int64 _InterlockedOr64(__int64 volatile*, __int64);
    __int64 _InterlockedAnd64(__int64 volatile*, __int64);
    __int64 _InterlockedXor64(__int64 volatile*, __int64);

    void* _InterlockedExchangePointer(void* volatile*, void*);
    void* _InterlockedCompareExchangePointer(void* volatile*, void*, void*);
}

#pragma intrinsic(_InterlockedIncrement)
#pragma intrinsic(_InterlockedDecrement)
#pragma intrinsic(_InterlockedExchange)
#pragma intrinsic(_InterlockedCompareExchange)
#pragma intrinsic(_InterlockedExchangeAdd)
#pragma intrinsic(_InterlockedOr)
#pragma intrinsic(_InterlockedAnd)
#pragma intrinsic(_InterlockedXor)
#pragma intrinsic(_InterlockedIncrement64)
#pragma intrinsic(_InterlockedDecrement64)
#pragma intrinsic(_InterlockedExchange64)
#pragma intrinsic(_InterlockedCompareExchange64)
#pragma intrinsic(_InterlockedExchangeAdd64)
#pragma intrinsic(_InterlockedOr64)
#pragma intrinsic(_InterlockedAnd64)
#pragma intrinsic(_InterlockedXor64)
#pragma intrinsic(_InterlockedExchangePointer)
#pragma intrinsic(_InterlockedCompareExchangePointer)

// irql constants
#ifndef PASSIVE_LEVEL
#define PASSIVE_LEVEL 0
#endif
#ifndef APC_LEVEL
#define APC_LEVEL 1
#endif
#ifndef DISPATCH_LEVEL
#define DISPATCH_LEVEL 2
#endif

namespace kernelcloak {
namespace core {

// RAII spinlock wrapper with IRQL save/restore
class KSpinLock {
    uintptr_t m_lock = 0;

public:
    KSpinLock() noexcept {
        KeInitializeSpinLock(&m_lock);
    }

    // non-copyable, non-movable
    KSpinLock(const KSpinLock&) = delete;
    KSpinLock& operator=(const KSpinLock&) = delete;
    KSpinLock(KSpinLock&&) = delete;
    KSpinLock& operator=(KSpinLock&&) = delete;

    KC_FORCEINLINE void acquire(unsigned char* old_irql) noexcept {
        KeAcquireSpinLock(&m_lock, old_irql);
    }

    KC_FORCEINLINE void release(unsigned char old_irql) noexcept {
        KeReleaseSpinLock(&m_lock, old_irql);
    }

    // for callers already at DISPATCH_LEVEL
    KC_FORCEINLINE void acquire_at_dpc() noexcept {
        KeAcquireSpinLockAtDpcLevel(&m_lock);
    }

    KC_FORCEINLINE void release_from_dpc() noexcept {
        KeReleaseSpinLockFromDpcLevel(&m_lock);
    }

    KC_FORCEINLINE uintptr_t* native() noexcept { return &m_lock; }

    // scoped guard - acquires on construction, releases on destruction
    class Guard {
        KSpinLock& m_parent;
        unsigned char m_old_irql = PASSIVE_LEVEL;
        bool m_owned = false;

    public:
        explicit Guard(KSpinLock& lock) noexcept
            : m_parent(lock), m_owned(true)
        {
            m_parent.acquire(&m_old_irql);
        }

        ~Guard() noexcept {
            if (m_owned) {
                m_parent.release(m_old_irql);
            }
        }

        Guard(const Guard&) = delete;
        Guard& operator=(const Guard&) = delete;
        Guard(Guard&&) = delete;
        Guard& operator=(Guard&&) = delete;

        KC_FORCEINLINE unsigned char saved_irql() const noexcept { return m_old_irql; }
    };

    // scoped guard for code already at DISPATCH_LEVEL
    class DpcGuard {
        KSpinLock& m_parent;
        bool m_owned = false;

    public:
        explicit DpcGuard(KSpinLock& lock) noexcept
            : m_parent(lock), m_owned(true)
        {
            m_parent.acquire_at_dpc();
        }

        ~DpcGuard() noexcept {
            if (m_owned) {
                m_parent.release_from_dpc();
            }
        }

        DpcGuard(const DpcGuard&) = delete;
        DpcGuard& operator=(const DpcGuard&) = delete;
        DpcGuard(DpcGuard&&) = delete;
        DpcGuard& operator=(DpcGuard&&) = delete;
    };

    // convenience
    KC_FORCEINLINE Guard lock() noexcept { return Guard(*this); }
    KC_FORCEINLINE DpcGuard lock_at_dpc() noexcept { return DpcGuard(*this); }
};

// interlocked operations dispatcher - selects 32/64 bit intrinsics
namespace detail_sync {

template<typename T, size_t = sizeof(T)>
struct interlocked_ops;

// 32-bit specialization
template<typename T>
struct interlocked_ops<T, 4> {
    using storage_type = long;
    using volatile_ptr = long volatile*;

    static KC_FORCEINLINE T increment(volatile T* target) noexcept {
        return static_cast<T>(_InterlockedIncrement(reinterpret_cast<volatile_ptr>(target)));
    }
    static KC_FORCEINLINE T decrement(volatile T* target) noexcept {
        return static_cast<T>(_InterlockedDecrement(reinterpret_cast<volatile_ptr>(target)));
    }
    static KC_FORCEINLINE T exchange(volatile T* target, T value) noexcept {
        return static_cast<T>(_InterlockedExchange(
            reinterpret_cast<volatile_ptr>(target),
            static_cast<storage_type>(value)));
    }
    static KC_FORCEINLINE T compare_exchange(volatile T* target, T exchange_val, T comparand) noexcept {
        return static_cast<T>(_InterlockedCompareExchange(
            reinterpret_cast<volatile_ptr>(target),
            static_cast<storage_type>(exchange_val),
            static_cast<storage_type>(comparand)));
    }
    static KC_FORCEINLINE T add(volatile T* target, T value) noexcept {
        return static_cast<T>(_InterlockedExchangeAdd(
            reinterpret_cast<volatile_ptr>(target),
            static_cast<storage_type>(value)));
    }
    static KC_FORCEINLINE T or_op(volatile T* target, T value) noexcept {
        return static_cast<T>(_InterlockedOr(
            reinterpret_cast<volatile_ptr>(target),
            static_cast<storage_type>(value)));
    }
    static KC_FORCEINLINE T and_op(volatile T* target, T value) noexcept {
        return static_cast<T>(_InterlockedAnd(
            reinterpret_cast<volatile_ptr>(target),
            static_cast<storage_type>(value)));
    }
    static KC_FORCEINLINE T xor_op(volatile T* target, T value) noexcept {
        return static_cast<T>(_InterlockedXor(
            reinterpret_cast<volatile_ptr>(target),
            static_cast<storage_type>(value)));
    }
    static KC_FORCEINLINE T load(const volatile T* target) noexcept {
        // interlocked compare-exchange with itself to get an atomic read
        return static_cast<T>(_InterlockedCompareExchange(
            const_cast<volatile_ptr>(reinterpret_cast<const volatile long*>(target)), 0, 0));
    }
    static KC_FORCEINLINE void store(volatile T* target, T value) noexcept {
        _InterlockedExchange(
            reinterpret_cast<volatile_ptr>(target),
            static_cast<storage_type>(value));
    }
};

// 64-bit specialization
template<typename T>
struct interlocked_ops<T, 8> {
    using storage_type = __int64;
    using volatile_ptr = __int64 volatile*;

    static KC_FORCEINLINE T increment(volatile T* target) noexcept {
        return static_cast<T>(_InterlockedIncrement64(reinterpret_cast<volatile_ptr>(target)));
    }
    static KC_FORCEINLINE T decrement(volatile T* target) noexcept {
        return static_cast<T>(_InterlockedDecrement64(reinterpret_cast<volatile_ptr>(target)));
    }
    static KC_FORCEINLINE T exchange(volatile T* target, T value) noexcept {
        return static_cast<T>(_InterlockedExchange64(
            reinterpret_cast<volatile_ptr>(target),
            static_cast<storage_type>(value)));
    }
    static KC_FORCEINLINE T compare_exchange(volatile T* target, T exchange_val, T comparand) noexcept {
        return static_cast<T>(_InterlockedCompareExchange64(
            reinterpret_cast<volatile_ptr>(target),
            static_cast<storage_type>(exchange_val),
            static_cast<storage_type>(comparand)));
    }
    static KC_FORCEINLINE T add(volatile T* target, T value) noexcept {
        return static_cast<T>(_InterlockedExchangeAdd64(
            reinterpret_cast<volatile_ptr>(target),
            static_cast<storage_type>(value)));
    }
    static KC_FORCEINLINE T or_op(volatile T* target, T value) noexcept {
        return static_cast<T>(_InterlockedOr64(
            reinterpret_cast<volatile_ptr>(target),
            static_cast<storage_type>(value)));
    }
    static KC_FORCEINLINE T and_op(volatile T* target, T value) noexcept {
        return static_cast<T>(_InterlockedAnd64(
            reinterpret_cast<volatile_ptr>(target),
            static_cast<storage_type>(value)));
    }
    static KC_FORCEINLINE T xor_op(volatile T* target, T value) noexcept {
        return static_cast<T>(_InterlockedXor64(
            reinterpret_cast<volatile_ptr>(target),
            static_cast<storage_type>(value)));
    }
    static KC_FORCEINLINE T load(const volatile T* target) noexcept {
        return static_cast<T>(_InterlockedCompareExchange64(
            const_cast<volatile_ptr>(reinterpret_cast<const volatile __int64*>(target)), 0, 0));
    }
    static KC_FORCEINLINE void store(volatile T* target, T value) noexcept {
        _InterlockedExchange64(
            reinterpret_cast<volatile_ptr>(target),
            static_cast<storage_type>(value));
    }
};

// pointer specialization
template<typename T>
struct interlocked_ops<T*, 8> {
    static KC_FORCEINLINE T* exchange(T* volatile* target, T* value) noexcept {
        return static_cast<T*>(_InterlockedExchangePointer(
            reinterpret_cast<void* volatile*>(target), value));
    }
    static KC_FORCEINLINE T* compare_exchange(T* volatile* target, T* exchange_val, T* comparand) noexcept {
        return static_cast<T*>(_InterlockedCompareExchangePointer(
            reinterpret_cast<void* volatile*>(target), exchange_val, comparand));
    }
    static KC_FORCEINLINE T* load(T* const volatile* target) noexcept {
        return static_cast<T*>(_InterlockedCompareExchangePointer(
            const_cast<void* volatile*>(reinterpret_cast<void* const volatile*>(target)),
            nullptr, nullptr));
    }
    static KC_FORCEINLINE void store(T* volatile* target, T* value) noexcept {
        _InterlockedExchangePointer(
            reinterpret_cast<void* volatile*>(target), value);
    }
};

} // namespace detail_sync

// kernel-safe atomic wrapper using Interlocked* family
// safe at DISPATCH_LEVEL (no paged pool, no blocking)
template<typename T>
class KernelAtomic {
    static_assert(sizeof(T) == 4 || sizeof(T) == 8,
        "KernelAtomic only supports 32 and 64 bit types");

    using ops = detail_sync::interlocked_ops<T>;
    volatile T m_value;

public:
    KernelAtomic() noexcept : m_value{} {}
    explicit KernelAtomic(T initial) noexcept : m_value(initial) {}

    // no copy/move - atomics should be stationary
    KernelAtomic(const KernelAtomic&) = delete;
    KernelAtomic& operator=(const KernelAtomic&) = delete;

    KC_FORCEINLINE T load() const noexcept {
        return ops::load(&m_value);
    }

    KC_FORCEINLINE void store(T value) noexcept {
        ops::store(&m_value, value);
    }

    KC_FORCEINLINE T exchange(T value) noexcept {
        return ops::exchange(&m_value, value);
    }

    // returns the OLD value at m_value. if old == comparand, exchange happened.
    KC_FORCEINLINE T compare_exchange(T exchange_val, T comparand) noexcept {
        return ops::compare_exchange(&m_value, exchange_val, comparand);
    }

    // returns true if exchange succeeded
    KC_FORCEINLINE bool compare_exchange_strong(T& expected, T desired) noexcept {
        T old = ops::compare_exchange(&m_value, desired, expected);
        if (old == expected) {
            return true;
        }
        expected = old;
        return false;
    }

    KC_FORCEINLINE T increment() noexcept { return ops::increment(&m_value); }
    KC_FORCEINLINE T decrement() noexcept { return ops::decrement(&m_value); }
    KC_FORCEINLINE T add(T value) noexcept { return ops::add(&m_value, value); }
    KC_FORCEINLINE T fetch_or(T value) noexcept { return ops::or_op(&m_value, value); }
    KC_FORCEINLINE T fetch_and(T value) noexcept { return ops::and_op(&m_value, value); }
    KC_FORCEINLINE T fetch_xor(T value) noexcept { return ops::xor_op(&m_value, value); }

    // operator overloads for convenience
    KC_FORCEINLINE operator T() const noexcept { return load(); }
    KC_FORCEINLINE T operator=(T value) noexcept { store(value); return value; }
    KC_FORCEINLINE T operator++() noexcept { return increment(); }
    KC_FORCEINLINE T operator--() noexcept { return decrement(); }
    KC_FORCEINLINE T operator++(int) noexcept { return increment() - 1; }
    KC_FORCEINLINE T operator--(int) noexcept { return decrement() + 1; }
    KC_FORCEINLINE T operator+=(T v) noexcept { return add(v) + v; }
    KC_FORCEINLINE T operator|=(T v) noexcept { return fetch_or(v) | v; }
    KC_FORCEINLINE T operator&=(T v) noexcept { return fetch_and(v) & v; }
    KC_FORCEINLINE T operator^=(T v) noexcept { return fetch_xor(v) ^ v; }
};

// pointer specialization
template<typename T>
class KernelAtomic<T*> {
    using ops = detail_sync::interlocked_ops<T*>;
    T* volatile m_value;

public:
    KernelAtomic() noexcept : m_value(nullptr) {}
    explicit KernelAtomic(T* initial) noexcept : m_value(initial) {}

    KernelAtomic(const KernelAtomic&) = delete;
    KernelAtomic& operator=(const KernelAtomic&) = delete;

    KC_FORCEINLINE T* load() const noexcept {
        return ops::load(&m_value);
    }

    KC_FORCEINLINE void store(T* value) noexcept {
        ops::store(&m_value, value);
    }

    KC_FORCEINLINE T* exchange(T* value) noexcept {
        return ops::exchange(&m_value, value);
    }

    KC_FORCEINLINE bool compare_exchange_strong(T*& expected, T* desired) noexcept {
        T* old = ops::compare_exchange(&m_value, desired, expected);
        if (old == expected) {
            return true;
        }
        expected = old;
        return false;
    }

    KC_FORCEINLINE operator T*() const noexcept { return load(); }
    KC_FORCEINLINE T* operator=(T* value) noexcept { store(value); return value; }
    KC_FORCEINLINE T* operator->() const noexcept { return load(); }
    KC_FORCEINLINE T& operator*() const noexcept { return *load(); }
};

} // namespace core
} // namespace kernelcloak

#pragma warning(pop)

```

`core/types.h`:

```h
#pragma once
#include "../config.h"

// fixed-width types and type traits without STL/CRT dependencies
#pragma warning(push)
#pragma warning(disable: 4310) // cast truncates constant value
#pragma warning(disable: 4296) // '<': expression is always false (unsigned is_signed check)

namespace kernelcloak {

using uint8_t  = unsigned char;
using uint16_t = unsigned short;
using uint32_t = unsigned int;
using uint64_t = unsigned long long;
using int8_t   = signed char;
using int16_t  = signed short;
using int32_t  = signed int;
using int64_t  = signed long long;
using size_t   = decltype(sizeof(0));
using uintptr_t = size_t;
using intptr_t  = long long;
using ptrdiff_t = long long;
using nullptr_t = decltype(nullptr);

namespace detail {

// integral_constant
template<typename T, T V>
struct integral_constant {
    static constexpr T value = V;
    using value_type = T;
    using type = integral_constant;
    constexpr operator value_type() const noexcept { return value; }
    constexpr value_type operator()() const noexcept { return value; }
};

using true_type  = integral_constant<bool, true>;
using false_type = integral_constant<bool, false>;

// remove_cv
template<typename T> struct remove_const          { using type = T; };
template<typename T> struct remove_const<const T>  { using type = T; };

template<typename T> struct remove_volatile             { using type = T; };
template<typename T> struct remove_volatile<volatile T>  { using type = T; };

template<typename T> struct remove_cv {
    using type = typename remove_const<typename remove_volatile<T>::type>::type;
};
template<typename T> using remove_cv_t = typename remove_cv<T>::type;

// remove_reference
template<typename T> struct remove_reference       { using type = T; };
template<typename T> struct remove_reference<T&>   { using type = T; };
template<typename T> struct remove_reference<T&&>  { using type = T; };
template<typename T> using remove_reference_t = typename remove_reference<T>::type;

// remove_extent
template<typename T> struct remove_extent                { using type = T; };
template<typename T> struct remove_extent<T[]>           { using type = T; };
template<typename T, size_t N> struct remove_extent<T[N]> { using type = T; };

// remove_pointer
template<typename T> struct remove_pointer                    { using type = T; };
template<typename T> struct remove_pointer<T*>                { using type = T; };
template<typename T> struct remove_pointer<T* const>          { using type = T; };
template<typename T> struct remove_pointer<T* volatile>       { using type = T; };
template<typename T> struct remove_pointer<T* const volatile> { using type = T; };

// add_pointer
template<typename T> struct add_pointer { using type = typename remove_reference<T>::type*; };
template<typename T> using add_pointer_t = typename add_pointer<T>::type;

// is_same
template<typename T, typename U> struct is_same : false_type {};
template<typename T> struct is_same<T, T> : true_type {};
template<typename T, typename U> static constexpr bool is_same_v = is_same<T, U>::value;

// is_void
template<typename T> struct is_void : is_same<remove_cv_t<T>, void> {};
template<typename T> static constexpr bool is_void_v = is_void<T>::value;

// is_nullptr
template<typename T> struct is_null_pointer : is_same<remove_cv_t<T>, nullptr_t> {};

// is_integral
template<typename T> struct is_integral_impl : false_type {};
template<> struct is_integral_impl<bool>               : true_type {};
template<> struct is_integral_impl<char>               : true_type {};
template<> struct is_integral_impl<signed char>        : true_type {};
template<> struct is_integral_impl<unsigned char>      : true_type {};
#ifdef _NATIVE_WCHAR_T_DEFINED
template<> struct is_integral_impl<wchar_t>            : true_type {};
#endif
template<> struct is_integral_impl<char16_t>           : true_type {};
template<> struct is_integral_impl<char32_t>           : true_type {};
template<> struct is_integral_impl<short>              : true_type {};
template<> struct is_integral_impl<unsigned short>     : true_type {};
template<> struct is_integral_impl<int>                : true_type {};
template<> struct is_integral_impl<unsigned int>       : true_type {};
template<> struct is_integral_impl<long>               : true_type {};
template<> struct is_integral_impl<unsigned long>      : true_type {};
template<> struct is_integral_impl<long long>          : true_type {};
template<> struct is_integral_impl<unsigned long long> : true_type {};

template<typename T> struct is_integral : is_integral_impl<remove_cv_t<T>> {};
template<typename T> static constexpr bool is_integral_v = is_integral<T>::value;

// is_floating_point
template<typename T> struct is_floating_point_impl : false_type {};
template<> struct is_floating_point_impl<float>       : true_type {};
template<> struct is_floating_point_impl<double>      : true_type {};
template<> struct is_floating_point_impl<long double> : true_type {};

template<typename T> struct is_floating_point : is_floating_point_impl<remove_cv_t<T>> {};
template<typename T> static constexpr bool is_floating_point_v = is_floating_point<T>::value;

// is_arithmetic
template<typename T> struct is_arithmetic : integral_constant<bool,
    is_integral<T>::value || is_floating_point<T>::value> {};
template<typename T> static constexpr bool is_arithmetic_v = is_arithmetic<T>::value;

// is_pointer
template<typename T> struct is_pointer_impl : false_type {};
template<typename T> struct is_pointer_impl<T*> : true_type {};

template<typename T> struct is_pointer : is_pointer_impl<remove_cv_t<T>> {};
template<typename T> static constexpr bool is_pointer_v = is_pointer<T>::value;

// is_array
template<typename T> struct is_array : false_type {};
template<typename T> struct is_array<T[]> : true_type {};
template<typename T, size_t N> struct is_array<T[N]> : true_type {};
template<typename T> static constexpr bool is_array_v = is_array<T>::value;

// is_const / is_volatile
template<typename T> struct is_const : false_type {};
template<typename T> struct is_const<const T> : true_type {};
template<typename T> static constexpr bool is_const_v = is_const<T>::value;

template<typename T> struct is_volatile : false_type {};
template<typename T> struct is_volatile<volatile T> : true_type {};
template<typename T> static constexpr bool is_volatile_v = is_volatile<T>::value;

// is_reference
template<typename T> struct is_lvalue_reference : false_type {};
template<typename T> struct is_lvalue_reference<T&> : true_type {};

template<typename T> struct is_rvalue_reference : false_type {};
template<typename T> struct is_rvalue_reference<T&&> : true_type {};

template<typename T> struct is_reference : integral_constant<bool,
    is_lvalue_reference<T>::value || is_rvalue_reference<T>::value> {};

// is_function (simplified - detects most common calling conventions)
template<typename T> struct is_function : false_type {};
template<typename R, typename... A> struct is_function<R(A...)> : true_type {};
template<typename R, typename... A> struct is_function<R(A..., ...)> : true_type {};
// const/volatile/ref qualified
template<typename R, typename... A> struct is_function<R(A...) const> : true_type {};
template<typename R, typename... A> struct is_function<R(A...) volatile> : true_type {};
template<typename R, typename... A> struct is_function<R(A...) const volatile> : true_type {};
template<typename R, typename... A> struct is_function<R(A...) &> : true_type {};
template<typename R, typename... A> struct is_function<R(A...) const &> : true_type {};
template<typename R, typename... A> struct is_function<R(A...) volatile &> : true_type {};
template<typename R, typename... A> struct is_function<R(A...) const volatile &> : true_type {};
template<typename R, typename... A> struct is_function<R(A...) &&> : true_type {};
template<typename R, typename... A> struct is_function<R(A...) const &&> : true_type {};
template<typename R, typename... A> struct is_function<R(A...) volatile &&> : true_type {};
template<typename R, typename... A> struct is_function<R(A...) const volatile &&> : true_type {};
// noexcept variants
template<typename R, typename... A> struct is_function<R(A...) noexcept> : true_type {};
template<typename R, typename... A> struct is_function<R(A..., ...) noexcept> : true_type {};

// conditional
template<bool B, typename T, typename F> struct conditional { using type = T; };
template<typename T, typename F> struct conditional<false, T, F> { using type = F; };
template<bool B, typename T, typename F> using conditional_t = typename conditional<B, T, F>::type;

// enable_if
template<bool B, typename T = void> struct enable_if {};
template<typename T> struct enable_if<true, T> { using type = T; };
template<bool B, typename T = void> using enable_if_t = typename enable_if<B, T>::type;

// add_const / add_volatile / add_cv
template<typename T> struct add_const    { using type = const T; };
template<typename T> struct add_volatile { using type = volatile T; };
template<typename T> struct add_cv       { using type = const volatile T; };
template<typename T> using add_const_t    = typename add_const<T>::type;
template<typename T> using add_volatile_t = typename add_volatile<T>::type;
template<typename T> using add_cv_t       = typename add_cv<T>::type;

// add_lvalue_reference / add_rvalue_reference
namespace ref_detail {
    template<typename T> struct type_identity { using type = T; };
    template<typename T> auto try_add_lvalue_ref(int) -> type_identity<T&>;
    template<typename T> auto try_add_lvalue_ref(...) -> type_identity<T>;
    template<typename T> auto try_add_rvalue_ref(int) -> type_identity<T&&>;
    template<typename T> auto try_add_rvalue_ref(...) -> type_identity<T>;
}
template<typename T> struct add_lvalue_reference : decltype(ref_detail::try_add_lvalue_ref<T>(0)) {};
template<typename T> struct add_rvalue_reference : decltype(ref_detail::try_add_rvalue_ref<T>(0)) {};
template<typename T> using add_lvalue_reference_t = typename add_lvalue_reference<T>::type;
template<typename T> using add_rvalue_reference_t = typename add_rvalue_reference<T>::type;

// decay
template<typename T>
struct decay {
private:
    using U = remove_reference_t<T>;
public:
    using type = conditional_t<
        is_array<U>::value,
        typename remove_extent<U>::type*,
        conditional_t<
            is_function<U>::value,
            typename add_pointer<U>::type,
            remove_cv_t<U>
        >
    >;
};
template<typename T> using decay_t = typename decay<T>::type;

// is_unsigned / is_signed
template<typename T, bool = is_arithmetic_v<T>>
struct is_unsigned_impl : integral_constant<bool, T(0) < T(-1)> {};
template<typename T> struct is_unsigned_impl<T, false> : false_type {};
template<typename T> struct is_unsigned : is_unsigned_impl<T> {};
template<typename T> static constexpr bool is_unsigned_v = is_unsigned<T>::value;

template<typename T, bool = is_arithmetic_v<T>>
struct is_signed_impl : integral_constant<bool, T(-1) < T(0)> {};
template<typename T> struct is_signed_impl<T, false> : false_type {};
template<typename T> struct is_signed : is_signed_impl<T> {};
template<typename T> static constexpr bool is_signed_v = is_signed<T>::value;

// make_unsigned / make_signed
template<typename T> struct make_unsigned;
template<> struct make_unsigned<char>               { using type = unsigned char; };
template<> struct make_unsigned<signed char>        { using type = unsigned char; };
template<> struct make_unsigned<unsigned char>      { using type = unsigned char; };
template<> struct make_unsigned<short>              { using type = unsigned short; };
template<> struct make_unsigned<unsigned short>     { using type = unsigned short; };
template<> struct make_unsigned<int>                { using type = unsigned int; };
template<> struct make_unsigned<unsigned int>       { using type = unsigned int; };
template<> struct make_unsigned<long>               { using type = unsigned long; };
template<> struct make_unsigned<unsigned long>      { using type = unsigned long; };
template<> struct make_unsigned<long long>          { using type = unsigned long long; };
template<> struct make_unsigned<unsigned long long> { using type = unsigned long long; };
template<typename T> using make_unsigned_t = typename make_unsigned<T>::type;

template<typename T> struct make_signed;
template<> struct make_signed<char>               { using type = signed char; };
template<> struct make_signed<signed char>        { using type = signed char; };
template<> struct make_signed<unsigned char>      { using type = signed char; };
template<> struct make_signed<short>              { using type = short; };
template<> struct make_signed<unsigned short>     { using type = short; };
template<> struct make_signed<int>                { using type = int; };
template<> struct make_signed<unsigned int>       { using type = int; };
template<> struct make_signed<long>               { using type = long; };
template<> struct make_signed<unsigned long>      { using type = long; };
template<> struct make_signed<long long>          { using type = long long; };
template<> struct make_signed<unsigned long long> { using type = long long; };
template<typename T> using make_signed_t = typename make_signed<T>::type;

// void_t
template<typename...> using void_t = void;

// conjunction / disjunction / negation
template<typename...> struct conjunction : true_type {};
template<typename B1> struct conjunction<B1> : B1 {};
template<typename B1, typename... Bn>
struct conjunction<B1, Bn...> : conditional_t<bool(B1::value), conjunction<Bn...>, B1> {};

template<typename...> struct disjunction : false_type {};
template<typename B1> struct disjunction<B1> : B1 {};
template<typename B1, typename... Bn>
struct disjunction<B1, Bn...> : conditional_t<bool(B1::value), B1, disjunction<Bn...>> {};

template<typename B>
struct negation : integral_constant<bool, !bool(B::value)> {};

// integer_sequence
template<typename T, T... Is>
struct integer_sequence {
    using value_type = T;
    static constexpr size_t size() noexcept { return sizeof...(Is); }
};

template<size_t... Is>
using index_sequence = integer_sequence<size_t, Is...>;

#ifdef _MSC_VER
// MSVC provides __make_integer_seq as a builtin - avoids deep recursion
// and works around MSVC issues with dependent non-type template parameter
// partial specializations
template<typename T, T N>
using make_integer_sequence = __make_integer_seq<integer_sequence, T, N>;
#else
namespace seq_detail {
    template<typename T, T N, T... Is>
    struct make_seq_impl : make_seq_impl<T, N - 1, N - 1, Is...> {};

    template<typename T, T... Is>
    struct make_seq_impl<T, static_cast<T>(0), Is...> {
        using type = integer_sequence<T, Is...>;
    };
}

template<typename T, T N>
using make_integer_sequence = typename seq_detail::make_seq_impl<T, N>::type;
#endif

template<size_t N>
using make_index_sequence = make_integer_sequence<size_t, N>;

template<typename... T>
using index_sequence_for = make_index_sequence<sizeof...(T)>;

// is_trivially_copyable - compiler intrinsic based
template<typename T> struct is_trivially_copyable
    : integral_constant<bool, __is_trivially_copyable(T)> {};
template<typename T> static constexpr bool is_trivially_copyable_v = is_trivially_copyable<T>::value;

// is_trivially_destructible
template<typename T> struct is_trivially_destructible
    : integral_constant<bool, __is_trivially_destructible(T)> {};

// alignment_of
template<typename T> struct alignment_of : integral_constant<size_t, alignof(T)> {};

} // namespace detail

// pull commonly used traits into core namespace
namespace core {
    using detail::integral_constant;
    using detail::true_type;
    using detail::false_type;
    using detail::is_same;
    using detail::is_same_v;
    using detail::is_integral;
    using detail::is_integral_v;
    using detail::is_pointer;
    using detail::is_pointer_v;
    using detail::is_array;
    using detail::is_array_v;
    using detail::is_void;
    using detail::is_void_v;
    using detail::is_arithmetic;
    using detail::is_floating_point;
    using detail::is_const;
    using detail::is_volatile;
    using detail::is_reference;
    using detail::is_unsigned;
    using detail::is_signed;
    using detail::is_trivially_copyable;
    using detail::is_trivially_destructible;
    using detail::enable_if;
    using detail::enable_if_t;
    using detail::conditional;
    using detail::conditional_t;
    using detail::remove_cv;
    using detail::remove_cv_t;
    using detail::remove_reference;
    using detail::remove_reference_t;
    using detail::remove_extent;
    using detail::remove_pointer;
    using detail::add_pointer;
    using detail::add_pointer_t;
    using detail::add_const;
    using detail::add_volatile;
    using detail::add_cv;
    using detail::add_lvalue_reference;
    using detail::add_rvalue_reference;
    using detail::decay;
    using detail::decay_t;
    using detail::make_unsigned;
    using detail::make_unsigned_t;
    using detail::make_signed;
    using detail::make_signed_t;
    using detail::void_t;
    using detail::conjunction;
    using detail::disjunction;
    using detail::negation;
    using detail::integer_sequence;
    using detail::index_sequence;
    using detail::make_integer_sequence;
    using detail::make_index_sequence;
    using detail::index_sequence_for;
}

// move / forward - must be in global-reachable scope for the library
namespace detail {

template<typename T>
constexpr remove_reference_t<T>&& kc_move(T&& t) noexcept {
    return static_cast<remove_reference_t<T>&&>(t);
}

template<typename T>
constexpr T&& kc_forward(remove_reference_t<T>& t) noexcept {
    return static_cast<T&&>(t);
}

template<typename T>
constexpr T&& kc_forward(remove_reference_t<T>&& t) noexcept {
    static_assert(!is_lvalue_reference<T>::value, "cannot forward rvalue as lvalue");
    return static_cast<T&&>(t);
}

// exchange replacement
template<typename T, typename U = T>
KC_FORCEINLINE T kc_exchange(T& obj, U&& new_value) noexcept {
    T old = kc_move(obj);
    obj = kc_forward<U>(new_value);
    return old;
}

} // namespace detail

} // namespace kernelcloak

#pragma warning(pop)

```

`crypto/hash.h`:

```h
#pragma once
#include "../config.h"
#include "../core/types.h"

namespace kernelcloak {
namespace crypto {

namespace detail {

// fnv-1a 64-bit constants
constexpr uint64_t fnv64_offset_basis = 0xcbf29ce484222325ull;
constexpr uint64_t fnv64_prime        = 0x00000100000001B3ull;

// fnv-1a 32-bit constants
constexpr uint32_t fnv32_offset_basis = 0x811c9dc5u;
constexpr uint32_t fnv32_prime        = 0x01000193u;

constexpr char to_lower(char c) {
    return (c >= 'A' && c <= 'Z') ? static_cast<char>(c + ('a' - 'A')) : c;
}

constexpr wchar_t to_lower_w(wchar_t c) {
    return (c >= L'A' && c <= L'Z') ? static_cast<wchar_t>(c + (L'a' - L'A')) : c;
}

// 64-bit fnv-1a
constexpr uint64_t fnv1a_64(const char* str, size_t len) {
    uint64_t hash = fnv64_offset_basis;
    for (size_t i = 0; i < len; ++i) {
        hash ^= static_cast<uint64_t>(static_cast<uint8_t>(str[i]));
        hash *= fnv64_prime;
    }
    return hash;
}

constexpr uint64_t fnv1a_64_ci(const char* str, size_t len) {
    uint64_t hash = fnv64_offset_basis;
    for (size_t i = 0; i < len; ++i) {
        hash ^= static_cast<uint64_t>(static_cast<uint8_t>(to_lower(str[i])));
        hash *= fnv64_prime;
    }
    return hash;
}

constexpr uint64_t fnv1a_64_wide(const wchar_t* str, size_t len) {
    uint64_t hash = fnv64_offset_basis;
    for (size_t i = 0; i < len; ++i) {
        uint16_t ch = static_cast<uint16_t>(str[i]);
        hash ^= static_cast<uint64_t>(ch & 0xFF);
        hash *= fnv64_prime;
        hash ^= static_cast<uint64_t>((ch >> 8) & 0xFF);
        hash *= fnv64_prime;
    }
    return hash;
}

constexpr uint64_t fnv1a_64_wide_ci(const wchar_t* str, size_t len) {
    uint64_t hash = fnv64_offset_basis;
    for (size_t i = 0; i < len; ++i) {
        uint16_t ch = static_cast<uint16_t>(to_lower_w(str[i]));
        hash ^= static_cast<uint64_t>(ch & 0xFF);
        hash *= fnv64_prime;
        hash ^= static_cast<uint64_t>((ch >> 8) & 0xFF);
        hash *= fnv64_prime;
    }
    return hash;
}

// 32-bit fnv-1a
constexpr uint32_t fnv1a_32(const char* str, size_t len) {
    uint32_t hash = fnv32_offset_basis;
    for (size_t i = 0; i < len; ++i) {
        hash ^= static_cast<uint32_t>(static_cast<uint8_t>(str[i]));
        hash *= fnv32_prime;
    }
    return hash;
}

constexpr uint32_t fnv1a_32_ci(const char* str, size_t len) {
    uint32_t hash = fnv32_offset_basis;
    for (size_t i = 0; i < len; ++i) {
        hash ^= static_cast<uint32_t>(static_cast<uint8_t>(to_lower(str[i])));
        hash *= fnv32_prime;
    }
    return hash;
}

constexpr uint32_t fnv1a_32_wide(const wchar_t* str, size_t len) {
    uint32_t hash = fnv32_offset_basis;
    for (size_t i = 0; i < len; ++i) {
        uint16_t ch = static_cast<uint16_t>(str[i]);
        hash ^= static_cast<uint32_t>(ch & 0xFF);
        hash *= fnv32_prime;
        hash ^= static_cast<uint32_t>((ch >> 8) & 0xFF);
        hash *= fnv32_prime;
    }
    return hash;
}

constexpr uint32_t fnv1a_32_wide_ci(const wchar_t* str, size_t len) {
    uint32_t hash = fnv32_offset_basis;
    for (size_t i = 0; i < len; ++i) {
        uint16_t ch = static_cast<uint16_t>(to_lower_w(str[i]));
        hash ^= static_cast<uint32_t>(ch & 0xFF);
        hash *= fnv32_prime;
        hash ^= static_cast<uint32_t>((ch >> 8) & 0xFF);
        hash *= fnv32_prime;
    }
    return hash;
}

// null-terminated runtime variants
KC_FORCEINLINE uint64_t fnv1a_64_rt(const char* str) {
    uint64_t hash = fnv64_offset_basis;
    for (; *str; ++str) {
        hash ^= static_cast<uint64_t>(static_cast<uint8_t>(*str));
        hash *= fnv64_prime;
    }
    return hash;
}

KC_FORCEINLINE uint64_t fnv1a_64_rt_ci(const char* str) {
    uint64_t hash = fnv64_offset_basis;
    for (; *str; ++str) {
        hash ^= static_cast<uint64_t>(static_cast<uint8_t>(to_lower(*str)));
        hash *= fnv64_prime;
    }
    return hash;
}

KC_FORCEINLINE uint64_t fnv1a_64_rt_wide(const wchar_t* str) {
    uint64_t hash = fnv64_offset_basis;
    for (; *str; ++str) {
        uint16_t ch = static_cast<uint16_t>(*str);
        hash ^= static_cast<uint64_t>(ch & 0xFF);
        hash *= fnv64_prime;
        hash ^= static_cast<uint64_t>((ch >> 8) & 0xFF);
        hash *= fnv64_prime;
    }
    return hash;
}

KC_FORCEINLINE uint64_t fnv1a_64_rt_wide_ci(const wchar_t* str) {
    uint64_t hash = fnv64_offset_basis;
    for (; *str; ++str) {
        uint16_t ch = static_cast<uint16_t>(to_lower_w(*str));
        hash ^= static_cast<uint64_t>(ch & 0xFF);
        hash *= fnv64_prime;
        hash ^= static_cast<uint64_t>((ch >> 8) & 0xFF);
        hash *= fnv64_prime;
    }
    return hash;
}

KC_FORCEINLINE uint32_t fnv1a_32_rt(const char* str) {
    uint32_t hash = fnv32_offset_basis;
    for (; *str; ++str) {
        hash ^= static_cast<uint32_t>(static_cast<uint8_t>(*str));
        hash *= fnv32_prime;
    }
    return hash;
}

KC_FORCEINLINE uint32_t fnv1a_32_rt_ci(const char* str) {
    uint32_t hash = fnv32_offset_basis;
    for (; *str; ++str) {
        hash ^= static_cast<uint32_t>(static_cast<uint8_t>(to_lower(*str)));
        hash *= fnv32_prime;
    }
    return hash;
}

KC_FORCEINLINE uint32_t fnv1a_32_rt_wide(const wchar_t* str) {
    uint32_t hash = fnv32_offset_basis;
    for (; *str; ++str) {
        uint16_t ch = static_cast<uint16_t>(*str);
        hash ^= static_cast<uint32_t>(ch & 0xFF);
        hash *= fnv32_prime;
        hash ^= static_cast<uint32_t>((ch >> 8) & 0xFF);
        hash *= fnv32_prime;
    }
    return hash;
}

KC_FORCEINLINE uint32_t fnv1a_32_rt_wide_ci(const wchar_t* str) {
    uint32_t hash = fnv32_offset_basis;
    for (; *str; ++str) {
        uint16_t ch = static_cast<uint16_t>(to_lower_w(*str));
        hash ^= static_cast<uint32_t>(ch & 0xFF);
        hash *= fnv32_prime;
        hash ^= static_cast<uint32_t>((ch >> 8) & 0xFF);
        hash *= fnv32_prime;
    }
    return hash;
}

// constexpr strlen for compile-time use
constexpr size_t ct_strlen(const char* s) {
    size_t len = 0;
    while (s[len]) ++len;
    return len;
}

constexpr size_t ct_wcslen(const wchar_t* s) {
    size_t len = 0;
    while (s[len]) ++len;
    return len;
}

} // namespace detail

// public constexpr API - 64-bit (default)
constexpr uint64_t hash(const char* str, size_t len) {
    return detail::fnv1a_64(str, len);
}

constexpr uint64_t hash_ci(const char* str, size_t len) {
    return detail::fnv1a_64_ci(str, len);
}

constexpr uint64_t hash_wide(const wchar_t* str, size_t len) {
    return detail::fnv1a_64_wide(str, len);
}

constexpr uint64_t hash_wide_ci(const wchar_t* str, size_t len) {
    return detail::fnv1a_64_wide_ci(str, len);
}

// public constexpr API - 32-bit
constexpr uint32_t hash32(const char* str, size_t len) {
    return detail::fnv1a_32(str, len);
}

constexpr uint32_t hash32_ci(const char* str, size_t len) {
    return detail::fnv1a_32_ci(str, len);
}

constexpr uint32_t hash32_wide(const wchar_t* str, size_t len) {
    return detail::fnv1a_32_wide(str, len);
}

constexpr uint32_t hash32_wide_ci(const wchar_t* str, size_t len) {
    return detail::fnv1a_32_wide_ci(str, len);
}

// runtime API - null-terminated strings
KC_FORCEINLINE uint64_t hash_rt(const char* str) {
    return detail::fnv1a_64_rt(str);
}

KC_FORCEINLINE uint64_t hash_rt_ci(const char* str) {
    return detail::fnv1a_64_rt_ci(str);
}

KC_FORCEINLINE uint64_t hash_rt_wide(const wchar_t* str) {
    return detail::fnv1a_64_rt_wide(str);
}

KC_FORCEINLINE uint64_t hash_rt_wide_ci(const wchar_t* str) {
    return detail::fnv1a_64_rt_wide_ci(str);
}

KC_FORCEINLINE uint32_t hash32_rt(const char* str) {
    return detail::fnv1a_32_rt(str);
}

KC_FORCEINLINE uint32_t hash32_rt_ci(const char* str) {
    return detail::fnv1a_32_rt_ci(str);
}

KC_FORCEINLINE uint32_t hash32_rt_wide(const wchar_t* str) {
    return detail::fnv1a_32_rt_wide(str);
}

KC_FORCEINLINE uint32_t hash32_rt_wide_ci(const wchar_t* str) {
    return detail::fnv1a_32_rt_wide_ci(str);
}

} // namespace crypto
} // namespace kernelcloak

// compile-time hash macros - 64-bit
// wrapped in integral_constant to FORCE compile-time evaluation as a template
// non-type parameter. this guarantees the string literal never gets a runtime
// address and prevents MSVC from emitting it into .rdata.
#define KC_HASH(s) \
    (::kernelcloak::detail::integral_constant< \
        ::kernelcloak::uint64_t, \
        ::kernelcloak::crypto::detail::fnv1a_64((s), ::kernelcloak::crypto::detail::ct_strlen(s))>::value)

#define KC_HASH_CI(s) \
    (::kernelcloak::detail::integral_constant< \
        ::kernelcloak::uint64_t, \
        ::kernelcloak::crypto::detail::fnv1a_64_ci((s), ::kernelcloak::crypto::detail::ct_strlen(s))>::value)

#define KC_HASH_WIDE(s) \
    (::kernelcloak::detail::integral_constant< \
        ::kernelcloak::uint64_t, \
        ::kernelcloak::crypto::detail::fnv1a_64_wide((s), ::kernelcloak::crypto::detail::ct_wcslen(s))>::value)

#define KC_HASH_WIDE_CI(s) \
    (::kernelcloak::detail::integral_constant< \
        ::kernelcloak::uint64_t, \
        ::kernelcloak::crypto::detail::fnv1a_64_wide_ci((s), ::kernelcloak::crypto::detail::ct_wcslen(s))>::value)

// compile-time hash macros - 32-bit
#define KC_HASH32(s) \
    (::kernelcloak::detail::integral_constant< \
        ::kernelcloak::uint32_t, \
        ::kernelcloak::crypto::detail::fnv1a_32((s), ::kernelcloak::crypto::detail::ct_strlen(s))>::value)

#define KC_HASH32_CI(s) \
    (::kernelcloak::detail::integral_constant< \
        ::kernelcloak::uint32_t, \
        ::kernelcloak::crypto::detail::fnv1a_32_ci((s), ::kernelcloak::crypto::detail::ct_strlen(s))>::value)

// runtime hash macros - 64-bit
#define KC_HASH_RT(s)    (::kernelcloak::crypto::detail::fnv1a_64_rt(s))
#define KC_HASH_RT_CI(s) (::kernelcloak::crypto::detail::fnv1a_64_rt_ci(s))

// runtime hash macros - wide
#define KC_HASH_RT_WIDE(s)    (::kernelcloak::crypto::detail::fnv1a_64_rt_wide(s))
#define KC_HASH_RT_WIDE_CI(s) (::kernelcloak::crypto::detail::fnv1a_64_rt_wide_ci(s))

```

`crypto/xor_cipher.h`:

```h
#pragma once
#include "../config.h"
#include "../core/types.h"
#include "../core/array.h"
#include "../core/random.h"

namespace kernelcloak {
namespace crypto {

namespace detail {

template<size_t KeyLen>
struct xor_key {
    uint8_t bytes[KeyLen];
};

template<size_t N, size_t KeyLen>
constexpr core::KArray<uint8_t, N> xor_encode(const xor_key<KeyLen>& key, const uint8_t (&input)[N]) {
    core::KArray<uint8_t, N> output{};
    uint8_t rolling = key.bytes[0];
    for (size_t i = 0; i < N; ++i) {
        uint8_t k = key.bytes[i % KeyLen] ^ rolling;
        output[i] = input[i] ^ k;
        rolling = static_cast<uint8_t>(rolling + input[i] + key.bytes[(i + 1) % KeyLen]);
    }
    return output;
}

template<size_t N, size_t KeyLen>
constexpr core::KArray<uint8_t, N> xor_encode_chars(const xor_key<KeyLen>& key, const char (&input)[N]) {
    core::KArray<uint8_t, N> output{};
    uint8_t rolling = key.bytes[0];
    for (size_t i = 0; i < N; ++i) {
        uint8_t b = static_cast<uint8_t>(input[i]);
        uint8_t k = key.bytes[i % KeyLen] ^ rolling;
        output[i] = b ^ k;
        rolling = static_cast<uint8_t>(rolling + b + key.bytes[(i + 1) % KeyLen]);
    }
    return output;
}

template<size_t N, size_t KeyLen>
KC_FORCEINLINE void xor_decode(const xor_key<KeyLen>& key, uint8_t* data) {
    uint8_t rolling = key.bytes[0];
    for (size_t i = 0; i < N; ++i) {
        uint8_t k = key.bytes[i % KeyLen] ^ rolling;
        uint8_t decoded = data[i] ^ k;
        rolling = static_cast<uint8_t>(rolling + decoded + key.bytes[(i + 1) % KeyLen]);
        data[i] = decoded;
    }
}

} // namespace detail

template<size_t N, uint32_t Seed>
class xor_encrypted {
    static constexpr size_t key_len = 16;

    static constexpr detail::xor_key<key_len> generate_key() {
        detail::xor_key<key_len> key{};
        core::ct_random rng(Seed);
        for (size_t i = 0; i < key_len; ++i)
            key.bytes[i] = static_cast<uint8_t>(rng.next() & 0xFF);
        return key;
    }

    static constexpr detail::xor_key<key_len> key_ = generate_key();

    core::KArray<uint8_t, N> data_;

public:
    constexpr xor_encrypted(const core::KArray<uint8_t, N>& encrypted)
        : data_(encrypted) {}

    KC_FORCEINLINE void decrypt(uint8_t* out) const {
        for (size_t i = 0; i < N; ++i)
            out[i] = data_[i];
        detail::xor_decode<N, key_len>(key_, out);
    }

    KC_FORCEINLINE core::KArray<uint8_t, N> decrypt() const {
        core::KArray<uint8_t, N> out{};
        for (size_t i = 0; i < N; ++i)
            out[i] = data_[i];
        detail::xor_decode<N, key_len>(key_, out.data());
        return out;
    }

    static constexpr size_t size() { return N; }
};

namespace detail {

template<size_t N, uint32_t Seed>
constexpr auto make_xor_encrypted(const uint8_t (&input)[N]) {
    constexpr size_t key_len = 16;
    core::ct_random rng(Seed);
    xor_key<key_len> key{};
    for (size_t i = 0; i < key_len; ++i)
        key.bytes[i] = static_cast<uint8_t>(rng.next() & 0xFF);
    auto enc = xor_encode<N, key_len>(key, input);
    return xor_encrypted<N, Seed>(enc);
}

template<size_t N, uint32_t Seed>
constexpr auto make_xor_from_chars(const char (&input)[N]) {
    constexpr size_t key_len = 16;
    core::ct_random rng(Seed);
    xor_key<key_len> key{};
    for (size_t i = 0; i < key_len; ++i)
        key.bytes[i] = static_cast<uint8_t>(rng.next() & 0xFF);
    auto enc = xor_encode_chars<N, key_len>(key, input);
    return xor_encrypted<N, Seed>(enc);
}

} // namespace detail

} // namespace crypto
} // namespace kernelcloak

#define KC_XOR_ENCRYPT(data) \
    ([]() { \
        constexpr auto _kc_enc = ::kernelcloak::crypto::detail::make_xor_encrypted< \
            sizeof(data), KC_RANDOM_CT()>(data); \
        return _kc_enc; \
    }())

#define KC_XOR_ENCRYPT_STR(str) \
    ([]() { \
        constexpr auto _kc_enc = ::kernelcloak::crypto::detail::make_xor_from_chars< \
            sizeof(str), KC_RANDOM_CT()>(str); \
        return _kc_enc; \
    }())

```

`crypto/xtea.h`:

```h
#pragma once
#include "../config.h"
#include "../core/types.h"
#include "../core/array.h"
#include "../core/random.h"

namespace kernelcloak {
namespace crypto {

namespace detail {

struct xtea_key {
    uint32_t k[4];
};

constexpr uint32_t xtea_delta = 0x9E3779B9u;

constexpr void xtea_encrypt_block(const xtea_key& key, uint32_t& v0, uint32_t& v1) {
    uint32_t sum = 0;
    for (int i = 0; i < KC_XTEA_ROUNDS; ++i) {
        v0 += (((v1 << 4) ^ (v1 >> 5)) + v1) ^ (sum + key.k[sum & 3]);
        sum += xtea_delta;
        v1 += (((v0 << 4) ^ (v0 >> 5)) + v0) ^ (sum + key.k[(sum >> 11) & 3]);
    }
}

constexpr void xtea_decrypt_block(const xtea_key& key, uint32_t& v0, uint32_t& v1) {
    uint32_t sum = xtea_delta * KC_XTEA_ROUNDS;
    for (int i = 0; i < KC_XTEA_ROUNDS; ++i) {
        v1 -= (((v0 << 4) ^ (v0 >> 5)) + v0) ^ (sum + key.k[(sum >> 11) & 3]);
        sum -= xtea_delta;
        v0 -= (((v1 << 4) ^ (v1 >> 5)) + v1) ^ (sum + key.k[sum & 3]);
    }
}

constexpr uint32_t read_u32_le(const uint8_t* data) {
    return static_cast<uint32_t>(data[0])
         | (static_cast<uint32_t>(data[1]) << 8)
         | (static_cast<uint32_t>(data[2]) << 16)
         | (static_cast<uint32_t>(data[3]) << 24);
}

constexpr void write_u32_le(uint8_t* data, uint32_t val) {
    data[0] = static_cast<uint8_t>(val);
    data[1] = static_cast<uint8_t>(val >> 8);
    data[2] = static_cast<uint8_t>(val >> 16);
    data[3] = static_cast<uint8_t>(val >> 24);
}

// constexpr-safe helpers that use KArray + offset instead of interior pointers
// (MSVC's constexpr evaluator can't handle &arr[off] passed to another function)
template<size_t N>
constexpr uint32_t read_u32_le_arr(const core::KArray<uint8_t, N>& arr, size_t off) {
    return static_cast<uint32_t>(arr[off])
         | (static_cast<uint32_t>(arr[off + 1]) << 8)
         | (static_cast<uint32_t>(arr[off + 2]) << 16)
         | (static_cast<uint32_t>(arr[off + 3]) << 24);
}

template<size_t N>
constexpr void write_u32_le_arr(core::KArray<uint8_t, N>& arr, size_t off, uint32_t val) {
    arr[off]     = static_cast<uint8_t>(val);
    arr[off + 1] = static_cast<uint8_t>(val >> 8);
    arr[off + 2] = static_cast<uint8_t>(val >> 16);
    arr[off + 3] = static_cast<uint8_t>(val >> 24);
}

template<size_t N>
constexpr core::KArray<uint8_t, N> xtea_encrypt_buffer(const xtea_key& key, const core::KArray<uint8_t, N>& input) {
    core::KArray<uint8_t, N> output{};

    for (size_t i = 0; i < N; ++i)
        output[i] = input[i];

    size_t full_blocks = N / 8;
    for (size_t b = 0; b < full_blocks; ++b) {
        size_t off = b * 8;
        uint32_t v0 = read_u32_le_arr(output, off);
        uint32_t v1 = read_u32_le_arr(output, off + 4);
        xtea_encrypt_block(key, v0, v1);
        write_u32_le_arr(output, off, v0);
        write_u32_le_arr(output, off + 4, v1);
    }

    // tail bytes XOR'd with key material
    size_t tail_start = full_blocks * 8;
    if (tail_start < N) {
        uint32_t pad0 = key.k[0] ^ key.k[2];
        uint32_t pad1 = key.k[1] ^ key.k[3];
        core::KArray<uint8_t, 8> pad{};
        write_u32_le_arr(pad, 0, pad0);
        write_u32_le_arr(pad, 4, pad1);
        for (size_t i = tail_start; i < N; ++i)
            output[i] ^= pad[i - tail_start];
    }

    return output;
}

template<size_t N>
KC_FORCEINLINE void xtea_decrypt_buffer(const xtea_key& key, uint8_t* data) {
    // process full 8-byte blocks
    size_t full_blocks = N / 8;
    for (size_t b = 0; b < full_blocks; ++b) {
        size_t off = b * 8;
        uint32_t v0 = read_u32_le(data + off);
        uint32_t v1 = read_u32_le(data + off + 4);
        xtea_decrypt_block(key, v0, v1);
        write_u32_le(data + off, v0);
        write_u32_le(data + off + 4, v1);
    }

    // tail bytes - same XOR as encrypt (symmetric)
    size_t tail_start = full_blocks * 8;
    if (tail_start < N) {
        uint32_t pad0 = key.k[0] ^ key.k[2];
        uint32_t pad1 = key.k[1] ^ key.k[3];
        uint8_t pad[8];
        write_u32_le(pad, pad0);
        write_u32_le(pad + 4, pad1);
        for (size_t i = tail_start; i < N; ++i)
            data[i] ^= pad[i - tail_start];
    }
}

} // namespace detail

template<size_t N, uint32_t K0, uint32_t K1, uint32_t K2, uint32_t K3>
class xtea_encrypted {
    static constexpr detail::xtea_key key_ = { { K0, K1, K2, K3 } };

    static constexpr core::KArray<uint8_t, N> encrypt_data(const uint8_t (&input)[N]) {
        core::KArray<uint8_t, N> buf{};
        for (size_t i = 0; i < N; ++i)
            buf[i] = input[i];
        return detail::xtea_encrypt_buffer<N>(key_, buf);
    }

    core::KArray<uint8_t, N> data_;

public:
    constexpr xtea_encrypted(const core::KArray<uint8_t, N>& encrypted)
        : data_(encrypted) {}

    KC_FORCEINLINE void decrypt(uint8_t* out) const {
        for (size_t i = 0; i < N; ++i)
            out[i] = data_[i];
        detail::xtea_decrypt_buffer<N>(key_, out);
    }

    KC_FORCEINLINE core::KArray<uint8_t, N> decrypt() const {
        core::KArray<uint8_t, N> out{};
        for (size_t i = 0; i < N; ++i)
            out[i] = data_[i];
        detail::xtea_decrypt_buffer<N>(key_, out.data());
        return out;
    }

    static constexpr size_t size() { return N; }
};

namespace detail {

template<size_t N, uint32_t K0, uint32_t K1, uint32_t K2, uint32_t K3>
constexpr auto make_xtea_encrypted(const uint8_t (&input)[N]) {
    core::KArray<uint8_t, N> buf{};
    for (size_t i = 0; i < N; ++i)
        buf[i] = input[i];
    auto enc = xtea_encrypt_buffer<N>({ { K0, K1, K2, K3 } }, buf);
    return xtea_encrypted<N, K0, K1, K2, K3>(enc);
}

template<size_t N, uint32_t K0, uint32_t K1, uint32_t K2, uint32_t K3>
constexpr auto make_xtea_from_chars(const char (&input)[N]) {
    core::KArray<uint8_t, N> buf{};
    for (size_t i = 0; i < N; ++i)
        buf[i] = static_cast<uint8_t>(input[i]);
    auto enc = xtea_encrypt_buffer<N>({ { K0, K1, K2, K3 } }, buf);
    return xtea_encrypted<N, K0, K1, K2, K3>(enc);
}

} // namespace detail

} // namespace crypto
} // namespace kernelcloak

// encrypt arbitrary byte data with per-site XTEA keys
#define KC_XTEA_ENCRYPT(data) \
    ([]() { \
        constexpr auto _kc_enc = ::kernelcloak::crypto::detail::make_xtea_encrypted< \
            sizeof(data), \
            KC_RANDOM_CT(), KC_RANDOM_CT(), KC_RANDOM_CT(), KC_RANDOM_CT()>(data); \
        return _kc_enc; \
    }())

// encrypt a string literal with XTEA
#define KC_XTEA_ENCRYPT_STR(str) \
    ([]() { \
        constexpr auto _kc_enc = ::kernelcloak::crypto::detail::make_xtea_from_chars< \
            sizeof(str), \
            KC_RANDOM_CT(), KC_RANDOM_CT(), KC_RANDOM_CT(), KC_RANDOM_CT()>(str); \
        return _kc_enc; \
    }())

```

`kernelcloak.h`:

```h
#pragma once

// KernelCloak - Header-only C++17 kernel-mode obfuscation library
// Include this single header to access all features.

// configuration and compiler macros
#include "config.h"

// core primitives (no external dependencies)
#include "core/types.h"
#include "core/array.h"
#include "core/memory.h"
#include "core/sync.h"
#include "core/random.h"
#include "core/string_utils.h"

// cryptographic primitives (depends on core)
#include "crypto/hash.h"
#include "crypto/xor_cipher.h"
#include "crypto/xtea.h"

// string obfuscation (depends on core + crypto)
#if KC_ENABLE_STRING_ENCRYPTION
#include "strings/encrypted_string.h"
#include "strings/encrypted_wstring.h"
#include "strings/stack_string.h"
#include "strings/layered_string.h"
#endif

// value and control flow obfuscation (depends on core)
#if KC_ENABLE_VALUE_OBFUSCATION
#include "obfuscation/value.h"
#endif

#if KC_ENABLE_MBA
#include "obfuscation/mba.h"
#include "obfuscation/compare.h"
#endif

#if KC_ENABLE_BOOLEAN_OBFUSCATION
#include "obfuscation/boolean.h"
#endif

#if KC_ENABLE_CONTROL_FLOW
#include "obfuscation/control_flow.h"
#endif

#if KC_ENABLE_CFG_FLATTEN
#include "obfuscation/cfg_flatten.h"
#include "obfuscation/cfg_protect.h"
#endif

// security features (depends on core + crypto)
// import_hiding comes first - other security headers use it for dynamic resolution
#if KC_ENABLE_IMPORT_HIDING
#include "security/import_hiding.h"
#endif

#if KC_ENABLE_ANTI_DEBUG
#include "security/anti_debug.h"
#endif

#if KC_ENABLE_ANTI_VM
#include "security/anti_vm.h"
#endif

#if KC_ENABLE_INTEGRITY
#include "security/integrity.h"
#endif

#if KC_ENABLE_PE_ERASE
#include "security/pe_erase.h"
#endif

```

`obfuscation/boolean.h`:

```h
#pragma once
#include "../config.h"
#include "../core/types.h"

#if KC_ENABLE_BOOLEAN_OBFUSCATION

#ifdef _MSC_VER
#include <intrin.h>
#endif

namespace kernelcloak {
namespace obfuscation {
namespace detail {

// opaque predicates - each is always true but not provable by the compiler
// volatile reads and intrinsics prevent MSVC /O2 from seeing through them

// rdtsc LSB | 1 is always odd
KC_NOINLINE inline bool opaque_true_0() {
    volatile unsigned __int64 tsc = __rdtsc();
    return (tsc | 1) & 1;
}

// stack address is never null
KC_NOINLINE inline bool opaque_true_1() {
    volatile int anchor = 0;
    volatile uintptr_t addr = reinterpret_cast<uintptr_t>(&anchor);
    return addr != 0;
}

// mathematical invariant: x*(x+1) is always even
KC_NOINLINE inline bool opaque_true_2() {
    volatile uint32_t x = static_cast<uint32_t>(__rdtsc());
    volatile uint32_t product = x * (x + 1);
    return (product & 1) == 0;
}

// (x | ~x) == ~0 for any x
KC_NOINLINE inline bool opaque_true_3() {
    volatile uint32_t x = static_cast<uint32_t>(__rdtsc());
    volatile uint32_t result = x | ~x;
    return result == ~static_cast<uint32_t>(0);
}

// x^x is always 0
KC_NOINLINE inline bool opaque_true_4() {
    volatile uint32_t x = static_cast<uint32_t>(__rdtsc());
    volatile uint32_t result = x ^ x;
    return result == 0;
}

// opaque false predicates - always false but not obvious

// rdtsc can't return exactly 0 in practice, but even if it could, &1 after |1 prevents it
KC_NOINLINE inline bool opaque_false_0() {
    volatile unsigned __int64 tsc = __rdtsc();
    return ((tsc | 1) & 1) == 0;
}

// stack is never at address 0
KC_NOINLINE inline bool opaque_false_1() {
    volatile int anchor = 0;
    volatile uintptr_t addr = reinterpret_cast<uintptr_t>(&anchor);
    return addr == 0;
}

// x*(x+1) is always even, so odd check is always false
KC_NOINLINE inline bool opaque_false_2() {
    volatile uint32_t x = static_cast<uint32_t>(__rdtsc());
    volatile uint32_t product = x * (x + 1);
    return (product & 1) != 0;
}

// variant selectors
template<int N>
struct opaque_true_selector;

template<> struct opaque_true_selector<0> { static KC_FORCEINLINE bool get() { return opaque_true_0(); } };
template<> struct opaque_true_selector<1> { static KC_FORCEINLINE bool get() { return opaque_true_1(); } };
template<> struct opaque_true_selector<2> { static KC_FORCEINLINE bool get() { return opaque_true_2(); } };
template<> struct opaque_true_selector<3> { static KC_FORCEINLINE bool get() { return opaque_true_3(); } };
template<> struct opaque_true_selector<4> { static KC_FORCEINLINE bool get() { return opaque_true_4(); } };

template<int N>
struct opaque_false_selector;

template<> struct opaque_false_selector<0> { static KC_FORCEINLINE bool get() { return opaque_false_0(); } };
template<> struct opaque_false_selector<1> { static KC_FORCEINLINE bool get() { return opaque_false_1(); } };
template<> struct opaque_false_selector<2> { static KC_FORCEINLINE bool get() { return opaque_false_2(); } };

// boolean wrapping - combines expression with opaque predicate
template<int Variant>
KC_FORCEINLINE bool wrap_bool(bool expr) {
    // expr AND opaque_true - result is expr but harder to analyze
    return expr & opaque_true_selector<Variant>::get();
}

} // namespace detail
} // namespace obfuscation
} // namespace kernelcloak

// each macro site gets a unique variant via __COUNTER__
#define KC_TRUE \
    (::kernelcloak::obfuscation::detail::opaque_true_selector< \
        static_cast<int>((__COUNTER__ * 0x45D9F3Bu ^ __LINE__) % 5)>::get())

#define KC_FALSE \
    (::kernelcloak::obfuscation::detail::opaque_false_selector< \
        static_cast<int>((__COUNTER__ * 0x45D9F3Bu ^ __LINE__) % 3)>::get())

#define KC_BOOL(expr) \
    (::kernelcloak::obfuscation::detail::wrap_bool< \
        static_cast<int>((__COUNTER__ * 0x45D9F3Bu ^ __LINE__) % 5)>( \
        static_cast<bool>(expr)))

#else // KC_ENABLE_BOOLEAN_OBFUSCATION disabled

#define KC_TRUE  (true)
#define KC_FALSE (false)
#define KC_BOOL(expr) (static_cast<bool>(expr))

#endif // KC_ENABLE_BOOLEAN_OBFUSCATION

```

`obfuscation/cfg_flatten.h`:

```h
#pragma once
#include "../config.h"
#include "../core/types.h"

#if KC_ENABLE_CFG_FLATTEN

#ifdef _MSC_VER
#include <intrin.h>
#endif

namespace kernelcloak {
namespace obfuscation {
namespace detail {

// keyed FNV-1a for block label hashing.
// takes a per-function seed instead of the standard offset basis so that
// label hashes can't be recovered by running standard FNV-1a against a
// wordlist. an analyst would need to extract each function's unique seed
// before they can even attempt to brute-force label names.
constexpr uint32_t cfg_hash(const char* str, uint32_t seed) {
    uint32_t h = seed;
    while (*str) {
        h ^= static_cast<uint32_t>(*str++);
        h *= 0x01000193u;
    }
    return h;
}

// dead block junk that uses volatile to survive DCE
KC_NOINLINE inline void cfg_dead_code() {
    volatile uint32_t x = 0xDEADC0DEu;
    volatile uint32_t y = x ^ 0xBAADF00Du;
    volatile uint32_t z = y * 0x1337u;
    (void)z;
    __nop();
}

} // namespace detail
} // namespace obfuscation
} // namespace kernelcloak

// state encryption key derived from __COUNTER__ at function definition site
// each flattened function gets a unique XOR key for state transitions
// AND a unique hash seed so case label values are unrecoverable without
// knowing the per-function key

#define KC_FLAT_FUNC(ret_type, name, ...) \
    ret_type name(__VA_ARGS__) { \
        constexpr ::kernelcloak::uint32_t _kc_flat_key = \
            static_cast<::kernelcloak::uint32_t>( \
                (__COUNTER__ + 1) * 0x45D9F3Bu ^ 0xCC9E2D51u); \
        volatile ::kernelcloak::uint32_t _kc_state = \
            ::kernelcloak::obfuscation::detail::cfg_hash("__entry", _kc_flat_key) ^ _kc_flat_key; \
        volatile bool _kc_running = true; \
        ret_type _kc_ret_val{}; \
        while (_kc_running) { \
            ::kernelcloak::uint32_t _kc_decoded = _kc_state ^ _kc_flat_key; \
            switch (_kc_decoded) { \
            case ::kernelcloak::obfuscation::detail::cfg_hash("__entry", _kc_flat_key):

#define KC_FLAT_BLOCK(label) \
                break; \
            case ::kernelcloak::obfuscation::detail::cfg_hash(#label, _kc_flat_key):

#define KC_FLAT_GOTO(label) \
                _kc_state = ::kernelcloak::obfuscation::detail::cfg_hash(#label, _kc_flat_key) ^ _kc_flat_key; \
                break;

#define KC_FLAT_IF(cond, true_label, false_label) \
                if (cond) { \
                    _kc_state = ::kernelcloak::obfuscation::detail::cfg_hash(#true_label, _kc_flat_key) ^ _kc_flat_key; \
                } else { \
                    _kc_state = ::kernelcloak::obfuscation::detail::cfg_hash(#false_label, _kc_flat_key) ^ _kc_flat_key; \
                } \
                break;

#define KC_FLAT_RETURN(value) \
                _kc_ret_val = (value); \
                _kc_running = false; \
                break;

// dead blocks injected before the default case
#define KC_FLAT_END() \
                break; \
            case ::kernelcloak::obfuscation::detail::cfg_hash("__dead_0", _kc_flat_key): \
                ::kernelcloak::obfuscation::detail::cfg_dead_code(); \
                _kc_state = ::kernelcloak::obfuscation::detail::cfg_hash("__dead_1", _kc_flat_key) ^ _kc_flat_key; \
                break; \
            case ::kernelcloak::obfuscation::detail::cfg_hash("__dead_1", _kc_flat_key): \
                { volatile ::kernelcloak::uint32_t _d = 0xFEEDu; (void)_d; } \
                _kc_state = ::kernelcloak::obfuscation::detail::cfg_hash("__dead_2", _kc_flat_key) ^ _kc_flat_key; \
                break; \
            case ::kernelcloak::obfuscation::detail::cfg_hash("__dead_2", _kc_flat_key): \
                __nop(); \
                _kc_running = false; \
                break; \
            default: \
                _kc_running = false; \
                break; \
            } \
        } \
        return _kc_ret_val; \
    }

// split variant for complex functions that need variable declarations
// between the function head and the dispatch loop.
// usage:
//   KC_FLAT_FUNC_HEAD(NTSTATUS, myFunc, int arg1, int arg2)
//       int local1 = 0;
//       int local2 = 0;
//   KC_FLAT_ENTER()
//       // __entry block body
//   KC_FLAT_BLOCK(next) ...
//   KC_FLAT_END()

#define KC_FLAT_FUNC_HEAD(ret_type, name, ...) \
    ret_type name(__VA_ARGS__) { \
        constexpr ::kernelcloak::uint32_t _kc_flat_key = \
            static_cast<::kernelcloak::uint32_t>( \
                (__COUNTER__ + 1) * 0x45D9F3Bu ^ 0xCC9E2D51u); \
        ret_type _kc_ret_val{};

#define KC_FLAT_ENTER() \
        volatile ::kernelcloak::uint32_t _kc_state = \
            ::kernelcloak::obfuscation::detail::cfg_hash("__entry", _kc_flat_key) ^ _kc_flat_key; \
        volatile bool _kc_running = true; \
        while (_kc_running) { \
            ::kernelcloak::uint32_t _kc_decoded = _kc_state ^ _kc_flat_key; \
            switch (_kc_decoded) { \
            case ::kernelcloak::obfuscation::detail::cfg_hash("__entry", _kc_flat_key):

#else // KC_ENABLE_CFG_FLATTEN disabled

// passthrough - these expand to normal function structure
// disabled mode is approximate since CFG flattening fundamentally changes structure
#define KC_FLAT_FUNC(ret_type, name, ...) \
    ret_type name(__VA_ARGS__) { \
        ret_type _kc_ret_val{};

#define KC_FLAT_BLOCK(label)
#define KC_FLAT_GOTO(label)
#define KC_FLAT_IF(cond, true_label, false_label)

#define KC_FLAT_RETURN(value) \
        return (value);

#define KC_FLAT_END() \
        return _kc_ret_val; \
    }

#define KC_FLAT_FUNC_HEAD(ret_type, name, ...) \
    ret_type name(__VA_ARGS__) { \
        ret_type _kc_ret_val{};

#define KC_FLAT_ENTER()

#endif // KC_ENABLE_CFG_FLATTEN

```

`obfuscation/cfg_protect.h`:

```h
#pragma once
#include "../config.h"
#include "../core/types.h"
#include "boolean.h"
#include "control_flow.h"

#if KC_ENABLE_CONTROL_FLOW

namespace kernelcloak {
namespace obfuscation {
namespace detail {

// lightweight protection state - volatile to prevent optimization
struct protect_guard {
    volatile uint32_t state;

    KC_FORCEINLINE protect_guard()
        : state(static_cast<uint32_t>(__rdtsc()) | 1) {}

    KC_FORCEINLINE ~protect_guard() {
        // exit barrier - opaque check that always passes
        volatile uint32_t s = state;
        if ((s | 1) == 0) {
            // unreachable but compiler can't prove it
            __nop();
        }
        state = 0;
    }

    KC_FORCEINLINE void checkpoint() {
        volatile uint32_t s = state;
        state = s ^ 0x9E3779B9u;
    }
};

} // namespace detail
} // namespace obfuscation
} // namespace kernelcloak

// KC_PROTECT - wraps a code body in lightweight CFG protection
// injects opaque predicates at entry/exit, junk between operations
// usage: auto result = KC_PROTECT(int, { /* body that produces int */ 42; });
#define KC_PROTECT(ret_type, body) \
    [&]() -> ret_type { \
        ::kernelcloak::obfuscation::detail::protect_guard _kc_pg; \
        KC_JUNK(); \
        if (KC_FALSE) { \
            volatile ::kernelcloak::uint32_t _kc_dead = 0xBADu; \
            (void)_kc_dead; \
        } \
        _kc_pg.checkpoint(); \
        ret_type _kc_result = [&]() -> ret_type body (); \
        _kc_pg.checkpoint(); \
        KC_JUNK(); \
        return _kc_result; \
    }()

// KC_PROTECT_VOID - same as KC_PROTECT but for void return type
#define KC_PROTECT_VOID(body) \
    [&]() { \
        ::kernelcloak::obfuscation::detail::protect_guard _kc_pg; \
        KC_JUNK(); \
        if (KC_FALSE) { \
            volatile ::kernelcloak::uint32_t _kc_dead = 0xBADu; \
            (void)_kc_dead; \
        } \
        _kc_pg.checkpoint(); \
        [&]() body (); \
        _kc_pg.checkpoint(); \
        KC_JUNK(); \
    }()

#else // KC_ENABLE_CONTROL_FLOW disabled

#define KC_PROTECT(ret_type, body) \
    [&]() -> ret_type body ()

#define KC_PROTECT_VOID(body) \
    [&]() body ()

#endif // KC_ENABLE_CONTROL_FLOW

```

`obfuscation/compare.h`:

```h
#pragma once
#include "../config.h"
#include "../core/types.h"
#include "mba.h"

#if KC_ENABLE_VALUE_OBFUSCATION

namespace kernelcloak {
namespace obfuscation {
namespace detail {

// obfuscated comparison helpers
// compute difference through MBA, then test result properties
// volatile intermediates prevent MSVC from simplifying back to cmp

template<typename T>
KC_FORCEINLINE bool obf_eq(T a, T b) {
    volatile T va = a, vb = b;
    // a == b iff (a ^ b) == 0, computed through MBA
    T diff = mba_xor<T, 0>::compute(va, vb);
    volatile T vdiff = diff;
    // cast to unsigned for the or-reduction
    using U = typename kernelcloak::detail::make_unsigned<T>::type;
    volatile U udiff = static_cast<U>(vdiff);
    return udiff == static_cast<U>(0);
}

template<typename T>
KC_FORCEINLINE bool obf_ne(T a, T b) {
    volatile T va = a, vb = b;
    T diff = mba_xor<T, 1>::compute(va, vb);
    volatile T vdiff = diff;
    using U = typename kernelcloak::detail::make_unsigned<T>::type;
    volatile U udiff = static_cast<U>(vdiff);
    return udiff != static_cast<U>(0);
}

template<typename T>
KC_FORCEINLINE bool obf_lt(T a, T b) {
    volatile T va = a, vb = b;
    // a < b iff (a - b) is negative for signed, or borrow for unsigned
    T diff = mba_sub<T, 0>::compute(va, vb);
    volatile T vdiff = diff;
    // check sign bit for signed types, use subtraction borrow for unsigned
    if constexpr (kernelcloak::detail::is_signed<T>::value) {
        return vdiff < static_cast<T>(0);
    } else {
        // for unsigned: a < b iff a - b wrapped (i.e., b > a)
        // use volatile to prevent optimization
        return va < vb ? true : false;
    }
}

template<typename T>
KC_FORCEINLINE bool obf_gt(T a, T b) {
    return obf_lt(b, a);
}

template<typename T>
KC_FORCEINLINE bool obf_le(T a, T b) {
    return !obf_gt(a, b);
}

template<typename T>
KC_FORCEINLINE bool obf_ge(T a, T b) {
    return !obf_lt(a, b);
}

template<typename T>
KC_FORCEINLINE uintptr_t as_uintptr(T v) noexcept {
    if constexpr (kernelcloak::detail::is_pointer<T>::value) {
        return reinterpret_cast<uintptr_t>(v);
    } else {
        return static_cast<uintptr_t>(v);
    }
}

template<typename A, typename B>
KC_FORCEINLINE bool obf_eq_any(A a, B b) {
    if constexpr (kernelcloak::detail::is_pointer<A>::value || kernelcloak::detail::is_pointer<B>::value) {
        return obf_eq<uintptr_t>(as_uintptr(a), as_uintptr(b));
    } else {
        using T = decltype(a + b);
        return obf_eq<T>(static_cast<T>(a), static_cast<T>(b));
    }
}

template<typename A, typename B>
KC_FORCEINLINE bool obf_ne_any(A a, B b) {
    if constexpr (kernelcloak::detail::is_pointer<A>::value || kernelcloak::detail::is_pointer<B>::value) {
        return obf_ne<uintptr_t>(as_uintptr(a), as_uintptr(b));
    } else {
        using T = decltype(a + b);
        return obf_ne<T>(static_cast<T>(a), static_cast<T>(b));
    }
}

template<typename A, typename B>
KC_FORCEINLINE bool obf_lt_any(A a, B b) {
    if constexpr (kernelcloak::detail::is_pointer<A>::value || kernelcloak::detail::is_pointer<B>::value) {
        return obf_lt<uintptr_t>(as_uintptr(a), as_uintptr(b));
    } else {
        using T = decltype(a + b);
        return obf_lt<T>(static_cast<T>(a), static_cast<T>(b));
    }
}

template<typename A, typename B>
KC_FORCEINLINE bool obf_gt_any(A a, B b) {
    return obf_lt_any(b, a);
}

template<typename A, typename B>
KC_FORCEINLINE bool obf_le_any(A a, B b) {
    return !obf_gt_any(a, b);
}

template<typename A, typename B>
KC_FORCEINLINE bool obf_ge_any(A a, B b) {
    return !obf_lt_any(a, b);
}

} // namespace detail
} // namespace obfuscation
} // namespace kernelcloak

#define KC_EQ(a, b) \
    [&]() -> bool { \
        return ::kernelcloak::obfuscation::detail::obf_eq_any((a), (b)); \
    }()

#define KC_NE(a, b) \
    [&]() -> bool { \
        return ::kernelcloak::obfuscation::detail::obf_ne_any((a), (b)); \
    }()

#define KC_LT(a, b) \
    [&]() -> bool { \
        return ::kernelcloak::obfuscation::detail::obf_lt_any((a), (b)); \
    }()

#define KC_GT(a, b) \
    [&]() -> bool { \
        return ::kernelcloak::obfuscation::detail::obf_gt_any((a), (b)); \
    }()

#define KC_LE(a, b) \
    [&]() -> bool { \
        return ::kernelcloak::obfuscation::detail::obf_le_any((a), (b)); \
    }()

#define KC_GE(a, b) \
    [&]() -> bool { \
        return ::kernelcloak::obfuscation::detail::obf_ge_any((a), (b)); \
    }()

#else // KC_ENABLE_VALUE_OBFUSCATION disabled

#define KC_EQ(a, b) ((a) == (b))
#define KC_NE(a, b) ((a) != (b))
#define KC_LT(a, b) ((a) <  (b))
#define KC_GT(a, b) ((a) >  (b))
#define KC_LE(a, b) ((a) <= (b))
#define KC_GE(a, b) ((a) >= (b))

#endif // KC_ENABLE_VALUE_OBFUSCATION

```

`obfuscation/control_flow.h`:

```h
#pragma once
#include "../config.h"
#include "../core/types.h"
#include "boolean.h"

#if KC_ENABLE_CONTROL_FLOW

#ifdef _MSC_VER
#include <intrin.h>
#endif

namespace kernelcloak {
namespace obfuscation {
namespace detail {

// junk instruction generation that survives DCE
// volatile writes ensure the compiler can't eliminate them

KC_NOINLINE inline void junk_volatile_write() {
    volatile uint32_t sink = __rdtsc() & 0xFFu;
    (void)sink;
}

KC_NOINLINE inline void junk_nop_sled() {
    __nop();
    __nop();
    __nop();
    volatile int x = 1;
    x = x ^ x;
    (void)x;
}

KC_NOINLINE inline void junk_arithmetic() {
    volatile uint32_t a = static_cast<uint32_t>(__rdtsc());
    volatile uint32_t b = a * 0x9E3779B9u;
    volatile uint32_t c = b ^ (a >> 16);
    volatile uint32_t d = c + a;
    (void)d;
}

KC_NOINLINE inline void junk_stack_noise() {
    volatile uint8_t garbage[16];
    volatile uint32_t seed = static_cast<uint32_t>(__rdtsc());
    for (int i = 0; i < 16; ++i) {
        garbage[i] = static_cast<uint8_t>(seed >> (i & 3));
    }
    (void)garbage;
}

// larger junk flow block with fake branching
KC_NOINLINE inline void junk_flow_block() {
    volatile uint32_t state = static_cast<uint32_t>(__rdtsc());
    volatile uint32_t counter = 0;

    // fake state machine - always terminates in 1 iteration
    // but compiler can't prove it statically
    do {
        state = state * 0x45D9F3Bu + 0x1B873593u;
        counter += 1;

        volatile uint32_t temp = state ^ counter;
        if (temp & 0x80000000u) {
            state = state >> 1;
        } else {
            state = state ^ 0xDEADBEEFu;
        }
    } while (counter == 0); // always false after first iteration

    volatile uint32_t sink = state;
    (void)sink;
}

// junk selector
template<int N>
struct junk_selector;

template<> struct junk_selector<0> { static KC_FORCEINLINE void emit() { junk_volatile_write(); } };
template<> struct junk_selector<1> { static KC_FORCEINLINE void emit() { junk_nop_sled(); } };
template<> struct junk_selector<2> { static KC_FORCEINLINE void emit() { junk_arithmetic(); } };
template<> struct junk_selector<3> { static KC_FORCEINLINE void emit() { junk_stack_noise(); } };

} // namespace detail
} // namespace obfuscation
} // namespace kernelcloak

// if/else with opaque predicate injection
// condition is wrapped in KC_BOOL and ANDed with an opaque true
#define KC_IF(expr) \
    if (KC_BOOL(expr) & KC_TRUE) {

#define KC_ELSE \
    } else { \
        if (KC_FALSE) { \
            volatile ::kernelcloak::uint32_t _kc_dead = 0xDEADu; \
            (void)_kc_dead; \
        }

#define KC_ENDIF \
    }

// single junk instruction block
#define KC_JUNK() \
    do { \
        ::kernelcloak::obfuscation::detail::junk_selector< \
            static_cast<int>((__COUNTER__ * 0x45D9F3Bu ^ __LINE__) % 4)>::emit(); \
    } while (0)

// larger junk flow block
#define KC_JUNK_FLOW() \
    do { \
        ::kernelcloak::obfuscation::detail::junk_flow_block(); \
    } while (0)

#else // KC_ENABLE_CONTROL_FLOW disabled

#define KC_IF(expr) if (expr) {
#define KC_ELSE    } else {
#define KC_ENDIF   }
#define KC_JUNK()       ((void)0)
#define KC_JUNK_FLOW()  ((void)0)

#endif // KC_ENABLE_CONTROL_FLOW

```

`obfuscation/mba.h`:

```h
#pragma once
#include "../config.h"
#include "../core/types.h"

#if KC_ENABLE_MBA

namespace kernelcloak {
namespace obfuscation {
namespace detail {

// mixed boolean arithmetic decompositions
// each operation has multiple equivalent forms selected by compile-time Variant param

// a + b decompositions
template<typename T, int Variant>
struct mba_add;

// (a ^ b) + 2*(a & b)
template<typename T>
struct mba_add<T, 0> {
    static KC_FORCEINLINE T compute(T a, T b) {
        volatile T va = a, vb = b;
        T x = va ^ vb;
        T y = va & vb;
        return x + (y << 1);
    }
};

// (a | b) + (a & b)
template<typename T>
struct mba_add<T, 1> {
    static KC_FORCEINLINE T compute(T a, T b) {
        volatile T va = a, vb = b;
        return (va | vb) + (va & vb);
    }
};

// ((a & b) << 1) + (a ^ b) + (0 & ~0) -- noise term cancels
template<typename T>
struct mba_add<T, 2> {
    static KC_FORCEINLINE T compute(T a, T b) {
        volatile T va = a, vb = b;
        T noise = static_cast<T>(0) & ~static_cast<T>(0);
        return ((va & vb) << 1) + (va ^ vb) + noise;
    }
};

// a - b decompositions
template<typename T, int Variant>
struct mba_sub;

// a + (~b) + 1
template<typename T>
struct mba_sub<T, 0> {
    static KC_FORCEINLINE T compute(T a, T b) {
        volatile T va = a, vb = b;
        return va + (~vb) + static_cast<T>(1);
    }
};

// (a ^ (~b)) + ((~(a | b) + (a & (~b))) << 1) -- simplified to: (a ^ ~b) - ~(a|b)*2 + (a&~b)*2
// actually just: (a & ~b) - (~a & b) which is a - b
template<typename T>
struct mba_sub<T, 1> {
    static KC_FORCEINLINE T compute(T a, T b) {
        volatile T va = a, vb = b;
        return (va & ~vb) - (~va & vb);
    }
};

// (a ^ b) - ((~a & b) << 1)
template<typename T>
struct mba_sub<T, 2> {
    static KC_FORCEINLINE T compute(T a, T b) {
        volatile T va = a, vb = b;
        return (va ^ vb) - ((~va & vb) << 1);
    }
};

// a & b decompositions
template<typename T, int Variant>
struct mba_and;

// (a + b - (a ^ b)) >> 1
template<typename T>
struct mba_and<T, 0> {
    static KC_FORCEINLINE T compute(T a, T b) {
        volatile T va = a, vb = b;
        // unsigned shift for correctness
        using U = typename kernelcloak::detail::make_unsigned<T>::type;
        return static_cast<T>((static_cast<U>(va) + static_cast<U>(vb)
            - static_cast<U>(va ^ vb)) >> 1);
    }
};

// ~(~a | ~b) -- de morgan
template<typename T>
struct mba_and<T, 1> {
    static KC_FORCEINLINE T compute(T a, T b) {
        volatile T va = a, vb = b;
        return ~(~va | ~vb);
    }
};

// ((a ^ b) ^ b) & b -- simplifies to a & b since (a^b)^b == a
template<typename T>
struct mba_and<T, 2> {
    static KC_FORCEINLINE T compute(T a, T b) {
        volatile T va = a, vb = b;
        T t = (va ^ vb) ^ vb;
        return t & vb;
    }
};

// a | b decompositions
template<typename T, int Variant>
struct mba_or;

// (a ^ b) + (a & b)  -- standard MBA identity
template<typename T>
struct mba_or<T, 0> {
    static KC_FORCEINLINE T compute(T a, T b) {
        volatile T va = a, vb = b;
        return (va ^ vb) + (va & vb);
    }
};

// ~(~a & ~b) -- de morgan
template<typename T>
struct mba_or<T, 1> {
    static KC_FORCEINLINE T compute(T a, T b) {
        volatile T va = a, vb = b;
        return ~(~va & ~vb);
    }
};

// ((a ^ b) | b) -- since (a^b)|b == a|b
template<typename T>
struct mba_or<T, 2> {
    static KC_FORCEINLINE T compute(T a, T b) {
        volatile T va = a, vb = b;
        return (va ^ vb) | vb;
    }
};

// a ^ b decompositions
template<typename T, int Variant>
struct mba_xor;

// (a | b) - (a & b)
template<typename T>
struct mba_xor<T, 0> {
    static KC_FORCEINLINE T compute(T a, T b) {
        volatile T va = a, vb = b;
        return (va | vb) - (va & vb);
    }
};

// (a & ~b) | (~a & b)
template<typename T>
struct mba_xor<T, 1> {
    static KC_FORCEINLINE T compute(T a, T b) {
        volatile T va = a, vb = b;
        return (va & ~vb) | (~va & vb);
    }
};

// ~(~(a & ~b) & ~(~a & b)) -- nested de morgan
template<typename T>
struct mba_xor<T, 2> {
    static KC_FORCEINLINE T compute(T a, T b) {
        volatile T va = a, vb = b;
        return ~(~(va & ~vb) & ~(~va & vb));
    }
};

// negation: -a = ~a + 1, with MBA wrapping
template<typename T, int Variant>
struct mba_neg;

template<typename T>
struct mba_neg<T, 0> {
    static KC_FORCEINLINE T compute(T a) {
        volatile T va = a;
        return (~va) + static_cast<T>(1);
    }
};

template<typename T>
struct mba_neg<T, 1> {
    static KC_FORCEINLINE T compute(T a) {
        volatile T va = a;
        // -a = (a ^ -1) + 1 = ~a + 1, but expressed differently
        return (va ^ static_cast<T>(-1)) + static_cast<T>(1);
    }
};

template<typename T>
struct mba_neg<T, 2> {
    static KC_FORCEINLINE T compute(T a) {
        volatile T va = a;
        // -a = ~a + 1 = (~a | 0) + (1 & ~0) -- noise wrapping
        T inv = ~va;
        T noise = static_cast<T>(1) & ~static_cast<T>(0);
        return (inv | static_cast<T>(0)) + noise;
    }
};

} // namespace detail
} // namespace obfuscation
} // namespace kernelcloak

// variant selection via __COUNTER__ mod 3
#define KC_MBA_VARIANT() ((__COUNTER__ * 0x45D9F3Bu ^ __LINE__) % 3)

#define KC_MBA(x) (x)

#define KC_ADD(a, b) \
    [&]() -> decltype((a) + (b)) { \
        using _kc_T = decltype((a) + (b)); \
        constexpr int _kc_v = static_cast<int>((__COUNTER__ * 0x45D9F3Bu ^ __LINE__) % 3); \
        return ::kernelcloak::obfuscation::detail::mba_add<_kc_T, _kc_v>::compute( \
            static_cast<_kc_T>(a), static_cast<_kc_T>(b)); \
    }()

#define KC_SUB(a, b) \
    [&]() -> decltype((a) - (b)) { \
        using _kc_T = decltype((a) - (b)); \
        constexpr int _kc_v = static_cast<int>((__COUNTER__ * 0x45D9F3Bu ^ __LINE__) % 3); \
        return ::kernelcloak::obfuscation::detail::mba_sub<_kc_T, _kc_v>::compute( \
            static_cast<_kc_T>(a), static_cast<_kc_T>(b)); \
    }()

#define KC_AND(a, b) \
    [&]() -> decltype((a) & (b)) { \
        using _kc_T = decltype((a) & (b)); \
        constexpr int _kc_v = static_cast<int>((__COUNTER__ * 0x45D9F3Bu ^ __LINE__) % 3); \
        return ::kernelcloak::obfuscation::detail::mba_and<_kc_T, _kc_v>::compute( \
            static_cast<_kc_T>(a), static_cast<_kc_T>(b)); \
    }()

#define KC_OR(a, b) \
    [&]() -> decltype((a) | (b)) { \
        using _kc_T = decltype((a) | (b)); \
        constexpr int _kc_v = static_cast<int>((__COUNTER__ * 0x45D9F3Bu ^ __LINE__) % 3); \
        return ::kernelcloak::obfuscation::detail::mba_or<_kc_T, _kc_v>::compute( \
            static_cast<_kc_T>(a), static_cast<_kc_T>(b)); \
    }()

#define KC_XOR(a, b) \
    [&]() -> decltype((a) ^ (b)) { \
        using _kc_T = decltype((a) ^ (b)); \
        constexpr int _kc_v = static_cast<int>((__COUNTER__ * 0x45D9F3Bu ^ __LINE__) % 3); \
        return ::kernelcloak::obfuscation::detail::mba_xor<_kc_T, _kc_v>::compute( \
            static_cast<_kc_T>(a), static_cast<_kc_T>(b)); \
    }()

#define KC_NEG(a) \
    [&]() -> decltype(-(a)) { \
        using _kc_T = decltype(-(a)); \
        constexpr int _kc_v = static_cast<int>((__COUNTER__ * 0x45D9F3Bu ^ __LINE__) % 3); \
        return ::kernelcloak::obfuscation::detail::mba_neg<_kc_T, _kc_v>::compute( \
            static_cast<_kc_T>(a)); \
    }()

#else // KC_ENABLE_MBA disabled

#define KC_MBA(x)     (x)
#define KC_ADD(a, b)  ((a) + (b))
#define KC_SUB(a, b)  ((a) - (b))
#define KC_AND(a, b)  ((a) & (b))
#define KC_OR(a, b)   ((a) | (b))
#define KC_XOR(a, b)  ((a) ^ (b))
#define KC_NEG(a)     (-(a))

#endif // KC_ENABLE_MBA

```

`obfuscation/value.h`:

```h
#pragma once
#include "../config.h"
#include "../core/types.h"

#if KC_ENABLE_VALUE_OBFUSCATION

#ifdef _MSC_VER
#include <intrin.h>
#endif

namespace kernelcloak {
namespace obfuscation {
namespace detail {

// compiler barrier to prevent MSVC from folding XOR pairs
KC_FORCEINLINE void compiler_barrier() {
#ifdef _MSC_VER
    _ReadWriteBarrier();
#else
    __asm__ __volatile__("" ::: "memory");
#endif
}

// obfuscated integer storage - value XOR'd with compile-time key
// dispatch is handled by obfuscated_value alias, no SFINAE needed here
template<typename T, uint32_t Key>
class obfuscated_int {
    volatile T stored_;

    static KC_FORCEINLINE T encode(T val) {
        return val ^ static_cast<T>(Key);
    }

    static KC_FORCEINLINE T decode(T val) {
        return val ^ static_cast<T>(Key);
    }

public:
    KC_FORCEINLINE obfuscated_int() : stored_(encode(T(0))) {}

    KC_FORCEINLINE obfuscated_int(T val) : stored_(encode(val)) {}

    KC_FORCEINLINE operator T() const {
        T tmp = stored_;
        compiler_barrier();
        return decode(tmp);
    }

    KC_FORCEINLINE obfuscated_int& operator=(T val) {
        stored_ = encode(val);
        return *this;
    }

    KC_FORCEINLINE obfuscated_int& operator+=(T val) {
        T cur = decode(stored_);
        stored_ = encode(cur + val);
        return *this;
    }

    KC_FORCEINLINE obfuscated_int& operator-=(T val) {
        T cur = decode(stored_);
        stored_ = encode(cur - val);
        return *this;
    }

    KC_FORCEINLINE obfuscated_int& operator*=(T val) {
        T cur = decode(stored_);
        stored_ = encode(cur * val);
        return *this;
    }

    KC_FORCEINLINE obfuscated_int& operator&=(T val) {
        T cur = decode(stored_);
        stored_ = encode(cur & val);
        return *this;
    }

    KC_FORCEINLINE obfuscated_int& operator|=(T val) {
        T cur = decode(stored_);
        stored_ = encode(cur | val);
        return *this;
    }

    KC_FORCEINLINE obfuscated_int& operator^=(T val) {
        T cur = decode(stored_);
        stored_ = encode(cur ^ val);
        return *this;
    }

    KC_FORCEINLINE obfuscated_int& operator++() {
        T cur = decode(stored_);
        stored_ = encode(cur + T(1));
        return *this;
    }

    KC_FORCEINLINE T operator++(int) {
        T cur = decode(stored_);
        stored_ = encode(cur + T(1));
        return cur;
    }

    KC_FORCEINLINE obfuscated_int& operator--() {
        T cur = decode(stored_);
        stored_ = encode(cur - T(1));
        return *this;
    }

    KC_FORCEINLINE T operator--(int) {
        T cur = decode(stored_);
        stored_ = encode(cur - T(1));
        return cur;
    }
};

// obfuscated pointer storage - XOR'd with compile-time key
template<typename T, uint32_t Key>
class obfuscated_ptr {
    volatile uintptr_t stored_;

    static KC_FORCEINLINE uintptr_t encode(T val) {
        return reinterpret_cast<uintptr_t>(val) ^ static_cast<uintptr_t>(Key);
    }

    static KC_FORCEINLINE T decode(uintptr_t val) {
        return reinterpret_cast<T>(val ^ static_cast<uintptr_t>(Key));
    }

public:
    KC_FORCEINLINE obfuscated_ptr() : stored_(encode(nullptr)) {}

    KC_FORCEINLINE obfuscated_ptr(T val) : stored_(encode(val)) {}

    KC_FORCEINLINE operator T() const {
        uintptr_t tmp = stored_;
        compiler_barrier();
        return decode(tmp);
    }

    KC_FORCEINLINE obfuscated_ptr& operator=(T val) {
        stored_ = encode(val);
        return *this;
    }

    KC_FORCEINLINE auto operator*() const -> decltype(*static_cast<T>(nullptr)) {
        return *static_cast<T>(decode(stored_));
    }

    KC_FORCEINLINE T operator->() const {
        return decode(stored_);
    }

    KC_FORCEINLINE bool operator==(T other) const {
        return decode(stored_) == other;
    }

    KC_FORCEINLINE bool operator!=(T other) const {
        return decode(stored_) != other;
    }
};

// type dispatcher - picks int vs ptr implementation
template<typename T, uint32_t Key>
using obfuscated_value = kernelcloak::detail::conditional_t<
    kernelcloak::detail::is_pointer<T>::value,
    obfuscated_ptr<T, Key>,
    obfuscated_int<T, Key>
>;

} // namespace detail
} // namespace obfuscation
} // namespace kernelcloak

#define KC_INT(x) \
    ::kernelcloak::obfuscation::detail::obfuscated_value< \
        decltype(x), \
        static_cast<::kernelcloak::uint32_t>( \
            (__COUNTER__ + 1) * 0x45D9F3Bu ^ __LINE__ * 0x1B873593u \
        )>(x)

#else // KC_ENABLE_VALUE_OBFUSCATION disabled

#define KC_INT(x) (x)

#endif // KC_ENABLE_VALUE_OBFUSCATION

```

`security/anti_debug.h`:

```h
#pragma once
#include "../config.h"
#include "../core/types.h"
#include "../core/memory.h"
#include "../crypto/hash.h"

#if KC_ENABLE_ANTI_DEBUG

#if KC_ENABLE_IMPORT_HIDING
#include "import_hiding.h"
#endif

// type declarations needed for both paths
#if !defined(_NTDDK_) && !defined(_WDMDDK_)
extern "C" {
#ifndef _LARGE_INTEGER_DEFINED
#define _LARGE_INTEGER_DEFINED
    typedef union _LARGE_INTEGER {
        struct { unsigned long LowPart; long HighPart; };
        __int64 QuadPart;
    } LARGE_INTEGER, *PLARGE_INTEGER;
#endif

    unsigned char __stdcall MmIsAddressValid(void* VirtualAddress);
}
#endif

extern "C" {
    unsigned __int64 __rdtsc();
    unsigned char __stdcall KeGetCurrentIrql();
#if defined(_M_X64) || defined(_M_AMD64)
    unsigned __int64 __readdr(unsigned int reg);
#endif
}

#pragma intrinsic(__rdtsc)
#if defined(_M_X64) || defined(_M_AMD64)
#pragma intrinsic(__readdr)
#endif

#ifndef PASSIVE_LEVEL
#define PASSIVE_LEVEL 0
#endif

namespace kernelcloak {
namespace security {

namespace detail {

// KUSER_SHARED_DATA on x64 is mapped at this fixed address
constexpr uintptr_t kuser_shared_data_addr = 0xFFFFF78000000000ull;

// offset of KdDebuggerEnabled byte within KUSER_SHARED_DATA
constexpr uintptr_t kd_debugger_enabled_offset = 0x2D4;

// timing threshold - single-stepping costs 10k+ TSC ticks per instruction,
// a simple operation should complete in well under 5000
constexpr uint64_t rdtsc_threshold = 5000;

// ─────────────────────────────────────────────────────────────
// when import hiding is available, resolve KdDebuggerEnabled,
// KdDebuggerNotPresent, KeQueryPerformanceCounter, and
// PsIsProcessBeingDebugged dynamically. this eliminates the
// anti-debug indicator IAT entries that static analysis flags.
// ─────────────────────────────────────────────────────────────

#if KC_ENABLE_IMPORT_HIDING

KC_NOINLINE inline bool check_kd_enabled() {
    __try {
        // KdDebuggerEnabled is a data export (BOOLEAN/UCHAR) from ntoskrnl.
        // resolve_import returns the address of the exported variable.
        auto kd_addr = resolve_import(KC_HASH_CI("ntoskrnl.exe"), KC_HASH_CI("KdDebuggerEnabled"));
        if (!kd_addr || !MmIsAddressValid(kd_addr))
            return false;

        return *static_cast<volatile unsigned char*>(kd_addr) != 0;
    } __except (1) {
        return false;
    }
}

// KdDebuggerNotPresent == FALSE means debugger IS present
KC_NOINLINE inline bool check_kd_not_present() {
    __try {
        auto kd_addr = resolve_import(KC_HASH_CI("ntoskrnl.exe"), KC_HASH_CI("KdDebuggerNotPresent"));
        if (!kd_addr || !MmIsAddressValid(kd_addr))
            return false;

        return *static_cast<volatile unsigned char*>(kd_addr) == 0;
    } __except (1) {
        return false;
    }
}

// KeQueryPerformanceCounter timing variant
KC_NOINLINE inline bool check_perf_counter_timing() {
    __try {
        using ke_qpc_t = LARGE_INTEGER(__stdcall*)(LARGE_INTEGER*);
        auto ke_qpc = reinterpret_cast<ke_qpc_t>(
            resolve_import(KC_HASH_CI("ntoskrnl.exe"), KC_HASH_CI("KeQueryPerformanceCounter")));
        if (!ke_qpc)
            return false;

        LARGE_INTEGER freq;
        LARGE_INTEGER t1 = ke_qpc(&freq);

        volatile uint64_t dummy = 0;
        for (int i = 0; i < 10; ++i)
            dummy += i;

        LARGE_INTEGER t2 = ke_qpc(nullptr);
        int64_t delta = t2.QuadPart - t1.QuadPart;

        // 50us threshold
        int64_t threshold = freq.QuadPart / 20000;
        if (threshold < 50) threshold = 50;

        return delta > threshold;
    } __except (1) {
        return false;
    }
}

// resolve PsIsProcessBeingDebugged via import hiding
// IRQL: PASSIVE_LEVEL only
KC_NOINLINE inline bool check_process_debugged() {
    if (KeGetCurrentIrql() != PASSIVE_LEVEL)
        return false;

    __try {
        auto fn = reinterpret_cast<unsigned char(__stdcall*)(void*)>(
            resolve_import(KC_HASH_CI("ntoskrnl.exe"), KC_HASH_CI("PsIsProcessBeingDebugged")));
        if (!fn)
            return false;

        auto get_proc = reinterpret_cast<void*(__stdcall*)()>(
            resolve_import(KC_HASH_CI("ntoskrnl.exe"), KC_HASH_CI("PsGetCurrentProcess")));
        if (!get_proc)
            return false;

        void* process = get_proc();
        if (!process)
            return false;

        return fn(process) != 0;
    } __except (1) {
        return false;
    }
}

KC_NOINLINE inline void take_response() {
#if KC_ANTI_DEBUG_RESPONSE == 1
    // resolve KeBugCheck dynamically to avoid IAT entry
    using ke_bugcheck_t = void(__stdcall*)(unsigned long);
    auto bugcheck = reinterpret_cast<ke_bugcheck_t>(
        resolve_import(KC_HASH_CI("ntoskrnl.exe"), KC_HASH_CI("KeBugCheck")));
    if (bugcheck)
        bugcheck(0x000000E2);
#elif KC_ANTI_DEBUG_RESPONSE == 2
    volatile uint8_t* sp = reinterpret_cast<volatile uint8_t*>(&sp);
    for (int i = 0; i < 4096; ++i)
        sp[i] = 0;
#else
    // response disabled
#endif
}

#else // KC_ENABLE_IMPORT_HIDING not available - fall back to static imports

// static path: these symbols will appear in IAT
#if !defined(_NTDDK_) && !defined(_WDMDDK_)
extern "C" {
    extern unsigned char KdDebuggerEnabled;
    extern unsigned char KdDebuggerNotPresent;
    LARGE_INTEGER __stdcall KeQueryPerformanceCounter(LARGE_INTEGER* PerformanceFrequency);
    void __stdcall KeBugCheck(unsigned long BugCheckCode);

#ifndef _UNICODE_STRING_DEFINED
#define _UNICODE_STRING_DEFINED
    struct _UNICODE_STRING {
        unsigned short Length;
        unsigned short MaximumLength;
        wchar_t* Buffer;
    };
    using UNICODE_STRING = _UNICODE_STRING;
    using PUNICODE_STRING = UNICODE_STRING*;
#endif

    void* __stdcall MmGetSystemRoutineAddress(UNICODE_STRING* SystemRoutineName);
}
#endif

KC_NOINLINE inline bool check_kd_enabled() {
    __try {
        return KdDebuggerEnabled != 0;
    } __except (1) {
        return false;
    }
}

KC_NOINLINE inline bool check_kd_not_present() {
    __try {
        return KdDebuggerNotPresent == 0;
    } __except (1) {
        return false;
    }
}

KC_NOINLINE inline bool check_perf_counter_timing() {
    __try {
        LARGE_INTEGER freq;
        LARGE_INTEGER t1 = KeQueryPerformanceCounter(&freq);

        volatile uint64_t dummy = 0;
        for (int i = 0; i < 10; ++i)
            dummy += i;

        LARGE_INTEGER t2 = KeQueryPerformanceCounter(nullptr);
        int64_t delta = t2.QuadPart - t1.QuadPart;

        int64_t threshold = freq.QuadPart / 20000;
        if (threshold < 50) threshold = 50;

        return delta > threshold;
    } __except (1) {
        return false;
    }
}

KC_NOINLINE inline bool check_process_debugged() {
    if (KeGetCurrentIrql() != PASSIVE_LEVEL)
        return false;

    __try {
        wchar_t name[] = L"PsIsProcessBeingDebugged";
        UNICODE_STRING uname;
        uname.Buffer = name;
        uname.Length = sizeof(name) - sizeof(wchar_t);
        uname.MaximumLength = sizeof(name);

        auto fn = reinterpret_cast<unsigned char(__stdcall*)(void*)>(
            MmGetSystemRoutineAddress(&uname));
        if (!fn)
            return false;

        wchar_t pname[] = L"PsGetCurrentProcess";
        UNICODE_STRING upname;
        upname.Buffer = pname;
        upname.Length = sizeof(pname) - sizeof(wchar_t);
        upname.MaximumLength = sizeof(pname);

        auto get_proc = reinterpret_cast<void*(__stdcall*)()>(
            MmGetSystemRoutineAddress(&upname));
        if (!get_proc)
            return false;

        void* process = get_proc();
        if (!process)
            return false;

        return fn(process) != 0;
    } __except (1) {
        return false;
    }
}

KC_NOINLINE inline void take_response() {
#if KC_ANTI_DEBUG_RESPONSE == 1
    KeBugCheck(0x000000E2);
#elif KC_ANTI_DEBUG_RESPONSE == 2
    volatile uint8_t* sp = reinterpret_cast<volatile uint8_t*>(&sp);
    for (int i = 0; i < 4096; ++i)
        sp[i] = 0;
#else
    // response disabled
#endif
}

#endif // KC_ENABLE_IMPORT_HIDING

// shared implementations (same for both paths)

// alternate path: read from KUSER_SHARED_DATA directly (no imports needed)
KC_NOINLINE inline bool check_shared_user_data() {
    __try {
        auto* ptr = reinterpret_cast<volatile uint8_t*>(
            kuser_shared_data_addr + kd_debugger_enabled_offset);
        return *ptr != 0;
    } __except (1) {
        return false;
    }
}

// hardware breakpoint detection via DR7 stub
KC_NOINLINE inline bool check_hardware_breakpoints() {
    __try {
        // check local and global enable bits for DR0-DR3
#if defined(_M_X64) || defined(_M_AMD64)
        uint64_t dr7 = __readdr(7);
        return (dr7 & 0xFFull) != 0;
#else
        return false;
#endif
    } __except (1) {
        return false;
    }
}

// RDTSC delta timing - detects single-stepping
KC_NOINLINE inline bool check_rdtsc_timing() {
    __try {
        volatile uint64_t dummy = 0;
        uint64_t t1 = __rdtsc();

        dummy = dummy + 1;
        dummy = dummy ^ 0x55;
        dummy = dummy + 1;
        dummy = dummy ^ 0xAA;
        dummy = dummy + 1;
        dummy = dummy ^ 0xFF;
        dummy = dummy + 1;

        uint64_t t2 = __rdtsc();
        return (t2 - t1) > rdtsc_threshold;
    } __except (1) {
        return false;
    }
}

KC_FORCEINLINE bool detect_kernel_debugger() {
    return check_kd_enabled() || check_kd_not_present() || check_shared_user_data();
}

KC_NOINLINE inline bool is_debugged() {
    if (detect_kernel_debugger())
        return true;
    if (check_rdtsc_timing())
        return true;
    if (check_hardware_breakpoints())
        return true;
    if (check_process_debugged())
        return true;
    return false;
}

} // namespace detail

} // namespace security
} // namespace kernelcloak

#define KC_ANTI_DEBUG() \
    do { \
        if (::kernelcloak::security::detail::is_debugged()) { \
            ::kernelcloak::security::detail::take_response(); \
        } \
    } while (0)

#define KC_IS_DEBUGGED() \
    (::kernelcloak::security::detail::is_debugged())

#define KC_HAS_HWBP() \
    (::kernelcloak::security::detail::check_hardware_breakpoints())

#define KC_TIMING_CHECK() \
    (::kernelcloak::security::detail::check_rdtsc_timing())

#define KC_DETECT_KERNEL_DBG() \
    (::kernelcloak::security::detail::detect_kernel_debugger())

#else // KC_ENABLE_ANTI_DEBUG disabled

#define KC_ANTI_DEBUG()         do {} while (0)
#define KC_IS_DEBUGGED()        (false)
#define KC_HAS_HWBP()           (false)
#define KC_TIMING_CHECK()       (false)
#define KC_DETECT_KERNEL_DBG()  (false)

#endif // KC_ENABLE_ANTI_DEBUG


```

`security/anti_vm.h`:

```h
#pragma once
#include "../config.h"
#include "../core/types.h"
#include "../core/memory.h"
#include "../crypto/hash.h"

#if KC_ENABLE_ANTI_VM

extern "C" {
    void __cpuid(int cpuInfo[4], int function_id);
    unsigned __int64 __readmsr(unsigned long register_id);
    unsigned char __stdcall KeGetCurrentIrql();
}

#if !defined(_NTDDK_) && !defined(_WDMDDK_)
extern "C" {
#ifndef _UNICODE_STRING_DEFINED
#define _UNICODE_STRING_DEFINED
    struct _UNICODE_STRING {
        unsigned short Length;
        unsigned short MaximumLength;
        wchar_t* Buffer;
    };
    using UNICODE_STRING = _UNICODE_STRING;
    using PUNICODE_STRING = UNICODE_STRING*;
#endif

    // IRQL: PASSIVE_LEVEL
    long __stdcall ZwOpenKey(void** KeyHandle, unsigned long DesiredAccess, void* ObjectAttributes);
    long __stdcall ZwQueryValueKey(void* KeyHandle, UNICODE_STRING* ValueName,
                                   unsigned long KeyValueInformationClass,
                                   void* KeyValueInformation, unsigned long Length,
                                   unsigned long* ResultLength);
    long __stdcall ZwClose(void* Handle);
    void __stdcall RtlInitUnicodeString(UNICODE_STRING* DestinationString, const wchar_t* SourceString);
    void __stdcall KeBugCheck(unsigned long BugCheckCode);
}
#endif

#pragma intrinsic(__cpuid)
#pragma intrinsic(__readmsr)

// irql constants (avoid relying on core/sync.h include order)
#ifndef PASSIVE_LEVEL
#define PASSIVE_LEVEL 0
#endif

#ifndef OBJ_CASE_INSENSITIVE
#define OBJ_CASE_INSENSITIVE 0x00000040
#endif
#ifndef OBJ_KERNEL_HANDLE
#define OBJ_KERNEL_HANDLE 0x00000200
#endif
#ifndef KEY_READ
#define KEY_READ 0x20019
#endif
#ifndef STATUS_SUCCESS
#define STATUS_SUCCESS 0x00000000L
#endif
// only define as macro when ntddk.h isn't present (it's an enum constant there)
#if !defined(_NTDDK_) && !defined(_WDMDDK_)
#ifndef KeyValuePartialInformation
#define KeyValuePartialInformation 2
#endif
#endif

namespace kernelcloak {
namespace security {

namespace detail {

struct kc_object_attributes {
    unsigned long Length;
    void* RootDirectory;
    UNICODE_STRING* ObjectName;
    unsigned long Attributes;
    void* SecurityDescriptor;
    void* SecurityQualityOfService;
};

struct key_value_partial_info {
    unsigned long TitleIndex;
    unsigned long Type;
    unsigned long DataLength;
    unsigned char Data[1];
};

// known hypervisor vendor string hashes (12-byte CPUID vendor, FNV-1a 64-bit)
namespace vm_vendors {
    constexpr uint64_t vmware     = crypto::detail::fnv1a_64("VMwareVMware", 12);
    constexpr uint64_t virtualbox = crypto::detail::fnv1a_64("VBoxVBoxVBox", 12);
    constexpr uint64_t hyperv     = crypto::detail::fnv1a_64("Microsoft Hv", 12);
    constexpr uint64_t kvm        = crypto::detail::fnv1a_64("KVMKVMKVM\0\0\0", 12);
    constexpr uint64_t xen        = crypto::detail::fnv1a_64("XenVMMXenVMM", 12);
    constexpr uint64_t qemu       = crypto::detail::fnv1a_64("TCGTCGTCGTCG", 12);
    constexpr uint64_t parallels  = crypto::detail::fnv1a_64("prl hyperv  ", 12);
}

// CPUID leaf 1, ECX bit 31
KC_NOINLINE inline bool check_hypervisor_bit() {
    __try {
        int regs[4] = {};
        __cpuid(regs, 1);
        return (regs[2] & (1 << 31)) != 0;
    } __except (1) {
        return false;
    }
}

// CPUID leaf 0x40000000 vendor string -> FNV-1a hash, or 0
KC_NOINLINE inline uint64_t get_hypervisor_vendor() {
    __try {
        if (!check_hypervisor_bit())
            return 0;

        int regs[4] = {};
        __cpuid(regs, 0x40000000);

        char vendor[13] = {};
        *reinterpret_cast<int*>(&vendor[0]) = regs[1];
        *reinterpret_cast<int*>(&vendor[4]) = regs[2];
        *reinterpret_cast<int*>(&vendor[8]) = regs[3];

        return crypto::detail::fnv1a_64(vendor, 12);
    } __except (1) {
        return 0;
    }
}

// hyper-v MSR read - faults if the hyper-v MSR interface isn't present
KC_NOINLINE inline bool check_hyperv_msr() {
    __try {
        (void)__readmsr(0x40000000);
        return true;
    } __except (1) {
        return false;
    }
}

// IRQL: PASSIVE_LEVEL
KC_NOINLINE inline bool registry_key_exists(const wchar_t* path) {
    __try {
        UNICODE_STRING key_path;
        RtlInitUnicodeString(&key_path, path);

#if defined(_NTDDK_) || defined(_WDMDDK_)
        OBJECT_ATTRIBUTES oa;
        InitializeObjectAttributes(&oa, &key_path,
            OBJ_CASE_INSENSITIVE | OBJ_KERNEL_HANDLE, nullptr, nullptr);
#else
        kc_object_attributes oa = {};
        oa.Length = sizeof(kc_object_attributes);
        oa.ObjectName = &key_path;
        oa.Attributes = OBJ_CASE_INSENSITIVE | OBJ_KERNEL_HANDLE;
#endif

        void* key_handle = nullptr;
        long status = ZwOpenKey(&key_handle, KEY_READ, &oa);
        if (status == STATUS_SUCCESS && key_handle) {
            ZwClose(key_handle);
            return true;
        }
        return false;
    } __except (1) {
        return false;
    }
}

// IRQL: PASSIVE_LEVEL
KC_NOINLINE inline bool registry_read_string(const wchar_t* key_path, const wchar_t* value_name,
                                       wchar_t* buffer, uint32_t buffer_chars) {
    __try {
        if (!buffer || buffer_chars == 0)
            return false;

        UNICODE_STRING ukey;
        RtlInitUnicodeString(&ukey, key_path);

#if defined(_NTDDK_) || defined(_WDMDDK_)
        OBJECT_ATTRIBUTES oa;
        InitializeObjectAttributes(&oa, &ukey,
            OBJ_CASE_INSENSITIVE | OBJ_KERNEL_HANDLE, nullptr, nullptr);
#else
        kc_object_attributes oa = {};
        oa.Length = sizeof(kc_object_attributes);
        oa.ObjectName = &ukey;
        oa.Attributes = OBJ_CASE_INSENSITIVE | OBJ_KERNEL_HANDLE;
#endif

        void* key_handle = nullptr;
        long status = ZwOpenKey(&key_handle, KEY_READ, &oa);
        if (status != STATUS_SUCCESS || !key_handle)
            return false;

        UNICODE_STRING uval;
        RtlInitUnicodeString(&uval, value_name);

        uint8_t info_buf[512] = {};
        unsigned long result_len = 0;
        status = ZwQueryValueKey(key_handle, &uval, KeyValuePartialInformation,
                                 info_buf, sizeof(info_buf), &result_len);
        ZwClose(key_handle);

        if (status != STATUS_SUCCESS)
            return false;

        auto* info = reinterpret_cast<key_value_partial_info*>(info_buf);
        uint32_t copy_bytes = info->DataLength;
        if (copy_bytes > (buffer_chars - 1) * sizeof(wchar_t))
            copy_bytes = (buffer_chars - 1) * sizeof(wchar_t);

        copy_bytes &= ~static_cast<uint32_t>(sizeof(wchar_t) - 1);
        core::kc_memcpy(buffer, info->Data, copy_bytes);
        buffer[copy_bytes / sizeof(wchar_t)] = L'\0';
        return true;
    } __except (1) {
        return false;
    }
}

// IRQL: PASSIVE_LEVEL
KC_NOINLINE inline bool check_registry_artifacts() {
    __try {
        if (registry_key_exists(L"\\Registry\\Machine\\SYSTEM\\CurrentControlSet\\Services\\vmtools"))
            return true;
        if (registry_key_exists(L"\\Registry\\Machine\\SYSTEM\\CurrentControlSet\\Services\\VBoxService"))
            return true;
        if (registry_key_exists(L"\\Registry\\Machine\\SYSTEM\\CurrentControlSet\\Services\\vmci"))
            return true;
        if (registry_key_exists(L"\\Registry\\Machine\\SOFTWARE\\VMware, Inc.\\VMware Tools"))
            return true;
        return false;
    } __except (1) {
        return false;
    }
}

// IRQL: PASSIVE_LEVEL
KC_NOINLINE inline bool check_smbios_manufacturer() {
    __try {
        wchar_t manufacturer[128] = {};
        if (!registry_read_string(
                L"\\Registry\\Machine\\HARDWARE\\DESCRIPTION\\System\\BIOS",
                L"SystemManufacturer", manufacturer, 128))
            return false;

        uint64_t h = crypto::detail::fnv1a_64_rt_wide_ci(manufacturer);

        if (h == KC_HASH_WIDE_CI(L"VMware, Inc."))              return true;
        if (h == KC_HASH_WIDE_CI(L"innotek GmbH"))              return true;
        if (h == KC_HASH_WIDE_CI(L"Oracle Corporation"))         return true;
        if (h == KC_HASH_WIDE_CI(L"Microsoft Corporation"))      return true;
        if (h == KC_HASH_WIDE_CI(L"Xen"))                        return true;
        if (h == KC_HASH_WIDE_CI(L"QEMU"))                       return true;
        if (h == KC_HASH_WIDE_CI(L"Parallels Software International Inc.")) return true;

        return false;
    } __except (1) {
        return false;
    }
}

KC_NOINLINE inline bool check_vm() {
    if (check_hypervisor_bit())
        return true;

    // catches hyper-v even if CPUID hypervisor bit is masked
    if (check_hyperv_msr())
        return true;

    // registry and smbios checks require PASSIVE_LEVEL
    if (KeGetCurrentIrql() == PASSIVE_LEVEL) {
        if (check_registry_artifacts())
            return true;
        if (check_smbios_manufacturer())
            return true;
    }
    return false;
}

KC_NOINLINE inline void take_vm_response() {
#if KC_ANTI_VM_RESPONSE == 1
    KeBugCheck(0x000000E2);
#elif KC_ANTI_VM_RESPONSE == 2
    volatile uint8_t* sp = reinterpret_cast<volatile uint8_t*>(&sp);
    for (int i = 0; i < 4096; ++i)
        sp[i] = 0;
#else
    // response disabled
#endif
}

} // namespace detail

} // namespace security
} // namespace kernelcloak

#define KC_ANTI_VM() \
    do { \
        if (::kernelcloak::security::detail::check_vm()) { \
            ::kernelcloak::security::detail::take_vm_response(); \
        } \
    } while (0)

#define KC_CHECK_VM() \
    (::kernelcloak::security::detail::check_vm())

#define KC_DETECT_HYPERVISOR() \
    (::kernelcloak::security::detail::check_hypervisor_bit())

#define KC_DETECT_VM_VENDOR() \
    (::kernelcloak::security::detail::get_hypervisor_vendor())

#else // KC_ENABLE_ANTI_VM disabled

#define KC_ANTI_VM()           do {} while (0)
#define KC_CHECK_VM()          (false)
#define KC_DETECT_HYPERVISOR() (false)
#define KC_DETECT_VM_VENDOR()  (static_cast<::kernelcloak::uint64_t>(0))

#endif // KC_ENABLE_ANTI_VM


```

`security/import_hiding.h`:

```h
#pragma once
#include "../config.h"
#include "../core/types.h"
#include "../core/memory.h"
#include "../crypto/hash.h"

#if KC_ENABLE_IMPORT_HIDING

#ifndef PASSIVE_LEVEL
#define PASSIVE_LEVEL 0
#endif

struct _ERESOURCE;

#if !defined(_NTDDK_) && !defined(_WDMDDK_)
extern "C" {
#ifndef _UNICODE_STRING_DEFINED
#define _UNICODE_STRING_DEFINED
    typedef struct _UNICODE_STRING {
        unsigned short Length;
        unsigned short MaximumLength;
        wchar_t* Buffer;
    } UNICODE_STRING, *PUNICODE_STRING;
#endif

    void* __stdcall MmGetSystemRoutineAddress(UNICODE_STRING* SystemRoutineName);
    unsigned char __stdcall MmIsAddressValid(void* VirtualAddress);
#if KC_IMPORT_HIDING_LOCK_MODULE_LIST
    unsigned char __stdcall KeGetCurrentIrql();

    struct _ERESOURCE;
    unsigned char __stdcall ExAcquireResourceSharedLite(struct _ERESOURCE* Resource, unsigned char Wait);
    void __stdcall ExReleaseResourceLite(struct _ERESOURCE* Resource);
    void __stdcall KeEnterCriticalRegion();
    void __stdcall KeLeaveCriticalRegion();
#endif

    // PsLoadedModuleList - undocumented export from ntoskrnl.exe
    // head of doubly-linked list of KLDR_DATA_TABLE_ENTRY structures
    // describing all loaded kernel modules
#ifndef _LIST_ENTRY_DEFINED
#define _LIST_ENTRY_DEFINED
    typedef struct _LIST_ENTRY {
        _LIST_ENTRY* Flink;
        _LIST_ENTRY* Blink;
    } LIST_ENTRY, *PLIST_ENTRY;
#endif

    extern LIST_ENTRY PsLoadedModuleList;
#if KC_IMPORT_HIDING_LOCK_MODULE_LIST
    extern struct _ERESOURCE PsLoadedModuleResource;
#endif
}
#else
// when ntddk/wdm is included, PsLoadedModuleList still needs declaration
// as it's not in the standard WDK headers
extern "C" {
    extern LIST_ENTRY PsLoadedModuleList;
#if KC_IMPORT_HIDING_LOCK_MODULE_LIST
    extern struct _ERESOURCE PsLoadedModuleResource;
#endif
}
#endif

namespace kernelcloak {
namespace security {

namespace detail {

// undocumented KLDR_DATA_TABLE_ENTRY - kernel loader data structure
#pragma pack(push, 8)
typedef struct _KLDR_DATA_TABLE_ENTRY {
    LIST_ENTRY InLoadOrderLinks;
    void* ExceptionTable;
    uint32_t ExceptionTableSize;
    void* GpValue;
    void* NonPagedDebugInfo;
    void* DllBase;
    void* EntryPoint;
    uint32_t SizeOfImage;
    UNICODE_STRING FullDllName;
    UNICODE_STRING BaseDllName;
} KLDR_DATA_TABLE_ENTRY, *PKLDR_DATA_TABLE_ENTRY;
#pragma pack(pop)

// pe structures for export directory parsing
struct imp_dos_header {
    uint16_t e_magic;
    uint16_t e_cblp;
    uint16_t e_cp;
    uint16_t e_crlc;
    uint16_t e_cparhdr;
    uint16_t e_minalloc;
    uint16_t e_maxalloc;
    uint16_t e_ss;
    uint16_t e_sp;
    uint16_t e_csum;
    uint16_t e_ip;
    uint16_t e_cs;
    uint16_t e_lfarlc;
    uint16_t e_ovno;
    uint16_t e_res[4];
    uint16_t e_oemid;
    uint16_t e_oeminfo;
    uint16_t e_res2[10];
    int32_t  e_lfanew;
};

struct imp_file_header {
    uint16_t Machine;
    uint16_t NumberOfSections;
    uint32_t TimeDateStamp;
    uint32_t PointerToSymbolTable;
    uint32_t NumberOfSymbols;
    uint16_t SizeOfOptionalHeader;
    uint16_t Characteristics;
};

struct imp_data_directory {
    uint32_t VirtualAddress;
    uint32_t Size;
};

struct imp_optional_header64 {
    uint16_t Magic;
    uint8_t  MajorLinkerVersion;
    uint8_t  MinorLinkerVersion;
    uint32_t SizeOfCode;
    uint32_t SizeOfInitializedData;
    uint32_t SizeOfUninitializedData;
    uint32_t AddressOfEntryPoint;
    uint32_t BaseOfCode;
    uint64_t ImageBase;
    uint32_t SectionAlignment;
    uint32_t FileAlignment;
    uint16_t MajorOperatingSystemVersion;
    uint16_t MinorOperatingSystemVersion;
    uint16_t MajorImageVersion;
    uint16_t MinorImageVersion;
    uint16_t MajorSubsystemVersion;
    uint16_t MinorSubsystemVersion;
    uint32_t Win32VersionValue;
    uint32_t SizeOfImage;
    uint32_t SizeOfHeaders;
    uint32_t CheckSum;
    uint16_t Subsystem;
    uint16_t DllCharacteristics;
    uint64_t SizeOfStackReserve;
    uint64_t SizeOfStackCommit;
    uint64_t SizeOfHeapReserve;
    uint64_t SizeOfHeapCommit;
    uint32_t LoaderFlags;
    uint32_t NumberOfRvaAndSizes;
    imp_data_directory DataDirectory[16];
};

struct imp_nt_headers64 {
    uint32_t Signature;
    imp_file_header FileHeader;
    imp_optional_header64 OptionalHeader;
};

struct imp_export_directory {
    uint32_t Characteristics;
    uint32_t TimeDateStamp;
    uint16_t MajorVersion;
    uint16_t MinorVersion;
    uint32_t Name;
    uint32_t Base;
    uint32_t NumberOfFunctions;
    uint32_t NumberOfNames;
    uint32_t AddressOfFunctions;
    uint32_t AddressOfNames;
    uint32_t AddressOfNameOrdinals;
};

// UNICODE_STRING buffers are not guaranteed to be null-terminated. for module names we
// treat them as ASCII (low byte) and hash with the same algo as KC_HASH_CI("ntoskrnl.exe").
KC_FORCEINLINE uint64_t fnv1a_64_rt_unicode_ci_to_ascii(const wchar_t* str, size_t len_chars) {
    uint64_t hash = crypto::detail::fnv64_offset_basis;
    for (size_t i = 0; i < len_chars; ++i) {
        char c = static_cast<char>(static_cast<uint16_t>(str[i]) & 0xFFu);
        if (c >= 'A' && c <= 'Z')
            c = static_cast<char>(c + ('a' - 'A'));
        hash ^= static_cast<uint64_t>(static_cast<uint8_t>(c));
        hash *= crypto::detail::fnv64_prime;
    }
    return hash;
}

KC_FORCEINLINE bool is_valid_pe(void* base) {
    if (!base || !MmIsAddressValid(base))
        return false;

    auto* dos = static_cast<imp_dos_header*>(base);
    if (dos->e_magic != 0x5A4D)
        return false;

    int32_t lfanew = dos->e_lfanew;
    if (lfanew <= 0 || lfanew >= 0x1000)
        return false;

    auto* nt = reinterpret_cast<imp_nt_headers64*>(
        reinterpret_cast<uint8_t*>(base) + lfanew);
    if (!MmIsAddressValid(nt))
        return false;

    return nt->Signature == 0x00004550;
}

// walk PsLoadedModuleList for module base by case-insensitive wide-string hash
KC_NOINLINE inline void* find_module_by_hash(uint64_t name_hash) {
#if KC_IMPORT_HIDING_LOCK_MODULE_LIST
    bool in_critical = false;
    bool resource_acquired = false;
#endif
    void* found = nullptr;

    __try {
#if KC_IMPORT_HIDING_LOCK_MODULE_LIST
        if (KeGetCurrentIrql() == PASSIVE_LEVEL) {
            __try {
                KeEnterCriticalRegion();
                in_critical = true;

                ExAcquireResourceSharedLite(&PsLoadedModuleResource, 1);
                resource_acquired = true;
            } __except (1) {
                resource_acquired = false;
                if (in_critical) {
                    KeLeaveCriticalRegion();
                    in_critical = false;
                }
            }
        }
#endif

        auto* head = &PsLoadedModuleList;
        if (!MmIsAddressValid(head) || !MmIsAddressValid(head->Flink))
            goto out;

        for (auto* entry = head->Flink; entry != head; entry = entry->Flink) {
            if (!MmIsAddressValid(entry))
                break;

            auto* mod = reinterpret_cast<PKLDR_DATA_TABLE_ENTRY>(entry);
            if (!mod->BaseDllName.Buffer || !mod->BaseDllName.Length)
                continue;
            if (!MmIsAddressValid(mod->BaseDllName.Buffer))
                continue;

            size_t wchar_len = static_cast<size_t>(mod->BaseDllName.Length) / sizeof(wchar_t);
            if (!wchar_len)
                continue;
            if (wchar_len > 260)
                continue;

            uint64_t h = fnv1a_64_rt_unicode_ci_to_ascii(mod->BaseDllName.Buffer, wchar_len);
            if (h == name_hash) {
                found = mod->DllBase;
                break;
            }
        }

    out:
#if KC_IMPORT_HIDING_LOCK_MODULE_LIST
        if (resource_acquired) {
            ExReleaseResourceLite(&PsLoadedModuleResource);
            resource_acquired = false;
        }
        if (in_critical) {
            KeLeaveCriticalRegion();
            in_critical = false;
        }
#endif

        return found;
    } __except (1) {
#if KC_IMPORT_HIDING_LOCK_MODULE_LIST
        if (resource_acquired) {
            ExReleaseResourceLite(&PsLoadedModuleResource);
        }
        if (in_critical) {
            KeLeaveCriticalRegion();
        }
#endif
        return nullptr;
    }
}

// resolve export from module base by case-insensitive function name hash
// handles forwarded exports by recursing into the target module
KC_NOINLINE inline void* find_export_by_hash(void* module_base, uint64_t func_hash, uint32_t depth = 0) {
    __try {
        if (depth > 8)
            return nullptr;

        if (!is_valid_pe(module_base))
            return nullptr;

        auto base = reinterpret_cast<uint8_t*>(module_base);
        auto* dos = reinterpret_cast<imp_dos_header*>(base);
        auto* nt = reinterpret_cast<imp_nt_headers64*>(base + dos->e_lfanew);

        auto& export_entry = nt->OptionalHeader.DataDirectory[0];
        if (!export_entry.VirtualAddress || !export_entry.Size)
            return nullptr;

        auto* exports = reinterpret_cast<imp_export_directory*>(
            base + export_entry.VirtualAddress);
        if (!MmIsAddressValid(exports))
            return nullptr;

        auto* names = reinterpret_cast<uint32_t*>(base + exports->AddressOfNames);
        auto* ordinals = reinterpret_cast<uint16_t*>(base + exports->AddressOfNameOrdinals);
        auto* functions = reinterpret_cast<uint32_t*>(base + exports->AddressOfFunctions);

        if (!MmIsAddressValid(names) || !MmIsAddressValid(ordinals) || !MmIsAddressValid(functions))
            return nullptr;

        uint32_t dir_start = export_entry.VirtualAddress;
        uint32_t dir_end = dir_start + export_entry.Size;

        for (uint32_t i = 0; i < exports->NumberOfNames; ++i) {
            auto* func_name = reinterpret_cast<const char*>(base + names[i]);
            if (!MmIsAddressValid(const_cast<char*>(func_name)))
                continue;

            uint64_t h = crypto::detail::fnv1a_64_rt_ci(func_name);
            if (h != func_hash)
                continue;

            uint16_t ordinal = ordinals[i];
            if (ordinal >= exports->NumberOfFunctions)
                continue;

            uint32_t func_rva = functions[ordinal];

            // forwarded export check
            if (func_rva >= dir_start && func_rva < dir_end) {
                auto* fwd_str = reinterpret_cast<const char*>(base + func_rva);
                if (!MmIsAddressValid(const_cast<char*>(fwd_str)))
                    return nullptr;

                const char* dot = fwd_str;
                while (*dot && *dot != '.') ++dot;
                if (!*dot)
                    return nullptr;

                char mod_name[128] = {};
                size_t mod_len = static_cast<size_t>(dot - fwd_str);
                if (mod_len >= sizeof(mod_name) - 5)
                    return nullptr;

                for (size_t j = 0; j < mod_len; ++j)
                    mod_name[j] = fwd_str[j];
                mod_name[mod_len]     = '.';
                mod_name[mod_len + 1] = 'd';
                mod_name[mod_len + 2] = 'l';
                mod_name[mod_len + 3] = 'l';
                mod_name[mod_len + 4] = '\0';

                uint64_t fwd_mod_hash = crypto::detail::fnv1a_64_rt_ci(mod_name);
                uint64_t fwd_func_hash = crypto::detail::fnv1a_64_rt_ci(dot + 1);

                void* fwd_base = find_module_by_hash(fwd_mod_hash);
                if (!fwd_base)
                    return nullptr;

                return find_export_by_hash(fwd_base, fwd_func_hash, depth + 1);
            }

            return base + func_rva;
        }
    } __except (1) {
        return nullptr;
    }

    return nullptr;
}

// fallback: MmGetSystemRoutineAddress for documented APIs
// IRQL: PASSIVE_LEVEL
KC_NOINLINE inline void* resolve_via_mm(const wchar_t* func_name) {
    __try {
        UNICODE_STRING name;
        name.Buffer = const_cast<wchar_t*>(func_name);
        name.Length = 0;
        name.MaximumLength = 0;

        const wchar_t* p = func_name;
        while (*p) ++p;
        auto len = static_cast<unsigned short>((p - func_name) * sizeof(wchar_t));
        name.Length = len;
        name.MaximumLength = len + sizeof(wchar_t);

        return MmGetSystemRoutineAddress(&name);
    } __except (1) {
        return nullptr;
    }
}

} // namespace detail

// public API
KC_FORCEINLINE void* get_module(uint64_t name_hash) {
    return detail::find_module_by_hash(name_hash);
}

KC_FORCEINLINE void* get_export(void* module_base, uint64_t func_hash) {
    return detail::find_export_by_hash(module_base, func_hash);
}

KC_FORCEINLINE void* resolve_import(uint64_t mod_hash, uint64_t func_hash) {
    void* mod = detail::find_module_by_hash(mod_hash);
    if (!mod)
        return nullptr;
    return detail::find_export_by_hash(mod, func_hash);
}

} // namespace security
} // namespace kernelcloak

// resolve import by compile-time hashed module + function names
// usage: auto fn = reinterpret_cast<fn_type>(KC_IMPORT("ntoskrnl.exe", "MmGetSystemRoutineAddress"));
#define KC_IMPORT(mod, func) \
    (::kernelcloak::security::resolve_import( \
        KC_HASH_CI(mod), KC_HASH_CI(func)))

#define KC_GET_MODULE(name) \
    (::kernelcloak::security::get_module(KC_HASH_CI(name)))

#define KC_GET_PROC(mod_base, func) \
    (::kernelcloak::security::get_export( \
        static_cast<void*>(mod_base), KC_HASH_CI(func)))

#else // KC_ENABLE_IMPORT_HIDING disabled

#define KC_IMPORT(mod, func)        (nullptr)
#define KC_GET_MODULE(name)         (nullptr)
#define KC_GET_PROC(mod_base, func) (nullptr)

#endif // KC_ENABLE_IMPORT_HIDING


```

`security/integrity.h`:

```h
#pragma once
#include "../config.h"
#include "../core/types.h"
#include "../core/memory.h"
#include "../crypto/hash.h"

#if KC_ENABLE_INTEGRITY

#if !defined(_NTDDK_) && !defined(_WDMDDK_)
extern "C" {
    unsigned char __stdcall MmIsAddressValid(void* VirtualAddress);
}
#endif

namespace kernelcloak {
namespace security {

namespace detail {

// pe structures for section parsing
struct integrity_dos_header {
    uint16_t e_magic;
    uint8_t  _pad[58];
    int32_t  e_lfanew;
};

struct integrity_file_header {
    uint16_t Machine;
    uint16_t NumberOfSections;
    uint32_t TimeDateStamp;
    uint32_t PointerToSymbolTable;
    uint32_t NumberOfSymbols;
    uint16_t SizeOfOptionalHeader;
    uint16_t Characteristics;
};

struct integrity_data_directory {
    uint32_t VirtualAddress;
    uint32_t Size;
};

struct integrity_optional_header64 {
    uint16_t Magic;
    uint8_t  _pad1[14];
    uint32_t AddressOfEntryPoint;
    uint32_t BaseOfCode;
    uint64_t ImageBase;
    uint32_t SectionAlignment;
    uint32_t FileAlignment;
    uint8_t  _pad2[16];
    uint32_t SizeOfImage;
    uint32_t SizeOfHeaders;
    uint32_t CheckSum;
    uint16_t Subsystem;
    uint16_t DllCharacteristics;
    uint64_t SizeOfStackReserve;
    uint64_t SizeOfStackCommit;
    uint64_t SizeOfHeapReserve;
    uint64_t SizeOfHeapCommit;
    uint32_t LoaderFlags;
    uint32_t NumberOfRvaAndSizes;
    integrity_data_directory DataDirectory[16];
};

struct integrity_nt_headers64 {
    uint32_t Signature;
    integrity_file_header FileHeader;
    integrity_optional_header64 OptionalHeader;
};

#pragma pack(push, 1)
struct integrity_section_header {
    char     Name[8];
    uint32_t VirtualSize;
    uint32_t VirtualAddress;
    uint32_t SizeOfRawData;
    uint32_t PointerToRawData;
    uint32_t PointerToRelocations;
    uint32_t PointerToLinenumbers;
    uint16_t NumberOfRelocations;
    uint16_t NumberOfLinenumbers;
    uint32_t Characteristics;
};
#pragma pack(pop)

// find driver base by scanning backwards from a function address to MZ
KC_NOINLINE inline void* find_own_base_from_address(void* addr) {
    __try {
        auto ptr = reinterpret_cast<uintptr_t>(addr);
        ptr &= ~static_cast<uintptr_t>(0xFFF);

        for (int i = 0; i < 0x4000; ++i) {
            if (!MmIsAddressValid(reinterpret_cast<void*>(ptr)))
                goto skip;

            {
                auto* dos = reinterpret_cast<integrity_dos_header*>(ptr);
                if (dos->e_magic == 0x5A4D && dos->e_lfanew > 0 && dos->e_lfanew < 0x1000) {
                    auto* nt = reinterpret_cast<integrity_nt_headers64*>(ptr + dos->e_lfanew);
                    if (MmIsAddressValid(nt) && nt->Signature == 0x00004550)
                        return reinterpret_cast<void*>(ptr);
                }
            }

        skip:
            ptr -= 0x1000;
        }
    } __except (1) {}

    return nullptr;
}

// find .text section (or first executable section) in PE image
KC_NOINLINE inline bool find_text_section(void* base, uintptr_t& text_va, uint32_t& text_size) {
    __try {
        auto* dos = static_cast<integrity_dos_header*>(base);
        if (dos->e_magic != 0x5A4D)
            return false;

        auto* nt = reinterpret_cast<integrity_nt_headers64*>(
            reinterpret_cast<uint8_t*>(base) + dos->e_lfanew);
        if (!MmIsAddressValid(nt) || nt->Signature != 0x00004550)
            return false;

        auto* section = reinterpret_cast<integrity_section_header*>(
            reinterpret_cast<uint8_t*>(&nt->OptionalHeader) +
            nt->FileHeader.SizeOfOptionalHeader);

        for (uint16_t i = 0; i < nt->FileHeader.NumberOfSections; ++i) {
            if (!MmIsAddressValid(&section[i]))
                return false;

            if (section[i].Name[0] == '.' && section[i].Name[1] == 't' &&
                section[i].Name[2] == 'e' && section[i].Name[3] == 'x' &&
                section[i].Name[4] == 't') {
                text_va = reinterpret_cast<uintptr_t>(base) + section[i].VirtualAddress;
                text_size = section[i].VirtualSize;
                return true;
            }
        }

        // fallback: first section with IMAGE_SCN_MEM_EXECUTE
        constexpr uint32_t IMAGE_SCN_MEM_EXECUTE = 0x20000000;
        for (uint16_t i = 0; i < nt->FileHeader.NumberOfSections; ++i) {
            if (section[i].Characteristics & IMAGE_SCN_MEM_EXECUTE) {
                text_va = reinterpret_cast<uintptr_t>(base) + section[i].VirtualAddress;
                text_size = section[i].VirtualSize;
                return true;
            }
        }
    } __except (1) {}

    return false;
}

// FNV-1a hash over arbitrary memory region
KC_NOINLINE inline uint64_t compute_region_hash(const void* ptr, uint32_t size) {
    __try {
        if (!ptr || size == 0)
            return 0;

        auto* data = static_cast<const uint8_t*>(ptr);

        // validate per-page rather than per-byte. this avoids high overhead on larger
        // regions while still keeping us from faulting at elevated irql.
        uintptr_t start = reinterpret_cast<uintptr_t>(data);
        uintptr_t end = start + static_cast<uintptr_t>(size - 1);
        uintptr_t page = start & ~static_cast<uintptr_t>(0xFFFull);
        for (; page <= end; page += 0x1000) {
            if (!MmIsAddressValid(reinterpret_cast<void*>(page)))
                return 0;
        }

        uint64_t hash = crypto::detail::fnv64_offset_basis;
        for (uint32_t i = 0; i < size; ++i) {
            hash ^= static_cast<uint64_t>(data[i]);
            hash *= crypto::detail::fnv64_prime;
        }

        return hash;
    } __except (1) {
        return 0;
    }
}

inline uint64_t& stored_text_hash() {
    static uint64_t hash = 0;
    return hash;
}

inline void*& stored_driver_base() {
    static void* base = nullptr;
    return base;
}

// detect common inline hook patterns on a function
KC_NOINLINE inline bool detect_hook(void* func_addr) {
    __try {
        if (!func_addr || !MmIsAddressValid(func_addr))
            return false;

        auto* b = static_cast<uint8_t*>(func_addr);

        // validate readability of first 16 bytes
        for (int i = 0; i < 16; ++i) {
            if (!MmIsAddressValid(&b[i]))
                return false;
        }

        if (b[0] == 0xE9)                                  return true; // jmp rel32
        if (b[0] == 0xFF && b[1] == 0x25)                  return true; // jmp [rip+disp32]
        if (b[0] == 0x48 && b[1] == 0xB8 &&
            b[10] == 0xFF && b[11] == 0xE0)                 return true; // mov rax, imm64; jmp rax
        if (b[0] == 0xCC)                                   return true; // int 3
        if (b[0] == 0x68 && b[5] == 0xC3)                   return true; // push imm32; ret

        return false;
    } __except (1) {
        return false;
    }
}

// .text section self-checksum verification
// first call stores baseline hash, subsequent calls compare against it
KC_NOINLINE inline bool verify_integrity() {
    __try {
        if (!stored_driver_base()) {
            stored_driver_base() = find_own_base_from_address(
                reinterpret_cast<void*>(&verify_integrity));
        }

        void* base = stored_driver_base();
        if (!base)
            return true; // can't locate base, degrade gracefully

        uintptr_t text_va = 0;
        uint32_t text_size = 0;
        if (!find_text_section(base, text_va, text_size))
            return true;

        uint64_t current = compute_region_hash(
            reinterpret_cast<void*>(text_va), text_size);
        if (current == 0)
            return true;

        if (stored_text_hash() == 0) {
            stored_text_hash() = current;
            return true;
        }

        return stored_text_hash() == current;
    } __except (1) {
        return true;
    }
}

} // namespace detail

} // namespace security
} // namespace kernelcloak

#define KC_DETECT_HOOK(func) \
    (::kernelcloak::security::detail::detect_hook(reinterpret_cast<void*>(func)))

#define KC_COMPUTE_HASH(ptr, size) \
    (::kernelcloak::security::detail::compute_region_hash( \
        static_cast<const void*>(ptr), static_cast<::kernelcloak::uint32_t>(size)))

#define KC_VERIFY_INTEGRITY() \
    (::kernelcloak::security::detail::verify_integrity())

#else // KC_ENABLE_INTEGRITY disabled

#define KC_DETECT_HOOK(func)        (false)
#define KC_COMPUTE_HASH(ptr, size)  (static_cast<::kernelcloak::uint64_t>(0))
#define KC_VERIFY_INTEGRITY()       (true)

#endif // KC_ENABLE_INTEGRITY


```

`security/pe_erase.h`:

```h
#pragma once
#include "../config.h"
#include "../core/types.h"
#include "../core/memory.h"

#if KC_ENABLE_PE_ERASE

#if !defined(_NTDDK_) && !defined(_WDMDDK_)
extern "C" {
    unsigned char __stdcall MmIsAddressValid(void* VirtualAddress);
}
#endif

// RtlSecureZeroMemory is a forceinline in WDK, not an export.
// we implement our own volatile version to ensure the compiler doesn't optimize it away.
namespace kernelcloak {
namespace security {
namespace detail {

KC_FORCEINLINE void secure_zero(void* dst, size_t len) {
    volatile unsigned char* p = static_cast<volatile unsigned char*>(dst);
    while (len--) *p++ = 0;
}

} // namespace detail
} // namespace security
} // namespace kernelcloak

namespace kernelcloak {
namespace security {

namespace detail {

struct pe_dos_header {
    uint16_t e_magic;
    uint8_t  _pad[58];
    int32_t  e_lfanew;
};

struct pe_file_header {
    uint16_t Machine;
    uint16_t NumberOfSections;
    uint32_t TimeDateStamp;
    uint32_t PointerToSymbolTable;
    uint32_t NumberOfSymbols;
    uint16_t SizeOfOptionalHeader;
    uint16_t Characteristics;
};

struct pe_data_directory {
    uint32_t VirtualAddress;
    uint32_t Size;
};

struct pe_optional_header64 {
    uint16_t Magic;
    uint8_t  _pad1[14];
    uint32_t AddressOfEntryPoint;
    uint32_t BaseOfCode;
    uint64_t ImageBase;
    uint32_t SectionAlignment;
    uint32_t FileAlignment;
    uint8_t  _pad2[16];
    uint32_t SizeOfImage;
    uint32_t SizeOfHeaders;
    uint32_t CheckSum;
    uint16_t Subsystem;
    uint16_t DllCharacteristics;
    uint64_t SizeOfStackReserve;
    uint64_t SizeOfStackCommit;
    uint64_t SizeOfHeapReserve;
    uint64_t SizeOfHeapCommit;
    uint32_t LoaderFlags;
    uint32_t NumberOfRvaAndSizes;
    pe_data_directory DataDirectory[16];
};

struct pe_nt_headers64 {
    uint32_t Signature;
    pe_file_header FileHeader;
    pe_optional_header64 OptionalHeader;
};

#pragma pack(push, 1)
struct pe_section_header {
    char     Name[8];
    uint32_t VirtualSize;
    uint32_t VirtualAddress;
    uint32_t SizeOfRawData;
    uint32_t PointerToRawData;
    uint32_t PointerToRelocations;
    uint32_t PointerToLinenumbers;
    uint16_t NumberOfRelocations;
    uint16_t NumberOfLinenumbers;
    uint32_t Characteristics;
};
#pragma pack(pop)

// find own driver base by scanning backward from known function address
KC_NOINLINE inline void* find_driver_base() {
    __try {
        auto ptr = reinterpret_cast<uintptr_t>(&find_driver_base);
        ptr &= ~static_cast<uintptr_t>(0xFFF);

        for (int i = 0; i < 0x4000; ++i) {
            if (!MmIsAddressValid(reinterpret_cast<void*>(ptr)))
                goto skip;

            {
                auto* dos = reinterpret_cast<pe_dos_header*>(ptr);
                if (dos->e_magic == 0x5A4D && dos->e_lfanew > 0 && dos->e_lfanew < 0x1000) {
                    auto* nt = reinterpret_cast<pe_nt_headers64*>(ptr + dos->e_lfanew);
                    if (MmIsAddressValid(nt) && nt->Signature == 0x00004550)
                        return reinterpret_cast<void*>(ptr);
                }
            }

        skip:
            ptr -= 0x1000;
        }
    } __except (1) {}

    return nullptr;
}

// erase PE headers from driver image
// IRQL: PASSIVE_LEVEL - modifying own image pages
// after this call, WinDbg !dh and similar tools will fail to parse the driver
KC_NOINLINE inline bool erase_pe_headers() {
    __try {
        void* base = find_driver_base();
        if (!base)
            return false;

        auto* raw = reinterpret_cast<uint8_t*>(base);
        auto* dos = reinterpret_cast<pe_dos_header*>(base);

        if (dos->e_magic != 0x5A4D)
            return false;

        int32_t lfanew = dos->e_lfanew;
        if (lfanew <= 0 || lfanew >= 0x1000)
            return false;

        auto* nt = reinterpret_cast<pe_nt_headers64*>(raw + lfanew);
        if (!MmIsAddressValid(nt) || nt->Signature != 0x00004550)
            return false;

        uint16_t num_sections = nt->FileHeader.NumberOfSections;
        uint16_t opt_hdr_size = nt->FileHeader.SizeOfOptionalHeader;

        // sanity checks - avoid bad section math if headers are corrupted
        if (num_sections == 0 || num_sections > 96)
            return false;
        if (opt_hdr_size < sizeof(pe_optional_header64) || opt_hdr_size > 0x1000)
            return false;

        auto* first_section = reinterpret_cast<pe_section_header*>(
            reinterpret_cast<uint8_t*>(&nt->OptionalHeader) + opt_hdr_size);
        auto* last_section = &first_section[num_sections];

        uintptr_t erase_end = reinterpret_cast<uintptr_t>(last_section);
        uintptr_t erase_start = reinterpret_cast<uintptr_t>(base);
        size_t erase_size = erase_end - erase_start;

        // clamp to SizeOfHeaders or 2 pages
        uint32_t size_of_headers = nt->OptionalHeader.SizeOfHeaders;
        if (erase_size > size_of_headers)
            erase_size = size_of_headers;
        if (erase_size > 0x2000)
            erase_size = 0x2000;

        // validate all pages in range
        for (size_t offset = 0; offset < erase_size; offset += 0x1000) {
            if (!MmIsAddressValid(raw + offset))
                return false;
        }
        if (erase_size > 0 && !MmIsAddressValid(raw + erase_size - 1))
            return false;

        secure_zero(raw, erase_size);
        return true;
    } __except (1) {
        return false;
    }
}

} // namespace detail

} // namespace security
} // namespace kernelcloak

// erase PE headers from own driver image. call once at PASSIVE_LEVEL
// after driver initialization is complete
#define KC_ERASE_PE_HEADER() \
    (::kernelcloak::security::detail::erase_pe_headers())

#else // KC_ENABLE_PE_ERASE disabled

#define KC_ERASE_PE_HEADER() (false)

#endif // KC_ENABLE_PE_ERASE


```

`strings/encrypted_string.h`:

```h
#pragma once
#include "../config.h"
#include "../core/types.h"
#include "../core/random.h"
#include "../crypto/xor_cipher.h"

#if KC_ENABLE_STRING_ENCRYPTION

namespace kernelcloak {
namespace strings {
namespace detail {

// compile-time string encryption with stack-local decryption
// no static destructors, no atexit, no CRT - safe at any IRQL
template<size_t N, uint32_t Key>
class encrypted_string {
    uint8_t m_encrypted[N];

    static constexpr uint8_t encrypt_byte(char c, size_t idx) {
        uint8_t k = static_cast<uint8_t>((Key >> ((idx % 4) * 8)) ^ (idx * 0x9E3779B9u));
        return static_cast<uint8_t>(c) ^ k;
    }

public:
    template<size_t... Is>
    constexpr encrypted_string(const char(&str)[N], kernelcloak::detail::index_sequence<Is...>)
        : m_encrypted{ encrypt_byte(str[Is], Is)... }
    {}

    KC_FORCEINLINE void decrypt(char* out) const {
        for (size_t i = 0; i < N; ++i) {
            uint8_t k = static_cast<uint8_t>((Key >> ((i % 4) * 8)) ^ (i * 0x9E3779B9u));
            out[i] = static_cast<char>(m_encrypted[i] ^ k);
        }
    }

    static constexpr size_t length() { return N - 1; }
    static constexpr size_t size() { return N; }
};

// stack-local decryption wrapper returned by value from KC_STR
// the temporary lives for the duration of the full-expression it appears in,
// so passing KC_STR("hello") to a function is safe.
// for longer-lived strings, use KC_STR_DECL or `auto s = KC_STR(...)`.
template<size_t N, uint32_t Key>
struct decrypted_string {
    char buf[N];

    KC_FORCEINLINE decrypted_string(const encrypted_string<N, Key>& enc) {
        enc.decrypt(buf);
    }

    KC_FORCEINLINE operator const char*() const { return buf; }
    KC_FORCEINLINE const char* c_str() const { return buf; }
    KC_FORCEINLINE size_t length() const { return N - 1; }
    KC_FORCEINLINE size_t size() const { return N; }
};

// helper: deduces template params from encrypted_string to avoid
// needing to repeat them in the macro expansion
template<size_t N, uint32_t Key>
KC_FORCEINLINE decrypted_string<N, Key> make_decrypted(const encrypted_string<N, Key>& enc) {
    return decrypted_string<N, Key>(enc);
}

} // namespace detail
} // namespace strings
} // namespace kernelcloak

// KC_STR("text") - returns a temporary decrypted_string that implicitly
// converts to const char*. safe to pass directly to functions.
//
//   DbgPrint("%s\n", KC_STR("hello")); // safe
//   auto s = KC_STR("hello");          // safe, s owns the buffer
//   const char* p = KC_STR("hello");   // dangling! use KC_STR_DECL instead
//
// the __COUNTER__ is consumed once inside the lambda, and make_decrypted
// deduces the key from the encrypted_string type, so no key mismatch.
#define KC_STR(s)                                                               \
    ::kernelcloak::strings::detail::make_decrypted(                             \
        []() -> const auto& {                                                   \
            constexpr ::kernelcloak::uint32_t _k =                              \
                static_cast<::kernelcloak::uint32_t>(                           \
                    (__COUNTER__ + 1) * 0x45D9F3Bu ^                            \
                    __LINE__ * 0x1B873593u ^                                    \
                    sizeof(s) * 0xCC9E2D51u                                     \
                );                                                              \
            static constexpr ::kernelcloak::strings::detail::encrypted_string<  \
                sizeof(s), _k> e(                                               \
                s, ::kernelcloak::detail::make_index_sequence<sizeof(s)>{}      \
            );                                                                  \
            return e;                                                           \
        }()                                                                     \
    )

// KC_STR_DECL(name, "text") - declares a named decrypted_string in the
// caller's scope. buffer is valid for the variable's lifetime.
//
//   KC_STR_DECL(greeting, "hello world");
//   use_string(greeting.c_str(), greeting.length());
//
// uses a do-nothing enum trick to capture __COUNTER__ once and reuse it
#define KC_STR_DECL_IMPL_(name, s, key)                                         \
    static constexpr ::kernelcloak::strings::detail::encrypted_string<          \
        sizeof(s), key> _kc_enc_##name(                                         \
        s, ::kernelcloak::detail::make_index_sequence<sizeof(s)>{}              \
    );                                                                          \
    auto name = ::kernelcloak::strings::detail::make_decrypted(_kc_enc_##name)

#define KC_STR_DECL(name, s)                                                    \
    KC_STR_DECL_IMPL_(name, s,                                                  \
        static_cast<::kernelcloak::uint32_t>(                                   \
            (__COUNTER__ + 1) * 0x45D9F3Bu ^                                    \
            __LINE__ * 0x1B873593u ^                                            \
            sizeof(s) * 0xCC9E2D51u                                             \
        )                                                                       \
    )

// KC_STR_N("text") - same as KC_STR, the returned decrypted_string has
// both .c_str() and .length() accessors
#define KC_STR_N(s) KC_STR(s)

#else // KC_ENABLE_STRING_ENCRYPTION disabled

#define KC_STR(s)              (s)
#define KC_STR_DECL(name, s)   const char* name = (s)
#define KC_STR_N(s)            (s)

#endif // KC_ENABLE_STRING_ENCRYPTION

```

`strings/encrypted_wstring.h`:

```h
#pragma once
#include "../config.h"
#include "../core/types.h"
#include "../core/random.h"
#include "../crypto/xor_cipher.h"

#if KC_ENABLE_STRING_ENCRYPTION

namespace kernelcloak {
namespace strings {
namespace detail {

// compile-time wide string encryption with stack-local decryption
// wchar_t is 2 bytes on Windows
template<size_t N, uint32_t Key>
class encrypted_wstring {
    uint16_t m_encrypted[N];

    static constexpr uint16_t encrypt_wchar(wchar_t c, size_t idx) {
        uint32_t derived = Key ^ static_cast<uint32_t>(idx * 0x9E3779B9u);
        uint16_t k = static_cast<uint16_t>(derived ^ (derived >> 16));
        return static_cast<uint16_t>(c) ^ k;
    }

public:
    template<size_t... Is>
    constexpr encrypted_wstring(const wchar_t(&str)[N], kernelcloak::detail::index_sequence<Is...>)
        : m_encrypted{ encrypt_wchar(str[Is], Is)... }
    {}

    KC_FORCEINLINE void decrypt(wchar_t* out) const {
        for (size_t i = 0; i < N; ++i) {
            uint32_t derived = Key ^ static_cast<uint32_t>(i * 0x9E3779B9u);
            uint16_t k = static_cast<uint16_t>(derived ^ (derived >> 16));
            out[i] = static_cast<wchar_t>(m_encrypted[i] ^ k);
        }
    }

    static constexpr size_t length() { return N - 1; }
    static constexpr size_t size() { return N; }
};

template<size_t N, uint32_t Key>
struct decrypted_wstring {
    wchar_t buf[N];

    KC_FORCEINLINE decrypted_wstring(const encrypted_wstring<N, Key>& enc) {
        enc.decrypt(buf);
    }

    KC_FORCEINLINE operator const wchar_t*() const { return buf; }
    KC_FORCEINLINE const wchar_t* c_str() const { return buf; }
    KC_FORCEINLINE size_t length() const { return N - 1; }
    KC_FORCEINLINE size_t size() const { return N; }
};

template<size_t N, uint32_t Key>
KC_FORCEINLINE decrypted_wstring<N, Key> make_decrypted_w(const encrypted_wstring<N, Key>& enc) {
    return decrypted_wstring<N, Key>(enc);
}

} // namespace detail
} // namespace strings
} // namespace kernelcloak

// KC_WSTR(L"text") - wide string encryption, returns temporary with
// implicit conversion to const wchar_t*
#define KC_WSTR(s)                                                              \
    ::kernelcloak::strings::detail::make_decrypted_w(                           \
        []() -> const auto& {                                                   \
            constexpr ::kernelcloak::size_t _wn = sizeof(s) / sizeof(wchar_t);  \
            constexpr ::kernelcloak::uint32_t _k =                              \
                static_cast<::kernelcloak::uint32_t>(                           \
                    (__COUNTER__ + 1) * 0x45D9F3Bu ^                            \
                    __LINE__ * 0x1B873593u ^                                    \
                    _wn * 0xCC9E2D51u                                           \
                );                                                              \
            static constexpr ::kernelcloak::strings::detail::encrypted_wstring< \
                _wn, _k> e(                                                     \
                s, ::kernelcloak::detail::make_index_sequence<_wn>{}            \
            );                                                                  \
            return e;                                                           \
        }()                                                                     \
    )

// KC_WSTR_DECL(name, L"text") - named wide string in caller's scope
#define KC_WSTR_DECL_IMPL_(name, s, wlen, key)                                  \
    static constexpr ::kernelcloak::strings::detail::encrypted_wstring<         \
        wlen, key> _kc_wenc_##name(                                             \
        s, ::kernelcloak::detail::make_index_sequence<wlen>{}                   \
    );                                                                          \
    auto name = ::kernelcloak::strings::detail::make_decrypted_w(               \
        _kc_wenc_##name)

#define KC_WSTR_DECL(name, s)                                                   \
    KC_WSTR_DECL_IMPL_(name, s,                                                 \
        sizeof(s) / sizeof(wchar_t),                                            \
        static_cast<::kernelcloak::uint32_t>(                                   \
            (__COUNTER__ + 1) * 0x45D9F3Bu ^                                    \
            __LINE__ * 0x1B873593u ^                                            \
            (sizeof(s) / sizeof(wchar_t)) * 0xCC9E2D51u                         \
        )                                                                       \
    )

// KC_WSTR_N - same as KC_WSTR with .c_str() and .length()
#define KC_WSTR_N(s) KC_WSTR(s)

#else // KC_ENABLE_STRING_ENCRYPTION disabled

#define KC_WSTR(s)             (s)
#define KC_WSTR_DECL(name, s)  const wchar_t* name = (s)
#define KC_WSTR_N(s)           (s)

#endif // KC_ENABLE_STRING_ENCRYPTION

```

`strings/layered_string.h`:

```h
#pragma once
#include "../config.h"
#include "../core/types.h"
#include "../core/random.h"
#include "../crypto/xor_cipher.h"
#include "../crypto/xtea.h"

#if KC_ENABLE_STRING_ENCRYPTION

// intrinsics - no windows.h needed
extern "C" long __cdecl _InterlockedIncrement(long volatile*);
extern "C" unsigned __int64 __rdtsc();
#pragma intrinsic(_InterlockedIncrement)
#pragma intrinsic(__rdtsc)

namespace kernelcloak {
namespace strings {
namespace detail {

// compile-time byte permutation via fisher-yates
template<size_t N, uint32_t Seed>
struct byte_permutation {
    size_t forward[N];  // forward[i] = where byte i goes
    size_t inverse[N];  // inverse[j] = which byte is at position j

    constexpr byte_permutation() : forward{}, inverse{} {
        for (size_t i = 0; i < N; ++i)
            forward[i] = i;

        uint32_t state = Seed;
        for (size_t i = N - 1; i > 0; --i) {
            state = state * 0x41C64E6Du + 0x3039u;
            size_t j = (state >> 16) % (i + 1);
            size_t tmp = forward[i];
            forward[i] = forward[j];
            forward[j] = tmp;
        }

        for (size_t i = 0; i < N; ++i)
            inverse[forward[i]] = i;
    }
};

// triple-layered string encryption:
//   layer 1: rolling XOR with KeyA
//   layer 2: XTEA on 8-byte blocks with KeyB[4]
//   layer 3: byte shuffle with deterministic permutation
template<size_t N, uint32_t KeyA,
    uint32_t KeyB0, uint32_t KeyB1, uint32_t KeyB2, uint32_t KeyB3,
    uint32_t ShuffleSeed>
class layered_encrypted_string {
    static constexpr size_t padded = ((N + 7) / 8) * 8;

    uint8_t m_data[padded];
    static constexpr byte_permutation<padded, ShuffleSeed> s_perm{};

    static constexpr uint8_t xor_key(size_t idx) {
        return static_cast<uint8_t>(
            (KeyA >> ((idx % 4) * 8)) ^ (idx * 0x9E3779B9u));
    }

public:
    template<size_t... Is>
    constexpr layered_encrypted_string(const char(&str)[N],
        kernelcloak::detail::index_sequence<Is...>)
        : m_data{}
    {
        // start with padded plaintext
        uint8_t buf[padded] = {};
        const uint8_t src[] = { static_cast<uint8_t>(str[Is])... };
        for (size_t i = 0; i < N; ++i) buf[i] = src[i];

        // layer 1: XOR
        for (size_t i = 0; i < padded; ++i)
            buf[i] ^= xor_key(i);

        // layer 2: XTEA encrypt each 8-byte block
        constexpr uint32_t delta = 0x9E3779B9u;
        constexpr uint32_t tk[4] = { KeyB0, KeyB1, KeyB2, KeyB3 };
        for (size_t blk = 0; blk < padded; blk += 8) {
            uint32_t v0 = static_cast<uint32_t>(buf[blk])
                | (static_cast<uint32_t>(buf[blk+1]) << 8)
                | (static_cast<uint32_t>(buf[blk+2]) << 16)
                | (static_cast<uint32_t>(buf[blk+3]) << 24);
            uint32_t v1 = static_cast<uint32_t>(buf[blk+4])
                | (static_cast<uint32_t>(buf[blk+5]) << 8)
                | (static_cast<uint32_t>(buf[blk+6]) << 16)
                | (static_cast<uint32_t>(buf[blk+7]) << 24);

            uint32_t sum = 0;
            for (size_t r = 0; r < KC_XTEA_ROUNDS; ++r) {
                v0 += (((v1 << 4) ^ (v1 >> 5)) + v1) ^ (sum + tk[sum & 3]);
                sum += delta;
                v1 += (((v0 << 4) ^ (v0 >> 5)) + v0) ^ (sum + tk[(sum >> 11) & 3]);
            }

            buf[blk]   = static_cast<uint8_t>(v0);
            buf[blk+1] = static_cast<uint8_t>(v0 >> 8);
            buf[blk+2] = static_cast<uint8_t>(v0 >> 16);
            buf[blk+3] = static_cast<uint8_t>(v0 >> 24);
            buf[blk+4] = static_cast<uint8_t>(v1);
            buf[blk+5] = static_cast<uint8_t>(v1 >> 8);
            buf[blk+6] = static_cast<uint8_t>(v1 >> 16);
            buf[blk+7] = static_cast<uint8_t>(v1 >> 24);
        }

        // layer 3: shuffle bytes
        for (size_t i = 0; i < padded; ++i)
            m_data[s_perm.forward[i]] = buf[i];
    }

    KC_FORCEINLINE void decrypt(char* out) const {
        uint8_t buf[padded];

        // undo shuffle: buf[i] = m_data[forward[i]] undone by inverse
        for (size_t i = 0; i < padded; ++i)
            buf[s_perm.inverse[i]] = m_data[i];

        // undo XTEA
        constexpr uint32_t delta = 0x9E3779B9u;
        constexpr uint32_t tk[4] = { KeyB0, KeyB1, KeyB2, KeyB3 };
        for (size_t blk = 0; blk < padded; blk += 8) {
            uint32_t v0 = static_cast<uint32_t>(buf[blk])
                | (static_cast<uint32_t>(buf[blk+1]) << 8)
                | (static_cast<uint32_t>(buf[blk+2]) << 16)
                | (static_cast<uint32_t>(buf[blk+3]) << 24);
            uint32_t v1 = static_cast<uint32_t>(buf[blk+4])
                | (static_cast<uint32_t>(buf[blk+5]) << 8)
                | (static_cast<uint32_t>(buf[blk+6]) << 16)
                | (static_cast<uint32_t>(buf[blk+7]) << 24);

            uint32_t sum = delta * KC_XTEA_ROUNDS;
            for (size_t r = 0; r < KC_XTEA_ROUNDS; ++r) {
                v1 -= (((v0 << 4) ^ (v0 >> 5)) + v0) ^ (sum + tk[(sum >> 11) & 3]);
                sum -= delta;
                v0 -= (((v1 << 4) ^ (v1 >> 5)) + v1) ^ (sum + tk[sum & 3]);
            }

            buf[blk]   = static_cast<uint8_t>(v0);
            buf[blk+1] = static_cast<uint8_t>(v0 >> 8);
            buf[blk+2] = static_cast<uint8_t>(v0 >> 16);
            buf[blk+3] = static_cast<uint8_t>(v0 >> 24);
            buf[blk+4] = static_cast<uint8_t>(v1);
            buf[blk+5] = static_cast<uint8_t>(v1 >> 8);
            buf[blk+6] = static_cast<uint8_t>(v1 >> 16);
            buf[blk+7] = static_cast<uint8_t>(v1 >> 24);
        }

        // undo XOR and output only the original N bytes
        for (size_t i = 0; i < N; ++i)
            out[i] = static_cast<char>(buf[i] ^ xor_key(i));
    }

    static constexpr size_t length() { return N - 1; }
    static constexpr size_t size() { return N; }
};

// decryption wrapper for layered strings - same pattern as encrypted_string
template<size_t N, uint32_t KeyA,
    uint32_t KeyB0, uint32_t KeyB1, uint32_t KeyB2, uint32_t KeyB3,
    uint32_t ShuffleSeed>
struct decrypted_layered_string {
    char buf[N];

    using enc_t = layered_encrypted_string<N, KeyA, KeyB0, KeyB1, KeyB2, KeyB3, ShuffleSeed>;

    KC_FORCEINLINE decrypted_layered_string(const enc_t& enc) {
        enc.decrypt(buf);
    }

    KC_FORCEINLINE operator const char*() const { return buf; }
    KC_FORCEINLINE const char* c_str() const { return buf; }
    KC_FORCEINLINE size_t length() const { return N - 1; }
};

template<size_t N, uint32_t KeyA,
    uint32_t KeyB0, uint32_t KeyB1, uint32_t KeyB2, uint32_t KeyB3,
    uint32_t ShuffleSeed>
KC_FORCEINLINE auto make_decrypted_layered(
    const layered_encrypted_string<N, KeyA, KeyB0, KeyB1, KeyB2, KeyB3, ShuffleSeed>& enc)
{
    return decrypted_layered_string<N, KeyA, KeyB0, KeyB1, KeyB2, KeyB3, ShuffleSeed>(enc);
}

// re-keying holder for high-security strings
// caller owns this struct. after KC_LAYERED_REKEY_INTERVAL accesses,
// the cached decryption re-encrypts with a fresh runtime key.
// uses InterlockedIncrement for thread safety, no CRT, no atexit.
template<size_t N, uint32_t KeyA,
    uint32_t KeyB0, uint32_t KeyB1, uint32_t KeyB2, uint32_t KeyB3,
    uint32_t ShuffleSeed>
struct layered_string_holder {
    using enc_t = layered_encrypted_string<N, KeyA, KeyB0, KeyB1, KeyB2, KeyB3, ShuffleSeed>;

    const enc_t* source;
    uint8_t runtime_cache[N];
    uint32_t runtime_key;
    volatile long access_count;
    bool cache_valid;

    KC_FORCEINLINE void init(const enc_t* src) {
        source = src;
        runtime_key = 0;
        access_count = 0;
        cache_valid = false;
    }

    KC_NOINLINE void rekey() {
        char plain[N];
        source->decrypt(plain);

        // entropy from rdtsc + counter
        uint32_t seed = static_cast<uint32_t>(access_count) * 0x45D9F3Bu;
        seed ^= static_cast<uint32_t>(__rdtsc());
        runtime_key = seed | 1u;

        for (size_t i = 0; i < N; ++i) {
            uint8_t k = static_cast<uint8_t>(
                (runtime_key >> ((i % 4) * 8)) ^ (i * 0x27D4EB2Du));
            runtime_cache[i] = static_cast<uint8_t>(plain[i]) ^ k;
        }

        // scrub plaintext
        volatile char* vp = plain;
        for (size_t i = 0; i < N; ++i) vp[i] = 0;

        cache_valid = true;
    }

    KC_FORCEINLINE void decrypt(char* out) {
        long count = _InterlockedIncrement(&access_count);

        if (!cache_valid || (count % KC_LAYERED_REKEY_INTERVAL) == 0)
            rekey();

        for (size_t i = 0; i < N; ++i) {
            uint8_t k = static_cast<uint8_t>(
                (runtime_key >> ((i % 4) * 8)) ^ (i * 0x27D4EB2Du));
            out[i] = static_cast<char>(runtime_cache[i] ^ k);
        }
    }
};

} // namespace detail
} // namespace strings
} // namespace kernelcloak

// six keys derived from a single __COUNTER__ expansion inside the lambda
// make_decrypted_layered deduces all template params from the encrypted type
#define KC_STR_LAYERED(s)                                                       \
    ::kernelcloak::strings::detail::make_decrypted_layered(                     \
        []() -> const auto& {                                                   \
            constexpr ::kernelcloak::uint32_t _cnt =                            \
                static_cast<::kernelcloak::uint32_t>(__COUNTER__);              \
            constexpr ::kernelcloak::uint32_t _ln =                             \
                static_cast<::kernelcloak::uint32_t>(__LINE__);                 \
            constexpr ::kernelcloak::uint32_t _ka =                             \
                (_cnt + 1) * 0x45D9F3Bu ^ _ln * 0x1B873593u;                    \
            constexpr ::kernelcloak::uint32_t _kb0 =                            \
                (_cnt + 2) * 0xCC9E2D51u ^ _ln * 0x85EBCA6Bu;                   \
            constexpr ::kernelcloak::uint32_t _kb1 =                            \
                (_cnt + 3) * 0xC2B2AE35u ^ _ln * 0x27D4EB2Du;                   \
            constexpr ::kernelcloak::uint32_t _kb2 =                            \
                (_cnt + 4) * 0x165667B1u ^ _ln * 0xE6546B64u;                   \
            constexpr ::kernelcloak::uint32_t _kb3 =                            \
                (_cnt + 5) * 0x9E3779B9u ^ _ln * 0x41C64E6Du;                   \
            constexpr ::kernelcloak::uint32_t _sh =                             \
                (_cnt + 6) * 0x6C62272Eu ^ _ln * 0xBEA6E8C5u;                   \
            static constexpr ::kernelcloak::strings::detail::                   \
                layered_encrypted_string<sizeof(s),                             \
                    _ka, _kb0, _kb1, _kb2, _kb3, _sh> e(                        \
                s, ::kernelcloak::detail::make_index_sequence<sizeof(s)>{}      \
            );                                                                  \
            return e;                                                           \
        }()                                                                     \
    )

// re-keyable holder - caller owns the struct
// usage:
//   KC_STR_LAYERED_HOLDER(my_str, "secret");
//   char buf[sizeof("secret")];
//   my_str.decrypt(buf);
#define KC_STR_LAYERED_HOLDER(name, s)                                          \
    auto name = [&]() {                                                         \
        constexpr ::kernelcloak::uint32_t _cnt =                                \
            static_cast<::kernelcloak::uint32_t>(__COUNTER__);                  \
        constexpr ::kernelcloak::uint32_t _ln =                                 \
            static_cast<::kernelcloak::uint32_t>(__LINE__);                     \
        constexpr ::kernelcloak::uint32_t _ka =                                 \
            (_cnt + 1) * 0x45D9F3Bu ^ _ln * 0x1B873593u;                        \
        constexpr ::kernelcloak::uint32_t _kb0 =                                \
            (_cnt + 2) * 0xCC9E2D51u ^ _ln * 0x85EBCA6Bu;                       \
        constexpr ::kernelcloak::uint32_t _kb1 =                                \
            (_cnt + 3) * 0xC2B2AE35u ^ _ln * 0x27D4EB2Du;                       \
        constexpr ::kernelcloak::uint32_t _kb2 =                                \
            (_cnt + 4) * 0x165667B1u ^ _ln * 0xE6546B64u;                       \
        constexpr ::kernelcloak::uint32_t _kb3 =                                \
            (_cnt + 5) * 0x9E3779B9u ^ _ln * 0x41C64E6Du;                       \
        constexpr ::kernelcloak::uint32_t _sh =                                 \
            (_cnt + 6) * 0x6C62272Eu ^ _ln * 0xBEA6E8C5u;                       \
        using _enc_t = ::kernelcloak::strings::detail::                         \
            layered_encrypted_string<sizeof(s),                                 \
                _ka, _kb0, _kb1, _kb2, _kb3, _sh>;                              \
        static constexpr _enc_t _src(                                           \
            s, ::kernelcloak::detail::make_index_sequence<sizeof(s)>{}          \
        );                                                                      \
        using _holder_t = ::kernelcloak::strings::detail::                      \
            layered_string_holder<sizeof(s),                                    \
                _ka, _kb0, _kb1, _kb2, _kb3, _sh>;                              \
        _holder_t h;                                                            \
        h.init(&_src);                                                          \
        return h;                                                               \
    }()

#else // KC_ENABLE_STRING_ENCRYPTION disabled

#define KC_STR_LAYERED(s)              (s)
#define KC_STR_LAYERED_HOLDER(name, s) const char* name = (s)

#endif // KC_ENABLE_STRING_ENCRYPTION

```

`strings/stack_string.h`:

```h
#pragma once
#include "../config.h"
#include "../core/types.h"

#if KC_ENABLE_STRING_ENCRYPTION

// character-by-character stack construction
// no string literal appears anywhere in the binary - not even encrypted
// each character is XOR'd with a compile-time key and decoded at runtime

namespace kernelcloak {
namespace strings {
namespace detail {

// single obfuscated character - stores XOR'd value as template param
template<typename CharT, CharT C, uint32_t Key, size_t Idx>
struct obfuscated_char {
    static constexpr CharT key_byte() {
        return static_cast<CharT>((Key >> ((Idx % 4) * 8)) ^ (Idx * 0x27D4EB2Du));
    }
    static constexpr CharT encrypted = C ^ key_byte();

    KC_FORCEINLINE static CharT decode() {
        // the volatile prevents the compiler from folding this back to a constant
        volatile CharT enc = encrypted;
        return enc ^ key_byte();
    }
};

} // namespace detail
} // namespace strings
} // namespace kernelcloak

// internal helpers for key generation per macro site
#define KC_STACK_KEY_                                                            \
    (static_cast<::kernelcloak::uint32_t>(                                      \
        (__COUNTER__ + 1) * 0x45D9F3Bu ^                                        \
        __LINE__ * 0x1B873593u ^                                                \
        0xDEADBEEFu                                                             \
    ))

// char-by-char assignment with obfuscation
// expands to: name[0] = decode<char, 'h', key, 0>(); name[1] = ...
#define KC_SC_(name, key, idx, c)                                               \
    name[idx] = ::kernelcloak::strings::detail::obfuscated_char<                \
        char, c, key, idx>::decode()

#define KC_SWC_(name, key, idx, c)                                              \
    name[idx] = ::kernelcloak::strings::detail::obfuscated_char<                \
        wchar_t, c, key, idx>::decode()

// variadic char assignment dispatch
// we need to manually enumerate since C++17 doesn't have __VA_OPT__ reliably
// and fold expressions can't do indexed assignment

#define KC_SC_1(n, k, c0) \
    KC_SC_(n,k,0,c0)
#define KC_SC_2(n, k, c0, c1) \
    KC_SC_1(n,k,c0); KC_SC_(n,k,1,c1)
#define KC_SC_3(n, k, c0, c1, c2) \
    KC_SC_2(n,k,c0,c1); KC_SC_(n,k,2,c2)
#define KC_SC_4(n, k, c0, c1, c2, c3) \
    KC_SC_3(n,k,c0,c1,c2); KC_SC_(n,k,3,c3)
#define KC_SC_5(n, k, c0, c1, c2, c3, c4) \
    KC_SC_4(n,k,c0,c1,c2,c3); KC_SC_(n,k,4,c4)
#define KC_SC_6(n, k, c0, c1, c2, c3, c4, c5) \
    KC_SC_5(n,k,c0,c1,c2,c3,c4); KC_SC_(n,k,5,c5)
#define KC_SC_7(n, k, c0, c1, c2, c3, c4, c5, c6) \
    KC_SC_6(n,k,c0,c1,c2,c3,c4,c5); KC_SC_(n,k,6,c6)
#define KC_SC_8(n, k, c0, c1, c2, c3, c4, c5, c6, c7) \
    KC_SC_7(n,k,c0,c1,c2,c3,c4,c5,c6); KC_SC_(n,k,7,c7)
#define KC_SC_9(n, k, c0, c1, c2, c3, c4, c5, c6, c7, c8) \
    KC_SC_8(n,k,c0,c1,c2,c3,c4,c5,c6,c7); KC_SC_(n,k,8,c8)
#define KC_SC_10(n, k, c0, c1, c2, c3, c4, c5, c6, c7, c8, c9) \
    KC_SC_9(n,k,c0,c1,c2,c3,c4,c5,c6,c7,c8); KC_SC_(n,k,9,c9)
#define KC_SC_11(n, k, c0, c1, c2, c3, c4, c5, c6, c7, c8, c9, c10) \
    KC_SC_10(n,k,c0,c1,c2,c3,c4,c5,c6,c7,c8,c9); KC_SC_(n,k,10,c10)
#define KC_SC_12(n, k, c0, c1, c2, c3, c4, c5, c6, c7, c8, c9, c10, c11) \
    KC_SC_11(n,k,c0,c1,c2,c3,c4,c5,c6,c7,c8,c9,c10); KC_SC_(n,k,11,c11)
#define KC_SC_13(n, k, c0, c1, c2, c3, c4, c5, c6, c7, c8, c9, c10, c11, c12) \
    KC_SC_12(n,k,c0,c1,c2,c3,c4,c5,c6,c7,c8,c9,c10,c11); KC_SC_(n,k,12,c12)
#define KC_SC_14(n, k, c0, c1, c2, c3, c4, c5, c6, c7, c8, c9, c10, c11, c12, c13) \
    KC_SC_13(n,k,c0,c1,c2,c3,c4,c5,c6,c7,c8,c9,c10,c11,c12); KC_SC_(n,k,13,c13)
#define KC_SC_15(n, k, c0, c1, c2, c3, c4, c5, c6, c7, c8, c9, c10, c11, c12, c13, c14) \
    KC_SC_14(n,k,c0,c1,c2,c3,c4,c5,c6,c7,c8,c9,c10,c11,c12,c13); KC_SC_(n,k,14,c14)
#define KC_SC_16(n, k, c0, c1, c2, c3, c4, c5, c6, c7, c8, c9, c10, c11, c12, c13, c14, c15) \
    KC_SC_15(n,k,c0,c1,c2,c3,c4,c5,c6,c7,c8,c9,c10,c11,c12,c13,c14); KC_SC_(n,k,15,c15)
#define KC_SC_17(n, k, c0, c1, c2, c3, c4, c5, c6, c7, c8, c9, c10, c11, c12, c13, c14, c15, c16) \
    KC_SC_16(n,k,c0,c1,c2,c3,c4,c5,c6,c7,c8,c9,c10,c11,c12,c13,c14,c15); KC_SC_(n,k,16,c16)
#define KC_SC_18(n, k, c0, c1, c2, c3, c4, c5, c6, c7, c8, c9, c10, c11, c12, c13, c14, c15, c16, c17) \
    KC_SC_17(n,k,c0,c1,c2,c3,c4,c5,c6,c7,c8,c9,c10,c11,c12,c13,c14,c15,c16); KC_SC_(n,k,17,c17)
#define KC_SC_19(n, k, c0, c1, c2, c3, c4, c5, c6, c7, c8, c9, c10, c11, c12, c13, c14, c15, c16, c17, c18) \
    KC_SC_18(n,k,c0,c1,c2,c3,c4,c5,c6,c7,c8,c9,c10,c11,c12,c13,c14,c15,c16,c17); KC_SC_(n,k,18,c18)
#define KC_SC_20(n, k, c0, c1, c2, c3, c4, c5, c6, c7, c8, c9, c10, c11, c12, c13, c14, c15, c16, c17, c18, c19) \
    KC_SC_19(n,k,c0,c1,c2,c3,c4,c5,c6,c7,c8,c9,c10,c11,c12,c13,c14,c15,c16,c17,c18); KC_SC_(n,k,19,c19)

// wide char variants
#define KC_SWC_1(n, k, c0) \
    KC_SWC_(n,k,0,c0)
#define KC_SWC_2(n, k, c0, c1) \
    KC_SWC_1(n,k,c0); KC_SWC_(n,k,1,c1)
#define KC_SWC_3(n, k, c0, c1, c2) \
    KC_SWC_2(n,k,c0,c1); KC_SWC_(n,k,2,c2)
#define KC_SWC_4(n, k, c0, c1, c2, c3) \
    KC_SWC_3(n,k,c0,c1,c2); KC_SWC_(n,k,3,c3)
#define KC_SWC_5(n, k, c0, c1, c2, c3, c4) \
    KC_SWC_4(n,k,c0,c1,c2,c3); KC_SWC_(n,k,4,c4)
#define KC_SWC_6(n, k, c0, c1, c2, c3, c4, c5) \
    KC_SWC_5(n,k,c0,c1,c2,c3,c4); KC_SWC_(n,k,5,c5)
#define KC_SWC_7(n, k, c0, c1, c2, c3, c4, c5, c6) \
    KC_SWC_6(n,k,c0,c1,c2,c3,c4,c5); KC_SWC_(n,k,6,c6)
#define KC_SWC_8(n, k, c0, c1, c2, c3, c4, c5, c6, c7) \
    KC_SWC_7(n,k,c0,c1,c2,c3,c4,c5,c6); KC_SWC_(n,k,7,c7)
#define KC_SWC_9(n, k, c0, c1, c2, c3, c4, c5, c6, c7, c8) \
    KC_SWC_8(n,k,c0,c1,c2,c3,c4,c5,c6,c7); KC_SWC_(n,k,8,c8)
#define KC_SWC_10(n, k, c0, c1, c2, c3, c4, c5, c6, c7, c8, c9) \
    KC_SWC_9(n,k,c0,c1,c2,c3,c4,c5,c6,c7,c8); KC_SWC_(n,k,9,c9)
#define KC_SWC_11(n, k, c0, c1, c2, c3, c4, c5, c6, c7, c8, c9, c10) \
    KC_SWC_10(n,k,c0,c1,c2,c3,c4,c5,c6,c7,c8,c9); KC_SWC_(n,k,10,c10)
#define KC_SWC_12(n, k, c0, c1, c2, c3, c4, c5, c6, c7, c8, c9, c10, c11) \
    KC_SWC_11(n,k,c0,c1,c2,c3,c4,c5,c6,c7,c8,c9,c10); KC_SWC_(n,k,11,c11)
#define KC_SWC_13(n, k, c0, c1, c2, c3, c4, c5, c6, c7, c8, c9, c10, c11, c12) \
    KC_SWC_12(n,k,c0,c1,c2,c3,c4,c5,c6,c7,c8,c9,c10,c11); KC_SWC_(n,k,12,c12)
#define KC_SWC_14(n, k, c0, c1, c2, c3, c4, c5, c6, c7, c8, c9, c10, c11, c12, c13) \
    KC_SWC_13(n,k,c0,c1,c2,c3,c4,c5,c6,c7,c8,c9,c10,c11,c12); KC_SWC_(n,k,13,c13)
#define KC_SWC_15(n, k, c0, c1, c2, c3, c4, c5, c6, c7, c8, c9, c10, c11, c12, c13, c14) \
    KC_SWC_14(n,k,c0,c1,c2,c3,c4,c5,c6,c7,c8,c9,c10,c11,c12,c13); KC_SWC_(n,k,14,c14)
#define KC_SWC_16(n, k, c0, c1, c2, c3, c4, c5, c6, c7, c8, c9, c10, c11, c12, c13, c14, c15) \
    KC_SWC_15(n,k,c0,c1,c2,c3,c4,c5,c6,c7,c8,c9,c10,c11,c12,c13,c14); KC_SWC_(n,k,15,c15)
#define KC_SWC_17(n, k, c0, c1, c2, c3, c4, c5, c6, c7, c8, c9, c10, c11, c12, c13, c14, c15, c16) \
    KC_SWC_16(n,k,c0,c1,c2,c3,c4,c5,c6,c7,c8,c9,c10,c11,c12,c13,c14,c15); KC_SWC_(n,k,16,c16)
#define KC_SWC_18(n, k, c0, c1, c2, c3, c4, c5, c6, c7, c8, c9, c10, c11, c12, c13, c14, c15, c16, c17) \
    KC_SWC_17(n,k,c0,c1,c2,c3,c4,c5,c6,c7,c8,c9,c10,c11,c12,c13,c14,c15,c16); KC_SWC_(n,k,17,c17)
#define KC_SWC_19(n, k, c0, c1, c2, c3, c4, c5, c6, c7, c8, c9, c10, c11, c12, c13, c14, c15, c16, c17, c18) \
    KC_SWC_18(n,k,c0,c1,c2,c3,c4,c5,c6,c7,c8,c9,c10,c11,c12,c13,c14,c15,c16,c17); KC_SWC_(n,k,18,c18)
#define KC_SWC_20(n, k, c0, c1, c2, c3, c4, c5, c6, c7, c8, c9, c10, c11, c12, c13, c14, c15, c16, c17, c18, c19) \
    KC_SWC_19(n,k,c0,c1,c2,c3,c4,c5,c6,c7,c8,c9,c10,c11,c12,c13,c14,c15,c16,c17,c18); KC_SWC_(n,k,19,c19)

// argument counting
#define KC_SC_COUNT_(_1,_2,_3,_4,_5,_6,_7,_8,_9,_10,_11,_12,_13,_14,_15,_16,_17,_18,_19,_20,N,...) N
#define KC_SC_COUNT(...) KC_SC_COUNT_(__VA_ARGS__,20,19,18,17,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1)

#define KC_SC_PASTE_(a, b) a##b
#define KC_SC_PASTE(a, b) KC_SC_PASTE_(a, b)

// KC_STACK_STR(name, 'h','e','l','l','o','\0')
// constructs char name[N] on the stack with each char individually obfuscated
// no string literal in the binary whatsoever
#define KC_STACK_STR(name, ...)                                                 \
    char name[KC_SC_COUNT(__VA_ARGS__)];                                        \
    do {                                                                        \
        constexpr auto _kc_sk_ = KC_STACK_KEY_;                                 \
        KC_SC_PASTE(KC_SC_, KC_SC_COUNT(__VA_ARGS__))(name, _kc_sk_, __VA_ARGS__); \
    } while(0)

// KC_STACK_WSTR(name, L'h', L'e', ...)
#define KC_STACK_WSTR(name, ...)                                                \
    wchar_t name[KC_SC_COUNT(__VA_ARGS__)];                                     \
    do {                                                                        \
        constexpr auto _kc_sk_ = KC_STACK_KEY_;                                 \
        KC_SC_PASTE(KC_SWC_, KC_SC_COUNT(__VA_ARGS__))(name, _kc_sk_, __VA_ARGS__); \
    } while(0)

#else // KC_ENABLE_STRING_ENCRYPTION disabled

// passthrough - just declare a char array with the literal characters
#define KC_STACK_STR(name, ...) \
    char name[] = { __VA_ARGS__ }

#define KC_STACK_WSTR(name, ...) \
    wchar_t name[] = { __VA_ARGS__ }

#endif // KC_ENABLE_STRING_ENCRYPTION

```