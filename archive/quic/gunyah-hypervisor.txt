Project Path: arc_quic_gunyah-hypervisor_fbzuabo_

Source Tree:

```txt
arc_quic_gunyah-hypervisor_fbzuabo_
├── AUTHORS
├── CHANGELOG.md
├── CODE-OF-CONDUCT.md
├── CONTRIBUTING.md
├── LICENSE
├── README.md
├── SConstruct
├── config
│   ├── arch
│   │   ├── aarch64.conf
│   │   ├── armv8-64.conf
│   │   ├── armv8.conf
│   │   ├── cortex-a-v8_0.conf
│   │   ├── cortex-a-v8_2.conf
│   │   ├── cortex-a53.conf
│   │   ├── cortex-a55-a76.conf
│   │   ├── cortex-a55-a77.conf
│   │   ├── cortex-a55-a78-x1.conf
│   │   ├── cortex-a55.conf
│   │   ├── cortex-a78.conf
│   │   ├── gic-500.conf
│   │   ├── gic-600.conf
│   │   ├── gic-700-vlpi.conf
│   │   ├── gic-qemu.conf
│   │   └── qemu-armv8-5a-rng.conf
│   ├── featureset
│   │   ├── gunyah-rm-qemu.conf
│   │   └── unittests-qemu.conf
│   ├── include
│   │   ├── debug_no_cspace_rand.conf
│   │   ├── debug_no_kaslr.conf
│   │   └── debug_no_rootvm_aslr.conf
│   ├── platform
│   │   └── qemu.conf
│   └── quality
│       ├── debug.conf
│       ├── production-unchecked.conf
│       └── production.conf
├── configure.py
├── docs
│   ├── api
│   │   ├── Makefile
│   │   └── gunyah_api.md
│   ├── build.md
│   ├── images
│   │   └── logo-quic-on@h68.png
│   ├── linux.md
│   ├── qemu.md
│   ├── ramdisk.md
│   ├── setup.md
│   ├── terminology.md
│   └── test.md
├── hyp
│   ├── arch
│   │   ├── aarch64
│   │   │   ├── include
│   │   │   │   ├── asm
│   │   │   │   │   ├── asm_defs.inc
│   │   │   │   │   ├── barrier.h
│   │   │   │   │   ├── event.h
│   │   │   │   │   ├── interrupt.h
│   │   │   │   │   ├── nospec_checks.h
│   │   │   │   │   ├── panic.inc
│   │   │   │   │   ├── sysregs.h
│   │   │   │   │   └── system_registers.h
│   │   │   │   └── reg
│   │   │   │       └── registers_arm.inc
│   │   │   ├── link.lds
│   │   │   ├── registers.reg
│   │   │   ├── src
│   │   │   │   ├── asm_ordering.c
│   │   │   │   ├── nospec_checks.c
│   │   │   │   └── timestamp.c
│   │   │   ├── templates
│   │   │   │   └── hypregisters.h.tmpl
│   │   │   └── types.tc
│   │   ├── armv8
│   │   │   └── include
│   │   │       └── asm
│   │   │           ├── atomic.h
│   │   │           ├── cache.h
│   │   │           └── timestamp.h
│   │   ├── build.conf
│   │   ├── cortex-a-v8_0
│   │   │   ├── include
│   │   │   │   └── asm
│   │   │   │       ├── cpu.h
│   │   │   │       └── system_registers_cpu.h
│   │   │   ├── src
│   │   │   │   └── sysreg_init.c
│   │   │   └── sysreg_init.ev
│   │   ├── cortex-a-v8_2
│   │   │   ├── include
│   │   │   │   └── asm
│   │   │   │       ├── cpu.h
│   │   │   │       └── system_registers_cpu.h
│   │   │   ├── src
│   │   │   │   └── sysreg_init.c
│   │   │   └── sysreg_init.ev
│   │   ├── cortex-a-v9
│   │   │   ├── include
│   │   │   │   └── asm
│   │   │   │       ├── cpu.h
│   │   │   │       └── system_registers_cpu.h
│   │   │   ├── src
│   │   │   │   └── sysreg_init.c
│   │   │   └── sysreg_init.ev
│   │   ├── cortex-a53
│   │   │   └── include
│   │   │       └── asm
│   │   │           └── prefetch.h
│   │   ├── generic
│   │   │   └── include
│   │   │       ├── asm
│   │   │       │   └── prefetch.h
│   │   │       └── asm-generic
│   │   │           ├── asm_defs.inc
│   │   │           ├── atomic.h
│   │   │           ├── event.h
│   │   │           └── prefetch.h
│   │   └── qemu-armv8-5a-rng
│   │       └── include
│   │           └── asm
│   │               └── cpu.h
│   ├── core
│   │   ├── api
│   │   │   ├── aarch64
│   │   │   │   ├── hypercalls.hvc
│   │   │   │   └── templates
│   │   │   │       ├── c_wrapper.c.tmpl
│   │   │   │       └── hypcall_table.S.tmpl
│   │   │   ├── api.tc
│   │   │   ├── build.conf
│   │   │   ├── src
│   │   │   │   └── api.c
│   │   │   └── templates
│   │   │       └── hypcall_def.h.tmpl
│   │   ├── base
│   │   │   ├── aarch64
│   │   │   │   ├── enums.tc
│   │   │   │   ├── src
│   │   │   │   │   ├── cache.c
│   │   │   │   │   └── core_id.c
│   │   │   │   ├── sysregs.tc
│   │   │   │   └── types.tc
│   │   │   ├── armv8
│   │   │   │   └── enums.tc
│   │   │   ├── build.conf
│   │   │   ├── cortex-a-v8_2
│   │   │   │   └── sysregs_cpu.tc
│   │   │   ├── cortex-a-v9
│   │   │   │   └── sysregs_cpu.tc
│   │   │   ├── src
│   │   │   │   └── base.c
│   │   │   ├── templates
│   │   │   │   ├── accessors.c.tmpl
│   │   │   │   ├── hypconstants.h.tmpl
│   │   │   │   ├── hypcontainers.h.tmpl
│   │   │   │   ├── hypresult.c.tmpl
│   │   │   │   └── hypresult.h.tmpl
│   │   │   └── types.tc
│   │   ├── boot
│   │   │   ├── aarch64
│   │   │   │   ├── aarch64_boot.ev
│   │   │   │   ├── include
│   │   │   │   │   └── arch
│   │   │   │   │       └── reloc.h
│   │   │   │   └── src
│   │   │   │       ├── aarch64_boot.c
│   │   │   │       ├── init_el2.S
│   │   │   │       └── init_el2_mmu.S
│   │   │   ├── boot.ev
│   │   │   ├── boot.tc
│   │   │   ├── build.conf
│   │   │   ├── include
│   │   │   │   └── boot_init.h
│   │   │   └── src
│   │   │       ├── boot.c
│   │   │       └── rel_init.c
│   │   ├── cpulocal
│   │   │   ├── build.conf
│   │   │   ├── cpulocal.ev
│   │   │   ├── cpulocal.tc
│   │   │   └── src
│   │   │       └── cpulocal.c
│   │   ├── cspace_twolevel
│   │   │   ├── build.conf
│   │   │   ├── cspace.ev
│   │   │   ├── cspace.tc
│   │   │   ├── cspace_tests.ev
│   │   │   ├── cspace_tests.tc
│   │   │   ├── hypercalls.hvc
│   │   │   ├── include
│   │   │   │   └── cspace_object.h
│   │   │   ├── src
│   │   │   │   ├── cspace_tests.c
│   │   │   │   ├── cspace_twolevel.c
│   │   │   │   └── hypercalls.c
│   │   │   └── templates
│   │   │       ├── cspace_lookup.c.tmpl
│   │   │       ├── hyprights.h.tmpl
│   │   │       ├── object.c.tmpl
│   │   │       ├── object.ev.tmpl
│   │   │       └── rights.h.tmpl
│   │   ├── debug
│   │   │   ├── aarch64
│   │   │   │   ├── debug.ev
│   │   │   │   ├── debug.tc
│   │   │   │   ├── src
│   │   │   │   │   └── debug.c
│   │   │   │   └── templates
│   │   │   │       └── debug_bps.h.tmpl
│   │   │   ├── build.conf
│   │   │   ├── debug.ev
│   │   │   └── src
│   │   │       └── debug.c
│   │   ├── globals
│   │   │   ├── build.conf
│   │   │   ├── globals.ev
│   │   │   └── src
│   │   │       └── globals.c
│   │   ├── idle
│   │   │   ├── aarch64
│   │   │   │   └── src
│   │   │   │       └── idle.c
│   │   │   ├── build.conf
│   │   │   ├── idle.ev
│   │   │   ├── idle.tc
│   │   │   ├── include
│   │   │   │   └── idle_arch.h
│   │   │   └── src
│   │   │       └── idle.c
│   │   ├── ipi
│   │   │   ├── build.conf
│   │   │   ├── ipi.ev
│   │   │   ├── ipi.tc
│   │   │   └── src
│   │   │       └── ipi.c
│   │   ├── irq
│   │   │   ├── build.conf
│   │   │   ├── irq.ev
│   │   │   ├── irq.tc
│   │   │   └── src
│   │   │       └── irq.c
│   │   ├── irq_null
│   │   │   ├── build.conf
│   │   │   └── src
│   │   │       └── irq_null.c
│   │   ├── mutex_trivial
│   │   │   ├── build.conf
│   │   │   ├── mutex.tc
│   │   │   └── src
│   │   │       └── mutex_trivial.c
│   │   ├── object_standard
│   │   │   ├── build.conf
│   │   │   ├── hypercalls.hvc
│   │   │   ├── src
│   │   │   │   └── hypercalls.c
│   │   │   └── templates
│   │   │       ├── object.c.tmpl
│   │   │       └── object.tc.tmpl
│   │   ├── partition_standard
│   │   │   ├── armv8
│   │   │   │   ├── phys_access.ev
│   │   │   │   └── src
│   │   │   │       └── phys_access.c
│   │   │   ├── build.conf
│   │   │   ├── hypercalls.hvc
│   │   │   ├── partition.ev
│   │   │   ├── partition.tc
│   │   │   ├── src
│   │   │   │   ├── hypercalls.c
│   │   │   │   ├── init.c
│   │   │   │   └── partition.c
│   │   │   └── templates
│   │   │       ├── hypercalls.c.tmpl
│   │   │       ├── object.c.tmpl
│   │   │       ├── object.ev.tmpl
│   │   │       └── object.tc.tmpl
│   │   ├── power
│   │   │   ├── build.conf
│   │   │   ├── power.ev
│   │   │   ├── power.tc
│   │   │   └── src
│   │   │       └── power.c
│   │   ├── preempt
│   │   │   ├── build.conf
│   │   │   ├── include
│   │   │   │   └── preempt_attrs.h
│   │   │   ├── preempt.ev
│   │   │   ├── preempt.tc
│   │   │   └── src
│   │   │       └── preempt.c
│   │   ├── preempt_null
│   │   │   ├── build.conf
│   │   │   └── src
│   │   │       └── preempt_null.c
│   │   ├── rcu_bitmap
│   │   │   ├── build.conf
│   │   │   ├── rcu.ev
│   │   │   ├── rcu.tc
│   │   │   └── src
│   │   │       └── rcu_bitmap.c
│   │   ├── rcu_sync
│   │   │   ├── build.conf
│   │   │   ├── rcu_sync.ev
│   │   │   ├── rcu_sync.tc
│   │   │   └── src
│   │   │       └── rcu_sync.c
│   │   ├── scheduler_fprr
│   │   │   ├── build.conf
│   │   │   ├── include
│   │   │   │   └── scheduler_lock.h
│   │   │   ├── scheduler_fprr.ev
│   │   │   ├── scheduler_fprr.tc
│   │   │   └── src
│   │   │       ├── hypercalls.c
│   │   │       ├── scheduler_fprr.c
│   │   │       └── scheduler_tests.c
│   │   ├── scheduler_trivial
│   │   │   ├── build.conf
│   │   │   ├── scheduler_trivial.ev
│   │   │   ├── scheduler_trivial.tc
│   │   │   └── src
│   │   │       └── scheduler_trivial.c
│   │   ├── spinlock_null
│   │   │   ├── build.conf
│   │   │   ├── spinlock.tc
│   │   │   └── src
│   │   │       └── spinlock_null.c
│   │   ├── spinlock_ticket
│   │   │   ├── build.conf
│   │   │   ├── include
│   │   │   │   └── spinlock_attrs.h
│   │   │   ├── spinlock.tc
│   │   │   └── src
│   │   │       └── spinlock_ticket.c
│   │   ├── task_queue
│   │   │   ├── build.conf
│   │   │   ├── src
│   │   │   │   └── task_queue.c
│   │   │   ├── task_queue.ev
│   │   │   └── task_queue.tc
│   │   ├── tests
│   │   │   ├── build.conf
│   │   │   ├── src
│   │   │   │   ├── print_version.c
│   │   │   │   ├── spinlock_tests.c
│   │   │   │   ├── string_test.c
│   │   │   │   └── tests.c
│   │   │   ├── tests.ev
│   │   │   └── tests.tc
│   │   ├── thread_standard
│   │   │   ├── aarch64
│   │   │   │   ├── src
│   │   │   │   │   ├── thread_arch.c
│   │   │   │   │   └── thread_init.S
│   │   │   │   └── thread_aarch64.tc
│   │   │   ├── build.conf
│   │   │   ├── include
│   │   │   │   └── thread_arch.h
│   │   │   ├── src
│   │   │   │   ├── init.c
│   │   │   │   └── thread.c
│   │   │   ├── thread.ev
│   │   │   └── thread.tc
│   │   ├── timer
│   │   │   ├── build.conf
│   │   │   ├── src
│   │   │   │   ├── timer_queue.c
│   │   │   │   └── timer_tests.c
│   │   │   ├── timer.ev
│   │   │   ├── timer.tc
│   │   │   ├── timer_tests.ev
│   │   │   └── timer_tests.tc
│   │   ├── timer_lp
│   │   │   ├── build.conf
│   │   │   ├── src
│   │   │   │   └── timer_lp_queue.c
│   │   │   ├── timer_lp.ev
│   │   │   └── timer_lp.tc
│   │   ├── timer_null
│   │   │   ├── build.conf
│   │   │   ├── src
│   │   │   │   └── timer_queue_null.c
│   │   │   └── timer.tc
│   │   ├── util
│   │   │   ├── aarch64
│   │   │   │   └── src
│   │   │   │       ├── memcpy.S
│   │   │   │       ├── memset.S
│   │   │   │       └── string.c
│   │   │   ├── build.conf
│   │   │   ├── src
│   │   │   │   ├── assert.c
│   │   │   │   ├── bitmap.c
│   │   │   │   ├── list.c
│   │   │   │   ├── panic.c
│   │   │   │   └── refcount.c
│   │   │   ├── tests
│   │   │   │   ├── Makefile
│   │   │   │   ├── qemu
│   │   │   │   │   └── include
│   │   │   │   │       └── asm
│   │   │   │   │           └── cpu.h
│   │   │   │   └── string.c
│   │   │   └── util.tc
│   │   ├── vdevice
│   │   │   ├── aarch64
│   │   │   │   ├── src
│   │   │   │   │   └── vdevice.c
│   │   │   │   └── vdevice_aarch64.ev
│   │   │   ├── build.conf
│   │   │   ├── include
│   │   │   │   └── internal.h
│   │   │   ├── src
│   │   │   │   ├── access.c
│   │   │   │   └── vdevice.c
│   │   │   ├── vdevice.ev
│   │   │   └── vdevice.tc
│   │   ├── vectors
│   │   │   ├── aarch64
│   │   │   │   ├── include
│   │   │   │   │   ├── trap_dispatch.h
│   │   │   │   │   └── vectors_el2.inc
│   │   │   │   ├── src
│   │   │   │   │   ├── exception_debug.c
│   │   │   │   │   └── trap_dispatch.c
│   │   │   │   └── vectors.ev
│   │   │   ├── armv8-64
│   │   │   │   ├── src
│   │   │   │   │   ├── return.S
│   │   │   │   │   └── vectors.S
│   │   │   │   └── vectors.tc
│   │   │   └── build.conf
│   │   ├── virq_null
│   │   │   ├── build.conf
│   │   │   └── src
│   │   │       └── virq_null.c
│   │   └── wait_queue_broadcast
│   │       ├── build.conf
│   │       ├── src
│   │       │   └── wait_queue.c
│   │       ├── wait_queue.ev
│   │       └── wait_queue.tc
│   ├── debug
│   │   ├── object_lists
│   │   │   ├── build.conf
│   │   │   ├── object_lists.tc
│   │   │   └── templates
│   │   │       ├── object_lists.c.tmpl
│   │   │       ├── object_lists.ev.tmpl
│   │   │       └── object_lists.tc.tmpl
│   │   └── symbol_version
│   │       ├── aarch64
│   │       │   └── src
│   │       │       └── sym_ver.S
│   │       └── build.conf
│   ├── interfaces
│   │   ├── abort
│   │   │   ├── abort.ev
│   │   │   ├── abort.tc
│   │   │   ├── build.conf
│   │   │   └── include
│   │   │       └── abort.h
│   │   ├── addrspace
│   │   │   ├── addrspace.ev
│   │   │   ├── addrspace.hvc
│   │   │   ├── addrspace.tc
│   │   │   ├── build.conf
│   │   │   └── include
│   │   │       └── addrspace.h
│   │   ├── allocator
│   │   │   ├── allocator.ev
│   │   │   ├── allocator.tc
│   │   │   ├── build.conf
│   │   │   └── include
│   │   │       └── allocator.h
│   │   ├── api
│   │   │   ├── api.tc
│   │   │   └── build.conf
│   │   ├── arm_fgt
│   │   │   ├── arm_fgt.reg
│   │   │   ├── arm_fgt.tc
│   │   │   ├── build.conf
│   │   │   └── include
│   │   │       └── arm_fgt.h
│   │   ├── arm_sve
│   │   │   ├── arm_sve.tc
│   │   │   └── build.conf
│   │   ├── base
│   │   │   ├── armv8
│   │   │   │   └── base.tc
│   │   │   ├── base.tc
│   │   │   ├── build.conf
│   │   │   └── include
│   │   │       └── base.h
│   │   ├── boot
│   │   │   ├── boot.ev
│   │   │   ├── build.conf
│   │   │   └── include
│   │   │       ├── boot.h
│   │   │       └── reloc.h
│   │   ├── bootmem
│   │   │   ├── build.conf
│   │   │   └── include
│   │   │       └── bootmem.h
│   │   ├── cpulocal
│   │   │   ├── build.conf
│   │   │   └── include
│   │   │       └── cpulocal.h
│   │   ├── cspace
│   │   │   ├── build.conf
│   │   │   ├── hypercalls.hvc
│   │   │   ├── include
│   │   │   │   └── cspace.h
│   │   │   └── templates
│   │   │       ├── cspace.tc.tmpl
│   │   │       └── cspace_lookup.h.tmpl
│   │   ├── debug
│   │   │   ├── build.conf
│   │   │   └── debug.tc
│   │   ├── doorbell
│   │   │   ├── build.conf
│   │   │   ├── doorbell.hvc
│   │   │   └── doorbell.tc
│   │   ├── elf
│   │   │   ├── build.conf
│   │   │   └── include
│   │   │       ├── elf.h
│   │   │       └── elf_loader.h
│   │   ├── globals
│   │   │   ├── build.conf
│   │   │   ├── globals.tc
│   │   │   └── include
│   │   │       └── globals.h
│   │   ├── gpt
│   │   │   ├── build.conf
│   │   │   ├── gpt.ev
│   │   │   ├── gpt.tc
│   │   │   └── include
│   │   │       └── gpt.h
│   │   ├── hyp_aspace
│   │   │   ├── build.conf
│   │   │   ├── hyp_aspace.tc
│   │   │   └── include
│   │   │       └── hyp_aspace.h
│   │   ├── idle
│   │   │   ├── build.conf
│   │   │   ├── idle.ev
│   │   │   ├── idle.tc
│   │   │   └── include
│   │   │       └── idle.h
│   │   ├── ipi
│   │   │   ├── build.conf
│   │   │   ├── include
│   │   │   │   └── ipi.h
│   │   │   ├── ipi.ev
│   │   │   └── ipi.tc
│   │   ├── irq
│   │   │   ├── build.conf
│   │   │   ├── include
│   │   │   │   └── irq.h
│   │   │   ├── irq.ev
│   │   │   └── irq.tc
│   │   ├── log
│   │   │   ├── build.conf
│   │   │   ├── include
│   │   │   │   └── log.h
│   │   │   ├── log.ev
│   │   │   └── log.tc
│   │   ├── memdb
│   │   │   ├── build.conf
│   │   │   ├── include
│   │   │   │   └── memdb.h
│   │   │   └── memdb.tc
│   │   ├── memextent
│   │   │   ├── build.conf
│   │   │   ├── include
│   │   │   │   └── memextent.h
│   │   │   ├── memextent.ev
│   │   │   ├── memextent.hvc
│   │   │   └── memextent.tc
│   │   ├── msgqueue
│   │   │   ├── build.conf
│   │   │   ├── msgqueue.hvc
│   │   │   └── msgqueue.tc
│   │   ├── mutex
│   │   │   ├── build.conf
│   │   │   ├── include
│   │   │   │   └── mutex.h
│   │   │   └── mutex.ev
│   │   ├── object
│   │   │   ├── build.conf
│   │   │   └── templates
│   │   │       ├── object.ev.tmpl
│   │   │       ├── object.h.tmpl
│   │   │       └── object.tc.tmpl
│   │   ├── object_lists
│   │   │   └── build.conf
│   │   ├── partition
│   │   │   ├── build.conf
│   │   │   ├── include
│   │   │   │   ├── partition.h
│   │   │   │   └── partition_init.h
│   │   │   ├── partition.ev
│   │   │   ├── partition.tc
│   │   │   └── templates
│   │   │       └── partition_alloc.h.tmpl
│   │   ├── pgtable
│   │   │   ├── build.conf
│   │   │   ├── include
│   │   │   │   └── pgtable.h
│   │   │   ├── pgtable.ev
│   │   │   └── pgtable.tc
│   │   ├── platform
│   │   │   ├── build.conf
│   │   │   ├── include
│   │   │   │   ├── platform_cpu.h
│   │   │   │   ├── platform_features.h
│   │   │   │   ├── platform_ipi.h
│   │   │   │   ├── platform_irq.h
│   │   │   │   ├── platform_mem.h
│   │   │   │   ├── platform_pmu.h
│   │   │   │   ├── platform_prng.h
│   │   │   │   ├── platform_psci.h
│   │   │   │   ├── platform_security.h
│   │   │   │   ├── platform_timer.h
│   │   │   │   └── platform_timer_lp.h
│   │   │   ├── platform.ev
│   │   │   └── platform.tc
│   │   ├── power
│   │   │   ├── build.conf
│   │   │   ├── include
│   │   │   │   └── power.h
│   │   │   ├── power.ev
│   │   │   └── power.tc
│   │   ├── preempt
│   │   │   ├── build.conf
│   │   │   ├── include
│   │   │   │   └── preempt.h
│   │   │   └── preempt.tc
│   │   ├── prng
│   │   │   ├── build.conf
│   │   │   ├── include
│   │   │   │   └── prng.h
│   │   │   └── prng.hvc
│   │   ├── psci
│   │   │   ├── build.conf
│   │   │   ├── include
│   │   │   │   └── psci.h
│   │   │   ├── psci.ev
│   │   │   └── psci.tc
│   │   ├── qcbor
│   │   │   ├── build.conf
│   │   │   └── include
│   │   │       ├── qcbor
│   │   │       │   ├── UsefulBuf.h
│   │   │       │   ├── qcbor_common.h
│   │   │       │   ├── qcbor_encode.h
│   │   │       │   └── qcbor_private.h
│   │   │       └── qcbor.h
│   │   ├── rcu
│   │   │   ├── build.conf
│   │   │   ├── include
│   │   │   │   ├── rcu.h
│   │   │   │   └── rcu_attrs.h
│   │   │   ├── rcu.ev
│   │   │   └── rcu.tc
│   │   ├── root_env
│   │   │   ├── build.conf
│   │   │   └── root_env.tc
│   │   ├── scheduler
│   │   │   ├── build.conf
│   │   │   ├── hypercalls.hvc
│   │   │   ├── include
│   │   │   │   └── scheduler.h
│   │   │   ├── scheduler.ev
│   │   │   └── scheduler.tc
│   │   ├── slat
│   │   │   ├── aarch64
│   │   │   │   └── slat.tc
│   │   │   └── build.conf
│   │   ├── smc_trace
│   │   │   ├── build.conf
│   │   │   ├── include
│   │   │   │   └── smc_trace.h
│   │   │   └── smc_trace.tc
│   │   ├── smccc
│   │   │   ├── build.conf
│   │   │   ├── include
│   │   │   │   ├── smccc.ev.h
│   │   │   │   ├── smccc.h
│   │   │   │   └── smccc_platform.h
│   │   │   ├── smccc.ev
│   │   │   └── smccc.tc
│   │   ├── spinlock
│   │   │   ├── build.conf
│   │   │   ├── include
│   │   │   │   └── spinlock.h
│   │   │   └── spinlock.ev
│   │   ├── task_queue
│   │   │   ├── build.conf
│   │   │   ├── include
│   │   │   │   └── task_queue.h
│   │   │   ├── task_queue.ev
│   │   │   └── task_queue.tc
│   │   ├── tests
│   │   │   ├── build.conf
│   │   │   ├── tests.ev
│   │   │   └── tests.tc
│   │   ├── thread
│   │   │   ├── build.conf
│   │   │   ├── include
│   │   │   │   ├── thread.h
│   │   │   │   └── thread_init.h
│   │   │   ├── thread.ev
│   │   │   └── thread.tc
│   │   ├── timer
│   │   │   ├── build.conf
│   │   │   ├── include
│   │   │   │   └── timer_queue.h
│   │   │   ├── timer.ev
│   │   │   └── timer.tc
│   │   ├── timer_lp
│   │   │   └── build.conf
│   │   ├── trace
│   │   │   ├── build.conf
│   │   │   ├── include
│   │   │   │   ├── trace.h
│   │   │   │   └── trace_helpers.h
│   │   │   ├── trace.ev
│   │   │   ├── trace.hvc
│   │   │   └── trace.tc
│   │   ├── util
│   │   │   ├── bitmap.tc
│   │   │   ├── build.conf
│   │   │   ├── include
│   │   │   │   ├── assert.h
│   │   │   │   ├── atomic.h
│   │   │   │   ├── attributes.h
│   │   │   │   ├── bitmap.h
│   │   │   │   ├── compiler.h
│   │   │   │   ├── enum.h
│   │   │   │   ├── list.h
│   │   │   │   ├── panic.h
│   │   │   │   ├── refcount.h
│   │   │   │   ├── string.h
│   │   │   │   ├── types
│   │   │   │   │   └── bitmap.h
│   │   │   │   └── util.h
│   │   │   └── list.tc
│   │   ├── vcpu
│   │   │   ├── aarch64
│   │   │   │   └── vcpu.tc
│   │   │   ├── armv8
│   │   │   │   ├── traps.ev
│   │   │   │   └── vcpu.tc
│   │   │   ├── build.conf
│   │   │   ├── include
│   │   │   │   └── vcpu.h
│   │   │   ├── vcpu.ev
│   │   │   └── vcpu.tc
│   │   ├── vcpu_power
│   │   │   └── build.conf
│   │   ├── vcpu_run
│   │   │   ├── build.conf
│   │   │   ├── include
│   │   │   │   └── vcpu_run.h
│   │   │   ├── vcpu_run.ev
│   │   │   ├── vcpu_run.hvc
│   │   │   └── vcpu_run.tc
│   │   ├── vdevice
│   │   │   ├── build.conf
│   │   │   ├── include
│   │   │   │   └── vdevice.h
│   │   │   ├── vdevice.ev
│   │   │   └── vdevice.tc
│   │   ├── vectors
│   │   │   ├── armv8
│   │   │   │   └── traps.ev
│   │   │   └── build.conf
│   │   ├── vet
│   │   │   ├── build.conf
│   │   │   ├── include
│   │   │   │   └── vet.h
│   │   │   └── vet.tc
│   │   ├── vgic
│   │   │   ├── build.conf
│   │   │   └── include
│   │   │       └── vgic.h
│   │   ├── vic
│   │   │   ├── build.conf
│   │   │   ├── include
│   │   │   │   └── vic.h
│   │   │   ├── vic.ev
│   │   │   ├── vic.hvc
│   │   │   └── vic.tc
│   │   ├── virq
│   │   │   ├── build.conf
│   │   │   ├── include
│   │   │   │   └── virq.h
│   │   │   ├── virq.ev
│   │   │   └── virq.tc
│   │   ├── virtio_mmio
│   │   │   ├── build.conf
│   │   │   ├── virtio_mmio.hvc
│   │   │   └── virtio_mmio.tc
│   │   ├── vpm
│   │   │   ├── build.conf
│   │   │   ├── include
│   │   │   │   └── vpm.h
│   │   │   ├── vpm.ev
│   │   │   ├── vpm.hvc
│   │   │   └── vpm.tc
│   │   ├── vrtc
│   │   │   ├── build.conf
│   │   │   └── vrtc.hvc
│   │   └── wait_queue
│   │       ├── build.conf
│   │       ├── include
│   │       │   └── wait_queue.h
│   │       ├── wait_queue.ev
│   │       └── wait_queue.tc
│   ├── ipc
│   │   ├── doorbell
│   │   │   ├── build.conf
│   │   │   ├── doorbell.ev
│   │   │   ├── doorbell.tc
│   │   │   ├── include
│   │   │   │   └── doorbell.h
│   │   │   └── src
│   │   │       ├── doorbell.c
│   │   │       └── hypercalls.c
│   │   └── msgqueue
│   │       ├── build.conf
│   │       ├── include
│   │       │   ├── msgqueue.h
│   │       │   └── msgqueue_common.h
│   │       ├── msgqueue.ev
│   │       ├── msgqueue.tc
│   │       └── src
│   │           ├── hypercalls.c
│   │           ├── msgqueue.c
│   │           └── msgqueue_common.c
│   ├── mem
│   │   ├── addrspace
│   │   │   ├── aarch64
│   │   │   │   ├── addrspace.ev
│   │   │   │   ├── addrspace.tc
│   │   │   │   └── src
│   │   │   │       ├── lookup.c
│   │   │   │       └── vmmio.c
│   │   │   ├── addrspace.ev
│   │   │   ├── addrspace.tc
│   │   │   ├── armv8
│   │   │   │   ├── abort.ev
│   │   │   │   └── src
│   │   │   │       └── abort.c
│   │   │   ├── build.conf
│   │   │   └── src
│   │   │       ├── addrspace.c
│   │   │       └── hypercalls.c
│   │   ├── allocator_boot
│   │   │   ├── bootmem.ev
│   │   │   ├── bootmem.tc
│   │   │   ├── build.conf
│   │   │   └── src
│   │   │       └── bootmem.c
│   │   ├── allocator_list
│   │   │   ├── allocator.ev
│   │   │   ├── allocator.tc
│   │   │   ├── build.conf
│   │   │   └── src
│   │   │       └── freelist.c
│   │   ├── hyp_aspace
│   │   │   ├── armv8
│   │   │   │   ├── hyp_aspace.ev
│   │   │   │   └── src
│   │   │   │       └── hyp_aspace.c
│   │   │   ├── build.conf
│   │   │   ├── hyp_aspace.ev
│   │   │   └── hyp_aspace.tc
│   │   ├── memdb
│   │   │   ├── build.conf
│   │   │   ├── memdb_tests.ev
│   │   │   ├── memdb_tests.tc
│   │   │   ├── src
│   │   │   │   └── memdb_tests.c
│   │   │   └── tests
│   │   │       └── test.c
│   │   ├── memdb_bitmap
│   │   │   ├── build.conf
│   │   │   ├── memdb.ev
│   │   │   ├── memdb.tc
│   │   │   └── src
│   │   │       └── memdb.c
│   │   ├── memdb_gpt
│   │   │   ├── build.conf
│   │   │   ├── memdb.ev
│   │   │   ├── memdb.tc
│   │   │   └── src
│   │   │       └── memdb.c
│   │   ├── memextent
│   │   │   ├── build.conf
│   │   │   ├── memextent.ev
│   │   │   ├── memextent.tc
│   │   │   ├── memextent_tests.ev
│   │   │   ├── memextent_tests.tc
│   │   │   └── src
│   │   │       ├── hypercalls.c
│   │   │       ├── memextent.c
│   │   │       ├── memextent_basic.c
│   │   │       └── memextent_tests.c
│   │   ├── memextent_sparse
│   │   │   ├── build.conf
│   │   │   ├── memextent_sparse.ev
│   │   │   ├── memextent_sparse.tc
│   │   │   └── src
│   │   │       ├── memextent_sparse.c
│   │   │       └── memextent_tests.c
│   │   ├── pgtable
│   │   │   ├── armv8
│   │   │   │   ├── pgtable.ev
│   │   │   │   ├── pgtable.tc
│   │   │   │   └── src
│   │   │   │       └── pgtable.c
│   │   │   └── build.conf
│   │   └── useraccess
│   │       ├── aarch64
│   │       │   └── src
│   │       │       └── useraccess.c
│   │       ├── build.conf
│   │       └── include
│   │           └── useraccess.h
│   ├── misc
│   │   ├── abort
│   │   │   ├── abort.ev
│   │   │   ├── abort.tc
│   │   │   ├── build.conf
│   │   │   └── src
│   │   │       └── abort.c
│   │   ├── elf
│   │   │   ├── build.conf
│   │   │   └── src
│   │   │       └── elf_loader.c
│   │   ├── gpt
│   │   │   ├── build.conf
│   │   │   ├── gpt.ev
│   │   │   ├── gpt.tc
│   │   │   ├── src
│   │   │   │   ├── gpt.c
│   │   │   │   └── gpt_tests.c
│   │   │   └── tests
│   │   │       ├── Makefile
│   │   │       └── host_tests.c
│   │   ├── log_standard
│   │   │   ├── build.conf
│   │   │   ├── include
│   │   │   │   └── string_util.h
│   │   │   ├── log.ev
│   │   │   ├── log.tc
│   │   │   └── src
│   │   │       ├── debug.S
│   │   │       ├── log.c
│   │   │       └── string_util.c
│   │   ├── prng_hw
│   │   │   ├── build.conf
│   │   │   ├── prng_api.tc
│   │   │   └── src
│   │   │       └── prng_api.c
│   │   ├── prng_simple
│   │   │   ├── build.conf
│   │   │   ├── chacha20_test.ev
│   │   │   ├── include
│   │   │   │   └── chacha20.h
│   │   │   ├── prng_simple.ev
│   │   │   └── src
│   │   │       ├── chacha20.c
│   │   │       ├── chacha20_test.c
│   │   │       └── prng_simple.c
│   │   ├── qcbor
│   │   │   ├── build.conf
│   │   │   └── src
│   │   │       ├── UsefulBuf.c
│   │   │       └── qcbor_encode.c
│   │   ├── root_env
│   │   │   └── build.conf
│   │   ├── smc_trace
│   │   │   ├── build.conf
│   │   │   ├── smc_trace.tc
│   │   │   └── src
│   │   │       └── smc_trace.c
│   │   ├── spectre_arm
│   │   │   └── build.conf
│   │   ├── trace_null
│   │   │   ├── build.conf
│   │   │   ├── src
│   │   │   │   └── trace.c
│   │   │   └── trace.tc
│   │   ├── trace_standard
│   │   │   ├── build.conf
│   │   │   ├── src
│   │   │   │   ├── debug.S
│   │   │   │   ├── hypercalls.c
│   │   │   │   └── trace.c
│   │   │   ├── test
│   │   │   │   └── basic_test.c
│   │   │   ├── trace.ev
│   │   │   └── trace.tc
│   │   └── vet
│   │       ├── build.conf
│   │       ├── src
│   │       │   └── vet.c
│   │       ├── vet.ev
│   │       └── vet.tc
│   ├── platform
│   │   ├── arm_arch_timer
│   │   │   ├── aarch64
│   │   │   │   └── src
│   │   │   │       └── platform_timer.c
│   │   │   ├── build.conf
│   │   │   ├── platform_timer.ev
│   │   │   ├── platform_timer.tc
│   │   │   └── templates
│   │   │       └── platform_timer_consts.h.tmpl
│   │   ├── arm_arch_timer_lp
│   │   │   ├── build.conf
│   │   │   ├── platform_timer_lp-regs.tc
│   │   │   ├── platform_timer_lp.ev
│   │   │   ├── platform_timer_lp.tc
│   │   │   └── src
│   │   │       └── platform_timer_lp.c
│   │   ├── arm_dsu
│   │   │   ├── aarch64
│   │   │   │   └── src
│   │   │   │       └── platform_dsu.c
│   │   │   ├── build.conf
│   │   │   ├── platform_dsu.ev
│   │   │   └── platform_dsu.tc
│   │   ├── arm_fgt
│   │   │   ├── aarch64
│   │   │   │   ├── arm_fgt_aarch64.ev
│   │   │   │   ├── arm_fgt_aarch64.tc
│   │   │   │   └── src
│   │   │   │       └── arm_fgt.c
│   │   │   ├── arm_fgt.ev
│   │   │   └── build.conf
│   │   ├── arm_generic
│   │   │   ├── aarch64
│   │   │   │   └── src
│   │   │   │       └── cpu.c
│   │   │   └── build.conf
│   │   ├── arm_pmu
│   │   │   ├── aarch64
│   │   │   │   └── src
│   │   │   │       └── platform_pmu.c
│   │   │   ├── build.conf
│   │   │   ├── platform_pmu.ev
│   │   │   └── platform_pmu.tc
│   │   ├── arm_rng
│   │   │   ├── aarch64
│   │   │   │   └── src
│   │   │   │       └── rng.c
│   │   │   └── build.conf
│   │   ├── arm_smccc
│   │   │   ├── aarch64
│   │   │   │   └── src
│   │   │   │       └── smccc_call.c
│   │   │   └── build.conf
│   │   ├── arm_trng_fi
│   │   │   ├── arm_trng.ev
│   │   │   ├── arm_trng.tc
│   │   │   ├── build.conf
│   │   │   └── src
│   │   │       └── arm_trng.c
│   │   ├── ete
│   │   │   ├── aarch64
│   │   │   │   └── ete.tc
│   │   │   ├── build.conf
│   │   │   ├── ete.ev
│   │   │   ├── include
│   │   │   │   └── ete.h
│   │   │   ├── src
│   │   │   │   └── ete.c
│   │   │   └── templates
│   │   │       └── ete_save_restore.h.tmpl
│   │   ├── etm
│   │   │   ├── aarch64
│   │   │   │   └── etm-regs.tc
│   │   │   ├── build.conf
│   │   │   ├── etm.ev
│   │   │   ├── etm.tc
│   │   │   ├── include
│   │   │   │   └── etm.h
│   │   │   └── src
│   │   │       └── etm.c
│   │   ├── gicv3
│   │   │   ├── aarch64
│   │   │   │   └── gicv3-regs.tc
│   │   │   ├── build.conf
│   │   │   ├── gicv3.ev
│   │   │   ├── gicv3.tc
│   │   │   ├── include
│   │   │   │   ├── gicv3.h
│   │   │   │   └── gicv3_config.h
│   │   │   ├── src
│   │   │   │   └── gicv3.c
│   │   │   └── templates
│   │   │       └── gich_lrs.h.tmpl
│   │   ├── psci_smc
│   │   │   ├── aarch64
│   │   │   │   └── include
│   │   │   │       └── psci_smc_arch.h
│   │   │   ├── build.conf
│   │   │   ├── include
│   │   │   │   └── psci_smc.h
│   │   │   ├── psci_smc.tc
│   │   │   └── src
│   │   │       └── psci_smc.c
│   │   ├── soc_qemu
│   │   │   ├── build.conf
│   │   │   ├── include
│   │   │   │   └── uart.h
│   │   │   ├── soc_qemu.ev
│   │   │   ├── soc_qemu.tc
│   │   │   └── src
│   │   │       ├── abort.c
│   │   │       ├── addrspace.c
│   │   │       ├── boot.c
│   │   │       ├── cpu.c
│   │   │       ├── cpu_features.c
│   │   │       ├── head.S
│   │   │       ├── irq.c
│   │   │       ├── platform_psci.c
│   │   │       ├── prng.c
│   │   │       ├── soc_qemu.c
│   │   │       └── uart.c
│   │   └── trbe
│   │       ├── aarch64
│   │       │   └── trbe.tc
│   │       ├── build.conf
│   │       ├── include
│   │       │   └── trbe.h
│   │       └── src
│   │           └── trbe.c
│   └── vm
│       ├── arm_pv_time
│       │   ├── arm_pv_time.ev
│       │   ├── arm_pv_time.tc
│       │   ├── build.conf
│       │   └── src
│       │       └── arm_pv_time.c
│       ├── arm_vm_amu
│       │   ├── aarch64
│       │   │   ├── arm_vm_amu_aarch64.ev
│       │   │   ├── src
│       │   │   │   └── arm_vm_amu.c
│       │   │   └── templates
│       │   │       └── arm_vm_amu_counter_regs.c.tmpl
│       │   ├── build.conf
│       │   └── include
│       │       └── arm_vm_amu.h
│       ├── arm_vm_pmu
│       │   ├── aarch64
│       │   │   ├── arm_vm_pmu_aarch64.ev
│       │   │   ├── arm_vm_pmu_aarch64.tc
│       │   │   ├── include
│       │   │   │   └── arm_vm_pmu_event_regs.h
│       │   │   ├── src
│       │   │   │   └── arm_vm_pmu.c
│       │   │   └── templates
│       │   │       └── arm_vm_pmu_event_regs.c.tmpl
│       │   ├── arm_vm_pmu.ev
│       │   ├── arm_vm_pmu.tc
│       │   ├── build.conf
│       │   ├── include
│       │   │   └── arm_vm_pmu.h
│       │   └── src
│       │       └── arm_vm_pmu.c
│       ├── arm_vm_sve_simple
│       │   ├── aarch64
│       │   │   ├── arm_vm_sve_aarch64.ev
│       │   │   ├── arm_vm_sve_aarch64.tc
│       │   │   └── src
│       │   │       └── arm_vm_sve.c
│       │   └── build.conf
│       ├── arm_vm_timer
│       │   ├── aarch64
│       │   │   ├── arm_vm_timer_aarch64.tc
│       │   │   └── src
│       │   │       └── arm_vm_timer.c
│       │   ├── arm_vm_timer.ev
│       │   ├── arm_vm_timer.tc
│       │   ├── build.conf
│       │   ├── include
│       │   │   └── arm_vm_timer.h
│       │   └── src
│       │       ├── arm_vm_timer_irq.c
│       │       └── arm_vm_timer_thread.c
│       ├── psci
│       │   ├── aarch64
│       │   │   ├── psci.ev
│       │   │   └── src
│       │   │       └── psci.c
│       │   ├── build.conf
│       │   ├── include
│       │   │   ├── psci_arch.h
│       │   │   ├── psci_common.h
│       │   │   ├── psci_events.h
│       │   │   └── psci_pm_list.h
│       │   ├── psci.ev
│       │   ├── psci.tc
│       │   └── src
│       │       ├── psci_common.c
│       │       └── psci_pm_list.c
│       ├── psci_pc
│       │   ├── build.conf
│       │   ├── psci_pc.ev
│       │   └── src
│       │       └── psci_pc.c
│       ├── rootvm
│       │   ├── build.conf
│       │   ├── rootvm.ev
│       │   ├── rootvm.tc
│       │   └── src
│       │       └── rootvm_init.c
│       ├── rootvm_package
│       │   ├── build.conf
│       │   ├── rootvm_package.ev
│       │   ├── rootvm_package.tc
│       │   └── src
│       │       └── package.c
│       ├── slat
│       │   └── build.conf
│       ├── smccc
│       │   ├── aarch64
│       │   │   ├── smccc_64.ev
│       │   │   ├── src
│       │   │   │   └── smccc_64.c
│       │   │   └── templates
│       │   │       └── hyp_wrapper.c.tmpl
│       │   ├── build.conf
│       │   ├── include
│       │   │   └── smccc_hypercall.h
│       │   ├── smccc.ev
│       │   └── src
│       │       ├── smccc.c
│       │       └── smccc_hypercalls.c
│       ├── vcpu
│       │   ├── aarch64
│       │   │   ├── hypercalls.hvc
│       │   │   ├── include
│       │   │   │   ├── exception_dispatch.h
│       │   │   │   ├── exception_inject.h
│       │   │   │   ├── reg_access.h
│       │   │   │   └── vectors_vcpu.h
│       │   │   ├── src
│       │   │   │   ├── aarch64_init.c
│       │   │   │   ├── context_switch.c
│       │   │   │   ├── exception_inject.c
│       │   │   │   ├── hypercalls.c
│       │   │   │   ├── reg_access.c
│       │   │   │   ├── sysreg_traps.c
│       │   │   │   ├── trap_dispatch.c
│       │   │   │   └── wfi.c
│       │   │   ├── vcpu_aarch64.ev
│       │   │   └── vcpu_aarch64.tc
│       │   ├── armv8-64
│       │   │   ├── include
│       │   │   │   └── vectors_vcpu.inc
│       │   │   ├── src
│       │   │   │   ├── return.S
│       │   │   │   └── vectors.S
│       │   │   ├── templates
│       │   │   │   ├── vectors_tramp.S.tmpl
│       │   │   │   └── vectors_tramp.c.tmpl
│       │   │   └── vcpu_aarch64.ev
│       │   ├── build.conf
│       │   ├── cortex-a-v8_0
│       │   │   ├── src
│       │   │   │   ├── context_switch.c
│       │   │   │   └── sysreg_traps_cpu.c
│       │   │   └── vcpu_aarch64.tc
│       │   ├── cortex-a-v8_2
│       │   │   ├── src
│       │   │   │   ├── context_switch.c
│       │   │   │   └── sysreg_traps_cpu.c
│       │   │   └── vcpu_aarch64.tc
│       │   ├── cortex-a-v9
│       │   │   ├── src
│       │   │   │   ├── context_switch.c
│       │   │   │   └── sysreg_traps_cpu.c
│       │   │   └── vcpu_aarch64.tc
│       │   ├── include
│       │   │   └── vcpu.h
│       │   ├── qemu-armv8-5a-rng
│       │   │   └── src
│       │   │       ├── context_switch.c
│       │   │       └── sysreg_traps_cpu.c
│       │   ├── src
│       │   │   └── vcpu.c
│       │   ├── vcpu.ev
│       │   └── vcpu.tc
│       ├── vcpu_power
│       │   ├── build.conf
│       │   ├── src
│       │   │   └── vcpu_power.c
│       │   ├── vcpu_power.ev
│       │   └── vcpu_power.tc
│       ├── vcpu_run
│       │   ├── build.conf
│       │   ├── src
│       │   │   └── vcpu_run.c
│       │   ├── vcpu_run.ev
│       │   └── vcpu_run.tc
│       ├── vdebug
│       │   ├── aarch64
│       │   │   ├── src
│       │   │   │   └── vdebug.c
│       │   │   ├── vdebug.ev
│       │   │   └── vdebug.tc
│       │   └── build.conf
│       ├── vete
│       │   ├── build.conf
│       │   ├── src
│       │   │   └── vete.c
│       │   └── vete.ev
│       ├── vetm
│       │   ├── build.conf
│       │   ├── src
│       │   │   └── vetm.c
│       │   ├── vetm.ev
│       │   └── vetm.tc
│       ├── vetm_null
│       │   ├── build.conf
│       │   ├── src
│       │   │   └── vetm_null.c
│       │   └── vetm_null.ev
│       ├── vgic
│       │   ├── build.conf
│       │   ├── include
│       │   │   ├── internal.h
│       │   │   └── vgic.h
│       │   ├── src
│       │   │   ├── deliver.c
│       │   │   ├── distrib.c
│       │   │   ├── sysregs.c
│       │   │   ├── util.c
│       │   │   ├── vdevice.c
│       │   │   ├── vgic.c
│       │   │   └── vpe.c
│       │   ├── vgic.ev
│       │   ├── vgic.hvc
│       │   └── vgic.tc
│       ├── vic_base
│       │   ├── build.conf
│       │   ├── include
│       │   │   └── vic_base.h
│       │   ├── src
│       │   │   ├── forward_private.c
│       │   │   └── hypercalls.c
│       │   ├── vic_base.ev
│       │   └── vic_base.tc
│       ├── virtio_input
│       │   ├── build.conf
│       │   ├── include
│       │   │   └── virtio_input.h
│       │   ├── src
│       │   │   ├── hypercalls.c
│       │   │   └── virtio_input.c
│       │   ├── virtio_input.ev
│       │   ├── virtio_input.hvc
│       │   └── virtio_input.tc
│       ├── virtio_mmio
│       │   ├── build.conf
│       │   ├── include
│       │   │   └── virtio_mmio.h
│       │   ├── src
│       │   │   ├── hypercalls.c
│       │   │   ├── vdevice.c
│       │   │   └── virtio_mmio.c
│       │   ├── virtio_mmio.ev
│       │   └── virtio_mmio.tc
│       ├── vpm_base
│       │   ├── build.conf
│       │   ├── src
│       │   │   └── hypercalls.c
│       │   └── vpm_base.tc
│       ├── vrtc_pl031
│       │   ├── build.conf
│       │   ├── src
│       │   │   ├── hypercalls.c
│       │   │   └── vrtc_pl031.c
│       │   ├── vrtc_pl031.ev
│       │   └── vrtc_pl031.tc
│       └── vtrbe
│           ├── build.conf
│           ├── src
│           │   └── vtrbe.c
│           └── vtrbe.ev
├── repolint.json
└── tools
    ├── __init__.py
    ├── build
    │   ├── __init__.py
    │   ├── __main__.py
    │   ├── gen_sym_ver.py
    │   ├── gen_ver.py
    │   └── gen_ver.sh
    ├── codegen
    │   └── codegen.py
    ├── cpptest
    │   ├── Checkers_Man_All_Req.properties
    │   ├── Cyclomatic.properties
    │   ├── custom.psrc
    │   ├── cyclomatic_xml_to_json.py
    │   ├── gunyahkw.h
    │   ├── gunyahkw.kb
    │   ├── klocwork_xml_to_json.py
    │   └── misra_xml_to_json.py
    ├── debug
    │   └── tracebuf.py
    ├── elf
    │   └── package_apps.py
    ├── events
    │   ├── event_gen.py
    │   ├── ir.py
    │   ├── parser.py
    │   └── templates
    │       ├── c.tmpl
    │       ├── handlers.h.tmpl
    │       └── triggers.h.tmpl
    ├── grammars
    │   ├── events_dsl.lark
    │   ├── hypercalls_dsl.lark
    │   └── typed_dsl.lark
    ├── hypercalls
    │   ├── hypercall.py
    │   ├── hypercall_gen.py
    │   └── templates
    │       ├── guest_interface.c.tmpl
    │       ├── guest_interface.h.tmpl
    │       └── hypercall_api.tmpl
    ├── misc
    │   ├── convert-utf-8.sh
    │   ├── get_genfiles.py
    │   ├── setversion.sh
    │   └── update_cscope.sh
    ├── objects
    │   └── object_gen.py
    ├── registers
    │   └── register_gen.py
    ├── requirements.txt
    ├── typed
    │   ├── abi.py
    │   ├── ast_nodes.py
    │   ├── exceptions.py
    │   ├── ir.py
    │   ├── templates
    │   │   ├── bitfield-generic-accessor.tmpl
    │   │   └── bitfield-type.tmpl
    │   ├── test
    │   │   └── sample_inputs
    │   │       ├── basic_types.tc
    │   │       ├── bitfield.tc
    │   │       ├── bitmap.tc
    │   │       ├── conditional.tc
    │   │       ├── constexpr.tc
    │   │       ├── object_define.tc
    │   │       ├── object_extend.tc
    │   │       ├── struct.tc
    │   │       └── union.tc
    │   └── type_gen.py
    ├── utils
    │   ├── __init__.py
    │   └── genfile.py
    └── vim
        ├── examples
        │   └── cscope_gunyah.vim
        ├── ftdetect
        │   └── gunyah.vim
        └── syntax
            ├── events.vim
            └── types.vim

```

`AUTHORS`:

```
Qualcomm Innovation Center, Inc.

# The original Gunyah Hypervisor development team:
#  - Ali Pouladi
#  - Binglin Chen
#  - Carl van Schaik
#  - Jack Suann
#  - Philip Derrin
#  - Stephany Gamiz Perez

```

`CHANGELOG.md`:

```md
# Status and Changelog

This page documents current status, known issues and work in progress. Some of
these may impact your development or hypervisor usage.

## Open Issues

### 1. Secondary VM support

The Resource Manager being the root-VM, manages creation of the Primary VM
(HLOS) and controls the rights to create additional VMs. In the Gunyah Resource
Manager design, VM management services are provided by the Resource Manager

Gunyah patches are required in Linux and the CrosVM VMM to support SVM loading.

### Known issues:

- Only QEMU serial communication is tested. Using host Linux networking (qemu
  virtio) with adb (network) connection will permit greater flexibility in
  connecting to the device.
- SVM booting with Crosvm uses uart emulation, its very slow.
- Crosvm opens the UART console in the current terminal, so it is via the host
  uart terminal. We have not configured a way to open multiple terminals yet.
- Debugging a system running QEMU with a remote gdb connection is unstable.

### Untested scenarios:

- Launching of multiple SVM's simultaneously from PVM, because of the known
  issue of having only one console available.

### TODO list:

- Hardcoded platform parameters
    + Memory address ranges are hardcoded (get from dtb nodes)
    + Dtb address is hardcoded (get from register)

## Unreleased

Unreleased changes in the `develop` branch may be added here.

## Releases

Individual releases are tagged, and the latest release will be available in the `main` branch.

* No tagged releases have been made at this time.

## Contributions

Significant contributions are listed here.

### Initial Opensource Contribution

This is the initial contribution of source code to the Gunyah Hypervisor.

* Support for QEMU AArch64 Simulator
* Support unmodified Linux Primary VM kernel or with Gunyah patches for VM loading
* Support unmodified Linux Secondary VM kernel

```

`CODE-OF-CONDUCT.md`:

```md
# Contributor Covenant Code of Conduct

## Our Pledge

In the interest of fostering an open and welcoming environment, we as
contributors and maintainers pledge to making participation in our project and
our community a harassment-free experience for everyone, regardless of age, body
size, disability, ethnicity, gender identity and expression, level of experience,
nationality, personal appearance, race, religion, or sexual identity and
orientation.

## Our Standards

Examples of behavior that contributes to creating a positive environment
include:

* Using welcoming and inclusive language
* Being respectful of differing viewpoints and experiences
* Gracefully accepting constructive criticism
* Focusing on what is best for the community
* Showing empathy towards other community members

Examples of unacceptable behavior by participants include:

* The use of sexualized language or imagery and unwelcome sexual attention or
  advances
* Trolling, insulting/derogatory comments, and personal or political attacks
* Public or private harassment
* Publishing others' private information, such as a physical or electronic
  address, without explicit permission
* Other conduct which could reasonably be considered inappropriate in a
  professional setting

## Our Responsibilities

Project maintainers are responsible for clarifying the standards of acceptable
behavior and are expected to take appropriate and fair corrective action in
response to any instances of unacceptable behavior.

Project maintainers have the right and responsibility to remove, edit, or
reject comments, commits, code, wiki edits, issues, and other contributions
that are not aligned to this Code of Conduct, or to ban temporarily or
permanently any contributor for other behaviors that they deem inappropriate,
threatening, offensive, or harmful.

## Scope

This Code of Conduct applies both within project spaces and in public spaces
when an individual is representing the project or its community. Examples of
representing a project or community include using an official project e-mail
address, posting via an official social media account, or acting as an appointed
representative at an online or offline event. Representation of a project may be
further defined and clarified by project maintainers.

## Enforcement

Instances of abusive, harassing, or otherwise unacceptable behavior may be
reported by contacting the project team. All complaints will be reviewed
and investigated and will result in a response that is deemed necessary and
appropriate to the circumstances. The project team is obligated to maintain
confidentiality with regard to the reporter of an incident.
Further details of specific enforcement policies may be posted separately.

Project maintainers who do not follow or enforce the Code of Conduct in good
faith may face temporary or permanent repercussions as determined by other
members of the project's leadership.

## Attribution

This Code of Conduct is adapted from the [Contributor Covenant][homepage], version 1.4,
available at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html

[homepage]: https://www.contributor-covenant.org

```

`CONTRIBUTING.md`:

```md
# Contributing to Gunyah Hypervisor
Thank you for your interest in contributing to the Gunyah Hypervisor project!
Your support is essential for keeping this project great and for making it better.

- [Before you begin](#before-you-begin)
- [Guidelines](#guidelines)
- [Branching strategy](#branching-strategy)
- [Get source code](#get-the-source-code)
- [Development](#development)
  * [Build](#build)
  * [Test](#testing-on-qemu)
  * [Commit](#commit)
  * [Branch update](#branch-updates--rebasing)
  * [Branch cleanup](#branch-cleanup)
- [Submission](#submission)

## Before you begin
- Please read our [Code of Conduct](CODE-OF-CONDUCT.md) and [License](LICENSE) and ensure that you agree to abide by them.
- For every new feature or bug fix, always start a new issue on https://github.com/quic/gunyah-hypervisor/issues.
- To contribute a bug-fix, please follow the steps in the next sections without any further discussion.
- To contribute new features, extensions, utility functions or other significant changes, please describe and discuss the change with us via the GitHub issue that you created above.
  > *IMPORTANT:* **A pull request (PR) submitted without discussion and agreement with the project maintainers may be subject to rejection, or significant changes may be requested prior to its acceptance.**

## Guidelines
Please follow the guidelines below to increase the likelihood and speed of your PR acceptance:
- Follow the existing style in the file or folder where possible.
  * For Python we follow [pep8](https://www.python.org/dev/peps/pep-0008/)
  * For C, a similar style to [Linux kernel coding style](https://www.kernel.org/doc/html/v4.10/process/coding-style.html), but we do encourage to use brances even for single statements. A definitive LLVM 12 `.clang-format` file is provided and used by CI style checkers. Please apply clang-format to your files by running this command:
    ```
    git ls-files -- '*.[ch]' | xargs $LLVM/bin/clang-format --style=file -i
    ```
- Keep your change as focused as possible. If you want to make multiple independent changes, consider submitting them as separate PRs.
- Write a [good commit message](http://tbaggery.com/2008/04/19/a-note-about-git-commit-messages.html).
- Every commit **must be signed** with the [Developer Certificate of Origin](https://developercertificate.org) (by adding the `-s` option to your `git commit` command).
- Each PR submission will trigger a build, test, code quality check and static analysis processes. Submitters are required to fix all failures and warnings prior to acceptance.

## Branching strategy
Contributors should develop on [their own fork](https://help.github.com/en/github/getting-started-with-github/fork-a-repo) on branches based off of the `develop` branch and then pull requests should be made into the [upstream `develop` branch](https://github.com/quic/gunyah-hypervisor/tree/develop).

## Get the source code
Go to https://github.com/quic/gunyah-hypervisor and clone or fork the repo using [these instructions](https://help.github.com/en/github/getting-started-with-github/fork-a-repo).

[Sync your fork](https://help.github.com/en/github/collaborating-with-issues-and-pull-requests/syncing-a-fork) with the latest from the upstream repository.

Get the Gunyah Hypervisor code as follows:
```
git clone https://github.com/YOUR_USERNAME/gunyah-hypervisor.git
```
*IMPORTANT:* Setup your pre-commit and commit-msg hook using the following way:
```
cd gunyah-hypervisor
ln -s $(realpath -s .githooks/pre-commit) .git/hooks/pre-commit
ln -s $(realpath -s .githooks/commit-msg) .git/hooks/commit-msg
```

## Development
Start a new issue on https://github.com/quic/gunyah-hypervisor/issues.

Create a branch for your feature in your local checkout
```
cd gunyah-hypervisor
git checkout -b branch_short_feature_description
```

Now you may begin development. Once your development is complete, please ensure that the code builds successfully, that the clang-format has been applied and that the hypervisor boots and the primary VM starts successfully using the instructions in the next sections.

### Build
Follow these steps to build the code:

[Build instructions](docs/build.md)

### Testing on QEMU
To test that the hypervisor boots and the primary VM starts successfully use these instructions:

[Testing Instructions](docs/test.md)

### Commit
Commit the code and push it to your branch using the following procedure.

To display the files that you modified or added:
```
git status
```

To stage new (untracked) or existing files or folders for commit, do the following for each file or folder name that was added or changed:
```
git add <file or folder name that was added or changed>
```

To commit your changes *(using DCO sign-off as described above)*:
```
git commit -s
  <add interactive commit message>
```

To push your branch to your github fork (e.g. `origin`):
```
git push origin branch_short_feature_description
```

### Branch updates / rebasing
Before merging, it is recommended that you update your branch to the latest in branch `develop` using the following steps:
```
git fetch
git checkout develop
git pull origin develop
git checkout branch_short_feature_description
```
Rebase your changes:
```
git rebase develop
```
Fix any conflicts that may arise. Then complete the rebasing procedure as follows:
```
git status

# Run the next 2 commands ONLY IF you needed to fix any conflicts.
# Run this for each file that you changed
git add <file or folder name that was added or changed>
git rebase --continue
```
Re-build the code on your branch and run all tests. Then update your remote branch:
```
git push origin branch_short_feature_description --force-with-lease
```

### Branch cleanup
It is recommended that you commit code to your branches often. Prior to pushing the code and submitting PRs, please try to clean up your branch by squashing multiple commits together and amending commit messages as appropriate. Merge commits within PRs are not permitted.

See these pages for more details:
https://blog.carbonfive.com/2017/08/28/always-squash-and-rebase-your-git-commits
https://git-scm.com/book/en/v2/Git-Tools-Rewriting-History

## Submission
When you're ready to submit your code, issue a pull request from the branch on your FORK into the `develop` branch on the upstream repository using these [instructions](https://help.github.com/en/github/collaborating-with-issues-and-pull-requests/creating-a-pull-request-from-a-fork).
1. Go to your forked repo page `https://github.com/YOUR_USERNAME/gunyah-hypervisor` and click "New Pull Request".
1. Under "*compare changes*", select the base (destination) repository as `quic/gunyah-hypervisor` and the branch as `base:develop` to the left of the arrow.
1. Under "*compare changes*", select the head (source) repository as `YOUR_USERNAME/gunyah-hypervisor` and the branch as `base:branch_short_feature_description` to the right of the arrow.
1. Click "*Create Pull Request*" which will initiate the PR and take you to the PR page.
    - In the PR page, click *Reviewers* on the top left and select one. He/she will receive an email notification.
    - In the PR page, click *Assignee* on the top left and select one (optional). This person is usually the code submitter.
1. Wait for the outcome of the continuous integration (CI) build and test job, and for any review feedback from the project maintainers.

> NOTE: as described above, please obtains agreement from maintainers before submitting large changes or new features.

```

`LICENSE`:

```
© 2021 Qualcomm Innovation Center, Inc. All rights reserved.

Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:

1. Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.

2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.

3. Neither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

====================================================================================================================
Note: Individual files contain the following tag instead of the full license text.

SPDX-License-Identifier:    BSD-3-Clause

This enables machine processing of license information based on the SPDX License Identifiers that are available here: http://spdx.org/licenses/

```

`README.md`:

```md
[<img src="docs/images/logo-quic-on%40h68.png" height="68px" width="393px" alt="Qualcomm Innovation Center" align="right"/>](https://github.com/quic)

# Gunyah Hypervisor

Gunyah is a high performance, scalable and flexible hypervisor built for
demanding battery powered, real-time, safety and security use cases.

The Gunyah Hypervisor open source project provides a reference Type-1
hypervisor configuration suitable for general purpose hosting of multiple
trusted and dependent VMs.

## Gunyah Origins

*Gunyah* is an Australian Aboriginal word. See: https://en.wiktionary.org/wiki/gunyah

The Gunyah Hypervisor was developed by Qualcomm in Sydney Australia.

## Type-1 Hypervisor Concept

Gunyah is a Type-1 hypervisor, meaning that it runs independently of any
high-level OS kernel such as Linux and runs in a higher CPU privilege level
than VMs. It does not depend on any lower-privileged OS kernel/code for its
core functionality. This increases its security and can support a much smaller
trusted computing base than a Type-2 like hosted-hypervisors.

Gunyah's design principle is not dissimilar to a traditional microkernel in
that it provides only a minimal set of critical services to its clients, and
delegates the provision of non-critical services to non-privileged (or
less-privileged) processes, wherever this is possible without an adverse impact
on performance or security.

The hypervisor uses the CPU's virtualization mode and features to isolate
itself from OS kernels in VMs and isolate VMs from each other. On ArM, this
includes trapping and emulating registers as required, virtualizing core
platform devices, Arm's GIC virtualization support, and the CPU's Stage-2 MMU
to provide isolated VMs in EL1/0.

## Why Gunyah

- **Strong security**: Mobile payments, secure user-interface, and many more security sensitive use-cases all require strong security. Gunyah's design is suited to providing strong isolation guarantees and its small size is conducive to audit.
- **Performance**: Mobile devices are particularly demanding. Battery powered devices demand low software overheads to get the most performance per-watt. Gunyah is designed to have high performance with minimal impact to high-level operating systems.
- **Modularity**: The hypervisor is designed to be modular, allowing customization and enhancement by swapping out module implementations and adding new feature via new modules.

## Features

- **Threads and Scheduling**: The scheduler schedules virtual CPUs (VCPUs) on physical CPUs and enables time-sharing of the CPUs.
- **Memory Management**: Gunyah tracks memory ownership and use of all memory under its control. Memory partitioning between VMs is a fundamental security feature.
- **Interrupt Virtualization**: All interrupts are handled in the hypervisor and routed to the assigned VM.
- **Inter-VM Communication**: There are several different mechanisms provided for communicating between VMs.
- **Device Virtualization**: Para-virtualization of devices is supported using inter-VM communication. Low level system features and devices such as interrupt controllers are supported with emulation where required.

## Platform Support

Gunyah is architected to support multiple CPU architectures, so its core design
ensures architecture independence and portability in non-architecture specific
areas.

Gunyah currently supports the ARM64 (ARMv8+) architecure, it uses AArch64 EL2
in VHE mode by default.

We have developed an initial port of Gunyah to the QEMU Arm System emulator.
*Note QEMU v7+ is recommended*. Additional platforms are expected to be
supported in future contributions.

## Getting Started
- [Terminology](docs/terminology.md)
- Other required Gunyah related repositories:
    - [Setup Tools and Scripts](https://github.com/quic/gunyah-support-scripts) (Start setup here..!!)
    - [Hypervisor](https://github.com/quic/gunyah-hypervisor.git) (Gunyah core hypervisor, this repository)
    - [Resource Manager](https://github.com/quic/gunyah-resource-manager.git) (Platform policy engine)
    - [C Runtime](https://github.com/quic/gunyah-c-runtime.git) (C runtime environment for Resource Manager)

- [Setup Instructions](docs/setup.md)
    + [Quick Start Instructions](https://github.com/quic/gunyah-support-scripts/blob/develop/quickstart.md) (will take to the setup tools and scripts repository)
- [Build Instructions](docs/build.md)
- [Status and Changelog](CHANGELOG.md)

## Resources
- [Gunyah Hypercall API](docs/api/gunyah_api.md)

## Contributions
Thank you for your interest in contributing to Gunyah!

Please read our [Contributions Page](CONTRIBUTING.md) for more information on contributing features or bug fixes.

## Team
Gunyah was developed by Qualcomm and aims to be an open and community supported project.

Check out the [AUTHORS](AUTHORS) for major contributors.

## License
Gunyah is licensed on the BSD 3-clause "New" or "Revised" License.  Check out the [LICENSE](LICENSE) for more details.

```

`SConstruct`:

```
# coding: utf-8
#
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

import SCons.Script
import configure
import os

env_vars = {
    'PATH': os.environ['PATH'],
}

if 'QCOM_LLVM' in os.environ:
    env_vars['QCOM_LLVM'] = os.environ['QCOM_LLVM']

if 'LLVM' in os.environ:
    env_vars['LLVM'] = os.environ['LLVM']

env = Environment(tools={}, SCANNERS=[], BUILDERS={}, ENV=env_vars)
configure.SConsBuild(env, Builder, Action, arguments=SCons.Script.ARGUMENTS)()

```

`config/arch/aarch64.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

is_abi
# Use the Linux target because it knows how to link with LLD
target_triple aarch64-linux-gnu
defines_link
flags -mgeneral-regs-only -mtp=el2
configs ARCH_IS_64BIT=1 ARCH_ENDIAN_LITTLE=1

```

`config/arch/armv8-64.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

base_arch armv8
base_arch aarch64

```

`config/arch/armv8.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

configs ARCH_ARM=1

```

`config/arch/cortex-a-v8_0.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

base_arch armv8-64

configs ARCH_AARCH64_32BIT_EL0=1
configs ARCH_AARCH64_32BIT_EL0_ALL_CORES=1
# FIXME
configs ARCH_AARCH64_32BIT_EL1=0

# Mandatory architecture extensions in v8.0
configs ARCH_ARM_PMU_VER=3

# The number of implemented ICH_LR<n>_EL2 registers.
configs CPU_GICH_LR_COUNT=4U

# The number of implemented ICH_APR[01]R<n>_EL2 registers.
configs CPU_GICH_APR_COUNT=1U

# The number of implemented DBGB[CV]R_EL1 (HW breakpoint) registers.
configs CPU_DEBUG_BP_COUNT=6U

# The number of implemented DBGW[CV]R_EL1 (HW watchpoint) registers.
configs CPU_DEBUG_WP_COUNT=4U

# These CPUs always have an ETM.
configs PLATFORM_HAS_NO_ETM_BASE=0

```

`config/arch/cortex-a-v8_2.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

base_arch armv8-64

configs ARCH_AARCH64_32BIT_EL0=1
configs ARCH_AARCH64_32BIT_EL0_ALL_CORES=1
configs ARCH_AARCH64_32BIT_EL1=0

# Checked for Cortex-(A55,A65,A75,A76,A77,A78,X1)
configs CPU_HAS_NO_ACTLR_EL1=1
configs CPU_HAS_NO_AMAIR_EL1=1
configs CPU_HAS_NO_AFSR0_EL1=1
configs CPU_HAS_NO_AFSR1_EL1=1

# Mandatory architecture extensions in v8.2
configs ARCH_ARM_FEAT_CSV2=1
configs ARCH_ARM_FEAT_CSV3=1

configs ARCH_ARM_FEAT_HPDS=1
configs ARCH_ARM_FEAT_LSE=1
configs ARCH_ARM_FEAT_LOR=1
configs ARCH_ARM_FEAT_PAN=1
configs ARCH_ARM_FEAT_RDM=1
configs ARCH_ARM_FEAT_VHE=1
configs ARCH_ARM_FEAT_CRC32=1

configs ARCH_ARM_FEAT_ASMv8p2=1
configs ARCH_ARM_FEAT_PAN2=1
configs ARCH_ARM_FEAT_DPB=1
configs ARCH_ARM_FEAT_DEBUGv8p2=1
configs ARCH_ARM_FEAT_DotProd=1
configs ARCH_ARM_FEAT_RAS=1
configs ARCH_ARM_FEAT_TTCNP=1
configs ARCH_ARM_FEAT_XNX=1
configs ARCH_ARM_FEAT_UAO=1

configs ARCH_AARCH64_ASID16=1 ARCH_ARM_PMU_VER=3

# The number of implemented ICH_LR<n>_EL2 registers.
configs CPU_GICH_LR_COUNT=4U

# The number of implemented ICH_APR[01]R<n>_EL2 registers.
configs CPU_GICH_APR_COUNT=1U

# The number of implemented DBGB[CV]R_EL1 (HW breakpoint) registers.
configs CPU_DEBUG_BP_COUNT=6U

# The number of implemented DBGW[CV]R_EL1 (HW watchpoint) registers.
configs CPU_DEBUG_WP_COUNT=4U

# The level of support for ARMv8.4-TTRem on this CPU (encoded the same way
# as ID_AA64MMFR2_EL1.BBM).
configs CPU_PGTABLE_BBM_LEVEL=0U

# These CPUs always have an ETM.
configs PLATFORM_HAS_NO_ETM_BASE=0

```

`config/arch/cortex-a53.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

base_arch cortex-a-v8_0
flags -mcpu=cortex-a53

configs ARCH_CORE_IDS=CORTEX_A53

configs CPU_PGTABLE_BBM_LEVEL=0U

configs CPU_HAS_NO_ACTLR_EL1=1
configs CPU_HAS_NO_AMAIR_EL1=1
configs CPU_HAS_NO_AFSR0_EL1=1
configs CPU_HAS_NO_AFSR1_EL1=1

# Fixed fields in cortex-a53
configs PLATFORM_MPIDR_AFF3_MASK=0
configs PLATFORM_MPIDR_AFF3_SHIFT=0

```

`config/arch/cortex-a55-a76.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

base_arch cortex-a-v8_2
flags -mcpu=cortex-a76
configs ARCH_CORE_IDS=CORTEX_A55,CORTEX_A76

configs ARCH_AARCH64_BIG_END_ALL_CORES=1

configs ARCH_ARM_PMU_HPMN_UNPREDICTABLE=1

```

`config/arch/cortex-a55-a77.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

base_arch cortex-a-v8_2
flags -mcpu=cortex-a77
configs ARCH_CORE_IDS=CORTEX_A55,CORTEX_A77

configs ARCH_AARCH64_BIG_END_ALL_CORES=1

configs ARCH_ARM_FEAT_VMID16=1
configs ARCH_ARM_PMU_HPMN_UNPREDICTABLE=1

configs ARCH_ARM_FEAT_PMUv3p1=1
configs ARCH_ARM_FEAT_IESB=1
configs ARCH_ARM_FEAT_HPDS2=1

```

`config/arch/cortex-a55-a78-x1.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

base_arch cortex-a-v8_2
flags -mcpu=cortex-x1

configs ARCH_AARCH64_BIG_END_ALL_CORES=1

configs ARCH_CORE_IDS=CORTEX_A55,CORTEX_A78,CORTEX_X1
configs ARCH_ARM_FEAT_VMID16=1
configs ARCH_ARM_PMU_HPMN_UNPREDICTABLE=1

configs ARCH_ARM_FEAT_PMUv3p1=1
configs ARCH_ARM_FEAT_IESB=1
configs ARCH_ARM_FEAT_HPDS2=1

```

`config/arch/cortex-a55.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

base_arch cortex-a-v8_2
flags -mcpu=cortex-a55
configs ARCH_CORE_IDS=CORTEX_A55
configs ARCH_ARM_FEAT_VMID16=1
configs ARCH_ARM_PMU_HPMN_UNPREDICTABLE=1

configs ARCH_ARM_FEAT_PMUv3p1=1
configs ARCH_ARM_FEAT_IESB=1
configs ARCH_ARM_FEAT_HPDS2=1

```

`config/arch/cortex-a78.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

base_arch cortex-a-v8_2
flags -mcpu=cortex-a78
configs ARCH_CORE_IDS=CORTEX_A78
configs ARCH_ARM_FEAT_VMID16=1
configs ARCH_ARM_PMU_HPMN_UNPREDICTABLE=1

configs ARCH_ARM_FEAT_PMUv3p1=1
configs ARCH_ARM_FEAT_IESB=1
configs ARCH_ARM_FEAT_HPDS2=1

```

`config/arch/gic-500.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

configs GICV3_EXT_IRQS=0
# There may be an ITS, but it is not useful to the hypervisor without vLPIs
configs GICV3_HAS_ITS=0
configs GICV3_HAS_LPI=0
configs GICV3_HAS_VLPI=0
configs GICV3_HAS_VLPI_V4_1=0
configs GICV3_HAS_1N=1
configs GICV3_HAS_GICD_ICLAR=0
configs GICV3_HAS_SECURITY_DISABLED=0
configs PLATFORM_GICD_BASE=PLATFORM_GIC_BASE
configs PLATFORM_GICR_SIZE=(0x20000*PLATFORM_MAX_CORES)
configs PLATFORM_IDLE_WAKEUP_TIMEOUT_NS=5000

```

`config/arch/gic-600.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

configs GICV3_EXT_IRQS=0
# There may be an ITS, but it is not useful to the hypervisor without vLPIs
configs GICV3_HAS_ITS=0
configs GICV3_HAS_LPI=0
configs GICV3_HAS_VLPI=0
configs GICV3_HAS_VLPI_V4_1=0
configs GICV3_HAS_1N=1
configs GICV3_HAS_GICD_ICLAR=1
configs GICV3_HAS_SECURITY_DISABLED=0
configs PLATFORM_GICD_BASE=PLATFORM_GIC_BASE
configs PLATFORM_GICA_BASE=(PLATFORM_GIC_BASE+0x10000U)
configs PLATFORM_GITS_BASE=(PLATFORM_GIC_BASE+0x40000U)
configs PLATFORM_GITS_SIZE=(0x20000U*PLATFORM_GITS_COUNT)
configs PLATFORM_GICR_BASE=(PLATFORM_GITS_BASE+PLATFORM_GITS_SIZE)
configs PLATFORM_GICR_SIZE=(0x20000U*PLATFORM_GICR_COUNT)
configs PLATFORM_GIC_SIZE=(0x50000U+PLATFORM_GITS_SIZE+PLATFORM_GICR_SIZE)
configs PLATFORM_IDLE_WAKEUP_TIMEOUT_NS=5000

```

`config/arch/gic-700-vlpi.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

configs GICV3_HAS_ITS=1
configs GICV3_HAS_LPI=1
configs GICV3_HAS_VLPI=1
configs GICV3_HAS_VLPI_V4_1=1
configs GICV3_HAS_1N=1
configs GICV3_HAS_GICD_ICLAR=1
configs GICV3_HAS_SECURITY_DISABLED=0
configs PLATFORM_GICD_BASE=PLATFORM_GIC_BASE
configs PLATFORM_GICA_BASE=(PLATFORM_GIC_BASE+0x10000U)
configs PLATFORM_GITS_BASE=(PLATFORM_GIC_BASE+0x40000U)
configs PLATFORM_GITS_SIZE=(0x40000U*PLATFORM_GITS_COUNT)
configs PLATFORM_GICR_BASE=(PLATFORM_GITS_BASE+PLATFORM_GITS_SIZE)
configs PLATFORM_GICR_SIZE=(0x40000U*PLATFORM_GICR_COUNT)
configs PLATFORM_GIC_SIZE=(0x50000U+PLATFORM_GITS_SIZE+PLATFORM_GICR_SIZE)
configs PLATFORM_IDLE_WAKEUP_TIMEOUT_NS=5000

```

`config/arch/gic-qemu.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

configs GICV3_EXT_IRQS=0
# There may be an ITS, but it is not useful to the hypervisor without vLPIs
configs GICV3_HAS_ITS=0
configs GICV3_HAS_LPI=0
configs GICV3_HAS_VLPI=0
configs GICV3_HAS_VLPI_V4_1=0
configs GICV3_HAS_1N=0
configs GICV3_HAS_GICD_ICLAR=0
configs GICV3_HAS_SECURITY_DISABLED=1
configs PLATFORM_GICD_BASE=PLATFORM_GIC_BASE
configs PLATFORM_GITS_BASE=(PLATFORM_GIC_BASE+0x80000U)
configs PLATFORM_GITS_SIZE=(0x20000U*PLATFORM_GITS_COUNT)
configs PLATFORM_GICR_BASE=(PLATFORM_GITS_BASE+PLATFORM_GITS_SIZE)
configs PLATFORM_GICR_SIZE=(0x20000U*PLATFORM_MAX_CORES)
configs PLATFORM_GIC_SIZE=(0x50000U+PLATFORM_GITS_SIZE+PLATFORM_GICR_SIZE)
configs PLATFORM_IDLE_WAKEUP_TIMEOUT_NS=0

```

`config/arch/qemu-armv8-5a-rng.conf`:

```conf
# © 2022 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

base_arch armv8-64
flags -march=armv8.5-a+rng

configs PLATFORM_MAX_CORES=8U
configs PLATFORM_USABLE_CORES=0xFFU

configs PLATFORM_MPIDR_AFF0_MASK=0x7U
configs PLATFORM_MPIDR_AFF0_SHIFT=0
configs PLATFORM_MPIDR_AFF1_MASK=0U
configs PLATFORM_MPIDR_AFF1_SHIFT=0
configs PLATFORM_MPIDR_AFF2_MASK=0U
configs PLATFORM_MPIDR_AFF2_SHIFT=0
configs PLATFORM_MPIDR_AFF3_MASK=0U
configs PLATFORM_MPIDR_AFF3_SHIFT=0

configs ARCH_CORE_IDS=QEMU

configs ARCH_ARM_FEAT_AES=1
configs ARCH_ARM_FEAT_PMULL=1
configs ARCH_ARM_FEAT_SHA1=1
configs ARCH_ARM_FEAT_RNG=1

configs ARCH_AARCH64_BIG_END_ALL_CORES=1
configs ARCH_AARCH64_32BIT_EL0=1
configs ARCH_AARCH64_32BIT_EL0_ALL_CORES=1
configs ARCH_AARCH64_32BIT_EL1=0

configs ARCH_ARM_FEAT_VMID16=1
configs ARCH_ARM_PMU_HPMN_UNPREDICTABLE=1

configs ARCH_ARM_FEAT_PMUv3p1=1
configs ARCH_ARM_FEAT_IESB=1
configs ARCH_ARM_FEAT_HPDS2=1
# Assume sve128=on
configs ARCH_ARM_FEAT_SVE=1
configs PLATFORM_SVE_REG_SIZE=16U

configs ARCH_ARM_FEAT_CSV2=1
configs ARCH_ARM_FEAT_CSV3=1

configs ARCH_ARM_FEAT_HPDS=1
configs ARCH_ARM_FEAT_LSE=1
configs ARCH_ARM_FEAT_LOR=1
configs ARCH_ARM_FEAT_PAN=1
configs ARCH_ARM_FEAT_RDM=1
configs ARCH_ARM_FEAT_VHE=1
configs ARCH_ARM_FEAT_CRC32=1

configs ARCH_ARM_FEAT_ASMv8p2=1
configs ARCH_ARM_FEAT_PAN2=1
configs ARCH_ARM_FEAT_DPB=1
configs ARCH_ARM_FEAT_DEBUGv8p2=1
configs ARCH_ARM_FEAT_DotProd=1
configs ARCH_ARM_FEAT_RAS=1
configs ARCH_ARM_FEAT_TTCNP=1
configs ARCH_ARM_FEAT_XNX=1
configs ARCH_ARM_FEAT_UAO=1

configs ARCH_AARCH64_ASID16=1 ARCH_ARM_PMU_VER=3

# The number of implemented ICH_LR<n>_EL2 registers.
configs CPU_GICH_LR_COUNT=4U

# The number of implemented ICH_APR[01]R<n>_EL2 registers.
configs CPU_GICH_APR_COUNT=1U

# The number of implemented DBGB[CV]R_EL1 (HW breakpoint) registers.
configs CPU_DEBUG_BP_COUNT=6U

# The number of implemented DBGW[CV]R_EL1 (HW watchpoint) registers.
configs CPU_DEBUG_WP_COUNT=4U

# The level of support for ARMv8.4-TTRem on this CPU (encoded the same way
# as ID_AA64MMFR2_EL1.BBM).
configs CPU_PGTABLE_BBM_LEVEL=0U

```

`config/featureset/gunyah-rm-qemu.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

configs HYP_CONF_STR=gunyah
platforms qemu

module core/api
module core/base
module core/boot
module core/util
module misc/abort
module core/object_standard
module core/thread_standard
module core/idle
module core/scheduler_fprr
module core/partition_standard
module core/preempt
module core/cpulocal
module core/spinlock_ticket
module core/mutex_trivial
module core/rcu_bitmap
module core/cspace_twolevel
module core/vdevice
module core/ipi
module core/irq
module core/task_queue
module core/timer
module core/power
module core/wait_queue_broadcast
module core/globals
module debug/object_lists
module debug/symbol_version
module ipc/doorbell
module ipc/msgqueue
module mem/allocator_list
module mem/allocator_boot
module mem/memdb_gpt
module mem/hyp_aspace
module mem/pgtable
module mem/addrspace
module mem/memextent_sparse
module misc/elf
module misc/prng_hw
module misc/prng_simple
module misc/trace_standard
module misc/smc_trace
module misc/log_standard
module misc/qcbor
module misc/root_env
module platform/arm_generic
module platform/arm_smccc
module platform/arm_trng_fi
arch_module aarch64 misc/spectre_arm
module vm/rootvm
module vm/rootvm_package
module vm/slat
module vm/vcpu
module vm/vcpu_power
module vm/vcpu_run
module vm/virtio_mmio
module vm/virtio_input
module vm/vrtc_pl031
arch_module armv8 vm/smccc
arch_module armv8 vm/psci_pc
arch_module armv8 vm/vdebug
arch_module armv8 vm/vgic
arch_module armv8 vm/arm_vm_timer
arch_module armv8 vm/arm_vm_pmu
arch_module armv8 vm/arm_vm_sve_simple

```

`config/featureset/unittests-qemu.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

configs HYP_CONF_STR=unittest UNITTESTS=1
configs UNIT_TESTS=1
platforms qemu

module core/api
module core/base
module core/boot
module core/util
module misc/abort
module core/object_standard
module core/thread_standard
module core/idle
module core/scheduler_fprr
module core/partition_standard
module core/preempt
module core/cpulocal
module core/spinlock_ticket
module core/mutex_trivial
module core/rcu_bitmap
module core/cspace_twolevel
module core/tests
module core/vectors
module core/debug
module core/ipi
module core/irq
module core/virq_null
module core/timer
module core/power
module core/globals
module debug/object_lists
module debug/symbol_version
module mem/allocator_list
configs ALLOCATOR_DEBUG=1
module mem/allocator_boot
module mem/memdb_gpt
module mem/hyp_aspace
module mem/pgtable
module mem/addrspace
module mem/memextent_sparse
module misc/elf
module misc/gpt
module misc/prng_simple
module misc/trace_standard
module misc/log_standard
module misc/smc_trace
module misc/qcbor
arch_module aarch64 misc/spectre_arm
module platform/arm_generic
module platform/arm_smccc
module vm/slat
configs POWER_START_ALL_CORES=1

```

`config/include/debug_no_cspace_rand.conf`:

```conf
# © 2023 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

configs DISABLE_CSPACE_RAND=1

```

`config/include/debug_no_kaslr.conf`:

```conf
# © 2023 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

configs DISABLE_KASLR=1

```

`config/include/debug_no_rootvm_aslr.conf`:

```conf
# © 2023 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

configs DISABLE_ROOTVM_ASLR=1

```

`config/platform/qemu.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

configs PLATFORM_QEMU=1
configs PLATFORM_HAS_NO_ETM_BASE=1

base_arch qemu-armv8-5a-rng
base_arch gic-qemu
module platform/soc_qemu
module platform/arm_arch_timer
module platform/arm_pmu
module platform/arm_rng

# PLATFORM_LMA_BASE is used in linker script
# this is used to link Hyp image to this address
configs PLATFORM_LMA_BASE=0x80000000

# These define what is the installed memory range of
# the platform hardware (memsize while invoking qemu)
configs PLATFORM_DDR_BASE=0x40000000U
configs PLATFORM_DDR_SIZE=0x80000000U

# These define the amount of memory given to the HLOS
# which is defined in the DT loaded for linux
configs HLOS_VM_DDR_BASE=0x40000000U
configs HLOS_VM_DDR_SIZE=0x40000000U

# Address locations where Linux kernel, DT and RAMFS are
# loaded when Qemu is started
configs HLOS_ENTRY_POINT=0x41000000
configs HLOS_DT_BASE=0x40F00000
configs HLOS_RAM_FS_BASE=0x40800000

configs PLATFORM_HEAP_PRIVATE_SIZE=0x200000
configs PLATFORM_RW_DATA_SIZE=0x200000
configs PLATFORM_ROOTVM_LMA_BASE=0x80480000U
configs PLATFORM_ROOTVM_LMA_SIZE=0xa0000U
configs PLATFORM_PHYS_ADDRESS_BITS=36
configs PLATFORM_VM_ADDRESS_SPACE_BITS=36
configs PLATFORM_PGTABLE_4K_GRANULE=1
configs PLATFORM_ARCH_TIMER_FREQ=62500000
configs PLATFORM_HYP_ARCH_TIMER_IRQ=26
configs PLATFORM_VM_ARCH_VIRTUAL_TIMER_IRQ=27U
configs PLATFORM_VM_ARCH_PHYSICAL_TIMER_IRQ=30
configs PLATFORM_GIC_BASE=0x08000000U
configs PLATFORM_GICR_COUNT=PLATFORM_MAX_CORES
configs PLATFORM_MAX_CLUSTERS=1U
configs PLATFORM_MAX_HIERARCHY=1U
configs PLATFORM_GITS_COUNT=1
configs PLATFORM_GICH_IRQ=25
configs PLATFORM_PMU_IRQ=23
configs PLATFORM_VM_PMU_IRQ=23
configs PLATFORM_PMU_CNT_NUM=4
# We currently do not have a wdog QEMU platform implementation
configs WATCHDOG_DISABLE=1
# QEMU does not use affinity levels and uses original powerstate format
configs PLATFORM_PSCI_USE_ORIGINAL_POWERSTATE_FORMAT=1
configs PSCI_AFFINITY_LEVELS_NOT_SUPPORTED=1
configs PLATFORM_HAS_NO_DBGCLAIM_EL1=1
# QEMU supports version 0.2, which does not have set_suspend_mode call
configs PSCI_SET_SUSPEND_MODE_NOT_SUPPORTED=1

configs QCBOR_ENV_CONFIG_SIZE=0x4000

```

`config/quality/debug.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

configs VERBOSE=1
configs VERBOSE_TRACE=1
configs RESET_ON_ABORT=0
configs QUALITY=debug
flags -O1 -g -mstrict-align

include include/debug_no_kaslr
include include/debug_no_cspace_rand
include include/debug_no_rootvm_aslr

arch_configs aarch64 ARCH_SANITIZE_STACK_SIZE=1536U
arch_configs aarch64 VCPU_MIN_STACK_SIZE=8192U

# The trace entry numbers include the header
arch_configs qemu TRACE_BOOT_ENTRIES=128 PER_CPU_TRACE_ENTRIES=4096 TRACE_FORMAT=1
arch_configs qemu TRACE_AREA_SIZE=0x2000000 EXTRA_PRIVATE_HEAP_SIZE=0x100000 EXTRA_ROOT_HEAP_SIZE=0x300000

```

`config/quality/production-unchecked.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

configs NDEBUG=1
configs RESET_ON_ABORT=1
configs QUALITY=perf
flags -flto -O3 -g

# This is slightly overkill for LTO builds, but safe.
arch_configs aarch64 ARCH_SANITIZE_STACK_SIZE=1024

# The trace entry numbers include the header
arch_configs qemu TRACE_BOOT_ENTRIES=128 PER_CPU_TRACE_ENTRIES=4096 TRACE_FORMAT=1
arch_configs qemu TRACE_AREA_SIZE=0x2000000U EXTRA_PRIVATE_HEAP_SIZE=0x100000U EXTRA_ROOT_HEAP_SIZE=0x300000U

```

`config/quality/production.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

# Uncomment for additional traces
# configs VERBOSE_TRACE=1
configs RESET_ON_ABORT=1
configs QUALITY=prod
flags -flto -O3 -g

arch_configs aarch64 ARCH_SANITIZE_STACK_SIZE=1024

# The trace entry numbers include the header
arch_configs qemu TRACE_BOOT_ENTRIES=128 PER_CPU_TRACE_ENTRIES=4096 TRACE_FORMAT=1
arch_configs qemu TRACE_AREA_SIZE=0x2000000U EXTRA_PRIVATE_HEAP_SIZE=0x100000U EXTRA_ROOT_HEAP_SIZE=0x300000U

```

`configure.py`:

```py
#!/usr/bin/env python3
# coding: utf-8
#
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

"""
Top-level configuration file for Gunyah build system.

This module constructs an instance of AbstractBuildGraph, and passes it to the
real build system which is in tools/build.

The AbstractBuildGraph class provides an interface which can be used to
declare a build graph consisting of template rules, targets which are build
using those rules, and variables that are substituted into rule commands and
subsequent variable definitions. Implementations of this interface are
provided for Ninja and SCons.

If run as a standalone script, this module generates a Ninja rules file. If
called from a SConstruct or SConscript, it sets up a SCons build that has the
same semantics as the Ninja build. The SConstruct or SConscript will typically
contain the following code:

    import configure
    env = Environment(tools={}, SCANNERS=[], BUILDERS={}, ENV={...})
    configure.SConsBuild(env, Builder, Action, arguments=ARGUMENTS)(...)
"""


import os
import sys
import abc
import re
import runpy
import json


class ClangCompDB(object):

    def __init__(self, path, var_subst):
        self.path = path
        self.var_subst = var_subst
        self.commands = []

    def add_command(self, command, i, o, **local_env):
        self.commands.append({
            'directory': os.getcwd(),
            'command': command,
            'file': i,
            'output': o,
            'local_env': dict(local_env),
        })

    def finalise(self):
        for c in self.commands:
            c['command'] = self.var_subst(c['command'], **c['local_env'])
            del c['local_env']
        d = os.path.dirname(self.path)
        if d:
            try:
                os.makedirs(d)
            except OSError as e:
                import errno
                if e.errno != errno.EEXIST:
                    raise
        with open(self.path, 'w') as f:
            json.dump(self.commands, f)


class AbstractBuildGraph(object):

    __metaclass__ = abc.ABCMeta

    def __init__(self, _parent=None, arguments=None, build_dir=None):
        self._variants = []
        if _parent is None:
            self._is_variant = False
            self._arguments = {} if arguments is None else dict(arguments)
            self._build_dir = 'build' if build_dir is None else build_dir
            self._env = {}
            self._compdbs = {}
            self._rule_compdbs = {}
            self._rule_commands = {}
            self._rule_byproducts = {}
            self.add_gen_source(__file__)
        else:
            self._is_variant = True
            assert arguments is None
            self._arguments = _parent._arguments
            assert build_dir is not None
            self._build_dir = build_dir
            # Make local copies of the parent's environment and rules.
            self._env = dict(_parent._env)
            self._compdbs = dict(_parent._compdbs)
            self._rule_compdbs = dict(_parent._rule_compdbs)
            self._rule_commands = dict(_parent._rule_commands)
            self._rule_byproducts = dict(_parent._rule_byproducts)

    def __call__(self, **kwargs):
        for k in kwargs.keys():
            self.add_env(k, kwargs[k], replace=True)

        if not self._variants:
            try:
                runpy.run_module("tools.build", init_globals={'graph': self})
            except SystemExit as e:
                if e.code:
                    raise

        for c in self._compdbs.values():
            c.finalise()

    def get_argument(self, key, default=None):
        """
        Return the value of a command-line argument, or the specified default.
        """
        return self._arguments.get(key, default)

    @abc.abstractproperty
    def root_dir(self):
        """
        The root directory from which build commands are run.

        This is either absolute, relative to the root directory of the
        repository, or empty. It should be useda as the start= argument of an
        os.path.relpath() call for any path that is specified on a command
        line outside of the ${in} or ${out} variables (e.g. include search
        directories).
        """
        raise NotImplementedError

    @property
    def build_dir(self):
        """
        The base directory that should be used for build outputs.

        This is always a path relative to the working directory.
        """
        return self._build_dir

    def add_variant(self, build_dir):
        """
        Create a variant build, and return the build object.

        This may be called once or more before calling the object itself. If it
        is called at least once before calling the object, the parent build
        will not generate any build rules of its own; instead it will be
        configured to only execute its own variants. If the build generator
        calls this itself, it is responsible for making the distinction
        between variant builds and the top level.

        The specified directory is used as the build directory for the
        variant.
        """
        variant = self._make_variant(build_dir)
        self._variants.append(variant)
        return variant

    @abc.abstractmethod
    def _make_variant(self, build_dir):
        # Create a new build object for a variant based on this one.
        #
        # This is the default implementation, but it is likely to need to be
        # overridden, so subclasses must explicitly call the default through
        # super if they want to use it.
        return type(self)(build_dir=build_dir, _parent=self)

    @abc.abstractmethod
    def add_env(self, name, value, replace=False):
        """
        Add an environment variable.

        The value will be automatically substituted in future value arguments
        to add_env() and command arguments to add_target(), if named in those
        arguments in sh style, i.e. $name or ${name}.

        If the optional replace argument is true, then replacing an existing
        variable is allowed; otherwise it will raise KeyError.
        """
        self._env[name] = self._var_subst(value)

    @abc.abstractmethod
    def append_env(self, name, value, separator=' '):
        """
        Append to an environment variable.

        This is like add_env(), except that if the variable is already set,
        the given value will be appended to it. By default the values are
        separated by spaces, but the optional separator argument can be used
        to replace this.
        """
        if name in self._env:
            self._env[name] += separator
        else:
            self._env[name] = ''
        self._env[name] += self._var_subst(value)

    @abc.abstractmethod
    def get_env(self, name):
        """
        Fetch an environment variable.

        This will return the value of the named environment variable, which
        may have been either set by add_env() or append_env(), or else passed
        to the build system from the external environment.

        If the named value is unknown, this method throws KeyError.
        """
        return self._env[name]

    @abc.abstractmethod
    def add_rule(self, name, command, depfile=None, depfile_external=False,
                 compdbs=None, restat=False):
        """
        Add a build rule.

        The rule name must be unique, and must be a valid Python identifier
        that does not begin with an underscore.

        The command will be run to build targets that use this rule. The
        target name will be substituted for $out or ${out}, and the space
        separated input names will be substituted for $in or ${in}.

        If depfile is set, then it is assumed to be the name of a
        Makefile-style dependency file produced as a side-effect of running
        the command, and will be read (if it exists) to detect implicit
        dependencies (included headers, etc). The target name will be
        substituted for $out or ${out} in this name.

        If the depfile is not generated by the commands in the rule it self,
        then depfile_external should be set to true, otherwise the depfile will
        be added to the list of byproducts.

        If compdbs is set to a list of targets, then targets using this rule
        will be added to the compilation databases represented by those
        targets. The targets must be compilation databases created by calling
        add_compdb().
        """
        compdbs = self._expand_target_list(compdbs)
        rule_compdbs = []
        for c in compdbs:
            if c not in self._compdbs:
                raise KeyError("Not a compdb target: {:s}".format(c))
            rule_compdbs.append(self._compdbs[c])
        if rule_compdbs:
            self._rule_compdbs[name] = tuple(rule_compdbs)
        self._rule_commands[name] = command
        if depfile and not depfile_external:
            self._rule_byproducts[name] = depfile

    @abc.abstractmethod
    def add_target(self, targets, rule, sources=None, depends=None,
                   requires=None, byproducts=None, always=False, **local_env):
        """
        Build one or more targets using a previously created build rule.

        The named rule must be one that has previously been set up with
        add_rule(). That rule's command will be invoked with the given target
        and sources.

        The targets, sources, depends, requires and byproducts arguments are
        all lists of file paths relative to the top level build directory. The
        depends and requires lists may contain names of alias targets, created
        with the add_alias() method; otherwise, all elements of these lists
        must be regular files that either exist in the source tree or will be
        created during the build. If any of these arguments is specified as a
        string, it is treated as a whitespace-separated list.

        The targets are files created by the rule. These must be regular
        files; directories are not allowed. If any target is in a directory
        other than the top-level build directory, then that directory will be
        automatically created before the rule is run. When the rule is run,
        the list of targets will be substituted for the ${out} variable in the
        rule command.

        The listed sources are added as explicit input dependencies and are
        substituted for the ${in} variable in the rule command. Like the
        targets, this may be either a single list

        If a depends list is provided, it specifies additional implicit
        dependencies of this target. These behave the same as sources, except
        that they are not included in the substitution of ${in}.

        If a requires list is provided, it specifies order-only dependencies
        of this target. These are dependencies that are not named on the
        command line, and will not trigger a rebuild if they are newer than
        one of the targets.

        If a byproducts list is provided, it specifies additional products of
        compilation that are generated along with the primary target. These
        behave the same as targets, except that they are not included in the
        substitution of ${out}.

        If the "always" keyword is set to True, the target will be rebuilt
        every time it is used as a dependency.

        Any other keyword arguments are added temporarily to the environment
        while building this specific target, overriding any variables
        currently in the environment. Variable expansion is performed on the
        values in this dictionary. Variables may be appended by expanding
        their previous value in the new value. However, do not locally
        override a variable if its value is substituted in local overrides of
        _other_ variables; the effect of doing so is unspecified, and may vary
        between runs of an otherwise unchanged build.
        """
        sources = self._expand_target_list(sources)
        targets = self._expand_target_list(targets)
        local_env = {
            name: self._var_subst(value) for name, value in local_env.items()
        }
        local_env['in'] = ' '.join(getattr(n, 'abspath', str(n))
                                   for n in sources)
        local_env['out'] = ' '.join(getattr(n, 'abspath', str(n))
                                    for n in targets)
        cmd = self._rule_commands[rule]
        for compdb in self._rule_compdbs.get(rule, ()):
            for s in sources:
                compdb.add_command(cmd, s, targets[0], **local_env)

    @abc.abstractmethod
    def add_alias(self, alias, targets):
        """
        Add an alias (phony) target.

        This method creates a target that does not correspond to a file in the
        build directory, with dependencies on a specific list of other
        targets. It may be used to create aliases like "all", "install", etc.,
        which may then be named on the command line, as default targets, or as
        dependencies, like any other target.

        However, due to a misfeature in SCons, if you need to name an alias in
        a dependency list before defining it, you must wrap the alias's name
        with a call to the future_alias() method.
        """
        raise NotImplementedError

    def future_alias(self, alias):
        """
        Get a reference to an alias that may not have been defined yet.

        If it is necessary to name an alias in a dependency list prior to
        defining it, you must pass the name of the alias to this method and
        add the result to the dependency list. This is because SCons can't
        retroactively change a dependency from a file (the default) to an
        alias.
        """
        return alias

    @abc.abstractmethod
    def add_default_target(self, target, alias=False):
        """
        Add a default target.

        Targets named this way will be built if no target is specified on the
        command line.

        This can be called more than once; the effects are cumulative. If it
        is not called, the fallback is to build all targets that are not used
        as sources for other targets, and possibly also all other targets.
        """
        raise NotImplementedError

    @abc.abstractmethod
    def add_gen_source(self, source):
        """
        Add a generator source.

        Future builds will re-run the generator script if the named file
        changes. The base class calls this for the top-level generator script.
        It may also be called for indirect dependencies of the generator
        (Python modules, configuration files, etc).
        """
        raise NotImplementedError

    @abc.abstractmethod
    def add_gen_output(self, output):
        """
        Add a generator output.

        The generator script may produce additional outputs that build commands
        depend on. By declaring these outputs, the generator script can be
        re-run if the named file is missing or out of date.
        """
        raise NotImplementedError

    def add_compdb(self, target, form='clang'):
        """
        Add a compilation database target.

        If a type is specified, it is the name of one of the supported forms
        of compilation database:

        * 'clang' for Clang JSON

        The default is 'clang'.

        If a rule is attached to this compdb target, then all targets built
        using that rule will be written into the database file.

        This target becomes an implicit output of the build graph generation.
        """
        if form == 'clang':
            compdb = ClangCompDB(target, self._var_subst)
        else:
            raise NotImplementedError("Unknown compdb form: " + repr(form))
        self._compdbs[target] = compdb

    def _expand_target_list(self, target_list):
        """
        This is used to preprocess lists of targets, sources, etc.
        """
        if target_list is None:
            return ()
        elif isinstance(target_list, str):
            return tuple(target_list.split())
        else:
            return tuple(target_list)

    def _var_subst(self, s, **local_env):
        def shrepl(match):
            name = match.group(2) or match.group(3)
            if name in local_env:
                return local_env[name]
            try:
                return self.get_env(name)
            except KeyError:
                return ''
        shvars = re.compile(r'\$((\w+)\b|{(\w+)})')
        n = 1
        while n:
            s, n = shvars.subn(shrepl, s)
        return s


class NinjaBuild(AbstractBuildGraph):
    def __init__(self, ninja_file, **kwargs):
        self._lines = ['# Autogenerated, do not edit']
        self._gen_sources = set()
        self._gen_outputs = set()
        self._env_names = set()
        self._rule_names = {'phony'}
        self._ninja_file = ninja_file
        self._subninja_files = []

        rules_dir = os.path.dirname(ninja_file) or '.'
        try:
            os.makedirs(rules_dir)
        except FileExistsError:
            pass
        self._mkdir_cache = {'.', rules_dir}
        self._mkdir_targets = []

        super(NinjaBuild, self).__init__(**kwargs)

    def _make_variant(self, build_dir):
        ninja_file = os.path.join(build_dir, 'rules.ninja')
        self._subninja_files.append(ninja_file)
        self._lines.append('')
        self._lines.append('subninja ' + ninja_file)

        variant = type(self)(ninja_file, build_dir=build_dir, _parent=self)

        # Shadowed state
        variant._env_names = set(self._env_names)
        variant._rule_names = set(self._rule_names)

        # Shared state
        variant._gen_sources = self._gen_sources

        return variant

    @property
    def _all_ninja_files(self):
        return (self._ninja_file,) + tuple(f for v in self._variants
                                           for f in v._all_ninja_files)

    @property
    def _all_byproducts(self):
        byproducts = tuple(self._compdbs.keys())
        byproducts += tuple(self._gen_outputs)
        byproducts += tuple(f for v in self._variants
                            for f in v._all_byproducts)
        return byproducts

    @property
    def _phony_always(self):
        return os.path.join('tools', 'build', '.should-not-exist')

    def __call__(self, gen_cmd=None, **kwargs):
        super(NinjaBuild, self).__call__(**kwargs)

        if not self._is_variant:
            # Add a rule at the top level to rerun the generator script
            assert gen_cmd is not None
            self.add_rule('_gen_rules', gen_cmd, generator=True, restat=True)
            self.add_target(self._all_ninja_files, '_gen_rules',
                            depends=sorted(self._gen_sources),
                            byproducts=self._all_byproducts)

            # Add a phony rule for always-built targets
            self.add_alias(self._phony_always, [])

            # Add phony rules for all of the generator sources, so Ninja
            # does not fail if one of them disappears (e.g. if a module
            # is renamed, or an older branch is checked out)
            for f in sorted(self._gen_sources):
                self.add_alias(f, [])

        # Add a rule and targets for all of the automatically created parent
        # directories. We do this in deepest-first order at the end of the
        # build file because ninja -t clean always processes targets in the
        # order they appear, so it might otherwise fail to remove directories
        # that will become empty later.
        self.add_rule('_mkdir', 'mkdir -p ${out}')
        for d in reversed(self._mkdir_targets):
            self.add_target([d], '_mkdir', _is_auto_dir=True)

        # Write out the rules file
        with open(self._ninja_file, 'w') as f:
            f.write('\n'.join(self._lines) + '\n')

    @property
    def root_dir(self):
        """
        The root directory from which build commands are run.

        This is either absolute, relative to the root directory of the
        repository, or empty. It should be used as the start= argument of an
        os.path.relpath() call for any path that is specified on a command
        line outside of the ${in} or ${out} variables (e.g. include search
        directories).

        For Ninja, it is simply the empty string.
        """
        return ''

    def add_env(self, name, value, replace=False):
        if name in self._env_names and not replace:
            raise KeyError("Duplicate definition of env ${name}"
                           .format(name=name))
        super(NinjaBuild, self).add_env(name, value, replace=replace)
        self._env_names.add(name)
        self._lines.append('')
        self._lines.append('{name} = {value}'.format(**locals()))

    def append_env(self, name, value, separator=' '):
        if name in self._env_names:
            self._lines.append('')
            self._lines.append('{name} = ${{{name}}}{separator}{value}'
                               .format(**locals()))
            super(NinjaBuild, self).append_env(name, value, separator)
        else:
            self.add_env(name, value)

    def get_env(self, name):
        try:
            return super(NinjaBuild, self).get_env(name)
        except KeyError:
            return os.environ[name]

    def add_rule(self, name, command, depfile=None, depfile_external=False,
                 compdbs=None, generator=False, restat=False):
        if name in self._rule_names:
            raise KeyError("Duplicate definition of rule {name}"
                           .format(name=name))
        super(NinjaBuild, self).add_rule(name, command, depfile=depfile,
                                         depfile_external=depfile_external,
                                         compdbs=compdbs)
        self._rule_names.add(name)
        self._lines.append('')
        self._lines.append('rule ' + name)
        self._lines.append('    command = ' + command)
        self._lines.append('    description = ' + name + ' ${out}')
        if depfile is not None:
            self._lines.append('    depfile = ' + depfile)
        if generator:
            self._lines.append('    generator = true')
        if restat:
            self._lines.append('    restat = true')

    def add_target(self, targets, rule, sources=None, depends=None,
                   requires=None, byproducts=None, always=False,
                   _is_auto_dir=False, **local_env):
        super(NinjaBuild, self).add_target(
            targets, rule, sources=sources, depends=depends,
            requires=requires, byproducts=byproducts, **local_env)
        targets = self._expand_target_list(targets)
        sources = self._expand_target_list(sources)
        depends = self._expand_target_list(depends)
        requires = self._expand_target_list(requires)
        byproducts = self._expand_target_list(byproducts)

        if rule in self._rule_byproducts:
            depsfile = re.sub(r'\$(out\b|{out})', targets[0],
                              self._rule_byproducts[rule])
            byproducts = byproducts + (depsfile,)

        if not _is_auto_dir:
            # Automatically add a dependency on the parent directory of each
            # target that is not at the top level
            for t in targets:
                target_dir = os.path.dirname(os.path.normpath(t))
                if target_dir:
                    self._mkdir(target_dir)
                    requires = requires + (target_dir,)

        self._lines.append('')
        build_line = 'build ' + ' '.join(targets)
        if byproducts:
            build_line += ' | '
            build_line += ' '.join(byproducts)
        build_line += ' : ' + rule
        if sources:
            build_line += ' '
            build_line += ' '.join(sources)
        if depends:
            build_line += ' | '
            build_line += ' '.join(depends)
        if always:
            build_line += ' ' + self._phony_always + ' '
        if requires:
            build_line += ' || '
            build_line += ' '.join(requires)
        self._lines.append(build_line)

        for name in sorted(local_env.keys()):
            self._lines.append('    {} = {}'.format(name, local_env[name]))

    def add_alias(self, alias, targets):
        targets = self._expand_target_list(targets)
        self._lines.append('')
        self._lines.append('build ' + self._escape(alias) + ' : phony ' +
                           ' '.join(targets))

    def add_default_target(self, target, alias=False):
        self._lines.append('')
        self._lines.append('default ' + self._escape(target))

    def add_gen_source(self, source):
        self._gen_sources.add(os.path.normpath(source))

    def add_gen_output(self, output):
        self._gen_outputs.add(os.path.normpath(output))

    def _mkdir(self, target_dir):
        if target_dir in self._mkdir_cache:
            return
        # Always add parent directories first, if any. This ensures that
        # the _mkdir_targets list is ordered with the deepest directories
        # last.
        parent = os.path.dirname(target_dir)
        if parent:
            self._mkdir(parent)
        self._mkdir_cache.add(target_dir)
        self._mkdir_targets.append(target_dir)

    def _escape(self, path):
        return re.sub(r'([ \n:$])', r'$\1', path)

    def _expand_target_list(self, target_list):
        return tuple(self._escape(s)
                     for s in super(NinjaBuild, self)
                     ._expand_target_list(target_list))


class SConsBuild(AbstractBuildGraph):
    def __init__(self, env, Builder, Action, _parent=None, **kwargs):
        self.env = env
        self.Builder = Builder
        self.Action = Action
        self._rule_depfiles = {}
        self._root_dir = env.Dir('#.')
        if _parent is None:
            self._default_targets = []
        else:
            self._default_targets = _parent._default_targets
        super(SConsBuild, self).__init__(_parent=_parent, **kwargs)

    def __call__(self, **kwargs):
        super(SConsBuild, self).__call__(**kwargs)
        return self._default_targets

    @property
    def root_dir(self):
        """
        The root directory from which build commands are run.

        This is either absolute, relative to the root directory of the
        repository, or empty. It should be useda as the start= argument of an
        os.path.relpath() call for any path that is specified on a command
        line outside of the ${in} or ${out} variables (e.g. include search
        directories).

        For SCons, it is the root-relative path of the current directory.
        """
        return os.path.relpath(self._root_dir.abspath)

    def _make_variant(self, build_dir):
        return type(self)(self.env.Clone(), self.Builder, self.Action,
                          build_dir=build_dir, _parent=self)

    def add_env(self, name, value, replace=False):
        if not replace and name in self.env:
            raise KeyError("Duplicate definition of env ${name}"
                           .format(name=name))
        self.env.Replace(**{name: value})
        super(SConsBuild, self).add_env(name, value, replace=replace)

    def append_env(self, name, value, separator=' '):
        if name in self.env:
            self.env.Append(**{name: separator + value})
        else:
            self.env.Replace(**{name: value})
        super(SConsBuild, self).append_env(name, value, separator)

    def get_env(self, name):
        try:
            return super(SConsBuild, self).get_env(name)
        except KeyError:
            return self.env['ENV'][name]

    def add_rule(self, name, command, depfile=None, depfile_external=False,
                 compdbs=None, restat=False):
        if 'Rule_' + name in self.env['BUILDERS']:
            raise KeyError("Duplicate definition of rule {name}"
                           .format(name=name))
        super(SConsBuild, self).add_rule(name, command, depfile=depfile,
                                         depfile_external=depfile_external,
                                         compdbs=compdbs)
        # Replace the Ninja-style $in/$out variables with $SOURCES / $TARGETS
        command = re.sub(r'\$(in\b|{in})', '${SOURCES}', command)
        command = re.sub(r'\$(out\b|{out})', '${TARGETS}', command)
        description = name + ' ${TARGETS}'
        builder = self.Builder(action=self.Action(command, description))
        self.env.Append(BUILDERS={'Rule_' + name: builder})
        if depfile is not None:
            self._rule_depfiles[name] = depfile

    def add_target(self, targets, rule, sources=None, depends=None,
                   requires=None, byproducts=None, always=None, **local_env):
        super(SConsBuild, self).add_target(
            targets, rule, sources=sources, depends=depends,
            requires=requires, byproducts=byproducts, **local_env)
        targets = self._expand_target_list(targets)
        sources = self._expand_target_list(sources)
        depends = self._expand_target_list(depends)
        requires = self._expand_target_list(requires)
        byproducts = self._expand_target_list(byproducts)

        if rule in self._rule_byproducts:
            depsfile = re.sub(r'\$(out\b|{out})', targets[0],
                              self._rule_byproducts[rule])
            byproducts = byproducts + (depsfile,)

        tnodes = getattr(self.env, 'Rule_' + rule)(
            target=targets, source=sources, **local_env)
        if depends:
            self.env.Depends(tnodes, depends)
        if requires:
            self.env.Requires(tnodes, requires)
        if byproducts:
            self.env.SideEffect(byproducts, targets)
            # side-effects are not cleaned by default
            self.env.Clean(targets, byproducts)
        if always:
            self.env.AlwaysBuild(targets)
        if rule in self._rule_depfiles:
            depfile = re.sub(r'\$(out\b|{out})', targets[0],
                             self._rule_depfiles[rule])
            # Note: this is slightly broken; if the depfile is created by the
            # rule that it affects, SCons will spuriously rebuild everything
            # that uses it on the _second_ run after a clean. This appears to
            # be a deliberate feature; the SCons maintainers are ideologically
            # opposed to compiler generated depfiles. Ninja handles them
            # correctly.
            saved_dir = self.env.fs.getcwd()
            try:
                # Change to the root directory, so the depends in the depfile
                # will be interpreted relative to it.
                self.env.fs.chdir(self._root_dir, change_os_dir=False)
                # Note that depfile must be a plain path, not a File node, and
                # SCons will directly call open() on it. So it must be
                # relative to the repository, not to self._root_dir.
                self.env.ParseDepends(depfile)
            finally:
                self.env.fs.chdir(saved_dir, change_os_dir=False)

    def add_alias(self, alias, targets):
        targets = self._expand_target_list(targets)
        self.env.Alias(alias, targets)

    def future_alias(self, alias):
        return self.env.Alias(alias)

    def add_default_target(self, target, alias=False):
        if not alias:
            try:
                target = self.env.Entry(target)
            except ValueError:
                pass
        self.env.Default(target)
        self._default_targets.append(target)

    def add_gen_source(self, source):
        # Don't care about these, SCons regenerates on every run anyway
        pass

    def add_gen_output(self, output):
        # Don't care about these, SCons regenerates on every run anyway
        pass


if __name__ == '__main__':
    # Called stand-alone; generate a Ninja file.
    import pipes
    build = NinjaBuild('build.ninja',
                       arguments=dict(a.split('=', 1) for a in sys.argv[1:]))
    build(gen_cmd=' '.join((pipes.quote(arg) for arg in sys.argv)))

```

`docs/api/Makefile`:

```
%.pdf: %.md Makefile
	pandoc -s --toc --pdf-engine=xelatex -N --top-level-division=part \
		--metadata=title:'Gunyah Hypercall API' \
		--metadata=date:"Generated: `date \"+%a %d %B %Y\"`" \
		--variable=class:book \
		--variable=mainfont:LiberationSans \
		--variable=monofont:LiberationMono \
		--variable=papersize:a4  \
		--variable=margin-left:2.5cm  \
		--variable=margin-right:2.5cm  \
		--variable=margin-top:2.5cm  \
		--variable=margin-bottom:2.5cm  \
		$< -o $@

all: gunyah_api.pdf gunyah_api_qcom.pdf

```

`docs/api/gunyah_api.md`:

```md
# Gunyah API

## AArch64 HVC ABI

The Gunyah AArch64 hypercall interface generally follows the ARM AAPCS64 conventions for general purpose register argument and result passing, and preservation of registers, unless explicitly documented otherwise. The hypervisor does not use SIMD, Floating-Point or SVE registers in the hypercall interface.
Gunyah hypercalls use a range of HVC opcode immediate numbers, and reserves the following HVC immediate range:
```
hvc #0x6000
```
through to:
```
hvc #0x61ff
```
Note, Gunyah hypercalls encode the Call-ID in the HVC immediate, encoded within the instruction. This differs from the ARM defined and reserved `HVC #0` namespace, which uses `r0/x0` as the call identifier.

### General-purpose Register

| Register | Role in AAPCS64 | Role in Gunyah HVC |
|--|---|-----|
|     SP_EL0 / SP_EL1          |     The Stack Pointers                                                                                                                                               |     Preserved. (Callee-saved)                                                            |
|     r30 / LR                 |     The Link Register                                                                                                                                                |     Preserved. (Callee-saved)                                                            |
|     r29 / FR                 |     The Frame Register                                                                                                                                               |     Preserved. (Callee-saved)                                                            |
|     r19…r28                  |     Callee-saved registers                                                                                                                                           |     Preserved. (Callee-saved)                                                            |
|     r18                      |     The Platform Register, if needed; otherwise a temporary   register                                                                                               |     Preserved. (Callee-saved)                                                            |
|     r17                      |     IP1     The second intra-procedure-call temporary register (can   be used by call veneers and PLT code); at other times may be used as a   temporary register    |     Temporary. (Caller-saved)                                                            |
|     r16                      |     IP0     The first intra-procedure-call scratch register (can be   used by call veneers and PLT code); at other times may be used as a temporary   register       |     Temporary. (Caller-saved)                                                            |
|     r9…r15                   |     Temporary registers                                                                                                                                              |     Temporary. (Caller-saved)                                                            |
|     r8                       |     Indirect result location register                                                                                                                                |     Temporary. (Caller-saved)                                                            |
|     r0…r7                    |     Parameter / Result registers                                                                                                                                     |     Parameter / Result registers. <br>*Note: Unused result registers not preserved. (Caller-saved)*    |

Preserved registers are unchanged across the HVC call. Temporary registers have an unpredictable value on return and must be saved by the caller before making the HVC call.

### SIMD and Floating-Point and SVE Registers

Gunyah does not use SIMD and Floating-Point (or SVE) registers in the HVC API, and defines all SIMD and Floating-Point and SVE registers as *callee-saved*. (The register values are preserved across HVC calls).

## Common Types

The following types are commonly referred to in the HVC interface.

### Error Result

The hypervisor API, where possible, uses a consistent error return convention and non-overlapping error code values.

When an error result is returned from a hypercall, this is typically in the first result register (`X0`).

When an error result and one or more return values are returned from a hypercall, the error result is placed in (`X0`), and the return values are returned in `X1…X7`.

The error value of zero (`0`) is special and is named “OK”. It indicates that no error occurred, or that the operation was successful.

The error value of (`-1`) is special and indicates that the hypercall call is unimplemented.

### Boolean

A Boolean value with `0` representing *False*, and `1` representing *True*.

### CapID

Capabilities are objects within a Capability Space (CSpace). These are not directly accessible to hypervisor clients (VMs). Instead, a VM uses Capability IDs (CapID) which index into a CSpace and address capabilities in order to interact with the hypervisor’s access control.

A CapID is a register-sized opaque integer value, the value has no meaning outside the associated CSpace.

### Size

A value that represents the size in bytes of an object or buffer in memory.

### VMAddr

A pointer in the VM’s current virtual address space, in the context of the caller.

### VMPhysAddr

A pointer in the VM’s physical address space. In ARMv8 terminology, this is an IPA.

### Access Rights

An enumeration describing the rights given to a memory mapping. Follows the standard RWX format.

### Virtual IRQ Info

A bitfield type that identifies a virtual IRQ within a virtual interrupt controller.

*Virtual IRQ Info:*

| Bits | Mask | Description |
|-|---|-----|
|     23:0             |     `0x00FFFFFF`           |     Virtual IRQ Number. The valid range of this field is   defined by the platform-specific virtual interrupt controller implementation.   The range may be discontiguous, and some sub-ranges may have special meanings   (e.g. there may be a range reserved for VCPU-specific VIRQs).                                                                    |
|     31..24           |     `0xFF000000`           |     Target VCPU index. This is the attachment index of a VCPU   as defined by the hypercall API that configures the virtual interrupt   controller. Valid only if the virtual IRQ number is in a range reserved by   the virtual interrupt controller for VCPU-specific IRQs, and the operation   being performed is implemented for VCPU-specific IRQs.    |
|     63:32            |     `0xFFFFFFFF.00000000`  |     Reserved,   Must be Zero                                                                                                                                                                                                                                                                                                                                |

## Object Rights

Gunyah hypercalls identify objects through capabilities, and use the rights on the capability for operation-type access control. The rights field is a 32-bit bitmap of rights. The following section lists the capability rights values for the various object types.

### Generic Rights

Generic rights are valid for all object types.

| Right             |  Value            |
|-------------------|-------------------|
| Object Activate   |  `0x80000000`     |

### Partition Rights

| Right             |  Value            |
|-------------------|-------------------|
| Partition Object Create | `0x00000001`  |
| Partition Donate        | `0x00000002`  |

### Capability Space Rights

| Right             |  Value            |
|-------------------|-------------------|
| Cspace Cap Create  | `0x00000001` |
| Cspace Cap Delete  | `0x00000002` |
| Cspace Cap Copy    | `0x00000004` |
| Cspace Attach      | `0x00000008` |

### Address Space Rights

| Right             |  Value            |
|-------------------|-------------------|
| Address Space Attach | `0x00000001` |
| Address Space Map | `0x00000002` |
| Address Space Lookup | `0x00000004` |

### Memory Extent Rights

| Right             |  Value            |
|-------------------|-------------------|
| Memory Extent Map    | `0x00000001` |
| Memory Extent Derive | `0x00000002` |
| Memory Extent Attach | `0x00000004` |
| Memory Extent Lookup | `0x00000008` |
| Memory Extent Donate | `0x00000010` |

### Thread Rights

| Right             |  Value            |
|-------------------|-------------------|
| Thread Power On/Off  | `0x00000001` |
| Thread Set Affinity  | `0x00000002` |
| Thread Set Priority  | `0x00000004` |
| Thread Set Timeslice | `0x00000008` |
| Thread Yield To      | `0x00000010` |
| Thread Bind VIRQ     | `0x00000020` |
| Thread Access State  | `0x00000040` |
| Thread Lifecycle     | `0x00000080` |
| Thread Write Context | `0x00000100` |
| Thread Disable       | `0x00000200` |

### Doorbell Rights

| Right             |  Value            |
|-------------------|-------------------|
| Doorbell Send     | `0x00000001` |
| Doorbell Receive  | `0x00000002` |
| Doorbell Bind     | `0x00000004` |

### Message Queues Rights

| Right             |  Value            |
|-------------------|-------------------|
| Message Queue Send      | `0x00000001` |
| Message Queue Receive   | `0x00000002` |
| Message Queue Bind Send | `0x00000004` |
| Message Queue Bind Receive | `0x00000008` |

### Virtual Interrupt Controller Rights

| Right             |  Value            |
|-------------------|-------------------|
| Virtual Interrupt Controller Bind Source | `0x00000001` |
| Virtual Interrupt Controller Attach VCPU | `0x00000002` |

### HW IRQ Rights

| Right             |  Value            |
|-------------------|-------------------|
| HW IRQ Bind VIC   | `0x00000001` |

### Virtual PM Group Rights

| Right             |  Value            |
|-------------------|-------------------|
| Virtual PM Group Attach VCPU | `0x00000001` |
| Virtual PM Group Bind VIRQ   | `0x00000002` |
| Virtual PM Group Query       | `0x00000004` |

### Watchdog Rights

| Right             |  Value            |
|-------------------|-------------------|
| Watchdog Attach VCPU | `0x00000001` |
| Watchdog Bind VIRQ   | `0x00000002` |

### Virtual IO MMIO Rights

| Right             |  Value            |
|-------------------|-------------------|
| Virtual IO MMIO Bind Backend VIRQ  | `0x00000001` |
| Virtual IO MMIO Bind Frontend VIRQ | `0x00000002` |
| Virtual IO MMIO Assert VIRQ   | `0x00000004` |
| Virtual IO MMIO Config        | `0x00000008` |

### Virtual GIC ITS

| Right             |  Value            |
|-------------------|-------------------|
| Virtual GIC ITS Bind VIC | `0x00000001` |


## Hypervisor Identification

### Hypervisor API Version and Features

Identifies the hypervisor version and feature set.

|    **Hypercall**:       |     `hypervisor_identify`    |
|-------------------------|------------------------------|
|     Call number:        |     `hvc 0x6000`             |
|     Inputs:             |     None                     |
|     Outputs:            |     X0: Hyp API Info         |
|                         |     X1: API Flags 0          |
|                         |     X2: API Flags 1          |
|                         |     X3: API Flags 2          |


**Types:**

*Hyp API Info:*

| Bits | Mask | Description |
|-|---|-----|
| 13:0            | `0x00001FFF`           | API Version = “1”                                             |
| 14              | `0x00004000`           | 0 = API is Little Endian.   <br>1 = API is Big Endian.        |
| 15              | `0x00008000`           | If set to 1, the API is 64-bit, otherwise 32-bit.             |
| 55:16           | `0xFFFFFF.FFFF0000`    | Reserved                                                      |
| 63:56           | `0xFF000000.00000000`  | Hypervisor   variant.<br>- Unknown = 0x0<br>- Haven = 0x48    |

*API Flags 0:*

| Bits | Mask | Description |
|-|---|-----|
|     0                |     `0x1`                  |     1   = Partition and CSpace APIs supported                            |
|     1                |     `0x2`                  |     1 = Doorbell APIs supported                                          |
|     2                |     `0x4`                  |     1 = Message Queue APIs supported                                     |
|     3                |     `0x8`                  |     1 = Virtual Interrupt Controller and Virtual IRQ   APIs supported    |
|     4                |     `0x10`                 |     1 = Virtual Power Management APIs supported                          |
|     5                |     `0x20`                 |     1 = Virtual CPU APIs supported                                       |
|     6                |     `0x40`                 |     1 = Memory Extent APIs supported                                     |
|     7                |     `0x80`                 |     1 = Tracing control API supported                                    |
|     15:8             |     `0xFF00`               |     Reserved = 0 [TBD   additional API flags]                            |
|     16               |     `0x10000`              |     Reserved                                                             |
|     63:17            |     `0xFFFFFFFF.FFFE0000`  |     Reserved = 0 [TBD   additional API flags]                            |


*API Flags 1:*

| Bits | Mask | Description |
|-|---|-----|
|     0                |     `0x1`                  |     1 = ARM v8.2 SVE support    |
|     63:1             |     `0xFFFFFFFF.FFFFFFFF ` |     Reserved = 0                |

*API Flags 2:*

| Bits | Mask | Description |
|-|---|-----|
|     63:0             |     `0xFFFFFFFF.FFFFFFFF`  |     Reserved = 0     |


## Partitions

### Partition Object Creation

Allocates a new Partition object from the Partition and allocates a Capability ID from the CSpace.

|    **Hypercall**:       |      `partition_create_partition`    |
|-------------------------|--------------------------------------|
|     Call number:        |     `hvc 0x6001`                     |
|     Inputs:             |     X0: Partition CapID              |
|                         |     X1: CSpace CapID                 |
|                         |     X2: Reserved — Must be Zero      |
|     Outputs:            |     X0: Error Result                 |
|                         |     X1: Partition CapID              |

On successful creation, the new Partition object is created, and its state is OBJECT_STATE_INIT.

**Errors:**

OK – the operation was successful, and the result is valid.

ERROR_NOMEM – the creation failed due to memory allocation error.

Also see: [Capability Errors](#capability-errors)

### Capability Space Object Creation

Allocates a new CSpace object from the Partition and allocates a Capability ID from the CSpace.

|    **Hypercall**:       |      `partition_create_cspace`       |
|-------------------------|--------------------------------------|
|     Call number:        |     `hvc 0x6002`                     |
|     Inputs:             |     X0: Partition CapID              |
|                         |     X1: CSpace CapID                 |
|                         |     X2: Reserved — Must be Zero      |
|     Outputs:            |     X0: Error Result                 |
|                         |     X1: Cspace CapID                 |

On successful creation, the new CSpace object is created and its state is OBJECT_STATE_INIT.

**Errors:**

OK – the operation was successful, and the result is valid.

ERROR_NOMEM – the creation failed due to memory allocation error.

Also see: [Capability Errors](#capability-errors)


### Address Space Object Creation

Allocates a new Address Space object from the Partition and allocates a Capability ID from the CSpace.

|    **Hypercall**:       |      `partition_create_addrspace`    |
|-------------------------|--------------------------------------|
|     Call number:        |     `hvc 0x6003`                     |
|     Inputs:             |     X0: Partition CapID              |
|                         |     X1: CSpace CapID                 |
|                         |     X2: Reserved — Must be Zero      |
|     Outputs:            |     X0: Error Result                 |

On successful creation, the new Address Space object is created and its state is OBJECT_STATE_INIT.

**Errors:**

OK – the operation was successful, and the result is valid.

ERROR_NOMEM – the creation failed due to memory allocation error.

Also see: [Capability Errors](#capability-errors)

### Memory Extent Object Creation

Allocates a new Memory Extent object from the Partition and allocates a Capability ID from the CSpace.

|    **Hypercall**:       |      `partition_create_memextent`    |
|-------------------------|--------------------------------------|
|     Call number:        |     `hvc 0x6004`                     |
|     Inputs:             |     X0: Partition CapID              |
|                         |     X1: CSpace CapID                 |
|                         |     X2: Reserved — Must be Zero      |
|     Outputs:            |     X0: Error Result                 |
|                         |     X1: MemExtent CapID              |

On successful creation, the new Memory Extent object is created and its state is OBJECT_STATE_INIT.

**Errors:**

OK – the operation was successful, and the result is valid.

ERROR_NOMEM – the creation failed due to memory allocation error.

Also see: [Capability Errors](#capability-errors)

### Thread (Virtual CPU) Object Creation

Allocates a new Thread object from the Partition and allocates a Capability ID from the CSpace.

|    **Hypercall**:       |      `partition_create_thread`       |
|-------------------------|--------------------------------------|
|     Call number:        |     `hvc 0x6005`                     |
|     Inputs:             |     X0: Partition CapID              |
|                         |     X1: CSpace CapID                 |
|                         |     X2: Reserved — Must be Zero      |
|     Outputs:            |     X0: Error Result                 |
|                         |     X1: Thread CapID                 |

On successful creation, the new Thread object is created and its state is OBJECT_STATE_INIT.

**Errors:**

OK – the operation was successful, and the result is valid.

ERROR_NOMEM – the creation failed due to memory allocation error.

Also see: [Capability Errors](#capability-errors)

### Doorbell Object Creation

Allocates a new Doorbell object from the Partition and allocates a Capability ID from the CSpace.

|    **Hypercall**:       |      `partition_create_doorbell`     |
|-------------------------|--------------------------------------|
|     Call number:        |     `hvc 0x6006`                     |
|     Inputs:             |     X0: Partition CapID              |
|                         |     X1: CSpace CapID                 |
|                         |     X2: Reserved — Must be Zero      |
|     Outputs:            |     X0: Error Result                 |
|                         |     X1: Doorbell CapID               |

On successful creation, the new Doorbell object is created and its state is OBJECT_STATE_INIT.

**Errors:**

OK – the operation was successful, and the result is valid.

ERROR_NOMEM – the creation failed due to memory allocation error.

Also see: [Capability Errors](#capability-errors)

### Message Queue Object Creation

Allocates a new Message Queue object from the Partition and allocates a Capability ID from the CSpace.

|    **Hypercall**:       |      `partition_create_msgqueue`       |
|-------------------------|----------------------------------------|
|     Call number:        |     `hvc 0x6007`                       |
|     Inputs:             |     X0: Partition CapID                |
|                         |     X1: CSpace CapID                   |
|                         |     X2: Reserved — Must be Zero        |
|     Outputs:            |     X0: Error Result                   |
|                         |     X1: MessageQueue CapID             |

On successful creation, the new Message Queue object is created and its state is OBJECT_STATE_INIT.

**Errors:**

OK – the operation was successful, and the result is valid.

ERROR_NOMEM – the creation failed due to memory allocation error.

Also see: [Capability Errors](#capability-errors)

### Watchdog object creation

Allocates a new Watchdog object from the Partition and allocates a Capability ID from the CSpace.

|    **Hypercall**:       |      `partition_create_watchdog`     |
|-------------------------|--------------------------------------|
|     Call number:        |     `hvc 0x6009`                     |
|     Inputs:             |     X0: Partition CapID              |
|                         |     X1: CSpace CapID                 |
|                         |     X2: Reserved — Must be Zero      |
|     Outputs:            |     X0: Error Result                 |
|                         |     X1: Watchdog CapID               |

On successful creation, the new Watchdog object is created and its state is OBJECT_STATE_INIT.

**Errors:**

OK – the operation was successful, and the result is valid.

ERROR_NOMEM – the creation failed due to memory allocation error.

Also see: [Capability Errors](#capability-errors)

### Virtual Interrupt Controller Object Creation

Allocates a new Virtual Interrupt Controller object from the Partition and allocates a Capability ID from the CSpace.

|    **Hypercall**:       |      `partition_create_vic`          |
|-------------------------|--------------------------------------|
|     Call number:        |     `hvc 0x600A`                     |
|     Inputs:             |     X0: Partition CapID              |
|                         |     X1: CSpace CapID                 |
|                         |     X2: Reserved — Must be Zero      |
|     Outputs:            |     X0: Error Result                 |
|                         |     X1: Virtual IC CapID             |

On successful creation, the new Virtual Interrupt Controller object is created and its state is OBJECT_STATE_INIT.

**Errors:**

OK – the operation was successful, and the result is valid.

ERROR_NOMEM – the creation failed due to memory allocation error.

Also see: [Capability Errors](#capability-errors)

### Virtual PM Group Object Creation

Allocates a new Virtual PM Group object from the Partition and allocates a Capability ID from the CSpace.

|    **Hypercall**:       |      `partition_create_vpm_group`    |
|-------------------------|--------------------------------------|
|     Call number:        |     `hvc 0x600B`                     |
|     Inputs:             |     X0: Partition CapID              |
|                         |     X1: CSpace CapID                 |
|                         |     X2: Reserved — Must be Zero      |
|     Outputs:            |     X0: Error Result                 |
|                         |     X1: VPMGroup CapID               |

On successful creation, the new Virtual PM Group object is created and its state is OBJECT_STATE_INIT.

**Errors:**

OK – the operation was successful, and the result is valid.

ERROR_NOMEM – the creation failed due to memory allocation error.

Also see: [Capability Errors](#capability-errors)

### Virtual IO MMIO object creation

Allocates a new Virtual IO MMIO object from the Partition and allocates a Capability ID from the CSpace.

|    **Hypercall**:       |      `partition_create_virtio_mmio`   |
|-------------------------|---------------------------------------|
|     Call number:        |     `hvc 0x6048`                      |
|     Inputs:             |     X0: Partition CapID               |
|                         |     X1: CSpace CapID                  |
|                         |     X2: Reserved — Must be Zero       |
|     Outputs:            |     X0: Error Result                  |
|                         |     X1: VirtioMMIO CapID              |

On successful creation, the new Virtual IO MMIO object is created and its state is OBJECT_STATE_INIT.

**Errors:**

OK – the operation was successful, and the result is valid.

ERROR_NOMEM – the creation failed due to memory allocation error.

Also see: [Capability Errors](#capability-errors)

## Object Management

### Activate an Object

Activate an object.

|    **Hypercall**:       |      `object_activate`               |
|-------------------------|--------------------------------------|
|     Call number:        |     `hvc 0x600C`                     |
|     Inputs:             |     X0: Cap CapID                    |
|                         |     X1: Reserved — Must be Zero      |
|     Outputs:            |     X0: Error Result                 |

**Errors:**

OK – the operation was successful, and the object has moved into `OBJECT_STATE_ACTIVE` state.

ERROR_OBJECT_STATE – if the object is not in OBJECT_STATE_INIT state.

Additional [error codes](#error-code-enumeration) can be returned depending on the type of object to be activated.

Also see: [Capability Errors](#capability-errors)

### Activate an Object from a CSpace

Activate an object from a Cspace.

|    **Hypercall**:       |      `object_activate_from`          |
|-------------------------|--------------------------------------|
|     Call number:        |     `hvc 0x600D`                     |
|     Inputs:             |     X0: CSpace CapID                 |
|                         |     X1: Cap CapID                    |
|                         |     X2: Reserved — Must be Zero      |
|     Outputs:            |     X0: Error Result                 |

**Errors:**

OK – the operation was successful, and the result is valid.

ERROR_OBJECT_STATE – if the object is not in OBJECT_STATE_INIT state.

Additional [error codes](#error-code-enumeration) can be returned depending on the type of object to be activated.

Also see: [Capability Errors](#capability-errors)

### Reset an Object

Reset an object to its initial state.

|    **Hypercall**:       |      `object_reset`                  |
|-------------------------|--------------------------------------|
|     Call number:        |     `hvc 0x600E`                     |
|     Inputs:             |     X0: Cap CapID                    |
|                         |     X1: Reserved — Must be Zero      |
|     Outputs:            |     X0: Error Result                 |

**Errors:**

OK – the operation was successful, and the result is valid.

ERROR_UNIMPLEMENTED – if functionality not implemented.

Additional [error codes](#error-code-enumeration) can be returned depending on the type of object to be reset.

Also see: [Capability Errors](#capability-errors)

### Reset an Object from a CSpace

Reset an object from a Cspace to its initial state.

|    **Hypercall**:       |      `object_reset_from`             |
|-------------------------|--------------------------------------|
|     Call number:        |     `hvc 0x600F`                     |
|     Inputs:             |     X0: CSpace CapID                 |
|                         |     X1: Cap CapID                    |
|                         |     X2: Reserved — Must be Zero      |
|     Outputs:            |     X0: Error Result                 |

**Errors:**

OK – the operation was successful, and the result is valid.

ERROR_UNIMPLEMENTED – if functionality not implemented.

Additional [error codes](#error-code-enumeration) can be returned depending on the type of object to be reset.

Also see: [Capability Errors](#capability-errors)

### TBD object create / Partition

<!-- TODO: -->

0x6008 – `Reserved`

## Communication APIs

### Doorbell Management

#### Doorbell Bind

Binds a Doorbell to a virtual interrupt.

|    **Hypercall**:       |      `doorbell_bind_virq`            |
|-------------------------|--------------------------------------|
|     Call number:        |     `hvc 0x6010`                     |
|     Inputs:             |     X0: Doorbell CapID               |
|                         |     X1: Virtual IC CapID             |
|                         |     X2: Virtual IRQ Info             |
|                         |     X3: Reserved — Must be Zero      |
|     Outputs:            |     X0: Error Result                 |

**Errors:**

OK – the operation was successful, and the result is valid.

ERROR_NOMEM – the operation failed due to memory allocation error.

ERROR_VIRQ_BOUND – the specified doorbell is already bound to a VIRQ number.

ERROR_BUSY – the specified VIRQ number is already bound to a source.

ERROR_ARGUMENT_INVALID – a value passed in an argument was invalid. This could be due to an invalid Virtual IRQ Info value.

Also see: [Capability Errors](#capability-errors)

#### Doorbell Unbind

Unbinds a Doorbell from a virtual IRQ number.

|    **Hypercall**:       |      `doorbell_unbind_virq`          |
|-------------------------|--------------------------------------|
|     Call number:        |     `hvc 0x6011`                     |
|     Inputs:             |     X0: Doorbell CapID               |
|                         |     X1: Reserved — Must be Zero      |
|     Outputs:            |     X0: Error Result                 |

**Errors:**

OK – the operation was successful, or the Doorbell’s interrupt was already unbound.

Also see: [Capability Errors](#capability-errors)

#### Doorbell Send

Sets flags in the Doorbell, and possibly asserts the bound virtual interrupt.

The specified NewFlags will be set (with a bitwise-OR) in the Doorbell flags.

If a VIRQ has been bound to the Doorbell, it will be asserted after setting the flags if either of the following is true:

* The enabled Doorbell flags, as defined by the bitwise-AND of the flags and the EnableFlags argument to the most recent `doorbell_mask` call, is non-zero.
* The VIRQ is edge-triggered.

|    **Hypercall**:       |      `doorbell_send`                              |
|-------------------------|---------------------------------------------------|
|     Call number:        |     `hvc 0x6012`                                  |
|     Inputs:             |     X0: Doorbell CapID                            |
|                         |     X1: NewFlags FlagsBitmap                      |
|                         |     X2: Reserved — Must be Zero                   |
|     Outputs:            |     X0: Error Result                              |
|                         |     X1: OldFlags FlagsBitmap                      |

The returned OldFlags result contains the Doorbell’s previous unmasked flags before the NewFlags were added.

**Types:**

FlagsBitmap: unsigned 64-bit bitmap

**Errors:**

OK – the operation was successful, and the result is valid.

Also see: [Capability Errors](#capability-errors)

#### Doorbell Receive

Reads and clears the flags of the Doorbell, and possibly clears the bound virtual interrupt.

The specified ClearFlags will be set in the Doorbell flags. These must be nonzero; otherwise the call would have no effect.

If a VIRQ has been bound to the Doorbell, it will be cleared after clearing the flags if all of the following are true:

* The enabled Doorbell flags, as defined by the bitwise-AND of the flags and the EnableFlags argument to the most recent `doorbell_mask` call, is zero.
* The VIRQ is level-triggered.

The implementation does not guarantee that the VIRQ is cleared before the call returns. If level-triggered, the VIRQ is guaranteed to have been cleared before either of the following events occurs:

* The VIRQ is delivered after being individually unmasked using a platform-specific Virtual Interrupt Controller API. This includes EOI events, if the implementation supports them.
* An unspecified finite period of time has elapsed after the call is made.

If the VIRQ is edge-triggered, then this call's effect on it is unspecified.

|    **Hypercall**:       |      `doorbell_receive`                               |
|-------------------------|-------------------------------------------------------|
|     Call number:        |     `hvc 0x6013`                                      |
|     Inputs:             |     X0: Doorbell CapID                                |
|                         |     X1: ClearFlags FlagsBitmap – Must be non-zero.    |
|                         |     X2: Reserved — Must be Zero                       |
|     Outputs:            |     X0: Error Result                                  |
|                         |     X1: OldFlags FlagsBitmap                          |

The returned OldFlags result contains the Doorbell’s previous unmasked flags before the ClearFlags were removed.

**Types:**

FlagsBitmap: unsigned 64-bit bitmap

**Errors:**

OK – the operation was successful, and the result is valid.

ERROR_ARGUMENT_INVALID – if a zero ClearFlags value is passed in.

Also see: [Capability Errors](#capability-errors)

#### Doorbell Reset

Clears all the flags of the Doorbell and sets all bits in the Doorbell’s mask. If there is a pending bound virtual interrupt, it will be de-asserted.

|    **Hypercall**:       |      `doorbell_reset`                |
|-------------------------|--------------------------------------|
|     Call number:        |     `hvc 0x6014`                     |
|     Inputs:             |     X0: Doorbell CapID               |
|                         |     X1: Reserved — Must be Zero      |
|     Outputs:            |     X0: Error Result                 |

**Errors:**

OK – the operation was successful, and the result is valid.

Also see: [Capability Errors](#capability-errors)

#### Doorbell Mask

Sets the Doorbell object’s masks. A Doorbell object has two masks which are configured by the receiver to control which flags it is interested in, and which flags if any should be automatically acknowledged. The EnableMask is the mask of set flags that will cause an assertion of the Doorbell’s bound virtual interrupt. The EnableMask defaults to all-set if it is not configured. The AckMask controls which flags should be automatically cleared when the interrupt is asserted. The Doorbell objects flags are bitwise-NANDed with the AckMask when a interrupt is asserted. Note, the AckMask is unrelated to the EnableMask, and any flags not enabled for asserting an interrupt may be cleared by an AckMask covering those flags. The AckMask defaults to non-set if not configured. Doorbell flags that are not automatically cleared, must be cleared explicitly by the receiver of the virtual interrupt with the Doorbell Receive call prior to acknowledging the virtual interrupt, otherwise the interrupt may be re-asserted.

|    **Hypercall**:       |      `doorbell_mask`                 |
|-------------------------|--------------------------------------|
|     Call number:        |     `hvc 0x6015`                     |
|     Inputs:             |     X0: Doorbell CapID               |
|                         |     X1: EnableMask FlagsBitmap       |
|                         |     X2: AckMask FlagsBitmap          |
|                         |     X3: Reserved — Must be Zero      |
|     Outputs:            |     X0: Error Result                 |

**Types:**

FlagsBitmap: unsigned 64-bit bitmap of Boolean flags.

**Errors:**

OK – the operation was successful, and the result is valid.

Also see: [Capability Errors](#capability-errors)

#### Doorbell Halt

<!-- TODO: -->

0x6016 – `Reserved`

### Message Queue Management

#### Message Queue Bind Send vIRQ

Binds a Message Queue send interface to a virtual IRQ number.

|    **Hypercall**:       |      `msgqueue_bind_send_virq`       |
|-------------------------|--------------------------------------|
|     Call number:        |     `hvc 0x6017`                     |
|     Inputs:             |     X0: Message Queue CapID          |
|                         |     X1: Virtual IC CapID             |
|                         |     X2: Virtual IRQ Info             |
|                         |     X3: Reserved — Must be Zero      |
|     Outputs:            |     X0: Error Result                 |

**Errors:**

OK – the operation was successful, and the result is valid.

ERROR_NOMEM – the operation failed due to memory allocation error.

ERROR_VIRQ_BOUND – the specified message queue is already bound to a VIRQ number.

ERROR_BUSY – the specified VIRQ number is already bound to a source.

ERROR_ARGUMENT_INVALID – a value passed in an argument was invalid. This could be due to an invalid Virtual IRQ Info value, or invalid Message Queue End.

Also see: [Capability Errors](#capability-errors)

#### Message Queue Bind Receive vIRQ

Binds a Message Queue receive interface to a virtual IRQ number.

|    **Hypercall**:       |      `msgqueue_bind_receive_virq`    |
|-------------------------|--------------------------------------|
|     Call number:        |     `hvc 0x6018`                     |
|     Inputs:             |     X0: Message Queue CapID          |
|                         |     X1: Virtual IC CapID             |
|                         |     X2: Virtual IRQ Info             |
|                         |     X3: Reserved — Must be Zero      |
|     Outputs:            |     X0: Error Result                 |

**Errors:**

OK – the operation was successful, and the result is valid.

ERROR_NOMEM – the operation failed due to memory allocation error.

ERROR_VIRQ_BOUND – the specified doorbell is already bound to a VIRQ number.

ERROR_BUSY – the specified VIRQ number is already bound to a source.

ERROR_ARGUMENT_INVALID – a value passed in an argument was invalid. This could be due to an invalid Virtual IRQ Info value, or invalid Message Queue End.

Also see: [Capability Errors](#capability-errors)

#### Message Queue Unbind Send vIRQ

Unbinds a Message Queue send interface virtual IRQ number.

|    **Hypercall**:       |      `msgqueue_unbind_send_virq`     |
|-------------------------|--------------------------------------|
|     Call number:        |     `hvc 0x6019`                     |
|     Inputs:             |     X0: Message Queue CapID          |
|                         |     X1: Reserved — Must be Zero      |
|     Outputs:            |     X0: Error Result                 |

**Errors:**

OK – the operation was successful, or the Message Queue’s send interrupt was already unbound.

Also see: [Capability Errors](#capability-errors)

#### Message Queue Unbind Receive vIRQ

Unbinds a Message Queue receive interface virtual IRQ number.

|    **Hypercall**:       |      `msgqueue_unbind_receive_virq`  |
|-------------------------|--------------------------------------|
|     Call number:        |     `hvc 0x601A`                     |
|     Inputs:             |     X0: Message Queue CapID          |
|                         |     X1: Reserved — Must be Zero      |
|     Outputs:            |     X0: Error Result                 |

**Errors:**

OK – the operation was successful, or the Message Queue’s receive interrupt was already unbound.

Also see: [Capability Errors](#capability-errors)

#### Message Queue Send

Append a message to the tail of a Message Queue, if it is not full. The message is copied from a specified buffer in the caller’s address space. If the Message Queue’s used buffer count was previously below the not-empty interrupt threshold, any receive-side bound virtual interrupt will be asserted.

|    **Hypercall**:       |      `msgqueue_send`                       |
|-------------------------|--------------------------------------------|
|     Call number:        |     `hvc 0x601B`                           |
|     Inputs:             |     X0: Message Queue CapID                |
|                         |     X1: Size Size — Must be non-zero.      |
|                         |     X2: Data VMAddr                        |
|                         |     X3: MsgQSendFlags                      |
|                         |     X4: Reserved — Must be Zero            |
|     Outputs:            |     X0: Error Result                       |
|                         |     X1: NotFull Boolean                    |

**Types:**

*MsgQSendFlags:*

| Bits | Mask | Description |
|-|---|-----|
|     0                |     `0x1`                  |     Message Push                |
|     63:1             |     `0xFFFFFFFF.FFFFFFFE`  |     Reserved — Must be Zero     |

Message Push: If set to 0x1, this flag indicates that the hypervisor should push the message immediately to the receiver. This may cause a receive interrupt to be raised immediately, regardless of any interrupt threshold or interrupt delay configuration.

**Errors:**

OK – the operation was successful.

ERROR_MSGQUEUE_FULL – the Message Queue is full and cannot take the message.

ERROR_ARGUMENT_SIZE – the Size provided is zero, or larger than the Message Queues maximum message size.

ERROR_ADDR_INVALID – some, or the whole of the message buffer is not mapped.

Also see: [Capability Errors](#capability-errors)

#### Message Queue Receive

Fetch a message from the head of a Message Queue, if it is not empty, into a specified buffer in the caller’s address space. If the Message Queue’s used buffer count was previously greater than the not-full interrupt threshold, any send-side bound virtual interrupt will be asserted.

|    **Hypercall**:       |      `msgqueue_receive`              |
|-------------------------|--------------------------------------|
|     Call number:        |     `hvc 0x601C`                     |
|     Inputs:             |     X0: Message Queue CapID          |
|                         |     X1: Buffer VMAddr                |
|                         |     X2: MaximumSize Size             |
|                         |     X3: Reserved — Must be Zero      |
|     Outputs:            |     X0: Error Result                 |
|                         |     X1: Size Size                    |
|                         |     X2: NotEmpty Boolean             |

**Errors:**

OK – the operation was successful. In this case, Size is the number of bytes received, and NotEmpty is true if there are more messages available in the queue.

ERROR_MSGQUEUE_EMPTY – the Message Queue is empty and cannot fetch a message.

ERROR_ADDR_INVALID – some, or the whole of the message buffer is not mapped.

ERROR_ADDR_OVERFLOW – the message at the head of the queue is larger than the provided buffer, and could not be received.

Also see: [Capability Errors](#capability-errors)

#### Message Queue Flush

Rmoves all messages from a Message Queue. If the Message Queue was previously non-empty, any send bound virtual interrupt will be deasserted.

|    **Hypercall**:       |      `msgqueue_flush`                |
|-------------------------|--------------------------------------|
|     Call number:        |     `hvc 0x601D`                     |
|     Inputs:             |     X0: Message Queue CapID          |
|                         |     X1: Reserved — Must be Zero      |
|     Outputs:            |     X0: Error Result                 |

**Errors:**

OK – the operation was successful.

Also see: [Capability Errors](#capability-errors)

#### Messsage Queue Halt

<!-- TODO: -->

0x601E – `Reserved`

#### Message Queue Configure Send

Modify configuration of a Message Queue send interface. The interface allows for configuring of a Message Queue, including setting interrupt thresholds and timeouts.

|    **Hypercall**:       |      `msgqueue_configure_send`                    |
|-------------------------|---------------------------------------------------|
|     Call number:        |     `hvc 0x601F`                                  |
|     Inputs:             |     X0: Message Queue CapID                       |
|                         |     X1: NotFull interrupt threshold               |
|                         |     X2: NotFull threshold delay (microseconds)    |
|                         |     X3: Reserved — Must be -1                     |
|     Outputs:            |     X0: Error Result                              |

Any parameter passed in as -1 indicates no change to the corresponding is requested.

The NotFull threshold modifies the queue used-count at or below which a not-full queue condition is signaled and the send bound virtual interrupt is asserted. This value must be less than the Message Queue’s queue depth.

**Errors:**

OK – the operation was successful.

ERROR_UNIMPLEMENTED – if not implemented.

ERROR_ARGUMENT_INVALID – an argument was invalid.

Also see: [Capability Errors](#capability-errors)

#### Message Queue Configure Receive

Modify configuration of a Message Queue receive interface. The interface allows for configuring of a Message Queue, including setting interrupt thresholds and timeouts.

|    **Hypercall**:       |      `msgqueue_configure_receive`                  |
|-------------------------|----------------------------------------------------|
|     Call number:        |     `hvc 0x6020`                                   |
|     Inputs:             |     X0: Message Queue CapID                        |
|                         |     X1: NotEmpty interrupt threshold               |
|                         |     X2: NotEmpty threshold delay (microseconds)    |
|                         |     X3: Reserved — Must be -1                      |
|     Outputs:            |     X0: Error Result                               |

Any parameter passed in as -1 indicates no change to the corresponding is requested.

The NotEmpty threshold modifies the queue used-count at or above which a not-empty queue condition is signaled and an receive virtual interrupt asserted. This value must be nonzero and no greater than the Message Queue’s queue depth. A special value of -2 sets the threshold to the Message Queue’s queue-depth.

**Errors:**

OK – the operation was successful.

ERROR_UNIMPLEMENTED – if not implemented.

ERROR_ARGUMENT_INVALID – an argument was invalid.

Also see: [Capability Errors](#capability-errors)

#### Configure a Message Queue

Configure a Message Queue whose state is OBJECT_STATE_INIT.

|    **Hypercall**:       |      `msgqueue_configure`            |
|-------------------------|--------------------------------------|
|     Call number:        |     `hvc 0x6021`                     |
|     Inputs:             |     X0: Message Queue CapID          |
|                         |     X1: MessageQueueCreateInfo       |
|                         |     X2: Reserved — Must be Zero      |
|     Outputs:            |     X0: Error Result                 |

**Types:**

*MessageQueueCreateInfo:*

| Bits | Mask | Description |
|-|---|-----|
|     15:0             |     `0x0000FFFF`           |     Queue Depth                 |
|     31:15            |     `0xFFFF0000`           |     Max Message Size            |
|     63:32            |     `0xFFFFFFFF.00000000`  |     Reserved,   Must be Zero    |

**Errors:**

OK – the operation was successful, and the result is valid.

ERROR_OBJECT_STATE – if the message queue is not in OBJECT_STATE_INIT state.

ERROR_ARGUMENT_INVALID – an argument was invalid. This could be due to Queue Depth or Max Message size.

Also see: [Capability Errors](#capability-errors)

## Capability Management

APIs to manage capabilities in Capability Spaces (CSpace).

### Delete a Capability from a CSpace

Delete a Capability in a CSpace.

|    **Hypercall**:       |      `cspace_delete_cap_from`        |
|-------------------------|--------------------------------------|
|     Call number:        |     `hvc 0x6022`                     |
|     Inputs:             |     X0: CSpace CapID                 |
|                         |     X1: Cap CapID                    |
|                         |     X2: Reserved — Must be Zero      |
|     Outputs:            |     X0: Error Result                 |


**Errors:**

OK – the operation was successful, and the result is valid.

Also see: [Capability Errors](#capability-errors)

### Copy a Capability from a specific CSpace

Copy a Capability from one CSpace to another.

|    **Hypercall**:       |      `cspace_copy_cap_from`          |
|-------------------------|--------------------------------------|
|     Call number:        |     `hvc 0x6023`                     |
|     Inputs:             |     X0: SourceCSpace CapID           |
|                         |     X1: SourceCap CapID              |
|                         |     X2: DestCSpace CapID             |
|                         |     X3: RightsMask                   |
|                         |     X4: Reserved — Must be Zero      |
|     Outputs:            |     X0: Error Result                 |
|                         |     X1: New CapID                    |

**Errors:**

OK – the operation was successful, and the result is valid.

ERROR_NOMEM – the operation failed due to memory allocation error.

Also see: [Capability Errors](#capability-errors)

### Revoke a Capability from a CSpace

Revoke a Capability from another CSpace.

|    **Hypercall**:       |      `cspace_revoke_cap_from`        |
|-------------------------|--------------------------------------|
|     Call number:        |     `hvc 0x6024`                     |
|     Inputs:             |     X0: CSpace CapID                 |
|                         |     X1: Cap CapID                    |
|                         |     X2: Reserved — Must be Zero      |
|     Outputs:            |     X0: Error Result                 |

**Errors:**

OK – the operation was successful, and the result is valid.

ERROR_UNIMPLEMENTED – if functionality not implemented.

`TODO: TBD. Currently unimplemented`

Also see: [Capability Errors](#capability-errors)

### Configure a CSpace

Configure a CSpace whose state is OBJECT_STATE_INIT.

|    **Hypercall**:       |      `cspace_configure`              |
|-------------------------|--------------------------------------|
|     Call number:        |     `hvc 0x6025`                     |
|     Inputs:             |     X0: CSpace CapID                 |
|                         |     X1: MaxCaps                      |
|                         |     X2: Reserved — Must be Zero      |
|     Outputs:            |     X0: Error Result                 |

**Errors:**

OK – the operation was successful, and the result is valid.

ERROR_OBJECT_STATE – if the Cspace is not in OBJECT_STATE_INIT state.

ERROR_ARGUMENT_INVALID – a value passed in an argument was invalid. This could be due to an invalid Max Caps value.

Also see: [Capability Errors](#capability-errors)

### CSpace to Thread Attachment

Configure a CSpace whose state is OBJECT_STATE_INIT.

Attaches a thread to a CSpace. The Cspace object must have been activated before this function is called. The thread object must not have been activated.

|    **Hypercall**:       |      `cspace_attach_thread`          |
|-------------------------|--------------------------------------|
|     Call number:        |     `hvc 0x603e`                     |
|     Inputs:             |     X0: CSpace CapID                 |
|                         |     X1: Thread CapID                 |
|                         |     X2: Reserved — Must be Zero      |
|     Outputs:            |     X0: Error Result                 |

**Errors:**

OK – the operation was successful, and the result is valid.

ERROR_OBJECT_STATE – The Thread object has already been activated, or the Cspace object has not yet been activated.

Also see: [Capability Errors](#capability-errors)

### Revoke Children Capabilities from a master capability in a CSpace

Revoke children Capabilities from a CSpace.

|    **Hypercall**:       |      `cspace_revoke_caps_from`       |
|-------------------------|--------------------------------------|
|     Call number:        |     `hvc 0x6059`                     |
|     Inputs:             |     X0: CSpace CapID                 |
|                         |     X1: MasterCap CapID              |
|                         |     X2: Reserved — Must be Zero      |
|     Outputs:            |     X0: Error Result                 |

**Errors:**

OK – the operation was successful, and the result is valid.

Also see: [Capability Errors](#capability-errors)

## Interrupt Management

### Hardware IRQ Bind

Binds a hardware IRQ number to a virtual IRQ number.

|    **Hypercall**:       |      `hwirq_bind_virq`               |
|-------------------------|--------------------------------------|
|     Call number:        |     `hvc 0x6026`                     |
|     Inputs:             |     X0: HW IRQ CapID                 |
|                         |     X1: Virtual IC CapID             |
|                         |     X2: Virtual IRQ Info             |
|                         |     X3: Reserved — Must be Zero      |
|     Outputs:            |     X0: Error Result                 |

**Errors:**

OK – the operation was successful, and the result is valid.

ERROR_NOMEM – the operation failed due to memory allocation error.

ERROR_VIRQ_BOUND – the specified hardware IRQ is already bound to a VIRQ number.

ERROR_BUSY – the specified VIRQ number is already bound to a source.

ERROR_ARGUMENT_INVALID – a value passed in an argument was invalid. This could be due to an invalid Virtual IRQ Info value.

Also see: [Capability Errors](#capability-errors)

### Hardware IRQ Unbind

Unbinds a hardware IRQ number from a virtual IRQ number.

|    **Hypercall**:       |      `hwirq_unbind_virq`             |
|-------------------------|--------------------------------------|
|     Call number:        |     `hvc 0x6027`                     |
|     Inputs:             |     X0: HW IRQ CapID                 |
|                         |     X1: Reserved — Must be Zero      |
|     Outputs:            |     X0: Error Result                 |

**Errors:**

OK – the operation was successful, or the hardware IRQ was already unbound.

Also see: [Capability Errors](#capability-errors)

### Configure a Virtual Interrupt Controller

Configure a Virtual Interrupt Controller whose state is OBJECT_STATE_INIT.

This call sets the maximum number of VCPUs that can be attached to the Virtual Interrupt Controller and receive interrupts from it. It also sets the maximum number of shared (non-VCPU-specific) VIRQ sources that can be registered for delivery though the Virtual Interrupt Controller.

Note that both of these numbers may have implementation-defined upper bounds. Also note that the VIRQ numbers implemented by the controller do not necessarily range from 0 to the specified maximum and may not be contiguous; for example, for a virtual ARM GICv3.1, shared VIRQs are numbered in the ranges 32–1019 and 4096–5119.

|    **Hypercall**:       |      `vic_configure`                 |
|-------------------------|--------------------------------------|
|     Call number:        |     `hvc 0x6028`                     |
|     Inputs:             |     X0: VIC CapID                    |
|                         |     X1: MaxVCPUs                     |
|                         |     X2: MaxSharedVIRQs               |
|                         |     X3: Reserved — Must be Zero      |
|     Outputs:            |     X0: Error Result                 |

**Errors:**

OK – the operation was successful, and the result is valid.

ERROR_ARGUMENT_INVALID – A configuration value was out of range.

ERROR_OBJECT_STATE – The Virtual Interrupt Controller object has already been activated.

Also see: [Capability Errors](#capability-errors)

### Virtual Interrupt Controller to VCPU Attachment

Attaches a VCPU to a Virtual Interrupt Controller. The Virtual Interrupt Controller object must have been activated before this function is called. The VCPU object must not have been activated. An attachment index must be specified which is a non-negative integer less than the MaxVCPUs value used to configure the controller.

|    **Hypercall**:       |      `vic_attach_vcpu`               |
|-------------------------|--------------------------------------|
|     Call number:        |     `hvc 0x6029`                     |
|     Inputs:             |     X0: Virtual IC CapID             |
|                         |     X1: VCPU CapID                   |
|                         |     X2: Index                        |
|                         |     X3: Reserved — Must be Zero      |
|     Outputs:            |     X0: Error Result                 |

**Errors:**

OK – the operation was successful, and the result is valid.

ERROR_NOMEM – the operation failed due to memory allocation error.

ERROR_ARGUMENT_INVALID – the specified attachment index is outside the range supported by this Virtual Interrupt Controller.

ERROR_OBJECT_STATE – The VCPU object has already been activated, or the Virtual Interrupt Controller object has not yet been activated.

Also see: [Capability Errors](#capability-errors)

### MSI Source to Virtual Interrupt Controller Attachment

Attaches a message-signalled interrupt (MSI) source object to a Virtual Interrupt Controller, permitting interrupt messages from the source to be routed to virtual interrupts. The Virtual Interrupt Controller object must have been activated before this function is called. An attachment index must be specified which is unique among the MSI source attachments to the controller. If the MSI source has a memory-mapped interface, the attachment index may be used to determine its address in the VM address space.

Each MSI source capability represents one or more physical devices or buses. Capabilities are provided to the root VM at boot time and cannot be created dynamically, though some MSI source objects may permit capabilities to be derived with restricted rights. The number of MSI sources available depends on the target platform, and may be zero.

In the current implementation, the only type of MSI source supported is a GICv4 ITS. One MSI source capability is provided to the root VM for each physical ITS present in the system.

|    **Hypercall**:       |      `vic_bind_msi_source`                                    |
|-------------------------|---------------------------------------------------------------|
|     Call number:        |     `hvc 0x6056`                                              |
|     Inputs:             |     X0: Virtual IC CapID                                      |
|                         |     X1: MSI Source (platform-specific object type)   CapID    |
|                         |     X2: Index                                                 |
|                         |     X3: Reserved — Must be Zero                               |
|     Outputs:            |     X0: Error Result                                          |


**Errors:**

OK – the operation was successful, and the result is valid.

ERROR_NOMEM – the operation failed due to memory allocation error.

ERROR_ARGUMENT_INVALID – the specified attachment index is outside the range supported by this Virtual Interrupt Controller.

ERROR_OBJECT_STATE – The VCPU object has already been activated, or the Virtual Interrupt Controller object has not yet been activated.

Also see: [Capability Errors](#capability-errors)

## Address Space Management

### Address Space to Thread Attachment

Attaches an address space to a thread. The address space object must have been activated before this function is called. The thread object must not have been activated. It will be detached only during the deactivation of the thread.

|    **Hypercall**:       |      `addrspace_attach_thread`       |
|-------------------------|--------------------------------------|
|     Call number:        |     `hvc 0x602a`                     |
|     Inputs:             |     X0: Address Space CapID          |
|                         |     X1: Thread CapID                 |
|                         |     X2: Reserved — Must be Zero      |
|     Outputs:            |     X0: Error Result                 |

**Errors:**

OK – the operation was successful, and the result is valid.

ERROR_ARGUMENT_INVALID – a value passed in an argument was invalid. This could be due to a thread of kind different from VCPU.

ERROR_OBJECT_STATE – The Thread object has already been activated, or the Address Space object has not yet been activated.

Also see: [capability errors](#capability-errors)

### Address Space Map

Map a memory extent into a specified address space. By default, the entire memory extent is mapped, except for any carveouts contained within the extent.

If the Partial flag is set in Map Flags, only the range of the memory extent specified by Offset and Size will be mapped. If not set, these arguments are ignored. Partial mappings are only supported by sparse memory extents.

If successful, the hypervisor will automatically synchronise with other cores to ensure they have observed the map operation. This behaviour is skipped if the NoSync flag is set.

|    **Hypercall**:       |      `addrspace_map`                 |
|-------------------------|--------------------------------------|
|     Call number:        |     `hvc 0x602b`                     |
|     Inputs:             |     X0: Address Space CapID          |
|                         |     X1: Memory Extent CapID          |
|                         |     X2: Base VMAddr                  |
|                         |     X3: Map Attributes               |
|                         |     X4: Map Flags                    |
|                         |     X5: Offset                       |
|                         |     X6: Size                         |
|     Outputs:            |     X0: Error Result                 |

**Types:**

*Map Attributes:*

| Bits | Mask | Description |
|-|---|-----|
|     2..0             |     `0x7`                  |     User Access (if Supported)    |
|     6..4             |     `0x70`                 |     Kernel Access                 |
|     23:16            |     `0xFF0000`             |     Memory Type                   |
|     63:24,15:7,3     |     `0xFFFFFFFF.0000FF88`  |     Reserved,   Must be Zero      |

*Map Flags:*

| Bits | Mask | Description |
|-|---|-----|
|     0                |     `0x1`                  |     Partial                       |
|     31               |     `0x80000000`           |     NoSync                        |
|     30:1             |     `0x7FFFFFFE`           |     Reserved, Must be Zero        |

**Errors:**

OK – the operation was successful, and the result is valid.

ERROR_ARGUMENT_INVALID – a value passed in an argument was invalid. This could be due to an invalid Address Space.

ERROR_MEMEXTENT_MAPPINGS_FULL – the memory extent has exceeded its mappings capacity. Currently it can have up to 4 mappings.

ERROR_DENIED – the specified Address Space is not allowed to execute map operations.

ERROR_ARGUMENT_ALIGNMENT – the specified base address is not page size aligned.

ERROR_ADDR_OVERFLOW – the specified base address may cause an overflow.

Also see: [capability errors](#capability-errors)

### Address Space Unmap

Unmaps a memory extent from a specified address space. By default, the entire memory extent range is unmapped, except for any carveouts contained within the extent.

If the Partial flag is set in Map Flags, only the range of the Memory Extent specified by Offset and Size will be unmapped. If not set, these arguments are ignored. Partial unmappings are only supported by sparse memory memextents.

If successful, the hypervisor will automatically synchronise with other cores to ensure they have observed the unmap operation. This behaviour is skipped if the NoSync flag is set.

|    **Hypercall**:       |      `addrspace_unmap`               |
|-------------------------|--------------------------------------|
|     Call number:        |     `hvc 0x602c`                     |
|     Inputs:             |     X0: Address Space CapID          |
|                         |     X1: Memory Extent CapID          |
|                         |     X2: Base VMAddr                  |
|                         |     X3: Map Flags                    |
|                         |     X4: Offset                       |
|                         |     X5: Size                         |
|     Outputs:            |     X0: Error Result                 |

**Errors:**

OK – the operation was successful, and the result is valid.

ERROR_ARGUMENT_INVALID – a value passed in an argument was invalid. This could be due to an invalid Address Space or a non-existing mapping.

ERROR_DENIED – the specified Address Space is not allowed to execute map operations.

ERROR_ARGUMENT_ALIGNMENT – the specified base address is not page size aligned.

Also see: [capability errors](#capability-errors)

### Address Space Update Access

Update access rights on an existing mapping.

If the Partial flag is set in Map Flags, only the range of the Memory Extent specified by Offset and Size will be updated. If not set, these arguments are ignored. Partial access updates are only supported by sparse memory extents.

If successful, the hypervisor will automatically synchronise with other cores to ensure they have observed the mapping update. This behaviour is skipped if the NoSync flag is set.

|    **Hypercall**:       |      `addrspace_update_access`       |
|-------------------------|--------------------------------------|
|     Call number:        |     `hvc 0x602d`                     |
|     Inputs:             |     X0: Address Space CapID          |
|                         |     X1: Memory Extent CapID          |
|                         |     X2: Base VMAddr                  |
|                         |     X3: Update Attributes            |
|                         |     X4: Map Flags                    |
|                         |     X5: Offset                       |
|                         |     X6: Size                         |
|     Outputs:            |     X0: Error Result                 |

**Types:**

*Update Attributes:*

| Bits | Mask | Description |
|-|---|-----|
|     2..0             |     `0x7`                  |     User Access (if Supported)    |
|     6..4             |     `0x70`                 |     Kernel Access                 |
|     63:7,3           |     `0xFFFFFFFF.FFFFFF88`  |     Reserved,   Must be Zero      |

**Errors:**

OK – the operation was successful, and the result is valid.

ERROR_ARGUMENT_INVALID – a value passed in an argument was invalid. This could be due to an invalid Address Space or a non-existing mapping.

ERROR_ARGUMENT_ALIGNMENT – the specified base address is not page size aligned.

ERROR_DENIED – the specified Address Space is not allowed to update access of mappings.

Also see: [capability errors](#capability-errors)

### Configure an Address Space

Configure an address space whose state is OBJECT_STATE_INIT.

|    **Hypercall**:       |     `addrspace_configure`            |
|-------------------------|--------------------------------------|
|     Call number:        |     `hvc 0x602e`                     |
|     Inputs:             |     X0: Address Space CapID          |
|                         |     X1: VMID                         |
|                         |     X2: Reserved — Must be Zero      |
|     Outputs:            |     X0: Error Result                 |

**Types:**

16-Bit VMID, upper bits reserved and must be zero.

**Errors:**

OK – the operation was successful, and the result is valid.

ERROR_OBJECT_STATE – the Address Space object has already been activated.

ERROR_ARGUMENT_INVALID – a value passed in an argument was invalid. This could be due to an invalid VMID.

Also see: [capability errors](#capability-errors)

### Configure the information area of an Address Space

Configure the information area of an address space whose state is OBJECT_STATE_INIT.

|    **Hypercall**:       |     `addrspace_configure_info_area`  |
|-------------------------|--------------------------------------|
|     Call number:        |     `hvc 0x605b`                     |
|     Inputs:             |     X0: Address Space CapID          |
|                         |     X1: Info area memextent CapID    |
|                         |     X2: Info area IPA                |
|                         |     X3: Reserved — Must be Zero      |
|     Outputs:            |     X0: Error Result                 |

**Errors:**

OK – The operation was successful, and the result is valid.

ERROR_OBJECT_STATE – The Address Space object has already been activated.

ERROR_ADDR_INVALID – The provided IPA is invalid.

ERROR_ARGUMENT_INVALID – A value passed in an argument was invalid.

Also see: [capability errors](#capability-errors)

### Address Space to DMA-capable Object Attachment

Attaches an address space to any type of object that has a virtual DMA port which it can use to independently access memory in a VM address space. For types of object that have more than one virtual DMA port (e.g. a DMA-based IPC object), an index may be specified to indicate which port should be attached. Note that VCPUs do not access the VM address spaces through a virtual DMA port when executing VM code; they use a separate attachment call, described in [section](#address-space-to-thread-attachment) above.

Object types with virtual DMA ports generally require that this function is called before they are activated, unless use of the virtual DMA port is optional.

In the current implementation, the only object type with a virtual DMA port is the GICv3 compatible Virtual Interrupt Controller. The port is only present if the underlying physical interrupt controller is an GICv4, and GICv3 LPI support is enabled for the Virtual Interrupt Controller. If the port is present, it must be attached to an address space before the Virtual Interrupt Controller is activated.

|    **Hypercall**:       |      `addrspace_attach_vdma`                |
|-------------------------|---------------------------------------------|
|     Call number:        |     `hvc 0x602f`                            |
|     Inputs:             |     X0: Address Space CapID                 |
|                         |     X1: Virtual DMA-capable Object CapID    |
|                         |     X2: Virtual DMA Port Index              |
|                         |     X3: Reserved — Must be Zero             |
|     Outputs:            |     X0: Error Result                        |

**Errors:**

OK – the operation was successful.

ERROR_NOMEM – the operation failed due to memory allocation error.

ERROR_ARGUMENT_INVALID – the specified object is virtual DMA capable, but the port index is outside the valid range for the object.

ERROR_CSPACE_WRONG_OBJECT_TYPE – the specified virtual device object does not have any virtual DMA ports; or the specified address space object is not an address space.

ERROR_BUSY – the specified port already has an address space attached, and the object does not support changing an existing attachment.

ERROR_OBJECT_STATE – the Address Space object has not yet been activated.

Also see: [capability errors](#capability-errors)

### Address Space to Virtual Device Attachment

Attaches an address space to any type of object that presents a virtual memory-mapped device register interfaces. For types of object that have more than one virtual device interface, an index may be specified to indicate which interface should be attached. The meaning of this index depends on the object type.

After this call succeeds, accesses by any VCPU attached to the address space that lie within the specified IPA range and fault in the IPA translation will be forwarded to the specified virtual device for emulation. The addresses, access sizes, access types, and semantics of the emulated registers depend entirely on the device implementation. Also, the behaviour of any access that does not match an emulated register depends on the device implementation, and may include either faulting as if the virtual device was not attached, or returning a constant value (typically 0 or 0xff) for reads and ignoring writes.

Note that the register interface will not function correctly if any memory extent is mapped in the specified IPA range. The hypervisor will not check for such overlapping mappings.

The specified IPA range must be large enough to contain the selected register interface, and must not be attached to any other virtual device. If the specified range is undersized, some registers may not be accessible. If the specified range is oversized, any extra space will become unavailable to other virtual devices; the behaviour of an access to this extra space is unspecified.

|    **Hypercall**:       |      `addrspace_attach_vdevice`             |
|-------------------------|---------------------------------------------|
|     Call number:        |     `hvc 0x6062`                            |
|     Inputs:             |     X0: Address Space CapID                 |
|                         |     X1: Virtual Device Object CapID         |
|                         |     X2: Virtual Device Interface Index      |
|                         |     X3: Base IPA                            |
|                         |     X4: Size                                |
|                         |     X5: Reserved — Must be Zero             |
|     Outputs:            |     X0: Error Result                        |

**Index values:**

| **Type** | **Index** | **Size** | **Description** |
|--|-|--|-----|
| vGIC | 0 | 64KiB | GIC Distributor registers |
| vGIC | 1..N | 64KiB | GIC Redistributor registers for VCPUs 0..(N-1) |
| vITS | 0 | 64KiB | GIC ITS registers |

**Errors:**

OK – the operation was successful.

ERROR_NOMEM – the operation failed due to memory allocation error.

ERROR_ARGUMENT_INVALID – the specified object is a virtual device, but the interface index is outside the valid range for the object.

ERROR_CSPACE_WRONG_OBJECT_TYPE – the specified virtual device object does not support memory-mapped interfaces or is not a virtual device; or the specified address space object is not an address space.

ERROR_BUSY – the specified address range already contains a virtual device.

ERROR_OBJECT_STATE – the Address Space object has not yet been activated.

Also see: [capability errors](#capability-errors)

### Address Space Lookup

Lookup a memextent mapping in an address space. If successful, returns the offset and size within the memextent, as well as the attributes of the mapping.

|    **Hypercall**:       |      `addrspace_lookup`              |
|-------------------------|--------------------------------------|
|     Call number:        |     `hvc 0x605a`                     |
|     Inputs:             |     X0: Address Space CapID          |
|                         |     X1: Memory Extent CapID          |
|                         |     X2: Base VMAddr                  |
|                         |     X3: Size                         |
|                         |     X4: Reserved — Must be Zero      |
|     Outputs:            |     X0: Error Result                 |
|                         |     X1: Offset                       |
|                         |     X2: Size                         |
|                         |     X3: Map Attributes               |

**Types:**

*Map Attributes:*

See: [Address Space Map](#address-space-map)

**Errors:**

OK – the operation was successful, and the result is valid.

ERROR_ARGUMENT_INVALID – one of the given arguments is invalid. This could be due to an invalid Address Space.

ERROR_ARGUMENT_SIZE – the specified size is invalid.

ERROR_ARGUMENT_ALIGNMENT – the specified base address or size is not page size aligned.

ERROR_ADDR_OVERFLOW – the specified base address may cause an overflow.

ERROR_ADDR_INVALID – the specified base address is not mapped in the Address Space.

ERROR_MEMDB_NOT_OWNER – the memory mapped in the Address Space is not owned by the specified Memory Extent.

Also see: [capability errors](#capability-errors)

### Address Space Virtual MMIO Area Configuration

Configure the virtual MMIO device regions for the address space.

A virtual MMIO device region is a region of the address space in which translation faults may be handled by an unprivileged VMM residing in another VM.
This allows the unprivileged VMM to emulate memory-mapped I/O devices.
Note that other types of fault, such as permission or alignment faults, cannot be handled by this mechanism.
Also, depending on the architecture, this mechanism may only support translation faults generated by specific types of instruction.
On AArch64, it is limited to single-register load & store instructions without base register writeback, which are decoded by the CPU into the `ESR_EL2` syndrome bits.

This call may be made before or after activation of the address space object.
This is to permit delegation of the right to call this API to the VM that runs in the address space, so it can explicitly acknowledge that the specified region should not be used for sensitive data.

An address range that is added must not overlap any existing range, and must not wrap around the end of the address space.
There are no other restrictions on the size or alignment of ranges added to the address space.
However, a limit may be imposed on the total number of ranges added to an address space.

A removed address range must exactly match a single previously added address range. Note that removal of a range will prevent the VMM receiving any new faults that occur in that range after the removal operation completes, but does not guarantee that the VMM has finished handling all faults in the removed range.

|    **Hypercall**:       |      `addrspace_configure_vmmio`   |
|-------------------------|------------------------------------|
|     Call number:        |     `hvc 0x6060`                   |
|     Inputs:             |     X0: Address Space CapID        |
|                         |     X1: Base VMAddr                |
|                         |     X2: Size                       |
|                         |     X3: VMMIOConfigureOperation    |
|                         |     X4: Reserved   – Must be Zero  |
|     Outputs:            |     X0: Error Result               |

**Types:**

*VMMIOConfigureOperation:*

|      Operation Enumerator               |      Integer Value     |
|-----------------------------------------|------------------------|
|     VMMIO_CONFIGURE_OP_ADD_RANGE        |     0                  |
|     VMMIO_CONFIGURE_OP_REMOVE_RANGE     |     1                  |

**Errors:**

OK – the operation was successful.

ERROR_ADDR_OVERFLOW – the specified range wraps around the end of the address space.

ERROR_ADDR_INVALID – the specified range is not completely within the input address range of the address space.

ERROR_ARGUMENT_INVALID – the specified range to be added overlaps a previously added range, or the specified range to be removed does not match a previously added range.

ERROR_NORESOURCES – the number of nominated ranges has reached an implementation-defined limit, or the hypervisor was unable to allocate memory for bookkeeping.

ERROR_UNIMPLEMENTED — unprivileged VMMs are unable to handle faults in this configuration, or an unknown operation was requested.

Also see: [capability errors](#capability-errors)

## Memory Extent Management

### Memory Extent Modify

Perform a modification on a memory extent.

For range operations, only the range of the memory extent specified by Offset and Size will be modified. For all other operations these arguments are ignored.

For operations that affect address space mappings, the hypervisor will automatically synchronise with other cores to ensure they have observed any successful changes in mappings. This behaviour is skipped if the NoSync flag is set. For other operations the NoSync flag must be set as specified below.

|    **Hypercall**:       |      `memextent_modify`              |
|-------------------------|--------------------------------------|
|     Call number:        |     `hvc 0x6030`                     |
|     Inputs:             |     X0: Memory Extent CapID          |
|                         |     X1: Memextent Modify Flags       |
|                         |     X2: Offset                       |
|                         |     X3: Size                         |
|     Outputs:            |     X0: Error Result                 |

**Types:**

*MemExtent Modify Flags:*

|     Bit Numbers     |      Mask         |     Description                 |
|---------------------|-------------------|---------------------------------|
|     7:0             |     `0xFF`        |     Memextent Modify Operation  |
|     31              |     `0x80000000`  |     NoSync                      |
|     30:8            |     `0x7FFFFF00`  |     Reserved, Must be Zero      |

*MemExtent Modify Operation:*

|   Modify Operation                        |   Integer Value   |   Description                                                                                          |
|-------------------------------------------|-------------------|--------------------------------------------------------------------------------------------------------|
|   MEMEXTENT_MODIFY_OP_UNMAP_ALL           |   0               |   Unmap the memory extent from all address spaces it was mapped into.                                  |
|   MEMEXTENT_MODIFY_OP_ZERO_RANGE          |   1               |   Zero the owned memory of an extent within the specified range. The NoSync flag must be set.          |
|   MEMEXTENT_MODIFY_OP_CACHE_CLEAN_RANGE   |   2               |   Cache clean the owned memory of an extent within the specified range. The NoSync flag must be set.   |
|   MEMEXTENT_MODIFY_OP_CACHE_FLUSH_RANGE   |   3               |   Cache flush the owned memory of an extent within the specified range. The NoSync flag must be set.   |
|   MEMEXTENT_MODIFY_OP_SYNC_ALL            |   255             |   Synchronise all previous memory extent operations. The NoSync flag must not be set.                  |

**Errors:**

OK – the operation was successful, and the result is valid.

ERROR_ARGUMENT_INVALID – the specified modify flags are invalid.

Also see: [Capability Errors](#capability-errors)

### Configure a Memory Extent

Configure a memory extent whose state is OBJECT_STATE_INIT.

|    **Hypercall**:       |      `memextent_configure`           |
|-------------------------|--------------------------------------|
|     Call number:        |     `hvc 0x6031`                     |
|     Inputs:             |     X0: Memory Extent CapID          |
|                         |     X1: Phys Base                    |
|                         |     X2: Size                         |
|                         |     X3: MemExtent Attributes         |
|                         |     X4: Reserved — Must be Zero      |
|     Outputs:            |     X0: Error Result                 |

**Types:**

*MemExtent Attributes:*

| Bits | Mask | Description |
|-|---|-----|
|     2..0             |     `0x7`         |     Access Rights               |
|     9:8              |     `0x300`       |     MemExtent MemType           |
|     17:16            |     `0x30000`     |     MemExtent Type              |
|     31               |     `0x80000000`  |     List Append                 |
|     30:18,15:10,7:3  |     `0x7FFCFCF8`  |     Reserved,   Must be Zero    |

*Memextent Type*

|    Memextent Type     |   Integer Value     |   Description                                        |
|-----------------------|---------------------|------------------------------------------------------|
|     BASIC             |    0                |    Extent with basic functionality.                  |
|     SPARSE            |    1                |    Extent supporting donation and partial mappings.  |

*Memextent MemType*

|    Memextent MemType    |   Integer Value     |   Description                                      |
|-------------------------|---------------------|----------------------------------------------------|
|     ANY                 |    0                |    Allow mappings of any memory type.              |
|     DEVICE              |    1                |    Restrict mappings to device memory types only.  |
|     UNCACHED            |    2                |    Force mappings to be uncached.                  |
|     CACHED              |    3                |    Force mappings to be writeback cacheable.       |

**Errors:**

OK – the operation was successful, and the result is valid.

ERROR_ARGUMENT_INVALID – a value passed in an argument was invalid. This could be due to an invalid size or base address.

Also see: [Capability Errors](#capability-errors)

### Configure a Derived Memory Extent

Configure a derived memory extent whose state is OBJECT_STATE_INIT. The extent will be derived from the specified parent and its base address will be the base address of the parent plus the indicated offset.

|    **Hypercall**:       |      `memextent_configure_derive`     |
|-------------------------|---------------------------------------|
|     Call number:        |     `hvc 0x6032`                      |
|     Inputs:             |     X0: Memory Extent CapID           |
|                         |     X1: Parent Memory Extent CapID    |
|                         |     X2: Offset                        |
|                         |     X3: Size                          |
|                         |     X4: MemExtent Attributes          |
|                         |     X5: Reserved — Must be Zero       |
|     Outputs:            |     X0: Error Result                  |

**Errors:**

OK – the operation was successful, and the result is valid.

ERROR_ARGUMENT_INVALID – a value passed in an argument was invalid. This could be due to an invalid size or offset.

Also see: [Capability Errors](#capability-errors)

### Memory Extent Donate

Donate memory from one extent to another. This includes donations from parent to child, child to parent and between siblings.

For non-derived memory extents, the parent is considered to be the partition that was used to create the extent. Donation is only supported for sparse memory extents.

If successful, the hypervisor will automatically synchronise with other cores to ensure they have observed the donation and any mapping changes that may have occurred. This behaviour is skipped if the NoSync flag is set.

|    **Hypercall**:       |      `memextent_donate`               |
|-------------------------|---------------------------------------|
|     Call number:        |     `hvc 0x6033`                      |
|     Inputs:             |     X0: Memextent Donate Options      |
|                         |     X1: From CapID                    |
|                         |     X2: To CapID                      |
|                         |     X3: Offset                        |
|                         |     X4: Size                          |
|                         |     X5: Reserved — Must be Zero       |
|     Outputs:            |     X0: Error Result                  |

**Types:**

*Memextent Donate Options*

| Bits | Mask | Description |
|-|---|-----|
|     7:0             |     `0xFF`          |     Memextent Donate Type               |
|     31              |     `0x80000000`    |     NoSync                              |
|     30:8            |     `0x7FFFFF00`    |     Reserved — Must be Zero             |

*Memextent Donate Type*

|    Memextent Donate Type     |   Integer Value     |   Description                                   |
|------------------------------|---------------------|-------------------------------------------------|
|     TO_CHILD                 |    0                |    Donate to a child extent from its parent.    |
|     TO_PARENT                |    1                |    Donate from a child extent to its parent.    |
|     TO_SIBLING               |    2                |    Donate from one sibling extent to another.   |

**Errors:**

OK – the operation was successful, and the result is valid.

ERROR_ARGUMENT_INVALID – a value passed in an argument was invalid. This could be due to an invalid donate option, offset or size.

ERROR_ARGUMENT_SIZE – the Size provided is zero, or leads to an overflow.

ERROR_MEMDB_NOT_OWNER – the donating memory extent did not have ownership of the specified memory range.

Also see: [Capability Errors](#capability-errors)

## VCPU Management

### Configure a VCPU Thread

Configure a VCPU Thread whose state is OBJECT_STATE_INIT.

|    **Hypercall**:       |      `vcpu_configure`                |
|-------------------------|--------------------------------------|
|     Call number:        |     `hvc 0x6034`                     |
|     Inputs:             |     X0:   vCPU CapID                 |
|                         |     X1: vCPUOptionFlags              |
|                         |     X2: Reserved — Must be Zero      |
|     Outputs:            |     X0: Error Result                 |

**Types:**

*vCPUOptionFlags:*

| Bits | Mask | Description |
|-|---|-----|
|     0                |     `0x1`                  |     AArch64 Self-hosted Debug Enable    |
|     1                |     `0x2`                  |     VCPU containing HLOS VM             |
|     63:2             |     `0xFFFFFFFF.FFFFFFFE`  |     Reserved,   Must be Zero            |

AArch64 Self-hosted Debug: give the VCPU access to use AArch64 Self-hosted debug functionality and registers.

**Errors:**

OK – the operation was successful, and the result is valid.

ERROR_OBJECT_STATE – if the VCPU object is not in OBJECT_STATE_INIT state.

ERROR_ARGUMENT_INVALID – a value passed in an argument was invalid. This could be due to an invalid VCPU or option flag.

Also see: [Capability Errors](#capability-errors)

### Set or Change the Physical CPU Affinity of a VCPU Thread

Set the physical CPU that will schedule the specified VCPU thread.

This may be called for any VCPU thread object whose state is OBJECT_STATE_INIT. If the scheduler implementation supports migration of active threads, it may also be called for a VCPU thread object whose state is OBJECT_STATE_ACTIVE.

If the scheduler supports directed yields and/or automatic migration of threads, calling this function on a VCPU thread object prior to activation is optional. Otherwise, it is mandatory, and the object_activate call will fail with an ERROR_OBJECT_CONFIG result if it has not been called.

If the call targets a VCPU that is currently running on a different physical CPU to the one making the call, the affinity change is asynchronous; that is, the VCPU may still be running on the same physical CPU when it returns. The hypervisor will signal the affected physical CPU to stop execution of the VCPU as soon as possible, but makes no guarantee that this will happen within any specific time period.

|    **Hypercall**:       |      `vcpu_set_affinity`           |
|-------------------------|------------------------------------|
|     Call number:        |     `hvc 0x603d`                   |
|     Inputs:             |     X0:   vCPU CapID               |
|                         |     X1: Affinity CPUIndex          |
|                         |     X2: Reserved — Must be -1      |
|     Outputs:            |     X0: Error Result               |

**Types:**

CPUIndex — a number identifying the target physical CPU.

For hardware platforms with physical CPUs that are linearly numbered from 0, this is equal to the physical CPU number; for AArch64 platforms, this is the case if three of the four affinity fields in `MPIDR_EL1` have a zero value on every physical PE, and the CPUIndex corresponds to the value of the remaining `MPIDR_EL1` affinity field. Otherwise, the hypervisor’s platform driver defines the mapping between CPUIndex values and physical CPUs, and VMs may be informed of this mapping at boot time via the boot environment data.

The value -1 (`CPU_INDEX_INVALID`) may be used to indicate that the VCPU should not have affinity to any physical CPU. If the scheduler does not support automatic migration of threads, this will effectively disable the VCPU, so an additional object right (Thread Disable) is required in this case.

**Errors:**

OK – the operation was successful.

ERROR_OBJECT_STATE – the specified VCPU thread is active and the scheduler does not support migration of active threads.

ERROR_ARGUMENT_INVALID – the affinity value specified is out of range.

ERROR_DENIED – the specified VCPU is not permitted to change affinity because a physical-CPU-local resource, such as a private interrupt, has been assigned to it.

Also see: [Capability Errors](#capability-errors)

### Write to the Register Context of a VCPU Thread

Write a specified value to one of a VCPU's registers.

This may be called for any VCPU thread object that is currently in a virtual power-off state.
This includes VCPU objects that have not yet been activated.
Note that powering on a VCPU using a platform-specific power control API, such as `PSCI_CPU_ON`, might overwrite values set by this call.

The register to write is identified by an architecture-specific enumeration identifying the set or group of registers, and an index into that set or group.
The primary purpose of this hypercall is to set the initial state of a VCPU before it is powered on.
Therefore, the architecture will typically only define access to the general-purpose registers, excluding extended register sets such as system control registers and floating-point or vector registers.

|    **Hypercall**:       |      `vcpu_register_write`           |
|-------------------------|--------------------------------------|
|     Call number:        |     `hvc 0x6064`                     |
|     Inputs:             |     X0: vCPU CapID                   |
|                         |     X1: RegisterSet                  |
|                         |     X2: Index                        |
|                         |     X3: Value                        |
|                         |     X4: Reserved — Must be Zero      |
|     Outputs:            |     X0: Error Result                 |

**Types:**

*RegisterSet (AArch64)*

| **RegisterSet** | **Name** | **Indices** | **Description** |
|-|---|-|-----|
| 0 | `VCPU_REGISTER_SET_X` | 0–31 | 64-bit general purpose registers X0-X30 |
| 1 | `VCPU_REGISTER_SET_PC` | 0 | Program counter (4-byte aligned) |
| 2 | `VCPU_REGISTER_SET_SP_EL` | 0–1 | Stack pointers for EL0 and EL1 |

### Power on a VCPU Thread

Bring a VCPU Thread out of its initial virtual power-off state.

This call can also set the minimal initial execution state of the VCPU, including its entry point and a context pointer, avoiding the need to call `vcpu_register_write`.
The hypervisor does not dereference, check, or otherwise define any particular meaning for the context pointer.
It will be written to the first argument register in the VCPU's standard calling convention; for an AArch64 VCPU, this is X0.

The entry point and context pointer each have a corresponding flag in the flags argument which will cause this call to discard the provided value and preserve the current state of the respective VCPU register.

|    **Hypercall**:       |      `vcpu_poweron`                  |
|-------------------------|--------------------------------------|
|     Call number:        |     `hvc 0x6038`                     |
|     Inputs:             |     X0:   vCPU CapID                 |
|                         |     X1: EntryPointAddr VMPhysAddr    |
|                         |     X2: ContextPtr Register          |
|                         |     X3: vCPUPowerOnFlags             |
|     Outputs:            |     X0: Error Result                 |

**Types:**

*vCPUPowerOnFlags:*

| Bits | Mask | Description |
|-|---|-----|
|     0                |     `0x1`                  |     Preserve entry point        |
|     1                |     `0x2`                  |     Preserve context            |
|     63:2             |     `0xFFFFFFFF.FFFFFFFC`  |     Reserved — Must be Zero     |

**Errors:**

OK – the operation was successful, and the result is valid.

ERROR_ARGUMENT_INVALID – a value passed in an argument was invalid. This could be due to an invalid VCPU.

ERROR_BUSY – the specified VCPU is currently busy and cannot be powered on at the moment.

Also see: [Capability Errors](#capability-errors)

### Power off a VCPU Thread

Halt execution of the calling VCPU, and apply architecture-defined reset values to its register context.
The effect of the reset is architecture-specific, but will typically disable the first stage of address translation, and may also disable caches, mask interrupts, etc.

This call will not return when successful.

The specified VCPU capability must refer to the calling VCPU. Specifying any other VCPU is invalid.

The last-VCPU bit in the flags argument must be set if, and only if, the caller is either the sole powered-on VCPU attached to a Virtual PM Group, or not attached to a Virtual PM Group at all. If this flag is not set correctly, the call may return ERROR_DENIED. This requirement prevents a VM inadvertently powering off all of its VCPUs, which is a state it cannot recover from without outside assistance.

|    **Hypercall**:       |      `vcpu_poweroff`                 |
|-------------------------|--------------------------------------|
|     Call number:        |     `hvc 0x6039`                     |
|     Inputs:             |     X0:   vCPU CapID                 |
|                         |     X1: vCPUPowerOffFlags            |
|     Outputs:            |     X0: Error Result                 |

**Types:**

*vCPUPowerOffFlags:*

| Bits | Mask | Description |
|-|---|-----|
|     0                |     `0x1`                  |     Last VCPU to power off in VM |
|     63:1             |     `0xFFFFFFFF.FFFFFFFE`  |     Reserved — Must be Zero      |

**Errors:**

ERROR_ARGUMENT_INVALID – a value passed in an argument was invalid. This could be due to an unrecognised flag value, or specifying a VCPU that is not the caller.

ERROR_DENIED — the caller is the sole powered-on VCPU in a Virtual PM Group, and the last-VCPU flag was not set; or the caller is not the sole powered-on VCPU in a Virtual PM Group, and the last-VCPU flag was set.

Also see: [Capability Errors](#capability-errors)

### Set Priority of a VCPU Thread

Set a VCPU thread’s priority (if supported by the scheduler).

This may be called for any VCPU thread object whose state is OBJECT_STATE_INIT.

For the fixed priority round-robin scheduler, priorities range from 0 (lowest) to 63 (highest). If no priority is explicitly set, VCPU threads will default to a priority of 32.

|    **Hypercall**:       |      `vcpu_set_priority`   |
|-------------------------|----------------------------|
|     Call number:        |     `hvc 0x6046`           |
|     Inputs:             |     X0: VCPU CapID         |
|                         |     X1: Priority           |
|     Outputs:            |     X0: Error Result       |

**Errors:**

OK – The operation was successful.

ERROR_OBJECT_STATE – the specified VCPU thread is not in the init state.

ERROR_ARGUMENT_INVALID – the priority value specified is out of range.

Also see: [Capability Errors](#capability-errors)

### Set Timeslice of a VCPU Thread

Set a VCPU thread’s timeslice (if supported by the scheduler). Timeslices are specified in nanoseconds.

This may be called for any VCPU thread object whose state is OBJECT_STATE_INIT.

For the fixed priority round-robin scheduler, timeslices can range from 1ms to 100ms. If no timeslice is explicitly set, VCPU threads will default to a timeslice of 5ms.

|    **Hypercall**:       |      `vcpu_set_timeslice`   |
|-------------------------|-----------------------------|
|     Call number:        |     `hvc 0x6047`            |
|     Inputs:             |     X0: VCPU CapID          |
|                         |     X1: Timeslice           |
|     Outputs:            |     X0: Error Result        |

**Errors:**

OK – The operation was successful.

ERROR_OBJECT_STATE – the specified VCPU thread is not in the init state.

ERROR_ARGUMENT_INVALID – the timeslice value specified is out of range.

Also see: [Capability Errors](#capability-errors)

### VCPU vIRQ Bind

Each VCPU may have one or more associated virtual interrupt sources, depending on its configuration. This API binds one of those sources to a virtual IRQ number.

If the IRQ type is set to `VCPU_RUN_WAKEUP`, binding the IRQ will automatically place the VCPU into a state in which it can only be scheduled by calling `vcpu_run`. Refer to the [documentation](#run-a-proxy-scheduled-vcpu-thread) for that hypercall for further details.

|    **Hypercall**:       |      `vcpu_bind_virq`                |
|-------------------------|--------------------------------------|
|     Call number:        |     `hvc 0x605c`                     |
|     Inputs:             |     X0: VCPU CapID                   |
|                         |     X1: Virtual IC CapID             |
|                         |     X2: Virtual IRQ Info             |
|                         |     X3: VCPU Virtual IRQ Type        |
|                         |     X4: Reserved — Must be Zero      |
|     Outputs:            |     X0: Error Result                 |

**Types:**

|      VCPU Virtual IRQ Type     |      Integer Value     |
|--------------------------------|------------------------|
|     VCPU_RUN_WAKEUP            |     1                  |

**Errors:**

OK – the operation was successful, and the result is valid.

ERROR_NOMEM – the operation failed due to memory allocation error.

ERROR_VIRQ_BOUND – the specified VCPU is already bound to a VIRQ number.

ERROR_BUSY – the specified VIRQ number is already bound to a source.

ERROR_ARGUMENT_INVALID – a value passed in an argument was invalid. This could be due to an invalid Virtual IRQ Info value.

Also see: [Capability Errors](#capability-errors)

### VCPU vIRQ Unbind

Unbinds a VCPU interrupt source from a virtual IRQ number.

If the IRQ type is set to `VCPU_RUN_WAKEUP`, unbinding the IRQ will allow the VCPU to run without a `vcpu_run` call, subject to its normal scheduling parameters and state. Note that in some cases this can cause incorrect execution in the VCPU. Refer to the [documentation](#run-a-proxy-scheduled-vcpu-thread) for that hypercall for further details.

|    **Hypercall**:       |      `vcpu_unbind_virq`              |
|-------------------------|--------------------------------------|
|     Call number:        |     `hvc 0x605d`                     |
|     Inputs:             |     X0: VCPU CapID                   |
|                         |     X1: VCPU Virtual IRQ Type        |
|                         |     X2: Reserved — Must be Zero      |
|     Outputs:            |     X0: Error Result                 |

**Types:**

|      VCPU Virtual IRQ Type     |      Integer Value     |
|--------------------------------|------------------------|
|     VCPU_RUN_WAKEUP            |     1                  |

**Errors:**

OK – the operation was successful, or the VCPU interrupt was already unbound.

Also see: [Capability Errors](#capability-errors)

### Kill a VCPU thread

Places the VCPU thread in a killed state, forcing it to exit and end execution. The VCPU can no longer be scheduled once it has exited. If the calling VCPU is targeting itself, this call will not return if successful.

|    **Hypercall**:       |      `vcpu_kill`                     |
|-------------------------|--------------------------------------|
|     Call number:        |     `hvc 0x603a`                     |
|     Inputs:             |     X0: VCPU CapID                   |
|                         |     X1: Reserved — Must be Zero      |
|     Outputs:            |     X0: Error Result                 |

**Errors:**

OK – the operation was successful.

ERROR_ARGUMENT_INVALID – a value passed in an argument was invalid. This could be due to an invalid VCPU.

ERROR_OBJECT_STATE – the VCPU thread was not active, or has already been killed.

Also see: [Capability Errors](#capability-errors)

### Run a Proxy-Scheduled VCPU thread

Donates CPU time to a VCPU that is configured for proxy scheduling. This is an optional mechanism that gives a privileged VM's scheduler limited control over the scheduling of another VM's VCPUs.

This call may only be used on a VCPU that has a VIRQ bound to its `VCPU_RUN_WAKEUP` interrupt source. A VCPU that is in that state cannot be scheduled normally by the hypervisor scheduler; it will only execute when this hypercall is used to give it CPU time.

If all arguments are valid, this hypercall will attempt to context-switch to the specified VCPU. It returns when the caller is preempted or when the specified VCPU is unable to continue running. The VCPU state result indicates the reason that it was unable to continue.

Some states may return additional state-specific data to allow the caller to take appropriate actions, and/or require additional data to resume execution which must be passed to the next `vcpu_run` call for the same VCPU. Also, some states may persist for some length of time that can't be directly predicted by the caller; when the VCPU leaves one of these states, it will assert the VIRQ bound to its `VCPU_RUN_WAKEUP` interrupt source.

For this call to behave as intended, the specified VCPU should have lower scheduling priority than the caller. Otherwise, the return from this call may be delayed until execution of the specified VCPU is blocked or its own timeslice expires. This rule is not enforced by the implementation.

|    **Hypercall**:       |      `vcpu_run`                      |
|-------------------------|--------------------------------------|
|     Call number:        |     `hvc 0x6065`                     |
|     Inputs:             |     X0: VCPU CapID                   |
|                         |     X1: State-specific Resume Data 1 |
|                         |     X2: State-specific Resume Data 2 |
|                         |     X3: State-specific Resume Data 3 |
|                         |     X4: Reserved —Must be Zero       |
|     Outputs:            |     X0: Error Result                 |
|                         |     X1: VCPU Run State               |
|                         |     X2: State-specific Data 1        |
|                         |     X3: State-specific Data 2        |
|                         |     X4: State-specific Data 3        |

**Types**:

*VCPU Run State*:

The following table shows the expected types of the state-specific data and resume data for each state. A 0 indicates that the argument or result is currently reserved and must be zero.

| State | Name | State Data 1 | State Data 2 | State Data 3 | Resume Data 1 |
|-|--|--|--|--|--|
| 0x0 | `READY` | 0 | 0 | 0 | 0 |
| 0x1 | `EXPECTS_WAKEUP` | VCPU Sleep Type | 0 | 0 | 0 |
| 0x2 | `POWERED_OFF` | VCPU Poweroff Type | 0 | 0 | 0 |
| 0x3 | `BLOCKED` | 0 | 0 | 0 | 0 |
| 0x4 | `ADDRSPACE_VMMIO_READ` | VMPhysAddr | Size | 0 | Register |
| 0x5 | `ADDRSPACE_VMMIO_WRITE` | VMPhysAddr | Size | Register | 0 |
| 0x100 | `PSCI_SYSTEM_RESET` | PSCI Reset Type | 0 | 0 | 0 |

The Resume Data 2 and 3 arguments are currently unused and must be zero for all states.

0x0 `READY`
:The caller's hypervisor timeslice ended, or the caller received an interrupt. The caller should retry after handling any pending interrupts.

0x1 `EXPECTS_WAKEUP`
:The VCPU is waiting to receive an interrupt; for example, it may have executed a WFI instruction, or made a firmware call requesting entry into a low-power state. In the latter case, the state-specific data in X2 will be a platform-specific nonzero value indicating the requested power state. For a platform that implements Arm's PSCI standard, it is in the same format as the state argument to a `PSCI_CPU_SUSPEND` call. The `VCPU_RUN_WAKEUP` VIRQ will be asserted when the VCPU leaves this state.

0x2 `POWERED_OFF`
:The VCPU has not yet been started by calling `vcpu_poweron`, or has stopped itself by calling `vcpu_poweroff`, or has been terminated due to a reset request from another VM. If PSCI is implemented, this state is also reachable via PSCI calls. The `VCPU_RUN_WAKEUP` VIRQ will be asserted when the VCPU leaves this state. The first state data word contains a VCPU Poweroff Type value (defined below).

0x3 `BLOCKED`
:The VCPU is temporarily unable to run due to a hypervisor operation. This may include a hypercall made by the VCPU that transiently blocks it, or by an incomplete migration from another physical CPU. The caller should retry after yielding to the calling VM's scheduler.

0x4 `ADDRSPACE_VMMIO_READ`
:The VCPU has performed a read access to an unmapped stage 2 address inside a range previously nominated by a call to `addrspace_configure_vmmio`. The first two state data words contain the base IPA and the access size, respectively. The VCPU will be automatically resumed by the next `vcpu_run` call. The first resume data word for that call should be set to the value that will be returned by the read access.

0x5 `ADDRSPACE_VMMIO_WRITE`
:The VCPU has performed a write access to an unmapped stage 2 address inside a range previously nominated by a call to `addrspace_configure_vmmio`. The three state data words contain the base IPA, access size, and the value written by the access, respectively. The VCPU will be automatically resumed by the next `vcpu_run` call.

0x6 `FAULT`
: The VCPU has an unrecoverable fault.

0x100 `PSCI_SYSTEM_RESET`
:On a platform that implements PSCI, the VCPU has made a call to `PSCI_SYSTEM_RESET` or `PSCI_SYSTEM_RESET2`. The first state data word contains a PSCI Reset Type value (defined below). For a `PSCI_SYSTEM_RESET2` call, the second state data word contains the cookie value.

*VCPU Sleep Type:*

This is a platform-specific unsigned word indicating a low-power suspend state. The value 0 is reserved for a trapped wait-for-interrupt or halt instruction, such as the AArch64 `WFI` instruction.

If the platform implements PSCI, nonzero values are power state values as passed to `PSCI_CPU_SUSPEND`.

*VCPU Poweroff Type*:

| Value | Description |
|-|-----|
| 0 | Recoverable power-off state, e.g. `vcpu_poweroff` called. |
| 1 | Terminated; cannot run until the VM resets. |
| >1 | Reserved. |

*PSCI Reset Type:*

| Bits | Mask | Description |
|-|---|-----|
| 31:0  | `0xffffffff` | Reset type for `PSCI_SYSTEM_RESET2`; 0 for `PSCI_SYSTEM_RESET` |
| 61:32 | `0x3FFFFFFF.00000000` | Reserved — Must be Zero    |
| 62    | `0x40000000.00000000` | 1: `PSCI_SYSTEM_RESET2` SMC64 call, 0: SMC32 call |
| 63    | `0x80000000.00000000` | 1: `PSCI_SYSTEM_RESET` call, 0: `PSCI_SYSTEM_RESET2` |

**Errors:**

OK – the operation was successful.

ERROR_ARGUMENT_INVALID – a value passed in an argument was invalid. This could be due to an invalid VCPU.

ERROR_BUSY – the specified VCPU does not have a bound `VCPU_RUN_WAKEUP` VIRQ.

ERROR_OBJECT_STATE – the VCPU thread was not active, or has already been killed.

Also see: [Capability Errors](#capability-errors)

### Check the State of a Halted VCPU

Query the state of a VCPU that has generated a halt VIRQ to determine why it halted. The state is described the same way as for `vcpu_run`, but the VCPU is not required to be proxy-scheduled.

|    **Hypercall**:       |      `vcpu_run_check`                      |
|-------------------------|--------------------------------------|
|     Call number:        |     `hvc 0x6068`                     |
|     Inputs:             |     X0: VCPU CapID                   |
|                         |     X4: Reserved   – Must be Zero    |
|     Outputs:            |     X0: Error Result                 |
|                         |     X1: VCPU Run State               |
|                         |     X2: State-specific Data          |
|                         |     X3: State-specific Data          |
|                         |     X4: State-specific Data          |

**Types**:

Refer to the [documentation for `vcpu_run_thread`](#run-a-proxy-scheduled-vcpu-thread).

**Errors:**

OK – the operation was successful.

ERROR_ARGUMENT_INVALID – a value passed in an argument was invalid. This could be due to an invalid VCPU.

ERROR_BUSY — the specified VCPU is not halted.

ERROR_OBJECT_STATE – the VCPU thread was not active, or has already been killed.

Also see: [Capability Errors](#capability-errors)

## Scheduler Management

### Scheduler Yield

Informs the hypervisor scheduler that the caller is executing a low priority task or waiting for a non-wakeup event to occur, and wants to give other VCPUs a chance to run. A hint argument may be provided to suggest to the scheduler that a particular VCPU or class of VCPUs should be run instead.

|    **Hypercall**:       |      `scheduler_yield`               |
|-------------------------|--------------------------------------|
|     Call number:        |     `hvc 0x603b`                     |
|     Inputs:             |     X0: control                      |
|                         |     X1: arg1                         |
|                         |     X2: Reserved — Must be Zero      |
|     Outputs:            |     X0: Error Result                 |

*Control:*

| Bits | Mask | Description |
|-|---|-----|
|     15:0             |     `0xffff`      |     hint: Yield type hint.                                                                                                             |
|     31               |     `0x80000000`  |     imp_def:   Implementation defined flag. If set, the hint value specifies a scheduler   implementation specific yield operation.    |

Generic yield hints (imp_def flag = 0):
0x0	generic yield.
0x1	yield to target thread. arg1 = cap_id
0x2	yield to lower priority. arg1 = priority level

**Errors:**

OK – the operation was successful, and the result is valid.

ERROR_ARGUMENT_INVALID – an unsupported or invalid control/hint value was provided.

Also see: [Capability Errors](#capability-errors)

## Virtual PM Group Management

A Virtual PM Group is a collection of VCPUs which share a virtual power management state. This state may be accessible via a virtualised platform-specific interface; on AArch64 this is the Arm PSCI (Platform State Configuration Interface) API. Attachment to this object type is optional for VCPUs in single-processor VMs that do not participate in power management decisions.

### Configure a Virtual PM Group

Set configuration options for a Virtual PM Group whose state is `OBJECT_STATE_INIT`. Making this call is optional.

|    **Hypercall**:       |      `vpm_group_configure`           |
|-------------------------|--------------------------------------|
|     Call number:        |     `hvc 0x6066`                     |
|     Inputs:             |     X0: VPMGroup CapID               |
|                         |     X1: VPMGroupOptionFlags          |
|                         |     X2: Reserved — Must be Zero      |
|     Outputs:            |     X0: Error Result                 |

**Types:**

*VPMGroupOptionFlags:*

|      Bit Numbers     |      Mask                  |      Description                |
|----------------------|----------------------------|---------------------------------|
|     0                |     `0x1`                  |     Exclude from aggregation    |
|     63:1             |     `0xFFFFFFFF.FFFFFFFE`  |     Reserved — Must be Zero     |

**Errors:**

OK – the operation was successful.

ERROR_ARGUMENT_INVALID – an unsupported or invalid configuration option was specified.

Also see: [Capability Errors](#capability-errors)

#### Power State Aggregation

If the flags argument's "exclude from aggregation" bit is clear, which is the default configuration, the Virtual PM Group will collect power state votes from its attached VCPUs. These votes will be used when determining what power state the physical device should enter when one or more physical CPUs becomes idle. In general, a physical CPU will enter the shallowest available idle state permitted by the votes of its VCPUs, i.e. a state with wakeup latency no higher than the acceptable limit for each of the VCPUs.

If the "exclude from aggregation" bit is set, the platform-specific power management API calls will still be available, but their effect on the physical power state may be limited. Also, validation of the power management API calls may be relaxed; e.g. for Arm PSCI implementations, the power state argument to `PSCI_CPU_SUSPEND` will not be validated against the states supported by the physical device.

### Virtual PM Group to VCPU Attachment

Attaches a VCPU to a Virtual PM Group. The Virtual PM Group object must have been activated before this function is called. The VCPU object must not have been activated. An attachment index must be specified which must be a non-negative integer less than the maximum number of attachments supported by this Virtual PM Group object.

|    **Hypercall**:       |      `vpm_group_attach_vcpu`         |
|-------------------------|--------------------------------------|
|     Call number:        |     `hvc 0x603c`                     |
|     Inputs:             |     X0: VPMGroup CapID               |
|                         |     X1: VCPU CapID                   |
|                         |     X2: Index                        |
|                         |     X3: Reserved — Must be Zero      |
|     Outputs:            |     X0: Error Result                 |

**Errors:**

OK – the operation was successful, and the result is valid.

ERROR_NOMEM – the operation failed due to memory allocation error.

ERROR_ARGUMENT_INVALID – the specified attachment index is outside the range supported by this Virtual PM Group.

ERROR_OBJECT_STATE – The VCPU object has already been activated, or the Virtual PM Group object has not yet been activated.

Also see: [Capability Errors](#capability-errors)

### Virtual PM Group vIRQ Bind

Binds a Virtual PM Group to a virtual interrupt.

|    **Hypercall**:       |      `vpm_group_bind_virq`           |
|-------------------------|--------------------------------------|
|     Call number:        |     `hvc 0x6043`                     |
|     Inputs:             |     X0: VPMGroup CapID               |
|                         |     X1: Virtual IC CapID             |
|                         |     X2: Virtual IRQ Info             |
|                         |     X3: Reserved — Must be Zero      |
|     Outputs:            |     X0: Error Result                 |

**Errors:**

OK – the operation was successful, and the result is valid.

ERROR_NOMEM – the operation failed due to memory allocation error.

ERROR_VIRQ_BOUND – the specified virtual PM group is already bound to a VIRQ number.

ERROR_BUSY – the specified VIRQ number is already bound to a source.

ERROR_ARGUMENT_INVALID – a value passed in an argument was invalid. This could be due to an invalid Virtual IRQ Info value.

Also see: [Capability Errors](#capability-errors)

### Virtual PM Group vIRQ Unbind

Unbinds a Virtual PM Group from a virtual IRQ number.

|    **Hypercall**:       |      `vpm_group_unbind_virq`         |
|-------------------------|--------------------------------------|
|     Call number:        |     `hvc 0x6044`                     |
|     Inputs:             |     X0: VPMGroup CapID               |
|                         |     X1: Reserved — Must be Zero      |
|     Outputs:            |     X0: Error Result                 |

**Errors:**

OK – the operation was successful, or the virtual PM group interrupt was already unbound.

Also see: [Capability Errors](#capability-errors)

### Virtual PM Group Get State

Gets the state of the Virtual PM Group.

|    **Hypercall**:       |      `vpm_group_get_state`           |
|-------------------------|--------------------------------------|
|     Call number:        |     `hvc 0x6045`                     |
|     Inputs:             |     X0: VPMGroup CapID               |
|                         |     X1: Reserved — Must be Zero      |
|     Outputs:            |     X0: Error Result                 |
|                         |     X1: VPMState                     |

**Types:**

*VPMState:*

|      VPMState                                                        |      Integer Value     |
|----------------------------------------------------------------------|------------------------|
|     NO_STATE (invalid / non existant)                                |     0                  |
|     RUNNING (System is active)                                       |     1                  |
|     CPUS_SUSPENDED (System suspended after CPU_SUSPEND call)         |     2                  |
|     SYSTEM_SUSPENDED (System suspended after SYSTEM_SUSPEND call)    |     3                  |

**Errors:**

OK – the operation was successful, the result is valid.

Also see: [Capability Errors](#capability-errors)

## Trace Buffer Management

### Update trace class flags

Update the trace class flags values by specifying which flags to set and clear. Some bits are internal to the hypervisor, so their values passed in this hypercall will be ignored. This call will return the values of the flags after being updated.

|    **Hypercall**:       |      `trace_update_class_flags`      |
|-------------------------|--------------------------------------|
|     Call number:        |     `hvc 0x603f`                     |
|     Inputs:             |     X0: SetFlags                     |
|                         |     X1: ClearFlags                   |
|                         |     X2: Reserved — Must be Zero      |
|     Outputs:            |     X0: Error Result                 |
|                         |     X1: SetFlags                     |

**Errors:**

OK – the operation was successful, and the result is valid.

## Watchdog Management

### Configure a Watchdog

Configure a Watchdog whose state is OBJECT_STATE_INIT.

|    **Hypercall**:       |      `watchdog_configure`            |
|-------------------------|--------------------------------------|
|     Call number:        |     `hvc 0x6058`                     |
|     Inputs:             |     X0:   Watchdog CapID             |
|                         |     X1: WatchdogOptionFlags          |
|                         |     X2: Reserved — Must be Zero      |
|     Outputs:            |     X0: Error Result                 |

**Types:**

*WatchdogOptionFlags:*

| Bits | Mask | Description |
|-|---|-----|
|     0                |     `0x1`                  |     Critical bite               |
|     63:1             |     `0xFFFFFFFF.FFFFFFFE`  |     Reserved,   Must be Zero    |

**Errors:**

OK – the operation was successful, and the result is valid.

ERROR_OBJECT_STATE – if the Watchdog object is not in OBJECT_STATE_INIT state.

ERROR_ARGUMENT_INVALID – a value passed in an argument was invalid. This could be due to invalid option flags.

Also see: [Capability Errors](#capability-errors)

### Watchdog to vCPU Attachment

Attaches a Watchdog object to a vCPU. The Watchdog object must have been activated before this function is called. The VCPU object must not have been activated.

|    **Hypercall**:       |      `watchdog_attach_vcpu`          |
|-------------------------|--------------------------------------|
|     Call number:        |     `hvc 0x6040`                     |
|     Inputs:             |     X0: Watchdog CapID               |
|                         |     X1: vCPU CapID                   |
|                         |     X2: Reserved — Must be Zero      |
|     Outputs:            |     X0: Error Result                 |

**Errors:**

OK – the operation was successful, and the result is valid.

ERROR_ARGUMENT_INVALID – a value passed in an argument was invalid. This could be due a thread of kind different from VCPU or if the thread belongs to a HLOS VM.

ERROR_OBJECT_STATE – The thread object has already been activated or the watchdog has not been activated yet.

Also see: [Capability Errors](#capability-errors)

### Watchdog vIRQ Bind

Binds a Watchgdog (bark or bite) interface to a virtual IRQ number.

|    **Hypercall**:       |      `watchdog_bind_virq`          |
|-------------------------|------------------------------------|
|     Call number:        |     `hvc 0x6041`                   |
|     Inputs:             |     X0: Watchdog CapID             |
|                         |     X1: Virtual IC CapID           |
|                         |     X2: Virtual IRQ Info           |
|                         |     X3: WatchdogBindOptionFlags    |
|     Outputs:            |     X0: Error Result               |

**Types:**

*WatchdogBindOptionFlags:*

| Bits | Mask | Description |
|-|---|-----|
|     0                |     `0x1`                  |     Bite virq (If unset, bark virq)    |
|     63:1             |     `0xFFFFFFFF.FFFFFFFE`  |     Reserved,   Must be Zero           |

Bite virq: If set to 0x1, this flag indicates that it binds the bite virq, otherwise it binds the bark virq.

**Errors:**

OK – the operation was successful, and the result is valid.

ERROR_NOMEM – the operation failed due to memory allocation error.

ERROR_VIRQ_BOUND – the specified watchdog is already bound to a VIRQ number.

ERROR_BUSY – the specified VIRQ number is already bound to a source.

ERROR_ARGUMENT_INVALID – a value passed in an argument was invalid. This could be due to an invalid Virtual IRQ Info value, or invalid Watchdog object.

Also see: [Capability Errors](#capability-errors)

### Watchdog vIRQ Unbind

Unbinds a Watchdog (bark or bite) interface virtual IRQ number.

|    **Hypercall**:       |      `watchdog_unbind_virq`        |
|-------------------------|------------------------------------|
|     Call number:        |     `hvc 0x6042`                   |
|     Inputs:             |     X0: Watchdog CapID             |
|                         |     X1: WatchdogBindOptionFlags    |
|     Outputs:            |     X0: Error Result               |

**Types:**

*WatchdogBindOptionFlags:*

| Bits | Mask | Description |
|-|---|-----|
|     0                |     `0x1`                  |     Bite virq (If unset, bark virq)    |
|     63:1             |     `0xFFFFFFFF.FFFFFFFE`  |     Reserved,   Must be Zero           |

Bite virq: If set to 0x1, this flag indicates that it unbinds the bite virq, otherwise it unbinds the bark virq.

**Errors:**

OK – the operation was successful, or the watchdog interrupt was already unbound.

Also see: [Capability Errors](#capability-errors)

### Watchdog Runtime Management

Performs miscellaneous management operations on an arbitrary watchdog object (not necessarily the calling VM's watchdog). Currently, three operations are defined:

1. Freeze a watchdog's counter, preventing a bark or bite occurring (if no such event has already occurred).
1. Freeze a watchdog's counter as above, and also reset the counter to 0.
1. Unfreeze a watchdog's counter.

This is intended primarily for use by the manager of a proxy-scheduled VM, to prevent watchdog events occurring in the VM if the proxy threads cannot be scheduled.

Note that freeze and unfreeze operations are counted, and the watchdog counter will only progress while the the freeze count is zero (i.e. freeze and unfreeze operations are balanced). Also, freeze and unfreeze operations may be performed automatically by the hypervisor in some cases.

|    **Hypercall**:       |      `watchdog_manage`             |
|-------------------------|------------------------------------|
|     Call number:        |     `hvc 0x6063`                   |
|     Inputs:             |     X0: Watchdog CapID             |
|                         |     X1: WatchdogManageOperation    |
|     Outputs:            |     X0: Error Result               |

**Types:**

*WatchdogManageOperation*:

|      Operation Enumerator               |      Integer Value     |
|-----------------------------------------|------------------------|
|     WATCHDOG_MANAGE_OP_FREEZE           |     0                  |
|     WATCHDOG_MANAGE_OP_FREEZE_AND_RESET |     1                  |
|     WATCHDOG_MANAGE_OP_UNFREEZE         |     2                  |

**Errors:**

OK – the operation was successful, or the watchdog interrupt was already unbound.

ERROR_BUSY – the operation failed because it would otherwise have overflowed or underflowed the watchdog's freeze count.

Also see: [Capability Errors](#capability-errors)

## Virtual IO MMIO Management

### Configure a Virtual IO Interface Object

Configure a Virtual IO Interface Object whose state is OBJECT_STATE_INIT.

Every Virtual IO device must be attached to a Memory Extent Object that contains its common registers and assumed to be mapped with write permissions into the backend VM's address space. The caller must also bind the backend IRQs to the backend VM's Virtual Interrupt Controller.

The number of queues presented by the device must be set at configuration time, so the hypervisor can allocate memory for tracking the queue states.

The Memory Extent must be 4KiB in size. Its layout matches the register layout specified for MMIO devices in section 4.2.2 of the Virtual I/O Device (VIRTIO) 1.1 specification, followed by optional device-specific configuration starting at offset 0x100. The caller must map it with read-only permissions into the frontend VM's address space, and bind the device's frontend IRQs to the frontend VM's Virtual Interrupt Controller.

If the device type valid flag is set, then the specified device type must be one that is known to the hypervisor, and any appropriate type-specific hypercalls must be made before the device is permitted to exit its reset state. Otherwise, the device type argument is ignored.

|    **Hypercall**:       |      `virtio_mmio_configure`         |
|-------------------------|--------------------------------------|
|     Call number:        |     `hvc 0x6049`                     |
|     Inputs:             |     X0: VirtioMMIO CapID             |
|                         |     X1: Memextent CapID              |
|                         |     X2: VQsNum Integer               |
|                         |     X3: VirtioOptionFlags            |
|                         |     X4: DeviceType Integer           |
|                         |     X5: DeviceConfigSize Integer     |
|     Outputs:            |     X0: Error Result                 |

**Types:**

*VirtioOptionFlags:*

| Bit Numbers |  Mask                 | Description                             |
|-------------|-----------------------|-----------------------------------------|
| 6           | `0x40`                | Device type argument is valid           |
| 63:7,5:0    | `0xFFFFFFFF.FFFFFFBF` | Reserved — Must be Zero                 |

**Errors:**

OK – the operation was successful.

ERROR_OBJECT_STATE – if the Virtual IO MMIO object is not in OBJECT_STATE_INIT state.

ERROR_ARGUMENT_INVALID – a value passed in an argument was invalid. This could be due to VQsNum being larger than the maximum, or the specified Memory Extent object being of an unsupported type.

Also see: [Capability Errors](#capability-errors)

### Virtual IO MMIO Backend vIRQ Bind

Binds a Virtual IO MMIO backend interface to a virtual interrupt.

|    **Hypercall**:       |      `virtio_mmio_bind_backend_virq`   |
|-------------------------|----------------------------------------|
|     Call number:        |     `hvc 0x604a`                       |
|     Inputs:             |     X0: VirtioMMIO CapID               |
|                         |     X1: Virtual IC CapID               |
|                         |     X2: Virtual IRQ Info               |
|                         |     X3: Reserved — Must be Zero        |
|     Outputs:            |     X0: Error Result                   |

**Errors:**

OK – the operation was successful, and the result is valid.

ERROR_NOMEM – the operation failed due to memory allocation error.

ERROR_VIRQ_BOUND – the specified virtual IO MMIO is already bound to a VIRQ number.

ERROR_BUSY – the specified VIRQ number is already bound to a source.

ERROR_ARGUMENT_INVALID – a value passed in an argument was invalid. This could be due to an invalid Virtual IRQ Info value.

Also see: [Capability Errors](#capability-errors)

### Virtual IO MMIO Backend vIRQ Unbind

Unbinds a Virtual IO MMIO backend interface from a virtual IRQ number.

|    **Hypercall**:       |      `virtio_mmio_unbind_backend_virq`   |
|-------------------------|------------------------------------------|
|     Call number:        |     `hvc 0x604b`                         |
|     Inputs:             |     X0: VirtioMMIO CapID                 |
|                         |     X1: Reserved — Must be Zero          |
|     Outputs:            |     X0: Error Result                     |

**Errors:**

OK – the operation was successful, or the virtual IO MMIO interrupt was already unbound.

Also see: [Capability Errors](#capability-errors)

### Virtual IO MMIO Frontend vIRQ Bind

Binds a Virtual IO MMIO frontend interface to a virtual interrupt.

|    **Hypercall**:       |      `virtio_mmio_bind_frontend_virq`   |
|-------------------------|-----------------------------------------|
|     Call number:        |     `hvc 0x604c`                        |
|     Inputs:             |     X0: VirtioMMIO CapID                |
|                         |     X1: Virtual IC CapID                |
|                         |     X2: Virtual IRQ Info                |
|                         |     X3: Reserved — Must be Zero         |
|     Outputs:            |     X0: Error Result                    |

**Errors:**

OK – the operation was successful, and the result is valid.

ERROR_NOMEM – the operation failed due to memory allocation error.

ERROR_VIRQ_BOUND – the specified virtual IO MMIO is already bound to a VIRQ number.

ERROR_BUSY – the specified VIRQ number is already bound to a source.

ERROR_ARGUMENT_INVALID – a value passed in an argument was invalid. This could be due to an invalid Virtual IRQ Info value.

Also see: [Capability Errors](#capability-errors)

### Virtual IO MMIO Frontend vIRQ Unbind

Unbinds a Virtual IO MMIO backend interface from a virtual IRQ number.

|    **Hypercall**:       |      `virtio_mmio_unbind_frontend_virq`   |
|-------------------------|-------------------------------------------|
|     Call number:        |     `hvc 0x604d`                          |
|     Inputs:             |     X0: VirtioMMIO CapID                  |
|                         |     X1: Reserved — Must be Zero           |
|     Outputs:            |     X0: Error Result                      |

**Errors:**

OK – the operation was successful, or the virtual IO MMIO interrupt was already unbound.

Also see: [Capability Errors](#capability-errors)

### Virtual IO MMIO Backend Assert vIRQ

The backend makes this call to assert the virtual IRQ directed to the frontend and writes a bit mask of events that caused the assertion.

|    **Hypercall**:       |      `virtio_mmio_backend_assert_virq`   |
|-------------------------|------------------------------------------|
|     Call number:        |     `hvc 0x604e`                         |
|     Inputs:             |     X0: VirtioMMIO CapID                 |
|                         |     X1: InterruptStatus                  |
|                         |     X2: Reserved — Must be Zero          |
|     Outputs:            |     X0: Error Result                     |

**Errors:**

OK – the operation was successful, or the virtual IO MMIO interrupt was already unbound.

ERROR_DENIED – Cannot assert irq since there is a reset currently pending.

ERROR_ARGUMENT_INVALID – A value passed in an argument was invalid.

Also see: [Capability Errors](#capability-errors)

### Virtual IO MMIO Backend Set DeviceFeatures

Set the device features flags based on the specified device features selector. The device features specified must comply with the features enforced by the hypervisor (VIRTIO_F_VERSION_1, VIRTIO_F_ACCESS_PLATFORM, !VIRTIO_F_NOTIFICATION_DATA).

|    **Hypercall**:       |      `virtio_mmio_backend_set_dev_features`   |
|-------------------------|-----------------------------------------------|
|     Call number:        |     `hvc 0x604f`                              |
|     Inputs:             |     X0: VirtioMMIO CapID                      |
|                         |     X1: DeviceFeaturesSel                     |
|                         |     X2: DeviceFeatures                        |
|                         |     X3: Reserved — Must be Zero               |
|     Outputs:            |     X0: Error Result                          |

**Errors:**

OK – The operation was successful, and the result is valid.

ERROR_ARGUMENT_INVALID – A value passed in an argument was invalid.

ERROR_DENIED – Device features passed do not comply with the features enforced by the hypervisor.

Also see: [Capability Errors](#capability-errors)

### Virtual IO MMIO Backend Set QueueNumMax

Set maximum virtual queue size of the queue specified by the queue selector.

|    **Hypercall**:       |      `virtio_mmio_backend_set_queue_num_max`   |
|-------------------------|------------------------------------------------|
|     Call number:        |     `hvc 0x6050`                               |
|     Inputs:             |     X0: VirtioMMIO CapID                       |
|                         |     X1: QueueSel                               |
|                         |     X2: QueueNumMax                            |
|                         |     X3: Reserved — Must be Zero                |
|     Outputs:            |     X0: Error Result                           |

**Errors:**

OK – The operation was successful, and the result is valid.

ERROR_ARGUMENT_INVALID – A value passed in an argument was invalid.

Also see: [Capability Errors](#capability-errors)

### Virtual IO MMIO Backend Get DriverFeatures

Get the driver features flags based on the specified driver features selector.

|    **Hypercall**:       |      `virtio_mmio_backend_get_drv_features`   |
|-------------------------|-----------------------------------------------|
|     Call number:        |     `hvc 0x6051`                              |
|     Inputs:             |     X0: VirtioMMIO CapID                      |
|                         |     X1: DriverFeaturesSel                     |
|                         |     X2: Reserved — Must be Zero               |
|     Outputs:            |     X0: Error Result                          |
|                         |     X1: DriverFeatures                        |

**Errors:**

OK – The operation was successful, and the result is valid.

ERROR_ARGUMENT_INVALID – A value passed in an argument was invalid.

Also see: [Capability Errors](#capability-errors)

### Virtual IO MMIO Backend Get Queue Info

Get information from the queue specified by the queue selector.

|    **Hypercall**:       |      `virtio_mmio_backend_get_queue_info`   |
|-------------------------|---------------------------------------------|
|     Call number:        |     `hvc 0x6052`                            |
|     Inputs:             |     X0: VirtioMMIO CapID                    |
|                         |     X1: QueueSel                            |
|                         |     X2: Reserved — Must be Zero             |
|     Outputs:            |     X0: Error Result                        |
|                         |     X1: QueueNum                            |
|                         |     X2: QueueReady                          |
|                         |     X3: QueueDesc (low and high)            |
|                         |     X4: QueueDriver (low and high)          |
|                         |     X5: QueueDevice (low and high)          |

**Errors:**

OK – The operation was successful, and the result is valid.

ERROR_ARGUMENT_INVALID – A value passed in an argument was invalid.

Also see: [Capability Errors](#capability-errors)

### Virtual IO MMIO Backend Get Notification

The backend should make this call, when its VIRQ is asserted, to get a bitmap of the virtual queues that need to be notified and a bitmap of the reasons why the VIRQ was asserted. This calls also deasserts the backend’s VIRQ.

|    **Hypercall**:       |      `virtio_mmio_backend_get_notification`   |
|-------------------------|-----------------------------------------------|
|     Call number:        |     `hvc 0x6053`                              |
|     Inputs:             |     X0: VirtioMMIO CapID                      |
|                         |     X1: Reserved — Must be Zero               |
|     Outputs:            |     X0: Error Result                          |
|                         |     X1: VQs Bitmap                            |
|                         |     X2: NotifyReason Bitmap                   |

**Types:**

*NotifyReason:*

| Bits | Mask | Description |
|-|---|-----|
|     0                |     `0x1`                  |     1 = NEW_BUFFER: notifies the device that there are new buffers to process in a queue.                       |
|     1                |     `0x2`                  |     1 = RESET_RQST: notifies the device that a device reset has been requested.                                 |
|     3                |     `0x8`                  |     1 = DRIVER_OK: notifies the device that the frontend has set the DRIVER_OK bit of the Status register.      |
|     4                |     `0x10`                 |     1 = FAILED: notifies the device that the frontend has set the FAILED bit of the Status register.            |
|     63:5,2           |     `0xFFFFFFFF.FFFFFFE4`  |     Reserved = 0 [TBD notify reasons]                                                                           |

**Errors:**

OK – The operation was successful, and the result is valid.

ERROR_ARGUMENT_INVALID – A value passed in an argument was invalid.

Also see: [Capability Errors](#capability-errors)

### Virtual IO MMIO Backend Acknowledge Reset

The backend should make this call after a device reset is completed. This call will clear all bits in QueueReady for all queues in the device.

|    **Hypercall**:       |      `virtio_mmio_backend_acknowledge_reset`   |
|-------------------------|------------------------------------------------|
|     Call number:        |     `hvc 0x6054`                               |
|     Inputs:             |     X0: VirtioMMIO CapID                       |
|                         |     X1: Reserved — Must be Zero                |
|     Outputs:            |     X0: Error Result                           |

**Errors:**

OK – The operation was successful, and the result is valid.

ERROR_ARGUMENT_INVALID – A value passed in an argument was invalid.

Also see: [Capability Errors](#capability-errors)

### Virtual IO MMIO Backend Set Status

This calls sets status register.

|    **Hypercall**:       |      `virtio_mmio_backend_set_status`   |
|-------------------------|-----------------------------------------|
|     Call number:        |     `hvc 0x6055`                        |
|     Inputs:             |     X0: VirtioMMIO CapID                |
|                         |     X1: Status                          |
|                         |     X2: Reserved — Must be Zero         |
|     Outputs:            |     X0: Error Result                    |

**Errors:**

OK – The operation was successful, and the result is valid.

ERROR_ARGUMENT_INVALID – A value passed in an argument was invalid.

Also see: [Capability Errors](#capability-errors)

## Virtio Input Config Hypercalls

### Virtio Input Configure

Allocate storage for the large data items and set the values of the small data items (`dev_ids` and `propbits`, which each fit in a single machine register). For the two types that support `subsel`, this call will specify the number of distinct valid `subsel` values (which may be sparse).

The `NumEVTypes` value must be between 0 and 32 inclusive. The `NumAbsAxes` value must be between 0 and 64 inclusive. If these limits are exceeded, the call will return `ERROR_ARGUMENT_SIZE`.



|    **Hypercall**:       |      `virtio_input_configure`        |
|-------------------------|--------------------------------------|
|     Call number:        |     `hvc 0x605e`                      |
|     Inputs:             |     X0: Virtio CapID                 |
|                         |     X1: DevIDs                       |
|                         |     X2: PropBits                     |
|                         |     X3: NumEVTypes                   |
|                         |     X4: NumAbsAxes                   |
|                         |     X5: Reserved — Must be Zero      |
|     Outputs:            |     X0: Error Result                 |

**Types**:

_DevIDs_:

| Bits | Mask | Description |
|-|---|-----|
| 15:0 | `0xFFFF` | BusType |
| 31:16 | `0xFFFF0000` | Vendor |
| 47:32 | `0xFFFF.00000000` | Product |
| 63:48 | `0xFFFF0000.00000000` | Version |

**Errors:**

OK – The operation was successful, and the result is valid.

ERROR_ARGUMENT_INVALID – A value passed in an argument was invalid.

Also see: [Capability Errors](#capability-errors)

### Virtio Input Set Data

Copy data into the hypervisor's storage for one of the large data items, given its `sel` and `subsel` values, size, and the virtual address of a buffer in the caller's stage 1 address space. The data must not already have been configured for the given `sel` and `subsel` values.

|    **Hypercall**:       |      `virtio_input_set_data`         |
|-------------------------|--------------------------------------|
|     Call number:        |     `hvc 0x605f`                      |
|     Inputs:             |     X0: Virtio CapID                 |
|                         |     X1: Sel                          |
|                         |     X2: SubSel                       |
|                         |     X3: Size                         |
|                         |     X4: Data VMAddr                  |
|                         |     X5: Reserved — Must be Zero      |
|     Outputs:            |     X0: Error Result                 |

The specified `VMAddr` must point to a buffer of the specified size that is mapped in the caller's stage 1 and stage 2 address spaces.

The `Sel`, `SubSel` and `Size` arguments must fall within one of the following ranges:

| Sel  | SubSel | Size  |
|------|--------|-------|
| 1    | 0      | 0–128 |
| 2    | 0      | 0–128 |
| 0x11 | 0–31   | 0–128 |
| 0x12 | 0–63   | 20    |

All other combinations are invalid. The call will return `ERROR_ARGUMENT_INVALID` if `Sel` or `SubSel` is invalid or out of range, and `ERROR_ARGUMENT_SIZE` if `Size` is out of range for the specified combination of `Sel` and `SubSel`.

Also, the call must not be repeated with `Sel` set to 0x11 or 0x12 and `Size` set to a nonzero value for more distinct values of `SubSel` than were specified with the `NumEVTypes` and `NumAbsAxes` arguments, respectively, of the most recent `virtio_input_configure` call. The call will return `ERROR_NORESOURCES` if these limits are exceeded.

**Errors:**

OK – The operation was successful, and the result is valid.

ERROR_ARGUMENT_INVALID – A value passed in an argument was invalid.

Also see: [Capability Errors](#capability-errors)

## PRNG Management

### PRNG Get Entropy

Gets random numbers from a DRBG that is seeded by a TRNG. Typically this API will source randomness from a NIST/FIPS compliant hardware device.

|    **Hypercall**:   |  `prng_get_entropy`            |
|---------------------|--------------------------------|
|     Call number:    |  `hvc 0x6057`                  |
|     Inputs:         |  X0: NumBytes                  |
|                     |  X1: Reserved — Must be Zero   |
|     Outputs:        |  X0: Error Result              |
|                     |  X1: Data0                     |
|                     |  X2: Data1                     |
|                     |  X3: Data2                     |
|                     |  X4: Data3                     |

**Errors:**

OK – the operation was successful, and the result is valid.

ERROR_ARGUMENT_SIZE – the NumBytes provided is zero, or exceeds the possible bytes to be returned in the Data output registers.

ERROR_BUSY – Called within the read rate-limit window.

ERROR_UNIMPLEMENTED – if functionality not implemented.

Also see: [Capability Errors](#capability-errors)

## Error Results

### Error Code Enumeration

|      Error Enumerator                   |      Integer Value     |
|-----------------------------------------|------------------------|
|     OK                                  |     0                  |
|     ERROR_UNIMPLEMENTED                 |     -1                 |
|     ERROR_RETRY                         |     -2                 |
|                                         |                        |
|     ERROR_ARGUMENT_INVALID              |     1                  |
|     ERROR_ARGUMENT_SIZE                 |     2                  |
|     ERROR_ARGUMENT_ALIGNMENT            |     3                  |
|                                         |                        |
|     ERROR_NOMEM                         |     10                 |
|     ERROR_NORESOURCES                   |     11                 |
|     ERROR_ADDR_OVERFLOW                 |     20                 |
|     ERROR_ADDR_UNDERFLOW                |     21                 |
|     ERROR_ADDR_INVALID                  |     22                 |
|     ERROR_DENIED                        |     30                 |
|     ERROR_BUSY                          |     31                 |
|     ERROR_IDLE                          |     32                 |
|     ERROR_OBJECT_STATE                  |     33                 |
|     ERROR_OBJECT_CONFIG                 |     34                 |
|     ERROR_OBJECT_CONFIGURED             |     35                 |
|     ERROR_FAILURE                       |     36                 |
|                                         |                        |
|     ERROR_VIRQ_BOUND                    |     40                 |
|     ERROR_VIRQ_NOT_BOUND                |     41                 |
|                                         |                        |
|     ERROR_CSPACE_CAP_NULL               |     50                 |
|     ERROR_CSPACE_CAP_REVOKED            |     51                 |
|     ERROR_CSPACE_WRONG_OBJECT_TYPE      |     52                 |
|     ERROR_CSPACE_INSUFFICIENT_RIGHTS    |     53                 |
|     ERROR_CSPACE_FULL                   |     54                 |
|                                         |                        |
|     ERROR_MSGQUEUE_EMPTY                |     60                 |
|     ERROR_MSGQUEUE_FULL                 |     61                 |
|                                         |                        |
|     ERROR_MEMDB_NOT_OWNER               |     111                |
|     ERROR_MEMEXTENT_MAPPINGS_FULL       |     120                |
|     ERROR_MEMEXTENT_TYPE                |     121                |
|     ERROR_EXISTING_MAPPING              |     200                |

### Capability Errors

ERROR_CSPACE_CAP_NULL – invalid CapID.

ERROR_CSPACE_CAP_REVOKED – CapID no longer valid since it has already been revoked.

ERROR_CSPACE_WRONG_OBJECT_TYPE – CapID does not correspond with the specified object type.

ERROR_CSPACE_INSUFFICIENT_RIGHTS – CapID has not enough rights to execute operation.

ERROR_CSPACE_FULL – CSpace has reached maximum number of capabilities allowed.

```

`docs/build.md`:

```md
# Build Instructions

These instructions for building the Gunyah Hypervisor can be used for both local builds or building within a Docker container.

## Build Environment

Ensure your build environment is correctly setup. See [Setup Instructions](setup.md).

Always ensure you have activated the `gunyah-venv` *before* running `configure` or building.
```bash
. gunyah-venv/bin/activate
```

## Download the source code

The following repositories are needed to build a Gunyah Hypervisor image:

These should all be cloned into the same top-level directory (this assumed in the Docker setup).

- [Gunyah Hypervisor](https://github.com/quic/gunyah-hypervisor) — The Gunyah Hypervisor.
 ```bash
 git clone https://github.com/quic/gunyah-hypervisor.git hyp
 ```
- [Resource Manager](https://github.com/quic/gunyah-resource-manager) — The privileged root VM and VM manager supporting the Gunyah Hypervisor.
 ```bash
 git clone https://github.com/quic/gunyah-resource-manager.git resource-manager
 ```
- [Gunyah C Runtime](https://github.com/quic/gunyah-c-runtime) — A runtime for light-weight OS-less application VMs.
 ```bash
git clone https://github.com/quic/gunyah-c-runtime.git musl-c-runtime
 ```

## Build Configuration

The build system has several configuration parameters that must be set:

* `platform`: selects the target hardware platform
* `featureset`:  selects a named hypervisor architecture configuration
* `quality`: specifies the build quality, e.g. `debug`, `production` etc., which modify the build, such as including runtime assertions, compiler optimisations etc.

These parameters must be set on the build system's command line; if one or more
of them is left unset, the build system will print the known values for the
missing parameter and abort. You may specify a comma-separated list to select
multiple values for a parameter, or `all` to select every valid combination.
You may also specify simply `all=true`, which is equivalent to specifying `all`
for every parameter that is not otherwise specified. The when multiple options
are selected, each combination (variant) will be built in separate output
directories under the `build` directory.

Each project may be built using `ninja` or `scons` and the process for build configuration depends on the selected build tool used. See the sections below.

## Building

The Gunyah Hypervisor, Resource Manager and C runtime are built separately,
each following the similar build instructions below. These separate images need
to be packaged together into a final boot image.

> IMPORTANT! If making hypervisor public API changes, these changes will need to be updated in the Resource Manager and Runtime sources.

### Building with Ninja

To configure the build for use with *Ninja*, run `./configure.py <configuration>`
in the top-level source repository of the component, specifying the desired
configuration parameters.

For example, in each of the Gunyah Hypervisor, Resource Manager and Gunyah C Runtime source directories, run:
```sh
./configure.py platform=qemu featureset=gunyah-rm-qemu quality=production
```

or to build all available configurations for the QEMU platform:
```sh
./configure.py platform=qemu all=true
```

This will create a `build` directory and Ninja build rules file for each enabled build variant. Generally, the `configure` step only needs to be run once.

```sh
ninja
```

Run `ninja` to build. There is usually no need to specify `-j` or similar, as
Ninja will select this automatically. Ninja also will incrementally re-build if
run again after making code changes.
> Note, if configuration files are modified, Ninja will rerun the configuration tool with the previous parameters. However, you must manually rerun the configuration step if you rename or delete an existing module or configuration parameter, as Ninja will refuse to run if a build configuration file is missing.

To build a specific file (for example, a single variant when multiple variants have been configured), specify its full name as the target on the `ninja` command line.

To clean the build, run `ninja -t clean`. It should not be necessary to do this routinely.

### Building with SCons

To perform a standalone SCons build, run `scons`, specifying the configuration
parameters. For example, to build debug builds of all available feature sets
for the QEMU platform:

```sh
scons platform=qemu featureset=all quality=debug
```

Note, configuration parameters *must* be specified on every time you perform a SCons build; configuration is not cached.

To clean the build, run `scons -c all=true`, or use configuration parameters to select a specific variant to clean. It should not be necessary to do this routinely.

## Producing a Boot Image

Once you have built the Gunyah Hypervisor, Resource Manager and C Runtime, a boot image needs be generated.

To reduce the size of the boot image, the generated binaries of Resource Manager and C Runtime need to be stripped with the following commands:
```bash
$LLVM/bin/llvm-strip -o <path-to-resource-manager-src>/build/resource-manager.strip <path-to-resource-manager-src>/build/resource-manager
$LLVM/bin/llvm-strip -o <path-to-c-runtime-src>/build/runtime.strip <path-to-c-runtime-src>/build/runtime
```

The individual executables generated by building [Gunyah Hypervisor](https://github.com/quic/gunyah-hypervisor), [Resource Manager](https://github.com/quic/gunyah-resource-manager), and [Gunyah C Runtime](https://github.com/quic/gunyah-c-runtime) need to be integrated into a single `hypvm.elf` boot image.

You will need the [pyelftools](https://github.com/eliben/pyelftools) Python
module. This should be installed in the gunyah python virtual-env.  However, it
is available from its upstream project:
```bash
cd <tools-directory>
git clone https://github.com/eliben/pyelftools.git
```

To generate `hypvm.elf` boot image run these steps (substituting `<path>`s for each tool / executable):
```bash
cd <path-to-gunyah-hypervisor-src>
tools/elf/package_apps.py \
 -a <path-to-resource-manager-src>/build/resource-manager.strip \
 -r <path-to-c-runtime-src>/build/runtime.strip \
 <path-to-gunyah-hypervisor-src>/build/qemu/gunyah-rm-qemu/production/hyp.elf \
 -o <path-to-destination>/hypvm.elf
```
> Note, you may wish to pick a different hypervisor `hyp.elf` from a different build variant (i.e. `build/qemu/gunyah-rm-qemu/debug/`).

```

`docs/linux.md`:

```md
# Linux

Build a Linux kernel for use as the primary VM OS kernel on the QEMU simulator.

## Host cross-compilers

You will need an GCC aarch64 targeted compiler. It may be available in your Linux distribution, E.g.:
```bash
sudo apt install build-essential gcc-aarch64-linux-gnu binutils-aarch64-linux-gnu ncurses-dev
```

## Download the source code

Download the latest Linux source code:
> Note, we show a shallow clone to speed up checkouts

```bash
git clone --depth=1 https://github.com/torvalds/linux.git
```

## Build Linux kernel

Build Linux kernel with the default defconfig, cross-compiled for arm64.

```bash
cd linux/
make ARCH=arm64 CROSS_COMPILE=aarch64-linux-gnu- defconfig
make -j4 ARCH=arm64 CROSS_COMPILE=aarch64-linux-gnu-
cp ./arch/arm64/boot/Image <path-to-output-dir>/.
```

```

`docs/qemu.md`:

```md
# QEMU AArch64 Simulator

Note, Gunyah requires QEMU version >= `5.0.0`

## Installation

Install the dependencies described in the QEMU documentation.

For example for a Linux host:
https://wiki.qemu.org/Hosts/Linux

Download and build latest version:
https://www.qemu.org/download/#source

Make sure that it is built with aarch64-softmmu included in the target list.

For example, to build QEMU only for aarch64-softmmu target:

```bash
git clone https://git.qemu.org/git/qemu.git
cd qemu
git submodule init
git submodule update --recursive
./configure --target-list=aarch64-softmmu --enable-debug
make
```

```

`docs/ramdisk.md`:

```md
# A simple Busybox Ramdisk

A simple RAM disk is required at a minimum to boot Linux in a VM.

The following instructions create a RAM disk using Busybox 1.33.0.

## Build Busybox 1.33.0

Build Busybox cross compiled for AArch64:

```bash
wget -c https://busybox.net/downloads/busybox-1.33.0.tar.bz2
tar xf busybox-1.33.0.tar.bz2
cd busybox-1.33.0
make ARCH=arm CROSS_COMPILE=aarch64-linux-gnu- defconfig
make ARCH=arm CROSS_COMPILE=aarch64-linux-gnu- menuconfig
sed -i '/# CONFIG_STATIC is not set/c\CONFIG_STATIC=y' .config
ARCH=arm64 CROSS_COMPILE=aarch64-linux-gnu- make -j4
ARCH=arm64 CROSS_COMPILE=aarch64-linux-gnu- make install
```

## Create a RAM disk

Create a RAM disk using the Busybox build and place it in an output directory:

```bash
cd _install
mkdir proc sys dev etc etc/init.d
cat <<EOF > etc/init.d/rcS
#!bin/sh
mount -t proc none /proc
mount -t sysfs none /sys
EOF
chmod u+x etc/init.d/rcS
grep -v tty ../examples/inittab > ./etc/inittab
find . | cpio -o -H newc | gzip > <path-to-output-dir>/initrd.img
```

```

`docs/setup.md`:

```md
# Setup Instructions

## Development Environment

We recommend using a Docker container with the required compilers and tools for
convenience.

A separate _Gunyah Support Scripts_ repository is maintained with reference
Docker based environment instructions and scripts.

See:
[Gunyah Support Scripts](https://github.com/quic/gunyah-support-scripts/tree/develop)

```bash
git clone https://github.com/quic/gunyah-support-scripts.git
```

## Custom Dev Environment

If you intend to setup your own development environment, you can follow the
reference Docker setup on your development host. This process is not
documented.

### Toolchain

The Gunyah Hypervisor projects use the LLVM v15 toolchain, cross-compiled for
AArch64 and musl libc. This is due to standalone application VMs (Resource
Manager) are built with a runtime supporting the musl libc library.

### Set up environment variables

You must set the following environment variables:
```bash
export LLVM=/path/to/llvm15/
```
> Note, when using the toolchain built with the provided script, point to the "llvm-musl-install" generated folder.
> `export LLVM=/path/to/llvm-musl-install`

- To point to the C application sysroot:
```bash
export LOCAL_SYSROOT=/path/to/c-application-sysroot
```

### Install the Python dependencies

Create a virtual environment, activate it, and install the modules used by the auto-generation code:

```bash
python3 -m venv gunyah-venv
. gunyah-venv/bin/activate
pip install -r <path-to-gunyah-src>/tools/requirements.txt
```

We recommend installing the Python environment _outside_ the Gunyah source
directory. This is so the automatic dependency detection in the Python scripts
ignores modules imported from the virtual environment.

```

`docs/terminology.md`:

```md
## Terminology:

### PVM : **P**rimary **V**irtual **M**achine
- This is the VM that's created for the primary HLOS to execute
- Generally all the resources are assigned to this VM

### HLOS : **H**igh **L**evel **OS**
- Mean the main OS owning the majority of the HW device access.
- Typically used as a synonym for PVM.

### SVM : **S**econdary **V**irtual **M**achine
- These VM's are created and launched from the PVM

```

`docs/test.md`:

```md
# Testing Gunyah Hypervisor

We provide two ways of testing the hypervisor:
1. [Using a Docker container](#using-a-docker-container)
2. [Using local machine](#using-a-local-linux-machine)

## Using a Docker container

A Docker image can been built with the required compiler and QEMU Arm System emulator.
See [Setup Instructions](setup.md).

## Using a local Linux machine

1. Build and install a recent QEMU (v7.2 is tested):
    - See Docker support scripts for reference
2. Download and build the hypervisor source code:
    - [Setup Instructions](setup.md)
    - [Build instructions](build.md)
3. Download and build the latest Linux kernel:
    [Linux instructions](linux.md)
4. Create a RAM disk for Linux:
    [RAM disk instructions](ramdisk.md)
5. Generate a device tree for the QEMU platform:
    - See Docker support scripts for reference
7. Boot the Gunyah Hypervisor with the Linux VM on the QEMU simulator:
    - See Docker support scripts for reference

```

`hyp/arch/aarch64/include/asm/asm_defs.inc`:

```inc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <asm-generic/asm_defs.inc>

.macro vector vector_name:req
	function \vector_name, align=128, section=nosection
.endm

.macro vector_end vector_name:req
	function_end \vector_name
	.if (. - \vector_name) > 0x80
	.error "#\vector_name is too big"
	.endif
.endm

.macro	abs64 reg:req val64:req
	movz	\reg, (\val64) & 0xffff
	.ifne ((\val64) >> 16) & 0xffff
	movk	\reg, ((\val64) >> 16) & 0xffff, LSL 16
	.endif
	.ifne ((\val64) >> 32) & 0xffff
	movk	\reg, ((\val64) >> 32) & 0xffff, LSL 32
	.endif
	.ifne ((\val64) >> 48) & 0xffff
	movk	\reg, ((\val64) >> 48) & 0xffff, LSL 48
	.endif
.endm

.macro adrl reg:req sym:req
	adrp	\reg, \sym
	add	\reg, \reg, :lo12:\sym
.endm

.macro	get_tls_base , tls_base:req
	mrs	\tls_base, TPIDR_EL2
.endm

// Get the address of a thread-local symbol
.macro	adr_threadlocal, reg:req, sym:req, tls_base
.ifb \tls_base
	mrs	\reg, TPIDR_EL2
	add	\reg, \reg, :tprel_hi12:\sym
.else
	add	\reg, \tls_base, :tprel_hi12:\sym
.endif
	add	\reg, \reg, :tprel_lo12_nc:\sym
.endm

// Branch target identification helpers
#if defined(ARCH_ARM_FEAT_BTI)
#define BRANCH_TARGET(type, ...)	\
	bti	type;			\
	__VA_ARGS__
#else
#define BRANCH_TARGET(type, ...)	\
	__VA_ARGS__
#endif

```

`hyp/arch/aarch64/include/asm/barrier.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// Barrier and wait operations.
//
// These macros should not be used unless the event interface and other
// compiler barriers are unsuitable.

// Yield the CPU to another hardware thread (SMT) / or let a simulator give CPU
// time to somthing else.
#define asm_yield() __asm__ volatile("yield")

// Ensure that writes to CPU configuration registers and other similar events
// are visible to code executing on the CPU. For example, use this between
// enabling access to floating point registers and actually using those
// registers.
#define asm_context_sync_fence()	__asm__ volatile("isb" ::: "memory")
#define asm_context_sync_ordered(order) __asm__ volatile("isb" : "+m"(*order))

// The asm_ordering variable is used as an artificial dependency to order
// different individual asm statements with respect to each other in a way that
// is lighter weight than a full "memory" clobber.
extern asm_ordering_dummy_t asm_ordering;

```

`hyp/arch/aarch64/include/asm/event.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// From ARMv8.0 onwards (but not ARMv7), losing the local monitor triggers an
// event, so we can obtain the required semantics by loading with LDAX* and
// polling with WFE. The default store-release updates are sufficient on their
// own. See the generic event header for detailed requirements.

#define asm_event_wait(p) __asm__ volatile("wfe" ::"m"(*p))

// clang-format off
#define asm_event_load_before_wait(p) _Generic(				       \
	(p),								       \
	_Atomic uint64_t *: asm_event_load64_before_wait,		       \
	_Atomic uint32_t *: asm_event_load32_before_wait,		       \
	_Atomic uint16_t *: asm_event_load16_before_wait,		       \
	_Atomic uint8_t *: asm_event_load8_before_wait,		       \
	_Atomic bool *: asm_event_loadbool_before_wait)(p)
// clang-format on

#define asm_event_load_bf_before_wait(name, p)                                 \
	name##_cast(asm_event_load_before_wait(name##_atomic_ptr_raw(p)))

#include <asm-generic/event.h>

static inline ALWAYS_INLINE bool
asm_event_loadbool_before_wait(_Atomic bool *p)
{
	uint8_t ret;
	__asm__("ldaxrb %w0, %1" : "=r"(ret) : "Q"(*p));
	return ret != 0U;
}

static inline ALWAYS_INLINE uint8_t
asm_event_load8_before_wait(_Atomic uint8_t *p)
{
	uint8_t ret;
	__asm__("ldaxrb %w0, %1" : "=r"(ret) : "Q"(*p));
	return ret;
}

static inline ALWAYS_INLINE uint16_t
asm_event_load16_before_wait(_Atomic uint16_t *p)
{
	uint16_t ret;
	__asm__("ldaxrh %w0, %1" : "=r"(ret) : "Q"(*p));
	return ret;
}

static inline ALWAYS_INLINE uint32_t
asm_event_load32_before_wait(_Atomic uint32_t *p)
{
	uint32_t ret;
	__asm__("ldaxr %w0, %1" : "=r"(ret) : "Q"(*p));
	return ret;
}

static inline ALWAYS_INLINE uint64_t
asm_event_load64_before_wait(_Atomic uint64_t *p)
{
	uint64_t ret;
	__asm__("ldaxr %0, %1" : "=r"(ret) : "Q"(*p));
	return ret;
}

```

`hyp/arch/aarch64/include/asm/interrupt.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// Macros to enable or disable interrupts.

// Enable all aborts and interrupts, with a full compiler barrier. Self-hosted
// debug is left disabled.
#define asm_interrupt_enable() __asm__ volatile("msr daifclr, 0x7" ::: "memory")

// Enable all aborts and interrupts, with a compiler release fence. Self-hosted
// debug is left disabled.
//
// The argument should be a pointer to a flag that has previously been read to
// decide whether to enable interrupts. The pointer will not be dereferenced by
// this macro.
#define asm_interrupt_enable_release(flag_ptr)                                 \
	do {                                                                   \
		atomic_signal_fence(memory_order_release);                     \
		__asm__ volatile("msr daifclr, 0x7" : "+m"(*(flag_ptr)));      \
	} while ((_Bool)0)

// Disable all aborts and interrupts, with a full compiler barrier.
#define asm_interrupt_disable()                                                \
	__asm__ volatile("msr daifset, 0x7" ::: "memory")

// Disable all aborts and interrupts, with a compiler acquire fence.
//
// The argument should be a pointer to a flag that will be written (by the
// caller) after this macro completes to record the fact that we have
// disabled interrupts. The pointer will not be dereferenced by this macro.
//
// Warning: the flag must not be CPU-local if this configuration allows context
// switches in interrupt handlers! If it is, use asm_interrupt_disable() (with
// a full barrier) and ensure that the CPU ID is determined _after_ disabling
// interrupts. A thread-local flag is ok, however.
#define asm_interrupt_disable_acquire(flag_ptr)                                \
	do {                                                                   \
		__asm__ volatile("msr daifset, 0x7" ::"m"(*(flag_ptr)));       \
		atomic_signal_fence(memory_order_acquire);                     \
	} while ((_Bool)0)

```

`hyp/arch/aarch64/include/asm/nospec_checks.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// Check index is valid with speculation prevention.
//
// Check if (unsigned) val is less than limit. Returns an index_result_t which
// contains whether the index is valid, and returns and if valid, returns the
// speculation safe index value for subsequent indexing. Note: DO NOT use the
// input index in subsequent code to index, as it won't be speculation safe.
//
// Returns OK if index is in the range 0 .. (limit-1), and a speculation safe
// copy of index.
index_result_t
nospec_range_check(index_t val, index_t limit);

```

`hyp/arch/aarch64/include/asm/panic.inc`:

```inc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

.macro panic, panic_str:req
.pushsection .rodata.panic_str\@, "aMS", @progbits, 1
local panic_str\@:
	.asciz	"\panic_str"
.popsection
	adrl	x0, LOCAL(panic_str\@)
	bl	panic
.endm

```

`hyp/arch/aarch64/include/asm/sysregs.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#define sysreg64_read(reg, val)                                                \
	do {                                                                   \
		register_t reg##_read_val;                                     \
		/* Read 64-bit system register */                              \
		__asm__ volatile("mrs %0, " #reg ";" : "=r"(reg##_read_val));  \
		val = (__typeof__(val))reg##_read_val;                         \
	} while (0)

#define sysreg64_read_ordered(reg, val, ordering_var)                          \
	do {                                                                   \
		register_t reg##_read_val;                                     \
		/* Read 64-bit system register with ordering */                \
		__asm__ volatile("mrs %0, " #reg ";"                           \
				 : "=r"(reg##_read_val), "+m"(ordering_var));  \
		val = (__typeof__(val))reg##_read_val;                         \
	} while (0)

#define sysreg64_write(reg, val)                                               \
	do {                                                                   \
		register_t reg##_write_val = (register_t)val;                  \
		/* Write 64-bit system register */                             \
		__asm__ volatile("msr " #reg ", %0" : : "r"(reg##_write_val)); \
	} while (0)

#define sysreg64_write_ordered(reg, val, ordering_var)                         \
	do {                                                                   \
		register_t reg##_write_val = (register_t)val;                  \
		/* Write 64-bit system register with ordering */               \
		__asm__ volatile("msr " #reg ", %1"                            \
				 : "+m"(ordering_var)                          \
				 : "r"(reg##_write_val));                      \
	} while (0)

```

`hyp/arch/aarch64/include/asm/system_registers.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// AArch64 System Register Encoding
//
// This list is not exhaustive, it contains mostly registers likely to be
// trapped and accessed indirectly.

#define ISS_OP0_OP1_CRN_CRM_OP2(op0, op1, crn, crm, op2)                       \
	(uint32_t)(((3U & (uint32_t)(op0)) << 20) |                            \
		   ((7U & (uint32_t)(op2)) << 17) |                            \
		   ((7U & (uint32_t)(op1)) << 14) |                            \
		   ((15U & (uint32_t)(crn)) << 10) |                           \
		   ((15U & (uint32_t)(crm)) << 1))

// - op0 = 3 : Moves to and from non-debug System registers, Special-purpose
//             registers
#define ISS_MRS_MSR_REVIDR_EL1	     ISS_OP0_OP1_CRN_CRM_OP2(3, 0, 0, 0, 6)
#define ISS_MRS_MSR_ID_PFR0_EL1	     ISS_OP0_OP1_CRN_CRM_OP2(3, 0, 0, 1, 0)
#define ISS_MRS_MSR_ID_PFR1_EL1	     ISS_OP0_OP1_CRN_CRM_OP2(3, 0, 0, 1, 1)
#define ISS_MRS_MSR_ID_PFR2_EL1	     ISS_OP0_OP1_CRN_CRM_OP2(3, 0, 0, 3, 4)
#define ISS_MRS_MSR_ID_DFR0_EL1	     ISS_OP0_OP1_CRN_CRM_OP2(3, 0, 0, 1, 2)
#define ISS_MRS_MSR_ID_AFR0_EL1	     ISS_OP0_OP1_CRN_CRM_OP2(3, 0, 0, 1, 3)
#define ISS_MRS_MSR_ID_MMFR0_EL1     ISS_OP0_OP1_CRN_CRM_OP2(3, 0, 0, 1, 4)
#define ISS_MRS_MSR_ID_MMFR1_EL1     ISS_OP0_OP1_CRN_CRM_OP2(3, 0, 0, 1, 5)
#define ISS_MRS_MSR_ID_MMFR2_EL1     ISS_OP0_OP1_CRN_CRM_OP2(3, 0, 0, 1, 6)
#define ISS_MRS_MSR_ID_MMFR3_EL1     ISS_OP0_OP1_CRN_CRM_OP2(3, 0, 0, 1, 7)
#define ISS_MRS_MSR_ID_MMFR4_EL1     ISS_OP0_OP1_CRN_CRM_OP2(3, 0, 0, 2, 6)
#define ISS_MRS_MSR_ID_ISAR0_EL1     ISS_OP0_OP1_CRN_CRM_OP2(3, 0, 0, 2, 0)
#define ISS_MRS_MSR_ID_ISAR1_EL1     ISS_OP0_OP1_CRN_CRM_OP2(3, 0, 0, 2, 1)
#define ISS_MRS_MSR_ID_ISAR2_EL1     ISS_OP0_OP1_CRN_CRM_OP2(3, 0, 0, 2, 2)
#define ISS_MRS_MSR_ID_ISAR3_EL1     ISS_OP0_OP1_CRN_CRM_OP2(3, 0, 0, 2, 3)
#define ISS_MRS_MSR_ID_ISAR4_EL1     ISS_OP0_OP1_CRN_CRM_OP2(3, 0, 0, 2, 4)
#define ISS_MRS_MSR_ID_ISAR5_EL1     ISS_OP0_OP1_CRN_CRM_OP2(3, 0, 0, 2, 5)
#define ISS_MRS_MSR_ID_ISAR6_EL1     ISS_OP0_OP1_CRN_CRM_OP2(3, 0, 0, 2, 7)
#define ISS_MRS_MSR_MVFR0_EL1	     ISS_OP0_OP1_CRN_CRM_OP2(3, 0, 0, 3, 0)
#define ISS_MRS_MSR_MVFR1_EL1	     ISS_OP0_OP1_CRN_CRM_OP2(3, 0, 0, 3, 1)
#define ISS_MRS_MSR_MVFR2_EL1	     ISS_OP0_OP1_CRN_CRM_OP2(3, 0, 0, 3, 2)
#define ISS_MRS_MSR_ID_AA64PFR0_EL1  ISS_OP0_OP1_CRN_CRM_OP2(3, 0, 0, 4, 0)
#define ISS_MRS_MSR_ID_AA64PFR1_EL1  ISS_OP0_OP1_CRN_CRM_OP2(3, 0, 0, 4, 1)
#define ISS_MRS_MSR_ID_AA64ZFR0_EL1  ISS_OP0_OP1_CRN_CRM_OP2(3, 0, 0, 4, 4)
#define ISS_MRS_MSR_ID_AA64SMFR0_EL1 ISS_OP0_OP1_CRN_CRM_OP2(3, 0, 0, 4, 5)
#define ISS_MRS_MSR_ID_AA64DFR0_EL1  ISS_OP0_OP1_CRN_CRM_OP2(3, 0, 0, 5, 0)
#define ISS_MRS_MSR_ID_AA64DFR1_EL1  ISS_OP0_OP1_CRN_CRM_OP2(3, 0, 0, 5, 1)
#define ISS_MRS_MSR_ID_AA64AFR0_EL1  ISS_OP0_OP1_CRN_CRM_OP2(3, 0, 0, 5, 4)
#define ISS_MRS_MSR_ID_AA64AFR1_EL1  ISS_OP0_OP1_CRN_CRM_OP2(3, 0, 0, 5, 5)
#define ISS_MRS_MSR_ID_AA64ISAR0_EL1 ISS_OP0_OP1_CRN_CRM_OP2(3, 0, 0, 6, 0)
#define ISS_MRS_MSR_ID_AA64ISAR1_EL1 ISS_OP0_OP1_CRN_CRM_OP2(3, 0, 0, 6, 1)
#define ISS_MRS_MSR_ID_AA64ISAR2_EL1 ISS_OP0_OP1_CRN_CRM_OP2(3, 0, 0, 6, 2)
#define ISS_MRS_MSR_ID_AA64MMFR0_EL1 ISS_OP0_OP1_CRN_CRM_OP2(3, 0, 0, 7, 0)
#define ISS_MRS_MSR_ID_AA64MMFR1_EL1 ISS_OP0_OP1_CRN_CRM_OP2(3, 0, 0, 7, 1)
#define ISS_MRS_MSR_ID_AA64MMFR2_EL1 ISS_OP0_OP1_CRN_CRM_OP2(3, 0, 0, 7, 2)
#define ISS_MRS_MSR_ID_AA64MMFR3_EL1 ISS_OP0_OP1_CRN_CRM_OP2(3, 0, 0, 7, 3)
#define ISS_MRS_MSR_ID_AA64MMFR4_EL1 ISS_OP0_OP1_CRN_CRM_OP2(3, 0, 0, 7, 4)
#define ISS_MRS_MSR_AIDR_EL1	     ISS_OP0_OP1_CRN_CRM_OP2(3, 1, 0, 0, 7)
#define ISS_MRS_MSR_PMCR_EL0	     ISS_OP0_OP1_CRN_CRM_OP2(3, 3, 9, 12, 0)
#define ISS_MRS_MSR_PMCNTENSET_EL0   ISS_OP0_OP1_CRN_CRM_OP2(3, 3, 9, 12, 1)
#define ISS_MRS_MSR_PMCNTENCLR_EL0   ISS_OP0_OP1_CRN_CRM_OP2(3, 3, 9, 12, 2)
#define ISS_MRS_MSR_PMOVSCLR_EL0     ISS_OP0_OP1_CRN_CRM_OP2(3, 3, 9, 12, 3)
#define ISS_MRS_MSR_PMSWINC_EL0	     ISS_OP0_OP1_CRN_CRM_OP2(3, 3, 9, 12, 4)
#define ISS_MRS_MSR_PMSELR_EL0	     ISS_OP0_OP1_CRN_CRM_OP2(3, 3, 9, 12, 5)
#define ISS_MRS_MSR_PMCEID0_EL0	     ISS_OP0_OP1_CRN_CRM_OP2(3, 3, 9, 12, 6)
#define ISS_MRS_MSR_PMCEID1_EL0	     ISS_OP0_OP1_CRN_CRM_OP2(3, 3, 9, 12, 7)
#define ISS_MRS_MSR_PMCCNTR_EL0	     ISS_OP0_OP1_CRN_CRM_OP2(3, 3, 9, 13, 0)
#define ISS_MRS_MSR_PMXEVTYPER_EL0   ISS_OP0_OP1_CRN_CRM_OP2(3, 3, 9, 13, 1)
#define ISS_MRS_MSR_PMXEVCNTR_EL0    ISS_OP0_OP1_CRN_CRM_OP2(3, 3, 9, 13, 2)
#define ISS_MRS_MSR_PMUSERENR_EL0    ISS_OP0_OP1_CRN_CRM_OP2(3, 3, 9, 14, 0)
#define ISS_MRS_MSR_PMINTENSET_EL1   ISS_OP0_OP1_CRN_CRM_OP2(3, 0, 9, 14, 1)
#define ISS_MRS_MSR_PMINTENCLR_EL1   ISS_OP0_OP1_CRN_CRM_OP2(3, 0, 9, 14, 2)
#define ISS_MRS_MSR_PMOVSSET_EL0     ISS_OP0_OP1_CRN_CRM_OP2(3, 3, 9, 14, 3)
#define ISS_MRS_MSR_PMCCFILTR_EL0    ISS_OP0_OP1_CRN_CRM_OP2(3, 3, 14, 15, 7)

#define ISS_MRS_MSR_SCTLR_EL1	   ISS_OP0_OP1_CRN_CRM_OP2(3, 0, 1, 0, 0)
#define ISS_MRS_MSR_ACTLR_EL1	   ISS_OP0_OP1_CRN_CRM_OP2(3, 0, 1, 0, 1)
#define ISS_MRS_MSR_TTBR0_EL1	   ISS_OP0_OP1_CRN_CRM_OP2(3, 0, 2, 0, 0)
#define ISS_MRS_MSR_TTBR1_EL1	   ISS_OP0_OP1_CRN_CRM_OP2(3, 0, 2, 0, 1)
#define ISS_MRS_MSR_TCR_EL1	   ISS_OP0_OP1_CRN_CRM_OP2(3, 0, 2, 0, 2)
#define ISS_MRS_MSR_ESR_EL1	   ISS_OP0_OP1_CRN_CRM_OP2(3, 0, 5, 2, 0)
#define ISS_MRS_MSR_FAR_EL1	   ISS_OP0_OP1_CRN_CRM_OP2(3, 0, 6, 0, 0)
#define ISS_MRS_MSR_AFSR0_EL1	   ISS_OP0_OP1_CRN_CRM_OP2(3, 0, 5, 1, 0)
#define ISS_MRS_MSR_AFSR1_EL1	   ISS_OP0_OP1_CRN_CRM_OP2(3, 0, 5, 1, 1)
#define ISS_MRS_MSR_MAIR_EL1	   ISS_OP0_OP1_CRN_CRM_OP2(3, 0, 10, 2, 0)
#define ISS_MRS_MSR_AMAIR_EL1	   ISS_OP0_OP1_CRN_CRM_OP2(3, 0, 10, 3, 0)
#define ISS_MRS_MSR_CONTEXTIDR_EL1 ISS_OP0_OP1_CRN_CRM_OP2(3, 0, 13, 0, 1)

#define ISS_MRS_MSR_ICC_IAR0_EL1    ISS_OP0_OP1_CRN_CRM_OP2(3, 0, 12, 8, 0)
#define ISS_MRS_MSR_ICC_EOIR0_EL1   ISS_OP0_OP1_CRN_CRM_OP2(3, 0, 12, 8, 1)
#define ISS_MRS_MSR_ICC_HPPIR0_EL1  ISS_OP0_OP1_CRN_CRM_OP2(3, 0, 12, 8, 2)
#define ISS_MRS_MSR_ICC_BPR0_EL1    ISS_OP0_OP1_CRN_CRM_OP2(3, 0, 12, 8, 3)
#define ISS_MRS_MSR_ICC_AP0R0_EL1   ISS_OP0_OP1_CRN_CRM_OP2(3, 0, 12, 8, 4)
#define ISS_MRS_MSR_ICC_AP0R1_EL1   ISS_OP0_OP1_CRN_CRM_OP2(3, 0, 12, 8, 5)
#define ISS_MRS_MSR_ICC_AP0R2_EL1   ISS_OP0_OP1_CRN_CRM_OP2(3, 0, 12, 8, 6)
#define ISS_MRS_MSR_ICC_AP0R3_EL1   ISS_OP0_OP1_CRN_CRM_OP2(3, 0, 12, 8, 7)
#define ISS_MRS_MSR_ICC_AP1R0_EL1   ISS_OP0_OP1_CRN_CRM_OP2(3, 0, 12, 9, 0)
#define ISS_MRS_MSR_ICC_AP1R1_EL1   ISS_OP0_OP1_CRN_CRM_OP2(3, 0, 12, 9, 1)
#define ISS_MRS_MSR_ICC_AP1R2_EL1   ISS_OP0_OP1_CRN_CRM_OP2(3, 0, 12, 9, 2)
#define ISS_MRS_MSR_ICC_AP1R3_EL1   ISS_OP0_OP1_CRN_CRM_OP2(3, 0, 12, 9, 3)
#define ISS_MRS_MSR_ICC_DIR_EL1	    ISS_OP0_OP1_CRN_CRM_OP2(3, 0, 12, 11, 1)
#define ISS_MRS_MSR_ICC_SGI1R_EL1   ISS_OP0_OP1_CRN_CRM_OP2(3, 0, 12, 11, 5)
#define ISS_MRS_MSR_ICC_ASGI1R_EL1  ISS_OP0_OP1_CRN_CRM_OP2(3, 0, 12, 11, 6)
#define ISS_MRS_MSR_ICC_SGI0R_EL1   ISS_OP0_OP1_CRN_CRM_OP2(3, 0, 12, 11, 7)
#define ISS_MRS_MSR_ICC_IAR1_EL1    ISS_OP0_OP1_CRN_CRM_OP2(3, 0, 12, 12, 0)
#define ISS_MRS_MSR_ICC_EOIR1_EL1   ISS_OP0_OP1_CRN_CRM_OP2(3, 0, 12, 12, 1)
#define ISS_MRS_MSR_ICC_HPPIR1_EL1  ISS_OP0_OP1_CRN_CRM_OP2(3, 0, 12, 12, 2)
#define ISS_MRS_MSR_ICC_BPR1_EL1    ISS_OP0_OP1_CRN_CRM_OP2(3, 0, 12, 12, 3)
#define ISS_MRS_MSR_ICC_SRE_EL1	    ISS_OP0_OP1_CRN_CRM_OP2(3, 0, 12, 12, 5)
#define ISS_MRS_MSR_ICC_IGRPEN0_EL1 ISS_OP0_OP1_CRN_CRM_OP2(3, 0, 12, 12, 6)
#define ISS_MRS_MSR_ICC_IGRPEN1_EL1 ISS_OP0_OP1_CRN_CRM_OP2(3, 0, 12, 12, 7)

#define ISS_MRS_MSR_IMP_CLUSTERIDR_EL1 ISS_OP0_OP1_CRN_CRM_OP2(3, 0, 15, 3, 1)

#define ISS_MRS_MSR_DC_CSW  ISS_OP0_OP1_CRN_CRM_OP2(1, 0, 7, 10, 2)
#define ISS_MRS_MSR_DC_CISW ISS_OP0_OP1_CRN_CRM_OP2(1, 0, 7, 14, 2)
#define ISS_MRS_MSR_DC_ISW  ISS_OP0_OP1_CRN_CRM_OP2(1, 0, 7, 6, 2)

#if defined(ARCH_ARM_FEAT_AMUv1) || defined(ARCH_ARM_FEAT_AMUv1p1)
#define ISS_MRS_MSR_AMCR_EL0	    ISS_OP0_OP1_CRN_CRM_OP2(3, 3, 13, 2, 0)
#define ISS_MRS_MSR_AMCFGR_EL0	    ISS_OP0_OP1_CRN_CRM_OP2(3, 3, 13, 2, 1)
#define ISS_MRS_MSR_AMCGCR_EL0	    ISS_OP0_OP1_CRN_CRM_OP2(3, 3, 13, 2, 2)
#define ISS_MRS_MSR_AMUSERENR_EL0   ISS_OP0_OP1_CRN_CRM_OP2(3, 3, 13, 2, 3)
#define ISS_MRS_MSR_AMCNTENCLR0_EL0 ISS_OP0_OP1_CRN_CRM_OP2(3, 3, 13, 2, 4)
#define ISS_MRS_MSR_AMCNTENSET0_EL0 ISS_OP0_OP1_CRN_CRM_OP2(3, 3, 13, 2, 5)
#define ISS_MRS_MSR_AMCNTENCLR1_EL0 ISS_OP0_OP1_CRN_CRM_OP2(3, 3, 13, 3, 0)
#define ISS_MRS_MSR_AMCNTENSET1_EL0 ISS_OP0_OP1_CRN_CRM_OP2(3, 3, 13, 3, 1)
#if defined(ARCH_ARM_FEAT_AMUv1p1)
#define ISS_MRS_MSR_AMCG1IDR_EL0 ISS_OP0_OP1_CRN_CRM_OP2(3, 3, 13, 2, 6)
#endif
#endif

```

`hyp/arch/aarch64/include/reg/registers_arm.inc`:

```inc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier; BSD-3-Clause

#if defined(ARCH_ARM_FEAT_VHE)
#define VHE(X)	X##2 [X!]
#define VHE_V(X, V)	X##2 [V]
#define VHE_T(X, T)	X##2 <T> [X!]
#else
#define VHE(X)		X
#define VHE_V(X, V)	X [V]
#define VHE_T(X, T) 	X <T>
#endif

```

`hyp/arch/aarch64/link.lds`:

```lds
/*
 * © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
 *
 * SPDX-License-Identifier: BSD-3-Clause
 */

#define __ASSEMBLER__
#include <hypconstants.h>

#if ((PLATFORM_LMA_BASE & ((1 << 21) - 1)) != 0)
#error PLATFORM_LMA_BASE is not aligned to 2MB
#endif

EXTERN(__entry_el2)
ENTRY(__entry_el2_phys)

PHDRS
{
	text PT_LOAD;
	rodata PT_LOAD;
	dynreloc PT_LOAD;
	dynamic PT_DYNAMIC;
	data PT_LOAD;
	tls PT_TLS;
}


SECTIONS
{
	__entry_el2_phys = ABSOLUTE(__entry_el2 + PLATFORM_LMA_BASE);
	. = 0x0;
	image_virt_start = ADDR(.text);
	image_phys_start = LOADADDR(.text);
	.text : AT (PLATFORM_LMA_BASE) {
		*(.text.boot)
		*(.text.boot.*)
		*(SORT_BY_ALIGNMENT(.text .text.*))
		KEEP(*(.text.debug));
	} : text

	. = ALIGN(4096);
	rodata_base = .;
	.rodata : {
		*(SORT_BY_ALIGNMENT(.rodata .rodata.*))
	} : rodata

	. = ALIGN(8);
	.rela.dyn : { *(.rela.dyn) }

	. = ALIGN(8);
	.gnu.hash : { *(.gnu.hash) }

	. = ALIGN(8);
	.note : { *(.note) *(.note.*) }

	. = ALIGN(4096);
	.dynsym : { *(.dynsym) }
	. = ALIGN(8);
	.dynstr : { *(.dynstr) }

	. = ALIGN(4096);
	.dynamic : { *(.dynamic) } : dynamic : dynreloc

	/* Package data for RootVM */
	. = ALIGN(4096);
#ifdef PLATFORM_ROOTVM_PKG_START_BASE
	image_pkg_start = PLATFORM_ROOTVM_PKG_START_BASE;
#else
	image_pkg_start = . - image_virt_start + image_phys_start;
#endif

	/* align RW sections to the next 2MB page */
	. = ALIGN(0x200000);
	data_base = .;
	.data : {
		*(SORT_BY_ALIGNMENT(.data .data.*))
	} : data

	. = ALIGN(8);
	bss_base = .;
	.bss (NOLOAD) : {
		*(SORT_BY_ALIGNMENT(.bss .bss.*))
		*(COMMON)
	}

#if defined(TRACE_BOOT_ENTRIES)
	. = ALIGN(64);
	.heap.trace (NOLOAD) : {
		trace_boot_buffer = .;
		. = . + (TRACE_BUFFER_ENTRY_SIZE * TRACE_BOOT_ENTRIES);
	} : NONE
#endif

	. = ALIGN(64);
	. = ALIGN(MEMDB_MIN_SIZE);
#if PLATFORM_RW_DATA_SIZE < 0x200000
#error PLATFORM_RW_DATA_SIZE too small
#endif
	.heap.root (NOLOAD) : {
		heap_private_start = .;
		. = data_base + PLATFORM_RW_DATA_SIZE;
		heap_private_end = .;
	} : NONE
	image_virt_last = . - 1;
	image_phys_last = . - 1 - image_virt_start + image_phys_start;
	image_virt_end = .;

	/* Thread local storage - note, we do not support tdata! */
	. = 0;

	/* Force a link error if tdata present */
	.tdata : { *(.tdata) *(.tdata.*) } : tls
	ASSERT(. == 0, ".tdata: initialized tls is not supported")

	.tbss : {
		*(.tbss.current_thread)
		*(SORT_BY_ALIGNMENT(.tbss .tbss.*))
	} : tls
	tbss_size = SIZEOF(.tbss);
	tbss_align = ALIGNOF(.tbss);
}

```

`hyp/arch/aarch64/registers.reg`:

```reg
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier; BSD-3-Clause

#include <reg/registers_arm.inc>

ACTLR_EL1
ACTLR_EL2
VHE(AFSR0_EL1)
AFSR0_EL2
VHE(AFSR1_EL1)
AFSR1_EL2
AIDR_EL1 r
VHE(AMAIR_EL1)
AMAIR_EL2
CCSIDR_EL1 R
CCSIDR2_EL1 R
CLIDR_EL1 r
CNTFRQ_EL0
#if defined(ARCH_ARM_FEAT_VHE)
CNTHCTL_EL2 [E2H0 E2H1]
#else
CNTHCTL_EL2 [E2H0]
#endif
CNTHP_CTL_EL2 <CNT_CTL!> Orw
CNTHP_CVAL_EL2 <CNT_CVAL!> oOwR
CNTHP_TVAL_EL2 <CNT_TVAL!>
//CNTHPS_CTL_EL2
//CNTHPS_CVAL_EL2
//CNTHPS_TVAL_EL2
//CNTHV_CTL_EL2
//CNTHV_CVAL_EL2
//CNTHV_TVAL_EL2
//CNTHVS_CTL_EL2
//CNTHVS_CVAL_EL2
//CNTHVS_TVAL_EL2
VHE(CNTKCTL_EL1) oOrw

VHE_T(CNTP_CTL_EL0, CNT_CTL!) oOrwR
VHE_T(CNTP_CVAL_EL0, CNT_CVAL!) oOrwR
VHE_T(CNTP_TVAL_EL0, CNT_TVAL!)
CNTPCT_EL0 oOR
//CNTPS_CTL_EL1
//CNTPS_CVAL_EL1
//CNTPS_TVAL_EL1
VHE_T(CNTV_CTL_EL0, CNT_CTL!) oOrwR
VHE_T(CNTV_CVAL_EL0, CNT_CVAL!) oOrwR
VHE_T(CNTV_TVAL_EL0, CNT_TVAL!)

CNTVCT_EL0 R
#if defined(ARCH_ARM_FEAT_ECV)
CNTPOFF_EL2
#endif
CNTVOFF_EL2
VHE(CONTEXTIDR_EL1)
#if defined(ARCH_ARM_FEAT_VHE)
CONTEXTIDR_EL2
#endif
VHE(CPACR_EL1)
#if defined(ARCH_ARM_FEAT_VHE)
CPTR_EL2 [E2H0 E2H1] oOrw
#else
CPTR_EL2 [E2H0] oOrw
#endif
CSSELR_EL1
CTR_EL0 r
CurrentEL
DACR32_EL2
DAIF
DCZID_EL0 r
//DISR_EL1
VHE(ELR_EL1)

// We should never access ELR_EL2 or SPSR_EL2 from C since its value may
// change with preemption.
//ELR_EL2
//SPSR_EL2 [A32 A64]

#if (defined(ARCH_ARM_FEAT_RAS) || defined(ARCH_ARM_FEAT_RASv1p1))
//ERRIDR_EL1 r
ERRSELR_EL1 Orw
//ERXADDR_EL1
//ERXCTLR_EL1
//ERXFR_EL1 r
ERXMISC0_EL1 <uint64> Orw
//ERXMISC1_EL1
//ERXMISC2_EL1
//ERXMISC3_EL1
//ERXPFGCDN_EL1
//ERXPFGCTL_EL1
//ERXPFGF_EL1 r
ERXSTATUS_EL1 Orw
#endif

VHE(ESR_EL1)
ESR_EL2 Or
VHE(FAR_EL1)
FAR_EL2 Or
FPCR
FPSR
FPEXC32_EL2
HACR_EL2
HCR_EL2
HPFAR_EL2 Or
HSTR_EL2
//ID_AA64AFR0_EL1 r
//ID_AA64AFR1_EL1 r
ID_AA64DFR0_EL1 r
//ID_AA64DFR1_EL1 r
ID_AA64ISAR0_EL1 r
ID_AA64ISAR1_EL1 r
ID_AA64ISAR2_EL1 r
ID_AA64MMFR0_EL1 r
ID_AA64MMFR1_EL1 r
ID_AA64MMFR2_EL1 r
// Replace with the correct register names once we upgrade to a version of LLVM
// that recognises them
// FIXME:
S3_0_C0_C7_3 [ID_AA64MMFR3_EL1!] r
S3_0_C0_C7_4 [ID_AA64MMFR4_EL1!] r
ID_AA64PFR0_EL1 r
ID_AA64PFR1_EL1 r
//ID_AA64ZFR0_EL1 r
//ID_AFR0_EL1 r
ID_DFR0_EL1 r
//ID_ISAR0_EL1 r
//ID_ISAR1_EL1 r
//ID_ISAR2_EL1 r
//ID_ISAR3_EL1 r
//ID_ISAR4_EL1 r
//ID_ISAR5_EL1 r
//S3_0_C0_C2_7 [ID_ISAR6_EL1!] r
//ID_MMFR0_EL1 r
//ID_MMFR1_EL1 r
//ID_MMFR2_EL1 r
//ID_MMFR3_EL1 r
ID_MMFR4_EL1 r
ID_PFR0_EL1 r
ID_PFR1_EL1 r
ID_PFR2_EL1 r
//IFSR32_EL2
ISR_EL1 OR
//LORC_EL1
//LOREA_EL1
//LORID_EL1 r
//LORN_EL1
//LORSA_EL1
VHE(MAIR_EL1)
MAIR_EL2
MIDR_EL1 r
MPIDR_EL1 r
//MVFR0_EL1 r
//MVFR2_EL1 r
NZCV
PAN rW
PAR_EL1 [base] oOrwR
//PMBIDR_EL1 r
//PMBLIMITR_EL1
//PMBPTR_EL1
//PMBSR_EL1
//PMCCFILTR_EL0
//PMCCNTR_EL0
//PMCEID0_EL0 r
//PMCEID1_EL0 r
//PMCNTENCLR_EL0
PMCNTENSET_EL0 <uint64> r
PMCR_EL0 oOrw
//PMEVCNTR0_EL0
//PMEVCNTR1_EL0
//PMEVCNTR2_EL0
//PMEVCNTR3_EL0
//PMEVCNTR4_EL0
//PMEVCNTR5_EL0
//PMEVCNTR6_EL0
//PMEVCNTR7_EL0
//PMEVCNTR8_EL0
//PMEVCNTR9_EL0
//PMEVCNTR10_EL0
//PMEVCNTR11_EL0
//PMEVCNTR12_EL0
//PMEVCNTR13_EL0
//PMEVCNTR14_EL0
//PMEVCNTR15_EL0
//PMEVCNTR16_EL0
//PMEVCNTR17_EL0
//PMEVCNTR18_EL0
//PMEVCNTR19_EL0
//PMEVCNTR20_EL0
//PMEVCNTR21_EL0
//PMEVCNTR22_EL0
//PMEVCNTR23_EL0
//PMEVCNTR24_EL0
//PMEVCNTR25_EL0
//PMEVCNTR26_EL0
//PMEVCNTR27_EL0
//PMEVCNTR28_EL0
//PMEVCNTR29_EL0
//PMEVCNTR30_EL0
//PMEVTYPER0_EL0
//PMEVTYPER1_EL0
//PMEVTYPER2_EL0
//PMEVTYPER3_EL0
//PMEVTYPER4_EL0
//PMEVTYPER5_EL0
//PMEVTYPER6_EL0
//PMEVTYPER7_EL0
//PMEVTYPER8_EL0
//PMEVTYPER9_EL0
//PMEVTYPER10_EL0
//PMEVTYPER11_EL0
//PMEVTYPER12_EL0
//PMEVTYPER13_EL0
//PMEVTYPER14_EL0
//PMEVTYPER15_EL0
//PMEVTYPER16_EL0
//PMEVTYPER17_EL0
//PMEVTYPER18_EL0
//PMEVTYPER19_EL0
//PMEVTYPER20_EL0
//PMEVTYPER21_EL0
//PMEVTYPER22_EL0
//PMEVTYPER23_EL0
//PMEVTYPER24_EL0
//PMEVTYPER25_EL0
//PMEVTYPER26_EL0
//PMEVTYPER27_EL0
//PMEVTYPER28_EL0
//PMEVTYPER29_EL0
//PMEVTYPER30_EL0
//PMINTENCLR_EL1
//PMINTENSET_EL1
//PMMIR_EL1 r
//PMOVSCLR_EL0
//PMOVSSET_EL0
//VHE(PMSCR_EL1)
//PMSCR_EL2
//PMSELR_EL0
//PMSEVFR_EL1
//PMSFCR_EL1
//PMSICR_EL1
//PMSIDR_EL1 r
//PMSIRR_EL1
//PMSLATFR_EL1
//PMSWINC_EL0 w
//PMUSERENR_EL0
//PMXEVCNTR_EL0
//PMXEVTYPER_EL0
//REVIDR_EL1 r
VHE(SCTLR_EL1)
#if defined(ARCH_ARM_FEAT_VHE)
SCTLR_EL2 [VM E2H_TGE]
#else
SCTLR_EL2 [VM]
#endif
//SDER32_EL2
SP_EL0
SP_EL1
SP_EL2
SPSel
VHE_V(SPSR_EL1, SPSR_EL1_A64!)
VHE(TCR_EL1)
#if defined(ARCH_ARM_FEAT_VHE)
TCR_EL2 [E2H0 E2H1] rwW
#else
TCR_EL2 [E2H0] rwW
#endif
TPIDR_EL0
TPIDR_EL1
TPIDR_EL2
TPIDRRO_EL0
#if defined(ARCH_ARM_FEAT_TRF)
VHE(TRFCR_EL1) Orw
#endif
VHE(TTBR0_EL1)
TTBR0_EL2 rwW
VHE(TTBR1_EL1)
TTBR1_EL2 rW
UAO
VHE(VBAR_EL1)
VBAR_EL2
VDISR_EL2 <uint64> rw
VMPIDR_EL2 <MPIDR_EL1>
//VNCR_EL2
VPIDR_EL2 <MIDR_EL1>
VSESR_EL2 <uint64> w
//VSTCR_EL2
//VSTTBR_EL2
VTCR_EL2
VTTBR_EL2 oOrw
//VHE(ZCR_EL1)
//ZCR_EL2

#if defined(AARCH64_ICC_REGS)
// GIC-related regs:
ICC_ASGI1R_EL1 <ICC_SGIR_EL1> W
ICC_CTLR_EL1 oOrw
ICC_DIR_EL1 Ow
ICC_EOIR1_EL1 <ICC_EOIR_EL1> Ow
ICC_HPPIR1_EL1 <ICC_HPPIR_EL1> r
ICC_IAR1_EL1 <ICC_IAR_EL1> OR
ICC_IGRPEN1_EL1 <ICC_IGRPEN_EL1> Ow
ICC_PMR_EL1 Orw
ICC_SGI0R_EL1 <ICC_SGIR_EL1> Ow
ICC_SGI1R_EL1 <ICC_SGIR_EL1> Ow
ICC_SRE_EL1
ICC_SRE_EL2 Orw
ICH_AP0R0_EL2 <uint32> oOrw
ICH_AP0R1_EL2 <uint32> oOrw
ICH_AP0R2_EL2 <uint32> oOrw
ICH_AP0R3_EL2 <uint32> oOrw
ICH_AP1R0_EL2 <uint32> oOrw
ICH_AP1R1_EL2 <uint32> oOrw
ICH_AP1R2_EL2 <uint32> oOrw
ICH_AP1R3_EL2 <uint32> oOrw
ICH_EISR_EL2 <uint16> r
ICH_ELRSR_EL2 <uint16> Or
ICH_HCR_EL2
ICH_LR0_EL2 <ICH_LR_EL2> [HW0 HW1 base] Orw
ICH_LR1_EL2 <ICH_LR_EL2> [HW0 HW1 base] Orw
ICH_LR2_EL2 <ICH_LR_EL2> [HW0 HW1 base] Orw
ICH_LR3_EL2 <ICH_LR_EL2> [HW0 HW1 base] Orw
ICH_LR4_EL2 <ICH_LR_EL2> [HW0 HW1 base] Orw
ICH_LR5_EL2 <ICH_LR_EL2> [HW0 HW1 base] Orw
ICH_LR6_EL2 <ICH_LR_EL2> [HW0 HW1 base] Orw
ICH_LR7_EL2 <ICH_LR_EL2> [HW0 HW1 base] Orw
ICH_LR8_EL2 <ICH_LR_EL2> [HW0 HW1 base] Orw
ICH_LR9_EL2 <ICH_LR_EL2> [HW0 HW1 base] Orw
ICH_LR10_EL2 <ICH_LR_EL2> [HW0 HW1 base] Orw
ICH_LR11_EL2 <ICH_LR_EL2> [HW0 HW1 base] Orw
ICH_LR12_EL2 <ICH_LR_EL2> [HW0 HW1 base] Orw
ICH_LR13_EL2 <ICH_LR_EL2> [HW0 HW1 base] Orw
ICH_LR14_EL2 <ICH_LR_EL2> [HW0 HW1 base] Orw
ICH_LR15_EL2 <ICH_LR_EL2> [HW0 HW1 base] Orw
ICH_MISR_EL2 r
ICH_VMCR_EL2 oOrw
ICH_VTR_EL2 r
#endif

#if defined(ARCH_ARM_FEAT_PAuth)
APDAKeyHi_EL1 <uint64> rw
APDAKeyLo_EL1 <uint64> rw
APDBKeyHi_EL1 <uint64> rw
APDBKeyLo_EL1 <uint64> rw
APIAKeyHi_EL1 <uint64> rw
APIAKeyLo_EL1 <uint64> rw
APIBKeyHi_EL1 <uint64> rw
APIBKeyLo_EL1 <uint64> rw
APGAKeyHi_EL1 <uint64> rw
APGAKeyLo_EL1 <uint64> rw
#endif

#if defined(ARCH_ARM_FEAT_MTE) && defined(INTERFACE_VCPU)
GCR_EL1 rw
RGSR_EL1 rw
VHE(TFSR_EL1) rw
TFSRE0_EL1 rw
#endif

#if defined(MODULE_CORE_DEBUG)
// DBGAUTHSTATUS_EL1
DBGBCR0_EL1 <DBGBCR_EL1> Orw
DBGBCR1_EL1 <DBGBCR_EL1> Orw
DBGBCR2_EL1 <DBGBCR_EL1> Orw
DBGBCR3_EL1 <DBGBCR_EL1> Orw
DBGBCR4_EL1 <DBGBCR_EL1> Orw
DBGBCR5_EL1 <DBGBCR_EL1> Orw
DBGBCR6_EL1 <DBGBCR_EL1> Orw
DBGBCR7_EL1 <DBGBCR_EL1> Orw
DBGBCR8_EL1 <DBGBCR_EL1> Orw
DBGBCR9_EL1 <DBGBCR_EL1> Orw
DBGBCR10_EL1 <DBGBCR_EL1> Orw
DBGBCR11_EL1 <DBGBCR_EL1> Orw
DBGBCR12_EL1 <DBGBCR_EL1> Orw
DBGBCR13_EL1 <DBGBCR_EL1> Orw
DBGBCR14_EL1 <DBGBCR_EL1> Orw
DBGBCR15_EL1 <DBGBCR_EL1> Orw
DBGBVR0_EL1 <uint64> Orw
DBGBVR1_EL1 <uint64> Orw
DBGBVR2_EL1 <uint64> Orw
DBGBVR3_EL1 <uint64> Orw
DBGBVR4_EL1 <uint64> Orw
DBGBVR5_EL1 <uint64> Orw
DBGBVR6_EL1 <uint64> Orw
DBGBVR7_EL1 <uint64> Orw
DBGBVR8_EL1 <uint64> Orw
DBGBVR9_EL1 <uint64> Orw
DBGBVR10_EL1 <uint64> Orw
DBGBVR11_EL1 <uint64> Orw
DBGBVR12_EL1 <uint64> Orw
DBGBVR13_EL1 <uint64> Orw
DBGBVR14_EL1 <uint64> Orw
DBGBVR15_EL1 <uint64> Orw
DBGCLAIMCLR_EL1 <DBGCLAIM_EL1> Orw
DBGCLAIMSET_EL1 <DBGCLAIM_EL1> Ow
//DBGDTR_EL0 <uint64>
//DBGDTRRX_EL0 <uint32> Or
//DBGDTRTX_EL0 <uint32> Ow
//DBGPRCR_EL1
#if ARCH_AARCH64_32BIT_EL1
DBGVCR32_EL2 <uint32> Orw
#endif
DBGWCR0_EL1 <DBGWCR_EL1> Orw
DBGWCR1_EL1 <DBGWCR_EL1> Orw
DBGWCR2_EL1 <DBGWCR_EL1> Orw
DBGWCR3_EL1 <DBGWCR_EL1> Orw
DBGWCR4_EL1 <DBGWCR_EL1> Orw
DBGWCR5_EL1 <DBGWCR_EL1> Orw
DBGWCR6_EL1 <DBGWCR_EL1> Orw
DBGWCR7_EL1 <DBGWCR_EL1> Orw
DBGWCR8_EL1 <DBGWCR_EL1> Orw
DBGWCR9_EL1 <DBGWCR_EL1> Orw
DBGWCR10_EL1 <DBGWCR_EL1> Orw
DBGWCR11_EL1 <DBGWCR_EL1> Orw
DBGWCR12_EL1 <DBGWCR_EL1> Orw
DBGWCR13_EL1 <DBGWCR_EL1> Orw
DBGWCR14_EL1 <DBGWCR_EL1> Orw
DBGWCR15_EL1 <DBGWCR_EL1> Orw
DBGWVR0_EL1 <uint64> Orw
DBGWVR1_EL1 <uint64> Orw
DBGWVR2_EL1 <uint64> Orw
DBGWVR3_EL1 <uint64> Orw
DBGWVR4_EL1 <uint64> Orw
DBGWVR5_EL1 <uint64> Orw
DBGWVR6_EL1 <uint64> Orw
DBGWVR7_EL1 <uint64> Orw
DBGWVR8_EL1 <uint64> Orw
DBGWVR9_EL1 <uint64> Orw
DBGWVR10_EL1 <uint64> Orw
DBGWVR11_EL1 <uint64> Orw
DBGWVR12_EL1 <uint64> Orw
DBGWVR13_EL1 <uint64> Orw
DBGWVR14_EL1 <uint64> Orw
DBGWVR15_EL1 <uint64> Orw
MDCCINT_EL1 Orw
//MDCCSR_EL0 r
MDCR_EL2 oOrw
//MDRAR_EL1
MDSCR_EL1 Orw
OSDLR_EL1 Orw
OSDTRRX_EL1 <uint32> Orw
OSDTRTX_EL1 <uint32> Orw
OSECCR_EL1 <uint32> Orw
OSLAR_EL1 Orw
OSLSR_EL1 Or
//SDER32_EL2
#endif

#if defined(ARCH_ARM_FEAT_TRF)
TRFCR_EL2 Orw
#endif

#if defined(MODULE_PLATFORM_ETE)
TRCLAR <uint32> Orw
TRCIDR0 r
TRCIDR2 r
TRCIDR3 r
TRCIDR4 r
TRCIDR5 r
TRCSTATR Orw
TRCPRGCTLR <uint64> Orw
TRCCONFIGR <uint64> Orw
TRCAUXCTLR <uint64> Orw
TRCEVENTCTL0R <uint64> Orw
TRCEVENTCTL1R <uint64> Orw
TRCRSR <uint64> Orw
TRCSTALLCTLR <uint64> Orw
TRCTSCTLR <uint64> Orw
TRCSYNCPR <uint64> Orw
TRCCCCTLR <uint64> Orw
TRCBBCTLR <uint64> Orw
TRCTRACEIDR <uint64> Orw
TRCQCTLR <uint64> Orw
TRCVICTLR <uint64> Orw
TRCVIIECTLR <uint64> Orw
TRCVISSCTLR <uint64> Orw
TRCVIPCSSCTLR <uint64> Orw
TRCSEQEVR0 <uint64> Orw
TRCSEQEVR1 <uint64> Orw
TRCSEQEVR2 <uint64> Orw
TRCSEQRSTEVR <uint64> Orw
TRCSEQSTR <uint64> Orw
TRCEXTINSELR0 <uint64> Orw
TRCEXTINSELR1 <uint64> Orw
TRCEXTINSELR2 <uint64> Orw
TRCEXTINSELR3 <uint64> Orw
TRCCNTRLDVR0 <uint64> Orw
TRCCNTRLDVR1 <uint64> Orw
TRCCNTRLDVR2 <uint64> Orw
TRCCNTRLDVR3 <uint64> Orw
TRCCNTCTLR0 <uint64> Orw
TRCCNTCTLR1 <uint64> Orw
TRCCNTCTLR2 <uint64> Orw
TRCCNTCTLR3 <uint64> Orw
TRCCNTVR0 <uint64> Orw
TRCCNTVR1 <uint64> Orw
TRCCNTVR2 <uint64> Orw
TRCCNTVR3 <uint64> Orw
TRCIMSPEC1 <uint64> Orw
TRCIMSPEC2 <uint64> Orw
TRCIMSPEC3 <uint64> Orw
TRCIMSPEC4 <uint64> Orw
TRCIMSPEC5 <uint64> Orw
TRCIMSPEC6 <uint64> Orw
TRCIMSPEC7 <uint64> Orw
TRCRSCTLR2 <uint64> Orw
TRCRSCTLR3 <uint64> Orw
TRCRSCTLR4 <uint64> Orw
TRCRSCTLR5 <uint64> Orw
TRCRSCTLR6 <uint64> Orw
TRCRSCTLR7 <uint64> Orw
TRCRSCTLR8 <uint64> Orw
TRCRSCTLR9 <uint64> Orw
TRCRSCTLR10 <uint64> Orw
TRCRSCTLR11 <uint64> Orw
TRCRSCTLR12 <uint64> Orw
TRCRSCTLR13 <uint64> Orw
TRCRSCTLR14 <uint64> Orw
TRCRSCTLR15 <uint64> Orw
TRCRSCTLR16 <uint64> Orw
TRCRSCTLR17 <uint64> Orw
TRCRSCTLR18 <uint64> Orw
TRCRSCTLR19 <uint64> Orw
TRCRSCTLR20 <uint64> Orw
TRCRSCTLR21 <uint64> Orw
TRCRSCTLR22 <uint64> Orw
TRCRSCTLR23 <uint64> Orw
TRCRSCTLR24 <uint64> Orw
TRCRSCTLR25 <uint64> Orw
TRCRSCTLR26 <uint64> Orw
TRCRSCTLR27 <uint64> Orw
TRCRSCTLR28 <uint64> Orw
TRCRSCTLR29 <uint64> Orw
TRCRSCTLR30 <uint64> Orw
TRCRSCTLR31 <uint64> Orw
TRCSSCCR0 <uint64> Orw
TRCSSCCR1 <uint64> Orw
TRCSSCCR2 <uint64> Orw
TRCSSCCR3 <uint64> Orw
TRCSSCCR4 <uint64> Orw
TRCSSCCR5 <uint64> Orw
TRCSSCCR6 <uint64> Orw
TRCSSCCR7 <uint64> Orw
TRCSSCSR0 <uint64> Orw
TRCSSCSR1 <uint64> Orw
TRCSSCSR2 <uint64> Orw
TRCSSCSR3 <uint64> Orw
TRCSSCSR4 <uint64> Orw
TRCSSCSR5 <uint64> Orw
TRCSSCSR6 <uint64> Orw
TRCSSCSR7 <uint64> Orw
TRCSSPCICR0 <uint64> Orw
TRCSSPCICR1 <uint64> Orw
TRCSSPCICR2 <uint64> Orw
TRCSSPCICR3 <uint64> Orw
TRCSSPCICR4 <uint64> Orw
TRCSSPCICR5 <uint64> Orw
TRCSSPCICR6 <uint64> Orw
TRCSSPCICR7 <uint64> Orw
TRCACVR0 <uint64> Orw
TRCACVR1 <uint64> Orw
TRCACVR2 <uint64> Orw
TRCACVR3 <uint64> Orw
TRCACVR4 <uint64> Orw
TRCACVR5 <uint64> Orw
TRCACVR6 <uint64> Orw
TRCACVR7 <uint64> Orw
TRCACVR8 <uint64> Orw
TRCACVR9 <uint64> Orw
TRCACVR10 <uint64> Orw
TRCACVR11 <uint64> Orw
TRCACVR12 <uint64> Orw
TRCACVR13 <uint64> Orw
TRCACVR14 <uint64> Orw
TRCACVR15 <uint64> Orw
TRCACATR0 <uint64> Orw
TRCACATR1 <uint64> Orw
TRCACATR2 <uint64> Orw
TRCACATR3 <uint64> Orw
TRCACATR4 <uint64> Orw
TRCACATR5 <uint64> Orw
TRCACATR6 <uint64> Orw
TRCACATR7 <uint64> Orw
TRCACATR8 <uint64> Orw
TRCACATR9 <uint64> Orw
TRCACATR10 <uint64> Orw
TRCACATR11 <uint64> Orw
TRCACATR12 <uint64> Orw
TRCACATR13 <uint64> Orw
TRCACATR14 <uint64> Orw
TRCACATR15 <uint64> Orw
TRCCIDCVR0 <uint64> Orw
TRCCIDCVR1 <uint64> Orw
TRCCIDCVR2 <uint64> Orw
TRCCIDCVR3 <uint64> Orw
TRCCIDCVR4 <uint64> Orw
TRCCIDCVR5 <uint64> Orw
TRCCIDCVR6 <uint64> Orw
TRCCIDCVR7 <uint64> Orw
TRCVMIDCVR0 <uint64> Orw
TRCVMIDCVR1 <uint64> Orw
TRCVMIDCVR2 <uint64> Orw
TRCVMIDCVR3 <uint64> Orw
TRCVMIDCVR4 <uint64> Orw
TRCVMIDCVR5 <uint64> Orw
TRCVMIDCVR6 <uint64> Orw
TRCVMIDCVR7 <uint64> Orw
TRCCIDCCTLR0 <uint64> Orw
TRCCIDCCTLR1 <uint64> Orw
TRCVMIDCCTLR0 <uint64> Orw
TRCVMIDCCTLR1 <uint64> Orw

TRCCLAIMCLR <uint64> Orw
TRCCLAIMSET <uint64> Orw
#endif

#if defined(MODULE_PLATFORM_TRBE)
TRBLIMITR_EL1 Orw
TRBPTR_EL1 <uint64> Orw
TRBBASER_EL1 <uint64> Orw
TRBSR_EL1 <uint64> Orw
TRBMAR_EL1 <uint64> Orw
TRBTRG_EL1 <uint64> Orw
#endif

#if defined(ARCH_ARM_FEAT_RNG)
RNDR <uint64> O
RNDRRS <uint64> O
#endif

#if defined(ARCH_ARM_FEAT_AMUv1) || defined(ARCH_ARM_FEAT_AMUv1p1)
AMCR_EL0 r
AMCFGR_EL0 r
AMCGCR_EL0 r
AMCNTENCLR0_EL0 <uint64> r
AMCNTENCLR1_EL0 <uint64> r
AMCNTENSET0_EL0 <uint64> r
AMCNTENSET1_EL0 <uint64> r
AMUSERENR_EL0 <uint64>
#if defined(ARCH_ARM_FEAT_AMUv1p1)
AMCG1IDR_EL0 <uint64> r
#endif
#endif

#if defined(MODULE_PLATFORM_ARM_DSU) && defined(INTERFACE_VCPU)
S3_0_C15_C3_1 [IMP_CLUSTERIDR_EL1!] Or
#endif

#if defined(ARCH_ARM_FEAT_CSV2_2) || defined(ARCH_ARM_FEAT_CSV2_1p2) || \
    defined(ARCH_ARM_FEAT_CSV2_3)
SCXTNUM_EL0 <uint64>
SCXTNUM_EL1 <uint64>
SCXTNUM_EL2 <uint64>
#endif

```

`hyp/arch/aarch64/src/asm_ordering.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <hyptypes.h>

#include <asm/barrier.h>

asm_ordering_dummy_t asm_ordering;

```

`hyp/arch/aarch64/src/nospec_checks.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <hyptypes.h>

#include <asm/nospec_checks.h>

index_result_t
nospec_range_check(index_t val, index_t limit)
{
	index_result_t result = index_result_error(ERROR_ARGUMENT_INVALID);
	bool	       valid;

	__asm__ volatile("cmp %w[val], %w[limit];"
			 "cset %w[valid], lo;"
			 "csel %w[result_r], %w[val], wzr, lo;"
			 "csdb;"
			 : [valid] "=&r"(valid), [result_r] "=r"(result.r)
			 : [val] "r"(val), [limit] "r"(limit)
			 : "cc");

	if (valid) {
		result.e = OK;
	}

	return result;
}

```

`hyp/arch/aarch64/src/timestamp.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <hyptypes.h>

#include <hypregisters.h>

#include <platform_timer.h>

#include <asm/barrier.h>
#include <asm/timestamp.h>

uint64_t
arch_get_timestamp(void)
{
	__asm__ volatile("isb" : "+m"(asm_ordering));
	return platform_timer_get_current_ticks();
}

```

`hyp/arch/aarch64/templates/hypregisters.h.tmpl`:

```tmpl
// Automatically generated. Do not modify.
//
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

\#pragma clang diagnostic push
\#pragma clang diagnostic ignored "-Wshorten-64-to-32"
\#pragma clang diagnostic ignored "-Wimplicit-int-conversion"

#def cast(t, v)
#if t.startswith('int') or t.startswith('uint')
(${t}_t)(${v})
#else
${t}_cast(${v})#slurp
#end if
#end def
#def raw(t, v)
#if t.startswith('int') or t.startswith('uint')
(register_t)(${v})
#else
${t}_raw(${v})#slurp
#end if
#end def

#def register_read(r, vt, vn, volatile, ordered)
#set $suffix = ""
#if volatile
#set $suffix = $suffix + "_volatile"
#end if
#if ordered
#set $suffix = $suffix + "_ordered"
#end if
static inline ${vt}_t
register_${vn}_read${suffix}(
#if ordered
asm_ordering_dummy_t *ordering_var
#else
void
#end if
)
{
	register_t val;
	__asm__
#if volatile
	volatile
#end if
		("mrs %0, $r.name	;"
		: "=r" (val)
#if ordered
		  , "+m"(*ordering_var)
#end if
	);
	return ${cast(vt, 'val')};
}

#end def

#def register_write(r, vt, vn, barrier, ordered)
#set $suffix = ""
#if barrier
#set $suffix = $suffix + "_barrier"
#end if
#if ordered
#set $suffix = $suffix + "_ordered"
#end if
static inline void
register_${vn}_write${suffix}(const ${vt}_t val
#if ordered
, asm_ordering_dummy_t *ordering_var
#end if
)
{
	register_t raw = (register_t)${raw(vt, 'val')};
	__asm__ volatile("msr $r.name, %[r]" :
#if ordered
		  "+m"(*ordering_var)
#end if
		: [r] "rz" (raw)
#if barrier
		: "memory"
#end if
	);
}
#end def

#for r in $registers
#for vn, vt in $r.variants
##
#if $r.is_readable
#if $r.need_non_ordered
${register_read(r, vt, vn, False, False)}
#end if
#if $r.need_ordered
${register_read(r, vt, vn, False, True)}
#end if
#end if
##
#if $r.is_volatile
#if $r.need_non_ordered
${register_read(r, vt, vn, True, False)}
#end if
#if $r.need_ordered
${register_read(r, vt, vn, True, True)}
#end if
#end if
##
#if $r.is_writable
#if $r.need_non_ordered
${register_write(r, vt, vn, False, False)}
#end if
#if $r.need_ordered
${register_write(r, vt, vn, False, True)}
#end if
#end if
##
#if $r.is_writeable_barrier
#if $r.need_non_ordered
${register_write(r, vt, vn, True, False)}
#end if
#if $r.need_ordered
${register_write(r, vt, vn, True, True)}
#end if
#end if
#end for
#end for

\#pragma clang diagnostic pop

```

`hyp/arch/aarch64/types.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

define asm_ordering_dummy structure {
	dummy char;
};

define AARCH64_INST_EXCEPTION_VAL constant uint32 = 0xd4000000;
define AARCH64_INST_EXCEPTION_MASK constant uint32 = 0xff000000;

define AARCH64_INST_EXCEPTION_IMM16_MASK constant uint32 = 0x001fffe0;
define AARCH64_INST_EXCEPTION_IMM16_SHIFT constant = 5;

define AARCH64_INST_EXCEPTION_SUBTYPE_HLT_VAL constant uint32 = 0x00400000;
define AARCH64_INST_EXCEPTION_SUBTYPE_MASK constant uint32 = 0x00e0001f;

#if defined(ARCH_ARM_FEAT_PAuth)
define aarch64_pauth_key structure(aligned(16)) {
	lo	uint64;
	hi	uint64;
};

define aarch64_pauth_keys structure(aligned(16)) {
	da	structure aarch64_pauth_key;
	db	structure aarch64_pauth_key;
	ia	structure aarch64_pauth_key;
	ib	structure aarch64_pauth_key;
	ga	structure aarch64_pauth_key;
};
#endif // defined(ARCH_ARM_FEAT_PAuth)

```

`hyp/arch/armv8/include/asm/atomic.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// Device memory fences
//
// The atomic_thread_fence() builtin only generates a fence for CPU threads,
// which means the compiler is allowed to use a DMB ISH instruction. For device
// accesses this is not good enough; we need a DMB SY.
//
// Note that the instructions here are the same for AArch64 and ARMv8 AArch32.
#define atomic_device_fence(p)                                                 \
	do {                                                                   \
		switch (p) {                                                   \
		case memory_order_relaxed:                                     \
			atomic_thread_fence(memory_order_relaxed);             \
			break;                                                 \
		case memory_order_acquire:                                     \
		case memory_order_consume:                                     \
			__asm__ volatile("dmb ld" ::: "memory");               \
			break;                                                 \
		case memory_order_release:                                     \
		case memory_order_acq_rel:                                     \
		case memory_order_seq_cst:                                     \
		default:                                                       \
			__asm__ volatile("dmb sy" ::: "memory");               \
			break;                                                 \
		}                                                              \
	} while (0)

```

`hyp/arch/armv8/include/asm/cache.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#define CACHE_BARRIER_OBJECT_LOAD(x, order)                                    \
	__asm__ volatile("" : "=r"(order) : "m"(*(x)))
#define CACHE_BARRIER_OBJECT_STORE(x, order)                                   \
	__asm__ volatile("" : "+r"(order), "+m"(*(x)))
#define CACHE_BARRIER_MEMORY_LOAD(x, order)                                    \
	__asm__ volatile("" : "=r"(order) : : "memory")
#define CACHE_BARRIER_MEMORY_STORE(x, order)                                   \
	__asm__ volatile("" : "+r"(order) : : "memory")

#define CACHE_DO_OP(x, size, op, CACHE_BARRIER)                                \
	do {                                                                   \
		const size_t line_size_ = 1U << CPU_L1D_LINE_BITS;             \
                                                                               \
		uintptr_t line_base_ =                                         \
			util_balign_down((uintptr_t)(x), line_size_);          \
		uintptr_t  end_ = (uintptr_t)(x) + (size);                     \
		register_t ordering;                                           \
                                                                               \
		assert(!util_add_overflows((uintptr_t)x, (size)-1U));          \
                                                                               \
		CACHE_BARRIER##_LOAD(x, ordering);                             \
                                                                               \
		do {                                                           \
			__asm__ volatile("DC " #op ", %1"                      \
					 : "+r"(ordering)                      \
					 : "r"(line_base_));                   \
			line_base_ = line_base_ + line_size_;                  \
		} while (line_base_ < end_);                                   \
                                                                               \
		__asm__ volatile("dsb ish" : "+r"(ordering));                  \
		CACHE_BARRIER##_STORE(x, ordering);                            \
	} while (0)

#define CACHE_OP_RANGE(x, size, op)                                            \
	CACHE_DO_OP(x, size, op, CACHE_BARRIER_MEMORY)
#define CACHE_OP_OBJECT(x, op)                                                 \
	CACHE_DO_OP(&(x), sizeof(x), op, CACHE_BARRIER_OBJECT)

#define CACHE_CLEAN_RANGE(x, size)	      CACHE_OP_RANGE(x, size, CVAC)
#define CACHE_INVALIDATE_RANGE(x, size)	      CACHE_OP_RANGE(x, size, IVAC)
#define CACHE_CLEAN_INVALIDATE_RANGE(x, size) CACHE_OP_RANGE(x, size, CIVAC)

#define CACHE_CLEAN_OBJECT(x)		 CACHE_OP_OBJECT(x, CVAC)
#define CACHE_INVALIDATE_OBJECT(x)	 CACHE_OP_OBJECT(x, IVAC)
#define CACHE_CLEAN_INVALIDATE_OBJECT(x) CACHE_OP_OBJECT(x, CIVAC)

#define CACHE_OP_FIXED_RANGE(x, size, op)                                      \
	do {                                                                   \
		struct {                                                       \
			char p[size];                                          \
		} *x_ = (void *)x;                                             \
		CACHE_OP_OBJECT(*x_, op);                                      \
	} while (0)

#define CACHE_DEFINE_ARRAY_OP(type, elements, name, op)                        \
	static inline void cache_##name(type(*x)[elements])                    \
	{                                                                      \
		CACHE_OP_OBJECT(*x, op);                                       \
	}

#define CACHE_DEFINE_CLEAN_ARRAY(type, elements, name)                         \
	CACHE_DEFINE_ARRAY_OP(type, elements, clean_##name, CVAC)
#define CACHE_DEFINE_INVALIDATE_ARRAY(type, elements, name)                    \
	CACHE_DEFINE_ARRAY_OP(type, elements, invalidate_##name, IVAC)
#define CACHE_DEFINE_CLEAN_INVALIDATE_ARRAY(type, elements, name)              \
	CACHE_DEFINE_ARRAY_OP(type, elements, clean_invalidate_##name, CIVAC)

#define CACHE_DEFINE_OP(type, name, op)                                        \
	static inline void cache_##name(type *x)                               \
	{                                                                      \
		CACHE_OP_OBJECT(*x, op);                                       \
	}

#define CACHE_DEFINE_CLEAN(type, name) CACHE_DEFINE_OP(type, clean_##name, CVAC)
#define CACHE_DEFINE_INVALIDATE(type, name)                                    \
	CACHE_DEFINE_OP(type, invalidate_##name, IVAC)
#define CACHE_DEFINE_CLEAN_INVALIDATE(type, name)                              \
	CACHE_DEFINE_OP(type, clean_invalidate_##name, CIVAC)

#define CACHE_CLEAN_FIXED_RANGE(x, size) CACHE_OP_FIXED_RANGE(x, size, CVAC)
#define CACHE_INVALIDATE_FIXED_RANGE(x, size)                                  \
	CACHE_OP_FIXED_RANGE(x, size, IVAC)
#define CACHE_CLEAN_INVALIDATE_FIXED_RANGE(x, size)                            \
	CACHE_OP_FIXED_RANGE(x, size, CIVAC)

void
cache_clean_range(const void *data, size_t size);

```

`hyp/arch/armv8/include/asm/timestamp.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// Returns a 64-bit monotonically incrementing global timestamp.
uint64_t
arch_get_timestamp(void);

```

`hyp/arch/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

arch_types aarch64 types.tc
arch_template registers aarch64 hypregisters.h
arch_registers aarch64 registers.reg

arch_source aarch64 timestamp.c asm_ordering.c nospec_checks.c
arch_source cortex-a-v8_2 sysreg_init.c
arch_events cortex-a-v8_2 sysreg_init.ev
arch_source cortex-a-v9 sysreg_init.c
arch_events cortex-a-v9 sysreg_init.ev

```

`hyp/arch/cortex-a-v8_0/include/asm/cpu.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// Miscellaneous definitions describing the CPU implementation.

// The size in address bits of a line in the innermost visible data cache.
#define CPU_L1D_LINE_BITS 6U

// The size in address bits of the CPU's DC ZVA block. This is nearly always
// the same as CPU_L1D_LINE_BITS.
#define CPU_DCZVA_BITS 6U

// The largest difference between the source and destination pointers during
// the optimised memcpy() for this CPU. This is here because it might depend
// on CPU_L1D_LINE_BITS in some implementations.
#define CPU_MEMCPY_STRIDE 256U

```

`hyp/arch/cortex-a-v8_0/include/asm/system_registers_cpu.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// AArch64 System Register Encoding (CPU Implementation Defined Registers)
//
// This list is not exhaustive, it contains mostly registers likely to be
// trapped and accessed indirectly.

#define ISS_MRS_MSR_CPUACTLR_EL1 ISS_OP0_OP1_CRN_CRM_OP2(3, 1, 15, 2, 0)
#define ISS_MRS_MSR_CPUECTLR_EL1 ISS_OP0_OP1_CRN_CRM_OP2(3, 1, 15, 2, 1)

```

`hyp/arch/cortex-a-v8_0/src/sysreg_init.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <hyptypes.h>

#include <hypregisters.h>

#include "event_handlers.h"

void
arch_cortex_a_v80_handle_boot_cpu_warm_init(void)
{
	// ACTLR_EL2 controls EL1 access to some important features such as
	// CLUSTERPMU and power control registers, which can be exploited to
	// perform security/DoS attacks. Therefore, we deny all these accesses
	// by writing 0 to this register.
	ACTLR_EL2_t val = ACTLR_EL2_default();
	register_ACTLR_EL2_write(val);
}

```

`hyp/arch/cortex-a-v8_0/sysreg_init.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

module arch

subscribe boot_cpu_warm_init
	handler arch_cortex_a_v80_handle_boot_cpu_warm_init

```

`hyp/arch/cortex-a-v8_2/include/asm/cpu.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// Miscellaneous definitions describing the CPU implementation.

// The size in address bits of a line in the innermost visible data cache.
#define CPU_L1D_LINE_BITS 6U

// The size in address bits of the CPU's DC ZVA block. This is nearly always
// the same as CPU_L1D_LINE_BITS.
#define CPU_DCZVA_BITS 6U

// The largest difference between the source and destination pointers during
// the optimised memcpy() for this CPU. This is here because it might depend
// on CPU_L1D_LINE_BITS in some implementations.
#define CPU_MEMCPY_STRIDE 256U

```

`hyp/arch/cortex-a-v8_2/include/asm/system_registers_cpu.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// AArch64 System Register Encoding (CPU Implementation Defined Registers)
//
// This list is not exhaustive, it contains mostly registers likely to be
// trapped and accessed indirectly.

#define ISS_MRS_MSR_CPUACTLR_EL1 ISS_OP0_OP1_CRN_CRM_OP2(3, 0, 15, 1, 0)
// CPUACTLR2_EL1 does not exist on A55
#define ISS_MRS_MSR_A7X_CPUACTLR2_EL1 ISS_OP0_OP1_CRN_CRM_OP2(3, 0, 15, 1, 1)
#define ISS_MRS_MSR_CPUECTLR_EL1      ISS_OP0_OP1_CRN_CRM_OP2(3, 0, 15, 1, 4)
#define ISS_MRS_MSR_CPUPWRCTLR_EL1    ISS_OP0_OP1_CRN_CRM_OP2(3, 0, 15, 2, 7)

```

`hyp/arch/cortex-a-v8_2/src/sysreg_init.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <hyptypes.h>

#include <hypregisters.h>

#include "event_handlers.h"

void
arch_cortex_a_v82_handle_boot_cpu_warm_init(void)
{
	// ACTLR_EL2 controls EL1 access to some important features such as
	// CLUSTERPMU and power control registers, which can be exploited to
	// perform security/DoS attacks. Therefore, we deny all these accesses
	// by writing 0 to this register.
	ACTLR_EL2_t val = ACTLR_EL2_default();
	register_ACTLR_EL2_write(val);
}

```

`hyp/arch/cortex-a-v8_2/sysreg_init.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

module arch

subscribe boot_cpu_warm_init
	handler arch_cortex_a_v82_handle_boot_cpu_warm_init

```

`hyp/arch/cortex-a-v9/include/asm/cpu.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// Miscellaneous definitions describing the CPU implementation.

// The size in address bits of a line in the innermost visible data cache.
#define CPU_L1D_LINE_BITS 6U

// The size in address bits of the CPU's DC ZVA block. This is nearly always
// the same as CPU_L1D_LINE_BITS.
#define CPU_DCZVA_BITS 6U

// The largest difference between the source and destination pointers during
// the optimised memcpy() for this CPU. This is here because it might depend
// on CPU_L1D_LINE_BITS in some implementations.
#define CPU_MEMCPY_STRIDE 256U

```

`hyp/arch/cortex-a-v9/include/asm/system_registers_cpu.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// AArch64 System Register Encoding (CPU Implementation Defined Registers)
//
// This list is not exhaustive, it contains mostly registers likely to be
// trapped and accessed indirectly.

#define ISS_MRS_MSR_CPUACTLR_EL1      ISS_OP0_OP1_CRN_CRM_OP2(3, 0, 15, 1, 0)
#define ISS_MRS_MSR_A7X_CPUACTLR2_EL1 ISS_OP0_OP1_CRN_CRM_OP2(3, 0, 15, 1, 1)
#define ISS_MRS_MSR_CPUECTLR_EL1      ISS_OP0_OP1_CRN_CRM_OP2(3, 0, 15, 1, 4)
#define ISS_MRS_MSR_CPUPWRCTLR_EL1    ISS_OP0_OP1_CRN_CRM_OP2(3, 0, 15, 2, 7)

```

`hyp/arch/cortex-a-v9/src/sysreg_init.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <hyptypes.h>

#include <hypregisters.h>

#include "event_handlers.h"

void
arch_cortex_a_v9_handle_boot_cpu_warm_init(void)
{
	// ACTLR_EL2 controls EL1 access to some important features such as
	// CLUSTERPMU and power control registers, which can be exploited to
	// perform security/DoS attacks. Therefore, we deny all these accesses
	// by writing 0 to this register.
	ACTLR_EL2_t val = ACTLR_EL2_default();
	register_ACTLR_EL2_write(val);
}

```

`hyp/arch/cortex-a-v9/sysreg_init.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

module arch

subscribe boot_cpu_warm_init
	handler arch_cortex_a_v9_handle_boot_cpu_warm_init

```

`hyp/arch/cortex-a53/include/asm/prefetch.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// Non-temporal accesses perform poorly on Cortex-A53 because they suppress
// allocation in the L1 cache. In most cases it is better to do a regular
// keep-prefetch instead. Note that there is a bit in CPUACTLR that has the
// same effect, and it is set by default at reset on r0p4 and later.
#define prefetch_load_stream(addr)  prefetch_load_keep(addr)
#define prefetch_store_stream(addr) prefetch_store_keep(addr)

#include <asm-generic/prefetch.h>

```

`hyp/arch/generic/include/asm-generic/asm_defs.inc`:

```inc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// clang-format off
.macro function func:req local="global" section="section" align="align"
	.ifeqs "\local", "local"
	.else
	.ifeqs "\local", "global"
		.global \func
	.else
		.err "invalid argument to function macro"
	.endif
	.endif
	.ifeqs "\section", "section"
		.section .text.\func, "ax", @progbits
	.else
	.ifeqs "\section", "nosection"
	.else
		.err "invalid argument to function macro"
	.endif
	.endif
	.ifeqs "\align", "align"
		.balign 8
	.else
	.ifeqs "\align", "noalign"
	.else
		.balign \align
	.endif
	.endif

	.type	\func, STT_FUNC
\func:
.endm

.macro function_end func:req
	.size \func, . - \func
.endm

.macro function_chain prev_func:req next_func:req
	function_end \prev_func
	function \next_func, section=nosection align=noalign
.endm

#define LOCAL(name) .L##name

.macro	local, label
.L\label
.endm

.macro _data_sym sym:req size:req local="global" align="noalign"
	.ifeqs "\local", "local"
	.else
	.ifeqs "\local", "global"
		.global \sym
	.else
		.err "invalid argument to _data_sym macro"
	.endif
	.endif
	.ifeqs "\align", "noalign"
	.else
		.balign \align
	.endif
	.type	\sym, STT_OBJECT
	.size	\sym, \size
.endm

.macro _const sym:req size:req align:req local
	.section .rodata.\sym, "a", @progbits
	_data_sym \sym, size=\size, align=\align, local=\local
\sym:
.endm

.macro _data sym:req size:req align:req local
	.section .data.\sym, "aw", @progbits
	_data_sym \sym, size=\size, align=\align, local=\local
\sym:
.endm

.macro _bss sym:req size:req align:req local
	.section .bss.\sym, "aw", @nobits
	_data_sym \sym, size=\size, align=\align, local=\local
\sym:
.endm

.macro const32 sym, value=0, local="global"
	_const \sym, 4, 4, \local
	.4byte \value
	.previous
.endm

.macro const64 sym, value=0, local="global"
	_const \sym, 8, 8, \local
	.8byte \value
	.previous
.endm

.macro data32 sym, value=0, local="global"
	_data \sym, 4, 4, \local
	.4byte \value
	.previous
.endm

.macro data64 sym, value=0, local="global"
	_data \sym, 8, 8, \local
	.8byte \value
	.previous
.endm

.macro bss32 sym, local="global"
	_bss \sym, 4, 4, \local
	.4byte 0
	.previous
.endm

.macro bss64 sym, local="global"
	_bss \sym, 8, 8, \local
	.8byte 0
	.previous
.endm

	  // clang-format on

```

`hyp/arch/generic/include/asm-generic/atomic.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// Nothing to define; the defaults are in <atomic.h>

```

`hyp/arch/generic/include/asm-generic/event.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// Wait for or raise events.
//
// These macros may apply architecture-specific optimisations to improve the
// efficiency of inter-CPU signalling by polling shared variables.
//
// These are the default definitions, which provide adequate memory barriers
// but otherwise just busy-wait. This header should only be included by the
// architecture-specific asm/event.h, which can optionally define the
// asm_event_wait() macro to an operation that may sleep, and also define the
// other operations if necessary.

// Load a polled variable before a possible asm_event_wait().
//
// This load must be an acquire operation on the specified variable.
//
// To be safe on platforms that sleep in asm_event_wait(), the return value of
// this macro _must_ be used in an expression that determines whether to
// call asm_event_wait().
#if !defined(asm_event_load_before_wait)
#define asm_event_load_before_wait(p)                                          \
	atomic_load_explicit(p, memory_order_acquire)
#endif

// As above, but for a named bitfield type.
//
// This is needed to hide pointer casts that would otherwise be unsafe on
// platforms where asm_event_load_before_wait() needs type-specific inline asm,
// such as ARMv8.
#if !defined(asm_event_load_bf_before_wait)
#define asm_event_load_bf_before_wait(name, p) asm_event_load_before_wait(p)
#endif

// Poll after checking the result of asm_event_load_before_wait().
//
// Polling may place the calling CPU in a low-power halt state until the
// value read by asm_event_load_before_wait() is updated by either a remote CPU,
// or a local interrupt handler that interrupts the poll. The polled variable
// must be updated by either calling asm_event_store_and_wake(), or with some
// other store operation followed by a call to asm_event_wake_updated().
//
// Updates performed by remote CPUs in any other way, or performed by the local
// CPU other than in an interrupt handler that preempts the poll, are not
// guaranteed to wake a sleeping poll.
//
// Polling must be safe from races; that is, asm_event_wait() must return if
// an update inter-thread happens after asm_event_load_before_wait(), regardless
// of whether the update inter-thread happens after asm_event_wait() is called.
//
// Polling is not required to sleep until the polled value is updated;
// it may wake early or not sleep at all. If the CPU does not support this
// operation and will never sleep, ASM_EVENT_WAIT_IS_NOOP is defined to be
// nonzero.
#if !defined(asm_event_wait)
#define ASM_EVENT_WAIT_IS_NOOP 1
#define asm_event_wait(p)      ((void)0)
#else
#define ASM_EVENT_WAIT_IS_NOOP 0
#endif

// Store an event variable and wake CPUs waiting on it.
//
// This store must be a release operation on the specified variable.
#if !defined(asm_event_store_and_wake)
#define asm_event_store_and_wake(p, v)                                         \
	atomic_store_explicit((p), (v), memory_order_release)
#endif

// Wake CPUs waiting on one or more variables that have been updated with direct
// atomic_*() calls rather than by calling asm_event_store_and_wake().
//
// Any direct updates must either be release operations, or else followed by
// a release fence, prior to executing this operation.
//
// This may be more expensive than asm_event_store_and_wake(), especially for a
// single store.
#if !defined(asm_event_wake_updated)
#define asm_event_wake_updated() (void)0
#endif

```

`hyp/arch/generic/include/asm-generic/prefetch.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// Prefetch operations.
//
// Calling these macros may cause the compiler to generate hint instructions
// or otherwise reorder operations so that values are fetched by the CPU
// eagerly when it is known that they will be needed in the near future.
//
// Prefetch instructions, where available, can typically distinguish between
// addresses that will be loaded or stored; this distinction is useful on
// cache-coherent multiprocessor systems, where a store prefetch will try to
// bring the target cache line into an exclusive state.
//
// On some architectures, including ARMv8, the prefetch instructions can
// further distinguish between temporal and non-temporal accesses. A temporal,
// or "keep", prefetch means that the address will be accessed repeatedly and
// should be kept in the cache (i.e. the default behaviour of most caches). A
// non-temporal, or "stream", prefetch means that the address will be accessed
// only once, so cache allocations for it should be kept to a minimum - e.g.
// bypassing outer caches on eviction from the innermost cache.
//
// This is in an asm header to allow the macros to be replaced with asm
// directives or no-ops for targets where the compiler makes a suboptimal
// decision by default, typically because the target CPU has a broken
// implementation of the prefetch instructions.

#ifndef prefetch_load_keep
#define prefetch_load_keep(addr) __builtin_prefetch(addr, 0, 3)
#endif

#ifndef prefetch_store_keep
#define prefetch_store_keep(addr) __builtin_prefetch(addr, 1, 3)
#endif

#ifndef prefetch_load_stream
#define prefetch_load_stream(addr) __builtin_prefetch(addr, 0, 0)
#endif

#ifndef prefetch_store_stream
#define prefetch_store_stream(addr) __builtin_prefetch(addr, 1, 0)
#endif

```

`hyp/arch/generic/include/asm/prefetch.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <asm-generic/prefetch.h>

```

`hyp/arch/qemu-armv8-5a-rng/include/asm/cpu.h`:

```h
// © 2022 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// Miscellaneous definitions describing the CPU implementation.

// The size in address bits of a line in the innermost visible data cache.
#define CPU_L1D_LINE_BITS 6U

// The size in address bits of the CPU's DC ZVA block. This is nearly always
// the same as CPU_L1D_LINE_BITS.
#define CPU_DCZVA_BITS 6U

// The largest difference between the source and destination pointers during
// the optimised memcpy() for this CPU. This is here because it might depend
// on CPU_L1D_LINE_BITS in some implementations.
#define CPU_MEMCPY_STRIDE 256U

```

`hyp/core/api/aarch64/hypercalls.hvc`:

```hvc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

define hypervisor_identify hypercall {
	call_num 0x0;
	hyp_api_info	output bitfield hyp_api_info;
	api_flags_0	output bitfield hyp_api_flags0;
	api_flags_1	output bitfield hyp_api_flags1;
	api_flags_2	output bitfield hyp_api_flags2;
};

```

`hyp/core/api/aarch64/templates/c_wrapper.c.tmpl`:

```tmpl
// Automatically generated. Do not modify.
//
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause
#extends hypercall_api
#implements respond
#def prefix: $internal_prefix

\#include <assert.h>
\#include <hyptypes.h>

\#include <hypcall_def.h>
\#include <compiler.h>
\#include <thread.h>
\#include <trace.h>

\#include <events/thread.h>

#def trace_in(hypcall_num, hypcall)
    #set trace_fmt = $hypcall.name + ":"
    #for i, input in $hypcall.inputs[:5]
        #if not $input.ignore
            #set trace_fmt = trace_fmt + " {:#x}"
        #end if
    #end for
    TRACE(USER, HYPERCALL, "${trace_fmt}"
    #for i, input in $hypcall.inputs[:5]
        #if not $input.ignore
        , (register_t)($register_expr($input))
        #end if
    #end for
    );
#end def

#def trace_out(hypcall_num, hypcall)
    #set trace_fmt = $hypcall.name + " ret:"
    #for i, input in $hypcall.outputs[:5]
        #if not $input.ignore
            #set trace_fmt = trace_fmt + " {:#x}"
        #end if
    #end for
    TRACE(USER, HYPERCALL, "${trace_fmt}"
    #if len($hypcall.outputs) == 1
        , (register_t)ret_
    #else
        #for i, output in $hypcall.outputs[:5]
        , (register_t)ret_.$register_expr($output)
        #end for
    #end if
    );
#end def

#set $wrapper_suffix = "__c_wrapper"
#for hypcall_num in sorted($hypcall_dict.keys())
    #set $hypcall = $hypcall_dict[$hypcall_num]
    #if len($hypcall.outputs) > 1
    static_assert(sizeof(${return_type($hypcall)}) <= 8U * sizeof(register_t),
        "Return structure must fit in 8 machine registers");

    #end if
    #set $num_in = len($hypcall.inputs)
    #set $num_out = len($hypcall.outputs)
    #set $sensitive = $hypcall.properties.get('sensitive', False)

    #assert $num_in <= 8
    #assert $num_out <= 8

${type_signature($hypcall, suffix=$wrapper_suffix, ignored_inputs=True)}
	REQUIRE_PREEMPT_DISABLED;

${type_signature($hypcall, suffix=$wrapper_suffix, ignored_inputs=True)} {
    #if $hypcall.outputs
        $return_type($hypcall) ret_;
    #end if

    trigger_thread_entry_from_user_event(THREAD_ENTRY_REASON_HYPERCALL);

#if $sensitive
    TRACE(USER, HYPERCALL, "$hypcall.name");
#else
    $trace_in($hypcall_num, $hypcall)
#end if

    ## generate reserved input checks
    #set has_ignores = False
    #set error_ret = None
    #for r, output in $hypcall.outputs
        #if $output.ctype == 'error_t'
            #set error_ret = $output
            #break
        #end if
    #end for
    #if error_ret
        #for r, input in $hypcall.inputs
            #if $input.ignore
                #set has_ignores = True
    if (compiler_unexpected(${input.name} != (${input.ctype})${input.default}U)) {
                #if len($hypcall.outputs) == 1
        ret_ = ERROR_ARGUMENT_INVALID;
                #else
        ret_ = ($return_type($hypcall)){ .${error_ret.name} = ERROR_ARGUMENT_INVALID };
                #end if
        goto out;
    }
            #end if
        #end for
    #else
        // FIXME: unchecked reserved inputs
        #for r, input in $hypcall.inputs
            #if $input.ignore
    (void)$input.name;
            #end if
        #end for
    #end if

    ## call the implementation
    #if $hypcall.outputs
    ret_ =
    #end if
    $prefix${hypcall.name}(#slurp
    #set sep=''
    #for i, input in $hypcall.inputs
        #if not $input.ignore
            $sep$input.name
            #set sep=', '
        #end if
    #end for
    );

    #if has_ignores:
out:
    #end if

#if not $sensitive
    $trace_out($hypcall_num, $hypcall)
#end if

    trigger_thread_exit_to_user_event(THREAD_ENTRY_REASON_HYPERCALL);

    ## return the result, if any
    #if $hypcall.outputs
    return ret_;
    #else
        #if has_ignores:
    return;
        #end if
    #end if
}

#end for

```

`hyp/core/api/aarch64/templates/hypcall_table.S.tmpl`:

```tmpl
// Automatically generated. Do not modify.
//
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause
#extends hypercall_api
#implements respond
#def prefix: $internal_prefix

\#include <hypconstants.h>
\#include <asm/asm_defs.inc>

\#if defined(ARCH_ARM_FEAT_BTI)
#define BTI_J	bti j
#define BTI_NOP	nop
#define HYPERCALL_ALIGN	0x10
\#else
#define BTI_J
#define BTI_NOP
#define HYPERCALL_ALIGN	0x8
\#endif

\#if (ARCH_SANITIZE_STACK_SIZE % 16) != 0
\#error ARCH_SANITIZE_STACK_SIZE alignment
\#endif

#if len($hypcall_dict)
	.balign 16
	.global hypercall_table
	.type hypercall_table, #function
hypercall_table:
#for hypcall_num in range(0, max($hypcall_dict.keys()) + 1)
$hypcall_num:
    #if $hypcall_num not in $hypcall_dict
	BTI_J
	mov	x0, ENUM_ERROR_UNIMPLEMENTED
	b	vcpu_${prefix}return_sanitize_x1
	BTI_NOP	// Align hypercall
    #else
	#set $hypcall = $hypcall_dict[$hypcall_num]
	#set $num_in = len($hypcall.inputs)
	#set $num_out = len($hypcall.outputs)
	#set $sensitive = $hypcall.properties.get('sensitive', False)
	BTI_J
	adr	x9, ${prefix}${hypcall.name}__c_wrapper
	#if $num_out <= 2
	## return fits in registers
#if $sensitive
#error unimplemented
#end if
	b	vcpu_${prefix}entry_x${num_in}_x${num_out}
	#else
	## return is larger than two registers and must be indirect
#if $sensitive
	b	vcpu_${prefix}entry_x${num_in}_sensitive
#else
	b	vcpu_${prefix}entry_x${num_in}_slow
#end if
	#end if
	BTI_NOP	// Align hypercall
    #end if
.if . - ${hypcall_num}b != HYPERCALL_ALIGN
.err // bad alignment
.endif
#end for
	.size hypercall_table, . - hypercall_table

#for out in ["x0", "x1", "x2", "slow", "sensitive"]
	.balign 64
function vcpu_${prefix}entry_x0_${out}
#for input in range(0, 8)
#set $next_in = $input + 1
	## sanitise unused HVC argument registers
	mov	x${input}, xzr
function_chain vcpu_${prefix}entry_x${input}_${out}, vcpu_${prefix}entry_x${next_in}_${out}
#end for
#if $out in ["slow", "sensitive"]
	// Allocate stack space for the maximum allowed return size
	sub	sp, sp, 64
	mov	x8, sp

	// Zero-initialise the allocated stack space, since any unused returns
	// in the struct may be left uninitialised by the compiler and returned
	// to the caller.
	stp	xzr, xzr, [sp, #0]
	stp	xzr, xzr, [sp, #16]
	stp	xzr, xzr, [sp, #32]
	stp	xzr, xzr, [sp, #48]

	// Jump to the C handler
	blr	x9

	// Load x0..x7 from the return structure on the stack
	ldp	x0, x1, [sp, #0]
	ldp	x2, x3, [sp, #16]
	ldp	x4, x5, [sp, #32]
	ldp	x6, x7, [sp, #48]
	add	sp, sp, 64
#if $out == "sensitive"

	// Zero any sensitive stack values
	sub	x9, sp, ARCH_SANITIZE_STACK_SIZE
	mov	x10, sp
local zero_stack:
	stp	xzr, xzr, [x9], 16
	cmp	x9, x10
	bne	LOCAL(zero_stack)
#end if

	b	vcpu_${prefix}return_sanitize_x8
#else
	blr	x9
	b	vcpu_${prefix}return_sanitize_${out}
#end if
function_end vcpu_${prefix}entry_x8_${out}

#end for
#end if

```

`hyp/core/api/api.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

extend trace_class enumeration {
	USER = 2;
};

extend trace_id enumeration {
	HYPERCALL = 2;
};

```

`hyp/core/api/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

interface api
types api.tc
arch_hypercalls aarch64 hypercalls.hvc
source api.c
configs HYPERCALLS=1
template hypercalls hypcall_def.h
arch_template hypercalls aarch64 hypcall_table.S c_wrapper.c

```

`hyp/core/api/src/api.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <hyptypes.h>

#include <hypcall_def.h>

hypercall_hypervisor_identify_result_t
hypercall_hypervisor_identify(void)
{
	return (hypercall_hypervisor_identify_result_t){
		.hyp_api_info = hyp_api_info_default(),
		.api_flags_0  = hyp_api_flags0_default(),
		.api_flags_1  = hyp_api_flags1_default(),
		.api_flags_2  = hyp_api_flags2_default(),
	};
}

```

`hyp/core/api/templates/hypcall_def.h.tmpl`:

```tmpl
// Automatically generated. Do not modify.
//
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause
#extends hypercall_api
#implements respond

#def prefix: $internal_prefix

\#if !defined(__ASSEMBLER__)
#for hypcall_num in sorted($hypcall_dict.keys())
    #set $hypcall = $hypcall_dict[$hypcall_num]

    ${result_struct_definition($hypcall)}
    ${type_signature($hypcall)};
#end for

#for hypcall in sorted($vendor_hypcall_list, key=lambda x: x.name)

    ${result_struct_definition($hypcall)}
    ${type_signature($hypcall)};
#end for

\#else
#if len($hypcall_dict)
\#define HYPERCALL_BASE ${hex($hypcall_dict[0].abi.hypcall_base)}U
\#define HYPERCALL_NUM ${$max($hypcall_dict.keys()) + 1}U
#end if
\#endif

```

`hyp/core/base/aarch64/enums.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier; BSD-3-Clause

extend esr_ec enumeration {
	SVC64			= 0x15;
	HVC64_EL2		= 0x16;
	SMC64_EL2		= 0x17;
	SYSREG			= 0x18;
	SP_ALIGN		= 0x26;
	FP32			= 0x28;
	FP64			= 0x2c;
	SERROR			= 0x2f;
	BREAK_LO		= 0x30;
	BREAK			= 0x31;
	STEP_LO			= 0x32;
	STEP			= 0x33;
	WATCH_LO		= 0x34;
	WATCH			= 0x35;
	BKPT			= 0x38;
	VECTOR32_EL2		= 0x3a;
	BRK			= 0x3c;
};

#if defined(ARCH_ARM_FEAT_PAuth)
extend esr_ec enumeration {
	PAUTH			= 0x09;
#if defined(ARCH_ARM_FEAT_NV)
	ERET			= 0x1a;
#endif
#if defined(ARCH_ARM_FEAT_FPAC)
	FPAC			= 0x1c;
#endif
};
#endif

#if defined(ARCH_ARM_FEAT_BTI)
extend esr_ec enumeration {
	BTI			= 0x0d;
};
#endif

#if defined(ARCH_ARM_FEAT_SVE)
extend esr_ec enumeration {
	SVE			= 0x19;
};
#endif

#if defined(ARCH_ARM_FEAT_LS64)
extend esr_ec enumeration {
	LD64B_ST64B		= 0x0a;
};
#endif

#if defined(ARCH_ARM_FEAT_TME)
extend esr_ec enumeration {
	TSTART			= 0x1b;
};
#endif

#if defined(ARCH_ARM_FEAT_SME)
extend esr_ec enumeration {
	SME			= 0x1d;
};
#endif

#if defined(ARCH_ARM_FEAT_RME)
extend esr_ec enumeration {
	RME			= 0x1e;
};
#endif

#if defined(ARCH_ARM_FEAT_MOPS)
extend esr_ec enumeration {
	MOPS			= 0x27;
};
#endif

extend iss_da_ia_fsc enumeration {
	TRANSLATION_0		= 0x04;
	SYNC_TAG_CHECK		= 0x11;
	SYNC_EXTERN_WALK_0	= 0x14;
	SYNC_PARITY_ECC_WALK_0	= 0x1c;
	ALIGNMENT		= 0x21;
	IMP_DEF_LOCKDOWN	= 0x34;
	IMP_DEF_ATOMIC		= 0x35;
	SECTION_DOMAIN		= 0x3d;
	PAGE_DOMAIN		= 0x3e;
};

#if defined(ARCH_ARM_FEAT_HAFDBS)
extend iss_da_ia_fsc enumeration {
	ATOMIC_HW_UPDATE	= 0x31;
};
#endif

define iss_da_ia_set enumeration {
	UER		= 0b00;
	UC		= 0b10;
	UEO_CE		= 0b11;
};

define iss_serror_aet enumeration {
	UC		= 0b000;
	UEU		= 0b001;
	UEO		= 0b010;
	UER		= 0b011;
	CE		= 0b110;
};

define iss_serror_dfsc enumeration {
	Uncategorised	= 0b000000;
	Async_Int	= 0b010001;
};

define spsr_64bit_mode enumeration {
	el0t		= 0x00;
	el1t		= 0x04;
	el1h		= 0x05;
	el2t		= 0x08;
	el2h		= 0x09;
};

define currentel_el enumeration {
	el0		= 0x00;
	el1;
	el2;
	el3;
};

define tcr_tg0 enumeration {
	GRANULE_SIZE_4KB	= 0b00;
	GRANULE_SIZE_16KB	= 0b10;
	GRANULE_SIZE_64KB	= 0b01;
};

define tcr_tg1 enumeration {
	GRANULE_SIZE_4KB	= 0b10;
	GRANULE_SIZE_16KB	= 0b01;
	GRANULE_SIZE_64KB	= 0b11;
};

define tcr_rgn enumeration {
	NORMAL_NONCACHEABLE	= 0b00;
	NORMAL_WRITEBACK_RA_WA	= 0b01;
	NORMAL_WRITETHROUGH_RA	= 0b10;
	NORMAL_WRITEBACK_RA	= 0b11;
};

define tcr_sh enumeration {
	NON_SHAREABLE		= 0b00;
	OUTER_SHAREABLE		= 0b10;
	INNER_SHAREABLE		= 0b11;
};

define tcr_ps enumeration {
	SIZE_32BITS = 0b000;
	SIZE_36BITS = 0b001;
	SIZE_40BITS = 0b010;
	SIZE_42BITS = 0b011;
	SIZE_44BITS = 0b100;
	SIZE_48BITS = 0b101;
	SIZE_52BITS = 0b110;
};

#ifdef ARCH_ARM_FEAT_TLBIRANGE
define tlbi_range_tg enumeration {
	GRANULE_SIZE_4KB	= 0b01;
	GRANULE_SIZE_16KB	= 0b10;
	GRANULE_SIZE_64KB	= 0b11;
};
#endif

define cptr_zen enumeration {
	trap_tge		= 0x01;
	trap_all		= 0x02;
	trap_none		= 0x03;
};

#if defined(ARCH_ARM_FEAT_WFxT)
extend iss_wfx_ti enumeration {
	WFIT		= 0b10;
	WFET		= 0b11;
};
#endif

```

`hyp/core/base/aarch64/src/cache.c`:

```c
// © 2023 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>

#include <util.h>

#include <asm/cache.h>
#include <asm/cpu.h>

void
cache_clean_range(const void *data, size_t size)
{
	CACHE_CLEAN_RANGE(data, size);
}

```

`hyp/core/base/aarch64/src/core_id.c`:

```c
// © 2022 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <hyptypes.h>

#include <hypconstants.h>
#include <hypregisters.h>

#include <base.h>
#include <compiler.h>
#include <cpulocal.h>
#include <log.h>
#include <platform_cpu.h>
#include <preempt.h>
#include <trace.h>
#include <util.h>

// FIXME:
#if !defined(MODULE_PLATFORM_SOC_QCOM)
// Platforms may override this with their own implementation
core_id_t WEAK
platform_cpu_get_coreid(MIDR_EL1_t midr)
{
	(void)midr;
	return CORE_ID_UNKNOWN;
}
#endif

static core_id_t
get_core_id(uint16_t partnum, uint8_t variant, uint8_t revision)
{
	static const core_id_info_t core_id_map[] = {
		{ .part_num = 0xD03U, .core_id = CORE_ID_CORTEX_A53 },
		{ .part_num = 0xD05U, .core_id = CORE_ID_CORTEX_A55 },
		{ .part_num = 0xD07U, .core_id = CORE_ID_CORTEX_A57 },
		{ .part_num = 0xD08U, .core_id = CORE_ID_CORTEX_A72 },
		{ .part_num = 0xD09U, .core_id = CORE_ID_CORTEX_A73 },
		{ .part_num = 0xD0AU, .core_id = CORE_ID_CORTEX_A75 },
		{ .part_num = 0xD0BU, .core_id = CORE_ID_CORTEX_A76 },
		{ .part_num = 0xD0CU, .core_id = CORE_ID_NEOVERSE_N1 },
		{ .part_num = 0xD0DU, .core_id = CORE_ID_CORTEX_A77 },
		{ .part_num = 0xD0EU, .core_id = CORE_ID_CORTEX_A76AE },
		{ .part_num = 0xD40U, .core_id = CORE_ID_NEOVERSE_V1 },
		{ .part_num = 0xD41U, .core_id = CORE_ID_CORTEX_A78 },
		{ .part_num = 0xD42U, .core_id = CORE_ID_CORTEX_A78AE },
		{ .part_num = 0xD44U, .core_id = CORE_ID_CORTEX_X1 },
		{ .part_num = 0xD46U, .core_id = CORE_ID_CORTEX_A510 },
		{ .part_num = 0xD47U, .core_id = CORE_ID_CORTEX_A710 },
		{ .part_num = 0xD48U, .core_id = CORE_ID_CORTEX_X2 },
		{ .part_num = 0xD49U, .core_id = CORE_ID_NEOVERSE_N2 },
		{ .part_num = 0xD4BU, .core_id = CORE_ID_CORTEX_A78C },
		{ .part_num = 0xD4DU, .core_id = CORE_ID_CORTEX_A715 },
		{ .part_num = 0xD4EU, .core_id = CORE_ID_CORTEX_X3 },
		{ .part_num = 0xD80U, .core_id = CORE_ID_CORTEX_A520 },
	};
	// List of cores that have specific revisions.
	// If multiple revisions are assigned different core IDs, then keep
	// them sorted by highest (variant_min,revision_min) first.
	static const core_id_rev_info_t core_id_rev_map[] = {
		{ .part_num	= 0xD81U,
		  .core_id	= CORE_ID_CORTEX_A720,
		  .variant_min	= 0,
		  .revision_min = 1 },
		{ .part_num	= 0xD82U,
		  .core_id	= CORE_ID_CORTEX_X4,
		  .variant_min	= 0,
		  .revision_min = 1 }
	};

	core_id_t coreid;
	index_t	  i;

	for (i = 0U; i < util_array_size(core_id_map); i++) {
		if (partnum == core_id_map[i].part_num) {
			coreid = core_id_map[i].core_id;
			goto out;
		}
	}

	for (i = 0U; i < util_array_size(core_id_rev_map); i++) {
		if ((partnum == core_id_rev_map[i].part_num) ||
		    (variant > core_id_rev_map[i].variant_min) ||
		    ((variant == core_id_rev_map[i].variant_min) &&
		     (revision >= core_id_rev_map[i].revision_min))) {
			coreid = core_id_rev_map[i].core_id;
			goto out;
		}
	}

	coreid = CORE_ID_UNKNOWN;
out:
	return coreid;
}

core_id_t
get_current_core_id(void) REQUIRE_PREEMPT_DISABLED
{
	core_id_t coreid;

	assert_cpulocal_safe();

	MIDR_EL1_t midr = register_MIDR_EL1_read();

	uint8_t	 implementer = MIDR_EL1_get_Implementer(&midr);
	uint16_t partnum     = MIDR_EL1_get_PartNum(&midr);
	uint8_t	 variant     = MIDR_EL1_get_Variant(&midr);
	uint8_t	 revision    = MIDR_EL1_get_Revision(&midr);

	if ((char)implementer == 'A') {
		coreid = get_core_id(partnum, variant, revision);
	} else {
		coreid = CORE_ID_UNKNOWN;
	}

	if (coreid == CORE_ID_UNKNOWN) {
		coreid = platform_cpu_get_coreid(midr);
	}

#if defined(VERBOSE) && VERBOSE
	if (coreid == CORE_ID_UNKNOWN) {
		cpu_index_t cpu = cpulocal_get_index();

		LOG(DEBUG, WARN,
		    "detected unknown core ID, cpu: {:d}, MIDR: {:#08x}", cpu,
		    MIDR_EL1_raw(midr));
	}
#endif

	return coreid;
}

```

`hyp/core/base/aarch64/sysregs.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// AArch64 registers and bitfields definitions
// FIXME: split AArch32 common registers to an armv8 architecture file

define ACTLR_EL1 bitfield<64> {
	63:0		unknown=0;
};

define ACTLR_EL2 bitfield<64> {
	63:0		unknown=0;
};

define AFSR0_EL1 bitfield<64> {
	63:0		unknown=0;
};

define AFSR0_EL2 bitfield<64> {
	63:0		unknown=0;
};

define AFSR1_EL1 bitfield<64> {
	63:0		unknown=0;
};

define AFSR1_EL2 bitfield<64> {
	63:0		unknown=0;
};

define AIDR_EL1 bitfield<64> {
	63:0		unknown=0;
};

define AMAIR_EL1 bitfield<64> {
	63:0		unknown=0;
};

define AMAIR_EL2 bitfield<64> {
	63:0		unknown=0;
};

define CCSIDR_EL1 bitfield<64> {
	2:0		LineSize	uint8;
	12:3		Associativity	uint32;
	27:13		NumSets		uint32;
	31:28		unknown;
	63:32		unknown=0;
};

#if defined(ARCH_ARM_FEAT_CCIDX)
extend CCSIDR_EL1 bitfield {
	delete		Associativity;
	23:3		Associativity	uint32;
	31:24		unknown=0;
	delete		NumSets;
	55:32		NumSets		uint32;
	63:56		unknown=0;
};
#endif

define CCSIDR2_EL1 bitfield<64> {
	23:0		NumSets		uint32;
	63:24		unknown=0;
};

define CLIDR_EL1 bitfield<64> {
	2:0		Ctype1		uint8;
	5:3		Ctype2		uint8;
	8:6		Ctype3		uint8;
	11:9		Ctype4		uint8;
	14:12		Ctype5		uint8;
	17:15		Ctype6		uint8;
	20:18		Ctype7		uint8;
	23:21		LoUIS		uint8;
	26:24		LoC		uint8;
	29:27		LoUU		uint8;
	32:30		ICB		uint8;
	63:33		unknown=0;
};

#if defined(ARCH_ARM_FEAT_MTE)
extend CLIDR_EL1 bitfield {
	34:33		Ttype1		uint8;
	36:35		Ttype2		uint8;
	38:37		Ttype3		uint8;
	40:39		Ttype4		uint8;
	42:41		Ttype5		uint8;
	44:43		Ttype6		uint8;
	46:45		Ttype7		uint8;
};
#endif

define CNTFRQ_EL0 bitfield<64> {
	31:0		ClockFrequency	uint32;
	63:32		unknown=0;
};

define CNTHCTL_EL2_E2H0 bitfield<64> {
	0		EL1PCTEN	bool;
	1		EL1PCEN		bool;
	2		EVNTEN		bool;
	3		EVNTDIR		bool;
	7:4		EVNTI		uint8;
	63:8		unknown=0;
};

#if defined(ARCH_ARM_FEAT_VHE)
define CNTHCTL_EL2_E2H1 bitfield<64> {
	0		EL0PCTEN	bool;
	1		EL0VCTEN	bool;
	2		EVNTEN		bool;
	3		EVNTDIR		bool;
	7:4		EVNTI		uint8;
	8		EL0VTEN		bool;
	9		EL0PTEN		bool;
	10		EL1PCTEN	bool;
	11		EL1PTEN		bool;
	63:12		unknown=0;
};
#endif

#if defined(ARCH_ARM_FEAT_ECV)
extend CNTHCTL_EL2_E2H0 bitfield {
	12		ECV		bool;
	13		EL1TVT		bool;
	14		EL1TVCT		bool;
	15		EL1NVPCT	bool;
	16		EL1NVVCT	bool;
	17		EVNTIS		bool;
};

#if defined(ARCH_ARM_FEAT_VHE)
extend CNTHCTL_EL2_E2H1 bitfield {
	12		ECV		bool;
	13		EL1TVT		bool;
	14		EL1TVCT		bool;
	15		EL1NVPCT	bool;
	16		EL1NVVCT	bool;
	17		EVNTIS		bool;
};
#endif
#endif

define CNT_CTL bitfield<64> {
	0		ENABLE		bool;
	1		IMASK		bool;
	2		ISTATUS		bool;
	63:3		unknown=0;
};

define CNT_CVAL bitfield<64> {
	63:0		CompareValue	uint64;
};

define CNT_TVAL bitfield<64> {
	31:0		TimerValue	sint32;
	63:32		unknown=0;
};

define CNTKCTL_EL1 bitfield<64> {
	0		EL0PCTEN	bool;
	1		EL0VCTEN	bool;
	2		EVNTEN		bool;
	3		EVNTDIR		bool;
	7:4		EVNTI		uint8;
	8		EL0VTEN		bool;
	9		EL0PTEN		bool;
	63:10		unknown=0;
};

define CNTPCT_EL0 bitfield<64> {
	63:0		CountValue	uint64;
};

define CNTVCT_EL0 bitfield<64> {
	63:0		CountValue	uint64;
};

#if defined(ARCH_ARM_FEAT_ECV)
define CNTPOFF_EL2 bitfield<64> {
	63:0		PhysicalOffset	uint64;
};
#endif

define CNTVOFF_EL2 bitfield<64> {
	63:0		VirtualOffset	uint64;
};

#define CONTEXTIDR(el)							\
define CONTEXTIDR_##el bitfield<64> {					\
	31:0		PROCID		uint32;				\
	63:32		unknown=0;					\
};
CONTEXTIDR(EL1)
CONTEXTIDR(EL2)

define CPACR_EL1 bitfield<64> {
	15:0		unknown=0;
	17:16		unknown=0;
	19:18		unknown=0;
	21:20		FPEN		uint8;
	28		TTA		bool;
	63:29		unknown=0;
};

#if defined(ARCH_ARM_FEAT_SVE)
extend CPACR_EL1 bitfield {
	17:16		ZEN		uint8;
};
#endif

define CPTR_EL2_E2H0 bitfield<64> {
	7:0		unknown=0b11111111;
	8		unknown=1;
	9		unknown=1;
	10		TFP		bool;
	11		unknown=0;
	13:12		unknown=0b11;
	19:14		unknown=0;
	20		TTA		bool;
	29:21		unknown=0;
	30		unknown=0;
	31		TCPAC		bool;
	63:32		unknown=0;
};

#if defined(ARCH_ARM_FEAT_VHE)
define CPTR_EL2_E2H1 bitfield<64> {
	15:0		unknown=0;
	17:16		unknown=0;
	19:18		unknown=0;
	21:20		FPEN		uint8;
	27:22		unknown=0;
	28		TTA		bool;
	29		unknown=0;
	30		unknown=0;
	31		TCPAC		bool;
	63:32		unknown=0;
};
#endif

#if defined(ARCH_ARM_FEAT_SVE)
extend CPTR_EL2_E2H0 bitfield {
	8		TZ		bool;
};
#endif

#if defined(ARCH_ARM_FEAT_SVE)
extend CPTR_EL2_E2H1 bitfield {
	17:16		ZEN		enumeration cptr_zen;
};
#endif

#if defined(ARCH_ARM_FEAT_AMUv1) || defined(ARCH_ARM_FEAT_AMUv1p1)
extend CPTR_EL2_E2H1 bitfield {
	30		TAM		bool;
};
extend CPTR_EL2_E2H0 bitfield {
	30		TAM		bool;
};
#endif

define CSSELR_EL1 bitfield<64> {
	0		InD		bool;
	3:1		Level		uint8;
	63:4		unknown=0;
};

#if defined(ARCH_ARM_FEAT_MTE)
extend CSSELR_EL1 bitfield {
	4		TnD		bool;
};
#endif

define CTR_EL0 bitfield<64> {
	3:0		IminLine	uint8;
	13:4		unknown=0;
	15:14		L1Ip		uint8;
	19:16		DminLine	uint8;
	23:20		ERG		uint8;
	27:24		CWG		uint8;
	28		IDC		bool;
	29		DIC		bool;
	30		unknown=0;
	31		unknown=1;
	63:32		unknown=0;
};

#if defined(ARCH_ARM_FEAT_MTE)
extend CTR_EL0 bitfield {
	37:32		TminLine	uint8;
};
#endif

define CurrentEL bitfield<64> {
	1:0		unknown=0;
	3:2		EL		enumeration currentel_el;
	63:4		unknown=0;
};

define DACR32_EL2 bitfield<64> {
	1:0		D0		uint8;
	3:2		D1		uint8;
	5:4		D2		uint8;
	7:6		D3		uint8;
	9:8		D4		uint8;
	11:10		D5		uint8;
	13:12		D6		uint8;
	15:14		D7		uint8;
	17:16		D8		uint8;
	19:18		D9		uint8;
	21:20		D10		uint8;
	23:22		D11		uint8;
	25:24		D12		uint8;
	27:26		D13		uint8;
	29:28		D14		uint8;
	31:30		D15		uint8;
	63:32		unknown=0;
};

define DAIF bitfield<64> {
	5:0		unknown=0;
	6		F		bool;
	7		I		bool;
	8		A		bool;
	9		D		bool;
	63:10		unknown=0;
};

define DCZID_EL0 bitfield<64>(const) {
	3:0		BS		uint8;
	4		DZP		bool;
	63:5		unknown=0;
};

define DIT bitfield<64> {
	23:0		unknown=0;
	24		DIT		bool;
	63:25		unknown=0;
};

#define ELR(el)								\
define ELR_##el bitfield<64> {						\
	63:0		ReturnAddress	uint64;				\
};
ELR(EL1)
ELR(EL2)

#if defined(ARCH_ARM_FEAT_RAS) || defined(ARCH_ARM_FEAT_RASv1p1)
define ERRSELR_EL1 bitfield<64> {
	15:0		SEL		uint16;
	others		unknown=0;
};

define ERXSTATUS_EL1 bitfield<64>(const) {
	7:0		SERR		uint8;
	15:8		IERR		uint8;
	19:16		unknown=0;
	21:20		UET		uint8;
	22		PN		bool;
	23		DE		bool;
	25:24		CE		uint8;
	26		MV		bool;
	27		OF		bool;
	28		ER		bool;
	29		UE		bool;
	30		V		bool;
	31		AV		bool;
	others		unknown=0;
};
#endif

#define ESR(el)								\
define ESR_##el bitfield<64> {						\
	24:0		ISS		uint32;				\
	25		IL		bool;				\
	31:26		EC		enumeration esr_ec;		\
	63:32		unknown=0;					\
};
ESR(EL1)
ESR(EL2)

define FAR_EL1 bitfield<64> {
	63:0		VirtualAddress	uint64;
};

define FAR_EL2 bitfield<64> {
	63:0		VirtualAddress	uint64;
};

define FPCR bitfield<64> {
	7:0		unknown=0;
	8		IOE		bool;
	9		DZE		bool;
	10		OFE		bool;
	11		UFE		bool;
	12		IXE		bool;
	14:13		unknown=0;
	15		IDE		bool;
	18:16		Len		uint8;
	19		FZ16		bool;
	21:20		Stride		uint8;
	23:22		RMode		uint8;
	24		FZ		bool;
	25		DN		bool;
	26		AHP		bool;
	63:27		unknown=0;
};

define FPSR bitfield<64> {
	0		IOC		bool;
	1		DZC		bool;
	2		OFC		bool;
	3		UFC		bool;
	4		IXC		bool;
	6:5		unknown=0;
	7		IDC		bool;
	26:8		unknown=0;
	27		QC		bool;
	28		V		bool;
	29		C		bool;
	30		Z		bool;
	31		N		bool;
	63:32		unknown=0;
};

define FPEXC32_EL2 bitfield<64> {
	0		IOF		bool;
	1		DZF		bool;
	2		OFF		bool;
	3		UFF		bool;
	4		IXF		bool;
	6:5		unknown=0;
	7		IDF		bool;
	10:8		VECITR		uint8;
	25:11		unknown=0;
	26		TFV		bool;
	27		VV		bool;
	28		FP2V		bool;
	29		DEX		bool;
	30		EN		bool;
	31		EX		bool;
	63:32		unknown=0;
};

define HACR_EL2 bitfield<64> {
	63:0		unknown=0;
};

define HCR_EL2 bitfield<64> {
	0		VM		bool;
	1		SWIO		bool;
	2		PTW		bool;
	3		FMO		bool;
	4		IMO		bool;
	5		AMO		bool;
	6		VF		bool;
	7		VI		bool;
	8		VSE		bool;
	9		FB		bool;
	11:10		BSU		uint8;
	12		DC		bool;
	13		TWI		bool;
	14		TWE		bool;
	15		TID0		bool;
	16		TID1		bool;
	17		TID2		bool;
	18		TID3		bool;
	19		TSC		bool;
	20		TIDCP		bool;
	21		TACR		bool;
	22		TSW		bool;
	23		TPCP		bool;
	24		TPU		bool;
	25		TTLB		bool;
	26		TVM		bool;
	27		TGE		bool;
	28		TDZ		bool;
	29		HCD		bool;
	30		TRVM		bool;
	31		RW		bool;
	32		CD		bool;
	33		ID		bool;
	35:34		unknown=0;
	36		unknown=1;
	37		unknown=0;
	38		MIOCNCE		bool;
	39		unknown=0;
	41:40		unknown=0;
	43:42		unknown=0;
	44		unknown=0;
	45		unknown=0;
	46		unknown=0;
	47		unknown=0;
	63:48		unknown=0;
};

#if defined(ARCH_ARM_FEAT_VHE)
extend HCR_EL2 bitfield {
	34		E2H		bool;
	35		TLOR		bool;
};
#endif

#if defined(ARCH_ARM_FEAT_RAS) || defined(ARCH_ARM_FEAT_RASv1p1)
extend HCR_EL2 bitfield {
	36		TERR		bool;
	37		TEA		bool;
};
#endif

#if defined(ARCH_ARM_FEAT_PAuth)
extend HCR_EL2 bitfield {
	40		APK		bool;
	41		API		bool;
};
#endif

#if defined(ARCH_ARM_FEAT_NV)
extend HCR_EL2 bitfield {
	42		NV		bool;
	43		NV1		bool;
	44		AT		bool;
};
#endif

#if defined(ARCH_ARM_FEAT_NV2)
extend HCR_EL2 bitfield {
	45		NV2		bool;
};
#endif

#if defined(ARCH_ARM_FEAT_S2FWB)
extend HCR_EL2 bitfield {
	46		FWB		bool;
};
#endif

#if defined(ARCH_ARM_FEAT_RASv1p1)
extend HCR_EL2 bitfield {
	47		FIEN		bool;
};
#endif

#if defined(ARCH_ARM_FEAT_CSV2_2) || defined(ARCH_ARM_FEAT_CSV2_1p2) || \
	defined(ARCH_ARM_FEAT_CSV2_3)
#if !defined(ARCH_ARM_FEAT_CSV2)
#error ARCH_ARM_FEAT_CSV2 not defined
#endif
define ARCH_ARM_HAVE_SCXT constant bool = 1;

extend HCR_EL2 bitfield {
	53		EnSCXT		bool;
};
#endif

#if defined(ARCH_ARM_FEAT_MTE)
extend HCR_EL2 bitfield {
	56		ATA		bool;
	57		DCT		bool;
	58		TID5		bool;
};
#endif

define HPFAR_EL2 bitfield<64> {
	3:0		unknown=0;
	39:4		FIPA		uint64 lsl(12);
	43:40		unknown=0;
	62:44		unknown=0;
	63		unknown=0;
};

#if defined(ARCH_ARM_FEAT_LPA)
extend HPFAR_EL2 bitfield {
	delete		FIPA;
	43:4		FIPA		uint64 lsl(12);
};
#endif

#if defined(ARCH_ARM_FEAT_SEL2)
extend HPFAR_EL2 bitfield {
	63		NS		bool;
};
#endif

define HSTR_EL2 bitfield<64> {
	0		T0		bool;
	1		T1		bool;
	2		T2		bool;
	3		T3		bool;
	4		T4		bool;
	5		T5		bool;
	6		T6		bool;
	7		T7		bool;
	8		T8		bool;
	9		T9		bool;
	10		T10		bool;
	11		T11		bool;
	12		T12		bool;
	13		T13		bool;
	14		T14		bool;
	15		T15		bool;
	63:16		unknown=0;
};

define ID_AA64DFR0_EL1 bitfield<64> {
	3:0		DebugVer	uint8;
	7:4		TraceVer	uint8;
	11:8		PMUVer		uint8;
	15:12		BRPs		uint8 = 1;
	19:16		unknown=0;
	23:20		WRPs		uint8 = 1;
	27:24		unknown=0;
	31:28		CTX_CMPs	uint8;
	35:32		PMSVer		uint8;
	39:36		DoubleLock	uint8;
	43:40		TraceFilt	uint8;
	63:44		unknown=0;
};

define ID_AA64MMFR0_EL1 bitfield<64> {
	3:0		PARange		enumeration tcr_ps;
	7:4		ASIDBits	uint8;
	11:8		BigEnd		uint8;
	15:12		SNSMem		uint8;
	19:16		BigEndEL0	uint8;
	23:20		TGran16		uint8;
	27:24		TGran64		uint8;
	31:28		TGran4		uint8;
	35:32		TGran16_2	uint8;
	39:36		TGran64_2	uint8;
	43:40		TGran4_2	uint8;
	47:44		ExS		uint8;
	63:48		unknown=0;
	59:56		FGT		uint8;
	63:60		ECV		uint8;
};

define ID_AA64MMFR1_EL1 bitfield<64> {
	3:0		HAFDBS		uint8;
	7:4		VMIDBits	uint8;
	11:8		VH		uint8;
	15:12		HPDS		uint8;
	19:16		LO		uint8;
	23:20		PAN		uint8;
	27:24		SpecSEI		uint8;
	31:28		XNX		uint8;
	35:32		TWED		uint8;
	39:36		ETS		uint8;
	43:40		HCX		uint8;
	47:44		AFP		uint8;
	51:48		nTLBPA		uint8;
	55:52		TIDCP1		uint8;
	59:56		CMOW		uint8;
	63:60		ECBHB		uint8;
};

define ID_AA64MMFR2_EL1 bitfield<64> {
	3:0		CnP		uint8;
	7:4		UAO		uint8;
	11:8		LSM		uint8;
	15:12		IESB		uint8;
	19:16		VARange		uint8;
	23:20		CCIDX		uint8;
	27:24		NV		uint8;
	31:28		ST		uint8;
	35:32		AT		uint8;
	39:36		IDS		uint8;
	43:40		FWB		uint8;
	47:44		unknown=0;
	51:48		TTL		uint8;
	55:52		BBM		uint8;
	59:56		EVT		uint8;
	63:60		E0PD		uint8;
};

define ID_AA64MMFR3_EL1 bitfield<64> {
	3:0		TCRX		uint8;
	7:4		SCTLRX		uint8;
	27:8		unknown=0;
	31:28		MEC		uint8;
	59:32		unknown=0;
	63:60		Spec_FPACC	uint8;
};

define ID_AA64MMFR4_EL1 bitfield<64> {
	63:0		unknown=0;
};

define ID_AA64PFR0_EL1 bitfield<64> {
	3:0		EL0		uint8;
	7:4		EL1		uint8;
	11:8		EL2		uint8;
	15:12		EL3		uint8;
	19:16		FP		uint8;
	23:20		AdvSIMD		uint8;
	27:24		GIC		uint8;
	31:28		RAS		uint8;
	35:32		SVE		uint8;
	39:36		SEL2		uint8;
	43:40		MPAM		uint8;
	47:44		AMU		uint8;
	51:48		DIT		uint8;
	55:52		RME		uint8;
	59:56		CSV2		uint8;
	63:60		CSV3		uint8;
};

define ID_AA64PFR1_EL1 bitfield<64> {
	3:0		BT		uint8;
	7:4		SSBS		uint8;
	11:8		MTE		uint8;
	15:12		RAS_frac	uint8;
	19:16		MPAM_frac	uint8;
	27:24		SME		uint8;
	31:28		RNDR_trap	uint8;
	35:32		CSV2_frac	uint8;
	39:36		NMI		uint8;
	others		unknown=0;
};

define ID_AA64ISAR0_EL1 bitfield<64> {
	3:0		unknown=0;
	7:4		AES		uint8;
	11:8		SHA1		uint8;
	15:12		SHA2		uint8;
	19:16		CRC32		uint8;
	23:20		Atomic		uint8;
	27:24		unknown=0;
	31:28		RDM		uint8;
	35:32		SHA3		uint8;
	39:36		SM3		uint8;
	43:40		SM4		uint8;
	47:44		DP		uint8;
	51:48		FHM		uint8;
	55:52		TS		uint8;
	59:56		TLB		uint8;
	63:60		RNDR		uint8;
};

define ID_AA64ISAR1_EL1 bitfield<64> {
	3:0		DPB		uint8;
	7:4		APA		uint8;
	11:8		API		uint8;
	15:12		JSCVT		uint8;
	19:16		FCMA		uint8;
	23:20		LRCPC		uint8;
	27:24		GPA		uint8;
	31:28		GPI		uint8;
	35:32		FRINTTS		uint8;
	39:36		SB		uint8;
	43:40		SPECRES		uint8;
	47:44		BF16		uint8;
	51:48		DGH		uint8;
	55:52		I8MM		uint8;
	others		unknown=0;
};

define ID_AA64ISAR2_EL1 bitfield<64> {
	3:0		WFxT		uint8;
	7:4		RPRES		uint8;
	11:8		GPA3		uint8;
	15:12		APA3		uint8;
	19:16		MOPS		uint8;
	23:20		BC		uint8;
	27:24		PAC_frac	uint8;
	31:28		CLRBHB		uint8;
	63:32		unknown=0;
};

define ID_DFR0_EL1 bitfield<64> {
	3:0		CopDbg		uint8;
	7:4		CopSDbg		uint8;
	11:8		MMapDbg		uint8;
	15:12		CopTrc		uint8;
	19:16		MMapTrc		uint8;
	23:20		MProfDbg	uint8;
	27:24		PerfMon		uint8;
	31:28		TraceFilt	uint8;
	63:32		unknown=0;
};

define ID_PFR0_EL1 bitfield<64> {
	3:0		State0		uint8;
	7:4		State1		uint8;
	11:8		State2		uint8;
	15:12		State3		uint8;
	19:16		CSV2		uint8;
	23:20		AMU		uint8;
	27:24		DIT		uint8;
	31:28		RAS		uint8;
	63:32		unknown=0;
};

define ID_PFR1_EL1 bitfield<64> {
	3:0		ProgMod		uint8;
	7:4		Security	uint8;
	11:8		MProgMod	uint8;
	15:12		Virtualization	uint8;
	19:16		GenTimer	uint8;
	23:20		Sec_frac	uint8;
	27:24		Virt_frac	uint8;
	31:28		GIC		uint8;
	63:32		unknown=0;
};

define ID_PFR2_EL1 bitfield<64> {
	3:0		CSV3		uint8;
	7:4		SSBS		uint8;
	11:8		RAS_frac	uint8;
	63:12		unknown=0;
};

define ID_ISAR0_EL1 bitfield<64> {
	3:0		Swap		uint8;
	7:4		BitCount	uint8;
	11:8		BitField	uint8;
	15:12		CmpBranch	uint8;
	19:16		Coproc		uint8;
	23:20		Debug		uint8;
	27:24		Divide		uint8;
	63:28		unknown=0;
};

define ID_ISAR1_EL1 bitfield<64> {
	3:0		Endian		uint8;
	7:4		Except		uint8;
	11:8		Except_AR	uint8;
	15:12		Extend		uint8;
	19:16		IfThen		uint8;
	23:20		Immediate	uint8;
	27:24		Interwork	uint8;
	31:28		Jazelle		uint8;
	63:32		unknown=0;
};

define ID_ISAR2_EL1 bitfield<64> {
	3:0		LoadStore	uint8;
	7:4		MemHint		uint8;
	11:8		MultiAccessInt	uint8;
	15:12		Mult		uint8;
	19:16		MultS		uint8;
	23:20		MultU		uint8;
	27:24		PSR_AR		uint8;
	31:28		Reversal	uint8;
	63:32		unknown=0;
};

define ID_ISAR3_EL1 bitfield<64> {
	3:0		Saturate	uint8;
	7:4		SIMD		uint8;
	11:8		SVC		uint8;
	15:12		SynchPrim	uint8;
	19:16		TabBranch	uint8;
	23:20		T32Copy		uint8;
	27:24		TrueNOP		uint8;
	31:28		T32EE		uint8;
	63:32		unknown=0;
};

define ID_ISAR4_EL1 bitfield<64> {
	3:0		Unpriv		uint8;
	7:4		WithShifts	uint8;
	11:8		Writeback	uint8;
	15:12		SMC		uint8;
	19:16		Barrier		uint8;
	23:20		SynchPrim_frac	uint8;
	27:24		PSR_M		uint8;
	31:28		SWP_frac	uint8;
	63:32		unknown=0;
};

define ID_ISAR5_EL1 bitfield<64> {
	3:0		SEVL		uint8;
	7:4		AES		uint8;
	11:8		SHA1		uint8;
	15:12		SHA2		uint8;
	19:16		CRC32		uint8;
	23:20		unknown=0;
	27:24		RDM		uint8;
	31:28		VCMA		uint8;
	63:32		unknown=0;
};

define ID_ISAR6_EL1 bitfield<64> {
	3:0		JSCVT		uint8;
	7:4		DP		uint8;
	11:8		FHM		uint8;
	15:12		SB		uint8;
	19:16		SPECRES		uint8;
	63:20		unknown=0;
};

define ID_MMFR0_EL1 bitfield<64> {
	3:0		VMSA		uint8;
	7:4		PMSA		uint8;
	11:8		OuterShr	uint8;
	15:12		ShareLvl	uint8;
	19:16		TCM		uint8;
	23:20		AuxReg		uint8;
	27:24		FCSE		uint8;
	31:28		InnerShr	uint8;
	63:32		unknown=0;
};

define ID_MMFR1_EL1 bitfield<64> {
	3:0		L1HvdVA		uint8;
	7:4		L1UniVA		uint8;
	11:8		L1HvdSW		uint8;
	15:12		L1UniSW		uint8;
	19:16		L1Hvd		uint8;
	23:20		L1Uni		uint8;
	27:24		L1TstCln	uint8;
	31:28		BPred		uint8;
	63:32		unknown=0;
};

define ID_MMFR2_EL1 bitfield<64> {
	3:0		L1HvdFG		uint8;
	7:4		L1HvdBG		uint8;
	11:8		L1HvdRng	uint8;
	15:12		HvdTLB		uint8;
	19:16		UniTLB		uint8;
	23:20		MemBarr		uint8;
	27:24		WFIStall	uint8;
	31:28		HWAccFlg	uint8;
	63:32		unknown=0;
};

define ID_MMFR3_EL1 bitfield<64> {
	3:0		CMaintVA	uint8;
	7:4		CMaintSW	uint8;
	11:8		BPMaint		uint8;
	15:12		MaintBcst	uint8;
	19:16		PAN		uint8;
	23:20		CohWalk		uint8;
	27:24		CMemSz		uint8;
	31:28		Supersec	uint8;
	63:32		unknown=0;
};

define ID_MMFR4_EL1 bitfield<64> {
	3:0		SpecSEI		uint8;
	7:4		AC2		uint8;
	11:8		XNX		uint8;
	15:12		CnP		uint8;
	19:16		HPDS		uint8;
	23:20		LSM		uint8;
	27:24		CCIDX		uint8;
	31:28		EVT		uint8;
	63:32		unknown=0;
};

define ISR_EL1 bitfield<64> {
	5:0		unknown=0;
	6		F		bool;
	7		I		bool;
	8		A		bool;
	63:9		unknown=0;
};

define MAIR_ATTR enumeration {
	DEVICE_nGnRnE	= 0x0;
	DEVICE_nGnRE	= 0x4;
	DEVICE_nGRE	= 0x8;
	DEVICE_GRE	= 0xc;

	DEVICE_nGnRnE_XS= 0x1;
	DEVICE_nGnRE_XS	= 0x5;
	DEVICE_nGRE_XS	= 0x9;
	DEVICE_GRE_XS	= 0xd;

	NORMAL_NC	= 0x44; // Inner+outer non-cacheable
	NORMAL_WB_OUTER_NC = 0x4f; // Inner writeback RW alloc
#if defined(ARCH_ARM_FEAT_MTE)
	TAGGED_NORMAL_WB = 0xf0; // Inner+outer writeback RW alloc, MT enabled
#endif
	NORMAL_WB	= 0xff; // Inner/outer writeback RW alloc
	// Other combinations of types not enumerated
};

// Bits that indicate RW alloc hints for normal memory types (other than
// TAGGED_NORMAL_WB which is special)
define MAIR_ATTR_ALLOC_HINT_MASK constant enumeration MAIR_ATTR = 0x33;

#define MAIR(el)							\
define MAIR_##el bitfield<64> {						\
	7:0		Attr0		enumeration MAIR_ATTR;		\
	15:8		Attr1		enumeration MAIR_ATTR;		\
	23:16		Attr2		enumeration MAIR_ATTR;		\
	31:24		Attr3		enumeration MAIR_ATTR;		\
	39:32		Attr4		enumeration MAIR_ATTR;		\
	47:40		Attr5		enumeration MAIR_ATTR;		\
	55:48		Attr6		enumeration MAIR_ATTR;		\
	63:56		Attr7		enumeration MAIR_ATTR;		\
};
MAIR(EL1)
MAIR(EL2)

define MDCR_EL2 bitfield<64> {
	4:0		unknown=0b00001;
	7:5		unknown=0b011;
	8		TDE		bool = 0;
	9		TDA		bool = 1;
	10		TDOSA		bool = 1;
	11		TDRA		bool = 1;
	13:12		unknown=0;
	28:14		unknown=0b010001000101001;
	63:29		unknown=0;
};

#if ARCH_ARM_PMU_VER >= 3
extend MDCR_EL2 bitfield {
	4:0		HPMN		uint8;
	5		TPMCR		bool;
	6		TPM		bool;
	7		HPME		bool;
};
#endif

#if defined(ARCH_ARM_FEAT_SPE)
extend MDCR_EL2 bitfield {
	13:12		E2PB		uint8;
	14		TPMS		bool;
};
#endif

#if defined(ARCH_ARM_FEAT_PMUv3p1) || defined(ARCH_ARM_FEAT_PMUv3p4)
extend MDCR_EL2 bitfield {
	17		HPMD		bool;
};
#endif

#if defined(ARCH_ARM_FEAT_TRF)
extend MDCR_EL2 bitfield {
	19		TTRF		bool;
};
#endif

#if defined(MODULE_PLATFORM_TRBE)
extend ID_AA64DFR0_EL1 bitfield {
	47:44		TraceBuffer	uint8;
};
#endif

define MIDR_EL1 bitfield<64> {
	3:0		Revision	uint8;
	15:4		PartNum		uint16;
	19:16		Architecture	uint8;
	23:20		Variant		uint8;
	31:24		Implementer	uint8;
	63:32		unknown=0;
};

define MPIDR_EL1 bitfield<64> {
	7:0		Aff0		uint8;
	15:8		Aff1		uint8;
	23:16		Aff2		uint8;
	24		MT		bool;
	29:25		unknown=0;
	30		U		bool;
	31		unknown=1;
	39:32		Aff3		uint8;
	63:40		unknown=0;
};

define NZCV bitfield<64> {
	27:0		unknown=0;
	28		V		bool;
	29		C		bool;
	30		Z		bool;
	31		N		bool;
	63:32		unknown=0;
};

define PAN bitfield<64> {
	21:0		unknown=0;
	22		PAN		bool;
	63:23		unknown=0;
};

define PAR_EL1_base bitfield<64> {
	0		F		bool;
	63:1		unknown=0;
};

define PAR_EL1_F0 bitfield<64> {
	0		F		bool = 0;
	6:1		unknown=0;
	8:7		SH		enumeration vmsa_shareability;
	9		NS		bool;
	10		IMPDEF		bool;
	11		unknown=1;
	47:12		PA		uint64 lsl(12);
	51:48		unknown=0;
	55:52		unknown=0;
	63:56		ATTR		enumeration MAIR_ATTR;
};

#if defined(ARCH_ARM_FEAT_LPA)
extend PAR_EL1_F0 bitfield {
	delete		PA;
	51:12		PA		uint64 lsl(12);
};
#endif

define PAR_EL1_F1 bitfield<64> {
	0		F		bool = 1;
	6:1		FST		enumeration iss_da_ia_fsc;
	7		unknown=0;
	8		PTW		bool;
	9		S		bool;
	10		unknown=0;
	11		unknown=1;
	47:12		unknown=0;
	51:48		IMPDEF1		uint8;
	55:52		IMPDEF2		uint8;
	63:56		IMPDEF3		uint8;
};

define PAR_EL1 union {
	f0		bitfield PAR_EL1_F0;
	f1		bitfield PAR_EL1_F1;
	base		bitfield PAR_EL1_base;
};

define PMCR_EL0 bitfield<64> {
	0		E		bool;
	1		P		bool;
	2		C		bool;
	3		D		bool;
	4		X		bool;
	5		DP		bool;
	6		LC		bool;
	10:7		unknown=0;
	15:11		N		uint8;
	23:16		IDCODE		uint8;
	31:24		IMP		uint8;
	63:32		unknown=0;
};

#if defined(ARCH_ARM_FEAT_PMUv3p5)
extend PMCR_EL0 bitfield {
	7		LP		bool;
};
#endif

define SCTLR_EL1 bitfield<64> {
	0		M		bool;
	1		A		bool;
	2		C		bool;
	3		SA		bool;
	4		SA0		bool;
	5		CP15BEN		bool;
	6		unknown=0;
	7		ITD		bool;
	8		SED		bool;
	9		UMA		bool;
	10		unknown=0;
	11		unknown=1;
	12		I		bool;
	13		unknown=0;
	14		DZE		bool;
	15		UCT		bool;
	16		nTWI		bool;
	17		unknown=0;
	18		nTWE		bool;
	19		WXN		bool;
	20		unknown=1;
	21		unknown=0;
	22		unknown=1;
	23		unknown=1;
	24		E0E		bool;
	25		EE		bool;
	26		UCI		bool;
	27		unknown=0;
	29:28		unknown=0b11;
	31:30		unknown=0;
	63:32		unknown=0;
};

#if defined(ARCH_ARM_FEAT_PAN)
extend SCTLR_EL1 bitfield {
	23		SPAN		bool;
};
#endif

#if defined(ARCH_ARM_FEAT_IESB)
extend SCTLR_EL1 bitfield {
	21		IESB		bool;
};
#endif

#if defined(ARCH_ARM_FEAT_LSMAOC)
extend SCTLR_EL1 bitfield {
	28		nTLSMD		bool;
	29		LSMAOE		bool;
};
#endif

#if defined(ARCH_ARM_FEAT_PAuth)
extend SCTLR_EL1 bitfield {
	13		EnDB		bool;
	27		EnDA		bool;
	30		EnIB		bool;
	31		EnIA		bool;
};
#endif

#if defined(ARCH_ARM_FEAT_LSE2)
extend SCTLR_EL1 bitfield {
	6		nAA		bool;
};
#endif

#if defined(ARCH_ARM_FEAT_MTE)
extend SCTLR_EL1 bitfield {
	43		ATA		bool;
	42		ATA0		bool;
	41:40		TCF		uint8;
	39:38		TCF0		uint8;
	37		ITFSB		bool;
};
#endif

#if defined(ARCH_ARM_FEAT_SSBS)
extend SCTLR_EL1 bitfield {
	44		DSSBS		bool;
};
#endif

define SCTLR_EL2_VM bitfield<64> {
	0		M		bool;
	1		A		bool;
	2		C		bool;
	3		SA		bool;
	5:4		unknown=0b11;
	6		unknown=0;
	10:7		unknown=0;
	11		unknown=1;
	12		I		bool;
	13		unknown=0;
	15:14		unknown=0;
	16		unknown=1;
	17		unknown=0;
	18		unknown=1;
	19		WXN		bool;
	20		unknown=0;
	21		unknown=0;
	23:22		unknown=0b11;
	24		unknown=0;
	25		EE		bool;
	26		unknown=0;
	27		unknown=0;
	29:28		unknown=0b11;
	30		unknown=0;
	31		unknown=0;
	63:32		unknown=0;
};

#if defined(ARCH_ARM_FEAT_VHE)
define SCTLR_EL2_E2H_TGE bitfield<64> {
	0		M		bool;
	1		A		bool;
	2		C		bool;
	3		SA		bool;
	4		SA0		bool;
	5		CP15BEN		bool;
	6		unknown=0;
	7		ITD		bool;
	8		SED		bool;
	10:9		unknown=0;
	11		unknown=1;
	12		I		bool;
	13		unknown=0;
	14		DZE		bool;
	15		UCT		bool;
	16		nTWI		bool;
	17		unknown=0;
	18		nTWE		bool;
	19		WXN		bool;
	20		unknown=1;
	21		unknown=0;
	22		unknown=1;
	23		SPAN		bool;
	24		E0E		bool;
	25		EE		bool;
	26		UCI		bool;
	27		unknown=0;
	29:28		unknown=0b11;
	30		unknown=0;
	31		unknown=0;
	63:32		unknown=0;
};
#endif

#if defined(ARCH_ARM_FEAT_IESB)
extend SCTLR_EL2_VM bitfield {
	21		IESB		bool;
};
extend SCTLR_EL2_E2H_TGE bitfield {
	21		IESB		bool;
};
#endif

#if defined(ARCH_ARM_FEAT_LSMAOC)
extend SCTLR_EL2_E2H_TGE bitfield {
	28		nTLSMD		bool;
	29		LSMAOE		bool;
};
#endif

#if defined(ARCH_ARM_FEAT_PAuth)
extend SCTLR_EL2_VM bitfield {
	13		EnDB		bool;
	27		EnDA		bool;
	30		EnIB		bool;
	31		EnIA		bool;
};
extend SCTLR_EL2_E2H_TGE bitfield {
	13		EnDB		bool;
	27		EnDA		bool;
	30		EnIB		bool;
	31		EnIA		bool;
};
#endif

#if defined(ARCH_ARM_FEAT_LSE2)
extend SCTLR_EL2_VM bitfield {
	6		nAA		bool;
};
extend SCTLR_EL2_E2H_TGE bitfield {
	6		nAA		bool;
};
#endif

#if defined(ARCH_ARM_FEAT_MTE)
extend SCTLR_EL2_VM bitfield {
	43		ATA		bool;
	42		ATA0		bool;
	41:40		TCF		uint8;
	39:38		TCF0		uint8;
	37		ITFSB		bool;
};
extend SCTLR_EL2_E2H_TGE bitfield {
	43		ATA		bool;
	41:40		TCF		uint8;
	37		ITFSB		bool;
};
#endif

#if defined(ARCH_ARM_FEAT_SSBS)
extend SCTLR_EL2_VM bitfield {
	44		DSSBS		bool;
};
extend SCTLR_EL2_E2H_TGE bitfield {
	44		DSSBS		bool;
};
#endif

#define SP(el)								\
define SP_##el bitfield<64> {						\
	63:0		Address		uint64;				\
};
SP(EL0)
SP(EL1)
SP(EL2)

define SPSel bitfield<64> {
	0		SP		bool;
	63:1		unknown=0;
};

define SPSR_EL2_A32 bitfield<64> {
	4:0		M		enumeration spsr_32bit_mode;
	5		T		bool;
	6		F		bool;
	7		I		bool;
	8		A		bool;
	9		E		bool;
	15:10,26:25	IT		uint8;
	19:16		GE		uint8;
	20		IL		bool;
	21		SS		bool;
	22		unknown=0;
	23		unknown=0;
	24		unknown=0;
	27		Q		bool;
	28		V		bool;
	29		C		bool;
	30		Z		bool;
	31		N		bool;
	63:32		unknown=0;
};

#if defined(ARCH_ARM_FEAT_PAN)
extend SPSR_EL2_A32 bitfield {
	22		PAN		bool;
};
#endif

#if defined(ARCH_ARM_FEAT_DIT)
extend SPSR_EL2_A32 bitfield {
	23		DIT		bool;
};
#endif

#define SPSR_A64(el)							\
define SPSR_##el##_A64 bitfield<64> {					\
	4:0		M		enumeration spsr_64bit_mode;	\
	5		unknown=0;					\
	6		F		bool;				\
	7		I		bool;				\
	8		A		bool;				\
	9		D		bool;				\
	19:10		unknown=0;					\
	20		IL		bool;				\
	21		SS		bool;				\
	22		unknown=0;					\
	23		unknown=0;					\
	24		unknown=0;					\
	27:25		unknown=0;					\
	28		V		bool;				\
	29		C		bool;				\
	30		Z		bool;				\
	31		N		bool;				\
	63:32		unknown=0;					\
};
SPSR_A64(EL1)
SPSR_A64(EL2)

#if defined(ARCH_ARM_FEAT_BTI)
extend SPSR_EL1_A64 bitfield {
	11:10		BTYPE		uint8;
};
extend SPSR_EL2_A64 bitfield {
	11:10		BTYPE		uint8;
};
#endif

#if defined(ARCH_ARM_FEAT_SSBS)
extend SPSR_EL1_A64 bitfield {
	12		SSBS		bool;
};
extend SPSR_EL2_A64 bitfield {
	12		SSBS		bool;
};
#endif

#if defined(ARCH_ARM_FEAT_PAN)
extend SPSR_EL1_A64 bitfield {
	22		PAN		bool;
};
extend SPSR_EL2_A64 bitfield {
	22		PAN		bool;
};
#endif

#if defined(ARCH_ARM_FEAT_UAO)
extend SPSR_EL1_A64 bitfield {
	23		UAO		bool;
};
extend SPSR_EL2_A64 bitfield {
	23		UAO		bool;
};
#endif

#if defined(ARCH_ARM_FEAT_DIT)
extend SPSR_EL1_A64 bitfield {
	24		DIT		bool;
};
extend SPSR_EL2_A64 bitfield {
	24		DIT		bool;
};
#endif

#if defined(ARCH_ARM_FEAT_MTE)
extend SPSR_EL1_A64 bitfield {
	25		TCO		bool;
};
extend SPSR_EL2_A64 bitfield {
	25		TCO		bool;
};
#endif

define SPSR_EL2_base bitfield<64> {
	3:0				unknown=0;
	4		M4		bool;
	63:5				unknown=0;
};

define SPSR_EL2 union {
	a32		bitfield SPSR_EL2_A32;
	a64		bitfield SPSR_EL2_A64;
	base		bitfield SPSR_EL2_base;
};

#if defined(ARCH_ARM_FEAT_MTE)
define TCO bitfield<64> {
	24:0		unknown=0;
	25		TCO		bool;
	63:26		unknown=0;
};
#endif

define TCR_EL1 bitfield<64> {
	5:0		T0SZ		uint8;
	6		unknown=0;
	7		EPD0		bool;
	9:8		IRGN0		enumeration tcr_rgn;
	11:10		ORGN0		enumeration tcr_rgn;
	13:12		SH0		enumeration tcr_sh;
	15:14		TG0		enumeration tcr_tg0;
	21:16		T1SZ		uint8;
	22		A1		bool;
	23		EPD1		bool;
	25:24		IRGN1		enumeration tcr_rgn;
	27:26		ORGN1		enumeration tcr_rgn;
	29:28		SH1		enumeration tcr_sh;
	31:30		TG1		enumeration tcr_tg1;
	34:32		IPS		enumeration tcr_ps;
	35		unknown=0;
	36		AS		bool;
	37		TBI0		bool;
	38		TBI1		bool;
	40:39		unknown=0;
	42:41		unknown=0;
	50:43		unknown=0;
	52:51		unknown=0;
	54:53		unknown=0;
	63:55		unknown=0;
};

#if defined(ARCH_ARM_FEAT_HAFDBS)
extend TCR_EL1 bitfield {
	39		HA		bool;
	40		HD		bool;
};
#endif

#if defined(ARCH_ARM_FEAT_HPDS)
extend TCR_EL1 bitfield {
	41		HPD0		bool;
	42		HPD1		bool;
};
#endif

#if defined(ARCH_ARM_FEAT_HPDS2)
extend TCR_EL1 bitfield {
	43		HWU059		bool;
	44		HWU060		bool;
	45		HWU061		bool;
	46		HWU062		bool;
	47		HWU159		bool;
	48		HWU160		bool;
	49		HWU161		bool;
	50		HWU162		bool;
};
#endif

#if defined(ARCH_ARM_FEAT_SVE)
extend TCR_EL1 bitfield {
	53		NFD0		bool;
	54		NFD1		bool;
};
#endif

#if defined(ARCH_ARM_FEAT_PAuth)
extend TCR_EL1 bitfield {
	51		TBID0		bool;
	52		TBID1		bool;
};
#endif

#if defined(ARCH_ARM_FEAT_MTE)
extend TCR_EL1 bitfield {
	57		TCMA0		bool;
	58		TCMA1		bool;
};
#endif

define TCR_EL2_E2H0 bitfield<64> {
	5:0		T0SZ		uint8;
	7:6		unknown=0;
	9:8		IRGN0		enumeration tcr_rgn;
	11:10		ORGN0		enumeration tcr_rgn;
	13:12		SH0		enumeration tcr_sh;
	15:14		TG0		enumeration tcr_tg0;
	18:16		PS		enumeration tcr_ps;
	19		unknown=0;
	20		TBI		bool;
	22:21		unknown=0;
	23		unknown=1;
	24		unknown=0;
	28:25		unknown=0;
	29		unknown=0;
	30		unknown=0;
	31		unknown=1;
	63:32		unknown=0;
};

#if defined(ARCH_ARM_FEAT_VHE)
define TCR_EL2_E2H1 bitfield<64> {
	5:0		T0SZ		uint8;
	6		unknown=0;
	7		EPD0		bool;
	9:8		IRGN0		enumeration tcr_rgn;
	11:10		ORGN0		enumeration tcr_rgn;
	13:12		SH0		enumeration tcr_sh;
	15:14		TG0		enumeration tcr_tg0;
	21:16		T1SZ		uint8;
	22		A1		bool;
	23		EPD1		bool;
	25:24		IRGN1		enumeration tcr_rgn;
	27:26		ORGN1		enumeration tcr_rgn;
	29:28		SH1		enumeration tcr_sh;
	31:30		TG1		enumeration tcr_tg1;
	34:32		IPS		enumeration tcr_ps;
	35		unknown=0;
	36		AS		bool;
	37		unknown=0;
	38		TBI1		bool;
	63:39		unknown=0;
};
#endif

#if defined(ARCH_ARM_FEAT_HAFDBS)
extend TCR_EL2_E2H0 bitfield {
	21		HA		bool;
	22		HD		bool;
};
extend TCR_EL2_E2H1 bitfield {
	39		HA		bool;
	40		HD		bool;
};
#endif

#if defined(ARCH_ARM_FEAT_HPDS)
extend TCR_EL2_E2H0 bitfield {
	24		HPD		bool;
};
extend TCR_EL2_E2H1 bitfield {
	41		HPD0		bool;
	42		HPD1		bool;
};
#endif

#if defined(ARCH_ARM_FEAT_VHE)
extend TCR_EL2_E2H1 bitfield {
	37		TBI0		bool;
};
#endif

#if defined(ARCH_ARM_FEAT_HPDS2)
extend TCR_EL2_E2H0 bitfield {
	25		HWU059		bool;
	26		HWU060		bool;
	27		HWU061		bool;
	28		HWU062		bool;
};
extend TCR_EL2_E2H1 bitfield {
	43		HWU059		bool;
	44		HWU060		bool;
	45		HWU061		bool;
	46		HWU062		bool;
	47		HWU159		bool;
	48		HWU160		bool;
	49		HWU161		bool;
	50		HWU162		bool;
};
#endif

#if defined(ARCH_ARM_FEAT_SVE)
extend TCR_EL2_E2H1 bitfield {
	53		NFD0		bool;
	54		NFD1		bool;
};
#endif

#if defined(ARCH_ARM_FEAT_PAuth)
extend TCR_EL2_E2H0 bitfield {
	29		TBID		bool;
};
extend TCR_EL2_E2H1 bitfield {
	51		TBID0		bool;
	52		TBID1		bool;
};
#endif

#if defined(ARCH_ARM_FEAT_MTE)
extend TCR_EL2_E2H0 bitfield {
	30		TCMA		bool;
};
extend TCR_EL2_E2H1 bitfield {
	57		TCMA0		bool;
	58		TCMA1		bool;
};
#endif

#define TPIDR(el)							\
define TPIDR##el bitfield<64> {						\
	63:0		ThreadID	uint64;				\
};
TPIDR(_EL0)
TPIDR(RO_EL0)
TPIDR(_EL1)
TPIDR(_EL2)

#define TTBR(n, el)							\
define TTBR##n##_##el bitfield<64> {					\
	0		CnP		bool;				\
	47:1		BADDR		uint64 lsl(1);			\
	63:48		ASID		uint32;				\
};
TTBR(0,EL1)
TTBR(1,EL1)
TTBR(0,EL2)
TTBR(1,EL2)

#if defined(ARCH_ARM_FEAT_TRF)
define TRFCR_EL2 bitfield<64> {
	0	E0HTRE		bool = 0;
	1	E2TRE		bool = 0;
	3	CX		bool = 0;
	6:5	TS		uint8;
	others	unknown = 0;
};

define TRFCR_EL1_TS enumeration {
	VIRTUAL = 0;
	GUEST_PHYSICAL;
	PHYSICAL;
};

define TRFCR_EL1 bitfield<64> {
	0		E0TRE		bool;
	1		E1TRE		bool;
	6:5		TS		enumeration TRFCR_EL1_TS;
};
#endif

define UAO bitfield<64> {
	22:0		unknown=0;
	23		UAO		bool;
	63:24		unknown=0;
};

#define VBAR(el)							\
define VBAR_##el bitfield<64> {						\
	10:0		unknown=0;					\
	63:11		VectorBase	uint64 lsl(11);			\
};
VBAR(EL1)
VBAR(EL2)

define VTCR_EL2 bitfield<64> {
	5:0		T0SZ		uint8;
	7:6		SL0		uint8;
	9:8		IRGN0		enumeration tcr_rgn;
	11:10		ORGN0		enumeration tcr_rgn;
	13:12		SH0		enumeration tcr_sh;
	15:14		TG0		enumeration tcr_tg0;
	18:16		PS		enumeration tcr_ps;
	19		unknown=0;
	20		unknown=0;
	22:21		unknown=0;
	24:23		unknown=0;
	28:25		unknown=0;
	30:29		unknown=0;
	31		unknown=1;
	63:32		unknown=0;
};

#if defined(ARCH_ARM_FEAT_VMID16)
extend VTCR_EL2 bitfield {
	19		VS		bool;
};
#endif

#if defined(ARCH_ARM_FEAT_HAFDBS)
extend VTCR_EL2 bitfield {
	21		HA		bool;
	22		HD		bool;
};
#endif

#if defined(ARCH_ARM_FEAT_HPDS2)
extend VTCR_EL2 bitfield {
	25		HWU059		bool;
	26		HWU060		bool;
	27		HWU061		bool;
	28		HWU062		bool;
};
#endif

#if defined(ARCH_ARM_FEAT_SEL2)
extend VTCR_EL2 bitfield {
	29		NSW		bool;
	30		NSA		bool;
};
#endif

define VTTBR_EL2 bitfield<64> {
	0		CnP		bool;
	47:1		BADDR		uint64 lsl(1);
	55:48		VMID		uint32;
	63:56		unknown=0;
};


#if defined(ARCH_ARM_FEAT_VMID16)
extend VTTBR_EL2 bitfield {
	delete		VMID;
	63:48		VMID		uint32;
};
#endif



// ESR_EL2 ISS encodings (only some ISS encodings have been added)
define ESR_EL2_ISS_WFI_WFE bitfield<25> {
	0		TI		enumeration iss_wfx_ti;
	19:1		unknown=0;
	23:20		COND		uint8;
	24		CV		bool;
};

#if defined(ARCH_ARM_FEAT_WFxT)
extend ESR_EL2_ISS_WFI_WFE bitfield {
	delete		TI;
	1:0		TI		enumeration iss_wfx_ti;
	2		RV		bool;
	9:5		Rn		uint8;
};
#endif

define ESR_EL2_ISS_HVC bitfield<25> {
	15:0		imm16		uint16;
	24:16		unknown=0;
};

define ESR_EL2_ISS_SMC32 bitfield<25> {
	18:0		unknown=0;
	19		CCKNOWNPASS	bool;
	23:20		COND		uint8;
	24		CV		bool;
};

define ESR_EL2_ISS_SMC64 bitfield<25> {
	15:0		imm16		uint16;
	24:16		unknown=0;
};

define ESR_EL2_ISS_MSR_MRS bitfield<25> {
	0		Direction	bool;
	4:1		CRm		uint8;
	9:5		Rt		uint8;
	13:10		CRn		uint8;
	16:14		Op1		uint8;
	19:17		Op2		uint8;
	21:20		Op0		uint8;
	24:22		unknown=0;
};

define ESR_EL2_ISS_INST_ABORT bitfield<25> {
	5:0		IFSC		enumeration iss_da_ia_fsc;
	6		unknown=0;
	7		S1PTW		bool;
	8		unknown=0;
	9		EA		bool;
	10		FnV		bool;
	12:11		SET		enumeration iss_da_ia_set;
	24:13		unknown=0;
};

define ESR_EL2_ISS_DATA_ABORT bitfield<25> {
	5:0		DFSC		enumeration iss_da_ia_fsc;
	6		WnR		bool;
	7		S1PTW		bool;
	8		CM		bool;
	9		EA		bool;
	10		FnV		bool;
	12:11		SET		enumeration iss_da_ia_set;
	13		VNCR		bool;
	14		AR		bool;
	15		SF		bool;
	20:16		SRT		uint8;
	21		SSE		bool;
	23:22		SAS		enumeration iss_da_sas;
	24		ISV		bool;
};

define ESR_EL2_ISS_SERROR bitfield<25> {
	5:0		DFSC		enumeration iss_serror_dfsc;
	8:6		unknown=0;
	9		EA		bool;
	12:10		AET		enumeration iss_serror_aet;
	13		IESB		bool;
	23:14		unknown=0;
	24		IDS		bool;
};

#if ARCH_AARCH64_32BIT_EL0
define ESR_EL2_ISS_MCR_MRC bitfield<25> {
	0		Direction	bool;
	4:1		CRm		uint8;
	9:5		Rt		uint8;
	13:10		CRn		uint8;
	16:14		Opc1		uint8;
	19:17		Opc2		uint8;
	23:20		COND		uint8;
	24		CV		bool;
};

define ESR_EL2_ISS_MCRR_MRRC bitfield<25> {
	0		Direction	bool;
	4:1		CRm		uint8;
	9:5		Rt		uint8;
	14:10		Rt2		uint8;
	15		unknown=0;
	19:16		Opc1		uint8;
	23:20		COND		uint8;
	24		CV		bool;
};

define ESR_EL2_ISS_LDC_STC bitfield<25> {
	0		Direction	bool;
	3:1		AM		uint8;
	4		Offset		bool;
	9:5		Rn		uint8;
	11:10		unknown=0;
	19:12		imm8		uint8;
	23:20		COND		uint8;
	24		CV		bool;
};
#endif

#if defined(ARCH_ARM_FEAT_AMUv1) || defined(ARCH_ARM_FEAT_AMUv1p1)
define AMCR_EL0 bitfield<64> {
	10		HDBG		bool;
#if defined(ARCH_ARM_FEAT_AMUv1p1)
	17		CG1RZ		bool;
#endif
	others		unknown=0;
};

define AMCFGR_EL0 bitfield<64> {
	7:0		N		uint16;
	13:8		Size		uint8;
	24		HDBG		bool;
	31:28		NCG		uint8;
	others		unknown=0;
};

define AMCGCR_EL0 bitfield<64> {
	7:0		CG0NC		uint16;
	15:8		CG1NC		uint16;
	others		unknown=0;
};
#endif

#if defined(ARCH_ARM_FEAT_FGT)
define HFGWTR_EL2 bitfield<64> {
	0		AFSR0_EL1	bool;
	1		AFSR1_EL1	bool;
	3		AMAIR_EL1	bool;
	4		APDAKey		bool;
	5		APDBKey		bool;
	6		APGAKey		bool;
	7		APIAKey		bool;
	8		APIBKey		bool;
	11		CONTEXTIDR_EL1	bool;
	12		CPACR_EL1	bool;
	13		CSSELR_EL1	bool;
	16		ESR_EL1		bool;
	17		FAR_EL1		bool;
	19		LORC_EL1	bool;
	20		LOREA_EL1	bool;
	22		LORN_EL1	bool;
	23		LORSA_EL1	bool;
	24		MAIR_EL1	bool;
	27		PAR_EL1		bool;
	29		SCTLR_EL1	bool;
	30		SCTXNUM_EL1	bool;
	31		SCTXNUM_EL0	bool;
	32		TCR_EL1		bool;
	33		TPIDR_EL1	bool;
	34		TPIDRRO_EL0	bool;
	35		TPIDR_EL0	bool;
	36		TTBR0_EL1	bool;
	37		TTBR1_EL1	bool;
	38		VBAR_EL1	bool;
	39		ICC_IGRPENn_EL1	bool;
	41		ERRSELR_EL1	bool;
	43		ERXCTLR_EL1	bool;
	44		ERXSTATUS_EL1	bool;
	45		ERXMISCn_EL1	bool;
	47		ERXPFGCTL_EL1	bool;
	48		ERXPFGCDN_EL1	bool;
	49		ERXADDR_EL1	bool;
	50		nACCDATA_EL1	bool;
	54		nSMPRI_EL1	bool;
	55		nTPIDR2_EL0	bool;
	others		unknown=0;
};
#endif

```

`hyp/core/base/aarch64/types.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

define paddr_t public newtype uregister;

define PADDR_INVALID constant type paddr_t = -1;
define VADDR_INVALID constant uintptr = -1;

define core_id_info structure(optimize) {
	part_num	uint16;
	core_id		enumeration core_id;
};

define core_id_rev_info structure(optimize) {
	part_num	uint16;
	core_id		enumeration core_id;
	variant_min	uint8;
	revision_min	uint8;
};

```

`hyp/core/base/armv8/enums.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier; BSD-3-Clause

define esr_ec enumeration {
	UNKNOWN			= 0x00;
	WFIWFE			= 0x01;
	MCRMRC15		= 0x03;
	MCRRMRRC15		= 0x04;
	MCRMRC14		= 0x05;
	LDCSTC			= 0x06;
	FPEN			= 0x07;
	VMRS_EL2		= 0x08;
	MRRC14			= 0x0c;
	ILLEGAL			= 0x0e;
	SVC32			= 0x11;
	HVC32_EL2		= 0x12;
	SMC32_EL2		= 0x13;
	INST_ABT_LO		= 0x20;
	INST_ABT		= 0x21;
	PC_ALIGN		= 0x22;
	DATA_ABT_LO		= 0x24;
	DATA_ABT		= 0x25;
};

define iss_da_ia_fsc enumeration {
	ADDR_SIZE_0		= 0x00;
	ADDR_SIZE_1		= 0x01;
	ADDR_SIZE_2		= 0x02;
	ADDR_SIZE_3		= 0x03;
	TRANSLATION_1		= 0x05;
	TRANSLATION_2		= 0x06;
	TRANSLATION_3		= 0x07;
	ACCESS_FLAG_1		= 0x09;
	ACCESS_FLAG_2		= 0x0a;
	ACCESS_FLAG_3		= 0x0b;
	PERMISSION_1		= 0x0d;
	PERMISSION_2		= 0x0e;
	PERMISSION_3		= 0x0f;
	SYNC_EXTERNAL		= 0x10;
	SYNC_EXTERN_WALK_1	= 0x15;
	SYNC_EXTERN_WALK_2	= 0x16;
	SYNC_EXTERN_WALK_3	= 0x17;
	SYNC_PARITY_ECC		= 0x18;
	SYNC_PARITY_ECC_WALK_1	= 0x1d;
	SYNC_PARITY_ECC_WALK_2	= 0x1e;
	SYNC_PARITY_ECC_WALK_3	= 0x1f;
	DEBUG			= 0x22;
	TLB_CONFLICT		= 0x30;
};

define iss_da_sas enumeration {
	Byte		= 0b00;
	Halfword;
	Word;
	DoubleWord;
};

define spsr_32bit_mode enumeration {
	User		= 0b10000;
	FIQ		= 0b10001;
	IRQ		= 0b10010;
	Supervisor	= 0b10011;
	Abort		= 0b10111;
	Undefined	= 0b11011;
	System		= 0b11111;
};

define iss_wfx_ti enumeration {
	WFI		= 0b00;
	WFE		= 0b01;
};

```

`hyp/core/base/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

interface base
types types.tc
source base.c
arch_source aarch64 core_id.c cache.c
arch_types armv8 enums.tc
arch_types aarch64 types.tc enums.tc sysregs.tc
arch_types cortex-a-v8_2 sysregs_cpu.tc
arch_types cortex-a-v9 sysregs_cpu.tc
template typed hypconstants.h
template typed hypcontainers.h
template typed hypresult.h
template typed accessors.c
template typed hypresult.c
template typed_guestapi hypresult.h
template typed_guestapi accessors.c
template typed_guestapi hypresult.c

```

`hyp/core/base/cortex-a-v8_2/sysregs_cpu.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

extend ACTLR_EL2 bitfield {
	0	ACTLREN		bool;
	1	ECTLREN		bool;
	4	AMEN		bool;
	5	ERXPFGEN	bool;
	7	PWREN		bool;
	11	SMEN		bool;
	12	CLUSTERPMUEN	bool;
};

```

`hyp/core/base/cortex-a-v9/sysregs_cpu.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// This needs to be double-checked.
// FIXME:
extend ACTLR_EL2 bitfield {
	0	ACTLREN		bool;
	1	ECTLREN		bool;
	4	AMEN		bool;
	5	ERXPFGEN	bool;
	7	PWREN		bool;
	11	SMEN		bool;
	12	CLUSTERPMUEN	bool;
};

```

`hyp/core/base/src/base.c`:

```c
// © 2022 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>

// Gunyah compiler assumption sanity checks.
#if ARCH_IS_64BIT
static_assert(SIZE_MAX == UINT64_MAX, "SIZE_MAX smaller than machine width");
#else
#error unsupported
#endif

```

`hyp/core/base/templates/accessors.c.tmpl`:

```tmpl
// Automatically generated. Do not modify.
//
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#if $public_only
\#include <guest_types.h>
#else
\#include <hyptypes.h>
#end if

// Bitfield Accessors

#for $definition in $definitions
#if $definition.category == "bitfield"
#set type_name=$definition.type_name
#set unit_type=$definition.unit_type
#set unit_cnt=$definition.unit_count
#set compare_masks=$definition.compare_masks
#set init_values=$definition.init_values
#set all_fields_boolean=$definition.all_fields_boolean
#set boolean_masks=$definition.boolean_masks
#set trivial = True
#for i in range($unit_cnt)
#if $compare_masks[$i] != 0
#set trivial = False
#end if
#end for

void
${type_name}_init(${type_name}_t *bit_field) {
       *bit_field = ${type_name}_default();
}

#if $unit_cnt == 1
$unit_type
${type_name}_raw(${type_name}_t bit_field) {
	return bit_field.bf[0];
}

_Atomic $unit_type *
${type_name}_atomic_ptr_raw(_Atomic ${type_name}_t *ptr) {
	return (_Atomic $unit_type *)&((${type_name}_t *)ptr)->bf[0];
}

#end if
#set $unit_size_mask = (1 << definition.unit_size) - 1
${type_name}_t
${type_name}_clean(${type_name}_t bit_field) {
#if trivial
       (void)bit_field;
#end if
       return (${type_name}_t) { .bf = { #slurp
#for i in range($unit_cnt)
#if $compare_masks[$i] != 0
#set mask = hex($compare_masks[$i])
#if $init_values[$i] != 0
#set init = hex($init_values[$i])
## Keep any default values, only for `unknown = XX` fields
               // (${init}U & ~${mask}U) |
               ($unit_type)(${hex($init_values[$i] & ($compare_masks[$i] ^ unit_size_mask))}U) |
#end if
               (bit_field.bf[$i] & ${mask}U),
#else
#set init = hex($init_values[$i])
               ${init},
#end if
#end for
       } };
}

bool
${type_name}_is_equal(${type_name}_t b1, ${type_name}_t b2) {
#if trivial
       (void)b1;
       (void)b2;
#end if
       return#slurp
#set sep = ''
#for i in range($unit_cnt)
#if $compare_masks[$i] != 0
#set mask = hex($compare_masks[$i])
       $sep ((b1.bf[$i] & ${mask}U) == (b2.bf[$i] & ${mask}U))
#set sep = '&&'
#end if
#end for
#if trivial
       true
#end if
       ;
}

#if $definition.has_set_ops
#set bool_trivial=True
#for i in range($unit_cnt)
#if $all_fields_boolean
#set bool_trivial = False
#elif $boolean_masks[$i] != 0
#set bool_trivial = False
#end if
#end for

#if $all_fields_boolean
bool
${type_name}_is_empty(${type_name}_t bit_field)
{
return#slurp
#set $first = True
#for $unit in range($unit_cnt)
#if not $first
 &&#slurp
#end if
#set $first = False
 ((bit_field.bf[$unit] & ${hex($boolean_masks[$unit])}U) == 0U)#slurp
#end for
;
}
#end if

#if $all_fields_boolean
bool
${type_name}_is_clean(${type_name}_t bit_field)
{
return#slurp
#set $first = True
#for $unit in range($unit_cnt)
#if not $first
 &&#slurp
#end if
#set $first = False
 ((bit_field.bf[$unit] & ${hex($boolean_masks[$unit] ^ $unit_size_mask)}U) == ${hex($init_values[$i] & ~$compare_masks[$i])}U)#slurp
#end for
;
}
#end if

${type_name}_t
${type_name}_union(${type_name}_t b1, ${type_name}_t b2)
{
#if bool_trivial
       (void)b2;
#end if
       return (${type_name}_t){ .bf = { #slurp
#for i in range($unit_cnt)
#if $all_fields_boolean
               b1.bf[$i] | b2.bf[$i],
#elif $boolean_masks[$i] != 0
#set mask = hex($boolean_masks[$i])
               b1.bf[$i] | (b2.bf[$i] & ${mask}U),
#else
               b1.bf[$i],
#end if
#end for
       } };
}

${type_name}_t
${type_name}_intersection(${type_name}_t b1, ${type_name}_t b2)
{
#if bool_trivial
       (void)b2;
#end if
       return (${type_name}_t){ .bf = { #slurp
#for i in range($unit_cnt)
#if $all_fields_boolean
               b1.bf[$i] & b2.bf[$i],
#elif $boolean_masks[$i] != 0
#set mask = hex($boolean_masks[$i])
               b1.bf[$i] & (b2.bf[$i] | ~($unit_type)${mask}U),
#else
               b1.bf[$i],
#end if
#end for
       } };
}

${type_name}_t
${type_name}_inverse(${type_name}_t b)
{
       return (${type_name}_t){ .bf = { #slurp
#for i in range($unit_cnt)
#if $all_fields_boolean
               ~b.bf[$i],
#elif $boolean_masks[$i] != 0
#set mask = hex($boolean_masks[$i])
               b.bf[$i] ^ ${mask}U,
#else
               b.bf[$i],
#end if
#end for
       } };
}

${type_name}_t
${type_name}_difference(${type_name}_t b1, ${type_name}_t b2)
{
       ${type_name}_t not_b2 = ${type_name}_inverse(b2);
       return ${type_name}_intersection(b1, not_b2);
}

${type_name}_t
${type_name}_atomic_union(_Atomic ${type_name}_t *b1, ${type_name}_t b2, memory_order order)
{
       _Atomic $unit_type *bf = (_Atomic $unit_type *) & ((${type_name}_t *) b1)->bf[0];
       return (${type_name}_t){ .bf = { #slurp
#if $all_fields_boolean
               atomic_fetch_or_explicit(bf, b2.bf[0], order)
#else
               atomic_fetch_or_explicit(bf, b2.bf[0] & ${mask}U, order)
#end if
       } };
}

${type_name}_t
${type_name}_atomic_intersection(_Atomic ${type_name}_t *b1, ${type_name}_t b2, memory_order order)
{
       _Atomic $unit_type *bf = (_Atomic $unit_type *) & ((${type_name}_t *) b1)->bf[0];
       return (${type_name}_t){ .bf = { #slurp
#if $all_fields_boolean
               atomic_fetch_and_explicit(bf, b2.bf[0], order)
#else
               atomic_fetch_and_explicit(bf, b2.bf[0] | ~($unit_type)${mask}U, order)
#end if
       } };
}

${type_name}_t
${type_name}_atomic_difference(_Atomic ${type_name}_t *b1, ${type_name}_t b2, memory_order order)
{
       ${type_name}_t not_b2 = ${type_name}_inverse(b2);
       return ${type_name}_atomic_intersection(b1, not_b2, order);
}

#end if
#for $dec in $definition._all_declarations:
#if not $dec.is_ignore
#set field_type = $dec.compound_type
#set field_type_type_name = $dec.compound_type.gen_type_name(unqualified=True)
#if $dec.is_nested_bitfield
#set field_type_name = $field_type.type_name
#set field_unit_type = $field_type.definition.unit_type
#set val_expr = '(' + $dec.unit_type + ')' + $field_type_name + '_raw(val)'
#else
#set val_expr = '(' + $dec.unit_type + ')val'
#end if
#if not $dec.is_const:
void ${dec.bf_type_name}_set_${dec.field_name}(${dec.bf_type_name}_t *bit_field, ${field_type.gen_declaration('val')}) {
## Handle MISRA Boolean type casting
#if not $dec.is_nested_bitfield
#if ($field_type.basic_type.bitsize == 1) and ($field_type.basic_type.category == 'primitive')
	${dec.unit_type} bool_val = val ? (${dec.unit_type})1 : (${dec.unit_type})0;
#set val_expr = 'bool_val'
#end if
#end if
	${dec.unit_type} *bf = &bit_field->bf[0];
#for $map in $dec.field_maps:
#set $m = (1 << $map.length) - 1
#set unit = $map.mapped_bit // $dec.unit_size
#set mapped_bit = $map.mapped_bit % $dec.unit_size
    bf[$unit] &= (${dec.unit_type})${hex(((2 ** dec.unit_size) - 1) ^ (m << $mapped_bit))}U;
    bf[$unit] |= ((${val_expr} >> ${map.field_bit}U) & (${dec.unit_type})${hex(m)}U) << ${mapped_bit}U;
#end for
}

#end if
#if not $dec.is_writeonly:
${field_type_type_name}
${dec.bf_type_name}_get_${dec.field_name}(const ${dec.bf_type_name}_t *bit_field) {
    ${dec.unit_type} val = 0;
    const ${dec.unit_type} *bf = (const ${dec.unit_type} *)&bit_field->bf[0];
#set $bool_type = False
#if not $dec.is_nested_bitfield
#if ($field_type.basic_type.bitsize == 1) and ($field_type.basic_type.category == 'primitive')
#set $bool_type = True
#end if
#end if

#for $map in $dec.field_maps:
#set $m = (1 << $map.length) - 1
#set unit = $map.mapped_bit // $dec.unit_size
#set mapped_bit = $map.mapped_bit % $dec.unit_size
     val |= ((bf[$unit] >> ${mapped_bit}U) & (${dec.unit_type})${hex(m)}U) << ${map.field_bit}U;
#end for
#if $dec.field_signed
#set l=hex(1 << ($dec.field_length - 1)) + 'U'
    val = (val ^ $l) - $l;
    return (${field_type_type_name})val;
#else
#if not $dec.is_nested_bitfield
#if $bool_type
    return val != (${dec.unit_type})0;
#else if $field_type.is_pointer
    return (${field_type_type_name})(uintptr_t)val;
#else
    return (${field_type_type_name})val;
#end if
#else
    return ${field_type_name}_cast((${field_unit_type})val);
#end if
#end if
}

#end if
#if not $dec.is_writeonly and not $dec.is_const
void ${dec.bf_type_name}_copy_${dec.field_name}(
        ${dec.bf_type_name}_t *bit_field_dst,
        const ${dec.bf_type_name}_t *bit_field_src)
{
    ${dec.unit_type} *bf_dst = (${dec.unit_type} *)&bit_field_dst->bf[0];
    const ${dec.unit_type} *bf_src = (const ${dec.unit_type} *)&bit_field_src->bf[0];
#for $map in $dec.field_maps:
#set mapped_bit = $map.mapped_bit % $dec.unit_size
#set m = hex(((1 << $map.length) - 1) << $mapped_bit)
#set unit = $map.mapped_bit // $dec.unit_size
    bf_dst[$unit] &= ~($dec.unit_type)${m}U;
    bf_dst[$unit] |= bf_src[$unit] & ($dec.unit_type)${m}U;
#end for
}

#end if
#end if
#end for
#end if
#end for

```

`hyp/core/base/templates/hypconstants.h.tmpl`:

```tmpl
// Automatically generated. Do not modify.
//
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// Struct and object member offsets

#def print_offsets(prefix, base, d)
#for $name, $type, $ofs in $d.layout
#set upper_name = $name.upper()
#set offset = base + ofs
#if not $type.is_array
\#define ${prefix}_${upper_name} $offset
#else
\#define ${prefix}_${upper_name}(n) ($offset + ($type.base_type.size * (n)))
#end if
#if $type != $type.basic_type
#set d = getattr($type.basic_type, 'definition', None)
#if $d and $d.is_container
#set p = prefix + '_' + $name.upper()
$print_offsets($p, $offset, $d)
#end if
#end if
#end for
#end def

#for $d in $definitions
#if $d.is_container
#set typename = $d.type_name.upper()
\#define ${typename}_SIZE $d.size
\#define ${typename}_ALIGN $d.alignment
#set prefix = 'OFS_' + typename
$print_offsets(prefix, 0, d)
#end if

#end for



// Enumeration constants

#for $d in $definitions
#if $d.category == "enumeration"
#set typename = $d.type_name.upper()
#for e in d.enumerators
#set name = $e.name.upper()
\#define ENUM_${typename}_${name} ${int($e.value)}
#end for

\#define ENUM_${typename}_MIN_VALUE ($d.minimum_value)
\#define ENUM_${typename}_MAX_VALUE ($d.maximum_value)
#end if

#end for

// Bitfield shifts and masks

#for $d in $definitions
#if $d.category == "bitfield"
#set typename = $d.type_name.upper()
#for dec in d.fields
#set map = $dec.field_maps[0]
#set field_name = $dec.field_name.upper()
#if $dec.field_name != "unknown" and $len($dec.field_maps) == 1
#set $mask = ((1 << $map.length) - 1) << $map.mapped_bit
\#define ${typename}_${field_name}_SHIFT (${map.mapped_bit}U)
\#define ${typename}_${field_name}_BITS (${map.length}U)
\#define ${typename}_${field_name}_MASK (${hex(mask)}U)
#end if
\#define ${typename}_${field_name}_PRESHIFT (${map.field_bit}U)
#end for
#end if

#end for

\#if defined(__ASSEMBLER__)
// Untyped constants (assembly only)

#for $d in $definitions
#if $d.category == "constant"
\#define ${d.type_name} ${int(d.value)}
#end if
#end for

\#endif // defined(__ASSEMBLER__)

```

`hyp/core/base/templates/hypcontainers.h.tmpl`:

```tmpl
// Automatically generated. Do not modify.
//
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// The container_of macros for the tagged types

#for $d in $definitions
#if $d.is_container
#set outer_type = d.type_name + '_t'
#for $name, $type, $offset in $d.layout
#if $type.is_contained
static inline ${outer_type} *
${d.type_name}_container_of_${name}(${type.pointer.gen_declaration('ptr')}) {
	_Static_assert(offsetof(${outer_type}, ${name}) == $offset,
		"Generated offset for ${name} in ${outer_type} is incorrect");
	return ((${outer_type} *)((uintptr_t)(ptr) - $offset));
}
#end if
#end for
#end if

#end for

```

`hyp/core/base/templates/hypresult.c.tmpl`:

```tmpl
// Automatically generated. Do not modify.
//
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#if $public_only
\#include <guest_types.h>
#else
\#include <hyptypes.h>
#end if

// Result Accessors

#def declare_result(name, type_name=None)
#if $type_name is None
#set type_name = name
#end if
#if name.endswith('_t')
#set name = type_name[:-2]
#end if
${name}_result_t
${name}_result_error(error_t err)
{
	return (${name}_result_t){ .e = err };
}

${name}_result_t
${name}_result_ok(${type_name} ret)
{
	return (${name}_result_t){ .r = ret, .e = OK };
}

#end def
#def declare_result_ptr(name, type_name=None)
#if $type_name is None
#set type_name = name
#end if
#if name.endswith('_t')
#set name = type_name[:-2]
#end if
${name}_ptr_result_t
${name}_ptr_result_error(error_t err)
{
	return (${name}_ptr_result_t){ .e = err };
}

${name}_ptr_result_t
${name}_ptr_result_ok(${type_name} * ret)
{
	return (${name}_ptr_result_t){ .r = ret, .e = OK };
}

#end def

#for $d in $definitions
#set type_name = d.gen_type_name()
#if type_name is None
#continue
#end if
#set category = $d.category
#if $category == 'union'
#if $d.size <= 8
#set category = 'primitive'
#end if
#end if
#if $category == 'alternative'
#set basic_type = $d.compound_type.basic_type
#if not $basic_type.is_array and not $basic_type.is_pointer
#set category = $basic_type.category
#end if
#if $d.compound_type.is_atomic
## Skip types with _Atomic qualifiers, to avoid LLVM 12.0.2 bug
#continue
#end if
#end if
#if $category in ['enumeration', 'primitive', 'bitfield', 'structure'] and $d.size != 0
$declare_result(d.indicator, type_name)
$declare_result_ptr(d.indicator, type_name)
#else if $category in ['union', 'object']
$declare_result_ptr(d.indicator, type_name)
#end if

#end for

#for $d in $primitives
$declare_result(d.type_name, d.c_type_name)
#end for

$declare_result_ptr('void', 'void')

```

`hyp/core/base/templates/hypresult.h.tmpl`:

```tmpl
// Automatically generated. Do not modify.
//
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// _result_t type definitions and accessors

#def declare_result(name, type_name=None)
#if $type_name is None
#set type_name = name
#end if
#if name.endswith('_t')
#set name = type_name[:-2]
#end if
typedef struct ${name}_result {
	${type_name} r;
	error_t alignas(register_t) e;
} ${name}_result_t;

${name}_result_t
${name}_result_error(error_t err);
${name}_result_t
${name}_result_ok(${type_name} ret);
#end def
#def declare_result_ptr(name, type_name=None)
#if $type_name is None
#set type_name = name
#end if
#if name.endswith('_t')
#set name = type_name[:-2]
#end if
typedef struct ${name}_ptr_result {
	${type_name} * r;
	error_t alignas(register_t) e;
} ${name}_ptr_result_t;

${name}_ptr_result_t
${name}_ptr_result_error(error_t err);
${name}_ptr_result_t
${name}_ptr_result_ok(${type_name} * ret);
#end def
\#pragma clang diagnostic push
\#pragma clang diagnostic ignored "-Wpadded"

#for $d in $definitions
#set type_name = d.gen_type_name()
#if type_name is None
#continue
#end if
#set category = $d.category
#if $category == 'union'
#if $d.size <= 8
#set category = 'primitive'
#end if
#end if
#if $category == 'alternative'
#set basic_type = $d.compound_type.basic_type
#if not $basic_type.is_array and not $basic_type.is_pointer
#set category = $basic_type.category
#end if
#end if
#if $category in ['enumeration', 'primitive', 'bitfield', 'structure'] and $d.size != 0
$declare_result(d.indicator, type_name)
$declare_result_ptr(d.indicator, type_name)
#else if $category in ['union', 'object']
$declare_result_ptr(d.indicator, type_name)
#end if

#end for

#for $d in $primitives
$declare_result(d.type_name, d.c_type_name)
#end for

$declare_result_ptr('void', 'void')

\#pragma clang diagnostic pop

```

`hyp/core/base/types.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

define error public enumeration(explicit) {
	OK = 0 noprefix;
	UNIMPLEMENTED = -1;
	RETRY = -2;

	// Generic Argument Errors
	ARGUMENT_INVALID = 1;
	ARGUMENT_SIZE = 2;
	ARGUMENT_ALIGNMENT = 3;

	// Generic Allocation Errors
	NOMEM = 10;
	NORESOURCES = 11;

	// Generic Address Errors
	ADDR_OVERFLOW = 20;
	ADDR_UNDERFLOW = 21;
	ADDR_INVALID = 22;

	// Generic Calling Errors
	DENIED = 30;
	BUSY = 31;
	IDLE = 32;
	// 33, 34, 35 - Object API Errors
	FAILURE = 36;

	// Interrupt Interface Errors
	// 40..49

	// CSpace Interface Errors
	// 50..59

	// Message Queue Interface Errors
	// 60..69

	// String and Formatting Errors
	// 90..99

	// 100+ Module Implementation Specific Errors

	// 100s - Memory Allocator
	// 110s - Memory Database
};

define register_t public newtype uregister;
define sregister_t public newtype sregister;

define count_t public newtype uint32;
define index_t public newtype uint32;

define COUNT_INVALID constant type count_t = -1;
define INDEX_INVALID constant type index_t = -1;

// Dummy lockable structure used for thread safety analysis of public lock
// APIs that lock static variables, without having to expose the static
// variable. These are always declared extern; they don't need to be defined.
define opaque_lock structure(lockable) {
	dummy char;
};

```

`hyp/core/boot/aarch64/aarch64_boot.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

module boot

// core initialization

subscribe boot_runtime_first_init
	handler aarch64_handle_boot_runtime_init()
	priority 1
subscribe boot_runtime_warm_init
	handler aarch64_handle_boot_runtime_init()
	priority 1

```

`hyp/core/boot/aarch64/include/arch/reloc.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#define ARCH_CAN_PATCH(r_info)                                                 \
	(((R_TYPE(r_info) == R_AARCH64_NONE) ||                                \
	  (R_TYPE(r_info) == R_AARCH64_NULL) ||                                \
	  (R_TYPE(r_info) == R_AARCH64_RELATIVE)) &&                           \
	 (R_SYM(r_info) == 0U))

```

`hyp/core/boot/aarch64/src/aarch64_boot.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <hyptypes.h>

#include <hypregisters.h>

#include <asm/barrier.h>

#include "event_handlers.h"

void
aarch64_handle_boot_runtime_init(void)
{
#if defined(ARCH_ARM_FEAT_VHE)
	CPTR_EL2_E2H1_t cptr = CPTR_EL2_E2H1_default();
	register_CPTR_EL2_E2H1_write_ordered(cptr, &asm_ordering);
#else
	CPTR_EL2_E2H0_t cptr = CPTR_EL2_E2H0_default();
	register_CPTR_EL2_E2H0_write_ordered(cptr, &asm_ordering);
#endif
}

```

`hyp/core/boot/aarch64/src/init_el2.S`:

```S
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <hypconstants.h>

#include <asm/asm_defs.inc>

#include "vectors_el2.inc"

// This stack is used by boot_cold_init. It must be large enough to run
// all the first-boot event triggers. This memory is then reclaimed by
// the hypervisor_partition.
	_bss aarch64_boot_stack, BOOT_STACK_SIZE, 16
	.space BOOT_STACK_SIZE

#if defined(ARCH_ARM_FEAT_PAuth)
// Pointer authentication keys shared by all EL2 threads.
	_bss aarch64_pauth_keys, AARCH64_PAUTH_KEYS_SIZE, \
		AARCH64_PAUTH_KEYS_ALIGN
	.space AARCH64_PAUTH_KEYS_SIZE
#endif

#if !defined(__ARM_FEATURE_UNALIGNED)
#define SCTLR_EL2_BOOT_DEBUG SCTLR_EL2_VM_A_MASK
#else
#define SCTLR_EL2_BOOT_DEBUG 0
#endif
#define SCTLR_EL2_BOOT ( \
		SCTLR_EL2_BOOT_DEBUG | \
		SCTLR_EL2_VM_I_MASK | \
		SCTLR_EL2_VM_WXN_MASK | \
		SCTLR_EL2_VM_SA_MASK)

// Hypervisor's 256-bit PRNG value
	_data hypervisor_prng_seed, 32, 8, "global"
	.space 32

// Initialize the EL2 Environment
//
// This is called by the platform boot code on the first power-on of the boot
// CPU, with MMU and caches disabled.
//
// Input arguments:
//    x0-x3: 256-bit RNG seed from the platform code
//    w4: Logical CPU number from the platform code

	.section .text.boot.init
function aarch64_init, section=nosection
	// Disable debug, aborts and interrupts.
	msr	DAIFSet, 0xf

	// Save the seed
	adrl	x20, hypervisor_prng_seed
	stp	x0, x1, [x20]
	stp	x2, x3, [x20, 16]

	// Set the boot SCTLR
	abs64	x9, SCTLR_EL2_BOOT
	msr	SCTLR_EL2, x9
	isb

	// Save logical CPU number in w20 (call-preserved)
	mov	w20, w4

	// Set terminating frame pointer
	mov	x29, 0

	// Set physical boot stack
	adrl	x10, aarch64_boot_stack + BOOT_STACK_SIZE
	mov	sp, x10

	// Initialize the KASLR and return base virtual address
	bl	aarch64_init_kaslr
	// x0 return has hypervisor KASLR base address

	bic	x2, x0, 0x1fffff	// relocation offset
	adrl	x0, _DYNAMIC		// get address of _DYNAMIC
	bic	x1, x0, 0x1fffff	// current address offset

	// Apply ELF relocations
	bl	boot_rel_fixup

	// Initialize and enable the MMU
	mov	x2, 1
	bl	aarch64_init_address_space

	// w20 - contains logical cpu number
	//      (contiguous numbering: 0 .. N-1)
	cmp	w20, PLATFORM_MAX_CORES
	bge	aarch64_boot_error

	// Set virtual boot stack
	msr	SPSel, 1
	adrl	x10, aarch64_boot_stack + BOOT_STACK_SIZE
	mov	sp, x10

	// Set an invalid thread pointer address
	mov	x22, (1 << 63) - 1
	msr	TPIDR_EL2, x22

	// Runtime init sets TPIDR_EL2 and therefore must be called from
	// assembly to separate it from any code that might access
	// _Thread_local variables.
	mov	w0, w20
	bl	trigger_boot_runtime_first_init_event

	// Assert that the TPIDR_EL2 has been initialized
	mrs	x8, TPIDR_EL2
	cmp	x8, x22
	beq	aarch64_boot_error

#if defined(ARCH_ARM_FEAT_PAuth)
	// Generate the PAC keys
	adrl	x21, aarch64_pauth_keys
	mov	x23, AARCH64_PAUTH_KEYS_SIZE
1:
	bl	prng_get64
	sub	x23, x23, 8
	str	x0, [x21], 8
	cbnz	x23, 1b

	// Load the PAC keys into registers
	kernel_pauth_entry x0, x1, x2

	// Enable PAC
	mrs	x9, SCTLR_EL2
	orr	x9, x9, (SCTLR_EL2_VM_ENIA_MASK | SCTLR_EL2_VM_ENIB_MASK)
	orr	x9, x9, SCTLR_EL2_VM_ENDA_MASK
	orr	x9, x9, SCTLR_EL2_VM_ENDB_MASK
	msr	SCTLR_EL2, x9
#endif
	// Ensure changes from runtime init and PAC enable take effect
	isb

	mov	w0, w20
	b	boot_cold_init
function_end aarch64_init

// Cold-boot a secondary CPU
//
// This is called by the platform boot code on the first power-on of any CPU
// other than the boot CPU, with MMU and caches disabled.
//
// Input arguments:
//    x0: Virtual pointer to the CPU's idle thread
//    w1: Logical CPU number from the platform code

function aarch64_secondary_init, section=nosection
	// Disable debug, aborts and interrupts.
	msr	DAIFSet, 0xf

	// Set the boot SCTLR
	abs64	x9, SCTLR_EL2_BOOT
	msr	SCTLR_EL2, x9
	isb

	// Save arguments in callee-saved registers
	mov	x19, x0
	mov	x20, x1

	// Set terminating frame pointer
	mov	x29, 0

#if !defined(NDEBUG)
	// Misalign the stack pointer to force a fault if it is used
	mov	sp, 1
#endif

	// Initialize and enable the MMU
	mov	x2, 0
	bl	aarch64_init_address_space

	// w20 - contains logical cpu number
	//      (contiguous numbering: 0 .. N-1)
	cmp	w20, PLATFORM_MAX_CORES
	bge	aarch64_boot_error

	// Load the idle thread's stack pointer
	ldr	x0, [x19, OFS_THREAD_CONTEXT_SP]
	msr	SPSel, 1
	mov	sp, x0

	// Set an invalid thread pointer address
	mov	x22, (1 << 63) - 1
	msr	TPIDR_EL2, x22

	// Runtime init sets TPIDR_EL2 and therefore must be called from
	// assembly to separate it from any code that might access
	// _Thread_local variables.
	mov	x0, x19
	bl	trigger_boot_runtime_warm_init_event

	// Assert that the TPIDR_EL2 has been initialized
	mrs	x8, TPIDR_EL2
	cmp	x8, x22
	beq	aarch64_boot_error

#if defined(ARCH_ARM_FEAT_PAuth)
	// Load the PAC keys into registers
	kernel_pauth_entry x0, x1, x2

	// Enable PAC
	mrs	x9, SCTLR_EL2
	orr	x9, x9, (SCTLR_EL2_VM_ENIA_MASK | SCTLR_EL2_VM_ENIB_MASK)
	orr	x9, x9, SCTLR_EL2_VM_ENDA_MASK
	orr	x9, x9, SCTLR_EL2_VM_ENDB_MASK
	msr	SCTLR_EL2, x9
#endif
	// Ensure changes from runtime init and PAC enable take effect
	isb

	// Start triggering the secondary cold boot events
	mov	w0, w20
	b	boot_secondary_init
function_end aarch64_secondary_init

// Warm-boot a CPU
//
// This is called by the platform boot code on a resume from power-off suspend
// or restart after hotplug of any CPU, with MMU and caches disabled.
//
// Input arguments:
//    x0: Virtual pointer to the CPU's idle thread.

function aarch64_warm_init, section=nosection
	// Disable debug, aborts and interrupts.
	msr	DAIFSet, 0xf

	// Set the boot SCTLR
	abs64	x9, SCTLR_EL2_BOOT
	msr	SCTLR_EL2, x9
	isb

	// Save arguments in callee-saved registers
	mov	x19, x0

	// Set terminating frame pointer
	mov	x29, 0

#if !defined(NDEBUG)
	// Misalign the stack pointer to force a fault if it is used
	mov	sp, 1
#endif

	// Initialize and enable the MMU
	mov	x2, 0
	bl	aarch64_init_address_space

	// Load the idle thread's stack pointer
	ldr	x0, [x19, OFS_THREAD_CONTEXT_SP]
	msr	SPSel, 1
	mov	sp, x0

	// Set an invalid thread pointer address
	mov	x22, (1 << 63) - 1
	msr	TPIDR_EL2, x22

	// Runtime init sets TPIDR_EL2 and therefore must be called from
	// assembly to separate it from any code that might access
	// _Thread_local variables.
	mov	x0, x19
	bl	trigger_boot_runtime_warm_init_event

	// Assert that the TPIDR_EL2 has been initialized
	mrs	x8, TPIDR_EL2
	cmp	x8, x22
	beq	aarch64_boot_error

#if defined(ARCH_ARM_FEAT_PAuth)
	// Load the PAC keys into registers
	kernel_pauth_entry x0, x1, x2

	// Enable PAC
	mrs	x9, SCTLR_EL2
	orr	x9, x9, (SCTLR_EL2_VM_ENIA_MASK | SCTLR_EL2_VM_ENIB_MASK)
	orr	x9, x9, SCTLR_EL2_VM_ENDA_MASK
	orr	x9, x9, SCTLR_EL2_VM_ENDB_MASK
	msr	SCTLR_EL2, x9
#endif
	// Ensure changes from runtime init and PAC enable take effect
	isb

	// Start triggering the warm boot events
	b	boot_warm_init
function_end aarch64_warm_init

function aarch64_boot_error, section=nosection
9:
	wfi
	b	9b
function_end aarch64_boot_error

	// Boot information which is passed from the linker
const64 thread_size, tbss_size
const64 thread_align, tbss_align

```

`hyp/core/boot/aarch64/src/init_el2_mmu.S`:

```S
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <hypconstants.h>

#include <asm/asm_defs.inc>

bss64 aarch64_kaslr_base, local
bss64 hypervisor_prng_nonce

	.section .text.boot.init

function aarch64_init_generate_seed, local, section=nosection
	// KASLR - Seed is passed in x0-x3, however, for portability it may not
	// be random. Mix in some values from registers that are UNKNOWN after
	// reset: TPIDR_EL2, TTBR0_EL2, MAIR_EL2, VBAR_EL2 and FAR_EL2.
	abs64	x12, 0xb3b2c4c2400caad3L	// Large Prime
	abs64	x11, 0x4bccfdf1			// Random Constant
	mrs	x14, TPIDR_EL2
	crc32cx	w11, w11, x14
	mrs	x15, TTBR0_EL2
	crc32cx	w11, w11, x15
	mov	x13, x11
	mrs	x16, MAIR_EL2
	crc32cx	w11, w11, x16
	mrs	x17, VBAR_EL2
	crc32cx	w11, w11, x17
	mrs	x18, FAR_EL2
	crc32cx	w11, w11, x18
	eor	x13, x13, x11, ROR 32

	mul	x10, x14, x12
	rbit	x10, x10
	eor	x10, x10, x12
	mul	x10, x15, x10
	rbit	x10, x10
	eor	x10, x10, x12
	mul	x10, x16, x10
	rbit	x10, x10
	eor	x10, x10, x12
	mul	x10, x17, x10
	rbit	x10, x10
	eor	x10, x10, x12
	mul	x10, x18, x10

	// We have no persistent storage, so use chip UNKNOWN value as nonce
	adrp	x12, hypervisor_prng_nonce
	str	x13, [x12, :lo12:hypervisor_prng_nonce]

	eor	x10, x10, x10, ROR 32
	// Mix in with the platform provided seed.
	// FIXME: this should use a real hash function
	eor	x0, x0, x10
	eor	x0, x0, x11
	eor	x0, x0, x1
	eor	x0, x0, x2
	eor	x0, x0, x3
	ret
function_end aarch64_init_generate_seed

#define LOWER_ATTRS VMSA_PAGE_ENTRY_LOWER_ATTRS_SHIFT
#define UPPER_ATTRS VMSA_PAGE_ENTRY_UPPER_ATTRS_SHIFT

#define VSMAv8_LEVEL_BITS 9
#define VSMAv8_ADDRESS_BITS_LEVEL0                                             \
	(VSMAv8_ADDRESS_BITS_LEVEL1 + VSMAv8_LEVEL_BITS)
#define VSMAv8_ADDRESS_BITS_LEVEL1                                             \
	(VSMAv8_ADDRESS_BITS_LEVEL2 + VSMAv8_LEVEL_BITS)
#define VSMAv8_ADDRESS_BITS_LEVEL2                                             \
	(VSMAv8_ADDRESS_BITS_LEVEL3 + VSMAv8_LEVEL_BITS)
#define VSMAv8_ADDRESS_BITS_LEVEL3 12
#define VSMAv8_ENTRY_BITS 3
#define VSMAv8_ENTRY_SIZE (1 << VSMAv8_ENTRY_BITS)

#define VSMAv8_TABLE_TYPE 3
#define VSMAv8_BLOCK_TYPE 1

#define VSMAv8_TABLE_REFCOUNT_SHIFT VMSA_TABLE_ENTRY_REFCOUNT_SHIFT

#define VSMAv8_AP_SHIFT (LOWER_ATTRS + VMSA_STG1_LOWER_ATTRS_AP_SHIFT)
#define VSMAv8_AP_RW_EL0_NONE ENUM_VMSA_STG1_AP_EL0_NONE_UPPER_READ_WRITE
#define VSMAv8_AP_RW_EL0_RW ENUM_VMSA_STG1_AP_ALL_READ_WRITE
#define VSMAv8_AP_RO_EL0_NONE ENUM_VMSA_STG1_AP_EL0_NONE_UPPER_READ_ONLY
#define VSMAv8_AP_RO_EL0_RO ENUM_VMSA_STG1_AP_ALL_READ_ONLY

#define VSMAv8_SH_SHIFT (LOWER_ATTRS + VMSA_STG1_LOWER_ATTRS_SH_SHIFT)
#define VSMAv8_SH_NON_SHAREABLE ENUM_VMSA_SHAREABILITY_NON_SHAREABLE
#define VSMAv8_SH_OUTER ENUM_VMSA_SHAREABILITY_OUTER_SHAREABLE
#define VSMAv8_SH_INNER ENUM_VMSA_SHAREABILITY_INNER_SHAREABLE

#define VSMAv8_ATTRIDX_SHIFT (LOWER_ATTRS + VMSA_STG1_LOWER_ATTRS_ATTR_IDX_SHIFT)
#define VSMAv8_AF_SHIFT (LOWER_ATTRS + VMSA_STG1_LOWER_ATTRS_AF_SHIFT)
#define VSMAv8_NG_SHIFT (LOWER_ATTRS + VMSA_STG1_LOWER_ATTRS_NG_SHIFT)

#if defined(ARCH_ARM_FEAT_VHE)
#define VSMAv8_UXN_SHIFT (UPPER_ATTRS + VMSA_STG1_UPPER_ATTRS_UXN_SHIFT)
#define VSMAv8_PXN_SHIFT (UPPER_ATTRS + VMSA_STG1_UPPER_ATTRS_PXN_SHIFT)
#else
#define VSMAv8_XN_SHIFT (UPPER_ATTRS + VMSA_STG1_UPPER_ATTRS_XN_SHIFT)
#endif

#if defined(ARCH_ARM_FEAT_BTI)
#define VSMAv8_GP_SHIFT (UPPER_ATTRS + VMSA_STG1_UPPER_ATTRS_GP_SHIFT)
#endif

#define MAIR_DEFAULTS	0x0004080cbb4fff44
// 63:56 - Attr7 - 0x00 - Device-nGnRnE
// 55:48 - Attr6 - 0x04 - Device-nGnRE
// 47:40 - Attr5 - 0x08 - Device-nGRE
// 39:32 - Attr4 - 0x0c - Device-GRE
// 31:24 - Attr3 - 0xBB - Normal inner/outer WT/RA/WA
// 23:16 - Attr2 - 0x4F - Outer NC, Inner WB/RA/WA
// 15:8  - Attr1 - 0xFF - Normal inner/outer WB/RA/WA
// 7:0   - Attr0 - 0x44 - Normal inner/outer non-cachable.
#define MAIR_ATTRIDX_NORMAL 1

#define LOWER_ATTRIBUTES_HYP (\
		(MAIR_ATTRIDX_NORMAL << VSMAv8_ATTRIDX_SHIFT) | \
		(VSMAv8_SH_INNER << VSMAv8_SH_SHIFT) | \
		(1 << VSMAv8_AF_SHIFT) | \
		(0 << VSMAv8_NG_SHIFT))
#define LOWER_ATTRIBUTES_HYP_R (\
		LOWER_ATTRIBUTES_HYP | \
		(VSMAv8_AP_RO_EL0_NONE << VSMAv8_AP_SHIFT))
#define LOWER_ATTRIBUTES_HYP_RW ( \
		LOWER_ATTRIBUTES_HYP | \
		(VSMAv8_AP_RW_EL0_NONE << VSMAv8_AP_SHIFT))
#define UPPER_ATTRIBUTES_HYP_X \
	(0)
#define UPPER_ATTRIBUTES_HYP_NX \
	(1 << VSMAv8_PXN_SHIFT)
// (DBM << 51) | (CONTIG << 52) | (PXN << 53) | (UXN << 54)

#if defined(ARCH_ARM_FEAT_BTI)
#define GUARDED_PAGE (1 << VSMAv8_GP_SHIFT)
#endif

#define TCR_RGN ENUM_TCR_RGN_NORMAL_WRITEBACK_RA_WA
#define TCR_SH ENUM_TCR_SH_INNER_SHAREABLE
#define TCR_TG0 ENUM_TCR_TG0_GRANULE_SIZE_4KB
#define TCR_TG1 ENUM_TCR_TG1_GRANULE_SIZE_4KB

// Note: the nested macros are are needed to expand the config to a number
// before it is pasted into the enum macro name.
#define TCR_PS TCR_PS_FOR_CONFIG(PLATFORM_PHYS_ADDRESS_BITS)
#define TCR_PS_FOR_CONFIG(x) TCR_PS_FOR_SIZE(x)
#define TCR_PS_FOR_SIZE(x) ENUM_TCR_PS_SIZE_##x##BITS

#if defined(ARCH_ARM_FEAT_VHE)
#define TCR_EL2_HYP ( \
	TCR_EL2_E2H1_EPD0_MASK | \
	(HYP_T1SZ << TCR_EL2_E2H1_T1SZ_SHIFT) | \
	(0 << TCR_EL2_E2H1_A1_SHIFT) | \
	(TCR_RGN << TCR_EL2_E2H1_IRGN1_SHIFT) | \
	(TCR_RGN << TCR_EL2_E2H1_ORGN1_SHIFT) | \
	(TCR_SH << TCR_EL2_E2H1_SH1_SHIFT) | \
	(TCR_TG1 << TCR_EL2_E2H1_TG1_SHIFT) | \
	(TCR_PS << TCR_EL2_E2H1_IPS_SHIFT))
#else
#define TCR_EL2_HYP ( \
	(HYP_T0SZ << TCR_EL2_E2H0_T0SZ_SHIFT) | \
	(TCR_RGN << TCR_EL2_E2H0_IRGN0_SHIFT) | \
	(TCR_RGN << TCR_EL2_E2H0_ORGN0_SHIFT) | \
	(TCR_SH << TCR_EL2_E2H0_SH0_SHIFT) | \
	(TCR_TG0 << TCR_EL2_E2H0_TG0_SHIFT) | \
	(TCR_PS << TCR_EL2_E2H0_PS_SHIFT))
#endif

#if defined(__ARM_FEATURE_UNALIGNED)
#define SCTLR_EL2_VM_HYP_DEBUG 0
#else
// FIXME:
//#define SCTLR_EL2_VM_HYP_DEBUG SCTLR_EL2_VM_A_MASK
#define SCTLR_EL2_VM_HYP_DEBUG 0
#endif
#define SCTLR_EL2_VM_HYP ( \
		SCTLR_EL2_VM_HYP_DEBUG | \
		SCTLR_EL2_VM_M_MASK | \
		SCTLR_EL2_VM_C_MASK | \
		SCTLR_EL2_VM_SA_MASK | \
		SCTLR_EL2_VM_I_MASK | \
		SCTLR_EL2_VM_WXN_MASK)

#if defined(ARCH_ARM_FEAT_VHE)
// Enable EL2 E2H (hosted hypervisor) mode
#define HCR_EL2_HYP ( \
    HCR_EL2_FMO_MASK | HCR_EL2_IMO_MASK | HCR_EL2_AMO_MASK | \
    HCR_EL2_TGE_MASK | HCR_EL2_E2H_MASK)

// Kernel main pagetable in TTBR1_EL2
// 39 bit address space (3-level) for KASLR
// HYP_T1SZ = 64 - 39 = 25
#define HYP_T1SZ	(64 - HYP_ASPACE_HIGH_BITS)
#define HYP_LEVEL1_ALIGN	(37 - HYP_T1SZ)
#define HYP_LEVEL1_ENTRIES	(1U << (HYP_ASPACE_HIGH_BITS - 30))

#else
#define HCR_EL2_HYP ( \
    HCR_EL2_FMO_MASK | HCR_EL2_IMO_MASK | HCR_EL2_AMO_MASK | \
    HCR_EL2_TGE_MASK)

// Kernel main pagetable in TTBR0_EL2
// 39 bit address space (3-level) for KASLR
// HYP_T0SZ = 64 - 39 = 25
#define HYP_T0SZ	(64 - HYP_ASPACE_LOW_BITS)
#define HYP_LEVEL1_ALIGN	(40 - HYP_T0SZ)
#define HYP_LEVEL1_ENTRIES	(1U << (HYP_ASPACE_LOW_BITS - 30))
#endif

#define BITS_2MiB	21
#define BITS_1GiB	30
#define SIZE_2MiB	(1 << BITS_2MiB)
#define SIZE_1GiB	(1 << BITS_1GiB)
#define MASK_2MiB	(SIZE_2MiB - 1)
#define MASK_1GiB	(SIZE_1GiB - 1)


// Initialize the KASLR address
//
// This is called early in cold boot with MMU and data cache disabled. The
// stack is valid.
//
// Input / preserve registers:
//    x0-x3: KASLR seed from the platform code
//    x19-x28: preserved as per AAPCS64
// Returns:
//    x0: KASLR base
function aarch64_init_kaslr, section=nosection
	stp	x29, x30, [sp, -16]!
	mov	x29, sp

	bl	aarch64_init_generate_seed

	// To test pathological seed, define this below
#if defined(DISABLE_KASLR) && DISABLE_KASLR
	mov	x0, 0
#endif

	// Calculate KASLR virtual base address
local mm_calc_virt_base:
#if defined(ARCH_ARM_FEAT_VHE)
	mov	x10, 0xffffffffffe00000
	ubfiz	x9, x0, BITS_2MiB, (HYP_ASPACE_HIGH_BITS - BITS_2MiB)
	eor	x14, x10, x9

	// - ensure that the hypervisor image doesn't wrap
	adr	x10, image_virt_start
	bic	x10, x10, MASK_2MiB
	adrl	x11, image_virt_end
	sub	x10, x11, x10
	adds	x10, x10, x14
	bcc	1f

	// -- virtual address wraps or ends at -1, try again
	lsr	x0, x0, 18
	movk	x0, 0x5555, lsl 48
	b	LOCAL(mm_calc_virt_base)
#else
	// - KASLR base needs to be in the lower half of the the address space,
	// because the upper half is reserved for physaccess
	ubfiz	x9, x0, BITS_2MiB, (HYP_ASPACE_LOWER_HALF_BITS - BITS_2MiB)
	// Ensure the KASLR base is outside of the 1:1 area
	mov	x10, (1 << HYP_ASPACE_MAP_DIRECT_BITS)
	orr	x14, x10, x9

	// - ensure that the hypervisor image doesn't go out of the lower half of
	// the address space
	adr	x10, image_virt_start
	adrl	x11, image_virt_last
	sub	x10, x11, x10
	add	x10, x10, x14
	mov	x9, (1 << HYP_ASPACE_LOWER_HALF_BITS)
	cmp	x9, x10
	bhi	1f

	// -- virtual address overlaps the upper half, try again
	lsr	x0, x0, (HYP_ASPACE_LOWER_HALF_BITS - BITS_2MiB)
	movk	x0, 0x5555, lsl 32
	b	LOCAL(mm_calc_virt_base)
#endif

1:
	adrp	x9, aarch64_kaslr_base
	str	x14, [x9, :lo12:aarch64_kaslr_base]
	mov	x0, x14

	ldp	x29, x30, [sp], 16
	ret
function_end aarch64_init_kaslr

// Initialize the EL2 MMU.
//
// This is called early in boot with MMU and data cache disabled. The stack
// pointer may be invalid and should not be accessed.
//
// Input / preserve registers:
//    w2: bool first_boot
//    x30: Physical address of return (to be translated to virtual address)
//    x19-x28: preserved as per AAPCS64
function aarch64_init_address_space, section=nosection
	// Ensure TLB-EL2 is clean, dsb and isb deferred until before mmu enable
	tlbi	alle2

	abs64   x9, HCR_EL2_HYP
	msr	HCR_EL2, x9
	isb

	// === Setup translation registers ===
	// - setup TCR_EL2
	abs64	x28, TCR_EL2_HYP
	msr	TCR_EL2, x28
	// -- preserve x28 for use below

	// - setup MAIR_EL2
	abs64	x9, MAIR_DEFAULTS
	msr	MAIR_EL2, x9

	// - setup TTBRx_EL2 (hypervisor code / data)
	adrl	x12, aarch64_pt_ttbr_level1	// physical addresses of EL2 PT
#if defined(ARCH_ARM_FEAT_VHE)
	orr	x9, x12, TTBR1_EL2_CNP_MASK
	// -- preserve x12 for use below
	msr	TTBR1_EL2, x9
#else
	orr	x9, x12, TTBR0_EL2_CNP_MASK
	// -- preserve x12 for use below
	msr	TTBR0_EL2, x9
#endif
	isb

	// Load the KASLR base address
	adrp	x9, aarch64_kaslr_base
	ldr	x14, [x9, :lo12:aarch64_kaslr_base]

	// - calculate virtual return address
	and	x30, x30, MASK_2MiB
	add	x30, x30, x14

	// - calculate virtual address of vectors_boot_aarch64
	adrl	x9, vectors_boot_aarch64
	and	x9, x9, MASK_2MiB
	add	x9, x9, x14

	// - set the MMU-init vectors to: vectors_boot_aarch64
	// -- on enabling the MMU, the resulting prefetch abort will jump there
	msr	VBAR_EL2, x9

	// Skip the PT setup if this is not the first boot
	cbz	w2, LOCAL(aarch64_init_address_space_warm)

	// === Setup the TTBR1_EL2 mappings for the hypervisor
	// - create hypervisor text/ro 2MB mapping
	adrl	x13, aarch64_pt_ttbr_level2
	// -- calculate 2MB table entries
#if defined(ARCH_ARM_FEAT_VHE)
	ubfx	x10, x9, VSMAv8_ADDRESS_BITS_LEVEL1, HYP_ASPACE_HIGH_BITS - VSMAv8_ADDRESS_BITS_LEVEL1
#else
	ubfx	x10, x9, VSMAv8_ADDRESS_BITS_LEVEL1, HYP_ASPACE_LOW_BITS - VSMAv8_ADDRESS_BITS_LEVEL1
#endif
	add	x10, x12, x10, lsl VSMAv8_ENTRY_BITS	// x10 points to Level-1 table entry
	ubfx	x11, x9, VSMAv8_ADDRESS_BITS_LEVEL2, VSMAv8_LEVEL_BITS
	add	x11, x13, x11, lsl VSMAv8_ENTRY_BITS	// x11 points to Level-2 table entry
	// -- construct Level-1 table entry
	// Set initial entry count in the level below. Assume there's two entries.
	// Update with the actual count of entries used.
	mov	x9, (VSMAv8_TABLE_TYPE | (2 << VSMAv8_TABLE_REFCOUNT_SHIFT))
	orr	x9, x9, x13
	str	x9, [x10]
	// -- construct Level-2 2MB block entry - for hypervisor code
	adr	x12, image_virt_start		// calculate the hypervisor text physical base
	bic	x12, x12, MASK_2MiB		//
	orr	x9, x12, VSMAv8_BLOCK_TYPE
	movz	x10, LOWER_ATTRIBUTES_HYP_R
#if defined(ARCH_ARM_FEAT_BTI)
	movk	x10, ((UPPER_ATTRIBUTES_HYP_X | GUARDED_PAGE)  >> 48), LSL 48
#else
	movk	x10, (UPPER_ATTRIBUTES_HYP_X >> 48), LSL 48
#endif
	orr	x9, x9, x10
	str	x9, [x11]

	// - create hypervisor RW 2MB mapping
	adrp	x9, data_base			// calculate the hypervisor data physical base
	bic	x9, x9, MASK_2MiB		//
	add	x10, x9, x14
	sub	x15, x10, x12

	cmp	x15, x14
	b.eq	LOCAL(mm_init_failure)		// if RO and RW are on the same 2MiB page, fail
	lsr	x10, x15, BITS_1GiB
	cmp	x10, x14, lsr BITS_1GiB		// test whether RO and RW share the same level2 table
	beq	LOCAL(mm_init_rw_level2)

	// -- RW section falls on the next 1GiB page, so we need a new table
	//    entry and use the second level2 page-table
	adrl	x12, aarch64_pt_ttbr_level1
	add	x13, x13, (1 << (VSMAv8_LEVEL_BITS + VSMAv8_ENTRY_BITS))
	// -- Update entry count for first level 1 entry
#if defined(ARCH_ARM_FEAT_VHE)
	ubfx	x10, x14, VSMAv8_ADDRESS_BITS_LEVEL1, HYP_ASPACE_HIGH_BITS - VSMAv8_ADDRESS_BITS_LEVEL1
#else
	ubfx	x10, x14, VSMAv8_ADDRESS_BITS_LEVEL1, HYP_ASPACE_LOW_BITS - VSMAv8_ADDRESS_BITS_LEVEL1
#endif
	add	x10, x12, x10, lsl VSMAv8_ENTRY_BITS	// x10 points to Level-1 table entry
	ldr	x11, [x10]
	sub	x11, x11, (1 << VSMAv8_TABLE_REFCOUNT_SHIFT)
	str	x11, [x10]

	// -- calculate 2MB hyp table entries
#if defined(ARCH_ARM_FEAT_VHE)
	ubfx	x10, x15, VSMAv8_ADDRESS_BITS_LEVEL1, HYP_ASPACE_HIGH_BITS - VSMAv8_ADDRESS_BITS_LEVEL1
#else
	ubfx	x10, x15, VSMAv8_ADDRESS_BITS_LEVEL1, HYP_ASPACE_LOW_BITS - VSMAv8_ADDRESS_BITS_LEVEL1
#endif
	add	x10, x12, x10, lsl VSMAv8_ENTRY_BITS	// x10 points to Level-1 table entry
	ubfx	x11, x15, VSMAv8_ADDRESS_BITS_LEVEL2, VSMAv8_LEVEL_BITS
	add	x11, x13, x11, lsl VSMAv8_ENTRY_BITS	// x11 points to Level-2 table entry
	// -- construct Level-1 table entry
	mov	x12, (VSMAv8_TABLE_TYPE | (1 << VSMAv8_TABLE_REFCOUNT_SHIFT))
	orr	x12, x12, x13
	str	x12, [x10]

local mm_init_rw_level2:
	ubfx	x11, x15, VSMAv8_ADDRESS_BITS_LEVEL2, VSMAv8_LEVEL_BITS
	add	x11, x13, x11, lsl VSMAv8_ENTRY_BITS	// x11 points to Level-2 table entry
	orr	x9, x9, VSMAv8_BLOCK_TYPE
	movz	x10, LOWER_ATTRIBUTES_HYP_RW
#if defined(ARCH_ARM_FEAT_VHE)
	movk	x10, (UPPER_ATTRIBUTES_HYP_NX >> 48), LSL 48
#endif
	orr	x9, x9, x10
	str	x9, [x11]

local aarch64_init_address_space_warm:
	// Ensure we take the normal vector in the Data Abort
	msr	SPSel, 0

	// Flush outstanding stores and synchronize outstanding operations
	// including system register updates and TLB flushing.
	dsb	nsh
	isb

	// Setup SCTLR and Enable MMU
	abs64	x9, SCTLR_EL2_VM_HYP
	msr	SCTLR_EL2, x9
	// MMU is enabled, still 1:1
	// NOTE: We expect to fault here, and jump to the vector!
	isb

1:	// SHOULD NOT GET HERE!
	b	1b

local mm_init_failure:
1:
	wfi
	b	1b
function_end aarch64_init_address_space


.macro boot_vector name offset
	. = vectors_boot_aarch64 + \offset
vector_aarch64_\name\():
.endm


	.section .text.boot_vectors

	// Alignment for Boot Vectors
	.balign 2048

	// Vectors for booting EL2
	// w2 = bool first_boot
	// x30 = virtual return address
vectors_boot_aarch64:

boot_vector self_sync_mmu_init 0x0
	// For the first boot we must use the emergency vectors
	adr	x9, kernel_vectors_aarch64
	adr	x10, emergency_vectors_aarch64
	cmp	w2, 0
	csel	x9, x9, x10, eq
	msr	VBAR_EL2, x9
	isb
	msr	DAIFclr, 4
	ret	x30


// Hypervisor EL2 pagetables:

	.section .bss.hyp_pt.level1, "aw", @nobits
	.p2align HYP_LEVEL1_ALIGN
	.global aarch64_pt_ttbr_level1
aarch64_pt_ttbr_level1:
#if defined(ARCH_ARM_FEAT_VHE)
	.global aarch64_pt_ttbr1_level1
aarch64_pt_ttbr1_level1:
#else
	.global aarch64_pt_ttbr0_level1
aarch64_pt_ttbr0_level1:
#endif
	.space	HYP_LEVEL1_ENTRIES * 8

	// We have two 4KB level2 tables here, to handle the case where the
	// code/ro and rw data mappings are across a 1GiB boundary (separate
	// level 1 entries).
	.section .bss.hyp.pt.level2, "aw", @nobits
	.balign 4096
aarch64_pt_ttbr_level2:
	.space (1 << (VSMAv8_LEVEL_BITS + VSMAv8_ENTRY_BITS)) * 2

```

`hyp/core/boot/boot.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

module boot

subscribe boot_cold_init()
	priority 1000

subscribe idle_start()
	priority first

```

`hyp/core/boot/boot.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

define BOOT_ENV_RANGES_NUM public constant = 32;

define boot_env_phys_range public structure {
	base type paddr_t;
	size size;
};

define hyp_env_data structure {
};

```

`hyp/core/boot/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

base_module hyp/core/vectors
interface boot
types boot.tc
events boot.ev
local_include
arch_local_include aarch64
source boot.c
source rel_init.c
arch_events aarch64 aarch64_boot.ev
arch_source aarch64 init_el2.S
arch_source aarch64 init_el2_mmu.S
arch_source aarch64 aarch64_boot.c

```

`hyp/core/boot/include/boot_init.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// Arch-independent boot entry points.
//
// These are called from the arch-specific assembly entry points. They are
// responsible for triggering all of the boot module's events, other than the
// two boot_runtime_*_init which must be triggered directly from assembly to
// prevent problematic compiler optimisations.

// First power-on of the boot CPU.
noreturn void
boot_cold_init(cpu_index_t cpu);

// First power-on of any non-boot CPU.
noreturn void
boot_secondary_init(cpu_index_t cpu);

// Warm (second or later) power-on of any CPU.
noreturn void
boot_warm_init(void);

// Add address range to free ranges in env data stream
error_t
boot_add_free_range(uintptr_t object, memdb_type_t type,
		    qcbor_enc_ctxt_t *qcbor_enc_ctxt);

```

`hyp/core/boot/src/boot.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>

#include <hypversion.h>

#include <boot.h>
#include <compiler.h>
#include <log.h>
#include <memdb.h>
#include <prng.h>
#include <qcbor.h>
#include <thread_init.h>
#include <trace.h>
#include <util.h>

#include <events/boot.h>

#include "boot_init.h"
#include "event_handlers.h"

#define STR(x)	#x
#define XSTR(x) STR(x)

const char hypervisor_version[] = XSTR(HYP_CONF_STR) "-" XSTR(HYP_GIT_VERSION)
#if defined(QUALITY)
	" " XSTR(QUALITY)
#endif
	;
const char hypervisor_build_date[] = HYP_BUILD_DATE;

#pragma clang diagnostic push
#pragma clang diagnostic ignored "-Wreserved-identifier"
extern uintptr_t	 __stack_chk_guard;
uintptr_t __stack_chk_guard __attribute__((used, visibility("hidden")));
#pragma clang diagnostic pop

noreturn void
boot_cold_init(cpu_index_t cpu) LOCK_IMPL
{
	// Set the stack canary, either globally, or for the init thread if the
	// canary is thread-local. Note that we can't do this in an event
	// handler because that might trigger a stack check failure if the event
	// handler is not inlined (e.g. in debug builds).
	uint64_result_t guard_r = prng_get64();
	assert(guard_r.e == OK);
	__stack_chk_guard = (uintptr_t)guard_r.r;

	// We can't trace/log early because the CPU index and preemption count
	// in the thread are still uninitialized.

	trigger_boot_cpu_early_init_event();
	trigger_boot_cold_init_event(cpu);
	trigger_boot_cpu_cold_init_event(cpu);

	// It's safe to log now.
	TRACE_AND_LOG(ERROR, WARN, "Hypervisor cold boot, version: {:s} ({:s})",
		      (register_t)hypervisor_version,
		      (register_t)hypervisor_build_date);

	TRACE(DEBUG, INFO, "boot_cpu_warm_init");
	trigger_boot_cpu_warm_init_event();
	TRACE(DEBUG, INFO, "boot_hypervisor_start");
	trigger_boot_hypervisor_start_event();
	TRACE(DEBUG, INFO, "boot_cpu_start");
	trigger_boot_cpu_start_event();
	TRACE(DEBUG, INFO, "entering idle");
	thread_boot_set_idle();
}

#if defined(VERBOSE) && VERBOSE
#define STACK_GUARD_BYTE 0xb8
#define STACK_GUARD_SIZE 256U
#include <string.h>

#include <panic.h>

extern char aarch64_boot_stack[];
#endif

void
boot_handle_boot_cold_init(void)
{
#if defined(VERBOSE) && VERBOSE
	// Add a red-zone to the boot stack
	errno_t err_mem = memset_s(aarch64_boot_stack, STACK_GUARD_SIZE,
				   STACK_GUARD_BYTE, STACK_GUARD_SIZE);
	if (err_mem != 0) {
		panic("Error in memset_s operation!");
	}
#endif
}

void
boot_handle_idle_start(void)
{
#if defined(VERBOSE) && VERBOSE
	char *stack_bottom = (char *)aarch64_boot_stack;
	// Check red-zone in the boot stack
	for (index_t i = 0; i < STACK_GUARD_SIZE; i++) {
		if (stack_bottom[i] != (char)STACK_GUARD_BYTE) {
			panic("boot stack overflow!");
		}
	}
#endif
}

noreturn void
boot_secondary_init(cpu_index_t cpu) LOCK_IMPL
{
	// We can't trace/log early because the CPU index and preemption count
	// in the thread are still uninitialized

	trigger_boot_cpu_early_init_event();
	trigger_boot_cpu_cold_init_event(cpu);

	// It's safe to log now.
	TRACE_AND_LOG(INFO, WARN, "secondary cpu ({:d}) cold boot",
		      (register_t)cpu);

	trigger_boot_cpu_warm_init_event();
	trigger_boot_cpu_start_event();

	TRACE_LOCAL(DEBUG, INFO, "cpu cold boot complete");
	thread_boot_set_idle();
}

// Warm (second or later) power-on of any CPU.
noreturn void
boot_warm_init(void) LOCK_IMPL
{
	trigger_boot_cpu_early_init_event();
	TRACE_LOCAL(INFO, INFO, "cpu warm boot start");
	trigger_boot_cpu_warm_init_event();
	trigger_boot_cpu_start_event();
	TRACE_LOCAL(DEBUG, INFO, "cpu warm boot complete");
	thread_boot_set_idle();
}

static error_t
boot_do_memdb_walk(paddr_t base, size_t size, void *arg)
{
	qcbor_enc_ctxt_t *qcbor_enc_ctxt = (qcbor_enc_ctxt_t *)arg;

	if ((size == 0U) && (util_add_overflows(base, size - 1U))) {
		return ERROR_ARGUMENT_SIZE;
	}

	QCBOREncode_OpenArray(qcbor_enc_ctxt);

	QCBOREncode_AddUInt64(qcbor_enc_ctxt, base);
	QCBOREncode_AddUInt64(qcbor_enc_ctxt, size);

	QCBOREncode_CloseArray(qcbor_enc_ctxt);

	return OK;
}

error_t
boot_add_free_range(uintptr_t object, memdb_type_t type,
		    qcbor_enc_ctxt_t *qcbor_enc_ctxt)
{
	error_t ret;

	QCBOREncode_OpenArrayInMap(qcbor_enc_ctxt, "free_ranges");

	ret = memdb_walk(object, type, boot_do_memdb_walk,
			 (void *)qcbor_enc_ctxt);

	QCBOREncode_CloseArray(qcbor_enc_ctxt);

	return ret;
}

void
boot_start_hypervisor_handover(void)
{
	trigger_boot_hypervisor_handover_event();
}

```

`hyp/core/boot/src/rel_init.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <hyptypes.h>
#include <stddef.h>
#include <stdint.h>
#define USE_ELF64
#include <elf.h>
#include <reloc.h>

#include "arch/reloc.h"

// We must disable stack protection for this function, because the compiler
// might use a relocated absolute pointer to load the stack cookie in the
// function prologue, which will crash because this function hasn't run yet.
__attribute__((no_stack_protector)) void
boot_rel_fixup(Elf_Dyn *dyni, Elf_Addr addr_offset, Elf_Addr rel_offset)
{
	Elf_Xword dyn[DT_CNT];
	Elf_Rel	 *rel	   = NULL;
	Elf_Rel	 *rel_end  = NULL;
	Elf_Rela *rela	   = NULL;
	Elf_Rela *rela_end = NULL;

	// We avoid zeroing the dyn array with an initialiser list as the
	// compiler may optimise it to a memset, which may perform cache zeroing
	// operations that are not supported when the MMU is disabled.
	for (index_t i = 0; i < DT_CNT; i++) {
		dyn[i] = 0;
	}

	for (; dyni->d_tag != DT_NULL; dyni += 1) {
		if (dyni->d_tag < DT_CNT) {
			dyn[dyni->d_tag] = (Elf_Xword)dyni->d_un.d_ptr;
		}
	}

	rel	= (Elf_Rel *)(dyn[DT_REL] + addr_offset);
	rel_end = (Elf_Rel *)(dyn[DT_REL] + addr_offset + dyn[DT_RELSZ]);
	for (; rel < rel_end; rel++) {
		if (!ARCH_CAN_PATCH(rel->r_info)) {
			continue;
		}
		Elf_Addr *r = (Elf_Addr *)(rel->r_offset + addr_offset);
		*r += rel_offset;
	}

	rela	 = (Elf_Rela *)(dyn[DT_RELA] + addr_offset);
	rela_end = (Elf_Rela *)(dyn[DT_RELA] + addr_offset + dyn[DT_RELASZ]);
	for (; rela < rela_end; rela++) {
		if (!ARCH_CAN_PATCH(rela->r_info)) {
			continue;
		}
		Elf_Addr *r = (Elf_Addr *)(rela->r_offset + addr_offset);
		*r	    = rel_offset + (Elf_Addr)rela->r_addend;
	}
}

```

`hyp/core/cpulocal/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

interface cpulocal

assert_config PLATFORM_USABLE_CORES > 0
assert_config PLATFORM_MAX_CORES >= bin(PLATFORM_USABLE_CORES).count("1")

types cpulocal.tc
events cpulocal.ev
source cpulocal.c

```

`hyp/core/cpulocal/cpulocal.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

module cpulocal

// This needs to run first to set up the correct CPU number before the rest of
// the handlers run.
subscribe boot_cpu_cold_init
	priority first

subscribe object_create_thread

// This needs to run first to set up the correct CPU number before the rest of
// the handlers run.
subscribe thread_context_switch_post(prev)
	priority first
	require_preempt_disabled

```

`hyp/core/cpulocal/cpulocal.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

define cpu_index_t public newtype uint16;

define CPU_INDEX_INVALID public constant type cpu_index_t = -1;

extend thread object module cpulocal {
	current_cpu type cpu_index_t;
};

```

`hyp/core/cpulocal/src/cpulocal.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>

#include <compiler.h>
#include <cpulocal.h>
#include <idle.h>
#include <trace.h>

#include "event_handlers.h"

bool
cpulocal_index_valid(cpu_index_t index)
{
	return index < (cpu_index_t)PLATFORM_MAX_CORES;
}

cpu_index_t
cpulocal_check_index(cpu_index_t index)
{
	assert(cpulocal_index_valid(index));
	return index;
}

cpu_index_t
cpulocal_get_index_for_thread(const thread_t *thread)
{
	assert(thread != NULL);
	return thread->cpulocal_current_cpu;
}

cpu_index_t
cpulocal_get_index_unsafe(void)
{
	const thread_t *self = thread_get_self();
	return cpulocal_get_index_for_thread(self);
}

void
cpulocal_handle_boot_cpu_cold_init(cpu_index_t cpu_index)
{
	thread_t *self = thread_get_self();
	assert(self != NULL);

	// Ensure that the index is set early on the primary idle thread
	self->cpulocal_current_cpu = cpulocal_check_index(cpu_index);

	// This is the earliest point at which we can call TRACE(), so let's
	// do that now to let debuggers know that the CPU is coming online.
	TRACE_LOCAL(DEBUG, INFO, "CPU {:d} coming online", cpu_index);
}

error_t
cpulocal_handle_object_create_thread(thread_create_t thread_create)
{
	// The primary idle thread calls this on itself, having already set
	// its CPU index in the boot_cpu_cold_init handler above; so check that
	// we're not about to clobber the current thread's CPU index.
	if (thread_get_self() != thread_create.thread) {
		thread_create.thread->cpulocal_current_cpu = CPU_INDEX_INVALID;
	}

	return OK;
}

void
cpulocal_handle_thread_context_switch_post(thread_t *prev)
{
	thread_t *self = thread_get_self();
	assert(self != NULL);
	cpu_index_t this_cpu = CPU_INDEX_INVALID;

#if SCHEDULER_CAN_MIGRATE
	if (compiler_unexpected(prev == self)) {
		assert(idle_thread() == prev);
		this_cpu = self->scheduler_affinity;
	} else {
		assert(prev != NULL);
		this_cpu = cpulocal_check_index(prev->cpulocal_current_cpu);
		assert((self->kind != THREAD_KIND_IDLE) ||
		       (this_cpu == self->scheduler_affinity));
	}
#else
	this_cpu = self->scheduler_affinity;
#endif

	assert(this_cpu != CPU_INDEX_INVALID);
	prev->cpulocal_current_cpu = CPU_INDEX_INVALID;
	self->cpulocal_current_cpu = this_cpu;
}

```

`hyp/core/cspace_twolevel/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

interface partition cspace
types cspace.tc cspace_tests.tc
events cspace.ev cspace_tests.ev
local_include
source cspace_twolevel.c cspace_tests.c
template typed hyprights.h
template typed_guestapi rights.h
template first_class_object object.ev object.c cspace_lookup.c
source hypercalls.c
hypercalls hypercalls.hvc

```

`hyp/core/cspace_twolevel/cspace.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

module cspace_twolevel

subscribe object_create_cspace

subscribe object_activate_cspace

subscribe object_cleanup_cspace(cspace)

subscribe object_deactivate_thread

subscribe rcu_update[RCU_UPDATE_CLASS_CSPACE_RELEASE_LEVEL]
	handler cspace_destroy_cap_table(entry)

```

`hyp/core/cspace_twolevel/cspace.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <types/bitmap.h>

extend object_type enumeration {
	ANY;
};

extend object_header structure {
	cap_list structure list;
	cap_list_lock structure spinlock;
};

extend rcu_update_class enumeration {
	CSPACE_RELEASE_LEVEL;
};

define cap_value bitfield<16> {
	7:0	lower_index type index_t;
	15:8	upper_index type index_t;
};

extend cap_rights_cspace bitfield {
	0	cap_create	bool;	// right to create a cap
	1	cap_delete	bool;	// right to delete a cap
	2	cap_copy	bool;	// right to copy a cap from cspace
	3	attach		bool;	// right to attach a thread to cspace
	4	cap_revoke	bool;	// right to revoke caps from cspace
};

define cap_state enumeration(explicit) {
	NULL = 0;
	VALID = 1;
	REVOKED = 2;
};

define cap_info bitfield<32> {
	31	master_cap bool;
	15:8	type enumeration object_type;
	1:0	state enumeration cap_state;
	others	unknown=0;
};

define cap_data structure {
	object union object_ptr;
	rights type cap_rights_t;
	info bitfield cap_info;
};

// FIXME:
define cap structure(aligned(16)) {
	data		structure cap_data(atomic);
	cap_list_node	structure list_node(contained);
};

define cap_table_inner object {
	cspace pointer object cspace;
	partition pointer object partition;
	rcu_entry structure rcu_entry(contained);
	index type index_t;
	cap_count type count_t;
};

define cspace_inner object {
	cap_allocation_lock structure spinlock;
	max_caps type count_t;
	cap_count type count_t;
	revoked_cap_list_lock structure spinlock;
	revoked_cap_list structure list;
	id_mult uint64;
	id_inv uint64;
	id_rand_base uint64;
};

define CAP_TABLE_ALLOC_SIZE constant size = 2048;
define CSPACE_ALLOC_SIZE constant size = 2048;

define CAP_TABLE__CAP_SLOT_MEM_AVAIL constant size =
		CAP_TABLE_ALLOC_SIZE - sizeof(object cap_table_inner);
define CAP_TABLE_NUM_CAP_SLOTS constant type index_t =
		(CAP_TABLE__CAP_SLOT_MEM_AVAIL -
			(BITMAP_NUM_WORDS(CAP_TABLE__CAP_SLOT_MEM_AVAIL /
			sizeof(structure cap)) *
			sizeof(type register_t))) / sizeof(structure cap);

// FIXME:
define CSPACE__CAP_TABLE_MEM_AVAIL constant size =
		CSPACE_ALLOC_SIZE - sizeof(object cspace_inner) -
		sizeof(structure object_header) - sizeof(structure list_node);
define CSPACE_NUM_CAP_TABLES constant type index_t =
		(CSPACE__CAP_TABLE_MEM_AVAIL -
			(2 * BITMAP_NUM_WORDS(CSPACE__CAP_TABLE_MEM_AVAIL /
				sizeof(pointer object cap_table)) *
			sizeof(type register_t))) /
		sizeof(pointer object cap_table);

define cap_table object(aligned(CAP_TABLE_ALLOC_SIZE)) {
	ct_inner object(noprefix) cap_table_inner;
	used_slots BITMAP(CAP_TABLE_NUM_CAP_SLOTS, atomic);
	cap_slots array(CAP_TABLE_NUM_CAP_SLOTS) structure cap;
};

extend cspace object {
	c_inner object(noprefix) cspace_inner;
	allocated_tables BITMAP(CSPACE_NUM_CAP_TABLES, atomic);
	available_tables BITMAP(CSPACE_NUM_CAP_TABLES);
	tables array(CSPACE_NUM_CAP_TABLES) pointer(atomic) object cap_table;
};

```

`hyp/core/cspace_twolevel/cspace_tests.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

module cspace_twolevel

#if defined(UNIT_TESTS)

subscribe tests_init
	handler tests_cspace_init()

subscribe tests_start
	handler tests_cspace_start()
	require_preempt_disabled

#endif

```

`hyp/core/cspace_twolevel/cspace_tests.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#if defined(UNIT_TESTS)

extend cap_rights_cspace bitfield {
	auto	test bool;
};

#endif

```

`hyp/core/cspace_twolevel/hypercalls.hvc`:

```hvc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

define cspace_configure hypercall {
	call_num	0x25;
	cspace		input type cap_id_t;
	max_caps	input type count_t;
	res0		input uregister;
	error		output enumeration error;
};

```

`hyp/core/cspace_twolevel/include/cspace_object.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

cap_rights_t
cspace_get_rights_all(object_type_t type);

```

`hyp/core/cspace_twolevel/src/cspace_tests.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#if defined(UNIT_TESTS)

#include <assert.h>
#include <hyptypes.h>

#include <hyprights.h>

#include <atomic.h>
#include <cpulocal.h>
#include <cspace.h>
#include <object.h>
#include <partition.h>
#include <partition_alloc.h>
#include <spinlock.h>

#include <asm/event.h>

#include "event_handlers.h"

static cspace_t	      *test_cspace;
static cap_id_t	       test_cspace_master_cap;
static _Atomic count_t test_cspace_wait_count;
static _Atomic count_t test_cspace_finish_count;
static _Atomic uint8_t test_cspace_revoke_flag;

#define TEST_CAP_COPIES	     20U
#define TEST_CSPACE_MAX_CAPS ((PLATFORM_MAX_CORES * TEST_CAP_COPIES) + 1U)

CPULOCAL_DECLARE_STATIC(cap_id_t, test_caps)[TEST_CAP_COPIES];

void
tests_cspace_init(void)
{
	cspace_ptr_result_t cspace_ret;
	object_ptr_t	    obj;
	cap_id_result_t	    cap_ret;
	error_t		    err;

	cspace_create_t params = { NULL };

	cspace_ret = partition_allocate_cspace(partition_get_private(), params);
	assert(cspace_ret.e == OK);
	test_cspace = cspace_ret.r;
	spinlock_acquire(&test_cspace->header.lock);
	err = cspace_configure(test_cspace, TEST_CSPACE_MAX_CAPS);
	spinlock_release(&test_cspace->header.lock);
	assert(err == OK);
	err = object_activate_cspace(test_cspace);
	assert(err == OK);

	obj.cspace = test_cspace;
	cap_ret =
		cspace_create_master_cap(test_cspace, obj, OBJECT_TYPE_CSPACE);
	assert(cap_ret.e == OK);
	test_cspace_master_cap = cap_ret.r;

	atomic_init(&test_cspace_wait_count, 0U);
	atomic_init(&test_cspace_finish_count, PLATFORM_MAX_CORES);
	atomic_init(&test_cspace_revoke_flag, 0U);
}

static error_t
tests_cspace_cap_lookup(cap_id_t cap, cap_rights_t rights)
{
	object_ptr_result_t ret = cspace_lookup_object(
		test_cspace, cap, OBJECT_TYPE_CSPACE, rights, true);

	if (ret.e == OK) {
		assert(ret.r.cspace == test_cspace);
		object_put_cspace(ret.r.cspace);
	}

	return ret.e;
}

bool
tests_cspace_start(void)
{
	cap_id_result_t	    cap_ret;
	cap_id_t	   *cap		  = CPULOCAL(test_caps);
	cap_rights_cspace_t cspace_rights = cap_rights_cspace_default();
	cap_rights_t	    rights;
	error_t		    err;
	const count_t	    num_delete = TEST_CAP_COPIES / 2U;

	object_get_cspace_additional(test_cspace);

	cap_rights_cspace_set_test(&cspace_rights, true);
	rights = cap_rights_cspace_raw(cspace_rights);

	// Sync with other cores to maximise concurrent accesses
	(void)atomic_fetch_add_explicit(&test_cspace_wait_count, 1U,
					memory_order_relaxed);
	while (asm_event_load_before_wait(&test_cspace_wait_count) !=
	       PLATFORM_MAX_CORES) {
		asm_event_wait(&test_cspace_wait_count);
	}

	for (count_t i = 0U; i < TEST_CAP_COPIES; i++) {
		cap_ret = cspace_copy_cap(test_cspace, test_cspace,
					  test_cspace_master_cap, rights);
		assert(cap_ret.e == OK);
		cap[i] = cap_ret.r;
	}

	err = tests_cspace_cap_lookup(test_cspace_master_cap, rights);
	assert(err == OK);

	for (count_t i = 0U; i < TEST_CAP_COPIES; i++) {
		err = tests_cspace_cap_lookup(cap[i], rights);
		assert(err == OK);
	}

	cspace_rights = CAP_RIGHTS_CSPACE_ALL;
	rights	      = cap_rights_cspace_raw(cspace_rights);

	err = tests_cspace_cap_lookup(test_cspace_master_cap, rights);
	assert(err == OK);

	for (count_t i = 0U; i < TEST_CAP_COPIES; i++) {
		err = tests_cspace_cap_lookup(cap[i], rights);
		assert(err == ERROR_CSPACE_INSUFFICIENT_RIGHTS);
	}

	for (count_t i = 0U; i < num_delete; i++) {
		err = cspace_delete_cap(test_cspace, cap[i]);
		assert(err == OK);
	}

	if (atomic_fetch_sub_explicit(&test_cspace_finish_count, 1U,
				      memory_order_release) == 1U) {
		err = cspace_revoke_caps(test_cspace, test_cspace_master_cap);
		assert(err == OK);

		err = cspace_delete_cap(test_cspace, test_cspace_master_cap);
		assert(err == OK);

		asm_event_store_and_wake(&test_cspace_revoke_flag, 1U);
	} else {
		while (asm_event_load_before_wait(&test_cspace_revoke_flag) ==
		       0U) {
			asm_event_wait(&test_cspace_revoke_flag);
		}
	}

	// Delete the revoked caps
	for (count_t i = num_delete; i < TEST_CAP_COPIES; i++) {
		err = cspace_delete_cap(test_cspace, cap[i]);
		assert(err == OK);
	}

	object_put_cspace(test_cspace);

	return false;
}
#else

extern char unused;

#endif

```

`hyp/core/cspace_twolevel/src/cspace_twolevel.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>
#include <stdatomic.h>
#include <string.h>

#include <hypcontainers.h>
#include <hyprights.h>

#include <atomic.h>
#include <bitmap.h>
#include <compiler.h>
#include <cspace.h>
#include <list.h>
#include <object.h>
#include <panic.h>
#include <partition.h>
#include <prng.h>
#include <rcu.h>
#include <refcount.h>
#include <spinlock.h>
#include <thread.h>
#include <util.h>

#include "cspace_object.h"
#include "event_handlers.h"

#define CSPACE_MAX_CAP_COUNT_SUPPORTED                                         \
	(CAP_TABLE_NUM_CAP_SLOTS * CSPACE_NUM_CAP_TABLES)

static_assert(sizeof(cap_data_t) == 16U, "Cap data must be 16 bytes");
static_assert(atomic_is_lock_free((cap_data_t *)NULL),
	      "Cap data is not lock free");
static_assert(sizeof(cap_t) == 32U, "Cap must be 32 bytes");
static_assert(sizeof(cap_table_t) == CAP_TABLE_ALLOC_SIZE,
	      "Cap table not sized correctly");
static_assert(_Alignof(cap_table_t) == CAP_TABLE_ALLOC_SIZE,
	      "Cap table not aligned correctly");
static_assert(sizeof(cspace_t) == CSPACE_ALLOC_SIZE,
	      "Cspace not sized correctly");

cspace_t *
cspace_get_self(void)
{
	return thread_get_self()->cspace_cspace;
}

static cap_table_t *
cspace_get_cap_table(const cap_t *cap)
{
	return (cap_table_t *)util_balign_down((uintptr_t)cap,
					       sizeof(cap_table_t));
}

static index_t
cspace_get_cap_slot_index(const cap_table_t *table, const cap_t *cap)
{
	ptrdiff_t index = cap - &table->cap_slots[0];

	assert((index >= 0U) && (index < (ptrdiff_t)CAP_TABLE_NUM_CAP_SLOTS));
	assert(cap == &table->cap_slots[index]);

	return (index_t)index;
}

// VM visible cap-IDs are randomized. The encode and decode operations turn an
// internally linear cspace index and apply a random base and index multiply.
// This ensures that for each cspace the cap-IDs are unique and randomized on
// each boot.
//
// Currently only a 16-bit random multiplier is used. A larger 64-bit
// multiplier would be better, however that will require 128-bit multiplies and
// a more complex algorithm to find the inverse.

static error_t
cspace_init_id_encoder(cspace_t *cspace)
{
	error_t err;
#if !defined(DISABLE_CSPACE_RAND) || !DISABLE_CSPACE_RAND
	uint64_result_t rand_base, rand_mult;

	// Generate randomized ID space

	// We need to preserve the Cap-Id space of 0xffffffff.xxxxxxxx
	// for special capability values.  Invalid cap is -1 for example.
	// We pick a rand_base that won't allow such id ranges to be generated.
	do {
		rand_base = prng_get64();
		err	  = rand_base.e;
		if (err != OK) {
			goto out;
		}
	} while ((rand_base.r >> 32) >= 0xffffff00U);

	rand_mult = prng_get64();
	err	  = rand_mult.e;
	if (err == OK) {
		cspace->id_rand_base = rand_base.r;
		// Pick a non-zero random 16-bit number
		while ((rand_mult.r & 0xffffU) == 0U) {
			rand_mult.r = ((uint64_t)0x5555U << 48U) |
				      (rand_mult.r >> 16U);
		}
		// Calculate a 16-bit random multiplier and its inverse
		cspace->id_mult = rand_mult.r & 0xffffU;
		cspace->id_inv	= (util_bit(32) / cspace->id_mult) + 1U;
	}

out:
#else
	cspace->id_rand_base = 0U;
	cspace->id_mult	     = 1U;
	cspace->id_inv	     = util_bit(32U) + 1U;
	err		     = OK;
#endif
	return err;
}

static cap_id_t
cspace_encode_cap_id(const cspace_t *cspace, cap_value_t val)
{
	uint64_t v = (uint64_t)cap_value_raw(val);

	return (cap_id_t)((v * cspace->id_mult) ^ cspace->id_rand_base);
}

static cap_value_result_t
cspace_decode_cap_id(const cspace_t *cspace, cap_id_t id)
{
	cap_value_result_t ret;
	uint64_t	   r = id ^ cspace->id_rand_base;
	uint64_t	   v = (r * cspace->id_inv) >> 32U;

	if (compiler_expected((r == (uint64_t)(uint32_t)r) &&
			      (v == (uint64_t)(uint16_t)v))) {
		ret = cap_value_result_ok(cap_value_cast((uint16_t)v));
	} else {
		ret = cap_value_result_error(ERROR_ARGUMENT_INVALID);
	}

	return ret;
}

static error_t
cspace_cap_id_to_indices(const cspace_t *cspace, cap_id_t cap_id,
			 index_t *upper, index_t *lower)
{
	error_t		   err;
	cap_value_result_t ret = cspace_decode_cap_id(cspace, cap_id);

	if (compiler_expected(ret.e == OK)) {
		*lower = cap_value_get_lower_index(&ret.r);
		*upper = cap_value_get_upper_index(&ret.r);
		if (compiler_expected((*upper < CSPACE_NUM_CAP_TABLES) &&
				      (*lower < CAP_TABLE_NUM_CAP_SLOTS))) {
			err = OK;
		} else {
			err = ERROR_ARGUMENT_INVALID;
		}
	} else {
		err = ret.e;
	}

	return err;
}

static cap_id_t
cspace_indices_to_cap_id(const cspace_t *cspace, index_t upper, index_t lower)
{
	cap_value_t val = cap_value_default();

	cap_value_set_lower_index(&val, lower);
	cap_value_set_upper_index(&val, upper);

	return cspace_encode_cap_id(cspace, val);
}

static error_t
cspace_check_cap_data(cap_data_t data, object_type_t type, cap_rights_t rights)
{
	error_t	      err;
	object_type_t obj_type	    = cap_info_get_type(&data.info);
	cap_state_t   state	    = cap_info_get_state(&data.info);
	cap_rights_t  masked_rights = data.rights & rights;

	if (compiler_expected(state == CAP_STATE_VALID)) {
		if (compiler_expected(obj_type == type) ||
		    (type == OBJECT_TYPE_ANY)) {
			err = OK;
		} else {
			err = ERROR_CSPACE_WRONG_OBJECT_TYPE;
			goto out;
		}
	} else if (state == CAP_STATE_NULL) {
		err = ERROR_CSPACE_CAP_NULL;
		goto out;
	} else if (state == CAP_STATE_REVOKED) {
		err = ERROR_CSPACE_CAP_REVOKED;
		goto out;
	} else {
		panic("invalid cap state");
	}

	if (compiler_unexpected(masked_rights != rights)) {
		err = ERROR_CSPACE_INSUFFICIENT_RIGHTS;
	}
out:
	return err;
}

// Update the cap data for the given cap. Will only succeed if cap hasn't been
// modified since it was last read. As such, this function can also be used to
// check if a cap is unchanged after a previous read.
static error_t
cspace_update_cap_slot(cap_t *cap, cap_data_t *expected_data,
		       cap_data_t new_data)
{
	bool success = atomic_compare_exchange_strong_explicit(
		&cap->data, expected_data, new_data, memory_order_relaxed,
		memory_order_relaxed);

	return success ? OK : ERROR_BUSY;
}

static error_t
cspace_lookup_cap_slot(const cspace_t *cspace, cap_id_t cap_id, cap_t **cap)
{
	error_t	     err;
	index_t	     upper_index, lower_index;
	cap_table_t *table;

	err = cspace_cap_id_to_indices(cspace, cap_id, &upper_index,
				       &lower_index);
	if (compiler_expected(err == OK)) {
		table = atomic_load_consume(&cspace->tables[upper_index]);
		if (compiler_expected(table != NULL)) {
			*cap = &table->cap_slots[lower_index];
			err  = OK;
		} else {
			err = ERROR_CSPACE_CAP_NULL;
		}
	}

	return err;
}

static error_t
cspace_allocate_cap_table(cspace_t *cspace, cap_table_t **table,
			  index_t *upper_index)
{
	error_t		  err;
	void_ptr_result_t ret;
	index_t		  index;
	cap_table_t	 *new_table;
	partition_t	 *partition = cspace->header.partition;

	do {
		if (!bitmap_atomic_ffc(cspace->allocated_tables,
				       CSPACE_NUM_CAP_TABLES, &index)) {
			err = ERROR_CSPACE_FULL;
			goto allocate_cap_table_error;
		}
		// Loop until we successfully change bit state.
	} while (bitmap_atomic_test_and_set(cspace->allocated_tables, index,
					    memory_order_relaxed));

	ret = partition_alloc(partition, sizeof(cap_table_t),
			      alignof(cap_table_t));
	if (ret.e != OK) {
		(void)bitmap_atomic_test_and_clear(cspace->allocated_tables,
						   index, memory_order_relaxed);
		err = ERROR_NOMEM;
		goto allocate_cap_table_error;
	}

	new_table = (cap_table_t *)ret.r;
	(void)memset_s(new_table, sizeof(*new_table), 0, sizeof(*new_table));

	new_table->partition = object_get_partition_additional(partition);
	new_table->cspace    = cspace;
	new_table->index     = index;

	*table	     = new_table;
	*upper_index = index;
	err	     = OK;

allocate_cap_table_error:
	return err;
}

rcu_update_status_t
cspace_destroy_cap_table(rcu_entry_t *entry)
{
	index_t		    index;
	cap_table_t	   *table     = cap_table_container_of_rcu_entry(entry);
	partition_t	   *partition = table->partition;
	rcu_update_status_t ret	      = rcu_update_status_default();

	// If called via cspace destroy, there may still
	// be valid caps which also require destruction.
	for (; table->cap_count > 0U; table->cap_count--) {
		cap_t		*cap;
		cap_data_t	 data;
		object_type_t	 type;
		object_header_t *header;
		bool		 cap_list_empty;

		if (compiler_unexpected(!bitmap_atomic_ffs(
			    table->used_slots, CAP_TABLE_NUM_CAP_SLOTS,
			    &index))) {
			panic("cap table has incorrect cap_count on delete");
		}

		cap  = &table->cap_slots[index];
		data = atomic_load_relaxed(&cap->data);

		bitmap_atomic_clear(table->used_slots, index,
				    memory_order_relaxed);

		if (cap_info_get_state(&data.info) != CAP_STATE_VALID) {
			continue;
		}

		type   = cap_info_get_type(&data.info);
		header = object_get_header(type, data.object);
		spinlock_acquire(&header->cap_list_lock);
		(void)list_delete_node(&header->cap_list, &cap->cap_list_node);
		cap_list_empty = list_is_empty(&header->cap_list);
		spinlock_release(&header->cap_list_lock);

		if (cap_list_empty) {
			object_put(type, data.object);
		}
	}

	(void)partition_free(partition, table, sizeof(cap_table_t));
	object_put_partition(partition);

	return ret;
}

static error_t
cspace_allocate_cap_slot(cspace_t *cspace, cap_t **cap, cap_id_t *cap_id)
	REQUIRE_RCU_READ
{
	error_t	     err;
	cap_table_t *table;
	index_t	     upper_index, lower_index;

	spinlock_acquire(&cspace->cap_allocation_lock);

	if (cspace->cap_count == cspace->max_caps) {
		spinlock_release(&cspace->cap_allocation_lock);
		err = ERROR_CSPACE_FULL;
		goto allocate_cap_slot_error;
	}

	if (bitmap_ffs(cspace->available_tables, CSPACE_NUM_CAP_TABLES,
		       &upper_index)) {
		table = atomic_load_relaxed(&cspace->tables[upper_index]);
	} else {
		// Allocation may require preemption, so release the lock.
		spinlock_release(&cspace->cap_allocation_lock);
		rcu_read_finish();
		err = cspace_allocate_cap_table(cspace, &table, &upper_index);
		rcu_read_start();
		if (err != OK) {
			goto allocate_cap_slot_error;
		}
		// Re-acquire lock and attach table.
		spinlock_acquire(&cspace->cap_allocation_lock);
		// Store with release, as table initialisation
		// must be ordered before table attachment.
		atomic_store_release(&cspace->tables[upper_index], table);
		bitmap_set(cspace->available_tables, upper_index);
	}

	table->cap_count++;
	cspace->cap_count++;

	if (table->cap_count == CAP_TABLE_NUM_CAP_SLOTS) {
		bitmap_clear(cspace->available_tables, upper_index);
	}

	spinlock_release(&cspace->cap_allocation_lock);

	do {
		if (compiler_unexpected(!bitmap_atomic_ffc(
			    table->used_slots, CAP_TABLE_NUM_CAP_SLOTS,
			    &lower_index))) {
			panic("cap table has incorrect cap_count on allocate");
		}
		// Loop until we successfully change bit state.
	} while (bitmap_atomic_test_and_set(table->used_slots, lower_index,
					    memory_order_relaxed));

	*cap	= &table->cap_slots[lower_index];
	*cap_id = cspace_indices_to_cap_id(cspace, upper_index, lower_index);
	err	= OK;

allocate_cap_slot_error:
	return err;
}

// Assumes cap data is already set to null
static void
cspace_free_cap_slot(cspace_t *cspace, cap_t *cap)
{
	cap_table_t *table;
	index_t	     upper_index, lower_index;

	table	    = cspace_get_cap_table(cap);
	lower_index = cspace_get_cap_slot_index(table, cap);
	upper_index = table->index;

	(void)bitmap_atomic_test_and_clear(table->used_slots, lower_index,
					   memory_order_relaxed);

	spinlock_acquire(&cspace->cap_allocation_lock);

	if (table->cap_count == CAP_TABLE_NUM_CAP_SLOTS) {
		bitmap_set(cspace->available_tables, upper_index);
	}

	table->cap_count--;
	cspace->cap_count--;

	if (table->cap_count == 0U) {
		(void)bitmap_atomic_test_and_clear(cspace->allocated_tables,
						   upper_index,
						   memory_order_relaxed);
		bitmap_clear(cspace->available_tables, upper_index);
		atomic_store_relaxed(&cspace->tables[upper_index], NULL);
		rcu_enqueue(&table->rcu_entry,
			    RCU_UPDATE_CLASS_CSPACE_RELEASE_LEVEL);
	}

	spinlock_release(&cspace->cap_allocation_lock);
}

object_ptr_result_t
cspace_lookup_object(cspace_t *cspace, cap_id_t cap_id, object_type_t type,
		     cap_rights_t rights, bool active_only)
{
	error_t		    err;
	cap_t		   *cap;
	cap_data_t	    cap_data;
	object_ptr_result_t ret;

	assert(type != OBJECT_TYPE_ANY);

	rcu_read_start();

	err = cspace_lookup_cap_slot(cspace, cap_id, &cap);
	if (compiler_unexpected(err != OK)) {
		ret = object_ptr_result_error(err);
		goto lookup_object_error;
	}

	cap_data = atomic_load_consume(&cap->data);
	err	 = cspace_check_cap_data(cap_data, type, rights);
	if (compiler_unexpected(err != OK)) {
		ret = object_ptr_result_error(err);
		goto lookup_object_error;
	}
	if (active_only) {
		object_state_t obj_state = atomic_load_acquire(
			&object_get_header(type, cap_data.object)->state);
		if (compiler_unexpected(obj_state != OBJECT_STATE_ACTIVE)) {
			ret = object_ptr_result_error(ERROR_OBJECT_STATE);
			goto lookup_object_error;
		}
	}
	if (compiler_unexpected(!object_get_safe(type, cap_data.object))) {
		ret = object_ptr_result_error(ERROR_CSPACE_CAP_NULL);
		goto lookup_object_error;
	}
	ret = object_ptr_result_ok(cap_data.object);

lookup_object_error:
	rcu_read_finish();

	return ret;
}

object_ptr_result_t
cspace_lookup_object_any(cspace_t *cspace, cap_id_t cap_id,
			 cap_rights_generic_t rights, object_type_t *type)
{
	error_t		    err;
	cap_t		   *cap;
	cap_data_t	    cap_data;
	object_ptr_result_t ret;
	object_type_t	    obj_type = OBJECT_TYPE_ANY;

	assert(type != NULL);
	// Only valid generic object rights may be specified
	assert((~cap_rights_generic_raw(CAP_RIGHTS_GENERIC_ALL) &
		cap_rights_generic_raw(rights)) == 0U);

	rcu_read_start();

	err = cspace_lookup_cap_slot(cspace, cap_id, &cap);
	if (compiler_unexpected(err != OK)) {
		ret = object_ptr_result_error(err);
		goto lookup_object_error;
	}

	cap_data = atomic_load_consume(&cap->data);
	obj_type = cap_info_get_type(&cap_data.info);
	err	 = cspace_check_cap_data(cap_data, OBJECT_TYPE_ANY,
					 cap_rights_generic_raw(rights));
	if (compiler_unexpected(err != OK)) {
		ret = object_ptr_result_error(err);
		goto lookup_object_error;
	}
	if (compiler_unexpected(!object_get_safe(obj_type, cap_data.object))) {
		ret = object_ptr_result_error(ERROR_CSPACE_CAP_NULL);
		goto lookup_object_error;
	}
	ret = object_ptr_result_ok(cap_data.object);

lookup_object_error:
	*type = obj_type;
	rcu_read_finish();

	return ret;
}

error_t
cspace_twolevel_handle_object_create_cspace(cspace_create_t cspace_create)
{
	error_t	  err;
	cspace_t *cspace = cspace_create.cspace;

	// The cspace has been zeroed on allocation,
	// so just initialise non-zero fields.
	spinlock_init(&cspace->cap_allocation_lock);
	spinlock_init(&cspace->revoked_cap_list_lock);
	list_init(&cspace->revoked_cap_list);
	err = cspace_init_id_encoder(cspace);

	return err;
}

error_t
cspace_configure(cspace_t *cspace, count_t max_caps)
{
	error_t err;

	assert(atomic_load_relaxed(&cspace->header.state) == OBJECT_STATE_INIT);

	if (max_caps <= CSPACE_MAX_CAP_COUNT_SUPPORTED) {
		cspace->max_caps = max_caps;
		err		 = OK;
	} else {
		err = ERROR_ARGUMENT_INVALID;
	}

	return err;
}

error_t
cspace_twolevel_handle_object_activate_cspace(cspace_t *cspace)
{
	if (cspace->max_caps != 0U) {
		return OK;
	} else {
		return ERROR_OBJECT_CONFIG;
	}
}

void
cspace_twolevel_handle_object_cleanup_cspace(cspace_t *cspace)
{
	// Ensure all lower levels destroyed
	for (index_t i = 0U; i < CSPACE_NUM_CAP_TABLES; i++) {
		cap_table_t *table = atomic_load_relaxed(&cspace->tables[i]);
		if (table != NULL) {
			(void)cspace_destroy_cap_table(&table->rcu_entry);
		}
	}
}

cap_id_result_t
cspace_create_master_cap(cspace_t *cspace, object_ptr_t object,
			 object_type_t type)
{
	error_t		err;
	cap_t	       *new_cap;
	cap_data_t	cap_data;
	cap_id_t	new_cap_id;
	cap_id_result_t ret;

	assert(type != OBJECT_TYPE_ANY);

	// Objects are initialized with a refcount of 1 which is for the master
	// cap reference here.
	cap_data.object = object;
	cap_data.rights = cspace_get_rights_all(type);

	cap_info_init(&cap_data.info);
	cap_info_set_master_cap(&cap_data.info, true);
	cap_info_set_type(&cap_data.info, type);
	cap_info_set_state(&cap_data.info, CAP_STATE_VALID);

	rcu_read_start();

	err = cspace_allocate_cap_slot(cspace, &new_cap, &new_cap_id);
	if (err == OK) {
		object_header_t *header = object_get_header(type, object);
		// No need to hold cap list lock prior to cap being available
		// in the cspace. Instead, store cap data with release to
		// ensure object & cap list initialisation is ordered-before.
		list_insert_at_head(&header->cap_list, &new_cap->cap_list_node);
		atomic_store_release(&new_cap->data, cap_data);
		ret = cap_id_result_ok(new_cap_id);
	} else {
		ret = cap_id_result_error(err);
	}

	rcu_read_finish();

	return ret;
}

cap_id_result_t
cspace_copy_cap(cspace_t *target_cspace, cspace_t *parent_cspace,
		cap_id_t parent_id, cap_rights_t rights_mask)
{
	error_t		 err;
	cap_t		*new_cap, *parent_cap;
	cap_data_t	 cap_data;
	cap_id_t	 new_cap_id;
	object_header_t *header;
	cap_id_result_t	 ret;

	rcu_read_start();

	// We try to allocate the cap slot first, as we may need to
	// be preempted if allocating a cap table is required.
	err = cspace_allocate_cap_slot(target_cspace, &new_cap, &new_cap_id);
	if (err != OK) {
		ret = cap_id_result_error(err);
		goto copy_cap_error;
	}

	err = cspace_lookup_cap_slot(parent_cspace, parent_id, &parent_cap);
	if (err != OK) {
		cspace_free_cap_slot(target_cspace, new_cap);
		ret = cap_id_result_error(err);
		goto copy_cap_error;
	}

	cap_data = atomic_load_consume(&parent_cap->data);

	err = cspace_check_cap_data(cap_data, OBJECT_TYPE_ANY, 0U);
	if (err != OK) {
		cspace_free_cap_slot(target_cspace, new_cap);
		ret = cap_id_result_error(err);
		goto copy_cap_error;
	}
	cap_rights_t masked_rights = cap_data.rights & rights_mask;
	if (masked_rights == 0U) {
		cspace_free_cap_slot(target_cspace, new_cap);
		ret = cap_id_result_error(ERROR_CSPACE_INSUFFICIENT_RIGHTS);
		goto copy_cap_error;
	}

	header = object_get_header(cap_info_get_type(&cap_data.info),
				   cap_data.object);
	spinlock_acquire(&header->cap_list_lock);

	// Reload the parent cap data for the new cap and
	// ensure it has not changed.
	err = cspace_update_cap_slot(parent_cap, &cap_data, cap_data);
	if (err == OK) {
		// Reuse parent cap data with updated rights.
		cap_data.rights = masked_rights;
		// Ensure this is not created as the master cap
		cap_info_set_master_cap(&cap_data.info, false);
		atomic_store_relaxed(&new_cap->data, cap_data);
		list_insert_after_node(&header->cap_list,
				       &parent_cap->cap_list_node,
				       &new_cap->cap_list_node);
	}

	spinlock_release(&header->cap_list_lock);

	if (err == OK) {
		ret = cap_id_result_ok(new_cap_id);
	} else {
		cspace_free_cap_slot(target_cspace, new_cap);
		ret = cap_id_result_error(err);
	}

copy_cap_error:
	rcu_read_finish();
	return ret;
}

error_t
cspace_delete_cap(cspace_t *cspace, cap_id_t cap_id)
{
	error_t	      err;
	cap_t	     *cap;
	cap_data_t    cap_data, null_cap_data = { 0 };
	cap_state_t   state;
	object_type_t type;
	object_ptr_t  object;
	bool	      cap_list_empty = false;

	rcu_read_start();

	err = cspace_lookup_cap_slot(cspace, cap_id, &cap);
	if (err != OK) {
		goto delete_cap_error;
	}

	cap_data = atomic_load_consume(&cap->data);
	state	 = cap_info_get_state(&cap_data.info);
	type	 = cap_info_get_type(&cap_data.info);
	object	 = cap_data.object;

	if (state == CAP_STATE_VALID) {
		object_header_t *header = object_get_header(type, object);
		spinlock_acquire(&header->cap_list_lock);

		err = cspace_update_cap_slot(cap, &cap_data, null_cap_data);
		if (err == OK) {
			(void)list_delete_node(&header->cap_list,
					       &cap->cap_list_node);
			cap_list_empty = list_is_empty(&header->cap_list);
		}

		spinlock_release(&header->cap_list_lock);
	} else if (state == CAP_STATE_REVOKED) {
		spinlock_acquire(&cspace->revoked_cap_list_lock);

		err = cspace_update_cap_slot(cap, &cap_data, null_cap_data);
		if (err == OK) {
			(void)list_delete_node(&cspace->revoked_cap_list,
					       &cap->cap_list_node);
		}

		spinlock_release(&cspace->revoked_cap_list_lock);
	} else {
		err = ERROR_CSPACE_CAP_NULL;
	}

	if (err == OK) {
		cspace_free_cap_slot(cspace, cap);
		if (cap_list_empty) {
			object_put(type, object);
		}
	}

delete_cap_error:
	rcu_read_finish();
	return err;
}

error_t
cspace_revoke_caps(cspace_t *cspace, cap_id_t master_cap_id)
{
	error_t		 err;
	cap_t		*master_cap;
	cap_data_t	 master_cap_data;
	object_header_t *header;

	rcu_read_start();

	err = cspace_lookup_cap_slot(cspace, master_cap_id, &master_cap);
	if (err != OK) {
		goto revoke_caps_error;
	}

	master_cap_data = atomic_load_consume(&master_cap->data);
	err = cspace_check_cap_data(master_cap_data, OBJECT_TYPE_ANY, 0U);
	if (err != OK) {
		goto revoke_caps_error;
	}
	if (!cap_info_get_master_cap(&master_cap_data.info)) {
		err = ERROR_CSPACE_INSUFFICIENT_RIGHTS;
		goto revoke_caps_error;
	}

	header = object_get_header(cap_info_get_type(&master_cap_data.info),
				   master_cap_data.object);
	spinlock_acquire(&header->cap_list_lock);

	// Perform a no-op update on the master cap. If this fails,
	// the master cap data has changed.
	err = cspace_update_cap_slot(master_cap, &master_cap_data,
				     master_cap_data);
	if (err != OK) {
		spinlock_release(&header->cap_list_lock);
		goto revoke_caps_error;
	}

	// Child caps are always inserted after the parent, so the
	// master cap will be at the head of the object cap list.
	list_t *list = &header->cap_list;
	assert(list_get_head(list) == &master_cap->cap_list_node);

	// FIXME:
	cap_t *curr_cap = NULL;

	list_foreach_container_maydelete (curr_cap, list, cap, cap_list_node) {
		if (curr_cap == master_cap) {
			continue;
		}

		cap_data_t curr_cap_data = atomic_load_relaxed(&curr_cap->data);

		cap_info_set_state(&curr_cap_data.info, CAP_STATE_REVOKED);

		// Clear the object this cap points to, since the object could
		// be deleted by deleting the last valid cap, revoked caps
		// pointing to freed memory would make debugging confusing.
		curr_cap_data.object = (object_ptr_t){ 0 };

		// It is safe to get the child cap's cspace, as the child
		// cap must be destroyed before the cspace can be, and
		// this cannot happen while we have the cap list lock.
		cspace_t *curr_cspace = cspace_get_cap_table(curr_cap)->cspace;
		spinlock_acquire_nopreempt(&curr_cspace->revoked_cap_list_lock);

		// Child cap data won't change while we hold the locks,
		// so just atomically store the invalid data.
		atomic_store_relaxed(&curr_cap->data, curr_cap_data);
		(void)list_delete_node(&header->cap_list,
				       &curr_cap->cap_list_node);
		list_insert_at_head(&curr_cspace->revoked_cap_list,
				    &curr_cap->cap_list_node);
		spinlock_release_nopreempt(&curr_cspace->revoked_cap_list_lock);
	}

	spinlock_release(&header->cap_list_lock);

revoke_caps_error:
	rcu_read_finish();
	return err;
}

error_t
cspace_attach_thread(cspace_t *cspace, thread_t *thread)
{
	assert(thread != NULL);
	assert(cspace != NULL);
	assert(atomic_load_relaxed(&cspace->header.state) ==
	       OBJECT_STATE_ACTIVE);
	assert(atomic_load_relaxed(&thread->header.state) == OBJECT_STATE_INIT);

	if (thread->cspace_cspace != NULL) {
		object_put_cspace(thread->cspace_cspace);
	}

	thread->cspace_cspace = object_get_cspace_additional(cspace);

	return OK;
}

void
cspace_twolevel_handle_object_deactivate_thread(thread_t *thread)
{
	assert(thread != NULL);

	cspace_t *cspace = thread->cspace_cspace;

	if (cspace != NULL) {
		object_put_cspace(thread->cspace_cspace);
		thread->cspace_cspace = NULL;
	}
}

```

`hyp/core/cspace_twolevel/src/hypercalls.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#if defined(HYPERCALLS)
#include <assert.h>
#include <hyptypes.h>

#include <hypcall_def.h>
#include <hyprights.h>

#include <atomic.h>
#include <compiler.h>
#include <cspace.h>
#include <cspace_lookup.h>
#include <object.h>
#include <spinlock.h>

error_t
hypercall_cspace_delete_cap_from(cap_id_t cspace_cap, cap_id_t cap)
{
	error_t		    ret;
	cspace_ptr_result_t c;
	c = cspace_lookup_cspace(cspace_get_self(), cspace_cap,
				 CAP_RIGHTS_CSPACE_CAP_DELETE);
	if (compiler_unexpected(c.e != OK)) {
		ret = c.e;
		goto out;
	}
	cspace_t *cspace = c.r;

	ret = cspace_delete_cap(cspace, cap);

	object_put_cspace(cspace);
out:
	return ret;
}

hypercall_cspace_copy_cap_from_result_t
hypercall_cspace_copy_cap_from(cap_id_t src_cspace_cap, cap_id_t src_cap,
			       cap_id_t	    dest_cspace_cap,
			       cap_rights_t rights_mask)
{
	hypercall_cspace_copy_cap_from_result_t ret = { 0 };
	cspace_ptr_result_t			c;
	c = cspace_lookup_cspace(cspace_get_self(), src_cspace_cap,
				 CAP_RIGHTS_CSPACE_CAP_COPY);
	if (compiler_unexpected(c.e != OK)) {
		ret.error = c.e;
		goto out;
	}
	cspace_t *src_cspace = c.r;

	c = cspace_lookup_cspace(cspace_get_self(), dest_cspace_cap,
				 CAP_RIGHTS_CSPACE_CAP_CREATE);
	if (compiler_unexpected(c.e != OK)) {
		ret.error = c.e;
		goto out_src_cspace_release;
	}
	cspace_t *dest_cspace = c.r;

	cap_id_result_t new =
		cspace_copy_cap(dest_cspace, src_cspace, src_cap, rights_mask);
	if (new.e == OK) {
		ret.error   = OK;
		ret.new_cap = new.r;
	} else {
		ret.error = new.e;
	}

	object_put_cspace(dest_cspace);
out_src_cspace_release:
	object_put_cspace(src_cspace);
out:
	return ret;
}

error_t
hypercall_cspace_revoke_cap_from(cap_id_t src_cspace, cap_id_t src_cap)
{
	(void)src_cspace;
	(void)src_cap;
	return ERROR_UNIMPLEMENTED;
}

error_t
hypercall_cspace_revoke_caps_from(cap_id_t src_cspace, cap_id_t master_cap)
{
	error_t		    ret;
	cspace_ptr_result_t c;
	c = cspace_lookup_cspace(cspace_get_self(), src_cspace,
				 CAP_RIGHTS_CSPACE_CAP_REVOKE);
	if (compiler_unexpected(c.e != OK)) {
		ret = c.e;
		goto out;
	}
	cspace_t *cspace = c.r;

	ret = cspace_revoke_caps(cspace, master_cap);

	object_put_cspace(cspace);
out:
	return ret;
}

error_t
hypercall_cspace_configure(cap_id_t cspace_cap, count_t max_caps)
{
	error_t	      err;
	cspace_t     *cspace = cspace_get_self();
	object_type_t type;

	object_ptr_result_t o = cspace_lookup_object_any(
		cspace, cspace_cap, CAP_RIGHTS_GENERIC_OBJECT_ACTIVATE, &type);
	if (compiler_unexpected(o.e != OK)) {
		err = o.e;
		goto out;
	}
	if (type != OBJECT_TYPE_CSPACE) {
		err = ERROR_CSPACE_WRONG_OBJECT_TYPE;
		goto out_object_release;
	}

	cspace_t *target_cspace = o.r.cspace;

	spinlock_acquire(&target_cspace->header.lock);

	if (atomic_load_relaxed(&target_cspace->header.state) ==
	    OBJECT_STATE_INIT) {
		err = cspace_configure(target_cspace, max_caps);
	} else {
		err = ERROR_OBJECT_STATE;
	}

	spinlock_release(&target_cspace->header.lock);
out_object_release:
	object_put(type, o.r);
out:
	return err;
}

error_t
hypercall_cspace_attach_thread(cap_id_t cspace_cap, cap_id_t thread_cap)
{
	error_t	      ret;
	cspace_t     *cspace = cspace_get_self();
	object_type_t type;

	object_ptr_result_t o = cspace_lookup_object_any(
		cspace, thread_cap, CAP_RIGHTS_GENERIC_OBJECT_ACTIVATE, &type);
	if (compiler_unexpected(o.e != OK)) {
		ret = o.e;
		goto out;
	}

	if (type != OBJECT_TYPE_THREAD) {
		ret = ERROR_CSPACE_WRONG_OBJECT_TYPE;
		goto out_release;
	}

	thread_t *thread = o.r.thread;

	cspace_ptr_result_t c = cspace_lookup_cspace(cspace, cspace_cap,
						     CAP_RIGHTS_CSPACE_ATTACH);
	if (compiler_unexpected(c.e != OK)) {
		ret = c.e;
		goto out_release;
	}

	cspace_t *target_cspace = c.r;

	spinlock_acquire(&thread->header.lock);

	if (atomic_load_relaxed(&thread->header.state) == OBJECT_STATE_INIT) {
		ret = cspace_attach_thread(target_cspace, thread);
	} else {
		ret = ERROR_OBJECT_STATE;
	}

	spinlock_release(&thread->header.lock);

	object_put_cspace(target_cspace);

out_release:
	object_put(type, o.r);
out:
	return ret;
}

#else
extern int unused;
#endif

```

`hyp/core/cspace_twolevel/templates/cspace_lookup.c.tmpl`:

```tmpl
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

\#include <assert.h>
\#include <hyptypes.h>

\#include <compiler.h>
\#include <cspace.h>
\#include <cspace_lookup.h>
\#include <spinlock.h>

\#include "event_handlers.h"

#for obj in $object_list
#set o = str(obj)

static ${o}_ptr_result_t
cspace_lookup_${o}_common(cspace_t *cspace, cap_id_t cap_id,
			  cap_rights_${o}_t rights, bool active_only)
{
	object_ptr_result_t ret;
	${o}_ptr_result_t result;

	ret = cspace_lookup_object(cspace, cap_id, OBJECT_TYPE_${o.upper()},
				   cap_rights_${o}_raw(rights), active_only);
	if (compiler_expected(ret.e == OK)) {
		result = ${o}_ptr_result_ok(ret.r.${o});
	} else {
		result = ${o}_ptr_result_error(ret.e);
	}
	return result;
}

${o}_ptr_result_t
cspace_lookup_${o}(cspace_t *cspace, cap_id_t cap_id, cap_rights_${o}_t rights)
{
	return cspace_lookup_${o}_common(cspace, cap_id, rights, true);
}

${o}_ptr_result_t
cspace_lookup_${o}_any(cspace_t *cspace, cap_id_t cap_id, cap_rights_${o}_t rights)
{
	return cspace_lookup_${o}_common(cspace, cap_id, rights, false);
}
#end for

```

`hyp/core/cspace_twolevel/templates/hyprights.h.tmpl`:

```tmpl
// Automatically generated. Do not modify.
//
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// Cap rights convenience defines

#def print_rights(d)
#set typename = $d.type_name.upper()
#set all_rights = 0
#for dec in d.fields
#if $dec.field_name != "unknown" and $len($dec.field_maps) == 1
#set map = $dec.field_maps[0]
#set field_name = $dec.field_name.upper()
#set mask = ((1 << $map.length) - 1) << $map.mapped_bit
#set all_rights = all_rights + mask
\#define ${typename}_${field_name} ${d.type_name}_cast(${hex(mask)}U)
#end if
#end for
\#define ${typename}_ALL ${d.type_name}_cast(${hex(all_rights)}U)
#end def

#for $d in $definitions
#if $d.category == "bitfield"
#if $d.type_name.startswith("cap_rights")
$print_rights($d)
#end if
#end if
#end for

```

`hyp/core/cspace_twolevel/templates/object.c.tmpl`:

```tmpl
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

\#include <hyptypes.h>

\#include <hyprights.h>

\#include <panic.h>
\#include <spinlock.h>
\#include <list.h>

\#include "event_handlers.h"
\#include "cspace_object.h"

#for obj in $object_list
#set o = str(obj)

error_t
cspace_init_${o}_cap_list(${o}_create_t ${o}_create)
{
	${o}_t *obj = ${o}_create.${o};
	spinlock_init(&obj->header.cap_list_lock);
	list_init(&obj->header.cap_list);
	return OK;
}
#end for

\#pragma clang diagnostic push
\#pragma clang diagnostic ignored "-Wswitch-enum"

cap_rights_t
cspace_get_rights_all(object_type_t type)
{
	cap_rights_t ret;

	switch (type) {
#for obj in $object_list
#set o = str(obj)
	case $obj.type_enum():
		ret = cap_rights_${o}_raw(CAP_RIGHTS_${o.upper()}_ALL);
		break;
#end for
	default:
		panic("unknown object type");
	}

	return ret;
}

```

`hyp/core/cspace_twolevel/templates/object.ev.tmpl`:

```tmpl
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

module cspace_twolevel

#for obj in $object_list
#set o = str(obj)
subscribe object_create_${o}
	handler cspace_init_${o}_cap_list(${o}_create)
#end for

```

`hyp/core/cspace_twolevel/templates/rights.h.tmpl`:

```tmpl
// Automatically generated. Do not modify.
//
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// Hypervisor Cap Rights

#def print_rights(d)
#set typename = $d.type_name.upper()
#set all_rights = 0
#for dec in d.fields
#if $dec.field_name != "unknown" and $len($dec.field_maps) == 1
#set map = $dec.field_maps[0]
#set field_name = $dec.field_name.upper()
#set mask = ((1 << $map.length) - 1) << $map.mapped_bit
#set all_rights = all_rights + mask
\#define ${typename}_${field_name} (cap_rights_t)${hex(mask)}U
#end if
#end for
\#define ${typename}_ALL (cap_rights_t)${hex(all_rights)}U
#end def

#for $d in $definitions
#if $d.category == "bitfield"
#if $d.type_name.startswith("cap_rights")
$print_rights($d)
#end if
#end if
#end for

```

`hyp/core/debug/aarch64/debug.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

module debug

// High priority to permit debugger to attach as early as possible
subscribe boot_cpu_warm_init
	priority 500

#if PLATFORM_DEBUG_SAVE_STATE

subscribe power_cpu_suspend(may_poweroff)
	unwinder(may_poweroff)
	require_preempt_disabled

subscribe power_cpu_resume(was_poweroff)
	require_preempt_disabled

subscribe power_cpu_offline
	require_preempt_disabled

#endif

```

`hyp/core/debug/aarch64/debug.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

define OSDLR_EL1 bitfield<64> {
	0	dlk	bool;
	63:1	unknown=0;
};

define OSLAR_EL1 bitfield<64> {
	0	oslk	bool;
	63:1	unknown=0;
};

define OSLSR_EL1_OSLM enumeration {
	NOT_IMPLEMENTED = 0b0;
	IMPLEMENTED = 0b10;
};

define OSLSR_EL1 bitfield<64> {
	3,0	oslm	enumeration OSLSR_EL1_OSLM;
	1	oslk	bool;
	2	nTT	bool(const) = 0;
	63:4	unknown=0;
};

// We don't care about most of the contents of the EL1 debug registers at
// present; these will be fully defined if we ever implement self-debug for
// EL2 and/or a cross-VM debug API. The one thing we do care about is the
// enable bits in the control registers; if they are all clear, then we will
// disable the VCPU's debug access when saving its context.
define DBGBCR_EL1 bitfield<32> {
	0	E	bool;
	31:1	unknown;
};

define DBGWCR_EL1 bitfield<32> {
	0	E	bool;
	31:1	unknown;
};

define MDSCR_EL1 bitfield<32> {
	0	SS		bool;
	6	ERR		bool;
	12	TDCC		bool;
	13	KDE		bool;
	14	HDE		bool;
	15	MDE		bool;
	others	unknown=0;
};

define MDCCINT_EL1 bitfield<32> {
	31:0	unknown;
};

// Note: semantics of this register defined by PSCI, not ARMv8
define DBGCLAIM_EL1 bitfield<8> {
	0	debug_ext	bool;
	1	debug_self	bool;
	2	pmu_ext		bool;
	3	pmu_self	bool;
	7:4	unknown;
};

// Registers that are shared between self-hosted and external debug
define debug_common_registers structure {
	bvrs		array(CPU_DEBUG_BP_COUNT) uregister;
	wvrs		array(CPU_DEBUG_WP_COUNT) uregister;
	bcrs		array(CPU_DEBUG_BP_COUNT) bitfield DBGBCR_EL1;
	wcrs		array(CPU_DEBUG_WP_COUNT) bitfield DBGWCR_EL1;
	mdscr		bitfield MDSCR_EL1;
#if ARCH_AARCH64_32BIT_EL1
	dbgvcr		uint32;
#endif
};

#if PLATFORM_DEBUG_SAVE_STATE
// External debugger state, saved on powerdown suspend
define debug_ext_state structure {
	dbgclaim	bitfield DBGCLAIM_EL1;
	mdccint		bitfield MDCCINT_EL1;
	common		structure debug_common_registers;
	dtrrx		uint32;
	dtrtx		uint32;
	eccr		uint32;
};
#endif

```

`hyp/core/debug/aarch64/src/debug.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>

#include <hypregisters.h>

#include <cpulocal.h>

#include <events/debug.h>

#include <asm/barrier.h>

#include "debug_bps.h"
#include "event_handlers.h"

static asm_ordering_dummy_t debug_asm_order;

static void
debug_os_unlock(void)
{
	OSLAR_EL1_t oslar = OSLAR_EL1_default();
	OSLAR_EL1_set_oslk(&oslar, false);
	register_OSLAR_EL1_write_ordered(oslar, &debug_asm_order);
	OSDLR_EL1_t osdlr = OSDLR_EL1_default();
	OSDLR_EL1_set_dlk(&osdlr, false);
	register_OSDLR_EL1_write_ordered(osdlr, &debug_asm_order);
	asm_context_sync_ordered(&debug_asm_order);
}

void
debug_handle_boot_cpu_warm_init(void)
{
	debug_os_unlock();
}

#if PLATFORM_DEBUG_SAVE_STATE

CPULOCAL_DECLARE_STATIC(debug_ext_state_t, debug_ext_state);

static void
debug_os_lock(void)
{
	OSLAR_EL1_t oslar = OSLAR_EL1_default();
	OSLAR_EL1_set_oslk(&oslar, true);
	register_OSLAR_EL1_write_ordered(oslar, &debug_asm_order);
	asm_context_sync_ordered(&debug_asm_order);
}

static inline bool
debug_force_save_ext(void)
{
	return PLATFORM_DEBUG_SAVE_STATE > 1U;
}

void
debug_handle_power_cpu_offline(void)
{
	(void)debug_handle_power_cpu_suspend(true);
}

error_t
debug_handle_power_cpu_suspend(bool may_poweroff)
{
	if (may_poweroff) {
		debug_ext_state_t *state = &CPULOCAL(debug_ext_state);

		debug_os_lock();

#if defined(PLATFORM_HAS_NO_DBGCLAIM_EL1) && PLATFORM_HAS_NO_DBGCLAIM_EL1
		state->dbgclaim = DBGCLAIM_EL1_default();
#else
		state->dbgclaim =
			register_DBGCLAIMCLR_EL1_read_ordered(&debug_asm_order);
#endif

		if (debug_force_save_ext() ||
		    DBGCLAIM_EL1_get_debug_ext(&state->dbgclaim)) {
			state->mdccint = register_MDCCINT_EL1_read_ordered(
				&debug_asm_order);
			debug_save_common(&state->common, &debug_asm_order);
			state->dtrrx = register_OSDTRRX_EL1_read_ordered(
				&debug_asm_order);
			state->dtrtx = register_OSDTRTX_EL1_read_ordered(
				&debug_asm_order);
			state->eccr = register_OSECCR_EL1_read_ordered(
				&debug_asm_order);
		}
	}

	return OK;
}

void
debug_unwind_power_cpu_suspend(bool may_poweroff)
{
	if (may_poweroff) {
		debug_os_unlock();
	}
}

void
debug_handle_power_cpu_resume(bool was_poweroff)
{
	if (was_poweroff) {
		debug_ext_state_t *state = &CPULOCAL(debug_ext_state);

		if (debug_force_save_ext() ||
		    DBGCLAIM_EL1_get_debug_ext(&state->dbgclaim)) {
			// Lock just in case; the lock should already be set
			debug_os_lock();

			register_DBGCLAIMSET_EL1_write_ordered(
				state->dbgclaim, &debug_asm_order);
			register_MDCCINT_EL1_write_ordered(state->mdccint,
							   &debug_asm_order);
			debug_load_common(&state->common, &debug_asm_order);
			register_OSDTRRX_EL1_write_ordered(state->dtrrx,
							   &debug_asm_order);
			register_OSDTRTX_EL1_write_ordered(state->dtrtx,
							   &debug_asm_order);
			register_OSECCR_EL1_write_ordered(state->eccr,
							  &debug_asm_order);
			asm_context_sync_ordered(&debug_asm_order);
		}
	}

	debug_os_unlock();
}
#endif

```

`hyp/core/debug/aarch64/templates/debug_bps.h.tmpl`:

```tmpl
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

\#if defined(MODULE_CORE_DEBUG)
static inline bool
debug_save_common(debug_common_registers_t *regs,
		  asm_ordering_dummy_t *order)
{
	bool in_use = false;

#for bp in range(0, $CPU_DEBUG_BP_COUNT)
	regs->bvrs[${bp}] = register_DBGBVR${bp}_EL1_read_ordered(order);
#end for
#for wp in range(0, $CPU_DEBUG_WP_COUNT)
	regs->wvrs[${wp}] = register_DBGWVR${wp}_EL1_read_ordered(order);
#end for
#for bp in range(0, $CPU_DEBUG_BP_COUNT)
	regs->bcrs[${bp}] = register_DBGBCR${bp}_EL1_read_ordered(order);
        in_use = in_use || DBGBCR_EL1_get_E(&regs->bcrs[${bp}]);
#end for
#for wp in range(0, $CPU_DEBUG_WP_COUNT)
	regs->wcrs[${wp}] = register_DBGWCR${wp}_EL1_read_ordered(order);
        in_use = in_use || DBGWCR_EL1_get_E(&regs->wcrs[${wp}]);
#end for
	regs->mdscr = register_MDSCR_EL1_read_ordered(order);
        in_use = in_use && MDSCR_EL1_get_MDE(&regs->mdscr);
        in_use = in_use && MDSCR_EL1_get_KDE(&regs->mdscr);
        in_use = in_use || MDSCR_EL1_get_TDCC(&regs->mdscr);
        in_use = in_use || MDSCR_EL1_get_SS(&regs->mdscr);

\#if ARCH_AARCH64_32BIT_EL1
	regs->dbgvcr = register_DBGVCR32_EL2_read_ordered(order);
\#endif

	return in_use;
}

static inline void
debug_load_common(const debug_common_registers_t *regs,
		  asm_ordering_dummy_t *order)
{
#for bp in range(0, $CPU_DEBUG_BP_COUNT)
	register_DBGBVR${bp}_EL1_write_ordered(regs->bvrs[${bp}], order);
#end for
#for wp in range(0, $CPU_DEBUG_WP_COUNT)
	register_DBGWVR${wp}_EL1_write_ordered(regs->wvrs[${wp}], order);
#end for
#for bp in range(0, $CPU_DEBUG_BP_COUNT)
	register_DBGBCR${bp}_EL1_write_ordered(regs->bcrs[${bp}], order);
#end for
#for wp in range(0, $CPU_DEBUG_WP_COUNT)
	register_DBGWCR${wp}_EL1_write_ordered(regs->wcrs[${wp}], order);
#end for

	register_MDSCR_EL1_write_ordered(regs->mdscr, order);

\#if ARCH_AARCH64_32BIT_EL1
	register_DBGVCR32_EL2_write_ordered(regs->dbgvcr, order);
\#endif
}
\#endif

```

`hyp/core/debug/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

interface debug
events debug.ev
source debug.c
arch_types aarch64 debug.tc
arch_events aarch64 debug.ev
arch_template simple aarch64 debug_bps.h.tmpl
arch_source aarch64 debug.c

```

`hyp/core/debug/debug.ev`:

```ev
// © 2022 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

module debug

subscribe boot_cold_init()

```

`hyp/core/debug/src/debug.c`:

```c
// © 2022 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <hyptypes.h>

#include <compiler.h>
#include <log.h>
#include <platform_features.h>
#include <trace.h>

#include "event_handlers.h"

static bool debug_disabled = false;

void
debug_handle_boot_cold_init(void)
{
	platform_cpu_features_t features = platform_get_cpu_features();

	debug_disabled = platform_cpu_features_get_debug_disable(&features);
	if (debug_disabled) {
		LOG(ERROR, INFO, "debug disabled");
	}
}

```

`hyp/core/globals/build.conf`:

```conf
# © 2023 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

interface globals

events globals.ev
source globals.c

```

`hyp/core/globals/globals.ev`:

```ev
// © 2023 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

module globals

subscribe boot_cold_init()

```

`hyp/core/globals/src/globals.c`:

```c
// © 2023 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <hyptypes.h>

#include <globals.h>
#include <spinlock.h>

#include "event_handlers.h"

static global_options_t global_options;
static spinlock_t	global_options_lock;

void
globals_handle_boot_cold_init(void)
{
	spinlock_init(&global_options_lock);
}

const global_options_t *
globals_get_options(void)
{
	return &global_options;
}

void
globals_set_options(global_options_t set)
{
	spinlock_acquire(&global_options_lock);
	global_options = global_options_union(global_options, set);
	spinlock_release(&global_options_lock);
}

void
globals_clear_options(global_options_t clear)
{
	spinlock_acquire(&global_options_lock);
	global_options = global_options_difference(global_options, clear);
	spinlock_release(&global_options_lock);
}

```

`hyp/core/idle/aarch64/src/idle.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <hyptypes.h>

#include <hypregisters.h>

#include <irq.h>

#include <asm/barrier.h>

#include "idle_arch.h"

bool
idle_arch_wait(void)
{
	bool must_reschedule = false;

	__asm__ volatile("dsb ish; wfi; isb" : "+m"(asm_ordering));

	ISR_EL1_t isr = register_ISR_EL1_read_volatile_ordered(&asm_ordering);
	if (ISR_EL1_get_I(&isr)) {
		must_reschedule = irq_interrupt_dispatch();
	}

	return must_reschedule;
}

bool
idle_arch_wait_timeout(ticks_t timeout)
{
	bool must_reschedule = false;

#if defined(ARCH_ARM_FEAT_WFxT) && ARCH_ARM_FEAT_WFxT
	// Note: WFIT timeouts are based on CNTVCT_EL0, so this assumes that we
	// always set CNTVOFF_EL2 to 0!
	__asm__ volatile("dsb ish; wfit %1; isb"
			 : "+m"(asm_ordering)
			 : "r"(timeout));
#else
	(void)timeout;
	asm_context_sync_ordered(&asm_ordering);
#endif

	ISR_EL1_t isr = register_ISR_EL1_read_volatile_ordered(&asm_ordering);
	if (ISR_EL1_get_I(&isr)) {
		must_reschedule = irq_interrupt_dispatch();
	}

	return must_reschedule;
}

```

`hyp/core/idle/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

interface idle
local_include
source idle.c
arch_source aarch64 idle.c
events idle.ev
types idle.tc

```

`hyp/core/idle/idle.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

module idle

subscribe object_create_thread

subscribe boot_hypervisor_start
	handler idle_thread_init()
	require_preempt_disabled

subscribe thread_get_entry_fn[THREAD_KIND_IDLE]

subscribe thread_get_stack_base[THREAD_KIND_IDLE]

subscribe idle_start

subscribe boot_cold_init

```

`hyp/core/idle/idle.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

extend thread_kind enumeration {
	idle = 1;
};

extend scheduler_block enumeration {
	idle;
};

extend ipi_reason enumeration {
	IDLE;
};

```

`hyp/core/idle/include/idle_arch.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// Execute the architecture's basic wait-for-interrupt instruction.
//
// This function is called with interrupts disabled.
//
// If interrupts must be enabled for the wait instruction to function correctly,
// this function may enable them; in this case the architecture's vectors must
// be able to handle interrupts taken from the idle loop, even in non-
// preemptible configurations that otherwise do not take interrupts in the
// hypervisor. Interrupts must be disabled again before returning.
//
// If the wait instruction can work with interrupts disabled, this function
// must leave them disabled and call irq_interrupt_dispatch() directly after the
// wait. This call may be conditional on an explicit check for pending
// interrupts, if such a check is possible.
//
// This function returns true if a reschedule may be necessary. An
// implementation that enables interrupts must always return true.
bool
idle_arch_wait(void) REQUIRE_PREEMPT_DISABLED;

// Execute a wait-for-interrupt with a timeout.
//
// This is the same as idle_arch_wait(), except that a timeout can be specified
// (as an absolute ticks value) as the time at which the CPU will stop waiting.
// If possible, the implementation should execute a wait for interrupt
// instruction, and arrange to be woken at expiry of the timeout if no other
// event has occurred.
//
// The wokeup mechanism should not rely on interrupt delivery, and should not
// execute any non-trivial code; it is assumed that an architectural wakeup
// mechanism will be used (e.g. AArch64 FEAT_WFxT). If no such mechanism is
// available, the implementation should check for interrupts without waiting.
bool
idle_arch_wait_timeout(ticks_t timeout) REQUIRE_PREEMPT_DISABLED;

```

`hyp/core/idle/src/idle.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>

#include <atomic.h>
#include <bitmap.h>
#include <compiler.h>
#include <cpulocal.h>
#include <hyp_aspace.h>
#include <idle.h>
#include <ipi.h>
#include <irq.h>
#include <object.h>
#include <panic.h>
#include <partition.h>
#include <partition_alloc.h>
#include <preempt.h>
#include <scheduler.h>
#include <thread.h>
#include <timer_queue.h>
#include <trace.h>
#include <util.h>

#include <events/idle.h>
#include <events/object.h>

#include "event_handlers.h"
#include "idle_arch.h"

CPULOCAL_DECLARE_STATIC(thread_t *, idle_thread);

static uintptr_t idle_stack_base;

static thread_t *
idle_thread_create(cpu_index_t i)
{
	thread_create_t params = {
		.scheduler_affinity	  = i,
		.scheduler_affinity_valid = true,
		.scheduler_priority	  = SCHEDULER_MIN_PRIORITY,
		.scheduler_priority_valid = true,
		.kind			  = THREAD_KIND_IDLE,
	};

	thread_ptr_result_t ret =
		partition_allocate_thread(partition_get_private(), params);

	if (ret.e != OK) {
		panic("Unable to create idle thread");
	}

	return ret.r;
}

error_t
idle_handle_object_create_thread(thread_create_t thread_create)
{
	thread_t *thread = thread_create.thread;
	assert(thread != NULL);

	if (thread->kind == THREAD_KIND_IDLE) {
		scheduler_block_init(thread, SCHEDULER_BLOCK_IDLE);
	}

	return OK;
}

static void
idle_thread_init_boot(thread_t *thread, cpu_index_t i)
{
	thread_create_t params = {
		.scheduler_affinity	  = i,
		.scheduler_affinity_valid = true,
		.scheduler_priority	  = SCHEDULER_MIN_PRIORITY,
		.scheduler_priority_valid = true,
		.kind			  = THREAD_KIND_IDLE,
	};

	// Open-coded partition_allocate_thread minus the actual allocation,
	// which is done out of early bootmem in thread_early_init(), and the
	// refcount init which is done at the same time.
	partition_t *hyp_partition = partition_get_private();
	thread->header.partition =
		object_get_partition_additional(hyp_partition);
	thread->header.type = OBJECT_TYPE_THREAD;
	atomic_init(&thread->header.state, OBJECT_STATE_INIT);
	params.thread = thread;

	if (trigger_object_create_thread_event(params) != OK) {
		panic("Unable to create idle thread");
	}
}

void
idle_thread_init(void)
{
	// Allocate some address space for the idle stacks.
	size_t aspace_size =
		THREAD_STACK_MAP_ALIGN * ((size_t)PLATFORM_MAX_CORES + 1U);

	virt_range_result_t stack_range = hyp_aspace_allocate(aspace_size);
	if (stack_range.e != OK) {
		panic("Unable to allocate address space for idle stacks");
	}

	// Start the idle stack range at the next alignment boundary to
	// ensure we have guard pages before the first mapped stack.
	idle_stack_base =
		util_balign_up(stack_range.r.base + 1U, THREAD_STACK_MAP_ALIGN);

	const cpu_index_t cpu = cpulocal_get_index();

	for (cpu_index_t i = 0; cpulocal_index_valid(i); i++) {
		thread_t *thread_idle;

		if (cpu == i) {
			thread_t *self = thread_get_self();
			idle_thread_init_boot(self, i);
			thread_idle = self;
		} else {
			thread_idle = idle_thread_create(i);
		}

		// Each idle thread needs a single extra reference to prevent it
		// being deleted when it first starts. This is because it will
		// be switching from itself in thread_boot_set_idle(), so when
		// it releases the reference to the previous thread in
		// thread_arch_main(), it will in fact be releasing itself.
		(void)object_get_thread_additional(thread_idle);

		CPULOCAL_BY_INDEX(idle_thread, i) = thread_idle;
		if (object_activate_thread(thread_idle) != OK) {
			panic("Error activating idle thread");
		}

		assert(scheduler_is_blocked(CPULOCAL_BY_INDEX(idle_thread, i),
					    SCHEDULER_BLOCK_IDLE));
	}
}

extern void *aarch64_boot_stack;

static cpu_index_t boot_cpu = CPU_INDEX_INVALID;

void
idle_handle_boot_cold_init(cpu_index_t boot_cpu_index)
{
	boot_cpu = boot_cpu_index;
}

void
idle_handle_idle_start(void)
{
	partition_t *private = partition_get_private();

	size_t stack_size = BOOT_STACK_SIZE;

	// Free the boot stack
	// Find a better place to free the boot stack
	// FIXME:
	error_t err = partition_add_heap(
		private,
		partition_virt_to_phys(private, (uintptr_t)&aarch64_boot_stack),
		stack_size);
	if (err != OK) {
		panic("Error freeing stack to hypervisor partition");
	}
}

static noreturn void
idle_loop(uintptr_t unused_params)
{
	// We generally run the idle thread with preemption disabled. Handlers
	// for the idle_yield event may re-enable preemption, as long as they
	// are guaranteed to stop waiting and return true if preemption occurs.
	preempt_disable();

	const cpu_index_t this_cpu = cpulocal_get_index();

	if (compiler_unexpected(this_cpu == boot_cpu)) {
		// We need to do this only once
		boot_cpu = CPU_INDEX_INVALID;
		trigger_idle_start_event();
	}

	assert(idle_is_current());

	(void)unused_params;

	assert(scheduler_is_blocked(thread_get_self(), SCHEDULER_BLOCK_IDLE));

	do {
		scheduler_yield();

		// If yield returned, nothing is runnable
		TRACE(INFO, INFO, "no runnable VCPUs, entering idle");

		while (!idle_yield()) {
			// Retry until an IRQ or other wakeup event occurs
		}
	} while (1);
}

thread_func_t
idle_handle_thread_get_entry_fn(thread_kind_t kind)
{
	assert(kind == THREAD_KIND_IDLE);
	return idle_loop;
}

uintptr_t
idle_handle_thread_get_stack_base(thread_kind_t kind, thread_t *thread)
{
	assert(kind == THREAD_KIND_IDLE);
	assert(thread != NULL);

	cpu_index_t cpu = thread->scheduler_affinity;

	return idle_stack_base + ((uintptr_t)cpu * THREAD_STACK_MAP_ALIGN);
}

thread_t *
idle_thread(void)
{
	return CPULOCAL(idle_thread);
}

thread_t *
idle_thread_for(cpu_index_t cpu_index)
{
	return CPULOCAL_BY_INDEX(idle_thread, cpu_index);
}

bool
idle_is_current(void)
{
	return thread_get_self() == CPULOCAL(idle_thread);
}

bool
idle_yield(void)
{
	assert_preempt_disabled();

	bool	     must_schedule;
	idle_state_t state = trigger_idle_yield_event(idle_is_current());

	switch (state) {
	case IDLE_STATE_IDLE:
		must_schedule = idle_arch_wait();
		break;
	case IDLE_STATE_WAKEUP:
		must_schedule = false;
		break;
	case IDLE_STATE_RESCHEDULE:
		must_schedule = true;
		break;
	default:
		panic("Invalid idle state");
	}

	return must_schedule;
}

idle_state_t
idle_wakeup(void)
{
	idle_state_t ret;

	// Check for an immediately pending wakeup interrupt that triggers a
	// local reschedule. Note that misrouted or spurious IRQs that don't
	// cause local reschedules won't be counted here; we'll keep waiting.
	if (irq_interrupt_dispatch()) {
		ret = IDLE_STATE_RESCHEDULE;
		goto out;
	}

	// Check for a pending reschedule not directly caused by an interrupt.
	if (ipi_handle_relaxed()) {
		ret = IDLE_STATE_RESCHEDULE;
		goto out;
	}

#if (!defined(PLATFORM_IDLE_WAKEUP_NOWAIT) || !PLATFORM_IDLE_WAKEUP_NOWAIT) && \
	(PLATFORM_IDLE_WAKEUP_TIMEOUT_NS > 0)
	// Wait a while for a reschedule event to be triggered. As above,
	// misrouted or spurious IRQs don't count.
	ticks_t start_ticks = timer_get_current_timer_ticks();
	ticks_t wait_ticks  = timer_convert_ns_to_ticks(
		 (nanoseconds_t)PLATFORM_IDLE_WAKEUP_TIMEOUT_NS);
	ticks_t end_ticks = start_ticks + wait_ticks;
	do {
		if (idle_arch_wait_timeout(end_ticks)) {
			ret = IDLE_STATE_RESCHEDULE;
			goto out;
		}
	} while (timer_get_current_timer_ticks() <= end_ticks);
#endif // !PLATFORM_IDLE_WAKEUP_NOWAIT

	// Still no reschedule. Give up and restart the idle handlers.
	TRACE(INFO, INFO, "spurious wakeup, entering idle");
	ret = IDLE_STATE_WAKEUP;

out:
	return ret;
}

```

`hyp/core/ipi/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

interface ipi
events ipi.ev
types ipi.tc
source ipi.c

```

`hyp/core/ipi/ipi.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

module ipi

subscribe platform_ipi
	require_preempt_disabled

subscribe idle_yield
	// Run late, as this handler may sleep
	priority -100
	require_preempt_disabled

subscribe power_cpu_suspend()
	require_preempt_disabled

subscribe thread_exit_to_user
	require_preempt_disabled

subscribe optional preempt_interrupt
	require_preempt_disabled

// Other handlers might clear IPIs, so this has low priority to avoid
// processing those IPIs prematurely.
subscribe scheduler_quiescent
	priority -1000
	require_preempt_disabled

// This handler IPIs all cores to stop, it has high priority
subscribe scheduler_stop()
	priority 1000
	require_preempt_disabled

```

`hyp/core/ipi/ipi.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <asm/cpu.h>

define ipi_pending structure(aligned(1 << CPU_L1D_LINE_BITS)) {
	bits	type register_t(atomic);
};

```

`hyp/core/ipi/src/ipi.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>
#include <limits.h>

#include <atomic.h>
#include <compiler.h>
#include <cpulocal.h>
#include <idle.h>
#include <ipi.h>
#include <platform_ipi.h>
#include <platform_timer.h>
#include <preempt.h>
#include <scheduler.h>
#include <thread.h>
#include <util.h>

#include <events/ipi.h>
#include <events/scheduler.h>

#include <asm/barrier.h>
#include <asm/event.h>
#include <asm/interrupt.h>
#include <asm/prefetch.h>

#include "event_handlers.h"

// Set this to 1 to disable the fast idle optimisation for debugging
#define IPI_DEBUG_NO_FAST_IDLE 0

#define REGISTER_BITS (sizeof(register_t) * (size_t)CHAR_BIT)

// We enable the fast wakeup support by default if asm_event_wait() can sleep
// (as it will busy-wait otherwise) and preemption is enabled. We can possibly
// do it without preemption if asm_event_wait() is woken by pending disabled
// interrupts, but that's not the case on ARMv8.
//
// If interrupts are handled by a VM, we need to be able to ask the VM to send
// an IPI for us. This is not currently implemented, so we force fast wakeups in
// such configurations even though they will block pending interrupts.
// FIXME:
#if (!ASM_EVENT_WAIT_IS_NOOP && !defined(PREEMPT_NULL) &&                      \
     !IPI_DEBUG_NO_FAST_IDLE) ||                                               \
	defined(IPI_FORCE_FAST_WAKEUP_HACK)
#define IPI_FAST_WAKEUP 1
#else
#define IPI_FAST_WAKEUP 0
#endif

#if IPI_FAST_WAKEUP
#define IPI_WAITING_IN_IDLE util_bit(REGISTER_BITS - 1U)
static_assert(((size_t)IPI_REASON__MAX + 1U) < (REGISTER_BITS - 1U),
	      "IPI reasons must fit in one word, with a free bit");
#else
static_assert(((size_t)IPI_REASON__MAX + 1U) < REGISTER_BITS,
	      "IPI reasons must fit in one word");
#endif

CPULOCAL_DECLARE_STATIC(ipi_pending_t, ipi_pending);

void
ipi_others_relaxed(ipi_reason_t ipi)
{
	assert(ipi <= IPI_REASON__MAX);
	const register_t  ipi_bit  = util_bit(ipi);
	const cpu_index_t this_cpu = cpulocal_get_index();

	for (cpu_index_t i = 0U; cpulocal_index_valid(i); i++) {
		if (i == this_cpu) {
			continue;
		}
		(void)atomic_fetch_or_explicit(
			&CPULOCAL_BY_INDEX(ipi_pending, i).bits, ipi_bit,
			memory_order_relaxed);
	}
	atomic_thread_fence(memory_order_release);
	asm_event_wake_updated();
}

void
ipi_others(ipi_reason_t ipi)
{
	ipi_others_relaxed(ipi);
#if PLATFORM_IPI_LINES > ENUM_IPI_REASON_MAX_VALUE
	platform_ipi_others(ipi);
#else
	platform_ipi_others();
#endif
}

void
ipi_others_idle(ipi_reason_t ipi)
{
#if IPI_FAST_WAKEUP
	ipi_others_relaxed(ipi);
#else
	ipi_others(ipi);
#endif
}

static bool
ipi_one_and_check_wakeup_needed(ipi_reason_t ipi, cpu_index_t cpu)
{
	assert(ipi <= IPI_REASON__MAX);
	const register_t ipi_bit = util_bit(ipi);

	assert(cpulocal_index_valid(cpu));

	register_t old_val = atomic_fetch_or_explicit(
		&CPULOCAL_BY_INDEX(ipi_pending, cpu).bits, ipi_bit,
		memory_order_release);
	asm_event_wake_updated();

#if IPI_FAST_WAKEUP
	return (old_val & IPI_WAITING_IN_IDLE) == 0U;
#else
	(void)old_val;
	return true;
#endif
}

void
ipi_one(ipi_reason_t ipi, cpu_index_t cpu)
{
	if (ipi_one_and_check_wakeup_needed(ipi, cpu)) {
#if PLATFORM_IPI_LINES > ENUM_IPI_REASON_MAX_VALUE
		platform_ipi_one(ipi, cpu);
#else
		platform_ipi_one(cpu);
#endif
	}
}

void
ipi_one_relaxed(ipi_reason_t ipi, cpu_index_t cpu)
{
	(void)ipi_one_and_check_wakeup_needed(ipi, cpu);
}

void
ipi_one_idle(ipi_reason_t ipi, cpu_index_t cpu)
{
#if IPI_FAST_WAKEUP
	ipi_one_relaxed(ipi, cpu);
#else
	ipi_one(ipi, cpu);
#endif
}

bool
ipi_clear_relaxed(ipi_reason_t ipi)
{
	assert(ipi <= IPI_REASON__MAX);

	const register_t ipi_bit = util_bit(ipi);

	register_t old_val = atomic_fetch_and_explicit(
		&CPULOCAL(ipi_pending).bits, ~ipi_bit, memory_order_acquire);

	return ((old_val & ipi_bit) != 0U);
}

bool
ipi_clear(ipi_reason_t ipi)
{
#if PLATFORM_IPI_LINES > ENUM_IPI_REASON_MAX_VALUE
	platform_ipi_clear(ipi);
#endif
	return ipi_clear_relaxed(ipi);
}

#if IPI_FAST_WAKEUP || (PLATFORM_IPI_LINES <= ENUM_IPI_REASON_MAX_VALUE)
static bool
ipi_handle_pending(register_t pending) REQUIRE_PREEMPT_DISABLED
{
	bool reschedule = false;

	while (pending != 0U) {
		index_t bit = REGISTER_BITS - 1U - compiler_clz(pending);
		pending &= ~util_bit(bit);
		if (bit <= (index_t)IPI_REASON__MAX) {
			ipi_reason_t ipi = (ipi_reason_t)bit;
			if (trigger_ipi_received_event(ipi)) {
				reschedule = true;
			}
		}
	}

	return reschedule;
}
#endif

#if PLATFORM_IPI_LINES > ENUM_IPI_REASON_MAX_VALUE
bool
ipi_handle_platform_ipi(ipi_reason_t ipi)
{
	if (ipi_clear_relaxed(ipi) && trigger_ipi_received_event(ipi)) {
		// We can't reschedule immediately as that might leave other
		// IRQs unhandled, so defer the reschedule.
		//
		// This may trigger a local reschedule relaxed IPI, even if that
		// is the IPI we just tried to handle. That is OK; since it is
		// relaxed, we will pick it up before returning to userspace or
		// going idle.
		scheduler_trigger();
	}

	return true;
}
#else
bool
ipi_handle_platform_ipi(void)
{
	register_t pending = atomic_exchange_explicit(
		&CPULOCAL(ipi_pending).bits, 0U, memory_order_acquire);
	if (ipi_handle_pending(pending)) {
		scheduler_trigger();
	}

	return true;
}
#endif

bool
ipi_handle_relaxed(void)
{
	assert_preempt_disabled();
	bool reschedule = false;

	_Atomic register_t *local_pending = &CPULOCAL(ipi_pending).bits;
	prefetch_store_keep(local_pending);
	register_t pending = atomic_load_relaxed(local_pending);
	while (compiler_unexpected(pending != 0U)) {
		ipi_reason_t ipi =
			(ipi_reason_t)((register_t)(REGISTER_BITS - 1U -
						    compiler_clz(pending)));
		if (ipi_clear_relaxed(ipi) && trigger_ipi_received_event(ipi)) {
			reschedule = true;
		}
		pending = atomic_load_relaxed(local_pending);
	}

	return reschedule;
}

void
ipi_handle_thread_exit_to_user(thread_entry_reason_t reason)
{
	// Relaxed IPIs are handled directly by the IRQ module for interrupts.
	if (reason != THREAD_ENTRY_REASON_INTERRUPT) {
		if (ipi_handle_relaxed()) {
			(void)scheduler_schedule();
		}
	}
}

idle_state_t
ipi_handle_idle_yield(bool in_idle_thread)
{
	_Atomic register_t *local_pending = &CPULOCAL(ipi_pending).bits;

	prefetch_store_keep(local_pending);
#if IPI_FAST_WAKEUP
	bool	   must_schedule;
	register_t pending;
	do {
		// Mark ourselves as waiting in idle.
		(void)atomic_fetch_or_explicit(local_pending,
					       IPI_WAITING_IN_IDLE,
					       memory_order_relaxed);

		// Sleep until there is at least one event to handle or a
		// preemption clears IPI_WAITING_IN_IDLE.
		//
		// We must enable interrupts while waiting, because there is no
		// guarantee that asm_event_wait() will be woken by pending
		// interrupts. The ARM implementation of it, a WFE instruction,
		// is not woken. This means that preempt_interrupt_dispatch
		// needs to check the preempt disable count, and avoid context
		// switching if it is nonzero!
		asm_interrupt_enable_release(&local_pending);
		pending = asm_event_load_before_wait(local_pending);
		while (pending == IPI_WAITING_IN_IDLE) {
			asm_event_wait(local_pending);
			pending = asm_event_load_before_wait(local_pending);
		}
		asm_interrupt_disable_acquire(&local_pending);

		// Fetch and clear the events to handle; also clear the
		// IPI_WAITING_IN_IDLE bit if it is still set.
		pending = atomic_exchange_explicit(local_pending, 0U,
						   memory_order_acquire);

		// Handle the pending events, checking if a reschedule is
		// required.
		must_schedule =
			ipi_handle_pending(pending & ~IPI_WAITING_IN_IDLE);

		// Exit the loop if we must reschedule, we were preempted,
		// or we weren't triggered by the idle thread.
	} while (in_idle_thread && !must_schedule &&
		 ((pending & IPI_WAITING_IN_IDLE) != 0U));

	// Return and ensure we don't continue to WFI.
	return must_schedule ? IDLE_STATE_RESCHEDULE : IDLE_STATE_WAKEUP;
#else
	(void)in_idle_thread;
	return ipi_handle_relaxed() ? IDLE_STATE_RESCHEDULE : IDLE_STATE_IDLE;
#endif
}

error_t
ipi_handle_power_cpu_suspend(void)
{
	assert_preempt_disabled();

	bool reschedule = ipi_handle_relaxed();
	if (reschedule) {
		scheduler_trigger();
	}

	// Abort the suspend if we need to reschedule
	return reschedule ? ERROR_BUSY : OK;
}

#if !defined(PREEMPT_NULL)
bool
ipi_handle_preempt_interrupt(void)
{
#if IPI_FAST_WAKEUP
	// Clear the waiting-in-idle flag, to force idle_yield to exit.
	atomic_fetch_and_explicit(&CPULOCAL(ipi_pending).bits,
				  ~IPI_WAITING_IN_IDLE, memory_order_relaxed);
	// Note that IPIs are always handled by the caller after this event
	// completes, regardless of its result.
#endif
	return false;
}
#endif

void
ipi_handle_scheduler_quiescent(void)
{
	assert_preempt_disabled();

	bool reschedule = ipi_handle_relaxed();
	if (reschedule) {
		scheduler_trigger();
	}
}

void
ipi_handle_scheduler_stop(void)
{
	ipi_others(IPI_REASON_ABORT_STOP);

	// Delay approx 1ms to allow other cores to complete saving state.
	// We don't wait for acknowledgement since they may be unresponsive.
	uint32_t freq = platform_timer_get_frequency();

	uint64_t now = platform_timer_get_current_ticks();
	uint64_t end = now + ((uint64_t)freq / 1024U);

	while (now < end) {
		asm_yield();
		now = platform_timer_get_current_ticks();
	}
}

```

`hyp/core/irq/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

interface irq
first_class_object hwirq no_partition_create_hypcall
types irq.tc
events irq.ev
source irq.c

```

`hyp/core/irq/irq.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

module irq

subscribe boot_cold_init()

subscribe object_create_hwirq
	priority last

subscribe object_activate_hwirq
	// Run last so the IRQ is not inserted in the global table prematurely
	priority last

subscribe object_deactivate_hwirq
	priority first

#if IRQ_HAS_MSI
subscribe object_cleanup_hwirq(hwirq)
#endif

```

`hyp/core/irq/irq.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

extend hwirq object {
	irq type irq_t;
	action enumeration hwirq_action;
};

extend hwirq_create structure {
	irq type irq_t;
	action enumeration hwirq_action;
};

define hwirq_action enumeration {
	ignore;
};

```

`hyp/core/irq/src/irq.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>
#include <string.h>

#include <atomic.h>
#include <bitmap.h>
#include <compiler.h>
#include <globals.h>
#include <ipi.h>
#include <irq.h>
#include <object.h>
#include <panic.h>
#include <partition.h>
#include <partition_alloc.h>
#include <platform_irq.h>
#include <preempt.h>
#include <rcu.h>
#include <trace.h>

#include <events/irq.h>

#include "event_handlers.h"

#if IRQ_SPARSE_IDS
// Dynamically allocated two-level table of RCU-protected pointers to hwirq
// objects. No lock is needed to protect writes; they are done with
// compare-exchange, at both levels. Empty levels are never freed, on the
// assumption that IRQ numbers are set by hardware and therefore are likely to
// be reused.
#define IRQ_TABLE_L2_SIZE PGTABLE_HYP_PAGE_SIZE
#define IRQ_TABLE_L2_ENTRIES                                                   \
	(count_t)((IRQ_TABLE_L2_SIZE / sizeof(hwirq_t *_Atomic)))
static hwirq_t *_Atomic *_Atomic *irq_table_l1;
#else
// Dynamically allocated array of RCU-protected pointers to hwirq objects.
// No lock is needed to protect writes; they are done with compare-exchange.
static hwirq_t *_Atomic *irq_table;
#endif
static count_t irq_max_cache;

#if IRQ_HAS_MSI
static index_t		   irq_msi_bitmap_size;
static _Atomic register_t *irq_msi_bitmap;
#endif

void
irq_handle_boot_cold_init(void)
{
	irq_max_cache = (count_t)platform_irq_max();

#if IRQ_SPARSE_IDS
	count_t irq_table_entries =
		(irq_max_cache + IRQ_TABLE_L2_ENTRIES) / IRQ_TABLE_L2_ENTRIES;
#else
	count_t irq_table_entries = irq_max_cache + 1U;
#endif
	assert(irq_table_entries != 0U);

	size_t alloc_size  = irq_table_entries * sizeof(void *);
	size_t alloc_align = alignof(void *);

	void_ptr_result_t ptr_r = partition_alloc(partition_get_private(),
						  alloc_size, alloc_align);

	if (ptr_r.e != OK) {
		panic("Unable to allocate IRQ table");
	}

#if IRQ_SPARSE_IDS
	irq_table_l1 = ptr_r.r;
	(void)memset_s(irq_table_l1, alloc_size, 0, alloc_size);
#else
	irq_table		  = ptr_r.r;
	(void)memset_s(irq_table, alloc_size, 0, alloc_size);
#endif

#if IRQ_HAS_MSI
	irq_msi_bitmap_size =
		platform_irq_msi_max() - platform_irq_msi_base + 1U;
	count_t msi_bitmap_words = BITMAP_NUM_WORDS(irq_msi_bitmap_size);
	alloc_size		 = msi_bitmap_words * sizeof(register_t);

	ptr_r = partition_alloc(partition_get_private(), alloc_size,
				alignof(register_t));
	if (ptr_r.e != OK) {
		panic("Unable to allocate MSI allocator bitmap");
	}

	irq_msi_bitmap = ptr_r.r;
	(void)memset_s(irq_msi_bitmap, alloc_size, 0, alloc_size);
#endif
}

static hwirq_t *_Atomic *
irq_find_entry(irq_t irq, bool allocate)
{
	assert((count_t)irq <= irq_max_cache);

#if IRQ_SPARSE_IDS
	count_t		  irq_l1_index = irq / IRQ_TABLE_L2_ENTRIES;
	count_t		  irq_l2_index = irq % IRQ_TABLE_L2_ENTRIES;
	hwirq_t *_Atomic *irq_table_l2 =
		atomic_load_consume(&irq_table_l1[irq_l1_index]);

	if ((irq_table_l2 == NULL) && allocate) {
		size_t		  alloc_size  = IRQ_TABLE_L2_SIZE;
		size_t		  alloc_align = alignof(void *);
		void_ptr_result_t ptr_r	      = partition_alloc(
			      partition_get_private(), alloc_size, alloc_align);

		if (ptr_r.e == OK) {
			(void)memset_s(ptr_r.r, alloc_size, 0, alloc_size);

			if (atomic_compare_exchange_strong_explicit(
				    &irq_table_l1[irq_l1_index], &irq_table_l2,
				    (hwirq_t *_Atomic *)ptr_r.r,
				    memory_order_release,
				    memory_order_consume)) {
				irq_table_l2 = (hwirq_t *_Atomic *)ptr_r.r;
			} else {
				assert(irq_table_l2 != NULL);
				(void)partition_free(partition_get_private(),
						     ptr_r.r, alloc_size);
			}
		}
	}

	return (irq_table_l2 == NULL) ? NULL : &irq_table_l2[irq_l2_index];
#else
	(void)allocate;
	return &irq_table[irq];
#endif
}

hwirq_t *
irq_lookup_hwirq(irq_t irq)
{
	hwirq_t *_Atomic *entry = irq_find_entry(irq, false);
	return (entry == NULL) ? NULL : atomic_load_consume(entry);
}

error_t
irq_handle_object_create_hwirq(hwirq_create_t params)
{
	hwirq_t *hwirq = params.hwirq;
	assert(hwirq != NULL);

	hwirq->irq    = params.irq;
	hwirq->action = params.action;

	return OK;
}

error_t
irq_handle_object_activate_hwirq(hwirq_t *hwirq)
{
	error_t err = platform_irq_check(hwirq->irq);
	if (err != OK) {
		goto out;
	}

	// Locate the IRQ's entry in the global IRQ table, allocating table
	// levels if necessary.
	hwirq_t *_Atomic *entry = irq_find_entry(hwirq->irq, true);
	if (entry == NULL) {
		err = ERROR_NOMEM;
		goto out;
	}

	// Insert the pointer in the global table if the current entry in the
	// table is NULL. We do not keep a reference; this is an RCU-protected
	// pointer which is automatically set to NULL on object deletion. The
	// release ordering here matches the consume ordering in
	// lookup.
	hwirq_t *prev = NULL;
	if (!atomic_compare_exchange_strong_explicit(entry, &prev, hwirq,
						     memory_order_release,
						     memory_order_relaxed)) {
		// This IRQ is already registered.
		err = ERROR_BUSY;
		goto out;
	}

	// The IRQ is fully registered; give the handler an opportunity to
	// enable it if desired.
	(void)trigger_irq_registered_event(hwirq->action, hwirq->irq, hwirq);

out:
	return err;
}

irq_t
irq_max(void)
{
	return irq_max_cache;
}

void
irq_enable_shared(hwirq_t *hwirq)
{
	platform_irq_enable_shared(hwirq->irq);
}

void
irq_enable_local(hwirq_t *hwirq)
{
	platform_irq_enable_local(hwirq->irq);
}

void
irq_disable_shared_nosync(hwirq_t *hwirq)
{
	platform_irq_disable_shared(hwirq->irq);
}

void
irq_disable_local(hwirq_t *hwirq)
{
	platform_irq_disable_local(hwirq->irq);
}

void
irq_disable_local_nowait(hwirq_t *hwirq)
{
	platform_irq_disable_local_nowait(hwirq->irq);
}

void
irq_disable_shared_sync(hwirq_t *hwirq)
{
	irq_disable_shared_nosync(hwirq);

	// Wait for any in-progress IRQ deliveries on other CPUs to complete.
	//
	// This works regardless of the RCU implementation because IRQ delivery
	// itself is in an RCU critical section, and the
	// irq_disable_shared_nosync() is enough to guarantee that any delivery
	// that hasn't started its critical section yet will not receive the
	// IRQ.
	rcu_sync();
}

void
irq_deactivate(hwirq_t *hwirq)
{
	platform_irq_deactivate(hwirq->irq);
}

void
irq_handle_object_deactivate_hwirq(hwirq_t *hwirq)
{
	assert(hwirq != NULL);
	assert(hwirq->irq <= irq_max_cache);

	// This object was activated successfully, so it must already be in the
	// global table.
	hwirq_t *_Atomic *entry = irq_find_entry(hwirq->irq, false);
	assert(entry != NULL);
	assert(atomic_load_relaxed(entry) == hwirq);

	// Disable the physical IRQ if possible.
	if (platform_irq_is_percpu(hwirq->irq)) {
		// To make this take effect immediately across all CPUs we would
		// need to perform an IPI. That is a waste of effort since
		// irq_interrupt_dispatch() will disable IRQs with no handler
		// anyway, so we just disable it locally.
		preempt_disable();
		platform_irq_disable_local(hwirq->irq);
		preempt_enable();
	} else {
		platform_irq_disable_shared(hwirq->irq);
	}

	// Remove this HWIRQ from the dispatch table.
	atomic_store_relaxed(entry, NULL);
}

static void
disable_unhandled_irq(irq_result_t irq_r) REQUIRE_PREEMPT_DISABLED
{
	TRACE(ERROR, WARN, "disabling unhandled HW IRQ {:d}", irq_r.r);
	if (platform_irq_is_percpu(irq_r.r)) {
		platform_irq_disable_local(irq_r.r);
	} else {
		platform_irq_disable_shared(irq_r.r);
	}
	platform_irq_priority_drop(irq_r.r);
	platform_irq_deactivate(irq_r.r);
}

static bool
irq_interrupt_dispatch_one(void) REQUIRE_PREEMPT_DISABLED
{
	irq_result_t irq_r = platform_irq_acknowledge();
	bool	     ret   = true;

	if (irq_r.e == ERROR_RETRY) {
		// IRQ handled by the platform, probably an IPI
		goto out;
	} else if (compiler_unexpected(irq_r.e == ERROR_IDLE)) {
		// No IRQs are pending; exit
		ret = false;
		goto out;
	} else {
		assert(irq_r.e == OK);
		TRACE(INFO, INFO, "acknowledged HW IRQ {:d}", irq_r.r);

		// The entire IRQ delivery is an RCU critical section.
		//
		// Note that this naturally true anyway if we don't
		// allow interrupt nesting.
		//
		// Also, the alternative is to take a reference to the
		// hwirq, which might force us to tear down the hwirq
		// (and potentially the whole partition) in the
		// interrupt handler.
		rcu_read_start();
		hwirq_t *hwirq = irq_lookup_hwirq(irq_r.r);

		if (compiler_unexpected(hwirq == NULL)) {
			disable_unhandled_irq(irq_r);
			rcu_read_finish();
			goto out;
		}

		assert(hwirq->irq == irq_r.r);

		bool handled = trigger_irq_received_event(hwirq->action,
							  irq_r.r, hwirq);
		platform_irq_priority_drop(irq_r.r);
		if (handled) {
			platform_irq_deactivate(irq_r.r);
		}
		rcu_read_finish();
	}

out:
	return ret;
}

bool
irq_interrupt_dispatch(void)
{
	bool spurious = true;

	while (irq_interrupt_dispatch_one()) {
		spurious = false;
	}

	if (spurious) {
		TRACE(INFO, INFO, "spurious EL2 IRQ");
	}

	return ipi_handle_relaxed();
}

#if IRQ_HAS_MSI

hwirq_ptr_result_t
irq_allocate_msi(partition_t *partition, hwirq_action_t action)
{
	hwirq_ptr_result_t ret;
	index_t		   msi;

	assert(irq_msi_bitmap != NULL);

	do {
		if (!bitmap_atomic_ffc(irq_msi_bitmap, irq_msi_bitmap_size,
				       &msi)) {
			ret = hwirq_ptr_result_error(ERROR_BUSY);
			goto out;
		}
	} while (bitmap_atomic_test_and_set(irq_msi_bitmap, msi,
					    memory_order_relaxed));

	irq_t	       irq	    = msi + platform_irq_msi_base;
	hwirq_create_t hwirq_params = { .action = action, .irq = irq };
	ret = partition_allocate_hwirq(partition, hwirq_params);
	if (ret.e != OK) {
		bitmap_atomic_clear(irq_msi_bitmap, msi, memory_order_relaxed);
		goto out;
	}

	ret.e = object_activate_hwirq(ret.r);
	if (ret.e != OK) {
		// IRQ number will be freed by cleanup handler
		object_put_hwirq(ret.r);
		goto out;
	}

out:
	return ret;
}

void
irq_handle_object_cleanup_hwirq(hwirq_t *hwirq)
{
	if (hwirq->irq >= platform_irq_msi_base) {
		index_t msi = hwirq->irq - platform_irq_msi_base;
		if (msi < irq_msi_bitmap_size) {
			assert(irq_msi_bitmap != NULL);

			// Free the IRQ number from the MSI allocator
			bitmap_atomic_clear(irq_msi_bitmap, msi,
					    memory_order_release);
		}
	}
}
#endif // IRQ_HAS_MSI

```

`hyp/core/irq_null/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

interface irq
configs IRQ_NULL=1
source irq_null.c

```

`hyp/core/irq_null/src/irq_null.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <hyptypes.h>

#include <cpulocal.h>
#include <ipi.h>
#include <irq.h>
#include <rcu.h>
#include <scheduler.h>
#include <vcpu.h>

bool
irq_interrupt_dispatch(void)
{
	rcu_read_start();
	thread_t *primary_vcpu =
		scheduler_get_primary_vcpu(cpulocal_get_index());
	scheduler_lock(primary_vcpu);
	vcpu_wakeup(primary_vcpu);
	scheduler_unlock(primary_vcpu);
	rcu_read_finish();
	return ipi_handle_relaxed();
}

```

`hyp/core/mutex_trivial/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

interface mutex
types mutex.tc
source mutex_trivial.c

```

`hyp/core/mutex_trivial/mutex.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

define mutex structure(lockable) {
	lock structure spinlock;
};

```

`hyp/core/mutex_trivial/src/mutex_trivial.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// Trivial implementation of mutexes for configurations that have no scheduler
// in the hypervisor, either because the primary VM controls scheduling or
// because context switching is not supported at all. In this case, mutexes
// degenerate to spinlocks.

#include <hyptypes.h>

#include <mutex.h>
#include <spinlock.h>

#include <events/mutex.h>

void
mutex_init(mutex_t *lock)
{
	spinlock_init(&lock->lock);
	trigger_mutex_init_event(lock);
}

void
mutex_acquire(mutex_t *lock) LOCK_IMPL
{
	trigger_mutex_acquire_event(lock);
	spinlock_acquire(&lock->lock);
	trigger_mutex_acquired_event(lock);
}

bool
mutex_trylock(mutex_t *lock) LOCK_IMPL
{
	trigger_mutex_acquire_event(lock);
	if (!spinlock_trylock(&lock->lock)) {
		trigger_mutex_failed_event(lock);
		return false;
	}
	trigger_mutex_acquired_event(lock);
	return true;
}

void
mutex_release(mutex_t *lock) LOCK_IMPL
{
	trigger_mutex_release_event(lock);
	spinlock_release(&lock->lock);
	trigger_mutex_released_event(lock);
}

```

`hyp/core/object_standard/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

interface object
template first_class_object object.c object.tc
source hypercalls.c
hypercalls hypercalls.hvc

```

`hyp/core/object_standard/hypercalls.hvc`:

```hvc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

define object_activate hypercall {
	call_num 0xc;
	cap	input type cap_id_t;
	res0	input uregister;
	error	output enumeration error;
};

define object_activate_from hypercall {
	call_num 0xd;
	cspace	input type cap_id_t;
	cap	input type cap_id_t;
	res0	input uregister;
	error	output enumeration error;
};

define object_reset hypercall {
	call_num 0xe;
	cap	input type cap_id_t;
	res0	input uregister;
	error	output enumeration error;
};

define object_reset_from hypercall {
	call_num 0xf;
	cspace	input type cap_id_t;
	cap	input type cap_id_t;
	res0	input uregister;
	error	output enumeration error;
};

```

`hyp/core/object_standard/src/hypercalls.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#if defined(HYPERCALLS)
#include <hyptypes.h>

#include <hypcall_def.h>
#include <hyprights.h>

#include <compiler.h>
#include <cspace.h>
#include <cspace_lookup.h>
#include <object.h>
#include <thread.h>

error_t
hypercall_object_activate(cap_id_t cap)
{
	error_t	      err;
	cspace_t     *cspace = cspace_get_self();
	object_type_t type;

	object_ptr_result_t o = cspace_lookup_object_any(
		cspace, cap, CAP_RIGHTS_GENERIC_OBJECT_ACTIVATE, &type);
	if (compiler_unexpected(o.e != OK)) {
		err = o.e;
		goto out;
	}

	err = object_activate(type, o.r);
	object_put(type, o.r);
out:
	return err;
}

error_t
hypercall_object_activate_from(cap_id_t cspace_cap, cap_id_t cap)
{
	error_t	      err;
	cspace_t     *cspace = cspace_get_self();
	object_type_t type;

	cspace_ptr_result_t c;
	c = cspace_lookup_cspace(cspace, cspace_cap,
				 CAP_RIGHTS_CSPACE_CAP_CREATE);
	if (compiler_unexpected(c.e != OK)) {
		err = c.e;
		goto out;
	}
	cspace_t *dest_cspace = c.r;

	object_ptr_result_t o = cspace_lookup_object_any(
		dest_cspace, cap, CAP_RIGHTS_GENERIC_OBJECT_ACTIVATE, &type);
	if (compiler_unexpected(o.e != OK)) {
		err = o.e;
		goto out_dest_cspace_release;
	}

	err = object_activate(type, o.r);
	object_put(type, o.r);
out_dest_cspace_release:
	object_put_cspace(dest_cspace);
out:
	return err;
}

error_t
hypercall_object_reset(cap_id_t cap)
{
	(void)cap;
	return ERROR_UNIMPLEMENTED;
}

error_t
hypercall_object_reset_from(cap_id_t cspace, cap_id_t cap)
{
	(void)cspace;
	(void)cap;
	return ERROR_UNIMPLEMENTED;
}
#else
extern int unused;
#endif

```

`hyp/core/object_standard/templates/object.c.tmpl`:

```tmpl
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

\#include <hyptypes.h>

\#include <atomic.h>
\#include <attributes.h>
\#include <compiler.h>
\#include <object.h>
\#include <panic.h>
\#include <rcu.h>
\#include <refcount.h>
\#include <spinlock.h>

\#include <events/object.h>

#for obj in $object_list
#set o = str(obj)

${o}_t *
object_get_${o}_additional(${o}_t *${o})
{
	refcount_get_additional(&${o}->header.refcount);

	return ${o};
}

bool
object_get_${o}_safe(${o}_t *${o})
{
	return refcount_get_safe(&${o}->header.refcount);
}

static void NOINLINE
object_free_${o}(${o}_t *${o})
{
	object_state_t old_state = atomic_exchange_explicit(
		&${o}->header.state, OBJECT_STATE_DESTROYING,
		memory_order_relaxed);
	if (old_state == OBJECT_STATE_ACTIVE) {
		trigger_object_deactivate_${o}_event(${o});
	}
	rcu_enqueue(&${o}->header.rcu_entry, $obj.rcu_destroy_enum());
}

void
object_put_${o}(${o}_t *${o})
{
	if (compiler_unexpected(refcount_put(&${o}->header.refcount))) {
		object_free_${o}(${o});
	}
}

error_t
object_activate_${o}(${o}_t *${o})
{
	object_ptr_t o = { .${o} = ${o} };
	return object_activate($obj.type_enum(), o);
}

#end for

\#pragma clang diagnostic push
\#pragma clang diagnostic ignored "-Wswitch-enum"

object_ptr_t
object_get_additional(object_type_t type, object_ptr_t object)
{
	object_ptr_t ret;

	switch (type) {
#for obj in $object_list
#set o = str(obj)
	case $obj.type_enum():
		ret.${o} = object_get_${o}_additional(object.${o});
		break;
#end for
	default:
		panic("unknown object type");
	}

	return ret;
}

bool
object_get_safe(object_type_t type, object_ptr_t object)
{
	bool ret;

	switch (type) {
#for obj in $object_list
#set o = str(obj)
	case $obj.type_enum():
		ret = object_get_${o}_safe(object.${o});
		break;
#end for
	default:
		panic("unknown object type");
	}

	return ret;
}

void
object_put(object_type_t type, object_ptr_t object)
{
	switch (type) {
#for obj in $object_list
#set o = str(obj)
	case $obj.type_enum():
		object_put_${o}(object.${o});
		break;
#end for
	default:
		panic("unknown object type");
	}
}

object_header_t *
object_get_header(object_type_t type, object_ptr_t object)
{
	object_header_t *header;

	switch (type) {
#for obj in $object_list
#set o = str(obj)
	case $obj.type_enum():
		header = &object.${o}->header;
		break;
#end for
	default:
		panic("unknown object type");
	}

	return header;
}

error_t
object_activate(object_type_t type, object_ptr_t object)
{
	error_t ret;

	switch (type) {
#for obj in $object_list
#set o = str(obj)
	case $obj.type_enum():
		spinlock_acquire(&object.${o}->header.lock);

		if (atomic_load_relaxed(&object.${o}->header.state) ==
					OBJECT_STATE_INIT) {
			ret = trigger_object_activate_${o}_event(object.${o});

			if (ret == OK) {
				atomic_store_release(&object.${o}->header.state,
						     OBJECT_STATE_ACTIVE);
			} else {
				atomic_store_relaxed(&object.${o}->header.state,
						     OBJECT_STATE_FAILED);
			}

		} else {
			ret = ERROR_OBJECT_STATE;
		}

		spinlock_release(&object.${o}->header.lock);
		break;
#end for
	default:
		panic("unknown object type");
	}

	return ret;
}

\#pragma clang diagnostic pop

```

`hyp/core/object_standard/templates/object.tc.tmpl`:

```tmpl
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

define object_header structure {
	type		enumeration object_type;
	state		enumeration object_state(atomic);
	refcount	structure refcount;
	lock		structure spinlock;
	rcu_entry	structure rcu_entry(contained);
};

#for obj in $object_list
#set o = str(obj)

// TODO: add priority support to the type system to place the object_header
//       at the start of the object, needed for partition_revoke
//	"	header structure object_header priority=first;"
extend $o object {
	header structure object_header(contained, group(__object_header));
};
#end for

#for obj in $object_list
#set o = str(obj)
extend cap_rights_${o} bitfield {
	31	object_activate bool;
};
#end for
extend cap_rights_generic bitfield {
	31	object_activate bool;
};

```

`hyp/core/partition_standard/armv8/phys_access.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

module partition_standard

subscribe boot_cpu_warm_init
	handler partition_phys_access_cpu_warm_init()
	priority 10

```

`hyp/core/partition_standard/armv8/src/phys_access.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier; BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>

#include <hypregisters.h>

#include <compiler.h>
#include <hyp_aspace.h>
#include <log.h>
#include <panic.h>
#include <partition.h>
#include <trace.h>
#include <util.h>

#include "event_handlers.h"

void
partition_phys_access_cpu_warm_init(void)
{
#if ARCH_AARCH64_USE_PAN
	__asm__ volatile("msr PAN, 1" ::: "memory");
#else
	// Nothing to do here.
#endif
}

static bool
memory_attr_type_check(MAIR_ATTR_t memattr, paddr_t check_pa)
{
	bool ret = true;

	switch (memattr) {
	case MAIR_ATTR_DEVICE_NGNRNE:
	case MAIR_ATTR_DEVICE_NGNRE:
	case MAIR_ATTR_DEVICE_NGRE:
	case MAIR_ATTR_DEVICE_GRE:
		ret = false;
		break;
	case MAIR_ATTR_NORMAL_NC:
	case MAIR_ATTR_NORMAL_WB_OUTER_NC:
#if defined(ARCH_ARM_FEAT_MTE)
	case MAIR_ATTR_TAGGED_NORMAL_WB:
#endif
	case MAIR_ATTR_NORMAL_WB:
		break;
	case MAIR_ATTR_DEVICE_NGNRNE_XS:
	case MAIR_ATTR_DEVICE_NGNRE_XS:
	case MAIR_ATTR_DEVICE_NGRE_XS:
	case MAIR_ATTR_DEVICE_GRE_XS:
	default:
		LOG(ERROR, WARN,
		    "Unexpected look-up result in partition_phys_valid."
		    " PA:{:#x}, attr : {:#x}",
		    check_pa, (register_t)memattr);
		ret = false;
		break;
	}

	return ret;
}

bool
partition_phys_valid(paddr_t paddr, size_t size)
{
	bool ret = true;

	if (util_add_overflows(paddr, size)) {
		ret = false;
		goto out;
	}
	if (paddr >= util_bit(HYP_ASPACE_MAP_DIRECT_BITS)) {
		ret = false;
		goto out;
	}

	for (paddr_t check_pa = paddr; check_pa < (paddr + size);
	     check_pa += PGTABLE_HYP_PAGE_SIZE) {
		paddr_t	    pa_lookup;
		MAIR_ATTR_t memattr;
		void	   *check_va = (void *)((uintptr_t)check_pa +
						hyp_aspace_get_physaccess_offset());
		error_t err = hyp_aspace_va_to_pa_el2_read(check_va, &pa_lookup,
							   &memattr, NULL);

		if (err != OK) {
			LOG(DEBUG, INFO,
			    "partition_phys_valid failed for PA: {:#x}",
			    check_pa);
			ret = false;
			break;
		}

		if (compiler_unexpected(check_pa != pa_lookup)) {
			LOG(ERROR, WARN,
			    "Unexpected look-up result in partition_phys_valid."
			    " PA:{:#x}, looked-up PA: {:#x}",
			    check_pa, pa_lookup);
			panic("partition_phys_valid: Bad look-up result");
		}

		// We map the hyp_aspace_physaccess_offset as device type when
		// invalid.
		ret = memory_attr_type_check(memattr, check_pa);

		if (!ret) {
			break;
		}
	}

out:
	return ret;
}

void *
partition_phys_map(paddr_t paddr, size_t size)
{
	assert(!util_add_overflows(paddr, size));
	assert_debug(partition_phys_valid(paddr, size));

	return (void *)((uintptr_t)paddr + hyp_aspace_get_physaccess_offset());
}

void
partition_phys_access_enable(const void *ptr)
{
	(void)ptr;
#if ARCH_AARCH64_USE_PAN
	__asm__ volatile("msr PAN, 0" ::: "memory");
#else
	// Nothing to do here.
#endif
}

void
partition_phys_access_disable(const void *ptr)
{
	(void)ptr;
#if ARCH_AARCH64_USE_PAN
	__asm__ volatile("msr PAN, 1" ::: "memory");
#else
	// Nothing to do here.
#endif
}

void
partition_phys_unmap(const void *vaddr, paddr_t paddr, size_t size)
{
#if ARCH_AARCH64_USE_PAN
	(void)vaddr;
	(void)paddr;
	(void)size;

	// Nothing to do here.
#else
	(void)vaddr;
	(void)paddr;
	(void)size;

	// Nothing to do here.
#endif
}

```

`hyp/core/partition_standard/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

interface partition
events partition.ev
types partition.tc
source init.c partition.c
template first_class_object object.ev object.tc object.c hypercalls.c
arch_source armv8 phys_access.c
arch_events armv8 phys_access.ev
source hypercalls.c
hypercalls hypercalls.hvc

```

`hyp/core/partition_standard/hypercalls.hvc`:

```hvc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

define partition_create_partition hypercall {
	call_num	0x1;
	src_partition	input type cap_id_t;
	cspace		input type cap_id_t;
	res0		input uregister;
	error		output enumeration error;
	new_cap		output type cap_id_t;
};

// TODO: generate below from first_class_object list

define partition_create_cspace hypercall {
	call_num	0x2;
	src_partition	input type cap_id_t;
	cspace		input type cap_id_t;
	// cspace size
	res0		input uregister;
	error		output enumeration error;
	new_cap		output type cap_id_t;
};

define partition_create_addrspace hypercall {
	call_num	0x3;
	src_partition	input type cap_id_t;
	cspace		input type cap_id_t;
	res0		input uregister;
	error		output enumeration error;
	new_cap		output type cap_id_t;
};

define partition_create_memextent hypercall {
	call_num	0x4;
	src_partition	input type cap_id_t;
	cspace		input type cap_id_t;
	// base, size
	res0		input uregister;
	error		output enumeration error;
	new_cap		output type cap_id_t;
};

define partition_create_thread hypercall {
	call_num	0x5;
	src_partition	input type cap_id_t;
	cspace		input type cap_id_t;
	res0		input uregister;
	error		output enumeration error;
	new_cap		output type cap_id_t;
};

define partition_create_doorbell hypercall {
	call_num	0x6;
	src_partition	input type cap_id_t;
	cspace		input type cap_id_t;
	res0		input uregister;
	error		output enumeration error;
	new_cap		output type cap_id_t;
};

define partition_create_msgqueue hypercall {
	call_num	0x7;
	src_partition	input type cap_id_t;
	cspace		input type cap_id_t;
	// create info
	res0		input uregister;
	error		output enumeration error;
	new_cap		output type cap_id_t;
};

#if defined(INTERFACE_WATCHDOG)
define partition_create_watchdog hypercall {
	call_num	0x9;
	src_partition	input type cap_id_t;
	cspace		input type cap_id_t;
	res0		input uregister;
	error		output enumeration error;
	new_cap		output type cap_id_t;
};
#endif

define partition_create_vic hypercall {
	call_num	0xa;
	src_partition	input type cap_id_t;
	cspace		input type cap_id_t;
	res0		input uregister;
	error		output enumeration error;
	new_cap		output type cap_id_t;
};

define partition_create_vpm_group hypercall {
	call_num	0xb;
	src_partition	input type cap_id_t;
	cspace		input type cap_id_t;
	res0		input uregister;
	error		output enumeration error;
	new_cap		output type cap_id_t;
};

#if defined(INTERFACE_VIRTIO_MMIO)
define partition_create_virtio_mmio hypercall {
	call_num	0x48;
	src_partition	input type cap_id_t;
	cspace		input type cap_id_t;
	res0		input uregister;
	error		output enumeration error;
	new_cap		output type cap_id_t;
};
#endif

```

`hyp/core/partition_standard/partition.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

module partition_standard

subscribe boot_cold_init()
	priority first

subscribe boot_cold_init
	handler partition_standard_boot_add_private_heap()
	// This must run after hyp_aspace_handle_boot_cold_init (priority 20)
	// and memdb_handle_boot_cold_init (priority 10)
	priority 9

subscribe boot_hypervisor_start()
	// Run this early since it can add more heap memory
	priority 100

subscribe object_create_partition
	priority last

subscribe object_activate_partition

subscribe object_deactivate_partition()

```

`hyp/core/partition_standard/partition.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

define PARTITION_NUM_MAPPED_RANGE constant type count_t = 8;

extend object_header structure {
	partition	pointer object partition;
};

define partition_mapped_range structure {
	virt		uintptr;
	phys		type paddr_t;
	size		size;
};

extend partition object {
	allocator	structure allocator;
	mapped_ranges	array(PARTITION_NUM_MAPPED_RANGE)
				structure partition_mapped_range;
	options		bitfield partition_option_flags;
};

extend cap_rights_partition bitfield {
	0	object_create	bool;
	1	donate		bool;
};

define partition_option_flags bitfield<64> {
	0	privileged	bool = 0;
	others	unknown = 0;
};

```

`hyp/core/partition_standard/src/hypercalls.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#if defined(HYPERCALLS)
#include <assert.h>
#include <hyptypes.h>

#include <hypcall_def.h>

// Placeholders for unimplemented objects

// Dynamic creation of partitions is not yet implemented
hypercall_partition_create_partition_result_t
hypercall_partition_create_partition(cap_id_t src_partition_cap,
				     cap_id_t cspace_cap)
{
	(void)src_partition_cap;
	(void)cspace_cap;
	return (hypercall_partition_create_partition_result_t){
		.error	 = ERROR_UNIMPLEMENTED,
		.new_cap = CSPACE_CAP_INVALID,
	};
}

#else
extern int unused;
#endif

```

`hyp/core/partition_standard/src/init.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// This file defines partition_get_root().
#define ROOTVM_INIT 1

#include <assert.h>
#include <hyptypes.h>
#include <stdalign.h>
#include <string.h>

#include <allocator.h>
#include <atomic.h>
#include <attributes.h>
#include <bootmem.h>
#include <memdb.h>
#include <object.h>
#include <panic.h>
#include <partition.h>
#include <partition_alloc.h>
#include <partition_init.h>
#include <platform_mem.h>
#include <refcount.h>
#include <util.h>

#include <events/allocator.h>
#include <events/partition.h>

#include <asm/cpu.h>

#include "event_handlers.h"

static partition_t  hyp_partition;
static partition_t *root_partition;

extern const char image_virt_start;
extern const char image_virt_last;
extern const char image_phys_start;
extern const char image_phys_last;

static const uintptr_t virt_start = (uintptr_t)&image_virt_start;
static const paddr_t   phys_start = (paddr_t)&image_phys_start;
static const paddr_t   phys_last  = (paddr_t)&image_phys_last;

#if defined(ARCH_ARM) && ARCH_IS_64BIT
// Ensure hypervisor is 2MiB page size aligned to use AArch64 2M block mappings
static_assert(((size_t)PLATFORM_RW_DATA_SIZE & 0x1fffffU) == 0U,
	      "PLATFORM_RW_DATA_SIZE must be 2MB aligned");
static_assert(((size_t)PLATFORM_HEAP_PRIVATE_SIZE & 0xfffU) == 0U,
	      "PLATFORM_HEAP_PRIVATE_SIZE must be 4KB aligned");
#endif

void NOINLINE
partition_standard_handle_boot_cold_init(void)
{
	// Set up the hyp partition's header.
	refcount_init(&hyp_partition.header.refcount);
	hyp_partition.header.type = OBJECT_TYPE_PARTITION;
	atomic_store_release(&hyp_partition.header.state, OBJECT_STATE_ACTIVE);

	paddr_t hyp_heap_end =
		(phys_last + 1U) - ((size_t)PLATFORM_RW_DATA_SIZE -
				    (size_t)PLATFORM_HEAP_PRIVATE_SIZE);
	// Add hypervisor memory as a mapped range.
	hyp_partition.mapped_ranges[0].virt = virt_start;
	hyp_partition.mapped_ranges[0].phys = phys_start;
	hyp_partition.mapped_ranges[0].size =
		(size_t)(hyp_heap_end - phys_start);

	// Allocate management structures for the hypervisor allocator.
	if (allocator_init(&hyp_partition.allocator) != OK) {
		panic("allocator_init() failed for hyp partition");
	}

	// Configure partition to be privileged
	partition_option_flags_set_privileged(&hyp_partition.options, true);

	// Get remaining boot memory and assign it to hypervisor allocator.
	size_t		  hyp_alloc_size;
	void_ptr_result_t ret = bootmem_allocate_remaining(&hyp_alloc_size);
	if (ret.e != OK) {
		panic("no boot mem");
	}

	paddr_t phys = partition_virt_to_phys(&hyp_partition, (uintptr_t)ret.r);
	assert(phys != PADDR_INVALID);

	error_t err = trigger_allocator_add_ram_range_event(
		&hyp_partition, phys, (uintptr_t)ret.r, hyp_alloc_size);
	if (err != OK) {
		panic("Error moving bootmem to hyp_partition allocator");
	}
}

void NOINLINE
partition_standard_boot_add_private_heap(void)
{
	// Only the first 2MiB of RW data was mapped in the assembly mmu_init.
	// The remainder is mapped by hyp_aspace_handle_boot_cold_init. Because
	// of this, the additional memory if any needs to be added to the
	// hyp_partition allocator here.
	if ((size_t)PLATFORM_HEAP_PRIVATE_SIZE > 0x200000U) {
		size_t remaining_size =
			(size_t)PLATFORM_HEAP_PRIVATE_SIZE - 0x200000U;
		paddr_t remaining_phys =
			(phys_last + 1U) -
			((size_t)PLATFORM_RW_DATA_SIZE - 0x200000U);

		error_t err = partition_add_heap(&hyp_partition, remaining_phys,
						 remaining_size);
		if (err != OK) {
			panic("Error expanding hyp_partition allocator");
		}
	}
}

static void
partition_add_ram(partition_t *partition, paddr_t base, size_t size)
{
	error_t err;

#if defined(MODULE_MEM_MEMDB_GPT)
	// Add the ram range to the memory database.
	// FIXME:
	err = memdb_insert(&hyp_partition, base, base + (size - 1U),
			   (uintptr_t)partition, MEMDB_TYPE_PARTITION_NOMAP);
	if (err != OK) {
		panic("Error inserting ram to memdb");
	}
#endif

	// Notify modules about new ram. Memdb type for this range will be
	// updated to MEMDB_TYPE_PARTITION.
	err = trigger_partition_add_ram_range_event(partition, base, size);
	if (err != OK) {
		panic("Error adding ram to partition");
	}
}

void
partition_standard_handle_boot_hypervisor_start(void)
{
	// Allocate root partition from the hypervisor allocator
	partition_ptr_result_t part_ret = partition_allocate_partition(
		&hyp_partition, (partition_create_t){ 0 });
	if (part_ret.e != OK) {
		panic("Error allocating root partition");
	}
	root_partition = (partition_t *)part_ret.r;

	partition_option_flags_set_privileged(&root_partition->options, true);

	if (object_activate_partition(root_partition) != OK) {
		panic("Error activating root partition");
	}

	error_t err = platform_ram_probe();
	if (err != OK) {
		panic("Platform RAM probe failed");
	}

	platform_ram_info_t *ram_info = platform_get_ram_info();
	assert(ram_info != NULL);

	for (index_t i = 0U; i < ram_info->num_ranges; i++) {
		paddr_t rbase = ram_info->ram_range[i].base;
		size_t	rsize = ram_info->ram_range[i].size;

		assert(rsize != 0U);
		assert(!util_add_overflows(rbase, rsize - 1U));

		paddr_t rlast = rbase + (rsize - 1U);

		if ((phys_start > rbase) && (phys_start <= rlast)) {
			// Hyp image starts within the range; add the partial
			// range before the start of the hyp image
			partition_add_ram(root_partition, rbase,
					  (size_t)(phys_start - rbase));
		}

		if ((phys_last >= rbase) && (phys_last < rlast)) {
			// Hyp image ends within the range, add the partial
			// range after the end of the hyp image
			partition_add_ram(root_partition, phys_last + 1U,
					  (size_t)(rlast - phys_last));
		}

		if ((phys_last < rbase) || (phys_start > rlast)) {
			// No overlap with hyp image; add the entire range
			partition_add_ram(root_partition, rbase, rsize);
		}
	}
}

partition_t *
partition_get_private(void)
{
	return &hyp_partition;
}

partition_t *
partition_get_root(void)
{
	return root_partition;
}

```

`hyp/core/partition_standard/src/partition.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>

#include <allocator.h>
#include <compiler.h>
#include <hyp_aspace.h>
#include <log.h>
#include <memdb.h>
#include <object.h>
#include <panic.h>
#include <partition.h>
#include <pgtable.h>
#include <spinlock.h>
#include <trace.h>
#include <util.h>

#include <events/allocator.h>

#include "event_handlers.h"

void_ptr_result_t NOINLINE
partition_alloc(partition_t *partition, size_t bytes, size_t min_alignment)
{
	void_ptr_result_t ret;

	assert(bytes > 0U);

	ret = allocator_allocate_object(&partition->allocator, bytes,
					min_alignment);

	if (compiler_expected(ret.e == OK)) {
		assert(ret.r != NULL);
	}
	return ret;
}

error_t
partition_free(partition_t *partition, void *mem, size_t bytes)
{
	error_t ret;
	assert((bytes > 0U) && !util_add_overflows((uintptr_t)mem, bytes - 1U));
	assert(partition_virt_to_phys(partition, (uintptr_t)mem) !=
	       PADDR_INVALID);

	ret = allocator_deallocate_object(&partition->allocator, mem, bytes);

	return ret;
}

// FIXME: partition->mapped_ranges is not updated atomically. Its not an issue
// yet since its only done during single-threaded init. Once we support dynamic
// heap adjustment, it will become a problem.
// FIXME:

static uintptr_t
phys_to_virt(partition_t *partition, paddr_t phys, size_t size)
{
	uintptr_t virt = VADDR_INVALID;

	assert(!util_add_overflows(phys, size - 1U));

	for (count_t i = 0U; i < util_array_size(partition->mapped_ranges);
	     i++) {
		partition_mapped_range_t *mr = &partition->mapped_ranges[i];
		if (mr->size == 0U) {
			continue;
		}
		if ((phys >= mr->phys) &&
		    ((phys + (size - 1U)) <= (mr->phys + (mr->size - 1U)))) {
			virt = (uintptr_t)(phys - mr->phys) + mr->virt;
			break;
		}
	}

	return virt;
}

error_t
partition_free_phys(partition_t *partition, paddr_t phys, size_t bytes)
{
	uintptr_t virt = phys_to_virt(partition, phys, bytes);

	if (virt == VADDR_INVALID) {
		panic("Attempt to free memory not in partition");
	}

	return partition_free(partition, (void *)virt, bytes);
}

paddr_t
partition_virt_to_phys(partition_t *partition, uintptr_t addr)
{
	paddr_t phys = PADDR_INVALID;

	for (count_t i = 0U; i < util_array_size(partition->mapped_ranges);
	     i++) {
		partition_mapped_range_t *mr = &partition->mapped_ranges[i];
		if (mr->size == 0U) {
			continue;
		}
		if ((addr >= mr->virt) &&
		    (addr <= (mr->virt + (mr->size - 1U)))) {
			phys = (paddr_t)(addr - mr->virt) + mr->phys;
			break;
		}
	}

	return phys;
}

error_t
partition_standard_handle_object_create_partition(partition_create_t create)
{
	partition_t *partition = create.partition;
	assert(partition != NULL);

	return allocator_init(&partition->allocator);
}

error_t
partition_standard_handle_object_activate_partition(partition_t *partition)
{
	error_t err;

	assert(partition->header.partition != NULL);
	assert(partition->header.partition != partition);

	if (partition_option_flags_get_privileged(&partition->options) &&
	    !partition_option_flags_get_privileged(
		    &partition->header.partition->options)) {
		err = ERROR_DENIED;
		goto out;
	}

	// Partitions hold a reference to themselves to prevent asynchronous
	// destruction when the last capability is deleted.
	//
	// Partitions must be explicitly destroyed to ensure that all objects in
	// them are deactivated synchronously, especially threads which might
	// still be executing on other CPUs; this self-reference will be deleted
	// after that is done. This destruction operation is not yet
	// implemented.
	(void)object_get_partition_additional(partition);

	err = OK;
out:
	return err;
}

noreturn void
partition_standard_handle_object_deactivate_partition(void)
{
	// This is currently not implemented and not needed. The self-reference
	// taken in activate() above should prevent this, but we panic here to
	// ensure that it doesn't happen by accident.
	panic("Partition deactivation attempted");
}

error_t
partition_mem_donate(partition_t *src_partition, paddr_t base, size_t size,
		     partition_t *dst_partition, bool from_heap)
{
	error_t ret;

	partition_t *hyp_partition = partition_get_private();

	if ((size != 0U) && (!util_add_overflows(base, size - 1U))) {
		if (from_heap) {
			ret = memdb_update(hyp_partition, base,
					   base + (size - 1U),
					   (uintptr_t)dst_partition,
					   MEMDB_TYPE_PARTITION,
					   (uintptr_t)&src_partition->allocator,
					   MEMDB_TYPE_ALLOCATOR);
		} else {
			ret = memdb_update(
				hyp_partition, base, base + (size - 1U),
				(uintptr_t)dst_partition, MEMDB_TYPE_PARTITION,
				(uintptr_t)src_partition, MEMDB_TYPE_PARTITION);
		}
	} else {
		ret = ERROR_ARGUMENT_SIZE;
	}

	return ret;
}

error_t
partition_add_heap(partition_t *partition, paddr_t base, size_t size)
{
	error_t ret;

	assert(partition != NULL);
	assert(size != 0U);

	partition_t *hyp_partition = partition_get_private();

	if ((size != 0U) && (!util_add_overflows(base, size - 1U))) {
		ret = memdb_update(hyp_partition, base, base + (size - 1U),
				   (uintptr_t)&partition->allocator,
				   MEMDB_TYPE_ALLOCATOR, (uintptr_t)partition,
				   MEMDB_TYPE_PARTITION);
	} else {
		ret = ERROR_ARGUMENT_SIZE;
	}

	if (ret == OK) {
		uintptr_t virt = phys_to_virt(partition, base, size);
		assert(virt != VADDR_INVALID);
		ret = trigger_allocator_add_ram_range_event(partition, base,
							    virt, size);
	}

	return ret;
}

static error_t
new_memory_add(partition_t *partition, partition_t *hyp_partition, paddr_t phys,
	       size_t size)
{
	error_t	  ret = OK;
	uintptr_t virt;

	partition_mapped_range_t *mr = NULL;
	for (count_t i = 0U; i < util_array_size(partition->mapped_ranges);
	     i++) {
		if (partition->mapped_ranges[i].size == 0U) {
			mr = &partition->mapped_ranges[i];
			break;
		}
	}

	if (mr == NULL) {
		ret = ERROR_NORESOURCES;
		goto out;
	}

	// Use large page size for virt-phys alignment.
	paddr_t phys_align_base =
		util_balign_down(phys, PGTABLE_HYP_LARGE_PAGE_SIZE);
	size_t phys_align_offset = phys - phys_align_base;
	size_t phys_align_size	 = phys_align_offset + size;

	virt_range_result_t vr = hyp_aspace_allocate(phys_align_size);
	if (vr.e != OK) {
		ret = vr.e;
		goto out;
	}

	virt = vr.r.base + phys_align_offset;

	pgtable_hyp_start();
	// FIXME:
	ret = pgtable_hyp_map(hyp_partition, virt, size, phys,
			      PGTABLE_HYP_MEMTYPE_WRITEBACK, PGTABLE_ACCESS_RW,
			      VMSA_SHAREABILITY_INNER_SHAREABLE);
	pgtable_hyp_commit();
	if (ret == OK) {
		ret = trigger_allocator_add_ram_range_event(partition, phys,
							    virt, size);
	}
	if (ret != OK) {
		// FIXME:
		// This should unmap the failed range, freeing to the target
		// partition and preserve the levels that were preallocated,
		// followed by unmapping the preserved tables (if they are
		// empty), freeing to the hyp_partition.
		pgtable_hyp_start();
		pgtable_hyp_unmap(hyp_partition, virt, size,
				  PGTABLE_HYP_UNMAP_PRESERVE_NONE);
		pgtable_hyp_commit();
		hyp_aspace_deallocate(partition, vr.r);
	} else {
		mr->virt = virt;
		mr->phys = phys;
		mr->size = size;

		LOG(DEBUG, INFO,
		    "added heap: partition {:#x}, virt {:#x}, phys {:#x}, size {:#x}",
		    (uintptr_t)partition, virt, phys, size);
	}

out:
	return ret;
}

error_t
partition_map_and_add_heap(partition_t *partition, paddr_t phys, size_t size)
{
	error_t ret;
	error_t err = OK;

	assert(partition != NULL);
	assert(size != 0U);

	// This should not be called for memory already mapped.
	if (phys_to_virt(partition, phys, size) != VADDR_INVALID) {
		panic("Attempt to add memory already in partition");
	}

	// FIXME:
	// Mapping the partition should preallocate top page-table levels from
	// the hyp partition and then map with the target partition, but we
	// have a chicken-and-egg problem to solve: if the target partition has
	// no memory yet (because it is new) then it can't allocate page
	// tables. We will probably need to seed new partition allocators with
	// some memory from the parent partition.
	partition_t *hyp_partition = partition_get_private();

	if ((size == 0U) || (util_add_overflows(phys, size - 1U))) {
		ret = ERROR_ARGUMENT_SIZE;
		goto out;
	}

	if (!util_is_baligned(phys, PGTABLE_HYP_PAGE_SIZE) ||
	    !util_is_baligned(size, PGTABLE_HYP_PAGE_SIZE)) {
		ret = ERROR_ARGUMENT_ALIGNMENT;
		goto out;
	}

	ret = memdb_update(hyp_partition, phys, phys + (size - 1U),
			   (uintptr_t)&partition->allocator,
			   MEMDB_TYPE_ALLOCATOR, (uintptr_t)partition,
			   MEMDB_TYPE_PARTITION);
	if (ret != OK) {
		goto out;
	}

	spinlock_acquire(&partition->header.lock);

	// Add a new mapped range for the memory.
	ret = new_memory_add(partition, hyp_partition, phys, size);

	spinlock_release(&partition->header.lock);

	if (ret != OK) {
		err = memdb_update(hyp_partition, phys, phys + (size - 1U),
				   (uintptr_t)partition, MEMDB_TYPE_PARTITION,
				   (uintptr_t)&partition->allocator,
				   MEMDB_TYPE_ALLOCATOR);
		if (err != OK) {
			panic("Error updating memdb.");
		}
	}
out:
	return ret;
}

#if defined(PLATFORM_TRACE_STANDALONE_REGION)

static error_t
new_memory_add_trace(partition_t *partition, paddr_t phys, size_t size,
		     partition_mapped_range_t **mr, uintptr_result_t *virt_ret)
{
	error_t	  ret = OK;
	uintptr_t virt;

	for (count_t i = 0U; i < util_array_size(partition->mapped_ranges);
	     i++) {
		if (partition->mapped_ranges[i].size == 0U) {
			*mr = &partition->mapped_ranges[i];
			break;
		}
	}

	if (*mr == NULL) {
		ret = ERROR_NORESOURCES;
		goto out;
	}

	// Use large page size for virt-phys alignment.
	paddr_t phys_align_base =
		util_balign_down(phys, PGTABLE_HYP_LARGE_PAGE_SIZE);
	size_t phys_align_offset = phys - phys_align_base;
	size_t phys_align_size	 = phys_align_offset + size;

	virt_range_result_t vr = hyp_aspace_allocate(phys_align_size);
	if (vr.e != OK) {
		ret = vr.e;
		goto out;
	}

	virt	    = vr.r.base + phys_align_offset;
	(*mr)->virt = virt;
	(*mr)->phys = phys;
	(*mr)->size = size;

	pgtable_hyp_start();
	ret = pgtable_hyp_map(partition, virt, size, phys,
			      PGTABLE_HYP_MEMTYPE_WRITEBACK, PGTABLE_ACCESS_RW,
			      VMSA_SHAREABILITY_INNER_SHAREABLE);
	if (ret == OK) {
		pgtable_hyp_commit();
	} else {
		pgtable_hyp_unmap(partition, virt, size,
				  PGTABLE_HYP_UNMAP_PRESERVE_NONE);
		pgtable_hyp_commit();
		hyp_aspace_deallocate(partition, vr.r);
	}
	if (ret == OK) {
		(*virt_ret).r = virt;
		LOG(DEBUG, INFO,
		    "added trace: partition {:#x}, virt {:#x}, phys {:#x}, size {:#x}",
		    (uintptr_t)partition, (*virt_ret).r, phys, size);
	}

out:
	return ret;
}

uintptr_result_t
partition_map_and_add_trace(partition_t *partition, paddr_t phys, size_t size)
{
	error_t		 ret;
	error_t		 err	  = OK;
	uintptr_result_t virt_ret = { 0 };

	assert(partition != NULL);
	assert(size != 0U);

	if ((size == 0U) || (util_add_overflows(phys, size - 1U))) {
		ret = ERROR_ARGUMENT_SIZE;
		goto out;
	}

	if (!util_is_baligned(phys, PGTABLE_HYP_PAGE_SIZE) ||
	    !util_is_baligned(size, PGTABLE_HYP_PAGE_SIZE)) {
		ret = ERROR_ARGUMENT_ALIGNMENT;
		goto out;
	}

	partition_t *hyp_partition = partition_get_private();
	ret = memdb_update(hyp_partition, phys, phys + (size - 1U),
			   (uintptr_t)NULL, MEMDB_TYPE_TRACE,
			   (uintptr_t)partition, MEMDB_TYPE_PARTITION);
	if (ret != OK) {
		goto out;
	}

	// Add a new mapped range for the memory.
	partition_mapped_range_t *mr = NULL;

	ret = new_memory_add_trace(partition, phys, size, &mr, &virt_ret);

	if (ret != OK) {
		err = memdb_update(hyp_partition, phys, phys + (size - 1U),
				   (uintptr_t)partition, MEMDB_TYPE_PARTITION,
				   (uintptr_t)NULL, MEMDB_TYPE_TRACE);
		if (err != OK) {
			panic("Error updating memdb.");
		}

		if (mr != NULL) {
			mr->virt = 0U;
			mr->phys = 0U;
			mr->size = 0U;
		}
	}
out:
	virt_ret.e = ret;
	return virt_ret;
}
#endif

```

`hyp/core/partition_standard/templates/hypercalls.c.tmpl`:

```tmpl
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

\#if defined(HYPERCALLS)
\#include <assert.h>
\#include <hyptypes.h>

\#include <hypcall_def.h>
\#include <hypconstants.h>
\#include <hyprights.h>

\#include <compiler.h>
\#include <cspace.h>
\#include <cspace_lookup.h>
\#include <object.h>
\#include <partition_alloc.h>
\#include <thread.h>

\#include <events/object.h>

#for obj in $object_list
#set o = str(obj)
#if "no_partition_create_hypcall" in obj.config
#continue
#end if

hypercall_partition_create_${o}_result_t
hypercall_partition_create_${o}(cap_id_t src_partition_cap,
		cap_id_t cspace_cap)
{
	cspace_t *cspace = cspace_get_self();
	hypercall_partition_create_${o}_result_t ret = { 0 };

	partition_ptr_result_t p = cspace_lookup_partition(
		cspace, src_partition_cap,
		CAP_RIGHTS_PARTITION_OBJECT_CREATE);
	if (compiler_unexpected(p.e != OK)) {
		ret.error = p.e;
		goto out;
	}
	partition_t *src_partition = p.r;

	cspace_ptr_result_t c;
	c = cspace_lookup_cspace(cspace, cspace_cap,
		CAP_RIGHTS_CSPACE_CAP_CREATE);
	if (compiler_unexpected(c.e != OK)) {
		ret.error = c.e;
		goto out_partition_release;
	}
	cspace_t *dest_cspace = c.r;

	${o}_create_t params = { 0 };
	trigger_object_get_defaults_${o}_event(&params);

	${o}_ptr_result_t result =
		partition_allocate_${o}(src_partition, params);
	if (result.e != OK) {
		ret.error = result.e;
		goto out_cspace_release;
	}
	object_ptr_t obj_ptr;

	obj_ptr.${o} = result.r;
	cap_id_result_t capid_ret = cspace_create_master_cap(
		dest_cspace, obj_ptr, OBJECT_TYPE_${o.upper()});
	if (capid_ret.e != OK) {
		ret.error = capid_ret.e;
		object_put_${o}(obj_ptr.${o});
		goto out_cspace_release;
	}

	ret.error = OK;
	ret.new_cap = capid_ret.r;

out_cspace_release:
	object_put_cspace(dest_cspace);
out_partition_release:
	object_put_partition(src_partition);
out:
	return ret;
}
#end for
\#else
extern int unused;
\#endif

```

`hyp/core/partition_standard/templates/object.c.tmpl`:

```tmpl
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

\#include <hyptypes.h>
\#include <string.h>

\#include <hypcontainers.h>

\#include <atomic.h>
\#include <assert.h>
\#include <object.h>
\#include <partition.h>
\#include <partition_alloc.h>
\#include <rcu.h>
\#include <refcount.h>
\#include <spinlock.h>

\#include <events/object.h>

\#include "event_handlers.h"

#for obj in $object_list
#set o = str(obj)
#if o == "thread"
extern const size_t thread_size;
extern const size_t thread_align;
#end if

${o}_ptr_result_t
partition_allocate_${o}(partition_t *parent, ${o}_create_t create)
{
	void_ptr_result_t alloc_ret;
	${o}_ptr_result_t obj_ret;
	${o}_t *${o};

#if o == "thread"
	const size_t size = thread_size;
	const size_t align = thread_align;

	assert(size >= sizeof(${o}_t));
#else
	const size_t size = sizeof(${o}_t);
	const size_t align = alignof(${o}_t);
#end if
	alloc_ret = partition_alloc(parent, size, align);

	if (alloc_ret.e != OK) {
		obj_ret = ${o}_ptr_result_error(alloc_ret.e);
		goto allocate_${o}_error;
	}

	${o} = (${o}_t *)alloc_ret.r;
	(void)memset_s(${o}, size, 0, size);

	refcount_init(&${o}->header.refcount);
	spinlock_init(&${o}->header.lock);
	${o}->header.partition = object_get_partition_additional(parent);
	${o}->header.type = $obj.type_enum();
	atomic_init(&${o}->header.state, OBJECT_STATE_INIT);

	create.${o} = $o;

	error_t err = trigger_object_create_${o}_event(create);
	if (err != OK) {
		rcu_enqueue(&${o}->header.rcu_entry, $obj.rcu_destroy_enum());
		obj_ret = ${o}_ptr_result_error(err);
	} else {
		obj_ret = ${o}_ptr_result_ok(${o});
	}

allocate_${o}_error:
	return obj_ret;
}

rcu_update_status_t
partition_destroy_${o}(rcu_entry_t *entry)
{
	${o}_t *${o};
	object_header_t *header;
	rcu_update_status_t ret = rcu_update_status_default();

	header = object_header_container_of_rcu_entry(entry);
	${o} = ${o}_container_of_header(header);

	trigger_object_cleanup_${o}_event(&ret, ${o});

	partition_t *parent = ${o}->header.partition;
#if o == "thread"
	(void)partition_free(parent, ${o}, thread_size);
#else
	(void)partition_free(parent, ${o}, sizeof(${o}_t));
#end if
	object_put_partition(parent);

	return ret;
}

#end for

```

`hyp/core/partition_standard/templates/object.ev.tmpl`:

```tmpl
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

module partition_standard

#for obj in $object_list
#set o = str(obj)
subscribe rcu_update[$obj.rcu_destroy_enum()]
#set rcu_destroy_handler = "partition_destroy_{:s}(entry)".format(o)
        handler $rcu_destroy_handler
#end for

```

`hyp/core/partition_standard/templates/object.tc.tmpl`:

```tmpl
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

extend rcu_update_class enumeration {
#for obj in $object_list
#set o = str(obj)
	${o}_destroy;
#end for
};

```

`hyp/core/power/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

interface power
types power.tc
events power.ev
source power.c

```

`hyp/core/power/power.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

module power

subscribe boot_cold_init
	priority 100
	require_preempt_disabled

subscribe boot_cpu_warm_init
	priority first
	require_preempt_disabled

subscribe power_cpu_resume(was_poweroff)
	priority first
	require_preempt_disabled

subscribe power_cpu_suspend(state)
	priority last
	require_preempt_disabled

// This has higher priority than PSCI to avoid PSCI put the core into CPU_SUSPEND first
subscribe idle_yield
	priority 100
	require_preempt_disabled

subscribe timer_action[TIMER_ACTION_POWER_CPU_ON_RETRY]
	handler power_handle_timer_action(timer)
	require_preempt_disabled

#if defined(MODULE_VM_ROOTVM)
subscribe rootvm_started()
	require_preempt_disabled
#endif

subscribe boot_hypervisor_handover
	priority first

#if defined(POWER_START_ALL_CORES)
subscribe boot_hypervisor_start
	priority -1000
	require_preempt_disabled
#endif

```

`hyp/core/power/power.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

define power_voting structure {
	// Spinlock protecting the rest of this structure, and also the
	// CPU's entry in the power state array.
	lock		structure spinlock;
	// Number of votes to keep this core powered on.
	vote_count	type count_t;
	// Timer that is enqueued when an ERROR_RETRY error is returned
	// from platform_cpu_on().
	retry_timer	structure timer(contained);
	// Number of consecutive retries.
	retry_count	type count_t;
};

// Action for the timer above.
extend timer_action enumeration {
	power_cpu_on_retry;
};

define POWER_CPU_ON_RETRY_DELAY_NS constant type nanoseconds_t = 100000;
define MAX_CPU_ON_RETRIES constant type count_t = 2;

```

`hyp/core/power/src/power.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>

#include <hypcontainers.h>

#include <atomic.h>
#include <bitmap.h>
#include <cpulocal.h>
#include <ipi.h>
#include <panic.h>
#include <platform_cpu.h>
#include <power.h>
#include <preempt.h>
#include <rcu.h>
#include <scheduler.h>
#include <spinlock.h>
#include <timer_queue.h>
#include <util.h>

#include <events/power.h>

#include "event_handlers.h"

static ticks_t power_cpu_on_retry_delay_ticks;

static spinlock_t power_system_lock;
static BITMAP_DECLARE(PLATFORM_MAX_CORES, power_system_online_cpus)
	PROTECTED_BY(power_system_lock);
static platform_power_state_t
	power_system_suspend_state PROTECTED_BY(power_system_lock);

CPULOCAL_DECLARE_STATIC(power_voting_t, power_voting);

// This is protected by the lock in the corresponding power_voting_t structure,
// but must remain a separate array because it is exposed in crash minidumps.
CPULOCAL_DECLARE_STATIC(cpu_power_state_t, power_state);

const cpu_power_state_array_t *
power_get_cpu_states_for_debug(void)
{
	return &cpulocal_power_state;
}

void
power_handle_boot_cold_init(cpu_index_t boot_cpu)
{
	power_cpu_on_retry_delay_ticks =
		timer_convert_ns_to_ticks(POWER_CPU_ON_RETRY_DELAY_NS);
	assert(power_cpu_on_retry_delay_ticks != 0U);

	for (cpu_index_t cpu = 0U; cpu < PLATFORM_MAX_CORES; cpu++) {
		spinlock_init(&CPULOCAL_BY_INDEX(power_voting, cpu).lock);
		spinlock_acquire_nopreempt(
			&CPULOCAL_BY_INDEX(power_voting, cpu).lock);

		timer_init_object(
			&CPULOCAL_BY_INDEX(power_voting, cpu).retry_timer,
			TIMER_ACTION_POWER_CPU_ON_RETRY);
		CPULOCAL_BY_INDEX(power_voting, cpu).retry_count = 0U;

		// Initialize the boot CPU's vote count to 1 while booting to
		// prevent the cpu going to suspend. This will be decremented
		// once the rootvm setup is completed and the rootvm VCPU has
		// voted to keep the boot core powered on.
		CPULOCAL_BY_INDEX(power_voting, cpu).vote_count =
			(cpu == boot_cpu) ? 1U : 0U;

		CPULOCAL_BY_INDEX(power_state, cpu) =
			(cpu == boot_cpu) ? CPU_POWER_STATE_COLD_BOOT
					  : CPU_POWER_STATE_OFF;

		spinlock_release_nopreempt(
			&CPULOCAL_BY_INDEX(power_voting, cpu).lock);
	}

	spinlock_init(&power_system_lock);

	// FIXME:
	spinlock_acquire_nopreempt(&power_system_lock);
	bitmap_set(power_system_online_cpus, (index_t)boot_cpu);
	spinlock_release_nopreempt(&power_system_lock);
}

void
power_handle_boot_cpu_warm_init(void)
{
	spinlock_acquire_nopreempt(&CPULOCAL(power_voting).lock);
	cpu_power_state_t state = CPULOCAL(power_state);

	assert((state == CPU_POWER_STATE_COLD_BOOT) ||
	       (state == CPU_POWER_STATE_STARTED) ||
	       (state == CPU_POWER_STATE_SUSPEND));
	CPULOCAL(power_state) = CPU_POWER_STATE_ONLINE;

	if (state == CPU_POWER_STATE_STARTED) {
		trigger_power_cpu_online_event();

#if defined(DISABLE_PSCI_CPU_OFF) && DISABLE_PSCI_CPU_OFF
		power_voting_t *voting = &CPULOCAL(power_voting);
		voting->vote_count++;
#endif
	}
	spinlock_release_nopreempt(&CPULOCAL(power_voting).lock);

	// FIXME:
	spinlock_acquire_nopreempt(&power_system_lock);
	if (bitmap_empty(power_system_online_cpus, PLATFORM_MAX_CORES)) {
		// STARTED could be seen due to a last-cpu-suspend/cpu_on race.
		assert((state == CPU_POWER_STATE_STARTED) ||
		       (state == CPU_POWER_STATE_SUSPEND));
		trigger_power_system_resume_event(power_system_suspend_state);
	}
	bitmap_set(power_system_online_cpus, (index_t)cpulocal_get_index());
	spinlock_release_nopreempt(&power_system_lock);
}

error_t
power_handle_power_cpu_suspend(platform_power_state_t state)
{
	error_t	    err	   = OK;
	cpu_index_t cpu_id = cpulocal_get_index();

	// FIXME:
	spinlock_acquire_nopreempt(&power_system_lock);
	bitmap_clear(power_system_online_cpus, (index_t)cpu_id);
	if (bitmap_empty(power_system_online_cpus, PLATFORM_MAX_CORES)) {
		power_system_suspend_state = state;
		err = trigger_power_system_suspend_event(state);
		if (err != OK) {
			bitmap_set(power_system_online_cpus, (index_t)cpu_id);
		}
	}
	spinlock_release_nopreempt(&power_system_lock);

	if (err == OK) {
		spinlock_acquire_nopreempt(&CPULOCAL(power_voting).lock);
		assert(CPULOCAL(power_state) == CPU_POWER_STATE_ONLINE);
		CPULOCAL(power_state) = CPU_POWER_STATE_SUSPEND;
		spinlock_release_nopreempt(&CPULOCAL(power_voting).lock);
	}

	return err;
}

void
power_handle_power_cpu_resume(bool was_poweroff)
{
	// A cpu that was warm booted updates its state in the cpu warm-boot
	// event.
	if (!was_poweroff) {
		spinlock_acquire_nopreempt(&CPULOCAL(power_voting).lock);
		assert(CPULOCAL(power_state) == CPU_POWER_STATE_SUSPEND);
		CPULOCAL(power_state) = CPU_POWER_STATE_ONLINE;
		spinlock_release_nopreempt(&CPULOCAL(power_voting).lock);

		// FIXME:
		spinlock_acquire_nopreempt(&power_system_lock);
		if (bitmap_empty(power_system_online_cpus,
				 PLATFORM_MAX_CORES)) {
			trigger_power_system_resume_event(
				power_system_suspend_state);
		}
		bitmap_set(power_system_online_cpus,
			   (index_t)cpulocal_get_index());
		spinlock_release_nopreempt(&power_system_lock);
	} else {
		spinlock_acquire_nopreempt(&power_system_lock);
		// power_system_online_cpus should be updated in the warm init
		// event.
		assert(!bitmap_empty(power_system_online_cpus,
				     PLATFORM_MAX_CORES));
		spinlock_release_nopreempt(&power_system_lock);
	}
}

static error_t
power_try_cpu_on(power_voting_t *voting, cpu_index_t cpu)
	REQUIRE_LOCK(voting->lock)
{
	error_t ret;

	if (!platform_cpu_exists(cpu)) {
		ret = ERROR_ARGUMENT_INVALID;
		goto out;
	}

	cpu_power_state_t *state = &CPULOCAL_BY_INDEX(power_state, cpu);
	if ((*state != CPU_POWER_STATE_OFF) &&
	    (*state != CPU_POWER_STATE_OFFLINE)) {
		// CPU has already been started, or didn't get to power off.
		ret = OK;
		goto out;
	}

	ret = platform_cpu_on(cpu);

	if (ret == OK) {
		// Mark the CPU as started so we don't call cpu_on twice.
		*state		    = CPU_POWER_STATE_STARTED;
		voting->retry_count = 0U;
		goto out;
	} else if ((ret == ERROR_RETRY) &&
		   (voting->retry_count < MAX_CPU_ON_RETRIES)) {
		// We are racing with a power-off, and it is too late to prevent
		// the power-off completing. We need to wait until power-off is
		// complete and then retry. Enqueue the retry timer, if it is
		// not already queued.
		if (!timer_is_queued(&voting->retry_timer)) {
			timer_enqueue(&voting->retry_timer,
				      timer_get_current_timer_ticks() +
					      power_cpu_on_retry_delay_ticks);
		}

		// If we're racing with power-off, that means the CPU is
		// functional and the power-on should not fail, so report
		// success to the caller. If the retry does fail, we panic.
		ret = OK;
	} else if (ret == ERROR_RETRY) {
		// We ran out of retry attempts.
		ret = ERROR_FAILURE;
	} else {
		// platform_cpu_on() failed and cannot be retried; just return
		// the error status.
	}

out:
	return ret;
}

error_t
power_vote_cpu_on(cpu_index_t cpu)
{
	error_t ret;

	assert(cpulocal_index_valid(cpu));
	power_voting_t *voting = &CPULOCAL_BY_INDEX(power_voting, cpu);

	spinlock_acquire(&voting->lock);
	if (voting->vote_count == 0U) {
		ret = power_try_cpu_on(voting, cpu);
		if (ret != OK) {
			goto out;
		}
	}

	voting->vote_count++;
	ret = OK;

out:
	spinlock_release(&voting->lock);
	return ret;
}

void
power_vote_cpu_off(cpu_index_t cpu)
{
	assert(cpulocal_index_valid(cpu));
	power_voting_t *voting = &CPULOCAL_BY_INDEX(power_voting, cpu);

	spinlock_acquire(&voting->lock);
	assert(voting->vote_count > 0U);
	voting->vote_count--;

	if (voting->vote_count == 0U) {
		// Any outstanding retries can be cancelled.
		voting->retry_count = 0U;
		timer_dequeue(&voting->retry_timer);

		// Send an IPI to rerun the idle handlers in case the CPU
		// is already idle in WFI or suspend.
		ipi_one(IPI_REASON_IDLE, cpu);
	}
	spinlock_release(&voting->lock);
}

idle_state_t
power_handle_idle_yield(bool in_idle_thread)
{
	idle_state_t idle_state = IDLE_STATE_IDLE;

	if (!in_idle_thread) {
		goto out;
	}

	if (rcu_has_pending_updates()) {
		goto out;
	}

	power_voting_t *voting = &CPULOCAL(power_voting);
	spinlock_acquire_nopreempt(&voting->lock);
	if (voting->vote_count == 0U) {
		error_t err = OK;

		spinlock_acquire_nopreempt(&power_system_lock);
		cpu_index_t cpu_id = cpulocal_get_index();
		bitmap_clear(power_system_online_cpus, (index_t)cpu_id);
		if (bitmap_empty(power_system_online_cpus,
				 PLATFORM_MAX_CORES)) {
			power_system_suspend_state =
				(platform_power_state_t){ 0 };
			err = trigger_power_system_suspend_event(
				power_system_suspend_state);
			if (err != OK) {
				bitmap_set(power_system_online_cpus,
					   (index_t)cpu_id);
			}
		}
		spinlock_release_nopreempt(&power_system_lock);

		if (err == OK) {
			assert(CPULOCAL(power_state) == CPU_POWER_STATE_ONLINE);
			trigger_power_cpu_offline_event();
			CPULOCAL(power_state) = CPU_POWER_STATE_OFFLINE;
			spinlock_release_nopreempt(&voting->lock);

			platform_cpu_off();

			idle_state = IDLE_STATE_WAKEUP;
		} else {
			spinlock_release_nopreempt(&voting->lock);
		}
	} else {
		spinlock_release_nopreempt(&voting->lock);
	}

out:
	return idle_state;
}

bool
power_handle_timer_action(timer_t *timer)
{
	assert(timer != NULL);

	power_voting_t *voting = power_voting_container_of_retry_timer(timer);
	cpu_index_t	cpu    = CPULOCAL_PTR_INDEX(power_voting, voting);

	spinlock_acquire_nopreempt(&voting->lock);
	error_t ret = OK;
	if (voting->vote_count > 0U) {
		voting->retry_count++;
		ret = power_try_cpu_on(voting, cpu);
	}
	spinlock_release_nopreempt(&voting->lock);

	if (ret != OK) {
		panic("Failed to power on a CPU that was previously on");
	}

	return true;
}

#if defined(MODULE_VM_ROOTVM)
// The Boot CPU power count is initialised to 1. Decrement the count after the
// root VM initialization.
void
power_handle_rootvm_started(void)
{
	power_vote_cpu_off(cpulocal_get_index());
}
#endif

void
power_handle_boot_hypervisor_handover(void)
{
	// Ensure the running core is the only core online. There is no easy way
	// to do this race-free, but it doesn't really matter for our purpose.
	count_t on_count = 0;
	for (cpu_index_t cpu = 0U; cpu < PLATFORM_MAX_CORES; cpu++) {
		cpu_power_state_t state = CPULOCAL_BY_INDEX(power_state, cpu);
		if ((state != CPU_POWER_STATE_OFF) &&
		    (state != CPU_POWER_STATE_OFFLINE)) {
			on_count++;
		}
	}

	if (on_count != 1U) {
		panic("Hypervisor hand-over requested with multiple CPUs on");
	}
}

#if defined(POWER_START_ALL_CORES)
void
power_handle_boot_hypervisor_start(void)
{
	cpu_index_t boot_cpu = cpulocal_get_index();

	for (cpu_index_t cpu = 0U; cpulocal_index_valid(cpu); cpu++) {
		if (cpu == boot_cpu) {
			continue;
		}

		power_vote_cpu_on(cpu);
	}
}
#endif

```

`hyp/core/preempt/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

interface preempt
source preempt.c
types preempt.tc
events preempt.ev
macros preempt_attrs.h

```

`hyp/core/preempt/include/preempt_attrs.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#ifdef __EVENTS_DSL__
#define acquire_preempt_disabled acquire_lock preempt_disabled
#define release_preempt_disabled release_lock preempt_disabled
#define require_preempt_disabled require_lock preempt_disabled
#define exclude_preempt_disabled exclude_lock preempt_disabled
#else
#define ACQUIRE_PREEMPT_DISABLED ACQUIRE_LOCK(preempt_disabled)
#define TRY_ACQUIRE_PREEMPT_DISABLED(success)                                  \
	TRY_ACQUIRE_LOCK(success, preempt_disabled)
#define RELEASE_PREEMPT_DISABLED RELEASE_LOCK(preempt_disabled)
#define REQUIRE_PREEMPT_DISABLED REQUIRE_LOCK(preempt_disabled)
#define EXCLUDE_PREEMPT_DISABLED EXCLUDE_LOCK(preempt_disabled)
#endif

```

`hyp/core/preempt/preempt.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

interface preempt

handled_event preempt_interrupt

handled_event preempt_abort

module preempt

subscribe boot_cpu_early_init
	acquire_preempt_disabled

subscribe boot_cpu_start
	priority last
	release_preempt_disabled

subscribe thread_start
	priority first
	acquire_preempt_disabled

subscribe thread_entry_from_user
	priority last
	release_preempt_disabled

subscribe thread_exit_to_user
	priority first
	exclude_preempt_disabled
	acquire_preempt_disabled

subscribe scheduler_stop()
	priority first
	acquire_preempt_disabled

```

`hyp/core/preempt/preempt.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

define preempt_bits enumeration {
	count_max = 15;
	cpu_init;
	in_interrupt;
	abort_kernel;
};

```

`hyp/core/preempt/src/preempt.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// Preemption control functions for preemptible configurations.

#include <assert.h>
#include <hyptypes.h>

#include <compiler.h>
#include <irq.h>
#include <preempt.h>
#include <scheduler.h>
#include <trace.h>
#include <util.h>

#include <events/preempt.h>

#include <asm/interrupt.h>

#include "event_handlers.h"

static _Thread_local count_t preempt_disable_count;

static const count_t preempt_count_mask =
	(count_t)util_bit((index_t)PREEMPT_BITS_COUNT_MAX + 1U) - 1U;
static const count_t preempt_count_max = preempt_count_mask;

static_assert(PREEMPT_BITS_COUNT_MAX < PREEMPT_BITS_CPU_INIT,
	      "PREEMPT_BITS_CPU_INIT invalid");
static_assert(PREEMPT_BITS_COUNT_MAX < PREEMPT_BITS_IN_INTERRUPT,
	      "PREEMPT_BITS_IN_INTERRUPT invalid");
static_assert(PREEMPT_BITS_COUNT_MAX < PREEMPT_BITS_ABORT_KERNEL,
	      "PREEMPT_BITS_ABORT_KERNEL invalid");

static const count_t preempt_cpu_init =
	(count_t)util_bit((index_t)PREEMPT_BITS_CPU_INIT);
static const count_t preempt_in_interrupt =
	(count_t)util_bit((index_t)PREEMPT_BITS_IN_INTERRUPT);
static const count_t preempt_abort_kernel =
	(count_t)util_bit((index_t)PREEMPT_BITS_ABORT_KERNEL);

void
preempt_handle_boot_cpu_early_init(void) LOCK_IMPL
{
	// Prevent an accidental preempt-enable during the boot sequence.
	preempt_disable_count |= preempt_cpu_init;
}

void
preempt_handle_boot_cpu_start(void) LOCK_IMPL
{
	// Boot has finished; allow preemption to be enabled.
	assert((preempt_disable_count & preempt_cpu_init) != 0U);
	preempt_disable_count &= ~preempt_cpu_init;
}

void
preempt_handle_thread_start(void) LOCK_IMPL
{
	// Arrange for preemption to be enabled by the first
	// preempt_enable() call.
	//
	// Note that preempt_disable_count is briefly 0 in each newly
	// started thread even though preemption is always disabled
	// across context switches. To avoid problems we must ensure
	// that this setup is done as early as possible in new threads,
	// before anything that might call preempt_disable().
	preempt_disable_count = 1U;
}

void
preempt_handle_thread_entry_from_user(thread_entry_reason_t reason)
{
	assert(preempt_disable_count == 1U);

	if (reason == THREAD_ENTRY_REASON_INTERRUPT) {
		preempt_disable_count |= preempt_in_interrupt;
	}

	preempt_enable();
#if defined(VERBOSE) && VERBOSE
	assert_preempt_enabled();
#endif
}

void
preempt_handle_thread_exit_to_user(thread_entry_reason_t reason)
{
	assert_preempt_enabled();
	preempt_disable();

	if (reason == THREAD_ENTRY_REASON_INTERRUPT) {
		preempt_disable_count &= ~preempt_in_interrupt;
	}

	assert(preempt_disable_count == 1U);
}

void
preempt_disable(void) LOCK_IMPL
{
	assert((preempt_disable_count & preempt_count_mask) <
	       preempt_count_max);
	asm_interrupt_disable_acquire(&preempt_disable_count);
	preempt_disable_count++;
	assert_preempt_disabled();
}

void
preempt_enable(void) LOCK_IMPL
{
	assert((preempt_disable_count & preempt_count_mask) > 0U);
	preempt_disable_count--;
	if (preempt_disable_count == 0U) {
		asm_interrupt_enable_release(&preempt_disable_count);
	}
}

bool
preempt_interrupt_dispatch(void) LOCK_IMPL
{
	preempt_disable_count |= preempt_in_interrupt;

	if (!trigger_preempt_interrupt_event() && irq_interrupt_dispatch()) {
		if ((preempt_disable_count & preempt_count_mask) > 0U) {
			// Preemption is disabled; we are in some context that
			// needs to enable interrupts but can't permit a context
			// switch, e.g. the idle loop. Trigger a deferred
			// reschedule.
			scheduler_trigger();
		} else {
			(void)scheduler_schedule();
		}
	}

	preempt_disable_count &= ~preempt_in_interrupt;

	return false;
}

void
preempt_disable_in_irq(void) LOCK_IMPL
{
	assert((preempt_disable_count & preempt_in_interrupt) != 0U);
}

void
preempt_enable_in_irq(void) LOCK_IMPL
{
	assert((preempt_disable_count & preempt_in_interrupt) != 0U);
}

bool
preempt_abort_dispatch(void) LOCK_IMPL
{
	preempt_disable_count |= preempt_in_interrupt;

	bool ret = trigger_preempt_abort_event();

	preempt_disable_count &= ~preempt_in_interrupt;

	return ret;
}

void
preempt_handle_scheduler_stop(void) LOCK_IMPL
{
	count_t old_count = preempt_disable_count;
	asm_interrupt_disable_acquire(&preempt_disable_count);

	// Set the abort bit and clear the current count, to avoid an unbounded
	// recursion in case preempt_disable() fails the count overflow
	// assertion and the abort path calls preempt_disable() again.
	preempt_disable_count = preempt_abort_kernel;

	// Log the original preempt count.
	TRACE(DEBUG, INFO, "preempt: force disabled; previous count was {:#x}",
	      old_count);
}

void
assert_preempt_disabled(void)
{
	assert(preempt_disable_count != 0U);
}

void
assert_preempt_enabled(void)
{
	assert((preempt_disable_count & preempt_count_mask) == 0U);
}

```

`hyp/core/preempt_null/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

interface preempt
source preempt_null.c
configs PREEMPT_NULL=1

```

`hyp/core/preempt_null/src/preempt_null.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// Dummy preemption control functions for non-preemptible configurations.

#include <hyptypes.h>

#include <compiler.h>
#include <log.h>
#include <panic.h>
#include <preempt.h>
#include <trace.h>

void
preempt_disable(void)
{
}

void
preempt_enable(void)
{
}

bool
preempt_interrupt_dispatch(void)
{
#if !defined(NDEBUG)
	panic("Hypervisor interrupts should be disabled!");
#else
	LOG(ERROR, WARN, "Hypervisor interrupts should be disabled!");
#endif
	return true;
}

void
assert_preempt_disabled(void)
{
}

void
assert_preempt_enabled(void)
{
}

```

`hyp/core/rcu_bitmap/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

interface rcu
types rcu.tc
events rcu.ev
source rcu_bitmap.c
base_module hyp/core/rcu_sync

```

`hyp/core/rcu_bitmap/rcu.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

module rcu_bitmap

// Events that activate a CPU
subscribe preempt_interrupt()
	require_preempt_disabled
subscribe thread_entry_from_user()
	require_preempt_disabled
subscribe thread_context_switch_pre()
	require_preempt_disabled
subscribe power_cpu_online
	require_preempt_disabled

// Events that deactivate a CPU
subscribe idle_yield()
	require_preempt_disabled
#if defined(INTERFACE_VCPU)
subscribe vcpu_block_finish()
	require_preempt_disabled
#endif
subscribe thread_exit_to_user()
	require_preempt_disabled
subscribe power_cpu_suspend()
	require_preempt_disabled

// Events that quiesce a CPU but don't activate or deactivate it
subscribe scheduler_quiescent
	require_preempt_disabled

// Support for CPU hotplug is currently unimplemented. However, it is not used
// in the hypervisor at present, so that should not be a problem. We register
// this handler to ensure a link error if hotplug is ever enabled.
subscribe power_cpu_offline
	require_preempt_disabled

// Handlers for internal IPIs
subscribe ipi_received[IPI_REASON_RCU_QUIESCE]
	handler rcu_bitmap_quiesce()
	require_preempt_disabled
subscribe ipi_received[IPI_REASON_RCU_NOTIFY]
	handler rcu_bitmap_notify()
	require_preempt_disabled
subscribe ipi_received[IPI_REASON_RCU_UPDATE]
	handler rcu_bitmap_update()
	require_preempt_disabled

```

`hyp/core/rcu_bitmap/rcu.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// RCU based on a global quiescent state bitmap, as described in the PDCS '98
// paper by MacKenney and Slingwine.
//
// The paper describes this as a suboptimal solution, but the "much better"
// state counter algorithm used in Dynix/ptx (since 1994) relies on a periodic
// timer tick on every CPU, as do the later improvements used in Linux. The
// hypervisor currently does not have a periodic tick and we don't want to add
// one just for RCU. The original state counter algorithm also assumes that
// quiescent states are already counted for performance monitoring, which is
// also not currently the case in the hypervisor.
//
// To avoid the performance issue with bitmaps that was identified in the
// paper, in which a frequently quiescent CPU performs redundant and expensive
// atomic bit clear operations on the global bitmap, we keep a count of CPUs
// with pending updates, and track quiescent states only when this count is
// nonzero. When the count transitions from zero to nonzero, we IPI all
// online CPUs to ensure that their quiescent state tracking is updated.
//
// The hypervisor currently uses RCU only for object existence locks and, in
// some cases, detaching object links. These do not happen as frequently as
// as in a general purpose kernel such as Linux or Dynix/ptx, so the overhead
// of RCU when updates are queued is of relatively little importance, as long
// as the overhead is low when there are no updates.
//
// Also, we make use of some features described as part of the state counter
// implementation that are not strictly specific to that algorithm: per-CPU
// update batches, deferral of batch processing using a software interrupt
// (i.e. ipi_relaxed()), and a generation count.

#include <types/bitmap.h>
#include <asm/cpu.h>

extend rcu_entry structure {
	// We should pack the class in here so we can use a unified queue.
	// FIXME:
	next pointer structure rcu_entry;
};

extend ipi_reason enumeration {
	// Force a quiescent state. This is sent to all CPUs in the active
	// bitmap after the waiter count transitions from 0 to 1.
	rcu_quiesce;

	// Trigger a grace period check. This is sent to any remote CPU that
	// is known to be waiting for a grace period that has completed. It
	// is also asserted (relaxed) on the current CPU when a new update is
	// queued, and when a new grace period is requested.
	rcu_notify;

	// Process the current batch of updates. This is asserted (relaxed) on
	// the current CPU when updates are moved into the current batch at
	// the end of a grace period.
	rcu_update;
};

// Internal rcu_bitmap structures

// A batch of RCU updates, from one grace period on one CPU.
define rcu_batch structure {
	// We have a list head for each update class.
	// FIXME:
	heads array(maxof(enumeration rcu_update_class) + 1) pointer
		structure rcu_entry;
};

// An atomically accessible structure representing the current grace period.
// Note that we use a uint32 bitmap here rather than the generic bitmap type
// because we want to be able to pack this into 64 bits, so that 64-bit
// machines can access it with atomic load and store accesses.
// FIXME:
define rcu_grace_period structure(aligned(8)) {
	generation type count_t;
	cpu_bitmap uint32;
};

// The global state of RCU.
define rcu_state structure {
	// The number of CPUs that may have waiting updates. When this is 0
	// (which is the common case in the hypervisor), all quiescent state
	// detection and processing is skipped.
	//
	// This is strictly an upper bound; i.e. it is incremented before
	// updates are queued and decremented after they are dequeued, so it
	// never reaches 0 while CPUs are waiting.
	waiter_count type count_t(atomic);

	// The current grace period's generation number and cpu bitmap.
	current_period structure rcu_grace_period(atomic);

	// The highest grace period number any CPU is waiting for.
	max_target type count_t(atomic);

	// The set of CPUs that would need to acknowledge a grace period if it
	// started now. This excludes CPUs that are offline or suspended. It
	// also excludes CPUs that are in userspace or the idle thread.
	active_cpus uint32(atomic);
};

// The CPU-local state of RCU.
define rcu_cpu_state structure(aligned(1 << CPU_L1D_LINE_BITS)) {
	// The total number of updates in this CPU's batches.
	//
	// This is strictly an upper bound; i.e. it is incremented before
	// updates are queued and decremented after they are dequeued, so it
	// never reaches 0 while updates are queued.
	// FIXME: may not need to be atomic?
	update_count type count_t(atomic);

	// Local cache of this CPU's bit in the active set. This is used to
	// avoid having to touch the shared variable frequently when RCU is
	// idle. It should never be accessed across CPUs.
	is_active bool;

	// Local cache of whether ready_batch is non-empty. This is checked
	// in rcu_bitmap_notify(), to ensure that rcu_bitmap_update() is
	// completed first regardless of IPI processing order.
	ready_updates bool;

	// The grace period this CPU is currently waiting to reach. This is
	// atomic because it may be read lock-free by remote CPUs that
	// complete a grace period, to determine whether to IPI this CPU.
	target type count_t(atomic);

	// Update batch ready for processing now; if it is non-empty then an
	// IPI with reason RCU_UPDATE should have been raised to process it,
	// and ready_updates should be true.
	ready_batch structure rcu_batch;

	// Update batch that will be processed when target_period is reached.
	waiting_batch structure rcu_batch;

	// Update batch that is being accumulated for processing at the end of
	// the next grace period.
	next_batch structure rcu_batch;
};

```

`hyp/core/rcu_bitmap/src/rcu_bitmap.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>

#include <atomic.h>
#include <compiler.h>
#include <cpulocal.h>
#include <enum.h>
#include <idle.h>
#include <ipi.h>
#include <preempt.h>
#include <rcu.h>
#include <scheduler.h>
#include <util.h>

#include <events/rcu.h>

#include "event_handlers.h"

static_assert(PLATFORM_MAX_CORES <= 32U, "PLATFORM_MAX_CORES > 32");

static rcu_state_t rcu_state;
CPULOCAL_DECLARE_STATIC(rcu_cpu_state_t, rcu_state);

// The grace period counts can wrap around, so we can't use a simple comparison
// to distinguish between a token in the past and a token in the future. When
// comparing two tokens, we use the following value as a threshold difference,
// above which the token is presumed to have wrapped around.
static const count_t a_long_time =
	(count_t)util_bit((sizeof(count_t) * 8U) - 1U);

// Compare two counts, and return true if the first is before the second,
// assuming that both counts belong to CPUs actively participating in the
// counter ring. (This is effectively the same as a signed comparison, but
// performed manually on unsigned values because the behaviour of signed
// overflow is undefined.)
static inline bool
is_before(count_t a, count_t b)
{
	return (a - b) >= a_long_time;
}

void
rcu_read_start(void) LOCK_IMPL
{
	preempt_disable();
	trigger_rcu_read_start_event();
}

void
rcu_read_finish(void) LOCK_IMPL
{
	trigger_rcu_read_finish_event();
	preempt_enable();
}

static void
rcu_bitmap_refresh_active(void)
{
	uint32_t active_cpus = atomic_load_relaxed(&rcu_state.active_cpus);
	while (active_cpus != 0U) {
		cpu_index_t cpu = (cpu_index_t)compiler_ctz(active_cpus);
		// Request a reschedule, since it will either switch threads,
		// or trigger a scheduler quiescent event. We don't directly
		// send an IPI_REASON_RCU_QUIESCE here since when in the idle
		// thread, it may not return true and won't exit the fast-IPI
		// loop, so the idle_yield event won't be rerun and the CPU
		// won't be deactivated.
		ipi_one(IPI_REASON_RESCHEDULE, cpu);
		active_cpus &= (uint32_t)(~util_bit(cpu));
	}
}

static inline bool
rcu_bitmap_should_run(void)
{
	bool should_run = compiler_unexpected(
		atomic_load_relaxed(&rcu_state.waiter_count) > 0U);
	if (should_run) {
		atomic_thread_fence(memory_order_acquire);
	}
	return should_run;
}

void
rcu_enqueue(rcu_entry_t *rcu_entry, rcu_update_class_t rcu_update_class)
{
	preempt_disable();

	cpu_index_t	 cpu	  = cpulocal_get_index();
	rcu_cpu_state_t *my_state = &CPULOCAL_BY_INDEX(rcu_state, cpu);
	rcu_batch_t	*batch	  = &my_state->next_batch;

	if (atomic_fetch_add_explicit(&my_state->update_count, 1U,
				      memory_order_relaxed) == 0U) {
		if (atomic_fetch_add_explicit(&rcu_state.waiter_count, 1U,
					      memory_order_relaxed) == 0U) {
			// CPUs may have stopped tracking quiescent states
			// because there were no waiters, so prod them all.
			//
			// Note that any CPU sitting in idle or running in
			// a lower EL will take itself out of both the current
			// and active sets in response to this, allowing us
			// to ignore it until it starts doing something.
			rcu_bitmap_refresh_active();
		}
	}

	rcu_entry->next		       = batch->heads[rcu_update_class];
	batch->heads[rcu_update_class] = rcu_entry;

	// Trigger a relaxed IPI to request a new GP if possible. We could call
	// rcu_bitmap_notify() directly here, but using an IPI to defer it will
	// improve batching when there is no GP already in progress.
	ipi_one_relaxed(IPI_REASON_RCU_NOTIFY, cpu);

	preempt_enable();
}

// Events that activate a CPU (i.e. mark it as needing to ack GPs)
static void
rcu_bitmap_activate_cpu(void) REQUIRE_PREEMPT_DISABLED
{
	assert_cpulocal_safe();
	cpu_index_t	 cpu	  = cpulocal_get_index();
	uint32_t	 cpu_bit  = (uint32_t)util_bit(cpu);
	rcu_cpu_state_t *my_state = &CPULOCAL_BY_INDEX(rcu_state, cpu);

	if (compiler_unexpected(!my_state->is_active)) {
		// We're not in the active CPU set. Add ourselves.
		my_state->is_active = true;

		(void)atomic_fetch_or_explicit(&rcu_state.active_cpus, cpu_bit,
					       memory_order_relaxed);

		// Fence to ensure that we are in the active CPU set before
		// any other memory access that might cause this CPU to actually
		// need to be in that set (i.e. loads in RCU critical sections),
		// so that any new grace period that starts after such accesses
		// will see this CPU as active. This must be a seq_cst fence to
		// order loads after stores.
		//
		// The matching fence is in rcu_bitmap_quiesce(), when (and if)
		// it reads the active bitmap to copy it to the current bitmap.
		atomic_thread_fence(memory_order_seq_cst);
	}
}

void
rcu_bitmap_handle_thread_entry_from_user(void)
{
	rcu_bitmap_activate_cpu();
}

bool
rcu_bitmap_handle_preempt_interrupt(void)
{
	rcu_bitmap_activate_cpu();

	return false;
}

error_t
rcu_bitmap_handle_thread_context_switch_pre(void)
{
	if (thread_get_self()->kind == THREAD_KIND_IDLE) {
		rcu_bitmap_activate_cpu();
	}

	if (compiler_unexpected(rcu_bitmap_should_run())) {
		(void)ipi_clear(IPI_REASON_RCU_QUIESCE);
		if (rcu_bitmap_quiesce()) {
			scheduler_trigger();
		}
	}

	return OK;
}

void
rcu_bitmap_handle_power_cpu_online(void)
{
	rcu_bitmap_activate_cpu();
}

// Events that deactivate a CPU (i.e. mark it as not needing to ack GPs)
static void
rcu_bitmap_deactivate_cpu(void) REQUIRE_PREEMPT_DISABLED
{
	assert_preempt_disabled();
	cpu_index_t	 cpu	  = cpulocal_get_index();
	uint32_t	 cpu_bit  = (uint32_t)util_bit(cpu);
	rcu_cpu_state_t *my_state = &CPULOCAL_BY_INDEX(rcu_state, cpu);

	my_state->is_active = false;

	// Remove ourselves from the active set. Release ordering is needed to
	// ensure that it is done after the end of any critical sections.
	// However, it does not need ordering relative to the quiesce below;
	// if it happens late then at worst we might get a redundant IPI.
	(void)atomic_fetch_and_explicit(&rcu_state.active_cpus, ~cpu_bit,
					memory_order_relaxed);

	// This sequential consistency fence matches the one in
	// rcu_bitmap_quiesce when a new grace period starts, to ensure that
	// either this CPU goes first and clears its active bit (and the other
	// CPU sends us a quiesce IPI), or the other CPU goes first and starts
	// the new grace period before the quiesce.
	atomic_thread_fence(memory_order_seq_cst);

	(void)ipi_clear(IPI_REASON_RCU_QUIESCE);
	if (rcu_bitmap_quiesce()) {
		scheduler_trigger();
	}
}

idle_state_t
rcu_bitmap_handle_idle_yield(void)
{
	if (compiler_unexpected(rcu_bitmap_should_run())) {
		rcu_bitmap_deactivate_cpu();
	}

	return IDLE_STATE_IDLE;
}

#if defined(INTERFACE_VCPU)
void
rcu_bitmap_handle_vcpu_block_finish(void)
{
	rcu_bitmap_activate_cpu();
}
#endif

void
rcu_bitmap_handle_thread_exit_to_user(void)
{
	if (compiler_unexpected(rcu_bitmap_should_run())) {
		rcu_bitmap_deactivate_cpu();
	}
}

error_t
rcu_bitmap_handle_power_cpu_suspend(void)
{
	error_t ret = OK;

	rcu_cpu_state_t *my_state = &CPULOCAL(rcu_state);
	if (atomic_load_relaxed(&my_state->update_count) != 0U) {
		// Delay suspend, we still have pending updates on this CPU.
		ret = ERROR_BUSY;
	} else {
		// Always run update processing, even if there are currently no
		// pending updates. This is to prevent us being woken spuriously
		// later, which is much more expensive than a redundant
		// quiesce().
		rcu_bitmap_deactivate_cpu();
	}

	return ret;
}

// Events that quiesce a CPU but don't activate or deactivate it
void
rcu_bitmap_handle_scheduler_quiescent(void)
{
	(void)ipi_clear(IPI_REASON_RCU_QUIESCE);
	if (rcu_bitmap_quiesce()) {
		scheduler_trigger();
	}
}

// Handlers for internal IPIs
bool
rcu_bitmap_quiesce(void)
{
	assert_preempt_disabled();
	cpu_index_t this_cpu = cpulocal_get_index();
	uint32_t    cpu_bit  = (uint32_t)util_bit(this_cpu);
	bool	    new_period;
	bool	    reschedule = false;

	rcu_grace_period_t current_period =
		atomic_load_acquire(&rcu_state.current_period);
	rcu_grace_period_t next_period;

	do {
		next_period = current_period;

		next_period.cpu_bitmap &= ~cpu_bit;

		if (next_period.cpu_bitmap != 0U) {
			// There are still other CPUs to wait for, so we are not
			// starting a new period.
			new_period = false;
		} else {
			// We're the last CPU to acknowledge the current period.
			// Start a new one if there is a CPU that hasn't reached
			// its target yet.
			new_period =
				atomic_load_relaxed(&rcu_state.max_target) !=
				current_period.generation;

			if (new_period) {
				// Fence to ensure that the load of the new
				// active CPU set occurs after any stores on
				// this CPU that must occur before a new grace
				// period starts. This matches the fence in
				// rcu_bitmap_activate_cpu().
				//
				// Note that stores on other CPUs are ordered by
				// the acquire operation on the CPU bitmap load
				// on this CPU and the release operation on the
				// CPU bitmap store on the other CPUs (below).
				atomic_thread_fence(memory_order_seq_cst);

				next_period.cpu_bitmap = atomic_load_relaxed(
					&rcu_state.active_cpus);
				next_period.generation++;
			}
		}
	} while (!atomic_compare_exchange_strong_explicit(
		&rcu_state.current_period, &current_period, next_period,
		memory_order_acq_rel, memory_order_acquire));

	if (new_period) {
		// This matches the thread fence in rcu_bitmap_deactivate_cpu.
		atomic_thread_fence(memory_order_seq_cst);

		// Check the CPUs that have raced with us in deactivate.
		uint32_t cpus_needing_quiesce =
			next_period.cpu_bitmap &
			~atomic_load_relaxed(&rcu_state.active_cpus);

		// Successfully started a new period. Look for any remote CPUs
		// that may be waiting for it, and IPI them.
		for (cpu_index_t cpu = 0U; cpu < PLATFORM_MAX_CORES; cpu++) {
			if (cpu == this_cpu) {
				continue;
			}
			count_t target = atomic_load_relaxed(
				&CPULOCAL_BY_INDEX(rcu_state, cpu).target);
			if (!is_before(next_period.generation, target)) {
				ipi_one(IPI_REASON_RCU_NOTIFY, cpu);
			}
			// Handle any new CPUs needing quiesce due to a race
			// where they are deactivating themselves and us
			// reading the active_cpus for the next grace period
			// above.
			if ((cpus_needing_quiesce & util_bit(cpu)) != 0U) {
				ipi_one(IPI_REASON_RCU_QUIESCE, cpu);
			}
		}

		// Process the grace period completion on the current CPU.
		reschedule = rcu_bitmap_notify();

		// Trigger another quiesce on the current CPU.
		ipi_one_relaxed(IPI_REASON_RCU_QUIESCE, this_cpu);
	}

	return reschedule;
}

static void
rcu_bitmap_request_grace_period(rcu_cpu_state_t *my_state, count_t current_gen)
	REQUIRE_PREEMPT_DISABLED
{
	assert_preempt_disabled();

	// We need to wait for the next grace period (not the current one) to
	// end, because we may have enqueued new updates during the current
	// period. Therefore our target is the period after the next.
	count_t target = current_gen + 2U;
	atomic_store_relaxed(&my_state->target, target);

	// Update the max target period to be at least our new target.
	count_t old_max_target = atomic_load_relaxed(&rcu_state.max_target);
	do {
		if (is_before(target, old_max_target)) {
			// We don't need to update the max target.
			break;
		}
	} while (!atomic_compare_exchange_weak_explicit(
		&rcu_state.max_target, &old_max_target, target,
		memory_order_relaxed, memory_order_relaxed));
}

bool
rcu_bitmap_notify(void)
{
	bool reschedule = false;

	assert_preempt_disabled();

	rcu_cpu_state_t *my_state = &CPULOCAL(rcu_state);

	// If there are no updates queued on this CPU, do nothing.
	if (atomic_load_relaxed(&my_state->update_count) == 0U) {
		goto out;
	}

	// Update always needs to be handled before notify, to avoid having to
	// merge the ready batches. Note that we can't check the result of
	// ipi_clear() here, because that is not safe in an IPI handler.
	if (my_state->ready_updates) {
		(void)ipi_clear(IPI_REASON_RCU_UPDATE);
		reschedule = rcu_bitmap_update();
	}

	// Check whether the grace period we're currently waiting for (if any)
	// has expired. The acquire here matches the release in
	// rcu_bitmap_quiesce().
	count_t		   target = atomic_load_relaxed(&my_state->target);
	rcu_grace_period_t current_period =
		atomic_load_acquire(&rcu_state.current_period);
	if (is_before(current_period.generation, target)) {
		goto out;
	}

	// Advance the batches
	bool waiting_updates = false;
	ENUM_FOREACH(RCU_UPDATE_CLASS, update_class)
	{
		// Ready batch should have been emptied by rcu_bitmap_update()
		assert(my_state->ready_batch.heads[update_class] == NULL);

		// Collect the heads to be shifted for this class
		rcu_entry_t *waiting_head =
			my_state->waiting_batch.heads[update_class];
		rcu_entry_t *next_head =
			my_state->next_batch.heads[update_class];

		// Trigger further batch processing if necessary
		if (waiting_head != NULL) {
			my_state->ready_updates = true;
		}
		if (next_head != NULL) {
			waiting_updates = true;
		}

		// Advance the heads
		my_state->next_batch.heads[update_class]    = NULL;
		my_state->waiting_batch.heads[update_class] = next_head;
		my_state->ready_batch.heads[update_class]   = waiting_head;
	}

	// Request processing of updates if any are ready
	if (my_state->ready_updates) {
		ipi_one_relaxed(IPI_REASON_RCU_UPDATE, cpulocal_get_index());
	}

	// Start a new grace period if we still have updates waiting
	if (waiting_updates) {
		rcu_bitmap_request_grace_period(my_state,
						current_period.generation);

		if (current_period.cpu_bitmap == 0U) {
			ipi_one_relaxed(IPI_REASON_RCU_QUIESCE,
					cpulocal_get_index());
		}
	}

out:
	return reschedule;
}

bool
rcu_bitmap_update(void)
{
	// Call all the callbacks queued in the previous grace period
	count_t		 update_count = 0;
	rcu_cpu_state_t *my_state     = &CPULOCAL(rcu_state);

	rcu_update_status_t status = rcu_update_status_default();

	if (!my_state->ready_updates) {
		goto out;
	}

	ENUM_FOREACH(RCU_UPDATE_CLASS, update_class)
	{
		rcu_entry_t *entry = my_state->ready_batch.heads[update_class];
		my_state->ready_batch.heads[update_class] = NULL;

		while (entry != NULL) {
			// We must read the next pointer _before_ triggering
			// the update, in case the update handler frees the
			// object.
			rcu_entry_t *next = entry->next;
			status		  = rcu_update_status_union(
				   trigger_rcu_update_event(
					   (rcu_update_class_t)update_class,
					   entry),
				   status);
			entry = next;
			update_count++;
		}
	}

	if ((update_count != 0U) &&
	    (atomic_fetch_sub_explicit(&my_state->update_count, update_count,
				       memory_order_relaxed) == update_count)) {
		(void)atomic_fetch_sub_explicit(&rcu_state.waiter_count, 1U,
						memory_order_relaxed);
	}

	my_state->ready_updates = false;

out:
	return rcu_update_status_get_need_schedule(&status);
}

void
rcu_bitmap_handle_power_cpu_offline(void)
{
	// We shouldn't get here if there are any pending updates on this CPU.
	// The power aggregation code should have checked this by calling
	// rcu_has_pending_updates() before deciding to offline the core.
	assert(atomic_load_relaxed(&CPULOCAL(rcu_state).update_count) == 0U);

	// Always deactivate & quiesce the CPU, even if RCU doesn't need to run
	// at the moment. This is because the CPU might have been left active
	// when the last update was run, and it won't be able to deactivate
	// once it goes offline.
	rcu_bitmap_deactivate_cpu();
}

bool
rcu_has_pending_updates(void)
{
	return compiler_unexpected(rcu_bitmap_should_run()) &&
	       (atomic_load_relaxed(&CPULOCAL(rcu_state).update_count) != 0U);
}

```

`hyp/core/rcu_sync/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

interface rcu
types rcu_sync.tc
events rcu_sync.ev
source rcu_sync.c

```

`hyp/core/rcu_sync/rcu_sync.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

module rcu_sync

subscribe rcu_update[RCU_UPDATE_CLASS_SYNC_COMPLETE]
	handler rcu_sync_handle_update(entry)

subscribe scheduler_get_block_properties[SCHEDULER_BLOCK_RCU_SYNC]

#if defined(UNITTESTS) && UNITTESTS
subscribe tests_init
subscribe tests_start
#endif

```

`hyp/core/rcu_sync/rcu_sync.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

define rcu_sync_state structure {
	thread pointer(atomic) object thread;
	rcu_entry structure rcu_entry(contained);
	killable bool;
};

extend rcu_update_class enumeration {
	sync_complete;
};

extend scheduler_block enumeration {
	rcu_sync;
	rcu_sync_killable;
};

```

`hyp/core/rcu_sync/src/rcu_sync.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>

#include <hypcontainers.h>

#include <atomic.h>
#include <compiler.h>
#include <log.h>
#include <object.h>
#include <rcu.h>
#include <scheduler.h>
#include <thread.h>
#include <trace.h>

#include <asm/barrier.h>
#include <asm/event.h>

#include "event_handlers.h"

scheduler_block_properties_t
rcu_sync_handle_scheduler_get_block_properties(scheduler_block_t block)
{
	assert(block == SCHEDULER_BLOCK_RCU_SYNC);

	// Set the regular sync's block flag as non-killable to ensure killed
	// threads remain blocked until the grace period has finished.
	scheduler_block_properties_t props =
		scheduler_block_properties_default();
	scheduler_block_properties_set_non_killable(&props, true);

	return props;
}

void
rcu_sync(void)
{
	thread_t *thread = thread_get_self();

	scheduler_lock(thread);
	scheduler_block(thread, SCHEDULER_BLOCK_RCU_SYNC);
	rcu_sync_state_t state = {
		.thread = object_get_thread_additional(thread),
	};
	rcu_enqueue(&state.rcu_entry, RCU_UPDATE_CLASS_SYNC_COMPLETE);

	do {
		scheduler_unlock(thread);
		(void)scheduler_schedule();
		scheduler_lock(thread);
	} while (scheduler_is_blocked(thread, SCHEDULER_BLOCK_RCU_SYNC));

	scheduler_unlock(thread);
}

bool
rcu_sync_killable(void)
{
	thread_t *thread = thread_get_self();
	bool	  killed = false;

	// We use a thread-local sync state here, so it remains valid if we
	// return early and doesn't need to be cancelled (which the RCU API
	// currently does not allow).
	static _Thread_local rcu_sync_state_t state;

	// If the state struct's thread is already set, then an earlier killable
	// sync on this thread has not yet completed. We can't reuse it as that
	// may complete too early, so just fail immediately.
	if (compiler_unexpected(atomic_load_acquire(&state.thread) != NULL)) {
		killed = true;
	} else {
		scheduler_lock(thread);
		scheduler_block(thread, SCHEDULER_BLOCK_RCU_SYNC_KILLABLE);
		state.killable = true;
		atomic_store_relaxed(&state.thread,
				     object_get_thread_additional(thread));
		rcu_enqueue(&state.rcu_entry, RCU_UPDATE_CLASS_SYNC_COMPLETE);
		scheduler_unlock(thread);

		(void)scheduler_schedule();

		scheduler_lock(thread);
		if (scheduler_is_blocked(thread,
					 SCHEDULER_BLOCK_RCU_SYNC_KILLABLE)) {
			assert(thread_is_dying(thread_get_self()));
			(void)scheduler_unblock(
				thread, SCHEDULER_BLOCK_RCU_SYNC_KILLABLE);
			killed = true;
		}
		scheduler_unlock(thread);
	}

	return !killed;
}

rcu_update_status_t
rcu_sync_handle_update(rcu_entry_t *entry)
{
	rcu_update_status_t ret = rcu_update_status_default();

	rcu_sync_state_t *state	 = rcu_sync_state_container_of_rcu_entry(entry);
	thread_t	 *thread = atomic_load_relaxed(&state->thread);

	scheduler_block_t block = (state->killable)
					  ? SCHEDULER_BLOCK_RCU_SYNC_KILLABLE
					  : SCHEDULER_BLOCK_RCU_SYNC;

	scheduler_lock(thread);
	assert(scheduler_is_blocked(thread, block));

	if (scheduler_unblock(thread, block)) {
		rcu_update_status_set_need_schedule(&ret, true);
	}
	scheduler_unlock(thread);

	object_put_thread(thread);
	atomic_store_release(&state->thread, NULL);

	return ret;
}

#if defined(UNITTESTS) && UNITTESTS
#include <util.h>

static _Atomic count_t rcu_sync_test_ready_count;
static _Atomic bool    rcu_sync_test_start_flag;
static _Atomic bool    rcu_sync_test_success_flag;

void
rcu_sync_handle_tests_init(void)
{
	atomic_init(&rcu_sync_test_ready_count, 0U);
	atomic_init(&rcu_sync_test_start_flag, false);
	atomic_init(&rcu_sync_test_success_flag, false);
}

bool
rcu_sync_handle_tests_start(void)
{
	bool failed = false;

	count_t my_order = atomic_fetch_add_explicit(&rcu_sync_test_ready_count,
						     1U, memory_order_acquire);

	if ((my_order + 1U) == PLATFORM_MAX_CORES) {
		// We're the last core to be ready; trigger the test
		asm_event_store_and_wake(&rcu_sync_test_start_flag, true);

		rcu_sync();

		// Success (unless any other CPU sees this)
		atomic_store_release(&rcu_sync_test_success_flag, true);

		// Wait until we're the last core running
		while (asm_event_load_before_wait(&rcu_sync_test_ready_count) !=
		       1U) {
			asm_event_wait(&rcu_sync_test_ready_count);
		}

		LOG(DEBUG, INFO, "rcu_sync test complete");
	} else {
		rcu_read_start();

		// Wait for the last core to trigger the test
		while (!asm_event_load_before_wait(&rcu_sync_test_start_flag)) {
			asm_event_wait(&rcu_sync_test_start_flag);
		}

		// Spin for a while to give rcu_sync() time to return early
		for (count_t i = 0;
		     i < util_bit((24 * (my_order + 1)) / PLATFORM_MAX_CORES);
		     i++) {
			asm_yield();
		}

		// Make sure the test hasn't succeeded yet; that would indicate
		// that rcu_sync() returned early
		if (atomic_load_acquire(&rcu_sync_test_success_flag)) {
			LOG(DEBUG, WARN, "rcu_sync() returned too early!");
			failed = true;
		}

		rcu_read_finish();

		// Tell the main CPU we've finished
		(void)atomic_fetch_sub_explicit(&rcu_sync_test_ready_count, 1U,
						memory_order_release);
		asm_event_wake_updated();

		// Wait for the test to finish and permit quiescent states
		while (!atomic_load_acquire(&rcu_sync_test_success_flag)) {
			scheduler_yield();
		}
	}

	return failed;
}
#endif

```

`hyp/core/scheduler_fprr/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

interface scheduler
events scheduler_fprr.ev
types scheduler_fprr.tc
source scheduler_fprr.c hypercalls.c scheduler_tests.c
configs SCHEDULER_CAN_MIGRATE=1
configs SCHEDULER_HAS_TIMESLICE=1
macros scheduler_lock.h

```

`hyp/core/scheduler_fprr/include/scheduler_lock.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#ifdef __EVENTS_DSL__
#define require_scheduler_lock(t) require_spinlock(t->scheduler_lock)
#else
#define ACQUIRE_SCHEDULER_LOCK(thread) ACQUIRE_SPINLOCK(thread->scheduler_lock)
#define ACQUIRE_SCHEDULER_LOCK_NP(thread)                                      \
	ACQUIRE_SPINLOCK_NP(thread->scheduler_lock)
#define RELEASE_SCHEDULER_LOCK(thread) RELEASE_SPINLOCK(thread->scheduler_lock)
#define RELEASE_SCHEDULER_LOCK_NP(thread)                                      \
	RELEASE_SPINLOCK_NP(thread->scheduler_lock)
#define REQUIRE_SCHEDULER_LOCK(thread) REQUIRE_SPINLOCK(thread->scheduler_lock)
#define EXCLUDE_SCHEDULER_LOCK(thread) EXCLUDE_SPINLOCK(thread->scheduler_lock)
#endif

```

`hyp/core/scheduler_fprr/scheduler_fprr.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

module scheduler_fprr

subscribe boot_cold_init()

subscribe object_create_thread
	// Needs to run late, so other handlers can
	// set scheduler properties without locking.
	priority -100

subscribe object_activate_thread

#if defined(INTERFACE_VCPU)
subscribe vcpu_activate_thread
	unwinder scheduler_fprr_handle_object_deactivate_thread(thread)

subscribe vcpu_wakeup
	require_scheduler_lock(vcpu)

subscribe vcpu_expects_wakeup
#endif

subscribe object_deactivate_thread

subscribe thread_context_switch_pre(next)
	unwinder()
	priority first
	require_preempt_disabled

subscribe thread_context_switch_post(prev)
	priority last
	require_preempt_disabled

subscribe ipi_received[IPI_REASON_RESCHEDULE]
	handler scheduler_fprr_handle_ipi_reschedule()

subscribe timer_action[TIMER_ACTION_RESCHEDULE]
	handler scheduler_fprr_handle_timer_reschedule()
	require_preempt_disabled

subscribe rcu_update[RCU_UPDATE_CLASS_AFFINITY_CHANGED]
	handler scheduler_fprr_handle_affinity_change_update(entry)
	require_preempt_disabled

subscribe thread_killed
	// Run last so other handlers can prepare for the thread
	// running with block flags and other state ignored.
	priority last

subscribe thread_exited
	require_preempt_disabled

subscribe scheduler_get_block_properties[SCHEDULER_BLOCK_AFFINITY_CHANGED]

#if defined(UNIT_TESTS)
subscribe tests_init
	handler tests_scheduler_init()

subscribe tests_start
	handler tests_scheduler_start()
	require_preempt_disabled

subscribe thread_get_entry_fn[THREAD_KIND_SCHED_TEST]
	handler sched_test_get_entry_fn

subscribe thread_get_stack_base[THREAD_KIND_SCHED_TEST]
	handler sched_test_get_stack_base
#endif

```

`hyp/core/scheduler_fprr/scheduler_fprr.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <types/bitmap.h>

define SCHEDULER_VARIANT constant enumeration scheduler_variant = 0x1;

define SCHEDULER_NUM_PRIORITIES constant type priority_t = 64;
define SCHEDULER_MIN_PRIORITY public constant type priority_t = 0;
define SCHEDULER_MAX_PRIORITY public constant type priority_t =
	SCHEDULER_NUM_PRIORITIES - 1;
define SCHEDULER_DEFAULT_PRIORITY public constant type priority_t =
	SCHEDULER_NUM_PRIORITIES / 2;

define SCHEDULER_MAX_TIMESLICE public constant type nanoseconds_t =
	100000000; // 100ms
define SCHEDULER_MIN_TIMESLICE public constant type nanoseconds_t =
	100000; // 100µs
define SCHEDULER_DEFAULT_TIMESLICE public constant type nanoseconds_t =
	5000000; // 5ms

define SCHEDULER_NUM_BLOCK_BITS constant type index_t = maxof(enumeration scheduler_block) + 1;

define sched_state bitfield<16> {
	// True once a thread has been created.
	auto init bool;
	// True if a thread is currently running on a cpu.
	auto running bool;
	// True if the thread is currently queued on a cpu.
	auto queued bool;
	// True if the thread is running on a cpu and needs
	// to be requeued after context switching away.
	auto need_requeue bool;
	// True if the thread has been killed.
	auto killed bool;
	// True if the thread has exited.
	auto exited bool;
};

define scheduler structure {
	prio_bitmap BITMAP(SCHEDULER_NUM_PRIORITIES);
	runqueue array(SCHEDULER_NUM_PRIORITIES) structure list;
	active_thread pointer object thread;
	timer structure timer;
	schedtime type ticks_t;
	lock structure spinlock;
};

extend thread object module scheduler {
	block_bits BITMAP(SCHEDULER_NUM_BLOCK_BITS);
	list_node structure list_node(contained);
	rcu_entry structure rcu_entry(contained);
	priority type priority_t;
	base_timeslice type ticks_t;
	active_timeslice type ticks_t;
	schedtime type ticks_t;
	lock structure spinlock;
	pin_count type count_t;
	yield_to pointer object thread;
	yielding bool(atomic);
	active_affinity type cpu_index_t(atomic);
	prev_affinity type cpu_index_t;
	state bitfield sched_state;
};

extend ipi_reason enumeration {
	RESCHEDULE;
};

extend timer_action enumeration {
	RESCHEDULE;
};

extend thread_create structure module scheduler {
	priority type priority_t;
	priority_valid bool;
	timeslice type nanoseconds_t;
	timeslice_valid bool;
};

extend scheduler_block enumeration {
	affinity_changed;
};

extend rcu_update_class enumeration {
	affinity_changed;
};

#if defined(UNIT_TESTS)
extend thread_kind enumeration {
	sched_test = 2;
};

define sched_test_op enumeration {
	increment;
	wake;
	yieldto;
	affinity;
};

define sched_test_param bitfield<32> {
	31:16	parent type cpu_index_t;
	15:0	op enumeration sched_test_op;
};
#endif

```

`hyp/core/scheduler_fprr/src/hypercalls.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <hyptypes.h>

#include <hypcall_def.h>
#include <hyprights.h>

#include <atomic.h>
#include <cspace.h>
#include <cspace_lookup.h>
#include <object.h>
#include <scheduler.h>
#include <thread.h>

error_t
hypercall_scheduler_yield(scheduler_yield_control_t control, register_t arg1)
{
	scheduler_yield_hint_t hint =
		scheduler_yield_control_get_hint(&control);
	error_t ret;

	if (scheduler_yield_control_get_impl_def(&control)) {
		ret = ERROR_ARGUMENT_INVALID;
		goto out;
	}

	switch (hint) {
	case SCHEDULER_YIELD_HINT_YIELD:
		ret = OK;
		scheduler_yield();
		break;
	case SCHEDULER_YIELD_HINT_YIELD_TO_THREAD: {
		cap_id_t obj_id = (cap_id_t)arg1;

		thread_ptr_result_t result = cspace_lookup_thread(
			cspace_get_self(), obj_id, CAP_RIGHTS_THREAD_YIELD_TO);
		if (result.e != OK) {
			ret = result.e;
			goto out;
		}

		if (result.r != thread_get_self()) {
			scheduler_yield_to(result.r);
		}

		object_put_thread(result.r);
		ret = OK;
		break;
	}
	case SCHEDULER_YIELD_HINT_YIELD_LOWER:
	default:
		ret = ERROR_ARGUMENT_INVALID;
		break;
	}

out:
	return ret;
}

```

`hyp/core/scheduler_fprr/src/scheduler_fprr.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>
#include <string.h>

#include <hypcontainers.h>

#include <atomic.h>
#include <bitmap.h>
#include <compiler.h>
#include <cpulocal.h>
#include <enum.h>
#include <idle.h>
#include <ipi.h>
#include <list.h>
#include <object.h>
#include <panic.h>
#include <preempt.h>
#include <rcu.h>
#include <scheduler.h>
#include <spinlock.h>
#include <thread.h>
#include <timer_queue.h>
#include <trace.h>
#if defined(INTERFACE_VCPU)
#include <vcpu.h>
#endif

#include <events/scheduler.h>

#include <asm/event.h>

#include "event_handlers.h"

CPULOCAL_DECLARE_STATIC(scheduler_t, scheduler);
CPULOCAL_DECLARE_STATIC(thread_t *_Atomic, primary_thread);
CPULOCAL_DECLARE_STATIC(thread_t *, running_thread);
CPULOCAL_DECLARE_STATIC(thread_t *, yielded_from);

static BITMAP_DECLARE(SCHEDULER_NUM_BLOCK_BITS, non_killable_block_mask);

static_assert((SCHEDULER_DEFAULT_PRIORITY >= SCHEDULER_MIN_PRIORITY) &&
		      (SCHEDULER_DEFAULT_PRIORITY <= SCHEDULER_MAX_PRIORITY),
	      "Default priority is invalid.");
static_assert((SCHEDULER_DEFAULT_TIMESLICE <= SCHEDULER_MAX_TIMESLICE) &&
		      (SCHEDULER_DEFAULT_TIMESLICE >= SCHEDULER_MIN_TIMESLICE),
	      "Default timeslice is invalid.");
static_assert((index_t)SCHEDULER_BLOCK__MAX < BITMAP_WORD_BITS,
	      "Scheduler block flags must fit in a register");

static ticks_t
get_target_timeout(scheduler_t *scheduler, thread_t *target)
	REQUIRE_PREEMPT_DISABLED
{
	assert(scheduler != NULL);
	assert(target != NULL);

	return scheduler->schedtime + target->scheduler_active_timeslice;
}

static void
reset_sched_params(thread_t *target)
{
	assert(target != NULL);

	target->scheduler_active_timeslice = target->scheduler_base_timeslice;
}

static void
set_yield_to(thread_t *target, thread_t *yield_to)
{
	assert(target != NULL);
	assert(yield_to != NULL);
	assert(target != yield_to);
	assert(target->scheduler_yield_to == NULL);

	target->scheduler_yield_to = object_get_thread_additional(yield_to);
}

static void
discard_yield_to(thread_t *target)
{
	assert(target != NULL);
	assert(target->scheduler_yield_to != NULL);

	object_put_thread(target->scheduler_yield_to);
	target->scheduler_yield_to = NULL;
}

static void
end_directed_yield(thread_t *target)
{
	assert(target != NULL);

	atomic_store_relaxed(&target->scheduler_yielding, false);
}

static bool
update_timeslice(scheduler_t *scheduler, thread_t *target, ticks_t curticks)
	REQUIRE_PREEMPT_DISABLED
{
	assert(scheduler != NULL);
	assert(target != NULL);

	ticks_t timeout = get_target_timeout(scheduler, target);
	bool	expired = timeout <= curticks;

	if (expired) {
		reset_sched_params(target);
		end_directed_yield(target);
	} else {
		// Account for the time the target has used.
		target->scheduler_active_timeslice = timeout - curticks;
	}

	return expired;
}

static void
add_to_runqueue(scheduler_t *scheduler, thread_t *target, bool at_tail)
	REQUIRE_SPINLOCK(scheduler->lock)
{
	assert_preempt_disabled();
	assert_spinlock_held(&scheduler->lock);

	index_t i	  = SCHEDULER_MAX_PRIORITY - target->scheduler_priority;
	list_t *list	  = &scheduler->runqueue[i];
	bool	was_empty = list_is_empty(list);

	assert(was_empty || bitmap_isset(scheduler->prio_bitmap, i));

	if (at_tail) {
		list_insert_at_tail(list, &target->scheduler_list_node);
	} else {
		list_insert_at_head(list, &target->scheduler_list_node);
	}

	if (was_empty) {
		bitmap_set(scheduler->prio_bitmap, i);
	}
}

static void
remove_from_runqueue(scheduler_t *scheduler, thread_t *target)
	REQUIRE_SPINLOCK(scheduler->lock)
{
	assert_preempt_disabled();

	index_t	     i	  = SCHEDULER_MAX_PRIORITY - target->scheduler_priority;
	list_t	    *list = &scheduler->runqueue[i];
	list_node_t *node = &target->scheduler_list_node;
	bool	     was_head = node == list_get_head(list);

	assert(bitmap_isset(scheduler->prio_bitmap, i));

	if (!list_delete_node(list, node) && was_head) {
		assert(list_is_empty(list));
		bitmap_clear(scheduler->prio_bitmap, i);
	}
}

static thread_t *
pop_runqueue_head(scheduler_t *scheduler, index_t i)
	REQUIRE_SPINLOCK(scheduler->lock)
{
	assert_preempt_disabled();
	assert(bitmap_isset(scheduler->prio_bitmap, i));

	list_t *list = &scheduler->runqueue[i];
	assert(!list_is_empty(list));

	list_node_t *node = list_get_head(list);
	assert(node != NULL);

	thread_t *head = thread_container_of_scheduler_list_node(node);
	assert(head->scheduler_priority == (SCHEDULER_MAX_PRIORITY - i));
	remove_from_runqueue(scheduler, head);

	return head;
}

static bool
can_be_scheduled(const thread_t *thread) REQUIRE_SCHEDULER_LOCK(thread)
{
	assert_spinlock_held(&thread->scheduler_lock);

	register_t block_bits = thread->scheduler_block_bits[0];

	if (compiler_unexpected(
		    sched_state_get_killed(&thread->scheduler_state))) {
		block_bits &= non_killable_block_mask[0];
	}

	return bitmap_empty(&block_bits, SCHEDULER_NUM_BLOCK_BITS);
}

void
scheduler_fprr_handle_boot_cold_init(void)
{
	for (cpu_index_t i = 0U; i < PLATFORM_MAX_CORES; i++) {
		scheduler_t *scheduler = &CPULOCAL_BY_INDEX(scheduler, i);
		spinlock_init(&scheduler->lock);
		timer_init_object(&scheduler->timer, TIMER_ACTION_RESCHEDULE);
		for (index_t j = 0U; j < SCHEDULER_NUM_PRIORITIES; j++) {
			list_init(&scheduler->runqueue[j]);
		}
	}

	ENUM_FOREACH(SCHEDULER_BLOCK, block)
	{
		scheduler_block_properties_t props =
			trigger_scheduler_get_block_properties_event(
				(scheduler_block_t)block);
		if (scheduler_block_properties_get_non_killable(&props)) {
			bitmap_set(non_killable_block_mask, block);
		}
	}
}

error_t
scheduler_fprr_handle_object_create_thread(thread_create_t thread_create)
{
	thread_t *thread = thread_create.thread;

	assert(thread != NULL);
	assert(atomic_load_relaxed(&thread->state) == THREAD_STATE_INIT);
	assert(!sched_state_get_init(&thread->scheduler_state));
	assert(!bitmap_empty(thread->scheduler_block_bits,
			     SCHEDULER_NUM_BLOCK_BITS));

	spinlock_init(&thread->scheduler_lock);
	atomic_init(&thread->scheduler_active_affinity, CPU_INDEX_INVALID);
	thread->scheduler_prev_affinity = CPU_INDEX_INVALID;

	cpu_index_t cpu		   = thread_create.scheduler_affinity_valid
					     ? thread_create.scheduler_affinity
					     : CPU_INDEX_INVALID;
	thread->scheduler_affinity = cpu;

	priority_t prio = thread_create.scheduler_priority_valid
				  ? thread_create.scheduler_priority
				  : SCHEDULER_DEFAULT_PRIORITY;
	assert((prio <= SCHEDULER_MAX_PRIORITY) &&
	       (prio >= SCHEDULER_MIN_PRIORITY));
	thread->scheduler_priority = prio;

	nanoseconds_t timeslice = thread_create.scheduler_timeslice_valid
					  ? thread_create.scheduler_timeslice
					  : SCHEDULER_DEFAULT_TIMESLICE;
	assert((timeslice <= SCHEDULER_MAX_TIMESLICE) &&
	       (timeslice >= SCHEDULER_MIN_TIMESLICE));
	thread->scheduler_base_timeslice = timer_convert_ns_to_ticks(timeslice);

	sched_state_set_init(&thread->scheduler_state, true);

	return OK;
}

error_t
scheduler_fprr_handle_object_activate_thread(thread_t *thread)
{
	error_t err = OK;

#if !SCHEDULER_CAN_MIGRATE
	scheduler_lock(thread);
	// The thread must have a valid affinity if scheduler cannot migrate.
	if (!cpulocal_index_valid(thread->scheduler_affinity)) {
		err = ERROR_OBJECT_CONFIG;
	}
	scheduler_unlock(thread);
#else
	(void)thread;
#endif

	return err;
}

#if defined(INTERFACE_VCPU)
bool
scheduler_fprr_handle_vcpu_activate_thread(thread_t	      *thread,
					   vcpu_option_flags_t options)
{
	bool ret = false, pin = false;

	assert(thread->kind == THREAD_KIND_VCPU);

	scheduler_lock(thread);

	// Assert that the platform soc_* handler ran before us
	assert(vcpu_option_flags_get_hlos_vm(&thread->vcpu_options) ==
	       vcpu_option_flags_get_hlos_vm(&options));

	if (vcpu_option_flags_get_hlos_vm(&thread->vcpu_options)) {
		if (!cpulocal_index_valid(thread->scheduler_affinity)) {
			goto out;
		}

		thread_t *_Atomic *primary_thread_p = &CPULOCAL_BY_INDEX(
			primary_thread, thread->scheduler_affinity);
		thread_t *expected = NULL;
		if (atomic_compare_exchange_strong_explicit(
			    primary_thread_p, &expected, thread,
			    memory_order_relaxed, memory_order_relaxed)) {
			// The primary thread can't be migrated.
			pin = true;
		} else {
			goto out;
		}
	}

	if (vcpu_option_flags_get_pinned(&options)) {
		if (cpulocal_index_valid(thread->scheduler_affinity)) {
			pin = true;
		} else {
			goto out;
		}
	}

	if (pin) {
		scheduler_pin(thread);
		vcpu_option_flags_set_pinned(&thread->vcpu_options, true);
	}

	ret = true;
out:
	scheduler_unlock(thread);
	return ret;
}

void
scheduler_fprr_handle_vcpu_wakeup(thread_t *thread)
	REQUIRE_SCHEDULER_LOCK(thread)
{
	assert_spinlock_held(&thread->scheduler_lock);
	assert(thread->kind == THREAD_KIND_VCPU);

	bool was_yielding = atomic_exchange_explicit(
		&thread->scheduler_yielding, false, memory_order_relaxed);
	if (compiler_unexpected(was_yielding)) {
		cpu_index_t affinity = thread->scheduler_affinity;

		// The thread must have a valid affinity in order to perform
		// a directed yield; see remove_thread_from_scheduler().
		assert(cpulocal_index_valid(affinity));

		scheduler_t *scheduler =
			&CPULOCAL_BY_INDEX(scheduler, affinity);
		bool is_active;

		spinlock_acquire_nopreempt(&scheduler->lock);
		is_active = scheduler->active_thread == thread;
		spinlock_release_nopreempt(&scheduler->lock);

		if (is_active) {
			// The thread is actively yielding; trigger a reschedule
			// so the cancellation of the yield is observed.
			if (affinity != cpulocal_get_index()) {
				ipi_one(IPI_REASON_RESCHEDULE, affinity);
			} else {
				scheduler_trigger();
			}
		}
	}
}

bool
scheduler_fprr_handle_vcpu_expects_wakeup(const thread_t *thread)
{
	assert(thread->kind == THREAD_KIND_VCPU);

	return atomic_load_relaxed(&thread->scheduler_yielding);
}
#endif

void
scheduler_fprr_handle_object_deactivate_thread(thread_t *thread)
{
	assert(thread != NULL);

	if (cpulocal_index_valid(thread->scheduler_affinity)) {
		thread_t *_Atomic *primary_thread_p = &CPULOCAL_BY_INDEX(
			primary_thread, thread->scheduler_affinity);
		if (atomic_load_relaxed(primary_thread_p) == thread) {
			atomic_store_relaxed(primary_thread_p, NULL);
		}
	}
}

bool
scheduler_fprr_handle_ipi_reschedule(void)
{
	return true;
}

bool
scheduler_fprr_handle_timer_reschedule(void)
{
	assert_preempt_disabled();

	scheduler_trigger();

	return true;
}

scheduler_block_properties_t
scheduler_fprr_handle_scheduler_get_block_properties(scheduler_block_t block)
{
	assert(block == SCHEDULER_BLOCK_AFFINITY_CHANGED);

	scheduler_block_properties_t props =
		scheduler_block_properties_default();
	scheduler_block_properties_set_non_killable(&props, true);

	return props;
}

rcu_update_status_t
scheduler_fprr_handle_affinity_change_update(rcu_entry_t *entry)
{
	rcu_update_status_t ret = rcu_update_status_default();

	thread_t   *thread = thread_container_of_scheduler_rcu_entry(entry);
	cpu_index_t prev_cpu, next_cpu;

	scheduler_lock_nopreempt(thread);
	assert(scheduler_is_blocked(thread, SCHEDULER_BLOCK_AFFINITY_CHANGED));
	prev_cpu = thread->scheduler_prev_affinity;
	next_cpu = thread->scheduler_affinity;
	scheduler_unlock_nopreempt(thread);

	trigger_scheduler_affinity_changed_sync_event(thread, prev_cpu,
						      next_cpu);

	scheduler_lock_nopreempt(thread);
	if (scheduler_unblock(thread, SCHEDULER_BLOCK_AFFINITY_CHANGED)) {
		rcu_update_status_set_need_schedule(&ret, true);
	}
	scheduler_unlock_nopreempt(thread);

	object_put_thread(thread);

	return ret;
}

static void
set_next_timeout(scheduler_t *scheduler, thread_t *target)
	REQUIRE_SPINLOCK(scheduler->lock)
{
	assert_spinlock_held(&scheduler->lock);

	bool need_timeout = false;

	if (target != idle_thread()) {
		// A timeout needs to be set if the scheduler queue
		// for the current priority is not empty, or if we
		// may yield to another target.
		index_t i = SCHEDULER_MAX_PRIORITY - target->scheduler_priority;
		need_timeout = bitmap_isset(scheduler->prio_bitmap, i) ||
			       atomic_load_relaxed(&target->scheduler_yielding);
	}

	if (need_timeout) {
		ticks_t timeout = get_target_timeout(scheduler, target);
		timer_update(&scheduler->timer, timeout);
	} else {
		timer_dequeue(&scheduler->timer);
	}
}

static thread_t *
get_next_target(scheduler_t *scheduler, ticks_t curticks)
	REQUIRE_SPINLOCK(scheduler->lock)
{
	assert(scheduler != NULL);
	assert_spinlock_held(&scheduler->lock);

	thread_t *prev		    = scheduler->active_thread;
	thread_t *target	    = prev;
	bool	  timeslice_expired = false;
	index_t	  i;

	if (target != NULL) {
		timeslice_expired =
			update_timeslice(scheduler, target, curticks);
	}

	if (bitmap_ffs(scheduler->prio_bitmap, SCHEDULER_NUM_PRIORITIES, &i)) {
		priority_t prio = SCHEDULER_MAX_PRIORITY - i;
		// Always prefer targets with higher priority, and if timeslice
		// has been used up, targets with the same priority.
		bool should_switch =
			(target == NULL) ||
			(timeslice_expired
				 ? (prio >= target->scheduler_priority)
				 : (prio > target->scheduler_priority));
		if (should_switch) {
			target = pop_runqueue_head(scheduler, i);
		}
	}

	if (target != NULL) {
		scheduler->active_thread = target;
	} else {
		target			 = idle_thread();
		scheduler->active_thread = NULL;
	}

	scheduler->schedtime = curticks;

	if ((prev != NULL) && (target != prev)) {
		add_to_runqueue(scheduler, prev, timeslice_expired);
	}

	return target;
}

static bool
can_yield_to(thread_t *yield_to) REQUIRE_SCHEDULER_LOCK(yield_to)
{
	assert_preempt_disabled();

	thread_t   *current  = thread_get_self();
	cpu_index_t cpu	     = cpulocal_get_index();
	cpu_index_t affinity = yield_to->scheduler_affinity;
	bool	    yield    = true;

	// The target's affinity must be equal to this cpu or invalid.
	if (cpulocal_index_valid(affinity) && (affinity != cpu)) {
		yield = false;
		goto out;
	}

	// We can't yield to a thread if it is already running and
	// not the current thread.
	if (sched_state_get_running(&yield_to->scheduler_state) &&
	    (yield_to != current)) {
		yield = false;
		goto out;
	}

	if (!can_be_scheduled(yield_to)) {
		yield = false;
	}
out:
	return yield;
}

static thread_t *
select_yield_target(thread_t *target, bool *can_idle) REQUIRE_PREEMPT_DISABLED
{
	assert(target != NULL);
	assert(can_idle != NULL);
	assert_preempt_disabled();

	thread_t *next = target;

	CPULOCAL(yielded_from) = NULL;

	if (atomic_load_relaxed(&target->scheduler_yielding)) {
		thread_t *yield_to = target->scheduler_yield_to;
		assert(yield_to != NULL);

		scheduler_lock_nopreempt(yield_to);
		if (can_yield_to(yield_to)) {
			next		       = yield_to;
			CPULOCAL(yielded_from) = target;
			*can_idle	       = false;
		} else {
			end_directed_yield(target);
		}
		scheduler_unlock_nopreempt(yield_to);
	}

	return next;
}

bool
scheduler_schedule(void)
{
	bool must_schedule = true;
	bool switched	   = false;

	preempt_disable();

	while (must_schedule) {
		scheduler_t *scheduler = &CPULOCAL(scheduler);
		ticks_t	     curticks  = timer_get_current_timer_ticks();
		thread_t    *current   = thread_get_self();
		thread_t    *target;

		rcu_read_start();

		trigger_scheduler_schedule_event(current,
						 CPULOCAL(yielded_from),
						 scheduler->schedtime,
						 curticks);

		spinlock_acquire_nopreempt(&scheduler->lock);
		target	      = get_next_target(scheduler, curticks);
		bool can_idle = bitmap_empty(scheduler->prio_bitmap,
					     SCHEDULER_NUM_PRIORITIES);
		set_next_timeout(scheduler, target);
		spinlock_release_nopreempt(&scheduler->lock);

		target = select_yield_target(target, &can_idle);

		trigger_scheduler_selected_thread_event(target, &can_idle);

		if (target == current) {
			rcu_read_finish();
			trigger_scheduler_quiescent_event();
			must_schedule = false;
		} else if (object_get_thread_safe(target)) {
			// The reference obtained here will be released when the
			// thread stops running.
			rcu_read_finish();

			if (compiler_expected(
				    thread_switch_to(target, curticks) == OK)) {
				switched = true;
				must_schedule =
					ipi_clear(IPI_REASON_RESCHEDULE);
			} else {
				must_schedule = true;
			}
		} else {
			// Unable to obtain a reference to the target thread;
			// re-run the scheduler.
			rcu_read_finish();
			must_schedule = true;
		}
	}

	preempt_enable();

	return switched;
}

void
scheduler_trigger(void)
{
	cpu_index_t cpu = cpulocal_get_index();
	ipi_one_relaxed(IPI_REASON_RESCHEDULE, cpu);
}

void
scheduler_yield(void)
{
	thread_t *current = thread_get_self();

	preempt_disable();
	thread_t *yielded_from = CPULOCAL(yielded_from);
	if (yielded_from != NULL) {
		// End the directed yield to the current thread.
		end_directed_yield(yielded_from);
	} else {
		// Discard the rest of the current thread's timeslice.
		current->scheduler_active_timeslice = 0U;
	}
	(void)scheduler_schedule();
	preempt_enable();
}

void
scheduler_yield_to(thread_t *target)
{
	thread_t *current = thread_get_self();

	assert(current != target);

	preempt_disable();

	thread_t *yielded_from = CPULOCAL(yielded_from);
	if (yielded_from == target) {
		// We are trying to yield back to the thread that
		// yielded to us; end the original yield.
		end_directed_yield(yielded_from);
	} else if (yielded_from != NULL) {
		// Update the yielding thread's target.
		discard_yield_to(yielded_from);
		set_yield_to(yielded_from, target);
	} else {
#if defined(INTERFACE_VCPU)
		if ((current->kind == THREAD_KIND_VCPU) &&
		    vcpu_pending_wakeup()) {
			// The current thread has a pending wakeup;
			// skip the directed yield.
			goto out;
		}
#endif
		// Initiate a new directed yield. We must pin the current
		// thread, as allowing migration may result in the current
		// thread running simultaneously with its yield target.
		// Pinning the thread also makes accesses to the yield-to
		// pointer CPU-local for the duration of the yield, making
		// it safe to access without the thread lock.
		scheduler_lock_nopreempt(current);
		scheduler_pin(current);
		scheduler_unlock_nopreempt(current);
		set_yield_to(current, target);
		atomic_store_relaxed(&current->scheduler_yielding, true);
	}

	(void)scheduler_schedule();

	if (yielded_from == NULL) {
		discard_yield_to(current);
		scheduler_lock_nopreempt(current);
		scheduler_unpin(current);
		scheduler_unlock_nopreempt(current);
	}

#if defined(INTERFACE_VCPU)
out:
#endif
	preempt_enable();
}

void
scheduler_lock(thread_t *thread)
{
	spinlock_acquire(&thread->scheduler_lock);
}

void
scheduler_lock_nopreempt(thread_t *thread)
{
	spinlock_acquire_nopreempt(&thread->scheduler_lock);
}

void
scheduler_unlock(thread_t *thread)
{
	spinlock_release(&thread->scheduler_lock);
}

void
scheduler_unlock_nopreempt(thread_t *thread)
{
	spinlock_release_nopreempt(&thread->scheduler_lock);
}

static bool
add_thread_to_scheduler(thread_t *thread) REQUIRE_SCHEDULER_LOCK(thread)
{
	assert_spinlock_held(&thread->scheduler_lock);
	assert(!sched_state_get_running(&thread->scheduler_state));
	assert(!sched_state_get_queued(&thread->scheduler_state));
	assert(!sched_state_get_exited(&thread->scheduler_state));
	assert(can_be_scheduled(thread));

	bool	    need_schedule;
	cpu_index_t affinity = thread->scheduler_affinity;

	if (cpulocal_index_valid(affinity)) {
		cpu_index_t  cpu = cpulocal_get_index();
		scheduler_t *scheduler =
			&CPULOCAL_BY_INDEX(scheduler, affinity);

		spinlock_acquire_nopreempt(&scheduler->lock);

		if (scheduler->active_thread == NULL) {
			// The newly unblocked thread is the only one runnable,
			// so a reschedule will always be needed.
			need_schedule = true;
		} else if (bitmap_empty(scheduler->prio_bitmap,
					SCHEDULER_NUM_PRIORITIES)) {
			// The scheduler's current thread was scheduled with
			// can_idle set, so it may have gone idle without
			// rescheduling. Force a reschedule regardless of
			// priority, to ensure that it doesn't needlessly block
			// a lower-priority threads.
			need_schedule = true;
		} else {
			// There is already an active thread; a reschedule is
			// needed if the newly unblocked thread has equal or
			// higher priority
			need_schedule =
				thread->scheduler_priority >=
				scheduler->active_thread->scheduler_priority;
		}

		reset_sched_params(thread);
		sched_state_set_queued(&thread->scheduler_state, true);

		// Each thread has a reference to itself which remains until it
		// exits. Since threads are not runnable after exiting, the
		// scheduler queues can safely use this reference instead of
		// getting an additional one.
		add_to_runqueue(scheduler, thread, true);

		spinlock_release_nopreempt(&scheduler->lock);

		if (need_schedule && (cpu != affinity)) {
			ipi_one(IPI_REASON_RESCHEDULE, affinity);
			need_schedule = false;
		}
	} else {
		need_schedule = false;
	}

	return need_schedule;
}

static void
remove_thread_from_scheduler(thread_t *thread) REQUIRE_SCHEDULER_LOCK(thread)
{
	assert_spinlock_held(&thread->scheduler_lock);

	cpu_index_t affinity	 = thread->scheduler_affinity;
	bool	    was_yielding = atomic_exchange_explicit(
		       &thread->scheduler_yielding, false, memory_order_relaxed);

	if (cpulocal_index_valid(affinity)) {
		assert(sched_state_get_queued(&thread->scheduler_state));

		scheduler_t *scheduler =
			&CPULOCAL_BY_INDEX(scheduler, affinity);
		bool was_active = false;

		spinlock_acquire_nopreempt(&scheduler->lock);
		if (scheduler->active_thread == thread) {
			scheduler->active_thread = NULL;
			was_active		 = true;
		} else {
			remove_from_runqueue(scheduler, thread);
		}
		spinlock_release_nopreempt(&scheduler->lock);

		sched_state_set_queued(&thread->scheduler_state, false);

		if (compiler_unexpected(was_active && was_yielding)) {
			// The thread was actively yielding; trigger a
			// reschedule to ensure the yield ends.
			if (affinity != cpulocal_get_index()) {
				ipi_one(IPI_REASON_RESCHEDULE, affinity);
			} else {
				scheduler_trigger();
			}
		}
	} else {
		// Threads with invalid affinities cannot perform directed
		// yields; as they only run via directed yields, any call to
		// scheduler_yield_to() will update the yielding thread instead.
		assert(!was_yielding);
	}
}

static bool
resched_running_thread(thread_t *thread) REQUIRE_SCHEDULER_LOCK(thread)
{
	assert_spinlock_held(&thread->scheduler_lock);
	assert(sched_state_get_running(&thread->scheduler_state));
	assert(!sched_state_get_queued(&thread->scheduler_state) ||
	       sched_state_get_killed(&thread->scheduler_state));

	bool	    need_schedule = true;
	cpu_index_t cpu =
		atomic_load_relaxed(&thread->scheduler_active_affinity);

	assert(cpulocal_index_valid(cpu));

	if (cpu != cpulocal_get_index()) {
		ipi_one(IPI_REASON_RESCHEDULE, cpu);
		need_schedule = false;
	}

	return need_schedule;
}

static bool
start_affinity_changed_events(thread_t *thread) REQUIRE_SCHEDULER_LOCK(thread)
{
	assert_spinlock_held(&thread->scheduler_lock);
	assert(scheduler_is_blocked(thread, SCHEDULER_BLOCK_AFFINITY_CHANGED));

	bool need_sync = false, need_schedule = false;

	trigger_scheduler_affinity_changed_event(
		thread, thread->scheduler_prev_affinity,
		thread->scheduler_affinity, &need_sync);

	if (need_sync) {
		rcu_enqueue(&thread->scheduler_rcu_entry,
			    RCU_UPDATE_CLASS_AFFINITY_CHANGED);
	} else {
		need_schedule = scheduler_unblock(
			thread, SCHEDULER_BLOCK_AFFINITY_CHANGED);
		object_put_thread(thread);
	}

	return need_schedule;
}

error_t
scheduler_fprr_handle_thread_context_switch_pre(thread_t *next)
{
	assert_preempt_disabled();

	assert(next != thread_get_self());

	error_t	    err = OK;
	cpu_index_t cpu = cpulocal_get_index();

	scheduler_lock_nopreempt(next);
	cpu_index_t affinity	 = next->scheduler_affinity;
	thread_t   *yielded_from = CPULOCAL(yielded_from);

	// The next thread's affinity could have changed between target
	// selection and now; it may have been blocked by or is already running
	// on another CPU. Only set it running if it is still valid to do so.
	bool runnable = !sched_state_get_running(&next->scheduler_state) &&
			(can_be_scheduled(next) || (next == idle_thread()));
	bool affinity_valid =
		(affinity == cpu) ||
		(!cpulocal_index_valid(affinity) && (yielded_from != NULL));

	if (compiler_expected(runnable && affinity_valid)) {
		assert(!sched_state_get_need_requeue(&next->scheduler_state));
		assert(!sched_state_get_exited(&next->scheduler_state));
		sched_state_set_running(&next->scheduler_state, true);
		CPULOCAL(running_thread) = next;
		atomic_store_relaxed(&next->scheduler_active_affinity, cpu);
	} else {
		err = ERROR_DENIED;
		if (yielded_from != NULL) {
			end_directed_yield(yielded_from);
		}
	}
	scheduler_unlock_nopreempt(next);

	return err;
}

noreturn void
scheduler_fprr_unwind_thread_context_switch_pre(void)
{
	panic("Context switch pre failed!");
}

void
scheduler_fprr_handle_thread_context_switch_post(thread_t *prev)
{
	assert_preempt_disabled();

	bool need_schedule = false;

	scheduler_lock_nopreempt(prev);
	sched_state_set_running(&prev->scheduler_state, false);

	if (sched_state_get_need_requeue(&prev->scheduler_state)) {
		// The thread may have blocked after being marked for a
		// requeue. Ensure it is still runnable prior to adding
		// it to a scheduler queue.
		if (can_be_scheduled(prev)) {
			need_schedule = add_thread_to_scheduler(prev);
		}
		sched_state_set_need_requeue(&prev->scheduler_state, false);
	}

	if (scheduler_is_blocked(prev, SCHEDULER_BLOCK_AFFINITY_CHANGED)) {
		need_schedule = start_affinity_changed_events(prev);
	}

	// Store and wake for scheduler_sync().
	asm_event_store_and_wake(&prev->scheduler_active_affinity,
				 CPU_INDEX_INVALID);
	scheduler_unlock_nopreempt(prev);

	if (need_schedule) {
		scheduler_trigger();
	}
}

void
scheduler_block(thread_t *thread, scheduler_block_t block)
	REQUIRE_SCHEDULER_LOCK(thread)
{
	TRACE(INFO, INFO, "scheduler: block {:#x}, reason: {:d}, others: {:#x}",
	      (uintptr_t)thread, (register_t)block,
	      thread->scheduler_block_bits[0]);

	assert_spinlock_held(&thread->scheduler_lock);
	assert(block <= SCHEDULER_BLOCK__MAX);

	if (!bitmap_isset(thread->scheduler_block_bits, (index_t)block)) {
		trigger_scheduler_blocked_event(thread, block,
						can_be_scheduled(thread));
	}

	bitmap_set(thread->scheduler_block_bits, (index_t)block);
	if (sched_state_get_queued(&thread->scheduler_state) &&
	    !can_be_scheduled(thread)) {
		remove_thread_from_scheduler(thread);
	}
}

void
scheduler_block_init(thread_t *thread, scheduler_block_t block)
{
	assert(!sched_state_get_init(&thread->scheduler_state));
	assert(block <= SCHEDULER_BLOCK__MAX);
	bitmap_set(thread->scheduler_block_bits, (index_t)block);
}

bool
scheduler_unblock(thread_t *thread, scheduler_block_t block)
	REQUIRE_LOCK(thread->scheduler_lock)
{
	assert_spinlock_held(&thread->scheduler_lock);
	assert(block <= SCHEDULER_BLOCK__MAX);
	bool was_blocked = !can_be_scheduled(thread);
	bool block_was_set =
		bitmap_isset(thread->scheduler_block_bits, (index_t)block);
	bitmap_clear(thread->scheduler_block_bits, (index_t)block);
	bool now_runnable  = can_be_scheduled(thread);
	bool need_schedule = was_blocked && now_runnable;

	if (need_schedule) {
		assert(!sched_state_get_queued(&thread->scheduler_state));
		// The thread may have not finished running after the block.
		// If so, mark for requeue. Otherwise it is safe to directly
		// queue the thread.
		if (compiler_unexpected(sched_state_get_running(
			    &thread->scheduler_state))) {
			sched_state_set_need_requeue(&thread->scheduler_state,
						     true);
			need_schedule = resched_running_thread(thread);
		} else {
			need_schedule = add_thread_to_scheduler(thread);
		}
	}

	TRACE(INFO, INFO,
	      "scheduler: unblock {:#x}, reason: {:d}, others: {:#x}, local run: {:d}",
	      (uintptr_t)thread, (register_t)block,
	      thread->scheduler_block_bits[0], (register_t)need_schedule);

	if (block_was_set) {
		trigger_scheduler_unblocked_event(thread, block, now_runnable);
	}

	return need_schedule;
}

bool
scheduler_is_blocked(const thread_t *thread, scheduler_block_t block)
{
	assert(block <= SCHEDULER_BLOCK__MAX);
	return bitmap_isset(thread->scheduler_block_bits, (index_t)block);
}

bool
scheduler_is_runnable(const thread_t *thread)
{
	return can_be_scheduled(thread);
}

bool
scheduler_is_running(const thread_t *thread)
{
	return sched_state_get_running(&thread->scheduler_state);
}

thread_t *
scheduler_get_primary_vcpu(cpu_index_t cpu)
{
	return atomic_load_consume(&CPULOCAL_BY_INDEX(primary_thread, cpu));
}

void
scheduler_sync(thread_t *thread)
{
	_Atomic cpu_index_t *affinity_p = &thread->scheduler_active_affinity;

	cpu_index_t cpu = atomic_load_acquire(affinity_p);
	if (cpulocal_index_valid(cpu)) {
		ipi_one(IPI_REASON_RESCHEDULE, cpu);
		while (cpulocal_index_valid(
			asm_event_load_before_wait(affinity_p))) {
			asm_event_wait(affinity_p);
		}
	}
}

void
scheduler_pin(thread_t *thread)
{
	assert_spinlock_held(&thread->scheduler_lock);
	thread->scheduler_pin_count++;
}

void
scheduler_unpin(thread_t *thread)
{
	assert_spinlock_held(&thread->scheduler_lock);
	assert(thread->scheduler_pin_count > 0U);
	thread->scheduler_pin_count--;
}

cpu_index_t
scheduler_get_affinity(thread_t *thread)
{
	assert_spinlock_held(&thread->scheduler_lock);
	return thread->scheduler_affinity;
}

cpu_index_t
scheduler_get_active_affinity(thread_t *thread)
{
	assert_spinlock_held(&thread->scheduler_lock);

	cpu_index_t cpu =
		atomic_load_relaxed(&thread->scheduler_active_affinity);

	return cpulocal_index_valid(cpu) ? cpu : thread->scheduler_affinity;
}

error_t
scheduler_set_affinity(thread_t *thread, cpu_index_t target_cpu)
{
	assert_spinlock_held(&thread->scheduler_lock);

	error_t	    err		  = OK;
	bool	    need_schedule = false;
	cpu_index_t prev_cpu	  = thread->scheduler_affinity;

	if (prev_cpu == target_cpu) {
		goto out;
	}

	if (thread->scheduler_pin_count != 0U) {
		err = ERROR_DENIED;
		goto out;
	}

	if (scheduler_is_blocked(thread, SCHEDULER_BLOCK_AFFINITY_CHANGED)) {
		err = ERROR_RETRY;
		goto out;
	}

	err = trigger_scheduler_set_affinity_prepare_event(thread, prev_cpu,
							   target_cpu);
	if (err != OK) {
		goto out;
	}

	// Block the thread so affinity changes are serialised. We need to get
	// an additional reference to the thread, otherwise it may be deleted
	// prior to the completion of the affinity change.
	(void)object_get_thread_additional(thread);
	scheduler_block(thread, SCHEDULER_BLOCK_AFFINITY_CHANGED);

	thread->scheduler_prev_affinity = prev_cpu;
	thread->scheduler_affinity	= target_cpu;

	if (sched_state_get_running(&thread->scheduler_state)) {
		// Trigger a reschedule on the running thread's CPU; the
		// context switch will trigger the affinity changed event.
		need_schedule = resched_running_thread(thread);
	} else {
		need_schedule = start_affinity_changed_events(thread);
	}

	if (need_schedule) {
		scheduler_trigger();
	}

out:
	return err;
}

static void
update_sched_params(thread_t *thread, priority_t priority, ticks_t timeslice)
	REQUIRE_SCHEDULER_LOCK(thread)
{
	assert_spinlock_held(&thread->scheduler_lock);

	// If the thread is blocked, or is still running and has been marked for
	// a requeue, then it is safe to update the scheduler parameters without
	// any queue operations. If not, it first needs to be removed from its
	// queue before the update, then added back when it is safe to do so.
	bool requeue = can_be_scheduled(thread) &&
		       !sched_state_get_need_requeue(&thread->scheduler_state);

	if (requeue) {
		remove_thread_from_scheduler(thread);
	}

	thread->scheduler_priority	 = priority;
	thread->scheduler_base_timeslice = timeslice;

	if (requeue) {
		bool need_schedule;
		if (sched_state_get_running(&thread->scheduler_state)) {
			sched_state_set_need_requeue(&thread->scheduler_state,
						     true);
			need_schedule = resched_running_thread(thread);
		} else {
			need_schedule = add_thread_to_scheduler(thread);
		}

		if (need_schedule) {
			scheduler_trigger();
		}
	}
}

error_t
scheduler_set_priority(thread_t *thread, priority_t priority)
{
	error_t err = OK;

	assert_spinlock_held(&thread->scheduler_lock);

	// Verify the SCHEDULER_MIN_PRIORITY is configured other than 0 to add
	// another check for the 'priority' variable.
	static_assert(SCHEDULER_MIN_PRIORITY == 0U,
		      "zero minimum priority expected");

	if ((priority > SCHEDULER_MAX_PRIORITY)) {
		err = ERROR_ARGUMENT_INVALID;
		goto out;
	}

	if (thread->scheduler_priority != priority) {
		update_sched_params(thread, priority,
				    thread->scheduler_base_timeslice);
	}

out:
	return err;
}

error_t
scheduler_set_timeslice(thread_t *thread, nanoseconds_t timeslice)
{
	error_t err = OK;

	assert_spinlock_held(&thread->scheduler_lock);

	if ((timeslice > SCHEDULER_MAX_TIMESLICE) ||
	    (timeslice < SCHEDULER_MIN_TIMESLICE)) {
		err = ERROR_ARGUMENT_INVALID;
		goto out;
	}

	ticks_t timeslice_ticks = timer_convert_ns_to_ticks(timeslice);
	if (thread->scheduler_base_timeslice != timeslice_ticks) {
		update_sched_params(thread, thread->scheduler_priority,
				    timeslice_ticks);
	}

out:
	return err;
}

bool
scheduler_will_preempt_current(thread_t *thread)
{
	assert_spinlock_held(&thread->scheduler_lock);
	thread_t *current = thread_get_self();

	return (thread->scheduler_priority > current->scheduler_priority) ||
	       (current->kind == THREAD_KIND_IDLE);
}

void
scheduler_fprr_handle_thread_killed(thread_t *thread)
{
	assert(thread != NULL);

	bool need_schedule = false;

	scheduler_lock(thread);

	if (sched_state_get_exited(&thread->scheduler_state)) {
		// The thread managed to exit prior to this event being
		// triggered, nothing to do.
		goto out;
	}

	// Many of the block flags will be ignored once the killed
	// flag is set, so check if the thread becomes runnable.
	bool was_blocked = !can_be_scheduled(thread);
	sched_state_set_killed(&thread->scheduler_state, true);
	bool runnable = was_blocked && can_be_scheduled(thread);
	bool running  = sched_state_get_running(&thread->scheduler_state);

	if (runnable) {
		assert(!sched_state_get_queued(&thread->scheduler_state));

		if (running) {
			sched_state_set_need_requeue(&thread->scheduler_state,
						     true);
			need_schedule = resched_running_thread(thread);
		} else {
			need_schedule = add_thread_to_scheduler(thread);
		}
	} else if (running) {
		// If the thread is running remotely, we need to send
		// an IPI to ensure it exits in a timely manner.
		(void)resched_running_thread(thread);
	} else {
		// Thread is either still blocked or already
		// scheduled to run, so there is nothing to do.
	}

out:
	scheduler_unlock_nopreempt(thread);

	if (need_schedule) {
		scheduler_trigger();
	}

	preempt_enable();
}

void
scheduler_fprr_handle_thread_exited(void)
{
	assert_preempt_disabled();

	thread_t *thread = thread_get_self();

	scheduler_lock_nopreempt(thread);

	assert(atomic_load_relaxed(&thread->state) == THREAD_STATE_EXITED);
	assert(scheduler_is_blocked(thread, SCHEDULER_BLOCK_THREAD_LIFECYCLE));
	assert(sched_state_get_running(&thread->scheduler_state));

	if (sched_state_get_killed(&thread->scheduler_state)) {
		if (sched_state_get_queued(&thread->scheduler_state)) {
			remove_thread_from_scheduler(thread);
		}
		sched_state_set_killed(&thread->scheduler_state, false);
	}

	assert(!can_be_scheduled(thread));
	assert(!sched_state_get_queued(&thread->scheduler_state));

	sched_state_set_exited(&thread->scheduler_state, true);

	scheduler_unlock_nopreempt(thread);
}

```

`hyp/core/scheduler_fprr/src/scheduler_tests.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#if defined(UNIT_TESTS)

#include <assert.h>
#include <hyptypes.h>

#include <atomic.h>
#include <compiler.h>
#include <cpulocal.h>
#include <hyp_aspace.h>
#include <log.h>
#include <object.h>
#include <panic.h>
#include <partition.h>
#include <partition_alloc.h>
#include <preempt.h>
#include <scheduler.h>
#include <thread.h>
#include <timer_queue.h>
#include <trace.h>
#include <util.h>

#include <events/object.h>

#include <asm/event.h>

#include "event_handlers.h"

#define NUM_AFFINITY_SWITCH 20U

#define SCHED_TEST_STACK_AREA (4U << 20)

static uintptr_t	 sched_test_stack_base;
static uintptr_t	 sched_test_stack_end;
static _Atomic uintptr_t sched_test_stack_alloc;

static _Atomic count_t sync_flag;

CPULOCAL_DECLARE_STATIC(_Atomic uint8_t, wait_flag);
CPULOCAL_DECLARE_STATIC(thread_t *, test_thread);
CPULOCAL_DECLARE_STATIC(count_t, test_passed_count);
CPULOCAL_DECLARE_STATIC(_Atomic count_t, affinity_count);

static thread_ptr_result_t
create_thread(priority_t prio, cpu_index_t cpu, sched_test_op_t op)
	REQUIRE_PREEMPT_DISABLED
{
	thread_ptr_result_t ret;

	sched_test_param_t param = sched_test_param_default();
	sched_test_param_set_parent(&param, cpulocal_get_index());
	sched_test_param_set_op(&param, op);

	thread_create_t params = {
		.scheduler_affinity	  = cpu,
		.scheduler_affinity_valid = true,
		.scheduler_priority	  = prio,
		.scheduler_priority_valid = true,
		.kind			  = THREAD_KIND_SCHED_TEST,
		.params			  = sched_test_param_raw(param),
	};

	ret = partition_allocate_thread(partition_get_private(), params);
	if (ret.e != OK) {
		goto out;
	}

	error_t err = object_activate_thread(ret.r);
	if (err != OK) {
		object_put_thread(ret.r);
		ret = thread_ptr_result_error(err);
	}

out:
	return ret;
}

static void
destroy_thread(thread_t *thread)
{
	// Wait for the thread to exit so subsequent tests do not race with it.
	while (atomic_load_relaxed(&thread->state) != THREAD_STATE_EXITED) {
		scheduler_yield_to(thread);
	}

	object_put_thread(thread);
}

static void
schedule_check_switched(thread_t *thread, bool switch_expected)
{
	thread_t *current = thread_get_self();

	preempt_disable();
	if (scheduler_schedule()) {
		// We must have expected a switch.
		assert(switch_expected);
	} else if (switch_expected) {
		// If we didn't switch, then current must have already been
		// preempted. For current to run again, the other thread must
		// have exited or is yielding to us.
		assert((thread->scheduler_yield_to == current) ||
		       (atomic_load_relaxed(&thread->state) ==
			THREAD_STATE_EXITED));
	} else {
		// Nothing to check.
	}
	preempt_enable();
}

void
tests_scheduler_init(void)
{
	virt_range_result_t range = hyp_aspace_allocate(SCHED_TEST_STACK_AREA);
	assert(range.e == OK);

	sched_test_stack_base =
		util_balign_up(range.r.base + 1U, THREAD_STACK_MAP_ALIGN);
	sched_test_stack_end = range.r.base + (range.r.size - 1U);

	atomic_init(&sched_test_stack_alloc, sched_test_stack_base);
}

bool
tests_scheduler_start(void)
{
	thread_ptr_result_t ret;
	uint8_t		    old;

	// Test 1: priorities
	// priority > default: switch on schedule
	ret = create_thread(SCHEDULER_MAX_PRIORITY, cpulocal_get_index(),
			    SCHED_TEST_OP_INCREMENT);
	assert(ret.e == OK);

	schedule_check_switched(ret.r, true);

	old = atomic_load_relaxed(&CPULOCAL(wait_flag));
	assert(old == 1U);
	atomic_store_relaxed(&CPULOCAL(wait_flag), 0U);
	destroy_thread(ret.r);
	CPULOCAL(test_passed_count)++;

	// priority == default: switch on yield
	ret = create_thread(SCHEDULER_DEFAULT_PRIORITY, cpulocal_get_index(),
			    SCHED_TEST_OP_INCREMENT);
	assert(ret.e == OK);

	while (atomic_load_relaxed(&CPULOCAL(wait_flag)) == 0U) {
		scheduler_yield();
	}
	atomic_store_relaxed(&CPULOCAL(wait_flag), 0U);
	destroy_thread(ret.r);
	CPULOCAL(test_passed_count)++;

	// priority < default: switch on directed yield
	ret = create_thread(SCHEDULER_MIN_PRIORITY, cpulocal_get_index(),
			    SCHED_TEST_OP_INCREMENT);
	assert(ret.e == OK);

	schedule_check_switched(ret.r, false);

	while (atomic_load_relaxed(&CPULOCAL(wait_flag)) == 0U) {
		scheduler_yield_to(ret.r);
	}
	atomic_store_relaxed(&CPULOCAL(wait_flag), 0U);
	destroy_thread(ret.r);
	CPULOCAL(test_passed_count)++;

	// Test 2: wait for timeslice expiry
	ret = create_thread(SCHEDULER_DEFAULT_PRIORITY, cpulocal_get_index(),
			    SCHED_TEST_OP_WAKE);
	assert(ret.e == OK);

	// Yield to reset the current thread's timeslice, then wait for the
	// other thread to run and update the wait flag.
	scheduler_yield();
	_Atomic uint8_t *wait_flag = &CPULOCAL(wait_flag);
	atomic_store_relaxed(wait_flag, 1U);
	preempt_enable();
	while (asm_event_load_before_wait(wait_flag) == 1U) {
		asm_event_wait(wait_flag);
	}
	preempt_disable();

	assert(atomic_load_relaxed(&CPULOCAL(wait_flag)) == 0U);
	destroy_thread(ret.r);
	CPULOCAL(test_passed_count)++;

	// Test 3: double directed yield
	ret = create_thread(SCHEDULER_MIN_PRIORITY, CPU_INDEX_INVALID,
			    SCHED_TEST_OP_INCREMENT);
	assert(ret.e == OK);
	CPULOCAL(test_thread) = ret.r;

	ret = create_thread(SCHEDULER_MIN_PRIORITY + 1U, cpulocal_get_index(),
			    SCHED_TEST_OP_YIELDTO);
	assert(ret.e == OK);

	schedule_check_switched(ret.r, false);

	atomic_store_relaxed(&CPULOCAL(wait_flag), 1U);
	while (atomic_load_relaxed(&CPULOCAL(wait_flag)) == 1U) {
		scheduler_yield_to(ret.r);
	}
	atomic_store_relaxed(&CPULOCAL(wait_flag), 0U);

	destroy_thread(ret.r);
	destroy_thread(CPULOCAL(test_thread));
	CPULOCAL(test_passed_count)++;

#if SCHEDULER_CAN_MIGRATE
	error_t err;

	// Test 4: set affinity & yield to
	ret = create_thread(SCHEDULER_MAX_PRIORITY, CPU_INDEX_INVALID,
			    SCHED_TEST_OP_YIELDTO);
	assert(ret.e == OK);

	schedule_check_switched(ret.r, false);

	CPULOCAL(test_thread) = thread_get_self();
	scheduler_lock_nopreempt(ret.r);
	err = scheduler_set_affinity(ret.r, cpulocal_get_index());
	scheduler_unlock_nopreempt(ret.r);
	assert(err == OK);

	schedule_check_switched(ret.r, true);

	scheduler_yield_to(ret.r);
	destroy_thread(ret.r);
	CPULOCAL(test_passed_count)++;

	(void)atomic_fetch_add_explicit(&sync_flag, 1U, memory_order_relaxed);
	while (asm_event_load_before_wait(&sync_flag) < PLATFORM_MAX_CORES) {
		asm_event_wait(&sync_flag);
	}

	// Test 5: migrate running thread
	ret = create_thread(SCHEDULER_DEFAULT_PRIORITY, cpulocal_get_index(),
			    SCHED_TEST_OP_AFFINITY);
	assert(ret.e == OK);

	while (atomic_load_relaxed(&CPULOCAL(affinity_count)) <
	       NUM_AFFINITY_SWITCH) {
		scheduler_yield();
		scheduler_lock_nopreempt(ret.r);
		cpu_index_t affinity = (scheduler_get_affinity(ret.r) + 1U) %
				       PLATFORM_MAX_CORES;
		err = scheduler_set_affinity(ret.r, affinity);
		scheduler_unlock_nopreempt(ret.r);
		assert((err == OK) || (err == ERROR_RETRY));
	}

	// Ensure the thread is running on the current CPU so we can yield to it
	// and ensure it exits.
	do {
		scheduler_lock_nopreempt(ret.r);
		err = scheduler_set_affinity(ret.r, cpulocal_get_index());
		scheduler_unlock_nopreempt(ret.r);
		assert((err == OK) || (err == ERROR_RETRY));
	} while (err == ERROR_RETRY);

	destroy_thread(ret.r);
	CPULOCAL(test_passed_count)++;
#endif

	return false;
}

static void
sched_test_thread_entry(uintptr_t param)
{
	cpulocal_begin();

	sched_test_param_t test_param = sched_test_param_cast((uint32_t)param);
	sched_test_op_t	   op	      = sched_test_param_get_op(&test_param);

	switch (op) {
	case SCHED_TEST_OP_INCREMENT:
		(void)atomic_fetch_add_explicit(&CPULOCAL(wait_flag), 1U,
						memory_order_relaxed);
		break;
	case SCHED_TEST_OP_WAKE: {
		_Atomic uint8_t *wait_flag = &CPULOCAL(wait_flag);
		cpulocal_end();
		while (asm_event_load_before_wait(wait_flag) == 0U) {
			asm_event_wait(wait_flag);
		}
		asm_event_store_and_wake(wait_flag, 0U);
		cpulocal_begin();
		break;
	}
	case SCHED_TEST_OP_YIELDTO:
		while (atomic_load_relaxed(&CPULOCAL(wait_flag)) == 1U) {
			scheduler_yield_to(CPULOCAL(test_thread));
		}
		break;
	case SCHED_TEST_OP_AFFINITY: {
		cpu_index_t parent = sched_test_param_get_parent(&test_param);
		_Atomic count_t *aff_count =
			&CPULOCAL_BY_INDEX(affinity_count, parent);
		while (atomic_load_relaxed(aff_count) < NUM_AFFINITY_SWITCH) {
			(void)atomic_fetch_add_explicit(aff_count, 1U,
							memory_order_relaxed);
			scheduler_yield();
		}
		break;
	}
	default:
		panic("Invalid param for sched test thread!");
	}

	cpulocal_end();
}

thread_func_t
sched_test_get_entry_fn(thread_kind_t kind)
{
	assert(kind == THREAD_KIND_SCHED_TEST);

	return sched_test_thread_entry;
}

uintptr_t
sched_test_get_stack_base(thread_kind_t kind, thread_t *thread)
{
	assert(kind == THREAD_KIND_SCHED_TEST);
	assert(thread != NULL);

	size_t	  stack_area = THREAD_STACK_MAP_ALIGN;
	uintptr_t stack_base = atomic_fetch_add_explicit(
		&sched_test_stack_alloc, stack_area, memory_order_relaxed);

	assert(stack_base >= sched_test_stack_base);
	assert((stack_base + (stack_area - 1U)) <= sched_test_stack_end);

	return stack_base;
}
#else

extern char unused;

#endif

```

`hyp/core/scheduler_trivial/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

interface scheduler
events scheduler_trivial.ev
types scheduler_trivial.tc
source scheduler_trivial.c
configs SCHEDULER_CAN_MIGRATE=0

```

`hyp/core/scheduler_trivial/scheduler_trivial.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

module scheduler_trivial

subscribe object_create_thread

subscribe object_activate_thread
	unwinder scheduler_trivial_handle_object_deactivate_thread(thread)

subscribe object_deactivate_thread

subscribe boot_cold_init()

subscribe ipi_received[IPI_REASON_RESCHEDULE]
	handler scheduler_trivial_handle_ipi_reschedule()

```

`hyp/core/scheduler_trivial/scheduler_trivial.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <types/bitmap.h>

define SCHEDULER_VARIANT constant enumeration scheduler_variant = 0x0;

define SCHEDULER_NUM_BLOCK_BITS constant type index_t = maxof(enumeration scheduler_block) + 1;

extend thread object module scheduler {
	block_bits BITMAP(SCHEDULER_NUM_BLOCK_BITS);
	lock structure spinlock;
};

extend ipi_reason enumeration {
	RESCHEDULE;
};

```

`hyp/core/scheduler_trivial/src/scheduler_trivial.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>
#include <string.h>

#include <atomic.h>
#include <bitmap.h>
#include <compiler.h>
#include <cpulocal.h>
#include <idle.h>
#include <ipi.h>
#include <log.h>
#include <object.h>
#include <preempt.h>
#include <rcu.h>
#include <scheduler.h>
#include <spinlock.h>
#include <thread.h>
#include <trace.h>

#include <events/scheduler.h>

#include <asm/event.h>

#include "event_handlers.h"

// Writes protected by per-CPU scheduler lock; reads protected by RCU
CPULOCAL_DECLARE_STATIC(spinlock_t, active_thread_lock);
CPULOCAL_DECLARE_STATIC(thread_t *_Atomic, active_thread);

error_t
scheduler_trivial_handle_object_create_thread(thread_create_t thread_create)
{
	thread_t *thread = thread_create.thread;
	assert(thread != NULL);

	spinlock_init(&thread->scheduler_lock);

	cpu_index_t cpu		   = thread_create.scheduler_affinity_valid
					     ? thread_create.scheduler_affinity
					     : cpulocal_get_index();
	thread->scheduler_affinity = cpu;

	return OK;
}

error_t
scheduler_trivial_handle_object_activate_thread(thread_t *thread)
{
	error_t err;

	cpu_index_t cpu = thread->scheduler_affinity;

	spinlock_acquire(&CPULOCAL_BY_INDEX(active_thread_lock, cpu));
	thread_t *_Atomic *active_thread_p =
		&CPULOCAL_BY_INDEX(active_thread, cpu);
	if (bitmap_isset(thread->scheduler_block_bits, SCHEDULER_BLOCK_IDLE)) {
		// This is the idle thread; don't make it the active thread
		err = OK;
	} else if (atomic_load_relaxed(active_thread_p) == NULL) {
		// This is the active thread; remember it
		atomic_store_relaxed(active_thread_p, thread);
		err = OK;
	} else {
		err = ERROR_BUSY;
	}
	spinlock_release(&CPULOCAL_BY_INDEX(active_thread_lock, cpu));

	return err;
}

void
scheduler_trivial_handle_object_deactivate_thread(thread_t *thread)
{
	assert(thread != NULL);

	cpu_index_t cpu = thread->scheduler_affinity;
	assert(cpulocal_index_valid(cpu));

	spinlock_acquire(&CPULOCAL_BY_INDEX(active_thread_lock, cpu));
	thread_t *_Atomic *active_thread_p =
		&CPULOCAL_BY_INDEX(active_thread, cpu);
	if (atomic_load_relaxed(active_thread_p) == thread) {
		atomic_store_relaxed(active_thread_p, NULL);
	}
	spinlock_release(&CPULOCAL_BY_INDEX(active_thread_lock, cpu));
}

void
scheduler_trivial_handle_boot_cold_init(void)
{
	for (cpu_index_t i = 0; cpulocal_index_valid(i); i++) {
		spinlock_init(&CPULOCAL_BY_INDEX(active_thread_lock, i));
	}
}

bool
scheduler_schedule(void)
{
	preempt_disable();
	bool must_schedule = true;
	bool switched	   = false;

	for (count_t i = 0; must_schedule; i++) {
#if defined(NDEBUG)
		(void)i;
#else
		const count_t reschedule_warn_limit = 16;
		if (i == reschedule_warn_limit) {
			TRACE_AND_LOG(ERROR, WARN,
				      "Possible reschedule loop on CPU {:d}",
				      cpulocal_get_index());
		}
#endif

		rcu_read_start();

		thread_t *target =
			atomic_load_consume(&CPULOCAL(active_thread));

		if ((target != NULL) && !scheduler_is_runnable(target)) {
			target = NULL;
		}

		if ((target != NULL) && !object_get_thread_safe(target)) {
			target = NULL;
		}

		if (target == NULL) {
			target = object_get_thread_additional(idle_thread());
		}

		rcu_read_finish();

		bool can_idle = true;
		trigger_scheduler_selected_thread_event(target, &can_idle);

		if (target != thread_get_self()) {
			// The reference we took above will be released when the
			// thread stops running.
			error_t err = thread_switch_to(target);
			assert(err == OK);
			switched = true;
			must_schedule =
				ipi_clear_relaxed(IPI_REASON_RESCHEDULE);
		} else {
			trigger_scheduler_quiescent_event();
			object_put_thread(target);
			must_schedule = false;
		}
	}

	preempt_enable();

	return switched;
}

void
scheduler_trigger(void)
{
	// Note that we don't need to disable preemption here; if we are
	// preempted and switch CPU, that implies that the reschedule we
	// were being called to trigger has already happened.
	//
	// This function is typically called when preemption is off anyway (as
	// scheduler_schedule() would be called otherwise).
	cpu_index_t cpu = cpulocal_get_index();
	ipi_one_relaxed(IPI_REASON_RESCHEDULE, cpu);
}

void
scheduler_yield(void)
{
	(void)scheduler_schedule();
}

void
scheduler_yield_to(thread_t *target)
{
	(void)target;
	(void)scheduler_schedule();
}

void
scheduler_lock(thread_t *thread)
{
	spinlock_acquire(&thread->scheduler_lock);
}

void
scheduler_unlock(thread_t *thread)
{
	spinlock_release(&thread->scheduler_lock);
}

void
scheduler_block(thread_t *thread, scheduler_block_t block)
{
	TRACE(DEBUG, INFO,
	      "scheduler: block {:#x}, reason: {:d}, others: {:#x}",
	      (uintptr_t)thread, block, thread->scheduler_block_bits[0]);

	assert_preempt_disabled();
	assert(block <= SCHEDULER_BLOCK__MAX);
	bool block_was_set = bitmap_isset(thread->scheduler_block_bits, block);

	if (!bitmap_isset(thread->scheduler_block_bits, block)) {
		trigger_scheduler_blocked_event(thread, block,
						scheduler_is_runnable(thread));
	}

	bitmap_set(thread->scheduler_block_bits, block);
}

void
scheduler_block_init(thread_t *thread, scheduler_block_t block)
{
	assert(block <= SCHEDULER_BLOCK__MAX);
	bitmap_set(thread->scheduler_block_bits, block);
}

bool
scheduler_trivial_handle_ipi_reschedule(void)
{
	return true;
}

bool
scheduler_unblock(thread_t *thread, scheduler_block_t block)
{
	assert_preempt_disabled();
	assert(block <= SCHEDULER_BLOCK__MAX);
	bool block_was_set = bitmap_isset(thread->scheduler_block_bits, block);
	bitmap_clear(thread->scheduler_block_bits, block);
	bool now_runnable  = scheduler_is_runnable(thread);
	bool need_schedule = block_was_set && now_runnable;

	if (need_schedule) {
		cpu_index_t cpu = cpulocal_get_index();
		if (cpu != thread->scheduler_affinity) {
			ipi_one(IPI_REASON_RESCHEDULE,
				thread->scheduler_affinity);
			need_schedule = false;
		}
	}

	TRACE(DEBUG, INFO,
	      "scheduler: unblock {:#x}, reason: {:d}, others: {:#x}, local run: {:d}",
	      (uintptr_t)thread, block, thread->scheduler_block_bits[0],
	      need_schedule);

	if (block_was_set) {
		trigger_scheduler_unblocked_event(thread, block, now_runnable);
	}

	return need_schedule;
}

bool
scheduler_is_blocked(const thread_t *thread, scheduler_block_t block)
{
	assert(block <= SCHEDULER_BLOCK__MAX);
	return bitmap_isset(thread->scheduler_block_bits, block);
}

bool
scheduler_is_runnable(const thread_t *thread)
{
	return bitmap_empty(thread->scheduler_block_bits,
			    SCHEDULER_NUM_BLOCK_BITS);
}

bool
scheduler_is_running(const thread_t *thread)
{
	bool	    ret;
	cpu_index_t cpu = thread->scheduler_affinity;

	if (!cpulocal_index_valid(cpu)) {
		ret = false;
		goto out;
	}

	thread_t *active_thread =
		atomic_load_consume(&CPULOCAL_BY_INDEX(active_thread, cpu));
	bool active_runnable = scheduler_is_runnable(active_thread);

	// Its either the active_thread or idle thread.
	if (thread == active_thread) {
		ret = active_runnable;
	} else {
		ret = !active_runnable;
		assert(thread == idle_thread_for(cpu));
	}
out:
	return ret;
}

thread_t *
scheduler_get_primary_vcpu(cpu_index_t cpu)
{
	return atomic_load_consume(&CPULOCAL_BY_INDEX(active_thread, cpu));
}

void
scheduler_pin(thread_t *thread)
{
	assert_preempt_disabled();
	(void)thread;
}

void
scheduler_unpin(thread_t *thread)
{
	assert_preempt_disabled();
	(void)thread;
}

cpu_index_t
scheduler_get_affinity(thread_t *thread)
{
	assert_preempt_disabled();

	return thread->scheduler_affinity;
}

error_t
scheduler_set_affinity(thread_t *thread, cpu_index_t target_cpu)
{
	assert_preempt_disabled();

	error_t	    err	      = OK;
	bool	    need_sync = false;
	cpu_index_t prev_cpu  = thread->scheduler_affinity;

	if (prev_cpu == target_cpu) {
		goto out;
	}

	if (!cpulocal_index_valid(target_cpu)) {
		err = ERROR_ARGUMENT_INVALID;
		goto out;
	}

	thread->scheduler_affinity = target_cpu;
	err = trigger_scheduler_set_affinity_prepare_event(thread, prev_cpu,
							   target_cpu);
	if (err != OK) {
		goto out;
	}
	trigger_scheduler_affinity_changed_event(thread, prev_cpu, target_cpu,
						 &need_sync);

out:
	return err;
}

bool
scheduler_will_preempt_current(thread_t *thread)
{
	assert_preempt_disabled();
	return false;
}

```

`hyp/core/spinlock_null/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

interface spinlock
types spinlock.tc
source spinlock_null.c

```

`hyp/core/spinlock_null/spinlock.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

define spinlock structure {
	dummy uint8;
};

```

`hyp/core/spinlock_null/src/spinlock_null.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <hyptypes.h>

#include <spinlock.h>

// Dummy spinlock implementation used for uniprocessor builds.

void
spinlock_init(spinlock_t *lock)
{
	trigger_spinlock_init_event(lock);
}

void
spinlock_acquire(spinlock_t *lock)
{
	trigger_spinlock_acquire_event(lock);
	trigger_spinlock_acquired_event(lock);
}

bool
spinlock_trylock(spinlock_t *lock)
{
	// Always succeeds; this must not be used to prevent recursion!
	spinlock_acquire(lock);
	return true;
}

void
spinlock_release(spinlock_t *lock)
{
	trigger_spinlock_release_event(lock);
	trigger_spinlock_released_event(lock);
}

```

`hyp/core/spinlock_ticket/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

interface spinlock
types spinlock.tc
source spinlock_ticket.c
macros spinlock_attrs.h

```

`hyp/core/spinlock_ticket/include/spinlock_attrs.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#ifdef __EVENTS_DSL__
#define require_spinlock(lock)                                                 \
	require_preempt_disabled;                                              \
	require_lock(lock)
#else
#define ACQUIRE_SPINLOCK(lock)	  ACQUIRE_LOCK(lock) ACQUIRE_PREEMPT_DISABLED
#define ACQUIRE_SPINLOCK_NP(lock) ACQUIRE_LOCK(lock) REQUIRE_PREEMPT_DISABLED
#define TRY_ACQUIRE_SPINLOCK(success, lock)                                    \
	TRY_ACQUIRE_LOCK(success, lock) TRY_ACQUIRE_PREEMPT_DISABLED(success)
#define TRY_ACQUIRE_SPINLOCK_NP(success, lock)                                 \
	TRY_ACQUIRE_LOCK(success, lock) REQUIRE_PREEMPT_DISABLED
#define RELEASE_SPINLOCK(lock)	  RELEASE_LOCK(lock) RELEASE_PREEMPT_DISABLED
#define RELEASE_SPINLOCK_NP(lock) RELEASE_LOCK(lock) REQUIRE_PREEMPT_DISABLED
#define REQUIRE_SPINLOCK(lock)	  REQUIRE_LOCK(lock) REQUIRE_PREEMPT_DISABLED
#define EXCLUDE_SPINLOCK(lock)	  EXCLUDE_LOCK(lock)
#endif

```

`hyp/core/spinlock_ticket/spinlock.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

define spinlock structure(lockable, aligned(4)) {
	now_serving uint16(atomic);
	next_ticket uint16(atomic);
};

```

`hyp/core/spinlock_ticket/src/spinlock_ticket.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <hyptypes.h>

#include <atomic.h>
#include <preempt.h>
#include <spinlock.h>

#include <events/spinlock.h>

#include <asm/event.h>

// Ticket spinlock implementation, used for multiprocessor builds on
// architectures that have event-wait instructions (i.e. ARMv7 and ARMv8). If
// there is no event-wait then a more cache-efficient (but more complex) lock
// may be preferable.

void
spinlock_init(spinlock_t *lock)
{
	atomic_init(&lock->now_serving, 0);
	atomic_init(&lock->next_ticket, 0);
	trigger_spinlock_init_event(lock);
}

void
spinlock_acquire(spinlock_t *lock)
{
	preempt_disable();
	spinlock_acquire_nopreempt(lock);
}

void
spinlock_acquire_nopreempt(spinlock_t *lock) LOCK_IMPL
{
	trigger_spinlock_acquire_event(lock);

	// Take a ticket
	uint16_t my_ticket = atomic_fetch_add_explicit(&lock->next_ticket, 1,
						       memory_order_relaxed);

	// Wait until our ticket is being served
	while (asm_event_load_before_wait(&lock->now_serving) != my_ticket) {
		asm_event_wait(&lock->now_serving);
	}

	trigger_spinlock_acquired_event(lock);
}

bool
spinlock_trylock(spinlock_t *lock)
{
	bool success;

	preempt_disable();
	success = spinlock_trylock_nopreempt(lock);
	if (!success) {
		preempt_enable();
	}

	return success;
}

bool
spinlock_trylock_nopreempt(spinlock_t *lock) LOCK_IMPL
{
	trigger_spinlock_acquire_event(lock);

	// See which ticket is being served
	uint16_t now_serving = atomic_load_relaxed(&lock->now_serving);

	// Take a ticket, but only if it's being served already
	bool success = atomic_compare_exchange_strong_explicit(
		&lock->next_ticket, &now_serving, now_serving + 1U,
		memory_order_acquire, memory_order_relaxed);

	if (success) {
		trigger_spinlock_acquired_event(lock);
	} else {
		trigger_spinlock_failed_event(lock);
	}
	return success;
}

void
spinlock_release(spinlock_t *lock)
{
	spinlock_release_nopreempt(lock);
	preempt_enable();
}

void
spinlock_release_nopreempt(spinlock_t *lock) LOCK_IMPL
{
	uint16_t now_serving = atomic_load_relaxed(&lock->now_serving);

	trigger_spinlock_release_event(lock);

	// Start serving the next ticket
	asm_event_store_and_wake(&lock->now_serving, now_serving + 1U);

	trigger_spinlock_released_event(lock);
}

void
assert_spinlock_held(const spinlock_t *lock)
{
	assert_preempt_disabled();
	trigger_spinlock_assert_held_event(lock);
}

```

`hyp/core/task_queue/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

interface task_queue
source task_queue.c
types task_queue.tc
events task_queue.ev

```

`hyp/core/task_queue/src/task_queue.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>

#include <cpulocal.h>
#include <ipi.h>
#include <preempt.h>
#include <rcu.h>
#include <spinlock.h>
#include <task_queue.h>

#include <events/task_queue.h>

#include "event_handlers.h"

CPULOCAL_DECLARE_STATIC(task_queue_entry_t, task_queue_head);
CPULOCAL_DECLARE_STATIC(spinlock_t, task_queue_lock);

void
task_queue_handle_boot_cpu_cold_init(cpu_index_t cpu)
{
	spinlock_init(&CPULOCAL_BY_INDEX(task_queue_lock, cpu));
	task_queue_entry_t *head = &CPULOCAL_BY_INDEX(task_queue_head, cpu);
	task_queue_entry_bf_set_prev(&head->bf, head);
	task_queue_entry_bf_set_next(&head->bf, head);
	task_queue_entry_bf_set_class(&head->bf, TASK_QUEUE_CLASS_HEAD);
	task_queue_entry_bf_set_cpu(&head->bf, cpu);
}

void
task_queue_init(task_queue_entry_t *entry, task_queue_class_t task_class)
{
	entry->bf = task_queue_entry_bf_default();
	task_queue_entry_bf_set_class(&entry->bf, task_class);
	task_queue_entry_bf_set_cpu(&entry->bf, PLATFORM_MAX_CORES);
}

error_t
task_queue_schedule(task_queue_entry_t *entry)
{
	error_t err;

	// The entry must not be queued already.
	if (task_queue_entry_bf_get_cpu(&entry->bf) < PLATFORM_MAX_CORES) {
		err = ERROR_BUSY;
		goto out;
	}

	cpulocal_begin();
	cpu_index_t cpu = cpulocal_get_index();
	spinlock_acquire_nopreempt(&CPULOCAL_BY_INDEX(task_queue_lock, cpu));

	task_queue_entry_t *head = &CPULOCAL_BY_INDEX(task_queue_head, cpu);
	task_queue_entry_t *tail = task_queue_entry_bf_get_prev(&head->bf);

	task_queue_entry_bf_set_cpu(&entry->bf, cpu);
	task_queue_entry_bf_set_next(&entry->bf, head);
	task_queue_entry_bf_set_prev(&entry->bf, tail);
	task_queue_entry_bf_set_prev(&head->bf, entry);
	task_queue_entry_bf_set_next(&tail->bf, entry);

	spinlock_release_nopreempt(&CPULOCAL_BY_INDEX(task_queue_lock, cpu));
	cpulocal_end();

	ipi_one_relaxed(IPI_REASON_TASK_QUEUE, cpu);

	err = OK;
out:
	return err;
}

// Cancel future execution of a given task queue entry.
//
// Note that this does not cancel execution if it has already started. Any
// execution that has already started is not guaranteed to be complete until an
// RCU grace period has elapsed. Also, the entry may not be safely freed until
// an RCU grace period has elapsed.
error_t
task_queue_cancel(task_queue_entry_t *entry)
{
	error_t err;

	cpu_index_t cpu = task_queue_entry_bf_get_cpu(&entry->bf);

	if (cpu >= PLATFORM_MAX_CORES) {
		err = ERROR_IDLE;
		goto out;
	}

	spinlock_acquire(&CPULOCAL_BY_INDEX(task_queue_lock, cpu));

	task_queue_entry_t *next = task_queue_entry_bf_get_next(&entry->bf);
	task_queue_entry_t *prev = task_queue_entry_bf_get_prev(&entry->bf);

	task_queue_entry_bf_set_next(&prev->bf, next);
	task_queue_entry_bf_set_prev(&next->bf, prev);

	spinlock_release(&CPULOCAL_BY_INDEX(task_queue_lock, cpu));

	task_queue_entry_bf_set_prev(&entry->bf, NULL);
	task_queue_entry_bf_set_next(&entry->bf, NULL);
	task_queue_entry_bf_set_cpu(&entry->bf, PLATFORM_MAX_CORES);

	err = OK;
out:
	return err;
}

bool
task_queue_handle_ipi_received(void)
{
	assert_preempt_disabled();

	// Ensure that no deleted objects are freed while this handler is
	// running. Note that the pointers don't need the usual RCU barriers
	// because they are protected by the queue spinlock.
	rcu_read_start();

	cpu_index_t	    cpu	 = cpulocal_get_index();
	task_queue_entry_t *head = &CPULOCAL_BY_INDEX(task_queue_head, cpu);
	spinlock_t	   *lock = &CPULOCAL_BY_INDEX(task_queue_lock, cpu);

	spinlock_acquire_nopreempt(lock);
	task_queue_entry_t *entry = task_queue_entry_bf_get_next(&head->bf);
	while (entry != head) {
		// Remove the entry from the list
		task_queue_class_t task_class =
			task_queue_entry_bf_get_class(&entry->bf);
		task_queue_entry_t *next =
			task_queue_entry_bf_get_next(&entry->bf);
		task_queue_entry_bf_set_next(&head->bf, next);
		task_queue_entry_bf_set_prev(&next->bf, head);

		// Release the lock so deletions on other cores don't block,
		// and so we can safely queue tasks in the execute handler
		spinlock_release_nopreempt(lock);

		// Clear out the entry so it can be reused
		task_queue_init(entry, task_class);

		// Execute the task
		error_t err =
			trigger_task_queue_execute_event(task_class, entry);
		assert(err == OK);

		// Re-acquire the lock and find the next entry.
		spinlock_acquire_nopreempt(lock);
		entry = task_queue_entry_bf_get_next(&head->bf);
	}
	spinlock_release_nopreempt(lock);

	rcu_read_finish();

	return true;
}

```

`hyp/core/task_queue/task_queue.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

module task_queue

subscribe boot_cpu_cold_init(cpu_index)

subscribe ipi_received[IPI_REASON_TASK_QUEUE]()
	require_preempt_disabled

```

`hyp/core/task_queue/task_queue.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

define task_queue_entry_bf bitfield<128> {
	63:4	next	pointer structure task_queue_entry;
	auto	class	enumeration task_queue_class;
	127:68	prev	pointer structure task_queue_entry;
	auto<msb(PLATFORM_MAX_CORES)+1>
		cpu	type cpu_index_t;
};

extend task_queue_entry structure {
	bf bitfield task_queue_entry_bf(aligned(16));
};

extend task_queue_class enumeration {
	// Internal class used for the head of the per-CPU queues. This helps
	// to avoid special cases for the first queue element.
	HEAD = 0;
};

extend ipi_reason enumeration {
	task_queue;
};

```

`hyp/core/tests/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

interface tests
types tests.tc
events tests.ev
source tests.c
source spinlock_tests.c
source print_version.c

```

`hyp/core/tests/src/print_version.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <hyptypes.h>

#include <hypconstants.h>

#include <atomic.h>
#include <compiler.h>
#include <log.h>
#include <trace.h>

#include <events/tests.h>

#include "event_handlers.h"

extern const char  hypervisor_version[];
static const char *msg_ptr;

void
test_print_hyp_version_init(void)
{
	msg_ptr = hypervisor_version;
	return;
}

error_t
test_print_hyp_version(tests_run_id_t test_id)

{
	if (test_id == TESTS_RUN_ID_SMC_0) {
		LOG(USER, TEST, "{:s}", (register_t)msg_ptr);
		return OK;
	} else {
		return ERROR_UNIMPLEMENTED;
	}
}

```

`hyp/core/tests/src/spinlock_tests.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>

#include <atomic.h>
#include <bitmap.h>
#include <cpulocal.h>
#include <panic.h>
#include <partition.h>
#include <partition_alloc.h>
#include <spinlock.h>

#include <asm/event.h>

#include "event_handlers.h"

#define TEST_ITERATIONS 100

extern test_info_t test_info;
test_info_t	   test_info;
extern test_info_t test_spinlock_multi_info;
test_info_t	   test_spinlock_multi_info;
extern test_info_t test_spinlock_multi_lock[PLATFORM_MAX_CORES];
test_info_t	   test_spinlock_multi_lock[PLATFORM_MAX_CORES];

#if defined(UNIT_TESTS)
void
tests_spinlock_single_lock_init(void)
{
	// Initialize spinlock with boot cpu
	spinlock_init(&test_info.lock);
	test_info.count = 0;
}
#endif

// Only one core can increment the count at the time.
bool
tests_spinlock_single_lock(void)
{
	bool ret		  = false;
	bool wait_all_cores_start = true;
	bool wait_all_cores_end	  = true;

	spinlock_acquire_nopreempt(&test_info.lock);
	test_info.count++;
	spinlock_release_nopreempt(&test_info.lock);

	// Wait until all cores have reached this point to start.
	while (wait_all_cores_start) {
		spinlock_acquire_nopreempt(&test_info.lock);

		if (!cpulocal_index_valid((cpu_index_t)test_info.count)) {
			wait_all_cores_start = false;
		}

		spinlock_release_nopreempt(&test_info.lock);
	}

	for (count_t i = 0; i < TEST_ITERATIONS; i++) {
		spinlock_acquire_nopreempt(&test_info.lock);
		test_info.count++;
		spinlock_release_nopreempt(&test_info.lock);
	}

	// If test succeeds, count should be (TEST_ITERATIONS *
	// PLATFORM_MAX_CORES) + PLATFORM_MAX_CORES
	while (wait_all_cores_end) {
		spinlock_acquire_nopreempt(&test_info.lock);

		if (test_info.count == ((TEST_ITERATIONS * PLATFORM_MAX_CORES) +
					PLATFORM_MAX_CORES)) {
			wait_all_cores_end = false;
		}

		spinlock_release_nopreempt(&test_info.lock);
	}

	return ret;
}

#if defined(UNIT_TESTS)
void
tests_spinlock_multiple_locks_init(void)
{
	// Initialize spinlocks with boot cpu
	spinlock_init(&test_spinlock_multi_info.lock);
	test_spinlock_multi_info.count = 0;

	for (int i = 0; cpulocal_index_valid((cpu_index_t)i); i++) {
		spinlock_init(&test_spinlock_multi_lock[i].lock);
		test_spinlock_multi_lock[i].count = 0;
	}
}
#endif

// Dinning philosophers example
// Only philosophers that hold two forks can be eating at the same time.
// To avoid deadlock the odd and even numbers start picking differently.
bool
tests_spinlock_multiple_locks(void)
{
	bool ret		  = false;
	bool wait_all_cores_start = true;
	bool wait_all_cores_end	  = true;

	const cpu_index_t cpu = cpulocal_get_index();

	spinlock_acquire_nopreempt(&test_spinlock_multi_info.lock);
	test_spinlock_multi_info.count++;
	spinlock_release_nopreempt(&test_spinlock_multi_info.lock);

	index_t left  = cpu;
	index_t right = cpu + 1;

	if (cpu == (PLATFORM_MAX_CORES - 1)) {
		right = 0;
	}

	// Wait until all cores have reached this point to start.
	while (wait_all_cores_start) {
		spinlock_acquire_nopreempt(&test_spinlock_multi_info.lock);

		if (test_spinlock_multi_info.count == PLATFORM_MAX_CORES) {
			wait_all_cores_start = false;
		}

		spinlock_release_nopreempt(&test_spinlock_multi_info.lock);
	}

	for (count_t i = 0; i < TEST_ITERATIONS; i++) {
		if ((cpu % 2) == 0U) {
			spinlock_acquire_nopreempt(
				&test_spinlock_multi_lock[left].lock);
			spinlock_acquire_nopreempt(
				&test_spinlock_multi_lock[right].lock);
		} else {
			spinlock_acquire_nopreempt(
				&test_spinlock_multi_lock[right].lock);
			spinlock_acquire_nopreempt(
				&test_spinlock_multi_lock[left].lock);
		}

		test_spinlock_multi_lock[left].count++;
		test_spinlock_multi_lock[right].count++;

		spinlock_release_nopreempt(
			&test_spinlock_multi_lock[left].lock);
		spinlock_release_nopreempt(
			&test_spinlock_multi_lock[right].lock);
	}

	// If test succeeds, each fork count should be (2 * TEST_ITERATIONS)
	while (wait_all_cores_end) {
		spinlock_acquire_nopreempt(
			&test_spinlock_multi_lock[left].lock);

		if (test_spinlock_multi_lock[left].count ==
		    (2 * TEST_ITERATIONS)) {
			wait_all_cores_end = false;
		}

		spinlock_release_nopreempt(
			&test_spinlock_multi_lock[left].lock);
	}

	return ret;
}

```

`hyp/core/tests/src/string_test.c`:

```c
// © 2022 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <hyptypes.h>

#include <hypconstants.h>

#include <atomic.h>
#include <compiler.h>
#include <cpu.h>
#include <log.h>
#include <trace.h>

#include <events/tests.h>

static uint8_t test_data_buff[512], test_buff[2048];

#define CPU_MEMCPY_STRIDE 256

void
memmove_test(void)
{
	uint16_t i;
	uint8_t	 b = 0;

	for (i = 0; i < sizeof(test_data_buff); ++i) {
		test_data_buff[i] = b;
		test_buff[i]	  = b;

		++b;
		if (b == 251) {
			b = 0;
		}
	}

	uint8_t *test_src, *test_dst;

	test_src = test_buff;
	test_dst = test_src + CPU_MEMCPY_STRIDE + 1;

	memmove(test_dst, test_src, CPU_MEMCPY_STRIDE + 13);

	for (i = 0; i < (CPU_MEMCPY_STRIDE + 13); ++i) {
		if (test_dst[i] != test_data_buff[i]) {
			LOG(ERROR, WARN, "Err: pos {:d}, exp {:d}, act,{:d}\n",
			    i, test_dst[i], test_data_buff[i]);
		}
	}
}

```

`hyp/core/tests/src/tests.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>

#include <allocator.h>
#include <bitmap.h>
#include <compiler.h>
#include <cpulocal.h>
#include <hyp_aspace.h>
#include <log.h>
#include <memdb.h>
#include <object.h>
#include <panic.h>
#include <partition.h>
#include <partition_alloc.h>
#include <partition_init.h>
#include <preempt.h>
#include <scheduler.h>
#include <trace.h>
#include <util.h>

#include <events/tests.h>

#include "event_handlers.h"

static uintptr_t test_thread_stack_base;

#if defined(UNIT_TESTS)
static thread_t *
tests_thread_create(cpu_index_t i)
{
	thread_create_t params = {
		.scheduler_affinity	  = i,
		.scheduler_affinity_valid = true,
		.kind			  = THREAD_KIND_TEST,
		.params			  = i,
		.stack_size		  = THREAD_STACK_MAX_SIZE,
	};

	thread_ptr_result_t ret =
		partition_allocate_thread(partition_get_private(), params);

	if (ret.e != OK) {
		panic("Unable to create test thread");
	}

	if (object_activate_thread(ret.r) != OK) {
		panic("Error activating test thread");
	}

	return ret.r;
}
#endif

error_t
tests_handle_object_create_thread(thread_create_t create)
{
	thread_t *thread = create.thread;
	assert(thread != NULL);

	if (thread->kind == THREAD_KIND_TEST) {
		scheduler_block_init(thread, SCHEDULER_BLOCK_TEST);
	}

	return OK;
}

#if defined(UNIT_TESTS)
static void
tests_add_root_partition_heap(void)
{
	// Grab some kernel heap from the hypervisor_partition and give it to
	// the root partition allocator.
	void_ptr_result_t ret;
	size_t		  root_alloc_size = 0x20000;

	partition_t *hyp_partition  = partition_get_private();
	partition_t *root_partition = partition_get_root();

	ret = partition_alloc(hyp_partition, root_alloc_size, 4096U);
	if (ret.e != OK) {
		panic("Error allocating root partition heap");
	}

	paddr_t root_alloc_base =
		partition_virt_to_phys(hyp_partition, (uintptr_t)ret.r);

	error_t err = memdb_update(hyp_partition, root_alloc_base,
				   root_alloc_base + (root_alloc_size - 1U),
				   (uintptr_t)root_partition,
				   MEMDB_TYPE_PARTITION,
				   (uintptr_t)&hyp_partition->allocator,
				   MEMDB_TYPE_ALLOCATOR);
	if (err != OK) {
		panic("Error adding root partition heap memory");
	}

	err = partition_map_and_add_heap(root_partition, root_alloc_base,
					 root_alloc_size);
	if (err != OK) {
		panic("Error mapping root partition heap memory");
	}
}
#endif

static void
tests_alloc_stack_space(void)
{
	size_t aspace_size = THREAD_STACK_MAP_ALIGN * (PLATFORM_MAX_CORES + 1);

	virt_range_result_t stack_range = hyp_aspace_allocate(aspace_size);
	if (stack_range.e != OK) {
		panic("Unable to allocate address space for test thread stacks");
	}

	// Start the idle stack range at the next alignment boundary.
	test_thread_stack_base =
		util_balign_up(stack_range.r.base + 1U, THREAD_STACK_MAP_ALIGN);
}

void
tests_thread_init(void)
{
#if defined(UNIT_TESTS)
	tests_add_root_partition_heap();
#endif
	tests_alloc_stack_space();

	trigger_tests_init_event();

#if defined(UNIT_TESTS)
	for (cpu_index_t i = 0; cpulocal_index_valid(i); i++) {
		thread_t *thread = tests_thread_create(i);
		scheduler_lock(thread);
		scheduler_unblock(thread, SCHEDULER_BLOCK_TEST);
		scheduler_unlock(thread);

		// The thread has a reference to itself now (until it exits); we
		// don't need to hold onto it.
		object_put_thread(thread);
	}
#endif
}

static void
tests_main(uintptr_t cpu_index)
{
	preempt_disable();
	if (trigger_tests_start_event()) {
		panic("Tests are failing.");
	} else {
		LOG(DEBUG, INFO, "Tests completed successfully on CPU {:d}",
		    cpu_index);
	}
	preempt_enable();
}

thread_func_t
tests_handle_thread_get_entry_fn(thread_kind_t kind)
{
	assert(kind == THREAD_KIND_TEST);

	return tests_main;
}

uintptr_t
tests_handle_thread_get_stack_base(thread_kind_t kind, thread_t *thread)
{
	assert(kind == THREAD_KIND_TEST);
	assert(thread != NULL);

	cpu_index_t cpu = thread->scheduler_affinity;

	return test_thread_stack_base + (cpu * THREAD_STACK_MAP_ALIGN);
}

```

`hyp/core/tests/tests.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

module tests

subscribe boot_hypervisor_start
	handler tests_thread_init()
	priority -100

subscribe thread_get_stack_base[THREAD_KIND_TEST]

#if defined (UNIT_TESTS)
subscribe tests_init
	handler tests_spinlock_single_lock_init()
#endif

subscribe tests_start
	handler tests_spinlock_single_lock()
	require_preempt_disabled

#if defined (UNIT_TESTS)
subscribe tests_init
	handler tests_spinlock_multiple_locks_init()
#endif

subscribe tests_start
	handler tests_spinlock_multiple_locks()
	require_preempt_disabled

subscribe thread_get_entry_fn[THREAD_KIND_TEST]

subscribe object_create_thread

subscribe tests_init
	handler test_print_hyp_version_init()

subscribe tests_run[TESTS_RUN_ID_SMC_0]
	handler test_print_hyp_version(test_id)

```

`hyp/core/tests/tests.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

define test_info structure {
	count type count_t;
	lock structure spinlock;
};

extend thread_kind enumeration {
	test;
};

extend scheduler_block enumeration {
	test;
};

extend tests_run_id enumeration {
	SMC_0 = 0x0;
};

extend trace_id enumeration {
	TEST = 4;
};

```

`hyp/core/thread_standard/aarch64/src/thread_arch.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>

#include <compiler.h>
#include <idle.h>
#include <object.h>
#include <panic.h>
#include <partition.h>
#include <pgtable.h>
#include <platform_timer.h>
#include <preempt.h>
#include <prng.h>
#include <scheduler.h>
#include <thread.h>
#include <trace.h>

#include <events/thread.h>

#include "event_handlers.h"
#include "thread_arch.h"

typedef register_t (*fptr_t)(register_t arg);
typedef void (*fptr_noreturn_t)(register_t arg);

const size_t thread_stack_min_align    = 16;
const size_t thread_stack_alloc_align  = PGTABLE_HYP_PAGE_SIZE;
const size_t thread_stack_size_default = PGTABLE_HYP_PAGE_SIZE;

static size_t
thread_get_tls_offset(void)
{
	size_t offset = 0;
	__asm__("add     %0, %0, :tprel_hi12:current_thread	;"
		"add     %0, %0, :tprel_lo12_nc:current_thread	;"
		: "+r"(offset));
	return offset;
}

static uintptr_t
thread_get_tls_base(thread_t *thread)
{
	return (uintptr_t)thread - thread_get_tls_offset();
}

static noreturn void
thread_arch_main(thread_t *prev, ticks_t schedtime) LOCK_IMPL
{
	thread_t *thread = thread_get_self();

	trigger_thread_start_event();

	trigger_thread_context_switch_post_event(prev, schedtime, (ticks_t)0UL);
	object_put_thread(prev);

	thread_func_t thread_func =
		trigger_thread_get_entry_fn_event(thread->kind);
	trigger_thread_load_state_event(true);

	if (thread_func != NULL) {
		preempt_enable();
		thread_func(thread->params);
	}

	thread_exit();
}

thread_t *
thread_arch_switch_thread(thread_t *next_thread, ticks_t *schedtime)
{
	// The previous thread and the scheduling time must be kept in X0 and X1
	// to ensure that thread_arch_main() receives them as arguments on the
	// first context switch.
	register thread_t *old __asm__("x0")   = thread_get_self();
	register ticks_t   ticks __asm__("x1") = *schedtime;

	// The remaining hard-coded registers here are only needed to ensure a
	// correct clobber list below. The union of the clobber list, hard-coded
	// registers and explicitly saved registers (x29, sp and pc) must be the
	// entire integer register state.
	register register_t old_pc __asm__("x2");
	register register_t old_sp __asm__("x3");
	register register_t old_fp __asm__("x4");
	register uintptr_t  old_context __asm__("x5") =
		(uintptr_t)&old->context.pc;
	static_assert(offsetof(thread_t, context.sp) ==
			      offsetof(thread_t, context.pc) +
				      sizeof(next_thread->context.pc),
		      "PC and SP must be adjacent in context");
	static_assert(offsetof(thread_t, context.fp) ==
			      offsetof(thread_t, context.sp) +
				      sizeof(next_thread->context.sp),
		      "SP and FP must be adjacent in context");

	// The new PC must be in x16 or x17 so ARMv8.5-BTI will treat the BR
	// below as a call trampoline, and thus allow it to jump to the BTI C
	// instruction at a new thread's entry point.
	register register_t new_pc __asm__("x16") = next_thread->context.pc;
	register register_t new_sp __asm__("x6")  = next_thread->context.sp;
	register register_t new_fp __asm__("x7")  = next_thread->context.fp;
	register uintptr_t  new_tls_base __asm__("x8") =
		thread_get_tls_base(next_thread);

	__asm__ volatile(
		"adr	%[old_pc], .Lthread_continue.%=		;"
		"mov	%[old_sp], sp				;"
		"mov	%[old_fp], x29				;"
		"mov   sp, %[new_sp]				;"
		"mov   x29, %[new_fp]				;"
		"msr	TPIDR_EL2, %[new_tls_base]		;"
		"stp	%[old_pc], %[old_sp], [%[old_context]]	;"
		"str	%[old_fp], [%[old_context], 16]		;"
		"br	%[new_pc]				;"
		".Lthread_continue.%=:				;"
#if defined(ARCH_ARM_FEAT_BTI)
		"bti	j					;"
#endif
		: [old] "+r"(old), [old_pc] "=&r"(old_pc),
		  [old_sp] "=&r"(old_sp), [old_fp] "=&r"(old_fp),
		  [old_context] "+r"(old_context), [new_pc] "+r"(new_pc),
		  [new_sp] "+r"(new_sp), [new_fp] "+r"(new_fp),
		  [new_tls_base] "+r"(new_tls_base), [ticks] "+r"(ticks)
		: /* This must not have any inputs */
		: "x9", "x10", "x11", "x12", "x13", "x14", "x15", "x17", "x18",
		  "x19", "x20", "x21", "x22", "x23", "x24", "x25", "x26", "x27",
		  "x28", "x30", "cc", "memory");

	// Update schedtime from the tick count passed by the previous thread
	*schedtime = ticks;

	return old;
}

noreturn void
thread_arch_set_thread(thread_t *thread)
{
	// This should only be called on the idle thread during power-up, which
	// should already be the current thread for TLS. It discards the current
	// execution state.
	assert(thread == thread_get_self());
	assert(thread == idle_thread());

	// The previous thread and the scheduling time must be kept in X0 and X1
	// to ensure that thread_arch_main() receives them as arguments on the
	// first context switch during CPU cold boot. The scheduling time is set
	// to 0 because we consider the idle thread to have been scheduled at
	// the epoch. These are unused on warm boot, which is always resuming a
	// thread_freeze() call.
	register thread_t *old __asm__("x0")   = thread;
	register ticks_t   ticks __asm__("x1") = (ticks_t)0U;

	// The new PC must be in x16 or x17 so ARMv8.5-BTI will treat the BR
	// below as a call trampoline, and thus allow it to jump to the BTI C
	// instruction at a new thread's entry point.
	register register_t new_pc __asm__("x16");
	new_pc		  = thread->context.pc;
	register_t new_sp = thread->context.sp;
	register_t new_fp = thread->context.fp;

	__asm__ volatile(
		"mov   sp, %[new_sp]			;"
		"mov   x29, %[new_fp]			;"
		"br	%[new_pc]			;"
		:
		: [old] "r"(old), [ticks] "r"(ticks), [new_pc] "r"(new_pc),
		  [new_sp] "r"(new_sp), [new_fp] "r"(new_fp)
		: "memory");
	__builtin_unreachable();
}

register_t
thread_freeze(fptr_t fn, register_t param, register_t resumed_result)
{
	TRACE(INFO, INFO, "thread_freeze start fn: {:#x} param: {:#x}",
	      (uintptr_t)fn, (uintptr_t)param);

	trigger_thread_save_state_event();

	thread_t *thread = thread_get_self();
	assert(thread != NULL);

	// The parameter must be kept in X0 so the freeze function gets it as an
	// argument.
	register register_t x0 __asm__("x0") = param;

	// The remaining hard-coded registers here are only needed to
	// ensure a correct clobber list below. The union of the clobber
	// list, fixed output registers and explicitly saved registers
	// (x29, sp and pc) must be the entire integer register state.
	register register_t saved_pc __asm__("x1");
	register register_t saved_sp __asm__("x2");
	register uintptr_t  context __asm__("x3") =
		(uintptr_t)&thread->context.pc;
	register fptr_t fn_reg __asm__("x4") = fn;
	register bool	is_resuming __asm__("x5");

	static_assert(offsetof(thread_t, context.sp) ==
			      offsetof(thread_t, context.pc) +
				      sizeof(thread->context.pc),
		      "PC and SP must be adjacent in context");
	static_assert(offsetof(thread_t, context.fp) ==
			      offsetof(thread_t, context.sp) +
				      sizeof(thread->context.sp),
		      "SP and FP must be adjacent in context");

	__asm__ volatile(
		"adr	%[saved_pc], .Lthread_freeze.resumed.%=	;"
		"mov	%[saved_sp], sp				;"
		"stp	%[saved_pc], %[saved_sp], [%[context]]	;"
		"str	x29, [%[context], 16]			;"
		"blr	%[fn_reg]				;"
		"mov	%[is_resuming], 0			;"
		"b	.Lthread_freeze.done.%=			;"
		".Lthread_freeze.resumed.%=:			;"
#if defined(ARCH_ARM_FEAT_BTI)
		"bti	j					;"
#endif
		"mov	%[is_resuming], 1			;"
		".Lthread_freeze.done.%=:			;"
		: [is_resuming] "=%r"(is_resuming), [saved_pc] "=&r"(saved_pc),
		  [saved_sp] "=&r"(saved_sp), [context] "+r"(context),
		  [fn_reg] "+r"(fn_reg), "+r"(x0)
		: /* This must not have any inputs */
		: "x6", "x7", "x8", "x9", "x10", "x11", "x12", "x13", "x14",
		  "x15", "x16", "x17", "x18", "x19", "x20", "x21", "x22", "x23",
		  "x24", "x25", "x26", "x27", "x28", "x30", "cc", "memory");

	if (is_resuming) {
		x0 = resumed_result;
		trigger_thread_load_state_event(false);

		TRACE(INFO, INFO, "thread_freeze resumed: {:#x}", x0);
	} else {
		TRACE(INFO, INFO, "thread_freeze returned: {:#x}", x0);
	}

	return x0;
}

noreturn void
thread_reset_stack(fptr_noreturn_t fn, register_t param)
{
	thread_t	   *thread	     = thread_get_self();
	register register_t x0 __asm__("x0") = param;
	uintptr_t new_sp = (uintptr_t)thread->stack_base + thread->stack_size;

	__asm__ volatile("mov	sp, %[new_sp]	;"
			 "mov	x29, 0		;"
			 "blr	%[new_pc]	;"
			 :
			 : [new_pc] "r"(fn), [new_sp] "r"(new_sp), "r"(x0)
			 : "memory");
	panic("returned to thread_reset_stack()");
}

void
thread_arch_init_context(thread_t *thread)
{
	assert(thread != NULL);

	thread->context.pc = (uintptr_t)thread_arch_main;
	thread->context.sp = (uintptr_t)thread->stack_base + thread->stack_size;
	thread->context.fp = (uintptr_t)0;
}

error_t
thread_arch_map_stack(thread_t *thread)
{
	error_t err;

	assert(thread != NULL);
	assert(thread->stack_base != 0U);

	partition_t *partition = thread->header.partition;
	paddr_t	     stack_phys =
		partition_virt_to_phys(partition, thread->stack_mem);

	pgtable_hyp_start();
	err = pgtable_hyp_map(partition, thread->stack_base, thread->stack_size,
			      stack_phys, PGTABLE_HYP_MEMTYPE_WRITEBACK,
			      PGTABLE_ACCESS_RW,
			      VMSA_SHAREABILITY_INNER_SHAREABLE);
	pgtable_hyp_commit();

	return err;
}

void
thread_arch_unmap_stack(thread_t *thread)
{
	pgtable_hyp_start();
	pgtable_hyp_unmap(thread->header.partition, thread->stack_base,
			  thread->stack_size, thread->stack_size);
	pgtable_hyp_commit();
}

```

`hyp/core/thread_standard/aarch64/src/thread_init.S`:

```S
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <asm/asm_defs.inc>

// This function sets the TLS base register. It is intended to be the last
// operation in the boot_runtime_*_init events.
function thread_switch_boot_thread
	mov	x1, xzr
	add	x1, x1, :tprel_hi12:current_thread
	add	x1, x1, :tprel_lo12_nc:current_thread
	sub	x1, x0, x1
	msr	TPIDR_EL2, x1
	ret
function_end thread_switch_boot_thread

```

`hyp/core/thread_standard/aarch64/thread_aarch64.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

define arch_context structure {
	pc	type register_t;
	sp	type register_t;
	fp	type register_t;
};

```

`hyp/core/thread_standard/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

interface thread
local_include
types thread.tc
events thread.ev
source init.c thread.c
arch_types aarch64 thread_aarch64.tc
arch_source aarch64 thread_arch.c thread_init.S

```

`hyp/core/thread_standard/include/thread_arch.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// The functions in this file deal only with the C execution context; i.e.
// everything that is directly visible to the compiler.
//
// This typically includes the stack pointer, frame pointer, general-purpose
// registers, condition flags, and program counter. It may also include floating
// point and/or vector registers, where required by the architecture.
//
// This does not include control registers for guest VMs, the MMU, etc., which
// are switched by other modules' handlers for the context switch events.

// The default size and minimum alignment for a thread's kernel stack.
extern const size_t thread_stack_min_align;
extern const size_t thread_stack_alloc_align;
extern const size_t thread_stack_size_default;

// Map the kernel stack of a new thread.
error_t
thread_arch_map_stack(thread_t *thread);

// Unmap the kernel stack of a thread.
void
thread_arch_unmap_stack(thread_t *thread);

// Set up the execution context for a new thread.
void
thread_arch_init_context(thread_t *thread);

// Switch from the execution context of the current thread to some other thread.
//
// This function does not return until it is called again by some other thread,
// specifying this thread as an argument. At that point, it returns a pointer to
// the thread it switched from, which need not be the same as the thread that
// the original call switched to.
//
// The tick counter is the time at which the specified thread was scheduled;
// it is passed through to the new thread's context switch handlers to avoid a
// double read of the timer which can cause gaps in the time accounting and
// might be expensive on some platforms. On return, it is updated to be the
// value passed through by the previous thread.
thread_t *
thread_arch_switch_thread(thread_t *next_thread, ticks_t *schedtime);

// Set the current thread, assuming there was no previous current thread.
//
// This function is called at the end of the CPU power-on sequence.
noreturn void
thread_arch_set_thread(thread_t *next_thread) REQUIRE_PREEMPT_DISABLED;

```

`hyp/core/thread_standard/src/init.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>
#include <string.h>

#include <bootmem.h>
#include <idle.h>
#include <object.h>
#include <panic.h>
#include <refcount.h>
#include <thread.h>
#include <thread_init.h>

#include "event_handlers.h"
#include "thread_arch.h"

extern void
thread_switch_boot_thread(thread_t *new_thread);

// Thread size is the size of the whole thread TLS area to be allocated,
// which is larger than 'struct thread'
extern const size_t thread_size;
extern const size_t thread_align;

void
thread_standard_handle_boot_runtime_first_init(void)
{
	void_ptr_result_t ret;
	thread_t	 *idle_thread;

	// Allocate boot CPU idle thread and TLS out of bootmem.
	ret = bootmem_allocate(thread_size, thread_align);
	if (ret.e != OK) {
		panic("unable to allocate boot idle thread");
	}

	// For now, we just zero-initialise the thread and TLS data and init the
	// reference count. The real setup will be done in the idle module after
	// partitions and allocators are working.
	idle_thread = (thread_t *)ret.r;

	assert(thread_size >= sizeof(*idle_thread));
	errno_t err_mem = memset_s(idle_thread, thread_size, 0, thread_size);
	if (err_mem != 0) {
		panic("Error in memset_s operation!");
	}
	refcount_init(&idle_thread->header.refcount);

	// This must be the last operation in boot_runtime_first_init.
	thread_switch_boot_thread(idle_thread);
}

void
thread_standard_handle_boot_runtime_warm_init(thread_t *idle_thread)
{
	// This must be the last operation in boot_runtime_warm_init.
	thread_switch_boot_thread(idle_thread);
}

noreturn void
thread_boot_set_idle(void)
{
	thread_t *thread = thread_get_self();
	assert(thread == idle_thread());

	thread_arch_set_thread(thread);
}

```

`hyp/core/thread_standard/src/thread.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>
#if !defined(NDEBUG)
#include <string.h>
#endif

#include <atomic.h>
#include <compiler.h>
#include <object.h>
#include <panic.h>
#include <partition.h>
#include <preempt.h>
#include <scheduler.h>
#include <thread.h>
#include <trace.h>
#include <util.h>

#include <events/thread.h>

#include <asm/barrier.h>

#include "event_handlers.h"
#include "thread_arch.h"

// Externally visible for assembly
extern thread_t _Thread_local current_thread;

// The current thread
thread_t _Thread_local current_thread
	__attribute__((section(".tbss.current_thread")));

error_t
thread_standard_handle_object_create_thread(thread_create_t thread_create)
{
	error_t	  err	 = OK;
	thread_t *thread = thread_create.thread;

	assert(thread != NULL);

	thread->kind   = thread_create.kind;
	thread->params = thread_create.params;

	size_t stack_size = (thread_create.stack_size != 0U)
				    ? thread_create.stack_size
				    : thread_stack_size_default;
	if (stack_size > THREAD_STACK_MAX_SIZE) {
		err = ERROR_ARGUMENT_SIZE;
		goto out;
	}

	if (!util_is_baligned(stack_size, thread_stack_alloc_align)) {
		err = ERROR_ARGUMENT_ALIGNMENT;
		goto out;
	}

	void_ptr_result_t stack = partition_alloc(
		thread->header.partition, stack_size, thread_stack_alloc_align);
	if (stack.e != OK) {
		err = stack.e;
		goto out;
	}

#if !defined(NDEBUG)
	// Fill the stack with a pattern so we can detect maximum stack
	// depth
	(void)memset_s(stack.r, stack_size, 0x57, stack_size);
#endif

	thread->stack_mem  = (uintptr_t)stack.r;
	thread->stack_size = stack_size;

	scheduler_block_init(thread, SCHEDULER_BLOCK_THREAD_LIFECYCLE);

out:
	return err;
}

void
thread_standard_unwind_object_create_thread(error_t	    result,
					    thread_create_t create)
{
	thread_t *thread = create.thread;
	assert(thread != NULL);
	assert(result != OK);
	assert(atomic_load_relaxed(&thread->state) == THREAD_STATE_INIT);

	if (thread->stack_mem != 0U) {
		(void)partition_free(thread->header.partition,
				     (void *)thread->stack_mem,
				     thread->stack_size);
		thread->stack_mem = 0U;
	}
}

error_t
thread_standard_handle_object_activate_thread(thread_t *thread)
{
	error_t err = OK;

	assert(thread != NULL);

	// Get an appropriate address for the stack and map it there.
	thread->stack_base =
		trigger_thread_get_stack_base_event(thread->kind, thread);
	if (thread->stack_base == 0U) {
		err = ERROR_NOMEM;
		goto out;
	}

	assert(util_is_baligned(thread->stack_base, THREAD_STACK_MAP_ALIGN));

	err = thread_arch_map_stack(thread);
	if (err != OK) {
		thread->stack_base = 0U;
		goto out;
	}

	thread_arch_init_context(thread);

	// Put the thread into ready state and give it a reference to itself.
	// This reference is released in thread_exit(). At this point the thread
	// can only be deleted by another thread by calling thread_kill().
	(void)object_get_thread_additional(thread);
	atomic_store_relaxed(&thread->state, THREAD_STATE_READY);

	// Remove the lifecycle block, which allows the thread to be scheduled
	// (assuming nothing else blocked it).
	scheduler_lock(thread);
	if (scheduler_unblock(thread, SCHEDULER_BLOCK_THREAD_LIFECYCLE)) {
		scheduler_trigger();
	}
	scheduler_unlock(thread);
out:
	return err;
}

void
thread_standard_handle_object_deactivate_thread(thread_t *thread)
{
	assert(thread != NULL);

	thread_state_t state = atomic_load_relaxed(&thread->state);
	assert((state == THREAD_STATE_INIT) || (state == THREAD_STATE_EXITED));

	if (thread->stack_base != 0U) {
		thread_arch_unmap_stack(thread);
		thread->stack_base = 0U;
	}

	if (thread->stack_mem != 0U) {
		(void)partition_free(thread->header.partition,
				     (void *)thread->stack_mem,
				     thread->stack_size);
		thread->stack_mem = 0U;
	}
}

error_t
thread_standard_handle_thread_context_switch_pre(void)
{
	return OK;
}

thread_t *
thread_get_self(void)
{
	return &current_thread;
}

error_t
thread_switch_to(thread_t *thread, ticks_t schedtime)
{
	assert_preempt_disabled();

	thread_t *current = thread_get_self();
	assert(thread != current);

	TRACE_LOCAL(INFO, INFO, "thread: ctx switch from: {:#x} to: {:#x}",
		    (uintptr_t)current, (uintptr_t)thread);

	trigger_thread_save_state_event();
	error_t err =
		trigger_thread_context_switch_pre_event(thread, schedtime);
	if (compiler_unexpected(err != OK)) {
		object_put_thread(thread);
		goto out;
	}

	ticks_t	  prevticks = schedtime;
	thread_t *prev	    = thread_arch_switch_thread(thread, &schedtime);
	assert(prev != NULL);

	trigger_thread_context_switch_post_event(prev, schedtime, prevticks);
	object_put_thread(prev);

	trigger_thread_load_state_event(false);

	asm_context_sync_fence();
out:
	return err;
}

error_t
thread_kill(thread_t *thread)
{
	assert(thread != NULL);

	error_t	       err;
	thread_state_t expected_state = THREAD_STATE_READY;
	if (atomic_compare_exchange_strong_explicit(
		    &thread->state, &expected_state, THREAD_STATE_KILLED,
		    memory_order_relaxed, memory_order_relaxed)) {
		trigger_thread_killed_event(thread);
		err = OK;
	} else if ((expected_state == THREAD_STATE_KILLED) ||
		   (expected_state == THREAD_STATE_EXITED)) {
		// Thread was already killed, or has exited
		err = OK;
	} else {
		// Thread had not started yet
		err = ERROR_OBJECT_STATE;
	}

	return err;
}

bool
thread_is_dying(const thread_t *thread)
{
	assert(thread != NULL);
	return atomic_load_relaxed(&thread->state) == THREAD_STATE_KILLED;
}

bool
thread_has_exited(const thread_t *thread)
{
	assert(thread != NULL);
	return atomic_load_relaxed(&thread->state) == THREAD_STATE_EXITED;
}

noreturn void
thread_exit(void)
{
	thread_t *thread = thread_get_self();
	assert(thread != NULL);
	preempt_disable();

	atomic_store_relaxed(&thread->state, THREAD_STATE_EXITED);

	scheduler_lock_nopreempt(thread);
	scheduler_block(thread, SCHEDULER_BLOCK_THREAD_LIFECYCLE);
	scheduler_unlock_nopreempt(thread);

	// TODO: wake up anyone waiting in thread_join()

	trigger_thread_exited_event();

	// Release the thread's reference to itself (note that the CPU still
	// holds a reference, so this won't delete it immediately). This matches
	// the get in thread_create_finished().
	object_put_thread(thread);

	scheduler_yield();

	// This thread should never run again, unless it is explicitly reset
	// (which will prevent a switch returning here).
	panic("Switched to an exited thread!");
}

void
thread_standard_handle_thread_exit_to_user(void)
{
	thread_t *thread = thread_get_self();
	assert(thread != NULL);

	thread_state_t state = atomic_load_relaxed(&thread->state);
	if (compiler_unexpected(state == THREAD_STATE_KILLED)) {
		thread_exit();
	} else {
		assert(state == THREAD_STATE_READY);
	}
}

```

`hyp/core/thread_standard/thread.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

module thread_standard

subscribe boot_runtime_first_init ()
	priority last

subscribe boot_runtime_warm_init
	priority last

subscribe object_create_thread
	unwinder
	// Run first to ensure that thread kind is set
	priority first

subscribe object_activate_thread
	// Run last, because this handler can cause the thread to be scheduled
	// immediately on remote CPUs (before the activate event ends).
	priority last

subscribe object_deactivate_thread

// Add a dummy handler so the scheduler can always register an unwinder.
subscribe thread_context_switch_pre()

subscribe thread_exit_to_user()
	priority last

```

`hyp/core/thread_standard/thread.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

extend thread object {
	context		structure arch_context;
	stack_mem	uintptr;
};

extend scheduler_block enumeration {
	thread_lifecycle;
};

```

`hyp/core/timer/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

interface timer
types timer.tc
events timer.ev
source timer_queue.c
types timer_tests.tc
events timer_tests.ev
source timer_tests.c

```

`hyp/core/timer/src/timer_queue.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>

#include <hypcontainers.h>

#include <atomic.h>
#include <compiler.h>
#include <cpulocal.h>
#include <ipi.h>
#include <list.h>
#include <object.h>
#include <panic.h>
#include <partition.h>
#include <platform_cpu.h>
#include <platform_timer.h>
#include <preempt.h>
#include <spinlock.h>
#include <timer_queue.h>
#include <util.h>

#include <events/timer.h>

#include "event_handlers.h"

CPULOCAL_DECLARE_STATIC(timer_queue_t, timer_queue);

void
timer_handle_boot_cold_init(cpu_index_t boot_cpu_index)
{
	// Initialise all timer queues here as online CPUs may try to move
	// timers to CPUs that have not booted yet
	// Secondary CPUs will be set online by `power_cpu_online()` handler
	for (cpu_index_t cpu_index = 0U; cpu_index < PLATFORM_MAX_CORES;
	     cpu_index++) {
		timer_queue_t *tq = &CPULOCAL_BY_INDEX(timer_queue, cpu_index);
		spinlock_init(&tq->lock);
		list_init(&tq->list);
		tq->timeout = TIMER_INVALID_TIMEOUT;
		tq->online  = (cpu_index == boot_cpu_index);
	}
}

#if !defined(UNITTESTS) || !UNITTESTS
void
timer_handle_rootvm_init(hyp_env_data_t *hyp_env)
{
	hyp_env->timer_freq = timer_get_timer_frequency();
}
#endif

uint32_t
timer_get_timer_frequency(void)
{
	return platform_timer_get_frequency();
}

ticks_t
timer_get_current_timer_ticks(void)
{
	return platform_timer_get_current_ticks();
}

ticks_t
timer_convert_ns_to_ticks(nanoseconds_t ns)
{
	return platform_timer_convert_ns_to_ticks(ns);
}

nanoseconds_t
timer_convert_ticks_to_ns(ticks_t ticks)
{
	return platform_timer_convert_ticks_to_ns(ticks);
}

static bool
is_timeout_a_smaller_than_b(list_node_t *node_a, list_node_t *node_b)
{
	ticks_t timeout_a = timer_container_of_list_node(node_a)->timeout;
	ticks_t timeout_b = timer_container_of_list_node(node_b)->timeout;

	return timeout_a < timeout_b;
}

void
timer_init_object(timer_t *timer, timer_action_t action)
{
	assert(timer != NULL);

	timer->timeout = TIMER_INVALID_TIMEOUT;
	timer->action  = action;
	atomic_init(&timer->queue, NULL);
}

bool
timer_is_queued(timer_t *timer)
{
	assert(timer != NULL);

	return atomic_load_relaxed(&timer->queue) != NULL;
}

ticks_t
timer_queue_get_next_timeout(void)
{
	timer_queue_t *tq = &CPULOCAL(timer_queue);
	ticks_t	       timeout;

	spinlock_acquire_nopreempt(&tq->lock);
	timeout = tq->timeout;
	spinlock_release_nopreempt(&tq->lock);

	return timeout;
}

static void
timer_update_timeout(timer_queue_t *tq) REQUIRE_SPINLOCK(tq->lock)
{
	assert_preempt_disabled();
	assert(tq == &CPULOCAL(timer_queue));

	if (tq->timeout != TIMER_INVALID_TIMEOUT) {
		platform_timer_set_timeout(tq->timeout);
	} else {
		platform_timer_cancel_timeout();
	}
}

static void
timer_enqueue_internal(timer_queue_t *tq, timer_t *timer, ticks_t timeout)
	REQUIRE_SPINLOCK(tq->lock)
{
	assert_preempt_disabled();
	assert(tq == &CPULOCAL(timer_queue));
	assert(tq->online);

	// Set the timer's queue pointer. We need acquire ordering to ensure we
	// observe any previous dequeues on other CPUs.
	timer_queue_t *old_tq = NULL;
	if (!atomic_compare_exchange_strong_explicit(&timer->queue, &old_tq, tq,
						     memory_order_acquire,
						     memory_order_relaxed)) {
		// This timer is already queued; it is the caller's
		// responsibility to avoid this.
		panic("Request to enqueue a timer that is already queued");
	}

	// There is no need to check if the timeout is already in the past, as
	// the timer module generates a level-triggered interrupt if the timer
	// condition is already met.
	timer->timeout = timeout;

	bool new_head = list_insert_in_order(&tq->list, &timer->list_node,
					     is_timeout_a_smaller_than_b);
	if (new_head) {
		tq->timeout = timeout;
		timer_update_timeout(tq);
	}
}

static bool
timer_dequeue_internal(timer_queue_t *tq, timer_t *timer)
	REQUIRE_SPINLOCK(tq->lock)
{
	assert_preempt_disabled();

	bool new_timeout = false;

	// The timer may have expired between loading the timer's queue and
	// acquiring its lock. Ensure the timer's queue has not changed before
	// dequeuing.
	if (compiler_expected(atomic_load_relaxed(&timer->queue) == tq)) {
		bool new_head = list_delete_node(&tq->list, &timer->list_node);
		if (new_head) {
			list_node_t *head = list_get_head(&tq->list);
			tq->timeout =
				timer_container_of_list_node(head)->timeout;
			new_timeout = true;
		} else if (list_is_empty(&tq->list)) {
			tq->timeout = TIMER_INVALID_TIMEOUT;
			new_timeout = true;
		} else {
			// The queue's timeout has not changed.
		}

		// Clear the timer's queue pointer. We need release ordering to
		// ensure this dequeue is observed by the next enqueue.
		atomic_store_release(&timer->queue, NULL);
	}

	return new_timeout;
}

static void
timer_update_internal(timer_queue_t *tq, timer_t *timer, ticks_t timeout)
	REQUIRE_SPINLOCK(tq->lock)
{
	assert_preempt_disabled();
	assert(tq == &CPULOCAL(timer_queue));
	assert(tq->online);

	if (compiler_unexpected(tq != atomic_load_relaxed(&timer->queue))) {
		// There is a race with timer updates; it is the caller's
		// responsibility to prevent this.
		panic("Request to update a timer that is not queued on this CPU");
	}

	if (compiler_expected(timer->timeout != timeout)) {
		// There is no need to check if the timeout is already in the
		// past, as the timer module generates a level-triggered
		// interrupt if the timer condition is already met.

		// Delete timer from queue, update it, and add it again to queue

		bool new_head_delete =
			list_delete_node(&tq->list, &timer->list_node);

		timer->timeout = timeout;

		bool new_head_insert =
			list_insert_in_order(&tq->list, &timer->list_node,
					     is_timeout_a_smaller_than_b);

		if (new_head_delete || new_head_insert) {
			list_node_t *head = list_get_head(&tq->list);
			tq->timeout =
				timer_container_of_list_node(head)->timeout;
			timer_update_timeout(tq);
		}
	}
}

void
timer_enqueue(timer_t *timer, ticks_t timeout)
{
	assert(timer != NULL);

	preempt_disable();

	timer_queue_t *tq = &CPULOCAL(timer_queue);

	spinlock_acquire_nopreempt(&tq->lock);
	timer_enqueue_internal(tq, timer, timeout);
	spinlock_release_nopreempt(&tq->lock);

	preempt_enable();
}

void
timer_dequeue(timer_t *timer)
{
	assert(timer != NULL);

	timer_queue_t *tq = atomic_load_relaxed(&timer->queue);

	if (tq != NULL) {
		spinlock_acquire(&tq->lock);
		if (timer_dequeue_internal(tq, timer) &&
		    (tq == &CPULOCAL(timer_queue))) {
			timer_update_timeout(tq);
		}
		spinlock_release(&tq->lock);
	}
}

void
timer_update(timer_t *timer, ticks_t timeout)
{
	assert(timer != NULL);

	preempt_disable();

	timer_queue_t *old_tq = atomic_load_relaxed(&timer->queue);
	timer_queue_t *new_tq = &CPULOCAL(timer_queue);

	// If timer is queued on another CPU, it needs to be dequeued.
	if ((old_tq != NULL) && (old_tq != new_tq)) {
		spinlock_acquire_nopreempt(&old_tq->lock);
		(void)timer_dequeue_internal(old_tq, timer);
		spinlock_release_nopreempt(&old_tq->lock);
	}

	spinlock_acquire_nopreempt(&new_tq->lock);
	if (old_tq == new_tq) {
		timer_update_internal(new_tq, timer, timeout);
	} else {
		timer_enqueue_internal(new_tq, timer, timeout);
	}
	spinlock_release_nopreempt(&new_tq->lock);

	preempt_enable();
}

static void
timer_dequeue_expired(void) REQUIRE_PREEMPT_DISABLED
{
	ticks_t	       current_ticks = timer_get_current_timer_ticks();
	timer_queue_t *tq	     = &CPULOCAL(timer_queue);

	assert_preempt_disabled();

	spinlock_acquire_nopreempt(&tq->lock);

	while (tq->timeout <= current_ticks) {
		list_node_t *head  = list_get_head(&tq->list);
		timer_t	    *timer = timer_container_of_list_node(head);
		(void)timer_dequeue_internal(tq, timer);
		spinlock_release_nopreempt(&tq->lock);
		(void)trigger_timer_action_event(timer->action, timer);
		spinlock_acquire_nopreempt(&tq->lock);
	}

	timer_update_timeout(tq);
	spinlock_release_nopreempt(&tq->lock);
}

void
timer_handle_platform_timer_expiry(void)
{
	timer_dequeue_expired();
}

error_t
timer_handle_power_cpu_suspend(void)
{
	// TODO: Delay or reject attempted suspend if timeout is due to expire
	// sooner than the CPU can reach the requested power state.

#if defined(MODULE_CORE_TIMER_LP) && MODULE_CORE_TIMER_LP
	// The timer_lp module will enqueue the timeout on the global low power
	// timer, so we can cancel the core-local timer to avoid redundant
	// interrupts if the suspend finishes without entering a state that
	// stops the timer.
	platform_timer_cancel_timeout();
#endif

	return OK;
}

// Also handles power_cpu_resume
void
timer_handle_power_cpu_online(void)
{
	timer_dequeue_expired();

	// Mark this CPU timer queue as online
	timer_queue_t *tq = &CPULOCAL(timer_queue);
	assert_preempt_disabled();
	spinlock_acquire_nopreempt(&tq->lock);
	tq->online = true;
	spinlock_release_nopreempt(&tq->lock);
}

// A timer_queue operation has occurred that requires synchronisation, process
// our timer queue. Handle any expired timers as the timer might have expired
// since it was queued on this CPU and reprogram the platform timer if required.
bool NOINLINE
timer_handle_ipi_received(void)
{
	timer_dequeue_expired();

	return true;
}

static bool
timer_try_move_to_cpu(timer_t *timer, cpu_index_t target)
	REQUIRE_PREEMPT_DISABLED
{
	bool	       moved = false;
	timer_queue_t *ttq   = &CPULOCAL_BY_INDEX(timer_queue, target);

	assert_preempt_disabled();

	spinlock_acquire_nopreempt(&ttq->lock);

	// We can only use active CPU timer queues
	if (ttq->online) {
		// Update the timer queue to be on the new CPU
		timer_queue_t *old_ttq = NULL;
		if (!atomic_compare_exchange_strong_explicit(
			    &timer->queue, &old_ttq, ttq, memory_order_acquire,
			    memory_order_relaxed)) {
			panic("Request to move timer that is already queued");
		}

		// Call IPI if the queue HEAD changed so the target CPU can
		// update its local timer
		bool new_head =
			list_insert_in_order(&ttq->list, &timer->list_node,
					     is_timeout_a_smaller_than_b);
		if (new_head) {
			ttq->timeout = timer->timeout;
			spinlock_release_nopreempt(&ttq->lock);
			ipi_one(IPI_REASON_TIMER_QUEUE_SYNC, target);
		} else {
			spinlock_release_nopreempt(&ttq->lock);
		}
		moved = true;
	} else {
		spinlock_release_nopreempt(&ttq->lock);
	}

	return moved;
}

void
timer_handle_power_cpu_offline(void)
{
	// Try to move any timers to the next one up from this one.
	// If this is the last core, wrap around
	cpu_index_t our_index = cpulocal_get_index();
	cpu_index_t start =
		(cpu_index_t)((our_index + 1U) % PLATFORM_MAX_CORES);
	timer_queue_t *tq = &CPULOCAL(timer_queue);

	assert_preempt_disabled();
	spinlock_acquire_nopreempt(&tq->lock);

	// Mark this CPU timer queue as going down and cancel any pending timers
	tq->online = false;
	platform_timer_cancel_timeout();

	// Move all active timers in this CPU timer queue to an active CPU
	while (tq->timeout != TIMER_INVALID_TIMEOUT) {
		list_node_t *head  = list_get_head(&tq->list);
		timer_t	    *timer = timer_container_of_list_node(head);

		// Remove timer from this core.
		(void)timer_dequeue_internal(tq, timer);
		spinlock_release_nopreempt(&tq->lock);

		// The target core might go down while we are searching, so
		// always check if the target is active. Hopefully the target
		// Queue stays online so we check the last-used CPU first. If
		// we cannot find any active timer queues we panic. In reality
		// at least one CPU timer queue should always be online.
		bool	    found_target = false;
		cpu_index_t target	 = start;
		while (!found_target) {
			if (platform_cpu_exists(target)) {
				if (timer_try_move_to_cpu(timer, target)) {
					found_target = true;
					start	     = target;
					break;
				}
			}

			// Skip our CPU as we know we are going down
			// This could happen if the previously saved core is
			// down now and the initial search wrapped.
			target = (cpu_index_t)((target + 1U) %
					       PLATFORM_MAX_CORES);
			if (target == our_index) {
				target = (cpu_index_t)((target + 1U) %
						       PLATFORM_MAX_CORES);
			}
			if (target == start) {
				// we looped around without finding a target,
				// this should never happen.
				break;
			}
		}

		if (!found_target) {
			panic("Could not find target CPU for timer migration");
		}

		// Get the lock back to check the next timer
		spinlock_acquire_nopreempt(&tq->lock);
	}

	spinlock_release_nopreempt(&tq->lock);
}

```

`hyp/core/timer/src/timer_tests.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#if defined(UNIT_TESTS)

#include <assert.h>
#include <hyptypes.h>

#include <atomic.h>
#include <compiler.h>
#include <cpulocal.h>
#include <log.h>
#include <panic.h>
#include <preempt.h>
#include <timer_queue.h>
#include <trace.h>

#include "event_handlers.h"

#if defined(PLATFORM_QEMU)
#define MAX_TICKS_DIFFERENCE 0x500000
#else
// Leave enough slack for console messages to be printed
#define MAX_TICKS_DIFFERENCE 0x140
#endif

CPULOCAL_DECLARE_STATIC(timer_t, timer1);
CPULOCAL_DECLARE_STATIC(timer_t, timer2);
CPULOCAL_DECLARE_STATIC(uint8_t, test_num);
CPULOCAL_DECLARE_STATIC(_Atomic bool, in_progress);
CPULOCAL_DECLARE_STATIC(ticks_t, expected_timeout);

bool
tests_timer(void)
{
	ticks_t	 current_ticks;
	timer_t *timer1		  = &CPULOCAL(timer1);
	timer_t *timer2		  = &CPULOCAL(timer2);
	ticks_t *expected_timeout = &CPULOCAL(expected_timeout);

	_Atomic bool *in_progress = &CPULOCAL(in_progress);

	// Test 1
	// Enqueue a timer and make sure its expiry is received
	CPULOCAL(test_num) = 1;
	timer_init_object(timer1, TIMER_ACTION_TEST);
	timer_init_object(timer2, TIMER_ACTION_TEST);
	current_ticks = timer_get_current_timer_ticks();
	*expected_timeout =
		current_ticks + (0x100000 * (cpulocal_get_index() + 1));
	atomic_store_relaxed(&CPULOCAL(in_progress), true);
	timer_enqueue(timer1, *expected_timeout);

	preempt_enable();
	while (atomic_load_relaxed(in_progress)) {
	}
	preempt_disable();

	// Test 2
	// Enqueue two timers, dequeue the first one and make sure only the
	// expiry for the second one is received
	CPULOCAL(test_num)++;
	timer_init_object(timer1, TIMER_ACTION_TEST);
	timer_init_object(timer2, TIMER_ACTION_TEST);
	current_ticks = timer_get_current_timer_ticks();
	atomic_store_relaxed(&CPULOCAL(in_progress), true);
	timer_enqueue(timer1,
		      current_ticks + (0x100000 * (cpulocal_get_index() + 1)));

	*expected_timeout =
		current_ticks + (0x200000 * (cpulocal_get_index() + 1));
	timer_enqueue(timer2, *expected_timeout);

	timer_dequeue(timer1);

	preempt_enable();
	while (atomic_load_relaxed(in_progress)) {
	}
	preempt_disable();

	// TODO: Add more tests

	LOG(DEBUG, INFO, "Timer tests successfully finished on core {:d}",
	    cpulocal_get_index());
	return false;
}

bool
tests_timer_action(timer_t *timer)
{
	ticks_t *expected_timeout = &CPULOCAL(expected_timeout);
	ticks_t	 current_ticks	  = timer_get_current_timer_ticks();

	assert(timer != NULL);

	if (!atomic_load_relaxed(&CPULOCAL(in_progress))) {
		LOG(ERROR, PANIC,
		    "Unexpected timer expiry trigger on core {:d}",
		    cpulocal_get_index());
		panic("Unexpected timer expiry trigger");
	} else if (timer->timeout != *expected_timeout) {
		LOG(ERROR, PANIC,
		    "Timer expiry trigger (test {:d}) on core {:d}"
		    " arrived for the wrong timer; expected {:#x}, got {:#x}",
		    CPULOCAL(test_num), cpulocal_get_index(), *expected_timeout,
		    timer->timeout);
		panic("Timer expiry trigger arrived with wrong timeout");
	} else if (*expected_timeout > current_ticks) {
		LOG(ERROR, PANIC,
		    "Timer expiry trigger (test {:d}) on core {:d}"
		    " arrived too early; expected at {:#x}, arrived at {:#x}",
		    CPULOCAL(test_num), cpulocal_get_index(), *expected_timeout,
		    current_ticks);
		panic("Timer expiry trigger arrived too early");
	} else if (current_ticks - *expected_timeout > MAX_TICKS_DIFFERENCE) {
		LOG(ERROR, PANIC,
		    "Timer expiry trigger (test {:d}) on core {:d}"
		    " took too long to arrive; expected at"
		    "{:#x}, arrived at {:#x}, diff {:#x}",
		    CPULOCAL(test_num), cpulocal_get_index(), *expected_timeout,
		    current_ticks, current_ticks - *expected_timeout);
		panic("Timer expiry trigger arrived too late");
	} else {
		LOG(DEBUG, INFO,
		    "Timer interrupt (test {:d}): core {:d}, expected at {:#x}"
		    ", arrived at {:#x}, diff {:#x}",
		    CPULOCAL(test_num), cpulocal_get_index(), *expected_timeout,
		    current_ticks, current_ticks - *expected_timeout);
		atomic_store_relaxed(&CPULOCAL(in_progress), false);
	}

	return true;
}
#else

extern char unused;

#endif

```

`hyp/core/timer/timer.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

module timer

subscribe boot_cold_init

subscribe ipi_received[IPI_REASON_TIMER_QUEUE_SYNC]()
	require_preempt_disabled

#if !defined(UNITTESTS) || !UNITTESTS
subscribe rootvm_init(hyp_env)
#endif

subscribe platform_timer_expiry
	require_preempt_disabled

subscribe power_cpu_suspend()
	unwinder timer_handle_power_cpu_online()
	// Run early since it may reject suspends
	priority 100
	require_preempt_disabled

subscribe power_cpu_online()
	require_preempt_disabled

subscribe power_cpu_resume
	handler timer_handle_power_cpu_online()

subscribe power_cpu_offline()
	require_preempt_disabled

```

`hyp/core/timer/timer.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

define TIMER_INVALID_TIMEOUT constant type ticks_t = -1;

// An IPI used to manage timer queue synchronization. If a timer is moved from
// from one CPU timer queue to another the target CPU may need to update it's
// local timer with the new timeout.
extend ipi_reason enumeration {
	timer_queue_sync;
};

extend timer structure {
	timeout		type ticks_t;
	action		enumeration timer_action;
	queue		pointer(atomic) structure timer_queue;
	list_node	structure list_node(contained);
};

define timer_queue structure {
	timeout		type ticks_t;
	list		structure list;
	lock		structure spinlock;

	// False if the pCPU for this queue is powering off
	online	bool;
};

extend hyp_env_data structure {
	timer_freq	uint64;
};

```

`hyp/core/timer/timer_tests.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

module timer

#if defined(UNIT_TESTS)

subscribe tests_start
	handler tests_timer()
	require_preempt_disabled

subscribe timer_action[TIMER_ACTION_TEST]
	handler tests_timer_action(timer)
	require_preempt_disabled

#endif

```

`hyp/core/timer/timer_tests.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#if defined(UNIT_TESTS)

extend timer_action enumeration {
	test;
};

#endif

```

`hyp/core/timer_lp/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

interface timer_lp
types timer_lp.tc
events timer_lp.ev
source timer_lp_queue.c

```

`hyp/core/timer_lp/src/timer_lp_queue.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>

#include <hypcontainers.h>

#include <cpulocal.h>
#include <ipi.h>
#include <list.h>
#include <platform_timer_lp.h>
#include <preempt.h>
#include <spinlock.h>
#include <timer_queue.h>

#include "event_handlers.h"

static spinlock_t	timer_lp_queue_lock;
static timer_lp_queue_t timer_lp_queue PROTECTED_BY(timer_lp_queue_lock);

CPULOCAL_DECLARE_STATIC(timer_lp_t, timer_lp);

void
timer_lp_queue_handle_boot_cold_init(void)
{
	spinlock_init(&timer_lp_queue_lock);
	spinlock_acquire(&timer_lp_queue_lock);
	timer_lp_queue.timeout = TIMER_INVALID_TIMEOUT;
	list_init(&timer_lp_queue.list);
	spinlock_release(&timer_lp_queue_lock);
}

void
timer_lp_queue_handle_boot_cpu_cold_init(cpu_index_t cpu_index)
{
	timer_lp_t *timer = &CPULOCAL_BY_INDEX(timer_lp, cpu_index);
	timer->timeout	  = TIMER_INVALID_TIMEOUT;
	timer->cpu_index  = cpu_index;
}

static bool
is_timeout_a_smaller_than_b(list_node_t *node_a, list_node_t *node_b)
{
	bool smaller = false;

	ticks_t timeout_a = timer_lp_container_of_node(node_a)->timeout;
	ticks_t timeout_b = timer_lp_container_of_node(node_b)->timeout;

	if (timeout_a < timeout_b) {
		smaller = true;
	}

	return smaller;
}

static void
timer_lp_enqueue(timer_lp_t *timer, ticks_t timeout)
	REQUIRE_SPINLOCK(timer_lp_queue_lock)
{
	timer->timeout = timeout;

	bool new_head = list_insert_in_order(&timer_lp_queue.list, &timer->node,
					     is_timeout_a_smaller_than_b);

	if (new_head) {
		timer_lp_queue.timeout = timer->timeout;
		platform_timer_lp_set_timeout_and_route(timer->timeout,
							timer->cpu_index);
	}
}

static bool
timer_lp_dequeue(timer_lp_t *timer) REQUIRE_SPINLOCK(timer_lp_queue_lock)
{
	bool new_head = list_delete_node(&timer_lp_queue.list, &timer->node);
	bool need_update;

	if (new_head) {
		list_node_t *head	= list_get_head(&timer_lp_queue.list);
		timer_lp_t  *head_timer = timer_lp_container_of_node(head);

		timer_lp_queue.timeout = head_timer->timeout;
		need_update	       = true;
	} else if (list_is_empty(&timer_lp_queue.list)) {
		timer_lp_queue.timeout = TIMER_INVALID_TIMEOUT;
		need_update	       = true;
	} else {
		need_update = false;
	}

	timer->timeout = TIMER_INVALID_TIMEOUT;

	return need_update;
}

static void
timer_lp_queue_save_arch_timer(void) REQUIRE_SPINLOCK(timer_lp_queue_lock)
{
	// Get the next timeout of the local arch timer queue

	ticks_t timeout = timer_queue_get_next_timeout();
	if (timeout == TIMER_INVALID_TIMEOUT) {
		goto out;
	}

	timer_lp_t *timer = &CPULOCAL(timer_lp);
	assert(timer->timeout == TIMER_INVALID_TIMEOUT);

	timer_lp_enqueue(timer, timeout);

out:
	return;
}

error_t
timer_lp_handle_power_cpu_suspend(void)
{
	assert_preempt_disabled();

	// TODO: Delay or reject attempted suspend if timeout is due to expire
	// sooner than the CPU can reach the requested power state.

	spinlock_acquire_nopreempt(&timer_lp_queue_lock);

	timer_lp_queue_save_arch_timer();

	spinlock_release_nopreempt(&timer_lp_queue_lock);

	return OK;
}

static void
timer_lp_sync(bool force_update) REQUIRE_SPINLOCK(timer_lp_queue_lock)
{
	cpu_index_t cpu_index	  = cpulocal_get_index();
	ticks_t	    current_ticks = platform_timer_lp_get_current_ticks();
	bool	    do_update	  = force_update;

	assert_preempt_disabled();

	while (timer_lp_queue.timeout <= current_ticks) {
		list_node_t *head  = list_get_head(&timer_lp_queue.list);
		timer_lp_t  *timer = timer_lp_container_of_node(head);

		(void)timer_lp_dequeue(timer);
		// We just dequeued the head, so always update the timer
		do_update = true;

		if (timer->cpu_index != cpu_index) {
			ipi_one(IPI_REASON_RESCHEDULE, timer->cpu_index);
		}
	}

	if (!do_update) {
		// Queue head didn't change, nothing to do
	} else if (timer_lp_queue.timeout == TIMER_INVALID_TIMEOUT) {
		// Queue is now empty
		platform_timer_lp_cancel_timeout();
	} else {
		// Schedule the next timeout
		list_node_t *head	= list_get_head(&timer_lp_queue.list);
		timer_lp_t  *head_timer = timer_lp_container_of_node(head);
		platform_timer_lp_set_timeout_and_route(head_timer->timeout,
							head_timer->cpu_index);
	}
}

static void
timer_lp_queue_restore_arch_timer(void) REQUIRE_SPINLOCK(timer_lp_queue_lock)
{
	timer_lp_t *timer = &CPULOCAL(timer_lp);
	if (timer->timeout == TIMER_INVALID_TIMEOUT) {
		goto out;
	}

	if (timer_lp_dequeue(timer)) {
		timer_lp_sync(true);
	}

out:
	return;
}

void
timer_lp_handle_power_cpu_resume(void)
{
	assert_preempt_disabled();

	spinlock_acquire_nopreempt(&timer_lp_queue_lock);

	timer_lp_queue_restore_arch_timer();

	spinlock_release_nopreempt(&timer_lp_queue_lock);
}

void
timer_lp_handle_platform_timer_lp_expiry(void)
{
	spinlock_acquire_nopreempt(&timer_lp_queue_lock);

	timer_lp_sync(false);

	spinlock_release_nopreempt(&timer_lp_queue_lock);
}

```

`hyp/core/timer_lp/timer_lp.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

module timer_lp

subscribe boot_cold_init
	handler timer_lp_queue_handle_boot_cold_init()

subscribe boot_cpu_cold_init
	handler timer_lp_queue_handle_boot_cpu_cold_init

subscribe power_cpu_suspend()
	unwinder timer_lp_handle_power_cpu_resume()
	require_preempt_disabled

subscribe power_cpu_resume()
	require_preempt_disabled

subscribe platform_timer_lp_expiry
	require_preempt_disabled

```

`hyp/core/timer_lp/timer_lp.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

define timer_lp structure {
	timeout		type ticks_t;
	cpu_index	type cpu_index_t;
	node		structure list_node(contained);
};

define timer_lp_queue structure {
	timeout		type ticks_t;
	list		structure list;
};

```

`hyp/core/timer_null/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

interface timer
types timer.tc
source timer_queue_null.c

```

`hyp/core/timer_null/src/timer_queue_null.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>

#include <compiler.h>
#include <cpulocal.h>
#include <object.h>
#include <panic.h>
#include <partition.h>
#include <platform_timer.h>
#include <preempt.h>
#include <timer_queue.h>
#include <util.h>

uint32_t
timer_get_timer_frequency()
{
	return platform_timer_get_frequency();
}

ticks_t
timer_get_current_timer_ticks()
{
	return platform_timer_get_current_ticks();
}

ticks_t
timer_convert_ns_to_ticks(nanoseconds_t ns)
{
	return platform_timer_convert_ns_to_ticks(ns);
}

nanoseconds_t
timer_convert_ticks_to_ns(ticks_t ticks)
{
	return platform_timer_convert_ticks_to_ns(ticks);
}

void
timer_init_object(timer_t *timer)
{
	assert(timer != NULL);

	timer->prev    = NULL;
	timer->next    = NULL;
	timer->timeout = TIMER_INVALID_TIMEOUT;
}

void
timer_enqueue(timer_t *timer, ticks_t timeout, timer_action_t action)
{
	(void)timer;
	(void)timeout;
	(void)action;
}

void
timer_dequeue(timer_t *timer)
{
	(void)timer;
}

void
timer_update(timer_t *timer, ticks_t timeout)
{
	(void)timer;
	(void)timeout;
}

```

`hyp/core/timer_null/timer.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

define TIMER_INVALID_TIMEOUT constant type ticks_t = -1;

extend timer structure {
	timeout		type ticks_t;
	action		enumeration timer_action;
	prev		pointer structure timer;
	next		pointer structure timer;
};

```

`hyp/core/util/aarch64/src/memcpy.S`:

```S
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <asm/asm_defs.inc>
#include <asm/cpu.h>
#include <asm/panic.inc>

#if defined(ARCH_ARM_FEAT_BTI)
// Two instructions per jump
#define JUMP_SHIFT	3
#else
#define JUMP_SHIFT	2
#endif

// memcpy of no more than 31 bytes
function memcpy_below32
	// Assume the target is size-aligned and do the largest copies first.
	tbz	x2, 4, LOCAL(memcpy_below16)
	ldp	x3, x4, [x1], 16
	stp	x3, x4, [x0], 16
local memcpy_below16:
	tbz	x2, 3, 1f
	ldr	x3, [x1], 8
	str	x3, [x0], 8
1:
	tbz	x2, 2, 1f
	ldr	w3, [x1], 4
	str	w3, [x0], 4
1:
	tbz	x2, 1, 1f
	ldrh	w3, [x1], 2
	strh	w3, [x0], 2
1:
	tbz	x2, 0, 1f
	ldrb	w3, [x1]
	strb	w3, [x0]
1:
	ret
function_end memcpy_below32


// memcpy of at least 31 bytes (i.e. large enough to align up to 16 and do at
// least one 16-byte copy).
function memcpy_alignable
	// Align up the target address to 16. We know that the size (x2) is at
	// least 16 here, so we don't have to check it during this alignment.
	tbz	x0, 0, 1f
	ldrb	w3, [x1], 1
	sub	x2, x2, 1
	strb	w3, [x0], 1
1:
	tbz	x0, 1, 1f
	ldrh	w3, [x1], 2
	sub	x2, x2, 2
	strh	w3, [x0], 2
1:
	tbz	x0, 2, 1f
	ldr	w3, [x1], 4
	sub	x2, x2, 4
	str	w3, [x0], 4
1:
	tbz	x0, 3, 1f
	ldr	x3, [x1], 8
	sub	x2, x2, 8
	str	x3, [x0], 8
1:
	// At this point we've copied up to 15 bytes, so we know there are at
	// least 16 left. We can safely fall through to _align16.
	prfm	pldl1strm, [x1]
	prfm	pstl1keep, [x0]
function_chain memcpy_alignable, memcpy_align16
	// Use 128-byte chunks to do the copy, because we have 16 usable
	// temporary registers. Copy at least two chunks, so we can unroll the
	// loop and store one chunk while loading the next. Note that this is
	// slightly suboptimal for the A55, which can only dispatch one load
	// and one store each cycle.
#if CPU_MEMCPY_STRIDE != 0x100
#error CPU_MEMCPY_STRIDE is defined incorrectly
#endif

	// The first line was prefetched by the caller; prefetch the rest of
	// the first chunk.
.equ LOCAL(offset), 1 << CPU_L1D_LINE_BITS
.rept (0x80 >> CPU_L1D_LINE_BITS) - 1
	prfm	pldl1strm, [x1, LOCAL(offset)]
	prfm	pstl1keep, [x0, LOCAL(offset)]
.equ LOCAL(offset), LOCAL(offset) +  (1 << CPU_L1D_LINE_BITS)
.endr

	// If we don't have at least two chunks to copy, skip the loop.
	cmp	x2, 0x100
	b.lt	LOCAL(memcpy_below256)

	// Prefetch loads for the second chunk.
.rept (0x80 >> CPU_L1D_LINE_BITS)
	prfm	pldl1strm, [x1, LOCAL(offset)]
.equ LOCAL(offset), LOCAL(offset) +  (1 << CPU_L1D_LINE_BITS)
.endr

	// Offset the destination pointer so that we can put an 0x80 byte
	// post-index writeback on the last store in the loop.
	add	x0, x0, 0x70

	// Pre-calculate the size after the minimal copy, so that we can
	// decrement it by 0x80 and check for termination in a single
	// instruction during the copy loop.
	sub	x2, x2, 0x100

	// Load the first chunk. The last load will pre-index writeback the
	// source pointer, which allows us to do the same on the last load in
	// the loop.
	ldp	x3, x4, [x1, 0x0]
	ldp	x5, x6, [x1, 0x10]
	ldp	x7, x8, [x1, 0x20]
	ldp	x9, x10, [x1, 0x30]
	ldp	x11, x12, [x1, 0x40]
	ldp	x13, x14, [x1, 0x50]
	ldp	x15, x16, [x1, 0x60]
	ldp	x17, x18, [x1, 0x70]!

	// Loop storing chunk n while loading chunk n+1
local memcpy_chunk128_loop:
	// Reduce size by 128 (up to the end of chunk n+2); if it is still
	// positive we will continue the loop after chunk n+1 is loaded.
	subs	x2, x2, 0x80

	// Prefetch loads for chunk n+2 and stores for chunk n+1
.equ LOCAL(offset), 0
.rept (0x80 >> CPU_L1D_LINE_BITS)
	prfm	pstl1keep, [x0, LOCAL(offset) + 0x10]
	prfm	pldl1strm, [x1, LOCAL(offset) + 0x90]
.equ LOCAL(offset), LOCAL(offset) +  (1 << CPU_L1D_LINE_BITS)
.endr

	// Interleave the stores and loads; increment the pointers by 0x80
	// using the last instructions in the loop, to minimise stalls waiting
	// for the writebacks.
	stp	x3, x4, [x0, -0x70]
	ldp	x3, x4, [x1, 0x10]
	stp	x5, x6, [x0, -0x60]
	ldp	x5, x6, [x1, 0x20]
	stp	x7, x8, [x0, -0x50]
	ldp	x7, x8, [x1, 0x30]
	stp	x9, x10, [x0, -0x40]
	ldp	x9, x10, [x1, 0x40]
	stp	x11, x12, [x0, -0x30]
	ldp	x11, x12, [x1, 0x50]
	stp	x13, x14, [x0, -0x20]
	ldp	x13, x14, [x1, 0x60]
	stp	x15, x16, [x0, -0x10]
	ldp	x15, x16, [x1, 0x70]
	stp	x17, x18, [x0], 0x80
	ldp	x17, x18, [x1, 0x80]!

	// If the result of the subs above was non-negative then there's
	// another chunk to do
	b.ge	LOCAL(memcpy_chunk128_loop)

	// Prefetch stores for the chunk after the end of the loop.
.equ LOCAL(offset), 0
.rept (0x80 >> CPU_L1D_LINE_BITS)
	prfm	pstl1keep, [x0, LOCAL(offset) + 0x10]
.equ LOCAL(offset), LOCAL(offset) +  (1 << CPU_L1D_LINE_BITS)
.endr
	// The size has an offset of -128 at this point; undo it so we can
	// fall through to the smaller copies below.
	adds	x2, x2, 0x80

	// Store the last chunk.
	stp	x3, x4, [x0, -0x70]
	stp	x5, x6, [x0, -0x60]
	stp	x7, x8, [x0, -0x50]
	stp	x9, x10, [x0, -0x40]
	stp	x11, x12, [x0, -0x30]
	stp	x13, x14, [x0, -0x20]
	stp	x15, x16, [x0, -0x10]
	stp	x17, x18, [x0]

	// If there's nothing left to copy, just return.
	b.eq	LOCAL(return)

	// At this point, we have -16-byte offsets on both pointers; undo them
	// so we can fall through.
	add	x0, x0, 16
	add	x1, x1, 16

local memcpy_below256:
	// If there is at least one chunk left, copy one chunk.
	cmp	x2, 0x80
	blt	LOCAL(memcpy_below128)

	ldp	x3, x4, [x1]
	ldp	x5, x6, [x1, 0x10]
	ldp	x7, x8, [x1, 0x20]
	ldp	x9, x10, [x1, 0x30]
	stp	x3, x4, [x0]
	ldp	x11, x12, [x1, 0x40]
	stp	x5, x6, [x0, 0x10]
	ldp	x13, x14, [x1, 0x50]
	stp	x7, x8, [x0, 0x20]
	ldp	x15, x16, [x1, 0x60]
	stp	x9, x10, [x0, 0x30]
	ldp	x17, x18, [x1, 0x70]
	stp	x11, x12, [x0, 0x40]
	stp	x13, x14, [x0, 0x50]
	stp	x15, x16, [x0, 0x60]
	stp	x17, x18, [x0, 0x70]

	add	x0, x0, 0x80
	add	x1, x1, 0x80
	sub	x2, x2, 0x80

local memcpy_below128:
	// There are up to 7 16-byte blocks left to copy. Calculate jump
	// offsets into an ldp sequence and then an stp sequence to copy the
	// right number of them. Since there are only 7, we have two extra
	// temporary registers: x3 and x4.
	bic	x4, x2, (1 << 4) - 1
	adr	x3, 1f
	add	x0, x0, x4
	sub	x3, x3, x4, lsr #(4 - JUMP_SHIFT)
	add	x1, x1, x4
	sub	x2, x2, x4
	br	x3

0:
	BRANCH_TARGET(j, ldp	x5, x6, [x1, -0x70])
	BRANCH_TARGET(j, ldp	x7, x8, [x1, -0x60])
	BRANCH_TARGET(j, ldp	x9, x10, [x1, -0x50])
	BRANCH_TARGET(j, ldp	x11, x12, [x1, -0x40])
	BRANCH_TARGET(j, ldp	x13, x14, [x1, -0x30])
	BRANCH_TARGET(j, ldp	x15, x16, [x1, -0x20])
	BRANCH_TARGET(j, ldp	x17, x18, [x1, -0x10])
1:
.if	(. - 0b) != (7 * (1 << JUMP_SHIFT))
.error "Jump table error"
.endif
	BRANCH_TARGET(j,)

	adr	x3, 1f
	sub	x3, x3, x4, lsr #(4 - JUMP_SHIFT)
	br	x3

0:
	BRANCH_TARGET(j, stp	x5, x6, [x0, -0x70])
	BRANCH_TARGET(j, stp	x7, x8, [x0, -0x60])
	BRANCH_TARGET(j, stp	x9, x10, [x0, -0x50])
	BRANCH_TARGET(j, stp	x11, x12, [x0, -0x40])
	BRANCH_TARGET(j, stp	x13, x14, [x0, -0x30])
	BRANCH_TARGET(j, stp	x15, x16, [x0, -0x20])
	BRANCH_TARGET(j, stp	x17, x18, [x0, -0x10])
1:
.if	(. - 0b) != (7 * (1 << JUMP_SHIFT))
.error "Jump table error"
.endif
	BRANCH_TARGET(j,)
	// There must be less than 16 bytes left now.
	cbnz	x2, LOCAL(memcpy_below16)
local return:
	ret
function_end memcpy_align16


// Slow memcpy, used by memmove() when the destination overlaps the source and
// is between 1 and CPU_MEMCPY_STRIDE bytes above it. This should never be
// used in the hypervisor so we don't care about its performance. It exists
// only so the libc references to memmove() in the test program don't crash.
function memcpy_bytes
	cbz	x2, 2f
1:
	subs	x2, x2, 1
	ldrb	w3, [x1], 1
	strb	w3, [x0], 1
	b.ne	1b
2:
	ret
// Force a link failure if this function is used in hypervisor builds
.word	memcpy_bytes_is_defined_only_in_test_programs
function_end memcpy_bytes

```

`hyp/core/util/aarch64/src/memset.S`:

```S
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <asm/asm_defs.inc>
#include <asm/cpu.h>
#include <asm/panic.inc>


#if defined(ARCH_ARM_FEAT_BTI)
// Two instructions per jump
#define JUMP_SHIFT	3
#else
#define JUMP_SHIFT	2
#endif

// memset to zeros of no more than 31 bytes
function memset_zeros_below32
	// Assume the target is size-aligned and do the largest stores first.
	tbz	x1, 4, LOCAL(memset_zeros_below16)
	stp	xzr, xzr, [x0], 16
local memset_zeros_below16:
	tbz	x1, 3, 1f
	str	xzr, [x0], 8
1:
	tbz	x1, 2, 1f
	str	wzr, [x0], 4
1:
	tbz	x1, 1, 1f
	strh	wzr, [x0], 2
1:
	tbz	x1, 0, 1f
	strb	wzr, [x0]
1:
	ret
function_end memset_zeros_below32

// memset to zeros of at least 31 bytes (i.e. large enough to align up to 16
// and do at least one 16-byte copy).
function memset_zeros_alignable
	// Align up the target address to 16. We know that the size (x1) is at
	// least 16 here, so we don't have to check it during this alignment.
	tbz	x0, 0, 1f
	strb	wzr, [x0], 1
	sub	x1, x1, 1
1:
	tbz	x0, 1, 1f
	strh	wzr, [x0], 2
	sub	x1, x1, 2
1:
	tbz	x0, 2, 1f
	str	wzr, [x0], 4
	sub	x1, x1, 4
1:
	tbz	x0, 3, 1f
	str	xzr, [x0], 8
	sub	x1, x1, 8
1:
	// At this point we've cleared up to 15 bytes, so we know there are at
	// least 16 left. We can safely fall through to _align16.
function_chain memset_zeros_alignable, memset_zeros_align16
	// Determine how many stores need to be done to align to DCZVA_BITS,
	// and also whether the remaining size is less than a DC ZVA block.
	neg	x3, x0
	cmp	x1, 1 << CPU_DCZVA_BITS
	and	x2, x3, ((1 << CPU_DCZVA_BITS) - 1)

	// If we have less than a DC ZVA block left to copy, don't align up.
	b.lt	LOCAL(memset_zeros_no_dczva)

	// Align up to the DC ZVA size by calculating a jump into an stp
	// sequence based on the number of 16-byte chunks needed for alignment
	// (which may be 0). By the time we finish this, we might again have
	// less than one DC ZVA block left, so redo the comparison.
	adr	x5, 1f
	sub	x1, x1, x2
	add	x0, x0, x2
	sub	x5, x5, x2, lsr #(4 - JUMP_SHIFT)
	cmp	x1, 1 << CPU_DCZVA_BITS
	br	x5
0:
.equ LOCAL(offset), 16 - (1 << CPU_DCZVA_BITS)
.rept (1 << (CPU_DCZVA_BITS - 4) - 1)
	BRANCH_TARGET(j, stp	xzr, xzr, [x0, LOCAL(offset)])
.equ LOCAL(offset), LOCAL(offset) + 16
.endr
1:
.if	(. - 0b) != (((1 << (CPU_DCZVA_BITS - 4)) - 1) * (1 << JUMP_SHIFT))
.error "Jump table error"
.endif
	BRANCH_TARGET(j,)
	b.lt	LOCAL(memset_zeros_no_dczva)
function_chain memset_zeros_align16, memset_zeros_dczva
	bic	x2, x1, ((1 << CPU_DCZVA_BITS) - 1)
	and	x1, x1, ((1 << CPU_DCZVA_BITS) - 1)
1:
	subs	x2, x2, (1 << CPU_DCZVA_BITS)
	dc	zva, x0
	add	x0, x0, (1 << CPU_DCZVA_BITS)
	b.ne	1b

	cbz	x1, LOCAL(return)

local memset_zeros_no_dczva:
	// Less than one DC ZVA block left, so calculate a jump into an stp
	// sequence based on the number of remaining 16-byte chunks.
	bic	x4, x1, (1 << 4) - 1
	adr	x5, 1f
	sub	x1, x1, x4
	sub	x5, x5, x4, lsr #(4 - JUMP_SHIFT)
	add	x0, x0, x4
	br	x5
0:
.equ LOCAL(offset), 16 - (1 << CPU_DCZVA_BITS)
.rept (1 << (CPU_DCZVA_BITS - 4)) - 1
	BRANCH_TARGET(j, stp	xzr, xzr, [x0, LOCAL(offset)])
.equ LOCAL(offset), LOCAL(offset) + 16
.endr
1:
.if	(. - 0b) != (((1 << (CPU_DCZVA_BITS - 4)) - 1) * (1 << JUMP_SHIFT))
.error "Jump table error"
.endif
	BRANCH_TARGET(j,)
	// There must be less than 16 bytes left now.
	cbnz	x1, LOCAL(memset_zeros_below16)
local return:
	ret
function_end memset_zeros_dczva


// memset to nonzero of no more than 31 bytes. Note that x1 has been expanded
// to contain 8 copies of the nonzero byte to set.
function memset_below32
	// Assume the target is size-aligned and do the largest stores first.
	tbz	x2, 4, LOCAL(memset_below16)
	stp	x1, x1, [x0], 16
local memset_below16:
	tbz	x2, 3, 1f
	str	x1, [x0], 8
1:
	tbz	x2, 2, 1f
	str	w1, [x0], 4
1:
	tbz	x2, 1, 1f
	strh	w1, [x0], 2
1:
	tbz	x2, 0, 1f
	strb	w1, [x0]
1:
	ret
function_end memset_below32

// memset to nonzero of at least 31 bytes (i.e. large enough to align up to 16
// and do at least one 16-byte copy). For the _align16 entry point, x1 has
// been expanded to contain at 8 copies of the nonzero byte to set; for the
// _alignable entry point, it has not been expanded yet, but has been
// zero-extended from 8 bits if necessary.
function memset_alignable
	// Align up the target address to 16, and simultaneously duplicate the
	// byte to set until we have 16 copies of it. We know that the size
	// (x2) is at least 16 here, so we don't have to check it during this
	// alignment.
	orr	x3, x1, x1, lsl 8
	tbz	x0, 0, 1f
	strb	w1, [x0], 1
	sub	x2, x2, 1
1:
	orr	x1, x3, x3, lsl 16
	tbz	x0, 1, 1f
	strh	w3, [x0], 2
	sub	x2, x2, 2
1:
	orr	x3, x1, x1, lsl 32
	tbz	x0, 2, 1f
	str	w1, [x0], 4
	sub	x2, x2, 4
1:
	mov	x1, x3
	tbz	x0, 3, 1f
	str	x3, [x0], 8
	sub	x2, x2, 8
1:
	// At this point we've cleared up to 15 bytes, so we know there are at
	// least 16 left; also we have expanded x1. We can safely fall through
	// to _align16.
	prfm	pstl1keep, [x0]
function_chain memset_alignable, memset_align16
	// Use the larger of 128-byte chunks or cache lines for large copies
.equ LOCAL(chunk), 0x80
.ifge (1 << CPU_L1D_LINE_BITS) - LOCAL(chunk)
.equ LOCAL(chunk), 1 << CPU_L1D_LINE_BITS
.endif
	// The first line was prefetched by the caller; prefetch the rest of
	// the first chunk.
.equ LOCAL(offset), 1 << CPU_L1D_LINE_BITS
.rept (LOCAL(chunk) >> CPU_L1D_LINE_BITS) - 1
	prfm	pstl1keep, [x0, LOCAL(offset)]
.equ LOCAL(offset), LOCAL(offset) +  (1 << CPU_L1D_LINE_BITS)
.endr

	// Unrolled loop copying chunks
	subs	x3, x2, LOCAL(chunk)
	and	x2, x2, LOCAL(chunk) - 1
	b.lt	2f
1:
	// Prefetch the next chunk.
.rept (LOCAL(chunk) >> CPU_L1D_LINE_BITS)
	prfm	pstl1keep, [x0, LOCAL(offset)]
.equ LOCAL(offset), LOCAL(offset) +  (1 << CPU_L1D_LINE_BITS)
.endr
	// Write the chunk with a sequence of 16-byte stores.
.equ LOCAL(offset), 0
.rept (LOCAL(chunk) / 0x10)
	stp	x1, x1, [x0, LOCAL(offset)]
.equ LOCAL(offset), LOCAL(offset) + 0x10
.endr
	subs	x3, x3, LOCAL(chunk)
	add	x0, x0, LOCAL(chunk)
	b.ge	1b
2:
	// Calculated jump for up to 7 16-byte chunks.
	lsr	x4, x2, 4
	adr	x5, 1f
	sub	x2, x2, x4, lsl 4
	add	x0, x0, x4, lsl 4
	sub	x6, x5, x4, lsl JUMP_SHIFT
	br	x6
.equ LOCAL(offset), 0x10 - LOCAL(chunk)
.rept (LOCAL(chunk) / 0x10) - 1
	BRANCH_TARGET(j, stp	x1, x1, [x0, LOCAL(offset)])
.equ LOCAL(offset), LOCAL(offset) + 0x10
.endr
1:
	// There must be less than 16 bytes left now.
	BRANCH_TARGET(j,)
	cbnz	x2, LOCAL(memset_below16)
	ret
function_end memset_align16

```

`hyp/core/util/aarch64/src/string.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <stdint.h>
#include <stdnoreturn.h>
#include <string.h>

#include <compiler.h>
#include <panic.h>
#include <util.h>

#include <asm/cpu.h>
#include <asm/prefetch.h>

// Assembly functions. All of these come in at least three variants:
//
// - _align16 for at least 16 bytes with target known to be 16-aligned;
// - _alignable for at least 31 bytes with unknown target alignment;
// - _below32 for less than 32 bytes (i.e. one access of each size).
//
// Note the overlap between _alignable and _below32 at n==31; either variant
// may be used at that size. We use _below32 because the logic to trigger its
// first 16-byte copy is simpler.
//
// For memset to zero there is additionally a _dczva variant, where the target
// is aligned to a DC ZVA block (typically a 64-byte cache line) and is at
// least that size.
//
// The variants other than _below32 fall through to the more-aligned versions
// once the necessary alignment has been established.

// TODO: Clang does only simple constant propagation when LTO is enabled,
// preferring to leave it until LTO. However, the LLVM IR has no way to
// represent __builtin_constant_p(). So it is not really possible to use
// __builtin_constant_p() here to avoid runtime checks, since it will nearly
// always evaluate to 0.
//
// To make any static assertions or build-time variant selection actually
// effective, we need to move all of the definitions below into inlines in a
// header. We would then need to either include it here to generate extern
// definitions for the backend to use, or else define them in the assembly.

void
memcpy_below32(void *restrict s1, const void *restrict s2, size_t n);

void
memcpy_alignable(void *restrict s1, const void *restrict s2, size_t n);

void
memcpy_align16(void *restrict s1, const void *restrict s2, size_t n);

void
memcpy_bytes(void *restrict s1, const void *restrict s2, size_t n);

void
memset_zeros_alignable(void *s, size_t n);

void
memset_zeros_below32(void *s, size_t n);

void
memset_zeros_align16(void *s, size_t n);

void
memset_zeros_dczva(void *s, size_t n);

void
memset_alignable(void *s, uint8_t c, size_t n);

void
memset_below32(void *s, uint64_t cs, size_t n);

void
memset_align16(void *s, uint64_t cs, size_t n);

void *
memcpy(void *restrict s1, const void *restrict s2, size_t n)
{
	assert(compiler_sizeof_object(s1) >= n);
	assert(compiler_sizeof_object(s2) >= n);
	if (n == 0U) {
		// Nothing to do.
	} else if (n < 32U) {
		prefetch_store_keep(s1);
		prefetch_load_stream(s2);
		memcpy_below32(s1, s2, n);
	} else {
		prefetch_store_keep(s1);
		prefetch_load_stream(s2);
		uintptr_t a16 = (uintptr_t)s1 & (uintptr_t)15;
		if (a16 == 0U) {
			memcpy_align16(s1, s2, n);
		} else {
			memcpy_alignable(s1, s2, n);
		}
	}

	return s1;
}

static void
memmove_bytes_reverse(uint8_t *dst, const uint8_t *src, size_t n)
{
	assert((uintptr_t)src < (uintptr_t)dst);

	// move to a higher address, copy backwards
	const uint8_t *srcr;
	uint8_t	      *dstr;
	srcr = src + (n - 1U);
	dstr = dst + (n - 1U);

	for (; n != 0; n--) {
		*dstr = *srcr;
		dstr--;
		srcr--;
	}
}

void *
memmove(void *dst, const void *src, size_t n)
{
	if (n == 0) {
		goto out;
	}

	if (util_add_overflows((uintptr_t)dst, n - 1U) ||
	    util_add_overflows((uintptr_t)src, n - 1U)) {
		panic("memmove_bytes addr overflow");
	}

	if ((uintptr_t)dst == (uintptr_t)src) {
		// Nothing to do.
	} else if ((uintptr_t)dst < (uintptr_t)src) {
		(void)memcpy(dst, src, n);
	} else if ((uintptr_t)src + (n - 1) < (uintptr_t)dst) {
		(void)memcpy(dst, src, n);
	} else {
		(void)memmove_bytes_reverse(dst, src, n);
	}

out:
	return dst;
}

errno_t
memset_s(void *s, rsize_t smax, int c, rsize_t n)
{
	assert(compiler_sizeof_object(s) >= smax);
	uintptr_t a16 = (uintptr_t)s & (uintptr_t)15;

	errno_t err = 0;

	if (s == NULL) {
		err = 1;
		goto out_null;
	}
	if (n > smax) {
		err = 1;
		n   = smax;
	}

	if (n == 0U) {
		// Nothing to do.
	} else if (c == 0) {
		uintptr_t a_zva = (uintptr_t)s &
				  (uintptr_t)util_mask(CPU_DCZVA_BITS);
		if (n < 32U) {
			prefetch_store_keep(s);
			memset_zeros_below32(s, n);
		} else if ((a_zva == 0U) && ((n >> CPU_DCZVA_BITS) > 0U)) {
			memset_zeros_dczva(s, n);
		} else if (a16 == 0U) {
			prefetch_store_keep(s);
			memset_zeros_align16(s, n);
		} else {
			prefetch_store_keep(s);
			memset_zeros_alignable(s, n);
		}
	} else {
		uint64_t cs = (uint64_t)(uint8_t)c;
		cs |= cs << 8;
		cs |= cs << 16;
		cs |= cs << 32;
		if (n < 32U) {
			prefetch_store_keep(s);
			memset_below32(s, cs, n);
		} else if (a16 == 0U) {
			prefetch_store_keep(s);
			memset_align16(s, cs, n);
		} else {
			prefetch_store_keep(s);
			memset_alignable(s, (uint8_t)c, n);
		}
	}

out_null:
	return err;
}

void *
memset(void *s, int c, size_t n)
{
	(void)memset_s(s, n, c, n);
	return s;
}

size_t
strlen(const char *str)
{
	const char *end = str;

	assert(str != NULL);

	for (; *end != '\0'; end++) {
	}

	return (size_t)((uintptr_t)end - (uintptr_t)str);
}

char *
strchr(const char *str, int c)
{
	uintptr_t   ret = (uintptr_t)NULL;
	const char *end = str;

	for (; *end != '\0'; end++) {
		if (*end == (char)c) {
			ret = (uintptr_t)end;
			break;
		}
	}

	return (char *)ret;
}

```

`hyp/core/util/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

interface util
source bitmap.c
arch_source aarch64 string.c memset.S memcpy.S
source refcount.c
source assert.c
source panic.c
source list.c
types util.tc

```

`hyp/core/util/src/assert.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>
#if !defined(NDEBUG) && !defined(__KLOCWORK__)
#include <string.h>

#include <attributes.h>
#include <compiler.h>
#include <log.h>
#include <preempt.h>
#include <trace.h>

#include <events/abort.h>
#include <events/scheduler.h>

#include <asm/event.h>
#include <asm/interrupt.h>

noreturn void NOINLINE COLD
assert_failed(const char *file, int line, const char *func, const char *err)
	LOCK_IMPL
{
	const char *file_short;

	// Stop all cores and disable preemption
	trigger_scheduler_stop_event();

	size_t len = strlen(file);
	if (len < 64U) {
		file_short = file;
	} else {
		file_short = file + len - 64;

		char *file_strchr = strchr(file_short, (int)'/');
		if (file_strchr != NULL) {
			file_short = file_strchr + 1;
		}
	}

	TRACE_AND_LOG(ERROR, ASSERT_FAILED,
		      "Assert failed in {:s} at {:s}:{:d}: {:s}",
		      (register_t)func, (register_t)(uintptr_t)file_short,
		      (register_t)(uintptr_t)line, (register_t)(uintptr_t)err);

	trigger_abort_kernel_event(ABORT_REASON_ASSERTION);

	while (1) {
		asm_event_wait(err);
	}
}
#else
noreturn void NOINLINE
assert_failed(const char *file, int line, const char *func, const char *err);
#endif

```

`hyp/core/util/src/bitmap.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>

#include <atomic.h>
#include <bitmap.h>
#include <compiler.h>
#include <util.h>

#define BITMAP_SET_BIT(x) ((register_t)1U << (((x) % BITMAP_WORD_BITS)))
#define BITMAP_WORD(x)	  ((x) / BITMAP_WORD_BITS)
#define BITMAP_SIZE_ASSERT(bitmap, bit)                                        \
	assert((index_t)(compiler_sizeof_object(bitmap) /                      \
			 sizeof(register_t)) > BITMAP_WORD(bit))

bool
bitmap_isset(const register_t *bitmap, index_t bit)
{
	BITMAP_SIZE_ASSERT(bitmap, bit);

	index_t i = BITMAP_WORD(bit);

	return (bitmap[i] & BITMAP_SET_BIT(bit)) != 0U;
}

void
bitmap_set(register_t *bitmap, index_t bit)
{
	BITMAP_SIZE_ASSERT(bitmap, bit);

	index_t i = BITMAP_WORD(bit);

	bitmap[i] |= BITMAP_SET_BIT(bit);
}

void
bitmap_clear(register_t *bitmap, index_t bit)
{
	BITMAP_SIZE_ASSERT(bitmap, bit);

	index_t i = BITMAP_WORD(bit);

	bitmap[i] &= ~BITMAP_SET_BIT(bit);
}

register_t
bitmap_extract(const register_t *bitmap, index_t bit, index_t width)
{
	BITMAP_SIZE_ASSERT(bitmap, bit + width - 1U);
	assert((width <= BITMAP_WORD_BITS) &&
	       (BITMAP_WORD(bit) == BITMAP_WORD(bit + width - 1U)));

	index_t i = BITMAP_WORD(bit);

	return (bitmap[i] >> (bit % BITMAP_WORD_BITS)) & util_mask(width);
}

void
bitmap_insert(register_t *bitmap, index_t bit, index_t width, register_t value)
{
	BITMAP_SIZE_ASSERT(bitmap, bit + width - 1U);
	assert((width <= BITMAP_WORD_BITS) &&
	       (BITMAP_WORD(bit) == BITMAP_WORD(bit + width - 1U)));

	index_t i = BITMAP_WORD(bit);

	bitmap[i] &= ~(util_mask(width) << (bit % BITMAP_WORD_BITS));
	bitmap[i] |= (value & util_mask(width)) << (bit % BITMAP_WORD_BITS);
}

bool
bitmap_ffs(const register_t *bitmap, index_t num_bits, index_t *bit)
{
	assert(num_bits > 0U);
	BITMAP_SIZE_ASSERT(bitmap, num_bits - 1U);

	bool result = false;
	BITMAP_FOREACH_SET_BEGIN(i, bitmap, num_bits)
		result = true;
		*bit   = i;
		break;
	BITMAP_FOREACH_SET_END

	return result;
}

bool
bitmap_ffc(const register_t *bitmap, index_t num_bits, index_t *bit)
{
	assert(num_bits > 0U);
	BITMAP_SIZE_ASSERT(bitmap, num_bits - 1U);

	bool result = false;
	BITMAP_FOREACH_CLEAR_BEGIN(i, bitmap, num_bits)
		result = true;
		*bit   = i;
		break;
	BITMAP_FOREACH_CLEAR_END

	return result;
}

bool
bitmap_empty(const register_t *bitmap, index_t num_bits)
{
	assert(num_bits > 0U);
	BITMAP_SIZE_ASSERT(bitmap, num_bits - 1U);

	index_t i;
	bool	result = true;

	for (i = 0U; i < BITMAP_WORD(num_bits); i++) {
		if (bitmap[i] != 0U) {
			result = false;
			break;
		}
	}

	if ((i + 1U) == BITMAP_NUM_WORDS(num_bits)) {
		if ((bitmap[i] & (BITMAP_SET_BIT(num_bits) - 1U)) != 0U) {
			result = false;
		}
	}

	return result;
}

bool
bitmap_full(const register_t *bitmap, index_t num_bits)
{
	assert(num_bits > 0U);
	BITMAP_SIZE_ASSERT(bitmap, num_bits - 1U);

	index_t i;
	bool	result = true;

	for (i = 0U; i < BITMAP_WORD(num_bits); i++) {
		if (~bitmap[i] != 0U) {
			result = false;
			break;
		}
	}

	if ((i + 1U) == BITMAP_NUM_WORDS(num_bits)) {
		if ((~bitmap[i] & (BITMAP_SET_BIT(num_bits) - 1U)) != 0U) {
			result = false;
		}
	}

	return result;
}

bool
bitmap_atomic_isset(const _Atomic register_t *bitmap, index_t bit,
		    memory_order order)
{
	BITMAP_SIZE_ASSERT(bitmap, bit);

	index_t i = BITMAP_WORD(bit);

	return (atomic_load_explicit(&bitmap[i], order) &
		BITMAP_SET_BIT(bit)) != 0U;
}

bool
bitmap_atomic_test_and_set(_Atomic register_t *bitmap, index_t bit,
			   memory_order order)
{
	BITMAP_SIZE_ASSERT(bitmap, bit);

	index_t	   i   = BITMAP_WORD(bit);
	register_t old = atomic_fetch_or_explicit(&bitmap[i],
						  BITMAP_SET_BIT(bit), order);

	return (old & BITMAP_SET_BIT(bit)) != 0U;
}

bool
bitmap_atomic_test_and_clear(_Atomic register_t *bitmap, index_t bit,
			     memory_order order)
{
	BITMAP_SIZE_ASSERT(bitmap, bit);

	index_t	   i   = BITMAP_WORD(bit);
	register_t old = atomic_fetch_and_explicit(&bitmap[i],
						   ~BITMAP_SET_BIT(bit), order);

	return (old & BITMAP_SET_BIT(bit)) != 0U;
}

bool
bitmap_atomic_ffs(const _Atomic register_t *bitmap, index_t num_bits,
		  index_t *bit)
{
	assert(num_bits > 0U);
	BITMAP_SIZE_ASSERT(bitmap, num_bits - 1U);

	bool result = false;
	BITMAP_ATOMIC_FOREACH_SET_BEGIN(i, bitmap, num_bits)
		result = true;
		*bit   = i;
		break;
	BITMAP_ATOMIC_FOREACH_SET_END

	return result;
}

bool
bitmap_atomic_ffc(const _Atomic register_t *bitmap, index_t num_bits,
		  index_t *bit)
{
	assert(num_bits > 0U);
	BITMAP_SIZE_ASSERT(bitmap, num_bits - 1U);

	bool result = false;
	BITMAP_ATOMIC_FOREACH_CLEAR_BEGIN(i, bitmap, num_bits)
		result = true;
		*bit   = i;
		break;
	BITMAP_ATOMIC_FOREACH_CLEAR_END

	return result;
}

bool
bitmap_atomic_empty(const _Atomic register_t *bitmap, index_t num_bits)
{
	assert(num_bits > 0U);
	BITMAP_SIZE_ASSERT(bitmap, num_bits - 1U);

	index_t i;
	bool	result = true;

	for (i = 0U; i < BITMAP_WORD(num_bits); i++) {
		if (atomic_load_relaxed(&bitmap[i]) != 0U) {
			result = false;
			break;
		}
	}

	if ((i + 1U) == BITMAP_NUM_WORDS(num_bits)) {
		if ((atomic_load_relaxed(&bitmap[i]) &
		     (BITMAP_SET_BIT(num_bits) - 1U)) != 0U) {
			result = false;
		}
	}

	return result;
}

bool
bitmap_atomic_full(const _Atomic register_t *bitmap, index_t num_bits)
{
	assert(num_bits > 0U);
	BITMAP_SIZE_ASSERT(bitmap, num_bits - 1U);

	index_t i;
	bool	result = true;

	for (i = 0U; i < BITMAP_WORD(num_bits); i++) {
		if (~atomic_load_relaxed(&bitmap[i]) != 0U) {
			result = false;
			break;
		}
	}

	if ((i + 1U) == BITMAP_NUM_WORDS(num_bits)) {
		if ((~atomic_load_relaxed(&bitmap[i]) &
		     (BITMAP_SET_BIT(num_bits) - 1U)) != 0U) {
			result = false;
		}
	}

	return result;
}

register_t
bitmap_atomic_extract(const _Atomic register_t *bitmap, index_t bit,
		      index_t width, memory_order order)
{
	BITMAP_SIZE_ASSERT(bitmap, bit + width - 1U);
	assert((width <= BITMAP_WORD_BITS) &&
	       (BITMAP_WORD(bit) == BITMAP_WORD(bit + width - 1U)));

	index_t i = BITMAP_WORD(bit);

	return (atomic_load_explicit(&bitmap[i], order) >>
		(bit % BITMAP_WORD_BITS)) &
	       util_mask(width);
}

void
bitmap_atomic_insert(_Atomic register_t *bitmap, index_t bit, index_t width,
		     register_t value, memory_order order)
{
	BITMAP_SIZE_ASSERT(bitmap, bit + width - 1U);
	assert((width <= BITMAP_WORD_BITS) &&
	       (BITMAP_WORD(bit) == BITMAP_WORD(bit + width - 1U)));

	index_t i = BITMAP_WORD(bit);

	memory_order load_order =
		(order == memory_order_release)	  ? memory_order_relaxed
		: (order == memory_order_acq_rel) ? memory_order_acquire
						  : order;

	register_t old_word = atomic_load_explicit(&bitmap[i], load_order),
		   new_word;
	do {
		new_word = (old_word &
			    ~(util_mask(width) << (bit % BITMAP_WORD_BITS))) |
			   ((value & util_mask(width))
			    << (bit % BITMAP_WORD_BITS));
	} while (!atomic_compare_exchange_weak_explicit(
		&bitmap[i], &old_word, new_word, order, load_order));
}

```

`hyp/core/util/src/list.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>

#include <list.h>

void
list_init(list_t *list)
{
	assert(list != NULL);

	list_node_t *head = &list->head;

	atomic_store_relaxed(&head->next, head);
	head->prev = head;
}

bool
list_is_empty(list_t *list)
{
	assert(list != NULL);

	return (atomic_load_relaxed(&list->head.next) == &list->head);
}

list_node_t *
list_get_head(list_t *list)
{
	assert(list != NULL);

	list_node_t *node = atomic_load_relaxed(&list->head.next);

	return (node != &list->head) ? node : NULL;
}

static inline void
list_insert_at_head_explicit(list_t *list, list_node_t *node,
			     memory_order order)
{
	assert(list != NULL);
	assert(node != NULL);

	list_node_t *prev = &list->head;
	list_node_t *next = atomic_load_relaxed(&prev->next);

	node->prev = prev;
	atomic_store_relaxed(&node->next, next);

	atomic_store_explicit(&prev->next, node, order);
	next->prev = node;
}

void
list_insert_at_head(list_t *list, list_node_t *node)
{
	list_insert_at_head_explicit(list, node, memory_order_relaxed);
}

static inline void
list_insert_at_tail_explicit(list_t *list, list_node_t *node,
			     memory_order order)
{
	assert(list != NULL);
	assert(node != NULL);

	list_node_t *next = &list->head;
	list_node_t *prev = next->prev;

	node->prev = prev;
	atomic_store_relaxed(&node->next, next);

	atomic_store_explicit(&prev->next, node, order);
	next->prev = node;
}

void
list_insert_at_tail(list_t *list, list_node_t *node)
{
	list_insert_at_tail_explicit(list, node, memory_order_relaxed);
}

void
list_insert_at_tail_release(list_t *list, list_node_t *node)
{
	list_insert_at_tail_explicit(list, node, memory_order_release);
}

static inline bool
list_insert_in_order_explicit(list_t *list, list_node_t *node,
			      bool (*compare_fn)(list_node_t *a,
						 list_node_t *b),
			      memory_order order)
{
	assert(list != NULL);
	assert(node != NULL);

	bool	     new_head = false;
	list_node_t *head     = &list->head;

	list_node_t *prev = head;
	list_node_t *next = atomic_load_relaxed(&head->next);

	while (next != head) {
		if (compare_fn(node, next)) {
			break;
		}

		prev = next;
		next = atomic_load_relaxed(&prev->next);
	}

	if (prev == head) {
		new_head = true;
	}

	node->prev = prev;
	atomic_store_relaxed(&node->next, next);

	atomic_store_explicit(&prev->next, node, order);
	next->prev = node;

	return new_head;
}

bool
list_insert_in_order(list_t *list, list_node_t *node,
		     bool (*compare_fn)(list_node_t *a, list_node_t *b))
{
	return list_insert_in_order_explicit(list, node, compare_fn,
					     memory_order_relaxed);
}

static inline void
list_insert_after_node_explicit(list_t *list, list_node_t *prev,
				list_node_t *node, memory_order order)
{
	assert(node != NULL);
	assert(prev != NULL);

	(void)list;

	list_node_t *next = atomic_load_relaxed(&prev->next);

	node->prev = prev;
	atomic_store_relaxed(&node->next, next);

	atomic_store_explicit(&prev->next, node, order);
	next->prev = node;
}

void
list_insert_after_node(list_t *list, list_node_t *prev, list_node_t *node)
{
	list_insert_after_node_explicit(list, prev, node, memory_order_relaxed);
}

bool
list_delete_node(list_t *list, list_node_t *node)
{
	assert(list != NULL);
	assert(node != NULL);

	bool new_head = false;

	if ((atomic_load_relaxed(&node->next) == NULL) ||
	    (node->prev == NULL)) {
		goto out;
	}

	list_node_t *head = &list->head;
	list_node_t *next = atomic_load_relaxed(&node->next);
	list_node_t *prev = node->prev;

	atomic_store_relaxed(&prev->next, next);
	next->prev = prev;

	if ((prev == head) && (next != head)) {
		new_head = true;
	}

	// Note: we do not zero the node's pointers here, because there might
	// be a list_foreach_container_consume() that still holds a pointer to
	// the node.
out:
	return new_head;
}

```

`hyp/core/util/src/panic.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <hyptypes.h>

#include <attributes.h>
#include <compiler.h>
#include <log.h>
#include <panic.h>
#include <preempt.h>
#include <trace.h>

#include <events/abort.h>
#include <events/scheduler.h>

#include <asm/event.h>

noreturn void NOINLINE COLD
panic(const char *str) LOCK_IMPL
{
	void *from  = __builtin_return_address(0);
	void *frame = __builtin_frame_address(0);

	from = __builtin_extract_return_addr(from);

	// Stop all cores and disable preemption
	trigger_scheduler_stop_event();

	TRACE_AND_LOG(ERROR, PANIC, "Panic: {:s} from PC {:#x}, FP {:#x}",
		      (register_t)(uintptr_t)str, (register_t)(uintptr_t)from,
		      (register_t)(uintptr_t)frame);

	trigger_abort_kernel_event(ABORT_REASON_PANIC);

	while (1) {
		asm_event_wait(str);
	}
}

```

`hyp/core/util/src/refcount.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>
#include <stdbool.h>

#include <atomic.h>
#include <compiler.h>
#include <refcount.h>

// Initialise a reference count, with a single reference held.
void
refcount_init(refcount_t *ref)
{
	atomic_init(&ref->count, 1);
}

// Get a reference, assuming that the count is nonzero. This must only be used
// in cases where the the caller already knows that there is at least one
// reference that cannot be concurrently released by another thread, hence the
// name. No memory barrier is implied; adequate barriers should be provided by
// whatever other mechanism is used to guarantee that the count is nonzero,
// e.g. RCU.
void
refcount_get_additional(refcount_t *ref)
{
	uint32_t count =
		atomic_fetch_add_explicit(&ref->count, 1, memory_order_relaxed);

	assert(count > 0U);
	(void)count;
}

// Get a reference, without assuming that the count is nonzero. The caller
// must check the result; if it is false, the count had already reached zero
// and the reference could not be token. An acquire memory barrier is implied.
bool
refcount_get_safe(refcount_t *ref)
{
	uint32_t count	 = atomic_load_relaxed(&ref->count);
	bool	 success = false;

	while (count > 0U) {
		assert(count < (uint32_t)UINT32_MAX);
		if (atomic_compare_exchange_weak_explicit(
			    &ref->count, &count, count + 1U,
			    memory_order_acquire, memory_order_relaxed)) {
			success = true;
			break;
		}
	}

	return success;
}

// Release a reference. The caller must check the result; if it is true, the
// count has now reached zero and the caller must take action to free the
// underlying resource. This is always a release operation. If this reduces the
// count to zero (and returns true), it is also an acquire operation.
bool
refcount_put(refcount_t *ref)
{
	uint32_t count = atomic_fetch_sub_explicit(&ref->count, 1U,
						   memory_order_release);
	assert(count > 0U);
	if (compiler_expected(count > 1U)) {
		return false;
	} else {
		atomic_thread_fence(memory_order_acquire);
		return true;
	}
}

```

`hyp/core/util/tests/Makefile`:

```
CC=clang -target aarch64-linux-gnu

CFLAGS=-std=c11 --sysroot=/usr/aarch64-linux-gnu -O3 -flto -Wno-gcc-compat
CPPFLAGS=-D_DEFAULT_SOURCE -D__STDC_WANT_LIB_EXT1__=1 -DHYP_STANDALONE_TEST \
	 -I../../../interfaces/util/include \
	 -I../../../arch/aarch64/include \
	 -I../../../arch/generic/include \
	 -Iqemu/include

SRC=../tests/string.c \
    ../aarch64/src/string.c \
    ../aarch64/src/memset.S \
    ../aarch64/src/memcpy.S

LDFLAGS=-static

OBJ=string-test

$(OBJ): $(SRC)
	$(CC) $(CFLAGS) $(CPPFLAGS) $(LDFLAGS) $^ -o $@

```

`hyp/core/util/tests/qemu/include/asm/cpu.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// Miscellaneous definitions describing the CPU implementation.

// The size in address bits of a line in the innermost visible data cache.
#define CPU_L1D_LINE_BITS 6U

// The size in address bits of the CPU's DC ZVA block. This is nearly always
// the same as CPU_L1D_LINE_BITS.
#define CPU_DCZVA_BITS 9U

// The largest difference between the source and destination pointers during
// the optimised memcpy() for this CPU. This is here because it might depend
// on CPU_L1D_LINE_BITS in some implementations.
#define CPU_MEMCPY_STRIDE 256U

```

`hyp/core/util/tests/string.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// FIXME:
// Integrate these tests into unittest configuration

#include <assert.h>
#include <stdalign.h>
#include <stdbool.h>
#include <stddef.h>
#include <stdint.h>
#include <stdio.h>
#include <stdlib.h>
#include <stdnoreturn.h>
#include <string.h>

#include <attributes.h>
#include <errno.h>

#include <asm/cpu.h>

#define LARGE_ALIGN (1 << CPU_DCZVA_BITS)
#define SMALL_ALIGN 16
#define MAX_SIZE    2048

#define BUFFER_PAD  1024
#define BUFFER_SIZE (MAX_SIZE + LARGE_ALIGN + (2 * BUFFER_PAD))

#define INIT_BYTE   0xff
#define MEMSET_BYTE 0x42

static uint8_t alignas(LARGE_ALIGN) dst_buffer[BUFFER_SIZE];

static uint8_t alignas(LARGE_ALIGN) src_buffer[BUFFER_SIZE];

typedef int    errno_t;
typedef size_t rsize_t;
extern errno_t
memset_s(void *s, rsize_t smax, int c, size_t n);

noreturn void NOINLINE
assert_failed(const char *file, int line, const char *func, const char *err)
{
	fprintf(stderr, "Assert failed in %s at %s:%d: %s\n", func, file, line,
		err);
	abort();
}

noreturn void NOINLINE
panic(const char *msg)
{
	fprintf(stderr, "panic: %s\n", msg);
	abort();
}

static size_t
memchk(const volatile uint8_t *p, int c, size_t n)
{
	for (size_t i = 0; i < n; i++) {
		if (p[i] != (uint8_t)c) {
			return i;
		}
	}
	return n;
}

static size_t
memcmpchk(const volatile uint8_t *p, const volatile uint8_t *q, size_t n)
{
	for (size_t i = 0; i < n; i++) {
		if (p[i] != q[i]) {
			return i;
		}
	}
	return n;
}

static void
memset_test(size_t size, size_t misalign, int c)
{
	size_t start = BUFFER_PAD + misalign;
	size_t end   = start + size;
	size_t pos;

	// We assume that we can memset the whole buffer safely... hopefully
	// any bugs in it won't crash the test before we find them!
	memset(dst_buffer, INIT_BYTE, BUFFER_SIZE);

	memset(&dst_buffer[start], c, size);

	pos = memchk(dst_buffer, INIT_BYTE, start);
	if (pos < start) {
		fprintf(stderr,
			"FAILED: memset(buffer + %#zx, %#x, %#zx) set byte at offset -%#zx to %#x\n",
			start - BUFFER_PAD, c, size, start - pos,
			dst_buffer[pos]);
		exit(2);
	}

	pos = memchk(&dst_buffer[start], c, size);
	if (pos < size) {
		fprintf(stderr,
			"FAILED: memset(buffer + %#zx, %#x, %#zx) set byte at offset %#zx to %#x\n",
			start - BUFFER_PAD, c, size, pos,
			dst_buffer[start + pos]);
		exit(2);
	}

	pos = memchk(&dst_buffer[end], INIT_BYTE, BUFFER_PAD);
	if (pos < BUFFER_PAD) {
		fprintf(stderr,
			"FAILED: memset(buffer + %#zx, %#x, %#zx) set byte at offset %#zx to %#x\n",
			start - BUFFER_PAD, c, size, size + pos,
			dst_buffer[end + pos]);
		exit(2);
	}
}

static void
memset_s_test(size_t size, size_t misalign, int c)
{
	size_t start = BUFFER_PAD + misalign;
	size_t end   = start + size;
	size_t pos;

	// We assume that we can memset the whole buffer safely... hopefully
	// any bugs in it won't crash the test before we find them!
	memset_s(dst_buffer, BUFFER_SIZE, INIT_BYTE, BUFFER_SIZE);

	memset_s(&dst_buffer[start], BUFFER_SIZE, c, size);

	pos = memchk(dst_buffer, INIT_BYTE, start);
	if (pos < start) {
		fprintf(stderr,
			"FAILED: memset(buffer + %#zx, %#x, %#zx) set byte at offset -%#zx to %#x\n",
			start - BUFFER_PAD, c, size, start - pos,
			dst_buffer[pos]);
		exit(2);
	}

	pos = memchk(&dst_buffer[start], c, size);
	if (pos < size) {
		fprintf(stderr,
			"FAILED: memset(buffer + %#zx, %#x, %#zx) set byte at offset %#zx to %#x\n",
			start - BUFFER_PAD, c, size, pos,
			dst_buffer[start + pos]);
		exit(2);
	}

	pos = memchk(&dst_buffer[end], INIT_BYTE, BUFFER_PAD);
	if (pos < BUFFER_PAD) {
		fprintf(stderr,
			"FAILED: memset(buffer + %#zx, %#x, %#zx) set byte at offset %#zx to %#x\n",
			start - BUFFER_PAD, c, size, size + pos,
			dst_buffer[end + pos]);
		exit(2);
	}
}

static void
memset_tests(void)
{
	size_t size, dst_misalign;
	printf("Testing memset...");
	for (size = 0; size <= MAX_SIZE; size++) {
		if ((size % 64) == 0) {
			printf("\n%#5zx: .", size);
		} else {
			printf(".");
		}
		for (dst_misalign = 0; dst_misalign < LARGE_ALIGN;
		     dst_misalign++) {
			memset_test(size, dst_misalign, MEMSET_BYTE);
			memset_test(size, dst_misalign, 0);
			memset_s_test(size, dst_misalign, MEMSET_BYTE);
			memset_s_test(size, dst_misalign, 0);
		}
	}
	printf("\nPASS\n");
}

static void
memcpy_test(size_t size, size_t src_misalign, size_t dst_misalign)
{
	size_t src_start = BUFFER_PAD + src_misalign;
	size_t dst_start = BUFFER_PAD + dst_misalign;
	size_t dst_end	 = dst_start + size;
	size_t pos;

	// We tested memset first, so it should be safe to use it to clear
	// the destination buffer.
	memset(dst_buffer, INIT_BYTE, BUFFER_SIZE);

	memcpy(&dst_buffer[dst_start], &src_buffer[src_start], size);

	pos = memchk(dst_buffer, INIT_BYTE, dst_start);
	if (pos < dst_start) {
		fprintf(stderr,
			"FAILED: memcpy(dst + %#zx, src + %#zx, %#zx) set byte at offset -%#zx to %#x\n",
			dst_start - BUFFER_PAD, src_start - BUFFER_PAD, size,
			dst_start - pos, dst_buffer[pos]);
		exit(2);
	}

	pos = memcmpchk(&dst_buffer[dst_start], &src_buffer[src_start], size);
	if (pos < size) {
		fprintf(stderr,
			"FAILED: memcpy(dst + %#zx, src + %#zx, %#zx) set byte at offset %#zx to %#x (should be %#x)\n",
			dst_start - BUFFER_PAD, src_start - BUFFER_PAD, size,
			pos, dst_buffer[dst_start + pos],
			src_buffer[src_start + pos]);
		exit(2);
	}

	pos = memchk(&dst_buffer[dst_end], INIT_BYTE, BUFFER_PAD);
	if (pos < BUFFER_PAD) {
		fprintf(stderr,
			"FAILED: memcpy(dst + %#zx, src + %#zx, %#zx) set byte at offset %#zx to %#x\n",
			dst_start - BUFFER_PAD, src_start - BUFFER_PAD, size,
			size + pos, dst_buffer[dst_end + pos]);
		exit(2);
	}
}

static void
memcpy_tests(void)
{
	size_t size, src_misalign, dst_misalign;
	printf("Testing memcpy...");
	for (size = 0; size <= MAX_SIZE; size++) {
		if ((size % 64) == 0) {
			printf("\n%#5zx: .", size);
		} else {
			printf(".");
		}
		for (dst_misalign = 0; dst_misalign < LARGE_ALIGN;
		     dst_misalign++) {
			for (src_misalign = 0; src_misalign < SMALL_ALIGN;
			     src_misalign++) {
				memcpy_test(size, src_misalign, dst_misalign);
			}
		}
	}
	printf("\nPASS\n");
}

static void
memmove_test(size_t size, ptrdiff_t overlap)
{
	// We assume here that memmove() is based on memcpy(), so we don't need
	// to re-test with different alignments; just different amounts of
	// overlap is enough.
	size_t src_start = BUFFER_PAD + overlap;
	size_t src_end	 = src_start + size;
	size_t dst_start = BUFFER_PAD;
	size_t dst_end	 = dst_start + size;
	size_t pos;

	// We tested memset first, so it should be safe to use it to clear
	// the destination buffer.
	memset(dst_buffer, INIT_BYTE, BUFFER_SIZE);

	// We also tested memcpy already, so it should be safe to use it to copy
	// some random bytes from the source buffer into the destination buffer
	// at the source location.
	memcpy(&dst_buffer[src_start], src_buffer, size);

	// Now move from the source location to the destination location, both
	// within the destination buffer.
	memmove(&dst_buffer[dst_start], &dst_buffer[src_start], size);

	size_t start = (overlap > 0) ? dst_start : src_start;
	pos	     = memchk(dst_buffer, INIT_BYTE, start);
	if (pos < start) {
		fprintf(stderr,
			"FAILED: memmove(dst, dst + %td, %#zx) set byte at dst + %td to %#x (1)\n",
			overlap, size, dst_start - pos, dst_buffer[pos]);
		exit(2);
	}

	pos = memcmpchk(&dst_buffer[dst_start], src_buffer, size);
	if (pos < size) {
		fprintf(stderr,
			"FAILED: memmove(dst, dst + %td, %#zx) set byte at dst + %#zx to %#x (should be %#x) (2)\n",
			overlap, size, pos, dst_buffer[dst_start + pos],
			src_buffer[pos]);
		exit(2);
	}

	if ((overlap > 0) && (size > overlap)) {
		pos = memcmpchk(&dst_buffer[dst_end],
				&src_buffer[size - overlap], overlap);
		if (pos < overlap) {
			fprintf(stderr,
				"FAILED: memmove(dst, dst + %td, %#zx) set byte at dst + %td to %#x (3a, should be %#x, %#zx, %#zx)\n",
				overlap, size, size + pos,
				dst_buffer[dst_end + pos],
				src_buffer[size - overlap + pos], pos, overlap);
			exit(2);
		}
	} else if ((overlap < 0) && (size > -overlap)) {
		pos = memcmpchk(&dst_buffer[src_start], src_buffer, -overlap);
		if (pos < -overlap) {
			fprintf(stderr,
				"FAILED: memmove(dst, dst + %td, %#zx) set byte at dst + %td to %#x (3b, should be %#x, %#zx, %#zx)\n",
				overlap, size, overlap + pos,
				dst_buffer[src_start + pos], src_buffer[pos],
				pos, overlap);
			exit(2);
		}
	}

	size_t end = (overlap > 0) ? src_end : dst_end;
	size_t cnt = BUFFER_SIZE - end;
	pos	   = memchk(&dst_buffer[end], INIT_BYTE, cnt);
	if (pos < cnt) {
		fprintf(stderr,
			"FAILED: memmove(dst, dst + %td, %#zx) set byte at dst + %#zx to %#x (4)\n",
			overlap, size, end - dst_start, dst_buffer[end + pos]);
		exit(2);
	}
}

void
memmove_tests(void)
{
	size_t	  size;
	ptrdiff_t overlap;
	printf("Testing memmove...");

	for (size = 0; size <= MAX_SIZE; size++) {
		if ((size % 64) == 0) {
			printf("\n%#5zx: .", size);
		} else {
			printf(".");
		}
		static_assert(BUFFER_PAD <= (2 * LARGE_ALIGN),
			      "Buffer padding too small");
		for (overlap = (-2 * LARGE_ALIGN); overlap <= (2 * LARGE_ALIGN);
		     overlap++) {
			if (overlap == 0) {
				continue;
			}
			memmove_test(size, overlap);
		}
	}
}

int
main(void)
{
	uint64_t dczid;
	__asm__("mrs	%0, dczid_el0" : "=r"(dczid));

	if (dczid != CPU_DCZVA_BITS - 2) {
		fprintf(stderr,
			"ERROR: Unexpected DC ZVA ID: %#x (expected %#x)\n",
			(unsigned int)dczid, CPU_DCZVA_BITS - 2);
		return 1;
	}

	memset_tests();

	for (size_t i = 0; i < BUFFER_SIZE; i++) {
		src_buffer[i] = (uint8_t)random();
	}

	memcpy_tests();

	memmove_tests();

	return 0;
}

// Allow slow memmove() calls from libc.
extern char memcpy_bytes_is_defined_only_in_test_programs;
char	    memcpy_bytes_is_defined_only_in_test_programs;

```

`hyp/core/util/util.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

define refcount structure {
	count uint32(atomic);
};

extend trace_id enumeration {
	PANIC = 10;
	ASSERT_FAILED = 11;
};

```

`hyp/core/vdevice/aarch64/src/vdevice.c`:

```c
// © 2022 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>

#include <addrspace.h>
#include <atomic.h>
#include <compiler.h>
#include <rcu.h>
#include <thread.h>
#include <util.h>
#include <vcpu.h>

#include "event_handlers.h"
#include "internal.h"

vcpu_trap_result_t
vdevice_handle_vcpu_trap_data_abort_guest(ESR_EL2_t esr, vmaddr_result_t ipa,
					  FAR_EL2_t far)
{
	vcpu_trap_result_t ret	  = VCPU_TRAP_RESULT_UNHANDLED;
	register_t	   val	  = 0U;
	thread_t	  *thread = thread_get_self();

	ESR_EL2_ISS_DATA_ABORT_t iss =
		ESR_EL2_ISS_DATA_ABORT_cast(ESR_EL2_get_ISS(&esr));

	if (ESR_EL2_ISS_DATA_ABORT_get_ISV(&iss)) {
		bool is_write		= ESR_EL2_ISS_DATA_ABORT_get_WnR(&iss);
		bool is_acquire_release = ESR_EL2_ISS_DATA_ABORT_get_AR(&iss);
		iss_da_sas_t	sas	= ESR_EL2_ISS_DATA_ABORT_get_SAS(&iss);
		size_t		size	= (size_t)util_bit((count_t)sas);
		uint8_t		reg_num = ESR_EL2_ISS_DATA_ABORT_get_SRT(&iss);
		iss_da_ia_fsc_t fsc	= ESR_EL2_ISS_DATA_ABORT_get_DFSC(&iss);

		// ISV is not meaningful for a S1 page table walk fault
		assert(!ESR_EL2_ISS_DATA_ABORT_get_S1PTW(&iss));

		if (is_write) {
			val = vcpu_gpr_read(thread, reg_num);

			if (is_acquire_release) {
				atomic_thread_fence(memory_order_release);
			}
		}

		// Only translation and permission faults are considered for
		// vdevice access.
		if ((fsc == ISS_DA_IA_FSC_PERMISSION_1) ||
		    (fsc == ISS_DA_IA_FSC_PERMISSION_2) ||
		    (fsc == ISS_DA_IA_FSC_PERMISSION_3)) {
			// A permission fault may be a vdevice associated with
			// a physical address with a read-only mapping. Since
			// the IPA is not valid for permission faults, we must
			// look up the physical address from the faulting VA.
			rcu_read_start();
			gvaddr_t       va = FAR_EL2_get_VirtualAddress(&far);
			paddr_result_t paddr_res = addrspace_va_to_pa_read(va);

			// The lookup can fail if the guest unmapped or remapped
			// the faulting VA in stage 1 on another CPU after the
			// stage 2 fault was triggered. In that case, we must
			// retry the faulting instruction.
			if (paddr_res.e != OK) {
				ret = VCPU_TRAP_RESULT_RETRY;
			} else {
				ret = vdevice_access_phys(paddr_res.r, size,
							  &val, is_write);
			}
			rcu_read_finish();
		} else if ((fsc == ISS_DA_IA_FSC_TRANSLATION_0) ||
			   (fsc == ISS_DA_IA_FSC_TRANSLATION_1) ||
			   (fsc == ISS_DA_IA_FSC_TRANSLATION_2) ||
			   (fsc == ISS_DA_IA_FSC_TRANSLATION_3)) {
			// A translation fault may be a vdevice associated with
			// an IPA, with no underlying physical memory. Note
			// that the IPA is always valid for a translation
			// fault.
			assert(ipa.e == OK);
			ret = vdevice_access_ipa(ipa.r, size, &val, is_write);
		} else {
			// Wrong fault type; not handled by this module
		}

		if (!is_write && (ret == VCPU_TRAP_RESULT_EMULATED)) {
			// Do we need to sign-extend the result?
			if (ESR_EL2_ISS_DATA_ABORT_get_SSE(&iss) &&
			    (size != sizeof(uint64_t))) {
				uint64_t mask = util_bit((size * 8U) - 1U);
				val	      = (val ^ mask) - mask;
			}

			// Adjust the width if necessary
			if (!ESR_EL2_ISS_DATA_ABORT_get_SF(&iss)) {
				val = (uint32_t)val;
			}

			vcpu_gpr_write(thread, reg_num, val);

			if (is_acquire_release) {
				atomic_thread_fence(memory_order_acquire);
			}
		}
	}

	return ret;
}

```

`hyp/core/vdevice/aarch64/vdevice_aarch64.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

module vdevice

subscribe vcpu_trap_data_abort_guest(esr, ipa, far)

```

`hyp/core/vdevice/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

base_module hyp/misc/gpt
interface vdevice
types vdevice.tc
events vdevice.ev
local_include
source vdevice.c access.c
arch_events aarch64 vdevice_aarch64.ev
arch_source aarch64 vdevice.c

```

`hyp/core/vdevice/include/internal.h`:

```h
// © 2022 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// Emulate an access to a virtual device backed by physical memory.
//
// This function looks up the given physical address in the memdb, finds the
// corresponding memextent object, and checks whether it is associated with a
// virtual device. If so, it triggers the access event.
//
// This function is intended to be called from a permission fault handler after
// obtaining the physical address from the guest's address space. Since the
// address space might be concurrently modified to unmap the physical address,
// this must be called from an RCU critical section to ensure that the physical
// address is not reused before it has finished.
vcpu_trap_result_t
vdevice_access_phys(paddr_t pa, size_t size, register_t *val, bool is_write)
	REQUIRE_RCU_READ;

// Emulate an access to a virtual device that is not backed by physical memory.
//
// This function looks up the IPA in the current guest address space's virtual
// device mapping. If a virtual device is found, the access event will be
// triggered.
//
// This function is intended to be called from a translation fault handler.
vcpu_trap_result_t
vdevice_access_ipa(vmaddr_t ipa, size_t size, register_t *val, bool is_write);

```

`hyp/core/vdevice/src/access.c`:

```c
// © 2022 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>

#include <addrspace.h>
#include <atomic.h>
#include <gpt.h>
#include <memdb.h>
#include <memextent.h>
#include <rcu.h>

#include <events/vdevice.h>

#include "internal.h"

vcpu_trap_result_t
vdevice_access_phys(paddr_t pa, size_t size, register_t *val, bool is_write)
{
	vcpu_trap_result_t ret;

	memdb_obj_type_result_t res = memdb_lookup(pa);
	if ((res.e != OK) || (res.r.type != MEMDB_TYPE_EXTENT)) {
		ret = VCPU_TRAP_RESULT_UNHANDLED;
		goto out;
	}

	memextent_t *me = (memextent_t *)res.r.object;
	assert(me != NULL);
	vdevice_t *vdevice = atomic_load_consume(&me->vdevice);
	if (vdevice == NULL) {
		ret = VCPU_TRAP_RESULT_UNHANDLED;
		goto out;
	}

	size_result_t offset_r = memextent_get_offset_for_pa(me, pa, size);
	if (offset_r.e != OK) {
		ret = VCPU_TRAP_RESULT_UNHANDLED;
		goto out;
	}

	ret = trigger_vdevice_access_event(vdevice->type, vdevice, offset_r.r,
					   size, val, is_write);

out:
	return ret;
}

vcpu_trap_result_t
vdevice_access_ipa(vmaddr_t ipa, size_t size, register_t *val, bool is_write)
{
	vcpu_trap_result_t ret;

	addrspace_t *addrspace = addrspace_get_self();
	assert(addrspace != NULL);

	rcu_read_start();

	gpt_lookup_result_t lookup_ret =
		gpt_lookup(&addrspace->vdevice_gpt, ipa, size);

	if (lookup_ret.size != size) {
		ret = VCPU_TRAP_RESULT_UNHANDLED;
	} else if (lookup_ret.entry.type == GPT_TYPE_VDEVICE) {
		vdevice_t *vdevice = lookup_ret.entry.value.vdevice;
		assert(vdevice != NULL);
		assert((ipa >= vdevice->ipa) &&
		       ((ipa + size - 1U) <=
			(vdevice->ipa + vdevice->size - 1U)));

		ret = trigger_vdevice_access_event(vdevice->type, vdevice,
						   ipa - vdevice->ipa, size,
						   val, is_write);
	} else {
		assert(lookup_ret.entry.type == GPT_TYPE_EMPTY);

		// FIXME:
		ret = trigger_vdevice_access_fixed_addr_event(ipa, size, val,
							      is_write);
	}

	rcu_read_finish();

	return ret;
}

```

`hyp/core/vdevice/src/vdevice.c`:

```c
// © 2022 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>

#include <atomic.h>
#include <gpt.h>
#include <object.h>
#include <spinlock.h>
#include <util.h>
#include <vdevice.h>

#include "event_handlers.h"

error_t
vdevice_attach_phys(vdevice_t *vdevice, memextent_t *memextent)
{
	assert(vdevice != NULL);
	assert(memextent != NULL);
	assert(vdevice->type != VDEVICE_TYPE_NONE);

	vdevice_t *null_vdevice = NULL;
	return atomic_compare_exchange_strong_explicit(&memextent->vdevice,
						       &null_vdevice, vdevice,
						       memory_order_release,
						       memory_order_release)
		       ? OK
		       : ERROR_BUSY;
}

void
vdevice_detach_phys(vdevice_t *vdevice, memextent_t *memextent)
{
	vdevice_t *old_vdevice = atomic_exchange_explicit(
		&memextent->vdevice, NULL, memory_order_relaxed);
	assert(old_vdevice == vdevice);
}

bool
vdevice_handle_gpt_values_equal(gpt_type_t type, gpt_value_t x, gpt_value_t y)
{
	assert(type == GPT_TYPE_VDEVICE);

	return x.vdevice == y.vdevice;
}

error_t
vdevice_handle_object_create_addrspace(addrspace_create_t params)
{
	addrspace_t *addrspace = params.addrspace;
	assert(addrspace != NULL);

	spinlock_init(&addrspace->vdevice_lock);

	gpt_config_t config = gpt_config_default();
	gpt_config_set_max_bits(&config, VDEVICE_MAX_GPT_BITS);
	gpt_config_set_rcu_read(&config, true);

	return gpt_init(&addrspace->vdevice_gpt, addrspace->header.partition,
			config, util_bit((index_t)GPT_TYPE_VDEVICE));
}

void
vdevice_handle_object_cleanup_addrspace(addrspace_t *addrspace)
{
	assert(addrspace != NULL);

	gpt_destroy(&addrspace->vdevice_gpt);
}

error_t
vdevice_attach_vmaddr(vdevice_t *vdevice, addrspace_t *addrspace, vmaddr_t ipa,
		      size_t size)
{
	error_t err;

	assert(vdevice != NULL);
	assert(addrspace != NULL);
	assert(vdevice->type != VDEVICE_TYPE_NONE);

	if (vdevice->addrspace != NULL) {
		err = ERROR_BUSY;
		goto out;
	}

	gpt_entry_t entry = {
		.type  = GPT_TYPE_VDEVICE,
		.value = { .vdevice = vdevice },
	};

	spinlock_acquire(&addrspace->vdevice_lock);

	err = gpt_insert(&addrspace->vdevice_gpt, ipa, size, entry, true);

	spinlock_release(&addrspace->vdevice_lock);

	if (err == OK) {
		vdevice->addrspace = object_get_addrspace_additional(addrspace);
		vdevice->ipa	   = ipa;
		vdevice->size	   = size;
	}

out:
	return err;
}

void
vdevice_detach_vmaddr(vdevice_t *vdevice)
{
	assert(vdevice != NULL);
	assert(vdevice->type != VDEVICE_TYPE_NONE);

	addrspace_t *addrspace = vdevice->addrspace;
	assert(addrspace != NULL);

	gpt_entry_t entry = {
		.type  = GPT_TYPE_VDEVICE,
		.value = { .vdevice = vdevice },
	};

	spinlock_acquire(&addrspace->vdevice_lock);

	error_t err = gpt_remove(&addrspace->vdevice_gpt, vdevice->ipa,
				 vdevice->size, entry);
	assert(err == OK);

	spinlock_release(&addrspace->vdevice_lock);

	vdevice->addrspace = NULL;

	object_put_addrspace(addrspace);
}

```

`hyp/core/vdevice/vdevice.ev`:

```ev
// © 2022 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

module vdevice

subscribe gpt_values_equal[GPT_TYPE_VDEVICE]

subscribe object_create_addrspace
	unwinder

subscribe object_cleanup_addrspace(addrspace)

```

`hyp/core/vdevice/vdevice.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// FIXME: Get this from the addrspace instead?
define VDEVICE_MAX_GPT_BITS constant type count_t = 44;

extend memextent object {
	vdevice		pointer(atomic) structure vdevice;
};

extend vdevice structure {
	addrspace	pointer object addrspace;
	ipa		type vmaddr_t;
	size		size;
};

extend addrspace object {
	vdevice_gpt	structure gpt;
	vdevice_lock	structure spinlock;
};

extend gpt_type enumeration {
	vdevice;
};

extend gpt_value union {
	vdevice		pointer structure vdevice;
};

```

`hyp/core/vectors/aarch64/include/trap_dispatch.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

void
vectors_exception_dispatch(kernel_trap_frame_full_t *frame)
	REQUIRE_PREEMPT_DISABLED;

SPSR_EL2_A64_t
vectors_interrupt_dispatch(void) REQUIRE_PREEMPT_DISABLED;

void
vectors_dump_regs(kernel_trap_frame_full_t *frame);

```

`hyp/core/vectors/aarch64/include/vectors_el2.inc`:

```inc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// Size of the per-CPU memory reserved for nested fault context buffer and stack
#if !defined(NDEBUG)
#define NESTED_FAULT_STACK_SIZE_BITS 11		// 2048 bytes
#else
#define NESTED_FAULT_STACK_SIZE_BITS 10		// 1024 bytes
#endif
#define NESTED_FAULT_STACK_SIZE (1 << NESTED_FAULT_STACK_SIZE_BITS)

.macro	disable_phys_access
#if ARCH_AARCH64_USE_PAN
	// Ensure that accesses to 1:1 physical mappings are disabled. Note
	// that there's no way to have the CPU do this automatically for
	// traps from EL2 or a VCPU to EL2 (though it can be done with VHE for
	// traps from EL0 processes to EL2, when E2H=1 and TGE=1)
	msr	PAN, 1
#else
	// Phys access isn't controlled by the PAN bit; nothing to do here.
#endif
.endm

#if defined(ARCH_ARM_FEAT_PAuth)
// Define symbols for pointer auth offsets so we can access them from macros
.equ	pauth_DA_ofs, OFS_AARCH64_PAUTH_KEYS_DA
.equ	pauth_DB_ofs, OFS_AARCH64_PAUTH_KEYS_DB
.equ	pauth_IA_ofs, OFS_AARCH64_PAUTH_KEYS_IA
.equ	pauth_IB_ofs, OFS_AARCH64_PAUTH_KEYS_IB
.equ	pauth_GA_ofs, OFS_AARCH64_PAUTH_KEYS_GA

// Macro for loading an EL2 key
.macro  kernel_pauth_entry_key k:req, kp:req, kl:req, kh:req
	ldp	\kl, \kh, [\kp, pauth_\k\()_ofs]
	msr	AP\k\()KeyLo_EL1, \kl
	msr	AP\k\()KeyHi_EL1, \kh
.endm
#endif

// Macro for restoring the EL2 pointer auth keys (discarding any EL1 keys)
.macro kernel_pauth_entry kp:req, kl:req, kh:req
#if defined(ARCH_ARM_FEAT_PAuth)
	adrl	\kp, aarch64_pauth_keys
	kernel_pauth_entry_key DA, \kp, \kl, \kh
	kernel_pauth_entry_key DB, \kp, \kl, \kh
	kernel_pauth_entry_key IA, \kp, \kl, \kh
	kernel_pauth_entry_key IB, \kp, \kl, \kh
	kernel_pauth_entry_key GA, \kp, \kl, \kh
	isb
#endif
.endm

.macro	check_stack_overflow
	// Kernel stacks are aligned to twice their maximum allowed size.
	// This means valid stack accesses will never have the bit at
	// THREAD_STACK_MAX_SIZE set, otherwise we have a stack overflow.
	// We must temporarily swap SP and X0 in order to test this.
	sub	sp, sp, x0
	add	x0, sp, x0
	tst	x0, THREAD_STACK_MAX_SIZE
	sub	x0, sp, x0
	sub	sp, sp, x0
	neg	x0, x0
	b.ne	handle_stack_fault
.endm

.macro	save_kernel_context full:req overflow:req
	disable_phys_access

.if \full
	// SP (SP_EL2) points to the kernel stack in the thread structure
	// Skip 256 bytes of stack so we don't clobber any stack history useful
	// for debugging the exception.
	sub	sp, sp, KERNEL_TRAP_FRAME_FULL_SIZE + 0x100
.else
	// SP (SP_EL2) points to the kernel stack in the thread structure
	sub	sp, sp, KERNEL_TRAP_FRAME_SIZE
.endif

.if \overflow
	// The exception may have been triggered by a stack overflow,
	// so check for this before storing anything on the stack.
	check_stack_overflow
.endif

	stp	x0, x1, [sp, OFS_KERNEL_TRAP_FRAME_X(0)]
	mrs	x0, ELR_EL2
	stp	x2, x3, [sp, OFS_KERNEL_TRAP_FRAME_X(2)]
	// Recalculate the SP from before we pushed the frame. Note that we
	// can't do a regular PACIASP before changing SP because we need to
	// save a register to load ELR_EL2 into first, and doing that before
	// updating SP risks an infinite loop if the store faults.
.if \full
	add	x2, sp, KERNEL_TRAP_FRAME_FULL_SIZE + 0x100
.else
	add	x2, sp, KERNEL_TRAP_FRAME_SIZE
.endif

	stp	x4, x5, [sp, OFS_KERNEL_TRAP_FRAME_X(4)]
#if defined(ARCH_ARM_FEAT_PAuth)
	// Insert PAC bits in ELR_EL2 before we save it (matching the ERETAA
	// in vectors_kernel_return)
	pacia	x0, x2
#endif
	stp	x6, x7, [sp, OFS_KERNEL_TRAP_FRAME_X(6)]
	mrs	x1, SPSR_EL2
	stp	x8, x9, [sp, OFS_KERNEL_TRAP_FRAME_X(8)]
	stp	x10, x11, [sp, OFS_KERNEL_TRAP_FRAME_X(10)]
	stp	x12, x13, [sp, OFS_KERNEL_TRAP_FRAME_X(12)]
	stp	x14, x15, [sp, OFS_KERNEL_TRAP_FRAME_X(14)]
	stp	x16, x17, [sp, OFS_KERNEL_TRAP_FRAME_X(16)]
#if ((OFS_KERNEL_TRAP_FRAME_X30 + 8) != OFS_KERNEL_TRAP_FRAME_SPSR_EL2) || \
	((OFS_KERNEL_TRAP_FRAME_X29 + 8) != OFS_KERNEL_TRAP_FRAME_PC) || \
	((OFS_KERNEL_TRAP_FRAME_X(18) + 8) != OFS_KERNEL_TRAP_FRAME_SP_EL2)
#error The layout of kernel_trap_frame structure has changed
#endif
	// Save X18 and original SP_EL2
	stp	x18, x2, [sp, OFS_KERNEL_TRAP_FRAME_X(18)]
	stp	x29, x0, [sp, OFS_KERNEL_TRAP_FRAME_X29]
	add	x29, sp, OFS_KERNEL_TRAP_FRAME_X29
	stp	x30, x1, [sp, OFS_KERNEL_TRAP_FRAME_X30]

	// The callee-saved registers (X19-X28) are not saved here. If any
	// assembly code after this point wants to modify any of these
	// registers, it will need to save them first.
.endm

// Use this macro ONLY if jumping to panic or the kernel debugger afterwards.
// There is no function that can restore this frame.
.macro	save_kernel_context_full
	disable_phys_access

	// SP (SP_EL2) points to the kernel stack
	sub	sp, sp, KERNEL_TRAP_FRAME_FULL_SIZE

	stp	x0, x1, [sp, OFS_KERNEL_TRAP_FRAME_FULL_BASE_X(0)]
	stp	x2, x3, [sp, OFS_KERNEL_TRAP_FRAME_FULL_BASE_X(2)]
	stp	x4, x5, [sp, OFS_KERNEL_TRAP_FRAME_FULL_BASE_X(4)]
	stp	x6, x7, [sp, OFS_KERNEL_TRAP_FRAME_FULL_BASE_X(6)]
	stp	x8, x9, [sp, OFS_KERNEL_TRAP_FRAME_FULL_BASE_X(8)]
	add	x0, sp, KERNEL_TRAP_FRAME_FULL_SIZE
	stp	x10, x11, [sp, OFS_KERNEL_TRAP_FRAME_FULL_BASE_X(10)]
	stp	x12, x13, [sp, OFS_KERNEL_TRAP_FRAME_FULL_BASE_X(12)]
	mrs	x1, ELR_EL2
	stp	x14, x15, [sp, OFS_KERNEL_TRAP_FRAME_FULL_BASE_X(14)]
	stp	x16, x17, [sp, OFS_KERNEL_TRAP_FRAME_FULL_BASE_X(16)]
	mrs	x2, SPSR_EL2
#if ((OFS_KERNEL_TRAP_FRAME_FULL_BASE_X30 + 8) != OFS_KERNEL_TRAP_FRAME_FULL_BASE_SPSR_EL2) || \
	((OFS_KERNEL_TRAP_FRAME_FULL_BASE_X29 + 8) != OFS_KERNEL_TRAP_FRAME_FULL_BASE_PC) || \
	((OFS_KERNEL_TRAP_FRAME_FULL_BASE_X(18) + 8) != OFS_KERNEL_TRAP_FRAME_FULL_BASE_SP_EL2)
#error The layout of kernel_trap_frame structure has changed
#endif
	stp	x18, x0, [sp, OFS_KERNEL_TRAP_FRAME_FULL_BASE_X(18)]
	stp	x29, x1, [sp, OFS_KERNEL_TRAP_FRAME_FULL_BASE_X29]
	stp	x30, x2, [sp, OFS_KERNEL_TRAP_FRAME_FULL_BASE_X30]
	add	x29, sp, OFS_KERNEL_TRAP_FRAME_FULL_BASE_X29

	stp	x19, x20, [sp, OFS_KERNEL_TRAP_FRAME_FULL_X19]
	stp	x21, x22, [sp, OFS_KERNEL_TRAP_FRAME_FULL_X21]
	stp	x23, x24, [sp, OFS_KERNEL_TRAP_FRAME_FULL_X23]
	stp	x25, x26, [sp, OFS_KERNEL_TRAP_FRAME_FULL_X25]
	stp	x27, x28, [sp, OFS_KERNEL_TRAP_FRAME_FULL_X27]
.endm

// For the nested faults the stack may be corrupted, so we switch to a special
// stack set aside for this purpose.
.macro	save_kernel_context_stack_fault
	// At this point we don't care about preserving the value of TPIDR_EL0
	// and TPIDR_EL1
	msr	TPIDR_EL0, x0
	msr	TPIDR_EL1, x1

	// The emergency stacks must use the emergency vectors.
	adr	x0, emergency_vectors_aarch64
	msr	VBAR_EL2, x0
	isb

	// Get the CPU number and use it to calculate the address of the nested
	// fault context buffer for this CPU
	adr_threadlocal	x1, current_thread + OFS_THREAD_CPULOCAL_CURRENT_CPU
	ldrh	w1, [x1]

	adrl	x0, aarch64_emergency_stacks

	// X0 points to the special buffer we have allocated for nested faults,
	// and X1 is the CPU number of this core. The bottom of the stack for
	// this core is X0 + (X1 * NESTED_FAULT_STACK_SIZE).
	add	x0, x0, x1, lsl NESTED_FAULT_STACK_SIZE_BITS

	// Top of the stack is NESTED_FAULT_STACK_SIZE bytes higher. We also
	// need to subtract CONTEXT_REG_FRAME_SIZE_ALIGNED to cater for what we
	// are going to put on the stack. Do both in the same instruction.
	// Don't modify sp yet because we want to save it in the frame.
	add	x0, x0, NESTED_FAULT_STACK_SIZE - KERNEL_TRAP_FRAME_FULL_SIZE

	// Push a few extra registers
	stp	x2, x3, [x0, OFS_KERNEL_TRAP_FRAME_FULL_BASE_X(2)]

	// Restore the stashed x0 and x1 and push them too
	mrs	x2, TPIDR_EL0
	mrs	x3, TPIDR_EL1
	stp	x2, x3, [x0, OFS_KERNEL_TRAP_FRAME_FULL_BASE_X(0)]

	// Stash the faulting stack and update sp
	mov	x3, sp
	mov	sp, x0

	// Push everything else
	stp	x4, x5, [sp, OFS_KERNEL_TRAP_FRAME_FULL_BASE_X(4)]
	stp	x6, x7, [sp, OFS_KERNEL_TRAP_FRAME_FULL_BASE_X(6)]
	stp	x8, x9, [sp, OFS_KERNEL_TRAP_FRAME_FULL_BASE_X(8)]
	stp	x10, x11, [sp, OFS_KERNEL_TRAP_FRAME_FULL_BASE_X(10)]
	stp	x12, x13, [sp, OFS_KERNEL_TRAP_FRAME_FULL_BASE_X(12)]
	mrs	x1, ELR_EL2
	stp	x14, x15, [sp, OFS_KERNEL_TRAP_FRAME_FULL_BASE_X(14)]
	stp	x16, x17, [sp, OFS_KERNEL_TRAP_FRAME_FULL_BASE_X(16)]
	mrs	x2, SPSR_EL2
#if ((OFS_KERNEL_TRAP_FRAME_FULL_BASE_X30 + 8) != OFS_KERNEL_TRAP_FRAME_FULL_BASE_SPSR_EL2) || \
	((OFS_KERNEL_TRAP_FRAME_FULL_BASE_X29 + 8) != OFS_KERNEL_TRAP_FRAME_FULL_BASE_PC) || \
	((OFS_KERNEL_TRAP_FRAME_FULL_BASE_X(18) + 8) != OFS_KERNEL_TRAP_FRAME_FULL_BASE_SP_EL2)
#error The layout of kernel_trap_frame structure has changed
#endif
	stp	x18, x3, [sp, OFS_KERNEL_TRAP_FRAME_FULL_BASE_X(18)]
	stp	x29, x1, [sp, OFS_KERNEL_TRAP_FRAME_FULL_BASE_X29]
	stp	x30, x2, [sp, OFS_KERNEL_TRAP_FRAME_FULL_BASE_X30]
	add	x29, sp, OFS_KERNEL_TRAP_FRAME_FULL_BASE_X29

	stp	x19, x20, [sp, OFS_KERNEL_TRAP_FRAME_FULL_X19]
	stp	x21, x22, [sp, OFS_KERNEL_TRAP_FRAME_FULL_X21]
	stp	x23, x24, [sp, OFS_KERNEL_TRAP_FRAME_FULL_X23]
	stp	x25, x26, [sp, OFS_KERNEL_TRAP_FRAME_FULL_X25]
	stp	x27, x28, [sp, OFS_KERNEL_TRAP_FRAME_FULL_X27]
.endm

.macro thread_get_self, reg:req, tls_base, offset=0
.ifb \tls_base
	adr_threadlocal \reg, (current_thread + \offset)
.else
	adr_threadlocal \reg, (current_thread + \offset), \tls_base
.endif
.endm

// Hypervisor self vectors
.macro el2_vectors name:req, overflow:req
	vector vector_el2t_sync_\name\()
		save_kernel_context_full
		mov	x0, sp
		bl	dump_self_sync_fault
		panic	"EL2t vectors"
	vector_end vector_el2t_sync_\name\()

	vector vector_el2t_irq_\name\()
		save_kernel_context_full
		mov	x0, sp
		bl	dump_self_irq_fault
		panic	"EL2t vectors"
	vector_end vector_el2t_irq_\name\()

	vector vector_el2t_fiq_\name\()
		save_kernel_context_full
		mov	x0, sp
		bl	dump_self_fiq_fault
		panic	"EL2t vectors"
	vector_end vector_el2t_fiq_\name\()

	vector vector_el2t_serror_\name\()
		save_kernel_context_full
		mov	x0, sp
		bl	dump_self_serror
		panic	"EL2t vectors"
	vector_end vector_el2t_serror_\name\()


	// Hypervisor nested vectors
	vector vector_el2h_sync_\name\()
		save_kernel_context 1 \overflow

		mov	x0, sp
		bl	vectors_exception_dispatch_full
		mov	x0, 0
		b	vectors_kernel_return
	vector_end vector_el2h_sync_\name\()

	vector vector_el2h_irq_\name\()
		save_kernel_context 0 0

		bl	vectors_interrupt_dispatch
		b	vectors_kernel_return
	vector_end vector_el2h_irq_\name\()

	vector vector_el2h_fiq_\name\()
		// In current implementations, all FIQs should go directly to
		// TrustZone and thus if we ever get one, panic.
		save_kernel_context_full

		panic	"EL2h vectors"
	vector_end vector_el2h_fiq_\name\()

	vector vector_el2h_serror_\name\()
		save_kernel_context 1 0

		// The dispatcher will inject a virtual SError to the RAS VM.
		mov	x0, sp
		bl	vectors_exception_dispatch_full
		mov	x0, 0
		b	vectors_kernel_return
	vector_end vector_el2h_serror_\name\()
.endm

.macro default_guest_vectors name:req
	vector vector_guest64_sync_\name\()
		save_kernel_context_full
		panic	"64-bit guest vectors"
	vector_end vector_guest64_sync_\name\()

	vector vector_guest64_irq_\name\()
		save_kernel_context_full
		panic	"64-bit guest vectors"
	vector_end vector_guest64_irq_\name\()

	vector vector_guest64_fiq_\name\()
		save_kernel_context_full
		panic	"64-bit guest vectors"
	vector_end vector_guest64_fiq_\name\()

	vector vector_guest64_serror_\name\()
		save_kernel_context_full
		panic	"64-bit guest vectors"
	vector_end vector_guest64_serror_\name\()

	vector vector_guest32_sync_\name\()
		save_kernel_context_full
		panic	"32-bit guest vectors"
	vector_end vector_guest32_sync_\name\()

	vector vector_guest32_irq_\name\()
		save_kernel_context_full
		panic	"32-bit guest vectors"
	vector_end vector_guest32_irq_\name\()

	vector vector_guest32_fiq_\name\()
		save_kernel_context_full
		panic	"32-bit guest vectors"
	vector_end vector_guest32_fiq_\name\()

	vector vector_guest32_serror_\name\()
		save_kernel_context_full
		panic	"32-bit guest vectors"
	vector_end vector_guest32_serror_\name\()
.endm

.macro default_vectors name:req, overflow:req
	el2_vectors \name \overflow
	default_guest_vectors \name
.endm

```

`hyp/core/vectors/aarch64/src/exception_debug.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <hyptypes.h>

#include <hypregisters.h>

#include <compiler.h>
#include <log.h>
#include <trace.h>
#include <util.h>

#include <asm/barrier.h>

#include "trap_dispatch.h"

void
dump_self_sync_fault(kernel_trap_frame_full_t *frame);

void
dump_self_irq_fault(kernel_trap_frame_full_t *frame);

void
dump_self_fiq_fault(kernel_trap_frame_full_t *frame);

void
dump_self_serror(kernel_trap_frame_full_t *frame);

void
dump_nested_fault(kernel_trap_frame_full_t *frame);

void
vectors_dump_regs(kernel_trap_frame_full_t *frame)
{
	TRACE_AND_LOG(ERROR, INFO, "Dumping frame at {:#x}", (uintptr_t)frame);
	for (index_t i = 0; i < util_array_size(frame->base.x); i++) {
		TRACE_AND_LOG(ERROR, INFO, "X{:d} = {:#x}", i,
			      frame->base.x[i]);
	}
	TRACE_AND_LOG(ERROR, INFO, "X19 = {:#x}", frame->x19);
	TRACE_AND_LOG(ERROR, INFO, "X20 = {:#x}", frame->x20);
	TRACE_AND_LOG(ERROR, INFO, "X21 = {:#x}", frame->x21);
	TRACE_AND_LOG(ERROR, INFO, "X22 = {:#x}", frame->x22);
	TRACE_AND_LOG(ERROR, INFO, "X23 = {:#x}", frame->x23);
	TRACE_AND_LOG(ERROR, INFO, "X24 = {:#x}", frame->x24);
	TRACE_AND_LOG(ERROR, INFO, "X25 = {:#x}", frame->x25);
	TRACE_AND_LOG(ERROR, INFO, "X26 = {:#x}", frame->x26);
	TRACE_AND_LOG(ERROR, INFO, "X27 = {:#x}", frame->x27);
	TRACE_AND_LOG(ERROR, INFO, "X28 = {:#x}", frame->x28);
	TRACE_AND_LOG(ERROR, INFO, "X29 = {:#x}", frame->base.x29);
	TRACE_AND_LOG(ERROR, INFO, "X30 = {:#x}", frame->base.x30);

	TRACE_AND_LOG(ERROR, INFO, "SP_EL2 = {:#x}",
		      SP_EL2_raw(frame->base.sp_el2));
	TRACE_AND_LOG(ERROR, INFO, "ELR_EL2 = {:#x}",
		      ELR_EL2_raw(frame->base.pc));
	TRACE_AND_LOG(ERROR, INFO, "SPSR_EL2 = {:#x}",
		      SPSR_EL2_A64_raw(frame->base.spsr_el2));

	ESR_EL2_t esr = register_ESR_EL2_read_ordered(&asm_ordering);
	TRACE_AND_LOG(ERROR, INFO, "ESR_EL2 = {:#x}", ESR_EL2_raw(esr));
}

void
dump_self_sync_fault(kernel_trap_frame_full_t *frame)
{
	TRACE_AND_LOG(ERROR, WARN, "EL2t synchronous fault");
	vectors_dump_regs(frame);

	// FIXME:
}

void
dump_self_irq_fault(kernel_trap_frame_full_t *frame)
{
	TRACE_AND_LOG(ERROR, WARN, "EL2t IRQ");
	vectors_dump_regs(frame);

	// FIXME:
}

void
dump_self_fiq_fault(kernel_trap_frame_full_t *frame)
{
	TRACE_AND_LOG(ERROR, WARN, "EL2t FIQ fault");
	vectors_dump_regs(frame);

	// FIXME:
}

void
dump_self_serror(kernel_trap_frame_full_t *frame)
{
	TRACE_AND_LOG(ERROR, WARN, "EL2t SError fault");
	vectors_dump_regs(frame);

	// FIXME:
}

void
dump_nested_fault(kernel_trap_frame_full_t *frame)
{
	TRACE_AND_LOG(ERROR, WARN, "EL2 stack fault");
	vectors_dump_regs(frame);

	// FIXME:
}

```

`hyp/core/vectors/aarch64/src/trap_dispatch.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>

#include <hypregisters.h>

#include <attributes.h>
#include <compiler.h>
#include <cpulocal.h>
#include <hyp_aspace.h>
#include <log.h>
#include <panic.h>
#include <preempt.h>
#include <thread.h>
#include <trace.h>
#include <util.h>

#include <events/vectors.h>

#include <asm/barrier.h>

#include "event_handlers.h"
#include "trap_dispatch.h"

#if defined(ARCH_ARM_FEAT_PAuth)
static inline uintptr_t
remove_pointer_auth(uintptr_t addr)
{
	__asm__("xpaci %0" : "+r"(addr));
	return addr;
}
#endif

static inline uintptr_t
vectors_get_return_address(kernel_trap_frame_t *frame)
{
#if defined(ARCH_ARM_FEAT_PAuth)
	return remove_pointer_auth(ELR_EL2_get_ReturnAddress(&frame->pc));
#else
	return ELR_EL2_get_ReturnAddress(&frame->pc);
#endif
}

#if defined(ARCH_ARM_FEAT_PAuth)
static inline ALWAYS_INLINE uintptr_t
sign_pc_using_framepointer(uintptr_t pc, uintptr_t fp)
{
	// The new PC needs to be signed with a modifier equal to the value the
	// SP will have after restoring the frame, i.e. the address immediately
	// after the end of the frame. Note that this must be inlined and BTI
	// must be enabled to avoid providing a gadget for signing an arbitrary
	// return address.
	__asm__("pacia %0, %1" : "+r"(pc) : "r"(fp));
	return pc;
}
#endif

static inline ALWAYS_INLINE void
vectors_set_return_address(kernel_trap_frame_t *frame, uintptr_t pc)
{
#if defined(ARCH_ARM_FEAT_PAuth)
	ELR_EL2_set_ReturnAddress(
		&frame->pc,
		sign_pc_using_framepointer(
			pc, (uintptr_t)(SP_EL2_raw(frame->sp_el2))));
#else
	ELR_EL2_set_ReturnAddress(&frame->pc, pc);
#endif
}

// Dispatching EL2h synchronous traps
void
vectors_exception_dispatch(kernel_trap_frame_full_t *frame)
{
	bool	    handled	    = false;
	bool	    is_memory_fault = false;
	cpu_index_t cpu		    = cpulocal_get_index();

	ESR_EL2_t esr = register_ESR_EL2_read_ordered(&asm_ordering);
	esr_ec_t  ec  = ESR_EL2_get_EC(&esr);
	uintptr_t pc  = ELR_EL2_get_ReturnAddress(&frame->base.pc);
#if defined(ARCH_ARM_FEAT_PAuth)
	pc = remove_pointer_auth(pc);
#endif
	TRACE(ERROR, WARN,
	      "EL2 exception at PC = {:x} ESR_EL2 = {:#x}, LR = {:#x}, "
	      "SP = {:#x}, FP = {:#x}",
	      pc, ESR_EL2_raw(esr), frame->base.x30,
	      SP_EL2_raw(frame->base.sp_el2), frame->base.x29);

	switch (ec) {
	case ESR_EC_UNKNOWN:
		handled = trigger_vectors_trap_unknown_el2_event(&frame->base);
		break;

#if defined(ARCH_ARM_FEAT_BTI)
	case ESR_EC_BTI:
		TRACE_AND_LOG(ERROR, WARN,
			      "BTI abort in EL2 on CPU {:d}, from {:#x}, "
			      "LR = {:#x}, ESR_EL2 = {:#x}",
			      cpu, pc, frame->base.x30, ESR_EL2_raw(esr));
		panic("BTI abort in EL2");
#endif

	case ESR_EC_ILLEGAL:
		handled = trigger_vectors_trap_illegal_state_el2_event();
		break;

	case ESR_EC_INST_ABT:
		is_memory_fault = true;
		handled		= trigger_vectors_trap_pf_abort_el2_event(esr);
		break;

	case ESR_EC_PC_ALIGN:
		is_memory_fault = true;
		handled = trigger_vectors_trap_pc_alignment_fault_el2_event();
		break;

	case ESR_EC_DATA_ABT:
		is_memory_fault = true;
		handled = trigger_vectors_trap_data_abort_el2_event(esr);
		break;

	case ESR_EC_SP_ALIGN:
		handled = trigger_vectors_trap_sp_alignment_fault_el2_event();
		break;

	case ESR_EC_SERROR:
		handled = preempt_abort_dispatch();
		break;

	case ESR_EC_BRK:
		handled = trigger_vectors_trap_brk_el2_event(esr);
		break;

#if defined(ARCH_ARM_FEAT_PAuth) && defined(ARCH_ARM_FEAT_FPAC)
	case ESR_EC_FPAC:
		handled = trigger_vectors_trap_pauth_failed_el2_event(esr);
		break;
#endif

	case ESR_EC_BREAK:
	case ESR_EC_BREAK_LO:
	case ESR_EC_STEP:
	case ESR_EC_STEP_LO:
	case ESR_EC_WATCH:
	case ESR_EC_WATCH_LO:
		panic("EL2 debug trap");
	case ESR_EC_VECTOR32_EL2:
	case ESR_EC_MCRMRC15:
	case ESR_EC_MCRRMRRC15:
	case ESR_EC_MCRMRC14:
	case ESR_EC_VMRS_EL2:
	case ESR_EC_MRRC14:
	case ESR_EC_LDCSTC:
	case ESR_EC_SVC32:
	case ESR_EC_HVC32_EL2:
	case ESR_EC_SMC32_EL2:
	case ESR_EC_FP32:
	case ESR_EC_BKPT:
	case ESR_EC_WFIWFE:
	case ESR_EC_FPEN:
	case ESR_EC_SVC64:
	case ESR_EC_HVC64_EL2:
	case ESR_EC_SMC64_EL2:
	case ESR_EC_INST_ABT_LO:
	case ESR_EC_DATA_ABT_LO:
	case ESR_EC_FP64:
#if defined(ARCH_ARM_FEAT_PAuth)
	case ESR_EC_PAUTH:
#endif
#if defined(ARCH_ARM_FEAT_PAuth) && defined(ARCH_ARM_FEAT_NV)
	case ESR_EC_ERET:
#endif
	case ESR_EC_SYSREG:
#if defined(ARCH_ARM_FEAT_SVE)
	case ESR_EC_SVE:
#endif
#if defined(ARCH_ARM_FEAT_LS64)
	case ESR_EC_LD64B_ST64B:
#endif
#if defined(ARCH_ARM_FEAT_TME)
	case ESR_EC_TSTART:
#endif
#if defined(ARCH_ARM_FEAT_SME)
	case ESR_EC_SME:
#endif
#if defined(ARCH_ARM_FEAT_RME)
	case ESR_EC_RME:
#endif
#if defined(ARCH_ARM_FEAT_MOPS)
	case ESR_EC_MOPS:
#endif
	default:
		// Unexpected trap, fall through to panic
		break;
	}

	if (!handled) {
		if (is_memory_fault) {
			FAR_EL2_t far =
				register_FAR_EL2_read_ordered(&asm_ordering);
			TRACE_AND_LOG(ERROR, WARN,
				      "Unhandled EL2 trap on CPU {:d}, "
				      "ESR_EL2 = {:#x}, ELR_EL2 = {:#x}, "
				      "FAR_EL2 = {:#x}",
				      cpu, ESR_EL2_raw(esr), pc,
				      FAR_EL2_raw(far));
		} else {
			TRACE_AND_LOG(ERROR, WARN,
				      "Unhandled EL2 trap on CPU {:d}, "
				      "ESR_EL2 = {:#x}, ELR_EL2 = {:#x}",
				      cpu, ESR_EL2_raw(esr), pc);
		}

		vectors_dump_regs(frame);
		panic("Unhandled EL2 trap");
	}
}

SPSR_EL2_A64_t
vectors_interrupt_dispatch(void)
{
	SPSR_EL2_A64_t ret = { 0 };

	if (preempt_interrupt_dispatch()) {
		SPSR_EL2_A64_set_I(&ret, true);
	}

	return ret;
}

void
vectors_handle_abort_kernel(void)
{
#if defined(VERBOSE) && VERBOSE
	// HLT instruction will stop if an external debugger is attached,
	// otherwise it generates an exception and the trap handler below will
	// skip the instruction.
#if !defined(QQVP_SIMULATION_PLATFORM) || !QQVP_SIMULATION_PLATFORM
	// The QQVP model exits with invalid instruction here.
	// This should be resolved in model.
	__asm__ volatile("hlt 1" ::: "memory");
#endif
#endif
}

bool
vectors_handle_vectors_trap_unknown_el2(kernel_trap_frame_t *frame)
{
	bool	  ret = false;
	uintptr_t pc  = vectors_get_return_address(frame);

	error_t err =
		hyp_aspace_va_to_pa_el2_read((void *)pc, NULL, NULL, NULL);

	if (err != OK) {
		LOG(ERROR, WARN, "EL2 undef instruction bad PC: {:x}", pc);
		goto out;
	}

	assert(util_is_baligned(pc, 4));

	// Read the faulting EL2 instruction
	uint32_t inst = *(uint32_t *)pc;

	if ((inst & AARCH64_INST_EXCEPTION_MASK) ==
	    AARCH64_INST_EXCEPTION_VAL) {
		uint16_t imm16 =
			(uint16_t)((inst & AARCH64_INST_EXCEPTION_IMM16_MASK) >>
				   AARCH64_INST_EXCEPTION_IMM16_SHIFT);

		if ((inst & AARCH64_INST_EXCEPTION_SUBTYPE_MASK) ==
		    AARCH64_INST_EXCEPTION_SUBTYPE_HLT_VAL) {
			LOG(ERROR, WARN,
			    "skipping hlt instruction at PC: {:x}, imm16: {:x}",
			    pc, imm16);

			// Adjust PC past HLT instruction
			vectors_set_return_address(frame, pc + 4U);

			ret = true;
		}
	}

out:
	return ret;
}

```

`hyp/core/vectors/aarch64/vectors.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

module vectors

subscribe vectors_trap_unknown_el2

// This handler emits a HLT instruction to allow aborts on a debug build to
// stop in the debugger when an external debugger is attached.
// This currently has very high priority to stop before other actions like IPIs
// are sent. We may consider changing this priority if it causes problems.
subscribe abort_kernel()
	priority 2000

subscribe abort_kernel_remote
	handler vectors_handle_abort_kernel()

```

`hyp/core/vectors/armv8-64/src/return.S`:

```S
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <hypconstants.h>

#include <asm/asm_defs.inc>
#include "vectors_el2.inc"

	.section	.text

function vectors_kernel_return
	// X0 is a mask to perform a bit-wise OR into SPSR before restoring;
	// used to disable interrupts on return from preemption
.if ((OFS_KERNEL_TRAP_FRAME_X30 + 8) != OFS_KERNEL_TRAP_FRAME_SPSR_EL2) || \
	((OFS_KERNEL_TRAP_FRAME_X29 + 8) != OFS_KERNEL_TRAP_FRAME_PC) || \
	((OFS_KERNEL_TRAP_FRAME_X(18) + 8) != OFS_KERNEL_TRAP_FRAME_SP_EL2)
.error "Kernel trap frame layout has changed"
.endif
	prfm	pldl1keep, [sp, OFS_KERNEL_TRAP_FRAME_X(0)]
	// Load X18, SP_EL2
	ldp	x18, x1, [sp, OFS_KERNEL_TRAP_FRAME_X(18)]
	ldp	x29, x16, [sp, OFS_KERNEL_TRAP_FRAME_X29]
	// Load x30, SPSR_EL2
	ldp	x30, x17, [sp, OFS_KERNEL_TRAP_FRAME_X30]
	ldp	x2, x3, [sp, OFS_KERNEL_TRAP_FRAME_X(2)]
	ldp	x4, x5, [sp, OFS_KERNEL_TRAP_FRAME_X(4)]
	ldp	x6, x7, [sp, OFS_KERNEL_TRAP_FRAME_X(6)]
	ldp	x8, x9, [sp, OFS_KERNEL_TRAP_FRAME_X(8)]
	msr	ELR_EL2, x16
	ldp	x10, x11, [sp, OFS_KERNEL_TRAP_FRAME_X(10)]
	// OR X0 into SPSR
	orr	x17, x17, x0
	ldp	x12, x13, [sp, OFS_KERNEL_TRAP_FRAME_X(12)]
	msr	SPSR_EL2, x17
	ldp	x14, x15, [sp, OFS_KERNEL_TRAP_FRAME_X(14)]
	mov	x0, sp
	ldp	x16, x17, [sp, OFS_KERNEL_TRAP_FRAME_X(16)]
	mov	sp, x1
	ldp	x0, x1, [x0, OFS_KERNEL_TRAP_FRAME_X(0)]

#if defined(ARCH_ARM_FEAT_PAuth)
	// Matching PACIA is in save_kernel_context
	eretaa
#else
	eret
#endif
function_end vectors_kernel_return

```

`hyp/core/vectors/armv8-64/src/vectors.S`:

```S
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <hypconstants.h>

#include <asm/asm_defs.inc>
#include <asm/panic.inc>

#include "vectors_el2.inc"

	.section	.text.vectors

	// The vector table base is 2KB aligned
	// (16 vectors, 32 instructions each)
	.balign		2048
.global kernel_vectors_aarch64
kernel_vectors_aarch64:

default_vectors kernel 1

	.balign		2048
.global emergency_vectors_aarch64
emergency_vectors_aarch64:

default_vectors emergency 0

function vectors_exception_dispatch_full
	// In save_kernel_context, space was reserved for a full frame.
	// Save remaining callee-saved registers.
	stp	x19, x20, [sp, OFS_KERNEL_TRAP_FRAME_FULL_X19]
	stp	x21, x22, [sp, OFS_KERNEL_TRAP_FRAME_FULL_X21]
	stp	x23, x24, [sp, OFS_KERNEL_TRAP_FRAME_FULL_X23]
	stp	x25, x26, [sp, OFS_KERNEL_TRAP_FRAME_FULL_X25]
	stp	x27, x28, [sp, OFS_KERNEL_TRAP_FRAME_FULL_X27]

	b	vectors_exception_dispatch
function_end vectors_exception_dispatch_full

function handle_stack_fault
	save_kernel_context_stack_fault
#if !defined(NDEBUG)
	mov	x0, sp
	bl	dump_nested_fault
#endif
	panic	"Stack fault detected in EL2h vectors"
function_end handle_stack_fault

function __stack_chk_fail
	save_kernel_context_stack_fault
#if !defined(NDEBUG)
	mov	x0, sp
	bl	dump_nested_fault
#endif
	panic	"Stack corruption detected in EL2"
function_end __stack_chk_fail

	.section	.bss.vectors, "aw", @nobits
// Allocate a chunk of memory for each CPU for the nested fault stack
#define EMERGENCY_STACKS_SIZE (NESTED_FAULT_STACK_SIZE * PLATFORM_MAX_CORES)

	.balign	16
aarch64_emergency_stacks:
	.size aarch64_emergency_stacks, EMERGENCY_STACKS_SIZE
	.space EMERGENCY_STACKS_SIZE

```

`hyp/core/vectors/armv8-64/vectors.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier; BSD-3-Clause

// Don't change the order of the members below, they are used in assembly.

// Frame used for IRQs and returned from kernel
define kernel_trap_frame structure(aligned(16)) {
	x		array(19) type register_t;
	sp_el2		bitfield SP_EL2;
	x29		type register_t (aligned(16));
	pc		bitfield ELR_EL2;
	x30		type register_t;
	spsr_el2	bitfield SPSR_EL2_A64;
};

// Full frame used for debugging
define kernel_trap_frame_full structure (aligned(16)){
	// Base frame
	base		structure kernel_trap_frame;
	// Additional callee saved registers
	x19		type register_t;
	x20		type register_t;
	x21		type register_t;
	x22		type register_t;
	x23		type register_t;
	x24		type register_t;
	x25		type register_t;
	x26		type register_t;
	x27		type register_t;
	x28		type register_t;
};

```

`hyp/core/vectors/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

interface vectors
arch_local_include aarch64
arch_source aarch64 trap_dispatch.c exception_debug.c
arch_events aarch64 vectors.ev
arch_types armv8-64 vectors.tc
arch_source armv8-64 vectors.S return.S

```

`hyp/core/virq_null/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

interface virq
interface vic
source virq_null.c

```

`hyp/core/virq_null/src/virq_null.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <hyptypes.h>

#include <vic.h>
#include <virq.h>

error_t
vic_bind_shared(virq_source_t *source, vic_t *vic, virq_t virq,
		virq_trigger_t trigger)
{
	(void)source;
	(void)vic;
	(void)virq;
	(void)trigger;

	return ERROR_UNIMPLEMENTED;
}

error_t
vic_bind_private_vcpu(virq_source_t *source, thread_t *vcpu, virq_t virq,
		      virq_trigger_t trigger)
{
	(void)source;
	(void)vcpu;
	(void)virq;
	(void)trigger;

	return ERROR_UNIMPLEMENTED;
}

error_t
vic_bind_private_index(virq_source_t *source, vic_t *vic, index_t index,
		       virq_t virq, virq_trigger_t trigger)
{
	(void)source;
	(void)vic;
	(void)index;
	(void)virq;
	(void)trigger;

	return ERROR_UNIMPLEMENTED;
}

bool_result_t
virq_assert(virq_source_t *source, bool edge_only)
{
	(void)source;
	(void)edge_only;

	return bool_result_error(ERROR_VIRQ_NOT_BOUND);
}

error_t
virq_clear(virq_source_t *source)
{
	(void)source;

	return ERROR_VIRQ_NOT_BOUND;
}

bool_result_t
virq_query(virq_source_t *source)
{
	(void)source;

	return bool_result_error(ERROR_VIRQ_NOT_BOUND);
}

void
vic_unbind(virq_source_t *source)
{
	(void)source;
}

void
vic_unbind_sync(virq_source_t *source)
{
	(void)source;
}

```

`hyp/core/wait_queue_broadcast/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

interface wait_queue

source wait_queue.c
types wait_queue.tc
events wait_queue.ev

```

`hyp/core/wait_queue_broadcast/src/wait_queue.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>

#include <hypcontainers.h>

#include <list.h>
#include <preempt.h>
#include <scheduler.h>
#include <spinlock.h>
#include <thread.h>
#include <wait_queue.h>

#include "event_handlers.h"

// FIXME:
// Unify list code in util module

scheduler_block_properties_t
wait_queue_handle_scheduler_get_block_properties(scheduler_block_t block)
{
	assert(block == SCHEDULER_BLOCK_WAIT_QUEUE);

	scheduler_block_properties_t props =
		scheduler_block_properties_default();
	scheduler_block_properties_set_non_killable(&props, true);

	return props;
}

void
wait_queue_init(wait_queue_t *wait_queue)
{
	assert(wait_queue != NULL);

	spinlock_init(&wait_queue->lock);

	list_init(&wait_queue->list);
}

void
wait_queue_prepare(wait_queue_t *wait_queue) LOCK_IMPL
{
	thread_t    *self = thread_get_self();
	list_node_t *node = &self->wait_queue_list_node;

	assert(wait_queue != NULL);

	assert_preempt_enabled();
	preempt_disable();

	spinlock_acquire(&wait_queue->lock);

	// Insert at back
	list_insert_at_tail(&wait_queue->list, node);

	spinlock_release(&wait_queue->lock);
}

void
wait_queue_finish(wait_queue_t *wait_queue) LOCK_IMPL
{
	thread_t *self = thread_get_self();

	assert(wait_queue != NULL);

	spinlock_acquire(&wait_queue->lock);

	// Dequeue the thread
	list_node_t *node = &self->wait_queue_list_node;

	(void)list_delete_node(&wait_queue->list, node);

	spinlock_release(&wait_queue->lock);

	preempt_enable();
}

void
wait_queue_get(void) LOCK_IMPL
{
	thread_t *self = thread_get_self();
	assert_preempt_disabled();

	scheduler_lock(self);
	assert(!scheduler_is_blocked(self, SCHEDULER_BLOCK_WAIT_QUEUE));
	scheduler_block(self, SCHEDULER_BLOCK_WAIT_QUEUE);
	scheduler_unlock(self);

	atomic_thread_fence(memory_order_seq_cst);
}

void
wait_queue_put(void) LOCK_IMPL
{
	thread_t *self = thread_get_self();
	assert_preempt_disabled();

	scheduler_lock(self);
	(void)scheduler_unblock(self, SCHEDULER_BLOCK_WAIT_QUEUE);
	scheduler_unlock(self);
}

void
wait_queue_wait(void) LOCK_IMPL
{
	scheduler_yield();
	// returns when unblocked and scheduled again.
}

void
wait_queue_wakeup(wait_queue_t *wait_queue)
{
	assert(wait_queue != NULL);
	bool wakeup_any = false;

	// Order memory with respect to wait_queue_get()
	atomic_thread_fence(memory_order_seq_cst);

	spinlock_acquire(&wait_queue->lock);

	// Wakeup all waiters
	thread_t *thread;
	list_t	 *list = &wait_queue->list;

	list_foreach_container (thread, list, thread, wait_queue_list_node) {
		scheduler_lock_nopreempt(thread);
		if (scheduler_unblock(thread, SCHEDULER_BLOCK_WAIT_QUEUE)) {
			wakeup_any = true;
		}
		scheduler_unlock_nopreempt(thread);
	}

	spinlock_release_nopreempt(&wait_queue->lock);

	if (wakeup_any) {
		scheduler_trigger();
	}

	preempt_enable();
}

```

`hyp/core/wait_queue_broadcast/wait_queue.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

module wait_queue_broadcast

subscribe scheduler_get_block_properties[SCHEDULER_BLOCK_WAIT_QUEUE]
	handler wait_queue_handle_scheduler_get_block_properties

```

`hyp/core/wait_queue_broadcast/wait_queue.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

extend scheduler_block enumeration {
	wait_queue;
};

```

`hyp/debug/object_lists/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

interface object_lists
template first_class_object object_lists.tc object_lists.ev object_lists.c
types  object_lists.tc

```

`hyp/debug/object_lists/object_lists.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

extend partition object {
	partition_list_node structure list_node(contained);
};

```

`hyp/debug/object_lists/templates/object_lists.c.tmpl`:

```tmpl
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

\#include <assert.h>
\#include <hyptypes.h>

\#include <hypcontainers.h>

\#include <object.h>
\#include <spinlock.h>
\#include <list.h>
\#include <partition.h>

\#include "event_handlers.h"

extern list_t partition_list;
extern spinlock_t partition_list_lock;

list_t partition_list;
spinlock_t partition_list_lock;

void
object_lists_handle_boot_cold_init(void)
{
	spinlock_init(&partition_list_lock);
	list_init(&partition_list);

	partition_t *hyp_partition = partition_get_private();

	// Add hyp_partition manually
	spinlock_acquire(&partition_list_lock);
	list_insert_at_tail_release(&partition_list, &hyp_partition->partition_list_node);
	spinlock_release(&partition_list_lock);

#for obj in $object_list
#set o = str(obj)

#if o in ('partition','hwirq',)
#continue
#end if
	spinlock_init(&hyp_partition->${o}_list_lock);
	list_init(&hyp_partition->${o}_list);
#end for

}

error_t
object_lists_handle_object_create_partition(partition_create_t create)
{
	partition_t *partition = create.partition;

	spinlock_acquire(&partition_list_lock);
	list_insert_at_tail_release(&partition_list, &partition->partition_list_node);
	spinlock_release(&partition_list_lock);

#for obj in $object_list
#set o = str(obj)

#if o in ('partition','hwirq',)
#continue
#end if
	spinlock_init(&partition->${o}_list_lock);
	list_init(&partition->${o}_list);
#end for

	return OK;
}

void
object_lists_handle_object_cleanup_partition(partition_t *partition)
{
	spinlock_acquire(&partition_list_lock);
	(void)list_delete_node(&partition_list, &partition->partition_list_node);
	spinlock_release(&partition_list_lock);

#for obj in $object_list
#set o = str(obj)

#if o in ('partition','hwirq',)
#continue
#end if
	spinlock_acquire(&partition->${o}_list_lock);
	assert(list_is_empty(&partition->${o}_list));
	spinlock_release(&partition->${o}_list_lock);
#end for
}

#for obj in $object_list
#set o = str(obj)

#if o in ('partition','hwirq',)
#continue
#end if

error_t
object_lists_handle_object_create_${o}(${o}_create_t create)
{
	${o}_t *${o} = create.${o};

	partition_t *partition = ${o}->header.partition;

	spinlock_acquire(&partition->${o}_list_lock);
	list_insert_at_tail_release(&partition->${o}_list, &${o}->${o}_list_node);
	spinlock_release(&partition->${o}_list_lock);

	return OK;
}

void
object_lists_handle_object_cleanup_${o}(${o}_t *${o})
{
	partition_t *partition = ${o}->header.partition;

	spinlock_acquire(&partition->${o}_list_lock);
	(void)list_delete_node(&partition->${o}_list, &${o}->${o}_list_node);
	spinlock_release(&partition->${o}_list_lock);
}
#end for

```

`hyp/debug/object_lists/templates/object_lists.ev.tmpl`:

```tmpl
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

module object_lists

#for obj in $object_list
#set o = str(obj)

#if o in ('hwirq',)
#continue
#end if

subscribe object_create_${o}

subscribe object_cleanup_${o}(${o})
#end for

subscribe boot_cold_init()

```

`hyp/debug/object_lists/templates/object_lists.tc.tmpl`:

```tmpl
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#for obj in $object_list
#set o = str(obj)

#if o in ('partition','hwirq',)
#continue
#end if

extend partition object {
	${o}_list structure list;
};

extend $o object {
	${o}_list_node structure list_node(contained);
};
#end for

#for obj in $object_list
#set o = str(obj)

#if o in ('partition','hwirq',)
#continue
#end if

extend partition object {
	${o}_list_lock structure spinlock;
};
#end for

```

`hyp/debug/symbol_version/aarch64/src/sym_ver.S`:

```S
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <hypsymversion.h>

#include <asm/asm_defs.inc>

// .text.debug is a dummy section used to KEEP any symbols that may need to be
// externally visible and not optimized by LTO - usually for debuggers
.section .text.debug, "ax", @progbits
	adrp x0, hyp_sym_version_pointer
	adrp x1, platform_gicd_base
	adrp x2, platform_gicrs_bases
#if GICV3_HAS_ITS
	adrp x3, platform_gits_base
#endif

// We use a random value as the symbols version, put it directly in a symbol and
// in memory. The external debugger reads the values of this symbol and the
// memory. If they don't match it means the user has loaded the wrong symbols.
.equ	hyp_sym_version, HYP_SYM_VERSION
const64 hyp_sym_version_pointer, HYP_SYM_VERSION

```

`hyp/debug/symbol_version/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

arch_source aarch64 sym_ver.S

```

`hyp/interfaces/abort/abort.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

interface abort

event abort_kernel
	param reason: abort_reason_t

event abort_kernel_remote

```

`hyp/interfaces/abort/abort.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

define abort_reason enumeration {
	PANIC;
	ASSERTION;
};

```

`hyp/interfaces/abort/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

events abort.ev
types abort.tc

```

`hyp/interfaces/abort/include/abort.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

noreturn void
abort(const char *str, abort_reason_t reason);

```

`hyp/interfaces/addrspace/addrspace.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

interface addrspace

handled_event addrspace_attach_vdma
	param addrspace: addrspace_t *
	param vdma_device_cap: cap_id_t
	param index: index_t
	return: error_t = ERROR_CSPACE_WRONG_OBJECT_TYPE

handled_event addrspace_map
	param addrspace: addrspace_t *
	param vbase: vmaddr_t
	param size: size_t
	param phys: paddr_t
	param memtype: pgtable_vm_memtype_t
	param kernel_access: pgtable_access_t
	param user_access: pgtable_access_t
	return: error_t = ERROR_UNIMPLEMENTED

handled_event addrspace_unmap
	param addrspace: addrspace_t *
	param vbase: vmaddr_t
	param size: size_t
	param phys: paddr_t
	return: error_t = ERROR_UNIMPLEMENTED

handled_event addrspace_attach_vdevice
	param addrspace: addrspace_t *
	param vdevice_cap: cap_id_t
	param index: index_t
	param vbase: vmaddr_t
	param size: size_t
	param flags: addrspace_attach_vdevice_flags_t
	return: error_t = ERROR_CSPACE_WRONG_OBJECT_TYPE

```

`hyp/interfaces/addrspace/addrspace.hvc`:

```hvc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

define addrspace_attach_thread hypercall {
	call_num	0x2A;
	addrspace	input type cap_id_t;
	thread		input type cap_id_t;
	res0		input uregister;
	error		output enumeration error;
};

define addrspace_map hypercall {
	call_num	0x2B;
	addrspace	input type cap_id_t;
	memextent	input type cap_id_t;
	vbase		input type vmaddr_t;
	map_attrs	input bitfield memextent_mapping_attrs;
	map_flags	input bitfield addrspace_map_flags;
	// Below arguments are used only if partial is set in map_flags;
	// otherwise they are ignored for backwards compatibility.
	offset		input size;
	size		input size;
	error		output enumeration error;
};

define addrspace_unmap hypercall {
	call_num	0x2C;
	addrspace	input type cap_id_t;
	memextent	input type cap_id_t;
	vbase		input type vmaddr_t;
	map_flags	input bitfield addrspace_map_flags;
	// Below arguments are used only if partial is set in map_flags;
	// otherwise they are ignored for backwards compatibility.
	offset		input size;
	size		input size;
	error		output enumeration error;
};

define addrspace_update_access hypercall {
	call_num	0x2D;
	addrspace	input type cap_id_t;
	memextent	input type cap_id_t;
	vbase		input type vmaddr_t;
	access_attrs	input bitfield memextent_access_attrs;
	map_flags	input bitfield addrspace_map_flags;
	// Below arguments are used only if partial is set in map_flags;
	// otherwise they are ignored for backwards compatibility.
	offset		input size;
	size		input size;
	error		output enumeration error;
};

define addrspace_configure hypercall {
	call_num	0x2E;
	addrspace	input type cap_id_t;
	vmid		input type vmid_t;
	res0		input uregister;
	error		output enumeration error;
};

define addrspace_attach_vdma hypercall {
	call_num	0x2F;
	addrspace	input type cap_id_t;
	dma_device	input type cap_id_t;
	index		input type index_t;
	res0		input uregister;
	error		output enumeration error;
};

define addrspace_lookup hypercall {
	call_num	0x5A;
	addrspace	input type cap_id_t;
	memextent	input type cap_id_t;
	vbase		input type vmaddr_t;
	size		input size;
	res0		input uregister;
	error		output enumeration error;
	offset		output size;
	size		output size;
	map_attrs	output bitfield memextent_mapping_attrs;
};

define addrspace_configure_info_area hypercall {
	call_num	0x5B;
	addrspace	input type cap_id_t;
	info_area_me	input type cap_id_t;
	ipa		input type vmaddr_t;
	res0		input uregister;
	error		output enumeration error;
};

define addrspace_configure_vmmio hypercall {
	call_num	0x60;
	addrspace	input type cap_id_t;
	vbase		input type vmaddr_t;
	size		input size;
	op		input enumeration addrspace_vmmio_configure_op;
	res0		input uregister;
	error		output enumeration error;
};

define addrspace_attach_vdevice hypercall {
	call_num	0x62;
	addrspace	input type cap_id_t;
	vdevice		input type cap_id_t;
	index		input type index_t;
	vbase		input type vmaddr_t;
	size		input size;
	flags		input union addrspace_attach_vdevice_flags;
	error		output enumeration error;
};

```

`hyp/interfaces/addrspace/addrspace.tc`:

```tc
// © 2022 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

define addrspace_map_flags public bitfield<32> {
	0	partial	bool;
	31	no_sync	bool;
	30:1	res0_0	uregister(const) = 0;
};

define addrspace_lookup structure {
	phys		type paddr_t;
	size		size;
	memtype		enumeration pgtable_vm_memtype;
	kernel_access	enumeration pgtable_access;
	user_access	enumeration pgtable_access;
};

// Allocate one page for the info area for now
define MAX_VM_INFO_AREA_SIZE constant = PGTABLE_HYP_PAGE_SIZE;

// The modules that need to use the VM Info Area will extend this structure
// and add their own entries, then use the generated offsets. This structure
// should never be directly used or referenced.
define addrspace_info_area_layout structure {
};

define addrspace_vmmio_configure_op public enumeration(explicit) {
	ADD = 0;
	REMOVE = 1;
};

define addrspace_attach_vdevice_flags public union {
	raw		uint64;
};

```

`hyp/interfaces/addrspace/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

first_class_object addrspace
hypercalls addrspace.hvc
events addrspace.ev
types addrspace.tc

```

`hyp/interfaces/addrspace/include/addrspace.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// This function attaches an address space to a thread.
//
// An address space can only be attached to a thread before it starts running
// and the address space should already be in an activate state.
//
// The function returns an error if the thread is not vcpu kind, if it belongs
// to a HLOS VM and if it the address space has not been activated.
error_t
addrspace_attach_thread(addrspace_t *addrspace, thread_t *thread);

// Get a pointer to the thread's addrspace.
addrspace_t *
addrspace_get_self(void);

// Configure the address space.
//
// The object's header lock must be held and object state must be
// OBJECT_STATE_INIT.
error_t
addrspace_configure(addrspace_t *addrspace, vmid_t vmid);

// Configure the address space information area.
//
// The object's header lock must be held and object state must be
// OBJECT_STATE_INIT.
error_t
addrspace_configure_info_area(addrspace_t *addrspace, memextent_t *info_area_me,
			      vmaddr_t ipa);

// Nominate an address range as being handled by an unprivileged VMM.
//
// This can return ERROR_NORESOURCES if the implementation has reached a limit
// of nominated address ranges, ERROR_ARGUMENT_INVALID if the specified range
// overlaps an existing VMMIO range, or ERROR_UNIMPLEMENTED if there is no way
// to forward faults to an unprivileged VMM.
error_t
addrspace_add_vmmio_range(addrspace_t *addrspace, vmaddr_t base, size_t size);

// Remove an address range from the ranges handled by an unprivileged VMM.
//
// The range must match one that was previously added to the address space by
// calling addrspace_add_vmmio_range().
error_t
addrspace_remove_vmmio_range(addrspace_t *addrspace, vmaddr_t base,
			     size_t size);

// Translate a VA to PA in the current guest address space.
//
// Returns ERROR_DENIED if the lookup faults in stage 2 (including during the
// stage 1 page table walk), or ERROR_ADDR_INVALID if it faults in stage 1,
// regardless of the cause of the fault.
//
// Requires the RCU read lock, because the returned physical address might be
// unmapped concurrently and then reused after an RCU grace period.
paddr_result_t
addrspace_va_to_pa_read(gvaddr_t addr) REQUIRE_RCU_READ;

// Translate a VA to IPA in the current guest address space.
//
// Returns ERROR_DENIED if the lookup faults in stage 2 (during the stage 1 page
// table walk), or ERROR_ADDR_INVALID if it faults in stage 1, regardless of the
// cause of the fault.
vmaddr_result_t
addrspace_va_to_ipa_read(gvaddr_t addr);

// Check whether an address range is within the address space.
error_t
addrspace_check_range(addrspace_t *addrspace, vmaddr_t base, size_t size);

// Map into an addrspace.
error_t
addrspace_map(addrspace_t *addrspace, vmaddr_t vbase, size_t size, paddr_t phys,
	      pgtable_vm_memtype_t memtype, pgtable_access_t kernel_access,
	      pgtable_access_t user_access);

// Unmap from an addrspace.
error_t
addrspace_unmap(addrspace_t *addrspace, vmaddr_t vbase, size_t size,
		paddr_t phys);

// Lookup a mapping in the addrspace.
addrspace_lookup_result_t
addrspace_lookup(addrspace_t *addrspace, vmaddr_t vbase, size_t size);

```

`hyp/interfaces/allocator/allocator.ev`:

```ev
// © 2022 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

interface allocator

// Add normal memory from the partition to its allocator.
//
// The specified partition is the partition that we expect owns the memory.
setup_event allocator_add_ram_range
	param owner: partition_t *
	param phys_base: paddr_t
	param virt_base: uintptr_t
	param size: size_t
	return: error_t = ERROR_FAILURE
	success: OK

```

`hyp/interfaces/allocator/allocator.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

extend error enumeration {
	ALLOCATOR_RANGE_OVERLAPPING = 100;
	ALLOCATOR_MEM_INUSE = 101;
};

define allocator structure {
};

```

`hyp/interfaces/allocator/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

types allocator.tc
events allocator.ev

```

`hyp/interfaces/allocator/include/allocator.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// Memory allocator API functions

// Initialise an allocator. Called during partition creation.
//
// This must not allocate any memory; if the allocator needs to reserve memory
// for itself beyond what is in allocator_t, then that should be deferred until
// the first call to allocator_heap_add_memory().
error_t
allocator_init(allocator_t *allocator);

void_ptr_result_t
allocator_allocate_object(allocator_t *allocator, size_t size,
			  size_t min_alignment);

error_t
allocator_deallocate_object(allocator_t *allocator, void *object, size_t size);

error_t
allocator_heap_remove_memory(allocator_t *allocator, void *obj, size_t size);

```

`hyp/interfaces/api/api.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// FIXME:
#define HYP_VARIANT_GUNYAH 0x48
#define HYP_VARIANT_QUALCOMM 0x51

define hyp_variant public enumeration {
	UNKNOWN		= 0;
	GUNYAH		= HYP_VARIANT_GUNYAH;
	QUALCOMM	= HYP_VARIANT_QUALCOMM;
};

define hyp_api_info public bitfield<64>(const) {
	13:0		api_version	uint16 = 1;
	14		big_endian	bool = !ARCH_ENDIAN_LITTLE;
	15		is_64bit	bool = ARCH_IS_64BIT;
	55:16		unknown=0;
	63:56		variant		enumeration hyp_variant = HYP_VARIANT_QUALCOMM;
};

define hyp_api_flags0 public bitfield<64>(const) {
	0		partition_cspace bool = 0;
	1		doorbell	bool = 0;
	2		msgqueue	bool = 0;
	3		vic		bool = 0;
	4		vpm		bool = 0;
	5		vcpu		bool = 0;
	6		memextent	bool = 0;
	7		trace_ctrl	bool = 0;
	8		watchdog	bool = 0;
	9		virtio_mmio	bool = 0;
	10		prng		bool = 0;
	11		vcpu_run	bool = 0;
	16		reserved_16	bool = 0;
	31:28		scheduler	enumeration scheduler_variant = SCHEDULER_VARIANT;
	63:32,27:17,15:12 res0_0	uint64 = 0;
};

define hyp_api_flags1 public bitfield<64>(const) {
	63:0		res0_0		uint64 = 0;
};

define hyp_api_flags2 public bitfield<64>(const) {
	63:0		res0_0		uint64 = 0;
};

// Guest virtual address (Stage-1)
define gvaddr_t newtype uregister;

define user_ptr_t public newtype pointer char;

```

`hyp/interfaces/api/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

types api.tc

```

`hyp/interfaces/arm_fgt/arm_fgt.reg`:

```reg
// © 2023 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier; BSD-3-Clause

#if defined(ARCH_ARM_FEAT_FGT)
HFGWTR_EL2
#endif

```

`hyp/interfaces/arm_fgt/arm_fgt.tc`:

```tc
// © 2023 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

extend platform_cpu_features bitfield {
	auto		fgt_disable	bool = 1;
};

extend global_options bitfield {
	auto		fgt		bool = 0;
};

```

`hyp/interfaces/arm_fgt/build.conf`:

```conf
# © 2023 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

types arm_fgt.tc
registers arm_fgt.reg

```

`hyp/interfaces/arm_fgt/include/arm_fgt.h`:

```h
// © 2023 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

bool
arm_fgt_is_allowed(void);

```

`hyp/interfaces/arm_sve/arm_sve.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

extend platform_cpu_features bitfield {
	auto		sve_disable	bool = 1;
};

```

`hyp/interfaces/arm_sve/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

types arm_sve.tc

```

`hyp/interfaces/base/armv8/base.tc`:

```tc
// © 2022 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

extend core_id enumeration {
	CORTEX_A53;
	CORTEX_A55;
	CORTEX_A57;
	CORTEX_A72;
	CORTEX_A73;
	CORTEX_A75;
	CORTEX_A76;
	CORTEX_A76AE;
	CORTEX_A77;
	CORTEX_A78;
	CORTEX_A78AE;
	CORTEX_A78C;
	CORTEX_X1;
	CORTEX_X2;
	CORTEX_A710;
	CORTEX_A510;
	NEOVERSE_N1;
	NEOVERSE_N2;
	NEOVERSE_V1;
	CORTEX_A715;
	CORTEX_X3;
	CORTEX_A520;
	CORTEX_A720;
	CORTEX_X4;
	QEMU;
};

```

`hyp/interfaces/base/base.tc`:

```tc
// © 2022 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

define core_id enumeration {
		UNKNOWN;
};

```

`hyp/interfaces/base/build.conf`:

```conf
# © 2022 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

types base.tc
arch_types armv8 base.tc

```

`hyp/interfaces/base/include/base.h`:

```h
// © 2022 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

core_id_t
get_current_core_id(void);

```

`hyp/interfaces/boot/boot.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

interface boot

// Events triggered during CPU power-on sequences, either at boot or on resume
// from a power-off idle state. The events are triggered in the order they are
// listed here, but not every event is triggered in a given power-on sequence.
//
// All events other than boot_runtime_*_init are run in the context of the
// CPU's idle thread. After all events are complete, the CPU will enter the
// idle loop, which may immediately schedule and switch to a VCPU.
//
// The boot CPU's initial power-on sequence will complete all of these events
// before any secondary CPU starts, but there is no other serialisation of
// boot events; they may run concurrently on secondary CPUs and/or warm boots
// of the boot CPU.

// Runtime initialisation for the first power-on sequence.
//
// This should only be used to set up basic runtime support needed for
// execution of C code.
//
// This event runs with no current thread. Accesses to thread-local or
// CPU-local storage will not work; handlers must not call
// cpulocal_get_index(), nor access variables declared as _Thread_local.
event boot_runtime_first_init
	param boot_cpu_index: cpu_index_t

// Runtime initialisation for power-on sequences other than the first.
//
// This should only be used to set up basic runtime support needed for
// execution of C code.
//
// This event runs with no current thread. Accesses to thread-local or
// CPU-local storage will not work; handlers must not call
// cpulocal_get_index(), nor access variables declared as _Thread_local.
event boot_runtime_warm_init
	param idle_thread: thread_t *

// Prepare temporary boot-time CPU configuration.
//
// This runs for every power-on sequence after runtime initialisation.
//
// Accesses to thread-local storage can be used, but CPU-local storage will
// not work.
event boot_cpu_early_init

// Global data initialisation; triggered on the boot CPU during its first
// power-on sequence.
//
// This should be used to initialise global data structures sufficiently for
// other boot handlers to function.
//
// Accesses to CPU-local storage using the normal mechanisms will not work.
// The CPU index is provided as an input and may be used for manual accesses
// to CPU-local storage in handlers.
event boot_cold_init
	param boot_cpu_index: cpu_index_t

// CPU-local data initialisation; triggered on first power-on for each CPU.
//
// This should be used to initialise any CPU-local data structures, such as
// the idle thread (which does not get the usual object_create_thread event).
//
// The CPU index is provided as an input and may be used for manual accesses
// to CPU-local storage in handlers.
event boot_cpu_cold_init
	param cpu_index: cpu_index_t

// CPU initialisation; triggered on every power-on sequence.
//
// This should initialise any CPU control registers and per-CPU device
// registers which may be lost on entry to power-off states.
event boot_cpu_warm_init

// Hypervisor startup; sets up high-level hypervisor services.
//
// This runs only once, on the first power-on for the boot CPU. It is mostly
// used for setting up the root VM's runtime environment.
event boot_hypervisor_start

// Clean up temporary boot-time CPU configuration.
//
// This runs at the end of every power-on sequence, immediately before
// switching to the idle thread.
event boot_cpu_start

// Triggered when handover to another hypervisor is requested.
event boot_hypervisor_handover

```

`hyp/interfaces/boot/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

events boot.ev

```

`hyp/interfaces/boot/include/boot.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

extern const char hypervisor_version[];
extern const char hypervisor_build_date[];

// Triggers the hypervisor hand-over event
void
boot_start_hypervisor_handover(void) REQUIRE_PREEMPT_DISABLED;

```

`hyp/interfaces/boot/include/reloc.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

void
boot_rel_fixup(Elf_Dyn *dyni, Elf_Addr addr_offset, Elf_Addr rel_offset);

```

`hyp/interfaces/bootmem/include/bootmem.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// Allocate a block of memory
void_ptr_result_t
bootmem_allocate(size_t size, size_t align);

// Allocates all the remaining bootmem memory.
//
// This is used to hand off the remaining memory to the root partition memory
// allocator.
void_ptr_result_t
bootmem_allocate_remaining(size_t *size);

// Return the entire bootmem region, including any memory that has been
// allocated by the functions above.
//
// This is called while bootstrapping the memory ownership database.
void *
bootmem_get_region(size_t *size);

```

`hyp/interfaces/cpulocal/include/cpulocal.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// CPU-local storage.
//
// This is a generic implementation for architectures which don't have a
// conveniently readable CPU index register, and must use a TLS variable
// that is updated on context-switch.

// Declarations and accessors for CPU-local storage.
//
// Note that variables declared this way must not be accessed if it is
// possible for the calling thread to be preempted and then migrate to a
// different CPU. To avoid this, this header provides macros that can be
// used to mark a critical section that accesses CPU-local variables.

#if SCHEDULER_CAN_MIGRATE
#define cpulocal_begin()       preempt_disable()
#define cpulocal_end()	       preempt_enable()
#define assert_cpulocal_safe() assert_preempt_disabled()
#else
#define cpulocal_begin()       ((void)0)
#define cpulocal_end()	       ((void)0)
#define assert_cpulocal_safe() ((void)0)
#endif

// Declarators for CPU-local data
#define CPULOCAL_DECLARE_EXTERN(type, name)                                    \
	extern type cpulocal_##name[PLATFORM_MAX_CORES]
#define CPULOCAL_DECLARE(type, name) type cpulocal_##name[PLATFORM_MAX_CORES]
#define CPULOCAL_DECLARE_STATIC(type, name)                                    \
	static type cpulocal_##name[PLATFORM_MAX_CORES]

// Accessors for CPU-local data
#define CPULOCAL(name) cpulocal_##name[cpulocal_get_index()]
#define CPULOCAL_BY_INDEX(name, index)                                         \
	cpulocal_##name[cpulocal_check_index(index)]

// Given a pointer to an object that is known to be an element of a given
// CPU-local data array, obtain its index. The behaviour is undefined (and
// violates MISRA rule 18.2) if the object is not a member of the specified
// array.
#define CPULOCAL_PTR_INDEX(name, ptr)                                          \
	(assert(ptr != NULL),                                                  \
	 cpulocal_check_index((cpu_index_t)(ptr - cpulocal_##name)))

// Return true if a CPU index is valid.
bool
cpulocal_index_valid(cpu_index_t index);

// Validate and return a CPU index.
//
// In debug kernels, this will assert that the index is in range. The input is
// returned unchanged.
cpu_index_t
cpulocal_check_index(cpu_index_t index);

// Get the CPU index of the caller at the instant of the call.
//
// This is the same as cpulocal_get_index(), except that it does not require
// preemption to be disabled. The result may therefore be stale by the time
// the caller gets it. The result should only be used for purposes where this
// does not matter, e.g. for debug traces.
//
// The result of this function is not checked and may be invalid if called
// very early in the boot sequence.
cpu_index_t
cpulocal_get_index_unsafe(void);

// Get the CPU index of the caller.
//
// All calls to this function should be inside a critical section, which may
// be either an explicit preemption disable, a spinlock, or a cpulocal
// critical section. All uses of its result should occur before the
// corresponding critical section ends.
static inline cpu_index_t
cpulocal_get_index(void) REQUIRE_PREEMPT_DISABLED
{
	return cpulocal_check_index(cpulocal_get_index_unsafe());
}

// Get the CPU index of a specified thread.
//
// This will return a valid CPU ID if the thread is at any stage of execution on
// that CPU between the start of a thread_context_switch_pre event switching to
// the thread and the end of a thread_context_switch_post event switching away
// from the thread. If the thread is not running, it returns cpu_index_invalid.
//
// If the caller is not the specified thread, it should hold the scheduling lock
// for the thread, and all uses of its result should occur before that lock is
// released. If the caller is the specified thread, the same restrictions apply
// as for cpulocal_get_index().
cpu_index_t
cpulocal_get_index_for_thread(const thread_t *thread);

```

`hyp/interfaces/cspace/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

first_class_object cspace
template first_class_object cspace_lookup.h cspace.tc
hypercalls hypercalls.hvc

```

`hyp/interfaces/cspace/hypercalls.hvc`:

```hvc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

define cspace_delete_cap_from hypercall {
	call_num	0x22;
	cspace		input type cap_id_t;
	cap		input type cap_id_t;
	res0		input uregister;
	error		output enumeration error;
};

define cspace_copy_cap_from hypercall {
	call_num	0x23;
	src_cspace	input type cap_id_t;
	src_cap		input type cap_id_t;
	dest_cspace	input type cap_id_t;
	rights_mask	input type cap_rights_t;
	res0		input uregister;
	error		output enumeration error;
	new_cap		output type cap_id_t;
};

define cspace_revoke_cap_from hypercall {
	call_num	0x24;
	src_cspace	input type cap_id_t;
	src_cap		input type cap_id_t;
	res0		input uregister;
	error		output enumeration error;
};

define cspace_attach_thread hypercall {
	call_num	0x3e;
	cspace		input type cap_id_t;
	thread		input type cap_id_t;
	res0		input uregister;
	error		output enumeration error;
};

define cspace_revoke_caps_from hypercall {
	call_num	0x59;
	src_cspace	input type cap_id_t;
	master_cap	input type cap_id_t;
	res0		input uregister;
	error		output enumeration error;
};

```

`hyp/interfaces/cspace/include/cspace.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// Get a pointer to the thread's cspace.
// This does not take a reference to the cspace since it is not valid to change
// the cspace of a running thread.
cspace_t *
cspace_get_self(void);

// Lookup an object from a cap in a cspace. The object must be of the specified
// type. If active_only is true, the lookup will check the object state, and
// fail if it is not OBJECT_STATE_ACTIVE; checking the object state is the
// caller's responsibility otherwise. If successful, an additional reference to
// the object will be obtained.
object_ptr_result_t
cspace_lookup_object(cspace_t *cspace, cap_id_t cap_id, object_type_t type,
		     cap_rights_t rights, bool active_only);

// Lookup an object from a cap in a cspace. The object may be of any type, and
// in any state. It is the caller's responsibility to provide locking of the
// object header and to check the object state if required. If successful, an
// additional reference to the object will be obtained.
object_ptr_result_t
cspace_lookup_object_any(cspace_t *cspace, cap_id_t cap_id,
			 cap_rights_generic_t rights, object_type_t *type);

// Create the master cap for an object. The cspace will adopt the reference
// count of the object that was set when initialized after allocation.
cap_id_result_t
cspace_create_master_cap(cspace_t *cspace, object_ptr_t object,
			 object_type_t type);

cap_id_result_t
cspace_copy_cap(cspace_t *target_cspace, cspace_t *parent_cspace,
		cap_id_t parent_id, cap_rights_t rights_mask);

error_t
cspace_delete_cap(cspace_t *cspace, cap_id_t cap_id);

error_t
cspace_revoke_caps(cspace_t *cspace, cap_id_t master_cap_id);

// Configure the cspace.
// The object's header lock must be held and object state must be
// OBJECT_STATE_INIT.
error_t
cspace_configure(cspace_t *cspace, count_t max_caps);

error_t
cspace_attach_thread(cspace_t *cspace, thread_t *thread);

```

`hyp/interfaces/cspace/templates/cspace.tc.tmpl`:

```tmpl
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// Capability space interface types.

extend error enumeration {
	CSPACE_CAP_NULL	= 50;
	CSPACE_CAP_REVOKED = 51;
	CSPACE_WRONG_OBJECT_TYPE = 52;
	CSPACE_INSUFFICIENT_RIGHTS = 53;
	CSPACE_FULL = 54;
};

define cap_id_t public newtype uint64;

define CSPACE_CAP_INVALID public constant type cap_id_t = -1;

define cap_rights_t public newtype uint32;

define cap_rights_generic public bitfield<32> {
	others unknown=0;
};

extend thread object module cspace {
	cspace pointer object cspace;
};

#for obj in $object_list
#set o = str(obj)
define cap_rights_${o} public bitfield<32> {
	others	unknown=0;
};
#end for

```

`hyp/interfaces/cspace/templates/cspace_lookup.h.tmpl`:

```tmpl
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#for obj in $object_list
#set o = str(obj)

${o}_ptr_result_t
cspace_lookup_${o}(cspace_t *cspace, cap_id_t cap_id, cap_rights_${o}_t rights);

${o}_ptr_result_t
cspace_lookup_${o}_any(cspace_t *cspace, cap_id_t cap_id, cap_rights_${o}_t rights);
#end for

```

`hyp/interfaces/debug/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

types debug.tc

```

`hyp/interfaces/debug/debug.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

extend platform_cpu_features bitfield {
	auto		debug_disable	bool = 1;
};

```

`hyp/interfaces/doorbell/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

first_class_object doorbell
types doorbell.tc
hypercalls doorbell.hvc

```

`hyp/interfaces/doorbell/doorbell.hvc`:

```hvc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

define doorbell_bind_virq hypercall {
	call_num	0x10;
	doorbell	input type cap_id_t;
	vic		input type cap_id_t;
	virq		input type virq_t;
	res0		input uregister;
	error		output enumeration error;
};

define doorbell_unbind_virq hypercall {
	call_num	0x11;
	doorbell	input type cap_id_t;
	res0		input uregister;
	error		output enumeration error;
};

define doorbell_send hypercall {
	call_num	0x12;
	doorbell	input type cap_id_t;
	new_flags	input uint64;
	res0		input uregister;
	error		output enumeration error;
	old_flags	output uint64;
};

define doorbell_receive hypercall {
	call_num	0x13;
	doorbell	input type cap_id_t;
	clear_flags	input uint64;
	res0		input uregister;
	error		output enumeration error;
	old_flags	output uint64;
};

define doorbell_reset hypercall {
	call_num	0x14;
	doorbell	input type cap_id_t;
	res0		input uregister;
	error		output enumeration error;
};

define doorbell_mask hypercall {
	call_num	0x15;
	doorbell	input type cap_id_t;
	enable_mask	input uint64;
	ack_mask	input uint64;
	res0		input uregister;
	error		output enumeration error;
};

```

`hyp/interfaces/doorbell/doorbell.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#if defined(HYPERCALLS)
extend hyp_api_flags0 bitfield {
	delete	doorbell;
	1	doorbell bool = 1;
};
#endif

```

`hyp/interfaces/elf/include/elf.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

typedef uint16_t Elf_Half;
typedef uint32_t Elf_Word;

#if defined(USE_ELF64)

typedef int32_t	 Elf_Sword;
typedef int64_t	 Elf_Sxword;
typedef uint64_t Elf_Xword;
typedef uint64_t Elf_Addr;
typedef uint64_t Elf_Off;

#define R_TYPE(r_info) ((r_info)&0x7fffffffU)
#define R_SYM(r_info)  ((r_info) >> 32U)

#define ELF_CLASS ELF_CLASS_64

#elif defined(USE_ELF32)

#error unsupported USE_ELF32

#else
#error please define USE_ELF32 or USE_ELF64
#endif

#define EI_NIDENT 16

#define EI_MAG_STR                                                             \
	"\x7f"                                                                 \
	"ELF"
#define EI_MAG_SIZE 4

#define EI_CLASS      4
#define EI_DATA	      5
#define EI_VERSION    6
#define EI_OSABI      7
#define EI_ABIVERSION 8
#define EI_PAD	      9

#define ELF_CLASS_NONE 0U
#define ELF_CLASS_32   1U
#define ELF_CLASS_64   2U

#define ELF_DATA_NONE 0U
#define ELF_DATA_2LSB 1U
#define ELF_DATA_2MSB 2U

#define EV_NONE	   0U
#define EV_CURRENT 1U

#define ET_NONE 0U
#define ET_REL	1U
#define ET_EXEC 2U
#define ET_DYN	3U
#define ET_CORE 4U

#define EM_AARCH64 183U

#define PT_NULL	   0U
#define PT_LOAD	   1U
#define PT_DYNAMIC 2U
#define PT_INTERP  3U
#define PT_NOTE	   4U
#define PT_SHLIB   5U
#define PT_PHDR	   6U
#define PT_TLS	   7U
#define PT_NUM	   8U

#define PF_X 1U
#define PF_W 2U
#define PF_R 4U

#define DT_NULL	  0
#define DT_REL	  17
#define DT_RELSZ  18
#define DT_RELA	  7
#define DT_RELASZ 8
#define DT_CNT	  19

// Architecture relocation types
#define R_AARCH64_NONE	   0U
#define R_AARCH64_NULL	   256U
#define R_AARCH64_RELATIVE 1027U

typedef struct {
	Elf_Sxword d_tag;

	union {
		Elf_Xword d_val;
		Elf_Addr  d_ptr;
	} d_un;
} Elf_Dyn;

typedef struct {
	Elf_Addr  r_offset;
	Elf_Xword r_info;
} Elf_Rel;

typedef struct {
	Elf_Addr   r_offset;
	Elf_Xword  r_info;
	Elf_Sxword r_addend;
} Elf_Rela;

typedef struct {
	unsigned char e_ident[EI_NIDENT];

	Elf_Half e_type;
	Elf_Half e_machine;
	Elf_Word e_version;
	Elf_Addr e_entry;
	Elf_Off	 e_phoff;
	Elf_Off	 e_shoff;
	Elf_Word e_flags;

	Elf_Half e_ehsize;
	Elf_Half e_phentsize;
	Elf_Half e_phnum;
	Elf_Half e_shentsize;
	Elf_Half e_shnum;
	Elf_Half e_shstrndx;
} Elf_Ehdr;

typedef struct {
	Elf_Word  p_type;
	Elf_Word  p_flags;
	Elf_Off	  p_offset;
	Elf_Addr  p_vaddr;
	Elf_Addr  p_paddr;
	Elf_Xword p_filesz;
	Elf_Xword p_memsz;
	Elf_Xword p_align;
} Elf_Phdr;

```

`hyp/interfaces/elf/include/elf_loader.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// Read the ELF header its program headers and return true if valid
bool
elf_valid(void *elf_file, size_t max_size);

// Return ELF's entry point
Elf_Addr
elf_get_entry(void *elf_file);

// Return the number of program headers in the ELF file
count_t
elf_get_num_phdrs(void *elf_file);

// Return a pointer to a requested ELF program header
Elf_Phdr *
elf_get_phdr(void *elf_file, count_t index);

// Load the ELF file to its physical address as per its program headers
error_t
elf_load_phys(void *elf_file, size_t elf_max_size, paddr_t phys_base);

```

`hyp/interfaces/globals/build.conf`:

```conf
# © 2023 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

types globals.tc

```

`hyp/interfaces/globals/globals.tc`:

```tc
// © 2023 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

define global_options bitfield<64> {
};

```

`hyp/interfaces/globals/include/globals.h`:

```h
// © 2023 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

const global_options_t *
globals_get_options(void);

void
globals_set_options(const global_options_t set);

void
globals_clear_options(const global_options_t clear);

```

`hyp/interfaces/gpt/build.conf`:

```conf
# © 2022 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

events gpt.ev
types gpt.tc

```

`hyp/interfaces/gpt/gpt.ev`:

```ev
// © 2022 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

interface gpt

// Add an offset to a GPT value.
// The value can be left unmodified if not applicable.
selector_event gpt_value_add_offset
	selector type: gpt_type_t
	param value: gpt_value_t *
	param offset: size_t
	return: void

// Return true if two values of the same type are equal.
selector_event gpt_values_equal
	selector type: gpt_type_t
	param x: gpt_value_t
	param y: gpt_value_t
	return: bool = false

// Callback for the GPT type when performing a walk.
selector_event gpt_walk_callback
	selector callback: gpt_callback_t
	param entry: gpt_entry_t
	param base: size_t
	param size: size_t
	param arg: gpt_arg_t
	return: error_t = ERROR_ARGUMENT_INVALID

```

`hyp/interfaces/gpt/gpt.tc`:

```tc
// © 2022 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

define gpt structure {
};

define gpt_type enumeration {
	empty = 0;
};

define gpt_value union {
};

define gpt_entry structure {
	type	enumeration gpt_type;
	value	union gpt_value;
};

define gpt_lookup_result structure {
	entry	structure gpt_entry;
	size	size;
};

define gpt_callback enumeration {
	reserved = 0;
};

define gpt_arg union {
	raw	uintptr;
};

define gpt_config bitfield<32> {
	7:0	max_bits	type count_t;
	8	rcu_read	bool;
};

```

`hyp/interfaces/gpt/include/gpt.h`:

```h
// © 2022 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// Note that the GPT is not thread-safe by default; the caller must use an
// external lock or some other protection to prevent concurrent calls to these
// APIs. If the rcu_read option is set in the GPT's config, read-only operations
// will be protected by RCU, and external locking is only required for write
// operations.

// Initialise the GPT.
error_t
gpt_init(gpt_t *gpt, partition_t *partition, gpt_config_t config,
	 register_t allowed_types);

// Destroy the GPT.
void
gpt_destroy(gpt_t *gpt);

// Insert a range into the GPT.
//
// If expect_empty is true, the operation will fail if any part of the range is
// not found to be empty during insertion. Otherwise, any existing entries in
// the range will be overwritten.
error_t
gpt_insert(gpt_t *gpt, size_t base, size_t size, gpt_entry_t entry,
	   bool expect_empty);

// Update a range in the GPT.
//
// The update will fail if all entries over the range do not match the given old
// entry. If successful, all old entries will be replaced with the new entry.
error_t
gpt_update(gpt_t *gpt, size_t base, size_t size, gpt_entry_t old_entry,
	   gpt_entry_t new_entry);

// Remove a range from the GPT.
//
// This will fail if all entries over the range do not match the given entry.
error_t
gpt_remove(gpt_t *gpt, size_t base, size_t size, gpt_entry_t entry);

// Clear a range in the GPT.
error_t
gpt_clear(gpt_t *gpt, size_t base, size_t size);

// Clear the entire GPT.
void
gpt_clear_all(gpt_t *gpt);

// Returns true if the GPT is empty.
bool
gpt_is_empty(gpt_t *gpt);

// Lookup a range in the GPT.
//
// This function returns the entry found at the given base, and returns the size
// of this entry, which will be capped at max_size.
gpt_lookup_result_t
gpt_lookup(gpt_t *gpt, size_t base, size_t max_size);

// Returns true if an entry is contiguous over a range in the GPT.
bool
gpt_is_contiguous(gpt_t *gpt, size_t base, size_t size, gpt_entry_t entry);

// Walk over a range in the GPT and perform a callback for regions matching
// the given type.
error_t
gpt_walk(gpt_t *gpt, size_t base, size_t size, gpt_type_t type,
	 gpt_callback_t callback, gpt_arg_t arg);

// Walk over the GPT and dump all contiguous ranges.
//
// This function is intended for debug use only.
void
gpt_dump_ranges(gpt_t *gpt);

// Walk over the GPT and dump all levels.
//
// This function is intended for debug use only.
void
gpt_dump_levels(gpt_t *gpt);

```

`hyp/interfaces/hyp_aspace/build.conf`:

```conf
types hyp_aspace.tc

```

`hyp/interfaces/hyp_aspace/hyp_aspace.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#if defined(ARCH_ARM_FEAT_VHE) && defined(ARCH_ARM_FEAT_PAN)
define ARCH_AARCH64_USE_PAN constant = 1;
#else
define ARCH_AARCH64_USE_PAN constant = 0;
#endif

define virt_range structure {
	base uintptr;
	size size;
};

define phys_range structure {
	base type paddr_t;
	size size;
};

```

`hyp/interfaces/hyp_aspace/include/hyp_aspace.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// Allocate a contiguous block of virtual memory of at least the specified size.
//
// The size will be rounded up to the allocation granularity, which is typically
// several megabytes. The returned address may be randomised if KASLR is in use
// and should not be assumed to be contiguous with any prior allocations for the
// same partition.
virt_range_result_t
hyp_aspace_allocate(size_t min_size);

// Free a block of virtual memory previously returned by hyp_aspace_allocate().
void
hyp_aspace_deallocate(partition_t *partition, virt_range_t virt_range);

// Create a 1:1 mapping of the given physical address range, accessible by the
// hypervisor without calling partition_phys_access_begin.
//
// This should only be used by platform-specific legacy code that assumes 1:1
// mappings.
error_t
hyp_aspace_map_direct(partition_t *partition, paddr_t phys, size_t size,
		      pgtable_access_t access, pgtable_hyp_memtype_t memtype,
		      vmsa_shareability_t share);

// Remove a mapping created by hyp_aspace_map_direct().
error_t
hyp_aspace_unmap_direct(partition_t *partition, paddr_t phys, size_t size);

// Check for the existence of any mappings in the hypervisor address space for
// the given range. Note that when the kernel is using ARMv8.1-PAN (or an
// equivalent), there may be mappings in this range which are accessible only
// after calling partition_phys_access_begin(); this function ignores such
// mappings.
lookup_result_t
hyp_aspace_is_mapped(uintptr_t virt, size_t size, pgtable_access_t access);

error_t
hyp_aspace_va_to_pa_el2_read(void *addr, paddr_t *pa, MAIR_ATTR_t *memattr,
			     vmsa_shareability_t *shareability);

error_t
hyp_aspace_va_to_pa_el2_write(void *addr, paddr_t *pa, MAIR_ATTR_t *memattr,
			      vmsa_shareability_t *shareability);

// Return the offset used for the physaccess mappings. For the CPUs that support
// PAN this is a compile-time constant offset, and for the older CPUs it is
// randomised on every boot.
uintptr_t
hyp_aspace_get_physaccess_offset(void);

// Returns the base of the memory used for virtual address allocation
uintptr_t
hyp_aspace_get_alloc_base(void);

```

`hyp/interfaces/idle/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

events idle.ev
types idle.tc

```

`hyp/interfaces/idle/idle.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

interface idle

// Called repeatedly by the idle thread, interleaved with scheduler yields.
// May also be called from non-idle threads if there is nothing else runnable
// on a CPU.
//
// If a handler does not return IDLE_STATE_IDLE, the event will immediately
// return, and the scheduler must be called if IDLE_STATE_RESCHEDULE was
// returned. If all handlers return IDLE_STATE_IDLE, the architecture's basic
// wait-for-event instruction will be executed.
//
// This event is triggered with preemption disabled. A handler may re-enable
// preemption if necessary, but must arrange for any preemptions to
// immediately return true.
setup_event idle_yield
	param in_idle_thread: bool
	return: idle_state_t = IDLE_STATE_IDLE
	success: IDLE_STATE_IDLE

event idle_start

```

`hyp/interfaces/idle/idle.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

define idle_state enumeration {
	idle;
	wakeup;
	reschedule;
};

```

`hyp/interfaces/idle/include/idle.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// Idle thread and related APIs.

// Get the current CPU's idle thread. Must only be called in a cpulocal
// critical section.
thread_t *
idle_thread(void) REQUIRE_PREEMPT_DISABLED;

// Get the specified CPU's idle thread.
thread_t *
idle_thread_for(cpu_index_t cpu_index);

// True when running in the current CPU's idle thread.
bool
idle_is_current(void) REQUIRE_PREEMPT_DISABLED;

bool
idle_yield(void) REQUIRE_PREEMPT_DISABLED;

// Handle a wakeup event received during idle.
idle_state_t
idle_wakeup(void) REQUIRE_PREEMPT_DISABLED;

```

`hyp/interfaces/ipi/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

types ipi.tc
events ipi.ev

```

`hyp/interfaces/ipi/include/ipi.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// Routines for handling inter-processor interrupts.
//
// Modules may register an IPI reason code, and an associated handler for the
// ipi_received event. When an IPI is targeted at an online CPU, and has not
// been masked by that CPU, the handler for the IPI's reason code will be run on
// the CPU. Handlers run with preemption disabled, and may run from the idle
// thread or an interrupt handler; they return a boolean value which indicates
// whether a reschedule is needed.
//
// Each handler call is preceded by an acquire barrier which synchronises with
// the release barrier that was performed when the IPI was sent. Any subsequent
// IPI send operation that does not synchronise with that acquire barrier is
// guaranteed to trigger another call to the handler. A duplicate IPI send that
// does synchronise with the handler's acquire barrier may or may not trigger
// another handler call.
//
// If a _relaxed function is used to send an IPI, the targeted CPU(s) will
// handle the IPI next time they switch contexts. There is no particular promise
// of timely handling, even on idle CPUs.
//
// If an _idle function is used to send an IPI, it is guaranteed to be handled
// quickly whenever the targeted CPU(s) are idle. It will avoid interrupting a
// busy CPU if possible. This may be an alias of the corresponding _relaxed
// function if relaxed IPIs are guaranteed to wake idle CPUs; otherwise it is an
// alias of the corresponding normal IPI function. The _idle functions in the
// IPI module interface cannot wake the target CPU from suspend, therefore,
// using these functions incorrectly may lead to an indefinite CPU suspend,
// causing the system to hang.
//
// All of the receive-side functions below must be called with preemption
// disabled, except where noted.

// Send the specified IPI to all online CPUs other than the caller.
//
// This implies a release barrier.
void
ipi_others(ipi_reason_t ipi) REQUIRE_PREEMPT_DISABLED;

// Send the specified IPI to all online CPUs other than the caller, with low
// priority.
//
// This implies a release barrier.
void
ipi_others_relaxed(ipi_reason_t ipi) REQUIRE_PREEMPT_DISABLED;

// Send the specified IPI to all online CPUs other than the caller, with low
// priority, guaranteeing that idle CPUs will wake.
//
// This implies a release barrier.
//
// Do not use with the intention of waking a suspended CPU.
void
ipi_others_idle(ipi_reason_t ipi) REQUIRE_PREEMPT_DISABLED;

// Send the specified IPI to a single CPU.
//
// This implies a release barrier.
void
ipi_one(ipi_reason_t ipi, cpu_index_t cpu);

// Send the specified IPI to a single CPU, with low priority.
//
// This implies a release barrier.
void
ipi_one_relaxed(ipi_reason_t ipi, cpu_index_t cpu);

// Send the specified IPI to a single CPU, with low priority, guaranteeing that
// it will wake if idle.
//
// This implies a release barrier.
//
// Do not use with the intention of waking a suspended CPU.
void
ipi_one_idle(ipi_reason_t ipi, cpu_index_t cpu);

// Atomically check and clear the specified IPI reason.
//
// This can be used to prevent redundant invocations of an IPI handler. Call
// it immediately prior to taking the same action that the handler would take.
// However, note that it may incorrectly return false when called in an IPI
// handler, including a handler for a different IPI than the one being cleared.
//
// This function executes an acquire operation before returning true, equivalent
// to the acquire operation that is executed before calling a handler.
//
// If possible, this function will cancel any pending physical IPIs that other
// CPUs have asserted to signal the IPI, but note that not all interrupt
// controllers can reliably cancel an IPI without handling it.
//
// This function may be safely called with preemption enabled, from any context.
// However, its result must be ignored if it is called in an IPI handler or with
// preemption enabled.
bool
ipi_clear(ipi_reason_t ipi) REQUIRE_PREEMPT_DISABLED;

// Atomically check and clear the specified IPI, assuming it was a relaxed IPI.
//
// This can be used to prevent redundant invocations of an IPI handler. Call
// it immediately prior to taking the same action that the handler would take.
//
// This function executes an acquire operation before returning true, equivalent
// to the acquire operation that is executed before calling a handler.
//
// This function does not attempt to cancel pending physical IPIs, and therefore
// can avoid the cost of interacting with the interrupt controller for IPIs that
// are always, or mostly, raised using the _relaxed functions.
bool
ipi_clear_relaxed(ipi_reason_t ipi) REQUIRE_PREEMPT_DISABLED;

// Immediately handle any relaxed IPIs.
//
// Returns true if a reschedule is needed.
bool
ipi_handle_relaxed(void) REQUIRE_PREEMPT_DISABLED;

```

`hyp/interfaces/ipi/ipi.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

interface ipi

selector_event ipi_received
	selector reason: ipi_reason_t
	// Result is true if a reschedule is needed
	return: bool = false

```

`hyp/interfaces/ipi/ipi.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

define ipi_reason enumeration {
};

```

`hyp/interfaces/irq/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

types irq.tc
events irq.ev

```

`hyp/interfaces/irq/include/irq.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// Routines for handling hardware-triggered IRQs.

#if !defined(IRQ_NULL)
// Return the maximum valid hardware IRQ number.
irq_t
irq_max(void);

// Enable a hardware IRQ, which must not be per-CPU.
//
// This function always immediately enables the IRQ, regardless of how many
// times irq_disable_*() has been called; there is no nesting count. The caller
// is responsible for counting disables if necessary.
//
// Note that newly registered IRQs are always disabled, and IRQs are
// automatically disabled when they are deregistered.
void
irq_enable_shared(hwirq_t *hwirq);

// Enable a hardware per-CPU IRQ on the local CPU.
//
// The behaviour is the same as for irq_enable_shared(), but affects only the
// calling CPU.
void
irq_enable_local(hwirq_t *hwirq) REQUIRE_PREEMPT_DISABLED;

// Disable a hardware IRQ, which must not be per-CPU, and wait until any running
// handlers on remote CPUs have completed.
//
// This function might block the calling thread, so cannot be called with
// preemption disabled.
void
irq_disable_shared_sync(hwirq_t *hwirq);

// Disable a hardware IRQ, which must not be per-CPU, without waiting for
// handlers on remote CPUs to complete.
//
// This may be called with preemption disabled.
void
irq_disable_shared_nosync(hwirq_t *hwirq);

// Disable a hardware per-CPU IRQ on the local CPU.
//
// This may be called with preemption disabled.
void
irq_disable_local(hwirq_t *hwirq) REQUIRE_PREEMPT_DISABLED;

// Disable a hardware per-CPU IRQ on the local CPU, without waiting for the
// physical interrupt controller to acknowledge the disable (which may allow
// spurious interrupts to occur the call).
//
// This may be called with preemption disabled.
void
irq_disable_local_nowait(hwirq_t *hwirq) REQUIRE_PREEMPT_DISABLED;

// Deactivate an interrupt that has been handled after returning false from
// the irq_received handler. This is called automatically if the handler returns
// true.
void
irq_deactivate(hwirq_t *hwirq);

// Obtain the HW IRQ structure for a specific IRQ number.
//
// Must be called from an RCU critical section. No reference is taken to the
// result.
hwirq_t *
irq_lookup_hwirq(irq_t irq) REQUIRE_RCU_READ;

#if IRQ_HAS_MSI

// Allocate an IRQ number from the platform's message-signalled IRQ number
// range, and register a HW IRQ structure for it with the specified handler
// type.
//
// If there are no free MSI numbers, returns ERROR_BUSY.
//
// On success, returns a hwirq object with one reference held. No capability is
// created.
//
// The allocated MSI number will be automatically freed when the returned object
// is destroyed.
hwirq_ptr_result_t
irq_allocate_msi(partition_t *partition, hwirq_action_t action);

#endif // IRQ_HAS_MSI

#endif // !defined(IRQ_NULL)

// Handle interrupt assertion on the current CPU.
//
// Returns true if rescheduling is needed.
bool
irq_interrupt_dispatch(void) REQUIRE_PREEMPT_DISABLED;

```

`hyp/interfaces/irq/irq.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

interface irq

#if !defined(IRQ_NULL)
selector_event irq_registered
	selector action: hwirq_action_t
	param irq: irq_t
	param hwirq: hwirq_t *

selector_event irq_received
	selector action: hwirq_action_t
	param irq: irq_t
	param hwirq: hwirq_t *
	// Returns true if the interrupt has been fully handled and should be
	// deactivated; false if the interrupt has been forwarded to a guest
	// or queued for deferred handling.
	return: bool = false
#endif

```

`hyp/interfaces/irq/irq.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

define irq_trigger enumeration {
	LEVEL_HIGH;
	LEVEL_LOW;
	EDGE_RISING;
	EDGE_FALLING;
	EDGE_BOTH;
	MESSAGE;
};

```

`hyp/interfaces/log/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

types log.tc
events log.ev

```

`hyp/interfaces/log/include/log.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#if defined(PARASOFT_CYCLO)
#define LOG(tclass, id, ...)
#else
#define LOG(tclass, id, ...)                                                   \
	TRACE_EVENT(tclass, id, TRACE_ACTION_LOG, __VA_ARGS__)
#endif

#if defined(PARASOFT_CYCLO)
#define TRACE_AND_LOG(tclass, id, ...)
#else
#define TRACE_AND_LOG(tclass, id, ...)                                         \
	TRACE_EVENT(tclass, id, TRACE_ACTION_TRACE_AND_LOG, __VA_ARGS__)
#endif

```

`hyp/interfaces/log/log.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

interface log

event log_message
	param id: trace_id_t
	param str: const char *

```

`hyp/interfaces/log/log.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

extend error enumeration {
	STRING_TRUNCATED = 90;
	STRING_REACHED_END = 91;
	STRING_INVALID_FORMAT = 92;
	STRING_MISSING_PLACEHOLDER = 93;
	STRING_MISSING_ARGUMENT = 94;
};

```

`hyp/interfaces/memdb/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

types memdb.tc

```

`hyp/interfaces/memdb/include/memdb.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// FIXME: replace with a selector event
typedef error_t (*memdb_fnptr)(paddr_t, size_t, void *);

// Populate the memory database. If any entry from the range already has an
// owner, return error and do not update the database.
//
// The partition argument is the partition to use for memdb node allocations,
// and must always be the hypervisor private partition.
error_t
memdb_insert(partition_t *partition, paddr_t start_addr, paddr_t end_addr,
	     uintptr_t object, memdb_type_t obj_type);

// Change the ownership of the input address range. Checks if all entries of
// range were pointing to previous object. If so, update all entries to point to
// the new object. If not, return error.
//
// The partition argument is the partition to use for memdb node allocations,
// and must always be the hypervisor private partition.
error_t
memdb_update(partition_t *partition, paddr_t start_addr, paddr_t end_addr,
	     uintptr_t object, memdb_type_t obj_type, uintptr_t prev_object,
	     memdb_type_t prev_type);

// Find the entry corresponding to the input address and return the object and
// type the entry is pointing to.
//
// This function returns an RCU-protected reference and therefore, it needs to
// be called in a RCU critical section and maintain it until we finish using the
// returned object.
memdb_obj_type_result_t
memdb_lookup(paddr_t addr) REQUIRE_RCU_READ;

// Check if all the entries from the input address range point to the object
// passed as an argument
bool
memdb_is_ownership_contiguous(paddr_t start_addr, paddr_t end_addr,
			      uintptr_t object, memdb_type_t type);

// Walk through the entire database and add the address ranges that are owned
// by the object passed as argument.
error_t
memdb_walk(uintptr_t object, memdb_type_t type, memdb_fnptr fn, void *arg);

// Walk through a range of the database and add the address ranges that are
// owned by the object passed as argument.
error_t
memdb_range_walk(uintptr_t object, memdb_type_t type, paddr_t start,
		 paddr_t end, memdb_fnptr fn, void *arg);

```

`hyp/interfaces/memdb/memdb.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

define memdb_type enumeration {
	NOTYPE = 0;
	PARTITION;
	ALLOCATOR;
	EXTENT;
	TRACE;
};

define memdb_obj_type structure {
	object	uintptr;
	type	enumeration memdb_type;
};

extend error enumeration {
	MEMDB_EMPTY = 110;
	MEMDB_NOT_OWNER = 111;
};

extend trace_class enumeration {
	MEMDB = 7;
};

```

`hyp/interfaces/memextent/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

types memextent.tc
events memextent.ev
first_class_object memextent
hypercalls memextent.hvc

```

`hyp/interfaces/memextent/include/memextent.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// Memory extents.
//
// These are ranges of memory that can be mapped, whole, into VM address
// spaces. Basic memory extents, which are always implemented, are contiguous
// ranges of memory with no special semantics. Other memory extent types that
// may be implemented include scatter-gather extents and virtual devices.

// Configure a memory extent without parent.
//
// A memory extent without parent can only be of type basic. The object's header
// lock must be held held and object state must be OBJECT_STATE_INIT.
error_t
memextent_configure(memextent_t *me, paddr_t phys_base, size_t size,
		    memextent_attrs_t attributes);

// Configure derived basic memory extent.
//
// The object's header lock must be held and object state must be
// OBJECT_STATE_INIT.
error_t
memextent_configure_derive(memextent_t *me, memextent_t *parent, size_t offset,
			   size_t size, memextent_attrs_t attributes);

// Returns true if the memextent supports donation.
bool
memextent_supports_donation(memextent_t *me);

// Donate memory to a child extent.
//
// This can only be used for sparse memory extents. If the memory extent has
// been derived the memory is taken from the parent extent, otherwise the memory
// is taken from the extent's partition. If reverse is true, the memory is
// donated back to the parent instead.
error_t
memextent_donate_child(memextent_t *me, size_t offset, size_t size,
		       bool reverse);

// Donate memory to a sibling extent.
//
// This can only be used for sparse memory extents. Both from and to memory
// extents must have the same parent memory extent.
error_t
memextent_donate_sibling(memextent_t *from, memextent_t *to, size_t offset,
			 size_t size);

// Map a memory extent into a specified address space. The entire range is
// mapped, except for any carveouts contained within the extent.
//
// If there are any existing mappings in the affected region, they are replaced
// with the new mapping. There may still be in-progress EL2 operations using old
// mappings. These are RCU read operations and are guaranteed to complete (or
// start using the new mapping) by the end of the next grace period.
error_t
memextent_map(memextent_t *me, addrspace_t *addrspace, vmaddr_t vm_base,
	      memextent_mapping_attrs_t map_attrs);

// Map a portion of a memory extent into a specified address space. Only the
// range specified by offset and size is mapped, excluding any carveouts
// contained within this range.
//
// If there are any existing mappings in the affected region, they are replaced
// with the new mapping. There may still be in-progress EL2 operations using old
// mappings. These are RCU read operations and are guaranteed to complete (or
// start using the new mapping) by the end of the next grace period.
error_t
memextent_map_partial(memextent_t *me, addrspace_t *addrspace, vmaddr_t vm_base,
		      size_t offset, size_t size,
		      memextent_mapping_attrs_t map_attrs);

// Unmap a memory extent from a specified address space. The entire range is
// unmapped, except for any carveouts contained within the extent.
//
// There may still be in-progress EL2 operations using the removed mappings.
// These are RCU read operations and are guaranteed to complete (or fault due to
// the unmap) by the end of the next grace period.
error_t
memextent_unmap(memextent_t *me, addrspace_t *addrspace, vmaddr_t vm_base);

// Unmap a portion of a memory extent from a specified address space. Only the
// range specified by offset and size is unmapped, except for any carveouts
// contained within this range.
//
// There may still be in-progress EL2 operations using the removed mappings.
// These are RCU read operations and are guaranteed to complete (or fault due to
// the unmap) by the end of the next grace period.
error_t
memextent_unmap_partial(memextent_t *me, addrspace_t *addrspace,
			vmaddr_t vm_base, size_t offset, size_t size);

// Unmap a memory extent from all address spaces. The entire range is unmapped,
// except for any carveouts contained within the extent.
//
// There may still be in-progress EL2 operations using the removed mappings.
// These are RCU read operations and are guaranteed to complete (or fault due to
// the unmap) by the end of the next grace period.
void
memextent_unmap_all(memextent_t *me);

// Zero all owned regions of a memory extent in the given range.
error_t
memextent_zero_range(memextent_t *me, size_t offset, size_t size);

// Cache clean all owned regions of a memory extent in the given range.
error_t
memextent_cache_clean_range(memextent_t *me, size_t offset, size_t size);

// Cache flush all owned regions of a memory extent in the given range.
error_t
memextent_cache_flush_range(memextent_t *me, size_t offset, size_t size);

// Update the access rights on an existing mapping.
//
// There may still be in-progress EL2 operations using the old access rights.
// These are RCU read operations and are guaranteed to complete (or fault due to
// reduced access rights) by the end of the next grace period.
error_t
memextent_update_access(memextent_t *me, addrspace_t *addrspace,
			vmaddr_t		 vm_base,
			memextent_access_attrs_t access_attrs);

// Update the access rights on part of an existing mapping.
//
// There may still be in-progress EL2 operations using the old access rights.
// These are RCU read operations and are guaranteed to complete (or fault due to
// reduced access rights) by the end of the next grace period.
error_t
memextent_update_access_partial(memextent_t *me, addrspace_t *addrspace,
				vmaddr_t vm_base, size_t offset, size_t size,
				memextent_access_attrs_t access_attrs);

// Returns true if a memextent is mapped in an addrspace.
//
// If exclusive is true, this function will return false if the memextent is
// mapped in any other addrspace, regardless of whether it is mapped in the
// given addrspace.
bool
memextent_is_mapped(memextent_t *me, addrspace_t *addrspace, bool exclusive);

// Helper function to check the combination memory type of the memory extent and
// the mapping memtype.
bool
memextent_check_memtype(memextent_memtype_t  extent_type,
			pgtable_vm_memtype_t map_type);

// Helper function to derive a new memextent from a parent and activate it.
// This function does not create a capability to the new memextent.
memextent_ptr_result_t
memextent_derive(memextent_t *parent, paddr_t offset, size_t size,
		 memextent_memtype_t memtype, pgtable_access_t access,
		 memextent_type_t type);

// Temporarily retain all of the memextent's mappings.
//
// This acquires references to all addrspaces the memextent has mappings in,
// preventing the addrspace from being destroyed while looking up mappings.
void
memextent_retain_mappings(memextent_t *me) REQUIRE_LOCK(me->lock)
	ACQUIRE_LOCK(me->mappings);

// Release a memextent's retained mappings.
//
// This frees any references acquired in memextent_retain_mappings().
// If clear is true, all mappings will be cleared as well.
void
memextent_release_mappings(memextent_t *me, bool clear) REQUIRE_LOCK(me->lock)
	RELEASE_LOCK(me->mappings);

// Lookup a mapping in a memextent.
//
// The memextent's mappings must be retained when this is called. The supplied
// physical address and size must lie within the memextent, and the index must
// be less than MEMEXTENT_MAX_MAPS.
//
// If the returned addrspace is not NULL, the memextent has a mapping in the
// given range; otherwise the memextent is not mapped for this range. The
// returned size is valid for both of these cases, dictating the size of the
// mapping (or lack thereof). This size may be smaller than the given size if
// the range is only partially mapped.
memextent_mapping_t
memextent_lookup_mapping(memextent_t *me, paddr_t phys, size_t size, index_t i)
	REQUIRE_LOCK(me->lock) REQUIRE_LOCK(me->mappings);

// Claim and map a memextent for access in the hypervisor.
//
// The specified partition must be the owner of the object that the memextent
// is being attached to. Typically this is an address space or virtual device
// object.
//
// The specified extent must be a basic (not sparse) extent with normal memory
// type, no children, RW access, and no existing hypervisor attachment. While it
// remains attached, derivation of children or donation of memory to or from
// this extent will not be permitted. It may, however, be mapped in VM address
// spaces while attached, and existing VM address space mappings will not be
// removed by attachement.
//
// The specified virtual address range must be within a region allocated to the
// specified partition by hyp_aspace_allocate(). The specified size must be a
// nonzero multiple of the page size and no greater than the size of the
// memextent.
//
// If the memextent is provided through a hypervisor API, the caller should
// possess the MEMEXTENT_ATTACH right.
//
// If the call is successful, the entire specified address range will be mapped
// to the memextent.
error_t
memextent_attach(partition_t *owner, memextent_t *extent, uintptr_t hyp_va,
		 size_t size);

// Detach a memextent from the hypervisor.
//
// The specified owner and extent must match a previous successful call to
// memextent_attach().
void
memextent_detach(partition_t *owner, memextent_t *extent);

// Find the offset of a specified physical access within a memextent.
//
// This is intended to be used for handling faults. If any part of the specified
// physical address range is outside the memextent (including in a gap in a
// sparse memextent), it will return an error.
size_result_t
memextent_get_offset_for_pa(memextent_t *me, paddr_t pa, size_t size);

```

`hyp/interfaces/memextent/memextent.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

interface memextent

selector_event memextent_activate
	selector type: memextent_type_t
	param me: memextent_t *
	return: error_t = ERROR_MEMEXTENT_TYPE

selector_event memextent_activate_derive
	selector type: memextent_type_t
	param me: memextent_t *
	return: error_t = ERROR_MEMEXTENT_TYPE

selector_event memextent_supports_donation
	selector type: memextent_type_t
	param me: memextent_t *
	return: bool = false

selector_event memextent_donate_child
	selector type: memextent_type_t
	param me: memextent_t *
	param phys: paddr_t
	param size: size_t
	param reverse: bool
	return: error_t = ERROR_MEMEXTENT_TYPE

selector_event memextent_donate_sibling
	selector type: memextent_type_t
	param from: memextent_t *
	param to: memextent_t *
	param phys: paddr_t
	param size: size_t
	return: error_t = ERROR_MEMEXTENT_TYPE

selector_event memextent_map
	selector type: memextent_type_t
	param extent: memextent_t *
	param addrspace: addrspace_t *
	param vm_base: vmaddr_t
	param map_attrs: memextent_mapping_attrs_t
	return: error_t = ERROR_MEMEXTENT_TYPE

selector_event memextent_map_partial
	selector type: memextent_type_t
	param extent: memextent_t *
	param addrspace: addrspace_t *
	param vm_base: vmaddr_t
	param offset: size_t
	param size: size_t
	param map_attrs: memextent_mapping_attrs_t
	return: error_t = ERROR_MEMEXTENT_TYPE

selector_event memextent_unmap
	selector type: memextent_type_t
	param extent: memextent_t *
	param addrspace: addrspace_t *
	param vm_base: vmaddr_t
	return: error_t = ERROR_MEMEXTENT_TYPE

selector_event memextent_unmap_partial
	selector type: memextent_type_t
	param extent: memextent_t *
	param addrspace: addrspace_t *
	param vm_base: vmaddr_t
	param offset: size_t
	param size: size_t
	return: error_t = ERROR_MEMEXTENT_TYPE

selector_event memextent_unmap_all
	selector type: memextent_type_t
	param extent: memextent_t *
	return: bool = false

selector_event memextent_update_access
	selector type: memextent_type_t
	param extent: memextent_t *
	param addrspace: addrspace_t *
	param vm_base: vmaddr_t
	param access_attrs: memextent_access_attrs_t
	return: error_t = ERROR_MEMEXTENT_TYPE

selector_event memextent_update_access_partial
	selector type: memextent_type_t
	param extent: memextent_t *
	param addrspace: addrspace_t *
	param vm_base: vmaddr_t
	param offset: size_t
	param size: size_t
	param access_attrs: memextent_access_attrs_t
	return: error_t = ERROR_MEMEXTENT_TYPE

selector_event memextent_is_mapped
	selector type: memextent_type_t
	param me: memextent_t *
	param addrspace: addrspace_t *
	param exclusive: bool
	return: bool = false

selector_event memextent_deactivate
	selector type: memextent_type_t
	param extent: memextent_t *
	return: bool = false

selector_event memextent_cleanup
	selector type: memextent_type_t
	param extent: memextent_t *
	return: bool = false

selector_event memextent_retain_mappings
	selector type: memextent_type_t
	param me: memextent_t *

selector_event memextent_release_mappings
	selector type: memextent_type_t
	param me: memextent_t *
	param clear: bool

selector_event memextent_lookup_mapping
	selector type: memextent_type_t
	param me: memextent_t *
	param phys: paddr_t
	param size: size_t
	param i: index_t
	return: memextent_mapping_result_t = memextent_mapping_result_error(ERROR_MEMEXTENT_TYPE)

selector_event memextent_attach
	selector type: memextent_type_t
	param me: memextent_t *
	param hyp_va: uintptr_t
	param size: size_t
	param memtype: pgtable_hyp_memtype_t
	return: error_t = ERROR_MEMEXTENT_TYPE

selector_event memextent_detach
	selector type: memextent_type_t
	param me: memextent_t *
	return: bool = false

selector_event memextent_get_offset_for_pa
	selector type: memextent_type_t
	param extent: memextent_t *
	param pa: paddr_t
	param size: size_t
	return: size_result_t = size_result_error(ERROR_MEMEXTENT_TYPE)

```

`hyp/interfaces/memextent/memextent.hvc`:

```hvc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

define memextent_modify hypercall {
	call_num	0x30;
	memextent	input type cap_id_t;
	flags		input bitfield memextent_modify_flags;
	// The below arguments are only used for range operations;
	// otherwise they are ignored for backwards compatibility.
	offset		input size;
	size		input size;
	error		output enumeration error;
};

define memextent_configure hypercall {
	call_num	0x31;
	memextent	input type cap_id_t;
	phys_base	input type paddr_t;
	size		input size;
	attributes	input bitfield memextent_attrs;
	res0		input uregister;
	error		output enumeration error;
};

define memextent_configure_derive hypercall {
	call_num	0x32;
	memextent	input type cap_id_t;
	parent_memextent input type cap_id_t;
	offset		input size;
	size		input size;
	attributes	input bitfield memextent_attrs;
	res0		input uregister;
	error		output enumeration error;
};

define memextent_donate hypercall {
	call_num	0x61;
	options		input bitfield memextent_donate_options;
	from		input type cap_id_t;
	to		input type cap_id_t;
	offset		input size;
	size		input size;
	res0		input uregister;
	error		output enumeration error;
};

```

`hyp/interfaces/memextent/memextent.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

define memextent_type public enumeration {
	BASIC = 0;
	SPARSE = 1;
};

extend memextent object {
	parent	pointer object memextent;
	type	enumeration memextent_type;
};

extend memextent_create structure module memextent {
	device_mem bool;
};

extend error enumeration {
	MEMEXTENT_MAPPINGS_FULL = 120;
	MEMEXTENT_TYPE = 121;
};

// Memory extents have a simplified memtype which is distinct from
// pgtable_hyp_memtype. This is because there are only limited memory types
// which we need to enforce, but potentially a large number of memtypes
// available for mapping.
define memextent_memtype public enumeration(explicit) {
	ANY = 0;		// May map with any type
	DEVICE = 1;		// Device memory types only
	UNCACHED = 2;		// Force uncached
	CACHED = 3;		// Force writeback cached
};

// Bitfield for memextent configure API
define memextent_attrs public bitfield<32> {
	2:0	access enumeration pgtable_access;
	9:8	memtype enumeration memextent_memtype;
	17:16	type enumeration memextent_type;
	31	append bool;	// List append range
	// Reserved bits
	30:18,15:10,7:3 res_0 uregister(const) = 0;
};

// Bitfield for mapping API attributes
define memextent_mapping_attrs public bitfield<32> {
	2:0	user_access enumeration pgtable_access;
	6:4	kernel_access enumeration pgtable_access;
	23:16	memtype enumeration pgtable_vm_memtype;
	// Reserved bits
	31:24,15:8,7,3 res_0 uregister(const) = 0;
};

// Bitfield for mapping access change APIs
define memextent_access_attrs public bitfield<32> {
	2:0	user_access enumeration pgtable_access;
	6:4	kernel_access enumeration pgtable_access;
	// Reserved bits
	31:8,7,3 res_0 uregister(const) = 0;
};

define memextent_donate_type public enumeration(explicit) {
	to_child = 0;		// Donate from parent to child
	to_parent = 1;		// Donate from child to parent
	to_sibling = 2;		// Donate to extent with same parent
};

define memextent_donate_options public bitfield<32> {
	7:0	type	enumeration memextent_donate_type;
	30:8	res_0	uregister(const) = 0;
	31	no_sync	bool;
};

// Structure returned when looking up a mapping.
define memextent_mapping structure {
	addrspace	pointer object addrspace;
	vbase		type vmaddr_t;
	size		size;
	attrs		bitfield memextent_mapping_attrs;
};

define memextent_modify_op public enumeration(explicit) {
	unmap_all = 0;		// Remove all mappings from memextent
	zero_range = 1;		// Zero a range of the memextent
	cache_clean_range = 2;	// Cache clean a range of the memextent
	cache_flush_range = 3;	// Cache flush a range of the memextent
	sync_all = 255;		// Sync all previous memextent ops
};

define memextent_modify_flags public bitfield<32> {
	7:0	op	enumeration memextent_modify_op;
	30:8	res_0	uregister(const) = 0;
	31	no_sync	bool;
};

```

`hyp/interfaces/msgqueue/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

first_class_object msgqueue
types msgqueue.tc
hypercalls msgqueue.hvc

```

`hyp/interfaces/msgqueue/msgqueue.hvc`:

```hvc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

define msgqueue_bind_send_virq hypercall {
	call_num	0x17;
	msgqueue	input type cap_id_t;
	vic		input type cap_id_t;
	virq		input type virq_t;
	res0		input uregister;
	error		output enumeration error;
};

define msgqueue_bind_receive_virq hypercall {
	call_num	0x18;
	msgqueue	input type cap_id_t;
	vic		input type cap_id_t;
	virq		input type virq_t;
	res0		input uregister;
	error		output enumeration error;
};

define msgqueue_unbind_send_virq hypercall {
	call_num	0x19;
	msgqueue	input type cap_id_t;
	res0		input uregister;
	error		output enumeration error;
};

define msgqueue_unbind_receive_virq hypercall {
	call_num	0x1a;
	msgqueue	input type cap_id_t;
	res0		input uregister;
	error		output enumeration error;
};

define msgqueue_send hypercall {
	call_num	0x1b;
	msgqueue	input type cap_id_t;
	size		input size;
	data		input type user_ptr_t;
	send_flags	input bitfield msgqueue_send_flags;
	res0		input uregister;
	error		output enumeration error;
	not_full	output bool;
};

define msgqueue_receive hypercall {
	call_num	0x1c;
	msgqueue	input type cap_id_t;
	buffer		input type user_ptr_t;
	buf_size	input size;
	res0		input uregister;
	error		output enumeration error;
	size		output size;
	not_empty	output bool;
};

define msgqueue_flush hypercall {
	call_num	0x1d;
	msgqueue	input type cap_id_t;
	res0		input uregister;
	error		output enumeration error;
};

define msgqueue_configure_send hypercall {
	call_num	0x1f;
	msgqueue	input type cap_id_t;
	not_full_thres	input type count_t;
	not_full_holdoff input type count_t;
	res1		input uregister;
	error		output enumeration error;
};

define msgqueue_configure_receive hypercall {
	call_num	0x20;
	msgqueue	input type cap_id_t;
	not_empty_thres	input type count_t;
	not_empty_holdoff input type count_t;
	res1		input uregister;
	error		output enumeration error;
};

define msgqueue_configure hypercall {
	call_num	0x21;
	msgqueue	input type cap_id_t;
	create_info	input bitfield msgqueue_create_info;
	res0		input uregister;
	error		output enumeration error;
};

```

`hyp/interfaces/msgqueue/msgqueue.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#if defined(HYPERCALLS)
extend hyp_api_flags0 bitfield {
	delete	msgqueue;
	2	msgqueue bool = 1;
};
#endif

define kernel_or_gvaddr union {
	guest_addr	type gvaddr_t;
	kernel_addr	uintptr;
};

```

`hyp/interfaces/mutex/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

events mutex.ev

```

`hyp/interfaces/mutex/include/mutex.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

void
mutex_init(mutex_t *lock);

void
mutex_acquire(mutex_t *lock) ACQUIRE_LOCK(lock);

bool
mutex_trylock(mutex_t *lock) TRY_ACQUIRE_LOCK(true, lock);

void
mutex_release(mutex_t *lock) RELEASE_LOCK(lock);

```

`hyp/interfaces/mutex/mutex.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

interface mutex

// Mutex events.
//
// Note that there are separate events for before and after the various mutex
// operations.

event mutex_init
	param lock: mutex_t *

event mutex_acquire
	param lock: mutex_t *

event mutex_acquired
	param lock: mutex_t *

event mutex_failed
	param lock: mutex_t *

event mutex_release
	param lock: mutex_t *

event mutex_released
	param lock: mutex_t *

```

`hyp/interfaces/object/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

template first_class_object object.tc object.ev object.h

```

`hyp/interfaces/object/templates/object.ev.tmpl`:

```tmpl
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

interface object

// FIXME:
// Add rollback handlers
#for obj in $object_list
#set o = str(obj)

setup_event object_create_$o
	param ${o}_create: ${o}_create_t
	return: error_t = OK
	success: OK

setup_event object_activate_$o
	param ${o}: ${o}_t *
	return: error_t = OK
	success: OK

event object_deactivate_$o
	param ${o}: ${o}_t *

event object_cleanup_$o
	param status: rcu_update_status_t *
	param ${o}: ${o}_t *

event object_get_defaults_$o
	param ${o}_create: ${o}_create_t *
#end for

```

`hyp/interfaces/object/templates/object.h.tmpl`:

```tmpl
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#for obj in $object_list
#set o = str(obj)
${o}_t *
object_get_${o}_additional(${o}_t *${o});

bool
object_get_${o}_safe(${o}_t *${o});

void
object_put_${o}(${o}_t *${o});

error_t
object_activate_${o}(${o}_t *${o});

#end for
object_ptr_t
object_get_additional(object_type_t type, object_ptr_t object);

bool
object_get_safe(object_type_t type, object_ptr_t object);

void
object_put(object_type_t type, object_ptr_t object);

object_header_t *
object_get_header(object_type_t type, object_ptr_t object);

error_t
object_activate(object_type_t type, object_ptr_t object);

```

`hyp/interfaces/object/templates/object.tc.tmpl`:

```tmpl
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

define object_state enumeration {
	NONE = 0;
	INIT;
	ACTIVE;
	FAILED;
	DESTROYING;
};

extend error enumeration {
	OBJECT_STATE = 33;
	OBJECT_CONFIG = 34;
	OBJECT_CONFIGURED = 35;
};

define object_type enumeration {
#for obj in $object_list
#set o = str(obj)
	$o;
#end for
};

define object_ptr union {
#for obj in $object_list
#set o = str(obj)
	$o pointer object $o;
#end for
};

#for obj in $object_list
#set o = str(obj)
define $o object {
};
#end for

#for obj in $object_list
#set o = str(obj)

define ${o}_create structure {
	$o pointer object $o;
};
#end for

```

`hyp/interfaces/partition/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

first_class_object partition no_partition_create_hypcall
template first_class_object partition_alloc.h
types partition.tc
events partition.ev

```

`hyp/interfaces/partition/include/partition.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// Allocate memory from a partition
//
// This allocates memory of least the requested size and minimum alignment
// specified and returns a pointer to the start of the allocation.
//
// Note that the memory allocated is uninitialized and it is the caller's
// responsibility to initialize or zero it.
void_ptr_result_t
partition_alloc(partition_t *partition, size_t bytes, size_t min_alignment);

// Free memory back to a partition
//
// The memory must have been allocated from the partition that is it is freed
// back to.
error_t
partition_free(partition_t *partition, void *mem, size_t bytes);

// Free memory by physical address back to a partition
//
// The memory must have been allocated from the partition that is it is freed
// back to.
error_t
partition_free_phys(partition_t *partition, paddr_t phys, size_t bytes);

// Get the hypervisor's private partition.
//
// This is used for allocating objects that are owned by the hypervisor itself
// and used internally, such as the idle threads.
partition_t *
partition_get_private(void);

// Get the physical address of an object owned by this partition.
//
// This is used by a limited set of internal hypervisor functions that need to
// allocate physical memory for their own use, e.g. for page table levels.
//
// The specified address must have been returned by a partition_alloc() call
// for the specified partition, and not subsequently freed; the result is not
// guaranteed to be meaningful otherwise. The caller must not assume that
// hypervisor memory is physically contiguous beyond the range allocated by that
// partition_alloc() call.
paddr_t
partition_virt_to_phys(partition_t *partition, uintptr_t virt);

// Check whether the physical address range is valid, i.e. associated to any
// partition.
//
// The physical address must be normal memory owned by a partition, but it is
// not necessary to specify the partition.
bool
partition_phys_valid(paddr_t paddr, size_t size);

// Obtain a virtual address that may be used to access a specified physical
// address.
//
// The physical address must be normal memory owned by a partition, but it is
// not necessary to specify the partition.
//
// The returned virtual address must only be accessed between calls to
// partition_phys_access_enable() and partition_phys_access_disable().
//
// This function must be paired with a call to partition_phys_unmap(). Pairs of
// calls to these functions may be nested; there may be a limit on the number of
// levels of nesting, but it shall be large enough to simultaneously access
// every level of the largest page table supported by the platform.
//
// Some implementations of this function may need to construct mappings at
// runtime (and unmap them afterwards), and therefore may be relatively slow.
// Where possible, multiple accesses to one memory region should be batched to
// minimise calls to this function.
void *
partition_phys_map(paddr_t paddr, size_t size);

// Enable accesses to a physical address mapped with partition_phys_map().
//
// The pointer should point to the specific object that will be accessed.
//
// This function must be paired with a call to partition_phys_access_disable()
// with the same address argument. Pairs of calls to these functions must not be
// nested.
//
// This function should be fast enough to call before each individual access. If
// this is not possible, it must be a no-op, and all work necessary to enable
// access to the mapping must be performed by partition_phys_map().
void
partition_phys_access_enable(const void *ptr);

// Disable accesses to a physical address mapped with partition_phys_map().
//
// The pointer should point to the specific object that was accessed.
//
// This function should be fast enough to call after each individual access. If
// this is not possible, it should be a no-op.
void
partition_phys_access_disable(const void *ptr);

// Release a virtual address returned by partition_phys_unmap().
//
// The virtual address must not be accessed after calling this function.
void
partition_phys_unmap(const void *vaddr, paddr_t paddr, size_t size);

// Donate memory from partition
//
// Update address range ownership in the memory database from src_partition to
// dst_partition.
//
// If from_heap is false, the specified range must be unused memory owned by the
// source partition.
//
// If from_heap is true, the specified range must have been returned by a call
// to partition_alloc() with the specified source partition. This is useful when
// allocating a new partition during bootstrap, from within the hypervisor.
error_t
partition_mem_donate(partition_t *src_partition, paddr_t base, size_t size,
		     partition_t *dst_partition, bool from_heap);

// Add memory to the partition's allocators
//
// The memory must have been allocated from the partition that is it is freed
// back to.
error_t
partition_add_heap(partition_t *partition, paddr_t base, size_t size);

// Map range and add memory to the partition's allocator
//
// The memory must have been allocated from the partition that is it is freed
// back to.
error_t
partition_map_and_add_heap(partition_t *partition, paddr_t base, size_t size);

#if defined(PLATFORM_TRACE_STANDALONE_REGION)
// Map range and add memory to the partition's trace area
//
// The memory must have been allocated from the partition that is it is freed
// back to.
uintptr_result_t
partition_map_and_add_trace(partition_t *partition, paddr_t base, size_t size);
#endif

```

`hyp/interfaces/partition/include/partition_init.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#if (defined(UNIT_TESTS) && UNIT_TESTS) || defined(ROOTVM_INIT) ||             \
	defined(UEFITZT)

// Get the root VM's partition.
//
// This partition is used for constructing the root VM, and is the initial owner
// of every resource that is not used internally by the hypervisor.
//
// Do not add new calls to this function, except to donate memory to the root
// VM. Callers to partition_alloc() and related functions should always use a
// partition obtained either from an explicit partition capability, or from the
// object header of some first-class object that owns the allocation.
partition_t *
partition_get_root(void);

#endif

```

`hyp/interfaces/partition/partition.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier; BSD-3-Clause

interface partition

// Theses events are triggered for each contiguous region of normal memory
// added/removed from the hypervisor system. Add is called for memory returned
// by the platform memory probe, excluding the region occupied by the
// hypervisor image itself.
//
// They may also be called at runtime, for example hotplug memory or memory
// being protected/unprotected by higher security domain, such as memory
// transferred to non-secure world by TrustZone.

// Add normal memory to the system.
//
// The specified partition is the partition that will initially own the memory.
setup_event partition_add_ram_range
	param owner: partition_t *
	param phys_base: paddr_t
	param size: size_t
	return: error_t = OK
	success: OK

// Remove normal memory from the system.
//
// The specified partition is the partition that we expect owns the memory.
setup_event partition_remove_ram_range
	param owner: partition_t *
	param phys_base: paddr_t
	param size: size_t
	return: error_t = OK
	success: OK

```

`hyp/interfaces/partition/partition.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#if defined(HYPERCALLS)
extend hyp_api_flags0 bitfield {
	delete	partition_cspace;
	0	partition_cspace bool = 1;
};
#endif

```

`hyp/interfaces/partition/templates/partition_alloc.h.tmpl`:

```tmpl
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#for obj in $object_list
#set o = str(obj)

${o}_ptr_result_t
partition_allocate_${o}(partition_t *parent, ${o}_create_t create);
#end for

```

`hyp/interfaces/pgtable/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

types pgtable.tc
events pgtable.ev

```

`hyp/interfaces/pgtable/include/pgtable.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// Low-level page table manipulation routines.
//
// Functions in this interface generally should only be called by modules that
// are responsible for managing address spaces. Don't call them directly from
// more general modules (syscall handlers, etc).
//
// The map operation return errors in cases of inconsistent mappings where
// there are existing mappings present in the range or allocation failures.
// Unmap does not return error for non-existing mappings.
//
// The caller will generally need to operate on some higher-level model of the
// address space first, and hold locks on that model (possibly fine-grained)
// to prevent conflicting updates. The update operations defined here are not
// required to be thread-safe with respect to updates affecting overlapping
// address ranges.
// FIXME: The current implementation is not thread-safe and the caller must
// ensure that the address space being operated on is locked. This may possibly
// require different modules directly operating on a page-table to share a
// lock.
//
// A caller must always flag the start of a set of one or more map and unmap
// operations by calling the start function.  If synchronisation of updates
// with the page-table walkers (either locally or on other CPUs) can be
// deferred, then it will be deferred until a call is made to the corresponding
// commit function. The caller must always call the commit function before
// relying in any way on the updates having taken effect.
//
// In multi-processor systems, remote CPUs or IOMMU-protected devices using
// an affected address space might either continue to see the old mapping, or
// see a temporarily invalid mapping (which may extend outside the specified
// address range), especially if the mapping change has caused a page to
// change sizes. This will not occur for any memory access that the commit()
// call _inter-thread happens before_ (as defined by C18), or for any call
// to lookup() or lookup_range() that completes after an RCU grace period has
// elapsed after the commit function returns.
//
// In general, any function in this file that returns error_t or bool will
// validate its arguments and fail with an error code or false result if
// they are invalid. Any function that returns void will panic on invalid
// inputs.

//
// Hypervisor page table management.
//
// In order to correctly attribute ownership of page table levels, the caller
// must avoid allocating page table levels in one partition if they might be
// subsequently freed into another partition. This can be done by selecting
// some allocation block size that mappings will never cross, and pre-
// allocating page table levels down to that block size from a global pool.
//
// Mappings with NONE access type may be used to indicate that the hypervisor
// should only be permitted to access the mapping on behalf of a VM, and will
// take specific action to enable and disable such accesses (e.g. clearing and
// setting PAN on ARMv8.1). Not all architectures support this; a different
// technique must be used for useraccess on those that do not.
//

// Returns false if the specified address is unmapped.
bool
pgtable_hyp_lookup(uintptr_t virt, paddr_t *mapped_base, size_t *mapped_size,
		   pgtable_hyp_memtype_t *mapped_memtype,
		   pgtable_access_t	 *mapped_access);

// Returns false if there is no mapping in the specified range. If a mapping
// is found and can be efficiently determined to be the last mapping in the
// range, the boolean *remainder_unmapped will be set to true; otherwise it
// will be unchanged. Note that the returned mapping may extend beyond the
// specified range.
bool
pgtable_hyp_lookup_range(uintptr_t virt_base, size_t virt_size,
			 uintptr_t *mapped_virt, paddr_t *mapped_phys,
			 size_t		       *mapped_size,
			 pgtable_hyp_memtype_t *mapped_memtype,
			 pgtable_access_t      *mapped_access,
			 bool		       *remainder_unmapped);

// Creates page table levels owned by the given partition which are able to
// directly map entries covering the given size, but don't actually map
// anything. This is intended for preallocating levels using the hypervisor's
// private allocator, but might be more generally useful.
error_t
pgtable_hyp_preallocate(partition_t *partition, uintptr_t virt, size_t size);

extern opaque_lock_t pgtable_hyp_map_lock;

// Flag the start of one of more map or unmap calls.
void
pgtable_hyp_start(void) ACQUIRE_LOCK(pgtable_hyp_map_lock);

// Creates a new mapping, possibly merging adjacent mappings into large blocks.
//
// An error will be returned if there are any existing mappings in the given
// region that are not exactly identical to the requested mapping.
//
// If merge_limit is nonzero, then this will attempt to merge page table levels
// that become congruent as a result of this operation into larger pages, as
// long as the new size is less than merge_limit. Any page table levels freed by
// this will be freed into the specified partition, so merge_limit should be no
// greater than preserved_prealloc would be for an unmap operation in the same
// region.
//
// Note that this operation may cause transient translation aborts or TLB
// conflict aborts in the affected range or within a merge_limit aligned region
// around it. The caller is responsible for not making calls with a nonzero
// merge_limit that might have those effects on the hypervisor code, the stack
// of any hypervisor thread, or any other address that may be touched during the
// handling of a transient hypervisor fault.
error_t
pgtable_hyp_map_merge(partition_t *partition, uintptr_t virt, size_t size,
		      paddr_t phys, pgtable_hyp_memtype_t memtype,
		      pgtable_access_t access, vmsa_shareability_t shareability,
		      size_t merge_limit) REQUIRE_LOCK(pgtable_hyp_map_lock);

// Creates a new mapping. No attempt will be made to merge adjacent mappings.
static inline error_t
pgtable_hyp_map(partition_t *partition, uintptr_t virt, size_t size,
		paddr_t phys, pgtable_hyp_memtype_t memtype,
		pgtable_access_t access, vmsa_shareability_t shareability)
	REQUIRE_LOCK(pgtable_hyp_map_lock)
{
	return pgtable_hyp_map_merge(partition, virt, size, phys, memtype,
				     access, shareability, 0U);
}

// Creates a new mapping, replacing any existing mappings in the region, and
// possibly merging adjacent mappings into large blocks. The merge_limit
// argument has the same semantics as for @see pgtable_hyp_map_merge().
error_t
pgtable_hyp_remap_merge(partition_t *partition, uintptr_t virt, size_t size,
			paddr_t phys, pgtable_hyp_memtype_t memtype,
			pgtable_access_t    access,
			vmsa_shareability_t shareability, size_t merge_limit)
	REQUIRE_LOCK(pgtable_hyp_map_lock);

// Creates a new mapping, replacing any existing mappings in the region.
static inline error_t
pgtable_hyp_remap(partition_t *partition, uintptr_t virt, size_t size,
		  paddr_t phys, pgtable_hyp_memtype_t memtype,
		  pgtable_access_t access, vmsa_shareability_t shareability)
	REQUIRE_LOCK(pgtable_hyp_map_lock)
{
	return pgtable_hyp_remap_merge(partition, virt, size, phys, memtype,
				       access, shareability, 0U);
}

// Removes all mappings in the given range. Frees levels into the specified
// partition's allocators, but only if they cannot be used to create mappings
// of the size preserved_prealloc.  The preserved_prealloc field can therefore
// be used to prevent freeing of levels created by a previous hyp_preallocate
// call to the specified partition.
void
pgtable_hyp_unmap(partition_t *partition, uintptr_t virt, size_t size,
		  size_t preserved_prealloc) REQUIRE_LOCK(pgtable_hyp_map_lock);
#define PGTABLE_HYP_UNMAP_PRESERVE_ALL	0U
#define PGTABLE_HYP_UNMAP_PRESERVE_NONE util_bit((sizeof(uintptr_t) * 8U) - 1U)

// Ensure that all previous hypervisor map and unmap calls are complete.
void
pgtable_hyp_commit(void) RELEASE_LOCK(pgtable_hyp_map_lock);

//
// VM page table management.
//
// VM page tables don't have the same constraints for level preallocation &
// freeing because they are always entirely owned by one partition.
//
error_t
pgtable_vm_init(partition_t *partition, pgtable_vm_t *pgtable, vmid_t vmid);

// Free all resources for page table
void
pgtable_vm_destroy(partition_t *partition, pgtable_vm_t *pgtable);

// Returns false if the specified address is unmapped.
bool
pgtable_vm_lookup(pgtable_vm_t *pgtable, vmaddr_t virt, paddr_t *mapped_base,
		  size_t *mapped_size, pgtable_vm_memtype_t *mapped_memtype,
		  pgtable_access_t *mapped_vm_kernel_access,
		  pgtable_access_t *mapped_vm_user_access);

// Returns false if there is no mapping in the specified range. If a mapping
// is found and can be efficiently determined to be the last mapping in the
// range, the boolean *remainder_unmapped will be set to true; otherwise it
// will be unchanged. Note that the returned mapping may extend beyond the
// specified range.
bool
pgtable_vm_lookup_range(pgtable_vm_t *pgtable, vmaddr_t virt_base,
			size_t virt_size, vmaddr_t *mapped_virt,
			paddr_t *mapped_phys, size_t *mapped_size,
			pgtable_vm_memtype_t *mapped_memtype,
			pgtable_access_t     *mapped_vm_kernel_access,
			pgtable_access_t     *mapped_vm_user_access,
			bool		     *remainder_unmapped);

extern opaque_lock_t pgtable_vm_map_lock;

// Flag the start of one of more map or unmap calls.
void
pgtable_vm_start(pgtable_vm_t *pgtable) ACQUIRE_LOCK(pgtable)
	ACQUIRE_LOCK(pgtable_vm_map_lock);

// Creates a new mapping.
//
// If try_map is true, it returns an error if any existing mappings are present
// in the range that are not exactly identical to the requested mapping. If
// try_map is false, any existing mappings in the specified range are removed or
// updated.
//
// If allow_merge is true, then any page table levels that become congruent as a
// result of this operation will be merged into larger pages.
//
// pgtable_vm_start() must have been called before this call.
error_t
pgtable_vm_map(partition_t *partition, pgtable_vm_t *pgtable, vmaddr_t virt,
	       size_t size, paddr_t phys, pgtable_vm_memtype_t memtype,
	       pgtable_access_t vm_kernel_access,
	       pgtable_access_t vm_user_access, bool try_map, bool allow_merge)
	REQUIRE_LOCK(pgtable) REQUIRE_LOCK(pgtable_vm_map_lock);

// Removes all mappings in the given range. pgtable_vm_start() must have been
// called before this call.
void
pgtable_vm_unmap(partition_t *partition, pgtable_vm_t *pgtable, vmaddr_t virt,
		 size_t size) REQUIRE_LOCK(pgtable)
	REQUIRE_LOCK(pgtable_vm_map_lock);

// Remove only mappings that match the physical address within the specified
// range
void
pgtable_vm_unmap_matching(partition_t *partition, pgtable_vm_t *pgtable,
			  vmaddr_t virt, paddr_t phys, size_t size)
	REQUIRE_LOCK(pgtable) REQUIRE_LOCK(pgtable_vm_map_lock);

// Ensure that all previous VM map and unmap calls are complete.
void
pgtable_vm_commit(pgtable_vm_t *pgtable) RELEASE_LOCK(pgtable)
	RELEASE_LOCK(pgtable_vm_map_lock);

// Set VTCR and VTTBR registers with page table vtcr and vttbr bitfields values.
void
pgtable_vm_load_regs(pgtable_vm_t *vm_pgtable);

// Validate page table access
bool
pgtable_access_check(pgtable_access_t access, pgtable_access_t access_check);

// Mask a pagetable access
pgtable_access_t
pgtable_access_mask(pgtable_access_t access, pgtable_access_t access_mask);

// Check if input page table accesses are equal
bool
pgtable_access_is_equal(pgtable_access_t access, pgtable_access_t access_check);

// Get combined access
pgtable_access_t
pgtable_access_combine(pgtable_access_t access1, pgtable_access_t access2);

```

`hyp/interfaces/pgtable/pgtable.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

interface pgtable

// Event broadcasting a mapping change to a VM page-table.
event pgtable_vm_commit
	param pgtable: pgtable_vm_t *

```

`hyp/interfaces/pgtable/pgtable.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// Generic hypervisor address space memory types.
//
// We define only eight memory types to match the ARMv8 architecture stage-1
// page-table MAIR setting.
define pgtable_hyp_memtype enumeration(explicit) {
	// Non-cacheable memory, but no ordering guarantees
	WRITECOMBINE = 0;
	// Writeback-cacheable memory; allocate on reads and writes
	WRITEBACK = 1;
	// Writeback-cacheable in CPU caches, but not in external caches
	OUTER_WRITECOMBINE = 2;
	// Writethrough-cacheable memory
	WRITETHROUGH = 3;
	// Non-cacheable, and prevents speculative accesses
	NOSPEC = 4;
	// Like NOSPEC, but prevents gathering of adjacent / repeated accesses
	NOSPEC_NOCOMBINE = 5;
	// Like NOSPEC_NOCOMBINE, but prevents reordering by the CPU
	DEVICE = 6;
	// Like DEVICE, but prevents reordering across the whole memory system
	STRONG = 7;
};

// VM address space memory types.
//
// These are non-generic as (at least) on ARM, the memory types in stage-2
// page-tables are not absolute types, but instead are modifiers of the stage-1
// output memory type.
define pgtable_vm_memtype public enumeration(explicit);

// Generic mapping access permissions enumeration
define pgtable_access public enumeration(explicit) {
	NONE = 0;
	X = 1;
	W = 2;
	R = 4;
	RX = 5;
	RW = 6;
	RWX = 7;
};

define pgtable_vm structure(lockable) {
};

extend error enumeration {
	EXISTING_MAPPING = 200;
};

```

`hyp/interfaces/platform/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

events platform.ev
types platform.tc

```

`hyp/interfaces/platform/include/platform_cpu.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// Platform wrappers for SMP support.

// Check whether a cpu_index maps to cpu that exists. Note however, a CPU that
// exists may not be functional.
bool
platform_cpu_exists(cpu_index_t cpu);

// Power on the specified CPU.
error_t
platform_cpu_on(cpu_index_t cpu);

// Power off the calling CPU. Returns after the CPU is powered on again by a
// platform_cpu_on() call on another CPU.
void
platform_cpu_off(void) REQUIRE_PREEMPT_DISABLED;

// The system and CPUs are reset, and will restart from the firmware/bootloader.
void
platform_system_reset(void) REQUIRE_PREEMPT_DISABLED;

// Suspend the calling CPU until a wakeup event occurs.
//
// The argument is a platform-specific power state value which represents the
// deepest sleep state this call is permitted to enter. On success, the result
// is true if the CPU woke from a power-off state, and false if it either
// woke from a retention state or returned without suspending due to a pending
// wakeup event.
//
// This may fail with ERROR_ARGUMENT_INVALID if the power state argument is
// not understood or not permitted by the platform, or ERROR_DENIED if the
// attempt to sleep was aborted due to a pending wakeup on another CPU in the
// same power domain.
bool_result_t
platform_cpu_suspend(platform_power_state_t power_state)
	REQUIRE_PREEMPT_DISABLED;

// Set the suspend mode used by the hypervisor
//
// This function can only return OK if the following conditions are met:
// If switching from PC to OSI mode:
//	- All cores are either Running, OFF (using CPU_OFF or not booted yet),
//	or Suspended (using CPU_DEFAULT_SUSPEND)
//	- None of the processors has called CPU_SUSPEND since the last change of
//	mode or boot.
// If switching from OSI to PC mode: all cores other than the calling one are
// OFF (using CPU_OFF or not booted yet)
error_t
platform_psci_set_suspend_mode(psci_mode_t mode);

#if defined(PLATFORM_PSCI_DEFAULT_SUSPEND)
// Suspend the calling CPU until a wakeup event occurs. Similar to cpu suspend,
// but the caller does not need to specify a power state parameter.
bool_result_t
platform_cpu_default_suspend(void) REQUIRE_PREEMPT_DISABLED;
#endif

#if defined(ARCH_ARM)
platform_mpidr_mapping_t
platform_cpu_get_mpidr_mapping(void);

MPIDR_EL1_t
platform_cpu_map_index_to_mpidr(const platform_mpidr_mapping_t *mapping,
				index_t				index);

index_t
platform_cpu_map_mpidr_to_index(const platform_mpidr_mapping_t *mapping,
				MPIDR_EL1_t			mpidr);

bool
platform_cpu_map_mpidr_valid(const platform_mpidr_mapping_t *mapping,
			     MPIDR_EL1_t		     mpidr);

MPIDR_EL1_t
platform_cpu_index_to_mpidr(index_t index);

index_t
platform_cpu_mpidr_to_index(MPIDR_EL1_t mpidr);

bool
platform_cpu_mpidr_valid(MPIDR_EL1_t mpidr);

core_id_t
platform_cpu_get_coreid(MIDR_EL1_t midr);
#endif

#if defined(ARCH_ARM_FEAT_BTI)
bool
platform_cpu_bti_enabled(void);
#endif

uint32_t
platform_cpu_stack_size(void);

```

`hyp/interfaces/platform/include/platform_features.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

platform_cpu_features_t
platform_get_cpu_features(void);

```

`hyp/interfaces/platform/include/platform_ipi.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// Platform routines for raising and handling hardware IPIs.
//
// There are two variants of this API: one for when the platform has enough
// separate IPI lines to allocate one per registered reason, and another for
// when it doesn't. They are similar, but the latter does not take reason
// arguments and does not have mask and unmask calls.
//
// The semantics of the individual calls are similar to the high-level API in
// ipi.h, but they are not expected to provide any mechanism for fast-path
// delivery without raising a hardware interrupt, nor for multiplexing when
// there are more possible IPI reasons than physical IPI lines.

#include <hypconstants.h>

#if PLATFORM_IPI_LINES > ENUM_IPI_REASON_MAX_VALUE
void
platform_ipi_others(ipi_reason_t ipi);

void
platform_ipi_one(ipi_reason_t ipi, cpu_index_t cpu);

void
platform_ipi_mask(ipi_reason_t ipi);

void
platform_ipi_unmask(ipi_reason_t ipi);

void
platform_ipi_clear(ipi_reason_t ipi);
#else
void
platform_ipi_others(void);

void
platform_ipi_one(cpu_index_t cpu);
#endif

```

`hyp/interfaces/platform/include/platform_irq.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// Platform routines for handling hardware IRQs.
//
// These routines presume that the interrupt controller implements automatic
// masking of all interrupts until the end of the handler, and selectively
// extending that masking after the end of the interrupt handler for interrupts
// forwarded to a VM, similar to an ARM GICv2 or later with EOImode=1. If the
// interrupt controller does not implement these semantics, the platform driver
// must emulate them.
//
// The calling code should not assign any particular meaning to IRQ numbers,
// beyond assuming that they are not greater than the value returned by
// platform_irq_max().
//
// Note that the platform driver may handle some IRQs internally; for example,
// to signal IPIs, or to support a debug console. Such IRQs are not exposed to
// this interface.

// Obtain the maximum valid IRQ number.
//
// This returns an irq_t value which is the upper bound for valid irq_t values
// on this platform. Note that it is not necessarily the case that all irq_t
// values less than this value are valid; see platform_irq_check() below.
//
// This function may be called by a default-priority boot_cold_init handler. If
// the driver needs initialisation to determine this value, it should subscribe
// to boot_cold_init with a positive priority.
irq_t
platform_irq_max(void);

// Check whether a specific IRQ number is valid for handler registration.
//
// This function returns ERROR_DENIED for any IRQ number that is reserved by the
// platform driver or by a higher privileged level, ERROR_ARGUMENT_INVALID for
// any IRQ number that is greater than the value returned by platform_irq_max()
// or is otherwise known not to be implemented by the hardware, and OK for any
// other IRQ number.
error_t
platform_irq_check(irq_t irq);

// Check whether a specific IRQ number needs to use the percpu and/or local
// variants of the enable and disable APIs.
//
// Note that a true result does not necessarily mean that the IRQ number can be
// used independently on multiple CPUs.
//
// This function must only be called on IRQ numbers that have previously
// returned an OK result from platform_irq_check().
bool
platform_irq_is_percpu(irq_t irq);

// Enable delivery of a specified IRQ, which must not be per-CPU.
void
platform_irq_enable_shared(irq_t irq);

// Enable delivery of a specified per-CPU IRQ, on the calling CPU.
void
platform_irq_enable_local(irq_t irq) REQUIRE_PREEMPT_DISABLED;

// Enable delivery of a specified per-CPU IRQ, on the specified CPU.
//
// Note that some platforms are unable to implement this function if the CPU
// index is not that of the calling CPU.
void
platform_irq_enable_percpu(irq_t irq, cpu_index_t cpu);

// Disable delivery of a specified IRQ, which must not be per-CPU.
//
// On some platforms, this function must busy-wait until the IRQ controller has
// acknowledged that the interrupt is disabled.
void
platform_irq_disable_shared(irq_t irq);

// Disable delivery of a specified per-CPU IRQ, on the calling CPU.
//
// On some platforms, this function must busy-wait until the IRQ controller has
// acknowledged that the interrupt is disabled.
void
platform_irq_disable_local(irq_t irq) REQUIRE_PREEMPT_DISABLED;

// Disable delivery of a specified per-CPU IRQ, on the calling CPU, without
// waiting for the physical interrupt controller to acknowledge that delivery is
// disabled.
//
// This may be called if the in-hypervisor driver for the IRQ needs to disable
// it in performance-critical code (e.g. a context switch) and is prepared to
// handle any spurious interrupts that may occur as a result of not waiting.
void
platform_irq_disable_local_nowait(irq_t irq) REQUIRE_PREEMPT_DISABLED;

// Disable delivery of a specified per-CPU IRQ, on the specified CPU.
//
// On some platforms, this function must busy-wait until the IRQ controller has
// acknowledged that the interrupt is disabled.
//
// Note that some platforms are unable to implement this function if the CPU
// index is not that of the calling CPU.
void
platform_irq_disable_percpu(irq_t irq, cpu_index_t cpu);

// Acknowledge and activate an IRQ.
//
// This function acknowledges the highest-priority pending IRQ and returns its
// IRQ number. If there are multiple IRQs with equal highest priority, it
// selects one to acknowledge arbitrarily. Delivery of the acknowledged IRQ is
// disabled until the next platform_irq_deactivate() call for that IRQ, unless a
// platform-specific mechanism deactivates the IRQ.
//
// If there is an interrupt to acknowledge, but it is directed to the platform
// driver itself (e.g. to signal an IPI), the result will be set to ERROR_RETRY.
//
// If there are no pending interrupts left to acknowledge, the result will be
// set to ERROR_IDLE.
//
// This function must only be called from an interrupt handler. If it returns an
// IRQ number, that number must be passed to platform_irq_priority_drop() before
// the interrupt handler returns.
irq_result_t
platform_irq_acknowledge(void) REQUIRE_PREEMPT_DISABLED;

// De-prioritise an acknowledged IRQ.
//
// This function lowers the effective priority of an acknowledged IRQ, allowing
// its handling to be deferred until after the interrupt handler returns without
// blocking delivery of further interrupts.
//
// This must only be called for IRQs returned by platform_irq_acknowledge().
// Additionally, the specified IRQ must be the one most recently returned by
// that function on the current physical CPU, after excluding any IRQs that have
// already been passed to this function.
//
// This call must be followed (eventually) by a platform_irq_deactivate() call,
// unless the IRQ is forwarded to a VCPU and the platform provides a mechanism
// for forwarded IRQs to be deactivated directly by the VCPU.
void
platform_irq_priority_drop(irq_t irq);

// Deactivate an acknowledged IRQ.
//
// This function re-enables delivery of the specified interrupt after it was
// previously de-prioritised by platform_irq_priority_drop(). For a per-CPU IRQ
// it must be called on the same physical CPU that acknowledged the IRQ;
// otherwise it may be called on any CPU.
//
// Note that the platform may provide an alternative mechanism for performing
// this operation for IRQs that are forwarded to a VCPU via a hardware virtual
// IRQ delivery mechanism. In that case, the hypervisor need not call this
// function for any forwarded IRQ that uses that mechanism.
void
platform_irq_deactivate(irq_t irq);

// Deactivate an acknowledged per-CPU IRQ on a specified physical CPU.
//
// This works the same way as platform_irq_deactivate(), but allows per-CPU
// IRQs acknowledged on one physical CPU to be deactivated from a different
// physical CPU.
//
// Note that this may be slower than platform_irq_deactivate() for such IRQs,
// and may not be possible to implement without IPIs.
void
platform_irq_deactivate_percpu(irq_t irq, cpu_index_t cpu);

// Set trigger of per-CPU IRQ on a specified physical CPU.
// FIXME: add more details descriptions.
irq_trigger_result_t
platform_irq_set_mode_percpu(irq_t irq, irq_trigger_t trigger, cpu_index_t cpu);

// Support for message-signalled interrupts. These are dynamically allocated
// and programmed into the source devices, possibly after being mapped to a
// hardware ID that is tagged with a source device identifier (e.g. a PCI bus
// responder ID).
#if IRQ_HAS_MSI

// The base of the platform's MSI range.
//
// The platform is presumed to support message-signalled interrupts only for
// some contiguous limited range of IRQ numbers. This constant must be set to
// the first IRQ number in that range.
extern const irq_t platform_irq_msi_base;

// Obtain the platform's maximum MSI number.
//
// The platform is presumed to support message-signalled interrupts only for
// some contiguous limited range of IRQ numbers. This function should return
// the last IRQ number in that range.
//
// Note that the IRQ framework will configure a bitmap allocator covering the
// whole MSI ID range. Platforms that support a very large MSI range may
// constrain this to less than the true value to prevent the IRQ framework
// unnecessarily allocating a large amount of memory.
irq_t
platform_irq_msi_max(void);

// Obtain a list of known MSI source devices.
//
// The index is the controller index, which must be between 0 and
// PLATFORM_MSI_CONTROLLER_COUNT.
//
// These are the hardware-tagged devices attached to the specified MSI
// controller. For each device, this reports the tag, and the maximum number of
// vectors that may be required by the device. If this number is not known, 32
// is probably a reasonable choice.
//
// If the platform does not support hardware-tagged devices, it should return a
// single element with id 0.
extern const platform_msi_controller_t *
platform_irq_msi_devices(platform_msi_controller_id_t ctrl_index);

#endif // IRQ_HAS_MSI

// Query the IRQ delivery class of a CPU.
//
// In configurations where the virtual interrupt controller implements automatic
// load balancing of interrupts (e.g. GICv3 1-of-N delivery mode), the CPUs can
// be divided into two classes, and load-balanced IRQs can be separately enabled
// for each class.
//
// This is similar to a feature implemented in hardware by the GIC-600 and
// GIC-700; on platforms that have those interrupt controllers, this function
// should reflect the classes configured in the physical GICR_CLASSR registers
// (which are only accessible to EL3).
//
// On platforms where this distinction is not meaningful, this function should
// return 0U.
index_t
platform_irq_cpu_class(cpu_index_t cpu);

```

`hyp/interfaces/platform/include/platform_mem.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// Add platform specific memory to the root VM partition.
//
// The root VM partition is created without any heap (besides possibly a
// minimal amount of memory provided by the hyp_partition to seed it). The
// platform must provide the rest of the initial memory for the root partition.
void
platform_add_root_heap(partition_t *partition) REQUIRE_PREEMPT_DISABLED;

// Return platform DDR (normal memory) information.
// May only be called after platform_ram_probe().
platform_ram_info_t *
platform_get_ram_info(void);

// Should be called once on boot to probe RAM information.
// This function expects to be able map to the hyp pagetable.
error_t
platform_ram_probe(void);

#if defined(ARCH_ARM)
// Check whether a platform VM page table is undergoing a break-before-make.
//
// This function is called while handling translation aborts in an address space
// that has the platform_pgtable flag set. It should return true if there is any
// possibility that an in-progress update of the VM page table has unmapped
// pages temporarily as part of an architecturally required break-before-make
// sequence while changing the block size of an existing mapping.
//
// If this function returns true, any translation abort in a read-only address
// space will be retried until either the fault is resolved or this function
// returns false. The platform code must therefore only return true from this
// function for a bounded period of time.
//
// Note that it is not necessary for the platform code to handle the race
// between completion of the break-before-make and reporting of a fault that
// occurred before it completed. That race will be handled by the caller.
bool
platform_pgtable_undergoing_bbm(void);
#endif

```

`hyp/interfaces/platform/include/platform_pmu.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

void
platform_pmu_hw_irq_deactivate(void) REQUIRE_PREEMPT_DISABLED;

bool
platform_pmu_is_hw_irq_pending(void);

```

`hyp/interfaces/platform/include/platform_prng.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

error_t
platform_get_serial(uint32_t data[4]);

error_t
platform_get_random32(uint32_t *data);

error_t
platform_get_rng_uuid(uint32_t data[4]);

error_t
platform_get_entropy(platform_prng_data256_t *data);

```

`hyp/interfaces/platform/include/platform_psci.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause
// Checks that the suspend state is valid
//
// This function checks that the pcpu supports the cpu and cluster level
// state specified. If the specified state is not valid, then it returns
// PSCI_RET_INVALID_PARAMETERS.
psci_ret_t
platform_psci_suspend_state_validation(psci_suspend_powerstate_t suspend_state,
				       cpu_index_t cpu, psci_mode_t psci_mode);

// Gets the cpu state from the suspend power state
psci_cpu_state_t
platform_psci_get_cpu_state(psci_suspend_powerstate_t suspend_state);

// Sets cpu state to the stateid of the suspend power state
void
platform_psci_set_cpu_state(psci_suspend_powerstate_t *suspend_state,
			    psci_cpu_state_t	       cpu_state);

// Returns that shallowest state between two cpu states
psci_cpu_state_t
platform_psci_shallowest_cpu_state(psci_cpu_state_t state1,
				   psci_cpu_state_t state2);

// Returns the deepest cpu state supported by a cpu
psci_cpu_state_t
platform_psci_deepest_cpu_state(cpu_index_t cpu);

// Returns the deepest cpu-level suspend state id supported by a cpu
psci_suspend_powerstate_stateid_t
platform_psci_deepest_cpu_level_stateid(cpu_index_t cpu);

// Returns true if cpu state is in active state
bool
platform_psci_is_cpu_active(psci_cpu_state_t cpu_state);

// Returns true if cpus is in power collapse state
bool
platform_psci_is_cpu_poweroff(psci_cpu_state_t cpu_state);

// Returns true if cluster state is in active state
bool
platform_psci_is_cluster_active(psci_cluster_state_L3_t cluster_state);

// Returns the cluster indices
uint32_t
platform_psci_get_cluster_index(cpu_index_t cpu);

// Returns the start index of children in hierarchy/counts based on level and
// cpu
error_t
platform_psci_get_index_by_level(cpu_index_t cpu, uint32_t *start_idx,
				 uint32_t *children_counts, uint32_t level);

#if !defined(PSCI_AFFINITY_LEVELS_NOT_SUPPORTED) ||                            \
	!PSCI_AFFINITY_LEVELS_NOT_SUPPORTED
// Checks if cluster state corresponds to a power off state
bool
platform_psci_is_cluster_state_poweroff(psci_suspend_powerstate_t suspend_state);

// Returns true if cluster state is in active state
bool
platform_psci_is_cluster_active(psci_cluster_state_L3_t cluster_state);

// Checks if cluster state corresponds to a retention state
bool
platform_psci_is_cluster_state_retention(
	psci_suspend_powerstate_t suspend_state);

// Gets the cluster state from the suspend power state
psci_cluster_state_t
platform_psci_get_cluster_state(psci_suspend_powerstate_t suspend_state);

// Sets cluster state to the stateid of the suspend power state
void
platform_psci_set_cluster_state(psci_suspend_powerstate_t *suspend_state,
				psci_cluster_state_t	   cluster_state);

#if (PLATFORM_MAX_HIERARCHY == 2)
// Gets the system state from the suspend power state
psci_system_state_t
platform_psci_get_system_state(psci_suspend_powerstate_t suspend_state);

// Sets system state to the stateid of the suspend power state
void
platform_psci_set_system_state(psci_suspend_powerstate_t *suspend_state,
			       psci_system_state_t	  system_state);
#endif

// Returns the suspend level of the last cpu
index_t
platform_psci_get_last_cpu_level(psci_suspend_powerstate_t suspend_state);

// Sets the suspend level of the last cpu
void
platform_psci_set_last_cpu_level(psci_suspend_powerstate_t *suspend_state,
				 index_t		    last_cpu);

// Returns that shallowest state between two cluster states
psci_cluster_state_t
platform_psci_shallowest_cluster_state(psci_cluster_state_t state1,
				       uint16_t		    state2);

// Returns the deepest cluster-level suspend state supported by the system
psci_cluster_state_t
platform_psci_deepest_cluster_state(void);

// Returns the deepest cluster-level suspend state id supported by a cpu
psci_suspend_powerstate_stateid_t
platform_psci_deepest_cluster_level_stateid(cpu_index_t cpu);

#endif

```

`hyp/interfaces/platform/include/platform_security.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// Gets the device security state
//
// If this function returns true, meaning that the device is secure, then
// debugging is not permitted.
bool
platform_security_state_debug_disabled(void);

bool
platform_security_state_hlos_debug_disabled(void);

```

`hyp/interfaces/platform/include/platform_timer.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

ticks_t
platform_timer_get_timeout(void);

void
platform_timer_cancel_timeout(void) REQUIRE_PREEMPT_DISABLED;

// Must be called with preempt_disabled
void
platform_timer_set_timeout(ticks_t timeout) REQUIRE_PREEMPT_DISABLED;

uint32_t
platform_timer_get_frequency(void);

ticks_t
platform_timer_get_current_ticks(void);

ticks_t
platform_timer_convert_ns_to_ticks(nanoseconds_t ns);

nanoseconds_t
platform_timer_convert_ticks_to_ns(ticks_t ticks);

ticks_t
platform_timer_convert_ms_to_ticks(milliseconds_t ms);

milliseconds_t
platform_timer_convert_ticks_to_ms(ticks_t ticks);

void
platform_timer_ndelay(nanoseconds_t duration);

```

`hyp/interfaces/platform/include/platform_timer_lp.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

void
platform_timer_lp_set_timeout(ticks_t timeout) REQUIRE_PREEMPT_DISABLED;

ticks_t
platform_timer_lp_get_timeout(void);

void
platform_timer_lp_cancel_timeout(void) REQUIRE_PREEMPT_DISABLED;

uint32_t
platform_timer_lp_get_frequency(void);

ticks_t
platform_timer_lp_get_current_ticks(void);

void
platform_timer_lp_visibility(bool visible);

void
platform_timer_lp_set_timeout_and_route(ticks_t timeout, cpu_index_t cpu_index)
	REQUIRE_PREEMPT_DISABLED;

```

`hyp/interfaces/platform/platform.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <hypconstants.h>

interface platform

handled_event platform_ipi
#if PLATFORM_IPI_LINES > ENUM_IPI_REASON_MAX_VALUE
	param ipi: ipi_reason_t
#endif

event platform_timer_expiry

event platform_timer_lp_expiry

event platform_pmu_counter_overflow

event platform_watchdog_bark

```

`hyp/interfaces/platform/platform.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

define platform_prng_data256 structure {
	word array(8) uint32;
};

define PLATFORM_RAM_RANGES_MAX	constant type count_t = 32;

define platform_ram_range structure {
	base		type paddr_t;
	size		size;
};

define platform_ram_info structure {
	num_ranges	type count_t;
	ram_range	array(PLATFORM_RAM_RANGES_MAX)
			structure platform_ram_range;
};

define platform_cpu_features bitfield<32> {
};

#if IRQ_HAS_MSI

define platform_msi_controller_id_t newtype uint32;
define platform_msi_event_id_t newtype uint32;
define platform_msi_device_id_t newtype uint32;

define platform_msi_id bitfield<64> {
	31:0	event_id	type platform_msi_event_id_t;
	53:32	device_id	type platform_msi_device_id_t;
	63:54	its_index	type platform_msi_controller_id_t;
};

define platform_msi_device structure {
	device_id	type platform_msi_device_id_t;
	max_event	type platform_msi_event_id_t;
};

define platform_msi_controller structure {
	num_devices	type count_t;
	devices		pointer structure platform_msi_device;
};

#endif // IRQ_HAS_MSI

#if defined(ARCH_ARM)
define platform_mpidr_mapping structure {
	aff_shift	array(4) type count_t;
	aff_mask	array(4) uint8;
	uniprocessor	bool;
	multi_thread	bool;
};
#endif

```

`hyp/interfaces/power/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

events power.ev
types power.tc

```

`hyp/interfaces/power/include/power.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// Obtain a pointer to the array of CPU states.
//
// This array may be modified by a remote CPU at any time. No synchronisation is
// available. Therefore this must only be used for debug purposes, e.g. to
// expose the array in crash dumps.
const cpu_power_state_array_t *
power_get_cpu_states_for_debug(void);

// Vote to keep a CPU powered on.
//
// This may return ERROR_FAILURE if the specified CPU cannot be powered on for
// platform-specific reasons (such as the CPU being disabled by fuses or a
// hardware failure while powering it on), or ERROR_ARGUMENT_INVALID if the CPU
// index is out of range.
error_t
power_vote_cpu_on(cpu_index_t cpu);

// Revoke an earlier vote to keep a CPU powered on.
//
// This must only be called after a successful power_vote_cpu_on(). Calling it
// on the only powered-on CPU may leave the system in an unrecoverable state;
// it is the caller's responsibility to avoid this.
void
power_vote_cpu_off(cpu_index_t cpu);

```

`hyp/interfaces/power/power.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

interface power

// Triggered on a CPU that has been brought online as a result of an explicit
// power management call by another CPU.
//
// This cannot be triggered on the first running CPU in the system (either on
// the boot CPU, or on any other CPU that is the first to wake from a power-
// off suspend state), and therefore is never triggered in uniprocessor
// configurations.
//
// This also cannot be triggered on a CPU that is waking automatically from a
// power-off suspend state due to an interrupt or other wakeup event.
event power_cpu_online

// Triggered when a CPU takes itself offline.
//
// This cannot be triggered on the last running CPU in the system, and therefore
// is never triggered in uniprocessor configurations.
event power_cpu_offline

// Triggered when a CPU, which may the only online CPU in the system, is about
// to enter a low-power state from which it can wake automatically in response
// to a platform-defined wakeup event. Entry to the low-power state is aborted
// if any registered handler returns an error.
//
// The may_poweroff argument is true if the CPU will attempt to enter a
// power-off state. If this is true, and the power-off state is successfully
// entered, the state of any device or component within the CPU's power domain
// may be lost before the corresponding resume event. If that occurs, the
// boot_cpu_warm_init event will be triggered prior to power_cpu_resume, and
// the was_poweroff argument to the power_cpu_resume event will be true.
//
// The last_cpu argument is true if this is the last CPU to go to suspend. If
// so, a system-level suspend may occur. However, beware that there is no
// synchronisation between this event and a subsequent power_cpu_resume on
// another CPU. If synchronisation is required, use power_system_suspend
// instead.
//
// If a race with a wakeup on another CPU is detected, this should return
// ERROR_DENIED. If the suspend should be delayed with a WFI idle until some
// event occurs, this should return ERROR_BUSY. Any other error is treated as
// fatal.
setup_event power_cpu_suspend
	param state: platform_power_state_t
	param may_poweroff: bool
	param last_cpu: bool
	return: error_t = OK
	success: OK

// Triggered when a CPU, which may be the only online CPU in the system,
// returns from a low-power state. This is typically due to a wakeup event,
// but many platforms also allow this to occur spuriously.
//
// The was_poweroff argument is true if the CPU had successfully suspended
// into a power-off state before resuming. It is false if the CPU either
// suspended into a retention state, or failed to suspend at all; this may be
// the case even if the requested suspend state was a power-off state.
//
// The first_cpu argument is true if this is the first cpu to wake up.
// However, beware that there is no synchronisation between this event and a
// subsequent power_cpu_suspend on another CPU, even with last_cpu set. If
// synchronisation is required, use power_system_resume instead.
//
// In many cases, handlers for this event will be the same as unwinders for
// power_cpu_suspend; however, note that the was_poweroff argument has a
// slightly different meaning for resume and suspend-unwind.
event power_cpu_resume
	param was_poweroff: bool
	param first_cpu: bool

// Triggered when the system is about to enter a system-level low-power state
// from which it can wake automatically in response to a platform-defined
// wakeup event. Entry to the low-power state is aborted if any registered
// handler returns an error.
//
// This is called during power_cpu_suspend when the last_cpu argument is true.
// It is serialised with power_system_resume.
//
// If a race with a wakeup on another CPU is detected, this should return
// ERROR_DENIED. If the suspend should be delayed with a WFI idle until some
// event occurs, this should return ERROR_BUSY. Any other error is treated as
// fatal.
setup_event power_system_suspend
	param state: platform_power_state_t
	return: error_t = OK
	success: OK

// Triggered when the system has returned from a system-level low-power state.
// This is typically due to a wakeup event, but many platforms also allow this
// to occur spuriously.
//
// The specified state is the one that was previously requested by the last
// CPU to suspend. It is not necessarily the state that was entered.
//
// This is called during power_cpu_resume when the first_cpu argument is true,
// but only if it matches a preceding power_system_suspend that returned OK.
event power_system_resume
	param state: platform_power_state_t

// Triggered when a system reset request is received. This does not follow the
// same path as the kernel_abort as it is not a failure or hypervisor error.
// There is no option to hold-off this request, if this is desired, a separate
// framework is required to provide that prior to triggering this event.
// The reset_type and cookie are platform specific values.
handled_event power_system_reset
	param reset_type: uint64_t
	param cookie: uint64_t
	param error: error_t *

// Triggered when a system off request is received. There is no option to
// hold-off this request, if this is desired, a separate framework is required
// to provide that prior to triggering this event.
event power_system_off

```

`hyp/interfaces/power/power.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

define cpu_power_state enumeration(explicit) {
	off		= 0; // never booted
	cold_boot	= 1; // in cold boot path, warm boot not yet reached
	suspend		= 2; // suspend; might either resume or warm boot
	offline		= 3; // voted off and called platform_cpu_off()
	online		= 4; // finished warm boot
	started		= 5; // voted on, but not yet finished warm boot
};

define cpu_power_state_array_t newtype
	array(PLATFORM_MAX_CORES) enumeration cpu_power_state;

```

`hyp/interfaces/preempt/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

types preempt.tc

```

`hyp/interfaces/preempt/include/preempt.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// Disable preemption, if it is not already disabled.
//
// This prevents the current thread being switched. It may also disable
// interrupts, but the caller should not rely on this.
//
// Calls to this function may be nested. Each call must be matched by a
// call to preempt_enable().
void
preempt_disable(void) ACQUIRE_PREEMPT_DISABLED;

// Undo the effect of an earlier preempt_disable() call.
//
// If the matching preempt_disable() call disabled interrupts, then this call
// will re-enable them.
void
preempt_enable(void) RELEASE_PREEMPT_DISABLED;

// Handle an interrupt in hypervisor mode.
//
// This function must be called by the architecture's interrupt handling routine
// when an interrupt preempts execution of the hypervisor. It will arrange for
// the handling of the interrupt, but note that such handling may not complete
// before this function returns.
//
// If this function returns true, the caller must arrange for interrupts to be
// disabled upon return from the current interrupt. This is intended to allow
// preempt module implementations to defer handling of an interrupt; e.g. to
// allow preempt_disable() to avoid disabling interrupts if the CPU makes that
// too slow to do frequently.
bool
preempt_interrupt_dispatch(void);

// Assert that the caller is executing in an interrupt handler, and mark
// preemption as disabled for the purpose of static analysis.
void
preempt_disable_in_irq(void) ACQUIRE_PREEMPT_DISABLED;

// Assert that the caller is executing in an interrupt handler, and mark
// preemption as enabled for the purpose of static analysis.
void
preempt_enable_in_irq(void) RELEASE_PREEMPT_DISABLED;

// Handle an asynchronous abort in hypervisor mode.
//
// This function must be called by the architecture's exception or interrupt
// handling routine when an asynchronous abort preempts execution of the
// hypervisor. It will arrange for handling of the abort.
//
// The meaning of "asynchronous abort" is architecture-specific and includes,
// for example, an AArch64 SError interrupt or an x86 NMI.
bool
preempt_abort_dispatch(void);

// Assert that preemption is currently disabled.
//
// This calls assert(), so it is effective only if !defined(NDEBUG).
void
assert_preempt_disabled(void) REQUIRE_PREEMPT_DISABLED;

// Assert that preemption is currently enabled.
//
// This calls assert(), so it is effective only if !defined(NDEBUG).
void
assert_preempt_enabled(void) EXCLUDE_PREEMPT_DISABLED;

```

`hyp/interfaces/preempt/preempt.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

define preempt_disabled global structure opaque_lock;

```

`hyp/interfaces/prng/build.conf`:

```conf
hypercalls prng.hvc

```

`hyp/interfaces/prng/include/prng.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// Get 64-bits of random bits from the DRBG
uint64_result_t
prng_get64(void);

```

`hyp/interfaces/prng/prng.hvc`:

```hvc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// Get random numbers from a DRBG that is seeded by a TRNG.
// Typically this API will source randomness from a NIST/FIPS compliant
// hardware device.
// Note, data returned may remain on the hypervisor stack.
define prng_get_entropy hypercall {
	sensitve_return;
	call_num	0x57;
	num_bytes	input type count_t;
	res0		input uregister;
	error		output enumeration error;
	data0		output uint32;
	data1		output uint32;
	data2		output uint32;
	data3		output uint32;
};

```

`hyp/interfaces/psci/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

types psci.tc
events psci.ev

```

`hyp/interfaces/psci/include/psci.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// These flags are returned to userspace in vcpu_run results, so they are public
// API and must not be changed.
#define PSCI_REQUEST_SYSTEM_RESET     (1UL << 63)
#define PSCI_REQUEST_SYSTEM_RESET2_64 (1UL << 62)

```

`hyp/interfaces/psci/psci.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

interface psci

selector_event psci_features32
	selector function: psci_function_t
	return: uint32_t = SMCCC_UNKNOWN_FUNCTION32

selector_event psci_features64
	selector function: psci_function_t
	return: uint32_t = SMCCC_UNKNOWN_FUNCTION32

```

`hyp/interfaces/psci/psci.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// Common type definitions used by platform and virtual PSCI.

define psci_function enumeration(explicit) {
	PSCI_VERSION = 0;
	CPU_SUSPEND = 1;
	CPU_OFF = 2;
	CPU_ON = 3;
	AFFINITY_INFO = 4;
	MIGRATE = 5;
	MIGRATE_INFO_TYPE = 6;
	MIGRATE_INFO_UP_CPU = 7;
	SYSTEM_OFF = 8;
	SYSTEM_RESET = 9;
	PSCI_FEATURES = 0xA;
	CPU_FREEZE = 0xB;
	CPU_DEFAULT_SUSPEND = 0xC;
	NODE_HW_STATE = 0xD;
	SYSTEM_SUSPEND = 0xE;
	PSCI_SET_SUSPEND_MODE = 0xF;
	PSCI_STAT_RESIDENCY = 0x10;
	PSCI_STAT_COUNT = 0x11;
	SYSTEM_RESET2 = 0x12;
	MEM_PROTECT = 0x13;
	MEM_PROTECT_CHECK_RANGE = 0x14;
};

define psci_ret enumeration(explicit) {
	SUCCESS = 0;
	NOT_SUPPORTED = -1;
	INVALID_PARAMETERS = -2;
	DENIED = -3;
	ALREADY_ON = -4;
	ON_PENDING = -5;
	INTERNAL_FAILURE = -6;
	NOT_PRESENT = -7;
	DISABLED = -8;
	INVALID_ADDRESS = -9;
};

define psci_ret_affinity_info enumeration(explicit) {
	ON = 0;
	OFF = 1;
	ON_PENDING = 2;
};

define psci_mpidr bitfield<64> {
	7:0	Aff0		uint8;
	15:8	Aff1		uint8;
	23:16	Aff2		uint8;
	31:24	unknown = 0;
	39:32	Aff3		uint8;
	63:40	unknown = 0;
};

#if defined (PLATFORM_PSCI_USE_ORIGINAL_POWERSTATE_FORMAT)
define psci_suspend_powerstate bitfield<32> {
	15:0	StateID		uint32;
	16	StateType	enumeration psci_suspend_powerstate_type;
	23:17	unknown = 0;
	25:24	PowerLevel	uint32;
	31:26	unknown = 0;
};
#else
define psci_suspend_powerstate bitfield<32> {
	27:0	StateID		uint32;
	29:28	unknown = 0;
	30	StateType	enumeration psci_suspend_powerstate_type;
	31	unknown = 0;
};
#endif

define psci_suspend_powerstate_type enumeration(explicit) {
	standby_or_retention = 0;
	powerdown = 1;
};

define psci_mode enumeration(explicit) {
	PC = 0;
	OSI = 1;
};

#if defined(INTERFACE_VCPU_RUN)
extend vcpu_run_state enumeration {
	// VCPU made a PSCI_SYSTEM_RESET call to request a reset of the VM.
	// This is otherwise equivalent to a power-off state, and resuming the
	// VCPU will return it to a regular power-off state.
	psci_system_reset = 0x100;
};

extend vcpu_run_wakeup_from_state enumeration {
	// VCPU made a PSCI_CPU_SUSPEND call. The first state data word is the
	// PSCI suspend state argument.
	psci_cpu_suspend = 2;

	// VCPU made a PSCI_SYSTEM_SUSPEND call. The first state data word is
	// the deepest possible CPU suspend state (which may not be the same as
	// the system suspend state), for backwards compatibility with host
	// kernels that do not check this value.
	psci_system_suspend = 3;
};
#endif

```

`hyp/interfaces/qcbor/build.conf`:

```conf
# © 2022 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

```

`hyp/interfaces/qcbor/include/qcbor.h`:

```h
// © 2022 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#if defined(ARCH_ENDIAN_LITTLE) && ARCH_ENDIAN_LITTLE
#define USEFULBUF_CONFIG_LITTLE_ENDIAN 1
#elif defined(ARCH_ENDIAN_BIG) && ARCH_ENDIAN_BIG
#define USEFULBUF_CONFIG_BIG_ENDIAN 1
#endif

#include "qcbor/qcbor_encode.h"

typedef UsefulBuf  useful_buff_t;
typedef UsefulBufC const_useful_buff_t;

typedef QCBOREncodeContext qcbor_enc_ctxt_t;
typedef QCBORError	   qcbor_err_t;

```

`hyp/interfaces/qcbor/include/qcbor/UsefulBuf.h`:

```h
/*============================================================================
 Copyright (c) 2016-2018, The Linux Foundation.
 Copyright (c) 2018-2022, Laurence Lundblade.
 Copyright (c) 2021, Arm Limited. All rights reserved.

Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are
met:
    * Redistributions of source code must retain the above copyright
      notice, this list of conditions and the following disclaimer.
    * Redistributions in binary form must reproduce the above
      copyright notice, this list of conditions and the following
      disclaimer in the documentation and/or other materials provided
      with the distribution.
    * Neither the name of The Linux Foundation nor the names of its
      contributors, nor the name "Laurence Lundblade" may be used to
      endorse or promote products derived from this software without
      specific prior written permission.

THIS SOFTWARE IS PROVIDED "AS IS" AND ANY EXPRESS OR IMPLIED
WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF
MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT
ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS
BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR
BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,
WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE
OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN
IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 =============================================================================*/

/*============================================================================
 FILE:  UsefulBuf.h

 DESCRIPTION:  General purpose input and output buffers

 EDIT HISTORY FOR FILE:

 This section contains comments describing changes made to the module.
 Notice that changes are listed in reverse chronological order.

 when         who             what, where, why
 --------     ----            --------------------------------------------------
 4/11/2022    llundblade      Add GetOutPlace and Advance to UsefulOutBuf.
 9/21/2021    llundbla        Clarify UsefulOutBuf size calculation mode
 8/8/2021     dthaler/llundbla Work with C++ without compiler extensions
 5/11/2021    llundblade      Improve comments and comment formatting.
 3/6/2021     mcr/llundblade  Fix warnings related to --Wcast-qual
 2/17/2021    llundblade      Add method to go from a pointer to an offset.
 1/25/2020    llundblade      Add some casts so static anlyzers don't complain.
 5/21/2019    llundblade      #define configs for efficient endianness handling.
 5/16/2019    llundblade      Add UsefulOutBuf_IsBufferNULL().
 3/23/2019    llundblade      Big documentation & style update. No interface
			      change.
 3/6/2019     llundblade      Add UsefulBuf_IsValue()
 12/17/2018   llundblade      Remove const from UsefulBuf and UsefulBufC .len
 12/13/2018   llundblade      Documentation improvements
 09/18/2018   llundblade      Cleaner distinction between UsefulBuf and
			      UsefulBufC.
 02/02/18     llundbla        Full support for integers in and out; fix pointer
			      alignment bug. Incompatible change: integers
			      in/out are now in network byte order.
 08/12/17     llundbla        Added UsefulOutBuf_AtStart and UsefulBuf_Find
 06/27/17     llundbla        Fix UsefulBuf_Compare() bug. Only affected
			      comparison for < or > for unequal length buffers.
			      Added UsefulBuf_Set() function.
 05/30/17     llundbla        Functions for NULL UsefulBufs and const / unconst
 11/13/16     llundbla        Initial Version.

 =============================================================================*/

#ifndef UsefulBuf_h_
#define UsefulBuf_h_

/*
 * Endianness Configuration
 *
 * This code is written so it will work correctly on big- and
 * little-endian CPUs without configuration or any auto-detection of
 * endianness. All code here will run correctly regardless of the
 * endianness of the CPU it is running on.
 *
 * There are four C preprocessor macros that can be set with #define
 * to explicitly configure endianness handling. Setting them can
 * reduce code size a little and improve efficiency a little.
 *
 * Note that most of QCBOR is unaffected by this configuration.  Its
 * endianness handling is integrated with the code that handles
 * alignment and preferred serialization. This configuration does
 * affect QCBOR's (planned) implementation of integer arrays (tagged
 * arrays) and use of the functions here to serialize or deserialize
 * integers and floating-point values.
 *
 * Following is the recipe for configuring the endianness-related
 * #defines.
 *
 * The first option is to not define anything. This will work fine
 * with all CPUs, OS's and compilers. The code for encoding integers
 * may be a little larger and slower.
 *
 * If your CPU is big-endian then define
 * USEFULBUF_CONFIG_BIG_ENDIAN. This will give the most efficient code
 * for big-endian CPUs. It will be small and efficient because there
 * will be no byte swapping.
 *
 * Try defining USEFULBUF_CONFIG_HTON. This will work on most CPUs,
 * OS's and compilers, but not all. On big-endian CPUs this should
 * give the most efficient code, the same as
 * USEFULBUF_CONFIG_BIG_ENDIAN does. On little-endian CPUs it should
 * call the system-defined byte swapping method which is presumably
 * implemented efficiently. In some cases, this will be a dedicated
 * byte swap instruction like Intel's bswap.
 *
 * If USEFULBUF_CONFIG_HTON works and you know your CPU is
 * little-endian, it is also good to define
 * USEFULBUF_CONFIG_LITTLE_ENDIAN.
 *
 * if USEFULBUF_CONFIG_HTON doesn't work and you know your system is
 * little-endian, try defining both USEFULBUF_CONFIG_LITTLE_ENDIAN and
 * USEFULBUF_CONFIG_BSWAP. This should call the most efficient
 * system-defined byte swap method. However, note
 * https://hardwarebug.org/2010/01/14/beware-the-builtins/.  Perhaps
 * this is fixed now. Often hton() and ntoh() will call the built-in
 * __builtin_bswapXX()() function, so this size issue could affect
 * USEFULBUF_CONFIG_HTON.
 *
 * Last, run the tests. They must all pass.
 *
 * These #define config options affect the inline implementation of
 * UsefulOutBuf_InsertUint64() and UsefulInputBuf_GetUint64().  They
 * also affect the 16-, 32-bit, float and double versions of these
 * functions. Since they are inline, the size effect is not in the
 * UsefulBuf object code, but in the calling code.
 *
 * Summary:
 *   USEFULBUF_CONFIG_BIG_ENDIAN -- Force configuration to big-endian.
 *   USEFULBUF_CONFIG_LITTLE_ENDIAN -- Force to little-endian.
 *   USEFULBUF_CONFIG_HTON -- Use hton(), htonl(), ntohl()... to
 *     handle big and little-endian with system option.
 *   USEFULBUF_CONFIG_BSWAP -- With USEFULBUF_CONFIG_LITTLE_ENDIAN,
 *     use __builtin_bswapXX().
 *
 * It is possible to run this code in environments where using floating point is
 * not allowed. Defining USEFULBUF_DISABLE_ALL_FLOAT will disable all the code
 * that is related to handling floating point types, along with related
 * interfaces. This makes it possible to compile the code with the compile
 * option -mgeneral-regs-only.
 */

#if defined(USEFULBUF_CONFIG_BIG_ENDIAN) &&                                    \
	defined(USEFULBUF_CONFIG_LITTLE_ENDIAN)
#error "Cannot define both USEFULBUF_CONFIG_BIG_ENDIAN and USEFULBUF_CONFIG_LITTLE_ENDIAN"
#endif

#include <stddef.h> /* for size_t */
#include <stdint.h> /* for uint8_t, uint16_t.... */
#include <string.h> /* for strlen, memcpy, memmove, memset */

#ifdef USEFULBUF_CONFIG_HTON
#include <arpa/inet.h> /* for htons, htonl, htonll, ntohs... */
#endif

#ifdef __cplusplus
extern "C" {
#if 0
} /* Keep editor indention formatting happy */
#endif
#endif

/**
 * @file UsefulBuf.h
 *
 * The goal of this code is to make buffer and pointer manipulation
 * easier and safer when working with binary data.
 *
 * The @ref UsefulBuf, @ref UsefulOutBuf and @ref UsefulInputBuf
 * structures are used to represent buffers rather than ad hoc
 * pointers and lengths.
 *
 * With these it is possible to write code that does little or no
 * direct pointer manipulation for copying and formatting data. For
 * example, the QCBOR encoder was written using these and has less
 * pointer manipulation.
 *
 * While it is true that object code using these functions will be a
 * little larger and slower than a white-knuckle clever use of
 * pointers might be, but not by that much or enough to have an effect
 * for most use cases. For security-oriented code this is highly
 * worthwhile. Clarity, simplicity, reviewability and are more
 * important.
 *
 * There are some extra sanity and double checks in this code to help
 * catch coding errors and simple memory corruption. They are helpful,
 * but not a substitute for proper code review, input validation and
 * such.
 *
 * This code consists of a lot of inline functions and a few that are
 * not.  It should not generate very much object code, especially with
 * the optimizer turned up to @c -Os or @c -O3.
 */

/**
 * @ref UsefulBufC and @ref UsefulBuf are simple data structures to
 * hold a pointer and length for binary data.  In C99 this data
 * structure can be passed on the stack making a lot of code cleaner
 * than carrying around a pointer and length as two parameters.
 *
 * This is also conducive to secure coding practice as the length is
 * always carried with the pointer and the convention for handling a
 * pointer and a length is clear.
 *
 * While it might be possible to write buffer and pointer code more
 * efficiently in some use cases, the thought is that unless there is
 * an extreme need for performance (e.g., you are building a
 * gigabit-per-second IP router), it is probably better to have
 * cleaner code you can be most certain about the security of.
 *
 * The non-const @ref UsefulBuf is usually used to refer an empty
 * buffer to be filled in.  The length is the size of the buffer.
 *
 * The const @ref UsefulBufC is usually used to refer to some data
 * that has been filled in. The length is amount of valid data pointed
 * to.
 *
 * A common use mode is to pass a @ref UsefulBuf to a function, the
 * function puts some data in it, then the function returns a @ref
 * UsefulBufC refering to the data. The @ref UsefulBuf is a non-const
 * "in" parameter and the @ref UsefulBufC is a const "out" parameter
 * so the constness stays correct. There is no single "in,out"
 * parameter (if there was, it would have to be non-const).  Note that
 * the pointer returned in the @ref UsefulBufC usually ends up being
 * the same pointer passed in as a @ref UsefulBuf, though this is not
 * striclty required.
 *
 * A @ref UsefulBuf is null, it has no value, when @c ptr in it is
 * @c NULL.
 *
 * There are functions and macros for the following:
 *  - Initializing
 *  - Create initialized const @ref UsefulBufC from compiler literals
 *  - Create initialized const @ref UsefulBufC from NULL-terminated string
 *  - Make an empty @ref UsefulBuf on the stack
 *  - Checking whether a @ref UsefulBuf is null, empty or both
 *  - Copying, copying with offset, copying head or tail
 *  - Comparing and finding substrings
 *
 * See also @ref UsefulOutBuf. It is a richer structure that has both
 * the size of the valid data and the size of the buffer.
 *
 * @ref UsefulBuf is only 16 or 8 bytes on a 64- or 32-bit machine so
 * it can go on the stack and be a function parameter or return value.
 *
 * Another way to look at it is this. C has the NULL-terminated string
 * as a means for handling text strings, but no means or convention
 * for binary strings. Other languages do have such means, Rust, an
 * efficient compiled language, for example.
 *
 * @ref UsefulBuf is kind of like the Useful Pot Pooh gave Eeyore on
 * his birthday.  Eeyore's balloon fits beautifully, "it goes in and
 * out like anything".
 */
typedef struct q_useful_buf_c {
	const void *ptr;
	size_t	    len;
} UsefulBufC;

/**
 * This non-const @ref UsefulBuf is typically used for some allocated
 * memory that is to be filled in. The @c len is the amount of memory,
 * not the length of the valid data in the buffer.
 */
typedef struct q_useful_buf {
	void  *ptr;
	size_t len;
} UsefulBuf;

/**
 * A null @ref UsefulBufC is one that has no value in the same way a
 * @c NULL pointer has no value.  A @ref UsefulBufC is @c NULL when
 * the @c ptr field is @c NULL. It doesn't matter what @c len is.  See
 * UsefulBuf_IsEmpty() for the distinction between null and empty.
 */
/*
 * NULLUsefulBufC and few other macros have to be
 * definied differently in C than C++ because there
 * is no common construct for a literal structure.
 *
 * In C compound literals are used.
 *
 * In C++ list initalization is used. This only works
 * in C++11 and later.
 *
 * Note that some popular C++ compilers can handle compound
 * literals with on-by-default extensions, however
 * this code aims for full correctness with strict
 * compilers so they are not used.
 */
#ifdef __cplusplus
#define NULLUsefulBufC                                                         \
	{                                                                      \
		NULL, 0                                                        \
	}
#else
#define NULLUsefulBufC ((UsefulBufC){ NULL, 0 })
#endif

/**
 * A null @ref UsefulBuf is one that has no memory associated the same
 * way @c NULL points to nothing. It does not matter what @c len is.
 **/
#ifdef __cplusplus
#define NULLUsefulBuf                                                          \
	{                                                                      \
		NULL, 0                                                        \
	}
#else
#define NULLUsefulBuf ((UsefulBuf){ NULL, 0 })
#endif

/**
 * @brief Check if a @ref UsefulBuf is @ref NULLUsefulBuf or not.
 *
 * @param[in] UB The UsefulBuf to check.
 *
 * @return 1 if it is @ref NULLUsefulBuf, 0 if not.
 */
static inline int
UsefulBuf_IsNULL(UsefulBuf UB);

/**
 * @brief Check if a @ref UsefulBufC is @ref NULLUsefulBufC or not.
 *
 * @param[in] UB The @ref UsefulBufC to check.
 *
 * @return 1 if it is @c NULLUsefulBufC, 0 if not.
 */
static inline int
UsefulBuf_IsNULLC(UsefulBufC UB);

/**
 * @brief Check if a @ref UsefulBuf is empty or not.
 *
 * @param[in] UB The @ref UsefulBuf to check.
 *
 * @return 1 if it is empty, 0 if not.
 *
 * An "empty" @ref UsefulBuf is one that has a value and can be
 * considered to be set, but that value is of zero length.  It is
 * empty when @c len is zero. It doesn't matter what the @c ptr is.
 *
 * Many uses will not need to clearly distinguish a @c NULL @ref
 * UsefulBuf from an empty one and can have the @c ptr @c NULL and the
 * @c len 0.  However if a use of @ref UsefulBuf needs to make a
 * distinction then @c ptr should not be @c NULL when the @ref
 * UsefulBuf is considered empty, but not @c NULL.
 */
static inline int
UsefulBuf_IsEmpty(UsefulBuf UB);

/**
 * @brief Check if a @ref UsefulBufC is empty or not.
 *
 * @param[in] UB The @ref UsefulBufC to check.
 *
 * @return 1 if it is empty, 0 if not.
 */
static inline int
UsefulBuf_IsEmptyC(UsefulBufC UB);

/**
 * @brief Check if a @ref UsefulBuf is @ref NULLUsefulBuf or empty.
 *
 * @param[in] UB The @ref UsefulBuf to check.
 *
 * @return 1 if it is either @ref NULLUsefulBuf or empty, 0 if not.
 */
static inline int
UsefulBuf_IsNULLOrEmpty(UsefulBuf UB);

/**
 * @brief Check if a @ref UsefulBufC is @ref NULLUsefulBufC or empty.
 *
 * @param[in] UB The @ref UsefulBufC to check.
 *
 * @return 1 if it is either @ref NULLUsefulBufC or empty, 0 if not.
 */
static inline int
UsefulBuf_IsNULLOrEmptyC(UsefulBufC UB);

/**
 * @brief Convert a non-const @ref UsefulBuf to a const @ref UsefulBufC.
 *
 * @param[in] UB The @ref UsefulBuf to convert.
 *
 * @return A @ref UsefulBufC struct.
 */
static inline UsefulBufC
UsefulBuf_Const(const UsefulBuf UB);

/**
 * @brief Convert a const @ref UsefulBufC to a non-const @ref UsefulBuf.
 *
 * @param[in] UBC The @ref UsefulBuf to convert.
 *
 * @return A non-const @ref UsefulBuf struct.
 *
 * Use of this is not necessary for the intended use mode of @ref
 * UsefulBufC and @ref UsefulBuf.  In that mode, the @ref UsefulBuf is
 * created to describe a buffer that has not had any data put in
 * it. Then the data is put in it.  Then a @ref UsefulBufC is create
 * to describe the part with the data in it. This goes from non-const
 * to const, so this function is not needed.
 *
 * If the -Wcast-qual warning is enabled, this function can be used to
 * avoid that warning.
 */
static inline UsefulBuf
UsefulBuf_Unconst(const UsefulBufC UBC);

/**
 * Convert a literal string to a @ref UsefulBufC.
 *
 * @c szString must be a literal string that @c sizeof() works on.
 * This is better for literal strings than UsefulBuf_FromSZ() because
 * it generates less code. It will not work on non-literal strings.
 *
 * The terminating \0 (NULL) is NOT included in the length!
 */
#ifdef __cplusplus
#define UsefulBuf_FROM_SZ_LITERAL(szString)                                    \
	{                                                                      \
		(szString), sizeof(szString) - 1                               \
	}
#else
#define UsefulBuf_FROM_SZ_LITERAL(szString)                                    \
	((UsefulBufC){ (szString), sizeof(szString) - 1 })
#endif

/**
 * Convert a literal byte array to a @ref UsefulBufC.
 *
 * @c pBytes must be a literal string that @c sizeof() works on.  It
 * will not work on non-literal arrays.
 */
#ifdef __cplusplus
#define UsefulBuf_FROM_BYTE_ARRAY_LITERAL(pBytes)                              \
	{                                                                      \
		(pBytes), sizeof(pBytes)                                       \
	}
#else
#define UsefulBuf_FROM_BYTE_ARRAY_LITERAL(pBytes)                              \
	((UsefulBufC){ (pBytes), sizeof(pBytes) })
#endif

/**
 * Make an automatic variable named @c name of type @ref UsefulBuf and
 * point it to a stack variable of the given @c size.
 */
#define UsefulBuf_MAKE_STACK_UB(name, size)                                    \
	uint8_t	  pBuf##name##_[(size)];                                       \
	UsefulBuf name = { pBuf##name##_, sizeof(pBuf##name##_) }

/**
 * Make a byte array in to a @ref UsefulBuf. This is usually used on
 * stack variables or static variables.  Also see @ref
 * UsefulBuf_MAKE_STACK_UB.
 */
#ifdef __cplusplus
#define UsefulBuf_FROM_BYTE_ARRAY(pBytes)                                      \
	{                                                                      \
		(pBytes), sizeof(pBytes)                                       \
	}
#else
#define UsefulBuf_FROM_BYTE_ARRAY(pBytes)                                      \
	((UsefulBuf){ (pBytes), sizeof(pBytes) })
#endif

/**
 * @brief Convert a NULL-terminated string to a @ref UsefulBufC.
 *
 * @param[in] szString The string to convert.
 *
 * @return A @ref UsefulBufC struct.
 *
 * @c UsefulBufC.ptr points to the string so its lifetime must be
 * maintained.
 *
 * The terminating \0 (NULL) is NOT included in the length.
 */
static inline UsefulBufC
UsefulBuf_FromSZ(const char *szString);

/**
 * @brief Copy one @ref UsefulBuf into another at an offset.
 *
 * @param[in] Dest     Destination buffer to copy into.
 * @param[in] uOffset  The byte offset in @c Dest at which to copy to.
 * @param[in] Src      The bytes to copy.
 *
 * @return Pointer and length of the copy or @ref NULLUsefulBufC.
 *
 * This fails and returns @ref NULLUsefulBufC if @c offset is beyond the
 * size of @c Dest.
 *
 * This fails and returns @ref NULLUsefulBufC if the @c Src length
 * plus @c uOffset is greater than the length of @c Dest.
 *
 * The results are undefined if @c Dest and @c Src overlap.
 *
 * This assumes that there is valid data in @c Dest up to @c
 * uOffset. The @ref UsefulBufC returned starts at the beginning of @c
 * Dest and goes to @c Src.len @c + @c uOffset.
 */
UsefulBufC
UsefulBuf_CopyOffset(UsefulBuf Dest, size_t uOffset, const UsefulBufC Src);

/**
 * @brief Copy one @ref UsefulBuf into another.
 *
 * @param[in] Dest  The destination buffer to copy into.
 * @param[out] Src  The source to copy from.
 *
 * @return Filled in @ref UsefulBufC on success, @ref NULLUsefulBufC
 *         on failure.
 *
 * This fails if @c Src.len is greater than @c Dest.len.
 *
 * Note that like @c memcpy(), the pointers are not checked and this
 * will crash rather than return @ref NULLUsefulBufC if they are @c
 * NULL or invalid.
 *
 * The results are undefined if @c Dest and @c Src overlap.
 */
static inline UsefulBufC
UsefulBuf_Copy(UsefulBuf Dest, const UsefulBufC Src);

/**
 * @brief Set all bytes in a @ref UsefulBuf to a value, for example to 0.
 *
 * @param[in] pDest  The destination buffer to copy into.
 * @param[in] value  The value to set the bytes to.
 *
 * Note that like @c memset(), the pointer in @c pDest is not checked
 * and this will crash if @c NULL or invalid.
 */
static inline UsefulBufC
UsefulBuf_Set(UsefulBuf pDest, uint8_t value);

/**
 * @brief Copy a pointer into a @ref UsefulBuf.
 *
 * @param[in,out] Dest  The destination buffer to copy into.
 * @param[in] ptr       The source to copy from.
 * @param[in] uLen      Length of the source; amount to copy.
 *
 * @return Filled in @ref UsefulBufC on success, @ref NULLUsefulBufC
 *         on failure.
 *
 * This fails and returns @ref NULLUsefulBufC if @c uLen is greater
 * than @c pDest->len.
 *
 * Note that like @c memcpy(), the pointers are not checked and this
 * will crash, rather than return 1 if they are @c NULL or invalid.
 */
static inline UsefulBufC
UsefulBuf_CopyPtr(UsefulBuf Dest, const void *ptr, size_t uLen);

/**
 *  @brief Returns a truncation of a @ref UsefulBufC.
 *
 *  @param[in] UB       The buffer to get the head of.
 *  @param[in] uAmount  The number of bytes in the head.
 *
 *  @return A @ref UsefulBufC that is the head of UB.
 */
static inline UsefulBufC
UsefulBuf_Head(UsefulBufC UB, size_t uAmount);

/**
 * @brief  Returns bytes from the end of a @ref UsefulBufC.
 *
 * @param[in] UB       The buffer to get the tail of.
 * @param[in] uAmount  The offset from the start where the tail is to begin.
 *
 * @return A @ref UsefulBufC that is the tail of @c UB or @ref NULLUsefulBufC
 *         if @c uAmount is greater than the length of the @ref UsefulBufC.
 *
 * If @c UB.ptr is @c NULL, but @c UB.len is not zero, then the result will
 * be a @ref UsefulBufC with a @c NULL @c ptr and @c len with the length
 * of the tail.
 */
static inline UsefulBufC
UsefulBuf_Tail(UsefulBufC UB, size_t uAmount);

/**
 * @brief Compare one @ref UsefulBufC to another.
 *
 * @param[in] UB1  The first buffer to compare.
 * @param[in] UB2  The second buffer to compare.
 *
 * @return 0, positive or negative value.
 *
 * Returns a negative value if @c UB1 if is less than @c UB2. @c UB1 is
 * less than @c UB2 if it is shorter or the first byte that is not the
 * same is less.
 *
 * Returns 0 if the inputs are the same.
 *
 * Returns a positive value if @c UB2 is less than @c UB1.
 *
 * All that is of significance is that the result is positive, negative
 * or 0. (This doesn't return the difference between the first
 * non-matching byte like @c memcmp() ).
 */
int
UsefulBuf_Compare(const UsefulBufC UB1, const UsefulBufC UB2);

/**
 * @brief Find first byte that is not a particular byte value.
 *
 * @param[in] UB     The destination buffer for byte comparison.
 * @param[in] uValue The byte value to compare to.
 *
 * @return  Offset of first byte that isn't @c uValue or
 *          @c SIZE_MAX if all bytes are @c uValue.
 *
 * Note that unlike most comparison functions, 0
 * does not indicate a successful comparison, so the
 * test for match is:
 *
 *      UsefulBuf_IsValue(...) == SIZE_MAX
 *
 * If @c UB is null or empty, there is no match
 * and 0 is returned.
 */
size_t
UsefulBuf_IsValue(const UsefulBufC UB, uint8_t uValue);

/**
 * @brief Find one @ref UsefulBufC in another.
 *
 * @param[in] BytesToSearch  Buffer to search through.
 * @param[in] BytesToFind    Buffer with bytes to be found.
 *
 * @return Position of found bytes or @c SIZE_MAX if not found.
 */
size_t
UsefulBuf_FindBytes(UsefulBufC BytesToSearch, UsefulBufC BytesToFind);

/**
 @brief Convert a pointer to an offset with bounds checking.

 @param[in] UB  Pointer to the UsefulInputBuf.
 @param[in] p   Pointer to convert to offset.

 @return SIZE_MAX if @c p is out of range, the byte offset if not.
*/
static inline size_t
UsefulBuf_PointerToOffset(UsefulBufC UB, const void *p);

#ifndef USEFULBUF_DISABLE_DEPRECATED
/** Deprecated macro; use @ref UsefulBuf_FROM_SZ_LITERAL instead */
#define SZLiteralToUsefulBufC(szString) UsefulBuf_FROM_SZ_LITERAL(szString)

/** Deprecated macro; use UsefulBuf_MAKE_STACK_UB instead */
#define MakeUsefulBufOnStack(name, size)                                       \
	uint8_t	  __pBuf##name[(size)];                                        \
	UsefulBuf name = { __pBuf##name, sizeof(__pBuf##name) }

/** Deprecated macro; use @ref UsefulBuf_FROM_BYTE_ARRAY_LITERAL instead */
#define ByteArrayLiteralToUsefulBufC(pBytes)                                   \
	UsefulBuf_FROM_BYTE_ARRAY_LITERAL(pBytes)

/** Deprecated function; use UsefulBuf_Unconst() instead */
static inline UsefulBuf
UsefulBufC_Unconst(const UsefulBufC UBC)
{
	UsefulBuf UB;

	// See UsefulBuf_Unconst() implementation for comment on pragmas
#pragma GCC diagnostic push
#pragma GCC diagnostic ignored "-Wcast-qual"
	UB.ptr = (void *)UBC.ptr;
#pragma GCC diagnostic pop

	UB.len = UBC.len;

	return UB;
}
#endif /* USEFULBUF_DISABLE_DEPRECATED */

#ifndef USEFULBUF_DISABLE_ALL_FLOAT
/**
 * @brief Copy a @c float to a @c uint32_t.
 *
 * @param[in] f  Float value to copy.
 *
 * @return  A @c uint32_t with the float bits.
 *
 * Convenience function to avoid type punning, compiler warnings and
 * such. The optimizer usually reduces this to a simple assignment.  This
 * is a crusty corner of C.
 */
static inline uint32_t
UsefulBufUtil_CopyFloatToUint32(float f);

/**
 * @brief Copy a @c double to a @c uint64_t.
 *
 * @param[in] d  Double value to copy.
 *
 * @return  A @c uint64_t with the double bits.
 *
 * Convenience function to avoid type punning, compiler warnings and
 * such. The optimizer usually reduces this to a simple assignment.  This
 * is a crusty corner of C.
 */
static inline uint64_t
UsefulBufUtil_CopyDoubleToUint64(double d);

/**
 * @brief Copy a @c uint32_t to a @c float.
 *
 * @param[in] u32  Integer value to copy.
 *
 * @return  The value as a @c float.
 *
 * Convenience function to avoid type punning, compiler warnings and
 * such. The optimizer usually reduces this to a simple assignment.  This
 * is a crusty corner of C.
 */
static inline float
UsefulBufUtil_CopyUint32ToFloat(uint32_t u32);

/**
 * @brief Copy a @c uint64_t to a @c double.
 *
 * @param[in] u64  Integer value to copy.
 *
 * @return  The value as a @c double.
 *
 * Convenience function to avoid type punning, compiler warnings and
 * such. The optimizer usually reduces this to a simple assignment.  This
 * is a crusty corner of C.
 */
static inline double
UsefulBufUtil_CopyUint64ToDouble(uint64_t u64);
#endif /* USEFULBUF_DISABLE_ALL_FLOAT */

/**
 * UsefulOutBuf is a structure and functions (an object) for
 * serializing data into a buffer to encode for a network protocol or
 * write data to a file.
 *
 * The main idea is that all the pointer manipulation is performed by
 * @ref UsefulOutBuf functions so the caller doesn't have to do any
 * pointer manipulation.  The pointer manipulation is centralized.
 * This code has been reviewed and written carefully so it
 * spares the caller of much of this work and results in safer code
 * with less effort.
 *
 * The @ref UsefulOutBuf methods that add data to the output buffer
 * always check the length and will never write off the end of the
 * output buffer. If an attempt to add data that will not fit is made,
 * an internal error flag will be set and further attempts to add data
 * will not do anything.
 *
 * There is no way to ever write off the end of that buffer when
 * calling the @c UsefulOutBuf_AddXxx() and
 * @c UsefulOutBuf_InsertXxx() functions.
 *
 * The functions to add data do not report success of failure. The
 * caller only needs to check for an error in the final call, either
 * UsefulOutBuf_OutUBuf() or UsefulOutBuf_CopyOut() to get the
 * result. This makes the calling code cleaner.
 *
 * There is a utility function to get the error status anytime along
 * the way for a special circumstance. There are functions to see how
 * much room is left and see if some data will fit too, but their use
 * is generally unnecessary.
 *
 * The general call flow is:
 *
 *    - Initialize by calling @ref UsefulOutBuf_Init(). The output
 *      buffer given to it can be from the heap, stack or
 *      otherwise. @ref UsefulOutBuf_MakeOnStack is a convenience
 *      macro that makes a buffer on the stack and initializes it.
 *
 *    - Call methods like UsefulOutBuf_InsertString(),
 *      UsefulOutBuf_AppendUint32() and UsefulOutBuf_InsertUsefulBuf()
 *      to output data. The append calls add data to the end of the
 *      valid data. The insert calls take a position argument.
 *
 *    - Call UsefulOutBuf_OutUBuf() or UsefulOutBuf_CopyOut() to see
 *      there were no errors and to get the serialized output bytes.
 *
 * @ref UsefulOutBuf can be used in a mode to calculate the size of
 * what would be output without actually outputting anything.  This is
 * useful to calculate the size of a buffer that is to be allocated to
 * hold the output. See @ref SizeCalculateUsefulBuf.
 *
 * Methods like UsefulOutBuf_InsertUint64() always output in network
 * byte order (big endian).
 *
 * The possible errors are:
 *
 *  - The @ref UsefulOutBuf was not initialized or was corrupted.
 *
 *  - An attempt was made to add data that will not fit.
 *
 *  - An attempt was made to insert data at a position beyond the end of
 *    the buffer.
 *
 *  - An attempt was made to insert data at a position beyond the valid
 *    data in the buffer.
 *
 * Some inexpensive simple sanity checks are performed before every
 * data addition to guard against use of an uninitialized or corrupted
 * UsefulOutBuf.
 *
 * @ref UsefulOutBuf has been used to create a CBOR encoder. The CBOR
 * encoder has almost no pointer manipulation in it, is easier to
 * read, and easier to review.
 *
 * A @ref UsefulOutBuf is small and can go on the stack:
 *   - 32 bytes (27 bytes plus alignment padding) on a 64-bit CPU
 *   - 16 bytes (15 bytes plus alignment padding) on a 32-bit CPU
 */
typedef struct useful_out_buf {
	/* PRIVATE DATA STRUCTURE */
	UsefulBuf UB;	    /* Memory that is being output to */
	size_t	  data_len; /* length of the valid data, the insertion point */
	uint16_t  magic;    /* Used to detect corruption and lack
			     * of initialization */
	uint8_t err;
	uint8_t padding_[5];
} UsefulOutBuf;

/**
 * This is a @ref UsefulBuf value that can be passed to
 * UsefulOutBuf_Init() to have it calculate the size of the output
 * buffer needed. Pass this for @c Storage, call all the append and
 * insert functions normally, then call UsefulOutBuf_OutUBuf(). The
 * returned @ref UsefulBufC has the size.
 *
 * As one can see, this is just a NULL pointer and very large size.
 * The NULL pointer tells UsefulOutputBuf to not copy any data.
 */
#ifdef __cplusplus
#define SizeCalculateUsefulBuf                                                 \
	{                                                                      \
		NULL, SIZE_MAX                                                 \
	}
#else
#define SizeCalculateUsefulBuf ((UsefulBuf){ NULL, SIZE_MAX })
#endif

/**
 * @brief Initialize and supply the actual output buffer.
 *
 * @param[out] pUOutBuf  The @ref UsefulOutBuf to initialize.
 * @param[in] Storage    Buffer to output into.
 *
 * This initializes the @ref UsefulOutBuf with storage, sets the
 * current position to the beginning of the buffer and clears the
 * error state.
 *
 * See @ref SizeCalculateUsefulBuf for instructions on how to
 * initialize a @ref UsefulOutBuf to calculate the size that would be
 * output without actually outputting.
 *
 * This must be called before the @ref UsefulOutBuf is used.
 */
void
UsefulOutBuf_Init(UsefulOutBuf *pUOutBuf, UsefulBuf Storage);

/**
 * Convenience macro to make a @ref UsefulOutBuf on the stack and
 * initialize it with a stack buffer of the given size. The variable
 * will be named @c name.
 */
#define UsefulOutBuf_MakeOnStack(name, size)                                   \
	uint8_t	     __pBuf##name[(size)];                                     \
	UsefulOutBuf name;                                                     \
	UsefulOutBuf_Init(&(name), (UsefulBuf){ __pBuf##name, (size) });

/**
 * @brief Reset a @ref UsefulOutBuf for re use.
 *
 * @param[in] pUOutBuf Pointer to the @ref UsefulOutBuf
 *
 * This sets the amount of data in the output buffer to none and
 * clears the error state.
 *
 * The output buffer is still the same one and size as from the
 * UsefulOutBuf_Init() call.
 *
 * This doesn't zero the data, just resets to 0 bytes of valid data.
 */
static inline void
UsefulOutBuf_Reset(UsefulOutBuf *pUOutBuf);

/**
 * @brief Returns position of end of data in the @ref UsefulOutBuf.
 *
 * @param[in] pUOutBuf  Pointer to the @ref UsefulOutBuf.
 *
 * @return position of end of data.
 *
 * On a freshly initialized @ref UsefulOutBuf with no data added, this
 * will return 0. After 10 bytes have been added, it will return 10
 * and so on.
 *
 * Generally, there is no need to call this for most uses of @ref
 * UsefulOutBuf.
 */
static inline size_t
UsefulOutBuf_GetEndPosition(UsefulOutBuf *pUOutBuf);

/**
 * @brief Returns whether any data has been added to the @ref UsefulOutBuf.
 *
 * @param[in] pUOutBuf  Pointer to the @ref UsefulOutBuf.
 *
 * @return 1 if output position is at start, 0 if not.
 */
static inline int
UsefulOutBuf_AtStart(UsefulOutBuf *pUOutBuf);

/**
 * @brief Inserts bytes into the @ref UsefulOutBuf.
 *
 * @param[in] pUOutBuf  Pointer to the @ref UsefulOutBuf.
 * @param[in] NewData   The bytes to insert.
 * @param[in] uPos      Index in output buffer at which to insert.
 *
 * @c NewData is the pointer and length for the bytes to be added to
 * the output buffer. There must be room in the output buffer for all
 * of @c NewData or an error will occur.
 *
 * The insertion point must be between 0 and the current valid
 * data. If not, an error will occur. Appending data to the output
 * buffer is achieved by inserting at the end of the valid data. This
 * can be retrieved by calling UsefulOutBuf_GetEndPosition().
 *
 * When insertion is performed, the bytes between the insertion point
 * and the end of data previously added to the output buffer are slid
 * to the right to make room for the new data.
 *
 * Overlapping buffers are OK. @c NewData can point to data in the
 * output buffer.
 *
 * If an error occurs, an error state is set in the @ref
 * UsefulOutBuf. No error is returned.  All subsequent attempts to add
 * data will do nothing.
 *
 * The intended use is that all additions are made without checking
 * for an error. The error will be taken into account when
 * UsefulOutBuf_OutUBuf() returns @c NullUsefulBufC.
 * UsefulOutBuf_GetError() can also be called to check for an error.
 */
void
UsefulOutBuf_InsertUsefulBuf(UsefulOutBuf *pUOutBuf, UsefulBufC NewData,
			     size_t uPos);

/**
 * @brief Insert a data buffer into the @ref UsefulOutBuf.
 *
 * @param[in] pUOutBuf  Pointer to the @ref UsefulOutBuf.
 * @param[in] pBytes    Pointer to the bytes to insert
 * @param[in] uLen      Length of the bytes to insert
 * @param[in] uPos      Index in output buffer at which to insert
 *
 * See UsefulOutBuf_InsertUsefulBuf() for details. This is the same with
 * the difference being a pointer and length is passed in rather than an
 * @ref UsefulBufC.
 */
static inline void
UsefulOutBuf_InsertData(UsefulOutBuf *pUOutBuf, const void *pBytes, size_t uLen,
			size_t uPos);

/**
 * @brief Insert a NULL-terminated string into the UsefulOutBuf.
 *
 * @param[in] pUOutBuf  Pointer to the @ref UsefulOutBuf.
 * @param[in] szString  NULL-terminated string to insert.
 * @param[in] uPos      Index in output buffer at which to insert.
 */
static inline void
UsefulOutBuf_InsertString(UsefulOutBuf *pUOutBuf, const char *szString,
			  size_t uPos);

/**
 * @brief Insert a byte into the @ref UsefulOutBuf.
 *
 * @param[in] pUOutBuf  Pointer to the UsefulOutBuf.
 * @param[in] byte      Bytes to insert.
 * @param[in] uPos      Index in output buffer at which to insert.
 *
 * See UsefulOutBuf_InsertUsefulBuf() for details. This is the same
 * with the difference being a single byte is to be inserted.
 */
static inline void
UsefulOutBuf_InsertByte(UsefulOutBuf *pUOutBuf, uint8_t byte, size_t uPos);

/**
 * @brief Insert a 16-bit integer into the @ref UsefulOutBuf.
 *
 * @param[in] pUOutBuf    Pointer to the @ref UsefulOutBuf.
 * @param[in] uInteger16  Integer to insert.
 * @param[in] uPos        Index in output buffer at which to insert.
 *
 * See UsefulOutBuf_InsertUsefulBuf() for details. This is the same
 * with the difference being a two-byte integer is to be inserted.
 *
 * The integer will be inserted in network byte order (big endian).
 */
static inline void
UsefulOutBuf_InsertUint16(UsefulOutBuf *pUOutBuf, uint16_t uInteger16,
			  size_t uPos);

/**
 * @brief Insert a 32-bit integer into the @ref UsefulOutBuf.
 *
 * @param[in] pUOutBuf    Pointer to the @ref UsefulOutBuf.
 * @param[in] uInteger32  Integer to insert.
 * @param[in] uPos        Index in output buffer at which to insert.
 *
 * See UsefulOutBuf_InsertUsefulBuf() for details. This is the same
 * with the difference being a four-byte integer is to be inserted.
 *
 * The integer will be inserted in network byte order (big endian).
 */
static inline void
UsefulOutBuf_InsertUint32(UsefulOutBuf *pUOutBuf, uint32_t uInteger32,
			  size_t uPos);

/**
 * @brief Insert a 64-bit integer into the @ref UsefulOutBuf.
 *
 * @param[in] pUOutBuf    Pointer to the @ref UsefulOutBuf.
 * @param[in] uInteger64  Integer to insert.
 * @param[in] uPos        Index in output buffer at which to insert.
 *
 * See UsefulOutBuf_InsertUsefulBuf() for details. This is the same
 * with the difference being an eight-byte integer is to be inserted.
 *
 * The integer will be inserted in network byte order (big endian).
 */
static inline void
UsefulOutBuf_InsertUint64(UsefulOutBuf *pUOutBuf, uint64_t uInteger64,
			  size_t uPos);

#ifndef USEFULBUF_DISABLE_ALL_FLOAT
/**
 * @brief Insert a @c float into the @ref UsefulOutBuf.
 *
 * @param[in] pUOutBuf  Pointer to the @ref UsefulOutBuf.
 * @param[in] f         @c float to insert.
 * @param[in] uPos      Index in output buffer at which to insert.
 *
 * See UsefulOutBuf_InsertUsefulBuf() for details. This is the same
 * with the difference being a @c float is to be inserted.
 *
 * The @c float will be inserted in network byte order (big endian).
 */
static inline void
UsefulOutBuf_InsertFloat(UsefulOutBuf *pUOutBuf, float f, size_t uPos);

/**
 * @brief Insert a @c double into the @ref UsefulOutBuf.
 *
 * @param[in] pUOutBuf  Pointer to the @ref UsefulOutBuf.
 * @param[in] d         @c double  to insert.
 * @param[in] uPos      Index in output buffer at which to insert.
 *
 * See UsefulOutBuf_InsertUsefulBuf() for details. This is the same
 * with the difference being a @c double is to be inserted.
 *
 * The @c double will be inserted in network byte order (big endian).
 */
static inline void
UsefulOutBuf_InsertDouble(UsefulOutBuf *pUOutBuf, double d, size_t uPos);
#endif /* USEFULBUF_DISABLE_ALL_FLOAT */

/**
 * @brief Append a @ref UsefulBuf into the @ref UsefulOutBuf.
 *
 * @param[in] pUOutBuf  Pointer to the @ref UsefulOutBuf.
 * @param[in] NewData   The @ref UsefulBuf with the bytes to append.
 *
 * See UsefulOutBuf_InsertUsefulBuf() for details. This does the same
 * with the insertion point at the end of the valid data.
 */
static inline void
UsefulOutBuf_AppendUsefulBuf(UsefulOutBuf *pUOutBuf, UsefulBufC NewData);

/**
 * @brief Append bytes to the @ref UsefulOutBuf.
 *
 * @param[in] pUOutBuf  Pointer to the @ref UsefulOutBuf.
 * @param[in] pBytes    Pointer to bytes to append.
 * @param[in] uLen      Length of @c pBytes to append.
 *
 * See UsefulOutBuf_InsertData() for details. This does the same with
 * the insertion point at the end of the valid data.
 */
static inline void
UsefulOutBuf_AppendData(UsefulOutBuf *pUOutBuf, const void *pBytes,
			size_t uLen);

/**
 * @brief Append a NULL-terminated string to the @ref UsefulOutBuf
 *
 * @param[in] pUOutBuf  Pointer to the @ref UsefulOutBuf.
 * @param[in] szString  NULL-terminated string to append.
 */
static inline void
UsefulOutBuf_AppendString(UsefulOutBuf *pUOutBuf, const char *szString);

/**
 * @brief Append a byte to the @ref UsefulOutBuf
 *
 * @param[in] pUOutBuf  Pointer to the @ref UsefulOutBuf.
 * @param[in] byte      Bytes to append.
 *
 * See UsefulOutBuf_InsertByte() for details. This does the same
 * with the insertion point at the end of the valid data.
 */
static inline void
UsefulOutBuf_AppendByte(UsefulOutBuf *pUOutBuf, uint8_t byte);

/**
 * @brief Append an integer to the @ref UsefulOutBuf
 *
 * @param[in] pUOutBuf    Pointer to the @ref UsefulOutBuf.
 * @param[in] uInteger16  Integer to append.
 *
 * See UsefulOutBuf_InsertUint16() for details. This does the same
 * with the insertion point at the end of the valid data.
 *
 * The integer will be appended in network byte order (big endian).
 */
static inline void
UsefulOutBuf_AppendUint16(UsefulOutBuf *pUOutBuf, uint16_t uInteger16);

/**
 * @brief Append an integer to the @ref UsefulOutBuf
 *
 * @param[in] pUOutBuf    Pointer to the @ref UsefulOutBuf.
 * @param[in] uInteger32  Integer to append.
 *
 * See UsefulOutBuf_InsertUint32() for details. This does the same
 * with the insertion point at the end of the valid data.
 *
 * The integer will be appended in network byte order (big endian).
 */
static inline void
UsefulOutBuf_AppendUint32(UsefulOutBuf *pUOutBuf, uint32_t uInteger32);

/**
 * @brief Append an integer to the @ref UsefulOutBuf
 *
 * @param[in] pUOutBuf    Pointer to the @ref UsefulOutBuf.
 * @param[in] uInteger64  Integer to append.
 *
 * See UsefulOutBuf_InsertUint64() for details. This does the same
 * with the insertion point at the end of the valid data.
 *
 * The integer will be appended in network byte order (big endian).
 */
static inline void
UsefulOutBuf_AppendUint64(UsefulOutBuf *pUOutBuf, uint64_t uInteger64);

#ifndef USEFULBUF_DISABLE_ALL_FLOAT
/**
 * @brief Append a @c float to the @ref UsefulOutBuf
 *
 * @param[in] pUOutBuf  Pointer to the @ref UsefulOutBuf.
 * @param[in] f         @c float to append.
 *
 * See UsefulOutBuf_InsertFloat() for details. This does the same with
 * the insertion point at the end of the valid data.
 *
 * The float will be appended in network byte order (big endian).
 */
static inline void
UsefulOutBuf_AppendFloat(UsefulOutBuf *pUOutBuf, float f);

/**
 * @brief Append a @c double to the @ref UsefulOutBuf
 *
 * @param[in] pUOutBuf  Pointer to the @ref UsefulOutBuf.
 * @param[in] d         @c double to append.
 *
 * See UsefulOutBuf_InsertDouble() for details. This does the same
 * with the insertion point at the end of the valid data.
 *
 * The double will be appended in network byte order (big endian).
 */
static inline void
UsefulOutBuf_AppendDouble(UsefulOutBuf *pUOutBuf, double d);
#endif /* USEFULBUF_DISABLE_ALL_FLOAT */

/**
 * @brief Returns the current error status.
 *
 * @param[in] pUOutBuf Pointer to the @ref UsefulOutBuf.
 *
 * @return 0 if all OK, 1 on error.
 *
 * This returns the error status since a call to either
 * UsefulOutBuf_Reset() of UsefulOutBuf_Init().  Once a @ref UsefulOutBuf
 * goes into the error state, it will stay until one of those
 * functions is called.
 *
 * Possible error conditions are:
 *   - bytes to be inserted will not fit
 *   - insertion point is out of buffer or past valid data
 *   - current position is off end of buffer (probably corrupted or
 * uninitialized)
 *   - detect corruption / uninitialized by bad magic number
 */
static inline int
UsefulOutBuf_GetError(UsefulOutBuf *pUOutBuf);

/**
 * @brief Returns number of bytes unused used in the output buffer.
 *
 * @param[in] pUOutBuf Pointer to the @ref UsefulOutBuf.
 *
 * @return Number of unused bytes or zero.
 *
 * Because of the error handling strategy and checks in
 * UsefulOutBuf_InsertUsefulBuf() it is usually not necessary to use
 * this.
 */
static inline size_t
UsefulOutBuf_RoomLeft(UsefulOutBuf *pUOutBuf);

/**
 *@brief Returns 1 if some number of bytes will fit in the @ref UsefulOutBuf.
 *
 * @param[in] pUOutBuf  Pointer to the @ref UsefulOutBuf
 * @param[in] uLen      Number of bytes for which to check
 *
 * @return 1 if @c uLen bytes will fit, 0 if not.
 *
 * Because of the error handling strategy and checks in
 * UsefulOutBuf_InsertUsefulBuf() it is usually not necessary to use
 * this.
 */
static inline int
UsefulOutBuf_WillItFit(UsefulOutBuf *pUOutBuf, size_t uLen);

/**
 * @brief Returns 1 if buffer given to UsefulOutBuf_Init() was @c NULL.
 *
 * @param[in] pUOutBuf  Pointer to the @ref UsefulOutBuf
 *
 * @return 1 if buffer given to UsefulOutBuf_Init() was @c NULL.
 *
 * Giving a @c NULL output buffer to UsefulOutBuf_Init() is used when
 * just calculating the length of the encoded data.
 */
static inline int
UsefulOutBuf_IsBufferNULL(UsefulOutBuf *pUOutBuf);

/**
 * @brief Returns pointer and length of the output buffer not yet used.
 *
 * @param[in] pUOutBuf  Pointer to the @ref UsefulOutBuf.
 *
 * @return pointer and length of output buffer not used.
 *
 * This is an escape that allows the caller to write directly
 * to the output buffer without any checks. This doesn't
 * change the output buffer or state. It just returns a pointer
 * and length of the bytes remaining.
 *
 * This is useful to avoid having the bytes to be added all
 * in a contiguous buffer. Its use can save memory. A good
 * example is in the COSE encrypt implementation where
 * the output of the symmetric cipher can go directly
 * into the output buffer, rather than having to go into
 * an intermediate buffer.
 *
 * See UsefulOutBuf_Advance() which is used to tell
 * UsefulOutBuf how much was written.
 *
 * Warning: this bypasses the buffer safety provided by
 * UsefulOutBuf!
 */
static inline UsefulBuf
UsefulOutBuf_GetOutPlace(UsefulOutBuf *pUOutBuf);

/**
 * @brief Advance the amount output assuming it was written by the caller.
 *
 * @param[in] pUOutBuf  Pointer to the @ref UsefulOutBuf.
 * @param[in] uAmount  The amount to advance.
 *
 * This advances the position in the output buffer
 * by \c uAmount. This assumes that the
 * caller has written \c uAmount to the pointer obtained
 * with UsefulOutBuf_GetOutPlace().
 *
 * Warning: this bypasses the buffer safety provided by
 * UsefulOutBuf!
 */
void
UsefulOutBuf_Advance(UsefulOutBuf *pUOutBuf, size_t uAmount);

/**
 *  @brief Returns the resulting valid data in a UsefulOutBuf
 *
 *  @param[in] pUOutBuf Pointer to the @ref UsefulOutBuf.
 *
 *  @return The valid data in @ref UsefulOutBuf or
 *           @ref NULLUsefulBufC if there was an error adding data.
 *
 *  The storage for the returned data is the @c Storage parameter
 *  passed to UsefulOutBuf_Init(). See also UsefulOutBuf_CopyOut().
 *
 *  This can be called anytime and many times to get intermediate
 *  results. It doesn't change the data or reset the current position,
 *  so further data can be added.
 */
UsefulBufC
UsefulOutBuf_OutUBuf(UsefulOutBuf *pUOutBuf);

/**
 * @brief Copies the valid data into a supplied buffer
 *
 * @param[in] pUOutBuf  Pointer to the @ref UsefulOutBuf.
 * @param[out] Dest     The destination buffer to copy into.
 *
 * @return Pointer and length of copied data or @c NULLUsefulBufC
 *         if it will not fit in the @c Dest buffer or the error
 *         state was entered.
 *
 * This is the same as UsefulOutBuf_OutUBuf() except it copies the
 * data to @c Dest.
 */
UsefulBufC
UsefulOutBuf_CopyOut(UsefulOutBuf *pUOutBuf, UsefulBuf Dest);

/**
 * @ref UsefulInputBuf is the counterpart to @ref UsefulOutBuf. It is
 * for parsing data received.  Initialize it with the data from the
 * network. Then use the functions like UsefulInputBuf_GetBytes() to
 * get data chunks of various types. A position cursor is maintained
 * internally.
 *
 * As long as the functions here are used, there will never be any
 * reference off the end of the given buffer (except
 * UsefulInputBuf_SetBufferLength()). This is true even if they are
 * called incorrectly, an attempt is made to seek off the end of the
 * buffer or such. This makes it easier to write safe and correct
 * code.  For example, the QCBOR decoder implementation is safer and
 * easier to review through its use of @ref UsefulInputBuf.
 *
 * @ref UsefulInputBuf maintains an internal error state.  The
 * intended use is fetching data chunks without any error checks until
 * the end.  If there was any error, such as an attempt to fetch data
 * off the end, the error state is entered and no further data will be
 * returned. In the error state the @c UsefulInputBuf_GetXxxx()
 * functions return 0, or @c NULL or @ref NULLUsefulBufC. As long as
 * null is not dereferenced, the error check can be put off until the
 * end, simplifying the calling code.
 *
 * The integer and float parsing expects network byte order (big
 * endian).  Network byte order is what is used by TCP/IP, CBOR and
 * most internet protocols.
 *
 * Lots of inline functions are used to keep code size down. The
 * optimizer, particularly with the @c -Os or @c -O3, also reduces
 * code size a lot. The only non-inline code is
 * UsefulInputBuf_GetBytes().  It is less than 100 bytes so use of
 * @ref UsefulInputBuf doesn't add much code for all the messy
 * hard-to-get right issues with parsing binary protocols in C that it
 * solves.
 *
 * The parse context size is:
 *   - 64-bit machine: 16 + 8 + 2 + 1 (+ 5 bytes padding to align) = 32 bytes
 *   - 32-bit machine: 8 + 4 + 2 + 1 (+ 1 byte padding to align) = 16 bytes
 */
typedef struct useful_input_buf {
	/* PRIVATE DATA STRUCTURE */
	UsefulBufC UB;	   /* Data being parsed */
	size_t	   cursor; /* Current offset in data being parse */
	uint16_t   magic; /* Check for corrupted or uninitialized UsefulInputBuf
			   */
	uint8_t err;	  /* Set request goes off end or magic number is bad */
	uint8_t padding_[5];

} UsefulInputBuf;

#define UIB_MAGIC (0xB00F)

#ifdef ENABLE_DECODE_ROUTINES

/**
 * @brief Initialize the @ref UsefulInputBuf structure before use.
 *
 * @param[in] pUInBuf  Pointer to the @ref UsefulInputBuf.
 * @param[in] UB       The data to parse.
 */
static inline void
UsefulInputBuf_Init(UsefulInputBuf *pUInBuf, UsefulBufC UB);

/**
 * @brief Returns current position in input buffer.
 *
 * @param[in] pUInBuf  Pointer to the @ref UsefulInputBuf.
 *
 * @return Integer position of the cursor.
 *
 * The position that the next bytes will be returned from.
 */
static size_t
UsefulInputBuf_Tell(UsefulInputBuf *pUInBuf);

/**
 * @brief Sets the current position in input buffer.
 *
 * @param[in] pUInBuf  Pointer to the @ref UsefulInputBuf.
 * @param[in] uPos     Position to set to.
 *
 * If the position is off the end of the input buffer, the error state
 * is entered.
 *
 * Seeking to a valid position in the buffer will not reset the error
 * state. Only re-initialization will do that.
 */
static void
UsefulInputBuf_Seek(UsefulInputBuf *pUInBuf, size_t uPos);

/**
 * @brief Returns the number of bytes from the cursor to the end of the buffer,
 * the unconsumed bytes.
 *
 * @param[in] pUInBuf  Pointer to the @ref UsefulInputBuf.
 *
 * @return Number of bytes unconsumed or 0 on error.
 *
 * Returns 0 if the cursor is invalid or corruption of the
 * @ref UsefulInputBuf structure is detected.
 */
static size_t
UsefulInputBuf_BytesUnconsumed(UsefulInputBuf *pUInBuf);

/**
 * @brief Check if there are unconsumed bytes.
 *
 * @param[in] pUInBuf  Pointer to the @ref UsefulInputBuf.
 * @param[in] uLen     Number of bytes to check availability for.
 *
 * @return 1 if @c uLen bytes are available after the cursor, and 0 if not.
 */
static int
UsefulInputBuf_BytesAvailable(UsefulInputBuf *pUInBuf, size_t uLen);

/**
 * @brief Convert a pointer to an offset with bounds checking.
 *
 * @param[in] pUInBuf  Pointer to the @ref UsefulInputBuf.
 * @param[in] p        Pointer to convert to offset.
 *
 * @return SIZE_MAX if @c p is out of range, the byte offset if not.
 */
static inline size_t
UsefulInputBuf_PointerToOffset(UsefulInputBuf *pUInBuf, const void *p);

/**
 * @brief Get pointer to bytes out of the input buffer.
 *
 * @param[in] pUInBuf  Pointer to the @ref UsefulInputBuf.
 * @param[in] uNum     Number of bytes to get.
 *
 * @return Pointer to bytes.
 *
 * This consumes @c uNum bytes from the input buffer. This returns a
 * pointer to the start of the @c uNum bytes.
 *
 * If there are not @c uNum bytes in the input buffer, @c NULL will be
 * returned and the error state is entered.
 *
 * This advances the position cursor by @c uNum bytes.
 */
const void *
UsefulInputBuf_GetBytes(UsefulInputBuf *pUInBuf, size_t uNum);

/**
 * @brief Get @ref UsefulBuf out of the input buffer.
 *
 * @param[in] pUInBuf  Pointer to the @ref UsefulInputBuf.
 * @param[in] uNum     Number of bytes to get.
 *
 * @return A @ref UsefulBufC with ptr and length of bytes consumed.
 *
 * This consumes @c uNum bytes from the input buffer and returns the
 * pointer and length for them as a @ref UsefulBufC. The length
 * returned will always be @c uNum. The position cursor is advanced by
 * @c uNum bytes.
 *
 * If there are not @c uNum bytes in the input buffer, @ref
 * NULLUsefulBufC will be returned and the error state is entered.
 */
static inline UsefulBufC
UsefulInputBuf_GetUsefulBuf(UsefulInputBuf *pUInBuf, size_t uNum);

/**
 * @brief Get a byte out of the input buffer.
 *
 * @param[in] pUInBuf  Pointer to the @ref UsefulInputBuf.
 *
 * @return The byte.
 *
 * This consumes 1 byte from the input buffer, returns it and advances
 * the position cursor by 1.
 *
 * If there is not 1 byte in the buffer, 0 will be returned for the
 * byte and the error state is entered. To know if the 0 returned was
 * in error or the real value, the error state must be checked.  If
 * possible, put this off until all values are retrieved to have
 * smaller and simpler code, but if not possible
 * UsefulInputBuf_GetError() can be called. Also, in the error state
 * UsefulInputBuf_GetBytes() returns @c NULL *or the @c ptr from
 * UsefulInputBuf_GetUsefulBuf() is @c NULL.
 */
static inline uint8_t
UsefulInputBuf_GetByte(UsefulInputBuf *pUInBuf);

/**
 * @brief Get a @c uint16_t out of the input buffer.
 *
 * @param[in] pUInBuf  Pointer to the @ref UsefulInputBuf.
 *
 * @return The @c uint16_t.
 *
 * See UsefulInputBuf_GetByte(). This works the same, except it returns
 * a @c uint16_t and two bytes are consumed.
 *
 * The input bytes are interpreted in network order (big endian).
 */
static inline uint16_t
UsefulInputBuf_GetUint16(UsefulInputBuf *pUInBuf);

/**
 * @brief Get a @c uint32_t out of the input buffer.
 *
 * @param[in] pUInBuf  Pointer to the @ref UsefulInputBuf.
 *
 * @return The @c uint32_t.
 *
 * See UsefulInputBuf_GetByte(). This works the same, except it
 * returns a @c uint32_t and four bytes are consumed.
 *
 * The input bytes are interpreted in network order (big endian).
 */
static uint32_t
UsefulInputBuf_GetUint32(UsefulInputBuf *pUInBuf);

/**
 * @brief Get a @c uint64_t out of the input buffer.
 *
 * @param[in] pUInBuf  Pointer to the @ref UsefulInputBuf.
 *
 * @return The uint64_t.
 *
 * See UsefulInputBuf_GetByte(). This works the same, except it returns
 * a @c uint64_t and eight bytes are consumed.
 *
 * The input bytes are interpreted in network order (big endian).
 */
static uint64_t
UsefulInputBuf_GetUint64(UsefulInputBuf *pUInBuf);

#ifndef USEFULBUF_DISABLE_ALL_FLOAT
/**
 * @brief Get a float out of the input buffer.
 *
 * @param[in] pUInBuf  Pointer to the @ref UsefulInputBuf.
 *
 * @return The float.
 *
 * See UsefulInputBuf_GetByte(). This works the same, except it
 * returns a float and four bytes are consumed.
 *
 * The input bytes are interpreted in network order (big endian).
 */
static float
UsefulInputBuf_GetFloat(UsefulInputBuf *pUInBuf);

/**
 * @brief Get a double out of the input buffer.
 *
 * @param[in] pUInBuf  Pointer to the @ref UsefulInputBuf.
 *
 * @return The double.
 *
 * See UsefulInputBuf_GetByte(). This works the same, except it
 * returns a double and eight bytes are consumed.
 *
 * The input bytes are interpreted in network order (big endian).
 */
static double
UsefulInputBuf_GetDouble(UsefulInputBuf *pUInBuf);
#endif /* USEFULBUF_DISABLE_ALL_FLOAT */

/**
 * @brief Get the error status.
 *
 * @param[in] pUInBuf  Pointer to the @ref UsefulInputBuf.
 *
 * @return 0 if not in the error state, 1 if in the error state.
 *
 * This returns whether the @ref UsefulInputBuf is in the
 * error state or not.
 *
 * The error state is entered for one of these reasons:
 * - Attempt to fetch data past the end of the buffer
 * - Attempt to seek to a position past the end of the buffer
 * - Attempt to get data from an uninitialized  or corrupt instance
 *   of @ref UsefulInputBuf
 *
 * Once in the error state, it can only be cleared by calling
 * UsefulInputBuf_Init().
 *
 * For many use cases, it is possible to only call this once after all
 * the @c UsefulInputBuf_GetXxxx() calls have been made.  This is
 * possible if no reference to the data returned are needed before the
 * error state is checked.
 *
 * In some cases UsefulInputBuf_GetUsefulBuf() or
 * UsefulInputBuf_GetBytes() can stand in for this because they return
 * @c NULL if the error state has been entered. (The others can't stand
 * in because they don't return a clearly distinct error value.)
 */
static int
UsefulInputBuf_GetError(UsefulInputBuf *pUInBuf);

/**
 * @brief Gets the input buffer length.
 *
 * @param[in] pUInBuf  Pointer to the @ref UsefulInputBuf.
 *
 * @return The length of the input buffer.
 *
 * This returns the length of the input buffer set by
 * UsefulInputBuf_Init() or UsefulInputBuf_SetBufferLength().
 */
static inline size_t
UsefulInputBuf_GetBufferLength(UsefulInputBuf *pUInBuf);

/**
 * @brief Alters the input buffer length (use with caution).
 *
 * @param[in] pUInBuf  Pointer to the @ref UsefulInputBuf.
 * @param[in] uNewLen  The new length of the input buffer.
 *
 * This alters the internal remembered length of the input buffer set
 * when UsefulInputBuf_Init() was called.
 *
 * The new length given here should always be equal to or less than
 * the length given when UsefulInputBuf_Init() was called. Making it
 * larger allows @ref UsefulInputBuf to run off the input buffer.
 *
 * The typical use is to set a length shorter than that when
 * initialized to constrain parsing. If
 * UsefulInputBuf_GetBufferLength() was called before this, then the
 * original length can be restored with another call to this.
 *
 * This should be used with caution. It is the only
 * @ref UsefulInputBuf method that can violate the safety of input
 * buffer parsing.
 */
static void
UsefulInputBuf_SetBufferLength(UsefulInputBuf *pUInBuf, size_t uNewLen);

#endif /*  ENABLE_DECODE_ROUTINES */

/*----------------------------------------------------------
 Inline implementations.
 */
static inline int
UsefulBuf_IsNULL(UsefulBuf UB)
{
	return !UB.ptr;
}

static inline int
UsefulBuf_IsNULLC(UsefulBufC UB)
{
	return !UB.ptr;
}

static inline int
UsefulBuf_IsEmpty(UsefulBuf UB)
{
	return !UB.len;
}

static inline int
UsefulBuf_IsEmptyC(UsefulBufC UB)
{
	return !UB.len;
}

static inline int
UsefulBuf_IsNULLOrEmpty(UsefulBuf UB)
{
	return UsefulBuf_IsEmpty(UB) || UsefulBuf_IsNULL(UB);
}

static inline int
UsefulBuf_IsNULLOrEmptyC(UsefulBufC UB)
{
	return UsefulBuf_IsEmptyC(UB) || UsefulBuf_IsNULLC(UB);
}

static inline UsefulBufC
UsefulBuf_Const(const UsefulBuf UB)
{
	UsefulBufC UBC;
	UBC.ptr = UB.ptr;
	UBC.len = UB.len;

	return UBC;
}

static inline UsefulBuf
UsefulBuf_Unconst(const UsefulBufC UBC)
{
	UsefulBuf UB;

	/* -Wcast-qual is a good warning flag to use in general. This is
	 * the one place in UsefulBuf where it needs to be quieted. Since
	 * clang supports GCC pragmas, this works for clang too. */
#pragma GCC diagnostic push
#pragma GCC diagnostic ignored "-Wcast-qual"
	UB.ptr = (void *)UBC.ptr;
#pragma GCC diagnostic pop

	UB.len = UBC.len;

	return UB;
}

static inline UsefulBufC
UsefulBuf_FromSZ(const char *szString)
{
	UsefulBufC UBC;
	UBC.ptr = szString;
	UBC.len = strlen(szString);
	return UBC;
}

static inline UsefulBufC
UsefulBuf_Copy(UsefulBuf Dest, const UsefulBufC Src)
{
	return UsefulBuf_CopyOffset(Dest, 0, Src);
}

static inline UsefulBufC
UsefulBuf_Set(UsefulBuf Dest, uint8_t value)
{
	memset(Dest.ptr, value, Dest.len);

	UsefulBufC UBC;
	UBC.ptr = Dest.ptr;
	UBC.len = Dest.len;

	return UBC;
}

static inline UsefulBufC
UsefulBuf_CopyPtr(UsefulBuf Dest, const void *ptr, size_t len)
{
	UsefulBufC UBC;
	UBC.ptr = ptr;
	UBC.len = len;
	return UsefulBuf_Copy(Dest, UBC);
}

static inline UsefulBufC
UsefulBuf_Head(UsefulBufC UB, size_t uAmount)
{
	if (uAmount > UB.len) {
		return NULLUsefulBufC;
	}
	UsefulBufC UBC;

	UBC.ptr = UB.ptr;
	UBC.len = uAmount;

	return UBC;
}

static inline UsefulBufC
UsefulBuf_Tail(UsefulBufC UB, size_t uAmount)
{
	UsefulBufC ReturnValue;

	if (uAmount > UB.len) {
		ReturnValue = NULLUsefulBufC;
	} else if (UB.ptr == NULL) {
		ReturnValue.ptr = NULL;
		ReturnValue.len = UB.len - uAmount;
	} else {
		ReturnValue.ptr = (const uint8_t *)UB.ptr + uAmount;
		ReturnValue.len = UB.len - uAmount;
	}

	return ReturnValue;
}

static inline size_t
UsefulBuf_PointerToOffset(UsefulBufC UB, const void *p)
{
	if (UB.ptr == NULL) {
		return SIZE_MAX;
	}

	if (p < UB.ptr) {
		/* given pointer is before start of buffer */
		return SIZE_MAX;
	}

	// Cast to size_t (from ptrdiff_t) is OK because of check above
	const size_t uOffset =
		(size_t)((const uint8_t *)p - (const uint8_t *)UB.ptr);

	if (uOffset >= UB.len) {
		/* given pointer is off the end of the buffer */
		return SIZE_MAX;
	}

	return uOffset;
}

#ifndef USEFULBUF_DISABLE_ALL_FLOAT
static inline uint32_t
UsefulBufUtil_CopyFloatToUint32(float f)
{
	uint32_t u32;
	memcpy(&u32, &f, sizeof(uint32_t));
	return u32;
}

static inline uint64_t
UsefulBufUtil_CopyDoubleToUint64(double d)
{
	uint64_t u64;
	memcpy(&u64, &d, sizeof(uint64_t));
	return u64;
}

static inline double
UsefulBufUtil_CopyUint64ToDouble(uint64_t u64)
{
	double d;
	memcpy(&d, &u64, sizeof(uint64_t));
	return d;
}

static inline float
UsefulBufUtil_CopyUint32ToFloat(uint32_t u32)
{
	float f;
	memcpy(&f, &u32, sizeof(uint32_t));
	return f;
}
#endif /* USEFULBUF_DISABLE_ALL_FLOAT */

static inline void
UsefulOutBuf_Reset(UsefulOutBuf *pMe)
{
	pMe->data_len = 0;
	pMe->err      = 0;
}

static inline size_t
UsefulOutBuf_GetEndPosition(UsefulOutBuf *pMe)
{
	return pMe->data_len;
}

static inline int
UsefulOutBuf_AtStart(UsefulOutBuf *pMe)
{
	return 0 == pMe->data_len;
}

static inline void
UsefulOutBuf_InsertData(UsefulOutBuf *pMe, const void *pBytes, size_t uLen,
			size_t uPos)
{
	UsefulBufC Data = { pBytes, uLen };
	UsefulOutBuf_InsertUsefulBuf(pMe, Data, uPos);
}

static inline void
UsefulOutBuf_InsertString(UsefulOutBuf *pMe, const char *szString, size_t uPos)
{
	UsefulBufC UBC;
	UBC.ptr = szString;
	UBC.len = strlen(szString);

	UsefulOutBuf_InsertUsefulBuf(pMe, UBC, uPos);
}

static inline void
UsefulOutBuf_InsertByte(UsefulOutBuf *me, uint8_t byte, size_t uPos)
{
	UsefulOutBuf_InsertData(me, &byte, 1, uPos);
}

static inline void
UsefulOutBuf_InsertUint16(UsefulOutBuf *me, uint16_t uInteger16, size_t uPos)
{
	/* See UsefulOutBuf_InsertUint64() for comments on this code */

	const void *pBytes;

#if defined(USEFULBUF_CONFIG_BIG_ENDIAN)
	pBytes = &uInteger16;

#elif defined(USEFULBUF_CONFIG_HTON)
	uint16_t uTmp = htons(uInteger16);
	pBytes	      = &uTmp;

#elif defined(USEFULBUF_CONFIG_LITTLE_ENDIAN) && defined(USEFULBUF_CONFIG_BSWAP)
	uint16_t uTmp = __builtin_bswap16(uInteger16);
	pBytes	      = &uTmp;

#else
	uint8_t aTmp[2];

	aTmp[0] = (uint8_t)((uInteger16 & 0xff00) >> 8);
	aTmp[1] = (uint8_t)(uInteger16 & 0xff);

	pBytes = aTmp;
#endif

	UsefulOutBuf_InsertData(me, pBytes, 2, uPos);
}

static inline void
UsefulOutBuf_InsertUint32(UsefulOutBuf *pMe, uint32_t uInteger32, size_t uPos)
{
	/* See UsefulOutBuf_InsertUint64() for comments on this code */

	const void *pBytes;

#if defined(USEFULBUF_CONFIG_BIG_ENDIAN)
	pBytes = &uInteger32;

#elif defined(USEFULBUF_CONFIG_HTON)
	uint32_t uTmp = htonl(uInteger32);
	pBytes	      = &uTmp;

#elif defined(USEFULBUF_CONFIG_LITTLE_ENDIAN) && defined(USEFULBUF_CONFIG_BSWAP)
	uint32_t uTmp = __builtin_bswap32(uInteger32);

	pBytes = &uTmp;

#else
	uint8_t aTmp[4];

	aTmp[0] = (uint8_t)((uInteger32 & 0xff000000) >> 24);
	aTmp[1] = (uint8_t)((uInteger32 & 0xff0000) >> 16);
	aTmp[2] = (uint8_t)((uInteger32 & 0xff00) >> 8);
	aTmp[3] = (uint8_t)(uInteger32 & 0xff);

	pBytes = aTmp;
#endif

	UsefulOutBuf_InsertData(pMe, pBytes, 4, uPos);
}

static inline void
UsefulOutBuf_InsertUint64(UsefulOutBuf *pMe, uint64_t uInteger64, size_t uPos)
{
	const void *pBytes;

#if defined(USEFULBUF_CONFIG_BIG_ENDIAN)
	/* We have been told explicitly we are running on a big-endian
	 * machine. Network byte order is big endian, so just copy.  There
	 * is no issue with alignment here because uInteger64 is always
	 * aligned (and it doesn't matter if pBytes is aligned).
	 */
	pBytes = &uInteger64;

#elif defined(USEFULBUF_CONFIG_HTON)
	/* Use system function to handle big- and little-endian. This works
	 * on both big- and little-endian machines, but hton() is not
	 * always available or in a standard place so it is not used by
	 * default. With some compilers and CPUs the code for this is very
	 * compact through use of a special swap instruction and on
	 * big-endian machines hton() will reduce to nothing.
	 */
	uint64_t uTmp = htonll(uInteger64);

	pBytes = &uTmp;

#elif defined(USEFULBUF_CONFIG_LITTLE_ENDIAN) && defined(USEFULBUF_CONFIG_BSWAP)
	/* Use built-in function for byte swapping. This usually compiles
	 * to an efficient special byte swap instruction. Unlike hton() it
	 * does not do this conditionally on the CPU endianness, so this
	 * code is also conditional on USEFULBUF_CONFIG_LITTLE_ENDIAN
	 */
	uint64_t uTmp = __builtin_bswap64(uInteger64);

	pBytes = &uTmp;

#else
	/* Default which works on every CPU with no dependency on anything
	 * from the CPU, compiler, libraries or OS.  This always works, but
	 * it is usually a little larger and slower than hton().
	 */
	uint8_t aTmp[8];

	aTmp[0] = (uint8_t)((uInteger64 & 0xff00000000000000) >> 56);
	aTmp[1] = (uint8_t)((uInteger64 & 0xff000000000000) >> 48);
	aTmp[2] = (uint8_t)((uInteger64 & 0xff0000000000) >> 40);
	aTmp[3] = (uint8_t)((uInteger64 & 0xff00000000) >> 32);
	aTmp[4] = (uint8_t)((uInteger64 & 0xff000000) >> 24);
	aTmp[5] = (uint8_t)((uInteger64 & 0xff0000) >> 16);
	aTmp[6] = (uint8_t)((uInteger64 & 0xff00) >> 8);
	aTmp[7] = (uint8_t)(uInteger64 & 0xff);

	pBytes = aTmp;
#endif

	/* Do the insert */
	UsefulOutBuf_InsertData(pMe, pBytes, sizeof(uint64_t), uPos);
}

#ifndef USEFULBUF_DISABLE_ALL_FLOAT
static inline void
UsefulOutBuf_InsertFloat(UsefulOutBuf *pMe, float f, size_t uPos)
{
	UsefulOutBuf_InsertUint32(pMe, UsefulBufUtil_CopyFloatToUint32(f),
				  uPos);
}

static inline void
UsefulOutBuf_InsertDouble(UsefulOutBuf *pMe, double d, size_t uPos)
{
	UsefulOutBuf_InsertUint64(pMe, UsefulBufUtil_CopyDoubleToUint64(d),
				  uPos);
}
#endif /* USEFULBUF_DISABLE_ALL_FLOAT */

static inline void
UsefulOutBuf_AppendUsefulBuf(UsefulOutBuf *pMe, UsefulBufC NewData)
{
	/* An append is just a insert at the end */
	UsefulOutBuf_InsertUsefulBuf(pMe, NewData,
				     UsefulOutBuf_GetEndPosition(pMe));
}

static inline void
UsefulOutBuf_AppendData(UsefulOutBuf *pMe, const void *pBytes, size_t uLen)
{
	UsefulBufC Data = { pBytes, uLen };
	UsefulOutBuf_AppendUsefulBuf(pMe, Data);
}

static inline void
UsefulOutBuf_AppendString(UsefulOutBuf *pMe, const char *szString)
{
	UsefulBufC UBC;
	UBC.ptr = szString;
	UBC.len = strlen(szString);

	UsefulOutBuf_AppendUsefulBuf(pMe, UBC);
}

static inline void
UsefulOutBuf_AppendByte(UsefulOutBuf *pMe, uint8_t byte)
{
	UsefulOutBuf_AppendData(pMe, &byte, 1);
}

static inline void
UsefulOutBuf_AppendUint16(UsefulOutBuf *pMe, uint16_t uInteger16)
{
	UsefulOutBuf_InsertUint16(pMe, uInteger16,
				  UsefulOutBuf_GetEndPosition(pMe));
}

static inline void
UsefulOutBuf_AppendUint32(UsefulOutBuf *pMe, uint32_t uInteger32)
{
	UsefulOutBuf_InsertUint32(pMe, uInteger32,
				  UsefulOutBuf_GetEndPosition(pMe));
}

static inline void
UsefulOutBuf_AppendUint64(UsefulOutBuf *pMe, uint64_t uInteger64)
{
	UsefulOutBuf_InsertUint64(pMe, uInteger64,
				  UsefulOutBuf_GetEndPosition(pMe));
}

#ifndef USEFULBUF_DISABLE_ALL_FLOAT
static inline void
UsefulOutBuf_AppendFloat(UsefulOutBuf *pMe, float f)
{
	UsefulOutBuf_InsertFloat(pMe, f, UsefulOutBuf_GetEndPosition(pMe));
}

static inline void
UsefulOutBuf_AppendDouble(UsefulOutBuf *pMe, double d)
{
	UsefulOutBuf_InsertDouble(pMe, d, UsefulOutBuf_GetEndPosition(pMe));
}
#endif /* USEFULBUF_DISABLE_ALL_FLOAT */

static inline int
UsefulOutBuf_GetError(UsefulOutBuf *pMe)
{
	return pMe->err;
}

static inline size_t
UsefulOutBuf_RoomLeft(UsefulOutBuf *pMe)
{
	return pMe->UB.len - pMe->data_len;
}

static inline int
UsefulOutBuf_WillItFit(UsefulOutBuf *pMe, size_t uLen)
{
	return uLen <= UsefulOutBuf_RoomLeft(pMe);
}

static inline int
UsefulOutBuf_IsBufferNULL(UsefulOutBuf *pMe)
{
	return pMe->UB.ptr == NULL;
}

static inline UsefulBuf
UsefulOutBuf_GetOutPlace(UsefulOutBuf *pUOutBuf)
{
	UsefulBuf R;

	R.len = UsefulOutBuf_RoomLeft(pUOutBuf);
	if (R.len > 0 && pUOutBuf->UB.ptr != NULL) {
		R.ptr = (uint8_t *)pUOutBuf->UB.ptr + pUOutBuf->data_len;
	} else {
		R.ptr = NULL;
	}

	return R;
}

#ifdef ENABLE_DECODE_ROUTINES

static inline void
UsefulInputBuf_Init(UsefulInputBuf *pMe, UsefulBufC UB)
{
	pMe->cursor = 0;
	pMe->err    = 0;
	pMe->magic  = UIB_MAGIC;
	pMe->UB	    = UB;
}

static inline size_t
UsefulInputBuf_Tell(UsefulInputBuf *pMe)
{
	return pMe->cursor;
}

static inline size_t
UsefulInputBuf_GetBufferLength(UsefulInputBuf *pMe)
{
	return pMe->UB.len;
}

static inline void
UsefulInputBuf_Seek(UsefulInputBuf *pMe, size_t uPos)
{
	if (uPos > pMe->UB.len) {
		pMe->err = 1;
	} else {
		pMe->cursor = uPos;
	}
}

static inline size_t
UsefulInputBuf_BytesUnconsumed(UsefulInputBuf *pMe)
{
	/* Code Reviewers: THIS FUNCTION DOES POINTER MATH */

	/* Magic number is messed up. Either the structure got overwritten
	 * or was never initialized.
	 */
	if (pMe->magic != UIB_MAGIC) {
		return 0;
	}

	/* The cursor is off the end of the input buffer given.
	 * Presuming there are no bugs in this code, this should never happen.
	 * If it so, the struct was corrupted. The check is retained as
	 * as a defense in case there is a bug in this code or the struct is
	 * corrupted.
	 */
	if (pMe->cursor > pMe->UB.len) {
		return 0;
	}

	/* subtraction can't go negative because of check above */
	return pMe->UB.len - pMe->cursor;
}

static inline int
UsefulInputBuf_BytesAvailable(UsefulInputBuf *pMe, size_t uLen)
{
	return UsefulInputBuf_BytesUnconsumed(pMe) >= uLen ? 1 : 0;
}

static inline size_t
UsefulInputBuf_PointerToOffset(UsefulInputBuf *pUInBuf, const void *p)
{
	return UsefulBuf_PointerToOffset(pUInBuf->UB, p);
}

static inline UsefulBufC
UsefulInputBuf_GetUsefulBuf(UsefulInputBuf *pMe, size_t uNum)
{
	const void *pResult = UsefulInputBuf_GetBytes(pMe, uNum);
	if (!pResult) {
		return NULLUsefulBufC;
	} else {
		UsefulBufC UBC;
		UBC.ptr = pResult;
		UBC.len = uNum;
		return UBC;
	}
}

static inline uint8_t
UsefulInputBuf_GetByte(UsefulInputBuf *pMe)
{
	const void *pResult = UsefulInputBuf_GetBytes(pMe, sizeof(uint8_t));

	/* The ternary operator is subject to integer promotion, because
	 * the operands are smaller than int, so cast back to uint8_t is
	 * needed to be completely explicit about types (for static
	 * analyzers).
	 */
	return (uint8_t)(pResult ? *(const uint8_t *)pResult : 0);
}

static inline uint16_t
UsefulInputBuf_GetUint16(UsefulInputBuf *pMe)
{
	const uint8_t *pResult =
		(const uint8_t *)UsefulInputBuf_GetBytes(pMe, sizeof(uint16_t));

	if (!pResult) {
		return 0;
	}

	/* See UsefulInputBuf_GetUint64() for comments on this code */
#if defined(USEFULBUF_CONFIG_BIG_ENDIAN) || defined(USEFULBUF_CONFIG_HTON) ||  \
	defined(USEFULBUF_CONFIG_BSWAP)
	uint16_t uTmp;
	memcpy(&uTmp, pResult, sizeof(uint16_t));

#if defined(USEFULBUF_CONFIG_BIG_ENDIAN)
	return uTmp;

#elif defined(USEFULBUF_CONFIG_HTON)
	return ntohs(uTmp);

#else
	return __builtin_bswap16(uTmp);

#endif

#else

	/* The operations here are subject to integer promotion because the
	 * operands are smaller than int. They will be promoted to unsigned
	 * int for the shift and addition. The cast back to uint16_t is is
	 * needed to be completely explicit about types (for static
	 * analyzers).
	 */
	return (uint16_t)((pResult[0] << 8) + pResult[1]);

#endif
}

static inline uint32_t
UsefulInputBuf_GetUint32(UsefulInputBuf *pMe)
{
	const uint8_t *pResult =
		(const uint8_t *)UsefulInputBuf_GetBytes(pMe, sizeof(uint32_t));

	if (!pResult) {
		return 0;
	}

	/* See UsefulInputBuf_GetUint64() for comments on this code */
#if defined(USEFULBUF_CONFIG_BIG_ENDIAN) || defined(USEFULBUF_CONFIG_HTON) ||  \
	defined(USEFULBUF_CONFIG_BSWAP)
	uint32_t uTmp;
	memcpy(&uTmp, pResult, sizeof(uint32_t));

#if defined(USEFULBUF_CONFIG_BIG_ENDIAN)
	return uTmp;

#elif defined(USEFULBUF_CONFIG_HTON)
	return ntohl(uTmp);

#else
	return __builtin_bswap32(uTmp);

#endif

#else
	return ((uint32_t)pResult[0] << 24) + ((uint32_t)pResult[1] << 16) +
	       ((uint32_t)pResult[2] << 8) + (uint32_t)pResult[3];
#endif
}

static inline uint64_t
UsefulInputBuf_GetUint64(UsefulInputBuf *pMe)
{
	const uint8_t *pResult =
		(const uint8_t *)UsefulInputBuf_GetBytes(pMe, sizeof(uint64_t));

	if (!pResult) {
		return 0;
	}

#if defined(USEFULBUF_CONFIG_BIG_ENDIAN) || defined(USEFULBUF_CONFIG_HTON) ||  \
	defined(USEFULBUF_CONFIG_BSWAP)
	/* pResult will probably not be aligned.  This memcpy() moves the
	 * bytes into a temp variable safely for CPUs that can or can't do
	 * unaligned memory access. Many compilers will optimize the
	 * memcpy() into a simple move instruction.
	 */
	uint64_t uTmp;
	memcpy(&uTmp, pResult, sizeof(uint64_t));

#if defined(USEFULBUF_CONFIG_BIG_ENDIAN)
	/* We have been told expliclity this is a big-endian CPU.  Since
	 * network byte order is big-endian, there is nothing to do.
	 */

	return uTmp;

#elif defined(USEFULBUF_CONFIG_HTON)
	/* We have been told to use ntoh(), the system function to handle
	 * big- and little-endian. This works on both big- and
	 * little-endian machines, but ntoh() is not always available or in
	 * a standard place so it is not used by default. On some CPUs the
	 * code for this is very compact through use of a special swap
	 * instruction.
	 */

	return ntohll(uTmp);

#else
	/* Little-endian (since it is not USEFULBUF_CONFIG_BIG_ENDIAN) and
	 * USEFULBUF_CONFIG_BSWAP (since it is not USEFULBUF_CONFIG_HTON).
	 * __builtin_bswap64() and friends are not conditional on CPU
	 * endianness so this must only be used on little-endian machines.
	 */

	return __builtin_bswap64(uTmp);

#endif

#else
	/* This is the default code that works on every CPU and every
	 * endianness with no dependency on ntoh().  This works on CPUs
	 * that either allow or do not allow unaligned access. It will
	 * always work, but usually is a little less efficient than ntoh().
	 */

	return ((uint64_t)pResult[0] << 56) + ((uint64_t)pResult[1] << 48) +
	       ((uint64_t)pResult[2] << 40) + ((uint64_t)pResult[3] << 32) +
	       ((uint64_t)pResult[4] << 24) + ((uint64_t)pResult[5] << 16) +
	       ((uint64_t)pResult[6] << 8) + (uint64_t)pResult[7];
#endif
}

#ifndef USEFULBUF_DISABLE_ALL_FLOAT
static inline float
UsefulInputBuf_GetFloat(UsefulInputBuf *pMe)
{
	uint32_t uResult = UsefulInputBuf_GetUint32(pMe);

	return uResult ? UsefulBufUtil_CopyUint32ToFloat(uResult) : 0;
}

static inline double
UsefulInputBuf_GetDouble(UsefulInputBuf *pMe)
{
	uint64_t uResult = UsefulInputBuf_GetUint64(pMe);

	return uResult ? UsefulBufUtil_CopyUint64ToDouble(uResult) : 0;
}
#endif /* USEFULBUF_DISABLE_ALL_FLOAT */

static inline int
UsefulInputBuf_GetError(UsefulInputBuf *pMe)
{
	return pMe->err;
}

static inline void
UsefulInputBuf_SetBufferLength(UsefulInputBuf *pMe, size_t uNewLen)
{
	pMe->UB.len = uNewLen;
}
#endif /*  ENABLE_DECODE_ROUTINES */

#ifdef __cplusplus
}
#endif

#endif /* UsefulBuf_h_ */

```

`hyp/interfaces/qcbor/include/qcbor/qcbor_common.h`:

```h
/*==============================================================================
 Copyright (c) 2016-2018, The Linux Foundation.
 Copyright (c) 2018-2022, Laurence Lundblade.
 Copyright (c) 2021, Arm Limited.
 All rights reserved.

Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are
met:
    * Redistributions of source code must retain the above copyright
      notice, this list of conditions and the following disclaimer.
    * Redistributions in binary form must reproduce the above
      copyright notice, this list of conditions and the following
      disclaimer in the documentation and/or other materials provided
      with the distribution.
    * Neither the name of The Linux Foundation nor the names of its
      contributors, nor the name "Laurence Lundblade" may be used to
      endorse or promote products derived from this software without
      specific prior written permission.

THIS SOFTWARE IS PROVIDED "AS IS" AND ANY EXPRESS OR IMPLIED
WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF
MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT
ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS
BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR
BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,
WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE
OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN
IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 =============================================================================*/

#ifndef qcbor_common_h
#define qcbor_common_h

/**
 @file qcbor_common.h

 This define indicates a version of QCBOR that supports spiffy decode,
 the decode functions found in qcbor_spiffy_decode.h.

 Versions of QCBOR that support spiffy decode are backwards compatible
 with previous versions, but there are a few minor exceptions such as
 some aspects of tag handling that are different. This define can be
 used handle these variances.
*/
// #define QCBOR_SPIFFY_DECODE
#define QCBOR_DISABLE_EXP_AND_MANTISSA

/* It was originally defined as QCBOR_CONFIG_DISABLE_EXP_AND_MANTISSA,
 * but this is incosistent with all the other QCBOR_DISABLE_
 * #defines, so the name was changed and this was added for backwards
 * compatibility
 */
#ifdef QCBOR_CONFIG_DISABLE_EXP_AND_MANTISSA
#define QCBOR_DISABLE_EXP_AND_MANTISSA
#endif

/* If USEFULBUF_DISABLE_ALL_FLOATis defined then define
 * QCBOR_DISABLE_FLOAT_HW_USE and QCBOR_DISABLE_PREFERRED_FLOAT
 */
#define USEFULBUF_DISABLE_ALL_FLOAT
#ifdef USEFULBUF_DISABLE_ALL_FLOAT
#ifndef QCBOR_DISABLE_FLOAT_HW_USE
#define QCBOR_DISABLE_FLOAT_HW_USE
#endif /* QCBOR_DISABLE_FLOAT_HW_USE */
#ifndef QCBOR_DISABLE_PREFERRED_FLOAT
#define QCBOR_DISABLE_PREFERRED_FLOAT
#endif /* QCBOR_DISABLE_PREFERRED_FLOAT */
#endif /* USEFULBUF_DISABLE_ALL_FLOAT */

/* Standard CBOR Major type for positive integers of various lengths */
#define CBOR_MAJOR_TYPE_POSITIVE_INT 0

/* Standard CBOR Major type for negative integer of various lengths */
#define CBOR_MAJOR_TYPE_NEGATIVE_INT 1

/* Standard CBOR Major type for an array of arbitrary 8-bit bytes. */
#define CBOR_MAJOR_TYPE_BYTE_STRING 2

/* Standard CBOR Major type for a UTF-8 string. Note this is true 8-bit UTF8
 with no encoding and no NULL termination */
#define CBOR_MAJOR_TYPE_TEXT_STRING 3

/* Standard CBOR Major type for an ordered array of other CBOR data items */
#define CBOR_MAJOR_TYPE_ARRAY 4

/* Standard CBOR Major type for CBOR MAP. Maps an array of pairs. The
 first item in the pair is the "label" (key, name or identfier) and the second
 item is the value.  */
#define CBOR_MAJOR_TYPE_MAP 5

/* Standard CBOR major type for a tag number. This creates a CBOR "tag" that
 * is the tag number and a data item that follows as the tag content.
 *
 * Note that this was called an optional tag in RFC 7049, but there's
 * not really anything optional about it. It was misleading. It is
 * renamed in RFC 8949.
 */
#define CBOR_MAJOR_TYPE_TAG	 6
#define CBOR_MAJOR_TYPE_OPTIONAL 6

/* Standard CBOR extra simple types like floats and the values true and false */
#define CBOR_MAJOR_TYPE_SIMPLE 7

/*
 These are special values for the AdditionalInfo bits that are part of
 the first byte.  Mostly they encode the length of the data item.
 */
#define LEN_IS_ONE_BYTE	   24
#define LEN_IS_TWO_BYTES   25
#define LEN_IS_FOUR_BYTES  26
#define LEN_IS_EIGHT_BYTES 27
#define ADDINFO_RESERVED1  28
#define ADDINFO_RESERVED2  29
#define ADDINFO_RESERVED3  30
#define LEN_IS_INDEFINITE  31

/*
 24 is a special number for CBOR. Integers and lengths
 less than it are encoded in the same byte as the major type.
 */
#define CBOR_TWENTY_FOUR 24

/*
 Tags that are used with CBOR_MAJOR_TYPE_OPTIONAL. These
 are types defined in RFC 8949 and some additional ones
 in the IANA CBOR tags registry.
 */
/** See QCBOREncode_AddDateString(). */
#define CBOR_TAG_DATE_STRING 0
/** See QCBOREncode_AddDateEpoch(). */
#define CBOR_TAG_DATE_EPOCH 1
/** See QCBOREncode_AddPositiveBignum(). */
#define CBOR_TAG_POS_BIGNUM 2
/** See QCBOREncode_AddNegativeBignum(). */
#define CBOR_TAG_NEG_BIGNUM 3
/** CBOR tag for a two-element array representing a fraction with a
    mantissa and base-10 scaling factor. See QCBOREncode_AddDecimalFraction()
    and @ref expAndMantissa.
  */
#define CBOR_TAG_DECIMAL_FRACTION 4
/** CBOR tag for a two-element array representing a fraction with a
    mantissa and base-2 scaling factor. See QCBOREncode_AddBigFloat()
    and @ref expAndMantissa. */
#define CBOR_TAG_BIGFLOAT 5
/** Not Decoded by QCBOR. Tag for COSE format encryption with no recipient
    identification. See [RFC 8152, COSE]
    (https://tools.ietf.org/html/rfc8152). No API is provided for this
    tag. */
#define CBOR_TAG_COSE_ENCRYPT0 16
#define CBOR_TAG_COSE_ENCRYPTO 16
/** Not Decoded by QCBOR. Tag for COSE format MAC'd data with no recipient
    identification. See [RFC 8152, COSE]
    (https://tools.ietf.org/html/rfc8152). No API is provided for this
    tag.*/
#define CBOR_TAG_COSE_MAC0 17
/** Tag for COSE format single signature signing. No API is provided
    for this tag. See [RFC 8152, COSE]
    (https://tools.ietf.org/html/rfc8152). */
#define CBOR_TAG_COSE_SIGN1 18
/** A hint that the following byte string should be encoded in
    Base64URL when converting to JSON or similar text-based
    representations. Call @c
    QCBOREncode_AddTag(pCtx,CBOR_TAG_ENC_AS_B64URL) before the call to
    QCBOREncode_AddBytes(). */
#define CBOR_TAG_ENC_AS_B64URL 21
/** A hint that the following byte string should be encoded in Base64
    when converting to JSON or similar text-based
    representations. Call @c
    QCBOREncode_AddTag(pCtx,CBOR_TAG_ENC_AS_B64) before the call to
    QCBOREncode_AddBytes(). */
#define CBOR_TAG_ENC_AS_B64 22
/** A hint that the following byte string should be encoded in base-16
    format per [RFC 4648] (https://tools.ietf.org/html/rfc4648) when
    converting to JSON or similar text-based
    representations. Essentially, Base-16 encoding is the standard
    case- insensitive hex encoding and may be referred to as
    "hex". Call @c QCBOREncode_AddTag(pCtx,CBOR_TAG_ENC_AS_B16) before
    the call to QCBOREncode_AddBytes(). */
#define CBOR_TAG_ENC_AS_B16 23
/** See QCBORDecode_EnterBstrWrapped()). */
#define CBOR_TAG_CBOR 24
/** See QCBOREncode_AddURI(). */
#define CBOR_TAG_URI 32
/** See QCBOREncode_AddB64URLText(). */
#define CBOR_TAG_B64URL 33
/** See QCBOREncode_AddB64Text(). */
#define CBOR_TAG_B64 34
/** See QCBOREncode_AddRegex(). */
#define CBOR_TAG_REGEX 35
/** See QCBOREncode_AddMIMEData(). */
#define CBOR_TAG_MIME 36
/** See QCBOREncode_AddBinaryUUID(). */
#define CBOR_TAG_BIN_UUID 37
/** The data is a CBOR Web Token per [RFC 8392]
    (https://tools.ietf.org/html/rfc8932). No API is provided for this
    tag. */
#define CBOR_TAG_CWT 61
/** Tag for COSE format encryption. See [RFC 8152, COSE]
    (https://tools.ietf.org/html/rfc8152). No API is provided for this
    tag. */
#define CBOR_TAG_CBOR_SEQUENCE 63
/** Not Decoded by QCBOR. Tag for COSE format encryption with recipient
    identification. See [RFC 8152, COSE]
    (https://tools.ietf.org/html/rfc8152). No API is provided for this
    tag. */
#define CBOR_TAG_COSE_ENCRYPT 96
#define CBOR_TAG_ENCRYPT      96
/** Not Decoded by QCBOR. Tag for COSE format MAC. See [RFC 8152, COSE]
    (https://tools.ietf.org/html/rfc8152). No API is provided for this
    tag. */
#define CBOR_TAG_COSE_MAC 97
#define CBOR_TAG_MAC	  97
/** Not Decoded by QCBOR. Tag for COSE format signed data. See [RFC 8152, COSE]
    (https://tools.ietf.org/html/rfc8152). No API is provided for this
    tag. */
#define CBOR_TAG_COSE_SIGN 98
#define CBOR_TAG_SIGN	   98
/** Tag for date counted by days from Jan 1 1970 per [RFC 8943]
    (https://tools.ietf.org/html/rfc8943). See
    QCBOREncode_AddTDaysEpoch(). */
#define CBOR_TAG_DAYS_EPOCH 100
/** Not Decoded by QCBOR. World geographic coordinates. See ISO 6709, [RFC 5870]
    (https://tools.ietf.org/html/rfc5870) and WGS-84. No API is
    provided for this tag. */
#define CBOR_TAG_GEO_COORD 103
/** Binary MIME.*/
#define CBOR_TAG_BINARY_MIME 257
/** Tag for date string without time or time zone per [RFC 8943]
    (https://tools.ietf.org/html/rfc8943). See
    QCBOREncode_AddTDaysString(). */
#define CBOR_TAG_DAYS_STRING 1004
/** The magic number, self-described CBOR. No API is provided for this
    tag. */
#define CBOR_TAG_CBOR_MAGIC 55799

/** The 16-bit invalid tag from the CBOR tags registry */
#define CBOR_TAG_INVALID16 0xffff
/** The 32-bit invalid tag from the CBOR tags registry */
#define CBOR_TAG_INVALID32 0xffffffff
/** The 64-bit invalid tag from the CBOR tags registry */
#define CBOR_TAG_INVALID64 0xffffffffffffffff

/*
 Values for the 5 bits for items of major type 7
 */
#define CBOR_SIMPLEV_FALSE	    20
#define CBOR_SIMPLEV_TRUE	    21
#define CBOR_SIMPLEV_NULL	    22
#define CBOR_SIMPLEV_UNDEF	    23
#define CBOR_SIMPLEV_ONEBYTE	    24
#define HALF_PREC_FLOAT		    25
#define SINGLE_PREC_FLOAT	    26
#define DOUBLE_PREC_FLOAT	    27
#define CBOR_SIMPLE_BREAK	    31
#define CBOR_SIMPLEV_RESERVED_START CBOR_SIMPLEV_ONEBYTE
#define CBOR_SIMPLEV_RESERVED_END   CBOR_SIMPLE_BREAK

/**
 * Error codes returned by QCBOR Encoder and Decoder.
 *
 * The errors are grouped to keep the code size of
 * QCBORDecode_IsNotWellFormedError() and
 * QCBORDecode_IsUnrecoverableError() minimal.
 *
 *    1..19: Encode errors
 *    20..: Decode errors
 *    20-39: QCBORDecode_IsNotWellFormedError()
 *    30..59: QCBORDecode_IsUnrecoverableError()
 *    60..: Other decode errors
 *
 * Error renumbering may occur in the future when new error codes are
 * added for new QCBOR features.
 */
typedef enum {
	/** The encode or decode completely correctly. */
	QCBOR_SUCCESS = 0,

	/** The buffer provided for the encoded output when doing encoding
	    was too small and the encoded output will not fit. */
	QCBOR_ERR_BUFFER_TOO_SMALL = 1,

	/** During encoding, an attempt to create simple value between 24
	    and 31. */
	QCBOR_ERR_ENCODE_UNSUPPORTED = 2,

	/** During encoding, the length of the encoded CBOR exceeded
	    QCBOR_MAX_ARRAY_OFFSET, which is slightly less than
	    @c UINT32_MAX. */
	QCBOR_ERR_BUFFER_TOO_LARGE = 3,

	/** During encoding, the array or map nesting was deeper than this
	    implementation can handle. Note that in the interest of code
	    size and memory use, this implementation has a hard limit on
	    array nesting. The limit is defined as the constant @ref
	    QCBOR_MAX_ARRAY_NESTING. */
	QCBOR_ERR_ARRAY_NESTING_TOO_DEEP = 4,

	/** During encoding, @c QCBOREncode_CloseXxx() called with a
	    different type than is currently open.  */
	QCBOR_ERR_CLOSE_MISMATCH = 5,

	/** During encoding, the array or map had too many items in it.
	    This limit @ref QCBOR_MAX_ITEMS_IN_ARRAY, typically 65,535. */
	QCBOR_ERR_ARRAY_TOO_LONG = 6,

	/** During encoding, more arrays or maps were closed than
	    opened. This is a coding error on the part of the caller of the
	    encoder. */
	QCBOR_ERR_TOO_MANY_CLOSES = 7,

	/** During encoding the number of array or map opens was not
	    matched by the number of closes. Also occurs with opened
	    byte strings that are not closed. */
	QCBOR_ERR_ARRAY_OR_MAP_STILL_OPEN = 8,

	/** During encode, opening a byte string while a byte string is open
	    is not allowed. . */
	QCBOR_ERR_OPEN_BYTE_STRING = 9,

	/** Trying to cancel a byte string wrapping after items have been
	    added to it. */
	QCBOR_ERR_CANNOT_CANCEL = 10,

#define QCBOR_START_OF_NOT_WELL_FORMED_ERRORS 20

	/** During decoding, the CBOR is not well-formed because a simple
	    value between 0 and 31 is encoded in a two-byte integer rather
	    than one. */
	QCBOR_ERR_BAD_TYPE_7 = 20,

	/** During decoding, returned by QCBORDecode_Finish() if all the
	    inputs bytes have not been consumed. This is considered not
	    well-formed. */
	QCBOR_ERR_EXTRA_BYTES = 21,

	/** During decoding, some CBOR construct was encountered that this
	    decoder doesn't support, primarily this is the reserved
	    additional info values, 28 through 30. The CBOR is not
	    well-formed.*/
	QCBOR_ERR_UNSUPPORTED = 22,

	/** During decoding, the an array or map was not fully consumed.
	    Returned by QCBORDecode_Finish(). The CBOR is not
	    well-formed. */
	QCBOR_ERR_ARRAY_OR_MAP_UNCONSUMED = 23,

	/** During decoding, an integer type is encoded with a bad length
	    (that of an indefinite length string). The CBOR is not-well
	    formed. */
	QCBOR_ERR_BAD_INT = 24,

#define QCBOR_START_OF_UNRECOVERABLE_DECODE_ERRORS 30

	/** During decoding, one of the chunks in an indefinite-length
	    string is not of the type of the start of the string.  The CBOR
	    is not well-formed.  This error makes no further decoding
	    possible. */
	QCBOR_ERR_INDEFINITE_STRING_CHUNK = 30,

	/** During decoding, hit the end of the given data to decode. For
	    example, a byte string of 100 bytes was expected, but the end
	    of the input was hit before finding those 100 bytes.  Corrupted
	    CBOR input will often result in this error. See also @ref
	    QCBOR_ERR_NO_MORE_ITEMS. The CBOR is not well-formed.  This
	    error makes no further decoding possible. */
	QCBOR_ERR_HIT_END = 31,

	/** During decoding, a break occurred outside an indefinite-length
	    item. The CBOR is not well-formed. This error makes no further
	    decoding possible. */
	QCBOR_ERR_BAD_BREAK = 32,

#define QCBOR_END_OF_NOT_WELL_FORMED_ERRORS 39

	/** During decoding, the input is too large. It is greater than
	    QCBOR_MAX_DECODE_INPUT_SIZE. This is an implementation limit.
	    This error makes no further decoding possible. */
	QCBOR_ERR_INPUT_TOO_LARGE = 40,

	/** During decoding, the array or map nesting was deeper than this
	    implementation can handle. Note that in the interest of code
	    size and memory use, this implementation has a hard limit on
	    array nesting. The limit is defined as the constant @ref
	    QCBOR_MAX_ARRAY_NESTING. This error makes no further decoding
	    possible. */
	QCBOR_ERR_ARRAY_DECODE_NESTING_TOO_DEEP = 41,

	/** During decoding, the array or map had too many items in it.
	    This limit @ref QCBOR_MAX_ITEMS_IN_ARRAY, typically 65,534,
	    UINT16_MAX - 1. This error makes no further decoding
	    possible. */
	QCBOR_ERR_ARRAY_DECODE_TOO_LONG = 42,

	/** When decoding, a string's size is greater than what a size_t
	    can hold less 4. In all but some very strange situations this
	    is because of corrupt input CBOR and should be treated as
	    such. The strange situation is a CPU with a very small size_t
	    (e.g., a 16-bit CPU) and a large string (e.g., > 65KB). This
	    error makes no further decoding possible. */
	QCBOR_ERR_STRING_TOO_LONG = 43,

	/** Something is wrong with a decimal fraction or bigfloat such as
	    it not consisting of an array with two integers. This error
	    makes no further decoding possible. */
	QCBOR_ERR_BAD_EXP_AND_MANTISSA = 44,

	/** Unable to decode an indefinite-length string because no string
	    allocator was configured. See QCBORDecode_SetMemPool() or
	    QCBORDecode_SetUpAllocator().  This error makes no further
	    decoding possible. */
	QCBOR_ERR_NO_STRING_ALLOCATOR = 45,

	/** Error allocating space for a string, usually for an
	    indefinite-length string. This error makes no further decoding
	    possible. */
	QCBOR_ERR_STRING_ALLOCATE = 46,

	/** During decoding, the type of the label for a map entry is not
	    one that can be handled in the current decoding mode. Typically
	    this is because a label is not an intger or a string. This is
	    an implemation limit. */
	QCBOR_ERR_MAP_LABEL_TYPE = 47,

	/** When the built-in tag decoding encounters an unexpected type,
	    this error is returned. This error is unrecoverable because the
	    built-in tag decoding doesn't try to consume the unexpected
	    type. In previous versions of QCBOR this was considered a
	    recoverable error hence QCBOR_ERR_BAD_TAG_CONTENT. Going back
	    further, RFC 7049 use the name "optional tags". That name is no
	    longer used because "optional" was causing confusion. See
	    also @ref QCBOR_ERR_RECOVERABLE_BAD_TAG_CONTENT. */
	QCBOR_ERR_UNRECOVERABLE_TAG_CONTENT = 48,
	QCBOR_ERR_BAD_TAG_CONTENT	    = 48,
	QCBOR_ERR_BAD_OPT_TAG		    = 48,

	/** Indefinite length string handling is disabled and there is an
	    indefinite length string in the input CBOR. */
	QCBOR_ERR_INDEF_LEN_STRINGS_DISABLED = 49,

	/** Indefinite length arrays and maps handling are disabled and there is
	   an indefinite length map or array in the input CBOR. */
	QCBOR_ERR_INDEF_LEN_ARRAYS_DISABLED = 50,

#define QCBOR_END_OF_UNRECOVERABLE_DECODE_ERRORS 59

	/** More than @ref QCBOR_MAX_TAGS_PER_ITEM tags encountered for a
	    CBOR ITEM.  @ref QCBOR_MAX_TAGS_PER_ITEM is a limit of this
	    implementation.  During decoding, too many tags in the
	    caller-configured tag list, or not enough space in @ref
	    QCBORTagListOut. This error makes no further decoding
	    possible.  */
	QCBOR_ERR_TOO_MANY_TAGS = 60,

	/** When decoding for a specific type, the type was not was
	    expected.  */
	QCBOR_ERR_UNEXPECTED_TYPE = 61,

	/** Duplicate label in map detected. */
	QCBOR_ERR_DUPLICATE_LABEL = 62,

	/** During decoding, the buffer given to QCBORDecode_SetMemPool()
	    is either too small, smaller than
	    QCBOR_DECODE_MIN_MEM_POOL_SIZE or too large, larger than
	    UINT32_MAX. */
	QCBOR_ERR_MEM_POOL_SIZE = 63,

	/** During decoding, an integer smaller than INT64_MIN was received
	    (CBOR can represent integers smaller than INT64_MIN, but C
	    cannot). */
	QCBOR_ERR_INT_OVERFLOW = 64,

	/** During decoding, a date greater than +- 292 billion years from
	    Jan 1 1970 encountered during parsing. This is an
	    implementation limit. */
	QCBOR_ERR_DATE_OVERFLOW = 65,

	/** During decoding, @c QCBORDecode_ExitXxx() was called for a
	    different type than @c QCBORDecode_EnterXxx(). */
	QCBOR_ERR_EXIT_MISMATCH = 66,

	/** All well-formed data items have been consumed and there are no
	    more. If parsing a CBOR stream this indicates the non-error end
	    of the stream. If not parsing a CBOR stream / sequence, this
	    probably indicates that some data items expected are not
	    present.  See also @ref QCBOR_ERR_HIT_END. */
	QCBOR_ERR_NO_MORE_ITEMS = 67,

	/** When finding an item by label, an item with the requested label
	    was not found. */
	QCBOR_ERR_LABEL_NOT_FOUND = 68,

	/** Number conversion failed because of sign. For example a
	    negative int64_t can't be converted to a uint64_t */
	QCBOR_ERR_NUMBER_SIGN_CONVERSION = 69,

	/** When converting a decoded number, the value is too large or to
	    small for the conversion target */
	QCBOR_ERR_CONVERSION_UNDER_OVER_FLOW = 70,

	/** Trying to get an item by label when a map has not been
	    entered. */
	QCBOR_ERR_MAP_NOT_ENTERED = 71,

	/** A @ref QCBORItemCallback callback indicates processing should not
	    continue for some  non-CBOR reason. */
	QCBOR_ERR_CALLBACK_FAIL = 72,

	/** This error code is deprecated. Instead,
	    @ref QCBOR_ERR_HALF_PRECISION_DISABLED,
	    @ref QCBOR_ERR_HW_FLOAT_DISABLED or @ref
	   QCBOR_ERR_ALL_FLOAT_DISABLED is returned depending on the specific
	   floating-point functionality that is disabled and the type of
	   floating-point input. */
	QCBOR_ERR_FLOAT_DATE_DISABLED = 73,

	/** Support for half-precision float decoding is disabled. */
	QCBOR_ERR_HALF_PRECISION_DISABLED = 74,

	/** Use of floating-point HW is disabled. This affects all type
	    conversions to and from double and float types. */
	QCBOR_ERR_HW_FLOAT_DISABLED = 75,

	/** Unable to complete operation because a floating-point value
	    that is a NaN (not a number), that is too large, too small,
	    infinity or -infinity was encountered in encoded CBOR. Usually
	    this because conversion of the float-point value was being
	    attempted. */
	QCBOR_ERR_FLOAT_EXCEPTION = 76,

	/** Floating point support is completely turned off, encoding/decoding
	    floating point numbers is not possible. */
	QCBOR_ERR_ALL_FLOAT_DISABLED = 77,

	/** Like @ref QCBOR_ERR_UNRECOVERABLE_TAG_CONTENT, but recoverable.
	    If an implementation decodes a tag and can and does consume the
	    whole tag contents when it is not the correct tag content, this
	    error can be returned. None of the built-in tag decoders do
	    this (to save object code). */
	QCBOR_ERR_RECOVERABLE_BAD_TAG_CONTENT = 78

	/* This is stored in uint8_t; never add values > 255 */
} QCBORError;

/* Function for getting an error string from an error code */
const char *
qcbor_err_to_str(QCBORError err);

/**
 The maximum nesting of arrays and maps when encoding or decoding. The
 error @ref QCBOR_ERR_ARRAY_NESTING_TOO_DEEP will be returned on
 encoding or QCBOR_ERR_ARRAY_DECODE_NESTING_TOO_DEEP on decoding if it is
 exceeded.
 */
#define QCBOR_MAX_ARRAY_NESTING QCBOR_MAX_ARRAY_NESTING1

/**
 * The maximum number of items in a single array or map when encoding of
 * decoding.
 */
/* -1 because the value UINT16_MAX is used to track indefinite-length arrays */
#define QCBOR_MAX_ITEMS_IN_ARRAY (UINT16_MAX - 1)

/**
 This is deprecated. See QCBORDecode_GetNthTag() and
 QCBORDecode_GetNthTagOfLast() for tag handling.

 The maximum number of tags that can be in @ref QCBORTagListIn and passed to
 QCBORDecode_SetCallerConfiguredTagList()
 */
#define QCBOR_MAX_CUSTOM_TAGS 16

#endif /* qcbor_common_h */

```

`hyp/interfaces/qcbor/include/qcbor/qcbor_encode.h`:

```h
/*==============================================================================
 Copyright (c) 2016-2018, The Linux Foundation.
 Copyright (c) 2018-2021, Laurence Lundblade.
 Copyright (c) 2021, Arm Limited.
 All rights reserved.

Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are
met:
    * Redistributions of source code must retain the above copyright
      notice, this list of conditions and the following disclaimer.
    * Redistributions in binary form must reproduce the above
      copyright notice, this list of conditions and the following
      disclaimer in the documentation and/or other materials provided
      with the distribution.
    * Neither the name of The Linux Foundation nor the names of its
      contributors, nor the name "Laurence Lundblade" may be used to
      endorse or promote products derived from this software without
      specific prior written permission.

THIS SOFTWARE IS PROVIDED "AS IS" AND ANY EXPRESS OR IMPLIED
WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF
MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT
ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS
BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR
BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,
WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE
OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN
IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 =============================================================================*/

#ifndef qcbor_encode_h
#define qcbor_encode_h

#include <stdbool.h>

#include "qcbor/qcbor_common.h"
#include "qcbor/qcbor_private.h"

#ifdef __cplusplus
extern "C" {
#if 0
} // Keep editor indention formatting happy
#endif
#endif

/**
 @file qcbor_encode.h

 @anchor Overview

 # QCBOR Overview

 This implements CBOR -- Concise Binary Object Representation as
 defined in [RFC 8949] (https://tools.ietf.org/html/rfc8949). More
 information is at http://cbor.io.  This is a near-complete implementation of
 the specification. [RFC 8742] (https://tools.ietf.org/html/rfc8742) CBOR
 Sequences is also supported. Limitations are listed further down.

 See @ref Encoding for general discussion on encoding,
 @ref BasicDecode for general discussion on the basic decode features
 and @ref SpiffyDecode for general discussion on the easier-to-use
 decoder functions.

 CBOR is intentionally designed to be translatable to JSON, but not
 all CBOR can convert to JSON. See RFC 8949 for more info on how to
 construct CBOR that is the most JSON friendly.

 The memory model for encoding and decoding is that encoded CBOR must
 be in a contiguous buffer in memory.  During encoding the caller must
 supply an output buffer and if the encoding would go off the end of
 the buffer an error is returned.  During decoding the caller supplies
 the encoded CBOR in a contiguous buffer and the decoder returns
 pointers and lengths into that buffer for strings.

 This implementation does not require malloc. All data structures
 passed in/out of the APIs can fit on the stack.

 Decoding of indefinite-length strings is a special case that requires
 a "string allocator" to allocate memory into which the segments of
 the string are coalesced. Without this, decoding will error out if an
 indefinite-length string is encountered (indefinite-length maps and
 arrays do not require the string allocator). A simple string
 allocator called MemPool is built-in and will work if supplied with a
 block of memory to allocate. The string allocator can optionally use
 malloc() or some other custom scheme.

 Here are some terms and definitions:

 - "Item", "Data Item": An integer or string or such. The basic "thing" that
 CBOR is about. An array is an item itself that contains some items.

 - "Array": An ordered sequence of items, the same as JSON.

 - "Map": A collection of label/value pairs. Each pair is a data
 item. A JSON "object" is the same as a CBOR "map".

 - "Label": The data item in a pair in a map that names or identifies
 the pair, not the value. This implementation refers to it as a
 "label".  JSON refers to it as the "name". The CBOR RFC refers to it
 this as a "key".  This implementation chooses label instead because
 key is too easily confused with a cryptographic key. The COSE
 standard, which uses CBOR, has also chosen to use the term "label"
 rather than "key" for this same reason.

 - "Key": See "Label" above.

 - "Tag": A data item that is an explicitly labeled new data
 type made up of the tagging integer and the tag content.
 See @ref Tags-Overview and @ref Tag-Usage.

 - "Initial Byte": The first byte of an encoded item. Encoding and
 decoding of this byte is taken care of by the implementation.

 - "Additional Info": In addition to the major type, all data items
 have some other info. This is usually the length of the data but can
 be several other things. Encoding and decoding of this is taken care
 of by the implementation.

 CBOR has two mechanisms for tagging and labeling the data values like
 integers and strings. For example, an integer that represents
 someone's birthday in epoch seconds since Jan 1, 1970 could be
 encoded like this:

 - First it is CBOR_MAJOR_TYPE_POSITIVE_INT (@ref QCBOR_TYPE_INT64),
 the primitive positive integer.

 - Next it has a "tag" @ref CBOR_TAG_DATE_EPOCH indicating the integer
 represents a date in the form of the number of seconds since Jan 1,
 1970.

 - Last it has a string "label" like "BirthDate" indicating the
 meaning of the data.

 The encoded binary looks like this:

      a1                      # Map of 1 item
	 69                   # Indicates text string of 9 bytes
	   426972746844617465 # The text "BirthDate"
	c1                    # Tags next integer as epoch date
	   1a                 # Indicates a 4-byte integer
	       580d4172       # unsigned integer date 1477263730

 Implementors using this API will primarily work with
 labels. Generally, tags are only needed for making up new data
 types. This implementation covers most of the data types defined in
 the RFC using tags. It also, allows for the use of custom tags if
 necessary.

 This implementation explicitly supports labels that are text strings
 and integers. Text strings translate nicely into JSON objects and are
 very readable.  Integer labels are much less readable but can be very
 compact. If they are in the range of 0 to 23, they take up only one
 byte.

 CBOR allows a label to be any type of data including an array or a
 map. It is possible to use this API to construct and parse such
 labels, but it is not explicitly supported.

 @anchor Encoding

 ## Encoding

 A common encoding usage mode is to invoke the encoding twice. First
 with the output buffer as @ref SizeCalculateUsefulBuf to compute the
 length of the needed output buffer. The correct sized output buffer
 is allocated. The encoder is invoked a second time with the allocated
 output buffer.

 The double invocation is not required if the maximum output buffer
 size can be predicted. This is usually possible for simple CBOR
 structures.

 If a buffer too small to hold the encoded output is given, the error
 @ref QCBOR_ERR_BUFFER_TOO_SMALL will be returned. Data will never be
 written off the end of the output buffer no matter which functions
 here are called or what parameters are passed to them.

 The encoding error handling is simple. The only possible errors are
 trying to encode structures that are too large or too complex. There
 are no internal malloc calls so there will be no failures for out of
 memory.  The error state is tracked internally, so there is no need
 to check for errors when encoding. Only the return code from
 QCBOREncode_Finish() need be checked as once an error happens, the
 encoder goes into an error state and calls to it to add more data
 will do nothing. An error check is not needed after every data item
 is added.

 Encoding generally proceeds by calling QCBOREncode_Init(), calling
 lots of @c QCBOREncode_AddXxx() functions and calling
 QCBOREncode_Finish(). There are many @c QCBOREncode_AddXxx()
 functions for various data types. The input buffers need only to be
 valid during the @c QCBOREncode_AddXxx() calls as the data is copied
 into the output buffer.

 There are three `Add` functions for each data type. The first / main
 one for the type is for adding the data item to an array.  The second
 one's name ends in `ToMap`, is used for adding data items to maps and
 takes a string argument that is its label in the map. The third one
 ends in `ToMapN`, is also used for adding data items to maps, and
 takes an integer argument that is its label in the map.

 The simplest aggregate type is an array, which is a simple ordered
 set of items without labels the same as JSON arrays. Call
 QCBOREncode_OpenArray() to open a new array, then various @c
 QCBOREncode_AddXxx() functions to put items in the array and then
 QCBOREncode_CloseArray(). Nesting to the limit @ref
 QCBOR_MAX_ARRAY_NESTING is allowed.  All opens must be matched by
 closes or an encoding error will be returned.

 The other aggregate type is a map which does use labels. The `Add`
 functions that end in `ToMap` and `ToMapN` are convenient ways to add
 labeled data items to a map. You can also call any type of `Add`
 function once to add a label of any time and then call any type of
 `Add` again to add its value.

 Note that when you nest arrays or maps in a map, the nested array or
 map has a label.

 Many CBOR-based protocols start with an array or map. This makes them
 self-delimiting. No external length or end marker is needed to know
 the end. It is also possible not start this way, in which case this
 it is usually called a CBOR sequence which is described in
 [RFC 8742] (https://tools.ietf.org/html/rfc8742). This encoder supports
 either just by whether the first item added is an array, map or other.

 If QCBOR is compiled with QCBOR_DISABLE_ENCODE_USAGE_GUARDS defined,
 the errors QCBOR_ERR_CLOSE_MISMATCH, QCBOR_ERR_ARRAY_TOO_LONG,
 QCBOR_ERR_TOO_MANY_CLOSES, QCBOR_ERR_ARRAY_OR_MAP_STILL_OPEN, and
 QCBOR_ERR_ENCODE_UNSUPPORTED will never be returned. It is up to the
 caller to make sure that opened maps, arrays and byte-string wrapping
 is closed correctly and that QCBOREncode_AddType7() is called
 correctly.  With this defined, it is easier to make a mistake when
 authoring the encoding of a protocol that will output not well formed
 CBOR, but as long as the calling code is correct, it is safe to
 disable these checks. Bounds checking that prevents security issues
 in the code is still enforced. This define reduces the size of
 encoding object code by about 150 bytes.

 @anchor Tags-Overview

 ## Tags Overview

 Any CBOR data item can be made into a tag to add semantics, define a
 new data type or such. Some tags are fully standardized and some are
 just registered. Others are not registered and used in a proprietary
 way.

 Encoding and decoding of many of the registered tags is fully
 implemented by QCBOR. It is also possible to encode and decode tags
 that are not directly supported.  For many use cases the built-in tag
 support should be adequate.

 For example, the registered epoch date tag is supported in encoding
 by QCBOREncode_AddDateEpoch() and in decoding by @ref
 QCBOR_TYPE_DATE_EPOCH and the @c epochDate member of @ref
 QCBORItem. This is typical of the built-in tag support. There is an
 API to encode data for it and a @c QCBOR_TYPE_XXX when it is decoded.

 Tags are registered in the [IANA CBOR Tags Registry]
 (https://www.iana.org/assignments/cbor-tags/cbor-tags.xhtml). There
 are roughly three options to create a new tag. First, a public
 specification can be created and the new tag registered with IANA.
 This is the most formal. Second, the new tag can be registered with
 IANA with just a short description rather than a full specification.
 These tags must be greater than 256. Third, a tag can be used without
 any IANA registration, though the registry should be checked to see
 that the new value doesn't collide with one that is registered. The
 value of these tags must be 256 or larger.

 See also @ref CBORTags and @ref Tag-Usage

 The encoding side of tags not built-in is handled by
 QCBOREncode_AddTag() and is relatively simple. Tag decoding is more
 complex and mainly handled by QCBORDecode_GetNext(). Decoding of the
 structure of tagged data not built-in (if there is any) has to be
 implemented by the caller.

 @anchor Floating-Point

 ## Floating-Point

 By default QCBOR fully supports IEEE 754 floating-point:
  - Encode/decode of double, single and half-precision
  - CBOR preferred serialization of floating-point
  - Floating-point epoch dates

 For the most part, the type double is used in the interface for
 floating-point values. In the default configuration, all decoded
 floating-point values are returned as a double.

 With CBOR preferred serialization, the encoder outputs the smallest
 representation of the double or float that preserves precision. Zero,
 NaN and infinity are always output as a half-precision, each taking
 just 2 bytes. This reduces the number of bytes needed to encode
 double and single-precision, especially if zero, NaN and infinity are
 frequently used.

 To avoid use of preferred serialization in the standard configuration
 when encoding, use QCBOREncode_AddDoubleNoPreferred() or
 QCBOREncode_AddFloatNoPreferred().

 This implementation of preferred floating-point serialization and
 half-precision does not depend on the CPU having floating-point HW or
 the compiler bringing in a (sometimes large) library to compensate
 for lack of CPU support. This implementation uses shifts and masks
 rather than floating-point functions.

 To reduce overall object code by about 900 bytes, define
 QCBOR_DISABLE_PREFERRED_FLOAT. This will eliminate all support for
 preferred serialization and half-precision. An error will be returned
 when attempting to decode half-precision. A float will always be
 encoded and decoded as 32-bits and a double will always be encoded
 and decoded as 64 bits.

 Note that even if QCBOR_DISABLE_PREFERRED_FLOAT is not defined all
 the float-point encoding object code can be avoided by never calling
 any functions that encode double or float. Just not calling
 floating-point functions will reduce object code by about 500 bytes.

 On CPUs that have no floating-point hardware,
 QCBOR_DISABLE_FLOAT_HW_USE should be defined in most cases. If it is
 not, then the compiler will bring in possibly large software
 libraries to compensate. Defining QCBOR_DISABLE_FLOAT_HW_USE reduces
 object code size on CPUs with floating-point hardware by a tiny
 amount and eliminates the need for <math.h>

 When QCBOR_DISABLE_FLOAT_HW_USE is defined, trying to decoding
 floating-point dates will give error
 @ref QCBOR_ERR_FLOAT_DATE_DISABLED and decoded single-precision
 numbers will be returned as @ref QCBOR_TYPE_FLOAT instead of
 converting them to double as usual.

 If both QCBOR_DISABLE_FLOAT_HW_USE and QCBOR_DISABLE_PREFERRED_FLOAT
 are defined, then the only thing QCBOR can do is encode/decode a C
 float type as 32-bits and a C double type as 64-bits. Floating-point
 epoch dates will be unsupported.

 If USEFULBUF_DISABLE_ALL_FLOATis defined, then floating point support is
 completely disabled. Decoding functions return @ref
 QCBOR_ERR_ALL_FLOAT_DISABLED if a floating point value is encountered during
 decoding. Functions that are encoding floating point values are not available.

 ## Limitations

 Summary Limits of this implementation:
 - The entire encoded CBOR must fit into contiguous memory.
 - Max size of encoded / decoded CBOR data is a few bytes less than @c
 UINT32_MAX (4GB).
 - Max array / map nesting level when encoding / decoding is
   @ref QCBOR_MAX_ARRAY_NESTING (this is typically 15).
 - Max items in an array or map when encoding / decoding is
   @ref QCBOR_MAX_ITEMS_IN_ARRAY (typically 65,536).
 - Does not directly support labels in maps other than text strings & integers.
 - Does not directly support integer labels greater than @c INT64_MAX.
 - Epoch dates limited to @c INT64_MAX (+/- 292 billion years).
 - Exponents for bigfloats and decimal integers are limited to @c INT64_MAX.
 - Tags on labels are ignored during decoding.
 - The maximum tag nesting is @c QCBOR_MAX_TAGS_PER_ITEM (typically 4).
 - Works only on 32- and 64-bit CPUs (modifications could make it work
   on 16-bit CPUs).

 The public interface uses @c size_t for all lengths. Internally the
 implementation uses 32-bit lengths by design to use less memory and
 fit structures on the stack. This limits the encoded CBOR it can work
 with to size @c UINT32_MAX (4GB) which should be enough.

 This implementation assumes two's compliment integer machines. @c
 <stdint.h> also requires this. It is possible to modify this
 implementation for another integer representation, but all modern
 machines seem to be two's compliment.
 */

/**
 The size of the buffer to be passed to QCBOREncode_EncodeHead(). It is one
 byte larger than sizeof(uint64_t) + 1, the actual maximum size of the
 head of a CBOR data item because QCBOREncode_EncodeHead() needs
 one extra byte to work.
 */
#define QCBOR_HEAD_BUFFER_SIZE (sizeof(uint64_t) + 2)

/**
 Output the full CBOR tag. See @ref CBORTags, @ref Tag-Usage and
 @ref Tags-Overview.
 */
#define QCBOR_ENCODE_AS_TAG 0

/**
 Output only the 'borrowed' content format for the relevant tag.
 See @ref CBORTags, @ref Tag-Usage and @ref Tags-Overview.
 */
#define QCBOR_ENCODE_AS_BORROWED 1

/**
 QCBOREncodeContext is the data type that holds context for all the
 encoding functions. It is less than 200 bytes, so it can go on the
 stack. The contents are opaque, and the caller should not access
 internal members.  A context may be re used serially as long as it is
 re initialized.
 */
typedef struct QCBOREncodeContext_s QCBOREncodeContext;

/**
 Initialize the encoder to prepare to encode some CBOR.

 @param[in,out]  pCtx     The encoder context to initialize.
 @param[in]      Storage  The buffer into which the encoded result
			  will be written.

 Call this once at the start of an encoding of some CBOR. Then call
 the many functions like QCBOREncode_AddInt64() and
 QCBOREncode_AddText() to add the different data items. Finally, call
 QCBOREncode_Finish() to get the pointer and length of the encoded
 result.

 The primary purpose of this function is to give the pointer and
 length of the output buffer into which the encoded CBOR will be
 written. This is done with a @ref UsefulBuf structure, which is just
 a pointer and length (it is equivalent to two parameters, one a
 pointer and one a length, but a little prettier).

 The output buffer can be allocated any way (malloc, stack,
 static). It is just some memory that QCBOR writes to. The length must
 be the length of the allocated buffer. QCBOR will never write past
 that length, but might write up to that length. If the buffer is too
 small, encoding will go into an error state and not write anything
 further.

 If allocating on the stack the convenience macro
 UsefulBuf_MAKE_STACK_UB() can be used, but its use is not required.

 Since there is no reallocation or such, the output buffer must be
 correctly sized when passed in here. It is OK, but wasteful if it is
 too large. One way to pick the size is to figure out the maximum size
 that will ever be needed and hard code a buffer of that size.

 Another way to do it is to have QCBOR calculate it for you. To do
 this, pass @ref SizeCalculateUsefulBuf for @c Storage.
 Then call all the functions to add the CBOR exactly as if
 encoding for real. Finally, call QCBOREncode_FinishGetSize().
 Once the length is obtained, allocate a buffer of that
 size, call QCBOREncode_Init() again with the real buffer. Call all
 the add functions again and finally, QCBOREncode_Finish() to obtain
 the final result. This uses twice the CPU time, but that is
 usually not an issue.

 See QCBOREncode_Finish() for how the pointer and length for the
 encoded CBOR is returned.

 For practical purposes QCBOR can't output encoded CBOR larger than
 @c UINT32_MAX (4GB) even on 64-bit CPUs because the internal offsets
 used to track the start of an array/map are 32 bits to reduce the
 size of the encoding context.

 A @ref QCBOREncodeContext can be reused over and over as long as
 QCBOREncode_Init() is called before each use.
 */
void
QCBOREncode_Init(QCBOREncodeContext *pCtx, UsefulBuf Storage);

/**
 @brief  Add a signed 64-bit integer to the encoded output.

 @param[in] pCtx   The encoding context to add the integer to.
 @param[in] nNum   The integer to add.

 The integer will be encoded and added to the CBOR output.

 This function figures out the size and the sign and encodes in the
 correct minimal CBOR. Specifically, it will select CBOR major type 0
 or 1 based on sign and will encode to 1, 2, 4 or 8 bytes depending on
 the value of the integer. Values less than 24 effectively encode to
 one byte because they are encoded in with the CBOR major type.  This
 is a neat and efficient characteristic of CBOR that can be taken
 advantage of when designing CBOR-based protocols. If integers like
 tags can be kept between -23 and 23 they will be encoded in one byte
 including the major type.

 If you pass a smaller int, say an @c int16_t or a small value, say
 100, the encoding will still be CBOR's most compact that can
 represent the value.  For example, CBOR always encodes the value 0 as
 one byte, 0x00. The representation as 0x00 includes identification of
 the type as an integer too as the major type for an integer is 0. See
 [RFC 8949] (https://tools.ietf.org/html/rfc8949) Appendix A for more
 examples of CBOR encoding. This compact encoding is also preferred
 serialization CBOR as per section 34.1 in RFC 8949.

 There are no functions to add @c int16_t or @c int32_t because they
 are not necessary because this always encodes to the smallest number
 of bytes based on the value (If this code is running on a 32-bit
 machine having a way to add 32-bit integers would reduce code size
 some).

 If the encoding context is in an error state, this will do
 nothing. If an error occurs when adding this integer, the internal
 error flag will be set, and the error will be returned when
 QCBOREncode_Finish() is called.

 See also QCBOREncode_AddUInt64().
 */
void
QCBOREncode_AddInt64(QCBOREncodeContext *pCtx, int64_t nNum);

static void
QCBOREncode_AddInt64ToMap(QCBOREncodeContext *pCtx, const char *szLabel,
			  int64_t uNum);

static void
QCBOREncode_AddInt64ToMapN(QCBOREncodeContext *pCtx, int64_t nLabel,
			   int64_t uNum);

/**
 @brief  Add an unsigned 64-bit integer to the encoded output.

 @param[in] pCtx  The encoding context to add the integer to.
 @param[in] uNum  The integer to add.

 The integer will be encoded and added to the CBOR output.

 The only reason so use this function is for integers larger than @c
 INT64_MAX and smaller than @c UINT64_MAX. Otherwise
 QCBOREncode_AddInt64() will work fine.

 Error handling is the same as for QCBOREncode_AddInt64().
 */
void
QCBOREncode_AddUInt64(QCBOREncodeContext *pCtx, uint64_t uNum);

static void
QCBOREncode_AddUInt64ToMap(QCBOREncodeContext *pCtx, const char *szLabel,
			   uint64_t uNum);

static void
QCBOREncode_AddUInt64ToMapN(QCBOREncodeContext *pCtx, int64_t nLabel,
			    uint64_t uNum);

/**
 @brief  Add a UTF-8 text string to the encoded output.

 @param[in] pCtx   The encoding context to add the text to.
 @param[in] Text   Pointer and length of text to add.

 The text passed in must be unencoded UTF-8 according to [RFC 3629]
 (https://tools.ietf.org/html/rfc3629). There is no NULL
 termination. The text is added as CBOR major type 3.

 If called with @c nBytesLen equal to 0, an empty string will be
 added. When @c nBytesLen is 0, @c pBytes may be @c NULL.

 Note that the restriction of the buffer length to a @c uint32_t is
 entirely intentional as this encoder is not capable of encoding
 lengths greater. This limit to 4GB for a text string should not be a
 problem.

 Text lines in Internet protocols (on the wire) are delimited by
 either a CRLF or just an LF. Officially many protocols specify CRLF,
 but implementations often work with either. CBOR type 3 text can be
 either line ending, even a mixture of both.

 Operating systems usually have a line end convention. Windows uses
 CRLF. Linux and MacOS use LF. Some applications on a given OS may
 work with either and some may not.

 The majority of use cases and CBOR protocols using type 3 text will
 work with either line ending. However, some use cases or protocols
 may not work with either in which case translation to and/or from the
 local line end convention, typically that of the OS, is necessary.

 QCBOR does no line ending translation for type 3 text when encoding
 and decoding.

 Error handling is the same as QCBOREncode_AddInt64().
 */
static void
QCBOREncode_AddText(QCBOREncodeContext *pCtx, UsefulBufC Text);

static void
QCBOREncode_AddTextToMap(QCBOREncodeContext *pCtx, const char *szLabel,
			 UsefulBufC Text);

static void
QCBOREncode_AddTextToMapN(QCBOREncodeContext *pCtx, int64_t nLabel,
			  UsefulBufC Text);

/**
 @brief  Add a UTF-8 text string to the encoded output.

 @param[in] pCtx      The encoding context to add the text to.
 @param[in] szString  Null-terminated text to add.

 This works the same as QCBOREncode_AddText().
 */
static void
QCBOREncode_AddSZString(QCBOREncodeContext *pCtx, const char *szString);

static void
QCBOREncode_AddSZStringToMap(QCBOREncodeContext *pCtx, const char *szLabel,
			     const char *szString);

static void
QCBOREncode_AddSZStringToMapN(QCBOREncodeContext *pCtx, int64_t nLabel,
			      const char *szString);

#ifndef USEFULBUF_DISABLE_ALL_FLOAT
/**
 @brief Add a double-precision floating-point number to the encoded output.

 @param[in] pCtx  The encoding context to add the double to.
 @param[in] dNum  The double-precision number to add.

 This encodes and outputs a floating-point number. CBOR major type 7
 is used.

 This implements preferred serialization, selectively encoding the
 double-precision floating-point number as either double-precision,
 single-precision or half-precision. Infinity, NaN and 0 are always
 encoded as half-precision. If no precision will be lost in the
 conversion to half-precision, then it will be converted and
 encoded. If not and no precision will be lost in conversion to
 single-precision, then it will be converted and encoded. If not, then
 no conversion is performed, and it encoded as a double-precision.

 Half-precision floating-point numbers take up 2 bytes, half that of
 single-precision, one quarter of double-precision

 This automatically reduces the size of encoded CBOR, maybe even by
 four if most of values are 0, infinity or NaN.

 When decoded, QCBOR will usually return these values as
 double-precision.

 It is possible to disable this preferred serialization when compiling
 QCBOR. In that case, this functions the same as
 QCBOREncode_AddDoubleNoPreferred().

 Error handling is the same as QCBOREncode_AddInt64().

 See also QCBOREncode_AddDoubleNoPreferred(), QCBOREncode_AddFloat()
 and QCBOREncode_AddFloatNoPreferred() and @ref Floating-Point.
 */
void
QCBOREncode_AddDouble(QCBOREncodeContext *pCtx, double dNum);

static void
QCBOREncode_AddDoubleToMap(QCBOREncodeContext *pCtx, const char *szLabel,
			   double dNum);

static void
QCBOREncode_AddDoubleToMapN(QCBOREncodeContext *pCtx, int64_t nLabel,
			    double dNum);

/**
 @brief Add a single-precision floating-point number to the encoded output.

 @param[in] pCtx  The encoding context to add the double to.
 @param[in] fNum  The single-precision number to add.

 This is identical to QCBOREncode_AddDouble() except the input is
 single-precision.

 See also QCBOREncode_AddDouble(), QCBOREncode_AddDoubleNoPreferred(),
 and QCBOREncode_AddFloatNoPreferred() and @ref Floating-Point.
*/
void
QCBOREncode_AddFloat(QCBOREncodeContext *pCtx, float fNum);

static void
QCBOREncode_AddFloatToMap(QCBOREncodeContext *pCtx, const char *szLabel,
			  float fNum);

static void
QCBOREncode_AddFloatToMapN(QCBOREncodeContext *pCtx, int64_t nLabel,
			   float dNum);

/**
 @brief Add a double-precision floating-point number without preferred encoding.

 @param[in] pCtx  The encoding context to add the double to.
 @param[in] dNum  The double-precision number to add.

 This always outputs the number as a 64-bit double-precision.
 Preferred serialization is not used.

 Error handling is the same as QCBOREncode_AddInt64().

 See also QCBOREncode_AddDouble(), QCBOREncode_AddFloat(), and
 QCBOREncode_AddFloatNoPreferred() and @ref Floating-Point.
*/
void
QCBOREncode_AddDoubleNoPreferred(QCBOREncodeContext *pCtx, double dNum);

static void
QCBOREncode_AddDoubleNoPreferredToMap(QCBOREncodeContext *pCtx,
				      const char *szLabel, double dNum);

static void
QCBOREncode_AddDoubleNoPreferredToMapN(QCBOREncodeContext *pCtx, int64_t nLabel,
				       double dNum);

/**
 @brief Add a single-precision floating-point number without preferred encoding.

 @param[in] pCtx  The encoding context to add the double to.
 @param[in] fNum  The single-precision number to add.

 This always outputs the number as a 32-bit single-precision.
 Preferred serialization is not used.

 Error handling is the same as QCBOREncode_AddInt64().

 See also QCBOREncode_AddDouble(), QCBOREncode_AddFloat(), and
 QCBOREncode_AddDoubleNoPreferred() and @ref Floating-Point.
*/
void
QCBOREncode_AddFloatNoPreferred(QCBOREncodeContext *pCtx, float fNum);

static void
QCBOREncode_AddFloatNoPreferredToMap(QCBOREncodeContext *pCtx,
				     const char *szLabel, float fNum);

static void
QCBOREncode_AddFloatNoPreferredToMapN(QCBOREncodeContext *pCtx, int64_t nLabel,
				      float fNum);
#endif /* USEFULBUF_DISABLE_ALL_FLOAT */

/**
 @brief Add an optional tag.

 @param[in] pCtx  The encoding context to add the tag to.
 @param[in] uTag  The tag to add

 This outputs a CBOR major type 6 item that tags the next data item
 that is output usually to indicate it is some new data type.

 For many of the common standard tags, a function to encode data using
 it is provided and this is not needed. For example,
 QCBOREncode_AddDateEpoch() already exists to output integers
 representing dates with the right tag.

 The tag is applied to the next data item added to the encoded
 output. That data item that is to be tagged can be of any major CBOR
 type. Any number of tags can be added to a data item by calling this
 multiple times before the data item is added.

 See @ref Tags-Overview for discussion of creating new non-standard
 tags. See QCBORDecode_GetNext() for discussion of decoding custom
 tags.
*/
void
QCBOREncode_AddTag(QCBOREncodeContext *pCtx, uint64_t uTag);

/**
 @brief  Add an epoch-based date.

 @param[in] pCtx  The encoding context to add the date to.
 @param[in] uTagRequirement  Either @ref QCBOR_ENCODE_AS_TAG or @ref
 QCBOR_ENCODE_AS_BORROWED.
 @param[in] nDate  Number of seconds since 1970-01-01T00:00Z in UTC time.

 As per RFC 8949 this is similar to UNIX/Linux/POSIX dates. This is
 the most compact way to specify a date and time in CBOR. Note that
 this is always UTC and does not include the time zone.  Use
 QCBOREncode_AddDateString() if you want to include the time zone.

 The preferred integer encoding rules apply here so the date will be encoded in
 a minimal number of bytes. Until about the year 2106 these dates will
 encode in 6 bytes -- one byte for the tag, one byte for the type and
 4 bytes for the integer. After that it will encode to 10 bytes.

 Negative values are supported for dates before 1970.

 If you care about leap-seconds and that level of accuracy, make sure
 the system you are running this code on does it correctly. This code
 just takes the value passed in.

 This implementation cannot encode fractional seconds using float or
 double even though that is allowed by CBOR, but you can encode them
 if you want to by calling QCBOREncode_AddTag() and QCBOREncode_AddDouble().

 Error handling is the same as QCBOREncode_AddInt64().

 See also QCBOREncode_AddTDaysEpoch().
 */
static void
QCBOREncode_AddTDateEpoch(QCBOREncodeContext *pCtx, uint8_t uTagRequirement,
			  int64_t nDate);

static void
QCBOREncode_AddTDateEpochToMapSZ(QCBOREncodeContext *pCtx, const char *szLabel,
				 uint8_t uTagRequirement, int64_t nDate);

static void
QCBOREncode_AddTDateEpochToMapN(QCBOREncodeContext *pCtx, int64_t nLabel,
				uint8_t uTagRequirement, int64_t nDate);

static void
QCBOREncode_AddDateEpoch(QCBOREncodeContext *pCtx, int64_t nDate);

static void
QCBOREncode_AddDateEpochToMap(QCBOREncodeContext *pCtx, const char *szLabel,
			      int64_t nDate);

static void
QCBOREncode_AddDateEpochToMapN(QCBOREncodeContext *pCtx, int64_t nLabel,
			       int64_t nDate);

/**
  @brief  Add an epoch-based day-count date.

  @param[in] pCtx             The encoding context to add the date to.
  @param[in] uTagRequirement  Either @ref QCBOR_ENCODE_AS_TAG or
			      @ref QCBOR_ENCODE_AS_BORROWED.
  @param[in] nDays            Number of days before or after 1970-01-0.

 This date format is described in
 [RFC 8943] (https://tools.ietf.org/html/rfc8943).

 The integer encoding rules apply here so the date will be encoded in
 a minimal number of bytes. Until about the year 2149 these dates will
 encode in 4 bytes -- one byte for the tag, one byte for the type and
 2 bytes for the integer.

 See also QCBOREncode_AddTDateEpoch().

*/
static void
QCBOREncode_AddTDaysEpoch(QCBOREncodeContext *pCtx, uint8_t uTagRequirement,
			  int64_t nDays);

static void
QCBOREncode_AddTDaysEpochToMapSZ(QCBOREncodeContext *pCtx, const char *szLabel,
				 uint8_t uTagRequirement, int64_t nDays);

static void
QCBOREncode_AddTDaysEpochToMapN(QCBOREncodeContext *pCtx, int64_t nLabel,
				uint8_t uTagRequirement, int64_t nDays);

/**
 @brief Add a byte string to the encoded output.

 @param[in] pCtx   The encoding context to add the bytes to.
 @param[in] Bytes  Pointer and length of the input data.

 Simply adds the bytes to the encoded output as CBOR major type 2.

 If called with @c Bytes.len equal to 0, an empty string will be
 added. When @c Bytes.len is 0, @c Bytes.ptr may be @c NULL.

 Error handling is the same as QCBOREncode_AddInt64().
 */
static void
QCBOREncode_AddBytes(QCBOREncodeContext *pCtx, UsefulBufC Bytes);

static void
QCBOREncode_AddBytesToMap(QCBOREncodeContext *pCtx, const char *szLabel,
			  UsefulBufC Bytes);

static void
QCBOREncode_AddBytesToMapN(QCBOREncodeContext *pCtx, int64_t nLabel,
			   UsefulBufC Bytes);

/**
 @brief Set up to write a byte string value directly to encoded output.

 @param[in] pCtx     The encoding context to add the bytes to.
 @param[out] pPlace  Pointer and length of place to write byte string value.

 QCBOREncode_AddBytes() is the normal way to encode a byte string.
 This is for special cases and by passes some of the pointer safety.

 The purpose of this is to output the bytes that make up a byte string
 value directly to the QCBOR output buffer so you don't need to have a
 copy of it in memory. This is particularly useful if the byte string
 is large, for example, the encrypted payload of a COSE_Encrypt
 message. The payload encryption algorithm can output directly to the
 encoded CBOR buffer, perhaps by making it the output buffer
 for some function (e.g. symmetric encryption) or by multiple writes.

 The pointer in \c pPlace is where to start writing. Writing is just
 copying bytes to the location by the pointer in \c pPlace.  Writing
 past the length in \c pPlace will be writing off the end of the
 output buffer.

 If there is no room in the output buffer @ref NULLUsefulBuf will be
 returned and there is no need to call QCBOREncode_CloseBytes().

 The byte string must be closed by calling QCBOREncode_CloseBytes().

 Warning: this bypasses some of the usual checks provided by QCBOR
 against writing off the end of the encoded output buffer.
 */
void
QCBOREncode_OpenBytes(QCBOREncodeContext *pCtx, UsefulBuf *pPlace);

static void
QCBOREncode_OpenBytesInMapSZ(QCBOREncodeContext *pCtx, const char *szLabel,
			     UsefulBuf *pPlace);

static void
QCBOREncode_OpenBytesInMapN(QCBOREncodeContext *pCtx, int64_t nLabel,
			    UsefulBuf *pPlace);

/**
  @brief Close out a byte string written directly to encoded output.

  @param[in] pCtx      The encoding context to add the bytes to.
  @param[out] uAmount  The number of bytes written, the length of the byte
 string.

 This closes out a call to QCBOREncode_OpenBytes().  This inserts a
 CBOR header at the front of the byte string value to make it a
 well-formed byte string.

 If there was no call to QCBOREncode_OpenBytes() then
 @ref QCBOR_ERR_TOO_MANY_CLOSES is set.
 */
void
QCBOREncode_CloseBytes(QCBOREncodeContext *pCtx, size_t uAmount);

/**
 @brief Add a binary UUID to the encoded output.

 @param[in] pCtx   The encoding context to add the UUID to.
 @param[in] uTagRequirement  Either @ref QCBOR_ENCODE_AS_TAG or @ref
 QCBOR_ENCODE_AS_BORROWED.
 @param[in] Bytes  Pointer and length of the binary UUID.

 A binary UUID as defined in [RFC 4122]
 (https://tools.ietf.org/html/rfc4122) is added to the output.

 It is output as CBOR major type 2, a binary string, with tag @ref
 CBOR_TAG_BIN_UUID indicating the binary string is a UUID.
 */
static void
QCBOREncode_AddTBinaryUUID(QCBOREncodeContext *pCtx, uint8_t uTagRequirement,
			   UsefulBufC Bytes);

static void
QCBOREncode_AddTBinaryUUIDToMapSZ(QCBOREncodeContext *pCtx, const char *szLabel,
				  uint8_t uTagRequirement, UsefulBufC Bytes);

static void
QCBOREncode_AddTBinaryUUIDToMapN(QCBOREncodeContext *pCtx, int64_t nLabel,
				 uint8_t uTagRequirement, UsefulBufC Bytes);

static void
QCBOREncode_AddBinaryUUID(QCBOREncodeContext *pCtx, UsefulBufC Bytes);

static void
QCBOREncode_AddBinaryUUIDToMap(QCBOREncodeContext *pCtx, const char *szLabel,
			       UsefulBufC Bytes);

static void
QCBOREncode_AddBinaryUUIDToMapN(QCBOREncodeContext *pCtx, int64_t nLabel,
				UsefulBufC Bytes);

/**
 @brief Add a positive big number to the encoded output.

 @param[in] pCtx   The encoding context to add the big number to.
 @param[in] uTagRequirement  Either @ref QCBOR_ENCODE_AS_TAG or @ref
 QCBOR_ENCODE_AS_BORROWED.
 @param[in] Bytes  Pointer and length of the big number.

 Big numbers are integers larger than 64-bits. Their format is
 described in [RFC 8949] (https://tools.ietf.org/html/rfc8949).

 It is output as CBOR major type 2, a binary string, with tag @ref
 CBOR_TAG_POS_BIGNUM indicating the binary string is a positive big
 number.

 Often big numbers are used to represent cryptographic keys, however,
 COSE which defines representations for keys chose not to use this
 particular type.
 */
static void
QCBOREncode_AddTPositiveBignum(QCBOREncodeContext *pCtx,
			       uint8_t uTagRequirement, UsefulBufC Bytes);

static void
QCBOREncode_AddTPositiveBignumToMapSZ(QCBOREncodeContext *pCtx,
				      const char	 *szLabel,
				      uint8_t		  uTagRequirement,
				      UsefulBufC	  Bytes);

static void
QCBOREncode_AddTPositiveBignumToMapN(QCBOREncodeContext *pCtx, int64_t nLabel,
				     uint8_t uTagRequirement, UsefulBufC Bytes);

static void
QCBOREncode_AddPositiveBignum(QCBOREncodeContext *pCtx, UsefulBufC Bytes);

static void
QCBOREncode_AddPositiveBignumToMap(QCBOREncodeContext *pCtx,
				   const char *szLabel, UsefulBufC Bytes);

static void
QCBOREncode_AddPositiveBignumToMapN(QCBOREncodeContext *pCtx, int64_t nLabel,
				    UsefulBufC Bytes);

/**
 @brief Add a negative big number to the encoded output.

 @param[in] pCtx   The encoding context to add the big number to.
 @param[in] uTagRequirement  Either @ref QCBOR_ENCODE_AS_TAG or @ref
 QCBOR_ENCODE_AS_BORROWED.
 @param[in] Bytes  Pointer and length of the big number.

 Big numbers are integers larger than 64-bits. Their format is
 described in [RFC 8949] (https://tools.ietf.org/html/rfc8949).

 It is output as CBOR major type 2, a binary string, with tag @ref
 CBOR_TAG_NEG_BIGNUM indicating the binary string is a negative big
 number.

 Often big numbers are used to represent cryptographic keys, however,
 COSE which defines representations for keys chose not to use this
 particular type.
 */
static void
QCBOREncode_AddTNegativeBignum(QCBOREncodeContext *pCtx,
			       uint8_t uTagRequirement, UsefulBufC Bytes);

static void
QCBOREncode_AddTNegativeBignumToMapSZ(QCBOREncodeContext *pCtx,
				      const char	 *szLabel,
				      uint8_t		  uTagRequirement,
				      UsefulBufC	  Bytes);

static void
QCBOREncode_AddTNegativeBignumToMapN(QCBOREncodeContext *pCtx, int64_t nLabel,
				     uint8_t uTagRequirement, UsefulBufC Bytes);

static void
QCBOREncode_AddNegativeBignum(QCBOREncodeContext *pCtx, UsefulBufC Bytes);

static void
QCBOREncode_AddNegativeBignumToMap(QCBOREncodeContext *pCtx,
				   const char *szLabel, UsefulBufC Bytes);

static void
QCBOREncode_AddNegativeBignumToMapN(QCBOREncodeContext *pCtx, int64_t nLabel,
				    UsefulBufC Bytes);

#ifndef QCBOR_DISABLE_EXP_AND_MANTISSA
/**
 @brief Add a decimal fraction to the encoded output.

 @param[in] pCtx            The encoding context to add the decimal fraction to.
 @param[in] uTagRequirement  Either @ref QCBOR_ENCODE_AS_TAG or @ref
 QCBOR_ENCODE_AS_BORROWED.
 @param[in] nMantissa       The mantissa.
 @param[in] nBase10Exponent The exponent.

 The value is nMantissa * 10 ^ nBase10Exponent.

 A decimal fraction is good for exact representation of some values
 that can't be represented exactly with standard C (IEEE 754)
 floating-point numbers.  Much larger and much smaller numbers can
 also be represented than floating-point because of the larger number
 of bits in the exponent.

 The decimal fraction is conveyed as two integers, a mantissa and a
 base-10 scaling factor.

 For example, 273.15 is represented by the two integers 27315 and -2.

 The exponent and mantissa have the range from @c INT64_MIN to
 @c INT64_MAX for both encoding and decoding (CBOR allows @c -UINT64_MAX
 to @c UINT64_MAX, but this implementation doesn't support this range to
 reduce code size and interface complexity a little).

 CBOR Preferred encoding of the integers is used, thus they will be encoded
 in the smallest number of bytes possible.

 See also QCBOREncode_AddDecimalFractionBigNum() for a decimal
 fraction with arbitrarily large precision and QCBOREncode_AddBigFloat().

 There is no representation of positive or negative infinity or NaN
 (Not a Number). Use QCBOREncode_AddDouble() to encode them.

 See @ref expAndMantissa for decoded representation.
 */
static void
QCBOREncode_AddTDecimalFraction(QCBOREncodeContext *pCtx,
				uint8_t uTagRequirement, int64_t nMantissa,
				int64_t nBase10Exponent);

static void
QCBOREncode_AddTDecimalFractionToMapSZ(QCBOREncodeContext *pCtx,
				       const char	  *szLabel,
				       uint8_t		   uTagRequirement,
				       int64_t		   nMantissa,
				       int64_t		   nBase10Exponent);

static void
QCBOREncode_AddTDecimalFractionToMapN(QCBOREncodeContext *pCtx, int64_t nLabel,
				      uint8_t uTagRequirement,
				      int64_t nMantissa,
				      int64_t nBase10Exponent);

static void
QCBOREncode_AddDecimalFraction(QCBOREncodeContext *pCtx, int64_t nMantissa,
			       int64_t nBase10Exponent);

static void
QCBOREncode_AddDecimalFractionToMap(QCBOREncodeContext *pCtx,
				    const char *szLabel, int64_t nMantissa,
				    int64_t nBase10Exponent);

static void
QCBOREncode_AddDecimalFractionToMapN(QCBOREncodeContext *pCtx, int64_t nLabel,
				     int64_t nMantissa,
				     int64_t nBase10Exponent);
/**
 @brief Add a decimal fraction with a big number mantissa to the encoded output.

 @param[in] pCtx            The encoding context to add the decimal fraction to.
 @param[in] uTagRequirement  Either @ref QCBOR_ENCODE_AS_TAG or @ref
 QCBOR_ENCODE_AS_BORROWED.
 @param[in] Mantissa        The mantissa.
 @param[in] bIsNegative     false if mantissa is positive, true if negative.
 @param[in] nBase10Exponent The exponent.

 This is the same as QCBOREncode_AddDecimalFraction() except the
 mantissa is a big number (See QCBOREncode_AddPositiveBignum())
 allowing for arbitrarily large precision.

 See @ref expAndMantissa for decoded representation.
 */
static void
QCBOREncode_AddTDecimalFractionBigNum(QCBOREncodeContext *pCtx,
				      uint8_t		  uTagRequirement,
				      UsefulBufC Mantissa, bool bIsNegative,
				      int64_t nBase10Exponent);

static void
QCBOREncode_AddTDecimalFractionBigNumToMapSZ(
	QCBOREncodeContext *pCtx, const char *szLabel, uint8_t uTagRequirement,
	UsefulBufC Mantissa, bool bIsNegative, int64_t nBase10Exponent);

static void
QCBOREncode_AddTDecimalFractionBigNumToMapN(
	QCBOREncodeContext *pCtx, int64_t nLabel, uint8_t uTagRequirement,
	UsefulBufC Mantissa, bool bIsNegative, int64_t nBase10Exponent);

static void
QCBOREncode_AddDecimalFractionBigNum(QCBOREncodeContext *pCtx,
				     UsefulBufC Mantissa, bool bIsNegative,
				     int64_t nBase10Exponent);

static void
QCBOREncode_AddDecimalFractionBigNumToMapSZ(QCBOREncodeContext *pCtx,
					    const char	       *szLabel,
					    UsefulBufC		Mantissa,
					    bool		bIsNegative,
					    int64_t nBase10Exponent);

static void
QCBOREncode_AddDecimalFractionBigNumToMapN(QCBOREncodeContext *pCtx,
					   int64_t nLabel, UsefulBufC Mantissa,
					   bool	   bIsNegative,
					   int64_t nBase10Exponent);

/**
 @brief Add a big floating-point number to the encoded output.

 @param[in] pCtx            The encoding context to add the bigfloat to.
 @param[in] uTagRequirement  Either @ref QCBOR_ENCODE_AS_TAG or @ref
 QCBOR_ENCODE_AS_BORROWED.
 @param[in] nMantissa       The mantissa.
 @param[in] nBase2Exponent  The exponent.

 The value is nMantissa * 2 ^ nBase2Exponent.

 "Bigfloats", as CBOR terms them, are similar to IEEE floating-point
 numbers in having a mantissa and base-2 exponent, but they are not
 supported by hardware or encoded the same. They explicitly use two
 CBOR-encoded integers to convey the mantissa and exponent, each of which
 can be 8, 16, 32 or 64 bits. With both the mantissa and exponent
 64 bits they can express more precision and a larger range than an
 IEEE double floating-point number. See
 QCBOREncode_AddBigFloatBigNum() for even more precision.

 For example, 1.5 would be represented by a mantissa of 3 and an
 exponent of -1.

 The exponent and mantissa have the range from @c INT64_MIN to
 @c INT64_MAX for both encoding and decoding (CBOR allows @c -UINT64_MAX
 to @c UINT64_MAX, but this implementation doesn't support this range to
 reduce code size and interface complexity a little).

 CBOR Preferred encoding of the integers is used, thus they will be encoded
 in the smallest number of bytes possible.

 This can also be used to represent floating-point numbers in
 environments that don't support IEEE 754.

 See @ref expAndMantissa for decoded representation.
 */
static void
QCBOREncode_AddTBigFloat(QCBOREncodeContext *pCtx, uint8_t uTagRequirement,
			 int64_t nMantissa, int64_t nBase2Exponent);

static void
QCBOREncode_AddTBigFloatToMapSZ(QCBOREncodeContext *pCtx, const char *szLabel,
				uint8_t uTagRequirement, int64_t nMantissa,
				int64_t nBase2Exponent);

static void
QCBOREncode_AddTBigFloatToMapN(QCBOREncodeContext *pCtx, int64_t nLabel,
			       uint8_t uTagRequirement, int64_t nMantissa,
			       int64_t nBase2Exponent);

static void
QCBOREncode_AddBigFloat(QCBOREncodeContext *pCtx, int64_t nMantissa,
			int64_t nBase2Exponent);

static void
QCBOREncode_AddBigFloatToMap(QCBOREncodeContext *pCtx, const char *szLabel,
			     int64_t nMantissa, int64_t nBase2Exponent);

static void
QCBOREncode_AddBigFloatToMapN(QCBOREncodeContext *pCtx, int64_t nLabel,
			      int64_t nMantissa, int64_t nBase2Exponent);

/**
 @brief Add a big floating-point number with a big number mantissa to
	the encoded output.

 @param[in] pCtx            The encoding context to add the bigfloat to.
 @param[in] uTagRequirement  Either @ref QCBOR_ENCODE_AS_TAG or @ref
 QCBOR_ENCODE_AS_BORROWED.
 @param[in] Mantissa        The mantissa.
 @param[in] bIsNegative     false if mantissa is positive, true if negative.
 @param[in] nBase2Exponent  The exponent.

 This is the same as QCBOREncode_AddBigFloat() except the mantissa is
 a big number (See QCBOREncode_AddPositiveBignum()) allowing for
 arbitrary precision.

 See @ref expAndMantissa for decoded representation.
 */
static void
QCBOREncode_AddTBigFloatBigNum(QCBOREncodeContext *pCtx,
			       uint8_t uTagRequirement, UsefulBufC Mantissa,
			       bool bIsNegative, int64_t nBase2Exponent);

static void
QCBOREncode_AddTBigFloatBigNumToMapSZ(QCBOREncodeContext *pCtx,
				      const char	 *szLabel,
				      uint8_t		  uTagRequirement,
				      UsefulBufC Mantissa, bool bIsNegative,
				      int64_t nBase2Exponent);

static void
QCBOREncode_AddTBigFloatBigNumToMapN(QCBOREncodeContext *pCtx, int64_t nLabel,
				     uint8_t	uTagRequirement,
				     UsefulBufC Mantissa, bool bIsNegative,
				     int64_t nBase2Exponent);

static void
QCBOREncode_AddBigFloatBigNum(QCBOREncodeContext *pCtx, UsefulBufC Mantissa,
			      bool bIsNegative, int64_t nBase2Exponent);

static void
QCBOREncode_AddBigFloatBigNumToMap(QCBOREncodeContext *pCtx,
				   const char *szLabel, UsefulBufC Mantissa,
				   bool bIsNegative, int64_t nBase2Exponent);

static void
QCBOREncode_AddBigFloatBigNumToMapN(QCBOREncodeContext *pCtx, int64_t nLabel,
				    UsefulBufC Mantissa, bool bIsNegative,
				    int64_t nBase2Exponent);
#endif /* QCBOR_DISABLE_EXP_AND_MANTISSA */

/**
 @brief Add a text URI to the encoded output.

 @param[in] pCtx  The encoding context to add the URI to.
 @param[in] uTagRequirement  Either @ref QCBOR_ENCODE_AS_TAG or @ref
 QCBOR_ENCODE_AS_BORROWED.
 @param[in] URI   Pointer and length of the URI.

 The format of URI must be per [RFC 3986]
 (https://tools.ietf.org/html/rfc3986).

 It is output as CBOR major type 3, a text string, with tag @ref
 CBOR_TAG_URI indicating the text string is a URI.

 A URI in a NULL-terminated string, @c szURI, can be easily added with
 this code:

      QCBOREncode_AddURI(pCtx, UsefulBuf_FromSZ(szURI));
 */
static void
QCBOREncode_AddTURI(QCBOREncodeContext *pCtx, uint8_t uTagRequirement,
		    UsefulBufC URI);

static void
QCBOREncode_AddTURIToMapSZ(QCBOREncodeContext *pCtx, const char *szLabel,
			   uint8_t uTagRequirement, UsefulBufC URI);

static void
QCBOREncode_AddTURIToMapN(QCBOREncodeContext *pCtx, int64_t nLabel,
			  uint8_t uTagRequirement, UsefulBufC URI);

static void
QCBOREncode_AddURI(QCBOREncodeContext *pCtx, UsefulBufC URI);

static void
QCBOREncode_AddURIToMap(QCBOREncodeContext *pCtx, const char *szLabel,
			UsefulBufC URI);

static void
QCBOREncode_AddURIToMapN(QCBOREncodeContext *pCtx, int64_t nLabel,
			 UsefulBufC URI);

/**
 @brief Add Base64-encoded text to encoded output.

 @param[in] pCtx     The encoding context to add the base-64 text to.
 @param[in] uTagRequirement  Either @ref QCBOR_ENCODE_AS_TAG or @ref
 QCBOR_ENCODE_AS_BORROWED.
 @param[in] B64Text  Pointer and length of the base-64 encoded text.

 The text content is Base64 encoded data per [RFC 4648]
 (https://tools.ietf.org/html/rfc4648).

 It is output as CBOR major type 3, a text string, with tag @ref
 CBOR_TAG_B64 indicating the text string is Base64 encoded.
 */
static void
QCBOREncode_AddTB64Text(QCBOREncodeContext *pCtx, uint8_t uTagRequirement,
			UsefulBufC B64Text);

static void
QCBOREncode_AddTB64TextToMapSZ(QCBOREncodeContext *pCtx, const char *szLabel,
			       uint8_t uTagRequirement, UsefulBufC B64Text);

static void
QCBOREncode_AddTB64TextToMapN(QCBOREncodeContext *pCtx, int64_t nLabel,
			      uint8_t uTagRequirement, UsefulBufC B64Text);

static void
QCBOREncode_AddB64Text(QCBOREncodeContext *pCtx, UsefulBufC B64Text);

static void
QCBOREncode_AddB64TextToMap(QCBOREncodeContext *pCtx, const char *szLabel,
			    UsefulBufC B64Text);

static void
QCBOREncode_AddB64TextToMapN(QCBOREncodeContext *pCtx, int64_t nLabel,
			     UsefulBufC B64Text);

/**
 @brief Add base64url encoded data to encoded output.

 @param[in] pCtx     The encoding context to add the base64url to.
 @param[in] uTagRequirement  Either @ref QCBOR_ENCODE_AS_TAG or @ref
 QCBOR_ENCODE_AS_BORROWED.
 @param[in] B64Text  Pointer and length of the base64url encoded text.

 The text content is base64URL encoded text as per [RFC 4648]
 (https://tools.ietf.org/html/rfc4648).

 It is output as CBOR major type 3, a text string, with tag @ref
 CBOR_TAG_B64URL indicating the text string is a Base64url encoded.
 */
static void
QCBOREncode_AddTB64URLText(QCBOREncodeContext *pCtx, uint8_t uTagRequirement,
			   UsefulBufC B64Text);

static void
QCBOREncode_AddTB64URLTextToMapSZ(QCBOREncodeContext *pCtx, const char *szLabel,
				  uint8_t uTagRequirement, UsefulBufC B64Text);

static void
QCBOREncode_AddTB64URLTextToMapN(QCBOREncodeContext *pCtx, int64_t nLabel,
				 uint8_t uTagRequirement, UsefulBufC B64Text);

static void
QCBOREncode_AddB64URLText(QCBOREncodeContext *pCtx, UsefulBufC B64Text);

static void
QCBOREncode_AddB64URLTextToMap(QCBOREncodeContext *pCtx, const char *szLabel,
			       UsefulBufC B64Text);

static void
QCBOREncode_AddB64URLTextToMapN(QCBOREncodeContext *pCtx, int64_t nLabel,
				UsefulBufC B64Text);

/**
 @brief Add Perl Compatible Regular Expression.

 @param[in] pCtx    The encoding context to add the regular expression to.
 @param[in] uTagRequirement  Either @ref QCBOR_ENCODE_AS_TAG or @ref
 QCBOR_ENCODE_AS_BORROWED.
 @param[in] Regex   Pointer and length of the regular expression.

 The text content is Perl Compatible Regular
 Expressions (PCRE) / JavaScript syntax [ECMA262].

 It is output as CBOR major type 3, a text string, with tag @ref
 CBOR_TAG_REGEX indicating the text string is a regular expression.
 */
static void
QCBOREncode_AddTRegex(QCBOREncodeContext *pCtx, uint8_t uTagRequirement,
		      UsefulBufC Regex);

static void
QCBOREncode_AddTRegexToMapSZ(QCBOREncodeContext *pCtx, const char *szLabel,
			     uint8_t uTagRequirement, UsefulBufC Regex);

static void
QCBOREncode_AddTRegexToMapN(QCBOREncodeContext *pCtx, int64_t nLabel,
			    uint8_t uTagRequirement, UsefulBufC Regex);

static void
QCBOREncode_AddRegex(QCBOREncodeContext *pCtx, UsefulBufC Regex);

static void
QCBOREncode_AddRegexToMap(QCBOREncodeContext *pCtx, const char *szLabel,
			  UsefulBufC Regex);

static void
QCBOREncode_AddRegexToMapN(QCBOREncodeContext *pCtx, int64_t nLabel,
			   UsefulBufC Regex);

/**
 @brief MIME encoded data to the encoded output.

 @param[in] pCtx             The encoding context to add the MIME data to.
 @param[in] uTagRequirement  Either @ref QCBOR_ENCODE_AS_TAG or
			     @ref QCBOR_ENCODE_AS_BORROWED.
 @param[in] MIMEData         Pointer and length of the MIME data.

 The text content is in MIME format per [RFC 2045]
 (https://tools.ietf.org/html/rfc2045) including the headers.

 It is output as CBOR major type 2, a binary string, with tag @ref
 CBOR_TAG_BINARY_MIME indicating the string is MIME data.  This
 outputs tag 257, not tag 36, as it can carry any type of MIME binary,
 7-bit, 8-bit, quoted-printable and base64 where tag 36 cannot.

 Previous versions of QCBOR, those before spiffy decode, output tag
 36. Decoding supports both tag 36 and 257.  (if the old behavior with
 tag 36 is needed, copy the inline functions below and change the tag
 number).

 See also QCBORDecode_GetMIMEMessage() and
 @ref QCBOR_TYPE_BINARY_MIME.

 This does no translation of line endings. See QCBOREncode_AddText()
 for a discussion of line endings in CBOR.
 */
static void
QCBOREncode_AddTMIMEData(QCBOREncodeContext *pCtx, uint8_t uTagRequirement,
			 UsefulBufC MIMEData);

static void
QCBOREncode_AddTMIMEDataToMapSZ(QCBOREncodeContext *pCtx, const char *szLabel,
				uint8_t uTagRequirement, UsefulBufC MIMEData);

static void
QCBOREncode_AddTMIMEDataToMapN(QCBOREncodeContext *pCtx, int64_t nLabel,
			       uint8_t uTagRequirement, UsefulBufC MIMEData);

static void
QCBOREncode_AddMIMEData(QCBOREncodeContext *pCtx, UsefulBufC MIMEData);

static void
QCBOREncode_AddMIMEDataToMap(QCBOREncodeContext *pCtx, const char *szLabel,
			     UsefulBufC MIMEData);

static void
QCBOREncode_AddMIMEDataToMapN(QCBOREncodeContext *pCtx, int64_t nLabel,
			      UsefulBufC MIMEData);

/**
 @brief  Add an RFC 3339 date string

 @param[in] pCtx    The encoding context to add the date to.
 @param[in] uTagRequirement  Either @ref QCBOR_ENCODE_AS_TAG or @ref
 QCBOR_ENCODE_AS_BORROWED.
 @param[in] szDate  Null-terminated string with date to add.

 The string szDate should be in the form of [RFC 3339]
 (https://tools.ietf.org/html/rfc3339) as defined by section 3.3 in
 [RFC 4287] (https://tools.ietf.org/html/rfc4287). This is as
 described in section 3.4.1 in [RFC 8949]
 (https://tools.ietf.org/html/rfc8949).

 Note that this function doesn't validate the format of the date string
 at all. If you add an incorrect format date string, the generated
 CBOR will be incorrect and the receiver may not be able to handle it.

 Error handling is the same as QCBOREncode_AddInt64().

 See also QCBOREncode_AddTDayString().
 */
static void
QCBOREncode_AddTDateString(QCBOREncodeContext *pCtx, uint8_t uTagRequirement,
			   const char *szDate);

static void
QCBOREncode_AddTDateStringToMapSZ(QCBOREncodeContext *pCtx, const char *szLabel,
				  uint8_t uTagRequirement, const char *szDate);

static void
QCBOREncode_AddTDateStringToMapN(QCBOREncodeContext *pCtx, int64_t nLabel,
				 uint8_t uTagRequirement, const char *szDate);

static void
QCBOREncode_AddDateString(QCBOREncodeContext *pCtx, const char *szDate);

static void
QCBOREncode_AddDateStringToMap(QCBOREncodeContext *pCtx, const char *szLabel,
			       const char *szDate);

static void
QCBOREncode_AddDateStringToMapN(QCBOREncodeContext *pCtx, int64_t nLabel,
				const char *szDate);

/**
 @brief  Add a date-only string.

 @param[in] pCtx             The encoding context to add the date to.
 @param[in] uTagRequirement  Either @ref QCBOR_ENCODE_AS_TAG or
			     @ref QCBOR_ENCODE_AS_BORROWED.
 @param[in] szDate           Null-terminated string with date to add.

 This date format is described in
 [RFC 8943] (https://tools.ietf.org/html/rfc8943), but that mainly
 references RFC 3339.  The string szDate must be in the forrm
 specified the ABNF for a full-date in
 [RFC 3339] (https://tools.ietf.org/html/rfc3339). Examples of this
 are "1985-04-12" and "1937-01-01".  The time and the time zone are
 never included.

 Note that this function doesn't validate the format of the date
 string at all. If you add an incorrect format date string, the
 generated CBOR will be incorrect and the receiver may not be able to
 handle it.

 Error handling is the same as QCBOREncode_AddInt64().

 See also QCBOREncode_AddTDateString().
 */
static void
QCBOREncode_AddTDaysString(QCBOREncodeContext *pCtx, uint8_t uTagRequirement,
			   const char *szDate);

static void
QCBOREncode_AddTDaysStringToMapSZ(QCBOREncodeContext *pCtx, const char *szLabel,
				  uint8_t uTagRequirement, const char *szDate);

static void
QCBOREncode_AddTDaysStringToMapN(QCBOREncodeContext *pCtx, int64_t nLabel,
				 uint8_t uTagRequirement, const char *szDate);

/**
 @brief  Add a standard Boolean.

 @param[in] pCtx   The encoding context to add the Boolean to.
 @param[in] b      true or false from @c <stdbool.h>.

 Adds a Boolean value as CBOR major type 7.

 Error handling is the same as QCBOREncode_AddInt64().
 */
static void
QCBOREncode_AddBool(QCBOREncodeContext *pCtx, bool b);

static void
QCBOREncode_AddBoolToMap(QCBOREncodeContext *pCtx, const char *szLabel, bool b);

static void
QCBOREncode_AddBoolToMapN(QCBOREncodeContext *pCtx, int64_t nLabel, bool b);

/**
 @brief  Add a NULL to the encoded output.

 @param[in] pCtx  The encoding context to add the NULL to.

 Adds the NULL value as CBOR major type 7.

 This NULL doesn't have any special meaning in CBOR such as a
 terminating value for a string or an empty value.

 Error handling is the same as QCBOREncode_AddInt64().
 */
static void
QCBOREncode_AddNULL(QCBOREncodeContext *pCtx);

static void
QCBOREncode_AddNULLToMap(QCBOREncodeContext *pCtx, const char *szLabel);

static void
QCBOREncode_AddNULLToMapN(QCBOREncodeContext *pCtx, int64_t nLabel);

/**
 @brief  Add an "undef" to the encoded output.

 @param[in] pCtx  The encoding context to add the "undef" to.

 Adds the undef value as CBOR major type 7.

 Note that this value will not translate to JSON.

 This Undef doesn't have any special meaning in CBOR such as a
 terminating value for a string or an empty value.

 Error handling is the same as QCBOREncode_AddInt64().
 */
static void
QCBOREncode_AddUndef(QCBOREncodeContext *pCtx);

static void
QCBOREncode_AddUndefToMap(QCBOREncodeContext *pCtx, const char *szLabel);

static void
QCBOREncode_AddUndefToMapN(QCBOREncodeContext *pCtx, int64_t nLabel);

/**
 @brief  Indicates that the next items added are in an array.

 @param[in] pCtx The encoding context to open the array in.

 Arrays are the basic CBOR aggregate or structure type. Call this
 function to start or open an array. Then call the various @c
 QCBOREncode_AddXxx() functions to add the items that go into the
 array. Then call QCBOREncode_CloseArray() when all items have been
 added. The data items in the array can be of any type and can be of
 mixed types.

 Nesting of arrays and maps is allowed and supported just by calling
 QCBOREncode_OpenArray() again before calling
 QCBOREncode_CloseArray().  While CBOR has no limit on nesting, this
 implementation does in order to keep it smaller and simpler.  The
 limit is @ref QCBOR_MAX_ARRAY_NESTING. This is the max number of
 times this can be called without calling
 QCBOREncode_CloseArray(). QCBOREncode_Finish() will return @ref
 QCBOR_ERR_ARRAY_NESTING_TOO_DEEP when it is called as this function
 just sets an error state and returns no value when this occurs.

 If you try to add more than @ref QCBOR_MAX_ITEMS_IN_ARRAY items to a
 single array or map, @ref QCBOR_ERR_ARRAY_TOO_LONG will be returned
 when QCBOREncode_Finish() is called.

 An array itself must have a label if it is being added to a map.
 Note that array elements do not have labels (but map elements do).

 An array itself may be tagged by calling QCBOREncode_AddTag() before this call.
 */
static void
QCBOREncode_OpenArray(QCBOREncodeContext *pCtx);

static void
QCBOREncode_OpenArrayInMap(QCBOREncodeContext *pCtx, const char *szLabel);

static void
QCBOREncode_OpenArrayInMapN(QCBOREncodeContext *pCtx, int64_t nLabel);

/**
 @brief Close an open array.

 @param[in] pCtx The encoding context to close the array in.

 The closes an array opened by QCBOREncode_OpenArray(). It reduces
 nesting level by one. All arrays (and maps) must be closed before
 calling QCBOREncode_Finish().

 When an error occurs as a result of this call, the encoder records
 the error and enters the error state. The error will be returned when
 QCBOREncode_Finish() is called.

 If this has been called more times than QCBOREncode_OpenArray(), then
 @ref QCBOR_ERR_TOO_MANY_CLOSES will be returned when QCBOREncode_Finish()
 is called.

 If this is called and it is not an array that is currently open, @ref
 QCBOR_ERR_CLOSE_MISMATCH will be returned when QCBOREncode_Finish()
 is called.
 */
static void
QCBOREncode_CloseArray(QCBOREncodeContext *pCtx);

/**
 @brief  Indicates that the next items added are in a map.

 @param[in] pCtx The encoding context to open the map in.

 See QCBOREncode_OpenArray() for more information, particularly error
 handling.

 CBOR maps are an aggregate type where each item in the map consists
 of a label and a value. They are similar to JSON objects.

 The value can be any CBOR type including another map.

 The label can also be any CBOR type, but in practice they are
 typically, integers as this gives the most compact output. They might
 also be text strings which gives readability and translation to JSON.

 Every @c QCBOREncode_AddXxx() call has one version that ends with @c
 InMap for adding items to maps with string labels and one that ends
 with @c InMapN that is for adding with integer labels.

 RFC 8949 uses the term "key" instead of "label".

 If you wish to use map labels that are neither integer labels nor
 text strings, then just call the QCBOREncode_AddXxx() function
 explicitly to add the label. Then call it again to add the value.

 See the [RFC 8949] (https://tools.ietf.org/html/rfc8949) for a lot
 more information on creating maps.
 */
static void
QCBOREncode_OpenMap(QCBOREncodeContext *pCtx);

static void
QCBOREncode_OpenMapInMap(QCBOREncodeContext *pCtx, const char *szLabel);

static void
QCBOREncode_OpenMapInMapN(QCBOREncodeContext *pCtx, int64_t nLabel);

/**
 @brief Close an open map.

 @param[in] pCtx The encoding context to close the map in .

 This closes a map opened by QCBOREncode_OpenMap(). It reduces nesting
 level by one.

 When an error occurs as a result of this call, the encoder records
 the error and enters the error state. The error will be returned when
 QCBOREncode_Finish() is called.

 If this has been called more times than QCBOREncode_OpenMap(),
 then @ref QCBOR_ERR_TOO_MANY_CLOSES will be returned when
 QCBOREncode_Finish() is called.

 If this is called and it is not a map that is currently open, @ref
 QCBOR_ERR_CLOSE_MISMATCH will be returned when QCBOREncode_Finish()
 is called.
 */
static void
QCBOREncode_CloseMap(QCBOREncodeContext *pCtx);

/**
 @brief Indicate start of encoded CBOR to be wrapped in a bstr.

 @param[in] pCtx The encoding context to open the bstr-wrapped CBOR in.

 All added encoded items between this call and a call to
 QCBOREncode_CloseBstrWrap2() will be wrapped in a bstr. They will
 appear in the final output as a byte string.  That byte string will
 contain encoded CBOR. This increases nesting level by one.

 The typical use case is for encoded CBOR that is to be
 cryptographically hashed, as part of a [RFC 8152, COSE]
 (https://tools.ietf.org/html/rfc8152) implementation. The
 wrapping byte string is taken as input by the hash function
 (which is why it is returned by QCBOREncode_CloseBstrWrap2()).
 It is also easy to recover on decoding with standard CBOR
 decoders.

 Using QCBOREncode_BstrWrap() and QCBOREncode_CloseBstrWrap2() avoids
 having to encode the items first in one buffer (e.g., the COSE
 payload) and then add that buffer as a bstr to another encoding
 (e.g. the COSE to-be-signed bytes, the @c Sig_structure) potentially
 halving the memory needed.

 CBOR by nature must be decoded item by item in order from the start.
 By wrapping some CBOR in a byte string, the decoding of that wrapped
 CBOR can be skipped. This is another use of wrapping, perhaps
 because the CBOR is large and deeply nested. Perhaps APIs for
 handling one defined CBOR message that is being embedded in another
 only take input as a byte string. Perhaps the desire is to be able
 to decode the out layer even in the wrapped has errors.
 */
static void
QCBOREncode_BstrWrap(QCBOREncodeContext *pCtx);

static void
QCBOREncode_BstrWrapInMap(QCBOREncodeContext *pCtx, const char *szLabel);

static void
QCBOREncode_BstrWrapInMapN(QCBOREncodeContext *pCtx, int64_t nLabel);

/**
 @brief Close a wrapping bstr.

 @param[in] pCtx              The encoding context to close of bstr wrapping in.
 @param[in] bIncludeCBORHead  Include the encoded CBOR head of the bstr
			      as well as the bytes in @c pWrappedCBOR.
 @param[out] pWrappedCBOR     A @ref UsefulBufC containing wrapped bytes.

 The closes a wrapping bstr opened by QCBOREncode_BstrWrap(). It reduces
 nesting level by one.

 A pointer and length of the enclosed encoded CBOR is returned in @c
 *pWrappedCBOR if it is not @c NULL. The main purpose of this is so
 this data can be hashed (e.g., with SHA-256) as part of a [RFC 8152,
 COSE] (https://tools.ietf.org/html/rfc8152)
 implementation. **WARNING**, this pointer and length should be used
 right away before any other calls to @c QCBOREncode_CloseXxx() as
 they will move data around and the pointer and length will no longer
 be to the correct encoded CBOR.

 When an error occurs as a result of this call, the encoder records
 the error and enters the error state. The error will be returned when
 QCBOREncode_Finish() is called.

 If this has been called more times than QCBOREncode_BstrWrap(), then
 @ref QCBOR_ERR_TOO_MANY_CLOSES will be returned when
 QCBOREncode_Finish() is called.

 If this is called and it is not a wrapping bstr that is currently
 open, @ref QCBOR_ERR_CLOSE_MISMATCH will be returned when
 QCBOREncode_Finish() is called.

 QCBOREncode_CloseBstrWrap() is a deprecated version of this function
 that is equivalent to the call with @c bIncludeCBORHead @c true.
 */
void
QCBOREncode_CloseBstrWrap2(QCBOREncodeContext *pCtx, bool bIncludeCBORHead,
			   UsefulBufC *pWrappedCBOR);

static void
QCBOREncode_CloseBstrWrap(QCBOREncodeContext *pCtx, UsefulBufC *pWrappedCBOR);

/**
 * @brief Cancel byte string wrapping.
 *
 * @param[in] pCtx       The encoding context.
 *
 * This cancels QCBOREncode_BstrWrap() making tghe encoding as if it
 * were never called.
 *
 * WARNING: This does not work on QCBOREncode_BstrWrapInMap()
 * or QCBOREncode_BstrWrapInMapN() and there is no error detection
 * of an attempt at their use.
 *
 * This only works if nothing has been added into the wrapped byte
 * string.  If something has been added, this sets the error
 * @ref QCBOR_ERR_CANNOT_CANCEL.
 */
void
QCBOREncode_CancelBstrWrap(QCBOREncodeContext *pCtx);

/**
 @brief Add some already-encoded CBOR bytes.

 @param[in] pCtx     The encoding context to add the already-encode CBOR to.
 @param[in] Encoded  The already-encoded CBOR to add to the context.

 The encoded CBOR being added must be fully conforming CBOR. It must
 be complete with no arrays or maps that are incomplete. While this
 encoder doesn't ever produce indefinite lengths, it is OK for the
 raw CBOR added here to have indefinite lengths.

 The raw CBOR added here is not checked in anyway. If it is not
 conforming or has open arrays or such, the final encoded CBOR
 will probably be wrong or not what was intended.

 If the encoded CBOR being added here contains multiple items, they
 must be enclosed in a map or array. At the top level the raw
 CBOR must be a single data item.
 */
static void
QCBOREncode_AddEncoded(QCBOREncodeContext *pCtx, UsefulBufC Encoded);

static void
QCBOREncode_AddEncodedToMap(QCBOREncodeContext *pCtx, const char *szLabel,
			    UsefulBufC Encoded);

static void
QCBOREncode_AddEncodedToMapN(QCBOREncodeContext *pCtx, int64_t nLabel,
			     UsefulBufC Encoded);

/**
 @brief Get the encoded result.

 @param[in] pCtx           The context to finish encoding with.
 @param[out] pEncodedCBOR  Structure in which the pointer and length of the
 encoded CBOR is returned.

 @retval QCBOR_SUCCESS                     Encoded CBOR is returned.

 @retval QCBOR_ERR_TOO_MANY_CLOSES         Nesting error

 @retval QCBOR_ERR_CLOSE_MISMATCH          Nesting error

 @retval QCBOR_ERR_ARRAY_OR_MAP_STILL_OPEN Nesting error

 @retval QCBOR_ERR_BUFFER_TOO_LARGE        Encoded output buffer size

 @retval QCBOR_ERR_BUFFER_TOO_SMALL        Encoded output buffer size

 @retval QCBOR_ERR_ARRAY_NESTING_TOO_DEEP  Implementation limit

 @retval QCBOR_ERR_ARRAY_TOO_LONG          Implementation limit

 On success, the pointer and length of the encoded CBOR are returned
 in @c *pEncodedCBOR. The pointer is the same pointer that was passed
 in to QCBOREncode_Init(). Note that it is not const when passed to
 QCBOREncode_Init(), but it is const when returned here.  The length
 will be smaller than or equal to the length passed in when
 QCBOREncode_Init() as this is the length of the actual result, not
 the size of the buffer it was written to.

 If a @c NULL was passed for @c Storage.ptr when QCBOREncode_Init()
 was called, @c NULL will be returned here, but the length will be
 that of the CBOR that would have been encoded.

 Encoding errors primarily manifest here as most other encoding function
 do no return an error. They just set the error state in the encode
 context after which no encoding function does anything.

 Three types of errors manifest here. The first type are nesting
 errors where the number of @c QCBOREncode_OpenXxx() calls do not
 match the number @c QCBOREncode_CloseXxx() calls. The solution is to
 fix the calling code.

 The second type of error is because the buffer given is either too
 small or too large. The remedy is to give a correctly sized buffer.

 The third type are due to limits in this implementation.  @ref
 QCBOR_ERR_ARRAY_NESTING_TOO_DEEP can be worked around by encoding the
 CBOR in two (or more) phases and adding the CBOR from the first phase
 to the second with @c QCBOREncode_AddEncoded().

 If an error is returned, the buffer may have partially encoded
 incorrect CBOR in it and it should not be used. Likewise, the length
 may be incorrect and should not be used.

 Note that the error could have occurred in one of the many @c
 QCBOREncode_AddXxx() calls long before QCBOREncode_Finish() was
 called. This error handling reduces the CBOR implementation size but
 makes debugging harder.

 This may be called multiple times. It will always return the same. It
 can also be interleaved with calls to QCBOREncode_FinishGetSize().

 QCBOREncode_GetErrorState() can be called to get the current
 error state in order to abort encoding early as an optimization, but
 calling it is is never required.
 */
QCBORError
QCBOREncode_Finish(QCBOREncodeContext *pCtx, UsefulBufC *pEncodedCBOR);

/**
 @brief Get the encoded CBOR and error status.

 @param[in] pCtx          The context to finish encoding with.
 @param[out] uEncodedLen  The length of the encoded or potentially
			  encoded CBOR in bytes.

 @return The same errors as QCBOREncode_Finish().

 This functions the same as QCBOREncode_Finish(), but only returns the
 size of the encoded output.
 */
QCBORError
QCBOREncode_FinishGetSize(QCBOREncodeContext *pCtx, size_t *uEncodedLen);

/**
 @brief Indicate whether output buffer is NULL or not.

 @param[in] pCtx  The encoding context.

 @return 1 if the output buffer is @c NULL.

 Sometimes a @c NULL input buffer is given to QCBOREncode_Init() so
 that the size of the generated CBOR can be calculated without
 allocating a buffer for it. This returns 1 when the output buffer is
 NULL and 0 when it is not.
*/
static int
QCBOREncode_IsBufferNULL(QCBOREncodeContext *pCtx);

/**
 @brief Get the encoding error state.

 @param[in] pCtx  The encoding context.

 @return One of @ref QCBORError. See return values from
	 QCBOREncode_Finish()

 Normally encoding errors need only be handled at the end of encoding
 when QCBOREncode_Finish() is called. This can be called to get the
 error result before finish should there be a need to halt encoding
 before QCBOREncode_Finish() is called.
*/
static QCBORError
QCBOREncode_GetErrorState(QCBOREncodeContext *pCtx);

/**
 Encode the "head" of a CBOR data item.

 @param buffer       Buffer to output the encoded head to; must be
		     @ref QCBOR_HEAD_BUFFER_SIZE bytes in size.
 @param uMajorType   One of CBOR_MAJOR_TYPE_XX.
 @param uMinLen      The minimum number of bytes to encode uNumber. Almost
 always this is 0 to use preferred minimal encoding. If this is 4, then even the
 values 0xffff and smaller will be encoded in 4 bytes. This is used primarily
 when encoding a float or double put into uNumber as the leading zero bytes for
 them must be encoded.
 @param uNumber      The numeric argument part of the CBOR head.
 @return             Pointer and length of the encoded head or
		     @ref NULLUsefulBufC if the output buffer is too small.

 Callers do not to need to call this for normal CBOR encoding. Note that it
 doesn't even take a @ref QCBOREncodeContext argument.

 This encodes the major type and argument part of a data item. The
 argument is an integer that is usually either the value or the length
 of the data item.

 This is exposed in the public interface to allow hashing of some CBOR
 data types, bstr in particular, a chunk at a time so the full CBOR
 doesn't have to be encoded in a contiguous buffer.

 For example, if you have a 100,000 byte binary blob in a buffer that
 needs to be a bstr encoded and then hashed. You could allocate a
 100,010 byte buffer and encode it normally. Alternatively, you can
 encode the head in a 10 byte buffer with this function, hash that and
 then hash the 100,000 bytes using the same hash context.

 See also QCBOREncode_AddBytesLenOnly();
 */
UsefulBufC
QCBOREncode_EncodeHead(UsefulBuf buffer, uint8_t uMajorType, uint8_t uMinLen,
		       uint64_t uNumber);

/* =========================================================================
     BEGINNING OF PRIVATE INLINE IMPLEMENTATION
   ========================================================================= */

/**
 @brief Semi-private method to add a buffer full of bytes to encoded output

 @param[in] pCtx       The encoding context to add the integer to.
 @param[in] uMajorType The CBOR major type of the bytes.
 @param[in] Bytes      The bytes to add.

 Use QCBOREncode_AddText() or QCBOREncode_AddBytes() or
 QCBOREncode_AddEncoded() instead. They are inline functions that call
 this and supply the correct major type. This function is public to
 make the inline functions work to keep the overall code size down and
 because the C language has no way to make it private.

 If this is called the major type should be @c
 CBOR_MAJOR_TYPE_TEXT_STRING, @c CBOR_MAJOR_TYPE_BYTE_STRING or @c
 CBOR_MAJOR_NONE_TYPE_RAW. The last one is special for adding
 already-encoded CBOR.
 */
void
QCBOREncode_AddBuffer(QCBOREncodeContext *pCtx, uint8_t uMajorType,
		      UsefulBufC Bytes);

/**
 @brief Semi-private method to open a map, array or bstr-wrapped CBOR

 @param[in] pCtx        The context to add to.
 @param[in] uMajorType  The major CBOR type to close

 Call QCBOREncode_OpenArray(), QCBOREncode_OpenMap() or
 QCBOREncode_BstrWrap() instead of this.
 */
void
QCBOREncode_OpenMapOrArray(QCBOREncodeContext *pCtx, uint8_t uMajorType);

/**
 @brief Semi-private method to open a map, array with indefinite length

 @param[in] pCtx        The context to add to.
 @param[in] uMajorType  The major CBOR type to close

 Call QCBOREncode_OpenArrayIndefiniteLength() or
 QCBOREncode_OpenMapIndefiniteLength() instead of this.
 */
void
QCBOREncode_OpenMapOrArrayIndefiniteLength(QCBOREncodeContext *pCtx,
					   uint8_t	       uMajorType);

/**
 @brief Semi-private method to close a map, array or bstr wrapped CBOR

 @param[in] pCtx           The context to add to.
 @param[in] uMajorType     The major CBOR type to close.

 Call QCBOREncode_CloseArray() or QCBOREncode_CloseMap() instead of this.
 */
void
QCBOREncode_CloseMapOrArray(QCBOREncodeContext *pCtx, uint8_t uMajorType);

/**
 @brief Semi-private method to close a map, array with indefinite length

 @param[in] pCtx           The context to add to.
 @param[in] uMajorType     The major CBOR type to close.

 Call QCBOREncode_CloseArrayIndefiniteLength() or
 QCBOREncode_CloseMapIndefiniteLength() instead of this.
 */
void
QCBOREncode_CloseMapOrArrayIndefiniteLength(QCBOREncodeContext *pCtx,
					    uint8_t		uMajorType);

/**
 @brief  Semi-private method to add simple types.

 @param[in] pCtx     The encoding context to add the simple value to.
 @param[in] uMinLen  Minimum encoding size for uNum. Usually 0.
 @param[in] uNum     One of CBOR_SIMPLEV_FALSE through _UNDEF or other.

 This is used to add simple types like true and false.

 Call QCBOREncode_AddBool(), QCBOREncode_AddNULL(),
 QCBOREncode_AddUndef() instead of this.

 This function can add simple values that are not defined by CBOR
 yet. This expansion point in CBOR should not be used unless they are
 standardized.

 Error handling is the same as QCBOREncode_AddInt64().
 */
void
QCBOREncode_AddType7(QCBOREncodeContext *pCtx, uint8_t uMinLen, uint64_t uNum);

/**
 @brief  Semi-private method to add bigfloats and decimal fractions.

 @param[in] pCtx               The encoding context to add the value to.
 @param[in] uTag               The type 6 tag indicating what this is to be.
 @param[in] BigNumMantissa     Is @ref NULLUsefulBufC if mantissa is an
			       @c int64_t or the actual big number mantissa
			       if not.
 @param[in] bBigNumIsNegative  This is @c true if the big number is negative.
 @param[in] nMantissa          The @c int64_t mantissa if it is not a big
 number.
 @param[in] nExponent          The exponent.

 This outputs either the @ref CBOR_TAG_DECIMAL_FRACTION or @ref
 CBOR_TAG_BIGFLOAT tag. if @c uTag is @ref CBOR_TAG_INVALID64, then
 this outputs the "borrowed" content format.

 The tag content output by this is an array with two members, the
 exponent and then the mantissa. The mantissa can be either a big
 number or an @c int64_t.

 This implementation cannot output an exponent further from 0 than
 @c INT64_MAX.

 To output a mantissa that is between INT64_MAX and UINT64_MAX from 0,
 it must be as a big number.

 Typically, QCBOREncode_AddDecimalFraction(), QCBOREncode_AddBigFloat(),
 QCBOREncode_AddDecimalFractionBigNum() or QCBOREncode_AddBigFloatBigNum()
 is called instead of this.
 */
void
QCBOREncode_AddExponentAndMantissa(QCBOREncodeContext *pCtx, uint64_t uTag,
				   UsefulBufC BigNumMantissa,
				   bool bBigNumIsNegative, int64_t nMantissa,
				   int64_t nExponent);

/**
 @brief Semi-private method to add only the type and length of a byte string.

 @param[in] pCtx    The context to initialize.
 @param[in] Bytes   Pointer and length of the input data.

 This is the same as QCBOREncode_AddBytes() except it only adds the
 CBOR encoding for the type and the length. It doesn't actually add
 the bytes. You can't actually produce correct CBOR with this and the
 rest of this API. It is only used for a special case where
 the valid CBOR is created manually by putting this type and length in
 and then adding the actual bytes. In particular, when only a hash of
 the encoded CBOR is needed, where the type and header are hashed
 separately and then the bytes is hashed. This makes it possible to
 implement COSE Sign1 with only one copy of the payload in the output
 buffer, rather than two, roughly cutting memory use in half.

 This is only used for this odd case, but this is a supported
 tested function.

 See also QCBOREncode_EncodeHead().
*/
static inline void
QCBOREncode_AddBytesLenOnly(QCBOREncodeContext *pCtx, UsefulBufC Bytes);

static inline void
QCBOREncode_AddBytesLenOnlyToMap(QCBOREncodeContext *pCtx, const char *szLabel,
				 UsefulBufC Bytes);

static inline void
QCBOREncode_AddBytesLenOnlyToMapN(QCBOREncodeContext *pCtx, int64_t nLabel,
				  UsefulBufC Bytes);

static inline void
QCBOREncode_AddInt64ToMap(QCBOREncodeContext *pMe, const char *szLabel,
			  int64_t uNum)
{
	// Use _AddBuffer() because _AddSZString() is defined below, not above
	QCBOREncode_AddBuffer(pMe, CBOR_MAJOR_TYPE_TEXT_STRING,
			      UsefulBuf_FromSZ(szLabel));
	QCBOREncode_AddInt64(pMe, uNum);
}

static inline void
QCBOREncode_AddInt64ToMapN(QCBOREncodeContext *pMe, int64_t nLabel,
			   int64_t uNum)
{
	QCBOREncode_AddInt64(pMe, nLabel);
	QCBOREncode_AddInt64(pMe, uNum);
}

static inline void
QCBOREncode_AddUInt64ToMap(QCBOREncodeContext *pMe, const char *szLabel,
			   uint64_t uNum)
{
	// Use _AddBuffer() because _AddSZString() is defined below, not above
	QCBOREncode_AddBuffer(pMe, CBOR_MAJOR_TYPE_TEXT_STRING,
			      UsefulBuf_FromSZ(szLabel));
	QCBOREncode_AddUInt64(pMe, uNum);
}

static inline void
QCBOREncode_AddUInt64ToMapN(QCBOREncodeContext *pMe, int64_t nLabel,
			    uint64_t uNum)
{
	QCBOREncode_AddInt64(pMe, nLabel);
	QCBOREncode_AddUInt64(pMe, uNum);
}

static inline void
QCBOREncode_AddText(QCBOREncodeContext *pMe, UsefulBufC Text)
{
	QCBOREncode_AddBuffer(pMe, CBOR_MAJOR_TYPE_TEXT_STRING, Text);
}

static inline void
QCBOREncode_AddTextToMap(QCBOREncodeContext *pMe, const char *szLabel,
			 UsefulBufC Text)
{
	QCBOREncode_AddText(pMe, UsefulBuf_FromSZ(szLabel));
	QCBOREncode_AddText(pMe, Text);
}

static inline void
QCBOREncode_AddTextToMapN(QCBOREncodeContext *pMe, int64_t nLabel,
			  UsefulBufC Text)
{
	QCBOREncode_AddInt64(pMe, nLabel);
	QCBOREncode_AddText(pMe, Text);
}

inline static void
QCBOREncode_AddSZString(QCBOREncodeContext *pMe, const char *szString)
{
	QCBOREncode_AddText(pMe, UsefulBuf_FromSZ(szString));
}

static inline void
QCBOREncode_AddSZStringToMap(QCBOREncodeContext *pMe, const char *szLabel,
			     const char *szString)
{
	QCBOREncode_AddSZString(pMe, szLabel);
	QCBOREncode_AddSZString(pMe, szString);
}

static inline void
QCBOREncode_AddSZStringToMapN(QCBOREncodeContext *pMe, int64_t nLabel,
			      const char *szString)
{
	QCBOREncode_AddInt64(pMe, nLabel);
	QCBOREncode_AddSZString(pMe, szString);
}

#ifndef USEFULBUF_DISABLE_ALL_FLOAT
static inline void
QCBOREncode_AddDoubleToMap(QCBOREncodeContext *pMe, const char *szLabel,
			   double dNum)
{
	QCBOREncode_AddSZString(pMe, szLabel);
	QCBOREncode_AddDouble(pMe, dNum);
}

static inline void
QCBOREncode_AddDoubleToMapN(QCBOREncodeContext *pMe, int64_t nLabel,
			    double dNum)
{
	QCBOREncode_AddInt64(pMe, nLabel);
	QCBOREncode_AddDouble(pMe, dNum);
}

static inline void
QCBOREncode_AddFloatToMap(QCBOREncodeContext *pMe, const char *szLabel,
			  float dNum)
{
	QCBOREncode_AddSZString(pMe, szLabel);
	QCBOREncode_AddFloat(pMe, dNum);
}

static inline void
QCBOREncode_AddFloatToMapN(QCBOREncodeContext *pMe, int64_t nLabel, float fNum)
{
	QCBOREncode_AddInt64(pMe, nLabel);
	QCBOREncode_AddFloat(pMe, fNum);
}

static inline void
QCBOREncode_AddDoubleNoPreferredToMap(QCBOREncodeContext *pMe,
				      const char *szLabel, double dNum)
{
	QCBOREncode_AddSZString(pMe, szLabel);
	QCBOREncode_AddDoubleNoPreferred(pMe, dNum);
}

static inline void
QCBOREncode_AddDoubleNoPreferredToMapN(QCBOREncodeContext *pMe, int64_t nLabel,
				       double dNum)
{
	QCBOREncode_AddInt64(pMe, nLabel);
	QCBOREncode_AddDoubleNoPreferred(pMe, dNum);
}

static inline void
QCBOREncode_AddFloatNoPreferredToMap(QCBOREncodeContext *pMe,
				     const char *szLabel, float dNum)
{
	QCBOREncode_AddSZString(pMe, szLabel);
	QCBOREncode_AddFloatNoPreferred(pMe, dNum);
}

static inline void
QCBOREncode_AddFloatNoPreferredToMapN(QCBOREncodeContext *pMe, int64_t nLabel,
				      float dNum)
{
	QCBOREncode_AddInt64(pMe, nLabel);
	QCBOREncode_AddFloatNoPreferred(pMe, dNum);
}
#endif /* USEFULBUF_DISABLE_ALL_FLOAT */

static inline void
QCBOREncode_AddTDateEpoch(QCBOREncodeContext *pMe, uint8_t uTag, int64_t nDate)
{
	if (uTag == QCBOR_ENCODE_AS_TAG) {
		QCBOREncode_AddTag(pMe, CBOR_TAG_DATE_EPOCH);
	}
	QCBOREncode_AddInt64(pMe, nDate);
}

static inline void
QCBOREncode_AddTDateEpochToMapSZ(QCBOREncodeContext *pMe, const char *szLabel,
				 uint8_t uTag, int64_t nDate)
{
	QCBOREncode_AddSZString(pMe, szLabel);
	QCBOREncode_AddTDateEpoch(pMe, uTag, nDate);
}

static inline void
QCBOREncode_AddTDateEpochToMapN(QCBOREncodeContext *pMe, int64_t nLabel,
				uint8_t uTag, int64_t nDate)
{
	QCBOREncode_AddInt64(pMe, nLabel);
	QCBOREncode_AddTDateEpoch(pMe, uTag, nDate);
}

static inline void
QCBOREncode_AddDateEpoch(QCBOREncodeContext *pMe, int64_t nDate)
{
	QCBOREncode_AddTDateEpoch(pMe, QCBOR_ENCODE_AS_TAG, nDate);
}

static inline void
QCBOREncode_AddDateEpochToMap(QCBOREncodeContext *pMe, const char *szLabel,
			      int64_t nDate)
{
	QCBOREncode_AddSZString(pMe, szLabel);
	QCBOREncode_AddDateEpoch(pMe, nDate);
}

static inline void
QCBOREncode_AddDateEpochToMapN(QCBOREncodeContext *pMe, int64_t nLabel,
			       int64_t nDate)
{
	QCBOREncode_AddInt64(pMe, nLabel);
	QCBOREncode_AddDateEpoch(pMe, nDate);
}

static inline void
QCBOREncode_AddTDaysEpoch(QCBOREncodeContext *pMe, uint8_t uTag, int64_t nDays)
{
	if (uTag == QCBOR_ENCODE_AS_TAG) {
		QCBOREncode_AddTag(pMe, CBOR_TAG_DAYS_EPOCH);
	}
	QCBOREncode_AddInt64(pMe, nDays);
}

static inline void
QCBOREncode_AddTDaysEpochToMapSZ(QCBOREncodeContext *pMe, const char *szLabel,
				 uint8_t uTag, int64_t nDays)
{
	QCBOREncode_AddSZString(pMe, szLabel);
	QCBOREncode_AddTDaysEpoch(pMe, uTag, nDays);
}

static inline void
QCBOREncode_AddTDaysEpochToMapN(QCBOREncodeContext *pMe, int64_t nLabel,
				uint8_t uTag, int64_t nDays)
{
	QCBOREncode_AddInt64(pMe, nLabel);
	QCBOREncode_AddTDaysEpoch(pMe, uTag, nDays);
}

static inline void
QCBOREncode_AddBytes(QCBOREncodeContext *pMe, UsefulBufC Bytes)
{
	QCBOREncode_AddBuffer(pMe, CBOR_MAJOR_TYPE_BYTE_STRING, Bytes);
}

static inline void
QCBOREncode_AddBytesToMap(QCBOREncodeContext *pMe, const char *szLabel,
			  UsefulBufC Bytes)
{
	QCBOREncode_AddSZString(pMe, szLabel);
	QCBOREncode_AddBytes(pMe, Bytes);
}

static inline void
QCBOREncode_AddBytesToMapN(QCBOREncodeContext *pMe, int64_t nLabel,
			   UsefulBufC Bytes)
{
	QCBOREncode_AddInt64(pMe, nLabel);
	QCBOREncode_AddBytes(pMe, Bytes);
}

static inline void
QCBOREncode_OpenBytesInMapSZ(QCBOREncodeContext *pMe, const char *szLabel,
			     UsefulBuf *pPlace)
{
	QCBOREncode_AddSZString(pMe, szLabel);
	QCBOREncode_OpenBytes(pMe, pPlace);
}

static inline void
QCBOREncode_OpenBytesInMapN(QCBOREncodeContext *pMe, int64_t nLabel,
			    UsefulBuf *pPlace)
{
	QCBOREncode_AddInt64(pMe, nLabel);
	QCBOREncode_OpenBytes(pMe, pPlace);
}

static inline void
QCBOREncode_AddBytesLenOnly(QCBOREncodeContext *pMe, UsefulBufC Bytes)
{
	QCBOREncode_AddBuffer(pMe, CBOR_MAJOR_NONE_TYPE_BSTR_LEN_ONLY, Bytes);
}

static inline void
QCBOREncode_AddBytesLenOnlyToMap(QCBOREncodeContext *pMe, const char *szLabel,
				 UsefulBufC Bytes)
{
	QCBOREncode_AddSZString(pMe, szLabel);
	QCBOREncode_AddBytesLenOnly(pMe, Bytes);
}

static inline void
QCBOREncode_AddBytesLenOnlyToMapN(QCBOREncodeContext *pMe, int64_t nLabel,
				  UsefulBufC Bytes)
{
	QCBOREncode_AddInt64(pMe, nLabel);
	QCBOREncode_AddBytesLenOnly(pMe, Bytes);
}

static inline void
QCBOREncode_AddTBinaryUUID(QCBOREncodeContext *pMe, uint8_t uTagRequirement,
			   UsefulBufC Bytes)
{
	if (uTagRequirement == QCBOR_ENCODE_AS_TAG) {
		QCBOREncode_AddTag(pMe, CBOR_TAG_BIN_UUID);
	}
	QCBOREncode_AddBytes(pMe, Bytes);
}

static inline void
QCBOREncode_AddTBinaryUUIDToMapSZ(QCBOREncodeContext *pMe, const char *szLabel,
				  uint8_t uTagRequirement, UsefulBufC Bytes)
{
	QCBOREncode_AddSZString(pMe, szLabel);
	QCBOREncode_AddTBinaryUUID(pMe, uTagRequirement, Bytes);
}

static inline void
QCBOREncode_AddTBinaryUUIDToMapN(QCBOREncodeContext *pMe, int64_t nLabel,
				 uint8_t uTagRequirement, UsefulBufC Bytes)
{
	QCBOREncode_AddInt64(pMe, nLabel);
	QCBOREncode_AddTBinaryUUID(pMe, uTagRequirement, Bytes);
}

static inline void
QCBOREncode_AddBinaryUUID(QCBOREncodeContext *pMe, UsefulBufC Bytes)
{
	QCBOREncode_AddTBinaryUUID(pMe, QCBOR_ENCODE_AS_TAG, Bytes);
}

static inline void
QCBOREncode_AddBinaryUUIDToMap(QCBOREncodeContext *pMe, const char *szLabel,
			       UsefulBufC Bytes)
{
	QCBOREncode_AddTBinaryUUIDToMapSZ(pMe, szLabel, QCBOR_ENCODE_AS_TAG,
					  Bytes);
}

static inline void
QCBOREncode_AddBinaryUUIDToMapN(QCBOREncodeContext *pMe, int64_t nLabel,
				UsefulBufC Bytes)
{
	QCBOREncode_AddTBinaryUUIDToMapN(pMe, nLabel, QCBOR_ENCODE_AS_TAG,
					 Bytes);
}

static inline void
QCBOREncode_AddTPositiveBignum(QCBOREncodeContext *pMe, uint8_t uTagRequirement,
			       UsefulBufC Bytes)
{
	if (uTagRequirement == QCBOR_ENCODE_AS_TAG) {
		QCBOREncode_AddTag(pMe, CBOR_TAG_POS_BIGNUM);
	}
	QCBOREncode_AddBytes(pMe, Bytes);
}

static inline void
QCBOREncode_AddTPositiveBignumToMapSZ(QCBOREncodeContext *pMe,
				      const char	 *szLabel,
				      uint8_t uTagRequirement, UsefulBufC Bytes)
{
	QCBOREncode_AddSZString(pMe, szLabel);
	QCBOREncode_AddTPositiveBignum(pMe, uTagRequirement, Bytes);
}

static inline void
QCBOREncode_AddTPositiveBignumToMapN(QCBOREncodeContext *pMe, int64_t nLabel,
				     uint8_t uTagRequirement, UsefulBufC Bytes)
{
	QCBOREncode_AddInt64(pMe, nLabel);
	QCBOREncode_AddTPositiveBignum(pMe, uTagRequirement, Bytes);
}

static inline void
QCBOREncode_AddPositiveBignum(QCBOREncodeContext *pMe, UsefulBufC Bytes)
{
	QCBOREncode_AddTPositiveBignum(pMe, QCBOR_ENCODE_AS_TAG, Bytes);
}

static inline void
QCBOREncode_AddPositiveBignumToMap(QCBOREncodeContext *pMe, const char *szLabel,
				   UsefulBufC Bytes)
{
	QCBOREncode_AddTPositiveBignumToMapSZ(pMe, szLabel, QCBOR_ENCODE_AS_TAG,
					      Bytes);
}

static inline void
QCBOREncode_AddPositiveBignumToMapN(QCBOREncodeContext *pMe, int64_t nLabel,
				    UsefulBufC Bytes)
{
	QCBOREncode_AddTPositiveBignumToMapN(pMe, nLabel, QCBOR_ENCODE_AS_TAG,
					     Bytes);
}

static inline void
QCBOREncode_AddTNegativeBignum(QCBOREncodeContext *pMe, uint8_t uTagRequirement,
			       UsefulBufC Bytes)
{
	if (uTagRequirement == QCBOR_ENCODE_AS_TAG) {
		QCBOREncode_AddTag(pMe, CBOR_TAG_NEG_BIGNUM);
	}
	QCBOREncode_AddBytes(pMe, Bytes);
}

static inline void
QCBOREncode_AddTNegativeBignumToMapSZ(QCBOREncodeContext *pMe,
				      const char	 *szLabel,
				      uint8_t uTagRequirement, UsefulBufC Bytes)
{
	QCBOREncode_AddSZString(pMe, szLabel);
	QCBOREncode_AddTNegativeBignum(pMe, uTagRequirement, Bytes);
}

static inline void
QCBOREncode_AddTNegativeBignumToMapN(QCBOREncodeContext *pMe, int64_t nLabel,
				     uint8_t uTagRequirement, UsefulBufC Bytes)
{
	QCBOREncode_AddInt64(pMe, nLabel);
	QCBOREncode_AddTNegativeBignum(pMe, uTagRequirement, Bytes);
}

static inline void
QCBOREncode_AddNegativeBignum(QCBOREncodeContext *pMe, UsefulBufC Bytes)
{
	QCBOREncode_AddTNegativeBignum(pMe, QCBOR_ENCODE_AS_TAG, Bytes);
}

static inline void
QCBOREncode_AddNegativeBignumToMap(QCBOREncodeContext *pMe, const char *szLabel,
				   UsefulBufC Bytes)
{
	QCBOREncode_AddTNegativeBignumToMapSZ(pMe, szLabel, QCBOR_ENCODE_AS_TAG,
					      Bytes);
}

static inline void
QCBOREncode_AddNegativeBignumToMapN(QCBOREncodeContext *pMe, int64_t nLabel,
				    UsefulBufC Bytes)
{
	QCBOREncode_AddTNegativeBignumToMapN(pMe, nLabel, QCBOR_ENCODE_AS_TAG,
					     Bytes);
}

#ifndef QCBOR_DISABLE_EXP_AND_MANTISSA

static inline void
QCBOREncode_AddTDecimalFraction(QCBOREncodeContext *pMe,
				uint8_t uTagRequirement, int64_t nMantissa,
				int64_t nBase10Exponent)
{
	uint64_t uTag;
	if (uTagRequirement == QCBOR_ENCODE_AS_TAG) {
		uTag = CBOR_TAG_DECIMAL_FRACTION;
	} else {
		uTag = CBOR_TAG_INVALID64;
	}
	QCBOREncode_AddExponentAndMantissa(pMe, uTag, NULLUsefulBufC, false,
					   nMantissa, nBase10Exponent);
}

static inline void
QCBOREncode_AddTDecimalFractionToMapSZ(QCBOREncodeContext *pMe,
				       const char	  *szLabel,
				       uint8_t		   uTagRequirement,
				       int64_t		   nMantissa,
				       int64_t		   nBase10Exponent)
{
	QCBOREncode_AddSZString(pMe, szLabel);
	QCBOREncode_AddTDecimalFraction(pMe, uTagRequirement, nMantissa,
					nBase10Exponent);
}

static inline void
QCBOREncode_AddTDecimalFractionToMapN(QCBOREncodeContext *pMe, int64_t nLabel,
				      uint8_t uTagRequirement,
				      int64_t nMantissa,
				      int64_t nBase10Exponent)
{
	QCBOREncode_AddInt64(pMe, nLabel);
	QCBOREncode_AddTDecimalFraction(pMe, uTagRequirement, nMantissa,
					nBase10Exponent);
}

static inline void
QCBOREncode_AddDecimalFraction(QCBOREncodeContext *pMe, int64_t nMantissa,
			       int64_t nBase10Exponent)
{
	QCBOREncode_AddTDecimalFraction(pMe, QCBOR_ENCODE_AS_TAG, nMantissa,
					nBase10Exponent);
}

static inline void
QCBOREncode_AddDecimalFractionToMap(QCBOREncodeContext *pMe,
				    const char *szLabel, int64_t nMantissa,
				    int64_t nBase10Exponent)
{
	QCBOREncode_AddTDecimalFractionToMapSZ(
		pMe, szLabel, QCBOR_ENCODE_AS_TAG, nMantissa, nBase10Exponent);
}

static inline void
QCBOREncode_AddDecimalFractionToMapN(QCBOREncodeContext *pMe, int64_t nLabel,
				     int64_t nMantissa, int64_t nBase10Exponent)
{
	QCBOREncode_AddTDecimalFractionToMapN(pMe, nLabel, QCBOR_ENCODE_AS_TAG,
					      nMantissa, nBase10Exponent);
}

static inline void
QCBOREncode_AddTDecimalFractionBigNum(QCBOREncodeContext *pMe,
				      uint8_t		  uTagRequirement,
				      UsefulBufC Mantissa, bool bIsNegative,
				      int64_t nBase10Exponent)
{
	uint64_t uTag;
	if (uTagRequirement == QCBOR_ENCODE_AS_TAG) {
		uTag = CBOR_TAG_DECIMAL_FRACTION;
	} else {
		uTag = CBOR_TAG_INVALID64;
	}
	QCBOREncode_AddExponentAndMantissa(pMe, uTag, Mantissa, bIsNegative, 0,
					   nBase10Exponent);
}

static inline void
QCBOREncode_AddTDecimalFractionBigNumToMapSZ(
	QCBOREncodeContext *pMe, const char *szLabel, uint8_t uTagRequirement,
	UsefulBufC Mantissa, bool bIsNegative, int64_t nBase10Exponent)
{
	QCBOREncode_AddSZString(pMe, szLabel);
	QCBOREncode_AddTDecimalFractionBigNum(pMe, uTagRequirement, Mantissa,
					      bIsNegative, nBase10Exponent);
}

static inline void
QCBOREncode_AddTDecimalFractionBigNumToMapN(
	QCBOREncodeContext *pMe, int64_t nLabel, uint8_t uTagRequirement,
	UsefulBufC Mantissa, bool bIsNegative, int64_t nBase10Exponent)
{
	QCBOREncode_AddInt64(pMe, nLabel);
	QCBOREncode_AddTDecimalFractionBigNum(pMe, uTagRequirement, Mantissa,
					      bIsNegative, nBase10Exponent);
}

static inline void
QCBOREncode_AddDecimalFractionBigNum(QCBOREncodeContext *pMe,
				     UsefulBufC Mantissa, bool bIsNegative,
				     int64_t nBase10Exponent)
{
	QCBOREncode_AddTDecimalFractionBigNum(pMe, QCBOR_ENCODE_AS_TAG,
					      Mantissa, bIsNegative,
					      nBase10Exponent);
}

static inline void
QCBOREncode_AddDecimalFractionBigNumToMapSZ(QCBOREncodeContext *pMe,
					    const char	       *szLabel,
					    UsefulBufC		Mantissa,
					    bool		bIsNegative,
					    int64_t		nBase10Exponent)
{
	QCBOREncode_AddTDecimalFractionBigNumToMapSZ(pMe, szLabel,
						     QCBOR_ENCODE_AS_TAG,
						     Mantissa, bIsNegative,
						     nBase10Exponent);
}

static inline void
QCBOREncode_AddDecimalFractionBigNumToMapN(QCBOREncodeContext *pMe,
					   int64_t nLabel, UsefulBufC Mantissa,
					   bool	   bIsNegative,
					   int64_t nBase2Exponent)
{
	QCBOREncode_AddTDecimalFractionBigNumToMapN(pMe, nLabel,
						    QCBOR_ENCODE_AS_TAG,
						    Mantissa, bIsNegative,
						    nBase2Exponent);
}

static inline void
QCBOREncode_AddTBigFloat(QCBOREncodeContext *pMe, uint8_t uTagRequirement,
			 int64_t nMantissa, int64_t nBase2Exponent)
{
	uint64_t uTag;
	if (uTagRequirement == QCBOR_ENCODE_AS_TAG) {
		uTag = CBOR_TAG_BIGFLOAT;
	} else {
		uTag = CBOR_TAG_INVALID64;
	}
	QCBOREncode_AddExponentAndMantissa(pMe, uTag, NULLUsefulBufC, false,
					   nMantissa, nBase2Exponent);
}

static inline void
QCBOREncode_AddTBigFloatToMapSZ(QCBOREncodeContext *pMe, const char *szLabel,
				uint8_t uTagRequirement, int64_t nMantissa,
				int64_t nBase2Exponent)
{
	QCBOREncode_AddSZString(pMe, szLabel);
	QCBOREncode_AddTBigFloat(pMe, uTagRequirement, nMantissa,
				 nBase2Exponent);
}

static inline void
QCBOREncode_AddTBigFloatToMapN(QCBOREncodeContext *pMe, int64_t nLabel,
			       uint8_t uTagRequirement, int64_t nMantissa,
			       int64_t nBase2Exponent)
{
	QCBOREncode_AddInt64(pMe, nLabel);
	QCBOREncode_AddTBigFloat(pMe, uTagRequirement, nMantissa,
				 nBase2Exponent);
}

static inline void
QCBOREncode_AddBigFloat(QCBOREncodeContext *pMe, int64_t nMantissa,
			int64_t nBase2Exponent)
{
	QCBOREncode_AddTBigFloat(pMe, QCBOR_ENCODE_AS_TAG, nMantissa,
				 nBase2Exponent);
}

static inline void
QCBOREncode_AddBigFloatToMap(QCBOREncodeContext *pMe, const char *szLabel,
			     int64_t nMantissa, int64_t nBase2Exponent)
{
	QCBOREncode_AddTBigFloatToMapSZ(pMe, szLabel, QCBOR_ENCODE_AS_TAG,
					nMantissa, nBase2Exponent);
}

static inline void
QCBOREncode_AddBigFloatToMapN(QCBOREncodeContext *pMe, int64_t nLabel,
			      int64_t nMantissa, int64_t nBase2Exponent)
{
	QCBOREncode_AddTBigFloatToMapN(pMe, nLabel, QCBOR_ENCODE_AS_TAG,
				       nMantissa, nBase2Exponent);
}

static inline void
QCBOREncode_AddTBigFloatBigNum(QCBOREncodeContext *pMe, uint8_t uTagRequirement,
			       UsefulBufC Mantissa, bool bIsNegative,
			       int64_t nBase2Exponent)
{
	uint64_t uTag;
	if (uTagRequirement == QCBOR_ENCODE_AS_TAG) {
		uTag = CBOR_TAG_BIGFLOAT;
	} else {
		uTag = CBOR_TAG_INVALID64;
	}
	QCBOREncode_AddExponentAndMantissa(pMe, uTag, Mantissa, bIsNegative, 0,
					   nBase2Exponent);
}

static inline void
QCBOREncode_AddTBigFloatBigNumToMapSZ(QCBOREncodeContext *pMe,
				      const char	 *szLabel,
				      uint8_t		  uTagRequirement,
				      UsefulBufC Mantissa, bool bIsNegative,
				      int64_t nBase2Exponent)
{
	QCBOREncode_AddSZString(pMe, szLabel);
	QCBOREncode_AddTBigFloatBigNum(pMe, uTagRequirement, Mantissa,
				       bIsNegative, nBase2Exponent);
}

static inline void
QCBOREncode_AddTBigFloatBigNumToMapN(QCBOREncodeContext *pMe, int64_t nLabel,
				     uint8_t	uTagRequirement,
				     UsefulBufC Mantissa, bool bIsNegative,
				     int64_t nBase2Exponent)
{
	QCBOREncode_AddInt64(pMe, nLabel);
	QCBOREncode_AddTBigFloatBigNum(pMe, uTagRequirement, Mantissa,
				       bIsNegative, nBase2Exponent);
}

static inline void
QCBOREncode_AddBigFloatBigNum(QCBOREncodeContext *pMe, UsefulBufC Mantissa,
			      bool bIsNegative, int64_t nBase2Exponent)
{
	QCBOREncode_AddTBigFloatBigNum(pMe, QCBOR_ENCODE_AS_TAG, Mantissa,
				       bIsNegative, nBase2Exponent);
}

static inline void
QCBOREncode_AddBigFloatBigNumToMap(QCBOREncodeContext *pMe, const char *szLabel,
				   UsefulBufC Mantissa, bool bIsNegative,
				   int64_t nBase2Exponent)
{
	QCBOREncode_AddTBigFloatBigNumToMapSZ(pMe, szLabel, QCBOR_ENCODE_AS_TAG,
					      Mantissa, bIsNegative,
					      nBase2Exponent);
}

static inline void
QCBOREncode_AddBigFloatBigNumToMapN(QCBOREncodeContext *pMe, int64_t nLabel,
				    UsefulBufC Mantissa, bool bIsNegative,
				    int64_t nBase2Exponent)
{
	QCBOREncode_AddTBigFloatBigNumToMapN(pMe, nLabel, QCBOR_ENCODE_AS_TAG,
					     Mantissa, bIsNegative,
					     nBase2Exponent);
}
#endif /* QCBOR_DISABLE_EXP_AND_MANTISSA */

static inline void
QCBOREncode_AddTURI(QCBOREncodeContext *pMe, uint8_t uTagRequirement,
		    UsefulBufC URI)
{
	if (uTagRequirement == QCBOR_ENCODE_AS_TAG) {
		QCBOREncode_AddTag(pMe, CBOR_TAG_URI);
	}
	QCBOREncode_AddText(pMe, URI);
}

static inline void
QCBOREncode_AddTURIToMapSZ(QCBOREncodeContext *pMe, const char *szLabel,
			   uint8_t uTagRequirement, UsefulBufC URI)
{
	QCBOREncode_AddSZString(pMe, szLabel);
	QCBOREncode_AddTURI(pMe, uTagRequirement, URI);
}

static inline void
QCBOREncode_AddTURIToMapN(QCBOREncodeContext *pMe, int64_t nLabel,
			  uint8_t uTagRequirement, UsefulBufC URI)
{
	QCBOREncode_AddInt64(pMe, nLabel);
	QCBOREncode_AddTURI(pMe, uTagRequirement, URI);
}

static inline void
QCBOREncode_AddURI(QCBOREncodeContext *pMe, UsefulBufC URI)
{
	QCBOREncode_AddTURI(pMe, QCBOR_ENCODE_AS_TAG, URI);
}

static inline void
QCBOREncode_AddURIToMap(QCBOREncodeContext *pMe, const char *szLabel,
			UsefulBufC URI)
{
	QCBOREncode_AddTURIToMapSZ(pMe, szLabel, QCBOR_ENCODE_AS_TAG, URI);
}

static inline void
QCBOREncode_AddURIToMapN(QCBOREncodeContext *pMe, int64_t nLabel,
			 UsefulBufC URI)
{
	QCBOREncode_AddTURIToMapN(pMe, nLabel, QCBOR_ENCODE_AS_TAG, URI);
}

static inline void
QCBOREncode_AddTB64Text(QCBOREncodeContext *pMe, uint8_t uTagRequirement,
			UsefulBufC B64Text)
{
	if (uTagRequirement == QCBOR_ENCODE_AS_TAG) {
		QCBOREncode_AddTag(pMe, CBOR_TAG_B64);
	}
	QCBOREncode_AddText(pMe, B64Text);
}

static inline void
QCBOREncode_AddTB64TextToMapSZ(QCBOREncodeContext *pMe, const char *szLabel,
			       uint8_t uTagRequirement, UsefulBufC B64Text)
{
	QCBOREncode_AddSZString(pMe, szLabel);
	QCBOREncode_AddTB64Text(pMe, uTagRequirement, B64Text);
}

static inline void
QCBOREncode_AddTB64TextToMapN(QCBOREncodeContext *pMe, int64_t nLabel,
			      uint8_t uTagRequirement, UsefulBufC B64Text)
{
	QCBOREncode_AddInt64(pMe, nLabel);
	QCBOREncode_AddTB64Text(pMe, uTagRequirement, B64Text);
}

static inline void
QCBOREncode_AddB64Text(QCBOREncodeContext *pMe, UsefulBufC B64Text)
{
	QCBOREncode_AddTB64Text(pMe, QCBOR_ENCODE_AS_TAG, B64Text);
}

static inline void
QCBOREncode_AddB64TextToMap(QCBOREncodeContext *pMe, const char *szLabel,
			    UsefulBufC B64Text)
{
	QCBOREncode_AddTB64TextToMapSZ(pMe, szLabel, QCBOR_ENCODE_AS_TAG,
				       B64Text);
}

static inline void
QCBOREncode_AddB64TextToMapN(QCBOREncodeContext *pMe, int64_t nLabel,
			     UsefulBufC B64Text)
{
	QCBOREncode_AddTB64TextToMapN(pMe, nLabel, QCBOR_ENCODE_AS_TAG,
				      B64Text);
}

static inline void
QCBOREncode_AddTB64URLText(QCBOREncodeContext *pMe, uint8_t uTagRequirement,
			   UsefulBufC B64Text)
{
	if (uTagRequirement == QCBOR_ENCODE_AS_TAG) {
		QCBOREncode_AddTag(pMe, CBOR_TAG_B64URL);
	}
	QCBOREncode_AddText(pMe, B64Text);
}

static inline void
QCBOREncode_AddTB64URLTextToMapSZ(QCBOREncodeContext *pMe, const char *szLabel,
				  uint8_t uTagRequirement, UsefulBufC B64Text)
{
	QCBOREncode_AddSZString(pMe, szLabel);
	QCBOREncode_AddTB64URLText(pMe, uTagRequirement, B64Text);
}

static inline void
QCBOREncode_AddTB64URLTextToMapN(QCBOREncodeContext *pMe, int64_t nLabel,
				 uint8_t uTagRequirement, UsefulBufC B64Text)
{
	QCBOREncode_AddInt64(pMe, nLabel);
	QCBOREncode_AddTB64URLText(pMe, uTagRequirement, B64Text);
}

static inline void
QCBOREncode_AddB64URLText(QCBOREncodeContext *pMe, UsefulBufC B64Text)
{
	QCBOREncode_AddTB64URLText(pMe, QCBOR_ENCODE_AS_TAG, B64Text);
}

static inline void
QCBOREncode_AddB64URLTextToMap(QCBOREncodeContext *pMe, const char *szLabel,
			       UsefulBufC B64Text)
{
	QCBOREncode_AddTB64URLTextToMapSZ(pMe, szLabel, QCBOR_ENCODE_AS_TAG,
					  B64Text);
}

static inline void
QCBOREncode_AddB64URLTextToMapN(QCBOREncodeContext *pMe, int64_t nLabel,
				UsefulBufC B64Text)
{
	QCBOREncode_AddTB64URLTextToMapN(pMe, nLabel, QCBOR_ENCODE_AS_TAG,
					 B64Text);
}

static inline void
QCBOREncode_AddTRegex(QCBOREncodeContext *pMe, uint8_t uTagRequirement,
		      UsefulBufC Bytes)
{
	if (uTagRequirement == QCBOR_ENCODE_AS_TAG) {
		QCBOREncode_AddTag(pMe, CBOR_TAG_REGEX);
	}
	QCBOREncode_AddText(pMe, Bytes);
}

static inline void
QCBOREncode_AddTRegexToMapSZ(QCBOREncodeContext *pMe, const char *szLabel,
			     uint8_t uTagRequirement, UsefulBufC Bytes)
{
	QCBOREncode_AddSZString(pMe, szLabel);
	QCBOREncode_AddTRegex(pMe, uTagRequirement, Bytes);
}

static inline void
QCBOREncode_AddTRegexToMapN(QCBOREncodeContext *pMe, int64_t nLabel,
			    uint8_t uTagRequirement, UsefulBufC Bytes)
{
	QCBOREncode_AddInt64(pMe, nLabel);
	QCBOREncode_AddTRegex(pMe, uTagRequirement, Bytes);
}

static inline void
QCBOREncode_AddRegex(QCBOREncodeContext *pMe, UsefulBufC Bytes)
{
	QCBOREncode_AddTRegex(pMe, QCBOR_ENCODE_AS_TAG, Bytes);
}

static inline void
QCBOREncode_AddRegexToMap(QCBOREncodeContext *pMe, const char *szLabel,
			  UsefulBufC Bytes)
{
	QCBOREncode_AddTRegexToMapSZ(pMe, szLabel, QCBOR_ENCODE_AS_TAG, Bytes);
}

static inline void
QCBOREncode_AddRegexToMapN(QCBOREncodeContext *pMe, int64_t nLabel,
			   UsefulBufC Bytes)
{
	QCBOREncode_AddTRegexToMapN(pMe, nLabel, QCBOR_ENCODE_AS_TAG, Bytes);
}

static inline void
QCBOREncode_AddTMIMEData(QCBOREncodeContext *pMe, uint8_t uTagRequirement,
			 UsefulBufC MIMEData)
{
	if (uTagRequirement == QCBOR_ENCODE_AS_TAG) {
		QCBOREncode_AddTag(pMe, CBOR_TAG_BINARY_MIME);
	}
	QCBOREncode_AddBytes(pMe, MIMEData);
}

static inline void
QCBOREncode_AddTMIMEDataToMapSZ(QCBOREncodeContext *pMe, const char *szLabel,
				uint8_t uTagRequirement, UsefulBufC MIMEData)
{
	QCBOREncode_AddSZString(pMe, szLabel);
	QCBOREncode_AddTMIMEData(pMe, uTagRequirement, MIMEData);
}

static inline void
QCBOREncode_AddTMIMEDataToMapN(QCBOREncodeContext *pMe, int64_t nLabel,
			       uint8_t uTagRequirement, UsefulBufC MIMEData)
{
	QCBOREncode_AddInt64(pMe, nLabel);
	QCBOREncode_AddTMIMEData(pMe, uTagRequirement, MIMEData);
}

static inline void
QCBOREncode_AddMIMEData(QCBOREncodeContext *pMe, UsefulBufC MIMEData)
{
	QCBOREncode_AddTMIMEData(pMe, QCBOR_ENCODE_AS_TAG, MIMEData);
}

static inline void
QCBOREncode_AddMIMEDataToMap(QCBOREncodeContext *pMe, const char *szLabel,
			     UsefulBufC MIMEData)
{
	QCBOREncode_AddTMIMEDataToMapSZ(pMe, szLabel, QCBOR_ENCODE_AS_TAG,
					MIMEData);
}

static inline void
QCBOREncode_AddMIMEDataToMapN(QCBOREncodeContext *pMe, int64_t nLabel,
			      UsefulBufC MIMEData)
{
	QCBOREncode_AddTMIMEDataToMapN(pMe, nLabel, QCBOR_ENCODE_AS_TAG,
				       MIMEData);
}

static inline void
QCBOREncode_AddTDateString(QCBOREncodeContext *pMe, uint8_t uTagRequirement,
			   const char *szDate)
{
	if (uTagRequirement == QCBOR_ENCODE_AS_TAG) {
		QCBOREncode_AddTag(pMe, CBOR_TAG_DATE_STRING);
	}
	QCBOREncode_AddSZString(pMe, szDate);
}

static inline void
QCBOREncode_AddTDateStringToMapSZ(QCBOREncodeContext *pMe, const char *szLabel,
				  uint8_t uTagRequirement, const char *szDate)
{
	QCBOREncode_AddSZString(pMe, szLabel);
	QCBOREncode_AddTDateString(pMe, uTagRequirement, szDate);
}

static inline void
QCBOREncode_AddTDateStringToMapN(QCBOREncodeContext *pMe, int64_t nLabel,
				 uint8_t uTagRequirement, const char *szDate)
{
	QCBOREncode_AddInt64(pMe, nLabel);
	QCBOREncode_AddTDateString(pMe, uTagRequirement, szDate);
}

static inline void
QCBOREncode_AddDateString(QCBOREncodeContext *pMe, const char *szDate)
{
	QCBOREncode_AddTDateString(pMe, QCBOR_ENCODE_AS_TAG, szDate);
}

static inline void
QCBOREncode_AddDateStringToMap(QCBOREncodeContext *pMe, const char *szLabel,
			       const char *szDate)
{
	QCBOREncode_AddTDateStringToMapSZ(pMe, szLabel, QCBOR_ENCODE_AS_TAG,
					  szDate);
}

static inline void
QCBOREncode_AddDateStringToMapN(QCBOREncodeContext *pMe, int64_t nLabel,
				const char *szDate)
{
	QCBOREncode_AddTDateStringToMapN(pMe, nLabel, QCBOR_ENCODE_AS_TAG,
					 szDate);
}

static inline void
QCBOREncode_AddTDaysString(QCBOREncodeContext *pMe, uint8_t uTagRequirement,
			   const char *szDate)
{
	if (uTagRequirement == QCBOR_ENCODE_AS_TAG) {
		QCBOREncode_AddTag(pMe, CBOR_TAG_DAYS_STRING);
	}
	QCBOREncode_AddSZString(pMe, szDate);
}

static inline void
QCBOREncode_AddTDaysStringToMapSZ(QCBOREncodeContext *pMe, const char *szLabel,
				  uint8_t uTagRequirement, const char *szDate)
{
	QCBOREncode_AddSZString(pMe, szLabel);
	QCBOREncode_AddTDaysString(pMe, uTagRequirement, szDate);
}

static inline void
QCBOREncode_AddTDaysStringToMapN(QCBOREncodeContext *pMe, int64_t nLabel,
				 uint8_t uTagRequirement, const char *szDate)
{
	QCBOREncode_AddInt64(pMe, nLabel);
	QCBOREncode_AddTDaysString(pMe, uTagRequirement, szDate);
}

static inline void
QCBOREncode_AddSimple(QCBOREncodeContext *pMe, uint64_t uNum)
{
	QCBOREncode_AddType7(pMe, 0, uNum);
}

static inline void
QCBOREncode_AddSimpleToMap(QCBOREncodeContext *pMe, const char *szLabel,
			   uint8_t uSimple)
{
	QCBOREncode_AddSZString(pMe, szLabel);
	QCBOREncode_AddSimple(pMe, uSimple);
}

static inline void
QCBOREncode_AddSimpleToMapN(QCBOREncodeContext *pMe, int nLabel,
			    uint8_t uSimple)
{
	QCBOREncode_AddInt64(pMe, nLabel);
	QCBOREncode_AddSimple(pMe, uSimple);
}

static inline void
QCBOREncode_AddBool(QCBOREncodeContext *pMe, bool b)
{
	uint8_t uSimple = CBOR_SIMPLEV_FALSE;
	if (b) {
		uSimple = CBOR_SIMPLEV_TRUE;
	}
	QCBOREncode_AddSimple(pMe, uSimple);
}

static inline void
QCBOREncode_AddBoolToMap(QCBOREncodeContext *pMe, const char *szLabel, bool b)
{
	QCBOREncode_AddSZString(pMe, szLabel);
	QCBOREncode_AddBool(pMe, b);
}

static inline void
QCBOREncode_AddBoolToMapN(QCBOREncodeContext *pMe, int64_t nLabel, bool b)
{
	QCBOREncode_AddInt64(pMe, nLabel);
	QCBOREncode_AddBool(pMe, b);
}

static inline void
QCBOREncode_AddNULL(QCBOREncodeContext *pMe)
{
	QCBOREncode_AddSimple(pMe, CBOR_SIMPLEV_NULL);
}

static inline void
QCBOREncode_AddNULLToMap(QCBOREncodeContext *pMe, const char *szLabel)
{
	QCBOREncode_AddSZString(pMe, szLabel);
	QCBOREncode_AddNULL(pMe);
}

static inline void
QCBOREncode_AddNULLToMapN(QCBOREncodeContext *pMe, int64_t nLabel)
{
	QCBOREncode_AddInt64(pMe, nLabel);
	QCBOREncode_AddNULL(pMe);
}

static inline void
QCBOREncode_AddUndef(QCBOREncodeContext *pMe)
{
	QCBOREncode_AddSimple(pMe, CBOR_SIMPLEV_UNDEF);
}

static inline void
QCBOREncode_AddUndefToMap(QCBOREncodeContext *pMe, const char *szLabel)
{
	QCBOREncode_AddSZString(pMe, szLabel);
	QCBOREncode_AddUndef(pMe);
}

static inline void
QCBOREncode_AddUndefToMapN(QCBOREncodeContext *pMe, int64_t nLabel)
{
	QCBOREncode_AddInt64(pMe, nLabel);
	QCBOREncode_AddUndef(pMe);
}

static inline void
QCBOREncode_OpenArray(QCBOREncodeContext *pMe)
{
	QCBOREncode_OpenMapOrArray(pMe, CBOR_MAJOR_TYPE_ARRAY);
}

static inline void
QCBOREncode_OpenArrayInMap(QCBOREncodeContext *pMe, const char *szLabel)
{
	QCBOREncode_AddSZString(pMe, szLabel);
	QCBOREncode_OpenArray(pMe);
}

static inline void
QCBOREncode_OpenArrayInMapN(QCBOREncodeContext *pMe, int64_t nLabel)
{
	QCBOREncode_AddInt64(pMe, nLabel);
	QCBOREncode_OpenArray(pMe);
}

static inline void
QCBOREncode_CloseArray(QCBOREncodeContext *pMe)
{
	QCBOREncode_CloseMapOrArray(pMe, CBOR_MAJOR_TYPE_ARRAY);
}

static inline void
QCBOREncode_OpenMap(QCBOREncodeContext *pMe)
{
	QCBOREncode_OpenMapOrArray(pMe, CBOR_MAJOR_TYPE_MAP);
}

static inline void
QCBOREncode_OpenMapInMap(QCBOREncodeContext *pMe, const char *szLabel)
{
	QCBOREncode_AddSZString(pMe, szLabel);
	QCBOREncode_OpenMap(pMe);
}

static inline void
QCBOREncode_OpenMapInMapN(QCBOREncodeContext *pMe, int64_t nLabel)
{
	QCBOREncode_AddInt64(pMe, nLabel);
	QCBOREncode_OpenMap(pMe);
}

static inline void
QCBOREncode_CloseMap(QCBOREncodeContext *pMe)
{
	QCBOREncode_CloseMapOrArray(pMe, CBOR_MAJOR_TYPE_MAP);
}

static inline void
QCBOREncode_OpenArrayIndefiniteLength(QCBOREncodeContext *pMe)
{
	QCBOREncode_OpenMapOrArrayIndefiniteLength(
		pMe, CBOR_MAJOR_NONE_TYPE_ARRAY_INDEFINITE_LEN);
}

static inline void
QCBOREncode_OpenArrayIndefiniteLengthInMap(QCBOREncodeContext *pMe,
					   const char	      *szLabel)
{
	QCBOREncode_AddSZString(pMe, szLabel);
	QCBOREncode_OpenArrayIndefiniteLength(pMe);
}

static inline void
QCBOREncode_OpenArrayIndefiniteLengthInMapN(QCBOREncodeContext *pMe,
					    int64_t		nLabel)
{
	QCBOREncode_AddInt64(pMe, nLabel);
	QCBOREncode_OpenArrayIndefiniteLength(pMe);
}

static inline void
QCBOREncode_CloseArrayIndefiniteLength(QCBOREncodeContext *pMe)
{
	QCBOREncode_CloseMapOrArrayIndefiniteLength(
		pMe, CBOR_MAJOR_NONE_TYPE_ARRAY_INDEFINITE_LEN);
}

static inline void
QCBOREncode_OpenMapIndefiniteLength(QCBOREncodeContext *pMe)
{
	QCBOREncode_OpenMapOrArrayIndefiniteLength(
		pMe, CBOR_MAJOR_NONE_TYPE_MAP_INDEFINITE_LEN);
}

static inline void
QCBOREncode_OpenMapIndefiniteLengthInMap(QCBOREncodeContext *pMe,
					 const char	    *szLabel)
{
	QCBOREncode_AddSZString(pMe, szLabel);
	QCBOREncode_OpenMapIndefiniteLength(pMe);
}

static inline void
QCBOREncode_OpenMapIndefiniteLengthInMapN(QCBOREncodeContext *pMe,
					  int64_t	      nLabel)
{
	QCBOREncode_AddInt64(pMe, nLabel);
	QCBOREncode_OpenMapIndefiniteLength(pMe);
}

static inline void
QCBOREncode_CloseMapIndefiniteLength(QCBOREncodeContext *pMe)
{
	QCBOREncode_CloseMapOrArrayIndefiniteLength(
		pMe, CBOR_MAJOR_NONE_TYPE_MAP_INDEFINITE_LEN);
}

static inline void
QCBOREncode_BstrWrap(QCBOREncodeContext *pMe)
{
	QCBOREncode_OpenMapOrArray(pMe, CBOR_MAJOR_TYPE_BYTE_STRING);
}

static inline void
QCBOREncode_BstrWrapInMap(QCBOREncodeContext *pMe, const char *szLabel)
{
	QCBOREncode_AddSZString(pMe, szLabel);
	QCBOREncode_BstrWrap(pMe);
}

static inline void
QCBOREncode_BstrWrapInMapN(QCBOREncodeContext *pMe, int64_t nLabel)
{
	QCBOREncode_AddInt64(pMe, nLabel);
	QCBOREncode_BstrWrap(pMe);
}

static inline void
QCBOREncode_CloseBstrWrap(QCBOREncodeContext *pMe, UsefulBufC *pWrappedCBOR)
{
	QCBOREncode_CloseBstrWrap2(pMe, true, pWrappedCBOR);
}

static inline void
QCBOREncode_AddEncoded(QCBOREncodeContext *pMe, UsefulBufC Encoded)
{
	QCBOREncode_AddBuffer(pMe, CBOR_MAJOR_NONE_TYPE_RAW, Encoded);
}

static inline void
QCBOREncode_AddEncodedToMap(QCBOREncodeContext *pMe, const char *szLabel,
			    UsefulBufC Encoded)
{
	QCBOREncode_AddSZString(pMe, szLabel);
	QCBOREncode_AddEncoded(pMe, Encoded);
}

static inline void
QCBOREncode_AddEncodedToMapN(QCBOREncodeContext *pMe, int64_t nLabel,
			     UsefulBufC Encoded)
{
	QCBOREncode_AddInt64(pMe, nLabel);
	QCBOREncode_AddEncoded(pMe, Encoded);
}

static inline int
QCBOREncode_IsBufferNULL(QCBOREncodeContext *pMe)
{
	return UsefulOutBuf_IsBufferNULL(&(pMe->OutBuf));
}

static inline QCBORError
QCBOREncode_GetErrorState(QCBOREncodeContext *pMe)
{
	if (UsefulOutBuf_GetError(&(pMe->OutBuf))) {
		// Items didn't fit in the buffer.
		// This check catches this condition for all the appends and
		// inserts so checks aren't needed when the appends and inserts
		// are performed. And of course UsefulBuf will never overrun the
		// input buffer given to it. No complex analysis of the error
		// handling in this file is needed to know that is true. Just
		// read the UsefulBuf code.
		pMe->uError = QCBOR_ERR_BUFFER_TOO_SMALL;
		// QCBOR_ERR_BUFFER_TOO_SMALL masks other errors, but that is
		// OK. Once the caller fixes this, they'll be unmasked.
	}

	return (QCBORError)pMe->uError;
}

/* ========================================================================
     END OF PRIVATE INLINE IMPLEMENTATION
   ======================================================================== */

#ifdef __cplusplus
}
#endif

#endif /* qcbor_encode_h */

```

`hyp/interfaces/qcbor/include/qcbor/qcbor_private.h`:

```h
/*==============================================================================
 Copyright (c) 2016-2018, The Linux Foundation.
 Copyright (c) 2018-2021, Laurence Lundblade.
 Copyright (c) 2021, Arm Limited.
 All rights reserved.

 Redistribution and use in source and binary forms, with or without
 modification, are permitted provided that the following conditions are
 met:
 * Redistributions of source code must retain the above copyright
 notice, this list of conditions and the following disclaimer.
 * Redistributions in binary form must reproduce the above
 copyright notice, this list of conditions and the following
 disclaimer in the documentation and/or other materials provided
 with the distribution.
 * Neither the name of The Linux Foundation nor the names of its
 contributors, nor the name "Laurence Lundblade" may be used to
 endorse or promote products derived from this software without
 specific prior written permission.

 THIS SOFTWARE IS PROVIDED "AS IS" AND ANY EXPRESS OR IMPLIED
 WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF
 MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT
 ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS
 BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
 CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
 SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR
 BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,
 WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE
 OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN
 IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 =============================================================================*/

#ifndef qcbor_private_h
#define qcbor_private_h

#include <stdint.h>

#include "UsefulBuf.h"

#ifdef __cplusplus
extern "C" {
#if 0
} // Keep editor indention formatting happy
#endif
#endif

/*
 The maxium nesting of arrays and maps when encoding or decoding.
 (Further down in the file there is a definition that refers to this
 that is public. This is done this way so there can be a nice
 separation of public and private parts in this file.
*/
#define QCBOR_MAX_ARRAY_NESTING1 15 // Do not increase this over 255

/* The largest offset to the start of an array or map. It is slightly
 less than UINT32_MAX so the error condition can be tested on 32-bit machines.
 UINT32_MAX comes from uStart in QCBORTrackNesting being a uin32_t.

 This will cause trouble on a machine where size_t is less than 32-bits.
 */
#define QCBOR_MAX_ARRAY_OFFSET (UINT32_MAX - 100)

/* The number of tags that are 16-bit or larger that can be handled
 in a decode.
 */
#define QCBOR_NUM_MAPPED_TAGS 4

/* The number of tags (of any size) recorded for an individual item. */
#define QCBOR_MAX_TAGS_PER_ITEM1 4

/*
 Convenience macro for selecting the proper return value in case floating
 point feature(s) are disabled.

 The macros:

   FLOAT_ERR_CODE_NO_FLOAT(x) Can be used when disabled floating point should
			      result error, and all other cases should return
			      'x'.

   The below macros always return QCBOR_ERR_ALL_FLOAT_DISABLED when all floating
   point is disabled.

   FLOAT_ERR_CODE_NO_HALF_PREC(x) Can be used when disabled preferred float
				  results in error, and all other cases should
				  return 'x'.
   FLOAT_ERR_CODE_NO_FLOAT_HW(x) Can be used when disabled hardware floating
				 point results in error, and all other cases
				 should return 'x'.
   FLOAT_ERR_CODE_NO_HALF_PREC_NO_FLOAT_HW(x) Can be used when either disabled
					      preferred float or disabling
					      hardware floating point results in
					      error, and all other cases should
					      return 'x'.
 */
#ifdef USEFULBUF_DISABLE_ALL_FLOAT
#define FLOAT_ERR_CODE_NO_FLOAT(x)		   QCBOR_ERR_ALL_FLOAT_DISABLED
#define FLOAT_ERR_CODE_NO_HALF_PREC(x)		   QCBOR_ERR_ALL_FLOAT_DISABLED
#define FLOAT_ERR_CODE_NO_FLOAT_HW(x)		   QCBOR_ERR_ALL_FLOAT_DISABLED
#define FLOAT_ERR_CODE_NO_HALF_PREC_NO_FLOAT_HW(x) QCBOR_ERR_ALL_FLOAT_DISABLED
#else /* USEFULBUF_DISABLE_ALL_FLOAT*/
#define FLOAT_ERR_CODE_NO_FLOAT(x) x
#ifdef QCBOR_DISABLE_PREFERRED_FLOAT
#define FLOAT_ERR_CODE_NO_HALF_PREC(x) QCBOR_ERR_HALF_PRECISION_DISABLED
#define FLOAT_ERR_CODE_NO_HALF_PREC_NO_FLOAT_HW(x)                             \
	QCBOR_ERR_HALF_PRECISION_DISABLED
#else /* QCBOR_DISABLE_PREFERRED_FLOAT */
#define FLOAT_ERR_CODE_NO_HALF_PREC(x) x
#ifdef QCBOR_DISABLE_FLOAT_HW_USE
#define FLOAT_ERR_CODE_NO_HALF_PREC_NO_FLOAT_HW(x) QCBOR_ERR_HW_FLOAT_DISABLED
#else
#define FLOAT_ERR_CODE_NO_HALF_PREC_NO_FLOAT_HW(x) x
#endif
#endif /* QCBOR_DISABLE_PREFERRED_FLOAT */
#ifdef QCBOR_DISABLE_FLOAT_HW_USE
#define FLOAT_ERR_CODE_NO_FLOAT_HW(x) QCBOR_ERR_HW_FLOAT_DISABLED
#else /* QCBOR_DISABLE_FLOAT_HW_USE */
#define FLOAT_ERR_CODE_NO_FLOAT_HW(x) x
#endif /* QCBOR_DISABLE_FLOAT_HW_USE */
#endif /*USEFULBUF_DISABLE_ALL_FLOAT*/

/*
 PRIVATE DATA STRUCTURE

 Holds the data for tracking array and map nesting during encoding. Pairs up
 with the Nesting_xxx functions to make an "object" to handle nesting encoding.

 uStart is a uint32_t instead of a size_t to keep the size of this
 struct down so it can be on the stack without any concern.  It would be about
 double if size_t was used instead.

 Size approximation (varies with CPU/compiler):
    64-bit machine: (15 + 1) * (4 + 2 + 1 + 1 pad) + 8 = 136 bytes
    32-bit machine: (15 + 1) * (4 + 2 + 1 + 1 pad) + 4 = 132 bytes
*/
typedef struct QCBORTrackNesting_s {
	// PRIVATE DATA STRUCTURE
	struct {
		// See function QCBOREncode_OpenMapOrArray() for details on how
		// this works
		uint32_t uStart; // uStart is the byte position where the array
				 // starts
		uint16_t uCount; // Number of items in the arrary or map; counts
				 // items in a map, not pairs of items
		uint8_t uMajorType; // Indicates if item is a map or an array
		uint8_t padding_[1];
	} pArrays[QCBOR_MAX_ARRAY_NESTING1 + 1], // stored state for the nesting
						 // levels
		*pCurrentNesting;		 // the current nesting level
} QCBORTrackNesting;

/*
 PRIVATE DATA STRUCTURE

 Context / data object for encoding some CBOR. Used by all encode functions to
 form a public "object" that does the job of encdoing.

 Size approximation (varies with CPU/compiler):
   64-bit machine: 27 + 1 (+ 4 padding) + 136 = 32 + 136 = 168 bytes
   32-bit machine: 15 + 1 + 132 = 148 bytes
*/
struct QCBOREncodeContext_s {
	// PRIVATE DATA STRUCTURE
	UsefulOutBuf OutBuf;	  // Pointer to output buffer, its length and
				  // position in it
	uint8_t		  uError; // Error state, always from QCBORError enum
	uint8_t		  padding_[7];
	QCBORTrackNesting nesting; // Keep track of array and map nesting
};

/*
 PRIVATE DATA STRUCTURE

 Holds the data for array and map nesting for decoding work. This
 structure and the DecodeNesting_Xxx() functions in qcbor_decode.c
 form an "object" that does the work for arrays and maps. All access
 to this structure is through DecodeNesting_Xxx() functions.

 64-bit machine size
   128 = 16 * 8 for the two unions
   64  = 16 * 4 for the uLevelType, 1 byte padded to 4 bytes for alignment
   16  = 16 bytes for two pointers
   208 TOTAL

 32-bit machine size is 200 bytes
 */
typedef struct QCBORDecodeNesting_s {
	// PRIVATE DATA STRUCTURE
	struct nesting_decode_level {
		/*
		 This keeps tracking info for each nesting level. There are two
		 main types of levels:
		   1) Byte count tracking. This is for the top level input CBOR
		   which might be a single item or a CBOR sequence and byte
		   string wrapped encoded CBOR.
		   2) Item tracking. This is for maps and arrays.

		 uLevelType has value QCBOR_TYPE_BYTE_STRING for 1) and
		 QCBOR_TYPE_MAP or QCBOR_TYPE_ARRAY or QCBOR_TYPE_MAP_AS_ARRAY
		 for 2).

		 Item tracking is either for definite or indefinite-length
		 maps/arrays. For definite lengths, the total count and items
		 unconsumed are tracked. For indefinite-length, uTotalCount is
		 QCBOR_COUNT_INDICATES_INDEFINITE_LENGTH (UINT16_MAX) and there
		 is no per-item count of members. For indefinite-length maps and
		 arrays, uCountCursor is UINT16_MAX if not consumed and zero if
		 it is consumed in the pre-order traversal. Additionally, if
		 entered in bounded mode, uCountCursor is
		 QCBOR_COUNT_INDICATES_ZERO_LENGTH to indicate it is empty.

		 This also records whether a level is bounded or not.  All
		 byte-count tracked levels (the top-level sequence and
		 bstr-wrapped CBOR) are bounded. Maps and arrays may or may not
		 be bounded. They are bounded if they were Entered() and not if
		 they were traversed with GetNext(). They are marked as bounded
		 by uStartOffset not being UINT32_MAX.
		 */
		/*
		 If uLevelType can put in a separately indexed array, the union/
		 struct will be 8 bytes rather than 9 and a lot of wasted
		 padding for alignment will be saved.
		 */
		uint8_t uLevelType;
		uint8_t padding_[3];

		union {
			struct {
#define QCBOR_COUNT_INDICATES_INDEFINITE_LENGTH UINT16_MAX
#define QCBOR_COUNT_INDICATES_ZERO_LENGTH	UINT16_MAX - 1
				uint16_t uCountTotal;
				uint16_t uCountCursor;
#define QCBOR_NON_BOUNDED_OFFSET UINT32_MAX
				uint32_t uStartOffset;
			} ma; /* for maps and arrays */

			struct {
				/* The end of the input before the bstr was
				 * entered so that it can be restored when the
				 * bstr is exited. */
				uint32_t uSavedEndOffset;
				/* The beginning of the bstr so that it can be
				 * rewound. */
				uint32_t uBstrStartOffset;
			} bs; /* for top-level sequence and bstr-wrapped CBOR */
		} u;
	} pLevels[QCBOR_MAX_ARRAY_NESTING1 + 1], *pCurrent, *pCurrentBounded;

	/*
	 pCurrent is for item-by-item pre-order traversal.

	 pCurrentBounded points to the current bounding level or is NULL if
	 there isn't one.

	 pCurrent must always be below pCurrentBounded as the pre-order
	 traversal is always bounded by the bounding level.

	 When a bounded level is entered, the pre-order traversal is set to
	 the first item in the bounded level. When a bounded level is
	 exited, the pre-order traversl is set to the next item after the
	 map, array or bstr. This may be more than one level up, or even
	 the end of the input CBOR.
	 */
} QCBORDecodeNesting;

typedef struct {
	// PRIVATE DATA STRUCTURE
	void *pAllocateCxt;
	UsefulBuf (*pfAllocator)(void *pAllocateCxt, void *pOldMem,
				 size_t uNewSize);
} QCBORInternalAllocator;

/*
 PRIVATE DATA STRUCTURE

 The decode context. This data structure plus the public QCBORDecode_xxx
 functions form an "object" that does CBOR decoding.

 Size approximation (varies with CPU/compiler):
   64-bit machine: 32 + 1 + 1 + 6 bytes padding + 72 + 16 + 8 + 8 = 144 bytes
   32-bit machine: 16 + 1 + 1 + 2 bytes padding + 68 +  8 + 8 + 4 = 108 bytes
 */
struct QCBORDecodeContext_s {
	// PRIVATE DATA STRUCTURE
	UsefulInputBuf InBuf;

	QCBORDecodeNesting nesting;

	// If a string allocator is configured for indefinite-length
	// strings, it is configured here.
	QCBORInternalAllocator StringAllocator;

	// These are special for the internal MemPool allocator.
	// They are not used otherwise. We tried packing these
	// in the MemPool itself, but there are issues
	// with memory alignment.
	uint32_t uMemPoolSize;
	uint32_t uMemPoolFreeOffset;

	// A cached offset to the end of the current map
	// 0 if no value is cached.
#define QCBOR_MAP_OFFSET_CACHE_INVALID UINT32_MAX
	uint32_t uMapEndOffsetCache;

	uint8_t	   uDecodeMode;
	uint8_t	   bStringAllocateAll;
	uint8_t	   padding_[2];
	QCBORError uLastError; // QCBORError stuffed into a uint8_t
	uint8_t	   padding_1[4];

	/* See MapTagNumber() for description of how tags are mapped. */
	uint64_t auMappedTags[QCBOR_NUM_MAPPED_TAGS];

	uint16_t uLastTags[QCBOR_MAX_TAGS_PER_ITEM1];
};

// Used internally in the impementation here
// Must not conflict with any of the official CBOR types
#define CBOR_MAJOR_NONE_TYPE_RAW	   9
#define CBOR_MAJOR_NONE_TAG_LABEL_REORDER  10
#define CBOR_MAJOR_NONE_TYPE_BSTR_LEN_ONLY 11
#define CBOR_MAJOR_NONE_TYPE_OPEN_BSTR	   12

// Add this to types to indicate they are to be encoded as indefinite lengths
#define QCBOR_INDEFINITE_LEN_TYPE_MODIFIER 0x80
#define CBOR_MAJOR_NONE_TYPE_ARRAY_INDEFINITE_LEN                              \
	CBOR_MAJOR_TYPE_ARRAY + QCBOR_INDEFINITE_LEN_TYPE_MODIFIER
#define CBOR_MAJOR_NONE_TYPE_MAP_INDEFINITE_LEN                                \
	CBOR_MAJOR_TYPE_MAP + QCBOR_INDEFINITE_LEN_TYPE_MODIFIER
#define CBOR_MAJOR_NONE_TYPE_SIMPLE_BREAK                                      \
	CBOR_MAJOR_TYPE_SIMPLE + QCBOR_INDEFINITE_LEN_TYPE_MODIFIER

/* Value of QCBORItem.val.string.len when the string length is
 * indefinite. Used temporarily in the implementation and never
 * returned in the public interface.
 */
#define QCBOR_STRING_LENGTH_INDEFINITE SIZE_MAX

/* The number of elements in a C array of a particular type */
#define C_ARRAY_COUNT(array, type) (sizeof(array) / sizeof(type))

#ifdef __cplusplus
}
#endif

#endif /* qcbor_private_h */

```

`hyp/interfaces/rcu/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

events rcu.ev
types rcu.tc
macros rcu_attrs.h

```

`hyp/interfaces/rcu/include/rcu.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// Start a read-side critical section.
//
// An RCU read-side critical section blocks execution of any update functions
// until it is ended by a matching call to rcu_read_finish(). These critical
// sections are permitted to nest; each call to this function must be balanced
// by exactly one matching call to rcu_read_finish().
//
// The caller must not assume that this function disables preemption.
void
rcu_read_start(void) ACQUIRE_RCU_READ;

// End a read-side critical section.
//
// This reverses the effect of the most recent unmatched rcu_read_start() call.
// If this ends the outermost nested critical section, then RCU update functions
// queued on any CPU after the start of the critical section are permitted to
// run.
void
rcu_read_finish(void) RELEASE_RCU_READ;

// Enqueue a write-side update.
//
// The given update will be processed at the end of the next grace period.
//
// The update is guaranteed not to run until all currently executing RCU
// critical sections have finished. However, there is no guarantee of ordering
// of separately enqueued updates with respect to each other. Nor is there a
// guarantee that they will run on the CPU that enqueued them.
void
rcu_enqueue(rcu_entry_t *rcu_entry, rcu_update_class_t rcu_update_class);

// Block until the current grace period ends.
//
// This is typically implemented using rcu_enqueue(), and therefore provides the
// same ordering guarantee: any currently executing RCU critical section must
// finish before this function can return, but other currently queued updates or
// rcu_sync() calls may not have completed.
void
rcu_sync(void);

// Block until the next grace period ends or the caller is killed.
//
// If this call returns true, it has the same semantics as rcu_sync(). If it
// returns false, the caller has been killed, and there are no ordering
// guarantees provided by RCU.
//
// Note that if this returns false, the caller has preemption disabled and may
// be running while blocked and/or on a CPU it does not have affinity to.
bool
rcu_sync_killable(void);

// Check for pending updates on the calling CPU.
//
// If this call returns true, the CPU should not be allowed to enter a low power
// state or power itself off.
bool
rcu_has_pending_updates(void) REQUIRE_PREEMPT_DISABLED;

```

`hyp/interfaces/rcu/include/rcu_attrs.h`:

```h
// © 2022 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#define ACQUIRE_RCU_READ ACQUIRE_READ(rcu_read)
#define RELEASE_RCU_READ RELEASE_READ(rcu_read)
#define REQUIRE_RCU_READ REQUIRE_READ(rcu_read)

```

`hyp/interfaces/rcu/rcu.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

interface rcu

selector_event rcu_update
	selector rcu_update_class: rcu_update_class_t
	param entry: rcu_entry_t *
	return: rcu_update_status_t = rcu_update_status_default()

event rcu_read_start

event rcu_read_finish

event rcu_grace_period_end
	param cpu_index: cpu_index_t

event rcu_grace_period_complete
	param cpu_index: cpu_index_t
	param status: rcu_update_status_t

```

`hyp/interfaces/rcu/rcu.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

define rcu_read global structure opaque_lock;

define rcu_update_class enumeration {
};

define rcu_entry structure {
};

define rcu_update_status bitfield<32> {
	auto need_schedule bool;
};

```

`hyp/interfaces/root_env/build.conf`:

```conf
# © 2023 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

types root_env.tc

```

`hyp/interfaces/root_env/root_env.tc`:

```tc
// © 2023 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

define root_env_mmio_range_properties public bitfield<64> {
	31:0	num_pages	uint32;	// Number of 4K pages
	34:32	access		enumeration pgtable_access; // Access permissions i.e., R, RW, RWX etc.,
	47:40	res_s2pt_attr	uint8;	// Reserved for S2 page attributes
	63	non_exclusive	bool;	// If true, this mmio range may overlap another mmio range descriptor
	others	unknown=0;
};

define root_env_mmio_range_descriptor public structure {
	address	type paddr_t;
	attrs	bitfield root_env_mmio_range_properties;
};

```

`hyp/interfaces/scheduler/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

types scheduler.tc
events scheduler.ev
hypercalls hypercalls.hvc

```

`hyp/interfaces/scheduler/hypercalls.hvc`:

```hvc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

define scheduler_yield hypercall {
	call_num	0x3b;
	control		input bitfield scheduler_yield_control;
	arg1		input uregister;
	res0		input uregister;
	error		output enumeration error;
};

```

`hyp/interfaces/scheduler/include/scheduler.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// Run the scheduler and possibly switch to a different thread, with a hint
// that the current thread should continue running if it is permitted to do so
// (i.e. nothing is higher priority).
//
// The caller must not be holding any spinlocks (including the scheduler lock
// for any thread) and must not be in an RCU read-side critical section.
//
// Returns true if the scheduler switched threads (and has switched back).
bool
scheduler_schedule(void);

// Trigger a scheduler run to occur once it is safe.
//
// This is intended to be used instead of scheduler_schedule() when a thread is
// unblocked in a context that cannot easily run the scheduler, such as during a
// context switch.
//
// The scheduler run is not guaranteed to until the next return to userspace or
// the idle thread.
void
scheduler_trigger(void) REQUIRE_PREEMPT_DISABLED;

// Run the scheduler and possibly switch to a different thread, with a hint
// that the current thread wants to yield the CPU even if it is still
// permitted to run.
//
// The caller must not be holding any spinlocks (including the scheduler lock
// for any thread) and must not be in an RCU read-side critical section.
void
scheduler_yield(void);

// Run the scheduler and possibly switch to a different thread, with a hint
// that the current thread wants to donate its current time allocation and
// priority to the specified thread.
//
// The caller must not be holding any spinlocks (including the scheduler lock
// for any thread) and must not be in an RCU read-side critical section. The
// caller must hold a reference to the specified thread, and must not be the
// specified thread.
//
// Note that this is not guaranteed to switch to the specified thread, which
// may be blocked, only runnable on a remote CPU, or of lower priority than
// another runnable thread even after priority donation:
void
scheduler_yield_to(thread_t *target);

// Lock a thread's scheduler state.
//
// Calling this function acquires a spinlock that protects the specified thread
// from concurrent changes to its scheduling state. Calls to this function must
// be balanced by calls to scheduler_unlock().
//
// A caller must not attempt to acquire scheduling locks for multiple threads
// concurrently.
void
scheduler_lock(thread_t *thread) ACQUIRE_SCHEDULER_LOCK(thread);

// Lock a thread's scheduler state, when preemption is known to be disabled.
void
scheduler_lock_nopreempt(thread_t *thread) ACQUIRE_SCHEDULER_LOCK_NP(thread);

// Unlock a thread's scheduler state.
//
// Calling this function releases the spinlock that was acquired by calling
// scheduler_lock(). Calls to this function must exactly balance calls to
// scheduler_lock().
void
scheduler_unlock(thread_t *thread) RELEASE_SCHEDULER_LOCK(thread);

// Unlock a thread's scheduler state, without enabling preemption.
void
scheduler_unlock_nopreempt(thread_t *thread) RELEASE_SCHEDULER_LOCK_NP(thread);

// Block a thread for a specified reason.
//
// Calling this function prevents the specified thread being chosen by the
// scheduler, until scheduler_unblock() is called on the thread for the same
// reason. Multiple blocks with the same reason do not nest, and can be
// cleared by a single unblock.
//
// The caller must either be the specified thread, or hold a reference to the
// specified thread, or be in an RCU read-side critical section. The caller must
// also hold the scheduling lock for the thread (see scheduler_lock()).
//
// Calling this function on the current thread does not immediately switch
// contexts; the caller must subsequently call scheduler_schedule() or
// scheduler_yield*() in that case (not scheduler_trigger()!). If this is done
// with preemption enabled, any subsequent preemption will not return until the
// thread has been unblocked by another thread, and any call to
// scheduler_yield*() may not occur until after that unblock, so it is
// preferable to call scheduler_schedule() or disable preemption first.
//
// Calling this function on a thread that is currently running on a remote CPU
// will not immediately interrupt that thread. Call scheduler_sync(thread) if
// it is necessary to wait until the target thread is not running.
void
scheduler_block(thread_t *thread, scheduler_block_t block)
	REQUIRE_SCHEDULER_LOCK(thread);

// Block a thread for a specified reason during creation.
//
// This function has the same functionality as scheduler_block(), but the caller
// is not required to hold the scheduling lock for the thread. However, this
// function can only be used by object_create_thread handlers on a newly created
// thread.
void
scheduler_block_init(thread_t *thread, scheduler_block_t block);

// Remove a block reason from a thread, possibly allowing it to run.
//
// The caller must either be the specified thread, or hold a reference to the
// specified thread, or be in an RCU read-side critical section. The caller must
// also hold the scheduling lock for the thread (see scheduler_lock());
//
// Calling this function on a thread that is immediately runnable on some CPU
// does not necessarily cause it to actually run. If this function returns true,
// the caller should call scheduler_schedule(), scheduler_trigger() or
// scheduler_yield*() afterwards to avoid delaying execution of the unblocked
// thread.
//
// Returns true if a scheduler run is needed as a consequence of this call.
bool
scheduler_unblock(thread_t *thread, scheduler_block_t block)
	REQUIRE_SCHEDULER_LOCK(thread);

// Return true if a thread is blocked for a specified reason.
//
// The caller must either be the specified thread, or hold a reference to the
// specified thread, or be in an RCU read-side critical section.
//
// Note that this function is inherently racy: if the specified thread might
// be blocked or unblocked with the specified reason by a third party, then it
// may return an incorrect value. It is the caller's responsibility to
// guarantee that such races do not occur, typically by calling
// scheduler_lock().
bool
scheduler_is_blocked(const thread_t *thread, scheduler_block_t block);

// Return true if a thread is available for scheduling.
//
// The caller must either be the specified thread, or hold a reference to the
// specified thread, or be in an RCU read-side critical section. The caller must
// also hold the scheduling lock for the thread (see scheduler_lock()).
//
// This function may ignore some block flags if the thread has been killed.
bool
scheduler_is_runnable(const thread_t *thread) REQUIRE_SCHEDULER_LOCK(thread);

// Return true if a thread is currently scheduled and running.
//
// The caller must either be the specified thread, or hold a reference to the
// specified thread, or be in an RCU read-side critical section. The caller must
// also hold the scheduling lock for the thread (see scheduler_lock()).
bool
scheduler_is_running(const thread_t *thread) REQUIRE_SCHEDULER_LOCK(thread);

// Wait until a specified thread is not running.
//
// The caller must not be holding any spinlocks and must not be an RCU
// read-side critical section. The caller must hold a reference to the
// specified thread.
//
// If the specified thread is not blocked, or may be unblocked by some other
// thread, this function may block indefinitely. There is no guarantee that
// the thread will not be running when this function returns; there is only a
// guarantee that the thread was not running at some time after the function
// was called.
//
// This function implies an acquire barrier that synchronises with a release
// barrier performed by the specified thread when it stopped running.
void
scheduler_sync(thread_t *thread);

// Pin a thread to its current physical CPU, preventing it from migrating to
// other physicals CPUs.
//
// The caller must either be the specified thread, or hold a reference to the
// specified thread, or be in an RCU read-side critical section. The caller must
// also hold the scheduling lock for the thread (see scheduler_lock()).
//
// Multiple calls to this function nest; the same number of calls to
// scheduler_unpin() are required before a thread becomes migratable again.
//
// This function is a no-op for schedulers that do not support migration.
void
scheduler_pin(thread_t *thread) REQUIRE_SCHEDULER_LOCK(thread);

// Unpin a thread from its current physical CPU.
//
// The caller must either be the specified thread, or hold a reference to the
// specified thread, or be in an RCU read-side critical section. The caller must
// also hold the scheduling lock for the thread (see scheduler_lock()).
//
// This function is a no-op for schedulers that do not support migration.
void
scheduler_unpin(thread_t *thread) REQUIRE_SCHEDULER_LOCK(thread);

// Get the primary VCPU for a specific physical CPU.
//
// This functions returns a pointer to a VCPU on the specified physical CPU that
// belongs to the primary HLOS VM, which is responsible for interrupt handling
// and hosts the primary scheduler. This is only defined in configurations that
// defer most decisions and all interrupt handling to the primary HLOS.
//
// This function does not take a reference to the returned thread, so it must be
// called from an RCU read-side critical section.
thread_t *
scheduler_get_primary_vcpu(cpu_index_t cpu) REQUIRE_RCU_READ;

// Returns the configured affinity of a thread.
//
// The caller must either be the specified thread, or hold a reference to the
// specified thread, or be in an RCU read-side critical section. The caller must
// also hold the scheduling lock for the thread (see scheduler_lock()).
//
// In most cases it is not correct for a thread to call this function on itself,
// because the result is the CPU that the scheduler _wants_ to schedule the
// thread on, not the CPU it _has_ scheduled the thread on. If the current
// thread wants to know which CPU it is running on, it can use
// cpulocal_get_index(); for threads that may be running remotely
// scheduler_get_active_affinity() should be used instead.
cpu_index_t
scheduler_get_affinity(thread_t *thread) REQUIRE_SCHEDULER_LOCK(thread);

// Returns the active affinity of a thread.
//
// The caller must either be the specified thread, or hold a reference to the
// specified thread, or be in an RCU read-side critical section. The caller must
// also hold the scheduling lock for the thread (see scheduler_lock()).
//
// This function can be used to determine which CPU a thread is actively running
// on, which may not reflect the configured affinity if it has been recently
// changed. If the thread is not currently running, this function will return
// the same result as scheduler_get_affinity().
cpu_index_t
scheduler_get_active_affinity(thread_t *thread) REQUIRE_SCHEDULER_LOCK(thread);

// Set the affinity of a thread.
//
// The caller must either be the specified thread, or hold a reference to the
// specified thread, or be in an RCU read-side critical section. The caller must
// also hold the scheduling lock for the thread (see scheduler_lock()).
//
// For schedulers that do not support migration, this function must only be
// called for threads that have not yet been activated. Threads that are pinned
// to a CPU cannot have their affinity changed.
error_t
scheduler_set_affinity(thread_t *thread, cpu_index_t target_cpu)
	REQUIRE_SCHEDULER_LOCK(thread);

error_t
scheduler_set_priority(thread_t *thread, priority_t priority)
	REQUIRE_SCHEDULER_LOCK(thread);

error_t
scheduler_set_timeslice(thread_t *thread, nanoseconds_t timeslice)
	REQUIRE_SCHEDULER_LOCK(thread);

// Returns true if the specified thread has sufficient priority to immediately
// preempt the currently running thread.
//
// This function assumes that the specified thread is able to run on the calling
// CPU, regardless of the current block flags, affinity, timeslice, etc.
//
// The scheduler lock for the specified thread must be held, and it is assumed
// not to be the current thread.
bool
scheduler_will_preempt_current(thread_t *thread) REQUIRE_SCHEDULER_LOCK(thread);

```

`hyp/interfaces/scheduler/scheduler.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

interface scheduler

// All of the events below are called with preemption disabled, and with no
// locks held other than the scheduler's internal locks, unless stated
// otherwise.

// This event is triggered during scheduler init for each scheduler block
// type, so that special properties for each block flag can be set.
selector_event scheduler_get_block_properties
	selector block: scheduler_block_t
	return: scheduler_block_properties_t = scheduler_block_properties_default()

// Stop the kernel.
// Used when triggering a system crash. This should stop user thread execution
// on all cores and disable preemption.
event scheduler_stop

// Triggered after the scheduler runs and elects not to switch threads.
event scheduler_quiescent

// Triggered after the scheduler selects the next thread to run, which may or
// may not already be the current thread. If the current thread is selected,
// this event is triggered before scheduler_quiescent.
//
// The can_idle flag is initialised to true if the specified thread can
// safely idle without running the scheduler; i.e. it is either the idle
// thread, or it is the only runnable non-idle thread on the CPU. If a module
// needs to prevent a thread idling without scheduling, it should zero the
// can_idle flag in a priority>0 handler.
//
// The scheduler lock must not be held when this event is triggered, but the
// caller must either hold an explicit reference to the thread or be in an RCU
// critical section.
event scheduler_selected_thread
	param thread: thread_t *
	param can_idle: bool *

// Prepare to change a thread's affinity.
//
// This event is for cases where we may want to deny certain affinity changes,
// e.g. the new CPU doesn't support a feature required by the affected VCPU.
//
// This event is triggered prior to the scheduler_affinity_changed event. The
// targeted thread has not yet been blocked and may be running either locally
// or remotely. The thread's scheduler lock is held.
//
// If this event returns an error then the affinity change is aborted.
setup_event scheduler_set_affinity_prepare
	param thread: thread_t *
	param prev_cpu: cpu_index_t
	param next_cpu: cpu_index_t
	return: error_t = OK
	success: OK

// Change a thread's affinity.
//
// This event is triggered after a thread is blocked to change affinity. The
// scheduler lock for the thread is held by the caller, and the thread is not
// current on any CPU (though it may be the previous thread in a
// thread_context_switch_post event).
//
// If it is necessary to take actions with no lock held, the need_sync
// parameter can be set to true to trigger a scheduler_affinity_changed_sync
// event after after a grace period.
//
// This event must not fail. If a module needs to prevent affinity changes in
// some cases, it must be checked in a scheduler_set_affinity_prepare handler,
// which will be called prior to this event. Note that the scheduler lock may
// be dropped between these events.
event scheduler_affinity_changed
	param thread: thread_t *
	param prev_cpu: cpu_index_t
	param next_cpu: cpu_index_t
	param need_sync: bool *

// Clean up after changing a thread's affinity.
//
// This event is triggered after the affinity of a thread is explicitly
// changed, if requested by a handler for the scheduler_affinity_changed
// event. It is triggered after a grace period, with no locks held.
//
// Modules that handle this event must not assume that they were responsible
// for triggering it; a different module may have triggered the event.
event scheduler_affinity_changed_sync
	param thread: thread_t *
	param prev_cpu: cpu_index_t
	param next_cpu: cpu_index_t

// This event is triggered just before the scheduler schedules the next thread.
// "yielded_from" is the value of CPULOCAL(yielded_from) for this CPU,
// "schedtime" is the start time of the previous scheduler run, and "curticks"
// is the start time of this scheduler run.
event scheduler_schedule
	param current: thread_t *
	param yielded_from: thread_t *
	param schedtime: ticks_t
	param curticks: ticks_t

// This event is triggered before the scheduler sets a given block flag on a
// thread, if that flag is not already set.
//
// The was_runnable argument is true if the thread was not blocked for any
// reason prior to setting the block flag (after excluding killable block
// flags, if the thread has been killed).
//
// The scheduler lock for the thread is held by the caller.
event scheduler_blocked
	param thread: thread_t *
	param block: scheduler_block_t
	param was_runnable: bool

// This event is triggered after the scheduler clears a given block flag on a
// thread, if that flag was not already clear.
//
// The now_runnable argument is true if the thread will not be blocked for any
// reason after to this event (after excluding killable block flags, if the
// thread has been killed).
//
// The scheduler lock for the thread is held by the caller.
event scheduler_unblocked
	param thread: thread_t *
	param block: scheduler_block_t
	param now_runnable: bool

```

`hyp/interfaces/scheduler/scheduler.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <types/bitmap.h>

define priority_t public newtype uint32;

define scheduler_block enumeration {
};

define scheduler_block_properties bitfield<32> {
	// Scheduler must respect block flag even
	// if the thread has been killed.
	auto	non_killable	bool;
};

define scheduler_variant public enumeration(explicit) {
	trivial = 0x0;
	fprr = 0x1;
};

extend thread object module scheduler {
	affinity type cpu_index_t;
};

extend thread_create structure module scheduler {
	affinity type cpu_index_t;
	affinity_valid bool;
};

extend cap_rights_thread bitfield {
	4	yield_to	bool;
};

define scheduler_yield_control public bitfield<32> {
	15:0		hint		enumeration scheduler_yield_hint;
	30:16		unknown=0;
	// Implementation defined scheduler hints set impl_def
	31		impl_def	bool;
};

// Standard yield hints
define scheduler_yield_hint public enumeration(explicit) {
	yield		= 0x0;		// generic yield
	yield_to_thread	= 0x1;		// yield to target thread
	yield_lower	= 0x2;		// yield to lower priority
};

```

`hyp/interfaces/slat/aarch64/slat.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// Second Level Address Translation (SLAT) Base Module

define vmaddr_t public newtype uint64;

define VMADDR_INVALID constant type vmaddr_t = -1;

```

`hyp/interfaces/slat/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

arch_types aarch64 slat.tc

```

`hyp/interfaces/smc_trace/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

types smc_trace.tc

```

`hyp/interfaces/smc_trace/include/smc_trace.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// SMC Trace interface
//
// SMC Trace logs SMC calls and returns for SMC calls, whether called by a user
// or internally in the hypervisor.

#define SMC_TRACE_CURRENT(id, num)                                             \
	static_assert(num <= SMC_TRACE_REG_MAX, "num out of range");           \
	smc_trace_log(id,                                                      \
		      (register_t(*)[SMC_TRACE_REG_MAX]) &                     \
			      thread_get_self()->vcpu_regs_gpr.x[0],           \
		      num)

void
smc_trace_init(partition_t *partition);

void
smc_trace_log(smc_trace_id_t id, register_t (*registers)[SMC_TRACE_REG_MAX],
	      count_t	     num_registers);

```

`hyp/interfaces/smc_trace/smc_trace.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

define smc_trace_id enumeration {
	INVALID		= 0x0;
	EL1_64ENT	= 0x10;		// SMC entry from EL1
	EL1_64RET	= 0x11;		// SMC return to EL1
	EL2_64CAL	= 0x20;		// SMC call from EL2 to TZ
	EL2_64RET	= 0x21;		// SMC return to EL2
	EL2_64INT	= 0x25;		// SMC interrupted return from TZ
	EL2_64RES	= 0x24;		// SMC resume to TZ
};

```

`hyp/interfaces/smccc/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

types smccc.tc
events smccc.ev

```

`hyp/interfaces/smccc/include/smccc.ev.h`:

```h
// © 2022 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#define SMCCC_ARCH_FUNCTION_32(fn, feat, h, ...)                               \
	subscribe smccc_call_fast_32_arch[(                                    \
		smccc_function_t)SMCCC_ARCH_FUNCTION_##fn];                    \
	handler	  smccc_##h(__VA_ARGS__);                                      \
	exclude_preempt_disabled.subscribe                                     \
		 smccc_arch_features_fast32[SMCCC_ARCH_FUNCTION_##fn];         \
	constant feat.

#define SMCCC_ARCH_FUNCTION_64(fn, feat, h, ...)                               \
	subscribe smccc_call_fast_64_arch[(                                    \
		smccc_function_t)SMCCC_ARCH_FUNCTION_##fn];                    \
	handler	  smccc_##h(__VA_ARGS__);                                      \
	exclude_preempt_disabled.subscribe                                     \
		 smccc_arch_features_fast64[SMCCC_ARCH_FUNCTION_##fn];         \
	constant feat.

#define SMCCC_STANDARD_HYP_FUNCTION_32(fn, feat, h, ...)                                     \
	subscribe			   smccc_call_fast_32_standard_hyp[(                 \
		 smccc_function_t)SMCCC_STANDARD_HYP_FUNCTION_##fn]; \
	handler				   smccc_##h(__VA_ARGS__);                           \
	exclude_preempt_disabled.subscribe smccc_standard_hyp_features_fast32                \
		[SMCCC_STANDARD_HYP_FUNCTION_##fn];                                          \
	constant feat.

#define SMCCC_STANDARD_HYP_FUNCTION_64(fn, feat, h, ...)                                     \
	subscribe			   smccc_call_fast_64_standard_hyp[(                 \
		 smccc_function_t)SMCCC_STANDARD_HYP_FUNCTION_##fn]; \
	handler				   smccc_##h(__VA_ARGS__);                           \
	exclude_preempt_disabled.subscribe smccc_standard_hyp_features_fast64                \
		[SMCCC_STANDARD_HYP_FUNCTION_##fn];                                          \
	constant feat.

```

`hyp/interfaces/smccc/include/smccc.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

void
smccc_1_1_call(smccc_function_id_t fn_id, uint64_t (*args)[6],
	       uint64_t (*ret)[4], uint64_t *session_ret, uint32_t client_id);

```

`hyp/interfaces/smccc/include/smccc_platform.h`:

```h
// © 2022 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

bool
smccc_handle_smc_platform_call(register_t args[7], bool is_hvc);

```

`hyp/interfaces/smccc/smccc.ev`:

```ev
// © 2022 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

interface smccc

#define SMCCC_DISPATCH(type, size)				\
selector_event smccc_dispatch_ ## type ## _ ## size;		\
	selector owner_id: smccc_owner_id_t;		\
	param function: smccc_function_t;			\
	param is_hvc: bool;					\
	param arg1: uint ## size ## _t;				\
	param arg2: uint ## size ## _t;				\
	param arg3: uint ## size ## _t;				\
	param arg4: uint ## size ## _t;				\
	param arg5: uint ## size ## _t;				\
	param arg6: uint ## size ## _t;				\
	param client_id: smccc_client_id_t;			\
	param ret0: uint ## size ## _t *;			\
	param ret1: uint ## size ## _t *;			\
	param ret2: uint ## size ## _t *;			\
	param ret3: uint ## size ## _t *.

SMCCC_DISPATCH(fast, 32)
SMCCC_DISPATCH(fast, 64)
SMCCC_DISPATCH(yielding, 32)
SMCCC_DISPATCH(yielding, 64)

#define _SMCCC_CALL_OWNER(type, size, owner)	\
selector_event smccc_call_ ## type ## _ ## size ## _ ## owner;	\
	selector function: smccc_function_t;			\
	param is_hvc: bool;					\
	param arg1: uint ## size ## _t;				\
	param arg2: uint ## size ## _t;				\
	param arg3: uint ## size ## _t;				\
	param arg4: uint ## size ## _t;				\
	param arg5: uint ## size ## _t;				\
	param arg6: uint ## size ## _t;				\
	param client_id: smccc_client_id_t;			\
	param ret0: uint ## size ## _t *;			\
	param ret1: uint ## size ## _t *;			\
	param ret2: uint ## size ## _t *;			\
	param ret3: uint ## size ## _t *.

#define SMCCC_CALL_OWNER(owner) \
	_SMCCC_CALL_OWNER(fast, 32, owner) \
	_SMCCC_CALL_OWNER(fast, 64, owner) \
	_SMCCC_CALL_OWNER(yielding, 32, owner) \
	_SMCCC_CALL_OWNER(yielding, 64, owner)

SMCCC_CALL_OWNER(arch)
SMCCC_CALL_OWNER(cpu)
SMCCC_CALL_OWNER(sip)
SMCCC_CALL_OWNER(oem)
SMCCC_CALL_OWNER(standard)
SMCCC_CALL_OWNER(standard_hyp)
SMCCC_CALL_OWNER(vendor_hyp)

selector_event smccc_arch_features_fast32
	selector function: smccc_arch_function_t
	return: uint32_t = SMCCC_UNKNOWN_FUNCTION32
selector_event smccc_arch_features_fast64
	selector function: smccc_arch_function_t
	return: uint32_t = SMCCC_UNKNOWN_FUNCTION32

selector_event smccc_standard_hyp_features_fast32
	selector function: smccc_standard_hyp_function_t
	return: uint32_t = SMCCC_UNKNOWN_FUNCTION32
selector_event smccc_standard_hyp_features_fast64
	selector function: smccc_standard_hyp_function_t
	return: uint32_t = SMCCC_UNKNOWN_FUNCTION32

```

`hyp/interfaces/smccc/smccc.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// Common definitions used by platform SMC calls made by the hypervisor, and
// virtual SMC calls handled by the hypervisor.

// SMCCC 1.3 implemented.
//
// Note that the new features in v1.3 relative to v1.1 are all optional
// (extra arg / return registers, the SMCCC_ARCH_SOC_ID function, and
// the sve_not_live hint bit in the function ID).
define SMCCC_VERSION public constant uint32 = 0x10003;

define smccc_owner_id public enumeration(explicit) {
	ARCH = 0;
	CPU = 1;
	SIP = 2;
	OEM = 3;
	STANDARD = 4;
	STANDARD_HYP = 5;
	VENDOR_HYP = 6;
};

define smccc_function_t public newtype uint16;

define smccc_function_id public bitfield<32> {
	15:0	function type smccc_function_t;
	16	sve_live_state_hint bool;	// from SMCCC v1.3+
	23:17	res0 uint32(const);
	29:24	owner_id enumeration smccc_owner_id;
	30	is_smc64 bool;
	31	is_fast bool;
};

define smccc_vendor_hyp_function_class public enumeration(explicit) {
	PLATFORM_CALL	= 0b00;
	HYPERCALL	= 0b10;
	SERVICE		= 0b11;
};

define smccc_vendor_hyp_function_id public bitfield<16> {
	15:14	call_class	enumeration smccc_vendor_hyp_function_class;
	13:0	function	uint16;
};

define smccc_client_id bitfield<32> {
	15:0	client_id uint16;
	31:16	secure_os_id uint16;
};

define SMCCC_UNKNOWN_FUNCTION64 public constant uint64 = -1;
define SMCCC_UNKNOWN_FUNCTION32 public constant uint32 = -1;

define smccc_arch_function public enumeration(explicit) {
	VERSION = 0;
	ARCH_FEATURES = 1;
	ARCH_SOC_ID = 2;
	ARCH_WORKAROUND_2 = 0x7fff;
	ARCH_WORKAROUND_1 = 0x8000;
};

define smccc_standard_hyp_function public enumeration(explicit) {
	CALL_COUNT = 0xff00;
	CALL_UID = 0xff01;
	REVISION = 0xff03;
};

// Gunyah SMCCC UUID: c1d58fcd-a453-5fdb-9265-ce36673d5f14
define SMCCC_GUNYAH_UID0 public constant uregister = 0xcd8fd5c1U;
define SMCCC_GUNYAH_UID1 public constant uregister = 0xdb5f53a4U;
define SMCCC_GUNYAH_UID2 public constant uregister = 0x36ce6592U;
define SMCCC_GUNYAH_UID3 public constant uregister = 0x145f3d67U;

// function id bits 13:0
define smccc_vendor_hyp_function public enumeration(explicit) {
	CALL_COUNT = 0x3f00;
	CALL_UID = 0x3f01;
	REVISION = 0x3f03;
};

```

`hyp/interfaces/spinlock/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

events spinlock.ev

```

`hyp/interfaces/spinlock/include/spinlock.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// Initialise a spinlock structure.
//
// This must be called exactly once for each lock, before any of the functions
// below are called.
void
spinlock_init(spinlock_t *lock);

// Acquire exclusive ownership of a lock, spinning indefinitely until it is
// acquired. Preemption will be disabled.
void
spinlock_acquire(spinlock_t *lock) ACQUIRE_SPINLOCK(lock);

// Attempt to immediately acquire exclusive ownership of a lock, and return true
// if it succeeds. If the lock is already exclusively held by another thread,
// return false with no side-effects.
//
// Preemption will be disabled if the lock is acquired.
bool
spinlock_trylock(spinlock_t *lock) TRY_ACQUIRE_SPINLOCK(true, lock);

// Release exclusive ownership of a lock. Preemption will be enabled.
void
spinlock_release(spinlock_t *lock) RELEASE_SPINLOCK(lock);

// As for spinlock_acquire(), but preemption must already be disabled and will
// not be disabled again.
void
spinlock_acquire_nopreempt(spinlock_t *lock) ACQUIRE_SPINLOCK_NP(lock);

// As for spinlock_trylock(), but preemption must already be disabled and will
// not be disabled again.
bool
spinlock_trylock_nopreempt(spinlock_t *lock)
	TRY_ACQUIRE_SPINLOCK_NP(true, lock);

// As for spinlock_release(), but preemption will not be enabled.
void
spinlock_release_nopreempt(spinlock_t *lock) RELEASE_SPINLOCK_NP(lock);

// Assert that a specific spinlock is exclusively held by the caller.
//
// This might only be a static check, especially in non-debug builds.
void
assert_spinlock_held(const spinlock_t *lock) REQUIRE_LOCK(lock)
	REQUIRE_PREEMPT_DISABLED;

```

`hyp/interfaces/spinlock/spinlock.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

interface spinlock

include <preempt.h>

// Spinlock events.
//
// Note that there are separate events for before and after the various spinlock
// operations.

event spinlock_init
	param lock: spinlock_t *

event spinlock_acquire
	param lock: spinlock_t *

event spinlock_acquired
	param lock: spinlock_t *

event spinlock_failed
	param lock: spinlock_t *

event spinlock_release
	param lock: spinlock_t *

event spinlock_released
	param lock: spinlock_t *

event spinlock_assert_held
	param lock: const spinlock_t *

```

`hyp/interfaces/task_queue/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

events task_queue.ev
types task_queue.tc

```

`hyp/interfaces/task_queue/include/task_queue.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// Configure a task queue entry with a specific class. This class should
// identify the container type and element of the entry.
void
task_queue_init(task_queue_entry_t *entry, task_queue_class_t task_class);

// Schedule future execution of a given task queue entry.
//
// All calls to this function and to task_queue_cancel() for the same entry must
// be serialised by the caller.
//
// The caller also must ensure that the entry is not freed until the task has
// executed. This can be done in either of the following ways:
//
// 1. Take a reference to the object containing the entry before calling this
//    function, and release it in the task_queue_execute handler or if this
//    function fails.
//
// 2. Call task_queue_cancel() in the containing object's deactivation handler,
//    and ensure that the task execution handler can tolerate the object being
//    concurrently deactivated.
//
// In implementations that share task queues between CPUs or allow cross-CPU
// execution of tasks, this call implies a release memory barrier that matches
// an acquire memory barrier before the task_queue_execute handler starts.
//
// If the task was already queued, this function returns ERROR_BUSY. Otherwise,
// the task_queue_execute handler will be executed once per successful call.
error_t
task_queue_schedule(task_queue_entry_t *entry);

// Cancel future execution of a given task queue entry.
//
// All calls to this function and to task_queue_schedule() for the same entry
// must be serialised by the caller.
//
// This function does not cancel execution if it has already started, and does
// not wait for execution to complete. Any execution that has already started is
// guaranteed to be complete after an RCU grace period has elapsed. Also, the
// entry may not be safely freed until an RCU grace period has elapsed.
//
// If the task was not queued, or had already started, this function returns
// ERROR_IDLE.
error_t
task_queue_cancel(task_queue_entry_t *entry);

```

`hyp/interfaces/task_queue/task_queue.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

interface task_queue

selector_event task_queue_execute
	selector task_queue_class: task_queue_class_t
	param entry: task_queue_entry_t *
	return: error_t = ERROR_UNIMPLEMENTED

```

`hyp/interfaces/task_queue/task_queue.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

define task_queue_entry structure {
};

define task_queue_class enumeration {
};

```

`hyp/interfaces/tests/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

events tests.ev
types  tests.tc

```

`hyp/interfaces/tests/tests.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

interface tests

event tests_init

handled_event tests_start

#if defined(INTERFACE_TESTS)
selector_event tests_run
	selector test_id: tests_run_id_t
	param arg0 : uint64_t
	param arg1 : uint64_t
	param arg2 : uint64_t
	param arg3 : uint64_t
	param arg4 : uint64_t
	param arg5 : uint64_t
	return: error_t = ERROR_UNIMPLEMENTED
#endif

```

`hyp/interfaces/tests/tests.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

define tests_run_id enumeration {
};

```

`hyp/interfaces/thread/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

types thread.tc
first_class_object thread
events thread.ev

```

`hyp/interfaces/thread/include/thread.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// A function pointer that is called to start a thread.
//
// This is the return type of the thread_get_entry_fn event.
//
// The type system does not support function pointers, but this type is never
// stored persistently, so we can declare it by hand.
typedef void (*thread_func_t)(uintptr_t param);

// Return the caller's thread structure.
thread_t *
thread_get_self(void);

// Switch immediately to the specified thread.
//
// The caller must be running with preemption disabled and must not be the
// specified thread. The caller must take an additional reference to the
// specified thread prior to the call, which will be released when the thread
// stops running. If the switch fails, the reference will be released
// immediately before returning.
//
// The second argument is the absolute time at which the scheduler made the
// decision to run this thread. It is not used directly by this module, but is
// passed to context switch event handlers for use in time accounting.
//
// This function will fail if the specified thread is already running on another
// CPU. The scheduler is responsible for guaranteeing this.
error_t
thread_switch_to(thread_t *thread, ticks_t curticks) REQUIRE_PREEMPT_DISABLED;

// Kill a thread. This marks it as exiting, sends an interrupt to any CPU that
// is currently running it, and switches to it on the current CPU if it is not
// currently running. The switch is performed regardless of the thread's
// scheduling state, including block state, CPU affinity, priority or
// timeslice length.
//
// The thread is not guaranteed to have exited when this function returns. If
// such a guarantee is required, call thread_join() on it afterwards.
//
// The caller must either be the specified thread, or hold a reference to the
// specified thread.
error_t
thread_kill(thread_t *thread);

// Return true if the specified thread has had thread_kill() called on it.
//
// This function has relaxed memory semantics. If the thread may be running on a
// remote CPU, or may have been killed by a remote CPU, it is the caller's
// responsibility to ensure that the memory access is ordered correctly.
//
// The caller must either be the specified thread, or hold a reference to the
// specified thread, or be in an RCU read-side critical section.
bool
thread_is_dying(const thread_t *thread);

// Return true if the specified thread has exited.
//
// This function has relaxed memory semantics. If the thread may be running on a
// remote CPU, or may have just exited on a remote CPU, it is the caller's
// responsibility to ensure that the memory access is ordered correctly.
//
// The caller must either hold a reference to the specified thread, or be in an
// RCU read-side critical section.
bool
thread_has_exited(const thread_t *thread);

// Block until a specified thread has exited.
//
// The caller must not be the specified thread, and must hold a reference to
// the specified thread. Also, the caller is responsible for preventing
// deadlocks in case the specified thread is in an uninterruptible wait.
void
thread_join(thread_t *thread);

// Block until a specified thread has exited or the caller is killed.
//
// The caller must not be the specified thread, and must hold a reference to
// the specified thread. The specified thread should have been killed; if it
// has not, this function may block indefinitely.
//
// Returns true if the specified thread has exited, or false if the caller was
// killed while waiting for it.
//
// Note that if this returns false, the caller has preemption disabled and may
// be running while blocked or on a CPU it does not have affinity to.
bool
thread_join_killable(thread_t *thread);

// Prepare to call a function that may power off the CPU.
//
// This function saves any parts of the register state that are needed to resume
// the calling thread, and then calls the specified function. If that function
// returns, its result will be passed through to the caller. If it does not
// return, then when the calling thread is restarted (typically by the warm boot
// path), this function will return the resumed_result argument to the caller.
//
// There is no opportunity to add the calling thread to a scheduler queue, so
// this should only be called by a thread that has strict affinity to the CPU
// that is potentially powering off. This will typically be an idle thread.
register_t
thread_freeze(register_t (*fn)(register_t), register_t param,
	      register_t resumed_result) REQUIRE_PREEMPT_DISABLED;

// Reset the current thread's stack.
//
// This can be called to discard the current stack before calling a function
// that does not return. If the specified function does return, it will panic.
//
// Care should be taken to call this only when the return stack does not need to
// be unwound to clean anything up: no locks held, no stack variables on global
// queues, etc.
noreturn void
thread_reset_stack(void (*fn)(register_t), register_t param);

// Exit the current thread.
//
// This should be called from any thread that reaches the top of its kernel
// stack if it is either in killed state or has no user context.
noreturn void
thread_exit(void);

```

`hyp/interfaces/thread/include/thread_init.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// Thread functions used only by the boot code.

// Switch to the idle thread at the end of the boot sequence.
noreturn void
thread_boot_set_idle(void) REQUIRE_PREEMPT_DISABLED;

```

`hyp/interfaces/thread/thread.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

interface thread

// needed for thread_func_t
include <thread.h>

// Perform thread-kind-specific setup work.
//
// This event is called on the thread itself before the first thread_load_state,
// to perform thread-specific setup work.
event thread_start

// Save state prior to a context-switch.
//
// This event may be called without a following context_switch event - such as
// for debugging, or if the context-switch is later aborted by an interrupt.
event thread_save_state

// Prepare to switch to the specified thread.
//
// This event is triggered in the context of the pre-switch thread, but
// handlers may safely access the next thread, which has been claimed by the
// scheduler on the current CPU and is known not to be running elsewhere.
//
// The first-priority handler for this event is reserved for the scheduler,
// which must do anything necessary to enforce the above claim if that was
// not done prior to starting the switch. This specifically includes executing
// any memory barriers necessary to ensure that the other handlers can safely
// access the next thread's context.
//
// The first-priority handler is allowed to fail if the scheduler decides it is
// not safe to continue the context switch; all other handlers must return OK.
//
// Note that the next thread may be the idle thread, and/or a new thread that
// has only had object_create_thread triggered on it and has never been
// executed.
setup_event thread_context_switch_pre
	param next: thread_t *
	param curticks: ticks_t
	return: error_t = OK
	success: OK

// Clean up after switching from the specified thread.
//
// This event is triggered in the context of the new thread, but handlers may
// safely access the previous thread, which is still owned by the scheduler on
// the current CPU and is known not to be running elsewhere.
//
// The last-priority handler for this event is reserved for the scheduler,
// which must do anything necessary to release the current CPU's claim on the
// previous thread, including returning it to scheduling queues and executing
// any memory barriers necessary to ensure that operations by other handlers
// are observable to any subsequent context switch to the thread on another
// CPU.
//
// Note that the prev thread may be the idle thread, even if the current
// thread is also the idle thread.
event thread_context_switch_post
	param prev: thread_t *
	param curticks: ticks_t
	param prevticks: ticks_t

// Return the thread's main function.
//
// This is called on the thread itself, immediately before thread_load_state.
// The returned function pointer will be called after thread_load_state; if it
// returns or is NULL, the thread will tail-call thread_exit().
selector_event thread_get_entry_fn
	selector kind: thread_kind_t
	return: thread_func_t = NULL

// Load state after a context-switch.
//
// If the starting argument is true, this is the first time the current thread
// has run.
event thread_load_state
	param starting: bool

// Handle entry from userspace.
//
// This is triggered immediately after entering the hypervisor from a lower
// privilege level. The thread module does not trigger this event itself; it
// must be triggered by any module that extends threads with userspace context
// support.
//
// The last-priority handler is reserved for the preempt module, which can
// enable preemption if appropriate. Other handlers can expect that preemption
// is disabled.
//
// The reason argument indicates the cause of the thread's entry to the
// hypervisor. It must not be THREAD_ENTRY_REASON_NONE.
event thread_entry_from_user
	param reason: thread_entry_reason_t

// Handle exit to userspace.
//
// This is triggered immediately before exiting the hypervisor to a lower
// privilege level. The thread module does not trigger this event itself; it
// must be triggered by any module that extends threads with userspace context
// support.
//
// The first-priority handler is reserved for the preempt module, which must
// disable preemption if it had been enabled upon entry from user. Other
// handlers can expect that preemption is disabled.
//
// The reason argument indicates the reason that the thread had entered the
// hypervisor. If the reason is THREAD_ENTRY_REASON_NONE, then the user
// context is newly initialised, either because the thread itself is new or
// because an exception or hypercall has caused the context to be reset.
event thread_exit_to_user
	param reason: thread_entry_reason_t

// Return the address to map the thread's kernel stack at.
//
// This is called during thread activation. The thread's stack will be mapped
// at the returned address, and used at this location while the thread is
// running. The base address must be aligned to THREAD_STACK_MAP_ALIGN, and
// handlers must ensure there are sufficient guard pages between each mapping.
selector_event thread_get_stack_base
	selector kind: thread_kind_t
	param thread: thread_t *
	return: uintptr_t = 0

// Facilitate a thread's exit after it has been killed.
//
// This event is triggered upon calling thread_kill(). Handlers can use this
// event to ensure that it exits in a timely manner.
event thread_killed
	param thread: thread_t *

// Handle a thread's exit.
//
// This event is triggered for the current thread upon calling thread_exit().
// It is called with preemption disabled.
event thread_exited

```

`hyp/interfaces/thread/thread.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

define THREAD_STACK_MAX_BITS constant type count_t = 15;
define THREAD_STACK_MAX_SIZE constant size = 1 << THREAD_STACK_MAX_BITS;

define THREAD_STACK_MAP_ALIGN_BITS constant type count_t = THREAD_STACK_MAX_BITS + 1;
define THREAD_STACK_MAP_ALIGN constant type count_t = 1 << THREAD_STACK_MAP_ALIGN_BITS;

define thread_kind enumeration {
};

define thread_state enumeration {
	// INIT state is set by zero-initialisation
	init = 0;
	ready;
	killed;
	exited;
};

define thread_entry_reason enumeration {
	none = 0;
	interrupt;
	exception;
	hypercall;
};

extend thread object {
	state enumeration thread_state(atomic);
	kind enumeration thread_kind;
	params uintptr;
	stack_base uintptr;
	stack_size size;
};

extend thread_create structure {
	kind enumeration thread_kind;
	params uintptr;
	stack_size size;
};

```

`hyp/interfaces/timer/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

types timer.tc
events timer.ev

```

`hyp/interfaces/timer/include/timer_queue.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// TODO: Add functions that work on other CPUs' queues. Important for migrating
// schedulers.
// FIXME:

// Initialise a timer object
void
timer_init_object(timer_t *timer, timer_action_t action);

// Returns whether this timer already belongs to a queue
bool
timer_is_queued(timer_t *timer);

// Add a timer object to this CPU's queue with the given absolute timeout
void
timer_enqueue(timer_t *timer, ticks_t timeout);

// Remove a timer object from a timer queue (if queued).
void
timer_dequeue(timer_t *timer);

// Update a timer object with a new absolute timeout. This will add the
// timer to this CPU's queue if not already queued.
void
timer_update(timer_t *timer, ticks_t timeout);

// Return the arch timer frequency
uint32_t
timer_get_timer_frequency(void);

// Return the counter value
ticks_t
timer_get_current_timer_ticks(void);

// Convert counter nanoseconds to ticks
ticks_t
timer_convert_ns_to_ticks(nanoseconds_t ns);

// Convert counter ticks to nanoseconds
nanoseconds_t
timer_convert_ticks_to_ns(ticks_t ticks);

// Get next timeout from cpu local queue
ticks_t
timer_queue_get_next_timeout(void) REQUIRE_PREEMPT_DISABLED;

```

`hyp/interfaces/timer/timer.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

interface timer

// Note, the timer argument passed to the timer_action handler will have been
// dequeued and is not-locked.
selector_event timer_action
	selector action_type: timer_action_t
	param	timer: timer_t *
	return: bool = false

```

`hyp/interfaces/timer/timer.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

define ticks_t public newtype uint64;
define nanoseconds_t public newtype uint64;
define microseconds_t public newtype uint64;
define milliseconds_t public newtype uint64;

define TIMER_NANOSECS_IN_SECOND constant type nanoseconds_t = 1000000000;
define TIMER_MICROSECS_IN_SECOND constant type microseconds_t = 1000000;
define TIMER_MILLISECS_IN_SECOND constant type microseconds_t = 1000;

define timer structure {
};

define timer_action enumeration {
};

```

`hyp/interfaces/trace/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

events trace.ev
types trace.tc
hypercalls trace.hvc

```

`hyp/interfaces/trace/include/trace.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// Trace interface and helper macros
//
// Traces are enabled and disabled by trace class. There up to 64 classes, and
// they are mapped to corresponding bits in a global register_t sized value.
//
// Trace ID is used to identify the trace event. There is no correlation between
// trace-id and trace-class, the caller shall select the class, or classes
// they wish the trace event to be dependent on.
//
// Note, some trace classes may be used internally by implementations, for
// example TRACE_BUFFER or TRACE_ADD classes.

#include <events/trace.h>

#define TRACE_ID(id)	    (TRACE_ID_##id)
#define TRACE_CLASS(tclass) (TRACE_CLASS_##tclass)
#define TRACE_CLASS_BITS(tclass)                                               \
	((register_t)1 << (index_t)TRACE_CLASS_##tclass)

#define TRACE_FUNC_I(id, action, a0, a1, a2, a3, a4, a5, n, ...)               \
	TRACE_ADD##n(TRACE_ID(id), action, a0, a1, a2, a3, a4, a5, __VA_ARGS__)
#define TRACE_FUNC(...)                                                        \
	TRACE_FUNC_I(__VA_ARGS__, 6, 5, 4, 3, 2, 1, 0, _unspecified_id)

extern trace_control_t hyp_trace;
extern register_t      trace_public_class_flags;

#define TRACE_MAYBE(classes, X)                                                \
	do {                                                                   \
		register_t class_enabled_ = atomic_load_explicit(              \
			&hyp_trace.enabled_class_flags, memory_order_relaxed); \
		if (compiler_unexpected((class_enabled_ & classes) != 0U)) {   \
			X;                                                     \
		}                                                              \
	} while (0)

// Used for single class trace
#if defined(PARASOFT_CYCLO)
#define TRACE(tclass, id, ...)
#else
#define TRACE(tclass, id, ...)                                                 \
	TRACE_EVENT(tclass, id, TRACE_ACTION_TRACE, __VA_ARGS__)
#endif
#define TRACE_LOCAL(tclass, id, ...)                                           \
	TRACE_EVENT(tclass, id, TRACE_ACTION_TRACE_LOCAL, __VA_ARGS__)

#define TRACE_EVENT(tclass, id, action, ...)                                   \
	TRACE_MAYBE(TRACE_CLASS_BITS(tclass),                                  \
		    TRACE_FUNC(id, action, __VA_ARGS__))

#define TRACE_ADD0(id, action, ...)                                            \
	trigger_trace_log_event(id, action, 0, 0, 0, 0, 0, 0)

#define TRACE_ADD1(id, action, a1, ...)                                        \
	trigger_trace_log_event(id, action, a1, 0, 0, 0, 0, 0)

#define TRACE_ADD2(id, action, a1, a2, ...)                                    \
	trigger_trace_log_event(id, action, a1, a2, 0, 0, 0, 0)

#define TRACE_ADD3(id, action, a1, a2, a3, ...)                                \
	trigger_trace_log_event(id, action, a1, a2, a3, 0, 0, 0)

#define TRACE_ADD4(id, action, a1, a2, a3, a4, ...)                            \
	trigger_trace_log_event(id, action, a1, a2, a3, a4, 0, 0)

#define TRACE_ADD5(id, action, a1, a2, a3, a4, a5, ...)                        \
	trigger_trace_log_event(id, action, a1, a2, a3, a4, a5, 0)

#define TRACE_ADD6(id, action, a1, a2, a3, a4, a5, a6, ...)                    \
	trigger_trace_log_event(id, action, a1, a2, a3, a4, a5, a6)

// Enable a set of trace classes.
//
// flags: the new flags to be enabled.
void
trace_set_class_flags(register_t flags);

// Disable a set of trace classes.
//
// flags: the flags to be disabled.
void
trace_clear_class_flags(register_t flags);

// Atomically update a set of trace classes.
//
// set_flags: the flags to be enabled.
// clear_flags: the flags to be disabled.
//
// Note: flags both set and cleared will remain set.
void
trace_update_class_flags(register_t set_flags, register_t clear_flags);

// Return the current status of trace classes.
register_t
trace_get_class_flags(void);

// Allocate and relocate trace buffer
//
// It stops using the trace boot buffer and starts using a dynamically allocated
// trace buffer of bigger size
void
trace_init(partition_t *partition, size_t size) REQUIRE_PREEMPT_DISABLED;

#if defined(PLATFORM_TRACE_STANDALONE_REGION)
// Use pre-allocated memory for trace buffer
//
// It stops using the trace boot buffer and starts using a pre-allocated
// trace buffer of bigger size
void
trace_single_region_init(partition_t *partition, paddr_t base, size_t size)
	REQUIRE_PREEMPT_DISABLED;
#endif

```

`hyp/interfaces/trace/include/trace_helpers.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#define TRACE_SET_CLASS(bitmap, trace_class)                                   \
	do {                                                                   \
		(bitmap) |= (register_t)1U                                     \
			    << ((index_t)TRACE_CLASS_##trace_class);           \
	} while (0)

#define TRACE_CLEAR_CLASS(bitmap, trace_class)                                 \
	do {                                                                   \
		(bitmap) &= (~((register_t)1U                                  \
			       << ((index_t)TRACE_CLASS_##trace_class)));      \
	} while (0)

#define TRACE_CLASS_ENABLED(bitmap, trace_class)                               \
	((bitmap) & ((register_t)1U << ((index_t)TRACE_CLASS_##trace_class)))

```

`hyp/interfaces/trace/trace.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

interface trace

event trace_log
	param id:	trace_id_t
	param action:	trace_action_t
	param arg0:	const char *
	param arg1:	register_t
	param arg2:	register_t
	param arg3:	register_t
	param arg4:	register_t
	param arg5:	register_t

```

`hyp/interfaces/trace/trace.hvc`:

```hvc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

define trace_update_class_flags hypercall {
	call_num	0x3f;
	set_flags	input uregister;
	clear_flags	input uregister;
	res0		input uregister;
	error		output enumeration error;
	flags		output uregister;
};

```

`hyp/interfaces/trace/trace.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// Trace class and trace ID definition.
//
// The number of enumerators in trace_class enumeration should be less than 32
// for _core_ classes to support 32-bit architectures.

define trace_class public enumeration(explicit) {
	ERROR = 0;	// Critical errors
	DEBUG = 1;	// Debugging information
	INFO = 6;	// Informative messages
};

define trace_id enumeration(explicit) {
	INFO = 0;
	WARN = 1;
	DEBUG = 3;
};

#if defined(HYPERCALLS)
extend hyp_api_flags0 bitfield {
	delete	trace_ctrl;
	7	trace_ctrl	bool = 1;
};
#endif

```

`hyp/interfaces/util/bitmap.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <types/bitmap.h>

define BITMAP_WORD_BITS constant type index_t = sizeof(type register_t) * 8;

```

`hyp/interfaces/util/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

types bitmap.tc
types list.tc
macros attributes.h

```

`hyp/interfaces/util/include/assert.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// Local version of the standard-defined assert.h

#if !defined(HYP_STANDALONE_TEST)
_Static_assert(__STDC_HOSTED__ == 0,
	       "This file deviates from MISRA rule 21.2 in hosted mode");
#endif

#define static_assert _Static_assert

// Use a Clang extension to assert that an expression is true if its value
// can be statically determined.
static inline _Bool
assert_if_const(_Bool x)
	__attribute__((diagnose_if(!(x), "Static assert failure", "error"),
		       always_inline))
{
	return x;
}

#if defined(__KLOCWORK__)
_Noreturn void
panic(const char *str);
#define assert(x) ((x) ? (void)0 : panic("assertion"))
#elif defined(NDEBUG)
// Strictly this should be defined to ((void)0), but since assert_if_const()
// has no runtime overhead we may as well use it. Also the fact that the
// expression is evaluated means we don't need to put maybe-unused annotations
// on variables that are only used in assert expressions.
#define assert(x) (void)assert_if_const(x)
#else
_Noreturn void
assert_failed(const char *file, int line, const char *func, const char *err);
#define assert(x)                                                              \
	(assert_if_const(x) ? (void)0                                          \
			    : assert_failed(__FILE__, __LINE__, __func__, #x))
#endif

#if defined(VERBOSE) && VERBOSE
#define assert_debug assert
#else
#define assert_debug(x) (void)assert_if_const(x)
#endif

```

`hyp/interfaces/util/include/atomic.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// More concise aliases for common atomic operations.

#include <asm/atomic.h>

// Shortcuts for load-relaxed and load-acquire
#define atomic_load_relaxed(p) atomic_load_explicit((p), memory_order_relaxed)
#define atomic_load_acquire(p) atomic_load_explicit((p), memory_order_acquire)

// Atomic load-consume.
//
// Perform a load-consume, with the semantics it should have rather than the
// semantics it is defined to have in the standard. On most CPUs, this is just
// a relaxed atomic load (assuming that volatile has the new semantics specified
// in C18, as it does in virtually every C implementation ever).
#if !defined(atomic_load_consume)
#define atomic_load_consume(p) atomic_load_explicit(p, memory_order_relaxed)
#endif

// Shortcuts for store-relaxed and store-release
#define atomic_store_relaxed(p, v)                                             \
	atomic_store_explicit((p), (v), memory_order_relaxed)
#define atomic_store_release(p, v)                                             \
	atomic_store_explicit((p), (v), memory_order_release)

// Device memory fences
//
// A fence affecting device accesses may need to use a stronger barrier
// instruction compared to a fence affecting only CPU threads. This macro
// may be redefined by <asm/atomic.h> to use stronger instructions if necessary.
#if !defined(atomic_device_fence)
#define atomic_device_fence(o) atomic_thread_fence(o)
#endif

```

`hyp/interfaces/util/include/attributes.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// This file documents the permitted function, type and variable attributes, and
// defines short names for them. Do not use any attribute specifier unless it is
// listed below.
//
// This file is automatically included in all source files; this is done with
// -imacro, so this file must not contain any non-preprocessor declarations.
//
// This rule does not apply to language constructs that have an effect that is
// similar or equivalent to an attribute, such as _Noreturn or _Alignas.

// Mark a function as cold. This is used to mark cold functions for
// which internal inlining or size increasing optimizations would be
// a waste of space.
#define COLD __attribute__((cold))

// Don't inline this function. This is used to mark functions for which
// inlining would be a waste of space and/or would make debugging inconvenient.
#define NOINLINE __attribute__((noinline))

// Always inline the function. This is used for certain inline assembler
// wrappers which cannot safely be wrapped in a function call, such as
// load-exclusive instructions which might lose their exclusivity.
#define ALWAYS_INLINE __attribute__((always_inline))

// Declare or define a function as weakly linked.
//
// If placed on a definition, this creates a weak definition, which can be
// overridden by a non-weak definition in another object.
//
// If placed on a declaration, this causes calls to the function to be linked as
// weak references, which do not require a definition to exist at all. The
// behaviour of calling an undefined function is unspecified, so this usage
// is not recommended.
#define WEAK __attribute__((weak))

//
// Clang thread safety analysis attributes.
//

// Declare a type as representing a thread-safety capability (in Clang SA terms;
// this is unrelated to the Gunyah API's object capabilities).
//
// Note that calling this "lockable" is misleading because it is not necessarily
// exclusive, but that's better than overloading "capability".
//
// The name is a human-readable string used in error messages.
#define LOCKABLE(name) __attribute__((capability(name)))

// Declare a variable as being protected by a specific lock.
#define PROTECTED_BY(lock) __attribute__((guarded_by(lock)))

// Declare a variable as being protected by a specific lock.
#define PTR_PROTECTED_BY(lock) __attribute__((pt_guarded_by(lock)))

// Declare a function as acquiring a specified lock object.
#define ACQUIRE_LOCK(lock) __attribute__((acquire_capability(lock)))

// Declare a function as acquiring a specified lock object if it returns a
// specified value.
#define TRY_ACQUIRE_LOCK(success, lock)                                        \
	__attribute__((try_acquire_capability(success, lock)))

// Declare a function as releasing a specified lock object.
#define RELEASE_LOCK(lock) __attribute__((release_capability(lock)))

// Declare a function as requiring a lock to be held.
#define REQUIRE_LOCK(lock) __attribute__((requires_capability(lock)))

// Declare a function as requiring a lock to _not_ be held.
#define EXCLUDE_LOCK(lock) __attribute__((requires_capability(!&(lock))))

// Declare a function as acquiring a shared reader lock.
#define ACQUIRE_READ(lock) __attribute__((acquire_shared_capability(lock)))

// Declare a function as acquiring a specified shared read lock if it
// returns a specified value.
#define TRY_ACQUIRE_READ(success, lock)                                        \
	__attribute__((try_acquire_shared_capability(success, lock)))

// Declare a function as releasing a shared reader lock.
#define RELEASE_READ(lock) __attribute__((release_shared_capability(lock)))

// Declare a function as requiring a shared reader lock to be held.
#define REQUIRE_READ(lock) __attribute__((requires_shared_capability(lock)))

// Declare a function as requiring a shared reader lock to _not_ be held.
#define EXCLUDE_READ(lock) __attribute__((locks_excluded(lock)))

// Define a function that implements a lock acquire or release. This disables
// checking of thread safety throughout the function, so the function should
// ideally do nothing other than implement the lock, hence the name.
//
// This attribute can also disable analysis in a function that is too complex
// for the analyser to understand; e.g. one that has conditional locking. Using
// it for this purpose so is strongly deprecated.
#define LOCK_IMPL __attribute__((no_thread_safety_analysis))

```

`hyp/interfaces/util/include/bitmap.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <limits.h>

#include <types/bitmap.h>

#define BITMAP_DECLARE(bits, name)     register_t name[BITMAP_NUM_WORDS(bits)]
#define BITMAP_DECLARE_PTR(bits, name) register_t(*name)[BITMAP_NUM_WORDS(bits)]

bool
bitmap_isset(const register_t *bitmap, index_t bit);

void
bitmap_set(register_t *bitmap, index_t bit);

void
bitmap_clear(register_t *bitmap, index_t bit);

register_t
bitmap_extract(const register_t *bitmap, index_t bit, index_t width);

void
bitmap_insert(register_t *bitmap, index_t bit, index_t width, register_t value);

bool
bitmap_ffs(const register_t *bitmap, index_t num_bits, index_t *bit);

bool
bitmap_ffc(const register_t *bitmap, index_t num_bits, index_t *bit);

bool
bitmap_empty(const register_t *bitmap, index_t num_bits);

bool
bitmap_full(const register_t *bitmap, index_t num_bits);

bool
bitmap_atomic_isset(const _Atomic register_t *bitmap, index_t bit,
		    memory_order order);

bool
bitmap_atomic_test_and_set(_Atomic register_t *bitmap, index_t bit,
			   memory_order order);

#define bitmap_atomic_set(bitmap, bit, order)                                  \
	(void)bitmap_atomic_test_and_set((bitmap), (bit), (order))

bool
bitmap_atomic_test_and_clear(_Atomic register_t *bitmap, index_t bit,
			     memory_order order);

#define bitmap_atomic_clear(bitmap, bit, order)                                \
	(void)bitmap_atomic_test_and_clear((bitmap), (bit), (order))

bool
bitmap_atomic_ffs(const _Atomic register_t *bitmap, index_t num_bits,
		  index_t *bit);

bool
bitmap_atomic_ffc(const _Atomic register_t *bitmap, index_t num_bits,
		  index_t *bit);

bool
bitmap_atomic_empty(const _Atomic register_t *bitmap, index_t num_bits);

bool
bitmap_atomic_full(const _Atomic register_t *bitmap, index_t num_bits);

register_t
bitmap_atomic_extract(const _Atomic register_t *bitmap, index_t bit,
		      index_t width, memory_order order);

void
bitmap_atomic_insert(_Atomic register_t *bitmap, index_t bit, index_t width,
		     register_t value, memory_order order);

static inline register_t
bitmap__get_word(const register_t *bitmap, index_t word)
{
	return bitmap[word];
}

static inline register_t
bitmap__atomic_get_word(const _Atomic register_t *bitmap, index_t word)
{
	return atomic_load_explicit(&bitmap[word], memory_order_relaxed);
}

// Loop macros for iterating over bitmaps. Note that these are written
// to avoid using break statements, so the body provided by the caller
// can use a break or goto statement without breaking MISRA rule 15.4.
#define BITMAP__FOREACH_BEGIN(i, w, r, b, g, n)                                \
	{                                                                      \
		index_t	   w = 0;                                              \
		register_t r = 0;                                              \
		while ((r != 0U) || ((w * BITMAP_WORD_BITS) < n)) {            \
			if (r == 0U) {                                         \
				r = g((b), (w));                               \
				w++;                                           \
			}                                                      \
			if (r != 0U) {                                         \
				index_t i = compiler_ctz(r);                   \
				r &= ~(register_t)1 << i;                      \
				i += ((w - 1U) * BITMAP_WORD_BITS);            \
				if (i >= n) {                                  \
					r = 0;                                 \
				} else {
// clang-format off
#define BITMAP__FOREACH_END }}}}
// clang-format on

#define BITMAP_FOREACH_SET_BEGIN(i, b, n)                                      \
	BITMAP__FOREACH_BEGIN(i, util_cpp_unique_ident(w),                     \
			      util_cpp_unique_ident(r), b, bitmap__get_word,   \
			      n)
#define BITMAP_FOREACH_SET_END BITMAP__FOREACH_END

#define BITMAP_FOREACH_CLEAR_BEGIN(i, b, n)                                    \
	BITMAP__FOREACH_BEGIN(i, util_cpp_unique_ident(w),                     \
			      util_cpp_unique_ident(r), b, ~bitmap__get_word,  \
			      n)
#define BITMAP_FOREACH_CLEAR_END BITMAP__FOREACH_END

#define BITMAP_ATOMIC_FOREACH_SET_BEGIN(i, b, n)                               \
	BITMAP__FOREACH_BEGIN(i, util_cpp_unique_ident(w),                     \
			      util_cpp_unique_ident(r), b,                     \
			      bitmap__atomic_get_word, n)
#define BITMAP_ATOMIC_FOREACH_SET_END BITMAP__FOREACH_END

#define BITMAP_ATOMIC_FOREACH_CLEAR_BEGIN(i, b, n)                             \
	BITMAP__FOREACH_BEGIN(i, util_cpp_unique_ident(w),                     \
			      util_cpp_unique_ident(r), b,                     \
			      ~bitmap__atomic_get_word, n)
#define BITMAP_ATOMIC_FOREACH_CLEAR_END BITMAP__FOREACH_END

```

`hyp/interfaces/util/include/compiler.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// Macros wrapping miscellaneous compiler builtins to make them easier to use
// correctly. Note that this is not intended for compiler independence, only
// for readability.

// Branch probability hints.
//
// Note that the argument and result of each of these macros must be
// essentially boolean (MISRA rule 14.4), but the argument and result of
// __builtin_expect have type long.
#define compiler_expected(x)   (__builtin_expect((x) ? 1 : 0, 1) != 0)
#define compiler_unexpected(x) (__builtin_expect((x) ? 1 : 0, 0) != 0)

// Bit operations.
//
// On ARM, prefer clz and clrsb as they expand to single instructions (CLZ and
// CLS). ffs and ctz need an extra RBIT first.

// clang-format off
#define compiler_ffs(x) (index_t)_Generic(				       \
	(x),								       \
	long long: __builtin_ffsll(x),					       \
	unsigned long long: __builtin_ffsll((long long)(x)),		       \
	long: __builtin_ffsl(x),					       \
	unsigned long: __builtin_ffsl((long)(x)),			       \
	int: __builtin_ffs(x),						       \
	unsigned int: __builtin_ffs((int)(x)))

#define compiler_clz(x) (assert((x) != 0U), (index_t)_Generic(		       \
	(x),								       \
	unsigned long long: __builtin_clzll,				       \
	unsigned long: __builtin_clzl,				       \
	unsigned int: __builtin_clz)(x))

#define compiler_ctz(x) (assert((x) != 0U), (index_t)_Generic(		       \
	(x),								       \
	unsigned long long: __builtin_ctzll,				       \
	unsigned long: __builtin_ctzl,				       \
	unsigned int: __builtin_ctz)(x))

#define compiler_clrsb(x) (index_t)_Generic(				       \
	(x), long long: __builtin_clrsbll,				       \
	long: __builtin_clrsbl,					       \
	int: __builtin_clrsb)(x)

#define compiler_popcount(x) (assert((x) != 0U), (index_t)_Generic(	       \
	(x),								       \
	unsigned long long: __builtin_popcountll,			       \
	unsigned long: __builtin_popcountl,				       \
	unsigned int: __builtin_popcount)(x))
// clang-format on

#define compiler_msb(x) ((sizeof(x) * 8U) - 1U - compiler_clz(x))

// Object sizes, for use in minimum buffer size assertions. These return
// (size_t)-1 if the size cannot be determined statically, so the assertion
// should become a no-op in that case. LLVM has an intrinsic for this, so
// the static determination can be made after inlining by LTO.
#define compiler_sizeof_object(ptr)    __builtin_object_size((ptr), 1)
#define compiler_sizeof_container(ptr) __builtin_object_size((ptr), 0)

// Mark a break statement as unreachable. This expected to be used at the end of
// switch cases to comply with MISRA rule 16.3, which requires a break statement
// to end the case regardless of whether it is reachable.

// clang-format off
#define compiler_unreachable_break \
	_Pragma("clang diagnostic push")                                       \
	_Pragma("clang diagnostic ignored \"-Wunreachable-code\"")             \
	break                                                                  \
	_Pragma("clang diagnostic pop")
// clang-format on

```

`hyp/interfaces/util/include/enum.h`:

```h
// © 2022 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#define ENUM_FOREACH(name, i)                                                  \
	static_assert(name##__MIN >= 0, "negative enum");                      \
	static_assert(name##__MAX >= name##__MIN, "invalid enum");             \
	for (index_t i = (index_t)name##__MIN; i <= (index_t)name##__MAX; i++)

```

`hyp/interfaces/util/include/list.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// The list implementation consists of a circular double linked list and the
// list type contains a head pointer to the first element of the list.

// All the following functions require the list to be locked if it may be
// accessed by other threads, unless noted otherwise.

#include <atomic.h>
#include <util.h>

void
list_init(list_t *list);

list_node_t *
list_get_head(list_t *list);

bool
list_is_empty(list_t *list);

void
list_insert_at_head(list_t *list, list_node_t *node);

void
list_insert_at_tail(list_t *list, list_node_t *node);

// This function inserts a node in order, where the ordering is defined by the
// caller.
//
// If we want, for example, to insert a node in increasing order, then the
// caller needs to provide a function pointer that returns true if node a is
// smaller than node b, according to the caller's criteria.
//
// Returns true if the new node is placed at the head of the list, or false if
// the new node has been inserted after the head.
bool
list_insert_in_order(list_t *list, list_node_t *node,
		     bool (*compare_fn)(list_node_t *a, list_node_t *b));

void
list_insert_after_node(list_t *list, list_node_t *prev, list_node_t *node);

// The _release variants of insert must be used on any list that is iterated
// with a _consume iterator.
void
list_insert_at_tail_release(list_t *list, list_node_t *node);

// This function returns true if node has been removed from head and the list is
// not empty after the deletion.
//
// If the list is ever iterated by a _consume iterator, then the specified node
// must not be either freed or added to another list until an RCU grace period
// has elapsed; i.e. rcu_enqueue() or rcu_sync() must be called after this
// function returns.
bool
list_delete_node(list_t *list, list_node_t *node);

// Simple iterator. The list must be locked if other threads might modify it,
// and the iterator must not delete nodes.
#define list_foreach(node, list)                                               \
	for ((node) = atomic_load_relaxed(&(list)->head.next);                 \
	     (node) != &(list)->head;                                          \
	     (node) = atomic_load_relaxed(&(node)->next))

#define list__foreach_container(container, list, cname, nname, n)              \
	list_node_t *n = atomic_load_relaxed(&list->head.next);                \
	container      = (n != &list->head) ? cname##_container_of_##nname(n)  \
					    : NULL;                            \
	for (; container != NULL;                                              \
	     n	       = atomic_load_relaxed(&n->next),                        \
	     container = (n != &list->head) ? cname##_container_of_##nname(n)  \
					    : NULL)

// Simple container iterator. The list must be locked if other threads might
// modify it, and the iterator must not delete nodes.
#define list_foreach_container(container, list, cname, nname)                  \
	list__foreach_container((container), (list), cname, nname,             \
				util_cpp_unique_ident(node))

#define list__foreach_container_safe(container, list, cname, nname, n, load)   \
	list_node_t *n = load(&list->head.next);                               \
	container      = (n != &list->head) ? cname##_container_of_##nname(n)  \
					    : NULL;                            \
	n	       = load(&n->next);                                       \
	for (; container != NULL;                                              \
	     container = (n != &list->head) ? cname##_container_of_##nname(n)  \
					    : NULL,                            \
	     n	       = load(&n->next))

// Deletion-safe container iterator. The list must be locked if other threads
// might modify it. The iterator may delete the current node.
#define list_foreach_container_maydelete(container, list, cname, nname)        \
	list__foreach_container_safe((container), (list), cname, nname,        \
				     util_cpp_unique_ident(next),              \
				     atomic_load_relaxed)

// RCU-safe container iterator. Must only be used within an RCU critical
// section. The list need not be locked, but other threads that insert nodes
// must use the _release variants of the insert functions, and any thread that
// deletes a node must allow an RCU grace period to elapse before either freeing
// the memory or adding it to a list again.
#define list_foreach_container_consume(container, list, cname, nname)          \
	list__foreach_container_safe((container), (list), cname, nname,        \
				     util_cpp_unique_ident(next),              \
				     atomic_load_consume)

```

`hyp/interfaces/util/include/panic.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

noreturn void
panic(const char *str);

```

`hyp/interfaces/util/include/refcount.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// Simple lock-free reference counting.

// Initialise a reference count, with a single reference held.
void
refcount_init(refcount_t *ref);

// Get a reference, assuming that the count is nonzero. This must only be used
// in cases where the caller already knows that there is at least one reference
// that cannot be concurrently released by another thread, hence the name. No
// memory barrier is implied; adequate barriers should be provided by whatever
// other mechanism is used to guarantee that the count is nonzero, e.g. RCU.
void
refcount_get_additional(refcount_t *ref);

// Get a reference, without assuming that the count is nonzero. The caller must
// check the result; if it is false, the count had already reached zero and the
// reference could not be token. An acquire memory barrier is implied.
bool
refcount_get_safe(refcount_t *ref);

// Release a reference. The caller must check the result; if it is true, the
// count has now reached zero and the caller must take action to free the
// underlying resource. A release memory barrier is implied.
bool
refcount_put(refcount_t *ref);

```

`hyp/interfaces/util/include/string.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// Local version of the standard-defined string.h
//
// Only the memory block manipulation functions (mem*()) are declared. The
// hypervisor has no need to operate on real strings, so the string
// manipulation functions (str*()) are left undefined.
//
// Note: MISRA Required Rule 21.2 states that reserved identifiers are not to
// be declared, and gives a memcpy declaration as a specific non-conforming
// example. However, the identifiers declared here (including memcpy) are
// _not_ reserved: the hypervisor is built in freestanding mode (as asserted
// below), which does not guarantee their presence and therefore must not give
// them special behaviour (see C18 clause 4, item 6). The Clang/GCC option
// -ffreestanding implies -fno-builtin for this reason.
//
// Also, we _must_ implement these functions ourselves with their standard
// semantics (regardless of MISRA 21.2) because the LLVM and GCC backends
// assume they are provided by the environment, and will generate calls to
// them even when the frontend is in freestanding mode.

#if !defined(HYP_STANDALONE_TEST)
_Static_assert(__STDC_HOSTED__ == 0,
	       "This file deviates from MISRA rule 21.2 in hosted mode");
#endif

// Define size_t and NULL
#include <stddef.h>

#define memscpy(s1, s1_size, s2, s2_size)                                      \
	((void)memcpy(s1, s2,                                                  \
		      ((s1_size) < (s2_size)) ? (s1_size) : (s2_size)),        \
	 ((s1_size) < (s2_size)) ? (s1_size) : (s2_size))

extern void *
memcpy(void *restrict s1, const void *restrict s2, size_t n);

extern void *
memmove(void *s1, const void *s2, size_t n);

extern void *
memset(void *s, int c, size_t n);

typedef int    errno_t;
typedef size_t rsize_t;

// A secure memset, guaranteed not to be optimized out
extern errno_t
memset_s(void *s, rsize_t smax, int c, rsize_t n);

extern size_t
strlen(const char *str);

extern char *
strchr(const char *str, int c);

```

`hyp/interfaces/util/include/types/bitmap.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// Bitmap definitions for the type DSL. This is also included by C code to
// get BITMAP_NUM_WORDS().

#define BITMAP_NUM_WORDS(x) (((x) + BITMAP_WORD_BITS - 1U) / BITMAP_WORD_BITS)

#if defined(__TYPED_DSL__)
#define BITMAP(bits, ...)                                                      \
	array(BITMAP_NUM_WORDS(bits)) type register_t(__VA_ARGS__)
#endif

```

`hyp/interfaces/util/include/util.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// Miscellaneous utility macros.
//
// These all have simple definitions - no compiler builtins or other language
// extensions. Look in compiler.h for those.

#define util_bit(b)  ((uintmax_t)1U << (b))
#define util_sbit(b) ((intmax_t)1 << (b))
#define util_mask(n) (util_bit(n) - 1U)

#define util_max(x, y) (((x) > (y)) ? (x) : (y))
#define util_min(x, y) (((x) < (y)) ? (x) : (y))

// Arithmetic predicates with intent that is not obvious when open-coded
#define util_is_p2_or_zero(x)	 (((x) & ((x)-1U)) == 0U)
#define util_is_p2(x)		 (((x) != 0U) && util_is_p2_or_zero(x))
#define util_is_baligned(x, a)	 (assert(util_is_p2(a)), (((x) & ((a)-1U)) == 0U))
#define util_is_p2aligned(x, b)	 (((x) & (util_bit(b) - 1U)) == 0U)
#define util_add_overflows(a, b) ((a) > ~(__typeof__((a) + (b)))(b))

// This version can be used in static asserts
#define util_is_baligned_assert(x, a)                                          \
	(util_is_p2(a) && (((x) & ((a)-1U)) == 0U))

// Align up or down to bytes (which must be a power of two)
#if defined(__TYPED_DSL__)
#define util_balign_down(x, a) ((x) & ~((a)-1U))
#else
#define util_balign_down(x, a)                                                 \
	(assert(util_is_p2(a)), (x) & ~((__typeof__(x))(a)-1U))
#endif
#define util_balign_up(x, a) util_balign_down((x) + ((a)-1U), a)

// Round up or down to a multiple of an unsigned constant, which may not be a
// power of two. Rounding to a non-constant at runtime should be avoided,
// because it will perform a slow divide operation.
#define util_round_down(x, a) ((x) - ((x) % (a)))
#define util_round_up(x, a)   util_round_down((x) + ((a)-1U), (a))

// Align up or down to a power-of-two size (in bits)
#define util_p2align_down(x, b)                                                \
	(assert((sizeof(x) * 8U) > (b)), (((x) >> (b)) << (b)))
#define util_p2align_up(x, b) util_p2align_down((x) + util_bit(b) - 1U, b)

// Generate an identifier that can be declared inside a macro without
// shadowing anything else declared in the same file, given a base name to
// disambiguate uses within one macro expansion. Generally the name should be
// prefixed with the name of the macro it's being used in.
//
// Note that this should only ever be used as a macro parameter; otherwise
// it is difficult to determine what identifier it expanded to.
#define util_cpp_unique_ident(name) util_cpp_paste_expanded(name, __LINE__)

// Paste two tokens together, after macro-expansion of the arguments.
#define util_cpp_paste_expanded(name, suffix) util_cpp_paste(name, suffix)

// Paste two tokens together, before macro-expansion of the arguments.
//
// This is only really useful in util_cpp_paste_expanded(). In any other macro
// definition, use ## directly, which is equivalent and more concise.
#define util_cpp_paste(name, suffix) name##suffix

// Return the number of elements in an array.
#define util_array_size(a) (sizeof(a) / sizeof((a)[0]))

// Return the size of a structure member.
#define util_sizeof_member(type, member) sizeof(((type *)NULL)->member)

// Check whether a given offset is with the bounds of a structure member
#define util_offset_in_range(offset, type, member)                             \
	(((offset) >= offsetof(type, member)) &&                               \
	 ((offset) <                                                           \
	  offsetof(type, member) + util_sizeof_member(type, member)))

```

`hyp/interfaces/util/list.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// Add list_node in object definition as:
// <node-name> structure list_node(contained);
// so that we can get the object from the node by calling
// <object-name>_container_of_<node-name>(list_node)

define list_node structure {
	next	pointer(atomic) structure list_node;
	prev	pointer structure list_node;
};

define list structure {
	head	structure list_node;
};

```

`hyp/interfaces/vcpu/aarch64/vcpu.tc`:

```tc
// © 2022 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

extend vcpu_register_set enumeration {
	X = 0; // index 0-30
	PC = 1; // index 0
	SP_EL = 2; // index 0-1
};

extend vcpu_option_flags bitfield {
	1		ras_error_handler	bool = 0;
	2		amu_counting_disabled	bool = 0;
	3		sve_allowed		bool = 0;
	4		debug_allowed		bool = 0;
	5		trace_allowed		bool = 0;
	// 7		reserved: mpam_allowed
	// 8		critical bool;
	63		hlos_vm			bool = 0;
	others		unknown = 0;
};

#if defined(ARCH_ARM_FEAT_CSV2_2) || defined(ARCH_ARM_FEAT_CSV2_1p2) || \
	defined(ARCH_ARM_FEAT_CSV2_3)
extend vcpu_runtime_flags bitfield {
	auto		scxt_allowed		bool = 0;
};
#endif

```

`hyp/interfaces/vcpu/armv8/traps.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// Add the events for the traps coming from AArch32
// FIXME:

interface vcpu

// Guest trap events
handled_event vcpu_trap_unknown
	param esr: ESR_EL2_t
	return: vcpu_trap_result_t = VCPU_TRAP_RESULT_UNHANDLED

handled_event vcpu_trap_wfe
	param iss: ESR_EL2_ISS_WFI_WFE_t
	return: vcpu_trap_result_t = VCPU_TRAP_RESULT_UNHANDLED

handled_event vcpu_trap_wfi
	param iss: ESR_EL2_ISS_WFI_WFE_t
	return: vcpu_trap_result_t = VCPU_TRAP_RESULT_UNHANDLED

handled_event vcpu_trap_fp_enabled
	param esr: ESR_EL2_t
	return: vcpu_trap_result_t = VCPU_TRAP_RESULT_UNHANDLED

#if defined(ARCH_ARM_FEAT_PAuth)
handled_event vcpu_trap_pauth
#endif

handled_event vcpu_trap_illegal_state

handled_event vcpu_trap_svc64
	param esr: ESR_EL2_t

handled_event vcpu_trap_hvc64
	param iss: ESR_EL2_ISS_HVC_t

handled_event vcpu_trap_smc64
	param iss: ESR_EL2_ISS_SMC64_t

handled_event vcpu_trap_sysreg_read
	param iss: ESR_EL2_ISS_MSR_MRS_t
	return: vcpu_trap_result_t = VCPU_TRAP_RESULT_UNHANDLED

handled_event vcpu_trap_sysreg_write
	param iss: ESR_EL2_ISS_MSR_MRS_t
	return: vcpu_trap_result_t = VCPU_TRAP_RESULT_UNHANDLED

#if defined(ARCH_ARM_FEAT_PAuth)
handled_event vcpu_trap_eret
	param esr: ESR_EL2_t
#endif

handled_event vcpu_trap_pf_abort_guest
	param esr: ESR_EL2_t
	param ipa: vmaddr_result_t
	param far: FAR_EL2_t
	return: vcpu_trap_result_t = VCPU_TRAP_RESULT_UNHANDLED

#if defined(ARCH_ARM_FEAT_SVE)
handled_event vcpu_trap_sve_access
	return: vcpu_trap_result_t = VCPU_TRAP_RESULT_UNHANDLED
#endif

handled_event vcpu_trap_pc_alignment_fault

handled_event vcpu_trap_data_abort_guest
	param esr: ESR_EL2_t
	param ipa: vmaddr_result_t
	param far: FAR_EL2_t
	return: vcpu_trap_result_t = VCPU_TRAP_RESULT_UNHANDLED

handled_event vcpu_trap_sp_alignment_fault

handled_event vcpu_trap_fp64
	param esr: ESR_EL2_t
	return: vcpu_trap_result_t = VCPU_TRAP_RESULT_UNHANDLED

handled_event vcpu_trap_breakpoint_guest
	param esr: ESR_EL2_t
	return: vcpu_trap_result_t = VCPU_TRAP_RESULT_UNHANDLED

handled_event vcpu_trap_software_step_guest
	param esr: ESR_EL2_t
	return: vcpu_trap_result_t = VCPU_TRAP_RESULT_UNHANDLED

handled_event vcpu_trap_watchpoint_guest
	param esr: ESR_EL2_t
	return: vcpu_trap_result_t = VCPU_TRAP_RESULT_UNHANDLED

handled_event vcpu_trap_brk_instruction_guest
	param esr: ESR_EL2_t
	return: vcpu_trap_result_t = VCPU_TRAP_RESULT_UNHANDLED

handled_event vcpu_trap_serror
	param iss: ESR_EL2_ISS_SERROR_t

// AArch32 traps
#if ARCH_AARCH64_32BIT_EL0
handled_event vcpu_trap_ldcstc_guest
	param iss: ESR_EL2_ISS_LDC_STC_t
	return: vcpu_trap_result_t = VCPU_TRAP_RESULT_UNHANDLED

handled_event vcpu_trap_mcrmrc14_guest
	param iss: ESR_EL2_ISS_MCR_MRC_t
	return: vcpu_trap_result_t = VCPU_TRAP_RESULT_UNHANDLED

handled_event vcpu_trap_mcrmrc15_guest
	param esr: ESR_EL2_t
	return: vcpu_trap_result_t = VCPU_TRAP_RESULT_UNHANDLED

handled_event vcpu_trap_mcrrmrrc15_guest
	param esr: ESR_EL2_t
	return: vcpu_trap_result_t = VCPU_TRAP_RESULT_UNHANDLED

handled_event vcpu_trap_mrrc14_guest
	param esr: ESR_EL2_t
	return: vcpu_trap_result_t = VCPU_TRAP_RESULT_UNHANDLED

handled_event vcpu_trap_bkpt_guest
	param esr: ESR_EL2_t
	return: vcpu_trap_result_t = VCPU_TRAP_RESULT_UNHANDLED
#endif

```

`hyp/interfaces/vcpu/armv8/vcpu.tc`:

```tc
// © 2022 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

extend platform_cpu_features bitfield {
	auto		scxt_disable	bool = 1;
};

```

`hyp/interfaces/vcpu/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

types vcpu.tc
events vcpu.ev
arch_events armv8 traps.ev
arch_types armv8 vcpu.tc
arch_types aarch64 vcpu.tc

```

`hyp/interfaces/vcpu/include/vcpu.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// Configure vcpu options
//
// The object's header lock must be held and object state must be
// OBJECT_STATE_INIT.
error_t
vcpu_configure(thread_t *thread, vcpu_option_flags_t vcpu_options);

// Set a VCPU's initial execution state and start execution.
//
// The target thread must be a VCPU, its scheduling lock must be held by the
// caller, and it must be currently blocked by the SCHEDULER_BLOCK_VCPU_OFF
// flag. This function clears that block flag, and returns true if the vcpu has
// become runnable (which implies that a call to scheduler_schedule() is
// needed).
//
// If the target VCPU has ever run, it must have called vcpu_poweroff() before
// this function is called on it.
bool_result_t
vcpu_poweron(thread_t *vcpu, vmaddr_result_t entry_point,
	     register_result_t context) REQUIRE_SCHEDULER_LOCK(vcpu);

// Halt execution of the current VCPU and tear down its execution state.
//
// The caller must be a runnable VCPU. This function will block the caller with
// the SCHEDULER_BLOCK_VCPU_OFF flag and yield. It does not return on success;
// if the thread is re-activated by a call to vcpu_poweron(), it will jump
// directly to the new userspace context.
//
// If the last_vcpu argument is true, the caller must be the only powered-on
// VCPU attached to a VPM group, or else not attached to a VPM group. Otherwise,
// it must be attached to a VPM group with at least one other powered-on VCPU.
// If the last_vcpu argument is not correct, this call may return ERROR_DENIED.
// This check is not performed if the force argument is true.
error_t
vcpu_poweroff(bool last_vcpu, bool force);

// Halt the current thread's VCPU execution state.
//
// This function will raise the VCPU's halt VIRQ, notifying its handler VM
// that the VCPU has halted. It can be called by any module that has
// already blocked a VCPU in a way that might need handling by RM.
noreturn void
vcpu_halted(void);

// Suspend a VCPU.
//
// This function will block the caller with the SCHEDULER_BLOCK_VCPU_SUSPEND
// flag and yield, unless the suspend is aborted by a handler for the
// vcpu_suspend event, in which case the caller remains runnable.
//
// The return value is OK if the caller was successfully suspended and has
// since resumed, unless a warm reset was requested while the thread was
// suspended, in which case the function does not return. It returns an error if
// the suspend was aborted.
error_t
vcpu_suspend(void);

// Warm-reset a suspended VCPU.
//
// This function initialises all EL1 system registers for the current VCPU,
// including any that have been virtualised, as described by the ARMv8 ARM
// revision E.a section D1.9.1, "PE state on reset to AArch64 state". The
// entry point and context are also set as they would be by vcpu_poweron().
void
vcpu_warm_reset(paddr_t entry_point, register_t context)
	EXCLUDE_PREEMPT_DISABLED;

// Resume a suspended VCPU.
//
// The target thread must be a VCPU, its scheduling lock must be held by the
// caller, and it must be currently blocked by the SCHEDULER_BLOCK_VCPU_SUSPEND
// flag. This function clears that block flag.
void
vcpu_resume(thread_t *vcpu) REQUIRE_SCHEDULER_LOCK(vcpu);

// Wake a specified VCPU from interrupt wait.
//
// The target thread must be a VCPU, and its scheduling lock must be held by the
// caller.
//
// This function clears any block flag associated with a wait for interrupts by
// the target VCPU. If the VCPU is not currently waiting for interrupts, this
// has no effect.
void
vcpu_wakeup(thread_t *vcpu) REQUIRE_SCHEDULER_LOCK(vcpu);

// Prevent the current VCPU entering interrupt wait.
//
// In preemptible configurations, it is possible for a WFI trap handler to be
// interrupted to deliver an IRQ that should prevent the trapped WFI sleeping.
// This function ensures that any currently interrupted WFI trap handler will
// not incorrectly sleep. It must be called whenever an ISR asserts a virtual
// IRQ on the current VCPU.
//
// This function is lock-free and can be safely called from any context. In non-
// preemptible configurations it is a no-op. If called outside of an ISR, it
// has no effect.
void
vcpu_wakeup_self(void);

// Query whether the specified VCPU is blocked on vcpu_wakeup().
//
// This is intended to be called during context switch to optimise wakeup
// sources by disabling them and/or not checking their states if they are not
// able to wake the thread.
bool
vcpu_expects_wakeup(const thread_t *thread);

// Query whether the current VCPU has a pending wakeup.
//
// This may be called to check whether a long-running operation on behalf of
// the current VCPU should be interrupted so it can return early. If it returns
// true, the caller should return to the VCPU as soon as is practical. The
// caller is responsible for guaranteeing that it can make progress if this
// persistently returns true (e.g. if EL1 is calling with interrupts disabled
// while an interrupt is pending).
//
// This must be called with preemption disabled.
bool
vcpu_pending_wakeup(void) REQUIRE_PREEMPT_DISABLED;

// Prepare to block hypervisor execution on behalf of the current VCPU.
//
// This function should be called before any potentially long running operation
// that will block execution of both the calling VCPU and the hypervisor itself.
// That includes waiting for interrupts on behalf of the VCPU (without switching
// to hypervisor idle), and service calls forwarded to higher privilege levels.
//
// This function returns the same result as vcpu_pending_wakeup(). Additionally,
// if it returns false, it may also reconfigure hardware-virtualised wakeup
// sources to be able to readily interrupt an operation at higher privilege
// levels if they would not be able to do that normally. In this case, the
// caller must also call vcpu_block_finish() to clean up after the operation
// returns or is interrupted.
//
// This must be called with preemption disabled. If it returns false, preemption
// must remain disabled until vpcu_block_finish() returns.
bool
vcpu_block_start(void) REQUIRE_PREEMPT_DISABLED;

// Clean up after blocking hypervisor execution on behalf of the current VCPU.
//
// This must be called exactly once for every call to vcpu_block_start() that
// returns false, prior to re-enabling preemption. It must not be called after
// a call to vcpu_block_start() returns true.
void
vcpu_block_finish(void) REQUIRE_PREEMPT_DISABLED;

// Return the VCPU's general purpose registers
register_t
vcpu_gpr_read(thread_t *thread, uint8_t reg_num);

// Update the VCPU's general purpose registers
void
vcpu_gpr_write(thread_t *thread, uint8_t reg_num, register_t value);

error_t
vcpu_bind_virq(thread_t *vcpu, vic_t *vic, virq_t virq,
	       vcpu_virq_type_t virq_type);

error_t
vcpu_unbind_virq(thread_t *vcpu, vcpu_virq_type_t virq_type);

```

`hyp/interfaces/vcpu/vcpu.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

interface vcpu

// Triggered when a VCPU is initialised by a call to vcpu_poweron().
//
// This event is triggered with the scheduler lock for the specified VCPU held
// by the caller. The VCPU is blocked by the SCHEDULER_BLOCK_VCPU_OFF flag.
//
// The valid returns are OK or ERROR_ARGUMENT_INVALID or errors forwarded
// from the PSCI SMC cpu_on call.
setup_event vcpu_poweron
	param vcpu: thread_t *
	return: error_t = OK
	success: OK

// Triggered when a VCPU halts itself by calling vcpu_poweroff().
//
// This event is triggered with the scheduler lock for the current VCPU held
// by the caller. The VCPU will be blocked by the SCHEDULER_BLOCK_VCPU_OFF
// flag after the event completes.
//
// The last_vcpu argument indicates whether this VCPU is expected to be the
// only powered-on VCPU in the VM. If the force argument is not set, handlers
// may assume or assert that the last_vcpu flag is correct, and return
// ERROR_DENIED otherwise.
//
// If the force argument is not set, handlers must return OK. No result other
// than OK or ERROR_DENIED is permitted.
setup_event vcpu_poweroff
	param current: thread_t *
	param last_vcpu: bool
	param force: bool
	return: error_t = OK
	success: OK

// Triggered when a VCPU has stopped execution due to a power-off or halt.
//
// This event is triggered in the context of the VCPU that is stopping, with
// preemption disabled after marking it blocked. Handlers must not enable
// preemption, because the thread will not resume if preempted.
//
// If the VCPU starts again after this event, there will be a vcpu_started
// event with the warm_reset parameter set to false.
event vcpu_stopped

// Triggered when the current VCPU is requesting entry to a virtual low-power
// state. Entry to the low-power state is denied if any registered handler
// returns an error. The error code may be passed on to the VCPU.
//
// This event is triggered with preemption disabled.
//
// Handlers that only want to check for pending wakeup events and return
// ERROR_BUSY should register for vcpu_pending_wakeup instead.
//
// The valid errors for vcpu_suspend are OK, ERROR_DENIED, or ERROR_BUSY.
setup_event vcpu_suspend
	param current: thread_t *
	return: error_t = OK
	success: OK

// Triggered when a VCPU has been woken from a virtual low-power state.
//
// This event is triggered with preemption disabled. Note that this is
// triggered by the resuming VCPU, not directly by the call to vcpu_resume().
//
// In many cases, handlers for this event will be the same as unwinders for
// vcpu_suspend.
event vcpu_resume
	param current: thread_t *

// Triggered when a VCPU is simulating a warm reset.
//
// This event is triggered prior to context-switching to a VCPU that must
// simulate a warm reset, in the process discarding some of its saved register
// state. This is generally a waste of time in both the hypervisor and the VM
// (which must immediately reconstruct the state that is discarded here).
// However, it may be needed for compatibility with power management APIs that
// are not properly hypervisor-aware, e.g. PSCI 1.1 in OS-initiated mode.
//
// Generally this will be similar to the register resetting done in the
// module's vcpu_activate/vcpu_poweroff handlers, but it may be possible to
// take some shortcuts.
event vcpu_warm_reset
	param vcpu: thread_t *

// Triggered when vcpu_wakeup() is called.
//
// This event is triggered with the scheduler lock for the specified VCPU
// held by the caller. The triggering of this is event is not conditional on flags
// being blocked.
event vcpu_wakeup
	param vcpu: thread_t *

// Triggered when vcpu_wakeup_self() is called.
//
// This event is triggered when the caller is runnable, and its scheduler lock
// is not held.
event vcpu_wakeup_self

// Triggered when vcpu_expects_wakeup() is called.
//
// Every module with a vcpu_wakeup() handler should handle this event and
// return true if the vcpu_wakeup() handler would have an effect on the
// specified thread.
handled_event vcpu_expects_wakeup
	param vcpu: const thread_t *

// Triggered when vcpu_pending_wakeup() is called.
//
// Handlers may return true if a wakeup event is pending on the current VCPU.
//
// Returning true from a handler will cause long-running service calls to be
// interrupted, and will also inhibit VCPU suspend (vcpu_suspend will return
// ERROR_BUSY).
//
// This may be called from any context where the caller is a VCPU. That means
// that the calling VCPU's scheduler lock may or may not be held. However,
// preemption must be disabled.
handled_event vcpu_pending_wakeup

// Triggered when vcpu_block_start() is called, after vcpu_pending_wakeup has
// returned false.
//
// Handlers may return true if a wakeup event is pending on the current VCPU,
// in cases where this is only known after reconfiguring the hardware. If the
// wakeup can be detected without reconfiguring hardware, the handler should
// be registered for the vcpu_pending_wakeup event instead.
//
// This can only be called from a VCPU context with preemption disabled.
setup_event vcpu_block_start
	return: bool = false
	success: false

// Triggered when vcpu_block_finish() is called.
event vcpu_block_finish

// Triggered when a VCPU starts execution after a power-off or halt state.
//
// The warm_reset argument is true if the VCPU is completing a warm reset. If
// it is false, then the VCPU is either starting for the first time, or is
// starting after a previous vcpu_stopped event. If it is true, the VCPU had
// been running previously (or was in power-off suspend) and has not executed
// a vcpu_poweroff event.
event vcpu_started
	param warm_reset: bool

// Triggered when vcpu_bind_virq() / vcpu_unbind_virq() are called.
//
// Handlers must be registered by any module that registers a VCPU IRQ type.
// They must call vic_bind_*() / vic_unbind_sync(), respectively, with the
// appropriate VIRQ source object and triggering type.
selector_event vcpu_bind_virq
	selector type: vcpu_virq_type_t
	param vcpu: thread_t *
	param vic: vic_t *
	param virq: virq_t
	return: error_t = ERROR_ARGUMENT_INVALID

selector_event vcpu_unbind_virq
	selector type: vcpu_virq_type_t
	param vcpu: thread_t *
	return: error_t = ERROR_ARGUMENT_INVALID

```

`hyp/interfaces/vcpu/vcpu.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

extend cap_rights_thread bitfield {
	0	power		bool;
	1	affinity	bool;
	2	priority	bool;
	3	timeslice	bool;
	5	bind_virq	bool;
	6	state		bool;
	7	lifecycle	bool;
	8	write_context	bool;
	9	disable		bool;
};

define vcpu_trap_result enumeration {
	unhandled;
	emulated;
	retry;
	fault;
};

define vcpu_virq_type public enumeration(explicit) {
	vcpu_run_wakeup = 1;
};

define vcpu_poweroff_flags public bitfield<64> {
	0	last_vcpu			bool;
	others	unknown=0;
};

define vcpu_register_set public enumeration(explicit) {
	// All values are arch-specific
};

// Relevant modules (such as the debug module) need to extend this bitfield
// and add their configuration flags for the hypercall_vcpu_configure
// hypercall. Then in their thread_activate handlers they need to check the
// values of these flags and act on them.
define vcpu_option_flags public bitfield<64> {
	0		pinned			bool = 0;
	8		critical		bool = 0;
	others		unknown = 0;
};

// Private vcpu flags for modules to consolidate runtime boolean flags.
//
// For VCPUs that have not yet been activated, this bitfield is protected by
// the VCPU's object state lock. Otherwise, it must only be accessed from the
// VCPU's own context.
define vcpu_runtime_flags bitfield<32> {
	others		unknown = 0;
};

define vcpu_poweron_flags public bitfield<64> {
	0	preserve_entry_point		bool;
	1	preserve_context		bool;
	others	unknown=0;
};

```

`hyp/interfaces/vcpu_power/build.conf`:

```conf
# © 2022 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

```

`hyp/interfaces/vcpu_run/build.conf`:

```conf
# © 2022 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

hypercalls vcpu_run.hvc
events vcpu_run.ev
types vcpu_run.tc

```

`hyp/interfaces/vcpu_run/include/vcpu_run.h`:

```h
// © 2022 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// Return true if vcpu_run is enabled for the specified VCPU.
//
// Returns false if vcpu_run is disabled or the specified thread is not a VCPU.
//
// Note that this is all-or-nothing for any given VCPU: either it is scheduled
// exclusively by calling the vcpu_run hypercall, or else it is scheduled by the
// EL2 scheduler and any attempt to call vcpu_run on it will fail.
//
// The caller must hold the specified thread's scheduler lock.
bool
vcpu_run_is_enabled(const thread_t *vcpu) REQUIRE_SCHEDULER_LOCK(vcpu);

```

`hyp/interfaces/vcpu_run/vcpu_run.ev`:

```ev
// © 2022 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

interface vcpu_run

// Triggered after vcpu_run_is_enabled() becomes true for a VCPU after vcpu
// activation. This dynamic vcpu activation is deprecated.
//
// The VCPU's scheduler lock is held by the caller.
event vcpu_run_enabled
	param vcpu: thread_t *

```

`hyp/interfaces/vcpu_run/vcpu_run.hvc`:

```hvc
// © 2022 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

define vcpu_run hypercall {
	call_num		0x65;
	cap_id			input type cap_id_t;
	resume_data_0		input type register_t;
	resume_data_1		input type register_t;
	resume_data_2		input type register_t;
	res0			input uregister;
	error			output enumeration error;
	vcpu_state		output enumeration vcpu_run_state;
	state_data_0		output type register_t;
	state_data_1		output type register_t;
	state_data_2		output type register_t;
};

define vcpu_run_check hypercall {
	call_num		0x68;
	cap_id			input type cap_id_t;
	res0			input uregister;
	error			output enumeration error;
	vcpu_state		output enumeration vcpu_run_state;
	state_data_0		output type register_t;
	state_data_1		output type register_t;
	state_data_2		output type register_t;
};

```

`hyp/interfaces/vcpu_run/vcpu_run.tc`:

```tc
// © 2022 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#if defined(HYPERCALLS)
extend hyp_api_flags0 bitfield {
	delete	vcpu_run;
	11	vcpu_run bool = 1;
};
#endif
define vcpu_run_state public enumeration(explicit) {
	// VCPU is ready to run on the next vcpu_run hypercall.
	ready = 0;

	// VCPU is sleeping until an interrupt arrives. The wakeup IRQ will be
	// asserted when that occurs.
	//
	// The first state data word contains a platform-specific description
	// of the sleep state. For example, for AArch64 VMs with PSCI enabled,
	// this contains the PSCI suspend state.
	//
	// The second state data word contains a vcpu_run_wakeup_from_state
	// enumeration, which is a platform-specific description of the reason
	// the VCPU is expecting a wakeup.
	expects_wakeup = 1;

	// VCPU is powered off and cannot execute until another VCPU triggers
	// a power-on event. The wakeup IRQ will be asserted when that occurs.
	// The first state data word will be a vcpu_run_poweroff_flags bitmap.
	powered_off = 2;

	// VCPU is blocked in EL2 for an unspecified reason. This state is
	// normally transient, and the EL1 caller should retry after yielding.
	blocked = 3;

	// VCPU has an unrecoverable fault.
	fault = 6;
};

// Type of the first state data word for VCPU_RUN_STATE_POWERED_OFF.
define vcpu_run_poweroff_flags public bitfield<32> {
	// True if the VCPU has permanently exited and cannot be powered on.
	0		exited		bool;
	others		unknown = 0;
};

define vcpu_run_wakeup_from_state public enumeration(explicit) {
	// For backwards compatibility with hypervisors that didn't specify
	// the cause of the sleep. Should not be used.
	unspecified = 0;
};

```

`hyp/interfaces/vdevice/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

events vdevice.ev
types vdevice.tc

```

`hyp/interfaces/vdevice/include/vdevice.h`:

```h
// © 2022 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// Configure a vdevice that is backed by a physical memory extent.
//
// The given memextent is presumed to be mapped (either before or after this
// call) with reduced permissions, typically read-only, in the guest's address
// space. Any permission fault received for this memextent will be forwarded to
// the access handler for the vdevice. The vdevice's type must be set before
// calling this function.
//
// The caller should ensure that the memextent meets any requirements it has for
// size, memory type / cache attributes, permissions, etc. Normally this would
// be done by calling memextent_attach().
error_t
vdevice_attach_phys(vdevice_t *vdevice, memextent_t *memextent);

// Tear down a vdevice's attachment to a physical memory extent. This must only
// be called after receiving an OK result from vdevice_attach_phys().
//
// Note that calls to the access handler are not guaranteed to be complete until
// an RCU grace period has elapsed after calling this function. If the access
// handler makes use of a pointer to or mapping of the memextent, the caller
// should not release or unmap the memextent until a grace period has elapsed.
void
vdevice_detach_phys(vdevice_t *vdevice, memextent_t *memextent);

// Configure a vdevice that is not backed by physical memory.
//
// After this call succeeds, any translation faults in the specified range will
// be forwarded to the access handler for the vdevice. The vdevice's type must
// be set before calling this function.
//
// The given address range in the addrspace is presumed to not be mapped to any
// physical memextent. If such a mapping exists or is created later, it may
// shadow the device.
//
// The caller is responsible for ensuring that calls to this function are
// serialised for each device. Note that multiple calls are not generally useful
// because only one attachment is allowed.
//
// This function will retain a reference to the specified address space.
error_t
vdevice_attach_vmaddr(vdevice_t *vdevice, addrspace_t *addrspace, vmaddr_t ipa,
		      size_t size);

// Tear down a vdevice's attachment to a guest address range. This must only
// be called after receiving an OK result from vdevice_attach_vmaddr().
//
// Note that calls to the access handler are not guaranteed to be complete and
// it is not safe to call vdevice_attach_vmaddr() again until an RCU grace
// period has elapsed after calling this function.
void
vdevice_detach_vmaddr(vdevice_t *vdevice);

```

`hyp/interfaces/vdevice/vdevice.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

interface vdevice

handled_event vdevice_access_fixed_addr
	param ipa:		vmaddr_t
	param access_size:	size_t
	param value:		register_t *
	param is_write:		bool
	return: vcpu_trap_result_t = VCPU_TRAP_RESULT_UNHANDLED

selector_event vdevice_access
	selector type_:		vdevice_type_t
	param vdevice:		vdevice_t *
	param offset:		size_t
	param access_size:	size_t
	param value:		register_t *
	param is_write:		bool
	return: vcpu_trap_result_t = VCPU_TRAP_RESULT_UNHANDLED

```

`hyp/interfaces/vdevice/vdevice.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

define vdevice_type enumeration {
	NONE = 0;
};

define vdevice structure {
	type		enumeration vdevice_type;
};

```

`hyp/interfaces/vectors/armv8/traps.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

interface vectors

// Hypervisor trap events
//
// These are all called directly from the hypervisor exception. It is not safe
// to call any function that might enable preemption.

handled_event vectors_trap_unknown_el2
	param frame: kernel_trap_frame_t *

handled_event vectors_trap_illegal_state_el2

handled_event vectors_trap_pf_abort_el2
	param esr: ESR_EL2_t

handled_event vectors_trap_pc_alignment_fault_el2

handled_event vectors_trap_data_abort_el2
	param esr: ESR_EL2_t

handled_event vectors_trap_sp_alignment_fault_el2

handled_event vectors_trap_brk_el2
	param esr: ESR_EL2_t

#if defined(ARCH_ARM_FEAT_PAuth) && defined(ARCH_ARM_FEAT_FPAC)
handled_event vectors_trap_pauth_failed_el2
	param iss: ESR_EL2_t
#endif

```

`hyp/interfaces/vectors/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

arch_events armv8 traps.ev

```

`hyp/interfaces/vet/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

types vet.tc

```

`hyp/interfaces/vet/include/vet.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// The Virtual Embedded Trace (VET) interface.

// The vet_ordering variable is used as an artificial assembly ordering
// dependency for modules implementing this API. It orders individual asm
// statements with respect to each other in a way that is lighter weight than a
// full "memory" clobber.
extern asm_ordering_dummy_t vet_ordering;

// Flush data for trace unit.
//
// Since a HW trace unit may have delays in transferring the trace byte stream
// to system infrastructure, we may need to explicitly flush it to ensure the
// trace stream is observable (mostly the trace buffer unit).
void
vet_flush_trace(thread_t *self);

// Disable trace unit.
//
// Trace unit should be configured to not generate additional trace data after
// disabling.
void
vet_disable_trace(void);

// Enable trace unit.
void
vet_enable_trace(void);

// Save current trace unit's thread context.
//
// After thread context is saved, access to the trace unit registers is
// disabled.
//
// The implementation depends on the configured policy. This can save all
// registers or just control the trace's enable/disable.
void
vet_save_trace_thread_context(thread_t *self) REQUIRE_PREEMPT_DISABLED;

// Restore a thread's trace buffer unit context.
//
// This reverses the actions of vet_save_trace_thread_context.
void
vet_restore_trace_thread_context(thread_t *self) REQUIRE_PREEMPT_DISABLED;

// Save trace unit context for local CPU before suspend.
//
// Note that this may modify the trace unit state, so an aborted suspend must
// be followed by a call to vet_restore_trace_power_context().
void
vet_save_trace_power_context(bool may_poweroff) REQUIRE_PREEMPT_DISABLED;

// Restore trace unit context for local CPU after resume or aborted suspend.
void
vet_restore_trace_power_context(bool was_poweroff) REQUIRE_PREEMPT_DISABLED;

// Flush data in the trace buffer unit.
//
// After this flush, all data pending in the trace buffer should be committed
// to memory. The implementation should ensure that this completes in finite
// time. If the trace buffer is located in memory with normal non-cacheable or
// device memory attributes, the write of trace data reaches the endpoint of
// that location in finite time.
void
vet_flush_buffer(thread_t *self);

// Disable trace buffer unit.
//
// After disabling the trace buffer, it still host software stack's
// responsibility to check if all data is written out to the buffer.
void
vet_disable_buffer(void);

// Enable trace buffer unit.
void
vet_enable_buffer(void);

// Save trace buffer unit thread context before power-off.
//
// Similar to vet_save_trace_thread_context, this may save trace buffer
// registers / information. However, it does not change any configuration
// and does not need to be called for non-poweroff suspends.
void
vet_save_buffer_thread_context(thread_t *self) REQUIRE_PREEMPT_DISABLED;

// Restore trace buffer unit thread context after power-off.
//
// This must be called when resuming from a power-off state. It need not be
// called when resuming from a retention state or aborting a power-off suspend.
void
vet_restore_buffer_thread_context(thread_t *self) REQUIRE_PREEMPT_DISABLED;

// Save trace buffer context for local CPU before power-off.
//
// This does not need to save any information which is already saved by thread
// context.
// NOTE: if register access is disabled, then we need to enable it before save/
// restore of the context.
void
vet_save_buffer_power_context(void) REQUIRE_PREEMPT_DISABLED;

// Restore trace buffer context for local CPU after power-on.
void
vet_restore_buffer_power_context(void) REQUIRE_PREEMPT_DISABLED;

// Update trace unit status for the current thread.
//
// This function checks the thread's current usage of trace infrastructure
// to guide the subsequent context-switch behaviour such as saving context.
void
vet_update_trace_unit_status(thread_t *self);

// Update trace buffer status for the current thread
//
// Similar to vet_update_trace_unit_status() for the trace buffer.
void
vet_update_trace_buffer_status(thread_t *self);

```

`hyp/interfaces/vet/vet.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

extend platform_cpu_features bitfield {
	auto		trace_disable	bool = 1;
};

```

`hyp/interfaces/vgic/include/vgic.h`:

```h
// © 2023 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

const platform_mpidr_mapping_t *
vgic_get_mpidr_mapping(const vic_t *vic);

```

`hyp/interfaces/vic/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

first_class_object vic
hypercalls vic.hvc
types vic.tc
events vic.ev

```

`hyp/interfaces/vic/include/vic.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// Interfaces for managing virtual interrupt controllers.
//
// Virtual IRQs are delivered by a virtual interrupt controller (VIC), which is
// a first-class object with defined relationships to a set of VCPUs. The VIC
// may implement a hypercall-based VM interface and/or emulate a hardware
// interrupt controller's register interface for delivering these interrupts.
//
// Each VIRQ uses one of two two routing types, specified by the VIRQ source
// when registering it with the controller, to select a VCPU to deliver to.
// These routing types are:
//
// - Private: targeted at a pre-determined VCPU, and with assertion state that
//   is specific ta that VCPU.
//
// - Shared: there is a single common state shared by all VCPUs. A target VCPU
//   is selected at runtime, based on implementation-defined criteria which may
//   be modified by the VM. Note that this may include affinity to a particular
//   VCPU, but unlike private routing this affinity is not an inherent property
//   of the VIRQ: it may be modifiable at runtime, may target more than one
//   VCPU, and may be influenced by the current state of the target VCPU.
//
// Note that the implementation may restrict shared and/or private VIRQ numbers
// to specific ranges which may or may not overlap. This interface does not
// provide any means of querying those ranges; the caller must either know them,
// or obtain VIRQ numbers from a resource manager VM that knows them.
//
// To register a VIRQ, call one of the vic_bind_* functions with a source
// structure owner by the caller, specifying an interrupt controller object,
// a VIRQ number in an appropriate range, and a triggering type defined by the
// caller. The type value will be used as a selector for the virq_check_pending
// event, which is triggered when a level-sensitive interrupt is synchronised.
//
// The caller is responsible for calling vic_unbind() on the source structure
// in the cleanup handler of the object containing the source structure.
//
// The caller must either hold references to all specified object(s) (including
// the object that contains the VIRQ source structure), or else be in an RCU
// read-side critical section.

// Get a pointer to a thread's vic
vic_t *
vic_get_vic(const thread_t *vcpu);

// Exclusively claim a shared VIRQ on the specified VIC.
//
// This prevents the VIRQ being claimed by any other source, and allows calls
// to virq_assert() and virq_clear().
//
// Note that this does not take a reference to the VIC. If the VIC is later
// freed, calls to virq_assert() will fail.
//
// This function should be used in preference to vic_bind_private_*() for any
// VIRQ that is not inherently bound to a VCPU. This includes nearly all VIRQs
// generated by first class objects other than the VCPU itself.
//
// Returns ERROR_VIRQ_BOUND if the specified source object has previously been
// bound to VIRQ, and not subsequently unbound by calling vic_unbind_sync().
// Returns ERROR_BUSY if the specified VIRQ has already been claimed. Returns
// ERROR_ARGUMENT_INVALID if the specified VIRQ number is out of range.
error_t
vic_bind_shared(virq_source_t *source, vic_t *vic, virq_t virq,
		virq_trigger_t trigger);

// Exclusively claim a private VIRQ on the specified VCPU.
//
// There are two variants of this function which differ only in the way the
// target is specified: one accepts a VCPU object pointer, the other a VIC
// object pointer and a VCPU attachment index.
//
// This operates the same way as vic_bind_shared(), but for private
// (VCPU-local) VIRQs. Note that it must be called for each VCPU that will
// receive the interrupt, with separate source objects. It is strongly
// recommended to repeat this call for every VCPU in the VM, using the same VIRQ
// number each time.
//
// Normally this function should only be used for VIRQs that are inherently
// associated with a particular VCPU and can only reasonably be handled by that
// VCPU; e.g. local timers or performance monitors. Anything else should use the
// shared version instead.
//
// Returns ERROR_VIRQ_BOUND if the specified source object has previously been
// bound to VIRQ, and not subsequently unbound by calling vic_unbind_sync().
// Returns ERROR_BUSY if the specified VIRQ has already been claimed. Returns
// ERROR_ARGUMENT_INVALID if the specified VIRQ number is out of range. Returns
// ERROR_OBJECT_CONFIG if the specified index does not have an associated VCPU
// or the attachment between the VCPU and VIC is concurrently broken.
error_t
vic_bind_private_vcpu(virq_source_t *source, thread_t *vcpu, virq_t virq,
		      virq_trigger_t trigger);
error_t
vic_bind_private_index(virq_source_t *source, vic_t *vic, index_t index,
		       virq_t virq, virq_trigger_t trigger);

// Release an exclusive claim to a VIRQ.
//
// Note that if the VIRQ source is currently pending, it will be cleared, as if
// virq_clear() was called. However, like virq_clear(), this function does not
// wait for cancellation of the specified VIRQ on every registered VCPU. If the
// VIRQ is currently asserted and routed to a VCPU that is active on a remote
// physical CPU, the interrupt may be spuriously delivered to the VM shortly
// after this function returns.
//
// The caller must ensure that an RCU grace period elapses between the return
// of this function and the deallocation of the storage containing the source
// structure. Note that this requirement is satisfied by calling this function
// from the enclosing first-class object's deactivate event handler.
//
// Any attempt to reuse the source structure for a new vic_bind_*() call is
// permitted to fail as if this function had not been called, even if an RCU
// grace period has elapsed.
//
// If the source has not claimed a VIRQ, or was claimed for a VIC or VCPU that
// has since been destroyed, this function has no effect.
void
vic_unbind(virq_source_t *source);

// Release an exclusive claim to a VIRQ and make the source ready for reuse.
//
// This function performs the same operation as vic_unbind(); additionally, it
// waits for the implicit virq_clear() operation to complete, and then resets
// the source so that it may be used by a subsequent vic_bind_*() call.
//
// This function may call the scheduler, and therefore must not be called from
// an RCU read-side critical section or a spinlock.
void
vic_unbind_sync(virq_source_t *source);

```

`hyp/interfaces/vic/vic.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

interface vic

selector_event vic_bind_hwirq
	selector action: hwirq_action_t
	param vic: vic_t *
	param hwirq: hwirq_t *
	param virq: virq_t
	return: error_t = ERROR_ARGUMENT_INVALID

selector_event vic_unbind_hwirq
	selector action: hwirq_action_t
	param hwirq: hwirq_t *
	return: error_t = ERROR_ARGUMENT_INVALID

handled_event vic_bind_msi_source
	param vic: vic_t *
	param msi_source_cap: cap_id_t
	return: error_t = ERROR_CSPACE_WRONG_OBJECT_TYPE

```

`hyp/interfaces/vic/vic.hvc`:

```hvc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

define hwirq_bind_virq hypercall {
	call_num 0x26;
	hwirq	input type cap_id_t;
	vic	input type cap_id_t;
	virq	input type virq_t;
	res0	input uregister;
	error	output enumeration error;
};

define hwirq_unbind_virq hypercall {
	call_num 0x27;
	hwirq	input type cap_id_t;
	res0	input uregister;
	error	output enumeration error;
};

define vic_configure hypercall {
	call_num 0x28;
	vic		input type cap_id_t;
	max_vcpus	input type count_t;
	max_virqs	input type count_t;
	vic_options	input bitfield vic_option_flags;
	// Virtual LPIs for GICv3/v4. Valid only if max_msis_valid is set in
	// vic_options; otherwise ignored for backwards compatibility.
	max_msis	input type count_t;
	error		output enumeration error;
};

define vic_attach_vcpu hypercall {
	call_num 0x29;
	vic		input type cap_id_t;
	vcpu		input type cap_id_t;
	index		input type index_t;
	res0		input uregister;
	error		output enumeration error;
};

define vic_bind_msi_source hypercall {
	call_num 0x56;
	vic		input type cap_id_t;
	// Platform-specific object type. Must have already been activated.
	//
	// Some platforms may not implement any MSI sources, or multiple types
	// of MSI source. For GICv3/v4, this is typically a virtual ITS.
	msi_source	input type cap_id_t;
	res0		input uregister;
	error		output enumeration error;
};

```

`hyp/interfaces/vic/vic.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#if !defined(IRQ_NULL)
extend cap_rights_hwirq bitfield {
	1	bind_vic	bool;
};
#endif

extend cap_rights_vic bitfield {
	0	bind_source	bool;
};

#if defined(HYPERCALLS)
extend hyp_api_flags0 bitfield {
	delete	vic;
	3	vic bool = 1;
};
#endif

define vic_option_flags public bitfield<64> {
	0	max_msis_valid		bool = 1;
	1	disable_default_addr	bool = 1;
	63:2	res0_0			uregister = 0;
};

```

`hyp/interfaces/virq/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

types virq.tc
events virq.ev

```

`hyp/interfaces/virq/include/virq.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// Interfaces for asserting virtual IRQs.
//
// Hardware-sourced interrupts may be handled internally by the VIC if some
// degree of hardware acceleration is available, e.g. for ARM GICv2 and later.
// Otherwise, they will be handled by a module that uses these APIs.
//
// Note that concurrent calls to the virq_assert() and virq_clear() functions
// for the same source may leave the VIRQ in an unknown state. Callers that
// expect level-triggered behaviour must serialise calls to these functions.
//
// The caller must either hold references to all specified object(s), or
// else be in an RCU read-side critical section.

// Assert a VIRQ source.
//
// If the edge_only parameter is true, then the VIRQ will only be asserted if
// it is configured for edge triggering, as if a hardware line had experienced
// an arbitrarily short transient signal (or was triggered by a message). This
// avoids the need to either call virq_clear() or register a virq_check_pending
// handler, and is more efficient than either of those options. Note that the
// VIC implementation may not support edge triggering for any specific VIRQ.
// Also note that the effect of such a call on a VIRQ source that was previously
// asserted with the edge_only parameter set to false is unpredictable.
//
// If the source has not claimed a VIRQ, or the target VIC or VCPU has been
// destroyed, this function returns ERROR_VIRQ_NOT_BOUND.
//
// On success, this function returns a boolean value which is true if the IRQ
// was delivered with edge triggering enabled.
bool_result_t
virq_assert(virq_source_t *source, bool edge_only);

// Deassert a level-triggered VIRQ source.
//
// This function has no effect for an edge-triggered VIRQ. It only affects VIRQs
// that either do not support edge triggering, or else have been configured for
// level triggering by the VM. Note that this configuration might change at
// runtime without notifying the VIRQ source, and there is no mechanism to query
// the current configuration.
//
// Returning false from a handler for the virq_check_pending event has the same
// effect as calling this function. The event handler is typically more
// efficient, but requires lock-free synchronisation that is not practical in
// some cases.
//
// Note that this function does not wait for cancellation of the specified VIRQ
// on every registered VCPU. If the VIRQ is currently asserted and routed to a
// VCPU that is active on a remote physical CPU, the interrupt may be spuriously
// delivered to the VM shortly after this function returns.
//
// If the source has not claimed a VIRQ, or the target VIC or VCPU has been
// destroyed, this function returns ERROR_VIRQ_NOT_BOUND. Otherwise, it returns
// OK, regardless of the prior state of the interrupt.
error_t
virq_clear(virq_source_t *source);

// Query whether a level-triggered VIRQ source is currently asserted.
//
// Returns true if the VIRQ has been asserted by a virq_assert() call (with
// edge_only set to false) and has not subsequently been cleared by either a
// call to virq_clear() or a false result from the virq_check_pending event.
//
// If the source has not claimed a VIRQ, or the target VIC or VCPU has been
// destroyed, this function returns ERROR_VIRQ_NOT_BOUND.
bool_result_t
virq_query(virq_source_t *source);

```

`hyp/interfaces/virq/virq.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

interface virq

// The virq_check_pending event is triggered to poll the status of a level-
// triggered virtual interrupt.
//
// If a handler is registered, and returns false, the VIRQ's pending state
// may be cleared as if virq_clear() had been called. This is typically more
// efficient than actually calling virq_clear(), especially if cross-CPU
// delivery is possible, e.g. for IPC objects.
//
// However, the result of the handler will not necessarily affect the VIRQ
// source's state. Changing the state may fail due to any concurrent VIRQ or
// VIC operation, or spuriously. In this case, the event will be triggered
// again, unless the concurrent operation made it redundant.
//
// The VIC implementation must guarantee that as soon as practical after an
// EOI by the VM for any VIRQ that has been delivered in level-triggering mode
// after being asserted by its VIRQ source, this event will be triggered
// repeatedly for that source until at least one of the following has
// occurred:
//
// 1.	the handler for this event returns false, and the level-pending flag is
//	successfully cleared;
//
// 2.	the handler for this event returns true (which is the default when
//	there is no registered handler); or
//
// 3.	a call to virq_clear() has completed successfully for this source at
//	any time after the VM last acknowledged the VIRQ. Note that if
//	virq_clear() is called early enough (i.e. completing after the VM
//	acknowledges the IRQ but before EOI), this event may not be triggered.
//
// The event may not be triggered in a timely manner, or at all, after an EOI
// of a VIRQ that was delivered in edge-triggering mode.
//
// In addition to the guaranteed triggers above, this event may be triggered
// at any other time while the interrupt is marked pending for level-triggered
// delivery. Any memory reads executed by the handler are guaranteed to be
// ordered after the read that determines that the interrupt is marked pending
// for level-triggered delivery.
//
// The reasserted argument is true if the source has been asserted more
// recently than the last acknowledgement of its VIRQ by the VM. This may
// include a previous true result from this handler. Any memory reads executed
// by the handler are guaranteed to be ordered after the read that determines
// the value of the reasserted argument.
//
// If the reasserted argument is false, then a false return from the handler is
// guaranteed to take effect only if it is ordered before any virq_assert(),
// and to have no effect otherwise. This guarantee is not provided if the
// reasserted argument is true.
//
// Apart from the above ordering guarantees, it is not generally possible to
// serialise handlers for this event and callers of virq_assert() and
// virq_clear(). In particular, handlers MUST NOT acquire any spinlock or
// equivalent that is held during virq_assert() or virq_clear() calls, because
// they may be called holding locks internal to the VIC implementation.
//
// This event may be called for a VIRQ that is asserted on a CPU while context
// switching to that CPU. This occurs in a thread_context_switch_post handler
// with lowered priority, so any context that the handler relies on must be
// restored at or before the default priority of thread_context_switch_post.
selector_event virq_check_pending
	selector trigger: virq_trigger_t
	param source: virq_source_t *
	param reasserted: bool
	return: bool = true

// The virq_set_enabled event is triggered when the VM enables or disables the
// specified VIRQ source.
//
// The handler need not take any action, especially for virtual interrupts,
// since the VIC code already checks its internally-maintained enable bit
// before forwarding VIRQs to the VM. It is provided mainly to allow hardware
// interrupt sources to be disabled when not needed.
selector_event virq_set_enabled
	selector trigger: virq_trigger_t
	param source: virq_source_t *
	param enabled: bool

// The virq_set_mode event is triggered when the VM attempts to switch the
// VIRQ between edge-triggered and level-triggered modes.
//
// The return value is the mode that was actually selected, which may not be
// the mode that was requested if the source only supports a limited range of
// modes.
//
// If no handler is registered, all mode changes will be accepted.
selector_event virq_set_mode
	selector trigger: virq_trigger_t
	param source: virq_source_t *
	param mode: irq_trigger_t
	return: irq_trigger_result_t = irq_trigger_result_ok(mode)

```

`hyp/interfaces/virq/virq.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

define virq_t public newtype uint32;

define virq_trigger enumeration {
	// Used for VIRQs with no virq_check_pending handler; if level
	// triggered, the source must call virq_clear().
	explicit;
};

define virq_source structure {
	virq		type virq_t;
	trigger		enumeration virq_trigger;
	// RCU-protected pointer to the targeted controller.
	vic		pointer(atomic) object vic;
	is_private	bool;
};

extend error enumeration {
	VIRQ_BOUND = 40;
	VIRQ_NOT_BOUND = 41;
};

```

`hyp/interfaces/virtio_mmio/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

first_class_object virtio_mmio
types virtio_mmio.tc
hypercalls virtio_mmio.hvc

```

`hyp/interfaces/virtio_mmio/virtio_mmio.hvc`:

```hvc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

define virtio_mmio_configure hypercall {
	call_num	0x49;
	virtio_mmio	input type cap_id_t;
	memextent	input type cap_id_t;
	vqs_num		input type count_t;
	flags		input bitfield virtio_option_flags;
	device_type		input enumeration virtio_device_type;
	error		output enumeration error;
};

define virtio_mmio_backend_bind_virq hypercall {
	call_num	0x4a;
	virtio_mmio	input type cap_id_t;
	vic		input type cap_id_t;
	virq		input type virq_t;
	res0		input uregister;
	error		output enumeration error;
};

define virtio_mmio_backend_unbind_virq hypercall {
	call_num	0x4b;
	virtio_mmio	input type cap_id_t;
	res0		input uregister;
	error		output enumeration error;
};

define virtio_mmio_frontend_bind_virq hypercall {
	call_num	0x4c;
	virtio_mmio	input type cap_id_t;
	vic		input type cap_id_t;
	virq		input type virq_t;
	res0		input uregister;
	error		output enumeration error;
};

define virtio_mmio_frontend_unbind_virq hypercall {
	call_num	0x4d;
	virtio_mmio	input type cap_id_t;
	res0		input uregister;
	error		output enumeration error;
};

define virtio_mmio_backend_assert_virq hypercall {
	call_num		0x4e;
	virtio_mmio		input type cap_id_t;
	interrupt_status	input uint32;
	res0			input uregister;
	error			output enumeration error;
};

define virtio_mmio_backend_set_dev_features hypercall {
	call_num	0x4f;
	virtio_mmio	input type cap_id_t;
	sel		input uint32;
	dev_feat	input uint32;
	res0		input uregister;
	error		output enumeration error;
};

define virtio_mmio_backend_set_queue_num_max hypercall {
	call_num	0x50;
	virtio_mmio	input type cap_id_t;
	sel		input uint32;
	queue_num_max	input uint32;
	res0		input uregister;
	error		output enumeration error;
};

define virtio_mmio_backend_get_drv_features hypercall {
	call_num	0x51;
	virtio_mmio	input type cap_id_t;
	sel		input uint32;
	res0		input uregister;
	error		output enumeration error;
	drv_feat	output uint32;
};

define virtio_mmio_backend_get_queue_info hypercall {
	call_num	0x52;
	virtio_mmio	input type cap_id_t;
	sel		input uint32;
	res0		input uregister;
	error		output enumeration error;
	queue_num	output uint32;
	queue_ready	output uint32;
	queue_desc	output uint64;
	queue_drv	output uint64;
	queue_dev	output uint64;
};

define virtio_mmio_backend_get_notification hypercall {
	call_num	0x53;
	virtio_mmio	input type cap_id_t;
	res0		input uregister;
	error		output enumeration error;
	vqs_bitmap	output type register_t;
	reason		output bitfield virtio_mmio_notify_reason;
};

define virtio_mmio_backend_acknowledge_reset hypercall {
	call_num	0x54;
	virtio_mmio	input type cap_id_t;
	res0		input uregister;
	error		output enumeration error;
};

define virtio_mmio_backend_update_status hypercall {
	call_num	0x55;
	virtio_mmio	input type cap_id_t;
	val		input uint32;
	res0		input uregister;
	error		output enumeration error;
};

```

`hyp/interfaces/virtio_mmio/virtio_mmio.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#if defined(HYPERCALLS)
extend hyp_api_flags0 bitfield {
	delete	virtio_mmio;
	9	virtio_mmio bool = 1;
};
#endif

define virtio_mmio_notify_reason public bitfield<64> {
	0	new_buffer	bool = 0;
	1	reset_rqst	bool = 0;
	2	res0_irq_ack	bool(const); // Formerly interrupt_ack
	3	driver_ok	bool = 0;
	4	failed		bool = 0;
	others	unknown = 0;
};

define virtio_option_flags public bitfield<64> {
	5:0		unknown = 0;
	6		valid_device_type	bool = 0;
	63:7		res0			uint64 = 0;
};

define virtio_config_space union {
	raw		array(VIRTIO_MMIO_REG_CONFIG_BYTES) uint8(atomic);
};

define virtio_device_type public enumeration(explicit) {
	INVALID = 0;
};

```

`hyp/interfaces/vpm/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

first_class_object vpm_group
types vpm.tc
events vpm.ev
hypercalls vpm.hvc

```

`hyp/interfaces/vpm/include/vpm.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#define vpm__vcpus_state_foreach(cpu_index, cpu_state, vcpus_state, i)         \
	cpu_state =                                                            \
		(psci_cpu_state_t)(uint32_t)(vcpus_state &                     \
					     PSCI_VCPUS_STATE_PER_VCPU_MASK);  \
	cpu_index = 0;                                                         \
	for (index_t i = 0; i < PSCI_VCPUS_STATE_MAX_INDEX;                    \
	     i += PSCI_VCPUS_STATE_PER_VCPU_BITS, cpu_index++,                 \
		     cpu_state =                                               \
			     (psci_cpu_state_t)(uint32_t)((vcpus_state >> i) & \
							  PSCI_VCPUS_STATE_PER_VCPU_MASK))

#define vpm_vcpus_state_foreach(cpu_index, cpu_state, vcpus_state)             \
	vpm__vcpus_state_foreach((cpu_index), (cpu_state), (vcpus_state),      \
				 util_cpp_unique_ident(i))

error_t
vpm_group_configure(vpm_group_t *vpm_group, vpm_group_option_flags_t flags);

error_t
vpm_attach(vpm_group_t *pg, thread_t *thread, index_t index);

error_t
vpm_bind_virq(vpm_group_t *vpm_group, vic_t *vic, virq_t virq);

void
vpm_unbind_virq(vpm_group_t *vpm_group);

vpm_state_t
vpm_get_state(vpm_group_t *vpm_group);

```

`hyp/interfaces/vpm/vpm.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

interface vpm

// Triggered when the last VCPU of a VPM group is requesting to enter to a
// suspend state.
// This event is called with the scheduler lock held.
event vpm_system_suspend
	param vcpu: thread_t *

// Triggered when the first VCPU of a VPM group wakes up from a suspend state.
// This event is called with the scheduler lock held.
event vpm_system_resume
	param vcpu: thread_t *

```

`hyp/interfaces/vpm/vpm.hvc`:

```hvc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

define vpm_group_configure hypercall {
	call_num	0x66;
	vpm_group	input type cap_id_t;
	flags		input type vpm_group_option_flags;
	res0		input uregister;
	error		output enumeration error;
};

define vpm_group_attach_vcpu hypercall {
	call_num	0x3c;
	vpm_group	input type cap_id_t;
	vcpu		input type cap_id_t;
	index		input type index_t;
	res0		input uregister;
	error		output enumeration error;
};

define vpm_group_bind_virq hypercall {
	call_num	0x43;
	vpm_group	input type cap_id_t;
	vic		input type cap_id_t;
	virq		input type virq_t;
	res0		input uregister;
	error		output enumeration error;
};

define vpm_group_unbind_virq hypercall {
	call_num	0x44;
	vpm_group	input type cap_id_t;
	res0		input uregister;
	error		output enumeration error;
};

define vpm_group_get_state hypercall {
	call_num	0x45;
	vpm_group	input type cap_id_t;
	res0		input uregister;
	error		output enumeration error;
	vpm_state	output uregister;
};

```

`hyp/interfaces/vpm/vpm.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#if defined(HYPERCALLS)
extend hyp_api_flags0 bitfield {
	delete	vpm;
	4	vpm bool = 1;
};
#endif

extend cap_rights_vpm_group bitfield {
	0	attach_vcpu	bool;
	1	bind_virq	bool;
	2	query		bool;
};

define vpm_group_option_flags public bitfield<64> {
	0	no_aggregation	bool;
	others	unknown=0;
};

define vpm_state public enumeration(explicit) {
	no_state = 0;		// Invalid / non existent
	running = 1;		// VPM is active
	cpus_suspended = 2;	// VPM is suspended after a CPU_SUSPEND call
	system_suspended = 3;	// VPM is suspended after a SYSTEM_SUSPEND call
};

```

`hyp/interfaces/vrtc/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

first_class_object vrtc
hypercalls vrtc.hvc

```

`hyp/interfaces/vrtc/vrtc.hvc`:

```hvc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

define partition_create_vrtc hypercall {
	vendor_hyp_call;
	src_partition	input type cap_id_t;
	cspace		input type cap_id_t;
	res0		input uregister;
	error		output enumeration error;
	new_cap		output type cap_id_t;
};

define vrtc_configure hypercall {
	vendor_hyp_call;
	vrtc		input type cap_id_t;
	ipa		input type vmaddr_t;
	res0		input uregister;
	error		output enumeration error;
};

define vrtc_set_time_base hypercall {
	vendor_hyp_call;
	vrtc		input type cap_id_t;
	time_base	input type nanoseconds_t;
	sys_timer_ref	input type ticks_t;
	res0		input uregister;
	error		output enumeration error;
};

define vrtc_attach_addrspace hypercall {
	vendor_hyp_call;
	vrtc		input type cap_id_t;
	addrspace	input type cap_id_t;
	res0		input uregister;
	error		output enumeration error;
};

```

`hyp/interfaces/wait_queue/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

events wait_queue.ev
types wait_queue.tc

```

`hyp/interfaces/wait_queue/include/wait_queue.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// A simple wait_queue for blocking threads while waiting on events.
// TODO: add support for interruption - e.g. destroying threads

// Ordering in the wait_queue API:
//
// An acquire operation is implied by any wait_queue_wait() call that sleeps;
// and a release operation on the wait queue is implied by any
// wait_queue_wakeup() call that wakes up at least one thread.

// Initialise the wait_queue.
void
wait_queue_init(wait_queue_t *wait_queue);

// Enqueue calling thread on the wait_queue.
//
// Must be called before calling wait_queue_get(), and finally the caller
// should call wait_queue_finish() when done.
void
wait_queue_prepare(wait_queue_t *wait_queue)
	ACQUIRE_PREEMPT_DISABLED ACQUIRE_LOCK(wait_queue);

// The calling thread enters a critical section where it is safe to perform the
// condition check without races. If the condition passes, then call
// wait_queue_put(), else call wait_queue_wait().
//
// wait_queue_prepare() must have been called prior to this.
void
wait_queue_get(void)
	REQUIRE_PREEMPT_DISABLED ACQUIRE_LOCK(wait_queue_condition);

// Exit the wait_queue critical section.
//
// Must be called after wait_queue_get() and the subsequent condition check
// succeeded. Must not be called after wait_queue_wait().
void
wait_queue_put(void)
	REQUIRE_PREEMPT_DISABLED RELEASE_LOCK(wait_queue_condition);

// Atomically exit the wait_queue critical section and block until a wakeup
// event.
//
// May be called after wait_queue_get() and the subsequent condition check
// fails and we want to yield. Must not be called after wait_queue_put().
void
wait_queue_wait(void)
	REQUIRE_PREEMPT_DISABLED RELEASE_LOCK(wait_queue_condition);

// Dequeue the thread from the wait_queue. Call this when woken up and the wait
// condition now passes, after wait_queue_put() or wait_queue_wait().
void
wait_queue_finish(wait_queue_t *wait_queue)
	RELEASE_PREEMPT_DISABLED RELEASE_LOCK(wait_queue)
		EXCLUDE_LOCK(wait_queue_condition);

// Perform a wakeup event on the wait_queue
void
wait_queue_wakeup(wait_queue_t *wait_queue);

```

`hyp/interfaces/wait_queue/wait_queue.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

interface wait_queue

```

`hyp/interfaces/wait_queue/wait_queue.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// A wait-queue structure, to be embedded in an object.
define wait_queue structure(lockable) {
	list	structure list;
	lock	structure spinlock;
};

extend thread object module wait_queue {
	list_node	structure list_node(contained);
};

// Used to validate correct ordering of wait_queue_get/wait/put calls.
define wait_queue_condition global structure opaque_lock;

```

`hyp/ipc/doorbell/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

interface doorbell
local_include
source doorbell.c hypercalls.c
events doorbell.ev
types doorbell.tc

```

`hyp/ipc/doorbell/doorbell.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

module doorbell

subscribe object_create_doorbell

subscribe object_deactivate_doorbell

subscribe virq_check_pending[VIRQ_TRIGGER_DOORBELL](source, reasserted)

```

`hyp/ipc/doorbell/doorbell.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

define doorbell_flags_t newtype uint64;

extend cap_rights_doorbell bitfield {
	0	send	bool;
	1	receive	bool;
	2	bind	bool; // right to bind virqs
};

extend doorbell object {
	flags		uint64;
	enable_mask	uint64;
	ack_mask	uint64;
	source		structure virq_source(contained);
	lock		structure spinlock;
};

extend virq_trigger enumeration {
	doorbell;
};

```

`hyp/ipc/doorbell/include/doorbell.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// Sets flags of the doorbell. Returns old flags
doorbell_flags_result_t
doorbell_send(doorbell_t *doorbell, doorbell_flags_t new_flags);

// Reads and clears the flags of the doorbell. Returns old flags.
doorbell_flags_result_t
doorbell_receive(doorbell_t *doorbell, doorbell_flags_t clear_flags);

// Clears all flags and sets all bits in the mask of the doorbell.
error_t
doorbell_reset(doorbell_t *doorbell);

// Sets the masks of the doorbell. The Enable Mask is the mask of set flags
// that will cause an assertion of the virtual interrupt bound to the doorbell.
// The Ack Mask controls which flags should be automatically cleared when the
// interrupt is asserted.
error_t
doorbell_mask(doorbell_t *doorbell, doorbell_flags_t enable_mask,
	      doorbell_flags_t ack_mask);

// Binds a Doorbell to a virtual interrupt.
error_t
doorbell_bind(doorbell_t *doorbell, vic_t *vic, virq_t virq);

// Unbinds a Doorbell from a virtual interrupt.
void
doorbell_unbind(doorbell_t *doorbell);

```

`hyp/ipc/doorbell/src/doorbell.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>

#include <hypcontainers.h>

#include <atomic.h>
#include <scheduler.h>
#include <spinlock.h>
#include <vic.h>
#include <virq.h>

#include "doorbell.h"
#include "event_handlers.h"

doorbell_flags_result_t
doorbell_send(doorbell_t *doorbell, doorbell_flags_t new_flags)
{
	doorbell_flags_result_t ret = { 0 };

	assert(doorbell != NULL);
	ret.e = OK;

	spinlock_acquire(&doorbell->lock);

	ret.r = doorbell->flags;

	doorbell->flags |= new_flags;

	// Level-triggered assert if there are flags enabled; else edge-only
	bool edge_only = (doorbell->flags & doorbell->enable_mask) == 0U;
	(void)virq_assert(&doorbell->source, edge_only);

	// Automatically clear ack_mask flags if there was a level assertion
	if (!edge_only) {
		doorbell->flags &= ~doorbell->ack_mask;
	}

	spinlock_release(&doorbell->lock);

	return ret;
}

doorbell_flags_result_t
doorbell_receive(doorbell_t *doorbell, doorbell_flags_t clear_flags)
{
	doorbell_flags_result_t ret = { 0 };

	assert(doorbell != NULL);
	ret.e = OK;

	if (clear_flags == 0U) {
		ret.e = ERROR_ARGUMENT_INVALID;
		goto out;
	}

	spinlock_acquire(&doorbell->lock);

	ret.r = doorbell->flags;

	doorbell->flags &= ~clear_flags;

	spinlock_release(&doorbell->lock);

out:
	return ret;
}

error_t
doorbell_reset(doorbell_t *doorbell)
{
	error_t ret = OK;

	assert(doorbell != NULL);

	spinlock_acquire(&doorbell->lock);

	// If there is a pending bound interrupt, it will be de-asserted
	(void)virq_clear(&doorbell->source);

	doorbell->flags	      = 0U;
	doorbell->ack_mask    = 0U;
	doorbell->enable_mask = ~doorbell->ack_mask;

	spinlock_release(&doorbell->lock);

	return ret;
}

error_t
doorbell_mask(doorbell_t *doorbell, doorbell_flags_t new_enable_mask,
	      doorbell_flags_t new_ack_mask)
{
	error_t ret = OK;

	assert(doorbell != NULL);

	spinlock_acquire(&doorbell->lock);

	bool was_asserted = (doorbell->flags & doorbell->enable_mask) != 0U;
	bool now_asserted = (doorbell->flags & new_enable_mask) != 0U;

	doorbell->enable_mask = new_enable_mask;
	doorbell->ack_mask    = new_ack_mask;

	if (was_asserted && !now_asserted) {
		// Deassert if new mask disables all currently asserted flags
		(void)virq_clear(&doorbell->source);

	} else if (!was_asserted && now_asserted) {
		// Assert if new mask enables flags that are already set
		(void)virq_assert(&doorbell->source, false);
		doorbell->flags &= ~doorbell->ack_mask;
	} else if (was_asserted && now_asserted) {
		doorbell->flags &= ~doorbell->ack_mask;
	} else {
		// Nothing to do.
	}

	spinlock_release(&doorbell->lock);

	return ret;
}

bool
doorbell_handle_virq_check_pending(virq_source_t *source, bool reasserted)
{
	bool ret;

	assert(source != NULL);

	doorbell_t *doorbell = doorbell_container_of_source(source);

	if (reasserted) {
		// Previous VIRQ wasn't delivered yet. If we return false in
		// this case, we can't be sure that we won't race with a
		// doorbell_send() or doorbell_mask() on another CPU.
		ret = true;
	} else {
		ret = ((doorbell->flags & doorbell->enable_mask) != 0U);
	}

	return ret;
}

error_t
doorbell_bind(doorbell_t *doorbell, vic_t *vic, virq_t virq)
{
	error_t ret = OK;

	assert(doorbell != NULL);
	assert(vic != NULL);

	ret = vic_bind_shared(&doorbell->source, vic, virq,
			      VIRQ_TRIGGER_DOORBELL);

	return ret;
}

void
doorbell_unbind(doorbell_t *doorbell)
{
	assert(doorbell != NULL);

	vic_unbind_sync(&doorbell->source);
}

error_t
doorbell_handle_object_create_doorbell(doorbell_create_t params)
{
	assert(params.doorbell != NULL);

	spinlock_init(&params.doorbell->lock);

	spinlock_acquire(&params.doorbell->lock);

	params.doorbell->flags	     = 0U;
	params.doorbell->ack_mask    = 0U;
	params.doorbell->enable_mask = ~params.doorbell->ack_mask;

	spinlock_release(&params.doorbell->lock);

	return OK;
}

void
doorbell_handle_object_deactivate_doorbell(doorbell_t *doorbell)
{
	assert(doorbell != NULL);

	spinlock_acquire(&doorbell->lock);

	vic_unbind(&doorbell->source);

	spinlock_release(&doorbell->lock);
}

```

`hyp/ipc/doorbell/src/hypercalls.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>

#include <hypcall_def.h>
#include <hyprights.h>

#include <compiler.h>
#include <cspace.h>
#include <cspace_lookup.h>
#include <object.h>
#include <thread.h>

#include "doorbell.h"

error_t
hypercall_doorbell_bind_virq(cap_id_t doorbell_cap, cap_id_t vic_cap,
			     virq_t virq)
{
	error_t	  err	 = OK;
	cspace_t *cspace = cspace_get_self();

	doorbell_ptr_result_t p = cspace_lookup_doorbell(
		cspace, doorbell_cap, CAP_RIGHTS_DOORBELL_BIND);
	if (compiler_unexpected(p.e != OK)) {
		err = p.e;
		goto out;
	}
	doorbell_t *doorbell = p.r;

	vic_ptr_result_t v =
		cspace_lookup_vic(cspace, vic_cap, CAP_RIGHTS_VIC_BIND_SOURCE);
	if (compiler_unexpected(v.e != OK)) {
		err = v.e;
		goto out_doorbell_release;
	}
	vic_t *vic = v.r;

	err = doorbell_bind(doorbell, vic, virq);

	object_put_vic(vic);
out_doorbell_release:
	object_put_doorbell(doorbell);
out:
	return err;
}

error_t
hypercall_doorbell_unbind_virq(cap_id_t doorbell_cap)
{
	error_t	  err	 = OK;
	cspace_t *cspace = cspace_get_self();

	doorbell_ptr_result_t p = cspace_lookup_doorbell(
		cspace, doorbell_cap, CAP_RIGHTS_DOORBELL_BIND);
	if (compiler_unexpected(p.e != OK)) {
		err = p.e;
		goto out;
	}
	doorbell_t *doorbell = p.r;

	doorbell_unbind(doorbell);

	object_put_doorbell(doorbell);
out:
	return err;
}

hypercall_doorbell_send_result_t
hypercall_doorbell_send(cap_id_t doorbell_cap, uint64_t new_flags)
{
	hypercall_doorbell_send_result_t ret	= { 0 };
	cspace_t			*cspace = cspace_get_self();

	doorbell_ptr_result_t p = cspace_lookup_doorbell(
		cspace, doorbell_cap, CAP_RIGHTS_DOORBELL_SEND);
	if (compiler_unexpected(p.e != OK)) {
		ret.error = p.e;
		goto out;
	}
	doorbell_t *doorbell = p.r;

	doorbell_flags_result_t res;
	res = doorbell_send(doorbell, (doorbell_flags_t)new_flags);
	if (res.e == OK) {
		ret.error     = OK;
		ret.old_flags = res.r;
	} else {
		ret.error = res.e;
	}

	object_put_doorbell(doorbell);
out:
	return ret;
}

hypercall_doorbell_receive_result_t
hypercall_doorbell_receive(cap_id_t doorbell_cap, uint64_t clear_flags)
{
	hypercall_doorbell_receive_result_t ret	   = { 0 };
	cspace_t			   *cspace = cspace_get_self();

	doorbell_ptr_result_t p = cspace_lookup_doorbell(
		cspace, doorbell_cap, CAP_RIGHTS_DOORBELL_RECEIVE);
	if (compiler_unexpected(p.e != OK)) {
		ret.error = p.e;
		goto out;
	}
	doorbell_t *doorbell = p.r;

	doorbell_flags_result_t res;
	res = doorbell_receive(doorbell, (doorbell_flags_t)clear_flags);
	if (res.e == OK) {
		ret.error     = OK;
		ret.old_flags = res.r;
	} else {
		ret.error = res.e;
	}

	object_put_doorbell(doorbell);
out:
	return ret;
}

error_t
hypercall_doorbell_reset(cap_id_t doorbell_cap)
{
	error_t	  err;
	cspace_t *cspace = cspace_get_self();

	doorbell_ptr_result_t p = cspace_lookup_doorbell(
		cspace, doorbell_cap, CAP_RIGHTS_DOORBELL_RECEIVE);
	if (compiler_unexpected(p.e != OK)) {
		err = p.e;
		goto out;
	}
	doorbell_t *doorbell = p.r;

	err = doorbell_reset(doorbell);

	object_put_doorbell(doorbell);
out:
	return err;
}

error_t
hypercall_doorbell_mask(cap_id_t doorbell_cap, uint64_t enable_mask,
			uint64_t ack_mask)
{
	error_t	  err;
	cspace_t *cspace = cspace_get_self();

	doorbell_ptr_result_t p = cspace_lookup_doorbell(
		cspace, doorbell_cap, CAP_RIGHTS_DOORBELL_RECEIVE);
	if (compiler_unexpected(p.e != OK)) {
		err = p.e;
		goto out;
	}
	doorbell_t *doorbell = p.r;

	err = doorbell_mask(doorbell, (doorbell_flags_t)enable_mask,
			    (doorbell_flags_t)ack_mask);

	object_put_doorbell(doorbell);
out:
	return err;
}

```

`hyp/ipc/msgqueue/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

interface msgqueue
local_include
events msgqueue.ev
types msgqueue.tc
base_module hyp/mem/useraccess
source msgqueue.c msgqueue_common.c hypercalls.c

```

`hyp/ipc/msgqueue/include/msgqueue.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// Appends a message to the tail of a message queue, if it is not full. If push
// is true or the buffer was previously below the not empty threshold, wake up
// receiver by asserting receiver virq. Return bool that indicates if queue is
// not full.
bool_result_t
msgqueue_send(msgqueue_t *msgqueue, size_t size, gvaddr_t data, bool push);

// Fetch a message from the head of a message queue, if it is not empty. If the
// buffer was previously above the not full threshold, wake up the sender by
// asserting sender virq. Return size of received message and bool that
// indicates if queue is not empty.
receive_info_result_t
msgqueue_receive(msgqueue_t *msgqueue, gvaddr_t buffer, size_t max_size);

// Removes all messages from message queue. If the message queue was previously
// not empty, deassert virq.
void
msgqueue_flush(msgqueue_t *msgqueue);

// Modify notfull configuration of a message queue send interface. Any parameter
// passed in as MSGQUEUE_THRESHOLD_UNCHANGED indicates no change to the
// corresponding is requested.
error_t
msgqueue_configure_send(msgqueue_t *msgqueue, count_t notfull_thd,
			count_t notfull_delay);

// Modify notemtpy configuration of a message queue receive interface. Any
// parameter passed in as MSGQUEUE_THRESHOLD_UNCHANGED indicates no change to
// the corresponding is requested. A notempty_thd special value of
// MSGQUEUE_THRESHOLD_MAXIMUM sets the threshold to the message queue’s depth.
error_t
msgqueue_configure_receive(msgqueue_t *msgqueue, count_t notempty_thd,
			   count_t notempty_delay);

// Binds message queue send interface to a virtual interrupt.
error_t
msgqueue_bind_send(msgqueue_t *msgqueue, vic_t *vic, virq_t virq);

// Binds message queue receive interface to a virtual interrupt.
error_t
msgqueue_bind_receive(msgqueue_t *msgqueue, vic_t *vic, virq_t virq);

// Unbinds message queue send interface from a virtual interrupt.
void
msgqueue_unbind_send(msgqueue_t *msgqueue);

// Unbinds message queue receive interface from a virtual interrupt.
void
msgqueue_unbind_receive(msgqueue_t *msgqueue);

```

`hyp/ipc/msgqueue/include/msgqueue_common.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// Configure the message queue.
// The object's header lock must be held and object state must be
// OBJECT_STATE_INIT.
error_t
msgqueue_configure(msgqueue_t *msgqueue, size_t max_msg_size,
		   count_t queue_depth);

// Send a message to a message queue
// The argument from_kernel, if true, indicates that the message is in a kernel
// buffer.
bool_result_t
msgqueue_send_msg(msgqueue_t *msgqueue, size_t size, kernel_or_gvaddr_t msg,
		  bool push, bool from_kernel);

// Receive a message from a message queue
// The argument to_kernel, if true, indicates that the destination message
// buffer is a kernel address.
receive_info_result_t
msgqueue_receive_msg(msgqueue_t *msgqueue, kernel_or_gvaddr_t buffer,
		     size_t max_size, bool to_kernel);

void
msgqueue_flush_queue(msgqueue_t *msgqueue);

error_t
msgqueue_bind(msgqueue_t *msgqueue, vic_t *vic, virq_t virq,
	      virq_source_t *source, virq_trigger_t trigger);

void
msgqueue_unbind(virq_source_t *source);

```

`hyp/ipc/msgqueue/msgqueue.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

module msgqueue

subscribe object_create_msgqueue

subscribe object_activate_msgqueue

subscribe object_deactivate_msgqueue

subscribe virq_check_pending[VIRQ_TRIGGER_MSGQUEUE_TX]
	handler msgqueue_tx_handle_virq_check_pending(source, reasserted)

subscribe virq_check_pending[VIRQ_TRIGGER_MSGQUEUE_RX]
	handler msgqueue_rx_handle_virq_check_pending(source, reasserted)

```

`hyp/ipc/msgqueue/msgqueue.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

define MSGQUEUE_DELAY_UNCHANGED public constant type count_t = -1;
define MSGQUEUE_THRESHOLD_UNCHANGED public constant type count_t = -1;
define MSGQUEUE_THRESHOLD_MAXIMUM public constant type count_t = -2;
define MSGQUEUE_MAX_QUEUE_DEPTH public constant type count_t = 256;
define MSGQUEUE_MAX_MAX_MSG_SIZE public constant type count_t = 1024;

extend cap_rights_msgqueue bitfield {
	0	send		bool;
	1	receive		bool;
	2	bind_send	bool;
	3	bind_receive	bool;
};

extend msgqueue object {
	buf		pointer uint8;
	count		type count_t;
	queue_size	size;
	max_msg_size	size;
	queue_depth	type count_t;
	head		type count_t;
	tail		type count_t;
	notfull_thd	type count_t;
	notempty_thd	type count_t;
	lock		structure spinlock;
	send_source	structure virq_source(contained);
	rcv_source	structure virq_source(contained);
};

define msgqueue_create_info public bitfield<64> {
	15:0	queue_depth	uint16;
	31:16	max_msg_size	uint16;
	others	unknown=0;
};

define msgqueue_send_flags public bitfield<32> {
	0	push		bool;
	others	unknown=0;
};

extend virq_trigger enumeration {
	msgqueue_tx;
	msgqueue_rx;
};

define receive_info structure {
	size		size;
	notempty	bool;
};

extend error enumeration {
	MSGQUEUE_EMPTY = 60;
	MSGQUEUE_FULL = 61;
};

```

`hyp/ipc/msgqueue/src/hypercalls.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>

#include <hypcall_def.h>
#include <hyprights.h>

#include <atomic.h>
#include <compiler.h>
#include <cspace.h>
#include <cspace_lookup.h>
#include <object.h>
#include <spinlock.h>
#include <thread.h>

#include "msgqueue.h"
#include "msgqueue_common.h"

error_t
hypercall_msgqueue_bind_send_virq(cap_id_t msgqueue_cap, cap_id_t vic_cap,
				  virq_t virq)
{
	error_t	  err;
	cspace_t *cspace = cspace_get_self();

	msgqueue_ptr_result_t p = cspace_lookup_msgqueue(
		cspace, msgqueue_cap, CAP_RIGHTS_MSGQUEUE_BIND_SEND);
	if (compiler_unexpected(p.e != OK)) {
		err = p.e;
		goto out;
	}
	msgqueue_t *msgqueue = p.r;

	vic_ptr_result_t v =
		cspace_lookup_vic(cspace, vic_cap, CAP_RIGHTS_VIC_BIND_SOURCE);
	if (compiler_unexpected(v.e != OK)) {
		err = v.e;
		goto out_msgqueue_release;
	}
	vic_t *vic = v.r;

	err = msgqueue_bind_send(msgqueue, vic, virq);

	object_put_vic(vic);
out_msgqueue_release:
	object_put_msgqueue(msgqueue);
out:
	return err;
}

error_t
hypercall_msgqueue_bind_receive_virq(cap_id_t msgqueue_cap, cap_id_t vic_cap,
				     virq_t virq)
{
	error_t	  err;
	cspace_t *cspace = cspace_get_self();

	msgqueue_ptr_result_t p = cspace_lookup_msgqueue(
		cspace, msgqueue_cap, CAP_RIGHTS_MSGQUEUE_BIND_RECEIVE);
	if (compiler_unexpected(p.e != OK)) {
		err = p.e;
		goto out;
	}
	msgqueue_t *msgqueue = p.r;

	vic_ptr_result_t v =
		cspace_lookup_vic(cspace, vic_cap, CAP_RIGHTS_VIC_BIND_SOURCE);
	if (compiler_unexpected(v.e != OK)) {
		err = v.e;
		goto out_msgqueue_release;
	}
	vic_t *vic = v.r;

	err = msgqueue_bind_receive(msgqueue, vic, virq);

	object_put_vic(vic);
out_msgqueue_release:
	object_put_msgqueue(msgqueue);
out:
	return err;
}

error_t
hypercall_msgqueue_unbind_send_virq(cap_id_t msgqueue_cap)
{
	error_t	  err	 = OK;
	cspace_t *cspace = cspace_get_self();

	msgqueue_ptr_result_t p = cspace_lookup_msgqueue(
		cspace, msgqueue_cap, CAP_RIGHTS_MSGQUEUE_BIND_SEND);
	if (compiler_unexpected(p.e != OK)) {
		err = p.e;
		goto out;
	}
	msgqueue_t *msgqueue = p.r;

	msgqueue_unbind_send(msgqueue);

	object_put_msgqueue(msgqueue);
out:
	return err;
}

error_t
hypercall_msgqueue_unbind_receive_virq(cap_id_t msgqueue_cap)
{
	error_t	  err	 = OK;
	cspace_t *cspace = cspace_get_self();

	msgqueue_ptr_result_t p = cspace_lookup_msgqueue(
		cspace, msgqueue_cap, CAP_RIGHTS_MSGQUEUE_BIND_RECEIVE);
	if (compiler_unexpected(p.e != OK)) {
		err = p.e;
		goto out;
	}
	msgqueue_t *msgqueue = p.r;

	msgqueue_unbind_receive(msgqueue);

	object_put_msgqueue(msgqueue);
out:
	return err;
}

hypercall_msgqueue_send_result_t
hypercall_msgqueue_send(cap_id_t msgqueue_cap, size_t size, user_ptr_t data,
			msgqueue_send_flags_t send_flags)
{
	hypercall_msgqueue_send_result_t ret	= { 0 };
	cspace_t			*cspace = cspace_get_self();

	msgqueue_ptr_result_t p = cspace_lookup_msgqueue(
		cspace, msgqueue_cap, CAP_RIGHTS_MSGQUEUE_SEND);
	if (compiler_unexpected(p.e != OK)) {
		ret.error = p.e;
		goto out;
	}
	msgqueue_t *msgqueue = p.r;

	bool push = msgqueue_send_flags_get_push(&send_flags);

	bool_result_t res = msgqueue_send(msgqueue, size, (gvaddr_t)data, push);

	ret.error    = res.e;
	ret.not_full = res.r;

	object_put_msgqueue(msgqueue);
out:
	return ret;
}

hypercall_msgqueue_receive_result_t
hypercall_msgqueue_receive(cap_id_t msgqueue_cap, user_ptr_t buffer,
			   size_t buf_size)
{
	hypercall_msgqueue_receive_result_t ret	   = { 0 };
	cspace_t			   *cspace = cspace_get_self();

	msgqueue_ptr_result_t p = cspace_lookup_msgqueue(
		cspace, msgqueue_cap, CAP_RIGHTS_MSGQUEUE_RECEIVE);
	if (compiler_unexpected(p.e != OK)) {
		ret.error = p.e;
		goto out;
	}
	msgqueue_t *msgqueue = p.r;

	receive_info_result_t res;
	res = msgqueue_receive(msgqueue, (gvaddr_t)buffer, buf_size);

	ret.error     = res.e;
	ret.size      = res.r.size;
	ret.not_empty = res.r.notempty;

	object_put_msgqueue(msgqueue);
out:
	return ret;
}

error_t
hypercall_msgqueue_flush(cap_id_t msgqueue_cap)
{
	error_t	  err;
	cspace_t *cspace = cspace_get_self();

	msgqueue_ptr_result_t p = cspace_lookup_msgqueue(
		cspace, msgqueue_cap, CAP_RIGHTS_MSGQUEUE_RECEIVE);
	if (compiler_unexpected(p.e != OK)) {
		err = p.e;
		goto out;
	}
	msgqueue_t *msgqueue = p.r;

	msgqueue_flush(msgqueue);
	err = OK;

	object_put_msgqueue(msgqueue);
out:
	return err;
}

error_t
hypercall_msgqueue_configure_send(cap_id_t msgqueue_cap, count_t not_full_thres,
				  count_t not_full_holdoff)
{
	error_t	  err;
	cspace_t *cspace = cspace_get_self();

	msgqueue_ptr_result_t p = cspace_lookup_msgqueue(
		cspace, msgqueue_cap, CAP_RIGHTS_MSGQUEUE_SEND);
	if (compiler_unexpected(p.e != OK)) {
		err = p.e;
		goto out;
	}
	msgqueue_t *msgqueue = p.r;

	err = msgqueue_configure_send(msgqueue, not_full_thres,
				      not_full_holdoff);

	object_put_msgqueue(msgqueue);
out:
	return err;
}

error_t
hypercall_msgqueue_configure_receive(cap_id_t msgqueue_cap,
				     count_t  not_empty_thres,
				     count_t  not_empty_holdoff)
{
	error_t	  err;
	cspace_t *cspace = cspace_get_self();

	msgqueue_ptr_result_t p = cspace_lookup_msgqueue(
		cspace, msgqueue_cap, CAP_RIGHTS_MSGQUEUE_RECEIVE);
	if (compiler_unexpected(p.e != OK)) {
		err = p.e;
		goto out;
	}
	msgqueue_t *msgqueue = p.r;

	err = msgqueue_configure_receive(msgqueue, not_empty_thres,
					 not_empty_holdoff);

	object_put_msgqueue(msgqueue);
out:
	return err;
}

error_t
hypercall_msgqueue_configure(cap_id_t		    msgqueue_cap,
			     msgqueue_create_info_t create_info)
{
	error_t	      err;
	cspace_t     *cspace = cspace_get_self();
	object_type_t type;

	object_ptr_result_t o = cspace_lookup_object_any(
		cspace, msgqueue_cap, CAP_RIGHTS_GENERIC_OBJECT_ACTIVATE,
		&type);
	if (compiler_unexpected(o.e != OK)) {
		err = o.e;
		goto out;
	}
	if (type != OBJECT_TYPE_MSGQUEUE) {
		err = ERROR_CSPACE_WRONG_OBJECT_TYPE;
		goto out_msgqueue_release;
	}

	msgqueue_t *target_msgqueue = o.r.msgqueue;

	spinlock_acquire(&target_msgqueue->header.lock);

	size_t max_msg_size =
		msgqueue_create_info_get_max_msg_size(&create_info);
	count_t queue_depth =
		msgqueue_create_info_get_queue_depth(&create_info);

	if (atomic_load_relaxed(&target_msgqueue->header.state) ==
	    OBJECT_STATE_INIT) {
		err = msgqueue_configure(target_msgqueue, max_msg_size,
					 queue_depth);
	} else {
		err = ERROR_OBJECT_STATE;
	}

	spinlock_release(&target_msgqueue->header.lock);
out_msgqueue_release:
	object_put(type, o.r);
out:
	return err;
}

```

`hyp/ipc/msgqueue/src/msgqueue.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>
#include <string.h>

#include <atomic.h>
#include <panic.h>
#include <partition.h>
#include <scheduler.h>
#include <spinlock.h>
#include <vic.h>
#include <virq.h>

#include "event_handlers.h"
#include "msgqueue.h"
#include "msgqueue_common.h"

bool_result_t
msgqueue_send(msgqueue_t *msgqueue, size_t size, gvaddr_t data, bool push)
{
	kernel_or_gvaddr_t data_union;
	data_union.guest_addr = data;

	return msgqueue_send_msg(msgqueue, size, data_union, push, false);
}

receive_info_result_t
msgqueue_receive(msgqueue_t *msgqueue, gvaddr_t buffer, size_t max_size)
{
	kernel_or_gvaddr_t buffer_union;
	buffer_union.guest_addr = buffer;

	return msgqueue_receive_msg(msgqueue, buffer_union, max_size, false);
}

void
msgqueue_flush(msgqueue_t *msgqueue)
{
	msgqueue_flush_queue(msgqueue);
}

error_t
msgqueue_configure_send(msgqueue_t *msgqueue, count_t notfull_thd,
			count_t notfull_delay)
{
	error_t ret = OK;

	assert(msgqueue != NULL);

	if (notfull_delay != MSGQUEUE_DELAY_UNCHANGED) {
		ret = ERROR_UNIMPLEMENTED;
		goto out;
	}

	if ((notfull_thd >= msgqueue->queue_depth) &&
	    (notfull_thd != MSGQUEUE_THRESHOLD_UNCHANGED)) {
		ret = ERROR_ARGUMENT_INVALID;
		goto out;
	}

	spinlock_acquire(&msgqueue->lock);

	if (notfull_thd != MSGQUEUE_THRESHOLD_UNCHANGED) {
		msgqueue->notfull_thd = notfull_thd;

		if (msgqueue->count <= msgqueue->notfull_thd) {
			(void)virq_assert(&msgqueue->send_source, false);
		} else {
			(void)virq_clear(&msgqueue->send_source);
		}
	}
	spinlock_release(&msgqueue->lock);
out:
	return ret;
}

error_t
msgqueue_configure_receive(msgqueue_t *msgqueue, count_t notempty_thd,
			   count_t notempty_delay)
{
	error_t ret = OK;

	assert(msgqueue != NULL);

	if (notempty_delay != MSGQUEUE_DELAY_UNCHANGED) {
		ret = ERROR_UNIMPLEMENTED;
		goto out;
	}

	if ((notempty_thd == 0U) ||
	    ((notempty_thd > msgqueue->queue_depth) &&
	     (notempty_thd != MSGQUEUE_THRESHOLD_MAXIMUM) &&
	     (notempty_thd != MSGQUEUE_THRESHOLD_UNCHANGED))) {
		ret = ERROR_ARGUMENT_INVALID;
		goto out;
	}

	spinlock_acquire(&msgqueue->lock);

	if (notempty_thd == MSGQUEUE_THRESHOLD_MAXIMUM) {
		msgqueue->notempty_thd = msgqueue->queue_depth;
	} else if (notempty_thd != MSGQUEUE_THRESHOLD_UNCHANGED) {
		msgqueue->notempty_thd = notempty_thd;

		if (msgqueue->count >= msgqueue->notempty_thd) {
			(void)virq_assert(&msgqueue->rcv_source, false);
		} else {
			(void)virq_clear(&msgqueue->rcv_source);
		}
	} else {
		// Nothing to do. Value stays the same.
	}

	spinlock_release(&msgqueue->lock);
out:
	return ret;
}

error_t
msgqueue_bind_send(msgqueue_t *msgqueue, vic_t *vic, virq_t virq)
{
	return msgqueue_bind(msgqueue, vic, virq, &msgqueue->send_source,
			     VIRQ_TRIGGER_MSGQUEUE_TX);
}

error_t
msgqueue_bind_receive(msgqueue_t *msgqueue, vic_t *vic, virq_t virq)
{
	return msgqueue_bind(msgqueue, vic, virq, &msgqueue->rcv_source,
			     VIRQ_TRIGGER_MSGQUEUE_RX);
}

void
msgqueue_unbind_send(msgqueue_t *msgqueue)
{
	msgqueue_unbind(&msgqueue->send_source);
}

void
msgqueue_unbind_receive(msgqueue_t *msgqueue)
{
	msgqueue_unbind(&msgqueue->rcv_source);
}

error_t
msgqueue_handle_object_create_msgqueue(msgqueue_create_t params)
{
	msgqueue_t *msgqueue = params.msgqueue;
	assert(msgqueue != NULL);
	spinlock_init(&msgqueue->lock);

	return OK;
}

error_t
msgqueue_configure(msgqueue_t *msgqueue, size_t max_msg_size,
		   count_t queue_depth)
{
	error_t ret = OK;

	assert(msgqueue != NULL);

	if ((queue_depth != 0U) && (max_msg_size != 0U) &&
	    (queue_depth < MSGQUEUE_MAX_QUEUE_DEPTH) &&
	    (max_msg_size < MSGQUEUE_MAX_MAX_MSG_SIZE)) {
		msgqueue->max_msg_size = max_msg_size;
		msgqueue->queue_depth  = queue_depth;
	} else {
		ret = ERROR_ARGUMENT_INVALID;
	}

	return ret;
}

error_t
msgqueue_handle_object_activate_msgqueue(msgqueue_t *msgqueue)
{
	error_t ret = OK;

	assert(msgqueue != NULL);
	assert(msgqueue->buf == NULL);

	if ((msgqueue->queue_depth == 0U) || (msgqueue->max_msg_size == 0U)) {
		ret = ERROR_OBJECT_CONFIG;
		goto out;
	}

	// Message queue size consists of the message maximum size, a field to
	// know the exact size of the message and how many messages can the
	// queue contain.
	size_t queue_size = (msgqueue->max_msg_size + sizeof(size_t)) *
			    msgqueue->queue_depth;
	partition_t *partition = msgqueue->header.partition;

	void_ptr_result_t res =
		partition_alloc(partition, queue_size, alignof(size_t));
	if (res.e != OK) {
		ret = res.e;
		goto out;
	}

	msgqueue->buf	       = (uint8_t *)res.r;
	msgqueue->count	       = 0U;
	msgqueue->queue_size   = queue_size;
	msgqueue->head	       = 0U;
	msgqueue->tail	       = 0U;
	msgqueue->notfull_thd  = msgqueue->queue_depth - 1U;
	msgqueue->notempty_thd = 1U;

out:
	return ret;
}

void
msgqueue_handle_object_deactivate_msgqueue(msgqueue_t *msgqueue)
{
	assert(msgqueue != NULL);

	if (msgqueue->buf != NULL) {
		error_t	     ret;
		partition_t *partition = msgqueue->header.partition;

		ret = partition_free(partition, msgqueue->buf,
				     msgqueue->queue_size);

		if (ret != OK) {
			panic("Error freeing msgqueue buffer");
		}
		msgqueue->buf = NULL;
	}

	vic_unbind(&msgqueue->send_source);
	vic_unbind(&msgqueue->rcv_source);
}

```

`hyp/ipc/msgqueue/src/msgqueue_common.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>
#include <string.h>

#include <hypcontainers.h>

#include <scheduler.h>
#include <spinlock.h>
#include <vic.h>
#include <virq.h>

#include "event_handlers.h"
#include "msgqueue_common.h"
#include "useraccess.h"

bool_result_t
msgqueue_send_msg(msgqueue_t *msgqueue, size_t size, kernel_or_gvaddr_t msg,
		  bool push, bool from_kernel)
{
	bool_result_t ret;

	ret.r = true;
	ret.e = OK;

	assert(msgqueue != NULL);

	spinlock_acquire(&msgqueue->lock);

	if (msgqueue->count == msgqueue->queue_depth) {
		ret.e = ERROR_MSGQUEUE_FULL;
		ret.r = false;
		goto out;
	}

	// Enqueue message at the tail of the queue
	void *hyp_va =
		(void *)(msgqueue->buf + msgqueue->tail + sizeof(size_t));

	if (from_kernel) {
		if (size > msgqueue->max_msg_size) {
			ret.e = ERROR_ARGUMENT_SIZE;
			ret.r = false;
			goto out;
		}

		(void)memcpy(hyp_va, (void *)msg.kernel_addr, size);
	} else {
		size_result_t ret_val = useraccess_copy_from_guest_va(
			hyp_va, msgqueue->max_msg_size, msg.guest_addr, size);
		if (ret_val.e != OK) {
			goto out;
		}
	}

	(void)memcpy(msgqueue->buf + msgqueue->tail, (uint8_t *)&size,
		     sizeof(size_t));
	msgqueue->count++;

	// Update tail value
	msgqueue->tail += (count_t)(msgqueue->max_msg_size + sizeof(size_t));

	if (msgqueue->tail == msgqueue->queue_size) {
		msgqueue->tail = 0U;
	}

	// If buffer was previously below the not empty threshold, we must
	// wake up the receiver side by asserting the receiver virq source.
	if (push || (msgqueue->count == msgqueue->notempty_thd)) {
		(void)virq_assert(&msgqueue->rcv_source, false);
	}

	if (msgqueue->count == msgqueue->queue_depth) {
		ret.r = false;
	}

out:
	spinlock_release(&msgqueue->lock);

	return ret;
}

receive_info_result_t
msgqueue_receive_msg(msgqueue_t *msgqueue, kernel_or_gvaddr_t buffer,
		     size_t max_size, bool to_kernel)
{
	receive_info_result_t ret  = { 0 };
	size_t		      size = 0U;

	ret.e	       = OK;
	ret.r.size     = 0U;
	ret.r.notempty = true;

	assert(msgqueue != NULL);
	assert(msgqueue->buf != NULL);

	spinlock_acquire(&msgqueue->lock);

	if (msgqueue->count == 0U) {
		ret.e	       = ERROR_MSGQUEUE_EMPTY;
		ret.r.notempty = false;
		goto out;
	}

	(void)memcpy((uint8_t *)&size, msgqueue->buf + msgqueue->head,
		     sizeof(size_t));

	// Dequeue message from the head of the queue
	void *hyp_va =
		(void *)(msgqueue->buf + msgqueue->head + sizeof(size_t));

	if (to_kernel) {
		if (size > max_size) {
			ret.e	       = ERROR_ARGUMENT_SIZE;
			ret.r.notempty = false;
			goto out;
		}

		(void)memcpy((void *)buffer.kernel_addr, hyp_va, size);
	} else {
		size_result_t ret_val = useraccess_copy_to_guest_va(
			buffer.guest_addr, max_size, hyp_va, size, false);
		if (ret_val.e != OK) {
			goto out;
		}
	}

	ret.r.size = size;
	msgqueue->count--;

	// Update head value
	msgqueue->head += (count_t)(msgqueue->max_msg_size + sizeof(size_t));
	assert(msgqueue->head <= msgqueue->queue_size);

	if (msgqueue->head == msgqueue->queue_size) {
		msgqueue->head = 0U;
	}

	// If buffer was previously above the not full threshold, we must let
	// the sender side know that it can send more messages.
	if (msgqueue->count == msgqueue->notfull_thd) {
		// We wake up the sender side by asserting the sender virq
		// source.
		(void)virq_assert(&msgqueue->send_source, false);
	}

	if (msgqueue->count == 0U) {
		ret.r.notempty = false;
	}

out:
	spinlock_release(&msgqueue->lock);

	return ret;
}

void
msgqueue_flush_queue(msgqueue_t *msgqueue)
{
	assert(msgqueue != NULL);
	assert(msgqueue->buf != NULL);

	spinlock_acquire(&msgqueue->lock);

	// If there is a pending bound interrupt, it will be de-asserted
	if (msgqueue->count != 0U) {
		(void)virq_assert(&msgqueue->send_source, false);
		(void)virq_clear(&msgqueue->rcv_source);
	}

	(void)memset_s(msgqueue->buf, msgqueue->queue_size, 0,
		       msgqueue->queue_size);
	msgqueue->count = 0U;
	msgqueue->head	= 0U;
	msgqueue->tail	= 0U;

	spinlock_release(&msgqueue->lock);
}

error_t
msgqueue_bind(msgqueue_t *msgqueue, vic_t *vic, virq_t virq,
	      virq_source_t *source, virq_trigger_t trigger)
{
	assert(msgqueue != NULL);
	assert(vic != NULL);
	assert(source != NULL);

	error_t ret = vic_bind_shared(source, vic, virq, trigger);

	return ret;
}

void
msgqueue_unbind(virq_source_t *source)
{
	assert(source != NULL);

	vic_unbind_sync(source);
}

bool
msgqueue_rx_handle_virq_check_pending(virq_source_t *source, bool reasserted)
{
	bool ret;

	assert(source != NULL);

	msgqueue_t *msgqueue = msgqueue_container_of_rcv_source(source);

	if (reasserted) {
		// Previous VIRQ wasn't delivered yet. If we return false in
		// this case, we can't be sure that we won't race with a
		// msgqueue_send_msg() on another CPU.
		ret = true;
	} else {
		ret = (msgqueue->count >= msgqueue->notempty_thd);
	}

	return ret;
}

bool
msgqueue_tx_handle_virq_check_pending(virq_source_t *source, bool reasserted)
{
	bool ret;

	assert(source != NULL);

	msgqueue_t *msgqueue = msgqueue_container_of_send_source(source);

	if (reasserted) {
		// Previous VIRQ wasn't delivered yet. If we return false in
		// this case, we can't be sure that we won't race with a
		// msgqueue_receive_msg() on another CPU.
		ret = true;
	} else {
		ret = (msgqueue->count <= msgqueue->notfull_thd);
	}

	return ret;
}

```

`hyp/mem/addrspace/aarch64/addrspace.ev`:

```ev
// © 2022 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

module addrspace

#if defined(INTERFACE_VCPU_RUN)
subscribe vdevice_access_fixed_addr
	priority last

subscribe vcpu_run_check
	require_scheduler_lock(vcpu)

subscribe vcpu_run_resume[VCPU_RUN_STATE_ADDRSPACE_VMMIO_READ]
	handler addrspace_handle_vcpu_run_resume_read(vcpu, resume_data_0)
	require_scheduler_lock(vcpu)

subscribe vcpu_run_resume[VCPU_RUN_STATE_ADDRSPACE_VMMIO_WRITE]
	handler addrspace_handle_vcpu_run_resume_write(vcpu)
	require_scheduler_lock(vcpu)
#endif

```

`hyp/mem/addrspace/aarch64/addrspace.tc`:

```tc
// © 2022 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#if defined(INTERFACE_VCPU_RUN)
extend scheduler_block enumeration {
	addrspace_vmmio_access;
};

extend vcpu_run_state enumeration {
	addrspace_vmmio_read = 4;
	addrspace_vmmio_write = 5;
};

extend thread object module addrspace {
	vmmio_access_ipa	type vmaddr_t;
	vmmio_access_size	size;
	vmmio_access_value	uregister;
	vmmio_access_write	bool;
};
#endif

```

`hyp/mem/addrspace/aarch64/src/lookup.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier; BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>
#include <limits.h>

#include <hypregisters.h>

#include <addrspace.h>
#include <thread.h>
#include <util.h>

#include <asm/barrier.h>

// Check whether an address range is within the address space.
error_t
addrspace_check_range(addrspace_t *addrspace, vmaddr_t base, size_t size)
{
	error_t err;

	if ((size != 0U) && util_add_overflows(base, size - 1U)) {
		err = ERROR_ADDR_OVERFLOW;
		goto out;
	}

	count_t bits = addrspace->vm_pgtable.control.address_bits;
	// Ensure that the value used for shift operation is within the limit 0
	// to sizeof(type) -1
	assert(bits < (sizeof(vmaddr_t) * (size_t)CHAR_BIT));
	if (base >= util_bit(bits)) {
		err = ERROR_ADDR_INVALID;
	} else if ((size != 0U) && ((base + size - 1U) >= util_bit(bits))) {
		err = ERROR_ARGUMENT_SIZE;
	} else {
		err = OK;
	}

out:
	return err;
}

#if defined(INTERFACE_VCPU)
paddr_result_t
addrspace_va_to_pa_read(gvaddr_t addr)
{
	paddr_result_t ret;

	thread_t *thread = thread_get_self();
	assert(thread->kind == THREAD_KIND_VCPU);

	PAR_EL1_base_t saved_par =
		register_PAR_EL1_base_read_ordered(&asm_ordering);
	__asm__ volatile("at	S12E1R, %[addr]"
			 : "+m"(asm_ordering)
			 : [addr] "r"(addr));
	asm_context_sync_ordered(&asm_ordering);
	PAR_EL1_t par = {
		.base = register_PAR_EL1_base_read_ordered(&asm_ordering),
	};
	register_PAR_EL1_base_write_ordered(saved_par, &asm_ordering);

	if (!PAR_EL1_base_get_F(&par.base)) {
		paddr_t pa = PAR_EL1_F0_get_PA(&par.f0);
		pa |= (gvaddr_t)addr & 0xfffU;
		ret = paddr_result_ok(pa);
	} else if (PAR_EL1_F1_get_S(&par.f1)) {
		// Stage 2 fault
		ret = paddr_result_error(ERROR_DENIED);
	} else {
		// Stage 1 fault
		ret = paddr_result_error(ERROR_ADDR_INVALID);
	}

	return ret;
}

vmaddr_result_t
addrspace_va_to_ipa_read(gvaddr_t addr)
{
	vmaddr_result_t ret;

	thread_t *thread = thread_get_self();
	assert(thread->kind == THREAD_KIND_VCPU);

	PAR_EL1_base_t saved_par =
		register_PAR_EL1_base_read_ordered(&asm_ordering);
	__asm__ volatile("at	S1E1R, %[addr]"
			 : "+m"(asm_ordering)
			 : [addr] "r"(addr));
	asm_context_sync_ordered(&asm_ordering);
	PAR_EL1_t par = {
		.base = register_PAR_EL1_base_read_ordered(&asm_ordering),
	};
	register_PAR_EL1_base_write_ordered(saved_par, &asm_ordering);

	if (!PAR_EL1_base_get_F(&par.base)) {
		vmaddr_t ipa = PAR_EL1_F0_get_PA(&par.f0);
		ipa |= (vmaddr_t)addr & 0xfffU;
		ret = vmaddr_result_ok(ipa);
	} else if (PAR_EL1_F1_get_S(&par.f1)) {
		// Stage 2 fault (on a S1 page table walk access)
		ret = vmaddr_result_error(ERROR_DENIED);
	} else {
		// Stage 1 fault
		ret = vmaddr_result_error(ERROR_ADDR_INVALID);
	}

	return ret;
}
#endif

```

`hyp/mem/addrspace/aarch64/src/vmmio.c`:

```c
// © 2022 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>

#if defined(INTERFACE_VCPU_RUN)
#include <gpt.h>
#include <rcu.h>
#include <scheduler.h>
#include <spinlock.h>
#include <thread.h>
#include <vcpu_run.h>

#include "event_handlers.h"

vcpu_trap_result_t
addrspace_handle_vdevice_access_fixed_addr(vmaddr_t ipa, size_t access_size,
					   register_t *value, bool is_write)
{
	thread_t *current = thread_get_self();

	vcpu_trap_result_t ret = VCPU_TRAP_RESULT_UNHANDLED;

	scheduler_lock(current);
	if (vcpu_run_is_enabled(current)) {
		addrspace_t *addrspace = current->addrspace;

		// We need to call gpt_lookup() in an RCU critical section to
		// ensure that levels aren't freed while it is accessing them,
		// but we can end the critical section immediately afterwards
		// since we are not dereferencing anything.
		rcu_read_start();
		gpt_lookup_result_t result =
			gpt_lookup(&addrspace->vmmio_ranges, ipa, access_size);
		rcu_read_finish();

		if ((result.size == access_size) &&
		    (result.entry.type == GPT_TYPE_VMMIO_RANGE)) {
			current->addrspace_vmmio_access_ipa  = ipa;
			current->addrspace_vmmio_access_size = access_size;
			current->addrspace_vmmio_access_value =
				is_write ? *value : 0U;
			current->addrspace_vmmio_access_write = is_write;

			scheduler_block(current,
					SCHEDULER_BLOCK_ADDRSPACE_VMMIO_ACCESS);
			scheduler_unlock_nopreempt(current);
			(void)scheduler_schedule();
			scheduler_lock_nopreempt(current);

			if (!is_write) {
				*value = current->addrspace_vmmio_access_value;
			}

			ret = VCPU_TRAP_RESULT_EMULATED;
		}
	}
	scheduler_unlock(current);

	return ret;
}

vcpu_run_state_t
addrspace_handle_vcpu_run_check(const thread_t *vcpu, register_t *state_data_0,
				register_t *state_data_1,
				register_t *state_data_2)
{
	vcpu_run_state_t ret;

	if (scheduler_is_blocked(vcpu,
				 SCHEDULER_BLOCK_ADDRSPACE_VMMIO_ACCESS)) {
		*state_data_0 = (register_t)vcpu->addrspace_vmmio_access_ipa;
		*state_data_1 = (register_t)vcpu->addrspace_vmmio_access_size;
		*state_data_2 = (register_t)vcpu->addrspace_vmmio_access_value;
		ret	      = vcpu->addrspace_vmmio_access_write
					? VCPU_RUN_STATE_ADDRSPACE_VMMIO_WRITE
					: VCPU_RUN_STATE_ADDRSPACE_VMMIO_READ;
	} else {
		ret = VCPU_RUN_STATE_BLOCKED;
	}

	return ret;
}

error_t
addrspace_handle_vcpu_run_resume_read(thread_t *vcpu, register_t resume_data_0)
{
	assert(scheduler_is_blocked(vcpu,
				    SCHEDULER_BLOCK_ADDRSPACE_VMMIO_ACCESS) &&
	       !vcpu->addrspace_vmmio_access_write);
	vcpu->addrspace_vmmio_access_value = resume_data_0;
	(void)scheduler_unblock(vcpu, SCHEDULER_BLOCK_ADDRSPACE_VMMIO_ACCESS);
	return OK;
}

error_t
addrspace_handle_vcpu_run_resume_write(thread_t *vcpu)
{
	assert(scheduler_is_blocked(vcpu,
				    SCHEDULER_BLOCK_ADDRSPACE_VMMIO_ACCESS) &&
	       vcpu->addrspace_vmmio_access_write);
	(void)scheduler_unblock(vcpu, SCHEDULER_BLOCK_ADDRSPACE_VMMIO_ACCESS);
	return OK;
}

#endif // INTERFACE_VCPU_RUN

```

`hyp/mem/addrspace/addrspace.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

module addrspace

subscribe boot_cold_init()

#if defined(INTERFACE_VCPU)
subscribe object_activate_thread

subscribe object_deactivate_thread

subscribe thread_load_state
	handler addrspace_context_switch_load()

subscribe thread_get_stack_base[THREAD_KIND_VCPU](thread)
#endif

subscribe object_create_addrspace(addrspace_create)
	unwinder addrspace_unwind_object_create_addrspace(addrspace_create)

subscribe object_cleanup_addrspace(addrspace)

subscribe object_activate_addrspace
	priority last

subscribe object_deactivate_addrspace
	priority last

#if defined(MODULE_VM_ROOTVM)
subscribe rootvm_init(root_thread, root_cspace, qcbor_enc_ctxt)
#endif

```

`hyp/mem/addrspace/addrspace.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <types/bitmap.h>

define ROOT_VM_VMID constant type vmid_t = 255;

define ADDRSPACE_MAX_THREADS constant type count_t = PLATFORM_MAX_CORES;

#if defined(INTERFACE_VCPU_RUN)
// This upper bound exists to prevent an unprivileged VM with the ability to
// add VMMIO ranges to its own address space executing a denial of service
// attack by adding many small ranges to the GPT and exhausting the partition
// heap. In order to safely reduce this count when a range is removed, we also
// require that removed ranges are identical to added ranges.
define ADDRSPACE_MAX_VMMIO_RANGES constant type count_t = 128;
#endif

extend cap_rights_addrspace bitfield {
	0	attach		bool;
	1	map		bool;
	2	lookup		bool;
	3	add_vmmio_range	bool;
};

define addrspace_information_area structure {
	hyp_va		pointer structure addrspace_info_area_layout;
	ipa		type vmaddr_t;
	me		pointer object memextent;
};

extend addrspace object {
	mapping_list_lock	structure spinlock;
	pgtable_lock		structure spinlock;
	vm_pgtable		structure pgtable_vm;
	vmid			type vmid_t;
	read_only		bool;
	platform_pgtable	bool;
	hyp_va_range		structure virt_range;
	stack_bitmap		BITMAP(ADDRSPACE_MAX_THREADS, atomic);
	info_area		structure addrspace_information_area;
#if defined(INTERFACE_VCPU_RUN)
	vmmio_range_lock	structure spinlock;
	vmmio_ranges		structure gpt;
	vmmio_range_count	type count_t;
#endif
};

extend gpt_type enumeration {
	vmmio_range;
};

extend gpt_value union {
	vmmio_range_base	type vmaddr_t;
};

extend thread object {
	addrspace		pointer object addrspace;
	stack_map_index		type index_t;
};

extend trace_ids bitfield {
	15:0	vmid	type vmid_t;
};

```

`hyp/mem/addrspace/armv8/abort.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

module addrspace

#if defined(INTERFACE_VCPU)
subscribe vcpu_trap_pf_abort_guest(esr, ipa, far)
	priority last

subscribe vcpu_trap_data_abort_guest(esr, ipa, far)
	priority last
#endif

```

`hyp/mem/addrspace/armv8/src/abort.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#if defined(INTERFACE_VCPU)
#include <assert.h>
#include <hyptypes.h>

#include <hypregisters.h>

#include <addrspace.h>
#include <atomic.h>
#include <platform_mem.h>
#include <rcu.h>
#include <spinlock.h>
#include <thread.h>

#include <asm/barrier.h>

#include "event_handlers.h"

static bool
addrspace_undergoing_bbm(addrspace_t *addrspace)
{
	bool ret;

	if (addrspace->platform_pgtable) {
		ret = platform_pgtable_undergoing_bbm();
	} else {
#if (CPU_PGTABLE_BBM_LEVEL == 0) && !defined(PLATFORM_PGTABLE_AVOID_BBM)
		// We use break-before-make for block splits and merges,
		// which might affect addresses outside the operation range
		// and therefore might cause faults that should be hidden.
		if (!spinlock_trylock(&addrspace->pgtable_lock)) {
			ret = true;
		} else {
			spinlock_release(&addrspace->pgtable_lock);
			ret = false;
		}
#else
		// Break-before-make is only used when changing the output
		// address or cache attributes, which shouldn't happen while
		// the affected pages are being accessed.
		ret = false;
#endif
	}

	return ret;
}

static vcpu_trap_result_t
addrspace_handle_guest_tlb_conflict(vmaddr_result_t ipa, FAR_EL2_t far,
				    bool s1ptw)
{
	// If this fault was not on a stage 1 PT walk, the ipa argument is not
	// valid, because the architecture allows the TLB to avoid caching it.
	// We can do a lookup on the VA to try to find it. This may fail if the
	// CPU caches S1-only translations and the conflict is in that cache.
	//
	// For a fault on a stage 1 PT walk, the ipa argument is always valid.
	if (!s1ptw) {
		ipa = addrspace_va_to_ipa_read(
			FAR_EL2_get_VirtualAddress(&far));
	} else {
		assert(ipa.e == OK);
	}

	asm_ordering_dummy_t tlbi_s2_ordering;
	if (ipa.e == OK) {
		// If the IPA is valid, the conflict may have been between S2
		// TLB entries, so flush the IPA from the S2 TLB. Note that if
		// our IPA lookup above failed, the conflict must be in S1+S2 or
		// S1-only entries, so no S2 flush is needed.
		vmsa_tlbi_ipa_input_t ipa_input = vmsa_tlbi_ipa_input_default();
		vmsa_tlbi_ipa_input_set_IPA(&ipa_input, ipa.r);
		__asm__ volatile(
			"tlbi IPAS2E1, %[VA]"
			: "=m"(tlbi_s2_ordering)
			: [VA] "r"(vmsa_tlbi_ipa_input_raw(ipa_input)));
	}

	// Regardless of whether the IPA is valid, there is always a possibility
	// that the conflict was on S1+S2 or S1-only entries. So we always flush
	// by VA. If the fault was on a stage 1 page table walk, the fault may
	// have been on a cached next-level entry, so we flush those too.
	asm_ordering_dummy_t  tlbi_s1_ordering;
	vmsa_tlbi_vaa_input_t va_input = vmsa_tlbi_vaa_input_default();
	vmsa_tlbi_vaa_input_set_VA(&va_input, FAR_EL2_get_VirtualAddress(&far));
	if (s1ptw) {
		__asm__ volatile("tlbi VAAE1, %[VA]"
				 : "=m"(tlbi_s1_ordering)
				 : [VA] "r"(vmsa_tlbi_vaa_input_raw(va_input)));
	} else {
		__asm__ volatile("tlbi VAALE1, %[VA]"
				 : "=m"(tlbi_s1_ordering)
				 : [VA] "r"(vmsa_tlbi_vaa_input_raw(va_input)));
	}

	__asm__ volatile("dsb nsh" ::"m"(tlbi_s1_ordering),
			 "m"(tlbi_s2_ordering));

	return VCPU_TRAP_RESULT_RETRY;
}

// Retry faults if they may have been caused by break before make during block
// splits in the direct physical access region
static vcpu_trap_result_t
addrspace_handle_guest_translation_fault(FAR_EL2_t far)
{
	vcpu_trap_result_t ret;

	uintptr_t addr = FAR_EL2_get_VirtualAddress(&far);

	thread_t *current = thread_get_self();
	assert(current != NULL);

	addrspace_t *addrspace = current->addrspace;
	assert(addrspace != NULL);

	rcu_read_start();
	if (!addrspace_undergoing_bbm(addrspace)) {
		// There is no BBM in progress, but there might have been when
		// the fault occurred. Perform a lookup to see whether the
		// accessed address is now mapped in S2.
		//
		// If the accessed address no longer faults in stage 2, we can
		// just retry the faulting access. Otherwise we can consider the
		// fault to be fatal, because there is no BBM operation still in
		// progress.
		ret = (addrspace_va_to_pa_read(addr).e != ERROR_DENIED)
			      ? VCPU_TRAP_RESULT_RETRY
			      : VCPU_TRAP_RESULT_UNHANDLED;
	} else {
		// A map operation is in progress, so retry until it finishes.
		// Note that we might get stuck here if the page table is
		// corrupt!
		ret = VCPU_TRAP_RESULT_RETRY;
	}
	rcu_read_finish();

	return ret;
}

vcpu_trap_result_t
addrspace_handle_vcpu_trap_data_abort_guest(ESR_EL2_t esr, vmaddr_result_t ipa,
					    FAR_EL2_t far)
{
	vcpu_trap_result_t ret = VCPU_TRAP_RESULT_UNHANDLED;

	ESR_EL2_ISS_DATA_ABORT_t iss =
		ESR_EL2_ISS_DATA_ABORT_cast(ESR_EL2_get_ISS(&esr));
	iss_da_ia_fsc_t fsc = ESR_EL2_ISS_DATA_ABORT_get_DFSC(&iss);

	if (fsc == ISS_DA_IA_FSC_TLB_CONFLICT) {
		ret = addrspace_handle_guest_tlb_conflict(
			ipa, far, ESR_EL2_ISS_DATA_ABORT_get_S1PTW(&iss));
	}

	// Only translation faults can be caused by BBM
	if ((fsc == ISS_DA_IA_FSC_TRANSLATION_1) ||
	    (fsc == ISS_DA_IA_FSC_TRANSLATION_2) ||
	    (fsc == ISS_DA_IA_FSC_TRANSLATION_3)) {
		ret = addrspace_handle_guest_translation_fault(far);
	}

	return ret;
}

vcpu_trap_result_t
addrspace_handle_vcpu_trap_pf_abort_guest(ESR_EL2_t esr, vmaddr_result_t ipa,
					  FAR_EL2_t far)
{
	vcpu_trap_result_t ret = VCPU_TRAP_RESULT_UNHANDLED;

	ESR_EL2_ISS_INST_ABORT_t iss =
		ESR_EL2_ISS_INST_ABORT_cast(ESR_EL2_get_ISS(&esr));
	iss_da_ia_fsc_t fsc = ESR_EL2_ISS_INST_ABORT_get_IFSC(&iss);

	if (fsc == ISS_DA_IA_FSC_TLB_CONFLICT) {
		ret = addrspace_handle_guest_tlb_conflict(
			ipa, far, ESR_EL2_ISS_INST_ABORT_get_S1PTW(&iss));
	}

	// Only translation faults can be caused by BBM
	if ((fsc == ISS_DA_IA_FSC_TRANSLATION_1) ||
	    (fsc == ISS_DA_IA_FSC_TRANSLATION_2) ||
	    (fsc == ISS_DA_IA_FSC_TRANSLATION_3)) {
		ret = addrspace_handle_guest_translation_fault(far);
	}

	return ret;
}
#else
extern char unused;
#endif

```

`hyp/mem/addrspace/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

interface addrspace
events addrspace.ev
types addrspace.tc
source addrspace.c hypercalls.c
arch_source aarch64 lookup.c vmmio.c
arch_events aarch64 addrspace.ev
arch_types aarch64 addrspace.tc
arch_events armv8 abort.ev
arch_source armv8 abort.c

```

`hyp/mem/addrspace/src/addrspace.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>
#include <string.h>

#include <hypconstants.h>
#include <hypregisters.h>
#include <hyprights.h>

#include <addrspace.h>
#include <atomic.h>
#include <bitmap.h>
#include <compiler.h>
#include <cpulocal.h>
#include <cspace.h>
#include <cspace_lookup.h>
#include <gpt.h>
#include <hyp_aspace.h>
#include <list.h>
#include <memextent.h>
#include <object.h>
#include <panic.h>
#include <partition.h>
#include <partition_alloc.h>
#include <pgtable.h>
#include <qcbor.h>
#include <spinlock.h>

#include <events/addrspace.h>

#include "event_handlers.h"

// FIXME: This file contains architecture specific details and should be
// refactored.

extern VTTBR_EL2_t hlos_vm_vttbr;

// FIXME: Limit VMIDs to reduce bitmap size
#if defined(ARCH_ARM_FEAT_VMID16)
#define NUM_VMIDS 256U
#else
#define NUM_VMIDS 256U
#endif

static _Atomic BITMAP_DECLARE(NUM_VMIDS, addrspace_vmids);

static_assert(ADDRSPACE_INFO_AREA_LAYOUT_SIZE <= MAX_VM_INFO_AREA_SIZE,
	      "Address space information area too small");

void
addrspace_handle_boot_cold_init(void)
{
	// Reserve VMID 0
	bool already_set = bitmap_atomic_test_and_set(addrspace_vmids, 0U,
						      memory_order_relaxed);
	assert(!already_set);
}

#if defined(INTERFACE_VCPU)
void
addrspace_context_switch_load(void)
{
	thread_t *thread = thread_get_self();

	if (compiler_expected(thread->kind == THREAD_KIND_VCPU)) {
		pgtable_vm_load_regs(&thread->addrspace->vm_pgtable);
	}
}

static void
addrspace_detach_thread(thread_t *thread)
{
	assert(thread != NULL);
	assert(thread->kind == THREAD_KIND_VCPU);
	assert(thread->addrspace != NULL);

	addrspace_t *addrspace = thread->addrspace;

	bitmap_atomic_clear(addrspace->stack_bitmap, thread->stack_map_index,
			    memory_order_relaxed);
	thread->addrspace = NULL;
	object_put_addrspace(addrspace);
}

error_t
addrspace_attach_thread(addrspace_t *addrspace, thread_t *thread)
{
	assert(thread != NULL);
	assert(addrspace != NULL);
	assert(atomic_load_relaxed(&addrspace->header.state) ==
	       OBJECT_STATE_ACTIVE);
	assert(atomic_load_relaxed(&thread->header.state) == OBJECT_STATE_INIT);

	error_t ret = OK;

	if (thread->kind != THREAD_KIND_VCPU) {
		ret = ERROR_ARGUMENT_INVALID;
		goto out;
	}

	index_t stack_index;
	do {
		if (!bitmap_atomic_ffc(addrspace->stack_bitmap,
				       ADDRSPACE_MAX_THREADS, &stack_index)) {
			ret = ERROR_NOMEM;
			goto out;
		}
	} while (bitmap_atomic_test_and_set(addrspace->stack_bitmap,
					    stack_index, memory_order_relaxed));

	if (thread->addrspace != NULL) {
		addrspace_detach_thread(thread);
	}

	thread->addrspace	= object_get_addrspace_additional(addrspace);
	thread->stack_map_index = stack_index;

	trace_ids_set_vmid(&thread->trace_ids, addrspace->vmid);

out:
	return ret;
}

addrspace_t *
addrspace_get_self(void)
{
	return thread_get_self()->addrspace;
}

error_t
addrspace_handle_object_activate_thread(thread_t *thread)
{
	error_t ret = OK;

	assert(thread != NULL);

	if ((thread->kind == THREAD_KIND_VCPU) && (thread->addrspace == NULL)) {
		ret = ERROR_OBJECT_CONFIG;
	}

	return ret;
}

void
addrspace_handle_object_deactivate_thread(thread_t *thread)
{
	if ((thread->kind == THREAD_KIND_VCPU) && (thread->addrspace != NULL)) {
		addrspace_detach_thread(thread);
	}
}

uintptr_t
addrspace_handle_thread_get_stack_base(thread_t *thread)
{
	assert(thread != NULL);
	assert(thread->kind == THREAD_KIND_VCPU);
	assert(thread->addrspace != NULL);

	virt_range_t *range = &thread->addrspace->hyp_va_range;

	// Align the starting base to the next boundary to ensure we have guard
	// pages before the first stack mapping.
	uintptr_t base =
		util_balign_up(range->base + 1U, THREAD_STACK_MAP_ALIGN);

	base += (uintptr_t)thread->stack_map_index * THREAD_STACK_MAP_ALIGN;

	assert((base + THREAD_STACK_MAX_SIZE) <
	       (range->base + (range->size - 1U)));

	return base;
}
#endif

#if defined(MODULE_VM_ROOTVM)
void
addrspace_handle_rootvm_init(thread_t *root_thread, cspace_t *root_cspace,
			     qcbor_enc_ctxt_t *qcbor_enc_ctxt)
{
	addrspace_create_t as_params = { NULL };

	// Create addrspace for root thread
	addrspace_ptr_result_t addrspace_ret = partition_allocate_addrspace(
		root_thread->header.partition, as_params);
	if (addrspace_ret.e != OK) {
		panic("Error creating root addrspace");
	}
	addrspace_t *root_addrspace = addrspace_ret.r;

	assert(!vcpu_option_flags_get_hlos_vm(&root_thread->vcpu_options));

	spinlock_acquire(&root_addrspace->header.lock);
	// FIXME:
	// Root VM address space could be smaller
	if (addrspace_configure(root_addrspace, ROOT_VM_VMID) != OK) {
		spinlock_release(&root_addrspace->header.lock);
		panic("Error configuring addrspace");
	}
	spinlock_release(&root_addrspace->header.lock);

	// Create a master cap for the addrspace
	object_ptr_t	obj_ptr	  = { .addrspace = root_addrspace };
	cap_id_result_t capid_ret = cspace_create_master_cap(
		root_cspace, obj_ptr, OBJECT_TYPE_ADDRSPACE);
	if (capid_ret.e != OK) {
		panic("Error create addrspace cap id.");
	}

	assert(qcbor_enc_ctxt != NULL);

	QCBOREncode_AddUInt64ToMap(qcbor_enc_ctxt, "addrspace_capid",
				   capid_ret.r);

	if (object_activate_addrspace(root_addrspace) != OK) {
		panic("Error activating addrspace");
	}

	// Attach address space to thread
	error_t ret = addrspace_attach_thread(root_addrspace, root_thread);
	if (ret != OK) {
		panic("Error attaching root addrspace to root thread.");
	}
}
#endif

error_t
addrspace_handle_object_create_addrspace(addrspace_create_t params)
{
	error_t ret;

	addrspace_t *addrspace = params.addrspace;
	assert(addrspace != NULL);
	spinlock_init(&addrspace->mapping_list_lock);
	spinlock_init(&addrspace->pgtable_lock);
#if defined(INTERFACE_VCPU_RUN)
	spinlock_init(&addrspace->vmmio_range_lock);
	gpt_config_t gpt_config = gpt_config_default();
	gpt_config_set_max_bits(&gpt_config, GPT_MAX_SIZE_BITS);
	gpt_config_set_rcu_read(&gpt_config, true);
	ret = gpt_init(&addrspace->vmmio_ranges, addrspace->header.partition,
		       gpt_config, util_bit(GPT_TYPE_VMMIO_RANGE));
	if (ret != OK) {
		goto out;
	}
#endif

	addrspace->info_area.ipa = VMADDR_INVALID;
	addrspace->info_area.me	 = NULL;

	// Allocate some hypervisor address space for the addrspace object use,
	// including kernel stacks of attached threads and vm_info_page.

	// Stack region size, including start and end guard regions.
	size_t stack_area_size =
		THREAD_STACK_MAP_ALIGN * (ADDRSPACE_MAX_THREADS + 2U);
	size_t alloc_size =
		stack_area_size +
		util_balign_up(MAX_VM_INFO_AREA_SIZE, PGTABLE_HYP_PAGE_SIZE);

	virt_range_result_t alloc_range = hyp_aspace_allocate(alloc_size);
	if (alloc_range.e == OK) {
		addrspace->hyp_va_range = alloc_range.r;

		addrspace->info_area.hyp_va =
			(addrspace_info_area_layout_t *)(alloc_range.r.base +
							 stack_area_size);
	}

	ret = alloc_range.e;
#if defined(INTERFACE_VCPU_RUN)
out:
#endif
	return ret;
}

void
addrspace_handle_object_cleanup_addrspace(addrspace_t *addrspace)
{
	assert(addrspace != NULL);

#if defined(INTERFACE_VCPU_RUN)
	gpt_destroy(&addrspace->vmmio_ranges);
#endif

	if (addrspace->hyp_va_range.size != 0U) {
		hyp_aspace_deallocate(addrspace->header.partition,
				      addrspace->hyp_va_range);
	}
}

void
addrspace_unwind_object_create_addrspace(addrspace_create_t params)
{
	addrspace_handle_object_cleanup_addrspace(params.addrspace);
}

error_t
addrspace_configure(addrspace_t *addrspace, vmid_t vmid)
{
	error_t ret = OK;

	assert(addrspace != NULL);

	if ((vmid == 0U) || (vmid >= NUM_VMIDS)) {
		ret = ERROR_ARGUMENT_INVALID;
	} else {
		addrspace->vmid = vmid;
	}

	return ret;
}

error_t
addrspace_configure_info_area(addrspace_t *addrspace, memextent_t *info_area_me,
			      vmaddr_t ipa)
{
	error_t ret = OK;

	assert(addrspace != NULL);

	size_t size = info_area_me->size;
	assert(size != 0);

#if (ADDRSPACE_INFO_AREA_LAYOUT_SIZE != 0)
	if ((size < (size_t)ADDRSPACE_INFO_AREA_LAYOUT_SIZE) ||
	    (size > (size_t)MAX_VM_INFO_AREA_SIZE))
#else
	if (size > (size_t)MAX_VM_INFO_AREA_SIZE)
#endif
	{
		ret = ERROR_ARGUMENT_INVALID;
		goto out;
	}

	if (!util_is_baligned(ipa, PGTABLE_HYP_PAGE_SIZE) ||
	    util_add_overflows(ipa, size) ||
	    ((ipa + size) > util_bit(PLATFORM_VM_ADDRESS_SPACE_BITS))) {
		ret = ERROR_ADDR_INVALID;
		goto out;
	}

	if ((info_area_me->type != MEMEXTENT_TYPE_BASIC) ||
	    (!pgtable_access_check(info_area_me->access, PGTABLE_ACCESS_RW)) ||
	    (info_area_me->memtype != MEMEXTENT_MEMTYPE_ANY)) {
		ret = ERROR_ARGUMENT_INVALID;
		goto out;
	}

	addrspace->info_area.ipa = ipa;
	if (addrspace->info_area.me != NULL) {
		object_put_memextent(addrspace->info_area.me);
	}
	addrspace->info_area.me = object_get_memextent_additional(info_area_me);

out:
	return ret;
}

error_t
addrspace_handle_object_activate_addrspace(addrspace_t *addrspace)
{
	error_t ret = OK;

	assert(addrspace != NULL);

	// FIXME:
	bool already_set = bitmap_atomic_test_and_set(
		addrspace_vmids, addrspace->vmid, memory_order_relaxed);
	if (already_set) {
		ret = ERROR_BUSY;
		goto out_busy;
	}

	partition_t *partition = addrspace->header.partition;
	ret = pgtable_vm_init(partition, &addrspace->vm_pgtable,
			      addrspace->vmid);
	if (ret != OK) {
		goto out_vmid_dealloc;
	}

	if (addrspace->info_area.me != NULL) {
		vmaddr_t  ipa  = addrspace->info_area.ipa;
		uintptr_t va   = (uintptr_t)addrspace->info_area.hyp_va;
		size_t	  size = addrspace->info_area.me->size;

		// Ensure the IPA is within VM's range
		ret = addrspace_check_range(addrspace, ipa, size);
		if (ret != OK) {
			goto out_vmid_dealloc;
		}

		// Attach the extent to the addrspace object.
		ret = memextent_attach(partition, addrspace->info_area.me, va,
				       size);
		if (ret != OK) {
			object_put_memextent(addrspace->info_area.me);
			addrspace->info_area.me = NULL;
			goto out_vmid_dealloc;
		}

		assert(va != 0u);
		(void)memset_s((uint8_t *)va, size, 0, size);
	}

out_vmid_dealloc:
	if (ret != OK) {
		// Undo the vmid allocation
		(void)bitmap_atomic_test_and_clear(
			addrspace_vmids, addrspace->vmid, memory_order_relaxed);
		addrspace->vmid = 0U;
	}
out_busy:
	return ret;
}

void
addrspace_handle_object_deactivate_addrspace(addrspace_t *addrspace)
{
	assert(addrspace != NULL);

	if (addrspace->info_area.me != NULL) {
		memextent_detach(addrspace->header.partition,
				 addrspace->info_area.me);

		object_put_memextent(addrspace->info_area.me);
		addrspace->info_area.me = NULL;
	}

	if (!addrspace->read_only) {
		pgtable_vm_destroy(addrspace->header.partition,
				   &addrspace->vm_pgtable);
	}

	bool set = bitmap_atomic_test_and_clear(
		addrspace_vmids, addrspace->vm_pgtable.control.vmid,
		memory_order_relaxed);
	if (!set) {
		panic("VMID bitmap never set or already cleared.");
	}
	addrspace->vmid = 0U;
}

error_t
addrspace_map(addrspace_t *addrspace, vmaddr_t vbase, size_t size, paddr_t phys,
	      pgtable_vm_memtype_t memtype, pgtable_access_t kernel_access,
	      pgtable_access_t user_access)
{
	error_t err = trigger_addrspace_map_event(addrspace, vbase, size, phys,
						  memtype, kernel_access,
						  user_access);
	if (err != ERROR_UNIMPLEMENTED) {
		goto out;
	}

	if (addrspace->read_only) {
		err = ERROR_DENIED;
		goto out;
	}

	spinlock_acquire(&addrspace->pgtable_lock);
	pgtable_vm_start(&addrspace->vm_pgtable);

	// We do not set the try_map option; we expect the caller to know if it
	// is overwriting an existing mapping.
	err = pgtable_vm_map(addrspace->header.partition,
			     &addrspace->vm_pgtable, vbase, size, phys, memtype,
			     kernel_access, user_access, false, false);

	pgtable_vm_commit(&addrspace->vm_pgtable);
	spinlock_release(&addrspace->pgtable_lock);

out:
	return err;
}

error_t
addrspace_unmap(addrspace_t *addrspace, vmaddr_t vbase, size_t size,
		paddr_t phys)
{
	error_t err =
		trigger_addrspace_unmap_event(addrspace, vbase, size, phys);
	if (err != ERROR_UNIMPLEMENTED) {
		goto out;
	}

	if (addrspace->read_only) {
		err = ERROR_DENIED;
		goto out;
	}

	spinlock_acquire(&addrspace->pgtable_lock);
	pgtable_vm_start(&addrspace->vm_pgtable);

	// Unmap only if the physical address is matching.
	pgtable_vm_unmap_matching(addrspace->header.partition,
				  &addrspace->vm_pgtable, vbase, phys, size);
	err = OK;

	pgtable_vm_commit(&addrspace->vm_pgtable);
	spinlock_release(&addrspace->pgtable_lock);

out:
	return err;
}

addrspace_lookup_result_t
addrspace_lookup(addrspace_t *addrspace, vmaddr_t vbase, size_t size)
{
	addrspace_lookup_result_t ret;

	assert(addrspace != NULL);

	if (size == 0U) {
		ret = addrspace_lookup_result_error(ERROR_ARGUMENT_SIZE);
		goto out;
	}

	if (util_add_overflows(vbase, size - 1U)) {
		ret = addrspace_lookup_result_error(ERROR_ADDR_OVERFLOW);
		goto out;
	}

	if (!util_is_baligned(vbase, PGTABLE_VM_PAGE_SIZE) ||
	    !util_is_baligned(size, PGTABLE_VM_PAGE_SIZE)) {
		ret = addrspace_lookup_result_error(ERROR_ARGUMENT_ALIGNMENT);
		goto out;
	}

	bool		     first_lookup   = true;
	paddr_t		     lookup_phys    = 0U;
	size_t		     lookup_size    = 0U;
	pgtable_vm_memtype_t lookup_memtype = PGTABLE_VM_MEMTYPE_NORMAL_WB;
	pgtable_access_t     lookup_kernel_access = PGTABLE_ACCESS_NONE;
	pgtable_access_t     lookup_user_access	  = PGTABLE_ACCESS_NONE;

	spinlock_acquire(&addrspace->pgtable_lock);

	bool   mapped	   = true;
	size_t mapped_size = 0U;
	for (size_t offset = 0U; offset < size; offset += mapped_size) {
		vmaddr_t	     curr = vbase + offset;
		paddr_t		     mapped_phys;
		pgtable_vm_memtype_t mapped_memtype;
		pgtable_access_t     mapped_kernel_access, mapped_user_access;

		mapped = pgtable_vm_lookup(&addrspace->vm_pgtable, curr,
					   &mapped_phys, &mapped_size,
					   &mapped_memtype,
					   &mapped_kernel_access,
					   &mapped_user_access);
		if (mapped) {
			size_t mapping_offset = curr & (mapped_size - 1U);
			mapped_phys += mapping_offset;
			mapped_size = util_min(mapped_size - mapping_offset,
					       size - offset);

			if (first_lookup) {
				lookup_phys	     = mapped_phys;
				lookup_size	     = mapped_size;
				lookup_memtype	     = mapped_memtype;
				lookup_kernel_access = mapped_kernel_access;
				lookup_user_access   = mapped_user_access;
				first_lookup	     = false;
			} else if (((lookup_phys + lookup_size) ==
				    mapped_phys) &&
				   (lookup_memtype == mapped_memtype) &&
				   (lookup_kernel_access ==
				    mapped_kernel_access) &&
				   (lookup_user_access == mapped_user_access)) {
				lookup_size += mapped_size;
			} else {
				// Mapped range no longer contiguous, end the
				// lookup.
				break;
			}
		} else {
			break;
		}
	}

	spinlock_release(&addrspace->pgtable_lock);

	if (first_lookup) {
		ret = addrspace_lookup_result_error(ERROR_ADDR_INVALID);
		goto out;
	}

	addrspace_lookup_t lookup = {
		.phys	       = lookup_phys,
		.size	       = lookup_size,
		.memtype       = lookup_memtype,
		.kernel_access = lookup_kernel_access,
		.user_access   = lookup_user_access,
	};

	ret = addrspace_lookup_result_ok(lookup);

out:
	return ret;
}

error_t
addrspace_add_vmmio_range(addrspace_t *addrspace, vmaddr_t base, size_t size)
{
	error_t ret;

#if defined(INTERFACE_VCPU_RUN)
	if (size == 0U) {
		ret = ERROR_ARGUMENT_SIZE;
		goto out;
	}

	if (util_add_overflows(base, size)) {
		ret = ERROR_ADDR_OVERFLOW;
		goto out;
	}

	spinlock_acquire(&addrspace->vmmio_range_lock);

	if (addrspace->vmmio_range_count == ADDRSPACE_MAX_VMMIO_RANGES) {
		ret = ERROR_NORESOURCES;
		goto out_locked;
	}

	gpt_entry_t entry = (gpt_entry_t){
		.type			= GPT_TYPE_VMMIO_RANGE,
		.value.vmmio_range_base = base,
	};

	ret = gpt_insert(&addrspace->vmmio_ranges, base, size, entry, true);

	if (ret == OK) {
		addrspace->vmmio_range_count++;
	}

out_locked:
	spinlock_release(&addrspace->vmmio_range_lock);
out:
#else // !INTERFACE_VCPU_RUN
	(void)addrspace;
	(void)base;
	(void)size;
	ret = ERROR_UNIMPLEMENTED;
#endif
	return ret;
}

error_t
addrspace_remove_vmmio_range(addrspace_t *addrspace, vmaddr_t base, size_t size)
{
	error_t ret;

#if defined(INTERFACE_VCPU_RUN)
	if (size == 0U) {
		ret = ERROR_ARGUMENT_SIZE;
		goto out;
	}

	if (util_add_overflows(base, size)) {
		ret = ERROR_ADDR_OVERFLOW;
		goto out;
	}

	spinlock_acquire(&addrspace->vmmio_range_lock);

	gpt_entry_t entry = (gpt_entry_t){
		.type			= GPT_TYPE_VMMIO_RANGE,
		.value.vmmio_range_base = base,
	};

	ret = gpt_remove(&addrspace->vmmio_ranges, base, size, entry);

	if (ret == OK) {
		assert(addrspace->vmmio_range_count > 0U);
		addrspace->vmmio_range_count--;
	}

	spinlock_release(&addrspace->vmmio_range_lock);
out:
#else // !INTERFACE_VCPU_RUN
	(void)addrspace;
	(void)base;
	(void)size;
	ret = ERROR_UNIMPLEMENTED;
#endif
	return ret;
}

```

`hyp/mem/addrspace/src/hypercalls.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>

#include <hypcall_def.h>
#include <hyprights.h>

#include <atomic.h>
#include <compiler.h>
#include <cspace.h>
#include <cspace_lookup.h>
#include <memdb.h>
#include <memextent.h>
#include <object.h>
#include <pgtable.h>
#include <rcu.h>
#include <spinlock.h>

#include "addrspace.h"
#include "events/addrspace.h"

error_t
hypercall_addrspace_attach_thread(cap_id_t addrspace_cap, cap_id_t thread_cap)
{
	error_t	      ret;
	cspace_t     *cspace = cspace_get_self();
	object_type_t type;

	object_ptr_result_t o = cspace_lookup_object_any(
		cspace, thread_cap, CAP_RIGHTS_GENERIC_OBJECT_ACTIVATE, &type);
	if (compiler_unexpected(o.e != OK)) {
		ret = o.e;
		goto out;
	}

	if (type != OBJECT_TYPE_THREAD) {
		ret = ERROR_CSPACE_WRONG_OBJECT_TYPE;
		goto out_thread_release;
	}

	thread_t *thread = o.r.thread;

	addrspace_ptr_result_t c = cspace_lookup_addrspace(
		cspace, addrspace_cap, CAP_RIGHTS_ADDRSPACE_ATTACH);
	if (compiler_unexpected(c.e != OK)) {
		ret = c.e;
		goto out_thread_release;
	}

	addrspace_t *addrspace = c.r;

	spinlock_acquire(&thread->header.lock);

	if (atomic_load_relaxed(&thread->header.state) == OBJECT_STATE_INIT) {
		ret = addrspace_attach_thread(addrspace, thread);
	} else {
		ret = ERROR_OBJECT_STATE;
	}

	spinlock_release(&thread->header.lock);

	object_put_addrspace(addrspace);

out_thread_release:
	object_put(type, o.r);
out:
	return ret;
}

error_t
hypercall_addrspace_attach_vdma(cap_id_t addrspace_cap, cap_id_t dma_device_cap,
				index_t index)
{
	error_t	  err;
	cspace_t *cspace = cspace_get_self();

	addrspace_ptr_result_t addrspace_r = cspace_lookup_addrspace(
		cspace, addrspace_cap, CAP_RIGHTS_ADDRSPACE_ATTACH);
	if (compiler_unexpected(addrspace_r.e != OK)) {
		err = addrspace_r.e;
		goto out;
	}

	err = trigger_addrspace_attach_vdma_event(addrspace_r.r, dma_device_cap,
						  index);

	object_put_addrspace(addrspace_r.r);
out:
	return err;
}

error_t
hypercall_addrspace_attach_vdevice(cap_id_t addrspace_cap, cap_id_t vdevice_cap,
				   index_t index, vmaddr_t vbase, size_t size,
				   addrspace_attach_vdevice_flags_t flags)
{
	error_t	  err;
	cspace_t *cspace = cspace_get_self();

	addrspace_ptr_result_t addrspace_r = cspace_lookup_addrspace(
		cspace, addrspace_cap, CAP_RIGHTS_ADDRSPACE_MAP);
	if (compiler_unexpected(addrspace_r.e != OK)) {
		err = addrspace_r.e;
		goto out;
	}

	err = trigger_addrspace_attach_vdevice_event(addrspace_r.r, vdevice_cap,
						     index, vbase, size, flags);

	object_put_addrspace(addrspace_r.r);
out:
	return err;
}

error_t
hypercall_addrspace_map(cap_id_t addrspace_cap, cap_id_t memextent_cap,
			vmaddr_t vbase, memextent_mapping_attrs_t map_attrs,
			addrspace_map_flags_t map_flags, size_t offset,
			size_t size)
{
	error_t	  ret;
	cspace_t *cspace = cspace_get_self();

	if ((memextent_mapping_attrs_get_res_0(&map_attrs) != 0U) ||
	    (addrspace_map_flags_get_res0_0(&map_flags) != 0U)) {
		ret = ERROR_ARGUMENT_INVALID;
		goto out;
	}

	addrspace_ptr_result_t c = cspace_lookup_addrspace(
		cspace, addrspace_cap, CAP_RIGHTS_ADDRSPACE_MAP);
	if (compiler_unexpected(c.e != OK)) {
		ret = c.e;
		goto out;
	}

	addrspace_t *addrspace = c.r;

	memextent_ptr_result_t m = cspace_lookup_memextent(
		cspace, memextent_cap, CAP_RIGHTS_MEMEXTENT_MAP);
	if (compiler_unexpected(m.e != OK)) {
		ret = m.e;
		goto out_addrspace_release;
	}

	memextent_t *memextent = m.r;

	if (addrspace_map_flags_get_partial(&map_flags)) {
		ret = memextent_map_partial(memextent, addrspace, vbase, offset,
					    size, map_attrs);
	} else {
		ret = memextent_map(memextent, addrspace, vbase, map_attrs);
	}

	if ((ret == OK) && !addrspace_map_flags_get_no_sync(&map_flags)) {
		// Wait for completion of EL2 operations using manual lookups
		rcu_sync();
	}

	object_put_memextent(memextent);
out_addrspace_release:
	object_put_addrspace(addrspace);
out:
	return ret;
}

error_t
hypercall_addrspace_unmap(cap_id_t addrspace_cap, cap_id_t memextent_cap,
			  vmaddr_t vbase, addrspace_map_flags_t map_flags,
			  size_t offset, size_t size)
{
	error_t	  ret;
	cspace_t *cspace = cspace_get_self();

	if (addrspace_map_flags_get_res0_0(&map_flags) != 0U) {
		ret = ERROR_ARGUMENT_INVALID;
		goto out;
	}

	addrspace_ptr_result_t c = cspace_lookup_addrspace(
		cspace, addrspace_cap, CAP_RIGHTS_ADDRSPACE_MAP);
	if (compiler_unexpected(c.e != OK)) {
		ret = c.e;
		goto out;
	}

	addrspace_t *addrspace = c.r;

	memextent_ptr_result_t m = cspace_lookup_memextent(
		cspace, memextent_cap, CAP_RIGHTS_MEMEXTENT_MAP);
	if (compiler_unexpected(m.e != OK)) {
		ret = m.e;
		goto out_addrspace_release;
	}

	memextent_t *memextent = m.r;

	if (addrspace_map_flags_get_partial(&map_flags)) {
		ret = memextent_unmap_partial(memextent, addrspace, vbase,
					      offset, size);
	} else {
		ret = memextent_unmap(memextent, addrspace, vbase);
	}

	if ((ret == OK) && !addrspace_map_flags_get_no_sync(&map_flags)) {
		// Wait for completion of EL2 operations using manual lookups
		rcu_sync();
	}

	object_put_memextent(memextent);
out_addrspace_release:
	object_put_addrspace(addrspace);
out:
	return ret;
}

error_t
hypercall_addrspace_update_access(cap_id_t addrspace_cap,
				  cap_id_t memextent_cap, vmaddr_t vbase,
				  memextent_access_attrs_t access_attrs,
				  addrspace_map_flags_t	   map_flags,
				  size_t offset, size_t size)
{
	error_t	  ret;
	cspace_t *cspace = cspace_get_self();

	if ((memextent_access_attrs_get_res_0(&access_attrs) != 0U) ||
	    (addrspace_map_flags_get_res0_0(&map_flags) != 0U)) {
		ret = ERROR_ARGUMENT_INVALID;
		goto out;
	}

	addrspace_ptr_result_t c = cspace_lookup_addrspace(
		cspace, addrspace_cap, CAP_RIGHTS_ADDRSPACE_MAP);
	if (compiler_unexpected(c.e != OK)) {
		ret = c.e;
		goto out;
	}

	addrspace_t *addrspace = c.r;

	memextent_ptr_result_t m = cspace_lookup_memextent(
		cspace, memextent_cap, CAP_RIGHTS_MEMEXTENT_MAP);
	if (compiler_unexpected(m.e != OK)) {
		ret = m.e;
		goto out_addrspace_release;
	}

	memextent_t *memextent = m.r;

	if (addrspace_map_flags_get_partial(&map_flags)) {
		ret = memextent_update_access_partial(memextent, addrspace,
						      vbase, offset, size,
						      access_attrs);
	} else {
		ret = memextent_update_access(memextent, addrspace, vbase,
					      access_attrs);
	}

	if ((ret == OK) && !addrspace_map_flags_get_no_sync(&map_flags)) {
		// Wait for completion of EL2 operations using manual lookups
		rcu_sync();
	}

	object_put_memextent(memextent);
out_addrspace_release:
	object_put_addrspace(addrspace);
out:
	return ret;
}

error_t
hypercall_addrspace_configure(cap_id_t addrspace_cap, vmid_t vmid)
{
	error_t	      err;
	cspace_t     *cspace = cspace_get_self();
	object_type_t type;

	object_ptr_result_t o = cspace_lookup_object_any(
		cspace, addrspace_cap, CAP_RIGHTS_GENERIC_OBJECT_ACTIVATE,
		&type);
	if (compiler_unexpected(o.e != OK)) {
		err = o.e;
		goto out;
	}
	if (type != OBJECT_TYPE_ADDRSPACE) {
		err = ERROR_CSPACE_WRONG_OBJECT_TYPE;
		goto out_addrspace_release;
	}

	addrspace_t *target_as = o.r.addrspace;

	spinlock_acquire(&target_as->header.lock);

	if (atomic_load_relaxed(&target_as->header.state) ==
	    OBJECT_STATE_INIT) {
		err = addrspace_configure(target_as, vmid);
	} else {
		err = ERROR_OBJECT_STATE;
	}

	spinlock_release(&target_as->header.lock);
out_addrspace_release:
	object_put(type, o.r);
out:
	return err;
}

hypercall_addrspace_lookup_result_t
hypercall_addrspace_lookup(cap_id_t addrspace_cap, cap_id_t memextent_cap,
			   vmaddr_t vbase, size_t size)
{
	hypercall_addrspace_lookup_result_t ret	   = { .error = OK };
	cspace_t			   *cspace = cspace_get_self();

	addrspace_ptr_result_t a = cspace_lookup_addrspace(
		cspace, addrspace_cap, CAP_RIGHTS_ADDRSPACE_LOOKUP);
	if (compiler_unexpected(a.e != OK)) {
		ret.error = a.e;
		goto out;
	}

	addrspace_t *addrspace = a.r;

	memextent_ptr_result_t m = cspace_lookup_memextent(
		cspace, memextent_cap, CAP_RIGHTS_MEMEXTENT_LOOKUP);
	if (compiler_unexpected(m.e != OK)) {
		ret.error = m.e;
		goto out_addrspace_release;
	}

	memextent_t *memextent = m.r;

	addrspace_lookup_result_t lookup_ret =
		addrspace_lookup(addrspace, vbase, size);
	if (lookup_ret.e != OK) {
		ret.error = lookup_ret.e;
		goto out_memextent_release;
	}

	// Determine if the memextent owns the range of memory returned by the
	// lookup.
	paddr_t phys_start = lookup_ret.r.phys;
	paddr_t phys_end   = phys_start + (lookup_ret.r.size - 1U);
	if (!memdb_is_ownership_contiguous(phys_start, phys_end,
					   (uintptr_t)memextent,
					   MEMDB_TYPE_EXTENT)) {
		ret.error = ERROR_MEMDB_NOT_OWNER;
		goto out_memextent_release;
	}

	assert((phys_start >= memextent->phys_base) &&
	       (phys_end <= (memextent->phys_base + (memextent->size - 1U))));

	memextent_mapping_attrs_t map_attrs = memextent_mapping_attrs_default();
	memextent_mapping_attrs_set_memtype(&map_attrs, lookup_ret.r.memtype);
	memextent_mapping_attrs_set_user_access(&map_attrs,
						lookup_ret.r.user_access);
	memextent_mapping_attrs_set_kernel_access(&map_attrs,
						  lookup_ret.r.kernel_access);

	ret.offset    = phys_start - memextent->phys_base;
	ret.size      = lookup_ret.r.size;
	ret.map_attrs = map_attrs;

out_memextent_release:
	object_put_memextent(memextent);
out_addrspace_release:
	object_put_addrspace(addrspace);
out:
	return ret;
}

error_t
hypercall_addrspace_configure_info_area(cap_id_t addrspace_cap,
					cap_id_t info_area_me_cap, vmaddr_t ipa)
{
	error_t	      err;
	cspace_t     *cspace = cspace_get_self();
	object_type_t type;

	object_ptr_result_t o = cspace_lookup_object_any(
		cspace, addrspace_cap, CAP_RIGHTS_GENERIC_OBJECT_ACTIVATE,
		&type);
	if (compiler_unexpected(o.e != OK)) {
		err = o.e;
		goto out_bad_cap;
	}
	if (type != OBJECT_TYPE_ADDRSPACE) {
		err = ERROR_CSPACE_WRONG_OBJECT_TYPE;
		goto out_addrspace_release;
	}
	addrspace_t *target_as = o.r.addrspace;

	memextent_ptr_result_t m = cspace_lookup_memextent(
		cspace, info_area_me_cap, CAP_RIGHTS_MEMEXTENT_ATTACH);
	if (compiler_unexpected(m.e != OK)) {
		err = m.e;
		goto out_addrspace_release;
	}
	memextent_t *info_area_me = m.r;

	spinlock_acquire(&target_as->header.lock);
	if (atomic_load_relaxed(&target_as->header.state) ==
	    OBJECT_STATE_INIT) {
		err = addrspace_configure_info_area(target_as, info_area_me,
						    ipa);
	} else {
		err = ERROR_OBJECT_STATE;
	}
	spinlock_release(&target_as->header.lock);

	object_put_memextent(info_area_me);
out_addrspace_release:
	object_put(type, o.r);
out_bad_cap:
	return err;
}

error_t
hypercall_addrspace_configure_vmmio(cap_id_t addrspace_cap, vmaddr_t vbase,
				    size_t			   size,
				    addrspace_vmmio_configure_op_t op)
{
	error_t	  err;
	cspace_t *cspace = cspace_get_self();

	addrspace_ptr_result_t o = cspace_lookup_addrspace_any(
		cspace, addrspace_cap, CAP_RIGHTS_ADDRSPACE_ADD_VMMIO_RANGE);
	if (compiler_unexpected(o.e != OK)) {
		err = o.e;
		goto out;
	}

	addrspace_t *target_as = o.r;

	object_state_t state = atomic_load_relaxed(&target_as->header.state);
	if ((state != OBJECT_STATE_INIT) && (state != OBJECT_STATE_ACTIVE)) {
		err = ERROR_OBJECT_STATE;
		goto out_ref;
	}

	switch (op) {
	case ADDRSPACE_VMMIO_CONFIGURE_OP_ADD:
		err = addrspace_add_vmmio_range(target_as, vbase, size);
		break;
	case ADDRSPACE_VMMIO_CONFIGURE_OP_REMOVE:
		err = addrspace_remove_vmmio_range(target_as, vbase, size);
		break;
	default:
		err = ERROR_UNIMPLEMENTED;
		break;
	}

out_ref:
	object_put_addrspace(o.r);
out:
	return err;
}

```

`hyp/mem/allocator_boot/bootmem.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

module allocator_boot

subscribe boot_runtime_first_init ()
	priority first

```

`hyp/mem/allocator_boot/bootmem.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

define bootmem_allocator structure {
	pool_base pointer uint8;
	pool_size size;
	alloc_offset size;
};

```

`hyp/mem/allocator_boot/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

interface bootmem
types bootmem.tc
events bootmem.ev
source bootmem.c

```

`hyp/mem/allocator_boot/src/bootmem.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>

#include <bootmem.h>
#include <util.h>

#include "event_handlers.h"

static bootmem_allocator_t bootmem_allocator;

// For now the hypervisor private heap is statically defined in the linker
// script. The intention is to replace this with dynamically determined memory -
// such as through boot configuration structures.
extern uint8_t heap_private_start;
extern uint8_t heap_private_end;
extern uint8_t image_virt_end;

void
allocator_boot_handle_boot_runtime_first_init(void)
{
	assert((uintptr_t)&heap_private_end > (uintptr_t)&heap_private_start);

	static_assert(
		PLATFORM_HEAP_PRIVATE_SIZE <= PLATFORM_RW_DATA_SIZE,
		"PLATFORM_HEAP_PRIVATE_SIZE must be <= PLATFORM_RW_DATA_SIZE");
	static_assert(PLATFORM_RW_DATA_SIZE >= 0x200000,
		      "PLATFORM_RW_DATA_SIZE must be >= 2MB");

	// We only give heap within the first 2MB RW page to the bootmem. We
	// will map the rest of the heap during the hyp_aspace init.
	void	 *base	  = &heap_private_start;
	uintptr_t map_end = util_balign_up((uintptr_t)base, 0x200000U);

	uintptr_t hyp_priv_end =
		(((uintptr_t)&image_virt_end) - (size_t)PLATFORM_RW_DATA_SIZE) +
		(size_t)PLATFORM_HEAP_PRIVATE_SIZE;
	uintptr_t end = util_min(map_end, hyp_priv_end);

	assert((uintptr_t)base < end);
	size_t size = end - (uintptr_t)base;

	assert(base != NULL);
	assert(size >= 0x1000U);

	bootmem_allocator.pool_base    = (uint8_t *)base;
	bootmem_allocator.pool_size    = size;
	bootmem_allocator.alloc_offset = 0;
}

void_ptr_result_t
bootmem_allocate(size_t size, size_t align)
{
	assert(bootmem_allocator.alloc_offset <= bootmem_allocator.pool_size);
	uintptr_t loc = (uintptr_t)bootmem_allocator.pool_base;

	assert(!util_add_overflows(loc, bootmem_allocator.alloc_offset));
	loc += bootmem_allocator.alloc_offset;

	if (!util_is_p2_or_zero(align)) {
		return void_ptr_result_error(ERROR_ARGUMENT_ALIGNMENT);
	}
	if (util_is_p2(align)) {
		loc = util_balign_up(loc, align);
	}

	size_t free_boot = bootmem_allocator.pool_size -
			   (loc - (uintptr_t)bootmem_allocator.pool_base);

	if (size > free_boot) {
		return void_ptr_result_error(ERROR_NOMEM);
	}

	bootmem_allocator.alloc_offset =
		(loc - (uintptr_t)bootmem_allocator.pool_base + size);

	return void_ptr_result_ok((void *)loc);
}

void_ptr_result_t
bootmem_allocate_remaining(size_t *size)
{
	assert(size != NULL);

	assert(bootmem_allocator.alloc_offset <= bootmem_allocator.pool_size);
	size_t free_boot =
		bootmem_allocator.pool_size - bootmem_allocator.alloc_offset;
	if (free_boot == 0U) {
		return void_ptr_result_error(ERROR_NOMEM);
	}
	*size = free_boot;

	uintptr_t loc = (uintptr_t)bootmem_allocator.pool_base;
	assert(!util_add_overflows(loc, bootmem_allocator.alloc_offset));
	loc += bootmem_allocator.alloc_offset;

	bootmem_allocator.alloc_offset = bootmem_allocator.pool_size;

	return void_ptr_result_ok((void *)loc);
}

void *
bootmem_get_region(size_t *size)
{
	*size = bootmem_allocator.pool_size;
	return bootmem_allocator.pool_base;
}

```

`hyp/mem/allocator_list/allocator.ev`:

```ev
// © 2022 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

module allocator_list

subscribe allocator_add_ram_range
	priority last

```

`hyp/mem/allocator_list/allocator.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

define allocator_node structure {
	size size;
	next pointer structure allocator_node;
};

extend allocator structure {
	heap pointer structure allocator_node;
	lock structure spinlock;
	total_size size;
	alloc_size size;
};

```

`hyp/mem/allocator_list/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

interface allocator

events allocator.ev
types allocator.tc
source freelist.c

```

`hyp/mem/allocator_list/src/freelist.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// The following copyright has been added because the code in this file is
// based on the objmanager_simple allocator from OKL4 3.0

// Australian Public Licence B (OZPLB)
//
// Version 1-0
//
// Copyright (c) 2006-2010, Open Kernel Labs, Inc.
//
// All rights reserved.
//
// Developed by: Embedded, Real-time and Operating Systems Program (ERTOS)
//               National ICT Australia
//               http://www.ertos.nicta.com.au
//
// Permission is granted by Open Kernel Labs, Inc., free of charge, to
// any person obtaining a copy of this software and any associated
// documentation files (the "Software") to deal with the Software without
// restriction, including (without limitation) the rights to use, copy,
// modify, adapt, merge, publish, distribute, communicate to the public,
// sublicense, and/or sell, lend or rent out copies of the Software, and
// to permit persons to whom the Software is furnished to do so, subject
// to the following conditions:
//
//     * Redistributions of source code must retain the above copyright
//       notice, this list of conditions and the following disclaimers.
//
//     * Redistributions in binary form must reproduce the above
//       copyright notice, this list of conditions and the following
//       disclaimers in the documentation and/or other materials provided
//       with the distribution.
//
//     * Neither the name of Open Kernel Labs, Inc., nor the names of its
//       contributors, may be used to endorse or promote products derived
//       from this Software without specific prior written permission.
//
// EXCEPT AS EXPRESSLY STATED IN THIS LICENCE AND TO THE FULL EXTENT
// PERMITTED BY APPLICABLE LAW, THE SOFTWARE IS PROVIDED "AS-IS", AND
// NATIONAL ICT AUSTRALIA AND ITS CONTRIBUTORS MAKE NO REPRESENTATIONS,
// WARRANTIES OR CONDITIONS OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING
// BUT NOT LIMITED TO ANY REPRESENTATIONS, WARRANTIES OR CONDITIONS
// REGARDING THE CONTENTS OR ACCURACY OF THE SOFTWARE, OR OF TITLE,
// MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, NONINFRINGEMENT,
// THE ABSENCE OF LATENT OR OTHER DEFECTS, OR THE PRESENCE OR ABSENCE OF
// ERRORS, WHETHER OR NOT DISCOVERABLE.
//
// TO THE FULL EXTENT PERMITTED BY APPLICABLE LAW, IN NO EVENT SHALL
// NATIONAL ICT AUSTRALIA OR ITS CONTRIBUTORS BE LIABLE ON ANY LEGAL
// THEORY (INCLUDING, WITHOUT LIMITATION, IN AN ACTION OF CONTRACT,
// NEGLIGENCE OR OTHERWISE) FOR ANY CLAIM, LOSS, DAMAGES OR OTHER
// LIABILITY, INCLUDING (WITHOUT LIMITATION) LOSS OF PRODUCTION OR
// OPERATION TIME, LOSS, DAMAGE OR CORRUPTION OF DATA OR RECORDS; OR LOSS
// OF ANTICIPATED SAVINGS, OPPORTUNITY, REVENUE, PROFIT OR GOODWILL, OR
// OTHER ECONOMIC LOSS; OR ANY SPECIAL, INCIDENTAL, INDIRECT,
// CONSEQUENTIAL, PUNITIVE OR EXEMPLARY DAMAGES, ARISING OUT OF OR IN
// CONNECTION WITH THIS LICENCE, THE SOFTWARE OR THE USE OF OR OTHER
// DEALINGS WITH THE SOFTWARE, EVEN IF NATIONAL ICT AUSTRALIA OR ITS
// CONTRIBUTORS HAVE BEEN ADVISED OF THE POSSIBILITY OF SUCH CLAIM, LOSS,
// DAMAGES OR OTHER LIABILITY.
//
// If applicable legislation implies representations, warranties, or
// conditions, or imposes obligations or liability on Open Kernel Labs, Inc.
// or one of its contributors in respect of the Software that
// cannot be wholly or partly excluded, restricted or modified, the
// liability of Open Kernel Labs, Inc. or the contributor is limited, to
// the full extent permitted by the applicable legislation, at its
// option, to:
// a.  in the case of goods, any one or more of the following:
// i.  the replacement of the goods or the supply of equivalent goods;
// ii.  the repair of the goods;
// iii. the payment of the cost of replacing the goods or of acquiring
//  equivalent goods;
// iv.  the payment of the cost of having the goods repaired; or
// b.  in the case of services:
// i.  the supplying of the services again; or
// ii.  the payment of the cost of having the services supplied again.
//
// The construction, validity and performance of this licence is governed
// by the laws in force in New South Wales, Australia.

#include <assert.h>
#include <hyptypes.h>
#include <string.h>

#include <allocator.h>
#include <attributes.h>
#include <spinlock.h>
#include <util.h>

#include "event_handlers.h"

// Maximum supported heap allocation size or alignment size. We filter out
// really large allocations so we can avoid having to think about corner-cases
// causing overflow.
#define MAX_ALLOC_SIZE	   (256UL * 1024UL * 1024UL)
#define MAX_ALIGNMENT_SIZE (16UL * 1024UL * 1024UL)

#define NODE_HEADER_SIZE (sizeof(allocator_node_t))

// Minimum allocation size from the heap.
#define HEAP_MIN_ALLOC NODE_HEADER_SIZE
#define HEAP_MIN_ALIGN NODE_HEADER_SIZE

// -------------- DEBUGGING --------------
#if defined(ALLOCATOR_DEBUG)
#define OVERFLOW_DEBUG
#define OVERFLOW_REDZONE_SIZE NODE_HEADER_SIZE
// #define DEBUG_PRINT
#endif
// ---------------------------------------

#if defined(ALLOCATOR_DEBUG)
#define CHECK_HEAP(x) check_heap_consistency(x)
#else
#define CHECK_HEAP(x)
#endif

#if defined(ALLOCATOR_DEBUG)
// Checking heap consistency:
// - Previous block should have virtual address before current block.
// - Blocks should not overlap, otherwise they should be merged.
// - Each block should be 8-byte aligned, and have a size of at least 8 bytes.
static void
check_heap_consistency(allocator_node_t *head)
{
	if (head != NULL) {
		allocator_node_t *previous = head;
		allocator_node_t *current  = head->next;

		while (current != NULL) {
#if defined(DEBUG_PRINT)
			printf("[%p : %lx] -> ", current, current->size);
#endif

			assert((uint64_t)previous < (uint64_t)current);
			assert(((uint64_t)previous + previous->size) <=
			       (uint64_t)current);
			assert(((uint64_t)current % NODE_HEADER_SIZE) == 0UL);
			assert(current->size >= NODE_HEADER_SIZE);

			previous = current;
			current	 = current->next;
		}
	}
}
#endif

// Valid cases:		   5 .------------.
//			     |   7 .------V
//   2    3		     |	   |      |
//   .--. .---.            4 .-. 6 .---.  V                8 .---. 9 .----.
//   |  | |   |              | |   |   |                     |   |   |    |
//   |  V |   V--------------+ V   |   V  +------------------+   V   |    V
//   |    |   |              |     |      |                  |       |
//   X    X   |              X     X      |                  |       X
//            +--------------+            +------------------+
//                 head                       last block
static error_t
list_add(allocator_node_t **head, allocator_node_t *node, size_t size)
{
	error_t ret = ERROR_ALLOCATOR_RANGE_OVERLAPPING;

	if (*head == NULL) {
		// 1. Add head to empty list
		*head	      = node;
		(*head)->size = size;
		(*head)->next = NULL;
	} else if (((uint64_t)node + size) < (uint64_t)(*head)) {
		// 2. Prepend to head if address range is before head
		node->next = *head;
		node->size = size;
		*head	   = node;
	} else if (((uint64_t)node + size) == (uint64_t)(*head)) {
		// 3. Merge with head
		node->size = size + (*head)->size;
		node->next = (*head)->next;
		*head	   = node;
	} else {
		allocator_node_t *previous = *head;
		allocator_node_t *current  = (*head)->next;

		while ((current != NULL) &&
		       ((uint64_t)node >= (uint64_t)(current))) {
			previous = current;
			current	 = current->next;
		}

		if (current != NULL) {
			if ((((uint64_t)previous + previous->size) ==
			     (uint64_t)node)) {
				if ((node + size) < current) {
					// 4. Merge with previous
					previous->size += size;
				} else if (((uint64_t)node + size) ==
					   (uint64_t)current) {
					// 5. Merge with previous & current
					previous->size += size + current->size;
					previous->next = current->next;
				} else {
					goto out;
				}
			} else if (((uint64_t)previous + previous->size) <
				   (uint64_t)node) {
				if (((uint64_t)node + size) <
				    (uint64_t)current) {
					// 6. Add between previous & current
					node->next     = current;
					node->size     = size;
					previous->next = node;
				} else if ((uint64_t)(node + size) ==
					   (uint64_t)current) {
					// 7. Merge with current
					node->size     = size + current->size;
					node->next     = current->next;
					previous->next = node;
				} else {
					goto out;
				}
			} else {
				goto out;
			}
		} else {
			if (((uint64_t)previous + previous->size) ==
			    (uint64_t)node) {
				// 8. Merge with previous
				previous->size += size;
			} else if (((uint64_t)previous + previous->size) <
				   (uint64_t)node) {
				// 9. Append node to list
				node->next     = NULL;
				node->size     = size;
				previous->next = node;
			} else {
				goto out;
			}
		}
	}

	ret = OK;

out:
#if defined(DEBUG_PRINT)
	if (ret != OK) {
		printf("ERROR: failed due to addresses overlapping\n");
	}
#endif

	return ret;
}

static error_t NOINLINE
allocator_heap_add_memory(allocator_t *allocator, uintptr_t addr, size_t size)
{
	allocator_node_t *block;
	error_t		  ret = OK;

	assert(addr != 0U);

	// Check input arguments
	if (!util_is_baligned(addr, NODE_HEADER_SIZE)) {
		uintptr_t new_addr = util_balign_up(addr, NODE_HEADER_SIZE);
		size -= (new_addr - addr);
		addr = new_addr;
	}
	if (!util_is_baligned(size, NODE_HEADER_SIZE)) {
		size = util_balign_down(size, NODE_HEADER_SIZE);
	}
	if (util_add_overflows(addr, size)) {
		ret = ERROR_ADDR_OVERFLOW;
	} else if (size < (2UL * NODE_HEADER_SIZE)) {
		ret = ERROR_ARGUMENT_SIZE;
	} else {
		// FIXME: Check if added memory is in kernel address space

		block = (allocator_node_t *)addr;

		// Add memory to the freelist
		spinlock_acquire(&allocator->lock);

		ret = list_add(&allocator->heap, block, size);
		if (ret == OK) {
			allocator->total_size += size;
		}

		spinlock_release(&allocator->lock);
	}

	return ret;
}

error_t
allocator_list_handle_allocator_add_ram_range(partition_t *owner,
					      paddr_t	   phys_base,
					      uintptr_t virt_base, size_t size)
{
	assert(owner != NULL);

	(void)phys_base;

	return allocator_heap_add_memory(&owner->allocator, virt_base, size);
}

// Cases:
//      1 .-----------------------.
//        |                       |
//        |                       V
//      3 |-----. 4 .----.  2 .---.
//        |     |   |    |    |   |
//	  |	V   |	 V    |	  V
//        X         X         X
//        +-----------------------+
//        |         current       |	X = aligned_alloc_start
//        |          node         |	V = aligned_alloc_end
//        +-----------------------+
//        ^                       ^
//    node_start              node_end
static void_ptr_result_t
allocate_from_node(allocator_node_t **head, allocator_node_t **previous,
		   allocator_node_t **current, size_t alloc_size,
		   size_t alloc_alignment)
{
	void_ptr_result_t ret;

	assert(*current != NULL);
	assert(util_is_p2(alloc_alignment));
	assert(alloc_size >= NODE_HEADER_SIZE);
	assert((alloc_size % NODE_HEADER_SIZE) == 0UL);

	uint64_t node_start = (uint64_t)*current;
	uint64_t node_end   = (uint64_t)*current + (*current)->size;
#if defined(OVERFLOW_DEBUG)
	node_start += OVERFLOW_REDZONE_SIZE;
#endif
	uint64_t aligned_alloc_start =
		util_balign_up(node_start, alloc_alignment);
#if defined(OVERFLOW_DEBUG)
	node_start -= OVERFLOW_REDZONE_SIZE;
	aligned_alloc_start -= OVERFLOW_REDZONE_SIZE;
#endif

	uint64_t aligned_alloc_end = aligned_alloc_start + alloc_size;

	if (util_add_overflows(aligned_alloc_start, alloc_size)) {
		ret = void_ptr_result_error(ERROR_ADDR_OVERFLOW);
	} else if ((aligned_alloc_end > node_end) ||
		   (aligned_alloc_start > node_end)) {
		ret = void_ptr_result_error(ERROR_NOMEM);
	} else {
		if (node_end == aligned_alloc_end) {
			if (node_start == aligned_alloc_start) {
				// 1. Allocate from entire node. Remove it from
				// list
				if (*previous != NULL) {
					(*previous)->next = (*current)->next;
				} else {
					*head = (*current)->next;
				}
			} else {
				// 2. Allocate from end of node
				(*current)->size -= alloc_size;
			}
		} else {
			if (node_start == aligned_alloc_start) {
				// 3. Allocate from start of node. Change start
				// addr
				allocator_node_t *next =
					(allocator_node_t *)aligned_alloc_end;

				next->next = (*current)->next;
				next->size = (*current)->size - alloc_size;

				if (*previous != NULL) {
					(*previous)->next = next;
				} else {
					*head = next;
				}
			} else {
				// 4. Allocate from middle of node. Create new
				// node after allocated section
				allocator_node_t *next =
					(allocator_node_t *)aligned_alloc_end;

				next->next	 = (*current)->next;
				next->size	 = node_end - aligned_alloc_end;
				(*current)->next = next;
				(*current)->size =
					aligned_alloc_start - node_start;
			}
		}

		ret = void_ptr_result_ok((void *)aligned_alloc_start);
	}

	return ret;
}

static void_ptr_result_t
allocate_block(allocator_node_t **head, size_t size, size_t alignment)
{
	void_ptr_result_t ret;

	assert(*head != NULL);
	assert((size % NODE_HEADER_SIZE) == 0UL);
	assert(size > 0UL);
	assert(util_is_p2(alignment));
	assert(alignment >= (size_t)sizeof(size_t));

	allocator_node_t *previous = NULL;
	allocator_node_t *current  = *head;

#if defined(DEBUG_PRINT)
	printf("%s: head %p: size %zu (0x%lx), alignment %zu\n", __func__,
	       *head, size, size, alignment);
#endif

	while (current != NULL) {
		void_ptr_result_t result = allocate_from_node(
			head, &previous, &current, size, alignment);

		if (result.e == OK) {
#if defined(DEBUG_PRINT)
			printf("  -- allocated %p\n", result);
#endif
			ret = result;
			goto out;
		}

		previous = current;
		current	 = current->next;
	}

#if defined(DEBUG_PRINT)
	printf("  -- out of memory\n");
#endif
	ret = void_ptr_result_error(ERROR_NOMEM);

out:
	return ret;
}

void_ptr_result_t
allocator_allocate_object(allocator_t *allocator, size_t size,
			  size_t min_alignment)
{
	void_ptr_result_t ret;

	size_t alignment = util_max(min_alignment, alignof(size_t));

	spinlock_acquire(&allocator->lock);

	if (allocator->heap == NULL) {
		ret = void_ptr_result_error(ERROR_NOMEM);
		goto error;
	}

	assert(size > 0UL);
	assert(alignment > 0UL);
	assert((alignment & (alignment - 1UL)) == 0UL);
	assert((alignment >= (size_t)sizeof(size_t)));

#if defined(DEBUG_PRINT)
	printf("%s:\nheap %p: size %zu, alignment %zu\n", __func__,
	       allocator->heap, size, alignment);
#endif
	if ((size > MAX_ALLOC_SIZE) || (alignment > MAX_ALIGNMENT_SIZE)) {
		ret = void_ptr_result_error(ERROR_ARGUMENT_INVALID);
		goto error;
	}

	size = util_balign_up(size, HEAP_MIN_ALLOC);

	if (alignment < HEAP_MIN_ALIGN) {
		alignment = HEAP_MIN_ALIGN;
	}

#if defined(DEBUG_PRINT)
	printf("After alignment. size: %zu alignment: %zu\n", size, alignment);
#endif

#if defined(OVERFLOW_DEBUG)
	size += 2UL * OVERFLOW_REDZONE_SIZE;
#endif

	CHECK_HEAP(allocator->heap);
	ret = allocate_block(&allocator->heap, size, alignment);
	CHECK_HEAP(allocator->heap);

	if (ret.e != OK) {
		goto error;
	}

	allocator->alloc_size += size;

#if defined(ALLOCATOR_DEBUG)
	char  *data  = (char *)ret.r;
	size_t start = 0UL;
	size_t end   = size;

#if defined(OVERFLOW_DEBUG)
	end = OVERFLOW_REDZONE_SIZE;
	memset(&data[start], 0xe7, end - start);
	start = end;
	end   = size - OVERFLOW_REDZONE_SIZE;
#endif
	memset(&data[start], 0xa5, end - start);
#if defined(OVERFLOW_DEBUG)
	start = end;
	end   = size;
	memset(&data[start], 0xe8, end - start);

	// Return address after the overflow check values
	ret.r = (void *)&data[OVERFLOW_REDZONE_SIZE];
#endif
#endif

error:
	spinlock_release(&allocator->lock);
	return ret;
}

// We will probably not be using list_remove() and heap_remove memory()
// functions since we will only have the possibility of adding memory to the
// heap. We will maybe remove when deleting a partition.
static void
list_remove(allocator_node_t **head, allocator_node_t *remove,
	    allocator_node_t *previous)
{
	if (previous == NULL) {
		*head = remove->next;
	} else {
		previous->next = remove->next;
	}
}

// TODO: Exported only for test code currently
error_t
allocator_heap_remove_memory(allocator_t *allocator, void *obj, size_t size);

// Returns -1 if addresses are still being used and therefore cannot be freed.
error_t NOINLINE
allocator_heap_remove_memory(allocator_t *allocator, void *obj, size_t size)
{
	error_t ret = ERROR_ALLOCATOR_MEM_INUSE;

	assert(obj != NULL);
	assert(allocator->heap != NULL);

	allocator_node_t *previous = NULL;
	allocator_node_t *current  = allocator->heap;
	uint64_t	  object_location;
	uint64_t	  current_location;
	uint64_t	  previous_location;
	uint64_t	  aligned_alloc_end;

	spinlock_acquire(&allocator->lock);

	size = util_balign_up(size, HEAP_MIN_ALLOC);

	while (((uint64_t)obj > (uint64_t)current) && (current != NULL)) {
		assert((uint64_t)previous < (uint64_t)obj);
		previous = current;
		current	 = current->next;
	}

	object_location	  = (uint64_t)obj;
	current_location  = (uint64_t)current;
	previous_location = (uint64_t)previous;

	assert(!util_add_overflows(object_location, size));
	aligned_alloc_end = object_location + size;

	assert((object_location <= current_location) ||
	       (current_location == 0UL));
	assert(object_location > previous_location);

	if (current_location == object_location) {
		if ((current_location + current->size) < aligned_alloc_end) {
			goto out;
		} else if ((current_location + current->size) ==
			   aligned_alloc_end) {
			list_remove(&allocator->heap, current, previous);
		} else {
			// Divide current into 2 nodes and remove first one
			allocator_node_t *new;

			new	      = (allocator_node_t *)aligned_alloc_end;
			new->next     = current->next;
			new->size     = current->size - size;
			current->next = new;
			current->size = size;
			list_remove(&allocator->heap, current, previous);
		}
	} else if (previous != NULL) {
		if ((previous_location + previous->size) < aligned_alloc_end) {
			goto out;
		}

		if ((previous_location + previous->size) == aligned_alloc_end) {
			// Reduce size of previous
			previous->size -= size;
		} else {
			// Divide previous into 3 nodes & remove middle one
			allocator_node_t *new;

			new	  = (allocator_node_t *)aligned_alloc_end;
			new->next = current;
			new->size = previous_location + previous->size -
				    aligned_alloc_end;

			previous->next = new;
			previous->size = object_location - previous_location;
		}
	} else {
		goto out;
	}

	ret = OK;
	allocator->total_size -= size;

out:
	spinlock_release(&allocator->lock);
	return ret;
}

static void
deallocate_block(allocator_node_t **head, void *object, size_t size)
{
	assert(object != NULL);
	assert(size >= NODE_HEADER_SIZE);
	assert((size % NODE_HEADER_SIZE) == 0UL);

	allocator_node_t *previous   = NULL;
	allocator_node_t *next	     = NULL;
	allocator_node_t *freed_node = NULL;

	uint64_t object_location;
	uint64_t next_location;
	uint64_t previous_location;

	if (*head == NULL) {
		freed_node	 = object;
		freed_node->size = size;
		freed_node->next = NULL;
		*head		 = freed_node;
		goto out;
	}

	previous = *head;
	next	 = (*head)->next;

	while (((uint64_t)object > (uint64_t)next) && (next != NULL)) {
		assert((uint64_t)previous < (uint64_t)next);
		previous = next;
		next	 = next->next;
	}

	object_location = (uint64_t)object;
	next_location	= (uint64_t)next;
	if (previous != NULL) {
		previous_location = (uint64_t)previous;
	} else {
		previous_location = (uint64_t)~0UL;
	}

	assert((object_location <= next_location) || (next_location == 0UL));

	if ((previous != NULL) &&
	    (previous_location + previous->size) == object_location) {
		// Combine the free memory into the previous node.
		assert(!util_add_overflows(previous->size, size));
		previous->size += size;
		freed_node = previous;

		// If necessary, connect the freed node with the next one.
		if ((next != NULL) && (((uint64_t)freed_node +
					freed_node->size) == next_location)) {
			freed_node->size += next->size;
			freed_node->next = next->next;
		}
	} else if ((previous == NULL) ||
		   (object_location < previous_location)) {
		// Create node as head
		freed_node	 = object;
		freed_node->size = size;
		freed_node->next = previous;
		*head		 = freed_node;

		// If necessary, connect the freed node with the next one.
		if ((previous != NULL) &&
		    (((uint64_t)freed_node + freed_node->size) ==
		     previous_location)) {
			freed_node->size += previous->size;
			freed_node->next = previous->next;
		}
	} else {
		assert(previous != NULL);

		// Create a new header in the object.
		freed_node	 = object;
		freed_node->size = size;
		freed_node->next = next;
		previous->next	 = freed_node;

		// If necessary, connect the freed node with the next one.
		if ((next != NULL) && (((uint64_t)freed_node +
					freed_node->size) == next_location)) {
			freed_node->size += next->size;
			freed_node->next = next->next;
		}
	}

out:
	return;
}

error_t
allocator_deallocate_object(allocator_t *allocator, void *object, size_t size)
{
	assert(object != NULL);
	assert(size > 0UL);

	spinlock_acquire(&allocator->lock);

#if defined(DEBUG_PRINT)
	if (allocator->heap != NULL) {
		printf("%s: heap %p: size %zu / --> %p\n", __func__,
		       allocator->heap, size, object);
	}
#endif

	size = util_balign_up(size, HEAP_MIN_ALLOC);

#if defined(OVERFLOW_DEBUG)
	// Increment size +(2*NODE_HEADER_SIZE) to also free the overflow check
	// values in NODE_HEADER_SIZE addresses before and after the object
	// And go to the address where the overflow check values start
	size += 2UL * OVERFLOW_REDZONE_SIZE;
	object = (void *)((uintptr_t)object - OVERFLOW_REDZONE_SIZE);
#endif

#if defined(ALLOCATOR_DEBUG)
	memset(object, 0xe3, size);
#endif

	CHECK_HEAP(allocator->heap);
	deallocate_block(&allocator->heap, object, size);
	CHECK_HEAP(allocator->heap);

	allocator->alloc_size -= size;

	spinlock_release(&allocator->lock);

	return OK;
}

error_t
allocator_init(allocator_t *allocator)
{
	assert(allocator->heap == NULL);

	allocator->total_size = 0UL;
	allocator->alloc_size = 0UL;

	spinlock_init(&allocator->lock);
	return OK;
}

```

`hyp/mem/hyp_aspace/armv8/hyp_aspace.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

module hyp_aspace

subscribe partition_add_ram_range(phys_base, size)
	unwinder(phys_base, size)

subscribe partition_remove_ram_range(phys_base, size)
	unwinder(phys_base, size)

subscribe vectors_trap_data_abort_el2

```

`hyp/mem/hyp_aspace/armv8/src/hyp_aspace.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier; BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>
#include <string.h>

#include <hypregisters.h>

#include <atomic.h>
#include <bitmap.h>
#include <compiler.h>
#include <hyp_aspace.h>
#include <log.h>
#include <object.h>
#include <panic.h>
#include <partition.h>
#include <pgtable.h>
#include <platform_mem.h>
#include <prng.h>
#include <spinlock.h>
#include <trace.h>
#include <util.h>

#include <asm/barrier.h>
#include <asm/cache.h>
#include <asm/cpu.h>

#include "event_handlers.h"

#define HYP_ASPACE_ALLOCATE_BITS (25U)
#define HYP_ASPACE_ALLOCATE_SIZE (size_t) util_bit(HYP_ASPACE_ALLOCATE_BITS)

#define BITS_2MiB 21
#define SIZE_2MiB (size_t) util_bit(BITS_2MiB)

static spinlock_t      hyp_aspace_direct_lock;
static const uintptr_t hyp_aspace_direct_end =
	util_bit(HYP_ASPACE_MAP_DIRECT_BITS) - 1U;

static spinlock_t	   hyp_aspace_alloc_lock;
static _Atomic register_t *hyp_aspace_regions;
#if defined(ARCH_ARM_FEAT_VHE)
static const uintptr_t hyp_aspace_alloc_base =
	0U - util_bit(HYP_ASPACE_HIGH_BITS);
static const uintptr_t hyp_aspace_alloc_end = ~(uintptr_t)0U;
#else
// The upper half of the address space (256GB) is reserved for the randomised
// constant-offset mappings. The lower half is shared between the direct maps
// and the VA allocator (64GB and 192GB respectively).
static const uintptr_t hyp_aspace_alloc_base =
	util_bit(HYP_ASPACE_MAP_DIRECT_BITS);
static const uintptr_t hyp_aspace_alloc_end =
	util_bit(HYP_ASPACE_LOWER_HALF_BITS) - 1;
#endif
static const size_t hyp_aspace_total_size =
	hyp_aspace_alloc_end - hyp_aspace_alloc_base + 1U;
static const size_t hyp_aspace_num_regions =
	hyp_aspace_total_size / HYP_ASPACE_ALLOCATE_SIZE;

extern const char image_virt_start;
extern const char image_virt_last;
extern const char image_phys_start;
extern const char image_phys_last;

static const uintptr_t virt_start     = (uintptr_t)&image_virt_start;
static const uintptr_t virt_end	      = (uintptr_t)&image_virt_last;
static const paddr_t   hyp_phys_start = (paddr_t)&image_phys_start;
static const paddr_t   hyp_phys_last  = (paddr_t)&image_phys_last;

static uintptr_t hyp_aspace_physaccess_offset;

void
hyp_aspace_handle_boot_cold_init(void)
{
	spinlock_init(&hyp_aspace_direct_lock);
	spinlock_init(&hyp_aspace_alloc_lock);

	partition_t *hyp_partition = partition_get_private();

	// First, map the kernel image, assuming that all of the initial page
	// tables are within its physical memory. This should be sufficient to
	// allow partition_phys_access_begin to work, so we can do other page
	// table operations with the private partition.

#if ARCH_AARCH64_USE_PAN
	// Congruent (constant offset) mappings to support physical address
	// access (partition_phys_*).
	// Access rights are set to PGTABLE_ACCESS_NONE, which creates mappings
	// that can only be accessed with PSTATE.PAN cleared.
	hyp_aspace_physaccess_offset = HYP_ASPACE_PHYSACCESS_OFFSET;
	pgtable_access_t access	     = PGTABLE_ACCESS_NONE;
#else
	// The upper half of the address space (256GB to 512MB) is reserved for
	// the randomised constant-offset mappings.

	// Generate a random number in the rage of 1/4th of the address space
	// (between 0 and 128GB) with the lower 21 bits (2MB) cleared. Then add
	// it to the  base of the physaccess offset which is at half-point of
	// the address space (256GB). This will give us a random physaccess
	// offset between half and 3/4th of the address space (256GB-384GB).

	uint64_result_t prng_ret = prng_get64();
	if (prng_ret.e != OK) {
		panic("Failed to get a random number");
	}

	// Clear the lower 21 bits (2MB) of the random number
	hyp_aspace_physaccess_offset = prng_ret.r & ~((1UL << 21) - 1);
	hyp_aspace_physaccess_offset &= HYP_ASPACE_PHYSACCESS_OFFSET_RND_MAX;
	hyp_aspace_physaccess_offset += HYP_ASPACE_PHYSACCESS_OFFSET_BASE;
	pgtable_access_t access = PGTABLE_ACCESS_RW;
#endif

	size_t phys_size = (size_t)(hyp_phys_last - hyp_phys_start + 1U);

	pgtable_hyp_start();
	error_t err = pgtable_hyp_map(
		hyp_partition,
		(uintptr_t)hyp_phys_start + hyp_aspace_physaccess_offset,
		phys_size, hyp_phys_start, PGTABLE_HYP_MEMTYPE_WRITEBACK,
		access, VMSA_SHAREABILITY_INNER_SHAREABLE);
	assert(err == OK);
	pgtable_hyp_commit();

	// Allocate the bitmap used for region allocations.
	size_t bitmap_size =
		BITMAP_NUM_WORDS(hyp_aspace_num_regions) * sizeof(register_t);
	void_ptr_result_t alloc_ret = partition_alloc(
		hyp_partition, bitmap_size, alignof(register_t));
	if (alloc_ret.e != OK) {
		panic("Unable to alloc aspace regions");
	}

	hyp_aspace_regions = alloc_ret.r;

	(void)memset_s(hyp_aspace_regions, bitmap_size, 0, bitmap_size);

	assert((virt_start >= hyp_aspace_alloc_base) &&
	       (virt_end <= hyp_aspace_alloc_end));

	// Reserve the already mapped hypervisor memory in the bitmap.
	index_t start_bit = (index_t)((virt_start - hyp_aspace_alloc_base) >>
				      HYP_ASPACE_ALLOCATE_BITS);
	index_t end_bit	  = (index_t)((virt_end - hyp_aspace_alloc_base) >>
				      HYP_ASPACE_ALLOCATE_BITS);

	for (index_t i = start_bit; i <= end_bit; i++) {
		bitmap_atomic_set(hyp_aspace_regions, i, memory_order_relaxed);
	}

	// map any remaining memory past the first 2MB of RW data which was
	// mapped by the assembly boot code.
	if ((size_t)PLATFORM_HEAP_PRIVATE_SIZE > SIZE_2MiB) {
		size_t remaining_size =
			(size_t)PLATFORM_HEAP_PRIVATE_SIZE - SIZE_2MiB;
		uintptr_t remaining_virt =
			(virt_end + 1U) -
			((size_t)PLATFORM_RW_DATA_SIZE - 0x200000U);
		paddr_t remaining_phys =
			(hyp_phys_last + 1U) -
			((size_t)PLATFORM_RW_DATA_SIZE - 0x200000U);

		pgtable_hyp_start();
		err = pgtable_hyp_map(hyp_partition, remaining_virt,
				      remaining_size, remaining_phys,
				      PGTABLE_HYP_MEMTYPE_WRITEBACK,
				      PGTABLE_ACCESS_RW,
				      VMSA_SHAREABILITY_INNER_SHAREABLE);
		assert(err == OK);
		pgtable_hyp_commit();
	}

	// Reserve page table levels to map the direct mapped area.
	err = pgtable_hyp_preallocate(hyp_partition, 0U,
				      util_bit(HYP_ASPACE_MAP_DIRECT_BITS));
	assert(err == OK);
}

error_t
hyp_aspace_handle_partition_add_ram_range(paddr_t phys_base, size_t size)
{
	partition_t *hyp_partition = partition_get_private();

	assert(util_is_baligned(phys_base, PGTABLE_HYP_PAGE_SIZE));
	assert(util_is_baligned(size, PGTABLE_HYP_PAGE_SIZE));

	if (util_add_overflows(phys_base, size - 1U) ||
	    ((phys_base + size - 1U) >= util_bit(HYP_ASPACE_MAP_DIRECT_BITS))) {
		LOG(ERROR, WARN, "Failed to add high memory: {:x}..{:x}\n",
		    phys_base, phys_base + size - 1U);
		return ERROR_ADDR_INVALID;
	}

	spinlock_acquire(&hyp_aspace_direct_lock);
	pgtable_hyp_start();
#if ARCH_AARCH64_USE_PAN
	pgtable_access_t access = PGTABLE_ACCESS_NONE;
#else
	pgtable_access_t access = PGTABLE_ACCESS_RW;
#endif
	uintptr_t virt = phys_base + hyp_aspace_physaccess_offset;
	error_t	  err =
		pgtable_hyp_remap_merge(hyp_partition, virt, size, phys_base,
					PGTABLE_HYP_MEMTYPE_WRITEBACK, access,
					VMSA_SHAREABILITY_INNER_SHAREABLE,
					util_bit(HYP_ASPACE_MAP_DIRECT_BITS));
	pgtable_hyp_commit();
	spinlock_release(&hyp_aspace_direct_lock);

	return err;
}

error_t
hyp_aspace_handle_partition_remove_ram_range(paddr_t phys_base, size_t size)
{
	partition_t *hyp_partition = partition_get_private();

	assert(util_is_baligned(phys_base, PGTABLE_HYP_PAGE_SIZE));
	assert(util_is_baligned(size, PGTABLE_HYP_PAGE_SIZE));

	// Remap the memory as DEVICE so that no speculative reads occur.
	spinlock_acquire(&hyp_aspace_direct_lock);
	pgtable_hyp_start();
	uintptr_t virt = phys_base + hyp_aspace_physaccess_offset;
	(void)pgtable_hyp_remap_merge(hyp_partition, virt, size, phys_base,
				      PGTABLE_HYP_MEMTYPE_DEVICE,
				      PGTABLE_ACCESS_RW,
				      VMSA_SHAREABILITY_INNER_SHAREABLE,
				      util_bit(HYP_ASPACE_MAP_DIRECT_BITS));
	pgtable_hyp_commit();
	spinlock_release(&hyp_aspace_direct_lock);

	// Clean the memory range being removed to ensure no future write-backs
	// occur. No need to remap since speculative reads after the cache
	// clean won't be written back.
	CACHE_CLEAN_INVALIDATE_RANGE((char *)virt, size);

	return OK;
}

void
hyp_aspace_unwind_partition_add_ram_range(paddr_t phys_base, size_t size)
{
	error_t ret =
		hyp_aspace_handle_partition_remove_ram_range(phys_base, size);
	assert(ret == OK);
}

void
hyp_aspace_unwind_partition_remove_ram_range(paddr_t phys_base, size_t size)
{
	error_t ret =
		hyp_aspace_handle_partition_add_ram_range(phys_base, size);
	assert(ret == OK);
}

static bool
reserve_range(index_t start_bit, index_t end_bit, index_t *fail_bit)
{
	bool success = true;

	assert(fail_bit != NULL);

	for (index_t i = start_bit; i <= end_bit; i++) {
		bool set = bitmap_atomic_test_and_set(hyp_aspace_regions, i,
						      memory_order_relaxed);
		if (set) {
			for (index_t j = start_bit; j < i; j++) {
				bitmap_atomic_clear(hyp_aspace_regions, j,
						    memory_order_relaxed);
			}
			success	  = false;
			*fail_bit = i;
			break;
		}
	}

	return success;
}

uintptr_t
hyp_aspace_get_physaccess_offset(void)
{
	return hyp_aspace_physaccess_offset;
}

uintptr_t
hyp_aspace_get_alloc_base(void)
{
	return hyp_aspace_alloc_base;
}

virt_range_result_t
hyp_aspace_allocate(size_t min_size)
{
	virt_range_result_t ret;

	size_t	size	 = util_p2align_up(min_size, HYP_ASPACE_ALLOCATE_BITS);
	count_t num_bits = (count_t)(size >> HYP_ASPACE_ALLOCATE_BITS);
	if ((num_bits == 0U) || (num_bits > hyp_aspace_num_regions)) {
		ret = virt_range_result_error(ERROR_ARGUMENT_SIZE);
		goto out;
	}

	// Use PRNG to get a random starting bit.
	uint64_result_t prng_ret = prng_get64();
	if (prng_ret.e != OK) {
		ret = virt_range_result_error(prng_ret.e);
		goto out;
	}

	index_t start_bit = (index_t)(prng_ret.r % hyp_aspace_num_regions);

	// Iterate over the allocation bitmap until we find a free range, or
	// we wrap around and reach the starting bit again.
	index_t bit	= start_bit;
	bool	wrapped = false, success = false;
	while (!wrapped || (bit < start_bit)) {
		index_t end_bit	 = bit + num_bits - 1U;
		index_t fail_bit = 0U;

		if (end_bit >= hyp_aspace_num_regions) {
			// Wrap to the start of the bitmap.
			wrapped = true;
			bit	= 0U;
			continue;
		}

		success = reserve_range(bit, end_bit, &fail_bit);
		if (success) {
			break;
		}

		// Retry after the bit that was already set.
		bit = fail_bit + 1U;
	}

	if (!success) {
		ret = virt_range_result_error(ERROR_NOMEM);
		goto out;
	}

	uintptr_t virt = hyp_aspace_alloc_base +
			 ((uintptr_t)bit << HYP_ASPACE_ALLOCATE_BITS);
	ret = virt_range_result_ok(
		(virt_range_t){ .base = virt, .size = size });

	partition_t *hyp_partition = partition_get_private();
	// Preallocate shared page table levels before mapping
	spinlock_acquire(&hyp_aspace_alloc_lock);
	for (size_t offset = 0U; offset < size;
	     offset += HYP_ASPACE_ALLOCATE_SIZE) {
		ret.e = pgtable_hyp_preallocate(hyp_partition, virt + offset,
						HYP_ASPACE_ALLOCATE_SIZE);
		if (ret.e != OK) {
			virt_range_t vr = { .base = (virt + offset),
					    .size = HYP_ASPACE_ALLOCATE_SIZE };
			spinlock_release(&hyp_aspace_alloc_lock);
			hyp_aspace_deallocate(hyp_partition, vr);
			goto out;
		}
	}
	spinlock_release(&hyp_aspace_alloc_lock);

out:
	return ret;
}

void
hyp_aspace_deallocate(partition_t *partition, virt_range_t virt_range)
{
	uintptr_t virt = virt_range.base;
	size_t	  size = virt_range.size;

	assert(!util_add_overflows(virt, size - 1U));
	assert((virt >= hyp_aspace_alloc_base) &&
	       ((virt + (size - 1U)) <= hyp_aspace_alloc_end));
	assert(util_is_p2aligned(virt, HYP_ASPACE_ALLOCATE_BITS));
	assert(util_is_p2aligned(size, HYP_ASPACE_ALLOCATE_BITS));

	index_t start_bit = (index_t)((virt - hyp_aspace_alloc_base) >>
				      HYP_ASPACE_ALLOCATE_BITS);
	assert(start_bit < hyp_aspace_num_regions);

	index_t end_bit =
		start_bit + (count_t)((size - 1U) >> HYP_ASPACE_ALLOCATE_BITS);
	assert(end_bit < hyp_aspace_num_regions);

	spinlock_acquire(&hyp_aspace_alloc_lock);
	// FIXME: Rather than unmap, this should check that no
	// page tables owned by the given partition remain.
	pgtable_hyp_start();
	pgtable_hyp_unmap(partition, virt, size, size);
	pgtable_hyp_unmap(partition_get_private(), virt, size,
			  PGTABLE_HYP_UNMAP_PRESERVE_NONE);
	pgtable_hyp_commit();
	spinlock_release(&hyp_aspace_alloc_lock);

	for (index_t i = start_bit; i <= end_bit; i++) {
		bool was_set = bitmap_atomic_test_and_clear(
			hyp_aspace_regions, i, memory_order_relaxed);
		assert(was_set);
	}
}

static error_t
hyp_aspace_check_region(uintptr_t virt, size_t size)
{
	error_t err;

	if (!util_is_baligned(virt, PGTABLE_HYP_PAGE_SIZE) ||
	    !util_is_baligned(size, PGTABLE_HYP_PAGE_SIZE)) {
		err = ERROR_ARGUMENT_ALIGNMENT;
	} else if (util_add_overflows(virt, size)) {
		err = ERROR_ARGUMENT_INVALID;
	} else if (virt + size - 1U > hyp_aspace_direct_end) {
		err = ERROR_ARGUMENT_INVALID;
	} else {
		err = OK;
	}

	return err;
}

error_t
hyp_aspace_map_direct(partition_t *partition, paddr_t phys, size_t size,
		      pgtable_access_t access, pgtable_hyp_memtype_t memtype,
		      vmsa_shareability_t share)
{
	error_t	  err;
	uintptr_t virt = (uintptr_t)phys;

	if ((paddr_t)virt != phys) {
		// Physical address truncated by cast to uintptr_t
		// (possible on 32-bit ARMv8 or ARMv7-VE)
		err = ERROR_ARGUMENT_INVALID;
		goto map_error;
	}

	err = hyp_aspace_check_region(virt, size);
	if (err != OK) {
		goto map_error;
	}

	spinlock_acquire(&hyp_aspace_direct_lock);
	pgtable_hyp_start();
	err = pgtable_hyp_map_merge(partition, virt, size, phys, memtype,
				    access, share,
				    util_bit(HYP_ASPACE_MAP_DIRECT_BITS));
	pgtable_hyp_commit();
	spinlock_release(&hyp_aspace_direct_lock);

map_error:
	return err;
}

error_t
hyp_aspace_unmap_direct(partition_t *partition, paddr_t phys, size_t size)
{
	uintptr_t virt = (uintptr_t)phys;
	error_t	  err;

	if ((paddr_t)virt != phys) {
		// Physical address truncated by cast to uintptr_t
		// (possible on 32-bit ARMv8 or ARMv7-VE)
		err = ERROR_ARGUMENT_INVALID;
		goto unmap_error;
	}

	err = hyp_aspace_check_region(virt, size);
	if (err != OK) {
		goto unmap_error;
	}

	spinlock_acquire(&hyp_aspace_direct_lock);
	pgtable_hyp_start();
	pgtable_hyp_unmap(partition, virt, size,
			  util_bit(HYP_ASPACE_MAP_DIRECT_BITS));
	pgtable_hyp_commit();
	spinlock_release(&hyp_aspace_direct_lock);

unmap_error:
	return err;
}

// Retry faults if they may have been caused by TLB conflicts or break before
// make during block splits or merges in the direct physical access region.
bool
hyp_aspace_handle_vectors_trap_data_abort_el2(ESR_EL2_t esr)
{
	bool			 handled = false;
	ESR_EL2_ISS_DATA_ABORT_t iss =
		ESR_EL2_ISS_DATA_ABORT_cast(ESR_EL2_get_ISS(&esr));
	iss_da_ia_fsc_t fsc = ESR_EL2_ISS_DATA_ABORT_get_DFSC(&iss);

	FAR_EL2_t far  = register_FAR_EL2_read_ordered(&asm_ordering);
	uintptr_t addr = FAR_EL2_get_VirtualAddress(&far);

#if (CPU_PGTABLE_BBM_LEVEL < 2) && !defined(PLATFORM_PGTABLE_AVOID_BBM)
	// If the FEAT_BBM level is 0, then block splits and merges will do
	// break before make, and we might get transient translation faults.
	// If the FEAT_BBM level is 1, then splits and merges will temporarily
	// set the nT bit in the block PTE while flushing the TLBs; the CPU is
	// allowed to treat this the same as an invalid entry.
	if ((fsc != ISS_DA_IA_FSC_TRANSLATION_1) &&
	    (fsc != ISS_DA_IA_FSC_TRANSLATION_2) &&
	    (fsc != ISS_DA_IA_FSC_TRANSLATION_3)) {
		goto out;
	}

	// Only handle faults that are in the direct access region
	if (addr > (hyp_aspace_physaccess_offset +
		    util_bit(HYP_ASPACE_MAP_DIRECT_BITS) - 1)) {
		goto out;
	}

	if (spinlock_trylock(&hyp_aspace_direct_lock)) {
		spinlock_release(&hyp_aspace_direct_lock);

		// There is no map in progress. Perform a lookup to see whether
		// the accessed address is now mapped.
		PAR_EL1_base_t saved_par =
			register_PAR_EL1_base_read_ordered(&asm_ordering);
		if (ESR_EL2_ISS_DATA_ABORT_get_WnR(&iss)) {
			__asm__ volatile("at	S1E2W, %[addr]		;"
					 "isb				;"
					 : "+m"(asm_ordering)
					 : [addr] "r"(addr));
		} else {
			__asm__ volatile("at	S1E2R, %[addr]		;"
					 "isb				;"
					 : "+m"(asm_ordering)
					 : [addr] "r"(addr));
		}
		PAR_EL1_base_t par_raw =
			register_PAR_EL1_base_read_ordered(&asm_ordering);
		register_PAR_EL1_base_write_ordered(saved_par, &asm_ordering);

		// If the accessed address is now mapped, we can just return
		// from the fault. Otherwise we can consider the fault to be
		// fatal, because there is no BBM operation still in progress.
		PAR_EL1_F0_t par = PAR_EL1_F0_cast(PAR_EL1_base_raw(par_raw));
		handled		 = !PAR_EL1_F0_get_F(&par);
	} else {
		// A map operation is in progress, so retry until it finishes.
		// Note that we might get stuck here if the page table is
		// corrupt!
		handled = true;
	}
#else
	// If the FEAT_BBM level is 2 we do block splits and merges without BBM
	// or the nT bit. So we might get TLB conflicts. If one occurs, we must
	// flush the TLB and retry. We don't need to broadcast the TLB flush,
	// because the operation causing the fault should do that.
	if (fsc == ISS_DA_IA_FSC_TLB_CONFLICT) {
		vmsa_tlbi_va_input_t input = vmsa_tlbi_va_input_default();
		vmsa_tlbi_va_input_set_VA(&input, addr);

		__asm__ volatile("tlbi VAE2, %[VA]; dsb nsh"
				 : "+m"(asm_ordering)
				 : [VA] "r"(vmsa_tlbi_va_input_raw(input)));

		handled = true;
		goto out;
	}
#endif

out:
	return handled;
}

lookup_result_t
hyp_aspace_is_mapped(uintptr_t virt, size_t size, pgtable_access_t access)
{
	int		map_count = 0;
	lookup_result_t ret	  = lookup_result_default();

	paddr_t		      phys, expected_phys;
	pgtable_hyp_memtype_t curr_memtype, prev_memtype;
	pgtable_access_t      curr_access, prev_access;

	bool mapped, have_access, first_lookup = true, consistent = true;
	bool direct = true, contiguous = true;

	if (access == PGTABLE_ACCESS_NONE) {
		goto is_mapped_return;
	}

	if (util_add_overflows(virt, size - 1U)) {
		goto is_mapped_return;
	}

	// Set dummy values to stop warnings
	expected_phys = 0;
	prev_memtype  = PGTABLE_HYP_MEMTYPE_WRITEBACK;
	prev_access   = PGTABLE_ACCESS_NONE;

	size_t offset = 0U;
	while (offset < size) {
		uintptr_t curr = virt + offset;
		size_t	  mapped_size;
		mapped = pgtable_hyp_lookup(curr, &phys, &mapped_size,
					    &curr_memtype, &curr_access);
		if (mapped) {
			size_t mapping_offset = curr & (mapped_size - 1U);
			phys += mapping_offset;
			mapped_size -= mapping_offset;

			if (!first_lookup) {
				consistent = consistent &&
					     (expected_phys == phys) &&
					     (prev_memtype == curr_memtype) &&
					     (prev_access == curr_access);
			} else {
				first_lookup = false;
			}

			have_access = pgtable_access_check(curr_access, access);
			direct	    = direct && (curr == phys);
			contiguous  = contiguous && have_access;

			if (have_access) {
				map_count++;
			}

			expected_phys = phys + mapped_size;
			prev_memtype  = curr_memtype;
			prev_access   = curr_access;
		} else {
			contiguous  = false;
			mapped_size = util_balign_up(curr + 1U,
						     PGTABLE_HYP_PAGE_SIZE) -
				      curr;
			expected_phys += mapped_size;
		}

		if (util_add_overflows(offset, mapped_size)) {
			break;
		}
		offset += mapped_size;
	}

	if (map_count > 0) {
		lookup_result_set_mapped(&ret, true);
		lookup_result_set_consistent(&ret, consistent);
		lookup_result_set_contiguous(&ret, contiguous);
		lookup_result_set_direct(&ret, direct);
	}

is_mapped_return:
	return ret;
}

error_t
hyp_aspace_va_to_pa_el2_read(void *addr, paddr_t *pa, MAIR_ATTR_t *memattr,
			     vmsa_shareability_t *shareability)
{
	bool success;

	PAR_EL1_base_t saved_par =
		register_PAR_EL1_base_read_ordered(&asm_ordering);

	__asm__ volatile("at	S1E2R, %[addr]		;"
			 "isb				;"
			 : "+m"(asm_ordering)
			 : [addr] "r"(addr));

	PAR_EL1_t par = {
		.base = register_PAR_EL1_base_read_ordered(&asm_ordering),
	};
	success = !PAR_EL1_base_get_F(&par.base);

	if (success) {
		if (pa != NULL) {
			*pa = PAR_EL1_F0_get_PA(&par.f0);
			*pa |= (paddr_t)addr & 0xfffU;
		}
		if (memattr != NULL) {
			*memattr = PAR_EL1_F0_get_ATTR(&par.f0);
		}
		if (shareability != NULL) {
			*shareability = PAR_EL1_F0_get_SH(&par.f0);
		}
	}

	register_PAR_EL1_base_write_ordered(saved_par, &asm_ordering);

	return success ? OK : ERROR_ADDR_INVALID;
}

error_t
hyp_aspace_va_to_pa_el2_write(void *addr, paddr_t *pa, MAIR_ATTR_t *memattr,
			      vmsa_shareability_t *shareability)
{
	bool success;

	PAR_EL1_base_t saved_par =
		register_PAR_EL1_base_read_ordered(&asm_ordering);

	__asm__ volatile("at	S1E2W, %[addr]		;"
			 "isb				;"
			 : "+m"(asm_ordering)
			 : [addr] "r"(addr));

	PAR_EL1_t par = {
		.base = register_PAR_EL1_base_read_ordered(&asm_ordering),
	};
	success = !PAR_EL1_base_get_F(&par.base);

	if (success) {
		if (pa != NULL) {
			*pa = PAR_EL1_F0_get_PA(&par.f0);
			*pa |= (paddr_t)addr & 0xfffU;
		}
		if (memattr != NULL) {
			*memattr = PAR_EL1_F0_get_ATTR(&par.f0);
		}
		if (shareability != NULL) {
			*shareability = PAR_EL1_F0_get_SH(&par.f0);
		}
	}

	register_PAR_EL1_base_write_ordered(saved_par, &asm_ordering);

	return success ? OK : ERROR_ADDR_INVALID;
}

```

`hyp/mem/hyp_aspace/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

interface hyp_aspace
types hyp_aspace.tc
events hyp_aspace.ev
arch_events armv8 hyp_aspace.ev
arch_source armv8 hyp_aspace.c

```

`hyp/mem/hyp_aspace/hyp_aspace.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

module hyp_aspace

subscribe boot_cold_init()
	// Must come after pagetable but before other events requiring the hyp
	// address space to be initialised.
	priority 20

```

`hyp/mem/hyp_aspace/hyp_aspace.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

define HYP_ASPACE_MAP_DIRECT_BITS constant type count_t =
	PLATFORM_PHYS_ADDRESS_BITS;

#if defined(ARCH_ARM_FEAT_VHE)
define HYP_ASPACE_LOW_BITS constant type count_t =
	HYP_ASPACE_MAP_DIRECT_BITS + 1;
define HYP_ASPACE_HIGH_BITS constant type count_t = 39;
define HYP_ASPACE_PHYSACCESS_OFFSET constant uintptr =
	(1 << HYP_ASPACE_MAP_DIRECT_BITS);
#else
define HYP_ASPACE_LOW_BITS constant type count_t = 39;
// Reserve the upper half of the address space for the randomised
// constant-offset mappings
define HYP_ASPACE_LOWER_HALF_BITS constant type count_t =
	HYP_ASPACE_LOW_BITS - 1;
define HYP_ASPACE_PHYSACCESS_OFFSET_BASE constant uintptr =
	(1 << (HYP_ASPACE_LOW_BITS - 1));
define HYP_ASPACE_PHYSACCESS_OFFSET_RND_MAX constant uintptr =
	(1 << (HYP_ASPACE_LOW_BITS - 2)) - 1;
#endif

define lookup_result bitfield<32> {
	auto	mapped bool;
	auto	consistent bool;
	auto	contiguous bool;
	auto	direct bool;
};

```

`hyp/mem/memdb/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

source memdb_tests.c
events memdb_tests.ev
types memdb_tests.tc

```

`hyp/mem/memdb/memdb_tests.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

module memdb

#if defined(UNIT_TESTS)
subscribe tests_init

subscribe tests_start
	priority last
	require_preempt_disabled
#endif

```

`hyp/mem/memdb/memdb_tests.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#if defined(UNIT_TESTS)

define MEMDB_RANGES_NUM public constant = 64;

define memdb_range structure {
	base	type paddr_t;
	size	size;
	obj	uintptr;
	type	enumeration memdb_type;
};

define memdb_data structure {
	ranges array(MEMDB_RANGES_NUM) structure memdb_range;
	ranges_count type count_t;
	ranges_index type index_t;
};

#endif

```

`hyp/mem/memdb/src/memdb_tests.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#if defined(UNIT_TESTS)

#include <assert.h>
#include <hyptypes.h>
#include <limits.h>
#include <string.h>

#include <compiler.h>
#include <cpulocal.h>
#include <log.h>
#include <memdb.h>
#include <panic.h>
#include <partition.h>
#include <partition_init.h>
#include <rcu.h>
#include <scheduler.h>
#include <spinlock.h>
#include <trace.h>
#include <util.h>

#include "event_handlers.h"

static count_t test_memdb_count;

static partition_t dummy_partition_1;
static partition_t dummy_partition_2;
static allocator_t dummy_allocator;

void
memdb_handle_tests_init(void)
{
	test_memdb_count = 0;
}

static error_t
memdb_test_add_free_range(paddr_t base, size_t size, void *arg)
{
	error_t	      ret	  = OK;
	memdb_data_t *memdb_data  = (memdb_data_t *)arg;
	bool	      first_entry = true;

	if ((size == 0U) && (util_add_overflows(base, size - 1))) {
		ret = ERROR_ARGUMENT_SIZE;
		goto error;
	}

	index_t index = memdb_data->ranges_count;

	if (index != 0U) {
		index--;
		first_entry = false;
	}

	if (!first_entry) {
		index++;
	}
	if (index >= MEMDB_RANGES_NUM) {
		LOG(ERROR, WARN, "memdb_data: no more free ranges");
	} else {
		memdb_data->ranges[index].base = base;
		memdb_data->ranges[index].size = size;
		memdb_data->ranges_count++;
		LOG(DEBUG, INFO, "range: [{:#x}..{:#x}]", base,
		    base + size - 1U);
	}

error:
	return ret;
}

static void
get_inserted_ranges(memdb_data_t *memdb_data, uintptr_t object,
		    memdb_type_t type)
{
	count_t count = memdb_data->ranges_count;

	if (memdb_walk(object, type, memdb_test_add_free_range,
		       (void *)memdb_data) != OK) {
		panic("Error doing the memory database walk");
	}

	for (index_t i = count; i < memdb_data->ranges_count; i++) {
		memdb_data->ranges[i].obj  = object;
		memdb_data->ranges[i].type = type;

		// Double check that the ranges are correct by checking if the
		// ranges are contiguous
		paddr_t start_addr = memdb_data->ranges[i].base;
		paddr_t end_addr   = memdb_data->ranges[i].base +
				   memdb_data->ranges[i].size - 1U;

		bool cont = memdb_is_ownership_contiguous(start_addr, end_addr,
							  object, type);
		if (!cont) {
			LOG(DEBUG, INFO,
			    "<<< BUG!! range {:#x}..{:#x} should be contiguouos",
			    start_addr, end_addr);
			assert(cont);
		}
	}
}

static void
check_ranges_in_memdb(memdb_data_t *memdb_data)
{
	memset(memdb_data, 0, sizeof(memdb_data_t));

	LOG(DEBUG, INFO, "----------------- RANGES IN MEMDB -----------------");
	LOG(DEBUG, INFO, "-- DUMMY PARTITION 1 --");
	get_inserted_ranges(memdb_data, (uintptr_t)&dummy_partition_1,
			    MEMDB_TYPE_PARTITION);

	LOG(DEBUG, INFO, "-- DUMMY PARTITION 2 --");
	get_inserted_ranges(memdb_data, (uintptr_t)&dummy_partition_2,
			    MEMDB_TYPE_PARTITION);

	LOG(DEBUG, INFO, "-- DUMMY ALLOCATOR --");
	get_inserted_ranges(memdb_data, (uintptr_t)&dummy_allocator,
			    MEMDB_TYPE_ALLOCATOR);
	LOG(DEBUG, INFO, "---------------------------------------------------");
}

static bool
is_range_in_memdb(memdb_data_t *memdb_data, paddr_t start_addr,
		  paddr_t end_addr)
{
	bool is_range_in_memdb = false;

	for (index_t i = 0; i < memdb_data->ranges_count; i++) {
		paddr_t start = memdb_data->ranges[i].base;
		paddr_t end   = memdb_data->ranges[i].base +
			      memdb_data->ranges[i].size - 1U;

		if (((start <= start_addr) && (end >= start_addr)) ||
		    ((start <= end_addr) && (end >= end_addr))) {
			LOG(DEBUG, INFO,
			    "Range {:#x}..{:#x} already used in {:#x}..{:#x}",
			    start_addr, end_addr, start, end);
			is_range_in_memdb = true;
			break;
		}
	}

	return is_range_in_memdb;
}

static void
memdb_test1(void)
{
	LOG(DEBUG, INFO, " Start TEST 1:");

	// Use addresses in: (0x300000000..0x5FFFFFFFFF)

	partition_t	       *hyp_partition = partition_get_private();
	paddr_t			start_addr    = 0U;
	paddr_t			end_addr      = 0U;
	uintptr_t		obj;
	memdb_type_t		type;
	uintptr_t		prev_obj;
	memdb_type_t		prev_type;
	error_t			err = OK;
	memdb_obj_type_result_t res;
	bool			cont;

	void_ptr_result_t alloc_ret;
	memdb_data_t	 *memdb_data;
	size_t		  memdb_data_size = sizeof(*memdb_data);

	alloc_ret = partition_alloc(hyp_partition, memdb_data_size,
				    alignof(*memdb_data));
	if (alloc_ret.e != OK) {
		panic("Allocate memdb_data_t failed");
	}

	memdb_data = (memdb_data_t *)alloc_ret.r;
	memset(memdb_data, 0, memdb_data_size);

	// In the tests we are going to use addresses within
	// (0x300000000..0x5FFFFFFFFF (one bit more)). We must make sure that
	// this range is not already in the memdb.

	start_addr = 0x3000000000;
	end_addr   = 0x5FFFFFFFFF;

	// Check which ranges are already occupied in the memdb so that I do not
	// use them for the tests
	check_ranges_in_memdb(memdb_data);

	bool is_range_used =
		is_range_in_memdb(memdb_data, start_addr, end_addr);
	assert(!is_range_used);

	rcu_read_start();
	res = memdb_lookup(start_addr);
	assert((res.e != OK) || (res.r.type == MEMDB_TYPE_NOTYPE));
	rcu_read_finish();

	start_addr = 0x3000000000;
	end_addr   = 0x300003FFFF;
	obj	   = (uintptr_t)&dummy_partition_2;
	type	   = MEMDB_TYPE_PARTITION;

	err = memdb_insert(hyp_partition, start_addr, end_addr, obj, type);
	assert(err == OK);

	// Check all ranges in memdb to see if everything has been done
	// correctly
	check_ranges_in_memdb(memdb_data);

	start_addr = 0x3000040000;
	end_addr   = 0x5FFFFFFFFFF;
	obj	   = (uintptr_t)&dummy_partition_1;
	type	   = MEMDB_TYPE_PARTITION;

	err = memdb_insert(hyp_partition, start_addr, end_addr, obj, type);
	assert(err == OK);

	// Lookup an address from &dummy_partition_1 that I know it is not
	// explicitly in an entry since it is in a skipped level due to a guard;
	paddr_t addr = 0x3000100000;
	rcu_read_start();
	res = memdb_lookup(addr);
	assert(res.e == OK);
	assert(res.r.object == obj);
	assert(res.r.type == type);
	rcu_read_finish();

	// Update ownership of range in skipped levels

	start_addr = 0x3000100000;
	end_addr   = 0x3000aFFFFF;
	obj	   = (uintptr_t)&dummy_allocator;
	type	   = MEMDB_TYPE_ALLOCATOR;
	prev_obj   = (uintptr_t)&dummy_partition_1;
	prev_type  = MEMDB_TYPE_PARTITION;

	err = memdb_update(hyp_partition, start_addr, end_addr, obj, type,
			   prev_obj, prev_type);
	assert(err == OK);

	start_addr = 0x3010000000;
	end_addr   = 0x33FFFFFFFF;
	obj	   = (uintptr_t)&dummy_allocator;
	type	   = MEMDB_TYPE_ALLOCATOR;
	prev_obj   = (uintptr_t)&dummy_partition_1;
	prev_type  = MEMDB_TYPE_PARTITION;

	err = memdb_update(hyp_partition, start_addr, end_addr, obj, type,
			   prev_obj, prev_type);
	assert(err == OK);

	// Check all ranges in memdb to see if everything has been done
	// correctly
	check_ranges_in_memdb(memdb_data);

	// Rollback ownership to root partition and see if the levels stay there
	// or the guard is set back and the levels are removed.

	start_addr = 0x3000100000;
	end_addr   = 0x3000aFFFFF;
	obj	   = (uintptr_t)&dummy_partition_1;
	type	   = MEMDB_TYPE_PARTITION;
	prev_obj   = (uintptr_t)&dummy_allocator;
	prev_type  = MEMDB_TYPE_ALLOCATOR;

	err = memdb_update(hyp_partition, start_addr, end_addr, obj, type,
			   prev_obj, prev_type);
	assert(err == OK);

	start_addr = 0x3010000000;
	end_addr   = 0x33FFFFFFFF;
	obj	   = (uintptr_t)&dummy_partition_1;
	type	   = MEMDB_TYPE_PARTITION;
	prev_obj   = (uintptr_t)&dummy_allocator;
	prev_type  = MEMDB_TYPE_ALLOCATOR;

	err = memdb_update(hyp_partition, start_addr, end_addr, obj, type,
			   prev_obj, prev_type);
	assert(err == OK);

	// Give all ownership to hyp partition of the ranges inserted in the
	// test
	start_addr = 0x3000040000;
	end_addr   = 0x5FFFFFFFFFF;
	obj	   = (uintptr_t)&dummy_partition_2;
	type	   = MEMDB_TYPE_PARTITION;
	prev_obj   = (uintptr_t)&dummy_partition_1;
	prev_type  = MEMDB_TYPE_PARTITION;

	err = memdb_update(hyp_partition, start_addr, end_addr, obj, type,
			   prev_obj, prev_type);
	assert(err == OK);

	// Check all ranges in memdb to see if everything has been done
	// correctly
	check_ranges_in_memdb(memdb_data);

	// Add a range that is not correct so that it needs to rollback all
	// previous entries.
	// Make it fail in the end address path
	start_addr = 0x3040000000;
	end_addr   = 0x6FFFFFFFFFF;
	obj	   = (uintptr_t)&dummy_allocator;
	type	   = MEMDB_TYPE_ALLOCATOR;
	prev_obj   = (uintptr_t)&dummy_partition_2;
	prev_type  = MEMDB_TYPE_PARTITION;

	err = memdb_update(hyp_partition, start_addr, end_addr, obj, type,
			   prev_obj, prev_type);
	assert(err == ERROR_MEMDB_NOT_OWNER);

	// Check that the rollback was done correctly
	start_addr = 0x3000000000;
	end_addr   = 0x5FFFFFFFFFF;

	cont = memdb_is_ownership_contiguous(start_addr, end_addr, prev_obj,
					     prev_type);
	assert(cont);

	// Check if rollback left everything correct
	check_ranges_in_memdb(memdb_data);

	// Change ownership of a range in the middle to then add a second range
	// that fails in the start path
	start_addr = 0x3040000000;
	end_addr   = 0x30FFFFFFFF;
	obj	   = (uintptr_t)&dummy_allocator;
	type	   = MEMDB_TYPE_ALLOCATOR;
	prev_obj   = (uintptr_t)&dummy_partition_2;
	prev_type  = MEMDB_TYPE_PARTITION;

	err = memdb_update(hyp_partition, start_addr, end_addr, obj, type,
			   prev_obj, prev_type);
	assert(err == OK);

	// Make it fail in the start path
	start_addr = 0x3000000000;
	end_addr   = 0x5FFFFFFFFFF;
	obj	   = (uintptr_t)&dummy_allocator;
	type	   = MEMDB_TYPE_ALLOCATOR;
	prev_obj   = (uintptr_t)&dummy_partition_2;
	prev_type  = MEMDB_TYPE_PARTITION;

	err = memdb_update(hyp_partition, start_addr, end_addr, obj, type,
			   prev_obj, prev_type);
	assert(err == ERROR_MEMDB_NOT_OWNER);

	start_addr = 0x3000000000;
	end_addr   = 0x303FFFFFFF;

	cont = memdb_is_ownership_contiguous(start_addr, end_addr, prev_obj,
					     prev_type);
	assert(cont);

	start_addr = 0x3040000000;
	end_addr   = 0x30FFFFFFFF;
	obj	   = (uintptr_t)&dummy_allocator;
	type	   = MEMDB_TYPE_ALLOCATOR;

	cont = memdb_is_ownership_contiguous(start_addr, end_addr, obj, type);
	assert(cont);
	partition_free(hyp_partition, memdb_data, memdb_data_size);
}

// This kind of tests do:
// - Success cases:
// 1. memdb_insert with range and object specified in the input arguments, but
// with type MEMDB_TYPE_TRACE as type. [needs to return OK]
// 2. memdb_update of same range to now have the type specified in input. [needs
// to return OK]
// 3. memdb_lookup and memdb_is_ownership_contiguous to check if the range has
// been added properly and every single entry has been updated with the correct
// object and type. [needs to return OK]
// - Failure cases:
// 4. memdb_insert the same range again [needs to return ERROR_MEMDB_NOT_OWNER]
// 5. memdb_update of same range of incorrect values for prev type [needs to
// return ERROR_MEMDB_NOT_OWNER]
// - Last success case:
// - memdb_is_ownership_contiguous to verify that the failure cases did not
// change the memdb
static void
memdb_test_insert_update(memdb_data_t *test_data, paddr_t start, paddr_t end)
{
	partition_t *hyp_partition = partition_get_private();

	void_ptr_result_t alloc_ret;
	memdb_data_t	 *memdb_data;
	size_t		  memdb_data_size = sizeof(*memdb_data);

	alloc_ret = partition_alloc(hyp_partition, memdb_data_size,
				    alignof(*memdb_data));
	if (alloc_ret.e != OK) {
		panic("Allocate memdb_data_t failed");
	}

	memdb_data = (memdb_data_t *)alloc_ret.r;
	memset(memdb_data, 0, memdb_data_size);

	assert(test_data->ranges_count != 0U);

	// Check which ranges are already occupied in the memdb so that I do not
	// use them for the tests
	check_ranges_in_memdb(memdb_data);

	bool is_range_used = is_range_in_memdb(memdb_data, start, end);
	assert(!is_range_used);

	rcu_read_start();
	memdb_obj_type_result_t res = memdb_lookup(start);
	assert((res.e != OK) || (res.r.type == MEMDB_TYPE_NOTYPE));
	rcu_read_finish();

	for (index_t i = 0; i < test_data->ranges_count; i++) {
		paddr_t start_addr = test_data->ranges[i].base;
		paddr_t end_addr   = test_data->ranges[i].base +
				   test_data->ranges[i].size - 1U;
		uintptr_t    obj  = test_data->ranges[i].obj;
		memdb_type_t type = test_data->ranges[i].type;

		// Do not use MEMDB_TYPE_EXTENT for the tests since we are going
		// to create a counter example with it.
		assert(type != MEMDB_TYPE_EXTENT);

		// Success cases:
		error_t err = memdb_insert(hyp_partition, start_addr, end_addr,
					   obj, MEMDB_TYPE_TRACE);
		if (err != OK) {
			LOG(DEBUG, INFO,
			    " memdb_insert ret:{%d}, should have returned: {:%d}",
			    (register_t)err, OK);
		}
		assert(err == OK);

		err = memdb_update(hyp_partition, start_addr, end_addr, obj,
				   type, obj, MEMDB_TYPE_TRACE);
		if (err != OK) {
			LOG(DEBUG, INFO,
			    " memdb_update ret:{%d}, should have returned: {:%d}",
			    (register_t)err, OK);
		}
		assert(err == OK);

		rcu_read_start();
		res = memdb_lookup(start_addr);
		assert(res.e == OK);
		assert(res.r.object == obj);
		assert(res.r.type == type);
		rcu_read_finish();

		bool cont = memdb_is_ownership_contiguous(start_addr, end_addr,
							  obj, type);
		assert(cont);

		// Failure cases:
		err = memdb_insert(hyp_partition, start_addr, end_addr, obj,
				   MEMDB_TYPE_TRACE);
		if (err != ERROR_MEMDB_NOT_OWNER) {
			LOG(DEBUG, INFO,
			    " memdb_insert ret:{%d}, should have returned: {:%d}",
			    (register_t)err, ERROR_MEMDB_NOT_OWNER);
		}
		assert(err == ERROR_MEMDB_NOT_OWNER);

		err = memdb_update(hyp_partition, start_addr, end_addr, obj,
				   type, (uintptr_t)NULL, MEMDB_TYPE_EXTENT);
		if (err != ERROR_MEMDB_NOT_OWNER) {
			LOG(DEBUG, INFO,
			    " memdb_update ret:{%d}, should have returned: {:%d}",
			    (register_t)err, ERROR_MEMDB_NOT_OWNER);
		}
		assert(err == ERROR_MEMDB_NOT_OWNER);

		// Verify that the failure cases did not modify the memdb
		cont = memdb_is_ownership_contiguous(start_addr, end_addr, obj,
						     type);
		assert(cont);
	}

	// Check all ranges in memdb to see if everything has been done
	// correctly
	check_ranges_in_memdb(memdb_data);
	partition_free(hyp_partition, memdb_data, memdb_data_size);
}

static void
memdb_test2(void)
{
	LOG(DEBUG, INFO, " Start TEST 2:");

	partition_t *hyp_partition = partition_get_private();

	void_ptr_result_t alloc_ret;
	memdb_data_t	 *test_data;
	size_t		  test_data_size = sizeof(*test_data);

	alloc_ret = partition_alloc(hyp_partition, test_data_size,
				    alignof(*test_data));
	if (alloc_ret.e != OK) {
		panic("Allocate memdb_data_t failed");
	}

	test_data = (memdb_data_t *)alloc_ret.r;
	memset(test_data, 0, test_data_size);

	test_data->ranges[0].base = 0x3000000;
	test_data->ranges[0].size = 0x0086000;
	test_data->ranges[0].obj  = (uintptr_t)&dummy_partition_2;
	test_data->ranges[0].type = MEMDB_TYPE_PARTITION;

	test_data->ranges[1].base = 0x5000000;
	test_data->ranges[1].size = 0x0080000;
	test_data->ranges[1].obj  = (uintptr_t)&dummy_partition_2;
	test_data->ranges[1].type = MEMDB_TYPE_PARTITION;

	test_data->ranges[2].base = 0x5100000;
	test_data->ranges[2].size = 0x0180000;
	test_data->ranges[2].obj  = (uintptr_t)&dummy_partition_2;
	test_data->ranges[2].type = MEMDB_TYPE_PARTITION;

	test_data->ranges_count = 3;

	memdb_test_insert_update(test_data, test_data->ranges[0].base,
				 test_data->ranges[2].base +
					 test_data->ranges[2].size - 1);
	partition_free(hyp_partition, test_data, test_data_size);
}

static void
memdb_test3(void)
{
	LOG(DEBUG, INFO, " Start TEST 3:");

	partition_t *hyp_partition = partition_get_private();

	void_ptr_result_t alloc_ret;
	memdb_data_t	 *test_data;
	size_t		  test_data_size = sizeof(*test_data);

	alloc_ret = partition_alloc(hyp_partition, test_data_size,
				    alignof(*test_data));
	if (alloc_ret.e != OK) {
		panic("Allocate memdb_data_t failed");
	}

	test_data = (memdb_data_t *)alloc_ret.r;
	memset(test_data, 0, test_data_size);

	test_data->ranges[0].base = 0xB00000000;
	test_data->ranges[0].size = 0x000860000;
	test_data->ranges[0].obj  = (uintptr_t)&dummy_partition_2;
	test_data->ranges[0].type = MEMDB_TYPE_PARTITION;

	test_data->ranges[1].base = 0xB08800000;
	test_data->ranges[1].size = 0x03F580000;
	test_data->ranges[1].obj  = (uintptr_t)&dummy_partition_2;
	test_data->ranges[1].type = MEMDB_TYPE_PARTITION;

	test_data->ranges[2].base = 0xC00DC0000;
	test_data->ranges[2].size = 0x000002000;
	test_data->ranges[2].obj  = (uintptr_t)&dummy_partition_2;
	test_data->ranges[2].type = MEMDB_TYPE_PARTITION;

	test_data->ranges[3].base = 0xC00C10000;
	test_data->ranges[3].size = 0x000002000;
	test_data->ranges[3].obj  = (uintptr_t)&dummy_partition_2;
	test_data->ranges[3].type = MEMDB_TYPE_PARTITION;

	test_data->ranges[4].base = 0xC18000000;
	test_data->ranges[4].size = 0x0BE800000;
	test_data->ranges[4].obj  = (uintptr_t)&dummy_partition_2;
	test_data->ranges[4].type = MEMDB_TYPE_PARTITION;

	test_data->ranges_count = 5;

	memdb_test_insert_update(test_data, test_data->ranges[0].base,
				 test_data->ranges[4].base +
					 test_data->ranges[4].size - 1);
	partition_free(hyp_partition, test_data, test_data_size);
}

static void
memdb_test4(void)
{
	LOG(DEBUG, INFO, " Start TEST 4:");

	partition_t *hyp_partition = partition_get_private();

	void_ptr_result_t alloc_ret;
	memdb_data_t	 *test_data;
	size_t		  test_data_size = sizeof(*test_data);

	alloc_ret = partition_alloc(hyp_partition, test_data_size,
				    alignof(*test_data));
	if (alloc_ret.e != OK) {
		panic("Allocate memdb_data_t failed");
	}

	test_data = (memdb_data_t *)alloc_ret.r;
	memset(test_data, 0, test_data_size);

	test_data->ranges[0].base = 0x80000000000;
	test_data->ranges[0].size = 0x00860000000;
	test_data->ranges[0].obj  = (uintptr_t)&dummy_partition_2;
	test_data->ranges[0].type = MEMDB_TYPE_PARTITION;

	test_data->ranges[1].base = 0x0C000000000000;
	test_data->ranges[1].size = 0x14000000000000;
	test_data->ranges[1].obj  = (uintptr_t)&dummy_partition_2;
	test_data->ranges[1].type = MEMDB_TYPE_PARTITION;

	test_data->ranges[2].base = 0x80DDC00000000;
	test_data->ranges[2].size = 0x0000200000000;
	test_data->ranges[2].obj  = (uintptr_t)&dummy_partition_2;
	test_data->ranges[2].type = MEMDB_TYPE_PARTITION;

	test_data->ranges[3].base = 0x80DC000000000;
	test_data->ranges[3].size = 0x0000300000000;
	test_data->ranges[3].obj  = (uintptr_t)&dummy_partition_2;
	test_data->ranges[3].type = MEMDB_TYPE_PARTITION;

	test_data->ranges[4].base = 0x8088000000000;
	test_data->ranges[4].size = 0x0048000000000;
	test_data->ranges[4].obj  = (uintptr_t)&dummy_partition_2;
	test_data->ranges[4].type = MEMDB_TYPE_PARTITION;

	test_data->ranges[5].base = 0x8240000000000;
	test_data->ranges[5].size = 0x3D60000000000;
	test_data->ranges[5].obj  = (uintptr_t)&dummy_partition_2;
	test_data->ranges[5].type = MEMDB_TYPE_PARTITION;

	test_data->ranges_count = 6;

	memdb_test_insert_update(test_data, test_data->ranges[0].base,
				 test_data->ranges[1].base +
					 test_data->ranges[1].size - 1);
	partition_free(hyp_partition, test_data, test_data_size);
}

static void
memdb_test5(void)
{
	// Test adding ranges that could possible fit in a single entry.

	LOG(DEBUG, INFO, " Start TEST 5:");

	partition_t *hyp_partition = partition_get_private();

	void_ptr_result_t alloc_ret;
	memdb_data_t	 *test_data;
	size_t		  test_data_size = sizeof(*test_data);

	alloc_ret = partition_alloc(hyp_partition, test_data_size,
				    alignof(*test_data));
	if (alloc_ret.e != OK) {
		panic("Allocate memdb_data_t failed");
	}

	test_data = (memdb_data_t *)alloc_ret.r;
	memset(test_data, 0, test_data_size);

	test_data->ranges[0].base = 0x0;
	test_data->ranges[0].size = 0x1000;
	test_data->ranges[0].obj  = (uintptr_t)&dummy_partition_2;
	test_data->ranges[0].type = MEMDB_TYPE_PARTITION;

	test_data->ranges[1].base = 0x1000;
	test_data->ranges[1].size = 0x1000;
	test_data->ranges[1].obj  = (uintptr_t)&dummy_partition_2;
	test_data->ranges[1].type = MEMDB_TYPE_PARTITION;

	test_data->ranges[2].base = 0x20000;
	test_data->ranges[2].size = 0x10000;
	test_data->ranges[2].obj  = (uintptr_t)&dummy_partition_2;
	test_data->ranges[2].type = MEMDB_TYPE_PARTITION;

	test_data->ranges[3].base = 0x300000;
	test_data->ranges[3].size = 0x100000;
	test_data->ranges[3].obj  = (uintptr_t)&dummy_partition_2;
	test_data->ranges[3].type = MEMDB_TYPE_PARTITION;

	test_data->ranges[4].base = 0x17C2000;
	test_data->ranges[4].size = 0x1000;
	test_data->ranges[4].obj  = (uintptr_t)&dummy_partition_1;
	test_data->ranges[4].type = MEMDB_TYPE_PARTITION;

	test_data->ranges_count = 5;

	memdb_test_insert_update(test_data, test_data->ranges[0].base,
				 test_data->ranges[5].base +
					 test_data->ranges[5].size - 1);
	partition_free(hyp_partition, test_data, test_data_size);
}

// Insert a new range and update within these ranges
static void
memdb_test_update(memdb_data_t *test_data, paddr_t start, paddr_t end,
		  memdb_type_t initial_type, uintptr_t initial_obj)
{
	partition_t *hyp_partition = partition_get_private();

	void_ptr_result_t alloc_ret;
	memdb_data_t	 *memdb_data;
	size_t		  memdb_data_size = sizeof(*memdb_data);

	alloc_ret = partition_alloc(hyp_partition, memdb_data_size,
				    alignof(*memdb_data));
	if (alloc_ret.e != OK) {
		panic("Allocate memdb_data_t failed");
	}

	memdb_data = (memdb_data_t *)alloc_ret.r;
	memset(memdb_data, 0, memdb_data_size);

	// Check which ranges are already occupied in the memdb so that I do not
	// use them for the tests
	check_ranges_in_memdb(memdb_data);

	bool is_range_used = is_range_in_memdb(memdb_data, start, end);
	assert(!is_range_used);

	rcu_read_start();
	memdb_obj_type_result_t res = memdb_lookup(start);
	assert((res.e != OK) || (res.r.type == MEMDB_TYPE_NOTYPE));
	rcu_read_finish();

	LOG(DEBUG, INFO, "<<< Adding range: {:#x}-{:#x}", start, end);

	error_t err = memdb_insert(hyp_partition, start, end, initial_obj,
				   initial_type);
	assert(err == OK);

	assert(test_data->ranges_count != 0U);

	// Update ownership
	for (index_t i = 0; i < test_data->ranges_count; i++) {
		paddr_t start_addr = test_data->ranges[i].base;
		paddr_t end_addr   = test_data->ranges[i].base +
				   test_data->ranges[i].size - 1U;
		uintptr_t    obj  = test_data->ranges[i].obj;
		memdb_type_t type = test_data->ranges[i].type;

		err = memdb_update(hyp_partition, start_addr, end_addr, obj,
				   type, initial_obj, initial_type);
		assert(err == OK);
	}

	// Check it has been added correctly
	for (index_t i = 0; i < test_data->ranges_count; i++) {
		paddr_t start_addr = test_data->ranges[i].base;
		paddr_t end_addr   = test_data->ranges[i].base +
				   test_data->ranges[i].size - 1U;
		uintptr_t    obj  = test_data->ranges[i].obj;
		memdb_type_t type = test_data->ranges[i].type;

		rcu_read_start();
		res = memdb_lookup(start_addr);
		assert(res.e == OK);
		assert(res.r.object == obj);
		assert(res.r.type == type);
		rcu_read_finish();

		bool cont = memdb_is_ownership_contiguous(start_addr, end_addr,
							  obj, type);
		assert(cont);
	}

	// Check all ranges in memdb to see if everything has been done
	// correctly
	check_ranges_in_memdb(memdb_data);

	// Rollback ownership
	for (index_t i = 0; i < test_data->ranges_count; i++) {
		paddr_t start_addr = test_data->ranges[i].base;
		paddr_t end_addr   = test_data->ranges[i].base +
				   test_data->ranges[i].size - 1U;
		uintptr_t    obj  = test_data->ranges[i].obj;
		memdb_type_t type = test_data->ranges[i].type;

		err = memdb_update(hyp_partition, start_addr, end_addr,
				   initial_obj, initial_type, obj, type);
		assert(err == OK);
	}

	rcu_read_start();
	res = memdb_lookup(start);
	assert(res.e == OK);
	assert(res.r.object == initial_obj);
	assert(res.r.type == initial_type);
	rcu_read_finish();

	bool cont = memdb_is_ownership_contiguous(start, end, initial_obj,
						  initial_type);
	assert(cont);

	// Check all ranges in memdb to see if everything has been done
	// correctly
	check_ranges_in_memdb(memdb_data);
	partition_free(hyp_partition, memdb_data, memdb_data_size);
}

static void
memdb_test0(void)
{
	// Test inserting one range and then updated ownership of smaller ranges
	// within. When these smaller ranges update their ownership back to the
	// initial owner, the levels should collapse.

	LOG(DEBUG, INFO, " Start TEST 0:");

	partition_t *hyp_partition = partition_get_private();

	void_ptr_result_t alloc_ret;
	memdb_data_t	 *test_data;
	size_t		  test_data_size = sizeof(*test_data);

	alloc_ret = partition_alloc(hyp_partition, test_data_size,
				    alignof(*test_data));
	if (alloc_ret.e != OK) {
		panic("Allocate memdb_data_t failed");
	}

	test_data = (memdb_data_t *)alloc_ret.r;
	memset(test_data, 0, test_data_size);

	test_data->ranges[0].base = 0x410fc4000;
	test_data->ranges[0].size = 0x1000;
	test_data->ranges[0].obj  = (uintptr_t)&dummy_partition_1;
	test_data->ranges[0].type = MEMDB_TYPE_TRACE;

	test_data->ranges[1].base = 0x57FFFf000;
	test_data->ranges[1].size = 0x1000;
	test_data->ranges[1].obj  = (uintptr_t)&dummy_partition_1;
	test_data->ranges[1].type = MEMDB_TYPE_TRACE;

	test_data->ranges[2].base = 0x3D8100000;
	test_data->ranges[2].size = 0x38EC0000;
	test_data->ranges[2].obj  = (uintptr_t)&dummy_partition_1;
	test_data->ranges[2].type = MEMDB_TYPE_TRACE;

	test_data->ranges_count = 3;

	paddr_t start_addr = 0x3D5000000;
	paddr_t end_addr   = 0x57FFFFFFF;

	memdb_test_update(test_data, start_addr, end_addr, MEMDB_TYPE_PARTITION,
			  (uintptr_t)&dummy_partition_1);

	// Insert a range in same address to see if the common level is locked
	paddr_t start_addr2 = 0x580000000;
	paddr_t end_addr2   = 0x69FFFFFFF;

	error_t err = memdb_insert(hyp_partition, start_addr2, end_addr2,
				   (uintptr_t)&dummy_partition_1,
				   MEMDB_TYPE_PARTITION);
	assert(err == OK);

	rcu_read_start();
	memdb_obj_type_result_t res = memdb_lookup(start_addr);
	assert(res.e == OK);
	rcu_read_finish();

	bool cont = memdb_is_ownership_contiguous(start_addr, end_addr2,
						  (uintptr_t)&dummy_partition_1,
						  MEMDB_TYPE_PARTITION);
	assert(cont);

	paddr_t start_addr3 = 0x380000000;
	paddr_t end_addr3   = 0x3D4FFFFFF;

	err = memdb_insert(hyp_partition, start_addr3, end_addr3,
			   (uintptr_t)&dummy_partition_1, MEMDB_TYPE_PARTITION);
	assert(err == OK);

	rcu_read_start();
	res = memdb_lookup(start_addr3);
	assert(res.e == OK);
	rcu_read_finish();

	cont = memdb_is_ownership_contiguous(start_addr3, end_addr2,
					     (uintptr_t)&dummy_partition_1,
					     MEMDB_TYPE_PARTITION);
	assert(cont);
}

static void
memdb_test6(void)
{
	LOG(DEBUG, INFO, " Start TEST 6:");

	partition_t	       *hyp_partition = partition_get_private();
	paddr_t			start_addr    = 0U;
	paddr_t			end_addr      = 0U;
	uintptr_t		obj;
	memdb_type_t		type;
	uintptr_t		prev_obj;
	memdb_type_t		prev_type;
	error_t			err = OK;
	memdb_obj_type_result_t res;

	void_ptr_result_t alloc_ret;
	memdb_data_t	 *memdb_data;
	size_t		  memdb_data_size = sizeof(*memdb_data);

	alloc_ret = partition_alloc(hyp_partition, memdb_data_size,
				    alignof(*memdb_data));
	if (alloc_ret.e != OK) {
		panic("memdb_test: allocate memdb_data failed");
	}

	memdb_data = (memdb_data_t *)alloc_ret.r;
	memset(memdb_data, 0, memdb_data_size);

	uintptr_t fake_extent = 0xffffff88e1e1e1e1U;

	start_addr = 0x2000000000000;
	end_addr   = 0x2ffffffffffff;

	bool is_range_used =
		is_range_in_memdb(memdb_data, start_addr, end_addr);
	assert(!is_range_used);

	rcu_read_start();
	res = memdb_lookup(start_addr);
	assert((res.e != OK) || (res.r.type == MEMDB_TYPE_NOTYPE));
	rcu_read_finish();

	start_addr = 0x2000000000000;
	end_addr   = 0x201ffffffffff;
	obj	   = fake_extent;
	type	   = MEMDB_TYPE_EXTENT;

	err = memdb_insert(hyp_partition, start_addr, end_addr, obj, type);
	assert(err == OK);

	start_addr = 0x2080000000000;
	end_addr   = 0x2080fffffffff;
	obj	   = (uintptr_t)&dummy_partition_2;
	type	   = MEMDB_TYPE_PARTITION;

	err = memdb_insert(hyp_partition, start_addr, end_addr, obj, type);
	assert(err == OK);

	start_addr = 0x2090000000000;
	end_addr   = 0x213ffffffffff;
	obj	   = (uintptr_t)&dummy_partition_1;
	type	   = MEMDB_TYPE_PARTITION;

	err = memdb_insert(hyp_partition, start_addr, end_addr, obj, type);
	assert(err == OK);

	// Dump initial state
	check_ranges_in_memdb(memdb_data);
	memset(memdb_data, 0, sizeof(memdb_data_t));
	LOG(DEBUG, INFO, "----------------- RANGES IN MEMDB -----------------");
	LOG(DEBUG, INFO, "-- FAKE EXTENT --");
	get_inserted_ranges(memdb_data, fake_extent, MEMDB_TYPE_EXTENT);

	start_addr = 0x2100020000000;
	end_addr   = 0x2100020ffffff;
	obj	   = (uintptr_t)&dummy_partition_1;
	type	   = MEMDB_TYPE_TRACE;
	prev_obj   = (uintptr_t)&dummy_partition_1;
	prev_type  = MEMDB_TYPE_PARTITION;

	err = memdb_update(hyp_partition, start_addr, end_addr, obj, type,
			   prev_obj, prev_type);
	assert(err == OK);

	// Dump state
	check_ranges_in_memdb(memdb_data);
	memset(memdb_data, 0, sizeof(memdb_data_t));
	LOG(DEBUG, INFO, "----------------- RANGES IN MEMDB -----------------");
	LOG(DEBUG, INFO, "-- FAKE EXTENT --");
	get_inserted_ranges(memdb_data, fake_extent, MEMDB_TYPE_EXTENT);

	start_addr = 0x2100020000000;
	end_addr   = 0x2100020ffffff;
	obj	   = (uintptr_t)&dummy_partition_1;
	type	   = MEMDB_TYPE_PARTITION;
	prev_obj   = (uintptr_t)&dummy_partition_1;
	prev_type  = MEMDB_TYPE_TRACE;

	err = memdb_update(hyp_partition, start_addr, end_addr, obj, type,
			   prev_obj, prev_type);
	assert(err == OK);

	// Dump state
	check_ranges_in_memdb(memdb_data);
	memset(memdb_data, 0, sizeof(memdb_data_t));
	LOG(DEBUG, INFO, "----------------- RANGES IN MEMDB -----------------");
	LOG(DEBUG, INFO, "-- FAKE EXTENT --");
	get_inserted_ranges(memdb_data, fake_extent, MEMDB_TYPE_EXTENT);
	partition_free(hyp_partition, memdb_data, memdb_data_size);
}

static void
memdb_test7(void)
{
	LOG(DEBUG, INFO, " Start TEST 7:");

	partition_t	       *hyp_partition = partition_get_private();
	paddr_t			start_addr    = 0U;
	paddr_t			end_addr      = 0U;
	uintptr_t		obj;
	memdb_type_t		type;
	uintptr_t		prev_obj;
	memdb_type_t		prev_type;
	error_t			err = OK;
	memdb_obj_type_result_t res;

	void_ptr_result_t alloc_ret;
	memdb_data_t	 *memdb_data;
	size_t		  memdb_data_size = sizeof(*memdb_data);

	alloc_ret = partition_alloc(hyp_partition, memdb_data_size,
				    alignof(*memdb_data));
	if (alloc_ret.e != OK) {
		panic("memdb_test: allocate memdb_data failed");
	}

	memdb_data = (memdb_data_t *)alloc_ret.r;
	memset(memdb_data, 0, memdb_data_size);

	uintptr_t fake_extent = 0xffffff88e1e1e1e1U;

	start_addr = 0x3000000000000;
	end_addr   = 0x3ffffffffffff;

	bool is_range_used =
		is_range_in_memdb(memdb_data, start_addr, end_addr);
	assert(!is_range_used);

	rcu_read_start();
	res = memdb_lookup(start_addr);
	assert((res.e != OK) || (res.r.type == MEMDB_TYPE_NOTYPE));
	rcu_read_finish();

	start_addr = 0x3000000000000;
	end_addr   = 0x300001fffffff;
	obj	   = fake_extent;
	type	   = MEMDB_TYPE_EXTENT;

	err = memdb_insert(hyp_partition, start_addr, end_addr, obj, type);
	assert(err == OK);

	start_addr = 0x3000080000000;
	end_addr   = 0x3000080ffffff;
	obj	   = (uintptr_t)&dummy_partition_2;
	type	   = MEMDB_TYPE_PARTITION;

	err = memdb_insert(hyp_partition, start_addr, end_addr, obj, type);
	assert(err == OK);

	start_addr = 0x3000090000000;
	end_addr   = 0x300123fffffff;
	obj	   = (uintptr_t)&dummy_partition_1;
	type	   = MEMDB_TYPE_PARTITION;

	err = memdb_insert(hyp_partition, start_addr, end_addr, obj, type);
	assert(err == OK);

	// Dump initial state
	check_ranges_in_memdb(memdb_data);
	memset(memdb_data, 0, sizeof(memdb_data_t));
	LOG(DEBUG, INFO, "----------------- RANGES IN MEMDB -----------------");
	LOG(DEBUG, INFO, "-- FAKE EXTENT --");
	get_inserted_ranges(memdb_data, fake_extent, MEMDB_TYPE_EXTENT);

	start_addr = 0x3000100020000;
	end_addr   = 0x3000100020fff;
	obj	   = (uintptr_t)&dummy_partition_1;
	type	   = MEMDB_TYPE_TRACE;
	prev_obj   = (uintptr_t)&dummy_partition_1;
	prev_type  = MEMDB_TYPE_PARTITION;

	err = memdb_update(hyp_partition, start_addr, end_addr, obj, type,
			   prev_obj, prev_type);
	assert(err == OK);

	// Dump state
	check_ranges_in_memdb(memdb_data);
	memset(memdb_data, 0, sizeof(memdb_data_t));
	LOG(DEBUG, INFO, "----------------- RANGES IN MEMDB -----------------");
	LOG(DEBUG, INFO, "-- FAKE EXTENT --");
	get_inserted_ranges(memdb_data, fake_extent, MEMDB_TYPE_EXTENT);

	start_addr = 0x3000100020000;
	end_addr   = 0x3000100020fff;
	obj	   = (uintptr_t)&dummy_partition_1;
	type	   = MEMDB_TYPE_PARTITION;
	prev_obj   = (uintptr_t)&dummy_partition_1;
	prev_type  = MEMDB_TYPE_TRACE;

	err = memdb_update(hyp_partition, start_addr, end_addr, obj, type,
			   prev_obj, prev_type);
	assert(err == OK);

	// Dump state
	check_ranges_in_memdb(memdb_data);
	memset(memdb_data, 0, sizeof(memdb_data_t));
	LOG(DEBUG, INFO, "----------------- RANGES IN MEMDB -----------------");
	LOG(DEBUG, INFO, "-- FAKE EXTENT --");
	get_inserted_ranges(memdb_data, fake_extent, MEMDB_TYPE_EXTENT);
	partition_free(hyp_partition, memdb_data, memdb_data_size);
}

static void
memdb_test8(void)
{
	LOG(DEBUG, INFO, " Start TEST 8:");

	partition_t	       *hyp_partition = partition_get_private();
	paddr_t			start_addr    = 0U;
	paddr_t			end_addr      = 0U;
	uintptr_t		obj;
	memdb_type_t		type;
	uintptr_t		prev_obj;
	memdb_type_t		prev_type;
	error_t			err = OK;
	memdb_obj_type_result_t res;

	void_ptr_result_t alloc_ret;
	memdb_data_t	 *memdb_data;
	size_t		  memdb_data_size = sizeof(*memdb_data);

	alloc_ret = partition_alloc(hyp_partition, memdb_data_size,
				    alignof(*memdb_data));
	if (alloc_ret.e != OK) {
		panic("memdb_test: allocate memdb_data failed");
	}

	memdb_data = (memdb_data_t *)alloc_ret.r;
	memset(memdb_data, 0, memdb_data_size);

	uintptr_t fake_extent = 0xffffff88e1e1e1e1U;

	start_addr = 0x4000000000000;
	end_addr   = 0x4ffffffffffff;

	bool is_range_used =
		is_range_in_memdb(memdb_data, start_addr, end_addr);
	assert(!is_range_used);

	rcu_read_start();
	res = memdb_lookup(start_addr);
	assert((res.e != OK) || (res.r.type == MEMDB_TYPE_NOTYPE));
	rcu_read_finish();

	start_addr = 0x4000000000000;
	end_addr   = 0x400001fffffff;
	obj	   = fake_extent;
	type	   = MEMDB_TYPE_EXTENT;

	err = memdb_insert(hyp_partition, start_addr, end_addr, obj, type);
	assert(err == OK);

	start_addr = 0x4000080000000;
	end_addr   = 0x4000080ffffff;
	obj	   = (uintptr_t)&dummy_partition_2;
	type	   = MEMDB_TYPE_PARTITION;

	err = memdb_insert(hyp_partition, start_addr, end_addr, obj, type);
	assert(err == OK);

	// Dump state
	check_ranges_in_memdb(memdb_data);
	memset(memdb_data, 0, sizeof(memdb_data_t));
	LOG(DEBUG, INFO, "----------------- RANGES IN MEMDB -----------------");
	LOG(DEBUG, INFO, "-- FAKE EXTENT --");
	get_inserted_ranges(memdb_data, fake_extent, MEMDB_TYPE_EXTENT);

	start_addr = 0x4000a0000c000;
	end_addr   = 0x4000a0000cfff;
	obj	   = (uintptr_t)&dummy_partition_1;
	type	   = MEMDB_TYPE_PARTITION;

	err = memdb_insert(hyp_partition, start_addr, end_addr, obj, type);
	assert(err == OK);

	// Dump state
	check_ranges_in_memdb(memdb_data);
	memset(memdb_data, 0, sizeof(memdb_data_t));
	LOG(DEBUG, INFO, "----------------- RANGES IN MEMDB -----------------");
	LOG(DEBUG, INFO, "-- FAKE EXTENT --");
	get_inserted_ranges(memdb_data, fake_extent, MEMDB_TYPE_EXTENT);

	start_addr = 0x4000a0000d000;
	end_addr   = 0x4000cffffffff;
	obj	   = (uintptr_t)&dummy_partition_1;
	type	   = MEMDB_TYPE_PARTITION;

	err = memdb_insert(hyp_partition, start_addr, end_addr, obj, type);
	assert(err == OK);

	// Dump state
	check_ranges_in_memdb(memdb_data);
	memset(memdb_data, 0, sizeof(memdb_data_t));
	LOG(DEBUG, INFO, "----------------- RANGES IN MEMDB -----------------");
	LOG(DEBUG, INFO, "-- FAKE EXTENT --");
	get_inserted_ranges(memdb_data, fake_extent, MEMDB_TYPE_EXTENT);

	start_addr = 0x4000a00000000;
	end_addr   = 0x4000a0000bfff;
	obj	   = (uintptr_t)&dummy_partition_1;
	type	   = MEMDB_TYPE_PARTITION;

	err = memdb_insert(hyp_partition, start_addr, end_addr, obj, type);
	assert(err == OK);

	// Dump state
	check_ranges_in_memdb(memdb_data);
	memset(memdb_data, 0, sizeof(memdb_data_t));
	LOG(DEBUG, INFO, "----------------- RANGES IN MEMDB -----------------");
	LOG(DEBUG, INFO, "-- FAKE EXTENT --");
	get_inserted_ranges(memdb_data, fake_extent, MEMDB_TYPE_EXTENT);

	start_addr = 0x4000a00012000;
	end_addr   = 0x4000a00012fff;
	obj	   = (uintptr_t)&dummy_partition_1;
	type	   = MEMDB_TYPE_TRACE;
	prev_obj   = (uintptr_t)&dummy_partition_1;
	prev_type  = MEMDB_TYPE_PARTITION;

	err = memdb_update(hyp_partition, start_addr, end_addr, obj, type,
			   prev_obj, prev_type);
	assert(err == OK);

	// Dump state
	check_ranges_in_memdb(memdb_data);
	memset(memdb_data, 0, sizeof(memdb_data_t));
	LOG(DEBUG, INFO, "----------------- RANGES IN MEMDB -----------------");
	LOG(DEBUG, INFO, "-- FAKE EXTENT --");
	get_inserted_ranges(memdb_data, fake_extent, MEMDB_TYPE_EXTENT);

	start_addr = 0x4000a00013000;
	end_addr   = 0x4000a0e012fff;
	obj	   = (uintptr_t)&dummy_partition_1;
	type	   = MEMDB_TYPE_TRACE;
	prev_obj   = (uintptr_t)&dummy_partition_1;
	prev_type  = MEMDB_TYPE_PARTITION;

	err = memdb_update(hyp_partition, start_addr, end_addr, obj, type,
			   prev_obj, prev_type);
	assert(err == OK);

	// Dump state
	check_ranges_in_memdb(memdb_data);
	memset(memdb_data, 0, sizeof(memdb_data_t));
	LOG(DEBUG, INFO, "----------------- RANGES IN MEMDB -----------------");
	LOG(DEBUG, INFO, "-- FAKE EXTENT --");
	get_inserted_ranges(memdb_data, fake_extent, MEMDB_TYPE_EXTENT);

	start_addr = 0x4000a00012000;
	end_addr   = 0x4000a0e012fff;
	obj	   = (uintptr_t)&dummy_partition_1;
	type	   = MEMDB_TYPE_PARTITION;
	prev_obj   = (uintptr_t)&dummy_partition_1;
	prev_type  = MEMDB_TYPE_TRACE;

	err = memdb_update(hyp_partition, start_addr, end_addr, obj, type,
			   prev_obj, prev_type);
	assert(err == OK);

	// Dump state
	check_ranges_in_memdb(memdb_data);
	memset(memdb_data, 0, sizeof(memdb_data_t));
	LOG(DEBUG, INFO, "----------------- RANGES IN MEMDB -----------------");
	LOG(DEBUG, INFO, "-- FAKE EXTENT --");
	get_inserted_ranges(memdb_data, fake_extent, MEMDB_TYPE_EXTENT);
	partition_free(hyp_partition, memdb_data, memdb_data_size);
}

static void
memdb_test9(void)
{
	LOG(DEBUG, INFO, " Start TEST 9:");

	partition_t	       *hyp_partition = partition_get_private();
	paddr_t			start_addr    = 0U;
	paddr_t			end_addr      = 0U;
	uintptr_t		obj;
	memdb_type_t		type;
	error_t			err = OK;
	memdb_obj_type_result_t res;

	void_ptr_result_t alloc_ret;
	memdb_data_t	 *memdb_data;
	size_t		  memdb_data_size = sizeof(*memdb_data);

	alloc_ret = partition_alloc(hyp_partition, memdb_data_size,
				    alignof(*memdb_data));
	if (alloc_ret.e != OK) {
		panic("memdb_test: allocate memdb_data failed");
	}

	memdb_data = (memdb_data_t *)alloc_ret.r;
	memset(memdb_data, 0, memdb_data_size);

	// Check initial present ranges
	check_ranges_in_memdb(memdb_data);

	// Allocate dummy region we can use to replace the allocator's freelist
#if defined(MODULE_MEM_MEMDB_GPT)
	size_t dummy_size = sizeof(memdb_level_t) * 4;
#elif defined(MODULE_MEM_MEMDB_BITMAP)
	size_t dummy_size = sizeof(memdb_level_table_t) * 4;
#else
#error Determine free heap size to cause OOM during the below memdb_insert
#endif
	allocator_node_t *dummy_heap = NULL;
	alloc_ret = partition_alloc(hyp_partition, dummy_size,
				    alignof(allocator_node_t));
	if (alloc_ret.e != OK) {
		panic("memdb_test: allocate dummy region failed");
	}
	dummy_heap = alloc_ret.r;

	// Cause an out of memory error to see if the rollback is done correctly
	start_addr = 0x61234567890000;
	end_addr   = 0x62234567890fff;

	bool is_range_used =
		is_range_in_memdb(memdb_data, start_addr, end_addr);
	assert(!is_range_used);

	rcu_read_start();
	res = memdb_lookup(start_addr);
	assert((res.e != OK) || (res.r.type == MEMDB_TYPE_NOTYPE));
	rcu_read_finish();

	obj  = (uintptr_t)&dummy_partition_1;
	type = MEMDB_TYPE_PARTITION;

	// Make sure any outstanding RCU work has completed, so there won't
	// be any frees into the allocator while we have swapped its heap with
	// the dummy.
	rcu_sync();
	rcu_sync();

	// Swap the real heap with the dummy one
	spinlock_acquire(&hyp_partition->allocator.lock);
	allocator_node_t *saved_heap = hyp_partition->allocator.heap;
	*dummy_heap = (allocator_node_t){ .size = dummy_size, .next = NULL };
	hyp_partition->allocator.heap = dummy_heap;
	spinlock_release(&hyp_partition->allocator.lock);

	// Should run out of memory because this range needs to create several
	// levels (if they have not been created in previous tests) and there is
	// not much memory left in allocator
	err = memdb_insert(hyp_partition, start_addr, end_addr, obj, type);
	assert(err == ERROR_NOMEM);

	// Verify that the address does not exist in the memdb
	rcu_read_start();
	res = memdb_lookup(start_addr);
	assert((res.e != OK) || (res.r.type == MEMDB_TYPE_NOTYPE));
	rcu_read_finish();

	// Verify that all memory allocated during the attempted insert has
	// been freed by the time an RCU grace period has expired.
	rcu_sync();
	rcu_sync();
	spinlock_acquire(&hyp_partition->allocator.lock);
	assert(hyp_partition->allocator.heap == dummy_heap);
	assert(dummy_heap->size == dummy_size);

	// Swap the heap back and retry inserting the same range. This time it
	// should succeed.
	hyp_partition->allocator.heap = saved_heap;
	spinlock_release(&hyp_partition->allocator.lock);
	partition_free(hyp_partition, dummy_heap, dummy_size);

	err = memdb_insert(hyp_partition, start_addr, end_addr, obj, type);
	assert(err == OK);
}

static error_t
verify_range(paddr_t base, size_t size, void *arg)
{
	memdb_data_t *memdb_data = (memdb_data_t *)arg;
	index_t	      i		 = memdb_data->ranges_index;

	assert(i < memdb_data->ranges_count);
	assert(base == memdb_data->ranges[i].base);
	assert(size == memdb_data->ranges[i].size);

	memdb_data->ranges_index++;

	return OK;
}

static void
memdb_test10(void)
{
	LOG(DEBUG, INFO, " Start TEST 10:");

	error_t	     err;
	partition_t *hyp_partition  = partition_get_private();
	partition_t *fake_partition = (partition_t *)0x123124;

	void_ptr_result_t alloc_ret;
	memdb_data_t	 *memdb_data;
	size_t		  memdb_data_size = sizeof(*memdb_data);

	alloc_ret = partition_alloc(hyp_partition, memdb_data_size,
				    alignof(*memdb_data));
	if (alloc_ret.e != OK) {
		panic("memdb_test: allocate memdb_data failed");
	}

	memdb_data = (memdb_data_t *)alloc_ret.r;
	memset(memdb_data, 0, memdb_data_size);

	paddr_t base0 = 0x1082800000U;
	size_t	size0 = 0x55800000U;

	err = memdb_insert(hyp_partition, base0, base0 + size0 - 1U,
			   (uintptr_t)fake_partition, MEMDB_TYPE_PARTITION);
	assert(err == OK);

	paddr_t base1 = 0x10D8200000U;
	size_t	size1 = 0xE0000U;

	err = memdb_insert(hyp_partition, base1, base1 + size1 - 1U,
			   (uintptr_t)fake_partition, MEMDB_TYPE_PARTITION);
	assert(err == OK);

	memdb_data->ranges[0].base = base0;
	memdb_data->ranges[0].size = size0;
	memdb_data->ranges[1].base = base1;
	memdb_data->ranges[1].size = size1;
	memdb_data->ranges_count   = 2U;

	err = memdb_walk((uintptr_t)fake_partition, MEMDB_TYPE_PARTITION,
			 verify_range, memdb_data);
	assert(err == OK);

	assert(memdb_data->ranges_index == memdb_data->ranges_count);

	partition_free(hyp_partition, memdb_data, memdb_data_size);
}

static void
memdb_test11(void)
{
#if defined(MEMDB_BITMAP_OBJECTS)
	LOG(DEBUG, INFO, " Start TEST 11:");

	memdb_data_t test_data = { 0 };

	paddr_t start_addr = 0x5000a00000000;
	paddr_t end_addr   = 0x5000a00ffffff;

	static_assert(MEMDB_BITMAP_OBJECTS <= util_array_size(test_data.ranges),
		      "Test data is too small");
	for (index_t i = 0U; i < MEMDB_BITMAP_OBJECTS; i++) {
		test_data.ranges[i].base = start_addr + (0x2000 * i);
		test_data.ranges[i].size = 0x1000;
		test_data.ranges[i].obj =
			((uintptr_t)&dummy_partition_2) + (MEMDB_MIN_SIZE * i);
		test_data.ranges[i].type = MEMDB_TYPE_TRACE;
		test_data.ranges_count++;
	}

	memdb_test_update(&test_data, start_addr, end_addr,
			  MEMDB_TYPE_PARTITION, (uintptr_t)&dummy_partition_1);
#else // !defined(MEMDB_BITMAP_OBJECTS)
	LOG(DEBUG, INFO, " Skip TEST 11 (not using memdb_bitmap)");
#endif
}

bool
memdb_handle_tests_start(void)
{
	static _Atomic count_t core_start_count;
	static _Atomic bool    tests_done;
	cpu_index_t	       this_cpu = cpulocal_get_index();

	(void)atomic_fetch_add(&core_start_count, 1U);
	while (atomic_load(&core_start_count) < PLATFORM_MAX_CORES) {
		scheduler_yield();
	}

	if (this_cpu != 0U) {
		goto wait;
	}

	LOG(DEBUG, INFO, "Start memdb tests");

	// Test updates. Check that after update operations the levels collapse
	// properly when needed. If they don't then
	// memdb_is_ownership_contiguous will give the wrong result.
	memdb_test0();

	memdb_test1();

	// Test of adding/modifying guards.
	// Insert range that adds a guard and update the range ownership so that
	// we can verify that the range has been added correctly.
	memdb_test2();
	memdb_test3();
	// This test needs to create a new level (and update guard) due to a
	// mismatch in the guard when finding the common level. It should then
	// create a new level in the start path due to again a mismatch in
	// guard. This should all be successful if there is there are no bugs
	// left in the creation of intermediate level due to guards mismatch
	memdb_test4();
	memdb_test5();

	// Split/merge tests
	memdb_test6();
	memdb_test7();
	memdb_test8();

	// Test handling of out of memory error
	memdb_test9();

	// Test walk over two ranges with empty space from guard.
	memdb_test10();

	// Test conversion of bitmap levels to table levels for memdb_bitmap
	memdb_test11();

	LOG(DEBUG, INFO, "Memdb tests successfully finished ");
	atomic_store(&tests_done, true);

wait:
	(void)0;

	// Make all threads wait for test to end
	while (!atomic_load(&tests_done)) {
		scheduler_yield();
	}

	return false;
}
#else

extern char unused;

#endif

```

`hyp/mem/memdb/tests/test.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <hyptypes.h>
#include <limits.h>
#include <stdatomic.h>
#include <stddef.h>
#include <stdint.h>
#include <stdio.h>
#include <stdlib.h>

#include <allocator.h>
#include <memdb.h>

#define MEMDB_BITS_PER_ENTRY_MASK ((1UL << MEMDB_BITS_PER_ENTRY) - 1)
#define ADDR_SIZE		  (sizeof(paddr_t) * CHAR_BIT)
// levels + 1 for root
#define MAX_LEVELS (ADDR_SIZE / MEMDB_BITS_PER_ENTRY) + 1

partition_t partition;

void
print_level(memdb_level_t *level)
{
	if (level == NULL) {
		printf("Empty database.\n");
	}

	printf("Level lock: %p\n", &level->lock);

	for (int i = 0; i < MEMDB_NUM_ENTRIES; i++) {
		memdb_entry_t tmp_entry = atomic_load_explicit(
			&level->level[i], memory_order_relaxed);
		memdb_type_t tmp_type =
			memdb_entry_info_get_type(&tmp_entry.info);
		count_t tmp_shifts =
			memdb_entry_info_get_shifts(&tmp_entry.info);
		uint64_t tmp_guard =
			memdb_entry_info_get_guard(&tmp_entry.info);

		if (tmp_type != MEMDB_TYPE_NOTYPE) {
			// printf("| %d ", tmp_type);
			uint32_t tt = tmp_entry.info.bf[0] & 0x7;
			printf("| %d ", tt);
			if (tmp_shifts != ADDR_SIZE) {
				printf("guard_shifts: %d ", tmp_shifts);
				printf("guard: %#lx ", tmp_guard);
			}
		} else {
			printf("| - ");
		}
	}
	printf("|\n");

	for (int i = 0; i < MEMDB_NUM_ENTRIES; i++) {
		memdb_entry_t tmp_entry = atomic_load_explicit(
			&level->level[i], memory_order_relaxed);
		memdb_type_t tmp_type =
			memdb_entry_info_get_type(&tmp_entry.info);
		void *next = &tmp_entry.next;

		if (tmp_type != MEMDB_TYPE_NOTYPE) {
			if (tmp_type == MEMDB_TYPE_LEVEL) {
				printf("----- Level below index: %d -----\n",
				       i);
				print_level(next);
				printf("---------------------------------\n");
			}
		}
	}
}

void
print_memdb(void)
{
#if 0
	// To print the database we need to declare memdb_t memdb in memdb.h not
	// in mem_ownership_db.c
	memdb_entry_t tmp_root =
		atomic_load_explicit(&memdb.root, memory_order_relaxed);
	uintptr_t *  next	= &tmp_root.next;
	memdb_type_t tmp_type	= memdb_entry_info_get_type(&tmp_root.info);
	count_t     tmp_shifts = memdb_entry_info_get_shifts(&tmp_root.info);
	paddr_t	     tmp_guard	= memdb_entry_info_get_guard(&tmp_root.info);

	if (next == NULL) {
		printf("Empty memory database\n");
		return;
	} else {
		printf("------- Memory Ownership Database -------\n");
		printf("root guard: %#lx\n", tmp_guard);
		printf("root guard shifts: %d\n", tmp_shifts);
		printf("root type: %d\n", tmp_type);
		printf("root lock pointer: %p\n", &memdb.lock);
	}

	memdb_level_t *level = (memdb_level_t *)next;

	printf("\n");

	if (level == NULL) {
		printf("Empty database");
	} else {
		print_level(level);
	}

	printf("\n-----------------------------------------\n\n");
#endif
}

void
print_memdb_empty(void)
{
	//	print_memdb();
}

// Insert two ranges in database
int
test1()
{
	partition_t partition;
	int	    ret	      = 0;
	size_t	    alignment = sizeof(void *);
	size_t	    pool_size = 4096 * 100;

	partition.allocator.heap = NULL;

	/* ---------------- Giving memory to heap --------------------- */
	uintptr_t block = (uintptr_t)malloc(pool_size);

	ret = allocator_heap_add_memory(&partition.allocator, (void *)block,
					pool_size);
	if (!ret) {
		printf("Memory added to heap\n");
	}

	/* ---------------- Init memory database --------------------- */
	ret = memdb_init();

	if (!ret) {
		printf("Mem db init correct!\n");
	} else {
		printf("Error init!\n");
	}

	size_t	     obj_size = 1024;
	memdb_type_t type     = MEMDB_TYPE_PARTITION;

	size_t	     obj_size2 = 4096;
	memdb_type_t type2     = MEMDB_TYPE_ALLOCATOR;

	/* ---------- Allocate object. partition for example ---------------- */
	void_ptr_result_t res;
	res = allocator_allocate_object(&partition.allocator, obj_size,
					alignment);
	uintptr_t object = (uintptr_t)res.r;

	if (res.e != OK) {
		printf("Object allocation failed\n");
	} else {
		printf("Object allocation SUCCESS\n");
	}

	uint64_t start_addr = (uint64_t)object;
	uint64_t end_addr   = (uint64_t)object + (obj_size - 1);

	/* ---------- Allocate object. hypervisor for example ----------------
	 */
	res = allocator_allocate_object(&partition.allocator, obj_size2,
					alignment);
	uintptr_t object2 = (uintptr_t)res.r;
	if (res.e != OK) {
		printf("\nObject allocation failed\n");
	} else {
		printf("\nObject allocation SUCCESS\n");
	}

	uint64_t start_addr2 = (uint64_t)object2;
	uint64_t end_addr2   = (uint64_t)object2 + (obj_size2 - 1);

	/* ---------------- Insert object in database --------------------- */
	printf("\nstart_addr: %#lx, end_addr: %#lx\n", start_addr, end_addr);

	ret = memdb_insert(&partition, start_addr, end_addr, (uintptr_t)object,
			   type);
	if (!ret) {
		print_memdb_empty();
		printf("\nmemdb_insert SUCCESS\n\n");
	} else {
		print_memdb_empty();
		printf("\nmemdb_insert FAILED\n\n");
		goto error;
	}

	/* ---------------- Insert object in database --------------------- */
	printf("\nstart_addr2: %#lx, end_addr2: %#lx\n\n", start_addr2,
	       end_addr2);

	ret = memdb_insert(&partition, start_addr2, end_addr2,
			   (uintptr_t)object2, type2);
	if (!ret) {
		print_memdb_empty();
		printf("\nmemdb_insert SUCCESS\n\n");
	} else {
		print_memdb_empty();
		printf("\nmemdb_insert FAILED\n\n");
		goto error;
	}
error:
	return ret;
}

// Insert one range in database and do two updates.
int
test2()
{
	partition_t partition;
	int	    ret	      = 0;
	size_t	    alignment = sizeof(void *);

	memdb_type_t type  = MEMDB_TYPE_PARTITION;
	memdb_type_t type1 = MEMDB_TYPE_ALLOCATOR;
	memdb_type_t type2 = MEMDB_TYPE_EXTENT;

	size_t pool_size = 4096 * 100;
	size_t obj_size1 = 4096;
	size_t obj_size2 = 1024;

	partition.allocator.heap = NULL;

	/* ---------------- Giving memory to heap --------------------- */
	uintptr_t block = (uintptr_t)malloc(pool_size);

	ret = allocator_heap_add_memory(&partition.allocator, (void *)block,
					pool_size);
	if (!ret) {
		printf("Memory added to heap\n");
	}

	/* ---------- Allocate object. partition for example ---------------- */
	void_ptr_result_t res;
	res = allocator_allocate_object(&partition.allocator, obj_size1,
					alignment);
	uintptr_t object1 = (uintptr_t)res.r;
	if (res.e != OK) {
		printf("Object allocation failed\n");
	} else {
		printf("Object allocation SUCCESS\n");
	}

	uint64_t start_addr1 = (uint64_t)object1;
	uint64_t end_addr1   = (uint64_t)object1 + (obj_size1 - 1);

	/* ---------- Allocate object. hypervisor for example ----------------
	 */
	res = allocator_allocate_object(&partition.allocator, obj_size2,
					alignment);
	uintptr_t object2 = (uintptr_t)res.r;
	if (res.e != OK) {
		printf("\nObject allocation failed\n");
	} else {
		printf("\nObject allocation SUCCESS\n");
	}

	uint64_t start_addr2 = (uint64_t)object2;
	uint64_t end_addr2   = (uint64_t)object2 + (obj_size2 - 1);

	/* ---------------- Init memory database --------------------- */
	ret = memdb_init();

	if (!ret) {
		printf("Mem db init correct!\n");
	} else {
		printf("Error init!\n");
	}

	/* ---------------- Insert object in database --------------------- */
	uint64_t start_addr = (uint64_t)block;
	uint64_t end_addr   = (uint64_t)block + (pool_size - 1);

	printf("\nstart_addr: %#lx, end_addr: %#lx\n", start_addr, end_addr);

	ret = memdb_insert(&partition, start_addr, end_addr, (uintptr_t)block,
			   type);
	if (!ret) {
		print_memdb_empty();
		printf("\nmemdb_insert SUCCESS\n\n");
	} else {
		print_memdb_empty();
		printf("\nmemdb_insert FAILED\n\n");
		goto error;
	}

	/* ---------------- Update database --------------------- */

	printf("\nstart_addr1: %#lx, end_addr1: %#lx\n", start_addr1,
	       end_addr1);

	ret = memdb_update(&partition, start_addr1, end_addr1, object1, type1,
			   block, type);
	if (!ret) {
		print_memdb_empty();
		printf("\nmemdb_update SUCCESS\n\n");
	} else {
		print_memdb_empty();
		printf("\nmemdb_update FAILED\n\n");
		goto error;
	}

	/* ---------------- Update database --------------------- */

	printf("\nstart_addr2: %#lx, end_addr2: %#lx\n", start_addr2,
	       end_addr2);

	ret = memdb_update(&partition, start_addr2, end_addr2, object2, type2,
			   block, type);
	if (!ret) {
		print_memdb_empty();
		printf("\nmemdb_update SUCCESS\n\n");
	} else {
		print_memdb_empty();
		printf("\nmemdb_update FAILED\n\n");
		goto error;
	}
error:
	return ret;
}

// One insertion, two updates and update back to the initial state
int
test3()
{
	partition_t partition;
	int	    ret	      = 0;
	size_t	    alignment = sizeof(void *);

	memdb_type_t type  = MEMDB_TYPE_PARTITION;
	memdb_type_t type1 = MEMDB_TYPE_ALLOCATOR;
	memdb_type_t type2 = MEMDB_TYPE_EXTENT;

	size_t pool_size = 4096 * 100;
	size_t obj_size1 = 4096;
	size_t obj_size2 = 1024;

	partition.allocator.heap = NULL;

	/* ---------------- Giving memory to heap --------------------- */
	uintptr_t block = (uintptr_t)malloc(pool_size);

	ret = allocator_heap_add_memory(&partition.allocator, (void *)block,
					pool_size);
	if (!ret) {
		printf("Memory added to heap\n");
	}

	/* ---------- Allocate object. partition for example ---------------- */
	void_ptr_result_t res;
	res = allocator_allocate_object(&partition.allocator, obj_size1,
					alignment);
	uintptr_t object1 = (uintptr_t)res.r;

	if (res.e != OK) {
		printf("Object allocation failed\n");
		goto error;
	} else {
		printf("Object allocation SUCCESS\n");
	}

	uint64_t start_addr1 = (uint64_t)object1;
	uint64_t end_addr1   = (uint64_t)object1 + (obj_size1 - 1);

	/* ---------- Allocate object. hypervisor for example ----------------
	 */
	res = allocator_allocate_object(&partition.allocator, obj_size2,
					alignment);
	uintptr_t object2 = (uintptr_t)res.r;
	if (res.e != OK) {
		printf("\nObject allocation failed\n");
		goto error;
	} else {
		printf("\nObject allocation SUCCESS\n");
	}

	uint64_t start_addr2 = (uint64_t)object2;
	uint64_t end_addr2   = (uint64_t)object2 + (obj_size2 - 1);

	/* ---------------- Init memory database --------------------- */
	ret = memdb_init();

	if (!ret) {
		printf("Mem db init correct!\n");
	} else {
		printf("Error init!\n");
		goto error;
	}

	/* ---------------- Insert object in database --------------------- */
	uint64_t start_addr = (uint64_t)block;
	uint64_t end_addr   = (uint64_t)block + (pool_size - 1);

	printf("\nstart_addr: %#lx, end_addr: %#lx\n", start_addr, end_addr);

	ret = memdb_insert(&partition, start_addr, end_addr, (uintptr_t)block,
			   type);
	if (!ret) {
		print_memdb_empty();
		printf("\nmemdb_insert SUCCESS\n\n");
	} else {
		print_memdb_empty();
		printf("\nmemdb_insert FAILED\n\n");
		goto error;
	}

	/* ---------------- Update database --------------------- */

	printf("\nstart_addr1: %#lx, end_addr1: %#lx\n", start_addr1,
	       end_addr1);

	ret = memdb_update(&partition, start_addr1, end_addr1, object1, type1,
			   block, type);
	if (!ret) {
		print_memdb_empty();
		printf("\nmemdb_update SUCCESS\n\n");
	} else {
		print_memdb_empty();
		printf("\nmemdb_update FAILED\n\n");
		goto error;
	}

	/* ---------------- Update database --------------------- */

	printf("\nstart_addr2: %#lx, end_addr2: %#lx\n", start_addr2,
	       end_addr2);

	ret = memdb_update(&partition, start_addr2, end_addr2, object2, type2,
			   block, type);
	if (!ret) {
		print_memdb_empty();
		printf("\nmemdb_update SUCCESS\n\n");
	} else {
		print_memdb_empty();
		printf("\nmemdb_update FAILED\n\n");
		goto error;
	}

	/* ---------------- Update back database --------------------- */

	printf("\nstart_addr1: %#lx, end_addr1: %#lx\n", start_addr1,
	       end_addr1);

	ret = memdb_update(&partition, start_addr1, end_addr1, block, type,
			   object1, type1);
	if (!ret) {
		print_memdb_empty();
		printf("\nmemdb_update back SUCCESS\n\n");
	} else {
		print_memdb_empty();
		printf("\nmemdb_update back FAILED\n\n");
		goto error;
	}

	/* ---------------- Update back database --------------------- */

	printf("\nstart_addr2: %#lx, end_addr2: %#lx\n", start_addr2,
	       end_addr2);

	ret = memdb_update(&partition, start_addr2, end_addr2, block, type,
			   object2, type2);
	if (!ret) {
		print_memdb_empty();
		printf("\nmemdb_update back SUCCESS\n\n");
	} else {
		print_memdb_empty();
		printf("\nmemdb_update back FAILED\n\n");
		goto error;
	}
error:
	return ret;
}

// Two insertion, 2 updates, 2 updates back to state after insertions
int
test4()
{
	partition_t partition;
	int	    ret	      = 0;
	size_t	    alignment = 4096;

	memdb_type_t type  = MEMDB_TYPE_PARTITION;
	memdb_type_t type1 = MEMDB_TYPE_ALLOCATOR;
	memdb_type_t type2 = MEMDB_TYPE_EXTENT;

	size_t pool_size  = 4096 * 100;
	size_t pool_size2 = 1024;
	size_t obj_size1  = 4096;
	size_t obj_size2  = 1024;

	partition.allocator.heap = NULL;

	/* ---------------- Giving memory to heap --------------------- */
	uintptr_t block = (uintptr_t)aligned_alloc(alignment, pool_size);

	ret = allocator_heap_add_memory(&partition.allocator, (void *)block,
					pool_size);
	if (!ret) {
		printf("Memory added to heap\n");
	}

	/* ----------------- Hypervisor memory ------------------------ */
	uintptr_t block2 = (uintptr_t)aligned_alloc(alignment, pool_size2);

	/* ---------- Allocate object. partition for example ---------------- */
	void_ptr_result_t res;
	res = allocator_allocate_object(&partition.allocator, obj_size1,
					alignment);
	uintptr_t object1 = (uintptr_t)res.r;
	if (res.e != OK) {
		printf("Object allocation failed\n");
		goto error;
	} else {
		printf("Object allocation SUCCESS\n");
	}

	uint64_t start_addr1 = (uint64_t)object1;
	uint64_t end_addr1   = (uint64_t)object1 + (obj_size1 - 1);

	/* ---------- Allocate object. hypervisor for example ----------------
	 */
	res = allocator_allocate_object(&partition.allocator, obj_size2,
					alignment);

	uintptr_t object2 = (uintptr_t)res.r;
	if (res.e != OK) {
		printf("\nObject allocation failed\n");
		goto error;
	} else {
		printf("\nObject allocation SUCCESS\n");
	}

	uint64_t start_addr2 = (uint64_t)object2;
	uint64_t end_addr2   = (uint64_t)object2 + (obj_size2 - 1);

	/* ---------------- Init memory database --------------------- */
	ret = memdb_init();

	if (!ret) {
		printf("Mem db init correct!\n");
	} else {
		printf("Error init!\n");
		goto error;
	}

	/* ---------------- Insert object in database --------------------- */
	uint64_t start_addr = (uint64_t)block;
	uint64_t end_addr   = (uint64_t)block + (pool_size - 1);

	printf("\nstart_addr: %#lx, end_addr: %#lx\n", start_addr, end_addr);
	printf("\nnew type: %d\n", type);

	ret = memdb_insert(&partition, start_addr, end_addr, (uintptr_t)block,
			   type);
	if (!ret) {
		printf("\nmemdb_insert SUCCESS\n\n");
		print_memdb_empty();
	} else {
		printf("\nmemdb_insert FAILED\n\n");
		print_memdb_empty();
		goto error;
	}

	/* ---------------- Insert object in database --------------------- */
	uint64_t start_addrh = (uint64_t)block2;
	uint64_t end_addrh   = (uint64_t)block2 + (pool_size2 - 1);

	printf("\nstart_addr: %#lx, end_addr: %#lx\n", start_addrh, end_addrh);
	printf("\nnew type: %d\n", MEMDB_TYPE_ALLOCATOR);

	ret = memdb_insert(&partition, start_addrh, end_addrh,
			   (uintptr_t)block2, MEMDB_TYPE_ALLOCATOR);

	if (!ret) {
		printf("\nmemdb_insert SUCCESS\n\n");
		print_memdb_empty();
	} else {
		print_memdb_empty();
		printf("\nmemdb_insert FAILED\n\n");
		goto error;
	}

	/* ---------------- Update database --------------------- */

	printf("\nstart_addr1: %#lx, end_addr1: %#lx\n", start_addr1,
	       end_addr1);
	printf("\nnew type: %d old type: %d\n", type1, type);

	ret = memdb_update(&partition, start_addr1, end_addr1, object1, type1,
			   block, type);

	if (!ret) {
		printf("\nmemdb_update SUCCESS\n\n");
		print_memdb_empty();
	} else {
		print_memdb_empty();
		printf("\nmemdb_update FAILED\n\n");
		goto error;
	}

	/* ---------------- Update database --------------------- */
	printf("\nstart_addr2: %#lx, end_addr2: %#lx\n", start_addr2,
	       end_addr2);
	printf("\nnew type: %d old type: %d\n", type2, type);

	ret = memdb_update(&partition, start_addr2, end_addr2, object2, type2,
			   block, type);

	if (!ret) {
		print_memdb_empty();
		printf("\nmemdb_update SUCCESS\n\n");
	} else {
		print_memdb_empty();
		printf("\nmemdb_update FAILED\n\n");
		goto error;
	}

	/* ---------------- Update back database --------------------- */

	printf("\nstart_addr1: %#lx, end_addr1: %#lx\n", start_addr1,
	       end_addr1);
	printf("\nnew type: %d old type: %d\n", type, type1);

	ret = memdb_update(&partition, start_addr1, end_addr1, block, type,
			   object1, type1);
	if (!ret) {
		print_memdb_empty();
		printf("\nmemdb_update back SUCCESS\n\n");
	} else {
		print_memdb_empty();
		printf("\nmemdb_update back FAILED\n\n");
		goto error;
	}

	/* ---------------- Update back database --------------------- */

	printf("\nstart_addr2: %#lx, end_addr2: %#lx\n", start_addr2,
	       end_addr2);
	printf("\nnew type: %d old type: %d\n", type, type2);

	ret = memdb_update(&partition, start_addr2, end_addr2, block, type,
			   object2, type2);
	if (!ret) {
		print_memdb_empty();
		printf("\nmemdb_update back SUCCESS\n\n");
	} else {
		print_memdb_empty();
		printf("\nmemdb_update back FAILED\n\n");
		goto error;
	}
error:
	return ret;
}

// 1 insertion, 2 updates, 2 checks of contiguouness (1 should succeed and the
// other one fail)
int
test5()
{
	partition_t partition;
	int	    ret	      = 0;
	size_t	    alignment = sizeof(void *);

	memdb_type_t type  = MEMDB_TYPE_PARTITION;
	memdb_type_t type1 = MEMDB_TYPE_ALLOCATOR;
	memdb_type_t type2 = MEMDB_TYPE_EXTENT;

	size_t pool_size = 4096 * 100;
	size_t obj_size1 = 4096;
	size_t obj_size2 = 1024;

	partition.allocator.heap = NULL;

	/* ---------------- Giving memory to heap --------------------- */
	uintptr_t block = (uintptr_t)malloc(pool_size);

	ret = allocator_heap_add_memory(&partition.allocator, (void *)block,
					pool_size);
	if (!ret) {
		printf("Memory added to heap\n");
	}

	/* ---------- Allocate object. partition for example ---------------- */
	void_ptr_result_t res;
	res = allocator_allocate_object(&partition.allocator, obj_size1,
					alignment);

	uintptr_t object1 = (uintptr_t)res.r;
	if (res.e != OK) {
		printf("Object allocation failed\n");
	} else {
		printf("Object allocation SUCCESS\n");
	}

	uint64_t start_addr1 = (uint64_t)object1;
	uint64_t end_addr1   = (uint64_t)object1 + (obj_size1 - 1);

	/* ---------- Allocate object. hypervisor for example ----------------
	 */
	res = allocator_allocate_object(&partition.allocator, obj_size2,
					alignment);

	uintptr_t object2 = (uintptr_t)res.r;
	if (res.e != OK) {
		printf("\nObject allocation failed\n");
	} else {
		printf("\nObject allocation SUCCESS\n");
	}

	uint64_t start_addr2 = (uint64_t)object2;
	uint64_t end_addr2   = (uint64_t)object2 + (obj_size2 - 1);

	/* ---------------- Init memory database --------------------- */
	ret = memdb_init();

	if (!ret) {
		printf("Mem db init correct!\n");
	} else {
		printf("Error init!\n");
	}

	/* ---------------- Insert object in database --------------------- */
	uint64_t start_addr = (uint64_t)block;
	uint64_t end_addr   = (uint64_t)block + (pool_size - 1);

	printf("\nstart_addr: %#lx, end_addr: %#lx\n", start_addr, end_addr);

	ret = memdb_insert(&partition, start_addr, end_addr, (uintptr_t)block,
			   type);
	if (!ret) {
		print_memdb_empty();
		printf("\nmemdb_insert SUCCESS\n\n");
	} else {
		print_memdb_empty();
		printf("\nmemdb_insert FAILED\n\n");
		goto error;
	}

	/* ---------------- Update database --------------------- */

	printf("\nstart_addr1: %#lx, end_addr1: %#lx\n", start_addr1,
	       end_addr1);

	ret = memdb_update(&partition, start_addr1, end_addr1, object1, type1,
			   block, type);
	if (!ret) {
		print_memdb_empty();
		printf("\nmemdb_update SUCCESS\n\n");
	} else {
		print_memdb_empty();
		printf("\nmemdb_update FAILED\n\n");
		goto error;
	}

	/* ---------------- Update database --------------------- */

	printf("\nstart_addr2: %#lx, end_addr2: %#lx\n", start_addr2,
	       end_addr2);

	ret = memdb_update(&partition, start_addr2, end_addr2, object2, type2,
			   block, type);
	if (!ret) {
		print_memdb_empty();
		printf("\nmemdb_update SUCCESS\n\n");
	} else {
		print_memdb_empty();
		printf("\nmemdb_update FAILED\n\n");
		goto error;
	}

	/* ------------------- Is contiguous ? -------------------- */
	// Should succeed:
	printf("\nIs start_addr1: %#lx, end_addr1: %#lx contiguous??\n",
	       start_addr1, end_addr1);

	bool ans = memdb_is_ownership_contiguous(start_addr1, end_addr1,
						 object1, type1);
	if (ans) {
		printf("\nmemdb_is_ownership_contiguous SUCCESS\n\n");
	} else {
		printf("\nmemdb_is_ownership_contiguous FAILED\n\n");
		goto error;
	}

	/* ------------------- Is contiguous ? -------------------- */
	// Should fail:
	printf("\nIs start_addr1: %#lx, end_addr1: %#lx contiguous??\n",
	       start_addr1 - 1, end_addr1);

	ans = memdb_is_ownership_contiguous(start_addr1 - 1, end_addr1, object1,
					    type1);
	if (ans) {
		printf("\nmemdb_is_ownership_contiguous SUCCESS\n\n");
		ret = -1;
		goto error;
	} else {
		printf("\nmemdb_is_ownership_contiguous FAILED as expected.\n\n");
		ret = 0;
	}
error:
	return ret;
}

// 1 insertion, 2 updates and 2 lookups
int
test6()
{
	partition_t partition;
	int	    ret	      = 0;
	size_t	    alignment = 16;

	memdb_type_t type  = MEMDB_TYPE_PARTITION;
	memdb_type_t type1 = MEMDB_TYPE_ALLOCATOR;
	memdb_type_t type2 = MEMDB_TYPE_EXTENT;

	size_t pool_size = 4096 * 100;
	size_t obj_size1 = 4096;
	size_t obj_size2 = 1024;

	partition.allocator.heap = NULL;

	/* ---------------- Giving memory to heap --------------------- */
	uintptr_t block = (uintptr_t)malloc(pool_size);

	ret = allocator_heap_add_memory(&partition.allocator, (void *)block,
					pool_size);
	if (!ret) {
		printf("Memory added to heap\n");
	}

	/* ---------- Allocate object. partition for example ---------------- */
	void_ptr_result_t res;
	res = allocator_allocate_object(&partition.allocator, obj_size1,
					alignment);

	uintptr_t object1 = (uintptr_t)res.r;
	if (res.e != OK) {
		printf("Object allocation failed\n");
	} else {
		printf("Object allocation SUCCESS\n");
	}

	uint64_t start_addr1 = (uint64_t)object1;
	uint64_t end_addr1   = (uint64_t)object1 + (obj_size1 - 1);

	/* ---------- Allocate object. hypervisor for example ----------------
	 */
	res = allocator_allocate_object(&partition.allocator, obj_size2,
					alignment);

	uintptr_t object2 = (uintptr_t)res.r;
	if (res.e != OK) {
		printf("\nObject allocation failed\n");
	} else {
		printf("\nObject allocation SUCCESS\n");
	}

	uint64_t start_addr2 = (uint64_t)object2;
	uint64_t end_addr2   = (uint64_t)object2 + (obj_size2 - 1);

	/* ---------------- Init memory database --------------------- */
	ret = memdb_init();

	if (!ret) {
		printf("Mem db init correct!\n");
	} else {
		printf("Error init!\n");
	}

	/* ---------------- Insert object in database --------------------- */
	uint64_t start_addr = (uint64_t)block;
	uint64_t end_addr   = (uint64_t)block + (pool_size - 1);

	printf("\nstart_addr: %#lx, end_addr: %#lx\n", start_addr, end_addr);

	ret = memdb_insert(&partition, start_addr, end_addr, (uintptr_t)block,
			   type);
	if (!ret) {
		print_memdb_empty();
		printf("\nmemdb_insert SUCCESS\n\n");
	} else {
		print_memdb_empty();
		printf("\nmemdb_insert FAILED\n\n");
		goto error;
	}

	/* ---------------- Update database --------------------- */

	printf("\nstart_addr1: %#lx, end_addr1: %#lx\n", start_addr1,
	       end_addr1);

	ret = memdb_update(&partition, start_addr1, end_addr1, object1, type1,
			   block, type);
	if (!ret) {
		print_memdb_empty();
		printf("\nmemdb_update SUCCESS\n\n");
	} else {
		print_memdb_empty();
		printf("\nmemdb_update FAILED\n\n");
		goto error;
	}

	/* ---------------- Update database --------------------- */

	printf("\nstart_addr2: %#lx, end_addr2: %#lx\n", start_addr2,
	       end_addr2);

	ret = memdb_update(&partition, start_addr2, end_addr2, object2, type2,
			   block, type);
	if (!ret) {
		print_memdb_empty();
		printf("\nmemdb_update SUCCESS\n\n");
	} else {
		print_memdb_empty();
		printf("\nmemdb_update FAILED\n\n");
		goto error;
	}

	/* ----------------- lookup --------------------- */
	printf("\nLooking for start_addr1: %#lx\n", start_addr1);
	memdb_obj_type_result_t tot_res;

	tot_res = memdb_lookup(start_addr1);
	ret	= tot_res.e;
	if (tot_res.e == OK) {
		printf("\nmemdb_lookup SUCCESS. type: %d\n\n", tot_res.r.type);
	} else {
		printf("\nmemdb_lookup FAILED\n\n");
		goto error;
	}
error:
	return ret;
}

// 2 insertions, 2 updates, 2 lookups
int
test7()
{
	partition_t partition;
	int	    ret	      = 0;
	size_t	    alignment = sizeof(void *);

	memdb_type_t type  = MEMDB_TYPE_PARTITION;
	memdb_type_t type1 = MEMDB_TYPE_ALLOCATOR;
	memdb_type_t type2 = MEMDB_TYPE_EXTENT;

	size_t pool_size  = 4096 * 100;
	size_t pool_size2 = 1024;
	size_t obj_size1  = 4096;
	size_t obj_size2  = 1024;

	partition.allocator.heap = NULL;

	/* ---------------- Giving memory to heap --------------------- */
	uintptr_t block = (uintptr_t)malloc(pool_size);

	ret = allocator_heap_add_memory(&partition.allocator, (void *)block,
					pool_size);
	if (!ret) {
		printf("Memory added to heap\n");
	}

	/* ----------------- Hypervisor memory ------------------------ */
	uintptr_t block2 = (uintptr_t)malloc(pool_size2);

	/* ---------- Allocate object. partition for example ---------------- */
	void_ptr_result_t res;
	res = allocator_allocate_object(&partition.allocator, obj_size1,
					alignment);

	uintptr_t object1 = (uintptr_t)res.r;
	if (res.e != OK) {
		printf("Object allocation failed\n");
		goto error;
	} else {
		printf("Object allocation SUCCESS\n");
	}

	uint64_t start_addr1 = (uint64_t)object1;
	uint64_t end_addr1   = (uint64_t)object1 + (obj_size1 - 1);

	/* ---------- Allocate object. hypervisor for example ----------------
	 */
	res = allocator_allocate_object(&partition.allocator, obj_size2,
					alignment);

	uintptr_t object2 = (uintptr_t)res.r;
	if (res.e != OK) {
		printf("\nObject allocation failed\n");
		goto error;
	} else {
		printf("\nObject allocation SUCCESS\n");
	}

	uint64_t start_addr2 = (uint64_t)object2;
	uint64_t end_addr2   = (uint64_t)object2 + (obj_size2 - 1);

	/* ---------------- Init memory database --------------------- */
	ret = memdb_init();

	if (!ret) {
		printf("Mem db init correct!\n");
	} else {
		printf("Error init!\n");
		goto error;
	}

	/* ---------------- Insert object in database --------------------- */
	uint64_t start_addr = (uint64_t)block;
	uint64_t end_addr   = (uint64_t)block + (pool_size - 1);

	printf("\nstart_addr: %#lx, end_addr: %#lx\n", start_addr, end_addr);
	printf("\nnew type: %d\n", type);

	ret = memdb_insert(&partition, start_addr, end_addr, (uintptr_t)block,
			   type);
	if (!ret) {
		print_memdb_empty();
		printf("\nmemdb_insert SUCCESS\n\n");
	} else {
		printf("\nmemdb_insert FAILED\n\n");
		print_memdb_empty();
		goto error;
	}

	/* ---------------- Insert object in database --------------------- */
	uint64_t start_addrh = (uint64_t)block2;
	uint64_t end_addrh   = (uint64_t)block2 + (pool_size2 - 1);

	printf("\nstart_addr: %#lx, end_addr: %#lx\n", start_addrh, end_addrh);
	printf("\nnew type: %d\n", MEMDB_TYPE_ALLOCATOR);

	ret = memdb_insert(&partition, start_addrh, end_addrh,
			   (uintptr_t)block2, MEMDB_TYPE_ALLOCATOR);

	if (!ret) {
		print_memdb_empty();
		printf("\nmemdb_insert SUCCESS\n\n");
	} else {
		print_memdb_empty();
		printf("\nmemdb_insert FAILED\n\n");
		goto error;
	}

	/* ---------------- Update database --------------------- */

	printf("\nstart_addr1: %#lx, end_addr1: %#lx\n", start_addr1,
	       end_addr1);
	printf("\nnew type: %d old type: %d\n", type1, type);

	ret = memdb_update(&partition, start_addr1, end_addr1, object1, type1,
			   block, type);

	if (!ret) {
		print_memdb_empty();
		printf("\nmemdb_update SUCCESS\n\n");
	} else {
		print_memdb_empty();
		printf("\nmemdb_update FAILED\n\n");
		goto error;
	}

	/* ---------------- Update database --------------------- */

	printf("\nstart_addr2: %#lx, end_addr2: %#lx\n", start_addr2,
	       end_addr2);
	printf("\nnew type: %d old type: %d\n", type2, type);

	ret = memdb_update(&partition, start_addr2, end_addr2, object2, type2,
			   block, type);

	if (!ret) {
		print_memdb_empty();
		printf("\nmemdb_update SUCCESS\n\n");
	} else {
		print_memdb_empty();
		printf("\nmemdb_update FAILED\n\n");
		goto error;
	}

	/* ----------------- lookup --------------------- */
	printf("\nLooking for start_addr1: %#lx\n", start_addr1);
	memdb_obj_type_result_t tot_res;

	tot_res = memdb_lookup(start_addr1);
	ret	= tot_res.e;
	if (tot_res.e == OK) {
		printf("\nmemdb_lookup SUCCESS. type: %d\n\n", tot_res.r.type);
	} else {
		printf("\nmemdb_lookup FAILED\n\n");
		goto error;
	}

	/* ----------------- lookup --------------------- */
	printf("\nLooking for start_addrh: %#lx\n", start_addr1);
	tot_res = memdb_lookup(start_addrh);
	ret	= tot_res.e;
	if (tot_res.e == OK) {
		printf("\nmemdb_lookup SUCCESS. type: %d\n\n", tot_res.r.type);
	} else {
		printf("\nmemdb_lookup FAILED\n\n");
		goto error;
	}
error:
	return ret;
}

// 2 insertions, 2 updates, 2 updates back
// (Address ranges with 64 guard)
int
test8()
{
	partition_t partition;
	int	    ret	      = 0;
	size_t	    alignment = 4096;

	memdb_type_t type  = MEMDB_TYPE_PARTITION;
	memdb_type_t type1 = MEMDB_TYPE_ALLOCATOR;
	memdb_type_t type2 = MEMDB_TYPE_EXTENT;

	size_t pool_size  = 4096 * 100;
	size_t pool_size2 = 1024;
	size_t obj_size1  = 4096;
	size_t obj_size2  = 1024;

	partition.allocator.heap = NULL;

	/* ---------------- Giving memory to heap --------------------- */
	uintptr_t block = (uintptr_t)aligned_alloc(alignment, pool_size);

	ret = allocator_heap_add_memory(&partition.allocator, (void *)block,
					pool_size);
	if (!ret) {
		printf("Memory added to heap\n");
	}

	/* ----------------- Hypervisor memory ------------------------ */
	uintptr_t block2 = (uintptr_t)aligned_alloc(alignment, pool_size2);

	/* ---------- Allocate object. partition for example ---------------- */
	void_ptr_result_t res;
	res = allocator_allocate_object(&partition.allocator, obj_size1,
					alignment);

	uintptr_t object1 = (uintptr_t)res.r;
	if (res.e != OK) {
		printf("Object allocation failed\n");
		goto error;
	} else {
		printf("Object allocation SUCCESS\n");
	}

	uint64_t start_addr1 = (uint64_t)object1;
	uint64_t end_addr1   = (uint64_t)object1 + (obj_size1 - 1);

	/* ---------- Allocate object. hypervisor for example ----------------
	 */
	res = allocator_allocate_object(&partition.allocator, obj_size2,
					alignment);

	uintptr_t object2 = (uintptr_t)res.r;
	if (res.e != OK) {
		printf("\nObject allocation failed\n");
		goto error;
	} else {
		printf("\nObject allocation SUCCESS\n");
	}

	uint64_t start_addr2 = (uint64_t)object2;
	uint64_t end_addr2   = (uint64_t)object2 + (obj_size2 - 1);

	/* ---------------- Init memory database --------------------- */
	ret = memdb_init();

	if (!ret) {
		printf("Mem db init correct!\n");
	} else {
		printf("Error init!\n");
		goto error;
	}

	/* ---------------- Insert object in database --------------------- */
	uint64_t start_addr = 0;
	uint64_t end_addr   = 0;
	end_addr	    = (~(end_addr & 0) - 4096);

	printf("\nstart_addr: %#lx, end_addr: %#lx\n", start_addr, end_addr);
	printf("\nnew type: %d\n", type);

	ret = memdb_insert(&partition, start_addr, end_addr, (uintptr_t)block,
			   type);
	if (!ret) {
		print_memdb_empty();
		printf("\nmemdb_insert SUCCESS\n\n");
	} else {
		print_memdb_empty();
		printf("\nmemdb_insert FAILED\n\n");
		goto error;
	}

	/* ---------------- Insert object in database --------------------- */
	uint64_t start_addrh = 0;
	uint64_t end_addrh   = 0;

	start_addrh = (~(start_addrh & 0) - 4096) + 1;
	end_addrh   = (~(end_addrh & 0));

	printf("\nstart_addr: %#lx, end_addr: %#lx\n", start_addrh, end_addrh);
	printf("\nnew type: %d\n", MEMDB_TYPE_ALLOCATOR);

	ret = memdb_insert(&partition, start_addrh, end_addrh,
			   (uintptr_t)block2, MEMDB_TYPE_ALLOCATOR);

	if (!ret) {
		print_memdb_empty();
		printf("\nmemdb_insert SUCCESS\n\n");
	} else {
		print_memdb_empty();
		printf("\nmemdb_insert FAILED\n\n");
		goto error;
	}

	/* ---------------- Update database --------------------- */
	start_addr1 = start_addr + 6;
	end_addr1   = end_addr - (4096000000000000000);

	printf("\nstart_addr1: %#lx, end_addr1: %#lx\n", start_addr1,
	       end_addr1);
	printf("\nnew type: %d old type: %d\n", type1, type);

	ret = memdb_update(&partition, start_addr1, end_addr1, object1, type1,
			   block, type);

	if (!ret) {
		print_memdb_empty();
		printf("\nmemdb_update SUCCESS\n\n");
	} else {
		print_memdb_empty();
		printf("\nmemdb_update FAILED. ret = %d\n\n", ret);
		goto error;
	}

	/* ---------------- Update database --------------------- */
	start_addr2 = end_addr1 + 1;
	end_addr2   = end_addr;

	printf("\nstart_addr2: %#lx, end_addr2: %#lx\n", start_addr2,
	       end_addr2);
	printf("\nnew type: %d old type: %d\n", type2, type);

	ret = memdb_update(&partition, start_addr2, end_addr2, object2, type2,
			   block, type);

	if (!ret) {
		print_memdb_empty();
		printf("\nmemdb_update SUCCESS\n\n");
	} else {
		print_memdb_empty();
		printf("\nmemdb_update FAILED\n\n");
		goto error;
	}

	/* ---------------- Update back database --------------------- */

	printf("\nstart_addr1: %#lx, end_addr1: %#lx\n", start_addr1,
	       end_addr1);
	printf("\nnew type: %d old type: %d\n", type, type1);

	ret = memdb_update(&partition, start_addr1, end_addr1, block, type,
			   object1, type1);
	if (!ret) {
		print_memdb_empty();
		printf("\nmemdb_update back SUCCESS\n\n");
	} else {
		print_memdb_empty();
		printf("\nmemdb_update back FAILED\n\n");
		goto error;
	}

	/* ---------------- Update back database --------------------- */

	printf("\nstart_addr2: %#lx, end_addr2: %#lx\n", start_addr2,
	       end_addr2);
	printf("\nnew type: %d old type: %d\n", type, type2);

	ret = memdb_update(&partition, start_addr2, end_addr2, block, type,
			   object2, type2);
	if (!ret) {
		print_memdb_empty();
		printf("\nmemdb_update back SUCCESS\n\n");
	} else {
		print_memdb_empty();
		printf("\nmemdb_update back FAILED\n\n");
		goto error;
	}
error:
	return ret;
}

// Rollback. (2 insertion, 2 updates (last one rolled back))
int
test9()
{
	partition_t partition;
	int	    ret	      = 0;
	size_t	    alignment = 4096;

	memdb_type_t type  = MEMDB_TYPE_PARTITION;
	memdb_type_t type1 = MEMDB_TYPE_ALLOCATOR;
	memdb_type_t type2 = MEMDB_TYPE_EXTENT;

	size_t pool_size  = 4096 * 100;
	size_t pool_size2 = 1024;
	size_t obj_size1  = 4096;
	size_t obj_size2  = 1024;

	partition.allocator.heap = NULL;

	/* ---------------- Giving memory to heap --------------------- */
	uintptr_t block = (uintptr_t)aligned_alloc(alignment, pool_size);

	ret = allocator_heap_add_memory(&partition.allocator, (void *)block,
					pool_size);
	if (!ret) {
		printf("Memory added to heap\n");
	}

	/* ----------------- Hypervisor memory ------------------------ */
	uintptr_t block2 = (uintptr_t)aligned_alloc(alignment, pool_size2);

	/* ---------- Allocate object. partition for example ---------------- */
	void_ptr_result_t res;
	res = allocator_allocate_object(&partition.allocator, obj_size1,
					alignment);

	uintptr_t object1 = (uintptr_t)res.r;
	if (res.e != OK) {
		printf("Object allocation failed\n");
		goto error;
	} else {
		printf("Object allocation SUCCESS\n");
	}

	uint64_t start_addr1 = (uint64_t)object1;
	uint64_t end_addr1   = (uint64_t)object1 + (obj_size1 - 1);

	/* ---------- Allocate object. hypervisor for example ----------------
	 */
	res = allocator_allocate_object(&partition.allocator, obj_size2,
					alignment);

	uintptr_t object2 = (uintptr_t)res.r;
	if (res.e != OK) {
		printf("\nObject allocation failed\n");
		goto error;
	} else {
		printf("\nObject allocation SUCCESS\n");
	}

	uint64_t start_addr2 = (uint64_t)object2;
	uint64_t end_addr2   = (uint64_t)object2 + (obj_size2 - 1);

	/* ---------------- Init memory database --------------------- */
	ret = memdb_init();

	if (!ret) {
		printf("Mem db init correct!\n");
	} else {
		printf("Error init!\n");
		goto error;
	}

	/* ---------------- Insert object in database --------------------- */
	uint64_t start_addr = 139944292126720;
	uint64_t end_addr   = 139944292536319;

	printf("\nstart_addr: %#lx, end_addr: %#lx\n", start_addr, end_addr);
	printf("\nnew type: %d\n", type);

	ret = memdb_insert(&partition, start_addr, end_addr, (uintptr_t)block,
			   type);
	if (!ret) {
		print_memdb_empty();
		printf("\nmemdb_insert SUCCESS\n\n");
	} else {
		print_memdb_empty();
		printf("\nmemdb_insert FAILED\n\n");
		goto error;
	}

	/* ---------------- Insert object in database --------------------- */
	uint64_t start_addrh = (uint64_t)block2;
	uint64_t end_addrh   = (uint64_t)block2 + (pool_size2 - 1);

	printf("\nstart_addr: %#lx, end_addr: %#lx\n", start_addrh, end_addrh);
	printf("\nnew type: %d\n", MEMDB_TYPE_ALLOCATOR);

	ret = memdb_insert(&partition, start_addrh, end_addrh,
			   (uintptr_t)block2, MEMDB_TYPE_ALLOCATOR);

	if (!ret) {
		print_memdb_empty();
		printf("\nmemdb_insert SUCCESS\n\n");
	} else {
		print_memdb_empty();
		printf("\nmemdb_insert FAILED: %d\n\n", ret);
		goto error;
	}

	/* ---------------- Update database --------------------- */
	start_addr1 = start_addr + (4096 * 4);
	end_addr1   = end_addr;

	printf("\nstart_addr1: %#lx, end_addr1: %#lx\n", start_addr1,
	       end_addr1);
	printf("\nnew type: %d old type: %d\n", type1, type);

	ret = memdb_update(&partition, start_addr1, end_addr1, object1, type1,
			   block, type);

	if (!ret) {
		print_memdb_empty();
		printf("\nmemdb_update SUCCESS\n\n");
	} else {
		print_memdb_empty();
		printf("\nmemdb_update FAILED\n\n");
		goto error;
	}

	/* ---------------- Update database --------------------- */
	start_addr2 = start_addr;
	end_addr2   = start_addr1;

	printf("\nstart_addr2: %#lx, end_addr2: %#lx\n", start_addr2,
	       end_addr2);
	printf("\nnew type: %d old type: %d\n", type2, type);

	ret = memdb_update(&partition, start_addr2, end_addr2, object2, type2,
			   block, type);

	if (!ret) {
		print_memdb_empty();
		printf("\nmemdb_update SUCCESS, should not have succeeded!!!\n\n");
		ret = -1;
		goto error;
	} else {
		print_memdb_empty();
		printf("\nmemdb_update FAILED as expected (rollback).\n\n");
		ret = 0;
	}

error:
	return ret;
}

// Insert a second range that has to check a guard in the end
// path. Guard matches
int
test10()
{
	partition_t partition;
	int	    ret	      = 0;
	size_t	    alignment = 4096;

	memdb_type_t type = MEMDB_TYPE_PARTITION;

	size_t pool_size  = 4096 * 100;
	size_t pool_size2 = 1024;
	size_t obj_size1  = 4096;

	partition.allocator.heap = NULL;

	/* ---------------- Giving memory to heap --------------------- */
	uintptr_t block = (uintptr_t)aligned_alloc(alignment, pool_size);

	ret = allocator_heap_add_memory(&partition.allocator, (void *)block,
					pool_size);
	if (!ret) {
		printf("Memory added to heap\n");
	}

	/* ----------------- Hypervisor memory ------------------------ */
	uintptr_t block2 = (uintptr_t)aligned_alloc(alignment, pool_size2);

	/* ---------- Allocate object. partition for example ---------------- */
	void_ptr_result_t res;
	res = allocator_allocate_object(&partition.allocator, obj_size1,
					alignment);
	if (res.e != OK) {
		printf("Object allocation failed\n");
		goto error;
	} else {
		printf("Object allocation SUCCESS\n");
	}

	/* ---------------- Init memory database --------------------- */
	ret = memdb_init();

	if (!ret) {
		printf("Mem db init correct!\n");
	} else {
		printf("Error init!\n");
		goto error;
	}

	/* ---------------- Insert object in database --------------------- */
	uint64_t start_addrh = 0xfffffffffffff000;
	uint64_t end_addrh   = 0xffffffffffffffff;

	printf("\nstart_addrh: %#lx, end_addrh: %#lx\n", start_addrh,
	       end_addrh);
	printf("\nnew type: %d\n", MEMDB_TYPE_ALLOCATOR);

	ret = memdb_insert(&partition, start_addrh, end_addrh,
			   (uintptr_t)block2, MEMDB_TYPE_ALLOCATOR);

	if (!ret) {
		print_memdb_empty();
		printf("\nmemdb_insert SUCCESS\n\n");
	} else {
		print_memdb_empty();
		printf("\nmemdb_insert FAILED\n\n");
		goto error;
	}

	/* ---------------- Insert object in database --------------------- */
	uint64_t start_addr = 0x0;
	uint64_t end_addr   = 0xffffffffffffefff;

	printf("\nstart_addr: %#lx, end_addr: %#lx\n", start_addr, end_addr);
	printf("\nnew type: %d\n", type);

	ret = memdb_insert(&partition, start_addr, end_addr, (uintptr_t)block,
			   type);
	if (!ret) {
		print_memdb_empty();
		printf("\nmemdb_insert SUCCESS\n\n");
	} else {
		print_memdb_empty();
		printf("\nmemdb_insert FAILED\n\n");
		goto error;
	}

	/* ----------------- lookup --------------------- */
	uint64_t addr = 0xfffffffffffeeffe;

	printf("\nLooking for addr: %#lx\n", addr);
	memdb_obj_type_result_t tot_res;

	tot_res = memdb_lookup(addr);
	ret	= tot_res.e;
	if (tot_res.e == OK) {
		printf("\nmemdb_lookup SUCCESS. type: %d\n\n", tot_res.r.type);
	} else {
		printf("\nmemdb_lookup FAILED\n\n");
		goto error;
	}

error:
	return ret;
}

// Insert a range with a root guard and then insert another range that remove
// root guard
int
test11()
{
	partition_t partition;
	int	    ret	      = 0;
	size_t	    alignment = sizeof(void *);
	size_t	    pool_size = 4096 * 100;

	partition.allocator.heap = NULL;

	/* ---------------- Giving memory to heap --------------------- */
	uintptr_t block = (uintptr_t)malloc(pool_size);

	ret = allocator_heap_add_memory(&partition.allocator, (void *)block,
					pool_size);
	if (!ret) {
		printf("Memory added to heap\n");
	}

	/* ---------------- Init memory database --------------------- */
	ret = memdb_init();

	if (!ret) {
		printf("Mem db init correct!\n");
	} else {
		printf("Error init!\n");
	}

	size_t	     obj_size = 1024;
	memdb_type_t type     = MEMDB_TYPE_PARTITION;

	size_t	     obj_size2 = 4096;
	memdb_type_t type2     = MEMDB_TYPE_ALLOCATOR;

	/* ---------- Allocate object. partition for example ---------------- */
	void_ptr_result_t res;
	res = allocator_allocate_object(&partition.allocator, obj_size,
					alignment);

	uintptr_t object = (uintptr_t)res.r;
	if (res.e != OK) {
		printf("Object allocation failed\n");
	} else {
		printf("Object allocation SUCCESS\n");
	}

	uint64_t start_addr = (uint64_t)object;
	uint64_t end_addr   = (uint64_t)object + (obj_size - 1);

	/* ---------- Allocate object. hypervisor for example ----------------
	 */
	res = allocator_allocate_object(&partition.allocator, obj_size2,
					alignment);

	uintptr_t object2 = (uintptr_t)res.r;
	if (res.e != OK) {
		printf("\nObject allocation failed\n");
	} else {
		printf("\nObject allocation SUCCESS\n");
	}

	uint64_t start_addr2 = (uint64_t)object2;
	uint64_t end_addr2   = (uint64_t)object2 + (obj_size2 - 1);

	/* ---------------- Insert object in database --------------------- */
	start_addr = 0;
	end_addr   = 15;

	printf("\nstart_addr: %#lx, end_addr: %#lx\n", start_addr, end_addr);

	ret = memdb_insert(&partition, start_addr, end_addr, (uintptr_t)object,
			   type);
	if (!ret) {
		print_memdb_empty();
		printf("\nmemdb_insert SUCCESS\n\n");
	} else {
		print_memdb_empty();
		printf("\nmemdb_insert FAILED\n\n");
		goto error;
	}

	/* ---------------- Insert object in database --------------------- */
	start_addr2 = ~(start_addr2 & 0) - 15;
	end_addr2   = ~(end_addr2 & 0);

	printf("\nstart_addr2: %#lx, end_addr2: %#lx\n\n", start_addr2,
	       end_addr2);

	ret = memdb_insert(&partition, start_addr2, end_addr2,
			   (uintptr_t)object2, type2);
	if (!ret) {
		print_memdb_empty();
		printf("\nmemdb_insert SUCCESS\n\n");
	} else {
		print_memdb_empty();
		printf("\nmemdb_insert FAILED\n\n");
		goto error;
	}
error:
	return ret;
}

// 2 non contiguous insertions and 1 update that should fail due to
// contiguousness
int
test12()
{
	partition_t partition;
	int	    ret	      = 0;
	size_t	    alignment = 4096;

	memdb_type_t type = MEMDB_TYPE_PARTITION;

	size_t pool_size = 4096 * 100;
	size_t obj_size1 = 4096;
	size_t obj_size2 = 1024;

	partition.allocator.heap = NULL;

	/* ---------------- Giving memory to heap --------------------- */
	uintptr_t block = (uintptr_t)aligned_alloc(alignment, pool_size);

	ret = allocator_heap_add_memory(&partition.allocator, (void *)block,
					pool_size);
	if (!ret) {
		printf("Memory added to heap\n");
	}

	/* ----------------- Hypervisor memory ------------------------ */
	// uintptr_t block2 = (uintptr_t)aligned_alloc(alignment, pool_size2);

	/* ---------- Allocate object. partition for example ---------------- */
	void_ptr_result_t res;
	res = allocator_allocate_object(&partition.allocator, obj_size1,
					alignment);

	uintptr_t object1 = (uintptr_t)res.r;
	if (res.e != OK) {
		printf("Object allocation failed\n");
		goto error;
	} else {
		printf("Object allocation SUCCESS\n");
	}

	uint64_t start_addr1 = (uint64_t)object1;
	uint64_t end_addr1   = (uint64_t)object1 + (obj_size1 - 1);

	/* ---------- Allocate object. hypervisor for example ----------------
	 */
	res = allocator_allocate_object(&partition.allocator, obj_size2,
					alignment);
	if (res.e != OK) {
		printf("\nObject allocation failed\n");
		goto error;
	} else {
		printf("\nObject allocation SUCCESS\n");
	}

	/* ---------------- Init memory database --------------------- */
	ret = memdb_init();

	if (!ret) {
		printf("Mem db init correct!\n");
	} else {
		printf("Error init!\n");
		goto error;
	}

	/* ---------------- Insert object in database --------------------- */
	uint64_t start_addr = 0x4000;
	uint64_t end_addr   = 0x7fff;

	printf("\nstart_addr: %#lx, end_addr: %#lx\n", start_addr, end_addr);
	printf("\nnew type: %d\n", type);

	ret = memdb_insert(&partition, start_addr, end_addr, (uintptr_t)block,
			   type);
	if (!ret) {
		printf("\nmemdb_insert SUCCESS\n\n");
		print_memdb_empty();
	} else {
		printf("\nmemdb_insert FAILED\n\n");
		print_memdb_empty();
		goto error;
	}

	/* ---------------- Insert object in database --------------------- */
	uint64_t start_addrh = 0x1380;
	uint64_t end_addrh   = 0x13ff;

	printf("\nstart_addr: %#lx, end_addr: %#lx\n", start_addrh, end_addrh);
	printf("\nnew type: %d\n", MEMDB_TYPE_ALLOCATOR);

	ret = memdb_insert(&partition, start_addrh, end_addrh, (uintptr_t)block,
			   type);

	if (!ret) {
		printf("\nmemdb_insert SUCCESS\n\n");
		print_memdb_empty();
	} else {
		print_memdb_empty();
		printf("\nmemdb_insert FAILED\n\n");
		goto error;
	}

	/* ---------------- Update database --------------------- */
	start_addr1 = 0x1380;
	end_addr1   = 0x7fff;

	printf("\nstart_addr1: %#lx, end_addr1: %#lx\n", start_addr1,
	       end_addr1);
	printf("\nnew type: %d old type: %d\n", 4, type);

	ret = memdb_update(&partition, start_addr1, end_addr1, object1, 4,
			   block, type);

	if (!ret) {
		print_memdb_empty();
		printf("\nmemdb_update SUCCESS. should have failed!!\n\n");
		ret = -1;
		goto error;
	} else {
		print_memdb_empty();
		printf("\nmemdb_update FAILED as expected\n\n");
		ret = 0;
	}

error:
	return ret;
}

// Insert a second range that has to check a guard in the end
// path. Guard partially matches
int
test13()
{
	partition_t partition;
	int	    ret	      = 0;
	size_t	    alignment = 4096;

	memdb_type_t type = MEMDB_TYPE_PARTITION;

	size_t pool_size  = 4096 * 100;
	size_t pool_size2 = 1024;
	size_t obj_size1  = 4096;

	partition.allocator.heap = NULL;

	/* ---------------- Giving memory to heap --------------------- */
	uintptr_t block = (uintptr_t)aligned_alloc(alignment, pool_size);

	ret = allocator_heap_add_memory(&partition.allocator, (void *)block,
					pool_size);
	if (!ret) {
		printf("Memory added to heap\n");
	}

	/* ----------------- Hypervisor memory ------------------------ */
	uintptr_t block2 = (uintptr_t)aligned_alloc(alignment, pool_size2);

	/* ---------- Allocate object. partition for example ---------------- */
	void_ptr_result_t res;
	res = allocator_allocate_object(&partition.allocator, obj_size1,
					alignment);
	if (res.e != OK) {
		printf("Object allocation failed\n");
		goto error;
	} else {
		printf("Object allocation SUCCESS\n");
	}

	/* ---------------- Init memory database --------------------- */
	ret = memdb_init();

	if (!ret) {
		printf("Mem db init correct!\n");
	} else {
		printf("Error init!\n");
		goto error;
	}

	/* ---------------- Insert object in database --------------------- */
	uint64_t start_addrh = 0xfffffffffffff000;
	uint64_t end_addrh   = 0xffffffffffffffff;

	printf("\nstart_addrh: %#lx, end_addrh: %#lx\n", start_addrh,
	       end_addrh);
	printf("\nnew type: %d\n", MEMDB_TYPE_ALLOCATOR);

	ret = memdb_insert(&partition, start_addrh, end_addrh,
			   (uintptr_t)block2, MEMDB_TYPE_ALLOCATOR);

	if (!ret) {
		print_memdb_empty();
		printf("\nmemdb_insert SUCCESS\n\n");
	} else {
		print_memdb_empty();
		printf("\nmemdb_insert FAILED\n\n");
		goto error;
	}

	/* ---------------- Insert object in database --------------------- */
	uint64_t start_addr = 0x0;
	uint64_t end_addr   = 0xfffffffffffeeffe;

	printf("\nstart_addr: %#lx, end_addr: %#lx\n", start_addr, end_addr);
	printf("\nnew type: %d\n", type);

	ret = memdb_insert(&partition, start_addr, end_addr, (uintptr_t)block,
			   type);
	if (!ret) {
		print_memdb_empty();
		printf("\nmemdb_insert SUCCESS\n\n");
	} else {
		print_memdb_empty();
		printf("\nmemdb_insert FAILED\n\n");
		goto error;
	}

	/* ----------------- lookup --------------------- */
	uint64_t addr = 0xfffffffffffeeffe;

	printf("\nLooking for addr: %#lx\n", addr);
	memdb_obj_type_result_t tot_res;

	tot_res = memdb_lookup(addr);
	ret	= tot_res.e;
	if (tot_res.e == OK) {
		printf("\nmemdb_lookup SUCCESS. type: %d\n\n", tot_res.r.type);
	} else {
		printf("\nmemdb_lookup FAILED\n\n");
		goto error;
	}

error:
	return ret;
}

paddr_t returned_base;
size_t	returned_size;

error_t
add_free_range(paddr_t base, size_t size, void *arg)
{
	(void)arg;
	// uint64_t end_addr = base + (size - 1);
	// printf("add_free_range: base: %#lx - size: %lu - end_addr: %#lx\n",
	// base, size, end_addr);
	printf("add_free_range: base: %#lx - size: %#lx\n", base, size);

	returned_base = base;
	returned_size = size;

	return OK;
}

// 2 insertions, 2 updates, 2 lookups, 2 memwalk with guards
int
test14()
{
	partition_t partition;
	int	    ret	      = 0;
	size_t	    alignment = sizeof(void *);

	memdb_type_t type  = MEMDB_TYPE_PARTITION;
	memdb_type_t type1 = MEMDB_TYPE_ALLOCATOR;
	memdb_type_t type2 = MEMDB_TYPE_EXTENT;

	size_t pool_size  = 4096 * 100;
	size_t pool_size2 = 1024;
	size_t obj_size1  = 4096;
	size_t obj_size2  = 1024;

	partition.allocator.heap = NULL;

	/* ---------------- Giving memory to heap --------------------- */
	uintptr_t block = (uintptr_t)malloc(pool_size);

	ret = allocator_heap_add_memory(&partition.allocator, (void *)block,
					pool_size);
	if (!ret) {
		printf("Memory added to heap\n");
	}

	/* ----------------- Hypervisor memory ------------------------ */
	uintptr_t block2 = (uintptr_t)malloc(pool_size2);

	/* ---------- Allocate object. partition for example ---------------- */
	void_ptr_result_t res;
	res = allocator_allocate_object(&partition.allocator, obj_size1,
					alignment);

	uintptr_t object1 = (uintptr_t)res.r;
	if (res.e != OK) {
		printf("Object allocation failed\n");
		goto error;
	} else {
		printf("Object allocation SUCCESS\n");
	}

	uint64_t start_addr1 = (uint64_t)object1;
	uint64_t end_addr1   = (uint64_t)object1 + (obj_size1 - 1);

	/* ---------- Allocate object. hypervisor for example ----------------
	 */
	res = allocator_allocate_object(&partition.allocator, obj_size2,
					alignment);

	uintptr_t object2 = (uintptr_t)res.r;
	if (res.e != OK) {
		printf("\nObject allocation failed\n");
		goto error;
	} else {
		printf("\nObject allocation SUCCESS\n");
	}

	uint64_t start_addr2 = (uint64_t)object2;
	uint64_t end_addr2   = (uint64_t)object2 + (obj_size2 - 1);

	/* ---------------- Init memory database --------------------- */
	ret = memdb_init();

	if (!ret) {
		printf("Mem db init correct!\n");
	} else {
		printf("Error init!\n");
		goto error;
	}

	/* ---------------- Insert object in database --------------------- */
	uint64_t start_addr = (uint64_t)block;
	uint64_t end_addr   = (uint64_t)block + (pool_size - 1);
	uint64_t range_size = (end_addr - start_addr + 1);

	printf("\nstart_addr: %#lx, end_addr: %#lx, size: %#lx\n", start_addr,
	       end_addr, range_size);
	printf("\nnew type: %d\n", type);

	ret = memdb_insert(&partition, start_addr, end_addr, block, type);
	if (!ret) {
		print_memdb_empty();
		printf("\nmemdb_insert SUCCESS\n\n");
	} else {
		printf("\nmemdb_insert FAILED\n\n");
		print_memdb_empty();
		goto error;
	}

	/* ---------------- Insert object in database --------------------- */
	uint64_t start_addrh = (uint64_t)block2;
	uint64_t end_addrh   = (uint64_t)block2 + (pool_size2 - 1);
	uint64_t range_size2 = (end_addrh - start_addrh + 1);

	printf("\nstart_addr: %#lx, end_addr: %#lx, size: %#lx\n", start_addrh,
	       end_addrh, (end_addrh - start_addrh + 1));
	printf("\nnew type: %d\n", type1);

	ret = memdb_insert(&partition, start_addrh, end_addrh, block2, type1);

	if (!ret) {
		print_memdb_empty();
		printf("\nmemdb_insert SUCCESS\n\n");
	} else {
		print_memdb_empty();
		printf("\nmemdb_insert FAILED\n\n");
		goto error;
	}

	/* ---------------- MEMDB WALK --------------------- */
	printf("Mem walk to match type: %d\n", type);
	void   *arg  = NULL;
	error_t resl = memdb_walk(block, type, add_free_range, arg);
	if ((resl == OK) && (returned_base == start_addr) &&
	    (returned_size == range_size)) {
		printf("memdb walk SUCCESS\n\n");
	} else {
		printf("memdb walk FAILED\n");
		ret = resl;
		if (!(returned_base == start_addr)) {
			printf("returned_base: %#lx - start_addr: %#lx\n",
			       returned_base, start_addr);
			ret = -1;
		}
		if (!(returned_size == range_size)) {
			printf("returned_size: %#lx - size: %lx\n\n",
			       returned_size, range_size);
			ret = -1;
		}
		goto error;
	}

	/* ---------------- Update database --------------------- */

	printf("\nstart_addr1: %#lx, end_addr1: %#lx, size: %#lx\n",
	       start_addr1, end_addr1, obj_size1);
	printf("\nnew type: %d old type: %d\n", type1, type);
	uint64_t range_size3 = (end_addr1 - start_addr1 + 1);

	ret = memdb_update(&partition, start_addr1, end_addr1, block2, type1,
			   block, type);

	if (!ret) {
		print_memdb_empty();
		printf("\nmemdb_update SUCCESS\n\n");
	} else {
		print_memdb_empty();
		printf("\nmemdb_update FAILED\n\n");
		goto error;
	}

	/* ---------------- Update database --------------------- */

	printf("\nstart_addr2: %#lx, end_addr2: %#lx, size: %#lx\n",
	       start_addr2, end_addr2, obj_size2);
	printf("\nnew type: %d old type: %d\n", type2, type);

	ret = memdb_update(&partition, start_addr2, end_addr2, object2, type2,
			   block, type);

	if (!ret) {
		print_memdb_empty();
		printf("\nmemdb_update SUCCESS\n\n");
	} else {
		print_memdb_empty();
		printf("\nmemdb_update FAILED\n\n");
		goto error;
	}

	/* ---------------- MEMDB WALK --------------------- */
	printf("Mem walk to match type: %d\n", type1);
	resl = memdb_walk(block2, type1, add_free_range, arg);

	if ((resl == OK) &&
	    ((returned_base == start_addrh) ||
	     (returned_base == start_addr1)) &&
	    ((returned_size == range_size2) ||
	     (returned_size == range_size3))) {
		printf("memdb walk SUCCESS\n\n");
	} else {
		printf("memdb walk FAILED\n\n");
		ret = resl;
		if (!(returned_base == start_addrh)) {
			printf("returned_base: %#lx - start_addr: %#lx\n",
			       returned_base, start_addrh);
			ret = -1;
		}
		if (!(returned_size == range_size2)) {
			printf("returned_size: %#lx - size: %#lx\n",
			       returned_size, range_size2);
			ret = -1;
		}
		goto error;
	}

	/* ----------------- lookup --------------------- */
	printf("\nLooking for start_addr1: %#lx\n", start_addr1);
	memdb_obj_type_result_t tot_res;

	tot_res = memdb_lookup(start_addr1);
	ret	= tot_res.e;
	if (tot_res.e == OK) {
		printf("\nmemdb_lookup SUCCESS. type: %d\n\n", tot_res.r.type);
	} else {
		printf("\nmemdb_lookup FAILED\n\n");
		goto error;
	}

	/* ----------------- lookup --------------------- */
	printf("\nLooking for start_addrh: %#lx\n", start_addr1);
	tot_res = memdb_lookup(start_addrh);
	ret	= tot_res.e;
	if (tot_res.e == OK) {
		printf("\nmemdb_lookup SUCCESS. type: %d\n\n", tot_res.r.type);
	} else {
		printf("\nmemdb_lookup FAILED\n\n");
		goto error;
	}
error:
	return ret;
}

// 2 insertions, 2 updates, 2 lookups, 2 memwalk without guard
int
test15()
{
	partition_t partition;
	int	    ret	      = 0;
	size_t	    alignment = sizeof(void *);

	memdb_type_t type  = MEMDB_TYPE_PARTITION;
	memdb_type_t type1 = MEMDB_TYPE_ALLOCATOR;
	memdb_type_t type2 = MEMDB_TYPE_EXTENT;

	size_t pool_size  = 4096 * 100;
	size_t pool_size2 = 1024;
	size_t obj_size1  = 4096;
	size_t obj_size2  = 1024;

	partition.allocator.heap = NULL;

	/* ---------------- Giving memory to heap --------------------- */
	uintptr_t block = (uintptr_t)malloc(pool_size);

	ret = allocator_heap_add_memory(&partition.allocator, (void *)block,
					pool_size);
	if (!ret) {
		printf("Memory added to heap\n");
	}

	/* ----------------- Hypervisor memory ------------------------ */
	uintptr_t block2 = (uintptr_t)malloc(pool_size2);

	/* ---------- Allocate object. partition for example ---------------- */
	void_ptr_result_t res;
	res = allocator_allocate_object(&partition.allocator, obj_size1,
					alignment);

	uintptr_t object1 = (uintptr_t)res.r;
	if (res.e != OK) {
		printf("Object allocation failed\n");
		goto error;
	} else {
		printf("Object allocation SUCCESS\n");
	}

	uint64_t start_addr1 = (uint64_t)object1;
	uint64_t end_addr1   = (uint64_t)object1 + (obj_size1 - 1);

	/* ---------- Allocate object. hypervisor for example ----------------
	 */
	res = allocator_allocate_object(&partition.allocator, obj_size2,
					alignment);

	uintptr_t object2 = (uintptr_t)res.r;
	if (res.e != OK) {
		printf("\nObject allocation failed\n");
		goto error;
	} else {
		printf("\nObject allocation SUCCESS\n");
	}

	uint64_t start_addr2 = (uint64_t)object2;
	uint64_t end_addr2   = (uint64_t)object2 + (obj_size2 - 1);

	/* ---------------- Init memory database --------------------- */
	ret = memdb_init();

	if (!ret) {
		printf("Mem db init correct!\n");
	} else {
		printf("Error init!\n");
		goto error;
	}

	/* ---------------- Insert object in database --------------------- */
	uint64_t start_addr = 0x4;
	uint64_t end_addr   = 0xfffffffffffeeffe;
	uint64_t range_size = (end_addr - start_addr + 1);

	printf("\nstart_addr: %#lx, end_addr: %#lx, size: %#lx\n", start_addr,
	       end_addr, (end_addr - start_addr + 1));
	printf("\nnew type: %d\n", type);

	ret = memdb_insert(&partition, start_addr, end_addr, block, type);
	if (!ret) {
		print_memdb_empty();
		printf("\nmemdb_insert SUCCESS\n\n");
	} else {
		printf("\nmemdb_insert FAILED\n\n");
		print_memdb_empty();
		goto error;
	}

	/* ---------------- Insert object in database --------------------- */
	uint64_t start_addrh = 0xffffffffffffff00;
	uint64_t end_addrh   = 0xffffffffffffffff;
	uint64_t range_size2 = (end_addrh - start_addrh + 1);

	printf("\nstart_addr: %#lx, end_addr: %#lx, size: %#lx\n", start_addrh,
	       end_addrh, (end_addrh - start_addrh + 1));
	printf("\nnew type: %d\n", type1);

	ret = memdb_insert(&partition, start_addrh, end_addrh, block2, type1);

	if (!ret) {
		print_memdb_empty();
		printf("\nmemdb_insert SUCCESS\n\n");
	} else {
		print_memdb_empty();
		printf("\nmemdb_insert FAILED\n\n");
		goto error;
	}

	/* ---------------- MEMDB WALK --------------------- */
	printf("Mem walk to match type: %d\n", type);
	void   *arg  = NULL;
	error_t resl = memdb_walk(block, type, add_free_range, arg);
	if ((resl == OK) && (returned_base == start_addr) &&
	    (returned_size == range_size)) {
		printf("memdb walk SUCCESS\n\n");
	} else {
		printf("memdb walk FAILED\n\n");
		if (!(returned_base == start_addr)) {
			printf("returned_base: %#lx start_addr: %#lx\n",
			       returned_base, start_addr);
		}
		if (!(returned_size == range_size)) {
			printf("returned_size: %#lx, range_size: %#lx\n",
			       returned_size, range_size);
		}
		ret = -1;
		goto error;
	}

	/* ---------------- Update database --------------------- */

	printf("\nstart_addr1: %#lx, end_addr1: %#lx, size: %#lx\n",
	       start_addr1, end_addr1, obj_size1);
	printf("\nnew type: %d old type: %d\n", type1, type);

	ret = memdb_update(&partition, start_addr1, end_addr1, block2, type1,
			   block, type);

	if (!ret) {
		print_memdb_empty();
		printf("\nmemdb_update SUCCESS\n\n");
	} else {
		print_memdb_empty();
		printf("\nmemdb_update FAILED\n\n");
		goto error;
	}

	/* ---------------- Update database --------------------- */

	printf("\nstart_addr2: %#lx, end_addr2: %#lx, size: %#lx\n",
	       start_addr2, end_addr2, obj_size2);
	printf("\nnew type: %d old type: %d\n", type2, type);

	ret = memdb_update(&partition, start_addr2, end_addr2, object2, type2,
			   block, type);

	if (!ret) {
		print_memdb_empty();
		printf("\nmemdb_update SUCCESS\n\n");
	} else {
		print_memdb_empty();
		printf("\nmemdb_update FAILED\n\n");
		goto error;
	}

	/* ---------------- MEMDB WALK --------------------- */
	printf("Mem walk to match type: %d\n", type1);
	resl = memdb_walk(block2, type1, add_free_range, arg);
	if ((resl == OK) && (returned_base == start_addrh) &&
	    (returned_size == range_size2)) {
		printf("memdb walk SUCCESS\n\n");
	} else {
		printf("memdb walk FAILED\n\n");
		if (!(returned_base == start_addrh)) {
			printf("returned_base: %#lx start_addr: %#lx\n",
			       returned_base, start_addrh);
		}
		if (!(returned_size == range_size2)) {
			printf("returned_size: %#lx, range_size: %#lx\n",
			       returned_size, range_size2);
		}
		ret = -1;

		goto error;
	}

	/* ----------------- lookup --------------------- */
	printf("\nLooking for start_addr1: %#lx\n", start_addr1);
	memdb_obj_type_result_t tot_res;

	tot_res = memdb_lookup(start_addr1);
	ret	= tot_res.e;
	if (tot_res.e == OK) {
		printf("\nmemdb_lookup SUCCESS. type: %d\n\n", tot_res.r.type);
	} else {
		printf("\nmemdb_lookup FAILED\n\n");
		goto error;
	}

	/* ----------------- lookup --------------------- */
	printf("\nLooking for start_addrh: %#lx\n", start_addr1);
	tot_res = memdb_lookup(start_addrh);
	ret	= tot_res.e;
	if (tot_res.e == OK) {
		printf("\nmemdb_lookup SUCCESS. type: %d\n\n", tot_res.r.type);
	} else {
		printf("\nmemdb_lookup FAILED\n\n");
		goto error;
	}
error:
	return ret;
}

// 2 insertions, 2 updates, 2 lookups, 2 memwalk RANGE with guards
int
test16()
{
	partition_t partition;
	int	    ret	      = 0;
	size_t	    alignment = sizeof(void *);

	memdb_type_t type  = MEMDB_TYPE_PARTITION;
	memdb_type_t type1 = MEMDB_TYPE_ALLOCATOR;
	memdb_type_t type2 = MEMDB_TYPE_EXTENT;

	size_t pool_size  = 4096 * 100;
	size_t pool_size2 = 1024;
	size_t obj_size1  = 4096;
	size_t obj_size2  = 1024;

	partition.allocator.heap = NULL;

	/* ---------------- Giving memory to heap --------------------- */
	uintptr_t block = (uintptr_t)malloc(pool_size);

	ret = allocator_heap_add_memory(&partition.allocator, (void *)block,
					pool_size);
	if (!ret) {
		printf("Memory added to heap\n");
	}

	/* ----------------- Hypervisor memory ------------------------ */
	uintptr_t block2 = (uintptr_t)malloc(pool_size2);

	/* ---------- Allocate object. partition for example ---------------- */
	void_ptr_result_t res;
	res = allocator_allocate_object(&partition.allocator, obj_size1,
					alignment);

	uintptr_t object1 = (uintptr_t)res.r;
	if (res.e != OK) {
		printf("Object allocation failed\n");
		goto error;
	} else {
		printf("Object allocation SUCCESS\n");
	}

	uint64_t start_addr1 = (uint64_t)object1;
	uint64_t end_addr1   = (uint64_t)object1 + (obj_size1 - 1);

	/* ---------- Allocate object. hypervisor for example ----------------
	 */
	res = allocator_allocate_object(&partition.allocator, obj_size2,
					alignment);

	uintptr_t object2 = (uintptr_t)res.r;
	if (res.e != OK) {
		printf("\nObject allocation failed\n");
		goto error;
	} else {
		printf("\nObject allocation SUCCESS\n");
	}

	uint64_t start_addr2 = (uint64_t)object2;
	uint64_t end_addr2   = (uint64_t)object2 + (obj_size2 - 1);

	/* ---------------- Init memory database --------------------- */
	ret = memdb_init();

	if (!ret) {
		printf("Mem db init correct!\n");
	} else {
		printf("Error init!\n");
		goto error;
	}

	/* ---------------- Insert object in database --------------------- */
	uint64_t start_addr = (uint64_t)block;
	uint64_t end_addr   = (uint64_t)block + (pool_size - 1);
	uint64_t range_size = (end_addr - start_addr + 1);

	printf("\nstart_addr: %#lx, end_addr: %#lx, size: %#lx\n", start_addr,
	       end_addr, range_size);
	printf("\nnew type: %d\n", type);

	ret = memdb_insert(&partition, start_addr, end_addr, block, type);
	if (!ret) {
		print_memdb_empty();
		printf("\nmemdb_insert SUCCESS\n\n");
	} else {
		printf("\nmemdb_insert FAILED\n\n");
		print_memdb_empty();
		goto error;
	}

	/* ---------------- Insert object in database --------------------- */
	uint64_t start_addrh = (uint64_t)block2;
	uint64_t end_addrh   = (uint64_t)block2 + (pool_size2 - 1);
	uint64_t range_size2 = (end_addrh - start_addrh + 1);

	printf("\nstart_addr: %#lx, end_addr: %#lx, size: %#lx\n", start_addrh,
	       end_addrh, (end_addrh - start_addrh + 1));
	printf("\nnew type: %d\n", type1);

	ret = memdb_insert(&partition, start_addrh, end_addrh, block2, type1);

	if (!ret) {
		print_memdb_empty();
		printf("\nmemdb_insert SUCCESS\n\n");
	} else {
		print_memdb_empty();
		printf("\nmemdb_insert FAILED\n\n");
		goto error;
	}

	/* ---------------- MEMDB RANGE WALK --------------------- */
	printf("Mem range walk to match type: %d\n", type);
	void   *arg  = NULL;
	error_t resl = memdb_range_walk(block, type, start_addr, end_addr,
					add_free_range, arg);
	if ((resl == OK) && (returned_base == start_addr) &&
	    (returned_size == range_size)) {
		printf("memdb range walk SUCCESS\n\n");
	} else {
		printf("memdb range walk FAILED\n");
		ret = resl;
		if (!(returned_base == start_addr)) {
			printf("returned_base: %#lx - start_addr: %#lx\n",
			       returned_base, start_addr);
			ret = -1;
		}
		if (!(returned_size == range_size)) {
			printf("returned_size: %#lx - size: %lx\n\n",
			       returned_size, range_size);
			ret = -1;
		}
		goto error;
	}

	/* ---------------- Update database --------------------- */

	printf("\nstart_addr1: %#lx, end_addr1: %#lx, size: %#lx\n",
	       start_addr1, end_addr1, obj_size1);
	printf("\nnew type: %d old type: %d\n", type1, type);
	uint64_t range_size3 = (end_addr1 - start_addr1 + 1);

	ret = memdb_update(&partition, start_addr1, end_addr1, block2, type1,
			   block, type);

	if (!ret) {
		print_memdb_empty();
		printf("\nmemdb_update SUCCESS\n\n");
	} else {
		print_memdb_empty();
		printf("\nmemdb_update FAILED\n\n");
		goto error;
	}

	/* ---------------- Update database --------------------- */

	printf("\nstart_addr2: %#lx, end_addr2: %#lx, size: %#lx\n",
	       start_addr2, end_addr2, obj_size2);
	printf("\nnew type: %d old type: %d\n", type2, type);

	ret = memdb_update(&partition, start_addr2, end_addr2, object2, type2,
			   block, type);

	if (!ret) {
		print_memdb_empty();
		printf("\nmemdb_update SUCCESS\n\n");
	} else {
		print_memdb_empty();
		printf("\nmemdb_update FAILED\n\n");
		goto error;
	}

	/* ---------------- MEMDB RANGE WALK --------------------- */
	printf("Mem range walk to match type: %d\n", type1);
	resl = memdb_range_walk(block2, type1, start_addrh, end_addrh,
				add_free_range, arg);

	if ((resl == OK) && (returned_base == start_addrh) &&
	    (returned_size == range_size2)) {
		printf("memdb range walk SUCCESS\n\n");
	} else {
		printf("memdb range walk FAILED\n\n");
		ret = resl;
		if (!(returned_base == start_addrh)) {
			printf("returned_base: %#lx - start_addr: %#lx\n",
			       returned_base, start_addrh);
			ret = -1;
		}
		if (!(returned_size == range_size2)) {
			printf("returned_size: %#lx - size: %#lx\n",
			       returned_size, range_size2);
			ret = -1;
		}
		goto error;
	}

	/* ---------------- MEMDB RANGE WALK --------------------- */
	printf("Mem range walk to match type: %d\n", type1);
	resl = memdb_range_walk(block2, type1, start_addr1, end_addr1,
				add_free_range, arg);

	if ((resl == OK) && (returned_base == start_addr1) &&
	    (returned_size == range_size3)) {
		printf("memdb range walk SUCCESS\n\n");
	} else {
		printf("memdb range walk FAILED\n\n");
		ret = resl;
		if (!(returned_base == start_addr1)) {
			printf("returned_base: %#lx - start_addr: %#lx\n",
			       returned_base, start_addr1);
			ret = -1;
		}
		if (!(returned_size == range_size3)) {
			printf("returned_size: %#lx - size: %#lx\n",
			       returned_size, range_size3);
			ret = -1;
		}
		goto error;
	}

	/* ----------------- lookup --------------------- */
	printf("\nLooking for start_addr1: %#lx\n", start_addr1);
	memdb_obj_type_result_t tot_res;

	tot_res = memdb_lookup(start_addr1);
	ret	= tot_res.e;
	if (tot_res.e == OK) {
		printf("\nmemdb_lookup SUCCESS. type: %d\n\n", tot_res.r.type);
	} else {
		printf("\nmemdb_lookup FAILED\n\n");
		goto error;
	}

	/* ----------------- lookup --------------------- */
	printf("\nLooking for start_addrh: %#lx\n", start_addr1);
	tot_res = memdb_lookup(start_addrh);
	ret	= tot_res.e;
	if (tot_res.e == OK) {
		printf("\nmemdb_lookup SUCCESS. type: %d\n\n", tot_res.r.type);
	} else {
		printf("\nmemdb_lookup FAILED\n\n");
		goto error;
	}
error:
	return ret;
}

// 2 insertions, 2 updates, 2 lookups, 2 memwalk RANGE without guard
int
test17()
{
	partition_t partition;
	int	    ret	      = 0;
	size_t	    alignment = sizeof(void *);

	memdb_type_t type  = MEMDB_TYPE_PARTITION;
	memdb_type_t type1 = MEMDB_TYPE_ALLOCATOR;
	memdb_type_t type2 = MEMDB_TYPE_EXTENT;

	size_t pool_size  = 4096 * 100;
	size_t pool_size2 = 1024;
	size_t obj_size1  = 4096;
	size_t obj_size2  = 1024;

	partition.allocator.heap = NULL;

	/* ---------------- Giving memory to heap --------------------- */
	uintptr_t block = (uintptr_t)malloc(pool_size);

	ret = allocator_heap_add_memory(&partition.allocator, (void *)block,
					pool_size);
	if (!ret) {
		printf("Memory added to heap\n");
	}

	/* ----------------- Hypervisor memory ------------------------ */
	uintptr_t block2 = (uintptr_t)malloc(pool_size2);

	/* ---------- Allocate object. partition for example ---------------- */
	void_ptr_result_t res;
	res = allocator_allocate_object(&partition.allocator, obj_size1,
					alignment);

	uintptr_t object1 = (uintptr_t)res.r;
	if (res.e != OK) {
		printf("Object allocation failed\n");
		goto error;
	} else {
		printf("Object allocation SUCCESS\n");
	}

	uint64_t start_addr1 = (uint64_t)object1;
	uint64_t end_addr1   = (uint64_t)object1 + (obj_size1 - 1);

	/* ---------- Allocate object. hypervisor for example ----------------
	 */
	res = allocator_allocate_object(&partition.allocator, obj_size2,
					alignment);

	uintptr_t object2 = (uintptr_t)res.r;
	if (res.e != OK) {
		printf("\nObject allocation failed\n");
		goto error;
	} else {
		printf("\nObject allocation SUCCESS\n");
	}

	uint64_t start_addr2 = (uint64_t)object2;
	uint64_t end_addr2   = (uint64_t)object2 + (obj_size2 - 1);

	/* ---------------- Init memory database --------------------- */
	ret = memdb_init();

	if (!ret) {
		printf("Mem db init correct!\n");
	} else {
		printf("Error init!\n");
		goto error;
	}

	/* ---------------- Insert object in database --------------------- */
	uint64_t start_addr = 0x4;
	uint64_t end_addr   = 0xfffffffffffeeffe;
	uint64_t range_size = (end_addr - start_addr + 1);

	printf("\nstart_addr: %#lx, end_addr: %#lx, size: %#lx\n", start_addr,
	       end_addr, (end_addr - start_addr + 1));
	printf("\nnew type: %d\n", type);

	ret = memdb_insert(&partition, start_addr, end_addr, block, type);
	if (!ret) {
		print_memdb_empty();
		printf("\nmemdb_insert SUCCESS\n\n");
	} else {
		printf("\nmemdb_insert FAILED\n\n");
		print_memdb_empty();
		goto error;
	}

	/* ---------------- Insert object in database --------------------- */
	uint64_t start_addrh = 0xffffffffffffff00;
	uint64_t end_addrh   = 0xffffffffffffffff;
	uint64_t range_size2 = (end_addrh - start_addrh + 1);

	printf("\nstart_addr: %#lx, end_addr: %#lx, size: %#lx\n", start_addrh,
	       end_addrh, (end_addrh - start_addrh + 1));
	printf("\nnew type: %d\n", type1);

	ret = memdb_insert(&partition, start_addrh, end_addrh, block2, type1);

	if (!ret) {
		print_memdb_empty();
		printf("\nmemdb_insert SUCCESS\n\n");
	} else {
		print_memdb_empty();
		printf("\nmemdb_insert FAILED\n\n");
		goto error;
	}

	/* ---------------- MEMDB RANGE WALK --------------------- */
	printf("Mem range walk to match type: %d\n", type);
	void   *arg  = NULL;
	error_t resl = memdb_range_walk(block, type, start_addr, end_addr,
					add_free_range, arg);
	if ((resl == OK) && (returned_base == start_addr) &&
	    (returned_size == range_size)) {
		printf("memdb range walk SUCCESS\n\n");
	} else {
		printf("memdb range walk FAILED\n\n");
		if (!(returned_base == start_addr)) {
			printf("returned_base: %#lx start_addr: %#lx\n",
			       returned_base, start_addr);
		}
		if (!(returned_size == range_size)) {
			printf("returned_size: %#lx, range_size: %#lx\n",
			       returned_size, range_size);
		}
		ret = -1;
		goto error;
	}

	/* ---------------- Update database --------------------- */

	printf("\nstart_addr1: %#lx, end_addr1: %#lx, size: %#lx\n",
	       start_addr1, end_addr1, obj_size1);
	printf("\nnew type: %d old type: %d\n", type1, type);
	uint64_t range_size3 = (end_addr1 - start_addr1 + 1);

	ret = memdb_update(&partition, start_addr1, end_addr1, block2, type1,
			   block, type);

	if (!ret) {
		print_memdb_empty();
		printf("\nmemdb_update SUCCESS\n\n");
	} else {
		print_memdb_empty();
		printf("\nmemdb_update FAILED\n\n");
		goto error;
	}

	/* ---------------- Update database --------------------- */

	printf("\nstart_addr2: %#lx, end_addr2: %#lx, size: %#lx\n",
	       start_addr2, end_addr2, obj_size2);
	printf("\nnew type: %d old type: %d\n", type2, type);

	ret = memdb_update(&partition, start_addr2, end_addr2, object2, type2,
			   block, type);

	if (!ret) {
		print_memdb_empty();
		printf("\nmemdb_update SUCCESS\n\n");
	} else {
		print_memdb_empty();
		printf("\nmemdb_update FAILED\n\n");
		goto error;
	}

	/* ---------------- MEMDB RANGE WALK --------------------- */
	printf("Mem walk to match type: %d\n", type1);
	resl = memdb_range_walk(block2, type1, start_addrh, end_addrh,
				add_free_range, arg);
	if ((resl == OK) && (returned_base == start_addrh) &&
	    (returned_size == range_size2)) {
		printf("memdb range walk SUCCESS\n\n");
	} else {
		printf("memdb range walk FAILED\n\n");
		if (!(returned_base == start_addrh)) {
			printf("returned_base: %#lx start_addr: %#lx\n",
			       returned_base, start_addrh);
		}
		if (!(returned_size == range_size2)) {
			printf("returned_size: %#lx, range_size: %#lx\n",
			       returned_size, range_size2);
		}
		ret = -1;

		goto error;
	}

	/* ---------------- MEMDB RANGE WALK --------------------- */
	printf("Mem walk to match type: %d\n", type1);
	resl = memdb_range_walk(block2, type1, start_addr1, end_addr1,
				add_free_range, arg);
	if ((resl == OK) && (returned_base == start_addr1) &&
	    (returned_size == range_size3)) {
		printf("memdb range walk SUCCESS\n\n");
	} else {
		printf("memdb range walk FAILED\n\n");
		if (!(returned_base == start_addr1)) {
			printf("returned_base: %#lx start_addr: %#lx\n",
			       returned_base, start_addr1);
		}
		if (!(returned_size == range_size3)) {
			printf("returned_size: %#lx, range_size: %#lx\n",
			       returned_size, range_size3);
		}
		ret = -1;

		goto error;
	}

	/* ----------------- lookup --------------------- */
	printf("\nLooking for start_addr1: %#lx\n", start_addr1);
	memdb_obj_type_result_t tot_res;

	tot_res = memdb_lookup(start_addr1);
	ret	= tot_res.e;
	if (tot_res.e == OK) {
		printf("\nmemdb_lookup SUCCESS. type: %d\n\n", tot_res.r.type);
	} else {
		printf("\nmemdb_lookup FAILED\n\n");
		goto error;
	}

	/* ----------------- lookup --------------------- */
	printf("\nLooking for start_addrh: %#lx\n", start_addr1);
	tot_res = memdb_lookup(start_addrh);
	ret	= tot_res.e;
	if (tot_res.e == OK) {
		printf("\nmemdb_lookup SUCCESS. type: %d\n\n", tot_res.r.type);
	} else {
		printf("\nmemdb_lookup FAILED\n\n");
		goto error;
	}
error:
	return ret;
}

#define NUM_TESTS 17

int (*func_ptr[NUM_TESTS])(void) = {
	test1,	// Insert two ranges in db
	test2,	// One insertion and two updates
	test3,	// One insertion, two updates, and two updates back to state
		// after insertion
	test4,	// 2 insertions, 2 updates, 2 updates back to state after
		// insertions
	test5,	// 1 insertion, 2 updates, 2 check of contiguousness (1 must
		// succeed and the other one fail)
	test6,	// 1 insertion, 2 updates, 2 lookups
	test7,	// 2 insertions, 2 updates, 2 lookups
	test8,	// Address ranges with 64 guard (2 insertions, 2 updates, 2
		// updates back)
	test9,	// Rollback. (2 insertion, 2 updates (last one rolled back))
	test10, // Insert a second range that has to check a guard in the end
		// path. Guard matches
	test11, // Insert a range with a root guard and then insert another
		// range that remove root guard
	test12, // 2 non contiguous insertions and 1 update that should fail due
		// to contiguousness
	test13, // Insert a second range that has to check a guard in the end
		// path. Guard partially matches
	test14, // 2 insertions, 2 updates, 2 lookups, 2 mem walk with GUARDS
	test15, // 2 insertions, 2 updates, 2 lookups, 2 mem walk without guards
	test16, // 2 insertions, 2 updates, 2 lookups, 2 mem RANGE walk with
		// GUARDS
	test17, // 2 insertions, 2 updates, 2 lookups, 2 mem RANGE walk without
		// guards
};

int
main()
{
	int test = 0;
	int ret	 = 0;

	switch (test) {
	case 0:
		for (int i = 0; i < NUM_TESTS; i++) {
			printf("\n\n_____________________________________________________ TEST %d ________________________________________________\n\n",
			       i + 1);
			ret = func_ptr[i]();
			if (ret != 0) {
				printf("FAILED test: %d\n", i + 1);
				break;
			}
		}
		if (ret == 0) {
			printf("All %d tests passed!\n", NUM_TESTS);
		}
		break;

	case 1:
		test1();
		break;
	case 2:
		test2();
		break;
	case 3:
		test3();
		break;
	case 4:
		test4();
		break;
	case 5:
		// is memory contiguous test
		test5();
		break;
	case 6:
		// lookup test
		test6();
		break;
	case 7:
		// lookup test
		test7();
		break;
	case 8:
		// Address ranges with 64 guard
		test8();
		break;
	case 9:
		// Rollback test
		test9();
		break;
	case 10:
		// Insert a second range that has to check a guard in the end
		// path. Guard matches
		test10();
		break;
	case 11:
		test11();
		break;
	case 12:
		test12();
		break;
	case 13:
		// Insert a second range that has to check a guard in the end
		// path. Guard partially matches
		test13();
		break;
	}
}

```

`hyp/mem/memdb_bitmap/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

interface memdb
base_module hyp/mem/memdb
source memdb.c
events memdb.ev
types memdb.tc

```

`hyp/mem/memdb_bitmap/memdb.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

module memdb_bitmap

subscribe rcu_update[RCU_UPDATE_CLASS_MEMDB_RELEASE_LEVEL_TABLE]
	handler memdb_bitmap_free_level_table(entry)

subscribe rcu_update[RCU_UPDATE_CLASS_MEMDB_RELEASE_LEVEL_BITMAP]
	handler memdb_bitmap_free_level_bitmap(entry)

subscribe boot_cold_init()
	// Must run after pagetable init and hyp_aspace initialisation.
	priority 10

subscribe partition_add_ram_range(owner, phys_base, size)
	unwinder

subscribe partition_remove_ram_range(owner, phys_base, size)
	unwinder

```

`hyp/mem/memdb_bitmap/memdb.tc`:

```tc
// © 2023 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <types/bitmap.h>
#include <util.h>

define MEMDB_BITS_PER_LEVEL constant type index_t = 8;
define MEMDB_NUM_ENTRIES constant type index_t = 1 << MEMDB_BITS_PER_LEVEL;

// In unit test configurations, we give the memdb some extra levels so the
// test cases don't interfere with the actual memdb.
#if defined(UNITTESTS)
define MEMDB_MAX_BITS constant type index_t = 64 - MEMDB_BITS_PER_LEVEL;
#else
define MEMDB_MAX_BITS constant type index_t = PLATFORM_PHYS_ADDRESS_BITS;
#endif

// We set the start bits to ensure that we can place a bitmap level such that
// each object ID in the bitmap covers exactly one stage 2 page. This
// maximises the memory efficiency of the memdb when recording single-page
// memory shares, such as when the shared memory is allocated from an HLOS
// VM's page allocator.
define MEMDB_PAGE_BITS constant type index_t = msb(PGTABLE_VM_PAGE_SIZE);
define MEMDB_START_BITS constant type index_t =
		MEMDB_PAGE_BITS +
		util_round_up(MEMDB_MAX_BITS - MEMDB_PAGE_BITS,
			MEMDB_BITS_PER_LEVEL);
define MEMDB_ROOT_ENTRY_BITS constant type index_t =
		MEMDB_START_BITS - MEMDB_BITS_PER_LEVEL;

// The granularity of the memdb might be between 1 byte (0 bits of address)
// and half of MEMDB_NUM_ENTRIES (MEMDB_BITS_PER_LEVEL-1 bits of address).
// This is generally ok as long as it is less than MEMDB_PAGE_BITS, because
// the memdb doesn't need to track sub-page allocations in most cases. The
// exception is allocator ranges at boot time which may not be page aligned,
// but can be aligned up to MEMDB_MIN_SIZE if necessary.
define MEMDB_MIN_BITS constant type index_t =
		MEMDB_PAGE_BITS -
		util_round_down(MEMDB_PAGE_BITS, MEMDB_BITS_PER_LEVEL);
define MEMDB_MIN_SIZE constant size = (1 << MEMDB_MIN_BITS);

extend rcu_update_class enumeration {
	memdb_release_level_table;
	memdb_release_level_bitmap;
};

define MEMDB_OBJECT_ALIGN_P2 constant type index_t = 3;

define memdb_entry bitfield<64> {
	auto	entry_type	enumeration memdb_type;
	// Use a union of pointer types here so the type system can check
	// that the pointer alignment is sufficient.
	// FIXME:
	63:(MEMDB_OBJECT_ALIGN_P2)
		entry_ptr	uintptr lsl(MEMDB_OBJECT_ALIGN_P2);
};

define memdb_level_table structure {
	entries		array(MEMDB_NUM_ENTRIES) bitfield memdb_entry(atomic);
	rcu_entry	structure rcu_entry(contained);
};

define MEMDB_BITMAP_ID_BITS constant type index_t = 4;
define MEMDB_BITMAP_SIZE constant type index_t =
	MEMDB_BITMAP_ID_BITS * MEMDB_NUM_ENTRIES;
define MEMDB_BITMAP_OBJECTS constant type index_t = 1 << MEMDB_BITMAP_ID_BITS;

// Must be large enough to fit MEMDB_NUM_ENTRIES
define memdb_bitmap_count_t newtype uint16;

define memdb_level_bitmap structure {
	bitmap		BITMAP(MEMDB_BITMAP_SIZE, atomic);
	objects		array(MEMDB_BITMAP_OBJECTS)
			bitfield memdb_entry(atomic);
	counts		array(MEMDB_BITMAP_OBJECTS) type memdb_bitmap_count_t;
	rcu_entry	structure rcu_entry(contained);
};

extend memdb_type enumeration {
	LEVEL_TABLE;
	LEVEL_BITMAP;
};

```

`hyp/mem/memdb_bitmap/src/memdb.c`:

```c
// © 2023 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>
#include <string.h>

#include <hypcontainers.h>

#include <atomic.h>
#include <bitmap.h>
#include <bootmem.h>
#include <compiler.h>
#include <log.h>
#include <memdb.h>
#include <panic.h>
#include <partition.h>
#include <rcu.h>
#include <spinlock.h>
#include <trace.h>
#include <util.h>

#include "event_handlers.h"

static spinlock_t memdb_lock;

static_assert((uint64_t)MEMDB_TYPE_NOTYPE == 0U,
	      "Zero-initialised memdb entries must be empty");
static memdb_level_table_t memdb_root;

extern const char image_phys_start;
extern const char image_phys_last;

// We rely on the bitmap extract and insert operations being atomic, which is
// only possible if the field never spans two machine words. This is the case if
// the field's size is a power of two, or if the whole bitmap fits in one word.
//
// The AArch64 LDP & CASP instructions could be used to atomically access two
// adjacent words if FEAT_LSE2 is implemented, but there is no easy way to make
// use of that from platform-independent C code, and it doesn't work on older
// ARMv8 hardware without FEAT_LSE2.
static_assert(util_is_p2(MEMDB_BITMAP_ID_BITS) ||
		      (MEMDB_BITMAP_SIZE < BITMAP_WORD_BITS),
	      "Bitmap extract & insert must be atomic");

void
memdb_bitmap_handle_boot_cold_init(void)
{
	partition_t *hyp_partition = partition_get_private();
	assert(hyp_partition != NULL);

	spinlock_init(&memdb_lock);

	// Assign the hypervisor's ELF image to the private partition.
	error_t err = memdb_insert(hyp_partition, (paddr_t)&image_phys_start,
				   (paddr_t)&image_phys_last,
				   (uintptr_t)hyp_partition,
				   MEMDB_TYPE_PARTITION);
	if (err != OK) {
		panic("Error adding boot memory to hyp_partition");
	}

	// Obtain the initial bootmem range and change its ownership to the
	// hypervisor's allocator. We assume here that no other memory has been
	// assigned to any allocators yet.
	size_t bootmem_size	 = 0U;
	void  *bootmem_virt_base = bootmem_get_region(&bootmem_size);
	assert((bootmem_size != 0U) && (bootmem_virt_base != NULL));
	paddr_t bootmem_phys_base = partition_virt_to_phys(
		hyp_partition, (uintptr_t)bootmem_virt_base);
	assert(!util_add_overflows(bootmem_phys_base, bootmem_size - 1U));

	// Update ownership of the hypervisor partition's allocator memory
	err = memdb_update(hyp_partition, bootmem_phys_base,
			   bootmem_phys_base + (bootmem_size - 1U),
			   (uintptr_t)&hyp_partition->allocator,
			   MEMDB_TYPE_ALLOCATOR, (uintptr_t)hyp_partition,
			   MEMDB_TYPE_PARTITION);
	if (err != OK) {
		panic("Error updating bootmem allocator memory");
	}
}

static error_t
memdb_range_check(paddr_t start_addr, paddr_t end_addr)
{
	error_t err;

	if (start_addr >= end_addr) {
		err = ERROR_ARGUMENT_INVALID;
	} else if (end_addr >= util_bit(MEMDB_MAX_BITS)) {
		err = ERROR_ARGUMENT_SIZE;
	} else if (!util_is_p2aligned(start_addr, MEMDB_MIN_BITS) ||
		   !util_is_p2aligned(end_addr + 1U, MEMDB_MIN_BITS)) {
		err = ERROR_ARGUMENT_ALIGNMENT;
	} else {
		err = OK;
	}

	return err;
}

static void
memdb_release_level_table(memdb_level_table_t *table)
{
	rcu_enqueue(&table->rcu_entry,
		    RCU_UPDATE_CLASS_MEMDB_RELEASE_LEVEL_TABLE);
}

rcu_update_status_t
memdb_bitmap_free_level_table(rcu_entry_t *entry)
{
	rcu_update_status_t ret		  = rcu_update_status_default();
	partition_t	   *hyp_partition = partition_get_private();

	memdb_level_table_t *table =
		memdb_level_table_container_of_rcu_entry(entry);

	error_t err = partition_free(hyp_partition, table, sizeof(*table));
	if (err != OK) {
		panic(__func__);
	}

	return ret;
}

static void
memdb_release_level_bitmap(memdb_level_bitmap_t *bitmap)
{
	rcu_enqueue(&bitmap->rcu_entry,
		    RCU_UPDATE_CLASS_MEMDB_RELEASE_LEVEL_BITMAP);
}

rcu_update_status_t
memdb_bitmap_free_level_bitmap(rcu_entry_t *entry)
{
	rcu_update_status_t ret		  = rcu_update_status_default();
	partition_t	   *hyp_partition = partition_get_private();

	memdb_level_bitmap_t *bitmap =
		memdb_level_bitmap_container_of_rcu_entry(entry);

	error_t err = partition_free(hyp_partition, bitmap, sizeof(*bitmap));
	if (err != OK) {
		panic(__func__);
	}

	return ret;
}

static memdb_entry_t
memdb_entry_for_object(uintptr_t object, memdb_type_t obj_type)
{
	memdb_entry_t entry = memdb_entry_default();
	memdb_entry_set_entry_ptr(&entry, object);
	memdb_entry_set_entry_type(&entry, obj_type);
	return entry;
}

static memdb_level_bitmap_ptr_result_t
memdb_create_bitmap(memdb_entry_t initial_entry)
{
	memdb_level_bitmap_ptr_result_t ret;
	partition_t		       *hyp_partition = partition_get_private();

	void_ptr_result_t alloc_ret =
		partition_alloc(hyp_partition, sizeof(memdb_level_bitmap_t),
				alignof(memdb_level_bitmap_t));
	if (alloc_ret.e != OK) {
		ret = memdb_level_bitmap_ptr_result_error(alloc_ret.e);
		goto out;
	}
	ret = memdb_level_bitmap_ptr_result_ok(
		(memdb_level_bitmap_t *)alloc_ret.r);

	static_assert(MEMDB_NUM_ENTRIES <
			      util_bit(8U * sizeof(memdb_bitmap_count_t)),
		      "memdb_bitmap_count_t is too small");
	*ret.r = (memdb_level_bitmap_t){
		.objects = { [0] = initial_entry, },
		.counts = { [0] = (memdb_bitmap_count_t)MEMDB_NUM_ENTRIES, },
	};

out:
	return ret;
}

static memdb_level_table_ptr_result_t
memdb_create_table(memdb_entry_t initial_entry)
{
	memdb_level_table_ptr_result_t ret;
	partition_t		      *hyp_partition = partition_get_private();

	void_ptr_result_t alloc_ret =
		partition_alloc(hyp_partition, sizeof(memdb_level_table_t),
				alignof(memdb_level_table_t));
	if (alloc_ret.e != OK) {
		ret = memdb_level_table_ptr_result_error(alloc_ret.e);
		goto out;
	}
	ret = memdb_level_table_ptr_result_ok(
		(memdb_level_table_t *)alloc_ret.r);

	// Here we encounter an ugly quirk of C11 atomics: The compiler will not
	// let us use a plain { 0 } to zero-initialise this structure because
	// its first member is an array of atomic structs, and 0 is not a legal
	// initialiser for an atomic struct.
	*ret.r = (memdb_level_table_t){
		.rcu_entry = 0,
	};

	// Fill all of the entries with the initial entry value.
	for (index_t i = 0U; i < util_array_size(ret.r->entries); i++) {
		atomic_init(&ret.r->entries[i], initial_entry);
	}

out:
	return ret;
}

static memdb_level_table_ptr_result_t
memdb_convert_bitmap(memdb_level_bitmap_t *bitmap) REQUIRE_LOCK(memdb_lock)
{
	memdb_level_table_ptr_result_t ret =
		memdb_create_table(memdb_entry_default());
	if (ret.e != OK) {
		goto out;
	}

	memdb_entry_t objects[MEMDB_BITMAP_OBJECTS] = { 0 };
	for (index_t i = 0U; i < util_array_size(objects); i++) {
		objects[i] = atomic_load_relaxed(&bitmap->objects[i]);
	}

	for (index_t i = 0U; i < MEMDB_NUM_ENTRIES; i++) {
		const index_t cur_id = (index_t)bitmap_atomic_extract(
			bitmap->bitmap, i * MEMDB_BITMAP_ID_BITS,
			MEMDB_BITMAP_ID_BITS, memory_order_relaxed);
		atomic_init(&ret.r->entries[i], objects[cur_id]);
	}

out:
	return ret;
}

static memdb_level_bitmap_ptr_result_t
memdb_duplicate_bitmap(memdb_level_bitmap_t *bitmap) REQUIRE_LOCK(memdb_lock)
{
	memdb_level_bitmap_ptr_result_t ret;
	partition_t		       *hyp_partition = partition_get_private();

	void_ptr_result_t alloc_ret =
		partition_alloc(hyp_partition, sizeof(memdb_level_bitmap_t),
				alignof(memdb_level_bitmap_t));
	if (alloc_ret.e != OK) {
		ret = memdb_level_bitmap_ptr_result_error(alloc_ret.e);
		goto out;
	}
	ret = memdb_level_bitmap_ptr_result_ok(
		(memdb_level_bitmap_t *)alloc_ret.r);

	*ret.r = (memdb_level_bitmap_t){ 0U };

	for (index_t i = 0U; i < util_array_size(bitmap->objects); i++) {
		// Copy only the objects with nonzero counts, so that the other
		// IDs can be allocated to new objects.
		if (bitmap->counts[i] != 0U) {
			atomic_init(&ret.r->objects[i],
				    atomic_load_relaxed(&bitmap->objects[i]));
			ret.r->counts[i] = bitmap->counts[i];
		}
	}

	for (index_t i = 0U; i < util_array_size(bitmap->bitmap); i++) {
		atomic_init(&ret.r->bitmap[i],
			    atomic_load_relaxed(&bitmap->bitmap[i]));
	}

out:
	return ret;
}

static index_t
memdb_entry_index(paddr_t addr, index_t entry_bits)
{
	return (index_t)((addr >> entry_bits) &
			 util_mask(MEMDB_BITS_PER_LEVEL));
}

static index_result_t
memdb_update_bitmap_check_owner(index_t start_index, index_t end_index,
				memdb_entry_t		    old_entry,
				const memdb_level_bitmap_t *bitmap)
{
	index_result_t ret;

	index_t old_id;
	for (old_id = 0U; old_id < MEMDB_BITMAP_OBJECTS; old_id++) {
		if ((bitmap->counts[old_id] != 0U) &&
		    memdb_entry_is_equal(
			    atomic_load_relaxed(&bitmap->objects[old_id]),
			    old_entry)) {
			break;
		}
	}
	if (old_id == MEMDB_BITMAP_OBJECTS) {
		// Old entry isn't present anywhere in this bitmap
		ret = index_result_error(ERROR_MEMDB_NOT_OWNER);
		goto out;
	}

	// TODO: This could be optimised by using splat, xor and CLZ to find
	// contiguous ranges.
	for (index_t i = start_index; i <= end_index; i++) {
		const index_t cur_id = (index_t)bitmap_atomic_extract(
			bitmap->bitmap, i * MEMDB_BITMAP_ID_BITS,
			MEMDB_BITMAP_ID_BITS, memory_order_relaxed);

		if (cur_id != old_id) {
			ret = index_result_error(ERROR_MEMDB_NOT_OWNER);
			goto out;
		}
	}

	ret = index_result_ok(old_id);
out:
	return ret;
}

// Returns true if the bitmap's entries are all identical, so it can be
// collapsed into a single entry. Returns:
// - ERROR_MEMDB_NOT_OWNER if the update is invalid. In this case, the update
//   must be rolled back and the error returned to the caller.
// - ERROR_BUSY if the bitmap's object IDs are all in use, so it would need to
//   be converted to a table to perform the requested update.
// - ERROR_RETRY if the bitmap's object IDs have all been allocated, but one
//   of them has a usage count of 0, so the update needs RCU synchronisation.
// - ERROR_ARGUMENT_ALIGNMENT if the start or end address is within the range
//   represented by a field in the bitmap, so the bitmap must be converted to a
//   table to allow a next-level table to be created.
static bool_result_t
memdb_update_bitmap(paddr_t start, paddr_t end, memdb_entry_t old_entry,
		    memdb_entry_t new_entry, memdb_level_bitmap_t *bitmap,
		    index_t entry_bits) REQUIRE_LOCK(memdb_lock)
{
	bool_result_t ret;

	assert((entry_bits <= MEMDB_ROOT_ENTRY_BITS) &&
	       ((start >> (entry_bits + MEMDB_BITS_PER_LEVEL)) ==
		(end >> (entry_bits + MEMDB_BITS_PER_LEVEL))));

	const index_t start_index  = memdb_entry_index(start, entry_bits);
	const index_t end_index	   = memdb_entry_index(end, entry_bits);
	const count_t changed_bits = end_index - start_index + 1U;

	// We do all of the ownership checks before making any changes. There
	// are two reasons for this: first, we avoid having to implement
	// rollback; second, it prevents triggering bitmap to table conversion
	// by returning ERROR_BUSY for an update that would fail anyway.
	const index_result_t old_id_r = memdb_update_bitmap_check_owner(
		start_index, end_index, old_entry, bitmap);
	if (old_id_r.e != OK) {
		ret = bool_result_error(old_id_r.e);
		goto out;
	}
	const index_t old_id = old_id_r.r;

	// At this point we know that the update will succeed; now we need to
	// determine whether it can be represented by the bitmap. First, check
	// that the address range is exactly equal to the bit range.
	if (!util_is_p2aligned(start, entry_bits) ||
	    !util_is_p2aligned(end + 1U, entry_bits)) {
		ret = bool_result_error(ERROR_ARGUMENT_ALIGNMENT);
		goto out;
	}

	// Try to find an existing ID for the new entry.
	index_t new_id;
	for (new_id = 0U; new_id < MEMDB_BITMAP_OBJECTS; new_id++) {
		if (memdb_entry_is_equal(
			    atomic_load_relaxed(&bitmap->objects[new_id]),
			    new_entry)) {
			break;
		}
	}

	if (new_id == MEMDB_BITMAP_OBJECTS) {
		bool should_retry = false;

		// No existing ID. Try to find an ID that can be claimed.
		for (new_id = 0U; new_id < MEMDB_BITMAP_OBJECTS; new_id++) {
			if (bitmap->counts[new_id] != 0U) {
				// ID is in use already.
				continue;
			}

			if (memdb_entry_is_equal(
				    atomic_load_relaxed(
					    &bitmap->objects[new_id]),
				    memdb_entry_default())) {
				// Entry has never been used; we can claim it.
				break;
			}

			// Found an ID that was previously used, but we can't
			// safely recycle it in place because concurrent RCU
			// readers might have seen its ID in the bitmap. If
			// this is the only free ID, we can ask the caller to
			// update a clone of the bitmap.
			should_retry = true;
		}

		if (new_id == MEMDB_BITMAP_OBJECTS) {
			// No available IDs.
			ret = bool_result_error(should_retry ? ERROR_RETRY
							     : ERROR_BUSY);
			goto out;
		}
	}

	// Update the bitmap and the entry table.
	assert((new_id != old_id) && (new_id < MEMDB_BITMAP_OBJECTS));
	for (index_t i = start_index; i <= end_index; i++) {
		bitmap_atomic_insert(bitmap->bitmap, i * MEMDB_BITMAP_ID_BITS,
				     MEMDB_BITMAP_ID_BITS, new_id,
				     memory_order_relaxed);
	}

	atomic_store_release(&bitmap->objects[new_id], new_entry);
	bitmap->counts[new_id] =
		(memdb_bitmap_count_t)(bitmap->counts[new_id] + changed_bits);
	bitmap->counts[old_id] =
		(memdb_bitmap_count_t)(bitmap->counts[old_id] - changed_bits);

	// If the new ID's count is now equal to the total number of entries,
	// the bitmap has become contiguous and can be pruned.
	// TODO: We could also do this by splatting the ID and comparing it
	// to the whole bitmap; it would be a bit slower, but then we wouldn't
	// need the counts and could save some more space.
	ret = bool_result_ok(bitmap->counts[new_id] == MEMDB_NUM_ENTRIES);

out:
	return ret;
}

static error_t
memdb_update_table_entry(paddr_t start, paddr_t end, memdb_entry_t old_entry,
			 memdb_entry_t new_entry, memdb_level_table_t *table,
			 index_t entry_bits, index_t entry_index)
	REQUIRE_LOCK(memdb_lock);

static bool
memdb_update_table_check_contig(index_t start_index, index_t end_index,
				memdb_entry_t		   new_entry,
				const memdb_level_table_t *table)
{
	// Determine whether new_entry is now completely filling the table. We
	// need to check the start and end slots because the update might have
	// put next-level entries there, but anything between them is
	// necessarily already equal to new_entry so we can skip those.
	bool is_contig = true;

	for (index_t i = 0; is_contig && (i <= start_index); i++) {
		const memdb_entry_t cur_entry =
			atomic_load_consume(&table->entries[i]);
		if (!memdb_entry_is_equal(cur_entry, new_entry)) {
			is_contig = false;
		}
	}

	for (index_t i = end_index; is_contig && (i < MEMDB_NUM_ENTRIES); i++) {
		const memdb_entry_t cur_entry =
			atomic_load_consume(&table->entries[i]);
		if (!memdb_entry_is_equal(cur_entry, new_entry)) {
			is_contig = false;
		}
	}

	return is_contig;
}

// Returns true if the table's entries are all identical, so it can be collapsed
// into a single entry.
static bool_result_t
memdb_update_table(paddr_t start, paddr_t end, memdb_entry_t old_entry,
		   memdb_entry_t new_entry, memdb_level_table_t *table,
		   index_t entry_bits) REQUIRE_LOCK(memdb_lock)
{
	assert((entry_bits <= MEMDB_ROOT_ENTRY_BITS) &&
	       ((start >> (entry_bits + MEMDB_BITS_PER_LEVEL)) ==
		(end >> (entry_bits + MEMDB_BITS_PER_LEVEL))));

	// Work on one entry at a time. If a failure occurs, we start rolling
	// back the changes.
	const index_t start_index = memdb_entry_index(start, entry_bits);
	const index_t end_index	  = memdb_entry_index(end, entry_bits);
	paddr_t	      table_start = start &
			      ~util_mask(entry_bits + MEMDB_BITS_PER_LEVEL);

	error_t err = OK;
	index_t update_index;
	for (update_index = start_index; update_index <= end_index;
	     update_index++) {
		paddr_t entry_start =
			util_max(start, table_start + ((paddr_t)update_index *
						       util_bit(entry_bits)));
		paddr_t entry_end = util_min(
			end, table_start + ((((paddr_t)update_index + 1U) *
					     util_bit(entry_bits)) -
					    1U));
		err = memdb_update_table_entry(entry_start, entry_end,
					       old_entry, new_entry, table,
					       entry_bits, update_index);
		if (err != OK) {
			break;
		}
	}

	if (err != OK) {
		index_t rollback_index;
		for (rollback_index = start_index;
		     rollback_index < update_index; rollback_index++) {
			paddr_t entry_start = util_max(
				start, table_start + ((paddr_t)rollback_index *
						      util_bit(entry_bits)));
			paddr_t entry_end = util_min(
				end,
				table_start + ((((paddr_t)rollback_index + 1U) *
						util_bit(entry_bits)) -
					       1U));
			error_t rollback_err = memdb_update_table_entry(
				entry_start, entry_end, new_entry, old_entry,
				table, entry_bits, rollback_index);
			if (rollback_err != OK) {
				panic("memdb_update_table: rollback failure");
			}
		}
	}

	bool_result_t ret;
	if (err == OK) {
		ret = bool_result_ok(memdb_update_table_check_contig(
			start_index, end_index, new_entry, table));
	} else {
		ret = bool_result_error(err);
	}
	return ret;
}

static error_t
memdb_update_table_entry_level_table(
	paddr_t start, paddr_t end, memdb_entry_t old_entry,
	memdb_entry_t new_entry, memdb_level_table_t *table, index_t entry_bits,
	memdb_entry_t cur_entry, index_t entry_index) REQUIRE_LOCK(memdb_lock)
{
	uintptr_t	     cur_ptr	= memdb_entry_get_entry_ptr(&cur_entry);
	memdb_level_table_t *next_table = (memdb_level_table_t *)cur_ptr;
	bool_result_t	     is_contig_ret =
		memdb_update_table(start, end, old_entry, new_entry, next_table,
				   entry_bits - MEMDB_BITS_PER_LEVEL);
	error_t err = is_contig_ret.e;

	if ((err == OK) && is_contig_ret.r) {
		// Next level has become contiguous and is no longer
		// needed. Replace the entry with the new entry and
		// release the table.
		atomic_store_release(&table->entries[entry_index], new_entry);
		memdb_release_level_table(next_table);
	}

	return err;
}

static error_t
memdb_update_table_entry_level_bitmap(
	paddr_t start, paddr_t end, memdb_entry_t old_entry,
	memdb_entry_t new_entry, memdb_level_table_t *table, index_t entry_bits,
	memdb_entry_t cur_entry, index_t entry_index) REQUIRE_LOCK(memdb_lock)
{
	uintptr_t	      cur_ptr = memdb_entry_get_entry_ptr(&cur_entry);
	memdb_level_bitmap_t *next_bitmap   = (memdb_level_bitmap_t *)cur_ptr;
	bool_result_t	      is_contig_ret = memdb_update_bitmap(
		start, end, old_entry, new_entry, next_bitmap,
		entry_bits - MEMDB_BITS_PER_LEVEL);
	error_t err = is_contig_ret.e;

	if ((err == ERROR_BUSY) || (err == ERROR_ARGUMENT_ALIGNMENT)) {
		// Requested update is not possible with a bitmap. We
		// must convert it to a table and try again.
		memdb_level_table_ptr_result_t table_ret =
			memdb_convert_bitmap(next_bitmap);
		err = table_ret.e;
		if (err == OK) {
			is_contig_ret = memdb_update_table(
				start, end, old_entry, new_entry, table_ret.r,
				entry_bits - MEMDB_BITS_PER_LEVEL);
			if ((err == OK) && !is_contig_ret.r) {
				memdb_entry_t table_entry =
					memdb_entry_default();
				memdb_entry_set_entry_type(
					&table_entry, MEMDB_TYPE_LEVEL_TABLE);
				memdb_entry_set_entry_ptr(
					&table_entry, (uintptr_t)table_ret.r);
				atomic_store_release(
					&table->entries[entry_index],
					table_entry);
				memdb_release_level_bitmap(next_bitmap);
			} else {
				// Update failed, or succeeded and made
				// the table contiguous. The table is
				// no longer needed.
				memdb_release_level_table(table_ret.r);
			}
		}
	} else if (err == ERROR_RETRY) {
		// Requested update needs to be done on a copy of the bitmap
		// to avoid recycling object IDs.
		memdb_level_bitmap_ptr_result_t new_bitmap_ret =
			memdb_duplicate_bitmap(next_bitmap);
		err = new_bitmap_ret.e;
		if (err == OK) {
			is_contig_ret = memdb_update_bitmap(
				start, end, old_entry, new_entry,
				new_bitmap_ret.r,
				entry_bits - MEMDB_BITS_PER_LEVEL);
			if ((err == OK) && !is_contig_ret.r) {
				memdb_entry_t bitmap_entry =
					memdb_entry_default();
				memdb_entry_set_entry_type(
					&bitmap_entry, MEMDB_TYPE_LEVEL_BITMAP);
				memdb_entry_set_entry_ptr(
					&bitmap_entry,
					(uintptr_t)new_bitmap_ret.r);
				atomic_store_release(
					&table->entries[entry_index],
					bitmap_entry);
				memdb_release_level_bitmap(next_bitmap);
			} else {
				// Update failed, or succeeded and made
				// the table contiguous. The new bitmap is
				// no longer needed.
				memdb_release_level_bitmap(new_bitmap_ret.r);
			}
		}
	} else {
		// No special action needed for other errors.
	}

	if ((err == OK) && is_contig_ret.r) {
		// Next level has become contiguous and is no longer
		// needed. Replace the entry with the new entry and
		// release the bitmap.
		atomic_store_release(&table->entries[entry_index], new_entry);
		memdb_release_level_bitmap(next_bitmap);
	}

	return err;
}

static error_t
memdb_update_table_entry_split_bitmap(paddr_t start, paddr_t end,
				      memdb_entry_t	   old_entry,
				      memdb_entry_t	   new_entry,
				      memdb_level_table_t *table,
				      index_t entry_bits, index_t entry_index)
	REQUIRE_LOCK(memdb_lock)
{
	memdb_level_bitmap_ptr_result_t bitmap_ret =
		memdb_create_bitmap(old_entry);
	error_t err = bitmap_ret.e;

	if (compiler_expected(err == OK)) {
		bool_result_t is_contig_r = memdb_update_bitmap(
			start, end, old_entry, new_entry, bitmap_ret.r,
			entry_bits - MEMDB_BITS_PER_LEVEL);
		err = is_contig_r.e;

		if (compiler_expected(err == OK)) {
			assert(!is_contig_r.r);
			memdb_entry_t bitmap_entry = memdb_entry_default();
			memdb_entry_set_entry_type(&bitmap_entry,
						   MEMDB_TYPE_LEVEL_BITMAP);
			memdb_entry_set_entry_ptr(&bitmap_entry,
						  (uintptr_t)bitmap_ret.r);
			atomic_store_release(&table->entries[entry_index],
					     bitmap_entry);
		} else {
			// Update failed. The bitmap is no longer needed.
			memdb_release_level_bitmap(bitmap_ret.r);
		}
	}

	return err;
}

static error_t
memdb_update_table_entry_split_table(paddr_t start, paddr_t end,
				     memdb_entry_t	  old_entry,
				     memdb_entry_t	  new_entry,
				     memdb_level_table_t *table,
				     index_t entry_bits, index_t entry_index)
	REQUIRE_LOCK(memdb_lock)
{
	memdb_level_table_ptr_result_t table_ret =
		memdb_create_table(old_entry);
	error_t err = table_ret.e;

	if (compiler_expected(err == OK)) {
		bool_result_t is_contig_r = memdb_update_table(
			start, end, old_entry, new_entry, table_ret.r,
			entry_bits - MEMDB_BITS_PER_LEVEL);
		err = is_contig_r.e;

		if (compiler_expected(err == OK)) {
			assert(!is_contig_r.r);
			memdb_entry_t table_entry = memdb_entry_default();
			memdb_entry_set_entry_type(&table_entry,
						   MEMDB_TYPE_LEVEL_TABLE);
			memdb_entry_set_entry_ptr(&table_entry,
						  (uintptr_t)table_ret.r);
			atomic_store_release(&table->entries[entry_index],
					     table_entry);
		} else {
			// Update failed. The table is no longer needed.
			memdb_release_level_table(table_ret.r);
		}
	}

	return err;
}

static error_t
memdb_update_table_entry(paddr_t start, paddr_t end, memdb_entry_t old_entry,
			 memdb_entry_t new_entry, memdb_level_table_t *table,
			 index_t entry_bits, index_t entry_index)
	REQUIRE_LOCK(memdb_lock)
{
	error_t err;

	assert((entry_bits <= MEMDB_ROOT_ENTRY_BITS) &&
	       ((start >> entry_bits) == (end >> entry_bits)));

	const memdb_entry_t cur_entry =
		atomic_load_consume(&table->entries[entry_index]);
	memdb_type_t cur_type = memdb_entry_get_entry_type(&cur_entry);

	if (cur_type == MEMDB_TYPE_LEVEL_TABLE) {
		err = memdb_update_table_entry_level_table(
			start, end, old_entry, new_entry, table, entry_bits,
			cur_entry, entry_index);

	} else if (cur_type == MEMDB_TYPE_LEVEL_BITMAP) {
		err = memdb_update_table_entry_level_bitmap(
			start, end, old_entry, new_entry, table, entry_bits,
			cur_entry, entry_index);

	} else if (!memdb_entry_is_equal(cur_entry, old_entry)) {
		// The existing entry must be equal to the specified old entry.
		err = ERROR_MEMDB_NOT_OWNER;

	} else if (util_is_p2aligned(start, entry_bits) &&
		   util_is_p2aligned(end + 1U, entry_bits)) {
		// If the existing entry's whole range is covered; replace it.
		atomic_store_release(&table->entries[entry_index], new_entry);
		err = OK;

	} else if (entry_bits <= MEMDB_MIN_BITS) {
		// We can't create any deeper levels, so the alignment check
		// failure is fatal.
		err = ERROR_ARGUMENT_ALIGNMENT;

	} else if ((entry_bits == (MEMDB_PAGE_BITS + MEMDB_BITS_PER_LEVEL)) &&
		   util_is_p2aligned(start, MEMDB_PAGE_BITS) &&
		   util_is_p2aligned(end + 1U, MEMDB_PAGE_BITS)) {
		// If the next level entries are page sized and the range is
		// page aligned, we should create a next-level bitmap.
		err = memdb_update_table_entry_split_bitmap(
			start, end, old_entry, new_entry, table, entry_bits,
			entry_index);
	} else {
		// Create a next-level table.
		err = memdb_update_table_entry_split_table(start, end,
							   old_entry, new_entry,
							   table, entry_bits,
							   entry_index);
	}

	return err;
}

error_t
memdb_insert(partition_t *partition, paddr_t start_addr, paddr_t end_addr,
	     uintptr_t object, memdb_type_t obj_type)
{
	return memdb_update(partition, start_addr, end_addr, object, obj_type,
			    0U, MEMDB_TYPE_NOTYPE);
}

error_t
memdb_update(partition_t *partition, paddr_t start_addr, paddr_t end_addr,
	     uintptr_t object, memdb_type_t obj_type, uintptr_t prev_object,
	     memdb_type_t prev_type)
{
	assert(partition == partition_get_private());

	error_t err = memdb_range_check(start_addr, end_addr);
	if (err != OK) {
		LOG(ERROR, WARN,
		    "memdb: range invalid for update: {:#x} .. {:#x}: {:d}",
		    start_addr, end_addr, (register_t)err);
		goto out;
	}

	const memdb_entry_t new_entry =
		memdb_entry_for_object(object, obj_type);
	const memdb_entry_t old_entry =
		memdb_entry_for_object(prev_object, prev_type);
	spinlock_acquire(&memdb_lock);
	err = memdb_update_table(start_addr, end_addr, old_entry, new_entry,
				 &memdb_root, MEMDB_ROOT_ENTRY_BITS)
		      .e;
	spinlock_release(&memdb_lock);

	if (err == OK) {
		TRACE(MEMDB, INFO,
		      "memdb_update: {:#x}..{:#x} - {:#x} -> {:#x}", start_addr,
		      end_addr, memdb_entry_raw(old_entry),
		      memdb_entry_raw(new_entry));

#if defined(MEMDB_DEBUG)
		// Check that the range was added correctly
		bool cont = memdb_is_ownership_contiguous(start_addr, end_addr,
							  object, obj_type);
		if (!cont) {
			LOG(ERROR, INFO,
			    "<<< memdb_update BUG!! range {:#x}..{:#x} should be contiguous",
			    start_addr, end_addr);
			panic("BUG in memdb_update");
		}
#endif
	} else {
		TRACE(MEMDB, INFO,
		      "memdb: Error updating {:#x}..{:#x} - {:#x} -> {:#x}: {:d}",
		      start_addr, end_addr, memdb_entry_raw(old_entry),
		      memdb_entry_raw(new_entry), (register_t)err);
	}

out:
	return err;
}

static memdb_obj_type_result_t
memdb_lookup_bitmap(paddr_t addr, const memdb_level_bitmap_t *bitmap,
		    index_t entry_bits) REQUIRE_RCU_READ
{
	assert(entry_bits <= MEMDB_ROOT_ENTRY_BITS);
	const index_t index	= memdb_entry_index(addr, entry_bits);
	index_t	      object_id = (index_t)bitmap_atomic_extract(
		      bitmap->bitmap, index * MEMDB_BITMAP_ID_BITS,
		      MEMDB_BITMAP_ID_BITS, memory_order_relaxed);
	const memdb_entry_t entry =
		atomic_load_consume(&bitmap->objects[object_id]);

	memdb_obj_type_result_t ret;
	memdb_type_t		entry_type = memdb_entry_get_entry_type(&entry);
	uintptr_t		entry_ptr  = memdb_entry_get_entry_ptr(&entry);

	// Next level entries would duplicate entire branches of the tree, so
	// they shouldn't be present in a bitmap level.
	assert((entry_type != MEMDB_TYPE_LEVEL_TABLE) &&
	       (entry_type != MEMDB_TYPE_LEVEL_BITMAP));

	if (entry_type == MEMDB_TYPE_NOTYPE) {
		ret = memdb_obj_type_result_error(ERROR_MEMDB_EMPTY);
	} else {
		ret = memdb_obj_type_result_ok((memdb_obj_type_t){
			.object = entry_ptr,
			.type	= entry_type,
		});
	}

	return ret;
}

static memdb_obj_type_result_t
memdb_lookup_table(paddr_t addr, const memdb_level_table_t *table,
		   index_t entry_bits) REQUIRE_RCU_READ
{
	assert(entry_bits <= MEMDB_ROOT_ENTRY_BITS);
	const index_t	    index = memdb_entry_index(addr, entry_bits);
	const memdb_entry_t entry = atomic_load_consume(&table->entries[index]);

	memdb_obj_type_result_t ret;

	memdb_type_t entry_type = memdb_entry_get_entry_type(&entry);
	uintptr_t    entry_ptr	= memdb_entry_get_entry_ptr(&entry);
	if (entry_type == MEMDB_TYPE_NOTYPE) {
		ret = memdb_obj_type_result_error(ERROR_MEMDB_EMPTY);
	} else if (entry_type == MEMDB_TYPE_LEVEL_TABLE) {
		ret = memdb_lookup_table(addr, (memdb_level_table_t *)entry_ptr,
					 entry_bits - MEMDB_BITS_PER_LEVEL);
	} else if (entry_type == MEMDB_TYPE_LEVEL_BITMAP) {
		ret = memdb_lookup_bitmap(addr,
					  (memdb_level_bitmap_t *)entry_ptr,
					  entry_bits - MEMDB_BITS_PER_LEVEL);
	} else {
		ret = memdb_obj_type_result_ok((memdb_obj_type_t){
			.object = entry_ptr,
			.type	= entry_type,
		});
	}

	return ret;
}

memdb_obj_type_result_t
memdb_lookup(paddr_t addr)
{
	memdb_obj_type_result_t ret;

	if (addr >= util_bit(MEMDB_MAX_BITS)) {
		ret = memdb_obj_type_result_error(ERROR_ARGUMENT_INVALID);
	} else {
		ret = memdb_lookup_table(addr, &memdb_root,
					 MEMDB_ROOT_ENTRY_BITS);
	}

	return ret;
}

static bool
memdb_is_contig_entry(paddr_t start, paddr_t end, memdb_entry_t entry,
		      memdb_entry_t cur_entry, index_t entry_bits)
	REQUIRE_RCU_READ;

static bool
memdb_is_contig_bitmap(paddr_t start, paddr_t end, memdb_entry_t entry,
		       const memdb_level_bitmap_t *bitmap, index_t entry_bits)
	REQUIRE_RCU_READ
{
	bool is_contig = true;

	assert((entry_bits <= MEMDB_ROOT_ENTRY_BITS) &&
	       ((start >> (entry_bits + MEMDB_BITS_PER_LEVEL)) ==
		(end >> (entry_bits + MEMDB_BITS_PER_LEVEL))));

	index_t object_id;
	for (object_id = 0U; object_id < MEMDB_BITMAP_OBJECTS; object_id++) {
		if (memdb_entry_is_equal(
			    atomic_load_relaxed(&bitmap->objects[object_id]),
			    entry)) {
			break;
		}
	}

	// Order the object ID search before the bitmap reads (if it succeeded)
	// and anything that is conditional on the result of the contiguous
	// check (if it failed).
	atomic_thread_fence(memory_order_acquire);

	if (object_id == MEMDB_BITMAP_OBJECTS) {
		// The requested entry is not in this bitmap at all.
		is_contig = false;
		goto out;
	}

	const index_t start_index = memdb_entry_index(start, entry_bits);
	const index_t end_index	  = memdb_entry_index(end, entry_bits);

	// TODO: This could be optimised by using splat, xor and CLZ to find
	// contiguous ranges.
	for (index_t i = start_index; i <= end_index; i++) {
		const index_t cur_object_id = (index_t)bitmap_atomic_extract(
			bitmap->bitmap, i * MEMDB_BITMAP_ID_BITS,
			MEMDB_BITMAP_ID_BITS, memory_order_relaxed);

		if (cur_object_id != object_id) {
			is_contig = false;
			break;
		}
	}

out:
	return is_contig;
}

static bool
memdb_is_contig_table(paddr_t start, paddr_t end, memdb_entry_t entry,
		      const memdb_level_table_t *table, index_t entry_bits)
	REQUIRE_RCU_READ
{
	bool is_contig = true;

	assert((entry_bits <= MEMDB_ROOT_ENTRY_BITS) &&
	       ((start >> (entry_bits + MEMDB_BITS_PER_LEVEL)) ==
		(end >> (entry_bits + MEMDB_BITS_PER_LEVEL))));

	const index_t start_index = memdb_entry_index(start, entry_bits);
	const index_t end_index	  = memdb_entry_index(end, entry_bits);

	paddr_t entry_start = start;
	for (index_t i = start_index; i <= end_index; i++) {
		const memdb_entry_t cur_entry =
			atomic_load_consume(&table->entries[i]);
		paddr_t entry_end =
			util_min(end, entry_start | util_mask(entry_bits));

		is_contig = memdb_is_contig_entry(entry_start, entry_end, entry,
						  cur_entry, entry_bits);
		if (!is_contig) {
			break;
		}

		entry_start = entry_end + 1U;
	}

	return is_contig;
}

static bool
memdb_is_contig_entry(paddr_t start, paddr_t end, memdb_entry_t entry,
		      memdb_entry_t cur_entry, index_t entry_bits)
	REQUIRE_RCU_READ
{
	assert((entry_bits <= MEMDB_ROOT_ENTRY_BITS) &&
	       ((start >> entry_bits) == (end >> entry_bits)));

	bool is_contig;

	memdb_type_t cur_type = memdb_entry_get_entry_type(&cur_entry);
	uintptr_t    cur_ptr  = memdb_entry_get_entry_ptr(&cur_entry);

	if (cur_type == MEMDB_TYPE_LEVEL_TABLE) {
		is_contig = memdb_is_contig_table(
			start, end, entry, (memdb_level_table_t *)cur_ptr,
			entry_bits - MEMDB_BITS_PER_LEVEL);
	} else if (cur_type == MEMDB_TYPE_LEVEL_BITMAP) {
		is_contig = memdb_is_contig_bitmap(
			start, end, entry, (memdb_level_bitmap_t *)cur_ptr,
			entry_bits - MEMDB_BITS_PER_LEVEL);
	} else {
		is_contig = memdb_entry_is_equal(entry, cur_entry);
	}

	return is_contig;
}

bool
memdb_is_ownership_contiguous(paddr_t start_addr, paddr_t end_addr,
			      uintptr_t object, memdb_type_t obj_type)
{
	const memdb_entry_t entry = memdb_entry_for_object(object, obj_type);
	rcu_read_start();
	bool result = memdb_is_contig_table(start_addr, end_addr, entry,
					    &memdb_root, MEMDB_ROOT_ENTRY_BITS);
	rcu_read_finish();
	return result;
}

error_t
memdb_walk(uintptr_t object, memdb_type_t type, memdb_fnptr fn, void *arg)
{
	return memdb_range_walk(object, type, 0U, util_mask(MEMDB_MAX_BITS), fn,
				arg);
}

static size_result_t
memdb_walk_entry(memdb_entry_t entry, paddr_t start, paddr_t end,
		 const memdb_entry_t cur_entry, index_t entry_bits,
		 memdb_fnptr fn, void *arg, size_t pending_size)
	REQUIRE_RCU_READ;

static size_result_t
memdb_walk_table(memdb_entry_t entry, paddr_t start, paddr_t end,
		 const memdb_level_table_t *table, index_t entry_bits,
		 memdb_fnptr fn, void *arg, size_t pending_size)
	REQUIRE_RCU_READ
{
	size_result_t ret = size_result_ok(pending_size);

	assert((entry_bits <= MEMDB_ROOT_ENTRY_BITS) &&
	       ((start >> (entry_bits + MEMDB_BITS_PER_LEVEL)) ==
		(end >> (entry_bits + MEMDB_BITS_PER_LEVEL))));

	const index_t start_index = memdb_entry_index(start, entry_bits);
	const index_t end_index	  = memdb_entry_index(end, entry_bits);

	paddr_t entry_start = start;
	for (index_t i = start_index; i <= end_index; i++) {
		const memdb_entry_t cur_entry =
			atomic_load_consume(&table->entries[i]);
		paddr_t entry_end =
			util_min(end, entry_start | util_mask(entry_bits));

		ret = memdb_walk_entry(entry, entry_start, entry_end, cur_entry,
				       entry_bits, fn, arg, ret.r);
		if (ret.e != OK) {
			break;
		}

		entry_start = entry_end + 1U;
	}

	return ret;
}

static size_result_t
memdb_walk_bitmap(memdb_entry_t entry, paddr_t start, paddr_t end,
		  const memdb_level_bitmap_t *bitmap, index_t entry_bits,
		  memdb_fnptr fn, void *arg, size_t pending_size)
	REQUIRE_RCU_READ
{
	size_result_t ret;

	assert((entry_bits <= MEMDB_ROOT_ENTRY_BITS) &&
	       ((start >> (entry_bits + MEMDB_BITS_PER_LEVEL)) ==
		(end >> (entry_bits + MEMDB_BITS_PER_LEVEL))));

	index_t object_id;
	for (object_id = 0U; object_id < MEMDB_BITMAP_OBJECTS; object_id++) {
		if (memdb_entry_is_equal(
			    atomic_load_relaxed(&bitmap->objects[object_id]),
			    entry)) {
			break;
		}
	}

	// Order the object ID search before the bitmap reads (if it succeeded)
	// and the handler function (if it failed).
	atomic_thread_fence(memory_order_acquire);

	if (object_id == MEMDB_BITMAP_OBJECTS) {
		// The requested entry is not in this bitmap at all. Call the
		// handler function for the pending range if necessary.
		error_t err = OK;
		if (pending_size != 0U) {
			err = fn(start - pending_size, pending_size, arg);
		}
		ret = (size_result_t){ .e = err, .r = 0U };
		goto out;
	}
	ret = size_result_ok(pending_size);

	const index_t start_index = memdb_entry_index(start, entry_bits);
	const index_t end_index	  = memdb_entry_index(end, entry_bits);

	// TODO: This could be optimised by using splat, xor and CLZ to find
	// contiguous ranges.
	paddr_t entry_start = start;
	for (index_t i = start_index; i <= end_index; i++) {
		const index_t cur_object_id = (index_t)bitmap_atomic_extract(
			bitmap->bitmap, i * MEMDB_BITMAP_ID_BITS,
			MEMDB_BITMAP_ID_BITS, memory_order_relaxed);

		paddr_t entry_end =
			util_min(end, entry_start | util_mask(entry_bits));

		if (cur_object_id == object_id) {
			// Matching; increase the pending size
			ret.r += (entry_end - entry_start) + 1U;
		} else if (ret.r != 0U) {
			// Not matching, and pending size is nonzero; call the
			// handler function for the pending range
			error_t err = fn(entry_start - ret.r, ret.r, arg);
			ret	    = (size_result_t){ .e = err, .r = 0U };
			if (err != OK) {
				break;
			}
		} else {
			// Neither matching nor pending, nothing to do.
		}

		entry_start = entry_end + 1U;
	}

out:
	return ret;
}

static size_result_t
memdb_walk_entry(memdb_entry_t entry, paddr_t start, paddr_t end,
		 const memdb_entry_t cur_entry, index_t entry_bits,
		 memdb_fnptr fn, void *arg, size_t pending_size)
	REQUIRE_RCU_READ
{
	assert((entry_bits <= MEMDB_ROOT_ENTRY_BITS) &&
	       ((start >> entry_bits) == (end >> entry_bits)));

	size_result_t ret;

	memdb_type_t cur_type = memdb_entry_get_entry_type(&cur_entry);
	uintptr_t    cur_ptr  = memdb_entry_get_entry_ptr(&cur_entry);

	if (cur_type == MEMDB_TYPE_LEVEL_TABLE) {
		ret = memdb_walk_table(entry, start, end,
				       (memdb_level_table_t *)cur_ptr,
				       entry_bits - MEMDB_BITS_PER_LEVEL, fn,
				       arg, pending_size);
	} else if (cur_type == MEMDB_TYPE_LEVEL_BITMAP) {
		ret = memdb_walk_bitmap(entry, start, end,
					(memdb_level_bitmap_t *)cur_ptr,
					entry_bits - MEMDB_BITS_PER_LEVEL, fn,
					arg, pending_size);
	} else if (memdb_entry_is_equal(entry, cur_entry)) {
		// Matching; increase the pending size
		ret = size_result_ok(pending_size + (end - start) + 1U);
	} else if (pending_size != 0U) {
		error_t err = fn(start - pending_size, pending_size, arg);
		ret	    = (size_result_t){ .e = err, .r = 0U };
	} else {
		ret = size_result_ok(0U);
	}

	return ret;
}

error_t
memdb_range_walk(uintptr_t object, memdb_type_t obj_type, paddr_t start_addr,
		 paddr_t end_addr, memdb_fnptr fn, void *arg)
{
	error_t err;
	if (obj_type == MEMDB_TYPE_NOTYPE) {
		err = ERROR_ARGUMENT_INVALID;
		goto out;
	}

	const memdb_entry_t entry = memdb_entry_for_object(object, obj_type);

	// Truncate the range at the maximum address.
	paddr_t end = util_min(end_addr, util_mask(MEMDB_MAX_BITS));

	if (start_addr > end) {
		// The range contains no addresses, so there's nothing to do.
		err = OK;
		goto out;
	}

	rcu_read_start();
	size_result_t ret = memdb_walk_table(entry, start_addr, end,
					     &memdb_root, MEMDB_ROOT_ENTRY_BITS,
					     fn, arg, 0U);
	if ((ret.e == OK) && (ret.r != 0U)) {
		err = fn(end - ret.r + 1U, ret.r, arg);
	} else {
		err = ret.e;
	}
	rcu_read_finish();

out:
	return err;
}

error_t
memdb_bitmap_handle_partition_add_ram_range(partition_t *owner,
					    paddr_t phys_base, size_t size)
{
	partition_t *hyp_partition = partition_get_private();

	assert(size > 0U);
	assert(!util_add_overflows(phys_base, size - 1U));

	error_t err = memdb_insert(hyp_partition, phys_base,
				   phys_base + (size - 1U), (uintptr_t)owner,
				   MEMDB_TYPE_PARTITION);
	if (err != OK) {
		LOG(ERROR, WARN,
		    "memdb: Error adding ram {:#x}..{:#x} to partition {:x}, err = {:d}",
		    phys_base, phys_base + size - 1U, (register_t)owner,
		    (register_t)err);
	}

	return err;
}

error_t
memdb_bitmap_handle_partition_remove_ram_range(partition_t *owner,
					       paddr_t phys_base, size_t size)
{
	partition_t *hyp_partition = partition_get_private();

	assert(size > 0U);
	assert(!util_add_overflows(phys_base, size - 1U));

	error_t err = memdb_update(hyp_partition, phys_base,
				   phys_base + (size - 1U), 0U,
				   MEMDB_TYPE_NOTYPE, (uintptr_t)owner,
				   MEMDB_TYPE_PARTITION);
	if (err != OK) {
		LOG(ERROR, WARN,
		    "memdb: Error removing ram {:#x}..{:#x} from partition {:x}, err = {:d}",
		    phys_base, phys_base + size - 1U, (register_t)owner,
		    (register_t)err);
	}

	return err;
}

```

`hyp/mem/memdb_gpt/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

interface memdb
base_module hyp/mem/memdb
source memdb.c
events memdb.ev
types memdb.tc

```

`hyp/mem/memdb_gpt/memdb.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

module memdb_gpt

subscribe rcu_update[RCU_UPDATE_CLASS_MEMDB_RELEASE_LEVEL]
	handler memdb_deallocate_level(entry)

subscribe boot_cold_init()
	// Must run after pagetable init and hyp_aspace initialisation.
	priority 10

subscribe partition_add_ram_range(owner, phys_base, size)
	unwinder

subscribe partition_remove_ram_range(owner, phys_base, size)
	unwinder

```

`hyp/mem/memdb_gpt/memdb.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

define MEMDB_BITS_PER_ENTRY constant type index_t = 4;
define MEMDB_NUM_ENTRIES constant type index_t = (1 << MEMDB_BITS_PER_ENTRY);

// The GPT memdb starts all levels at a multiple of MEMDB_BITS_PER_ENTRY, so
// it has no alignment requirements.
define MEMDB_MIN_BITS constant type index_t = 0;
define MEMDB_MIN_SIZE constant size = (1 << MEMDB_MIN_BITS);

extend rcu_update_class enumeration {
	memdb_release_level;
};

define memdb_entry_info bitfield<64> {
	3:0	type enumeration memdb_type;
	11:4	shifts type count_t;
	63:12	guard uint64;
};

define memdb_entry structure {
	info	bitfield memdb_entry_info;
	next	uintptr;
};

// The aligned() directive here should be automatic.
// FIXME:
define memdb_level structure {
	level		array(MEMDB_NUM_ENTRIES) structure memdb_entry(atomic,
			aligned(sizeof(structure memdb_entry)));
	lock		structure spinlock;
	rcu_entry	structure rcu_entry(contained);
	allocator	pointer structure allocator;
};

// FIXME: remove aligned() when Issue #27 is fixed.
define memdb structure {
	root	structure memdb_entry(atomic,
		aligned(sizeof(structure memdb_entry)));
	lock	structure spinlock;
};

define memdb_op enumeration {
	INSERT = 0;
	UPDATE;
	ROLLBACK;
	LOOKUP;
	CONTIGUOUSNESS;
};

extend memdb_type enumeration {
	LEVEL;
	// FIXME:
	// Memory which may not be mapped, e.g. converted to secure memory
	PARTITION_NOMAP;
};

```

`hyp/mem/memdb_gpt/src/memdb.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// How the memory database works:
//
// - Finding the common level:
// We first calculate the common bits between the start and end address passed
// as arguments. To know which are the common bits in the address, we use
// 'shifts' so that we can do (addr >> shifts) and get the common bits.
// With these common bits we either create (in case of insertion) or search for
// the level where the paths of the start and end address separate, this level
// is what we call 'common level'.
// In an insertion, we will use the common bits and shifts to set 'guard' and
// 'guard shifts' to entries. We use guards so that we can skip levels where all
// their entries are empty except the entry that points to the next level. We
// will only (for now) use guards, when possible, between the root and the
// common level. In no other operations guards should be set.
//
// - Adding start and end address:
// Once we found the common level, then we use the start/end address to go down
// levels (or create them if needed) until we reach to the levels where all bits
// of the address have been covered.
//
// - Going down levels:
// As we go down the levels we will do it by jumping from one entry of a level
// to a next one in the next level. We need to check if the entry contains a
// guard, and if so we need to check if the guard matches to our address (addr
// >> guard_shifts) and act accordingly. In insertion, there are several corner
// cases we will need to take care of and do some adjustments if the guard
// matches or not, in the rest of the operations if a guard does not match with
// the address then we will return error as probably the address we are
// searching for is not in the database.
//
// - Synchronization:
// 1. Atomic operations: we will read and write entries atomically using
// atomic_load_explicit and atomic_store_explicit, mostly with
// memory_order_relaxed. We will only use memory_order_release when we create a
// level and update parent to point to the new level.
// 2. RCUs: we use rcu_read_start() and rcu_read_finish() in lookups and when
// checking the contiguousness of an address range. We use rcu_enqueue() when we
// want to remove a level from the database, we will trigger the RCU update
// event handler that takes care of deallocating a level.
// 3. Spinlocks: We will only use spinlocks in insertion and updates. Above the
// common level, we will initially always be holding 2 locks. We will be going
// down levels and checking if the current level needs the lock. If the current
// level needs the locks (because its values are going to be modified or it
// might be collapsed since all entries except the current one point to the same
// object we are going to insert in the database) then we will keep the lock of
// the previous level, the current one and all consecutive levels. If the
// current level, does NOT need the lock then we will remove the lock to
// previous level and continue to hold the current one for now.

#include <assert.h>
#include <hyptypes.h>
#include <limits.h>

#include <hypcontainers.h>

#include <allocator.h>
#include <atomic.h>
#include <bootmem.h>
#include <compiler.h>
#include <log.h>
#include <memdb.h>
#include <panic.h>
#include <partition.h>
#include <rcu.h>
#include <spinlock.h>
#include <trace.h>
#include <trace_helpers.h>
#include <util.h>

#include "event_handlers.h"

#define MEMDB_BITS_PER_ENTRY_MASK util_mask(MEMDB_BITS_PER_ENTRY)
#define ADDR_SIZE		  (sizeof(paddr_t) * (size_t)CHAR_BIT)
// levels + 1 for root
#define MAX_LEVELS (ADDR_SIZE / MEMDB_BITS_PER_ENTRY) + 1U

#if (defined(VERBOSE) && VERBOSE) || defined(UNIT_TESTS)
#define MEMDB_DEBUG 1
#endif

static memdb_t memdb;

extern const char image_phys_start;
extern const char image_phys_last;

static const paddr_t phys_start = (paddr_t)&image_phys_start;
static const paddr_t phys_end	= (paddr_t)&image_phys_last;

typedef struct start_path_s {
	memdb_level_t *levels[MAX_LEVELS];
	index_t	       indexes[MAX_LEVELS];
	count_t	       count;
} start_path_t;

typedef struct locked_levels_s {
	spinlock_t	      *locks[MAX_LEVELS];
	_Atomic memdb_entry_t *entries[MAX_LEVELS];
	count_t		       count;
	uint8_t		       pad_end_[4];
} locked_levels_t;

static count_t
lowest_unmatching_bits(paddr_t start_addr, paddr_t end_addr)
{
	assert(start_addr != end_addr);

	paddr_t ret = ADDR_SIZE - compiler_clz(start_addr ^ end_addr);

	assert(ret <= ADDR_SIZE);

	return (count_t)ret;
}

static count_t
calculate_common_bits(paddr_t start_addr, paddr_t end_addr)
{
	count_t shifts;

	shifts = lowest_unmatching_bits(start_addr, end_addr);
	shifts = util_balign_up(shifts, MEMDB_BITS_PER_ENTRY);

	return shifts;
}

static index_t
get_next_index(paddr_t addr, count_t *shifts)
{
	assert(*shifts != 0U);
	assert(*shifts <= ADDR_SIZE);

	*shifts -= MEMDB_BITS_PER_ENTRY;

	return ((count_t)(addr >> *shifts) &
		(count_t)MEMDB_BITS_PER_ENTRY_MASK);
}

static void
atomic_entry_write(_Atomic memdb_entry_t *entry_dst, memory_order order,
		   paddr_t guard, count_t guard_shifts, memdb_type_t type,
		   uintptr_t object)
{
	memdb_entry_t entry_src = { { 0 }, (uintptr_t)0U };

	memdb_entry_info_set_guard(&entry_src.info, guard);
	memdb_entry_info_set_shifts(&entry_src.info, guard_shifts);
	memdb_entry_info_set_type(&entry_src.info, type);
	entry_src.next = object;
	atomic_store_explicit(entry_dst, entry_src, order);
}

static memdb_entry_t
atomic_entry_read(_Atomic memdb_entry_t *entry_src, paddr_t *guard,
		  count_t *guard_shifts, memdb_type_t *type, uintptr_t *next)
{
	memdb_entry_t entry_dst = atomic_load_consume(entry_src);

	*guard	      = memdb_entry_info_get_guard(&entry_dst.info);
	*guard_shifts = memdb_entry_info_get_shifts(&entry_dst.info);
	*type	      = memdb_entry_info_get_type(&entry_dst.info);
	*next	      = entry_dst.next;

	return entry_dst;
}

static void
init_level(memdb_level_t *level, allocator_t *allocator, memdb_type_t type,
	   uintptr_t obj)
{
	spinlock_init(&level->lock);
	level->allocator = allocator;

	for (index_t i = 0; i < MEMDB_NUM_ENTRIES; i++) {
		// Guard shifts of 64 (ADDR_SIZE) means there is no guard.
		atomic_entry_write(&level->level[i], memory_order_relaxed,
				   (paddr_t)MEMDB_TYPE_LEVEL,
				   (count_t)ADDR_SIZE, type, obj);
	}
}

static memdb_level_ptr_result_t
create_level(allocator_t *allocator, memdb_type_t type, uintptr_t obj)
{
	memdb_level_ptr_result_t ret;

	void_ptr_result_t res = allocator_allocate_object(
		allocator, sizeof(memdb_level_t), alignof(memdb_level_t));

	if (res.e != OK) {
		LOG(ERROR, WARN, "memdb allocate err: {:d}", (register_t)res.e);
		ret = memdb_level_ptr_result_error(res.e);
		goto out;
	}

	memdb_level_t *level = res.r;
	init_level(level, allocator, type, obj);

	ret.e = OK;
	ret.r = level;

out:
	return ret;
}

// Check if the level entries point to the same object. If we pass an index
// different from MEMDB_NUM_ENTRIES, it will check all entries except that index
static bool
are_all_entries_same(memdb_level_t *level, uintptr_t object, index_t index,
		     memdb_type_t type, index_t start, index_t end)
{
	bool ret = true;

	for (index_t i = start; i < end; i++) {
		memdb_entry_t level_entry = atomic_load_explicit(
			&level->level[i], memory_order_relaxed);
		if ((i != index) &&
		    ((memdb_entry_info_get_type(&level_entry.info) != type) ||
		     (level_entry.next != object))) {
			ret = false;
			break;
		}
	}

	return ret;
}

rcu_update_status_t
memdb_deallocate_level(rcu_entry_t *entry)
{
	rcu_update_status_t ret = rcu_update_status_default();
	error_t		    err;

	memdb_level_t *level = memdb_level_container_of_rcu_entry(entry);

	allocator_t *allocator = level->allocator;

	err = allocator_deallocate_object(allocator, level,
					  sizeof(memdb_level_t));
	if (err != OK) {
		panic("Error deallocating level");
	}

	return ret;
}

// Unlock levels, but check before if all entries of level are the same. If so,
// update parent with pointer to object, unlock level and deallocate level using
// RCU.
//
// The entry parent of the level will always be in the previous index.
// Lock level[x], parent entry[x -1]
static void
unlock_levels(locked_levels_t *locked_levels) LOCK_IMPL
{
	bool	     optimize = true;
	bool	     res      = false;
	memdb_type_t type;
	paddr_t	     guard;
	count_t	     guard_shifts;
	uintptr_t    next;

	assert(locked_levels->count != 0U);

	for (count_t i = (locked_levels->count - 1U); i > 0U; i--) {
		memdb_entry_t entry = atomic_load_explicit(
			locked_levels->entries[i - 1U], memory_order_relaxed);
		memdb_level_t *level = (memdb_level_t *)entry.next;

#if defined(MEMDB_DEBUG)
		// Check that levels are not added twice
		memdb_entry_t curr_entry = atomic_load_explicit(
			locked_levels->entries[i], memory_order_relaxed);
		assert(curr_entry.next != entry.next);

		// Check that all consecutive levels after the first one in the
		// array have been added
		bool cont_level = false;

		for (count_t k = 0; k < MEMDB_NUM_ENTRIES; k++) {
			memdb_entry_t cmp_entry = atomic_load_explicit(
				&level->level[k], memory_order_relaxed);
			if (cmp_entry.next == curr_entry.next) {
				cont_level = true;
				break;
			}
		}
		assert(cont_level);
#endif
		if (optimize) {
			(void)atomic_entry_read(&level->level[0], &guard,
						&guard_shifts, &type, &next);

			res = are_all_entries_same(level, next,
						   MEMDB_NUM_ENTRIES, type, 0,
						   MEMDB_NUM_ENTRIES);
			if (res) {
				// Update parent and deallocate level.
				atomic_entry_write(
					locked_levels->entries[i - 1U],
					memory_order_relaxed, guard,
					guard_shifts, type, next);

				spinlock_release(locked_levels->locks[i]);

				rcu_enqueue(
					&level->rcu_entry,
					RCU_UPDATE_CLASS_MEMDB_RELEASE_LEVEL);

				continue;
			} else {
				optimize = false;
			}
		}

		spinlock_release(locked_levels->locks[i]);
	}

	if (locked_levels->locks[0] != NULL) {
		spinlock_release(locked_levels->locks[0]);
	}
}

static paddr_t
calculate_address(paddr_t addr, count_t shifts, index_t index)
{
	paddr_t result = util_p2align_down(
		addr, ((paddr_t)shifts + MEMDB_BITS_PER_ENTRY));

	assert(index < util_bit(MEMDB_BITS_PER_ENTRY));

	result |= (uint64_t)index << shifts;
	result |= util_mask(shifts);

	return result;
}

static error_t
fill_level_entries(memdb_level_t *level, uintptr_t object, memdb_type_t type,
		   uintptr_t prev_object, memdb_type_t prev_type,
		   index_t start_index, index_t end_index, paddr_t addr,
		   paddr_t *last_success_addr, count_t shifts, memdb_op_t op)
{
	error_t ret	     = OK;
	index_t failed_index = 0;

	if (start_index == end_index) {
		goto out;
	}

	for (index_t i = start_index; i < end_index; i++) {
		memdb_entry_t level_entry = atomic_load_explicit(
			&level->level[i], memory_order_relaxed);

		if ((memdb_entry_info_get_type(&level_entry.info) !=
		     prev_type) ||
		    (level_entry.next != prev_object)) {
			failed_index = i;
			ret	     = ERROR_MEMDB_NOT_OWNER;
			goto end_function;
		}
		atomic_entry_write(&level->level[i], memory_order_relaxed, 0,
				   (count_t)ADDR_SIZE, type, object);
	}

end_function:
	if (ret != OK) {
		if (failed_index > start_index) {
			*last_success_addr = calculate_address(
				addr, shifts, failed_index - 1U);
		}
	} else {
		if ((op != MEMDB_OP_ROLLBACK) && (start_index != end_index)) {
			*last_success_addr =
				calculate_address(addr, shifts, end_index - 1U);
		}
	}
out:
	return ret;
}

static void
lock_level(memdb_level_t *level, index_t index, locked_levels_t *locked_levels)
	LOCK_IMPL
{
	assert(locked_levels->count < MAX_LEVELS);

	spinlock_acquire(&level->lock);
	locked_levels->locks[locked_levels->count]   = &level->lock;
	locked_levels->entries[locked_levels->count] = &level->level[index];
	locked_levels->count++;
}

static error_t
check_guard(count_t guard_shifts, paddr_t guard, paddr_t addr, count_t *shifts)
{
	error_t ret = OK;

	if (guard_shifts != ADDR_SIZE) {
		if (guard != (addr >> guard_shifts)) {
			ret = ERROR_ADDR_INVALID;
		} else {
			if (shifts != NULL) {
				*shifts = guard_shifts;
			}
		}
	}

	return ret;
}

static error_t
create_n_levels(allocator_t *allocator, memdb_level_t **level, bool start,
		count_t *shifts, index_t *index, paddr_t addr, uintptr_t object,
		memdb_type_t type, uintptr_t prev_object,
		memdb_type_t prev_type, start_path_t *start_path,
		locked_levels_t *locked_levels, memdb_level_t *first_level,
		memdb_level_t **common_level, count_t *common_level_shifts,
		memdb_op_t op, paddr_t *last_success_addr, count_t limit)
{
	error_t	     ret	= OK;
	paddr_t	     comparison = 0;
	paddr_t	     level_guard;
	count_t	     level_guard_shifts;
	memdb_type_t level_type;
	uintptr_t    level_next;

	if (!start) {
		// Compare remaining end address bits with all ones
		comparison = util_mask(*shifts);
	}

	(void)atomic_entry_read(&(*level)->level[*index], &level_guard,
				&level_guard_shifts, &level_type, &level_next);

	// Create levels and update parent entry to point to new level.
	while ((*shifts != limit) &&
	       ((util_mask(*shifts) & addr) != comparison)) {
		if ((op != MEMDB_OP_ROLLBACK) && (*level != first_level)) {
			lock_level(*level, *index, locked_levels);
		}

		memdb_level_ptr_result_t res =
			create_level(allocator, prev_type, prev_object);
		if (res.e != OK) {
			ret = res.e;
			goto error;
		}

		memdb_level_t *next_level = res.r;

		level_guard	   = 0;
		level_guard_shifts = (count_t)ADDR_SIZE;
		level_type	   = MEMDB_TYPE_LEVEL;
		level_next	   = (uintptr_t)next_level;

		atomic_entry_write(&(*level)->level[*index],
				   memory_order_release, level_guard,
				   level_guard_shifts, level_type, level_next);
		if (start) {
			count_t aux_shifts = *shifts + MEMDB_BITS_PER_ENTRY;

			if ((op == MEMDB_OP_ROLLBACK) &&
			    (aux_shifts != ADDR_SIZE) &&
			    ((*last_success_addr >> aux_shifts) ==
			     (addr >> aux_shifts))) {
				*common_level	     = *level;
				*common_level_shifts = *shifts;
			}

			if ((start_path->count == 0U) ||
			    (start_path->levels[start_path->count - 1U] !=
			     *level)) {
				start_path->levels[start_path->count]  = *level;
				start_path->indexes[start_path->count] = *index;
				start_path->count++;
			}
		}

		if ((!start) && (*level != first_level)) {
			ret = fill_level_entries(
				*level, object, type, prev_object, prev_type, 0,
				*index, addr, last_success_addr, *shifts, op);
			if (ret != OK) {
				// We add it to list of levels so that it can
				// get optimized. Dummy index.
				lock_level(next_level, 0, locked_levels);
				goto error;
			}
		}

		*index = get_next_index(addr, shifts);

		if (!start) {
			comparison = util_mask(*shifts);
		}

		*level = next_level;
	}

error:
	return ret;
}

static error_t
go_down_levels(memdb_level_t *first_level, memdb_level_t **level, paddr_t addr,
	       index_t *index, count_t *shifts, memdb_op_t op, bool start,
	       start_path_t *start_path, locked_levels_t *locked_levels,
	       uintptr_t object, memdb_type_t type, uintptr_t prev_object,
	       memdb_type_t prev_type, memdb_level_t **common_level,
	       count_t *common_level_shifts, paddr_t *last_success_addr,
	       allocator_t *allocator)
{
	paddr_t	     level_guard;
	count_t	     level_guard_shifts;
	memdb_type_t level_type;
	uintptr_t    level_next;
	error_t	     ret = OK;

	(void)atomic_entry_read(&(*level)->level[*index], &level_guard,
				&level_guard_shifts, &level_type, &level_next);

	// We need to go down the levels until we find an empty entry or we run
	// out of remaining bits. In the former case, return error since the
	// address already has an owner.
	while ((level_type == MEMDB_TYPE_LEVEL) && (*shifts != 0U)) {
		if (start) {
			count_t level_shifts = *shifts + MEMDB_BITS_PER_ENTRY;

			if ((start) && (op == MEMDB_OP_ROLLBACK) &&
			    (level_shifts != ADDR_SIZE) &&
			    ((*last_success_addr >> level_shifts) ==
			     (addr >> level_shifts))) {
				*common_level	     = *level;
				*common_level_shifts = *shifts;
			}

			if ((start_path->count == 0U) ||
			    (start_path->levels[start_path->count - 1U] !=
			     *level)) {
				start_path->levels[start_path->count]  = *level;
				start_path->indexes[start_path->count] = *index;
				start_path->count++;
			}
		}

		if ((op == MEMDB_OP_INSERT) &&
		    (level_guard_shifts != ADDR_SIZE)) {
			memdb_level_t *last_level = (memdb_level_t *)level_next;
			count_t	       last_shifts;

			ret = check_guard(level_guard_shifts, level_guard, addr,
					  NULL);
			if (ret == OK) {
				// Guard matches: remove guard and create
				// intermediate levels covering the guard bits.

				last_shifts		 = level_guard_shifts;
				level_guard		 = 0;
				level_guard_shifts	 = (count_t)ADDR_SIZE;
				memdb_level_t *level_aux = *level;

				ret = create_n_levels(
					allocator, level, start, shifts, index,
					addr, object, type, prev_object,
					prev_type, start_path, locked_levels,
					first_level, common_level,
					common_level_shifts, op,
					last_success_addr, last_shifts);
				if (ret != OK) {
					goto end_function;
				}

				atomic_entry_write(&(*level)->level[*index],
						   memory_order_release,
						   level_guard,
						   level_guard_shifts,
						   level_type,
						   (uintptr_t)last_level);

				if (start && (*level != level_aux)) {
					count_t level_shifts =
						*shifts + MEMDB_BITS_PER_ENTRY;

					if ((start) &&
					    (op == MEMDB_OP_ROLLBACK) &&
					    (level_shifts !=
					     (count_t)ADDR_SIZE) &&
					    ((*last_success_addr >>
					      level_shifts) ==
					     (addr >> level_shifts))) {
						*common_level	     = *level;
						*common_level_shifts = *shifts;
					}
					start_path->levels[start_path->count] =
						*level;
					start_path->indexes[start_path->count] =
						*index;
					start_path->count++;
				}

			} else {
				// Guard does not match: create intermediate
				// levels that cover only matching bits.
				// There are always some matching bits, at
				// least the one of the entry index.

				count_t aux_shifts = 0;
				paddr_t tmp_cmn	   = addr >> level_guard_shifts;

				// We update guard to common bits between them.
				aux_shifts = calculate_common_bits(level_guard,
								   tmp_cmn);

				if ((aux_shifts + level_guard_shifts) !=
				    (count_t)ADDR_SIZE) {
					index_t new_index;

					last_shifts = level_guard_shifts +
						      aux_shifts -
						      MEMDB_BITS_PER_ENTRY;
					memdb_level_t *level_aux = *level;

					ret = create_n_levels(
						allocator, level, start, shifts,
						index, addr, object, type,
						prev_object, prev_type,
						start_path, locked_levels,
						first_level, common_level,
						common_level_shifts, op,
						last_success_addr, last_shifts);
					if (ret != OK) {
						goto end_function;
					}

					new_index = get_next_index(level_guard,
								   &aux_shifts);

					// Add old guard in index
					atomic_entry_write(
						&(*level)->level[new_index],
						memory_order_release,
						level_guard, level_guard_shifts,
						level_type,
						(uintptr_t)last_level);

					if (start && (*level != level_aux)) {
						count_t level_shifts =
							*shifts +
							MEMDB_BITS_PER_ENTRY;

						if ((start) &&
						    (op == MEMDB_OP_ROLLBACK) &&
						    (level_shifts !=
						     (count_t)ADDR_SIZE) &&
						    ((*last_success_addr >>
						      level_shifts) ==
						     (addr >> level_shifts))) {
							*common_level = *level;
							*common_level_shifts =
								*shifts;
						}
						start_path->levels
							[start_path->count] =
							*level;
						start_path->indexes
							[start_path->count] =
							*index;
						start_path->count++;
					}
				}
				goto end_function;
			}

		} else {
			// If entry has guard, it must match with common bits.
			ret = check_guard(level_guard_shifts, level_guard, addr,
					  shifts);
			if (ret != OK) {
				assert(op != MEMDB_OP_ROLLBACK);
				goto end_function;
			}
		}

		if (*level != first_level) {
			if ((op == MEMDB_OP_INSERT) ||
			    (op == MEMDB_OP_UPDATE)) {
				lock_level(*level, *index, locked_levels);
			}

			if (!start) {
				ret = fill_level_entries(*level, object, type,
							 prev_object, prev_type,
							 0, *index, addr,
							 last_success_addr,
							 *shifts, op);
				if (ret != OK) {
					goto end_function;
				}
			}
		}

		*level = (memdb_level_t *)level_next;
		*index = get_next_index(addr, shifts);

		(void)atomic_entry_read(&(*level)->level[*index], &level_guard,
					&level_guard_shifts, &level_type,
					&level_next);
	}

	if ((level_type != prev_type) || (level_next != prev_object) ||
	    ((*shifts == 0U) && (prev_type == MEMDB_TYPE_NOTYPE))) {
		ret = ERROR_MEMDB_NOT_OWNER;
		goto end_function;
	}

end_function:
	return ret;
}

static error_t
add_address(allocator_t *allocator, uintptr_t object, memdb_type_t type,
	    memdb_level_t *first_level, paddr_t addr,
	    count_t first_level_shifts, bool start, uintptr_t prev_object,
	    memdb_type_t prev_type, paddr_t *last_success_addr,
	    locked_levels_t *locked_levels, memdb_op_t op)
{
	memdb_level_t *level  = first_level;
	count_t	       shifts = first_level_shifts;
	index_t	       index  = get_next_index(addr, &shifts);
	paddr_t	       level_guard;
	count_t	       level_guard_shifts;
	memdb_type_t   level_type;
	uintptr_t      level_next;
	start_path_t   start_path	   = { { NULL }, { 0 }, 0 };
	count_t	       common_level_shifts = 0;
	memdb_level_t *common_level	   = NULL;
	count_t	       count		   = 0;
	error_t	       ret		   = OK;

	ret = go_down_levels(first_level, &level, addr, &index, &shifts, op,
			     start, &start_path, locked_levels, object, type,
			     prev_object, prev_type, &common_level,
			     &common_level_shifts, last_success_addr,
			     allocator);
	if (ret != OK) {
		goto end_function;
	}

	assert(shifts != ADDR_SIZE);

	ret = create_n_levels(allocator, &level, start, &shifts, &index, addr,
			      object, type, prev_object, prev_type, &start_path,
			      locked_levels, first_level, &common_level,
			      &common_level_shifts, op, last_success_addr, 0);
	if (ret != OK) {
		goto end_function;
	}

	if ((op != MEMDB_OP_ROLLBACK) && (level != first_level)) {
		lock_level(level, index, locked_levels);
	}

	// If we are in the last MEMDB_BITS_PER_ENTRY bits or if the remaining
	// bits of start address are all zeroes or the remaining bits of end
	// address are all ones, then we can directly point to the object.
	if ((!start) && (level != first_level)) {
		ret = fill_level_entries(level, object, type, prev_object,
					 prev_type, 0, index, addr,
					 last_success_addr, shifts, op);
		if (ret != OK) {
			goto end_function;
		}
	}

	level_guard	   = 0;
	level_guard_shifts = (count_t)ADDR_SIZE;
	level_type	   = type;
	level_next	   = object;

	atomic_entry_write(&level->level[index], memory_order_relaxed,
			   level_guard, level_guard_shifts, level_type,
			   level_next);

	if (op != MEMDB_OP_ROLLBACK) {
		*last_success_addr = calculate_address(addr, shifts, index);
	}

	if (!start) {
		goto end_function;
	}

	count_t aux_shifts = shifts + MEMDB_BITS_PER_ENTRY;

	// Rest of function only applicable for start path
	if ((op == MEMDB_OP_ROLLBACK) && (aux_shifts != ADDR_SIZE) &&
	    ((*last_success_addr >> aux_shifts) == (addr >> aux_shifts))) {
		common_level	    = level;
		common_level_shifts = shifts;
	}

	if ((start_path.count == 0U) ||
	    (start_path.levels[start_path.count - 1U] != level)) {
		start_path.levels[start_path.count]  = level;
		start_path.indexes[start_path.count] = index;
		start_path.count++;
	}

	if (common_level == NULL) {
		common_level	    = first_level;
		common_level_shifts = first_level_shifts - MEMDB_BITS_PER_ENTRY;
	}

	count = start_path.count - 1U;

	// Fill entries from start_index+1 to MEMDB_NUM_ENTRIES in start path
	// levels
	while (start_path.levels[count] != common_level) {
		index_t start_index = start_path.indexes[count] + 1U;

		level = start_path.levels[count];

		ret = fill_level_entries(level, object, type, prev_object,
					 prev_type, start_index,
					 MEMDB_NUM_ENTRIES, addr,
					 last_success_addr, shifts, op);
		if (ret != OK) {
			goto end_function;
		}
		count--;
	}

	if ((op == MEMDB_OP_ROLLBACK) && (count != 0U)) {
		level		    = start_path.levels[count];
		index_t start_index = start_path.indexes[count] + 1U;
		index_t end_index =
			(index_t)(((*last_success_addr >> common_level_shifts) &
				   MEMDB_BITS_PER_ENTRY_MASK) +
				  1U);

		// Fill intermediate entries of new common level
		ret = fill_level_entries(level, object, type, prev_object,
					 prev_type, start_index, end_index,
					 addr, last_success_addr, shifts, op);
		if (ret != OK) {
			goto end_function;
		}

		*last_success_addr = (paddr_t)-1;
	}

end_function:
	return ret;
}

// Adds start and end address entries and intermediate entries between them.
// First go down to the level where the start address is located, then go up
// to the common levels adding all entries between start_index+1 to
// MEMDB_NUM_ENTRIES in each level, then add entries from start_index+1 to
// end_index-1 in the common level, and finally go done to the level where the
// end address is, adding all the entries from 0 to end_index-1 in each level.
//
// If there an entry points to an object different from prev_object, it means
// the address already has an owner. If so, return error and rollback to
// initial state by calling this function again but now the end_addr will be the
// last_success_addr.
static error_t
add_address_range(allocator_t *allocator, paddr_t start_addr, paddr_t end_addr,
		  memdb_level_t *common_level, count_t shifts, uintptr_t object,
		  memdb_type_t type, uintptr_t prev_object,
		  memdb_type_t prev_type, locked_levels_t *end_locked_levels,
		  locked_levels_t *start_locked_levels,
		  paddr_t *last_success_addr, memdb_op_t op)
{
	count_t start_shifts = shifts;
	count_t end_shifts   = shifts;
	index_t start_index  = get_next_index(start_addr, &start_shifts);
	index_t end_index    = get_next_index(end_addr, &end_shifts);
	bool	rollback     = false;
	paddr_t mask	     = util_mask(start_shifts);
	error_t ret	     = OK;

	if (op == MEMDB_OP_ROLLBACK) {
		rollback = true;
	}

	// Add entry already if range is covered by only one entry
	if ((start_index == end_index) && ((mask & start_addr) == 0U) &&
	    ((mask & end_addr) == mask)) {
		count_t	     level_guard_shifts;
		paddr_t	     level_guard;
		memdb_type_t level_type;
		uintptr_t    level_next;

		(void)atomic_entry_read(&common_level->level[start_index],
					&level_guard, &level_guard_shifts,
					&level_type, &level_next);
		if ((level_type == prev_type) && (level_next == prev_object)) {
			atomic_entry_write(&common_level->level[start_index],
					   memory_order_relaxed, 0,
					   (count_t)ADDR_SIZE, type, object);
		} else {
			ret = ERROR_MEMDB_NOT_OWNER;
		}
		goto end_function;
	}

	if (!rollback) {
		// For the start entries, I add the entry from the common level
		// since it might be updated if level below is collapsed. I do
		// not add the lock since it is already in the end locks array.
		start_locked_levels->entries[0] =
			&common_level->level[start_index];
		start_locked_levels->locks[0] = NULL;
		start_locked_levels->count++;
	}

	// Find START address entry and point it to object
	ret = add_address(allocator, object, type, common_level, start_addr,
			  shifts, true, prev_object, prev_type,
			  last_success_addr, start_locked_levels, op);

	if (ret != OK) {
		goto end_function;
	}
	if (rollback && (*last_success_addr == 0U)) {
		goto end_function;
	}

	// Fill first level intermediate entries between start end end
	ret = fill_level_entries(common_level, object, type, prev_object,
				 prev_type, start_index + 1U, end_index,
				 start_addr, last_success_addr, end_shifts, op);
	if (ret != OK) {
		goto end_function;
	}

	// Find END address entry and point it to object
	ret = add_address(allocator, object, type, common_level, end_addr,
			  shifts, false, prev_object, prev_type,
			  last_success_addr, end_locked_levels, op);
end_function:
	return ret;
}

static error_t
compare_adjust_bits(count_t guard_shifts, count_t shifts,
		    count_t *extra_guard_shifts, count_t *extra_shifts,
		    paddr_t guard, paddr_t addr, bool insert)
{
	paddr_t tmp_guard = guard;
	paddr_t tmp_cmn;
	error_t ret = OK;

	if (guard_shifts > shifts) {
		*extra_shifts = guard_shifts - shifts;
	} else if (guard_shifts < shifts) {
		if (insert) {
			*extra_guard_shifts = shifts - guard_shifts;
		} else {
			ret = ERROR_ADDR_INVALID;
			goto end_function;
		}
	} else {
		// No need to adjust the guard_shifts value
	}

	if (insert) {
		if ((guard_shifts + *extra_guard_shifts) !=
		    (count_t)ADDR_SIZE) {
			tmp_guard = guard >> *extra_guard_shifts;
		} else {
			tmp_guard = 0;
		}
	}

	if ((shifts + *extra_shifts) != (count_t)ADDR_SIZE) {
		tmp_cmn = addr >> (shifts + *extra_shifts);
	} else {
		tmp_cmn = 0;
	}

	assert((shifts + *extra_shifts) <= ADDR_SIZE);
	assert((guard_shifts + *extra_guard_shifts) <= ADDR_SIZE);

	// If guard & common shifts differ, we calculate the highest common
	// bits between them and keep track of the remaining bits.
	if ((tmp_guard ^ tmp_cmn) != 0U) {
		count_t aux_shifts = 0;

		if (!insert) {
			ret = ERROR_ADDR_INVALID;
			goto end_function;
		}

		aux_shifts = calculate_common_bits(tmp_guard, tmp_cmn);

		// If there are no common bits between them, the guard
		// will not act as a shortcut.
		*extra_guard_shifts += aux_shifts;
		*extra_shifts += aux_shifts;

		assert((shifts + *extra_shifts) <= ADDR_SIZE);
		assert((guard_shifts + *extra_guard_shifts) <= ADDR_SIZE);

		if (*extra_guard_shifts != ADDR_SIZE) {
			tmp_guard = guard >> *extra_guard_shifts;
		} else {
			tmp_guard = 0;
		}
		if ((shifts + *extra_shifts) != (count_t)ADDR_SIZE) {
			tmp_cmn = addr >> (shifts + *extra_shifts);
		} else {
			tmp_cmn = 0;
		}
	}
	assert((tmp_guard ^ tmp_cmn) == 0U);

end_function:
	return ret;
}

static error_t
add_extra_shifts_update(allocator_t *allocator, count_t *shifts,
			count_t extra_shifts, uintptr_t next,
			paddr_t start_addr, paddr_t end_addr, uintptr_t object,
			memdb_type_t obj_type, uintptr_t prev_object,
			memdb_type_t prev_type, memdb_level_t **common_level,
			bool locking, bool lock_taken,
			locked_levels_t *locked_levels) LOCK_IMPL
{
	count_t	       rem_cmn_shifts = *shifts + extra_shifts;
	memdb_level_t *level	      = (memdb_level_t *)next;
	bool	       new_level      = false;
	count_t	       level_guard_shifts;
	paddr_t	       level_guard;
	memdb_type_t   level_type;
	uintptr_t      level_next;
	error_t	       ret = OK;

	// If !locking it means that we are in the middle of a
	// MEMDB_OP_CONTIGUOUSNESS op

	while (rem_cmn_shifts != *shifts) {
		index_t index = get_next_index(start_addr, &rem_cmn_shifts);

		if (locking) {
			// Lock level and check if it is needed. If so,
			// we keep lock to previous and current level
			// and lock all next levels. If not, we remove
			// lock from previous level.
			lock_level(level, index, locked_levels);
		}

		(void)atomic_entry_read(&level->level[index], &level_guard,
					&level_guard_shifts, &level_type,
					&level_next);

		// If entry has guard, it must match with common bits.
		ret = check_guard(level_guard_shifts, level_guard, end_addr,
				  &rem_cmn_shifts);
		if (ret != OK) {
			goto end_function;
		}

		if (level_type == MEMDB_TYPE_LEVEL) {
			// Go down levels until common level or we
			// reach an entry pointing to previous object

			if ((locking) && (!lock_taken) &&
			    !are_all_entries_same(level, object, index,
						  obj_type, 0,
						  MEMDB_NUM_ENTRIES)) {
				// Current level does not need lock,
				// remove previous level lock and reset
				// locked levels count.
				spinlock_t *lock = locked_levels->locks[0];

				spinlock_release(lock);

				assert(locked_levels->count == 2U);

				locked_levels->entries[0] =
					locked_levels->entries[1];
				locked_levels->locks[0] =
					locked_levels->locks[1];
				locked_levels->entries[1] =
					(_Atomic(memdb_entry_t) *)NULL;
				locked_levels->locks[1] = (spinlock_t *)NULL;
				locked_levels->count	= 1;
			} else if (locking) {
				// Current level needs to be locked, so all
				// next levels also need to be.
				lock_taken = true;
			} else {
				// Nothing to do
			}

			level = (memdb_level_t *)level_next;

			if (rem_cmn_shifts == *shifts) {
				*common_level = level;

				if (locking) {
					count_t tmp_shifts = *shifts;

					index = get_next_index(end_addr,
							       &tmp_shifts);
					lock_level(*common_level, index,
						   locked_levels);
				}
			}
		} else if (locking &&
			   ((new_level) || ((level_type == prev_type) &&
					    (level_next == prev_object)))) {
			// Create new level with all entries
			// pointing to prev owner
			memdb_level_ptr_result_t res =
				create_level(allocator, prev_type, prev_object);
			if (res.e != OK) {
				ret = res.e;
				goto end_function;
			}

			memdb_level_t *next_level = res.r;

			// Keep current and next levels lock since
			// current level will be modified.
			lock_taken = true;

			level_type = MEMDB_TYPE_LEVEL;
			level_next = (uintptr_t)next_level;

			atomic_entry_write(&level->level[index],
					   memory_order_release, level_guard,
					   level_guard_shifts, level_type,
					   level_next);

			if (rem_cmn_shifts == *shifts) {
				count_t tmp_shifts = *shifts;

				*common_level = next_level;

				index = get_next_index(end_addr, &tmp_shifts);
				lock_level(*common_level, index, locked_levels);
			} else {
				new_level = true;
				level	  = next_level;
			}
		} else if (!locking && (level_type == obj_type) &&
			   (level_next == object)) {
			*common_level = level;
			*shifts	      = rem_cmn_shifts + MEMDB_BITS_PER_ENTRY;
			goto end_function;
		} else if (level_type == MEMDB_TYPE_NOTYPE) {
			ret = ERROR_ADDR_INVALID;
			goto end_function;
		} else {
			ret = ERROR_MEMDB_NOT_OWNER;
			goto end_function;
		}
	}

end_function:
	return ret;
}

static error_t
add_extra_guard_shifts(allocator_t *allocator, count_t guard_shifts,
		       paddr_t guard, uintptr_t *next, memdb_type_t root_type,
		       count_t		extra_guard_shifts,
		       locked_levels_t *locked_levels, paddr_t end_addr)
{
	error_t ret;
	paddr_t new_guard;
	count_t level_guard_shifts;
	paddr_t level_guard;

	memdb_level_ptr_result_t res =
		create_level(allocator, MEMDB_TYPE_NOTYPE, 0);
	if (res.e != OK) {
		ret = res.e;
		goto error;
	}

	memdb_level_t *level = res.r;

	assert(extra_guard_shifts != 0U);

	count_t new_guard_shifts = guard_shifts + extra_guard_shifts;

	assert(new_guard_shifts <= ADDR_SIZE);

	if (new_guard_shifts != ADDR_SIZE) {
		new_guard = guard >> extra_guard_shifts;
	} else {
		new_guard = 0;
	}

	index_t index = get_next_index(guard, &extra_guard_shifts);

	count_t tmp_shifts = new_guard_shifts;
	lock_level(level, get_next_index(end_addr, &tmp_shifts), locked_levels);

	level_guard	   = guard;
	level_guard_shifts = guard_shifts;

	atomic_entry_write(&level->level[index], memory_order_relaxed,
			   level_guard, level_guard_shifts, root_type, *next);

	root_type = MEMDB_TYPE_LEVEL;
	*next	  = (uintptr_t)level;

	atomic_entry_write(&memdb.root, memory_order_release, new_guard,
			   new_guard_shifts, root_type, *next);

	ret = OK;

error:
	return ret;
}

static error_t
create_intermediate_level(allocator_t *allocator, paddr_t start_addr,
			  memdb_level_t *level, count_t level_shifts,
			  count_t shifts, index_t index)
{
	error_t	     ret;
	paddr_t	     lower_guard;
	count_t	     lower_guard_shifts;
	memdb_type_t lower_type;
	uintptr_t    lower_next;

	// Set guard equal to common bits and create level.

	(void)atomic_entry_read(&level->level[index], &lower_guard,
				&lower_guard_shifts, &lower_type, &lower_next);

	paddr_t lower_addr = lower_guard << lower_guard_shifts;

	assert(lower_guard_shifts != (count_t)ADDR_SIZE);

	count_t tmp_shifts = calculate_common_bits(lower_addr, start_addr);

	memdb_level_ptr_result_t res =
		create_level(allocator, MEMDB_TYPE_NOTYPE, 0);
	if (res.e != OK) {
		ret = res.e;
		goto error;
	}

	memdb_level_t *next_level = res.r;

	shifts			   = shifts > tmp_shifts ? shifts : tmp_shifts;
	count_t level_guard_shifts = shifts;
	paddr_t level_guard	   = (lower_guard << lower_guard_shifts) >>
			      level_guard_shifts;

	assert(level_guard_shifts < level_shifts);

	if (level_guard_shifts == (level_shifts - MEMDB_BITS_PER_ENTRY)) {
		// No guard if no levels are skipped.
		level_guard_shifts = (count_t)ADDR_SIZE;
		level_guard	   = 0U;
	}

	uintptr_t level_next = (uintptr_t)next_level;

	atomic_entry_write(&level->level[index], memory_order_release,
			   level_guard, level_guard_shifts, MEMDB_TYPE_LEVEL,
			   level_next);

	assert(lower_guard_shifts < shifts);
	tmp_shifts = shifts;

	// Chain lower_level from next_level
	count_t new_index = get_next_index(lower_addr, &tmp_shifts);

	if (lower_guard_shifts == (shifts - MEMDB_BITS_PER_ENTRY)) {
		// No guard if no levels are skipped.
		lower_guard_shifts = (count_t)ADDR_SIZE;
		lower_guard	   = 0U;
	}

	atomic_entry_write(&next_level->level[new_index], memory_order_relaxed,
			   lower_guard, lower_guard_shifts, lower_type,
			   lower_next);

	ret = OK;
error:
	return ret;
}

static error_t
add_extra_shifts(allocator_t *allocator, count_t shifts, count_t extra_shifts,
		 uintptr_t next, paddr_t start_addr, paddr_t end_addr,
		 uintptr_t object, memdb_type_t obj_type,
		 memdb_level_t **common_level, bool lock_taken,
		 locked_levels_t *locked_levels) LOCK_IMPL
{
	count_t	       level_guard_shifts;
	paddr_t	       level_guard;
	memdb_type_t   level_type;
	uintptr_t      level_next;
	count_t	       rem_cmn_shifts = shifts + extra_shifts;
	memdb_level_t *level	      = (memdb_level_t *)next;
	error_t	       ret	      = OK;

	while (rem_cmn_shifts != shifts) {
		index_t llevel_index = locked_levels->count - 1U;
		count_t level_shifts = rem_cmn_shifts;

		index_t index = get_next_index(start_addr, &rem_cmn_shifts);

		// Lock level and check if it is needed. If so, we keep lock to
		// prev and current level and lock all next levels. If not, we
		// remove lock from previous level.
		if (locked_levels->locks[llevel_index] != &level->lock) {
			lock_level(level, index, locked_levels);
		}

		(void)atomic_entry_read(&level->level[index], &level_guard,
					&level_guard_shifts, &level_type,
					&level_next);

		if (level_type != MEMDB_TYPE_NOTYPE) {
			if ((!lock_taken) &&
			    !are_all_entries_same(level, object, index,
						  obj_type, 0,
						  MEMDB_NUM_ENTRIES)) {
				// Current level doesn't need lock, remove prev
				// level lock and reset locked levels count.
				spinlock_t *lock = locked_levels->locks[0];

				spinlock_release(lock);

				assert(locked_levels->count == 2U);

				locked_levels->entries[0] =
					locked_levels->entries[1];
				locked_levels->locks[0] =
					locked_levels->locks[1];
				locked_levels->entries[1] =
					(_Atomic(memdb_entry_t) *)NULL;
				locked_levels->locks[1] = (spinlock_t *)NULL;
				locked_levels->count	= 1;
			} else {
				// Current level needs to hold lock, so all
				// next levels also.
				lock_taken = true;
			}

			if (rem_cmn_shifts == shifts) {
				ret = OK;
				if ((level_guard_shifts != ADDR_SIZE) &&
				    (level_guard_shifts != shifts)) {
					assert(level_guard_shifts < shifts);
					// Next level guard is smaller than
					// shifts we need to create an
					// intermediate level.
					ret = ERROR_ADDR_INVALID;
				}
			} else {
				ret = check_guard(level_guard_shifts,
						  level_guard, end_addr,
						  &rem_cmn_shifts);
			}
			// If guard, does it match with common bits?
			// 1. No  -> create an intermediate level.
			// 2. Yes -> (type == level) ?
			//	a. Yes -> go down to next level.
			//	b. No  -> error (already has owner).
			if (ret != OK) {
				ret = create_intermediate_level(
					allocator, start_addr, level,
					level_shifts, shifts, index);
				if (ret != OK) {
					goto end_function;
				}
				// Retry this level
				rem_cmn_shifts = level_shifts;

				lock_taken = true;
			} else {
				// Go down levels until common level
				if (level_type == MEMDB_TYPE_LEVEL) {
					level = (memdb_level_t *)level_next;
				} else {
					ret = ERROR_MEMDB_NOT_OWNER;
					goto end_function;
				}
			}

			if (rem_cmn_shifts == shifts) {
				*common_level = level;

				count_t tmp_shifts = shifts;
				index = get_next_index(end_addr, &tmp_shifts);
				lock_level(*common_level, index, locked_levels);
			}
		} else {
			// Set guard equal to common bits and create level.
			count_t tmp_shifts = shifts;

			memdb_level_ptr_result_t res =
				create_level(allocator, MEMDB_TYPE_NOTYPE, 0);
			if (res.e != OK) {
				ret = res.e;
				goto end_function;
			}

			memdb_level_t *next_level = res.r;

			// Keep current and next levels lock since current level
			// values will be modified.
			lock_taken = true;

			if (shifts != ADDR_SIZE) {
				level_guard = start_addr >> shifts;
			} else {
				level_guard = 0;
			}
			level_guard_shifts = shifts;
			level_type	   = MEMDB_TYPE_LEVEL;
			level_next	   = (uintptr_t)next_level;

			atomic_entry_write(&level->level[index],
					   memory_order_release, level_guard,
					   level_guard_shifts, level_type,
					   level_next);

			index = get_next_index(end_addr, &tmp_shifts);
			lock_level(next_level, index, locked_levels);

			rem_cmn_shifts = shifts;
			*common_level  = next_level;
		}
	}

end_function:
	return ret;
}

static error_t
find_common_level(paddr_t start_addr, paddr_t end_addr,
		  memdb_level_t **common_level, count_t *shifts,
		  allocator_t *allocator, uintptr_t object,
		  memdb_type_t obj_type, uintptr_t prev_object,
		  memdb_type_t prev_type, locked_levels_t *locked_levels,
		  bool insert, bool first) LOCK_IMPL
{
	error_t	     ret	= OK;
	bool	     lock_taken = false;
	count_t	     guard_shifts;
	paddr_t	     guard;
	memdb_type_t root_type;
	uintptr_t    next;
	count_t	     extra_shifts	= 0;
	count_t	     extra_guard_shifts = 0;
	bool	     locking		= false;

	if (locked_levels != NULL) {
		locking = true;
	}

	// We calculate the first common bits between start and end address
	// and save shifts (must be multiple of MEMDB_BITS_PER_ENTRY).
	*shifts = calculate_common_bits(start_addr, end_addr);

	// FIXME: check how to remove this restriction.
	// To simplify the code, we do not allow the root to point directly to
	// the object. If the remaining bits of start address are all zeroes
	// and all ones for end address, instead of making the root point to
	// the object, we will set the guard to be MEMDB_BITS_PER_ENTRY shorter
	// and add a level just after the root.
	if (*shifts != ADDR_SIZE) {
		paddr_t mask = util_mask(*shifts);

		if (((mask & start_addr) == 0U) &&
		    ((mask & end_addr) == mask)) {
			*shifts += MEMDB_BITS_PER_ENTRY;
		}
	}

	(void)atomic_entry_read(&memdb.root, &guard, &guard_shifts, &root_type,
				&next);

	if ((!first) && (root_type == MEMDB_TYPE_NOTYPE)) {
		ret = ERROR_MEMDB_EMPTY;
		goto end_function;
	}

	if (locking) {
		// Lock root until we know it is not need.
		spinlock_acquire(&memdb.lock);
		locked_levels->entries[0] = &memdb.root;
		locked_levels->locks[0]	  = &memdb.lock;
		locked_levels->count	  = 1;
	}

	if (first) {
		goto end_function;
	}

	// To compare guard & common bits, their length must be equal.
	ret = compare_adjust_bits(guard_shifts, *shifts, &extra_guard_shifts,
				  &extra_shifts, guard, start_addr, insert);
	if (ret != OK) {
		goto end_function;
	}

	assert(root_type == MEMDB_TYPE_LEVEL);
	assert((*shifts + extra_shifts) <= ADDR_SIZE);
	assert((guard_shifts + extra_guard_shifts) <= ADDR_SIZE);
	assert(insert || (extra_guard_shifts == 0U));
	assert(!insert || (locked_levels != NULL));
	assert((allocator != NULL) || (locked_levels == NULL));

	// If there are extra guard shifts, the guard needs to be updated and a
	// new level created to add the remaining guard.
	if (extra_guard_shifts != 0U) {
		// Root must keep lock since we need to modify its values.
		// Therefore, all consecutive levels should hold locks.
		lock_taken = true;

		ret = add_extra_guard_shifts(allocator, guard_shifts, guard,
					     &next, root_type,
					     extra_guard_shifts, locked_levels,
					     end_addr);
		if (ret != OK) {
			goto end_function;
		}
	}

	// If there are extra common shifts, we need to find the common level.
	if (extra_shifts != 0U) {
		if (!insert) {
			ret = add_extra_shifts_update(
				allocator, shifts, extra_shifts, next,
				start_addr, end_addr, object, obj_type,
				prev_object, prev_type, common_level, locking,
				lock_taken, locked_levels);
		} else {
			ret = add_extra_shifts(allocator, *shifts, extra_shifts,
					       next, start_addr, end_addr,
					       object, obj_type, common_level,
					       lock_taken, locked_levels);
		}
	} else {
		*common_level = (memdb_level_t *)next;

		// Lock common level if it is not already locked
		if (locking && !lock_taken) {
			count_t aux_shifts = *shifts;
			index_t index = get_next_index(end_addr, &aux_shifts);
			lock_level(*common_level, index, locked_levels);
		}
	}

end_function:
	return ret;
}

// - start_locked_levels : refer to locks held from levels after the common
// level to the level where the start address is.
//
// - end_locked_levels : refer to the locks held from the root to the common
// level to the level where the end address is located.
static error_t
add_range(allocator_t *allocator, paddr_t start_addr, paddr_t end_addr,
	  memdb_level_t *common_level, count_t shifts, uintptr_t object,
	  memdb_type_t obj_type, uintptr_t prev_object, memdb_type_t prev_type,
	  locked_levels_t *end_locked_levels, error_t init_error, memdb_op_t op)

{
	paddr_t		last_success_addr   = (paddr_t)-1;
	error_t		ret		    = OK;
	locked_levels_t start_locked_levels = { { NULL }, { NULL }, 0, { 0 } };

	if (init_error != OK) {
		ret = init_error;
		goto unlock_levels_l;
	}

	ret = add_address_range(allocator, start_addr, end_addr, common_level,
				shifts, object, obj_type, prev_object,
				prev_type, end_locked_levels,
				&start_locked_levels, &last_success_addr, op);

	if ((ret != OK) && (start_addr <= last_success_addr) &&
	    (last_success_addr != (paddr_t)-1)) {
		// Rolling back the entries to old owner.
		(void)add_address_range(allocator, start_addr,
					last_success_addr, common_level, shifts,
					prev_object, prev_type, object,
					obj_type, end_locked_levels,
					&start_locked_levels,
					&last_success_addr, MEMDB_OP_ROLLBACK);
	}

unlock_levels_l:
	if (start_locked_levels.count != 0U) {
		unlock_levels(&start_locked_levels);
	}

	if (end_locked_levels->count != 0U) {
		unlock_levels(end_locked_levels);
	}

	return ret;
}

static error_t
check_address(memdb_level_t *first_level, memdb_level_t **level, paddr_t addr,
	      index_t *index, count_t *shifts, memdb_op_t op, bool start,
	      uintptr_t object, memdb_type_t type)
{
	paddr_t	     level_guard;
	count_t	     level_guard_shifts;
	memdb_type_t level_type;
	uintptr_t    level_next;
	error_t	     ret = OK;

	(void)atomic_entry_read(&(*level)->level[*index], &level_guard,
				&level_guard_shifts, &level_type, &level_next);

	// We need to go down the levels until we find an empty entry or we run
	// out of remaining bits. In the former case, return error since the
	// address already has an owner.
	while ((level_type == MEMDB_TYPE_LEVEL) && (*shifts != 0U)) {
		// If entry has guard, it must match with common bits.
		ret = check_guard(level_guard_shifts, level_guard, addr,
				  shifts);

		if (ret != OK) {
			goto error;
		}

		if ((*level != first_level) &&
		    (op == MEMDB_OP_CONTIGUOUSNESS)) {
			index_t start_index = 0;
			index_t end_index   = 0;
			bool	res	    = false;

			if (start) {
				start_index = *index + 1U;
				end_index   = MEMDB_NUM_ENTRIES;
			} else {
				start_index = 0;
				end_index   = *index;
			}

			res = are_all_entries_same(*level, object,
						   MEMDB_NUM_ENTRIES, type,
						   start_index, end_index);
			if (!res) {
				ret = ERROR_MEMDB_NOT_OWNER;
				goto error;
			}
		}

		*level = (memdb_level_t *)level_next;
		*index = get_next_index(addr, shifts);

		(void)atomic_entry_read(&(*level)->level[*index], &level_guard,
					&level_guard_shifts, &level_type,
					&level_next);
	}

	if ((op == MEMDB_OP_CONTIGUOUSNESS) &&
	    ((level_type != type) || (level_next != object))) {
		ret = ERROR_MEMDB_NOT_OWNER;
		goto error;
	}

error:
	return ret;
}

// Populate the memory database. If any entry from the range already has an
// owner, return error and do not update the database.
error_t
memdb_insert(partition_t *partition, paddr_t start_addr, paddr_t end_addr,
	     uintptr_t object, memdb_type_t obj_type)
{
	error_t		ret	      = OK;
	locked_levels_t locked_levels = { { NULL }, { NULL }, 0, { 0 } };
	paddr_t		guard;
	count_t		guard_shifts;
	memdb_type_t	root_type;
	uintptr_t	next;
	memdb_level_t  *common_level = NULL;
	count_t		shifts;
	bool		insert	    = true;
	bool		first_entry = false;

	// Overlapping addresses and the entire address space will not be passed
	// as an argument to the function
	assert((start_addr != end_addr) && (start_addr < end_addr));
	assert((start_addr != 0U) || (~end_addr != 0U));
	assert(partition != NULL);

	allocator_t *allocator = &partition->allocator;

	(void)atomic_entry_read(&memdb.root, &guard, &guard_shifts, &root_type,
				&next);

	if (root_type == MEMDB_TYPE_NOTYPE) {
		first_entry = true;
	}

	ret = find_common_level(start_addr, end_addr, &common_level, &shifts,
				allocator, object, obj_type, 0,
				MEMDB_TYPE_NOTYPE, &locked_levels, insert,
				first_entry);
	if (ret != OK) {
		goto end_function;
	}

	// FIXME: remove this case and handle as any other new level.
	if (first_entry) {
		// Empty database. The root guard will be equal to the common
		// bits between start and end address.
		guard_shifts = shifts;

		if (shifts != ADDR_SIZE) {
			guard = start_addr >> shifts;
		} else {
			guard = 0;
		}

		// Create a new level and add address range entries.
		memdb_level_ptr_result_t res =
			create_level(allocator, MEMDB_TYPE_NOTYPE, 0);
		if (res.e != OK) {
			ret = res.e;
			goto end_function;
		}

		memdb_level_t *first_level = res.r;

		count_t aux_shifts = shifts;
		index_t index	   = get_next_index(start_addr, &aux_shifts);

		lock_level(first_level, index, &locked_levels);

		root_type = MEMDB_TYPE_LEVEL;
		next	  = (uintptr_t)first_level;

		atomic_entry_write(&memdb.root, memory_order_release, guard,
				   guard_shifts, root_type, next);

		common_level = first_level;
	}

end_function:

	// Add range from level after the common bits on
	ret = add_range(allocator, start_addr, end_addr, common_level, shifts,
			object, obj_type, 0, MEMDB_TYPE_NOTYPE, &locked_levels,
			ret, MEMDB_OP_INSERT);

	if (ret == OK) {
		TRACE(MEMDB, INFO,
		      "memdb_insert: {:#x}..{:#x} - obj({:#x}) - type({:d})",
		      start_addr, end_addr, object, (register_t)obj_type);

#if defined(MEMDB_DEBUG)
		// Check that the range was added correctly
		bool cont = memdb_is_ownership_contiguous(start_addr, end_addr,
							  object, obj_type);
		if (!cont) {
			LOG(ERROR, INFO,
			    "<<< memdb_insert BUG!! range {:#x}..{:#x} should be contiguous",
			    start_addr, end_addr);
			panic("BUG in memdb_insert");
		}
#endif
	} else {
		TRACE(MEMDB, INFO,
		      "memdb: Error inserting {:#x}..{:#x} - obj({:#x}) - type({:d}), err = {:d}",
		      start_addr, end_addr, object, (register_t)obj_type,
		      (register_t)ret);
	}

	return ret;
}

// Change the ownership of the input address range. Checks if all entries of
// range were pointing to previous object. If so, update all entries to point to
// the new object. If not, return error.
error_t
memdb_update(partition_t *partition, paddr_t start_addr, paddr_t end_addr,
	     uintptr_t object, memdb_type_t obj_type, uintptr_t prev_object,
	     memdb_type_t prev_type)
{
	error_t		ret = OK;
	count_t		shifts;
	locked_levels_t locked_levels = { { NULL }, { NULL }, 0, { 0 } };
	memdb_level_t  *common_level  = NULL;

	// We need to find the common level, the level where all the first
	// common bits between start and end address are covered. Then, add
	// entries from the address range from that level on.

	// Overlapping addresses and the entire address space will not be passed
	// as an argument to the function
	assert((start_addr != end_addr) && (start_addr < end_addr));
	assert((start_addr != 0U) || (~end_addr != 0U));
	assert(partition != NULL);

	allocator_t *allocator = &partition->allocator;

	ret = find_common_level(start_addr, end_addr, &common_level, &shifts,
				allocator, object, obj_type, prev_object,
				prev_type, &locked_levels, false, false);

	ret = add_range(allocator, start_addr, end_addr, common_level, shifts,
			object, obj_type, prev_object, prev_type,
			&locked_levels, ret, MEMDB_OP_UPDATE);

	if (ret == OK) {
		TRACE(MEMDB, INFO,
		      "memdb_update: {:#x}..{:#x} - obj({:#x}) - type({:d})",
		      start_addr, end_addr, object, (register_t)obj_type);

#if defined(MEMDB_DEBUG)
		// Check that the range was added correctly
		bool cont = memdb_is_ownership_contiguous(start_addr, end_addr,
							  object, obj_type);
		if (!cont) {
			LOG(ERROR, INFO,
			    "<<< memdb_update BUG!! range {:#x}..{:#x} should be contiguous",
			    start_addr, end_addr);
			panic("BUG in memdb_update");
		}
#endif
	} else {
		TRACE(MEMDB, INFO,
		      "memdb: Error updating {:#x}..{:#x} - obj({:#x}) - type({:d}), err = {:d}",
		      start_addr, end_addr, object, (register_t)obj_type,
		      (register_t)ret);
	}

	return ret;
}

// Check if all the entries from the input address range point to the object
// passed as an argument
bool
memdb_is_ownership_contiguous(paddr_t start_addr, paddr_t end_addr,
			      uintptr_t object, memdb_type_t type)
{
	memdb_level_t *common_level = NULL;
	count_t	       shifts;
	error_t	       ret	= OK;
	bool	       ret_bool = true;
	bool	       start	= true;

	rcu_read_start();

	memdb_entry_t root_entry =
		atomic_load_explicit(&memdb.root, memory_order_relaxed);
	if ((memdb_entry_info_get_type(&root_entry.info)) ==
	    MEMDB_TYPE_NOTYPE) {
		ret_bool = false;
		goto end_function;
	}

	assert((start_addr != end_addr) && (start_addr < end_addr));
	assert((start_addr != 0U) || (~end_addr != 0U));

	ret = find_common_level(start_addr, end_addr, &common_level, &shifts,
				NULL, object, type, 0, MEMDB_TYPE_LEVEL, NULL,
				false, false);
	if (ret != OK) {
		ret_bool = false;
		goto end_function;
	}

	count_t start_shifts = shifts;
	count_t end_shifts   = shifts;
	index_t start_index  = get_next_index(start_addr, &start_shifts);
	index_t end_index    = get_next_index(end_addr, &end_shifts);

	// Go down levels until START entry and check if it is equal to object.
	index_t	       index = start_index;
	memdb_level_t *level = common_level;

	ret = check_address(common_level, &level, start_addr, &index,
			    &start_shifts, MEMDB_OP_CONTIGUOUSNESS, start,
			    object, type);
	if (ret != OK) {
		ret_bool = false;
		goto end_function;
	}

	// Check first level intermediate entries between start end end
	ret_bool = are_all_entries_same(common_level, object, MEMDB_NUM_ENTRIES,
					type, start_index + 1U, end_index);
	if (!ret_bool) {
		goto end_function;
	}

	// Go down levels until END entry and check if it is equal to object.
	index = end_index;
	level = common_level;

	ret = check_address(common_level, &level, end_addr, &index, &end_shifts,
			    MEMDB_OP_CONTIGUOUSNESS, !start, object, type);
	if (ret != OK) {
		ret_bool = false;
		goto end_function;
	}

end_function:
	rcu_read_finish();

	return ret_bool;
}

// Find the entry corresponding to the input address and return the object and
// type the entry is pointing to
memdb_obj_type_result_t
memdb_lookup(paddr_t addr)
{
	memdb_obj_type_result_t ret = { 0 };
	paddr_t			guard;
	count_t			guard_shifts;
	memdb_type_t		root_type;
	uintptr_t		next;
	memdb_level_t	       *level;
	index_t			index;
	bool			start = true;

	(void)atomic_entry_read(&memdb.root, &guard, &guard_shifts, &root_type,
				&next);

	if (root_type == MEMDB_TYPE_NOTYPE) {
		ret.e = ERROR_MEMDB_EMPTY;
		goto end_function;
	}

	// If entry has guard, it must match with common bits.
	ret.e = check_guard(guard_shifts, guard, addr, NULL);
	if (ret.e != OK) {
		goto end_function;
	}

	level = (memdb_level_t *)next;
	index = get_next_index(addr, &guard_shifts);

	// Go down levels until we get to input address
	// Dummy start argument, does not affect lookup.
	ret.e = check_address((memdb_level_t *)next, &level, addr, &index,
			      &guard_shifts, MEMDB_OP_LOOKUP, start, 0,
			      MEMDB_TYPE_LEVEL);
	if (ret.e != OK) {
		ret.r.type   = MEMDB_TYPE_NOTYPE;
		ret.r.object = 0;
	} else {
		memdb_entry_t entry = atomic_load_explicit(
			&level->level[index], memory_order_relaxed);

		ret.r.type   = memdb_entry_info_get_type(&entry.info);
		ret.r.object = entry.next;
	}

end_function:
	return ret;
}

static error_t
memdb_do_walk(uintptr_t object, memdb_type_t type, memdb_fnptr fn, void *arg,
	      memdb_level_t *level, paddr_t covered_bits, count_t shifts,
	      paddr_t start_addr, paddr_t end_addr, bool all_memdb)
{
	count_t	       count	    = 0;
	index_t	       index	    = 0;
	paddr_t	       pending_base = 0;
	size_t	       pending_size = 0;
	error_t	       ret	    = OK;
	count_t	       guard_shifts;
	paddr_t	       guard;
	memdb_type_t   next_type;
	uintptr_t      next;
	index_t	       index_stack[MAX_LEVELS]	 = { 0 };
	count_t	       shifts_stack[MAX_LEVELS]	 = { 0 };
	paddr_t	       covered_stack[MAX_LEVELS] = { 0 };
	memdb_level_t *levels[MAX_LEVELS]	 = { NULL };

	if (!all_memdb) {
		index = get_next_index(start_addr, &shifts);
	}

	do {
		if (count > 0U) {
			count--;
			level	     = levels[count];
			index	     = index_stack[count];
			covered_bits = covered_stack[count];
			shifts	     = shifts_stack[count];
		}

		while (index != MEMDB_NUM_ENTRIES) {
			paddr_t base = (covered_bits << MEMDB_BITS_PER_ENTRY) |
				       index;
			base = base << shifts;

			// Stop iteration if we have reached to the end address,
			// when we are not walking through the entire database
			if (!all_memdb && (base > end_addr)) {
				count = 0U;
				break;
			}

			(void)atomic_entry_read(&level->level[index], &guard,
						&guard_shifts, &next_type,
						&next);

			size_t size;
			if (guard_shifts != ADDR_SIZE) {
				assert(next_type == MEMDB_TYPE_LEVEL);
				base = guard << guard_shifts;
				size = util_bit(guard_shifts);
			} else {
				size = util_bit(shifts);
			}

			// Skip entry if it is before the start address.
			if (!all_memdb && ((base + size - 1U) < start_addr)) {
				index++;
				continue;
			}

			if ((next_type == type) && (next == object)) {
				// Entry points to the object. If the entry is
				// contiguous with the current pending range, we
				// add this entry to it; otherwise we flush it
				// and start a new one.

				if (!all_memdb) {
					if (base < start_addr) {
						size -= start_addr - base;
						base = start_addr;
					}

					if ((base + size - 1U) > end_addr) {
						size -= (base + size - 1U) -
							end_addr;
					}
				}

				assert((pending_base + pending_size) <= base);

				if ((pending_base + pending_size) == base) {
					pending_size += size;
					index++;
					continue;
				}
			} else if (next_type == MEMDB_TYPE_LEVEL) {
				// We move down to the next level and iterate
				// through all its entries. We save current
				// level so that we can eventually return to it
				// and continue iterating through its entries,
				// starting from the next index on.

				covered_stack[count] = covered_bits;
				shifts_stack[count]  = shifts;
				levels[count]	     = level;
				index_stack[count]   = index + 1U;
				count++;

				if (guard_shifts == ADDR_SIZE) {
					covered_bits =
						(covered_bits
						 << MEMDB_BITS_PER_ENTRY) |
						index;
					shifts -= MEMDB_BITS_PER_ENTRY;
				} else {
					covered_bits = guard;
					shifts	     = guard_shifts -
						 MEMDB_BITS_PER_ENTRY;
				}

				level = (memdb_level_t *)next;
				index = 0;
				continue;
			} else {
				// Entry does not point to object.
				base = 0U;
				size = 0U;
			}

			if (pending_size != 0U) {
				ret = fn(pending_base, pending_size, arg);
				if (ret != OK) {
					goto error;
				}
			}

			pending_base = base;
			pending_size = size;
			index++;
		}
	} while (count > 0U);

	if (pending_size != 0U) {
		ret = fn(pending_base, pending_size, arg);
		if (ret != OK) {
			goto error;
		}
	}
error:
	return ret;
}

// Walk through a range of the database and add the address ranges that are
// owned by the object passed as argument.
// FIXME: replace function pointer with a selector event
error_t
memdb_range_walk(uintptr_t object, memdb_type_t type, paddr_t start_addr,
		 paddr_t end_addr, memdb_fnptr fn, void *arg)
{
	error_t	       ret	    = OK;
	paddr_t	       covered_bits = 0;
	count_t	       guard_shifts;
	paddr_t	       guard;
	memdb_type_t   next_type;
	uintptr_t      next;
	count_t	       shifts;
	memdb_level_t *common_level = NULL;

	rcu_read_start();

	(void)atomic_entry_read(&memdb.root, &guard, &guard_shifts, &next_type,
				&next);

	if (next_type == MEMDB_TYPE_NOTYPE) {
		ret = ERROR_MEMDB_EMPTY;
		goto error;
	}

	assert(next_type == MEMDB_TYPE_LEVEL);
	assert((start_addr != end_addr) && (start_addr < end_addr));
	assert((start_addr != 0U) || (~end_addr != 0U));

	ret = find_common_level(start_addr, end_addr, &common_level, &shifts,
				NULL, object, type, 0, MEMDB_TYPE_LEVEL, NULL,
				false, false);
	if (ret != OK) {
		// Start from the root level.
		common_level = (memdb_level_t *)next;
		shifts	     = guard_shifts;
	}

	if (shifts != ADDR_SIZE) {
		covered_bits = start_addr >> shifts;
	}

	ret = memdb_do_walk(object, type, fn, arg, common_level, covered_bits,
			    shifts, start_addr, end_addr, false);

error:
	rcu_read_finish();

	return ret;
}

// Walk through the entire database and add the address ranges that are owned
// by the object passed as argument.
// FIXME: replace function pointer with a selector event
error_t
memdb_walk(uintptr_t object, memdb_type_t type, memdb_fnptr fn, void *arg)
{
	error_t	     ret	  = OK;
	paddr_t	     covered_bits = 0;
	count_t	     guard_shifts;
	paddr_t	     guard;
	memdb_type_t next_type;
	uintptr_t    next;
	count_t	     shifts = (count_t)(ADDR_SIZE - MEMDB_BITS_PER_ENTRY);

	rcu_read_start();

	(void)atomic_entry_read(&memdb.root, &guard, &guard_shifts, &next_type,
				&next);

	if (next_type == MEMDB_TYPE_NOTYPE) {
		ret = ERROR_MEMDB_EMPTY;
		goto error;
	}

	assert(next_type == MEMDB_TYPE_LEVEL);

	memdb_level_t *level = (memdb_level_t *)next;

	if (guard_shifts != ADDR_SIZE) {
		covered_bits = guard;
	}

	shifts = guard_shifts - MEMDB_BITS_PER_ENTRY;

	ret = memdb_do_walk(object, type, fn, arg, level, covered_bits, shifts,
			    0, 0, true);
error:
	rcu_read_finish();

	return ret;
}

static void
memdb_init(void)
{
	atomic_entry_write(&memdb.root, memory_order_relaxed, 0,
			   (count_t)ADDR_SIZE, MEMDB_TYPE_NOTYPE, 0);
	spinlock_init(&memdb.lock);
}

void
memdb_gpt_handle_boot_cold_init(void)
{
#if !defined(NDEBUG)
	register_t flags = 0U;
	TRACE_SET_CLASS(flags, MEMDB);
	trace_set_class_flags(flags);
#endif

	partition_t *hyp_partition = partition_get_private();
	assert(hyp_partition != NULL);

	// Initialize memory ownership database
	memdb_init();

	// Assign the hypervisor's ELF image to the private partition.
	error_t err = memdb_insert(hyp_partition, phys_start, phys_end,
				   (uintptr_t)hyp_partition,
				   MEMDB_TYPE_PARTITION);
	if (err != OK) {
		panic("Error adding boot memory to hyp_partition");
	}

	// Obtain the initial bootmem range and change its ownership to the
	// hypervisor's allocator. We assume here that no other memory has been
	// assigned to any allocators yet.
	size_t bootmem_size	 = 0U;
	void  *bootmem_virt_base = bootmem_get_region(&bootmem_size);
	assert((bootmem_size != 0U) && (bootmem_virt_base != NULL));
	paddr_t bootmem_phys_base = partition_virt_to_phys(
		hyp_partition, (uintptr_t)bootmem_virt_base);
	assert(!util_add_overflows(bootmem_phys_base, bootmem_size - 1U));

	// Update ownership of the hypervisor partition's allocator memory
	err = memdb_update(hyp_partition, bootmem_phys_base,
			   bootmem_phys_base + (bootmem_size - 1U),
			   (uintptr_t)&hyp_partition->allocator,
			   MEMDB_TYPE_ALLOCATOR, (uintptr_t)hyp_partition,
			   MEMDB_TYPE_PARTITION);
	if (err != OK) {
		panic("Error updating bootmem allocator memory");
	}
}

error_t
memdb_gpt_handle_partition_add_ram_range(partition_t *owner, paddr_t phys_base,
					 size_t size)
{
	partition_t *hyp_partition = partition_get_private();

	assert(size > 0U);
	assert(!util_add_overflows(phys_base, size - 1U));

	// FIXME:
	// We should use memdb_insert() once this is safe to do so.
	error_t err = memdb_update(hyp_partition, phys_base,
				   phys_base + (size - 1U), (uintptr_t)owner,
				   MEMDB_TYPE_PARTITION, (uintptr_t)owner,
				   MEMDB_TYPE_PARTITION_NOMAP);
	if (err != OK) {
		LOG(ERROR, WARN,
		    "memdb: Error adding ram {:#x}..{:#x} to partition {:x}, err = {:d}",
		    phys_base, phys_base + size - 1U, (register_t)owner,
		    (register_t)err);
	}

	return err;
}

error_t
memdb_gpt_handle_partition_remove_ram_range(partition_t *owner,
					    paddr_t phys_base, size_t size)
{
	partition_t *hyp_partition = partition_get_private();

	assert(size > 0U);
	assert(!util_add_overflows(phys_base, size - 1U));

	// FIXME:
	// We should use memdb_insert() once this is safe to do so.
	error_t err = memdb_update(hyp_partition, phys_base,
				   phys_base + (size - 1U), (uintptr_t)owner,
				   MEMDB_TYPE_PARTITION_NOMAP, (uintptr_t)owner,
				   MEMDB_TYPE_PARTITION);
	if (err != OK) {
		LOG(ERROR, WARN,
		    "memdb: Error removing ram {:#x}..{:#x} from partition {:x}, err = {:d}",
		    phys_base, phys_base + size - 1U, (register_t)owner,
		    (register_t)err);
	}

	return err;
}

```

`hyp/mem/memextent/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

interface memextent
events memextent.ev memextent_tests.ev
types memextent.tc memextent_tests.tc
source memextent.c memextent_basic.c memextent_tests.c hypercalls.c

```

`hyp/mem/memextent/memextent.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

module memextent

subscribe object_create_memextent

// This activate handler must be last, otherwise we would need an unwinder to
// unmap on other handler failures.
subscribe object_activate_memextent
	priority last

subscribe object_deactivate_memextent(memextent)

subscribe object_cleanup_memextent(memextent)

// BASIC memory extent

subscribe memextent_activate[MEMEXTENT_TYPE_BASIC]
	handler memextent_activate_basic(me)

subscribe memextent_activate_derive[MEMEXTENT_TYPE_BASIC]
	handler memextent_activate_derive_basic(me)

subscribe memextent_map[MEMEXTENT_TYPE_BASIC]
	handler memextent_map_basic(extent, addrspace, vm_base, map_attrs)

subscribe memextent_unmap[MEMEXTENT_TYPE_BASIC]
	handler memextent_unmap_basic(extent, addrspace, vm_base)

subscribe memextent_unmap_all[MEMEXTENT_TYPE_BASIC]
	handler memextent_unmap_all_basic(extent)

subscribe memextent_update_access[MEMEXTENT_TYPE_BASIC]
	handler memextent_update_access_basic(extent, addrspace, vm_base,
	access_attrs)

subscribe memextent_is_mapped[MEMEXTENT_TYPE_BASIC]
	handler memextent_is_mapped_basic(me, addrspace, exclusive)

subscribe memextent_deactivate[MEMEXTENT_TYPE_BASIC]
	handler memextent_deactivate_basic(extent)

subscribe memextent_cleanup[MEMEXTENT_TYPE_BASIC]
	handler memextent_cleanup_basic(extent)

subscribe memextent_retain_mappings[MEMEXTENT_TYPE_BASIC]
	handler memextent_retain_mappings_basic(me)

subscribe memextent_release_mappings[MEMEXTENT_TYPE_BASIC]
	handler memextent_release_mappings_basic(me, clear)

subscribe memextent_lookup_mapping[MEMEXTENT_TYPE_BASIC]
	handler memextent_lookup_mapping_basic(me, phys, size, i)

subscribe memextent_attach[MEMEXTENT_TYPE_BASIC]
	handler memextent_attach_basic(me, hyp_va, size, memtype)

subscribe memextent_detach[MEMEXTENT_TYPE_BASIC]
	handler memextent_detach_basic(me)

subscribe object_create_addrspace
	handler memextent_create_addrspace_basic

subscribe object_deactivate_addrspace
	handler memextent_deactivate_addrspace_basic

subscribe memextent_get_offset_for_pa[MEMEXTENT_TYPE_BASIC]
	handler memextent_get_offset_for_pa_basic(extent, pa, size)

```

`hyp/mem/memextent/memextent.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

define MEMEXTENT_MAX_MAPS constant type count_t = 4;

extend cap_rights_memextent bitfield {
	0	map	bool;
	1	derive	bool;
	2	attach	bool;
	3	lookup	bool;
	4	donate	bool;
};

extend addrspace object {
	basic_mapping_list	structure list;
};

define memextent_basic_arg structure {
	me		pointer object memextent;
	map		array(MEMEXTENT_MAX_MAPS) pointer structure memextent_basic_mapping;
	failed_address	type paddr_t;
};

define memextent_basic_mapping structure {
	// RCU-protected addrspace pointer
	addrspace		pointer(atomic) object addrspace;
	mapping_list_node	structure list_node(contained);
	vbase			type vmaddr_t;
	attrs			bitfield memextent_mapping_attrs;
	retained		bool;
};

define memextent_map_ptr union(lockable) {
	basic	pointer structure memextent_basic_mapping;
};

extend memextent object {
	lock			structure spinlock;
	phys_base		type paddr_t;
	size			size;
	memtype			enumeration memextent_memtype;
	access			enumeration pgtable_access;
	children_list		structure list;
	children_list_node	structure list_node(contained);
	mappings		union memextent_map_ptr;
	active			bool;
	device_mem		bool;
	attached_address	uintptr;
	attached_size		size;
};

define memextent_clean_flags bitfield<32> {
	auto	zero	bool;
	auto	flush	bool;
};

#if defined(HYPERCALLS)
extend hyp_api_flags0 bitfield {
	delete	memextent;
	6	memextent bool = 1;
};
#endif

```

`hyp/mem/memextent/memextent_tests.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

module memextent

#if defined(UNIT_TESTS)

subscribe tests_init
	handler tests_memextent_init()

subscribe tests_start
	handler tests_memextent()
	require_preempt_disabled

#endif

```

`hyp/mem/memextent/memextent_tests.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#if defined(UNIT_TESTS)

define test_free_range structure {
	phys_base	array(16) type paddr_t;
	size		array(16) size;
	count		type count_t;
};

#endif

```

`hyp/mem/memextent/src/hypercalls.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#if defined(HYPERCALLS)
#include <hyptypes.h>

#include <hypcall_def.h>
#include <hyprights.h>

#include <atomic.h>
#include <compiler.h>
#include <cspace.h>
#include <cspace_lookup.h>
#include <memextent.h>
#include <object.h>
#include <pgtable.h>
#include <rcu.h>
#include <spinlock.h>

error_t
hypercall_memextent_modify(cap_id_t		    memextent_cap,
			   memextent_modify_flags_t flags, size_t offset,
			   size_t size)
{
	error_t	  err	 = OK;
	cspace_t *cspace = cspace_get_self();

	// FIXME:
	if (memextent_modify_flags_get_res_0(&flags) != 0U) {
		err = ERROR_ARGUMENT_INVALID;
		goto out;
	}

	memextent_ptr_result_t m = cspace_lookup_memextent(
		cspace, memextent_cap, CAP_RIGHTS_MEMEXTENT_MAP);
	if (compiler_unexpected(m.e != OK)) {
		err = m.e;
		goto out;
	}

	memextent_t *memextent = m.r;
	bool	     need_sync = !memextent_modify_flags_get_no_sync(&flags);

	memextent_modify_op_t op = memextent_modify_flags_get_op(&flags);
	if (op == MEMEXTENT_MODIFY_OP_UNMAP_ALL) {
		memextent_unmap_all(memextent);
	} else if ((op == MEMEXTENT_MODIFY_OP_ZERO_RANGE) && !need_sync) {
		err = memextent_zero_range(memextent, offset, size);
	} else if ((op == MEMEXTENT_MODIFY_OP_CACHE_CLEAN_RANGE) &&
		   !need_sync) {
		err = memextent_cache_clean_range(memextent, offset, size);
	} else if ((op == MEMEXTENT_MODIFY_OP_CACHE_FLUSH_RANGE) &&
		   !need_sync) {
		err = memextent_cache_flush_range(memextent, offset, size);
	} else if (op == MEMEXTENT_MODIFY_OP_SYNC_ALL) {
		err = need_sync ? OK : ERROR_ARGUMENT_INVALID;
	} else {
		err = ERROR_ARGUMENT_INVALID;
	}

	if ((err == OK) && need_sync) {
		// Wait for completion of EL2 operations using manual lookups
		rcu_sync();
	}

	object_put_memextent(memextent);
out:
	return err;
}

error_t
hypercall_memextent_configure(cap_id_t memextent_cap, paddr_t phys_base,
			      size_t size, memextent_attrs_t attributes)
{
	error_t	      err;
	cspace_t     *cspace = cspace_get_self();
	object_type_t type;

	object_ptr_result_t o = cspace_lookup_object_any(
		cspace, memextent_cap, CAP_RIGHTS_GENERIC_OBJECT_ACTIVATE,
		&type);
	if (compiler_unexpected(o.e != OK)) {
		err = o.e;
		goto out;
	}
	if (type != OBJECT_TYPE_MEMEXTENT) {
		err = ERROR_CSPACE_WRONG_OBJECT_TYPE;
		goto out_memextent_release;
	}

	memextent_t *target_me = o.r.memextent;

	spinlock_acquire(&target_me->header.lock);

	if (atomic_load_relaxed(&target_me->header.state) ==
	    OBJECT_STATE_INIT) {
		err = memextent_configure(target_me, phys_base, size,
					  attributes);
	} else {
		err = ERROR_OBJECT_STATE;
	}

	spinlock_release(&target_me->header.lock);
out_memextent_release:
	object_put(type, o.r);
out:
	return err;
}

error_t
hypercall_memextent_configure_derive(cap_id_t memextent_cap,
				     cap_id_t parent_memextent_cap,
				     size_t offset, size_t size,
				     memextent_attrs_t attributes)
{
	error_t	      err;
	cspace_t     *cspace = cspace_get_self();
	object_type_t type;

	memextent_ptr_result_t m = cspace_lookup_memextent(
		cspace, parent_memextent_cap, CAP_RIGHTS_MEMEXTENT_DERIVE);
	if (compiler_unexpected(m.e != OK)) {
		err = m.e;
		goto out;
	}

	memextent_t *parent = m.r;

	object_ptr_result_t o = cspace_lookup_object_any(
		cspace, memextent_cap, CAP_RIGHTS_GENERIC_OBJECT_ACTIVATE,
		&type);
	if (compiler_unexpected(o.e != OK)) {
		err = o.e;
		goto out_parent_release;
	}
	if (type != OBJECT_TYPE_MEMEXTENT) {
		err = ERROR_CSPACE_WRONG_OBJECT_TYPE;
		goto out_memextent_release;
	}

	memextent_t *target_me = o.r.memextent;

	spinlock_acquire(&target_me->header.lock);

	if (atomic_load_relaxed(&target_me->header.state) ==
	    OBJECT_STATE_INIT) {
		err = memextent_configure_derive(target_me, parent, offset,
						 size, attributes);

	} else {
		err = ERROR_OBJECT_STATE;
	}

	spinlock_release(&target_me->header.lock);
out_memextent_release:
	object_put(type, o.r);
out_parent_release:
	object_put_memextent(parent);
out:
	return err;
}

static error_t
hypercall_memextent_donate_child(cap_id_t parent_cap, cap_id_t child_cap,
				 size_t offset, size_t size, bool reverse)
{
	error_t	  err	 = OK;
	cspace_t *cspace = cspace_get_self();

	memextent_ptr_result_t child = cspace_lookup_memextent(
		cspace, child_cap, CAP_RIGHTS_MEMEXTENT_DONATE);
	if (compiler_unexpected(child.e != OK)) {
		err = child.e;
		goto out;
	}

	// We don't actually need a reference to the parent for the donate; the
	// child already has a reference. So after sanity checking the provided
	// parent cap we can immediately drop the reference.
	if (child.r->parent != NULL) {
		memextent_ptr_result_t m = cspace_lookup_memextent(
			cspace, parent_cap, CAP_RIGHTS_MEMEXTENT_DONATE);
		if (compiler_unexpected(m.e != OK)) {
			err = m.e;
			goto out_child_release;
		}

		if (child.r->parent != m.r) {
			err = ERROR_ARGUMENT_INVALID;
		}

		object_put_memextent(m.r);
	} else {
		partition_ptr_result_t p = cspace_lookup_partition(
			cspace, parent_cap, CAP_RIGHTS_PARTITION_DONATE);
		if (compiler_unexpected(p.e != OK)) {
			err = p.e;
			goto out_child_release;
		}

		if (child.r->header.partition != p.r) {
			err = ERROR_ARGUMENT_INVALID;
		}

		object_put_partition(p.r);
	}

	if (err == OK) {
		err = memextent_donate_child(child.r, offset, size, reverse);
	}

out_child_release:
	object_put_memextent(child.r);
out:
	return err;
}

static error_t
hypercall_memextent_donate_sibling(cap_id_t from, cap_id_t to, size_t offset,
				   size_t size)
{
	error_t	  err;
	cspace_t *cspace = cspace_get_self();

	memextent_ptr_result_t m1 = cspace_lookup_memextent(
		cspace, from, CAP_RIGHTS_MEMEXTENT_DONATE);
	if (compiler_unexpected(m1.e != OK)) {
		err = m1.e;
		goto out;
	}

	memextent_ptr_result_t m2 = cspace_lookup_memextent(
		cspace, to, CAP_RIGHTS_MEMEXTENT_DONATE);
	if (compiler_unexpected(m2.e != OK)) {
		err = m2.e;
		goto out_m1_release;
	}

	err = memextent_donate_sibling(m1.r, m2.r, offset, size);

	object_put_memextent(m2.r);
out_m1_release:
	object_put_memextent(m1.r);
out:
	return err;
}

error_t
hypercall_memextent_donate(memextent_donate_options_t options, cap_id_t from,
			   cap_id_t to, size_t offset, size_t size)
{
	error_t err;

	if (memextent_donate_options_get_res_0(&options) != 0U) {
		err = ERROR_ARGUMENT_INVALID;
		goto out;
	}

	memextent_donate_type_t type =
		memextent_donate_options_get_type(&options);
	if (type == MEMEXTENT_DONATE_TYPE_TO_CHILD) {
		err = hypercall_memextent_donate_child(from, to, offset, size,
						       false);
	} else if (type == MEMEXTENT_DONATE_TYPE_TO_PARENT) {
		err = hypercall_memextent_donate_child(to, from, offset, size,
						       true);
	} else if (type == MEMEXTENT_DONATE_TYPE_TO_SIBLING) {
		err = hypercall_memextent_donate_sibling(from, to, offset,
							 size);
	} else {
		err = ERROR_ARGUMENT_INVALID;
	}

	if ((err == OK) && !memextent_donate_options_get_no_sync(&options)) {
		// The donation may have caused addrspace mappings to change.
		// Wait for completion of EL2 operations using manual lookups.
		rcu_sync();
	}

out:
	return err;
}

#else
extern int unused;
#endif

```

`hyp/mem/memextent/src/memextent.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>
#include <string.h>

#include <bitmap.h>
#include <compiler.h>
#include <list.h>
#include <memdb.h>
#include <memextent.h>
#include <object.h>
#include <panic.h>
#include <partition.h>
#include <partition_alloc.h>
#include <pgtable.h>
#include <rcu.h>
#include <spinlock.h>
#include <util.h>

#include <events/memextent.h>
#include <events/object.h>

#include <asm/cache.h>
#include <asm/cpu.h>

#include "event_handlers.h"

error_t
memextent_handle_object_create_memextent(memextent_create_t params)
{
	memextent_t *memextent = params.memextent;
	assert(memextent != NULL);
	spinlock_init(&memextent->lock);
	list_init(&memextent->children_list);

	memextent->device_mem = params.memextent_device_mem;

	return OK;
}

static bool
memextent_validate_attrs(memextent_type_t type, memextent_memtype_t memtype,
			 pgtable_access_t access)
{
	bool ret = true;

	switch (type) {
	case MEMEXTENT_TYPE_BASIC:
	case MEMEXTENT_TYPE_SPARSE:
		break;
	default:
		ret = false;
		break;
	}

	if (!ret) {
		goto out;
	}

	switch (memtype) {
	case MEMEXTENT_MEMTYPE_ANY:
	case MEMEXTENT_MEMTYPE_DEVICE:
	case MEMEXTENT_MEMTYPE_UNCACHED:
#if defined(ARCH_AARCH64_USE_S2FWB)
	case MEMEXTENT_MEMTYPE_CACHED:
#endif
		break;
#if !defined(ARCH_AARCH64_USE_S2FWB)
	// Without S2FWB, we cannot force cached mappings
	case MEMEXTENT_MEMTYPE_CACHED:
#endif
	default:
		ret = false;
		break;
	}

	if (!ret) {
		goto out;
	}

	switch (access) {
	case PGTABLE_ACCESS_X:
	case PGTABLE_ACCESS_W:
	case PGTABLE_ACCESS_R:
	case PGTABLE_ACCESS_RX:
	case PGTABLE_ACCESS_RW:
	case PGTABLE_ACCESS_RWX:
		break;
	case PGTABLE_ACCESS_NONE:
	default:
		ret = false;
		break;
	}

out:
	return ret;
}

error_t
memextent_configure(memextent_t *me, paddr_t phys_base, size_t size,
		    memextent_attrs_t attributes)
{
	error_t ret = OK;

	assert(me != NULL);

	// The address range must not wrap around the end of the address space
	if ((size == 0U) || util_add_overflows(phys_base, size - 1U)) {
		ret = ERROR_ARGUMENT_INVALID;
		goto out;
	}

	if (!util_is_baligned(phys_base, PGTABLE_VM_PAGE_SIZE) ||
	    !util_is_baligned(size, PGTABLE_VM_PAGE_SIZE)) {
		ret = ERROR_ARGUMENT_ALIGNMENT;
		goto out;
	}

	if ((memextent_attrs_get_res_0(&attributes) != 0U) ||
	    (memextent_attrs_get_append(&attributes))) {
		ret = ERROR_ARGUMENT_INVALID;
		goto out;
	}

	memextent_type_t    type    = memextent_attrs_get_type(&attributes);
	memextent_memtype_t memtype = memextent_attrs_get_memtype(&attributes);
	pgtable_access_t    access  = memextent_attrs_get_access(&attributes);
	if (!memextent_validate_attrs(type, memtype, access)) {
		ret = ERROR_ARGUMENT_INVALID;
		goto out;
	}

	me->type      = type;
	me->phys_base = phys_base;
	me->size      = size;
	me->memtype   = memtype;
	me->access    = access;

	if (me->parent != NULL) {
		object_put_memextent(me->parent);
	}

	me->parent = NULL;
out:
	return ret;
}

// FIXME:
error_t
memextent_configure_derive(memextent_t *me, memextent_t *parent, size_t offset,
			   size_t size, memextent_attrs_t attributes)
{
	error_t ret = OK;

	assert(parent != NULL);
	assert(me != NULL);

	spinlock_acquire(&parent->lock);

	if ((size == 0U) || util_add_overflows(offset, size - 1U)) {
		ret = ERROR_ARGUMENT_INVALID;
		goto out;
	}

	if (util_add_overflows(parent->phys_base, offset) ||
	    ((parent->phys_base + offset) >=
	     (parent->phys_base + parent->size)) ||
	    ((offset + size) > parent->size)) {
		ret = ERROR_ADDR_INVALID;
		goto out;
	}

	if (!util_is_baligned(offset, PGTABLE_VM_PAGE_SIZE) ||
	    !util_is_baligned(size, PGTABLE_VM_PAGE_SIZE)) {
		ret = ERROR_ARGUMENT_ALIGNMENT;
		goto out;
	}

	if ((memextent_attrs_get_res_0(&attributes) != 0U) ||
	    (memextent_attrs_get_append(&attributes))) {
		ret = ERROR_ARGUMENT_INVALID;
		goto out;
	}

	memextent_type_t    type    = memextent_attrs_get_type(&attributes);
	memextent_memtype_t memtype = memextent_attrs_get_memtype(&attributes);
	pgtable_access_t    access  = memextent_attrs_get_access(&attributes);
	if (!memextent_validate_attrs(type, memtype, access)) {
		ret = ERROR_ARGUMENT_INVALID;
		goto out;
	}

	if (!pgtable_access_check(parent->access, access)) {
		ret = ERROR_ARGUMENT_INVALID;
		goto out;
	}

	if ((parent->memtype != MEMEXTENT_MEMTYPE_ANY) &&
	    (parent->memtype != memtype)) {
		ret = ERROR_ARGUMENT_INVALID;
		goto out;
	}

	paddr_t phys_base = parent->phys_base + offset;

	me->type      = type;
	me->phys_base = phys_base;
	me->size      = size;
	me->memtype   = memtype;
	me->access    = access;

	if (me->parent != NULL) {
		object_put_memextent(me->parent);
	}

	me->parent = object_get_memextent_additional(parent);

out:
	spinlock_release(&parent->lock);

	return ret;
}

error_t
memextent_handle_object_activate_memextent(memextent_t *me)
{
	assert(me != NULL);
	error_t ret = OK;

	if (me->parent != NULL) {
		assert(!me->device_mem);

		// Check new memtype is compatible with parent type
		switch (me->parent->memtype) {
		case MEMEXTENT_MEMTYPE_ANY:
			break;
		case MEMEXTENT_MEMTYPE_DEVICE:
		case MEMEXTENT_MEMTYPE_UNCACHED:
#if defined(ARCH_AARCH64_USE_S2FWB)
		case MEMEXTENT_MEMTYPE_CACHED:
#endif
			if (me->memtype != me->parent->memtype) {
				ret = ERROR_ARGUMENT_INVALID;
			}
			break;
#if !defined(ARCH_AARCH64_USE_S2FWB)
		// Without S2FWB, we cannot force cached mappings
		case MEMEXTENT_MEMTYPE_CACHED:
#endif
		default:
			ret = ERROR_OBJECT_CONFIG;
			break;
		}
		if (ret != OK) {
			goto out;
		}

		assert((me->access & me->parent->access) == me->access);

		ret = trigger_memextent_activate_derive_event(me->type, me);
	} else {
		if (me->size == 0U) {
			ret = ERROR_OBJECT_CONFIG;
			goto out;
		}

		ret = trigger_memextent_activate_event(me->type, me);
	}

	if (ret == OK) {
		me->active = true;
	}
out:
	return ret;
}

bool
memextent_supports_donation(memextent_t *me)
{
	return trigger_memextent_supports_donation_event(me->type, me);
}

static bool
extent_range_valid(memextent_t *me, paddr_t phys, size_t size)
{
	assert(!util_add_overflows(phys, size - 1U));

	return (me->phys_base <= phys) &&
	       ((me->phys_base + (me->size - 1U)) >= (phys + (size - 1U)));
}

error_t
memextent_donate_child(memextent_t *me, size_t offset, size_t size,
		       bool reverse)
{
	error_t ret;

	if (!util_is_baligned(offset, PGTABLE_VM_PAGE_SIZE) ||
	    !util_is_baligned(size, PGTABLE_VM_PAGE_SIZE)) {
		ret = ERROR_ARGUMENT_ALIGNMENT;
		goto out;
	}

	if (util_add_overflows(me->phys_base, offset)) {
		ret = ERROR_ARGUMENT_INVALID;
		goto out;
	}

	paddr_t phys = me->phys_base + offset;

	if ((size == 0U) || util_add_overflows(phys, size - 1U)) {
		ret = ERROR_ARGUMENT_SIZE;
		goto out;
	}

	if (!extent_range_valid(me, phys, size)) {
		ret = ERROR_ARGUMENT_INVALID;
		goto out;
	}

	ret = trigger_memextent_donate_child_event(me->type, me, phys, size,
						   reverse);

out:
	return ret;
}

error_t
memextent_donate_sibling(memextent_t *from, memextent_t *to, size_t offset,
			 size_t size)
{
	error_t ret;

	if (!util_is_baligned(offset, PGTABLE_VM_PAGE_SIZE) ||
	    !util_is_baligned(size, PGTABLE_VM_PAGE_SIZE)) {
		ret = ERROR_ARGUMENT_ALIGNMENT;
		goto out;
	}

	if (util_add_overflows(from->phys_base, offset)) {
		ret = ERROR_ARGUMENT_INVALID;
		goto out;
	}

	paddr_t phys = from->phys_base + offset;

	if ((size == 0U) || util_add_overflows(phys, size - 1U)) {
		ret = ERROR_ARGUMENT_SIZE;
		goto out;
	}

	if (!extent_range_valid(from, phys, size) ||
	    !extent_range_valid(to, phys, size)) {
		ret = ERROR_ARGUMENT_INVALID;
		goto out;
	}

	if ((from == to) || (from->parent == NULL) ||
	    (from->parent != to->parent)) {
		ret = ERROR_ARGUMENT_INVALID;
		goto out;
	}

	ret = trigger_memextent_donate_sibling_event(from->type, from, to, phys,
						     size);

out:
	return ret;
}

static bool
memextent_check_map_attrs(memextent_t		   *extent,
			  memextent_mapping_attrs_t map_attrs)
{
	pgtable_access_t access_user =
		memextent_mapping_attrs_get_user_access(&map_attrs);
	pgtable_access_t access_kernel =
		memextent_mapping_attrs_get_kernel_access(&map_attrs);
	pgtable_vm_memtype_t memtype =
		memextent_mapping_attrs_get_memtype(&map_attrs);

	return (pgtable_access_check(extent->access, access_user)) &&
	       pgtable_access_check(extent->access, access_kernel) &&
	       memextent_check_memtype(extent->memtype, memtype);
}

error_t
memextent_map(memextent_t *extent, addrspace_t *addrspace, vmaddr_t vm_base,
	      memextent_mapping_attrs_t map_attrs)
{
	error_t ret;

	if (!util_is_baligned(vm_base, PGTABLE_VM_PAGE_SIZE)) {
		ret = ERROR_ARGUMENT_ALIGNMENT;
		goto out;
	}

	if (!memextent_check_map_attrs(extent, map_attrs)) {
		ret = ERROR_ARGUMENT_INVALID;
		goto out;
	}

	if (addrspace->read_only) {
		ret = ERROR_DENIED;
	} else {
		ret = trigger_memextent_map_event(
			extent->type, extent, addrspace, vm_base, map_attrs);
	}

out:
	return ret;
}

error_t
memextent_map_partial(memextent_t *extent, addrspace_t *addrspace,
		      vmaddr_t vm_base, size_t offset, size_t size,
		      memextent_mapping_attrs_t map_attrs)
{
	error_t ret;

	if (!util_is_baligned(vm_base, PGTABLE_VM_PAGE_SIZE) ||
	    !util_is_baligned(offset, PGTABLE_VM_PAGE_SIZE) ||
	    !util_is_baligned(size, PGTABLE_VM_PAGE_SIZE)) {
		ret = ERROR_ARGUMENT_ALIGNMENT;
		goto out;
	}

	if ((size == 0U) || util_add_overflows(offset, size - 1U) ||
	    util_add_overflows(vm_base, size - 1U)) {
		ret = ERROR_ARGUMENT_SIZE;
		goto out;
	}

	if ((offset + (size - 1U)) >= extent->size) {
		ret = ERROR_ARGUMENT_SIZE;
		goto out;
	}

	if (!memextent_check_map_attrs(extent, map_attrs)) {
		ret = ERROR_ARGUMENT_INVALID;
		goto out;
	}

	if (addrspace->read_only) {
		ret = ERROR_DENIED;
	} else {
		ret = trigger_memextent_map_partial_event(extent->type, extent,
							  addrspace, vm_base,
							  offset, size,
							  map_attrs);
	}

out:
	return ret;
}

error_t
memextent_unmap(memextent_t *extent, addrspace_t *addrspace, vmaddr_t vm_base)
{
	error_t ret;

	if (!util_is_baligned(vm_base, PGTABLE_VM_PAGE_SIZE)) {
		ret = ERROR_ARGUMENT_ALIGNMENT;
		goto out;
	}

	if (addrspace->read_only) {
		ret = ERROR_DENIED;
	} else {
		ret = trigger_memextent_unmap_event(extent->type, extent,
						    addrspace, vm_base);
	}

out:
	return ret;
}

error_t
memextent_unmap_partial(memextent_t *extent, addrspace_t *addrspace,
			vmaddr_t vm_base, size_t offset, size_t size)
{
	error_t ret;

	if (!util_is_baligned(vm_base, PGTABLE_VM_PAGE_SIZE) ||
	    !util_is_baligned(offset, PGTABLE_VM_PAGE_SIZE) ||
	    !util_is_baligned(size, PGTABLE_VM_PAGE_SIZE)) {
		ret = ERROR_ARGUMENT_ALIGNMENT;
		goto out;
	}

	if ((size == 0U) || util_add_overflows(offset, size - 1U) ||
	    util_add_overflows(vm_base, size - 1U)) {
		ret = ERROR_ARGUMENT_SIZE;
		goto out;
	}

	if ((offset + (size - 1U)) >= extent->size) {
		ret = ERROR_ARGUMENT_SIZE;
		goto out;
	}

	if (addrspace->read_only) {
		ret = ERROR_DENIED;
	} else {
		ret = trigger_memextent_unmap_partial_event(
			extent->type, extent, addrspace, vm_base, offset, size);
	}

out:
	return ret;
}

void
memextent_unmap_all(memextent_t *extent)
{
	if (!trigger_memextent_unmap_all_event(extent->type, extent)) {
		panic("Invalid memory extent unmap all!");
	}
}

static error_t
memextent_do_clean(paddr_t base, size_t size, void *arg)
{
	memextent_clean_flags_t *flags = (memextent_clean_flags_t *)arg;
	assert(flags != NULL);

	void *addr = partition_phys_map(base, size);
	partition_phys_access_enable(addr);

	if (memextent_clean_flags_get_zero(flags)) {
		(void)memset_s(addr, size, 0, size);
	}

	if (memextent_clean_flags_get_flush(flags)) {
		CACHE_CLEAN_INVALIDATE_RANGE((uint8_t *)addr, size);
	} else {
		CACHE_CLEAN_RANGE((uint8_t *)addr, size);
	}

	partition_phys_access_disable(addr);
	partition_phys_unmap(addr, base, size);

	return OK;
}

static error_t
memextent_clean_range(memextent_t *extent, size_t offset, size_t size,
		      memextent_clean_flags_t flags)
{
	error_t err;

	if ((extent->memtype == MEMEXTENT_MEMTYPE_DEVICE) ||
	    !pgtable_access_check(extent->access, PGTABLE_ACCESS_W)) {
		err = ERROR_DENIED;
		goto out;
	}

	if (!util_is_baligned(offset, PGTABLE_VM_PAGE_SIZE) ||
	    !util_is_baligned(size, PGTABLE_VM_PAGE_SIZE)) {
		err = ERROR_ARGUMENT_ALIGNMENT;
		goto out;
	}

	if (util_add_overflows(extent->phys_base, offset)) {
		err = ERROR_ARGUMENT_INVALID;
		goto out;
	}

	paddr_t phys = extent->phys_base + offset;

	if ((size == 0U) || util_add_overflows(phys, size - 1U)) {
		err = ERROR_ARGUMENT_SIZE;
		goto out;
	}

	if (!extent_range_valid(extent, phys, size)) {
		err = ERROR_ARGUMENT_INVALID;
		goto out;
	}

	err = memdb_range_walk((uintptr_t)extent, MEMDB_TYPE_EXTENT, phys,
			       phys + size - 1U, memextent_do_clean, &flags);

out:
	return err;
}

error_t
memextent_zero_range(memextent_t *extent, size_t offset, size_t size)
{
	memextent_clean_flags_t flags = memextent_clean_flags_default();
	memextent_clean_flags_set_zero(&flags, true);

	return memextent_clean_range(extent, offset, size, flags);
}

error_t
memextent_cache_clean_range(memextent_t *me, size_t offset, size_t size)
{
	return memextent_clean_range(me, offset, size,
				     memextent_clean_flags_default());
}

error_t
memextent_cache_flush_range(memextent_t *me, size_t offset, size_t size)
{
	memextent_clean_flags_t flags = memextent_clean_flags_default();
	memextent_clean_flags_set_flush(&flags, true);

	return memextent_clean_range(me, offset, size, flags);
}

static bool
memextent_check_access_attrs(memextent_t	     *extent,
			     memextent_access_attrs_t access_attrs)
{
	pgtable_access_t access_user =
		memextent_access_attrs_get_user_access(&access_attrs);
	pgtable_access_t access_kernel =
		memextent_access_attrs_get_kernel_access(&access_attrs);

	return (pgtable_access_check(extent->access, access_user) &&
		pgtable_access_check(extent->access, access_kernel));
}

error_t
memextent_update_access(memextent_t *extent, addrspace_t *addrspace,
			vmaddr_t vm_base, memextent_access_attrs_t access_attrs)
{
	error_t ret;

	if (!memextent_check_access_attrs(extent, access_attrs)) {
		ret = ERROR_ARGUMENT_INVALID;
		goto out;
	}

	if (!util_is_baligned(vm_base, PGTABLE_VM_PAGE_SIZE)) {
		ret = ERROR_ARGUMENT_ALIGNMENT;
		goto out;
	}

	if (addrspace->read_only) {
		ret = ERROR_DENIED;
	} else {
		ret = trigger_memextent_update_access_event(
			extent->type, extent, addrspace, vm_base, access_attrs);
	}

out:
	return ret;
}

error_t
memextent_update_access_partial(memextent_t *extent, addrspace_t *addrspace,
				vmaddr_t vm_base, size_t offset, size_t size,
				memextent_access_attrs_t access_attrs)
{
	error_t ret;

	if (!memextent_check_access_attrs(extent, access_attrs)) {
		ret = ERROR_ARGUMENT_INVALID;
		goto out;
	}

	if (!util_is_baligned(vm_base, PGTABLE_VM_PAGE_SIZE) ||
	    !util_is_baligned(offset, PGTABLE_VM_PAGE_SIZE) ||
	    !util_is_baligned(size, PGTABLE_VM_PAGE_SIZE)) {
		ret = ERROR_ARGUMENT_ALIGNMENT;
		goto out;
	}

	if ((size == 0U) || util_add_overflows(offset, size - 1U) ||
	    util_add_overflows(vm_base, size - 1U)) {
		ret = ERROR_ARGUMENT_SIZE;
		goto out;
	}

	if ((offset + (size - 1U)) >= extent->size) {
		ret = ERROR_ARGUMENT_SIZE;
		goto out;
	}

	if (addrspace->read_only) {
		ret = ERROR_DENIED;
	} else {
		ret = trigger_memextent_update_access_partial_event(
			extent->type, extent, addrspace, vm_base, offset, size,
			access_attrs);
	}

out:
	return ret;
}

bool
memextent_is_mapped(memextent_t *me, addrspace_t *addrspace, bool exclusive)
{
	assert(me != NULL);
	assert(addrspace != NULL);

	return trigger_memextent_is_mapped_event(me->type, me, addrspace,
						 exclusive);
}

void
memextent_handle_object_deactivate_memextent(memextent_t *memextent)
{
	if (!trigger_memextent_deactivate_event(memextent->type, memextent)) {
		panic("Invalid memory extent deactivate!");
	}
}

void
memextent_handle_object_cleanup_memextent(memextent_t *memextent)
{
	if (!trigger_memextent_cleanup_event(memextent->type, memextent)) {
		panic("Invalid memory extent cleanup!");
	}

	if (memextent->parent != NULL) {
		object_put_memextent(memextent->parent);
		memextent->parent = NULL;
	}
}

size_result_t
memextent_get_offset_for_pa(memextent_t *memextent, paddr_t pa, size_t size)
{
	return trigger_memextent_get_offset_for_pa_event(memextent->type,
							 memextent, pa, size);
}

#if defined(ARCH_AARCH64_USE_S2FWB)
#if !defined(ARCH_ARM_FEAT_S2FWB)
#error S2FWB requires ARCH_ARM_FEAT_S2FWB
#endif
#error S2FWB support not implemented
#endif

// FIXME: move this to arch dependent code
bool
memextent_check_memtype(memextent_memtype_t  extent_type,
			pgtable_vm_memtype_t map_type)
{
	bool success = false;

	// Check valid type
	switch (map_type) {
	case PGTABLE_VM_MEMTYPE_DEVICE_NGNRNE:
	case PGTABLE_VM_MEMTYPE_DEVICE_NGNRE:
	case PGTABLE_VM_MEMTYPE_DEVICE_NGRE:
	case PGTABLE_VM_MEMTYPE_DEVICE_GRE:
		if ((extent_type == MEMEXTENT_MEMTYPE_ANY) ||
		    (extent_type == MEMEXTENT_MEMTYPE_DEVICE) ||
		    (extent_type == MEMEXTENT_MEMTYPE_UNCACHED)) {
			success = true;
		}
		break;
	case PGTABLE_VM_MEMTYPE_NORMAL_NC:
		if ((extent_type == MEMEXTENT_MEMTYPE_ANY) ||
		    (extent_type == MEMEXTENT_MEMTYPE_UNCACHED)) {
			success = true;
		}
		break;
	case PGTABLE_VM_MEMTYPE_NORMAL_WB: {
#if defined(ARCH_AARCH64_USE_S2FWB)
		if ((extent_type == MEMEXTENT_MEMTYPE_ANY) ||
		    (extent_type == MEMEXTENT_MEMTYPE_CACHED))
#else
		if (extent_type == MEMEXTENT_MEMTYPE_ANY)
#endif
		{
			success = true;
		}
	} break;
	case PGTABLE_VM_MEMTYPE_NORMAL_WT:
	case PGTABLE_VM_MEMTYPE_NORMAL_OWT_IWB:
	case PGTABLE_VM_MEMTYPE_NORMAL_OWB_INC:
	case PGTABLE_VM_MEMTYPE_NORMAL_OWB_IWT:
	case PGTABLE_VM_MEMTYPE_NORMAL_ONC_IWT:
	case PGTABLE_VM_MEMTYPE_NORMAL_ONC_IWB:
	case PGTABLE_VM_MEMTYPE_NORMAL_OWT_INC:
		if (extent_type == MEMEXTENT_MEMTYPE_ANY) {
			success = true;
		}
		break;
	default:
		success = false;
		break;
	}

	return success;
}

memextent_ptr_result_t
memextent_derive(memextent_t *parent, paddr_t offset, size_t size,
		 memextent_memtype_t memtype, pgtable_access_t access,
		 memextent_type_t type)
{
	memextent_create_t     params_me = { .memextent		   = NULL,
					     .memextent_device_mem = false };
	memextent_ptr_result_t me_ret;
	me_ret = partition_allocate_memextent(parent->header.partition,
					      params_me);
	if (me_ret.e != OK) {
		goto out;
	}

	memextent_t	 *me	= me_ret.r;
	memextent_attrs_t attrs = memextent_attrs_default();
	memextent_attrs_set_access(&attrs, access);
	memextent_attrs_set_memtype(&attrs, memtype);
	memextent_attrs_set_type(&attrs, type);

	spinlock_acquire(&me->header.lock);

	me_ret.e = memextent_configure_derive(me, parent, offset, size, attrs);
	if (me_ret.e != OK) {
		spinlock_release(&me->header.lock);
		me_ret.r = NULL;
		object_put_memextent(me);
		goto out;
	}
	spinlock_release(&me->header.lock);

	me_ret.e = object_activate_memextent(me);
	if (me_ret.e != OK) {
		object_put_memextent(me);
		me_ret.r = NULL;
	}

out:
	return me_ret;
}

void
memextent_retain_mappings(memextent_t *me) LOCK_IMPL
{
	(void)trigger_memextent_retain_mappings_event(me->type, me);
}

void
memextent_release_mappings(memextent_t *me, bool clear) LOCK_IMPL
{
	(void)trigger_memextent_release_mappings_event(me->type, me, clear);
}

memextent_mapping_t
memextent_lookup_mapping(memextent_t *me, paddr_t phys, size_t size, index_t i)
{
	memextent_mapping_result_t ret;

	ret = trigger_memextent_lookup_mapping_event(me->type, me, phys, size,
						     i);
	assert(ret.e == OK);

	return ret.r;
}

error_t
memextent_attach(partition_t *owner, memextent_t *me, uintptr_t hyp_va,
		 size_t size)
{
	assert(owner != NULL);
	assert(me != NULL);

	error_t ret = OK;

	if (owner != me->header.partition) {
		ret = ERROR_DENIED;
		goto out;
	}

	if (!pgtable_access_check(me->access, PGTABLE_ACCESS_RW)) {
		ret = ERROR_DENIED;
		goto out;
	}

	if (me->size < size) {
		ret = ERROR_ARGUMENT_SIZE;
		goto out;
	}

	pgtable_hyp_memtype_t memtype;
	switch (me->memtype) {
	case MEMEXTENT_MEMTYPE_CACHED:
	case MEMEXTENT_MEMTYPE_ANY:
		memtype = PGTABLE_HYP_MEMTYPE_WRITEBACK;
		break;
	case MEMEXTENT_MEMTYPE_DEVICE:
		memtype = PGTABLE_HYP_MEMTYPE_DEVICE;
		break;
	case MEMEXTENT_MEMTYPE_UNCACHED:
		memtype = PGTABLE_HYP_MEMTYPE_WRITECOMBINE;
		break;
	default:
		ret = ERROR_ARGUMENT_INVALID;
		break;
	}
	if (ret == ERROR_ARGUMENT_INVALID) {
		goto out;
	}

	ret = trigger_memextent_attach_event(me->type, me, hyp_va, size,
					     memtype);
out:
	return ret;
}

void
memextent_detach(partition_t *owner, memextent_t *me)
{
	assert(owner != NULL);
	assert(me != NULL);
	assert(owner == me->header.partition);

	bool handled = trigger_memextent_detach_event(me->type, me);
	assert(handled);
}

```

`hyp/mem/memextent/src/memextent_basic.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>
#include <string.h>

#include <hypcontainers.h>

#include <addrspace.h>
#include <bitmap.h>
#include <compiler.h>
#include <list.h>
#include <memdb.h>
#include <memextent.h>
#include <object.h>
#include <panic.h>
#include <partition.h>
#include <partition_alloc.h>
#include <pgtable.h>
#include <rcu.h>
#include <spinlock.h>
#include <util.h>

#include <events/memextent.h>

#include "event_handlers.h"

static error_t
allocate_mappings(memextent_t *me)
{
	error_t	     ret       = OK;
	partition_t *partition = me->header.partition;
	const size_t alloc_size =
		sizeof(memextent_basic_mapping_t) * MEMEXTENT_MAX_MAPS;
	const size_t alloc_align = alignof(memextent_basic_mapping_t);

	void_ptr_result_t alloc_ret =
		partition_alloc(partition, alloc_size, alloc_align);
	if (alloc_ret.e != OK) {
		ret = alloc_ret.e;
		goto out;
	}

	(void)memset_s(alloc_ret.r, alloc_size, 0, alloc_size);

	me->mappings.basic = alloc_ret.r;

out:
	return ret;
}

static void
free_mappings(memextent_t *me)
{
	partition_t *partition = me->header.partition;
	const size_t alloc_size =
		sizeof(memextent_basic_mapping_t) * MEMEXTENT_MAX_MAPS;

	assert(me->mappings.basic != NULL);

	(void)partition_free(partition, me->mappings.basic, alloc_size);

	me->mappings.basic = NULL;
}

// Needs to be called holding a reference to the addrspace to be used
static error_t
memextent_do_map(memextent_t *me, memextent_basic_mapping_t *map, size_t offset,
		 size_t size)
{
	assert((me != NULL) && (map != NULL));
	assert((size > 0U) && (size <= me->size));
	assert(!util_add_overflows(me->phys_base, offset));
	assert(!util_add_overflows(map->vbase, offset));
	assert(!util_add_overflows(me->phys_base + offset, size - 1U));
	assert(!util_add_overflows(map->vbase + offset, size - 1U));

	addrspace_t *const s = atomic_load_relaxed(&map->addrspace);
	assert((s != NULL) && !s->read_only);

	return addrspace_map(
		s, map->vbase + offset, size, me->phys_base + offset,
		memextent_mapping_attrs_get_memtype(&map->attrs),
		memextent_mapping_attrs_get_kernel_access(&map->attrs),
		memextent_mapping_attrs_get_user_access(&map->attrs));
}

// Needs to be called holding a reference to the addrspace to be used
static void
memextent_remove_map_from_addrspace_list(memextent_basic_mapping_t *map)
{
	assert(map != NULL);

	addrspace_t *as = atomic_load_relaxed(&map->addrspace);
	assert(as != NULL);

	spinlock_acquire(&as->mapping_list_lock);
	(void)list_delete_node(&as->basic_mapping_list,
			       &map->mapping_list_node);
	spinlock_release(&as->mapping_list_lock);

	atomic_store_relaxed(&map->addrspace, NULL);
}

error_t
memextent_activate_basic(memextent_t *me)
{
	error_t	     ret;
	partition_t *hyp_partition = partition_get_private();

	assert(me != NULL);
	assert(hyp_partition != NULL);

	ret = allocate_mappings(me);
	if (ret != OK) {
		goto out;
	}

	if (me->device_mem) {
		assert(me->memtype == MEMEXTENT_MEMTYPE_DEVICE);

		ret = memdb_insert(hyp_partition, me->phys_base,
				   me->phys_base + (me->size - 1U),
				   (uintptr_t)me, MEMDB_TYPE_EXTENT);
	} else {
		partition_t *partition = me->header.partition;
		assert(partition != NULL);

		ret = memdb_update(hyp_partition, me->phys_base,
				   me->phys_base + (me->size - 1U),
				   (uintptr_t)me, MEMDB_TYPE_EXTENT,
				   (uintptr_t)partition, MEMDB_TYPE_PARTITION);

		if (ret == ERROR_MEMDB_NOT_OWNER) {
			// We might have failed to take ownership
			// because a previously deleted memextent has
			// not yet been cleaned up, so wait for an RCU
			// grace period and then retry. If it still
			// fails after that, there's a real conflict.
			rcu_sync();
			ret = memdb_update(hyp_partition, me->phys_base,
					   me->phys_base + (me->size - 1U),
					   (uintptr_t)me, MEMDB_TYPE_EXTENT,
					   (uintptr_t)partition,
					   MEMDB_TYPE_PARTITION);
		}
	}

	if (ret != OK) {
		free_mappings(me);
	}

out:
	return ret;
}

static void
memextent_revert_activation_mappings(memextent_t *me,
				     partition_t *hyp_partition)
	REQUIRE_SPINLOCK(me->parent->lock) REQUIRE_LOCK(me->parent->mappings)
{
	error_t err;
	for (index_t i = 0; i < MEMEXTENT_MAX_MAPS; i++) {
		memextent_basic_mapping_t *map = &me->mappings.basic[i];

		addrspace_t *as = atomic_load_relaxed(&map->addrspace);
		if (as == NULL) {
			continue;
		}

		memextent_mapping_t parent_map = memextent_lookup_mapping(
			me->parent, me->phys_base, me->size, i);
		assert(as == parent_map.addrspace);

		if (!memextent_mapping_attrs_is_equal(map->attrs,
						      parent_map.attrs)) {
			map->attrs = parent_map.attrs;

			err = memextent_do_map(me, map, 0, me->size);
			assert(err == OK);
		}

		memextent_remove_map_from_addrspace_list(map);
	}

	// Revert the earlier memdb update.
	err = memdb_update(hyp_partition, me->phys_base,
			   me->phys_base + (me->size - 1U),
			   (uintptr_t)me->parent, MEMDB_TYPE_EXTENT,
			   (uintptr_t)me, MEMDB_TYPE_EXTENT);
	assert(err == OK);
}

error_t
memextent_activate_derive_basic(memextent_t *me)
{
	error_t	     ret	   = OK;
	partition_t *hyp_partition = partition_get_private();

	assert(me != NULL);
	assert(me->parent != NULL);

	ret = allocate_mappings(me);
	if (ret != OK) {
		goto out;
	}

	bool retried = false;
	while (1) {
		spinlock_acquire(&me->parent->lock);

		if (me->parent->attached_size != 0U) {
			ret = ERROR_BUSY;
			goto out_locked_parent;
		}

		// Take the mapping lock before the memdb update, because we
		// haven't set up the mapping pointers yet. We do that after the
		// memdb update so we don't have to undo them if the memdb
		// update fails.
		spinlock_acquire_nopreempt(&me->lock);

		ret = memdb_update(hyp_partition, me->phys_base,
				   me->phys_base + (me->size - 1U),
				   (uintptr_t)me, MEMDB_TYPE_EXTENT,
				   (uintptr_t)me->parent, MEMDB_TYPE_EXTENT);
		if (ret == OK) {
			break;
		}
		if ((ret != ERROR_MEMDB_NOT_OWNER) || retried) {
			goto out_locked;
		}

		// We might have failed to take ownership because a previously
		// deleted memextent has not yet been cleaned up, so drop the
		// locks, wait for an RCU grace period, and then retry. If it
		// still fails after that, there's a real conflict.
		spinlock_release_nopreempt(&me->lock);
		spinlock_release(&me->parent->lock);
		rcu_sync();
		retried = true;
	}

	memextent_retain_mappings(me->parent);

	for (index_t i = 0U; (i < MEMEXTENT_MAX_MAPS); i++) {
		memextent_basic_mapping_t *map = &me->mappings.basic[i];

		memextent_mapping_t parent_map = memextent_lookup_mapping(
			me->parent, me->phys_base, me->size, i);
		if (parent_map.size != me->size) {
			// The parent is partially mapped over the child's
			// range; we cannot handle this with a basic memextent.
			ret = ERROR_DENIED;
			break;
		}

		addrspace_t *as = parent_map.addrspace;
		if (as == NULL) {
			continue;
		}

		atomic_store_relaxed(&map->addrspace, as);
		map->vbase = parent_map.vbase;
		map->attrs = parent_map.attrs;

		spinlock_acquire_nopreempt(&as->mapping_list_lock);
		list_insert_at_head(&as->basic_mapping_list,
				    &map->mapping_list_node);
		spinlock_release_nopreempt(&as->mapping_list_lock);

		pgtable_access_t access_user =
			memextent_mapping_attrs_get_user_access(&map->attrs);
		pgtable_access_t access_kernel =
			memextent_mapping_attrs_get_kernel_access(&map->attrs);

		// Reduce access rights on the map
		memextent_mapping_attrs_set_user_access(
			&map->attrs,
			pgtable_access_mask(access_user, me->access));
		memextent_mapping_attrs_set_kernel_access(
			&map->attrs,
			pgtable_access_mask(access_kernel, me->access));

		// If accesses are the same then mapping can be inherited from
		// parent, if not, remap memextent to update access.
		if (!memextent_mapping_attrs_is_equal(map->attrs,
						      parent_map.attrs)) {
			ret = memextent_do_map(me, map, 0, me->size);
			if (ret != OK) {
				memextent_remove_map_from_addrspace_list(map);
				break;
			}
		}
	}

	if (ret != OK) {
		// Revert any remappings that were made.
		memextent_revert_activation_mappings(me, hyp_partition);
	}

	memextent_release_mappings(me->parent, false);

	list_insert_at_head(&me->parent->children_list,
			    &me->children_list_node);

out_locked:
	spinlock_release_nopreempt(&me->lock);
out_locked_parent:
	spinlock_release(&me->parent->lock);

	if (ret != OK) {
		free_mappings(me);
	}

out:
	return ret;
}

// Needs to be called holding a reference to the addrspace to be used
static void
memextent_do_unmap(memextent_t *me, memextent_basic_mapping_t *map,
		   size_t offset, size_t size)
{
	assert((me != NULL) && (map != NULL));
	assert((size > 0U) && (size <= me->size));
	assert(!util_add_overflows(map->vbase, offset));
	assert(!util_add_overflows(map->vbase + offset, size - 1U));

	addrspace_t *const s = atomic_load_relaxed(&map->addrspace);
	assert((s != NULL) && !s->read_only);

	error_t err = addrspace_unmap(s, map->vbase + offset, size,
				      me->phys_base + offset);
	assert(err == OK);
}

static error_t
memextent_map_range(paddr_t base, size_t size, void *arg)
{
	error_t ret = OK;

	if ((size == 0U) || (util_add_overflows(base, size - 1))) {
		ret = ERROR_ARGUMENT_SIZE;
		goto error;
	}

	assert(arg != NULL);

	memextent_basic_arg_t *args = (memextent_basic_arg_t *)arg;

	assert((args->me != NULL) && (args->map[0] != NULL));

	size_t offset = base - args->me->phys_base;

	ret = memextent_do_map(args->me, args->map[0], offset, size);
	if (ret != OK) {
		args->failed_address = base;
	}

error:
	return ret;
}

static error_t
memextent_unmap_range(paddr_t base, size_t size, void *arg)
{
	error_t ret = OK;

	if ((size == 0U) || (util_add_overflows(base, size - 1))) {
		ret = ERROR_ARGUMENT_SIZE;
		goto error;
	}

	assert(arg != NULL);

	memextent_basic_arg_t *args = (memextent_basic_arg_t *)arg;

	assert((args->me != NULL) && (args->map[0] != NULL));

	size_t	offset = base - args->me->phys_base;
	index_t i      = 0;

	while ((args->map[i] != NULL) && (i < util_array_size(args->map))) {
		memextent_do_unmap(args->me, args->map[i], offset, size);
		i++;
	}

error:
	return ret;
}

error_t
memextent_map_basic(memextent_t *me, addrspace_t *addrspace, vmaddr_t vm_base,
		    memextent_mapping_attrs_t map_attrs)
{
	assert((me != NULL) && (addrspace != NULL));

	error_t ret	      = OK;
	bool	mappings_full = true;

	if (util_add_overflows(vm_base, me->size - 1U)) {
		ret = ERROR_ADDR_OVERFLOW;
		goto out;
	}

	spinlock_acquire(&me->lock);

	memextent_basic_mapping_t *map = NULL;
	for (index_t i = 0; i < MEMEXTENT_MAX_MAPS; i++) {
		map = &me->mappings.basic[i];

		// The mapping may have been used by a now deactivated
		// addrspace; use a load-acquire to ensure we observe the
		// removal from the addrspace's mapping list in
		// memextent_deactivate_addrspace_basic().
		if (atomic_load_acquire(&map->addrspace) == NULL) {
			mappings_full = false;
			break;
		}
	}

	if (mappings_full) {
		ret = ERROR_MEMEXTENT_MAPPINGS_FULL;
		goto out_locked;
	}

	pgtable_access_t access_user =
		memextent_mapping_attrs_get_user_access(&map_attrs);
	pgtable_access_t access_kernel =
		memextent_mapping_attrs_get_kernel_access(&map_attrs);
	pgtable_vm_memtype_t memtype =
		memextent_mapping_attrs_get_memtype(&map_attrs);

	// Add mapping to address space's list
	spinlock_acquire_nopreempt(&addrspace->mapping_list_lock);
	list_insert_at_head(&addrspace->basic_mapping_list,
			    &map->mapping_list_node);
	spinlock_release_nopreempt(&addrspace->mapping_list_lock);

	atomic_store_relaxed(&map->addrspace, addrspace);
	map->vbase = vm_base;

	memextent_mapping_attrs_set_memtype(&map->attrs, memtype);
	memextent_mapping_attrs_set_user_access(&map->attrs, access_user);
	memextent_mapping_attrs_set_kernel_access(&map->attrs, access_kernel);

	if (list_is_empty(&me->children_list)) {
		ret = memextent_do_map(me, map, 0, me->size);
		goto out_mapping_recorded;
	}

	memextent_basic_arg_t arg = { me, { map }, 0 };

	// Walk through the memory extent physical range and map the contiguous
	// ranges it owns.
	ret = memdb_range_walk((uintptr_t)me, MEMDB_TYPE_EXTENT, me->phys_base,
			       me->phys_base + (me->size - 1U),
			       memextent_map_range, (void *)&arg);

	// If a range failed to be mapped, we need to rollback and unmap the
	// ranges that have already been mapped
	if ((ret != OK) && (arg.failed_address != me->phys_base)) {
		(void)memdb_range_walk((uintptr_t)me, MEMDB_TYPE_EXTENT,
				       me->phys_base, arg.failed_address - 1U,
				       memextent_unmap_range, (void *)&arg);
	}

out_mapping_recorded:
	// If mapping failed, clear the map structure.
	if (ret != OK) {
		spinlock_acquire_nopreempt(&addrspace->mapping_list_lock);
		(void)list_delete_node(&addrspace->basic_mapping_list,
				       &map->mapping_list_node);
		spinlock_release_nopreempt(&addrspace->mapping_list_lock);
		atomic_store_relaxed(&map->addrspace, NULL);
	}
out_locked:
	spinlock_release(&me->lock);
out:
	return ret;
}

error_t
memextent_unmap_basic(memextent_t *me, addrspace_t *addrspace, vmaddr_t vm_base)
{
	assert((me != NULL) && (addrspace != NULL));

	error_t ret		     = OK;
	bool	addrspace_not_mapped = true;

	spinlock_acquire(&me->lock);

	memextent_basic_mapping_t *map = NULL;
	for (index_t i = 0; i < MEMEXTENT_MAX_MAPS; i++) {
		map = &me->mappings.basic[i];

		if ((atomic_load_relaxed(&map->addrspace) == addrspace) &&
		    (map->vbase == vm_base)) {
			addrspace_not_mapped = false;
			break;
		}
	}

	if (addrspace_not_mapped) {
		ret = ERROR_ADDR_INVALID;
		goto out;
	}

	if (list_is_empty(&me->children_list)) {
		memextent_do_unmap(me, map, 0, me->size);
	} else {
		memextent_basic_arg_t arg = { me, { map }, 0 };

		// Walk through the memory extent physical range and unmap the
		// contiguous ranges it owns.
		ret = memdb_range_walk((uintptr_t)me, MEMDB_TYPE_EXTENT,
				       me->phys_base,
				       me->phys_base + (me->size - 1U),
				       memextent_unmap_range, (void *)&arg);
	}

	assert(ret == OK);
	memextent_remove_map_from_addrspace_list(map);
out:
	spinlock_release(&me->lock);
	return ret;
}

bool
memextent_unmap_all_basic(memextent_t *me)
{
	assert(me != NULL);

	memextent_basic_arg_t arg   = { me, { NULL }, 0 };
	index_t		      index = 0;

	spinlock_acquire(&me->lock);

	// RCU protects ->addrspace
	rcu_read_start();
	for (index_t j = 0; j < MEMEXTENT_MAX_MAPS; j++) {
		memextent_basic_mapping_t *map = &me->mappings.basic[j];

		addrspace_t *addrspace = atomic_load_consume(&map->addrspace);
		if (addrspace != NULL) {
			// Take a reference to the address space to ensure that
			// we don't race with its destruction.
			if (!object_get_addrspace_safe(addrspace)) {
				continue;
			}

			if (list_is_empty(&me->children_list)) {
				memextent_do_unmap(me, map, 0, me->size);
				memextent_remove_map_from_addrspace_list(map);
				object_put_addrspace(addrspace);
			} else {
				arg.map[index] = map;
				index++;
			}
		}
	}
	rcu_read_finish();

	if (index != 0U) {
		assert(!list_is_empty(&me->children_list));

		// Walk through the memory extent physical range and unmap the
		// contiguous ranges it owns.
		error_t ret = memdb_range_walk((uintptr_t)me, MEMDB_TYPE_EXTENT,
					       me->phys_base,
					       me->phys_base + (me->size - 1U),
					       memextent_unmap_range,
					       (void *)&arg);
		assert(ret == OK);

		// Remove mapping from their corresponding address space's list
		for (index_t j = 0; j < index; j++) {
			memextent_basic_mapping_t *map = arg.map[j];
			assert(map != NULL);

			addrspace_t *as = atomic_load_relaxed(&map->addrspace);
			assert(as != NULL);

			memextent_remove_map_from_addrspace_list(map);
			object_put_addrspace(as);
		}
	}

	spinlock_release(&me->lock);

	return true;
}

error_t
memextent_update_access_basic(memextent_t *me, addrspace_t *addrspace,
			      vmaddr_t		       vm_base,
			      memextent_access_attrs_t access_attrs)
{
	assert((me != NULL) && (addrspace != NULL));

	error_t ret		     = OK;
	bool	addrspace_not_mapped = true;

	memextent_basic_mapping_t *map = NULL;

	spinlock_acquire(&me->lock);

	for (index_t j = 0; j < MEMEXTENT_MAX_MAPS; j++) {
		map = &me->mappings.basic[j];

		if ((atomic_load_relaxed(&map->addrspace) == addrspace) &&
		    (map->vbase == vm_base)) {
			addrspace_not_mapped = false;
			break;
		}
	}

	if (addrspace_not_mapped) {
		ret = ERROR_ADDR_INVALID;
		goto out;
	}

	memextent_mapping_attrs_t old_attrs = map->attrs;

	pgtable_access_t access_user =
		memextent_access_attrs_get_user_access(&access_attrs);
	pgtable_access_t access_kernel =
		memextent_access_attrs_get_kernel_access(&access_attrs);

	memextent_mapping_attrs_set_user_access(&map->attrs, access_user);
	memextent_mapping_attrs_set_kernel_access(&map->attrs, access_kernel);

	if (list_is_empty(&me->children_list)) {
		ret = memextent_do_map(me, map, 0, me->size);
		if (ret != OK) {
			// Restore the old mapping attributes.
			map->attrs = old_attrs;
		}
	} else {
		memextent_basic_arg_t arg = { me, { map }, 0 };

		// Walk through the memory extent physical range and remap the
		// contiguous ranges it owns with the new mapping attributes.
		ret = memdb_range_walk((uintptr_t)me, MEMDB_TYPE_EXTENT,
				       me->phys_base,
				       me->phys_base + (me->size - 1U),
				       memextent_map_range, (void *)&arg);

		// If a range failed to be remapped, we need to rollback and
		// remap the modified ranges with the original attributes.
		if (ret != OK) {
			map->attrs = old_attrs;
			if (arg.failed_address != me->phys_base) {
				(void)memdb_range_walk(
					(uintptr_t)me, MEMDB_TYPE_EXTENT,
					me->phys_base, arg.failed_address - 1U,
					memextent_map_range, (void *)&arg);
			}
		}
	}

out:
	spinlock_release(&me->lock);

	return ret;
}

bool
memextent_is_mapped_basic(memextent_t *me, addrspace_t *addrspace,
			  bool exclusive)
{
	bool ret = false;

	for (index_t i = 0; i < MEMEXTENT_MAX_MAPS; i++) {
		memextent_basic_mapping_t *map = &me->mappings.basic[i];

		addrspace_t *as = atomic_load_relaxed(&map->addrspace);
		if (as == addrspace) {
			ret = true;
		} else if (as != NULL) {
			ret = false;
		} else {
			continue;
		}

		if (ret != exclusive) {
			break;
		}
	}

	return ret;
}

// Revert mappings of extent to the parent, assuming that the extent has no
// children.
static void
memextent_restore_parent_mappings(memextent_t *me)
{
	assert((me != NULL) && (me->parent != NULL));

	memextent_t *parent = me->parent;

	memextent_mapping_t child_maps[MEMEXTENT_MAX_MAPS]  = { 0 };
	memextent_mapping_t parent_maps[MEMEXTENT_MAX_MAPS] = { 0 };

	spinlock_acquire(&parent->lock);
	spinlock_acquire_nopreempt(&me->lock);

	memextent_retain_mappings(me);
	memextent_retain_mappings(parent);

	for (index_t i = 0; i < MEMEXTENT_MAX_MAPS; i++) {
		child_maps[i] = memextent_lookup_mapping(me, me->phys_base,
							 me->size, i);
	}

	size_t offset = 0U;
	while (offset < me->size) {
		paddr_t phys = me->phys_base + offset;
		size_t	size = me->size - offset;

		bool child_match[MEMEXTENT_MAX_MAPS]  = { 0 };
		bool parent_match[MEMEXTENT_MAX_MAPS] = { 0 };

		for (index_t i = 0; i < MEMEXTENT_MAX_MAPS; i++) {
			parent_maps[i] =
				memextent_lookup_mapping(parent, phys, size, i);

			memextent_mapping_t *pmap = &parent_maps[i];

			// We only want to revert the range covered by the
			// parent's smallest mapping (or unmapped range).
			size = util_min(pmap->size, size);

			if (pmap->addrspace == NULL) {
				continue;
			}

			for (index_t j = 0; j < MEMEXTENT_MAX_MAPS; j++) {
				memextent_mapping_t *cmap = &child_maps[j];

				if ((cmap->addrspace == NULL) ||
				    (cmap->addrspace != pmap->addrspace)) {
					continue;
				}

				bool vbase_match = cmap->vbase == pmap->vbase;
				bool attrs_match =
					memextent_mapping_attrs_is_equal(
						cmap->attrs, pmap->attrs);

				// We only need to unmap the child's mapping if
				// the vbase does not match. If vbase matches
				// but attrs don't, applying the parent's
				// mapping will overwrite the child's.
				parent_match[i] = vbase_match && attrs_match;
				child_match[j]	= vbase_match;
			}
		}

		for (index_t i = 0; i < MEMEXTENT_MAX_MAPS; i++) {
			memextent_mapping_t *cmap = &child_maps[i];
			memextent_mapping_t *pmap = &parent_maps[i];

			if ((cmap->addrspace != NULL) && !child_match[i]) {
				error_t err = addrspace_unmap(cmap->addrspace,
							      cmap->vbase, size,
							      phys);
				assert(err == OK);
			}

			if ((pmap->addrspace != NULL) && !parent_match[i]) {
				pgtable_vm_memtype_t memtype =
					memextent_mapping_attrs_get_memtype(
						&pmap->attrs);
				pgtable_access_t kernel_access =
					memextent_mapping_attrs_get_kernel_access(
						&pmap->attrs);
				pgtable_access_t user_access =
					memextent_mapping_attrs_get_user_access(
						&pmap->attrs);

				error_t err = addrspace_map(pmap->addrspace,
							    pmap->vbase, size,
							    phys, memtype,
							    kernel_access,
							    user_access);
				if (err != OK) {
					panic("Failed revert to parent mapping");
				}
			}
		}

		offset += size;
	}

	memextent_release_mappings(parent, false);
	memextent_release_mappings(me, true);

	spinlock_release_nopreempt(&me->lock);
	spinlock_release(&parent->lock);
}

bool
memextent_deactivate_basic(memextent_t *me)
{
	assert(me != NULL);

	// There should be no children by this time
	assert(list_is_empty(&me->children_list));

	if (me->parent != NULL) {
		memextent_restore_parent_mappings(me);
	} else {
		(void)memextent_unmap_all_basic(me);
	}

	return true;
}

bool
memextent_cleanup_basic(memextent_t *me)
{
	assert(me != NULL);

	if (!me->active) {
		// not active; we haven't claimed ownership of any memory
		goto out;
	}

	// release ownership of the range
	void	    *new_owner;
	memdb_type_t new_type;

	memextent_t *parent = me->parent;
	if (me->parent != NULL) {
		new_owner = parent;
		new_type  = MEMDB_TYPE_EXTENT;
	} else {
		new_owner = me->header.partition;
		new_type  = MEMDB_TYPE_PARTITION;
	}

	partition_t *hyp_partition = partition_get_private();

	error_t err = memdb_update(hyp_partition, me->phys_base,
				   me->phys_base + (me->size - 1U),
				   (uintptr_t)new_owner, new_type,
				   (uintptr_t)me, MEMDB_TYPE_EXTENT);
	assert(err == OK);

	// Remove extent from parent's children list
	if (parent != NULL) {
		spinlock_acquire(&parent->lock);
		(void)list_delete_node(&parent->children_list,
				       &me->children_list_node);
		spinlock_release(&parent->lock);
	}

	free_mappings(me);

out:
	return true;
}

bool
memextent_retain_mappings_basic(memextent_t *me)
{
	assert(me != NULL);

	// RCU protects ->addrspace
	rcu_read_start();
	for (index_t i = 0; i < MEMEXTENT_MAX_MAPS; i++) {
		memextent_basic_mapping_t *map = &me->mappings.basic[i];

		addrspace_t *as = atomic_load_consume(&map->addrspace);
		if ((as != NULL) && object_get_addrspace_safe(as)) {
			map->retained = true;
		}
	}
	rcu_read_finish();

	return true;
}

bool
memextent_release_mappings_basic(memextent_t *me, bool clear)
{
	assert(me != NULL);

	for (index_t i = 0; i < MEMEXTENT_MAX_MAPS; i++) {
		memextent_basic_mapping_t *map = &me->mappings.basic[i];

		if (!map->retained) {
			continue;
		}

		addrspace_t *as = atomic_load_relaxed(&map->addrspace);
		assert(as != NULL);

		if (clear) {
			memextent_remove_map_from_addrspace_list(map);
		}

		object_put_addrspace(as);
		map->retained = false;
	}

	return true;
}

memextent_mapping_result_t
memextent_lookup_mapping_basic(memextent_t *me, paddr_t phys, size_t size,
			       index_t i)
{
	assert(me != NULL);
	assert(i < MEMEXTENT_MAX_MAPS);
	assert((phys >= me->phys_base) &&
	       ((phys + (size - 1U)) <= (me->phys_base + (me->size - 1U))));

	memextent_mapping_t ret = {
		.size = size,
	};

	memextent_basic_mapping_t *map = &me->mappings.basic[i];

	if (map->retained) {
		addrspace_t *as = atomic_load_relaxed(&map->addrspace);
		assert(as != NULL);

		ret.addrspace = as;
		ret.vbase     = map->vbase + (phys - me->phys_base);
		ret.attrs     = map->attrs;
	}

	return memextent_mapping_result_ok(ret);
}

error_t
memextent_create_addrspace_basic(addrspace_create_t params)
{
	addrspace_t *addrspace = params.addrspace;
	assert(addrspace != NULL);

	list_init(&addrspace->basic_mapping_list);

	return OK;
}

error_t
memextent_attach_basic(memextent_t *me, uintptr_t hyp_va, size_t size,
		       pgtable_hyp_memtype_t memtype)
{
	error_t ret;

	assert(me != NULL);

	spinlock_acquire(&me->lock);

	if (!list_is_empty(&me->children_list)) {
		ret = ERROR_BUSY;
		goto out_locked;
	}

	pgtable_hyp_start();
	ret = pgtable_hyp_map(me->header.partition, hyp_va, size, me->phys_base,
			      memtype, PGTABLE_ACCESS_RW,
			      VMSA_SHAREABILITY_INNER_SHAREABLE);
	pgtable_hyp_commit();

	if (ret == OK) {
		me->attached_address = hyp_va;
		me->attached_size    = size;
	}

out_locked:
	spinlock_release(&me->lock);

	return ret;
}

bool
memextent_detach_basic(memextent_t *me)
{
	assert(me != NULL);

	spinlock_acquire(&me->lock);
	assert(me->attached_size != 0);

	pgtable_hyp_start();
	pgtable_hyp_unmap(me->header.partition, me->attached_address,
			  me->attached_size, me->attached_size);
	pgtable_hyp_commit();

	me->attached_size = 0;
	spinlock_release(&me->lock);

	return true;
}

void
memextent_deactivate_addrspace_basic(addrspace_t *addrspace)
{
	assert(addrspace != NULL);

	spinlock_acquire(&addrspace->mapping_list_lock);

	list_t *list = &addrspace->basic_mapping_list;

	// Remove all mappings from addrspace
	memextent_basic_mapping_t *map = NULL;
	list_foreach_container_maydelete (map, list, memextent_basic_mapping,
					  mapping_list_node) {
		(void)list_delete_node(list, &map->mapping_list_node);
		// We use a store-release to ensure that this list deletion is
		// observed before using this mapping for another addrspace in
		// memextent_map_basic().
		atomic_store_release(&map->addrspace, NULL);
	}

	spinlock_release(&addrspace->mapping_list_lock);
}

size_result_t
memextent_get_offset_for_pa_basic(memextent_t *me, paddr_t pa, size_t size)
{
	size_result_t ret;

	if (util_add_overflows(pa, size - 1U)) {
		ret = size_result_error(ERROR_ADDR_OVERFLOW);
	} else if ((pa < me->phys_base) ||
		   ((pa + size - 1U) > (me->phys_base + me->size - 1U))) {
		ret = size_result_error(ERROR_ADDR_INVALID);
	} else {
		ret = size_result_ok(pa - me->phys_base);
	}

	return ret;
}

```

`hyp/mem/memextent/src/memextent_tests.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#if defined(UNIT_TESTS)
#include <assert.h>
#include <hyptypes.h>

#include <addrspace.h>
#include <atomic.h>
#include <bitmap.h>
#include <compiler.h>
#include <cpulocal.h>
#include <list.h>
#include <log.h>
#include <memdb.h>
#include <memextent.h>
#include <object.h>
#include <panic.h>
#include <partition.h>
#include <partition_alloc.h>
#include <partition_init.h>
#include <pgtable.h>
#include <spinlock.h>
#include <trace.h>

#include <events/object.h>

#include "event_handlers.h"

extern count_t tests_memextent_count;
count_t	       tests_memextent_count;

extern spinlock_t test_memextent_spinlock;
spinlock_t	  test_memextent_spinlock;

static addrspace_t *as;
static addrspace_t *as2;

static partition_t *partition;

#if !defined(NDEBUG)
extern void
pgtable_vm_dump(pgtable_vm_t *pgtable);
#endif

rcu_update_status_t
partition_destroy_memextent(rcu_entry_t *entry);

void
tests_memextent_init(void)
{
	spinlock_init(&test_memextent_spinlock);

	partition = partition_get_root();

	addrspace_ptr_result_t ret;

	addrspace_create_t params = { NULL };

	ret = partition_allocate_addrspace(partition, params);
	if (ret.e != OK) {
		panic("Failed address space creation");
	}

	as = ret.r;

	addrspace_create_t params2 = { NULL };

	ret = partition_allocate_addrspace(partition, params2);
	if (ret.e != OK) {
		panic("Failed address space 2 creation");
	}

	as2 = ret.r;

	// Dummy vmids
	if (addrspace_configure(as, 65U) != OK) {
		panic("Failed addrspace configuration");
	}

	if (addrspace_configure(as2, 66U) != OK) {
		panic("Failed addrspace 2 configuration");
	}

	if (object_activate_addrspace(as) != OK) {
		panic("Failed addrspace activation");
	}

	if (object_activate_addrspace(as2) != OK) {
		panic("Failed addrspace 2 activation");
	}
}

static error_t
get_free_mem_range(paddr_t base, size_t size, void *arg)
{
	test_free_range_t *free_range = (test_free_range_t *)arg;

	if (free_range->count >= util_array_size(free_range->phys_base)) {
		panic("Too many free ranges");
	}

	free_range->phys_base[free_range->count] = base;
	free_range->size[free_range->count]	 = size;
	free_range->count++;

	return OK;
}

static paddr_t
tests_find_free_range(void)
{
	error_t err = OK;

	// Get free ranges of physical memory from partition
	test_free_range_t free_range = { { 0 }, { 0 }, 0, { 0 } };

	err = memdb_walk((uintptr_t)partition, MEMDB_TYPE_PARTITION,
			 get_free_mem_range, (void *)&free_range);
	if (err != OK) {
		panic("Failed mem walk");
	}

	bool	free_range_found = false;
	paddr_t phys_base	 = 0;

	// Find a range that is big enough to contain the extents
	for (index_t i = 0; i < free_range.count; i++) {
		if (free_range.size[i] >= (4096 * 6)) {
			phys_base	 = free_range.phys_base[0];
			free_range_found = true;
			break;
		}
	}

	if (!free_range_found) {
		panic("No free range big enough");
	}

	return phys_base;
}

static memextent_t *
create_memextent(paddr_t phys_base, size_t size, memextent_memtype_t memtype,
		 pgtable_access_t access)
{
	memextent_ptr_result_t me_ret;
	memextent_create_t     params = { 0 };

	me_ret = partition_allocate_memextent(partition, params);
	if (me_ret.e != OK) {
		panic("Failed creation of new mem extent");
	}
	memextent_t *me = me_ret.r;

	spinlock_acquire(&me->header.lock);
	memextent_attrs_t attrs = memextent_attrs_default();
	memextent_attrs_set_access(&attrs, access);
	memextent_attrs_set_memtype(&attrs, memtype);
	me_ret.e = memextent_configure(me, phys_base, size, attrs);
	if (me_ret.e != OK) {
		panic("Failed configuration of new mem extent");
	}
	spinlock_release(&me->header.lock);

	me_ret.e = object_activate_memextent(me);
	if (me_ret.e != OK) {
		panic("Failed activation of new mem extent");
	}

	return me;
}

//		  ----> extent 1
//		  |     [map as,
//		  |	 map as2]
//     partition -|
//		  |		  ----> extent 2.1 ----> extent 2.1.1
//		  |		  |
//		  ----> extent 2 -|
//			[map as]  |
//		     [after all   ----> extent 2.2 ----> extent 2.2.1
//		      derivations,     [unmap as,	 (update access)
//		      unmap as &        map as2]
//                    map as2]
static bool
tests_memextent_test1(paddr_t phys_base)
{
	bool	ret = false;
	error_t err = OK;

	paddr_t vm_base = phys_base;
	size_t	size	= 4096;

	// Mem extents specifications
	memextent_memtype_t memtype = MEMEXTENT_MEMTYPE_ANY;
	pgtable_access_t    access  = PGTABLE_ACCESS_RW;

	// Create 2 new memory extents from the partition

	memextent_t *me = create_memextent(phys_base, size, memtype, access);

	paddr_t phys_base2 = phys_base + size;
	paddr_t vm_base2   = vm_base + size;
	size_t	size2	   = 4096 * 5;

	memextent_t *me2 = create_memextent(phys_base2, size2, memtype, access);

#if !defined(NDEBUG)
	// Check empty pagetables
	LOG(DEBUG, INFO, "+--------------- EMPTY pgtable 1:\n");
	pgtable_vm_dump(&as->vm_pgtable);
	LOG(DEBUG, INFO, "+--------------- EMPTY pgtable 2:\n");
	pgtable_vm_dump(&as2->vm_pgtable);
#endif

	// Map extents. First mem extent is mapped into 2 address spaces and
	// second only into one.

	memextent_mapping_attrs_t map_attrs;

	memextent_mapping_attrs_set_user_access(&map_attrs, PGTABLE_ACCESS_RW);
	memextent_mapping_attrs_set_kernel_access(&map_attrs,
						  PGTABLE_ACCESS_RW);
	memextent_mapping_attrs_set_memtype(&map_attrs,
					    PGTABLE_VM_MEMTYPE_DEVICE_NGNRNE);

	err = memextent_map(me, as, vm_base, map_attrs);
	if (err != OK) {
		panic("Failed mapping of mem extent");
	}

	err = memextent_map(me, as2, vm_base, map_attrs);
	if (err != OK) {
		panic("Failed mapping of mem extent to address space 2");
	}

	err = memextent_map(me2, as, vm_base2, map_attrs);
	if (err != OK) {
		panic("Failed mapping of mem extent 2");
	}

#if !defined(NDEBUG)
	// Check mappings in pagetables
	LOG(DEBUG, INFO, "+------------- 2 mappings pgtable 1:\n");
	pgtable_vm_dump(&as->vm_pgtable);
	LOG(DEBUG, INFO, "+------------- 1 mapping pgtable 2:\n");
	pgtable_vm_dump(&as2->vm_pgtable);
#endif

	// Derive 2 memory extents from second mem extent previously created
	// Derive mem extent from the beginning of the parent extent and second
	// from the last 2 pages of parent
	size_t offset = 0;
	size_t size3  = 4096;

	memextent_ptr_result_t me_ret;

	me_ret = memextent_derive(me2, offset, size3, memtype, access,
				  MEMEXTENT_TYPE_BASIC);
	if (me_ret.e != OK) {
		panic("Failed creation of derived mem extent");
	}

	memextent_t *me_d = me_ret.r;

	size_t	offset2	 = 4096 * 2;
	size_t	size4	 = 4096 * 2;
	paddr_t vm_base3 = vm_base2 + offset2;

	me_ret = memextent_derive(me2, offset2, size4, memtype, access,
				  MEMEXTENT_TYPE_BASIC);
	if (me_ret.e != OK) {
		panic("Failed creation of derived mem extent");
	}

	memextent_t *me_d2 = me_ret.r;

	// Unmap extent from as and map it to as2
	err = memextent_unmap(me_d2, as, vm_base3);
	if (err != OK) {
		panic("Failed memextent unmapping");
	}

#if !defined(NDEBUG)
	// Check mappings in pagetables
	LOG(DEBUG, INFO, "+------------ 1 unmapping pgtable 1:\n");
	pgtable_vm_dump(&as->vm_pgtable);
#endif

	err = memextent_map(me_d2, as2, vm_base3, map_attrs);
	if (err != OK) {
		panic("Failed mapping of mem extent derived 2");
	}

#if !defined(NDEBUG)
	// Check mappings in pagetables
	LOG(DEBUG, INFO, "+------------ 1 mapping pgtable 2:\n");
	pgtable_vm_dump(&as2->vm_pgtable);
#endif

	// Derive memory extent from the entire derived extent.
	me_ret = memextent_derive(me_d, offset, size3, memtype, access,
				  MEMEXTENT_TYPE_BASIC);
	if (me_ret.e != OK) {
		panic("Failed creation of derived mem extent");
	}

	memextent_t *me_dd = me_ret.r;

	// Derive memory extent from first page of derived extent.
	me_ret = memextent_derive(me_d2, offset, size3, memtype, access,
				  MEMEXTENT_TYPE_BASIC);
	if (me_ret.e != OK) {
		panic("Failed creation of derived mem extent");
	}

	memextent_t *me_dd2 = me_ret.r;

	// Update access of mapping
	memextent_access_attrs_t access_attrs;

	memextent_access_attrs_set_user_access(&access_attrs, PGTABLE_ACCESS_R);
	memextent_access_attrs_set_kernel_access(&access_attrs,
						 PGTABLE_ACCESS_R);

	err = memextent_update_access(me_dd2, as2, vm_base3, access_attrs);
	if (err != OK) {
		panic("Failed memextent update access");
	}

#if !defined(NDEBUG)
	// Check mappings in pagetables
	LOG(DEBUG, INFO, "+------------ access updated pgtable 1:\n");
	pgtable_vm_dump(&as2->vm_pgtable);
#endif

	// Unmap extent 2 from as and map it to as2. This implies that only two
	// ranges of the extent will change the mapping since the rest of the
	// ranges are owned by the children
	err = memextent_unmap(me2, as, vm_base2);
	if (err != OK) {
		panic("Failed memextent unmapping");
	}

#if !defined(NDEBUG)
	// Check mappings in pagetables
	LOG(DEBUG, INFO, "+------------ 1 unmapping pgtable 1:\n");
	pgtable_vm_dump(&as->vm_pgtable);
#endif

	err = memextent_map(me2, as2, vm_base2, map_attrs);
	if (err != OK) {
#if !defined(NDEBUG)
		// Check mappings in pagetables
		LOG(DEBUG, INFO, "+------------ mapping failed pgtable 2:\n");
		pgtable_vm_dump(&as2->vm_pgtable);
#endif
		panic("Failed mapping of mem extent 2");
	}

	// deactivate and indirectly unmap all extents from lowest children to
	// parent

	// Uncomment to make the destruction now and be able to see the pgtable
	// update instead of doing it sometime later by the rcu_update
#if 0
	trigger_object_deactivate_memextent_event(me_dd2);
	(void)partition_destroy_memextent(&me_dd2->header.rcu_entry);

	trigger_object_deactivate_memextent_event(me_dd);
	(void)partition_destroy_memextent(&me_dd->header.rcu_entry);

	trigger_object_deactivate_memextent_event(me_d2);
	(void)partition_destroy_memextent(&me_d2->header.rcu_entry);

	trigger_object_deactivate_memextent_event(me_d);
	(void)partition_destroy_memextent(&me_d->header.rcu_entry);

	trigger_object_deactivate_memextent_event(me2);
	(void)partition_destroy_memextent(&me2->header.rcu_entry);

	trigger_object_deactivate_memextent_event(me);
	(void)partition_destroy_memextent(&me->header.rcu_entry);
#else
	object_put_memextent(me_dd2);
	object_put_memextent(me_dd);
	object_put_memextent(me_d2);
	object_put_memextent(me_d);
	object_put_memextent(me2);
	object_put_memextent(me);
#endif

#if !defined(NDEBUG)
	// Check mappings in pagetables
	LOG(DEBUG, INFO, "+--------------- NO MAPS pgtable 1:\n");
	pgtable_vm_dump(&as->vm_pgtable);
	LOG(DEBUG, INFO, "+--------------- NO MAPS pgtable 2:\n");
	pgtable_vm_dump(&as2->vm_pgtable);
#endif

	return ret;
}

//	extent 1		extent 2
//	   |			   |
//	   V			   |
//   map as in vm_base		   |
//         |			   |
//         V			   |
//      extent 1.1		   |
//         |			   V
//         |		   map as in vm_base2
//         |		(indirectly unmaps ext 1.1)
//         V			   |
//    deactivate extent 1.1           |
//         |			   |
//         V			   |
//    unmap and deactivate extent 1   |
//			           V
//		unmap and deactivate extent 2
//
static bool
tests_memextent_test2(paddr_t phys_base)
{
	bool	ret = false;
	error_t err = OK;

	paddr_t vm_base = phys_base;
	size_t	size	= 4096 * 3;

	// Mem extents specifications
	memextent_memtype_t memtype = MEMEXTENT_MEMTYPE_DEVICE;
	pgtable_access_t    access  = PGTABLE_ACCESS_RW;

	// Create 2 new memory extents from the partition
	memextent_t *me = create_memextent(phys_base, size, memtype, access);

	paddr_t phys_base2 = phys_base + size;
	size_t	size2	   = 4096;

	memextent_t *me2 = create_memextent(phys_base2, size2, memtype, access);

#if !defined(NDEBUG)
	LOG(DEBUG, INFO, "+--------------- EMPTY pgtable 1:\n");
	pgtable_vm_dump(&as->vm_pgtable);
#endif

	// Map extents first mem extent to as
	memextent_mapping_attrs_t map_attrs;

	memextent_mapping_attrs_set_user_access(&map_attrs, PGTABLE_ACCESS_RW);
	memextent_mapping_attrs_set_kernel_access(&map_attrs,
						  PGTABLE_ACCESS_RW);
	memextent_mapping_attrs_set_memtype(&map_attrs,
					    PGTABLE_VM_MEMTYPE_DEVICE_NGNRNE);

	err = memextent_map(me, as, vm_base, map_attrs);
	if (err != OK) {
		panic("Failed mapping of mem extent");
	}

#if !defined(NDEBUG)
	LOG(DEBUG, INFO, "+------------- 1 mapping pgtable 1:\n");
	pgtable_vm_dump(&as->vm_pgtable);
#endif

	// Derive 1 memory extent of one page from first mem extent starting
	// from phys_base + page
	size_t	offset	 = 4096;
	size_t	size3	 = 4096;
	paddr_t vm_base2 = vm_base + offset;

	memextent_ptr_result_t me_ret;

	me_ret = memextent_derive(me, offset, size3, memtype, access,
				  MEMEXTENT_TYPE_BASIC);
	if (me_ret.e != OK) {
		panic("Failed creation of derived mem extent");
	}

	memextent_t *me_d = me_ret.r;

	// Map mem extent 2 to as at vm_base2. This will first unmap the derived
	// extent from as before mapping the same virtual address to extent 2
	err = memextent_map(me2, as, vm_base2, map_attrs);
	if (err != OK) {
		panic("Failed mapping of mem extent 2");
	}

#if !defined(NDEBUG)
	LOG(DEBUG, INFO, "+------------ 1 mapping pgtable 1:\n");
	pgtable_vm_dump(&as->vm_pgtable);
#endif

	// deactivate derived extent and check what happens with the mapping of
	// vm_base 2 that used to be owned by the parent

	// Uncomment to make the destruction now and be able to see the pgtable
	// update instead of doing it sometime later by the rcu_update
#if 0
	trigger_object_deactivate_memextent_event(me_d);
	(void)partition_destroy_memextent(&me_d->header.rcu_entry);

#if !defined(NDEBUG)
	LOG(DEBUG, INFO, "+--------------- deactivate me_d pgtable 1:\n");
	pgtable_vm_dump(&as->vm_pgtable);
#endif
	trigger_object_deactivate_memextent_event(me);
	(void)partition_destroy_memextent(&me->header.rcu_entry);

#if !defined(NDEBUG)
	LOG(DEBUG, INFO, "+--------------- deactivate me pgtable 1:\n");
	pgtable_vm_dump(&as->vm_pgtable);
#endif
	trigger_object_deactivate_memextent_event(me2);
	(void)partition_destroy_memextent(&me2->header.rcu_entry);

#if !defined(NDEBUG)
	LOG(DEBUG, INFO, "+--------------- deactivate me2 pgtable 1:\n");
	pgtable_vm_dump(&as->vm_pgtable);
#endif
#else
	object_put_memextent(me_d);
	object_put_memextent(me);
	object_put_memextent(me2);
#endif

	return ret;
}

bool
tests_memextent(void)
{
	bool wait_all_cores_end	  = true;
	bool wait_all_cores_start = true;

	spinlock_acquire_nopreempt(&test_memextent_spinlock);
	tests_memextent_count++;
	spinlock_release_nopreempt(&test_memextent_spinlock);

	// Wait until all cores have reached this point to start.
	while (wait_all_cores_start) {
		spinlock_acquire_nopreempt(&test_memextent_spinlock);

		if (tests_memextent_count == (PLATFORM_MAX_CORES)) {
			wait_all_cores_start = false;
		}

		spinlock_release_nopreempt(&test_memextent_spinlock);
	}

	if (cpulocal_get_index() != 0U) {
		goto wait;
	}

	LOG(DEBUG, INFO, "Memextent tests start");

	paddr_t phys_base = tests_find_free_range();

	tests_memextent_test1(phys_base);

	phys_base = tests_find_free_range();

	tests_memextent_test2(phys_base);
	spinlock_acquire_nopreempt(&test_memextent_spinlock);
	tests_memextent_count++;
	spinlock_release_nopreempt(&test_memextent_spinlock);

	LOG(DEBUG, INFO, "Memextent tests finished");
wait:

	// Make all threads wait for test to end
	while (wait_all_cores_end) {
		spinlock_acquire_nopreempt(&test_memextent_spinlock);

		if (tests_memextent_count == PLATFORM_MAX_CORES + 1) {
			wait_all_cores_end = false;
		}

		spinlock_release_nopreempt(&test_memextent_spinlock);
	}

	return false;
}
#else

extern char unused;

#endif

```

`hyp/mem/memextent_sparse/build.conf`:

```conf
# © 2022 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

base_module hyp/mem/memextent
base_module hyp/misc/gpt
events memextent_sparse.ev
types memextent_sparse.tc
source memextent_sparse.c memextent_tests.c

```

`hyp/mem/memextent_sparse/memextent_sparse.ev`:

```ev
// © 2022 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

module memextent_sparse

subscribe gpt_value_add_offset[GPT_TYPE_MEMEXTENT_MAPPING]
	handler memextent_mapping_add_offset(value, offset)

subscribe gpt_values_equal[GPT_TYPE_MEMEXTENT_MAPPING]
	handler memextent_mappings_equal(x, y)

subscribe memextent_activate[MEMEXTENT_TYPE_SPARSE]
	handler memextent_activate_sparse(me)

subscribe memextent_activate_derive[MEMEXTENT_TYPE_SPARSE]
	handler memextent_activate_derive_sparse(me)

subscribe memextent_supports_donation[MEMEXTENT_TYPE_SPARSE]
	handler memextent_supports_donation_sparse()

subscribe memextent_donate_child[MEMEXTENT_TYPE_SPARSE]
	handler memextent_donate_child_sparse(me, phys, size, reverse)

subscribe memextent_donate_sibling[MEMEXTENT_TYPE_SPARSE]
	handler memextent_donate_sibling_sparse(from, to, phys, size)

subscribe memextent_map[MEMEXTENT_TYPE_SPARSE]
	handler memextent_map_sparse(extent, addrspace, vm_base, map_attrs)

subscribe memextent_map_partial[MEMEXTENT_TYPE_SPARSE]
	handler memextent_map_partial_sparse(extent, addrspace, vm_base, offset, size, map_attrs)

subscribe memextent_unmap[MEMEXTENT_TYPE_SPARSE]
	handler memextent_unmap_sparse(extent, addrspace, vm_base)

subscribe memextent_unmap_partial[MEMEXTENT_TYPE_SPARSE]
	handler memextent_unmap_partial_sparse(extent, addrspace, vm_base, offset, size)

subscribe memextent_unmap_all[MEMEXTENT_TYPE_SPARSE]
	handler memextent_unmap_all_sparse(extent)

subscribe memextent_update_access[MEMEXTENT_TYPE_SPARSE]
	handler memextent_update_access_sparse(extent, addrspace, vm_base, access_attrs)

subscribe memextent_update_access_partial[MEMEXTENT_TYPE_SPARSE]
	handler memextent_update_access_partial_sparse(extent, addrspace, vm_base,
							offset, size, access_attrs)

subscribe memextent_is_mapped[MEMEXTENT_TYPE_SPARSE]
	handler memextent_is_mapped_sparse(me, addrspace, exclusive)

subscribe memextent_deactivate[MEMEXTENT_TYPE_SPARSE]
	handler memextent_deactivate_sparse(extent)

subscribe memextent_cleanup[MEMEXTENT_TYPE_SPARSE]
	handler memextent_cleanup_sparse(extent)

subscribe memextent_retain_mappings[MEMEXTENT_TYPE_SPARSE]
	handler memextent_retain_mappings_sparse(me)

subscribe memextent_release_mappings[MEMEXTENT_TYPE_SPARSE]
	handler memextent_release_mappings_sparse(me, clear)

subscribe memextent_lookup_mapping[MEMEXTENT_TYPE_SPARSE]
	handler memextent_lookup_mapping_sparse(me, phys, size, i)

subscribe object_create_addrspace
	handler memextent_create_addrspace_sparse

subscribe object_deactivate_addrspace
	handler memextent_deactivate_addrspace_sparse

#if defined(UNIT_TESTS)
subscribe tests_init
	handler tests_memextent_sparse_init()

subscribe tests_start
	handler tests_memextent_sparse_start()
#endif

```

`hyp/mem/memextent_sparse/memextent_sparse.tc`:

```tc
// © 2022 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

define GPT_PHYS_BITS constant type count_t = 44;
define GPT_VBASE_BITS constant type count_t = 48;

extend addrspace object {
	sparse_mapping_list	structure list;
};

define memextent_gpt_map bitfield<64> {
	auto<GPT_VBASE_BITS>	vbase		type vmaddr_t;
	auto<8>			memtype		enumeration pgtable_vm_memtype;
	auto<3>			user_access	enumeration pgtable_access;
	auto<3>			kernel_access	enumeration pgtable_access;
	// For unmap operations, we only care about comparing vbases, so this
	// flag indicates that we should ignore the mapping attributes.
	auto			ignore_attrs	bool;
};

extend gpt_type enumeration {
	memextent_mapping;
};

extend gpt_value union {
	me_map		bitfield memextent_gpt_map;
};

define memextent_sparse_arg structure {
	addrspace	pointer object addrspace;
	vbase		type vmaddr_t;
	pbase		type paddr_t;
	memtype		enumeration pgtable_vm_memtype;
	user_access	enumeration pgtable_access;
	kernel_access	enumeration pgtable_access;
	fail_addr	type paddr_t;
};

define memextent_gpt_arg structure {
	addrspace	pointer object addrspace;
};

define memextent_sparse_mapping structure {
	// RCU-protected addrspace pointer
	addrspace		pointer(atomic) object addrspace;
	mapping_list_node	structure list_node(contained);
	gpt			structure gpt;
	retained		bool;
};

extend memextent_map_ptr union {
	sparse		pointer structure memextent_sparse_mapping;
};

```

`hyp/mem/memextent_sparse/src/memextent_sparse.c`:

```c
// © 2022 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>
#include <string.h>

#include <hypcontainers.h>

#include <addrspace.h>
#include <atomic.h>
#include <gpt.h>
#include <list.h>
#include <memdb.h>
#include <memextent.h>
#include <object.h>
#include <panic.h>
#include <partition.h>
#include <pgtable.h>
#include <rcu.h>
#include <spinlock.h>
#include <util.h>

#include "event_handlers.h"

void
memextent_mapping_add_offset(gpt_value_t *value, size_t offset)
{
	vmaddr_t vbase = memextent_gpt_map_get_vbase(&value->me_map);

	memextent_gpt_map_set_vbase(&value->me_map, vbase + offset);
}

bool
memextent_mappings_equal(gpt_value_t x, gpt_value_t y)
{
	bool ret;

	if (memextent_gpt_map_get_ignore_attrs(&x.me_map) ||
	    memextent_gpt_map_get_ignore_attrs(&y.me_map)) {
		// We only need to check if the vbases are equal.
		ret = memextent_gpt_map_get_vbase(&x.me_map) ==
		      memextent_gpt_map_get_vbase(&y.me_map);
	} else {
		ret = memextent_gpt_map_is_equal(x.me_map, y.me_map);
	}

	return ret;
}

static error_t
allocate_sparse_mappings(memextent_t *me)
{
	error_t	     ret       = OK;
	partition_t *partition = me->header.partition;
	const size_t alloc_size =
		sizeof(memextent_sparse_mapping_t) * MEMEXTENT_MAX_MAPS;
	const size_t alloc_align = alignof(memextent_sparse_mapping_t);

	void_ptr_result_t alloc_ret =
		partition_alloc(partition, alloc_size, alloc_align);
	if (alloc_ret.e != OK) {
		ret = alloc_ret.e;
		goto out;
	}

	(void)memset_s(alloc_ret.r, alloc_size, 0, alloc_size);

	me->mappings.sparse = alloc_ret.r;

	for (index_t i = 0U; i < MEMEXTENT_MAX_MAPS; i++) {
		gpt_config_t config = gpt_config_default();
		gpt_config_set_max_bits(&config, GPT_PHYS_BITS);

		ret = gpt_init(&me->mappings.sparse[i].gpt, partition, config,
			       util_bit(GPT_TYPE_MEMEXTENT_MAPPING));
		assert(ret == OK);
	}

out:
	return ret;
}

static void
free_sparse_mappings(memextent_t *me)
{
	partition_t *partition = me->header.partition;
	const size_t alloc_size =
		sizeof(memextent_sparse_mapping_t) * MEMEXTENT_MAX_MAPS;

	assert(me->mappings.sparse != NULL);

	for (index_t i = 0U; i < MEMEXTENT_MAX_MAPS; i++) {
		gpt_destroy(&me->mappings.sparse[i].gpt);
	}

	(void)partition_free(partition, me->mappings.sparse, alloc_size);

	me->mappings.sparse = NULL;
}

static error_t
insert_gpt_mapping(memextent_sparse_mapping_t *map, paddr_t phys, size_t size,
		   vmaddr_t vbase, memextent_mapping_attrs_t attrs)
{
	assert(map != NULL);

	pgtable_vm_memtype_t memtype =
		memextent_mapping_attrs_get_memtype(&attrs);
	pgtable_access_t user_access =
		memextent_mapping_attrs_get_user_access(&attrs);
	pgtable_access_t kernel_access =
		memextent_mapping_attrs_get_kernel_access(&attrs);

	memextent_gpt_map_t gpt_map = memextent_gpt_map_default();
	memextent_gpt_map_set_vbase(&gpt_map, vbase);
	memextent_gpt_map_set_memtype(&gpt_map, memtype);
	memextent_gpt_map_set_user_access(&gpt_map, user_access);
	memextent_gpt_map_set_kernel_access(&gpt_map, kernel_access);

	gpt_entry_t gpt_entry = {
		.type  = GPT_TYPE_MEMEXTENT_MAPPING,
		.value = { .me_map = gpt_map },
	};

	return gpt_insert(&map->gpt, phys, size, gpt_entry, true);
}

static error_t
remove_gpt_mapping(memextent_sparse_mapping_t *map, paddr_t phys, size_t size,
		   vmaddr_t vbase)
{
	assert(map != NULL);

	memextent_gpt_map_t gpt_map = memextent_gpt_map_default();
	memextent_gpt_map_set_vbase(&gpt_map, vbase);
	memextent_gpt_map_set_ignore_attrs(&gpt_map, true);

	gpt_entry_t gpt_entry = {
		.type  = GPT_TYPE_MEMEXTENT_MAPPING,
		.value = { .me_map = gpt_map },
	};

	return gpt_remove(&map->gpt, phys, size, gpt_entry);
}

static error_t
update_gpt_mapping(memextent_sparse_mapping_t *map, paddr_t phys, size_t size,
		   memextent_gpt_map_t old_gpt_map,
		   memextent_gpt_map_t new_gpt_map)
{
	assert(map != NULL);
	assert(!memextent_gpt_map_get_ignore_attrs(&old_gpt_map));
	assert(!memextent_gpt_map_get_ignore_attrs(&new_gpt_map));

	gpt_entry_t old_gpt_entry = {
		.type  = GPT_TYPE_MEMEXTENT_MAPPING,
		.value = { .me_map = old_gpt_map },
	};

	gpt_entry_t new_gpt_entry = {
		.type  = GPT_TYPE_MEMEXTENT_MAPPING,
		.value = { .me_map = new_gpt_map },
	};

	return gpt_update(&map->gpt, phys, size, old_gpt_entry, new_gpt_entry);
}

static void
delete_sparse_mapping(memextent_sparse_mapping_t *map, addrspace_t *addrspace)
	REQUIRE_PREEMPT_DISABLED
{
	assert(atomic_load_relaxed(&map->addrspace) == addrspace);
	assert(gpt_is_empty(&map->gpt));

	spinlock_acquire_nopreempt(&addrspace->mapping_list_lock);
	(void)list_delete_node(&addrspace->basic_mapping_list,
			       &map->mapping_list_node);
	spinlock_release_nopreempt(&addrspace->mapping_list_lock);

	atomic_store_relaxed(&map->addrspace, NULL);
}

static bool
apply_access_mask(memextent_t *me, memextent_mapping_attrs_t *attrs)
{
	pgtable_access_t old_user_access =
		memextent_mapping_attrs_get_user_access(attrs);
	pgtable_access_t old_kernel_access =
		memextent_mapping_attrs_get_kernel_access(attrs);

	pgtable_access_t new_user_access =
		pgtable_access_mask(old_user_access, me->access);
	pgtable_access_t new_kernel_access =
		pgtable_access_mask(old_kernel_access, me->access);

	memextent_mapping_attrs_set_user_access(attrs, new_user_access);
	memextent_mapping_attrs_set_kernel_access(attrs, new_kernel_access);

	return (old_user_access != new_user_access) ||
	       (old_kernel_access != new_kernel_access);
}

static error_t
add_sparse_mapping(memextent_t *me, addrspace_t *addrspace, paddr_t phys,
		   size_t size, vmaddr_t vbase, memextent_mapping_attrs_t attrs)
	REQUIRE_SPINLOCK(me->lock)
{
	error_t err    = OK;
	bool	mapped = false;

	assert(me != NULL);
	assert(addrspace != NULL);

	memextent_sparse_mapping_t *empty_map = NULL;

	// First, try to use an existing mapping with matching addrspace.
	for (index_t i = 0U; !mapped && (i < MEMEXTENT_MAX_MAPS); i++) {
		memextent_sparse_mapping_t *map = &me->mappings.sparse[i];

		addrspace_t *as = atomic_load_relaxed(&map->addrspace);
		if (as == addrspace) {
			err = insert_gpt_mapping(map, phys, size, vbase, attrs);
			if (err == OK) {
				mapped = true;
			} else if (err == ERROR_BUSY) {
				// There is an overlapping entry in this
				// mapping's GPT, but we can try again with a
				// different mapping.
				err = OK;
			} else {
				// Unexpected GPT error.
				break;
			}
		} else if ((as == NULL) && (empty_map == NULL)) {
			empty_map = map;
		} else {
			// Mapping in use by another addrspace, or we have
			// already found an earlier empty mapping, continue.
		}
	}

	if (mapped || (err != OK)) {
		goto out;
	}

	if (empty_map == NULL) {
		err = ERROR_MEMEXTENT_MAPPINGS_FULL;
		goto out;
	}

	// We need an acquire fence as the empty mapping may have been cleared
	// without the memextent lock if the previous addrspace was destroyed.
	// This synchronises the earlier relaxed load of map->addrspace with the
	// store-release in memextent_deactivate_addrspace_sparse().
	atomic_thread_fence(memory_order_acquire);

	err = insert_gpt_mapping(empty_map, phys, size, vbase, attrs);
	if (err != OK) {
		goto out;
	}

	spinlock_acquire_nopreempt(&addrspace->mapping_list_lock);
	list_insert_at_head(&addrspace->sparse_mapping_list,
			    &empty_map->mapping_list_node);
	spinlock_release_nopreempt(&addrspace->mapping_list_lock);
	atomic_store_relaxed(&empty_map->addrspace, addrspace);

out:
	return err;
}

static error_t
remove_sparse_mapping(memextent_t *me, addrspace_t *addrspace, paddr_t phys,
		      size_t size, vmaddr_t vbase) REQUIRE_SPINLOCK(me->lock)
{
	error_t err	 = OK;
	bool	unmapped = false;

	assert(me != NULL);
	assert(addrspace != NULL);

	for (index_t i = 0U; !unmapped && (i < MEMEXTENT_MAX_MAPS); i++) {
		memextent_sparse_mapping_t *map = &me->mappings.sparse[i];

		addrspace_t *as = atomic_load_relaxed(&map->addrspace);
		if (as != addrspace) {
			continue;
		}

		err = remove_gpt_mapping(map, phys, size, vbase);
		if (err == OK) {
			unmapped = true;
			if (gpt_is_empty(&map->gpt)) {
				delete_sparse_mapping(map, addrspace);
			}
		} else if (err == ERROR_BUSY) {
			// The entry was not found in this mapping's GPT, but
			// may be in another mapping.
			err = OK;
		} else {
			// Unexpected GPT error.
			goto out;
		}
	}

	if (!unmapped) {
		err = ERROR_ADDR_INVALID;
	}

out:
	return err;
}

static error_t
memextent_map_range_sparse(paddr_t phys, size_t size, void *arg)
{
	error_t err;

	assert(size != 0U);
	assert(!util_add_overflows(phys, size - 1U));
	assert(arg != NULL);

	memextent_sparse_arg_t *me_arg = (memextent_sparse_arg_t *)arg;

	size_t offset = phys - me_arg->pbase;

	err = addrspace_map(me_arg->addrspace, me_arg->vbase + offset, size,
			    phys, me_arg->memtype, me_arg->kernel_access,
			    me_arg->user_access);
	if (err != OK) {
		me_arg->fail_addr = phys;
	}

	return err;
}

static error_t
memextent_unmap_range_sparse(paddr_t phys, size_t size, void *arg)
{
	assert(size != 0U);
	assert(!util_add_overflows(phys, size - 1U));
	assert(arg != NULL);

	memextent_sparse_arg_t *me_arg = (memextent_sparse_arg_t *)arg;

	size_t offset = phys - me_arg->pbase;

	return addrspace_unmap(me_arg->addrspace, me_arg->vbase + offset, size,
			       phys);
}

static error_t
do_as_map(addrspace_t *as, vmaddr_t vbase, size_t size, paddr_t phys,
	  memextent_mapping_attrs_t attrs)
{
	assert(as != NULL);

	pgtable_vm_memtype_t memtype =
		memextent_mapping_attrs_get_memtype(&attrs);
	pgtable_access_t kernel_access =
		memextent_mapping_attrs_get_kernel_access(&attrs);
	pgtable_access_t user_access =
		memextent_mapping_attrs_get_user_access(&attrs);

	return addrspace_map(as, vbase, size, phys, memtype, kernel_access,
			     user_access);
}

static error_t
apply_mappings(memextent_t *me, paddr_t phys, size_t size, bool unmap,
	       size_t *fail_offset) REQUIRE_SPINLOCK(me->lock)
	REQUIRE_LOCK(me->mappings)
{
	error_t err = OK;

	memextent_mapping_t maps[MEMEXTENT_MAX_MAPS] = { 0 };

	size_t offset = 0U;
	while (offset < size) {
		paddr_t curr_phys = phys + offset;
		size_t	curr_size = size - offset;

		for (index_t i = 0U; i < MEMEXTENT_MAX_MAPS; i++) {
			maps[i] = memextent_lookup_mapping(me, curr_phys,
							   curr_size, i);
			// For each iteration, we only want to transfer the
			// range covered by the smallest mapping (or unmapped
			// range).
			curr_size = util_min(maps[i].size, curr_size);
		}

		index_t fail_idx = 0U;
		for (index_t i = 0U; i < MEMEXTENT_MAX_MAPS; i++) {
			if (maps[i].addrspace == NULL) {
				continue;
			}

			if (unmap) {
				err = addrspace_unmap(maps[i].addrspace,
						      maps[i].vbase, curr_size,
						      curr_phys);
			} else {
				err = do_as_map(maps[i].addrspace,
						maps[i].vbase, curr_size,
						curr_phys, maps[i].attrs);
			}

			if (err != OK) {
				fail_idx = i;
				break;
			}
		}
		if (unmap && (err != OK)) {
			break;
		}

		if (err != OK) {
			if (fail_offset != NULL) {
				*fail_offset = offset;
			} else {
				// If fail_offset wasn't provided then we assume
				// the caller cannot recover from the error.
				panic("Failed to apply sparse mappings");
			}

			for (index_t i = 0U; i < fail_idx; i++) {
				if (maps[i].addrspace == NULL) {
					continue;
				}

				error_t revert_err;
				if (unmap) {
					revert_err = do_as_map(
						maps[i].addrspace,
						maps[i].vbase, curr_size,
						curr_phys, maps[i].attrs);
				} else {
					revert_err = addrspace_unmap(
						maps[i].addrspace,
						maps[i].vbase, curr_size,
						curr_phys);
				}

				if (revert_err != OK) {
					panic("Failed to revert sparse mappings");
				}
			}

			break;
		}

		offset += curr_size;
	}

	return err;
}

static void
revert_mapping_transfer(memextent_mapping_t x_mappings[],
			memextent_mapping_t y_mappings[], const bool x_match[],
			const bool y_match[], paddr_t curr_phys,
			size_t curr_size, index_t x_idx, index_t y_idx)
{
	for (index_t i = 0U; i < MEMEXTENT_MAX_MAPS; i++) {
		memextent_mapping_t *xmap = &x_mappings[i];
		memextent_mapping_t *ymap = &y_mappings[i];

		error_t revert_err = OK;

		if ((i < x_idx) && (xmap->addrspace != NULL) && !x_match[i]) {
			revert_err = do_as_map(xmap->addrspace, xmap->vbase,
					       curr_size, curr_phys,
					       xmap->attrs);
		}

		if ((revert_err == OK) && (i < y_idx) &&
		    (ymap->addrspace != NULL) && !y_match[i]) {
			revert_err = addrspace_unmap(ymap->addrspace,
						     ymap->vbase, curr_size,
						     curr_phys);
		}

		if (revert_err != OK) {
			panic("Failed to revert mapping transfer");
		}
	}
}

static error_t
do_mapping_transfer(memextent_t *x, memextent_t *y, paddr_t phys, size_t size,
		    size_t *fail_offset) REQUIRE_SPINLOCK(x->lock)
	REQUIRE_SPINLOCK(y->lock) REQUIRE_LOCK(x->mappings)
		REQUIRE_LOCK(y->mappings)
{
	error_t err = OK;

	memextent_mapping_t x_mappings[MEMEXTENT_MAX_MAPS] = { 0 };
	memextent_mapping_t y_mappings[MEMEXTENT_MAX_MAPS] = { 0 };

	size_t offset = 0U;
	while (offset < size) {
		paddr_t curr_phys = phys + offset;
		size_t	curr_size = size - offset;

		bool x_match[MEMEXTENT_MAX_MAPS] = { 0 };
		bool y_match[MEMEXTENT_MAX_MAPS] = { 0 };

		for (index_t i = 0U; i < MEMEXTENT_MAX_MAPS; i++) {
			x_mappings[i] = memextent_lookup_mapping(x, curr_phys,
								 curr_size, i);
			y_mappings[i] = memextent_lookup_mapping(y, curr_phys,
								 curr_size, i);

			// For each iteration, we only want to transfer the
			// range covered by the smallest mapping (or unmapped
			// range).
			curr_size = util_min(x_mappings[i].size, curr_size);
			curr_size = util_min(y_mappings[i].size, curr_size);
		}

		for (index_t i = 0U; i < MEMEXTENT_MAX_MAPS; i++) {
			memextent_mapping_t *xmap = &x_mappings[i];
			if (xmap->addrspace == NULL) {
				continue;
			}

			for (index_t j = 0U; j < MEMEXTENT_MAX_MAPS; j++) {
				memextent_mapping_t *ymap = &y_mappings[j];
				if (xmap->addrspace != ymap->addrspace) {
					continue;
				}

				bool vbase_match = xmap->vbase == ymap->vbase;
				bool attrs_match =
					memextent_mapping_attrs_is_equal(
						xmap->attrs, ymap->attrs);

				// We only need to unmap from x if the vbase
				// does not match. If the vbases match but the
				// attrs don't, applying y's mapping will
				// overwrite the mapping from x.
				x_match[i] = vbase_match;
				y_match[j] = vbase_match && attrs_match;
			}
		}

		index_t x_idx = 0U;
		index_t y_idx = 0U;
		for (index_t i = 0U; i < MEMEXTENT_MAX_MAPS; i++) {
			memextent_mapping_t *xmap = &x_mappings[i];
			memextent_mapping_t *ymap = &y_mappings[i];

			if ((xmap->addrspace != NULL) && !x_match[i]) {
				err = addrspace_unmap(xmap->addrspace,
						      xmap->vbase, curr_size,
						      curr_phys);
				if (err != OK) {
					break;
				}
			}

			x_idx++;

			if ((ymap->addrspace != NULL) && !y_match[i]) {
				err = do_as_map(ymap->addrspace, ymap->vbase,
						curr_size, curr_phys,
						ymap->attrs);
				if (err != OK) {
					break;
				}
			}

			y_idx++;
		}

		if (err != OK) {
			if (fail_offset != NULL) {
				*fail_offset = offset;
			} else {
				// If fail_offset wasn't provided then we assume
				// the caller cannot recover from the error.
				panic("Failed to do sparse mapping transfer");
			}

			revert_mapping_transfer(x_mappings, y_mappings, x_match,
						y_match, curr_phys, curr_size,
						x_idx, y_idx);
			break;
		}

		offset += curr_size;
	}

	return err;
}

static error_t
update_memdb_partition_and_extent(memextent_t *me, paddr_t phys, size_t size,
				  bool to_partition) REQUIRE_SPINLOCK(me->lock)
{
	error_t	     ret;
	partition_t *hyp_partition = partition_get_private();

	assert(hyp_partition != NULL);
	assert(me != NULL);
	assert(!util_add_overflows(phys, size - 1U));

	partition_t *parent_partition = me->header.partition;

	assert(parent_partition != NULL);

	uintptr_t    object, prev_object;
	memdb_type_t type, prev_type;

	if (to_partition) {
		object	    = (uintptr_t)parent_partition;
		type	    = MEMDB_TYPE_PARTITION;
		prev_object = (uintptr_t)me;
		prev_type   = MEMDB_TYPE_EXTENT;
	} else {
		object	    = (uintptr_t)me;
		type	    = MEMDB_TYPE_EXTENT;
		prev_object = (uintptr_t)parent_partition;
		prev_type   = MEMDB_TYPE_PARTITION;
	}

	paddr_t end = phys + (size - 1U);

	ret = memdb_update(hyp_partition, phys, end, object, type, prev_object,
			   prev_type);
	if (ret == ERROR_MEMDB_NOT_OWNER) {
		// We might have failed to take ownership because a previously
		// deleted memextent has not yet been cleaned up, so wait for a
		// RCU grace period and then retry. If it still fails after
		// that, there's a real conflict.
		spinlock_release(&me->lock);
		rcu_sync();
		spinlock_acquire(&me->lock);
		ret = memdb_update(hyp_partition, phys, end, object, type,
				   prev_object, prev_type);
	}

	return ret;
}

static error_t
update_memdb_two_extents(memextent_t *from, memextent_t *to, paddr_t phys,
			 size_t size, bool from_locked_first)
	REQUIRE_SPINLOCK(from->lock) REQUIRE_SPINLOCK(to->lock)
{
	error_t	     ret;
	partition_t *hyp_partition = partition_get_private();

	assert(hyp_partition != NULL);
	assert(from != NULL);
	assert(to != NULL);
	assert(!util_add_overflows(phys, size - 1U));

	paddr_t end = phys + (size - 1U);

	ret = memdb_update(hyp_partition, phys, end, (uintptr_t)to,
			   MEMDB_TYPE_EXTENT, (uintptr_t)from,
			   MEMDB_TYPE_EXTENT);
	if (ret == ERROR_MEMDB_NOT_OWNER) {
		// We might have failed to take ownership because a previously
		// deleted memextent has not yet been cleaned up, so wait for a
		// RCU grace period and then retry. If it still fails after
		// that, there's a real conflict.
		if (from_locked_first) {
			spinlock_release_nopreempt(&to->lock);
			spinlock_release(&from->lock);
		} else {
			spinlock_release_nopreempt(&from->lock);
			spinlock_release(&to->lock);
		}

		rcu_sync();

		if (from_locked_first) {
			spinlock_acquire(&from->lock);
			spinlock_acquire_nopreempt(&to->lock);
		} else {
			spinlock_acquire(&to->lock);
			spinlock_acquire_nopreempt(&from->lock);
		}

		ret = memdb_update(hyp_partition, phys, end, (uintptr_t)to,
				   MEMDB_TYPE_EXTENT, (uintptr_t)from,
				   MEMDB_TYPE_EXTENT);
	}

	return ret;
}

static error_t
get_phys_range(paddr_t phys, size_t size, void *arg)
{
	phys_range_result_t *ret = (phys_range_result_t *)arg;
	assert(ret != NULL);

	phys_range_t range = {
		.base = phys,
		.size = size,
	};

	*ret = phys_range_result_ok(range);

	return ERROR_RETRY;
}

static phys_range_result_t
lookup_phys_range(memextent_t *me, size_t *offset)
{
	phys_range_result_t ret = phys_range_result_error(ERROR_FAILURE);

	assert(offset != NULL);
	assert(*offset < me->size);

	paddr_t start = me->phys_base + *offset;
	paddr_t end   = me->phys_base + (me->size - 1U);

	error_t err = memdb_range_walk((uintptr_t)me, MEMDB_TYPE_EXTENT, start,
				       end, get_phys_range, &ret);
	assert((err == OK) || (ret.e == OK));

	if (ret.e == OK) {
		*offset = ret.r.base + ret.r.size - me->phys_base;
	}

	return ret;
}

error_t
memextent_activate_sparse(memextent_t *me)
{
	error_t	     ret;
	partition_t *hyp_partition = partition_get_private();

	assert(me != NULL);
	assert(hyp_partition != NULL);

	ret = allocate_sparse_mappings(me);
	if (ret != OK) {
		goto out;
	}

	if (me->device_mem) {
		assert(me->memtype == MEMEXTENT_MEMTYPE_DEVICE);

		ret = memdb_insert(hyp_partition, me->phys_base,
				   me->phys_base + (me->size - 1U),
				   (uintptr_t)me, MEMDB_TYPE_EXTENT);
		if (ret != OK) {
			free_sparse_mappings(me);
		}
	} else {
		// Memory will be added to the memextent after
		// activation; there is nothing to do now.
	}

out:
	return ret;
}

error_t
memextent_activate_derive_sparse(memextent_t *me)
{
	error_t	     ret;
	partition_t *hyp_partition = partition_get_private();

	assert(me != NULL);
	assert(me->parent != NULL);

	ret = allocate_sparse_mappings(me);
	if (ret != OK) {
		goto out;
	}

	spinlock_acquire(&me->parent->lock);
	spinlock_acquire_nopreempt(&me->lock);

	if (me->parent->attached_size != 0U) {
		ret = ERROR_BUSY;
		goto out_locked;
	}

	bool transfer = !memextent_supports_donation(me->parent);
	if (transfer) {
		// The parent does not support donation, so we need to transfer
		// ownership of the memextent's entire range now.
		ret = update_memdb_two_extents(me->parent, me, me->phys_base,
					       me->size, true);
		if (ret != OK) {
			goto out_locked;
		}
	}

	memextent_retain_mappings(me->parent);

	bool access_changed = false;
	for (index_t i = 0U; i < MEMEXTENT_MAX_MAPS; i++) {
		size_t offset = 0U;
		while (offset < me->size) {
			paddr_t phys = me->phys_base + offset;
			size_t	size = me->size - offset;

			memextent_mapping_t parent_map =
				memextent_lookup_mapping(me->parent, phys, size,
							 i);
			offset += parent_map.size;

			if (parent_map.addrspace == NULL) {
				continue;
			}

			memextent_mapping_attrs_t attrs = parent_map.attrs;

			if (apply_access_mask(me, &attrs)) {
				access_changed = true;
			}

			ret = add_sparse_mapping(me, parent_map.addrspace, phys,
						 parent_map.size,
						 parent_map.vbase, attrs);
			if (ret != OK) {
				break;
			}
		}
	}

	if ((ret == OK) && transfer && access_changed) {
		// The child memextent has modified the mappings of memory it
		// now owns. Ensure these mappings changes are applied.
		size_t fail_offset = 0U;

		memextent_retain_mappings(me);

		ret = do_mapping_transfer(me->parent, me, me->phys_base,
					  me->size, &fail_offset);
		if (ret != OK) {
			// Revert mapping changes.
			(void)do_mapping_transfer(me, me->parent, me->phys_base,
						  fail_offset, NULL);
		}

		memextent_release_mappings(me, ret != OK);
	}

	memextent_release_mappings(me->parent, false);

	if (ret == OK) {
		list_insert_at_head(&me->parent->children_list,
				    &me->children_list_node);
	} else if (transfer) {
		error_t err = memdb_update(hyp_partition, me->phys_base,
					   me->phys_base + (me->size - 1U),
					   (uintptr_t)me->parent,
					   MEMDB_TYPE_EXTENT, (uintptr_t)me,
					   MEMDB_TYPE_EXTENT);
		assert(err == OK);
	} else {
		// Nothing to do.
	}

out_locked:
	spinlock_release_nopreempt(&me->lock);
	spinlock_release(&me->parent->lock);

	if (ret != OK) {
		free_sparse_mappings(me);
	}

out:
	return ret;
}

bool
memextent_supports_donation_sparse(void)
{
	return true;
}

static error_t
donate_memextents_common(memextent_t *from, memextent_t *to, paddr_t phys,
			 size_t size, bool lock_from_first)
{
	error_t ret;

	if (lock_from_first) {
		spinlock_acquire(&from->lock);
		spinlock_acquire_nopreempt(&to->lock);
	} else {
		spinlock_acquire(&to->lock);
		spinlock_acquire_nopreempt(&from->lock);
	}

	ret = update_memdb_two_extents(from, to, phys, size, lock_from_first);
	if (ret != OK) {
		goto out;
	}

	size_t fail_offset = 0U;

	memextent_retain_mappings(from);
	memextent_retain_mappings(to);

	ret = do_mapping_transfer(from, to, phys, size, &fail_offset);
	if (ret != OK) {
		(void)do_mapping_transfer(from, to, phys, fail_offset, NULL);
	}

	memextent_release_mappings(to, false);
	memextent_release_mappings(from, false);

out:
	if (lock_from_first) {
		spinlock_release_nopreempt(&to->lock);
		spinlock_release(&from->lock);
	} else {
		spinlock_release_nopreempt(&from->lock);
		spinlock_release(&to->lock);
	}

	return ret;
}

error_t
memextent_donate_child_sparse(memextent_t *me, paddr_t phys, size_t size,
			      bool reverse)
{
	error_t ret;

	assert(me != NULL);

	if (me->parent != NULL) {
		if (me->parent->type != MEMEXTENT_TYPE_SPARSE) {
			ret = ERROR_ARGUMENT_INVALID;
			goto out;
		}

		// The parent extent is always locked first.
		if (reverse) {
			ret = donate_memextents_common(me, me->parent, phys,
						       size, false);
		} else {
			ret = donate_memextents_common(me->parent, me, phys,
						       size, true);
		}

		goto out;
	}

	spinlock_acquire(&me->lock);

	ret = update_memdb_partition_and_extent(me, phys, size, reverse);
	if (ret != OK) {
		goto unlock_me;
	}

	size_t fail_offset = 0U;
	memextent_retain_mappings(me);

	ret = apply_mappings(me, phys, size, reverse, &fail_offset);
	if (ret != OK) {
		(void)apply_mappings(me, phys, fail_offset, true, NULL);
	}

	memextent_release_mappings(me, false);

unlock_me:
	spinlock_release(&me->lock);
out:
	return ret;
}

error_t
memextent_donate_sibling_sparse(memextent_t *from, memextent_t *to,
				paddr_t phys, size_t size)
{
	error_t ret;

	assert(from != to);
	assert(from->parent == to->parent);

	if (to->type != MEMEXTENT_TYPE_SPARSE) {
		ret = ERROR_ARGUMENT_INVALID;
		goto out;
	}

	// To prevent deadlocks, we need to obtain the memextents' locks in a
	// consistent order. Lock the child at the lower address first.
	ret = donate_memextents_common(from, to, phys, size, from < to);

out:
	return ret;
}

error_t
memextent_map_sparse(memextent_t *me, addrspace_t *addrspace, vmaddr_t vm_base,
		     memextent_mapping_attrs_t map_attrs)
{
	return memextent_map_partial_sparse(me, addrspace, vm_base, 0U,
					    me->size, map_attrs);
}

error_t
memextent_map_partial_sparse(memextent_t *me, addrspace_t *addrspace,
			     vmaddr_t vm_base, size_t offset, size_t size,
			     memextent_mapping_attrs_t map_attrs)
{
	error_t ret;

	assert(!util_add_overflows(offset, size - 1U));
	assert(!util_add_overflows(vm_base, size - 1U));

	if (vm_base + (size - 1U) >= util_bit(GPT_VBASE_BITS)) {
		ret = ERROR_ADDR_INVALID;
		goto out;
	}

	paddr_t phys = me->phys_base + offset;

	spinlock_acquire(&me->lock);

	ret = add_sparse_mapping(me, addrspace, phys, size, vm_base, map_attrs);
	if (ret != OK) {
		goto out_locked;
	}

	pgtable_vm_memtype_t memtype =
		memextent_mapping_attrs_get_memtype(&map_attrs);
	pgtable_access_t user_access =
		memextent_mapping_attrs_get_user_access(&map_attrs);
	pgtable_access_t kernel_access =
		memextent_mapping_attrs_get_kernel_access(&map_attrs);

	memextent_sparse_arg_t arg = {
		.addrspace     = addrspace,
		.vbase	       = vm_base,
		.pbase	       = phys,
		.memtype       = memtype,
		.user_access   = user_access,
		.kernel_access = kernel_access,
	};

	ret = memdb_range_walk((uintptr_t)me, MEMDB_TYPE_EXTENT, phys,
			       phys + (size - 1U), memextent_map_range_sparse,
			       &arg);
	if (ret != OK) {
		error_t err;

		if (arg.fail_addr != phys) {
			// Unmap any ranges that were mapped in the memdb walk.
			err = memdb_range_walk((uintptr_t)me, MEMDB_TYPE_EXTENT,
					       phys, arg.fail_addr - 1U,
					       memextent_unmap_range_sparse,
					       &arg);
			assert(err == OK);
		}

		err = remove_sparse_mapping(me, addrspace, phys, size, vm_base);
		assert(err == OK);
	}

out_locked:
	spinlock_release(&me->lock);
out:
	return ret;
}

error_t
memextent_unmap_sparse(memextent_t *me, addrspace_t *addrspace,
		       vmaddr_t vm_base)
{
	return memextent_unmap_partial_sparse(me, addrspace, vm_base, 0U,
					      me->size);
}

error_t
memextent_unmap_partial_sparse(memextent_t *me, addrspace_t *addrspace,
			       vmaddr_t vm_base, size_t offset, size_t size)
{
	error_t ret;

	assert(!util_add_overflows(offset, size - 1U));
	assert(!util_add_overflows(vm_base, size - 1U));

	if (vm_base + (size - 1U) >= util_bit(GPT_VBASE_BITS)) {
		ret = ERROR_ADDR_INVALID;
		goto out;
	}

	paddr_t phys = me->phys_base + offset;

	spinlock_acquire(&me->lock);

	ret = remove_sparse_mapping(me, addrspace, phys, size, vm_base);
	if (ret != OK) {
		goto out_locked;
	}

	memextent_sparse_arg_t arg = {
		.addrspace = addrspace,
		.vbase	   = vm_base,
		.pbase	   = phys,
	};

	ret = memdb_range_walk((uintptr_t)me, MEMDB_TYPE_EXTENT, phys,
			       phys + (size - 1U), memextent_unmap_range_sparse,
			       &arg);
	assert(ret == OK);

out_locked:
	spinlock_release(&me->lock);
out:
	return ret;
}

bool
memextent_unmap_all_sparse(memextent_t *me)
{
	spinlock_acquire(&me->lock);
	memextent_retain_mappings(me);

	size_t offset = 0U;
	while (offset < me->size) {
		phys_range_result_t range = lookup_phys_range(me, &offset);
		if (range.e != OK) {
			break;
		}

		error_t err = apply_mappings(me, range.r.base, range.r.size,
					     true, NULL);
		assert(err == OK);
	}

	memextent_release_mappings(me, true);
	spinlock_release(&me->lock);

	return true;
}

error_t
memextent_update_access_sparse(memextent_t *me, addrspace_t *addrspace,
			       vmaddr_t			vm_base,
			       memextent_access_attrs_t access_attrs)
{
	return memextent_update_access_partial_sparse(me, addrspace, vm_base, 0,
						      me->size, access_attrs);
}

error_t
memextent_update_access_partial_sparse(memextent_t *me, addrspace_t *addrspace,
				       vmaddr_t vm_base, size_t offset,
				       size_t			size,
				       memextent_access_attrs_t access_attrs)
{
	error_t ret;
	paddr_t phys = me->phys_base + offset;

	assert(!util_add_overflows(offset, size - 1U));
	assert(!util_add_overflows(vm_base, size - 1U));

	if (vm_base + (size - 1U) >= util_bit(GPT_VBASE_BITS)) {
		ret = ERROR_ADDR_INVALID;
		goto out;
	}

	spinlock_acquire(&me->lock);

	memextent_sparse_mapping_t *update_map	= NULL;
	memextent_gpt_map_t	    old_gpt_map = memextent_gpt_map_default();

	for (index_t i = 0U; i < MEMEXTENT_MAX_MAPS; i++) {
		memextent_sparse_mapping_t *map = &me->mappings.sparse[i];

		addrspace_t *as = atomic_load_relaxed(&map->addrspace);
		if (as != addrspace) {
			continue;
		}

		// We need to keep the existing memtype when updating access.
		// Perform a lookup on the first page of the mapping so we know
		// what it is. If the memtype isn't consistent for the range
		// then the GPT update will detect this and return an error.
		gpt_lookup_result_t lookup_ret =
			gpt_lookup(&map->gpt, phys, PGTABLE_VM_PAGE_SIZE);
		if (lookup_ret.entry.type == GPT_TYPE_EMPTY) {
			continue;
		}

		assert(lookup_ret.entry.type == GPT_TYPE_MEMEXTENT_MAPPING);

		old_gpt_map = lookup_ret.entry.value.me_map;
		if (memextent_gpt_map_get_vbase(&old_gpt_map) == vm_base) {
			break;
		}
	}

	if (update_map == NULL) {
		ret = ERROR_ADDR_INVALID;
		goto out_locked;
	}

	pgtable_access_t new_user_access =
		memextent_access_attrs_get_user_access(&access_attrs);
	pgtable_access_t new_kernel_access =
		memextent_access_attrs_get_kernel_access(&access_attrs);

	memextent_gpt_map_t new_gpt_map = old_gpt_map;
	memextent_gpt_map_set_user_access(&new_gpt_map, new_user_access);
	memextent_gpt_map_set_kernel_access(&new_gpt_map, new_kernel_access);

	ret = update_gpt_mapping(update_map, phys, size, old_gpt_map,
				 new_gpt_map);
	if (ret != OK) {
		goto out_locked;
	}

	memextent_sparse_arg_t arg = {
		.addrspace     = addrspace,
		.vbase	       = vm_base,
		.pbase	       = phys,
		.memtype       = memextent_gpt_map_get_memtype(&new_gpt_map),
		.user_access   = new_user_access,
		.kernel_access = new_kernel_access,
	};

	ret = memdb_range_walk((uintptr_t)me, MEMDB_TYPE_EXTENT, phys,
			       phys + (size - 1U), memextent_map_range_sparse,
			       &arg);
	if (ret != OK) {
		error_t err;

		if (arg.fail_addr != phys) {
			// Revert any access changes applied to the addrspace.
			arg.user_access =
				memextent_gpt_map_get_user_access(&old_gpt_map);
			arg.kernel_access = memextent_gpt_map_get_kernel_access(
				&old_gpt_map);

			err = memdb_range_walk((uintptr_t)me, MEMDB_TYPE_EXTENT,
					       phys, arg.fail_addr - 1U,
					       memextent_map_range_sparse,
					       &arg);
			assert(err == OK);
		}

		// Revert the GPT update.
		err = update_gpt_mapping(update_map, phys, size, new_gpt_map,
					 old_gpt_map);
		assert(err == OK);
	}

out_locked:
	spinlock_release(&me->lock);
out:
	return ret;
}

bool
memextent_is_mapped_sparse(memextent_t *me, addrspace_t *addrspace,
			   bool exclusive)
{
	bool ret = false;

	for (index_t i = 0; i < MEMEXTENT_MAX_MAPS; i++) {
		memextent_sparse_mapping_t *map = &me->mappings.sparse[i];

		addrspace_t *as = atomic_load_relaxed(&map->addrspace);
		if (as == addrspace) {
			ret = true;
		} else if (as != NULL) {
			ret = false;
		} else {
			continue;
		}

		if (ret != exclusive) {
			break;
		}
	}

	return ret;
}

bool
memextent_deactivate_sparse(memextent_t *me)
{
	assert(me != NULL);

	// There should be no children by this time
	assert(list_is_empty(&me->children_list));

	if (me->parent == NULL) {
		(void)memextent_unmap_all_sparse(me);
		goto out;
	}

	spinlock_acquire(&me->parent->lock);
	spinlock_acquire_nopreempt(&me->lock);

	memextent_retain_mappings(me->parent);
	memextent_retain_mappings(me);

	size_t offset = 0U;
	while (offset < me->size) {
		phys_range_result_t range = lookup_phys_range(me, &offset);
		if (range.e != OK) {
			break;
		}

		error_t err = do_mapping_transfer(me, me->parent, range.r.base,
						  range.r.size, NULL);
		assert(err == OK);
	}

	memextent_release_mappings(me->parent, false);
	memextent_release_mappings(me, true);

	spinlock_release_nopreempt(&me->lock);
	spinlock_release(&me->parent->lock);

out:
	return true;
}

static error_t
memextent_return_range(paddr_t base, size_t size, void *arg)
{
	partition_t *hyp_partition = partition_get_private();

	assert(hyp_partition != NULL);
	assert(size != 0U);
	assert(!util_add_overflows(base, size - 1U));
	assert(arg != NULL);

	memextent_t *me = (memextent_t *)arg;
	uintptr_t    parent;
	memdb_type_t parent_type;

	if (me->parent != NULL) {
		parent	    = (uintptr_t)me->parent;
		parent_type = MEMDB_TYPE_EXTENT;
	} else {
		parent	    = (uintptr_t)me->header.partition;
		parent_type = MEMDB_TYPE_PARTITION;
	}

	return memdb_update(hyp_partition, base, base + (size - 1U), parent,
			    parent_type, (uintptr_t)me, MEMDB_TYPE_EXTENT);
}

bool
memextent_cleanup_sparse(memextent_t *me)
{
	assert(me != NULL);

	if (!me->active) {
		goto out;
	}

	// Walk over the memextent's range and donate any memory still
	// owned by the extent back to the parent.
	error_t err = memdb_range_walk((uintptr_t)me, MEMDB_TYPE_EXTENT,
				       me->phys_base,
				       me->phys_base + (me->size - 1U),
				       memextent_return_range, me);
	assert(err == OK);

	memextent_t *parent = me->parent;
	if (parent != NULL) {
		// Remove extent from parent's list of children.
		spinlock_acquire(&parent->lock);
		(void)list_delete_node(&parent->children_list,
				       &me->children_list_node);
		spinlock_release(&parent->lock);
	}

	free_sparse_mappings(me);

out:
	return true;
}

bool
memextent_retain_mappings_sparse(memextent_t *me) REQUIRE_SPINLOCK(me->lock)
{
	assert(me != NULL);

	rcu_read_start();
	for (index_t i = 0U; i < MEMEXTENT_MAX_MAPS; i++) {
		memextent_sparse_mapping_t *map = &me->mappings.sparse[i];

		addrspace_t *as = atomic_load_consume(&map->addrspace);
		if ((as != NULL) && object_get_addrspace_safe(as)) {
			map->retained = true;
		}
	}
	rcu_read_finish();

	return true;
}

bool
memextent_release_mappings_sparse(memextent_t *me, bool clear)
	REQUIRE_SPINLOCK(me->lock)
{
	assert(me != NULL);

	for (index_t i = 0U; i < MEMEXTENT_MAX_MAPS; i++) {
		memextent_sparse_mapping_t *map = &me->mappings.sparse[i];

		if (!map->retained) {
			continue;
		}

		addrspace_t *as = atomic_load_relaxed(&map->addrspace);
		assert(as != NULL);

		if (clear) {
			gpt_clear_all(&map->gpt);
			delete_sparse_mapping(map, as);
		}

		object_put_addrspace(as);
		map->retained = false;
	}

	return true;
}

memextent_mapping_result_t
memextent_lookup_mapping_sparse(memextent_t *me, paddr_t phys, size_t size,
				index_t i)
{
	assert(me != NULL);
	assert(i < MEMEXTENT_MAX_MAPS);
	assert((phys >= me->phys_base) &&
	       ((phys + (size - 1U)) <= (me->phys_base + (me->size - 1U))));

	memextent_mapping_t ret = {
		.size = size,
	};

	memextent_sparse_mapping_t *map = &me->mappings.sparse[i];

	if (!map->retained) {
		goto out;
	}

	addrspace_t *as = atomic_load_relaxed(&map->addrspace);
	assert(as != NULL);

	gpt_lookup_result_t lookup = gpt_lookup(&map->gpt, phys, size);

	ret.size = lookup.size;

	if (lookup.entry.type == GPT_TYPE_EMPTY) {
		goto out;
	}

	assert(lookup.entry.type == GPT_TYPE_MEMEXTENT_MAPPING);

	memextent_gpt_map_t gpt_map = lookup.entry.value.me_map;
	assert(!memextent_gpt_map_get_ignore_attrs(&gpt_map));

	memextent_mapping_attrs_t attrs = memextent_mapping_attrs_default();
	memextent_mapping_attrs_set_memtype(
		&attrs, memextent_gpt_map_get_memtype(&gpt_map));
	memextent_mapping_attrs_set_user_access(
		&attrs, memextent_gpt_map_get_user_access(&gpt_map));
	memextent_mapping_attrs_set_kernel_access(
		&attrs, memextent_gpt_map_get_kernel_access(&gpt_map));

	ret.addrspace = as;
	ret.vbase     = memextent_gpt_map_get_vbase(&gpt_map);
	ret.attrs     = attrs;

out:
	return memextent_mapping_result_ok(ret);
}

error_t
memextent_create_addrspace_sparse(addrspace_create_t params)
{
	addrspace_t *addrspace = params.addrspace;
	assert(addrspace != NULL);

	list_init(&addrspace->sparse_mapping_list);

	return OK;
}

void
memextent_deactivate_addrspace_sparse(addrspace_t *addrspace)
{
	assert(addrspace != NULL);

	spinlock_acquire(&addrspace->mapping_list_lock);

	list_t *list = &addrspace->sparse_mapping_list;

	memextent_sparse_mapping_t *map = NULL;
	list_foreach_container_maydelete (map, list, memextent_sparse_mapping,
					  mapping_list_node) {
		// An object_put() call is a release operation, and if the
		// refcount reaches zero it is also an acquire operation. As
		// such, we should have observed all prior updates to the GPT
		// despite not holding the memextent lock. Additionally, the
		// mapping won't be reused until the addrspace pointer is
		// cleared below, so it is also safe to clear the GPT without
		// holding the lock.
		gpt_clear_all(&map->gpt);
		(void)list_delete_node(list, &map->mapping_list_node);

		// Use store-release to ensure the above updates are observed
		// when the empty mapping is reused. This matches with the
		// acquire fence in add_sparse_mapping().
		atomic_store_release(&map->addrspace, NULL);
	}

	spinlock_release(&addrspace->mapping_list_lock);
}

```

`hyp/mem/memextent_sparse/src/memextent_tests.c`:

```c
// © 2022 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#if defined(UNIT_TESTS)

#include <assert.h>
#include <hyptypes.h>

#include <addrspace.h>
#include <compiler.h>
#include <cpulocal.h>
#include <log.h>
#include <memdb.h>
#include <memextent.h>
#include <object.h>
#include <panic.h>
#include <partition.h>
#include <partition_alloc.h>
#include <partition_init.h>
#include <pgtable.h>
#include <preempt.h>
#include <trace.h>

#include <asm/event.h>

#include "event_handlers.h"

#define PHYS_MAX (1UL << GPT_PHYS_BITS)

static addrspace_t *as1;
static addrspace_t *as2;

static _Atomic bool tests_complete;

static addrspace_t *
create_addrspace(vmid_t vmid)
{
	partition_t *partition = partition_get_root();
	assert(partition != NULL);

	addrspace_ptr_result_t ret;
	addrspace_create_t     params = { NULL };

	ret = partition_allocate_addrspace(partition, params);
	if (ret.e != OK) {
		panic("Failed to create addrspace");
	}

	if (addrspace_configure(ret.r, vmid) != OK) {
		panic("Failed addrspace configuration");
	}

	if (object_activate_addrspace(ret.r) != OK) {
		panic("Failed addrspace activation");
	}

	return ret.r;
}

static memextent_t *
create_memextent(memextent_t *parent, size_t offset, size_t size, bool sparse)
{
	partition_t *partition = partition_get_root();
	assert(partition != NULL);

	memextent_ptr_result_t ret;
	memextent_create_t     params = { .memextent_device_mem = false };

	ret = partition_allocate_memextent(partition, params);
	if (ret.e != OK) {
		panic("Failed to create addrspace");
	}

	memextent_attrs_t attrs = memextent_attrs_default();
	memextent_attrs_set_memtype(&attrs, MEMEXTENT_MEMTYPE_ANY);
	memextent_attrs_set_access(&attrs, PGTABLE_ACCESS_RWX);
	if (sparse) {
		memextent_attrs_set_type(&attrs, MEMEXTENT_TYPE_SPARSE);
	}

	if (parent != NULL) {
		if (memextent_configure_derive(ret.r, parent, offset, size,
					       attrs)) {
			panic("Failed to configure derived memextent");
		}
	} else {
		if (memextent_configure(ret.r, offset, size, attrs) != OK) {
			panic("Failed to configure memextent");
		}
	}

	if (object_activate_memextent(ret.r) != OK) {
		panic("Failed to activate memextent");
	}

	return ret.r;
}

static error_t
phys_range_walk(paddr_t phys, size_t size, void *arg)
{
	phys_range_result_t *ret = (phys_range_result_t *)arg;
	assert(ret != NULL);

	if ((ret->e != OK) && (size >= ret->r.size)) {
		ret->r.base = phys;
		ret->e	    = OK;
	}

	return OK;
}

static paddr_t
get_free_phys_range(size_t min_size)
{
	partition_t *partition = partition_get_root();
	assert(partition != NULL);

	phys_range_t	    range = { .size = min_size };
	phys_range_result_t ret	  = { .r = range, .e = ERROR_NOMEM };

	error_t err = memdb_walk((uintptr_t)partition, MEMDB_TYPE_PARTITION,
				 phys_range_walk, &ret);
	if ((err != OK) || (ret.e != OK)) {
		panic("Failed to find free phys range");
	}

	return ret.r.base;
}

static error_t
map_memextent(memextent_t *me, addrspace_t *as, vmaddr_t vbase, size_t offset,
	      size_t size, pgtable_vm_memtype_t memtype,
	      pgtable_access_t access)
{
	memextent_mapping_attrs_t map_attrs = memextent_mapping_attrs_default();
	memextent_mapping_attrs_set_memtype(&map_attrs, memtype);
	memextent_mapping_attrs_set_user_access(&map_attrs, access);
	memextent_mapping_attrs_set_kernel_access(&map_attrs, access);

	return memextent_map_partial(me, as, vbase, offset, size, map_attrs);
}

static bool
lookup_addrspace(addrspace_t *as, vmaddr_t vbase, paddr_t expected_phys,
		 pgtable_vm_memtype_t expected_memtype,
		 pgtable_access_t     expected_access)
{
	bool ret = false;

	paddr_t		     mapped_base;
	size_t		     mapped_size;
	pgtable_vm_memtype_t mapped_memtype;
	pgtable_access_t     mapped_vm_kernel_access;
	pgtable_access_t     mapped_vm_user_access;

	bool mapped = pgtable_vm_lookup(&as->vm_pgtable, vbase, &mapped_base,
					&mapped_size, &mapped_memtype,
					&mapped_vm_kernel_access,
					&mapped_vm_user_access);

	if (mapped) {
		mapped_base += vbase & (mapped_size - 1U);
		ret = (expected_phys == mapped_base) &&
		      (expected_memtype == mapped_memtype) &&
		      (expected_access == mapped_vm_kernel_access) &&
		      (expected_access == mapped_vm_user_access);
	}

	return ret;
}

static bool
is_owner(memextent_t *me, paddr_t phys, size_t size)
{
	return memdb_is_ownership_contiguous(phys, phys + size - 1U,
					     (uintptr_t)me, MEMDB_TYPE_EXTENT);
}

void
tests_memextent_sparse_init(void)
{
	as1 = create_addrspace(33U);
	as2 = create_addrspace(44U);
}

bool
tests_memextent_sparse_start(void)
{
	error_t err;

	cpulocal_begin();
	cpu_index_t cpu = cpulocal_get_index();
	cpulocal_end();

	if (cpu != 0U) {
		goto wait;
	}

	LOG(DEBUG, INFO, "Starting sparse memextent tests");

	memextent_t *me_0_0 = create_memextent(NULL, 0U, PHYS_MAX, true);
	assert(me_0_0 != NULL);

	// Test 1: Apply mapping after donate from partition.
	vmaddr_t	     vbase   = 0x80000000U;
	size_t		     size    = PGTABLE_VM_PAGE_SIZE;
	paddr_t		     phys    = get_free_phys_range(size);
	pgtable_vm_memtype_t memtype = PGTABLE_VM_MEMTYPE_NORMAL_WB;
	pgtable_access_t     access  = PGTABLE_ACCESS_RW;

	err = map_memextent(me_0_0, as1, vbase, phys, size, memtype, access);
	assert(err == OK);

	bool mapped = lookup_addrspace(as1, vbase, phys, memtype, access);
	assert(!mapped);

	err = memextent_donate_child(me_0_0, phys, size, false);
	assert(err == OK);

	mapped = lookup_addrspace(as1, vbase, phys, memtype, access);
	assert(mapped);

	err = memextent_donate_child(me_0_0, phys, size, true);
	assert(err == OK);

	mapped = lookup_addrspace(as1, vbase, phys, memtype, access);
	assert(!mapped);

	// Test 2: Donate between siblings.
	memextent_t *me_1_0 = create_memextent(me_0_0, 0U, PHYS_MAX, true);
	assert(me_1_0 != NULL);

	memextent_t *me_1_1 = create_memextent(me_0_0, 0U, PHYS_MAX, true);
	assert(me_1_1 != NULL);

	size = 0x10000U;
	phys = get_free_phys_range(size);

	err = memextent_donate_child(me_0_0, phys, size, false);
	assert(err == OK);

	bool owner = is_owner(me_0_0, phys, size);
	assert(owner);

	vmaddr_t vbase_1  = 0x60000000U;
	vmaddr_t vbase_2a = 0x340404000U;
	vmaddr_t vbase_2b = 0x288840000U;

	err = map_memextent(me_1_0, as1, vbase_1, phys, 0x6000U, memtype,
			    access);
	assert(err == OK);

	err = map_memextent(me_1_0, as2, vbase_2a, phys, size, memtype, access);
	assert(err == OK);

	err = map_memextent(me_1_1, as1, vbase_1 + 0x4000U, phys + 0x4000U,
			    0x6000U, memtype, access);
	assert(err == OK);

	err = map_memextent(me_1_1, as2, vbase_2b, phys, size, memtype, access);
	assert(err == OK);

	mapped = lookup_addrspace(as1, vbase_1, phys, memtype, access);
	assert(!mapped);
	mapped = lookup_addrspace(as1, vbase_1 + 0x6000U, phys + 0x6000U,
				  memtype, access);
	assert(!mapped);
	mapped = lookup_addrspace(as2, vbase_2a, phys, memtype, access);
	assert(!mapped);
	mapped = lookup_addrspace(as2, vbase_2b, phys, memtype, access);
	assert(!mapped);

	err = memextent_donate_child(me_1_0, phys, size, false);
	assert(err == OK);

	owner = is_owner(me_1_0, phys, size);
	assert(owner);

	mapped = lookup_addrspace(as1, vbase_1, phys, memtype, access);
	assert(mapped);
	mapped = lookup_addrspace(as1, vbase_1 + 0x6000U, phys + 0x6000U,
				  memtype, access);
	assert(!mapped);
	mapped = lookup_addrspace(as2, vbase_2a, phys, memtype, access);
	assert(mapped);
	mapped = lookup_addrspace(as2, vbase_2b, phys, memtype, access);
	assert(!mapped);

	err = memextent_donate_sibling(me_1_0, me_1_1, phys, size);
	assert(err == OK);

	owner = is_owner(me_1_1, phys, size);
	assert(owner);

	mapped = lookup_addrspace(as1, vbase_1, phys, memtype, access);
	assert(!mapped);
	mapped = lookup_addrspace(as1, vbase_1 + 0x6000U, phys + 0x6000U,
				  memtype, access);
	assert(mapped);
	mapped = lookup_addrspace(as2, vbase_2a, phys, memtype, access);
	assert(!mapped);
	mapped = lookup_addrspace(as2, vbase_2b, phys, memtype, access);
	assert(mapped);

	err = memextent_unmap_partial(me_1_0, as1, vbase_1, phys, 0x6000U);
	assert(err == OK);

	err = memextent_unmap_partial(me_1_0, as2, vbase_2a, phys, size);
	assert(err == OK);

	memextent_unmap_all(me_1_1);

	mapped = lookup_addrspace(as1, vbase_1, phys, memtype, access);
	assert(!mapped);
	mapped = lookup_addrspace(as1, vbase_1 + 0x6000U, phys + 0x6000U,
				  memtype, access);
	assert(!mapped);
	mapped = lookup_addrspace(as2, vbase_2a, phys, memtype, access);
	assert(!mapped);
	mapped = lookup_addrspace(as2, vbase_2b, phys, memtype, access);
	assert(!mapped);

	object_put_addrspace(as1);
	object_put_addrspace(as2);

	object_put_memextent(me_0_0);
	object_put_memextent(me_1_0);
	object_put_memextent(me_1_1);

	LOG(DEBUG, INFO, "Finished sparse memextent tests");

	asm_event_store_and_wake(&tests_complete, true);

wait:
	while (!asm_event_load_before_wait(&tests_complete)) {
		asm_event_wait(&tests_complete);
	}

	return false;
}

#else

extern char unused;

#endif

```

`hyp/mem/pgtable/armv8/pgtable.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

module pgtable

subscribe boot_cold_init()
	// Must be called prior to memdb and hyp address space initialisation.
	priority 30

subscribe boot_runtime_warm_init()

```

`hyp/mem/pgtable/armv8/pgtable.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// FIXME: move this to the vm/address space module
define vmid_t public newtype uint16;

#if defined(ARCH_AARCH64_USE_S2FWB)
#if !defined(ARCH_ARM_FEAT_S2FWB)
#error S2FWB requires ARCH_ARM_FEAT_S2FWB
#endif
#error S2FWB support not implemented
#else
extend pgtable_vm_memtype enumeration {
	DEVICE_nGnRnE	= 0;
	DEVICE_nGnRE	= 1;
	DEVICE_nGRE	= 2;
	DEVICE_GRE	= 3;

	NORMAL_NC	= 0b0101;
	NORMAL_ONC_IWT	= 0b0110;
	NORMAL_ONC_IWB	= 0b0111;
	NORMAL_OWT_INC	= 0b1001;
	NORMAL_WT	= 0b1010;
	NORMAL_OWT_IWB	= 0b1011;
	NORMAL_OWB_INC	= 0b1101;
	NORMAL_OWB_IWT	= 0b1110;
	NORMAL_WB	= 0b1111;
};
#endif

// FIXME: define these as a stronger types
define vmsa_upper_attrs_t newtype uint16;
define vmsa_lower_attrs_t newtype uint16;

// FIXME: assume LPA is no defined
define vmsa_general_entry bitfield<64> {
	0		is_valid		bool = 0;
	1		is_table		bool = 0;
	63:2		unknown = 0;
};

define vmsa_level_table_t newtype bitfield vmsa_general_entry(atomic);

define vmsa_block_entry bitfield<64> {
	1:0		type			uint8(const) = 0b1;
	11:2		lower_attrs		type vmsa_lower_attrs_t = 0;
	15:12		unknown = 0;
#if defined(ARCH_ARM_FEAT_BBM)
	16		nT			bool = 0;
#else
	16		unknown = 0;
#endif
	47:17		OutputAddress		type paddr_t lsl(17);
	63:48		upper_attrs		type vmsa_upper_attrs_t = 0;
};

define vmsa_table_entry bitfield<64> {
	1:0		type			uint8(const) = 0b11;
	11:2		SW_Ignored1		uint16 = 0;
	47:12		NextLevelTableAddress	type paddr_t lsl(12);
	51:48		unknown = 0;
	58:52		SW_Ignored2		uint16 = 0;
	/* ignore the attributes for table entry */
	63:59		unknown = 0;
};

// FIXME: assume LPA is not defined, or else for granule 64K,
// layout is different
define vmsa_page_entry bitfield<64> {
	1:0		type			uint8(const) = 0b11;
	11:2		lower_attrs		type vmsa_lower_attrs_t = 0;
	47:12		OutputAddress		type paddr_t lsl(12);
	63:48		upper_attrs		type vmsa_upper_attrs_t = 0;
};

// Common type for page/block entry attribute
define vmsa_page_and_block_attrs_entry bitfield<64> {
	1:0		type			uint8(const) = 0b11;
	11:2		lower_attrs		type vmsa_lower_attrs_t = 0;
	47:12		unknown = 0;
	63:48		upper_attrs		type vmsa_upper_attrs_t = 0;
};

// union type for use where the entry type is unknown
define vmsa_entry union {
		base bitfield vmsa_general_entry;
		page bitfield vmsa_page_entry;
		block bitfield vmsa_block_entry;
		table bitfield vmsa_table_entry;
		attrs bitfield vmsa_page_and_block_attrs_entry;
};

// common lower attribute structure
define vmsa_common_lower_attrs bitfield<10> {
	5:0		unknown;
	7:6		SH			enumeration vmsa_shareability;
	8		AF			bool(const);
	9		unknown;
};

// common upper attribute structure
define vmsa_common_upper_attrs bitfield<16> {
	2:0		unknown = 0;
	3		DBM			bool = 0;
	4		cont			bool = 0;
	6:5		unknown = 0;
	10:7		unknown = 0;
	14:11		PBHA			uint8 = 0;
	15		unknown = 0;
};

// page and block stage-1 upper attributes
define vmsa_stg1_upper_attrs bitfield<16> {
#if defined(ARCH_ARM_FEAT_BTI)
	1:0		res0			uint8(const) = 0;
	2		GP			bool = 0;
#else
	2:0		res0			uint8(const) = 0;
#endif
	3		DBM			bool = 0;
	4		cont			bool = 0;
#if defined(ARCH_ARM_FEAT_VHE)
	5		PXN			bool = 1;
	6		UXN			bool = 1;
#else
	5		unknown = 0;
	6		XN			bool = 1;
#endif
	10:7		SW_Ignored1		uint8 = 0;
	14:11		PBHA			uint8 = 0;
	15		SW_Ignored2		uint8 = 0;
};

define vmsa_stg1_ap enumeration(explicit) {
	EL0_NONE_UPPER_READ_WRITE = 0x0;
	ALL_READ_WRITE = 0x1;
	EL0_NONE_UPPER_READ_ONLY = 0x2;
	ALL_READ_ONLY = 0x3;
};

define vmsa_shareability enumeration(explicit) {
	NON_SHAREABLE = 0x0;
	RESERVED = 0x1;
	OUTER_SHAREABLE = 0x2;
	INNER_SHAREABLE = 0x3;
};

define vmsa_stg1_lower_attrs bitfield<10> {
	2:0		attr_idx		enumeration pgtable_hyp_memtype = 0;
	3		NS			bool = 0;
	// FIXME: could not accept enumeration as constant value, use
	// actually value instead
	5:4		AP			enumeration vmsa_stg1_ap = 2;
	7:6		SH			enumeration vmsa_shareability = 3;
	8		AF			bool(const) = 1;
	9		nG			bool = 0;
};

define vmsa_stg2_upper_attrs bitfield<16> {
	2:0		unknown = 0;
	3		DBM			bool = 0;
	4		cont			bool = 0;
#if defined(ARCH_ARM_FEAT_XNX)
	5		PXNxorUXN		bool;
	6		UXN			bool;
#else
	5		unknown = 0;
	6		XN			bool;
#endif
	10:7		SW_Ignored1		uint8 = 0;
	// assume ttpbha is implemented
	14:11		PBHA			uint8 = 0;
	15		unknown = 0;
};

define vmsa_s2ap enumeration(explicit) {
	NONE = 0x0;
	READ_ONLY = 0x1;
	WRITE_ONLY = 0x2;
	READ_WRITE = 0x3;
};

define vmsa_stg2_lower_attrs bitfield<10> {
	3:0		mem_attr		enumeration pgtable_vm_memtype;
	5:4		S2AP			enumeration vmsa_s2ap = 0;
	7:6		SH			enumeration vmsa_shareability = 3;
	8		AF			bool(const) = 1;
	9		unknown = 0;
};

define vmsa_tlbi_va_input bitfield<64> {
	43:0		VA			type vmaddr_t lsl(12);
	47:44		TTL			uint8 = 0;
	63:48		ASID			uint16 = 0;
};

define vmsa_tlbi_vaa_input bitfield<64> {
	43:0		VA			type vmaddr_t lsl(12);
	47:44		TTL			uint8 = 0;
	63:48		unknown = 0;
};

define vmsa_tlbi_ipa_input bitfield<64> {
	35:0		IPA			type vmaddr_t lsl(12);
	// without LPA implemented, or it's IPA
	39:36		unknown = 0;
	43:40		unknown = 0;
	47:44		TTL			uint8 = 0;
	62:48		unknown = 0;
	// if in non-secure env, it's reserved.
	63		NS			bool = 0;
};

#ifdef ARCH_ARM_FEAT_TLBIRANGE
define vmsa_tlbi_va_range_input bitfield<64> {
	36:0		BaseADDR		uint64;
	38:37		TTL			uint8 = 0;
	43:39		NUM			uint8;
	45:44		SCALE			uint8;
	47:46		TG			enumeration tlbi_range_tg;
	63:48		ASID			uint16 = 0;
};

define vmsa_tlbi_ipa_range_input bitfield<64> {
	36:0		BaseADDR		uint64;
	38:37		TTL			uint8 = 0;
	43:39		NUM			uint8;
	45:44		SCALE			uint8;
	47:46		TG			enumeration tlbi_range_tg;
	62:48		unknown = 0;
	63		NS			bool = 0;
};

define TLBI_RANGE_NUM_MAX	constant uint8 = (1 << 5) - 1;
define TLBI_RANGE_SCALE_MAX	constant uint8 = (1 << 2) - 1;
#endif

// Gunyah extensions to table entries (software bit usage)
extend vmsa_table_entry bitfield {
	// Use the table entry ignore bits to hold a reference count, keeping
	// track of the number of entries in the immediately lower level in
	// addition to the number of modifier threads operating at or below
	// this entry.
	delete		SW_Ignored1;
#if (PGTABLE_HYP_PAGE_SIZE > 4096) || (PGTABLE_VM_PAGE_SIZE > 4096)
	// For large granules the lower 10 ignored bits are not enough to hold
	// the reference count; use the upper ignored bits as well. For the
	// largest granule size, 64KiB, we need 14 bits (max 8192 next level
	// entries).
	delete		SW_Ignored2;
	55:52,11:2	refcount type count_t = 0;
#else
	// For 4KiB granules the reference count only needs 10 bits (max 512
	// next level entries).
	11:2		refcount type count_t = 0;
#endif
};

define pgtable_stage_type enumeration(noprefix) {
	PGTABLE_HYP_STAGE_1;
	PGTABLE_VM_STAGE_2;
};

define pgtable_entry_types bitfield<8> {
	auto next_level_table bool;
	auto page bool;
	auto block bool;
	auto invalid bool;
	auto reserved bool;
	auto error bool;
	others unknown = 0;
};

define pgtable_level enumeration(noprefix) {
	PGTABLE_LEVEL_0 = 0;
	PGTABLE_LEVEL_1;
	PGTABLE_LEVEL_2;
	PGTABLE_LEVEL_3;
	PGTABLE_LEVEL_OFFSET;
};

define PGTABLE_INVALID_LEVEL constant type index_t = -1;

define pgtable_level_info structure {
	// [msb:lsb], range identifies the index in the address
	msb type index_t;
	lsb type index_t;
	// OutputAddress mask for next page entry, directly & to remove
	// useless bits
	table_mask type paddr_t;
	// OutputAddress mask for block/page entry, directly & to remove
	// useless bits
	block_and_page_output_address_mask type paddr_t;
	is_offset bool;
	allowed_types bitfield pgtable_entry_types;
	// the address range size of each entry on this level
	addr_size size;
	entry_cnt type count_t;
	level enumeration pgtable_level;
	contiguous_entry_cnt size;
};

// When on a cpu with ARMv8.2-LPA, and using a 52-bit VTCR_EL2.PS setting, the
// VTTBR_EL2 has a minimum 64 byte alignment requirement 64KB granules with
// ARMv8.2-LPA. To simplify, we always impose this alignment requirement.
define VMSA_TABLE_MIN_ALIGN constant size = 64;

define pgtable structure {
	start_level	uint8;
	vmid		type vmid_t;
	granule_shift	type count_t;
	start_level_size size;

	root_pgtable type paddr_t;
	// FIXME:
	// root pointer type vmsa_level_table_t;
	root pointer bitfield vmsa_general_entry(atomic);

	address_bits type count_t;
};

extend pgtable_vm structure {
	control		structure pgtable(contained);
	vtcr_el2	bitfield VTCR_EL2;
	vttbr_el2	bitfield VTTBR_EL2;
	issue_dvm_cmd	bool;
};

define pgtable_hyp object {
#if defined(ARCH_ARM_FEAT_VHE)
	top_control structure pgtable;
#endif
	bottom_control structure pgtable;
	lock structure spinlock;
};

define pgtable_modifier_ret enumeration {
	STOP = 0;
	ERROR;
	CONTINUE;
};

define pgtable_translation_table_walk_event enumeration {
	MMAP;
	UNMAP;
	UNMAP_MATCH;
	LOOKUP;
	PREALLOC;
#ifndef NDEBUG
	DUMP;
#endif
#ifdef HOST_TEST
	EXTERNAL;
#endif
};

define pgtable_map_modifier_args structure {
	orig_virtual_address type vmaddr_t;
	orig_size size;
	phys type paddr_t;
	partition pointer object partition;
	upper_attrs type vmsa_upper_attrs_t;
	lower_attrs type vmsa_lower_attrs_t;
	// It needs to alloc new page table level during mapping, this start
	// index records the last level whose sub level needs to alloc page
	// table. Map modifier uses this index to map new physical address with
	// roll back capability.
	// With value == -1 means there's no new page table allocated since
	// level 0 page table is always there.
	new_page_start_level type index_t;
	partially_mapped_size size;
	merge_limit size;
	error enumeration error;
	stage enumeration pgtable_stage_type;
	try_map bool;
	outer_shareable bool;
};

define pgtable_lookup_modifier_args structure {
	phys type paddr_t;
	size size;
	// must be a block/page entry
	entry union vmsa_entry;
};

define pgtable_unmap_modifier_args structure {
	partition pointer object partition;
	// original virtual address, unchanged
	preserved_size size;
	stack array(maxof(enumeration pgtable_level) + 1) type paddr_t;
	phys type paddr_t;
	size size;
	stage enumeration pgtable_stage_type;
	outer_shareable bool;
};

define pgtable_prealloc_modifier_args structure {
	partition pointer object partition;
	// It needs to alloc new page table level during mapping, this start
	// index records the last level whose sub level needs to alloc page
	// table. Map modifier uses this index to map new physical address with
	// roll back capability.
	// With value == -1 means there's no new page table allocated since
	// level 0 page table is always there.
	new_page_start_level type index_t;
	error enumeration error;
};

```

`hyp/mem/pgtable/armv8/src/pgtable.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// TODO:
//
// * Make this code thread-safe. Currently the calling code is required to take
// a lock before using these functions. The level reference counting needs to be
// implemented for modification operations, and RCU used for level deletions.
//
// * Fix misra for pointer type cast.
//
// * add more checks in API level. Like, the size should be multiple of page
// size.
//
// * use the only one category of level instead of two.
//
// * change internal function to use return type (value + error code)
//
// * add test case for contiguous bit.

#include <assert.h>
#include <hyptypes.h>
#include <string.h>
#if defined(HOST_TEST)
#include <stdio.h>
#endif

#include <hypconstants.h>
#include <hypcontainers.h>

#if !defined(HOST_TEST)
#include <hypregisters.h>

#include <log.h>
#include <preempt.h>
#include <thread.h>
#include <trace.h>
#else
#include <string_util.h>
#endif

#include <compiler.h>
#include <hyp_aspace.h>
#include <panic.h>
#include <partition.h>
#include <pgtable.h>
#include <spinlock.h>
#include <util.h>

#if !defined(HOST_TEST)
#include <asm/barrier.h>
#endif

#ifdef HOST_TEST
#define TEST_EXPORTED
#else
#define TEST_EXPORTED static
#endif

#include <platform_cpu.h>

#include "event_handlers.h"
#include "events/pgtable.h"

#define SHIFT_4K  (12U)
#define SHIFT_16K (14U)
#define SHIFT_64K (16U)

// mask for [e, s]
#define segment_mask(e, s) (util_mask((e) + 1U) & (~util_mask(s)))

// Every legal entry type except next level tables
static const pgtable_entry_types_t VMSA_ENTRY_TYPE_LEAF =
	pgtable_entry_types_cast(PGTABLE_ENTRY_TYPES_BLOCK_MASK |
				 PGTABLE_ENTRY_TYPES_PAGE_MASK |
				 PGTABLE_ENTRY_TYPES_INVALID_MASK);

#if defined(HOST_TEST)
// Definitions for host test

bool pgtable_op = true;

#define LOG(...) LOG_I(__VA_ARGS__, 5, 4, 3, 2, 1, 0, _unspecified_id)
#define LOG_I(tclass, log_level, fmt, a0, a1, a2, a3, a4, n, ...)              \
	LOG_##n((fmt), (a0), (a1), (a2), (a3), (a4), __VA_ARGS__)
#define LOG_0(fmt, ...)			LOG_V((fmt), 0, 0, 0, 0, 0)
#define LOG_1(fmt, a0, ...)		LOG_V((fmt), (a0), 0, 0, 0, 0)
#define LOG_2(fmt, a0, a1, ...)		LOG_V((fmt), (a0), (a1), 0, 0, 0)
#define LOG_3(fmt, a0, a1, a2, ...)	LOG_V((fmt), (a0), (a1), (a2), 0, 0)
#define LOG_4(fmt, a0, a1, a2, a3, ...) LOG_V((fmt), (a0), (a1), (a2), (a3), 0)
#define LOG_5(fmt, a0, a1, a2, a3, a4, ...)                                    \
	LOG_V((fmt), (a0), (a1), (a2), (a3), (a4))
#define LOG_V(fmt, a0, a1, a2, a3, a4)                                         \
	do {                                                                   \
		char log[1024];                                                \
		snprint(log, 1024, (fmt), (a0), (a1), (a2), (a3), (a4));       \
		puts(log);                                                     \
	} while (0)

#define PGTABLE_VM_PAGE_SIZE 4096

#define PGTABLE_TRANSLATION_TABLE_WALK_EVENT_EXTERNAL                          \
	(PGTABLE_TRANSLATION_TABLE_WALK_EVENT__MAX + 1)
#else
// For target HW

#if defined(NDEBUG)
// pgtable_op is not actually defined for NDEBUG
extern bool pgtable_op;
#else
static _Thread_local bool pgtable_op;
#endif

extern vmsa_general_entry_t aarch64_pt_ttbr_level1;

#endif

#if (CPU_PGTABLE_BBM_LEVEL > 0) && !defined(ARCH_ARM_FEAT_BBM)
#error CPU_PGTABLE_BBM_LEVEL > 0 but ARCH_ARM_FEAT_BBM not defined
#endif

#define PGTABLE_LEVEL_NUM ((index_t)PGTABLE_LEVEL__MAX + 1U)

typedef struct stack_elem {
	paddr_t		    paddr;
	vmsa_level_table_t *table;
	count_t		    entry_cnt;
	bool		    mapped;
	bool		    need_unmap;
	char		    padding[2];
} stack_elem_t;

typedef struct {
	uint8_t level;
	uint8_t padding[7];
	size_t	size;
} get_start_level_info_ret_t;

#if defined(PLATFORM_PGTABLE_4K_GRANULE)
// Statically support only 4k granule size for now
#define level_conf info_4k_granules

static const pgtable_level_info_t info_4k_granules[PGTABLE_LEVEL_NUM] = {
	// level 0
	{ .msb				      = 47,
	  .lsb				      = 39,
	  .table_mask			      = segment_mask(47U, 12),
	  .block_and_page_output_address_mask = 0U,
	  .is_offset			      = false,
	  .allowed_types		      = pgtable_entry_types_cast(
		       PGTABLE_ENTRY_TYPES_NEXT_LEVEL_TABLE_MASK),
	  .addr_size		= util_bit(39),
	  .entry_cnt		= (count_t)util_bit(9),
	  .level		= PGTABLE_LEVEL_0,
	  .contiguous_entry_cnt = 0U },
	// level 1
	{ .msb				      = 38,
	  .lsb				      = 30,
	  .table_mask			      = segment_mask(47U, 12),
	  .block_and_page_output_address_mask = segment_mask(47U, 30),
	  .is_offset			      = false,
	  .allowed_types		      = pgtable_entry_types_cast(
		       PGTABLE_ENTRY_TYPES_NEXT_LEVEL_TABLE_MASK |
		       PGTABLE_ENTRY_TYPES_BLOCK_MASK),
	  .addr_size		= util_bit(30),
	  .entry_cnt		= (count_t)util_bit(9),
	  .level		= PGTABLE_LEVEL_1,
	  .contiguous_entry_cnt = 16U },
	// level 2
	{ .msb				      = 29,
	  .lsb				      = 21,
	  .table_mask			      = segment_mask(47U, 12),
	  .block_and_page_output_address_mask = segment_mask(47U, 21),
	  .is_offset			      = false,
	  .allowed_types		      = pgtable_entry_types_cast(
		       PGTABLE_ENTRY_TYPES_NEXT_LEVEL_TABLE_MASK |
		       PGTABLE_ENTRY_TYPES_BLOCK_MASK),
	  .addr_size		= util_bit(21),
	  .entry_cnt		= (count_t)util_bit(9),
	  .level		= PGTABLE_LEVEL_2,
	  .contiguous_entry_cnt = 16U },
	// level 3
	{ .msb				      = 20,
	  .lsb				      = 12,
	  .table_mask			      = 0,
	  .block_and_page_output_address_mask = segment_mask(47U, 12),
	  .is_offset			      = false,
	  .allowed_types =
		  pgtable_entry_types_cast(PGTABLE_ENTRY_TYPES_PAGE_MASK),
	  .addr_size		= util_bit(12),
	  .entry_cnt		= (count_t)util_bit(9),
	  .level		= PGTABLE_LEVEL_3,
	  .contiguous_entry_cnt = 16U },
	// offset
	{ .msb				      = 11,
	  .lsb				      = 0,
	  .table_mask			      = 0U,
	  .block_and_page_output_address_mask = 0U,
	  .is_offset			      = true,
	  .allowed_types		      = pgtable_entry_types_cast(0U),
	  .addr_size			      = 0U,
	  .entry_cnt			      = 0U,
	  .level			      = PGTABLE_LEVEL_OFFSET,
	  .contiguous_entry_cnt		      = 0U }
};

#elif defined(PLATFORM_PGTABLE_16K_GRANULE)
#define level_conf info_16k_granules

// FIXME: temporarily disable it, enable it for run time configuration
static const pgtable_level_info_t info_16k_granules[PGTABLE_LEVEL_NUM] = {
	// FIXME: level 0 is not permitted for stage-2 (in VTCR_EL2), must use
	// two concatenated level-1 entries.
	// level 0
	{ .msb				      = 47,
	  .lsb				      = 47,
	  .table_mask			      = segment_mask(47U, 14),
	  .block_and_page_output_address_mask = 0U,
	  .is_offset			      = false,
	  .allowed_types		      = pgtable_entry_types_cast(
		       PGTABLE_ENTRY_TYPES_NEXT_LEVEL_TABLE_MASK),
	  .addr_size		= util_bit(47),
	  .entry_cnt		= (count_t)util_bit(1),
	  .level		= PGTABLE_LEVEL_0,
	  .contiguous_entry_cnt = 0U },
	// level 1
	{ .msb				      = 46,
	  .lsb				      = 36,
	  .table_mask			      = segment_mask(47U, 14),
	  .block_and_page_output_address_mask = segment_mask(47U, 36),
	  .is_offset			      = false,
	  .allowed_types		      = pgtable_entry_types_cast(
		       PGTABLE_ENTRY_TYPES_NEXT_LEVEL_TABLE_MASK),
	  .addr_size		= util_bit(36),
	  .entry_cnt		= (count_t)util_bit(11),
	  .level		= PGTABLE_LEVEL_1,
	  .contiguous_entry_cnt = 0U },
	// level 2
	{ .msb				      = 35,
	  .lsb				      = 25,
	  .table_mask			      = segment_mask(47U, 14),
	  .block_and_page_output_address_mask = segment_mask(47U, 25),
	  .is_offset			      = false,
	  .allowed_types		      = pgtable_entry_types_cast(
		       PGTABLE_ENTRY_TYPES_NEXT_LEVEL_TABLE_MASK |
		       PGTABLE_ENTRY_TYPES_BLOCK_MASK),
	  .addr_size		= util_bit(25),
	  .entry_cnt		= (count_t)util_bit(11),
	  .level		= PGTABLE_LEVEL_2,
	  .contiguous_entry_cnt = 32U },
	// level 3
	{ .msb				      = 24,
	  .lsb				      = 14,
	  .table_mask			      = 0U,
	  .block_and_page_output_address_mask = segment_mask(47U, 14),
	  .is_offset			      = false,
	  .allowed_types =
		  pgtable_entry_types_cast(PGTABLE_ENTRY_TYPES_PAGE_MASK),
	  .addr_size		= util_bit(14),
	  .entry_cnt		= (count_t)util_bit(11),
	  .level		= PGTABLE_LEVEL_3,
	  .contiguous_entry_cnt = 128U },
	// offset
	{ .msb				      = 13,
	  .lsb				      = 0,
	  .table_mask			      = 0U,
	  .block_and_page_output_address_mask = 0U,
	  .is_offset			      = true,
	  .allowed_types		      = pgtable_entry_types_cast(0U),
	  .addr_size			      = 0U,
	  .entry_cnt			      = 0U,
	  .level			      = PGTABLE_LEVEL_OFFSET,
	  .contiguous_entry_cnt		      = 0U }
};

#elif defined(PLATFORM_PGTABLE_64K_GRANULE)
#define level_conf info_64k_granules

// NOTE: check page 2416, table D5-20 properties of the address lookup levels
// 64kb granule size
static const pgtable_level_info_t info_64k_granules[PGTABLE_LEVEL_NUM] = {
	// level 0
	{ .msb				      = 47,
	  .lsb				      = 42,
	  .table_mask			      = segment_mask(47U, 16),
	  .block_and_page_output_address_mask = 0U,
	  .is_offset			      = false,
	  // No LPA support, so no block type
	  .allowed_types = pgtable_entry_types_cast(
		  PGTABLE_ENTRY_TYPES_NEXT_LEVEL_TABLE_MASK),
	  .addr_size		= util_bit(42),
	  .entry_cnt		= (count_t)util_bit(6),
	  .level		= PGTABLE_LEVEL_1,
	  .contiguous_entry_cnt = 0U },
	// level 1
	{ .msb				      = 41,
	  .lsb				      = 29,
	  .table_mask			      = segment_mask(47U, 16),
	  .block_and_page_output_address_mask = segment_mask(47U, 29),
	  .is_offset			      = false,
	  .allowed_types		      = pgtable_entry_types_cast(
		       PGTABLE_ENTRY_TYPES_NEXT_LEVEL_TABLE_MASK |
		       PGTABLE_ENTRY_TYPES_BLOCK_MASK),
	  .addr_size		= util_bit(29),
	  .entry_cnt		= (count_t)util_bit(13),
	  .level		= PGTABLE_LEVEL_2,
	  .contiguous_entry_cnt = 32U },
	// level 2
	{ .msb				      = 28,
	  .lsb				      = 16,
	  .table_mask			      = 0U,
	  .block_and_page_output_address_mask = segment_mask(47U, 16),
	  .is_offset			      = false,
	  .allowed_types =
		  pgtable_entry_types_cast(PGTABLE_ENTRY_TYPES_PAGE_MASK),
	  .addr_size		= util_bit(16),
	  .entry_cnt		= (count_t)util_bit(13),
	  .level		= PGTABLE_LEVEL_3,
	  .contiguous_entry_cnt = 32U },
	// offset
	{ .msb				      = 15,
	  .lsb				      = 0,
	  .table_mask			      = 0U,
	  .block_and_page_output_address_mask = 0U,
	  .is_offset			      = true,
	  .allowed_types		      = pgtable_entry_types_cast(0U),
	  .addr_size			      = 0U,
	  .entry_cnt			      = 0U,
	  .level			      = PGTABLE_LEVEL_OFFSET,
	  .contiguous_entry_cnt		      = 0U }
};
#else
#error Need to specify page table granule for pgtable module
#endif

static pgtable_hyp_t hyp_pgtable;
static paddr_t	     ttbr0_phys;

#if !defined(NDEBUG)
// just for debug
void
pgtable_hyp_dump(void);

void
pgtable_vm_dump(pgtable_vm_t *pgt);
#endif // !defined(NDEBUG)

#if defined(HOST_TEST)
// Private type for external modifier, only used by test cases
typedef pgtable_modifier_ret_t (*ext_func_t)(
	pgtable_t *pgt, vmaddr_t virtual_address, size_t size, index_t idx,
	index_t level, pgtable_entry_types_t type,
	stack_elem_t stack[PGTABLE_LEVEL_NUM], void *data, index_t *next_level,
	vmaddr_t *next_virtual_address, size_t *next_size, paddr_t next_table);

typedef struct ext_modifier_args {
	ext_func_t func;
	void	  *data;
} ext_modifier_args_t;

void
pgtable_hyp_ext(vmaddr_t virtual_address, size_t size,
		pgtable_entry_types_t entry_types, ext_func_t func, void *data);

void
pgtable_vm_ext(pgtable_vm_t *pgtable, vmaddr_t virtual_address, size_t size,
	       pgtable_entry_types_t entry_types, ext_func_t func, void *data);
#endif // defined(HOST_TEST)

static void
hyp_tlbi_va(vmaddr_t virtual_address)
{
	// FIXME: before invalidate tlb, should we wait for all device/normal
	// memory write operations done.
	vmsa_tlbi_va_input_t input;

	vmsa_tlbi_va_input_init(&input);
	vmsa_tlbi_va_input_set_VA(&input, virtual_address);

#ifndef HOST_TEST
	__asm__ volatile("tlbi VAE2IS, %[VA]	;"
			 : "+m"(asm_ordering)
			 : [VA] "r"(vmsa_tlbi_va_input_raw(input)));
#endif
}

static void
vm_tlbi_ipa(vmaddr_t virtual_address, bool outer_shareable)
{
	vmsa_tlbi_ipa_input_t input;

	vmsa_tlbi_ipa_input_init(&input);
	vmsa_tlbi_ipa_input_set_IPA(&input, virtual_address);

#ifndef HOST_TEST
#ifdef ARCH_ARM_FEAT_TLBIOS
	if (outer_shareable) {
		__asm__ volatile("tlbi IPAS2E1OS, %[VA]	;"
				 : "+m"(asm_ordering)
				 : [VA] "r"(vmsa_tlbi_ipa_input_raw(input)));
	} else {
		__asm__ volatile("tlbi IPAS2E1IS, %[VA]	;"
				 : "+m"(asm_ordering)
				 : [VA] "r"(vmsa_tlbi_ipa_input_raw(input)));
	}

#else
	(void)outer_shareable;
	__asm__ volatile("tlbi IPAS2E1IS, %[VA]	;"
			 : "+m"(asm_ordering)
			 : [VA] "r"(vmsa_tlbi_ipa_input_raw(input)));
#endif
#endif
}

#ifdef ARCH_ARM_FEAT_TLBIRANGE
static tlbi_range_tg_t
hyp_tlbi_range_get_tg(count_t granule_shift)
{
	tlbi_range_tg_t tg;

	switch (granule_shift) {
	case SHIFT_4K:
		tg = TLBI_RANGE_TG_GRANULE_SIZE_4KB;
		break;
	case SHIFT_16K:
		tg = TLBI_RANGE_TG_GRANULE_SIZE_16KB;
		break;
	case SHIFT_64K:
		tg = TLBI_RANGE_TG_GRANULE_SIZE_64KB;
		break;
	default:
		panic("Invalid granule size");
	}

	return tg;
}

// Find a (scale, num) pair for the requested range.
//
// The range covered by the TLBI range instructions is:
// ((NUM + 1) * (2 ^ (5 * SCALE + 1)) * Translation_Granule_Size
//
// Returns false if the requested size is bigger than the maximum possible range
// size (8GB for 4K granules) after alignment.
static bool
hyp_tlbi_range_find_scale_num(uint64_t size, count_t granule_shift,
			      uint8_t *scale, uint8_t *num)
{
	uint8_t	 calc_scale;
	uint64_t granules = size >> granule_shift;

	for (calc_scale = 0U; calc_scale <= TLBI_RANGE_SCALE_MAX;
	     calc_scale++) {
		count_t	 scale_shift = (5U * (count_t)calc_scale) + 1U;
		uint64_t aligned_granules =
			util_p2align_up(granules, scale_shift);
		uint64_t calc_num = (aligned_granules >> scale_shift) - 1U;
		if (calc_num <= TLBI_RANGE_NUM_MAX) {
			// Found a pair of scale, num
			*scale = calc_scale;
			*num   = (uint8_t)calc_num;
			break;
		}
	}

	return calc_scale <= TLBI_RANGE_SCALE_MAX;
}

static void
hyp_tlbi_va_range(vmaddr_t va_start, size_t size, count_t granule_shift)
{
	uint8_t num, scale;

	bool ret = hyp_tlbi_range_find_scale_num(size, granule_shift, &scale,
						 &num);

	if (ret) {
		vmsa_tlbi_va_range_input_t input;
		vmsa_tlbi_va_range_input_init(&input);
		vmsa_tlbi_va_range_input_set_BaseADDR(
			&input, va_start >> granule_shift);
		vmsa_tlbi_va_range_input_set_NUM(&input, num);
		vmsa_tlbi_va_range_input_set_SCALE(&input, scale);
		vmsa_tlbi_va_range_input_set_TG(
			&input, hyp_tlbi_range_get_tg(granule_shift));

#ifndef HOST_TEST
		__asm__ volatile(
			"tlbi RVAE2IS, %[VA]	;"
			: "+m"(asm_ordering)
			: [VA] "r"(vmsa_tlbi_va_range_input_raw(input)));
#endif
	} else {
		// Range is >8GB; flush the whole address space.
		__asm__ volatile("tlbi ALLE2IS" : "+m"(asm_ordering));
	}
}

static void
hyp_tlbi_ipa_range(vmaddr_t ipa_start, size_t size, count_t granule_shift,
		   bool outer_shareable)
{
	uint8_t num, scale;

	bool ret = hyp_tlbi_range_find_scale_num(size, granule_shift, &scale,
						 &num);
	if (ret) {
		vmsa_tlbi_ipa_range_input_t input;
		vmsa_tlbi_ipa_range_input_init(&input);
		vmsa_tlbi_ipa_range_input_set_BaseADDR(
			&input, ipa_start >> granule_shift);
		vmsa_tlbi_ipa_range_input_set_NUM(&input, num);
		vmsa_tlbi_ipa_range_input_set_SCALE(&input, scale);
		vmsa_tlbi_ipa_range_input_set_TG(
			&input, hyp_tlbi_range_get_tg(granule_shift));

#ifndef HOST_TEST
#if defined(ARCH_ARM_FEAT_TLBIOS)
		if (outer_shareable) {
			__asm__ volatile(
				"tlbi RIPAS2E1OS, %[VA]	;"
				: "+m"(asm_ordering)
				: [VA] "r"(
					vmsa_tlbi_ipa_range_input_raw(input)));
		} else {
			__asm__ volatile(
				"tlbi RIPAS2E1IS, %[VA]	;"
				: "+m"(asm_ordering)
				: [VA] "r"(
					vmsa_tlbi_ipa_range_input_raw(input)));
		}
#else
		(void)outer_shareable;
		__asm__ volatile(
			"tlbi RIPAS2E1IS, %[VA]	;"
			: "+m"(asm_ordering)
			: [VA] "r"(vmsa_tlbi_ipa_range_input_raw(input)));
#endif
#endif
	} else {
#ifndef HOST_TEST
		// Range is >8GB; flush the whole address space.
#if defined(ARCH_ARM_FEAT_TLBIOS)
		if (outer_shareable) {
			__asm__ volatile("tlbi VMALLS12E1OS"
					 : "+m"(asm_ordering));
		} else {
			__asm__ volatile("tlbi VMALLS12E1IS"
					 : "+m"(asm_ordering));
		}
#else
		__asm__ volatile("tlbi VMALLS12E1IS" : "+m"(asm_ordering));
#endif
#endif
	}
}
#endif

static void
dsb_st(bool outer_shareable)
{
#ifndef HOST_TEST
#if defined(ARCH_ARM_FEAT_TLBIOS)
	if (outer_shareable) {
		__asm__ volatile("dsb oshst" ::: "memory");
	} else {
		__asm__ volatile("dsb ishst" ::: "memory");
	}
#else
	(void)outer_shareable;
	__asm__ volatile("dsb ishst" ::: "memory");
#endif
#endif
}

static void
dsb(bool outer_shareable)
{
#ifndef HOST_TEST
#if defined(ARCH_ARM_FEAT_TLBIOS)
	if (outer_shareable) {
		__asm__ volatile("dsb osh" ::: "memory");
	} else {
		__asm__ volatile("dsb ish" ::: "memory");
	}
#else
	(void)outer_shareable;
	__asm__ volatile("dsb ish" ::: "memory");
#endif
#endif
}

static void
vm_tlbi_vmalle1(bool outer_shareable)
{
#ifndef HOST_TEST
#ifdef ARCH_ARM_FEAT_TLBIOS
	if (outer_shareable) {
		__asm__ volatile("tlbi VMALLE1OS" ::: "memory");
	} else {
		__asm__ volatile("tlbi VMALLE1IS" ::: "memory");
	}
#else
	(void)outer_shareable;
	__asm__ volatile("tlbi VMALLE1IS" ::: "memory");
#endif
#endif
}

// return true if it's top virt address
static bool
is_high_virtual_address(vmaddr_t virtual_address);

// check if the virtual address (VA/IPA) bit count is under restriction.
// true if it's right
static bool
addr_check(vmaddr_t virtual_address, size_t bit_count, bool is_high);

#if defined(HOST_TEST)
// Unit test need these helper functions
vmsa_entry_t
get_entry(vmsa_level_table_t *table, index_t idx);

pgtable_entry_types_t
get_entry_type(vmsa_entry_t *entry, const pgtable_level_info_t *level_info);

void
get_entry_paddr(const pgtable_level_info_t *level_info, vmsa_entry_t *entry,
		pgtable_entry_types_t type, paddr_t *paddr);

count_t
get_table_refcount(vmsa_level_table_t *table, index_t idx);
#else
static vmsa_entry_t
get_entry(vmsa_level_table_t *table, index_t idx);

static pgtable_entry_types_t
get_entry_type(vmsa_entry_t *entry, const pgtable_level_info_t *level_info);

static void
get_entry_paddr(const pgtable_level_info_t *level_info, vmsa_entry_t *entry,
		pgtable_entry_types_t type, paddr_t *paddr);

static count_t
get_table_refcount(vmsa_level_table_t *table, index_t idx);
#endif

static void
set_table_refcount(vmsa_level_table_t *table, index_t idx, count_t refcount);

static pgtable_vm_memtype_t
map_stg2_attr_to_memtype(vmsa_lower_attrs_t attrs);

static pgtable_hyp_memtype_t
map_stg1_attr_to_memtype(vmsa_lower_attrs_t attrs);

static vmsa_lower_attrs_t
get_lower_attr(vmsa_entry_t entry);

static vmsa_upper_attrs_t
get_upper_attr(vmsa_entry_t entry);

static pgtable_access_t
map_stg1_attr_to_access(vmsa_upper_attrs_t upper_attrs,
			vmsa_lower_attrs_t lower_attrs);

static void
map_stg2_attr_to_access(vmsa_upper_attrs_t upper_attrs,
			vmsa_lower_attrs_t lower_attrs,
			pgtable_access_t  *kernel_access,
			pgtable_access_t  *user_access);

static void
map_stg2_memtype_to_attrs(pgtable_vm_memtype_t	   memtype,
			  vmsa_stg2_lower_attrs_t *lower_attrs);

static void
map_stg1_memtype_to_attrs(pgtable_hyp_memtype_t	   memtype,
			  vmsa_stg1_lower_attrs_t *lower_attrs);

static void
map_stg1_access_to_attrs(pgtable_access_t	  access,
			 vmsa_stg1_upper_attrs_t *upper_attrs,
			 vmsa_stg1_lower_attrs_t *lower_attrs);

static void
map_stg2_access_to_attrs(pgtable_access_t	  kernel_access,
			 pgtable_access_t	  user_access,
			 vmsa_stg2_upper_attrs_t *upper_attrs,
			 vmsa_stg2_lower_attrs_t *lower_attrs);

static void
set_invalid_entry(vmsa_level_table_t *table, index_t idx);

static void
set_page_entry(vmsa_level_table_t *table, index_t idx, paddr_t addr,
	       vmsa_upper_attrs_t upper_attrs, vmsa_lower_attrs_t lower_attrs,
	       bool contiguous, bool fence);

static void
set_block_entry(vmsa_level_table_t *table, index_t idx, paddr_t addr,
		vmsa_upper_attrs_t upper_attrs, vmsa_lower_attrs_t lower_attrs,
		bool contiguous, bool fence, bool notlb);

#if CPU_PGTABLE_BBM_LEVEL == 1U
static void
set_notlb_flag(vmsa_label_table_t *table, index_t idx, bool nt);
#endif

// Helper function for translation table walking. Stop walking if modifier
// returns false
static bool
translation_table_walk(pgtable_t *pgt, vmaddr_t virtual_address,
		       size_t virtual_address_size,
		       pgtable_translation_table_walk_event_t event,
		       pgtable_entry_types_t expected, void *data);

static error_t
alloc_level_table(partition_t *partition, size_t size, size_t alignment,
		  paddr_t *paddr, vmsa_level_table_t **table);

static void
set_pgtables(vmaddr_t virtual_address, stack_elem_t stack[PGTABLE_LEVEL_NUM],
	     index_t first_new_table_level, index_t cur_level,
	     count_t initial_refcount, index_t start_level,
	     bool outer_shareable);

static pgtable_modifier_ret_t
map_modifier(pgtable_t *pgt, vmaddr_t virtual_address, size_t size,
	     vmsa_entry_t cur_entry, index_t idx, index_t cur_level,
	     pgtable_entry_types_t type, stack_elem_t stack[PGTABLE_LEVEL_NUM],
	     void *data, index_t *next_level, vmaddr_t *next_virtual_address,
	     size_t *next_size, paddr_t next_table);

static pgtable_modifier_ret_t
lookup_modifier(pgtable_t *pgt, vmsa_entry_t cur_entry, index_t level,
		pgtable_entry_types_t type, void *data);

#if 0
static bool
map_should_set_cont(vmaddr_t virtual_address, size_t size,
		    vmaddr_t entry_address, index_t level);
#endif

static bool
unmap_should_clear_cont(vmaddr_t virtual_address, size_t size, index_t level);

static void
unmap_clear_cont_bit(vmsa_level_table_t *table, vmaddr_t virtual_address,
		     index_t			       level,
		     vmsa_page_and_block_attrs_entry_t attr_entry,
		     pgtable_unmap_modifier_args_t    *margs,
		     count_t granule_shift, index_t start_level);

static pgtable_modifier_ret_t
unmap_modifier(pgtable_t *pgt, vmaddr_t virtual_address, size_t size,
	       index_t idx, index_t level, pgtable_entry_types_t type,
	       stack_elem_t stack[PGTABLE_LEVEL_NUM], void *data,
	       index_t *next_level, vmaddr_t *next_virtual_address,
	       size_t *next_size, bool only_matching);

static pgtable_modifier_ret_t
prealloc_modifier(pgtable_t *pgt, vmaddr_t virtual_address, size_t size,
		  index_t level, pgtable_entry_types_t type,
		  stack_elem_t stack[PGTABLE_LEVEL_NUM], void *data,
		  index_t *next_level, vmaddr_t *next_virtual_address,
		  size_t *next_size);

// Return entry idx, it can make sure the returned index is always in the
// range
static inline index_t
get_index(vmaddr_t addr, const pgtable_level_info_t *info, bool is_first_level)
{
	index_t index;
	if (compiler_unexpected(is_first_level &&
				!is_high_virtual_address(addr))) {
		// Handle contiguous tables
		index = (index_t)(addr >> info->lsb);
	} else {
		index = (index_t)((addr & segment_mask(info->msb, info->lsb)) >>
				  info->lsb);
	}
	return index;
}

#ifndef NDEBUG
static inline vmaddr_t
set_index(vmaddr_t addr, const pgtable_level_info_t *info, index_t idx)
{
	// FIXME: double check if it might cause issue to clear [63, 48] bits
	return (addr & (~segment_mask(info->msb, info->lsb))) |
	       (((vmaddr_t)idx << info->lsb) &
		segment_mask(info->msb, info->lsb));
}
#endif

static inline vmaddr_t
step_virtual_address(vmaddr_t virtual_address, const pgtable_level_info_t *info)
{
	// should be fine if it overflows, might need to report error
	return (virtual_address + info->addr_size) & (~util_mask(info->lsb));
}

// Return the actual size of current entry within the specified virtual address
// range.
static inline size_t
size_on_level(vmaddr_t virtual_address, size_t size,
	      const pgtable_level_info_t *level_info)
{
	vmaddr_t v_s = virtual_address, v_e = virtual_address + size - 1U;
	vmaddr_t l_s = 0U, l_e = 0U;

	assert(!util_add_overflows(virtual_address, size - 1U));

	l_s = (virtual_address >> level_info->lsb) << level_info->lsb;
	// even for the level 0, it will set the bit 49, will not overflow for
	// 64 bit
	l_e = l_s + level_info->addr_size - 1U;

	assert(!util_add_overflows(l_s, level_info->addr_size - 1U));

	l_s = util_max(l_s, v_s);
	l_e = util_min(v_e, l_e);

	return l_e - l_s + 1U;
}

static inline vmaddr_t
entry_start_address(vmaddr_t			virtual_address,
		    const pgtable_level_info_t *level_info)
{
	return (virtual_address >> level_info->lsb) << level_info->lsb;
}

static inline bool
is_preserved_table_entry(size_t			     preserved_size,
			 const pgtable_level_info_t *level_info)
{
	assert(util_is_p2_or_zero(preserved_size));
	return preserved_size < level_info->addr_size;
}

bool
pgtable_access_check(pgtable_access_t access, pgtable_access_t access_check)
{
	return (((register_t)access & (register_t)access_check) ==
		(register_t)access_check);
}

bool
pgtable_access_is_equal(pgtable_access_t access, pgtable_access_t access_check)
{
	return ((register_t)access == (register_t)access_check);
}

pgtable_access_t
pgtable_access_mask(pgtable_access_t access, pgtable_access_t access_mask)
{
	register_t combined_access =
		((register_t)access & (register_t)access_mask);
	return (pgtable_access_t)combined_access;
}

pgtable_access_t
pgtable_access_combine(pgtable_access_t access1, pgtable_access_t access2)
{
	register_t combined_access =
		((register_t)access1 | (register_t)access2);
	return (pgtable_access_t)combined_access;
}

// Helper function to manipulate page table entry
TEST_EXPORTED vmsa_entry_t
get_entry(vmsa_level_table_t *table, index_t idx)
{
	partition_phys_access_enable(&table[idx]);
	vmsa_entry_t entry = {
		.base = atomic_load_explicit(&table[idx], memory_order_relaxed),
	};

	partition_phys_access_disable(&table[idx]);
	return entry;
}

static bool
is_high_virtual_address(vmaddr_t virtual_address)
{
#if ARCH_IS_64BIT
	// When PAC, MTE or TBI are enabled, the high bits other than bit 55
	// may be used for other purposes. Bit 55 is always used to select the
	// TTBR, regardless of these features or the address space sizes.
	return (virtual_address & util_bit(55U)) != 0U;
#else
#error unimplemented
#endif
}

static bool
addr_check(vmaddr_t virtual_address, size_t bit_count, bool is_high)
{
#if ARCH_IS_64BIT
	static_assert(sizeof(vmaddr_t) == 8U, "vmaddr_t expected to be 64bits");

	uint64_t v     = is_high ? ~virtual_address : virtual_address;
	size_t	 count = (v == 0U) ? 0U : 64U - (compiler_clz(v) + 1);
#else
#error unimplemented
#endif

	return count <= bit_count;
}

TEST_EXPORTED pgtable_entry_types_t
get_entry_type(vmsa_entry_t *entry, const pgtable_level_info_t *level_info)
{
	pgtable_entry_types_t ret = pgtable_entry_types_default();
	vmsa_general_entry_t  g	  = entry->base;

	if (vmsa_general_entry_get_is_valid(&g)) {
		if (vmsa_general_entry_get_is_table(&g)) {
			if (pgtable_entry_types_get_next_level_table(
				    &level_info->allowed_types)) {
				pgtable_entry_types_set_next_level_table(&ret,
									 true);
			} else {
				pgtable_entry_types_set_page(&ret, true);
			}
		} else {
			if (pgtable_entry_types_get_block(
				    &level_info->allowed_types)) {
				pgtable_entry_types_set_block(&ret, true);
			} else {
				pgtable_entry_types_set_reserved(&ret, true);
			}
		}
	} else {
		pgtable_entry_types_set_invalid(&ret, true);
	}

	return ret;
}

TEST_EXPORTED void
get_entry_paddr(const pgtable_level_info_t *level_info, vmsa_entry_t *entry,
		pgtable_entry_types_t type, paddr_t *paddr)
{
	vmsa_block_entry_t blk;
	vmsa_page_entry_t  pg;
	vmsa_table_entry_t tb;

	*paddr = 0U;
	if (pgtable_entry_types_get_block(&type)) {
		blk    = entry->block;
		*paddr = vmsa_block_entry_get_OutputAddress(&blk) &
			 level_info->block_and_page_output_address_mask;

	} else if (pgtable_entry_types_get_page(&type)) {
		pg     = entry->page;
		*paddr = vmsa_page_entry_get_OutputAddress(&pg) &
			 level_info->block_and_page_output_address_mask;
	} else if (pgtable_entry_types_get_next_level_table(&type)) {
		tb     = entry->table;
		*paddr = vmsa_table_entry_get_NextLevelTableAddress(&tb) &
			 level_info->table_mask;
	} else {
		panic("Invalid entry get paddr");
	}
}

TEST_EXPORTED count_t
get_table_refcount(vmsa_level_table_t *table, index_t idx)
{
	vmsa_entry_t	   g	 = get_entry(table, idx);
	vmsa_table_entry_t entry = g.table;

	return vmsa_table_entry_get_refcount(&entry);
}

static inline void
set_table_refcount(vmsa_level_table_t *table, index_t idx, count_t refcount)
{
	vmsa_entry_t	   g   = get_entry(table, idx);
	vmsa_table_entry_t val = g.table;

	vmsa_table_entry_set_refcount(&val, refcount);
	partition_phys_access_enable(&table[idx]);
	g.table = val;
	atomic_store_explicit(&table[idx], g.base, memory_order_relaxed);
	partition_phys_access_disable(&table[idx]);
}

static pgtable_vm_memtype_t
map_stg2_attr_to_memtype(vmsa_lower_attrs_t attrs)
{
	vmsa_stg2_lower_attrs_t val = vmsa_stg2_lower_attrs_cast(attrs);
	return vmsa_stg2_lower_attrs_get_mem_attr(&val);
}

static pgtable_hyp_memtype_t
map_stg1_attr_to_memtype(vmsa_lower_attrs_t attrs)
{
	vmsa_stg1_lower_attrs_t val = vmsa_stg1_lower_attrs_cast(attrs);
	// only the MAIR index decides the memory type, it's directly map
	return vmsa_stg1_lower_attrs_get_attr_idx(&val);
}

static vmsa_lower_attrs_t
get_lower_attr(vmsa_entry_t entry)
{
	vmsa_page_and_block_attrs_entry_t val = entry.attrs;
	return vmsa_page_and_block_attrs_entry_get_lower_attrs(&val);
}

static vmsa_upper_attrs_t
get_upper_attr(vmsa_entry_t entry)
{
	vmsa_page_and_block_attrs_entry_t val = entry.attrs;
	return vmsa_page_and_block_attrs_entry_get_upper_attrs(&val);
}

static pgtable_access_t
map_stg1_attr_to_access(vmsa_upper_attrs_t upper_attrs,
			vmsa_lower_attrs_t lower_attrs)
{
	vmsa_stg1_lower_attrs_t l   = vmsa_stg1_lower_attrs_cast(lower_attrs);
	vmsa_stg1_upper_attrs_t u   = vmsa_stg1_upper_attrs_cast(upper_attrs);
	bool			xn  = false;
	vmsa_stg1_ap_t		ap  = VMSA_STG1_AP_EL0_NONE_UPPER_READ_ONLY;
	pgtable_access_t	ret = PGTABLE_ACCESS_NONE;

#if defined(ARCH_ARM_FEAT_VHE)
	xn = vmsa_stg1_upper_attrs_get_PXN(&u);
#else
	xn = vmsa_stg1_upper_attrs_get_XN(&u);
#endif
	ap = vmsa_stg1_lower_attrs_get_AP(&l);

	switch (ap) {
#if ARCH_AARCH64_USE_PAN
	case VMSA_STG1_AP_ALL_READ_WRITE:
	case VMSA_STG1_AP_ALL_READ_ONLY:
		// EL0 has access, so no access in EL2 (unless PAN is disabled)
		ret = PGTABLE_ACCESS_NONE;
		break;
#else // !ARCH_AARCH64_USE_PAN
	case VMSA_STG1_AP_ALL_READ_WRITE:
#endif
	case VMSA_STG1_AP_EL0_NONE_UPPER_READ_WRITE:
		// XN is ignored due to SCTLR_EL2.WXN=1; it should be true
		assert(xn);
		ret = PGTABLE_ACCESS_RW;
		break;
#if !ARCH_AARCH64_USE_PAN
	case VMSA_STG1_AP_ALL_READ_ONLY:
#endif
	case VMSA_STG1_AP_EL0_NONE_UPPER_READ_ONLY:
		ret = xn ? PGTABLE_ACCESS_R : PGTABLE_ACCESS_RX;
		break;
	default:
		// Access None
		break;
	}

	return ret;
}

static void
map_stg2_attr_to_access(vmsa_upper_attrs_t upper_attrs,
			vmsa_lower_attrs_t lower_attrs,
			pgtable_access_t  *kernel_access,
			pgtable_access_t  *user_access)
{
	// Map from S2AP to R and W access bits
	static const pgtable_access_t stg2_ap_map[] = {
		// AP 0x0
		PGTABLE_ACCESS_NONE,
		// AP 0x1
		PGTABLE_ACCESS_R,
		// AP 0x2
		PGTABLE_ACCESS_W,
		// AP 0x3
		PGTABLE_ACCESS_RW,
	};

	vmsa_stg2_lower_attrs_t l = vmsa_stg2_lower_attrs_cast(lower_attrs);
	vmsa_stg2_upper_attrs_t u = vmsa_stg2_upper_attrs_cast(upper_attrs);

	vmsa_s2ap_t	 ap = vmsa_stg2_lower_attrs_get_S2AP(&l);
	pgtable_access_t rw = stg2_ap_map[ap];

#if defined(ARCH_ARM_FEAT_XNX)
	bool uxn	 = vmsa_stg2_upper_attrs_get_UXN(&u);
	bool pxn_xor_uxn = vmsa_stg2_upper_attrs_get_PXNxorUXN(&u);
	bool pxn	 = pxn_xor_uxn != uxn;
	*user_access = uxn ? rw : pgtable_access_combine(rw, PGTABLE_ACCESS_X);
	*kernel_access = pxn ? rw
			     : pgtable_access_combine(rw, PGTABLE_ACCESS_X);
#else
	bool xn	       = vmsa_stg2_upper_attrs_get_XN(&u);
	*user_access   = xn ? rw : pgtable_access_combine(rw, PGTABLE_ACCESS_X);
	*kernel_access = *user_access;
#endif
}

static void
map_stg2_memtype_to_attrs(pgtable_vm_memtype_t	   memtype,
			  vmsa_stg2_lower_attrs_t *lower_attrs)
{
	vmsa_stg2_lower_attrs_set_mem_attr(lower_attrs, memtype);
	switch (memtype) {
	case PGTABLE_VM_MEMTYPE_NORMAL_NC:
	case PGTABLE_VM_MEMTYPE_NORMAL_ONC_IWT:
	case PGTABLE_VM_MEMTYPE_NORMAL_ONC_IWB:
	case PGTABLE_VM_MEMTYPE_NORMAL_OWT_INC:
	case PGTABLE_VM_MEMTYPE_NORMAL_WT:
	case PGTABLE_VM_MEMTYPE_NORMAL_OWT_IWB:
	case PGTABLE_VM_MEMTYPE_NORMAL_OWB_INC:
	case PGTABLE_VM_MEMTYPE_NORMAL_OWB_IWT:
	case PGTABLE_VM_MEMTYPE_NORMAL_WB:
#if SCHEDULER_CAN_MIGRATE
		vmsa_stg2_lower_attrs_set_SH(lower_attrs,
					     VMSA_SHAREABILITY_INNER_SHAREABLE);
#else
		vmsa_stg2_lower_attrs_set_SH(lower_attrs,
					     VMSA_SHAREABILITY_NON_SHAREABLE);
#endif
		break;
	case PGTABLE_VM_MEMTYPE_DEVICE_NGNRNE:
	case PGTABLE_VM_MEMTYPE_DEVICE_NGNRE:
	case PGTABLE_VM_MEMTYPE_DEVICE_NGRE:
	case PGTABLE_VM_MEMTYPE_DEVICE_GRE:
	default:
		vmsa_stg2_lower_attrs_set_SH(lower_attrs,
					     VMSA_SHAREABILITY_NON_SHAREABLE);
		break;
	}
}

static void
map_stg1_memtype_to_attrs(pgtable_hyp_memtype_t	   memtype,
			  vmsa_stg1_lower_attrs_t *lower_attrs)
{
	vmsa_stg1_lower_attrs_set_attr_idx(lower_attrs, memtype);
}

static void
map_stg1_access_to_attrs(pgtable_access_t	  access,
			 vmsa_stg1_upper_attrs_t *upper_attrs,
			 vmsa_stg1_lower_attrs_t *lower_attrs)
{
	bool	       xn;
	vmsa_stg1_ap_t ap;

	switch (access) {
	case PGTABLE_ACCESS_RX:
	case PGTABLE_ACCESS_X:
		xn = false;
		break;
	case PGTABLE_ACCESS_NONE:
	case PGTABLE_ACCESS_W:
	case PGTABLE_ACCESS_R:
	case PGTABLE_ACCESS_RW:
		xn = true;
		break;
	case PGTABLE_ACCESS_RWX:
	default:
		panic("Invalid stg1 access type");
	}

	/* set AP */
	switch (access) {
	case PGTABLE_ACCESS_W:
	case PGTABLE_ACCESS_RW:
		ap = VMSA_STG1_AP_EL0_NONE_UPPER_READ_WRITE;
		break;
	case PGTABLE_ACCESS_NONE:
#if ARCH_AARCH64_USE_PAN
		ap = VMSA_STG1_AP_ALL_READ_WRITE;
		break;
#endif
	case PGTABLE_ACCESS_R:
	case PGTABLE_ACCESS_RX:
	case PGTABLE_ACCESS_X:
		ap = VMSA_STG1_AP_EL0_NONE_UPPER_READ_ONLY;
		break;
	case PGTABLE_ACCESS_RWX:
	default:
		panic("Invalid stg1 access type");
	}

	vmsa_stg1_lower_attrs_set_AP(lower_attrs, ap);
#if defined(ARCH_ARM_FEAT_VHE)
	vmsa_stg1_upper_attrs_set_PXN(upper_attrs, xn);
	// For compatibility with EL2 single stage MMU keep these equal.
	vmsa_stg1_upper_attrs_set_UXN(upper_attrs, xn);
#else
	vmsa_stg1_upper_attrs_set_XN(upper_attrs, xn);
#endif

#if defined(ARCH_ARM_FEAT_BTI)
	// Guard the executable pages only if requested from platform
	vmsa_stg1_upper_attrs_set_GP(upper_attrs,
				     (!xn && platform_cpu_bti_enabled()));
#endif
}

static void
map_stg2_access_to_attrs(pgtable_access_t	  kernel_access,
			 pgtable_access_t	  user_access,
			 vmsa_stg2_upper_attrs_t *upper_attrs,
			 vmsa_stg2_lower_attrs_t *lower_attrs)
{
	bool kernel_exec =
		pgtable_access_check(kernel_access, PGTABLE_ACCESS_X);
	bool user_exec = pgtable_access_check(user_access, PGTABLE_ACCESS_X);

#if defined(ARCH_ARM_FEAT_XNX)
	vmsa_stg2_upper_attrs_set_UXN(upper_attrs, !user_exec);
	vmsa_stg2_upper_attrs_set_PXNxorUXN(upper_attrs,
					    kernel_exec != user_exec);
#else
	vmsa_stg2_upper_attrs_set_XN(upper_attrs, !kernel_exec || !user_exec);
#endif

	// set AP
	// kernel access and user access (RW) should be the same
	vmsa_s2ap_t ap;
	static_assert(PGTABLE_ACCESS_X == 1,
		      "expect PGTABLE_ACCESS_X is bit 0");
	assert(((kernel_access ^ user_access) >> 1) == 0);
	assert(!pgtable_access_is_equal(kernel_access, PGTABLE_ACCESS_X));

	switch (kernel_access) {
	case PGTABLE_ACCESS_R:
	case PGTABLE_ACCESS_RX:
		ap = VMSA_S2AP_READ_ONLY;
		break;
	case PGTABLE_ACCESS_W:
		ap = VMSA_S2AP_WRITE_ONLY;
		break;
	case PGTABLE_ACCESS_RW:
	case PGTABLE_ACCESS_RWX:
		ap = VMSA_S2AP_READ_WRITE;
		break;
	case PGTABLE_ACCESS_NONE:
	case PGTABLE_ACCESS_X:
	default:
		ap = VMSA_S2AP_NONE;
		break;
	}

	vmsa_stg2_lower_attrs_set_S2AP(lower_attrs, ap);
}

static void
set_invalid_entry(vmsa_level_table_t *table, index_t idx)
{
	vmsa_general_entry_t entry = { 0 };

	partition_phys_access_enable(&table[idx]);
	atomic_store_explicit(&table[idx], entry, memory_order_relaxed);
	partition_phys_access_disable(&table[idx]);
}

static void
set_table_entry(vmsa_level_table_t *table, index_t idx, paddr_t addr,
		count_t count, bool outer_shareable)
{
	vmsa_table_entry_t entry = vmsa_table_entry_default();

	vmsa_table_entry_set_NextLevelTableAddress(&entry, addr);
	vmsa_table_entry_set_refcount(&entry, count);

	// Ensure prior writes are observable to the TLB walker
	dsb_st(outer_shareable);

	partition_phys_access_enable(&table[idx]);
	vmsa_entry_t g = { .table = entry };
	atomic_store_explicit(&table[idx], g.base, memory_order_relaxed);
	partition_phys_access_disable(&table[idx]);
}

static void
set_page_entry(vmsa_level_table_t *table, index_t idx, paddr_t addr,
	       vmsa_upper_attrs_t upper_attrs, vmsa_lower_attrs_t lower_attrs,
	       bool contiguous, bool fence)
{
	vmsa_page_entry_t	  entry = vmsa_page_entry_default();
	vmsa_common_upper_attrs_t u;

	u = vmsa_common_upper_attrs_cast(upper_attrs);
	vmsa_common_upper_attrs_set_cont(&u, contiguous);

	vmsa_page_entry_set_lower_attrs(&entry, lower_attrs);
	vmsa_page_entry_set_upper_attrs(
		&entry, (vmsa_upper_attrs_t)vmsa_common_upper_attrs_raw(u));
	vmsa_page_entry_set_OutputAddress(&entry, addr);

	partition_phys_access_enable(&table[idx]);
	vmsa_entry_t g = { .page = entry };
	if (fence) {
		atomic_store_explicit(&table[idx], g.base,
				      memory_order_release);
	} else {
		atomic_store_explicit(&table[idx], g.base,
				      memory_order_relaxed);
	}
	partition_phys_access_disable(&table[idx]);
}

static void
set_block_entry(vmsa_level_table_t *table, index_t idx, paddr_t addr,
		vmsa_upper_attrs_t upper_attrs, vmsa_lower_attrs_t lower_attrs,
		bool contiguous, bool fence, bool notlb)
{
	vmsa_block_entry_t	  entry = vmsa_block_entry_default();
	vmsa_common_upper_attrs_t u;

	u = vmsa_common_upper_attrs_cast(upper_attrs);
	vmsa_common_upper_attrs_set_cont(&u, contiguous);

	vmsa_block_entry_set_lower_attrs(&entry, lower_attrs);
	vmsa_block_entry_set_upper_attrs(
		&entry, (vmsa_upper_attrs_t)vmsa_common_upper_attrs_raw(u));
	vmsa_block_entry_set_OutputAddress(&entry, addr);
#if CPU_PGTABLE_BBM_LEVEL > 0U
	vmsa_block_entry_set_nT(&entry, notlb);
#else
	(void)notlb;
#endif

	partition_phys_access_enable(&table[idx]);
	vmsa_entry_t g = { .block = entry };
	if (fence) {
		atomic_store_explicit(&table[idx], g.base,
				      memory_order_release);
	} else {
		atomic_store_explicit(&table[idx], g.base,
				      memory_order_relaxed);
	}
	partition_phys_access_disable(&table[idx]);
}

#if CPU_PGTABLE_BBM_LEVEL == 1U
static void
set_notlb_flag(vmsa_label_table_t *table, index_t idx, bool nt)
{
	vmsa_block_entry_t entry;

	partition_phys_access_enable(&table[idx]);
	atomic_load_explicit(&table[idx], entry, memory_order_relaxed);
	vmsa_general_entry_t  g	   = { .block = entry_cnt };
	pgtable_entry_types_t type = get_entry_type(&g);
	assert(pgtable_entry_types_get_block(&type));
	vmsa_block_entry_set_nT(g.block, nt);
	atomic_store_explicit(&table[idx], g.base, memory_order_relaxed);
	partition_phys_access_disable(&table[idx]);
}
#endif // CPU_PGTABLE_BBM_LEVEL == 1U

static error_t
alloc_level_table(partition_t *partition, size_t size, size_t alignment,
		  paddr_t *paddr, vmsa_level_table_t **table)
{
	void_ptr_result_t alloc_ret;

	// actually only used to allocate a page
	alloc_ret = partition_alloc(partition, size, alignment);
	if (compiler_expected(alloc_ret.e == OK)) {
		(void)memset_s(alloc_ret.r, size, 0, size);

		*table = (vmsa_level_table_t *)alloc_ret.r;
		*paddr = partition_virt_to_phys(partition,
						(uintptr_t)alloc_ret.r);
		assert(*paddr != PADDR_INVALID);
	}
	return alloc_ret.e;
}

// Helper function to map all sub page table/set entry count, following a FIFO
// order, so the last entry to write is the one which actually hook the whole
// new page table levels on the existing page table.
static void
set_pgtables(vmaddr_t virtual_address, stack_elem_t stack[PGTABLE_LEVEL_NUM],
	     index_t first_new_table_level, index_t cur_level,
	     count_t initial_refcount, index_t start_level,
	     bool outer_shareable)
{
	paddr_t			    lower;
	vmsa_level_table_t	   *table;
	const pgtable_level_info_t *level_info = NULL;
	index_t			    idx;
	vmsa_entry_t		    g;
	pgtable_entry_types_t	    type;
	count_t			    refcount = initial_refcount;
	index_t			    level    = cur_level;

	while (first_new_table_level < level) {
		lower = stack[level].paddr;
		table = stack[level - 1U].table;

		assert(stack[level - 1U].mapped);

		level_info = &level_conf[level - 1U];

		idx  = get_index(virtual_address, level_info,
				 ((level - 1U) == start_level));
		g    = get_entry(table, idx);
		type = get_entry_type(&g, level_info);

		if (pgtable_entry_types_get_next_level_table(&type)) {
			// This should be the last level we are updating
			assert(first_new_table_level == (level - 1U));

			// Update the table's entry count
			refcount = get_table_refcount(table, idx) + 1U;
			set_table_refcount(table, idx, refcount);
		} else {
			// Write the table entry.
			set_table_entry(table, idx, lower, refcount,
					outer_shareable);

			// The refcount for the remaining levels should be 1.
			refcount = 1;
		}

		level--;
	}
}

// Check if the existing mapping can remain unchanged.
static bool
pgtable_maybe_keep_mapping(vmsa_entry_t cur_entry, pgtable_entry_types_t type,
			   pgtable_map_modifier_args_t *margs, index_t level)
{
	assert(pgtable_entry_types_get_block(&type) ||
	       pgtable_entry_types_get_page(&type));
	const pgtable_level_info_t *cur_level_info = &level_conf[level];

	paddr_t expected_phys = margs->phys & ~util_mask(cur_level_info->lsb);

	paddr_t phys_addr;
	get_entry_paddr(cur_level_info, &cur_entry, type, &phys_addr);
	vmsa_upper_attrs_t upper_attrs = get_upper_attr(cur_entry);
	vmsa_lower_attrs_t lower_attrs = get_lower_attr(cur_entry);

	bool keep_mapping = (phys_addr == expected_phys) &&
			    (upper_attrs == margs->upper_attrs) &&
			    (lower_attrs == margs->lower_attrs);
	if (keep_mapping) {
		margs->phys = expected_phys + cur_level_info->addr_size;
	}
	return keep_mapping;
}

// Check if only the page access needs to be changed and update it.
static bool
pgtable_maybe_update_access(pgtable_t	*pgt,
			    stack_elem_t stack[PGTABLE_LEVEL_NUM], index_t idx,
			    pgtable_entry_types_t	 type,
			    pgtable_map_modifier_args_t *margs, index_t level,
			    vmaddr_t virtual_address, size_t size,
			    vmaddr_t *next_virtual_address, size_t *next_size,
			    index_t *next_level)
{
	bool ret = false;

	// If only the entry's access permissions are changing, this can be
	// done without a break before make.

	const pgtable_level_info_t *cur_level_info = &level_conf[level];

	size_t	 addr_size = cur_level_info->addr_size;
	vmaddr_t entry_virtual_address =
		entry_start_address(virtual_address, cur_level_info);
	vmaddr_t start_virtual_address = virtual_address;

	if (pgtable_entry_types_get_block(&type) &&
	    ((virtual_address != entry_virtual_address) ||
	     (size < addr_size))) {
		goto out;
	}
	assert(virtual_address == entry_virtual_address);

	size_t idx_stop = util_min(idx + (size >> cur_level_info->lsb),
				   stack[level].entry_cnt);

	paddr_t cur_phys = margs->phys;

	vmsa_level_table_t *table = stack[level].table;
	partition_phys_access_enable(&table[0]);
	while (idx != idx_stop) {
		vmsa_entry_t cur_entry = {
			.base = atomic_load_explicit(&table[idx],
						     memory_order_relaxed),
		};
		vmsa_upper_attrs_t upper_attrs = get_upper_attr(cur_entry);
		vmsa_lower_attrs_t lower_attrs = get_lower_attr(cur_entry);
#if defined(ARCH_ARM_FEAT_XNX)
		uint64_t xn_mask = VMSA_STG2_UPPER_ATTRS_UXN_MASK |
				   VMSA_STG2_UPPER_ATTRS_PXNXORUXN_MASK;
#else
		uint64_t xn_mask = VMSA_STG2_UPPER_ATTRS_XN_MASK;
#endif
		uint64_t s2ap_mask = VMSA_STG2_LOWER_ATTRS_S2AP_MASK;

		pgtable_entry_types_t cur_type =
			get_entry_type(&cur_entry, cur_level_info);
		if (!pgtable_entry_types_is_equal(cur_type, type)) {
			goto out_access;
		}

		paddr_t phys_addr;
		get_entry_paddr(&level_conf[level], &cur_entry, type,
				&phys_addr);

		if (phys_addr != cur_phys) {
			goto out_access;
		}
		vmsa_common_upper_attrs_t upper_attrs_bitfield =
			vmsa_common_upper_attrs_cast(upper_attrs);
		if (vmsa_common_upper_attrs_get_cont(&upper_attrs_bitfield)) {
			goto out_access;
		}
		if ((upper_attrs & ~xn_mask) !=
		    (margs->upper_attrs & ~xn_mask)) {
			goto out_access;
		}
		if ((lower_attrs & ~s2ap_mask) !=
		    (margs->lower_attrs & ~s2ap_mask)) {
			goto out_access;
		}

		vmsa_page_entry_t entry = cur_entry.page;

		if ((upper_attrs & xn_mask) != (margs->upper_attrs & xn_mask)) {
			vmsa_page_entry_set_upper_attrs(&entry,
							margs->upper_attrs);
		}
		if ((lower_attrs & s2ap_mask) !=
		    (margs->lower_attrs & s2ap_mask)) {
			vmsa_page_entry_set_lower_attrs(&entry,
							margs->lower_attrs);
		}

		cur_entry.page = entry;
		atomic_store_explicit(&table[idx], cur_entry.base,
				      memory_order_release);

		idx++;
		cur_phys += cur_level_info->addr_size;
		virtual_address += cur_level_info->addr_size;
	}
	partition_phys_access_disable(&table[0]);

	ret = true;

	size_t updated_size = cur_phys - margs->phys;

#if defined(ARCH_ARM_FEAT_TLBIRANGE)
	if (margs->stage == PGTABLE_HYP_STAGE_1) {
		dsb_st(false);
		hyp_tlbi_va_range(start_virtual_address, updated_size,
				  pgt->granule_shift);
	} else {
		dsb_st(margs->outer_shareable);
		hyp_tlbi_ipa_range(start_virtual_address, updated_size,
				   pgt->granule_shift, margs->outer_shareable);
	}
#else
	dsb_st(margs->outer_shareable);

	for (size_t offset = 0U; offset < updated_size; offset += addr_size) {
		if (margs->stage == PGTABLE_HYP_STAGE_1) {
			hyp_tlbi_va(start_virtual_address + offset);
		} else {
			vm_tlbi_ipa(start_virtual_address + offset,
				    margs->outer_shareable);
		}
	}
#endif

	*next_size	      = size - updated_size;
	margs->phys	      = cur_phys;
	*next_virtual_address = virtual_address;

	// Walk back up the tree if needed
	if (idx == stack[level].entry_cnt) {
		idx -= 1U; // Last updated index
		while (idx == stack[level].entry_cnt - 1U) {
			if (level == pgt->start_level) {
				break;
			} else {
				level--;
			}

			cur_level_info = &level_conf[level];
			// virtual_address is already stepped, use previous one
			// to check
			idx = get_index(virtual_address, cur_level_info,
					(level == pgt->start_level));
		}
		*next_level = level;
	}

out:
	return ret;
out_access:
	partition_phys_access_disable(&table[0]);
	goto out;
}

static error_t
pgtable_add_table_entry(pgtable_t *pgt, pgtable_map_modifier_args_t *margs,
			index_t cur_level, stack_elem_t *stack,
			vmaddr_t virtual_address, size_t size,
			index_t *next_level, vmaddr_t *next_virtual_address,
			size_t *next_size, bool set_start_level)
{
	error_t		    ret;
	paddr_t		    new_pgtable_paddr;
	vmsa_level_table_t *new_pgt	 = NULL;
	index_t		    level	 = cur_level;
	size_t		    pgtable_size = util_bit(pgt->granule_shift);

	// allocate page and fill right value first, then update entry
	// to existing table
	ret = alloc_level_table(margs->partition, pgtable_size, pgtable_size,
				&new_pgtable_paddr, &new_pgt);
	if (ret != OK) {
		LOG(ERROR, WARN, "Failed to alloc page table level.\n");
		margs->error = ret;
		goto out;
	}

	if ((margs->new_page_start_level == PGTABLE_INVALID_LEVEL) &&
	    set_start_level) {
		margs->new_page_start_level =
			level > pgt->start_level ? level - 1U : level;
	}

	if (level >= (PGTABLE_LEVEL_NUM - 1U)) {
		LOG(ERROR, WARN, "invalid level ({:d}).\n", level);
		ret = ERROR_ARGUMENT_INVALID;
		goto out;
	}

	// just record the new level in the stack
	stack[level + 1U] = (stack_elem_t){
		.paddr	    = new_pgtable_paddr,
		.table	    = new_pgt,
		.mapped	    = true,
		.need_unmap = false,
		.entry_cnt  = level_conf[level + 1U].entry_cnt,
	};

	// guide translation_table_walk step into the new sub page table
	// level
	*next_level	      = level + 1U;
	*next_virtual_address = virtual_address;
	*next_size	      = size;

out:
	return ret;
}

// Split a block into the next smaller block size.
//
// This is called when a map or unmap operation encounters a block entry that
// overlaps but is not completely covered by the range of the operation.
static pgtable_modifier_ret_t
pgtable_split_block(pgtable_t *pgt, vmaddr_t virtual_address, size_t size,
		    vmsa_entry_t cur_entry, index_t idx, index_t level,
		    pgtable_entry_types_t	 type,
		    stack_elem_t		 stack[PGTABLE_LEVEL_NUM],
		    pgtable_map_modifier_args_t *margs, index_t *next_level,
		    vmaddr_t *next_virtual_address, size_t *next_size)
{
	pgtable_modifier_ret_t vret = PGTABLE_MODIFIER_RET_CONTINUE;
	error_t		       ret;

	assert(pgtable_entry_types_get_block(&level_conf[level].allowed_types));

	// Get the values of the block before it is invalidated
	const pgtable_level_info_t *cur_level_info = &level_conf[level];
	size_t			    addr_size	   = cur_level_info->addr_size;
	vmaddr_t		    entry_virtual_address =
		entry_start_address(virtual_address, cur_level_info);
	vmsa_upper_attrs_t cur_upper_attrs = get_upper_attr(cur_entry);
	vmsa_lower_attrs_t cur_lower_attrs = get_lower_attr(cur_entry);
	paddr_t		   phys_addr;
	get_entry_paddr(cur_level_info, &cur_entry, type, &phys_addr);

#if (CPU_PGTABLE_BBM_LEVEL < 2U) && !defined(PLATFORM_PGTABLE_AVOID_BBM)
	// We can't just replace the large entry; coherency might be broken. We
	// need a TLB flush.
#if CPU_PGTABLE_BBM_LEVEL == 0U
	// The nT bit is not supported; we need a full break-before-make
	// sequence, with an invalid entry in the page table. This might trigger
	// spurious stage 2 faults on other cores or SMMUs.
	set_invalid_entry(stack[level].table, idx);
#else
	// The nT bit is supported; we can set it before flushing the TLB to
	// ensure that the large mapping isn't cached again before we replace it
	// with the split mappings, but the entry itself can remain valid so
	// SMMUs and other cores may not fault.
	set_notlb_flag(stack[level].table, idx, true);
#endif

	// Flush the TLB entry
	if (margs->stage == PGTABLE_HYP_STAGE_1) {
		dsb_st(false);
		hyp_tlbi_va(entry_virtual_address);
	} else {
		dsb_st(margs->outer_shareable);
		vm_tlbi_ipa(entry_virtual_address, margs->outer_shareable);
		// The full stage-1 flushing below is really sub-optimal.
		// FIXME:
		dsb(margs->outer_shareable);
		vm_tlbi_vmalle1(margs->outer_shareable);
	}
#else // (CPU_PGTABLE_BBM_LEVEL >= 2U) || PLATFORM_PGTABLE_AVOID_BBM
      // Either the CPU supports page size changes without BBM, or we must
      // avoid BBM to prevent unrecoverable SMMU faults. We will just replace
      // the block entry with the split table entry.
#endif

	ret = pgtable_add_table_entry(pgt, margs, level, stack, virtual_address,
				      size, next_level, next_virtual_address,
				      next_size, false);
	if (ret != OK) {
		vret = PGTABLE_MODIFIER_RET_ERROR;
		goto out;
	}

	// Update current level and values as now we want to add all the
	// pages to the table just created
	level		= *next_level;
	virtual_address = *next_virtual_address;

	cur_level_info = &level_conf[level];

	size_t	page_size = cur_level_info->addr_size;
	count_t new_pages = (count_t)(addr_size / page_size);
	assert(new_pages == cur_level_info->entry_cnt);

#if 0
	// FIXME: also need to search forwards for occupied entries
	bool contiguous = map_should_set_cont(
		margs->orig_virtual_address, margs->orig_size,
		virtual_address, level);
#else
	bool contiguous = false;
#endif
	bool	page_block_fence;
	index_t new_page_start_level;

	if (margs->new_page_start_level != PGTABLE_INVALID_LEVEL) {
		new_page_start_level	    = margs->new_page_start_level;
		page_block_fence	    = false;
		margs->new_page_start_level = PGTABLE_INVALID_LEVEL;
	} else {
		new_page_start_level = level > pgt->start_level ? level - 1U
								: level;
		page_block_fence     = true;
	}

	// Create all pages that cover the old block and hook them to
	// the new table entry
	assert(virtual_address >= entry_virtual_address);

	assert(pgtable_entry_types_get_block(&type));
	const bool use_block =
		pgtable_entry_types_get_block(&cur_level_info->allowed_types);

	paddr_t phys;
	idx = 0;
	for (count_t i = 0; i < new_pages; i++) {
		vmsa_upper_attrs_t upper_attrs;
		vmsa_lower_attrs_t lower_attrs;

		phys	    = phys_addr;
		upper_attrs = cur_upper_attrs;
		lower_attrs = cur_lower_attrs;

		phys_addr += page_size;

		if (use_block) {
			set_block_entry(stack[level].table, idx, phys,
					upper_attrs, lower_attrs, contiguous,
					page_block_fence, false);
		} else {
			set_page_entry(stack[level].table, idx, phys,
				       upper_attrs, lower_attrs, contiguous,
				       page_block_fence);
		}
		assert(!util_add_overflows(margs->phys, (paddr_t)page_size));
		assert(!util_add_overflows(phys_addr, (paddr_t)page_size));
		idx++;
	}

#if (CPU_PGTABLE_BBM_LEVEL < 2U) && !defined(PLATFORM_PGTABLE_AVOID_BBM)
	// There is a dsb_st in set_pgtables() which is sufficient for FEAT_ETS2
#if !defined(ARCH_ARM_FEAT_ETS2) || !ARCH_ARM_FEAT_ETS2
	// Wait for the TLB flush before inserting the new table entry
	dsb(margs->outer_shareable);
#endif
#endif
	set_pgtables(entry_virtual_address, stack, new_page_start_level, level,
		     new_pages, pgt->start_level, margs->outer_shareable);

#if (CPU_PGTABLE_BBM_LEVEL >= 2U) || defined(PLATFORM_PGTABLE_AVOID_BBM)
	// Flush the old entry from the TLB now, to avoid TLB conflicts later.
	if (margs->stage == PGTABLE_HYP_STAGE_1) {
		dsb_st(false);
		hyp_tlbi_va(entry_virtual_address);
	} else {
		dsb_st(margs->outer_shareable);
		vm_tlbi_ipa(entry_virtual_address, margs->outer_shareable);
	}
#endif

out:
	return vret;
}

// Attempt to merge a subtree into a single block.
//
// This is called when a remap operation encounters a next-level table entry. It
// checks whether the map operation is able to merge the next-level table into a
// single large block, and replaces it with that block if possible.
//
// The criteria are as follows:
//
// - This level's entry size is less than the operation's merge limit
// - Block mappings are allowed at this level
// - The input and output addresses are equal modulo this level's entry size
// - The input address range is aligned to the next level's entry size
// - If the next-level table is allowed to contain next-level table entries, it
//   does not contain any such entries
// - If try_map is true, any entries in the next-level table that are inside the
//   mapped region are invalid
// - If the next-level table is not completely within the mapped region, any
//   entries that are outside the mapped region are congruent, i.e. have the
//   same attributes and physical addresses that they would have if the mapped
//   region was extended to cover them
static pgtable_modifier_ret_t
pgtable_maybe_merge_block(pgtable_t *pgt, vmaddr_t virtual_address, size_t size,
			  vmsa_entry_t cur_entry, index_t idx, index_t level,
			  pgtable_entry_types_t	       type,
			  stack_elem_t		       stack[PGTABLE_LEVEL_NUM],
			  pgtable_map_modifier_args_t *margs,
			  index_t *next_level, paddr_t next_table_paddr)
{
	assert(pgtable_entry_types_get_next_level_table(&type));
	assert(*next_level < PGTABLE_LEVEL_NUM);

	pgtable_modifier_ret_t	    vret = PGTABLE_MODIFIER_RET_CONTINUE;
	const pgtable_level_info_t *cur_level_info = &level_conf[level];

	if (cur_level_info->addr_size >= margs->merge_limit) {
		// Block size exceeds the merge limit. There are three reasons
		// we enforce this:
		//
		// - If a VM address space with an SMMU or other IOMMU attached
		//   that can't handle break-before-make or TLB conflicts
		//   safely, we need to disable merging entirely.
		//
		// - In the hypervisor address space, we need to avoid merging
		//   pages in a way that might cause a fault on an address that
		//   is touched during the hypervisor's handling of that fault,
		//   since that would fault recursively.
		//
		// - In the hypervisor address space, we need to avoid merging
		//   across partition ownership boundaries, because that might
		//   free the next-level table into the wrong partition.
		goto out;
	}

	if (!pgtable_entry_types_get_block(&cur_level_info->allowed_types)) {
		// Block entries aren't possible at this level. This might
		// happen if merge limit is set to ~0U for a VM address space.
		goto out;
	}

	if ((virtual_address & util_mask(cur_level_info->lsb)) !=
	    (margs->phys & util_mask(cur_level_info->lsb))) {
		// Input and output addresses misaligned for block size.
		goto out;
	}

	const pgtable_level_info_t *next_level_info = &level_conf[*next_level];
	size_t			    level_size =
		size_on_level(virtual_address, size, cur_level_info);
	if (((virtual_address & util_mask(next_level_info->lsb)) != 0U) ||
	    ((level_size & util_mask(next_level_info->lsb)) != 0U)) {
		// Mapping will be partial at the next level. A merge
		// might be possible at the next level, but we currently
		// can't extend it to this level as that requires a
		// multi-level search for non-congruent entries.
		goto out;
	}

	count_t covered_entries = (count_t)(level_size >> next_level_info->lsb);
	count_t other_entries	= next_level_info->entry_cnt - covered_entries;

	count_t table_refcount =
		vmsa_table_entry_get_refcount(&cur_entry.table);

	if (table_refcount < other_entries) {
		// Some of the entries not covered by the map operation must be
		// invalid, so a merge won't be possible.
		goto out;
	}

	vmaddr_t entry_virtual_address =
		entry_start_address(virtual_address, cur_level_info);
	paddr_t entry_phys = margs->phys & ~util_mask(cur_level_info->lsb);

	bool need_search = ((pgtable_entry_types_get_next_level_table(
				    &next_level_info->allowed_types)) ||
			    (other_entries > 0U) ||
			    (margs->try_map && (table_refcount > 0U)));

	if (need_search) {
		// It's possible that the level to be merged contains entries
		// that will prevent the merge. Map the level to be merged.
		vmsa_level_table_t *next_table =
			(vmsa_level_table_t *)partition_phys_map(
				next_table_paddr, util_bit(pgt->granule_shift));
		if (next_table == NULL) {
			LOG(ERROR, WARN,
			    "Failed to map table (pa {:#x}, level {:d}) for merge\n",
			    next_table_paddr, *next_level);
			vret = PGTABLE_MODIFIER_RET_ERROR;
			goto out;
		}

		// Check for any next-level entries that will prevent merge
		vmaddr_t next_level_addr = entry_virtual_address;
		paddr_t	 expected_phys	 = entry_phys;
		index_t	 next_level_idx;
		for (next_level_idx = 0U;
		     next_level_idx < next_level_info->entry_cnt;
		     next_level_idx++) {
			vmsa_entry_t next_level_entry =
				get_entry(next_table, next_level_idx);
			pgtable_entry_types_t next_level_type = get_entry_type(
				&next_level_entry, next_level_info);
			if (pgtable_entry_types_get_next_level_table(
				    &next_level_type)) {
				// We don't try to handle multi-level merges.
				// It's complex and typically not worth the
				// effort.
				break;
			}
			if ((margs->try_map &&
			     (!pgtable_entry_types_get_invalid(
				     &next_level_type))) ||
			    ((next_level_addr < virtual_address) ||
			     ((next_level_addr + next_level_info->addr_size -
			       1U) > (virtual_address + size - 1U)))) {
				// This entry is not completely covered by the
				// current map operation. It must be valid, and
				// must map the same physical address as the
				// current map operation would, with the same
				// attributes.
				if ((!pgtable_entry_types_get_block(
					    &next_level_type)) &&
				    (!pgtable_entry_types_get_page(
					    &next_level_type))) {
					// Not valid. Merging would incorrectly
					// map it.
					break;
				}

				paddr_t phys_addr;
				get_entry_paddr(next_level_info,
						&next_level_entry,
						next_level_type, &phys_addr);
				vmsa_upper_attrs_t upper_attrs =
					get_upper_attr(next_level_entry);
				vmsa_lower_attrs_t lower_attrs =
					get_lower_attr(next_level_entry);
				if ((phys_addr != expected_phys) ||
				    (upper_attrs != margs->upper_attrs) ||
				    (lower_attrs != margs->lower_attrs)) {
					// Inconsistent mapping; can't merge.
					break;
				}
			}
			expected_phys += next_level_info->addr_size;
			next_level_addr += next_level_info->addr_size;
		}

		partition_phys_unmap(next_table, next_table_paddr,
				     util_bit(pgt->granule_shift));

		if (next_level_idx < next_level_info->entry_cnt) {
			// We exited the next-level table check early, which
			// means we found an entry that prevented merge. Nothing
			// more to do.
			goto out;
		}
	}

	assert(stack[level].mapped);
	vmsa_level_table_t *cur_table = stack[level].table;

#if (CPU_PGTABLE_BBM_LEVEL < 1U) && !defined(PLATFORM_PGTABLE_AVOID_BBM)
	// The nT bit is not supported; we need a full break-before-make
	// sequence, with an invalid entry in the page table. This might
	// trigger spurious stage 2 faults on other cores or SMMUs.
	set_invalid_entry(stack[level].table, idx);
#elif (CPU_PGTABLE_BBM_LEVEL < 2U) && !defined(PLATFORM_PGTABLE_AVOID_BBM)
	// We can write the new block entry with the nT bit set, and then flush
	// the old page entries. The new entry will stay out of the TLBs until
	// we clear the nT bit below, so there will be no TLB conflicts, but
	// there may be translation faults depending on the CPU implementation.
	set_block_entry(cur_table, idx, entry_phys, margs->upper_attrs,
			margs->lower_attrs, false, false, true);

#else // (CPU_PGTABLE_BBM_LEVEL >= 2U) || PLATFORM_PGTABLE_AVOID_BBM

	// Either the CPU supports block size changes without BBM, or we must
	// avoid BBM operations to prevent unrecoverable SMMU faults.
	//
	// We can just go ahead and write the new block entry. We still flush
	// the TLB afterwards to avoid TLB conflicts. It is probably cheaper to
	// do that now than to take up to 512 TLB conflict faults later,
	// especially if the CPU supports range flushes.
	set_block_entry(cur_table, idx, entry_phys, margs->upper_attrs,
			margs->lower_attrs, false, false, false);

#endif

	// Flush the TLB entries for the merged pages.
	vmaddr_t next_level_addr = entry_virtual_address;
#ifdef ARCH_ARM_FEAT_TLBIRANGE
	if (margs->stage == PGTABLE_HYP_STAGE_1) {
		dsb_st(false);
		hyp_tlbi_va_range(entry_virtual_address,
				  cur_level_info->addr_size,
				  pgt->granule_shift);
	} else {
		dsb_st(margs->outer_shareable);
		hyp_tlbi_ipa_range(next_level_addr, cur_level_info->addr_size,
				   pgt->granule_shift, margs->outer_shareable);
	}
#else
	dsb_st((margs->stage != PGTABLE_HYP_STAGE_1) && margs->outer_shareable);
	for (index_t i = 0; i < next_level_info->entry_cnt; i++) {
		if (margs->stage == PGTABLE_HYP_STAGE_1) {
			hyp_tlbi_va(next_level_addr);
		} else {
			vm_tlbi_ipa(next_level_addr, margs->outer_shareable);
		}
		next_level_addr += next_level_info->addr_size;
	}
#endif

#if (CPU_PGTABLE_BBM_LEVEL < 2U) && !defined(PLATFORM_PGTABLE_AVOID_BBM)
	if (margs->stage != PGTABLE_HYP_STAGE_1) {
		// The full stage-1 flushing below is really sub-optimal.
		// FIXME:
		dsb(margs->outer_shareable);
		vm_tlbi_vmalle1(margs->outer_shareable);
	}

	// Wait for the TLB flush before inserting the new table entry
	dsb((margs->stage != PGTABLE_HYP_STAGE_1) && margs->outer_shareable);

	set_block_entry(cur_table, idx, entry_phys, margs->upper_attrs,
			margs->lower_attrs, false, false, false);
#else
	// Wait for the TLB flush before reusing the freed page table memory
	dsb((margs->stage != PGTABLE_HYP_STAGE_1) && margs->outer_shareable);
#endif

	// Release the page table memory
	(void)partition_free_phys(margs->partition, next_table_paddr,
				  util_bit(pgt->granule_shift));

	// Ensure that translation_table_walk revisits the entry we just
	// replaced, instead of traversing into the now-freed table. We don't
	// update the virt or phys addresses or size, because the next call to
	// map_modifier() will call pgtable_maybe_keep_mapping() to do it.
	*next_level = level;

out:
	return vret;
}

static pgtable_modifier_ret_t
pgtable_modify_mapping(pgtable_t *pgt, vmaddr_t virtual_address, size_t size,
		       vmsa_entry_t cur_entry, index_t idx, index_t cur_level,
		       pgtable_entry_types_t	    type,
		       stack_elem_t		    stack[PGTABLE_LEVEL_NUM],
		       pgtable_map_modifier_args_t *margs, index_t *next_level,
		       vmaddr_t *next_virtual_address, size_t *next_size)
{
	pgtable_modifier_ret_t vret  = PGTABLE_MODIFIER_RET_CONTINUE;
	index_t		       level = cur_level;

	const pgtable_level_info_t *cur_level_info = &level_conf[level];
	size_t			    addr_size	   = cur_level_info->addr_size;
	vmaddr_t		    entry_virtual_address =
		entry_start_address(virtual_address, cur_level_info);

	if (pgtable_entry_types_get_block(&type) &&
	    ((virtual_address != entry_virtual_address) ||
	     (size < addr_size))) {
		// Split the block into pages
		vret = pgtable_split_block(pgt, virtual_address, size,
					   cur_entry, idx, level, type, stack,
					   margs, next_level,
					   next_virtual_address, next_size);
	} else {
		// The new mapping will cover this entire range, either because
		// it's a single page, or because it's a block that didn't need
		// to be split. We need to unmap the existing page or block.
		pgtable_unmap_modifier_args_t margs2 = { 0 };

		margs2.partition      = margs->partition;
		margs2.preserved_size = PGTABLE_HYP_UNMAP_PRESERVE_NONE;
		margs2.stage	      = margs->stage;

		vret = unmap_modifier(pgt, virtual_address, addr_size, idx,
				      cur_level, type, stack, &margs2,
				      next_level, next_virtual_address,
				      next_size, false);

		if (margs->stage == PGTABLE_VM_STAGE_2) {
			// flush entire stage 1 tlb
			dsb(margs->outer_shareable);
			vm_tlbi_vmalle1(margs->outer_shareable);
			dsb(margs->outer_shareable);
		} else {
			dsb(false);
		}

		// Retry at the same address, so we can do the make part of the
		// break-before-make sequence.
		*next_virtual_address = virtual_address;
		*next_size	      = size;
	}

	return vret;
}

static void
map_modifier_insert_new_leaf(const pgtable_t *pgt, vmaddr_t virtual_address,
			     index_t idx, stack_elem_t stack[PGTABLE_LEVEL_NUM],
			     pgtable_map_modifier_args_t *margs,
			     size_t addr_size, index_t level,
			     const bool		 use_block,
			     vmsa_level_table_t *cur_table)
{
	index_t new_page_start_level;
	bool	page_block_fence;

	if (margs->new_page_start_level != PGTABLE_INVALID_LEVEL) {
		new_page_start_level	    = margs->new_page_start_level;
		page_block_fence	    = false;
		margs->new_page_start_level = PGTABLE_INVALID_LEVEL;
	} else {
		// if current level is start level, no need to update
		// entry count
		new_page_start_level = (level > pgt->start_level) ? (level - 1U)
								  : level;
		page_block_fence     = true;
	}

#if 0
	// FIXME: also need to search forwards for occupied entries
	bool contiguous = map_should_set_cont(
		margs->orig_virtual_address, margs->orig_size,
		virtual_address, level);
#else
	bool contiguous = false;
#endif

	// allowed to map a block
	if (use_block) {
		set_block_entry(cur_table, idx, margs->phys, margs->upper_attrs,
				margs->lower_attrs, contiguous,
				page_block_fence, false);
	} else {
		set_page_entry(cur_table, idx, margs->phys, margs->upper_attrs,
			       margs->lower_attrs, contiguous,
			       page_block_fence);
	}

	// check if need to set all page table levels
	set_pgtables(virtual_address, stack, new_page_start_level, level, 1U,
		     pgt->start_level, margs->outer_shareable);

	// update the physical address for next mapping
	margs->phys += addr_size;
	assert(!util_add_overflows(margs->phys, addr_size));
}

static pgtable_modifier_ret_t
map_modifier_update_existing_leaf(
	pgtable_t *pgt, vmaddr_t virtual_address, size_t size,
	vmsa_entry_t cur_entry, index_t idx, index_t cur_level,
	pgtable_entry_types_t type, stack_elem_t stack[PGTABLE_LEVEL_NUM],
	index_t *next_level, vmaddr_t *next_virtual_address, size_t *next_size,
	pgtable_map_modifier_args_t *margs)
{
	pgtable_modifier_ret_t vret = PGTABLE_MODIFIER_RET_CONTINUE;
	// If the existing mapping is consistent with the required
	// mapping, we don't need to do anything, even if try_map is
	// true.
	if (pgtable_maybe_keep_mapping(cur_entry, type, margs, cur_level)) {
		goto out;
	}

	// If try_map is set, we will abort the mapping operation.
	if (margs->try_map) {
		margs->error		     = ERROR_EXISTING_MAPPING;
		margs->partially_mapped_size = margs->orig_size - size;
		vret			     = PGTABLE_MODIFIER_RET_ERROR;
		goto out;
	}

	// If this is only an access update, we can do it in place.
	if (pgtable_maybe_update_access(
		    pgt, stack, idx, type, margs, cur_level, virtual_address,
		    size, next_virtual_address, next_size, next_level)) {
		goto out;
	}

	// We need a non-trivial update using a BBM sequence and/or a
	// block split.
	vret = pgtable_modify_mapping(pgt, virtual_address, size, cur_entry,
				      idx, cur_level, type, stack, margs,
				      next_level, next_virtual_address,
				      next_size);

out:
	return vret;
}

// @brief Modify current entry for mapping the specified virt address to the
// physical address.
//
// This modifier simply focuses on just one entry during the mapping procedure.
// Depends on the type of the entry, it may:
// * directly map the physical address as block/page
// * allocate/setup the page table level, and recursively (up to MAX_LEVEL) to
// handle the mapping. After the current page table level entry setup, it will
// drive @see translation_table_walk to the next entry at the same level.
static pgtable_modifier_ret_t
map_modifier(pgtable_t *pgt, vmaddr_t virtual_address, size_t size,
	     vmsa_entry_t cur_entry, index_t idx, index_t cur_level,
	     pgtable_entry_types_t type, stack_elem_t stack[PGTABLE_LEVEL_NUM],
	     void *data, index_t *next_level, vmaddr_t *next_virtual_address,
	     size_t *next_size, paddr_t next_table)
{
	pgtable_map_modifier_args_t *margs =
		(pgtable_map_modifier_args_t *)data;
	pgtable_modifier_ret_t vret = PGTABLE_MODIFIER_RET_CONTINUE;

	// Attempt merging or replacement of small mappings.
	if (pgtable_entry_types_get_next_level_table(&type)) {
		vret = pgtable_maybe_merge_block(pgt, virtual_address, size,
						 cur_entry, idx, cur_level,
						 type, stack, margs, next_level,
						 next_table);
		goto out;
	}

	// Handle existing block or page mappings.
	if (!pgtable_entry_types_get_invalid(&type)) {
		vret = map_modifier_update_existing_leaf(
			pgt, virtual_address, size, cur_entry, idx, cur_level,
			type, stack, next_level, next_virtual_address,
			next_size, margs);
		goto out;
	}

	assert(data != NULL);
	assert(pgt != NULL);

	// current level should be mapped
	index_t level = cur_level;
	assert(stack[level].mapped);
	vmsa_level_table_t *cur_table = stack[level].table;

	const pgtable_level_info_t *cur_level_info = &level_conf[cur_level];
	size_t			    addr_size	   = cur_level_info->addr_size;
	pgtable_entry_types_t	    allowed = cur_level_info->allowed_types;

	const bool use_block = pgtable_entry_types_get_block(&allowed);
	size_t	   level_size =
		size_on_level(virtual_address, size, cur_level_info);

	if ((addr_size <= level_size) &&
	    (use_block || pgtable_entry_types_get_page(&allowed)) &&
	    (util_is_baligned(margs->phys, addr_size))) {
		map_modifier_insert_new_leaf(pgt, virtual_address, idx, stack,
					     margs, addr_size, level, use_block,
					     cur_table);
	} else if (pgtable_entry_types_get_next_level_table(&allowed)) {
		error_t ret = pgtable_add_table_entry(
			pgt, margs, level, stack, virtual_address, size,
			next_level, next_virtual_address, next_size, true);
		if (ret != OK) {
			vret = PGTABLE_MODIFIER_RET_ERROR;
			goto failed_map;
		}
	} else {
		LOG(ERROR, WARN, "Unexpected condition during mapping:\n");
		LOG(ERROR, WARN,
		    "Mapping pa({:x}) to va({:x}), size({:d}), level({:d})",
		    margs->phys, virtual_address, size, (register_t)level);
		// should not be here
		panic("map_modifier bad type");
	}

failed_map:
	cur_table = NULL;

	// free all pages if something wrong
	if ((vret == PGTABLE_MODIFIER_RET_ERROR) &&
	    (margs->new_page_start_level != PGTABLE_INVALID_LEVEL)) {
		size_t pgtable_size = util_bit(pgt->granule_shift);
		while (margs->new_page_start_level < level) {
			// all new table level, no need to unmap
			assert(!stack[level].need_unmap);
			(void)partition_free(margs->partition,
					     stack[level].table, pgtable_size);
			stack[level].paddr  = 0U;
			stack[level].table  = NULL;
			stack[level].mapped = false;
			level--;
		}
	}

out:
	return vret;
}

// @brief Collect information while walking along the virtual address.
//
// This modifier is actually just a visitor, it accumulates the size of the
// entry/entries, and return to the caller. NOTE that the memory type and
// memory attribute should be the same, or else, it only return the attributes
// of the last entry.
static pgtable_modifier_ret_t
lookup_modifier(pgtable_t *pgt, vmsa_entry_t cur_entry, index_t level,
		pgtable_entry_types_t type, void *data)
{
	pgtable_lookup_modifier_args_t *margs =
		(pgtable_lookup_modifier_args_t *)data;
	const pgtable_level_info_t *cur_level_info = &level_conf[level];

	// expected types in the walk should be page|block
	assert(pgtable_entry_types_get_page(&type) ||
	       pgtable_entry_types_get_block(&type));

	assert(pgt != NULL);

	get_entry_paddr(cur_level_info, &cur_entry, type, &margs->phys);

	margs->entry = cur_entry;

	// set size & return for check
	margs->size = cur_level_info->addr_size;

	return PGTABLE_MODIFIER_RET_STOP;
}

// helper to check entry count from the parent page table level,
// free empty upper levels if needed
static void
check_refcount(pgtable_t *pgt, partition_t *partition, vmaddr_t virtual_address,
	       size_t size, index_t upper_level,
	       stack_elem_t stack[PGTABLE_LEVEL_NUM], bool need_dec,
	       const pgtable_unmap_modifier_args_t *margs, index_t *next_level,
	       vmaddr_t *next_virtual_address, size_t *next_size)
{
	const pgtable_level_info_t *cur_level_info = NULL;
	vmsa_level_table_t	   *cur_table	   = NULL;
	index_t			    level	   = upper_level;
	index_t			    cur_idx;
	count_t			    refcount;
	bool			    is_preserved = false;
	stack_elem_t		   *free_list[PGTABLE_LEVEL_NUM];
	index_t			    free_idx = 0;
	bool			    dec	     = need_dec;
	size_t			    walked_size;

	while (level >= pgt->start_level) {
		assert(stack[level].mapped);
		cur_table = stack[level].table;

		cur_level_info = &level_conf[level];
		cur_idx	       = get_index(virtual_address, cur_level_info,
					   (level == pgt->start_level));
		refcount       = get_table_refcount(cur_table, cur_idx);

		if (dec) {
			// decrease entry count
			refcount--;
			set_table_refcount(cur_table, cur_idx, refcount);
			dec = false;
		}

		if (refcount == 0U) {
			is_preserved = is_preserved_table_entry(
				margs->preserved_size, cur_level_info);

			if (is_preserved) {
				break;
			}

			// Make sure the general page table walk does not step
			// into the level that is being freed. The correct step
			// might either be forward one entry (if there are more
			// entries in the current level) or up one level (if
			// this is the last entry in the current level).
			// The following diagram shows the edge case:
			//
			//         Next Entry
			//            +
			//            |
			//            |
			//+-----------v-------------+
			//|           T T           |    *next_level
			//+-----------+-+-----------+
			//            | |
			//      +-----+ +--+
			//      |          |
			//  +---v----+  +--v-+--------+
			//  |      |B|  |  T |      |B|  level
			//  +--------+  +-+--+--------+
			//                |
			//                v
			//              +-+-+------+
			//              | T |      |   freed
			//              +-+-+------+
			//                |
			//                v
			//              +----------+
			//              |P|        |   freed
			//              +----------+
			// Here two levels are freed, the *next entry* is the
			// entry for next iteration.
			*next_level = util_min(*next_level, level);
			// bigger virtual address, further
			*next_virtual_address =
				util_max(*next_virtual_address,
					 step_virtual_address(virtual_address,
							      cur_level_info));
			walked_size = (*next_virtual_address - virtual_address);
			*next_size  = util_max(size, walked_size) - walked_size;

			free_list[free_idx] = &stack[level + 1U];
			free_idx++;
			// invalidate current entry
			set_invalid_entry(cur_table, cur_idx);

			dec = true;
		}

		if ((refcount == 0U) && (level > 0U)) {
			// will decrease it's parent level entry count
			level--;
		} else {
			// break if current level entry count is non-zero
			break;
		}

		cur_table = NULL;
	}

	if (free_idx > 0U) {
		// We need to ensure that the removed levels are no longer
		// reachable and all of their walk cache entries are removed
		// before we reuse the memory for any other purpose.
		if (margs->stage == PGTABLE_HYP_STAGE_1) {
			dsb_st(false);
			hyp_tlbi_va(virtual_address);
			dsb(false);
		} else {
			dsb_st(margs->outer_shareable);
			vm_tlbi_ipa(virtual_address, margs->outer_shareable);
			dsb(margs->outer_shareable);
		}
	}

	while (free_idx > 0U) {
		free_idx--;

		if (free_list[free_idx]->need_unmap) {
			// Only used by unmap, should always need unmap
			partition_phys_unmap(free_list[free_idx]->table,
					     free_list[free_idx]->paddr,
					     util_bit(pgt->granule_shift));
			free_list[free_idx]->need_unmap = false;
		}

		(void)partition_free_phys(partition, free_list[free_idx]->paddr,
					  util_bit(pgt->granule_shift));
		free_list[free_idx]->table  = NULL;
		free_list[free_idx]->paddr  = 0U;
		free_list[free_idx]->mapped = false;
	}
}

#if 0
static bool
map_should_set_cont(vmaddr_t virtual_address, size_t size,
		    vmaddr_t entry_address, index_t level)
{
	const pgtable_level_info_t *info = &level_conf[level];

	assert(info->contiguous_entry_cnt != 0U);

	size_t	 cont_size  = info->addr_size * info->contiguous_entry_cnt;
	vmaddr_t cont_start = util_balign_down(entry_address, cont_size);

	assert(!util_add_overflows(cont_start, cont_size - 1U));
	vmaddr_t cont_end = cont_start + cont_size - 1U;

	assert(!util_add_overflows(virtual_address, size - 1U));
	vmaddr_t virtual_end = virtual_address + size - 1U;

	return (cont_start >= virtual_address) && (cont_end &= virtual_end);
}
#endif

static bool
unmap_should_clear_cont(vmaddr_t virtual_address, size_t size, index_t level)
{
	const pgtable_level_info_t *info = &level_conf[level];

	assert(info->contiguous_entry_cnt != 0U);

	size_t	 cont_size  = info->addr_size * info->contiguous_entry_cnt;
	vmaddr_t cont_start = util_balign_down(virtual_address, cont_size);

	assert(!util_add_overflows(cont_start, cont_size - 1U));
	vmaddr_t cont_end = cont_start + cont_size - 1U;

	assert(!util_add_overflows(virtual_address, size - 1U));
	vmaddr_t virtual_end = virtual_address + size - 1U;

	return (cont_start < virtual_address) || (cont_end > virtual_end);
}

static void
unmap_clear_cont_bit(vmsa_level_table_t *table, vmaddr_t virtual_address,
		     index_t			       level,
		     vmsa_page_and_block_attrs_entry_t attr_entry,
		     pgtable_unmap_modifier_args_t    *margs,
		     count_t granule_shift, index_t start_level)
{
	const pgtable_level_info_t *info = &level_conf[level];

	assert(info->contiguous_entry_cnt != 0U);

	// get index range in current table to clear cont bit
	index_t cur_idx =
		get_index(virtual_address, info, (level == start_level));
	index_t idx_start =
		util_balign_down(cur_idx, info->contiguous_entry_cnt);
	index_t idx_end =
		(index_t)(idx_start + info->contiguous_entry_cnt - 1U);

	// start break-before-make sequence: clear all contiguous entries
	for (index_t idx = idx_start; idx <= idx_end; idx++) {
		set_invalid_entry(table, idx);
	}

	// flush all contiguous entries from TLB (note that the CPU may not
	// implement the contiguous bit at this level, so we are required to
	// flush addresses in all entries)
	vmaddr_t vaddr =
		virtual_address &
		~((util_bit(info->lsb) * info->contiguous_entry_cnt) - 1U);
#ifdef ARCH_ARM_FEAT_TLBIRANGE
	if (margs->stage == PGTABLE_HYP_STAGE_1) {
		dsb_st(false);
		hyp_tlbi_va_range(vaddr,
				  info->contiguous_entry_cnt * info->addr_size,
				  granule_shift);
	} else {
		dsb_st(margs->outer_shareable);
		hyp_tlbi_ipa_range(vaddr,
				   info->contiguous_entry_cnt * info->addr_size,
				   granule_shift, margs->outer_shareable);
	}
#else
	dsb_st(margs->outer_shareable);
	for (index_t i = 0; i < info->contiguous_entry_cnt; i++) {
		if (margs->stage == PGTABLE_HYP_STAGE_1) {
			hyp_tlbi_va(vaddr);
		} else {
			vm_tlbi_ipa(vaddr, margs->outer_shareable);
		}
		vaddr += info->addr_size;
	}
	(void)granule_shift;
#endif

	// Restore the entries other than cur_idx, with the cont bit cleared
	vmsa_upper_attrs_t upper_attrs =
		vmsa_page_and_block_attrs_entry_get_upper_attrs(&attr_entry);
	vmsa_lower_attrs_t lower_attrs =
		vmsa_page_and_block_attrs_entry_get_lower_attrs(&attr_entry);
	vmsa_common_upper_attrs_t upper_attrs_bitfield =
		vmsa_common_upper_attrs_cast(upper_attrs);
	assert(vmsa_common_upper_attrs_get_cont(&upper_attrs_bitfield));
	vmsa_common_upper_attrs_set_cont(&upper_attrs_bitfield, false);
	upper_attrs = vmsa_common_upper_attrs_raw(upper_attrs_bitfield);
	vmsa_page_and_block_attrs_entry_set_upper_attrs(&attr_entry,
							upper_attrs);

	vmsa_entry_t entry = {
		.attrs = attr_entry,
	};

	const bool use_block =
		pgtable_entry_types_get_block(&info->allowed_types);
	paddr_t		      entry_phys = 0U;
	pgtable_entry_types_t type	 = pgtable_entry_types_default();
	for (index_t idx = idx_start; idx <= idx_end; idx++) {
		if (idx == cur_idx) {
			// This should be left invalid
		} else if (use_block) {
			pgtable_entry_types_set_block(&type, true);
			get_entry_paddr(info, &entry, type, &entry_phys);
			entry_phys &= ~((util_bit(info->lsb) *
					 info->contiguous_entry_cnt) -
					1U);

			set_block_entry(table, idx, entry_phys, upper_attrs,
					lower_attrs, false, false, false);
		} else {
			pgtable_entry_types_set_page(&type, true);
			get_entry_paddr(info, &entry, type, &entry_phys);
			entry_phys &= ~((util_bit(info->lsb) *
					 info->contiguous_entry_cnt) -
					1U);
			set_page_entry(table, idx, entry_phys, upper_attrs,
				       lower_attrs, false, false);
		}
		entry_phys += info->addr_size;
	}
}

// @brief Unmap the current entry if possible.
//
// This modifier will try to:
// * Decrease the reference count (entry count) of current entry. If it's
// allowed (not preserved) and possible (ref count == 0), it will free the
// next page table level. In this case, It will guide @see
// translation_table_walk to step onto the next entry at the same level, and
// update the size as well.
// * Invalidate current entry.
static pgtable_modifier_ret_t
unmap_modifier(pgtable_t *pgt, vmaddr_t virtual_address, size_t size,
	       index_t idx, index_t level, pgtable_entry_types_t type,
	       stack_elem_t stack[PGTABLE_LEVEL_NUM], void *data,
	       index_t *next_level, vmaddr_t *next_virtual_address,
	       size_t *next_size, bool only_matching)
{
	const pgtable_level_info_t    *cur_level_info = NULL;
	pgtable_unmap_modifier_args_t *margs =
		(pgtable_unmap_modifier_args_t *)data;
	pgtable_modifier_ret_t vret	 = PGTABLE_MODIFIER_RET_CONTINUE;
	vmsa_level_table_t    *cur_table = NULL;
	vmsa_entry_t	       cur_entry;
	bool		       need_dec = false;

	assert(pgt != NULL);

	// current level should be mapped
	assert(stack[level].mapped);
	cur_table = stack[level].table;

	cur_level_info = &level_conf[level];
	// FIXME: if cur_entry is not used, remove it
	cur_entry = get_entry(cur_table, idx);

	// Set invalid entry and unmap/free the page level
	// NOTE: it's possible to forecast if we can free the whole sub page
	// table levels when we got a page table level entry, but it's just for
	// certain cases (especially the last second page)

	// No need to decrease entry count in upper page table level by default,
	// for INVALID entry.
	need_dec = false;

	if (only_matching && (pgtable_entry_types_get_block(&type) ||
			      pgtable_entry_types_get_page(&type))) {
		// Check if it is mapped to different phys_addr than expected
		// If so, do not unmap this address
		paddr_t phys_addr;

		get_entry_paddr(cur_level_info, &cur_entry, type, &phys_addr);
		if ((phys_addr < margs->phys) ||
		    (phys_addr > (margs->phys + margs->size - 1U))) {
			goto out;
		}
	}

	// Split the block if necessary
	if (pgtable_entry_types_get_block(&type)) {
		size_t	 addr_size = cur_level_info->addr_size;
		vmaddr_t entry_virtual_address =
			entry_start_address(virtual_address, cur_level_info);
		if ((virtual_address != entry_virtual_address) ||
		    (size < addr_size)) {
			// Partial unmap; split the block into 4K pages.
			vmsa_page_and_block_attrs_entry_t attr_entry =
				vmsa_page_and_block_attrs_entry_cast(
					vmsa_general_entry_raw(cur_entry.base));
			vmsa_lower_attrs_t lower_attrs;
			lower_attrs =
				vmsa_page_and_block_attrs_entry_get_lower_attrs(
					&attr_entry);
			vmsa_upper_attrs_t upper_attrs =
				vmsa_page_and_block_attrs_entry_get_upper_attrs(
					&attr_entry);
			paddr_t entry_phys;
			get_entry_paddr(cur_level_info, &cur_entry, type,
					&entry_phys);

			pgtable_map_modifier_args_t mremap_args = { 0 };
			mremap_args.phys			= entry_phys;
			mremap_args.partition	= margs->partition;
			mremap_args.lower_attrs = lower_attrs;
			mremap_args.upper_attrs = upper_attrs;
			mremap_args.new_page_start_level =
				PGTABLE_INVALID_LEVEL;
			mremap_args.try_map = true;
			mremap_args.stage   = margs->stage;

			vret = pgtable_split_block(
				pgt, virtual_address, size, cur_entry, idx,
				level, type, stack, &mremap_args, next_level,
				next_virtual_address, next_size);

			goto out;
		}
	}

	if (pgtable_entry_types_get_block(&type) ||
	    pgtable_entry_types_get_page(&type)) {
		vmsa_upper_attrs_t upper_attrs = get_upper_attr(cur_entry);
		vmsa_common_upper_attrs_t upper_attrs_bitfield =
			vmsa_common_upper_attrs_cast(upper_attrs);

		// clear contiguous bit if needed
		if (vmsa_common_upper_attrs_get_cont(&upper_attrs_bitfield) &&
		    unmap_should_clear_cont(virtual_address, size, level)) {
			vmsa_page_and_block_attrs_entry_t attr_entry =
				vmsa_page_and_block_attrs_entry_cast(
					vmsa_general_entry_raw(cur_entry.base));
			unmap_clear_cont_bit(cur_table, virtual_address, level,
					     attr_entry, margs,
					     pgt->granule_shift,
					     pgt->start_level);
		} else {
			set_invalid_entry(cur_table, idx);

			// need to decrease entry count for this table level
			need_dec = true;

			if (margs->stage == PGTABLE_HYP_STAGE_1) {
				dsb_st(false);
				hyp_tlbi_va(virtual_address);
			} else {
				dsb_st(margs->outer_shareable);
				vm_tlbi_ipa(virtual_address,
					    margs->outer_shareable);
			}
		}
	} else {
		assert(pgtable_entry_types_get_invalid(&type));
	}

	if (level != pgt->start_level) {
		check_refcount(pgt, margs->partition, virtual_address, size,
			       level - 1U, stack, need_dec, margs, next_level,
			       next_virtual_address, next_size);
	}

out:
	cur_table = NULL;

	return vret;
}

// @brief Pre-allocate specified page table level for certain virtual address
// range.
//
// This modifier just allocate/adds some page table level using the specified
// partition. The usage of this call is to guarantee the currently used page
// table level is still valid after release certain partition.
static pgtable_modifier_ret_t
prealloc_modifier(pgtable_t *pgt, vmaddr_t virtual_address, size_t size,
		  index_t level, pgtable_entry_types_t type,
		  stack_elem_t stack[PGTABLE_LEVEL_NUM], void *data,
		  index_t *next_level, vmaddr_t *next_virtual_address,
		  size_t *next_size)
{
	pgtable_prealloc_modifier_args_t *margs =
		(pgtable_prealloc_modifier_args_t *)data;
	pgtable_modifier_ret_t	    vret = PGTABLE_MODIFIER_RET_CONTINUE;
	error_t			    ret	 = OK;
	const pgtable_level_info_t *cur_level_info = NULL;
	paddr_t			    new_pgt_paddr;
	size_t			    addr_size = 0U, level_size = 0U;
	vmsa_level_table_t	   *new_pgt = NULL;

	assert(pgtable_entry_types_get_invalid(&type));
	assert(data != NULL);
	assert(pgt != NULL);

	assert(stack[level].mapped);

	cur_level_info = &level_conf[level];
	addr_size      = cur_level_info->addr_size;
	level_size     = size_on_level(virtual_address, size, cur_level_info);

	// FIXME: since all size are at least page aligned, level_size should also
	// be page aligned, add assert here

	if (addr_size <= level_size) {
		// hook pages to the existing page table levels
		// for the case that the root level is the level need to
		// preserve, it just return since new_page_start_level is not
		// set
		if (margs->new_page_start_level != PGTABLE_INVALID_LEVEL) {
			set_pgtables(virtual_address, stack,
				     margs->new_page_start_level, level, 0U,
				     pgt->start_level, false);

			margs->new_page_start_level = PGTABLE_INVALID_LEVEL;
		}

		// go to next entry at the same level
		goto out;
	} else {
		// if (addr_size > level_size)
		ret = alloc_level_table(margs->partition,
					util_bit(pgt->granule_shift),
					util_bit(pgt->granule_shift),
					&new_pgt_paddr, &new_pgt);
		if (ret != OK) {
			LOG(ERROR, WARN, "Failed to allocate page.\n");
			vret	     = PGTABLE_MODIFIER_RET_ERROR;
			margs->error = ret;
			goto out;
		}

		if (margs->new_page_start_level == PGTABLE_INVALID_LEVEL) {
			margs->new_page_start_level =
				level > pgt->start_level ? level - 1U : level;
		}

		stack[level + 1U] = (stack_elem_t){
			.paddr	    = new_pgt_paddr,
			.table	    = new_pgt,
			.mapped	    = true,
			.need_unmap = false,
			.entry_cnt  = level_conf[level + 1U].entry_cnt,
		};

		// step into the next sub level, with nothing stepped
		*next_virtual_address = virtual_address;
		*next_size	      = size;
		*next_level	      = level + 1U;
	}

out:
	return vret;
}

#if !defined(NDEBUG)
static pgtable_modifier_ret_t
dump_modifier(vmaddr_t virtual_address, size_t size,
	      stack_elem_t stack[PGTABLE_LEVEL_NUM], index_t idx, index_t level,
	      pgtable_entry_types_t type)
{
	const pgtable_level_info_t *cur_level_info = NULL;
	vmsa_level_table_t	   *cur_table	   = NULL;
	vmsa_entry_t		    cur_entry;
	uint64_t		   *entry_val = &cur_entry.base.bf[0];
	paddr_t			    p;
	count_t			    refcount;
	vmaddr_t		    cur_virtual_address;
	const char		   *msg_type = "[X]";
	char			    indent[16];
	index_t			    i;
	pgtable_modifier_ret_t	    vret      = PGTABLE_MODIFIER_RET_CONTINUE;
	size_t			    addr_size = 0U;

	if (size == 0U) {
		vret = PGTABLE_MODIFIER_RET_STOP;
		goto out;
	}

	assert(stack[level].mapped);
	cur_table = stack[level].table;

	cur_level_info = &level_conf[level];
	addr_size      = cur_level_info->addr_size;
	cur_entry      = get_entry(cur_table, idx);
	refcount       = get_table_refcount(cur_table, idx);

	if (!pgtable_entry_types_get_invalid(&type)) {
		get_entry_paddr(cur_level_info, &cur_entry, type, &p);
	} else {
		p = 0U;
	}

	// FIXME: check if cur_virtual_address is right
	cur_virtual_address = set_index(virtual_address, cur_level_info, idx) &
			      (~util_mask(cur_level_info->lsb));

	assert((size_t)level < (sizeof(indent) - 1U));
	indent[0] = '|';
	for (i = 1; i <= level; i++) {
		indent[i] = '\t';
	}
	indent[i] = '\0';

	if (pgtable_entry_types_get_next_level_table(&type)) {
		msg_type = "[Table]";
		LOG(DEBUG, INFO,
		    "{:s}->{:s} entry[{:#x}] virtual_address({:#x})",
		    (register_t)indent, (register_t)msg_type, *entry_val,
		    cur_virtual_address);
		LOG(DEBUG, INFO,
		    "{:s}phys({:#x}) idx({:d}) cnt({:d}) level({:d})",
		    (register_t)indent, (register_t)p, (register_t)idx,
		    (register_t)refcount, (register_t)cur_level_info->level);
		LOG(DEBUG, INFO, "{:s}addr_size({:#x})", (register_t)indent,
		    (register_t)addr_size);
	} else if (pgtable_entry_types_get_block(&type) ||
		   pgtable_entry_types_get_page(&type)) {
		if (pgtable_entry_types_get_block(&type)) {
			msg_type = "[Block]";
		} else {
			msg_type = "[Page]";
		}
		LOG(DEBUG, INFO,
		    "{:s}->{:s} entry[{:#x}] virtual_address({:#x})",
		    (register_t)indent, (register_t)msg_type,
		    (register_t)*entry_val, (register_t)cur_virtual_address);
		LOG(DEBUG, INFO, "{:s}phys({:#x}) idx({:d}) level({:d})",
		    (register_t)indent, (register_t)p, (register_t)idx,
		    (register_t)cur_level_info->level);
		LOG(DEBUG, INFO, "{:s}addr_size({:#x})", (register_t)indent,
		    (register_t)addr_size);
	} else {
		if (!pgtable_entry_types_get_invalid(&type)) {
			if (pgtable_entry_types_get_reserved(&type)) {
				msg_type = "[Reserved]";
			} else if (pgtable_entry_types_get_error(&type)) {
				msg_type = "[Error]";
			} else {
				// Nothing to do
			}
			LOG(DEBUG, INFO,
			    "{:s}->{:s} virtual_address({:#x}) idx({:d})",
			    (register_t)indent, (register_t)msg_type,
			    (register_t)cur_virtual_address, (register_t)idx);
		}
	}

out:
	cur_table = NULL;

	return vret;
}
#endif // !defined(NDEBUG)

#if defined(HOST_TEST)
static pgtable_modifier_ret_t
external_modifier(pgtable_t *pgt, vmaddr_t virtual_address, size_t size,
		  index_t idx, index_t level, pgtable_entry_types_t type,
		  stack_elem_t stack[PGTABLE_LEVEL_NUM], void *data,
		  index_t *next_level, vmaddr_t *next_virtual_address,
		  size_t *next_size, paddr_t next_table)
{
	ext_modifier_args_t   *margs	 = (ext_modifier_args_t *)data;
	void		      *func_data = margs->data;
	pgtable_modifier_ret_t ret	 = PGTABLE_MODIFIER_RET_STOP;

	if (margs->func != NULL) {
		ret = margs->func(pgt, virtual_address, size, idx, level, type,
				  stack, func_data, next_level,
				  next_virtual_address, next_size, next_table);
	}

	return ret;
}
#endif // !defined(HOST_TEST)

// @brief Generic code to walk through translation table.
//
// This function is generic for stage 1 and stage 2 translation table walking.
// Depends on the specified event, it triggers proper function (modifier) to
// handle current table entry.
// When the modifier function is called, it can get the following information:
// * Current start virtual address
// * Current entry type
// * Current page table level physical address
// * Current page table level
//   There are two kinds of level. One level is 0 based. The other concept of
//   level is based on ARM reference manual. It's defined @see level_type.
//   For example, the 64K granule page table with LPA feature does not have
//   the first level.
//   The previous level is used to control the loop, and the second level is
//   used to manipulate page table level stack.
// * Remaining size for the walking
//   If the address range specified by current start virtual address and
//   remaining size is fully visited, this function will return.
// * The current page table control structure
// * The page table level physical address stack
//   This stack can be used to get back to upper level.
// * A private pointer which modifier can interpret by itself, it remains the
//   same during the walking procedure.
//
// The modifier function can also control the walking by set:
// * Next page table physical address
// * Next level
// * Next start virtual address
// * Next remaining size
// And these four variables can fully control the flow of walking. Also, the
// return value of the modifier function provides a simple way to
// stop/continue, or report errors.
//
// The walking process is surely ended:
// * Finished visiting the specified address range.
// * Reach the maximum virtual address.
// But when the modifier changed internal variables, it's modifier's
// responsibility to make sure it can ended.
//
// @param pgt page table control structure.
// @param root_pa the physical address of the page table we started to walk.
// It's allowed to use the mid level page tables to do the walking.
// @param root the virtual address of page table.
// @param virtual_address the start virtual address.
// @param size the size of virtual address it needs to visit.
// @param event specifies the modifier to call.
// @param expected specifies the table entry type the modifier needs.
// @param data specifier an opaque data structure specific for modifier.
// @return true if the finished the walking without error. Or false indicates
// the failure.
static bool
translation_table_walk(pgtable_t *pgt, vmaddr_t virtual_address,
		       size_t virtual_address_size,
		       pgtable_translation_table_walk_event_t event,
		       pgtable_entry_types_t expected, void *data)
{
	paddr_t		      root_pa	  = pgt->root_pgtable;
	vmsa_level_table_t   *root	  = pgt->root;
	index_t		      start_level = pgt->start_level;
	index_t		      prev_level;
	index_t		      prev_idx;
	vmaddr_t	      prev_virtual_address;
	size_t		      prev_size;
	vmsa_entry_t	      prev_entry;
	pgtable_entry_types_t prev_type;

	// loop control variable
	index_t	 cur_level	     = start_level;
	paddr_t	 cur_table_paddr     = 0U;
	vmaddr_t cur_virtual_address = virtual_address;

	const pgtable_level_info_t *cur_level_info = NULL;
	index_t			    cur_idx;
	stack_elem_t		    stack[PGTABLE_LEVEL_NUM];
	vmsa_level_table_t	   *cur_table = NULL;
	vmsa_entry_t		    cur_entry;
	pgtable_entry_types_t	    cur_type;
	size_t			    cur_size = virtual_address_size;
	// ret: indicates whether walking is successful or not.
	// done: indicates the walking got a stop sign and need to return. It
	// can be changed by modifier.
	// ignores the modifier.
	bool ret = false, done = false;

	stack[start_level]	  = (stack_elem_t){ 0U };
	stack[start_level].paddr  = root_pa;
	stack[start_level].table  = root;
	stack[start_level].mapped = true;
	stack[start_level].entry_cnt =
		(count_t)(pgt->start_level_size / sizeof(cur_entry));

	while (cur_level < (index_t)util_array_size(level_conf)) {
		cur_level_info = &level_conf[cur_level];
		cur_idx	       = get_index(cur_virtual_address, cur_level_info,
					   (cur_level == start_level));

		if (compiler_unexpected(cur_level_info->is_offset)) {
			// Arrived offset segment, mapping is supposed to
			// be finished
			panic("pgtable walk depth error");
		}

		if (compiler_unexpected(cur_idx >=
					stack[cur_level].entry_cnt)) {
			// Index is outside the bounds of the table; address
			// range was not properly range-checked
			LOG(ERROR, WARN,
			    "Stepped out of the table (va {:#x}, level {:d}, idx {:d})",
			    cur_virtual_address, cur_level, cur_idx);
			panic("pgtable walk");
		}

		cur_table_paddr = stack[cur_level].paddr;
		if (stack[cur_level].mapped) {
			cur_table = stack[cur_level].table;
		} else {
			cur_table = (vmsa_level_table_t *)partition_phys_map(
				cur_table_paddr,
				stack[cur_level].entry_cnt * sizeof(cur_entry));
			if (compiler_unexpected(cur_table == NULL)) {
				LOG(ERROR, WARN,
				    "Failed to map table (pa {:#x}, level {:d}, idx {:d})\n",
				    cur_table_paddr, cur_level, cur_idx);
				panic("pgtable fault");
			}

			stack[cur_level].table	    = cur_table;
			stack[cur_level].mapped	    = true;
			stack[cur_level].need_unmap = true;
		}

		cur_entry = get_entry(cur_table, cur_idx);
		cur_type  = get_entry_type(&cur_entry, cur_level_info);

		// record the argument for modifier
		prev_virtual_address = cur_virtual_address;
		prev_level	     = cur_level;
		prev_idx	     = cur_idx;
		prev_entry	     = cur_entry;
		prev_type	     = cur_type;
		prev_size	     = cur_size;

		if (pgtable_entry_types_get_next_level_table(&cur_type)) {
			cur_level++;
			assert(cur_level < PGTABLE_LEVEL_NUM);

			get_entry_paddr(
				cur_level_info, &cur_entry,
				pgtable_entry_types_cast(
					PGTABLE_ENTRY_TYPES_NEXT_LEVEL_TABLE_MASK),
				&cur_table_paddr);

			cur_level_info		   = &level_conf[cur_level];
			stack[cur_level]	   = (stack_elem_t){ 0U };
			stack[cur_level].paddr	   = cur_table_paddr;
			stack[cur_level].mapped	   = false;
			stack[cur_level].table	   = NULL;
			stack[cur_level].entry_cnt = cur_level_info->entry_cnt;
		} else if (pgtable_entry_types_get_invalid(&cur_type) ||
			   pgtable_entry_types_get_page(&cur_type) ||
			   pgtable_entry_types_get_block(&cur_type)) {
			// for invalid entry, it must be handled by modifier.
			// Also by default, the next entry for the walking
			// is simply set to the next entry. The modifier should
			// guide the walking to go to the proper next entry.
			// Unless the modifier asks for continue the walking,
			// the walking process must stopped by default.

			// update virt address to visit next entry
			cur_virtual_address = step_virtual_address(
				cur_virtual_address, cur_level_info);
			size_t step_size =
				(cur_virtual_address - prev_virtual_address);

			if (cur_size >= step_size) {
				cur_size -= step_size;
			} else {
				cur_size = 0U;
			}

			// If we're at the lowest level
			if (pgtable_entry_types_get_page(
				    &cur_level_info->allowed_types)) {
				if (compiler_unexpected(
					    prev_size <
					    cur_level_info->addr_size)) {
					// wrong, size must be at least multiple
					// of page
					panic("pgtable bad size");
				}
			}

			if (cur_size == 0U) {
				// the whole walk is done, but modifier can
				// still ask for loop by changing the size
				done = true;
				ret  = true;
			} else {
				done = false;
				ret  = true;
			}

			// Iterate up on the last entry in the level(s)
			while (cur_idx == (stack[cur_level].entry_cnt - 1U)) {
				if (cur_level == start_level) {
					done = true;
					break;
				} else {
					cur_level--;
				}

				cur_level_info = &level_conf[cur_level];
				// cur_virtual_address is already stepped, use
				// previous one to check
				cur_idx = get_index(prev_virtual_address,
						    cur_level_info,
						    (cur_level == start_level));
			}
		} else {
			// shouldn't be here
			panic("pgtable corrupt entry");
		}

		cur_table = NULL;

		if (!pgtable_entry_types_is_empty(
			    pgtable_entry_types_intersection(prev_type,
							     expected))) {
			pgtable_modifier_ret_t vret;

			switch (event) {
			case PGTABLE_TRANSLATION_TABLE_WALK_EVENT_MMAP:
				vret = map_modifier(pgt, prev_virtual_address,
						    prev_size, prev_entry,
						    prev_idx, prev_level,
						    prev_type, stack, data,
						    &cur_level,
						    &cur_virtual_address,
						    &cur_size, cur_table_paddr);
				break;
			case PGTABLE_TRANSLATION_TABLE_WALK_EVENT_UNMAP:
				vret = unmap_modifier(pgt, prev_virtual_address,
						      prev_size, prev_idx,
						      prev_level, prev_type,
						      stack, data, &cur_level,
						      &cur_virtual_address,
						      &cur_size, false);
				break;
			case PGTABLE_TRANSLATION_TABLE_WALK_EVENT_UNMAP_MATCH:
				vret = unmap_modifier(pgt, prev_virtual_address,
						      prev_size, prev_idx,
						      prev_level, prev_type,
						      stack, data, &cur_level,
						      &cur_virtual_address,
						      &cur_size, true);
				break;
			case PGTABLE_TRANSLATION_TABLE_WALK_EVENT_LOOKUP:
				vret = lookup_modifier(pgt, prev_entry,
						       prev_level, prev_type,
						       data);
				break;
			case PGTABLE_TRANSLATION_TABLE_WALK_EVENT_PREALLOC:
				vret = prealloc_modifier(
					pgt, prev_virtual_address, prev_size,
					prev_level, prev_type, stack, data,
					&cur_level, &cur_virtual_address,
					&cur_size);
				break;
#ifndef NDEBUG
			case PGTABLE_TRANSLATION_TABLE_WALK_EVENT_DUMP:
				vret = dump_modifier(prev_virtual_address,
						     prev_size, stack, prev_idx,
						     prev_level, prev_type);
				break;
#endif
#if defined(HOST_TEST)
			case PGTABLE_TRANSLATION_TABLE_WALK_EVENT_EXTERNAL:
				vret = external_modifier(
					pgt, prev_virtual_address, prev_size,
					prev_idx, prev_level, prev_type, stack,
					data, &cur_level, &cur_virtual_address,
					&cur_size, cur_table_paddr);
				break;
#endif
			default:
				panic("pgtable bad event");
			}

			if (vret == PGTABLE_MODIFIER_RET_STOP) {
				ret  = true;
				done = true;
			} else if (vret == PGTABLE_MODIFIER_RET_ERROR) {
				ret  = false;
				done = true;
			} else if (vret == PGTABLE_MODIFIER_RET_CONTINUE) {
				// It's modifier's responsibility to work around
				// the walk error if it wishes to continue
				ret  = true;
				done = false;
			} else {
				// unknown return, just stop
				panic("pgtable bad vret");
			}
		}

		while (prev_level > cur_level) {
			// Discard page table level which is not used. If
			// modifier changes stack, it's modifier's
			// responsibility to unmap & maintain correct stack
			// status.
			// Since it's possible for modifier to unmap the table
			// level, need to double check if need to unmap the
			// levels here.
			if (!stack[prev_level].mapped) {
				prev_level--;
				continue;
			}

			if (stack[prev_level].need_unmap) {
				partition_phys_unmap(
					stack[prev_level].table,
					stack[prev_level].paddr,
					util_bit(pgt->granule_shift));
				stack[prev_level].need_unmap = false;
			}
			stack[prev_level].table	 = NULL;
			stack[prev_level].paddr	 = 0U;
			stack[prev_level].mapped = false;
			prev_level--;
		}

		// only next table should continue the loop
		if (done || (cur_size == 0U)) {
			break;
		}
	}
	while (cur_level > start_level) {
		if (stack[cur_level].mapped && stack[cur_level].need_unmap) {
			partition_phys_unmap(stack[cur_level].table,
					     stack[cur_level].paddr,
					     util_bit(pgt->granule_shift));
			stack[cur_level].need_unmap = false;
		}
		stack[cur_level].mapped = false;
		stack[cur_level].table	= NULL;
		cur_level--;
	}

	return ret;
}

static get_start_level_info_ret_t
get_start_level_info(const pgtable_level_info_t *infos, index_t msb,
		     bool is_stage2)
{
	get_start_level_info_ret_t ret = { .level = 0U, .size = 0UL };

	uint8_t level;
	count_t msb_offset = is_stage2 ? 4U : 0U;

	for (level = PGTABLE_LEVEL_NUM - 1U; level < PGTABLE_LEVEL_NUM;
	     level--) {
		const pgtable_level_info_t *level_info = &infos[level];
		if ((msb <= (level_info->msb + msb_offset)) &&
		    (msb >= level_info->lsb)) {
			size_t entry_cnt = util_bit(msb - level_info->lsb + 1U);
			ret.level	 = level;
			ret.size = sizeof(vmsa_general_entry_t) * entry_cnt;
			break;
		}
	}

	return ret;
}

void
pgtable_handle_boot_cold_init(void)
{
	index_t	      bottom_msb;
	const count_t page_shift = SHIFT_4K;
	partition_t  *partition	 = partition_get_private();

#if !defined(HOST_TEST)
	ID_AA64MMFR2_EL1_t mmfr2 = register_ID_AA64MMFR2_EL1_read();
	assert(ID_AA64MMFR2_EL1_get_BBM(&mmfr2) >= CPU_PGTABLE_BBM_LEVEL);
#endif
	spinlock_init(&hyp_pgtable.lock);

	hyp_pgtable.bottom_control.granule_shift = page_shift;
	hyp_pgtable.bottom_control.address_bits	 = HYP_ASPACE_LOW_BITS;
	bottom_msb				 = HYP_ASPACE_LOW_BITS - 1U;

	assert((HYP_ASPACE_LOW_BITS != level_conf[0].msb + 1) ||
	       (HYP_ASPACE_LOW_BITS != level_conf[1].msb + 1) ||
	       (HYP_ASPACE_LOW_BITS != level_conf[2].msb + 1) ||
	       (HYP_ASPACE_LOW_BITS != level_conf[3].msb + 1));

	get_start_level_info_ret_t bottom_info =
		get_start_level_info(level_conf, bottom_msb, false);
	hyp_pgtable.bottom_control.start_level	    = bottom_info.level;
	hyp_pgtable.bottom_control.start_level_size = bottom_info.size;

#if defined(ARCH_ARM_FEAT_VHE)
	index_t top_msb;
	error_t ret = OK;

	// FIXME: refine with more configurable code
	hyp_pgtable.top_control.granule_shift = page_shift;
	hyp_pgtable.top_control.address_bits  = HYP_ASPACE_HIGH_BITS;
	top_msb				      = HYP_ASPACE_HIGH_BITS - 1U;
	// FIXME: change to static check (with constant?)??
	// Might be better to use hyp_pgtable.top_control.address_bits
	assert((HYP_ASPACE_HIGH_BITS != level_conf[0].msb + 1) ||
	       (HYP_ASPACE_HIGH_BITS != level_conf[1].msb + 1) ||
	       (HYP_ASPACE_HIGH_BITS != level_conf[2].msb + 1) ||
	       (HYP_ASPACE_HIGH_BITS != level_conf[3].msb + 1));

	// update level info based on virtual_address bits
	get_start_level_info_ret_t top_info =
		get_start_level_info(level_conf, top_msb, false);
	hyp_pgtable.top_control.start_level	 = top_info.level;
	hyp_pgtable.top_control.start_level_size = top_info.size;

#if defined(HOST_TEST)
	// allocate the top page table
	ret = alloc_level_table(partition, top_info.size,
				util_max(top_info.size, VMSA_TABLE_MIN_ALIGN),
				&hyp_pgtable.top_control.root_pgtable,
				&hyp_pgtable.top_control.root);
	if (ret != OK) {
		LOG(ERROR, WARN, "Failed to allocate high page table level.\n");
		goto out;
	}
#else
	hyp_pgtable.top_control.root =
		(vmsa_level_table_t *)&aarch64_pt_ttbr_level1;
	hyp_pgtable.top_control.root_pgtable = partition_virt_to_phys(
		partition, (uintptr_t)hyp_pgtable.top_control.root);
#endif

	// allocate the root page table
	ret = alloc_level_table(partition, bottom_info.size,
				util_max(bottom_info.size,
					 VMSA_TABLE_MIN_ALIGN),
				&hyp_pgtable.bottom_control.root_pgtable,
				&hyp_pgtable.bottom_control.root);
	if (ret != OK) {
		LOG(ERROR, WARN,
		    "Failed to allocate bottom page table level.\n");
		goto out;
	}
#else
	hyp_pgtable.bottom_control.root =
		(vmsa_level_table_t *)&aarch64_pt_ttbr_level1;
	hyp_pgtable.bottom_control.root_pgtable = partition_virt_to_phys(
		partition, (uintptr_t)hyp_pgtable.bottom_control.root);
#endif

	ttbr0_phys = hyp_pgtable.bottom_control.root_pgtable;

	// activate the lower address space now for cold-boot
	pgtable_handle_boot_runtime_warm_init();

#if defined(ARCH_ARM_FEAT_VHE)
out:
	if (ret != OK) {
		panic("Failed to initialize hypervisor root page-table");
	}
#endif
}

#if !defined(HOST_TEST)
void
pgtable_handle_boot_runtime_warm_init(void)
{
#if defined(ARCH_ARM_FEAT_VHE)
	TTBR0_EL2_t ttbr0_val = TTBR0_EL2_default();
	TTBR0_EL2_set_BADDR(&ttbr0_val, ttbr0_phys);
	TTBR0_EL2_set_CnP(&ttbr0_val, true);

	TCR_EL2_E2H1_t tcr_val = register_TCR_EL2_E2H1_read();
	TCR_EL2_E2H1_set_T0SZ(&tcr_val, (uint8_t)(64U - HYP_ASPACE_LOW_BITS));
	TCR_EL2_E2H1_set_EPD0(&tcr_val, false);
	TCR_EL2_E2H1_set_ORGN0(&tcr_val, TCR_RGN_NORMAL_WRITEBACK_RA_WA);
	TCR_EL2_E2H1_set_IRGN0(&tcr_val, TCR_RGN_NORMAL_WRITEBACK_RA_WA);
	TCR_EL2_E2H1_set_SH0(&tcr_val, TCR_SH_INNER_SHAREABLE);
	TCR_EL2_E2H1_set_TG0(&tcr_val, TCR_TG0_GRANULE_SIZE_4KB);

	dsb(false);
	register_TTBR0_EL2_write_barrier(ttbr0_val);
	asm_context_sync_fence();
	register_TCR_EL2_E2H1_write_barrier(tcr_val);
	asm_context_sync_fence();
#endif
}
#endif

#if defined(HOST_TEST)
void
pgtable_hyp_destroy(partition_t *partition)
{
	vmaddr_t virtual_address = 0x0U;
	size_t	 size		 = 0x0U;

	assert(partition != NULL);

	// we should unmap everything
	virtual_address = 0x0U;
	size		= util_bit(hyp_pgtable.bottom_control.address_bits);
	pgtable_hyp_unmap(partition, virtual_address, size,
			  PGTABLE_HYP_UNMAP_PRESERVE_NONE);

	virtual_address = ~util_mask(hyp_pgtable.top_control.address_bits);
	size		= util_bit(hyp_pgtable.top_control.address_bits);
	pgtable_hyp_unmap(partition, virtual_address, size,
			  PGTABLE_HYP_UNMAP_PRESERVE_NONE);

	// free top level page table
	partition_free(partition, hyp_pgtable.top_control.root,
		       util_bit(hyp_pgtable.top_control.granule_shift));
	hyp_pgtable.top_control.root = NULL;
	partition_free(partition, hyp_pgtable.bottom_control.root,
		       util_bit(hyp_pgtable.bottom_control.granule_shift));
	hyp_pgtable.bottom_control.root = NULL;

	memset(&hyp_pgtable, 0, sizeof(hyp_pgtable));
}
#endif

bool
pgtable_hyp_lookup(uintptr_t virtual_address, paddr_t *mapped_base,
		   size_t *mapped_size, pgtable_hyp_memtype_t *mapped_memtype,
		   pgtable_access_t *mapped_access)
{
	bool			       walk_ret = false;
	pgtable_lookup_modifier_args_t margs	= { 0 };
	pgtable_entry_types_t entry_types	= pgtable_entry_types_default();
	vmsa_upper_attrs_t    upper_attrs;
	vmsa_lower_attrs_t    lower_attrs;
	pgtable_t	     *pgt = NULL;

	assert(mapped_base != NULL);
	assert(mapped_size != NULL);
	assert(mapped_memtype != NULL);
	assert(mapped_access != NULL);

	bool is_high = is_high_virtual_address(virtual_address);
	if (is_high) {
#if defined(ARCH_ARM_FEAT_VHE)
		pgt = &hyp_pgtable.top_control;
#else
		walk_ret = false;
		goto out;
#endif
	} else {
		pgt = &hyp_pgtable.bottom_control;
	}

	if (!addr_check(virtual_address, pgt->address_bits, is_high)) {
		walk_ret = false;
		goto out;
	}

	pgtable_entry_types_set_block(&entry_types, true);
	pgtable_entry_types_set_page(&entry_types, true);
	// just try to lookup a page, but if it's a block, the modifier will
	// stop the walk and return success
	walk_ret = translation_table_walk(
		pgt, virtual_address, util_bit(pgt->granule_shift),
		PGTABLE_TRANSLATION_TABLE_WALK_EVENT_LOOKUP, entry_types,
		&margs);

	if (margs.size == 0U) {
		// Return error (not-mapped) if lookup found no pages.
		walk_ret = false;
	}

	if (walk_ret) {
		*mapped_base = margs.phys;
		*mapped_size = margs.size;

		// FIXME: we can simplify below 4 line
		lower_attrs	= get_lower_attr(margs.entry);
		upper_attrs	= get_upper_attr(margs.entry);
		*mapped_memtype = map_stg1_attr_to_memtype(lower_attrs);
		*mapped_access =
			map_stg1_attr_to_access(upper_attrs, lower_attrs);
	} else {
		*mapped_base	= 0U;
		*mapped_size	= 0U;
		*mapped_memtype = PGTABLE_HYP_MEMTYPE_WRITEBACK;
		*mapped_access	= PGTABLE_ACCESS_NONE;
	}

out:
	return walk_ret;
}

error_t
pgtable_hyp_preallocate(partition_t *partition, uintptr_t virtual_address,
			size_t size)
{
	pgtable_prealloc_modifier_args_t margs = { 0 };
	pgtable_t			*pgt   = NULL;
	pgtable_entry_types_t entry_types      = pgtable_entry_types_default();

	assert(partition != NULL);
	assert((size & (size - 1)) == 0U);
	assert((virtual_address & (size - 1)) == 0);

	bool is_high = is_high_virtual_address(virtual_address);
	if (is_high) {
#if defined(ARCH_ARM_FEAT_VHE)
		pgt = &hyp_pgtable.top_control;
#else
		margs.error = ERROR_ADDR_INVALID;
		goto out;
#endif
	} else {
		pgt = &hyp_pgtable.bottom_control;
	}

	assert(!util_add_overflows(virtual_address, size - 1));

	assert(addr_check(virtual_address, pgt->address_bits, is_high) &&
	       addr_check(virtual_address + size - 1, pgt->address_bits,
			  is_high));

	margs.partition		   = partition;
	margs.new_page_start_level = PGTABLE_INVALID_LEVEL;
	margs.error		   = OK;

	pgtable_entry_types_set_invalid(&entry_types, true);
	bool walk_ret = translation_table_walk(
		pgt, virtual_address, size,
		PGTABLE_TRANSLATION_TABLE_WALK_EVENT_PREALLOC, entry_types,
		&margs);

	if (!walk_ret && (margs.error == OK)) {
		margs.error = ERROR_FAILURE;
		goto out;
	}

out:
	return margs.error;
}

// FIXME: right now assume the virt address with size is free,
// no need to retry
// FIXME: assume the size must be single page size or available block
// size, or else, just map it as one single page.
static error_t
pgtable_do_hyp_map(partition_t *partition, uintptr_t virtual_address,
		   size_t size, paddr_t phys, pgtable_hyp_memtype_t memtype,
		   pgtable_access_t access, vmsa_shareability_t shareability,
		   bool try_map, size_t merge_limit)
	REQUIRE_LOCK(pgtable_hyp_map_lock)
{
	pgtable_map_modifier_args_t margs = { 0 };
	vmsa_stg1_lower_attrs_t	    l;
	vmsa_stg1_upper_attrs_t	    u;
	pgtable_t		   *pgt = NULL;

	assert(pgtable_op);

	assert(partition != NULL);

	bool is_high = is_high_virtual_address(virtual_address);
	if (is_high) {
#if defined(ARCH_ARM_FEAT_VHE)
		pgt = &hyp_pgtable.top_control;
#else
		margs.error = ERROR_ADDR_INVALID;
		goto out;
#endif
	} else {
		pgt = &hyp_pgtable.bottom_control;
	}

	if (util_add_overflows(virtual_address, size - 1U)) {
		margs.error = ERROR_ADDR_OVERFLOW;
		goto out;
	}

	if (!util_is_p2aligned(virtual_address, pgt->granule_shift)) {
		margs.error = ERROR_ARGUMENT_ALIGNMENT;
		goto out;
	}

	if (!util_is_p2aligned(phys, pgt->granule_shift)) {
		margs.error = ERROR_ARGUMENT_ALIGNMENT;
		goto out;
	}

	if (!util_is_p2aligned(size, pgt->granule_shift)) {
		margs.error = ERROR_ARGUMENT_ALIGNMENT;
		goto out;
	}

	if (!addr_check(virtual_address, pgt->address_bits, is_high) ||
	    !addr_check(virtual_address + size - 1U, pgt->address_bits,
			is_high)) {
		margs.error = ERROR_ADDR_INVALID;
		goto out;
	}

	margs.orig_virtual_address = virtual_address;
	margs.orig_size		   = size;
	margs.phys		   = phys;
	margs.partition		   = partition;
	vmsa_stg1_lower_attrs_init(&l);
	vmsa_stg1_upper_attrs_init(&u);

	map_stg1_memtype_to_attrs(memtype, &l);
	map_stg1_access_to_attrs(access, &u, &l);
	vmsa_stg1_lower_attrs_set_SH(&l, shareability);
	margs.lower_attrs	   = vmsa_stg1_lower_attrs_raw(l);
	margs.upper_attrs	   = vmsa_stg1_upper_attrs_raw(u);
	margs.new_page_start_level = PGTABLE_INVALID_LEVEL;
	margs.error		   = OK;
	margs.try_map		   = try_map;
	margs.stage		   = PGTABLE_HYP_STAGE_1;
	margs.merge_limit	   = merge_limit;

	// FIXME: try to unify the level number, just use one kind of level
	pgtable_entry_types_t entry_types = VMSA_ENTRY_TYPE_LEAF;
	pgtable_entry_types_set_next_level_table(&entry_types, true);
	bool walk_ret = translation_table_walk(
		pgt, virtual_address, size,
		PGTABLE_TRANSLATION_TABLE_WALK_EVENT_MMAP, entry_types, &margs);

	if (!walk_ret && (margs.error == OK)) {
		margs.error = ERROR_FAILURE;
	}
	if ((margs.error != OK) && (margs.partially_mapped_size != 0U)) {
		pgtable_hyp_unmap(partition, virtual_address,
				  margs.partially_mapped_size,
				  PGTABLE_HYP_UNMAP_PRESERVE_ALL);
	}
out:
	return margs.error;
}

error_t
pgtable_hyp_map_merge(partition_t *partition, uintptr_t virtual_address,
		      size_t size, paddr_t phys, pgtable_hyp_memtype_t memtype,
		      pgtable_access_t access, vmsa_shareability_t shareability,
		      size_t merge_limit)
{
	return pgtable_do_hyp_map(partition, virtual_address, size, phys,
				  memtype, access, shareability, true,
				  merge_limit);
}

error_t
pgtable_hyp_remap_merge(partition_t *partition, uintptr_t virtual_address,
			size_t size, paddr_t phys,
			pgtable_hyp_memtype_t memtype, pgtable_access_t access,
			vmsa_shareability_t shareability, size_t merge_limit)
{
	return pgtable_do_hyp_map(partition, virtual_address, size, phys,
				  memtype, access, shareability, false,
				  merge_limit);
}

// FIXME: assume the size must be multiple of single page size or available
// block size, or else, just unmap it with page size aligned range.
// May be something like some blocks + several pages.
//
// Also, will unmap the vaddress without considering the memtype and access
// permission.
//
// It's caller's responsibility to make sure the virt address is already fully
// mapped. There's no roll back, so any failure will cause partially unmap
// operation.
void
pgtable_hyp_unmap(partition_t *partition, uintptr_t virtual_address,
		  size_t size, size_t preserved_prealloc)
{
	pgtable_unmap_modifier_args_t margs = { 0 };
	pgtable_t		     *pgt   = NULL;

	assert(pgtable_op);

	assert(partition != NULL);
	assert(util_is_p2_or_zero(preserved_prealloc));

	bool is_high = is_high_virtual_address(virtual_address);
	if (is_high) {
#if defined(ARCH_ARM_FEAT_VHE)
		pgt = &hyp_pgtable.top_control;
#else
		goto out;
#endif
	} else {
		pgt = &hyp_pgtable.bottom_control;
	}

	assert(!util_add_overflows(virtual_address, size - 1));

	assert(addr_check(virtual_address, pgt->address_bits, is_high));
	assert(addr_check(virtual_address + size - 1, pgt->address_bits,
			  is_high));

	assert(util_is_p2aligned(virtual_address, pgt->granule_shift));
	assert(util_is_p2aligned(size, pgt->granule_shift));

	margs.partition	     = partition;
	margs.preserved_size = preserved_prealloc;
	margs.stage	     = PGTABLE_HYP_STAGE_1;

	bool walk_ret = translation_table_walk(
		pgt, virtual_address, size,
		PGTABLE_TRANSLATION_TABLE_WALK_EVENT_UNMAP,
		VMSA_ENTRY_TYPE_LEAF, &margs);
	if (!walk_ret) {
		panic("Error in pgtable_hyp_unmap");
	}

#if !defined(ARCH_ARM_FEAT_VHE)
out:
#endif
	return;
}

void
pgtable_hyp_start(void) LOCK_IMPL
{
	// Nothing to do here.

	// The pgtable_hyp code has to run with a lock and preempt disabled to
	// ensure forward progress and because the code is not thread safe.
	spinlock_acquire(&hyp_pgtable.lock);
#if !defined(NDEBUG)
	assert(!pgtable_op);
	pgtable_op = true;
#endif
}

void
pgtable_hyp_commit(void) LOCK_IMPL
{
	dsb(false);
	// An ISB is needed if the CPU does not implement FEAT_ETS2. If ETS2
	// is available, we can skip the ISB, because we never dynamically
	// create executable mappings in EL2 address space.
#if !defined(ARCH_ARM_FEAT_ETS2) || !ARCH_ARM_FEAT_ETS2
	asm_context_sync_fence();
#endif
#if !defined(NDEBUG)
	assert(pgtable_op);
	pgtable_op = false;
#endif
	spinlock_release(&hyp_pgtable.lock);
}

#ifndef NDEBUG
void
pgtable_hyp_dump(void)
{
	pgtable_entry_types_t entry_types =
		pgtable_entry_types_inverse(pgtable_entry_types_default());
	vmaddr_t virtual_address = 0U;
	size_t	 size		 = 0U;

	LOG(DEBUG, INFO, "+---------------- page table ----------------\n");
#if defined(ARCH_ARM_FEAT_VHE)
	LOG(DEBUG, INFO, "| TTBR1[{:#x}]:\n",
	    hyp_pgtable.top_control.root_pgtable);
	size = util_bit(hyp_pgtable.top_control.address_bits);
	(void)translation_table_walk(&hyp_pgtable.top_control, virtual_address,
				     size,
				     PGTABLE_TRANSLATION_TABLE_WALK_EVENT_DUMP,
				     entry_types, NULL);
#endif
	LOG(DEBUG, INFO, "\n");
	LOG(DEBUG, INFO, "| TTBR0[{:#x}]:\n",
	    hyp_pgtable.bottom_control.root_pgtable);
	size = util_bit(hyp_pgtable.bottom_control.address_bits);
	(void)translation_table_walk(&hyp_pgtable.bottom_control,
				     virtual_address, size,
				     PGTABLE_TRANSLATION_TABLE_WALK_EVENT_DUMP,
				     entry_types, NULL);
	LOG(DEBUG, INFO, "+--------------------------------------------\n\n");
}
#endif

#ifdef HOST_TEST
void
pgtable_hyp_ext(vmaddr_t virtual_address, size_t size,
		pgtable_entry_types_t entry_types, ext_func_t func, void *data)
{
	ext_modifier_args_t margs = { 0 };
	pgtable_t	   *pgt	  = NULL;

	bool is_high = is_high_virtual_address(virtual_address);
	if (is_high) {
		pgt = &hyp_pgtable.top_control;
	} else {
		pgt = &hyp_pgtable.bottom_control;
	}

	assert(addr_check(virtual_address, pgt->address_bits, is_high));
	assert(addr_check(virtual_address + size - 1, pgt->address_bits,
			  is_high));

	margs.func = func;
	margs.data = data;

	if (!util_is_p2aligned(size, pgt->granule_shift) ||
	    !util_is_p2aligned(size, pgt->granule_shift)) {
		LOG(DEBUG, INFO, "size not aligned\n");
		goto out;
	}
	if (!addr_check(virtual_address, pgt->address_bits, is_high)) {
		LOG(DEBUG, INFO, "address out of range\n");
		goto out;
	}
	if (!util_is_p2aligned(virtual_address, pgt->granule_shift)) {
		LOG(DEBUG, INFO, "address not aligned\n");
		goto out;
	}

	(void)translation_table_walk(
		pgt, virtual_address, size,
		PGTABLE_TRANSLATION_TABLE_WALK_EVENT_EXTERNAL, entry_types,
		&margs);
out:
	return;
}
#endif

#ifndef NDEBUG
void
pgtable_vm_dump(pgtable_vm_t *pgt)
{
	assert(pgt != NULL);

	pgtable_entry_types_t entry_types =
		pgtable_entry_types_inverse(pgtable_entry_types_default());

	size_t size = util_bit(pgt->control.address_bits);

	LOG(DEBUG, INFO, "+---------------- page table ----------------\n");
	LOG(DEBUG, INFO, "| TTBR({:#x}):\n", pgt->control.root_pgtable);
	(void)translation_table_walk(&pgt->control, 0L, size,
				     PGTABLE_TRANSLATION_TABLE_WALK_EVENT_DUMP,
				     entry_types, NULL);
	LOG(DEBUG, INFO, "+--------------------------------------------\n\n");
}
#endif

#ifdef HOST_TEST
void
pgtable_vm_ext(pgtable_vm_t *pgt, vmaddr_t virtual_address, size_t size,
	       pgtable_entry_types_t entry_types, ext_func_t func, void *data)
{
	ext_modifier_args_t margs = { 0 };

	assert(pgt != NULL);
	assert(addr_check(virtual_address, pgt->control.address_bits, false));
	assert(addr_check(virtual_address + size - 1, pgt->control.address_bits,
			  false));

	margs.func = func;
	margs.data = data;

	(void)translation_table_walk(
		&pgt->control, virtual_address, size,
		PGTABLE_TRANSLATION_TABLE_WALK_EVENT_EXTERNAL, entry_types,
		&margs);
}
#endif

static tcr_tg0_t
vtcr_get_tg0_code(size_t granule_shift)
{
	tcr_tg0_t tg0;

	switch (granule_shift) {
	case SHIFT_4K:
		tg0 = TCR_TG0_GRANULE_SIZE_4KB;
		break;
	case SHIFT_16K:
		tg0 = TCR_TG0_GRANULE_SIZE_16KB;
		break;
	case SHIFT_64K:
		tg0 = TCR_TG0_GRANULE_SIZE_64KB;
		break;
	default:
		panic("Invalid granule size");
	}

	return tg0;
}

#if !defined(HOST_TEST)
// Note: the nested macros are are needed to expand the config to a number
// before it is pasted into the enum name.
#define PLATFORM_TCR_PS	     TCR_PS_FOR_CONFIG(PLATFORM_PHYS_ADDRESS_BITS)
#define TCR_PS_FOR_CONFIG(x) TCR_PS_FOR_SIZE(x)
#define TCR_PS_FOR_SIZE(x)   TCR_PS_SIZE_##x##BITS

static void
pgtable_vm_init_regs(pgtable_vm_t *vm_pgtable)
{
	assert(vm_pgtable != NULL);

	// Init Virtualization Translation Control Register

	VTCR_EL2_init(&vm_pgtable->vtcr_el2);

	uint8_t t0sz = (uint8_t)(64U - vm_pgtable->control.address_bits);

	VTCR_EL2_set_T0SZ(&vm_pgtable->vtcr_el2, t0sz);

	if (vm_pgtable->control.granule_shift == SHIFT_4K) {
		switch (vm_pgtable->control.start_level) {
		case 0:
			VTCR_EL2_set_SL0(&vm_pgtable->vtcr_el2, 0x2);
			break;
		case 1:
			VTCR_EL2_set_SL0(&vm_pgtable->vtcr_el2, 0x1);
			break;
		case 2:
			VTCR_EL2_set_SL0(&vm_pgtable->vtcr_el2, 0x0);
			break;
		default:
			panic("Invalid SL0");
		}
	} else {
		switch (vm_pgtable->control.start_level) {
		case 1:
			VTCR_EL2_set_SL0(&vm_pgtable->vtcr_el2, 0x2);
			break;
		case 2:
			VTCR_EL2_set_SL0(&vm_pgtable->vtcr_el2, 0x1);
			break;
		case 3:
			VTCR_EL2_set_SL0(&vm_pgtable->vtcr_el2, 0x0);
			break;
		case 0:
		default:
			panic("Invalid SL0");
		}
	}
	VTCR_EL2_set_IRGN0(&vm_pgtable->vtcr_el2,
			   TCR_RGN_NORMAL_WRITEBACK_RA_WA);
	VTCR_EL2_set_ORGN0(&vm_pgtable->vtcr_el2,
			   TCR_RGN_NORMAL_WRITEBACK_RA_WA);
	VTCR_EL2_set_SH0(&vm_pgtable->vtcr_el2, TCR_SH_INNER_SHAREABLE);

	tcr_tg0_t tg0 = vtcr_get_tg0_code(vm_pgtable->control.granule_shift);
	VTCR_EL2_set_TG0(&vm_pgtable->vtcr_el2, tg0);

	// The output size is defined by the platform.
	VTCR_EL2_set_PS(&vm_pgtable->vtcr_el2, PLATFORM_TCR_PS);

	// The platform's implemented physical address space must be no
	// larger than the CPU's implemented physical address size.
	ID_AA64MMFR0_EL1_t id_aa64mmfr0 = register_ID_AA64MMFR0_EL1_read();
	assert(ID_AA64MMFR0_EL1_get_PARange(&id_aa64mmfr0) >=
	       VTCR_EL2_get_PS(&vm_pgtable->vtcr_el2));

	// The stage-2 input address size must be no larger than the CPU's
	// implemented physical address size (though it may be larger than the
	// platform's implemented physical address space, if that is smaller
	// than the CPU's).
	switch (ID_AA64MMFR0_EL1_get_PARange(&id_aa64mmfr0)) {
	case TCR_PS_SIZE_32BITS:
		assert(vm_pgtable->control.address_bits <= 32);
		break;
	case TCR_PS_SIZE_36BITS:
		assert(vm_pgtable->control.address_bits <= 36);
		break;
	case TCR_PS_SIZE_40BITS:
		assert(vm_pgtable->control.address_bits <= 40);
		break;
	case TCR_PS_SIZE_42BITS:
		assert(vm_pgtable->control.address_bits <= 42);
		break;
	case TCR_PS_SIZE_44BITS:
		assert(vm_pgtable->control.address_bits <= 44);
		break;
	case TCR_PS_SIZE_48BITS:
		assert(vm_pgtable->control.address_bits <= 48);
		break;
	case TCR_PS_SIZE_52BITS:
		assert(vm_pgtable->control.address_bits <= 52);
		break;
	default:
		panic("bad PARange");
	}

#if defined(ARCH_ARM_FEAT_VMID16)
	VTCR_EL2_set_VS(&vm_pgtable->vtcr_el2, true);
#endif

#if defined(ARCH_ARM_FEAT_HAFDBS)
	VTCR_EL2_set_HA(&vm_pgtable->vtcr_el2, true);
	ID_AA64MMFR1_EL1_t hw_mmfr1 = register_ID_AA64MMFR1_EL1_read();
	if (ID_AA64MMFR1_EL1_get_HAFDBS(&hw_mmfr1) == 2U) {
		VTCR_EL2_set_HD(&vm_pgtable->vtcr_el2, true);
	}
#endif

#if defined(ARCH_ARM_FEAT_HPDS2)
	VTCR_EL2_set_HWU059(&vm_pgtable->vtcr_el2, false);
	VTCR_EL2_set_HWU060(&vm_pgtable->vtcr_el2, false);
	VTCR_EL2_set_HWU061(&vm_pgtable->vtcr_el2, false);
	VTCR_EL2_set_HWU062(&vm_pgtable->vtcr_el2, false);
#endif

#if defined(ARCH_ARM_FEAT_SEC_EL2)
	VTCR_EL2_set_NSW(&vm_pgtable->vtcr_el2, true);
	VTCR_EL2_set_NSA(&vm_pgtable->vtcr_el2, true);
#endif

	// Init Virtualization Translation Table Base Register

	VTTBR_EL2_init(&vm_pgtable->vttbr_el2);
	VTTBR_EL2_set_CnP(&vm_pgtable->vttbr_el2, true);
	VTTBR_EL2_set_BADDR(&vm_pgtable->vttbr_el2,
			    vm_pgtable->control.root_pgtable);
#if defined(ARCH_ARM_FEAT_VMID16)
	VTTBR_EL2_set_VMID(&vm_pgtable->vttbr_el2, vm_pgtable->control.vmid);
#else
	VTTBR_EL2_set_VMID(&vm_pgtable->vttbr_el2,
			   (uint8_t)vm_pgtable->control.vmid);
#endif
}

void
pgtable_vm_load_regs(pgtable_vm_t *vm_pgtable)
{
	register_VTCR_EL2_write(vm_pgtable->vtcr_el2);
	register_VTTBR_EL2_write(vm_pgtable->vttbr_el2);
}
#endif

error_t
pgtable_vm_init(partition_t *partition, pgtable_vm_t *pgtable, vmid_t vmid)
{
	error_t ret = OK;
	index_t msb;

	if (pgtable->control.root != NULL) {
		// Address already setup by another module
		assert(pgtable->control.vmid == vmid);
		goto out;
	}

	// FIXME:
	// FIXME: refine with more configurable code
#if PGTABLE_VM_PAGE_SIZE == 4096
	pgtable->control.granule_shift = SHIFT_4K;
#else
#error untested granule size
#endif
	pgtable->control.address_bits = PLATFORM_VM_ADDRESS_SPACE_BITS;
	msb		      = (index_t)PLATFORM_VM_ADDRESS_SPACE_BITS - 1U;
	pgtable->control.vmid = vmid;

	get_start_level_info_ret_t info =
		get_start_level_info(level_conf, msb, true);
	pgtable->control.start_level	  = info.level;
	pgtable->control.start_level_size = info.size;
	pgtable->issue_dvm_cmd		  = false;

	// allocate the level 0 page table
	ret = alloc_level_table(partition, info.size,
				util_max(info.size, VMSA_TABLE_MIN_ALIGN),
				&pgtable->control.root_pgtable,
				&pgtable->control.root);
	if (ret != OK) {
		goto out;
	}

#if !defined(HOST_TEST)
	pgtable_vm_init_regs(pgtable);
#endif
	dsb(false);

out:
	return ret;
}

void
pgtable_vm_destroy(partition_t *partition, pgtable_vm_t *pgtable)
{
	vmaddr_t virtual_address = 0x0U;
	size_t	 size		 = 0x0U;

	assert(partition != NULL);
	assert(pgtable != NULL);
	assert(pgtable->control.root != NULL);

	virtual_address = 0x0U;
	size		= util_bit(pgtable->control.address_bits);
	// we should unmap everything
	pgtable_vm_start(pgtable);
	pgtable_vm_unmap(partition, pgtable, virtual_address, size);
	pgtable_vm_commit(pgtable);

	// free top level page table
	(void)partition_free(partition, pgtable->control.root,
			     pgtable->control.start_level_size);
	pgtable->control.root = NULL;
}

bool
pgtable_vm_lookup(pgtable_vm_t *pgtable, vmaddr_t virtual_address,
		  paddr_t *mapped_base, size_t *mapped_size,
		  pgtable_vm_memtype_t *mapped_memtype,
		  pgtable_access_t     *mapped_vm_kernel_access,
		  pgtable_access_t     *mapped_vm_user_access)
{
	bool			       walk_ret;
	pgtable_lookup_modifier_args_t margs = { 0 };
	pgtable_entry_types_t entry_types    = pgtable_entry_types_default();
	vmsa_upper_attrs_t    upper_attrs;
	vmsa_lower_attrs_t    lower_attrs;

	assert(pgtable != NULL);
	assert(mapped_base != NULL);
	assert(mapped_size != NULL);
	assert(mapped_memtype != NULL);
	assert(mapped_vm_kernel_access != NULL);
	assert(mapped_vm_user_access != NULL);

	if (!addr_check(virtual_address, pgtable->control.address_bits,
			false)) {
		// Address is out of range
		walk_ret = false;
		goto out;
	}

	pgtable_entry_types_set_block(&entry_types, true);
	pgtable_entry_types_set_page(&entry_types, true);

	// just try to lookup a page, but if it's a block, the modifier will
	// stop the walk and return success
	walk_ret = translation_table_walk(
		&pgtable->control, virtual_address,
		util_bit(pgtable->control.granule_shift),
		PGTABLE_TRANSLATION_TABLE_WALK_EVENT_LOOKUP, entry_types,
		&margs);

	if (margs.size == 0U) {
		// Return error (not-mapped) if lookup found no pages.
		walk_ret = false;
	}

	if (walk_ret) {
		*mapped_base = margs.phys;
		*mapped_size = margs.size;

		lower_attrs	= get_lower_attr(margs.entry);
		upper_attrs	= get_upper_attr(margs.entry);
		*mapped_memtype = map_stg2_attr_to_memtype(lower_attrs);
		map_stg2_attr_to_access(upper_attrs, lower_attrs,
					mapped_vm_kernel_access,
					mapped_vm_user_access);

	} else {
		*mapped_base		 = 0U;
		*mapped_size		 = 0U;
		*mapped_memtype		 = PGTABLE_VM_MEMTYPE_DEVICE_NGNRNE;
		*mapped_vm_kernel_access = PGTABLE_ACCESS_NONE;
		*mapped_vm_user_access	 = PGTABLE_ACCESS_NONE;
	}

out:
	return walk_ret;
}

// FIXME: right now assume the virt address with size is free,
// no need to retry
// FIXME: assume the size must be single page size or available block
// size, or else, just map it as one single page.
error_t
pgtable_vm_map(partition_t *partition, pgtable_vm_t *pgtable,
	       vmaddr_t virtual_address, size_t size, paddr_t phys,
	       pgtable_vm_memtype_t memtype, pgtable_access_t vm_kernel_access,
	       pgtable_access_t vm_user_access, bool try_map, bool allow_merge)
{
	pgtable_map_modifier_args_t margs = { 0 };
	vmsa_stg2_lower_attrs_t	    l;
	vmsa_stg2_upper_attrs_t	    u;

	assert(pgtable_op);

	assert(pgtable != NULL);
	assert(partition != NULL);

	if (!addr_check(virtual_address, pgtable->control.address_bits,
			false)) {
		margs.error = ERROR_ADDR_INVALID;
		goto fail;
	}

	if (util_add_overflows(virtual_address, size - 1U) ||
	    !addr_check(virtual_address + size - 1U,
			pgtable->control.address_bits, false)) {
		margs.error = ERROR_ADDR_OVERFLOW;
		goto fail;
	}

	// FIXME:
	// Supporting different granule sizes will need support and additional
	// checking to be added to memextent code.
	if (!util_is_p2aligned(virtual_address,
			       pgtable->control.granule_shift) ||
	    !util_is_p2aligned(phys, pgtable->control.granule_shift) ||
	    !util_is_p2aligned(size, pgtable->control.granule_shift)) {
		margs.error = ERROR_ARGUMENT_ALIGNMENT;
		goto fail;
	}

	// FIXME: how to check phys, read tcr in init?
	// FIXME: no need to to check vm memtype, right?

	margs.orig_virtual_address = virtual_address;
	margs.orig_size		   = size;
	margs.phys		   = phys;
	margs.partition		   = partition;
	vmsa_stg2_lower_attrs_init(&l);
	vmsa_stg2_upper_attrs_init(&u);
	map_stg2_memtype_to_attrs(memtype, &l);
	map_stg2_access_to_attrs(vm_kernel_access, vm_user_access, &u, &l);
	margs.lower_attrs	   = vmsa_stg2_lower_attrs_raw(l);
	margs.upper_attrs	   = vmsa_stg2_upper_attrs_raw(u);
	margs.new_page_start_level = PGTABLE_INVALID_LEVEL;
	margs.error		   = OK;
	margs.try_map		   = try_map;
	margs.stage		   = PGTABLE_VM_STAGE_2;
	margs.outer_shareable	   = pgtable->issue_dvm_cmd;
#if (CPU_PGTABLE_BBM_LEVEL > 0) || !defined(PLATFORM_PGTABLE_AVOID_BBM)
	// We can either trigger TLB conflicts safely because they will be
	// delivered to EL2, or else can use BBM.
	margs.merge_limit = allow_merge ? ~(size_t)0U : 0U;
#else
	// We can't use BBM, and merging without it might cause TLB conflict
	// aborts in EL1. This is unsafe because:
	// - the EL1 abort handler might trigger the same abort again, and
	// - Linux VMs treat TLB conflict aborts as fatal errors.
	(void)allow_merge;
	margs.merge_limit = false;
#endif

	// FIXME: try to unify the level number, just use one kind of level
	pgtable_entry_types_t entry_types = VMSA_ENTRY_TYPE_LEAF;
	pgtable_entry_types_set_next_level_table(&entry_types, true);
	bool walk_ret = translation_table_walk(
		&pgtable->control, virtual_address, size,
		PGTABLE_TRANSLATION_TABLE_WALK_EVENT_MMAP, entry_types, &margs);

	if (!walk_ret && (margs.error == OK)) {
		margs.error = ERROR_FAILURE;
	}
	if ((margs.error != OK) && (margs.partially_mapped_size != 0U)) {
		pgtable_vm_unmap(partition, pgtable, virtual_address,
				 margs.partially_mapped_size);
	}

fail:
	return margs.error;
}

void
pgtable_vm_unmap(partition_t *partition, pgtable_vm_t *pgtable,
		 vmaddr_t virtual_address, size_t size)
{
	pgtable_unmap_modifier_args_t margs = { 0 };

	assert(pgtable_op);

	assert(pgtable != NULL);
	assert(partition != NULL);

	if (!addr_check(virtual_address, pgtable->control.address_bits,
			false)) {
		panic("Bad arguments in pgtable_vm_unmap");
	}

	if (util_add_overflows(virtual_address, size - 1U) ||
	    !addr_check(virtual_address + size - 1U,
			pgtable->control.address_bits, false)) {
		panic("Bad arguments in pgtable_vm_unmap");
	}

	if (!util_is_p2aligned(virtual_address,
			       pgtable->control.granule_shift) ||
	    !util_is_p2aligned(size, pgtable->control.granule_shift)) {
		panic("Bad arguments in pgtable_vm_unmap");
	}

	margs.partition = partition;
	// no need to preserve table levels here
	margs.preserved_size  = PGTABLE_HYP_UNMAP_PRESERVE_NONE;
	margs.stage	      = PGTABLE_VM_STAGE_2;
	margs.outer_shareable = pgtable->issue_dvm_cmd;

	bool walk_ret = translation_table_walk(
		&pgtable->control, virtual_address, size,
		PGTABLE_TRANSLATION_TABLE_WALK_EVENT_UNMAP,
		VMSA_ENTRY_TYPE_LEAF, &margs);
	if (!walk_ret) {
		panic("Error in pgtable_vm_unmap");
	}
}

void
pgtable_vm_unmap_matching(partition_t *partition, pgtable_vm_t *pgtable,
			  vmaddr_t virtual_address, paddr_t phys, size_t size)
{
	pgtable_unmap_modifier_args_t margs = { 0 };

	assert(pgtable_op);

	assert(pgtable != NULL);
	assert(partition != NULL);

	if (!addr_check(virtual_address, pgtable->control.address_bits,
			false)) {
		panic("Bad arguments in pgtable_vm_unmap_matching");
	}

	if (util_add_overflows(virtual_address, size - 1U) ||
	    !addr_check(virtual_address + size - 1U,
			pgtable->control.address_bits, false)) {
		panic("Bad arguments in pgtable_vm_unmap_matching");
	}

	margs.partition = partition;
	// no need to preserve table levels here
	margs.preserved_size  = PGTABLE_HYP_UNMAP_PRESERVE_NONE;
	margs.stage	      = PGTABLE_VM_STAGE_2;
	margs.phys	      = phys;
	margs.size	      = size;
	margs.outer_shareable = pgtable->issue_dvm_cmd;

	bool walk_ret = translation_table_walk(
		&pgtable->control, virtual_address, size,
		PGTABLE_TRANSLATION_TABLE_WALK_EVENT_UNMAP_MATCH,
		VMSA_ENTRY_TYPE_LEAF, &margs);
	if (!walk_ret) {
		panic("Error in pgtable_vm_unmap_matching");
	}
}

void
pgtable_vm_start(pgtable_vm_t *pgtable) LOCK_IMPL
{
	assert(pgtable != NULL);
#ifndef HOST_TEST
	// FIXME:
	// We need to to run VM pagetable code with preempt disable due to
	// TLB flushes.
	preempt_disable();
#if !defined(NDEBUG)
	assert(!pgtable_op);
	pgtable_op = true;
#endif

	thread_t *thread = thread_get_self();

	// Since the pagetable code may need to flush the target VMID, we need
	// to ensure that it is current for the pagetable operations.
	// We set the VMID which is in the VTTBR register. Note, no need to set
	// VTCR - so ensure no TLB walks take place!.  This also assumes that
	// preempt is disabled otherwise a context-switch would restore the
	// original registers.
	if ((thread->addrspace == NULL) ||
	    (&thread->addrspace->vm_pgtable != pgtable)) {
		register_VTTBR_EL2_write_ordered(pgtable->vttbr_el2,
						 &asm_ordering);
		asm_context_sync_ordered(&asm_ordering);
	}
#endif
}

void
pgtable_vm_commit(pgtable_vm_t *pgtable) LOCK_IMPL
{
#ifndef HOST_TEST
#if !defined(NDEBUG)
	assert(pgtable_op);
	pgtable_op = false;
#endif

	dsb(pgtable->issue_dvm_cmd);
	// This is only needed when unmapping. Consider some flags to
	// track to flush requirements.
	vm_tlbi_vmalle1(pgtable->issue_dvm_cmd);
	dsb(pgtable->issue_dvm_cmd);

	thread_t *thread = thread_get_self();

	// Since the pagetable code flushes the target VMID, we set it as the
	// current VMID for the pagetable operations. We need to restore the
	// original VMID (in VTTBR_EL2) here.
	if ((thread->addrspace != NULL) &&
	    (&thread->addrspace->vm_pgtable != pgtable)) {
		register_VTTBR_EL2_write_ordered(
			thread->addrspace->vm_pgtable.vttbr_el2, &asm_ordering);
	}

	preempt_enable();
	trigger_pgtable_vm_commit_event(pgtable);
#endif // !HOST_TEST
}

```

`hyp/mem/pgtable/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

interface pgtable

arch_events armv8 pgtable.ev
arch_types armv8 pgtable.tc
arch_source armv8 pgtable.c
arch_configs armv8 PGTABLE_HYP_PAGE_SIZE=4096U
arch_configs armv8 PGTABLE_HYP_LARGE_PAGE_SIZE=2097152U
arch_configs armv8 PGTABLE_VM_PAGE_SIZE=4096U

```

`hyp/mem/useraccess/aarch64/src/useraccess.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>
#include <string.h>

#include <hypregisters.h>

#include <compiler.h>
#include <partition.h>
#include <pgtable.h>
#include <rcu.h>
#include <util.h>

#include <asm/barrier.h>
#include <asm/cache.h>
#include <asm/cpu.h>

#include "useraccess.h"

static void
useraccess_clean_range(const uint8_t *va, size_t size)
{
	CACHE_CLEAN_RANGE(va, size);
}

static void
useraccess_clean_invalidate_range(const uint8_t *va, size_t size)
{
	CACHE_CLEAN_INVALIDATE_RANGE(va, size);
}

static size_t
useraccess_copy_from_to_translated_pa(PAR_EL1_t par, gvaddr_t guest_va,
				      size_t page_size, size_t page_offset,
				      bool from_guest, void *hyp_buf,
				      size_t remaining)
{
	paddr_t guest_pa = PAR_EL1_F0_get_PA(&par.f0);
	guest_pa |= (paddr_t)guest_va & (page_size - 1U);

	size_t mapped_size = page_size - page_offset;
	void  *va	   = partition_phys_map(guest_pa, mapped_size);

	MAIR_ATTR_t attr = PAR_EL1_F0_get_ATTR(&par.f0);
	bool writeback = ((index_t)attr | (index_t)MAIR_ATTR_ALLOC_HINT_MASK) ==
			 (index_t)MAIR_ATTR_NORMAL_WB;
#if defined(ARCH_ARM_FEAT_MTE)
	writeback = writeback || (attr == MAIR_ATTR_TAGGED_NORMAL_WB);
#endif

	partition_phys_access_enable(va);

	if (compiler_unexpected(from_guest && !writeback)) {
		useraccess_clean_range((uint8_t *)va,
				       util_min(remaining, mapped_size));
	}

	size_t copied_size;
	if (from_guest) {
		copied_size = memscpy(hyp_buf, remaining, va, mapped_size);
	} else {
		copied_size = memscpy(va, mapped_size, hyp_buf, remaining);
	}

	if (compiler_unexpected(!from_guest && !writeback)) {
		useraccess_clean_invalidate_range((uint8_t *)va, copied_size);
	}

	partition_phys_access_disable(va);

	partition_phys_unmap(va, guest_pa, mapped_size);

	return copied_size;
}

static size_result_t
useraccess_copy_from_to_guest_va(gvaddr_t gvaddr, void *hvaddr, size_t size,
				 bool from_guest, bool force_access)
{
	error_t	 ret	   = OK;
	size_t	 remaining = size;
	gvaddr_t guest_va  = gvaddr;
	void	*hyp_buf   = hvaddr;

	assert(hyp_buf != NULL);
	assert(remaining != 0U);

	if (util_add_overflows((uintptr_t)hvaddr, size - 1U) ||
	    util_add_overflows(gvaddr, size - 1U)) {
		ret = ERROR_ADDR_OVERFLOW;
		goto out;
	}

	PAR_EL1_base_t saved_par =
		register_PAR_EL1_base_read_volatile_ordered(&asm_ordering);

	const size_t page_size	 = 4096U;
	size_t	     page_offset = gvaddr & (page_size - 1U);

	do {
		// Guest stage 2 lookups are in RCU read-side critical sections
		// so that unmap or access change operations can wait for them
		// to complete.
		rcu_read_start();

		if (from_guest || force_access) {
			__asm__ volatile("at S12E1R, %[guest_va];"
					 "isb                   ;"
					 : "+m"(asm_ordering)
					 : [guest_va] "r"(guest_va));
		} else {
			__asm__ volatile("at S12E1W, %[guest_va];"
					 "isb                   ;"
					 : "+m"(asm_ordering)
					 : [guest_va] "r"(guest_va));
		}

		PAR_EL1_t par = {
			.base = register_PAR_EL1_base_read_volatile_ordered(
				&asm_ordering),
		};

		if (compiler_expected(!PAR_EL1_base_get_F(&par.base))) {
			// No fault; copy to/from the translated PA
			size_t copied_size =
				useraccess_copy_from_to_translated_pa(
					par, guest_va, page_size, page_offset,
					from_guest, hyp_buf, remaining);
			assert(copied_size > 0U);
			guest_va += copied_size;
			hyp_buf = (void *)((uintptr_t)hyp_buf + copied_size);
			remaining -= copied_size;
			page_offset = 0U;
		} else if (!PAR_EL1_F1_get_S(&par.f1)) {
			// Stage 1 fault (reason is not distinguished here)
			ret = ERROR_ARGUMENT_INVALID;
		} else {
			// Stage 2 fault; return DENIED for permission faults,
			// ADDR_INVALID otherwise
			iss_da_ia_fsc_t fst = PAR_EL1_F1_get_FST(&par.f1);
			ret = ((fst == ISS_DA_IA_FSC_PERMISSION_1) ||
			       (fst == ISS_DA_IA_FSC_PERMISSION_2) ||
			       (fst == ISS_DA_IA_FSC_PERMISSION_3))
				      ? ERROR_DENIED
				      : ERROR_ADDR_INVALID;
		}

		rcu_read_finish();
	} while ((remaining != 0U) && (ret == OK));

	register_PAR_EL1_base_write_ordered(saved_par, &asm_ordering);

out:
	return (size_result_t){ .e = ret, .r = size - remaining };
}

size_result_t
useraccess_copy_from_guest_va(void *hyp_va, size_t hsize, gvaddr_t guest_va,
			      size_t gsize)
{
	size_result_t ret;
	bool force_access = false; // only write to guest_va requires this flag.
	if ((gsize == 0U) || (hsize < gsize)) {
		ret = size_result_error(ERROR_ARGUMENT_SIZE);
	} else {
		ret = useraccess_copy_from_to_guest_va(guest_va, hyp_va, gsize,
						       true, force_access);
	}
	return ret;
}

size_result_t
useraccess_copy_to_guest_va(gvaddr_t guest_va, size_t gsize, const void *hyp_va,
			    size_t hsize, bool force_access)
{
	size_result_t ret;
	if ((hsize == 0U) || (gsize < hsize)) {
		ret = size_result_error(ERROR_ARGUMENT_SIZE);
	} else {
		ret = useraccess_copy_from_to_guest_va(
			guest_va, (void *)(uintptr_t)hyp_va, hsize, false,
			force_access);
	}
	return ret;
}

static size_result_t
useraccess_copy_from_to_guest_ipa(addrspace_t *addrspace, vmaddr_t ipa,
				  void *hvaddr, size_t size, bool from_guest,
				  bool force_access, bool force_coherent)
{
	error_t ret    = OK;
	size_t	offset = 0U;

	if (util_add_overflows((uintptr_t)hvaddr, size - 1U) ||
	    util_add_overflows(ipa, size - 1U)) {
		ret = ERROR_ADDR_OVERFLOW;
		goto out;
	}

	while (offset < size) {
		paddr_t		     mapped_base;
		size_t		     mapped_size;
		pgtable_vm_memtype_t mapped_memtype;
		pgtable_access_t     mapped_vm_kernel_access;
		pgtable_access_t     mapped_vm_user_access;

		// Guest stage 2 lookups are in RCU read-side critical sections
		// so that unmap or access change operations can wait for them
		// to complete.
		rcu_read_start();

		if (!pgtable_vm_lookup(
			    &addrspace->vm_pgtable, ipa + offset, &mapped_base,
			    &mapped_size, &mapped_memtype,
			    &mapped_vm_kernel_access, &mapped_vm_user_access)) {
			rcu_read_finish();
			ret = ERROR_ADDR_INVALID;
			break;
		}

		if (!force_access &&
		    !pgtable_access_check(mapped_vm_kernel_access,
					  (from_guest ? PGTABLE_ACCESS_R
						      : PGTABLE_ACCESS_W))) {
			rcu_read_finish();
			ret = ERROR_DENIED;
			break;
		}

		size_t mapping_offset = (ipa + offset) & (mapped_size - 1U);
		mapped_base += mapping_offset;
		mapped_size -= mapping_offset;

		uint8_t *vm_addr = partition_phys_map(mapped_base, mapped_size);
		partition_phys_access_enable(vm_addr);

		uint8_t *hyp_va	  = (uint8_t *)hvaddr + offset;
		size_t	 hyp_size = size - offset;
		size_t	 copied_size;

		if (from_guest) {
			if (force_coherent ||
			    (mapped_memtype != PGTABLE_VM_MEMTYPE_NORMAL_WB)) {
				useraccess_clean_invalidate_range(
					vm_addr,
					util_min(mapped_size, hyp_size));
			}

			copied_size =
				memscpy(hyp_va, hyp_size, vm_addr, mapped_size);
		} else {
			copied_size =
				memscpy(vm_addr, mapped_size, hyp_va, hyp_size);

			if (force_coherent ||
			    (mapped_memtype != PGTABLE_VM_MEMTYPE_NORMAL_WB)) {
				useraccess_clean_range(vm_addr, copied_size);
			}
		}

		partition_phys_access_disable(vm_addr);
		partition_phys_unmap(vm_addr, mapped_base, mapped_size);

		rcu_read_finish();

		offset += copied_size;
	}

out:
	return (size_result_t){ .e = ret, .r = offset };
}

size_result_t
useraccess_copy_from_guest_ipa(addrspace_t *addrspace, void *hyp_va,
			       size_t hsize, vmaddr_t guest_ipa, size_t gsize,
			       bool force_access, bool force_coherent)
{
	size_result_t ret;
	if ((gsize == 0U) || (hsize < gsize)) {
		ret = size_result_error(ERROR_ARGUMENT_SIZE);
	} else {
		ret = useraccess_copy_from_to_guest_ipa(addrspace, guest_ipa,
							hyp_va, gsize, true,
							force_access,
							force_coherent);
	}
	return ret;
}

size_result_t
useraccess_copy_to_guest_ipa(addrspace_t *addrspace, vmaddr_t guest_ipa,
			     size_t gsize, const void *hyp_va, size_t hsize,
			     bool force_access, bool force_coherent)
{
	size_result_t ret;
	if ((hsize == 0U) || (gsize < hsize)) {
		ret = size_result_error(ERROR_ARGUMENT_SIZE);
	} else {
		ret = useraccess_copy_from_to_guest_ipa(
			addrspace, guest_ipa, (void *)(uintptr_t)hyp_va, hsize,
			false, force_access, force_coherent);
	}
	return ret;
}

```

`hyp/mem/useraccess/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

local_include
arch_source aarch64 useraccess.c

```

`hyp/mem/useraccess/include/useraccess.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// Copy in or out of VM memory by VA or IPA.
//
// These functions will all return ERROR_ADDR_INVALID if the specified address
// is not mapped in stage 2; ERROR_DENIED if the address is mapped but the
// requested access will permission-fault; ERROR_ARGUMENT_SIZE if the source
// size is zero or the destination size is smaller than the source size; or OK
// if the requested copy was completely successful. In case of errors, the data
// may have been partially copied.
//
// If the force_access argument is true, the copy will ignore the write access
// bit in both stages.
//
// The _va functions will return ERROR_ARGUMENT_INVALID if the access faults in
// stage 1. This includes permission faults.
//
// The _va functions will automatically perform cache maintenance if necessary
// to ensure that the access is coherent with the EL1 view of memory at the
// specified address (though aliases with different stage 1 attributes may
// remain incoherent).
//
// The _ipa functions will perform cache maintenance if the
// force_coherent argument is false and the stage 2 mapping forces a
// non-writeback cache attribute, or if the force_coherent argument is true and
// the stage 2 mapping does not force a writeback attribute; the latter
// behaviour is mostly useful for emulating I/O devices using read-only memory.

size_result_t
useraccess_copy_from_guest_va(void *hyp_va, size_t hsize, gvaddr_t guest_va,
			      size_t gsize);

size_result_t
useraccess_copy_to_guest_va(gvaddr_t guest_va, size_t gsize, const void *hyp_va,
			    size_t hsize, bool force_access);

size_result_t
useraccess_copy_from_guest_ipa(addrspace_t *addrspace, void *hyp_va,
			       size_t hsize, vmaddr_t guest_ipa, size_t gsize,
			       bool force_access, bool force_coherent);

size_result_t
useraccess_copy_to_guest_ipa(addrspace_t *addrspace, vmaddr_t guest_ipa,
			     size_t gsize, const void *hyp_va, size_t hsize,
			     bool force_access, bool force_coherent);

```

`hyp/misc/abort/abort.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

module abort

subscribe ipi_received[IPI_REASON_ABORT_STOP]() noreturn

subscribe scheduler_stop()
	require_preempt_disabled

```

`hyp/misc/abort/abort.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause


extend ipi_reason enumeration {
	abort_stop;
};

```

`hyp/misc/abort/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

interface abort

source abort.c
events abort.ev
types abort.tc

```

`hyp/misc/abort/src/abort.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// Note, do not call panic or assert here, they will recurse!

#include <hyptypes.h>

#include <abort.h>
#include <attributes.h>
#include <compiler.h>
#include <idle.h>
#include <ipi.h>
#include <log.h>
#include <preempt.h>
#include <thread.h>
#include <trace.h>

#include <events/abort.h>
#include <events/scheduler.h>
#include <events/thread.h>

#include <asm/event.h>

#include "event_handlers.h"

void NOINLINE
abort_handle_scheduler_stop(void)
{
	if (!idle_is_current()) {
		trigger_thread_save_state_event();
	}
}

noreturn void NOINLINE
abort_handle_ipi_received(void)
{
	preempt_disable();

	if (!idle_is_current()) {
		trigger_thread_save_state_event();
	}

	trigger_abort_kernel_remote_event();

	void *mem = NULL;
	while (1) {
		asm_event_wait(&mem);
	}
}

noreturn void NOINLINE COLD
abort(const char *str, abort_reason_t reason) LOCK_IMPL
{
	void *from  = __builtin_return_address(0);
	void *frame = __builtin_frame_address(0);

	from = __builtin_extract_return_addr(from);

	// Stop all cores and disable preemption
	trigger_scheduler_stop_event();

	TRACE_AND_LOG(ERROR, PANIC, "Abort: {:s} from PC {:#x}, FP {:#x}",
		      (register_t)(uintptr_t)str, (register_t)(uintptr_t)from,
		      (register_t)(uintptr_t)frame);

	trigger_abort_kernel_event(reason);

	while (1) {
		asm_event_wait(str);
	}
}

```

`hyp/misc/elf/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

interface elf

source elf_loader.c

```

`hyp/misc/elf/src/elf_loader.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>
#include <string.h>

#if ARCH_IS_64BIT
#define USE_ELF64
#endif

#include <compiler.h>
#include <elf.h>
#include <elf_loader.h>
#include <log.h>
#include <partition.h>
#include <pgtable.h>
#include <trace.h>
#include <util.h>

static const unsigned char *elf_ident = (const unsigned char *)EI_MAG_STR;

// Simple unoptimized non-terminated string comparison
static bool
str_equal(const unsigned char *s1, const unsigned char *s2, size_t n)
{
	bool	ret = true;
	index_t i;

	assert(n > 0);

	for (i = 0; i < n; i++) {
		if (s1[i] != s2[i]) {
			ret = false;
			break;
		}
	}

	return ret;
}

bool
elf_valid(void *elf_file, size_t max_size)
{
	index_t	  i;
	bool	  ret	   = false;
	uintptr_t area_end = (uintptr_t)elf_file + max_size;

	Elf_Ehdr *ehdr = (Elf_Ehdr *)(uintptr_t)elf_file;

	if ((area_end < (uintptr_t)elf_file) ||
	    util_add_overflows((uintptr_t)ehdr, sizeof(ehdr)) ||
	    (((uintptr_t)ehdr + sizeof(ehdr)) > area_end)) {
		goto out;
	}

	if (!str_equal(ehdr->e_ident, elf_ident, EI_MAG_SIZE)) {
		goto out;
	}
	if (ehdr->e_ident[EI_CLASS] != ELF_CLASS) {
		goto out;
	}
	if (ehdr->e_ident[EI_DATA] != ELF_DATA_2LSB) {
		goto out;
	}
	if (ehdr->e_ident[EI_VERSION] != EV_CURRENT) {
		goto out;
	}
	if (ehdr->e_ident[EI_OSABI] != 0U) {
		goto out;
	}
	if (ehdr->e_ident[EI_ABIVERSION] != 0U) {
		goto out;
	}
	if (ehdr->e_type != ET_DYN) {
		goto out;
	}
#if defined(ARCH_ARM)
	if (ehdr->e_machine != EM_AARCH64) {
		goto out;
	}
#else
#error unimplemented
#endif

	if (util_add_overflows((uintptr_t)elf_file, ehdr->e_phoff)) {
		goto out;
	}

	Elf_Phdr *phdr	= (Elf_Phdr *)((uintptr_t)elf_file + ehdr->e_phoff);
	Elf_Half  phnum = ehdr->e_phnum;

	if (ehdr->e_phentsize != sizeof(Elf_Phdr)) {
		goto out;
	}
	if (util_add_overflows((uintptr_t)phdr, sizeof(phdr) * phnum) ||
	    ((((uintptr_t)phdr + (sizeof(phdr) * phnum)) > area_end))) {
		goto out;
	}

	// Ensure there is at least one load segment
	for (i = 0; i < phnum; i++) {
		if (phdr[i].p_type == PT_LOAD) {
			ret = true;
			break;
		}
	}

out:
	return ret;
}

Elf_Addr
elf_get_entry(void *elf_file)
{
	Elf_Ehdr *ehdr = (Elf_Ehdr *)(uintptr_t)elf_file;

	return ehdr->e_entry;
}

count_t
elf_get_num_phdrs(void *elf_file)
{
	Elf_Ehdr *ehdr = (Elf_Ehdr *)(uintptr_t)elf_file;

	return ehdr->e_phnum;
}

Elf_Phdr *
elf_get_phdr(void *elf_file, count_t index)
{
	Elf_Ehdr *ehdr	= (Elf_Ehdr *)(uintptr_t)elf_file;
	Elf_Phdr *phdr	= (Elf_Phdr *)((uintptr_t)elf_file + ehdr->e_phoff);
	Elf_Half  phnum = ehdr->e_phnum;

	assert(index < phnum);

	return &phdr[index];
}

error_t
elf_load_phys(void *elf_file, size_t elf_max_size, paddr_t phys_base)
{
	error_t	  ret;
	index_t	  i;
	uintptr_t elf_base = (uintptr_t)elf_file;
	Elf_Ehdr *ehdr	   = (Elf_Ehdr *)(uintptr_t)elf_file;
	Elf_Phdr *phdr	   = (Elf_Phdr *)((uintptr_t)elf_file + ehdr->e_phoff);
	Elf_Half  phnum	   = ehdr->e_phnum;

	// Copy all segments to the requested memory addresses
	for (i = 0; i < phnum; i++) {
		Elf_Phdr *cur_phdr = &phdr[i];
		Elf_Word  type	   = cur_phdr->p_type;

		// FIXME:
		assert(type != PT_TLS);

		if (type != PT_LOAD) {
			continue;
		}

		if (cur_phdr->p_filesz > cur_phdr->p_memsz) {
			ret = ERROR_ARGUMENT_SIZE;
			goto out;
		}

		if (util_add_overflows(cur_phdr->p_paddr, cur_phdr->p_memsz) ||
		    util_add_overflows(phys_base,
				       cur_phdr->p_paddr + cur_phdr->p_memsz) ||
		    ((cur_phdr->p_offset + cur_phdr->p_filesz) >
		     elf_max_size)) {
			ret = ERROR_ARGUMENT_SIZE;
			goto out;
		}

		if (util_add_overflows(cur_phdr->p_offset, cur_phdr->p_memsz) ||
		    util_add_overflows(elf_base, cur_phdr->p_offset +
							 cur_phdr->p_memsz)) {
			ret = ERROR_ARGUMENT_SIZE;
			goto out;
		}

		uintptr_t seg_base = elf_base + cur_phdr->p_offset;
		paddr_t	  seg_dest = phys_base + cur_phdr->p_paddr;

		char *dest =
			(char *)partition_phys_map(seg_dest, cur_phdr->p_memsz);
		partition_phys_access_enable(dest);

		// copy elf segment data
		(void)memcpy(dest, (char *)seg_base, cur_phdr->p_filesz);
		// zero bss
		size_t bss_size = cur_phdr->p_memsz - cur_phdr->p_filesz;
		(void)memset_s(dest + cur_phdr->p_filesz, bss_size, 0,
			       bss_size);

		partition_phys_access_disable(dest);
		partition_phys_unmap(dest, seg_dest, cur_phdr->p_memsz);

		LOG(DEBUG, INFO, "Elf copied from {:#x} to {:#x} - size {:#x}",
		    seg_base, seg_dest, cur_phdr->p_filesz);
	}

	ret = OK;
out:
	return ret;
}

```

`hyp/misc/gpt/build.conf`:

```conf
# © 2022 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

interface gpt
source gpt.c gpt_tests.c
types gpt.tc
events gpt.ev

```

`hyp/misc/gpt/gpt.ev`:

```ev
// © 2022 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

module gpt

subscribe gpt_values_equal[GPT_TYPE_EMPTY]
	handler gpt_handle_empty_values_equal()

subscribe gpt_walk_callback[GPT_CALLBACK_RESERVED]
	handler gpt_handle_reserved_callback() noreturn

subscribe rcu_update[RCU_UPDATE_CLASS_GPT_FREE_LEVEL]
	handler gpt_handle_rcu_free_level(entry)

#if defined(UNIT_TESTS)
subscribe tests_init

subscribe tests_start

subscribe gpt_value_add_offset[GPT_TYPE_TEST_A, GPT_TYPE_TEST_B, GPT_TYPE_TEST_C]
	handler gpt_tests_add_offset

subscribe gpt_values_equal[GPT_TYPE_TEST_A, GPT_TYPE_TEST_B, GPT_TYPE_TEST_C]
	handler gpt_tests_values_equal(x, y)

subscribe gpt_walk_callback[GPT_CALLBACK_TEST]
	handler gpt_tests_callback(entry, base, size, arg)
#endif

```

`hyp/misc/gpt/gpt.tc`:

```tc
// © 2022 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

define GPT_MAX_SIZE_BITS constant type count_t = 52;
define GPT_MAX_SIZE constant size = 1 << GPT_MAX_SIZE_BITS;

define GPT_LEVEL_LOG2_BITS constant type count_t = 2;
define GPT_LEVEL_BITS constant type count_t = 1 << GPT_LEVEL_LOG2_BITS;
define GPT_LEVEL_ENTRIES constant type count_t = 1 << GPT_LEVEL_BITS;

define GPT_MAX_LEVELS constant type count_t =
		(GPT_MAX_SIZE_BITS + GPT_LEVEL_BITS - 1) / GPT_LEVEL_BITS;

define GPT_SHIFT_BITS constant type count_t = 6 - GPT_LEVEL_LOG2_BITS;
define GPT_TYPE_BITS constant type count_t =
		64 - GPT_MAX_SIZE_BITS - GPT_SHIFT_BITS;

extend gpt_type enumeration {
	level;
};

extend rcu_update_class enumeration {
	gpt_free_level;
};

define gpt_pte_info bitfield<64> {
	auto<GPT_MAX_SIZE_BITS>	guard	size;
	auto<GPT_SHIFT_BITS>	shifts	type count_t lsl(GPT_LEVEL_LOG2_BITS);
	auto<GPT_TYPE_BITS>	type	enumeration gpt_type;
};

define gpt_pte structure {
	info	bitfield gpt_pte_info;
	value	union gpt_value;
};

define gpt_level_non_atomic structure {
	entries		array(GPT_LEVEL_ENTRIES) structure gpt_pte;
};

// FIXME:
define gpt_level_atomic structure(aligned(16)) {
	entries		array(GPT_LEVEL_ENTRIES) structure gpt_pte(atomic);
	rcu_entry	structure rcu_entry(contained);
	partition	pointer object partition;
};

define gpt_level union {
	raw		uintptr;
	non_atomic	pointer structure gpt_level_non_atomic;
	atomic		pointer structure gpt_level_atomic;
};

extend gpt_value union {
	raw	uint64;
	level	union gpt_level;
};

// FIXME:
define gpt_root union(aligned(16)) {
	non_atomic	structure gpt_pte;
	atomic		structure gpt_pte(atomic);
};

extend gpt structure {
	root		union gpt_root;
	partition	pointer object partition;
	config		bitfield gpt_config;
	allowed_types	uregister;
};

define gpt_read_op enumeration {
	lookup;
	is_contiguous;
	walk;
	dump_range;
};

define gpt_read_data structure {
	entry	structure gpt_entry;
	base	size;
	size	size;
	cb	enumeration gpt_callback;
	arg	union gpt_arg;
};

define gpt_frame_info bitfield<64> {
	auto<GPT_MAX_SIZE_BITS>	addr	size;
	auto<GPT_SHIFT_BITS>	shifts	type count_t lsl(GPT_LEVEL_LOG2_BITS);
	auto<GPT_LEVEL_BITS>	index	type index_t;
	auto			dirty	bool;
};

define gpt_stack_frame structure {
	level	union gpt_level;
	info	bitfield gpt_frame_info;
};

define gpt_stack structure {
	depth	type count_t;
	frame	array(GPT_MAX_LEVELS) structure gpt_stack_frame;
};

#if defined(UNIT_TESTS)
extend gpt_type enumeration {
	test_a;
	test_b;
	test_c;
};

extend gpt_callback enumeration {
	test;
};

extend gpt_arg union {
	test	uregister;
};
#endif

```

`hyp/misc/gpt/src/gpt.c`:

```c
// © 2022 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>
#include <limits.h>
#include <string.h>

#include <hypcontainers.h>

#include <atomic.h>
#include <bitmap.h>
#include <compiler.h>
#include <gpt.h>
#include <log.h>
#include <object.h>
#include <panic.h>
#include <partition.h>
#include <rcu.h>
#include <trace.h>
#include <util.h>

#include <events/gpt.h>

#include "event_handlers.h"

#define SIZE_T_BITS (sizeof(size_t) * (size_t)CHAR_BIT)

static_assert(sizeof(gpt_value_t) <= sizeof(uint64_t),
	      "GPT value must not be larger than 64-bits!");
static_assert(GPT_TYPE__MAX < util_bit(GPT_TYPE_BITS),
	      "GPT type does not fit in PTE bitfield!");

bool
gpt_handle_empty_values_equal(void)
{
	return true;
}

void
gpt_handle_reserved_callback(void)
{
	panic("gpt: Reserved callback used");
}

static gpt_pte_t
gpt_pte_empty(void)
{
	return (gpt_pte_t){
		.info  = gpt_pte_info_default(),
		.value = { .raw = 0U },
	};
}

static size_t
get_max_size(gpt_config_t config)
{
	return util_bit(gpt_config_get_max_bits(&config));
}

static size_t
get_pte_addr(gpt_pte_t pte)
{
	size_t	guard  = gpt_pte_info_get_guard(&pte.info);
	count_t shifts = gpt_pte_info_get_shifts(&pte.info);

	return guard << shifts;
}

static size_t
get_pte_size(gpt_pte_t pte)
{
	return util_bit(gpt_pte_info_get_shifts(&pte.info));
}

static bool
guard_matching(gpt_pte_t pte, size_t addr)
{
	assert(gpt_pte_info_get_type(&pte.info) != GPT_TYPE_EMPTY);

	size_t	guard  = gpt_pte_info_get_guard(&pte.info);
	count_t shifts = gpt_pte_info_get_shifts(&pte.info);

	return (addr >> shifts) == guard;
}

static bool
entries_equal(gpt_entry_t a, gpt_entry_t b)
{
	return (a.type == b.type) &&
	       trigger_gpt_values_equal_event(a.type, a.value, b.value);
}

static gpt_pte_t
load_atomic_pte(_Atomic gpt_pte_t *p)
{
	return atomic_load_consume(p);
}

static void
store_atomic_pte(_Atomic gpt_pte_t *p, gpt_pte_t pte, bool init)
{
	if (init) {
		atomic_init(p, pte);
	} else {
		atomic_store_release(p, pte);
	}
}

static gpt_pte_t
load_root_pte(gpt_root_t *root, gpt_config_t config)
{
	gpt_pte_t pte;

	if (gpt_config_get_rcu_read(&config)) {
		pte = load_atomic_pte(&root->atomic);
	} else {
		pte = root->non_atomic;
	}

	return pte;
}

static gpt_pte_t
load_level_pte(gpt_config_t config, gpt_level_t level, index_t i)
{
	gpt_pte_t pte;

	if (gpt_config_get_rcu_read(&config)) {
		pte = load_atomic_pte(&level.atomic->entries[i]);
	} else {
		pte = level.non_atomic->entries[i];
	}

	return pte;
}

static void
store_root_pte(gpt_root_t *root, gpt_config_t config, gpt_pte_t pte, bool init)
{
	if (gpt_config_get_rcu_read(&config)) {
		store_atomic_pte(&root->atomic, pte, init);
	} else {
		root->non_atomic = pte;
	}
}

static void
store_level_pte(gpt_config_t config, gpt_level_t level, index_t i,
		gpt_pte_t pte, bool init)
{
	if (gpt_config_get_rcu_read(&config)) {
		store_atomic_pte(&level.atomic->entries[i], pte, init);
	} else {
		level.non_atomic->entries[i] = pte;
	}
}

static bool
entry_is_valid(gpt_t *gpt, gpt_entry_t entry)
{
	return (entry.type <= GPT_TYPE__MAX) &&
	       bitmap_isset(&gpt->allowed_types, (index_t)entry.type);
}

static bool
entry_is_valid_or_empty(gpt_t *gpt, gpt_entry_t entry)
{
	return entry_is_valid(gpt, entry) || (entry.type == GPT_TYPE_EMPTY);
}

static bool
pte_and_entry_equal(gpt_pte_t pte, size_t curr, gpt_entry_t entry)
{
	assert(guard_matching(pte, curr));

	size_t	    pte_addr  = get_pte_addr(pte);
	gpt_type_t  pte_type  = gpt_pte_info_get_type(&pte.info);
	gpt_value_t pte_value = pte.value;

	trigger_gpt_value_add_offset_event(pte_type, &pte_value,
					   curr - pte_addr);

	gpt_entry_t other = {
		.type  = pte_type,
		.value = pte_value,
	};

	return entries_equal(entry, other);
}

static bool
can_replace_pte(size_t curr, size_t rem, gpt_pte_t pte)
{
	size_t pte_addr = get_pte_addr(pte);
	size_t pte_size = get_pte_size(pte);

	assert(!guard_matching(pte, curr));

	// If the remaining region completely covers the range of the PTE, it is
	// safe to replace the PTE.
	return (curr <= pte_addr) && ((curr + rem) >= (pte_addr + pte_size));
}

static bool
pte_will_conflict(size_t curr, size_t rem, gpt_entry_t old, gpt_pte_t pte)
{
	bool ret = true;

	assert(!guard_matching(pte, curr));

	if (old.type != GPT_TYPE_EMPTY) {
		// We expected the GPT to not be empty at this point.
		goto out;
	}

	if (gpt_pte_info_get_type(&pte.info) == GPT_TYPE_LEVEL) {
		// We can only be sure of a conflict with a level if its entire
		// range is going to be replaced. Otherwise, we must traverse
		// the level first to be sure of a conflict.
		ret = can_replace_pte(curr, rem, pte);
		goto out;
	}

	size_t pte_addr = get_pte_addr(pte);

	// If the range to update overlaps with the PTE, we have a conflict.
	ret = (pte_addr >= curr) && (pte_addr < (curr + rem));

out:
	return ret;
}

static count_t
get_common_shifts(gpt_pte_t pte, size_t addr)
{
	count_t clz = compiler_clz(addr ^ get_pte_addr(pte));

	return (count_t)SIZE_T_BITS - util_balign_down(clz, GPT_LEVEL_BITS);
}

static index_t
get_level_index(count_t shifts, size_t addr)
{
	assert(shifts >= GPT_LEVEL_BITS);

	return (index_t)((addr >> (shifts - GPT_LEVEL_BITS)) &
			 util_mask(GPT_LEVEL_BITS));
}

static gpt_stack_frame_t *
get_curr_stack_frame(gpt_stack_t *stack)
{
	assert(stack->depth != 0U);

	index_t i = stack->depth - 1U;
	assert(i < GPT_MAX_LEVELS);

	return &stack->frame[i];
}

static count_t
get_max_entry_shifts(gpt_stack_t *stack)
{
	count_t shifts = GPT_MAX_SIZE_BITS;

	if (stack->depth != 0U) {
		gpt_stack_frame_t *frame = get_curr_stack_frame(stack);
		assert(frame != NULL);
		shifts = gpt_frame_info_get_shifts(&frame->info);
	}

	return shifts;
}

static count_t
get_max_possible_shifts(gpt_stack_t *stack, size_t curr, size_t rem)
{
	count_t shifts = get_max_entry_shifts(stack);

	assert(rem > 0U);

	if (curr != 0U) {
		count_t align_bits =
			util_balign_down(compiler_ctz(curr), GPT_LEVEL_BITS);
		shifts = util_min(shifts, align_bits);
	}

	if (util_bit(shifts) > rem) {
		shifts = util_balign_down(compiler_ctz(rem), GPT_LEVEL_BITS);
	}

	return shifts;
}

static gpt_level_t
get_level_from_pte(gpt_pte_t pte)
{
	gpt_level_t level = pte.value.level;
	assert(level.raw != 0U);

	return level;
}

static void
go_down_level(gpt_config_t config, gpt_stack_t *stack, size_t curr,
	      gpt_pte_t pte)
{
	gpt_level_t level = get_level_from_pte(pte);

	assert(guard_matching(pte, curr));

	stack->depth++;

	gpt_stack_frame_t *frame = get_curr_stack_frame(stack);
	assert(frame != NULL);

	count_t shifts = gpt_pte_info_get_shifts(&pte.info);
	assert(shifts >= GPT_LEVEL_BITS);

	size_t addr = get_pte_addr(pte);
	assert(addr < get_max_size(config));

	gpt_frame_info_t info = gpt_frame_info_default();
	gpt_frame_info_set_addr(&info, addr);
	gpt_frame_info_set_shifts(&info, shifts - GPT_LEVEL_BITS);

	frame->level = level;
	frame->info  = info;
}

static bool
check_ptes_consistent(gpt_pte_t a, gpt_pte_t b, size_t offset)
{
	bool ret = false;

	gpt_type_t type = gpt_pte_info_get_type(&a.info);
	if (type != gpt_pte_info_get_type(&b.info)) {
		goto out;
	}

	gpt_value_t x = a.value;
	gpt_value_t y = b.value;

	trigger_gpt_value_add_offset_event(type, &x, offset);

	ret = trigger_gpt_values_equal_event(type, x, y);

out:
	return ret;
}

static void
write_pte_to_level(gpt_root_t *root, gpt_config_t config, gpt_stack_t *stack,
		   gpt_pte_t pte)
{
	if (stack->depth == 0U) {
		store_root_pte(root, config, pte, false);
	} else {
		gpt_stack_frame_t *frame = get_curr_stack_frame(stack);
		assert(frame != NULL);

		index_t i = gpt_frame_info_get_index(&frame->info);
		assert(i < GPT_LEVEL_ENTRIES);

		store_level_pte(config, frame->level, i, pte, false);
		gpt_frame_info_set_dirty(&frame->info, true);
	}
}

rcu_update_status_t
gpt_handle_rcu_free_level(rcu_entry_t *entry)
{
	gpt_level_atomic_t *level =
		gpt_level_atomic_container_of_rcu_entry(entry);
	assert(level != NULL);
	assert(level->partition != NULL);

	(void)partition_free(level->partition, level, sizeof(*level));

	return rcu_update_status_default();
}

static void
free_level(gpt_config_t config, partition_t *partition, gpt_level_t level)
{
	if (gpt_config_get_rcu_read(&config)) {
		rcu_enqueue(&level.atomic->rcu_entry,
			    RCU_UPDATE_CLASS_GPT_FREE_LEVEL);
	} else {
		(void)partition_free(partition, level.non_atomic,
				     sizeof(gpt_level_non_atomic_t));
	}
}

static void
try_clean(gpt_root_t *root, gpt_config_t config, partition_t *partition,
	  gpt_stack_t *stack, gpt_level_t level, count_t entry_shifts)
{
	count_t	  filled_count	  = 0U;
	gpt_pte_t first_pte	  = gpt_pte_empty();
	gpt_pte_t last_filled_pte = gpt_pte_empty();
	bool	  can_merge	  = true;

	assert(partition != NULL);

	for (index_t i = 0U; i < GPT_LEVEL_ENTRIES; i++) {
		gpt_pte_t curr_pte = load_level_pte(config, level, i);

		if (gpt_pte_info_get_type(&curr_pte.info) == GPT_TYPE_EMPTY) {
			can_merge = false;
		} else {
			filled_count++;
			last_filled_pte = curr_pte;
			// Merging entries is only possible if they fill up
			// the entire level.
			if (gpt_pte_info_get_shifts(&curr_pte.info) !=
			    entry_shifts) {
				can_merge = false;
			}
		}

		if (can_merge) {
			if (i == 0U) {
				first_pte = curr_pte;
			} else {
				size_t offset = (size_t)i << entry_shifts;
				if (!check_ptes_consistent(first_pte, curr_pte,
							   offset)) {
					can_merge = false;
				}
			}
		} else if (filled_count > 1U) {
			break;
		} else {
			// We may still be able to clean, continue iterating.
		}
	}

	if (filled_count <= 1U) {
		// Either the level is empty, or the last filled
		// PTE is the only one in the level.
		write_pte_to_level(root, config, stack, last_filled_pte);
		free_level(config, partition, level);
	} else if (can_merge) {
		// All entries consistent, we can merge into one PTE.
		assert(filled_count == GPT_LEVEL_ENTRIES);
		count_t new_shifts = entry_shifts + GPT_LEVEL_BITS;
		size_t	new_guard  = get_pte_addr(first_pte) >> new_shifts;
		gpt_pte_info_set_guard(&first_pte.info, new_guard);
		gpt_pte_info_set_shifts(&first_pte.info, new_shifts);
		write_pte_to_level(root, config, stack, first_pte);
		free_level(config, partition, level);
	} else {
		// No cases where we can free the level, do nothing.
	}
}

static void
go_up_level(gpt_root_t *root, gpt_config_t config, partition_t *partition,
	    gpt_stack_t *stack, bool write)
{
	assert(stack->depth > 0U);

	gpt_stack_frame_t *frame = get_curr_stack_frame(stack);
	assert(frame != NULL);

	stack->depth--;

	if (write && gpt_frame_info_get_dirty(&frame->info)) {
		// FIXME: Do we need a better heuristic to determine if a
		// clean is required? We could maintain a count of filled
		// entries in each level, but do we want this additional
		// memory consumption?
		count_t shifts = gpt_frame_info_get_shifts(&frame->info);
		try_clean(root, config, partition, stack, frame->level, shifts);
	} else {
		assert(!gpt_frame_info_get_dirty(&frame->info));
	}
}

static gpt_pte_t
get_curr_pte(gpt_root_t *root, gpt_config_t config, partition_t *partition,
	     gpt_stack_t *stack, size_t curr, bool write)
{
	gpt_pte_t pte;

	while (stack->depth > 0U) {
		gpt_stack_frame_t *frame = get_curr_stack_frame(stack);
		assert(frame != NULL);

		count_t shifts = gpt_frame_info_get_shifts(&frame->info);
		size_t	addr   = gpt_frame_info_get_addr(&frame->info);
		assert(curr >= addr);

		index_t idx = (index_t)((curr - addr) >> shifts);
		if (idx < GPT_LEVEL_ENTRIES) {
			gpt_frame_info_set_index(&frame->info, idx);
			pte = load_level_pte(config, frame->level, idx);
			goto out;
		}

		go_up_level(root, config, partition, stack, write);
	}

	assert(stack->depth == 0U);

	pte = load_root_pte(root, config);

out:
	return pte;
}

static void
update_curr_pte(gpt_root_t *root, gpt_config_t config, gpt_stack_t *stack,
		size_t addr, count_t shifts, gpt_type_t type, gpt_value_t value)
{
	gpt_pte_t new_pte = gpt_pte_empty();

	if (type != GPT_TYPE_EMPTY) {
		gpt_pte_info_set_guard(&new_pte.info, addr >> shifts);
		gpt_pte_info_set_shifts(&new_pte.info, shifts);
		gpt_pte_info_set_type(&new_pte.info, type);
		new_pte.value = value;
	}

	write_pte_to_level(root, config, stack, new_pte);
}

static void
split_pte_and_fill_level(gpt_config_t config, gpt_level_t level,
			 gpt_pte_t old_pte, count_t shifts)
{
	size_t	    pte_addr = get_pte_addr(old_pte);
	size_t	    pte_size = util_bit(shifts);
	gpt_type_t  type     = gpt_pte_info_get_type(&old_pte.info);
	gpt_value_t value    = old_pte.value;

	gpt_pte_t new_pte = old_pte;
	gpt_pte_info_set_shifts(&new_pte.info, shifts);

	for (index_t i = 0U; i < GPT_LEVEL_ENTRIES; i++) {
		gpt_pte_info_set_guard(&new_pte.info, pte_addr >> shifts);
		new_pte.value = value;

		store_level_pte(config, level, i, new_pte, true);

		pte_addr += pte_size;

		trigger_gpt_value_add_offset_event(type, &value, pte_size);
	}
}

static error_t
allocate_level(gpt_root_t *root, gpt_config_t config, partition_t *partition,
	       gpt_stack_t *stack, gpt_pte_t old_pte, count_t new_shifts,
	       bool fill)
{
	error_t	    err = OK;
	gpt_level_t level;
	size_t	    alloc_size;
	size_t	    alloc_align;

	if (gpt_config_get_rcu_read(&config)) {
		alloc_size  = sizeof(gpt_level_atomic_t);
		alloc_align = alignof(gpt_level_atomic_t);
	} else {
		alloc_size  = sizeof(gpt_level_non_atomic_t);
		alloc_align = alignof(gpt_level_non_atomic_t);
	}

	void_ptr_result_t alloc_ret =
		partition_alloc(partition, alloc_size, alloc_align);
	if (alloc_ret.e != OK) {
		err = alloc_ret.e;
		goto out;
	}

	if (gpt_config_get_rcu_read(&config)) {
		level.atomic  = (gpt_level_atomic_t *)alloc_ret.r;
		*level.atomic = (gpt_level_atomic_t){ .partition = partition };
	} else {
		level.non_atomic  = (gpt_level_non_atomic_t *)alloc_ret.r;
		*level.non_atomic = (gpt_level_non_atomic_t){ 0 };
	}

	size_t	    addr       = get_pte_addr(old_pte);
	count_t	    old_shifts = gpt_pte_info_get_shifts(&old_pte.info);
	gpt_value_t value      = { .level = level };

	if (fill) {
		assert(old_shifts == new_shifts);
		split_pte_and_fill_level(config, level, old_pte,
					 new_shifts - GPT_LEVEL_BITS);
	} else {
		assert(old_shifts < new_shifts);
		index_t i = get_level_index(new_shifts, addr);
		store_level_pte(config, level, i, old_pte, true);
	}

	update_curr_pte(root, config, stack, addr, new_shifts, GPT_TYPE_LEVEL,
			value);

out:
	return err;
}

static void
free_all_levels(gpt_config_t config, partition_t *partition, gpt_pte_t pte)
{
	gpt_level_t levels[GPT_MAX_LEVELS]    = { get_level_from_pte(pte) };
	index_t	    level_idx[GPT_MAX_LEVELS] = { 0 };

	count_t depth = 1U;
	while (depth > 0U) {
		index_t i = depth - 1U;
		assert(i < GPT_MAX_LEVELS);

		gpt_level_t level = levels[i];
		assert(level.raw != 0U);

		index_t j = level_idx[i];
		if (j == GPT_LEVEL_ENTRIES) {
			free_level(config, partition, level);
			levels[i].raw = 0U;
			level_idx[i]  = 0U;
			depth--;
			continue;
		}

		gpt_pte_t curr_pte = load_level_pte(config, level, j);
		if (gpt_pte_info_get_type(&curr_pte.info) == GPT_TYPE_LEVEL) {
			assert(i < (GPT_MAX_LEVELS - 1U));
			levels[i + 1U] = get_level_from_pte(curr_pte);
			depth++;
		}

		level_idx[i]++;
	}
}

static size_t
update_curr_pte_and_get_size(gpt_root_t *root, gpt_config_t config,
			     gpt_stack_t *stack, size_t curr, size_t rem,
			     gpt_entry_t new)
{
	count_t shifts = get_max_possible_shifts(stack, curr, rem);

	update_curr_pte(root, config, stack, curr, shifts, new.type, new.value);

	return util_bit(shifts);
}

static size_t
get_next_pte_base(gpt_stack_t *stack, size_t curr)
{
	count_t shifts = get_max_entry_shifts(stack);

	return util_p2align_down(curr, shifts) + util_bit(shifts);
}

static size_result_t
handle_write(gpt_root_t *root, gpt_config_t config, partition_t *partition,
	     gpt_stack_t *stack, size_t curr, size_t rem, gpt_entry_t old,
	     gpt_entry_t new, bool match)
{
	size_result_t ret = size_result_ok(0U);
	gpt_pte_t     pte =
		get_curr_pte(root, config, partition, stack, curr, true);
	gpt_type_t type = gpt_pte_info_get_type(&pte.info);

	if (type == GPT_TYPE_EMPTY) {
		// Empty PTEs don't have a valid guard, which is why we can't
		// perform a guard check here.
		if (match && (old.type != GPT_TYPE_EMPTY)) {
			// We expected a non-empty PTE at this point.
			ret.e = ERROR_BUSY;
		} else if (new.type == GPT_TYPE_EMPTY) {
			// No need to update an already empty entry, skip to the
			// next entry.
			ret.r = get_next_pte_base(stack, curr) - curr;
		} else {
			// We can safely update the PTE.
			ret.r = update_curr_pte_and_get_size(
				root, config, stack, curr, rem, new);
		}
	} else if (!guard_matching(pte, curr)) {
		// The current address isn't mapped in the GPT.
		if (!match && can_replace_pte(curr, rem, pte)) {
			// It is safe to overwrite this PTE.
			ret.r = update_curr_pte_and_get_size(
				root, config, stack, curr, rem, new);
			// If the old PTE was a level, we need to ensure it
			// and all levels below it are freed.
			if (type == GPT_TYPE_LEVEL) {
				free_all_levels(config, partition, pte);
			}
		} else if (match && pte_will_conflict(curr, rem, old, pte)) {
			// We either expected the GPT to be filled at the
			// current offset, or to be empty at the PTE's offset.
			ret.e = ERROR_BUSY;
		} else {
			// Allocate a new common level and retry.
			count_t shifts = get_common_shifts(pte, curr);
			ret.e = allocate_level(root, config, partition, stack,
					       pte, shifts, false);
		}
	} else if (type == GPT_TYPE_LEVEL) {
		// Guard matches for this level, traverse down it.
		go_down_level(config, stack, curr, pte);
	} else if (match && !pte_and_entry_equal(pte, curr, old)) {
		// We aren't matching the expected old value.
		ret.e = ERROR_BUSY;
	} else {
		// The PTE has an old value, which we may not be fully
		// replacing. Determine if we need to split the PTE into a new
		// level or not.
		count_t old_shifts = gpt_pte_info_get_shifts(&pte.info);
		count_t new_shifts = get_max_possible_shifts(stack, curr, rem);
		if (old_shifts > new_shifts) {
			assert(old_shifts >= GPT_LEVEL_BITS);
			// Split entry into a new level and retry.
			ret.e = allocate_level(root, config, partition, stack,
					       pte, old_shifts, true);
		} else if ((old_shifts < new_shifts) && match) {
			// The old PTE doesn't cover the entire region that we
			// want to update, which means there is a mismatch.
			ret.e = ERROR_BUSY;
		} else {
			// Either the shifts are matching or we don't care about
			// the old entry, we can safely update the PTE.
			ret.r = update_curr_pte_and_get_size(
				root, config, stack, curr, rem, new);
		}
	}

	return ret;
}

static error_t
do_walk_callback(gpt_read_data_t *data)
{
	error_t err = OK;

	if (data->size > 0U) {
		err = trigger_gpt_walk_callback_event(data->cb, data->entry,
						      data->base, data->size,
						      data->arg);
	}

	return err;
}

static void
log_range(size_t base, size_t size, gpt_entry_t entry)
{
	if ((entry.type != GPT_TYPE_EMPTY) && (size > 0U)) {
		LOG(DEBUG, INFO, "[{:#x}, {:#x}]: type {:d}, value {:#x}", base,
		    size, entry.type, entry.value.raw);
	}
}

static size_result_t
handle_read(gpt_root_t *root, gpt_config_t config, gpt_stack_t *stack,
	    size_t curr, size_t rem, gpt_read_op_t op, gpt_read_data_t *data)
{
	size_result_t ret = size_result_ok(0U);
	gpt_pte_t   pte	 = get_curr_pte(root, config, NULL, stack, curr, false);
	gpt_type_t  type = gpt_pte_info_get_type(&pte.info);
	gpt_value_t value = { .raw = 0U };

	size_t pte_addr = get_pte_addr(pte);
	size_t pte_size = get_pte_size(pte);
	size_t end_addr;

	if (type == GPT_TYPE_EMPTY) {
		// The empty range ends at the next PTE.
		end_addr = get_next_pte_base(stack, curr);
	} else if (!guard_matching(pte, curr)) {
		// The current address isn't mapped in the GPT, so we treat it
		// as empty.
		type = GPT_TYPE_EMPTY;
		if (curr < pte_addr) {
			// The range ends at the start of the PTE.
			end_addr = pte_addr;
		} else {
			// The range ends at the next PTE.
			end_addr = get_next_pte_base(stack, curr);
		}
	} else if (type == GPT_TYPE_LEVEL) {
		go_down_level(config, stack, curr, pte);
		goto out;
	} else {
		end_addr = util_balign_down(curr + pte_size, pte_size);
		value	 = pte.value;

		trigger_gpt_value_add_offset_event(type, &value,
						   curr - pte_addr);
	}

	size_t size = util_min(end_addr - curr, rem);

	gpt_entry_t curr_entry = {
		.type  = type,
		.value = value,
	};

	gpt_entry_t cmp_entry = data->entry;

	trigger_gpt_value_add_offset_event(cmp_entry.type, &cmp_entry.value,
					   data->size);

	bool equal = entries_equal(curr_entry, cmp_entry);
	if (equal) {
		data->size += size;
	} else {
		if (op == GPT_READ_OP_LOOKUP) {
			if (data->base == curr) {
				data->entry = curr_entry;
				data->size  = size;
			} else {
				ret.e = ERROR_FAILURE;
			}
		} else if (op == GPT_READ_OP_IS_CONTIGUOUS) {
			ret.e = ERROR_FAILURE;
		} else if (op == GPT_READ_OP_WALK) {
			ret.e = do_walk_callback(data);
			if (curr_entry.type == cmp_entry.type) {
				data->base  = curr;
				data->size  = size;
				data->entry = curr_entry;
			} else {
				data->base = curr + size;
				data->size = 0U;
			}
		} else if (op == GPT_READ_OP_DUMP_RANGE) {
			log_range(data->base, data->size, data->entry);
			data->entry = curr_entry;
			data->base  = curr;
			data->size  = size;
		} else {
			panic("gpt: Invalid read operation");
		}
	}

	ret.r = size;

out:
	return ret;
}

static size_result_t
gpt_do_write(gpt_t *gpt, size_t base, size_t size, gpt_entry_t old,
	     gpt_entry_t new, bool match)
{
	size_result_t ret	= size_result_ok(0U);
	gpt_root_t   *root	= &gpt->root;
	gpt_config_t  config	= gpt->config;
	partition_t  *partition = gpt->partition;

	gpt_stack_t stack;
	stack.depth = 0U;

	gpt_entry_t x = old;
	gpt_entry_t y = new;

	size_t offset = 0U;
	while ((ret.e == OK) && (offset < size)) {
		ret = handle_write(root, config, partition, &stack,
				   base + offset, size - offset, x, y, match);
		if ((ret.e == OK) && (ret.r != 0U)) {
			offset += ret.r;
			trigger_gpt_value_add_offset_event(x.type, &x.value,
							   ret.r);
			trigger_gpt_value_add_offset_event(y.type, &y.value,
							   ret.r);
		}
	}

	ret.r = offset;

	// Unwind the GPT stack to finish any required cleanup.
	while (stack.depth > 0U) {
		go_up_level(root, config, partition, &stack, true);
	}

	return ret;
}

static error_t
gpt_write(gpt_t *gpt, size_t base, size_t size, gpt_entry_t old,
	  gpt_entry_t new, bool match)
{
	size_result_t ret;

	assert(gpt != NULL);

	if ((size == 0U) || util_add_overflows(base, size - 1U)) {
		ret = size_result_error(ERROR_ARGUMENT_INVALID);
		goto out;
	}

	if ((base + size - 1U) > (get_max_size(gpt->config) - 1U)) {
		ret = size_result_error(ERROR_ARGUMENT_SIZE);
		goto out;
	}

	assert(entry_is_valid_or_empty(gpt, old));
	assert(entry_is_valid_or_empty(gpt, new));

	ret = gpt_do_write(gpt, base, size, old, new, match);
	if ((ret.e != OK) && (ret.r != 0U)) {
		size_result_t revert =
			gpt_do_write(gpt, base, ret.r, new, old, true);
		if (revert.e != OK) {
			panic("gpt: Failed to revert write!");
		}
	}

out:
	return ret.e;
}

static gpt_entry_t
gpt_entry_empty(void)
{
	return (gpt_entry_t){
		.type  = GPT_TYPE_EMPTY,
		.value = { .raw = 0U },
	};
}

static error_t
gpt_read(gpt_t *gpt, size_t base, size_t size, gpt_read_op_t op,
	 gpt_read_data_t *data)
{
	size_result_t ret    = size_result_ok(0U);
	gpt_root_t   *root   = &gpt->root;
	gpt_config_t  config = gpt->config;

	if ((size == 0U) || util_add_overflows(base, size - 1U)) {
		ret.e = ERROR_ARGUMENT_INVALID;
		goto out;
	}

	if ((base + size - 1U) > (get_max_size(gpt->config) - 1U)) {
		ret.e = ERROR_ARGUMENT_SIZE;
		goto out;
	}

	assert(entry_is_valid_or_empty(gpt, data->entry));

	gpt_stack_t stack;
	stack.depth = 0U;

	size_t offset = 0U;
	while ((ret.e == OK) && (offset < size)) {
		ret = handle_read(root, config, &stack, base + offset,
				  size - offset, op, data);
		offset += ret.r;
	}

out:
	return ret.e;
}

error_t
gpt_init(gpt_t *gpt, partition_t *partition, gpt_config_t config,
	 register_t allowed_types)
{
	error_t err = OK;

	if (gpt_config_get_max_bits(&config) > GPT_MAX_SIZE_BITS) {
		err = ERROR_ARGUMENT_INVALID;
		goto out;
	}

	if (bitmap_isset(&allowed_types, (index_t)GPT_TYPE_EMPTY) ||
	    bitmap_isset(&allowed_types, (index_t)GPT_TYPE_LEVEL) ||
	    ((allowed_types & ~util_mask((index_t)GPT_TYPE__MAX + 1U)) != 0U)) {
		err = ERROR_ARGUMENT_INVALID;
		goto out;
	}

	store_root_pte(&gpt->root, config, gpt_pte_empty(), true);

	gpt->partition	   = object_get_partition_additional(partition);
	gpt->config	   = config;
	gpt->allowed_types = allowed_types;

out:
	return err;
}

void
gpt_destroy(gpt_t *gpt)
{
	gpt_clear_all(gpt);
	object_put_partition(gpt->partition);
}

error_t
gpt_insert(gpt_t *gpt, size_t base, size_t size, gpt_entry_t entry,
	   bool expect_empty)
{
	error_t err;

	if (entry_is_valid(gpt, entry)) {
		err = gpt_write(gpt, base, size, gpt_entry_empty(), entry,
				expect_empty);
	} else {
		err = ERROR_ARGUMENT_INVALID;
	}

	return err;
}

error_t
gpt_update(gpt_t *gpt, size_t base, size_t size, gpt_entry_t old_entry,
	   gpt_entry_t new_entry)
{
	error_t err;

	if (entry_is_valid(gpt, old_entry) && entry_is_valid(gpt, new_entry)) {
		err = gpt_write(gpt, base, size, old_entry, new_entry, true);
	} else {
		err = ERROR_ARGUMENT_INVALID;
	}

	return err;
}

error_t
gpt_remove(gpt_t *gpt, size_t base, size_t size, gpt_entry_t entry)
{
	error_t err;

	if (entry_is_valid(gpt, entry)) {
		err = gpt_write(gpt, base, size, entry, gpt_entry_empty(),
				true);
	} else {
		err = ERROR_ARGUMENT_INVALID;
	}

	return err;
}

error_t
gpt_clear(gpt_t *gpt, size_t base, size_t size)
{
	return gpt_write(gpt, base, size, gpt_entry_empty(), gpt_entry_empty(),
			 false);
}

void
gpt_clear_all(gpt_t *gpt)
{
	error_t err = gpt_clear(gpt, 0U, get_max_size(gpt->config));
	assert(err == OK);
}

bool
gpt_is_empty(gpt_t *gpt)
{
	gpt_pte_t pte = load_root_pte(&gpt->root, gpt->config);

	return gpt_pte_info_get_type(&pte.info) == GPT_TYPE_EMPTY;
}

gpt_lookup_result_t
gpt_lookup(gpt_t *gpt, size_t base, size_t max_size)
{
	gpt_read_data_t read = { .base = base };

	error_t err = gpt_read(gpt, base, max_size, GPT_READ_OP_LOOKUP, &read);
	assert((err == OK) || (err == ERROR_FAILURE));

	return (gpt_lookup_result_t){
		.entry = read.entry,
		.size  = read.size,
	};
}

bool
gpt_is_contiguous(gpt_t *gpt, size_t base, size_t size, gpt_entry_t entry)
{
	bool		ret;
	gpt_read_data_t read = {
		.entry = entry,
	};

	if (entry_is_valid(gpt, entry)) {
		ret = gpt_read(gpt, base, size, GPT_READ_OP_IS_CONTIGUOUS,
			       &read) == OK;
	} else {
		ret = false;
	}

	return ret;
}

#if defined(UNIT_TESTS)
error_t
gpt_walk(gpt_t *gpt, size_t base, size_t size, gpt_type_t type,
	 gpt_callback_t callback, gpt_arg_t arg)
{
	error_t		err;
	gpt_read_data_t read = {
		.entry = { .type = type },
		.cb    = callback,
		.arg   = arg,
	};

	if ((callback < GPT_CALLBACK__MIN) || (callback > GPT_CALLBACK__MAX) ||
	    (callback == GPT_CALLBACK_RESERVED)) {
		err = ERROR_ARGUMENT_INVALID;
		goto out;
	}

	if (entry_is_valid(gpt, read.entry)) {
		err = gpt_read(gpt, base, size, GPT_READ_OP_WALK, &read);
		if (err == OK) {
			err = do_walk_callback(&read);
		}
	} else {
		err = ERROR_ARGUMENT_INVALID;
	}

out:
	return err;
}
#endif

void
gpt_dump_ranges(gpt_t *gpt)
{
	gpt_read_data_t read = { .base = 0U, .size = 0U };

	LOG(DEBUG, INFO, "Dumping ranges of GPT {:#x}", (register_t)gpt);

	error_t err = gpt_read(gpt, 0U, get_max_size(gpt->config),
			       GPT_READ_OP_DUMP_RANGE, &read);
	assert(err == OK);

	log_range(read.base, read.size, read.entry);
}

void
gpt_dump_levels(gpt_t *gpt)
{
	gpt_root_t  *root   = &gpt->root;
	gpt_config_t config = gpt->config;

	LOG(DEBUG, INFO, "Dumping levels of GPT {:#x}", (register_t)gpt);

	gpt_stack_t stack;
	stack.depth = 0U;

	size_t curr = 0U;
	while (curr < get_max_size(config)) {
		gpt_pte_t pte =
			get_curr_pte(root, config, NULL, &stack, curr, false);
		count_t entry_shifts = get_max_entry_shifts(&stack);

		if (!util_is_p2aligned(curr, entry_shifts)) {
			// We have already logged this PTE, go to the next
			// entry.
			curr = util_p2align_up(curr, entry_shifts);
			continue;
		}

		size_t	    guard  = gpt_pte_info_get_guard(&pte.info);
		count_t	    shifts = gpt_pte_info_get_shifts(&pte.info);
		gpt_type_t  type   = gpt_pte_info_get_type(&pte.info);
		gpt_value_t value  = pte.value;

		LOG(DEBUG, INFO, "{:d} {:#x} {:d} {:d} {:#x}", stack.depth,
		    guard, shifts, type, value.raw);

		if (type == GPT_TYPE_LEVEL) {
			curr = get_pte_addr(pte);
			go_down_level(config, &stack, curr, pte);
		} else {
			curr += util_bit(entry_shifts);
		}
	}
}

```

`hyp/misc/gpt/src/gpt_tests.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#if defined(UNIT_TESTS)

#include <assert.h>
#include <hyptypes.h>

#include <bitmap.h>
#include <compiler.h>
#include <cpulocal.h>
#include <gpt.h>
#include <log.h>
#include <partition_init.h>
#include <preempt.h>
#include <trace.h>
#include <util.h>

#include "event_handlers.h"

static gpt_t gpt;

static gpt_entry_t
test_entry_init(gpt_type_t type, uint64_t value)
{
	return (gpt_entry_t){
		.type  = type,
		.value = { .raw = value },
	};
}

void
gpt_tests_add_offset(gpt_type_t type, gpt_value_t *value, size_t offset)
{
	value->raw += (type != GPT_TYPE_TEST_C) ? offset : (offset * 2);
}

bool
gpt_tests_values_equal(gpt_value_t x, gpt_value_t y)
{
	return x.raw == y.raw;
}

error_t
gpt_tests_callback(gpt_entry_t entry, size_t base, size_t size, gpt_arg_t arg)
{
	LOG(DEBUG, INFO,
	    "GPT callback: t {:d}, v {:#x}, [{:#x}, {:#x}], arg {:#x}",
	    entry.type, entry.value.raw, base, size, arg.test);

	return OK;
}

void
gpt_handle_tests_init(void)
{
	partition_t *partition = partition_get_root();
	assert(partition != NULL);

	gpt_config_t config = gpt_config_default();
	gpt_config_set_max_bits(&config, GPT_MAX_SIZE_BITS);

	register_t types = 0U;
	bitmap_set(&types, GPT_TYPE_TEST_A);
	bitmap_set(&types, GPT_TYPE_TEST_B);
	bitmap_set(&types, GPT_TYPE_TEST_C);

	error_t err = gpt_init(&gpt, partition, config, types);
	assert(err == OK);
}

bool
gpt_handle_tests_start(void)
{
	error_t err;

	preempt_disable();

	if (cpulocal_get_index() != 0U) {
		goto out;
	}

	assert(gpt_is_empty(&gpt));

	size_t	    base, size;
	gpt_entry_t e1, e2;

	base = 0x80000000U;
	size = 0x70000;
	e1   = test_entry_init(GPT_TYPE_TEST_A, base);

	err = gpt_insert(&gpt, base, size, e1, true);
	assert(err == OK);

	base = 0x80001000U;
	size = 0x4500;
	e1   = test_entry_init(GPT_TYPE_TEST_A, base);
	e2   = test_entry_init(GPT_TYPE_TEST_A, 0x900000);

	err = gpt_update(&gpt, base, size, e1, e2);
	assert(err == OK);

	base = 0x80020010U;
	size = 0x3;
	e2   = test_entry_init(GPT_TYPE_TEST_B, base);

	err = gpt_insert(&gpt, base, size, e2, false);
	assert(err == OK);

	base = 0x80040400U;
	size = 3;
	e1   = test_entry_init(GPT_TYPE_TEST_A, 0x80040400U);
	e2   = test_entry_init(GPT_TYPE_TEST_C, 0x400);

	err = gpt_update(&gpt, base, size, e1, e2);
	assert(err == OK);

	base = 0x80055555;
	size = 1234;
	e1   = test_entry_init(GPT_TYPE_TEST_A, 0x80055555);

	err = gpt_remove(&gpt, base, size, e1);
	assert(err == OK);

	gpt_dump_ranges(&gpt);

	base = 0x80000050;
	size = 0x20;
	e1   = test_entry_init(GPT_TYPE_TEST_A, 0x80000050);

	assert(!gpt_is_empty(&gpt));

	bool ret = gpt_is_contiguous(&gpt, base, size, e1);
	assert(ret);

	gpt_lookup_result_t lookup = gpt_lookup(&gpt, 0x8, 1U);
	LOG(DEBUG, INFO, "Lookup returned: {:d} {:#x} ({:#x})",
	    lookup.entry.type, lookup.entry.value.raw, lookup.size);

	lookup = gpt_lookup(&gpt, 0x80040001, 2);
	LOG(DEBUG, INFO, "Lookup returned: {:d} {:#x} ({:#x})",
	    lookup.entry.type, lookup.entry.value.raw, lookup.size);

	lookup = gpt_lookup(&gpt, 0x80050006, 0x20000);
	LOG(DEBUG, INFO, "Lookup returned: {:d} {:#x} ({:#x})",
	    lookup.entry.type, lookup.entry.value.raw, lookup.size);

	gpt_arg_t arg;

	base	 = 0x80000001;
	size	 = 0x6f000;
	arg.test = 0xfeed;

	err = gpt_walk(&gpt, base, size, GPT_TYPE_TEST_A, GPT_CALLBACK_TEST,
		       arg);
	assert(err == OK);

	base	 = 0x80040200U;
	size	 = 0x800;
	arg.test = 0xbeef;

	err = gpt_walk(&gpt, base, size, GPT_TYPE_TEST_C, GPT_CALLBACK_TEST,
		       arg);
	assert(err == OK);

	base = 0x100100U;
	size = 0x1;
	e1   = test_entry_init(GPT_TYPE_TEST_A, base);

	err = gpt_insert(&gpt, base, size, e1, false);
	assert(err == OK);

	base = 0x100300U;
	size = 0x1;
	e1   = test_entry_init(GPT_TYPE_TEST_A, base);

	err = gpt_insert(&gpt, base, size, e1, false);
	assert(err == OK);

	base = 0x100100U;
	size = 0x1;
	e1   = test_entry_init(GPT_TYPE_TEST_A, base);

	err = gpt_remove(&gpt, base, size, e1);
	assert(err == OK);

	gpt_dump_ranges(&gpt);

	// Partially invalid update.
	base = 0x80030000U;
	size = 0x50000;
	e1   = test_entry_init(GPT_TYPE_TEST_A, base);
	e2   = test_entry_init(GPT_TYPE_TEST_B, base);

	err = gpt_update(&gpt, base, size, e1, e2);
	assert(err != OK);

	size = 0x10;

	err = gpt_update(&gpt, base, size, e1, e2);
	assert(err == OK);

	gpt_dump_ranges(&gpt);

	// Attempt to insert invalid type.
	e1 = test_entry_init(GPT_TYPE_LEVEL, 0x213123123123);

	err = gpt_insert(&gpt, 0x919100123f23, 0x1012301230, e1, false);
	assert(err != OK);

	base = 0x70000000U;
	size = 0x20000000U;
	e1   = test_entry_init(GPT_TYPE_TEST_B, 0x50000000U);

	err = gpt_insert(&gpt, base, size, e1, false);
	assert(err == OK);

	gpt_dump_levels(&gpt);

	err = gpt_clear(&gpt, 0U, 0x100000000U);
	assert(err == OK);

	assert(gpt_is_empty(&gpt));

	base = 0U;
	size = 1U;
	e1   = test_entry_init(GPT_TYPE_TEST_A, base);

	err = gpt_insert(&gpt, base, size, e1, false);
	assert(err == OK);

	for (index_t i = 0U; i < GPT_MAX_SIZE_BITS; i += GPT_LEVEL_BITS) {
		base = util_bit(i);
		size = 1U;
		e1   = test_entry_init(GPT_TYPE_TEST_A, base);

		err = gpt_insert(&gpt, base, size, e1, false);
		assert(err == OK);
	}

	gpt_dump_ranges(&gpt);

	for (index_t i = 0U; i < GPT_MAX_SIZE_BITS; i += GPT_LEVEL_BITS) {
		base = util_bit(i);
		size = 1U;
		e1   = test_entry_init(GPT_TYPE_TEST_A, base);

		err = gpt_remove(&gpt, base, size, e1);
		assert(err == OK);
	}

	gpt_clear_all(&gpt);

	for (index_t i = 0U; i < GPT_LEVEL_ENTRIES; i++) {
		base = 0xffff00000000 + (i << 8);
		size = 1U << 8;
		e1   = test_entry_init(GPT_TYPE_TEST_A, base);

		err = gpt_insert(&gpt, base, size, e1, true);
		assert(err == OK);
	}

	gpt_dump_levels(&gpt);

	e1 = test_entry_init(GPT_TYPE_TEST_A, 1U);

	err = gpt_insert(&gpt, 0x1000, 1U, e1, false);
	assert(err == OK);

	err = gpt_insert(&gpt, 0x1002, 1U, e1, false);
	assert(err == OK);

	err = gpt_insert(&gpt, 0x1002, 1U, e1, false);
	assert(err == OK);

	e2 = test_entry_init(GPT_TYPE_TEST_B, 1U);

	err = gpt_insert(&gpt, 0x1010, 1U, e2, false);
	assert(err == OK);

	gpt_dump_levels(&gpt);

	e1 = test_entry_init(GPT_TYPE_TEST_C, 2U);

	err = gpt_insert(&gpt, 0U, 0x2000, e1, true);
	assert(err != OK);

	gpt_clear(&gpt, 1U, 0x10000U);

	gpt_dump_levels(&gpt);

	gpt_clear_all(&gpt);

	e1 = test_entry_init(GPT_TYPE_TEST_A, 0xdeadbeefbeadfeedU);

	// Base and size cause overflow
	err = gpt_insert(&gpt, 0xffffffffffffff00, 0x0, e1, true);
	assert(err == ERROR_ARGUMENT_INVALID);

	err = gpt_insert(&gpt, 0xffffffffffffff00, 0x100, e1, true);
	assert(err == ERROR_ARGUMENT_SIZE);

	err = gpt_insert(&gpt, 0xffffffffffffffff, 0x33333333, e1, true);
	assert(err == ERROR_ARGUMENT_INVALID);

	// Larger than max size supported
	err = gpt_insert(&gpt, util_bit(GPT_MAX_SIZE_BITS), 0x1, e1, true);
	assert(err == ERROR_ARGUMENT_SIZE);

	err = gpt_insert(&gpt, util_bit(GPT_MAX_SIZE_BITS + 1), 0x33333333, e1,
			 true);
	assert(err == ERROR_ARGUMENT_SIZE);

	err = gpt_insert(&gpt, util_bit(GPT_MAX_SIZE_BITS) - 1, 0x1, e1, true);
	assert(err == OK);

	err = gpt_insert(&gpt, util_bit(GPT_MAX_SIZE_BITS) - 1, 0x1, e1, true);
	assert(err == ERROR_BUSY);

	err = gpt_clear(&gpt, util_bit(GPT_MAX_SIZE_BITS) - 1, 0x1U);
	assert(err == OK);

	err = gpt_insert(&gpt, util_bit(GPT_MAX_SIZE_BITS) - 2, 0x2, e1, true);
	assert(err == OK);

	err = gpt_insert(&gpt, util_bit(GPT_MAX_SIZE_BITS) - 1, 0x1, e1, true);
	assert(err == ERROR_BUSY);

	err = gpt_walk(&gpt, 0, util_bit(GPT_MAX_SIZE_BITS), GPT_TYPE_TEST_A,
		       GPT_CALLBACK_TEST, arg);
	assert(err == OK);

	err = gpt_clear(&gpt, util_bit(GPT_MAX_SIZE_BITS) - 1, 0x1U);
	assert(err == OK);

	err = gpt_walk(&gpt, 0, util_bit(GPT_MAX_SIZE_BITS), GPT_TYPE_TEST_A,
		       GPT_CALLBACK_TEST, arg);
	assert(err == OK);

	err = gpt_insert(&gpt, util_bit(GPT_MAX_SIZE_BITS) - 2, 0x1, e1, true);
	assert(err == ERROR_BUSY);

	e2 = e1;
	e2.value.raw += 1;
	err = gpt_insert(&gpt, util_bit(GPT_MAX_SIZE_BITS) - 1, 0x1, e2, true);
	assert(err == OK);

	err = gpt_walk(&gpt, 0, util_bit(GPT_MAX_SIZE_BITS), GPT_TYPE_TEST_A,
		       GPT_CALLBACK_TEST, arg);
	assert(err == OK);

	err = gpt_insert(&gpt, 0x4000000000000, 0x33333333, e1, true);
	assert(err == OK);

	e1 = test_entry_init(GPT_TYPE_TEST_A, 0x3333444455550000U);

	err = gpt_insert(&gpt, 0x234000000000, 0x679823213, e1, true);
	assert(err == OK);

	// Attempt to insert range with overlap at end
	err = gpt_insert(&gpt, 0x233cdf123000, 0x82681e3f2, e1, true);
	assert(err != OK);

	e1 = test_entry_init(GPT_TYPE_TEST_A, 0x3333444455556666U);
	e2 = test_entry_init(GPT_TYPE_TEST_B, 0x777788889999);

	// Attempt to update range that doesn't match at the end
	err = gpt_update(&gpt, 0x234000006666, 0x73f8c532ab, e1, e2);
	assert(err != OK);

	err = gpt_update(&gpt, 0x234000006666, 0x123e34, e1, e2);
	assert(err == OK);

	gpt_dump_ranges(&gpt);

	base = 0x7ab2348e293;
	size = 0x123809193;

	e1 = test_entry_init(GPT_TYPE_TEST_A, 0x183741ea175);

	err = gpt_insert(&gpt, 0U, base, e1, false);
	assert(err == OK);

	e1.value.raw += base + size;

	err = gpt_insert(&gpt, base + size, GPT_MAX_SIZE - base - size, e1,
			 false);
	assert(err == OK);

	e1.value.raw -= size;

	err = gpt_insert(&gpt, base, size, e1, false);
	assert(err == OK);

	e1.value.raw -= base;

	// GPT should now have a single contiguous entry
	assert(gpt_is_contiguous(&gpt, 0U, GPT_MAX_SIZE, e1));

	gpt_dump_ranges(&gpt);

	gpt_destroy(&gpt);
out:
	preempt_enable();

	return false;
}

#else

extern char unused;

#endif

```

`hyp/misc/gpt/tests/Makefile`:

```
# © 2022 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

CC=clang

INCLUDE=-I../../../interfaces/gpt/include
INCLUDE+=-I../../../../build/qemu/unittests-qemu/debug/include
INCLUDE+=-I../../../../build/qemu/unittests-qemu/debug/events/include
INCLUDE+=-I../../../../build/qemu/unittests-qemu/debug/events/gpt/include
INCLUDE+=-I../../../../build/qemu/unittests-qemu/debug/objects/include
INCLUDE+=-I../../../interfaces/cpulocal/include
INCLUDE+=-I../../../interfaces/log/include
INCLUDE+=-I../../../interfaces/object/include
INCLUDE+=-I../../../interfaces/partition/include
INCLUDE+=-I../../../interfaces/preempt/include
INCLUDE+=-I../../../interfaces/rcu/include
INCLUDE+=-I../../../interfaces/trace/include
INCLUDE+=-I../../../interfaces/util/include
INCLUDE+=-I../../../misc/log_standard/include
INCLUDE+=-I../../../arch/armv8/include
INCLUDE+=-imacros ../../../interfaces/util/include/attributes.h
INCLUDE+=-imacros ../../../core/spinlock_ticket/include/spinlock_attrs.h
INCLUDE+=-imacros ../../../core/preempt/include/preempt_attrs.h
INCLUDE+=-imacros ../../../interfaces/rcu/include/rcu_attrs.h

DEF=-DHYP_STANDALONE_TEST
DEF+=-DUNIT_TESTS

CFLAGS+=-m64 -mcx16 -std=gnu18 -O1 -g
CFLAGS+=-Wall -Werror -Wno-gcc-compat -Wno-gnu-alignof-expression
CFLAGS+=-ffunction-sections -fdata-sections
CFLAGS+=-fsanitize=address -fno-omit-frame-pointer
#CFLAGS+=--coverage
CFLAGS+=$(INCLUDE)
CFLAGS+=$(DEF)

LDFLAGS+=-Wl,--gc-sections

SRC=../src/gpt.c ../src/gpt_tests.c host_tests.c
SRC+=../../../core/util/src/bitmap.c
SRC+=../../../misc/log_standard/src/string_util.c
SRC+=../../../../build/qemu/unittests-qemu/debug/hyp/core/base/accessors.c
SRC+=../../../../build/qemu/unittests-qemu/debug/hyp/core/base/hypresult.c

OBJ=$(patsubst %.c,%.o,$(SRC))

TARGET=gpt_tests

%.o: %.c
	$(CC) $(CFLAGS) -c $< -o $@

$(TARGET): $(OBJ)
	$(CC) $(CFLAGS) $(LDFLAGS) $(OBJ) -o $@

default: $(TARGET)

clean:
	rm -f $(TARGET) $(OBJ)

```

`hyp/misc/gpt/tests/host_tests.c`:

```c
// © 2022 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>

#define timer_t hyp_timer_t
#include <hyptypes.h>
#undef timer_t

#define register_t std_register_t
#include <stdio.h>
#include <stdlib.h>
#undef register_t

#include <compiler.h>
#include <gpt.h>
#include <log.h>
#include <object.h>
#include <panic.h>
#include <partition.h>
#include <trace.h>
#include <util.h>

#include "event_handlers.h"
#include "string_util.h"
#include "trace_helpers.h"

gpt_t		gpt;
partition_t	host_partition;
trace_control_t hyp_trace;

void
assert_failed(const char *file, int line, const char *func, const char *err)
{
	printf("Assert failed in %s at %s:%d: %s\n", func, file, line, err);
	exit(-1);
}

void
panic(const char *str)
{
	printf("Panic: %s\n", str);
	exit(-1);
}

static void
trace_and_log_init(void)
{
	register_t flags = 0U;

	TRACE_SET_CLASS(flags, ERROR);
	TRACE_SET_CLASS(flags, DEBUG);

	atomic_init(&hyp_trace.enabled_class_flags, flags);
}

void
trigger_trace_log_event(trace_id_t id, trace_action_t action, const char *arg0,
			register_t arg1, register_t arg2, register_t arg3,
			register_t arg4, register_t arg5)
{
	char log[1024];
	(void)snprint(log, util_array_size(log), arg0, arg1, arg2, arg3, arg4,
		      arg5);
	puts(log);
}

partition_t *
object_get_partition_additional(partition_t *partition)
{
	assert(partition != NULL);

	return partition;
}

void
object_put_partition(partition_t *partition)
{
	assert(partition != NULL);
}

partition_t *
partition_get_root(void)
{
	return &host_partition;
}

void_ptr_result_t
partition_alloc(partition_t *partition, size_t bytes, size_t min_alignment)
{
	assert(partition != NULL);
	assert(bytes > 0U);

	void *mem = aligned_alloc(min_alignment, bytes);

	return (mem != NULL) ? void_ptr_result_ok(mem)
			     : void_ptr_result_error(ERROR_NOMEM);
}

error_t
partition_free(partition_t *partition, void *mem, size_t bytes)
{
	assert(partition != NULL);
	assert(bytes > 0U);

	free(mem);

	return OK;
}

void
preempt_disable(void)
{
}

void
preempt_enable(void)
{
}

void
rcu_read_start(void)
{
}

void
rcu_read_finish(void)
{
}

void
rcu_enqueue(rcu_entry_t *rcu_entry, rcu_update_class_t rcu_update_class)
{
	assert(rcu_update_class == RCU_UPDATE_CLASS_GPT_FREE_LEVEL);

	(void)gpt_handle_rcu_free_level(rcu_entry);
}

cpu_index_t
cpulocal_check_index(cpu_index_t cpu)
{
	return cpu;
}

cpu_index_t
cpulocal_get_index_unsafe(void)
{
	return 0U;
}

void
trigger_gpt_value_add_offset_event(gpt_type_t type, gpt_value_t *value,
				   size_t offset)
{
	if ((type == GPT_TYPE_TEST_A) || (type == GPT_TYPE_TEST_B) ||
	    (type == GPT_TYPE_TEST_C)) {
		gpt_tests_add_offset(type, value, offset);
	} else {
		// Nothing to do
	}
}

bool
trigger_gpt_values_equal_event(gpt_type_t type, gpt_value_t x, gpt_value_t y)
{
	bool ret;

	if ((type == GPT_TYPE_TEST_A) || (type == GPT_TYPE_TEST_B) ||
	    (type == GPT_TYPE_TEST_C)) {
		ret = gpt_tests_values_equal(x, y);
	} else if (GPT_TYPE_EMPTY) {
		ret = gpt_handle_empty_values_equal();
	} else {
		ret = false;
	}

	return ret;
}

error_t
trigger_gpt_walk_callback_event(gpt_callback_t callback, gpt_entry_t entry,
				size_t base, size_t size, gpt_arg_t arg)
{
	error_t ret;

	if (callback == GPT_CALLBACK_RESERVED) {
		gpt_handle_reserved_callback();
	} else if (callback == GPT_CALLBACK_TEST) {
		ret = gpt_tests_callback(entry, base, size, arg);
	} else {
		ret = ERROR_ARGUMENT_INVALID;
	}

	return ret;
}

int
main(void)
{
	trace_and_log_init();

	gpt_handle_tests_init();

	gpt_handle_tests_start();

	return 0;
}

```

`hyp/misc/log_standard/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

interface log
local_include include
events log.ev
source log.c string_util.c
source debug.S
types log.tc
configs LOG_BUFFER_SIZE=16384

```

`hyp/misc/log_standard/include/string_util.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// snprint produces null terminated output string, of maximum length not
// exceeding size characters (including the terminating null).
//
// Unlike the C printf family of functions, the format string is however a
// python format string like syntax.
// For example  "This is a hex value: {:x}" will print the hex representation
// of the corresponding argument.
//
// Not all python format string syntax is implemented, including positional
// arguments. The following approximate python format string syntax is
// accepted.
//	[[fill]align][sign][#][0][minimumwidth][.precision][type]
//
// This function returns the count of bytes written, up to a maximum of size-1.
// A return value of size or larger indicates that the output was truncated.
size_result_t
snprint(char *str, size_t size, const char *format, register_t arg0,
	register_t arg1, register_t arg2, register_t arg3, register_t arg4);

```

`hyp/misc/log_standard/log.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

module log_standard

subscribe trace_log(id, action, arg0, arg1, arg2, arg3, arg4, arg5)

subscribe boot_runtime_first_init
	handler log_init()

```

`hyp/misc/log_standard/log.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// '6LOG' = '364c4f47'
// - Note, external tools can also use this to detect endianness.
define LOG_MAGIC constant = 0x474f4c36;

// This structure is a singleton, and is accessible from all CPUs.
define log_control object {
	log_magic	uint32(const);
	head		type index_t(atomic);
	buffer_size	size(const);
	log_buffer	pointer(const) char;
};

extend trace_class enumeration {
	LOG_BUFFER = 5;
};

extend trace_action enumeration {
	LOG;
	TRACE_AND_LOG;
};

```

`hyp/misc/log_standard/src/debug.S`:

```S
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause


// .text.debug is a dummy section used to KEEP any symbols that may need to be
// externally visible and not optimized by LTO - usually for debuggers
	.section .text.debug, "ax", @progbits
	adrp	x0, hyp_log

```

`hyp/misc/log_standard/src/log.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>
#include <stdatomic.h>
#include <string.h>

#include <hypconstants.h>
#include <hypregisters.h>

#include <compiler.h>
#include <cpulocal.h>
#include <platform_timer.h>
#include <trace.h>
#include <util.h>

#include <events/log.h>

#include <asm/cache.h>
#include <asm/cpu.h>
#include <asm/prefetch.h>

#include "event_handlers.h"
#include "string_util.h"
#include "trace_helpers.h"

// FIXME: the log temporary buffer is placed on the stack
// Note: thread_stack_size_default = 4096
#define LOG_TIMESTAMP_BUFFER_SIZE 24U
#define LOG_ENTRY_BUFFER_SIZE	  256U

extern char hyp_log_buffer[];
char	    hyp_log_buffer[LOG_BUFFER_SIZE];

static_assert((size_t)LOG_BUFFER_SIZE > LOG_ENTRY_BUFFER_SIZE,
	      "LOG_BUFFER_SIZE too small");

// Global visibility - for debug
extern log_control_t hyp_log;

log_control_t hyp_log = { .log_magic   = LOG_MAGIC,
			  .head	       = 0,
			  .buffer_size = LOG_BUFFER_SIZE,
			  .log_buffer  = hyp_log_buffer };

void
log_init(void)
{
	register_t flags = 0U;
	TRACE_SET_CLASS(flags, LOG_BUFFER);
	trace_set_class_flags(flags);

	assert_debug(hyp_log.buffer_size == (index_t)LOG_BUFFER_SIZE);
}

void
log_standard_handle_trace_log(trace_id_t id, trace_action_t action,
			      const char *fmt, register_t arg0, register_t arg1,
			      register_t arg2, register_t arg3, register_t arg4)
{
	index_t	      next_idx, prev_idx, orig_idx;
	size_result_t ret;
	size_t	      entry_size, timestamp_size;
	char	      entry_buf[LOG_ENTRY_BUFFER_SIZE];

	// Add the data to the log buffer only if:
	// - The requested action is logging, and logging is enabled
	bool	   log_action  = ((action == TRACE_ACTION_LOG) ||
				  (action == TRACE_ACTION_TRACE_AND_LOG));
	register_t class_flags = trace_get_class_flags();
	if (compiler_unexpected(
		    ((!log_action ||
		      ((class_flags & TRACE_CLASS_BITS(LOG_BUFFER)) == 0U))))) {
		goto out;
	}

	// Add the time-stamp and core number
	// We could use platform_convert_ticks_to_ns() here, but the resulting
	// values will be too big and unwieldy to use. Since log formatting is a
	// slow process anyway, might as well add some nice time-stamps.
	ticks_t	      now = platform_timer_get_current_ticks();
	nanoseconds_t ns  = platform_timer_convert_ticks_to_ns(now);

	microseconds_t usec = ns / (microseconds_t)1000;
	uint64_t       sec  = usec / TIMER_MICROSECS_IN_SECOND;

	ret = snprint(entry_buf, LOG_TIMESTAMP_BUFFER_SIZE,
		      "{:d} {:4d}.{:06d} ", cpulocal_get_index_unsafe(), sec,
		      usec % TIMER_MICROSECS_IN_SECOND, 0, 0);
	if (ret.e == ERROR_STRING_TRUNCATED) {
		// The truncated string will have a NULL terminator, remove it
		timestamp_size = LOG_TIMESTAMP_BUFFER_SIZE - 1U;
	} else if (ret.e != OK) {
		// Should not happen
		goto out;
	} else {
		// Do not count the NULL character since we will write over it
		// shortly.
		timestamp_size = ret.r;
	}

	// Add the log message after the time-stamp
	ret = snprint(entry_buf + timestamp_size,
		      LOG_ENTRY_BUFFER_SIZE - timestamp_size, fmt, arg0, arg1,
		      arg2, arg3, arg4);
	if (ret.e == ERROR_STRING_TRUNCATED) {
		entry_size = LOG_ENTRY_BUFFER_SIZE;
	} else if ((ret.e == ERROR_STRING_MISSING_ARGUMENT) || (ret.r == 0U)) {
		goto out;
	} else {
		entry_size = timestamp_size + ret.r + 1U;
		assert(entry_size <= LOG_ENTRY_BUFFER_SIZE);
	}

	const index_t buffer_size = (index_t)LOG_BUFFER_SIZE;

	// Inform the subscribers of the entry without the time-stamp
	trigger_log_message_event(id, entry_buf + timestamp_size);

	// Atomically update the index first
	orig_idx = atomic_fetch_add_explicit(&hyp_log.head, entry_size,
					     memory_order_relaxed);
	prev_idx = orig_idx;
	while (compiler_unexpected(prev_idx >= buffer_size)) {
		prev_idx -= buffer_size;
	}

	next_idx = orig_idx + (index_t)entry_size;
	// If we wrap, something is really wrong
	assert(next_idx > orig_idx);

	if (compiler_unexpected(next_idx >= buffer_size)) {
		index_t old_idx = next_idx;

		while (next_idx >= buffer_size) {
			next_idx -= buffer_size;
		}

		// Try to reduce the index in the shared variable so it is no
		// longer overflowed. We don't care if it fails because that
		// means somebody else has done it concurrently.
		(void)atomic_compare_exchange_strong_explicit(
			&hyp_log.head, &old_idx, next_idx, memory_order_relaxed,
			memory_order_relaxed);
	}

	prefetch_store_stream(&hyp_log.log_buffer[prev_idx]);

	size_t buf_remaining = (size_t)buffer_size - (size_t)prev_idx;

	// Copy the whole entry if it fits
	if (compiler_expected(buf_remaining >= entry_size)) {
		(void)memcpy(&hyp_log.log_buffer[prev_idx], entry_buf,
			     entry_size);
		cache_clean_range(&hyp_log.log_buffer[prev_idx], entry_size);
	} else {
		// Otherwise copy the first bit of entry to the tail of the
		// buffer and wrap to the start for the remainder.
		size_t first_part = buf_remaining;
		(void)memcpy(&hyp_log.log_buffer[prev_idx], entry_buf,
			     first_part);
		cache_clean_range(&hyp_log.log_buffer[prev_idx], first_part);

		size_t second_part = entry_size - first_part;

		(void)memcpy(&hyp_log.log_buffer[0], entry_buf + first_part,
			     second_part);
		cache_clean_range(&hyp_log.log_buffer[0], second_part);
	}
out:
	// Nothing to do
	return;
}

```

`hyp/misc/log_standard/src/string_util.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>
#include <stdbool.h>
#include <stdint.h>
#include <string.h>

#include <util.h>

#include "string_util.h"

// TODO: Add more failure test cases

#define MAX_TERMINATOR 16U
#define MAX_ARG_CNT    5U

// The string formatter processing states, the order of these needs to be
// preserved.
#define STAGE_START	      0U
#define STAGE_SPECIFIER_START 1U
#define STAGE_ALIGN	      2U
#define STAGE_SIGN	      3U
#define STAGE_ALTERNATIVE     4U
#define STAGE_ZERO_PADDING    5U
#define STAGE_MINWIDTH	      6U
#define STAGE_PRECISE	      7U
#define STAGE_TYPE	      8U
#define STAGE_END	      9U

// Token for components in one format descriptor
typedef struct token {
	// Indicate the token type
	count_t stage;
} token_t;

typedef enum align_e {
	// The same as left, use white space
	ALIGN_DEFAULT = 0,
	ALIGN_LEFT,
	ALIGN_RIGHT,
	// Only valid for numeric
	ALIGN_AFTER_SIGN,
	ALIGN_CENTER
} align_t;

typedef enum sign_e {
	// default
	SIGN_NEG = 0,
	SIGN_BOTH,
	SIGN_POS_LEADING
} sign_t;

typedef enum var_type {
	// Default
	VAR_TYPE_NONE = 0,
	// b, binary
	VAR_TYPE_BIN,
	// FIXME: only support ASCII now, no unicode
	// VAR_TYPE_CHAR,
	// d, decimal
	VAR_TYPE_DEC,
	// o, octal
	VAR_TYPE_OCTAL,
	// x, hex, lower case
	VAR_TYPE_LOW_HEX,
	// X, hex, upper case
	// VAR_TYPE_UP_HEX,
	// FIXME: Don't support n and None, which is the same as d
	// e, exponential
	// VAR_TYPE_LOW_EXP,
	// FIXME: Support E
	// f, float
	// VAR_TYPE_LOW_FLOAT,
	// s, string
	VAR_TYPE_STRING,
	// FIXME: support F, G, n, %
} var_type_t;

typedef struct fmt_info {
	// Default is none type, if it's none, nothing make sense in this
	// structure, so memset 0 is good enough to initialise it
	var_type_t type;
	// Space by default, \0 means default
	char	fill_char;
	bool	alternate_form;
	bool	zero_padding;
	uint8_t padding2[1];
	align_t alignment;
	sign_t	sign;
	// Minimum width, default 0 means no restrict
	size_t min_width;
	// Precise for float/double, ignored for integer, max width for non
	// digital. Default 0 means 2 precise
	size_t	    precise;
	const char *minwidth_start;
	const char *precise_start;
} fmt_info_t;

typedef enum ret_token {
	// Move to next char
	RET_TOKEN_NEXT_CHAR,
	// Check the same char in next stage
	RET_TOKEN_NEXT_STAGE,
	// This format is done
	RET_TOKEN_STOP,
	// Found a token, should goto next char with next stage
	RET_TOKEN_FOUND,
	// Got invalid input in the [% ] format descriptor
	RET_TOKEN_ERROR,
} ret_token_t;

// Check whether a buffer contains the specified character within the specified
// distance (size).
static inline index_t
strnidx(const char *buf, size_t size, char c)
{
	index_t i;

	for (i = 0U; i < size; i++) {
		if (buf[i] == c) {
			break;
		}
	}

	return i;
}

// Return true if c is in [min, max]
static inline bool
in_range(const char c, const char min, const char max)
{
	return c >= min && c <= max;
}

static inline uint64_t
atodec(const char *c, size_t len)
{
	uint64_t v = 0U;

	while (len != 0U) {
		v *= 10U;
		// TODO: Check if *c is in '0' and '9' for debug
		v += (uint64_t)(*c) - (uint64_t)('0');
		c++;
		len--;
	}

	return v;
}

static size_t
insert_padding(char *buf, size_t size, char fill_char, size_t len)
{
	size_t padding = 0U;

	while ((size > 0U) && (len > 0U)) {
		*buf = fill_char;
		buf++;
		padding++;
		len--;
		size--;
	}

	return padding;
}

static error_t
itoa_insert_sign(const fmt_info_t *info, bool positive, size_t *remaining,
		 char **pos_ptr)
{
	error_t ret = OK;
	char   *pos = *pos_ptr;

	switch (info->sign) {
	case SIGN_BOTH:
		if (positive) {
			*pos = '+';
		} else {
			*pos = '-';
		}
		pos++;
		(*remaining)--;
		if (*remaining == 0U) {
			ret = ERROR_STRING_TRUNCATED;
			goto out;
		}
		break;

	case SIGN_POS_LEADING:
		if (positive) {
			*pos = ' ';
		} else {
			*pos = '-';
		}
		pos++;
		(*remaining)--;
		if (*remaining == 0U) {
			ret = ERROR_STRING_TRUNCATED;
			goto out;
		}
		break;

	case SIGN_NEG:
	default:
		if (!positive) {
			*pos = '-';
			(*remaining)--;
			if (*remaining == 0U) {
				ret = ERROR_STRING_TRUNCATED;
				goto out;
			}
			pos++;
		}
		break;
	}

out:
	*pos_ptr = pos;
	return ret;
}

static error_t
itoa_insert_base(uint8_t base, size_t *remaining, char **pos_ptr)
{
	error_t ret = OK;
	char   *pos = *pos_ptr;

	switch (base) {
	case 2:
		*pos = 'b';
		pos++;
		(*remaining)--;
		break;
	case 8:
		*pos = 'o';
		pos++;
		(*remaining)--;
		break;
	case 16:
		*pos = 'x';
		pos++;
		(*remaining)--;
		break;
	default:
		// Unusual base. Nothing to do
		break;
	}

	if (*remaining == 0U) {
		ret = ERROR_STRING_TRUNCATED;
		goto out;
	}

	switch (base) {
	case 2:
	case 8:
	case 16:
		*pos = '0';
		pos++;
		(*remaining)--;
		break;
	default:
		// Unusual base. Nothing to do
		break;
	}

	if (*remaining == 0U) {
		ret = ERROR_STRING_TRUNCATED;
	}

out:
	*pos_ptr = pos;
	return ret;
}

static inline error_t
itoa(char *buf, size_t *size, uint64_t val, uint8_t base, fmt_info_t *info,
     bool positive)
{
	const char digit[] = { '0', '1', '2', '3', '4', '5', '6', '7',
			       '8', '9', 'a', 'b', 'c', 'd', 'e', 'f' };
	char	  *pos = buf, padding_char = ' ', *tail = NULL;
	size_t	   padding_cnt = 0U, content_cnt = 0U, padding_ret = 0U;
	size_t	   padding_left_cnt = 0U, padding_right_cnt = 0U;
	size_t	   padding_after_sign = 0U, padding_after_prefix = 0U;
	size_t	   remaining = *size;
	error_t	   ret	     = OK;

	assert(base <= 16);

	if (remaining == 0U) {
		ret = ERROR_STRING_TRUNCATED;
		goto out;
	}

	do {
		index_t i = (index_t)(val % base);
		*pos	  = digit[i];
		content_cnt++;
		pos++;
		remaining--;
		if (remaining == 0U) {
			ret = ERROR_STRING_TRUNCATED;
			goto reverse_out;
		}
		val = val / base;
	} while (val != 0U);

	padding_cnt = util_max(info->min_width, content_cnt) - content_cnt;

	if ((padding_cnt > 0U) && info->alternate_form && (base != 10U)) {
		// Remove two chars of prefix
		padding_cnt -= 2U;
	}
	if ((padding_cnt > 0U) &&
	    ((info->sign == SIGN_BOTH) || (info->sign == SIGN_POS_LEADING) ||
	     ((info->sign == SIGN_NEG) && !positive))) {
		padding_cnt -= 1U;
	}
	// Padding left with white space by default
	padding_left_cnt = padding_cnt;

	// FIXME: Ignore precise for integer, might report error
	// FIXME: Slightly different zero padding behavior, it takes priority
	if (info->zero_padding) {
		padding_after_prefix = padding_cnt;
		padding_after_sign   = 0U;
		padding_left_cnt     = 0U;
		padding_right_cnt    = 0U;
		padding_char	     = '0';
	}

	if ((info->alignment != ALIGN_DEFAULT) && (info->fill_char != '\0')) {
		padding_char = info->fill_char;
	}

	if (info->alignment == ALIGN_AFTER_SIGN) {
		padding_after_prefix = 0U;
		padding_after_sign   = padding_cnt;
		padding_left_cnt     = 0U;
		padding_right_cnt    = 0U;
	} else if (info->alignment == ALIGN_LEFT) {
		// align content to left, add padding to right
		padding_after_prefix = 0U;
		padding_after_sign   = 0U;
		padding_left_cnt     = 0U;
		padding_right_cnt    = padding_cnt;
	} else if (info->alignment == ALIGN_RIGHT) {
		// align content to right, add padding to left
		padding_after_prefix = 0U;
		padding_after_sign   = 0U;
		padding_left_cnt     = padding_cnt;
		padding_right_cnt    = 0U;
	} else if (info->alignment == ALIGN_CENTER) {
		padding_after_prefix = 0U;
		padding_after_sign   = 0U;
		padding_left_cnt     = padding_cnt / 2U;
		padding_right_cnt    = padding_cnt - padding_left_cnt;
	} else {
		// Nothing to do
	}

	padding_ret = insert_padding(pos, remaining, padding_char,
				     padding_after_prefix);
	pos += padding_ret;
	remaining -= padding_ret;
	if ((remaining == 0U) || (padding_ret < padding_after_prefix)) {
		ret = ERROR_STRING_TRUNCATED;
		goto reverse_out;
	}

	if (info->alternate_form) {
		ret = itoa_insert_base(base, &remaining, &pos);
		if (ret != OK) {
			goto reverse_out;
		}
	}

	padding_ret = insert_padding(pos, remaining, padding_char,
				     padding_after_sign);
	pos += padding_ret;
	remaining -= padding_ret;
	if ((remaining == 0U) || (padding_ret < padding_after_sign)) {
		ret = ERROR_STRING_TRUNCATED;
		goto reverse_out;
	}

	ret = itoa_insert_sign(info, positive, &remaining, &pos);
	if (ret != OK) {
		goto reverse_out;
	}

	padding_ret =
		insert_padding(pos, remaining, padding_char, padding_left_cnt);
	pos += padding_ret;
	remaining -= padding_ret;
	// last padding action, no explicit truncation check

reverse_out:
	// Reverse the final string
	tail = pos;
	pos--;
	while (buf < pos) {
		char tmp = *buf;
		*buf	 = *pos;
		*pos	 = tmp;
		pos--;
		buf++;
	}

	padding_ret = insert_padding(tail, remaining, padding_char,
				     padding_right_cnt);
	remaining -= padding_ret;

	*size = remaining;

out:
	return ret;
}

static inline error_t
sitoa(char *buf, size_t *size, int64_t val, uint8_t base, fmt_info_t *info)
{
	bool positive = true;

	if (val < 0) {
		positive = false;
		val	 = -val;
	} else {
		positive = true;
	}

	return itoa(buf, size, (uint64_t)val, base, info, positive);
}

static inline error_t
stringtoa(char *buf, size_t *size, char *val_str, fmt_info_t *info)
{
	error_t ret	  = OK;
	size_t	remaining = *size;

	if (val_str == NULL) {
		ret = ERROR_STRING_MISSING_ARGUMENT;
		goto out;
	}

	char  *pos = buf, padding_char = ' ';
	size_t slen	   = strlen(val_str);
	size_t padding_cnt = 0U, p = 0U;
	size_t padding_left_cnt = 0U, padding_right_cnt = 0U;

	if (info->precise != 0U) {
		slen = util_min(slen, info->precise);
	}

	padding_cnt = util_max(slen, info->min_width) - slen;

	padding_left_cnt = padding_cnt;
	if ((info->alignment != ALIGN_DEFAULT) && (info->fill_char != '\0')) {
		padding_char = info->fill_char;
	}

	if (info->alignment == ALIGN_LEFT) {
		padding_left_cnt  = padding_cnt;
		padding_right_cnt = 0U;
	} else if (info->alignment == ALIGN_RIGHT) {
		padding_left_cnt  = 0U;
		padding_right_cnt = padding_cnt;
	} else if (info->alignment == ALIGN_CENTER) {
		padding_left_cnt  = padding_cnt / 2U;
		padding_right_cnt = padding_cnt - padding_left_cnt;
	} else {
		// Nothing to do.
	}

	p = util_min(padding_left_cnt, remaining);
	if (p > 0U) {
		(void)insert_padding(pos, p, padding_char, p);
		remaining -= p;
		pos += p;
	}
	if ((remaining + p) < padding_left_cnt) {
		ret = ERROR_STRING_TRUNCATED;
		goto out;
	}

	p = util_min(slen, remaining);
	if (p > 0U) {
		(void)memcpy(pos, val_str, p);
		remaining -= p;
		pos += p;
	}
	if ((remaining + p) < slen) {
		ret = ERROR_STRING_TRUNCATED;
		goto out;
	}

	p = util_min(padding_right_cnt, remaining);
	if (p > 0U) {
		(void)insert_padding(pos, p, padding_char, p);
		remaining -= p;
	}
	if ((remaining + p) < padding_right_cnt) {
		ret = ERROR_STRING_TRUNCATED;
		goto out;
	}

	ret = OK;

out:
	*size = remaining;
	return ret;
}

// The following check can depend on the {*(fmt - 1), *fmt, *(fmt + 1)} to check
// (except start check)
static inline ret_token_t
check_start(const char *fmt, fmt_info_t *info)
{
	(void)info;

	if (*fmt == '{') {
		return RET_TOKEN_FOUND;
	}
	return RET_TOKEN_NEXT_CHAR;
}

static inline ret_token_t
check_specifier_start(const char *fmt, fmt_info_t *info)
{
	(void)info;

	// ignore white space
	if (*fmt == ' ') {
		return RET_TOKEN_NEXT_CHAR;
	}

	if (*fmt == ':') {
		return RET_TOKEN_FOUND;
	}

	return RET_TOKEN_NEXT_CHAR;
}

static inline ret_token_t
check_align(const char *fmt, fmt_info_t *info)
{
	// Mapping from input to output, by index (readable duplication)
	const char    stopper[] = { '<', '>', '=', '^' };
	const align_t output[]	= { ALIGN_LEFT, ALIGN_RIGHT, ALIGN_AFTER_SIGN,
				    ALIGN_CENTER };
	const size_t  len	= util_array_size(stopper);
	// Default to skip alignment stage if not found
	ret_token_t ret = RET_TOKEN_NEXT_STAGE;

	const char *next = NULL;
	if (*(fmt + 1) != '\0') {
		next = fmt + 1;
	}

	// Check for a padding character using look-ahead
	if ((next != NULL) && (strnidx(stopper, len, *next) < len)) {
		if (info->fill_char == '\0') {
			info->fill_char = *fmt;
			ret		= RET_TOKEN_NEXT_CHAR;
		} else {
			// e.g. '{: >>5d}' is an error
			ret = RET_TOKEN_ERROR;
		}
	} else {
		index_t i = strnidx(stopper, len, *fmt);
		if (i < len) {
			info->alignment = output[i];

			ret = RET_TOKEN_FOUND;
		}
	}

	return ret;
}

static inline ret_token_t
check_sign(const char *fmt, fmt_info_t *info)
{
	const char   stopper[] = { '+', '-', ' ' };
	const sign_t output[]  = { SIGN_BOTH, SIGN_NEG, SIGN_POS_LEADING };
	const size_t len       = util_array_size(stopper);

	index_t i = strnidx(stopper, len, *fmt);
	if (i < len) {
		info->sign = output[i];
		return RET_TOKEN_FOUND;
	}

	return RET_TOKEN_NEXT_STAGE;
}

static inline ret_token_t
check_alternative(const char *fmt, fmt_info_t *info)
{
	if (*fmt == '#') {
		info->alternate_form = true;
		return RET_TOKEN_FOUND;
	}

	return RET_TOKEN_NEXT_STAGE;
}

static inline ret_token_t
check_zeropadding(const char *fmt, fmt_info_t *info)
{
	if (*fmt == '0') {
		info->zero_padding = true;
		return RET_TOKEN_FOUND;
	}

	return RET_TOKEN_NEXT_STAGE;
}

// NOTE: Make sure current fmt is not the tail of the fmt, (not \0)
static inline ret_token_t
check_minwidth(const char *fmt, fmt_info_t *info)
{
	ret_token_t ret = RET_TOKEN_NEXT_STAGE;

	if (in_range(*fmt, '0', '9')) {
		if (info->minwidth_start == NULL) {
			info->minwidth_start = fmt;
		}

		// If the next is still digital, greedily consume them
		if (in_range(fmt[1], '0', '9')) {
			ret = RET_TOKEN_NEXT_CHAR;
		} else {
			ptrdiff_t len = fmt - info->minwidth_start;
			assert(len >= 0);
			info->min_width	     = atodec(info->minwidth_start,
						      ((size_t)len + 1U));
			info->minwidth_start = NULL;
			ret		     = RET_TOKEN_FOUND;
		}
	} else {
		ret		     = RET_TOKEN_NEXT_STAGE;
		info->minwidth_start = NULL;
	}

	return ret;
}

static inline ret_token_t
check_precise(const char *fmt, fmt_info_t *info)
{
	ret_token_t ret;

	if ((*fmt == '.') && in_range(fmt[1], '0', '9')) {
		info->precise_start = NULL;
		return RET_TOKEN_NEXT_CHAR;
	}

	if (in_range(*fmt, '0', '9')) {
		if (info->precise_start == NULL) {
			info->precise_start = fmt;
		}

		// If the next is still
		if (in_range(fmt[1], '0', '9')) {
			ret = RET_TOKEN_NEXT_CHAR;
		} else {
			ptrdiff_t len = fmt - info->precise_start;
			assert(len >= 0);
			info->precise =
				atodec(info->precise_start, ((size_t)len + 1U));
			info->precise_start = NULL;
			ret		    = RET_TOKEN_FOUND;
		}
	} else {
		ret		    = RET_TOKEN_NEXT_STAGE;
		info->precise_start = NULL;
	}

	return ret;
}

static inline ret_token_t
check_type(const char *fmt, fmt_info_t *info)
{
	const char	 stopper[] = { 'b', 'd', 'o', 'x', 's' };
	const var_type_t output[]  = {
		 VAR_TYPE_BIN,	   VAR_TYPE_DEC,    VAR_TYPE_OCTAL,
		 VAR_TYPE_LOW_HEX, VAR_TYPE_STRING,
	};
	const size_t len = util_array_size(stopper);
	index_t	     i	 = strnidx(stopper, len, *fmt);

	if (i < len) {
		info->type = output[i];
		return RET_TOKEN_FOUND;
	}

	return RET_TOKEN_ERROR;
}

static inline ret_token_t
check_end(const char *fmt, fmt_info_t *info)
{
	(void)info;

	// Ignore white space
	if (*fmt == ' ') {
		return RET_TOKEN_NEXT_CHAR;
	}

	if (*fmt == '}') {
		return RET_TOKEN_STOP;
	}

	return RET_TOKEN_ERROR;
}

static inline ret_token_t
check_token(count_t stage, const char *fmt, fmt_info_t *info)
{
	ret_token_t ret = RET_TOKEN_ERROR;

	switch (stage) {
	case STAGE_START:
		ret = check_start(fmt, info);
		break;
	case STAGE_SPECIFIER_START:
		ret = check_specifier_start(fmt, info);
		break;
	case STAGE_ALIGN:
		ret = check_align(fmt, info);
		break;
	case STAGE_SIGN:
		ret = check_sign(fmt, info);
		break;
	case STAGE_ALTERNATIVE:
		ret = check_alternative(fmt, info);
		break;
	case STAGE_ZERO_PADDING:
		ret = check_zeropadding(fmt, info);
		break;
	case STAGE_MINWIDTH:
		ret = check_minwidth(fmt, info);
		break;
	case STAGE_PRECISE:
		ret = check_precise(fmt, info);
		break;
	case STAGE_TYPE:
		ret = check_type(fmt, info);
		break;
	case STAGE_END:
		ret = check_end(fmt, info);
		break;
	default:
		// Nothing to do
		break;
	}

	return ret;
}

// Process to the next format descriptor, then construct the format information
// structure, return it and the length of character consumed in format string,
// and the literal character length, which can be directly copy to output
// buffer.
static inline error_t
get_next_fmt(const char *fmt, fmt_info_t *info, size_t *consumed_len,
	     size_t *literal_len, bool *end)
{
	index_t	    idx	      = 0U;
	error_t	    ret	      = OK;
	count_t	    stage     = STAGE_START;
	ret_token_t ret_check = RET_TOKEN_ERROR;

	while (fmt[idx] != '\0') {
		ret_check = check_token(stage, fmt + idx, info);

		switch (ret_check) {
		case RET_TOKEN_NEXT_CHAR:
			idx++;
			break;

		case RET_TOKEN_NEXT_STAGE:
			stage++;
			break;

		case RET_TOKEN_STOP:
			*consumed_len = (size_t)idx + 1U;
			return OK;

		case RET_TOKEN_FOUND:
			if (stage == STAGE_START) {
				*literal_len = idx;
			}

			idx++;
			stage++;
			break;

		case RET_TOKEN_ERROR:
		default:
			return ERROR_STRING_INVALID_FORMAT;
		}
	}

	if (fmt[idx] == '\0') {
		// Nothing found
		if (stage == STAGE_START) {
			*literal_len  = idx;
			*consumed_len = (size_t)idx + 1U;
		}
		*end = true;
	}

	return ret;
}

// Generate string based on argument & format information, write these
// characters into output buffer. If the size is bigger than buffer size,
// return STRING_TRUNCATED. The len should contain the actually bytes written in
// the output buffer.
static inline error_t
gen_str(char *buf, size_t size, fmt_info_t *info, register_t arg, size_t *len)
{
	error_t ret	  = OK;
	size_t	remaining = size;

	switch (info->type) {
	case VAR_TYPE_BIN:
		ret = itoa(buf, &remaining, (uint64_t)arg, 2, info, true);
		break;
	case VAR_TYPE_DEC:
		ret = sitoa(buf, &remaining, (int64_t)arg, 10, info);
		break;
	case VAR_TYPE_OCTAL:
		ret = itoa(buf, &remaining, (uint64_t)arg, 8, info, true);
		break;
	case VAR_TYPE_LOW_HEX:
		ret = itoa(buf, &remaining, (uint64_t)arg, 16, info, true);
		break;
	case VAR_TYPE_STRING:
		ret = stringtoa(buf, &remaining, (char *)arg, info);
		break;
	case VAR_TYPE_NONE:
	default:
		ret = ERROR_STRING_INVALID_FORMAT;
		break;
	}

	*len = size - remaining;
	return ret;
}

size_result_t
snprint(char *str, size_t size, const char *format, register_t arg0,
	register_t arg1, register_t arg2, register_t arg3, register_t arg4)
{
	const char *fmt = format;
	// Current buffer pointer, increasing while filling strings into
	char	  *buf		     = str;
	error_t	   ret		     = OK;
	size_t	   remaining	     = size - 1U; // space for terminating null
	register_t args[MAX_ARG_CNT] = { arg0, arg1, arg2, arg3, arg4 };
	index_t	   arg_idx	     = 0U;
	bool	   end		     = false;

	while (remaining != 0U) {
		fmt_info_t info = { 0 };
		size_t	   p;
		size_t	   consumed_len = 0U;
		size_t	   literal_len	= 0U;

		// Handle the next argument, return the format information and
		// the length consumed in the input format string
		ret = get_next_fmt(fmt, &info, &consumed_len, &literal_len,
				   &end);
		if (ret != OK) {
			break;
		}

		// Copy literal characters to output buffer
		p = util_min(literal_len, remaining);
		if (p > 0U) {
			(void)memcpy(buf, fmt, p);
		}

		fmt += consumed_len;
		buf += p;
		remaining -= p;

		// Not enough space for the fmt chars
		if ((remaining + p) < literal_len) {
			ret = ERROR_STRING_TRUNCATED;
			break;
		}

		if (info.type != VAR_TYPE_NONE) {
			if (arg_idx == MAX_ARG_CNT) {
				// Exceeded number of placeholders
				ret = ERROR_STRING_MISSING_PLACEHOLDER;
				break;
			}

			// Produce output for the current formatter substring
			ret = gen_str(buf, remaining, &info, args[arg_idx],
				      &consumed_len);
			if (ret == OK) {
				assert(consumed_len <= remaining);
				// Step output buffer
				buf = buf + consumed_len;
				remaining -= consumed_len;
				// Proceed to the next argument
				arg_idx++;
			}
		}

		// End processing on any error
		if (end || (ret != OK)) {
			break;
		}
	}

	// Add the terminator
	*buf = '\0';

	size_t written;
	if (ret == ERROR_STRING_TRUNCATED) {
		written = size;
	} else {
		written = (size - 1U) - remaining;
	}
	return (size_result_t){ .e = ret, .r = written };
}

```

`hyp/misc/prng_hw/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

interface prng

types prng_api.tc
source prng_api.c

```

`hyp/misc/prng_hw/prng_api.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#if defined(HYPERCALLS)
extend hyp_api_flags0 bitfield {
	delete	prng;
	10	prng	bool = 1;
};
#endif

extend thread object {
	prng_last_read	type ticks_t;
};

```

`hyp/misc/prng_hw/src/prng_api.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>

#include <hypcall_def.h>

#include <platform_prng.h>
#include <platform_timer.h>
#include <thread.h>
#include <util.h>

hypercall_prng_get_entropy_result_t
hypercall_prng_get_entropy(count_t num_bytes)
{
	hypercall_prng_get_entropy_result_t ret = { 0 };

	if ((num_bytes == 0U) || (num_bytes > (sizeof(uint32_t) * 4U))) {
		ret.error = ERROR_ARGUMENT_SIZE;
		goto out;
	}
	if (!util_is_baligned(num_bytes, sizeof(uint32_t))) {
		ret.error = ERROR_ARGUMENT_ALIGNMENT;
		goto out;
	}

	ret.error = OK;

	thread_t *thread = thread_get_self();
	ticks_t	  now	 = platform_timer_get_current_ticks();

	// The bottom two bits encode the number reads per window, to permit up
	// to 128*4 (512-bits) to be read within the rate-limit window.
	ticks_t last_read = thread->prng_last_read & ~util_mask(2);
	assert(now >= last_read);

	count_t read_count = (count_t)(thread->prng_last_read & util_mask(2));

	// Read rate-limit window is 33ms per thread to reduce DoS.
	if ((now - last_read) < platform_timer_convert_ns_to_ticks(33000000U)) {
		if (read_count == util_mask(2)) {
			ret.error = ERROR_BUSY;
			goto out;
		}
		read_count++;
		thread->prng_last_read = last_read | read_count;
	} else {
		thread->prng_last_read = now & ~util_mask(2);
	}

	if (num_bytes >= sizeof(uint32_t)) {
		error_t err = platform_get_random32(&ret.data0);

		if (err != OK) {
			ret.error = err;
			goto out;
		}
	}
	if (num_bytes >= (2U * sizeof(uint32_t))) {
		error_t err = platform_get_random32(&ret.data1);

		if (err != OK) {
			ret.error = err;
			goto out;
		}
	}
	if (num_bytes >= (3U * sizeof(uint32_t))) {
		error_t err = platform_get_random32(&ret.data2);

		if (err != OK) {
			ret.error = err;
			goto out;
		}
	}
	if (num_bytes >= (4U * sizeof(uint32_t))) {
		error_t err = platform_get_random32(&ret.data3);

		if (err != OK) {
			ret.error = err;
			goto out;
		}
	}
out:
	// On any error, don't return any data
	if (ret.error != OK) {
		ret.data0 = 0U;
		ret.data1 = 0U;
		ret.data2 = 0U;
		ret.data3 = 0U;
	}
	return ret;
}

```

`hyp/misc/prng_simple/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

interface prng

events prng_simple.ev
events chacha20_test.ev

local_include
source prng_simple.c chacha20.c chacha20_test.c

```

`hyp/misc/prng_simple/chacha20_test.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

module prng_simple

#if defined(UNIT_TESTS)

subscribe tests_start
	handler tests_chacha20_start()

#endif

```

`hyp/misc/prng_simple/include/chacha20.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// Block function of the chacha20 cipher
void
chacha20_block(const uint32_t (*key)[8], uint32_t counter,
	       const uint32_t (*nonce)[3], uint32_t (*out)[16]);

```

`hyp/misc/prng_simple/prng_simple.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

module prng_simple

subscribe boot_runtime_first_init()
	require_preempt_disabled

subscribe boot_hypervisor_start()

```

`hyp/misc/prng_simple/src/chacha20.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <hyptypes.h>
#include <string.h>

#include "chacha20.h"

// Subset of ChaCha20 cipher (block generation) for DRBG.
// Implementation based on RFC8439

static const uint32_t chacha20_const[4] = { 0x61707865U, 0x3320646eU,
					    0x79622d32U, 0x6b206574U };

static uint32_t
rotl32(uint32_t x, index_t shift)
{
	return (x << shift) | (x >> (32U - shift));
}

static void
Qround(uint32_t (*state)[16], index_t a, index_t b, index_t c, index_t d)
{
	(*state)[a] += (*state)[b];
	(*state)[d] ^= (*state)[a];
	(*state)[d] = rotl32((*state)[d], 16U);
	(*state)[c] += (*state)[d];
	(*state)[b] ^= (*state)[c];
	(*state)[b] = rotl32((*state)[b], 12U);
	(*state)[a] += (*state)[b];
	(*state)[d] ^= (*state)[a];
	(*state)[d] = rotl32((*state)[d], 8U);
	(*state)[c] += (*state)[d];
	(*state)[b] ^= (*state)[c];
	(*state)[b] = rotl32((*state)[b], 7U);
}

static void
chacha20_inner_block(uint32_t (*state)[16])
{
	Qround(state, 0U, 4U, 8U, 12U);
	Qround(state, 1U, 5U, 9U, 13U);
	Qround(state, 2U, 6U, 10U, 14U);
	Qround(state, 3U, 7U, 11U, 15U);
	Qround(state, 0U, 5U, 10U, 15U);
	Qround(state, 1U, 6U, 11U, 12U);
	Qround(state, 2U, 7U, 8U, 13U);
	Qround(state, 3U, 4U, 9U, 14U);
}

void
chacha20_block(const uint32_t (*key)[8], uint32_t counter,
	       const uint32_t (*nonce)[3], uint32_t (*out)[16])
{
	count_t i;

	// Setup output with input
	for (i = 0U; i < 4U; i++) {
		(*out)[i] = chacha20_const[i];
	}
	for (i = 0U; i < 8U; i++) {
		(*out)[i + 4U] = (*key)[i];
	}
	(*out)[12] = counter;
	for (i = 0U; i < 3U; i++) {
		(*out)[i + 13U] = (*nonce)[i];
	}

	// Run 20 rounds (10 column interleaved with 10 diagonal rounds)
	for (i = 0U; i < 10U; i++) {
		chacha20_inner_block(out);
	}

	// Add the original state to the result
	for (i = 0U; i < 4U; i++) {
		(*out)[i] += chacha20_const[i];
	}
	for (i = 0U; i < 8U; i++) {
		(*out)[4U + i] += (*key)[i];
	}
	(*out)[12] += counter;
	for (i = 0U; i < 3U; i++) {
		(*out)[13U + i] += (*nonce)[i];
	}
}

```

`hyp/misc/prng_simple/src/chacha20_test.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#if defined(UNIT_TESTS)
#include <hyptypes.h>

#include <compiler.h>
#include <log.h>
#include <trace.h>

#include "chacha20.h"
#include "event_handlers.h"

bool
tests_chacha20_start(void)
{
	// Test vectors from RFC 8439
	static const uint32_t key[8] = { 0x03020100U, 0x07060504U, 0x0b0a0908U,
					 0x0f0e0d0cU, 0x13121110U, 0x17161514U,
					 0x1b1a1918U, 0x1f1e1d1cU };
	static const uint32_t nonce[3] = { 0x09000000U, 0x4a000000U,
					   0x00000000U };
	uint32_t	      counter  = 1U;

	uint32_t out[16];

	uint32_t i;

	chacha20_block(&key, counter, &nonce, &out);

	static const uint32_t expected[16] = {
		0xe4e7f110U, 0x15593bd1U, 0x1fdd0f50U, 0xc47120a3U,
		0xc7f4d1c7U, 0x0368c033U, 0x9aaa2204U, 0x4e6cd4c3U,
		0x466482d2U, 0x09aa9f07U, 0x05d7c214U, 0xa2028bd9U,
		0xd19c12b5U, 0xb94e16deU, 0xe883d0cbU, 0x4e3c50a2
	};

	bool fail = false;

	for (i = 0U; i < 16U; i++) {
		if (out[i] != expected[i]) {
			fail = true;
			break;
		}
	}

	if (fail) {
		LOG(ERROR, INFO, "chacha20 self-test failed");
	}

	return fail;
}

#else

extern char unused;

#endif

```

`hyp/misc/prng_simple/src/prng_simple.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// This PRNG implements a "fast-key-erasure RNG" as described by D.J.Bernstein
// https://blog.cr.yp.to/20170723-random.html
//
// The algorithm ensures that the RNG won't contribute to any failure of
// forward security of its clients. Random data is generated into a buffer
// using a key, then the key used is immediately destroyed, and a new key from
// the first output block is created.
//
// Requests for randomness return data from the buffer. When the buffer is
// exhausted, new randomness is generated, with another new key being generated
// as described above. Additionally, the random bytes returned are cleared from
// the buffer for similar forward security reasons.
//
// This implementation uses the block function from the ChaCha20 stream cipher
// which is used to generate a pseudo-random bitstream in counter mode, and is
// much faster than alternative approaches, such as hash/HMAC based DRGBs, and
// counter-cipher schemes such as AES-CTR-DRBG (which don't immediately destroy
// the key).
//
// Finally, randomness from a HW RNG is added to the key periodically. An
// update timestamp is maintained, and when requesting randomness, if the last
// update was more than 5 minutes go, new randomness is added.

#include <assert.h>
#include <hyptypes.h>
#include <string.h>

#include <hypregisters.h>

#include <bootmem.h>
#include <compiler.h>
#include <log.h>
#include <panic.h>
#include <platform_prng.h>
#include <platform_timer.h>
#include <prng.h>
#include <spinlock.h>
#include <trace.h>
#include <util.h>

#include <asm/cache.h>
#include <asm/cpu.h>

#include "chacha20.h"
#include "event_handlers.h"

#define WORD_BITS	   32U
#define BLOCK_WORDS	   (512U / WORD_BITS)
#define KEY_WORDS	   (256U / WORD_BITS)
#define BUFFER_KEY_OFFSET  0U
#define BUFFER_DATA_OFFSET KEY_WORDS // first bytes are reserved for the key
#define BUFFER_BLOCKS	   4U
#define BUFFER_WORDS	   (BUFFER_BLOCKS * BLOCK_WORDS)

#define REKEY_TIMEOUT_NS ((uint64_t)300U * 1000000000U) // 300 seconds

extern uint32_t hypervisor_prng_seed[KEY_WORDS];
extern uint64_t hypervisor_prng_nonce;

#define CACHE_LINE_SIZE (1 << CPU_L1D_LINE_BITS)

typedef struct {
	uint32_t alignas(CACHE_LINE_SIZE) key[KEY_WORDS];

	ticks_t key_timestamp;
	ticks_t key_timeout;
	count_t pool_index; // index in units of words

	uint32_t nonce[3];

	uint32_t alignas(CACHE_LINE_SIZE)
		entropy_pool[BUFFER_BLOCKS][BLOCK_WORDS];
} prng_data_t;

static bool prng_initialized = false;

static spinlock_t   prng_lock;
static prng_data_t *prng_data PTR_PROTECTED_BY(prng_lock);

void
prng_simple_handle_boot_runtime_first_init(void)
{
	spinlock_init(&prng_lock);
	spinlock_acquire_nopreempt(&prng_lock);

	void_ptr_result_t ret;

	// Allocate boot entropy pool
	ret = bootmem_allocate(sizeof(prng_data_t), alignof(prng_data_t));
	if (ret.e != OK) {
		panic("unable to allocate boot entropy pool");
	}

	prng_data = (prng_data_t *)ret.r;
	assert(prng_data != NULL);

	(void)memset_s(prng_data, sizeof(*prng_data), 0, sizeof(*prng_data));

	prng_data->pool_index = BUFFER_WORDS; // Buffer is Empty
	(void)memscpy(&prng_data->key, sizeof(prng_data->key),
		      hypervisor_prng_seed, sizeof(prng_data->key));

	// Ensure no stale copies remain in ram
	assert(hypervisor_prng_seed != NULL);
	(void)memset_s(hypervisor_prng_seed, sizeof(hypervisor_prng_seed), 0,
		       sizeof(hypervisor_prng_seed));
	CACHE_CLEAN_INVALIDATE_OBJECT(hypervisor_prng_seed);

	prng_data->key_timestamp = platform_timer_get_current_ticks();
	prng_data->key_timeout =
		platform_timer_convert_ns_to_ticks(REKEY_TIMEOUT_NS);

	uint32_t serial[4];

	error_t err = platform_get_serial(serial);
	if (err != OK) {
		panic("unable to get serial number");
	}

	prng_data->nonce[0] = serial[0];
	prng_data->nonce[1] = serial[1];
	prng_data->nonce[2] = serial[2];

	// Add in some chip specific noise
	prng_data->nonce[1] ^= (uint32_t)(hypervisor_prng_nonce & 0xffffffffU);
	prng_data->nonce[2] ^= (uint32_t)(hypervisor_prng_nonce >> 32);

	// Ensure no stale copies remain in ram
	(void)memset_s(&hypervisor_prng_nonce, sizeof(hypervisor_prng_nonce), 0,
		       sizeof(hypervisor_prng_nonce));
	CACHE_CLEAN_INVALIDATE_OBJECT(hypervisor_prng_nonce);

	prng_initialized = true;
	spinlock_release_nopreempt(&prng_lock);
}

void
prng_simple_handle_boot_hypervisor_start(void)
{
	// FIXME:
	// Post boot prng_data protection
	//  * allocate an unmapped 4K page for the prng_data
	//  * Aarch64 PAN implementation:
	//    - map the page with EL2&0 user-rw permissions
	//    - enable PAN to access the prng_data
	//  * copy the boot prng_data to the new page and zero it afterwards
	//  * update prng_data pointer to new location
}

static bool
add_platform_entropy(void) REQUIRE_SPINLOCK(prng_lock)
{
	error_t ret;
	bool	success;
	platform_prng_data256_t new;

	ret = platform_get_entropy(&new);
	if (ret == OK) {
		// mix in new key entropy
		prng_data->key[0] ^= new.word[0];
		prng_data->key[1] ^= new.word[1];
		prng_data->key[2] ^= new.word[2];
		prng_data->key[3] ^= new.word[3];
		prng_data->key[4] ^= new.word[4];
		prng_data->key[5] ^= new.word[5];
		prng_data->key[6] ^= new.word[6];
		prng_data->key[7] ^= new.word[7];

		// Ensure no stale copy remains on the stack
		(void)memset_s(&new, sizeof(new), 0, sizeof(new));
		CACHE_CLEAN_INVALIDATE_OBJECT(new);

		success = true;
	} else if (ret == ERROR_BUSY) {
		LOG(DEBUG, INFO, "platform_get_entropy busy");
		success = false;
	} else {
		LOG(ERROR, WARN, "platform_get_entropy err: {:d}",
		    (register_t)ret);
		panic("Failed to get platform_get_entropy");
	}

	return success;
}

static void
prng_update(void) REQUIRE_SPINLOCK(prng_lock)
{
	uint32_t counter = 1U;
	count_t	 i;

	ticks_t now = platform_timer_get_current_ticks();

	// Add new key entropy periodically, this is not critical if platform
	// is busy, we'll try again next time.
	if ((now - prng_data->key_timestamp) > prng_data->key_timeout) {
		if (add_platform_entropy()) {
			prng_data->key_timestamp = now;
		}
	}

	// Generate a new set of blocks
	for (i = 0U; i < BUFFER_BLOCKS; i++) {
		chacha20_block(&prng_data->key, counter, &prng_data->nonce,
			       &prng_data->entropy_pool[i]);
		counter++;
	}
	// Nonce must not be repeated for the same key! Even though we re-key
	// below, we increment the nonce anyway!
	prng_data->nonce[0] += 1U;
	if (prng_data->nonce[0] == 0U) {
		// Addition overflow of nonce[0]
		prng_data->nonce[1] += 1U;
		if (prng_data->nonce[1] == 0U) {
			// Addition overflow of nonce[1]
			prng_data->nonce[2] += 1U;
		}
	}

	// Fast key update from block 0
	(void)memscpy(prng_data->key, sizeof(prng_data->key),
		      &prng_data->entropy_pool[0],
		      sizeof(prng_data->entropy_pool[0]));
	// Ensure no stale copies remain in ram
	CACHE_CLEAN_FIXED_RANGE(prng_data->key, 32U);
	// Clear the used bytes just in case
	(void)memset_s(&prng_data->entropy_pool[0],
		       sizeof(prng_data->entropy_pool[0]), 0,
		       BUFFER_DATA_OFFSET * sizeof(uint32_t));
	// Ensure no stale copies remain in ram
	CACHE_CLEAN_FIXED_RANGE(&prng_data->entropy_pool[0],
				BUFFER_DATA_OFFSET * sizeof(uint32_t));

	prng_data->pool_index = BUFFER_DATA_OFFSET;
}

uint64_result_t
prng_get64(void)
{
	uint64_result_t ret;

	assert(prng_initialized);

	spinlock_acquire(&prng_lock);

	count_t index = prng_data->pool_index;

	if (index > (BUFFER_WORDS - (64U / WORD_BITS))) {
		// Not enough buffered randomness, get more
		prng_update();
		index = prng_data->pool_index;
	}
	prng_data->pool_index += (64U / WORD_BITS);

	index_t	  block = index / BLOCK_WORDS;
	index_t	  word	= index % BLOCK_WORDS;
	uint32_t *data	= &prng_data->entropy_pool[block][word];

	ret.r = data[0];
	ret.r |= (uint64_t)data[1] << 32;

	ret.e = OK;
	// Pointer difference in bytes
	ptrdiff_t len = (char *)data - (char *)prng_data->entropy_pool[0];

	assert(len >= 0);

	// Clear used bits
	(void)memset_s(data, sizeof(prng_data->entropy_pool) - (size_t)len, 0,
		       sizeof(ret.r));
	// Ensure used bits are cleared from caches
	CACHE_CLEAN_FIXED_RANGE(data, sizeof(ret.r));

	spinlock_release(&prng_lock);

	return ret;
}

```

`hyp/misc/qcbor/build.conf`:

```conf
# © 2022 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

interface qcbor
source qcbor_encode.c UsefulBuf.c

```

`hyp/misc/qcbor/src/UsefulBuf.c`:

```c
/*==============================================================================
 Copyright (c) 2016-2018, The Linux Foundation.
 Copyright (c) 2018-2022, Laurence Lundblade.

Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are
met:
    * Redistributions of source code must retain the above copyright
      notice, this list of conditions and the following disclaimer.
    * Redistributions in binary form must reproduce the above
      copyright notice, this list of conditions and the following
      disclaimer in the documentation and/or other materials provided
      with the distribution.
    * Neither the name of The Linux Foundation nor the names of its
      contributors, nor the name "Laurence Lundblade" may be used to
      endorse or promote products derived from this software without
      specific prior written permission.

THIS SOFTWARE IS PROVIDED "AS IS" AND ANY EXPRESS OR IMPLIED
WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF
MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT
ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS
BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR
BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,
WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE
OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN
IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 =============================================================================*/

/*=============================================================================
 FILE:  UsefulBuf.c

 DESCRIPTION:  General purpose input and output buffers

 EDIT HISTORY FOR FILE:

 This section contains comments describing changes made to the module.
 Notice that changes are listed in reverse chronological order.

 when        who          what, where, why
 --------    ----         ---------------------------------------------------
 19/12/2022  llundblade   Don't pass NULL to memmove when adding empty data.
 4/11/2022    llundblade  Add GetOutPlace and Advance to UsefulOutBuf
 3/6/2021     mcr/llundblade  Fix warnings related to --Wcast-qual
 01/28/2020  llundblade   Refine integer signedness to quiet static analysis.
 01/08/2020  llundblade   Documentation corrections & improved code formatting.
 11/08/2019  llundblade   Re check pointer math and update comments
 3/6/2019    llundblade   Add UsefulBuf_IsValue()
 09/07/17    llundbla     Fix critical bug in UsefulBuf_Find() -- a read off
			  the end of memory when the bytes to find is longer
			  than the bytes to search.
 06/27/17    llundbla     Fix UsefulBuf_Compare() bug. Only affected comparison
			  for < or > for unequal length buffers.  Added
			  UsefulBuf_Set() function.
 05/30/17    llundbla     Functions for NULL UsefulBufs and const / unconst
 11/13/16    llundbla     Initial Version.

 ============================================================================*/

#define ENABLE_DECODE_ROUTINES
#include "qcbor/UsefulBuf.h"

// used to catch use of uninitialized or corrupted UsefulOutBuf
#define USEFUL_OUT_BUF_MAGIC (0x0B0F)

int
memcmp(const void *s1, const void *s2, size_t n);

/*
 Public function -- see UsefulBuf.h
 */
UsefulBufC
UsefulBuf_CopyOffset(UsefulBuf Dest, size_t uOffset, const UsefulBufC Src)
{
	// Do this with subtraction so it doesn't give erroneous
	// result if uOffset + Src.len overflows
	if (uOffset > Dest.len || Src.len > Dest.len - uOffset) { // uOffset +
								  // Src.len >
								  // Dest.len
		return NULLUsefulBufC;
	}

	memcpy((uint8_t *)Dest.ptr + uOffset, Src.ptr, Src.len);

	return (UsefulBufC){ Dest.ptr, Src.len + uOffset };
}

/*
   Public function -- see UsefulBuf.h
 */
int
UsefulBuf_Compare(const UsefulBufC UB1, const UsefulBufC UB2)
{
	// use the comparisons rather than subtracting lengths to
	// return an int instead of a size_t
	if (UB1.len < UB2.len) {
		return -1;
	} else if (UB1.len > UB2.len) {
		return 1;
	} // else UB1.len == UB2.len

	return memcmp(UB1.ptr, UB2.ptr, UB1.len);
}

/*
 Public function -- see UsefulBuf.h
 */
size_t
UsefulBuf_IsValue(const UsefulBufC UB, uint8_t uValue)
{
	if (UsefulBuf_IsNULLOrEmptyC(UB)) {
		/* Not a match */
		return 0;
	}

	const uint8_t *const pEnd = (const uint8_t *)UB.ptr + UB.len;
	for (const uint8_t *p = UB.ptr; p < pEnd; p++) {
		if (*p != uValue) {
			/* Byte didn't match */
			/* Cast from signed  to unsigned . Safe because the loop
			 * increments.*/
			return (size_t)(p - (const uint8_t *)UB.ptr);
		}
	}

	/* Success. All bytes matched */
	return SIZE_MAX;
}

/*
 Public function -- see UsefulBuf.h
 */
size_t
UsefulBuf_FindBytes(UsefulBufC BytesToSearch, UsefulBufC BytesToFind)
{
	if (BytesToSearch.len < BytesToFind.len) {
		return SIZE_MAX;
	}

	for (size_t uPos = 0; uPos <= BytesToSearch.len - BytesToFind.len;
	     uPos++) {
		if (!UsefulBuf_Compare(
			    (UsefulBufC){ ((const uint8_t *)BytesToSearch.ptr) +
						  uPos,
					  BytesToFind.len },
			    BytesToFind)) {
			return uPos;
		}
	}

	return SIZE_MAX;
}

/*
 Public function -- see UsefulBuf.h

 Code Reviewers: THIS FUNCTION DOES POINTER MATH
 */
void
UsefulOutBuf_Init(UsefulOutBuf *pMe, UsefulBuf Storage)
{
	pMe->magic = USEFUL_OUT_BUF_MAGIC;
	UsefulOutBuf_Reset(pMe);
	pMe->UB = Storage;

#if 0
   // This check is off by default.

   // The following check fails on ThreadX

    // Sanity check on the pointer and size to be sure we are not
    // passed a buffer that goes off the end of the address space.
    // Given this test, we know that all unsigned lengths less than
    // me->size are valid and won't wrap in any pointer additions
    // based off of pStorage in the rest of this code.
    const uintptr_t ptrM = UINTPTR_MAX - Storage.len;
    if(Storage.ptr && (uintptr_t)Storage.ptr > ptrM) // Check #0
        me->err = 1;
#endif
}

/*
 Public function -- see UsefulBuf.h

 The core of UsefulOutBuf -- put some bytes in the buffer without writing off
			     the end of it.

 Code Reviewers: THIS FUNCTION DOES POINTER MATH

 This function inserts the source buffer, NewData, into the destination
 buffer, me->UB.ptr.

 Destination is represented as:
   me->UB.ptr -- start of the buffer
   me->UB.len -- size of the buffer UB.ptr
   me->data_len -- length of value data in UB

 Source is data:
   NewData.ptr -- start of source buffer
   NewData.len -- length of source buffer

 Insertion point:
   uInsertionPos.

 Steps:

 0. Corruption checks on UsefulOutBuf

 1. Figure out if the new data will fit or not

 2. Is insertion position in the range of valid data?

 3. If insertion point is not at the end, slide data to the right of the
    insertion point to the right

 4. Put the new data in at the insertion position.

 */
void
UsefulOutBuf_InsertUsefulBuf(UsefulOutBuf *pMe, UsefulBufC NewData,
			     size_t uInsertionPos)
{
	if (pMe->err) {
		// Already in error state.
		return;
	}

	/* 0. Sanity check the UsefulOutBuf structure */
	// A "counter measure". If magic number is not the right number it
	// probably means me was not initialized or it was corrupted. Attackers
	// can defeat this, but it is a hurdle and does good with very
	// little code.
	if (pMe->magic != USEFUL_OUT_BUF_MAGIC) {
		pMe->err = 1;
		return; // Magic number is wrong due to uninitalization or
			// corrption
	}

	// Make sure valid data is less than buffer size. This would only occur
	// if there was corruption of me, but it is also part of the checks to
	// be sure there is no pointer arithmatic under/overflow.
	if (pMe->data_len > pMe->UB.len) { // Check #1
		pMe->err = 1;
		// Offset of valid data is off the end of the UsefulOutBuf due
		// to uninitialization or corruption
		return;
	}

	/* 1. Will it fit? */
	// WillItFit() is the same as: NewData.len <= (me->UB.len -
	// me->data_len) Check #1 makes sure subtraction in RoomLeft will not
	// wrap around
	if (!UsefulOutBuf_WillItFit(pMe, NewData.len)) { // Check #2
		// The new data will not fit into the the buffer.
		pMe->err = 1;
		return;
	}

	/* 2. Check the Insertion Position */
	// This, with Check #1, also confirms that uInsertionPos <= me->data_len
	// and that uInsertionPos + pMe->UB.ptr will not wrap around the end of
	// the address space.
	if (uInsertionPos > pMe->data_len) { // Check #3
		// Off the end of the valid data in the buffer.
		pMe->err = 1;
		return;
	}

	/* 3. Slide existing data to the right */
	if (!UsefulOutBuf_IsBufferNULL(pMe)) {
		uint8_t *pSourceOfMove =
			((uint8_t *)pMe->UB.ptr) + uInsertionPos; // PtrMath #1
		size_t uNumBytesToMove =
			pMe->data_len - uInsertionPos; // PtrMath
						       // #2
		uint8_t *pDestinationOfMove =
			pSourceOfMove + NewData.len; // PtrMath
						     // #3

		// To know memmove won't go off end of destination, see PtrMath
		// #4 Use memove because it handles overlapping buffers
		memmove(pDestinationOfMove, pSourceOfMove, uNumBytesToMove);

		/* 4. Put the new data in */
		uint8_t *pInsertionPoint = pSourceOfMove;
		// To know memmove won't go off end of destination, see PtrMath
		// #5
		if (NewData.ptr != NULL) {
			memmove(pInsertionPoint, NewData.ptr, NewData.len);
		}
	}

	pMe->data_len += NewData.len;
}

/*
 Rationale that describes why the above pointer math is safe

 PtrMath #1 will never wrap around over because
    Check #0 in UsefulOutBuf_Init makes sure me->UB.ptr + me->UB.len doesn't
 wrap Check #1 makes sure me->data_len is less than me->UB.len Check #3 makes
 sure uInsertionPos is less than me->data_len

 PtrMath #2 will never wrap around under because
    Check #3 makes sure uInsertionPos is less than me->data_len

 PtrMath #3 will never wrap around over because
    PtrMath #1 is checked resulting in pSourceOfMove being between me->UB.ptr
 and me->UB.ptr + me->data_len Check #2 that NewData.len will fit in the unused
 space left in me->UB

 PtrMath #4 will never wrap under because
    Calculation for extent or memmove is uRoomInDestination  = me->UB.len -
 (uInsertionPos + NewData.len) Check #3 makes sure uInsertionPos is less than
 me->data_len Check #3 allows Check #2 to be refactored as NewData.Len >
 (me->size - uInsertionPos) This algebraically rearranges to me->size >
 uInsertionPos + NewData.len

 PtrMath #5 will never wrap under because
    Calculation for extent of memove is uRoomInDestination = me->UB.len -
 uInsertionPos; Check #1 makes sure me->data_len is less than me->size Check #3
 makes sure uInsertionPos is less than me->data_len
 */

/*
 * Public function for advancing data length. See qcbor/UsefulBuf.h
 */
void
UsefulOutBuf_Advance(UsefulOutBuf *pMe, size_t uAmount)
{
	/* This function is a trimmed down version of
	 * UsefulOutBuf_InsertUsefulBuf(). This could be combined with the
	 * code in UsefulOutBuf_InsertUsefulBuf(), but that would make
	 * UsefulOutBuf_InsertUsefulBuf() bigger and this will be very
	 * rarely used.
	 */

	if (pMe->err) {
		/* Already in error state. */
		return;
	}

	/* 0. Sanity check the UsefulOutBuf structure
	 *
	 * A "counter measure". If magic number is not the right number it
	 * probably means me was not initialized or it was
	 * corrupted. Attackers can defeat this, but it is a hurdle and
	 * does good with very little code.
	 */
	if (pMe->magic != USEFUL_OUT_BUF_MAGIC) {
		pMe->err = 1;
		return; /* Magic number is wrong due to uninitalization or
			   corrption */
	}

	/* Make sure valid data is less than buffer size. This would only
	 * occur if there was corruption of me, but it is also part of the
	 * checks to be sure there is no pointer arithmatic
	 * under/overflow.
	 */
	if (pMe->data_len > pMe->UB.len) { // Check #1
		pMe->err = 1;
		/* Offset of valid data is off the end of the UsefulOutBuf due
		 * to uninitialization or corruption.
		 */
		return;
	}

	/* 1. Will it fit?
	 *
	 * WillItFit() is the same as: NewData.len <= (me->UB.len -
	 * me->data_len) Check #1 makes sure subtraction in RoomLeft will
	 * not wrap around
	 */
	if (!UsefulOutBuf_WillItFit(pMe, uAmount)) { /* Check #2 */
		/* The new data will not fit into the the buffer. */
		pMe->err = 1;
		return;
	}

	pMe->data_len += uAmount;
}

/*
 Public function -- see UsefulBuf.h
 */
UsefulBufC
UsefulOutBuf_OutUBuf(UsefulOutBuf *pMe)
{
	if (pMe->err) {
		return NULLUsefulBufC;
	}

	if (pMe->magic != USEFUL_OUT_BUF_MAGIC) {
		pMe->err = 1;
		return NULLUsefulBufC;
	}

	return (UsefulBufC){ pMe->UB.ptr, pMe->data_len };
}

/*
 Public function -- see UsefulBuf.h

 Copy out the data accumulated in to the output buffer.
 */
UsefulBufC
UsefulOutBuf_CopyOut(UsefulOutBuf *pMe, UsefulBuf pDest)
{
	const UsefulBufC Tmp = UsefulOutBuf_OutUBuf(pMe);
	if (UsefulBuf_IsNULLC(Tmp)) {
		return NULLUsefulBufC;
	}
	return UsefulBuf_Copy(pDest, Tmp);
}

/*
 Public function -- see UsefulBuf.h

 The core of UsefulInputBuf -- consume bytes without going off end of buffer.

 Code Reviewers: THIS FUNCTION DOES POINTER MATH
 */
const void *
UsefulInputBuf_GetBytes(UsefulInputBuf *pMe, size_t uAmount)
{
	// Already in error state. Do nothing.
	if (pMe->err) {
		return NULL;
	}

	if (!UsefulInputBuf_BytesAvailable(pMe, uAmount)) {
		// Number of bytes asked for at current position are more than
		// available
		pMe->err = 1;
		return NULL;
	}

	// This is going to succeed
	const void *const result = ((const uint8_t *)pMe->UB.ptr) + pMe->cursor;
	// Will not overflow because of check using
	// UsefulInputBuf_BytesAvailable()
	pMe->cursor += uAmount;
	return result;
}

```

`hyp/misc/qcbor/src/qcbor_encode.c`:

```c
/*==============================================================================
 Copyright (c) 2016-2018, The Linux Foundation.
 Copyright (c) 2018-2022, Laurence Lundblade.
 Copyright (c) 2021, Arm Limited.
 All rights reserved.

Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are
met:
    * Redistributions of source code must retain the above copyright
      notice, this list of conditions and the following disclaimer.
    * Redistributions in binary form must reproduce the above
      copyright notice, this list of conditions and the following
      disclaimer in the documentation and/or other materials provided
      with the distribution.
    * Neither the name of The Linux Foundation nor the names of its
      contributors, nor the name "Laurence Lundblade" may be used to
      endorse or promote products derived from this software without
      specific prior written permission.

THIS SOFTWARE IS PROVIDED "AS IS" AND ANY EXPRESS OR IMPLIED
WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF
MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT
ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS
BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR
BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,
WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE
OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN
IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 =============================================================================*/

#include "qcbor/qcbor_encode.h"

/**
 * The entire implementation of the QCBOR encoder.
 */

/*
 * == Nesting Tracking ==
 *
 * The following functions and data type QCBORTrackNesting implement
 * the nesting management for encoding.
 *
 * CBOR's two nesting types, arrays and maps, are tracked here. There
 * is a limit of QCBOR_MAX_ARRAY_NESTING to the number of arrays and
 * maps that can be nested in one encoding so the encoding context
 * stays small enough to fit on the stack.
 *
 * When an array/map is opened, pCurrentNesting points to the element
 * in pArrays that records the type, start position and accumulates a
 * count of the number of items added. When closed the start position
 * is used to go back and fill in the type and number of items in the
 * array/map.
 *
 * Encoded output can be a CBOR Sequence (RFC 8742) in which case
 * there is no top-level array or map. It starts out with a string,
 * integer or other non-aggregate type. It may have an array or map
 * other than at the start, in which case that nesting is tracked
 * here.
 *
 * QCBOR has a special feature to allow constructing byte string
 * wrapped CBOR directly into the output buffer, so no extra buffer is
 * needed for byte string wrapping.  This is implemented as nesting
 * with the type CBOR_MAJOR_TYPE_BYTE_STRING and is tracked here. Byte
 * string wrapped CBOR is used by COSE for data that is to be hashed.
 */
static inline void
Nesting_Init(QCBORTrackNesting *pNesting)
{
	/* Assumes pNesting has been zeroed. */
	pNesting->pCurrentNesting = &pNesting->pArrays[0];
	/* Implied CBOR array at the top nesting level. This is never
	 * returned, but makes the item count work correctly.
	 */
	pNesting->pCurrentNesting->uMajorType = CBOR_MAJOR_TYPE_ARRAY;
}

static inline uint8_t
Nesting_Increase(QCBORTrackNesting *pNesting, uint8_t uMajorType, uint32_t uPos)
{
	if (pNesting->pCurrentNesting ==
	    &pNesting->pArrays[QCBOR_MAX_ARRAY_NESTING]) {
		return QCBOR_ERR_ARRAY_NESTING_TOO_DEEP;
	} else {
		pNesting->pCurrentNesting++;
		pNesting->pCurrentNesting->uCount     = 0;
		pNesting->pCurrentNesting->uStart     = uPos;
		pNesting->pCurrentNesting->uMajorType = uMajorType;
		return QCBOR_SUCCESS;
	}
}

static inline void
Nesting_Decrease(QCBORTrackNesting *pNesting)
{
	if (pNesting->pCurrentNesting > &pNesting->pArrays[0]) {
		pNesting->pCurrentNesting--;
	}
}

static inline uint8_t
Nesting_Increment(QCBORTrackNesting *pNesting)
{
#ifndef QCBOR_DISABLE_ENCODE_USAGE_GUARDS
	if (1 >= QCBOR_MAX_ITEMS_IN_ARRAY - pNesting->pCurrentNesting->uCount) {
		return QCBOR_ERR_ARRAY_TOO_LONG;
	}
#endif /* QCBOR_DISABLE_ENCODE_USAGE_GUARDS */

	pNesting->pCurrentNesting->uCount++;

	return QCBOR_SUCCESS;
}

static inline void
Nesting_Decrement(QCBORTrackNesting *pNesting)
{
	/* No error check for going below 0 here needed because this
	 * is only used by QCBOREncode_CancelBstrWrap() and it checks
	 * the nesting level before calling this. */
	pNesting->pCurrentNesting->uCount--;
}

static inline uint16_t
Nesting_GetCount(QCBORTrackNesting *pNesting)
{
	/* The nesting count recorded is always the actual number of
	 * individual data items in the array or map. For arrays CBOR uses
	 * the actual item count. For maps, CBOR uses the number of pairs.
	 * This function returns the number needed for the CBOR encoding,
	 * so it divides the number of items by two for maps to get the
	 * number of pairs.
	 */
	if (pNesting->pCurrentNesting->uMajorType == CBOR_MAJOR_TYPE_MAP) {
		/* Cast back to uint16_t after integer promotion from bit shift
		 */
		return (uint16_t)(pNesting->pCurrentNesting->uCount >> 1);
	} else {
		return pNesting->pCurrentNesting->uCount;
	}
}

static inline uint32_t
Nesting_GetStartPos(QCBORTrackNesting *pNesting)
{
	return pNesting->pCurrentNesting->uStart;
}

#ifndef QCBOR_DISABLE_ENCODE_USAGE_GUARDS
static inline uint8_t
Nesting_GetMajorType(QCBORTrackNesting *pNesting)
{
	return pNesting->pCurrentNesting->uMajorType;
}

static inline bool
Nesting_IsInNest(QCBORTrackNesting *pNesting)
{
	return pNesting->pCurrentNesting == &pNesting->pArrays[0] ? false
								  : true;
}
#endif /* QCBOR_DISABLE_ENCODE_USAGE_GUARDS */

/*
 * == Major CBOR Types ==
 *
 * Encoding of the major CBOR types is by these functions:
 *
 * CBOR Major Type  Public Function
 * 0                QCBOREncode_AddUInt64()
 * 0, 1             QCBOREncode_AddUInt64(), QCBOREncode_AddInt64()
 * 2, 3             QCBOREncode_AddBuffer()
 * 4, 5             QCBOREncode_OpenMapOrArray(), QCBOREncode_CloseMapOrArray(),
 *                  QCBOREncode_OpenMapOrArrayIndefiniteLength(),
 *                  QCBOREncode_CloseMapOrArrayIndefiniteLength()
 * 6                QCBOREncode_AddTag()
 * 7                QCBOREncode_AddDouble(), QCBOREncode_AddFloat(),
 *                  QCBOREncode_AddDoubleNoPreferred(),
 *                  QCBOREncode_AddFloatNoPreferred(), QCBOREncode_AddType7()
 *
 * Additionally, encoding of decimal fractions and bigfloats is by
 * QCBOREncode_AddExponentAndMantissa() and byte strings that wrap
 * encoded CBOR are handled by QCBOREncode_OpenMapOrArray() and
 * QCBOREncode_CloseBstrWrap2().
 *
 *
 * == Error Tracking Plan ==
 *
 * Errors are tracked internally and not returned until
 * QCBOREncode_Finish() or QCBOREncode_GetErrorState() is called. The
 * CBOR errors are in me->uError.  UsefulOutBuf also tracks whether
 * the buffer is full or not in its context.  Once either of these
 * errors is set they are never cleared. Only QCBOREncode_Init()
 * resets them. Or said another way, they must never be cleared or
 * we'll tell the caller all is good when it is not.
 *
 * Only one error code is reported by QCBOREncode_Finish() even if
 * there are multiple errors. The last one set wins. The caller might
 * have to fix one error to reveal the next one they have to fix.
 * This is OK.
 *
 * The buffer full error tracked by UsefulBuf is only pulled out of
 * UsefulBuf in QCBOREncode_Finish() so it is the one that usually
 * wins.  UsefulBuf will never go off the end of the buffer even if it
 * is called again and again when full.
 *
 * QCBOR_DISABLE_ENCODE_USAGE_GUARDS disables about half of the error
 * checks here to reduce code size by about 150 bytes leaving only the
 * checks for size to avoid buffer overflow. If the calling code is
 * completely correct, checks are completely unnecessary.  For
 * example, there is no need to check that all the opens are matched
 * by a close.
 *
 * QCBOR_DISABLE_ENCODE_USAGE_GUARDS also disables the check for more
 * than QCBOR_MAX_ITEMS_IN_ARRAY in an array. Since
 * QCBOR_MAX_ITEMS_IN_ARRAY is very large (65,535) it is very unlikely
 * to be reached. If it is reached, the count will wrap around to zero
 * and CBOR that is not well formed will be produced, but there will
 * be no buffers overrun and new security issues in the code.
 *
 * The 8 errors returned here fall into three categories:
 *
 * Sizes
 *   QCBOR_ERR_BUFFER_TOO_LARGE        -- Encoded output exceeded UINT32_MAX
 *   QCBOR_ERR_BUFFER_TOO_SMALL        -- Output buffer too small
 *   QCBOR_ERR_ARRAY_NESTING_TOO_DEEP  -- Nesting > QCBOR_MAX_ARRAY_NESTING1
 *   QCBOR_ERR_ARRAY_TOO_LONG          -- Too many items added to an array/map
 * [1]
 *
 * Nesting constructed incorrectly
 *   QCBOR_ERR_TOO_MANY_CLOSES         -- More close calls than opens [1]
 *   QCBOR_ERR_CLOSE_MISMATCH          -- Type of close does not match open [1]
 *   QCBOR_ERR_ARRAY_OR_MAP_STILL_OPEN -- Finish called without enough closes
 * [1]
 *
 * Would generate not-well-formed CBOR
 *   QCBOR_ERR_ENCODE_UNSUPPORTED      -- Simple type between 24 and 31 [1]
 *
 * [1] indicated disabled by QCBOR_DISABLE_ENCODE_USAGE_GUARDS
 */

/*
 Public function for initialization. See qcbor/qcbor_encode.h
 */
void
QCBOREncode_Init(QCBOREncodeContext *me, UsefulBuf Storage)
{
	memset(me, 0, sizeof(QCBOREncodeContext));
	UsefulOutBuf_Init(&(me->OutBuf), Storage);
	Nesting_Init(&(me->nesting));
}

/*
 * Public function to encode a CBOR head. See qcbor/qcbor_encode.h
 */
UsefulBufC
QCBOREncode_EncodeHead(UsefulBuf buffer, uint8_t uMajorType, uint8_t uMinLen,
		       uint64_t uArgument)
{
	/*
	 * == Description of the CBOR Head ==
	 *
	 *    The head of a CBOR data item
	 *  +---+-----+ +--------+ +--------+ +--------+      +--------+
	 *  |M T|  A R G U M E N T . . .                               |
	 *  +---+-----+ +--------+ +--------+ +--------+ ...  +--------+
	 *
	 * Every CBOR data item has a "head". It is made up of the "major
	 * type" and the "argument".
	 *
	 * The major type indicates whether the data item is an integer,
	 * string, array or such. It is encoded in 3 bits giving it a range
	 * from 0 to 7.  0 indicates the major type is a positive integer,
	 * 1 a negative integer, 2 a byte string and so on.
	 *
	 * These 3 bits are the first part of the "initial byte" in a data
	 * item.  Every data item has an initial byte, and some only have
	 * the initial byte.
	 *
	 * The argument is essentially a number between 0 and UINT64_MAX
	 * (18446744073709551615). This number is interpreted to mean
	 * different things for the different major types. For major type
	 * 0, a positive integer, it is value of the data item. For major
	 * type 2, a byte string, it is the length in bytes of the byte
	 * string. For major type 4, an array, it is the number of data
	 * items in the array.
	 *
	 * Special encoding is used so that the argument values less than
	 * 24 can be encoded very compactly in the same byte as the major
	 * type is encoded. When the lower 5 bits of the initial byte have
	 * a value less than 24, then that is the value of the argument.
	 *
	 * If the lower 5 bits of the initial byte are less than 24, then
	 * they are the value of the argument. This allows integer values 0
	 * - 23 to be CBOR encoded in just one byte.
	 *
	 * When the value of lower 5 bits are 24, 25, 26, or 27 the
	 * argument is encoded in 1, 2, 4 or 8 bytes following the initial
	 * byte in network byte order (bit endian). The cases when it is
	 * 28, 29 and 30 are reserved for future use. The value 31 is a
	 * special indicator for indefinite length strings, arrays and
	 * maps.
	 *
	 * The lower 5 bits are called the "additional information."
	 *
	 * Thus the CBOR head may be 1, 2, 3, 5 or 9 bytes long.
	 *
	 * It is legal in CBOR to encode the argument using any of these
	 * lengths even if it could be encoded in a shorter length. For
	 * example it is legal to encode a data item representing the
	 * positive integer 0 in 9 bytes even though it could be encoded in
	 * only 0. This is legal to allow for for very simple code or even
	 * hardware-only implementations that just output a register
	 * directly.
	 *
	 * CBOR defines preferred encoding as the encoding of the argument
	 * in the smallest number of bytes needed to encode it.
	 *
	 * This function takes the major type and argument as inputs and
	 * outputs the encoded CBOR head for them. It does conversion to
	 * network byte order.  It implements CBOR preferred encoding,
	 * outputting the shortest representation of the argument.
	 *
	 * == Endian Conversion ==
	 *
	 * This code does endian conversion without hton() or knowing the
	 * endianness of the machine by using masks and shifts. This avoids
	 * the dependency on hton() and the mess of figuring out how to
	 * find the machine's endianness.
	 *
	 * This is a good efficient implementation on little-endian
	 * machines.  A faster and smaller implementation is possible on
	 * big-endian machines because CBOR/network byte order is
	 * big-endian. However big-endian machines are uncommon.
	 *
	 * On x86, this is about 150 bytes instead of 500 bytes for the
	 * original, more formal unoptimized code.
	 *
	 * This also does the CBOR preferred shortest encoding for integers
	 * and is called to do endian conversion for floats.
	 *
	 * It works backwards from the least significant byte to the most
	 * significant byte.
	 *
	 * == Floating Point ==
	 *
	 * When the major type is 7 and the 5 lower bits have the values
	 * 25, 26 or 27, the argument is a floating-point number that is
	 * half, single or double-precision. Note that it is not the
	 * conversion from a floating-point value to an integer value like
	 * converting 0x00 to 0.00, it is the interpretation of the bits in
	 * the argument as an IEEE 754 float-point number.
	 *
	 * Floating-point numbers must be converted to network byte
	 * order. That is accomplished here by exactly the same code that
	 * converts integer arguments to network byte order.
	 *
	 * There is preferred encoding for floating-point numbers in CBOR,
	 * but it is very different than for integers and it is not
	 * implemented here.  Half-precision is preferred to
	 * single-precision which is preferred to double-precision only if
	 * the conversion can be performed without loss of precision. Zero
	 * and infinity can always be converted to half-precision, without
	 * loss but 3.141592653589 cannot.
	 *
	 * The way this function knows to not do preferred encoding on the
	 * argument passed here when it is a floating point number is the
	 * uMinLen parameter. It should be 2, 4 or 8 for half, single and
	 * double precision floating point values. This prevents and the
	 * incorrect removal of leading zeros when encoding arguments that
	 * are floating-point numbers.
	 *
	 * == Use of Type int and Static Analyzers ==
	 *
	 * The type int is used here for several variables because of the
	 * way integer promotion works in C for variables that are uint8_t
	 * or uint16_t. The basic rule is that they will always be promoted
	 * to int if they will fit. These integer variables here need only
	 * hold values less than 255 so they will always fit into an int.
	 *
	 * Most of values stored are never negative, so one might think
	 * that unsigned int would be more correct than int. However the C
	 * integer promotion rules only promote to unsigned int if the
	 * result won't fit into an int even if the promotion is for an
	 * unsigned variable like uint8_t.
	 *
	 * By declaring these int, there are few implicit conversions and
	 * fewer casts needed. Code size is reduced a little. It makes
	 * static analyzers happier.
	 *
	 * Note also that declaring these uint8_t won't stop integer wrap
	 * around if the code is wrong. It won't make the code more
	 * correct.
	 *
	 * https://stackoverflow.com/questions/46073295/implicit-type-promotion-rules
	 * https://stackoverflow.com/questions/589575/what-does-the-c-standard-state-the-size-of-int-long-type-to-be
	 *
	 * Code Reviewers: THIS FUNCTION DOES POINTER MATH
	 */

	/* The buffer must have room for the largest CBOR HEAD + one
	 * extra. The one extra is needed for this code to work as it does
	 * a pre-decrement.
	 */
	if (buffer.len < QCBOR_HEAD_BUFFER_SIZE) {
		return NULLUsefulBufC;
	}

	/* Pointer to last valid byte in the buffer */
	uint8_t *const pBufferEnd =
		&((uint8_t *)buffer.ptr)[QCBOR_HEAD_BUFFER_SIZE - 1];

	/* Point to the last byte and work backwards */
	uint8_t *pByte = pBufferEnd;
	/* The 5 bits in the initial byte that are not the major type */
	int nAdditionalInfo;

	if (uMajorType > QCBOR_INDEFINITE_LEN_TYPE_MODIFIER) {
		/* Special case for start & end of indefinite length */
		uMajorType = uMajorType - QCBOR_INDEFINITE_LEN_TYPE_MODIFIER;
		/* This takes advantage of design of CBOR where additional info
		 * is 31 for both opening and closing indefinite length
		 * maps and arrays.
		 */
#if CBOR_SIMPLE_BREAK != LEN_IS_INDEFINITE
#error additional info for opening array not the same as for closing
#endif
		nAdditionalInfo = CBOR_SIMPLE_BREAK;

	} else if (uArgument < CBOR_TWENTY_FOUR && uMinLen == 0) {
		/* Simple case where argument is < 24 */
		nAdditionalInfo = (int)uArgument;

	} else {
		/* This encodes the argument in 1,2,4 or 8 bytes. The outer loop
		 * runs once for 1 byte and 4 times for 8 bytes.  The inner loop
		 * runs 1, 2 or 4 times depending on outer loop counter. This
		 * works backwards shifting 8 bits off the argument being
		 * encoded at a time until all bits from uArgument have been
		 * encoded and the minimum encoding size is reached.  Minimum
		 * encoding size is for floating-point numbers that have some
		 * zero-value bytes that must be output.
		 */
		static const uint8_t aIterate[] = { 1, 1, 2, 4 };

		/* uMinLen passed in is unsigned, but goes negative in the loop
		 * so it must be converted to a signed value.
		 */
		int nMinLen = (int)uMinLen;
		int i;
		for (i = 0; uArgument || nMinLen > 0; i++) {
			const int nIterations = (int)aIterate[i];
			for (int j = 0; j < nIterations; j++) {
				*--pByte  = (uint8_t)(uArgument & 0xff);
				uArgument = uArgument >> 8;
			}
			nMinLen -= nIterations;
		}

		nAdditionalInfo = LEN_IS_ONE_BYTE - 1 + i;
	}

	/* This expression integer-promotes to type int. The code above in
	 * function guarantees that nAdditionalInfo will never be larger
	 * than 0x1f. The caller may pass in a too-large uMajor type. The
	 * conversion to unint8_t will cause an integer wrap around and
	 * incorrect CBOR will be generated, but no security issue will
	 * occur.
	 */
	const int nInitialByte = (uMajorType << 5) + nAdditionalInfo;
	*--pByte	       = (uint8_t)nInitialByte;

#ifdef EXTRA_ENCODE_HEAD_CHECK
	/* This is a sanity check that can be turned on to verify the
	 * pointer math in this function is not going wrong. Turn it on and
	 * run the whole test suite to perform the check.
	 */
	if (pBufferEnd - pByte > 9 || pBufferEnd - pByte < 1 ||
	    pByte < (uint8_t *)buffer.ptr) {
		return NULLUsefulBufC;
	}
#endif /* EXTRA_ENCODE_HEAD_CHECK */

	/* Length will not go negative because the loops run for at most 8
	 * decrements of pByte, only one other decrement is made, and the array
	 * is sized for this.
	 */
	return (UsefulBufC){ pByte, (size_t)(pBufferEnd - pByte) };
}

/**
 * @brief Append the CBOR head, the major type and argument
 *
 * @param me          Encoder context.
 * @param uMajorType  Major type to insert.
 * @param uArgument   The argument (an integer value or a length).
 * @param uMinLen     The minimum number of bytes for encoding the CBOR
 * argument.
 *
 * This formats the CBOR "head" and appends it to the output.
 */
static void
AppendCBORHead(QCBOREncodeContext *me, uint8_t uMajorType, uint64_t uArgument,
	       uint8_t uMinLen)
{
	/* A stack buffer large enough for a CBOR head */
	UsefulBuf_MAKE_STACK_UB(pBufferForEncodedHead, QCBOR_HEAD_BUFFER_SIZE);

	UsefulBufC EncodedHead = QCBOREncode_EncodeHead(
		pBufferForEncodedHead, uMajorType, uMinLen, uArgument);

	/* No check for EncodedHead == NULLUsefulBufC is performed here to
	 * save object code. It is very clear that pBufferForEncodedHead is
	 * the correct size. If EncodedHead == NULLUsefulBufC then
	 * UsefulOutBuf_AppendUsefulBuf() will do nothing so there is no
	 * security hole introduced.
	 */

	UsefulOutBuf_AppendUsefulBuf(&(me->OutBuf), EncodedHead);
}

/**
 * @brief Check for errors when decreasing nesting.
 *
 * @param pMe          QCBOR encoding context.
 * @param uMajorType  The major type of the nesting.
 *
 * Check that there is no previous error, that there is actually some
 * nesting and that the major type of the opening of the nesting
 * matches the major type of the nesting being closed.
 *
 * This is called when closing maps, arrays, byte string wrapping and
 * open/close of byte strings.
 */
static bool
CheckDecreaseNesting(QCBOREncodeContext *pMe, uint8_t uMajorType)
{
#ifndef QCBOR_DISABLE_ENCODE_USAGE_GUARDS
	if (pMe->uError != QCBOR_SUCCESS) {
		return true;
	}

	if (!Nesting_IsInNest(&(pMe->nesting))) {
		pMe->uError = QCBOR_ERR_TOO_MANY_CLOSES;
		return true;
	}

	if (Nesting_GetMajorType(&(pMe->nesting)) != uMajorType) {
		pMe->uError = QCBOR_ERR_CLOSE_MISMATCH;
		return true;
	}

#else
	/* None of these checks are performed if the encode guards are
	 * turned off as they all relate to correct calling.
	 *
	 * Turning off all these checks does not turn off any checking for
	 * buffer overflows or pointer issues.
	 */

	(void)uMajorType;
	(void)pMe;
#endif

	return false;
}

/**
 * @brief Insert the CBOR head for a map, array or wrapped bstr
 *
 * @param me          QCBOR encoding context.
 * @param uMajorType  One of CBOR_MAJOR_TYPE_XXXX.
 * @param uLen        The length of the data item.
 *
 * When an array, map or bstr was opened, nothing was done but note
 * the position. This function goes back to that position and inserts
 * the CBOR Head with the major type and length.
 */
static void
InsertCBORHead(QCBOREncodeContext *me, uint8_t uMajorType, size_t uLen)
{
	if (CheckDecreaseNesting(me, uMajorType)) {
		return;
	}

	if (uMajorType == CBOR_MAJOR_NONE_TYPE_OPEN_BSTR) {
		uMajorType = CBOR_MAJOR_TYPE_BYTE_STRING;
	}

	/* A stack buffer large enough for a CBOR head (9 bytes) */
	UsefulBuf_MAKE_STACK_UB(pBufferForEncodedHead, QCBOR_HEAD_BUFFER_SIZE);

	UsefulBufC EncodedHead = QCBOREncode_EncodeHead(pBufferForEncodedHead,
							uMajorType, 0, uLen);

	/* No check for EncodedHead == NULLUsefulBufC is performed here to
	 * save object code. It is very clear that pBufferForEncodedHead is
	 * the correct size. If EncodedHead == NULLUsefulBufC then
	 * UsefulOutBuf_InsertUsefulBuf() will do nothing so there is no
	 * security hole introduced.
	 */
	UsefulOutBuf_InsertUsefulBuf(&(me->OutBuf), EncodedHead,
				     Nesting_GetStartPos(&(me->nesting)));

	Nesting_Decrease(&(me->nesting));
}

/**
 * @brief Increment item counter for maps and arrays.
 *
 * @param pMe          QCBOR encoding context.
 *
 * This is mostly a separate function to make code more readable and
 * to have fewer occurrences of #ifndef QCBOR_DISABLE_ENCODE_USAGE_GUARDS
 */
static inline void
IncrementMapOrArrayCount(QCBOREncodeContext *pMe)
{
#ifndef QCBOR_DISABLE_ENCODE_USAGE_GUARDS
	if (pMe->uError == QCBOR_SUCCESS) {
		pMe->uError = Nesting_Increment(&(pMe->nesting));
	}
#else
	(void)Nesting_Increment(&(pMe->nesting));
#endif /* QCBOR_DISABLE_ENCODE_USAGE_GUARDS */
}

/*
 * Public functions for adding unsigned integers. See qcbor/qcbor_encode.h
 */
void
QCBOREncode_AddUInt64(QCBOREncodeContext *me, uint64_t uValue)
{
	AppendCBORHead(me, CBOR_MAJOR_TYPE_POSITIVE_INT, uValue, 0);

	IncrementMapOrArrayCount(me);
}

/*
 * Public functions for adding signed integers. See qcbor/qcbor_encode.h
 */
void
QCBOREncode_AddInt64(QCBOREncodeContext *me, int64_t nNum)
{
	uint8_t	 uMajorType;
	uint64_t uValue;

	if (nNum < 0) {
		/* In CBOR -1 encodes as 0x00 with major type negative int.
		 * First add one as a signed integer because that will not
		 * overflow. Then change the sign as needed for encoding.  (The
		 * opposite order, changing the sign and subtracting, can cause
		 * an overflow when encoding INT64_MIN. */
		int64_t nTmp = nNum + 1;
		uValue	     = (uint64_t)-nTmp;
		uMajorType   = CBOR_MAJOR_TYPE_NEGATIVE_INT;
	} else {
		uValue	   = (uint64_t)nNum;
		uMajorType = CBOR_MAJOR_TYPE_POSITIVE_INT;
	}
	AppendCBORHead(me, uMajorType, uValue, 0);

	IncrementMapOrArrayCount(me);
}

/*
 * Semi-private function. It is exposed to user of the interface, but
 * one of its inline wrappers will usually be called instead of this.
 *
 * See qcbor/qcbor_encode.h
 *
 * This does the work of adding actual strings bytes to the CBOR
 * output (as opposed to adding numbers and opening / closing
 * aggregate types).

 * There are four use cases:
 *   CBOR_MAJOR_TYPE_BYTE_STRING -- Byte strings
 *   CBOR_MAJOR_TYPE_TEXT_STRING -- Text strings
 *   CBOR_MAJOR_NONE_TYPE_RAW -- Already-encoded CBOR
 *   CBOR_MAJOR_NONE_TYPE_BSTR_LEN_ONLY -- Special case
 *
 * The first two add the head plus the actual bytes. The third just
 * adds the bytes as the heas is presumed to be in the bytes. The
 * fourth just adds the head for the very special case of
 * QCBOREncode_AddBytesLenOnly().
 */
void
QCBOREncode_AddBuffer(QCBOREncodeContext *me, uint8_t uMajorType,
		      UsefulBufC Bytes)
{
	/* If it is not Raw CBOR, add the type and the length */
	if (uMajorType != CBOR_MAJOR_NONE_TYPE_RAW) {
		uint8_t uRealMajorType = uMajorType;
		if (uRealMajorType == CBOR_MAJOR_NONE_TYPE_BSTR_LEN_ONLY) {
			uRealMajorType = CBOR_MAJOR_TYPE_BYTE_STRING;
		}
		AppendCBORHead(me, uRealMajorType, Bytes.len, 0);
	}

	if (uMajorType != CBOR_MAJOR_NONE_TYPE_BSTR_LEN_ONLY) {
		/* Actually add the bytes */
		UsefulOutBuf_AppendUsefulBuf(&(me->OutBuf), Bytes);
	}

	IncrementMapOrArrayCount(me);
}

/*
 * Public functions for adding a tag. See qcbor/qcbor_encode.h
 */
void
QCBOREncode_AddTag(QCBOREncodeContext *me, uint64_t uTag)
{
	AppendCBORHead(me, CBOR_MAJOR_TYPE_TAG, uTag, 0);
}

/*
 * Semi-private function. It is exposed to user of the interface, but
 * one of its inline wrappers will usually be called instead of this.
 *
 * See header qcbor/qcbor_encode.h
 */
void
QCBOREncode_AddType7(QCBOREncodeContext *me, uint8_t uMinLen, uint64_t uNum)
{
#ifndef QCBOR_DISABLE_ENCODE_USAGE_GUARDS
	if (me->uError == QCBOR_SUCCESS) {
		if (uNum >= CBOR_SIMPLEV_RESERVED_START &&
		    uNum <= CBOR_SIMPLEV_RESERVED_END) {
			me->uError = QCBOR_ERR_ENCODE_UNSUPPORTED;
			return;
		}
	}
#endif /* QCBOR_DISABLE_ENCODE_USAGE_GUARDS */

	/* AppendCBORHead() does endian swapping for the float / double */
	AppendCBORHead(me, CBOR_MAJOR_TYPE_SIMPLE, uNum, uMinLen);

	IncrementMapOrArrayCount(me);
}

#ifndef USEFULBUF_DISABLE_ALL_FLOAT
/*
 * Public functions for adding a double. See qcbor/qcbor_encode.h
 */
void
QCBOREncode_AddDoubleNoPreferred(QCBOREncodeContext *me, double dNum)
{
	QCBOREncode_AddType7(me, sizeof(uint64_t),
			     UsefulBufUtil_CopyDoubleToUint64(dNum));
}

/*
 * Public functions for adding a double. See qcbor/qcbor_encode.h
 */
void
QCBOREncode_AddDouble(QCBOREncodeContext *me, double dNum)
{
#ifndef QCBOR_DISABLE_PREFERRED_FLOAT
	const IEEE754_union uNum = IEEE754_DoubleToSmallest(dNum);

	QCBOREncode_AddType7(me, uNum.uSize, uNum.uValue);
#else  /* QCBOR_DISABLE_PREFERRED_FLOAT */
	QCBOREncode_AddDoubleNoPreferred(me, dNum);
#endif /* QCBOR_DISABLE_PREFERRED_FLOAT */
}

/*
 * Public functions for adding a float. See qcbor/qcbor_encode.h
 */
void
QCBOREncode_AddFloatNoPreferred(QCBOREncodeContext *me, float fNum)
{
	QCBOREncode_AddType7(me, sizeof(uint32_t),
			     UsefulBufUtil_CopyFloatToUint32(fNum));
}

/*
 * Public functions for adding a float. See qcbor/qcbor_encode.h
 */
void
QCBOREncode_AddFloat(QCBOREncodeContext *me, float fNum)
{
#ifndef QCBOR_DISABLE_PREFERRED_FLOAT
	const IEEE754_union uNum = IEEE754_FloatToSmallest(fNum);

	QCBOREncode_AddType7(me, uNum.uSize, uNum.uValue);
#else  /* QCBOR_DISABLE_PREFERRED_FLOAT */
	QCBOREncode_AddFloatNoPreferred(me, fNum);
#endif /* QCBOR_DISABLE_PREFERRED_FLOAT */
}
#endif /* USEFULBUF_DISABLE_ALL_FLOAT */

#ifndef QCBOR_DISABLE_EXP_AND_MANTISSA
/*
 * Semi-public function. It is exposed to the user of the interface,
 * but one of the inline wrappers will usually be called rather than
 * this.
 *
 * See qcbor/qcbor_encode.h
 *
 * Improvement: create another version of this that only takes a big
 * number mantissa and converts the output to a type 0 or 1 integer
 * when mantissa is small enough.
 */
void
QCBOREncode_AddExponentAndMantissa(QCBOREncodeContext *pMe, uint64_t uTag,
				   UsefulBufC BigNumMantissa,
				   bool bBigNumIsNegative, int64_t nMantissa,
				   int64_t nExponent)
{
	/* This is for encoding either a big float or a decimal fraction,
	 * both of which are an array of two items, an exponent and a
	 * mantissa.  The difference between the two is that the exponent
	 * is base-2 for big floats and base-10 for decimal fractions, but
	 * that has no effect on the code here.
	 */
	if (uTag != CBOR_TAG_INVALID64) {
		QCBOREncode_AddTag(pMe, uTag);
	}
	QCBOREncode_OpenArray(pMe);
	QCBOREncode_AddInt64(pMe, nExponent);
	if (!UsefulBuf_IsNULLC(BigNumMantissa)) {
		if (bBigNumIsNegative) {
			QCBOREncode_AddNegativeBignum(pMe, BigNumMantissa);
		} else {
			QCBOREncode_AddPositiveBignum(pMe, BigNumMantissa);
		}
	} else {
		QCBOREncode_AddInt64(pMe, nMantissa);
	}
	QCBOREncode_CloseArray(pMe);
}
#endif /* QCBOR_DISABLE_EXP_AND_MANTISSA */

/*
 * Semi-public function. It is exposed to the user of the interface,
 * but one of the inline wrappers will usually be called rather than
 * this.
 *
 * See qcbor/qcbor_encode.h
 */
void
QCBOREncode_OpenMapOrArray(QCBOREncodeContext *me, uint8_t uMajorType)
{
	/* Add one item to the nesting level we are in for the new map or array
	 */
	IncrementMapOrArrayCount(me);

	/* The offset where the length of an array or map will get written
	 * is stored in a uint32_t, not a size_t to keep stack usage
	 * smaller. This checks to be sure there is no wrap around when
	 * recording the offset.  Note that on 64-bit machines CBOR larger
	 * than 4GB can be encoded as long as no array/map offsets occur
	 * past the 4GB mark, but the public interface says that the
	 * maximum is 4GB to keep the discussion simpler.
	 */
	size_t uEndPosition = UsefulOutBuf_GetEndPosition(&(me->OutBuf));

	/* QCBOR_MAX_ARRAY_OFFSET is slightly less than UINT32_MAX so this
	 * code can run on a 32-bit machine and tests can pass on a 32-bit
	 * machine. If it was exactly UINT32_MAX, then this code would not
	 * compile or run on a 32-bit machine and an #ifdef or some machine
	 * size detection would be needed reducing portability.
	 */
	if (uEndPosition >= QCBOR_MAX_ARRAY_OFFSET) {
		me->uError = QCBOR_ERR_BUFFER_TOO_LARGE;

	} else {
		/* Increase nesting level because this is a map or array.  Cast
		 * from size_t to uin32_t is safe because of check above.
		 */
		me->uError = Nesting_Increase(&(me->nesting), uMajorType,
					      (uint32_t)uEndPosition);
	}
}

/*
 * Semi-public function. It is exposed to the user of the interface,
 * but one of the inline wrappers will usually be called rather than
 * this.
 *
 * See qcbor/qcbor_encode.h
 */
void
QCBOREncode_OpenMapOrArrayIndefiniteLength(QCBOREncodeContext *me,
					   uint8_t	       uMajorType)
{
	/* Insert the indefinite length marker (0x9f for arrays, 0xbf for maps)
	 */
	AppendCBORHead(me, uMajorType, 0, 0);

	/* Call the definite-length opener just to do the bookkeeping for
	 * nesting.  It will record the position of the opening item in the
	 * encoded output but this is not used when closing this open.
	 */
	QCBOREncode_OpenMapOrArray(me, uMajorType);
}

/*
 * Public functions for closing arrays and maps. See qcbor/qcbor_encode.h
 */
void
QCBOREncode_CloseMapOrArray(QCBOREncodeContext *me, uint8_t uMajorType)
{
	InsertCBORHead(me, uMajorType, Nesting_GetCount(&(me->nesting)));
}

/*
 * Public functions for closing bstr wrapping. See qcbor/qcbor_encode.h
 */
void
QCBOREncode_CloseBstrWrap2(QCBOREncodeContext *me, bool bIncludeCBORHead,
			   UsefulBufC *pWrappedCBOR)
{
	const size_t uInsertPosition = Nesting_GetStartPos(&(me->nesting));
	const size_t uEndPosition = UsefulOutBuf_GetEndPosition(&(me->OutBuf));

	/* This subtraction can't go negative because the UsefulOutBuf
	 * always only grows and never shrinks. UsefulOutBut itself also
	 * has defenses such that it won't write where it should not even
	 * if given incorrect input lengths.
	 */
	const size_t uBstrLen = uEndPosition - uInsertPosition;

	/* Actually insert */
	InsertCBORHead(me, CBOR_MAJOR_TYPE_BYTE_STRING, uBstrLen);

	if (pWrappedCBOR) {
		/* Return pointer and length to the enclosed encoded CBOR. The
		 * intended use is for it to be hashed (e.g., SHA-256) in a COSE
		 * implementation.  This must be used right away, as the pointer
		 * and length go invalid on any subsequent calls to this
		 * function because there might be calls to
		 * InsertEncodedTypeAndNumber() that slides data to the right.
		 */
		size_t uStartOfNew = uInsertPosition;
		if (!bIncludeCBORHead) {
			/* Skip over the CBOR head to just get the inserted bstr
			 */
			const size_t uNewEndPosition =
				UsefulOutBuf_GetEndPosition(&(me->OutBuf));
			uStartOfNew += uNewEndPosition - uEndPosition;
		}
		const UsefulBufC PartialResult =
			UsefulOutBuf_OutUBuf(&(me->OutBuf));
		*pWrappedCBOR = UsefulBuf_Tail(PartialResult, uStartOfNew);
	}
}

/*
 * Public function for canceling a bstr wrap. See qcbor/qcbor_encode.h
 */
void
QCBOREncode_CancelBstrWrap(QCBOREncodeContext *pMe)
{
	if (CheckDecreaseNesting(pMe, CBOR_MAJOR_TYPE_BYTE_STRING)) {
		return;
	}

#ifndef QCBOR_DISABLE_ENCODE_USAGE_GUARDS
	const size_t uCurrent = UsefulOutBuf_GetEndPosition(&(pMe->OutBuf));
	if (pMe->nesting.pCurrentNesting->uStart != uCurrent) {
		pMe->uError = QCBOR_ERR_CANNOT_CANCEL;
		return;
	}
	/* QCBOREncode_CancelBstrWrap() can't correctly undo
	 * QCBOREncode_BstrWrapInMap() or QCBOREncode_BstrWrapInMapN(). It
	 * can't undo the labels they add. It also doesn't catch the error
	 * of using it this way.  QCBOREncode_CancelBstrWrap() is used
	 * infrequently and the the result is incorrect CBOR, not a
	 * security hole, so no extra code or state is added to handle this
	 * condition.
	 */
#endif /* QCBOR_DISABLE_ENCODE_USAGE_GUARDS */

	Nesting_Decrease(&(pMe->nesting));
	Nesting_Decrement(&(pMe->nesting));
}

/*
 * Public function for opening a byte string. See qcbor/qcbor_encode.h
 */
void
QCBOREncode_OpenBytes(QCBOREncodeContext *pMe, UsefulBuf *pPlace)
{
	*pPlace = UsefulOutBuf_GetOutPlace(&(pMe->OutBuf));
#ifndef QCBOR_DISABLE_ENCODE_USAGE_GUARDS
	// TODO: is this right?
	uint8_t uMajorType = Nesting_GetMajorType(&(pMe->nesting));
	if (uMajorType == CBOR_MAJOR_NONE_TYPE_OPEN_BSTR) {
		pMe->uError = QCBOR_ERR_OPEN_BYTE_STRING;
		return;
	}
#endif /* QCBOR_DISABLE_ENCODE_USAGE_GUARDS */

	QCBOREncode_OpenMapOrArray(pMe, CBOR_MAJOR_NONE_TYPE_OPEN_BSTR);
}

/*
 * Public function for closing a byte string. See qcbor/qcbor_encode.h
 */
void
QCBOREncode_CloseBytes(QCBOREncodeContext *pMe, const size_t uAmount)
{
	UsefulOutBuf_Advance(&(pMe->OutBuf), uAmount);
	if (UsefulOutBuf_GetError(&(pMe->OutBuf))) {
		/* Advance too far. Normal off-end error handling in effect
		 * here. */
		return;
	}

	InsertCBORHead(pMe, CBOR_MAJOR_NONE_TYPE_OPEN_BSTR, uAmount);
}

/*
 * Public function for closing arrays and maps. See qcbor/qcbor_encode.h
 */
void
QCBOREncode_CloseMapOrArrayIndefiniteLength(QCBOREncodeContext *pMe,
					    uint8_t		uMajorType)
{
	if (CheckDecreaseNesting(pMe, uMajorType)) {
		return;
	}

	/* Append the break marker (0xff for both arrays and maps) */
	AppendCBORHead(pMe, CBOR_MAJOR_NONE_TYPE_SIMPLE_BREAK,
		       CBOR_SIMPLE_BREAK, 0);
	Nesting_Decrease(&(pMe->nesting));
}

/*
 * Public function to finish and get the encoded result. See
 * qcbor/qcbor_encode.h
 */
QCBORError
QCBOREncode_Finish(QCBOREncodeContext *me, UsefulBufC *pEncodedCBOR)
{
	QCBORError uReturn = QCBOREncode_GetErrorState(me);

	if (uReturn != QCBOR_SUCCESS) {
		goto Done;
	}

#ifndef QCBOR_DISABLE_ENCODE_USAGE_GUARDS
	if (Nesting_IsInNest(&(me->nesting))) {
		uReturn = QCBOR_ERR_ARRAY_OR_MAP_STILL_OPEN;
		goto Done;
	}
#endif /* QCBOR_DISABLE_ENCODE_USAGE_GUARDS */

	*pEncodedCBOR = UsefulOutBuf_OutUBuf(&(me->OutBuf));

Done:
	return uReturn;
}

/*
 * Public functions to get size of the encoded result. See qcbor/qcbor_encode.h
 */
QCBORError
QCBOREncode_FinishGetSize(QCBOREncodeContext *me, size_t *puEncodedLen)
{
	UsefulBufC Enc;

	QCBORError nReturn = QCBOREncode_Finish(me, &Enc);

	if (nReturn == QCBOR_SUCCESS) {
		*puEncodedLen = Enc.len;
	}

	return nReturn;
}

```

`hyp/misc/root_env/build.conf`:

```conf
# © 2023 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

interface root_env

```

`hyp/misc/smc_trace/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

interface smc_trace
source smc_trace.c
types smc_trace.tc
# 32KB SMC trace size
configs HYP_SMC_LOG_NUM=255U

```

`hyp/misc/smc_trace/smc_trace.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

define SMC_TRACE_REG_MAX constant type count_t = 14;

define smc_trace_entry structure {
	// Using explicit sized registers rather than a bitfield for debuggers
	id		enumeration smc_trace_id;
	pcpu		uint8;
	vcpu		uint8;
	vmid		type vmid_t;
	x		array(14) type register_t;
	timestamp	uint64;
	regs		uint8;		// The number of X registers used above
	res0		uint8(const);
};

define smc_trace structure(aligned(64)) {
	entries		array(HYP_SMC_LOG_NUM) structure smc_trace_entry;
	next_idx	type index_t(atomic);
};

```

`hyp/misc/smc_trace/src/smc_trace.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>
#include <stdatomic.h>
#include <string.h>

#include <hypconstants.h>

#include <compiler.h>
#include <cpulocal.h>
#include <panic.h>
#include <partition.h>
#include <preempt.h>
#include <smc_trace.h>
#include <thread.h>
#include <util.h>

#include <asm/prefetch.h>
#include <asm/timestamp.h>

extern smc_trace_t *hyp_smc_trace;
smc_trace_t	   *hyp_smc_trace;

void
smc_trace_init(partition_t *partition)
{
	if (hyp_smc_trace != NULL) {
		panic("smc_trace_init already initialized");
	}

	void_ptr_result_t alloc_ret = partition_alloc(
		partition, sizeof(*hyp_smc_trace), alignof(*hyp_smc_trace));
	if (alloc_ret.e != OK) {
		panic("Error allocating smc trace buffer");
	}

	hyp_smc_trace = (smc_trace_t *)alloc_ret.r;
	(void)memset_s(hyp_smc_trace, sizeof(*hyp_smc_trace), 0,
		       sizeof(*hyp_smc_trace));
}

void
smc_trace_log(smc_trace_id_t id, register_t (*registers)[SMC_TRACE_REG_MAX],
	      count_t	     num_registers)
{
	if (hyp_smc_trace == NULL) {
		goto out;
	}

	assert(num_registers <= SMC_TRACE_REG_MAX);
	uint64_t timestamp = arch_get_timestamp();

	cpu_index_t pcpu = cpulocal_get_index_unsafe();
	cpu_index_t vcpu = 0U;
	vmid_t	    vmid = 0U;

#if defined(INTERFACE_VCPU)
	thread_t *current = thread_get_self();

	if (compiler_expected(current->kind == THREAD_KIND_VCPU)) {
		assert(current->addrspace != NULL);
		vmid = current->addrspace->vmid;
		vcpu = current->psci_index;
	}
#endif

	index_t cur_idx = atomic_fetch_add_explicit(&hyp_smc_trace->next_idx, 1,
						    memory_order_consume);
	if (cur_idx >= HYP_SMC_LOG_NUM) {
		index_t next_idx = cur_idx + 1U;
		cur_idx -= HYP_SMC_LOG_NUM;
		(void)atomic_compare_exchange_strong_explicit(
			&hyp_smc_trace->next_idx, &next_idx, cur_idx + 1U,
			memory_order_relaxed, memory_order_relaxed);
	}
	assert(cur_idx < HYP_SMC_LOG_NUM);

	smc_trace_entry_t *entry = &hyp_smc_trace->entries[cur_idx];

	// Reduce likelihood of half-written trace entries being dumped.
	preempt_disable();

	prefetch_store_stream(entry);

	entry->id	 = id;
	entry->pcpu	 = (uint8_t)pcpu;
	entry->vcpu	 = (uint8_t)vcpu;
	entry->vmid	 = vmid;
	entry->regs	 = (uint8_t)num_registers;
	entry->timestamp = timestamp;

	num_registers = util_min(num_registers, SMC_TRACE_REG_MAX);

	for (count_t i = 0; i < num_registers; i++) {
		entry->x[i] = (*registers)[i];
	}
	for (count_t i = num_registers; i < SMC_TRACE_REG_MAX; i++) {
		entry->x[i] = 0U;
	}

	preempt_enable();

out:
	return;
}

```

`hyp/misc/spectre_arm/build.conf`:

```conf
# © 2022 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

configs SPECTRE_CORTEX_A53_NO_SPECULATION=1
configs SPECTRE_CORTEX_A55_NO_SPECULATION=1
configs SPECTRE_QEMU_NO_SPECULATION=1
configs SPECTRE_CORTEX_A57_BHB_LOOP_FLUSH=8
configs SPECTRE_CORTEX_A72_BHB_LOOP_FLUSH=8
configs SPECTRE_CORTEX_A73_BPIALL=1
configs SPECTRE_CORTEX_A75_BPIALL=1
configs SPECTRE_CORTEX_A76_BHB_LOOP_FLUSH=24
configs SPECTRE_CORTEX_A76AE_BHB_LOOP_FLUSH=24
configs SPECTRE_CORTEX_A77_BHB_LOOP_FLUSH=24
configs SPECTRE_CORTEX_A78_BHB_LOOP_FLUSH=32
configs SPECTRE_CORTEX_A78AE_BHB_LOOP_FLUSH=32
configs SPECTRE_CORTEX_A78C_BHB_LOOP_FLUSH=32
configs SPECTRE_CORTEX_X1_BHB_LOOP_FLUSH=32
configs SPECTRE_CORTEX_X2_BHB_LOOP_FLUSH=32
configs SPECTRE_CORTEX_A510_NO_SPECULATION=1
configs SPECTRE_CORTEX_A710_BHB_LOOP_FLUSH=32
configs SPECTRE_CORTEX_A715_BHB_LOOP_FLUSH=38
configs SPECTRE_NEOVERSE_N1_BHB_LOOP_FLUSH=24
configs SPECTRE_NEOVERSE_N2_BHB_LOOP_FLUSH=32
configs SPECTRE_NEOVERSE_V1_BHB_LOOP_FLUSH=32
configs SPECTRE_CORTEX_X3_BHB_LOOP_FLUSH=132
configs SPECTRE_CORTEX_A520_NO_SPECULATION=1
configs SPECTRE_CORTEX_A720_BHB_LOOP_FLUSH=132
configs SPECTRE_CORTEX_X4_BHB_LOOP_FLUSH=132

# Unknown cores, we expect a HW workaround is present. We check the ID
# registers and panic if this isn't true.
configs SPECTRE_UNKNOWN_BHB_HW_WORKAROUND=1

```

`hyp/misc/trace_null/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

interface trace
source trace.c
types trace.tc

```

`hyp/misc/trace_null/src/trace.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <hyptypes.h>

#include <trace.h>

trace_control_t trace_control;

void
trace_set_class_flags(register_t flags)
{
	(void)flags;
}

void
trace_clear_class_flags(register_t flags)
{
	(void)flags;
}

register_t
trace_get_class_flags()
{
	return (register_t)0;
}

```

`hyp/misc/trace_null/trace.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

define trace_control structure {
	enabled_class_flags type register_t(atomic);
};

```

`hyp/misc/trace_standard/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

interface trace
events trace.ev
source trace.c hypercalls.c
source debug.S
types trace.tc

```

`hyp/misc/trace_standard/src/debug.S`:

```S
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause


// .text.debug is a dummy section used to KEEP any symbols that may need to be
// externally visible and not optimized by LTO - usually for debuggers
	.section .text.debug, "ax", @progbits
	adrp	x0, hyp_trace

```

`hyp/misc/trace_standard/src/hypercalls.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#if defined(HYPERCALLS)
#include <hyptypes.h>

#include <hypcall_def.h>

#include <platform_security.h>
#include <trace.h>

hypercall_trace_update_class_flags_result_t
hypercall_trace_update_class_flags(register_t set_flags, register_t clear_flags)
{
	hypercall_trace_update_class_flags_result_t res = { 0 };

	if (platform_security_state_debug_disabled()) {
		res.error = ERROR_DENIED;
		goto out;
	}

	// Clear the bits that hypercalls are not allowed to change
	register_t set	 = set_flags & trace_public_class_flags;
	register_t clear = clear_flags & trace_public_class_flags;

	trace_update_class_flags(set, clear);

	res.flags = trace_get_class_flags();
	res.error = OK;

out:
	return res;
}
#else
extern int unused;
#endif

```

`hyp/misc/trace_standard/src/trace.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>
#include <string.h>

#include <hypconstants.h>

#include <atomic.h>
#include <bitmap.h>
#include <compiler.h>
#include <cpulocal.h>
#include <hyp_aspace.h>
#include <panic.h>
#include <partition.h>
#include <platform_mem.h>
#include <thread.h>
#include <trace.h>
#include <util.h>

#include <asm/cache.h>
#include <asm/cpu.h>
#include <asm/prefetch.h>
#include <asm/timestamp.h>

#include "event_handlers.h"
#include "trace_helpers.h"

static_assert((uintmax_t)PLATFORM_MAX_CORES <
		      ((uintmax_t)1 << TRACE_INFO_CPU_ID_BITS),
	      "CPU-ID does not fit in info");
static_assert((uintmax_t)ENUM_TRACE_ID_MAX_VALUE <
		      ((uintmax_t)1 << TRACE_TAG_TRACE_ID_BITS),
	      "Trace ID does not fit in tag");
static_assert(TRACE_BUFFER_ENTRY_SIZE == TRACE_BUFFER_HEADER_SIZE,
	      "Trace header should be the same size as an entry");

trace_control_t hyp_trace = { .magic = TRACE_MAGIC, .version = TRACE_VERSION };
register_t	trace_public_class_flags;

extern trace_buffer_header_t trace_boot_buffer;

CPULOCAL_DECLARE_STATIC(trace_buffer_header_t *, trace_buffer);
static trace_buffer_header_t *trace_buffer_global;

// Tracing API
//
// A set of function help to log trace easily. The macro TRACE can help to
// construct the correct parameter to call the API.

static void
trace_init_common(partition_t *partition, void *base, size_t size,
		  count_t buffer_count, trace_buffer_header_t *tbuffers[])
{
	count_t global_entries, local_entries;

	assert(size != 0U);
	assert(base != NULL);
	assert(buffer_count != 0U);

	if (buffer_count == 1U) {
		// Allocate all the area to the global buffer
		global_entries =
			(count_t)(size / (size_t)TRACE_BUFFER_ENTRY_SIZE);
		local_entries = 0;
	} else {
		// Ensure the count is one global buffer + one per each CPU
		assert(buffer_count == TRACE_BUFFER_NUM);
		// Ensure the size left for the global buffer is at least equal
		// to the size reserved for each local buffer
		assert(size >= (PER_CPU_TRACE_ENTRIES *
				TRACE_BUFFER_ENTRY_SIZE * TRACE_BUFFER_NUM));
		global_entries =
			(count_t)((size / (size_t)TRACE_BUFFER_ENTRY_SIZE) -
				  ((size_t)PER_CPU_TRACE_ENTRIES *
				   PLATFORM_MAX_CORES));
		local_entries = PER_CPU_TRACE_ENTRIES;
	}

	hyp_trace.header = (trace_buffer_header_t *)base;
	hyp_trace.header_phys =
		partition_virt_to_phys(partition, (uintptr_t)base);

	count_t		       entries;
	trace_buffer_header_t *ptr = (trace_buffer_header_t *)base;
	for (count_t i = 0U; i < buffer_count; i++) {
		if (i == 0U) {
			entries = global_entries;
		} else {
			entries = local_entries;
		}
		trace_buffer_header_t *tb = ptr;
		ptr += entries;

		*tb		= (trace_buffer_header_t){ 0U };
		tb->buf_magic	= TRACE_MAGIC_BUFFER;
		tb->entries	= entries - 1U;
		tb->not_wrapped = true;

		atomic_init(&tb->head, 0);

		tbuffers[i] = tb;
	}

	hyp_trace.num_bufs = buffer_count;
	// Total size of the trace buffer, in units of 64 bytes.
	hyp_trace.area_size_64 = (uint32_t)(size / 64U);
}

void
trace_boot_init(void)
{
	register_t flags = 0U;

	hyp_trace.flags = trace_control_flags_default();
	trace_control_flags_set_format(&hyp_trace.flags,
				       (trace_format_t)TRACE_FORMAT);

	// Default to enable trace buffer and error traces
	TRACE_SET_CLASS(flags, ERROR);
#if !defined(NDEBUG)
	TRACE_SET_CLASS(flags, INFO);
#if !defined(UNITTESTS) || !UNITTESTS
	TRACE_SET_CLASS(flags, USER);
#endif
#endif
#if defined(VERBOSE_TRACE) && VERBOSE_TRACE
	TRACE_SET_CLASS(flags, DEBUG);
#endif
	atomic_init(&hyp_trace.enabled_class_flags, flags);

	// Setup internal flags that cannot be changed by hypercalls
	trace_public_class_flags = ~(register_t)0;
	TRACE_CLEAR_CLASS(trace_public_class_flags, ERROR);
	TRACE_CLEAR_CLASS(trace_public_class_flags, LOG_BUFFER);

	trace_init_common(
		partition_get_private(), &trace_boot_buffer,
		((size_t)TRACE_BOOT_ENTRIES * (size_t)TRACE_BUFFER_ENTRY_SIZE),
		1U, &trace_buffer_global);
}

static void
trace_buffer_init(partition_t *partition, void *base, size_t size)
	REQUIRE_PREEMPT_DISABLED
{
	assert(size != 0);
	assert(base != NULL);

	trace_buffer_header_t *tbs[TRACE_BUFFER_NUM];
	trace_init_common(partition, base, size, TRACE_BUFFER_NUM, tbs);
	// The global buffer will be the first, followed by the local buffers
	trace_buffer_global = tbs[0];
	for (cpu_index_t i = 0U; i < PLATFORM_MAX_CORES; i++) {
		bitmap_set(tbs[i + 1U]->cpu_mask, i);
		// The global buffer is first, hence the increment by 1
		CPULOCAL_BY_INDEX(trace_buffer, i) = tbs[i + 1];
	}

	// Copy the log entries from the boot trace into the newly allocated
	// trace of the boot CPU, which is the current CPU
	cpu_index_t	       cpu_id = cpulocal_get_index();
	trace_buffer_header_t *trace_buffer =
		CPULOCAL_BY_INDEX(trace_buffer, cpu_id);
	assert(trace_boot_buffer.entries < trace_buffer->entries);

	trace_buffer_header_t *tb = &trace_boot_buffer;
	index_t head = atomic_load_explicit(&tb->head, memory_order_relaxed);
	size_t	cpy_size = head * sizeof(trace_buffer_entry_t);

	if (cpy_size != 0U) {
		// The log entries follow on immediately after the header
		char *src_buf = (char *)(tb + 1);
		char *dst_buf = (char *)(trace_buffer + 1);

		(void)memcpy(dst_buf, src_buf, cpy_size);

		CACHE_CLEAN_INVALIDATE_RANGE(dst_buf, cpy_size);
	}

	atomic_store_release(&trace_buffer->head, head);
}

#if defined(PLATFORM_TRACE_STANDALONE_REGION)
void
trace_single_region_init(partition_t *partition, paddr_t base, size_t size)
{
	assert(size != 0);
	assert(base != 0);

	// Call to initiaize the trace buffer
	trace_buffer_init(partition, (void *)base, size);
}
#else
void
trace_init(partition_t *partition, size_t size)
{
	assert(size != 0);

	void_ptr_result_t alloc_ret = partition_alloc(
		partition, size, alignof(trace_buffer_header_t));
	if (alloc_ret.e != OK) {
		panic("Error allocating trace buffer");
	}

	// Call to initiaize the trace buffer
	trace_buffer_init(partition, alloc_ret.r, size);
}
#endif

// Log a trace with specified trace class.
//
// id: ID of this trace event.
// argn: information to store for this trace.
void
trace_standard_handle_trace_log(trace_id_t id, trace_action_t action,
				const char *fmt, register_t arg0,
				register_t arg1, register_t arg2,
				register_t arg3, register_t arg4)
{
	trace_buffer_header_t *tb;
	trace_info_t	       trace_info;
	trace_tag_t	       trace_tag;
	index_t		       head, entries;

	cpu_index_t cpu_id;
	uint64_t    timestamp;

	// Add the data to the trace buffer only if:
	// - The requested action is tracing, and tracing is enabled, or
	// - The requested action is logging, and putting log messages in the
	// trace buffer is enabled.
	bool	   trace_action = ((action == TRACE_ACTION_TRACE) ||
				   (action == TRACE_ACTION_TRACE_LOCAL) ||
				   (action == TRACE_ACTION_TRACE_AND_LOG));
	bool	   log_action	= ((action == TRACE_ACTION_LOG) ||
			   (action == TRACE_ACTION_TRACE_AND_LOG));
	register_t class_flags	= trace_get_class_flags();
	if (compiler_unexpected(
		    !trace_action &&
		    (!log_action ||
		     ((class_flags & TRACE_CLASS_BITS(TRACE_LOG_BUFFER)) ==
		      0U)))) {
		goto out;
	}

	cpu_id	  = cpulocal_get_index_unsafe();
	timestamp = arch_get_timestamp();

	trace_info_init(&trace_info);
	trace_info_set_cpu_id(&trace_info, cpu_id);
	trace_info_set_timestamp(&trace_info, timestamp);

	trace_tag_init(&trace_tag);
	trace_tag_set_trace_id(&trace_tag, id);
#if TRACE_FORMAT == 1
	thread_t *thread = thread_get_self();
	trace_tag_set_trace_ids(&trace_tag, trace_ids_raw(thread->trace_ids));
#else
#error unsupported format
#endif

	// Use the local buffer if the requested action is TRACE_LOCAL and we
	// are not still using the boot trace
	trace_buffer_header_t *cpu_tb = CPULOCAL_BY_INDEX(trace_buffer, cpu_id);
	if ((action == TRACE_ACTION_TRACE_LOCAL) && (cpu_tb != NULL)) {
		tb = cpu_tb;
	} else {
		tb = trace_buffer_global;
	}

	entries = tb->entries;

	// Atomically grab the next entry in the buffer
	head = atomic_fetch_add_explicit(&tb->head, 1, memory_order_consume);
	if (compiler_unexpected(head >= entries)) {
		index_t new_head = head + 1U;

		tb->not_wrapped = false;
		head -= entries;

		(void)atomic_compare_exchange_strong_explicit(
			&tb->head, &new_head, head + 1U, memory_order_relaxed,
			memory_order_relaxed);
	}

	trace_buffer_entry_t *buffers =
		(trace_buffer_entry_t *)((uintptr_t)tb +
					 (uintptr_t)TRACE_BUFFER_HEADER_SIZE);

#if defined(ARCH_ARM) && defined(ARCH_IS_64BIT) && ARCH_IS_64BIT
	// Store using non-temporal store instructions. Also, if the entry
	// fits within a DC ZVA block (typically one cache line), then zero
	// the entry first so the CPU doesn't waste time filling the cache;
	// and if the entry covers an entire cache line, flush it immediately
	// so it doesn't hang around if stnp is ineffective (as the manuals
	// suggest is the case for Cortex-A7x).
	__asm__ volatile(
#if ((1 << CPU_DCZVA_BITS) <= TRACE_BUFFER_ENTRY_SIZE) &&                      \
	((1 << CPU_DCZVA_BITS) <= TRACE_BUFFER_ENTRY_ALIGN)
		"dc zva, %[entry_addr];"
#endif
		"stnp %[info], %[tag], [%[entry_addr], 0];"
		"stnp %[fmt], %[arg0], [%[entry_addr], 16];"
		"stnp %[arg1], %[arg2], [%[entry_addr], 32];"
		"stnp %[arg3], %[arg4], [%[entry_addr], 48];"
#if ((1 << CPU_L1D_LINE_BITS) <= TRACE_BUFFER_ENTRY_SIZE) &&                   \
	((1 << CPU_L1D_LINE_BITS) <= TRACE_BUFFER_ENTRY_ALIGN)
		"dc civac, %[entry_addr];"
#endif
		: [entry] "=m"(buffers[head])
		: [entry_addr] "r"(&buffers[head]),
		  [info] "r"(trace_info_raw(trace_info)),
		  [tag] "r"(trace_tag_raw(trace_tag)), [fmt] "r"(fmt),
		  [arg0] "r"(arg0), [arg1] "r"(arg1), [arg2] "r"(arg2),
		  [arg3] "r"(arg3), [arg4] "r"(arg4));
#else
	prefetch_store_stream(&buffers[head]);

	buffers[head].info    = trace_info;
	buffers[head].tag     = trace_tag;
	buffers[head].fmt     = fmt;
	buffers[head].args[0] = arg0;
	buffers[head].args[1] = arg1;
	buffers[head].args[2] = arg2;
	buffers[head].args[3] = arg3;
	buffers[head].args[4] = arg4;
#endif

out:
	return;
}

void
trace_set_class_flags(register_t flags)
{
	(void)atomic_fetch_or_explicit(&hyp_trace.enabled_class_flags, flags,
				       memory_order_relaxed);
}

void
trace_clear_class_flags(register_t flags)
{
	(void)atomic_fetch_and_explicit(&hyp_trace.enabled_class_flags, ~flags,
					memory_order_relaxed);
}

void
trace_update_class_flags(register_t set_flags, register_t clear_flags)
{
	register_t flags = atomic_load_explicit(&hyp_trace.enabled_class_flags,
						memory_order_relaxed);

	register_t new_flags;
	do {
		new_flags = flags & ~clear_flags;
		new_flags |= set_flags;
	} while (!atomic_compare_exchange_strong_explicit(
		&hyp_trace.enabled_class_flags, &flags, new_flags,
		memory_order_relaxed, memory_order_relaxed));
}

register_t
trace_get_class_flags(void)
{
	return atomic_load_explicit(&hyp_trace.enabled_class_flags,
				    memory_order_relaxed);
}

```

`hyp/misc/trace_standard/test/basic_test.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <hyptypes.h>
#include <stdio.h>

#include <libgen.h>
#include <pthread.h>
#include <time.h>
#include <unistd.h>

#include <asm/cpu.h>

#include "trace.h"

// NOTE: should be no more than maximum cpu count, right now it's 8
#define THREAD_CNT 8

int thread_id[THREAD_CNT];

pthread_t tid[THREAD_CNT];

extern void
btrace_dump(void);

cpu_index_t
cpulocal_check_index(cpu_index_t i)
{
	return i;
}

// Simulate local cpu id with pthread id
cpu_index_t
cpulocal_get_index(void)
{
	pthread_t id = pthread_self();
	int	  i  = 0;

	for (i = 0; i < THREAD_CNT; ++i) {
		if (pthread_equal(tid[i], id)) {
			break;
		}
	}

	return i;
}

int
compiler_ffs(long int x)
{
	return ffsl(x);
}

void
preempt_disable(void)
{
}

void
preempt_enable(void)
{
}

paddr_t
get_paddr(void *ptr)
{
	return 0UL;
}

size_t
get_cpu_cnt(void)
{
	return 4;
}

thread_t *
thread_get_self(void)
{
	return (thread_t *)pthread_self();
}

void *
thread_run(void *val)
{
	int	      i		= 0;
	int	     *tid	= (int *)val;
	trace_class_t class_map = 0L;
	trace_id_t    id	= 0;

	switch (*tid) {
	case 0:
		TRACE_SET_CLASS(class_map, SCHED);
		id = TRACE_ID(SWITCH_TO_IDLE);
		break;
	case 1:
		TRACE_SET_CLASS(class_map, SYSCALL);
		id = TRACE_ID(CONTEXT_SWITCH);
		break;
	case 2:
		TRACE_SET_CLASS(class_map, INTERRUPTS);
		id = TRACE_ID(YIELD);
		break;
	case 3:
		TRACE_SET_CLASS(class_map, LOCK);
		id = TRACE_ID(ERROR);
		break;
	default:
		TRACE_SET_CLASS(class_map, SCHED);
		TRACE_SET_CLASS(class_map, LOCK);
		id = TRACE_ID(EXCEPTION);
		break;
	}

	while (i < 10000) {
		if (i == 500 && *tid == 0) {
			trace_clear_class_flags(0x7);
		}
		TRACE_LONG(class_map, id, 0xff, i);
		++i;
	}

	return NULL;
}

// return the cycle count, and here assume 1 micro sec for each cycle
uint64_t
asm_get_timestamp(void)
{
	struct timespec ts;
	int		ret  = 0;
	long		time = 0L;

	ret = clock_gettime(CLOCK_REALTIME, &ts);
	if (ret != 0) {
		return 0L;
	}

	time = ts.tv_sec * 1000000;
	time += ts.tv_nsec / 1000;

	return (uint64_t)time;
}

void
help(char *app_name)
{
	printf("Usage: %s [OPTION]...\n", basename(app_name));
	printf("Run the binary trace test case from host development PC\n\n");
	printf("Arguments:\n");
	printf("\t -s \t\t specify the size of trace buffer, "
	       "default 1024 bytes \n");
	printf("\t -f \t\t specify the enabled event to trace, default 0xF\n");
	printf("\n");
	printf("The trace buffer size should be multiple of cache line, \n"
	       "which normally is 64 bytes\n");

	return;
}

int
main(int argc, char *argv[])
{
	int	   i		 = 0;
	int	   ret		 = 0;
	size_t	   trace_buf_sz	 = 1024;
	register_t enabled_flags = 0xFL;
	int	   opt;

	while ((opt = getopt(argc, argv, "s:f:h")) != -1) {
		switch (opt) {
		case 's':
			trace_buf_sz = atoi(optarg);
			break;
		case 'f':
			enabled_flags = (uint64_t)strtol(optarg, NULL, 16);
			break;
		case 'h':
			help(argv[0]);
			break;
		default:
			help(argv[0]);
			exit(-1);
		}
	}

	btrace_init();

#if 1
	for (i = 0; i < THREAD_CNT; ++i) {
		thread_id[i] = i;
		ret = pthread_create(&tid[i], NULL, thread_run, &thread_id[i]);
		if (ret != 0) {
			printf("Error: failed to create thread.\n");
			exit(-1);
		}
	}

	for (i = 0; i < THREAD_CNT; ++i) {
		pthread_join(tid[i], NULL);
	}
#else
	thread_id[0] = 0;
	thread_run(&thread_id[0]);
#endif

	printf("trace result: \n");

	btrace_dump();

	return 0;
}

```

`hyp/misc/trace_standard/trace.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

module trace_standard

subscribe boot_runtime_first_init
	handler trace_boot_init()
	priority 10

subscribe trace_log(id, action, arg0, arg1, arg2, arg3, arg4, arg5)
	priority 10

```

`hyp/misc/trace_standard/trace.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// '6TRA' = '36545241'
// '6BUF' = '36425546'
// - Note, external tools can also use these to detect endianness.
define TRACE_MAGIC constant = 0x41525436;
define TRACE_MAGIC_BUFFER constant = 0x46554236;

define TRACE_VERSION constant = 0x0100;

#include <types/bitmap.h>

extend trace_class enumeration {
	// TRACE_BUFFER = 3;	// Deprecated
	TRACE_LOG_BUFFER = 4;	// Put the log messages in the trace buffer
};

define trace_format enumeration {
	kernel = 0;
	hypervisor_armv8 = 1;
};

define trace_action enumeration {
	TRACE;
	TRACE_LOCAL;
};

define trace_info bitfield<64> {
	55:0	timestamp	uint64;
	63:56	cpu_id		type cpu_index_t;
};

define trace_tag bitfield<64> {
	15:0	trace_id	enumeration trace_id;
#if TRACE_FORMAT == 0
	63:16	thread		sregister;
#elif TRACE_FORMAT == 1
	47:16	trace_ids	uint32;
	55:48	unknown=0;
	59:56	unknown=0;
	63:60	nargs		uint8;
#else
#error Unknown trace format
#endif
};

define trace_ids bitfield<32> {
	others	unknown=0;
};

extend thread object {
	trace_ids	bitfield trace_ids;
};

#define WORD_BYTES (sizeof(uregister))

define trace_buffer_entry structure(aligned(64)) {
	info	bitfield trace_info(atomic);
	tag	bitfield trace_tag;
	fmt	pointer char(const);
	args	array(5) uregister;
};

define trace_buffer_header structure(aligned(64)) {
	buf_magic	uint32;

	// size of the buffer in units of entries, excluding the header
	entries		type index_t;

	// bitmap of cpus logging to the trace_buffer
	cpu_mask	BITMAP(256);

	// current head of the circular buffer
	head		type index_t(atomic);

	// The flag to indicate if this buffer has wrapped around.
	// Use inverted logic for backwards compatibility.
	not_wrapped	bool;
};

define trace_control_flags bitfield<16> {
	0	regs_64		bool = ARCH_IS_64BIT;
	1	little_endian	bool = ARCH_ENDIAN_LITTLE;
	3:2	unknown=0;
	// FIXME: not supported yet: enumerator default values
	7:4	format		enumeration trace_format;// = hypervisor_armv8;
	15:8	unknown=0;
};

// Control structure for binary trace logging.
//
// This structure is singleton, and is accessible from all CPUs. This structure
// is also prepared for debug dumping.
define trace_control structure {
	magic		uint32(const);
	version		uint16(const);
	flags		bitfield trace_control_flags;

	// Possibly duplicated in cpu_local struct
	enabled_class_flags type register_t(atomic);
	num_bufs	type count_t;
	area_size_64	uint32;

	header_phys	type paddr_t;
	header		pointer structure trace_buffer_header;
};

// One per each CPU, plus one for the global buffer
// The global buffer will be the first, followed by the local buffers
define TRACE_BUFFER_NUM constant = (PLATFORM_MAX_CORES + 1);

```

`hyp/misc/vet/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

interface vet
types vet.tc
events vet.ev
source vet.c

```

`hyp/misc/vet/src/vet.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>

#include <compiler.h>
#include <cpulocal.h>
#include <log.h>
#include <platform_features.h>
#include <preempt.h>
#include <rcu.h>
#include <scheduler.h>
#include <trace.h>
#include <vet.h>

#include "event_handlers.h"

asm_ordering_dummy_t vet_ordering;

static bool trace_disabled = false;

void
vet_handle_boot_cold_init(void)
{
	platform_cpu_features_t features = platform_get_cpu_features();

	trace_disabled = platform_cpu_features_get_trace_disable(&features);
	if (trace_disabled) {
		LOG(ERROR, INFO, "trace disabled");
	}
}

error_t
vet_handle_thread_context_switch_pre(void)
{
	thread_t *vcpu = thread_get_self();
	assert(vcpu != NULL);

	if (vcpu_option_flags_get_trace_allowed(&vcpu->vcpu_options)) {
		vet_update_trace_unit_status(vcpu);

		if (vcpu->vet_trace_unit_enabled) {
			vet_flush_trace(vcpu);
			vet_disable_trace();
			vet_save_trace_thread_context(vcpu);
		}

		vet_update_trace_buffer_status(vcpu);

		if (vcpu->vet_trace_buffer_enabled) {
			vet_flush_buffer(vcpu);
			vet_disable_buffer();
			vet_save_buffer_thread_context(vcpu);
		}
	}

	return OK;
}

void
vet_handle_thread_load_state(void)
{
	thread_t *vcpu = thread_get_self();
	assert(vcpu != NULL);

	if (vcpu_option_flags_get_trace_allowed(&vcpu->vcpu_options)) {
		if (vcpu->vet_trace_buffer_enabled) {
			vet_restore_buffer_thread_context(vcpu);
			vet_enable_buffer();
		}

		if (vcpu->vet_trace_unit_enabled) {
			vet_restore_trace_thread_context(vcpu);
			vet_enable_trace();
		}
	}
}

bool
vet_handle_vcpu_activate_thread(thread_t *thread, vcpu_option_flags_t options)
{
	bool ret;

	assert(thread->kind == THREAD_KIND_VCPU);

	bool hlos	   = vcpu_option_flags_get_hlos_vm(&options);
	bool trace_allowed = vcpu_option_flags_get_trace_allowed(&options);

	// TODO: currently we always give HLOS trace access.
	if (trace_allowed && trace_disabled) {
		// Not permitted
		ret = false;
	} else if (hlos && !trace_disabled) {
		// Give HLOS threads trace access
		vcpu_option_flags_set_trace_allowed(&thread->vcpu_options,
						    true);
		ret = true;
	} else if (!hlos && trace_allowed) {
		// Not supported
		ret = false;
	} else {
		ret = true;
	}

	return ret;
}

error_t
vet_handle_power_cpu_suspend(bool may_poweroff)
{
	assert_cpulocal_safe();
	rcu_read_start();

	thread_t *vcpu = scheduler_get_primary_vcpu(cpulocal_get_index());

	if (may_poweroff && (vcpu != NULL) && vcpu->vet_trace_buffer_enabled) {
		vet_save_buffer_power_context();
	}

	if ((vcpu != NULL) && vcpu->vet_trace_unit_enabled) {
		vet_save_trace_power_context(may_poweroff);
	}

	rcu_read_finish();
	return OK;
}

void
vet_unwind_power_cpu_suspend(void)
{
	assert_cpulocal_safe();
	rcu_read_start();

	thread_t *vcpu = scheduler_get_primary_vcpu(cpulocal_get_index());

	if ((vcpu != NULL) && vcpu->vet_trace_unit_enabled) {
		vet_restore_trace_power_context(false);
	}

	rcu_read_finish();
}

void
vet_handle_power_cpu_resume(bool was_poweroff)
{
	assert_cpulocal_safe();
	rcu_read_start();

	thread_t *vcpu = scheduler_get_primary_vcpu(cpulocal_get_index());

	if (was_poweroff && (vcpu != NULL) && vcpu->vet_trace_buffer_enabled) {
		vet_restore_buffer_power_context();
	}

	if ((vcpu != NULL) && vcpu->vet_trace_unit_enabled) {
		vet_restore_trace_power_context(was_poweroff);
	}

	rcu_read_finish();
}

```

`hyp/misc/vet/vet.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

module vet

subscribe boot_cold_init()

subscribe thread_context_switch_pre()
	require_preempt_disabled

subscribe thread_load_state()
	require_preempt_disabled

subscribe vcpu_activate_thread

subscribe power_cpu_suspend(may_poweroff)
	unwinder()
	require_preempt_disabled

subscribe power_cpu_resume(was_poweroff)
	require_preempt_disabled

```

`hyp/misc/vet/vet.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

extend thread object module vet {
	// Indicate if the trace unit/buffer is enabled. If it's not enabled,
	// we can skip the thread/power context save/restore.
	trace_unit_enabled		bool;
	trace_buffer_enabled		bool;
};

```

`hyp/platform/arm_arch_timer/aarch64/src/platform_timer.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>

#include <hypregisters.h>

#include <irq.h>
#include <object.h>
#include <panic.h>
#include <preempt.h>

#include <asm/barrier.h>

#include "event_handlers.h"
#include "platform_timer.h"
#include "platform_timer_consts.h"

#if !defined(IRQ_NULL)
#include <partition.h>
#include <partition_alloc.h>

#include <events/platform.h>
#endif

#if !defined(IRQ_NULL)
static hwirq_t *hyp_timer_hwirq;
#endif

static void
platform_timer_enable_and_unmask(void)
{
	CNT_CTL_t cnthp_ctl;

	CNT_CTL_init(&cnthp_ctl);
	CNT_CTL_set_ENABLE(&cnthp_ctl, true);
	CNT_CTL_set_IMASK(&cnthp_ctl, false);
	register_CNTHP_CTL_EL2_write_ordered(cnthp_ctl, &asm_ordering);
}

void
platform_timer_cancel_timeout(void)
{
	CNT_CTL_t cnthp_ctl;

	CNT_CTL_init(&cnthp_ctl);
	CNT_CTL_set_ENABLE(&cnthp_ctl, false);
	CNT_CTL_set_IMASK(&cnthp_ctl, true);
	register_CNTHP_CTL_EL2_write_ordered(cnthp_ctl, &asm_ordering);
	__asm__ volatile("isb" : "+m"(asm_ordering));
}

uint32_t
platform_timer_get_frequency(void)
{
	return PLATFORM_ARCH_TIMER_FREQ;
}

uint64_t
platform_timer_get_current_ticks(void)
{
	// This register read below is allowed to occur speculatively at any
	// time after the most recent context sync event. If caller the wants
	// it to actually reflect the exact current time, it must execute an
	// ordered ISB before calling this function.
	CNTPCT_EL0_t cntpct =
		register_CNTPCT_EL0_read_volatile_ordered(&asm_ordering);

	return CNTPCT_EL0_get_CountValue(&cntpct);
}

uint64_t
platform_timer_get_timeout(void)
{
	CNT_CVAL_t cnthp_cval =
		register_CNTHP_CVAL_EL2_read_volatile_ordered(&asm_ordering);

	return CNT_CVAL_get_CompareValue(&cnthp_cval);
}

void
platform_timer_set_timeout(ticks_t timeout)
{
	assert_preempt_disabled();

	register_CNTHP_CVAL_EL2_write_ordered(CNT_CVAL_cast(timeout),
					      &asm_ordering);
	platform_timer_enable_and_unmask();
	__asm__ volatile("isb" : "+m"(asm_ordering));
}

ticks_t
platform_timer_convert_ns_to_ticks(nanoseconds_t ns)
{
	return (ticks_t)((ns * PLATFORM_TIMER_NS_TO_FREQ_MULT) /
			 PLATFORM_TIMER_FREQ_TO_NS_MULT);
}

nanoseconds_t
platform_timer_convert_ticks_to_ns(ticks_t ticks)
{
	return (nanoseconds_t)((ticks * PLATFORM_TIMER_FREQ_TO_NS_MULT) /
			       PLATFORM_TIMER_NS_TO_FREQ_MULT);
}

ticks_t
platform_timer_convert_ms_to_ticks(milliseconds_t ms)
{
	return (ticks_t)((ms * PLATFORM_TIMER_MS_TO_FREQ_MULT) /
			 PLATFORM_TIMER_FREQ_TO_MS_MULT);
}

milliseconds_t
platform_timer_convert_ticks_to_ms(ticks_t ticks)
{
	return (milliseconds_t)((ticks * PLATFORM_TIMER_FREQ_TO_MS_MULT) /
				PLATFORM_TIMER_MS_TO_FREQ_MULT);
}

void
platform_timer_handle_boot_cpu_cold_init(void)
{
	CNTFRQ_EL0_t cntfrq = register_CNTFRQ_EL0_read();
	assert(CNTFRQ_EL0_get_ClockFrequency(&cntfrq) ==
	       PLATFORM_ARCH_TIMER_FREQ);

#if !defined(IRQ_NULL)
	if (hyp_timer_hwirq != NULL) {
		irq_enable_local(hyp_timer_hwirq);
	}
#endif
}

#if !defined(IRQ_NULL)
void
platform_timer_handle_boot_hypervisor_start(void)
{
	// Create the hyp arch timer IRQ
	hwirq_create_t params = {
		.irq	= PLATFORM_HYP_ARCH_TIMER_IRQ,
		.action = HWIRQ_ACTION_HYP_TIMER,
	};

	hwirq_ptr_result_t ret =
		partition_allocate_hwirq(partition_get_private(), params);

	if (ret.e != OK) {
		panic("Failed to create Hyp Timer IRQ");
	}

	if (object_activate_hwirq(ret.r) != OK) {
		panic("Failed to activate Hyp Timer IRQ");
	}

	hyp_timer_hwirq = ret.r;

	irq_enable_local(hyp_timer_hwirq);
}

bool
platform_timer_handle_irq_received(void)
{
	trigger_platform_timer_expiry_event();

	return true;
}

void
platform_timer_ndelay(nanoseconds_t duration)
{
	ticks_t cur_ticks      = platform_timer_get_current_ticks();
	ticks_t duration_ticks = platform_timer_convert_ns_to_ticks(duration);
	ticks_t target_ticks   = cur_ticks + duration_ticks;

	// NOTE: assume we don't have overflow case since it covers huge range.
	// And assumes the timer is always enabled/configured correctly.
	while (platform_timer_get_current_ticks() < target_ticks) {
		// Wait for the delay period
	}
}
#endif

```

`hyp/platform/arm_arch_timer/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

interface platform
types platform_timer.tc
events platform_timer.ev
template simple platform_timer_consts.h.tmpl
arch_source aarch64 platform_timer.c

```

`hyp/platform/arm_arch_timer/platform_timer.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

module arm_arch_timer

subscribe boot_cpu_cold_init
	handler platform_timer_handle_boot_cpu_cold_init()
	require_preempt_disabled

#if !defined(IRQ_NULL)
subscribe boot_hypervisor_start
	handler platform_timer_handle_boot_hypervisor_start
	require_preempt_disabled

subscribe irq_received[HWIRQ_ACTION_HYP_TIMER]
	handler platform_timer_handle_irq_received()
	require_preempt_disabled
#endif

```

`hyp/platform/arm_arch_timer/platform_timer.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#if !defined(IRQ_NULL)
extend hwirq_action enumeration {
	hyp_timer;
};
#endif

```

`hyp/platform/arm_arch_timer/templates/platform_timer_consts.h.tmpl`:

```tmpl
// Automatically generated. Do not modify.
//
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#import math

#set $ns_in_s = 1000000000
#set $gcd = math.gcd(ns_in_s, $PLATFORM_ARCH_TIMER_FREQ)

#set $ns_to_freq_mult = $PLATFORM_ARCH_TIMER_FREQ//$gcd
#set $freq_to_ns_mult = $ns_in_s//$gcd

## Ensure that we can have at least 20 yrs uptime without timer overflow
#set $secs_per_year = 60*60*24*365.25
#assert ($PLATFORM_ARCH_TIMER_FREQ * $freq_to_ns_mult * $secs_per_year * 20) < (1 << 64)

\#define PLATFORM_TIMER_NS_TO_FREQ_MULT (uint64_t)$ns_to_freq_mult
\#define PLATFORM_TIMER_FREQ_TO_NS_MULT (uint64_t)$freq_to_ns_mult

#set $ms_in_s = 1000
#set $gcd = math.gcd(ms_in_s, $PLATFORM_ARCH_TIMER_FREQ)

#set $ms_to_freq_mult = $PLATFORM_ARCH_TIMER_FREQ//$gcd
#set $freq_to_ms_mult = $ms_in_s//$gcd

\#define PLATFORM_TIMER_MS_TO_FREQ_MULT (uint64_t)$ms_to_freq_mult
\#define PLATFORM_TIMER_FREQ_TO_MS_MULT (uint64_t)$freq_to_ms_mult

```

`hyp/platform/arm_arch_timer_lp/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

interface platform
base_module hyp/platform/gicv3
types platform_timer_lp.tc platform_timer_lp-regs.tc
events platform_timer_lp.ev
source platform_timer_lp.c

```

`hyp/platform/arm_arch_timer_lp/platform_timer_lp-regs.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// Lower-power timer registers and bitfields definitions

define CNTEL0ACR bitfield<32> {
	0	EL0PCTEN	bool;
	1	EL0VCTEN	bool;
	7:2	unknown=0;
	8	EL0VTEN		bool;
	9	EL0PTEN		bool;
	31:10	unknown=0;
};

define CNTFRQ bitfield<32> {
	31:0	ClockFrequency	uint32;
};

#define CNTx_CTL(x)							\
define CNT##x##_CTL bitfield<32> {					\
	0	ENABLE		bool;					\
	1	IMASK		bool;					\
	2	ISTATUS		bool;					\
	31:3	unknown=0;						\
};
CNTx_CTL(P)
CNTx_CTL(V)

```

`hyp/platform/arm_arch_timer_lp/platform_timer_lp.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

module arm_arch_timer_lp

subscribe boot_cold_init
	handler platform_timer_lp_handle_boot_cold_init()

subscribe boot_hypervisor_start
	handler platform_timer_lp_handle_boot_hypervisor_start

subscribe irq_received[HWIRQ_ACTION_HYP_TIMER_LP]
	handler platform_timer_lp_handle_irq_received()
	require_preempt_disabled

#if defined(MODULE_VM_ROOTVM)
subscribe rootvm_init
	handler platform_timer_lp_handle_rootvm_init(root_cspace, hyp_env)
	// We need to run after the creation of the device memeextent and when
	// when VIRQs can are ready to be bound
	priority -11
#endif

```

`hyp/platform/arm_arch_timer_lp/platform_timer_lp.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

define cntbase structure(aligned(PGTABLE_HYP_PAGE_SIZE)) {
	pct @ 0x000	uint64(atomic);
	vct @ 0x008	uint64(atomic);
	frq @ 0x010	uint32(atomic);
	el0acr @ 0x014	bitfield CNTEL0ACR(atomic);
	voff @ 0x018	uint64(atomic);
	p_cval @ 0x020	uint64(atomic);
	p_tval @ 0x028	uint32(atomic);
	p_ctl @ 0x02c	bitfield CNTP_CTL(atomic);
	v_cval @ 0x030	uint64(atomic);
	v_tval @ 0x038	uint32(atomic);
	v_ctl @ 0x03c	bitfield CNTV_CTL(atomic);
};

extend hwirq_action enumeration {
	hyp_timer_lp;
};

```

`hyp/platform/arm_arch_timer_lp/src/platform_timer_lp.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>

#include <hyprights.h>

#include <atomic.h>
#include <compiler.h>
#include <cpulocal.h>
#include <cspace.h>
#include <cspace_lookup.h>
#include <hyp_aspace.h>
#include <irq.h>
#include <memextent.h>
#include <object.h>
#include <panic.h>
#include <partition.h>
#include <partition_alloc.h>
#include <pgtable.h>
#include <platform_cpu.h>
#include <platform_timer_lp.h>
#include <preempt.h>

#include <events/platform.h>

#include <asm/barrier.h>

#include "event_handlers.h"
#include "gicv3.h"

static cntbase_t *hyp_timer_cnt;
static size_t	  virt_hyp_timer_size;

static GICD_IROUTER_t current_route;

static void
platform_timer_lp_enable_and_unmask(void)
{
	CNTP_CTL_t cntp_ctl;
	CNTP_CTL_init(&cntp_ctl);
	CNTP_CTL_set_ENABLE(&cntp_ctl, true);
	CNTP_CTL_set_IMASK(&cntp_ctl, false);

	atomic_store_relaxed(&hyp_timer_cnt->p_ctl, cntp_ctl);
}

void
platform_timer_lp_set_timeout(ticks_t timeout)
{
	assert_preempt_disabled();

	atomic_store_relaxed(&hyp_timer_cnt->p_cval, timeout);
	platform_timer_lp_enable_and_unmask();
}

uint64_t
platform_timer_lp_get_timeout(void)
{
	return atomic_load_relaxed(&hyp_timer_cnt->p_cval);
}

void
platform_timer_lp_cancel_timeout(void)
{
	CNTP_CTL_t cntp_ctl;
	CNTP_CTL_init(&cntp_ctl);
	CNTP_CTL_set_ENABLE(&cntp_ctl, false);
	CNTP_CTL_set_IMASK(&cntp_ctl, true);

	atomic_store_relaxed(&hyp_timer_cnt->p_ctl, cntp_ctl);
}

uint32_t
platform_timer_lp_get_frequency(void)
{
	return atomic_load_relaxed(&hyp_timer_cnt->frq);
}

uint64_t
platform_timer_lp_get_current_ticks(void)
{
	return atomic_load_relaxed(&hyp_timer_cnt->pct);
}

void
platform_timer_lp_visibility(bool visible)
{
	CNTEL0ACR_t acr = CNTEL0ACR_default();

	CNTEL0ACR_set_EL0VCTEN(&acr, visible);
	CNTEL0ACR_set_EL0VTEN(&acr, visible);

	atomic_store_relaxed(&hyp_timer_cnt->el0acr, acr);
}

void
platform_timer_lp_handle_boot_cold_init(void)
{
	size_t timer_size = PGTABLE_HYP_PAGE_SIZE;

	// Allocate hyp_timer_cnt

	virt_range_result_t range = hyp_aspace_allocate(timer_size);
	if (range.e != OK) {
		panic("timer_lp: Allocation failed.");
	}

	hyp_timer_cnt	    = (cntbase_t *)range.r.base;
	virt_hyp_timer_size = range.r.size;

	// Map the low power timer

	pgtable_hyp_start();

	error_t err = pgtable_hyp_map(
		partition_get_private(), (uintptr_t)hyp_timer_cnt, timer_size,
		PLATFORM_HYP_ARCH_TIMER_LP_BASE, PGTABLE_HYP_MEMTYPE_DEVICE,
		PGTABLE_ACCESS_RW, VMSA_SHAREABILITY_NON_SHAREABLE);
	if (err != OK) {
		panic("timer_lp: Mapping failed.");
	}

	pgtable_hyp_commit();

	assert(platform_timer_lp_get_frequency() ==
	       PLATFORM_ARCH_TIMER_LP_FREQ);

	// In this code we are assuming that the generic timer is in sync with
	// the low power timer and therefore, we do not need to convert
	// timeouts.
	static_assert(PLATFORM_ARCH_TIMER_LP_FREQ == PLATFORM_ARCH_TIMER_FREQ,
		      "Arch timer and lp timer must have the same frequency");

	platform_timer_lp_visibility(true);

	// FIXME:
	// Unmap/remap the priviledged timer frame in the HLOS S2 address space
}

void
platform_timer_lp_handle_boot_hypervisor_start(void)
{
	// Create the low power timer IRQ
	hwirq_create_t params = {
		.irq	= PLATFORM_HYP_ARCH_TIMER_LP_IRQ,
		.action = HWIRQ_ACTION_HYP_TIMER_LP,
	};

	hwirq_ptr_result_t ret =
		partition_allocate_hwirq(partition_get_private(), params);

	if (ret.e != OK) {
		panic("Failed to create low power timer IRQ");
	}

	if (object_activate_hwirq(ret.r) != OK) {
		panic("Failed to activate low power timer IRQ");
	}

	irq_enable_shared(ret.r);
}

bool
platform_timer_lp_handle_irq_received(void)
{
	trigger_platform_timer_lp_expiry_event();

	return true;
}

void
platform_timer_lp_set_timeout_and_route(ticks_t timeout, cpu_index_t cpu_index)
{
	MPIDR_EL1_t    mpidr	  = platform_cpu_index_to_mpidr(cpu_index);
	GICD_IROUTER_t phys_route = GICD_IROUTER_default();
	GICD_IROUTER_set_IRM(&phys_route, false);
	GICD_IROUTER_set_Aff0(&phys_route, MPIDR_EL1_get_Aff0(&mpidr));
	GICD_IROUTER_set_Aff1(&phys_route, MPIDR_EL1_get_Aff1(&mpidr));
	GICD_IROUTER_set_Aff2(&phys_route, MPIDR_EL1_get_Aff2(&mpidr));
	GICD_IROUTER_set_Aff3(&phys_route, MPIDR_EL1_get_Aff3(&mpidr));

	if (!GICD_IROUTER_is_equal(current_route, phys_route)) {
		if (gicv3_spi_set_route(PLATFORM_HYP_ARCH_TIMER_LP_IRQ,
					phys_route) != OK) {
			panic("LPTimer: Failed to set the IRQ route!");
		}
		current_route = phys_route;
	}

	platform_timer_lp_set_timeout(timeout);
}

#if defined(MODULE_VM_ROOTVM)
void
platform_timer_lp_handle_rootvm_init(cspace_t *cspace, hyp_env_data_t *hyp_env)
{
	memextent_ptr_result_t m = cspace_lookup_memextent(
		cspace, hyp_env->device_me_capid, CAP_RIGHTS_MEMEXTENT_DERIVE);
	if (compiler_unexpected(m.e != OK)) {
		panic("Failed to find device memextent.");
	}

	memextent_t *parent = m.r;

	memextent_ptr_result_t me_ret =
		memextent_derive(parent, PLATFORM_HYP_ARCH_TIMER_LP_BASE,
				 (size_t)1U << 12, MEMEXTENT_MEMTYPE_DEVICE,
				 PGTABLE_ACCESS_RW, MEMEXTENT_TYPE_BASIC);
	if (me_ret.e != OK) {
		panic("Failed creation of low power timer memextent");
	}

	object_put_memextent(parent);
}
#endif

```

`hyp/platform/arm_dsu/aarch64/src/platform_dsu.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>

#include <hypregisters.h>

#include <compiler.h>
#include <irq.h>
#include <log.h>
#include <panic.h>
#include <preempt.h>
#include <thread.h>
#include <trace.h>
#if defined(INTERFACE_VCPU)
#include <vcpu.h>
#endif

#include <asm/barrier.h>
#include <asm/system_registers.h>

#include "event_handlers.h"

#if defined(INTERFACE_VCPU)

// The module to trap and emulate the read accesses to the cluster register. We
// currently only emulate IMP_CLUSTERIDR_EL1 and treat the rest as RAZ.

// Before reading any cluster registers we need to apply the DSU SCLK
// gating erratum (2,313,941) workaround, which is executing a dummy
// cache maintenance operation instruction immediately prior to
// accessing the register.
static inline void
platform_dsu_apply_sclk_gating_erratum_workaround(void) REQUIRE_PREEMPT_DISABLED
{
	register_t dummy = 0;

	assert_preempt_disabled();
	__asm__ volatile("DC CIVAC, %[VA]; "
			 : "+m"(asm_ordering)
			 : [VA] "r"(&dummy));
}

static inline IMP_CLUSTERIDR_EL1_t
register_CLUSTERIDR_EL1_read(void)
{
	IMP_CLUSTERIDR_EL1_t val;

	preempt_disable();
	platform_dsu_apply_sclk_gating_erratum_workaround();
	val = register_IMP_CLUSTERIDR_EL1_read_ordered(&asm_ordering);
	preempt_enable();

	return val;
}

vcpu_trap_result_t
arm_dsu_handle_vcpu_trap_sysreg_read(ESR_EL2_ISS_MSR_MRS_t iss)
{
	register_t	   val	  = 0ULL; // Default action is RAZ
	vcpu_trap_result_t ret	  = VCPU_TRAP_RESULT_EMULATED;
	thread_t	  *thread = thread_get_self();

	// Assert this is a read
	assert(ESR_EL2_ISS_MSR_MRS_get_Direction(&iss));

	uint8_t reg_num = ESR_EL2_ISS_MSR_MRS_get_Rt(&iss);

	// Remove the fields that are not used in the comparison
	ESR_EL2_ISS_MSR_MRS_t temp_iss = iss;
	ESR_EL2_ISS_MSR_MRS_set_Rt(&temp_iss, 0U);
	ESR_EL2_ISS_MSR_MRS_set_Direction(&temp_iss, false);

	switch (ESR_EL2_ISS_MSR_MRS_raw(temp_iss)) {
	case ISS_MRS_MSR_IMP_CLUSTERIDR_EL1: {
		IMP_CLUSTERIDR_EL1_t clusteridr =
			register_CLUSTERIDR_EL1_read();
		val = IMP_CLUSTERIDR_EL1_raw(clusteridr);
		break;
	}
	default: {
		uint8_t crn, crm;

		crn = ESR_EL2_ISS_MSR_MRS_get_CRn(&iss);
		crm = ESR_EL2_ISS_MSR_MRS_get_CRm(&iss);

		if ((crn == 15U) && ((crm == 3U) || (crm == 4U))) {
			TRACE_AND_LOG(DEBUG, WARN,
				      "Emulated RAZ for cluster register: ISS "
				      "{:#x}",
				      ESR_EL2_ISS_MSR_MRS_raw(iss));
		} else {
			ret = VCPU_TRAP_RESULT_UNHANDLED;
		}
		break;
	}
	}

	// Update the thread's register
	if (ret == VCPU_TRAP_RESULT_EMULATED) {
		vcpu_gpr_write(thread, reg_num, val);
	}

	return ret;
}

#endif

```

`hyp/platform/arm_dsu/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

interface platform
types platform_dsu.tc
events platform_dsu.ev
arch_source aarch64 platform_dsu.c

```

`hyp/platform/arm_dsu/platform_dsu.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

module arm_dsu

#if defined(INTERFACE_VCPU)

subscribe vcpu_trap_sysreg_read

#endif

```

`hyp/platform/arm_dsu/platform_dsu.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#if defined(INTERFACE_VCPU)

define IMP_CLUSTERIDR_EL1 bitfield<64> {
	3:0		Revision	uint8;
	7:4		Variant		uint8;
	63:8		unknown=0;
};

#endif

```

`hyp/platform/arm_fgt/aarch64/arm_fgt_aarch64.ev`:

```ev
// © 2023 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

module arm_fgt

#if defined(INTERFACE_VCPU) && defined(ARCH_ARM_FEAT_FGT) && ARCH_ARM_FEAT_FGT
subscribe thread_load_state()
#endif

```

`hyp/platform/arm_fgt/aarch64/arm_fgt_aarch64.tc`:

```tc
// © 2023 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#if defined(INTERFACE_VCPU) && defined(ARCH_ARM_FEAT_FGT) && ARCH_ARM_FEAT_FGT
extend vcpu_el2_registers structure {
	hfgwtr_el2	bitfield HFGWTR_EL2;
};
#endif

```

`hyp/platform/arm_fgt/aarch64/src/arm_fgt.c`:

```c
// © 2023 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>

#if defined(ARCH_ARM_FEAT_FGT) && ARCH_ARM_FEAT_FGT

#include <hypregisters.h>

#include <arm_fgt.h>
#include <compiler.h>
#include <globals.h>
#include <platform_features.h>

#include "event_handlers.h"

bool
arm_fgt_is_allowed(void)
{
#if defined(PLATFORM_FGT_OPTIONAL)
	const global_options_t *global_options = globals_get_options();
	return compiler_expected(global_options_get_fgt(global_options));
#else
	return true;
#endif
}

void
arm_fgt_handle_boot_cold_init(void)
{
	global_options_t options = global_options_default();
	global_options_set_fgt(&options, true);
	globals_set_options(options);

#if defined(PLATFORM_FGT_OPTIONAL)
	// TZ might be restricting access to FGT, check first
	platform_cpu_features_t features = platform_get_cpu_features();
	if (platform_cpu_features_get_fgt_disable(&features)) {
		globals_clear_options(options);
	}
#endif
}

#if defined(INTERFACE_VCPU)
void
arm_fgt_handle_thread_load_state(void)
{
	thread_t *thread = thread_get_self();
	if (compiler_expected((thread->kind == THREAD_KIND_VCPU) &&
			      arm_fgt_is_allowed())) {
		register_HFGWTR_EL2_write(thread->vcpu_regs_el2.hfgwtr_el2);
	}
}
#endif

#endif

```

`hyp/platform/arm_fgt/arm_fgt.ev`:

```ev
// © 2023 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

module arm_fgt

#if defined(ARCH_ARM_FEAT_FGT) && ARCH_ARM_FEAT_FGT
subscribe boot_cold_init()
#endif

```

`hyp/platform/arm_fgt/build.conf`:

```conf
# © 2023 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

interface arm_fgt

events arm_fgt.ev
arch_types aarch64 arm_fgt_aarch64.tc
arch_events aarch64 arm_fgt_aarch64.ev
arch_source aarch64 arm_fgt.c

```

`hyp/platform/arm_generic/aarch64/src/cpu.c`:

```c
// © 2022 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>

#include <hypregisters.h>

#include <compiler.h>
#include <cpulocal.h>
#include <platform_cpu.h>

platform_mpidr_mapping_t
platform_cpu_get_mpidr_mapping(void)
{
	MPIDR_EL1_t real_mpidr = register_MPIDR_EL1_read();

	return (platform_mpidr_mapping_t){
		.aff_shift    = { PLATFORM_MPIDR_AFF0_SHIFT,
				  PLATFORM_MPIDR_AFF1_SHIFT,
				  PLATFORM_MPIDR_AFF2_SHIFT,
				  PLATFORM_MPIDR_AFF3_SHIFT },
		.aff_mask     = { PLATFORM_MPIDR_AFF0_MASK,
				  PLATFORM_MPIDR_AFF1_MASK,
				  PLATFORM_MPIDR_AFF2_MASK,
				  PLATFORM_MPIDR_AFF3_MASK },
		.multi_thread = MPIDR_EL1_get_MT(&real_mpidr),
		.uniprocessor = MPIDR_EL1_get_U(&real_mpidr),
	};
}

MPIDR_EL1_t
platform_cpu_map_index_to_mpidr(const platform_mpidr_mapping_t *mapping,
				index_t				index)
{
	MPIDR_EL1_t mpidr = MPIDR_EL1_default();

	assert(mapping->aff_shift[0] < 32U);
	assert(mapping->aff_shift[1] < 32U);
	assert(mapping->aff_shift[2] < 32U);
	assert(mapping->aff_shift[3] < 32U);

	MPIDR_EL1_set_Aff0(&mpidr, (uint8_t)((index >> mapping->aff_shift[0]) &
					     mapping->aff_mask[0]));
	MPIDR_EL1_set_Aff1(&mpidr, (uint8_t)((index >> mapping->aff_shift[1]) &
					     mapping->aff_mask[1]));
	MPIDR_EL1_set_Aff2(&mpidr, (uint8_t)((index >> mapping->aff_shift[2]) &
					     mapping->aff_mask[2]));
	MPIDR_EL1_set_Aff3(&mpidr, (uint8_t)((index >> mapping->aff_shift[3]) &
					     mapping->aff_mask[3]));
	MPIDR_EL1_set_MT(&mpidr, mapping->multi_thread);
	MPIDR_EL1_set_U(&mpidr, mapping->uniprocessor);

	return mpidr;
}

index_t
platform_cpu_map_mpidr_to_index(const platform_mpidr_mapping_t *mapping,
				MPIDR_EL1_t			mpidr)
{
	index_t index = 0U;

	assert(mapping->aff_shift[0] < 32U);
	assert(mapping->aff_shift[1] < 32U);
	assert(mapping->aff_shift[2] < 32U);
	assert(mapping->aff_shift[3] < 32U);

	index |= ((index_t)MPIDR_EL1_get_Aff0(&mpidr) &
		  (index_t)mapping->aff_mask[0])
		 << mapping->aff_shift[0];
	index |= ((index_t)MPIDR_EL1_get_Aff1(&mpidr) &
		  (index_t)mapping->aff_mask[1])
		 << mapping->aff_shift[1];
	index |= ((index_t)MPIDR_EL1_get_Aff2(&mpidr) &
		  (index_t)mapping->aff_mask[2])
		 << mapping->aff_shift[2];
	index |= ((index_t)MPIDR_EL1_get_Aff3(&mpidr) &
		  (index_t)mapping->aff_mask[3])
		 << mapping->aff_shift[3];

	return index;
}

bool
platform_cpu_map_mpidr_valid(const platform_mpidr_mapping_t *mapping,
			     MPIDR_EL1_t		     mpidr)
{
	bool valid = true;

	assert(mapping->aff_shift[0] < 32U);
	assert(mapping->aff_shift[1] < 32U);
	assert(mapping->aff_shift[2] < 32U);
	assert(mapping->aff_shift[3] < 32U);

	if ((MPIDR_EL1_get_Aff0(&mpidr) & ~mapping->aff_mask[0]) != 0U) {
		valid = false;
	}
	if ((MPIDR_EL1_get_Aff1(&mpidr) & ~mapping->aff_mask[1]) != 0U) {
		valid = false;
	}
	if ((MPIDR_EL1_get_Aff2(&mpidr) & ~mapping->aff_mask[2]) != 0U) {
		valid = false;
	}
	if ((MPIDR_EL1_get_Aff3(&mpidr) & ~mapping->aff_mask[3]) != 0U) {
		valid = false;
	}

	return valid;
}

MPIDR_EL1_t
platform_cpu_index_to_mpidr(index_t index)
{
	platform_mpidr_mapping_t mapping = platform_cpu_get_mpidr_mapping();
	return platform_cpu_map_index_to_mpidr(&mapping, index);
}

index_t
platform_cpu_mpidr_to_index(MPIDR_EL1_t mpidr)
{
	platform_mpidr_mapping_t mapping = platform_cpu_get_mpidr_mapping();
	return platform_cpu_map_mpidr_to_index(&mapping, mpidr);
}

bool
platform_cpu_mpidr_valid(MPIDR_EL1_t mpidr)
{
	platform_mpidr_mapping_t mapping = platform_cpu_get_mpidr_mapping();
	return platform_cpu_map_mpidr_valid(&mapping, mpidr);
}

```

`hyp/platform/arm_generic/build.conf`:

```conf
# © 2022 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

interface platform

arch_source aarch64 cpu.c

```

`hyp/platform/arm_pmu/aarch64/src/platform_pmu.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>

#include <hypregisters.h>

#include <compiler.h>
#include <cpulocal.h>
#include <irq.h>
#include <object.h>
#include <panic.h>
#include <partition.h>
#include <partition_alloc.h>
#include <preempt.h>
#include <trace.h>

#include <events/platform.h>

#include <asm/barrier.h>
#include <asm/sysregs.h>
#include <asm/system_registers.h>

#include "event_handlers.h"
#include "platform_pmu.h"

static hwirq_t *pmu_hwirq;
CPULOCAL_DECLARE_STATIC(bool, pmu_irq_active);

void
platform_pmu_handle_boot_cpu_cold_init(void)
{
	// Disable all the interrupts at the startup
	sysreg64_write(PMINTENCLR_EL1, ~0UL);
	CPULOCAL(pmu_irq_active) = false;

	if (pmu_hwirq != NULL) {
		irq_enable_local(pmu_hwirq);
	}
}

void
platform_pmu_handle_boot_hypervisor_start(void)
{
	// Create the PMU IRQ
	hwirq_create_t params = {
		.irq	= PLATFORM_PMU_IRQ,
		.action = HWIRQ_ACTION_PMU,
	};

	hwirq_ptr_result_t ret =
		partition_allocate_hwirq(partition_get_private(), params);

	if (ret.e != OK) {
		panic("Failed to create PMU IRQ");
	}

	if (object_activate_hwirq(ret.r) != OK) {
		panic("Failed to activate PMU IRQ");
	}

	pmu_hwirq = ret.r;

	irq_enable_local(pmu_hwirq);
}

bool
platform_pmu_is_hw_irq_pending(void)
{
	uint64_t pmovsset, pmintenset;

	sysreg64_read_ordered(PMINTENSET_EL1, pmintenset, asm_ordering);
	sysreg64_read_ordered(PMOVSSET_EL0, pmovsset, asm_ordering);

	return (pmovsset & pmintenset) != 0U;
}

void
platform_pmu_hw_irq_deactivate(void)
{
	if (CPULOCAL(pmu_irq_active)) {
		CPULOCAL(pmu_irq_active) = false;
		irq_deactivate(pmu_hwirq);
	}
}

error_t
arm_pmu_handle_power_cpu_suspend(void)
{
	platform_pmu_hw_irq_deactivate();

	return OK;
}

bool
platform_pmu_handle_irq_received(void)
{
	bool deactivate = true;

	if (platform_pmu_is_hw_irq_pending()) {
		CPULOCAL(pmu_irq_active) = true;
		trigger_platform_pmu_counter_overflow_event();

		// Leave the IRQ active until the guest has cleared the
		// corresponding overflow flag.
		deactivate = false;
	} else {
		TRACE(DEBUG, INFO, "Spurious PMU IRQ");
	}

	return deactivate;
}

```

`hyp/platform/arm_pmu/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

interface platform
types platform_pmu.tc
events platform_pmu.ev
arch_source aarch64 platform_pmu.c

```

`hyp/platform/arm_pmu/platform_pmu.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

module arm_pmu

subscribe boot_cpu_cold_init
	handler platform_pmu_handle_boot_cpu_cold_init()
	require_preempt_disabled

subscribe boot_hypervisor_start
	handler platform_pmu_handle_boot_hypervisor_start
	require_preempt_disabled

subscribe irq_received[HWIRQ_ACTION_PMU]
	handler platform_pmu_handle_irq_received()
	require_preempt_disabled

subscribe power_cpu_suspend()
	require_preempt_disabled

```

`hyp/platform/arm_pmu/platform_pmu.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

extend hwirq_action enumeration {
	pmu;
};

```

`hyp/platform/arm_rng/aarch64/src/rng.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>
#include <string.h>

#include <hypregisters.h>

#include <atomic.h>
#include <attributes.h>
#include <cpulocal.h>
#include <platform_prng.h>
#include <preempt.h>

#include <asm/barrier.h>

#if !defined(ARCH_ARM_FEAT_RNG) || !ARCH_ARM_FEAT_RNG
#error ARCH_ARM_FEAT_RNG not set
#endif

// We use a per-cpu counter in case the implementation is not shared, and we
// need to ensure reseeding occurs on each core. If the prng HW is shared,
// then the worst case reseeding interval is 32*(N cores).
CPULOCAL_DECLARE_STATIC(count_t, rng_reseed_count);

error_t NOINLINE
platform_get_entropy(platform_prng_data256_t *data)
{
	error_t	 ret	      = ERROR_FAILURE;
	count_t	 i	      = 0U;
	count_t	 retries      = 64;
	uint64_t prng_data[4] = { 0 };

	assert(data != NULL);

	do {
		bool	 ok;
		uint64_t res;
		__asm__ volatile("mrs	%[res], RNDR	;"
				 "cset	%w[ok], ne	;"
				 : [res] "=r"(res), [ok] "=r"(ok)::"cc");
		if (ok) {
			prng_data[i] = res;
			i++;
		} else {
			retries--;
		}
	} while ((i < 4U) && (retries != 0U));

	if (i == 4U) {
		(void)memscpy(data, sizeof(*data), &prng_data,
			      sizeof(prng_data));
		ret = OK;

		// Issue a reseed read, ignoring the result.
		uint64_t tmp = register_RNDRRS_read_ordered(&asm_ordering);
		(void)tmp;
	}

	return ret;
}

error_t NOINLINE
platform_get_random32(uint32_t *data)
{
	error_t	 ret	 = ERROR_BUSY;
	count_t	 retries = 16;
	uint64_t res;
	bool	 ok = false;

	assert(data != NULL);

	cpulocal_begin();

	do {
		__asm__ volatile("mrs	%[res], RNDR	;"
				 "cset	%w[ok], ne	;"
				 : [res] "=r"(res), [ok] "+r"(ok)::"cc");
		retries--;
	} while ((!ok) && (retries != 0U));

	if (ok) {
		*data = (uint32_t)res;
		ret   = OK;

		count_t count = CPULOCAL(rng_reseed_count)++;

		if ((count % 32) == 0U) {
			// Issue a reseed read, ignoring the result.
			uint64_t tmp =
				register_RNDRRS_read_ordered(&asm_ordering);
			(void)tmp;
		}
	}
	cpulocal_end();

	return ret;
}

error_t
platform_get_rng_uuid(uint32_t data[4])
{
	// Gunyah generic RNDR - ARM TRNG interface UUID
	data[0] = 0x45546e21U;
	data[1] = 0x92a1433dU;
	data[2] = 0xa2ea5fe2U;
	data[3] = 0x16397d4eU;
	return OK;
}

```

`hyp/platform/arm_rng/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

interface platform
arch_source aarch64 rng.c

```

`hyp/platform/arm_smccc/aarch64/src/smccc_call.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>

#include <preempt.h>
#include <smccc.h>

#if defined(INTERFACE_VCPU)
#include <cpulocal.h>
#include <platform_ipi.h>
#include <thread.h>
#include <vcpu.h>
#endif

#if defined(INTERFACE_SMC_TRACE)
#include <string.h>

#include <smc_trace.h>
#endif

static void
smccc_1_1_do_call(smccc_function_id_t fn_id, uint64_t (*args)[6],
		  uint64_t (*ret)[4], uint64_t *session_ret, uint32_t client_id)
{
#if defined(INTERFACE_SMC_TRACE)
	register_t trace_regs[SMC_TRACE_REG_MAX];

	trace_regs[0] = smccc_function_id_raw(fn_id);
	(void)memscpy(&trace_regs[1],
		      sizeof(trace_regs) - sizeof(trace_regs[0]), *args,
		      sizeof(*args));
	trace_regs[7] = client_id;

	smc_trace_log(SMC_TRACE_ID_EL2_64CAL, &trace_regs, 8U);
#endif

	register register_t x0 __asm__("x0") = smccc_function_id_raw(fn_id);
	register register_t x1 __asm__("x1") = (*args)[0];
	register register_t x2 __asm__("x2") = (*args)[1];
	register register_t x3 __asm__("x3") = (*args)[2];
	register register_t x4 __asm__("x4") = (*args)[3];
	register register_t x5 __asm__("x5") = (*args)[4];
	register register_t x6 __asm__("x6") = (*args)[5];
	register register_t x7 __asm__("x7") = client_id;

	// Note: In ARM DEN0028B (SMCCC is not versioned), and X4-X17 defined
	// as unpredictable scratch registers and may not be preserved after an
	// SMC call. From ARM DEN0028C, X4-X17 are explicitly required to be
	// preserved. There are three SMCCC versions called out (1.0, 1.1 and
	// 1.2 - DEN 0028C/D) with no mention of the previous defined behaviour,
	// or which version changed to SMC register return semantics. We
	// therefore treat X4-X17 return state as unpredictable here.
	//
	// Note too, the hypervisor EL1-EL2 SMCCC interface implemented does
	// preserve unused result registers and temporary registers X4-X17 for
	// future 1.2+ compatibility.
	__asm__ volatile("smc    #0\n"
			 : "+r"(x0), "+r"(x1), "+r"(x2), "+r"(x3), "+r"(x6),
			   "+r"(x4), "+r"(x5), "+r"(x7)
			 :
			 : "x8", "x9", "x10", "x11", "x12", "x13", "x14", "x15",
			   "x16", "x17", "memory");

	(*ret)[0] = x0;
	(*ret)[1] = x1;
	(*ret)[2] = x2;
	(*ret)[3] = x3;

	if (session_ret != NULL) {
		*session_ret = x6;
	}

#if defined(INTERFACE_SMC_TRACE)
	(void)memscpy(&trace_regs[0], sizeof(trace_regs), *ret, sizeof(*ret));
	trace_regs[4] = 0U;
	trace_regs[5] = 0U;
	trace_regs[6] = x6;

	smc_trace_log(SMC_TRACE_ID_EL2_64RET, &trace_regs, 7U);
#endif
}

void
smccc_1_1_call(smccc_function_id_t fn_id, uint64_t (*args)[6],
	       uint64_t (*ret)[4], uint64_t *session_ret, uint32_t client_id)
{
	assert(args != NULL);
	assert(ret != NULL);

#if defined(INTERFACE_VCPU)
	bool is_vcpu = (client_id != CLIENT_ID_HYP);
	bool is_fast = smccc_function_id_get_is_fast(&fn_id);

	if (is_vcpu && !is_fast) {
		preempt_disable();
		assert(thread_get_self()->kind == THREAD_KIND_VCPU);
		bool pending_wakeup = vcpu_block_start();
		if (pending_wakeup) {
			// Assert a local IPI. This notifies secure world of the
			// wakeup, while still allowing for the SMC to make some
			// progress.
			platform_ipi_one(cpulocal_get_index());
		}

		smccc_1_1_do_call(fn_id, args, ret, session_ret, client_id);

		if (!pending_wakeup) {
			vcpu_block_finish();
		}
		preempt_enable();
	} else
#endif
	{
		// Note: it is important that preemption is not disabled across
		// the SMC instruction in the fast call path, because it is used
		// via thread_freeze() to make PSCI calls that do not return.
		smccc_1_1_do_call(fn_id, args, ret, session_ret, client_id);
	}
}

```

`hyp/platform/arm_smccc/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

interface smccc
arch_source aarch64 smccc_call.c

```

`hyp/platform/arm_trng_fi/arm_trng.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

module arm_trng_fi

// Subscribe to the vcpu_trap_smc64 event directly instead of
// smccc_call_fast_*_event so that return values don't need to be passed on the
// stack.
subscribe vcpu_trap_smc64
	priority -10
subscribe vcpu_trap_hvc64
	priority -10

```

`hyp/platform/arm_trng_fi/arm_trng.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// Common type definitions used for
// ARM True Random Number Generator Firmware Interface.

define arm_trng_function enumeration(explicit) {
	TRNG_VERSION = 0x50;
	TRNG_FEATURES = 0x51;
	TRNG_GET_UUID = 0x52;
	TRNG_RNG = 0x53;
	// The last ID in the TRNG function ID range (0x50..0x5f).
	LAST_ID = 0x5F;
};

define arm_trng_ret enumeration(explicit) {
	SUCCESS = 0;
	NOT_SUPPORTED = -1;
	INVALID_PARAMETERS = -2;
	NO_ENTROPY = -3;
};

```

`hyp/platform/arm_trng_fi/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

interface platform
types arm_trng.tc
events arm_trng.ev
source arm_trng.c

```

`hyp/platform/arm_trng_fi/src/arm_trng.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>
#include <string.h>

#include <compiler.h>
#include <platform_prng.h>
#include <thread.h>
#include <util.h>

#include <asm/cache.h>
#include <asm/cpu.h>

#include "event_handlers.h"

// FIXME: ABI checks disabled since Linux driver is non-compliant.
#define LINUX_TRNG_WORKAROUND

static void NOINLINE
arm_trng_fi_read(vcpu_gpr_t *regs, uint64_t bits, bool smc64)
{
	// TRNG_RND requires x1-x3 to be zero on error
	regs->x[1] = 0x0U;
	regs->x[2] = 0x0U;
	regs->x[3] = 0x0U;

	if ((bits == 0U) || (bits > (smc64 ? 192U : 96U))) {
		regs->x[0] = (uint64_t)ARM_TRNG_RET_INVALID_PARAMETERS;
	} else {
		uint32_t data[192 / 32] = { 0 };
		count_t	 remain		= (count_t)bits;
		int32_t	 i		= (int32_t)util_array_size(data) - 1;

		assert(bits <= 192);

		// Read N-bits of entropy
		while (remain != 0U) {
			assert(i >= 0);

			error_t err = platform_get_random32(&data[i]);
			if (err != OK) {
				break;
			}
			if (remain < 32U) {
				// Mask any unrequested bits
				data[i] &= (uint32_t)util_mask(remain);
				remain = 0U;
			} else {
				remain -= 32U;
			}
			i--;
		}
		if (remain != 0U) {
			regs->x[0] = (uint64_t)ARM_TRNG_RET_NO_ENTROPY;
			goto out;
		}
		// Copy out entropy
		if (smc64) {
			regs->x[3] = data[5] | ((uint64_t)data[4] << 32);
			regs->x[2] = data[3] | ((uint64_t)data[2] << 32);
			regs->x[1] = data[1] | ((uint64_t)data[0] << 32);
		} else {
			regs->x[3] = data[5];
			regs->x[2] = data[4];
			regs->x[1] = data[3];
		}
		// Erase entropy from the stack
		(void)memset_s(data, sizeof(data), 0, sizeof(data));
		CACHE_CLEAN_INVALIDATE_OBJECT(data);

		regs->x[0] = (uint64_t)ARM_TRNG_RET_SUCCESS;
	}
out:
	return;
}

static bool
arm_trng_fi_check_mbz(index_t ra, index_t rb, bool smc64)
{
	assert(rb > ra);
#if defined(LINUX_TRNG_WORKAROUND)

	(void)ra;
	(void)rb;
	(void)smc64;
	return true;
#else
	thread_t *current = thread_get_self();
	bool	  failed  = false;

	for (index_t i = ra; i <= rb; i++) {
		register_t r = current->vcpu_regs_gpr.x[i];
		if (!smc64) {
			r = r & 0xffffffffU;
		}
		if (r != 0x0U) {
			failed = true;
		}
	}

	return !failed;
#endif
}

static bool
arm_trng_fi_handle_call(void)
{
	bool		    handled = false;
	thread_t	   *current = thread_get_self();
	smccc_function_id_t function_id =
		smccc_function_id_cast((uint32_t)current->vcpu_regs_gpr.x[0]);
	smccc_owner_id_t owner_id =
		smccc_function_id_get_owner_id(&function_id);
	smccc_function_t function =
		smccc_function_id_get_function(&function_id);

	if (compiler_expected((owner_id != SMCCC_OWNER_ID_STANDARD) ||
			      (!smccc_function_id_get_is_fast(&function_id)))) {
		goto out;
	}
	if ((function < (smccc_function_t)ARM_TRNG_FUNCTION__MIN) ||
	    (function > (smccc_function_t)ARM_TRNG_FUNCTION__MAX)) {
		goto out;
	}

	arm_trng_function_t f = (arm_trng_function_t)function;
	bool is_smc64	      = smccc_function_id_get_is_smc64(&function_id);

	// Setup the default return
	handled = true;

	// Default return is Unknown Function
	if (is_smc64) {
		current->vcpu_regs_gpr.x[0] = SMCCC_UNKNOWN_FUNCTION64;
	} else {
		current->vcpu_regs_gpr.x[0] = SMCCC_UNKNOWN_FUNCTION32;
	}

	if (is_smc64) {
		switch (f) {
		case ARM_TRNG_FUNCTION_TRNG_RNG:
			if (!arm_trng_fi_check_mbz(2, 7, is_smc64)) {
				current->vcpu_regs_gpr.x[0] = (uint64_t)
					ARM_TRNG_RET_INVALID_PARAMETERS;
				break;
			}

			arm_trng_fi_read(&current->vcpu_regs_gpr,
					 current->vcpu_regs_gpr.x[1], is_smc64);
			break;
		case ARM_TRNG_FUNCTION_TRNG_VERSION:
		case ARM_TRNG_FUNCTION_TRNG_FEATURES:
		case ARM_TRNG_FUNCTION_TRNG_GET_UUID:
		case ARM_TRNG_FUNCTION_LAST_ID:
		default:
			// Unimplemented
			break;
		}
	} else {
		switch (f) {
		case ARM_TRNG_FUNCTION_TRNG_VERSION:
			if (!arm_trng_fi_check_mbz(1, 7, is_smc64)) {
				current->vcpu_regs_gpr.x[0] = (uint64_t)
					ARM_TRNG_RET_INVALID_PARAMETERS;
				break;
			}

			current->vcpu_regs_gpr.x[0] = 0x10000;
			current->vcpu_regs_gpr.x[1] = 0x0;
			current->vcpu_regs_gpr.x[2] = 0x0;
			current->vcpu_regs_gpr.x[3] = 0x0;

			break;
		case ARM_TRNG_FUNCTION_TRNG_FEATURES: {
			if (!arm_trng_fi_check_mbz(2, 7, is_smc64)) {
				current->vcpu_regs_gpr.x[0] = (uint64_t)
					ARM_TRNG_RET_INVALID_PARAMETERS;
				break;
			}

			current->vcpu_regs_gpr.x[0] =
				(uint64_t)ARM_TRNG_RET_NOT_SUPPORTED;

			smccc_function_id_t fid = smccc_function_id_cast(
				(uint32_t)current->vcpu_regs_gpr.x[1]);
			if ((smccc_function_id_get_owner_id(&fid) !=
			     SMCCC_OWNER_ID_STANDARD) ||
			    !smccc_function_id_get_is_fast(&fid) ||
			    (smccc_function_id_get_res0(&fid) != 0U)) {
				break;
			}
			smccc_function_t fn =
				smccc_function_id_get_function(&fid);
			switch ((arm_trng_function_t)fn) {
			case ARM_TRNG_FUNCTION_TRNG_VERSION:
			case ARM_TRNG_FUNCTION_TRNG_FEATURES:
			case ARM_TRNG_FUNCTION_TRNG_GET_UUID:
				if (!smccc_function_id_get_is_smc64(&fid)) {
					current->vcpu_regs_gpr.x[0] =
						(uint64_t)ARM_TRNG_RET_SUCCESS;
				}
				break;
			case ARM_TRNG_FUNCTION_TRNG_RNG:
				current->vcpu_regs_gpr.x[0] =
					(uint64_t)ARM_TRNG_RET_SUCCESS;
				break;
			case ARM_TRNG_FUNCTION_LAST_ID:
			default:
				// Nothing to do
				break;
			}
			break;
		}
		case ARM_TRNG_FUNCTION_TRNG_GET_UUID:
			if (!arm_trng_fi_check_mbz(1, 7, is_smc64)) {
				current->vcpu_regs_gpr.x[0] = (uint64_t)
					ARM_TRNG_RET_INVALID_PARAMETERS;
				break;
			}
			uint32_t uuid[4] = { 0xffffffffU, 0U, 0U, 0U };

			if (platform_get_rng_uuid(uuid) != OK) {
				current->vcpu_regs_gpr.x[0] =
					(uint64_t)ARM_TRNG_RET_NOT_SUPPORTED;
				break;
			}
			assert(uuid[0] != 0xffffffffU);

			current->vcpu_regs_gpr.x[0] = uuid[0];
			current->vcpu_regs_gpr.x[1] = uuid[1];
			current->vcpu_regs_gpr.x[2] = uuid[2];
			current->vcpu_regs_gpr.x[3] = uuid[3];

			break;
		case ARM_TRNG_FUNCTION_TRNG_RNG:
			if (!arm_trng_fi_check_mbz(2, 7, is_smc64)) {
				current->vcpu_regs_gpr.x[0] = (uint64_t)
					ARM_TRNG_RET_INVALID_PARAMETERS;
				break;
			}

			arm_trng_fi_read(
				&current->vcpu_regs_gpr,
				(uint64_t)(uint32_t)current->vcpu_regs_gpr.x[1],
				is_smc64);
			break;
		case ARM_TRNG_FUNCTION_LAST_ID:
		default:
			// Nothing to do
			break;
		}
	}

out:
	return handled;
}

bool
arm_trng_fi_handle_vcpu_trap_smc64(ESR_EL2_ISS_SMC64_t iss)
{
	bool handled = false;

	if (compiler_unexpected(ESR_EL2_ISS_SMC64_get_imm16(&iss) ==
				(uint16_t)0U)) {
		handled = arm_trng_fi_handle_call();
	}

	return handled;
}

bool
arm_trng_fi_handle_vcpu_trap_hvc64(ESR_EL2_ISS_HVC_t iss)
{
	bool handled = false;

	if (compiler_unexpected(ESR_EL2_ISS_HVC_get_imm16(&iss) ==
				(uint16_t)0U)) {
		handled = arm_trng_fi_handle_call();
	}

	return handled;
}

```

`hyp/platform/ete/aarch64/ete.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause


define ETE_TRCLAR_UNLOCK constant = 0xC5ACCE55;
define ETE_TRCLAR_LOCK constant = 0x0;

define TRCSTATR bitfield<64> {
	0	idle		bool;
	1	pmstable	bool;
	others	unknown = 0;
};

define TRCPRGCTLR bitfield<64> {
	0	EN		bool;
	others	unknown = 0;
};

define TRCIDR5 bitfield<64> {
	11:9		NUMEXTINSEL	uint8;
	27:25		NUMSEQSTATE	uint8;
	30:28		NUMCNTR	uint8;
	others		unknown = 0;
};

define TRCIDR4 bitfield<64> {
	3:0		NUMACPAIRS	uint8;
	7:4		NUMDVC		uint8;
	8		SUPPDAC	uint8;
	15:12		NUMPC		uint8;
	19:16		NUMRSPAIR	uint8;
	23:20		NUMSSCC	uint8;
	27:24		NUMCIDC	uint8;
	31:28		NUMVMIDC	uint8;
	others		unknown = 0;
};

define TRCIDR3 bitfield<64> {
	26		STALLCTL	bool;
	others		unknown = 0;
};

define TRCIDR2 bitfield<64> {
	9:5		CIDSIZE	uint8;
	14:10		VMIDSIZE	uint8;
	others		unknown = 0;
};

define TRCIDR0 bitfield<64> {
	14		QFILT		uint8;
	others		unknown = 0;
};

define ete_context structure {
	TRCPRGCTLR	uint64;
	TRCCONFIGR	uint64;
	TRCAUXCTLR	uint64;
	TRCEVENTCTL0R	uint64;
	TRCEVENTCTL1R	uint64;
	TRCRSR		uint64;

	TRCSTALLCTLR	uint64;

	TRCTSCTLR	uint64;
	TRCSYNCPR	uint64;
	TRCCCCTLR	uint64;
	TRCBBCTLR	uint64;
	TRCTRACEIDR	uint64;
	TRCQCTLR	uint64;
	TRCVICTLR	uint64;
	TRCVIIECTLR	uint64;
	TRCVISSCTLR	uint64;

#if TRCIDR4_NUMPC > 0
	TRCVIPCSSCTLR	uint64;
#endif

#if TRCIDR5_NUMSEQSTATE > 0
	TRCSEQEVR	array(3) uint64;
#endif

	TRCSEQRSTEVR	uint64;
	TRCSEQSTR	uint64;

	TRCEXTINSELR	array(TRCIDR5_NUMEXTINSEL) uint64;

	TRCCNTRLDVR	array(TRCIDR5_NUMCNTR) uint64;

	TRCCNTCTLR	array(TRCIDR5_NUMCNTR) uint64;

	TRCCNTVR	array(TRCIDR5_NUMCNTR) uint64;

	// following are optional
	// TRCIMSPEC1	uint64;
	// TRCIMSPEC2	uint64;
	// TRCIMSPEC3	uint64;
	// TRCIMSPEC4	uint64;
	// TRCIMSPEC5	uint64;
	// TRCIMSPEC6	uint64;
	// TRCIMSPEC7	uint64;

	// special case
	// ((TRCIDR4.NUMRSPAIR + 1) * 2) > n
	// range 2 - 31
	TRCRSCTLR	array(TRCRSCTLR_CNT) uint64;

	TRCSSCCR	array(TRCIDR4_NUMSSCC) uint64;

	TRCSSCSR	array(TRCIDR4_NUMSSCC) uint64;

#if TRCIDR4_NUMPC > 0
	// special case
	// TRCSSCSR<n>.PC == 0b1
	TRCSSPCICR	array(TRCIDR4_NUMPC) uint64;
#endif


	// TRCIDR4.NUMACPAIRS * 2 > n
	TRCACVR	array(TRCACVR_CNT) uint64;

	// TRCIDR4.NUMACPAIRS * 2 > n
	TRCACATR	array(TRCACATR_CNT) uint64;

	TRCCIDCVR	array(TRCIDR4_NUMCIDC) uint64;

	TRCVMIDCVR	array(TRCIDR4_NUMVMIDC) uint64;

#if (TRCIDR4_NUMCIDC > 0x0) && (TRCIDR2_CIDSIZE > 0)
	TRCCIDCCTLR0	uint64;
#endif

#if (TRCIDR4_NUMCIDC > 0x4) && (TRCIDR2_CIDSIZE > 0)
	TRCCIDCCTLR1	uint64;
#endif

#if (TRCIDR4_NUMVMIDC > 0x0) && (TRCIDR2_VMIDSIZE > 0)
	TRCVMIDCCTLR0	uint64;
#endif

#if (TRCIDR4_NUMVMIDC > 0x4) && (TRCIDR2_VMIDSIZE > 0)
	TRCVMIDCCTLR1	uint64;
#endif
	TRFCR_EL1	bitfield TRFCR_EL1;
};

```

`hyp/platform/ete/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

arch_types aarch64 ete.tc
local_include
template typed ete_save_restore.h
source ete.c
events ete.ev

```

`hyp/platform/ete/ete.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

module ete

subscribe boot_cpu_cold_init()

```

`hyp/platform/ete/include/ete.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

void
ete_save_context_percpu(cpu_index_t cpu, bool may_poweroff);

void
ete_restore_context_percpu(cpu_index_t cpu, bool was_poweroff);

```

`hyp/platform/ete/src/ete.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>

#include <hypregisters.h>

#include <atomic.h>
#include <compiler.h>
#include <cpulocal.h>
#include <hyp_aspace.h>
#include <log.h>
#include <panic.h>
#include <partition.h>
#include <platform_timer.h>
#include <trace.h>
#include <vet.h>

#include <asm/barrier.h>
#include <asm/sysregs.h>

#include "ete.h"
#include "ete_save_restore.h"
#include "event_handlers.h"

CPULOCAL_DECLARE_STATIC(ete_context_t, ete_contexts);
CPULOCAL_DECLARE_STATIC(uint64_t, ete_claim_tag);

void
ete_handle_boot_cpu_cold_init(void)
{
	TRCIDR2_t trcidr2 = register_TRCIDR2_read();
	TRCIDR4_t trcidr4 = register_TRCIDR4_read();
	TRCIDR5_t trcidr5 = register_TRCIDR5_read();

	(void)trcidr2;
	assert(TRCIDR2_get_CIDSIZE(&trcidr2) == TRCIDR2_CIDSIZE);
	assert(TRCIDR2_get_VMIDSIZE(&trcidr2) == TRCIDR2_VMIDSIZE);

	(void)trcidr4;
	assert(TRCIDR4_get_NUMPC(&trcidr4) == TRCIDR4_NUMPC);

	assert(TRCIDR4_get_NUMRSPAIR(&trcidr4) == TRCIDR4_NUMRSPAIR);
	assert(TRCIDR4_get_NUMACPAIRS(&trcidr4) == TRCIDR4_NUMACPAIRS);

	assert(TRCIDR4_get_NUMSSCC(&trcidr4) == TRCIDR4_NUMSSCC);
	assert(TRCIDR4_get_NUMCIDC(&trcidr4) == TRCIDR4_NUMCIDC);
	assert(TRCIDR4_get_NUMVMIDC(&trcidr4) == TRCIDR4_NUMVMIDC);

	(void)trcidr5;
	assert(TRCIDR5_get_NUMSEQSTATE(&trcidr5) == TRCIDR5_NUMSEQSTATE);
	assert(TRCIDR5_get_NUMEXTINSEL(&trcidr5) == TRCIDR5_NUMEXTINSEL);
	assert(TRCIDR5_get_NUMCNTR(&trcidr5) == TRCIDR5_NUMCNTR);
}

static void
ete_wait_stable(bool wait_pmstable, bool wait_idle)
{
	bool pmstable = false;
	bool idle     = false;

	// wait 100us
	ticks_t start = platform_timer_get_current_ticks();
	ticks_t timeout =
		start + platform_timer_convert_ns_to_ticks(100U * 1000U);
	while (1) {
		TRCSTATR_t trcstatr =
			register_TRCSTATR_read_ordered(&vet_ordering);

		// should be a volatile read
		pmstable = TRCSTATR_get_pmstable(&trcstatr);
		idle	 = TRCSTATR_get_idle(&trcstatr);

		// compilated for exit condition:
		// * when pmstable and idle are true
		// * when idle are true and not check pmstable
		// * when pmstable are true and not check idle
		if ((pmstable && idle) || (idle && (!wait_pmstable)) ||
		    (pmstable && (!wait_idle))) {
			break;
		}

		if (platform_timer_get_current_ticks() > timeout) {
			TRACE_AND_LOG(ERROR, INFO,
				      "ETE: programmers model is not stable");
			break;
		}
	}
}

void
ete_save_context_percpu(cpu_index_t cpu, bool may_poweroff)
{
	// Synchronise the trace unit. EL2 trace is always prohibited, so
	// we don't need to prohibit trace first.
	__asm__ volatile("tsb csync" : "+m"(vet_ordering));

	ete_context_t *ctx = &CPULOCAL_BY_INDEX(ete_contexts, cpu);

	// Save and clear TRCPRGCTLR
	ctx->TRCPRGCTLR = register_TRCPRGCTLR_read_ordered(&vet_ordering);
	register_TRCPRGCTLR_write_ordered(0U, &vet_ordering);
	asm_context_sync_ordered(&vet_ordering);

	if (may_poweroff) {
		// Wait until the programming interface is stable
		ete_wait_stable(true, false);

		// Save the remaining registers
		ete_save_registers(ctx, &vet_ordering);
		CPULOCAL_BY_INDEX(ete_claim_tag, cpu) =
			register_TRCCLAIMCLR_read_ordered(&vet_ordering);

		// Wait until all writes to the trace buffer are complete
		ete_wait_stable(false, true);
	} else {
		// Wait until all writes to the trace buffer are complete
		ete_wait_stable(true, true);
	}
}

void
ete_restore_context_percpu(cpu_index_t cpu, bool was_poweroff)
{
	ete_context_t *ctx = &CPULOCAL_BY_INDEX(ete_contexts, cpu);

	if (was_poweroff) {
		// Restore all of the registers other than TRCPRGCTLR
		ete_restore_registers(ctx, &vet_ordering);
		asm_context_sync_ordered(&vet_ordering);
	}

	register_TRCPRGCTLR_write_ordered(ctx->TRCPRGCTLR, &vet_ordering);
	asm_context_sync_ordered(&vet_ordering);

	register_TRCCLAIMSET_write_ordered(
		CPULOCAL_BY_INDEX(ete_claim_tag, cpu), &vet_ordering);
}

```

`hyp/platform/ete/templates/ete_save_restore.h.tmpl`:

```tmpl
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

static void
ete_save_registers(ete_context_t *context,
    asm_ordering_dummy_t *ordering_var)
{
#set ete_contexts = [c for c in $definitions \
     if c.category == 'structure' and c.type_name == 'ete_context']
#assert len($ete_contexts) == 1
#set ete_context = $ete_contexts[0]

#for d in $ete_context.declarations
    #if d.member_name == "TRCRSCTLR"
        #continue
    #end if
    #set is_array = False
    #set array_size = 0

    #if $d.complex_type
        #set current_type = $d.compound_type

        #while $current_type is not None
            #if $current_type.is_array
                #set is_array = True
                #set array_size = $current_type.length
                #break
            #end if

            #set $current_type = $current_type.base_type

            #if $current_type is None
                #break
            #end if
        #end while
    #end if

    #if $is_array
        #set idx = 0
        #set reg_offset = 0

        #if d.member_name == "TRCRSCTLR"
            #set reg_offset = 2
        #end if

        #while $idx < $array_size
            #set reg_idx = $idx + $reg_offset

            #if d.member_name == "TRCSSPCICR"
                uint64_t trcsscsr${reg_idx} = register_TRCSSCSR${reg_idx}_read_ordered(ordering_var);
                if (trcsscsr${reg_idx} & 0x8) {
            #end if

                    context->${d.member_name}[${idx}] = register_${d.member_name}${reg_idx}_read_ordered(ordering_var);

            #if d.member_name == "TRCSSPCICR"
                }
            #end if

            #set idx = $idx + 1
        #end while
    #else
        #if d.member_name == "TRCSTALLCTLR"
            TRCIDR3_t trcidr3 = register_TRCIDR3_read();
            if (TRCIDR3_get_STALLCTL(&trcidr3)) {
        #else if d.member_name == "TRCQCTLR"
            TRCIDR0_t trcidr0 = register_TRCIDR0_read();
            if (TRCIDR0_get_QFILT(&trcidr0)) {
        #end if

                context->$d.member_name = register_${d.member_name}_read_ordered(ordering_var);

        #if d.member_name == "TRCSTALLCTLR"
            }
        #else if d.member_name == "TRCQCTLR"
            }
        #end if
    #end if
#end for
}

static void
ete_restore_registers(ete_context_t *context,
    asm_ordering_dummy_t *ordering_var)
{
#set ete_contexts = [c for c in $definitions \
     if c.category == 'structure' and c.type_name == 'ete_context']
#assert len($ete_contexts) == 1
#set ete_context = $ete_contexts[0]

#for d in $ete_context.declarations
    #if d.member_name == "TRCRSCTLR"
        #continue
    #end if
    #set is_array = False
    #set array_size = 0

    #if $d.complex_type
        #set current_type = $d.compound_type

        #while $current_type is not None
            #if $current_type.is_array
                #set is_array = True
                #set array_size = $current_type.length
                #break
            #end if

            #set $current_type = $current_type.base_type

            #if $current_type is None
                #break
            #end if
        #end while
    #end if

    #if $is_array
        #set idx = 0
        #set reg_offset = 0

        #if d.member_name == "TRCRSCTLR"
            #set reg_offset = 2
        #end if

        #while $idx < $array_size
            #set reg_idx = $idx + $reg_offset

            #if d.member_name == "TRCSSPCICR"
                uint64_t trcsscsr${reg_idx} = register_TRCSSCSR${reg_idx}_read_ordered(ordering_var);
                if (trcsscsr${reg_idx} & 0x8) {
            #end if

                    register_${d.member_name}${reg_idx}_write_ordered(context->${d.member_name}[${idx}], ordering_var);

            #if d.member_name == "TRCSSPCICR"
                }
            #end if

            #set idx = $idx + 1
        #end while
    #else
        #if d.member_name == "TRCSTALLCTLR"
            TRCIDR3_t trcidr3 = register_TRCIDR3_read();
            if (TRCIDR3_get_STALLCTL(&trcidr3)) {
        #else if d.member_name == "TRCQCTLR"
            TRCIDR0_t trcidr0 = register_TRCIDR0_read();
            if (TRCIDR0_get_QFILT(&trcidr0)) {
        #end if

                register_${d.member_name}_write_ordered(context->$d.member_name, ordering_var);

        #if d.member_name == "TRCSTALLCTLR"
            }
        #else if d.member_name == "TRCQCTLR"
            }
        #end if

    #end if
#end for
}

```

`hyp/platform/etm/aarch64/etm-regs.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

define ETM_TRCVI_CTLR_EXLEVEL_S bitfield<4> {
	0		el0		bool = 0x1;
	1		el1		bool = 0x1;
	2		res0		bool;
	3		el3		bool = 0x1;
};

define ETM_TRCVI_CTLR_EXLEVEL_NS bitfield<4> {
	0		el0		bool = 0x1;
	1		el1		bool = 0x1;
	2		el2		bool = 0x1;
	3		res0		bool;
};

define ETM_TRCVI_CTLR bitfield<32> {
	7:0		event			uint8;
	8		res0			bool;
	9		ssstatus		bool;
	10		trcreset		bool;
	11		trcerr			bool;
	15:12		res1			uint8;
	19:16		exlevel_s		uint8 = 0xf;
	23:20		exlevel_ns		uint8 = 0xf;
	31:24		unknown=0;
};

define ETM_EVENT bitfield<8> {
	4:0	sel			uint8;
	6:5	res0			uint8;
	7	is_single_resource	bool;
};


define ETM_TRCSTATR bitfield<32> {
	0	idle		bool;
	1	pmstable	bool;
	31:2	res0		uint32;
};

define ETM_TRCOSLSR bitfield<32> {
	3,0	oslm		uint8;
	1	oslk		bool;
	2	ntt		bool;
};

```

`hyp/platform/etm/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

types etm.tc
arch_types aarch64 etm-regs.tc
local_include
events etm.ev
source etm.c

```

`hyp/platform/etm/etm.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

module etm

subscribe boot_hypervisor_start
	// Run after soc module init so we can query security state
	priority -1

subscribe power_cpu_online
	require_preempt_disabled

subscribe power_cpu_suspend(may_poweroff)
	unwinder(may_poweroff)
	require_preempt_disabled

subscribe power_cpu_resume(was_poweroff)
	require_preempt_disabled

subscribe power_cpu_offline
	require_preempt_disabled

```

`hyp/platform/etm/etm.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// register offset

define ETM_TRCLAR_UNLOCK constant = 0xC5ACCE55;
define ETM_TRCLAR_LOCK constant = 0x0;

define ETM_TRCOSLAR_UNLOCK constant = 0x0;
define ETM_TRCOSLAR_LOCK constant = 0x1;

define ETM_TRCPRGCTLR_ENABLE constant = 0x1;

define etm_trcdv structure(aligned(16)) {
	value	uint64;
};

define etm structure(aligned(PGTABLE_HYP_PAGE_SIZE)) {
	// main control & configuration regsters
	trcprgctlr @ 0x4	uint32(atomic);
	trcprocselr @ 0x8	uint32(atomic);
	trcstatr @ 0xC		bitfield ETM_TRCSTATR(atomic);
	trcconfigr @ 0x10	uint32(atomic);
	trcauxctlr @ 0x18	uint32(atomic);
	trceventctl0r @ 0x20	uint32(atomic);
	trceventctl1r @ 0x24	uint32(atomic);
	trcstallctlr @ 0x2c	uint32(atomic);
	trctsctlr @ 0x30	uint32(atomic);
	trcsyncpr @ 0x34	uint32(atomic);
	trcccctlr @ 0x38	uint32(atomic);
	trcbbctlr @ 0x3c	uint32(atomic);
	trctraceidr @ 0x40	uint32(atomic);
	trcqctlr @ 0x44		uint32(atomic);

	// filtering control registers
	trcvictlr @ 0x80	uint32(atomic);
	trcviiectlr @ 0x84	uint32(atomic);
	trcvissctlr @ 0x88	uint32(atomic);
	trcvipcssctlr @ 0x8c	uint32(atomic);
	trcvdctlr @ 0xa0	uint32(atomic);
	trcvdsacctlr @ 0xa4	uint32(atomic);
	trcvdarcctlr @ 0xa8	uint32(atomic);

	// derived resources registers
	trcseqevr @ 0x100	array(3) uint32(atomic);
	trcseqrstevr @ 0x118	uint32(atomic);
	trcseqstr @ 0x11c	uint32(atomic);
	trcextinselr @ 0x120	uint32(atomic);
	trccntrldvr @ 0x140	array(4) uint32(atomic);
	trccntctlr @ 0x150	array(4) uint32(atomic);
	trccntvr @ 0x160	array(4) uint32(atomic);

	// resource selection registers (note: elements 0 and 1 are reserved)
	trcrsctlr2 @ 0x208	array(30) uint32(atomic);

	// single shot comparator registers
	trcssccr @ 0x280	array(8) uint32(atomic);
	trcsscsr @ 0x2a0	array(8) uint32(atomic);
	trcsspcicr @ 0x2c0	array(8) uint32(atomic);

	// OS lock registers
	trcoslar @ 0x0300	uint32(atomic);
	trcoslsr @ 0x0304	bitfield ETM_TRCOSLSR(atomic);

	// comparator registers
	trcacvr @ 0x400		array(16) uint64(atomic);
	trcacatr @ 0x480	array(16) uint64(atomic);
	trcdvcvr @ 0x500	array(8) structure etm_trcdv;
	trcdvcmr @ 0x580	array(8) structure etm_trcdv;
	trccidcvr @ 0x600	array(8) uint64(atomic);
	trcvmidcvr @ 0x640	array(8) uint64(atomic);
	trccidcctlr @ 0x680	array(2) uint32(atomic);
	trcvmidcctlr @ 0x688	array(2) uint32(atomic);

	// Software lock and claim tag registers
	trcclaimset @ 0xfa0	uint32(atomic);
	trcclaimclr @ 0xfa4	uint32(atomic);
	trclar @ 0x0FB0		uint32(atomic);
};

```

`hyp/platform/etm/include/etm.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

void
etm_set_reg(cpu_index_t cpu, size_t offset, register_t val, size_t access_size);

void
etm_get_reg(cpu_index_t cpu, size_t offset, register_t *val,
	    size_t access_size);

void
etm_save_context_percpu(cpu_index_t cpu);

void
etm_restore_context_percpu(cpu_index_t cpu);

```

`hyp/platform/etm/src/etm.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>
#include <string.h>

#include <atomic.h>
#include <compiler.h>
#include <cpulocal.h>
#include <hyp_aspace.h>
#include <log.h>
#include <panic.h>
#include <partition.h>
#include <pgtable.h>
#include <platform_security.h>
#include <platform_timer.h>
#include <trace.h>
#include <util.h>

#include "etm.h"
#include "event_handlers.h"

#if defined(PLATFORM_ETM_REG_WRITE_WORKAROUND)
// this work around is for context save, since we are writing lots of registers
// back to back, it could block other master on NOC.
#define CTX_WRITE_WORKAROUND platform_timer_ndelay(20000)
#else
#define CTX_WRITE_WORKAROUND
#endif

#if !defined(ETM_USE_SOFTWARE_LOCK)
// Using or implementing TRCLAR is deprecated. Linux doesn't use it.
#define ETM_USE_SOFTWARE_LOCK 0
#endif

typedef struct {
	size_t reg_offset;
	size_t access_size;
	size_t count;
	size_t stride;
} context_register_info_t;

static etm_t *mapped_etms[PLATFORM_MAX_CORES];

static register_t *etm_contexts[PLATFORM_MAX_CORES];

static uint32_t etm_claim_tag[PLATFORM_MAX_CORES];

static uint32_t etm_cprgctlr[PLATFORM_MAX_CORES];

#define ETM_REGISTER(name)                                                     \
	{                                                                      \
		.reg_offset  = offsetof(etm_t, name),                          \
		.access_size = util_sizeof_member(etm_t, name), .count = 1,    \
		.stride = 0                                                    \
	}

#define ETM_REGISTER_ARRAY(name)                                               \
	{                                                                      \
		.reg_offset  = offsetof(etm_t, name),                          \
		.access_size = util_sizeof_member(etm_t, name[0]),             \
		.count	     = util_sizeof_member(etm_t, name) /               \
			 util_sizeof_member(etm_t, name[0]),                   \
		.stride = util_sizeof_member(etm_t, name[0])                   \
	}

#define ETM_REGISTER_SPARSE_ARRAY(name)                                        \
	{                                                                      \
		.reg_offset  = offsetof(etm_t, name),                          \
		.access_size = util_sizeof_member(etm_t, name[0].value),       \
		.count	     = util_sizeof_member(etm_t, name) /               \
			 util_sizeof_member(etm_t, name[0]),                   \
		.stride = util_sizeof_member(etm_t, name[0])                   \
	}

// NOTE: registers are saved in the context memory region based on their
// index in context_register_list. Make sure the alignment is correct
static const context_register_info_t context_register_list[] = {
	// main control & configuration regsters
	ETM_REGISTER(trcprocselr),
	ETM_REGISTER(trcconfigr),
	ETM_REGISTER(trcauxctlr),
	ETM_REGISTER(trceventctl0r),
	ETM_REGISTER(trceventctl1r),
	ETM_REGISTER(trcstallctlr),
	ETM_REGISTER(trctsctlr),
	ETM_REGISTER(trcsyncpr),
	ETM_REGISTER(trcccctlr),
	ETM_REGISTER(trcbbctlr),
	ETM_REGISTER(trctraceidr),
	ETM_REGISTER(trcqctlr),

	// filtering control registers
	ETM_REGISTER(trcvictlr),
	ETM_REGISTER(trcviiectlr),
	ETM_REGISTER(trcvissctlr),
	ETM_REGISTER(trcvipcssctlr),
	ETM_REGISTER(trcvdctlr),
	ETM_REGISTER(trcvdsacctlr),
	ETM_REGISTER(trcvdarcctlr),

	// derived resources registers
	ETM_REGISTER_ARRAY(trcseqevr),
	ETM_REGISTER(trcseqrstevr),
	ETM_REGISTER(trcseqstr),
	ETM_REGISTER(trcextinselr),
	ETM_REGISTER_ARRAY(trccntrldvr),
	ETM_REGISTER_ARRAY(trccntctlr),
	ETM_REGISTER_ARRAY(trccntvr),

	// resource selection registers
	ETM_REGISTER_ARRAY(trcrsctlr2),

	// comparator registers
	ETM_REGISTER_ARRAY(trcacvr),
	ETM_REGISTER_ARRAY(trcacatr),

	ETM_REGISTER_SPARSE_ARRAY(trcdvcvr),
	ETM_REGISTER_SPARSE_ARRAY(trcdvcmr),

	ETM_REGISTER_ARRAY(trccidcvr),
	ETM_REGISTER_ARRAY(trccidcctlr),

	ETM_REGISTER_ARRAY(trcvmidcvr),
	ETM_REGISTER_ARRAY(trcvmidcctlr),

	// single shot comparator registers
	ETM_REGISTER_ARRAY(trcssccr),
	ETM_REGISTER_ARRAY(trcsscsr),
	ETM_REGISTER_ARRAY(trcsspcicr),
};

static size_t
etm_get_context_size_percpu(void)
{
	size_t ret = 0UL;

	// can be optimized by read last entry offset + size, but need to
	// restrict the timing to call this context
	for (index_t i = 0; i < util_array_size(context_register_list); i++) {
		const context_register_info_t *info = &context_register_list[i];
		ret += sizeof(uint64_t) * info->count;
	}
	return ret;
}

void
etm_handle_boot_hypervisor_start(void)
{
	if (compiler_expected(platform_security_state_debug_disabled())) {
		goto out;
	}

	partition_t *hyp_partition = partition_get_private();

	// FIXME: remove when read from device tree
	paddr_t etm_base   = PLATFORM_ETM_BASE;
	size_t	etm_stride = PLATFORM_ETM_STRIDE;

	for (cpu_index_t i = 0; cpulocal_index_valid(i); i++) {
		virt_range_result_t range =
			hyp_aspace_allocate(PLATFORM_ETM_SIZE_PERCPU);
		if (range.e != OK) {
			panic("ETM: Address allocation failed.");
		}

		paddr_t cur_base = etm_base + i * etm_stride;

		pgtable_hyp_start();

		error_t ret = pgtable_hyp_map(
			hyp_partition, range.r.base, PLATFORM_ETM_SIZE_PERCPU,
			cur_base, PGTABLE_HYP_MEMTYPE_NOSPEC_NOCOMBINE,
			PGTABLE_ACCESS_RW, VMSA_SHAREABILITY_NON_SHAREABLE);
		if (ret != OK) {
			panic("ETM: Mapping of etm register failed.");
		}
		mapped_etms[i] = (etm_t *)range.r.base;

		pgtable_hyp_commit();
	}

	// allocate contexts
	size_t context_size = etm_get_context_size_percpu();
	for (cpu_index_t i = 0; cpulocal_index_valid(i); i++) {
		void_ptr_result_t alloc_r = partition_alloc(
			hyp_partition, context_size, alignof(uint64_t));
		if (alloc_r.e != OK) {
			panic("failed to allocate ETM context memory");
		}

		etm_contexts[i] = (register_t *)alloc_r.r;
		(void)memset_s(etm_contexts[i], context_size, 0, context_size);
	}

out:
	return;
}

void
etm_set_reg(cpu_index_t cpu, size_t offset, register_t val, size_t access_size)
{
	(void)access_size;

	assert(cpulocal_index_valid(cpu));

	assert(offset < (sizeof(*mapped_etms[cpu]) - access_size));
	uintptr_t base = (uintptr_t)mapped_etms[cpu];

	if (access_size == sizeof(uint32_t)) {
		_Atomic uint32_t *reg = (_Atomic uint32_t *)(base + offset);
		atomic_store_relaxed(reg, val);
	} else if (access_size == sizeof(uint64_t)) {
		_Atomic uint64_t *reg = (_Atomic uint64_t *)(base + offset);
		atomic_store_relaxed(reg, val);
	} else {
		panic("ETM: invalid access size");
	}
}

void
etm_get_reg(cpu_index_t cpu, size_t offset, register_t *val, size_t access_size)
{
	(void)access_size;

	assert(cpulocal_index_valid(cpu));

	uintptr_t base = (uintptr_t)mapped_etms[cpu];

	if (access_size == sizeof(uint32_t)) {
		// Regards the ETM v4 doc: HW implementation supports 32-bit
		// accesses to access 32-bit registers or either half of a
		// 64-bit register.
		_Atomic uint32_t *reg = (_Atomic uint32_t *)(base + offset);

		*val = atomic_load_relaxed(reg);
	} else if (access_size == sizeof(uint64_t)) {
		_Atomic uint64_t *reg = (_Atomic uint64_t *)(base + offset);

		*val = atomic_load_relaxed(reg);
	} else {
		panic("ETM: invalid access size");
	}
}

static void
etm_unlock_percpu(cpu_index_t cpu)
{
#if ETM_USE_SOFTWARE_LOCK
	atomic_store_relaxed(&mapped_etms[cpu]->trclar, ETM_TRCLAR_UNLOCK);
	CTX_WRITE_WORKAROUND;
#else
	(void)cpu;
#endif
}

#if ETM_USE_SOFTWARE_LOCK
static void
etm_lock_percpu(cpu_index_t cpu)
{
	atomic_store_relaxed(&mapped_etms[cpu]->trclar, ETM_TRCLAR_LOCK);
	CTX_WRITE_WORKAROUND;
}
#endif

static void
etm_os_unlock_percpu(cpu_index_t cpu)
{
	atomic_store_relaxed(&mapped_etms[cpu]->trcoslar, ETM_TRCOSLAR_UNLOCK);

	// Note: no write delay workaround for this register, to avoid delaying
	// resume when the ETM is not being used. It is always written last
	// in the sequence anyway, so a delay after it is useless.
}

static void
etm_os_lock_percpu(cpu_index_t cpu)
{
	atomic_store_relaxed(&mapped_etms[cpu]->trcoslar, ETM_TRCOSLAR_LOCK);

	// Note: no write delay workaround for this register, to avoid delaying
	// suspend when the ETM is not being used. The suspend sequence should
	// start with a conditional CTX_WRITE_WORKAROUND as a substitute.
}

static index_t
etm_save_context_registers(cpu_index_t cpu, const context_register_info_t *info,
			   index_t context_register_index)
{
	register_t *context = etm_contexts[cpu];

	index_t cur_register_index = context_register_index;

	for (index_t i = 0; i < info->count; i++, cur_register_index++) {
		size_t reg_offset = info->reg_offset + i * info->stride;

		register_t *reg = context + cur_register_index;

		etm_get_reg(cpu, reg_offset, reg, info->access_size);
	}

	return cur_register_index;
}

void
etm_save_context_percpu(cpu_index_t cpu)
{
	// dsb isb
	__asm__ volatile("dsb ish; isb" ::: "memory");

	// Delay after taking the OS lock in the caller
	CTX_WRITE_WORKAROUND;

	// pull trcstatr.pmstable until it's stable
	// wait up to 100us
	ticks_t start = platform_timer_get_current_ticks();
	ticks_t timeout =
		start + platform_timer_convert_ns_to_ticks(100U * 1000U);
	do {
		ETM_TRCSTATR_t trcstatr =
			atomic_load_relaxed(&mapped_etms[cpu]->trcstatr);
		if (ETM_TRCSTATR_get_pmstable(&trcstatr)) {
			break;
		}

		if (platform_timer_get_current_ticks() > timeout) {
			TRACE_AND_LOG(ERROR, INFO,
				      "ETM: programmers model is not stable");
			break;
		}
	} while (1);

	etm_cprgctlr[cpu] = atomic_load_relaxed(&mapped_etms[cpu]->trcprgctlr);
	if ((etm_cprgctlr[cpu] & ETM_TRCPRGCTLR_ENABLE) != 0U) {
		// save all context registers
		index_t context_register_index = 0U;
		for (index_t i = 0; i < util_array_size(context_register_list);
		     i++) {
			const context_register_info_t *info =
				&context_register_list[i];

			context_register_index = etm_save_context_registers(
				cpu, info, context_register_index);
		}

		etm_claim_tag[cpu] =
			atomic_load_relaxed(&mapped_etms[cpu]->trcclaimclr);

		// poll until trcstatr.idle
		count_t idle_count = 100U;
		bool	idle	   = false;
		while (idle_count > 0U) {
			// should be a volatile read
			ETM_TRCSTATR_t trcstatr = atomic_load_relaxed(
				&mapped_etms[cpu]->trcstatr);
			idle = ETM_TRCSTATR_get_idle(&trcstatr);

			if (idle) {
				break;
			}

			idle_count--;
			platform_timer_ndelay(1000);
		}

		if ((!idle) && (idle_count == 0)) {
			LOG(ERROR, WARN,
			    "ETM: waiting idle timeout for context save");
		}
	}
	return;
}

static index_t
etm_restore_context_registers(cpu_index_t		     cpu,
			      const context_register_info_t *info,
			      index_t context_register_index)
{
	register_t *context = etm_contexts[cpu];

	index_t cur_register_index = context_register_index;

	for (index_t i = 0; i < info->count; i++, cur_register_index++) {
		size_t reg_offset = info->reg_offset + i * info->stride;

		register_t *reg = context + cur_register_index;

		etm_set_reg(cpu, reg_offset, *reg, info->access_size);
		CTX_WRITE_WORKAROUND;
	}

	return cur_register_index;
}

void
etm_restore_context_percpu(cpu_index_t cpu)
{
	if (((etm_cprgctlr[cpu]) & ETM_TRCPRGCTLR_ENABLE) != 0U) {
		// restore all context registers
		index_t context_register_index = 0U;
		for (index_t i = 0; i < util_array_size(context_register_list);
		     i++) {
			const context_register_info_t *info =
				&context_register_list[i];

			context_register_index = etm_restore_context_registers(
				cpu, info, context_register_index);
		}

		// set claim tag
		atomic_store_relaxed(&mapped_etms[cpu]->trcclaimset,
				     etm_claim_tag[cpu]);
		CTX_WRITE_WORKAROUND;

		atomic_store_relaxed(&mapped_etms[cpu]->trcprgctlr,
				     ETM_TRCPRGCTLR_ENABLE);
	}

	return;
}

void
etm_handle_power_cpu_online(void)
{
	if (compiler_unexpected(!platform_security_state_debug_disabled())) {
		cpu_index_t cpu = cpulocal_get_index();
		etm_unlock_percpu(cpu);
		etm_os_unlock_percpu(cpu);
	}
}

void
etm_handle_power_cpu_offline(void)
{
	(void)etm_handle_power_cpu_suspend(true);
}

error_t
etm_handle_power_cpu_suspend(bool may_poweroff)
{
	if (may_poweroff &&
	    compiler_unexpected(!platform_security_state_debug_disabled())) {
		cpu_index_t cpu = cpulocal_get_index();

		etm_unlock_percpu(cpu);
		etm_os_lock_percpu(cpu);

		etm_save_context_percpu(cpu);
	}

	return OK;
}

void
etm_unwind_power_cpu_suspend(bool may_poweroff)
{
	if (may_poweroff &&
	    compiler_unexpected(!platform_security_state_debug_disabled())) {
		cpu_index_t cpu = cpulocal_get_index();
		etm_os_unlock_percpu(cpu);

#if ETM_USE_SOFTWARE_LOCK
#error Restore software lock from before suspend (don't lock unconditionally)
#endif
	}
}

void
etm_handle_power_cpu_resume(bool was_poweroff)
{
	if (compiler_expected(platform_security_state_debug_disabled())) {
		goto out;
	}

	cpu_index_t cpu = cpulocal_get_index();

	if (was_poweroff) {
		etm_unlock_percpu(cpu);

		// check lock os los with trcoslsr.oslk == 1
		ETM_TRCOSLSR_t trcoslsr =
			atomic_load_relaxed(&mapped_etms[cpu]->trcoslsr);
		if (!ETM_TRCOSLSR_get_oslk(&trcoslsr)) {
			LOG(ERROR, WARN, "etm: os is not locked");
			etm_os_lock_percpu(cpu);
		}

		etm_restore_context_percpu(cpu);
	}

	etm_os_unlock_percpu(cpu);

#if ETM_USE_SOFTWARE_LOCK
#error Restore software lock from before suspend (don't lock unconditionally)
#endif

out:
	return;
}

```

`hyp/platform/gicv3/aarch64/gicv3-regs.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// GICv3 registers and bitfields definitions

define ICC_BPR_EL1 bitfield<64> {
	2:0		BinaryPoint	uint8;
	63:3		unknown=0;
};

define ICC_CTLR_EL1_IDbits enumeration {
	SIZE_16 = 0;
	SIZE_24 = 1;
};

define ICC_CTLR_EL1 bitfield<64> {
	0		CBPR		bool;
	1		EOImode		bool;
	5:2		unknown=0;
	6		PMHE		bool;
	7		unknown=0;
	10:8		PRIbits		type count_t;
	13:11		IDbits		enumeration ICC_CTLR_EL1_IDbits;
	14		SEIS		bool;
	15		A3V		bool;
	17:16		unknown=0;
	18		RSS		bool;
	19		ExtRange	bool;
	63:20		unknown=0;
};

define ICC_DIR_EL1 bitfield<64> {
	23:0		INTID		type irq_t;
	63:24		unknown=0;
};

define ICC_EOIR_EL1 bitfield<64> {
	23:0		INTID		type irq_t;
	63:24		unknown=0;
};

define ICC_HPPIR_EL1 bitfield<64> {
	23:0		INTID		type irq_t;
	63:24		unknown=0;
};

define ICC_IAR_EL1 bitfield<64> {
	23:0		INTID		type irq_t;
	63:24		unknown=0;
};

define ICC_IGRPEN_EL1 bitfield<64> {
	0		Enable		bool;
	63:1		unknown=0;
};

define ICC_PMR_EL1 bitfield<64> {
	7:0		Priority	uint8;
	63:8		unknown=0;
};

define ICC_SGIR_EL1 bitfield<64> {
	15:0		TargetList	uint16;
	23:16		Aff1		uint8;
	27:24		INTID		type irq_t;
	31:28		unknown=0;
	39:32		Aff2		uint8;
	40		IRM		bool;
	43:41		unknown=0;
	47:44		RS		uint8;
	55:48		Aff3		uint8;
	63:56		unknown=0;
};

define ICC_SRE_EL1 bitfield<64> {
	0		SRE		bool;
	1		DFB		bool;
	2		DIB		bool;
	63:3		unknown=0;
};

define ICC_SRE_EL2 bitfield<64> {
	0		SRE		bool;
	1		DFB		bool;
	2		DIB		bool;
	3		Enable		bool;
	63:4		unknown=0;
};

define ICH_HCR_EL2 bitfield<64> {
	0		En		bool;
	1		UIE		bool;
	2		LRENPIE		bool;
	3		NPIE		bool;
	4		VGrp0EIE	bool;
	5		VGrp0DIE	bool;
	6		VGrp1EIE	bool;
	7		VGrp1DIE	bool;
#if GICV3_HAS_VLPI_V4_1
	// Note: despite the name, this is negated (1 means not counted)
	8		vSGIEOICount	bool;
#else
	8		unknown=0;
#endif
	9		unknown=0;
	10		TC		bool;
	11		TALL0		bool;
	12		TALL1		bool;
	13		TSEI		bool;
	14		TDIR		bool;
	26:15		unknown=0;
	31:27		EOIcount	type count_t;
	63:32		unknown=0;
};

define ICH_LR_EL2_HW1 bitfield<64> {
	31:0		vINTID		type irq_t;
	44:32		pINTID		type irq_t;
	47:45		unknown=0;
	55:48		Priority	uint8;
	59:56		unknown=0;
	60		Group		bool;
	61		HW		bool(const) = 1;
	63:62		State		enumeration ICH_LR_EL2_State;
};

define ICH_LR_EL2_HW0 bitfield<64> {
	31:0		vINTID		type irq_t;
	40:32		unknown=0;
	41		EOI		bool;
	47:42		unknown=0;
	55:48		Priority	uint8;
	59:56		unknown=0;
	60		Group		bool;
	61		HW		bool(const) = 0;
	63:62		State		enumeration ICH_LR_EL2_State;
};

define ICH_LR_EL2_base bitfield<64> {
	31:0		vINTID		type irq_t;
	47:32		unknown=0;
	55:48		Priority	uint8;
	59:56		unknown=0;
	60		Group		bool;
	61		HW		bool;
	63:62		State		enumeration ICH_LR_EL2_State;
};

define ICH_LR_EL2_State enumeration(explicit) {
	INVALID = 0b00;
	PENDING = 0b01;
	ACTIVE = 0b10;
	PENDING_ACTIVE = 0b11;
};

define ICH_LR_EL2 union {
	hw		bitfield ICH_LR_EL2_HW1;
	sw		bitfield ICH_LR_EL2_HW0;
	base		bitfield ICH_LR_EL2_base;
};

define ICH_MISR_EL2 bitfield<64> {
	0		EOI		bool;
	1		U		bool;
	2		LRENP		bool;
	3		NP		bool;
	4		VGrp0E		bool;
	5		VGrp0D		bool;
	6		VGrp1E		bool;
	7		VGrp1D		bool;
	63:8		unknown=0;
};

define ICH_VMCR_EL2 bitfield<64> {
	0		VENG0		bool;
	1		VENG1		bool;
	2		VAckCtl		bool;
	3		VFIQEn		bool;
	4		VCBPR		bool;
	8:5		unknown=0;
	9		VEOIM		bool;
	17:10		unknown=0;
	20:18		VBPR1		uint8;
	23:21		VBPR0		uint8;
	31:24		VPMR		uint8;
	63:32		unknown=0;
};

define ICH_VTR_EL2 bitfield<64> {
	4:0		ListRegs	type count_t;
	18:5		unknown=0;
	19		TDS		bool;
	20		nV4		bool;
	21		A3V		bool;
	22		SEIS		bool;
	25:23		IDbits		enumeration ICC_CTLR_EL1_IDbits;
	28:26		PREbits		type count_t;
	31:29		PRIbits		type count_t;
	63:32		unknown=0;
};

```

`hyp/platform/gicv3/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

local_include
types gicv3.tc
arch_types aarch64 gicv3-regs.tc
configs IRQ_SPARSE_IDS=GICV3_HAS_LPI IRQ_HAS_MSI=GICV3_HAS_LPI
configs PLATFORM_MSI_CONTROLLER_COUNT=PLATFORM_GITS_COUNT
arch_configs aarch64 AARCH64_ICC_REGS=1
template simple gich_lrs.h.tmpl
events gicv3.ev
source gicv3.c

```

`hyp/platform/gicv3/gicv3.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

module gicv3

subscribe boot_cold_init

// Since other modules might be enabling their hardware interrupts in
// cpu_cold_init, we need to make sure GIC's handler for this event runs before
// the handlers of the other modules.
subscribe boot_cpu_cold_init
	priority 10

subscribe boot_cpu_warm_init
	require_preempt_disabled

subscribe power_cpu_suspend()
	unwinder gicv3_handle_power_cpu_resume()
	// This writes to the GIC which is slow, so run it late
	priority -10
	require_preempt_disabled

subscribe power_cpu_resume()
	require_preempt_disabled

#if defined(INTERFACE_VCPU) && INTERFACE_VCPU && GICV3_HAS_1N

subscribe vcpu_poweron(vcpu)
	require_scheduler_lock(vcpu)

subscribe vcpu_poweroff(current)
	require_scheduler_lock(current)

#endif // INTERFACE_VCPU && GICV3_HAS_1N

#if defined(GICV3_ENABLE_VPE) && GICV3_ENABLE_VPE
subscribe scheduler_affinity_changed(thread, prev_cpu, next_cpu)
	require_scheduler_lock(thread)

#if GICV3_HAS_VLPI_V4_1
subscribe object_create_thread

subscribe irq_received[HWIRQ_ACTION_GICV3_ITS_DOORBELL]
	handler gicv3_vpe_handle_irq_received_doorbell(hwirq)

#endif // GICV3_HAS_VLPI_V4_1
#endif // defined(GICV3_ENABLE_VPE) && GICV3_ENABLE_VPE

#if GICV3_HAS_ITS
subscribe abort_kernel()
#endif

subscribe boot_hypervisor_handover
	require_preempt_disabled

subscribe power_cpu_online()
	require_preempt_disabled

subscribe power_cpu_offline()
	require_preempt_disabled

```

`hyp/platform/gicv3/gicv3.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <util.h>
#include <types/bitmap.h>

#include "gicv3_config.h"

define irq_t newtype uint32;

#if GICV3_HAS_ITS
// We always allocate one fixed collection per physical CPU. They're
// apparently intended for group rerouting of IRQs but that is not terribly
// useful in practice.
// FIXME:
//define gic_its_ic_id_t newtype type cpu_index_t;
define gic_its_ic_id_t newtype uint16;

// Redistributor ID, in the form used by the ITS's commands. Depending on the
// ITS implementation this could be either the physical base address of the
// redistributor, or its processor number as used in legacy mode (which is
// typically but not necessarily the same as cpu_index_t).
define gic_its_rdbase_pta0_t newtype uint16;
define gic_its_rdbase_pta1 bitfield<64> {
	35:0	paddr	type paddr_t lsl(16);
};
define gic_its_rdbase union {
	// Used in command bitfields, as unions are not directly allowed
	raw	uregister;
	// Used if the GITS_TYPER.PTA bit is 1
	pta1	bitfield gic_its_rdbase_pta1;
	// Used if the GITS_TYPER.PTA bit is 0
	pta0	type gic_its_rdbase_pta0_t;
};

#if GICV3_HAS_VLPI
define gic_its_vpe_id_t newtype uint16;
#endif

#endif // GICV3_HAS_ITS

define GIC_SGI_BASE constant type irq_t = 0;
define GIC_SGI_NUM constant type count_t = 16;
define GIC_PPI_BASE constant type irq_t = GIC_SGI_BASE + GIC_SGI_NUM;
define GIC_PPI_NUM constant type count_t = 16;
define GIC_SPI_BASE constant type irq_t = GIC_PPI_BASE + GIC_PPI_NUM;
define GIC_SPI_NUM constant type count_t = 988;
define GIC_LPI_BASE constant type irq_t = 8192;
// There is no GIC_LPI_NUM constant because we are free to define it ourselves,
// up to the limit imposed by GICD_TYPER.IDbits (which may be very large)

define GIC_SPECIAL_INTIDS_BASE constant type irq_t =
	GIC_SPI_BASE + GIC_SPI_NUM;
define GIC_SPECIAL_INTIDS_NUM constant type count_t = 4;
define GIC_RES_BASE constant type irq_t =
	GIC_SPECIAL_INTIDS_BASE + GIC_SPECIAL_INTIDS_NUM;
define GIC_RES_NUM constant type count_t = 32;

define GIC_PPI_EXT_BASE constant type irq_t =
	GIC_RES_BASE + GIC_RES_NUM;
define GIC_PPI_EXT_NUM constant type count_t = 64;

define GIC_RES1_BASE constant type irq_t =
	GIC_PPI_EXT_BASE + GIC_PPI_EXT_NUM;
define GIC_RES1_NUM constant type count_t = 2976;

define GIC_SPI_EXT_BASE constant type irq_t =
	GIC_RES1_BASE + GIC_RES1_NUM;
define GIC_SPI_EXT_NUM constant type count_t = 1024;

// The highest possible priority for a secure (group 0) IRQ when performing a
// secure access to IPRIORITYR, or for a normal (group 1) IRQ when performing
// a non-secure access. Note that for non-secure accesses, all group 0 IRQs
// appear to be fixed at this priority, even though their real priority is
// higher.
define GIC_PRIORITY_HIGHEST constant uint8 = 0;

// The highest possible priority for a normal (group 1) IRQ when performing a
// secure access to IPRIORITYR, or for a virtual group 1 IRQ in ICH_LR<n>_EL2.
define GIC_PRIORITY_NORMAL constant uint8 = 0x80;

// The lowest possible priority for an IRQ, which prevents it ever being
// delivered. Note that some of the low bits may be cleared when reading back
// this value from IPRIORITYR; do not compare equality to this.
define GIC_PRIORITY_LOWEST constant uint8 = 0xff;

// GICv3 memory-mapped register interface layouts

#define GICD_ARRAY_SIZE(n) \
	(util_balign_up((GIC_SPI_BASE + GIC_SPI_NUM), n) / n)
#define GICD_ARRAY_SIZE_EXT(n) \
	(util_balign_up(GIC_SPI_EXT_NUM, n) / n)

define gicd structure(aligned(PGTABLE_HYP_PAGE_SIZE)) {
	ctlr @ 0x0000		union GICD_CTLR(atomic);
	typer @ 0x0004		bitfield GICD_TYPER(atomic);
	iidr @ 0x0008		bitfield GICD_IIDR(atomic);
	typer2 @ 0x000c		bitfield GICD_TYPER2(atomic);
	statusr @ 0x0010	bitfield GICD_STATUSR(atomic);
	setspi_nsr @ 0x0040	bitfield GICD_CLRSPI_SETSPI_NSR_SR(atomic);
	clrspi_nsr @ 0x0048	bitfield GICD_CLRSPI_SETSPI_NSR_SR(atomic);
	setspi_sr @ 0x0050	bitfield GICD_CLRSPI_SETSPI_NSR_SR(atomic);
	clrspi_sr @ 0x0058	bitfield GICD_CLRSPI_SETSPI_NSR_SR(atomic);
	igroupr @ 0x0080	array(GICD_ARRAY_SIZE(32)) uint32(atomic);
	isenabler @ 0x0100	array(GICD_ARRAY_SIZE(32)) uint32(atomic);
	icenabler @ 0x0180	array(GICD_ARRAY_SIZE(32)) uint32(atomic);
	ispendr @ 0x0200	array(GICD_ARRAY_SIZE(32)) uint32(atomic);
	icpendr @ 0x0280	array(GICD_ARRAY_SIZE(32)) uint32(atomic);
	isactiver @ 0x0300	array(GICD_ARRAY_SIZE(32)) uint32(atomic);
	icactiver @ 0x0380	array(GICD_ARRAY_SIZE(32)) uint32(atomic);
	ipriorityr @ 0x0400	array(GICD_ARRAY_SIZE(1)) uint8(atomic);
	itargetsr @ 0x0800	array(GICD_ARRAY_SIZE(1)) uint8(atomic);
	icfgr @ 0x0C00		array(GICD_ARRAY_SIZE(16)) uint32(atomic);
	igrpmodr @ 0x0D00	array(GICD_ARRAY_SIZE(32)) uint32(atomic);
	nsacr @ 0x0E00		array(GICD_ARRAY_SIZE(16)) uint32(atomic);
	sgir @ 0x0F00		bitfield GICD_SGIR(atomic);
	cpendsgir @ 0x0F10	array(GICD_ARRAY_SIZE_EXT(64)) uint8(atomic);
	spendsgir @ 0x0F20	array(GICD_ARRAY_SIZE_EXT(64)) uint8(atomic);
	igroupr_e @ 0x1000	array(GICD_ARRAY_SIZE_EXT(32)) uint32(atomic);
	isenabler_e @ 0x1200	array(GICD_ARRAY_SIZE_EXT(32)) uint32(atomic);
	icenabler_e @ 0x1400	array(GICD_ARRAY_SIZE_EXT(32)) uint32(atomic);
	ispendr_e @ 0x1600	array(GICD_ARRAY_SIZE_EXT(32)) uint32(atomic);
	icpendr_e @ 0x1800	array(GICD_ARRAY_SIZE_EXT(32)) uint32(atomic);
	isactiver_e @ 0x1A00	array(GICD_ARRAY_SIZE_EXT(32)) uint32(atomic);
	icactiver_e @ 0x1C00	array(GICD_ARRAY_SIZE_EXT(32)) uint32(atomic);
	ipriorityr_e @ 0x2000	array(GICD_ARRAY_SIZE_EXT(1)) uint8(atomic);
	icfgr_e @ 0x3000	array(GICD_ARRAY_SIZE_EXT(16)) uint32(atomic);
	igrpmodr_e @ 0x3400	array(GICD_ARRAY_SIZE_EXT(32)) uint32(atomic);
	nsacr_e @ 0x3600	array(GICD_ARRAY_SIZE_EXT(32)) uint32(atomic);
	irouter @ 0x6100	array(GIC_SPI_NUM) bitfield GICD_IROUTER(atomic);
	irouter_e @ 0x8000	array(GIC_SPI_EXT_NUM) bitfield GICD_IROUTER(atomic);
#if GICV3_HAS_GICD_ICLAR
	// GIC-600 / GIC-700 IRQ class registers to control 1-of-N routing
	iclar @ 0xe000		array(GICD_ARRAY_SIZE(16)) uint32(atomic);
	iclar_e @ 0xec00	array(GICD_ARRAY_SIZE_EXT(16)) uint32(atomic);
#endif
};

// Instead of adding PIDR2 to the gicd structure above, define a separate
// constant for its offset. This is to avoid making the struct unnecessarily
// bloated.
define OFS_GICD_PIDR2 constant = 0xFFE8;

define gicr_rd_base structure(aligned(PGTABLE_HYP_PAGE_SIZE)) {
	ctlr @ 0x0000		bitfield GICR_CTLR(atomic);
	iidr @ 0x0004		bitfield GICR_IIDR(atomic);
	typer @ 0x0008		bitfield GICR_TYPER(atomic);
	statusr @ 0x0010	bitfield GICR_STATUSR(atomic);
	waker @ 0x0014		bitfield GICR_WAKER(atomic);
	setlpir @ 0x0040	bitfield GICR_CLRLPIR_SETLPIR(atomic);
	clrlpir @ 0x0048	bitfield GICR_CLRLPIR_SETLPIR(atomic);
	propbaser @ 0x0070	bitfield GICR_PROPBASER(atomic);
	pendbaser @ 0x0078	bitfield GICR_PENDBASER(atomic);
	invlpir @ 0x00A0	bitfield GICR_INVLPIR(atomic);
	invallr @ 0x00B0	bitfield GICR_INVALLR(atomic);
	syncr @ 0x00C0		bitfield GICR_SYNCR(atomic);
};

#if GICV3_HAS_VLPI
define gicr_vlpi_base structure(aligned(PGTABLE_HYP_PAGE_SIZE)) {
	vpropbaser @ 0x0070	bitfield GICR_VPROPBASER(atomic);
	vpendbaser @ 0x0078	bitfield GICR_VPENDBASER(atomic);
#if GICV3_HAS_VLPI_V4_1
	vsgir @ 0x0080		bitfield GICR_VSGIR(atomic);
	vsgipendr @ 0x0088	bitfield GICR_VSGIPENDR(atomic);
#endif
};
#endif

define gicr_sgi_base structure(aligned(PGTABLE_HYP_PAGE_SIZE)) {
	reserved @ 0x0000	uint32(atomic);
	igroupr0 @ 0x0080	uint32(atomic);
	igroupr_e @ 0x0084	array(2) uint32(atomic);
	isenabler0 @ 0x0100	uint32(atomic);
	isenabler_e @ 0x0104	array(2) uint32(atomic);
	icenabler0 @ 0x0180	uint32(atomic);
	icenabler_e @ 0x0184	array(2) uint32(atomic);
	ispendr0 @ 0x0200	uint32(atomic);
	ispendr_e @ 0x0204	array(2) uint32(atomic);
	icpendr0 @ 0x0280	uint32(atomic);
	icpendr_e @ 0x0284	array(2) uint32(atomic);
	isactiver0 @ 0x0300	uint32(atomic);
	isactiver_e @ 0x0304	array(2) uint32(atomic);
	icactiver0 @ 0x0380	uint32(atomic);
	icactiver_e @ 0x0384	array(2) uint32(atomic);
	ipriorityr @ 0x0400	array(GIC_PPI_BASE + GIC_PPI_NUM) uint8(atomic);
	ipriorityr_e @ 0x0420	array(GIC_PPI_EXT_NUM) uint8(atomic);
	icfgr @ 0x0C00		array(2) uint32(atomic);
	icfgr_e @ 0x0C08	array(4) uint32(atomic);
	igrpmodr0 @ 0x0D00	uint32(atomic);
	igrpmodr_e @ 0x0D04	array(2) uint32(atomic);
	nsacr @ 0x0E00		uint32(atomic);
};

define gicr structure(aligned(65536)) {
	rd @ 0	structure gicr_rd_base;
	PIDR2 @ 0xFFE8		uint32(atomic);
	sgi @ 0x10000	structure gicr_sgi_base;
#if GICV3_HAS_VLPI
	vlpi @ 0x20000	structure gicr_vlpi_base;
#endif
};

define gicr_cpu structure {
	icc_sgi1r		bitfield ICC_SGIR_EL1;
	gicr			pointer structure gicr;
#if GICV3_HAS_LPI
	lpi_pending_bitmap	pointer type register_t;
	lpi_pendbase		bitfield GICR_PENDBASER;
#if defined(GICV3_ENABLE_VPE) && GICV3_ENABLE_VPE && GICV3_HAS_VLPI_V4_1
	vsgi_query_lock		structure spinlock;
#endif
#endif

	// False if the CPU is powering down
	// Protect this with the SPI lock when we enable, route or migrate
	// a SPI
	online			bool;
};

#if GICV3_HAS_LPI
define gic_lpi_prop bitfield<8> {
	0	enable		bool;
	1	res1		bool(const)=1;
	7:2	priority	uint8 lsl(2);
};
#endif

define GICR_PAGE_MASK	constant = ((1 << 16) - 1);

#if GICV3_HAS_VLPI
// GIC Redistributor stride (four 64k pages per core)
define GICR_STRIDE_SHIFT	constant uint8 = 16 + 2;
#else
// GIC Redistributor stride (two 64k pages per core)
define GICR_STRIDE_SHIFT	constant uint8 = 16 + 1;
#endif

define gicv3_irq_type enumeration {
	sgi;
	ppi;
	spi;
	special;
#if GICV3_EXT_IRQS
	ppi_ext;
	spi_ext;
#endif
#if GICV3_HAS_LPI
	lpi;
#endif
	reserved;
};

// Memory-mapped registers (common to AArch32 & AArch64)

define GICD_CLRSPI_SETSPI_NSR_SR bitfield<32> {
	12:0		INTID		type irq_t;
	31:13		unknown=0;
};

define GICD_CTLR_S bitfield<32> {
	0		EnableGrp0	bool;
	1		EnableGrp1NS	bool;
	2		EnableGrp1S	bool;
	3		unknown=0;
	4		ARE_S		bool;
	5		ARE_NS		bool;
	6		DS		bool(const) = 0;
	7		E1NWF		bool;
	30:8		unknown=0;
	31		RWP		bool;
};

define GICD_CTLR_NS bitfield<32> {
	0		EnableGrp1	bool;
	1		EnableGrp1A	bool;
	3:2		unknown=0;
	4		ARE_NS		bool;
	30:5		unknown=0;
	31		RWP		bool;
};

define GICD_CTLR_DS bitfield<32> {
	0		EnableGrp0	bool;
	1		EnableGrp1	bool;
	3:2		unknown=0;
	4		ARE		bool;
	5		unknown=0;
	6		DS		bool(const) = 1;
	7		E1NWF		bool;
	8		nASSGIreq	bool;
	30:9		unknown=0;
	31		RWP		bool;
};

define GICD_CTLR union {
	s		bitfield GICD_CTLR_S;
	ns		bitfield GICD_CTLR_NS;
	ds		bitfield GICD_CTLR_DS;
};

define GICD_IIDR bitfield<32> {
	11:0		Implementer	uint16;
	15:12		Revision	uint8;
	19:16		Variant		uint8;
	23:20		unknown=0;
	31:24		ProductID	uint8;
};

define GICD_IROUTER bitfield<64> {
	7:0		Aff0		uint8;
	15:8		Aff1		uint8;
	23:16		Aff2		uint8;
	30:24		unknown=0;
	31		IRM		bool;
	39:32		Aff3		uint8;
	63:40		unknown=0;
};

define GICD_SGIR bitfield<32> {
	3:0		INTID			type irq_t;
	14:4		unknown=0;
	15		NSATT			bool;
	23:16		CPUTargetList		uint8;
	25:24		TargetListFilter	uint8;
	31:26		unknown=0;
};

define GICD_STATUSR bitfield<32> {
	0		RRD		bool;
	1		WRD		bool;
	2		RWOD		bool;
	3		WROD		bool;
	31:4		unknown=0;
};

define GICD_TYPER bitfield<32> {
	4:0		ITLinesNumber	type count_t;
	7:5		CPUNumber	type count_t;
	8		ESPI		bool;
	9		unknown=0;
	10		SecurityExtn	bool;
	15:11		num_LPIs	type count_t;
	16		MBIS		bool;
	17		LPIS		bool;
	18		DVIS		bool;
	23:19		IDbits		type count_t;
	24		A3V		bool;
	25		No1N		bool;
	26		RSS		bool;
	31:27		ESPI_range	type count_t;
};

define GICD_TYPER2 bitfield<32> {
	4:0		VID		type count_t;
	6:5		unknown=0;
	7		VIL		bool;
	8		nASSGIcap	bool;
	31:9		unknown=0;
};

define GICR_CLRLPIR_SETLPIR bitfield<64> {
	31:0		pINTID		type irq_t;
	63:32		unknown=0;
};

define GICR_CTLR bitfield<32> {
	0		Enable_LPIs	bool;
	1		CES		bool;
	2		IR		bool;
	3		RWP		bool;
	23:4		unknown=0;
	24		DPG0		bool;
	25		DPG1NS		bool;
	26		DPG1S		bool;
	30:27		unknown=0;
	31		UWP		bool;
};

define GICR_IIDR bitfield<32> {
	11:0		Implementer	uint16;
	15:12		Revision	uint8;
	19:16		Variant		uint8;
	23:20		unknown=0;
	31:24		ProductID	uint8;
};

define GICR_INVALLR bitfield<64> {
	31:0		unknown=0;
#if GICV3_HAS_VLPI_V4_1
	47:32		vPEID		type gic_its_vpe_id_t;
	62:48		unknown=0;
	63		V		bool;
#else
	63:32		unknown=0;
#endif
};

define GICR_INVLPIR bitfield<64> {
	31:0		pINTID		type irq_t;
#if GICV3_HAS_VLPI_V4_1
	47:32		vPEID		type gic_its_vpe_id_t;
	62:48		unknown=0;
	63		V		bool;
#else
	63:32		unknown=0;
#endif
};

define GICR_PENDBASER bitfield<64> {
	6:0		unknown=0;
	9:7		InnerCache	uint8;
	11:10		Shareability	uint8;
	15:12		unknown=0;
	51:16		PA		uint64 lsl(16);
	55:52		unknown=0;
	58:56		OuterCache	uint8;
	61:59		unknown=0;
	62		PTZ		bool;
	63		unknown=0;
};

define GICR_PROPBASER bitfield<64> {
	4:0		IDbits		type count_t;
	6:5		unknown=0;
	9:7		InnerCache	uint8;
	11:10		Shareability	uint8;
	51:12		PA		uint64 lsl(12);
	55:52		unknown=0;
	58:56		OuterCache	uint8;
	63:59		unknown=0;
};

define GICR_STATUSR bitfield<32> {
	0		RRD		bool;
	1		WRD		bool;
	2		RWOD		bool;
	3		WROD		bool;
	31:4		unknown=0;
};

define GICR_SYNCR bitfield<32> {
	0		Busy		bool;
	31:1		unknown=0;
};

define GICR_TYPER_PPInum enumeration(explicit) {
	MAX_31 = 0;
	MAX_1087 = 1;
	MAX_1119 = 2;
};

define GICR_TYPER bitfield<64> {
	0		PLPIS		bool;
	1		VLPIS		bool;
	2		Dirty		bool;
	3		DirectLPI	bool;
	4		Last		bool;
	5		DPGS		bool;
	6		MPAM		bool;
	7		RVPEID		bool;
	23:8		Processor_Num	type index_t;
	25:24		CommonLPIAff	uint8;
	26		VSGI		bool;
	31:27		PPInum		enumeration GICR_TYPER_PPInum;
	39:32		Aff0		uint8;
	47:40		Aff1		uint8;
	55:48		Aff2		uint8;
	63:56		Aff3		uint8;
};

#if GICV3_HAS_VLPI_V4_1

define GICR_VPENDBASER bitfield<64> {
	15:0		vPEID		type gic_its_vpe_id_t;
	57:16		unknown=0;
	58		vGrp1En		bool;
	59		vGrp0En		bool;
	60		Dirty		bool;
	61		PendingLast	bool;
	62		Doorbell	bool;
	63		Valid		bool;
};

define GICR_VPROPBASER bitfield<64> {
	6:0	Size			type count_t;
	9:7	InnerCache		uint8;
	11:10	Shareability		uint8;
	51:12	Physical_Address	type paddr_t lsl(12);
	52	Z			bool;
	54:53	Page_Size		enumeration GITS_BASER_Page_Size;
	55	Indirect		bool;
	58:56	OuterCache		uint8;
	61:59	Entry_Size		size;
	62	unknown=0;
	63	Valid			bool;
};

define GICR_VSGIR bitfield<32> {
	15:0		vPEID		type gic_its_vpe_id_t;
	31:16		unknown=0;
};

define GICR_VSGIPENDR bitfield<32> {
	15:0		Pending		uint32;
	30:16		unknown=0;
	31		Busy		bool;
};

#else

define GICR_VPENDBASER bitfield<64> {
	6:0		unknown=0;
	9:7		InnerCache	uint8;
	11:10		Shareability	uint8;
	15:12		unknown=0;
	51:16		PA		type paddr_t lsl(16);
	55:52		unknown=0;
	58:56		OuterCache	uint8;
	59		unknown=0;
	60		Dirty		bool;
	61		PendingLast	bool;
	62		IDAI		bool;
	63		Valid		bool;
};

define GICR_VPROPBASER bitfield<64> {
	4:0		IDbits		uint8;
	6:5		unknown=0;
	9:7		InnerCache	uint8;
	11:10		Shareability	uint8;
	51:12		PA		type paddr_t lsl(12);
	55:52		unknown=0;
	58:56		OuterCache	uint8;
	63:59		unknown=0;
};

#endif

define GICR_WAKER bitfield<32> {
	0		IMPDEF		bool;
	1		ProcessorSleep	bool;
	2		ChildrenAsleep	bool;
	30:3		unknown=0;
	31		IMPDEF2		bool;
};

define GICC_BPR_ABPR bitfield<64> {
	2:0		BinaryPoint	uint8;
	63:3		unknown=0;
};

define GICC_EOIR_HPPIR_IAR_DIR bitfield<64> {
	23:0		INTID		type irq_t;
	63:24		unknown=0;
};

define GICC_CTLR_DS0 bitfield<64> {
	0		EnableGrp1	bool;
	4:1		unknown=0;
	5		FIQBypDisGrp1	bool;
	6		IRQBypDisGrp1	bool;
	8:7		unknown=0;
	9		EOImodeNS	bool;
	63:10		unknown=0;
};

define GICC_CTLR_DS1 bitfield<64> {
	0		EnableGrp0	bool;
	1		EnableGrp1	bool;
	2		unknown=0;
	3		FIQEn		bool;
	4		CBPR		bool;
	5		FIQBypDisGrp0	bool;
	6		IRQBypDisGrp0	bool;
	7		FIQBypDisGrp1	bool;
	8		IRQBypDisGrp1	bool;
	9		EOImode		bool;
	63:10		unknown=0;
};

define GICC_IIDR bitfield<64> {
	11:0		Implementer	uint16;
	15:12		Revision	uint8;
	19:16		ArchVer		uint8;
	31:20		ProductID	uint16;
	63:32		unknown=0;
};

define GICC_PMR_RPR bitfield<64> {
	7:0		Priority	uint8;
	63:8		unknown=0;
};

define GICC_STATUSR bitfield<64> {
	0		RRD		bool;
	1		WRD		bool;
	2		RWOD		bool;
	3		WROD		bool;
	4		ASV		bool;
	63:5		unknown=0;
};

define GICV_CTLR bitfield<64> {
	0		EnableGrp0	bool;
	1		EnableGrp1	bool;
	2		AckCtl		bool;
	3		FIQEn		bool;
	4		CBPR		bool;
	8:5		unknown=0;
	9		EOImode		bool;
	63:10		unknown=0;
};

define GICV_IIDR bitfield<64> {
	11:0		Implementer	uint16;
	15:12		Revision	uint8;
	19:16		ArchVer		uint8;
	31:20		ProductID	uint16;
	63:32		unknown=0;
};

#if GICV3_HAS_ITS

// ITS register maps

define gits_ctl_base structure(aligned(PGTABLE_HYP_PAGE_SIZE)) {
	ctlr @ 0x0		bitfield GITS_CTLR(atomic);
	iidr @ 0x4		bitfield GITS_IIDR(atomic);
	typer @ 0x8		bitfield GITS_TYPER(atomic);
	mpamidr @ 0x10		bitfield GITS_MPAMIDR(atomic);
	partidr @ 0x14		bitfield GITS_PARTIDR(atomic);
#if GICV3_HAS_VLPI_V4_1
	mpidr @ 0x18		bitfield GITS_MPIDR(atomic);
#endif
	cbaser @ 0x80		bitfield GITS_CBASER(atomic);
	cwriter @ 0x88		bitfield GITS_CWRITER(atomic);
	creadr @ 0x90		bitfield GITS_CREADR(atomic);
	baser @ 0x100		array(8) bitfield GITS_BASER(atomic);
};

define gits_xlate_base structure(aligned(PGTABLE_HYP_PAGE_SIZE)) {
	translater @ 0x40	bitfield GITS_TRANSLATER(atomic);
};

#if GICV3_HAS_VLPI_V4_1
define gits_vsgi_base structure(aligned(PGTABLE_HYP_PAGE_SIZE)) {
	sgir @ 0x20		bitfield GITS_SGIR(atomic);
};
#endif

define gits structure(aligned(65536)) {
	ctl @ 0			structure gits_ctl_base;
	PIDR2 @ 0xFFE8		uint32(atomic);
	xlate @ 0x10000		structure gits_xlate_base;
#if GICV3_HAS_VLPI_V4_1
	vsgi @ 0x20000		structure gits_vsgi_base;
#endif
};

#if GICV3_HAS_VLPI_V4_1
// GIC ITS stride (three 64k pages per ITS, rounded up to four)
define GITS_STRIDE_SHIFT	constant = 16 + 2;
#else
// GIC ITS stride (two 64k pages per ITS)
define GITS_STRIDE_SHIFT	constant = 16 + 1;
#endif

// ITS memory-mapped registers

define GITS_BASER_Page_Size enumeration(explicit) {
	SIZE_4KB = 0b00;
	SIZE_16KB = 0b01;
	SIZE_64KB = 0b10;
};

define GITS_BASER_Type enumeration(explicit) {
	Unimplemented = 0b000;
	Devices = 0b001;
#if GICV3_HAS_VLPI_V4_1
	vPEs = 0b010;
#endif
	Collections = 0b100;
};

define GITS_BASER bitfield<64> {
	7:0	Size			type count_t;
	9:8	Page_Size		enumeration GITS_BASER_Page_Size;
	11:10	Shareability		uint8;
	47:12	Physical_Address	type paddr_t lsl(12);
	52:48	Entry_Size		size;
	55:53	OuterCache		uint8;
	58:56	Type			enumeration GITS_BASER_Type;
	61:59	InnerCache		uint8;
	62	Indirect		bool;
	63	Valid			bool;
};

define GITS_BASER_Indirect_entry bitfield<64> {
	11:0	unknown=0;
	51:12	Physical_Address	type paddr_t lsl(12);
	62:52	unknown=0;
	63	Valid			bool;
};

define GITS_CBASER bitfield<64> {
	7:0	Size			type count_t;
	9:8	unknown=0;
	11:10	Shareability		uint8;
	// The manual includes bits 15:12 in the physical address, but also
	// says that the queue must be 64k-aligned.
	15:12	unknown=0;
	51:16	Physical_Address	type paddr_t lsl(16);
	52	unknown=0;
	55:53	OuterCache		uint8;
	58:56	unknown=0;
	61:59	InnerCache		uint8;
	62	unknown=0;
	63	Valid			bool;
};

define GITS_CREADR bitfield<64> {
	0	Stalled			bool;
	4:1	unknown=0;
	// This is defined as "Offset" in the spec, but if we don't shift it
	// left then it becomes an index into the array of command structures,
	// which is more convenient to work with.
	19:5	Index			type index_t;
	63:20	unknown=0;
};

define GITS_CTLR bitfield<32> {
	0	Enabled			bool;
	1	ImDe			bool;
	3:2	unknown=0;
	7:4	ITS_Number		type index_t;
	30:8	unknown=0;
	31	Quiescent		bool;
};

define GITS_CWRITER bitfield<64> {
	0	Retry			bool;
	4:1	unknown=0;
	// See comment in GITS_CREADR
	19:5	Index			type index_t;
	63:20	unknown=0;
};

define GITS_IIDR bitfield<32> {
	11:0		Implementer	uint16;
	15:12		Revision	uint8;
	19:16		Variant		uint8;
	23:20		unknown=0;
	31:24		ProductID	uint8;
};

define GITS_MPAMIDR bitfield<32> {
	15:0		PARTIDmax	uint16;
	23:16		PMGmax		uint8;
	31:24		unknown=0;
};

#if GICV3_HAS_VLPI_V4_1
define GITS_MPIDR bitfield<32> {
	7:0		unknown=0;
	15:8		Aff1		uint8;
	23:16		Aff2		uint8;
	31:24		Aff3		uint8;
};
#endif // GICV3_HAS_VLPI_V4_1

define GITS_PARTIDR bitfield<32> {
	15:0		PARTID		uint16;
	23:16		PMG		uint8;
	31:24		unknown=0;
};

#if GICV3_HAS_VLPI_V4_1
define GITS_SGIR bitfield<64> {
	3:0		vINTID		type virq_t;
	31:4		unknown=0;
	47:32		vPEID		type gic_its_vpe_id_t;
	63:48		unknown=0;
};
#endif // GICV3_HAS_VLPI_V4_1

define GITS_TRANSLATER bitfield<32> {
	31:0		event_id	type platform_msi_event_id_t;
};

define GITS_TYPER_SVPET enumeration(explicit) {
	NOT_SHARED = 0b00;
	SHARED_AFF3 = 0b01;
	SHARED_AFF2 = 0b10;
	SHARED_AFF1 = 0b11;
};

define GITS_TYPER bitfield<64> {
	0		Physical	bool;
	1		Virtual		bool;
	2		CCT		bool;
	3		unknown;	// IMP DEF
	7:4		ITT_entry_size	size;
	12:8		ID_bits		type count_t;
	17:13		Devbits		type count_t;
	18		SEIs		bool;
	19		PTA		bool;
	23:20		unknown=0;
	31:24		HCC		type count_t;
	35:32		CIDbits		type count_t;
	36		CIL		bool;
	37		VMOVP		bool;
	38		MPAM		bool;
	39		VSGI		bool;
	40		VMAPP		bool;
	42:41		SVPET		enumeration GITS_TYPER_SVPET;
	43		nID		bool;
	63:44		unknown=0;
};

// ITS commands

// Note: these are duplicated in the type definitions below.
// FIXME:
define gic_its_cmd_id enumeration {
	clear = 0x4;
	discard = 0xf;
	int = 0x3;
	inv = 0xc;
	invall = 0xd;
#if GICV3_HAS_VLPI_V4_1
	invdb = 0x2e;
#endif
	mapc = 0x9;
	mapd = 0x8;
	mapi = 0xb;
	mapti = 0xa;
	movall = 0xe;
	movi = 0x1;
	sync = 0x5;
#if GICV3_HAS_VLPI
	vinvall = 0x2d;
	vmapi = 0x2b;
	vmapp = 0x29;
	vmapti = 0x2a;
	vmovi = 0x21;
	vmovp = 0x22;
#if GICV3_HAS_VLPI_V4_1
	vsgi = 0x23;
#endif
#endif
};

define gic_its_cmd_base bitfield<256> {
	7:0	cmd		enumeration gic_its_cmd_id;
	255:8	unknown=0;
};

define gic_its_cmd_clear bitfield<256> {
	7:0	cmd		enumeration gic_its_cmd_id(const)=0x4;
	31:8	unknown=0;
	63:32	device_id	type platform_msi_device_id_t;
	95:64	event_id	type platform_msi_event_id_t;
	255:96	unknown=0;
};

define gic_its_cmd_discard bitfield<256> {
	7:0	cmd		enumeration gic_its_cmd_id(const)=0xf;
	31:8	unknown=0;
	63:32	device_id	type platform_msi_device_id_t;
	95:64	event_id	type platform_msi_event_id_t;
	255:96	unknown=0;
};

define gic_its_cmd_int bitfield<256> {
	7:0	cmd		enumeration gic_its_cmd_id(const)=0x3;
	31:8	unknown=0;
	63:32	device_id	type platform_msi_device_id_t;
	95:64	event_id	type platform_msi_event_id_t;
	255:96	unknown=0;
};

define gic_its_cmd_inv bitfield<256> {
	7:0	cmd		enumeration gic_its_cmd_id(const)=0xc;
	31:8	unknown=0;
	63:32	device_id	type platform_msi_device_id_t;
	95:64	event_id	type platform_msi_event_id_t;
	255:96	unknown=0;
};

define gic_its_cmd_invall bitfield<256> {
	7:0	cmd		enumeration gic_its_cmd_id(const)=0xd;
	127:8	unknown=0;
	143:128	icid		type gic_its_ic_id_t;
	255:144	unknown=0;
};

#if GICV3_HAS_VLPI_V4_1

define gic_its_cmd_invdb bitfield<256> {
	7:0	cmd		enumeration gic_its_cmd_id(const)=0x2e;
	95:8	unknown=0;
	111:96	vpe_id		type gic_its_vpe_id_t;
	255:112	unknown=0;
};

#endif // GICV3_HAS_VLPI_V4_1

define gic_its_cmd_mapc bitfield<256> {
	7:0	cmd		enumeration gic_its_cmd_id(const)=0x9;
	127:8	unknown=0;
	143:128	icid		type gic_its_ic_id_t;
	179:144	rdbase		uregister; // union gic_its_rdbase;
	190:180 unknown=0;
	191	valid		bool;
	255:192	unknown=0;
};

define gic_its_cmd_mapd bitfield<256> {
	7:0	cmd		enumeration gic_its_cmd_id(const)=0x8;
	31:8	unknown=0;
	63:32	device_id	type platform_msi_device_id_t;
	68:64	size		type count_t;
	135:69	unknown=0;
	179:136	itt_addr	type paddr_t lsl(8);
	190:180 unknown=0;
	191	valid		bool;
	255:192 unknown=0;
};

define gic_its_cmd_mapi bitfield<256> {
	7:0	cmd		enumeration gic_its_cmd_id(const)=0xb;
	31:8	unknown=0;
	63:32	device_id	type platform_msi_device_id_t;
	95:64	event_id	type platform_msi_event_id_t;
	127:96	unknown=0;
	143:128	icid		type gic_its_ic_id_t;
	255:144	unknown=0;
};

define gic_its_cmd_mapti bitfield<256> {
	7:0	cmd		enumeration gic_its_cmd_id(const)=0xa;
	31:8	unknown=0;
	63:32	device_id	type platform_msi_device_id_t;
	95:64	event_id	type platform_msi_event_id_t;
	127:96	lpi		type irq_t;
	143:128	icid		type gic_its_ic_id_t;
	255:144	unknown=0;
};

define gic_its_cmd_movall bitfield<256> {
	7:0	cmd		enumeration gic_its_cmd_id(const)=0xe;
	143:8	unknown=0;
	179:144	rdbase1		uregister; // union gic_its_rdbase;
	207:180	unknown=0;
	243:208	rdbase2		uregister; // union gic_its_rdbase;
	255:244 unknown=0;
};

define gic_its_cmd_movi bitfield<256> {
	7:0	cmd		enumeration gic_its_cmd_id(const)=0x1;
	31:8	unknown=0;
	63:32	device_id	type platform_msi_device_id_t;
	95:64	event_id	type platform_msi_event_id_t;
	127:96	unknown=0;
	143:128	icid		type gic_its_ic_id_t;
	255:144	unknown=0;
};

define gic_its_cmd_sync bitfield<256> {
	7:0	cmd		enumeration gic_its_cmd_id(const)=0x5;
	143:8	unknown=0;
	179:144	rdbase		uregister; // union gic_its_rdbase;
	255:180 unknown=0;
};

#if GICV3_HAS_VLPI

define gic_its_cmd_vinvall bitfield<256> {
	7:0	cmd		enumeration gic_its_cmd_id(const)=0x2d;
	95:8	unknown=0;
	111:96	vpe_id		type gic_its_vpe_id_t;
	255:112	unknown=0;
};

define gic_its_cmd_vmapi bitfield<256> {
	7:0	cmd		enumeration gic_its_cmd_id(const)=0x2b;
	31:8	unknown=0;
	63:32	device_id	type platform_msi_device_id_t;
	95:64	event_id	type platform_msi_event_id_t;
	111:96	vpe_id		type gic_its_vpe_id_t;
	159:112 unknown=0;
	191:160	db_lpi		type irq_t;
	255:192 unknown=0;
};

#if !GICV3_HAS_VLPI_V4_1

define gic_its_cmd_vmapp bitfield<256> {
	7:0	cmd		enumeration gic_its_cmd_id(const)=0x29;
	95:8	unknown=0;
	111:96	vpe_id		type gic_its_vpe_id_t;
	143:112	unknown=0;
	179:144	rdbase		uregister; // union gic_its_rdbase;
	190:180 unknown=0;
	191	valid		bool;
	// Note: the VPT_size field in the GICv4.1 VMAPP command is _not_
	// offset by 1, so we deviate from the naming in the spec to ensure
	// that this field is used correctly.
	196:192	vpt_size_minus_one	type count_t;
	207:197	unknown=0;
	243:208	vpt_addr	type paddr_t lsl(16);
	255:244 unknown=0;
};

#else // GICV3_HAS_VLPI_V4_1

define gic_its_cmd_vmapp bitfield<256> {
	7:0	cmd		enumeration gic_its_cmd_id(const)=0x29;
	8	alloc		bool;
	9	ptz		bool;
	15:10	unknown=0;
	51:16	vconf_addr	type paddr_t lsl(16);
	63:52	unknown=0;
	95:64	db_lpi		type irq_t;
	111:96	vpe_id		type gic_its_vpe_id_t;
	143:112	unknown=0;
	179:144	rdbase		uregister; // union gic_its_rdbase;
	190:180 unknown=0;
	191	valid		bool;
	199:192	vpt_size	type count_t;
	207:200	unknown=0;
	243:208	vpt_addr	type paddr_t lsl(16);
	255:244 unknown=0;
};

#endif // GICV3_HAS_VLPI_V4_1

define gic_its_cmd_vmapti bitfield<256> {
	7:0	cmd		enumeration gic_its_cmd_id(const)=0x2a;
	31:8	unknown=0;
	63:32	device_id	type platform_msi_device_id_t;
	95:64	event_id	type platform_msi_event_id_t;
	111:96	vpe_id		type gic_its_vpe_id_t;
	127:112	unknown=0;
	159:128 vlpi		type virq_t;
	191:160	db_lpi		type irq_t;
	255:192 unknown=0;
};

define gic_its_cmd_vmovi bitfield<256> {
	7:0	cmd		enumeration gic_its_cmd_id(const)=0x21;
	31:8	unknown=0;
	63:32	device_id	type platform_msi_device_id_t;
	95:64	event_id	type platform_msi_event_id_t;
	111:96	vpe_id		type gic_its_vpe_id_t;
	127:112	unknown=0;
	128	db		bool;
	159:129	unknown=0;
	191:160	db_lpi		type irq_t;
	255:192 unknown=0;
};

#if !GICV3_HAS_VLPI_V4_1

define gic_its_cmd_vmovp bitfield<256> {
	7:0	cmd		enumeration gic_its_cmd_id(const)=0x22;
	31:8	unknown=0;
	47:32	seqnum		uint16;
	63:48	unknown=0;
	79:64	itslist		uint16;
	95:80	unknown=0;
	111:96	vpe_id		type gic_its_vpe_id_t;
	143:112	unknown=0;
	179:144	rdbase		uregister; // union gic_its_rdbase;
	255:180 unknown=0;
};

#else // GICV3_HAS_VLPI_V4_1

define gic_its_cmd_vmovp bitfield<256> {
	7:0	cmd		enumeration gic_its_cmd_id(const)=0x22;
	31:8	unknown=0;
	47:32	seqnum		uint16;
	63:48	unknown=0;
	79:64	itslist		uint16;
	95:80	unknown=0;
	111:96	vpe_id		type gic_its_vpe_id_t;
	143:112	unknown=0;
	179:144	rdbase		uregister; // union gic_its_rdbase;
	190:180 unknown=0;
	191	db		bool;
	223:192	db_lpi		type irq_t;
	255:224 unknown=0;
};

define gic_its_cmd_vsgi bitfield<256> {
	7:0	cmd		enumeration gic_its_cmd_id(const)=0x23;
	8	enable		bool;
	9	clear		bool;
	10	group1		bool;
	19:11	unknown=0;
	23:20	priority	uint8 lsl(4);
	31:24	unknown=0;
	35:32	sgi		type irq_t;
	95:36	unknown=0;
	111:96	vpe_id		type gic_its_vpe_id_t;
	255:112	unknown=0;
};

#endif // GICV3_HAS_VLPI_V4_1

define gic_its_cmd_vsync bitfield<256> {
	7:0	cmd		enumeration gic_its_cmd_id(const)=0x25;
	95:8	unknown=0;
	111:96	vpe_id		type gic_its_vpe_id_t;
	255:112	unknown=0;
};

#endif // GICV3_HAS_VLPI

define gic_its_cmd union(aligned(32)) {
	clear		bitfield gic_its_cmd_clear;
	discard		bitfield gic_its_cmd_discard;
	// Extra underscore to avoid keyword conflict with C's int type
	int_		bitfield gic_its_cmd_int;
	inv		bitfield gic_its_cmd_inv;
	invall		bitfield gic_its_cmd_invall;
#if GICV3_HAS_VLPI_V4_1
	invdb		bitfield gic_its_cmd_invdb;
#endif
	mapc		bitfield gic_its_cmd_mapc;
	mapd		bitfield gic_its_cmd_mapd;
	mapi		bitfield gic_its_cmd_mapi;
	mapti		bitfield gic_its_cmd_mapti;
	movall		bitfield gic_its_cmd_movall;
	movi		bitfield gic_its_cmd_movi;
	sync		bitfield gic_its_cmd_sync;
#if GICV3_HAS_VLPI
	vinvall		bitfield gic_its_cmd_vinvall;
	vmapi		bitfield gic_its_cmd_vmapi;
	vmapp		bitfield gic_its_cmd_vmapp;
	vmapti		bitfield gic_its_cmd_vmapti;
	vmovi		bitfield gic_its_cmd_vmovi;
	vmovp		bitfield gic_its_cmd_vmovp;
#if GICV3_HAS_VLPI_V4_1
	vsgi		bitfield gic_its_cmd_vsgi;
#endif
	vsync		bitfield gic_its_cmd_vsync;
#endif // GICV3_HAS_VLPI

	// Used to read the command ID when parsing commands
	base		bitfield gic_its_cmd_base;
};

// Per-ITS driver state structure
define gicv3_its_driver_state structure {
	// Virtual address of the memory-mapped registers
	regs		pointer structure gits;

	// RDbase values used for ITS commands. Depending on the ITS these
	// might either be physical pointers to the redistributors, or the GIC's
	// CPU indices.
	rdbases		array(PLATFORM_MAX_CORES) union gic_its_rdbase;

	// GITS_BASER values used to rewrite the bases on resume.
	saved_basers	array(8) bitfield GITS_BASER;

	// Level 1 tables for indirect bases. NULL if indirection is not used.
	indirect_device_table	structure gicv3_its_indirect_table;

	// Virtual and physical pointers to this ITS's command queue
	cmd_queue	pointer array(GICV3_ITS_QUEUE_LEN) union gic_its_cmd;
	cmd_queue_phys	type paddr_t;
	// Next sequence number to be allocated in the command queue
	cmd_queue_head	type count_t;
	// Next sequence number to be processed by the ITS
	cmd_queue_cached_tail	type count_t;
	// Lock protecting the command queue
	cmd_queue_lock	structure spinlock;
	// True if commands need to be cache-flushed before submission
	cmd_queue_flush	bool;

	// First unused device ID
	first_unused	type platform_msi_device_id_t;
	// True if the first unused device ID has already been reserved
	first_unused_reserved	bool;
};

// Driver state for an indirect ITS table; used to ensure that second level
// tables have been allocated before submitting ITS commands that use them.
define gicv3_its_indirect_table structure {
	// Hypervisor virtual address of the first-level table.
	first_level	pointer bitfield GITS_BASER_Indirect_entry(atomic);

	// Page / L2 table size (which the hardware might fix to >4K)
	page_size	size;

	// Number of entries in the first and second level tables.
	entries_l1	type count_t;
	entries_l2	type count_t;
};

#if GICV3_HAS_VLPI

// Per-VCPU state
extend thread object module gicv3_its {
	// Globally unique VPE ID, allocated by the ITS driver
	vpe_id		type gic_its_vpe_id_t;

	// Physical CPU to which this VPE was last mapped in the ITS
	mapped_cpu	type cpu_index_t;

#if GICV3_HAS_VLPI_V4_1
	// Doorbell LPI, allocated by the ITS driver
	doorbell	pointer object hwirq;

	// True if the GICR may not have finished reading the pending tables
	// yet after waking this VCPU with a doorbell, so there may be a
	// pending vSGI or vLPI that is not yet delivered.
	//
	// In this case it is not safe to allow the VCPU to enter a low-power
	// state, because doing so might trap the VCPU in a loop where a
	// pending vSGI or vLPI repeatedly wakes it but it never stays awake
	// long enough to actually receive the interrupt.
	need_wakeup_check	bool;
#else // !GICV3_HAS_VLPI_V4_1
	// LPI configuration and pending table physical addresses, provided by
	// the VGITS. For GICv4.1 these are passed to the hardware with VMAPP
	// and don't need to be stored in software (though the virtual GICR
	// will store the corresponding virtual addresses).
	config_table	type paddr_t;
	pending_table	type paddr_t;
#endif // !GICV3_HAS_VLPI_V4_1
};

extend hwirq_action enumeration {
	gicv3_its_doorbell;
};

extend hwirq object module gicv3_its {
	// RCU-protected pointer to the VCPU woken by this doorbell
	vcpu		pointer(atomic) object thread;
};

#endif // GICV3_HAS_VLPI

#endif // GICV3_HAS_ITS

```

`hyp/platform/gicv3/include/gicv3.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// IRQ functions

count_t
gicv3_irq_max(void);

gicv3_irq_type_t
gicv3_get_irq_type(irq_t irq);

bool
gicv3_irq_is_percpu(irq_t irq);

error_t
gicv3_irq_check(irq_t irq);

void
gicv3_irq_enable_shared(irq_t irq);

void
gicv3_irq_enable_local(irq_t irq) REQUIRE_PREEMPT_DISABLED;

void
gicv3_irq_enable_percpu(irq_t irq, cpu_index_t cpu);

void
gicv3_irq_disable_shared(irq_t irq);

void
gicv3_irq_disable_local(irq_t irq) REQUIRE_PREEMPT_DISABLED;

void
gicv3_irq_disable_local_nowait(irq_t irq) REQUIRE_PREEMPT_DISABLED;

void
gicv3_irq_disable_percpu(irq_t irq, cpu_index_t cpu);

void
gicv3_irq_cancel_nowait(irq_t irq);

irq_trigger_result_t
gicv3_irq_set_trigger_shared(irq_t irq, irq_trigger_t trigger);

irq_trigger_result_t
gicv3_irq_set_trigger_percpu(irq_t irq, irq_trigger_t trigger, cpu_index_t cpu);

error_t
gicv3_spi_set_route(irq_t irq, GICD_IROUTER_t route);

#if GICV3_HAS_GICD_ICLAR
error_t
gicv3_spi_set_classes(irq_t irq, bool class0, bool class1);
#endif

irq_result_t
gicv3_irq_acknowledge(void) REQUIRE_PREEMPT_DISABLED;

void
gicv3_irq_priority_drop(irq_t irq);

void
gicv3_irq_deactivate(irq_t irq);

void
gicv3_irq_deactivate_percpu(irq_t irq, cpu_index_t cpu);

// IPI specific functions

void
gicv3_ipi_others(ipi_reason_t ipi);

void
gicv3_ipi_one(ipi_reason_t ipi, cpu_index_t cpu);

void
gicv3_ipi_clear(ipi_reason_t ipi);

// Accessor to gicd for the unit tests configuration.
#if defined(UNIT_TESTS)
gicd_t *
gicv3_get_gicd_pointer(void);

#if GICV3_EXT_IRQS
irq_trigger_result_t
gicv3_irq_set_trigger_shared(irq_t irq, irq_trigger_t trigger);
#endif
#endif

#if GICV3_HAS_LPI

// LPI configuration cache invalidation.
//
// If the virtual GICR internally caches VLPI configuration (rather than mapping
// a guest-accessible address to the ITS directly), it must have already updated
// the cache before calling any of these functions.
//
// There are five variants: LPI by (device, event) pair, LPI by IRQ number, VLPI
// by VIRQ number, all LPIs by physical CPU ID, or all VLPIs by VCPU.
//
// The first variant queues an INV command on the relevant ITS. It is only
// implemented if there is at least one ITS, and is therefore declared in
// gicv3_its.h rather than here.
//
// The second and third variants are only implemented on GICv4.1, or (for the
// second variant) on GICv3 with no ITS. On GICv4.0 or GICv3 with an ITS, the
// caller must instead either find or synthesise a (device, event) pair that is
// mapped to the given LPI or VLPI, and then call the first variant.
//
// The fourth variant uses the GICR if possible (GICv4.1 or GICv3 with no ITS)
// and queues an INVALL command on the ITS otherwise.
//
// The fifth variant is only available on GICv4.1; the caller must otherwise
// scan the virtual IC and call the first variant for every (device, event) pair
// mapped to it.
//
// These operations are not guaranteed to complete immediately. The first
// variant returns a sequence number which can be used to poll or wait using the
// functions above. The remaining variants have corresponding functions to poll
// completion of all preceding calls for a specified PCPU or VCPU; note that
// they may spuriously show non-completion because all VCPUs affine to a PCPU
// share the completion state of that PCPU.
#if !GICV3_HAS_ITS || GICV3_HAS_VLPI_V4_1
void
gicv3_lpi_inv_by_id(cpu_index_t cpu, irq_t lpi);
#endif

#if GICV3_HAS_VLPI_V4_1
void
gicv3_vlpi_inv_by_id(thread_t *vcpu, virq_t vlpi);
#endif

void
gicv3_lpi_inv_all(cpu_index_t cpu);

#if GICV3_HAS_VLPI_V4_1
void
gicv3_vlpi_inv_all(thread_t *vcpu);
#endif

bool
gicv3_lpi_inv_pending(cpu_index_t cpu);

#if GICV3_HAS_VLPI_V4_1
bool
gicv3_vlpi_inv_pending(thread_t *vcpu);
#endif

#if defined(GICV3_ENABLE_VPE) && GICV3_ENABLE_VPE

// Virtual PE scheduling.
//
// These functions must be called to inform the GICR when the current VCPU
// has been mapped to a vPE ID with gicv3_its_vpe_map() and is not currently
// blocked in EL2 or EL3 nor set to sleep in its virtual GICR_WAKER.
//
// Points at which these functions must be called include context switching,
// entering or leaving the WFI fastpath, entering or leaving an interruptible
// call to EL3, or changing GICR_WAKER.ProcessorSleep or GICR_CTLR.EnableLPIs
// on the current VCPU.
//
// The _schedule function takes boolean arguments indicating whether direct vSGI
// delivery and the default doorbell should be enabled for each of the two
// interrupt groups. If these values must be changed for the running VCPU, e.g.
// due to a GICD_CTLR write, the VCPU must be descheduled and then scheduled
// with the new values. Note that these values have no effect on LPIs with
// individual doorbells, and therefore do nothing for GICv4.0.
//
// This function must call gicv3_vpe_sync_deschedule() to wait for the most
// recent deschedule to complete, so it should be called as late as possible.
void
gicv3_vpe_schedule(bool enable_group0, bool enable_group1)
	REQUIRE_PREEMPT_DISABLED;

// The _deschedule function takes a boolean argument indicating whether the
// previously scheduled VCPU is waiting for interrupts, and therefore requires a
// doorbell IRQ to wake it. It returns a boolean value which is true if a
// doorbell was requested but at least one VLPI or VSGI was already pending, in
// which case the VCPU should to be woken and rescheduled immediately.
//
// This function may not take effect immediately, as the GICR may take some
// time to scan its pending VLPI tables and synchronise with the ITSs to fully
// deschedule the vPE, and this function only waits for that synchronisation to
// complete if enable_doorbell is true. Subsequent calls to _schedule must call
// gicv3_vpe_sync_deschedule() to wait until it has taken effect. Therefore this
// function should be called as early as possible once it is known that a VCPU
// must be descheduled.
bool
gicv3_vpe_deschedule(bool enable_doorbell) REQUIRE_PREEMPT_DISABLED;

// Check whether a VCPU can safely block waiting for interrupts.
//
// Returns true if the current VCPU was previously woken by a pending vLPI or
// vSGI, a gicv3_vpe_schedule() call has been made for the current VCPU, and the
// GICR is not yet known to have finished scheduling the VCPU.
//
// This is used to prevent the VCPU entering a loop where it is woken by a
// doorbell or the PendingLast bit due to a pending vLPI or vSGI, but then
// blocks agoin before the GICR delivers the interrupt.
//
// The VGIC must ensure that this is called at some point during any VCPU idle
// loop or suspend / resume path such that the VCPU does not block while it
// returns true, and will observe the pending interrupt after it returns false.
//
// If the retry_trap argument is true, the result will indicate the state of the
// GICR before this function was called (i.e. when the trap that triggered it
// occurred). Otherwise, it will indicate the state of the GICR after the
// function was called.
bool
gicv3_vpe_check_wakeup(bool retry_trap);

// Poll until any pending vPE deschedule is complete on the specified CPU.
//
// If the maybe_scheduled boolean is false, this function asserts that there
// is no currently scheduled vPE. If it is true, the function has no effect if
// there is a currently scheduled vPE. This is called by gicv3_vpe_schedule(),
// but may also be called elsewhere when it is necessary to guarantee that the
// GICR has completely descheduled a VCPU.
void
gicv3_vpe_sync_deschedule(cpu_index_t cpu, bool maybe_scheduled)
	REQUIRE_PREEMPT_DISABLED;

#if GICV3_HAS_VLPI_V4_1
// Ask the GICR for a specific VCPU's pending vSGI state.
uint32_result_t
gicv3_vpe_vsgi_query(thread_t *vcpu);
#endif

#endif // GICV3_ENABLE_VPE

#endif // GICV3_HAS_LPI

```

`hyp/platform/gicv3/include/gicv3_config.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// Driver configuration constants for the GICv3 driver.

// Default priority for all interrupts.
#define GIC_PRIORITY_DEFAULT 0xA0U

#if GICV3_HAS_ITS

// Command queue length
#define GICV3_ITS_QUEUE_LEN 128U

#if GICV3_HAS_VLPI

// Default vPE ID range. We don't support sharing these, so this limits the
// number of VCPUs that may be attached to at least one VGITS.
#if !defined(GICV3_ITS_VPES)
#define GICV3_ITS_VPES 64U
#endif

#endif // GICV3_HAS_ITS

#endif // GICV3_HAS_ITS

```

`hyp/platform/gicv3/src/gicv3.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>
#include <string.h>

#include <hypconstants.h>
#include <hypregisters.h>

#include <atomic.h>
#include <bitmap.h>
#include <compiler.h>
#include <cpulocal.h>
#include <hyp_aspace.h>
#include <log.h>
#include <panic.h>
#include <partition.h>
#include <pgtable.h>
#include <platform_cpu.h>
#include <platform_ipi.h>
#include <platform_irq.h>
#include <preempt.h>
#include <scheduler.h>
#include <spinlock.h>
#include <thread.h>
#include <trace.h>
#include <util.h>
#if defined(GICV3_ENABLE_VPE) && GICV3_ENABLE_VPE
#include <vcpu.h>
#endif

#include <events/platform.h>

#include <asm/barrier.h>

#include "event_handlers.h"
#include "gicv3.h"
#include "gicv3_config.h"

#if defined(VERBOSE) && VERBOSE
#define GICV3_DEBUG 1
#else
#define GICV3_DEBUG 0
#endif

static_assert(!GICV3_HAS_ITS || GICV3_HAS_LPI,
	      "An ITS cannot be present without LPI support");
#if defined(GICV3_ENABLE_VPE)
static_assert(!GICV3_ENABLE_VPE || GICV3_HAS_VLPI,
	      "VPE support cannot be enabled unless VLPIs are implemented");
#endif
static_assert(!GICV3_HAS_VLPI || GICV3_HAS_ITS,
	      "VLPIs (GICv4) cannot be implemented without an ITS");
static_assert(!GICV3_HAS_VLPI_V4_1 || GICV3_HAS_VLPI,
	      "VPEs (GICv4.1) cannot be implemented without VLPIs (GICv4.0)");
static_assert(!GICV3_HAS_LPI || !GICV3_HAS_ITS || GICV3_HAS_VLPI_V4_1,
	      "LPIs only supported if ITS is absent or GICv4.1 is implemented");

#define GICD_ENABLE_GET_N(x) ((x) >> 5)
#define GIC_ENABLE_BIT(x)    (uint32_t)(util_bit((x)&31UL))

static gicd_t *gicd;
static gicr_t *mapped_gicrs[PLATFORM_GICR_COUNT];
#if GICV3_HAS_ITS
static gits_t *mapped_gitss[PLATFORM_GITS_COUNT];
#endif

// It is sometimes desirable to be able to inspect the state of GIC in the
// debugger from the secure world point of view. Expose the physical addresses
// of GICD, GICR and GITS so the debuggers can find them.
extern paddr_t platform_gicd_base;
paddr_t	       platform_gicd_base = PLATFORM_GIC_BASE;
extern paddr_t platform_gicrs_bases[PLATFORM_GICR_COUNT];
paddr_t	       platform_gicrs_bases[PLATFORM_GICR_COUNT];
#if GICV3_HAS_ITS
extern paddr_t platform_gits_base;
paddr_t	       platform_gits_base = PLATFORM_GITS_BASE;
#endif

// Several of the IRQ configuration registers can only be updated using writes
// that affect more than one IRQ at a time, notably GICD_ICFGR and GICD_ICLAR.
// This lock is used to make the read-modify-write sequences atomic.
static spinlock_t bitmap_update_lock;

// Guard between SPI set route and CPU poweroff SPI migration
static spinlock_t spi_route_lock;

#if GICV3_HAS_LPI
#if GICV3_HAS_VLPI_V4_1
// Currently we only need one LPI per VCPU with a VGIC attachment, to be used
// as a GICv4.1 default scheduling doorbell. We therefore allocate the minimum
// nonzero number of LPIs.
#define GIC_LPI_NUM 8192U
#else
#error define GIC_LPI_NUM
#endif
static_assert(util_is_p2(GIC_LPI_BASE + GIC_LPI_NUM) && (GIC_LPI_NUM > 0),
	      "Hard-coded max LPI count must be 8192 less than a power of two");

#define GIC_LPI_PROP_ALIGNMENT ((size_t)util_bit(GICR_PROPBASER_PA_SHIFT))
static gic_lpi_prop_t alignas(GIC_LPI_PROP_ALIGNMENT)
	gic_lpi_prop_table[GIC_LPI_NUM];

static GICR_PROPBASER_t gic_lpi_propbase;
#endif // GICV3_HAS_LPI

CPULOCAL_DECLARE_STATIC(gicr_cpu_t, gicr_cpu);

static GICD_CTLR_NS_t
gicd_wait_for_write(void)
{
	// Order the write we're waiting for before the loads in the poll
	atomic_device_fence(memory_order_seq_cst);

	GICD_CTLR_t ctlr = atomic_load_relaxed(&gicd->ctlr);

	while (GICD_CTLR_NS_get_RWP(&ctlr.ns) != 0U) {
		asm_yield();
		ctlr = atomic_load_relaxed(&gicd->ctlr);
	}

	// Order the successful load in the poll before anything afterwards
	atomic_device_fence(memory_order_acquire);

	return ctlr.ns;
}

static void
gicr_wait_for_write(gicr_t *gicr)
{
	// Order the write we're waiting for before the loads in the poll
	atomic_device_fence(memory_order_seq_cst);

	GICR_CTLR_t ctlr = atomic_load_relaxed(&gicr->rd.ctlr);

	while (GICR_CTLR_get_RWP(&ctlr) != 0U) {
		asm_yield();
		ctlr = atomic_load_relaxed(&gicr->rd.ctlr);
	}

	// Order the successful load in the poll before anything afterwards
	atomic_device_fence(memory_order_acquire);
}

#if GICV3_HAS_LPI && (!GICV3_HAS_ITS || GICV3_HAS_VLPI_V4_1)
static void
gicr_wait_for_sync(gicr_t *gicr)
{
	// Order the write we're waiting for before the loads in the poll
	atomic_device_fence(memory_order_seq_cst);

	GICR_SYNCR_t syncr = atomic_load_relaxed(&gicr->rd.syncr);

	while (GICR_SYNCR_get_Busy(&syncr) != 0U) {
		asm_yield();
		syncr = atomic_load_relaxed(&gicr->rd.syncr);
	}

	// Order the successful load in the poll before anything afterwards
	atomic_device_fence(memory_order_acquire);
}
#endif

static count_t gicv3_spi_max_cache;
#if GICV3_EXT_IRQS
static count_t gicv3_spi_ext_max_cache;
static count_t gicv3_ppi_ext_max_cache;
#endif
#if GICV3_HAS_LPI
static count_t gicv3_lpi_max_cache;
#endif

static error_t
gicv3_spi_set_route_internal(irq_t irq, GICD_IROUTER_t route);

static void
gicr_set_percpu(cpu_index_t cpu)
{
	MPIDR_EL1_t mpidr = platform_cpu_index_to_mpidr(cpu);
	uint8_t	    aff0  = MPIDR_EL1_get_Aff0(&mpidr);
	uint8_t	    aff1  = MPIDR_EL1_get_Aff1(&mpidr);
	uint8_t	    aff2  = MPIDR_EL1_get_Aff2(&mpidr);
	uint8_t	    aff3  = MPIDR_EL1_get_Aff3(&mpidr);

	size_t gicr_stride = util_bit(GICR_STRIDE_SHIFT); // 64k for v3, 128k
							  // for v4
	GICR_TYPER_t gicr_typer;
	gicr_t	    *gicr = NULL;
#if GICV3_HAS_ITS
	paddr_t gicr_phys = 0U;
#endif

	// Search for the redistributor that matches this affinity value. We
	// assume that the stride that separates all redistributors is the same.
	for (index_t i = 0U; i < PLATFORM_GICR_COUNT; i++) {
		gicr = mapped_gicrs[i];
		assert(gicr != NULL);

#if GICV3_HAS_ITS
		gicr_phys = platform_gicrs_bases[i];
#endif
		gicr_typer = atomic_load_relaxed(&gicr->rd.typer);

		if ((GICR_TYPER_get_Aff0(&gicr_typer) == aff0) &&
		    (GICR_TYPER_get_Aff1(&gicr_typer) == aff1) &&
		    (GICR_TYPER_get_Aff2(&gicr_typer) == aff2) &&
		    (GICR_TYPER_get_Aff3(&gicr_typer) == aff3)) {
			break;
		} else {
			gicr = (gicr_t *)((paddr_t)gicr + gicr_stride);
		}
		if (GICR_TYPER_get_Last(&gicr_typer)) {
			break;
		}
	}

	if ((gicr == NULL) || (GICR_TYPER_get_Aff0(&gicr_typer) != aff0) ||
	    (GICR_TYPER_get_Aff1(&gicr_typer) != aff1) ||
	    (GICR_TYPER_get_Aff2(&gicr_typer) != aff2) ||
	    (GICR_TYPER_get_Aff3(&gicr_typer) != aff3)) {
		panic("gicv3: Unable to find CPU's redistributor.");
	}

	CPULOCAL_BY_INDEX(gicr_cpu, cpu).gicr = gicr;

#if GICV3_HAS_ITS
	// Inform the ITS driver of this CPU's redistributor
	gicv3_its_init_cpu(cpu, gicr, gicr_phys,
			   GICR_TYPER_get_Processor_Num(&gicr_typer));
#endif // GICV3_HAS_ITS
}

count_t
gicv3_irq_max(void)
{
	count_t result = gicv3_spi_max_cache;

#if GICV3_EXT_IRQS
	if (gicv3_spi_ext_max_cache != 0U) {
		result = gicv3_spi_ext_max_cache;
	} else if (gicv3_ppi_ext_max_cache != 0U) {
		result = gicv3_ppi_ext_max_cache;
	} else {
		// No extended IRQs implemented
	}
#endif

#if GICV3_HAS_LPI
	if (gicv3_lpi_max_cache != 0U) {
		result = gicv3_lpi_max_cache;
	}
#endif

	return result;
}

gicv3_irq_type_t
gicv3_get_irq_type(irq_t irq)
{
	gicv3_irq_type_t type;

	if (irq < GIC_SGI_BASE + GIC_SGI_NUM) {
		type = GICV3_IRQ_TYPE_SGI;
	} else if ((irq >= GIC_PPI_BASE) &&
		   (irq < (GIC_PPI_BASE + GIC_PPI_NUM))) {
		type = GICV3_IRQ_TYPE_PPI;
	} else if ((irq >= GIC_SPI_BASE) &&
		   (irq < (GIC_SPI_BASE + GIC_SPI_NUM)) &&
		   (irq <= gicv3_spi_max_cache)) {
		type = GICV3_IRQ_TYPE_SPI;
	} else if ((irq >= GIC_SPECIAL_INTIDS_BASE) &&
		   (irq < (GIC_SPECIAL_INTIDS_BASE + GIC_SPECIAL_INTIDS_NUM))) {
		type = GICV3_IRQ_TYPE_SPECIAL;
#if GICV3_HAS_LPI
	} else if ((irq >= GIC_LPI_BASE) &&
		   (irq < (GIC_LPI_BASE + GIC_LPI_NUM)) &&
		   (irq <= gicv3_lpi_max_cache)) {
		type = GICV3_IRQ_TYPE_LPI;
#endif
#if GICV3_EXT_IRQS
	} else if ((irq >= GIC_PPI_EXT_BASE) &&
		   (irq < (GIC_PPI_EXT_BASE + GIC_PPI_EXT_NUM)) &&
		   (irq <= gicv3_ppi_ext_max_cache)) {
		type = GICV3_IRQ_TYPE_PPI_EXT;
	} else if ((irq >= GIC_SPI_EXT_BASE) &&
		   (irq < (GIC_SPI_EXT_BASE + GIC_SPI_EXT_NUM)) &&
		   (irq <= gicv3_spi_ext_max_cache)) {
		type = GICV3_IRQ_TYPE_SPI_EXT;
#endif
	} else {
		type = GICV3_IRQ_TYPE_RESERVED;
	}

	return type;
}

bool
gicv3_irq_is_percpu(irq_t irq)
{
	bool ret;

	switch (gicv3_get_irq_type(irq)) {
	case GICV3_IRQ_TYPE_SGI:
	case GICV3_IRQ_TYPE_PPI:
#if GICV3_EXT_IRQS
	case GICV3_IRQ_TYPE_PPI_EXT:
#endif
#if GICV3_HAS_LPI
	case GICV3_IRQ_TYPE_LPI:
		// LPIs are treated as percpu because we need to know which GICR
		// to operate on. Note that we don't support platforms with an
		// ITS unless they also have GICv4.1. If we did, we would have
		// to reverse translate the LPI number to an event and device
		// ID, and use them to queue an ITS command.
#endif
		ret = true;
		break;
	case GICV3_IRQ_TYPE_SPI:
#if GICV3_EXT_IRQS
	case GICV3_IRQ_TYPE_SPI_EXT:
#endif
	case GICV3_IRQ_TYPE_SPECIAL:
	case GICV3_IRQ_TYPE_RESERVED:
	default:
		ret = false;
		break;
	}

	return ret;
}

static bool
is_irq_reserved(irq_t irq)
{
	uint8_t ipriority;

	// Assume that all CPUs have the same set of reserved SGIs / PPIs,
	// so it doesn't matter which GICR we check.
	assert(irq <= gicv3_irq_max());
	cpu_index_t cpu	 = cpulocal_check_index(cpulocal_get_index_unsafe());
	gicr_t	   *gicr = CPULOCAL_BY_INDEX(gicr_cpu, cpu).gicr;

	switch (gicv3_get_irq_type(irq)) {
	case GICV3_IRQ_TYPE_SGI:
	case GICV3_IRQ_TYPE_PPI:
		ipriority = atomic_load_relaxed(&gicr->sgi.ipriorityr[irq]);
		break;
	case GICV3_IRQ_TYPE_SPI:
		ipriority = atomic_load_relaxed(&gicd->ipriorityr[irq]);
		break;
#if GICV3_HAS_LPI
	case GICV3_IRQ_TYPE_LPI:
		// All LPIs are in group 1 and are not reserved.
		ipriority = GIC_PRIORITY_DEFAULT;
		break;
#endif
#if GICV3_EXT_IRQS
	case GICV3_IRQ_TYPE_PPI_EXT:
		ipriority = atomic_load_relaxed(
			&gicr->sgi.ipriorityr_e[irq - GIC_PPI_EXT_BASE]);
		break;
	case GICV3_IRQ_TYPE_SPI_EXT:
		ipriority = atomic_load_relaxed(
			&gicd->ipriorityr_e[irq - GIC_SPI_EXT_BASE]);
		break;
#endif
	case GICV3_IRQ_TYPE_SPECIAL:
	case GICV3_IRQ_TYPE_RESERVED:
	default:
		// Always reserved.
		ipriority = 0U;
		break;
	}

	// All interrupts with priority zero are reserved.
	return (ipriority == 0U);
}

error_t
gicv3_irq_check(irq_t irq)
{
	error_t ret = OK;

	if (irq > gicv3_irq_max()) {
		ret = ERROR_ARGUMENT_INVALID;
	} else if (is_irq_reserved(irq)) {
		ret = ERROR_DENIED;
	} else {
		ret = OK;
	}

	return ret;
}

static bool
gicv3_spi_is_enabled(irq_t irq)
{
	bool		 enabled  = false;
	gicv3_irq_type_t irq_type = gicv3_get_irq_type(irq);

#if GICV3_EXT_IRQS
	assert(irq_type == GICV3_IRQ_TYPE_SPI ||
	       irq_type == GICV3_IRQ_TYPE_SPI_EXT);
#else
	assert(irq_type == GICV3_IRQ_TYPE_SPI);
#endif
	if (irq_type == GICV3_IRQ_TYPE_SPI) {
		uint32_t isenabler = atomic_load_relaxed(
			&gicd->isenabler[GICD_ENABLE_GET_N(irq)]);
		enabled = (isenabler & GIC_ENABLE_BIT(irq)) != 0U;
	}
#if GICV3_EXT_IRQS
	else {
		// irq_type == GICV3_IRQ_TYPE_SPI_EXT
		uint32_t isenabler =
			atomic_load_relaxed(&gicd->isenabler[GICD_ENABLE_GET_N(
				irq - GIC_SPI_EXT_BASE)]);
		enabled = (isenabler &
			   GIC_ENABLE_BIT(irq - GIC_SPI_EXT_BASE)) != 0U;
	}
#endif
	return enabled;
}

static bool
gicv3_spi_get_route_cpu_affinity(const GICD_IROUTER_t *route, cpu_index_t *cpu)
{
	bool found = false;

	MPIDR_EL1_t mpidr = MPIDR_EL1_default();
	MPIDR_EL1_set_Aff0(&mpidr, GICD_IROUTER_get_Aff0(route));
	MPIDR_EL1_set_Aff1(&mpidr, GICD_IROUTER_get_Aff1(route));
	MPIDR_EL1_set_Aff2(&mpidr, GICD_IROUTER_get_Aff2(route));
	MPIDR_EL1_set_Aff3(&mpidr, GICD_IROUTER_get_Aff3(route));

	platform_mpidr_mapping_t mapping = platform_cpu_get_mpidr_mapping();

	if (platform_cpu_map_mpidr_valid(&mapping, mpidr)) {
		*cpu  = (cpu_index_t)platform_cpu_map_mpidr_to_index(&mapping,
								     mpidr);
		found = true;
	}

	return found;
}

static void
gicv3_spi_set_route_cpu_affinity(GICD_IROUTER_t *route, cpu_index_t cpu)
{
	assert(cpu < PLATFORM_MAX_CORES);

	MPIDR_EL1_t mpidr = platform_cpu_index_to_mpidr(cpu);
	GICD_IROUTER_set_IRM(route, 0);
	GICD_IROUTER_set_Aff0(route, MPIDR_EL1_get_Aff0(&mpidr));
	GICD_IROUTER_set_Aff1(route, MPIDR_EL1_get_Aff1(&mpidr));
	GICD_IROUTER_set_Aff2(route, MPIDR_EL1_get_Aff2(&mpidr));
	GICD_IROUTER_set_Aff3(route, MPIDR_EL1_get_Aff3(&mpidr));
}

#if GICV3_HAS_LPI || GICV3_HAS_ITS
static void
gicv3_boot_cold_init_lpis(GICD_TYPER_t typer, partition_t *hyp_partition,
			  GICR_TYPER_t gicr_typer)
{
	// Check that LPIs are actually supported, and determine the maximum
	// LPI number.
	bool	have_lpis = GICD_TYPER_get_LPIS(&typer);
	count_t idbits	  = GICD_TYPER_get_IDbits(&typer);
	count_t lpibits	  = GICD_TYPER_get_num_LPIs(&typer);
	if (!have_lpis || (idbits < 13U)) {
		gicv3_lpi_max_cache = 0U;
	} else if (lpibits == 0U) {
		gicv3_lpi_max_cache = (count_t)util_mask(idbits + 1U);
	} else {
		gicv3_lpi_max_cache =
			GIC_LPI_BASE + (count_t)util_mask(lpibits + 1U);
	}

	// Limit configured LPIs to the hard-coded maximum if the hardware
	// supports more than we need.
	gicv3_lpi_max_cache =
		util_min(gicv3_lpi_max_cache, GIC_LPI_BASE + GIC_LPI_NUM - 1U);

	if (gicv3_lpi_max_cache > 0U) {
		// Set all LPI property entries to disabled and default
		// priority.
		gic_lpi_prop_t lpi_prop = gic_lpi_prop_default();
		gic_lpi_prop_set_priority(&lpi_prop, GIC_PRIORITY_DEFAULT);
		for (count_t i = 0U; i < GIC_LPI_NUM; i++) {
			gic_lpi_prop_table[i] = lpi_prop;
		}

#if defined(GICV3_CACHE_INCOHERENT) && GICV3_CACHE_INCOHERENT
		CACHE_CLEAN_OBJECT(gic_lpi_prop_table);
#endif

		// Calculate the GICR_PROPBASE value that points to the config
		// table
		gic_lpi_propbase = GICR_PROPBASER_default();
		// IDbits: the number of bits required to represent all
		// configured LPIs, minus one
		GICR_PROPBASER_set_IDbits(
			&gic_lpi_propbase,
			32U - compiler_clz((uint32_t)gicv3_lpi_max_cache) - 1U);
		// InnerCache == 7: Inner write back, read + write alloc
		GICR_PROPBASER_set_InnerCache(&gic_lpi_propbase, 7U);
		// Shareability == 1: Inner shareable
		GICR_PROPBASER_set_Shareability(&gic_lpi_propbase, 1U);
		// PA: physical address of the LPI property table
		GICR_PROPBASER_set_PA(
			&gic_lpi_propbase,
			partition_virt_to_phys(hyp_partition,
					       (uintptr_t)&gic_lpi_prop_table));
		// OuterCache == 0: Inner and outer attributes are the same
		GICR_PROPBASER_set_OuterCache(&gic_lpi_propbase, 0U);

		// Allocate LPI pending bitmap and calculate GICR_PENDBASE per
		// CPU
		for (cpu_index_t i = 0U; i < PLATFORM_MAX_CORES; i++) {
			// One bit per IRQ number (including the first 8192
			// which are not LPIs); must be 64k-aligned and
			// zero-initialised. We allocate these from the heap
			// rather than BSS in the hope of not wasting the space
			// between them (~0.5MiB total)
			size_t lpi_bitmap_sz = (size_t)gicv3_lpi_max_cache / 8U;
			void_ptr_result_t alloc_r = partition_alloc(
				hyp_partition, lpi_bitmap_sz,
				(size_t)util_bit(GICR_PENDBASER_PA_SHIFT));
			if (alloc_r.e != OK) {
				panic("Unable to allocate physical LPI pending bitmap");
			}
			(void)memset_s(alloc_r.r, lpi_bitmap_sz, 0,
				       lpi_bitmap_sz);
			CPULOCAL_BY_INDEX(gicr_cpu, i).lpi_pending_bitmap =
				(register_t *)alloc_r.r;

			GICR_PENDBASER_t pendbase = GICR_PENDBASER_default();
			// InnerCache == 7: Inner write back, read + write alloc
			GICR_PENDBASER_set_InnerCache(&pendbase, 7U);
			// Shareability == 1: Inner shareable
			GICR_PENDBASER_set_Shareability(&pendbase, 1U);
			// PA: 64k-aligned physical address of the LPI pending
			// bitmap
			paddr_t lpi_bitmap_phys = partition_virt_to_phys(
				hyp_partition, (uintptr_t)alloc_r.r);
			assert(util_is_p2aligned(lpi_bitmap_phys,
						 GICR_PENDBASER_PA_SHIFT));
			GICR_PENDBASER_set_PA(&pendbase, lpi_bitmap_phys);
			// OuterCache == 0: Inner and outer attributes are the
			// same
			GICR_PENDBASER_set_OuterCache(&pendbase, 0U);
			// PTZ: table has been zeroed
			GICR_PENDBASER_set_PTZ(&pendbase, true);
			CPULOCAL_BY_INDEX(gicr_cpu, i).lpi_pendbase = pendbase;

#if defined(GICV3_CACHE_INCOHERENT) && GICV3_CACHE_INCOHERENT
			CACHE_CLEAN_RANGE(lpi_pending_bitmap, lpi_bitmap_sz);
#endif

#if defined(GICV3_ENABLE_VPE) && GICV3_ENABLE_VPE && GICV3_HAS_VLPI_V4_1
			spinlock_init(
				&CPULOCAL_BY_INDEX(gicr_cpu, i).vsgi_query_lock);
#endif
		}
	}

#if GICV3_HAS_ITS
	// Initialise the ITSs
	gicv3_its_init(&mapped_gitss,
		       (count_t)4U - (count_t)GICR_TYPER_get_CommonLPIAff(
					     &gicr_typer));
#endif // GICV3_HAS_ITS

#if GICV3_HAS_VLPI_V4_1
	// Check the supported vPE range
	GICD_TYPER2_t typer2   = atomic_load_relaxed(&gicd->typer2);
	count_t	      vpe_bits = GICD_TYPER2_get_VIL(&typer2)
					 ? 16U
					 : (GICD_TYPER2_get_VID(&typer2) + 1U);
	assert(GICV3_ITS_VPES < ((count_t)util_bit(vpe_bits)));
#endif // GICV3_HAS_VLPI_V4_1
}
#endif

#if GICV3_HAS_ITS
static void
gicv3_map_its(size_t gicr_size, size_t gits_size, partition_t *hyp_partition,
	      size_t gits_stride)
{
	// Map the ITS registers and calculate their virtual addresses.
	mapped_gitss[0] = (gits_t *)util_p2align_up(
		(uintptr_t)mapped_gicrs[0] + gicr_size, GITS_STRIDE_SHIFT);

	// Note: we are assuming here that the ITSs are physically contiguous,
	// which is a reasonable configuration but is not actually required by
	// the spec. This is not yet a significant issue because we have no
	// platform with multiple ITSs.
	pgtable_hyp_start();
	error_t ret = pgtable_hyp_map(hyp_partition, (uintptr_t)mapped_gitss[0],
				      gits_size, platform_gits_base,
				      PGTABLE_HYP_MEMTYPE_NOSPEC_NOCOMBINE,
				      PGTABLE_ACCESS_RW,
				      VMSA_SHAREABILITY_NON_SHAREABLE);
	if (ret != OK) {
		panic("gicv3: Mapping of ITSs failed.");
	}
	pgtable_hyp_commit();
#if (PLATFORM_GITS_COUNT > 1U)
	for (cpu_index_t i = 1U; i < PLATFORM_GITS_COUNT; i++) {
		mapped_gitss[i] = (gits_t *)((uintptr_t)mapped_gitss[i - 1U] +
					     gits_stride);
	}
#else
	(void)gits_stride;
#endif
}
#endif

static void
gicv3_map_gicd_and_gicrs(size_t gicr_size, partition_t *hyp_partition,
			 size_t gicd_size, virt_range_result_t range)
{
	paddr_t gicr_base   = PLATFORM_GICR_BASE;
	size_t	gicr_stride = (size_t)util_bit(GICR_STRIDE_SHIFT);

	pgtable_hyp_start();

	// Map the distributor
	gicd	    = (gicd_t *)range.r.base;
	error_t ret = pgtable_hyp_map(hyp_partition, (uintptr_t)gicd, gicd_size,
				      platform_gicd_base,
				      PGTABLE_HYP_MEMTYPE_NOSPEC_NOCOMBINE,
				      PGTABLE_ACCESS_RW,
				      VMSA_SHAREABILITY_NON_SHAREABLE);
	if (ret != OK) {
		panic("gicv3: Mapping of distributor failed.");
	}

	// Map the redistributors and calculate their addresses
	platform_gicrs_bases[0] = gicr_base;
	mapped_gicrs[0] =
		(gicr_t *)(range.r.base +
			   util_p2align_up(gicd_size, GICR_STRIDE_SHIFT));
	ret = pgtable_hyp_map(hyp_partition, (uintptr_t)mapped_gicrs[0],
			      gicr_size, platform_gicrs_bases[0],
			      PGTABLE_HYP_MEMTYPE_NOSPEC_NOCOMBINE,
			      PGTABLE_ACCESS_RW,
			      VMSA_SHAREABILITY_NON_SHAREABLE);
	if (ret != OK) {
		panic("gicv3: Mapping of redistributors failed.");
	}

	pgtable_hyp_commit();

	for (index_t i = 1; i < PLATFORM_GICR_COUNT; i++) {
		mapped_gicrs[i] = (gicr_t *)((uintptr_t)mapped_gicrs[i - 1U] +
					     gicr_stride);
		platform_gicrs_bases[i] =
			platform_gicrs_bases[i - 1U] + gicr_stride;

		// Ensure that DPG1NS is set, so the GICD does not try to route
		// 1-of-N interrupts to this GICR before we have set it up (when
		// the CPU first powers on), assuming that TZ has enabled E1NWF.
		// It is implementation defined whether this affects CPUs that
		// are sleeping, but it does work for GIC-600 and GIC-700.
		GICR_CTLR_t gicr_ctlr =
			atomic_load_relaxed(&mapped_gicrs[i]->rd.ctlr);
		GICR_CTLR_set_DPG1NS(&gicr_ctlr, true);
		atomic_store_relaxed(&mapped_gicrs[i]->rd.ctlr, gicr_ctlr);
	}
}

// In boot_cold we map the distributor and all the redistributors, based on
// their base addresses and sizes read from the device tree. We then initialize
// the distributor.
void
gicv3_handle_boot_cold_init(cpu_index_t cpu)
{
	partition_t *hyp_partition = partition_get_private();

	// There must be enough GICRs for all of the usable cores. This cannot
	// be a static assertion.
	assert(PLATFORM_GICR_COUNT >= compiler_popcount(PLATFORM_USABLE_CORES));

	// FIXME: remove when read from device tree
	size_t gicr_size = PLATFORM_GICR_COUNT * util_bit(GICR_STRIDE_SHIFT);
	size_t gicd_size = 0x10000U; // GICD is always 64K

	static_assert(PLATFORM_GICR_SIZE == PLATFORM_GICR_COUNT
						    << GICR_STRIDE_SHIFT,
		      "bad PLATFORM_GICR_SIZE");

#if GICV3_HAS_ITS
	size_t gits_stride = (size_t)util_bit(GITS_STRIDE_SHIFT);
	size_t gits_size   = (size_t)PLATFORM_GITS_COUNT << GITS_STRIDE_SHIFT;
#else
	size_t gits_size = 0U;
#endif

	virt_range_result_t range = hyp_aspace_allocate(
		util_p2align_up(gicd_size, GICR_STRIDE_SHIFT) + gicr_size +
		gits_size);
	if (range.e != OK) {
		panic("gicv3: Address allocation failed.");
	}

	gicv3_map_gicd_and_gicrs(gicr_size, hyp_partition, gicd_size, range);

#if GICV3_HAS_ITS
	gicv3_map_its(gicr_size, gits_size, hyp_partition, gits_stride);
#endif

	// Disable the distributor
	atomic_store_relaxed(&gicd->ctlr,
			     (GICD_CTLR_t){ .ns = GICD_CTLR_NS_default() });
	GICD_CTLR_NS_t ctlr = gicd_wait_for_write();

	// Calculate the number of supported IRQs
	GICD_TYPER_t typer = atomic_load_relaxed(&gicd->typer);

	count_t spi_ranges  = GICD_TYPER_get_ITLinesNumber(&typer) + 1U;
	gicv3_spi_max_cache = util_min(GIC_SPI_BASE + GIC_SPI_NUM - 1U,
				       (32U * spi_ranges) - 1U);

#if GICV3_EXT_IRQS || GICV3_HAS_ITS
	// Pick an arbitrary GICR to probe extended PPI and common LPI affinity
	// (we assume that these are the same across all GICRs)
	gicr_t	    *gicr	= mapped_gicrs[0];
	GICR_TYPER_t gicr_typer = atomic_load_relaxed(&gicr->rd.typer);
#endif

#if GICV3_EXT_IRQS
	count_t espi_ranges;

	if (GICD_TYPER_get_ESPI(&typer)) {
		espi_ranges = GICD_TYPER_get_ESPI_range(&typer) + 1U;
		gicv3_spi_ext_max_cache =
			GIC_SPI_EXT_BASE + (32U * espi_ranges) - 1U;
	} else {
		espi_ranges		= 0U;
		gicv3_spi_ext_max_cache = 0U;
	}

	GICR_TYPER_PPInum_t eppi = GICR_TYPER_get_PPInum(&gicr_typer);

	switch (eppi) {
	case GICR_TYPER_PPINUM_MAX_1087:
		gicv3_ppi_ext_max_cache = 1087U;
		break;
	case GICR_TYPER_PPINUM_MAX_1119:
		gicv3_ppi_ext_max_cache = 1119U;
		break;
	case GICR_TYPER_PPINUM_MAX_31:
	default:
		gicv3_ppi_ext_max_cache = 0U;
		break;
	}
#endif

#if GICV3_HAS_1N
	assert(!GICD_TYPER_get_No1N(&typer));
#endif

	// Enable non-secure state affinity routing
	GICD_CTLR_NS_set_ARE_NS(&ctlr, true);

	atomic_store_relaxed(&gicd->ctlr, (GICD_CTLR_t){ .ns = ctlr });
	ctlr = gicd_wait_for_write();

#if GICV3_HAS_SECURITY_DISABLED
	// If security disabled set all interrupts to group 1
	GICD_CTLR_t ctlr_ds = atomic_load_relaxed(&(gicd->ctlr));
	assert(GICD_CTLR_DS_get_DS(&ctlr_ds.ds));

	for (index_t i = 1U; i < spi_ranges; i++) {
		atomic_store_relaxed(&gicd->igroupr[i], 0xffffffffU);
	}
#if GICV3_EXT_IRQS
	for (index_t i = 0U; i < espi_ranges; i++) {
		atomic_store_relaxed(&gicd->igroupr_e[i], 0xffffffffU);
	}
#endif
#endif

	// Configure all SPIs to the default priority
	for (irq_t i = GIC_SPI_BASE; i <= gicv3_spi_max_cache; i++) {
		atomic_store_relaxed(&gicd->ipriorityr[i],
				     GIC_PRIORITY_DEFAULT);
	}

#if GICV3_EXT_IRQS
	// Configure all extended SPIs to the default priority
	for (irq_t i = GIC_SPI_EXT_BASE; i <= gicv3_spi_ext_max_cache; i++) {
		atomic_store_relaxed(&gicd->ipriorityr_e[i - GIC_SPI_EXT_BASE],
				     GIC_PRIORITY_DEFAULT);
	}
#endif

#if GICV3_HAS_LPI
	gicv3_boot_cold_init_lpis(typer, hyp_partition, gicr_typer);
#endif // GICV3_HAS_LPI

	// Route all SPIs to the boot CPU by default.
	MPIDR_EL1_t    mpidr   = platform_cpu_index_to_mpidr(cpu);
	uint8_t	       aff0    = MPIDR_EL1_get_Aff0(&mpidr);
	uint8_t	       aff1    = MPIDR_EL1_get_Aff1(&mpidr);
	uint8_t	       aff2    = MPIDR_EL1_get_Aff2(&mpidr);
	uint8_t	       aff3    = MPIDR_EL1_get_Aff3(&mpidr);
	GICD_IROUTER_t irouter = GICD_IROUTER_default();
	GICD_IROUTER_set_IRM(&irouter, false);
	GICD_IROUTER_set_Aff0(&irouter, aff0);
	GICD_IROUTER_set_Aff1(&irouter, aff1);
	GICD_IROUTER_set_Aff2(&irouter, aff2);
	GICD_IROUTER_set_Aff3(&irouter, aff3);

	for (irq_t i = GIC_SPI_BASE; i <= gicv3_spi_max_cache; i++) {
		atomic_store_relaxed(&gicd->irouter[i - GIC_SPI_BASE], irouter);
	}
#if GICV3_EXT_IRQS
	for (irq_t i = GIC_SPI_EXT_BASE; i <= gicv3_spi_ext_max_cache; i++) {
		atomic_store_relaxed(&gicd->irouter_e[i - GIC_SPI_EXT_BASE],
				     irouter);
	}
#endif

	// Enable Affinity Group 1 interrupts
	GICD_CTLR_NS_set_EnableGrp1A(&ctlr, true);
	atomic_store_relaxed(&gicd->ctlr, (GICD_CTLR_t){ .ns = ctlr });

	// Disable forwarding of the all SPIs interrupts
	// First 32 bits (index 0) correspond to SGIs and PPIs, which are now
	// handled in the redistributor. Therefore, we start from index 1.
	for (index_t i = 1U; i < spi_ranges; i++) {
		atomic_store_relaxed(&gicd->icenabler[i], 0xffffffffU);
	}
#if GICV3_EXT_IRQS
	for (index_t i = 0U; i < espi_ranges; i++) {
		atomic_store_relaxed(&gicd->icenabler_e[i], 0xffffffffU);
	}
#endif
	(void)gicd_wait_for_write();

	// Set up the cached SGIRs used for IPIs targeting each CPU
	for (cpu_index_t i = 0U; i < PLATFORM_MAX_CORES; i++) {
		mpidr = platform_cpu_index_to_mpidr(i);

		aff0 = MPIDR_EL1_get_Aff0(&mpidr);
		aff1 = MPIDR_EL1_get_Aff1(&mpidr);
		aff2 = MPIDR_EL1_get_Aff2(&mpidr);
		aff3 = MPIDR_EL1_get_Aff3(&mpidr);

		ICC_SGIR_EL1_t icc_sgi1r = ICC_SGIR_EL1_default();
		ICC_SGIR_EL1_set_TargetList(&icc_sgi1r,
					    (uint16_t)util_bit(aff0 % 16U));
		ICC_SGIR_EL1_set_RS(&icc_sgi1r, aff0 / 16U);
		ICC_SGIR_EL1_set_Aff1(&icc_sgi1r, aff1);
		ICC_SGIR_EL1_set_Aff2(&icc_sgi1r, aff2);
		ICC_SGIR_EL1_set_Aff3(&icc_sgi1r, aff3);

		CPULOCAL_BY_INDEX(gicr_cpu, i).icc_sgi1r = icc_sgi1r;
	}

	// Set up gicr for each CPU
	for (cpu_index_t i = 0U; i < PLATFORM_MAX_CORES; i++) {
		if (platform_cpu_exists(i)) {
			gicr_set_percpu(i);

			// Set online state for interrupt migration at poweroff
			// Set boot cpu as online. Secondary CPUs will be set
			// by 'power_cpu_online' handler
			gicr_cpu_t *gc = &CPULOCAL_BY_INDEX(gicr_cpu, i);
			gc->online     = (i == cpu);
		}
	}

#if GICV3_HAS_GICD_ICLAR
	// The physical GIC implements IRQ classes for 1-of-N IRQs. The virtual
	// GIC will expose these to VMs through an implementation-defined
	// register of its own (GICD_SETCLASSR).
	GICD_IIDR_t iidr = atomic_load_relaxed(&gicd->iidr);
	// Implementer must be ARM (JEP106 code: [0x4] 0x3b)
	assert(GICD_IIDR_get_Implementer(&iidr) == 0x43bU);
	// Product ID must be 2 (GIC-600) or 4 (GIC-700)
	assert((GICD_IIDR_get_ProductID(&iidr) == 2U) ||
	       (GICD_IIDR_get_ProductID(&iidr) == 4U));
#endif

	spinlock_init(&bitmap_update_lock);
	spinlock_init(&spi_route_lock);
}

// In the boot_cpu_cold we search for the redistributor that corresponds to the
// current cpu by comparing the affinity values.
void
gicv3_handle_boot_cpu_cold_init(cpu_index_t cpu)
{
	gicr_t *gicr = CPULOCAL_BY_INDEX(gicr_cpu, cpu).gicr;

	// Configure all interrupts to the default priority
	for (irq_t i = GIC_SGI_BASE; i < GIC_PPI_BASE; i++) {
		atomic_store_relaxed(&gicr->sgi.ipriorityr[i],
				     GIC_PRIORITY_DEFAULT);
	}
	for (irq_t i = GIC_PPI_BASE; i < GIC_SPI_BASE; i++) {
		atomic_store_relaxed(&gicr->sgi.ipriorityr[i],
				     GIC_PRIORITY_DEFAULT);
	}

#if GICV3_EXT_IRQS
	for (irq_t i = GIC_PPI_EXT_BASE; i < gicv3_ppi_ext_max_cache; i++) {
		atomic_store_relaxed(
			&gicr->sgi.ipriorityr_e[i - GIC_PPI_EXT_BASE],
			GIC_PRIORITY_DEFAULT);
	}
#endif

#if GICV3_HAS_SECURITY_DISABLED
	// If security disabled set all interrupts to group 1
	GICD_CTLR_t ctlr_ds = atomic_load_relaxed(&(gicd->ctlr));
	assert(GICD_CTLR_DS_get_DS(&ctlr_ds.ds));

	atomic_store_relaxed(&gicr->sgi.igroupr0, 0xffffffffU);
#if GICV3_EXT_IRQS
	if (gicv3_ppi_ext_max_cache >= GIC_PPI_EXT_BASE) {
		atomic_store_relaxed(&gicr->sgi.igroupr_e[0], 0xffffffffU);
	}
	if (gicv3_ppi_ext_max_cache >= (GIC_PPI_EXT_BASE + 32U)) {
		atomic_store_relaxed(&gicr->sgi.igroupr_e[1], 0xffffffffU);
	}
#endif

	GICR_WAKER_t waker = GICR_WAKER_default();
	GICR_WAKER_set_ProcessorSleep(&waker, false);
	atomic_store_relaxed(&gicr->rd.waker, waker);

	// Wait for gicr to be on
	GICR_WAKER_t waker_read;
	do {
		waker_read = atomic_load_acquire(&gicr->rd.waker);
	} while (GICR_WAKER_get_ChildrenAsleep(&waker_read) != 0U);
#endif

	// Disable all local IRQs
	atomic_store_relaxed(&gicr->sgi.icenabler0, 0xffffffffU);
#if GICV3_EXT_IRQS
	if (gicv3_ppi_ext_max_cache >= GIC_PPI_EXT_BASE) {
		atomic_store_relaxed(&gicr->sgi.icenabler_e[0], 0xffffffffU);
	}
	if (gicv3_ppi_ext_max_cache >= (GIC_PPI_EXT_BASE + 32U)) {
		atomic_store_relaxed(&gicr->sgi.icenabler_e[1], 0xffffffffU);
	}
#endif
	gicr_wait_for_write(gicr);

	GICR_CTLR_t ctlr = atomic_load_relaxed(&gicr->rd.ctlr);

	// If LPIs have already been enabled, we can't set the base registers
	// if we have LPI support, and we (possibly) can't disable them if we
	// don't support them.
	assert(!GICR_CTLR_get_Enable_LPIs(&ctlr));

	// Enable 1-of-N targeting to this CPU
	GICR_CTLR_set_DPG1NS(&ctlr, false);
	atomic_store_relaxed(&gicr->rd.ctlr, ctlr);

#if GICV3_HAS_LPI
	GICR_TYPER_t typer = atomic_load_relaxed(&gicr->rd.typer);

	assert(GICR_TYPER_get_PLPIS(&typer));

#if !GICV3_HAS_ITS
	// Direct LPIs must be supported if there is no ITS; otherwise there
	// would be no way to generate LPIs.
	//
	// Note that three of the five registers covered by this feature,
	// INVLPIR, INVALLR and SYNCR, are also mandatory in GICv4.1; however,
	// SETLPIR and CLRLPIR are not, so this bit might not be set by a
	// GICv4.1 implementation. In GICv4.1, a separate ID bit is added to
	// indicate support for those three registers; see below.
	assert(GICR_TYPER_get_DirectLPI(&typer));
#endif // !GICV3_HAS_ITS

#if GICV3_HAS_VLPI_V4_1
	// GICv4.1 requires the GICR to use the vPE-format VPENDBASER, and to
	// support the invalidate registers (a subset of DirectLPI), VSGI
	// delivery, and polling for completion of vPE scheduling.
	assert(GICR_TYPER_get_RVPEID(&typer) && GICR_TYPER_get_VSGI(&typer) &&
	       GICR_TYPER_get_Dirty(&typer) && GICR_CTLR_get_IR(&ctlr));
#elif GICV3_HAS_VLPI
	// GICv4.0 requires the GICR not to use the vPE-format VPENDBASER.
	assert(!GICR_TYPER_get_RVPEID(&typer));
#endif

	if (gicv3_lpi_max_cache > 0U) {
		// Set the base registers
		atomic_store_relaxed(&gicr->rd.propbaser, gic_lpi_propbase);
		GICR_PENDBASER_t *pendbase =
			&CPULOCAL_BY_INDEX(gicr_cpu, cpu).lpi_pendbase;
		atomic_store_relaxed(&gicr->rd.pendbaser, *pendbase);

		// Read back pendbaser to assert that shareability is nonzero
		// (i.e. accesses to the table are cache-coherent) and also to
		// clear PTZ in case we need to rewrite it
		*pendbase = atomic_load_relaxed(&gicr->rd.pendbaser);
		assert(GICR_PENDBASER_get_Shareability(pendbase) != 0U);

		// Enable LPIs (note: this may be permanent until reset)
		GICR_CTLR_set_Enable_LPIs(&ctlr, true);
		atomic_store_release(&gicr->rd.ctlr, ctlr);
	}

#endif // GICV3_HAS_LPI

#if PLATFORM_IPI_LINES > ENUM_IPI_REASON_MAX_VALUE
	// Enable the SGIs used by IPIs
	atomic_store_release(&gicr->sgi.isenabler0,
			     util_mask(ENUM_IPI_REASON_MAX_VALUE + 1U));
#else
	// Enable the shared SGI for all IPIs
	atomic_store_release(&gicr->sgi.isenabler0, 0x1);
#endif
}

// Redistributor control register initialization
void
gicv3_handle_boot_cpu_warm_init(void)
{
	asm_ordering_dummy_t gic_init_order;

#if GICV3_HAS_LPI
	GICR_CTLR_t ctlr =
		atomic_load_relaxed(&CPULOCAL(gicr_cpu).gicr->rd.ctlr);

	// LPIs should have already been enabled
	assert(GICR_CTLR_get_Enable_LPIs(&ctlr) == (gicv3_lpi_max_cache != 0U));
#endif

	// Enable system register access and disable FIQ and IRQ bypass
	ICC_SRE_EL2_t icc_sre = ICC_SRE_EL2_default();
	// Trap EL1 accesses to ICC_SRE_EL1
	ICC_SRE_EL2_set_Enable(&icc_sre, false);
	// Disable IRQ and FIQ bypass
	ICC_SRE_EL2_set_DIB(&icc_sre, true);
	ICC_SRE_EL2_set_DFB(&icc_sre, true);
	// Enable system register accesses
	ICC_SRE_EL2_set_SRE(&icc_sre, true);
	register_ICC_SRE_EL2_write_ordered(icc_sre, &gic_init_order);
	asm_context_sync_ordered(&gic_init_order);

	// Configure PMR to allow all interrupt priorities
	ICC_PMR_EL1_t icc_pmr = ICC_PMR_EL1_default();
	ICC_PMR_EL1_set_Priority(&icc_pmr, 0xff);
	register_ICC_PMR_EL1_write_ordered(icc_pmr, &gic_init_order);

	// Set EOImode to 1, so we can drop priority before delivery to VMs
	ICC_CTLR_EL1_t icc_ctrl = register_ICC_CTLR_EL1_read();
	ICC_CTLR_EL1_set_EOImode(&icc_ctrl, true);
	register_ICC_CTLR_EL1_write_ordered(icc_ctrl, &gic_init_order);

	// Enable group 1 interrupts
	ICC_IGRPEN_EL1_t icc_grpen1 = ICC_IGRPEN_EL1_default();
	ICC_IGRPEN_EL1_set_Enable(&icc_grpen1, true);
	register_ICC_IGRPEN1_EL1_write_ordered(icc_grpen1, &gic_init_order);
	asm_context_sync_ordered(&gic_init_order);

#if GICV3_DEBUG
	gicr_t *gicr = CPULOCAL(gicr_cpu).gicr;
	TRACE_LOCAL(
		DEBUG, INFO,
		"gicv3 cpu warm init, en {:#x} act {:#x} grp {:#x} hpp {:#x}",
		atomic_load_relaxed(&gicr->sgi.isenabler0),
		atomic_load_relaxed(&gicr->sgi.isactiver0),
		atomic_load_relaxed(&gicr->sgi.igroupr0),
		ICC_HPPIR_EL1_raw(register_ICC_HPPIR1_EL1_read()));
#endif
}

error_t
gicv3_handle_power_cpu_suspend(void)
{
	// Disable group 1 interrupts
	ICC_IGRPEN_EL1_t icc_grpen1 = ICC_IGRPEN_EL1_default();
	ICC_IGRPEN_EL1_set_Enable(&icc_grpen1, false);
	register_ICC_IGRPEN1_EL1_write_ordered(icc_grpen1, &asm_ordering);

#if GICV3_DEBUG || GICV3_HAS_SECURITY_DISABLED
	gicr_t *gicr = CPULOCAL(gicr_cpu).gicr;
#endif
#if GICV3_DEBUG
	TRACE_LOCAL(DEBUG, INFO,
		    "gicv3 cpu suspend, en {:#x} act {:#x} grp {:#x} hpp {:#x}",
		    atomic_load_relaxed(&gicr->sgi.isenabler0),
		    atomic_load_relaxed(&gicr->sgi.isactiver0),
		    atomic_load_relaxed(&gicr->sgi.igroupr0),
		    ICC_HPPIR_EL1_raw(register_ICC_HPPIR1_EL1_read()));
#endif

#if GICV3_HAS_SECURITY_DISABLED
	// Ensure that the IGRPEN1_EL1 write has completed
	__asm__ volatile("isb; dsb sy;" ::: "memory");

	// Set ProcessorSleep, so that the redistributor hands over ownership
	// of any pending interrupts before it powers off
	GICR_WAKER_t waker = GICR_WAKER_default();
	GICR_WAKER_set_ProcessorSleep(&waker, true);
	atomic_store_relaxed(&gicr->rd.waker, waker);
#endif

#if defined(GICV3_ENABLE_VPE) && GICV3_ENABLE_VPE
	// Ensure that the GICR has finished descheduling the last vPE.
	gicv3_vpe_sync_deschedule(cpulocal_get_index(), false);
#endif

#if GICV3_HAS_SECURITY_DISABLED
	// Wait for gicr to be off
	GICR_WAKER_t waker_read;
	do {
		waker_read = atomic_load_acquire(&gicr->rd.waker);
	} while (GICR_WAKER_get_ChildrenAsleep(&waker_read) == 0U);
#endif

	return OK;
}

void
gicv3_handle_power_cpu_resume(void)
{
	asm_ordering_dummy_t gic_enable_order;

	// Enable group 1 interrupts
	ICC_IGRPEN_EL1_t icc_grpen1 = ICC_IGRPEN_EL1_default();
	ICC_IGRPEN_EL1_set_Enable(&icc_grpen1, true);
	register_ICC_IGRPEN1_EL1_write_ordered(icc_grpen1, &gic_enable_order);
	asm_context_sync_ordered(&gic_enable_order);

#if GICV3_DEBUG || GICV3_HAS_SECURITY_DISABLED
	gicr_t *gicr = CPULOCAL(gicr_cpu).gicr;
#endif
#if GICV3_DEBUG
	TRACE_LOCAL(DEBUG, INFO,
		    "gicv3 cpu resume, en {:#x} act {:#x} grp {:#x} hpp {:#x}",
		    atomic_load_relaxed(&gicr->sgi.isenabler0),
		    atomic_load_relaxed(&gicr->sgi.isactiver0),
		    atomic_load_relaxed(&gicr->sgi.igroupr0),
		    ICC_HPPIR_EL1_raw(register_ICC_HPPIR1_EL1_read()));
#endif

#if GICV3_HAS_SECURITY_DISABLED
	// Clear ProcessorSleep, so that it can start handling interrupts.
	GICR_WAKER_t waker = GICR_WAKER_default();
	GICR_WAKER_set_ProcessorSleep(&waker, false);
	atomic_store_relaxed(&gicr->rd.waker, waker);

	// Wait for gicr to be on
	GICR_WAKER_t waker_read;
	do {
		waker_read = atomic_load_acquire(&gicr->rd.waker);
	} while (GICR_WAKER_get_ChildrenAsleep(&waker_read) != 0U);
#endif
}

void
gicv3_irq_enable_shared(irq_t irq)
{
	assert(irq <= gicv3_irq_max());
	gicv3_irq_type_t irq_type = gicv3_get_irq_type(irq);
#if GICV3_EXT_IRQS
	assert(irq_type == GICV3_IRQ_TYPE_SPI ||
	       irq_type == GICV3_IRQ_TYPE_SPI_EXT);
#else
	assert(irq_type == GICV3_IRQ_TYPE_SPI);
#endif

	// Ensure the route is still valid if we enable the irq for the
	// first time

	// Take the SPI lock so we can fetch the current route
	// If it has a specific target CPU (not 1:N) ensure it is online
	spinlock_acquire(&spi_route_lock);

	if (!gicv3_spi_is_enabled(irq)) {
#if GICV3_EXT_IRQS
		GICD_IROUTER_t route;
		if (irq_type == GICV3_IRQ_TYPE_SPI) {
			route = atomic_load_relaxed(
				&gicd->irouter[irq - GIC_SPI_BASE]);
		} else {
			route = atomic_load_relaxed(
				&gicd->irouter_e[irq - GIC_SPI_EXT_BASE]);
		}
#else
		GICD_IROUTER_t route =
			atomic_load_relaxed(&gicd->irouter[irq - GIC_SPI_BASE]);
#endif
		if (!GICD_IROUTER_get_IRM(&route)) {
			cpu_index_t target;
			gicr_cpu_t *gc = NULL;
			bool valid = gicv3_spi_get_route_cpu_affinity(&route,
								      &target);
			if (valid) {
				gc = &CPULOCAL_BY_INDEX(gicr_cpu, target);
			}
			// Set affinity to this CPU if needed
			if (!valid || !gc->online) {
				gc = &CPULOCAL(gicr_cpu);
				assert(gc->online);
				gicv3_spi_set_route_cpu_affinity(
					&route, cpulocal_get_index());
				(void)gicv3_spi_set_route_internal(irq, route);
			}
		}
	}

	{
#if GICV3_EXT_IRQS
		if (irq_type == GICV3_IRQ_TYPE_SPI) {
			atomic_store_release(
				&gicd->isenabler[GICD_ENABLE_GET_N(irq)],
				GIC_ENABLE_BIT(irq));
		} else {
			atomic_store_release(
				&gicd->isenabler_e[GICD_ENABLE_GET_N(
					irq - GIC_SPI_EXT_BASE)],
				GIC_ENABLE_BIT(irq - GIC_SPI_EXT_BASE));
		}
#else
		atomic_store_release(&gicd->isenabler[GICD_ENABLE_GET_N(irq)],
				     GIC_ENABLE_BIT(irq));
#endif
	}

	spinlock_release(&spi_route_lock);
}

void
gicv3_irq_enable_percpu(irq_t irq, cpu_index_t cpu)
{
	assert(irq <= gicv3_irq_max());

	gicr_t		*gicr	  = CPULOCAL_BY_INDEX(gicr_cpu, cpu).gicr;
	gicv3_irq_type_t irq_type = gicv3_get_irq_type(irq);

	switch (irq_type) {
	case GICV3_IRQ_TYPE_SGI:
	case GICV3_IRQ_TYPE_PPI: {
		{
			atomic_store_release(&gicr->sgi.isenabler0,
					     GIC_ENABLE_BIT(irq));
		}
		break;
	}

#if GICV3_EXT_IRQS
	case GICV3_IRQ_TYPE_PPI_EXT: {
		// Extended PPI
		{
			atomic_store_release(
				&gicr->sgi.isenabler_e[GICD_ENABLE_GET_N(
					irq - GIC_PPI_EXT_BASE)],
				GIC_ENABLE_BIT(irq - GIC_PPI_EXT_BASE));
		}
		break;
	}
#endif
#if GICV3_HAS_LPI
	case GICV3_IRQ_TYPE_LPI:
		gic_lpi_prop_set_enable(&gic_lpi_prop_table[irq - GIC_LPI_BASE],
					true);
		gicv3_lpi_inv_by_id(cpu, irq);
		break;
#endif
	case GICV3_IRQ_TYPE_SPI:
#if GICV3_EXT_IRQS
	case GICV3_IRQ_TYPE_SPI_EXT:
#endif
	case GICV3_IRQ_TYPE_SPECIAL:
	case GICV3_IRQ_TYPE_RESERVED:
	default:
		panic("Incorrect IRQ type");
	}
}

void
gicv3_irq_enable_local(irq_t irq)
{
	assert_cpulocal_safe();
	gicv3_irq_enable_percpu(irq, cpulocal_get_index());
}

void
gicv3_irq_disable_shared(irq_t irq)
{
	assert(irq <= gicv3_irq_max());

	gicv3_irq_type_t irq_type = gicv3_get_irq_type(irq);

	switch (irq_type) {
	case GICV3_IRQ_TYPE_SPI:
		{
			atomic_store_relaxed(
				&gicd->icenabler[GICD_ENABLE_GET_N(irq)],
				GIC_ENABLE_BIT(irq));
		}
		(void)gicd_wait_for_write();
		break;

#if GICV3_EXT_IRQS
	case GICV3_IRQ_TYPE_SPI_EXT: {
		// Extended SPI
		{
			atomic_store_relaxed(
				&gicd->icenabler_e[GICD_ENABLE_GET_N(
					irq - GIC_SPI_EXT_BASE)],
				GIC_ENABLE_BIT(irq - GIC_SPI_EXT_BASE));
		}
		(void)gicd_wait_for_write();
		break;
	}
#endif
#if GICV3_HAS_LPI
	case GICV3_IRQ_TYPE_LPI:
#endif
	case GICV3_IRQ_TYPE_SGI:
	case GICV3_IRQ_TYPE_PPI:
#if GICV3_EXT_IRQS
	case GICV3_IRQ_TYPE_PPI_EXT:
#endif
	case GICV3_IRQ_TYPE_SPECIAL:
	case GICV3_IRQ_TYPE_RESERVED:
	default:
		panic("Incorrect IRQ type");
	}
}

void
gicv3_irq_cancel_nowait(irq_t irq)
{
	assert(irq <= gicv3_irq_max());

	gicv3_irq_type_t irq_type = gicv3_get_irq_type(irq);

	switch (irq_type) {
	case GICV3_IRQ_TYPE_SPI:
		atomic_store_relaxed(&gicd->icpendr[GICD_ENABLE_GET_N(irq)],
				     GIC_ENABLE_BIT(irq));
		// The spec does not give us any way to wait for this to
		// complete, hence the nowait() in the name. There is also no
		// guarantee of timely completion.
		break;

#if GICV3_EXT_IRQS
	case GICV3_IRQ_TYPE_SPI_EXT: {
		// Extended SPI
		atomic_store_relaxed(&gicd->icpendr_e[GICD_ENABLE_GET_N(
					     irq - GIC_SPI_EXT_BASE)],
				     GIC_ENABLE_BIT(irq - GIC_SPI_EXT_BASE));
		// As above, there is no way to guarantee completion.
		break;
	}
#endif
#if GICV3_HAS_LPI
	case GICV3_IRQ_TYPE_LPI:
#endif
	case GICV3_IRQ_TYPE_SGI:
	case GICV3_IRQ_TYPE_PPI:
#if GICV3_EXT_IRQS
	case GICV3_IRQ_TYPE_PPI_EXT:
#endif
	case GICV3_IRQ_TYPE_SPECIAL:
	case GICV3_IRQ_TYPE_RESERVED:
	default:
		panic("Incorrect IRQ type");
	}
}

static void
gicv3_irq_disable_percpu_nowait(irq_t irq, cpu_index_t cpu)
{
	assert(irq <= gicv3_irq_max());

	gicr_t		*gicr	  = CPULOCAL_BY_INDEX(gicr_cpu, cpu).gicr;
	gicv3_irq_type_t irq_type = gicv3_get_irq_type(irq);

	switch (irq_type) {
	case GICV3_IRQ_TYPE_SGI:
	case GICV3_IRQ_TYPE_PPI: {
		{
			atomic_store_relaxed(&gicr->sgi.icenabler0,
					     GIC_ENABLE_BIT(irq));
		}
		break;
	}
#if GICV3_EXT_IRQS
	case GICV3_IRQ_TYPE_PPI_EXT: {
		// Extended PPI
		{
			atomic_store_relaxed(
				&gicr->sgi.icenabler_e[GICD_ENABLE_GET_N(
					irq - GIC_PPI_EXT_BASE)],
				GIC_ENABLE_BIT(irq - GIC_PPI_EXT_BASE));
		}
		break;
	}
#endif
#if GICV3_HAS_LPI
	case GICV3_IRQ_TYPE_LPI:
		gic_lpi_prop_set_enable(&gic_lpi_prop_table[irq - GIC_LPI_BASE],
					false);
		gicv3_lpi_inv_by_id(cpu, irq);
		break;
#endif
	case GICV3_IRQ_TYPE_SPI:
#if GICV3_EXT_IRQS
	case GICV3_IRQ_TYPE_SPI_EXT:
#endif
	case GICV3_IRQ_TYPE_SPECIAL:
	case GICV3_IRQ_TYPE_RESERVED:
	default:
		panic("Incorrect IRQ type");
	}
}

void
gicv3_irq_disable_percpu(irq_t irq, cpu_index_t cpu)
{
	gicv3_irq_disable_percpu_nowait(irq, cpu);
#if GICV3_HAS_LPI
	if (gicv3_get_irq_type(irq) == GICV3_IRQ_TYPE_LPI) {
		gicr_wait_for_sync(CPULOCAL_BY_INDEX(gicr_cpu, cpu).gicr);
	} else {
		gicr_wait_for_write(CPULOCAL_BY_INDEX(gicr_cpu, cpu).gicr);
	}
#else
	gicr_wait_for_write(CPULOCAL_BY_INDEX(gicr_cpu, cpu).gicr);
#endif
}

void
gicv3_irq_disable_local(irq_t irq)
{
	assert_cpulocal_safe();
	gicv3_irq_disable_percpu(irq, cpulocal_get_index());
}

void
gicv3_irq_disable_local_nowait(irq_t irq)
{
	assert_cpulocal_safe();
	gicv3_irq_disable_percpu_nowait(irq, cpulocal_get_index());
}

irq_trigger_result_t
gicv3_irq_set_trigger_percpu(irq_t irq, irq_trigger_t trigger, cpu_index_t cpu)
{
	irq_trigger_result_t ret;

	gicr_t		*gicr	  = CPULOCAL_BY_INDEX(gicr_cpu, cpu).gicr;
	gicv3_irq_type_t irq_type = gicv3_get_irq_type(irq);

	// We do not support this behavior for now
	if (trigger == IRQ_TRIGGER_MESSAGE) {
		ret = irq_trigger_result_error(ERROR_ARGUMENT_INVALID);
		goto end_function;
	}

	spinlock_acquire(&bitmap_update_lock);

	switch (irq_type) {
	case GICV3_IRQ_TYPE_PPI: {
		uint32_t isenabler0 =
			atomic_load_relaxed(&gicr->sgi.isenabler0);
		bool enabled = (isenabler0 & GIC_ENABLE_BIT(irq)) != 0U;

		if (enabled) {
			gicv3_irq_disable_percpu(irq, cpu);
		}

		register_t icfg =
			(register_t)atomic_load_relaxed(&gicr->sgi.icfgr[1]);

		if ((trigger == IRQ_TRIGGER_LEVEL_HIGH) ||
		    (trigger == IRQ_TRIGGER_LEVEL_LOW)) {
			bitmap_clear(&icfg, ((irq % 16U) * 2U) + 1U);
		} else {
			bitmap_set(&icfg, ((irq % 16U) * 2U) + 1U);
		}

		atomic_store_relaxed(&gicr->sgi.icfgr[1], (uint32_t)icfg);

		if (enabled) {
			gicv3_irq_enable_percpu(irq, cpu);
		}

		// Read back the value in case it could not be changed
		icfg = atomic_load_relaxed(&gicr->sgi.icfgr[1]);
		ret  = irq_trigger_result_ok(
			 bitmap_isset(&icfg, ((irq % 16U) * 2U) + 1U)
				 ? IRQ_TRIGGER_EDGE_RISING
				 : IRQ_TRIGGER_LEVEL_HIGH);

		break;
	}

#if GICV3_EXT_IRQS
	case GICV3_IRQ_TYPE_PPI_EXT: {
		// Extended PPI
		uint32_t isenabler_e = atomic_load_relaxed(
			&gicr->sgi.isenabler_e[GICD_ENABLE_GET_N(
				irq - GIC_PPI_EXT_BASE)]);
		bool enabled = (isenabler_e &
				GIC_ENABLE_BIT(irq - GIC_PPI_EXT_BASE)) != 0U;

		if (enabled) {
			gicv3_irq_disable_percpu(irq, cpu);
		}

		register_t icfg = (register_t)atomic_load_relaxed(
			&gicr->sgi.icfgr_e[(irq - GIC_PPI_EXT_BASE) / 16U]);

		if ((trigger == IRQ_TRIGGER_LEVEL_HIGH) ||
		    (trigger == IRQ_TRIGGER_LEVEL_LOW)) {
			bitmap_clear(&icfg, ((irq % 16U) * 2U) + 1U);
		} else {
			bitmap_set(&icfg, ((irq % 16U) * 2U) + 1U);
		}

		atomic_store_relaxed(
			&gicr->sgi.icfgr_e[(irq - GIC_PPI_EXT_BASE) / 16U],
			(uint32_t)icfg);

		if (enabled) {
			gicv3_irq_enable_percpu(irq, cpu);
		}

		// Read back the value in case it could not be changed
		icfg = (register_t)atomic_load_relaxed(
			&gicr->sgi.icfgr_e[(irq - GIC_PPI_EXT_BASE) / 16U]);
		ret = irq_trigger_result_ok(
			bitmap_isset(&icfg, ((irq % 16U) * 2U) + 1U)
				? IRQ_TRIGGER_EDGE_RISING
				: IRQ_TRIGGER_LEVEL_HIGH);

		break;
	}
#endif

	case GICV3_IRQ_TYPE_SGI:
	case GICV3_IRQ_TYPE_SPI:
#if GICV3_EXT_IRQS
	case GICV3_IRQ_TYPE_SPI_EXT:
#endif
#if GICV3_HAS_LPI
	case GICV3_IRQ_TYPE_LPI:
#endif
	case GICV3_IRQ_TYPE_SPECIAL:
	case GICV3_IRQ_TYPE_RESERVED:
	default:
		// No action required as irq is not handled.
		ret = irq_trigger_result_error(ERROR_UNIMPLEMENTED);
		break;
	}

	spinlock_release(&bitmap_update_lock);

end_function:
	return ret;
}

irq_trigger_result_t
gicv3_irq_set_trigger_shared(irq_t irq, irq_trigger_t trigger)
{
	irq_trigger_result_t ret;

	assert(irq <= gicv3_irq_max());

	spinlock_acquire(&bitmap_update_lock);

	switch (gicv3_get_irq_type(irq)) {
	case GICV3_IRQ_TYPE_SPI: {
		// Disable the interrupt if it is already enabled
		uint32_t isenabler = atomic_load_relaxed(
			&gicd->isenabler[GICD_ENABLE_GET_N(irq)]);
		bool enabled = (isenabler & GIC_ENABLE_BIT(irq)) != 0U;

		if (enabled) {
			gicv3_irq_disable_shared(irq);
		}

		register_t icfg =
			(register_t)atomic_load_relaxed(&gicd->icfgr[irq / 16]);

		if ((trigger == IRQ_TRIGGER_LEVEL_HIGH) ||
		    (trigger == IRQ_TRIGGER_LEVEL_LOW)) {
			bitmap_clear(&icfg, ((irq % 16U) * 2U) + 1U);
		} else {
			bitmap_set(&icfg, ((irq % 16U) * 2U) + 1U);
		}

		atomic_store_relaxed(&gicd->icfgr[irq / 16], (uint32_t)icfg);

		if (enabled) {
			gicv3_irq_enable_shared(irq);
		}

		// Read back the value in case it could not be changed
		icfg = atomic_load_relaxed(&gicd->icfgr[irq / 16]);
		ret  = irq_trigger_result_ok(
			 bitmap_isset(&icfg, ((irq % 16U) * 2U) + 1U)
				 ? IRQ_TRIGGER_EDGE_RISING
				 : IRQ_TRIGGER_LEVEL_HIGH);

		break;
	}
#if GICV3_EXT_IRQS
	case GICV3_IRQ_TYPE_SPI_EXT: {
		// Disable the interrupt if it is already enabled
		uint32_t isenabler_e = atomic_load_relaxed(
			&gicd->isenabler_e[GICD_ENABLE_GET_N(
				irq - GIC_SPI_EXT_BASE)]);
		bool enabled = (isenabler_e &
				GIC_ENABLE_BIT(irq - GIC_SPI_EXT_BASE)) != 0U;

		if (enabled) {
			gicv3_irq_disable_shared(irq);
		}

		register_t icfg = (register_t)atomic_load_relaxed(
			&gicd->icfgr_e[(irq - GIC_SPI_EXT_BASE) / 16U]);

		if ((trigger == IRQ_TRIGGER_LEVEL_HIGH) ||
		    (trigger == IRQ_TRIGGER_LEVEL_LOW)) {
			bitmap_clear(&icfg, ((irq % 16U) * 2U) + 1U);
		} else {
			bitmap_set(&icfg, ((irq % 16U) * 2U) + 1U);
		}

		atomic_store_relaxed(
			&gicd->icfgr_e[(irq - GIC_SPI_EXT_BASE) / 16U],
			(uint32_t)icfg);

		if (enabled) {
			gicv3_irq_enable_shared(irq);
		}

		// Read back the value in case it could not be changed
		icfg = (register_t)atomic_load_relaxed(
			&gicd->icfgr_e[(irq - GIC_SPI_EXT_BASE) / 16U]);
		ret = irq_trigger_result_ok(
			bitmap_isset(&icfg, ((irq % 16U) * 2U) + 1U)
				? IRQ_TRIGGER_EDGE_RISING
				: IRQ_TRIGGER_LEVEL_HIGH);

		break;
	}
#endif
#if GICV3_HAS_LPI
	case GICV3_IRQ_TYPE_LPI:
		// LPIs are always message-signalled
		ret = irq_trigger_result_ok(IRQ_TRIGGER_MESSAGE);
		break;
#endif
	case GICV3_IRQ_TYPE_SGI:
	case GICV3_IRQ_TYPE_PPI:
#if GICV3_EXT_IRQS
	case GICV3_IRQ_TYPE_PPI_EXT:
#endif
	case GICV3_IRQ_TYPE_SPECIAL:
	case GICV3_IRQ_TYPE_RESERVED:
	default:
		// No action required as irq is not handled.
		ret = irq_trigger_result_error(ERROR_UNIMPLEMENTED);
		break;
	}

	spinlock_release(&bitmap_update_lock);

	return ret;
}

static error_t
gicv3_spi_set_route_internal(irq_t irq, GICD_IROUTER_t route)
	REQUIRE_SPINLOCK(spi_route_lock)
{
	error_t ret;

	assert_preempt_disabled();

	{
		switch (gicv3_get_irq_type(irq)) {
		case GICV3_IRQ_TYPE_SPI:
			atomic_store_relaxed(&gicd->irouter[irq - GIC_SPI_BASE],
					     route);
			ret = OK;
			break;
#if GICV3_EXT_IRQS
		case GICV3_IRQ_TYPE_SPI_EXT:
			atomic_store_relaxed(
				&gicd->irouter_e[irq - GIC_SPI_EXT_BASE],
				route);
			ret = OK;
			break;
#endif
		case GICV3_IRQ_TYPE_SGI:
		case GICV3_IRQ_TYPE_PPI:
#if GICV3_EXT_IRQS
		case GICV3_IRQ_TYPE_PPI_EXT:
#endif
#if GICV3_HAS_LPI
		case GICV3_IRQ_TYPE_LPI:
#endif
		case GICV3_IRQ_TYPE_SPECIAL:
		case GICV3_IRQ_TYPE_RESERVED:
		default:
			ret = ERROR_ARGUMENT_INVALID;
			break;
		}
	}

	return ret;
}

error_t
gicv3_spi_set_route(irq_t irq, GICD_IROUTER_t route)
{
	error_t ret = ERROR_ARGUMENT_INVALID;

	spinlock_acquire(&spi_route_lock);

	// If the SPI is enabled and routes to a specific CPU we need to check
	// it is online. The route is also checked when the SPI is enabled.
	// If using 1:N routing, the GIC decides which CPU should get it.
	if (!GICD_IROUTER_get_IRM(&route)) {
		cpu_index_t cpu;

		// Determine the target CPU for the route
		if (!gicv3_spi_get_route_cpu_affinity(&route, &cpu)) {
			goto out;
		}

		// If the interrupt is enabled check the target CPU is online
		if (gicv3_spi_is_enabled(irq)) {
			gicr_cpu_t *gc = &CPULOCAL_BY_INDEX(gicr_cpu, cpu);

			// If the target CPU is offline adjust the route
			if (!gc->online) {
				gc = &CPULOCAL(gicr_cpu);
				assert(gc->online);
				gicv3_spi_set_route_cpu_affinity(
					&route, cpulocal_get_index());
			}
		}
	}

	ret = gicv3_spi_set_route_internal(irq, route);

out:
	spinlock_release(&spi_route_lock);
	return ret;
}

#if GICV3_HAS_GICD_ICLAR
error_t
gicv3_spi_set_classes(irq_t irq, bool class0, bool class1)
{
	error_t ret;

	spinlock_acquire(&bitmap_update_lock);

	switch (gicv3_get_irq_type(irq)) {
	case GICV3_IRQ_TYPE_SPI: {
		register_t iclar =
			(register_t)atomic_load_relaxed(&gicd->iclar[irq / 16]);

		if (class0) {
			bitmap_clear(&iclar, (irq % 16U) * 2U);
		} else {
			bitmap_set(&iclar, (irq % 16U) * 2U);
		}

		if (class1) {
			bitmap_clear(&iclar, ((irq % 16U) * 2U) + 1U);
		} else {
			bitmap_set(&iclar, ((irq % 16U) * 2U) + 1U);
		}

		// This must be a store-release to ensure that it takes effect
		// after any preceding write to GICD_IROUTER<irq>, since these
		// bits are RAZ/WI until GICD_IROUTER<irq>.IRM is set.
		atomic_store_release(&gicd->iclar[irq / 16], (uint32_t)iclar);

		ret = OK;
		break;
	}
#if GICV3_EXT_IRQS
	case GICV3_IRQ_TYPE_SPI_EXT: {
		register_t iclar = (register_t)atomic_load_relaxed(
			&gicd->iclar_e[(irq - GIC_SPI_EXT_BASE) / 16]);

		if (class0) {
			bitmap_clear(&iclar, (irq % 16U) * 2U);
		} else {
			bitmap_set(&iclar, (irq % 16U) * 2U);
		}

		if (class1) {
			bitmap_clear(&iclar, ((irq % 16U) * 2U) + 1U);
		} else {
			bitmap_set(&iclar, ((irq % 16U) * 2U) + 1U);
		}

		// This must be a store-release to ensure that it takes effect
		// after any preceding write to GICD_IROUTER<irq>E, since these
		// bits are RAZ/WI until GICD_IROUTER<irq>E.IRM is set.
		atomic_store_release(
			&gicd->iclar_e[(irq - GIC_SPI_EXT_BASE) / 16],
			(uint32_t)iclar);

		ret = OK;
		break;
	}
#endif
	case GICV3_IRQ_TYPE_SGI:
	case GICV3_IRQ_TYPE_PPI:
#if GICV3_EXT_IRQS
	case GICV3_IRQ_TYPE_PPI_EXT:
#endif
#if GICV3_HAS_LPI
	case GICV3_IRQ_TYPE_LPI:
#endif
	case GICV3_IRQ_TYPE_SPECIAL:
	case GICV3_IRQ_TYPE_RESERVED:
	default:
		ret = ERROR_ARGUMENT_INVALID;
		break;
	}

	spinlock_release(&bitmap_update_lock);

	return ret;
}
#endif

irq_result_t
gicv3_irq_acknowledge(void)
{
	irq_result_t ret = { 0 };

	ICC_IAR_EL1_t iar =
		register_ICC_IAR1_EL1_read_volatile_ordered(&asm_ordering);

	uint32_t intid = ICC_IAR_EL1_get_INTID(&iar);

	// 1023 is returned if there is no pending interrupt with sufficient
	// priority for it to be signaled to the PE, or if the highest priority
	// pending interrupt is not appropriate for the current security state
	// or interrupt group that is associated with the System register.
	if (intid == 1023U) {
		ret.e = ERROR_IDLE;
		goto error;
	}

	// Ensure distributor has activated the interrupt before prio drop
	__asm__ volatile("isb; dsb sy" : "+m"(asm_ordering));

	if (gicv3_get_irq_type(intid) == GICV3_IRQ_TYPE_SGI) {
		gicv3_irq_priority_drop(intid);
#if PLATFORM_IPI_LINES > ENUM_IPI_REASON_MAX_VALUE
		assert(intid <= ENUM_IPI_REASON_MAX_VALUE);
		trigger_platform_ipi_event((ipi_reason_t)intid);
#else
		(void)trigger_platform_ipi_event();
#endif
		gicv3_irq_deactivate(intid);
		ret.e = ERROR_RETRY;
	} else {
		ret.e = OK;
		ret.r = intid;
	}

error:
	return ret;
}

void
gicv3_irq_priority_drop(irq_t irq)
{
	assert(irq <= gicv3_irq_max());

	ICC_EOIR_EL1_t eoir = ICC_EOIR_EL1_default();

	ICC_EOIR_EL1_set_INTID(&eoir, irq);

	// No need for a barrier here: nothing we do to handle this IRQ
	// before the priority drop will affect whether we get a different
	// IRQ after the drop.

	register_ICC_EOIR1_EL1_write_ordered(eoir, &asm_ordering);
}

void
gicv3_irq_deactivate(irq_t irq)
{
	assert(irq <= gicv3_irq_max());

#if GICV3_HAS_LPI
	if (irq >= GIC_LPI_BASE) {
		// Deactivation is meaningless for LPIs (apart from the priority
		// drop which is done separately), so skip the barriers and
		// register write
	} else
#endif
	{
		ICC_DIR_EL1_t dir = ICC_DIR_EL1_default();

		ICC_DIR_EL1_set_INTID(&dir, irq);

		// Ensure interrupt handling is complete
		__asm__ volatile("dsb sy; isb" ::: "memory");

		register_ICC_DIR_EL1_write_ordered(dir, &asm_ordering);
	}
}

void
gicv3_irq_deactivate_percpu(irq_t irq, cpu_index_t cpu)
{
	gicr_t *gicr = CPULOCAL_BY_INDEX(gicr_cpu, cpu).gicr;

	if (gicr == NULL) {
		LOG(DEBUG, INFO, "gicr is NULL for cpu(%u):\n", cpu);
		goto out;
	}

	switch (gicv3_get_irq_type(irq)) {
	case GICV3_IRQ_TYPE_SGI:
	case GICV3_IRQ_TYPE_PPI: {
		atomic_store_relaxed(&gicr->sgi.icactiver0,
				     GIC_ENABLE_BIT(irq));
		break;
	}
#if GICV3_EXT_IRQS
	case GICV3_IRQ_TYPE_PPI_EXT: {
		// Extended PPI
		atomic_store_relaxed(&gicr->sgi.icactiver_e[GICD_ENABLE_GET_N(
					     irq - GIC_PPI_EXT_BASE)],
				     GIC_ENABLE_BIT(irq - GIC_PPI_EXT_BASE));
		break;
	}
#endif
	case GICV3_IRQ_TYPE_SPI:
#if GICV3_EXT_IRQS
	case GICV3_IRQ_TYPE_SPI_EXT:
#endif
#if GICV3_HAS_LPI
	case GICV3_IRQ_TYPE_LPI:
#endif
	case GICV3_IRQ_TYPE_SPECIAL:
	case GICV3_IRQ_TYPE_RESERVED:
	default:
		panic("Incorrect IRQ type");
	}

out:
	return;
}

#if PLATFORM_IPI_LINES > ENUM_IPI_REASON_MAX_VALUE
void
platform_ipi_others(ipi_reason_t ipi)
{
	ICC_SGIR_EL1_t sgir = ICC_SGIR_EL1_default();
	ICC_SGIR_EL1_set_IRM(&sgir, true);
	ICC_SGIR_EL1_set_INTID(&sgir, (irq_t)ipi);

	__asm__ volatile("dsb sy; isb" ::: "memory");

	register_ICC_SGI1R_EL1_write_ordered(sgir, &asm_ordering);
}

void
platform_ipi_one(ipi_reason_t ipi, cpu_index_t cpu)
{
	assert((ipi < GIC_SGI_NUM) && cpulocal_index_valid(cpu));

	ICC_SGIR_EL1_t sgir = CPULOCAL_BY_INDEX(gicr_cpu, cpu).icc_sgi1r;
	ICC_SGIR_EL1_set_INTID(&sgir, (irq_t)ipi);

	__asm__ volatile("dsb sy; isb" ::: "memory");

	register_ICC_SGI1R_EL1_write_ordered(sgir, &asm_ordering);
}

void
platform_ipi_clear(ipi_reason_t ipi)
{
	(void)ipi;
}

void
platform_ipi_mask(ipi_reason_t ipi)
{
	gicv3_irq_disable_shared((irq_t)ipi);
}

void
platform_ipi_unmask(ipi_reason_t ipi)
{
	gicv3_irq_enable_shared((irq_t)ipi);
}
#else
void
platform_ipi_others(void)
{
	ICC_SGIR_EL1_t sgir = ICC_SGIR_EL1_default();
	ICC_SGIR_EL1_set_IRM(&sgir, true);
	ICC_SGIR_EL1_set_INTID(&sgir, 0U);

	__asm__ volatile("dsb sy; isb" ::: "memory");

	register_ICC_SGI1R_EL1_write_ordered(sgir, &asm_ordering);
}

void
platform_ipi_one(cpu_index_t cpu)
{
	assert(cpulocal_index_valid(cpu));

	ICC_SGIR_EL1_t sgir = CPULOCAL_BY_INDEX(gicr_cpu, cpu).icc_sgi1r;
	ICC_SGIR_EL1_set_INTID(&sgir, 0U);

	__asm__ volatile("dsb sy; isb" ::: "memory");

	register_ICC_SGI1R_EL1_write_ordered(sgir, &asm_ordering);
}
#endif

#if defined(INTERFACE_VCPU) && INTERFACE_VCPU && GICV3_HAS_1N

error_t
gicv3_handle_vcpu_poweron(thread_t *vcpu)
{
	if (vcpu_option_flags_get_hlos_vm(&vcpu->vcpu_options)) {
		cpu_index_t cpu = scheduler_get_affinity(vcpu);

		// Enable 1-of-N targeting to the VCPU's physical CPU.
		//
		// No locking, because we assume that the hlos_vm flag is only
		// set on one VCPU per physical CPU. Also we are assuming here
		// that DPGs are implemented.
		gicr_t	   *gicr      = CPULOCAL_BY_INDEX(gicr_cpu, cpu).gicr;
		GICR_CTLR_t gicr_ctlr = atomic_load_relaxed(&gicr->rd.ctlr);
		GICR_CTLR_set_DPG1NS(&gicr_ctlr, false);
		atomic_store_relaxed(&gicr->rd.ctlr, gicr_ctlr);
	}

	return OK;
}

error_t
gicv3_handle_vcpu_poweroff(thread_t *vcpu)
{
	if (vcpu_option_flags_get_hlos_vm(&vcpu->vcpu_options)) {
		cpu_index_t cpu = scheduler_get_affinity(vcpu);

		// Disable 1-of-N targeting to the VCPU's physical CPU.
		//
		// No locking, because we assume that the hlos_vm flag is only
		// set on one VCPU per physical CPU. Also we are assuming here
		// that DPGs are implemented.
		gicr_t	   *gicr      = CPULOCAL_BY_INDEX(gicr_cpu, cpu).gicr;
		GICR_CTLR_t gicr_ctlr = atomic_load_relaxed(&gicr->rd.ctlr);
		GICR_CTLR_set_DPG1NS(&gicr_ctlr, true);
		atomic_store_relaxed(&gicr->rd.ctlr, gicr_ctlr);
	}

	return OK;
}

#endif // INTERFACE_VCPU && GICV3_HAS_1N

#if GICV3_HAS_LPI
const irq_t platform_irq_msi_base = GIC_LPI_BASE;

irq_t
platform_irq_msi_max(void)
{
	return gicv3_lpi_max_cache;
}

#if !GICV3_HAS_ITS || GICV3_HAS_VLPI_V4_1
void
gicv3_lpi_inv_by_id(cpu_index_t cpu, irq_t lpi)
{
	gicr_t *gicr = CPULOCAL_BY_INDEX(gicr_cpu, cpu).gicr;

	GICR_INVLPIR_t invlpir = GICR_INVLPIR_default();
	GICR_INVLPIR_set_pINTID(&invlpir, lpi);
	atomic_store_release(&gicr->rd.invlpir, invlpir);
}

// This is an ITS function for GICv3 with ITS or GICv4.0, and a GICR function
// for GICv3 without ITS or GICv4.1. See gicv3_its.c for the ITS version.
void
gicv3_lpi_inv_all(cpu_index_t cpu)
{
	gicr_t *gicr = CPULOCAL_BY_INDEX(gicr_cpu, cpu).gicr;

	GICR_INVALLR_t invallr = GICR_INVALLR_default();
	atomic_store_release(&gicr->rd.invallr, invallr);
}

// This is an ITS function for GICv3 with ITS or GICv4.0, and a GICR function
// for GICv3 without ITS or GICv4.1. See gicv3_its.c for the ITS version.
bool
gicv3_lpi_inv_pending(cpu_index_t cpu)
{
	gicr_t	    *gicr  = CPULOCAL_BY_INDEX(gicr_cpu, cpu).gicr;
	GICR_SYNCR_t syncr = atomic_load_acquire(&gicr->rd.syncr);
	return GICR_SYNCR_get_Busy(&syncr);
}
#endif // !GICV3_HAS_ITS || GICV3_HAS_VLPI_V4_1

#if defined(GICV3_ENABLE_VPE) && GICV3_ENABLE_VPE

#if GICV3_HAS_VLPI_V4_1
void
gicv3_vlpi_inv_by_id(thread_t *vcpu, virq_t vlpi)
{
	scheduler_lock(vcpu);
	if ((vcpu->gicv3_its_doorbell != NULL) &&
	    (vcpu->scheduler_affinity < PLATFORM_MAX_CORES)) {
		gicr_t *gicr =
			CPULOCAL_BY_INDEX(gicr_cpu, vcpu->scheduler_affinity)
				.gicr;

		GICR_INVLPIR_t invlpir = GICR_INVLPIR_default();
		GICR_INVLPIR_set_V(&invlpir, true);
		GICR_INVLPIR_set_vPEID(&invlpir, vcpu->gicv3_its_vpe_id);
		GICR_INVLPIR_set_pINTID(&invlpir, vlpi);
		atomic_store_release(&gicr->rd.invlpir, invlpir);
	}
	scheduler_unlock(vcpu);
}

void
gicv3_vlpi_inv_all(thread_t *vcpu)
{
	scheduler_lock(vcpu);
	if ((vcpu->gicv3_its_doorbell != NULL) &&
	    (vcpu->scheduler_affinity < PLATFORM_MAX_CORES)) {
		gicr_t *gicr =
			CPULOCAL_BY_INDEX(gicr_cpu, vcpu->scheduler_affinity)
				.gicr;

		GICR_INVALLR_t invallr = GICR_INVALLR_default();
		GICR_INVALLR_set_V(&invallr, true);
		GICR_INVALLR_set_vPEID(&invallr, vcpu->gicv3_its_vpe_id);
		atomic_store_release(&gicr->rd.invallr, invallr);
	}
	scheduler_unlock(vcpu);
}

bool
gicv3_vlpi_inv_pending(thread_t *vcpu)
{
	bool busy = false;

	scheduler_lock(vcpu);
	if ((vcpu->gicv3_its_doorbell != NULL) &&
	    (vcpu->scheduler_affinity < PLATFORM_MAX_CORES)) {
		gicr_t *gicr =
			CPULOCAL_BY_INDEX(gicr_cpu, vcpu->scheduler_affinity)
				.gicr;
		GICR_SYNCR_t syncr = atomic_load_acquire(&gicr->rd.syncr);
		busy		   = GICR_SYNCR_get_Busy(&syncr);
	}
	scheduler_unlock(vcpu);

	return busy;
}

void
gicv3_vpe_schedule(bool enable_group0, bool enable_group1)
{
	// Current thread must be a mapped VCPU
	thread_t *current = thread_get_self();
	assert(current != NULL);
	assert_preempt_disabled();

	if (cpulocal_index_valid(current->gicv3_its_mapped_cpu)) {
		assert(cpulocal_get_index() == current->gicv3_its_mapped_cpu);
		gicr_t *gicr = CPULOCAL(gicr_cpu).gicr;
		assert(gicr != NULL);

		// Wait until the GICR has finished descheduling any previous
		// vPE. Note that we only wait in deschedule if we're enabling
		// the doorbell.
		gicv3_vpe_sync_deschedule(cpulocal_get_index(), false);

		// Write the new valid VPENDBASER
		GICR_VPENDBASER_t vpendbaser = GICR_VPENDBASER_default();
		GICR_VPENDBASER_set_vPEID(&vpendbaser,
					  current->gicv3_its_vpe_id);
		GICR_VPENDBASER_set_vGrp1En(&vpendbaser, enable_group1);
		GICR_VPENDBASER_set_vGrp0En(&vpendbaser, enable_group0);
		// Note: PendingLast is RES1 when setting Valid
		GICR_VPENDBASER_set_PendingLast(&vpendbaser, true);
		GICR_VPENDBASER_set_Valid(&vpendbaser, true);
		atomic_store_relaxed(&gicr->vlpi.vpendbaser, vpendbaser);

		TRACE(DEBUG, INFO,
		      "gicv3_vpe_schedule: {:#x} -> vpendbase {:#x}",
		      (uintptr_t)current, GICR_VPENDBASER_raw(vpendbaser));
	}
}

bool
gicv3_vpe_deschedule(bool enable_doorbell)
{
	bool wakeup = false;

	// Current thread must be a mapped VCPU
	thread_t *current = thread_get_self();
	assert(current != NULL);
	assert_preempt_disabled();

	if (cpulocal_index_valid(current->gicv3_its_mapped_cpu)) {
		assert(cpulocal_get_index() == current->gicv3_its_mapped_cpu);
		gicr_t *gicr = CPULOCAL(gicr_cpu).gicr;
		assert(gicr != NULL);

		// Wait until the pending table has been parsed after any
		// previous vPE scheduling operation, which could have been very
		// recent. The GICR appears to reject deschedule operations if
		// this is not done.
		GICR_VPENDBASER_t vpendbaser;
		do {
			vpendbaser =
				atomic_load_relaxed(&gicr->vlpi.vpendbaser);
			assert(GICR_VPENDBASER_get_Valid(&vpendbaser));
		} while (GICR_VPENDBASER_get_Dirty(&vpendbaser));

		// Write an invalid VPENDBASER.
		GICR_VPENDBASER_set_Valid(&vpendbaser, false);
		GICR_VPENDBASER_set_Doorbell(&vpendbaser, enable_doorbell);
		// If we are not enabling the doorbell, write PendingLast as 1
		// to tell the GICR that we don't care, so the GICR can avoid
		// wasting time calculating it.
		GICR_VPENDBASER_set_PendingLast(&vpendbaser, !enable_doorbell);
		atomic_store_relaxed(&gicr->vlpi.vpendbaser, vpendbaser);

		TRACE(DEBUG, INFO,
		      "gicv3_vpe_deschedule: {:#x} -> vpendbase {:#x}",
		      (uintptr_t)current, GICR_VPENDBASER_raw(vpendbaser));

		if (enable_doorbell) {
			// Read back VPENDBASER to get PendingLast which
			// indicates that a VLPI or VSGI is already pending
			// (which will suppress the doorbell LPI, so we must
			// wake the thread now).
			//
			// This could be deferred to reduce the time spent in
			// the loop, but it must be done before the new thread's
			// ICH_MDCR_EL2 is loaded, so PendingLast calculation
			// uses this thread's group enable bits.
			do {
				vpendbaser = atomic_load_relaxed(
					&gicr->vlpi.vpendbaser);
				assert(!GICR_VPENDBASER_get_Valid(&vpendbaser));
			} while (GICR_VPENDBASER_get_Dirty(&vpendbaser));
			wakeup = GICR_VPENDBASER_get_PendingLast(&vpendbaser);

			if (wakeup) {
				current->gicv3_its_need_wakeup_check = true;
			}
		}
	}

	return wakeup;
}

bool
gicv3_vpe_check_wakeup(bool retry_trap)
{
	thread_t *current    = thread_get_self();
	bool	  might_wake = false;
	assert(current->kind == THREAD_KIND_VCPU);

	cpulocal_begin();

	if (cpulocal_index_valid(current->gicv3_its_mapped_cpu) &&
	    current->gicv3_its_need_wakeup_check) {
		// The VCPU is scheduled in the GICR. Check whether the
		// scheduling operation has finished; if so, we won't need to
		// exit next time we get here.
		assert(cpulocal_get_index() == current->gicv3_its_mapped_cpu);
		gicr_t *gicr = CPULOCAL(gicr_cpu).gicr;
		assert(gicr != NULL);
		GICR_VPENDBASER_t vpendbaser =
			atomic_load_relaxed(&gicr->vlpi.vpendbaser);
		TRACE(DEBUG, INFO,
		      "gicv3_vpe_check_wakeup: may wakeup, vpendbase {:#x}, retry {:d}",
		      GICR_VPENDBASER_raw(vpendbaser), (register_t)retry_trap);
		assert(GICR_VPENDBASER_get_Valid(&vpendbaser));
		if (!GICR_VPENDBASER_get_Dirty(&vpendbaser)) {
			__asm__ volatile("dsb ish" ::: "memory");
			current->gicv3_its_need_wakeup_check = false;
		}

		if (retry_trap || GICR_VPENDBASER_get_Dirty(&vpendbaser)) {
			might_wake = true;
		}
	} else {
		TRACE(DEBUG, INFO, "gicv3_vpe_check_wakeup: no wakeup");
	}

	cpulocal_end();

	return might_wake;
}

void
gicv3_vpe_sync_deschedule(cpu_index_t cpu, bool maybe_scheduled)
{
	assert_cpulocal_safe();
	gicr_t *gicr = CPULOCAL_BY_INDEX(gicr_cpu, cpu).gicr;
	assert(gicr != NULL);

	GICR_VPENDBASER_t vpendbaser;
	do {
		vpendbaser = atomic_load_relaxed(&gicr->vlpi.vpendbaser);
		TRACE(DEBUG, INFO, "gicv3_vpe_sync_deschedule: vpendbase {:#x}",
		      GICR_VPENDBASER_raw(vpendbaser));
		assert(maybe_scheduled ||
		       !GICR_VPENDBASER_get_Valid(&vpendbaser));
	} while (!GICR_VPENDBASER_get_Valid(&vpendbaser) &&
		 GICR_VPENDBASER_get_Dirty(&vpendbaser));
}

bool
gicv3_vpe_handle_irq_received_doorbell(hwirq_t *hwirq)
{
	thread_t *vcpu = atomic_load_consume(&hwirq->gicv3_its_vcpu);

	scheduler_lock(vcpu);
	vcpu->gicv3_its_need_wakeup_check = true;
	vcpu_wakeup(vcpu);
	scheduler_unlock(vcpu);

	return true;
}

uint32_result_t
gicv3_vpe_vsgi_query(thread_t *vcpu)
{
	uint32_result_t ret;
	cpu_index_t	cpu = vcpu->gicv3_its_mapped_cpu;

	if (cpulocal_index_valid(cpu)) {
		gicr_t	   *gicr = CPULOCAL_BY_INDEX(gicr_cpu, cpu).gicr;
		spinlock_t *lock =
			&CPULOCAL_BY_INDEX(gicr_cpu, cpu).vsgi_query_lock;

		spinlock_acquire(lock);
		GICR_VSGIPENDR_t pend;
#if !defined(NDEBUG)
		pend = atomic_load_acquire(&gicr->vlpi.vsgipendr);
		assert(!GICR_VSGIPENDR_get_Busy(&pend));
#endif
		GICR_VSGIR_t vsgir = GICR_VSGIR_default();
		GICR_VSGIR_set_vPEID(&vsgir, vcpu->gicv3_its_vpe_id);
		atomic_store_release(&gicr->vlpi.vsgir, vsgir);
		do {
			pend = atomic_load_relaxed(&gicr->vlpi.vsgipendr);
		} while (GICR_VSGIPENDR_get_Busy(&pend));
		spinlock_release(lock);

		ret = uint32_result_ok(GICR_VSGIPENDR_get_Pending(&pend));
	} else {
		ret = uint32_result_error(ERROR_IDLE);
	}

	return ret;
}

#elif GICV3_HAS_VLPI // && !GICV3_HAS_VLPI_V4_1
#error VPE scheduling is not implemented for GICv4.0
#endif // GICV3_HAS_VLPI && !GICV3_HAS_VLPI_V4_1

#endif // defined(GICV3_ENABLE_VPE) && GICV3_ENABLE_VPE

static void
gicv3_lpi_disable_all(void) REQUIRE_PREEMPT_DISABLED
{
#if defined(GICV3_ENABLE_VPE) && GICV3_ENABLE_VPE
	// Deschedule the current vPE, if any, and discard wakeups.
	(void)gicv3_vpe_deschedule(false);
#endif

#if defined(GICV3_ENABLE_VPE) && GICV3_ENABLE_VPE
	for (cpu_index_t cpu = 0U; cpu < PLATFORM_MAX_CORES; cpu++) {
		if (CPULOCAL_BY_INDEX(gicr_cpu, cpu).gicr == NULL) {
			// No GICR for this CPU index, skip it
			continue;
		}

		// Ensure that the GICR has finished descheduling the last vPE
		gicv3_vpe_sync_deschedule(cpu, false);
	}
#endif

	for (index_t i = 0U; i < PLATFORM_GICR_COUNT; i++) {
		gicr_t *gicr = mapped_gicrs[i];

		// Disable LPIs and clear the vPE table base
		GICR_CTLR_t gicr_ctlr = atomic_load_relaxed(&gicr->rd.ctlr);
		GICR_CTLR_set_Enable_LPIs(&gicr_ctlr, false);
		atomic_store_relaxed(&gicr->rd.ctlr, gicr_ctlr);
#if defined(GICV3_ENABLE_VPE) && GICV3_ENABLE_VPE
		atomic_store_relaxed(&gicr->vlpi.vpropbaser,
				     GICR_VPROPBASER_default());
#endif

		// Wait for the GICR to finish disabling LPIs before clearing
		// the PROPBASE and PENDBASE registers
		gicr_wait_for_write(gicr);

		atomic_store_relaxed(&gicr->rd.propbaser,
				     GICR_PROPBASER_default());
		atomic_store_relaxed(&gicr->rd.pendbaser,
				     GICR_PENDBASER_default());
	}
}
#endif // GICV3_HAS_LPI

void
gicv3_handle_boot_hypervisor_handover(void)
{
	// Disable group 1 interrupts on the local CPU
	ICC_IGRPEN_EL1_t icc_grpen1 = ICC_IGRPEN_EL1_default();
	ICC_IGRPEN_EL1_set_Enable(&icc_grpen1, false);
	register_ICC_IGRPEN1_EL1_write_ordered(icc_grpen1, &asm_ordering);

	// Disable affinity-routed group 1 interrupts at the distributor
	GICD_CTLR_t ctlr = atomic_load_relaxed(&gicd->ctlr);
	GICD_CTLR_NS_set_EnableGrp1A(&ctlr.ns, false);
	atomic_store_relaxed(&gicd->ctlr, ctlr);
	(void)gicd_wait_for_write();

	GICD_TYPER_t typer	= atomic_load_relaxed(&gicd->typer);
	count_t	     spi_ranges = GICD_TYPER_get_ITLinesNumber(&typer) + 1U;

	// Deactivate and clear edge pending for all SPIs
	for (index_t i = 1U; i < spi_ranges; i++) {
		atomic_store_relaxed(&gicd->icactiver[i], 0xffffffffU);
		atomic_store_relaxed(&gicd->icpendr[i], 0xffffffffU);
	}

	// Deactivate and clear edge pending for PPIs on all GICRs
	for (index_t i = 0U; i < PLATFORM_GICR_COUNT; i++) {
		gicr_t *gicr = mapped_gicrs[i];
		atomic_store_relaxed(&gicr->sgi.icactiver0, 0xffffffffU);
		atomic_store_relaxed(&gicr->sgi.icpendr0, 0xffffffffU);
	}

#if GICV3_EXT_IRQS
	count_t espi_ranges;
	if (GICD_TYPER_get_ESPI(&typer)) {
		espi_ranges = GICD_TYPER_get_ESPI_range(&typer) + 1U;
		gicv3_spi_ext_max_cache =
			GIC_SPI_EXT_BASE + (32U * espi_ranges) - 1U;
	} else {
		espi_ranges		= 0U;
		gicv3_spi_ext_max_cache = 0U;
	}

	// Deactivate and clear edge pending for all extended SPIs
	for (index_t i = 0U; i < espi_ranges; i++) {
		atomic_store_relaxed(&gicd->icactiver_e[i], 0xffffffffU);
		atomic_store_relaxed(&gicd->icpendr_e[i], 0xffffffffU);
	}

	GICR_TYPER_t gicr_typer =
		atomic_load_relaxed(&mapped_gicrs[0]->rd.typer);
	GICR_TYPER_PPInum_t eppi = GICR_TYPER_get_PPInum(&gicr_typer);

	count_t eppi_ranges;
	switch (eppi) {
	case GICR_TYPER_PPINUM_MAX_1087:
		eppi_ranges = 1U;
		break;
	case GICR_TYPER_PPINUM_MAX_1119:
		eppi_ranges = 2U;
		break;
	case GICR_TYPER_PPINUM_MAX_31:
	default:
		eppi_ranges = 0U;
		break;
	}

	// Deactivate and clear edge pending for extended PPIs on all GICRs
	for (index_t i = 0U; i < PLATFORM_GICR_COUNT; i++) {
		gicr_t *gicr = mapped_gicrs[i];
		for (index_t j = 0U; j < eppi_ranges; j++) {
			atomic_store_relaxed(&gicr->sgi.icactiver_e[j],
					     0xffffffffU);
			atomic_store_relaxed(&gicr->sgi.icpendr_e[j],
					     0xffffffffU);
		}
	}
#endif

#if GICV3_HAS_LPI
	gicv3_lpi_disable_all();
#endif

#if GICV3_HAS_ITS
	gicv3_its_disable_all(&mapped_gitss);
#endif
}

void
gicv3_handle_power_cpu_online(void)
{
	gicr_cpu_t *gc = &CPULOCAL(gicr_cpu);

	// Mark this CPU as online and available for SPI route migration
	assert_preempt_disabled();
	spinlock_acquire_nopreempt(&spi_route_lock);
	gc->online = true;
	spinlock_release_nopreempt(&spi_route_lock);
}

// Tries to move 32 continuous SPIs non-extended or extended from irq_base to a
// specified CPU, given route route_cmp it's trying to move away from.
static void
gicv3_try_move_32_spis_to_cpu(cpu_index_t target, irq_t irq_base,
			      GICD_IROUTER_t route_cmp) REQUIRE_PREEMPT_DISABLED
{
	assert_preempt_disabled();
	// Take the SPI lock so we can search the route table safely
	spinlock_acquire_nopreempt(&spi_route_lock);
	// To advance 32 IRQs at a time, our base should start on a boundary
	assert(irq_base % 32 == 0);
	gicv3_irq_type_t irq_base_type = gicv3_get_irq_type(irq_base);
	uint32_t	 isenabler;
#if GICV3_EXT_IRQS
	assert(irq_base_type == GICV3_IRQ_TYPE_SPI ||
	       irq_base_type == GICV3_IRQ_TYPE_SPI_EXT);
	if (irq_base_type == GICV3_IRQ_TYPE_SPI) {
		isenabler = atomic_load_relaxed(
			&gicd->isenabler[GICD_ENABLE_GET_N(irq_base)]);
	} else {
		// irq_base_type == GICV3_IRQ_TYPE_SPI_EXT
		isenabler = atomic_load_relaxed(
			&gicd->isenabler_e[GICD_ENABLE_GET_N(
				irq_base - GIC_SPI_EXT_BASE)]);
	}
#else
	assert(irq_base_type == GICV3_IRQ_TYPE_SPI);
	isenabler = atomic_load_relaxed(
		&gicd->isenabler[GICD_ENABLE_GET_N(irq_base)]);
#endif

	GICD_IROUTER_t route = GICD_IROUTER_default();
	while (isenabler != 0U) {
		index_t i = compiler_ctz(isenabler);
		isenabler &= ~((index_t)util_bit(i));

		irq_t irq = irq_base + i;

		// Special or reserved IRQs should never be enabled
#if GICV3_EXT_IRQS
		if (irq_base_type == GICV3_IRQ_TYPE_SPI) {
			assert(gicv3_get_irq_type(irq) == GICV3_IRQ_TYPE_SPI);
			route = atomic_load_relaxed(
				&gicd->irouter[irq - GIC_SPI_BASE]);
		} else {
			// irq_base_type == GICV3_IRQ_TYPE_SPI_EXT
			route = atomic_load_relaxed(
				&gicd->irouter_e[irq - GIC_SPI_EXT_BASE]);
		}
#else
		assert(gicv3_get_irq_type(irq) == GICV3_IRQ_TYPE_SPI);
		route = atomic_load_relaxed(&gicd->irouter[irq - GIC_SPI_BASE]);
#endif
		if (GICD_IROUTER_is_equal(route_cmp, route)) {
			gicv3_spi_set_route_cpu_affinity(&route, target);
			error_t ret = gicv3_spi_set_route_internal(irq, route);
			if (ret != OK) {
				panic("Failed to move SPI");
			}
		}
	}
	spinlock_release_nopreempt(&spi_route_lock);
}

static bool
gicv3_try_move_spis_to_cpu(cpu_index_t target) REQUIRE_PREEMPT_DISABLED
{
	// Hold the spi_route_lock before scanning the SPI route table so it
	// does not change while we are working with it. Holding the lock also
	// prevents the target CPU from going offline while setting routes.

	bool	    moved = false;
	gicr_cpu_t *gc	  = &CPULOCAL_BY_INDEX(gicr_cpu, target);

	assert_preempt_disabled();

	if (gc->online) {
		// We have an online target CPU we can use. Search SPI routes
		// that need migration
		GICD_IROUTER_t route_cmp = GICD_IROUTER_default();

		// Take the SPI lock so we can search the route table safely
		spinlock_acquire_nopreempt(&spi_route_lock);
		// Create a route with our affinity to compare against
		// Ignore 1:N routes
		gicv3_spi_set_route_cpu_affinity(&route_cmp,
						 cpulocal_get_index());
		spinlock_release_nopreempt(&spi_route_lock);

		for (irq_t irq_base = GIC_SPI_BASE;
		     irq_base <= gicv3_spi_max_cache; irq_base += 32U) {
			gicv3_try_move_32_spis_to_cpu(target, irq_base,
						      route_cmp);
		}

#if GICV3_EXT_IRQS
		for (irq_t irq_base = GIC_SPI_EXT_BASE;
		     irq_base <= gicv3_spi_ext_max_cache; irq_base += 32U) {
			gicv3_try_move_32_spis_to_cpu(target, irq_base,
						      route_cmp);
		}
#endif
		moved = true;
	}

	return moved;
}

void
gicv3_handle_power_cpu_offline(void)
{
	// Migrate any SPIs routed to this CPU to another online CPU.
	// If the IRQ is mapped to the wrong CPU it will get fixed the next
	// time the IRQ occurs.

	// Try to move any SPI IRQs to the next CPU up from this one.
	// If this is the last CPU, wrap around
	cpu_index_t my_index = cpulocal_get_index();
	cpu_index_t target =
		(cpu_index_t)((my_index + 1U) % PLATFORM_MAX_CORES);
	gicr_cpu_t *gc = &CPULOCAL(gicr_cpu);

	assert_preempt_disabled();

	spinlock_acquire_nopreempt(&spi_route_lock);

	gc->online = false;

	spinlock_release_nopreempt(&spi_route_lock);

	// Find an online target CPU before we scan SPI routes
	bool found_target = false;
	while (!found_target) {
		if (platform_cpu_exists(target)) {
			if (gicv3_try_move_spis_to_cpu(target)) {
				found_target = true;
				break;
			}
		}

		// Try the next CPU
		target = (cpu_index_t)((target + 1U) % PLATFORM_MAX_CORES);
		if (target == my_index) {
			// we looped around without finding a target,
			// this should never happen.
			break;
		}
	}

	if (!found_target) {
		panic("Could not find target CPU for SPI migration");
	}
}

#if defined(UNIT_TESTS)
gicd_t *
gicv3_get_gicd_pointer(void)
{
	return gicd;
}
#endif

```

`hyp/platform/gicv3/templates/gich_lrs.h.tmpl`:

```tmpl
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

static inline ICH_LR_EL2_t
gicv3_read_ich_lr(index_t i, asm_ordering_dummy_t *ordering)
{
	ICH_LR_EL2_t ret;

	switch (i) {
#for lr in range(0, $CPU_GICH_LR_COUNT)
	case ${lr}U:
		ret.base = register_ICH_LR${lr}_EL2_base_read_ordered(ordering);
		break;
#end for
	default:
		panic("Out-of-range LR");
		break;
	}

	return ret;
}

static inline void
gicv3_write_ich_lr(index_t i, ICH_LR_EL2_t val,
		   asm_ordering_dummy_t *ordering) {
	switch (i) {
#for lr in range(0, $CPU_GICH_LR_COUNT)
	case ${lr}U:
		register_ICH_LR${lr}_EL2_base_write_ordered(val.base, ordering);
		break;
#end for
	default:
		panic("Out-of-range LR");
		break;
	}
}

static inline void
gicv3_read_ich_aprs(uint32_t *ap0rs, uint32_t *ap1rs) {
#for group in (0, 1)
#for i in range(0, $CPU_GICH_APR_COUNT)
	ap${group}rs[$i] = register_ICH_AP${group}R${i}_EL2_read();
#end for
#end for
}

static inline void
gicv3_write_ich_aprs(const uint32_t *ap0rs, const uint32_t *ap1rs) {
	asm_ordering_dummy_t apr_write_ordering;
#for group in (0, 1)
#for i in range(0, $CPU_GICH_APR_COUNT)
	register_ICH_AP${group}R${i}_EL2_write_ordered(ap${group}rs[${i}],
						       &apr_write_ordering);
#end for
#end for
}

#for group in (0, 1)
static inline void
gicv3_ich_ap${group}r_clear_highest(void) {
	uint32_t apr;
#for i in range(0, $CPU_GICH_APR_COUNT)
	apr = register_ICH_AP${group}R${i}_EL2_read();
        if (apr != 0U) {
		index_t bit = compiler_ctz(apr);
		apr &= ~util_bit(bit);
		register_ICH_AP${group}R${i}_EL2_write(apr);
		goto out;
        }
#end for
out:
	return;
}
#end for

```

`hyp/platform/psci_smc/aarch64/include/psci_smc_arch.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

static inline psci_ret_t
psci_smc_fn_call(psci_function_t fn, register_t arg_0, register_t arg_1,
		 register_t arg_2)
{
	smccc_function_id_t fn_id = smccc_function_id_default();
	smccc_function_id_set_is_fast(&fn_id, true);
	smccc_function_id_set_is_smc64(&fn_id, true);
	smccc_function_id_set_owner_id(&fn_id, SMCCC_OWNER_ID_STANDARD);
	smccc_function_id_set_function(&fn_id, (smccc_function_t)fn);

	uint64_t hyp_args[6] = { arg_0, arg_1, arg_2, 0, 0, 0 };
	uint64_t hyp_ret[4]  = { 0 };

	smccc_1_1_call(fn_id, &hyp_args, &hyp_ret, NULL, CLIENT_ID_HYP);

	return (psci_ret_t)hyp_ret[0];
}

static inline psci_ret_t
psci_smc_fn_call32(psci_function_t fn, uint32_t arg_0, uint32_t arg_1,
		   uint32_t arg_2)
{
	smccc_function_id_t fn_id = smccc_function_id_default();
	smccc_function_id_set_is_fast(&fn_id, true);
	smccc_function_id_set_is_smc64(&fn_id, false);
	smccc_function_id_set_owner_id(&fn_id, SMCCC_OWNER_ID_STANDARD);
	smccc_function_id_set_function(&fn_id, (smccc_function_t)fn);

	uint64_t hyp_args[6] = { arg_0, arg_1, arg_2, 0, 0, 0 };
	uint64_t hyp_ret[4]  = { 0 };

	smccc_1_1_call(fn_id, &hyp_args, &hyp_ret, NULL, CLIENT_ID_HYP);

	return (psci_ret_t)hyp_ret[0];
}

static inline register_t
psci_smc_fn_call_reg(psci_function_t fn, register_t arg_0, register_t arg_1,
		     register_t arg_2)
{
	smccc_function_id_t fn_id = smccc_function_id_default();
	smccc_function_id_set_is_fast(&fn_id, true);
	smccc_function_id_set_is_smc64(&fn_id, true);
	smccc_function_id_set_owner_id(&fn_id, SMCCC_OWNER_ID_STANDARD);
	smccc_function_id_set_function(&fn_id, (smccc_function_t)fn);

	uint64_t hyp_args[6] = { arg_0, arg_1, arg_2, 0, 0, 0 };
	uint64_t hyp_ret[4]  = { 0 };

	smccc_1_1_call(fn_id, &hyp_args, &hyp_ret, NULL, CLIENT_ID_HYP);

	return hyp_ret[0];
}

```

`hyp/platform/psci_smc/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

interface psci
interface smccc
local_include
arch_local_include aarch64
source psci_smc.c
types psci_smc.tc

```

`hyp/platform/psci_smc/include/psci_smc.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

uint32_t
psci_smc_psci_version(void);

error_t
psci_smc_cpu_suspend(register_t power_state, register_t entry_point,
		     register_t context_id);

#if defined(PLATFORM_PSCI_DEFAULT_SUSPEND)
error_t
psci_smc_cpu_default_suspend(paddr_t entry_point, register_t context_id);
#endif

error_t
psci_smc_system_reset(void);

error_t
psci_smc_cpu_off(void);

error_t
psci_smc_cpu_on(psci_mpidr_t cpu_id, register_t entry_point,
		register_t context_id);

sint32_result_t
psci_smc_psci_features(psci_function_t fn, bool smc64);

error_t
psci_smc_cpu_freeze(void);

error_t
psci_smc_psci_set_suspend_mode(psci_mode_t mode);

register_t
psci_smc_psci_stat_count(psci_mpidr_t cpu_id, register_t power_state);

```

`hyp/platform/psci_smc/psci_smc.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

define platform_power_state_t newtype bitfield psci_suspend_powerstate;

```

`hyp/platform/psci_smc/src/psci_smc.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <hyptypes.h>

#include <hypconstants.h>

#include <cpulocal.h>
#include <panic.h>
#include <power.h>
#include <smccc.h>

#include "psci_smc.h"
#include "psci_smc_arch.h"

uint32_t
psci_smc_psci_version(void)
{
	psci_ret_t ret =
		psci_smc_fn_call32(PSCI_FUNCTION_PSCI_VERSION, 0, 0, 0);
	return (uint32_t)ret;
}

error_t
psci_smc_cpu_suspend(register_t power_state, paddr_t entry_point,
		     register_t context_id)
{
	error_t err;

	switch (psci_smc_fn_call(PSCI_FUNCTION_CPU_SUSPEND, power_state,
				 entry_point, context_id)) {
	case PSCI_RET_SUCCESS:
		err = OK;
		break;
	case PSCI_RET_INVALID_PARAMETERS:
	case PSCI_RET_INVALID_ADDRESS:
		err = ERROR_ARGUMENT_INVALID;
		break;
	case PSCI_RET_DENIED:
		// Note: OS-initiated mode only
		err = ERROR_DENIED;
		break;
	case PSCI_RET_NOT_SUPPORTED:
	case PSCI_RET_ALREADY_ON:
	case PSCI_RET_ON_PENDING:
	case PSCI_RET_INTERNAL_FAILURE:
	case PSCI_RET_NOT_PRESENT:
	case PSCI_RET_DISABLED:
	default:
		panic("Unexpected PSCI result");
	}

	return err;
}

#if defined(PLATFORM_PSCI_DEFAULT_SUSPEND)
error_t
psci_smc_cpu_default_suspend(paddr_t entry_point, register_t context_id)
{
	error_t err;

	switch (psci_smc_fn_call(PSCI_FUNCTION_CPU_DEFAULT_SUSPEND, entry_point,
				 context_id, 0U)) {
	case PSCI_RET_SUCCESS:
		err = OK;
		break;
	case PSCI_RET_INVALID_PARAMETERS:
	case PSCI_RET_INVALID_ADDRESS:
		err = ERROR_ARGUMENT_INVALID;
		break;
	case PSCI_RET_DENIED:
		// Note: OS-initiated mode only
		err = ERROR_DENIED;
		break;
	case PSCI_RET_NOT_SUPPORTED:
	case PSCI_RET_ALREADY_ON:
	case PSCI_RET_ON_PENDING:
	case PSCI_RET_INTERNAL_FAILURE:
	case PSCI_RET_NOT_PRESENT:
	case PSCI_RET_DISABLED:
	default:
		panic("Unexpected PSCI result");
	}

	return err;
}
#endif

error_t
psci_smc_system_reset(void)
{
	error_t err;

	switch (psci_smc_fn_call32(PSCI_FUNCTION_SYSTEM_RESET, 0, 0, 0)) {
	case PSCI_RET_NOT_SUPPORTED:
		err = ERROR_UNIMPLEMENTED;
		break;
	case PSCI_RET_INVALID_PARAMETERS:
		err = ERROR_ARGUMENT_INVALID;
		break;
	case PSCI_RET_SUCCESS:
	case PSCI_RET_DENIED:
	case PSCI_RET_ALREADY_ON:
	case PSCI_RET_ON_PENDING:
	case PSCI_RET_INTERNAL_FAILURE:
	case PSCI_RET_NOT_PRESENT:
	case PSCI_RET_DISABLED:
	case PSCI_RET_INVALID_ADDRESS:
	default:
		panic("Unexpected PSCI result");
	}

	return err;
}

error_t
psci_smc_cpu_off(void)
{
	error_t err;

	switch (psci_smc_fn_call32(PSCI_FUNCTION_CPU_OFF, 0, 0, 0)) {
	case PSCI_RET_DENIED:
		err = ERROR_DENIED;
		break;
	case PSCI_RET_SUCCESS:
	case PSCI_RET_NOT_SUPPORTED:
	case PSCI_RET_INVALID_PARAMETERS:
	case PSCI_RET_ALREADY_ON:
	case PSCI_RET_ON_PENDING:
	case PSCI_RET_INTERNAL_FAILURE:
	case PSCI_RET_NOT_PRESENT:
	case PSCI_RET_DISABLED:
	case PSCI_RET_INVALID_ADDRESS:
	default:
		panic("Unexpected PSCI result");
	}

	return err;
}

error_t
psci_smc_cpu_on(psci_mpidr_t cpu_id, paddr_t entry_point, register_t context_id)
{
	error_t err;

	switch (psci_smc_fn_call(PSCI_FUNCTION_CPU_ON, psci_mpidr_raw(cpu_id),
				 entry_point, context_id)) {
	case PSCI_RET_SUCCESS:
	case PSCI_RET_ON_PENDING:
		err = OK;
		break;
	case PSCI_RET_ALREADY_ON:
		err = ERROR_RETRY;
		break;
	case PSCI_RET_INVALID_PARAMETERS:
	case PSCI_RET_INVALID_ADDRESS:
		err = ERROR_ARGUMENT_INVALID;
		break;
	case PSCI_RET_INTERNAL_FAILURE:
		err = ERROR_FAILURE;
		break;
	case PSCI_RET_DENIED:
	case PSCI_RET_NOT_SUPPORTED:
	case PSCI_RET_NOT_PRESENT:
	case PSCI_RET_DISABLED:
	default:
		panic("Unexpected PSCI result");
	}

	return err;
}

sint32_result_t
psci_smc_psci_features(psci_function_t fn, bool smc64)
{
	smccc_function_id_t fn_id = smccc_function_id_default();
	smccc_function_id_set_is_fast(&fn_id, true);
	smccc_function_id_set_is_smc64(&fn_id, smc64);
	smccc_function_id_set_owner_id(&fn_id, SMCCC_OWNER_ID_STANDARD);
	smccc_function_id_set_function(&fn_id, (smccc_function_t)fn);

	sint32_result_t ret;
	psci_ret_t psci_ret = psci_smc_fn_call32(PSCI_FUNCTION_PSCI_FEATURES,
						 smccc_function_id_raw(fn_id),
						 0, 0);
	ret.r		    = (int32_t)psci_ret;

	switch ((psci_ret_t)ret.r) {
	case PSCI_RET_NOT_SUPPORTED:
		ret.e = ERROR_UNIMPLEMENTED;
		break;
	case PSCI_RET_SUCCESS:
	case PSCI_RET_INVALID_PARAMETERS:
	case PSCI_RET_DENIED:
	case PSCI_RET_ALREADY_ON:
	case PSCI_RET_ON_PENDING:
	case PSCI_RET_INTERNAL_FAILURE:
	case PSCI_RET_NOT_PRESENT:
	case PSCI_RET_DISABLED:
	case PSCI_RET_INVALID_ADDRESS:
	default:
		if (ret.r >= 0) {
			ret.e = OK;
			break;
		}
		panic("Unexpected PSCI result");
	}

	return ret;
}

error_t
psci_smc_cpu_freeze(void)
{
	error_t err;

	switch (psci_smc_fn_call32(PSCI_FUNCTION_CPU_FREEZE, 0, 0, 0)) {
	case PSCI_RET_NOT_SUPPORTED:
		err = ERROR_UNIMPLEMENTED;
		break;
	case PSCI_RET_DENIED:
		err = ERROR_DENIED;
		break;
	case PSCI_RET_SUCCESS:
	case PSCI_RET_INVALID_PARAMETERS:
	case PSCI_RET_ALREADY_ON:
	case PSCI_RET_ON_PENDING:
	case PSCI_RET_INTERNAL_FAILURE:
	case PSCI_RET_NOT_PRESENT:
	case PSCI_RET_DISABLED:
	case PSCI_RET_INVALID_ADDRESS:
	default:
		panic("Unexpected PSCI result");
	}

	return err;
}

error_t
psci_smc_psci_set_suspend_mode(psci_mode_t mode)
{
	error_t err;

	switch (psci_smc_fn_call32(PSCI_FUNCTION_PSCI_SET_SUSPEND_MODE,
				   (uint32_t)mode, 0, 0)) {
	case PSCI_RET_SUCCESS:
		err = OK;
		break;
	case PSCI_RET_NOT_SUPPORTED:
		err = ERROR_UNIMPLEMENTED;
		break;
	case PSCI_RET_INVALID_PARAMETERS:
		err = ERROR_ARGUMENT_INVALID;
		break;
	case PSCI_RET_DENIED:
		err = ERROR_DENIED;
		break;
	case PSCI_RET_INVALID_ADDRESS:
	case PSCI_RET_ALREADY_ON:
	case PSCI_RET_ON_PENDING:
	case PSCI_RET_INTERNAL_FAILURE:
	case PSCI_RET_NOT_PRESENT:
	case PSCI_RET_DISABLED:
	default:
		panic("Unexpected PSCI result");
	}

	return err;
}

register_t
psci_smc_psci_stat_count(psci_mpidr_t cpu_id, register_t power_state)
{
	return psci_smc_fn_call_reg(PSCI_FUNCTION_PSCI_STAT_COUNT,
				    psci_mpidr_raw(cpu_id), power_state, 0);
}

```

`hyp/platform/soc_qemu/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

interface platform
configs BOOT_STACK_SIZE=6144
configs PLATFORM_IPI_LINES=8
base_module hyp/platform/gicv3
base_module hyp/platform/psci_smc
local_include
source boot.c cpu.c irq.c platform_psci.c prng.c soc_qemu.c head.S abort.c uart.c
source cpu_features.c addrspace.c
types soc_qemu.tc
events soc_qemu.ev

# EL2 needs to save the debug state on suspend, EL3 currently doesn't do it.
configs PLATFORM_DEBUG_SAVE_STATE=0

arch_configs qemu PLATFORM_DEVICES_BASE=0x0U PLATFORM_DEVICES_SIZE=0x40000000U

arch_configs qemu PLATFORM_UART_BASE=0x9000000U PLATFORM_UART_SIZE=0x1000U

```

`hyp/platform/soc_qemu/include/uart.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

void
soc_qemu_console_puts(const char *msg);

```

`hyp/platform/soc_qemu/soc_qemu.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

module soc_qemu

subscribe boot_cold_init
	handler soc_qemu_uart_init()
	priority 5
	require_preempt_disabled

subscribe boot_cpu_cold_init

#if !defined(UNIT_TESTS)
subscribe rootvm_init(root_partition, root_cspace, hyp_env, qcbor_enc_ctxt)
	priority -10
#endif

#if !defined(UNIT_TESTS)
subscribe vcpu_activate_thread
	// Run before the scheduler handler.
	priority 10
#endif

subscribe power_system_off
	// Run after other normal priority handlers as we'll power-off the
	// system here.
	priority last

subscribe power_system_reset
	// Run after other normal priority handlers as we'll reset the system
	// here.
	priority last

subscribe log_message

```

`hyp/platform/soc_qemu/soc_qemu.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

extend hyp_env_data structure {
	device_me_capid		type cap_id_t;
	entry_hlos		type vmaddr_t;
};

#if defined (PLATFORM_PSCI_USE_ORIGINAL_POWERSTATE_FORMAT)
extend psci_suspend_powerstate bitfield {
	delete StateID;
	15:0	StateID bitfield psci_suspend_powerstate_stateid;
};

define psci_suspend_powerstate_stateid bitfield<16> {
	3:0	cpu	type psci_cpu_state_t;
	7:4	cluster	type psci_cluster_state_L3_t;
		others	unknown=0;
};
#endif

define psci_cpu_state_t newtype uint32;
define psci_cluster_state_L3_t newtype uint32;

define CLIENT_ID_HYP constant uint32 = 0;

define soc_qemu_uart structure(aligned(PGTABLE_HYP_PAGE_SIZE)) {
	dr @0x0		uint32(atomic);
	tfr @0x18	uint32(atomic);
};

```

`hyp/platform/soc_qemu/src/abort.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <hyptypes.h>

#include <smccc.h>

#include "event_handlers.h"

void
soc_qemu_handle_power_system_off(void)
{
	uint64_t hyp_args[6] = { 0 };
	uint64_t hyp_ret[4]  = { 0 };

	smccc_function_id_t fn_id = smccc_function_id_default();

	smccc_function_id_set_owner_id(&fn_id, SMCCC_OWNER_ID_STANDARD);
	smccc_function_id_set_function(&fn_id, PSCI_FUNCTION_SYSTEM_OFF);
	smccc_function_id_set_is_smc64(&fn_id, false);
	smccc_function_id_set_is_fast(&fn_id, true);

	smccc_1_1_call(fn_id, &hyp_args, &hyp_ret, NULL, CLIENT_ID_HYP);
}

bool
soc_qemu_handle_power_system_reset(uint64_t reset_type, uint64_t cookie,
				   error_t *error)
{
	(void)reset_type;
	(void)cookie;

	// FIXME: when doing system_reset on QEMU, hypervisor was starting in the
	// correct entry point, but the static variables did not seem to be
	// reinitialized. When this is fixed, handle this call by doing a
	// PSCI_FUNCTION_SYSTEM_RESET/2 SMC

	*error = ERROR_UNIMPLEMENTED;

	return true;
}

```

`hyp/platform/soc_qemu/src/addrspace.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>

#include <platform_mem.h>

bool
platform_pgtable_undergoing_bbm(void)
{
	return false;
}

```

`hyp/platform/soc_qemu/src/boot.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>

#include <cspace.h>
#include <memdb.h>
#include <memextent.h>
#include <object.h>
#include <panic.h>
#include <partition.h>
#include <partition_alloc.h>
#include <platform_mem.h>
#include <spinlock.h>
#include <trace.h>

#include "event_handlers.h"

static platform_ram_info_t ram_info;

error_t
platform_ram_probe(void)
{
	// FIXME: The RAM memory size is currently hardcoded to 1GB. We need to
	// find a better solution for this, possibly by using a
	// system-device-tree approach. We need to make sure that hyp RAM memory
	// ranges do not overlap with the ranges specified in the QEMU start
	// command.
	ram_info.num_ranges = 0x1;
	// TODO: Get info from DT
	ram_info.ram_range[0].base = PLATFORM_DDR_BASE;
	ram_info.ram_range[0].size = PLATFORM_DDR_SIZE;

	return OK;
}

platform_ram_info_t *
platform_get_ram_info(void)
{
	assert(ram_info.num_ranges != 0U);
	return &ram_info;
}

void
platform_add_root_heap(partition_t *partition)
{
	// We allocate 36MiB of memory from the Hyp labelled memory in the ram
	// partition table freelist.
	//  - We give 36MiB to the root partition heap and then allocate 32 MiB
	//  from the allocator to the trace buffer
	size_t trace_size      = TRACE_AREA_SIZE;
	size_t heap_extra_size = EXTRA_ROOT_HEAP_SIZE;
	size_t priv_size       = EXTRA_PRIVATE_HEAP_SIZE;

	uint64_t alloc_size = trace_size + heap_extra_size + priv_size;

	// FIXME: Currently using the end memory of the hardcoded 1Gb hyp RAM
	// memory size. We need to find a better solution for this, possibly by
	// dynamically reading the RAM memory end address from a device tree.

	paddr_t base = PLATFORM_DDR_BASE + PLATFORM_DDR_SIZE - alloc_size;

	// Add 1MiB to the hypervisor private partition
	error_t err = partition_mem_donate(partition, base, priv_size,
					   partition_get_private(), false);
	if (err != OK) {
		panic("Error donating memory");
	}

	err = partition_map_and_add_heap(partition_get_private(), base,
					 priv_size);
	if (err != OK) {
		panic("Error adding root partition heap memory");
	}

	base += priv_size;
	alloc_size -= priv_size;

	// Add the rest to the partition's heap.
	err = partition_map_and_add_heap(partition, base, alloc_size);
	if (err != OK) {
		panic("Error adding root partition heap memory");
	}

	// Allocate memory for the trace_buffer
	trace_init(partition, trace_size);
}

#if !defined(UNIT_TESTS)
static memextent_t *
create_memextent(partition_t *root_partition, cspace_t *root_cspace,
		 paddr_t phys_base, size_t size, pgtable_access_t access,
		 memextent_memtype_t memtype, cap_id_t *new_cap_id)
{
	bool device_mem = (memtype == MEMEXTENT_MEMTYPE_DEVICE);

	memextent_create_t     params_me = { .memextent = NULL,
					     .memextent_device_mem = device_mem };
	memextent_ptr_result_t me_ret;
	me_ret = partition_allocate_memextent(root_partition, params_me);
	if (me_ret.e != OK) {
		panic("Failed creation of memextent");
	}
	memextent_t *me = me_ret.r;

	memextent_attrs_t attrs = memextent_attrs_default();
	memextent_attrs_set_access(&attrs, access);
	memextent_attrs_set_memtype(&attrs, memtype);
#if defined(MODULE_MEM_MEMEXTENT_SPARSE)
	if (device_mem) {
		memextent_attrs_set_type(&attrs, MEMEXTENT_TYPE_SPARSE);
	}
#endif

	spinlock_acquire(&me->header.lock);
	error_t ret = memextent_configure(me, phys_base, size, attrs);
	if (ret != OK) {
		panic("Failed configuration of memextent");
	}
	spinlock_release(&me->header.lock);

	// Create a master cap for the memextent
	object_ptr_t obj_ptr;
	obj_ptr.memextent	  = me;
	cap_id_result_t capid_ret = cspace_create_master_cap(
		root_cspace, obj_ptr, OBJECT_TYPE_MEMEXTENT);
	if (capid_ret.e != OK) {
		panic("Error create memextent cap id.");
	}

	ret = object_activate_memextent(me);
	if (ret != OK) {
		panic("Failed activation of mem extent");
	}

	*new_cap_id = capid_ret.r;

	return me;
}

void
soc_qemu_handle_rootvm_init(partition_t *root_partition, cspace_t *root_cspace,
			    hyp_env_data_t   *hyp_env,
			    qcbor_enc_ctxt_t *qcbor_enc_ctxt)
{
	// FIXME: The memory layout for QEMU is hardcoded here. We need to find a
	// better solution for this, possibly by using a system-device-tree
	// approach, that is consumed by us, and used to generate the HLOS VM
	// device-tree. We will also need to get the addresses such as
	// hlos-entry from this config such that ultimately these can all be
	// inputs from QEMU/user.
	paddr_t hlos_vm_base = HLOS_VM_DDR_BASE;
	paddr_t hlos_vm_size = HLOS_VM_DDR_SIZE;

	assert(qcbor_enc_ctxt != NULL);

	// VM memory node. Includes entry point, DT, and rootfs
	QCBOREncode_AddUInt64ToMap(qcbor_enc_ctxt, "hlos_vm_base",
				   hlos_vm_base);
	QCBOREncode_AddUInt64ToMap(qcbor_enc_ctxt, "hlos_vm_size",
				   hlos_vm_size);
	QCBOREncode_AddUInt64ToMap(qcbor_enc_ctxt, "entry_hlos",
				   HLOS_ENTRY_POINT);
	QCBOREncode_AddUInt64ToMap(qcbor_enc_ctxt, "hlos_dt_base",
				   HLOS_DT_BASE);
	QCBOREncode_AddUInt64ToMap(qcbor_enc_ctxt, "hlos_ramfs_base",
				   HLOS_RAM_FS_BASE);
	QCBOREncode_AddUInt64ToMap(qcbor_enc_ctxt, "device_me_base",
				   PLATFORM_DEVICES_BASE);
	QCBOREncode_AddUInt64ToMap(qcbor_enc_ctxt, "device_me_size",
				   PLATFORM_DEVICES_SIZE);

#if defined(WATCHDOG_DISABLE)
	QCBOREncode_AddUInt64ToMap(qcbor_enc_ctxt, "watchdog_supported", false);
#endif

	// Create a device memextent to cover the full HW physical address
	// space reserved for devices, so that the resource manager can derive
	// device memextents.
	// Long term the intention is for a system device-tree to allow fine
	// grained memextent creation.

	memextent_t *me = create_memextent(
		root_partition, root_cspace, PLATFORM_DEVICES_BASE,
		PLATFORM_DEVICES_SIZE, PGTABLE_ACCESS_RW,
		MEMEXTENT_MEMTYPE_DEVICE, &hyp_env->device_me_capid);

	QCBOREncode_AddUInt64ToMap(qcbor_enc_ctxt, "device_me_capid",
				   hyp_env->device_me_capid);

	// Derive memextents for GICD, GICR and watchdog to effectively remove
	// them from the device memextent we provide to the rootvm.

	memextent_ptr_result_t me_ret;
	me_ret = memextent_derive(me, PLATFORM_GICD_BASE, 0x10000U,
				  MEMEXTENT_MEMTYPE_DEVICE, PGTABLE_ACCESS_RW,
				  MEMEXTENT_TYPE_BASIC);
	if (me_ret.e != OK) {
		panic("Failed creation of gicd memextent");
	}
	me_ret = memextent_derive(me, PLATFORM_GICR_BASE,
				  (PLATFORM_MAX_CORES << GICR_STRIDE_SHIFT),
				  MEMEXTENT_MEMTYPE_DEVICE, PGTABLE_ACCESS_RW,
				  MEMEXTENT_TYPE_BASIC);
	if (me_ret.e != OK) {
		panic("Failed creation of gicr memextent");
	}

	// Derive extent for UART and share it with RM
	me_ret = memextent_derive(me, PLATFORM_UART_BASE, PLATFORM_UART_SIZE,
				  MEMEXTENT_MEMTYPE_DEVICE, PGTABLE_ACCESS_RW,
				  MEMEXTENT_TYPE_BASIC);
	if (me_ret.e != OK) {
		panic("Failed creation of uart memextent");
	}

	// Create a master cap for the uart memextent
	object_ptr_t obj_ptr;
	obj_ptr.memextent	  = me_ret.r;
	cap_id_result_t capid_ret = cspace_create_master_cap(
		root_cspace, obj_ptr, OBJECT_TYPE_MEMEXTENT);
	if (capid_ret.e != OK) {
		panic("Error create memextent cap id.");
	}

	QCBOREncode_AddUInt64ToMap(qcbor_enc_ctxt, "uart_address",
				   PLATFORM_UART_BASE);
	QCBOREncode_AddUInt64ToMap(qcbor_enc_ctxt, "uart_me_capid",
				   capid_ret.r);
}
#endif

```

`hyp/platform/soc_qemu/src/cpu.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>

#include <compiler.h>
#include <cpulocal.h>
#include <idle.h>
#include <panic.h>
#include <partition.h>
#include <platform_cpu.h>
#include <platform_psci.h>
#include <preempt.h>
#include <psci.h>
#include <thread.h>
#include <util.h>

#include "event_handlers.h"
#include "psci_smc.h"

// The entry points are really functions, but we don't use function types for
// them because they are never directly called from C, and using function types
// here would force us to break MISRA required rule 11.1 in platform_cpu_on().
extern const char soc_qemu_entry_cold_secondary;
extern const char soc_qemu_entry_warm;

CPULOCAL_DECLARE_STATIC(bool, cpu_started);

void
soc_qemu_handle_boot_cpu_cold_init(cpu_index_t cpu)
{
	CPULOCAL_BY_INDEX(cpu_started, cpu) = true;
}

bool
platform_cpu_exists(cpu_index_t cpu)
{
	assert(cpu < PLATFORM_MAX_CORES);

	return compiler_expected((util_bit(cpu) & PLATFORM_USABLE_CORES) != 0U);
}

error_t
platform_cpu_on(cpu_index_t cpu)
{
	MPIDR_EL1_t mpidr  = platform_cpu_index_to_mpidr(cpu);
	thread_t   *thread = idle_thread_for(cpu);
	uintptr_t   entry_virt =
		  CPULOCAL_BY_INDEX(cpu_started, cpu)
			  ? (uintptr_t)&soc_qemu_entry_warm
			  : (uintptr_t)&soc_qemu_entry_cold_secondary;
	psci_mpidr_t psci_mpidr = psci_mpidr_default();
	psci_mpidr_set_Aff0(&psci_mpidr, MPIDR_EL1_get_Aff0(&mpidr));
	psci_mpidr_set_Aff1(&psci_mpidr, MPIDR_EL1_get_Aff1(&mpidr));
	psci_mpidr_set_Aff2(&psci_mpidr, MPIDR_EL1_get_Aff2(&mpidr));
	psci_mpidr_set_Aff3(&psci_mpidr, MPIDR_EL1_get_Aff3(&mpidr));
	return psci_smc_cpu_on(psci_mpidr,
			       partition_virt_to_phys(partition_get_private(),
						      entry_virt),
			       (uintptr_t)thread);
}

static noreturn register_t
psci_smc_system_reset_arg(register_t unused)
{
	(void)unused;

	psci_smc_system_reset();

	panic("psci_smc_system_reset failed!");
}

void
platform_system_reset(void)
{
	thread_freeze(psci_smc_system_reset_arg, 0, 0);
}

static noreturn register_t
psci_smc_cpu_off_arg(register_t unused)
{
	(void)unused;

	psci_smc_cpu_off();

	panic("psci_smc_cpu_off failed!");
}

void
platform_cpu_off(void)
{
	assert(idle_is_current());

	thread_freeze(psci_smc_cpu_off_arg, 0U, 0U);
}

static register_t
psci_smc_cpu_suspend_arg(register_t power_state) REQUIRE_PREEMPT_DISABLED
{
	thread_t *idle = idle_thread();

	paddr_t entry_phys = partition_virt_to_phys(
		partition_get_private(), (uintptr_t)&soc_qemu_entry_warm);

	error_t ret =
		psci_smc_cpu_suspend(power_state, entry_phys, (register_t)idle);

	return (register_t)ret;
}

bool_result_t
platform_cpu_suspend(psci_suspend_powerstate_t power_state)
{
	register_t ret;

	assert(idle_is_current());

	ret = thread_freeze(psci_smc_cpu_suspend_arg,
			    psci_suspend_powerstate_raw(power_state), ~0UL);

	return (ret == 0UL)    ? bool_result_ok(false)
	       : (ret == ~0UL) ? bool_result_ok(true)
			       : bool_result_error((error_t)ret);
}

error_t
platform_psci_set_suspend_mode(psci_mode_t mode)
{
	return psci_smc_psci_set_suspend_mode(mode);
}

#if defined(PLATFORM_PSCI_DEFAULT_SUSPEND)
static register_t
psci_smc_cpu_default_suspend_arg(register_t unused)
{
	(void)unused;

	thread_t *idle = idle_thread();

	paddr_t entry_phys = partition_virt_to_phys(
		partition_get_private(), (uintptr_t)&soc_qemu_entry_warm);

	error_t ret =
		psci_smc_cpu_default_suspend(entry_phys, (register_t)idle);

	return (register_t)ret;
}

bool_result_t
platform_cpu_default_suspend(void)
{
	register_t ret;

	assert(idle_is_current());
	ret = thread_freeze(psci_smc_cpu_default_suspend_arg, 0UL, ~0UL);

	return (ret == 0UL)    ? bool_result_ok(false)
	       : (ret == ~0UL) ? bool_result_ok(true)
			       : bool_result_error((error_t)ret);
}
#endif

```

`hyp/platform/soc_qemu/src/cpu_features.c`:

```c
// © 2022 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <hyptypes.h>

#include <platform_features.h>
#include <smccc.h>

platform_cpu_features_t
platform_get_cpu_features(void)
{
	platform_cpu_features_t features = platform_cpu_features_default();

#if defined(MODULE_VM_ARM_VM_MTE)
	platform_cpu_features_set_mte_disable(&features, false);
#endif
#if defined(INTERFACE_VET)
	platform_cpu_features_set_trace_disable(&features, false);
#endif
#if defined(INTERFACE_DEBUG)
	platform_cpu_features_set_debug_disable(&features, false);
#endif
#if defined(MODULE_VM_ARM_VM_SVE_SIMPLE)
	platform_cpu_features_set_sve_disable(&features, false);
#endif

	return features;
}

```

`hyp/platform/soc_qemu/src/head.S`:

```S
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <hypconstants.h>

#include <asm/asm_defs.inc>

const64 soc_qemu_hyp_arg0

	.section .text.boot
function __entry_el2, section=nosection
	// Enable I-cache
	abs64	x10, 0x30401800
	msr	SCTLR_EL2, x10
	adrl	x1, soc_qemu_hyp_arg0
	str	x0, [x1]

	// Get a 256-bit random seed into x0..x3
local get_x0:
	mrs	x0, RNDRRS
	b.eq	LOCAL(get_x0)
local get_x1:
	mrs	x1, RNDR
	b.eq	LOCAL(get_x1)
local get_x2:
	mrs	x2, RNDR
	b.eq	LOCAL(get_x2)
local get_x3:
	mrs	x3, RNDR
	b.eq	LOCAL(get_x3)

	mrs	x4, MPIDR_EL1
	ubfx	x4, x4, MPIDR_EL1_AFF0_SHIFT, MPIDR_EL1_AFF0_BITS

	b	aarch64_init

local prng_fail:
	wfe
	b	LOCAL(prng_fail)
function_end __entry_el2

function soc_qemu_entry_cold_secondary
	mrs	x1, MPIDR_EL1
	ubfx	x1, x1, MPIDR_EL1_AFF0_SHIFT, MPIDR_EL1_AFF0_BITS
	b	aarch64_secondary_init
function_end soc_qemu_entry_cold_secondary

function soc_qemu_entry_warm
	b	aarch64_warm_init
function_end soc_qemu_entry_warm

```

`hyp/platform/soc_qemu/src/irq.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>

#include <platform_irq.h>

#include "gicv3.h"

irq_t
platform_irq_max(void)
{
	return gicv3_irq_max();
}

error_t
platform_irq_check(irq_t irq)
{
	return gicv3_irq_check(irq);
}

bool
platform_irq_is_percpu(irq_t irq)
{
	return gicv3_irq_is_percpu(irq);
}

void
platform_irq_enable_shared(irq_t irq)
{
	gicv3_irq_enable_shared(irq);
}

void
platform_irq_enable_local(irq_t irq)
{
	gicv3_irq_enable_local(irq);
}

void
platform_irq_disable_shared(irq_t irq)
{
	gicv3_irq_disable_shared(irq);
}

void
platform_irq_disable_local(irq_t irq)
{
	gicv3_irq_disable_local(irq);
}

void
platform_irq_enable_percpu(irq_t irq, cpu_index_t cpu)
{
	gicv3_irq_enable_percpu(irq, cpu);
}

void
platform_irq_disable_percpu(irq_t irq, cpu_index_t cpu)
{
	gicv3_irq_disable_percpu(irq, cpu);
}

void
platform_irq_disable_local_nowait(irq_t irq)
{
	gicv3_irq_disable_local_nowait(irq);
}

irq_result_t
platform_irq_acknowledge(void)
{
	irq_result_t ret;

	ret = gicv3_irq_acknowledge();

	return ret;
}

void
platform_irq_priority_drop(irq_t irq)
{
	gicv3_irq_priority_drop(irq);
}

void
platform_irq_deactivate(irq_t irq)
{
	gicv3_irq_deactivate(irq);
}

void
platform_irq_deactivate_percpu(irq_t irq, cpu_index_t cpu)
{
	gicv3_irq_deactivate_percpu(irq, cpu);
}

irq_trigger_result_t
platform_irq_set_mode_percpu(irq_t irq, irq_trigger_t trigger, cpu_index_t cpu)
{
	return gicv3_irq_set_trigger_percpu(irq, trigger, cpu);
}

```

`hyp/platform/soc_qemu/src/platform_psci.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>

#if !defined(UNIT_TESTS)
#include <platform_psci.h>
#include <util.h>

#include "event_handlers.h"

bool
platform_psci_is_cpu_active(psci_cpu_state_t cpu_state)
{
	// We will treat a cpu as active if it's none zero since QEMU does not
	// care about cpu states
	return (cpu_state == 0U);
}

bool
platform_psci_is_cpu_poweroff(psci_cpu_state_t cpu_state)
{
	(void)cpu_state;

	// Powerdown not supported in QEMU, it always goes into WFI
	return false;
}

bool
platform_psci_is_cluster_active(psci_cluster_state_L3_t cluster_state)
{
	(void)cluster_state;
	return true;
}

psci_cpu_state_t
platform_psci_get_cpu_state(psci_suspend_powerstate_t suspend_state)
{
	psci_suspend_powerstate_stateid_t stateid =
		psci_suspend_powerstate_get_StateID(&suspend_state);

	return psci_suspend_powerstate_stateid_get_cpu(&stateid);
}

void
platform_psci_set_cpu_state(psci_suspend_powerstate_t *suspend_state,
			    psci_cpu_state_t	       cpu_state)
{
	psci_suspend_powerstate_stateid_t stateid =
		psci_suspend_powerstate_get_StateID(suspend_state);
	psci_suspend_powerstate_stateid_set_cpu(&stateid, cpu_state);
	psci_suspend_powerstate_set_StateID(suspend_state, stateid);
}

psci_cpu_state_t
platform_psci_shallowest_cpu_state(psci_cpu_state_t state1,
				   psci_cpu_state_t state2)
{
	return (psci_cpu_state_t)(util_min(state1, state2));
}

psci_cpu_state_t
platform_psci_deepest_cpu_state(cpu_index_t cpu)
{
	(void)cpu;

	// Since QEMU does not care about cpu suspend states, we will use 0 as
	// active and non-zero as suspended.
	return (psci_cpu_state_t)(1);
}

psci_suspend_powerstate_stateid_t
platform_psci_deepest_cpu_level_stateid(cpu_index_t cpu)
{
	(void)cpu;

	// Since QEMU does not care about cpu suspend states, we'll use 0 as
	// active and non-zero as suspended.
	return psci_suspend_powerstate_stateid_cast(1);
}

psci_ret_t
platform_psci_suspend_state_validation(psci_suspend_powerstate_t suspend_state,
				       cpu_index_t cpu, psci_mode_t psci_mode)
{
	(void)suspend_state;
	(void)cpu;
	(void)psci_mode;

	// QEMU does not care about suspend states since it only goes to WFI.
	return PSCI_RET_SUCCESS;
}

// Returns the cluster indices
uint32_t
platform_psci_get_cluster_index(cpu_index_t cpu)
{
	(void)cpu;
	return 0U;
}

error_t
platform_psci_get_index_by_level(cpu_index_t cpu, uint32_t *start_idx,
				 uint32_t *children_counts, uint32_t level)
{
	(void)cpu;
	(void)level;
	*start_idx	 = 0U;
	*children_counts = PLATFORM_MAX_CORES;

	return OK;
}

#endif

```

`hyp/platform/soc_qemu/src/prng.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>

#include <platform_prng.h>

error_t
platform_get_serial(uint32_t data[4])
{
	data[0] = 0U;
	data[1] = 0U;
	data[2] = 0U;
	data[3] = 0U;

	return OK;
}

```

`hyp/platform/soc_qemu/src/soc_qemu.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>

#include <bitmap.h>
#include <panic.h>
#include <platform_cpu.h>
#include <platform_security.h>
#include <smccc_platform.h>

#include "event_handlers.h"

bool
platform_security_state_debug_disabled(void)
{
	return false;
}

uint32_t
platform_cpu_stack_size(void)
{
	return 0;
}

bool
smccc_handle_smc_platform_call(register_t args[7], bool is_hvc)
	EXCLUDE_PREEMPT_DISABLED
{
	(void)is_hvc;
	args[0] = (register_t)SMCCC_UNKNOWN_FUNCTION64;
	return true;
}

// Overrides the weak imlementation in core_id.c
core_id_t
platform_cpu_get_coreid(MIDR_EL1_t midr)
{
	(void)midr;
	return CORE_ID_QEMU;
}

#if !defined(UNIT_TESTS)
static _Atomic BITMAP_DECLARE(PLATFORM_MAX_CORES, hlos_vm_cpus);

bool
soc_qemu_handle_vcpu_activate_thread(thread_t		*thread,
				     vcpu_option_flags_t options)
{
	bool ret = true;

	assert(thread != NULL);
	assert(thread->kind == THREAD_KIND_VCPU);

	if (vcpu_option_flags_get_hlos_vm(&options)) {
		bool already_set = bitmap_atomic_test_and_set(
			hlos_vm_cpus, thread->scheduler_affinity,
			memory_order_relaxed);
		if (already_set) {
			ret = false;
			goto out;
		}

		// Validated, set the flag in the thread
		vcpu_option_flags_set_hlos_vm(&thread->vcpu_options, true);
	}

out:
	return ret;
}
#endif

```

`hyp/platform/soc_qemu/src/uart.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>
#include <string.h>

#include <atomic.h>
#include <hyp_aspace.h>
#include <panic.h>
#include <partition.h>
#include <pgtable.h>
#include <preempt.h>
#include <spinlock.h>

#include "event_handlers.h"
#include "uart.h"

static soc_qemu_uart_t *uart;
static spinlock_t	uart_lock;

static void
uart_putc(const char c)
{
	while ((atomic_load_relaxed(&uart->tfr) & ((uint32_t)1U << 5)) != 0U) {
	}

	atomic_store_relaxed(&uart->dr, c);
}

static char *banner = "[HYP] ";

static void
uart_write(const char *out, size_t size)
{
	size_t	    remain = size;
	const char *pos	   = out;

	for (size_t i = 0; i < strlen(banner); i++) {
		uart_putc(banner[i]);
	}

	while (remain > 0) {
		char c;

		if (*pos == '\n') {
			c = '\r';
			uart_putc(c);
		}

		c = *pos;
		uart_putc(c);
		pos++;
		remain--;
	}

	uart_putc('\n');
}

void
soc_qemu_console_puts(const char *msg)
{
	spinlock_acquire(&uart_lock);
	if (uart != NULL) {
		uart_write(msg, strlen(msg));
	}
	spinlock_release(&uart_lock);
}

void
soc_qemu_handle_log_message(trace_id_t id, const char *str)
{
#if defined(VERBOSE) && VERBOSE
	(void)id;

	soc_qemu_console_puts(str);
#else
	if ((id == TRACE_ID_WARN) || (id == TRACE_ID_PANIC) ||
	    (id == TRACE_ID_ASSERT_FAILED) ||
#if defined(INTERFACE_TESTS)
	    (id == TRACE_ID_TEST) ||
#endif
	    (id == TRACE_ID_DEBUG)) {
		soc_qemu_console_puts(str);
	}
#endif
}

void
soc_qemu_uart_init(void)
{
	spinlock_init(&uart_lock);

	virt_range_result_t range = hyp_aspace_allocate(PLATFORM_UART_SIZE);
	if (range.e != OK) {
		panic("uart: Address allocation failed.");
	}

	pgtable_hyp_start();

	// Map UART
	uart	    = (soc_qemu_uart_t *)range.r.base;
	error_t ret = pgtable_hyp_map(partition_get_private(), (uintptr_t)uart,
				      PLATFORM_UART_SIZE, PLATFORM_UART_BASE,
				      PGTABLE_HYP_MEMTYPE_NOSPEC_NOCOMBINE,
				      PGTABLE_ACCESS_RW,
				      VMSA_SHAREABILITY_NON_SHAREABLE);
	if (ret != OK) {
		panic("uart: Mapping failed.");
	}

	pgtable_hyp_commit();
}

```

`hyp/platform/trbe/aarch64/trbe.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

extend MDCR_EL2 bitfield {
	25:24		E2TB		uint8;
};

define TRBLIMITR_EL1 bitfield<64> {
	0	E		bool;
	2:1	FM		uint8;
	4:3	TM		uint8;
	5	nVM		bool;
	63:12	LIMIT		uint64 lsl(12);
	others	unknown = 0;
};

define trbe_context structure {
	TRBLIMITR_EL1	bitfield TRBLIMITR_EL1;
	TRBPTR_EL1	uint64;
	TRBBASER_EL1	uint64;
	TRBSR_EL1	uint64;
	TRBMAR_EL1	uint64;
	TRBTRG_EL1	uint64;
};

```

`hyp/platform/trbe/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

arch_types aarch64 trbe.tc
local_include
source trbe.c

```

`hyp/platform/trbe/include/trbe.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

void
trbe_save_context_percpu(cpu_index_t cpu);

void
trbe_restore_context_percpu(cpu_index_t cpu);

```

`hyp/platform/trbe/src/trbe.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <hyptypes.h>

#include <hypregisters.h>

#include <cpulocal.h>
#include <hyp_aspace.h>
#include <panic.h>
#include <partition.h>
#include <vet.h>

#include <asm/sysregs.h>

#include "trbe.h"

CPULOCAL_DECLARE_STATIC(trbe_context_t, trbe_contexts);

void
trbe_save_context_percpu(cpu_index_t cpu)
{
	CPULOCAL_BY_INDEX(trbe_contexts, cpu).TRBLIMITR_EL1 =
		register_TRBLIMITR_EL1_read_ordered(&vet_ordering);

	CPULOCAL_BY_INDEX(trbe_contexts, cpu).TRBPTR_EL1 =
		register_TRBPTR_EL1_read_ordered(&vet_ordering);

	CPULOCAL_BY_INDEX(trbe_contexts, cpu).TRBBASER_EL1 =
		register_TRBBASER_EL1_read_ordered(&vet_ordering);

	CPULOCAL_BY_INDEX(trbe_contexts, cpu).TRBSR_EL1 =
		register_TRBSR_EL1_read_ordered(&vet_ordering);

	CPULOCAL_BY_INDEX(trbe_contexts, cpu).TRBMAR_EL1 =
		register_TRBMAR_EL1_read_ordered(&vet_ordering);

	CPULOCAL_BY_INDEX(trbe_contexts, cpu).TRBTRG_EL1 =
		register_TRBTRG_EL1_read_ordered(&vet_ordering);
}

void
trbe_restore_context_percpu(cpu_index_t cpu)
{
	register_TRBLIMITR_EL1_write_ordered(
		CPULOCAL_BY_INDEX(trbe_contexts, cpu).TRBLIMITR_EL1,
		&vet_ordering);

	register_TRBPTR_EL1_write_ordered(
		CPULOCAL_BY_INDEX(trbe_contexts, cpu).TRBPTR_EL1,
		&vet_ordering);

	register_TRBBASER_EL1_write_ordered(
		CPULOCAL_BY_INDEX(trbe_contexts, cpu).TRBBASER_EL1,
		&vet_ordering);

	register_TRBSR_EL1_write_ordered(
		CPULOCAL_BY_INDEX(trbe_contexts, cpu).TRBSR_EL1, &vet_ordering);

	register_TRBMAR_EL1_write_ordered(
		CPULOCAL_BY_INDEX(trbe_contexts, cpu).TRBMAR_EL1,
		&vet_ordering);

	register_TRBTRG_EL1_write_ordered(
		CPULOCAL_BY_INDEX(trbe_contexts, cpu).TRBTRG_EL1,
		&vet_ordering);
}

```

`hyp/vm/arm_pv_time/arm_pv_time.ev`:

```ev
// © 2022 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

module arm_pv_time

#include <smccc.ev.h>

SMCCC_STANDARD_HYP_FUNCTION_64(PV_TIME_FEATURES, 0, pv_time_features, arg1, ret0)
SMCCC_STANDARD_HYP_FUNCTION_64(PV_TIME_ST, 0, pv_time_st, arg1, ret0)

subscribe object_create_thread(thread_create)

subscribe vcpu_activate_thread(thread)

subscribe scheduler_schedule

subscribe thread_context_switch_post(curticks, prevticks)

subscribe scheduler_blocked(thread, block)

subscribe scheduler_unblocked(thread, block)

```

`hyp/vm/arm_pv_time/arm_pv_time.tc`:

```tc
// © 2022 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

define pv_time_data structure(aligned(64)) {
	revision	uint32;
	attributes	uint32;

	// Stolen time after conversion to nanoseconds.
	//
	// The specification requires accesses to this variable to be
	// (single-copy) atomic, but does not require explicit ordering.
	stolen_ns	uint64(atomic);
};

// Packed self-unblock state.
define arm_pv_time_self_block_state bitfield<64> {
	// The last block state the thread put itself into. This is only valid
	// if last_unblocked is 0.
	//
	// At initialisation, this is set to SCHEDULER_BLOCK_THREAD_LIFECYCLE.
	// VCPU activation changes it to SCHEDULER_BLOCK_VCPU_OFF.
	auto		block		enumeration scheduler_block;

	// Time at which the thread was last unblocked from a self-imposed
	// block state (i.e. one in in which the thread voluntarily gave up
	// the CPU and which should not be counted as stolen).
	//
	// This is zero while the thread is in a self-imposed block state.
	// The thread's initial block state (prior to first starting) is
	// considered self-imposed for this purpose.
	auto<60>	last_unblocked	type ticks_t;
};

define arm_pv_time structure {
	// Pointer to the hypervisor mapping of the VM-accessible stolen time.
	//
	// If non-NULL, this has the same lifetime as the thread itself, and
	// should only be accessed while holding a reference to the thread.
	data			pointer structure pv_time_data;

	// Stolen time in ticks. Must only be accessed by the thread.
	stolen_ticks		type ticks_t;

	// Time spent in a directed yield since the thread last ran. Must only
	// be accessed by the thread, or by a CPU that is in a directed yield
	// from the thread.
	yield_time		type ticks_t;

	// The packed last-block state, as defined above.
	//
	// This can only be accessed by the thread itself during context
	// switch or entry into a block state, or while unblocking the thread
	// while holding its scheduler lock. It is therefore ordered, but not
	// directly protected, by the scheduler lock, so it needs to be atomic
	// but can use relaxed ordering.
	self_block		bitfield arm_pv_time_self_block_state(atomic);
};

extend thread object {
	arm_pv_time structure arm_pv_time;
};

extend smccc_standard_hyp_function enumeration {
	PV_TIME_FEATURES = 0x20;
	PV_TIME_ST = 0x21;
};

extend addrspace_info_area_layout structure {
	pv_time_data	array(PLATFORM_MAX_CORES) structure pv_time_data;
};

```

`hyp/vm/arm_pv_time/build.conf`:

```conf
# © 2022 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

events arm_pv_time.ev
types arm_pv_time.tc
source arm_pv_time.c

```

`hyp/vm/arm_pv_time/src/arm_pv_time.c`:

```c
// © 2022 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>

#include <atomic.h>
#include <compiler.h>
#include <panic.h>
#include <platform_timer.h>
#include <thread.h>
#include <trace.h>
#include <util.h>

#include "event_handlers.h"

#if !defined(MODULE_VM_VGIC)
#error Unable to determine a unique VCPU index (vgic_gicr_index not present)
#endif

bool
smccc_pv_time_features(uint64_t arg1, uint64_t *ret0)
{
	smccc_function_id_t fn_id    = smccc_function_id_cast((uint32_t)arg1);
	bool		    is_smc64 = smccc_function_id_get_is_smc64(&fn_id);
	bool		    is_fast  = smccc_function_id_get_is_fast(&fn_id);
	uint32_t	    res0     = smccc_function_id_get_res0(&fn_id);
	smccc_function_t    fn	     = smccc_function_id_get_function(&fn_id);
	smccc_owner_id_t    owner_id = smccc_function_id_get_owner_id(&fn_id);
	uint64_t	    ret	     = SMCCC_UNKNOWN_FUNCTION64;

	if ((owner_id == SMCCC_OWNER_ID_STANDARD_HYP) && (res0 == 0U) &&
	    is_fast && is_smc64) {
		switch ((smccc_standard_hyp_function_t)fn) {
		case SMCCC_STANDARD_HYP_FUNCTION_PV_TIME_FEATURES:
			ret = 0;
			break;
		case SMCCC_STANDARD_HYP_FUNCTION_PV_TIME_ST: {
			thread_t *current = thread_get_self();
			if (current->addrspace->info_area.me != NULL) {
				ret = 0;
			}
			break;
		}
		case SMCCC_STANDARD_HYP_FUNCTION_CALL_COUNT:
		case SMCCC_STANDARD_HYP_FUNCTION_CALL_UID:
		case SMCCC_STANDARD_HYP_FUNCTION_REVISION:
		default:
			// Nothing to do.
			break;
		}
	}

	*ret0 = ret;
	return true;
}

bool
smccc_pv_time_st(uint64_t arg1, uint64_t *ret0)
{
	(void)arg1;

	thread_t *current = thread_get_self();
	uint64_t  ret	  = SMCCC_UNKNOWN_FUNCTION64;

	if (current->addrspace->info_area.me != NULL) {
		index_t index = current->vgic_gicr_index;
		assert(index < PLATFORM_MAX_CORES);
		size_t offset = offsetof(addrspace_info_area_layout_t,
					 pv_time_data[index]);
		assert((offset + sizeof(pv_time_data_t)) <=
		       current->addrspace->info_area.me->size);
		ret = current->addrspace->info_area.ipa + offset;
	}

	*ret0 = ret;
	return true;
}

error_t
arm_pv_time_handle_object_create_thread(thread_create_t thread_create)
{
	thread_t *thread = thread_create.thread;
	assert(thread != NULL);

	arm_pv_time_self_block_state_t new_state =
		arm_pv_time_self_block_state_default();
	arm_pv_time_self_block_state_set_block(
		&new_state, SCHEDULER_BLOCK_THREAD_LIFECYCLE);
	atomic_store_relaxed(&thread->arm_pv_time.self_block, new_state);

	return OK;
}

bool
arm_pv_time_handle_vcpu_activate_thread(thread_t *thread)
{
	assert(thread != NULL);

	arm_pv_time_self_block_state_t new_state =
		arm_pv_time_self_block_state_default();
	arm_pv_time_self_block_state_set_block(&new_state,
					       SCHEDULER_BLOCK_VCPU_OFF);
	atomic_store_relaxed(&thread->arm_pv_time.self_block, new_state);

	if ((thread->addrspace->info_area.me != NULL)) {
		index_t index = thread->vgic_gicr_index;
		assert(index < PLATFORM_MAX_CORES);
		assert(thread->addrspace->info_area.hyp_va != NULL);
		thread->arm_pv_time.data = &thread->addrspace->info_area.hyp_va
						    ->pv_time_data[index];

		thread->arm_pv_time.data->revision   = 0U;
		thread->arm_pv_time.data->attributes = 0U;
		atomic_init(&thread->arm_pv_time.data->stolen_ns, 0U);
	}

	return true;
}

void
arm_pv_time_handle_scheduler_schedule(thread_t *current, thread_t *yielded_from,
				      ticks_t schedtime, ticks_t curticks)
{
	assert(current == thread_get_self());
	assert(curticks >= schedtime);

	// Avoid counting time in directed yields as stolen
	if (yielded_from != NULL) {
		yielded_from->arm_pv_time.yield_time += curticks - schedtime;
		TRACE(DEBUG, INFO,
		      "arm_pv_time: {:#x} added yield time {:d}, curticks {:d}",
		      (uintptr_t)yielded_from, curticks - schedtime,
		      yielded_from->arm_pv_time.yield_time);
	}
}

void
arm_pv_time_handle_thread_context_switch_post(ticks_t curticks,
					      ticks_t prevticks)
{
	thread_t *current = thread_get_self();

	// The start of the stolen time period is the absolute time the thread
	// stopped running plus the relative time it spent yielding, or the
	// absolute time the thread was last unblocked after blocking itself,
	// whichever is later.
	ticks_t adjusted_last_run = prevticks + current->arm_pv_time.yield_time;
	// Note that the scheduler reads curticks before acquiring any locks,
	// so it is possible that the last unblock occurred on a remote CPU
	// _after_ curticks. Checking for this explicitly is probably cheaper
	// than the synchronisation required to prevent it.
	arm_pv_time_self_block_state_t state =
		atomic_load_relaxed(&current->arm_pv_time.self_block);
	ticks_t last_self_unblock = util_min(
		curticks,
		arm_pv_time_self_block_state_get_last_unblocked(&state));
	ticks_t steal_start = util_max(last_self_unblock, adjusted_last_run);

	TRACE(DEBUG, INFO,
	      "arm_pv_time: {:#x} increment steal time by {:d}ns; "
	      "last run at {:d}ns (+ {:d}ns yielding), unblocked at {:d}ns",
	      (uintptr_t)current,
	      platform_timer_convert_ticks_to_ns(curticks - steal_start),
	      platform_timer_convert_ticks_to_ns(prevticks),
	      platform_timer_convert_ticks_to_ns(
		      current->arm_pv_time.yield_time),
	      platform_timer_convert_ticks_to_ns(last_self_unblock));

	assert((curticks >= adjusted_last_run) &&
	       (curticks >= last_self_unblock));

	current->arm_pv_time.yield_time = 0;
	current->arm_pv_time.stolen_ticks += curticks - steal_start;
	if (current->arm_pv_time.data != NULL) {
		uint64_t stolen_ns = platform_timer_convert_ticks_to_ns(
			current->arm_pv_time.stolen_ticks);
		atomic_store_relaxed(&current->arm_pv_time.data->stolen_ns,
				     stolen_ns);
	}
}

void
arm_pv_time_handle_scheduler_blocked(thread_t *thread, scheduler_block_t block)
{
	if (thread == thread_get_self()) {
		// Thread has blocked itself, presumably voluntarily. Reset the
		// last-unblock time.
		TRACE(DEBUG, INFO, "arm_pv_time: blocking self {:#x} ({:d})",
		      (uintptr_t)thread, (register_t)block);
		arm_pv_time_self_block_state_t new_state =
			arm_pv_time_self_block_state_default();
		arm_pv_time_self_block_state_set_block(&new_state, block);
		atomic_store_relaxed(&thread->arm_pv_time.self_block,
				     new_state);
	}
}

void
arm_pv_time_handle_scheduler_unblocked(thread_t		*thread,
				       scheduler_block_t block)
{
	arm_pv_time_self_block_state_t state =
		atomic_load_relaxed(&thread->arm_pv_time.self_block);
	ticks_t last_self_unblock =
		arm_pv_time_self_block_state_get_last_unblocked(&state);
	scheduler_block_t self_block =
		arm_pv_time_self_block_state_get_block(&state);
	if ((last_self_unblock == 0U) && (block == self_block)) {
		// Thread has been woken after blocking itself, or is becoming
		// runnable for the first time.
		TRACE(DEBUG, INFO, "arm_pv_time: unblocking {:#x}",
		      (uintptr_t)thread);
		arm_pv_time_self_block_state_t new_state =
			arm_pv_time_self_block_state_default();
		arm_pv_time_self_block_state_set_last_unblocked(
			&new_state, platform_timer_get_current_ticks());
		atomic_store_relaxed(&thread->arm_pv_time.self_block,
				     new_state);
	}
}

```

`hyp/vm/arm_vm_amu/aarch64/arm_vm_amu_aarch64.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

module arm_vm_amu

#if defined(ARCH_ARM_FEAT_AMUv1) || defined(ARCH_ARM_FEAT_AMUv1p1)
subscribe boot_cpu_cold_init

subscribe vcpu_activate_thread

subscribe thread_context_switch_pre(next)

subscribe thread_context_switch_post(prev)

// The AMU counters are read often, give them a higher priority
subscribe vcpu_trap_sysreg_read
	priority 10

subscribe vcpu_trap_sysreg_write
#endif

```

`hyp/vm/arm_vm_amu/aarch64/src/arm_vm_amu.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>

#include <hypregisters.h>

#include <compiler.h>
#include <cpulocal.h>
#include <panic.h>
#include <preempt.h>
#include <scheduler.h>
#include <vcpu.h>

#include <asm/barrier.h>
#include <asm/sysregs.h>
#include <asm/system_registers.h>

#include "arm_vm_amu.h"
#include "event_handlers.h"

// The design:
// Only HLOS is given access to the AMU component. However, HLOS should not see
// how much the counters increment during the execution of the sensitive VMs.
//
// Unfortunately, only the highest EL can write to the AMU control registers and
// counters; therefore we can't protect against the AMU cross-exposure
// by simply disabling the counters dynamically or context switching them.
//
// Therefore we use a set of CPU-local variables to keep track of how much each
// counter increments during the sensitive threads. We do this by subtracting
// the counter value from our variable before switching to a sensitive thread,
// and adding the counter value when switching away from it.
//
// All the AMU accesses from HLOS are trapped. When HLOS tries to read a counter
// we return the hardware value minus our internal offset from above.
// The AMU counters take centuries to overflow, so arithmetic overflows are not
// a concern.
//
// This is not needed for counter 1 ("constant frequency cycles" counter),
// which is essentially defined the same as the ARM physical counter and
// virtualising it does not provide any additional security.

#if defined(ARCH_ARM_FEAT_AMUv1p1)
#error Implement the AMU virtual offset registers
#error Investigate the need to support non-consecutive auxiliary counters
#endif

#if defined(ARCH_ARM_FEAT_AMUv1) || defined(ARCH_ARM_FEAT_AMUv1p1)
CPULOCAL_DECLARE_STATIC(uint64_t, amu_counter_offsets)[PLATFORM_AMU_CNT_NUM];
CPULOCAL_DECLARE_STATIC(uint64_t, amu_aux_counter_offsets)
[PLATFORM_AMU_AUX_CNT_NUM];

void
arm_vm_amu_handle_boot_cpu_cold_init(cpu_index_t cpu_index)
{
	uint64_t *amu_counter_offsets =
		CPULOCAL_BY_INDEX(amu_counter_offsets, cpu_index);
	uint64_t *amu_aux_counter_offsets =
		CPULOCAL_BY_INDEX(amu_aux_counter_offsets, cpu_index);

	for (index_t i = 0; i < PLATFORM_AMU_CNT_NUM; i++) {
		amu_counter_offsets[i] = 0;
	}
	for (index_t i = 0; i < PLATFORM_AMU_AUX_CNT_NUM; i++) {
		amu_aux_counter_offsets[i] = 0;
	}

	AMCFGR_EL0_t amcfgr = register_AMCFGR_EL0_read();
	AMCGCR_EL0_t amcgcr = register_AMCGCR_EL0_read();

#if defined(ARCH_ARM_FEAT_AMUv1p1)
#error TODO: Check the AMU counter bitmap
#else
	if ((AMCFGR_EL0_get_N(&amcfgr) + 1U) !=
	    (PLATFORM_AMU_CNT_NUM + PLATFORM_AMU_AUX_CNT_NUM)) {
		panic("Incorrect CPU AMU count");
	}
#endif
	if ((AMCGCR_EL0_get_CG0NC(&amcgcr) != PLATFORM_AMU_CNT_NUM) ||
	    (AMCGCR_EL0_get_CG1NC(&amcgcr) != PLATFORM_AMU_AUX_CNT_NUM)) {
		panic("Incorrect CPU AMU group counts");
	}
}

bool
arm_vm_amu_handle_vcpu_activate_thread(thread_t		  *thread,
				       vcpu_option_flags_t options)
{
	assert(thread != NULL);
	assert(thread->kind == THREAD_KIND_VCPU);

	// Trap accesses to AMU registers. For HLOS we will emulate
	// them, for the rest of the VMs we will leave them unhandled
	// and inject an abort.
	CPTR_EL2_E2H1_set_TAM(&thread->vcpu_regs_el2.cptr_el2, true);

	vcpu_option_flags_set_amu_counting_disabled(
		&thread->vcpu_options,
		vcpu_option_flags_get_amu_counting_disabled(&options));

	return true;
}

error_t
arm_vm_amu_handle_thread_context_switch_pre(thread_t *next)
{
	// If about to switch to a sensitive thread, take a snapshot of the AMU
	// counters by subtracting them from the offsets.
	// In theory it is not necessary to do this if we are coming from
	// another sensitive thread, but adding the required extra checks will
	// likely degrade the performance as this will be a rare occurrence.
	if (compiler_unexpected((next->kind == THREAD_KIND_VCPU) &&
				(vcpu_option_flags_get_amu_counting_disabled(
					&next->vcpu_options)))) {
		cpulocal_begin();
		arm_vm_amu_subtract_counters(&CPULOCAL(amu_counter_offsets));
		arm_vm_amu_subtract_aux_counters(
			&CPULOCAL(amu_aux_counter_offsets));
		cpulocal_end();
	}

	return OK;
}

void
arm_vm_amu_handle_thread_context_switch_post(thread_t *prev)
{
	// If about to switch away from a sensitive thread, take a snapshot of
	// the AMU counters by adding them to the offsets.
	// In theory it is not necessary to do this if we are switching to
	// another sensitive thread, but adding the required extra checks will
	// likely degrade the performance as this will be a rare occurrence.
	if (compiler_unexpected((prev->kind == THREAD_KIND_VCPU) &&
				(vcpu_option_flags_get_amu_counting_disabled(
					&prev->vcpu_options)))) {
		cpulocal_begin();
		arm_vm_amu_add_counters(&CPULOCAL(amu_counter_offsets));
		arm_vm_amu_add_aux_counters(&CPULOCAL(amu_aux_counter_offsets));
		cpulocal_end();
	}
}

static vcpu_trap_result_t
arm_vm_amu_get_event_register(ESR_EL2_ISS_MSR_MRS_t iss, uint64_t *val)
{
	vcpu_trap_result_t ret;
	uint8_t		   opc0 = ESR_EL2_ISS_MSR_MRS_get_Op0(&iss);
	uint8_t		   opc1 = ESR_EL2_ISS_MSR_MRS_get_Op1(&iss);
	uint8_t		   crn	= ESR_EL2_ISS_MSR_MRS_get_CRn(&iss);

	if ((opc0 == 3U) && (opc1 == 3U) && (crn == 13U)) {
		uint8_t crm   = ESR_EL2_ISS_MSR_MRS_get_CRm(&iss);
		uint8_t opc2  = ESR_EL2_ISS_MSR_MRS_get_Op2(&iss);
		uint8_t index = (uint8_t)((crm & 1U) << 2U) | opc2;

		if (((crm == 4U) || (crm == 5U)) &&
		    (index < PLATFORM_AMU_CNT_NUM)) {
			// Event counter registers
			cpulocal_begin();
			uint64_t *offsets = CPULOCAL(amu_counter_offsets);
			*val		  = arm_vm_amu_get_counter(index);
			if (index != 1U) {
				// Adjust the counter value
				*val -= offsets[index];
			}
			cpulocal_end();

			ret = VCPU_TRAP_RESULT_EMULATED;
		} else if (((crm == 6U) || (crm == 7U)) &&
			   (index < PLATFORM_AMU_CNT_NUM)) {
			// Event type registers
			*val = arm_vm_amu_get_event_type(index);

			ret = VCPU_TRAP_RESULT_EMULATED;
		} else if (((crm == 12U) || (crm == 13U)) &&
			   (index < PLATFORM_AMU_AUX_CNT_NUM)) {
			// Auxiliary event counter registers
			cpulocal_begin();
			uint64_t *offsets = CPULOCAL(amu_aux_counter_offsets);
			*val		  = arm_vm_amu_get_aux_counter(index);
			// Adjust the counter value
			*val -= offsets[index];
			cpulocal_end();

			ret = VCPU_TRAP_RESULT_EMULATED;
		} else if (((crm == 14U) || (crm == 15U)) &&
			   (index < PLATFORM_AMU_AUX_CNT_NUM)) {
			// Auxiliary event type registers
			*val = arm_vm_amu_get_aux_event_type(index);

			ret = VCPU_TRAP_RESULT_EMULATED;
		} else {
			// Not an AMU event register
			ret = VCPU_TRAP_RESULT_UNHANDLED;
		}
	} else {
		ret = VCPU_TRAP_RESULT_UNHANDLED;
	}

	return ret;
}

vcpu_trap_result_t
arm_vm_amu_handle_vcpu_trap_sysreg_read(ESR_EL2_ISS_MSR_MRS_t iss)
{
	register_t	   val	  = 0U;
	vcpu_trap_result_t ret	  = VCPU_TRAP_RESULT_EMULATED;
	thread_t	  *thread = thread_get_self();

	if (!vcpu_option_flags_get_hlos_vm(&thread->vcpu_options)) {
		// Only HLOS is allowed to read the AMU registers
		ret = VCPU_TRAP_RESULT_UNHANDLED;
		goto out;
	}

	// Assert this is a read
	assert(ESR_EL2_ISS_MSR_MRS_get_Direction(&iss));

	uint8_t reg_num = ESR_EL2_ISS_MSR_MRS_get_Rt(&iss);

	// Remove the fields that are not used in the comparison
	ESR_EL2_ISS_MSR_MRS_t temp_iss = iss;
	ESR_EL2_ISS_MSR_MRS_set_Rt(&temp_iss, 0U);
	ESR_EL2_ISS_MSR_MRS_set_Direction(&temp_iss, false);

	switch (ESR_EL2_ISS_MSR_MRS_raw(temp_iss)) {
	case ISS_MRS_MSR_AMCR_EL0: {
		AMCR_EL0_t amcr = AMCR_EL0_default();
		val		= AMCR_EL0_raw(amcr);
		break;
	}
	case ISS_MRS_MSR_AMCFGR_EL0: {
		AMCFGR_EL0_t amcfgr    = AMCFGR_EL0_default();
		AMCFGR_EL0_t amcfgr_hw = register_AMCFGR_EL0_read();

		AMCFGR_EL0_copy_HDBG(&amcfgr, &amcfgr_hw);
		AMCFGR_EL0_set_Size(&amcfgr, 63U);
		// With traps, it is possible to virtualise the number of HW
		// counters; return the number of emulated counters.
		AMCFGR_EL0_set_N(&amcfgr,
				 (uint16_t)(PLATFORM_AMU_CNT_NUM +
					    PLATFORM_AMU_AUX_CNT_NUM - 1U));
		AMCFGR_EL0_set_NCG(&amcfgr,
				   PLATFORM_AMU_AUX_CNT_NUM > 0U ? 1U : 0U);
		val = AMCFGR_EL0_raw(amcfgr);
		break;
	}
	case ISS_MRS_MSR_AMCGCR_EL0: {
		AMCGCR_EL0_t amcgcr = AMCGCR_EL0_default();

		// With traps, it is possible to virtualise the number of HW
		// counters; return the number of emulated counters.
		AMCGCR_EL0_set_CG0NC(&amcgcr, PLATFORM_AMU_CNT_NUM);
		AMCGCR_EL0_set_CG1NC(&amcgcr, PLATFORM_AMU_AUX_CNT_NUM);
		val = AMCGCR_EL0_raw(amcgcr);
		break;
	}
	case ISS_MRS_MSR_AMUSERENR_EL0:
		val = register_AMUSERENR_EL0_read();
		break;
#if defined(ARCH_ARM_FEAT_AMUv1p1)
	case ISS_MRS_MSR_AMCG1IDR_EL0:
		val = register_AMCG1IDR_EL0_read();
		break;
#endif
	default:
		ret = arm_vm_amu_get_event_register(iss, &val);
		break;
	}

	// Update the thread's register
	if (ret == VCPU_TRAP_RESULT_EMULATED) {
		vcpu_gpr_write(thread, reg_num, val);
	}

out:
	return ret;
}

vcpu_trap_result_t
arm_vm_amu_handle_vcpu_trap_sysreg_write(ESR_EL2_ISS_MSR_MRS_t iss)
{
	vcpu_trap_result_t ret	  = VCPU_TRAP_RESULT_UNHANDLED;
	thread_t	  *thread = thread_get_self();

	if (!vcpu_option_flags_get_hlos_vm(&thread->vcpu_options)) {
		// Only HLOS is allowed to modify the AMU registers
		goto out;
	}

	// Assert this is a write
	assert(!ESR_EL2_ISS_MSR_MRS_get_Direction(&iss));

	// Read the thread's register
	uint8_t	   reg_num = ESR_EL2_ISS_MSR_MRS_get_Rt(&iss);
	register_t val	   = vcpu_gpr_read(thread, reg_num);

	// Remove the fields that are not used in the comparison
	ESR_EL2_ISS_MSR_MRS_t temp_iss = iss;
	ESR_EL2_ISS_MSR_MRS_set_Rt(&temp_iss, 0U);
	ESR_EL2_ISS_MSR_MRS_set_Direction(&temp_iss, false);

	// Only AMUSERNR_EL0 may be written by the guest
	switch (ESR_EL2_ISS_MSR_MRS_raw(temp_iss)) {
	case ISS_MRS_MSR_AMUSERENR_EL0: {
		SPSR_EL2_A64_t	  spsr_el2 = thread->vcpu_regs_gpr.spsr_el2.a64;
		spsr_64bit_mode_t spsr_m   = SPSR_EL2_A64_get_M(&spsr_el2);
		// A trap from EL0 means a HW bug
		assert((spsr_m & 0xfU) != 0U);

		register_AMUSERENR_EL0_write(val);
		ret = VCPU_TRAP_RESULT_EMULATED;
		break;
	}
	default:
		// Do Nothing
		break;
	}

out:
	return ret;
}
#endif

```

`hyp/vm/arm_vm_amu/aarch64/templates/arm_vm_amu_counter_regs.c.tmpl`:

```tmpl
// Automatically generated. Do not modify.
//
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

\#include <assert.h>
\#include <hyptypes.h>

\#include <hypregisters.h>

\#include <compiler.h>
\#include <log.h>
\#include <trace.h>

\#include <asm/barrier.h>
\#include <asm/sysregs.h>

\#include "arm_vm_amu.h"

\#if defined(ARCH_ARM_FEAT_AMUv1) || defined(ARCH_ARM_FEAT_AMUv1p1)
#set $cnt_num = $PLATFORM_AMU_CNT_NUM
#set $aux_cnt_num = $PLATFORM_AMU_AUX_CNT_NUM

uint64_t
arm_vm_amu_get_counter(index_t index)
{
	uint64_t val;

	switch (index)
	{
#for i in range(0, $cnt_num)
	case ${i}:
		sysreg64_read_ordered(AMEVCNTR0${i}_EL0, val, asm_ordering);
	break;
#end for
	default:
		TRACE_AND_LOG(DEBUG, WARN,
			      "Read of non-existing AMU counter {:d} ", index);
		val = 0;
		break;
	}

	return val;

}

uint64_t
arm_vm_amu_get_aux_counter(index_t index)
{
	uint64_t val;

	switch (index)
	{
#for i in range(0, $aux_cnt_num)
	case ${i}:
		sysreg64_read_ordered(AMEVCNTR1${i}_EL0, val, asm_ordering);
	break;
#end for
	default:
		TRACE_AND_LOG(
			DEBUG, WARN,
			"Read of non-existing AMU auxiliary counter {:d} ",
			index);
		val = 0;
		break;
	}

	return val;

}

uint64_t
arm_vm_amu_get_event_type(index_t index)
{
	uint64_t val;

	switch (index)
	{
#for i in range(0, $cnt_num)
	case ${i}:
		sysreg64_read_ordered(AMEVTYPER0${i}_EL0, val, asm_ordering);
	break;
#end for
	default:
		TRACE_AND_LOG(DEBUG, WARN,
			      "Read of non-existing AMU event type {:d} ",
			      index);
		val = 0;
		break;
	}

	return val;

}

uint64_t
arm_vm_amu_get_aux_event_type(index_t index)
{
	uint64_t val;

	switch (index)
	{
#for i in range(0, $aux_cnt_num)
	case ${i}:
		sysreg64_read_ordered(AMEVTYPER1${i}_EL0, val, asm_ordering);
	break;
#end for
	default:
		TRACE_AND_LOG(
			DEBUG, WARN,
			"Read of non-existing AMU auxiliary event type {:d} ",
			index);
		val = 0;
		break;
	}

	return val;

}

void
arm_vm_amu_add_counters(arm_vm_amu_offsets_t *offsets)
{
	uint64_t val;

#for i in range(0, $cnt_num)
#if i != 1
	sysreg64_read_ordered(AMEVCNTR0${i}_EL0, val, asm_ordering);
	(*offsets)[${i}] += val;
#end if
#end for
}

void
arm_vm_amu_subtract_counters(arm_vm_amu_offsets_t *offsets)
{
	uint64_t val;

#for i in range(0, $cnt_num)
#if i != 1
	sysreg64_read_ordered(AMEVCNTR0${i}_EL0, val, asm_ordering);
	(*offsets)[${i}] -= val;
#end if
#end for
}

void
arm_vm_amu_add_aux_counters(arm_vm_amu_aux_offsets_t *offsets)
{
	uint64_t val;

#for i in range(0, $aux_cnt_num)
	sysreg64_read_ordered(AMEVCNTR1${i}_EL0, val, asm_ordering);
	(*offsets)[${i}] += val;
#end for
}

void
arm_vm_amu_subtract_aux_counters(arm_vm_amu_aux_offsets_t *offsets)
{
	uint64_t val;

#for i in range(0, $aux_cnt_num)
	sysreg64_read_ordered(AMEVCNTR1${i}_EL0, val, asm_ordering);
	(*offsets)[${i}] -= val;
#end for
}
\#endif

```

`hyp/vm/arm_vm_amu/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

local_include
arch_events aarch64 arm_vm_amu_aarch64.ev
arch_source aarch64 arm_vm_amu.c
arch_template simple aarch64 arm_vm_amu_counter_regs.c.tmpl

```

`hyp/vm/arm_vm_amu/include/arm_vm_amu.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#if defined(ARCH_ARM_FEAT_AMUv1) || defined(ARCH_ARM_FEAT_AMUv1p1)
typedef uint64_t arm_vm_amu_offsets_t[PLATFORM_AMU_CNT_NUM];
typedef uint64_t arm_vm_amu_aux_offsets_t[PLATFORM_AMU_AUX_CNT_NUM];

uint64_t
arm_vm_amu_get_counter(index_t index);

uint64_t
arm_vm_amu_get_aux_counter(index_t index);

uint64_t
arm_vm_amu_get_event_type(index_t index);

uint64_t
arm_vm_amu_get_aux_event_type(index_t index);

void
arm_vm_amu_add_counters(arm_vm_amu_offsets_t *offsets);

void
arm_vm_amu_subtract_counters(arm_vm_amu_offsets_t *offsets);

void
arm_vm_amu_add_aux_counters(arm_vm_amu_aux_offsets_t *offsets);

void
arm_vm_amu_subtract_aux_counters(arm_vm_amu_aux_offsets_t *offsets);
#endif

```

`hyp/vm/arm_vm_pmu/aarch64/arm_vm_pmu_aarch64.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

module arm_vm_pmu

subscribe object_activate_thread
	handler arm_vm_pmu_aarch64_handle_object_activate_thread

// The PMU counters are read often, give them a higher priority
subscribe vcpu_trap_sysreg_read
	handler arm_vm_pmu_handle_vcpu_trap_sysreg_access
	priority 5

subscribe vcpu_trap_sysreg_write
	handler arm_vm_pmu_handle_vcpu_trap_sysreg_access

```

`hyp/vm/arm_vm_pmu/aarch64/arm_vm_pmu_aarch64.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

define vcpu_pmu_registers structure {
	pmcr_el0	bitfield PMCR_EL0;
	pmselr_el0	uint64;
	pmccntr_el0	uint64;
	pmovsset_el0	uint32;
	pmuserenr_el0	uint32;
	pmccfiltr_el0	uint32;
	pmintenset_el1	uint32;
	pmcntenset_el0	uint32;
#if defined(ARCH_ARM_FEAT_PMUv3p5)
	pmevcntr_el0	array(PLATFORM_PMU_CNT_NUM) uint64;
	pmevtyper_el0	array(PLATFORM_PMU_CNT_NUM) uint64;
#else
	pmevcntr_el0	array(PLATFORM_PMU_CNT_NUM) uint32;
	pmevtyper_el0	array(PLATFORM_PMU_CNT_NUM) uint32;
#endif
};

define pmu structure {
	pmu_regs	structure vcpu_pmu_registers;
};

extend thread object {
	pmu		structure pmu(contained);
};

```

`hyp/vm/arm_vm_pmu/aarch64/include/arm_vm_pmu_event_regs.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

void
arm_vm_pmu_load_counters_state(thread_t *thread);

void
arm_vm_pmu_save_counters_state(thread_t *thread);

```

`hyp/vm/arm_vm_pmu/aarch64/src/arm_vm_pmu.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>

#include <hypcontainers.h>
#include <hypregisters.h>

#include <atomic.h>
#include <compiler.h>
#include <cpulocal.h>
#include <irq.h>
#include <panic.h>
#include <preempt.h>
#include <vcpu.h>
#include <virq.h>

#include <asm/barrier.h>
#include <asm/sysregs.h>
#include <asm/system_registers.h>

#include "arm_vm_pmu.h"
#include "arm_vm_pmu_event_regs.h"
#include "event_handlers.h"
#include "platform_pmu.h"

// Design: "semi-lazy" context-switching. The aim is to context switch the PMU
// registers only when the VM is actively using PMU (a PMU register is accessed
// or at least one counter is enabled). This way if Linux accesses PMU only at
// boot time and never again, we won't be context-switching its PMU registers
// for the rest of its lifetime.
// All threads will initially have access to PMU registers disabled (MDCR_EL2).
// When a PMU access is trapped, the thread will be given access to PMU for the
// time-slice. When this thread is context-switched out, its PMU registers will
// be saved.
// When switching to a thread, the thread's state of PMU counters is checked to
// see if the thread is actively using PMU. If yes, the thread will be given PMU
// access for the time-slice and its PMU context is loaded. If no, the PMU
// access traps are enabled until the next access happens, as explained above.

// Debugger considerations:
// According to the PSCI specification bit 2 of DBGCLAIM says whether PMU is
// being used by the external debuggers. We need to investigate how this affects
// the context switching of the PMU registers. Currently it looks like Linux
// does not actually comply with this standard anyway, except for writing the
// debugger claim bits in the statistical profiling driver.
// FIXME:

#if (ARCH_ARM_PMU_VER < 3)
#error Only PMUv3 and above can be implemented in ARMv8/ARMv9.
#endif

static bool
arm_vm_pmu_counters_enabled(thread_t *current)
{
	// Check if the global enable flag is set and at least one counter is
	// enabled.
	return (PMCR_EL0_get_E(&current->pmu.pmu_regs.pmcr_el0) &&
		(current->pmu.pmu_regs.pmcntenset_el0 != 0U));
}

static bool
arm_vm_pmu_is_el1_trap_enabled(thread_t *current)
{
	MDCR_EL2_t mdcr = current->vcpu_regs_el2.mdcr_el2;
	return (MDCR_EL2_get_TPM(&mdcr) || MDCR_EL2_get_TPMCR(&mdcr));
}

static void
arm_vm_pmu_el1_trap_set_enable(thread_t *current, bool enable)
{
	MDCR_EL2_set_TPM(&current->vcpu_regs_el2.mdcr_el2, enable);
	MDCR_EL2_set_TPMCR(&current->vcpu_regs_el2.mdcr_el2, enable);
	register_MDCR_EL2_write(current->vcpu_regs_el2.mdcr_el2);
}

error_t
arm_vm_pmu_aarch64_handle_object_activate_thread(thread_t *thread)
{
	// Set the correct number of event counters
	PMCR_EL0_t pmcr_el0 = register_PMCR_EL0_read();
	MDCR_EL2_set_HPMN(&thread->vcpu_regs_el2.mdcr_el2,
			  PMCR_EL0_get_N(&pmcr_el0));
#if defined(ARCH_ARM_FEAT_PMUv3p1)
	// Prohibit event counting at EL2
	MDCR_EL2_set_HPMD(&thread->vcpu_regs_el2.mdcr_el2, true);
#endif

	// Enable PMU access traps
	MDCR_EL2_set_TPM(&thread->vcpu_regs_el2.mdcr_el2, true);
	MDCR_EL2_set_TPMCR(&thread->vcpu_regs_el2.mdcr_el2, true);

	return OK;
}

static void
arm_vm_pmu_save_state(thread_t *thread)
{
	sysreg64_read_ordered(PMINTENSET_EL1,
			      thread->pmu.pmu_regs.pmintenset_el1,
			      asm_ordering);
	sysreg64_read_ordered(PMCNTENSET_EL0,
			      thread->pmu.pmu_regs.pmcntenset_el0,
			      asm_ordering);

	thread->pmu.pmu_regs.pmcr_el0 =
		register_PMCR_EL0_read_ordered(&asm_ordering);
	sysreg64_read_ordered(PMCCNTR_EL0, thread->pmu.pmu_regs.pmccntr_el0,
			      asm_ordering);
	sysreg64_read_ordered(PMSELR_EL0, thread->pmu.pmu_regs.pmselr_el0,
			      asm_ordering);
	sysreg64_read_ordered(PMUSERENR_EL0, thread->pmu.pmu_regs.pmuserenr_el0,
			      asm_ordering);
	sysreg64_read_ordered(PMCCFILTR_EL0, thread->pmu.pmu_regs.pmccfiltr_el0,
			      asm_ordering);

	arm_vm_pmu_save_counters_state(thread);

	sysreg64_read_ordered(PMOVSSET_EL0, thread->pmu.pmu_regs.pmovsset_el0,
			      asm_ordering);

#if !defined(ARCH_ARM_FEAT_PMUv3p1)
	// Event counting cannot be prohibited at EL2. Do an ISB to make sure
	// the operation above completes before we continue. This to ensure that
	// the register reads above are not delayed until after some sensitive
	// operation.
	asm_context_sync_ordered(&asm_ordering);
#endif
}

static void
arm_vm_pmu_load_state(thread_t *thread)
{
	arm_vm_pmu_load_counters_state(thread);

	sysreg64_write_ordered(PMINTENCLR_EL1,
			       ~thread->pmu.pmu_regs.pmintenset_el1,
			       asm_ordering);
	sysreg64_write_ordered(PMINTENSET_EL1,
			       thread->pmu.pmu_regs.pmintenset_el1,
			       asm_ordering);

	sysreg64_write_ordered(PMOVSCLR_EL0, ~thread->pmu.pmu_regs.pmovsset_el0,
			       asm_ordering);
	sysreg64_write_ordered(PMOVSSET_EL0, thread->pmu.pmu_regs.pmovsset_el0,
			       asm_ordering);

	register_PMCR_EL0_write_ordered(thread->pmu.pmu_regs.pmcr_el0,
					&asm_ordering);
	sysreg64_write_ordered(PMCCNTR_EL0, thread->pmu.pmu_regs.pmccntr_el0,
			       asm_ordering);
	sysreg64_write_ordered(PMSELR_EL0, thread->pmu.pmu_regs.pmselr_el0,
			       asm_ordering);
	sysreg64_write_ordered(PMUSERENR_EL0,
			       thread->pmu.pmu_regs.pmuserenr_el0,
			       asm_ordering);
	sysreg64_write_ordered(PMCCFILTR_EL0,
			       thread->pmu.pmu_regs.pmccfiltr_el0,
			       asm_ordering);

	sysreg64_write_ordered(PMCNTENCLR_EL0,
			       ~thread->pmu.pmu_regs.pmcntenset_el0,
			       asm_ordering);
	sysreg64_write_ordered(PMCNTENSET_EL0,
			       thread->pmu.pmu_regs.pmcntenset_el0,
			       asm_ordering);
}

void
arm_vm_pmu_handle_thread_save_state(void)
{
	thread_t *thread = thread_get_self();

	if (compiler_expected(thread->kind == THREAD_KIND_VCPU) &&
	    compiler_unexpected(!arm_vm_pmu_is_el1_trap_enabled(thread))) {
		// PMU access was enabled for this timeslice, save the state
		arm_vm_pmu_save_state(thread);
	}
}

void
arm_vm_pmu_handle_thread_context_switch_post(void)
{
	thread_t *thread = thread_get_self();

	if (compiler_expected(thread->kind == THREAD_KIND_VCPU)) {
		bool_result_t asserted = virq_query(&thread->pmu.pmu_virq_src);
		if ((asserted.e == OK) && !asserted.r) {
			platform_pmu_hw_irq_deactivate();
		}

		// If the thread is actively using PMU, grant it access
		if (arm_vm_pmu_counters_enabled(thread)) {
			MDCR_EL2_set_TPM(&thread->vcpu_regs_el2.mdcr_el2,
					 false);
			MDCR_EL2_set_TPMCR(&thread->vcpu_regs_el2.mdcr_el2,
					   false);
		} else {
			MDCR_EL2_set_TPM(&thread->vcpu_regs_el2.mdcr_el2, true);
			MDCR_EL2_set_TPMCR(&thread->vcpu_regs_el2.mdcr_el2,
					   true);
		}
	}
}

void
arm_vm_pmu_handle_thread_load_state(void)
{
	thread_t *thread = thread_get_self();

	if (compiler_unexpected(thread->kind != THREAD_KIND_VCPU)) {
		// Idle thread. Turn off the counters and the interrupts.
		sysreg64_write_ordered(PMINTENCLR_EL1, ~0UL, asm_ordering);
		sysreg64_write_ordered(PMCNTENCLR_EL0, ~0UL, asm_ordering);
	} else if (compiler_unexpected(arm_vm_pmu_counters_enabled(thread))) {
		// The thread is actively using PMU. The context_switch_post
		// has already disabled traps for this thread above, and it will
		// get loaded into MDCR_EL2 during the context switch load
		// process in the generic module. Load its PMU context here.
		arm_vm_pmu_load_state(thread);
	} else {
		// The thread is not actively using PMU. The context_switch_post
		// has already enabled traps for this thread above, and it will
		// get loaded into MDCR_EL2 during the context switch load
		// process in the generic module. If it tries to access PMU,
		// in the trap handler we load its context and give it access.
		//
		// No need to sanitise the PMU registers (even though they might
		// have the values from the old thread), because this thread
		// doesn't currently have access to thems.
		//
		// Turn off the counters and the interrupts.
		sysreg64_write_ordered(PMINTENCLR_EL1, ~0UL, asm_ordering);
		sysreg64_write_ordered(PMCNTENCLR_EL0, ~0UL, asm_ordering);
	}
}

bool
arm_vm_pmu_handle_virq_check_pending(virq_source_t *source)
{
	bool ret = true;

	pmu_t	 *pmu	 = pmu_container_of_pmu_virq_src(source);
	thread_t *thread = thread_container_of_pmu(pmu);
	assert(thread != NULL);

	if (thread == thread_get_self()) {
		ret = platform_pmu_is_hw_irq_pending();

		if (!ret) {
			platform_pmu_hw_irq_deactivate();
		}
	}
	return ret;
}

void
arm_vm_pmu_handle_platform_pmu_counter_overflow(void)
{
	thread_t *thread = thread_get_self();

	(void)virq_assert(&thread->pmu.pmu_virq_src, false);
}

vcpu_trap_result_t
arm_vm_pmu_handle_vcpu_trap_sysreg_access(ESR_EL2_ISS_MSR_MRS_t iss)
{
	vcpu_trap_result_t ret	  = VCPU_TRAP_RESULT_UNHANDLED;
	thread_t	  *thread = thread_get_self();

	// Remove the fields that are not used in the comparison
	ESR_EL2_ISS_MSR_MRS_t temp_iss = iss;
	ESR_EL2_ISS_MSR_MRS_set_Rt(&temp_iss, 0U);
	ESR_EL2_ISS_MSR_MRS_set_Direction(&temp_iss, false);

	switch (ESR_EL2_ISS_MSR_MRS_raw(temp_iss)) {
	case ISS_MRS_MSR_PMCR_EL0:
	case ISS_MRS_MSR_PMCNTENSET_EL0:
	case ISS_MRS_MSR_PMCNTENCLR_EL0:
	case ISS_MRS_MSR_PMOVSCLR_EL0:
	case ISS_MRS_MSR_PMSWINC_EL0:
	case ISS_MRS_MSR_PMSELR_EL0:
	case ISS_MRS_MSR_PMCEID0_EL0:
	case ISS_MRS_MSR_PMCEID1_EL0:
	case ISS_MRS_MSR_PMCCNTR_EL0:
	case ISS_MRS_MSR_PMXEVTYPER_EL0:
	case ISS_MRS_MSR_PMXEVCNTR_EL0:
	case ISS_MRS_MSR_PMUSERENR_EL0:
	case ISS_MRS_MSR_PMINTENSET_EL1:
	case ISS_MRS_MSR_PMINTENCLR_EL1:
	case ISS_MRS_MSR_PMOVSSET_EL0:
	case ISS_MRS_MSR_PMCCFILTR_EL0:
		ret = VCPU_TRAP_RESULT_RETRY;
		break;
	default: {
		uint8_t opc0, opc1, crn, crm;

		opc0 = ESR_EL2_ISS_MSR_MRS_get_Op0(&iss);
		opc1 = ESR_EL2_ISS_MSR_MRS_get_Op1(&iss);
		crn  = ESR_EL2_ISS_MSR_MRS_get_CRn(&iss);
		crm  = ESR_EL2_ISS_MSR_MRS_get_CRm(&iss);

		if ((opc0 == 3U) && (opc1 == 3U) && (crn == 14U) &&
		    (crm >= 8U)) {
			// PMEVCNTR and PMEVTYPER registers
			ret = VCPU_TRAP_RESULT_RETRY;
		}
		break;
	}
	}

	if (ret == VCPU_TRAP_RESULT_RETRY) {
		// The thread is trying to access PMU. Allow access for this
		// time-slice by disabling the PMU traps.
		arm_vm_pmu_el1_trap_set_enable(thread, false);
		// If The thread has already accessed PMU in the past, load its
		// PMU state. Otherwise the load below acts as a sanitiser.
		arm_vm_pmu_load_state(thread);
	}

	return ret;
}

```

`hyp/vm/arm_vm_pmu/aarch64/templates/arm_vm_pmu_event_regs.c.tmpl`:

```tmpl
// Automatically generated. Do not modify.
//
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

\#include <assert.h>
\#include <hyptypes.h>

\#include <hypregisters.h>

\#include <asm/barrier.h>
\#include <asm/sysregs.h>

\#include "arm_vm_pmu_event_regs.h"

#set $num = $PLATFORM_PMU_CNT_NUM

void
arm_vm_pmu_load_counters_state(thread_t *thread)
{
#for i in range(0, $num)
	sysreg64_write_ordered(PMEVCNTR${i}_EL0,
			       thread->pmu.pmu_regs.pmevcntr_el0[${i}],
			       asm_ordering);
	sysreg64_write_ordered(PMEVTYPER${i}_EL0,
			       thread->pmu.pmu_regs.pmevtyper_el0[${i}],
			       asm_ordering);

#end for
}

void
arm_vm_pmu_save_counters_state(thread_t *thread)
{
#for i in range(0, $num)
	sysreg64_read_ordered(PMEVCNTR${i}_EL0,
			       thread->pmu.pmu_regs.pmevcntr_el0[${i}],
				      asm_ordering);
	sysreg64_read_ordered(PMEVTYPER${i}_EL0,
			       thread->pmu.pmu_regs.pmevtyper_el0[${i}],
				      asm_ordering);

#end for
}

```

`hyp/vm/arm_vm_pmu/arm_vm_pmu.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

module arm_vm_pmu

subscribe thread_save_state()

subscribe thread_load_state()

subscribe thread_context_switch_post()
	require_preempt_disabled

subscribe object_activate_thread
	unwinder arm_vm_pmu_handle_object_deactivate_thread(thread)

subscribe object_deactivate_thread

subscribe platform_pmu_counter_overflow

subscribe virq_check_pending[VIRQ_TRIGGER_PMU](source)
	require_preempt_disabled

```

`hyp/vm/arm_vm_pmu/arm_vm_pmu.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

extend pmu structure {
	pmu_virq_src	structure virq_source(contained);
};

extend virq_trigger enumeration {
	pmu;
};

```

`hyp/vm/arm_vm_pmu/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

types arm_vm_pmu.tc
arch_types aarch64 arm_vm_pmu_aarch64.tc
events arm_vm_pmu.ev
arch_events aarch64 arm_vm_pmu_aarch64.ev
local_include
arch_local_include aarch64
arch_template simple aarch64 arm_vm_pmu_event_regs.c.tmpl
source arm_vm_pmu.c
arch_source aarch64 arm_vm_pmu.c

```

`hyp/vm/arm_vm_pmu/include/arm_vm_pmu.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

```

`hyp/vm/arm_vm_pmu/src/arm_vm_pmu.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>

#include <hypregisters.h>

#include <irq.h>
#include <timer_queue.h>
#include <vic.h>
#include <virq.h>

#include <asm/barrier.h>

#include "arm_vm_pmu.h"
#include "event_handlers.h"

error_t
arm_vm_pmu_handle_object_activate_thread(thread_t *thread)
{
	error_t ret = OK;

	if (thread->kind == THREAD_KIND_VCPU) {
		ret = vic_bind_private_vcpu(&thread->pmu.pmu_virq_src, thread,
					    PLATFORM_VM_PMU_IRQ,
					    VIRQ_TRIGGER_PMU);
	}

	return ret;
}

void
arm_vm_pmu_handle_object_deactivate_thread(thread_t *thread)
{
	if (thread->kind == THREAD_KIND_VCPU) {
		vic_unbind(&thread->pmu.pmu_virq_src);
	}
}

```

`hyp/vm/arm_vm_sve_simple/aarch64/arm_vm_sve_aarch64.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

module arm_vm_sve_simple

#if defined(ARCH_ARM_FEAT_SVE)
subscribe boot_runtime_first_init
	handler arm_vm_sve_simple_handle_boot_runtime_init()
subscribe boot_runtime_warm_init
	handler arm_vm_sve_simple_handle_boot_runtime_init()
subscribe boot_cold_init()
subscribe boot_cpu_warm_init()

subscribe rootvm_init(qcbor_enc_ctxt)

subscribe vcpu_activate_thread

subscribe vcpu_trap_sysreg_read
#endif

```

`hyp/vm/arm_vm_sve_simple/aarch64/arm_vm_sve_aarch64.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#if defined(ARCH_ARM_FEAT_SVE)
define ZCR_EL2 bitfield<64> {
	3:0		LEN		uint8;
	others		unknown=0;
};

define SVE_Z_MIN_REG_SIZE constant type count_t = 16;
define SVE_Z_REG_SIZE constant type count_t = PLATFORM_SVE_REG_SIZE;
#endif

```

`hyp/vm/arm_vm_sve_simple/aarch64/src/arm_vm_sve.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>

#if defined(ARCH_ARM_FEAT_SVE)

#include <hypregisters.h>

#include <compiler.h>
#include <log.h>
#include <platform_features.h>
#include <thread.h>
#include <trace.h>
#include <vcpu.h>

#include <asm/barrier.h>
#include <asm/system_registers.h>

#include "event_handlers.h"

// A simple SVE module that allows SVE access to HLOS only.

static bool sve_disabled = false;

// Ensure the value of SVE_Z_REG_SIZE (PLATFORM_SVE_REG_SIZE) is sane
static_assert(SVE_Z_REG_SIZE >= SVE_Z_MIN_REG_SIZE,
	      "SVE register size should be minimum 16 bytes");

// Due a LLVM12.0 design choice, "-mgeneral-regs-only" also excludes the SVE
// registers. Therefore ".arch_extension sve;" needs to be added to all the
// inline "asm" statements that access SVE.
// For this reason the SVE code uses MSR/MRS directly instead of generated
// read/write accessors.
static inline void
register_ZCR_EL2_write(const ZCR_EL2_t val)
{
	register_t raw = (register_t)ZCR_EL2_raw(val);
	__asm__ volatile(".arch_extension sve;"
			 "msr ZCR_EL2, %[r]"
			 :
			 : [r] "rz"(raw));
}

static inline uint64_t
register_ID_AA64ZFR0_EL1_read(void)
{
	register_t val;
	__asm__ volatile(".arch_extension sve;"
			 "mrs %0, ID_AA64ZFR0_EL1;"
			 : "=r"(val));
	return (uint64_t)(val);
}

void
arm_vm_sve_simple_handle_boot_runtime_init(void)
{
	// Before writing ZCR_EL2, make sure EL2 has access to SVE subsystem.
	// Unfortunately the same bit that controls EL1/EL0 access also controls
	// EL2 access.
	CPTR_EL2_E2H1_t cptr =
		register_CPTR_EL2_E2H1_read_ordered(&asm_ordering);
	CPTR_EL2_E2H1_set_ZEN(&cptr, CPTR_ZEN_TRAP_NONE);
	register_CPTR_EL2_E2H1_write_ordered(cptr, &asm_ordering);
}

void
arm_vm_sve_simple_handle_boot_cold_init(void)
{
	platform_cpu_features_t features = platform_get_cpu_features();

	sve_disabled = platform_cpu_features_get_sve_disable(&features);
}

void
arm_vm_sve_simple_handle_boot_cpu_warm_init(void)
{
	if (!sve_disabled) {
		// Initialise ZCR_EL2 as its reset value is architecturally
		// UNKNOWN. SVE register size is (ZCR_EL2.LEN + 1) * 128 bits.
		// SVE_Z_REG_SIZE is in bytes.
		ZCR_EL2_t zcr = ZCR_EL2_default();
		ZCR_EL2_set_LEN(&zcr,
				(uint8_t)(((SVE_Z_REG_SIZE << 3) >> 7) - 1U));

		register_ZCR_EL2_write(zcr);
		// No need to disable SVE access, the context-switch code will
		// do it if necessary (if we are switching to a non-HLOS VM)
	}
}

void
arm_vm_sve_simple_handle_rootvm_init(qcbor_enc_ctxt_t *qcbor_enc_ctxt)
{
	assert(qcbor_enc_ctxt != NULL);

	QCBOREncode_AddBoolToMap(qcbor_enc_ctxt, "sve_supported",
				 !sve_disabled);
}

bool
arm_vm_sve_simple_handle_vcpu_activate_thread(thread_t		 *thread,
					      vcpu_option_flags_t options)
{
	assert(thread != NULL);
	bool ret;

	if (thread->kind == THREAD_KIND_VCPU) {
		bool hlos	 = vcpu_option_flags_get_hlos_vm(&options);
		bool sve_allowed = vcpu_option_flags_get_sve_allowed(&options);

		if (sve_allowed && sve_disabled) {
			// Not permitted
			ret = false;
		} else if (hlos && sve_allowed) {
			// Give HLOS threads SVE access
			CPTR_EL2_E2H1_set_ZEN(&thread->vcpu_regs_el2.cptr_el2,
					      CPTR_ZEN_TRAP_NONE);
			vcpu_option_flags_set_sve_allowed(&thread->vcpu_options,
							  true);
			ret = true;
		} else if (!hlos && sve_allowed) {
			// Not supported
			ret = false;
		} else {
			CPTR_EL2_E2H1_set_ZEN(&thread->vcpu_regs_el2.cptr_el2,
					      CPTR_ZEN_TRAP_ALL);
			ret = true;
		}
	} else {
		ret = true;
	}

	return ret;
}

// ID_AA64ZFR0_EL1 is trapped through HCR_EL2.TID3
vcpu_trap_result_t
arm_vm_sve_simple_handle_vcpu_trap_sysreg_read(ESR_EL2_ISS_MSR_MRS_t iss)
{
	vcpu_trap_result_t ret;
	thread_t	  *thread = thread_get_self();

	// Assert this is a read
	assert(ESR_EL2_ISS_MSR_MRS_get_Direction(&iss));

	// Remove the fields that are not used in the comparison
	ESR_EL2_ISS_MSR_MRS_t temp_iss = iss;
	ESR_EL2_ISS_MSR_MRS_set_Rt(&temp_iss, 0U);
	ESR_EL2_ISS_MSR_MRS_set_Direction(&temp_iss, false);

	switch (ESR_EL2_ISS_MSR_MRS_raw(temp_iss)) {
	case ISS_MRS_MSR_ID_AA64ZFR0_EL1:
		ret = VCPU_TRAP_RESULT_EMULATED;
		uint64_t val;
		if (vcpu_option_flags_get_sve_allowed(&thread->vcpu_options)) {
			val = register_ID_AA64ZFR0_EL1_read();
		} else {
			val = 0U;
		}
		uint8_t reg_num = ESR_EL2_ISS_MSR_MRS_get_Rt(&iss);
		vcpu_gpr_write(thread, reg_num, val);
		break;
	default:
		ret = VCPU_TRAP_RESULT_UNHANDLED;
		break;
	}

	return ret;
}
#endif

```

`hyp/vm/arm_vm_sve_simple/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

interface arm_sve
arch_types aarch64 arm_vm_sve_aarch64.tc
arch_events aarch64 arm_vm_sve_aarch64.ev
arch_source aarch64 arm_vm_sve.c

```

`hyp/vm/arm_vm_timer/aarch64/arm_vm_timer_aarch64.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

extend vcpu_el1_registers structure {
	cntkctl_el1	bitfield CNTKCTL_EL1;
	cntv_ctl_el0	bitfield CNT_CTL;
	cntp_ctl_el0	bitfield CNT_CTL;
	cntv_cval_el0	bitfield CNT_CVAL;
	cntp_cval_el0	bitfield CNT_CVAL;
};

```

`hyp/vm/arm_vm_timer/aarch64/src/arm_vm_timer.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>

#include <hypconstants.h>
#include <hypregisters.h>

#include <compiler.h>
#include <cpulocal.h>
#include <irq.h>
#include <object.h>
#include <panic.h>
#include <partition.h>
#include <partition_alloc.h>
#include <preempt.h>
#include <scheduler.h>
#include <trace.h>

#include <asm/barrier.h>

#include "arm_vm_timer.h"
#include "event_handlers.h"

#if defined(VERBOSE) && VERBOSE
#define VM_TIMER_DEBUG 1
#else
#define VM_TIMER_DEBUG 0
#endif

#define ARM_VM_TIMER_TYPE_NUM (ENUM_ARM_VM_TIMER_TYPE_MAX_VALUE + 1)

static hwirq_t *arm_vm_timer_hwirq[ARM_VM_TIMER_TYPE_NUM];
CPULOCAL_DECLARE_STATIC(bool, arm_vm_timer_irq_active)[ARM_VM_TIMER_TYPE_NUM];

void
arm_vm_timer_init(arm_vm_timer_type_t tt)
{
	CNT_CTL_t cnt_ctl;

	CNT_CTL_init(&cnt_ctl);
	CNT_CTL_set_IMASK(&cnt_ctl, true);

	if (tt == ARM_VM_TIMER_TYPE_VIRTUAL) {
		register_CNTV_CTL_EL0_write_ordered(cnt_ctl, &asm_ordering);
	} else if (tt == ARM_VM_TIMER_TYPE_PHYSICAL) {
		register_CNTP_CTL_EL0_write_ordered(cnt_ctl, &asm_ordering);
	} else {
		panic("Invalid timer");
	}
}

bool
arm_vm_timer_is_irq_enabled(arm_vm_timer_type_t tt)
{
	CNT_CTL_t cnt_ctl;

	if (tt == ARM_VM_TIMER_TYPE_VIRTUAL) {
		cnt_ctl = register_CNTV_CTL_EL0_read_volatile_ordered(
			&asm_ordering);
	} else if (tt == ARM_VM_TIMER_TYPE_PHYSICAL) {
		cnt_ctl = register_CNTP_CTL_EL0_read_volatile_ordered(
			&asm_ordering);
	} else {
		panic("Invalid timer");
	}

	return (CNT_CTL_get_ENABLE(&cnt_ctl) && !CNT_CTL_get_IMASK(&cnt_ctl));
}

bool
arm_vm_timer_is_irq_pending(arm_vm_timer_type_t tt)
{
	CNT_CTL_t cnt_ctl;

	if (tt == ARM_VM_TIMER_TYPE_VIRTUAL) {
		cnt_ctl = register_CNTV_CTL_EL0_read_volatile_ordered(
			&asm_ordering);
	} else if (tt == ARM_VM_TIMER_TYPE_PHYSICAL) {
		cnt_ctl = register_CNTP_CTL_EL0_read_volatile_ordered(
			&asm_ordering);
	} else {
		panic("Invalid timer");
	}

	return (CNT_CTL_get_ENABLE(&cnt_ctl) && !CNT_CTL_get_IMASK(&cnt_ctl) &&
		CNT_CTL_get_ISTATUS(&cnt_ctl));
}

void
arm_vm_timer_cancel_timeout(arm_vm_timer_type_t tt)
{
	CNT_CTL_t cnt_ctl;

	CNT_CTL_init(&cnt_ctl);
	CNT_CTL_set_ENABLE(&cnt_ctl, false);

	if (tt == ARM_VM_TIMER_TYPE_VIRTUAL) {
		register_CNTV_CTL_EL0_write_ordered(cnt_ctl, &asm_ordering);
	} else if (tt == ARM_VM_TIMER_TYPE_PHYSICAL) {
		register_CNTP_CTL_EL0_write_ordered(cnt_ctl, &asm_ordering);
	} else {
		panic("Invalid timer");
	}
}

bool
arm_vm_timer_get_is_expired(arm_vm_timer_type_t tt)
{
	CNT_CTL_t cnt_ctl;

	if (tt == ARM_VM_TIMER_TYPE_VIRTUAL) {
		cnt_ctl = register_CNTV_CTL_EL0_read_volatile_ordered(
			&asm_ordering);
	} else if (tt == ARM_VM_TIMER_TYPE_PHYSICAL) {
		cnt_ctl = register_CNTP_CTL_EL0_read_volatile_ordered(
			&asm_ordering);
	} else {
		panic("Invalid timer");
	}

	assert(CNT_CTL_get_ENABLE(&cnt_ctl));
	return CNT_CTL_get_ISTATUS(&cnt_ctl);
}

uint32_t
arm_vm_timer_get_freqeuncy(void)
{
	CNTFRQ_EL0_t cntfrq = register_CNTFRQ_EL0_read();

	return CNTFRQ_EL0_get_ClockFrequency(&cntfrq);
}

uint64_t
arm_vm_timer_get_ticks(void)
{
	// This register read below is allowed to occur speculatively at any
	// time after the most recent context sync event. If caller the wants
	// it to actually reflect the exact current time, it must execute an
	// ordered ISB before calling this function.
	CNTPCT_EL0_t cntpct =
		register_CNTPCT_EL0_read_volatile_ordered(&asm_ordering);

	return CNTPCT_EL0_get_CountValue(&cntpct);
}

uint64_t
arm_vm_timer_get_timeout(arm_vm_timer_type_t tt)
{
	CNT_CVAL_t cnt_cval;

	if (tt == ARM_VM_TIMER_TYPE_VIRTUAL) {
		cnt_cval = register_CNTV_CVAL_EL0_read_volatile();
	} else if (tt == ARM_VM_TIMER_TYPE_PHYSICAL) {
		cnt_cval = register_CNTP_CVAL_EL0_read_volatile();
	} else {
		panic("Invalid timer");
	}

	return CNT_CVAL_get_CompareValue(&cnt_cval);
}

void
arm_vm_timer_arch_timer_hw_irq_activated(arm_vm_timer_type_t tt)
{
	if ((tt == ARM_VM_TIMER_TYPE_PHYSICAL) ||
	    (tt == ARM_VM_TIMER_TYPE_VIRTUAL)) {
		CPULOCAL(arm_vm_timer_irq_active)[tt] = true;
	} else {
		panic("Invalid timer");
	}
}

void
arm_vm_timer_arch_timer_hw_irq_deactivate(arm_vm_timer_type_t tt)
{
	if ((tt == ARM_VM_TIMER_TYPE_PHYSICAL) ||
	    (tt == ARM_VM_TIMER_TYPE_VIRTUAL)) {
		if (CPULOCAL(arm_vm_timer_irq_active)[tt]) {
			CPULOCAL(arm_vm_timer_irq_active)[tt] = false;
			irq_deactivate(arm_vm_timer_hwirq[tt]);
		}
	} else {
		panic("Invalid timer");
	}
}

void
arm_vm_timer_handle_boot_cpu_cold_init(void)
{
	for (int tt = ENUM_ARM_VM_TIMER_TYPE_MIN_VALUE;
	     tt < ARM_VM_TIMER_TYPE_NUM; tt++) {
		CPULOCAL(arm_vm_timer_irq_active)[tt] = false;
	}
}

void
arm_vm_timer_handle_boot_hypervisor_start(void)
{
	hwirq_ptr_result_t ret;
	hwirq_create_t	   params[] = {
		    {
			    .irq    = PLATFORM_VM_ARCH_VIRTUAL_TIMER_IRQ,
			    .action = HWIRQ_ACTION_VM_TIMER,
		    },
		    {
			    .irq    = PLATFORM_VM_ARCH_PHYSICAL_TIMER_IRQ,
			    .action = HWIRQ_ACTION_VM_TIMER,
		    }
	};

	for (int tt = ENUM_ARM_VM_TIMER_TYPE_MIN_VALUE;
	     tt < ARM_VM_TIMER_TYPE_NUM; tt++) {
		ret = partition_allocate_hwirq(partition_get_private(),
					       params[tt]);

		if ((ret.e != OK) || (object_activate_hwirq(ret.r) != OK)) {
			panic("Failed to enable VM Timer IRQ");
		}

		arm_vm_timer_hwirq[tt] = ret.r;
		irq_enable_local(arm_vm_timer_hwirq[tt]);
	}
}

error_t
arm_vm_timer_handle_power_cpu_suspend(void)
{
	arm_vm_timer_arch_timer_hw_irq_deactivate(ARM_VM_TIMER_TYPE_VIRTUAL);
	arm_vm_timer_arch_timer_hw_irq_deactivate(ARM_VM_TIMER_TYPE_PHYSICAL);

	return OK;
}

void
arm_vm_timer_handle_boot_cpu_warm_init(void)
{
	arm_vm_timer_init(ARM_VM_TIMER_TYPE_VIRTUAL);
	arm_vm_timer_init(ARM_VM_TIMER_TYPE_PHYSICAL);

#if defined(ARCH_ARM_FEAT_VHE)
	CNTHCTL_EL2_E2H1_t cnthctl;
	CNTHCTL_EL2_E2H1_init(&cnthctl);

	CNTHCTL_EL2_E2H1_set_EL1PTEN(&cnthctl, true);
	CNTHCTL_EL2_E2H1_set_EL1PCTEN(&cnthctl, true);

	// TODO: Determine correct setting for EVNTI
	CNTHCTL_EL2_E2H1_set_EVNTI(&cnthctl, 5);
	CNTHCTL_EL2_E2H1_set_EVNTDIR(&cnthctl, false);
	CNTHCTL_EL2_E2H1_set_EVNTEN(&cnthctl, false);

	// These four are here for completeness and are not strictly necessary.
	CNTHCTL_EL2_E2H1_set_EL0PTEN(&cnthctl, true);
	CNTHCTL_EL2_E2H1_set_EL0VTEN(&cnthctl, true);
	CNTHCTL_EL2_E2H1_set_EL0VCTEN(&cnthctl, true);
	CNTHCTL_EL2_E2H1_set_EL0PCTEN(&cnthctl, true);

#if defined(ARCH_ARM_FEAT_ECV)
	// Explicitly disable the ECV feature and the access traps for the
	// virtual timer and counter registers.
	CNTHCTL_EL2_E2H1_set_ECV(&cnthctl, false);
	CNTHCTL_EL2_E2H1_set_EL1TVT(&cnthctl, false);
	CNTHCTL_EL2_E2H1_set_EL1TVCT(&cnthctl, false);
#endif

	register_CNTHCTL_EL2_E2H1_write(cnthctl);
#else
	CNTHCTL_EL2_E2H0_t cnthctl;
	CNTHCTL_EL2_E2H0_init(&cnthctl);

	// In order to disable the physical timer at EL0 and EL1 we trap the
	// accesses to the physical timer registers but do not provide a handler
	// for the trap, causing a synchronous data abort to be injected to the
	// guest.
	CNTHCTL_EL2_E2H0_set_EL1PCEN(&cnthctl, true);
	CNTHCTL_EL2_E2H0_set_EL1PCTEN(&cnthctl, true);

	// TODO: Determine correct setting for EVNTI
	CNTHCTL_EL2_E2H0_set_EVNTI(&cnthctl, 5);
	CNTHCTL_EL2_E2H0_set_EVNTDIR(&cnthctl, false);
	CNTHCTL_EL2_E2H0_set_EVNTEN(&cnthctl, false);

#if defined(ARCH_ARM_FEAT_ECV)
	// Explicitly disable the ECV feature and the access traps for the
	// virtual timer and counter registers.
	CNTHCTL_EL2_E2H0_set_ECV(&cnthctl, false);
	CNTHCTL_EL2_E2H0_set_EL1TVT(&cnthctl, false);
	CNTHCTL_EL2_E2H0_set_EL1TVCT(&cnthctl, false);
#endif

	register_CNTHCTL_EL2_E2H0_write(cnthctl);
#endif

#if VM_TIMER_DEBUG
	TRACE_LOCAL(
		DEBUG, INFO,
		"arm_vm_timer warm boot pcnt {:#x} vctl {:#x} vact {:d} pact {:d}",
		CNTPCT_EL0_raw(register_CNTPCT_EL0_read_volatile_ordered(
			&asm_ordering)),
		CNT_CTL_raw(register_CNTV_CTL_EL0_read_ordered(&asm_ordering)),
		(register_t)CPULOCAL(
			arm_vm_timer_irq_active)[ARM_VM_TIMER_TYPE_VIRTUAL],
		(register_t)CPULOCAL(
			arm_vm_timer_irq_active)[ARM_VM_TIMER_TYPE_PHYSICAL]);
#endif

	register_CNTVOFF_EL2_write(CNTVOFF_EL2_cast(0U));

	for (int tt = ENUM_ARM_VM_TIMER_TYPE_MIN_VALUE;
	     tt < ARM_VM_TIMER_TYPE_NUM; tt++) {
		if (arm_vm_timer_hwirq[tt] != NULL) {
			irq_enable_local(arm_vm_timer_hwirq[tt]);
		}
	}
}

bool
arm_vm_timer_is_irq_enabled_thread(thread_t *thread, arm_vm_timer_type_t tt)
{
	CNT_CTL_t cnt_ctl;

	if (tt == ARM_VM_TIMER_TYPE_VIRTUAL) {
		cnt_ctl = thread->vcpu_regs_el1.cntv_ctl_el0;
	} else if (tt == ARM_VM_TIMER_TYPE_PHYSICAL) {
		cnt_ctl = thread->vcpu_regs_el1.cntp_ctl_el0;
	} else {
		panic("Invalid timer");
	}

	return (CNT_CTL_get_ENABLE(&cnt_ctl) && !CNT_CTL_get_IMASK(&cnt_ctl));
}

ticks_t
arm_vm_timer_get_timeout_thread(thread_t *thread, arm_vm_timer_type_t tt)
{
	CNT_CVAL_t cnt_cval;

	if (tt == ARM_VM_TIMER_TYPE_VIRTUAL) {
		cnt_cval = thread->vcpu_regs_el1.cntv_cval_el0;
	} else if (tt == ARM_VM_TIMER_TYPE_PHYSICAL) {
		cnt_cval = thread->vcpu_regs_el1.cntp_cval_el0;
	} else {
		panic("Invalid timer");
	}

	return CNT_CVAL_get_CompareValue(&cnt_cval);
}

void
arm_vm_timer_load_state(thread_t *thread)
{
	register_CNTKCTL_EL1_write_ordered(thread->vcpu_regs_el1.cntkctl_el1,
					   &asm_ordering);
	register_CNTV_CTL_EL0_write_ordered(thread->vcpu_regs_el1.cntv_ctl_el0,
					    &asm_ordering);
	register_CNTV_CVAL_EL0_write_ordered(
		thread->vcpu_regs_el1.cntv_cval_el0, &asm_ordering);
	register_CNTP_CTL_EL0_write_ordered(thread->vcpu_regs_el1.cntp_ctl_el0,
					    &asm_ordering);
	register_CNTP_CVAL_EL0_write_ordered(
		thread->vcpu_regs_el1.cntp_cval_el0, &asm_ordering);
}

void
arm_vm_timer_handle_thread_save_state(void)
{
	thread_t *thread = thread_get_self();

	if ((compiler_expected(thread->kind == THREAD_KIND_VCPU)) &&
	    !scheduler_is_blocked(thread, SCHEDULER_BLOCK_VCPU_OFF)) {
		thread->vcpu_regs_el1.cntkctl_el1 = register_CNTKCTL_EL1_read();
		thread->vcpu_regs_el1.cntv_ctl_el0 =
			register_CNTV_CTL_EL0_read();
		thread->vcpu_regs_el1.cntv_cval_el0 =
			register_CNTV_CVAL_EL0_read();
		thread->vcpu_regs_el1.cntp_ctl_el0 =
			register_CNTP_CTL_EL0_read();
		thread->vcpu_regs_el1.cntp_cval_el0 =
			register_CNTP_CVAL_EL0_read();
	}
}

```

`hyp/vm/arm_vm_timer/arm_vm_timer.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

module arm_vm_timer

subscribe boot_hypervisor_start
	require_preempt_disabled

subscribe boot_cpu_cold_init()
	require_preempt_disabled

subscribe boot_cpu_warm_init
	require_preempt_disabled

subscribe thread_save_state()
	require_preempt_disabled

subscribe thread_context_switch_pre()
	require_preempt_disabled

subscribe thread_context_switch_post()
	require_preempt_disabled

subscribe object_create_thread

subscribe object_activate_thread
	unwinder arm_vm_timer_handle_object_deactivate_thread(thread)

subscribe object_deactivate_thread

subscribe timer_action[TIMER_ACTION_VIRTUAL_TIMER]
	handler arm_vm_timer_handle_timer_action(action_type, timer)

subscribe timer_action[TIMER_ACTION_PHYSICAL_TIMER]
	handler arm_vm_timer_handle_timer_action(action_type, timer)

subscribe virq_check_pending[VIRQ_TRIGGER_VIRTUAL_TIMER]
	handler arm_vm_timer_handle_virq_check_pending(trigger, source)
	require_preempt_disabled

subscribe virq_check_pending[VIRQ_TRIGGER_PHYSICAL_TIMER]
	handler arm_vm_timer_handle_virq_check_pending(trigger, source)
	require_preempt_disabled

subscribe irq_received[HWIRQ_ACTION_VM_TIMER](irq)
	require_preempt_disabled

subscribe vcpu_stopped()

subscribe power_cpu_suspend()
	require_preempt_disabled

subscribe vcpu_suspend()

```

`hyp/vm/arm_vm_timer/arm_vm_timer.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

define arm_vm_timer_type enumeration {
	VIRTUAL;
	PHYSICAL;
};

extend thread object {
	virtual_timer structure timer(contained);
	virtual_timer_virq_src structure virq_source(contained);

	physical_timer structure timer(contained);
	physical_timer_virq_src structure virq_source(contained);
};

extend timer_action enumeration {
	virtual_timer;
	physical_timer;
};

extend virq_trigger enumeration {
	virtual_timer;
	physical_timer;
};

extend hwirq_action enumeration {
	vm_timer;
};

```

`hyp/vm/arm_vm_timer/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

types arm_vm_timer.tc
arch_types aarch64 arm_vm_timer_aarch64.tc
events arm_vm_timer.ev
local_include
source arm_vm_timer_thread.c
source arm_vm_timer_irq.c
arch_source aarch64 arm_vm_timer.c

```

`hyp/vm/arm_vm_timer/include/arm_vm_timer.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

void
arm_vm_timer_init(arm_vm_timer_type_t tt);

bool
arm_vm_timer_get_is_expired(arm_vm_timer_type_t tt);

bool
arm_vm_timer_is_irq_enabled(arm_vm_timer_type_t tt);

bool
arm_vm_timer_is_irq_pending(arm_vm_timer_type_t tt);

void
arm_vm_timer_load_state(thread_t *thread);

void
arm_vm_timer_cancel_timeout(arm_vm_timer_type_t tt);

uint32_t
arm_vm_timer_get_freqeuncy(void);

ticks_t
arm_vm_timer_get_ticks(void);

ticks_t
arm_vm_timer_get_timeout(arm_vm_timer_type_t tt);

// Checks the timer control register in a thread's saved context.
// Returns true if the timer is enabled and its interrupt is not masked.
bool
arm_vm_timer_is_irq_enabled_thread(thread_t *thread, arm_vm_timer_type_t tt);

ticks_t
arm_vm_timer_get_timeout_thread(thread_t *thread, arm_vm_timer_type_t tt);

void
arm_vm_timer_arch_timer_hw_irq_activated(arm_vm_timer_type_t tt)
	REQUIRE_PREEMPT_DISABLED;

void
arm_vm_timer_arch_timer_hw_irq_deactivate(arm_vm_timer_type_t tt)
	REQUIRE_PREEMPT_DISABLED;

```

`hyp/vm/arm_vm_timer/src/arm_vm_timer_irq.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>

#include <hypcontainers.h>

#include <atomic.h>
#include <compiler.h>
#include <panic.h>
#include <scheduler.h>
#include <trace.h>
#include <virq.h>

#include "arm_vm_timer.h"
#include "event_handlers.h"

static void
arm_vm_timer_inject_timer_virq(thread_t *thread, arm_vm_timer_type_t tt)
{
	if (tt == ARM_VM_TIMER_TYPE_VIRTUAL) {
		(void)virq_assert(&thread->virtual_timer_virq_src, false);
	} else if (tt == ARM_VM_TIMER_TYPE_PHYSICAL) {
		(void)virq_assert(&thread->physical_timer_virq_src, false);
	} else {
		panic("Invalid timer");
	}
}

// Handle the timer queue expiry coming from the hyp arch timer
static void
arm_vm_timer_type_timer_action(thread_t *thread, arm_vm_timer_type_t tt)
{
	bool is_current = thread_get_self() == thread;

	if (is_current && arm_vm_timer_is_irq_pending(tt)) {
		arm_vm_timer_inject_timer_virq(thread, tt);
	} else if (!is_current &&
		   arm_vm_timer_is_irq_enabled_thread(thread, tt)) {
		arm_vm_timer_inject_timer_virq(thread, tt);
	} else {
		TRACE(DEBUG, INFO, "Redundant VM hyp timeout");
	}
}

bool
arm_vm_timer_handle_timer_action(timer_action_t action_type, timer_t *timer)
{
	if (action_type == TIMER_ACTION_VIRTUAL_TIMER) {
		arm_vm_timer_type_timer_action(
			thread_container_of_virtual_timer(timer),
			ARM_VM_TIMER_TYPE_VIRTUAL);
	} else if (action_type == TIMER_ACTION_PHYSICAL_TIMER) {
		arm_vm_timer_type_timer_action(
			thread_container_of_physical_timer(timer),
			ARM_VM_TIMER_TYPE_PHYSICAL);
	} else {
		TRACE(DEBUG, INFO, "Spurious VM hyp timeout");
	}

	return true;
}

// Handle the VM arch timer expiry
static bool
arm_vm_timer_type_irq_received(thread_t *thread, arm_vm_timer_type_t tt)
	REQUIRE_PREEMPT_DISABLED
{
	bool injected = false;

	if (arm_vm_timer_is_irq_pending(tt)) {
		arm_vm_timer_inject_timer_virq(thread, tt);
		arm_vm_timer_arch_timer_hw_irq_activated(tt);
		injected = true;
	} else {
		TRACE(DEBUG, INFO, "Spurious VM timer IRQ");
	}

	return injected;
}

bool
arm_vm_timer_handle_irq_received(irq_t irq)
{
	bool	  injected = false;
	thread_t *thread   = thread_get_self();

	if (irq == PLATFORM_VM_ARCH_VIRTUAL_TIMER_IRQ) {
		injected = arm_vm_timer_type_irq_received(
			thread, ARM_VM_TIMER_TYPE_VIRTUAL);
	} else if (irq == PLATFORM_VM_ARCH_PHYSICAL_TIMER_IRQ) {
		injected = arm_vm_timer_type_irq_received(
			thread, ARM_VM_TIMER_TYPE_PHYSICAL);
	} else {
		panic("Invalid VM timer IRQ");
	}

	return !injected;
}

static bool
arm_vm_timer_virq_check_pending(thread_t *thread, arm_vm_timer_type_t tt)
	REQUIRE_PREEMPT_DISABLED
{
	bool ret = true;

	if (thread == thread_get_self()) {
		ret = arm_vm_timer_is_irq_pending(tt);

		if (!ret) {
			arm_vm_timer_arch_timer_hw_irq_deactivate(tt);
		}
	}

	return ret;
}

bool
arm_vm_timer_handle_virq_check_pending(virq_trigger_t trigger,
				       virq_source_t *source)
{
	bool ret = true;

	if (trigger == VIRQ_TRIGGER_VIRTUAL_TIMER) {
		ret = arm_vm_timer_virq_check_pending(
			thread_container_of_virtual_timer_virq_src(source),
			ARM_VM_TIMER_TYPE_VIRTUAL);

	} else if (trigger == VIRQ_TRIGGER_PHYSICAL_TIMER) {
		ret = arm_vm_timer_virq_check_pending(
			thread_container_of_physical_timer_virq_src(source),
			ARM_VM_TIMER_TYPE_PHYSICAL);
	} else {
		/* Do Nothing */
	}

	return ret;
}

```

`hyp/vm/arm_vm_timer/src/arm_vm_timer_thread.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>

#include <hypregisters.h>

#include <compiler.h>
#include <irq.h>
#include <timer_queue.h>
#include <vcpu.h>
#include <vic.h>
#include <virq.h>

#include <asm/barrier.h>

#include "arm_vm_timer.h"
#include "event_handlers.h"

error_t
arm_vm_timer_handle_object_create_thread(thread_create_t thread_create)
{
	thread_t *thread = thread_create.thread;
	assert(thread != NULL);

	if (thread->kind == THREAD_KIND_VCPU) {
		timer_init_object(&thread->virtual_timer,
				  TIMER_ACTION_VIRTUAL_TIMER);
		timer_init_object(&thread->physical_timer,
				  TIMER_ACTION_PHYSICAL_TIMER);
	}

	return OK;
}

error_t
arm_vm_timer_handle_object_activate_thread(thread_t *thread)
{
	error_t ret = OK;

	if (thread->kind == THREAD_KIND_VCPU) {
		ret = vic_bind_private_vcpu(&thread->virtual_timer_virq_src,
					    thread,
					    PLATFORM_VM_ARCH_VIRTUAL_TIMER_IRQ,
					    VIRQ_TRIGGER_VIRTUAL_TIMER);
		if (ret == OK) {
			ret = vic_bind_private_vcpu(
				&thread->physical_timer_virq_src, thread,
				PLATFORM_VM_ARCH_PHYSICAL_TIMER_IRQ,
				VIRQ_TRIGGER_PHYSICAL_TIMER);

			if (ret != OK) {
				vic_unbind(&thread->virtual_timer_virq_src);
			}
		}
	}

	return ret;
}

void
arm_vm_timer_handle_object_deactivate_thread(thread_t *thread)
{
	if (thread->kind == THREAD_KIND_VCPU) {
		vic_unbind(&thread->virtual_timer_virq_src);
		timer_dequeue(&thread->virtual_timer);

		vic_unbind(&thread->physical_timer_virq_src);
		timer_dequeue(&thread->physical_timer);
	}
}

error_t
arm_vm_timer_handle_thread_context_switch_pre(void)
{
	thread_t *thread = thread_get_self();

	// Enqueue thread's timeout if it is enabled, not already queued, and is
	// capable of waking the VCPU
	if ((compiler_expected(thread->kind == THREAD_KIND_VCPU) &&
	     vcpu_expects_wakeup(thread))) {
		if (arm_vm_timer_is_irq_enabled_thread(
			    thread, ARM_VM_TIMER_TYPE_VIRTUAL)) {
			timer_update(&thread->virtual_timer,
				     arm_vm_timer_get_timeout_thread(
					     thread,
					     ARM_VM_TIMER_TYPE_VIRTUAL));
		}

		if (arm_vm_timer_is_irq_enabled_thread(
			    thread, ARM_VM_TIMER_TYPE_PHYSICAL)) {
			timer_update(&thread->physical_timer,
				     arm_vm_timer_get_timeout_thread(
					     thread,
					     ARM_VM_TIMER_TYPE_PHYSICAL));
		}
	}

	return OK;
}

void
arm_vm_timer_handle_thread_context_switch_post(void)
{
	thread_t *thread = thread_get_self();
	if (compiler_expected(thread->kind == THREAD_KIND_VCPU)) {
		arm_vm_timer_load_state(thread);

		bool_result_t asserted;

		asserted = virq_query(&thread->virtual_timer_virq_src);
		if ((asserted.e == OK) && !asserted.r) {
			arm_vm_timer_arch_timer_hw_irq_deactivate(
				ARM_VM_TIMER_TYPE_VIRTUAL);
		}

		asserted = virq_query(&thread->physical_timer_virq_src);
		if ((asserted.e == OK) && !asserted.r) {
			arm_vm_timer_arch_timer_hw_irq_deactivate(
				ARM_VM_TIMER_TYPE_PHYSICAL);
		}
	} else {
		// Disable the timer and its IRQ
		arm_vm_timer_cancel_timeout(ARM_VM_TIMER_TYPE_VIRTUAL);
		arm_vm_timer_cancel_timeout(ARM_VM_TIMER_TYPE_PHYSICAL);
	}
}

void
arm_vm_timer_handle_vcpu_stopped(void)
{
	thread_t *thread = thread_get_self();

	// Disable the timer and its IRQ, so that context switch will not
	// lead us to enqueue an EL2 timer for a VCPU that can't be woken.
	arm_vm_timer_cancel_timeout(ARM_VM_TIMER_TYPE_VIRTUAL);
	arm_vm_timer_cancel_timeout(ARM_VM_TIMER_TYPE_PHYSICAL);

	// Ensure that the EL2 timer has not been lazily left queued.
	timer_dequeue(&thread->virtual_timer);
	timer_dequeue(&thread->physical_timer);
}

error_t
arm_vm_timer_handle_vcpu_suspend(void)
{
	thread_t *thread = thread_get_self();

	// Ensure that the EL2 timer has not been lazily left queued.
	if (timer_is_queued(&thread->virtual_timer) &&
	    !arm_vm_timer_is_irq_enabled(ARM_VM_TIMER_TYPE_VIRTUAL)) {
		timer_dequeue(&thread->virtual_timer);
	}

	if (timer_is_queued(&thread->physical_timer) &&
	    !arm_vm_timer_is_irq_enabled(ARM_VM_TIMER_TYPE_PHYSICAL)) {
		timer_dequeue(&thread->physical_timer);
	}

	return OK;
}

```

`hyp/vm/psci/aarch64/psci.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

module psci

subscribe vcpu_trap_wfi()

subscribe scheduler_selected_thread
	// Raised priority because this can clear *can_idle
	priority 1

```

`hyp/vm/psci/aarch64/src/psci.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <hyptypes.h>

#include <hypconstants.h>
#include <hypregisters.h>

#include <platform_cpu.h>

#include "event_handlers.h"
#include "psci_arch.h"

void
psci_handle_scheduler_selected_thread(thread_t *thread, bool *can_idle)
{
	if (thread->vpm_mode == VPM_MODE_IDLE) {
		// This thread can't be allowed to disable the WFI trap,
		// because WFI votes to suspend the physical CPU.
		*can_idle = false;
	}
}

vcpu_trap_result_t
psci_handle_vcpu_trap_wfi(void)
{
	return psci_handle_trapped_idle() ? VCPU_TRAP_RESULT_EMULATED
					  : VCPU_TRAP_RESULT_UNHANDLED;
}

```

`hyp/vm/psci/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

base_module hyp/vm/vpm_base
interface psci
local_include
events psci.ev
types psci.tc
source psci_common.c psci_pm_list.c
arch_source aarch64 psci.c
arch_events aarch64 psci.ev

```

`hyp/vm/psci/include/psci_arch.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

bool
psci_handle_trapped_idle(void);

```

`hyp/vm/psci/include/psci_common.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// Called by psci_common

error_t
psci_vcpu_suspend(thread_t *current) REQUIRE_PREEMPT_DISABLED;

void
psci_vcpu_resume(thread_t *thread) REQUIRE_PREEMPT_DISABLED;

void
psci_vcpu_clear_vcpu_state(thread_t *thread, cpu_index_t target_cpu)
	REQUIRE_PREEMPT_DISABLED REQUIRE_SCHEDULER_LOCK(thread);

uint32_t
psci_cpu_suspend_features(void);

// Implemented by psci_common

psci_ret_t
psci_suspend(psci_suspend_powerstate_t suspend_state,
	     paddr_t entry_point_address, register_t context_id)
	EXCLUDE_PREEMPT_DISABLED;

bool
psci_set_vpm_active_pcpus_bit(cpu_index_t bit);

bool
psci_clear_vpm_active_pcpus_bit(cpu_index_t bit);

void
psci_vpm_active_vcpus_get(cpu_index_t cpu, thread_t *vcpu)
	REQUIRE_SCHEDULER_LOCK(vcpu);

void
psci_vpm_active_vcpus_put(cpu_index_t cpu, thread_t *vcpu)
	REQUIRE_SCHEDULER_LOCK(vcpu);

bool
psci_vpm_active_vcpus_is_zero(cpu_index_t cpu);

bool
vcpus_state_is_any_awake(vpm_group_suspend_state_t vm_state, uint32_t level,
			 cpu_index_t cpu);

void
vcpus_state_set(vpm_group_suspend_state_t *vm_state, cpu_index_t cpu,
		psci_cpu_state_t cpu_state);

void
vcpus_state_clear(vpm_group_suspend_state_t *vm_state, cpu_index_t cpu);

```

`hyp/vm/psci/include/psci_events.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// clang-format off

#define PSCI_FUNCTION(fn, feat, h, ...)				\
subscribe smccc_call_fast_32_standard[(smccc_function_t)PSCI_FUNCTION_ ## fn];	\
	handler psci_ ## h ## _32(__VA_ARGS__);			\
	exclude_preempt_disabled.				\
subscribe psci_features32[PSCI_FUNCTION_ ## fn];		\
	constant feat.						\
subscribe smccc_call_fast_64_standard[(smccc_function_t)PSCI_FUNCTION_ ## fn];	\
	handler psci_ ## h ## _64(__VA_ARGS__);			\
	exclude_preempt_disabled.				\
subscribe psci_features64[PSCI_FUNCTION_ ## fn];		\
	constant feat.

#define PSCI_FUNCTION32(fn, feat, h, ...)			\
subscribe smccc_call_fast_32_standard[(smccc_function_t)PSCI_FUNCTION_ ## fn];	\
	handler psci_ ## h(__VA_ARGS__);			\
	exclude_preempt_disabled.				\
subscribe psci_features32[PSCI_FUNCTION_ ## fn];		\
	constant feat.

#define PSCI_FUNCTION_PERVM(fn, h, ...)				\
subscribe smccc_call_fast_32_standard[(smccc_function_t)PSCI_FUNCTION_ ## fn];	\
	handler psci_ ## h ## _32(__VA_ARGS__);			\
	exclude_preempt_disabled.				\
subscribe psci_features32[PSCI_FUNCTION_ ## fn];		\
	handler psci_ ## h ## _32_features().			\
subscribe smccc_call_fast_64_standard[(smccc_function_t)PSCI_FUNCTION_ ## fn];	\
	handler psci_ ## h ## _64(__VA_ARGS__);			\
	exclude_preempt_disabled.				\
subscribe psci_features64[PSCI_FUNCTION_ ## fn];		\
	handler psci_ ## h ## _64_features().			\

#define PSCI_FUNCTION32_PERVM(fn, h, ...)			\
subscribe smccc_call_fast_32_standard[(smccc_function_t)PSCI_FUNCTION_ ## fn];	\
	handler psci_ ## h(__VA_ARGS__);			\
	exclude_preempt_disabled.				\
subscribe psci_features32[PSCI_FUNCTION_ ## fn];		\
	handler psci_ ## h ## _features().

```

`hyp/vm/psci/include/psci_pm_list.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// Initialize all per core vcpu pm list
void
psci_pm_list_init(void);

// Get the current cpu list of vcpus that participate in power management
// decisions
list_t *
psci_pm_list_get_self(void) REQUIRE_PREEMPT_DISABLED;

// Add vcpu to specified cpu pm list
void
psci_pm_list_insert(cpu_index_t cpu_index, thread_t *vcpu);

// Remove vcpu to specified cpu pm list
void
psci_pm_list_delete(cpu_index_t cpu_index, thread_t *vcpu);

```

`hyp/vm/psci/psci.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

module psci

#include "psci_events.h"

PSCI_FUNCTION32(PSCI_VERSION, 0U, version, ret0)
PSCI_FUNCTION_PERVM(CPU_SUSPEND, cpu_suspend, arg1, arg2, arg3, ret0)
PSCI_FUNCTION32(CPU_OFF, 0U, cpu_off, ret0)
PSCI_FUNCTION(CPU_ON, 0U, cpu_on, arg1, arg2, arg3, ret0)
PSCI_FUNCTION(AFFINITY_INFO, 0U, affinity_info, arg1, arg2, ret0)
//PSCI_FUNCTION32(MIGRATE, 0U, migrate, arg1, ret0)
//PSCI_FUNCTION32(MIGRATE_INFO_TYPE, 0U, migrate_info_type, ret0)
//PSCI_FUNCTION(MIGRATE_INFO_UP_CPU, 0U, migrate_info_up_cpu, ret0)
PSCI_FUNCTION32(SYSTEM_OFF, 0U, system_off)
PSCI_FUNCTION32(SYSTEM_RESET, 0U, system_reset)
PSCI_FUNCTION32(PSCI_FEATURES, 0U, features, arg1, ret0)
//PSCI_FUNCTION32(CPU_FREEZE, 0U, cpu_freeze, ret0)
PSCI_FUNCTION(CPU_DEFAULT_SUSPEND, 0U, cpu_default_suspend, arg1, arg2, ret0)
//PSCI_FUNCTION(NODE_HW_STATE, 0U, node_hw_state, arg1, arg2, ret0)
//PSCI_FUNCTION(SYSTEM_SUSPEND, 0U, system_suspend, arg1, arg2, ret0)
//PSCI_FUNCTION32(PSCI_SET_SUSPEND_MODE, 0U, set_suspend_mode, arg1, ret0)
//PSCI_FUNCTION(PSCI_STAT_RESIDENCY, 0U, stat_residency, arg1, arg2, ret0)
//PSCI_FUNCTION(PSCI_STAT_COUNT, 0U, stat_count, arg1, arg2, ret0)
PSCI_FUNCTION(SYSTEM_RESET2, 0U, system_reset2, arg1, arg2, ret0)
//PSCI_FUNCTION32(MEM_PROTECT, 0U, mem_protect, arg1, ret0)
//PSCI_FUNCTION(MEM_PROTECT_CHECK_RANGE, 0U, mem_protect_check_range, arg1, arg2, ret0)

subscribe object_create_thread

subscribe object_activate_thread
	// Run early to ensure that MPIDR is set correctly, since other
	// modules may rely on it (especially VGIC, which is priority 1)
	priority 50

subscribe object_deactivate_thread

subscribe object_deactivate_vpm_group

subscribe vcpu_suspend
	unwinder(current)
	require_preempt_disabled

subscribe vcpu_started

subscribe vcpu_wakeup
	require_scheduler_lock(vcpu)

subscribe vcpu_wakeup_self

subscribe vcpu_expects_wakeup

#if defined(INTERFACE_VCPU_RUN)
subscribe vcpu_run_check(vcpu, state_data_0, state_data_1)
#endif

subscribe vcpu_poweron
	priority last
	require_scheduler_lock(vcpu)

subscribe vcpu_resume
	require_preempt_disabled

subscribe vcpu_poweroff
	// First so it can deny poweroff without unwinding other modules.
	priority first
	require_scheduler_lock(current)

subscribe vcpu_stopped

subscribe vcpu_activate_thread(thread)
	unwinder psci_handle_object_deactivate_thread(thread)
	// Run after the scheduler handler
	priority -100

subscribe boot_cold_init()

subscribe boot_cpu_cold_init

subscribe scheduler_affinity_changed(thread, prev_cpu, next_cpu, need_sync)
	require_scheduler_lock(thread)

subscribe scheduler_affinity_changed_sync(thread, next_cpu)
	require_preempt_disabled

subscribe task_queue_execute[TASK_QUEUE_CLASS_VPM_GROUP_VIRQ](entry)

subscribe power_cpu_online()
	require_preempt_disabled

subscribe power_cpu_offline()
	require_preempt_disabled

```

`hyp/vm/psci/psci.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <types/bitmap.h>

// PSCI 1.1 implemented
#define PSCI_VER 0x10001

define PSCI_VERSION constant uint32 = PSCI_VER;

// We need only 3 bits to encode the cpu level state of a vcpu
define PSCI_VCPUS_STATE_PER_VCPU_BITS constant type count_t = 3;
define PSCI_PER_CLUSTER_STATE_BITS constant type count_t = 3;
define PSCI_VCPUS_STATE_PER_VCPU_MASK constant uint8 = 0x7;
define PSCI_PER_CLUSTER_STATE_BITS_MASK constant uint8 = 0x7;
define PSCI_VCPUS_STATE_BITS constant type count_t = (PLATFORM_MAX_CORES * PSCI_VCPUS_STATE_PER_VCPU_BITS);
define PSCI_CLUSTER_STATE_BITS constant type count_t = (PLATFORM_MAX_CLUSTERS * PSCI_PER_CLUSTER_STATE_BITS);
define PSCI_VCPUS_STATE_MAX_VCPUS constant type count_t = PSCI_VCPUS_STATE_BITS/PSCI_VCPUS_STATE_PER_VCPU_BITS;
define PSCI_VCPUS_STATE_MAX_INDEX constant type count_t = PLATFORM_MAX_CORES*PSCI_VCPUS_STATE_PER_VCPU_BITS;

define vpm_group_suspend_state bitfield<128> {
	auto<PSCI_VCPUS_STATE_BITS>	vcpus_state	uint64;
	auto<PSCI_CLUSTER_STATE_BITS>	cluster_state	uint16;
	auto	system_suspend		bool;
	others	unknown=0;
};

// FIXME:
extend vpm_group object module psci {
	cpus			array(PLATFORM_MAX_CORES) pointer(atomic) object thread;
	online_count		type count_t(atomic);
	mode			enumeration psci_mode;
	vm_suspend_state	bitfield vpm_group_suspend_state(atomic, aligned(16));
	system_suspend_virq	structure virq_source(contained);
	virq_task		structure task_queue_entry(contained);
	lock			structure spinlock;
	system_suspend_count	sint8;
};

extend task_queue_class enumeration {
	vpm_group_virq;
};

extend vpm_mode enumeration {
	PSCI;
};

extend thread object module psci {
	// Reference-counted virtual PM group pointer
	group pointer object vpm_group;

	// Attachment index for the PSCI group. This is a CPU index because
	// the PSCI implementation currently assumes that the set of valid
	// virtual MPIDRs is the same as the valid physical MPIDRs, even if
	// there is no strict CPU affinity.
	index type cpu_index_t;

	// PSCI suspend state requested by this VCPU; valid only for a VCPU
	// blocked by SCHEDULER_BLOCK_VCPU_SUSPEND.
	suspend_state bitfield psci_suspend_powerstate;

	pm_list_node structure list_node(contained);

	// True if the VCPU is being migrated between PM lists.
	migrate bool;

#if defined(INTERFACE_VCPU_RUN)
	// True if the VCPU has called PSCI_SYSTEM_RESET.
	system_reset bool;
	// PSCI_SYSTEM_RESET2 parameters, returned in state data.
	system_reset_type uint64;
	system_reset_cookie uint64;
#endif

	// Tracks when the VCPU becomes inactive due to being off, suspended
	// or migrating. When zero, the VCPU is considered active and will be
	// accounted for in vpm_active_vcpus.
	inactive_count type count_t;
};

extend virq_trigger enumeration {
	vpm_group;
};

extend trace_ids bitfield {
	23:16	vcpu_index	type cpu_index_t;
};

extend trace_class enumeration {
	PSCI = 16;
};

extend trace_id enumeration {
	PSCI_PSTATE_VALIDATION = 0x30;
	PSCI_VPM_STATE_CHANGED = 0x31;
	PSCI_VPM_SYSTEM_SUSPEND = 0x32;
	PSCI_VPM_SYSTEM_RESUME = 0x33;
	PSCI_VPM_VCPU_SUSPEND = 0x34;
	PSCI_VPM_VCPU_RESUME = 0x35;
	PSCI_SYSTEM_SUSPEND = 0x36;
	PSCI_SYSTEM_RESUME = 0x37;
};

```

`hyp/vm/psci/src/psci_common.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>
#include <limits.h>

#include <hypcontainers.h>

#include <atomic.h>
#include <compiler.h>
#include <cpulocal.h>
#include <ipi.h>
#include <object.h>
#include <panic.h>
#include <platform_cpu.h>
#include <platform_psci.h>
#include <preempt.h>
#include <psci.h>
#include <rcu.h>
#include <scheduler.h>
#include <thread.h>
#include <trace.h>
#include <trace_helpers.h>
#include <util.h>
#include <vcpu.h>
#include <vgic.h>
#include <vic.h>
#include <virq.h>
#include <vpm.h>

#include <events/power.h>
#include <events/psci.h>

#include "event_handlers.h"
#include "psci_arch.h"
#include "psci_common.h"
#include "psci_pm_list.h"

CPULOCAL_DECLARE_STATIC(_Atomic count_t, vpm_active_vcpus);

// vpm_active_pcpus_bitmap could be made a count to avoid a bitmap.
#define REGISTER_BITS (sizeof(register_t) * (size_t)CHAR_BIT)
static_assert(PLATFORM_MAX_CORES <= REGISTER_BITS,
	      "PLATFORM_MAX_CORES > REGISTER_BITS");
static _Atomic register_t vpm_active_pcpus_bitmap;

void
psci_handle_boot_cold_init(void)
{
#if !defined(NDEBUG)
	register_t flags = 0U;
	TRACE_SET_CLASS(flags, PSCI);
	trace_set_class_flags(flags);
#endif

	psci_pm_list_init();
}

bool
psci_set_vpm_active_pcpus_bit(cpu_index_t bit)
{
	register_t old = atomic_fetch_or_explicit(
		&vpm_active_pcpus_bitmap, util_bit(bit), memory_order_relaxed);

	return old == 0U;
}

// Returns true if bitmap becomes zero after clearing bit
bool
psci_clear_vpm_active_pcpus_bit(cpu_index_t bit)
{
	register_t cleared_bit = ~util_bit(bit);

	register_t old = atomic_fetch_and_explicit(
		&vpm_active_pcpus_bitmap, cleared_bit, memory_order_relaxed);

	return (old & cleared_bit) == 0U;
}

void
psci_handle_boot_cpu_cold_init(cpu_index_t cpu)
{
	atomic_store_relaxed(&CPULOCAL_BY_INDEX(vpm_active_vcpus, cpu), 0U);
	(void)psci_set_vpm_active_pcpus_bit(cpu);
}

void
psci_vpm_active_vcpus_get(cpu_index_t cpu, thread_t *vcpu)
{
	assert(cpulocal_index_valid(cpu));
	assert(vcpu->psci_inactive_count != 0U);

	vcpu->psci_inactive_count--;
	if (vcpu->psci_inactive_count == 0U) {
		(void)atomic_fetch_add_explicit(
			&CPULOCAL_BY_INDEX(vpm_active_vcpus, cpu), 1U,
			memory_order_relaxed);
	}
}

void
psci_vpm_active_vcpus_put(cpu_index_t cpu, thread_t *vcpu)
{
	assert(cpulocal_index_valid(cpu));

	vcpu->psci_inactive_count++;
	if (vcpu->psci_inactive_count == 1U) {
		count_t old = atomic_fetch_sub_explicit(
			&CPULOCAL_BY_INDEX(vpm_active_vcpus, cpu), 1U,
			memory_order_relaxed);
		assert(old != 0U);
	}
}

bool
psci_vpm_active_vcpus_is_zero(cpu_index_t cpu)
{
	assert(cpulocal_index_valid(cpu));

	return atomic_load_relaxed(&CPULOCAL_BY_INDEX(vpm_active_vcpus, cpu)) ==
	       0;
}

bool
psci_handle_vcpu_activate_thread(thread_t *thread)
{
	bool ret = true;

	assert(thread != NULL);
	assert(thread->kind == THREAD_KIND_VCPU);

	scheduler_lock(thread);

	// Determine the initial inactive count for the VCPU.
	thread->psci_inactive_count = 0U;

	if (scheduler_is_blocked(thread, SCHEDULER_BLOCK_VCPU_OFF)) {
		// VCPU is inactive because it is powered off.
		thread->psci_inactive_count++;
	}
	// VCPU can't be suspended or in WFI yet.
	assert(!scheduler_is_blocked(thread, SCHEDULER_BLOCK_VCPU_SUSPEND));
	assert(!scheduler_is_blocked(thread, SCHEDULER_BLOCK_VCPU_WFI));

	cpu_index_t cpu = scheduler_get_affinity(thread);
	if (cpulocal_index_valid(cpu)) {
		if (thread->psci_group != NULL) {
			psci_pm_list_insert(cpu, thread);
		}
	} else {
		// VCPU is inactive because it has no valid affinity.
		thread->psci_inactive_count++;
	}

	// If the VCPU is initially active, make sure the CPU stays awake.
	if (thread->psci_inactive_count == 0U) {
		assert(cpulocal_index_valid(cpu));
		(void)atomic_fetch_add_explicit(
			&CPULOCAL_BY_INDEX(vpm_active_vcpus, cpu), 1U,
			memory_order_relaxed);
	}

	scheduler_unlock(thread);

	return ret;
}

void
psci_handle_scheduler_affinity_changed(thread_t *thread, cpu_index_t prev_cpu,
				       cpu_index_t next_cpu, bool *need_sync)
{
	object_state_t state = atomic_load_acquire(&thread->header.state);

	if ((state == OBJECT_STATE_ACTIVE) &&
	    (thread->vpm_mode != VPM_MODE_NONE)) {
		if (cpulocal_index_valid(prev_cpu)) {
			if (thread->vpm_mode == VPM_MODE_PSCI) {
				psci_pm_list_delete(prev_cpu, thread);
			}
			psci_vpm_active_vcpus_put(prev_cpu, thread);
		}

		if (cpulocal_index_valid(next_cpu)) {
			psci_vpm_active_vcpus_get(next_cpu, thread);
			if (thread->vpm_mode == VPM_MODE_PSCI) {
				thread->psci_migrate = true;
				*need_sync	     = true;
			}
		}
	}
}

void
psci_handle_scheduler_affinity_changed_sync(thread_t   *thread,
					    cpu_index_t next_cpu)
{
	if (thread->psci_migrate) {
		assert(thread->kind == THREAD_KIND_VCPU);
		assert(thread->vpm_mode == VPM_MODE_PSCI);
		assert(cpulocal_index_valid(next_cpu));

		psci_pm_list_insert(next_cpu, thread);

		thread->psci_migrate = false;
	}
}

static bool
psci_mpidr_matches_thread(MPIDR_EL1_t a, psci_mpidr_t b)
{
	return (MPIDR_EL1_get_Aff0(&a) == psci_mpidr_get_Aff0(&b)) &&
	       (MPIDR_EL1_get_Aff1(&a) == psci_mpidr_get_Aff1(&b)) &&
	       (MPIDR_EL1_get_Aff2(&a) == psci_mpidr_get_Aff2(&b)) &&
	       (MPIDR_EL1_get_Aff3(&a) == psci_mpidr_get_Aff3(&b));
}

static MPIDR_EL1_t
psci_mpidr_to_cpu(psci_mpidr_t psci_mpidr)
{
	MPIDR_EL1_t mpidr = MPIDR_EL1_default();

	MPIDR_EL1_set_Aff0(&mpidr, psci_mpidr_get_Aff0(&psci_mpidr));
	MPIDR_EL1_set_Aff1(&mpidr, psci_mpidr_get_Aff1(&psci_mpidr));
	MPIDR_EL1_set_Aff2(&mpidr, psci_mpidr_get_Aff2(&psci_mpidr));
	MPIDR_EL1_set_Aff3(&mpidr, psci_mpidr_get_Aff3(&psci_mpidr));

	return mpidr;
}

static thread_t *
psci_get_thread_by_mpidr(psci_mpidr_t mpidr)
{
	thread_t    *current	= thread_get_self();
	thread_t    *result	= NULL;
	vpm_group_t *psci_group = current->psci_group;

	assert(psci_group != NULL);

	// This function is not performance-critical; it is only called during
	// PSCI_CPU_ON and PSCI_AFFINITY_INFO. A simple linear search of the VPM
	// group is good enough.
	rcu_read_start();
	for (index_t i = 0U; i < util_array_size(psci_group->psci_cpus); i++) {
		thread_t *thread =
			atomic_load_consume(&psci_group->psci_cpus[i]);
		if ((thread != NULL) &&
		    psci_mpidr_matches_thread(thread->vcpu_regs_mpidr_el1,
					      mpidr) &&
		    object_get_thread_safe(thread)) {
			result = thread;
			break;
		}
	}
	rcu_read_finish();

	return result;
}

bool
psci_version(uint32_t *ret0)
{
	bool	  handled;
	thread_t *current = thread_get_self();

	if (compiler_unexpected(current->psci_group == NULL)) {
		handled = false;
	} else {
		*ret0	= PSCI_VERSION;
		handled = true;
	}
	return handled;
}

psci_ret_t
psci_suspend(psci_suspend_powerstate_t suspend_state,
	     paddr_t entry_point_address, register_t context_id)
{
	psci_ret_t ret;
	thread_t  *current = thread_get_self();

	current->psci_suspend_state = suspend_state;

	error_t err = vcpu_suspend();
	if (err == ERROR_DENIED) {
		TRACE(PSCI, PSCI_PSTATE_VALIDATION,
		      "psci_suspend: DENIED - pstate {:#x} - VM {:d}",
		      psci_suspend_powerstate_raw(suspend_state),
		      current->addrspace->vmid);
		ret = PSCI_RET_DENIED;
		goto out;
	} else if (err == ERROR_ARGUMENT_INVALID) {
		TRACE(PSCI, PSCI_PSTATE_VALIDATION,
		      "psci suspend: INVALID_PARAMETERS - pstate {:#x} - VM {:d}",
		      psci_suspend_powerstate_raw(suspend_state),
		      current->addrspace->vmid);
		ret = PSCI_RET_INVALID_PARAMETERS;
		goto out;
	} else if (err == ERROR_BUSY) {
		// It did not suspend due to a pending interrupt
		ret = PSCI_RET_SUCCESS;
		goto out;
	} else if (err == OK) {
		ret = PSCI_RET_SUCCESS;
	} else {
		panic("unhandled vcpu_suspend error");
	}

	// Warm reset VCPU unconditionally from the psci mode to make the
	// cpuidle stats work
	if ((psci_suspend_powerstate_get_StateType(&suspend_state) ==
	     PSCI_SUSPEND_POWERSTATE_TYPE_POWERDOWN)) {
		vcpu_warm_reset(entry_point_address, context_id);
	}

out:
	return ret;
}

static psci_ret_t
psci_cpu_suspend(psci_suspend_powerstate_t suspend_state,
		 paddr_t entry_point_address, register_t context_id)
	EXCLUDE_PREEMPT_DISABLED
{
	psci_ret_t ret;
	thread_t  *current = thread_get_self();

	// If the VCPU is participating in aggregation, we need to check with
	// platform code that the requested state is valid. Otherwise, all
	// requested states are accepted and treated equally.
	if (current->vpm_mode == VPM_MODE_PSCI) {
		assert(current->psci_group != NULL);

		// FIXME:
		cpulocal_begin();
		ret = platform_psci_suspend_state_validation(
			suspend_state, cpulocal_get_index(),
			current->psci_group->psci_mode);
		cpulocal_end();
		if (ret != PSCI_RET_SUCCESS) {
			TRACE(PSCI, PSCI_PSTATE_VALIDATION,
			      "psci_cpu_suspend: INVALID_PARAMETERS - pstate {:#x} - VM {:d}",
			      psci_suspend_powerstate_raw(suspend_state),
			      current->addrspace->vmid);
			goto out;
		}
	}

	ret = psci_suspend(suspend_state, entry_point_address, context_id);

out:
	return ret;
}

uint32_t
psci_cpu_suspend_32_features(void)
{
	return psci_cpu_suspend_features();
}

uint32_t
psci_cpu_suspend_64_features(void)
{
	return psci_cpu_suspend_features();
}

bool
psci_cpu_suspend_32(uint32_t arg1, uint32_t arg2, uint32_t arg3, uint32_t *ret0)
{
	bool	  handled;
	thread_t *current = thread_get_self();

	if (compiler_unexpected(current->psci_group == NULL)) {
		handled = false;
	} else {
		psci_ret_t ret = psci_cpu_suspend(
			psci_suspend_powerstate_cast(arg1), arg2, arg3);
		*ret0	= (uint32_t)ret;
		handled = true;
	}
	return handled;
}

bool
psci_cpu_suspend_64(uint64_t arg1, uint64_t arg2, uint64_t arg3, uint64_t *ret0)
{
	bool	  handled;
	thread_t *current = thread_get_self();

	if (compiler_unexpected(current->psci_group == NULL)) {
		handled = false;
	} else {
		psci_ret_t ret = psci_cpu_suspend(
			psci_suspend_powerstate_cast((uint32_t)arg1), arg2,
			arg3);
		*ret0	= (uint64_t)ret;
		handled = true;
	}
	return handled;
}

// Same as psci_cpu_suspend, but it sets the suspend state to the deepest
// cpu-level.
static psci_ret_t
psci_cpu_default_suspend(paddr_t entry_point_address, register_t context_id)
	EXCLUDE_PREEMPT_DISABLED
{
	psci_ret_t ret;

	psci_suspend_powerstate_t pstate = psci_suspend_powerstate_default();

	// FIXME:
	cpulocal_begin();
	psci_suspend_powerstate_stateid_t stateid =
		platform_psci_deepest_cpu_level_stateid(cpulocal_get_index());
	cpulocal_end();

	psci_suspend_powerstate_set_StateID(&pstate, stateid);
	psci_suspend_powerstate_set_StateType(
		&pstate, PSCI_SUSPEND_POWERSTATE_TYPE_POWERDOWN);

	ret = psci_suspend(pstate, entry_point_address, context_id);

	return ret;
}

bool
psci_cpu_default_suspend_32(uint32_t arg1, uint32_t arg2, uint32_t *ret0)
{
	bool	  handled;
	thread_t *current = thread_get_self();

	if (compiler_unexpected(current->psci_group == NULL)) {
		handled = false;
	} else {
		psci_ret_t ret = psci_cpu_default_suspend(arg1, arg2);
		*ret0	       = (uint32_t)ret;
		handled	       = true;
	}
	return handled;
}

bool
psci_cpu_default_suspend_64(uint64_t arg1, uint64_t arg2, uint64_t *ret0)
{
	bool	  handled;
	thread_t *current = thread_get_self();

	if (compiler_unexpected(current->psci_group == NULL)) {
		handled = false;
	} else {
		psci_ret_t ret = psci_cpu_default_suspend(arg1, arg2);
		*ret0	       = (uint64_t)ret;
		handled	       = true;
	}
	return handled;
}

bool
psci_cpu_off(uint32_t *ret0)
{
	bool	  handled;
	thread_t *current = thread_get_self();

	if (compiler_unexpected(current->psci_group == NULL)) {
		handled = false;
	} else {
		error_t ret = vcpu_poweroff(false, false);
		// If we return, the only reason should be DENIED
		assert(ret == ERROR_DENIED);
		*ret0	= (uint32_t)PSCI_RET_DENIED;
		handled = true;
	}
	return handled;
}

static psci_ret_t
psci_cpu_on(psci_mpidr_t cpu, paddr_t entry_point_address,
	    register_t context_id)
{
	thread_t  *thread = psci_get_thread_by_mpidr(cpu);
	psci_ret_t ret;

	if (compiler_unexpected(thread == NULL)) {
		thread_t *current = thread_get_self();
		vic_t	 *vic	  = vic_get_vic(current);
		if (vic == NULL) {
			ret = PSCI_RET_INVALID_PARAMETERS;
		} else {
			// Check whether MPIDR was valid or not. Note, we
			// currently use PLATFORM_MAX_CORES instead of a per
			// psci group
			MPIDR_EL1_t mpidr = psci_mpidr_to_cpu(cpu);

			const platform_mpidr_mapping_t *mpidr_mapping =
				vgic_get_mpidr_mapping(vic);

			bool valid = platform_cpu_map_mpidr_valid(mpidr_mapping,
								  mpidr);

			index_t index = platform_cpu_map_mpidr_to_index(
				mpidr_mapping, mpidr);

			if (!valid || (index > PLATFORM_MAX_CORES)) {
				ret = PSCI_RET_INVALID_PARAMETERS;
			} else {
				ret = PSCI_RET_INTERNAL_FAILURE;
			}
		}
	} else {
		bool reschedule = false;

		scheduler_lock(thread);
		if (vcpu_option_flags_get_pinned(&thread->vcpu_options) &&
		    !platform_cpu_exists(thread->scheduler_affinity)) {
			ret = PSCI_RET_INTERNAL_FAILURE;
			goto unlock;
		}

		if (scheduler_is_blocked(thread, SCHEDULER_BLOCK_VCPU_OFF)) {
			bool_result_t result = vcpu_poweron(
				thread, vmaddr_result_ok(entry_point_address),
				register_result_ok(context_id));
			reschedule = result.r;
			ret	   = (result.e == OK) ? PSCI_RET_SUCCESS
				     : (result.e == ERROR_FAILURE)
					     ? PSCI_RET_INTERNAL_FAILURE
				     : (result.e == ERROR_RETRY)
					     ? PSCI_RET_ALREADY_ON
					     : PSCI_RET_INVALID_PARAMETERS;
		} else {
			ret = PSCI_RET_ALREADY_ON;
		}
	unlock:
		scheduler_unlock(thread);
		object_put_thread(thread);

		if (reschedule) {
			(void)scheduler_schedule();
		}
	}

	return ret;
}

bool
psci_cpu_on_32(uint32_t arg1, uint32_t arg2, uint32_t arg3, uint32_t *ret0)
{
	bool	  handled;
	thread_t *current = thread_get_self();

	if (compiler_unexpected(current->psci_group == NULL)) {
		handled = false;
	} else {
		psci_ret_t ret = psci_cpu_on(psci_mpidr_cast(arg1), arg2, arg3);
		*ret0	       = (uint32_t)ret;
		handled	       = true;
	}
	return handled;
}

bool
psci_cpu_on_64(uint64_t arg1, uint64_t arg2, uint64_t arg3, uint64_t *ret0)
{
	bool	  handled;
	thread_t *current = thread_get_self();

	if (compiler_unexpected(current->psci_group == NULL)) {
		handled = false;
	} else {
		psci_ret_t ret = psci_cpu_on(psci_mpidr_cast(arg1), arg2, arg3);
		*ret0	       = (uint64_t)ret;
		handled	       = true;
	}
	return handled;
}

static psci_ret_t
psci_affinity_info(psci_mpidr_t affinity, uint32_t lowest_affinity_level)
{
	psci_ret_t ret;

	thread_t *thread = psci_get_thread_by_mpidr(affinity);
	if (thread == NULL) {
		ret = PSCI_RET_INVALID_PARAMETERS;
	} else if (lowest_affinity_level != 0U) {
		// lowest_affinity_level is legacy from PSCI 0.2; we are
		// allowed to fail if it is nonzero (which indicates a
		// query of the cluster-level state).
		ret = PSCI_RET_INVALID_PARAMETERS;
	} else {
		// Don't bother locking, this is inherently racy anyway
		psci_ret_affinity_info_t info =
			scheduler_is_blocked(thread, SCHEDULER_BLOCK_VCPU_OFF)
				? PSCI_RET_AFFINITY_INFO_OFF
				: PSCI_RET_AFFINITY_INFO_ON;
		ret = (psci_ret_t)info;
	}

	if (thread != NULL) {
		object_put_thread(thread);
	}

	return ret;
}

bool
psci_affinity_info_32(uint32_t arg1, uint32_t arg2, uint32_t *ret0)
{
	bool	  handled;
	thread_t *current = thread_get_self();

	if (compiler_unexpected(current->psci_group == NULL)) {
		handled = false;
	} else {
		psci_ret_t ret =
			psci_affinity_info(psci_mpidr_cast(arg1), arg2);
		*ret0	= (uint32_t)ret;
		handled = true;
	}
	return handled;
}

bool
psci_affinity_info_64(uint64_t arg1, uint64_t arg2, uint64_t *ret0)
{
	bool	  handled;
	thread_t *current = thread_get_self();

	if (compiler_unexpected(current->psci_group == NULL)) {
		handled = false;
	} else {
		psci_ret_t ret = psci_affinity_info(psci_mpidr_cast(arg1),
						    (uint32_t)arg2);
		*ret0	       = (uint64_t)ret;
		handled	       = true;
	}
	return handled;
}

static noreturn void
psci_stop_all_vcpus(void)
{
	thread_t *current = thread_get_self();
	assert(current != NULL);
	assert(current->kind == THREAD_KIND_VCPU);

	vpm_group_t *psci_group = current->psci_group;
	if (psci_group != NULL) {
		for (index_t i = 0U; i < util_array_size(psci_group->psci_cpus);
		     i++) {
			thread_t *thread =
				atomic_load_consume(&psci_group->psci_cpus[i]);
			if ((thread != NULL) && (thread != current)) {
				error_t err = thread_kill(thread);
				if (err != OK) {
					panic("Unable to kill VCPU");
				}
			}
		}
	}

	// Force power off
	(void)vcpu_poweroff(false, true);
	// We should not be denied when force is true
	panic("vcpu_poweroff(force=true) returned");
}

bool
psci_system_off(void)
{
	bool	  handled;
	thread_t *current = thread_get_self();

	if (compiler_unexpected(current->psci_group == NULL)) {
		handled = false;
	} else {
		if (vcpu_option_flags_get_critical(&current->vcpu_options)) {
			// HLOS VM calls to this function are passed directly to
			// the firmware, to power off the physical device.
			trigger_power_system_off_event();
			panic("system_off event returned");
		}

		psci_stop_all_vcpus();
	}

	return handled;
}

bool
psci_system_reset(void)
{
	bool	  handled;
	thread_t *current = thread_get_self();

	if (compiler_unexpected(current->psci_group == NULL)) {
		handled = false;
	} else {
		if (vcpu_option_flags_get_critical(&current->vcpu_options)) {
			// HLOS VM calls to this function are passed directly to
			// the firmware, to reset the physical device.
			error_t ret = OK;
			(void)trigger_power_system_reset_event(
				PSCI_REQUEST_SYSTEM_RESET, 0U, &ret);
			panic("system_reset event returned");
		}

#if defined(INTERFACE_VCPU_RUN)
		// Tell the proxy thread that a reset was requested
		thread_get_self()->psci_system_reset = true;
		thread_get_self()->psci_system_reset_type =
			PSCI_REQUEST_SYSTEM_RESET;
		thread_get_self()->psci_system_reset_cookie = 0U;
#endif

		psci_stop_all_vcpus();
	}

	return handled;
}

static uint32_t
psci_system_reset2(uint64_t reset_type, uint64_t cookie)
{
	uint32_t  ret;
	thread_t *current = thread_get_self();

	if (vcpu_option_flags_get_critical(&current->vcpu_options)) {
		// HLOS VM calls to this function are passed directly to the
		// firmware, to reset the physical device.
		error_t error = OK;
		(void)trigger_power_system_reset_event(reset_type, cookie,
						       &error);

		if (error == ERROR_ARGUMENT_INVALID) {
			ret = (uint32_t)PSCI_RET_INVALID_PARAMETERS;
		} else {
			ret = (uint32_t)PSCI_RET_NOT_SUPPORTED;
		}
	} else {
#if defined(INTERFACE_VCPU_RUN)
		// Tell the proxy thread that a reset was requested
		thread_get_self()->psci_system_reset	    = true;
		thread_get_self()->psci_system_reset_type   = reset_type;
		thread_get_self()->psci_system_reset_cookie = cookie;
#endif

		psci_stop_all_vcpus();
	}

	return ret;
}

bool
psci_system_reset2_32(uint32_t arg1, uint32_t arg2, uint32_t *ret0)
{
	bool	  handled;
	thread_t *current = thread_get_self();

	if (compiler_unexpected(current->psci_group == NULL)) {
		handled = false;
	} else {
		*ret0	= psci_system_reset2(arg1, arg2);
		handled = true;
	}

	return handled;
}

bool
psci_system_reset2_64(uint64_t arg1, uint64_t arg2, uint64_t *ret0)
{
	bool	  handled;
	thread_t *current = thread_get_self();

	if (compiler_unexpected(current->psci_group == NULL)) {
		handled = false;
	} else {
		*ret0 = psci_system_reset2(
			(uint32_t)arg1 | PSCI_REQUEST_SYSTEM_RESET2_64, arg2);
		handled = true;
	}

	return handled;
}

bool
psci_features(uint32_t arg1, uint32_t *ret0)
{
	thread_t *current  = thread_get_self();
	bool	  has_psci = current->psci_group != NULL;

	smccc_function_id_t fn_id = smccc_function_id_cast(arg1);
	uint32_t	    ret	  = SMCCC_UNKNOWN_FUNCTION32;
	smccc_function_t    fn	  = smccc_function_id_get_function(&fn_id);

	if (has_psci &&
	    (smccc_function_id_get_owner_id(&fn_id) ==
	     SMCCC_OWNER_ID_STANDARD) &&
	    smccc_function_id_get_is_fast(&fn_id) &&
	    (smccc_function_id_get_res0(&fn_id) == 0U)) {
		ret = smccc_function_id_get_is_smc64(&fn_id)
			      ? trigger_psci_features64_event(
					(psci_function_t)fn)
			      : trigger_psci_features32_event(
					(psci_function_t)fn);
	} else if ((smccc_function_id_get_owner_id(&fn_id) ==
		    SMCCC_OWNER_ID_ARCH) &&
		   smccc_function_id_get_is_fast(&fn_id) &&
		   !smccc_function_id_get_is_smc64(&fn_id) &&
		   (smccc_function_id_get_res0(&fn_id) == 0U) &&
		   (fn == (smccc_function_t)SMCCC_ARCH_FUNCTION_VERSION)) {
		// SMCCC>=1.1 is implemented and SMCCC_VERSION is safe to call.
		ret = (uint32_t)PSCI_RET_SUCCESS;
	} else {
		/* Do Nothing */
	}

	*ret0 = ret;
	return true;
}

error_t
psci_handle_object_create_thread(thread_create_t thread_create)
{
	thread_t *thread = thread_create.thread;
	assert(thread != NULL);

	// FIXME:
	psci_suspend_powerstate_t pstate = psci_suspend_powerstate_default();
#if !defined(PSCI_AFFINITY_LEVELS_NOT_SUPPORTED) ||                            \
	!PSCI_AFFINITY_LEVELS_NOT_SUPPORTED
	psci_suspend_powerstate_stateid_t stateid =
		platform_psci_deepest_cluster_level_stateid(
			thread->scheduler_affinity);
#else
	psci_suspend_powerstate_stateid_t stateid =
		platform_psci_deepest_cpu_level_stateid(
			thread->scheduler_affinity);
#endif
	psci_suspend_powerstate_set_StateID(&pstate, stateid);
	psci_suspend_powerstate_set_StateType(
		&pstate, PSCI_SUSPEND_POWERSTATE_TYPE_POWERDOWN);

	// Initialize to deepest possible state
	thread->psci_suspend_state = pstate;

	return OK;
}

error_t
psci_handle_object_activate_thread(thread_t *thread)
{
	error_t err;
	if (thread->kind != THREAD_KIND_VCPU) {
		thread->vpm_mode = VPM_MODE_NONE;
		err		 = OK;
	} else if (thread->psci_group == NULL) {
		thread->vpm_mode = VPM_MODE_IDLE;
		err		 = OK;
	} else {
		assert(scheduler_is_blocked(thread, SCHEDULER_BLOCK_VCPU_OFF));

		thread->vpm_mode  = vpm_group_option_flags_get_no_aggregation(
					    &thread->psci_group->options)
					    ? VPM_MODE_NONE
					    : VPM_MODE_PSCI;
		cpu_index_t index = thread->psci_index;
		thread_t   *tmp_null = NULL;

		if (!cpulocal_index_valid(index)) {
			err = ERROR_OBJECT_CONFIG;
		} else if (!atomic_compare_exchange_strong_explicit(
				   &thread->psci_group->psci_cpus[index],
				   &tmp_null, thread, memory_order_release,
				   memory_order_relaxed)) {
			err = ERROR_DENIED;
		} else {
			err = OK;
		}
	}
	return err;
}

void
psci_handle_object_deactivate_thread(thread_t *thread)
{
	assert(thread != NULL);

	if (thread->psci_group != NULL) {
		thread_t   *tmp	  = thread;
		cpu_index_t index = thread->psci_index;

		(void)atomic_compare_exchange_strong_explicit(
			&thread->psci_group->psci_cpus[index], &tmp, NULL,
			memory_order_relaxed, memory_order_relaxed);
		object_put_vpm_group(thread->psci_group);
	}

	if (thread->vpm_mode == VPM_MODE_PSCI) {
		scheduler_lock(thread);
		psci_pm_list_delete(scheduler_get_affinity(thread), thread);
		scheduler_unlock(thread);
	}
}

void
psci_handle_object_deactivate_vpm_group(vpm_group_t *pg)
{
	for (cpu_index_t i = 0; cpulocal_index_valid(i); i++) {
		assert(atomic_load_relaxed(&pg->psci_cpus[i]) == NULL);
	}

	cpulocal_begin();
	ipi_one_relaxed(IPI_REASON_IDLE, cpulocal_get_index());
	ipi_others_idle(IPI_REASON_IDLE);
	cpulocal_end();
}

error_t
vpm_group_configure(vpm_group_t *vpm_group, vpm_group_option_flags_t flags)
{
	vpm_group->options = flags;

	return OK;
}

error_t
vpm_attach(vpm_group_t *pg, thread_t *thread, index_t index)
{
	assert(pg != NULL);
	assert(thread != NULL);
	assert(atomic_load_relaxed(&thread->header.state) == OBJECT_STATE_INIT);
	assert(atomic_load_relaxed(&pg->header.state) == OBJECT_STATE_ACTIVE);

	error_t err;

	if (!cpulocal_index_valid((cpu_index_t)index)) {
		err = ERROR_ARGUMENT_INVALID;
	} else if (thread->kind != THREAD_KIND_VCPU) {
		err = ERROR_ARGUMENT_INVALID;
	} else {
		if (thread->psci_group != NULL) {
			object_put_vpm_group(thread->psci_group);
		}

		thread->psci_group = object_get_vpm_group_additional(pg);
		thread->psci_index = (cpu_index_t)index;
		trace_ids_set_vcpu_index(&thread->trace_ids,
					 (cpu_index_t)index);

		err = OK;
	}

	return err;
}

error_t
psci_handle_task_queue_execute(task_queue_entry_t *task_entry)
{
	assert(task_entry != NULL);
	vpm_group_t *vpm_group =
		vpm_group_container_of_psci_virq_task(task_entry);

	(void)virq_assert(&vpm_group->psci_system_suspend_virq, true);
	object_put_vpm_group(vpm_group);

	return OK;
}

error_t
vpm_bind_virq(vpm_group_t *vpm_group, vic_t *vic, virq_t virq)
{
	error_t ret;

	assert(vpm_group != NULL);
	assert(vic != NULL);

	ret = vic_bind_shared(&vpm_group->psci_system_suspend_virq, vic, virq,
			      VIRQ_TRIGGER_VPM_GROUP);

	return ret;
}

void
vpm_unbind_virq(vpm_group_t *vpm_group)
{
	assert(vpm_group != NULL);

	vic_unbind_sync(&vpm_group->psci_system_suspend_virq);
}

bool
vcpus_state_is_any_awake(vpm_group_suspend_state_t vm_state, uint32_t level,
			 cpu_index_t cpu)
{
	bool	 vcpu_awake = false;
	error_t	 ret;
	uint32_t start_idx = 0, children_counts = 0;

	uint64_t vcpus_state =
		vpm_group_suspend_state_get_vcpus_state(&vm_state);

	uint16_t vcluster_state =
		vpm_group_suspend_state_get_cluster_state(&vm_state);

	ret = platform_psci_get_index_by_level(cpu, &start_idx,
					       &children_counts, level);

	index_t idle_state = 0U, psci_index;
	if (ret != OK) {
		goto out;
	}

	for (index_t i = 0U; i < children_counts; i++) {
		// Check if another vcpu is awake
		psci_index = start_idx + i;

		if (level == 1U) {
			idle_state =
				(index_t)(vcpus_state >>
					  (psci_index *
					   PSCI_VCPUS_STATE_PER_VCPU_BITS)) &
				PSCI_VCPUS_STATE_PER_VCPU_MASK;
			if (platform_psci_is_cpu_active(
				    (psci_cpu_state_t)idle_state)) {
				vcpu_awake = true;
				goto out;
			}
		} else if (level == 2U) {
			idle_state = ((index_t)vcluster_state >>
				      ((psci_index % (PLATFORM_MAX_CORES)) *
				       PSCI_PER_CLUSTER_STATE_BITS)) &
				     PSCI_PER_CLUSTER_STATE_BITS_MASK;
			if (platform_psci_is_cluster_active(
				    (psci_cluster_state_L3_t)idle_state)) {
				vcpu_awake = true;
				goto out;
			}
		} else {
			// Only two levels are implemented. Return false
		}
	}
out:
	return vcpu_awake;
}

void
vcpus_state_set(vpm_group_suspend_state_t *vm_state, cpu_index_t cpu,
		psci_cpu_state_t cpu_state)
{
	uint64_t vcpus_state =
		vpm_group_suspend_state_get_vcpus_state(vm_state);

	vcpus_state &= ~((uint64_t)PSCI_VCPUS_STATE_PER_VCPU_MASK
			 << (cpu * PSCI_VCPUS_STATE_PER_VCPU_BITS));
	vcpus_state |= (uint64_t)cpu_state
		       << (cpu * PSCI_VCPUS_STATE_PER_VCPU_BITS);

	vpm_group_suspend_state_set_vcpus_state(vm_state, vcpus_state);
}

void
vcpus_state_clear(vpm_group_suspend_state_t *vm_state, cpu_index_t cpu)
{
	uint64_t vcpus_state =
		vpm_group_suspend_state_get_vcpus_state(vm_state);

	vcpus_state &= ~((uint64_t)PSCI_VCPUS_STATE_PER_VCPU_MASK
			 << (cpu * PSCI_VCPUS_STATE_PER_VCPU_BITS));

	vpm_group_suspend_state_set_vcpus_state(vm_state, vcpus_state);
}

error_t
psci_handle_vcpu_suspend(thread_t *current)
{
	error_t ret;

	if (current->vpm_mode != VPM_MODE_NONE) {
		ret = psci_vcpu_suspend(current);
	} else {
		ret = OK;
	}

	if (ret == OK) {
		TRACE(PSCI, PSCI_VPM_VCPU_SUSPEND,
		      "psci vcpu suspend: {:#x} - VM {:d}", (uintptr_t)current,
		      current->addrspace->vmid);
	}

	return ret;
}

void
psci_unwind_vcpu_suspend(thread_t *current)
{
	if (current->vpm_mode != VPM_MODE_NONE) {
		psci_vcpu_resume(current);
	}
}

bool
psci_handle_trapped_idle(void)
{
	thread_t *current = thread_get_self();
	bool	  handled = false;

	if (current->vpm_mode == VPM_MODE_IDLE) {
		error_t err = vcpu_suspend();
		if ((err != OK) && (err != ERROR_BUSY)) {
			panic("unhandled vcpu_suspend error (WFI)");
		}
		handled = true;
	}

	return handled;
}

void
psci_handle_vcpu_resume(thread_t *vcpu)
{
	TRACE(PSCI, PSCI_VPM_VCPU_RESUME,
	      "psci vcpu resume: {:#x} - VM {:d} - VCPU {:d}", (uintptr_t)vcpu,
	      vcpu->addrspace->vmid, vcpu->psci_index);

	if (vcpu->vpm_mode != VPM_MODE_NONE) {
		psci_vcpu_resume(vcpu);
	}
}

void
psci_handle_vcpu_started(bool warm_reset)
{
	// If the VCPU has been warm-reset, there was no vcpu_stopped event and
	// no automatic psci_vcpu_suspend() call, so there's no need for a
	// wakeup here.
	if (!warm_reset) {
		thread_t *current = thread_get_self();

		TRACE(PSCI, PSCI_VPM_VCPU_RESUME,
		      "psci vcpu started: {:#x} - VM {:d}", (uintptr_t)current,
		      current->addrspace->vmid);

		if (current->vpm_mode != VPM_MODE_NONE) {
			preempt_disable();
			psci_vcpu_resume(current);
			preempt_enable();
		}
	}
}

void
psci_handle_vcpu_wakeup(thread_t *vcpu)
{
	if (scheduler_is_blocked(vcpu, SCHEDULER_BLOCK_VCPU_SUSPEND)) {
		vcpu_resume(vcpu);
	}
}

void
psci_handle_vcpu_wakeup_self(void)
{
	thread_t *current = thread_get_self();
	assert(!scheduler_is_blocked(current, SCHEDULER_BLOCK_VCPU_SUSPEND) ||
	       thread_is_dying(current));
}

bool
psci_handle_vcpu_expects_wakeup(const thread_t *thread)
{
	return scheduler_is_blocked(thread, SCHEDULER_BLOCK_VCPU_SUSPEND);
}

#if defined(INTERFACE_VCPU_RUN)
vcpu_run_state_t
psci_handle_vcpu_run_check(const thread_t *thread, register_t *state_data_0,
			   register_t *state_data_1)
{
	vcpu_run_state_t ret;

	if (thread->psci_system_reset) {
		ret	      = VCPU_RUN_STATE_PSCI_SYSTEM_RESET;
		*state_data_0 = thread->psci_system_reset_type;
		*state_data_1 = thread->psci_system_reset_cookie;
	} else if (psci_handle_vcpu_expects_wakeup(thread)) {
		ret = VCPU_RUN_STATE_EXPECTS_WAKEUP;
		*state_data_0 =
			psci_suspend_powerstate_raw(thread->psci_suspend_state);
		vpm_group_t *vpm_group = thread->psci_group;
		bool	     system_suspend;
		if (vpm_group != NULL) {
			vpm_group_suspend_state_t vm_state =
				atomic_load_acquire(
					&vpm_group->psci_vm_suspend_state);
			system_suspend =
				vpm_group_suspend_state_get_system_suspend(
					&vm_state);
		} else {
			system_suspend = false;
		}
		vcpu_run_wakeup_from_state_t from_state =
			system_suspend
				? VCPU_RUN_WAKEUP_FROM_STATE_PSCI_SYSTEM_SUSPEND
				: VCPU_RUN_WAKEUP_FROM_STATE_PSCI_CPU_SUSPEND;
		*state_data_1 = (register_t)from_state;
	} else {
		ret = VCPU_RUN_STATE_BLOCKED;
	}

	return ret;
}
#endif

error_t
psci_handle_vcpu_poweron(thread_t *vcpu)
{
	if (compiler_unexpected(vcpu->psci_group == NULL)) {
		goto out;
	}

	(void)atomic_fetch_add_explicit(&vcpu->psci_group->psci_online_count,
					1U, memory_order_relaxed);
	cpu_index_t cpu = vcpu->scheduler_affinity;
	if (cpulocal_index_valid(cpu)) {
		psci_vcpu_clear_vcpu_state(vcpu, cpu);
	}

out:
	return OK;
}

error_t
psci_handle_vcpu_poweroff(thread_t *vcpu, bool last_cpu, bool force)
{
	error_t	     ret;
	vpm_group_t *psci_group = vcpu->psci_group;

	if (psci_group == NULL) {
		// This is always the last CPU in the VM, so permit the poweroff
		// request if and only if it is intended for the last CPU or is
		// forced.
		ret = (last_cpu || force) ? OK : ERROR_DENIED;
	} else if (vcpu->vpm_mode == VPM_MODE_PSCI) {
		count_t online_cpus =
			atomic_load_relaxed(&psci_group->psci_online_count);
		do {
			assert(online_cpus > 0U);
			if (!force && (last_cpu != (online_cpus == 1U))) {
				ret = ERROR_DENIED;
				goto out;
			}
		} while (!atomic_compare_exchange_weak_explicit(
			&psci_group->psci_online_count, &online_cpus,
			online_cpus - 1U, memory_order_relaxed,
			memory_order_relaxed));

		ret = OK;
	} else {
		assert(vcpu->vpm_mode == VPM_MODE_NONE);
		ret = OK;
	}

out:
	return ret;
}

void
psci_handle_vcpu_stopped(void)
{
	thread_t *vcpu = thread_get_self();

	if (vcpu->psci_group != NULL) {
		// Stopping a VCPU forces it into a power-off suspend state.
		psci_suspend_powerstate_t pstate =
			psci_suspend_powerstate_default();
		psci_suspend_powerstate_set_StateType(
			&pstate, PSCI_SUSPEND_POWERSTATE_TYPE_POWERDOWN);
		psci_suspend_powerstate_stateid_t stateid;

		preempt_disable();
		cpu_index_t cpu = cpulocal_get_index();

#if !defined(PSCI_AFFINITY_LEVELS_NOT_SUPPORTED) ||                            \
	!PSCI_AFFINITY_LEVELS_NOT_SUPPORTED
		// FIXME:
		if (vcpu->psci_group->psci_mode == PSCI_MODE_PC) {
			stateid = platform_psci_deepest_cluster_level_stateid(
				cpu);
		} else
#endif
		{
			stateid = platform_psci_deepest_cpu_level_stateid(cpu);
		}
		preempt_enable();

		psci_suspend_powerstate_set_StateID(&pstate, stateid);
		vcpu->psci_suspend_state = pstate;
	}

	if (vcpu->vpm_mode != VPM_MODE_NONE) {
		preempt_disable();
		error_t ret = psci_vcpu_suspend(vcpu);
		preempt_enable();
		// Note that psci_vcpu_suspend can only fail if we are in OSI
		// mode and requesting a cluster suspend state, which can't
		// happen here because we set a non-cluster state above.
		assert(ret == OK);
	}
}

void
psci_handle_power_cpu_online(void)
{
	(void)psci_set_vpm_active_pcpus_bit(cpulocal_get_index());
}

void
psci_handle_power_cpu_offline(void)
{
	(void)psci_clear_vpm_active_pcpus_bit(cpulocal_get_index());
}

```

`hyp/vm/psci/src/psci_pm_list.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>

#include <cpulocal.h>
#include <ipi.h>
#include <list.h>
#include <spinlock.h>

#include "event_handlers.h"
#include "psci_pm_list.h"

CPULOCAL_DECLARE_STATIC(list_t, vcpu_pm_list);
CPULOCAL_DECLARE_STATIC(spinlock_t, vcpu_pm_list_lock);

void
psci_pm_list_init(void)
{
	for (cpu_index_t cpu = 0U; cpu < PLATFORM_MAX_CORES; cpu++) {
		list_init(&CPULOCAL_BY_INDEX(vcpu_pm_list, cpu));
		spinlock_init(&CPULOCAL_BY_INDEX(vcpu_pm_list_lock, cpu));
	}
}

list_t *
psci_pm_list_get_self(void)
{
	return &CPULOCAL(vcpu_pm_list);
}

void
psci_pm_list_insert(cpu_index_t cpu_index, thread_t *vcpu)
{
	list_t *list = &CPULOCAL_BY_INDEX(vcpu_pm_list, cpu_index);

	spinlock_acquire(&CPULOCAL_BY_INDEX(vcpu_pm_list_lock, cpu_index));
	list_insert_at_tail_release(list, &vcpu->psci_pm_list_node);
	spinlock_release(&CPULOCAL_BY_INDEX(vcpu_pm_list_lock, cpu_index));
}

void
psci_pm_list_delete(cpu_index_t cpu_index, thread_t *vcpu)
{
	list_t *list = &CPULOCAL_BY_INDEX(vcpu_pm_list, cpu_index);

	spinlock_acquire(&CPULOCAL_BY_INDEX(vcpu_pm_list_lock, cpu_index));
	(void)list_delete_node(list, &vcpu->psci_pm_list_node);
	spinlock_release(&CPULOCAL_BY_INDEX(vcpu_pm_list_lock, cpu_index));

	ipi_one_idle(IPI_REASON_IDLE, cpu_index);
}

```

`hyp/vm/psci_pc/build.conf`:

```conf
# © 2022 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

base_module hyp/vm/psci
events psci_pc.ev
source psci_pc.c

```

`hyp/vm/psci_pc/psci_pc.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

module psci_pc

#include "psci_events.h"

PSCI_FUNCTION32(PSCI_SET_SUSPEND_MODE, 0U, pc_set_suspend_mode, arg1, ret0)

subscribe boot_cold_init()

subscribe object_activate_vpm_group

subscribe idle_yield
	// Run late, but before handlers that may sleep, to check
	// whether we should suspend the physical CPU instead
	priority -10
	require_preempt_disabled

```

`hyp/vm/psci_pc/src/psci_pc.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>

#include <hypcontainers.h>
#include <hyprights.h>

#include <atomic.h>
#include <bitmap.h>
#include <compiler.h>
#include <cpulocal.h>
#include <cspace.h>
#include <cspace_lookup.h>
#include <idle.h>
#include <ipi.h>
#include <list.h>
#include <log.h>
#include <object.h>
#include <panic.h>
#include <partition_alloc.h>
#include <platform_cpu.h>
#include <platform_psci.h>
#include <preempt.h>
#include <psci.h>
#include <rcu.h>
#include <scheduler.h>
#include <spinlock.h>
#include <task_queue.h>
#include <thread.h>
#include <timer_queue.h>
#include <trace.h>
#include <trace_helpers.h>
#include <vcpu.h>
#include <vic.h>
#include <virq.h>
#include <vpm.h>

#include <events/power.h>
#include <events/psci.h>
#include <events/vpm.h>

#include "event_handlers.h"
#include "psci_arch.h"
#include "psci_common.h"
#include "psci_pm_list.h"

void
psci_pc_handle_boot_cold_init(void)
{
#if !defined(PSCI_SET_SUSPEND_MODE_NOT_SUPPORTED) ||                           \
	!PSCI_SET_SUSPEND_MODE_NOT_SUPPORTED
	error_t ret = platform_psci_set_suspend_mode(PSCI_MODE_PC);
	assert(ret == OK);
#endif
}

uint32_t
psci_cpu_suspend_features(void)
{
	uint32_t ret;

	// Only Platform Co-ordinated mode, extended StateID
	ret = 2U;

	return ret;
}

bool
psci_pc_set_suspend_mode(uint32_t arg1, uint32_t *ret0)
{
	bool	   handled;
	psci_ret_t ret;

	thread_t *current = thread_get_self();

	if (current->psci_group == NULL) {
		handled = false;
	} else {
		if (arg1 == PSCI_MODE_PC) {
			ret = PSCI_RET_SUCCESS;
		} else {
			ret = PSCI_RET_INVALID_PARAMETERS;
			TRACE(PSCI, INFO,
			      "psci_set_suspend_mode - INVALID_PARAMETERS - VM {:d}",
			      current->addrspace->vmid);
		}
		*ret0	= (uint32_t)ret;
		handled = true;
	}

	return handled;
}

error_t
psci_pc_handle_object_activate_vpm_group(vpm_group_t *pg)
{
	spinlock_init(&pg->psci_lock);
	task_queue_init(&pg->psci_virq_task, TASK_QUEUE_CLASS_VPM_GROUP_VIRQ);

	// Default psci mode to be platfom-coordinated
	pg->psci_mode = PSCI_MODE_PC;

	// Initialize vcpus states of the vpm to the deepest suspend state
	// FIXME:
	cpulocal_begin();
	psci_cpu_state_t cpu_state =
		platform_psci_deepest_cpu_state(cpulocal_get_index());
	cpulocal_end();

	vpm_group_suspend_state_t vm_state = vpm_group_suspend_state_default();

	for (cpu_index_t i = 0;
	     i < (PSCI_VCPUS_STATE_BITS / PSCI_VCPUS_STATE_PER_VCPU_BITS);
	     i++) {
		vcpus_state_set(&vm_state, i, cpu_state);
	}

	atomic_store_release(&pg->psci_vm_suspend_state, vm_state);

	return OK;
}

vpm_state_t
vpm_get_state(vpm_group_t *vpm_group)
{
	vpm_state_t vpm_state = VPM_STATE_NO_STATE;

	vpm_group_suspend_state_t vm_state =
		atomic_load_acquire(&vpm_group->psci_vm_suspend_state);

	if (vcpus_state_is_any_awake(vm_state, PLATFORM_MAX_HIERARCHY, 0)) {
		vpm_state = VPM_STATE_RUNNING;
	} else {
		vpm_state = VPM_STATE_CPUS_SUSPENDED;
	}

	return vpm_state;
}

/*
 * this clears the vcpu state for core which has started to boot from
 * hw followed by firmware cluster and suspend states are still
 * cleared by the same in wake-up path by calling into psci_vcpu_wakeup
 */
void
psci_vcpu_clear_vcpu_state(thread_t *thread, cpu_index_t target_cpu)
{
	(void)target_cpu;
	if (thread->vpm_mode != VPM_MODE_PSCI) {
		goto out;
	}

	assert(thread->psci_group != NULL);

	vpm_group_t *vpm_group = thread->psci_group;
	cpu_index_t  vcpu_id   = thread->psci_index;

	thread->psci_suspend_state = psci_suspend_powerstate_default();

	vpm_group_suspend_state_t old_state =
		atomic_load_relaxed(&vpm_group->psci_vm_suspend_state);
	vpm_group_suspend_state_t new_state;

	new_state = old_state;
	vcpus_state_clear(&new_state, vcpu_id);

out:
	// Nothing to do for non PSCI threads
	return;
}

void
psci_vcpu_resume(thread_t *thread)
{
	assert(thread->vpm_mode != VPM_MODE_NONE);

	scheduler_lock_nopreempt(thread);
	psci_vpm_active_vcpus_get(scheduler_get_active_affinity(thread),
				  thread);
	scheduler_unlock_nopreempt(thread);

	if (thread->vpm_mode != VPM_MODE_PSCI) {
		goto out;
	}

	assert(thread->psci_group != NULL);

	vpm_group_t *vpm_group = thread->psci_group;
	cpu_index_t  vcpu_id   = thread->psci_index;

	thread->psci_suspend_state = psci_suspend_powerstate_default();

	vpm_group_suspend_state_t old_state =
		atomic_load_relaxed(&vpm_group->psci_vm_suspend_state);
	vpm_group_suspend_state_t new_state;

	do {
		new_state = old_state;

		vcpus_state_clear(&new_state, vcpu_id);

	} while (!atomic_compare_exchange_strong_explicit(
		&vpm_group->psci_vm_suspend_state, &old_state, new_state,
		memory_order_relaxed, memory_order_relaxed));

out:
	// Nothing to do for non PSCI threads
	return;
}

error_t
psci_vcpu_suspend(thread_t *current)
{
	assert(current->vpm_mode != VPM_MODE_NONE);

	// Decrement refcount of the PCPU
	scheduler_lock_nopreempt(current);
	psci_vpm_active_vcpus_put(scheduler_get_active_affinity(current),
				  current);
	scheduler_unlock_nopreempt(current);

	if (current->vpm_mode != VPM_MODE_PSCI) {
		goto out;
	}

	assert(current->psci_group != NULL);

	vpm_group_t	*vpm_group = current->psci_group;
	cpu_index_t	 vcpu_id   = current->psci_index;
	psci_cpu_state_t cpu_state =
		platform_psci_get_cpu_state(current->psci_suspend_state);

	vpm_group_suspend_state_t new_state;
	vpm_group_suspend_state_t old_state;

	// Set vcpus_state of corresponding cpu.
	old_state = atomic_load_relaxed(&vpm_group->psci_vm_suspend_state);

	do {
		new_state = old_state;
		vcpus_state_set(&new_state, vcpu_id, cpu_state);

	} while (!atomic_compare_exchange_strong_explicit(
		&vpm_group->psci_vm_suspend_state, &old_state, new_state,
		memory_order_relaxed, memory_order_relaxed));

out:
	return OK;
}

idle_state_t
psci_pc_handle_idle_yield(bool in_idle_thread)
{
	assert_preempt_disabled();

	idle_state_t idle_state = IDLE_STATE_IDLE;

	if (!in_idle_thread) {
		goto out;
	}

	if (rcu_has_pending_updates()) {
		goto out;
	}

	cpu_index_t cpu = cpulocal_get_index();

	// Check if there is any vcpu running in this cpu
	if (!psci_vpm_active_vcpus_is_zero(cpu)) {
		goto out;
	}

	thread_t *vcpu	       = NULL;
	list_t	 *psci_pm_list = psci_pm_list_get_self();
	assert(psci_pm_list != NULL);

	psci_suspend_powerstate_t pstate = psci_suspend_powerstate_default();
	psci_cpu_state_t cpu_state	 = platform_psci_deepest_cpu_state(cpu);

	// Iterate through affine VCPUs and get the shallowest cpu-level state
	rcu_read_start();
	list_foreach_container_consume (vcpu, psci_pm_list, thread,
					psci_pm_list_node) {
		psci_cpu_state_t cpu1 =
			platform_psci_get_cpu_state(vcpu->psci_suspend_state);
		cpu_state = platform_psci_shallowest_cpu_state(cpu_state, cpu1);
	}
	rcu_read_finish();

	// Do not go to suspend if shallowest cpu state is zero. This may happen
	// if a vcpu has started after doing the initial check of 'any vcpu
	// running in this cpu' and therefore has been added to the psci_pm_list
	// with a psci_suspend_state of 0.
	if (cpu_state == 0U) {
		goto out;
	}

	platform_psci_set_cpu_state(&pstate, cpu_state);

	if (platform_psci_is_cpu_poweroff(cpu_state)) {
		psci_suspend_powerstate_set_StateType(
			&pstate, PSCI_SUSPEND_POWERSTATE_TYPE_POWERDOWN);
	} else {
		psci_suspend_powerstate_set_StateType(
			&pstate,
			PSCI_SUSPEND_POWERSTATE_TYPE_STANDBY_OR_RETENTION);
	}

	bool last_cpu = false;

	if (psci_clear_vpm_active_pcpus_bit(cpu)) {
		last_cpu = true;
	}

	// Fence to prevent any power_cpu_suspend event handlers conditional on
	// last_cpu (especially the trigger of power_system_suspend) being
	// reordered before the psci_clear_vpm_active_pcpus_bit() above. This
	// matches the fence before the resume event below.
	atomic_thread_fence(memory_order_seq_cst);

	error_t suspend_result = trigger_power_cpu_suspend_event(
		pstate,
		psci_suspend_powerstate_get_StateType(&pstate) ==
			PSCI_SUSPEND_POWERSTATE_TYPE_POWERDOWN,
		last_cpu);
	if (suspend_result == OK) {
		bool_result_t ret;

		TRACE(PSCI, INFO, "psci power_cpu_suspend {:#x}",
		      psci_suspend_powerstate_raw(pstate));

		ret = platform_cpu_suspend(pstate);

		// Check if this is the first cpu to wake up
		bool first_cpu = psci_set_vpm_active_pcpus_bit(cpu);
		suspend_result = ret.e;

		// Fence to prevent any power_cpu_resume event handlers
		// conditional on first_cpu (especially the trigger of
		// power_system_resume) being reordered before the
		// psci_set_vpm_active_pcpus_bit() above. This matches the
		// fence before the suspend event above.
		atomic_thread_fence(memory_order_seq_cst);

		trigger_power_cpu_resume_event((ret.e == OK) && ret.r,
					       first_cpu);
		TRACE(PSCI, INFO,
		      "psci power_cpu_suspend wakeup; poweroff {:d} system_resume {:d} error {:d}",
		      ret.r, first_cpu, (register_t)ret.e);
	} else {
		TRACE(PSCI, INFO, "psci power_cpu_suspend failed: {:d}",
		      (unsigned int)suspend_result);
		(void)psci_set_vpm_active_pcpus_bit(cpu);
	}

	if ((suspend_result == OK) || (suspend_result == ERROR_BUSY)) {
		// Return from successful suspend, or suspend failure due to a
		// pending wakeup. Poll for wakeup events.
		idle_state = idle_wakeup();
	} else if (suspend_result != ERROR_DENIED) {
		TRACE_AND_LOG(ERROR, WARN, "ERROR: psci suspend error {:d}",
			      (register_t)suspend_result);
		panic("unhandled suspend error");
	} else {
		// suspend state was denied, re-run psci aggregation.
		idle_state = IDLE_STATE_WAKEUP;
	}

out:
	return idle_state;
}

```

`hyp/vm/rootvm/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

events rootvm.ev
types rootvm.tc
base_module hyp/core/boot
source rootvm_init.c

```

`hyp/vm/rootvm/rootvm.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

interface rootvm

include <qcbor.h>

event rootvm_init
	param root_partition: partition_t *
	param root_thread: thread_t *
	param root_cspace: cspace_t *
	param hyp_env: hyp_env_data_t *
	param qcbor_enc_ctxt: qcbor_enc_ctxt_t *

event rootvm_init_late
	param root_partition: partition_t *
	param root_thread: thread_t *
	param root_cspace: cspace_t *
	param hyp_env: const hyp_env_data_t *

event rootvm_started
	param root_thread: thread_t *

module rootvm

subscribe boot_hypervisor_start
	handler rootvm_init()
	priority last
	require_preempt_disabled

```

`hyp/vm/rootvm/rootvm.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// Place the root VM at the default scheduling priority. This will typically
// be shared with most HLOS VMs.
//
// Note: VMs with higher priority than this must perform directed yields to
// the root VM while waiting for it to complete an operation. VMs with lower
// priority than this may be blocked by long-running root VM operations.
define ROOTVM_PRIORITY public constant type priority_t =
	SCHEDULER_DEFAULT_PRIORITY;

extend hyp_env_data structure {
	vcpu_capid		type cap_id_t;
	entry_ipa		type vmaddr_t;
	env_ipa			type vmaddr_t;
	env_data_size		size;
};

define ROOTVM_ENV_DATA_SIGNATURE public constant = 0x454D5652;
define ROOTVM_ENV_DATA_VERSION public constant = 0x1000;

// Do NOT extend this structure, since its cross image interface
define rt_env_data public structure {
	signature	uint32;
	version	uint16;

	runtime_ipa	type vmaddr_t;
	app_ipa		type vmaddr_t;
	app_heap_ipa	type vmaddr_t;
	app_heap_size	size;

	vcpu_capid	type cap_id_t;
	timer_freq	uint64;
	gicd_base	type paddr_t;
	gicr_base	type paddr_t;

	rm_config_offset	size;
	rm_config_size	size;
};

define RM_ENV_DATA_SIGNATURE public constant = 0x524D4544;
define RM_ENV_DATA_VERSION public constant = 0x1000;

// Do NOT extend this structure, since its cross image interface
define rm_env_data_hdr public structure {
	signature	uint32;
	version	uint16;
	data_payload_offset	uint32;
	data_payload_size	uint32;
};

```

`hyp/vm/rootvm/src/rootvm_init.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// rootvm_init() is allowed to call partition_get_root().
#define ROOTVM_INIT 1

#include <assert.h>
#include <hyptypes.h>
#include <string.h>

#include <attributes.h>
#include <bitmap.h>
#include <cpulocal.h>
#include <cspace.h>
#include <memdb.h>
#include <object.h>
#include <panic.h>
#include <partition.h>
#include <partition_alloc.h>
#include <partition_init.h>
#include <platform_mem.h>
#include <qcbor.h>
#include <scheduler.h>
#include <spinlock.h>
#include <thread.h>
#include <util.h>
#include <vcpu.h>

#include <events/object.h>
#include <events/rootvm.h>

#include <asm/cache.h>
#include <asm/cpu.h>

#include "boot_init.h"
#include "event_handlers.h"

// FIXME: remove when we have a device tree where to read it from
// dummy value.
#define MAX_CAPS 2048

static void
copy_rm_env_data_to_rootvm_mem(hyp_env_data_t		hyp_env,
			       const rm_env_data_hdr_t *rm_env_data,
			       rt_env_data_t *crt_env, uint32_t env_data_size)
{
	paddr_t hyp_env_phys = hyp_env.env_ipa - hyp_env.me_ipa_base +
			       PLATFORM_ROOTVM_LMA_BASE;
	assert(util_is_baligned(hyp_env_phys, PGTABLE_VM_PAGE_SIZE));

	void *va = partition_phys_map(hyp_env_phys, env_data_size);
	partition_phys_access_enable(va);

	(void)memcpy(va, (void *)crt_env,
		     (rm_env_data->data_payload_size + sizeof(*rm_env_data) +
		      sizeof(*crt_env)));
	CACHE_CLEAN_RANGE((rm_env_data_hdr_t *)va,
			  (rm_env_data->data_payload_size +
			   sizeof(*rm_env_data) + sizeof(*crt_env)));

	partition_phys_access_disable(va);
	partition_phys_unmap(va, hyp_env_phys, env_data_size);
}

static void
rootvm_close_env_data(qcbor_enc_ctxt_t	*qcbor_enc_ctxt,
		      rm_env_data_hdr_t *rm_env_data)
{
	qcbor_err_t	    cb_err;
	const_useful_buff_t payload_out_buff;

	payload_out_buff.ptr = NULL;
	payload_out_buff.len = 0;

	cb_err = QCBOREncode_Finish(qcbor_enc_ctxt, &payload_out_buff);

	if (cb_err != QCBOR_SUCCESS) {
		panic("Env data encoding error, increase the buffer size");
	}

	rm_env_data->data_payload_size = (uint32_t)payload_out_buff.len;
}

typedef struct {
	hyp_env_data_t	   hyp_env;
	qcbor_enc_ctxt_t  *qcbor_enc_ctxt;
	rm_env_data_hdr_t *rm_env_data;
	rt_env_data_t	  *crt_env;
} rootvm_init_env_info;

static rootvm_init_env_info
rootvm_init_env_data(partition_t *root_partition, uint32_t env_data_size)
{
	void_ptr_result_t alloc_ret;
	hyp_env_data_t	  hyp_env; // Local on stack used as context
	qcbor_enc_ctxt_t *qcbor_enc_ctxt;

	rm_env_data_hdr_t *rm_env_data;
	rt_env_data_t	  *crt_env;
	uint32_t	   remaining_size;

	alloc_ret = partition_alloc(root_partition, env_data_size,
				    PGTABLE_VM_PAGE_SIZE);
	if (alloc_ret.e != OK) {
		panic("Allocate env_data failed");
	}
	crt_env = (rt_env_data_t *)alloc_ret.r;
	(void)memset_s(crt_env, env_data_size, 0, env_data_size);

	alloc_ret = partition_alloc(root_partition, sizeof(*qcbor_enc_ctxt),
				    alignof(*qcbor_enc_ctxt));
	if (alloc_ret.e != OK) {
		panic("Allocate cbor_ctxt failed");
	}

	qcbor_enc_ctxt = (qcbor_enc_ctxt_t *)alloc_ret.r;

	(void)memset_s(qcbor_enc_ctxt, sizeof(*qcbor_enc_ctxt), 0,
		       sizeof(*qcbor_enc_ctxt));

	(void)memset_s(&hyp_env, sizeof(hyp_env), 0, sizeof(hyp_env));

	hyp_env.env_data_size = env_data_size;
	remaining_size	      = env_data_size;

	crt_env->signature = ROOTVM_ENV_DATA_SIGNATURE;
	crt_env->version   = 1;

	size_t rm_config_offset =
		util_balign_up(sizeof(*crt_env), alignof(*rm_env_data));
	assert(remaining_size >= (rm_config_offset + sizeof(*rm_env_data)));

	remaining_size -= rm_config_offset;
	rm_env_data =
		(rm_env_data_hdr_t *)((uintptr_t)crt_env + rm_config_offset);

	crt_env->rm_config_offset = rm_config_offset;
	crt_env->rm_config_size	  = remaining_size;

	rm_env_data->signature		 = RM_ENV_DATA_SIGNATURE;
	rm_env_data->version		 = 1;
	rm_env_data->data_payload_offset = sizeof(*rm_env_data);
	rm_env_data->data_payload_size	 = 0U;

	remaining_size -= sizeof(*rm_env_data);

	useful_buff_t qcbor_data_buff;
	qcbor_data_buff.ptr =
		(((uint8_t *)rm_env_data) + rm_env_data->data_payload_offset);
	qcbor_data_buff.len = remaining_size;

	QCBOREncode_Init(qcbor_enc_ctxt, qcbor_data_buff);

	return (rootvm_init_env_info){
		.crt_env	= crt_env,
		.hyp_env	= hyp_env,
		.qcbor_enc_ctxt = qcbor_enc_ctxt,
		.rm_env_data	= rm_env_data,
	};
}

void NOINLINE
rootvm_init(void)
{
	static_assert(SCHEDULER_NUM_PRIORITIES >= (priority_t)3U,
		      "unexpected scheduler configuration");
	static_assert(ROOTVM_PRIORITY <= VCPU_MAX_PRIORITY,
		      "unexpected scheduler configuration");

	thread_create_t params = {
		.scheduler_affinity	  = cpulocal_get_index(),
		.scheduler_affinity_valid = true,
		.scheduler_priority	  = ROOTVM_PRIORITY,
		.scheduler_priority_valid = true,
	};

	partition_t *root_partition = partition_get_root();

	assert(root_partition != NULL);

	platform_add_root_heap(root_partition);

	// Create cspace for root partition
	cspace_create_t cs_params = { NULL };

	cspace_ptr_result_t cspace_ret =
		partition_allocate_cspace(root_partition, cs_params);
	if (cspace_ret.e != OK) {
		goto cspace_fail;
	}
	cspace_t *root_cspace = cspace_ret.r;

	spinlock_acquire_nopreempt(&root_cspace->header.lock);
	if (cspace_configure(root_cspace, MAX_CAPS) != OK) {
		spinlock_release_nopreempt(&root_cspace->header.lock);
		goto cspace_fail;
	}
	spinlock_release_nopreempt(&root_cspace->header.lock);

	if (object_activate_cspace(root_cspace) != OK) {
		goto cspace_fail;
	}

	trigger_object_get_defaults_thread_event(&params);

	// Allocate and setup the root thread
	thread_ptr_result_t thd_ret =
		partition_allocate_thread(root_partition, params);
	if (thd_ret.e != OK) {
		panic("Error allocating root thread");
	}
	thread_t *root_thread = (thread_t *)thd_ret.r;

	vcpu_option_flags_t vcpu_options = vcpu_option_flags_default();

	vcpu_option_flags_set_critical(&vcpu_options, true);

	if (vcpu_configure(root_thread, vcpu_options) != OK) {
		panic("Error configuring vcpu");
	}

	// Attach root cspace to root thread
	if (cspace_attach_thread(root_cspace, root_thread) != OK) {
		panic("Error attaching cspace to root thread");
	}

	// Give the root cspace a cap to itself
	object_ptr_t obj_ptr;

	obj_ptr.cspace		  = root_cspace;
	cap_id_result_t capid_ret = cspace_create_master_cap(
		root_cspace, obj_ptr, OBJECT_TYPE_CSPACE);
	if (capid_ret.e != OK) {
		goto cspace_fail;
	}

	uint32_t env_data_size = QCBOR_ENV_CONFIG_SIZE;

	rootvm_init_env_info info =
		rootvm_init_env_data(root_partition, env_data_size);

	hyp_env_data_t	  hyp_env	 = info.hyp_env;
	qcbor_enc_ctxt_t *qcbor_enc_ctxt = info.qcbor_enc_ctxt;

	rm_env_data_hdr_t *rm_env_data = info.rm_env_data;
	rt_env_data_t	  *crt_env     = info.crt_env;

	QCBOREncode_OpenMap(qcbor_enc_ctxt);

	QCBOREncode_AddUInt64ToMap(qcbor_enc_ctxt, "cspace_capid", capid_ret.r);

	// Take extra reference so that the deletion of the master cap does not
	// accidentally destroy the partition.
	root_partition = object_get_partition_additional(root_partition);

	// Create caps for the root partition and thread
	obj_ptr.partition = root_partition;
	capid_ret	  = cspace_create_master_cap(root_cspace, obj_ptr,
						     OBJECT_TYPE_PARTITION);
	if (capid_ret.e != OK) {
		panic("Error creating root partition cap");
	}
	QCBOREncode_AddUInt64ToMap(qcbor_enc_ctxt, "partition_capid",
				   capid_ret.r);

	obj_ptr.thread = root_thread;
	capid_ret      = cspace_create_master_cap(root_cspace, obj_ptr,
						  OBJECT_TYPE_THREAD);
	if (capid_ret.e != OK) {
		panic("Error creating root partition cap");
	}
	QCBOREncode_AddUInt64ToMap(qcbor_enc_ctxt, "vcpu_capid", capid_ret.r);
	crt_env->vcpu_capid = capid_ret.r;

	// Do a memdb walk to get all the available memory ranges of the root
	// partition and save in the rm_env_data
	if (boot_add_free_range((uintptr_t)root_partition, MEMDB_TYPE_PARTITION,
				qcbor_enc_ctxt) != OK) {
		panic("Error doing the memory database walk");
	}

	trigger_rootvm_init_event(root_partition, root_thread, root_cspace,
				  &hyp_env, qcbor_enc_ctxt);

	QCBOREncode_CloseMap(qcbor_enc_ctxt);

	rootvm_close_env_data(qcbor_enc_ctxt, rm_env_data);

	crt_env->runtime_ipa   = hyp_env.runtime_ipa;
	crt_env->app_ipa       = hyp_env.app_ipa;
	crt_env->app_heap_ipa  = hyp_env.app_heap_ipa;
	crt_env->app_heap_size = hyp_env.app_heap_size;
	crt_env->timer_freq    = hyp_env.timer_freq;
	crt_env->gicd_base     = hyp_env.gicd_base;
	crt_env->gicr_base     = hyp_env.gicr_base;

	// Copy the rm_env_data to the root VM memory
	copy_rm_env_data_to_rootvm_mem(hyp_env, rm_env_data, crt_env,
				       env_data_size);

	// Setup the root VM thread
	if (object_activate_thread(root_thread) != OK) {
		panic("Error activating root thread");
	}

	trigger_rootvm_init_late_event(root_partition, root_thread, root_cspace,
				       &hyp_env);

	scheduler_lock_nopreempt(root_thread);
	// FIXME: eventually pass as dtb, for now the rm_env_data ipa is passed
	// directly.
	bool_result_t power_ret =
		vcpu_poweron(root_thread, vmaddr_result_ok(hyp_env.entry_ipa),
			     register_result_ok(hyp_env.env_ipa));
	if (power_ret.e != OK) {
		panic("Error vcpu poweron");
	}

	// Allow other modules to clean up after root VM creation.
	trigger_rootvm_started_event(root_thread);
	scheduler_unlock_nopreempt(root_thread);
	(void)partition_free(root_partition, crt_env, env_data_size);
	rm_env_data = NULL;

	return;

cspace_fail:
	panic("Error creating root cspace cap");
}

```

`hyp/vm/rootvm_package/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

events rootvm_package.ev
types rootvm_package.tc

source package.c

```

`hyp/vm/rootvm_package/rootvm_package.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

module rootvm_package

subscribe rootvm_init(root_partition, root_thread, root_cspace, hyp_env, qcbor_enc_ctxt)
	require_preempt_disabled

```

`hyp/vm/rootvm_package/rootvm_package.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause


define rootvm_package_image_type enumeration {
	UNKNOWN		= 0;
	RUNTIME		= 1;
	APPLICATION	= 2;
};

define rootvm_package_entry structure {
	type	uint32;
	offset	uint32;
};

define ROOTVM_PACKAGE_IDENT constant uint32 = 0x47504b47;
define ROOTVM_PACKAGE_ITEMS_MAX constant = 3;

define rootvm_package_header structure {
	ident	uint32;
	items	uint32;

	list	array(ROOTVM_PACKAGE_ITEMS_MAX) structure rootvm_package_entry;
};
extend hyp_env_data structure {
	me_ipa_base	type vmaddr_t;
	ipa_offset	uintptr;
	app_ipa		type vmaddr_t;
	runtime_ipa	type vmaddr_t;
	app_heap_ipa	type vmaddr_t;
	app_heap_size	size;
};

```

`hyp/vm/rootvm_package/src/package.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>

#include <hyp_aspace.h>

#if ARCH_IS_64BIT
#define USE_ELF64
#endif
#include <string.h>

#include <compiler.h>
#include <cpulocal.h>
#include <cspace.h>
#include <elf.h>
#include <elf_loader.h>
#include <log.h>
#include <memextent.h>
#include <object.h>
#include <panic.h>
#include <partition.h>
#include <partition_alloc.h>
#include <pgtable.h>
#include <prng.h>
#include <spinlock.h>
#include <trace.h>
#include <util.h>
#include <vcpu.h>

#include "event_handlers.h"

// The physical address symbol pointing to the package image
extern const char image_pkg_start;

static memextent_t *
create_memextent(partition_t *root_partition, cspace_t *root_cspace,
		 paddr_t phys_base, size_t size, cap_id_t *new_cap_id,
		 pgtable_access_t access)
{
	error_t		    ret;
	memextent_memtype_t memtype = MEMEXTENT_MEMTYPE_ANY;

	memextent_create_t     params_me = { .memextent		   = NULL,
					     .memextent_device_mem = false };
	memextent_ptr_result_t me_ret;
	me_ret = partition_allocate_memextent(root_partition, params_me);
	if (me_ret.e != OK) {
		panic("Failed creation of new mem extent");
	}
	memextent_t *me = me_ret.r;

	spinlock_acquire(&me->header.lock);
	memextent_attrs_t attrs = memextent_attrs_default();
	memextent_attrs_set_access(&attrs, access);
	memextent_attrs_set_memtype(&attrs, memtype);
	ret = memextent_configure(me, phys_base, size, attrs);
	if (ret != OK) {
		panic("Failed configuration of new mem extent");
	}
	spinlock_release(&me->header.lock);

	// Create a master cap for the memextent
	object_ptr_t obj_ptr;
	obj_ptr.memextent	  = me;
	cap_id_result_t capid_ret = cspace_create_master_cap(
		root_cspace, obj_ptr, OBJECT_TYPE_MEMEXTENT);
	if (capid_ret.e != OK) {
		panic("Error create memextent cap id.");
	}

	ret = object_activate_memextent(me);
	if (ret != OK) {
		panic("Failed activation of new mem extent");
	}

	*new_cap_id = capid_ret.r;

	return me;
}

static paddr_t
rootvm_package_load_elf(void *elf, size_t elf_max_size, addrspace_t *addrspace,
			vmaddr_t ipa_base, paddr_t phys_base,
			memextent_t *me_rm)
{
	error_t err;
	count_t i;
	paddr_t limit = 0;

	assert(phys_base >= PLATFORM_ROOTVM_LMA_BASE);
	size_t offset = phys_base - PLATFORM_ROOTVM_LMA_BASE;

	paddr_t range_start = PLATFORM_ROOTVM_LMA_BASE;
	paddr_t range_end = PLATFORM_ROOTVM_LMA_BASE + PLATFORM_ROOTVM_LMA_SIZE;

	for (i = 0; i < elf_get_num_phdrs(elf); i++) {
		Elf_Phdr *phdr = elf_get_phdr(elf, i);
		assert(phdr != NULL);

		if (phdr->p_type != PT_LOAD) {
			continue;
		}
		uintptr_t seg_file_base = (uintptr_t)elf + phdr->p_offset;
		uintptr_t seg_file_end	= seg_file_base + phdr->p_filesz;

		// check all segments will fit within rootvm_mem area
		if (util_add_overflows(phdr->p_paddr, phdr->p_memsz)) {
			panic("ELF program header address + size overflow");
		}
		paddr_t seg_end = phdr->p_paddr + phdr->p_memsz;
		limit		= util_max(limit, seg_end);

		// sanity check input elf file does not overlap
		if (((uintptr_t)elf < range_end) &&
		    (seg_file_end > range_start)) {
			panic("ELF overlaps rootvm_mem area");
		}

		// Map segment in root VM address space using p_flags
		pgtable_access_t access = PGTABLE_ACCESS_R;
		assert((phdr->p_flags & PF_R) != 0U);
		if ((phdr->p_flags & PF_W) != 0U) {
			access = pgtable_access_combine(access,
							PGTABLE_ACCESS_W);
		}
		if ((phdr->p_flags & PF_X) != 0U) {
			access = pgtable_access_combine(access,
							PGTABLE_ACCESS_X);
		}

		// Derive extents from RM memory extent
		// FIXME: this may fail if ELF segments not page aligned
		size_t size =
			util_balign_up(phdr->p_memsz, PGTABLE_VM_PAGE_SIZE);

		memextent_ptr_result_t me_ret = memextent_derive(
			me_rm, offset, size, MEMEXTENT_MEMTYPE_ANY, access,
			MEMEXTENT_TYPE_BASIC);
		if (me_ret.e != OK) {
			panic("Failed creation of derived mem extent");
		}

		memextent_mapping_attrs_t map_attrs =
			memextent_mapping_attrs_default();
		memextent_mapping_attrs_set_user_access(&map_attrs, access);
		memextent_mapping_attrs_set_kernel_access(&map_attrs, access);
		memextent_mapping_attrs_set_memtype(
			&map_attrs, PGTABLE_VM_MEMTYPE_NORMAL_WB);

		// Map the ELF segment
		if (memextent_map(me_ret.r, addrspace, ipa_base + offset,
				  map_attrs) != OK) {
			panic("Error mapping to root VM address space");
		}

		offset += size;
	}

	limit = limit + phys_base;

	if (limit > range_end) {
		panic("ELF segment out of range");
	}

	err = elf_load_phys(elf, elf_max_size, phys_base);
	if (err != OK) {
		panic("Error loading ELF");
	}

	return util_balign_up(limit, (paddr_t)PGTABLE_HYP_PAGE_SIZE);
}

static void
update_cores_info(qcbor_enc_ctxt_t *qcbor_enc_ctxt) REQUIRE_PREEMPT_DISABLED
{
	cpu_index_t boot_core;
	uint64_t    usable_cores;

	assert(qcbor_enc_ctxt != NULL);

	boot_core = cpulocal_get_index();
	assert(PLATFORM_MAX_CORES > boot_core);

	QCBOREncode_AddUInt64ToMap(qcbor_enc_ctxt, "boot_core", boot_core);

	usable_cores = PLATFORM_USABLE_CORES;
	assert((usable_cores & util_bit(boot_core)) != 0);
	QCBOREncode_AddUInt64ToMap(qcbor_enc_ctxt, "usable_cores",
				   usable_cores);

	index_t max_idx = (index_t)((sizeof(usable_cores) * 8U) - 1U) -
			  compiler_clz(usable_cores);
	// can be a static assertion
	assert(max_idx < PLATFORM_MAX_CORES);
}

void
rootvm_package_handle_rootvm_init(partition_t *root_partition,
				  thread_t *root_thread, cspace_t *root_cspace,
				  hyp_env_data_t   *hyp_env,
				  qcbor_enc_ctxt_t *qcbor_enc_ctxt)
{
	error_t ret;

	assert(qcbor_enc_ctxt != NULL);

	assert(root_partition != NULL);
	assert(root_thread != NULL);
	assert(root_cspace != NULL);
	assert(root_thread->addrspace != NULL);

	addrspace_t *addrspace = root_thread->addrspace;

	// FIXME: we could read headers and map incrementally as needed using
	// segment rights. A single 512KiB mapping is sufficient for now!
	size_t		    map_size	= 0x00080000;
	virt_range_result_t map_range_r = hyp_aspace_allocate(map_size);
	assert(map_range_r.e == OK);

	pgtable_hyp_start();
	ret = pgtable_hyp_map(root_partition, map_range_r.r.base, map_size,
			      (paddr_t)&image_pkg_start,
			      PGTABLE_HYP_MEMTYPE_WRITEBACK, PGTABLE_ACCESS_R,
			      VMSA_SHAREABILITY_INNER_SHAREABLE);
	assert(ret == OK);
	pgtable_hyp_commit();

	rootvm_package_header_t *pkg_hdr =
		(rootvm_package_header_t *)map_range_r.r.base;

	if (pkg_hdr->ident != ROOTVM_PACKAGE_IDENT) {
		panic("RootVM package header not found!");
	}
	if (pkg_hdr->items >= (uint32_t)ROOTVM_PACKAGE_ITEMS_MAX) {
		panic("Invalid pkg_hdr");
	}

	paddr_t load_base = PLATFORM_ROOTVM_LMA_BASE;
	paddr_t load_next = load_base;

	// Create memory extent for the RM with randomized base
	uint64_t random;
#if !defined(DISABLE_ROOTVM_ASLR)
	uint64_result_t res = prng_get64();
	assert(res.e == OK);
	random = res.r;
#else
	random = 0x10000000U;
#endif

#if 0
	// FIXME:
	// Root VM address space could be smaller
	// Currently limit usable address space to 1GiB
	vmaddr_t addr_limit = (vmaddr_t)util_bit(30);

	vmaddr_t ipa = (vmaddr_t)rand % (addr_limit - PLATFORM_ROOTVM_LMA_SIZE -
					 PGTABLE_VM_PAGE_SIZE);
	ipa += PGTABLE_VM_PAGE_SIZE; // avoid use of the zero page
	ipa = util_balign_down(ipa, PGTABLE_VM_PAGE_SIZE);
#else
	(void)random;
	vmaddr_t ipa = PLATFORM_ROOTVM_LMA_BASE;
#endif

	// Map the root_thread memory as RW by default. Elf segments will be
	// remapped with the required rights.
	cap_id_t     me_cap;
	memextent_t *me		 = create_memextent(root_partition, root_cspace,
						    PLATFORM_ROOTVM_LMA_BASE,
						    PLATFORM_ROOTVM_LMA_SIZE, &me_cap,
						    PGTABLE_ACCESS_RWX);
	vmaddr_t     runtime_ipa = 0U;
	vmaddr_t     app_ipa	 = 0U;
	size_t	     offset	 = 0U;

	index_t i;
	for (i = 0U; i < (index_t)pkg_hdr->items; i++) {
		rootvm_package_image_type_t t =
			(rootvm_package_image_type_t)pkg_hdr->list[i].type;

		switch (t) {
		case ROOTVM_PACKAGE_IMAGE_TYPE_RUNTIME:
		case ROOTVM_PACKAGE_IMAGE_TYPE_APPLICATION:
			LOG(DEBUG, INFO,
			    "Processing package image ({:d}) type={:d}", i, t);

			void *elf = (void *)(map_range_r.r.base +
					     pkg_hdr->list[i].offset);

			if (pkg_hdr->list[i].offset > map_size) {
				panic("ELF out of valid region");
			}

			size_t elf_max_size =
				map_size - pkg_hdr->list[i].offset;

			if (!elf_valid(elf, elf_max_size)) {
				panic("Invalid package ELF");
			}

			if (t == ROOTVM_PACKAGE_IMAGE_TYPE_RUNTIME) {
				runtime_ipa = ipa + offset;
				if (hyp_env->entry_ipa != 0U) {
					panic("Multiple RootVM runtime images");
				}
				hyp_env->entry_ipa =
					elf_get_entry(elf) + runtime_ipa;
			} else {
				app_ipa = ipa + offset;
			}

			load_next = rootvm_package_load_elf(elf, elf_max_size,
							    addrspace, ipa,
							    load_next, me);
			break;
		case ROOTVM_PACKAGE_IMAGE_TYPE_UNKNOWN:
		default:
			panic("Bad image type");
		}

		offset = load_next - PLATFORM_ROOTVM_LMA_BASE;
	}

	memextent_mapping_attrs_t map_attrs = memextent_mapping_attrs_default();
	memextent_mapping_attrs_set_user_access(&map_attrs, PGTABLE_ACCESS_RW);
	memextent_mapping_attrs_set_kernel_access(&map_attrs,
						  PGTABLE_ACCESS_RW);
	memextent_mapping_attrs_set_memtype(&map_attrs,
					    PGTABLE_VM_MEMTYPE_NORMAL_WB);

	// Map all the remaining root VM memory as RW
	if (memextent_map(me, addrspace, ipa, map_attrs) != OK) {
		panic("Error mapping to root VM address space");
	}

	assert(util_is_baligned(offset, PGTABLE_VM_PAGE_SIZE));

	vmaddr_t env_data_ipa = ipa + offset;
	offset += util_balign_up(hyp_env->env_data_size, PGTABLE_VM_PAGE_SIZE);

	vmaddr_t app_heap_ipa  = ipa + offset;
	size_t	 app_heap_size = PLATFORM_ROOTVM_LMA_SIZE - offset;

	// The C runtime expects the heap to be page aligned.
	assert(util_is_baligned(app_heap_ipa, PGTABLE_VM_PAGE_SIZE));
	assert(util_is_baligned(app_heap_size, PGTABLE_VM_PAGE_SIZE));

	// Add info of the memory left in RM to hyp_env_data, so that it can be
	// later used for the boot info structure for example.
	hyp_env->me_ipa_base = ipa;
	hyp_env->env_ipa     = env_data_ipa;

	hyp_env->app_ipa       = app_ipa;
	hyp_env->runtime_ipa   = runtime_ipa;
	hyp_env->ipa_offset    = ipa - PLATFORM_ROOTVM_LMA_BASE;
	hyp_env->app_heap_ipa  = app_heap_ipa;
	hyp_env->app_heap_size = app_heap_size;

	QCBOREncode_AddUInt64ToMap(qcbor_enc_ctxt, "me_ipa_base",
				   hyp_env->me_ipa_base);
	QCBOREncode_AddUInt64ToMap(qcbor_enc_ctxt, "ipa_offset",
				   hyp_env->ipa_offset);

	QCBOREncode_AddUInt64ToMap(qcbor_enc_ctxt, "me_capid", me_cap);
	QCBOREncode_AddUInt64ToMap(qcbor_enc_ctxt, "me_size",
				   PLATFORM_ROOTVM_LMA_SIZE);

	update_cores_info(qcbor_enc_ctxt);

	LOG(DEBUG, INFO, "runtime_ipa: {:#x}", runtime_ipa);
	LOG(DEBUG, INFO, "app_ipa: {:#x}", app_ipa);
	LOG(DEBUG, INFO, "env_data_ipa: {:#x}", env_data_ipa);
	LOG(DEBUG, INFO, "app_heap_ipa: {:#x}", app_heap_ipa);

	hyp_aspace_deallocate(root_partition, map_range_r.r);

	// New code has been loaded, so we need to invalidate any physical
	// I-cache entries possibly prefetched
	__asm__ volatile("dsb ish; ic ialluis" ::: "memory");
}

```

`hyp/vm/slat/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

interface slat

```

`hyp/vm/smccc/aarch64/smccc_64.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

module smccc

subscribe vcpu_trap_smc64(iss)
	exclude_preempt_disabled

subscribe vcpu_trap_hvc64(iss)
	exclude_preempt_disabled

subscribe vcpu_trap_smc64
	handler smccc_handle_vcpu_trap_default()
	priority last
	exclude_preempt_disabled

subscribe vcpu_trap_hvc64
	handler smccc_handle_vcpu_trap_default()
	priority last
	exclude_preempt_disabled

```

`hyp/vm/smccc/aarch64/src/smccc_64.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <hyptypes.h>

#include <thread.h>

#include <events/smccc.h>

#include "event_handlers.h"
#include "smccc_hypercall.h"

static bool
smccc_handle_call(bool is_hvc) EXCLUDE_PREEMPT_DISABLED
{
	bool		    handled;
	thread_t	   *current = thread_get_self();
	smccc_function_id_t function_id =
		smccc_function_id_cast((uint32_t)current->vcpu_regs_gpr.x[0]);

	uint32_t res0 = smccc_function_id_get_res0(&function_id);
	if (res0 != 0U) {
		current->vcpu_regs_gpr.x[0] =
			(register_t)SMCCC_UNKNOWN_FUNCTION64;
		handled = true;
		goto out;
	}

	// TODO: the smccc handling below needs to be refactored, to permit
	// registering ranges of service IDs, rather than registering
	// individual calls directly. The current approach allows for unknown
	// call IDs to be unhandled and fallthrough to a later module, which is
	// undesirable.
	//
	// For SMCCC based hypercalls, we need function ID range-base handling,
	// so its currently called directly here.
	if (smccc_handle_hypercall_wrapper(function_id, is_hvc)) {
		handled = true;
		goto out;
	}

	if (smccc_function_id_get_is_smc64(&function_id)) {
		uint64_t ret0 = (uint64_t)current->vcpu_regs_gpr.x[0];
		uint64_t ret1 = (uint64_t)current->vcpu_regs_gpr.x[1];
		uint64_t ret2 = (uint64_t)current->vcpu_regs_gpr.x[2];
		uint64_t ret3 = (uint64_t)current->vcpu_regs_gpr.x[3];

		if (smccc_function_id_get_is_fast(&function_id)) {
			handled = trigger_smccc_dispatch_fast_64_event(
				smccc_function_id_get_owner_id(&function_id),
				smccc_function_id_get_function(&function_id),
				is_hvc, (uint64_t)current->vcpu_regs_gpr.x[1],
				(uint64_t)current->vcpu_regs_gpr.x[2],
				(uint64_t)current->vcpu_regs_gpr.x[3],
				(uint64_t)current->vcpu_regs_gpr.x[4],
				(uint64_t)current->vcpu_regs_gpr.x[5],
				(uint64_t)current->vcpu_regs_gpr.x[6],
				smccc_client_id_cast(
					(uint32_t)current->vcpu_regs_gpr.x[7]),
				&ret0, &ret1, &ret2, &ret3);
		} else {
			handled = trigger_smccc_dispatch_yielding_64_event(
				smccc_function_id_get_owner_id(&function_id),
				smccc_function_id_get_function(&function_id),
				is_hvc, (uint64_t)current->vcpu_regs_gpr.x[1],
				(uint64_t)current->vcpu_regs_gpr.x[2],
				(uint64_t)current->vcpu_regs_gpr.x[3],
				(uint64_t)current->vcpu_regs_gpr.x[4],
				(uint64_t)current->vcpu_regs_gpr.x[5],
				(uint64_t)current->vcpu_regs_gpr.x[6],
				smccc_client_id_cast(
					(uint32_t)current->vcpu_regs_gpr.x[7]),
				&ret0, &ret1, &ret2, &ret3);
		}

		if (handled) {
			current->vcpu_regs_gpr.x[0] = (register_t)ret0;
			current->vcpu_regs_gpr.x[1] = (register_t)ret1;
			current->vcpu_regs_gpr.x[2] = (register_t)ret2;
			current->vcpu_regs_gpr.x[3] = (register_t)ret3;
		}
	} else {
		uint32_t ret0 = (uint32_t)current->vcpu_regs_gpr.x[0];
		uint32_t ret1 = (uint32_t)current->vcpu_regs_gpr.x[1];
		uint32_t ret2 = (uint32_t)current->vcpu_regs_gpr.x[2];
		uint32_t ret3 = (uint32_t)current->vcpu_regs_gpr.x[3];

		if (smccc_function_id_get_is_fast(&function_id)) {
			handled = trigger_smccc_dispatch_fast_32_event(
				smccc_function_id_get_owner_id(&function_id),
				smccc_function_id_get_function(&function_id),
				is_hvc, (uint32_t)current->vcpu_regs_gpr.x[1],
				(uint32_t)current->vcpu_regs_gpr.x[2],
				(uint32_t)current->vcpu_regs_gpr.x[3],
				(uint32_t)current->vcpu_regs_gpr.x[4],
				(uint32_t)current->vcpu_regs_gpr.x[5],
				(uint32_t)current->vcpu_regs_gpr.x[6],
				smccc_client_id_cast(
					(uint32_t)current->vcpu_regs_gpr.x[7]),
				&ret0, &ret1, &ret2, &ret3);
		} else {
			handled = trigger_smccc_dispatch_yielding_32_event(
				smccc_function_id_get_owner_id(&function_id),
				smccc_function_id_get_function(&function_id),
				is_hvc, (uint32_t)current->vcpu_regs_gpr.x[1],
				(uint32_t)current->vcpu_regs_gpr.x[2],
				(uint32_t)current->vcpu_regs_gpr.x[3],
				(uint32_t)current->vcpu_regs_gpr.x[4],
				(uint32_t)current->vcpu_regs_gpr.x[5],
				(uint32_t)current->vcpu_regs_gpr.x[6],
				smccc_client_id_cast(
					(uint32_t)current->vcpu_regs_gpr.x[7]),
				&ret0, &ret1, &ret2, &ret3);
		}

		if (handled) {
			current->vcpu_regs_gpr.x[0] = (register_t)ret0;
			current->vcpu_regs_gpr.x[1] = (register_t)ret1;
			current->vcpu_regs_gpr.x[2] = (register_t)ret2;
			current->vcpu_regs_gpr.x[3] = (register_t)ret3;
		}
	}

out:
	return handled;
}

bool
smccc_handle_vcpu_trap_smc64(ESR_EL2_ISS_SMC64_t iss)
{
	bool handled = false;

	if (ESR_EL2_ISS_SMC64_get_imm16(&iss) == (uint16_t)0U) {
		handled = smccc_handle_call(false);
	}

	return handled;
}

bool
smccc_handle_vcpu_trap_hvc64(ESR_EL2_ISS_HVC_t iss)
{
	bool handled = false;

	if (ESR_EL2_ISS_HVC_get_imm16(&iss) == (uint16_t)0U) {
		handled = smccc_handle_call(true);
	}

	return handled;
}

bool
smccc_handle_vcpu_trap_default(void)
{
	// We always fallback to returning -1, otherwise we'll deliver an
	// exception to the VCPU.
	thread_t *current	    = thread_get_self();
	current->vcpu_regs_gpr.x[0] = (register_t)SMCCC_UNKNOWN_FUNCTION64;

	return true;
}

```

`hyp/vm/smccc/aarch64/templates/hyp_wrapper.c.tmpl`:

```tmpl

#def prefix: hypercall_
#set $wrapper_suffix = "__hyp_wrapper"

\#include <assert.h>
\#include <hyptypes.h>

\#include <compiler.h>
\#include <hypcall_def.h>
\#include <thread.h>
\#include <trace.h>

#def return_type(hypcall)
#if len(hypcall.outputs) > 1
${prefix}${hypcall.name}_result_t#slurp
#else
${hypcall.outputs[0][1].ctype}#slurp
#end if
#end def

#def register_expr(variable)
#if variable.category == 'bitfield'
## FIXME: this is an implementation detail of the type system
${variable.name}.bf[0]#slurp
#else
${variable.name}#slurp
#end if
#end def

#def input_cast(variable, val)
#if variable.category == 'bitfield'
## FIXME: this is an implementation detail of the type system
${variable.type_name}_cast((uint${8 * variable.size}_t)${val})#slurp
#elif variable.category == 'union'
(${variable.ctype}){ .raw = ${val} }#slurp
#else
(${variable.ctype})${val}#slurp
#end if
#end def

#for hypcall_num in sorted($hypcall_dict.keys())
#set $hypcall = $hypcall_dict[$hypcall_num]

#set $num_in = len($hypcall.inputs)
#set $num_out = len($hypcall.outputs)

#if $num_in > 8 or $num_out > 8
#error too many hypcall arguments: ${hypcall.name}: input $num_in, output $num_out
#end if

static void
${hypcall.name}${wrapper_suffix}(register_t *args) {
    #if $hypcall.outputs
        $return_type($hypcall) ret_;
    #end if

    ## call the implementation
    #if $hypcall.outputs
    ret_ =
    #end if
    $prefix${hypcall.name}(#slurp
    #set xar=1
    #set sep=''
    #for i, input in $hypcall.inputs
        #if not $input.ignore
#set $val = "args[{:d}]".format($xar)
	    ${sep}$input_cast(input, $val)
            #set sep=', '
        #end if
        #set xar=xar+1
    #end for
    );

    ## return the result, if any
    #if len(hypcall.outputs) > 1
        #set xar=0
        #for i, output in hypcall.outputs
            ##
            ## assuming complex struct, we only return the first
            ## 'register_t'.
            ##
            args[$xar] = (register_t)(ret_.$register_expr($output));
            #set xar=xar+1
        #end for
    #else if hypcall.outputs
        args[0] = (register_t)ret_;
    #end if
}

#end for

void
smccc_hypercall_table_wrapper(count_t hyp_num, register_t *args);

void
smccc_hypercall_table_wrapper(count_t hyp_num, register_t *args) {
	TRACE(USER, HYPERCALL, "smccc hyp: {:#x}: {:#x} {:#x} {:#x}, {:#x}",
	      (register_t)(hyp_num), args[1], args[2], args[3], args[4]);
	bool trace_ret = true;

	switch (hyp_num) {
#for hypcall_num in sorted($hypcall_dict.keys())
#set $hypcall = $hypcall_dict[$hypcall_num]
#set $sensitive = $hypcall.properties.get('sensitive', False)
		case $hypcall_num:
#if $sensitive
			// Sensitive hypercall
			trace_ret = false;
#end if
			${hypcall.name}${wrapper_suffix}(args);
			break;
#end for
		default:
			args[0] = (register_t)SMCCC_UNKNOWN_FUNCTION64;
			break;
	}

	if (compiler_expected(trace_ret)) {
		TRACE(USER, HYPERCALL,
		      "smccc ret: {:#x} {:#x} {:#x} {:#x} {:#x}",
		      args[0], args[1], args[2], args[3], args[4]);
	}
}

```

`hyp/vm/smccc/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

interface smccc
events smccc.ev
local_include
source smccc.c smccc_hypercalls.c
arch_events aarch64 smccc_64.ev
arch_source aarch64 smccc_64.c
arch_template hypercalls aarch64 hyp_wrapper.c

```

`hyp/vm/smccc/include/smccc_hypercall.h`:

```h
// © 2022 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

void
smccc_hypercall_table_wrapper(count_t hyp_num, register_t args[7]);

bool
smccc_handle_hypercall_wrapper(smccc_function_id_t smc_id, bool is_hvc);

```

`hyp/vm/smccc/smccc.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

module smccc

#define _SMCCC_DISPATCH_OWNER(type_size, owner, owner_id)			\
subscribe smccc_dispatch_ ## type_size[SMCCC_OWNER_ID_ ## owner_id];	\
	handler trigger_smccc_call_ ## type_size ## _ ## owner ## _event(	\
		function, is_hvc, arg1, arg2, arg3, arg4, arg5, arg6,		\
		client_id, ret0, ret1, ret2, ret3);				\
	exclude_preempt_disabled.

#define SMCCC_DISPATCH_OWNER(owner, owner_id) \
	_SMCCC_DISPATCH_OWNER(fast_32, owner, owner_id) \
	_SMCCC_DISPATCH_OWNER(fast_64, owner, owner_id) \
	_SMCCC_DISPATCH_OWNER(yielding_32, owner, owner_id) \
	_SMCCC_DISPATCH_OWNER(yielding_64, owner, owner_id)

SMCCC_DISPATCH_OWNER(arch, ARCH)
SMCCC_DISPATCH_OWNER(cpu, CPU)
SMCCC_DISPATCH_OWNER(sip, SIP)
SMCCC_DISPATCH_OWNER(oem, OEM)
SMCCC_DISPATCH_OWNER(standard, STANDARD)
SMCCC_DISPATCH_OWNER(standard_hyp, STANDARD_HYP)
SMCCC_DISPATCH_OWNER(vendor_hyp, VENDOR_HYP)

#include <smccc.ev.h>

SMCCC_ARCH_FUNCTION_32(VERSION, 0, version, ret0)
SMCCC_ARCH_FUNCTION_32(ARCH_FEATURES, 0, arch_features, arg1, ret0)
SMCCC_STANDARD_HYP_FUNCTION_32(CALL_UID, 0, std_hyp_call_uid, ret0, ret1, ret2, ret3)
SMCCC_STANDARD_HYP_FUNCTION_32(REVISION, 0, std_hyp_revision, ret0, ret1)

```

`hyp/vm/smccc/src/smccc.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <hyptypes.h>

#include <events/smccc.h>

#include "event_handlers.h"

bool
smccc_version(uint32_t *ret0)
{
	*ret0 = SMCCC_VERSION;
	return true;
}

bool
smccc_arch_features(uint32_t arg1, uint32_t *ret0)
{
	smccc_function_id_t fn_id    = smccc_function_id_cast(arg1);
	bool		    is_smc64 = smccc_function_id_get_is_smc64(&fn_id);
	smccc_function_t    fn	     = smccc_function_id_get_function(&fn_id);
	uint32_t	    ret;

	if ((smccc_function_id_get_owner_id(&fn_id) == SMCCC_OWNER_ID_ARCH) &&
	    smccc_function_id_get_is_fast(&fn_id) &&
	    (smccc_function_id_get_res0(&fn_id) == 0U)) {
		if (is_smc64) {
			ret = trigger_smccc_arch_features_fast64_event(
				(smccc_arch_function_t)fn);
		} else {
			ret = trigger_smccc_arch_features_fast32_event(
				(smccc_arch_function_t)fn);
		}
	} else if ((smccc_function_id_get_owner_id(&fn_id) ==
		    SMCCC_OWNER_ID_STANDARD_HYP) &&
		   smccc_function_id_get_is_fast(&fn_id) &&
		   (smccc_function_id_get_res0(&fn_id) == 0U)) {
		if (is_smc64) {
			ret = trigger_smccc_standard_hyp_features_fast64_event(
				(smccc_standard_hyp_function_t)fn);
		} else {
			ret = trigger_smccc_standard_hyp_features_fast32_event(
				(smccc_standard_hyp_function_t)fn);
		}
	} else {
		ret = SMCCC_UNKNOWN_FUNCTION32;
	}

	*ret0 = ret;
	return true;
}

bool
smccc_std_hyp_call_uid(uint32_t *ret0, uint32_t *ret1, uint32_t *ret2,
		       uint32_t *ret3)
{
	*ret0 = (uint32_t)SMCCC_GUNYAH_UID0;
	*ret1 = (uint32_t)SMCCC_GUNYAH_UID1;
	*ret2 = (uint32_t)SMCCC_GUNYAH_UID2;
	*ret3 = (uint32_t)SMCCC_GUNYAH_UID3;

	return true;
}

bool
smccc_std_hyp_revision(uint32_t *ret0, uint32_t *ret1)
{
	// From: ARM DEN 0028E
	// Incompatible argument changes cannot be made to an
	// existing SMC or HVC call. A new call is required.
	//
	// Major revision numbers must be incremented when:
	// - Any SMC or HVC call is removed.
	// Minor revision numbers must be incremented when:
	// - Any SMC or HVC call is added.
	// - Backwards compatible changes are made to existing
	//   function arguments.
	*ret0 = 1U; // Major Revision
	*ret1 = 0U; // Minor Revision

	return true;
}

```

`hyp/vm/smccc/src/smccc_hypercalls.c`:

```c
// © 2022 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <hyptypes.h>

#include <smccc_platform.h>
#include <thread.h>

#include <events/smccc.h>

#include "smccc_hypercall.h"

bool
smccc_handle_hypercall_wrapper(smccc_function_id_t smc_id, bool is_hvc)
{
	bool handled;

	smccc_function_t smc_func  = smccc_function_id_get_function(&smc_id);
	smccc_owner_id_t smc_owner = smccc_function_id_get_owner_id(&smc_id);

	if (smc_owner != SMCCC_OWNER_ID_VENDOR_HYP) {
		handled = false;
		goto out;
	}

	bool is_smc64 = smccc_function_id_get_is_smc64(&smc_id);
	bool is_fast  = smccc_function_id_get_is_fast(&smc_id);

	thread_t   *current = thread_get_self();
	register_t *args    = &current->vcpu_regs_gpr.x[0];

	smccc_vendor_hyp_function_id_t smc_type =
		smccc_vendor_hyp_function_id_cast((uint16_t)(smc_func));

	switch (smccc_vendor_hyp_function_id_get_call_class(&smc_type)) {
	case SMCCC_VENDOR_HYP_FUNCTION_CLASS_PLATFORM_CALL:
		handled = smccc_handle_smc_platform_call(args, is_hvc);
		break;
	case SMCCC_VENDOR_HYP_FUNCTION_CLASS_HYPERCALL:
		if (is_fast && is_smc64) {
			uint32_t hyp_num =
				smccc_vendor_hyp_function_id_get_function(
					&smc_type);
			smccc_hypercall_table_wrapper(hyp_num, args);
		} else {
			args[0] = (register_t)SMCCC_UNKNOWN_FUNCTION64;
		}
		handled = true;
		break;
	case SMCCC_VENDOR_HYP_FUNCTION_CLASS_SERVICE:
		if (is_fast && !is_smc64) {
			uint16_t func =
				smccc_vendor_hyp_function_id_get_function(
					&smc_type);
			switch ((smccc_vendor_hyp_function_t)func) {
			case SMCCC_VENDOR_HYP_FUNCTION_CALL_UID:
				args[0] = SMCCC_GUNYAH_UID0;
				args[1] = SMCCC_GUNYAH_UID1;
				args[2] = SMCCC_GUNYAH_UID2;
				args[3] = SMCCC_GUNYAH_UID3;
				break;
			case SMCCC_VENDOR_HYP_FUNCTION_REVISION:
				args[0] = (register_t)hyp_api_info_raw(
					hyp_api_info_default());
				break;
			case SMCCC_VENDOR_HYP_FUNCTION_CALL_COUNT:
				// Deprecated
			default:
				args[0] = (register_t)SMCCC_UNKNOWN_FUNCTION64;
				break;
			}
		} else {
			args[0] = (register_t)SMCCC_UNKNOWN_FUNCTION64;
		}
		handled = true;
		break;
	default:
		args[0] = (register_t)SMCCC_UNKNOWN_FUNCTION64;
		handled = true;
		break;
	}

out:
	return handled;
}

```

`hyp/vm/vcpu/aarch64/hypercalls.hvc`:

```hvc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

define vcpu_configure hypercall {
	call_num	0x34;
	cap_id		input type cap_id_t;
	vcpu_options	input bitfield vcpu_option_flags;
	res0		input uregister;
	error		output enumeration error;
};

define vcpu_register_write hypercall {
	call_num	0x64;
	vcpu		input type cap_id_t;
	register_set	input enumeration vcpu_register_set;
	register_index	input type index_t;
	value		input uregister;
	res0		input uregister;
	error		output enumeration error;
};

define vcpu_set_affinity hypercall {
	call_num	0x3d;
	cap_id		input type cap_id_t;
	affinity	input type cpu_index_t;
	res1		input uregister;
	error		output enumeration error;
};

define vcpu_poweron hypercall {
	call_num	0x38;
	cap_id		input type cap_id_t;
	entry_point	input uregister;
	context		input uregister;
	flags		input bitfield vcpu_poweron_flags;
	error		output enumeration error;
};

define vcpu_poweroff hypercall {
	call_num	0x39;
	cap_id		input type cap_id_t;
	flags		input bitfield vcpu_poweroff_flags;
	error		output enumeration error;
};

define vcpu_kill hypercall {
	call_num	0x3a;
	cap_id		input type cap_id_t;
	res0		input uregister;
	error		output enumeration error;
};

define vcpu_set_priority hypercall {
	call_num	0x46;
	cap_id		input type cap_id_t;
	priority	input type priority_t;
	error		output enumeration error;
};

define vcpu_set_timeslice hypercall {
	call_num	0x47;
	cap_id		input type cap_id_t;
	timeslice	input type nanoseconds_t;
	error		output enumeration error;
};

define vcpu_bind_virq hypercall {
	call_num	0x5c;
	vcpu		input type cap_id_t;
	vic		input type cap_id_t;
	virq		input type virq_t;
	virq_type	input enumeration vcpu_virq_type;
	res0		input uregister;
	error		output enumeration error;
};

define vcpu_unbind_virq hypercall {
	call_num	0x5d;
	vcpu		input type cap_id_t;
	virq_type	input enumeration vcpu_virq_type;
	res0		input uregister;
	error		output enumeration error;
};

```

`hyp/vm/vcpu/aarch64/include/exception_dispatch.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

void
vcpu_interrupt_dispatch(void) REQUIRE_PREEMPT_DISABLED;

void
vcpu_exception_dispatch(bool is_aarch64) REQUIRE_PREEMPT_DISABLED;

void
vcpu_error_dispatch(void) REQUIRE_PREEMPT_DISABLED;

```

`hyp/vm/vcpu/aarch64/include/exception_inject.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

bool
inject_inst_data_abort(ESR_EL2_t esr_el2, esr_ec_t ec, iss_da_ia_fsc_t fsc,
		       FAR_EL2_t far, vmaddr_t ipa, bool is_data_abort);

void
inject_undef_abort(ESR_EL2_t esr_el2);

```

`hyp/vm/vcpu/aarch64/include/reg_access.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

error_t
vcpu_register_write(thread_t *vcpu, vcpu_register_set_t register_set,
		    index_t register_index, register_t value);

```

`hyp/vm/vcpu/aarch64/include/vectors_vcpu.h`:

```h
// © 2022 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

CPULOCAL_DECLARE_EXTERN(uintptr_t, vcpu_aarch64_vectors);

```

`hyp/vm/vcpu/aarch64/src/aarch64_init.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>
#include <string.h>

#include <hypconstants.h>
#include <hypregisters.h>

#include <compiler.h>
#include <log.h>
#include <preempt.h>
#include <scheduler.h>
#include <thread.h>
#include <trace.h>
#include <vcpu.h>
#include <virq.h>

#include <events/thread.h>
#include <events/vcpu.h>

#include <asm/barrier.h>
#include <asm/sysregs.h>

#include "event_handlers.h"

#if defined(ARCH_ARM_HAVE_SCXT)
#include <atomic.h>
#include <platform_features.h>
#include <prng.h>

static bool		scxt_disabled = false;
static _Atomic uint64_t scxt_count;
#endif

void
vcpu_handle_boot_runtime_init(void)
{
	// Disable floating-point traps
#if defined(ARCH_ARM_FEAT_VHE)
	CPTR_EL2_E2H1_t cptr =
		register_CPTR_EL2_E2H1_read_ordered(&asm_ordering);
	CPTR_EL2_E2H1_set_FPEN(&cptr, 3);
	register_CPTR_EL2_E2H1_write_ordered(cptr, &asm_ordering);
#else
	CPTR_EL2_E2H0_t cptr =
		register_CPTR_EL2_E2H0_read_ordered(&asm_ordering);
	CPTR_EL2_E2H0_set_TFP(&cptr, 0);
	register_CPTR_EL2_E2H0_write_ordered(cptr, &asm_ordering);
#endif
}

void
vcpu_handle_boot_cpu_warm_init(void)
{
#if defined(ARCH_ARM_FEAT_VHE)
	CONTEXTIDR_EL2_t ctxidr = CONTEXTIDR_EL2_default();
	register_CONTEXTIDR_EL2_write(ctxidr);
#endif

#if !SCHEDULER_CAN_MIGRATE
	// Expose the real MIDR to VMs; no need to context-switch it.
	register_VPIDR_EL2_write(register_MIDR_EL1_read());
#endif

	// Although ARM recommends these traps do not trap AArch32 EL0 to EL2,
	// it is implementation defined, so zero this register.
	HSTR_EL2_t hstr = HSTR_EL2_cast(0U);
	register_HSTR_EL2_write(hstr);
#if defined(ARCH_ARM_HAVE_SCXT)
	if (!scxt_disabled) {
		register_SCXTNUM_EL2_write(atomic_fetch_add_explicit(
			&scxt_count, 1U, memory_order_relaxed));
	}
#endif
}

#if defined(ARCH_ARM_HAVE_SCXT)
void
vcpu_handle_boot_cold_init(void)
{
	platform_cpu_features_t features = platform_get_cpu_features();

	scxt_disabled = platform_cpu_features_get_scxt_disable(&features);

	if (!scxt_disabled) {
		uint64_result_t rand_r = prng_get64();
		assert(rand_r.e == OK);
		atomic_store_relaxed(&scxt_count, rand_r.r);
	} else {
		LOG(DEBUG, DEBUG, "platform SCXTNUM_ELx access disabled!");
	}
}
#endif

static void
arch_vcpu_el1_registers_init(thread_t *vcpu)
{
	if (thread_get_self() == vcpu) {
		register_SCTLR_EL1_write(SCTLR_EL1_default());
	} else {
		SCTLR_EL1_init(&vcpu->vcpu_regs_el1.sctlr_el1);
	}
}

static void
arch_vcpu_el2_registers_init(vcpu_el2_registers_t *el2_regs)
{
#if defined(ARCH_ARM_FEAT_VHE)
	CPTR_EL2_E2H1_init(&el2_regs->cptr_el2);
	CPTR_EL2_E2H1_set_FPEN(&el2_regs->cptr_el2, 3);
#else
	CPTR_EL2_E2H0_init(&el2_regs->cptr_el2);
	CPTR_EL2_E2H0_set_TFP(&el2_regs->cptr_el2, 0);
#endif

	HCR_EL2_init(&el2_regs->hcr_el2);
	HCR_EL2_set_VM(&el2_regs->hcr_el2, true);
	HCR_EL2_set_SWIO(&el2_regs->hcr_el2, true);
	HCR_EL2_set_PTW(&el2_regs->hcr_el2, false);
	HCR_EL2_set_FMO(&el2_regs->hcr_el2, true);
	HCR_EL2_set_IMO(&el2_regs->hcr_el2, true);
	HCR_EL2_set_AMO(&el2_regs->hcr_el2, true);
	HCR_EL2_set_VF(&el2_regs->hcr_el2, false);
	HCR_EL2_set_VI(&el2_regs->hcr_el2, false);
	HCR_EL2_set_VSE(&el2_regs->hcr_el2, false);
	HCR_EL2_set_FB(&el2_regs->hcr_el2, false);
	HCR_EL2_set_BSU(&el2_regs->hcr_el2, 0);
	HCR_EL2_set_DC(&el2_regs->hcr_el2, false);
	HCR_EL2_set_TWI(&el2_regs->hcr_el2, true);
	HCR_EL2_set_TWE(&el2_regs->hcr_el2, false);
	HCR_EL2_set_TID0(&el2_regs->hcr_el2, false);
	HCR_EL2_set_TID1(&el2_regs->hcr_el2, false);
	HCR_EL2_set_TID2(&el2_regs->hcr_el2, false);
	HCR_EL2_set_TID3(&el2_regs->hcr_el2, true);
	HCR_EL2_set_TSC(&el2_regs->hcr_el2, true);
	HCR_EL2_set_TIDCP(&el2_regs->hcr_el2, true);
	HCR_EL2_set_TACR(&el2_regs->hcr_el2, true);
	HCR_EL2_set_TSW(&el2_regs->hcr_el2, true);
	HCR_EL2_set_TPCP(&el2_regs->hcr_el2, false);
	HCR_EL2_set_TPU(&el2_regs->hcr_el2, false);
	HCR_EL2_set_TTLB(&el2_regs->hcr_el2, false);
	HCR_EL2_set_TVM(&el2_regs->hcr_el2, false);
	HCR_EL2_set_TDZ(&el2_regs->hcr_el2, false);
	HCR_EL2_set_HCD(&el2_regs->hcr_el2, false);
	HCR_EL2_set_TRVM(&el2_regs->hcr_el2, false);
	HCR_EL2_set_RW(&el2_regs->hcr_el2, true);
	HCR_EL2_set_CD(&el2_regs->hcr_el2, false);
	HCR_EL2_set_ID(&el2_regs->hcr_el2, false);

	// We allow the guest to set its own inner and outer cacheability,
	// regardless of whether this may mean that memory accessed by another
	// agent (e.g. the Hypervisor) might cause a loss of coherency due to
	// mismatched memory attributes. Note, that this should never
	// constitute a secure issue as the Hypervisor must properly validate
	// any arguments from VM memory. The guest is aware of the Hypervisor
	// and it is its responsibility to ensure that memory used for
	// communication with the Hypervisor or other VMs, has the correct
	// attributes.
	HCR_EL2_set_MIOCNCE(&el2_regs->hcr_el2, true);

#if defined(ARCH_ARM_FEAT_VHE)
	HCR_EL2_set_E2H(&el2_regs->hcr_el2, true);
#endif
	HCR_EL2_set_TGE(&el2_regs->hcr_el2, false);

#if defined(ARCH_ARM_FEAT_LOR)
	// FIXME: we could temporarily set TLOR to false if we encounter Linux
	// using these registers
	HCR_EL2_set_TLOR(&el2_regs->hcr_el2, true);
#endif

#if defined(ARCH_ARM_FEAT_PAuth)
	HCR_EL2_set_APK(&el2_regs->hcr_el2, true);
	HCR_EL2_set_API(&el2_regs->hcr_el2, true);
#endif

#if defined(ARCH_ARM_FEAT_NV)
	HCR_EL2_set_AT(&el2_regs->hcr_el2, false);
	HCR_EL2_set_NV(&el2_regs->hcr_el2, false);
	HCR_EL2_set_NV1(&el2_regs->hcr_el2, false);
#endif

#if defined(ARCH_ARM_FEAT_NV2)
	HCR_EL2_set_NV2(&el2_regs->hcr_el2, false);
#endif

#if defined(ARCH_ARM_FEAT_S2FWB)
	HCR_EL2_set_FWB(&el2_regs->hcr_el2, false);
#endif

#if defined(ARCH_ARM_FEAT_RASv1p1)
	HCR_EL2_set_FIEN(&el2_regs->hcr_el2, false);
#endif

	MDCR_EL2_init(&el2_regs->mdcr_el2);
	// Enable all debug register traps by default
	MDCR_EL2_set_TDA(&el2_regs->mdcr_el2, true);
	MDCR_EL2_set_TDOSA(&el2_regs->mdcr_el2, true);
	MDCR_EL2_set_TDRA(&el2_regs->mdcr_el2, true);
	// Don't trap debug exceptions. The only ones not controlled by the
	// registers trapped above are BRK / BKPT which are never cross-VM
	MDCR_EL2_set_TDE(&el2_regs->mdcr_el2, false);
#if defined(ARCH_ARM_PMU_V3)
	// Enable PMU access traps by default
	MDCR_EL2_set_TPM(&el2_regs->mdcr_el2, true);
	MDCR_EL2_set_TPMCR(&el2_regs->mdcr_el2, true);
#endif
#if defined(ARCH_ARM_FEAT_SPE)
	// Enable SPE traps by default
	MDCR_EL2_set_TPMS(&el2_regs->mdcr_el2, true);
#endif
#if defined(ARCH_ARM_FEAT_TRF)
	// Enable trace traps by default
	MDCR_EL2_set_TTRF(&el2_regs->mdcr_el2, true);
#endif

	// FIXME: HACR_EL2 - per CPU type
}

void
vcpu_handle_rootvm_init(thread_t *root_thread)
{
	vcpu_el2_registers_t *el2_regs = &root_thread->vcpu_regs_el2;

	// Run the root VM with HCR.DC set, so we don't need a stg-1 page-table
	// Set TVM to detect the VM attempts to enable stg-1 MMU,
	// Note however we don't support switching off HCR.DC yet!
	HCR_EL2_set_DC(&el2_regs->hcr_el2, true);
	HCR_EL2_set_TVM(&el2_regs->hcr_el2, true);
}

error_t
vcpu_arch_handle_object_create_thread(thread_create_t thread_create)
{
	error_t	  err	 = OK;
	thread_t *thread = thread_create.thread;
	assert(thread != NULL);

	if (thread->kind == THREAD_KIND_VCPU) {
		// Set up nonzero init values for EL2 registers
		arch_vcpu_el2_registers_init(&thread->vcpu_regs_el2);

		// Indicate that the VCPU is uniprocessor by default. The vgic
		// module will override this if the VCPU is attached to a VIC.
		thread->vcpu_regs_mpidr_el1 = MPIDR_EL1_default();
		MPIDR_EL1_set_U(&thread->vcpu_regs_mpidr_el1, true);

#if defined(ARCH_ARM_HAVE_SCXT)
		if (!scxt_disabled) {
			vcpu_runtime_flags_set_scxt_allowed(&thread->vcpu_flags,
							    true);
		}
#endif
	}

	return err;
}

#if SCHEDULER_CAN_MIGRATE
void
vcpu_arch_handle_thread_start(void)
{
	thread_t *thread = thread_get_self();

	if (thread->kind == THREAD_KIND_VCPU) {
		if (vcpu_option_flags_get_pinned(&thread->vcpu_options)) {
			// The VCPU won't migrate, so expose the real MIDR.
			thread->vcpu_regs_midr_el1 = register_MIDR_EL1_read();
		} else {
			// Use a MIDR distinct from that of a real CPU.
			// Otherwise the guest may try to use features
			// or errata workarounds that are unsupported.
			MIDR_EL1_t midr = MIDR_EL1_default();
			MIDR_EL1_set_Architecture(&midr, 0xfU);
			MIDR_EL1_set_Implementer(&midr, 0U);
			MIDR_EL1_set_PartNum(&midr, 0x48U);
			MIDR_EL1_set_Variant(&midr, 0U);
			MIDR_EL1_set_Revision(&midr, 0U);
			thread->vcpu_regs_midr_el1 = midr;
			// Use virtual ID registers for this VCPU.
			HCR_EL2_set_TID1(&thread->vcpu_regs_el2.hcr_el2, true);
			// For migratable threads, we ensure TLB operations are
			// broadcast to all inner-shareable cores. Since Linux
			// VMs normally do this anyway, there should be no real
			// impact, and thus should be the same as forcing a TLB
			// flush at migrate time. We also ensure that all
			// barriers apply to at least the inner-shareable
			// domain.
			HCR_EL2_set_FB(&thread->vcpu_regs_el2.hcr_el2, true);
			HCR_EL2_set_BSU(&thread->vcpu_regs_el2.hcr_el2, 1U);
		}
	}
}
#endif

noreturn void
vcpu_exception_return(uintptr_t unused_param);

static noreturn void
vcpu_thread_start(bool warm_reset) EXCLUDE_PREEMPT_DISABLED
{
	trigger_vcpu_started_event(warm_reset);
	trigger_thread_exit_to_user_event(THREAD_ENTRY_REASON_NONE);
	thread_reset_stack(vcpu_exception_return, 0U);
}

static noreturn void
vcpu_thread_entry(uintptr_t unused_param) EXCLUDE_PREEMPT_DISABLED
{
	(void)unused_param;
	vcpu_thread_start(false);
}

thread_func_t
vcpu_handle_thread_get_entry_fn(void)
{
	assert(thread_get_self()->kind == THREAD_KIND_VCPU);

	return vcpu_thread_entry;
}

error_t
vcpu_configure(thread_t *thread, vcpu_option_flags_t vcpu_options)
{
	error_t ret = OK;

	assert(thread != NULL);
	assert(thread->kind == THREAD_KIND_VCPU);

	thread->vcpu_options = vcpu_options;

	return ret;
}

static void
vcpu_reset_execution_context(thread_t *vcpu)
{
	assert((vcpu != NULL) && (vcpu->kind == THREAD_KIND_VCPU));
	assert((thread_get_self() == vcpu) ||
	       scheduler_is_blocked(vcpu, SCHEDULER_BLOCK_VCPU_OFF));

	// Reset the EL1 registers.
	arch_vcpu_el1_registers_init(vcpu);

	// Reset the EL1 processor state: EL1H mode, all interrupts disabled.
	SPSR_EL2_A64_t spsr_el2 = SPSR_EL2_A64_default();
	SPSR_EL2_A64_set_M(&spsr_el2, SPSR_64BIT_MODE_EL1H);
	SPSR_EL2_A64_set_D(&spsr_el2, true);
	SPSR_EL2_A64_set_A(&spsr_el2, true);
	SPSR_EL2_A64_set_I(&spsr_el2, true);
	SPSR_EL2_A64_set_F(&spsr_el2, true);
	vcpu->vcpu_regs_gpr.spsr_el2.a64 = spsr_el2;
}

bool_result_t
vcpu_poweron(thread_t *vcpu, vmaddr_result_t entry_point,
	     register_result_t context)
{
	error_t err = OK;
	bool	ret = false;

	assert(vcpu != NULL);
	assert(vcpu->kind == THREAD_KIND_VCPU);
	assert(scheduler_is_blocked(vcpu, SCHEDULER_BLOCK_VCPU_OFF));

	if (thread_is_dying(vcpu) || thread_has_exited(vcpu)) {
		err = ERROR_FAILURE;
	}

	if (err == OK) {
		err = trigger_vcpu_poweron_event(vcpu);
	}

	if (err == OK) {
		vcpu_reset_execution_context(vcpu);
		if (entry_point.e == OK) {
			vcpu->vcpu_regs_gpr.pc = ELR_EL2_cast(entry_point.r);
		}
		if (context.e == OK) {
			vcpu->vcpu_regs_gpr.x[0] = context.r;
		}

		// We must have a valid address space and stage 2 must be
		// enabled. Otherwise the guest can trivially take over the
		// hypervisor.
		assert(HCR_EL2_get_VM(&vcpu->vcpu_regs_el2.hcr_el2) &&
		       (VTTBR_EL2_get_BADDR(
				&vcpu->addrspace->vm_pgtable.vttbr_el2) != 0U));

		ret = scheduler_unblock(vcpu, SCHEDULER_BLOCK_VCPU_OFF);
	}

	return (bool_result_t){ .e = err, .r = ret };
}

error_t
vcpu_poweroff(bool last_cpu, bool force)
{
	thread_t *current = thread_get_self();
	assert(current->kind == THREAD_KIND_VCPU);

	scheduler_lock(current);

	error_t ret = trigger_vcpu_poweroff_event(current, last_cpu, force);
	if (ret == OK) {
		scheduler_block(current, SCHEDULER_BLOCK_VCPU_OFF);
		scheduler_unlock_nopreempt(current);

		if (force) {
			preempt_enable();
			vcpu_halted();
			// not reached
		} else {
			trigger_vcpu_stopped_event();
			scheduler_yield();

			// If we get here, then someone has called
			// vcpu_poweron() on us.
			preempt_enable();
			vcpu_thread_start(false);
			// not reached
		}
	}

	scheduler_unlock(current);
	return ret;
}

#if defined(MODULE_VM_VCPU_RUN)
vcpu_run_state_t
vcpu_handle_vcpu_run_check(const thread_t *vcpu, register_t *state_data_0)
{
	vcpu_run_state_t ret = VCPU_RUN_STATE_BLOCKED;
	if (scheduler_is_blocked(vcpu, SCHEDULER_BLOCK_VCPU_FAULT)) {
		ret = VCPU_RUN_STATE_FAULT;
	} else if (scheduler_is_blocked(vcpu, SCHEDULER_BLOCK_VCPU_OFF)) {
		vcpu_run_poweroff_flags_t flags =
			vcpu_run_poweroff_flags_default();
		ret	      = VCPU_RUN_STATE_POWERED_OFF;
		*state_data_0 = vcpu_run_poweroff_flags_raw(flags);
	} else {
		// Nothing to do
	}
	return ret;
}
#endif

error_t
vcpu_suspend(void)
{
	error_t	  ret	  = OK;
	thread_t *current = thread_get_self();
	assert(current->kind == THREAD_KIND_VCPU);

	// Disable preemption so we don't try to deliver interrupts to the
	// current thread while it is suspended. We could handle that case in
	// vcpu_wakeup_self(), but we want that function to be fast.
	preempt_disable();

	if (vcpu_pending_wakeup()) {
		ret = ERROR_BUSY;
	} else {
		ret = trigger_vcpu_suspend_event(current);
	}

	if (ret == OK) {
		scheduler_lock_nopreempt(current);
		scheduler_block(current, SCHEDULER_BLOCK_VCPU_SUSPEND);
		scheduler_unlock_nopreempt(current);

		scheduler_yield();

		trigger_vcpu_resume_event(current);
	}

	preempt_enable();

	return ret;
}

void
vcpu_resume(thread_t *vcpu)
{
	assert(vcpu != NULL);
	assert(vcpu->kind == THREAD_KIND_VCPU);
	assert(scheduler_is_blocked(vcpu, SCHEDULER_BLOCK_VCPU_SUSPEND));

	if (scheduler_unblock(vcpu, SCHEDULER_BLOCK_VCPU_SUSPEND)) {
		scheduler_trigger();
	}
}

noreturn void
vcpu_warm_reset(paddr_t entry_point, register_t context)
{
	thread_t *vcpu = thread_get_self();

	assert(vcpu->kind == THREAD_KIND_VCPU);

	// Inform any other modules of the warm reset
	trigger_vcpu_warm_reset_event(vcpu);

	// Set the thread's startup context
	vcpu_reset_execution_context(vcpu);
	vcpu->vcpu_regs_gpr.pc	 = ELR_EL2_cast(entry_point);
	vcpu->vcpu_regs_gpr.x[0] = context;

	// We've been warm-reset; jump directly to the entry point.
	vcpu_thread_start(true);
}

noreturn void
vcpu_halted(void)
{
	thread_t *current = thread_get_self();

	assert(current->kind == THREAD_KIND_VCPU);

	preempt_disable();

	trigger_vcpu_stopped_event();

	(void)virq_assert(&current->vcpu_halt_virq_src, true);

	scheduler_yield();

	// If we get here, then someone resumed the halted vcpu.
	preempt_enable();
	vcpu_thread_start(false);
}

```

`hyp/vm/vcpu/aarch64/src/context_switch.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>

#include <hypregisters.h>

#include <compiler.h>
#include <cpulocal.h>
#include <scheduler.h>
#include <thread.h>

#include <asm/barrier.h>
#include <asm/sysregs.h>

#include "event_handlers.h"
#include "vectors_vcpu.h"

// VCPU_TRACE_CONTEXT_SAVED and VCPU_DEBUG_CONTEXT_SAVED defines are used as
// sanity checks to test whether the configuration is correct and we don't leak
// trace and debug context registers between VMs, or permit tracing the
// hypervisor.
#if defined(MODULE_VM_VETM) || defined(MODULE_VM_VETM_NULL)
#define VCPU_TRACE_CONTEXT_SAVED 1
#elif defined(PLATFORM_HAS_NO_ETM_BASE) && (PLATFORM_HAS_NO_ETM_BASE != 0)
#pragma message(                                                               \
	"PLATFORM_HAS_NO_ETM_BASE is nonzero; if an ETM is present it may be"  \
	" accessible and could trace the hypervisor.")
#define VCPU_TRACE_CONTEXT_SAVED 1
#endif

void
vcpu_context_switch_load(void)
{
	static_assert(VCPU_TRACE_CONTEXT_SAVED && VCPU_DEBUG_CONTEXT_SAVED,
		      "AArch64 VCPUs must context-switch trace & debug state");

	thread_t *thread = thread_get_self();

#if defined(ARCH_ARM_FEAT_VHE)
	CONTEXTIDR_EL2_t ctxidr = CONTEXTIDR_EL2_default();
	CONTEXTIDR_EL2_set_PROCID(&ctxidr, (uint32_t)(uintptr_t)thread);
	register_CONTEXTIDR_EL2_write(ctxidr);
#endif

	if (compiler_expected(thread->kind == THREAD_KIND_VCPU)) {
		register_CPACR_EL1_write(thread->vcpu_regs_el1.cpacr_el1);
		register_CSSELR_EL1_write(thread->vcpu_regs_el1.csselr_el1);
		register_CONTEXTIDR_EL1_write(
			thread->vcpu_regs_el1.contextidr_el1);
		register_ELR_EL1_write(thread->vcpu_regs_el1.elr_el1);
		register_ESR_EL1_write(thread->vcpu_regs_el1.esr_el1);
		register_FAR_EL1_write(thread->vcpu_regs_el1.far_el1);
		register_PAR_EL1_base_write(thread->vcpu_regs_el1.par_el1.base);
		register_MAIR_EL1_write(thread->vcpu_regs_el1.mair_el1);
		register_SCTLR_EL1_write(thread->vcpu_regs_el1.sctlr_el1);
		register_SP_EL0_write(thread->vcpu_regs_el1.sp_el0);
		register_SP_EL1_write(thread->vcpu_regs_el1.sp_el1);
		register_SPSR_EL1_A64_write(thread->vcpu_regs_el1.spsr_el1);
		register_TCR_EL1_write(thread->vcpu_regs_el1.tcr_el1);
		register_TPIDR_EL0_write(thread->vcpu_regs_el1.tpidr_el0);
		register_TPIDR_EL1_write(thread->vcpu_regs_el1.tpidr_el1);
		register_TPIDRRO_EL0_write(thread->vcpu_regs_el1.tpidrro_el0);
		register_TTBR0_EL1_write(thread->vcpu_regs_el1.ttbr0_el1);
		register_TTBR1_EL1_write(thread->vcpu_regs_el1.ttbr1_el1);
		register_VBAR_EL1_write(thread->vcpu_regs_el1.vbar_el1);
		register_VMPIDR_EL2_write(thread->vcpu_regs_mpidr_el1);
#if SCHEDULER_CAN_MIGRATE
		register_VPIDR_EL2_write(thread->vcpu_regs_midr_el1);
#endif
#if !defined(CPU_HAS_NO_ACTLR_EL1)
		register_ACTLR_EL1_write(thread->vcpu_regs_el1.actlr_el1);
#endif
#if !defined(CPU_HAS_NO_AMAIR_EL1)
		register_AMAIR_EL1_write(thread->vcpu_regs_el1.amair_el1);
#endif
#if !defined(CPU_HAS_NO_AFSR0_EL1)
		register_AFSR0_EL1_write(thread->vcpu_regs_el1.afsr0_el1);
#endif
#if !defined(CPU_HAS_NO_AFSR1_EL1)
		register_AFSR1_EL1_write(thread->vcpu_regs_el1.afsr1_el1);
#endif

		// Floating-point access should not be disabled for any VM
#if defined(ARCH_ARM_FEAT_VHE)
		assert_debug(CPTR_EL2_E2H1_get_FPEN(
				     &thread->vcpu_regs_el2.cptr_el2) == 3);
		register_CPTR_EL2_E2H1_write(thread->vcpu_regs_el2.cptr_el2);
#else
		assert_debug(CPTR_EL2_E2H0_get_TFP(
				     &thread->vcpu_regs_el2.cptr_el2) == 0);
		register_CPTR_EL2_E2H0_write(thread->vcpu_regs_el2.cptr_el2);
#endif

#if defined(VERBOSE) && VERBOSE
#if defined(ARCH_ARM_FEAT_VHE)
		assert_debug(HCR_EL2_get_E2H(&thread->vcpu_regs_el2.hcr_el2));
		assert_debug(!HCR_EL2_get_TGE(&thread->vcpu_regs_el2.hcr_el2));
#endif
		assert_debug(HCR_EL2_get_VM(&thread->vcpu_regs_el2.hcr_el2));
#endif
		register_HCR_EL2_write(thread->vcpu_regs_el2.hcr_el2);

		register_MDCR_EL2_write(thread->vcpu_regs_el2.mdcr_el2);

		register_VBAR_EL2_write(
			VBAR_EL2_cast(CPULOCAL(vcpu_aarch64_vectors)));

		register_FPCR_write(thread->vcpu_regs_fpr.fpcr);
		register_FPSR_write(thread->vcpu_regs_fpr.fpsr);

#if defined(ARCH_ARM_HAVE_SCXT)
		if (vcpu_runtime_flags_get_scxt_allowed(&thread->vcpu_flags)) {
			register_SCXTNUM_EL0_write(
				thread->vcpu_regs_el1.scxtnum_el0);
			register_SCXTNUM_EL1_write(
				thread->vcpu_regs_el1.scxtnum_el1);
		}
#endif
		__asm__ volatile("ldp	q0, q1, [%[q]]		;"
				 "ldp	q2, q3, [%[q], 32]	;"
				 "ldp	q4, q5, [%[q], 64]	;"
				 "ldp	q6, q7, [%[q], 96]	;"
				 "ldp	q8, q9, [%[q], 128]	;"
				 "ldp	q10, q11, [%[q], 160]	;"
				 "ldp	q12, q13, [%[q], 192]	;"
				 "ldp	q14, q15, [%[q], 224]	;"
				 "ldp	q16, q17, [%[q], 256]	;"
				 "ldp	q18, q19, [%[q], 288]	;"
				 "ldp	q20, q21, [%[q], 320]	;"
				 "ldp	q22, q23, [%[q], 352]	;"
				 "ldp	q24, q25, [%[q], 384]	;"
				 "ldp	q26, q27, [%[q], 416]	;"
				 "ldp	q28, q29, [%[q], 448]	;"
				 "ldp	q30, q31, [%[q], 480]	;"
				 :
				 : [q] "r"(thread->vcpu_regs_fpr.q),
				   "m"(thread->vcpu_regs_fpr));
	} else {
		// Set the constant non-VCPU HCR
		HCR_EL2_t nonvm_hcr = HCR_EL2_default();
		HCR_EL2_set_FMO(&nonvm_hcr, true);
		HCR_EL2_set_IMO(&nonvm_hcr, true);
		HCR_EL2_set_AMO(&nonvm_hcr, true);
#if defined(ARCH_ARM_FEAT_VHE)
		HCR_EL2_set_E2H(&nonvm_hcr, true);
#endif
		HCR_EL2_set_TGE(&nonvm_hcr, true);
		register_HCR_EL2_write(nonvm_hcr);
	}
}

void
vcpu_context_switch_save(void)
{
	thread_t *thread = thread_get_self();

	if (compiler_expected(
		    (thread->kind == THREAD_KIND_VCPU) &&
		    !scheduler_is_blocked(thread, SCHEDULER_BLOCK_VCPU_OFF))) {
		thread->vcpu_regs_el1.cpacr_el1	 = register_CPACR_EL1_read();
		thread->vcpu_regs_el1.csselr_el1 = register_CSSELR_EL1_read();
		thread->vcpu_regs_el1.contextidr_el1 =
			register_CONTEXTIDR_EL1_read();
		thread->vcpu_regs_el1.elr_el1 = register_ELR_EL1_read();
		thread->vcpu_regs_el1.esr_el1 = register_ESR_EL1_read();
		thread->vcpu_regs_el1.far_el1 = register_FAR_EL1_read();
		thread->vcpu_regs_el1.par_el1.base =
			register_PAR_EL1_base_read();
		thread->vcpu_regs_el1.mair_el1	= register_MAIR_EL1_read();
		thread->vcpu_regs_el1.sctlr_el1 = register_SCTLR_EL1_read();
		thread->vcpu_regs_el1.sp_el1	= register_SP_EL1_read();
		thread->vcpu_regs_el1.sp_el0	= register_SP_EL0_read();
		thread->vcpu_regs_el1.spsr_el1	= register_SPSR_EL1_A64_read();
		thread->vcpu_regs_el1.tcr_el1	= register_TCR_EL1_read();
		thread->vcpu_regs_el1.tpidr_el0 = register_TPIDR_EL0_read();
		thread->vcpu_regs_el1.tpidr_el1 = register_TPIDR_EL1_read();
		thread->vcpu_regs_el1.tpidrro_el0 = register_TPIDRRO_EL0_read();
		thread->vcpu_regs_el1.ttbr0_el1	  = register_TTBR0_EL1_read();
		thread->vcpu_regs_el1.ttbr1_el1	  = register_TTBR1_EL1_read();
		thread->vcpu_regs_el1.vbar_el1	  = register_VBAR_EL1_read();
#if !defined(CPU_HAS_NO_ACTLR_EL1)
		thread->vcpu_regs_el1.actlr_el1 = register_ACTLR_EL1_read();
#endif
#if !defined(CPU_HAS_NO_AMAIR_EL1)
		thread->vcpu_regs_el1.amair_el1 = register_AMAIR_EL1_read();
#endif
#if !defined(CPU_HAS_NO_AFSR0_EL1)
		thread->vcpu_regs_el1.afsr0_el1 = register_AFSR0_EL1_read();
#endif
#if !defined(CPU_HAS_NO_AFSR1_EL1)
		thread->vcpu_regs_el1.afsr1_el1 = register_AFSR1_EL1_read();
#endif

		// Read back HCR_EL2 as VSE may have been cleared.
		thread->vcpu_regs_el2.hcr_el2 = register_HCR_EL2_read();
		thread->vcpu_regs_fpr.fpcr    = register_FPCR_read();
		thread->vcpu_regs_fpr.fpsr    = register_FPSR_read();

#if defined(ARCH_ARM_HAVE_SCXT)
		if (vcpu_runtime_flags_get_scxt_allowed(&thread->vcpu_flags)) {
			thread->vcpu_regs_el1.scxtnum_el0 =
				register_SCXTNUM_EL0_read();
			thread->vcpu_regs_el1.scxtnum_el1 =
				register_SCXTNUM_EL1_read();
		}
#endif
		__asm__ volatile("stp	q0, q1, [%[q]]		;"
				 "stp	q2, q3, [%[q], 32]	;"
				 "stp	q4, q5, [%[q], 64]	;"
				 "stp	q6, q7, [%[q], 96]	;"
				 "stp	q8, q9, [%[q], 128]	;"
				 "stp	q10, q11, [%[q], 160]	;"
				 "stp	q12, q13, [%[q], 192]	;"
				 "stp	q14, q15, [%[q], 224]	;"
				 "stp	q16, q17, [%[q], 256]	;"
				 "stp	q18, q19, [%[q], 288]	;"
				 "stp	q20, q21, [%[q], 320]	;"
				 "stp	q22, q23, [%[q], 352]	;"
				 "stp	q24, q25, [%[q], 384]	;"
				 "stp	q26, q27, [%[q], 416]	;"
				 "stp	q28, q29, [%[q], 448]	;"
				 "stp	q30, q31, [%[q], 480]	;"
				 : "=m"(thread->vcpu_regs_fpr)
				 : [q] "r"(thread->vcpu_regs_fpr.q));

#if SCHEDULER_CAN_MIGRATE
		if (!vcpu_option_flags_get_pinned(&thread->vcpu_options)) {
			// We need a DSB to ensure that any cache or TLB op
			// executed by the VCPU in EL1 is complete before the
			// VCPU potentially migrates. Otherwise the VCPU may
			// execute its own DSB on the wrong CPU, and proceed
			// before the maintenance operation completes.
			__asm__ volatile("dsb ish" ::"m"(asm_ordering));
		}
#endif
	}
}

```

`hyp/vm/vcpu/aarch64/src/exception_inject.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>

#include <hypconstants.h>
#include <hypregisters.h>

#include <abort.h>
#include <compiler.h>
#include <log.h>
#include <panic.h>
#include <scheduler.h>
#include <thread.h>
#include <trace.h>
#include <util.h>
#include <vcpu.h>

#include "event_handlers.h"
#include "exception_inject.h"

#if ARCH_AARCH64_32BIT_EL1
#error Exception injection to 32-bit EL1 is not implemented
#endif

static void
exception_inject(ESR_EL1_t esr_el1)
{
	VBAR_EL1_t vbar;
	ELR_EL2_t  elr_el2;
	thread_t  *thread = thread_get_self();
	register_t guest_vector;

	vbar	     = register_VBAR_EL1_read();
	guest_vector = VBAR_EL1_get_VectorBase(&vbar);

	SPSR_EL2_A64_t spsr_el2 = thread->vcpu_regs_gpr.spsr_el2.a64;
	// Adjust the vector based on the mode we came from
	// FIXME:
	spsr_64bit_mode_t spsr_m = SPSR_EL2_A64_get_M(&spsr_el2);
	switch (spsr_m) {
	case SPSR_64BIT_MODE_EL0T:
		// Exception from 64-bit EL0
		guest_vector += 0x400U;
		break;
	case SPSR_64BIT_MODE_EL1T:
		// Exception from EL1 with SP_EL0
		// No adjustment needed
		break;
	case SPSR_64BIT_MODE_EL1H:
		// Exception from EL1 with SP_EL1
		guest_vector += 0x200U;
		break;
	case SPSR_64BIT_MODE_EL2T:
	case SPSR_64BIT_MODE_EL2H:
		// Illegal mode, panic
		panic("Illegal CPU mode: injecting exception to EL2");
	default:
		// Either illegal M, or exceptions coming from 32-bit EL0/EL1
		// For now we only support 32-bit EL0
		if (spsr_m == (spsr_64bit_mode_t)SPSR_32BIT_MODE_USER) {
			// Exception from 32-bit EL0 User
			guest_vector += 0x600U;
			break;
		}
		// Illegal mode, panic
		panic("Illegal or unsupported CPU mode");
	}

	// Set up guest's SPSR
	SPSR_EL1_A64_t spsr_el1 = SPSR_EL1_A64_cast(SPSR_EL2_A64_raw(spsr_el2));
	register_SPSR_EL1_A64_write(spsr_el1);

	// Set mode to EL1H, mask AIF, clear IL and SS
	SPSR_EL2_A64_set_A(&spsr_el2, true);
	SPSR_EL2_A64_set_I(&spsr_el2, true);
	SPSR_EL2_A64_set_F(&spsr_el2, true);
	SPSR_EL2_A64_set_D(&spsr_el2, true);
	SPSR_EL2_A64_set_IL(&spsr_el2, false);
	SPSR_EL2_A64_set_SS(&spsr_el2, false);
	SPSR_EL2_A64_set_M(&spsr_el2, SPSR_64BIT_MODE_EL1H);
#if defined(ARCH_ARM_FEAT_SSBS) || defined(ARCH_ARM_FEAT_PAN)
	SCTLR_EL1_t sctlr_el1 = register_SCTLR_EL1_read();
#if defined(ARCH_ARM_FEAT_SSBS)
	SPSR_EL2_A64_set_SSBS(&spsr_el2, SCTLR_EL1_get_DSSBS(&sctlr_el1));
#endif
#if defined(ARCH_ARM_FEAT_PAN)
	if (!SCTLR_EL1_get_SPAN(&sctlr_el1)) {
		SPSR_EL2_A64_set_PAN(&spsr_el2, true);
	}
#endif
#endif
#if defined(ARCH_ARM_FEAT_UAO)
	SPSR_EL2_A64_set_UAO(&spsr_el2, false);
#endif
#if defined(ARCH_ARM_FEAT_MTE)
	SPSR_EL2_A64_set_TCO(&spsr_el2, true);
#endif
#if defined(ARCH_ARM_FEAT_BTI)
	SPSR_EL2_A64_set_BTYPE(&spsr_el2, 0);
#endif
	thread->vcpu_regs_gpr.spsr_el2.a64 = spsr_el2;

	// Tell the guest where the exception came from
	elr_el2		  = thread->vcpu_regs_gpr.pc;
	ELR_EL1_t elr_el1 = ELR_EL1_cast(ELR_EL2_raw(elr_el2));
	register_ELR_EL1_write(elr_el1);

	register_ESR_EL1_write(esr_el1);

	// Return to the guest's vector
	ELR_EL2_set_ReturnAddress(&elr_el2, guest_vector);
	thread->vcpu_regs_gpr.pc = elr_el2;
}

bool
inject_inst_data_abort(ESR_EL2_t esr_el2, esr_ec_t ec, iss_da_ia_fsc_t fsc,
		       FAR_EL2_t far, vmaddr_t ipa, bool is_data_abort)
{
	thread_t      *thread	= thread_get_self();
	ESR_EL1_t      esr_el1	= ESR_EL1_cast(ESR_EL2_raw(esr_el2));
	SPSR_EL2_A64_t spsr_el2 = thread->vcpu_regs_gpr.spsr_el2.a64;
	gvaddr_t       va	= FAR_EL2_get_VirtualAddress(&far);

	assert(thread->kind == THREAD_KIND_VCPU);
	assert(thread->addrspace != NULL);

	// Assert EC is instruction/data abort from lower levels
	assert((ec == ESR_EC_INST_ABT_LO) || (ec == ESR_EC_DATA_ABT_LO));

	// Check the reason behind the abort
	switch (fsc) {
	case ISS_DA_IA_FSC_ADDR_SIZE_0:	  // Address size fault - level 0 of
					  // translation or the TTB
	case ISS_DA_IA_FSC_ADDR_SIZE_1:	  // Address size fault - level 1
	case ISS_DA_IA_FSC_ADDR_SIZE_2:	  // Address size fault - level 2
	case ISS_DA_IA_FSC_ADDR_SIZE_3:	  // Address size fault - level 3
	case ISS_DA_IA_FSC_TRANSLATION_0: // Translation fault, level 0
	case ISS_DA_IA_FSC_TRANSLATION_1: // Translation fault, level 1
	case ISS_DA_IA_FSC_TRANSLATION_2: // Translation fault, level 2
	case ISS_DA_IA_FSC_TRANSLATION_3: // Translation fault, level 3
	case ISS_DA_IA_FSC_PERMISSION_1:  // Permission fault, level 1
	case ISS_DA_IA_FSC_PERMISSION_2:  // Permission fault, level 2
	case ISS_DA_IA_FSC_PERMISSION_3:  // Permission fault, level 3
	case ISS_DA_IA_FSC_ALIGNMENT: {	  // Alignment fault
#if defined(VERBOSE) && VERBOSE
		// Injecting an abort from the guest EL1H sync vector will
		// cause an exception inject loop, so block the vcpu instead.
		if (SPSR_EL2_A64_get_M(&spsr_el2) == SPSR_64BIT_MODE_EL1H) {
			VBAR_EL1_t vbar = register_VBAR_EL1_read();
			uint64_t   pc	= ELR_EL2_get_ReturnAddress(
				    &thread->vcpu_regs_gpr.pc);
			uint64_t el1h_sync_vector =
				VBAR_EL1_get_VectorBase(&vbar) + 0x200U;
			if (util_balign_down(pc, 0x80U) == el1h_sync_vector) {
				VTTBR_EL2_t vttbr = register_VTTBR_EL2_read();
				TRACE_AND_LOG(
					DEBUG, DEBUG,
					"Detected exception inject loop from "
					"VM {:d}, previous ESR_EL1 = {:#x}, "
					"ELR_EL1 = {:#x}, VBAR_EL1 = {:#x}",
					VTTBR_EL2_get_VMID(&vttbr),
					ESR_EL1_raw(register_ESR_EL1_read()),
					ELR_EL1_raw(register_ELR_EL1_read()),
					VBAR_EL1_raw(vbar));
				scheduler_lock(thread);
				scheduler_block(thread,
						SCHEDULER_BLOCK_VCPU_FAULT);
				scheduler_unlock(thread);
				vcpu_halted();
			}
		}
#endif
		// Inject a synchronous external abort
		spsr_64bit_mode_t mode = SPSR_EL2_A64_get_M(&spsr_el2);
		if ((mode == SPSR_64BIT_MODE_EL1T) ||
		    (mode == SPSR_64BIT_MODE_EL1H)) {
			if (is_data_abort) {
				// Data abort from EL1
				ESR_EL1_set_EC(&esr_el1, ESR_EC_DATA_ABT);
			} else {
				// Instruction abort from EL1
				ESR_EL1_set_EC(&esr_el1, ESR_EC_INST_ABT);
			}
		} else {
			if (is_data_abort) {
				// Data abort from EL0
				ESR_EL1_set_EC(&esr_el1, ESR_EC_DATA_ABT_LO);
			} else {
				// Instruction abort from EL0
				ESR_EL1_set_EC(&esr_el1, ESR_EC_INST_ABT_LO);
			}
		}

		// Change ISS.FSC to synchronous external abort, clear ISV, SSE,
		// SF, AR, EA, S1PTW, SAS and SRT.
		if (is_data_abort) {
			ESR_EL2_ISS_DATA_ABORT_t iss;
			ESR_EL2_ISS_DATA_ABORT_init(&iss);
			ESR_EL2_ISS_DATA_ABORT_set_DFSC(
				&iss, ISS_DA_IA_FSC_SYNC_EXTERNAL);
			ESR_EL1_set_ISS(&esr_el1,
					ESR_EL2_ISS_DATA_ABORT_raw(iss));
		} else {
			ESR_EL2_ISS_INST_ABORT_t iss;
			ESR_EL2_ISS_INST_ABORT_init(&iss);
			ESR_EL2_ISS_INST_ABORT_set_IFSC(
				&iss, ISS_DA_IA_FSC_SYNC_EXTERNAL);
			ESR_EL1_set_ISS(&esr_el1,
					ESR_EL2_ISS_INST_ABORT_raw(iss));
		}

		register_FAR_EL1_write(FAR_EL1_cast(FAR_EL2_raw(far)));

		TRACE_AND_LOG(ERROR, WARN,
			      "Injecting instruction/data abort to VM {:d}, "
			      "original ESR_EL2 = {:#x}, fault VA = {:#x}, "
			      "fault IPA = {:#x}, ELR_EL2 = {:#x}",
			      thread->addrspace->vmid, ESR_EL2_raw(esr_el2), va,
			      ipa, ELR_EL2_raw(thread->vcpu_regs_gpr.pc));

		// Inject the fault to the guest
		exception_inject(esr_el1);
		break;
	}
	case ISS_DA_IA_FSC_ACCESS_FLAG_1:
	case ISS_DA_IA_FSC_ACCESS_FLAG_2:
	case ISS_DA_IA_FSC_ACCESS_FLAG_3:
	case ISS_DA_IA_FSC_SYNC_EXTERNAL:
	case ISS_DA_IA_FSC_SYNC_EXTERN_WALK_0:
	case ISS_DA_IA_FSC_SYNC_EXTERN_WALK_1:
	case ISS_DA_IA_FSC_SYNC_EXTERN_WALK_2:
	case ISS_DA_IA_FSC_SYNC_EXTERN_WALK_3:
	case ISS_DA_IA_FSC_SYNC_PARITY_ECC:
	case ISS_DA_IA_FSC_SYNC_PARITY_ECC_WALK_0:
	case ISS_DA_IA_FSC_SYNC_PARITY_ECC_WALK_1:
	case ISS_DA_IA_FSC_SYNC_PARITY_ECC_WALK_2:
	case ISS_DA_IA_FSC_SYNC_PARITY_ECC_WALK_3:
	case ISS_DA_IA_FSC_SYNC_TAG_CHECK:
	case ISS_DA_IA_FSC_TLB_CONFLICT:
#if defined(ARCH_ARM_FEAT_HAFDBS)
	case ISS_DA_IA_FSC_ATOMIC_HW_UPDATE:
#endif
	case ISS_DA_IA_FSC_PAGE_DOMAIN:
	case ISS_DA_IA_FSC_SECTION_DOMAIN:
	case ISS_DA_IA_FSC_DEBUG:
	case ISS_DA_IA_FSC_IMP_DEF_LOCKDOWN:
	case ISS_DA_IA_FSC_IMP_DEF_ATOMIC:
	default: {
		TRACE_AND_LOG(ERROR, WARN,
			      "instruction/data abort from VM {:d}, "
			      "ESR_EL2 = {:#x}, fault VA = {:#x}, "
			      "fault IPA = {:#x}, ELR_EL2 = {:#x}",
			      thread->addrspace->vmid, ESR_EL2_raw(esr_el2), va,
			      ipa, ELR_EL2_raw(thread->vcpu_regs_gpr.pc));

		// We will get here if this is a:
		// - Access flag fault
		// - TLB walk fault
		// - Section domain fault
		// - Page domain fault
		// - IMPLEMENTATION DEFINED fault
		// Also the following have already been checked by the caller:
		// - TLB conflict
		// - Unsupported atomic hardware update file (ARM_FEAT_HAFDBS)

		if (vcpu_option_flags_get_critical(&thread->vcpu_options)) {
			abort("Unhandled instruction/data abort",
			      ABORT_REASON_UNHANDLED_EXCEPTION);
		} else {
			// The above faults cannot be triggered by the VM, so
			// halt the VCPU without revealing any fault state.
			scheduler_lock(thread);
			scheduler_block(thread, SCHEDULER_BLOCK_VCPU_FAULT);
			scheduler_unlock(thread);
			vcpu_halted();
		}
	}
	}

	return true;
}

void
inject_undef_abort(ESR_EL2_t esr_el2)
{
	ESR_EL1_t esr_el1;

	ESR_EL1_init(&esr_el1);
	ESR_EL1_set_IL(&esr_el1, ESR_EL2_get_IL(&esr_el2));
	ESR_EL1_set_EC(&esr_el1, ESR_EC_UNKNOWN);

	thread_t *thread = thread_get_self();
	TRACE_AND_LOG(INFO, DEBUG,
		      "Injecting unknown abort to VM {:d}, "
		      "original ESR_EL2 {:#x}",
		      thread->addrspace->vmid, ESR_EL2_raw(esr_el2),
		      ESR_EL2_raw(esr_el2));

	// Inject the fault to the guest
	exception_inject(esr_el1);
}

// Default handler for BRK instructions that injects a BRK back to the guest
// VM.
vcpu_trap_result_t
vcpu_handle_vcpu_trap_brk_instruction_guest(ESR_EL2_t esr)
{
	ESR_EL1_t esr_el1 = ESR_EL1_cast(ESR_EL2_raw(esr));

	exception_inject(esr_el1);

	return VCPU_TRAP_RESULT_RETRY;
}

```

`hyp/vm/vcpu/aarch64/src/hypercalls.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#if defined(HYPERCALLS)
#include <assert.h>
#include <hyptypes.h>

#include <hypcall_def.h>
#include <hyprights.h>

#include <atomic.h>
#include <compiler.h>
#include <cpulocal.h>
#include <cspace.h>
#include <cspace_lookup.h>
#include <object.h>
#include <platform_cpu.h>
#include <scheduler.h>
#include <spinlock.h>
#include <thread.h>
#include <util.h>
#include <vcpu.h>

#include "event_handlers.h"
#include "reg_access.h"

// This hypercall should be called before the vCPU is activated. It copies the
// provided flags into a variable called vcpu_options in the thread structure.
// Relevant modules (such as the debug module) need to extend the
// vcpu_option_flags bitfield to add their configuration flags, and in their
// thread_activate handlers they need to check the values of these flags (by
// looking at the thread's vcpu_options variable) and act on them.
error_t
hypercall_vcpu_configure(cap_id_t cap_id, vcpu_option_flags_t vcpu_options)
{
	error_t ret = OK;

	// Check for unknown option flags
	vcpu_option_flags_t clean = vcpu_option_flags_clean(vcpu_options);
	if (vcpu_option_flags_raw(vcpu_options) !=
	    vcpu_option_flags_raw(clean)) {
		ret = ERROR_ARGUMENT_INVALID;
		goto out;
	}

	cspace_t	   *cspace = cspace_get_self();
	object_type_t	    type;
	object_ptr_result_t result = cspace_lookup_object_any(
		cspace, cap_id, CAP_RIGHTS_GENERIC_OBJECT_ACTIVATE, &type);
	if (compiler_unexpected(result.e != OK)) {
		ret = result.e;
		goto out;
	}

	if (compiler_unexpected(type != OBJECT_TYPE_THREAD)) {
		ret = ERROR_CSPACE_WRONG_OBJECT_TYPE;
		object_put(type, result.r);
		goto out;
	}

	thread_t *vcpu = result.r.thread;

	if (compiler_expected(vcpu->kind == THREAD_KIND_VCPU)) {
		spinlock_acquire(&vcpu->header.lock);
		object_state_t state = atomic_load_relaxed(&vcpu->header.state);
		if (state == OBJECT_STATE_INIT) {
			ret = vcpu_configure(vcpu, vcpu_options);
		} else {
			ret = ERROR_OBJECT_STATE;
		}
		spinlock_release(&vcpu->header.lock);
	} else {
		ret = ERROR_ARGUMENT_INVALID;
	}

	object_put_thread(vcpu);
out:
	return ret;
}

error_t
hypercall_vcpu_register_write(cap_id_t		  vcpu_cap,
			      vcpu_register_set_t register_set,
			      index_t register_index, register_t value)
{
	error_t	  ret;
	cspace_t *cspace = cspace_get_self();

	thread_ptr_result_t result = cspace_lookup_thread_any(
		cspace, vcpu_cap, CAP_RIGHTS_THREAD_WRITE_CONTEXT);
	if (compiler_unexpected(result.e != OK)) {
		ret = result.e;
		goto out;
	}

	thread_t *vcpu = result.r;

	ret = vcpu_register_write(vcpu, register_set, register_index, value);

	object_put_thread(vcpu);
out:
	return ret;
}

error_t
hypercall_vcpu_bind_virq(cap_id_t vcpu_cap, cap_id_t vic_cap, virq_t virq,
			 vcpu_virq_type_t virq_type)
{
	error_t	  err;
	cspace_t *cspace = cspace_get_self();

	thread_ptr_result_t result = cspace_lookup_thread(
		cspace, vcpu_cap, CAP_RIGHTS_THREAD_BIND_VIRQ);
	if (compiler_unexpected(result.e != OK)) {
		err = result.e;
		goto out;
	}
	thread_t *vcpu = result.r;

	vic_ptr_result_t v =
		cspace_lookup_vic(cspace, vic_cap, CAP_RIGHTS_VIC_BIND_SOURCE);
	if (compiler_unexpected(v.e != OK)) {
		err = v.e;
		goto out_release_vcpu;
	}
	vic_t *vic = v.r;

	err = vcpu_bind_virq(vcpu, vic, virq, virq_type);

	object_put_vic(vic);
out_release_vcpu:
	object_put_thread(vcpu);
out:
	return err;
}

error_t
hypercall_vcpu_unbind_virq(cap_id_t vcpu_cap, vcpu_virq_type_t virq_type)
{
	error_t	  err	 = OK;
	cspace_t *cspace = cspace_get_self();

	thread_ptr_result_t result = cspace_lookup_thread(
		cspace, vcpu_cap, CAP_RIGHTS_THREAD_BIND_VIRQ);
	if (compiler_unexpected(result.e != OK)) {
		err = result.e;
		goto out;
	}
	thread_t *vcpu = result.r;

	err = vcpu_unbind_virq(vcpu, virq_type);

	object_put_thread(vcpu);
out:
	return err;
}

error_t
hypercall_vcpu_set_affinity(cap_id_t cap_id, cpu_index_t affinity)
{
	error_t		    ret;
	cspace_t	   *cspace = cspace_get_self();
	cap_rights_thread_t required_rights;

	if (affinity == CPU_INDEX_INVALID) {
#if SCHEDULER_CAN_MIGRATE
		// Thread will become non-runnable
		required_rights = cap_rights_thread_union(
			CAP_RIGHTS_THREAD_AFFINITY, CAP_RIGHTS_THREAD_DISABLE);
#else
		ret = ERROR_UNIMPLEMENTED;
		goto out;
#endif
	} else if (!platform_cpu_exists(affinity)) {
		ret = ERROR_ARGUMENT_INVALID;
		goto out;
	} else {
		// Affinity is valid
		required_rights = CAP_RIGHTS_THREAD_AFFINITY;
	}

	thread_ptr_result_t result =
		cspace_lookup_thread_any(cspace, cap_id, required_rights);
	if (compiler_unexpected(result.e != OK)) {
		ret = result.e;
		goto out;
	}

	thread_t *vcpu = result.r;

	if (compiler_unexpected(vcpu->kind != THREAD_KIND_VCPU)) {
		ret = ERROR_ARGUMENT_INVALID;
		object_put_thread(vcpu);
		goto out;
	}

	spinlock_acquire(&vcpu->header.lock);
	object_state_t state = atomic_load_relaxed(&vcpu->header.state);
#if SCHEDULER_CAN_MIGRATE
	if ((state == OBJECT_STATE_INIT) || (state == OBJECT_STATE_ACTIVE)) {
#else
	if (state == OBJECT_STATE_INIT) {
#endif
		scheduler_lock_nopreempt(vcpu);
		ret = scheduler_set_affinity(vcpu, affinity);
		scheduler_unlock_nopreempt(vcpu);
	} else {
		ret = ERROR_OBJECT_STATE;
	}
	spinlock_release(&vcpu->header.lock);

	object_put_thread(vcpu);
out:
	return ret;
}

error_t
hypercall_vcpu_poweron(cap_id_t cap_id, uint64_t entry_point, uint64_t context,
		       vcpu_poweron_flags_t flags)
{
	error_t	  ret	 = OK;
	cspace_t *cspace = cspace_get_self();

	if (!vcpu_poweron_flags_is_clean(flags)) {
		ret = ERROR_ARGUMENT_INVALID;
		goto out;
	}

	thread_ptr_result_t result =
		cspace_lookup_thread(cspace, cap_id, CAP_RIGHTS_THREAD_POWER);
	if (compiler_unexpected(result.e != OK)) {
		ret = result.e;
		goto out;
	}

	thread_t *vcpu = result.r;

	if (compiler_expected(vcpu->kind == THREAD_KIND_VCPU)) {
		bool reschedule = false;

		scheduler_lock(vcpu);
		if (scheduler_is_blocked(vcpu, SCHEDULER_BLOCK_VCPU_OFF)) {
			bool_result_t poweron_result = vcpu_poweron(
				vcpu,
				vcpu_poweron_flags_get_preserve_entry_point(
					&flags)
					? vmaddr_result_error(
						  ERROR_ARGUMENT_INVALID)
					: vmaddr_result_ok(entry_point),
				vcpu_poweron_flags_get_preserve_context(&flags)
					? register_result_error(
						  ERROR_ARGUMENT_INVALID)
					: register_result_ok(context));
			reschedule = poweron_result.r;
			ret	   = poweron_result.e;
		} else {
			ret = ERROR_BUSY;
		}
		scheduler_unlock(vcpu);
		object_put_thread(vcpu);

		if (reschedule) {
			(void)scheduler_schedule();
		}
	} else {
		ret = ERROR_ARGUMENT_INVALID;
		object_put_thread(vcpu);
	}
out:
	return ret;
}

error_t
hypercall_vcpu_poweroff(cap_id_t cap_id, vcpu_poweroff_flags_t flags)
{
	error_t	  ret	 = OK;
	cspace_t *cspace = cspace_get_self();

	if (!vcpu_poweroff_flags_is_clean(flags)) {
		ret = ERROR_ARGUMENT_INVALID;
		goto out;
	}

	thread_ptr_result_t result =
		cspace_lookup_thread(cspace, cap_id, CAP_RIGHTS_THREAD_POWER);
	if (compiler_unexpected(result.e != OK)) {
		ret = result.e;
		goto out;
	}

	thread_t *vcpu = result.r;

	if (compiler_expected(vcpu->kind == THREAD_KIND_VCPU) &&
	    (vcpu == thread_get_self())) {
		// We can (and must) safely release our reference to the VCPU
		// here, because we know it's the current thread so the
		// scheduler will keep a reference to it. Since vcpu_poweroff()
		// does not return, failing to release this reference will
		// leave the thread as a zombie after it halts.
		object_put_thread(vcpu);

		ret = vcpu_poweroff(vcpu_poweroff_flags_get_last_vcpu(&flags),
				    false);
		// It will not reach here if it succeeded
	} else {
		ret = ERROR_ARGUMENT_INVALID;
		object_put_thread(vcpu);
	}

out:
	return ret;
}

error_t
hypercall_vcpu_set_priority(cap_id_t cap_id, priority_t priority)
{
	error_t	  ret;
	cspace_t *cspace = cspace_get_self();

	thread_ptr_result_t result = cspace_lookup_thread_any(
		cspace, cap_id, CAP_RIGHTS_THREAD_PRIORITY);
	if (compiler_unexpected(result.e != OK)) {
		ret = result.e;
		goto out;
	}

	thread_t *vcpu = result.r;

	if (compiler_unexpected(vcpu->kind != THREAD_KIND_VCPU)) {
		ret = ERROR_ARGUMENT_INVALID;
		object_put_thread(vcpu);
		goto out;
	}

	if (priority > VCPU_MAX_PRIORITY) {
		ret = ERROR_DENIED;
		object_put_thread(vcpu);
		goto out;
	}

	spinlock_acquire(&vcpu->header.lock);
	object_state_t state = atomic_load_relaxed(&vcpu->header.state);
	if (state == OBJECT_STATE_INIT) {
		scheduler_lock_nopreempt(vcpu);
		ret = scheduler_set_priority(vcpu, priority);
		scheduler_unlock_nopreempt(vcpu);
	} else {
		ret = ERROR_OBJECT_STATE;
	}
	spinlock_release(&vcpu->header.lock);

	object_put_thread(vcpu);
out:
	return ret;
}

error_t
hypercall_vcpu_set_timeslice(cap_id_t cap_id, nanoseconds_t timeslice)
{
	error_t	  ret;
	cspace_t *cspace = cspace_get_self();

	thread_ptr_result_t result = cspace_lookup_thread_any(
		cspace, cap_id, CAP_RIGHTS_THREAD_TIMESLICE);
	if (compiler_unexpected(result.e != OK)) {
		ret = result.e;
		goto out;
	}

	thread_t *vcpu = result.r;

	if (compiler_unexpected(vcpu->kind != THREAD_KIND_VCPU)) {
		ret = ERROR_ARGUMENT_INVALID;
		object_put_thread(vcpu);
		goto out;
	}

	spinlock_acquire(&vcpu->header.lock);
	object_state_t state = atomic_load_relaxed(&vcpu->header.state);
	if (state == OBJECT_STATE_INIT) {
		scheduler_lock_nopreempt(vcpu);
		ret = scheduler_set_timeslice(vcpu, timeslice);
		scheduler_unlock_nopreempt(vcpu);
	} else {
		ret = ERROR_OBJECT_STATE;
	}
	spinlock_release(&vcpu->header.lock);

	object_put_thread(vcpu);
out:
	return ret;
}

error_t
hypercall_vcpu_kill(cap_id_t cap_id)
{
	error_t	  ret;
	cspace_t *cspace = cspace_get_self();

	thread_ptr_result_t result = cspace_lookup_thread(
		cspace, cap_id, CAP_RIGHTS_THREAD_LIFECYCLE);
	if (compiler_unexpected(result.e != OK)) {
		ret = result.e;
		goto out;
	}

	thread_t *vcpu = result.r;

	if (compiler_expected(vcpu->kind == THREAD_KIND_VCPU)) {
		ret = thread_kill(vcpu);
	} else {
		ret = ERROR_ARGUMENT_INVALID;
	}

	object_put_thread(vcpu);
out:
	return ret;
}

#else
extern int unused;
#endif

```

`hyp/vm/vcpu/aarch64/src/reg_access.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>

#include <atomic.h>
#include <compiler.h>
#include <object.h>
#include <scheduler.h>
#include <thread.h>
#include <util.h>
#include <vcpu.h>

#include "reg_access.h"

register_t
vcpu_gpr_read(thread_t *thread, uint8_t reg_num)
{
	register_t value;

	assert(reg_num <= 31U);

	if (compiler_expected(reg_num != 31U)) {
		value = thread->vcpu_regs_gpr.x[reg_num];
	} else {
		value = 0;
	}

	return value;
}

void
vcpu_gpr_write(thread_t *thread, uint8_t reg_num, register_t value)
{
	assert(reg_num <= 31U);

	if (compiler_expected(reg_num != 31U)) {
		thread->vcpu_regs_gpr.x[reg_num] = value;
	}
}

error_t
vcpu_register_write(thread_t *vcpu, vcpu_register_set_t register_set,
		    index_t register_index, register_t value)
{
	error_t err;

	if (compiler_expected(vcpu->kind != THREAD_KIND_VCPU)) {
		err = ERROR_ARGUMENT_INVALID;
		goto out;
	}

	scheduler_lock(vcpu);

	thread_state_t state = atomic_load_relaxed(&vcpu->state);
	if ((state != THREAD_STATE_INIT) && (state != THREAD_STATE_READY)) {
		// Thread has been killed or exited
		err = ERROR_OBJECT_STATE;
		goto out_locked;
	}

	if (!scheduler_is_blocked(vcpu, SCHEDULER_BLOCK_VCPU_OFF)) {
		// Not safe to access registers of a runnable VCPU
		err = ERROR_BUSY;
		goto out_locked;
	}

	switch (register_set) {
	case VCPU_REGISTER_SET_X:
		if (register_index < 31U) {
			vcpu_gpr_write(vcpu, (uint8_t)register_index, value);
			err = OK;
		} else {
			err = ERROR_ARGUMENT_INVALID;
		}
		break;
	case VCPU_REGISTER_SET_PC:
#if ARCH_AARCH64_32BIT_EL1
#error alignment check is not correct for AArch32
#endif
		if ((register_index == 0U) && util_is_baligned(value, 4U)) {
			vcpu->vcpu_regs_gpr.pc = ELR_EL2_cast(value);
			err		       = OK;
		} else {
			err = ERROR_ARGUMENT_INVALID;
		}
		break;
	case VCPU_REGISTER_SET_SP_EL:
		if (!util_is_baligned(value, 16U)) {
			err = ERROR_ARGUMENT_INVALID;
		} else if (register_index == 0U) {
			vcpu->vcpu_regs_el1.sp_el0 = SP_EL0_cast(value);
			err			   = OK;
		} else if (register_index == 1U) {
			vcpu->vcpu_regs_el1.sp_el1 = SP_EL1_cast(value);
			err			   = OK;
		} else {
			err = ERROR_ARGUMENT_INVALID;
		}
		break;
	default:
		err = ERROR_ARGUMENT_INVALID;
		break;
	}

out_locked:
	scheduler_unlock(vcpu);
out:
	return err;
}

```

`hyp/vm/vcpu/aarch64/src/sysreg_traps.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>

#include <hypregisters.h>

#include <compiler.h>
#include <log.h>
#include <preempt.h>
#include <thread.h>
#include <trace.h>
#include <vcpu.h>
#if defined(ARCH_ARM_FEAT_MPAM)
#include <arm_mpam.h>
#endif
#if defined(ARCH_ARM_FEAT_MTE)
#include <arm_mte.h>
#endif

#include <asm/sysregs.h>
#include <asm/system_registers.h>

#include "event_handlers.h"

static_assert((ARCH_AARCH64_32BIT_EL1 && ARCH_AARCH64_32BIT_EL0) ||
		      !ARCH_AARCH64_32BIT_EL1,
	      "32BIT_EL1 implies 32BIT_EL0");

#if defined(ARCH_ARM_FEAT_SHA512) != defined(ARCH_ARM_FEAT_SHA3)
#error ARCH_ARM_FEAT_SHA512 and ARCH_ARM_FEAT_SHA3 mismatch
#endif

#if SCHEDULER_CAN_MIGRATE
static bool
read_virtual_id_register(ESR_EL2_ISS_MSR_MRS_t iss, uint8_t reg_num)
{
	register_t reg_val = 0U;
	bool	   handled = true;
	thread_t  *thread  = thread_get_self();

	switch (ESR_EL2_ISS_MSR_MRS_raw(iss)) {
	// Trapped with HCR_EL2.TID1
	case ISS_MRS_MSR_REVIDR_EL1:
		// RAZ
		break;
	case ISS_MRS_MSR_AIDR_EL1:
		// RAZ
		break;
	// Trapped with HCR_EL2.TID2
	// Trapped with HCR_EL2.TID3
	case ISS_MRS_MSR_MVFR0_EL1:
		// TODO - it is possible that not all cores support the same
		// features. For non-pinned vcpus, we return the HW MVFRx_EL1
		// values, which has potential to return incorrect values.  If
		// this becomes a problem, we need to define a subset ID value
		// per machine.
#if ARCH_AARCH64_32BIT_EL0 && ARCH_AARCH64_32BIT_EL0_ALL_CORES
		sysreg64_read(MVFR0_EL1, reg_val);
#else
		reg_val = 0U; // Return defined as UNKNOWN
#endif
		break;
	case ISS_MRS_MSR_MVFR1_EL1:
#if ARCH_AARCH64_32BIT_EL0 && ARCH_AARCH64_32BIT_EL0_ALL_CORES
		sysreg64_read(MVFR1_EL1, reg_val);
#else
		reg_val = 0U; // Return defined as UNKNOWN
#endif
		break;
	case ISS_MRS_MSR_MVFR2_EL1:
#if ARCH_AARCH64_32BIT_EL0 && ARCH_AARCH64_32BIT_EL0_ALL_CORES
		sysreg64_read(MVFR2_EL1, reg_val);
#else
		reg_val = 0U; // Return defined as UNKNOWN
#endif
		break;
	case ISS_MRS_MSR_ID_AA64PFR0_EL1: {
		ID_AA64PFR0_EL1_t pfr0 = ID_AA64PFR0_EL1_default();
#if ARCH_AARCH64_32BIT_EL0 && ARCH_AARCH64_32BIT_EL0_ALL_CORES
		ID_AA64PFR0_EL1_set_EL0(&pfr0, 2U);
#else
		ID_AA64PFR0_EL1_set_EL0(&pfr0, 1U);
#endif
#if ARCH_AARCH64_32BIT_EL1
		ID_AA64PFR0_EL1_set_EL1(&pfr0, 2U);
#else
		ID_AA64PFR0_EL1_set_EL1(&pfr0, 1U);
#endif
		ID_AA64PFR0_EL1_set_EL2(&pfr0, 1U);
		ID_AA64PFR0_EL1_set_EL3(&pfr0, 1U);
#if defined(ARCH_ARM_FEAT_FP16)
		ID_AA64PFR0_EL1_set_FP(&pfr0, 1U);
		ID_AA64PFR0_EL1_set_AdvSIMD(&pfr0, 1U);
#endif
		ID_AA64PFR0_EL1_set_GIC(&pfr0, 1U);

		if (vcpu_option_flags_get_ras_error_handler(
			    &thread->vcpu_options)) {
#if defined(ARCH_ARM_FEAT_RASv1p1)
			ID_AA64PFR0_EL1_set_RAS(&pfr0, 2);
#elif defined(ARCH_ARM_FEAT_RAS)
			ID_AA64PFR0_EL1_set_RAS(&pfr0, 1);
#else
			// Nothing to do, the field is already 0
#endif
		}

#if defined(ARCH_ARM_FEAT_SEL2)
		ID_AA64PFR0_EL1_set_SEL2(&pfr0, 1U);
#endif
#if defined(ARCH_ARM_FEAT_DIT)
		ID_AA64PFR0_EL1_set_DIT(&pfr0, 1U);
#endif
#if defined(ARCH_ARM_HAVE_SCXT) &&                                             \
	(defined(ARCH_ARM_FEAT_CSV2_2) || defined(ARCH_ARM_FEAT_CSV2_3))
		if (vcpu_runtime_flags_get_scxt_allowed(&thread->vcpu_flags)) {
#if defined(ARCH_ARM_FEAT_CSV2_3)
			ID_AA64PFR0_EL1_set_CSV2(&pfr0, 3U);
#else
			ID_AA64PFR0_EL1_set_CSV2(&pfr0, 2U);
#endif
		} else {
			ID_AA64PFR0_EL1_set_CSV2(&pfr0, 1U);
		}
#elif defined(ARCH_ARM_FEAT_CSV2)
		ID_AA64PFR0_EL1_set_CSV2(&pfr0, 1U);
#endif
#if defined(ARCH_ARM_FEAT_CSV3)
		ID_AA64PFR0_EL1_set_CSV3(&pfr0, 1U);
#endif
#if defined(ARCH_ARM_FEAT_MPAM)
		if (arm_mpam_is_allowed() &&
		    vcpu_option_flags_get_mpam_allowed(&thread->vcpu_options)) {
			ID_AA64PFR0_EL1_t hw_pfr0 =
				register_ID_AA64PFR0_EL1_read();
			ID_AA64PFR0_EL1_copy_MPAM(&pfr0, &hw_pfr0);
		}
#endif
		reg_val = ID_AA64PFR0_EL1_raw(pfr0);
		break;
	}
	case ISS_MRS_MSR_ID_AA64PFR1_EL1: {
		ID_AA64PFR1_EL1_t pfr1 = ID_AA64PFR1_EL1_default();
#if defined(ARCH_ARM_FEAT_BTI)
		ID_AA64PFR1_EL1_set_BT(&pfr1, 1U);
#endif
#if defined(ARCH_ARM_FEAT_SSBS)
#if defined(ARCH_ARM_FEAT_SSBS_MSR_MRS)
		ID_AA64PFR1_EL1_set_SSBS(&pfr1, 2U);
#else
		ID_AA64PFR1_EL1_set_SSBS(&pfr1, 1U);
#endif
#endif
#if defined(ARCH_ARM_HAVE_SCXT) && defined(ARCH_ARM_FEAT_CSV2_1p2)
		if (vcpu_runtime_flags_get_scxt_allowed(&thread->vcpu_flags)) {
			ID_AA64PFR1_EL1_set_CSV2_frac(&pfr0, 2U);
		} else {
			ID_AA64PFR1_EL1_set_CSV2_frac(&pfr0, 1U);
		}
#elif defined(ARCH_ARM_FEAT_CSV2)
#if defined(ARCH_ARM_FEAT_CSV2_1p1) || defined(ARCH_ARM_FEAT_CSV2_1p2)
		ID_AA64PFR1_EL1_set_CSV2_frac(&pfr0, 1U);
#endif
#endif
#if defined(ARCH_ARM_FEAT_MTE)
		if (arm_mte_is_allowed()) {
			ID_AA64PFR1_EL1_t hw_pfr1 =
				register_ID_AA64PFR1_EL1_read();
			ID_AA64PFR1_EL1_copy_MTE(&pfr1, &hw_pfr1);
		}
#endif
#if defined(ARCH_ARM_FEAT_MPAM)
		if (arm_mpam_is_allowed() &&
		    vcpu_option_flags_get_mpam_allowed(&thread->vcpu_options)) {
			ID_AA64PFR1_EL1_t hw_pfr1 =
				register_ID_AA64PFR1_EL1_read();
			ID_AA64PFR1_EL1_copy_MPAM_frac(&pfr1, &hw_pfr1);
		}
#endif
		reg_val = ID_AA64PFR1_EL1_raw(pfr1);
		break;
	}
	case ISS_MRS_MSR_ID_AA64ISAR0_EL1: {
		ID_AA64ISAR0_EL1_t isar0 = ID_AA64ISAR0_EL1_default();
#if defined(ARCH_ARM_FEAT_PMULL)
		ID_AA64ISAR0_EL1_set_AES(&isar0, 2U);
#elif defined(ARCH_ARM_FEAT_AES)
		ID_AA64ISAR0_EL1_set_AES(&isar0, 1U);
#endif
#if defined(ARCH_ARM_FEAT_SHA1)
		ID_AA64ISAR0_EL1_set_SHA1(&isar0, 1U);
#endif
#if defined(ARCH_ARM_FEAT_SHA512)
		ID_AA64ISAR0_EL1_set_SHA2(&isar0, 2U);
#elif defined(ARCH_ARM_FEAT_SHA256)
		ID_AA64ISAR0_EL1_set_SHA2(&isar0, 1U);
#endif
#if defined(ARCH_ARM_FEAT_CRC32)
		ID_AA64ISAR0_EL1_set_CRC32(&isar0, 1U);
#endif
#if defined(ARCH_ARM_FEAT_VHE)
		ID_AA64ISAR0_EL1_set_Atomic(&isar0, 2U);
#endif
#if defined(ARCH_ARM_FEAT_RDM)
		ID_AA64ISAR0_EL1_set_RDM(&isar0, 1U);
#endif
#if defined(ARCH_ARM_FEAT_SHA3)
		ID_AA64ISAR0_EL1_set_SHA3(&isar0, 1U);
#endif
#if defined(ARCH_ARM_FEAT_SM3)
		ID_AA64ISAR0_EL1_set_SM3(&isar0, 1U);
#endif
#if defined(ARCH_ARM_FEAT_SM4)
		ID_AA64ISAR0_EL1_set_SM4(&isar0, 1U);
#endif
#if defined(ARCH_ARM_FEAT_DotProd)
		ID_AA64ISAR0_EL1_set_DP(&isar0, 1U);
#endif
#if defined(ARCH_ARM_FEAT_FHM)
		ID_AA64ISAR0_EL1_set_FHM(&isar0, 1U);
#endif
#if defined(ARCH_ARM_FEAT_FlagM2)
		ID_AA64ISAR0_EL1_set_TS(&isar0, 2U);
#elif defined(ARCH_ARM_FEAT_FlagM)
		ID_AA64ISAR0_EL1_set_TS(&isar0, 1U);
#endif
#if defined(ARCH_ARM_FEAT_TLBIRANGE)
		ID_AA64ISAR0_EL1_set_TLB(&isar0, 2U);
#elif defined(ARCH_ARM_FEAT_TLBIOS)
		ID_AA64ISAR0_EL1_set_TLB(&isar0, 1U);
#endif
#if defined(ARCH_ARM_FEAT_RNG)
		ID_AA64ISAR0_EL1_set_RNDR(&isar0, 2U);
#endif
		reg_val = ID_AA64ISAR0_EL1_raw(isar0);
		break;
	}
	case ISS_MRS_MSR_ID_AA64ISAR1_EL1: {
		ID_AA64ISAR1_EL1_t isar1    = ID_AA64ISAR1_EL1_default();
		ID_AA64ISAR1_EL1_t hw_isar1 = register_ID_AA64ISAR1_EL1_read();
		(void)hw_isar1;
#if defined(ARCH_ARM_FEAT_DPB2)
		ID_AA64ISAR1_EL1_set_DPB(&isar1, 2U);
#elif defined(ARCH_ARM_FEAT_DPB)
		ID_AA64ISAR1_EL1_set_DPB(&isar1, 1U);
#endif
#if defined(ARCH_ARM_FEAT_JSCVT)
		ID_AA64ISAR1_EL1_set_JSCVT(&isar1, 1U);
#endif
#if defined(ARCH_ARM_FEAT_FCMA)
		ID_AA64ISAR1_EL1_set_FCMA(&isar1, 1U);
#endif
#if defined(ARCH_ARM_FEAT_LRCPC2)
		ID_AA64ISAR1_EL1_set_LRCPC(&isar1, 2U);
#elif defined(ARCH_ARM_FEAT_LRCPC)
		ID_AA64ISAR1_EL1_set_LRCPC(&isar1, 1U);
#endif
#if defined(ARCH_ARM_FEAT_FRINTTS)
		ID_AA64ISAR1_EL1_set_FRINTTS(&isar1, 1U);
#endif
#if defined(ARCH_ARM_FEAT_SB)
		ID_AA64ISAR1_EL1_set_SB(&isar1, 1U);
#endif
#if defined(ARCH_ARM_FEAT_SPECRES)
		ID_AA64ISAR1_EL1_set_SPECRES(&isar1, 1U);
#endif
#if defined(ARCH_ARM_FEAT_PAuth)
		ID_AA64ISAR1_EL1_copy_APA(&isar1, &hw_isar1);
		ID_AA64ISAR1_EL1_copy_API(&isar1, &hw_isar1);
		ID_AA64ISAR1_EL1_copy_GPA(&isar1, &hw_isar1);
		ID_AA64ISAR1_EL1_copy_GPI(&isar1, &hw_isar1);
#endif
#if defined(ARCH_ARM_FEAT_DGH)
		ID_AA64ISAR1_EL1_set_DGH(&isar1, 1U);
#endif
#if defined(ARCH_ARM_FEAT_BF16)
		ID_AA64ISAR1_EL1_set_BF16(&isar1, 1U);
#endif
#if defined(ARCH_ARM_FEAT_I8MM)
		ID_AA64ISAR1_EL1_set_I8MM(&isar1, 1U);
#endif
		reg_val = ID_AA64ISAR1_EL1_raw(isar1);
		break;
	}
	case ISS_MRS_MSR_ID_AA64ISAR2_EL1: {
		ID_AA64ISAR2_EL1_t isar2    = ID_AA64ISAR2_EL1_default();
		ID_AA64ISAR2_EL1_t hw_isar2 = register_ID_AA64ISAR2_EL1_read();
		(void)hw_isar2;
#if defined(ARCH_ARM_FEAT_PAuth)
		ID_AA64ISAR2_EL1_copy_APA3(&isar2, &hw_isar2);
		ID_AA64ISAR2_EL1_copy_GPA3(&isar2, &hw_isar2);
		ID_AA64ISAR2_EL1_copy_PAC_frac(&isar2, &hw_isar2);
#endif
#if defined(ARCH_ARM_FEAT_CLRBHB)
		ID_AA64ISAR2_EL1_copy_CLRBHB(&isar2, &hw_isar2);
#endif
#if defined(ARCH_ARM_FEAT_WFxT)
		// Copy the hardware values across once FEAT_WFxT is implemented
		// FIXME:
		// ID_AA64ISAR2_EL1_copy_WFxT(&isar2, &hw_isar2);
#endif

		reg_val = ID_AA64ISAR2_EL1_raw(isar2);
		break;
	}

	case ISS_MRS_MSR_ID_AA64MMFR0_EL1: {
		ID_AA64MMFR0_EL1_t mmfr0 = ID_AA64MMFR0_EL1_default();

		// FIXME: match PLATFORM_VM_ADDRESS_SPACE_BITS
		ID_AA64MMFR0_EL1_set_PARange(&mmfr0, TCR_PS_SIZE_36BITS);
#if defined(ARCH_AARCH64_ASID16)
		ID_AA64MMFR0_EL1_set_ASIDBits(&mmfr0, 2U);
#endif
		ID_AA64MMFR0_EL1_set_SNSMem(&mmfr0, 1U);
#if defined(ARCH_AARCH64_BIG_END_ALL_CORES) && ARCH_AARCH64_BIG_END_ALL_CORES
		ID_AA64MMFR0_EL1_set_BigEnd(&mmfr0, 1U);
		ID_AA64MMFR0_EL1_set_BigEndEL0(&mmfr0, 0U);
#elif defined(ARCH_AARCH64_BIG_END_EL0_ALL_CORES) &&                           \
	ARCH_AARCH64_BIG_END_EL0_ALL_CORES
		ID_AA64MMFR0_EL1_set_BigEnd(&mmfr0, 0U);
		ID_AA64MMFR0_EL1_set_BigEndEL0(&mmfr0, 1U);
#endif
		ID_AA64MMFR0_EL1_set_TGran4(&mmfr0, 0U);
		ID_AA64MMFR0_EL1_set_TGran16(&mmfr0, 0U);
		ID_AA64MMFR0_EL1_set_TGran64(&mmfr0, 0xfU);
#if defined(ARCH_ARM_FEAT_ExS)
		ID_AA64MMFR0_EL1_set_ExS(&mmfr0, 1U);
#endif
#if defined(ARCH_ARM_FEAT_ECV)
		ID_AA64MMFR0_EL1_set_ECV(&mmfr0, 1U);
#endif

		reg_val = ID_AA64MMFR0_EL1_raw(mmfr0);
		break;
	}
	case ISS_MRS_MSR_ID_AA64MMFR1_EL1: {
		ID_AA64MMFR1_EL1_t hw_mmfr1 = register_ID_AA64MMFR1_EL1_read();
		ID_AA64MMFR1_EL1_t mmfr1    = ID_AA64MMFR1_EL1_default();
#if defined(ARCH_ARM_FEAT_HPDS2)
		ID_AA64MMFR1_EL1_set_HPDS(&mmfr1, 2U);
#elif defined(ARCH_ARM_FEAT_HPDS)
		ID_AA64MMFR1_EL1_set_HPDS(&mmfr1, 1U);
#endif
#if defined(ARCH_ARM_FEAT_HAFDBS)
		ID_AA64MMFR1_EL1_copy_HAFDBS(&mmfr1, &hw_mmfr1);
#endif
#if defined(ARCH_ARM_FEAT_VMID16)
		ID_AA64MMFR1_EL1_set_VMIDBits(&mmfr1, 2U);
#endif
#if defined(ARCH_ARM_FEAT_VHE)
		ID_AA64MMFR1_EL1_set_VH(&mmfr1, 1U);
#endif
#if defined(ARCH_ARM_FEAT_LOR)
		ID_AA64MMFR1_EL1_set_LO(&mmfr1, 1U);
#endif
#if defined(ARCH_ARM_FEAT_PAN3)
		ID_AA64MMFR1_EL1_set_PAN(&mmfr1, 3U);
#elif defined(ARCH_ARM_FEAT_PAN2) // now known as FEAT_PAN2
		ID_AA64MMFR1_EL1_set_PAN(&mmfr1, 2U);
#elif defined(ARCH_ARM_FEAT_PAN)
		ID_AA64MMFR1_EL1_set_PAN(&mmfr1, 1U);
#endif
#if defined(ARCH_ARM_FEAT_XNX)
		ID_AA64MMFR1_EL1_set_XNX(&mmfr1, 1U);
#endif
#if defined(ARCH_ARM_FEAT_TWED)
		ID_AA64MMFR1_EL1_set_TWED(&mmfr1, 1U);
#endif
#if defined(ARCH_ARM_FEAT_ETS)
		ID_AA64MMFR1_EL1_set_ETS(&mmfr1, 1U);
#endif
#if defined(ARCH_ARM_FEAT_HCX)
		ID_AA64MMFR1_EL1_set_HCX(&mmfr1, 1U);
#endif
#if defined(ARCH_ARM_FEAT_AFP)
		ID_AA64MMFR1_EL1_set_AFP(&mmfr1, 1U);
#endif
#if defined(ARCH_ARM_FEAT_nTLBPA)
		ID_AA64MMFR1_EL1_set_nTLBPAwAFP(&mmfr1, 1U);
#endif
#if defined(ARCH_ARM_FEAT_TIDCP1)
		ID_AA64MMFR1_EL1_set_TIDCP1(&mmfr1, 1U);
#endif
#if defined(ARCH_ARM_FEAT_CMOW)
		ID_AA64MMFR1_EL1_set_CMOW(&mmfr1, 1U);
#endif
#if defined(ARCH_ARM_FEAT_ECBHB)
		ID_AA64MMFR1_EL1_copy_ECBHB(&mmfr1, &hw_mmfr1);
#endif
		reg_val = ID_AA64MMFR1_EL1_raw(mmfr1);
		(void)hw_mmfr1;
		break;
	}
	case ISS_MRS_MSR_ID_AA64MMFR2_EL1: {
		ID_AA64MMFR2_EL1_t mmfr2 = ID_AA64MMFR2_EL1_default();
#if defined(ARCH_ARM_FEAT_TTCNP)
		ID_AA64MMFR2_EL1_set_CnP(&mmfr2, 1U);
#endif
#if defined(ARCH_ARM_FEAT_UAO)
		ID_AA64MMFR2_EL1_set_UAO(&mmfr2, 1U);
#endif
#if defined(ARCH_ARM_FEAT_LSMAOC)
		ID_AA64MMFR2_EL1_set_LSM(&mmfr2, 1U);
#endif
#if defined(ARCH_ARM_FEAT_IESB)
		ID_AA64MMFR2_EL1_set_IESB(&mmfr2, 1U);
#endif
#if defined(ARCH_ARM_FEAT_LVA)
		ID_AA64MMFR2_EL1_set_VARange(&mmfr2, 1U);
#endif
#if defined(ARCH_ARM_FEAT_CCIDX)
		ID_AA64MMFR2_EL1_set_CCIDX(&mmfr2, 1U);
#endif
#if defined(ARCH_ARM_FEAT_NV2)
		ID_AA64MMFR2_EL1_set_NV(&mmfr2, 2U);
#elif defined(ARCH_ARM_FEAT_NV)
		ID_AA64MMFR2_EL1_set_NV(&mmfr2, 1U);
#endif
#if defined(ARCH_ARM_FEAT_TTST)
		ID_AA64MMFR2_EL1_set_ST(&mmfr2, 1U);
#endif
#if defined(ARCH_ARM_FEAT_LSE2)
		ID_AA64MMFR2_EL1_set_AT(&mmfr2, 1U);
#endif
#if defined(ARCH_ARM_FEAT_IDST)
		ID_AA64MMFR2_EL1_set_IDS(&mmfr2, 1U);
#endif
#if defined(ARCH_ARM_FEAT_SEL2)
		ID_AA64MMFR2_EL1_set_FWB(&mmfr2, 1U);
#endif
#if defined(ARCH_ARM_FEAT_TTL)
		ID_AA64MMFR2_EL1_set_IDS(&mmfr2, 1U);
#endif
#if defined(ARCH_ARM_FEAT_BBM) || defined(ARCH_ARM_FEAT_EVT)
		ID_AA64MMFR2_EL1_t hw_mmfr2 = register_ID_AA64MMFR2_EL1_read();
#if defined(ARCH_ARM_FEAT_BBM)
		ID_AA64MMFR2_EL1_copy_BBM(&mmfr2, &hw_mmfr2);
#endif
#if defined(ARCH_ARM_FEAT_EVT)
		ID_AA64MMFR2_EL1_copy_EVT(&mmfr2, &hw_mmfr2);
#endif
#endif
#if defined(ARCH_ARM_FEAT_E0PD)
		ID_AA64MMFR2_EL1_set_E0PD(&mmfr2, 1U);
#endif
		reg_val = ID_AA64MMFR2_EL1_raw(mmfr2);
		break;
	}
	case ISS_MRS_MSR_ID_AA64MMFR3_EL1: {
		ID_AA64MMFR3_EL1_t mmfr3    = ID_AA64MMFR3_EL1_default();
		ID_AA64MMFR3_EL1_t hw_mmfr3 = register_ID_AA64MMFR3_EL1_read();
		ID_AA64MMFR3_EL1_copy_Spec_FPACC(&mmfr3, &hw_mmfr3);
		reg_val = ID_AA64MMFR3_EL1_raw(mmfr3);
		break;
	}
	case ISS_MRS_MSR_ID_AA64MMFR4_EL1:
		reg_val = 0;
		break;
	case ISS_MRS_MSR_ID_PFR0_EL1: {
		ID_PFR0_EL1_t pfr0 = ID_PFR0_EL1_default();
		ID_PFR0_EL1_set_State0(&pfr0, 1U);
		ID_PFR0_EL1_set_State1(&pfr0, 3U);
		ID_PFR0_EL1_set_State2(&pfr0, 1U);
#if defined(ARCH_ARM_HAVE_SCXT) &&                                             \
	(defined(ARCH_ARM_FEAT_CSV2_2) || defined(ARCH_ARM_FEAT_CSV2_3))
		if (vcpu_runtime_flags_get_scxt_allowed(&thread->vcpu_flags)) {
			// At the time of writing, ARM does not have CSV2_3
			// encoding for ID_PFR0_EL1.CSV2
			ID_PFR0_EL1_set_CSV2(&pfr0, 2U);
		} else {
			ID_PFR0_EL1_set_CSV2(&pfr0, 1U);
		}
#elif defined(ARCH_ARM_FEAT_CSV2)
		ID_PFR0_EL1_set_CSV2(&pfr0, 1U);
#endif
#if defined(ARCH_ARM_FEAT_DIT)
		ID_PFR0_EL1_set_DIT(&pfr0, 1U);
#endif
		if (vcpu_option_flags_get_ras_error_handler(
			    &thread->vcpu_options)) {
#if defined(ARCH_ARM_FEAT_RASv1p1)
			ID_PFR0_EL1_set_RAS(&pfr0, 2);
#elif defined(ARCH_ARM_FEAT_RAS)
			ID_PFR0_EL1_set_RAS(&pfr0, 1);
#else
			// Nothing to do, the field is already 0
#endif
		}
		reg_val = ID_PFR0_EL1_raw(pfr0);
		break;
	}
	case ISS_MRS_MSR_ID_PFR1_EL1: {
		ID_PFR1_EL1_t pfr1 = ID_PFR1_EL1_default();
#if ARCH_AARCH64_32BIT_EL1
		ID_PFR1_EL1_set_ProgMod(&pfr1, 1U);
		ID_PFR1_EL1_set_Security(&pfr1, 1U);
		ID_PFR1_EL1_set_Virtualization(&pfr1, 1U);
#endif
		ID_PFR1_EL1_set_GenTimer(&pfr1, 1U);
		ID_PFR1_EL1_set_GIC(&pfr1, 1U);
		reg_val = ID_PFR1_EL1_raw(pfr1);
		break;
	}
	case ISS_MRS_MSR_ID_PFR2_EL1: {
		ID_PFR2_EL1_t pfr2 = ID_PFR2_EL1_default();
#if defined(ARCH_ARM_FEAT_CSV3)
		ID_PFR2_EL1_set_CSV3(&pfr2, 1U);
#endif
#if defined(ARCH_ARM_FEAT_SSBS)
		ID_PFR2_EL1_set_SSBS(&pfr2, 1U);
#endif
		reg_val = ID_PFR2_EL1_raw(pfr2);
		break;
	}
	case ISS_MRS_MSR_ID_DFR0_EL1: {
		ID_DFR0_EL1_t hw_dfr0 = ID_DFR0_EL1_default();
		ID_DFR0_EL1_t dfr0    = register_ID_DFR0_EL1_read();

		// The debug, trace, PMU and SPE modules must correctly support
		// the values reported by the hardware. All we do here is to
		// zero out fields for features we don't support.

#if defined(MODULE_VM_VDEBUG)
		ID_DFR0_EL1_copy_CopDbg(&dfr0, &hw_dfr0);
		ID_DFR0_EL1_copy_CopSDbg(&dfr0, &hw_dfr0);
		ID_DFR0_EL1_copy_MMapDbg(&dfr0, &hw_dfr0);
#endif
#if defined(MODULE_PLATFORM_ARM_PMU)
		ID_DFR0_EL1_copy_PerfMon(&dfr0, &hw_dfr0);
#endif

		reg_val = ID_DFR0_EL1_raw(dfr0);
		break;
	}
	case ISS_MRS_MSR_ID_ISAR0_EL1: {
		ID_ISAR0_EL1_t isar0 = ID_ISAR0_EL1_default();
		ID_ISAR0_EL1_set_BitCount(&isar0, 1U);
		ID_ISAR0_EL1_set_BitField(&isar0, 1U);
		ID_ISAR0_EL1_set_CmpBranch(&isar0, 1U);
		ID_ISAR0_EL1_set_Divide(&isar0, 2U);
		reg_val = ID_ISAR0_EL1_raw(isar0);
		break;
	}
	case ISS_MRS_MSR_ID_ISAR1_EL1: {
		ID_ISAR1_EL1_t isar1 = ID_ISAR1_EL1_default();
		ID_ISAR1_EL1_set_Except(&isar1, 1U);
		ID_ISAR1_EL1_set_Except_AR(&isar1, 1U);
		ID_ISAR1_EL1_set_Extend(&isar1, 2U);
		ID_ISAR1_EL1_set_IfThen(&isar1, 1U);
		ID_ISAR1_EL1_set_Immediate(&isar1, 1U);
		ID_ISAR1_EL1_set_Interwork(&isar1, 3U);
		ID_ISAR1_EL1_set_Jazelle(&isar1, 1U);
		reg_val = ID_ISAR1_EL1_raw(isar1);
		break;
	}
	case ISS_MRS_MSR_ID_ISAR2_EL1: {
		ID_ISAR2_EL1_t isar2 = ID_ISAR2_EL1_default();
		ID_ISAR2_EL1_set_LoadStore(&isar2, 2U);
		ID_ISAR2_EL1_set_MemHint(&isar2, 4U);
		ID_ISAR2_EL1_set_MultiAccessInt(&isar2, 0U);
		ID_ISAR2_EL1_set_Mult(&isar2, 2U);
		ID_ISAR2_EL1_set_MultS(&isar2, 3U);
		ID_ISAR2_EL1_set_MultU(&isar2, 2U);
		ID_ISAR2_EL1_set_PSR_AR(&isar2, 1U);
		ID_ISAR2_EL1_set_Reversal(&isar2, 2U);
		reg_val = ID_ISAR2_EL1_raw(isar2);
		break;
	}
	case ISS_MRS_MSR_ID_ISAR3_EL1: {
		ID_ISAR3_EL1_t isar3 = ID_ISAR3_EL1_default();
		ID_ISAR3_EL1_set_Saturate(&isar3, 1U);
		ID_ISAR3_EL1_set_SIMD(&isar3, 3U);
		ID_ISAR3_EL1_set_SVC(&isar3, 1U);
		ID_ISAR3_EL1_set_SynchPrim(&isar3, 2U);
		ID_ISAR3_EL1_set_TabBranch(&isar3, 1U);
		ID_ISAR3_EL1_set_T32Copy(&isar3, 1U);
		ID_ISAR3_EL1_set_TrueNOP(&isar3, 1U);
		reg_val = ID_ISAR3_EL1_raw(isar3);
		break;
	}
	case ISS_MRS_MSR_ID_ISAR4_EL1: {
		ID_ISAR4_EL1_t isar4 = ID_ISAR4_EL1_default();
		ID_ISAR4_EL1_set_Unpriv(&isar4, 2U);
		ID_ISAR4_EL1_set_WithShifts(&isar4, 4U);
		ID_ISAR4_EL1_set_Writeback(&isar4, 1U);
#if ARCH_AARCH64_32BIT_EL1
		ID_ISAR4_EL1_set_SMC(&isar4, 1U);
#endif
		ID_ISAR4_EL1_set_Barrier(&isar4, 1U);
		reg_val = ID_ISAR4_EL1_raw(isar4);
		break;
	}
	case ISS_MRS_MSR_ID_ISAR5_EL1: {
		ID_ISAR5_EL1_t isar5 = ID_ISAR5_EL1_default();
		ID_ISAR5_EL1_set_SEVL(&isar5, 1U);
#if defined(ARCH_ARM_FEAT_PMULL)
		ID_ISAR5_EL1_set_AES(&isar5, 2U);
#elif defined(ARCH_ARM_FEAT_AES)
		ID_ISAR5_EL1_set_AES(&isar5, 1U);
#endif
#if defined(ARCH_ARM_FEAT_SHA1)
		ID_ISAR5_EL1_set_SHA1(&isar5, 1U);
		ID_ISAR5_EL1_set_SHA2(&isar5, 1U);
#endif
#if defined(ARCH_ARM_FEAT_CRC32)
		ID_ISAR5_EL1_set_CRC32(&isar5, 1U);
#endif
#if defined(ARCH_ARM_FEAT_RDM)
		ID_ISAR5_EL1_set_RDM(&isar5, 1U);
#endif
#if defined(ARCH_ARM_FEAT_FCMA)
		ID_ISAR5_EL1_set_VCMA(&isar5, 2U);
#endif
		reg_val = ID_ISAR5_EL1_raw(isar5);
		break;
	}
	case ISS_MRS_MSR_ID_ISAR6_EL1: {
		ID_ISAR6_EL1_t isar6 = ID_ISAR6_EL1_default();
#if defined(ARCH_ARM_FEAT_JSCVT)
		ID_ISAR6_EL1_set_JSCVT(&isar6, 1U);
#endif
#if defined(ARCH_ARM_FEAT_DotProd)
		ID_ISAR6_EL1_set_DP(&isar6, 1U);
#endif
#if defined(ARCH_ARM_FEAT_FHM)
		ID_ISAR6_EL1_set_FHM(&isar6, 1U);
#endif
#if defined(ARCH_ARM_FEAT_SB)
		ID_ISAR6_EL1_set_SB(&isar6, 1U);
#endif
#if defined(ARCH_ARM_FEAT_SPECRES)
		ID_ISAR6_EL1_set_SPECRES(&isar6, 1U);
#endif
		reg_val = ID_ISAR6_EL1_raw(isar6);
		break;
	}
	case ISS_MRS_MSR_ID_MMFR0_EL1: {
		ID_MMFR0_EL1_t mmfr0 = ID_MMFR0_EL1_default();
		ID_MMFR0_EL1_set_VMSA(&mmfr0, 5U);
		ID_MMFR0_EL1_set_OuterShr(&mmfr0, 1U);
		ID_MMFR0_EL1_set_ShareLvl(&mmfr0, 1U);
		ID_MMFR0_EL1_set_AuxReg(&mmfr0, 2U);
		ID_MMFR0_EL1_set_InnerShr(&mmfr0, 1U);
		reg_val = ID_MMFR0_EL1_raw(mmfr0);
		break;
	}
	case ISS_MRS_MSR_ID_MMFR1_EL1: {
		ID_MMFR1_EL1_t mmfr1 = ID_MMFR1_EL1_default();
		ID_MMFR1_EL1_set_BPred(&mmfr1, 4U);
		reg_val = ID_MMFR1_EL1_raw(mmfr1);
		break;
	}
	case ISS_MRS_MSR_ID_MMFR2_EL1: {
		ID_MMFR2_EL1_t mmfr2 = ID_MMFR2_EL1_default();
		ID_MMFR2_EL1_set_UniTLB(&mmfr2, 6U);
		ID_MMFR2_EL1_set_MemBarr(&mmfr2, 2U);
		ID_MMFR2_EL1_set_WFIStall(&mmfr2, 1U);
		reg_val = ID_MMFR2_EL1_raw(mmfr2);
		break;
	}
	case ISS_MRS_MSR_ID_MMFR3_EL1: {
		ID_MMFR3_EL1_t mmfr3 = ID_MMFR3_EL1_default();
		ID_MMFR3_EL1_set_CMaintVA(&mmfr3, 1U);
		ID_MMFR3_EL1_set_CMaintSW(&mmfr3, 1U);
		ID_MMFR3_EL1_set_BPMaint(&mmfr3, 2U);
		ID_MMFR3_EL1_set_MaintBcst(&mmfr3, 2U);
#if defined(ARCH_ARM_FEAT_PAN3)
		ID_MMFR3_EL1_set_PAN(&mmfr3, 3U);
#elif defined(ARCH_ARM_FEAT_PAN2) // now known as FEAT_PAN2
		ID_MMFR3_EL1_set_PAN(&mmfr3, 2U);
#elif defined(ARCH_ARM_FEAT_PAN)
		ID_MMFR3_EL1_set_PAN(&mmfr3, 1U);
#endif
		ID_MMFR3_EL1_set_CohWalk(&mmfr3, 1U);
		ID_MMFR3_EL1_set_CMemSz(&mmfr3, 2U);
		reg_val = ID_MMFR3_EL1_raw(mmfr3);
		break;
	}
	case ISS_MRS_MSR_ID_MMFR4_EL1: {
		ID_MMFR4_EL1_t mmfr4 = ID_MMFR4_EL1_default();
		ID_MMFR4_EL1_set_AC2(&mmfr4, 1U);
#if defined(ARCH_ARM_FEAT_XNX)
		ID_MMFR4_EL1_set_XNX(&mmfr4, 1U);
#endif
#if defined(ARCH_ARM_FEAT_TTCNP)
		ID_MMFR4_EL1_set_CnP(&mmfr4, 1U);
#endif
#if defined(ARCH_ARM_FEAT_HPDS2)
		ID_MMFR4_EL1_set_HPDS(&mmfr4, 2U);
#elif defined(ARCH_ARM_FEAT_AA32HPD)
		ID_MMFR4_EL1_set_HPDS(&mmfr4, 1U);
#endif
#if defined(ARCH_ARM_FEAT_LSMAOC)
		ID_MMFR4_EL1_set_LSM(&mmfr4, 1U);
#endif
#if defined(ARCH_ARM_FEAT_CCIDX)
		ID_MMFR4_EL1_set_CCIDX(&mmfr4, 1U);
#endif
#if defined(ARCH_ARM_FEAT_EVT)
		ID_MMFR4_EL1_t hw_mmfr4 = register_ID_MMFR4_EL1_read();
		ID_MMFR4_EL1_copy_EVT(&mmfr4, &hw_mmfr4);
#endif
		reg_val = ID_MMFR4_EL1_raw(mmfr4);
		break;
	}
	case ISS_MRS_MSR_ID_AA64DFR1_EL1:
	case ISS_MRS_MSR_ID_AA64AFR0_EL1:
	case ISS_MRS_MSR_ID_AA64AFR1_EL1:
	case ISS_MRS_MSR_ID_AFR0_EL1:
	case ISS_MRS_MSR_ID_AA64SMFR0_EL1:
		// RAZ
		break;
	default:
		handled = false;
		break;
	}

	if (handled) {
		vcpu_gpr_write(thread, reg_num, reg_val);
	}

	return handled;
}
#endif

static vcpu_trap_result_t
default_sys_read(const ESR_EL2_ISS_MSR_MRS_t *iss, register_t *reg_val_ptr)
{
	vcpu_trap_result_t ret	   = VCPU_TRAP_RESULT_EMULATED;
	register_t	   reg_val = 0ULL;

	uint8_t opc0, opc1, crn, crm;

	opc0 = ESR_EL2_ISS_MSR_MRS_get_Op0(iss);
	opc1 = ESR_EL2_ISS_MSR_MRS_get_Op1(iss);
	crn  = ESR_EL2_ISS_MSR_MRS_get_CRn(iss);
	crm  = ESR_EL2_ISS_MSR_MRS_get_CRm(iss);

	if ((opc0 == 3U) && (opc1 == 0U) && (crn == 0U) && (crm >= 1U) &&
	    (crm <= 7U)) {
		// It is IMPLEMENTATION DEFINED whether HCR_EL2.TID3
		// traps MRS accesses to the registers in this range
		// (that have not been handled above). If we ever get
		// here print a debug message so we can investigate.
		TRACE_AND_LOG(DEBUG, DEBUG,
			      "Emulated RAZ for ID register: ISS {:#x}",
			      ESR_EL2_ISS_MSR_MRS_raw(*iss));
		reg_val = 0U;
	} else {
		ret = VCPU_TRAP_RESULT_UNHANDLED;
	}

	*reg_val_ptr = reg_val;
	return ret;
}

static register_t
sys_aa64mmfr3_read(void)
{
	register_t reg_val = 0ULL;

	ID_AA64MMFR3_EL1_t mmfr3    = ID_AA64MMFR3_EL1_default();
	ID_AA64MMFR3_EL1_t hw_mmfr3 = register_ID_AA64MMFR3_EL1_read();
	ID_AA64MMFR3_EL1_copy_Spec_FPACC(&mmfr3, &hw_mmfr3);
	reg_val = ID_AA64MMFR3_EL1_raw(mmfr3);

	return reg_val;
}

static register_t
sys_aa64mmfr2_read(void)
{
	register_t reg_val = 0ULL;

	ID_AA64MMFR2_EL1_t mmfr2 = register_ID_AA64MMFR2_EL1_read();

	mmfr2 = ID_AA64MMFR2_EL1_clean(mmfr2);

	reg_val = ID_AA64MMFR2_EL1_raw(mmfr2);

	return reg_val;
}

static register_t
sys_aa64mmfr1_read(void)
{
	register_t reg_val = 0ULL;

	ID_AA64MMFR1_EL1_t mmfr1 = register_ID_AA64MMFR1_EL1_read();

	mmfr1 = ID_AA64MMFR1_EL1_clean(mmfr1);

#if defined(ARCH_ARM_FEAT_PAN3)
	assert(ID_AA64MMFR1_EL1_get_PAN(&mmfr1) >= 3U);
	ID_AA64MMFR1_EL1_set_PAN(&mmfr1, 3U);
#elif defined(ARCH_ARM_FEAT_PAN2) // now known as FEAT_PAN2
	assert(ID_AA64MMFR1_EL1_get_PAN(&mmfr1) >= 2U);
	ID_AA64MMFR1_EL1_set_PAN(&mmfr1, 2U);
#elif defined(ARCH_ARM_FEAT_PAN)
	assert(ID_AA64MMFR1_EL1_get_PAN(&mmfr1) >= 1U);
	ID_AA64MMFR1_EL1_set_PAN(&mmfr1, 1U);
#else
	ID_AA64MMFR1_EL1_set_PAN(&mmfr1, 0U);
#endif
	reg_val = ID_AA64MMFR1_EL1_raw(mmfr1);

	return reg_val;
}

static register_t
sys_aa64mmfr0_read(void)
{
	register_t reg_val = 0ULL;

	ID_AA64MMFR0_EL1_t mmfr0 = register_ID_AA64MMFR0_EL1_read();

	mmfr0 = ID_AA64MMFR0_EL1_clean(mmfr0);

	reg_val = ID_AA64MMFR0_EL1_raw(mmfr0);

	return reg_val;
}

static register_t
sys_aa64isar2_read(void)
{
	register_t reg_val = 0ULL;

	ID_AA64ISAR2_EL1_t isar2 = register_ID_AA64ISAR2_EL1_read();

	isar2 = ID_AA64ISAR2_EL1_clean(isar2);

#if !defined(ARCH_ARM_FEAT_PAuth)
	// When PAUTH using QARMA3 is disabled, hide it from the VM
	ID_AA64ISAR2_EL1_set_APA3(&isar2, 0U);
	ID_AA64ISAR2_EL1_set_GPA3(&isar2, 0U);
	ID_AA64ISAR2_EL1_set_PAC_frac(&isar2, 0U);
#endif
#if defined(ARCH_ARM_FEAT_WFxT)
	// Remove once FEAT_WFxT is implemented
	// FIXME:
	ID_AA64ISAR2_EL1_set_WFxT(&isar2, 0U);
#endif
	reg_val = ID_AA64ISAR2_EL1_raw(isar2);

	return reg_val;
}

static register_t
sys_aa64isar1_read(void)
{
	register_t reg_val = 0ULL;

	ID_AA64ISAR1_EL1_t isar1 = register_ID_AA64ISAR1_EL1_read();

	isar1 = ID_AA64ISAR1_EL1_clean(isar1);
#if !defined(ARCH_ARM_FEAT_BF16)
	ID_AA64ISAR1_EL1_set_BF16(&isar1, 0U);
#endif
#if !defined(ARCH_ARM_FEAT_PAuth)
	// When no PAUTH is enabled, hide it from the VM
	ID_AA64ISAR1_EL1_set_APA(&isar1, 0U);
	ID_AA64ISAR1_EL1_set_API(&isar1, 0U);
	ID_AA64ISAR1_EL1_set_GPA(&isar1, 0U);
	ID_AA64ISAR1_EL1_set_GPI(&isar1, 0U);
#endif
	reg_val = ID_AA64ISAR1_EL1_raw(isar1);

	return reg_val;
}

static register_t
sys_aa64isar0_read(void)
{
	register_t reg_val = 0ULL;

	ID_AA64ISAR0_EL1_t isar0 = register_ID_AA64ISAR0_EL1_read();

	isar0 = ID_AA64ISAR0_EL1_clean(isar0);

	reg_val = ID_AA64ISAR0_EL1_raw(isar0);

	return reg_val;
}

static register_t
sys_aa64dfr0_read(const thread_t *thread)
{
	register_t reg_val = 0ULL;

	ID_AA64DFR0_EL1_t dfr0	  = ID_AA64DFR0_EL1_default();
	ID_AA64DFR0_EL1_t hw_dfr0 = register_ID_AA64DFR0_EL1_read();

	// The debug, trace, PMU and SPE modules must correctly support
	// the values reported by the hardware. All we do here is to
	// zero out fields for missing modules.

#if defined(MODULE_VM_VDEBUG)
	// Note that ARMv8-A does not allow 0 (not implemented) in this
	// field. So without this module is not really supported.
	ID_AA64DFR0_EL1_copy_DebugVer(&dfr0, &hw_dfr0);

	ID_AA64DFR0_EL1_copy_BRPs(&dfr0, &hw_dfr0);
	ID_AA64DFR0_EL1_copy_WRPs(&dfr0, &hw_dfr0);
	ID_AA64DFR0_EL1_copy_CTX_CMPs(&dfr0, &hw_dfr0);
	ID_AA64DFR0_EL1_copy_DoubleLock(&dfr0, &hw_dfr0);
#endif
#if defined(MODULE_VM_ARM_VM_PMU)
	ID_AA64DFR0_EL1_copy_PMUVer(&dfr0, &hw_dfr0);
#endif
#if defined(INTERFACE_VET)
	// Set IDs for VMs allowed to trace
	if (vcpu_option_flags_get_trace_allowed(&thread->vcpu_options)) {
#if defined(MODULE_VM_VETE)
		ID_AA64DFR0_EL1_copy_TraceVer(&dfr0, &hw_dfr0);
		ID_AA64DFR0_EL1_copy_TraceFilt(&dfr0, &hw_dfr0);
#endif
#if defined(MODULE_VM_VTRBE)
		ID_AA64DFR0_EL1_copy_TraceBuffer(&dfr0, &hw_dfr0);
#endif
	}
#else
	(void)thread;
#endif

#if defined(MODULE_SPE)
	ID_AA64DFR0_EL1_copy_PMSVer(&dfr0, &hw_dfr0);
#endif

	reg_val = ID_AA64DFR0_EL1_raw(dfr0);

	return reg_val;
}

static register_t
sys_aa64pfr1_read(const thread_t *thread)
{
	register_t reg_val = 0ULL;

	ID_AA64PFR1_EL1_t pfr1 = register_ID_AA64PFR1_EL1_read();

	pfr1 = ID_AA64PFR1_EL1_clean(pfr1);
#if defined(ARCH_ARM_FEAT_MTE)
	if (!arm_mte_is_allowed()) {
		ID_AA64PFR1_EL1_set_MTE(&pfr1, 0);
	}
#else
	ID_AA64PFR1_EL1_set_MTE(&pfr1, 0);
#endif
#if defined(ARCH_ARM_FEAT_RAS) || defined(ARCH_ARM_FEAT_RASv1p1)
	if (!vcpu_option_flags_get_ras_error_handler(&thread->vcpu_options)) {
		ID_AA64PFR1_EL1_set_RAS_frac(&pfr1, 0);
	}
#else
	(void)thread;
#endif
#if defined(ARCH_ARM_HAVE_SCXT) && defined(ARCH_ARM_FEAT_CSV2_1p2)
	if (!vcpu_option_flags_get_scxt_allowed(&thread->vcpu_options)) {
		ID_AA64PFR1_EL1_set_CSV2_frac(&pfr1, 1U);
	}
#elif defined(ARCH_ARM_FEAT_CSV2_1p1)
	ID_AA64PFR1_EL1_set_CSV2_frac(&pfr1, 1U);
#else
	ID_AA64PFR1_EL1_set_CSV2_frac(&pfr1, 0U);
	(void)thread;
#endif

#if defined(ARCH_ARM_FEAT_MPAM)
	if (!arm_mpam_is_allowed() ||
	    !vcpu_option_flags_get_mpam_allowed(&thread->vcpu_options)) {
		// No MPAM
		ID_AA64PFR1_EL1_set_MPAM_frac(&pfr1, 0);
	}
#else
	// No MPAM
	ID_AA64PFR1_EL1_set_MPAM_frac(&pfr1, 0);
	(void)thread;
#endif
	// No SME / NMI
	ID_AA64PFR1_EL1_set_SME(&pfr1, 0);
	ID_AA64PFR1_EL1_set_NMI(&pfr1, 0);

	reg_val = ID_AA64PFR1_EL1_raw(pfr1);

	return reg_val;
}

static register_t
sys_aa64pfr0_read(const thread_t *thread)
{
	register_t reg_val = 0ULL;

	ID_AA64PFR0_EL1_t pfr0 = register_ID_AA64PFR0_EL1_read();

	pfr0 = ID_AA64PFR0_EL1_clean(pfr0);
#if !ARCH_AARCH64_32BIT_EL0
	// Require EL0 to be 64-bit only, even if core supports 32-bit
	ID_AA64PFR0_EL1_set_EL0(&pfr0, 1U);
#endif
#if !ARCH_AARCH64_32BIT_EL1
	// Require EL1 to be 64-bit only, even if core supports 32-bit
	ID_AA64PFR0_EL1_set_EL1(&pfr0, 1U);
#endif
	ID_AA64PFR0_EL1_set_EL2(&pfr0, 1U);
	ID_AA64PFR0_EL1_set_EL3(&pfr0, 1U);
#if defined(ARCH_ARM_HAVE_SCXT)
	if (!vcpu_runtime_flags_get_scxt_allowed(&thread->vcpu_flags)) {
		ID_AA64PFR0_EL1_set_CSV2(&pfr0, 1U);
	}
#elif defined(ARCH_ARM_FEAT_CSV2)
	ID_AA64PFR0_EL1_set_CSV2(&pfr0, 1U);
	(void)thread;
#else
	(void)thread;
#endif

#if defined(ARCH_ARM_FEAT_MPAM)
	if (!arm_mpam_is_allowed() ||
	    !vcpu_option_flags_get_mpam_allowed(&thread->vcpu_options)) {
		// No MPAM
		ID_AA64PFR0_EL1_set_MPAM(&pfr0, 0);
	}
#else
	// No MPAM
	ID_AA64PFR0_EL1_set_MPAM(&pfr0, 0);
	(void)thread;
#endif

#if defined(ARCH_ARM_FEAT_SVE)
	// Tell non-SVE allowed guests that there is no SVE
	if (!vcpu_option_flags_get_sve_allowed(&thread->vcpu_options)) {
		ID_AA64PFR0_EL1_set_SVE(&pfr0, 0);
	}
#else
	// No SVE
	ID_AA64PFR0_EL1_set_SVE(&pfr0, 0);
	(void)thread;
#endif

#if defined(ARCH_ARM_FEAT_RAS) || defined(ARCH_ARM_FEAT_RASv1p1)
	// Tell non-RAS handler guests there is no RAS
	if (!vcpu_option_flags_get_ras_error_handler(&thread->vcpu_options)) {
		ID_AA64PFR0_EL1_set_RAS(&pfr0, 0);
	}
#endif
#if defined(ARCH_ARM_FEAT_AMUv1) || defined(ARCH_ARM_FEAT_AMUv1p1)
	// Tell non-HLOS guests that there is no AMU
	if (!vcpu_option_flags_get_hlos_vm(&thread->vcpu_options)) {
		ID_AA64PFR0_EL1_set_AMU(&pfr0, 0);
	}
#else
	(void)thread;
#endif
#if !defined(ARCH_ARM_FEAT_SEL2)
	ID_AA64PFR0_EL1_set_SEL2(&pfr0, 0U);
#endif
	ID_AA64PFR0_EL1_set_RME(&pfr0, 0U);

	reg_val = ID_AA64PFR0_EL1_raw(pfr0);

	return reg_val;
}

static register_t
sys_mmfr3_read(void)
{
	register_t reg_val = 0ULL;
	sysreg64_read(ID_MMFR3_EL1, reg_val);
	ID_MMFR3_EL1_t mmfr1 = ID_MMFR3_EL1_cast(reg_val);
#if defined(ARCH_ARM_FEAT_PAN3)
	assert(ID_MMFR3_EL1_get_PAN(&mmfr1) >= 3U);
	ID_MMFR3_EL1_set_PAN(&mmfr1, 3U);
#elif defined(ARCH_ARM_FEAT_PAN2) // now known as FEAT_PAN2
	assert(ID_MMFR3_EL1_get_PAN(&mmfr1) >= 2U);
	ID_MMFR3_EL1_set_PAN(&mmfr1, 2U);
#elif defined(ARCH_ARM_FEAT_PAN)
	assert(ID_MMFR3_EL1_get_PAN(&mmfr1) >= 1U);
	ID_MMFR3_EL1_set_PAN(&mmfr1, 1U);
#else
	ID_MMFR3_EL1_set_PAN(&mmfr1, 0U);
#endif
	reg_val = ID_MMFR3_EL1_raw(mmfr1);

	return reg_val;
}

static register_t
sys_dfr0_read(const thread_t *thread)
{
	register_t    reg_val = 0ULL;
	ID_DFR0_EL1_t dfr0    = register_ID_DFR0_EL1_read();

	// The debug, trace, PMU and SPE modules must correctly support
	// the values reported by the hardware. All we do here is to
	// zero out fields for features we don't support.

#if !defined(MODULE_VM_VDEBUG)
	// Note that ARMv8-A does not allow 0 (not implemented) in the
	// CopDbg field. So this configuration is not really supported.
	ID_DFR0_EL1_set_CopDbg(&dfr0, 0U);
	ID_DFR0_EL1_set_CopSDbg(&dfr0, 0U);
	ID_DFR0_EL1_set_MMapDbg(&dfr0, 0U);
	ID_DFR0_EL1_set_MProfDbg(&dfr0, 0U);
#endif

#if defined(MODULE_VM_VETE)
	// Only the HLOS VM is allowed to trace
	if (!vcpu_option_flags_get_trace_allowed(&thread->vcpu_options)) {
		ID_DFR0_EL1_set_CopTrc(&dfr0, 0U);
		ID_DFR0_EL1_set_TraceFilt(&dfr0, 0U);
	}
#else
	ID_DFR0_EL1_set_CopTrc(&dfr0, 0U);
	ID_DFR0_EL1_set_TraceFilt(&dfr0, 0U);
	(void)thread;
#endif
#if defined(MODULE_VM_VETM)
	// Only the HLOS VM is allowed to trace
	if (!vcpu_option_flags_get_trace_allowed(&thread->vcpu_options)) {
		ID_DFR0_EL1_set_MMapTrc(&dfr0, 0U);
	}
#else
	ID_DFR0_EL1_set_MMapTrc(&dfr0, 0U);
	(void)thread;
#endif
#if !defined(MODULE_PLATFORM_ARM_PMU)
	ID_DFR0_EL1_set_PerfMon(&dfr0, 0U);
#endif

	reg_val = ID_DFR0_EL1_raw(dfr0);

	return reg_val;
}

static register_t
sys_pfr2_read(void)
{
	register_t    reg_val = 0ULL;
	ID_PFR2_EL1_t pfr2    = ID_PFR2_EL1_default();
#if defined(ARCH_ARM_FEAT_CSV3)
	ID_PFR2_EL1_set_CSV3(&pfr2, 1U);
#endif
#if defined(ARCH_ARM_FEAT_SSBS)
	ID_PFR2_EL1_set_SSBS(&pfr2, 1U);
#endif
	reg_val = ID_PFR2_EL1_raw(pfr2);

	return reg_val;
}

static register_t
sys_pfr1_read(void)
{
	register_t    reg_val = 0ULL;
	ID_PFR1_EL1_t pfr1    = register_ID_PFR1_EL1_read();

	reg_val = ID_PFR1_EL1_raw(pfr1);
	return reg_val;
}

static register_t
sys_pfr0_read(const thread_t *thread)
{
	register_t reg_val = 0ULL;

	ID_PFR0_EL1_t pfr0 = register_ID_PFR0_EL1_read();

#if defined(ARCH_ARM_FEAT_RAS) || defined(ARCH_ARM_FEAT_RASv1p1)
	// Tell non-RAS handler guests there is no RAS.
	if (!vcpu_option_flags_get_ras_error_handler(&thread->vcpu_options)) {
		ID_PFR0_EL1_set_RAS(&pfr0, 0);
	}
#else
	(void)thread;
#endif
#if defined(ARCH_ARM_FEAT_AMUv1) || defined(ARCH_ARM_FEAT_AMUv1p1)
	// Tell non-HLOS guests that there is no AMU
	if (!vcpu_option_flags_get_hlos_vm(&thread->vcpu_options)) {
		ID_PFR0_EL1_set_AMU(&pfr0, 0);
	}
#else
	(void)thread;
#endif
#if defined(ARCH_ARM_HAVE_SCXT)
	if (!vcpu_runtime_flags_get_scxt_allowed(&thread->vcpu_flags)) {
		ID_PFR0_EL1_set_CSV2(&pfr0, 1U);
	}
#elif defined(ARCH_ARM_FEAT_CSV2)
	ID_PFR0_EL1_set_CSV2(&pfr0, 1U);
	(void)thread;
#else
	(void)thread;
#endif

	reg_val = ID_PFR0_EL1_raw(pfr0);

	return reg_val;
}

// For the guests with no AMU access we should trap the AMU registers by setting
// CPTR_EL2.TAM and clearing ACTLR_EL2.AMEN. However the trapped registers
// should be handled in the AMU module, and not here.

vcpu_trap_result_t
sysreg_read(ESR_EL2_ISS_MSR_MRS_t iss)
{
	register_t	   reg_val = 0ULL; // Default action is RAZ
	vcpu_trap_result_t ret	   = VCPU_TRAP_RESULT_EMULATED;
	thread_t	  *thread  = thread_get_self();

	// Assert this is a read
	assert(ESR_EL2_ISS_MSR_MRS_get_Direction(&iss));

	uint8_t reg_num = ESR_EL2_ISS_MSR_MRS_get_Rt(&iss);

	// Remove the fields that are not used in the comparison
	ESR_EL2_ISS_MSR_MRS_t temp_iss = iss;
	ESR_EL2_ISS_MSR_MRS_set_Rt(&temp_iss, 0U);
	ESR_EL2_ISS_MSR_MRS_set_Direction(&temp_iss, false);

#if SCHEDULER_CAN_MIGRATE
	// If not pinned, use virtual ID register values.
	if (!vcpu_option_flags_get_pinned(&thread->vcpu_options) &&
	    read_virtual_id_register(temp_iss, reg_num)) {
		goto out;
	}
#endif

	switch (ESR_EL2_ISS_MSR_MRS_raw(temp_iss)) {
	// The registers trapped with HCR_EL2.TID3
	case ISS_MRS_MSR_ID_PFR0_EL1:
		reg_val = sys_pfr0_read(thread);
		break;
	case ISS_MRS_MSR_ID_PFR1_EL1:
		reg_val = sys_pfr1_read();
		break;
	case ISS_MRS_MSR_ID_PFR2_EL1:
		reg_val = sys_pfr2_read();
		break;
	case ISS_MRS_MSR_ID_DFR0_EL1:
		reg_val = sys_dfr0_read(thread);
		break;
	case ISS_MRS_MSR_ID_AFR0_EL1:
		// RES0 - We don't know any AFR0 bits
		break;
	case ISS_MRS_MSR_ID_MMFR0_EL1:
		sysreg64_read(ID_MMFR0_EL1, reg_val);
		break;
	case ISS_MRS_MSR_ID_MMFR1_EL1:
		sysreg64_read(ID_MMFR1_EL1, reg_val);
		break;
	case ISS_MRS_MSR_ID_MMFR2_EL1:
		sysreg64_read(ID_MMFR2_EL1, reg_val);
		break;
	case ISS_MRS_MSR_ID_MMFR3_EL1:
		reg_val = sys_mmfr3_read();
		break;
	case ISS_MRS_MSR_ID_MMFR4_EL1:
		sysreg64_read(ID_MMFR4_EL1, reg_val);
		break;
	case ISS_MRS_MSR_ID_ISAR0_EL1:
		sysreg64_read(ID_ISAR0_EL1, reg_val);
		break;
	case ISS_MRS_MSR_ID_ISAR1_EL1:
		sysreg64_read(ID_ISAR1_EL1, reg_val);
		break;
	case ISS_MRS_MSR_ID_ISAR2_EL1:
		sysreg64_read(ID_ISAR2_EL1, reg_val);
		break;
	case ISS_MRS_MSR_ID_ISAR3_EL1:
		sysreg64_read(ID_ISAR3_EL1, reg_val);
		break;
	case ISS_MRS_MSR_ID_ISAR4_EL1:
		sysreg64_read(ID_ISAR4_EL1, reg_val);
		break;
	case ISS_MRS_MSR_ID_ISAR5_EL1:
		sysreg64_read(ID_ISAR5_EL1, reg_val);
		break;
	case ISS_MRS_MSR_ID_ISAR6_EL1:
		sysreg64_read(S3_0_C0_C2_7, reg_val);
		break;
	case ISS_MRS_MSR_MVFR0_EL1:
		sysreg64_read(MVFR0_EL1, reg_val);
		break;
	case ISS_MRS_MSR_MVFR1_EL1:
		sysreg64_read(MVFR1_EL1, reg_val);
		break;
	case ISS_MRS_MSR_MVFR2_EL1:
		sysreg64_read(MVFR2_EL1, reg_val);
		break;
	case ISS_MRS_MSR_ID_AA64PFR0_EL1:
		reg_val = sys_aa64pfr0_read(thread);
		break;
	case ISS_MRS_MSR_ID_AA64PFR1_EL1:
		reg_val = sys_aa64pfr1_read(thread);
		break;
	case ISS_MRS_MSR_ID_AA64ZFR0_EL1:
#if defined(ARCH_ARM_FEAT_SVE)
		// The SVE module will handle this register
		ret = VCPU_TRAP_RESULT_UNHANDLED;
#else
		// When SVE is not implemented this register is RAZ, do nothing
#endif
		break;
	case ISS_MRS_MSR_ID_AA64SMFR0_EL1:
		// No Scalable Matrix Extension support for now
		break;
	case ISS_MRS_MSR_ID_AA64DFR0_EL1:
		reg_val = sys_aa64dfr0_read(thread);
		break;
	case ISS_MRS_MSR_ID_AA64DFR1_EL1:
		// RES0 - We don't know any AA64DFR1 bits
		break;
	case ISS_MRS_MSR_ID_AA64AFR0_EL1:
		// RES0 - We don't know any AA64AFR0 bits
		break;
	case ISS_MRS_MSR_ID_AA64AFR1_EL1:
		// RES0 - We don't know any AA64AFR1 bits
		break;
	case ISS_MRS_MSR_ID_AA64ISAR0_EL1:
		reg_val = sys_aa64isar0_read();
		break;
	case ISS_MRS_MSR_ID_AA64ISAR1_EL1:
		reg_val = sys_aa64isar1_read();
		break;
	case ISS_MRS_MSR_ID_AA64ISAR2_EL1:
		reg_val = sys_aa64isar2_read();
		break;
	case ISS_MRS_MSR_ID_AA64MMFR0_EL1:
		reg_val = sys_aa64mmfr0_read();
		break;
	case ISS_MRS_MSR_ID_AA64MMFR1_EL1:
		reg_val = sys_aa64mmfr1_read();
		break;
	case ISS_MRS_MSR_ID_AA64MMFR2_EL1:
		reg_val = sys_aa64mmfr2_read();
		break;
	case ISS_MRS_MSR_ID_AA64MMFR3_EL1:
		reg_val = sys_aa64mmfr3_read();
		break;
	case ISS_MRS_MSR_ID_AA64MMFR4_EL1:
		reg_val = 0;
		break;
	// The trapped ACTLR_EL1 by default returns 0 for reads.
	// The particular access should be handled in sysreg_read_cpu.
	case ISS_MRS_MSR_ACTLR_EL1:
		reg_val = 0U;
		break;
	default:
		ret = default_sys_read(&iss, &reg_val);
		break;
	}

	// Update the thread's register
	if (ret == VCPU_TRAP_RESULT_EMULATED) {
		vcpu_gpr_write(thread, reg_num, reg_val);
	}

#if SCHEDULER_CAN_MIGRATE
out:
#endif
	return ret;
}

vcpu_trap_result_t
sysreg_read_fallback(ESR_EL2_ISS_MSR_MRS_t iss)
{
	vcpu_trap_result_t ret	  = VCPU_TRAP_RESULT_UNHANDLED;
	thread_t	  *thread = thread_get_self();

	if (ESR_EL2_ISS_MSR_MRS_get_Op0(&iss) == 2U) {
		// Debug registers, RAZ by default
		vcpu_gpr_write(thread, ESR_EL2_ISS_MSR_MRS_get_Rt(&iss), 0U);
		ret = VCPU_TRAP_RESULT_EMULATED;
	}

	return ret;
}

vcpu_trap_result_t
sysreg_write(ESR_EL2_ISS_MSR_MRS_t iss)
{
	vcpu_trap_result_t ret	  = VCPU_TRAP_RESULT_EMULATED;
	thread_t	  *thread = thread_get_self();

	if (compiler_expected(ESR_EL2_ISS_MSR_MRS_get_Op0(&iss) != 1U)) {
		ret = VCPU_TRAP_RESULT_UNHANDLED;
		goto out;
	}

	// Assert this is a write
	assert(!ESR_EL2_ISS_MSR_MRS_get_Direction(&iss));

	// Remove the fields that are not used in the comparison
	ESR_EL2_ISS_MSR_MRS_t temp_iss = iss;
	ESR_EL2_ISS_MSR_MRS_set_Rt(&temp_iss, 0U);
	ESR_EL2_ISS_MSR_MRS_set_Direction(&temp_iss, false);

	switch (ESR_EL2_ISS_MSR_MRS_raw(temp_iss)) {
	// System instructions trapped with HCR_EL2.TSW
	case ISS_MRS_MSR_DC_CSW:
	case ISS_MRS_MSR_DC_CISW:
	case ISS_MRS_MSR_DC_ISW:
		// Set/way cache ops are not safe under virtualisation (or, in
		// most cases, without virtualisation) as they are vulnerable
		// to racing with prefetches through EL2 mappings, or with
		// migration if that is enabled. Warn if a VM executes one.
		TRACE_AND_LOG(DEBUG, INFO, "Unsafe DC *SW in VM {:d} @ {:#x}",
			      thread->addrspace->vmid,
			      ELR_EL2_raw(thread->vcpu_regs_gpr.pc));

		// However, they're only unsafe for the VM executing them
		// (because DC ISW is upgraded to DC CISW in hardware) so we
		// disable the trap after the first warning (except on physical
		// CPUs with an erratum that makes all set/way ops unsafe).
		// FIXME:
		preempt_disable();
		thread->vcpu_regs_el2.hcr_el2 = register_HCR_EL2_read();
		HCR_EL2_set_TSW(&thread->vcpu_regs_el2.hcr_el2, false);
		register_HCR_EL2_write(thread->vcpu_regs_el2.hcr_el2);
		preempt_enable();
		ret = VCPU_TRAP_RESULT_RETRY;
		break;
	default:
		ret = VCPU_TRAP_RESULT_UNHANDLED;
		break;
	}

out:
	return ret;
}

vcpu_trap_result_t
sysreg_write_fallback(ESR_EL2_ISS_MSR_MRS_t iss)
{
	vcpu_trap_result_t ret	  = VCPU_TRAP_RESULT_EMULATED;
	thread_t	  *thread = thread_get_self();

	// Read the thread's register
	uint8_t	   reg_num = ESR_EL2_ISS_MSR_MRS_get_Rt(&iss);
	register_t reg_val = vcpu_gpr_read(thread, reg_num);

	// Remove the fields that are not used in the comparison
	ESR_EL2_ISS_MSR_MRS_t temp_iss = iss;
	ESR_EL2_ISS_MSR_MRS_set_Rt(&temp_iss, 0U);
	ESR_EL2_ISS_MSR_MRS_set_Direction(&temp_iss, false);

	switch (ESR_EL2_ISS_MSR_MRS_raw(temp_iss)) {
	// The registers trapped with HCR_EL2.TVM
	case ISS_MRS_MSR_SCTLR_EL1: {
		SCTLR_EL1_t sctrl = SCTLR_EL1_cast(reg_val);
		// If HCR_EL2.DC is set, prevent VM's enabling Stg-1 MMU
		if (HCR_EL2_get_DC(&thread->vcpu_regs_el2.hcr_el2) &&
		    SCTLR_EL1_get_M(&sctrl)) {
			ret = VCPU_TRAP_RESULT_UNHANDLED;
		} else {
			register_SCTLR_EL1_write(sctrl);
		}
		break;
	}
	case ISS_MRS_MSR_TTBR0_EL1:
		register_TTBR0_EL1_write(TTBR0_EL1_cast(reg_val));
		break;
	case ISS_MRS_MSR_TTBR1_EL1:
		register_TTBR1_EL1_write(TTBR1_EL1_cast(reg_val));
		break;
	case ISS_MRS_MSR_TCR_EL1:
		register_TCR_EL1_write(TCR_EL1_cast(reg_val));
		break;
	case ISS_MRS_MSR_ESR_EL1:
		register_ESR_EL1_write(ESR_EL1_cast(reg_val));
		break;
	case ISS_MRS_MSR_FAR_EL1:
		register_FAR_EL1_write(FAR_EL1_cast(reg_val));
		break;
	case ISS_MRS_MSR_AFSR0_EL1:
		register_AFSR0_EL1_write(AFSR0_EL1_cast(reg_val));
		break;
	case ISS_MRS_MSR_AFSR1_EL1:
		register_AFSR1_EL1_write(AFSR1_EL1_cast(reg_val));
		break;
	case ISS_MRS_MSR_MAIR_EL1:
		register_MAIR_EL1_write(MAIR_EL1_cast(reg_val));
		break;
	case ISS_MRS_MSR_AMAIR_EL1:
		// WI
		break;
	// The trapped ACTLR_EL1 by default will be ignored for writes.
	// The particular access should be handled in sysreg_read_cpu.
	case ISS_MRS_MSR_ACTLR_EL1:
		// WI
		break;
	case ISS_MRS_MSR_CONTEXTIDR_EL1:
		register_CONTEXTIDR_EL1_write(CONTEXTIDR_EL1_cast(reg_val));
		break;
	default: {
		uint8_t opc0 = ESR_EL2_ISS_MSR_MRS_get_Op0(&iss);
		if (opc0 == 2U) {
			// Debug registers, WI by default
		} else {
			ret = VCPU_TRAP_RESULT_UNHANDLED;
		}
		break;
	}
	}

	return ret;
}

```

`hyp/vm/vcpu/aarch64/src/trap_dispatch.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>
#include <string.h>

#include <hypconstants.h>
#include <hypregisters.h>

#include <abort.h>
#include <addrspace.h>
#include <compiler.h>
#include <irq.h>
#include <log.h>
#include <panic.h>
#include <preempt.h>
#include <scheduler.h>
#include <smc_trace.h>
#include <thread.h>
#include <trace.h>

#include <events/thread.h>
#include <events/vcpu.h>

#include <asm/barrier.h>

#include "exception_dispatch.h"
#include "exception_inject.h"

#if ARCH_AARCH64_32BIT_EL1 && !ARCH_AARCH64_32BIT_EL0
#error invalid CPU config
#endif

static inline void
exception_skip_inst(bool is_il32)
{
	thread_t  *thread = thread_get_self();
	register_t pc = ELR_EL2_get_ReturnAddress(&thread->vcpu_regs_gpr.pc);

#if ARCH_AARCH64_32BIT_EL0
	pc += is_il32 ? 4U : 2U;

	SPSR_EL2_base_t spsr_base = thread->vcpu_regs_gpr.spsr_el2.base;

	if (SPSR_EL2_base_get_M4(&spsr_base)) {
		// Exception was in AArch32 execution. Update PSTATE.IT
		SPSR_EL2_A32_t spsr32 = thread->vcpu_regs_gpr.spsr_el2.a32;
		if (SPSR_EL2_A32_get_T(&spsr32)) {
			uint8_t IT = SPSR_EL2_A32_get_IT(&spsr32);
			if ((IT & 0xfU) == 0x8U) {
				// Was the last instruction in IT block
				IT = 0;
			} else {
				// Otherwise shift bits. This is safe even if
				// not in an IT block.
				IT = (uint8_t)((IT & 0xe0U) |
					       ((IT & 0xfU) << 1U));
			}
			SPSR_EL2_A32_set_IT(&spsr32, IT);

			thread->vcpu_regs_gpr.spsr_el2.a32 = spsr32;
		} else {
			assert(is_il32);
		}
	} else {
		assert(is_il32);
	}
#else
	assert(is_il32);
	pc += 4U;
#endif
	ELR_EL2_set_ReturnAddress(&thread->vcpu_regs_gpr.pc, pc);
}

static vcpu_trap_result_t
handle_inst_data_abort(ESR_EL2_t esr, esr_ec_t ec, FAR_EL2_t far,
		       HPFAR_EL2_t hpfar, iss_da_ia_fsc_t fsc, bool is_s1ptw,
		       bool is_data_abort)
{
	vcpu_trap_result_t ret = VCPU_TRAP_RESULT_UNHANDLED;
	gvaddr_t	   va  = FAR_EL2_get_VirtualAddress(&far);
	vmaddr_result_t	   ipa_r;

	if (is_s1ptw || (fsc == ISS_DA_IA_FSC_ADDR_SIZE_0) ||
	    (fsc == ISS_DA_IA_FSC_ADDR_SIZE_1) ||
	    (fsc == ISS_DA_IA_FSC_ADDR_SIZE_2) ||
	    (fsc == ISS_DA_IA_FSC_ADDR_SIZE_3) ||
	    (fsc == ISS_DA_IA_FSC_TRANSLATION_0) ||
	    (fsc == ISS_DA_IA_FSC_TRANSLATION_1) ||
	    (fsc == ISS_DA_IA_FSC_TRANSLATION_2) ||
	    (fsc == ISS_DA_IA_FSC_TRANSLATION_3) ||
	    (fsc == ISS_DA_IA_FSC_ACCESS_FLAG_1) ||
	    (fsc == ISS_DA_IA_FSC_ACCESS_FLAG_2) ||
	    (fsc == ISS_DA_IA_FSC_ACCESS_FLAG_3)) {
		// HPFAR_EL2 is valid; combine it with the sub-page bits
		// of the VA to find the exact IPA.
		ipa_r = vmaddr_result_ok(HPFAR_EL2_get_FIPA(&hpfar) |
					 (va & 0xfffU));
	} else {
		// HPFAR_EL2 was not set by the fault; we can't rely on it.
		ipa_r = vmaddr_result_error(ERROR_ADDR_INVALID);
	}

	// Call the event handlers for the DA/PA
	if (is_data_abort) {
		ret = trigger_vcpu_trap_data_abort_guest_event(esr, ipa_r, far);
	} else {
		ret = trigger_vcpu_trap_pf_abort_guest_event(esr, ipa_r, far);
	}

	// If faulting or still not handled, inject the abort to the guest
	if (((ret == VCPU_TRAP_RESULT_UNHANDLED) ||
	     (ret == VCPU_TRAP_RESULT_FAULT)) &&
	    inject_inst_data_abort(esr, ec, fsc, far, ipa_r.r, is_data_abort)) {
		ret = VCPU_TRAP_RESULT_RETRY;
	}

	return ret;
}

// Dispatching of guest interrupts
void
vcpu_interrupt_dispatch(void)
{
	trigger_thread_entry_from_user_event(THREAD_ENTRY_REASON_INTERRUPT);

	preempt_disable_in_irq();

	if (irq_interrupt_dispatch()) {
		(void)scheduler_schedule();
	}

	preempt_enable_in_irq();

	trigger_thread_exit_to_user_event(THREAD_ENTRY_REASON_INTERRUPT);
}

// Dispatching of guest synchronous exceptions
void
vcpu_exception_dispatch(bool is_aarch64)
{
	ESR_EL2_t   esr	  = register_ESR_EL2_read_ordered(&asm_ordering);
	FAR_EL2_t   far	  = register_FAR_EL2_read_ordered(&asm_ordering);
	HPFAR_EL2_t hpfar = register_HPFAR_EL2_read_ordered(&asm_ordering);

	trigger_thread_entry_from_user_event(THREAD_ENTRY_REASON_EXCEPTION);

	bool		   fatal  = false;
	vcpu_trap_result_t result = VCPU_TRAP_RESULT_UNHANDLED;

	esr_ec_t ec	 = ESR_EL2_get_EC(&esr);
	bool	 is_il32 = true;
	// FIXME:
	// For exceptions AArch32 execution, we need to determine whether the
	// trapped instruction passed its condition code. If it did not pass,
	// then skip the instruction. Remember special cases, such as BKPT in
	// IT blocks!
	// The decoding to do this is specific to each ESR_EL2.EC value, and
	// should probably be done within the switch cases below.
#if ARCH_AARCH64_32BIT_EL0
	is_il32 = ESR_EL2_get_IL(&esr);
#endif
#if !ARCH_AARCH64_32BIT_EL1
	assert(is_aarch64);
#endif

	switch (ec) {
	case ESR_EC_UNKNOWN:
		result = trigger_vcpu_trap_unknown_event(esr);
		break;

	case ESR_EC_WFIWFE: {
		ESR_EL2_ISS_WFI_WFE_t iss =
			ESR_EL2_ISS_WFI_WFE_cast(ESR_EL2_get_ISS(&esr));
#if ARCH_AARCH64_32BIT_EL1
		// FIXME:
#error Check the condition code
#endif
		switch (ESR_EL2_ISS_WFI_WFE_get_TI(&iss)) {
		case ISS_WFX_TI_WFE:
			result = trigger_vcpu_trap_wfe_event(iss);
			break;
		case ISS_WFX_TI_WFI:
			result = trigger_vcpu_trap_wfi_event(iss);
			break;
#if defined(ARCH_ARM_FEAT_WFxT)
		// These need events updated to pass timeout for FEAT_WFxT
		// support
		// FIXME:
		case ISS_WFX_TI_WFET:
			result = trigger_vcpu_trap_wfe_event(iss);
			break;
		case ISS_WFX_TI_WFIT:
			result = trigger_vcpu_trap_wfi_event(iss);
			break;
#endif
		default:
			// should not happen
			// result = VCPU_TRAP_RESULT_UNHANDLED
			break;
		}
		break;
	}
	case ESR_EC_FPEN: {
#if ARCH_AARCH64_32BIT_EL1
		// FIXME:
#error Check the condition code
#endif
		result = trigger_vcpu_trap_fp_enabled_event(esr);
		break;
	}

#if defined(ARCH_ARM_FEAT_PAuth)
	case ESR_EC_PAUTH:
		if (trigger_vcpu_trap_pauth_event()) {
			result = VCPU_TRAP_RESULT_RETRY;
		}
		break;

#if defined(ARCH_ARM_FEAT_NV)
	case ESR_EC_ERET:
		if (trigger_vcpu_trap_eret_event(esr)) {
			result = VCPU_TRAP_RESULT_RETRY;
		}
		break;
#endif
#endif // defined(ARCH_ARM_FEAT_FPAC)

	case ESR_EC_ILLEGAL:
		if (trigger_vcpu_trap_illegal_state_event()) {
			result = VCPU_TRAP_RESULT_RETRY;
		}
		break;

	case ESR_EC_SVC64:
		if (trigger_vcpu_trap_svc64_event(esr)) {
			// SVC is not an exception generating instruction for
			// EL2; it is trapped, and therefore the preferred
			// return address is the instruction itself. So, we
			// treat success as an emulated instruction so the PC
			// will be advanced in software.
			result = VCPU_TRAP_RESULT_EMULATED;
		}
		break;

	case ESR_EC_HVC64_EL2: {
		ESR_EL2_ISS_HVC_t iss =
			ESR_EL2_ISS_HVC_cast(ESR_EL2_get_ISS(&esr));
		if (trigger_vcpu_trap_hvc64_event(iss)) {
			// HVC is an exception generating instruction for EL2;
			// the preferred return address is the next instruction.
			// So, we treat success as a retry so the PC will not be
			// advanced again in software.
			result = VCPU_TRAP_RESULT_RETRY;
		}
		break;
	}

	case ESR_EC_SMC64_EL2: {
		ESR_EL2_ISS_SMC64_t iss =
			ESR_EL2_ISS_SMC64_cast(ESR_EL2_get_ISS(&esr));

		SMC_TRACE_CURRENT(SMC_TRACE_ID_EL1_64ENT, 8);

		if (trigger_vcpu_trap_smc64_event(iss)) {
			// SMC is not an exception generating instruction for
			// EL2; it is trapped, and therefore the preferred
			// return address is the instruction itself. So, we
			// treat success as an emulated instruction so the PC
			// will be advanced in software.
			result = VCPU_TRAP_RESULT_EMULATED;

			SMC_TRACE_CURRENT(SMC_TRACE_ID_EL1_64RET, 7);
		}

		break;
	}

	case ESR_EC_SYSREG: {
		ESR_EL2_ISS_MSR_MRS_t iss =
			ESR_EL2_ISS_MSR_MRS_cast(ESR_EL2_get_ISS(&esr));
		if (ESR_EL2_ISS_MSR_MRS_get_Direction(&iss)) {
			result = trigger_vcpu_trap_sysreg_read_event(iss);
		} else {
			result = trigger_vcpu_trap_sysreg_write_event(iss);
		}
		break;
	}
#if defined(ARCH_ARM_FEAT_SVE)
	case ESR_EC_SVE:
		result = trigger_vcpu_trap_sve_access_event();
		break;
#endif
	case ESR_EC_INST_ABT_LO: {
		ESR_EL2_ISS_INST_ABORT_t iss =
			ESR_EL2_ISS_INST_ABORT_cast(ESR_EL2_get_ISS(&esr));
		iss_da_ia_fsc_t fsc   = ESR_EL2_ISS_INST_ABORT_get_IFSC(&iss);
		bool		s1ptw = ESR_EL2_ISS_INST_ABORT_get_S1PTW(&iss);

		result = handle_inst_data_abort(esr, ec, far, hpfar, fsc, s1ptw,
						false);
		break;
	}
	case ESR_EC_PC_ALIGN:
		if (trigger_vcpu_trap_pc_alignment_fault_event()) {
			result = VCPU_TRAP_RESULT_RETRY;
		}
		break;

	case ESR_EC_DATA_ABT_LO: {
		ESR_EL2_ISS_DATA_ABORT_t iss =
			ESR_EL2_ISS_DATA_ABORT_cast(ESR_EL2_get_ISS(&esr));
		iss_da_ia_fsc_t fsc   = ESR_EL2_ISS_DATA_ABORT_get_DFSC(&iss);
		bool		s1ptw = ESR_EL2_ISS_DATA_ABORT_get_S1PTW(&iss);

		result = handle_inst_data_abort(esr, ec, far, hpfar, fsc, s1ptw,
						true);
		break;
	}
	case ESR_EC_SP_ALIGN:
		if (trigger_vcpu_trap_sp_alignment_fault_event()) {
			result = VCPU_TRAP_RESULT_RETRY;
		}
		break;

	case ESR_EC_FP64:
		result = trigger_vcpu_trap_fp64_event(esr);
		break;

	case ESR_EC_BREAK_LO:
		result = trigger_vcpu_trap_breakpoint_guest_event(esr);
		break;

	case ESR_EC_STEP_LO:
		result = trigger_vcpu_trap_software_step_guest_event(esr);
		break;

	case ESR_EC_WATCH_LO:
		result = trigger_vcpu_trap_watchpoint_guest_event(esr);
		break;

	case ESR_EC_BRK:
		result = trigger_vcpu_trap_brk_instruction_guest_event(esr);
		break;

		/* AArch32 traps which may come from EL0/1 */
#if ARCH_AARCH64_32BIT_EL0
	case ESR_EC_LDCSTC: {
		ESR_EL2_ISS_LDC_STC_t iss =
			ESR_EL2_ISS_LDC_STC_cast(ESR_EL2_get_ISS(&esr));
		result = trigger_vcpu_trap_ldcstc_guest_event(iss);
		break;
	}
	case ESR_EC_MCRMRC14: {
		ESR_EL2_ISS_MCR_MRC_t iss =
			ESR_EL2_ISS_MCR_MRC_cast(ESR_EL2_get_ISS(&esr));
		result = trigger_vcpu_trap_mcrmrc14_guest_event(iss);
		break;
	}
	case ESR_EC_MCRMRC15:
		result = trigger_vcpu_trap_mcrmrc15_guest_event(esr);
		break;
	case ESR_EC_MCRRMRRC15:
		result = trigger_vcpu_trap_mcrrmrrc15_guest_event(esr);
		break;
	case ESR_EC_MRRC14:
		result = trigger_vcpu_trap_mrrc14_guest_event(esr);
		break;
	case ESR_EC_BKPT:
		result = trigger_vcpu_trap_bkpt_guest_event(esr);
		break;
#else
	case ESR_EC_LDCSTC:
	case ESR_EC_MCRMRC14:
	case ESR_EC_MCRMRC15:
	case ESR_EC_MCRRMRRC15:
	case ESR_EC_MRRC14:
	case ESR_EC_BKPT:
		break;
#endif
	/* Asynchronous traps which don't come through this path */
	case ESR_EC_SERROR:
	/* AArch32 traps which may come when TGE=1 */
	case ESR_EC_FP32:
	/* AArch32 traps which may come only from EL1 */
	case ESR_EC_VMRS_EL2:
	case ESR_EC_SVC32:
	case ESR_EC_HVC32_EL2:
	case ESR_EC_SMC32_EL2:
	case ESR_EC_VECTOR32_EL2:
		// FIXME: Handle the traps coming from AArch32 EL1
		break;

	// EL2 traps, we should never get these here
	case ESR_EC_INST_ABT:
	case ESR_EC_DATA_ABT:
	case ESR_EC_BREAK:
	case ESR_EC_STEP:
	case ESR_EC_WATCH:
#if defined(ARCH_ARM_FEAT_BTI)
	case ESR_EC_BTI:
#endif
#if defined(ARCH_ARM_FEAT_PAuth) && defined(ARCH_ARM_FEAT_FPAC)
	case ESR_EC_FPAC:
#endif
#if defined(ARCH_ARM_FEAT_LS64)
	case ESR_EC_LD64B_ST64B:
#endif
#if defined(ARCH_ARM_FEAT_TME)
	case ESR_EC_TSTART:
#endif
#if defined(ARCH_ARM_FEAT_SME)
	case ESR_EC_SME:
#endif
#if defined(ARCH_ARM_FEAT_RME)
	case ESR_EC_RME:
#endif
#if defined(ARCH_ARM_FEAT_MOPS)
	case ESR_EC_MOPS:
#endif
#if defined(VERBOSE) && VERBOSE
	// Cause a fatal error in verbose builds so we can detect any unhandled
	// ECs
	default:
#endif
		fatal = true;
		break;
#if !(defined(VERBOSE) && VERBOSE)
	// On non-verbose builds pass the unexpected ECs back to the VM
	default:
		result = VCPU_TRAP_RESULT_UNHANDLED;
		break;
#endif
	}

	thread_t *thread = thread_get_self();

	if (compiler_unexpected(fatal)) {
		TRACE_AND_LOG(ERROR, WARN,
			      "Unexpected trap from VM {:d}, ESR_EL2 = {:#x}, "
			      "ELR_EL2 = {:#x}",
			      thread->addrspace->vmid, ESR_EL2_raw(esr),
			      ELR_EL2_raw(thread->vcpu_regs_gpr.pc));
		abort("Unexpected guest trap",
		      ABORT_REASON_UNHANDLED_EXCEPTION);
	}

	switch (result) {
	case VCPU_TRAP_RESULT_UNHANDLED:
		TRACE_AND_LOG(ERROR, WARN,
			      "Unhandled trap from VM {:d}, ESR_EL2 = {:#x}, "
			      "ELR_EL2 = {:#x}",
			      thread->addrspace->vmid, ESR_EL2_raw(esr),
			      ELR_EL2_raw(thread->vcpu_regs_gpr.pc));
		inject_undef_abort(esr);
		break;
	case VCPU_TRAP_RESULT_FAULT:
		inject_undef_abort(esr);
		break;
	case VCPU_TRAP_RESULT_EMULATED:
		exception_skip_inst(is_il32);
		break;
	case VCPU_TRAP_RESULT_RETRY:
	default:
		// Nothing to do here.
		break;
	}

	trigger_thread_exit_to_user_event(THREAD_ENTRY_REASON_EXCEPTION);
}

// Dispatching of guest asynchronous system errors
void
vcpu_error_dispatch(void)
{
	ESR_EL2_t esr = register_ESR_EL2_read_ordered(&asm_ordering);

	trigger_thread_entry_from_user_event(THREAD_ENTRY_REASON_INTERRUPT);

	preempt_disable_in_irq();

	ESR_EL2_ISS_SERROR_t iss =
		ESR_EL2_ISS_SERROR_cast(ESR_EL2_get_ISS(&esr));
	(void)trigger_vcpu_trap_serror_event(iss);

	preempt_enable_in_irq();

	trigger_thread_exit_to_user_event(THREAD_ENTRY_REASON_INTERRUPT);
}

```

`hyp/vm/vcpu/aarch64/src/wfi.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>

#include <hypregisters.h>

#include <compiler.h>
#include <idle.h>
#include <log.h>
#include <preempt.h>
#include <scheduler.h>
#include <thread.h>
#include <trace.h>
#include <vcpu.h>

#include <events/vcpu.h>

#include "event_handlers.h"

void
vcpu_handle_scheduler_selected_thread(thread_t *thread, bool *can_idle)
{
	assert(thread != NULL);

	// If idle in EL1 is allowed, this is used during context switch to
	// decide whether to enable the WFI trap; otherwise it is used in the
	// WFI trap handler to decide whether to call idle_yield() without
	// scheduling.
	vcpu_runtime_flags_set_vcpu_can_idle(&thread->vcpu_flags, *can_idle);
}

vcpu_trap_result_t
vcpu_handle_vcpu_trap_wfi(ESR_EL2_ISS_WFI_WFE_t iss)
{
	vcpu_trap_result_t ret = VCPU_TRAP_RESULT_EMULATED;

	thread_t *current = thread_get_self();
	assert(current->kind == THREAD_KIND_VCPU);

	assert_preempt_enabled();
	preempt_disable();

#if defined(ARCH_ARM_FEAT_WFxT)
	// Inject a trap to the guest if it uses the WFIT instruction
	// without checking their availability in the ID registers first.
	// Remove once support for FEAT_WFxT is added to the hypervisor.
	// FIXME:
	if (ESR_EL2_ISS_WFI_WFE_get_TI(&iss) == ISS_WFX_TI_WFIT) {
		TRACE_AND_LOG(ERROR, WARN, "WFIT trap from thread {:#x}",
			      (register_t)current);
		ret = VCPU_TRAP_RESULT_FAULT;
		goto out;
	}
#else
	(void)iss;
#endif

#if !defined(PREEMPT_NULL)
	bool vcpu_interrupted =
		vcpu_runtime_flags_get_vcpu_interrupted(&current->vcpu_flags);
#if !defined(VCPU_IDLE_IN_EL1) || !VCPU_IDLE_IN_EL1
	if (vcpu_runtime_flags_get_vcpu_can_idle(&current->vcpu_flags) &&
	    !vcpu_interrupted) {
		if (vcpu_block_start()) {
			goto out;
		}

		bool need_schedule;
		do {
			need_schedule = idle_yield();
			// We may have received a wakeup while idle, so recheck
			// the interrupted flag.
			vcpu_interrupted =
				vcpu_runtime_flags_get_vcpu_interrupted(
					&current->vcpu_flags);
		} while (!need_schedule && !vcpu_interrupted);

		vcpu_block_finish();

		// We only need to reschedule here if the VCPU was interrupted;
		// otherwise the reschedule is handled by the yield below after
		// setting the WFI block flag.
		if (need_schedule && vcpu_interrupted) {
			scheduler_schedule();
		}
	}
#endif // !VCPU_IDLE_IN_EL1
       // Check whether we were woken by an interrupt after the WFI trap was
       // taken. This could have been done either by a preemption before the
       // preempt_disable() above, or by an IPI during the idle_yield() in the
       // WFI fastpath (if it is enabled).
	if (vcpu_interrupted) {
		goto out;
	}
#endif // !PREEMPT_NULL

	scheduler_lock_nopreempt(current);
	scheduler_block(current, SCHEDULER_BLOCK_VCPU_WFI);
	scheduler_unlock_nopreempt(current);

	(void)scheduler_yield();

out:
	preempt_enable();

	return ret;
}

#if defined(VCPU_IDLE_IN_EL1) && VCPU_IDLE_IN_EL1
void
vcpu_handle_scheduler_quiescent(void)
{
	thread_t *current = thread_get_self();
	if (compiler_expected(current->kind == THREAD_KIND_VCPU)) {
		current->vcpu_regs_el2.hcr_el2 = register_HCR_EL2_read();
		HCR_EL2_set_TWI(&current->vcpu_regs_el2.hcr_el2,
				!vcpu_runtime_flags_get_vcpu_can_idle(
					&current->vcpu_flags));
		register_HCR_EL2_write(current->vcpu_regs_el2.hcr_el2);
	}
}

void
vcpu_handle_thread_context_switch_post(void)
{
	thread_t *current = thread_get_self();
	HCR_EL2_set_TWI(
		&current->vcpu_regs_el2.hcr_el2,
		!vcpu_runtime_flags_get_vcpu_can_idle(&current->vcpu_flags));
}
#endif // VCPU_IDLE_IN_EL1

void
vcpu_wakeup(thread_t *vcpu)
{
	assert(vcpu != NULL);
	assert(vcpu->kind == THREAD_KIND_VCPU);

#if !defined(PREEMPT_NULL)
	if (vcpu == thread_get_self()) {
		// Inhibit sleep in preempted WFI handlers (see above)
		vcpu_runtime_flags_set_vcpu_interrupted(&vcpu->vcpu_flags,
							true);
	}
#endif

	trigger_vcpu_wakeup_event(vcpu);

	if (scheduler_unblock(vcpu, SCHEDULER_BLOCK_VCPU_WFI)) {
		scheduler_trigger();
	}
}

void
vcpu_wakeup_self(void)
{
	thread_t *current = thread_get_self();
	assert(current->kind == THREAD_KIND_VCPU);

#if !defined(PREEMPT_NULL)
	// Inhibit sleep in preempted WFI handlers (see above)
	vcpu_runtime_flags_set_vcpu_interrupted(&current->vcpu_flags, true);
#endif

	trigger_vcpu_wakeup_self_event();
}

bool
vcpu_expects_wakeup(const thread_t *thread)
{
	assert(thread->kind == THREAD_KIND_VCPU);

	return scheduler_is_blocked(thread, SCHEDULER_BLOCK_VCPU_WFI) ||
	       trigger_vcpu_expects_wakeup_event(thread);
}

#if defined(MODULE_VM_VCPU_RUN)
vcpu_run_state_t
vcpu_arch_handle_vcpu_run_check(const thread_t *thread,
				register_t     *state_data_0,
				register_t     *state_data_1)
{
	vcpu_run_state_t state = VCPU_RUN_STATE_BLOCKED;
	if (scheduler_is_blocked(thread, SCHEDULER_BLOCK_VCPU_WFI)) {
		state	      = VCPU_RUN_STATE_EXPECTS_WAKEUP;
		*state_data_0 = 0U;
		*state_data_1 = (register_t)VCPU_RUN_WAKEUP_FROM_STATE_WFI;
	}
	return state;
}
#endif

bool
vcpu_pending_wakeup(void)
{
	thread_t *current = thread_get_self();
	assert(current->kind == THREAD_KIND_VCPU);

#if defined(PREEMPT_NULL)
	return trigger_vcpu_pending_wakeup_event();
#else
	return vcpu_runtime_flags_get_vcpu_interrupted(&current->vcpu_flags) ||
	       trigger_vcpu_pending_wakeup_event();
#endif
}

bool
vcpu_block_start(void)
{
	thread_t *current = thread_get_self();
	bool	  pending = vcpu_pending_wakeup();

	TRACE_LOCAL(DEBUG, INFO, "vcpu: {:#x} block start", (uintptr_t)current);

	if (!pending) {
		pending = trigger_vcpu_block_start_event();
	}

	if (compiler_unexpected(pending)) {
		TRACE_LOCAL(DEBUG, INFO, "vcpu: {:#x} block aborted, pending",
			    (uintptr_t)current);
	}
	return pending;
}

void
vcpu_block_finish(void)
{
	trigger_vcpu_block_finish_event();
	thread_t *current = thread_get_self();
	TRACE_LOCAL(DEBUG, INFO, "vcpu: {:#x} block finish",
		    (uintptr_t)current);
}

void
vcpu_handle_thread_exit_to_user(void)
{
#if !defined(PREEMPT_NULL)
	thread_t *current = thread_get_self();

	// Don't inhibit sleep in new WFI traps
	vcpu_runtime_flags_set_vcpu_interrupted(&current->vcpu_flags, false);
#endif
}

```

`hyp/vm/vcpu/aarch64/vcpu_aarch64.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// Add the events for the traps coming from AArch32
// FIXME:

module vcpu

// core initialization

subscribe boot_runtime_first_init
	handler vcpu_handle_boot_runtime_init()
subscribe boot_runtime_warm_init
	handler vcpu_handle_boot_runtime_init()
subscribe boot_cpu_warm_init()
#if defined(ARCH_ARM_FEAT_CSV2_2) || defined(ARCH_ARM_FEAT_CSV2_1p2) || \
	defined(ARCH_ARM_FEAT_CSV2_3)
subscribe boot_cold_init()
#endif

// new vcpu handlers

subscribe object_deactivate_thread

subscribe vcpu_activate_thread

subscribe thread_get_entry_fn[THREAD_KIND_VCPU] ()

subscribe object_create_thread
	handler vcpu_arch_handle_object_create_thread

#if SCHEDULER_CAN_MIGRATE
subscribe thread_start
	handler vcpu_arch_handle_thread_start
#endif

// vcpu context_switch handlers

subscribe thread_save_state
	handler vcpu_context_switch_save()
	require_preempt_disabled

subscribe thread_save_state
	handler vcpu_context_switch_cpu_save()
	require_preempt_disabled

subscribe thread_load_state
	handler vcpu_context_switch_load()
	require_preempt_disabled

subscribe thread_load_state
	handler vcpu_context_switch_cpu_load()
	require_preempt_disabled

// vcpu register trap handlers

subscribe vcpu_trap_sysreg_read
	handler sysreg_read

subscribe vcpu_trap_sysreg_read
	handler sysreg_read_cpu
	priority 1

subscribe vcpu_trap_sysreg_read
	handler sysreg_read_fallback
	priority last

subscribe vcpu_trap_sysreg_write
	handler sysreg_write
	priority first

subscribe vcpu_trap_sysreg_write
	handler sysreg_write_cpu
	priority 1

subscribe vcpu_trap_sysreg_write
	handler sysreg_write_fallback
	priority last

subscribe vcpu_trap_brk_instruction_guest
	priority last

// VCPU idle traps and wakeups

subscribe scheduler_selected_thread(thread, can_idle)

subscribe vcpu_trap_wfi
	priority last
	exclude_preempt_disabled

subscribe thread_exit_to_user()

#if defined(VCPU_IDLE_IN_EL1) && VCPU_IDLE_IN_EL1
subscribe scheduler_quiescent()

subscribe thread_context_switch_post()
#endif

#if defined(INTERFACE_VCPU_RUN)
subscribe vcpu_run_check
	handler vcpu_arch_handle_vcpu_run_check(vcpu, state_data_0, state_data_1)
#endif

// VCPU lifecycle and power management

subscribe rootvm_init(root_thread)

```

`hyp/vm/vcpu/aarch64/vcpu_aarch64.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

define vcpu_gpr structure {
	x		array(31) type register_t(aligned(16));
	pc		bitfield ELR_EL2;
	spsr_el2	union SPSR_EL2;
};

define vector_register_t newtype array(16) uint8();

define vcpu_vfp_registers structure {
	q		array(32) type vector_register_t(aligned(16));
	fpcr		bitfield FPCR;
	fpsr		bitfield FPSR;
};

define vcpu_el1_registers structure {
	contextidr_el1	bitfield CONTEXTIDR_EL1;
	cpacr_el1	bitfield CPACR_EL1;
	csselr_el1	bitfield CSSELR_EL1;
	elr_el1		bitfield ELR_EL1;
	esr_el1		bitfield ESR_EL1;
	far_el1		bitfield FAR_EL1;
	par_el1		union PAR_EL1;
	mair_el1	bitfield MAIR_EL1;
	sp_el0		bitfield SP_EL0;
	sp_el1		bitfield SP_EL1;
	spsr_el1	bitfield SPSR_EL1_A64;
	sctlr_el1	bitfield SCTLR_EL1;
	ttbr0_el1	bitfield TTBR0_EL1;
	ttbr1_el1	bitfield TTBR1_EL1;
	tpidr_el0	bitfield TPIDR_EL0;
	tpidrro_el0	bitfield TPIDRRO_EL0;
	tpidr_el1	bitfield TPIDR_EL1;
	tcr_el1		bitfield TCR_EL1;
	vbar_el1	bitfield VBAR_EL1;
#if defined(ARCH_ARM_FEAT_CSV2_2) || defined(ARCH_ARM_FEAT_CSV2_1p2) || \
	defined(ARCH_ARM_FEAT_CSV2_3)
	scxtnum_el0	uint64;
	scxtnum_el1	uint64;
#endif
#if !defined(CPU_HAS_NO_ACTLR_EL1)
	actlr_el1	bitfield ACTLR_EL1;
#endif
#if !defined(CPU_HAS_NO_AMAIR_EL1)
	amair_el1	bitfield AMAIR_EL1;
#endif
#if !defined(CPU_HAS_NO_AFSR0_EL1)
	afsr0_el1	bitfield AFSR0_EL1;
#endif
#if !defined(CPU_HAS_NO_AFSR1_EL1)
	afsr1_el1	bitfield AFSR1_EL1;
#endif
};

define vcpu_el2_registers structure {
#if defined(ARCH_ARM_FEAT_VHE)
	cptr_el2	bitfield CPTR_EL2_E2H1;
#else
	cptr_el2	bitfield CPTR_EL2_E2H0;
#endif
	hcr_el2		bitfield HCR_EL2;
	mdcr_el2	bitfield MDCR_EL2;
};

extend vcpu object {
	gpr	structure vcpu_gpr(group(context_switch, registers, a));
#if defined(ARCH_ARM_FEAT_PAuth)
	pauth	structure aarch64_pauth_keys(group(context_switch, registers, b));
#endif
	fpr	structure vcpu_vfp_registers(group(context_switch, registers, c));
	el1	structure vcpu_el1_registers(group(context_switch, registers, d));
	el2	structure vcpu_el2_registers(group(context_switch, registers, e));

	mpidr_el1	bitfield MPIDR_EL1(group(context_switch, registers, d));
#if SCHEDULER_CAN_MIGRATE
	midr_el1	bitfield MIDR_EL1(group(context_switch, registers, d));
#endif
};

#if !defined(PREEMPT_NULL)
extend vcpu_runtime_flags bitfield {
	auto	vcpu_interrupted	bool = 0;
	auto	vcpu_can_idle		bool = 0;
};
#endif

#if defined(INTERFACE_VCPU_RUN)
extend vcpu_run_wakeup_from_state enumeration {
	// VCPU is halted by a trapped WFI instruction.
	wfi = 1;
};
#endif

```

`hyp/vm/vcpu/armv8-64/include/vectors_vcpu.inc`:

```inc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// Make sure the structure layouts haven't changed
#if (((OFS_VCPU_GPR_X(30) + 8) != OFS_VCPU_GPR_PC) ||   \
     ((OFS_VCPU_GPR_PC + 8) != OFS_VCPU_GPR_SPSR_EL2))
#error The layout of vcpu_gpr_t has changed
#endif

#if defined(ARCH_ARM_FEAT_PAuth)
// Define symbols for pointer auth offsets so we can access them from macros
.equ	vcpu_pauth_tp_pauth_ofs, \
	OFS_THREAD_VCPU_REGS_PAUTH - OFS_THREAD_VCPU_REGS_GPR
.equ	pauth_DA_ofs, OFS_AARCH64_PAUTH_KEYS_DA
.equ	pauth_DB_ofs, OFS_AARCH64_PAUTH_KEYS_DB
.equ	pauth_IA_ofs, OFS_AARCH64_PAUTH_KEYS_IA
.equ	pauth_IB_ofs, OFS_AARCH64_PAUTH_KEYS_IB
.equ	pauth_GA_ofs, OFS_AARCH64_PAUTH_KEYS_GA

// Macro for saving an EL1 key and loading the corresponding EL2 key
.macro  vcpu_pauth_entry_key k:req, tp:req, kp:req, \
		tl:req, th:req, kl:req, kh:req
	mrs	\tl, AP\k\()KeyLo_EL1
	ldp	\kl, \kh, [\kp, pauth_\k\()_ofs]
	mrs	\th, AP\k\()KeyHi_EL1
	msr	AP\k\()KeyLo_EL1, \kl
	stp	\tl, \th, [\tp, vcpu_pauth_tp_pauth_ofs + pauth_\k\()_ofs]
	msr	AP\k\()KeyHi_EL1, \kh
.endm

// Macro for loading an EL1 key
.macro  vcpu_pauth_exit_key k:req, tp:req, tl:req, th:req
	ldp	\tl, \th, [\tp, vcpu_pauth_tp_pauth_ofs + pauth_\k\()_ofs]
	msr	AP\k\()KeyLo_EL1, \tl
	msr	AP\k\()KeyHi_EL1, \th
.endm
#endif

.macro  vcpu_pauth_entry tp:req, kp:req, tl:req, th:req, kl:req, kh:req
#if defined(ARCH_ARM_FEAT_PAuth)
	thread_get_self	\tp, offset=OFS_THREAD_VCPU_REGS_GPR
	vcpu_pauth_entry_thread \tp, \kp, \tl, \th, \kl, \kh
#endif
.endm

.macro  vcpu_pauth_entry_thread tp:req, kp:req, tl:req, th:req, kl:req, kh:req
#if defined(ARCH_ARM_FEAT_PAuth)
	adrl	\kp, aarch64_pauth_keys
	vcpu_pauth_entry_key DA, \tp, \kp, \tl, \th, \kl, \kh
	vcpu_pauth_entry_key DB, \tp, \kp, \tl, \th, \kl, \kh
	vcpu_pauth_entry_key IA, \tp, \kp, \tl, \th, \kl, \kh
	vcpu_pauth_entry_key IB, \tp, \kp, \tl, \th, \kl, \kh
	vcpu_pauth_entry_key GA, \tp, \kp, \tl, \th, \kl, \kh
	isb
#endif
.endm

.macro  vcpu_pauth_exit tp:req, tl:req, th:req, spsr:req, tmp:req
#if defined(ARCH_ARM_FEAT_PAuth)
	thread_get_self	\tp, offset=OFS_THREAD_VCPU_REGS_GPR
	vcpu_pauth_exit_thread \tp, \tl, \th, \spsr, \tmp
#endif
.endm

.macro  vcpu_pauth_exit_thread tp:req, tl:req, th:req, spsr:req, tmp:req
#if defined(ARCH_ARM_FEAT_PAuth)
	// There is no need to sign userspace return addresses, but we do need
	// to check that we are actually returning to userspace here: either
	// SPSR_EL2.M[4]=1 (for 32-bit mode) or SPSR_EL2.M[3]=0 (for EL1 or
	// EL0), i.e. SPSR_EL2[4:3] != 0b01.
	ubfx	\tmp, \spsr, 3, 2

	vcpu_pauth_exit_key DA, \tp, \tl, \th
	vcpu_pauth_exit_key DB, \tp, \tl, \th
	vcpu_pauth_exit_key IA, \tp, \tl, \th
	vcpu_pauth_exit_key IB, \tp, \tl, \th

	sub	\tmp, \tmp, 1

	vcpu_pauth_exit_key GA, \tp, \tl, \th

	cbz	\tmp, vcpu_pauth_exit_failed
#endif
.endm

.macro	save_guest_context_vcpu_zero_0_13_irq
	disable_phys_access

	// SP (SP_EL2) points to the kernel stack
	stp	x0, x1, [sp, -16]!		// Save X0 & X1
	thread_get_self	x0, offset=OFS_THREAD_VCPU_REGS_GPR

	stp	x2, x3, [x0, OFS_VCPU_GPR_X(2)]
	mrs	x1, SPSR_EL2
	stp	x4, x5, [x0, OFS_VCPU_GPR_X(4)]
	stp	x6, x7, [x0, OFS_VCPU_GPR_X(6)]
	clear_guest_registers "4,5"
	ldp	x2, x3, [sp], #16		// Recover X0 & X1
	stp	x8, x9, [x0, OFS_VCPU_GPR_X(8)]
	clear_guest_registers "6,7"
	str	x1, [x0, OFS_VCPU_GPR_SPSR_EL2]
	stp	x10, x11, [x0, OFS_VCPU_GPR_X(10)]
	clear_guest_registers "8,9"
	stp	x12, x13, [x0, OFS_VCPU_GPR_X(12)]
	mrs	x1, ELR_EL2
	clear_guest_registers "10,11"
	stp	x2, x3, [x0, OFS_VCPU_GPR_X(0)]
	clear_guest_registers "12,13"
	clear_guest_registers "2,3"
	stp	x30, x1, [x0, OFS_VCPU_GPR_X(30)]
	clear_guest_registers "1"
.endm

.macro	save_guest_context_vcpu_zero_14_29_irq
	stp	x14, x15, [x0, OFS_VCPU_GPR_X(14)]
	stp	x16, x17, [x0, OFS_VCPU_GPR_X(16)]
	clear_guest_registers "14,15"
	stp	x18, x19, [x0, OFS_VCPU_GPR_X(18)]
	clear_guest_registers "16,17"
	stp	x20, x21, [x0, OFS_VCPU_GPR_X(20)]
	clear_guest_registers "18,19"
	stp	x22, x23, [x0, OFS_VCPU_GPR_X(22)]
	clear_guest_registers "20,21"
	stp	x24, x25, [x0, OFS_VCPU_GPR_X(24)]
	clear_guest_registers "22,23"
	stp	x26, x27, [x0, OFS_VCPU_GPR_X(26)]
	clear_guest_registers "24"
	stp	x28, x29, [x0, OFS_VCPU_GPR_X(28)]

	vcpu_pauth_entry_thread x0, x25, x26, x27, x28, x29
	clear_guest_registers "25,26,27,28,29"
.endm

.macro	save_guest_exception_context_zero_1_29
	// SP (SP_EL2) points to the kernel stack
	stp	x0, x1, [sp, -16]!		// Save X0 & X1
	thread_get_self	x0, offset=OFS_THREAD_VCPU_REGS_GPR

	stp	x2, x3, [x0, OFS_VCPU_GPR_X(2)]
	mrs	x1, SPSR_EL2
	stp	x4, x5, [x0, OFS_VCPU_GPR_X(4)]
	stp	x6, x7, [x0, OFS_VCPU_GPR_X(6)]
	clear_guest_registers "4,5"
	ldp	x2, x3, [sp], #16		// Recover X0 & X1
	stp	x8, x9, [x0, OFS_VCPU_GPR_X(8)]
	clear_guest_registers "6,7"
	str	x1, [x0, OFS_VCPU_GPR_SPSR_EL2]
	stp	x10, x11, [x0, OFS_VCPU_GPR_X(10)]
	clear_guest_registers "8,9"
	stp	x12, x13, [x0, OFS_VCPU_GPR_X(12)]
	clear_guest_registers "10,11"
	stp	x2, x3, [x0, OFS_VCPU_GPR_X(0)]
	clear_guest_registers "12,13"
	stp	x14, x15, [x0, OFS_VCPU_GPR_X(14)]
	clear_guest_registers "2,3"
	stp	x16, x17, [x0, OFS_VCPU_GPR_X(16)]
	clear_guest_registers "14,15"
	stp	x18, x19, [x0, OFS_VCPU_GPR_X(18)]
	clear_guest_registers "16,17"
	stp	x20, x21, [x0, OFS_VCPU_GPR_X(20)]
	clear_guest_registers "18,19"
	stp	x22, x23, [x0, OFS_VCPU_GPR_X(22)]
	clear_guest_registers "20,21"
	stp	x24, x25, [x0, OFS_VCPU_GPR_X(24)]
	mrs	x1, ELR_EL2
	clear_guest_registers "22,23"
	stp	x26, x27, [x0, OFS_VCPU_GPR_X(26)]
	clear_guest_registers "24,25"
	stp	x28, x29, [x0, OFS_VCPU_GPR_X(28)]
	clear_guest_registers "26"
	stp	x30, x1, [x0, OFS_VCPU_GPR_X(30)]

	vcpu_pauth_entry_thread x0, x30, x27, x28, x29, x1
	clear_guest_registers "27,28,29,1"
.endm

// This macro zeros guest-vm register contents to make it harder for guest VM
// to influence branch prediction via gadgets in kernel code.
.macro	clear_guest_registers reglist
	.irp n,\reglist
	mov	x\n, xzr
	.endr
.endm

```

`hyp/vm/vcpu/armv8-64/src/return.S`:

```S
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <hypconstants.h>

#include <asm/asm_defs.inc>
#include <asm/panic.inc>

#include "vectors_el2.inc"
#include "vectors_vcpu.inc"


	.section	.text

// Returning from a guest exception
function vcpu_exception_return, align=32
	BRANCH_TARGET(c,)
	thread_get_self	x30, offset=OFS_THREAD_VCPU_REGS_GPR

	// Load ELR_EL2, SPSR_EL2
	ldp	x27, x28, [x30, OFS_VCPU_GPR_PC]

	vcpu_pauth_exit_thread x30, x0, x1, x28, x2

	// Restore the general purpose registers
	ldp	x0, x1, [x30, OFS_VCPU_GPR_X(0)]
	ldp	x2, x3, [x30, OFS_VCPU_GPR_X(2)]
	ldp	x4, x5, [x30, OFS_VCPU_GPR_X(4)]
	ldp	x6, x7, [x30, OFS_VCPU_GPR_X(6)]
	ldp	x8, x9, [x30, OFS_VCPU_GPR_X(8)]
	msr	ELR_EL2, x27
	ldp	x10, x11, [x30, OFS_VCPU_GPR_X(10)]
	ldp	x12, x13, [x30, OFS_VCPU_GPR_X(12)]
	ldp	x14, x15, [x30, OFS_VCPU_GPR_X(14)]
	ldp	x16, x17, [x30, OFS_VCPU_GPR_X(16)]
	msr	SPSR_EL2, x28
	ldp	x18, x19, [x30, OFS_VCPU_GPR_X(18)]
	ldp	x20, x21, [x30, OFS_VCPU_GPR_X(20)]
	ldp	x22, x23, [x30, OFS_VCPU_GPR_X(22)]
	ldp	x24, x25, [x30, OFS_VCPU_GPR_X(24)]
	ldp	x26, x27, [x30, OFS_VCPU_GPR_X(26)]
	ldp	x28, x29, [x30, OFS_VCPU_GPR_X(28)]
	ldr	x30, [x30, OFS_VCPU_GPR_X(30)]

	eret
function_end vcpu_exception_return

// Returning from a guest hypercall
//
// A fast call (where C returns in registers) may need to sanitise starting
// between X0 and X2. A slow call (where C returns in memory) always loads
// the first 8 registers from the stack, having sanitised them in memory
// before making the call.
function vcpu_hypercall_return_sanitize_x0, align=64
	mov	x0, xzr
function_chain vcpu_hypercall_return_sanitize_x0, vcpu_hypercall_return_sanitize_x1
	mov	x1, xzr
function_chain vcpu_hypercall_return_sanitize_x1, vcpu_hypercall_return_sanitize_x2
	mov	x2, xzr
	mov	x3, xzr
	mov	x4, xzr
	mov	x5, xzr
	mov	x6, xzr
	mov	x7, xzr
function_chain vcpu_hypercall_return_sanitize_x2, vcpu_hypercall_return_sanitize_x8
	// SP_EL2 points to the kernel stack in the thread structure

	// Restore ELR_EL2, SPSR_EL2
	ldp	x9, x10, [sp], #16

	vcpu_pauth_exit x19, x20, x21, x10, x22

	// Sanitise the AAPCS64-defined caller-saved registers to prevent
	// information leakage from compiled hypervisor code to the guest.
	// X30 is also caller-saved but was saved and restored above, as was
	// X18 which must be preserved at all times.

	// Restore caller saved registers
	ldp	x28, x29, [sp], #16
	mov	x11, xzr
	mov	x12, xzr
	ldp	x26, x27, [sp], #16
	msr	elr_el2, x9
	mov	x13, xzr
	ldp	x24, x25, [sp], #16
	msr	spsr_el2, x10
	mov	x14, xzr
	ldp	x22, x23, [sp], #16
	mov	x15, xzr
	mov	x16, xzr
	ldp	x20, x21, [sp], #16

	mov	x17, xzr

	// FIXME:
	// Temporarily preserve x8 for backwards compatibility. It should be
	// sanitised once this is removed.
	//mov	x8, xzr
	ldp	x8, x19, [sp], #16

	// Pop X18 and X30 that we had pushed on entry
	ldp	x18, x30, [sp], #16

	// Note: X9 = ELR_EL2 (not sensitive); X10 = SPSR_EL2 (possibly
	// sensitive and should be zeroed).
	mov	x10, xzr

	eret
function_end vcpu_hypercall_return_sanitize_x8

#if defined(ARCH_ARM_FEAT_PAuth)
function vcpu_pauth_exit_failed
	panic	"Invalid SPSR_EL2.M in VCPU exception return"
function_end vcpu_pauth_exit_failed
#endif

```

`hyp/vm/vcpu/armv8-64/src/vectors.S`:

```S
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <hypconstants.h>

#if defined(HYPERCALLS)
#include <hypcall_def.h>
#endif

#include <asm/asm_defs.inc>
#include <asm/cpu.h>
#include <asm/panic.inc>

#include "vectors_el2.inc"
#include "vectors_vcpu.inc"

.macro hypercall_decode_native el:req, ec:req
	// Extract the exception class
	mrs	x18, ESR_EL2
	lsr	w30, w18, 26

#if defined(HYPERCALLS)
	// Is it an HVC instruction?
	cmp	w30, \ec

	// Extract ESR_EL2.ISS (hypercall number)
	ubfx	x18, x18, 0, 16
	// Adjust in regards to the base hypercall number
	sub	x18, x18, HYPERCALL_BASE

	b.ne	LOCAL(\el\()_non_hypercall)
	// Is it a valid hypercall?
	cmp	x18, HYPERCALL_NUM
	b.hs	LOCAL(\el\()_non_hypercall)

	// FIXME:
	// Temporarily preserve x8 for backwards compatibility.
	stp	x8, x19, [sp, #-16]!

	// It's a native hypercall, we zero or trash HVC caller saved registers.
	// note: x9, x10, x11, x18, x30 are already overwritten here.
	clear_guest_registers "12,13,14"

	// Save and zero out callee saved registers. We don't let these get
	// passed as-is to the C code to prevent EL1 targeting EL2 gadgets.
	// Note: x19 saved above
	stp	x20, x21, [sp, #-16]!
	clear_guest_registers "15,16,17"
	stp	x22, x23, [sp, #-16]!
	clear_guest_registers "8,19"
	stp	x24, x25, [sp, #-16]!
	clear_guest_registers "20,21"
	stp	x26, x27, [sp, #-16]!
	clear_guest_registers "22,23"
	stp	x28, x29, [sp, #-16]!
	// moved to hypercall_64_entry (vector is full)
	// - clear_guest_registers "24,25,26,27,28,29"

	b	hypercall_64_entry
#endif

local \el\()_non_hypercall:
.endm

	.section	.text.vectors

	// The vector table base is 2KB aligned
	// (16 vectors, 32 instructions each)
	.balign		2048
.global vcpu_aarch64_vectors
vcpu_aarch64_vectors:

el2_vectors vcpu 1

// Guest vectors
// The assumption is that upon entry, SP (SP_EL2) points to the kernel stack,
// so we can start pushing to the stack without having to adjust SP first.
vector vector_guest64_sync
	disable_phys_access
	// First check whether this is a hypercall or some other exception, as
	// it affects which registers we need to save.

	stp	x18, x30, [sp, #-16]!

	hypercall_decode_native aarch64, ENUM_ESR_EC_HVC64_EL2

	// Not a hypercall, jump to the trap handler
	// Pop the original values of X18 and X30 as we need to save them in
	// the exception context
	ldp	x18, x30, [sp], #16

	b	exception_64_entry
vector_end vector_guest64_sync

vector vector_guest64_irq
	save_guest_context_vcpu_zero_0_13_irq

	b	irq_64_entry
vector_end vector_guest64_irq

vector vector_guest64_fiq
	// In current implementations, all FIQs should go directly to TrustZone
	// and thus if we ever get one, panic.
	save_kernel_context_full

	panic	"64-bit guest vectors"
vector_end vector_guest64_fiq

vector vector_guest64_serror
	save_guest_context_vcpu_zero_0_13_irq
	b	error_64_entry
vector_end vector_guest64_serror


#if ARCH_AARCH64_32BIT_EL1
vector vector_guest32_sync
	disable_phys_access
	// First check whether this is a hypercall or some other exception, as
	// it affects which registers we need to save.

	stp	x18, x30, [sp, #-16]!

	hypercall_decode_native aarch32, ENUM_ESR_EC_HVC32_EL2

	// Not a hypercall, jump to the trap handler
	// Pop the original values of X18 and X30 as we need to save them in
	// the exception context
	ldp	x18, x30, [sp], #16

	b	exception_32_entry
vector_end vector_guest32_sync

vector vector_guest32_irq
	save_kernel_context_vcpu_zero_0_13_irq
	b	irq_32_entry
vector_end vector_guest32_irq

vector vector_guest32_fiq
	// In current implementations, all FIQs should go directly to TrustZone
	// and thus if we ever get one, panic.
	save_kernel_context_full
	kernel_pauth_entry
	panic	"32-bit guest vectors"
vector_end vector_guest32_fiq

vector vector_guest32_serror
	save_kernel_context_vcpu_zero_0_13_irq
	bl	error_32_entry
vector_end vector_guest32_serror

#else // !ARCH_AARCH64_32BIT_EL1

// In theory we should never get to any of these
vector vector_guest32_sync
	save_kernel_context_full
	bl vector_guest32_panic
vector_end vector_guest32_sync

vector vector_guest32_irq
	save_kernel_context_full
	bl vector_guest32_panic
vector_end vector_guest32_irq

vector vector_guest32_fiq
	save_kernel_context_full
	bl vector_guest32_panic
vector_end vector_guest32_fiq

vector vector_guest32_serror
	save_kernel_context_full
	bl vector_guest32_panic
vector_end vector_guest32_serror

function vector_guest32_panic
	kernel_pauth_entry x0, x1, x2
	panic	"32-bit guest vectors"
function_end vector_guest32_panic

#endif


// Entry conditions:
//      x0..x7          - call arguments
//      x18             - hypercall number
//      x8,x12..x23     - zeroed
//      x9, x10, x11    - not zeroed/overwritten yet
//      x24..x29        - need to be zeroed
function hypercall_64_entry local, align=(1 << CPU_L1D_LINE_BITS)
	mrs	x10, elr_el2
	clear_guest_registers "24"
	mrs	x11, spsr_el2

	vcpu_pauth_entry x9, x25, x26, x27, x28, x29
	clear_guest_registers "25,26,27"

	// Get the hypercall table handler base address.
	adr	x9, hypercall_table

	// Save ELR_EL2 and SPSR_EL2.
	stp	x10, x11, [sp, #-16]!

	// Compute hypercall jump table index.
#if defined(ARCH_ARM_FEAT_BTI)
	add	x9, x9, x18, lsl 4
#else
	add	x9, x9, x18, lsl 3
#endif
	clear_guest_registers "28,29"

	// ELR_EL2, SPSR_EL2 are also VM controlled values, zero them as well.
	clear_guest_registers "10,11"

	// Jump to the hypercall table entry. The unused HVC argument
	// registers will be cleared before the handler is called.
	br	x9
function_end hypercall_64_entry

function exception_64_entry local, align=(1 << CPU_L1D_LINE_BITS)
	save_guest_exception_context_zero_1_29

	mov	x0, 1		// 1: AArch64
	bl	vcpu_exception_dispatch
	b	vcpu_exception_return
function_end exception_64_entry

function error_64_entry local, align=(1 << CPU_L1D_LINE_BITS)
	save_guest_context_vcpu_zero_14_29_irq

	bl	vcpu_error_dispatch
	b	vcpu_exception_return
function_end error_64_entry

function irq_64_entry local, align=(1 << CPU_L1D_LINE_BITS)
	save_guest_context_vcpu_zero_14_29_irq

	bl	vcpu_interrupt_dispatch
	b	vcpu_exception_return
function_end irq_64_entry

#if ARCH_AARCH64_32BIT_EL1
function exception_32_entry local, align=(1 << CPU_L1D_LINE_BITS)
	save_guest_exception_context_zero_1_29

	mov	x0, 0		// 0: AArch32
	bl	vcpu_exception_dispatch
	b	vcpu_exception_return
function_end exception_32_entry

function error_32_entry local, align=(1 << CPU_L1D_LINE_BITS)
	save_guest_context_vcpu_zero_14_29_irq

	bl	vcpu_error_dispatch
	b	vcpu_exception_return
function_end error_32_entry

function irq_32_entry local, align=(1 << CPU_L1D_LINE_BITS)
	save_guest_context_vcpu_zero_14_29_irq

	bl	vcpu_interrupt_dispatch
	b	vcpu_exception_return
function_end irq_32_entry
#endif

```

`hyp/vm/vcpu/armv8-64/templates/vectors_tramp.S.tmpl`:

```tmpl
// © 2022 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

\#include <hypconstants.h>

\#include <asm/asm_defs.inc>
\#include <asm/panic.inc>

\#include "vectors_el2.inc"

.macro mitigate_spectre_bhb_loop count:req
	stp	x18, xzr, [sp, #-16]!
	mov	x18, #\count

local bhb_flush_loop\@:
	b	LOCAL(bhb_flush_forward_jump\@)
local bhb_flush_forward_jump\@:
	subs	x18, x18, #1
	b.ne	LOCAL(bhb_flush_loop\@)

\#if defined(ARCH_ARM_FEAT_SB)
	sb
\#else
	dsb	nsh
	isb
\#endif
	ldp	x18, xzr, [sp], #16
.endm

.macro mitigate_spectre_bhb_clrbhb
	hint	#22
	isb
.endm

.macro vector_tramp name:req label:req content:req content_args:vararg
vector vector_\name\()_vcpu_tramp_\label
	\content \content_args
	b	vector_\name
vector_end vector_\name\()_vcpu_tramp_\label
.endm

.macro vcpu_vector name:req content:req content_args:vararg
	.section	.text.vectors;
	.balign		2048;
.global vcpu_aarch64_vectors_tramp_\name
vcpu_aarch64_vectors_tramp_\name:

	el2_vectors vcpu_tramp_\name 1

	vector_tramp guest64_sync \name \content \content_args
	vector_tramp guest64_irq \name \content \content_args
	vector_tramp guest64_fiq \name \content \content_args
	vector_tramp guest64_serror \name \content \content_args

	vector_tramp guest32_sync \name \content \content_args
	vector_tramp guest32_irq \name \content \content_args
	vector_tramp guest32_fiq \name \content \content_args
	vector_tramp guest32_serror \name \content \content_args
.endm

\#if defined(ARCH_ARM_FEAT_CLRBHB)
vcpu_vector clrbhb mitigate_spectre_bhb_clrbhb
\#endif
##
#set loop_counts = set()
#set target_cpu_ids = $ARCH_CORE_IDS.split(',')
##
#for cpu_id in $ARCH_CORE_IDS.split(',')
#set _loops = 'SPECTRE_{:s}_BHB_LOOP_FLUSH'.format(cpu_id)
#if self.varExists($_loops)
#set loops=int(self.getVar($_loops))
#silent loop_counts.add($loops)
#end if
#end for
##
#for loop_count in sorted(loop_counts)
#if loop_count > 0
vcpu_vector bhb_loop_${loop_count} mitigate_spectre_bhb_loop ${loop_count}
#end if
#end for

```

`hyp/vm/vcpu/armv8-64/templates/vectors_tramp.c.tmpl`:

```tmpl
// © 2022 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#set loop_counts = set()
#set loop_cores = dict()
#set unaffected_cores = dict()
#set unhandled_cores = dict()
#set hwworkaround_cores = dict()
#set target_cpu_ids = $ARCH_CORE_IDS.split(',')
##
#for cpu_id in $ARCH_CORE_IDS.split(',')
#set _nospec = 'SPECTRE_{:s}_NO_SPECULATION'.format(cpu_id)
#set _loops = 'SPECTRE_{:s}_BHB_LOOP_FLUSH'.format(cpu_id)
#set _bpiall = 'SPECTRE_{:s}_BPIALL'.format(cpu_id)
#set _hwwrk = 'SPECTRE_{:s}_BHB_HW_WORKAROUND'.format(cpu_id)
#if self.varExists($_loops)
#set loops=int(self.getVar($_loops))
#silent loop_counts.add($loops)
#silent loop_cores[$cpu_id] = $loops
#elif self.varExists($_bpiall)
#silent unhandled_cores[$cpu_id] = 1
#elif self.varExists($_hwwrk)
#silent hwworkaround_cores[$cpu_id] = 1
#elif self.varExists($_nospec)
#silent unaffected_cores[$cpu_id] = 1
#else
#silent print('Spectre BHB config missing for', cpu_id)
#silent sys.exit(1)
#end if
#end for
\#include <assert.h>
\#include <hyptypes.h>

\#include <hypregisters.h>

\#include <base.h>
\#include <compiler.h>
\#include <cpulocal.h>
\#include <log.h>
\#include <panic.h>
\#include <trace.h>

\#include "event_handlers.h"
\#include "vectors_vcpu.h"

CPULOCAL_DECLARE(uintptr_t, vcpu_aarch64_vectors);

extern uintptr_t vcpu_aarch64_vectors;
extern uintptr_t vcpu_aarch64_vectors_tramp_clrbhb;

#for loop_count in sorted(loop_counts)
extern uintptr_t vcpu_aarch64_vectors_tramp_bhb_loop_${loop_count};
#end for

void
vcpu_arch_handle_boot_cpu_cold_init(cpu_index_t cpu)
{
	core_id_t core_id = get_current_core_id();

	ID_AA64MMFR1_EL1_t mmfr1 = register_ID_AA64MMFR1_EL1_read();
\#if defined(ARCH_ARM_FEAT_CLRBHB)
	ID_AA64ISAR2_EL1_t isar2 = register_ID_AA64ISAR2_EL1_read();
\#endif

	if (ID_AA64MMFR1_EL1_get_ECBHB(&mmfr1) == 1U) {
		CPULOCAL_BY_INDEX(vcpu_aarch64_vectors, cpu) =
			(uintptr_t)&vcpu_aarch64_vectors;
	}
\#if defined(ARCH_ARM_FEAT_CLRBHB)
	else if (ID_AA64ISAR2_EL1_get_CLRBHB(&isar2) == 1U) {
		CPULOCAL_BY_INDEX(vcpu_aarch64_vectors, cpu) =
			(uintptr_t)&vcpu_aarch64_vectors_tramp_clrbhb;
	}
\#endif
	else {
\#pragma clang diagnostic push
\#pragma clang diagnostic ignored "-Wswitch-enum"
		switch(core_id) {
#for cpu_id in sorted(loop_cores):
		case CORE_ID_${cpu_id}:
			// BHB eviction loop in vector entry, ${loop_cores[cpu_id]} iterations
			CPULOCAL_BY_INDEX(vcpu_aarch64_vectors, cpu) =
				(uintptr_t)&vcpu_aarch64_vectors_tramp_bhb_loop_${loop_cores[cpu_id]};
			break;
#end for
#if len(unaffected_cores)
#for cpu_id in sorted(unaffected_cores):
		case CORE_ID_${cpu_id}:
#end for
			// Not vulnerable to Spectre-BHB
			CPULOCAL_BY_INDEX(vcpu_aarch64_vectors, cpu) =
				(uintptr_t)&vcpu_aarch64_vectors;
			break;
#end if
#if len(unhandled_cores):
#for cpu_id in sorted(unhandled_cores):
		case CORE_ID_${cpu_id}:
#end for
			// Needs an SMC call or switch to AArch32
			panic("No firmware support for spectre-BHB yet!");
#end if
#if len($hwworkaround_cores)
#for cpu_id in sorted(hwworkaround_cores):
		case CORE_ID_${cpu_id}:
#end for
			// Core is expected to have a HW workaround, ECBHB or
			// CLRBHB
\#if defined(ARCH_ARM_SPECTRE_BHB_WARN) && !defined(NDEBUG) && defined(VERBOSE) && VERBOSE
			CPULOCAL_BY_INDEX(vcpu_aarch64_vectors, cpu) =
				(uintptr_t)&vcpu_aarch64_vectors;
			LOG(ERROR, WARN,
			    "No spectre-BHB mitigation for unexpected core {:d}:{:d}",
			    cpu, (register_t)core_id);
			break;
\#else
			panic("No spectre-BHB mitigation for unexpected core");
\#endif
#end if
		default:
\#if defined(ARCH_ARM_SPECTRE_BHB_WARN) && !defined(NDEBUG) && defined(VERBOSE) && VERBOSE
			CPULOCAL_BY_INDEX(vcpu_aarch64_vectors, cpu) =
				(uintptr_t)&vcpu_aarch64_vectors;
			LOG(ERROR, WARN,
			    "No spectre-BHB mitigation registered for unknown core {:d}:{:d}",
			    cpu, (register_t)core_id);
			break;
\#else
			panic("No spectre-BHB mitigation registered for unknown core");
\#endif
		}
\#pragma clang diagnostic pop
	}
}

```

`hyp/vm/vcpu/armv8-64/vcpu_aarch64.ev`:

```ev
// © 2022 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

module vcpu

subscribe boot_cpu_cold_init
	handler vcpu_arch_handle_boot_cpu_cold_init

```

`hyp/vm/vcpu/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

interface vcpu

local_include
types vcpu.tc
events vcpu.ev
source vcpu.c

base_module hyp/core/vectors

arch_types aarch64 vcpu_aarch64.tc
arch_events aarch64 vcpu_aarch64.ev
arch_local_include aarch64
arch_source aarch64 sysreg_traps.c exception_inject.c reg_access.c wfi.c
arch_source aarch64 trap_dispatch.c aarch64_init.c context_switch.c
arch_events armv8-64 vcpu_aarch64.ev
arch_local_include armv8-64
arch_source armv8-64 vectors.S return.S

arch_hypercalls aarch64 hypercalls.hvc
arch_source aarch64 hypercalls.c

arch_template simple armv8-64 vectors_tramp.S.tmpl vectors_tramp.c.tmpl
arch_source cortex-a-v8_0 sysreg_traps_cpu.c
arch_source cortex-a-v8_0 context_switch.c
arch_types cortex-a-v8_0 vcpu_aarch64.tc
arch_source cortex-a-v8_2 sysreg_traps_cpu.c
arch_source cortex-a-v8_2 context_switch.c
arch_types cortex-a-v8_2 vcpu_aarch64.tc
arch_source cortex-a-v9 sysreg_traps_cpu.c
arch_source cortex-a-v9 context_switch.c
arch_types cortex-a-v9 vcpu_aarch64.tc
arch_source qemu-armv8-5a-rng sysreg_traps_cpu.c
arch_source qemu-armv8-5a-rng context_switch.c

```

`hyp/vm/vcpu/cortex-a-v8_0/src/context_switch.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <hyptypes.h>

#include <hypregisters.h>

#include <compiler.h>
#include <thread.h>

#include <asm/sysregs.h>

#include "event_handlers.h"

void
vcpu_context_switch_cpu_load(void)
{
	thread_t *thread = thread_get_self();

	if (compiler_expected(thread->kind == THREAD_KIND_VCPU)) {
		// No-op
	}
}

void
vcpu_context_switch_cpu_save(void)
{
	thread_t *thread = thread_get_self();

	if (compiler_expected(thread->kind == THREAD_KIND_VCPU)) {
		// No-op
	}
}

```

`hyp/vm/vcpu/cortex-a-v8_0/src/sysreg_traps_cpu.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>

#include <thread.h>
#include <vcpu.h>

#include <asm/system_registers.h>
#include <asm/system_registers_cpu.h>

#include "event_handlers.h"

vcpu_trap_result_t
sysreg_read_cpu(ESR_EL2_ISS_MSR_MRS_t iss)
{
	(void)iss;
	return VCPU_TRAP_RESULT_UNHANDLED;
}

// ACTLR_EL2 defaults to zero on reset, which disables write access to these
// registers and traps them to EL2. We want to keep it that way for now as
// writing to these registers generally has dangerous side effects and we don't
// want the guest to mess with them.
vcpu_trap_result_t
sysreg_write_cpu(ESR_EL2_ISS_MSR_MRS_t iss)
{
	uint8_t		   opc0, opc1, crn, crm;
	vcpu_trap_result_t ret = VCPU_TRAP_RESULT_EMULATED;

	// Assert this is a write
	assert(!ESR_EL2_ISS_MSR_MRS_get_Direction(&iss));

	// Remove the fields that are not used in the comparison
	ESR_EL2_ISS_MSR_MRS_t temp_iss = iss;
	ESR_EL2_ISS_MSR_MRS_set_Rt(&temp_iss, 0U);
	ESR_EL2_ISS_MSR_MRS_set_Direction(&temp_iss, false);

	switch (ESR_EL2_ISS_MSR_MRS_raw(temp_iss)) {
	case ISS_MRS_MSR_CPUACTLR_EL1:
	// CPUACTLR2_EL1 does not exist on A55
	case ISS_MRS_MSR_CPUECTLR_EL1:
		// WI
		break;

	default:
		opc0 = ESR_EL2_ISS_MSR_MRS_get_Op0(&iss);
		opc1 = ESR_EL2_ISS_MSR_MRS_get_Op1(&iss);
		crn  = ESR_EL2_ISS_MSR_MRS_get_CRn(&iss);
		crm  = ESR_EL2_ISS_MSR_MRS_get_CRm(&iss);

		if ((opc0 == 3) && (opc1 == 0) && (crn == 15) && (crm >= 3) &&
		    (crm <= 4)) {
			// CLUSTER* registers, all WI.
		} else if ((opc0 == 3) && ((opc1 == 0) || (opc1 == 6)) &&
			   (crn == 15) && (crm >= 5) && (crm <= 6)) {
			// CLUSTERPM* registers, all WI.
		} else {
			ret = VCPU_TRAP_RESULT_UNHANDLED;
		}
		break;
	}

	return ret;
}

```

`hyp/vm/vcpu/cortex-a-v8_0/vcpu_aarch64.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// Extend vcpu_el1_registers struct here if required for cpu/family specific
// EL1/0 registers, and add context switch handling.

```

`hyp/vm/vcpu/cortex-a-v8_2/src/context_switch.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <hyptypes.h>

#include <hypregisters.h>

#include <compiler.h>
#include <thread.h>

#include <asm/sysregs.h>

#include "event_handlers.h"

void
vcpu_context_switch_cpu_load(void)
{
	thread_t *thread = thread_get_self();

	if (compiler_expected(thread->kind == THREAD_KIND_VCPU)) {
		// No-op
	}
}

void
vcpu_context_switch_cpu_save(void)
{
	thread_t *thread = thread_get_self();

	if (compiler_expected(thread->kind == THREAD_KIND_VCPU)) {
		// No-op
	}
}

```

`hyp/vm/vcpu/cortex-a-v8_2/src/sysreg_traps_cpu.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>

#include <thread.h>
#include <vcpu.h>

#include <asm/system_registers.h>
#include <asm/system_registers_cpu.h>

#include "event_handlers.h"

vcpu_trap_result_t
sysreg_read_cpu(ESR_EL2_ISS_MSR_MRS_t iss)
{
	(void)iss;
	return VCPU_TRAP_RESULT_UNHANDLED;
}

// ACTLR_EL2 defaults to zero on reset, which disables write access to these
// registers and traps them to EL2. We want to keep it that way for now as
// writing to these registers generally has dangerous side effects and we don't
// want the guest to mess with them.
vcpu_trap_result_t
sysreg_write_cpu(ESR_EL2_ISS_MSR_MRS_t iss)
{
	uint8_t		   opc0, opc1, crn, crm;
	vcpu_trap_result_t ret = VCPU_TRAP_RESULT_EMULATED;

	// Assert this is a write
	assert(!ESR_EL2_ISS_MSR_MRS_get_Direction(&iss));

	// Remove the fields that are not used in the comparison
	ESR_EL2_ISS_MSR_MRS_t temp_iss = iss;
	ESR_EL2_ISS_MSR_MRS_set_Rt(&temp_iss, 0U);
	ESR_EL2_ISS_MSR_MRS_set_Direction(&temp_iss, false);

	switch (ESR_EL2_ISS_MSR_MRS_raw(temp_iss)) {
	case ISS_MRS_MSR_CPUACTLR_EL1:
	// CPUACTLR2_EL1 does not exist on A55
	case ISS_MRS_MSR_A7X_CPUACTLR2_EL1:
	case ISS_MRS_MSR_CPUECTLR_EL1:
	case ISS_MRS_MSR_CPUPWRCTLR_EL1:
		// WI
		break;

	default:
		opc0 = ESR_EL2_ISS_MSR_MRS_get_Op0(&iss);
		opc1 = ESR_EL2_ISS_MSR_MRS_get_Op1(&iss);
		crn  = ESR_EL2_ISS_MSR_MRS_get_CRn(&iss);
		crm  = ESR_EL2_ISS_MSR_MRS_get_CRm(&iss);

		if ((opc0 == 3) && (opc1 == 0) && (crn == 15) && (crm >= 3) &&
		    (crm <= 4)) {
			// CLUSTER* registers, all WI.
		} else if ((opc0 == 3) && ((opc1 == 0) || (opc1 == 6)) &&
			   (crn == 15) && (crm >= 5) && (crm <= 6)) {
			// CLUSTERPM* registers, all WI.
		} else {
			ret = VCPU_TRAP_RESULT_UNHANDLED;
		}
		break;
	}

	return ret;
}

```

`hyp/vm/vcpu/cortex-a-v8_2/vcpu_aarch64.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// Extend vcpu_el1_registers struct here if required for cpu/family specific
// EL1/0 registers, and add context switch handling.

```

`hyp/vm/vcpu/cortex-a-v9/src/context_switch.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <hyptypes.h>

#include <hypregisters.h>

#include <compiler.h>
#include <thread.h>

#include <asm/sysregs.h>

#include "event_handlers.h"

void
vcpu_context_switch_cpu_load(void)
{
	thread_t *thread = thread_get_self();

	if (compiler_expected(thread->kind == THREAD_KIND_VCPU)) {
		// No-op
	}
}

void
vcpu_context_switch_cpu_save(void)
{
	thread_t *thread = thread_get_self();

	if (compiler_expected(thread->kind == THREAD_KIND_VCPU)) {
		// No-op
	}
}

```

`hyp/vm/vcpu/cortex-a-v9/src/sysreg_traps_cpu.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>

#include <thread.h>
#include <vcpu.h>

#include <asm/system_registers.h>
#include <asm/system_registers_cpu.h>

#include "event_handlers.h"

vcpu_trap_result_t
sysreg_read_cpu(ESR_EL2_ISS_MSR_MRS_t iss)
{
	(void)iss;
	return VCPU_TRAP_RESULT_UNHANDLED;
}

// ACTLR_EL2 defaults to zero on reset, which disables write access to these
// registers and traps them to EL2. We want to keep it that way for now as
// writing to these registers generally has dangerous side effects and we don't
// want the guest to mess with them.
vcpu_trap_result_t
sysreg_write_cpu(ESR_EL2_ISS_MSR_MRS_t iss)
{
	uint8_t		   opc0, opc1, crn, crm;
	vcpu_trap_result_t ret = VCPU_TRAP_RESULT_EMULATED;

	// Assert this is a write
	assert(!ESR_EL2_ISS_MSR_MRS_get_Direction(&iss));

	// Remove the fields that are not used in the comparison
	ESR_EL2_ISS_MSR_MRS_t temp_iss = iss;
	ESR_EL2_ISS_MSR_MRS_set_Rt(&temp_iss, 0U);
	ESR_EL2_ISS_MSR_MRS_set_Direction(&temp_iss, false);

	switch (ESR_EL2_ISS_MSR_MRS_raw(temp_iss)) {
	case ISS_MRS_MSR_CPUACTLR_EL1:
	case ISS_MRS_MSR_A7X_CPUACTLR2_EL1:
	case ISS_MRS_MSR_CPUECTLR_EL1:
	case ISS_MRS_MSR_CPUPWRCTLR_EL1:
		// WI
		break;

	default:
		opc0 = ESR_EL2_ISS_MSR_MRS_get_Op0(&iss);
		opc1 = ESR_EL2_ISS_MSR_MRS_get_Op1(&iss);
		crn  = ESR_EL2_ISS_MSR_MRS_get_CRn(&iss);
		crm  = ESR_EL2_ISS_MSR_MRS_get_CRm(&iss);

		if ((opc0 == 3U) && (opc1 == 0U) && (crn == 15U) &&
		    (crm >= 3U) && (crm <= 4U)) {
			// CLUSTER* registers, all WI.
		} else if ((opc0 == 3U) && ((opc1 == 0U) || (opc1 == 6U)) &&
			   (crn == 15U) && (crm >= 5U) && (crm <= 6U)) {
			// CLUSTERPM* registers, all WI.
		} else {
			ret = VCPU_TRAP_RESULT_UNHANDLED;
		}
		break;
	}

	return ret;
}

```

`hyp/vm/vcpu/cortex-a-v9/vcpu_aarch64.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// Extend vcpu_el1_registers struct here if required for cpu/family specific
// EL1/0 registers, and add context switch handling.

```

`hyp/vm/vcpu/include/vcpu.h`:

```h
// © 2022 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

error_t
vcpu_bind_virq(thread_t *vcpu, vic_t *vic, virq_t virq,
	       vcpu_virq_type_t virq_type);

error_t
vcpu_unbind_virq(thread_t *vcpu, vcpu_virq_type_t virq_type);

```

`hyp/vm/vcpu/qemu-armv8-5a-rng/src/context_switch.c`:

```c
// © 2022 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <hyptypes.h>

#include <hypregisters.h>

#include <compiler.h>
#include <thread.h>

#include <asm/sysregs.h>

#include "event_handlers.h"

// There are no implementation specific EL1 registers to switch for QEMU.

void
vcpu_context_switch_cpu_load(void)
{
	thread_t *thread = thread_get_self();

	if (compiler_expected(thread->kind == THREAD_KIND_VCPU)) {
		// No-op
	}
}

void
vcpu_context_switch_cpu_save(void)
{
	thread_t *thread = thread_get_self();

	if (compiler_expected(thread->kind == THREAD_KIND_VCPU)) {
		// No-op
	}
}

```

`hyp/vm/vcpu/qemu-armv8-5a-rng/src/sysreg_traps_cpu.c`:

```c
// © 2022 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>

#include <thread.h>
#include <vcpu.h>

#include <asm/system_registers.h>
// #include <asm/system_registers_cpu.h>

#include "event_handlers.h"

// There are no implementation specific EL1 registers to emulate for QEMU.

vcpu_trap_result_t
sysreg_read_cpu(ESR_EL2_ISS_MSR_MRS_t iss)
{
	(void)iss;
	return VCPU_TRAP_RESULT_UNHANDLED;
}

vcpu_trap_result_t
sysreg_write_cpu(ESR_EL2_ISS_MSR_MRS_t iss)
{
	(void)iss;
	return VCPU_TRAP_RESULT_UNHANDLED;
}

```

`hyp/vm/vcpu/src/vcpu.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>

#include <hypcontainers.h>

#include <cpulocal.h>
#include <panic.h>
#include <platform_cpu.h>
#include <preempt.h>
#include <scheduler.h>
#include <util.h>
#include <vcpu.h>
#include <vic.h>
#include <virq.h>

#include <events/vcpu.h>

#include "event_handlers.h"
#include "vcpu.h"

void
vcpu_handle_object_get_defaults_thread(thread_create_t *create)
{
	uint32_t stack_size;

	assert(create != NULL);

	// This may be 0, which will fall back to the global default
	stack_size = platform_cpu_stack_size();

#if defined(VCPU_MIN_STACK_SIZE)
	stack_size = util_max(stack_size, VCPU_MIN_STACK_SIZE);
#endif

	assert((stack_size == 0U) ||
	       util_is_baligned(stack_size, PGTABLE_HYP_PAGE_SIZE));
	assert(stack_size <= THREAD_STACK_MAX_SIZE);

	create->stack_size = stack_size;
	create->kind	   = THREAD_KIND_VCPU;
}

error_t
vcpu_handle_object_create_thread(thread_create_t create)
{
	thread_t *thread = create.thread;
	assert(thread != NULL);
	error_t ret;

	if (thread->kind == THREAD_KIND_VCPU) {
		scheduler_block_init(thread, SCHEDULER_BLOCK_VCPU_OFF);
	}

	if (create.scheduler_priority_valid &&
	    (create.scheduler_priority > VCPU_MAX_PRIORITY)) {
		ret = ERROR_DENIED;
	} else {
		ret = OK;
	}

	return ret;
}

error_t
vcpu_handle_object_activate_thread(thread_t *thread)
{
	error_t ret = OK;

	assert(thread != NULL);

	if (thread->kind == THREAD_KIND_VCPU) {
		if (thread->cspace_cspace == NULL) {
			ret = ERROR_OBJECT_CONFIG;
			goto out;
		}

		if (cpulocal_index_valid(thread->scheduler_affinity) &&
		    !platform_cpu_exists(thread->scheduler_affinity)) {
			ret = ERROR_OBJECT_CONFIG;
			goto out;
		}

		// Reset thread's vcpu_options. Event handlers can set them
		// again. This prevents unchecked options from configure phase
		// being left in the thread options.
		vcpu_option_flags_t options = thread->vcpu_options;
		thread->vcpu_options	    = vcpu_option_flags_default();

		if (!trigger_vcpu_activate_thread_event(thread, options)) {
			ret = ERROR_OBJECT_CONFIG;
		}
	}

out:
	return ret;
}

void
vcpu_handle_thread_exited(void)
{
	thread_t *current = thread_get_self();
	assert(current != NULL);

	assert_preempt_disabled();

	if (current->kind == THREAD_KIND_VCPU) {
		if (vcpu_option_flags_get_critical(&current->vcpu_options)) {
			panic("Critical VCPU exited");
		}

		trigger_vcpu_stopped_event();
	}
}

bool
vcpu_handle_vcpu_activate_thread(thread_t *thread, vcpu_option_flags_t options)
{
	bool ret = false;

	assert(thread != NULL);
	assert(thread->kind == THREAD_KIND_VCPU);

	// Check that the partition has the right to mark the VCPU as critical.
	if (vcpu_option_flags_get_critical(&options) ||
	    vcpu_option_flags_get_hlos_vm(&options)) {
		if (!partition_option_flags_get_privileged(
			    &thread->header.partition->options)) {
			goto out;
		}

		vcpu_option_flags_set_critical(&thread->vcpu_options, true);
	}

	ret = true;

out:
	return ret;
}

void
vcpu_handle_object_deactivate_thread(thread_t *thread)
{
	if (thread->kind == THREAD_KIND_VCPU) {
		vic_unbind(&thread->vcpu_halt_virq_src);
	}
}

error_t
vcpu_bind_virq(thread_t *vcpu, vic_t *vic, virq_t virq,
	       vcpu_virq_type_t virq_type)
{
	return trigger_vcpu_bind_virq_event(virq_type, vcpu, vic, virq);
}

error_t
vcpu_unbind_virq(thread_t *vcpu, vcpu_virq_type_t virq_type)
{
	return trigger_vcpu_unbind_virq_event(virq_type, vcpu);
}

error_t
vcpu_handle_vcpu_bind_virq(thread_t *vcpu, vic_t *vic, virq_t virq)
{
	error_t err = vic_bind_shared(&vcpu->vcpu_halt_virq_src, vic, virq,
				      VIRQ_TRIGGER_VCPU_HALT);

	return err;
}

error_t
vcpu_handle_vcpu_unbind_virq(thread_t *vcpu)
{
	vic_unbind_sync(&vcpu->vcpu_halt_virq_src);

	return OK;
}

irq_trigger_result_t
vcpu_handle_virq_set_mode(void)
{
	return irq_trigger_result_ok(IRQ_TRIGGER_EDGE_RISING);
}

```

`hyp/vm/vcpu/vcpu.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

module vcpu

subscribe object_get_defaults_thread

subscribe object_create_thread

// Run (second) last, the handler configures vcpu options through its own
// setup_event which can't unwind correctly unless it is last here.
subscribe object_activate_thread
	priority -100

subscribe thread_exited
	require_preempt_disabled

#if defined(MODULE_VM_VCPU_RUN)
subscribe vcpu_run_check(vcpu, state_data_0)
	priority last
#endif

subscribe virq_set_mode[VIRQ_TRIGGER_VCPU_HALT]
	handler vcpu_handle_virq_set_mode()

subscribe vcpu_bind_virq[VCPU_VIRQ_TYPE_HALT](vcpu, vic, virq)

subscribe vcpu_unbind_virq[VCPU_VIRQ_TYPE_HALT](vcpu)

interface vcpu

setup_event vcpu_activate_thread
	param thread: thread_t *
	param options: vcpu_option_flags_t
	return: bool = true
	success: true

```

`hyp/vm/vcpu/vcpu.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// Reserve highest possible priority for EL2 tasks only (typically used for
// tasks that execute prior to boot)
define VCPU_MAX_PRIORITY public constant type priority_t =
	SCHEDULER_MAX_PRIORITY - 1;

define vcpu object {
};

extend thread object module vcpu {
	regs		object vcpu;
	// The option variable used for the hypercall_vcpu_configure hypercall.
	options		bitfield vcpu_option_flags(group(context_switch, x));
	flags		bitfield vcpu_runtime_flags(group(context_switch, x));
	halt_virq_src	structure virq_source(contained);
};

extend thread_kind enumeration {
	// Having THREAD_KIND_VCPU as 0 and using the correct compiler expected
	// and unexpected directives helps with optimising the context-switch
	// path and any other code that checks the thread kind against this
	// value, replacing the "cmp/b.ne" pair with a single "cbz" or "cbnz".
	vcpu = 0;
};

extend scheduler_block enumeration {
	vcpu_off;
	vcpu_suspend;
	vcpu_wfi;
	vcpu_fault;
};

extend hyp_api_flags0 bitfield {
	delete	vcpu;
	5	vcpu bool = 1;
};

extend abort_reason enumeration {
	UNHANDLED_EXCEPTION;
};

extend vcpu_virq_type enumeration {
	halt = 0;
};

extend virq_trigger enumeration {
	vcpu_halt;
};

```

`hyp/vm/vcpu_power/build.conf`:

```conf
# © 2022 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

types vcpu_power.tc
events vcpu_power.ev
source vcpu_power.c

```

`hyp/vm/vcpu_power/src/vcpu_power.c`:

```c
// © 2022 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>

#include <cpulocal.h>
#include <power.h>
#include <scheduler.h>
#include <thread.h>

#if defined(INTERFACE_VCPU_RUN)
#include <vcpu.h>
#include <vcpu_run.h>
#endif

#include "event_handlers.h"

error_t
vcpu_power_handle_vcpu_poweron(thread_t *vcpu)
{
	assert((vcpu != NULL) && !vcpu->vcpu_power_should_vote);
	vcpu->vcpu_power_should_vote = true;

	cpu_index_t cpu	     = scheduler_get_affinity(vcpu);
	bool	    can_vote = cpulocal_index_valid(cpu);

#if defined(INTERFACE_VCPU_RUN)
	if (vcpu_run_is_enabled(vcpu)) {
		can_vote = false;
	}
#endif

	error_t ret;
	if (can_vote) {
		ret = power_vote_cpu_on(cpu);
	} else {
		ret = OK;
	}

	return ret;
}

error_t
vcpu_power_handle_vcpu_poweroff(thread_t *vcpu)
{
	assert((vcpu != NULL) && vcpu->vcpu_power_should_vote);
	vcpu->vcpu_power_should_vote = false;

	cpu_index_t cpu	     = scheduler_get_affinity(vcpu);
	bool	    can_vote = cpulocal_index_valid(cpu);
#if defined(INTERFACE_VCPU_RUN)
	if (vcpu_run_is_enabled(vcpu)) {
		can_vote = false;
	}
#endif

	if (can_vote) {
		power_vote_cpu_off(cpu);
	}

	return OK;
}

void
vcpu_power_handle_vcpu_stopped(void)
{
	thread_t *vcpu = thread_get_self();
	assert((vcpu != NULL) && (vcpu->kind == THREAD_KIND_VCPU));

	scheduler_lock_nopreempt(vcpu);

	if (vcpu->vcpu_power_should_vote) {
		vcpu->vcpu_power_should_vote = false;

		cpu_index_t cpu	     = scheduler_get_affinity(vcpu);
		bool	    can_vote = cpulocal_index_valid(cpu);
#if defined(INTERFACE_VCPU_RUN)
		if (vcpu_run_is_enabled(vcpu)) {
			can_vote = false;
		}
#endif

		if (can_vote) {
			power_vote_cpu_off(cpu);
		}
	}

	scheduler_unlock_nopreempt(vcpu);
}

#if defined(INTERFACE_VCPU_RUN)
void
vcpu_power_handle_vcpu_run_enabled(thread_t *vcpu)
{
	cpu_index_t cpu	     = scheduler_get_affinity(vcpu);
	bool	    can_vote = cpulocal_index_valid(cpu);

	if (can_vote && vcpu->vcpu_power_should_vote) {
		power_vote_cpu_off(cpu);
	}
}
#endif

error_t
vcpu_power_handle_scheduler_set_affinity_prepare(thread_t   *vcpu,
						 cpu_index_t prev_cpu,
						 cpu_index_t next_cpu)
{
	error_t ret = OK;
	assert(prev_cpu != next_cpu);

	if (vcpu->kind != THREAD_KIND_VCPU) {
		goto out;
	}

#if defined(INTERFACE_VCPU_RUN)
	if (vcpu_run_is_enabled(vcpu)) {
		goto out;
	}
#endif

	if (vcpu->vcpu_power_should_vote) {
		if (cpulocal_index_valid(next_cpu)) {
			ret = power_vote_cpu_on(next_cpu);
		}
		if ((ret == OK) && cpulocal_index_valid(prev_cpu)) {
			power_vote_cpu_off(prev_cpu);
		}
	}

out:
	return ret;
}

```

`hyp/vm/vcpu_power/vcpu_power.ev`:

```ev
// © 2022 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

module vcpu_power

subscribe vcpu_poweron(vcpu)
	priority first
	require_scheduler_lock(vcpu)

subscribe vcpu_poweroff(current)
	require_scheduler_lock(current)

subscribe vcpu_stopped()
	require_preempt_disabled

#if defined(INTERFACE_VCPU_RUN)

subscribe vcpu_run_enabled
	require_scheduler_lock(vcpu)

#endif

subscribe scheduler_set_affinity_prepare(thread, prev_cpu, next_cpu)
	// Run late to avoid unwinding.
	priority last
	require_scheduler_lock(thread)

```

`hyp/vm/vcpu_power/vcpu_power.tc`:

```tc
// © 2022 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

extend thread object module vcpu_power {
	// True if this VCPU is currently powered on and not stopped; i.e. it
	// should be voting for power on of any CPU it has static affinity to.
	// This is protected by the VCPU's scheduler lock.
	should_vote		bool;
};

```

`hyp/vm/vcpu_run/build.conf`:

```conf
# © 2022 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

interface vcpu_run
types vcpu_run.tc
events vcpu_run.ev
source vcpu_run.c

```

`hyp/vm/vcpu_run/src/vcpu_run.c`:

```c
// © 2022 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>

#include <hypcall_def.h>
#include <hypcontainers.h>
#include <hyprights.h>

#include <atomic.h>
#include <compiler.h>
#include <cpulocal.h>
#include <cspace.h>
#include <cspace_lookup.h>
#include <object.h>
#include <preempt.h>
#include <scheduler.h>
#include <spinlock.h>
#include <task_queue.h>
#include <vcpu.h>
#include <vcpu_run.h>
#include <vic.h>
#include <virq.h>

#include <events/vcpu_run.h>

#include "event_handlers.h"

bool
vcpu_run_handle_vcpu_activate_thread(thread_t		*thread,
				     vcpu_option_flags_t options)
{
	assert(thread != NULL);

	if (thread->kind == THREAD_KIND_VCPU) {
		task_queue_init(&thread->vcpu_run_wakeup_virq_task,
				TASK_QUEUE_CLASS_VCPU_RUN_WAKEUP_VIRQ);

		thread->vcpu_run_last_state = VCPU_RUN_STATE_READY;
	}

	if (vcpu_option_flags_get_vcpu_run_scheduled(&options)) {
		vcpu_option_flags_set_vcpu_run_scheduled(&thread->vcpu_options,
							 true);
		scheduler_lock(thread);
		scheduler_block(thread, SCHEDULER_BLOCK_VCPU_RUN);
		scheduler_unlock(thread);

		thread->vcpu_run_enabled = true;
	}

	return true;
}

bool
vcpu_run_is_enabled(const thread_t *vcpu)
{
	return vcpu->vcpu_run_enabled;
}

static vcpu_run_state_t
do_vcpu_run_check(const thread_t *vcpu, register_t *state_data_0,
		  register_t *state_data_1, register_t *state_data_2)
	REQUIRE_SCHEDULER_LOCK(vcpu)
{
	vcpu_run_state_t ret;

	assert(vcpu->kind == THREAD_KIND_VCPU);
	assert(!scheduler_is_runnable(vcpu));

	thread_state_t state = atomic_load_relaxed(&vcpu->state);
	if (compiler_expected(state == THREAD_STATE_READY)) {
		ret = trigger_vcpu_run_check_event(vcpu, state_data_0,
						   state_data_1, state_data_2);
	} else if (state == THREAD_STATE_EXITED) {
		vcpu_run_poweroff_flags_t flags =
			vcpu_run_poweroff_flags_default();
		ret = VCPU_RUN_STATE_POWERED_OFF;
		vcpu_run_poweroff_flags_set_exited(&flags, true);
		*state_data_0 = vcpu_run_poweroff_flags_raw(flags);
	} else {
		assert(state == THREAD_STATE_KILLED);
		// The killed VCPU must be run until it has exited, and should
		// only ever be blocked for hypervisor internal reasons.
		ret = VCPU_RUN_STATE_BLOCKED;
	}

	return ret;
}

hypercall_vcpu_run_result_t
hypercall_vcpu_run(cap_id_t vcpu_cap_id, register_t resume_data_0,
		   register_t resume_data_1, register_t resume_data_2)
{
	hypercall_vcpu_run_result_t ret	   = { 0 };
	cspace_t		   *cspace = cspace_get_self();

	thread_ptr_result_t thread_r = cspace_lookup_thread(
		cspace, vcpu_cap_id,
		cap_rights_thread_union(CAP_RIGHTS_THREAD_AFFINITY,
					CAP_RIGHTS_THREAD_YIELD_TO));
	if (compiler_unexpected(thread_r.e != OK)) {
		ret.error = thread_r.e;
		goto out_err;
	}

	thread_t *vcpu = thread_r.r;
	if (compiler_unexpected(vcpu->kind != THREAD_KIND_VCPU)) {
		ret.error = ERROR_ARGUMENT_INVALID;
		goto out_obj_put_thread;
	}

	scheduler_lock(vcpu);
	if (!scheduler_is_blocked(vcpu, SCHEDULER_BLOCK_VCPU_RUN)) {
		// VCPU not proxy-scheduled, or is being run by another caller
		ret.error = ERROR_BUSY;
		goto unlock;
	}
	assert(vcpu_run_is_enabled(vcpu));

	ret.error = trigger_vcpu_run_resume_event(vcpu->vcpu_run_last_state,
						  vcpu, resume_data_0,
						  resume_data_1, resume_data_2);
	if (ret.error != OK) {
		goto unlock;
	}

	(void)scheduler_unblock(vcpu, SCHEDULER_BLOCK_VCPU_RUN);

	if (scheduler_is_runnable(vcpu)) {
		assert_cpulocal_safe();
		cpu_index_t this_pcpu = cpulocal_get_index();
		// Make sure the vCPU will run on this PCPU. Note that this
		// might block the thread for an RCU grace period, which will
		// show up as a brief transient VCPU_RUN_STATE_BLOCKED. To
		// prevent that persisting indefinitely, the caller should avoid
		// migration as much as possible.
		ret.error = scheduler_set_affinity(vcpu, this_pcpu);
		if (ret.error != OK) {
			goto unlock;
		}

		// Use a nopreempt unlock to make sure we don't get migrated
		scheduler_unlock_nopreempt(vcpu);
		scheduler_yield_to(vcpu);
		scheduler_lock_nopreempt(vcpu);
	}

	if (scheduler_is_runnable(vcpu)) {
		ret.vcpu_state = VCPU_RUN_STATE_READY;
	} else {
		ret.vcpu_state = do_vcpu_run_check(vcpu, &ret.state_data_0,
						   &ret.state_data_1,
						   &ret.state_data_2);
	}
	vcpu->vcpu_run_last_state = ret.vcpu_state;

	scheduler_block(vcpu, SCHEDULER_BLOCK_VCPU_RUN);
unlock:
	scheduler_unlock(vcpu);
out_obj_put_thread:
	object_put_thread(vcpu);
out_err:
	return ret;
}

hypercall_vcpu_run_check_result_t
hypercall_vcpu_run_check(cap_id_t vcpu_cap_id)
{
	hypercall_vcpu_run_check_result_t ret	 = { 0 };
	cspace_t			 *cspace = cspace_get_self();

	thread_ptr_result_t thread_r = cspace_lookup_thread(
		cspace, vcpu_cap_id,
		cap_rights_thread_union(CAP_RIGHTS_THREAD_BIND_VIRQ,
					CAP_RIGHTS_THREAD_STATE));
	if (compiler_unexpected(thread_r.e != OK)) {
		ret.error = thread_r.e;
		goto out_err;
	}

	thread_t *vcpu = thread_r.r;
	if (compiler_unexpected(vcpu->kind != THREAD_KIND_VCPU)) {
		ret.error = ERROR_ARGUMENT_INVALID;
		goto out_obj_put_thread;
	}

	scheduler_lock(vcpu);
	if (scheduler_is_runnable(vcpu)) {
		ret.error = ERROR_BUSY;
	} else {
		ret.vcpu_state = do_vcpu_run_check(vcpu, &ret.state_data_0,
						   &ret.state_data_1,
						   &ret.state_data_2);
		if (ret.vcpu_state == VCPU_RUN_STATE_BLOCKED) {
			ret.error = ERROR_BUSY;
		}
	}
	scheduler_unlock(vcpu);

out_obj_put_thread:
	object_put_thread(vcpu);
out_err:
	return ret;
}

error_t
vcpu_run_handle_vcpu_bind_virq(thread_t *vcpu, vic_t *vic, virq_t virq)
{
	error_t err;

	scheduler_lock(vcpu);

	if (scheduler_is_running(vcpu)) {
		err = ERROR_BUSY;
		goto out_unlock;
	}

	err = vic_bind_shared(&vcpu->vcpu_run_wakeup_virq, vic, virq,
			      VIRQ_TRIGGER_VCPU_RUN_WAKEUP);

	if ((err == OK) && !vcpu_run_is_enabled(vcpu)) {
		scheduler_block(vcpu, SCHEDULER_BLOCK_VCPU_RUN);
		vcpu->vcpu_run_enabled = true;
		trigger_vcpu_run_enabled_event(vcpu);
	}

out_unlock:
	scheduler_unlock(vcpu);

	return err;
}

error_t
vcpu_run_handle_vcpu_unbind_virq(thread_t *vcpu)
{
	vic_unbind_sync(&vcpu->vcpu_run_wakeup_virq);

	return OK;
}

error_t
vcpu_run_handle_task_queue_execute(task_queue_entry_t *task_entry)
{
	assert(task_entry != NULL);
	thread_t *vcpu =
		thread_container_of_vcpu_run_wakeup_virq_task(task_entry);

	assert(vcpu != NULL);
	assert(vcpu->kind == THREAD_KIND_VCPU);

	(void)virq_assert(&vcpu->vcpu_run_wakeup_virq, true);
	object_put_thread(vcpu);

	return OK;
}

void
vcpu_run_trigger_virq(thread_t *vcpu)
{
	assert(vcpu != NULL);
	assert(vcpu->kind == THREAD_KIND_VCPU);

	if (scheduler_is_blocked(vcpu, SCHEDULER_BLOCK_VCPU_RUN)) {
		(void)object_get_thread_additional(vcpu);
		if (task_queue_schedule(&vcpu->vcpu_run_wakeup_virq_task) !=
		    OK) {
			object_put_thread(vcpu);
		}
	}
}

error_t
vcpu_run_handle_vcpu_poweron(thread_t *vcpu)
{
	vcpu_run_trigger_virq(vcpu);
	return OK;
}

void
vcpu_run_handle_thread_killed(thread_t *thread)
{
	assert(thread != NULL);
	if (thread->kind == THREAD_KIND_VCPU) {
		// Killing the VCPU may have made it temporarily runnable so
		// it can unwind its EL2 stack. Raise a scheduling doorbell.
		vcpu_run_trigger_virq(thread);
	}
}

void
vcpu_run_handle_object_deactivate_thread(thread_t *thread)
{
	if (thread->kind == THREAD_KIND_VCPU) {
		vic_unbind(&thread->vcpu_run_wakeup_virq);
	}
}

scheduler_block_properties_t
vcpu_run_handle_scheduler_get_block_properties(scheduler_block_t block)
{
	assert(block == SCHEDULER_BLOCK_VCPU_RUN);

	// Set the vcpu_run block flag as non-killable to ensure that killed
	// VCPUs continue to be scheduled normally.
	scheduler_block_properties_t props =
		scheduler_block_properties_default();
	scheduler_block_properties_set_non_killable(&props, true);

	return props;
}

```

`hyp/vm/vcpu_run/vcpu_run.ev`:

```ev
// © 2022 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

interface vcpu_run

// Triggered at the start of vcpu_run() to handle resume data specified by a
// VCPU run state. A handler is needed for any run state that defines the
// semantics of the resume data words.
//
// The run state is the last value returned by vcpu_run_check for this VCPU.
// If vcpu_run_check has never been triggered before, the run state is
// VCPU_RUN_STATE_READY, which should not have a handler for this event.
//
// This event is called with the VCPU's scheduler lock held.
selector_event vcpu_run_resume
	selector run_state: vcpu_run_state_t
	param vcpu: thread_t *
	param resume_data_0: register_t
	param resume_data_1: register_t
	param resume_data_2: register_t
	return: error_t = OK

// Triggered by vcpu_run() if the targeted thread is found to be blocked,
// either before or after attempting to yield to it.
//
// Handlers for this event should try to determine why the VCPU cannot
// continue to run. If the reason is known, the corresponding vcpu_run_state_t
// value should be returned; otherwise, return VCPU_RUN_STATE_BLOCKED.
//
// A handler is needed in any module that either defines a vcpu_run_state
// value, or implements the vcpu_expects_wakeup event (which has a
// corresponding run state, VCPU_RUN_STATE_EXPECTS_WAKEUP).
//
// If the returned run state value defines extra data, e.g. the address and
// size of a faulting memory access, it should be returned in the state_data_*
// pointers.
//
// This event is called with the VCPU's scheduler lock held.
handled_event vcpu_run_check
	param vcpu: const thread_t *
	param state_data_0: register_t *
	param state_data_1: register_t *
	param state_data_2: register_t *
	return: vcpu_run_state_t = VCPU_RUN_STATE_BLOCKED

module vcpu_run

subscribe object_deactivate_thread(thread)

subscribe task_queue_execute[TASK_QUEUE_CLASS_VCPU_RUN_WAKEUP_VIRQ](entry)

subscribe vcpu_activate_thread

subscribe vcpu_wakeup
	handler vcpu_run_trigger_virq

subscribe vcpu_poweron(vcpu)

subscribe thread_killed

subscribe vcpu_bind_virq[VCPU_VIRQ_TYPE_VCPU_RUN_WAKEUP](vcpu, vic, virq)

subscribe vcpu_unbind_virq[VCPU_VIRQ_TYPE_VCPU_RUN_WAKEUP](vcpu)

subscribe scheduler_get_block_properties[SCHEDULER_BLOCK_VCPU_RUN]

```

`hyp/vm/vcpu_run/vcpu_run.tc`:

```tc
// © 2022 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

extend thread object module vcpu_run {
	wakeup_virq		structure virq_source(contained);
	wakeup_virq_task	structure task_queue_entry(contained);

	// Protected by the scheduler lock and the SCHEDULER_BLOCK_VCPU_RUN
	// flag; only a thread that has cleared that flag can run the VCPU and
	// update this state (even after dropping the lock). Also, any state
	// variables maintained by handlers for the resume & check events are
	// protected the same way.
	last_state		enumeration vcpu_run_state;

	// Deprecated, will be moved to vcpu options when dynamic enablement of
	// vcpu_run is removed.
	enabled			bool;	// Protected by the scheduler lock.
};

extend vcpu_option_flags bitfield {
	9	vcpu_run_scheduled	bool = 0;
};

extend virq_trigger enumeration {
	vcpu_run_wakeup;
};

extend task_queue_class enumeration {
	vcpu_run_wakeup_virq;
};

extend scheduler_block enumeration {
	vcpu_run;
};

```

`hyp/vm/vdebug/aarch64/src/vdebug.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>

#include <hypregisters.h>

#include <compiler.h>
#include <log.h>
#include <platform_security.h>
#include <thread.h>
#include <trace.h>
#include <vcpu.h>

#include <asm/barrier.h>

#include "debug_bps.h"
#include "event_handlers.h"

static asm_ordering_dummy_t vdebug_asm_order;

void
vdebug_handle_boot_cpu_cold_init(void)
{
	ID_AA64DFR0_EL1_t aa64dfr = register_ID_AA64DFR0_EL1_read();

	// Debug version must be between ARMv8.0 (6) and ARMv8.4 (9)
	assert(ID_AA64DFR0_EL1_get_DebugVer(&aa64dfr) >= 6U);
	assert(ID_AA64DFR0_EL1_get_DebugVer(&aa64dfr) <= 9U);

	// Supported breakpoint and watchpoint counts must be correct
	assert((ID_AA64DFR0_EL1_get_BRPs(&aa64dfr) + 1U) == CPU_DEBUG_BP_COUNT);
	assert((ID_AA64DFR0_EL1_get_WRPs(&aa64dfr) + 1U) == CPU_DEBUG_WP_COUNT);
}

bool
vdebug_handle_vcpu_activate_thread(thread_t	      *thread,
				   vcpu_option_flags_t options)
{
	assert(thread != NULL);
	assert(thread->kind == THREAD_KIND_VCPU);

	// Debug traps should all be enabled by default
	assert(MDCR_EL2_get_TDOSA(&thread->vcpu_regs_el2.mdcr_el2));
	assert(MDCR_EL2_get_TDA(&thread->vcpu_regs_el2.mdcr_el2));

	// TODO: Currently we allow debug access for all VCPUs by
	// default and ignore any vcpu config.
	(void)options;
	vcpu_option_flags_set_debug_allowed(&thread->vcpu_options, true);
	vcpu_runtime_flags_set_debug_active(&thread->vcpu_flags, false);

	return true;
}

void
vdebug_handle_thread_save_state(void)
{
	thread_t *current = thread_get_self();

	if (compiler_unexpected(vcpu_runtime_flags_get_debug_active(
		    &current->vcpu_flags))) {
		assert(current->kind == THREAD_KIND_VCPU);

		// Context-switch the debug registers only if
		// - The device security state disallows debugging, or
		// - The device security state allows debugging and the
		//   external debugger has not claimed the debug module.
		bool need_save = platform_security_state_debug_disabled();
		if (compiler_unexpected(!need_save)) {
#if defined(PLATFORM_HAS_NO_DBGCLAIM_EL1) && PLATFORM_HAS_NO_DBGCLAIM_EL1
			DBGCLAIM_EL1_t dbgclaim = DBGCLAIM_EL1_default();
#else
			DBGCLAIM_EL1_t dbgclaim =
				register_DBGCLAIMCLR_EL1_read_ordered(
					&vdebug_asm_order);
#endif
			need_save = !DBGCLAIM_EL1_get_debug_ext(&dbgclaim);
		}

		bool vdebug_enabled = false;
		if (compiler_expected(need_save)) {
			vdebug_enabled = debug_save_common(
				&current->vdebug_state, &vdebug_asm_order);
		}

		// If debug is no longer in use, ensure register accesses will
		// be trapped when we next switch back to this VCPU, so we can
		// safely avoid restoring the registers.
		if (!vdebug_enabled) {
			MDCR_EL2_set_TDA(&current->vcpu_regs_el2.mdcr_el2,
					 true);
			vcpu_runtime_flags_set_debug_active(
				&current->vcpu_flags, false);
		}
	}
}

void
vdebug_handle_thread_context_switch_post(thread_t *prev)
{
	thread_t *current = thread_get_self();

	if (compiler_unexpected(
		    vcpu_runtime_flags_get_debug_active(&prev->vcpu_flags) &&
		    !vcpu_runtime_flags_get_debug_active(
			    &current->vcpu_flags))) {
		// Write zeros to MDSCR_EL1.MDE and MDSCR_EL1.SS to disable
		// breakpoints and single-stepping, in case the previous VCPU
		// had them enabled.
		register_MDSCR_EL1_write_ordered(MDSCR_EL1_default(),
						 &vdebug_asm_order);
	}
}

void
vdebug_handle_thread_load_state(void)
{
	thread_t *current = thread_get_self();

	if (compiler_unexpected(vcpu_runtime_flags_get_debug_active(
		    &current->vcpu_flags))) {
		// Context-switch the debug registers only if
		// - The device security state disallows debugging, or
		// - The device security state allows debugging and the
		//   external debugger has not claimed the debug module.
		bool need_load = platform_security_state_debug_disabled();
		if (compiler_unexpected(!need_load)) {
#if defined(PLATFORM_HAS_NO_DBGCLAIM_EL1) && PLATFORM_HAS_NO_DBGCLAIM_EL1
			DBGCLAIM_EL1_t dbgclaim = DBGCLAIM_EL1_default();
#else
			DBGCLAIM_EL1_t dbgclaim =
				register_DBGCLAIMCLR_EL1_read_ordered(
					&vdebug_asm_order);
#endif
			need_load = !DBGCLAIM_EL1_get_debug_ext(&dbgclaim);
		}
		if (compiler_expected(need_load)) {
			debug_load_common(&current->vdebug_state,
					  &vdebug_asm_order);
		}
	}
}

// Common vcpu debug access handling.
//
// When this returns VCPU_TRAP_RESULT_EMULATED, the caller must emulate the
// instruction, which may include RAZ/WI.
static vcpu_trap_result_t
vdebug_handle_vcpu_debug_trap(void)
{
	vcpu_trap_result_t ret;
	thread_t	  *current = thread_get_self();

	bool external_debug = !platform_security_state_debug_disabled();
	if (compiler_unexpected(external_debug)) {
#if defined(PLATFORM_HAS_NO_DBGCLAIM_EL1) && PLATFORM_HAS_NO_DBGCLAIM_EL1
		DBGCLAIM_EL1_t dbgclaim = DBGCLAIM_EL1_default();
#else
		DBGCLAIM_EL1_t dbgclaim = register_DBGCLAIMCLR_EL1_read_ordered(
			&vdebug_asm_order);
#endif
		external_debug = DBGCLAIM_EL1_get_debug_ext(&dbgclaim);
	}

	if (!vcpu_option_flags_get_debug_allowed(&current->vcpu_options)) {
		// This VCPU isn't allowed to access debug. Fault immediately.
		ret = VCPU_TRAP_RESULT_FAULT;
	} else if (external_debug) {
		// The device security state allows debugging and the external
		// debugger has claimed the debug module.
		ret = VCPU_TRAP_RESULT_EMULATED;
	} else if (!vcpu_runtime_flags_get_debug_active(&current->vcpu_flags)) {
		// Lazily enable debug register access and restore context.
		vcpu_runtime_flags_set_debug_active(&current->vcpu_flags, true);
		debug_load_common(&current->vdebug_state, &vdebug_asm_order);

		// Disable the register access trap and retry.
		MDCR_EL2_set_TDA(&current->vcpu_regs_el2.mdcr_el2, false);
		register_MDCR_EL2_write(current->vcpu_regs_el2.mdcr_el2);
		ret = VCPU_TRAP_RESULT_RETRY;
	} else {
		// Possibly attempted OS lock or MDCR_EL2.TDCC is set?
		ret = VCPU_TRAP_RESULT_EMULATED;
	}

	return ret;
}

vcpu_trap_result_t
vdebug_handle_vcpu_trap_sysreg(ESR_EL2_ISS_MSR_MRS_t iss)
{
	vcpu_trap_result_t ret;

	if (compiler_expected(ESR_EL2_ISS_MSR_MRS_get_Op0(&iss) != 2U)) {
		// Not a debug register access.
		ret = VCPU_TRAP_RESULT_UNHANDLED;
	} else {
		ret = vdebug_handle_vcpu_debug_trap();

		if (ret == VCPU_TRAP_RESULT_EMULATED) {
			// Use default debug handler implementing RAZ/WI.
			ret = VCPU_TRAP_RESULT_UNHANDLED;
		}
	}

	return ret;
}

#if ARCH_AARCH64_32BIT_EL0
vcpu_trap_result_t
vdebug_handle_vcpu_trap_ldcstc_guest(ESR_EL2_ISS_LDC_STC_t iss)
{
	vcpu_trap_result_t ret = vdebug_handle_vcpu_debug_trap();

	if (ret == VCPU_TRAP_RESULT_EMULATED) {
		// Its complicated to emulate a load/store, just warn for now.
		(void)iss;

		TRACE_AND_LOG(ERROR, WARN,
			      "Warning, trapped AArch32 LDC/STC 0 ignored");
	}

	return ret;
}

vcpu_trap_result_t
vdebug_handle_vcpu_trap_mcrmrc14_guest(ESR_EL2_ISS_MCR_MRC_t iss)
{
	vcpu_trap_result_t ret;

	if (ESR_EL2_ISS_MCR_MRC_get_Opc1(&iss) != 0U) {
		// Not a debug register
		ret = VCPU_TRAP_RESULT_UNHANDLED;
	} else {
		ret = vdebug_handle_vcpu_debug_trap();
	}

	if (ret == VCPU_TRAP_RESULT_EMULATED) {
		thread_t *current = thread_get_self();

		if (ESR_EL2_ISS_MCR_MRC_get_Direction(&iss) == 1) {
			if ((ESR_EL2_ISS_MCR_MRC_get_CV(&iss) == 0) ||
			    (ESR_EL2_ISS_MCR_MRC_get_COND(&iss) != 0xeU)) {
				// TODO: Need to read COND/ITState/condition
				// flags to determined whether to emulate or
				// ignore.
				TRACE_AND_LOG(
					ERROR, WARN,
					"Warning, trapped conditional AArch32 debug register");
				ret = VCPU_TRAP_RESULT_UNHANDLED;
			} else {
				// Debug registers read RAZ by default
				vcpu_gpr_write(current,
					       ESR_EL2_ISS_MCR_MRC_get_Rt(&iss),
					       0U);
			}
		} else {
			// Write Ignored
		}
	}

	return ret;
}
#endif

```

`hyp/vm/vdebug/aarch64/vdebug.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

module vdebug

subscribe boot_cpu_cold_init()

#if defined(INTERFACE_VCPU)
subscribe vcpu_activate_thread
#endif

subscribe thread_save_state

subscribe thread_context_switch_post(prev)

subscribe thread_load_state()

subscribe vcpu_trap_sysreg_read
	handler vdebug_handle_vcpu_trap_sysreg

subscribe vcpu_trap_sysreg_write
	handler vdebug_handle_vcpu_trap_sysreg

#if ARCH_AARCH64_32BIT_EL0
subscribe vcpu_trap_ldcstc_guest

subscribe vcpu_trap_mcrmrc14_guest
#endif

```

`hyp/vm/vdebug/aarch64/vdebug.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

extend thread object module vdebug {
	state		structure debug_common_registers;
};

extend vcpu_runtime_flags bitfield {
	auto		debug_active		bool = 0;
};

```

`hyp/vm/vdebug/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

base_module hyp/core/debug
arch_events aarch64 vdebug.ev
arch_types aarch64 vdebug.tc
arch_source aarch64 vdebug.c
configs VCPU_DEBUG_CONTEXT_SAVED=1

```

`hyp/vm/vete/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

base_module hyp/platform/ete
base_module hyp/misc/vet
events vete.ev
source vete.c

```

`hyp/vm/vete/src/vete.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>

#include <hypregisters.h>

#include <compiler.h>
#include <cpulocal.h>
#include <log.h>
#include <preempt.h>
#include <scheduler.h>
#include <thread.h>
#include <trace.h>
#include <vet.h>

#include <asm/barrier.h>
#include <asm/system_registers.h>

#include "ete.h"
#include "event_handlers.h"

#define ISS_TRFCR_EL1 ISS_OP0_OP1_CRN_CRM_OP2(3, 0, 1, 2, 1)

void
vete_handle_boot_cpu_cold_init(void)
{
	ID_AA64DFR0_EL1_t id_aa64dfr0 = register_ID_AA64DFR0_EL1_read();
	// NOTE: ID_AA64DFR0.TraceVer just indicates if trace is implemented,
	// so here we use equal for assertion.
	assert(ID_AA64DFR0_EL1_get_TraceVer(&id_aa64dfr0) == 1U);
}

void
vete_handle_boot_cpu_warm_init(void)
{
	TRFCR_EL2_t trfcr = TRFCR_EL2_default();
	// prohibit trace of EL2
	TRFCR_EL2_set_E2TRE(&trfcr, 0);
	register_TRFCR_EL2_write_ordered(trfcr, &vet_ordering);
}

void
vet_update_trace_unit_status(thread_t *self)
{
	assert(self != NULL);

	TRCPRGCTLR_t trcprgctlr = TRCPRGCTLR_cast(
		register_TRCPRGCTLR_read_ordered(&vet_ordering));
	self->vet_trace_unit_enabled = TRCPRGCTLR_get_EN(&trcprgctlr);
}

void
vet_flush_trace(thread_t *self)
{
	assert(self != NULL);

	if (compiler_unexpected(self->vet_trace_unit_enabled)) {
		__asm__ volatile("tsb csync" : "+m"(vet_ordering));
	}
}

void
vet_disable_trace(void)
{
	TRCPRGCTLR_t trcprg_ctlr = TRCPRGCTLR_default();
	TRCPRGCTLR_set_EN(&trcprg_ctlr, false);
	register_TRCPRGCTLR_write_ordered(TRCPRGCTLR_raw(trcprg_ctlr),
					  &vet_ordering);
}

static void
vete_prohibit_registers_access(bool prohibit)
{
	thread_t *current = thread_get_self();

	MDCR_EL2_set_TTRF(&current->vcpu_regs_el2.mdcr_el2, prohibit);
	register_MDCR_EL2_write_ordered(current->vcpu_regs_el2.mdcr_el2,
					&vet_ordering);
}

void
vet_save_trace_thread_context(thread_t *self)
{
	(void)self;
	// disable trace register access by set CPTR_EL2.TTA=1
	vete_prohibit_registers_access(true);
}

void
vet_restore_trace_thread_context(thread_t *self)
{
	(void)self;
	// enable trace register access by clear CPTR_EL2.TAA=0
	vete_prohibit_registers_access(false);
}

void
vet_enable_trace(void)
{
	TRCPRGCTLR_t trcprg_ctlr = TRCPRGCTLR_default();
	TRCPRGCTLR_set_EN(&trcprg_ctlr, true);
	register_TRCPRGCTLR_write_ordered(TRCPRGCTLR_raw(trcprg_ctlr),
					  &vet_ordering);
}

void
vet_restore_trace_power_context(bool was_poweroff)
{
	// enable trace register access by clear CPTR_EL2.TAA=0
	vete_prohibit_registers_access(false);
	asm_context_sync_ordered(&vet_ordering);

	ete_restore_context_percpu(cpulocal_get_index(), was_poweroff);

	// disable trace register access by clear CPTR_EL2.TAA=1
	vete_prohibit_registers_access(true);
}

void
vet_save_trace_power_context(bool was_poweroff)
{
	vete_prohibit_registers_access(false);
	asm_context_sync_ordered(&vet_ordering);

	ete_save_context_percpu(cpulocal_get_index(), was_poweroff);

	// disable trace register access by clear CPTR_EL2.TAA=1
	vete_prohibit_registers_access(true);
}

vcpu_trap_result_t
vete_handle_vcpu_trap_sysreg(ESR_EL2_ISS_MSR_MRS_t iss)
{
	thread_t	  *current = thread_get_self();
	vcpu_trap_result_t ret;

	// Remove the fields that are not used in the comparison
	ESR_EL2_ISS_MSR_MRS_set_Rt(&iss, 0);
	ESR_EL2_ISS_MSR_MRS_set_Direction(&iss, false);

	if (((ESR_EL2_ISS_MSR_MRS_get_Op0(&iss) != 2U) ||
	     (ESR_EL2_ISS_MSR_MRS_get_Op1(&iss) != 1U)) &&
	    (ESR_EL2_ISS_MSR_MRS_raw(iss) != ISS_TRFCR_EL1)) {
		// Not a TRBE register access.
		ret = VCPU_TRAP_RESULT_UNHANDLED;
	} else if (!vcpu_option_flags_get_trace_allowed(
			   &current->vcpu_options)) {
		// This VCPU isn't allowed to access debug. Fault immediately.
		ret = VCPU_TRAP_RESULT_FAULT;
	} else if (!current->vet_trace_unit_enabled) {
		// Lazily enable trace register access and restore context.
		current->vet_trace_unit_enabled = true;

		// only enable the register access
		vete_prohibit_registers_access(false);

		ret = VCPU_TRAP_RESULT_RETRY;
	} else {
		// Probably an attempted OS lock; fall back to default RAZ/WI.
		ret = VCPU_TRAP_RESULT_UNHANDLED;
	}

	return ret;
}

```

`hyp/vm/vete/vete.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

module vete

subscribe boot_cpu_cold_init()

subscribe boot_cpu_warm_init()

subscribe vcpu_trap_sysreg_read
	handler vete_handle_vcpu_trap_sysreg

subscribe vcpu_trap_sysreg_write
	handler vete_handle_vcpu_trap_sysreg

```

`hyp/vm/vetm/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

base_module hyp/platform/etm
types vetm.tc
events vetm.ev
source vetm.c

```

`hyp/vm/vetm/src/vetm.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>

#include <hypregisters.h>

#include <atomic.h>
#include <compiler.h>
#include <cpulocal.h>
#include <log.h>
#include <panic.h>
#include <preempt.h>
#include <scheduler.h>
#include <trace.h>
#include <trace_helpers.h>

#include "etm.h"
#include "event_handlers.h"

void
vetm_handle_boot_hypervisor_start(void)
{
#if !defined(NDEBUG)
	register_t flags = 0U;
	TRACE_SET_CLASS(flags, VETM);
	trace_set_class_flags(flags);
#endif
}

void
vetm_handle_boot_cpu_cold_init(void)
{
	ID_AA64DFR0_EL1_t aa64dfr = register_ID_AA64DFR0_EL1_read();

	// Trace version must be 0 (no system register based trace)
	assert(ID_AA64DFR0_EL1_get_TraceVer(&aa64dfr) == 0U);

	// Trace buffer version must be 0 (no system register trace buffer)
	assert(ID_AA64DFR0_EL1_get_TraceFilt(&aa64dfr) == 0U);
}

static bool
vetm_access_allowed(size_t size, size_t offset)
{
	bool ret;

	// First check if the access is size-aligned
	if ((offset & (size - 1U)) != 0UL) {
		ret = false;
	} else if (size == sizeof(uint32_t)) {
		ret = (offset <= (ETM_SIZE_PERCPU - size));
	} else if (size == sizeof(uint64_t)) {
		ret = (offset <= (ETM_SIZE_PERCPU - size));
	} else {
		// Invalid access size
		ret = false;
	}

	return ret;
}

static ETM_TRCVI_CTLR_t
vetm_protect_trcvi_ctlr(ETM_TRCVI_CTLR_t trcvi_ctlr)
{
	ETM_TRCVI_CTLR_EXLEVEL_NS_t exlevel_ns = ETM_TRCVI_CTLR_EXLEVEL_NS_cast(
		ETM_TRCVI_CTLR_get_exlevel_ns(&trcvi_ctlr));

	// disable hypervisor tracing
	if (ETM_TRCVI_CTLR_EXLEVEL_NS_get_el2(&exlevel_ns)) {
		ETM_TRCVI_CTLR_EXLEVEL_NS_set_el2(&exlevel_ns, false);

		ETM_TRCVI_CTLR_set_exlevel_ns(
			&trcvi_ctlr, ETM_TRCVI_CTLR_EXLEVEL_NS_raw(exlevel_ns));
	}

	// remove secure world tracing
	ETM_TRCVI_CTLR_set_exlevel_s(&trcvi_ctlr, 0xf);

	return trcvi_ctlr;
}

static vcpu_trap_result_t
vetm_vdevice_write(thread_t *vcpu, cpu_index_t pcpu, size_t offset,
		   register_t val, size_t access_size)
{
	assert(vcpu != NULL);

	register_t write_val = val;

	if (offset == offsetof(etm_t, trcprgctlr)) {
		vcpu->vetm_enabled = ((val & 0x1U) != 0U);
	} else if (offset == offsetof(etm_t, trcvictlr)) {
		ETM_TRCVI_CTLR_t trcvi_ctlr =
			ETM_TRCVI_CTLR_cast((uint32_t)write_val);
		vcpu->vetm_trcvi_ctlr = vetm_protect_trcvi_ctlr(trcvi_ctlr);
		write_val = ETM_TRCVI_CTLR_raw(vcpu->vetm_trcvi_ctlr);
	}

	etm_set_reg(pcpu, offset, write_val, access_size);

	return VCPU_TRAP_RESULT_EMULATED;
}

static vcpu_trap_result_t
vetm_vdevice_read(thread_t *vcpu, cpu_index_t pcpu, size_t offset,
		  register_t *val, size_t access_size)
{
	(void)vcpu;

	etm_get_reg(pcpu, offset, val, access_size);

	return VCPU_TRAP_RESULT_EMULATED;
}

vcpu_trap_result_t
vetm_handle_vdevice_access_fixed_addr(vmaddr_t ipa, size_t access_size,
				      register_t *value, bool is_write)
{
	vcpu_trap_result_t ret = VCPU_TRAP_RESULT_UNHANDLED;

	cpulocal_begin();

	thread_t *vcpu = thread_get_self();
	assert(vcpu != NULL);

	if (!vcpu_option_flags_get_hlos_vm(&vcpu->vcpu_options)) {
		goto out;
	}

	if ((ipa >= PLATFORM_ETM_BASE) &&
	    (ipa <
	     PLATFORM_ETM_BASE + PLATFORM_ETM_STRIDE * PLATFORM_MAX_CORES)) {
		size_t	    base_offset = (size_t)(ipa - PLATFORM_ETM_BASE);
		cpu_index_t access_pcpu =
			(cpu_index_t)(base_offset / PLATFORM_ETM_STRIDE);
		size_t offset =
			ipa - PLATFORM_ETM_BASE - pcpu * PLATFORM_ETM_STRIDE;

		if ((pcpu == access_pcpu) &&
		    vetm_access_allowed(access_size, offset)) {
			if (is_write) {
				ret = vetm_vdevice_write(vcpu, pcpu, offset,
							 *value, access_size);
			} else {
				ret = vetm_vdevice_read(vcpu, pcpu, offset,
							value, access_size);
			}
		}
	}
out:
	cpulocal_end();

	return ret;
}

void
vetm_handle_thread_load_state(void)
{
	thread_t *vcpu = thread_get_self();
	assert(vcpu != NULL);

	if (vcpu_option_flags_get_hlos_vm(&vcpu->vcpu_options)) {
		etm_set_reg(pcpu, offsetof(etm_t, trcvictlr),
			    ETM_TRCVI_CTLR_raw(vcpu->vetm_trcvi_ctlr),
			    sizeof(vcpu->vetm_trcvi_ctlr));
	}
}

error_t
vetm_handle_thread_context_switch_pre(void)
{
	thread_t *vcpu = thread_get_self();
	assert(vcpu != NULL);

	if (vcpu_option_flags_get_hlos_vm(&vcpu->vcpu_options)) {
		// clear trcvi_ctlr
		ETM_TRCVI_CTLR_t trcvi_ctlr = ETM_TRCVI_CTLR_default();
		etm_set_reg(pcpu, offsetof(etm_t, trcvictlr),
			    ETM_TRCVI_CTLR_raw(trcvi_ctlr), sizeof(trcvi_ctlr));
	}

	return OK;
}

```

`hyp/vm/vetm/vetm.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

module vetm

subscribe boot_hypervisor_start()

subscribe boot_cpu_cold_init()

subscribe vdevice_access_fixed_addr

subscribe thread_context_switch_pre()

subscribe thread_load_state()

```

`hyp/vm/vetm/vetm.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

extend thread object module vetm {
	// Current trace filter setting for VCPU
	//
	// Trace filter protection guildline:
	// * only allows setting from HLOS
	// * disable tracing for hypervisor
	trcvi_ctlr	bitfield ETM_TRCVI_CTLR;

	// indicates if the etm is enabled or not
	enabled		bool;
};

extend trace_class enumeration {
	VETM = 20;
};

```

`hyp/vm/vetm_null/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

events vetm_null.ev
source vetm_null.c

```

`hyp/vm/vetm_null/src/vetm_null.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>

#include <cpulocal.h>
#include <scheduler.h>
#include <thread.h>
#include <trace.h>
#include <trace_helpers.h>

#include "event_handlers.h"

vcpu_trap_result_t
vetm_null_handle_vdevice_access_fixed_addr(vmaddr_t ipa, size_t access_size,
					   register_t *value, bool is_write)
{
	vcpu_trap_result_t ret = VCPU_TRAP_RESULT_UNHANDLED;
	(void)access_size;

	thread_t *vcpu = thread_get_self();
	assert(vcpu != NULL);

	if (!vcpu_option_flags_get_hlos_vm(&vcpu->vcpu_options)) {
		ret = false;
		goto out;
	}

	if ((ipa >= PLATFORM_ETM_BASE) &&
	    (ipa < (PLATFORM_ETM_BASE +
		    (PLATFORM_ETM_STRIDE * PLATFORM_MAX_CORES)))) {
		// Treat the entire ETM region as RAZ/WI
		if (!is_write) {
			*value = 0U;
		}
		ret = VCPU_TRAP_RESULT_EMULATED;
	}

out:
	return ret;
}

```

`hyp/vm/vetm_null/vetm_null.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

module vetm_null

subscribe vdevice_access_fixed_addr

```

`hyp/vm/vgic/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

local_include
interface vgic
base_module hyp/vm/vic_base
base_module hyp/mem/useraccess
base_module hyp/platform/gicv3
types vgic.tc
events vgic.ev
hypercalls vgic.hvc
source deliver.c distrib.c vdevice.c sysregs.c util.c vpe.c vgic.c
configs VGIC_HAS_EXT_IRQS=0
configs VGIC_HAS_1N=GICV3_HAS_1N
configs VGIC_HAS_1N_PRIORITY_CHECK=0
configs VGIC_HAS_LPI=GICV3_HAS_VLPI_V4_1
configs GICV3_ENABLE_VPE=VGIC_HAS_LPI
# Avoid trapping WFI if GICv4 is used, because doorbell wakeup latency is high
configs VCPU_IDLE_IN_EL1=VGIC_HAS_LPI
# Workaround for broken max IRQs calculation (1024 instead of 1020) in UEFI
configs VGIC_IGNORE_ARRAY_OVERFLOWS=1
configs VIC_BASE_FORWARD_PRIVATE=1

```

`hyp/vm/vgic/include/internal.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

//
// Debugging
//

#define VGIC_TRACE(id, vic, vcpu, fmt, ...)                                    \
	TRACE(VGIC, VGIC_##id, "{:#x} {:#x} " fmt, (uintptr_t)vic,             \
	      (uintptr_t)vcpu, __VA_ARGS__)

#define VGIC_DEBUG_TRACE(id, vic, vcpu, fmt, ...)                              \
	TRACE(VGIC_DEBUG, VGIC_##id, "{:#x} {:#x} " fmt, (uintptr_t)vic,       \
	      (uintptr_t)vcpu, __VA_ARGS__)

//
// VIRQ routing and delivery
//

thread_t *
vgic_get_route_from_state(vic_t *vic, vgic_delivery_state_t dstate,
			  bool use_local_vcpu);

thread_t *
vgic_get_route_for_spi(vic_t *vic, virq_t virq, bool use_local_vcpu);

thread_t *
vgic_find_target(vic_t *vic, virq_source_t *source);

vgic_delivery_state_t
vgic_deliver(virq_t virq, vic_t *vic, thread_t *vcpu, virq_source_t *source,
	     _Atomic vgic_delivery_state_t *dstate,
	     vgic_delivery_state_t assert_dstate, bool is_private)
	EXCLUDE_SCHEDULER_LOCK(vcpu);

bool
vgic_undeliver(vic_t *vic, thread_t *vcpu,
	       _Atomic vgic_delivery_state_t *dstate, virq_t virq,
	       vgic_delivery_state_t clear_dstate, bool check_route)
	EXCLUDE_SCHEDULER_LOCK(vcpu);

void
vgic_undeliver_all(vic_t *vic, thread_t *vcpu) EXCLUDE_SCHEDULER_LOCK(vcpu);

void
vgic_deactivate(vic_t *vic, thread_t *vcpu, virq_t virq,
		_Atomic vgic_delivery_state_t *dstate,
		vgic_delivery_state_t old_dstate, bool set_edge,
		bool set_hw_active);

void
vgic_sync_all(vic_t *vic, bool wakeup);

void
vgic_update_enables(vic_t *vic, GICD_CTLR_DS_t gicd_ctlr);

#if VGIC_HAS_LPI && GICV3_HAS_VLPI
void
vgic_vpe_schedule_current(void);
#endif

void
vgic_retry_unrouted(vic_t *vic);

cpu_index_t
vgic_lr_owner_lock(thread_t *vcpu) ACQUIRE_LOCK(vcpu->vgic_lr_owner_lock)
	ACQUIRE_PREEMPT_DISABLED;

cpu_index_t
vgic_lr_owner_lock_nopreempt(thread_t *vcpu)
	ACQUIRE_LOCK(vcpu->vgic_lr_owner_lock) REQUIRE_PREEMPT_DISABLED;

void
vgic_lr_owner_unlock(thread_t *vcpu) RELEASE_LOCK(vcpu->vgic_lr_owner_lock)
	RELEASE_PREEMPT_DISABLED;

void
vgic_lr_owner_unlock_nopreempt(thread_t *vcpu)
	RELEASE_LOCK(vcpu->vgic_lr_owner_lock) REQUIRE_PREEMPT_DISABLED;

index_result_t
vgic_get_index_for_mpidr(vic_t *vic, uint8_t aff0, uint8_t aff1, uint8_t aff2,
			 uint8_t aff3);

//
// Utility functions (IRQ types, bit manipulations etc)
//

#define hwirq_from_virq_source(p)                                              \
	(assert(p != NULL),                                                    \
	 assert(p->trigger == VIRQ_TRIGGER_VGIC_FORWARDED_SPI),                \
	 hwirq_container_of_vgic_spi_source(p))

#define fwd_private_from_virq_source(p)                                        \
	(assert(p != NULL),                                                    \
	 assert(p->trigger == VIRQ_TRIGGER_VIC_BASE_FORWARD_PRIVATE),          \
	 vic_forward_private_container_of_source(p))

vgic_irq_type_t
vgic_get_irq_type(virq_t irq);

bool
vgic_irq_is_private(virq_t virq);

bool
vgic_irq_is_spi(virq_t virq);

bool
vgic_irq_is_ppi(virq_t virq);

virq_source_t *
vgic_find_source(vic_t *vic, thread_t *vcpu, virq_t virq);

_Atomic(vgic_delivery_state_t) *
vgic_find_dstate(vic_t *vic, thread_t *vcpu, virq_t virq);

bool
vgic_delivery_state_is_level_asserted(const vgic_delivery_state_t *x);

bool
vgic_delivery_state_is_pending(const vgic_delivery_state_t *x);

void
vgic_read_lr_state(index_t i);

bool
vgic_has_lpis(vic_t *vic);

//
// Register trap handlers
//

// GICD
void
vgic_gicd_set_control(vic_t *vic, GICD_CTLR_DS_t ctlr);

void
vgic_gicd_set_statusr(vic_t *vic, GICD_STATUSR_t statusr, bool set);

void
vgic_gicd_change_irq_pending(vic_t *vic, irq_t irq_num, bool set, bool is_msi);

void
vgic_gicd_change_irq_enable(vic_t *vic, irq_t irq_num, bool set);

void
vgic_gicd_change_irq_active(vic_t *vic, irq_t irq_num, bool set);

void
vgic_gicd_set_irq_group(vic_t *vic, irq_t irq_num, bool is_group_1);

void
vgic_gicd_set_irq_priority(vic_t *vic, irq_t irq_num, uint8_t priority);

void
vgic_gicd_set_irq_config(vic_t *vic, irq_t irq_num, bool is_edge);

void
vgic_gicd_set_irq_router(vic_t *vic, irq_t irq_num, uint8_t aff0, uint8_t aff1,
			 uint8_t aff2, uint8_t aff3, bool is_1n);

#if GICV3_HAS_GICD_ICLAR
void
vgic_gicd_set_irq_classes(vic_t *vic, irq_t irq_num, bool class0, bool class1);
#endif

// GICR
thread_t *
vgic_get_thread_by_gicr_index(vic_t *vic, index_t gicr_num);

void
vgic_gicr_rd_set_control(vic_t *vic, thread_t *gicr_vcpu, GICR_CTLR_t ctlr);

GICR_CTLR_t
vgic_gicr_rd_get_control(vic_t *vic, thread_t *gicr_vcpu);

void
vgic_gicr_rd_set_statusr(thread_t *gicr_vcpu, GICR_STATUSR_t statusr, bool set);

void
vgic_gicr_rd_set_sleep(vic_t *vic, thread_t *gicr_vcpu, bool sleep);

bool
vgic_gicr_rd_check_sleep(thread_t *gicr_vcpu);

#if VGIC_HAS_LPI
// Note: the GICR_PROPBASER is shared between all VCPUs (as permitted by the
// GICv3 spec) so there is no gicr_vcpu argument here
void
vgic_gicr_rd_set_propbase(vic_t *vic, GICR_PROPBASER_t propbase);

void
vgic_gicr_rd_set_pendbase(vic_t *vic, thread_t *gicr_vcpu,
			  GICR_PENDBASER_t pendbase);
#endif

void
vgic_gicr_sgi_change_sgi_ppi_enable(vic_t *vic, thread_t *gicr_vcpu,
				    irq_t irq_num, bool set);

void
vgic_gicr_sgi_change_sgi_ppi_pending(vic_t *vic, thread_t *gicr_vcpu,
				     irq_t irq_num, bool set);

void
vgic_gicr_sgi_change_sgi_ppi_active(vic_t *vic, thread_t *gicr_vcpu,
				    irq_t irq_num, bool set);

void
vgic_gicr_sgi_set_sgi_ppi_group(vic_t *vic, thread_t *gicr_vcpu, irq_t irq_num,
				bool is_group_1);

void
vgic_gicr_sgi_set_sgi_ppi_priority(vic_t *vic, thread_t *gicr_vcpu,
				   irq_t irq_num, uint8_t priority);

void
vgic_gicr_sgi_set_ppi_config(vic_t *vic, thread_t *gicr_vcpu, irq_t irq_num,
			     bool is_edge);

// GICC
void
vgic_icc_set_group_enable(bool is_group_1, ICC_IGRPEN_EL1_t igrpen);

void
vgic_icc_irq_deactivate(vic_t *vic, irq_t irq_num);

void
vgic_icc_generate_sgi(vic_t *vic, ICC_SGIR_EL1_t sgir, bool is_group_1);

error_t
vgic_vsgi_assert(thread_t *gicr_vcpu, irq_t irq_num);

```

`hyp/vm/vgic/include/vgic.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// Interfaces used by the VGIC ITS for LPI cache invalidation.

#if VGIC_HAS_LPI
void
vgic_gicr_copy_propbase_one(vic_t *vic, thread_t *gicr_vcpu, irq_t vlpi);

#if GICV3_HAS_VLPI_V4_1
void
vgic_gicr_rd_invlpi(vic_t *vic, thread_t *gicr_vcpu, virq_t vlpi_num);

void
vgic_gicr_rd_invall(vic_t *vic, thread_t *gicr_vcpu);

bool
vgic_gicr_get_inv_pending(vic_t *vic, thread_t *gicr_vcpu);
#endif
#endif

```

`hyp/vm/vgic/src/deliver.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>

#include <hypcontainers.h>
#include <hypregisters.h>

#include <atomic.h>
#include <bitmap.h>
#include <compiler.h>
#include <cpulocal.h>
#include <ipi.h>
#include <irq.h>
#include <log.h>
#include <object.h>
#include <panic.h>
#include <partition.h>
#include <partition_alloc.h>
#include <platform_cpu.h>
#include <platform_irq.h>
#include <preempt.h>
#include <rcu.h>
#include <scheduler.h>
#include <spinlock.h>
#include <thread.h>
#include <trace.h>
#include <trace_helpers.h>
#include <util.h>
#include <vcpu.h>
#include <virq.h>

#include <events/virq.h>

#include <asm/barrier.h>

#if defined(ARCH_ARM_FEAT_FGT) && ARCH_ARM_FEAT_FGT
#include <arm_fgt.h>
#endif

#include "event_handlers.h"
#include "gich_lrs.h"
#include "gicv3.h"
#include "internal.h"

static hwirq_t *vgic_maintenance_hwirq;

static asm_ordering_dummy_t gich_lr_ordering;

static bool
vgic_fgt_allowed(void)
{
#if defined(ARCH_ARM_FEAT_FGT) && ARCH_ARM_FEAT_FGT
	return compiler_expected(arm_fgt_is_allowed());
#else
	return false;
#endif
}

void
vgic_handle_boot_hypervisor_start(void)
{
#if !defined(NDEBUG)
	register_t flags = 0U;
	TRACE_SET_CLASS(flags, VGIC);
#if defined(VERBOSE) && VERBOSE
	TRACE_SET_CLASS(flags, VGIC_DEBUG);
#endif
	trace_set_class_flags(flags);
#endif

	hwirq_create_t hwirq_args = {
		.irq	= PLATFORM_GICH_IRQ,
		.action = HWIRQ_ACTION_VGIC_MAINTENANCE,
	};
	hwirq_ptr_result_t hwirq_r =
		partition_allocate_hwirq(partition_get_private(), hwirq_args);
	if (hwirq_r.e != OK) {
		panic("Unable to create GICH HWIRQ");
	}
	if (object_activate_hwirq(hwirq_r.r) != OK) {
		panic("Unable to activate GICH HWIRQ");
	}
	vgic_maintenance_hwirq = hwirq_r.r;

	irq_enable_local(vgic_maintenance_hwirq);
}

void
vgic_handle_boot_cpu_warm_init(void)
{
	if (vgic_maintenance_hwirq != NULL) {
		irq_enable_local(vgic_maintenance_hwirq);
	}

	// Ensure that EL1 has SRE=1 set (this is hardwired to 1 on most ARMv8
	// platforms, but there's no harm in trying to set it anyway)
	ICC_SRE_EL1_t icc_sre = ICC_SRE_EL1_default();
	// Disable IRQ and FIQ bypass
	ICC_SRE_EL1_set_DIB(&icc_sre, true);
	ICC_SRE_EL1_set_DFB(&icc_sre, true);
	// Enable system register accesses
	ICC_SRE_EL1_set_SRE(&icc_sre, true);
	register_ICC_SRE_EL1_write(icc_sre);
}

void
vgic_read_lr_state(index_t i)
{
	thread_t *current = thread_get_self();
	assert((current != NULL) && (current->kind == THREAD_KIND_VCPU));

	assert_debug(i < CPU_GICH_LR_COUNT);
	vgic_lr_status_t *status = &current->vgic_lrs[i];

	// Read back the hardware register if necessary
	if (ICH_LR_EL2_base_get_State(&status->lr.base) !=
	    ICH_LR_EL2_STATE_INVALID) {
		status->lr = gicv3_read_ich_lr(i, &gich_lr_ordering);
	}
}

static void
vgic_write_lr(index_t i)
{
	assert_debug(i < CPU_GICH_LR_COUNT);
	thread_t *current = thread_get_self();
	assert_debug((current != NULL) && (current->kind == THREAD_KIND_VCPU));

	vgic_lr_status_t *status = &current->vgic_lrs[i];

	gicv3_write_ich_lr(i, status->lr, &gich_lr_ordering);
}

#if VGIC_HAS_1N
static bool
vgic_get_delivery_state_is_class0(vgic_delivery_state_t *dstate)
{
	bool ret;
#if defined(GICV3_HAS_GICD_ICLAR) && GICV3_HAS_GICD_ICLAR
	ret = !vgic_delivery_state_get_nclass0(dstate);
#else
	(void)dstate;
	ret = true;
#endif

	return ret;
}

static bool
vgic_get_delivery_state_is_class1(vgic_delivery_state_t *dstate)
{
	bool ret;
#if defined(GICV3_HAS_GICD_ICLAR) && GICV3_HAS_GICD_ICLAR
	ret = vgic_delivery_state_get_class1(dstate);
#else
	(void)dstate;
	ret = false;
#endif

	return ret;
}
#endif

// Determine whether a VCPU is a valid route for a given VIRQ.
//
// This is allowed to take the enabled groups into account, but must ignore the
// VCPU's priority mask, because ICV_CTLR_EL1[6] (the virtual ICC_CTLR_EL1.PMHE
// analogue) is RES0.
//
// This function must not have side-effects. It may be called without holding
// any locks, to assist with routing decisions, but the result is only
// guaranteed to be accurate if the LR owner lock is held.
static bool
vgic_route_allowed(vic_t *vic, thread_t *vcpu, vgic_delivery_state_t dstate)
{
	bool allowed;
	(void)vic;

	if (vgic_delivery_state_get_group1(&dstate)
		    ? !vcpu->vgic_group1_enabled
		    : !vcpu->vgic_group0_enabled) {
		allowed = false;
	}
#if VGIC_HAS_1N
	else if (vgic_delivery_state_get_route_1n(&dstate)) {
		// We don't implement DPG bits in the virtual GIC, so just
		// check the class bits.
		allowed = (platform_irq_cpu_class(
				   (cpu_index_t)vcpu->vgic_gicr_index) == 0U)
				  ? vgic_get_delivery_state_is_class0(&dstate)
				  : vgic_get_delivery_state_is_class1(&dstate);
	}
#endif
	else {
		// Is this VCPU the VIRQ's direct route?
		index_t route_index = vgic_delivery_state_get_route(&dstate);
		allowed		    = (vcpu->vgic_gicr_index == route_index);
	}

	return allowed;
}

static void
vgic_route_and_flag(vic_t *vic, virq_t virq, vgic_delivery_state_t new_dstate,
		    bool use_local_vcpu);

#if VGIC_HAS_1N
static void
vgic_spi_reset_route_1n(virq_source_t *source, vgic_delivery_state_t dstate)
{
	if ((source != NULL) &&
	    (source->trigger == VIRQ_TRIGGER_VGIC_FORWARDED_SPI)) {
		// Restore the 1-of-N route
		hwirq_t *hwirq = hwirq_from_virq_source(source);

		GICD_IROUTER_t route_1n = GICD_IROUTER_default();
		GICD_IROUTER_set_IRM(&route_1n, true);
		(void)gicv3_spi_set_route(hwirq->irq, route_1n);

#if GICV3_HAS_GICD_ICLAR
		// Set the HW IRQ's 1-of-N routing classes. Note that these are
		// reset in the hardware whenever the IRM bit is cleared.
		(void)gicv3_spi_set_classes(
			hwirq->irq, !vgic_delivery_state_get_nclass0(&dstate),
			vgic_delivery_state_get_class1(&dstate));
#else
		(void)dstate;
#endif
	}
}
#endif

// Check whether a level-triggered source is still asserting its interrupt.
static bool
vgic_virq_check_pending(virq_source_t *source, bool reasserted)
	REQUIRE_PREEMPT_DISABLED
{
	bool is_pending;

	if (compiler_unexpected(source == NULL)) {
		// Source has been detached since the IRQ was asserted.
		is_pending = false;
	} else {
		// The virq_check_pending event must guarantee that all memory
		// reads executed by the handler are ordered after the read that
		// determined (a) that the IRQ was marked level-pending, and (b)
		// the value of the reasserted argument. Since the callers of
		// this function make those determinations using relaxed atomic
		// reads of the delivery state, we need an acquire fence here to
		// enforce the correct ordering.
		atomic_thread_fence(memory_order_acquire);

		is_pending = trigger_virq_check_pending_event(
			source->trigger, source, reasserted);
	}

	return is_pending;
}

static bool
vgic_sync_lr_should_be_pending(bool lr_hw, bool lr_pending, bool lr_active,
			       bool allow_pending, bool hw_detach,
			       vgic_delivery_state_t *new_dstate)
{
	bool remove_hw, virq_pending;

	// If the IRQ is still pending, we need to deliver it again.
	virq_pending = vgic_delivery_state_get_enabled(new_dstate) &&
		       vgic_delivery_state_is_pending(new_dstate);

	// Determine whether to delist the IRQ, and whether the HW=1 bit
	// is being removed from a valid LR (whether delisted or not).
	if (!lr_active && (!virq_pending || !allow_pending)) {
		vgic_delivery_state_set_listed(new_dstate, false);
		vgic_delivery_state_set_active(new_dstate, false);
		remove_hw = lr_pending;
	} else if (virq_pending && allow_pending) {
		// We're going to leave the LR in pending state, so
		// clear the edge bit.
		vgic_delivery_state_set_edge(new_dstate, false);
		remove_hw = false;
	} else {
		// We are leaving the VIRQ listed in active state, and
		// can't set the pending state in the LR. If the VIRQ is
		// pending, we must trap EOI to deliver it elsewhere.
		remove_hw = virq_pending;
	}

	// If we're removing HW=1 from a valid LR, but not detaching
	// (and therefore deactivating) the HW IRQ, we need to reset the
	// hw_active bit so the HW IRQ will be deactivated later.
	if (lr_hw && remove_hw && !hw_detach) {
		vgic_delivery_state_set_hw_active(new_dstate, true);
	}

	return virq_pending;
}

static bool
vgic_sync_lr_check_src(vic_t *vic, thread_t *vcpu, virq_t virq,
		       vgic_delivery_state_t  old_dstate,
		       vgic_delivery_state_t  clear_dstate,
		       vgic_delivery_state_t *new_dstate, bool lr_hw,
		       bool lr_pending, bool lr_has_eoi, bool hw_detach)
	REQUIRE_PREEMPT_DISABLED
{
	virq_source_t *source	       = vgic_find_source(vic, vcpu, virq);
	bool	       need_deactivate = false;

	// If the LR is in pending state, reset the edge bit, unless it's being
	// explicitly cleared. Note that it will be cleared again later in the
	// sync_lr update if we decide to leave the LR in pending state.
	if (lr_pending && !vgic_delivery_state_get_edge(&clear_dstate)) {
		vgic_delivery_state_set_edge(new_dstate, true);
	}

	// If the IRQ is level-triggered, determine whether to leave it pending.
	if (vgic_delivery_state_get_level_src(&old_dstate) &&
	    !vgic_delivery_state_get_level_src(&clear_dstate)) {
		// level_src is set and is not being explicitly cleared.
		// Determine whether it should be cleared based on the LR's
		// pending state.
		if (lr_hw && (!lr_pending || hw_detach)) {
			// Pending state was consumed, so reset
			// level_src to hw_active (which preserves any
			// remote assertion).
			vgic_delivery_state_set_level_src(
				new_dstate,
				vgic_delivery_state_get_hw_active(&old_dstate));
		} else if (lr_has_eoi && compiler_expected(source != NULL) &&
			   (source->trigger ==
			    VIRQ_TRIGGER_VGIC_FORWARDED_SPI)) {
			// EOI occurred after a SW delivery; assume the HW
			// source is no longer pending, because the handler
			// probably cleared it. If it is still pending, then
			// the HW will re-deliver it after the deactivation.
			vgic_delivery_state_set_level_src(new_dstate, false);
		} else {
			bool reassert =
				lr_pending ||
				vgic_delivery_state_get_edge(&old_dstate);
			if (!vgic_virq_check_pending(source, reassert)) {
				vgic_delivery_state_set_level_src(new_dstate,
								  false);
			}
		}
	}

	// If the IRQ is no longer deliverable, deactivate the HW source.
	if (!vgic_delivery_state_is_pending(new_dstate) ||
	    !vgic_delivery_state_get_enabled(new_dstate)) {
		need_deactivate =
			vgic_delivery_state_get_hw_active(&old_dstate);
		vgic_delivery_state_set_hw_active(new_dstate, false);
	}

	return need_deactivate;
}

typedef struct {
	vgic_delivery_state_t new_dstate;
	bool		      virq_pending;
	bool		      hw_detach;
	bool		      allow_pending;
	bool		      deactivate_hw;
} vgic_sync_lr_update_t;

static vgic_sync_lr_update_t
vgic_sync_lr_update_delivery_state(vic_t *vic, thread_t *vcpu,
				   const vgic_lr_status_t *status,
				   vgic_delivery_state_t   clear_dstate,
				   bool lr_hw, bool lr_pending, virq_t virq,
				   bool lr_active) REQUIRE_PREEMPT_DISABLED
{
	vgic_delivery_state_t old_dstate = atomic_load_relaxed(status->dstate);
	bool hw_detach = vgic_delivery_state_get_hw_active(&clear_dstate);

	vgic_delivery_state_t new_dstate;
	bool		      virq_pending;
	bool		      allow_pending;
	bool		      deactivate_hw;

	const bool lr_has_eoi = !lr_hw && !lr_pending && !lr_active &&
				ICH_LR_EL2_HW0_get_EOI(&status->lr.sw);

	do {
		assert(vgic_delivery_state_get_listed(&old_dstate));
		new_dstate = vgic_delivery_state_difference(old_dstate,
							    clear_dstate);

		// Determine whether the LR can be left in pending state.
		allow_pending = (!lr_hw || !lr_active) &&
				vgic_delivery_state_get_enabled(&new_dstate) &&
				vgic_route_allowed(vic, vcpu, new_dstate);

		// We always handle HW detachment, even if not delisting. Note
		// that nobody can concurrently clear hw_detached, so we don't
		// need to reset the local hw_detached variable if it is false.
		if (vgic_delivery_state_get_hw_detached(&old_dstate)) {
			vgic_delivery_state_set_hw_detached(&new_dstate, false);
			hw_detach = true;
		}

		// Check the VIRQ's source and update the delivery state.
		deactivate_hw = vgic_sync_lr_check_src(
			vic, vcpu, virq, old_dstate, clear_dstate, &new_dstate,
			lr_hw, lr_pending, lr_has_eoi, hw_detach);

		// Determine the new pending state of the LR.
		virq_pending = vgic_sync_lr_should_be_pending(
			lr_hw, lr_pending, lr_active, allow_pending, hw_detach,
			&new_dstate);

		// The VIRQ should now be in sync.
		vgic_delivery_state_set_need_sync(&new_dstate, false);
	} while (!atomic_compare_exchange_strong_explicit(
		status->dstate, &old_dstate, new_dstate, memory_order_relaxed,
		memory_order_relaxed));

	VGIC_TRACE(DSTATE_CHANGED, vic, vcpu, "sync_lr {:d}: {:#x} -> {:#x}",
		   virq, vgic_delivery_state_raw(old_dstate),
		   vgic_delivery_state_raw(new_dstate));

	return (vgic_sync_lr_update_t){
		.new_dstate    = new_dstate,
		.virq_pending  = virq_pending,
		.hw_detach     = hw_detach,
		.allow_pending = allow_pending,
		.deactivate_hw = deactivate_hw,
	};
}

static void
vgic_sync_lr_update_lr(vic_t *vic, thread_t *vcpu, vgic_lr_status_t *status,
		       bool lr_pending, virq_t virq, bool lr_active,
		       bool virq_pending, bool allow_pending, bool lr_hw,
		       vgic_delivery_state_t new_dstate, bool use_local_vcpu)
	REQUIRE_PREEMPT_DISABLED
{
	if (!vgic_delivery_state_get_listed(&new_dstate)) {
		VGIC_TRACE(HWSTATE_CHANGED, vic, vcpu,
			   "sync_lr {:d}: delisted (pending {:d})", virq,
			   (register_t)virq_pending);

#if VGIC_HAS_1N
		if (vgic_delivery_state_get_route_1n(&new_dstate)) {
			virq_source_t *source =
				vgic_find_source(vic, vcpu, virq);
			vgic_spi_reset_route_1n(source, new_dstate);
		}
#endif
		status->dstate	= NULL;
		status->lr.base = ICH_LR_EL2_base_default();

		if (virq_pending) {
			vgic_route_and_flag(vic, virq, new_dstate,
					    use_local_vcpu);
		}
	} else if (!allow_pending) {
		VGIC_TRACE(HWSTATE_CHANGED, vic, vcpu,
			   "sync_lr {:d}: LR left active ({:s} pending)", virq,
			   (uintptr_t)(virq_pending ? "still" : "not"));

		// We may have been in pending and active state; remove the
		// pending state bit.
		assert(lr_active);
		ICH_LR_EL2_base_set_State(&status->lr.base,
					  ICH_LR_EL2_STATE_ACTIVE);

		if (virq_pending) {
			// The VIRQ is still pending. We need to set the EOI
			// trap bit in the LR to ensure that the IRQ can be
			// delivered again later. The HW=1 bit must be cleared
			// to do this; so, if it was previously set, we must
			// have reset hw_active in the dstate already.
			assert(!lr_hw ||
			       vgic_delivery_state_get_hw_active(&new_dstate));
			ICH_LR_EL2_base_set_HW(&status->lr.base, false);
			ICH_LR_EL2_HW0_set_EOI(&status->lr.sw, true);
		}
	} else if (virq_pending) {
		VGIC_TRACE(HWSTATE_CHANGED, vic, vcpu,
			   "sync_lr {:d}: LR set pending ({:s} active)", virq,
			   (uintptr_t)(lr_active ? "and" : "not"));

		// We can leave the LR in a pending state.
		ICH_LR_EL2_base_set_State(
			&status->lr.base,
			lr_active ? ICH_LR_EL2_STATE_PENDING_ACTIVE
				  : ICH_LR_EL2_STATE_PENDING);

		if (!lr_pending && !lr_active) {
			// This is a new delivery; make sure the VCPU is awake.
			if (vcpu == thread_get_self()) {
				vcpu_wakeup_self();
			} else {
				scheduler_lock_nopreempt(vcpu);
				vcpu_wakeup(vcpu);
				scheduler_unlock_nopreempt(vcpu);
			}

			// The dstate update above never clears hw_active, so
			// any new delivery must be HW=0, even if it came from a
			// forwarded SPI (which is unlikely because it must have
			// been misrouted). The HW bit might still be set from
			// an earlier delivery, so clear it here.
			ICH_LR_EL2_base_set_HW(&status->lr.base, false);

			// We need to trap EOI if the IRQ is level triggered or
			// the HW source is active.
			ICH_LR_EL2_HW0_set_EOI(
				&status->lr.sw,
				!vgic_delivery_state_get_cfg_is_edge(
					&new_dstate) ||
					vgic_delivery_state_get_hw_active(
						&new_dstate));
		} else if (vgic_delivery_state_get_hw_active(&new_dstate)) {
			// If the dstate update left hw_active set, we need to
			// force HW=0 and trap EOI to deactivate the HW IRQ.
			ICH_LR_EL2_base_set_HW(&status->lr.base, false);
			ICH_LR_EL2_HW0_set_EOI(&status->lr.sw, true);
		} else if (!ICH_LR_EL2_base_get_HW(&status->lr.base)) {
			// We also need to trap EOI for SW asserted level
			// triggered IRQs.
			ICH_LR_EL2_HW0_set_EOI(
				&status->lr.sw,
				!vgic_delivery_state_get_cfg_is_edge(
					&new_dstate));
		} else {
			// Existing HW delivery; EOI handled by physical GIC
		}
	} else {
		// The IRQ is remaining listed, is allowed to remain pending,
		// and does not need to be set pending; no LR change needed.
		VGIC_TRACE(HWSTATE_CHANGED, vic, vcpu,
			   "sync_lr {:d}: LR unchanged", virq);
	}
}

// Synchronise a VIRQ's delivery state with its LR.
//
// This is used for all updates to a currently listed VIRQ other than a local
// redelivery or deactivation. That includes disabling, clearing, rerouting,
// reprioritising, cross-CPU asserting or deactivating, handling an EOI trap, or
// releasing the source.
//
// Asserting a locally listed VIRQ is handled by vgic_redeliver_lr().
// Deactivating a locally listed VIRQ is handled by vgic_deactivate().
//
// The flags that are set in the clear_dstate argument, if any, will be cleared
// in the delivery state. This value must not have any flags set other than the
// four pending flags, the enabled flag, and the hardware active flag.
//
// If the current delivery state has the enable bit clear or clear_dstate has
// the enable bit set, the pending state will be removed from the LR regardless
// of the pending state of the interrupt (though the active state can remain in
// the LR).
//
// If the current delivery state has the hw_detached bit set or clear_dstate has
// the hw_active bit set, the HW bit of the LR will be cleared even if it is
// left listed. The HW bit of the LR may also be cleared if it is necessary to
// trap EOI to guarantee delivery of the IRQ.
//
// The specified VCPU must either be the current thread, or LR-locked by
// the caller and known not to be running remotely. If the VCPU is the current
// thread, the caller is responsible for syncing and updating the physical LR.
//
// For hardware interrupts, the level_src flag in clear_dstate may be overridden
// by the hw_active flag, if it has been set by a concurrent remote delivery;
// this is unnecessary for software interrupts because level_src changes are
// required to be serialised.
//
// If the VIRQ is still enabled and pending after clearing the pending and
// enable bits, it will be set pending in the LR if possible, or otherwise
// rerouted. If it is 1-of-N, the use_local_vcpu flag determines whether the
// current VCPU is given routing priority.
//
// The result is true if the VIRQ has been unlisted.
static bool
vgic_sync_lr(vic_t *vic, thread_t *vcpu, vgic_lr_status_t *status,
	     vgic_delivery_state_t clear_dstate, bool use_local_vcpu)
	REQUIRE_PREEMPT_DISABLED EXCLUDE_SCHEDULER_LOCK(vcpu)
{
	virq_t virq = ICH_LR_EL2_base_get_vINTID(&status->lr.base);

	assert(status->dstate != NULL);

	bool		   lr_hw = ICH_LR_EL2_base_get_HW(&status->lr.base);
	ICH_LR_EL2_State_t lr_state =
		ICH_LR_EL2_base_get_State(&status->lr.base);
	bool lr_pending = (lr_state == ICH_LR_EL2_STATE_PENDING) ||
			  (lr_state == ICH_LR_EL2_STATE_PENDING_ACTIVE);
	bool lr_active = (lr_state == ICH_LR_EL2_STATE_ACTIVE) ||
			 (lr_state == ICH_LR_EL2_STATE_PENDING_ACTIVE);

	vgic_delivery_state_t new_dstate;
	bool		      virq_pending;
	bool		      hw_detach;
	bool		      allow_pending;
	bool		      deactivate_hw;
	{
		vgic_sync_lr_update_t sync_lr_info =
			vgic_sync_lr_update_delivery_state(vic, vcpu, status,
							   clear_dstate, lr_hw,
							   lr_pending, virq,
							   lr_active);
		new_dstate    = sync_lr_info.new_dstate;
		virq_pending  = sync_lr_info.virq_pending;
		hw_detach     = sync_lr_info.hw_detach;
		allow_pending = sync_lr_info.allow_pending;
		deactivate_hw = sync_lr_info.deactivate_hw;
	}

	// If we're detaching a HW IRQ, clear the HW bit in the LR.
	if (compiler_unexpected(lr_hw && hw_detach)) {
		// If the LR was pending or active, the physical IRQ is still
		// active. Clearing the HW bit destroys our record that this
		// might be the case, so we have to deactivate at this point.
		if (lr_pending || lr_active) {
			assert(!vgic_delivery_state_get_hw_active(&new_dstate));
			irq_t irq = ICH_LR_EL2_HW1_get_pINTID(&status->lr.hw);
			VGIC_TRACE(
				HWSTATE_CHANGED, vic, vcpu,
				"sync_lr {:d}: deactivate HW IRQ {:d} (detach)",
				virq, irq);
			gicv3_irq_deactivate(irq);
		}

		// If the LR will remain valid, turn it into a SW IRQ.
		if (vgic_delivery_state_get_listed(&new_dstate)) {
			ICH_LR_EL2_base_set_HW(&status->lr.base, false);
			// If HW was 1 there must be no SW level assertion, so
			// we don't need to trap EOI
			ICH_LR_EL2_HW0_set_EOI(&status->lr.sw, false);
		}
	}

	// If we are clearing HW active for a SW LR, deactivate the HW IRQ.
	if (deactivate_hw) {
		virq_source_t *source = vgic_find_source(vic, vcpu, virq);
		assert((source != NULL) &&
		       (source->trigger == VIRQ_TRIGGER_VGIC_FORWARDED_SPI));
		hwirq_t *hwirq = hwirq_from_virq_source(source);

		VGIC_TRACE(HWSTATE_CHANGED, vic, vcpu,
			   "sync_lr {:d}: deactivate HW IRQ {:d} (EOI)", virq,
			   hwirq->irq);
		irq_deactivate(hwirq);
	}

	// Update the LR.
	vgic_sync_lr_update_lr(vic, vcpu, status, lr_pending, virq, lr_active,
			       virq_pending, allow_pending, lr_hw, new_dstate,
			       use_local_vcpu);

	return !vgic_delivery_state_get_listed(&new_dstate);
}

static bool
vgic_undeliver_update_hw_detach_and_sync(const vic_t *vic, const thread_t *vcpu,
					 virq_t				virq,
					 _Atomic vgic_delivery_state_t *dstate,
					 vgic_delivery_state_t clear_dstate,
					 vgic_delivery_state_t old_dstate,
					 bool		       check_route)
{
	vgic_delivery_state_t new_dstate;
	bool hw_detach = vgic_delivery_state_get_hw_active(&clear_dstate);
	vgic_delivery_state_set_hw_active(&clear_dstate, false);

	do {
		new_dstate = vgic_delivery_state_difference(old_dstate,
							    clear_dstate);

		if (!vgic_delivery_state_get_listed(&old_dstate)) {
			// Delisted by another thread; no sync needed.
		} else if (check_route) {
			// Force a sync regardless of pending state.
			vgic_delivery_state_set_need_sync(&new_dstate, true);
		} else if (!vgic_delivery_state_get_enabled(&new_dstate)) {
			// No longer enabled; a sync is required.
			vgic_delivery_state_set_need_sync(&new_dstate, true);
		} else if (!vgic_delivery_state_get_cfg_is_edge(&new_dstate) &&
			   !vgic_delivery_state_is_level_asserted(
				   &new_dstate)) {
			// No longer pending; a sync is required.
			vgic_delivery_state_set_need_sync(&new_dstate, true);
		} else {
			// Still pending and not reclaimed; no sync needed.
		}

		if (hw_detach && vgic_delivery_state_get_listed(&old_dstate)) {
			vgic_delivery_state_set_hw_detached(&new_dstate, true);
		}
	} while (!atomic_compare_exchange_strong_explicit(
		dstate, &old_dstate, new_dstate, memory_order_relaxed,
		memory_order_relaxed));

	VGIC_TRACE(DSTATE_CHANGED, vic, vcpu,
		   "undeliver-sync {:d}: {:#x} -> {:#x}", virq,
		   vgic_delivery_state_raw(old_dstate),
		   vgic_delivery_state_raw(new_dstate));

	return !vgic_delivery_state_get_listed(&old_dstate);
}

static vgic_delivery_state_t
vgic_undeliver_update_dstate(vic_t *vic, thread_t *vcpu,
			     _Atomic vgic_delivery_state_t *dstate, virq_t virq,
			     vgic_delivery_state_t  clear_dstate,
			     vgic_delivery_state_t *old_dstate)
	REQUIRE_PREEMPT_DISABLED
{
	vgic_delivery_state_t new_dstate;
	do {
		// If the VIRQ is not listed, update its flags directly.
		new_dstate = vgic_delivery_state_difference(*old_dstate,
							    clear_dstate);
		if (vgic_delivery_state_get_listed(old_dstate)) {
			break;
		}

		// If level_src is set and is not being explicitly cleared,
		// check whether we need to clear it.
		if (vgic_delivery_state_get_level_src(old_dstate) &&
		    !vgic_delivery_state_get_level_src(&clear_dstate)) {
			virq_source_t *source =
				vgic_find_source(vic, vcpu, virq);
			if (!vgic_virq_check_pending(
				    source,
				    vgic_delivery_state_get_edge(old_dstate))) {
				vgic_delivery_state_set_level_src(&new_dstate,
								  false);
			}
		}
	} while (!atomic_compare_exchange_strong_explicit(
		dstate, old_dstate, new_dstate, memory_order_relaxed,
		memory_order_relaxed));

	return new_dstate;
}

// Clear pending bits from a given VIRQ, and abort its delivery if necessary.
//
// This is used when disabling, rerouting, manually clearing, or releasing the
// source of a VIRQ.
//
// The specified VCPU is the current route of the VIRQ if it is shared (in which
// case it may be NULL), or the owner of the VIRQ if it is private.
//
// The pending flags in clear_dstate will be cleared in the delivery state.
// This value must not have any flags set other than the four pending flags,
// the enabled flag, and the hw_active flag. Also, the hw_active flag should
// always be set if the edge or level_src flags are set; this is because
// clearing a pending HW IRQ without deactivating it may make it undeliverable.
//
// If this function returns true, the interrupt is known not to have been listed
// anywhere at the time the pending flags were cleared. If it returns false, the
// interrupt may still be listed on remotely running VCPUs.
bool
vgic_undeliver(vic_t *vic, thread_t *vcpu,
	       _Atomic vgic_delivery_state_t *dstate, virq_t virq,
	       vgic_delivery_state_t clear_dstate, bool check_route)
{
	bool from_self = vcpu == thread_get_self();

	assert(vgic_delivery_state_get_hw_active(&clear_dstate) ||
	       (!vgic_delivery_state_get_edge(&clear_dstate) &&
		!vgic_delivery_state_get_level_src(&clear_dstate)));

	cpu_index_t remote_cpu = vgic_lr_owner_lock(vcpu);

	vgic_delivery_state_t old_dstate = atomic_load_relaxed(dstate);
	vgic_delivery_state_t new_dstate = vgic_undeliver_update_dstate(
		vic, vcpu, dstate, virq, clear_dstate, &old_dstate);

	bool unlisted = false;
	if (!vgic_delivery_state_get_listed(&old_dstate)) {
		VGIC_TRACE(DSTATE_CHANGED, vic, vcpu,
			   "undeliver-unlisted {:d}: {:#x} -> {:#x}", virq,
			   vgic_delivery_state_raw(old_dstate),
			   vgic_delivery_state_raw(new_dstate));

		// If we just cleared the HW active flag, deactivate the IRQ.
		if (vgic_delivery_state_get_hw_active(&old_dstate) &&
		    !vgic_delivery_state_get_hw_active(&new_dstate)) {
			virq_source_t *source =
				vgic_find_source(vic, vcpu, virq);

			hwirq_t *hwirq = hwirq_from_virq_source(source);
			VGIC_TRACE(HWSTATE_CHANGED, vic, vcpu,
				   "undeliver {:d}: deactivate HW IRQ {:d}",
				   virq, hwirq->irq);
			irq_deactivate(hwirq);
		}

		unlisted = true;
		goto out;
	}

	// If the VCPU we were given is not running or is ourselves, try to
	// directly undeliver the VIRQ. This may fail for shared VIRQs if the
	// route is out of date.
	if ((vcpu != NULL) && !cpulocal_index_valid(remote_cpu)) {
		for (index_t i = 0; i < CPU_GICH_LR_COUNT; i++) {
			if (vcpu->vgic_lrs[i].dstate != dstate) {
				continue;
			}

			if (from_self) {
				vgic_read_lr_state(i);
			}
			unlisted = vgic_sync_lr(vic, vcpu, &vcpu->vgic_lrs[i],
						clear_dstate, false);
			if (from_self) {
				vgic_write_lr(i);
			}

			goto out;
		}
	}

	// Fall back to requesting a sync.
	//
	// Note that this can't clear the pending state of an edge triggered
	// interrupt, so in that case we log a warning.
#if !defined(NDEBUG)
	if (vgic_delivery_state_get_edge(&clear_dstate)) {
		static _Thread_local bool warned_about_ignored_icpendr = false;
		if (!warned_about_ignored_icpendr) {
			TRACE_AND_LOG(
				DEBUG, INFO,
				"vcpu {:#x}: trapped GIC[DR]_ICPENDR write "
				"was cross-CPU; vIRQ {:d} may be left pending",
				(uintptr_t)(thread_t *)thread_get_self(), virq);
			warned_about_ignored_icpendr = true;
		}
	}
#endif
	// We can't directly clear hw_active on a remote CPU; we need to use
	// the hw_detached bit to ask the remote CPU to do it.
	unlisted = vgic_undeliver_update_hw_detach_and_sync(
		vic, vcpu, virq, dstate, clear_dstate, old_dstate, check_route);

out:
	vgic_lr_owner_unlock(vcpu);

	return unlisted;
}

typedef struct {
	ICH_LR_EL2_t	      new_lr;
	vgic_delivery_state_t new_dstate;
	bool		      force_eoi_trap;
	bool		      need_wakeup;
	uint8_t		      pad[2];
} vgic_redeliver_lr_info_t;

static vgic_redeliver_lr_info_t
vgic_redeliver_lr_update_state(const vic_t *vic, const thread_t *vcpu,
			       virq_source_t *source, virq_t virq,
			       ICH_LR_EL2_State_t      old_lr_state,
			       const vgic_lr_status_t *status,
			       vgic_delivery_state_t   old_dstate,
			       vgic_delivery_state_t   assert_dstate)
{
	vgic_delivery_state_t new_dstate =
		vgic_delivery_state_union(old_dstate, assert_dstate);
	bool	     is_hw  = vgic_delivery_state_get_hw_active(&new_dstate);
	ICH_LR_EL2_t new_lr = status->lr;
	bool	     force_eoi_trap = false;
	bool	     need_wakeup    = false;

	if (compiler_expected(old_lr_state == ICH_LR_EL2_STATE_INVALID)) {
		// Previous interrupt is gone; take the new one. Don't
		// bother to recheck level triggering yet; that will be
		// done when this interrupt ends.
		ICH_LR_EL2_base_set_HW(&new_lr.base, is_hw);
		if (is_hw) {
			vgic_delivery_state_set_hw_active(&new_dstate, false);
			ICH_LR_EL2_HW1_set_pINTID(
				&new_lr.hw,
				hwirq_from_virq_source(source)->irq);
		}
		ICH_LR_EL2_base_set_State(&new_lr.base,
					  ICH_LR_EL2_STATE_PENDING);

		// Interrupt is newly pending; we need to wake the VCPU.
		need_wakeup = true;
	} else if (compiler_unexpected(is_hw !=
				       ICH_LR_EL2_base_get_HW(&new_lr.base))) {
		// If we have both a SW and a HW source, deliver the SW
		// assertion first, and request an EOI maintenance
		// interrupt to deliver (or trigger reassertion of) the
		// HW source afterwards.
		if (ICH_LR_EL2_base_get_HW(&new_lr.base)) {
			ICH_LR_EL2_base_set_HW(&new_lr.base, false);
			vgic_delivery_state_set_hw_active(&new_dstate, true);

			VGIC_DEBUG_TRACE(
				HWSTATE_UNCHANGED, vic, vcpu,
				"redeliver {:d}: hw + sw; relisting as sw",
				virq);
		}
		force_eoi_trap = true;

		// Interrupt is either already pending (so the VCPU
		// should be awake) or is active (so not deliverable,
		// and the VCPU should not be woken); no need for a
		// wakeup.
	}
#if VGIC_HAS_LPI && GICV3_HAS_VLPI_V4_1
	else if ((old_lr_state == ICH_LR_EL2_STATE_ACTIVE) &&
		 vic->vsgis_enabled &&
		 (vgic_get_irq_type(virq) == VGIC_IRQ_TYPE_SGI)) {
		// A vSGI delivered by the ITS does not have an active
		// state, because it is really a vLPI in disguise. Make
		// software-delivered SGIs behave the same way.
		assert(!is_hw && !ICH_LR_EL2_base_get_HW(&new_lr.base));
		ICH_LR_EL2_base_set_State(&new_lr.base,
					  ICH_LR_EL2_STATE_PENDING);

		// Interrupt was previously active and is now pending,
		// so it has just become deliverable and we need to wake
		// the VCPU.
		need_wakeup = true;
	}
#endif
	else {
		// We should never get here for a hardware-mode LR,
		// since it would mean that we were risking a double
		// deactivate.
		assert(!is_hw && !ICH_LR_EL2_base_get_HW(&new_lr.base));

		// A software-mode LR that is in active state can me
		// moved straight to active+pending.
		if (old_lr_state == ICH_LR_EL2_STATE_ACTIVE) {
			ICH_LR_EL2_base_set_State(
				&new_lr.base, ICH_LR_EL2_STATE_PENDING_ACTIVE);
		} else {
			VGIC_DEBUG_TRACE(
				HWSTATE_UNCHANGED, vic, vcpu,
				"redeliver {:d}: redundant assertions merged",
				virq);
		}

		// Interrupt is already pending, so the VCPU should be
		// awake; no need for a wakeup.
	}

	vgic_delivery_state_set_edge(&new_dstate, force_eoi_trap);

	return (vgic_redeliver_lr_info_t){
		.new_dstate	= new_dstate,
		.force_eoi_trap = force_eoi_trap,
		.need_wakeup	= need_wakeup,
		.new_lr		= new_lr,
	};
}

static bool
vgic_redeliver_lr(vic_t *vic, thread_t *vcpu, virq_source_t *source,
		  _Atomic vgic_delivery_state_t *dstate,
		  vgic_delivery_state_t		*old_dstate,
		  vgic_delivery_state_t assert_dstate, index_t lr)
{
	assert_debug(lr < CPU_GICH_LR_COUNT);

	// Merge the old and new LR states.
	vgic_lr_status_t *status = &vcpu->vgic_lrs[lr];
	virq_t		  virq	 = ICH_LR_EL2_base_get_vINTID(&status->lr.base);

	// Update the delivery state.
	vgic_delivery_state_t new_dstate;
	ICH_LR_EL2_t	      new_lr;
	bool		      force_eoi_trap;
	bool		      need_wakeup;

	do {
		ICH_LR_EL2_State_t trace_state;
		assert(vgic_delivery_state_get_listed(old_dstate));

		ICH_LR_EL2_State_t old_lr_state =
			ICH_LR_EL2_base_get_State(&status->lr.base);

		{
			vgic_redeliver_lr_info_t vgic_redeliver_lr_info =
				vgic_redeliver_lr_update_state(
					vic, vcpu, source, virq, old_lr_state,
					status, *old_dstate, assert_dstate);
			new_dstate     = vgic_redeliver_lr_info.new_dstate;
			force_eoi_trap = vgic_redeliver_lr_info.force_eoi_trap;
			need_wakeup    = vgic_redeliver_lr_info.need_wakeup;
			new_lr	       = vgic_redeliver_lr_info.new_lr;
		}

		trace_state = ICH_LR_EL2_base_get_State(&new_lr.base);
		VGIC_TRACE(HWSTATE_CHANGED, vic, vcpu,
			   "redeliver {:d}: lr {:d} -> {:d}", virq,
			   (register_t)old_lr_state, (register_t)trace_state);
	} while (!atomic_compare_exchange_strong_explicit(
		dstate, old_dstate, new_dstate, memory_order_relaxed,
		memory_order_relaxed));

	status->lr = new_lr;

	VGIC_TRACE(DSTATE_CHANGED, vic, vcpu, "redeliver {:d}: {:#x} -> {:#x}",
		   virq, vgic_delivery_state_raw(*old_dstate),
		   vgic_delivery_state_raw(new_dstate));

	if (!ICH_LR_EL2_base_get_HW(&status->lr.base)) {
		ICH_LR_EL2_HW0_set_EOI(
			&status->lr.sw,
			force_eoi_trap ||
				(!vgic_delivery_state_get_cfg_is_edge(
					 &new_dstate) &&
				 vgic_delivery_state_is_level_asserted(
					 &new_dstate)));
	}

	return need_wakeup;
}

static bool_result_t
vgic_redeliver(vic_t *vic, thread_t *vcpu, virq_source_t *source,
	       _Atomic vgic_delivery_state_t *dstate,
	       vgic_delivery_state_t	     *old_dstate,
	       vgic_delivery_state_t	      assert_dstate)
{
	const bool    to_self = vcpu == thread_get_self();
	index_t	      i;
	bool_result_t ret = bool_result_error(ERROR_BUSY);

	for (i = 0; i < CPU_GICH_LR_COUNT; i++) {
		if (dstate == vcpu->vgic_lrs[i].dstate) {
			break;
		}
	}

	if (i < CPU_GICH_LR_COUNT) {
		// If we are targeting ourselves, read the current state.
		if (to_self) {
			vgic_read_lr_state(i);
		}

		ret = bool_result_ok(vgic_redeliver_lr(vic, vcpu, source,
						       dstate, old_dstate,
						       assert_dstate, i));

		// Update the affected list register.
		if (to_self) {
			vgic_write_lr(i);
		}
	}

	return ret;
}

// Returns true if a list register is empty: invalid, and either HW or not
// EOI-trapped. This is the same condition used by the hardware to set bits in
// ICH_ELRSR_EL2.
static inline bool
vgic_lr_is_empty(ICH_LR_EL2_t lr)
{
	return (ICH_LR_EL2_base_get_State(&lr.base) ==
		ICH_LR_EL2_STATE_INVALID) &&
	       (ICH_LR_EL2_base_get_HW(&lr.base) ||
		!ICH_LR_EL2_HW0_get_EOI(&lr.sw));
}

// Select an LR to deliver to, given the priority of the IRQ to deliver.
//
// The specified VCPU must either be the current thread, or LR-locked by
// the caller and known not to be running remotely.
//
// The caller must not assume that the selected LR is empty. Before using the
// LR it must check for and kick out any currently listed VIRQ, and update that
// VIRQ's state appropriately.
//
// On successful return, the value of *lr_priority is set to the priority of
// the pending interrupt listed in the selected LR, if any, or else to
// GIC_PRIORITY_LOWEST.
//
// The spec leaves it IMPLEMENTATION DEFINED whether priority decisions take the
// group bits and ICC group enable bits into account for directly routed
// interrupts (though 1-of-N interrupts, if supported must be delisted on ICC
// group disable, and all interrupts must be delisted on GICD group disable).
// See section 4.7.2 (page 64) in revision E. To keep this function simpler,
// we do not consider the ICC group enable bits.
static index_result_t
vgic_select_lr(thread_t *vcpu, uint8_t priority, uint8_t *lr_priority)
{
	bool	       to_self = vcpu == thread_get_self();
	index_result_t result  = index_result_error(ERROR_BUSY);

	// First look for an LR that has no associated IRQ at all.
	for (index_t i = 0; i < CPU_GICH_LR_COUNT; i++) {
		if (vcpu->vgic_lrs[i].dstate == NULL) {
			result	     = index_result_ok(i);
			*lr_priority = GIC_PRIORITY_LOWEST;
			goto out;
		}
	}

	// If the VCPU is the current thread, check for LRs that have become
	// empty since we last wrote to them; ELRSR is a hardware-generated
	// bitmap of these.
	if (to_self) {
		asm_context_sync_ordered(&gich_lr_ordering);
		register_t elrsr =
			register_ICH_ELRSR_EL2_read_ordered(&gich_lr_ordering);
		if (elrsr != 0U) {
			result	     = index_result_ok(compiler_ctz(elrsr));
			*lr_priority = GIC_PRIORITY_LOWEST;
			goto out;
		}
	}

	// Finally, check all the LRs, looking for (in order of preference):
	// - any inactive LR with no pending EOI maintenance IRQ, or
	// - the lowest-priority active or pending-and-active LR, or
	// - the lowest-priority pending LR, if it has lower priority than the
	//   VIRQ we're delivering.
	index_result_t result_pending	       = index_result_error(ERROR_BUSY);
	uint8_t	       priority_result_active  = 0U;
	uint8_t	       priority_result_pending = 0U;

	for (index_t i = 0; i < CPU_GICH_LR_COUNT; i++) {
		const vgic_lr_status_t *status = &vcpu->vgic_lrs[i];
		uint8_t			this_priority =
			ICH_LR_EL2_base_get_Priority(&status->lr.base);

		// If the VCPU is current and the LR was written in a valid
		// state, the hardware might have changed it to a different
		// valid state, so we must read it back. (It can't have been
		// either initially invalid or changed to invalid, because we
		// would have found it in a nonzero ELRSR above.)
		if (to_self) {
			vgic_read_lr_state(i);
		}

		ICH_LR_EL2_State_t state =
			ICH_LR_EL2_base_get_State(&status->lr.base);

		if (vgic_lr_is_empty(status->lr)) {
			// LR is empty; we can reclaim it immediately.
			result	     = index_result_ok(i);
			*lr_priority = GIC_PRIORITY_LOWEST;
			goto out;
		} else if (state == ICH_LR_EL2_STATE_INVALID) {
			// LR is inactive but has pending EOI maintenance. This
			// case is not handled by vgic_reclaim_lr() so we leave
			// this LR alone for now.
		} else if (state != ICH_LR_EL2_STATE_PENDING) {
			// LR is active or pending+active, so we can use it if
			// it has the lowest priority of any such LR. Note that
			// it must strictly be the lowest priority to make sure
			// we choose the right IRQs in the unlisted EOI handler.
			if (this_priority >= priority_result_active) {
				result		       = index_result_ok(i);
				*lr_priority	       = GIC_PRIORITY_LOWEST;
				priority_result_active = this_priority;
			}
		} else {
			// LR is pending, so we can use it if it has the lowest
			// priority of any such LR and is also lower priority
			// than the priority we're trying to deliver.
			if ((this_priority >= priority_result_pending) &&
			    (this_priority > priority)) {
				result_pending		= index_result_ok(i);
				priority_result_pending = this_priority;
			}
		}
	}

	if (priority_result_active == 0U) {
		// There were no active LRs; use the lowest-priority pending
		// one, if possible. Otherwise we have failed to find an LR.
		result = result_pending;
		if (result.e == OK) {
			*lr_priority = priority_result_pending;
		}
	}

out:
	return result;
}

// The number of VIRQs in each low (SPI + PPI) range other than the last SPI
// range (which has 4 fewer because of the "special" IRQ numbers 1020-1023).
#define VGIC_LOW_RANGE_SIZE                                                    \
	(count_t)((GIC_SPI_BASE + GIC_SPI_NUM + VGIC_LOW_RANGES - 1U) /        \
		  VGIC_LOW_RANGES)
static_assert(util_is_p2(VGIC_LOW_RANGE_SIZE),
	      "VGIC search ranges must have power-of-two sizes");
static_assert(VGIC_LOW_RANGE_SIZE > GIC_SPECIAL_INTIDS_NUM,
	      "VGIC search ranges must have size greater than 4");

// The number of VIRQs in a specific low range, taking into account the special
// IRQ numbers that immediately follow the SPIs.
static count_t
vgic_low_range_size(index_t range)
{
	return (range == (VGIC_LOW_RANGES - 1U))
		       ? (VGIC_LOW_RANGE_SIZE - GIC_SPECIAL_INTIDS_NUM)
		       : VGIC_LOW_RANGE_SIZE;
}

// Mark an unlisted interrupt as pending on a VCPU.
//
// This is called when an interrupt is pending on a VCPU but cannot be listed
// immediately, either because there are no free LRs and none of the occupied
// LRs have lower pending priority, or because the VCPU is running remotely.
//
// This function requires the targeted VCPU's LR lock to be held, and the remote
// CPU (if any) on which the VCPU is currently running to be specified. If the
// VCPU is not locked (e.g. because another VCPU is already locked), use
// vgic_flag_unlocked() instead.
static void
vgic_flag_locked(virq_t virq, thread_t *vcpu, uint8_t priority, bool group1,
		 cpu_index_t remote_cpu) REQUIRE_PREEMPT_DISABLED
{
	assert_preempt_disabled();

	count_t priority_shifted = (count_t)priority >> VGIC_PRIO_SHIFT;

	bitmap_atomic_set(vcpu->vgic_search_ranges_low[priority_shifted],
			  virq / VGIC_LOW_RANGE_SIZE, memory_order_release);

	bitmap_atomic_set(vcpu->vgic_search_prios, priority_shifted,
			  memory_order_release);

	if (group1 ? !vcpu->vgic_group1_enabled : !vcpu->vgic_group0_enabled) {
		// VCPU's GICR is asleep; nothing more to do.
	} else if (thread_get_self() == vcpu) {
		// We know that all LRs are occupied and not lower priority,
		// so sending an IPI here is not useful; enable NPIE instead
		if (!ICH_HCR_EL2_get_NPIE(&vcpu->vgic_ich_hcr)) {
			vcpu->vgic_ich_hcr = register_ICH_HCR_EL2_read();
			ICH_HCR_EL2_set_NPIE(&vcpu->vgic_ich_hcr, true);
			register_ICH_HCR_EL2_write(vcpu->vgic_ich_hcr);
		}
	} else if (cpulocal_index_valid(remote_cpu)) {
		ipi_one(IPI_REASON_VGIC_DELIVER, remote_cpu);
	} else {
		// NPIE being set will trigger a redeliver when switching
		ICH_HCR_EL2_set_NPIE(&vcpu->vgic_ich_hcr, true);
	}
}

// Mark an unlisted interrupt as pending on a VCPU.
//
// This is called when an interrupt is pending on a VCPU but cannot be listed
// immediately, either because:
//
// - another operation is already being performed on one of the VCPU's LRs and
//   an immediate delivery would recurse (which is prohibited by MISRA because
//   it might overflow the stack), or
//
// - the specified VCPU might be running remotely, and its LRs can't be locked
//   because another VCPU's LR lock is already held.
//
// This function must not assume that the targeted VCPU's LR lock is or is not
// held. It uses explicitly ordered accesses to ensure that the correct
// signalling is performed without having to acquire the LR lock.
static void
vgic_flag_unlocked(virq_t virq, thread_t *vcpu, uint8_t priority)
	REQUIRE_PREEMPT_DISABLED
{
	count_t priority_shifted = (count_t)priority >> VGIC_PRIO_SHIFT;

	if (!bitmap_atomic_test_and_set(
		    vcpu->vgic_search_ranges_low[priority_shifted],
		    virq / VGIC_LOW_RANGE_SIZE, memory_order_release)) {
		if (!bitmap_atomic_test_and_set(vcpu->vgic_search_prios,
						priority_shifted,
						memory_order_release)) {
			if (thread_get_self() == vcpu) {
				ipi_one_relaxed(IPI_REASON_VGIC_DELIVER,
						cpulocal_get_index());
				vcpu_wakeup_self();
			} else {
				// Match the seq_cst fences when the owner is
				// changed during the context switch.
				atomic_thread_fence(memory_order_seq_cst);

				cpu_index_t lr_owner = atomic_load_relaxed(
					&vcpu->vgic_lr_owner_lock.owner);

				if (cpulocal_index_valid(lr_owner)) {
					ipi_one(IPI_REASON_VGIC_DELIVER,
						lr_owner);
				} else {
					scheduler_lock_nopreempt(vcpu);
					vcpu_wakeup(vcpu);
					scheduler_unlock_nopreempt(vcpu);
				}
			}
		}
	}
}

// Mark an unlisted interrupt as pending without a specific target VCPU.
//
// This is called when an interrupt is pending in the virtual distributor, but
// cannot be assigned to a specific VCPU, either because:
//
// - it has a direct route that is out of range or identifies a VCPU that has
//   not been attached yet; or
//
// - it has 1-of-N routing, but is in a group that is disabled on all VCPUs.
static void
vgic_flag_unrouted(vic_t *vic, virq_t virq)
{
	bitmap_atomic_set(vic->search_ranges_low, virq / VGIC_LOW_RANGE_SIZE,
			  memory_order_release);
}

#if VGIC_HAS_1N
// The degree to which a VCPU is preferred as the route for a VIRQ, in order of
// increasing preference.
typedef enum {
	// The VCPU can't be immediately chosen as a target (though it can still
	// be chosen if E1NWF is set and all cores are asleep).
	VGIC_ROUTE_DENIED = 0,

	// The VCPU has affinity to a remote physical CPU but is not expecting a
	// wakeup, which implies that it is either running and possibly busy,
	// preempted by another VCPU, or blocked by the hypervisor.
	VGIC_ROUTE_REMOTE_BUSY,

	// The VCPU has affinity to the local CPU, but is not current, and the
	// current VCPU has equal or higher scheduler priority. It is likely to
	// sleep for several milliseconds while the other VCPU runs.
	VGIC_ROUTE_PREEMPTED,

	// The VCPU has affinity to the local CPU but is already handling an IRQ
	// with equal or higher IRQ priority. It is likely to be busy with the
	// other IRQ for tens of microseconds or more.
	VGIC_ROUTE_BUSY,

	// The VCPU has affinity to a remote physical CPU and is waiting for a
	// wakeup from WFI. Note that VCPUs in a virtual power-off suspend will
	// have their groups disabled, and therefore will return DENIED.
	VGIC_ROUTE_REMOTE,

	// The VCPU has affinity to the local CPU. It is either current with no
	// other vIRQs at equal or higher priority, or is in WFI and will
	// preempt the current thread if woken.
	VGIC_ROUTE_IMMEDIATE,
} vgic_route_preference_t;

// Determine the level of route preference for the specified VCPU.
//
// The VCPU's scheduler lock is held when this is called, so it is safe to query
// the scheduler state. However, note that the lock will be released before the
// result is used.
//
// The VCPU's LR owner lock is not held when this is called.
static vgic_route_preference_t
vgic_route_1n_preference(vic_t *vic, thread_t *vcpu,
			 vgic_delivery_state_t dstate)
	REQUIRE_SCHEDULER_LOCK(vcpu)
{
	vgic_route_preference_t ret	= VGIC_ROUTE_DENIED;
	thread_t	       *current = thread_get_self();

	if (compiler_unexpected(!vgic_route_allowed(vic, vcpu, dstate))) {
		ret = VGIC_ROUTE_DENIED;
	} else if (compiler_expected(vcpu == current)) {
#if VGIC_HAS_1N_PRIORITY_CHECK
		// Check whether any of the LRs are valid and higher priority.
		//
		// This is closest to the documented behaviour of the GIC-700,
		// but it is fairly expensive to do in software.
		//
		// Note that we can't just check whether the VCPU has IRQs
		// masked in PSTATE, because the Linux idle thread executes WFI
		// with interrupts masked.
		uint8_t new_priority =
			vgic_delivery_state_get_priority(&dstate);
		uint8_t current_priority = GIC_PRIORITY_LOWEST;
		for (index_t i = 0; i < CPU_GICH_LR_COUNT; i++) {
			vgic_lr_status_t *status = &vcpu->vgic_lrs[i];
			if (status->dstate == NULL) {
				continue;
			}
			vgic_read_lr_state(i);
			if (ICH_LR_EL2_base_get_State(&status->lr.base) ==
			    ICH_LR_EL2_STATE_INVALID) {
				continue;
			}
			// We could also check BPR if the LR is in
			// active state, but that is rarely used and
			// probably not worthwhile.
			current_priority = util_min(
				current_priority,
				ICH_LR_EL2_base_get_Priority(&status->lr.base));
		}
		ret = (new_priority < current_priority) ? VGIC_ROUTE_IMMEDIATE
							: VGIC_ROUTE_BUSY;
#else
		ret = VGIC_ROUTE_IMMEDIATE;
#endif
	} else if (cpulocal_get_index() !=
		   scheduler_get_active_affinity(vcpu)) {
		ret = vcpu_expects_wakeup(vcpu) ? VGIC_ROUTE_REMOTE
						: VGIC_ROUTE_REMOTE_BUSY;
	} else if (vcpu_expects_wakeup(vcpu) &&
		   scheduler_will_preempt_current(vcpu)) {
		ret = VGIC_ROUTE_IMMEDIATE;
	} else {
		ret = VGIC_ROUTE_PREEMPTED;
	}

	return ret;
}

static bool
vgic_retry_unrouted_virq(vic_t *vic, virq_t virq);

// Attempt to wake a VCPU to handle a 1-of-N SPI.
//
// This should be called after flagging a 1-of-N SPI as unrouted.
static void
vgic_wakeup_1n(vic_t *vic, virq_t virq, bool class0, bool class1)
{
	// Check whether 1-of-N wakeups are permitted by the VM.
	GICD_CTLR_DS_t gicd_ctlr = atomic_load_relaxed(&vic->gicd_ctlr);
	if (!GICD_CTLR_DS_get_E1NWF(&gicd_ctlr)) {
		VGIC_DEBUG_TRACE(ROUTE, vic, NULL, "wakeup-1n {:d}: disabled",
				 virq);
		goto out;
	}

	// Ensure that the sleep state checks are ordered after the IRQs are
	// flagged as unrouted. There is a matching fence between entering sleep
	// state and checking for unrouted VIRQs in vgic_gicr_rd_set_sleep().
	atomic_thread_fence(memory_order_seq_cst);

	// Find a VCPU that has its GICR in sleep state.
	//
	// Per section 11.1 of the GICv3 spec, we are allowed to wake any
	// arbitrary VCPU and assume that it will eventually handle the
	// interrupt. We don't need to monitor whether that has happened.
	//
	// We always start this search from the VCPU corresponding to the
	// current physical CPU, to reduce the chances of waking a second
	// physical CPU if the GIC has just chosen to wake this one.
	count_t start_point = cpulocal_check_index(cpulocal_get_index_unsafe());
	for (index_t i = 0U; i < vic->gicr_count; i++) {
		cpu_index_t cpu =
			(cpu_index_t)((i + start_point) % vic->gicr_count);
		thread_t *candidate =
			atomic_load_consume(&vic->gicr_vcpus[cpu]);
		if ((candidate == NULL) ||
		    ((platform_irq_cpu_class(cpu) == 0U) ? !class0 : !class1)) {
			// VCPU is missing, or IRQ is not enabled for this
			// VCPU's class
			continue;
		}
		vgic_sleep_state_t sleep_state =
			atomic_load_relaxed(&candidate->vgic_sleep);
		while (sleep_state == VGIC_SLEEP_STATE_ASLEEP) {
			if (atomic_compare_exchange_weak_explicit(
				    &candidate->vgic_sleep, &sleep_state,
				    VGIC_SLEEP_STATE_WAKEUP_1N,
				    memory_order_acquire,
				    memory_order_acquire)) {
				VGIC_DEBUG_TRACE(
					ROUTE, vic, candidate,
					"wakeup-1n {:d}: waking GICR {:d}",
					virq, candidate->vgic_gicr_index);
				scheduler_lock(candidate);
				vcpu_wakeup(candidate);
				scheduler_unlock(candidate);
				goto out;
			}
		}
		if (sleep_state == VGIC_SLEEP_STATE_WAKEUP_1N) {
			VGIC_TRACE(ROUTE, vic, NULL,
				   "wakeup-1n {:d}: GICR {:d} already waking",
				   virq, candidate->vgic_gicr_index);
			goto out;
		}
	}

	// If the VIRQ's classes have no sleeping VCPUs but also no VCPUs that
	// are currently valid targets, we must consider two possibilities:
	// at least one VCPU is concurrently in its resume path, or all VCPUs
	// are concurrently in their suspend paths or hotplugged.
	//
	// The first case, which is much more likely, has a race in which the
	// following sequence might occur:
	//
	//   1. Core A tries to route VIRQ, fails due to disabled group
	//   2. Core B enables group
	//   3. Core B checks for unrouted IRQs, finds none
	//   4. Core A marks VIRQ as unrouted, then calls this function
	//
	// To avoid leaving the VIRQ unrouted in this case, we retry routing.
	if (!vgic_retry_unrouted_virq(vic, virq)) {
		VGIC_TRACE(ROUTE, vic, NULL, "wakeup-1n {:d}: already woken",
			   virq);
		goto out;
	}

	// If the retry didn't work, then either there is a VCPU in its wakeup
	// path that has not enabled its IRQ groups yet, or else all VCPUs are
	// in their suspend paths and have not enabled sleep yet. We retry all
	// unrouted IRQs when enabling either IRQ groups or sleep, so there's
	// nothing more to do here.
	VGIC_TRACE(ROUTE, vic, NULL, "wakeup-1n {:d}: failed", virq);

out:
	(void)0;
}
#endif

// Choose a VCPU to receive an interrupt, given its delivery state.
//
// For 1-of-N delivery, if the use_local_vcpu argument is set, we check the VCPU
// for the local physical CPU first. Otherwise, we use round-robin to select the
// first VCPU to check. This option is typically set for hardware IRQ
// deliveries, and clear otherwise.
//
// This may return NULL if there is no suitable route. It must be called from an
// RCU critical section.
thread_t *
vgic_get_route_from_state(vic_t *vic, vgic_delivery_state_t dstate,
			  bool use_local_vcpu)
{
#if VGIC_HAS_1N
	thread_t *target = NULL;

	// If not 1-of-N, find and return the direct target.
	if (compiler_expected(!vgic_delivery_state_get_route_1n(&dstate))) {
		index_t route_index = vgic_delivery_state_get_route(&dstate);
		if (route_index < vic->gicr_count) {
			target = atomic_load_consume(
				&vic->gicr_vcpus[route_index]);
		}
		goto out;
	}

	count_t start_point;
	if (use_local_vcpu) {
		// Assuming that any VM receiving physical 1-of-N IRQs has a
		// 1:1 VCPU to PCPU mapping, start by checking the local VCPU.
		start_point = cpulocal_check_index(cpulocal_get_index_unsafe());
	} else {
		// Determine the starting point for VIRQ selection using
		// round-robin, if we didn't get a hint from the physical GIC.
		start_point = atomic_fetch_add_explicit(
			&vic->rr_start_point, 1U, memory_order_relaxed);
	}

	// Ensure that i + start_point doesn't overflow below, because
	// we might fail to check all VCPUs in that case.
	start_point %= vic->gicr_count;

	// Look for the best target.
	vgic_route_preference_t target_pref = VGIC_ROUTE_DENIED;
	for (index_t i = 0U; i < vic->gicr_count; i++) {
		thread_t *candidate = atomic_load_consume(
			&vic->gicr_vcpus[(i + start_point) % vic->gicr_count]);
		if (candidate == NULL) {
			continue;
		}
		scheduler_lock(candidate);
		vgic_route_preference_t candidate_pref =
			vgic_route_1n_preference(vic, candidate, dstate);
		scheduler_unlock(candidate);
		if (compiler_expected(candidate_pref == VGIC_ROUTE_IMMEDIATE)) {
			target = candidate;
			VGIC_DEBUG_TRACE(ROUTE, vic, target,
					 "route: {:d} immediate, checked {:d}",
					 target->vgic_gicr_index,
					 ((register_t)i + 1U));
			goto out;
		}
		if (candidate_pref > target_pref) {
			target	    = candidate;
			target_pref = candidate_pref;
		}
	}

	// If we found a valid target, return it.
	//
	// This should be unconditional, and everything beyond this point
	// should be moved to after the VIRQ has been flagged as unrouted.
	//
	// FIXME:
	if (compiler_expected(target != NULL)) {
		VGIC_DEBUG_TRACE(ROUTE, vic, target, "route: {:d} best ({:d})",
				 target->vgic_gicr_index,
				 (uint64_t)target_pref);
		goto out;
	}

	GICD_CTLR_DS_t gicd_ctlr     = atomic_load_relaxed(&vic->gicd_ctlr);
	bool	       trace_IsE1NWF = GICD_CTLR_DS_get_E1NWF(&gicd_ctlr);
	VGIC_TRACE(ROUTE, vic, target, "route: none (E1NWF={:d})",
		   (register_t)trace_IsE1NWF);

out:
	return target;
#else
	(void)use_local_vcpu;
	index_t route_index = vgic_delivery_state_get_route(&dstate);
	return (route_index < vic->gicr_count)
		       ? atomic_load_consume(&vic->gicr_vcpus[route_index])
		       : NULL;
#endif
}

// Choose a VCPU to receive an SPI, given its IRQ number.
//
// This may return NULL if there is no suitable route. It must be called from an
// RCU critical section.
thread_t *
vgic_get_route_for_spi(vic_t *vic, virq_t virq, bool use_local_vcpu)
{
	assert(vgic_irq_is_spi(virq));
	_Atomic vgic_delivery_state_t *dstate =
		vgic_find_dstate(vic, NULL, virq);
	return vgic_get_route_from_state(vic, atomic_load_relaxed(dstate),
					 use_local_vcpu);
}

// Choose a VCPU to receive an unlisted interrupt, mark it pending, and trigger
// a wakeup.
//
// This is called when rerouting a pending interrupt after delisting it. This
// may occur in a few different cases which are not clearly distinguished by the
// VGIC's data structures:
//
// 1. a pending and delivered VIRQ is delisted by sync after being rerouted
// 2. a pending and delivered VIRQ is delisted by local delivery of a
//    higher-priority unlisted VIRQ
// 3. a pending and undelivered VIRQ (which was previously asserted remotely)
//    is delisted when its LR is chosen by another VIRQ prior to its sync being
//    handled
// 4. a pending 1-of-N routed VIRQ is undelivered by a VCPU group disable due
//    to a GICR_CTLR write or destruction of the VCPU
// 5. a pending 1-of-N routed VIRQ loses a race to be delivered to a VCPU
//    before it disables the relevant group, and needs to be rerouted
//
// In most of these cases, we need to check the current route register and
// priority register for the interrupt, and reroute it based on those values.
static bool
vgic_try_route_and_flag(vic_t *vic, virq_t virq,
			vgic_delivery_state_t new_dstate, bool use_local_vcpu)
	REQUIRE_PREEMPT_DISABLED
{
	rcu_read_start();
	thread_t *target =
		vgic_get_route_from_state(vic, new_dstate, use_local_vcpu);

	if (target != NULL) {
		uint8_t priority =
			vgic_delivery_state_get_priority(&new_dstate);
		vgic_flag_unlocked(virq, target, priority);
	}

	rcu_read_finish();

	return (target != NULL);
}

// Wrapper for vgic_try_route_and_flag() that flags the VIRQ as unrouted on
// failure, and triggers a 1-of-N wakeup.
static void
vgic_route_and_flag(vic_t *vic, virq_t virq, vgic_delivery_state_t new_dstate,
		    bool use_local_vcpu) REQUIRE_PREEMPT_DISABLED
{
	if (!vgic_try_route_and_flag(vic, virq, new_dstate, use_local_vcpu)) {
		vgic_flag_unrouted(vic, virq);
#if VGIC_HAS_1N
		vgic_wakeup_1n(vic, virq,
			       vgic_get_delivery_state_is_class0(&new_dstate),
			       vgic_get_delivery_state_is_class1(&new_dstate));
#endif
	}
}

static vgic_delivery_state_t
vgic_reclaim_update_level_src_and_hw(const vic_t *vic, const thread_t *vcpu,
				     virq_t		    virq,
				     vgic_delivery_state_t *old_dstate,
				     bool lr_active, bool lr_hw,
				     vgic_lr_status_t *status,
				     virq_source_t    *source)
	REQUIRE_PREEMPT_DISABLED
{
	bool lr_pending = (ICH_LR_EL2_base_get_State(&status->lr.base) ==
			   ICH_LR_EL2_STATE_PENDING) ||
			  (ICH_LR_EL2_base_get_State(&status->lr.base) ==
			   ICH_LR_EL2_STATE_PENDING_ACTIVE);
	vgic_delivery_state_t new_dstate;
	bool		      need_deactivate;

	// We should never try to reclaim an LR that has a pending EOI trap;
	// it isn't handled correctly below, and needs vgic_sync_lr().
	assert(lr_pending || lr_active ||
	       ICH_LR_EL2_base_get_HW(&status->lr.base) ||
	       !ICH_LR_EL2_HW0_get_EOI(&status->lr.sw));

	do {
		new_dstate	= *old_dstate;
		need_deactivate = false;

		vgic_delivery_state_set_active(&new_dstate, lr_active);
		vgic_delivery_state_set_listed(&new_dstate, false);
		vgic_delivery_state_set_need_sync(&new_dstate, false);
		vgic_delivery_state_set_hw_detached(&new_dstate, false);
		if (lr_pending) {
			vgic_delivery_state_set_edge(&new_dstate, true);
		}

		// Update level_src and hw_active based on the LR state.
		if (lr_hw && vgic_delivery_state_get_hw_active(old_dstate)) {
			// If it's a hardware IRQ that has already been marked
			// active somewhere else, we don't need to change its
			// state beyond the abave. For this to happen, it must
			// have been inactive in the LR already.
			assert(!lr_pending && !lr_active);
		} else if (lr_hw && lr_pending &&
			   vgic_delivery_state_get_need_sync(old_dstate) &&
			   !vgic_delivery_state_get_cfg_is_edge(old_dstate)) {
			// If it's a pending hardware level-triggered interrupt
			// that has been marked for sync, we clear its pending
			// state and deactivate it early to force the hardware
			// to re-check it (and possibly re-route it in 1-of-N
			// mode).
			vgic_delivery_state_set_level_src(&new_dstate, false);
			need_deactivate = true;
		} else if (lr_hw && (lr_pending || lr_active)) {
			// If it's a pending or active hardware IRQ, we must
			// re-set hw_active, and clear level_src if it has
			// been acknowledged.
			vgic_delivery_state_set_hw_active(&new_dstate, true);
			if (!lr_pending) {
				vgic_delivery_state_set_level_src(&new_dstate,
								  false);
			}
		} else if (lr_hw) {
			// If it's a hardware IRQ that was deactivated directly,
			// reset level_src to the old hw_active (which preserves
			// any remote assertion).
			vgic_delivery_state_set_level_src(
				&new_dstate,
				vgic_delivery_state_get_hw_active(old_dstate));
		} else if (vgic_delivery_state_get_level_src(old_dstate)) {
			// It's a software IRQ with level_src set; call the
			// source to check whether it's still pending, and order
			// the check_pending event after the dstate read.
			bool reassert =
				lr_pending ||
				vgic_delivery_state_get_edge(old_dstate);
			if (!vgic_virq_check_pending(source, reassert)) {
				vgic_delivery_state_set_level_src(&new_dstate,
								  false);
			}
		} else {
			// Software IRQ with level_src clear; nothing to do.
		}
	} while (!atomic_compare_exchange_strong_explicit(
		status->dstate, old_dstate, new_dstate, memory_order_relaxed,
		memory_order_relaxed));

	VGIC_TRACE(DSTATE_CHANGED, vic, vcpu, "reclaim_lr {:d}: {:#x} -> {:#x}",
		   virq, vgic_delivery_state_raw(*old_dstate),
		   vgic_delivery_state_raw(new_dstate));

	if (need_deactivate) {
		VGIC_TRACE(HWSTATE_CHANGED, vic, vcpu,
			   "reclaim_lr {:d}: deactivate HW IRQ {:d}",
			   ICH_LR_EL2_HW1_get_vINTID(&status->lr.hw),
			   ICH_LR_EL2_HW1_get_pINTID(&status->lr.hw));
		gicv3_irq_deactivate(ICH_LR_EL2_HW1_get_pINTID(&status->lr.hw));
	}

	return new_dstate;
}

// Clear out a VIRQ from a specified LR and flag it to be delivered later.
//
// This is used when there are no empty LRs available to deliver an IRQ, but
// an LR is occupied by an IRQ that is either lower-priority, or already
// acknowledged, or (in the current thread) already deactivated. It is also
// used when tearing down a VCPU permanently, so active IRQs can't be left
// in the LRs as they are for a normal group disable. In the latter case, the
// reroute argument should be true, to force the route to be recalculated.
//
// The specified VCPU must either be the current thread, or LR-locked by the
// caller and known not to be running remotely. If the specified VCPU is the
// current thread, the caller must rewrite the LR after calling this function.
//
// The specified LR must be occupied. If it contains an active interrupt
// (regardless of its pending state), it must be the lowest-priority listed
// active interrupt on the VCPU, to ensure that the active_unlisted stack is
// correctly ordered.
static void
vgic_reclaim_lr(vic_t *vic, thread_t *vcpu, index_t lr, bool reroute)
	REQUIRE_LOCK(vcpu->vgic_lr_owner_lock) REQUIRE_PREEMPT_DISABLED
{
	const bool	  from_self = vcpu == thread_get_self();
	vgic_lr_status_t *status    = &vcpu->vgic_lrs[lr];
	assert(status->dstate != NULL);

	if (from_self) {
		vgic_read_lr_state(lr);
	}

	virq_t virq	 = ICH_LR_EL2_base_get_vINTID(&status->lr.base);
	bool   lr_hw	 = ICH_LR_EL2_base_get_HW(&status->lr.base);
	bool   lr_active = (ICH_LR_EL2_base_get_State(&status->lr.base) ==
			    ICH_LR_EL2_STATE_ACTIVE) ||
			 (ICH_LR_EL2_base_get_State(&status->lr.base) ==
			  ICH_LR_EL2_STATE_PENDING_ACTIVE);

#if VGIC_HAS_LPI && GICV3_HAS_VLPI_V4_1
	if (vic->vsgis_enabled &&
	    (vgic_get_irq_type(virq) == VGIC_IRQ_TYPE_SGI)) {
		// vSGIs have no active state.
		lr_active = false;
	}
#endif

	if (lr_active) {
		index_t i = vcpu->vgic_active_unlisted_count % VGIC_PRIORITIES;
		vcpu->vgic_active_unlisted[i] = virq;
		vcpu->vgic_active_unlisted_count++;
	}

	virq_source_t	     *source	 = vgic_find_source(vic, vcpu, virq);
	vgic_delivery_state_t old_dstate = atomic_load_relaxed(status->dstate);

	vgic_delivery_state_t new_dstate = vgic_reclaim_update_level_src_and_hw(
		vic, vcpu, virq, &old_dstate, lr_active, lr_hw, status, source);

#if VGIC_HAS_1N
	if (vgic_delivery_state_get_route_1n(&new_dstate)) {
		vgic_spi_reset_route_1n(source, new_dstate);
	}
#endif

	// The LR is no longer in use; clear out the status structure.
	status->dstate	= NULL;
	status->lr.base = ICH_LR_EL2_base_default();

	// Determine how this IRQ will be delivered, if necessary.
	if (vgic_delivery_state_get_enabled(&new_dstate) &&
	    vgic_delivery_state_is_pending(&new_dstate) &&
	    !vgic_delivery_state_get_active(&new_dstate)) {
		if (reroute || vgic_delivery_state_get_need_sync(&old_dstate)) {
			vgic_route_and_flag(vic, virq, new_dstate, false);
		} else {
			// Note: CPU_INDEX_INVALID because this VCPU is always
			// either current or not running.
			vgic_flag_locked(
				virq, vcpu,
				vgic_delivery_state_get_priority(&new_dstate),
				vgic_delivery_state_get_group1(&new_dstate),
				CPU_INDEX_INVALID);
		}
	}
}

static bool
vgic_sync_vcpu(thread_t *vcpu, bool hw_access);

static void
vgic_list_irq(vgic_delivery_state_t new_dstate, index_t lr, bool is_hw,
	      uint8_t priority, _Atomic vgic_delivery_state_t *dstate,
	      virq_t virq, vic_t *vic, thread_t *vcpu, virq_source_t *source,
	      bool to_self) REQUIRE_LOCK(vcpu->vgic_lr_owner_lock)
	REQUIRE_PREEMPT_DISABLED
{
	assert(vgic_delivery_state_get_listed(&new_dstate));
	assert_debug(lr < CPU_GICH_LR_COUNT);

	vgic_lr_status_t *status = &vcpu->vgic_lrs[lr];
	if (status->dstate != NULL) {
		vgic_reclaim_lr(vic, vcpu, lr, false);
		assert(status->dstate == NULL);
	}

#if VGIC_HAS_1N
	if (vgic_delivery_state_get_route_1n(&new_dstate) && (source != NULL) &&
	    (source->trigger == VIRQ_TRIGGER_VGIC_FORWARDED_SPI)) {
		// Set the HW IRQ's route to the VCPU's current physical core
		hwirq_t *hwirq = hwirq_from_virq_source(source);
		(void)gicv3_spi_set_route(hwirq->irq, vcpu->vgic_irouter);
	}
#endif

	status->dstate = dstate;
	ICH_LR_EL2_base_set_HW(&status->lr.base, is_hw);
	if (is_hw) {
		ICH_LR_EL2_HW1_set_pINTID(&status->lr.hw,
					  hwirq_from_virq_source(source)->irq);
	} else {
		ICH_LR_EL2_HW0_set_EOI(
			&status->lr.sw,
			!vgic_delivery_state_get_cfg_is_edge(&new_dstate) &&
				vgic_delivery_state_is_level_asserted(
					&new_dstate));
	}
	ICH_LR_EL2_base_set_vINTID(&status->lr.base, virq);
	ICH_LR_EL2_base_set_Priority(&status->lr.base, priority);
	ICH_LR_EL2_base_set_Group(&status->lr.base,
				  vgic_delivery_state_get_group1(&new_dstate));
	ICH_LR_EL2_base_set_State(&status->lr.base, ICH_LR_EL2_STATE_PENDING);

	if (to_self) {
		vgic_write_lr(lr);
	}
}

typedef struct {
	bool need_wakeup;
	bool need_sync_all;
} vgic_deliver_list_or_flag_info_t;

static vgic_deliver_list_or_flag_info_t
vgic_deliver_list_or_flag(vic_t *vic, thread_t *vcpu, virq_source_t *source,
			  vgic_delivery_state_t old_dstate,
			  vgic_delivery_state_t new_dstate, index_result_t lr_r,
			  _Atomic vgic_delivery_state_t *dstate, virq_t virq,
			  cpu_index_t remote_cpu, uint8_t lr_priority,
			  bool is_private, bool to_self, bool is_hw,
			  uint8_t priority, bool pending, bool enabled,
			  bool route_valid)
	REQUIRE_LOCK(vcpu->vgic_lr_owner_lock) REQUIRE_PREEMPT_DISABLED
{
	bool need_wakeup   = true;
	bool need_sync_all = false;

	assert(vcpu != NULL);
	thread_t *target = vcpu;

	if (!pending) {
		// Not pending; nothing more to do.
		need_wakeup = false;
	} else if (vgic_delivery_state_get_listed(&old_dstate)) {
		// IRQ is already listed remotely; send a sync IPI.
		assert(vgic_delivery_state_get_need_sync(&new_dstate));
		if (!is_private) {
			need_sync_all = true;
			need_wakeup   = false;
		} else if (cpulocal_index_valid(remote_cpu)) {
			ipi_one(IPI_REASON_VGIC_SYNC, remote_cpu);
		} else {
			TRACE(VGIC, INFO,
			      "vgic sync after failed redeliver of {:#x}: dstate {:#x} -> {:#x}",
			      virq, vgic_delivery_state_raw(old_dstate),
			      vgic_delivery_state_raw(new_dstate));

			(void)vgic_sync_vcpu(target, to_self);
		}
	} else if (!enabled) {
		// Not enabled; nothing more to do.
		need_wakeup = false;
	} else if (!route_valid) {
		// The route became invalid after it was selected. Try to
		// re-route and flag it, and if that fails, flag it as unrouted.
		// This function issues a wakeup, so we don't need to do it
		// below.
		vgic_route_and_flag(vic, virq, new_dstate, false);
		need_wakeup = false;
	} else if ((lr_r.e == OK) && (priority < lr_priority)) {
		// List the IRQ immediately.
		vgic_list_irq(new_dstate, lr_r.r, is_hw, priority, dstate, virq,
			      vic, vcpu, source, to_self);
	} else {
		assert(route_valid);
		// We have a valid route, but can't immediately list; set the
		// search flags in the target VCPU so it finds this VIRQ next
		// time it goes looking for something to deliver. A delivery IPI
		// is sent if the target is currently running.
		vgic_flag_locked(virq, target, priority,
				 vgic_delivery_state_get_group1(&new_dstate),
				 remote_cpu);
	}

	return (vgic_deliver_list_or_flag_info_t){
		.need_wakeup   = need_wakeup,
		.need_sync_all = need_sync_all,
	};
}

typedef struct {
	vgic_delivery_state_t new_dstate;
	vgic_delivery_state_t old_dstate;
	bool		      need_wakeup;
	bool		      need_sync_all;
	uint8_t		      pad[2];
} vgic_deliver_info_t;

static vgic_deliver_info_t
vgic_deliver_update_state(virq_t virq, vgic_delivery_state_t prev_dstate,
			  vgic_delivery_state_t		 assert_dstate,
			  _Atomic vgic_delivery_state_t *dstate, vic_t *vic,
			  thread_t *vcpu, cpu_index_t remote_cpu,
			  virq_source_t *source, bool is_private, bool to_self)
	REQUIRE_LOCK(vcpu->vgic_lr_owner_lock) REQUIRE_PREEMPT_DISABLED
{
	// Keep track of the LR allocated for delivery (if any) and the priority
	// of the VIRQ currently in it (if any).
	index_result_t lr_r = index_result_error(ERROR_BUSY);
	uint8_t	       priority;
	uint8_t	       lr_priority	= GIC_PRIORITY_LOWEST;
	uint8_t	       checked_priority = GIC_PRIORITY_LOWEST;
	bool	       pending;
	bool	       enabled;
	bool	       route_valid;
	bool	       is_hw;

	// Clarify for the static analyser that we have not allocated an LR yet
	// at this point.
	assert(lr_r.e != OK);

	vgic_delivery_state_t new_dstate;
	vgic_delivery_state_t old_dstate    = prev_dstate;
	bool		      need_wakeup   = true;
	bool		      need_sync_all = false;

	do {
		new_dstate =
			vgic_delivery_state_union(old_dstate, assert_dstate);
		is_hw	 = vgic_delivery_state_get_hw_active(&new_dstate);
		priority = vgic_delivery_state_get_priority(&new_dstate);

		pending	    = vgic_delivery_state_is_pending(&new_dstate);
		enabled	    = vgic_delivery_state_get_enabled(&new_dstate);
		route_valid = (vcpu != NULL) &&
			      vgic_route_allowed(vic, vcpu, new_dstate);

		if (vgic_delivery_state_get_listed(&old_dstate)) {
			// Already listed (and not redelivered locally, above);
			// just request a sync.
			vgic_delivery_state_set_need_sync(&new_dstate, true);
			continue;
		}

		if (!route_valid || !pending || !enabled ||
		    vgic_delivery_state_get_active(&old_dstate)) {
			// Can't deliver; just update the delivery state.
			continue;
		}

		// Try to allocate an LR, unless we have already done so at a
		// priority no lower than the current one.
		if ((lr_r.e != OK) && (priority < checked_priority) &&
		    !cpulocal_index_valid(remote_cpu)) {
			lr_r = vgic_select_lr(vcpu, priority, &lr_priority);
			checked_priority = priority;
		}

		if ((lr_r.e == OK) && (priority < lr_priority)) {
			// We're newly listing the IRQ.
			vgic_delivery_state_set_listed(&new_dstate, true);
			vgic_delivery_state_set_edge(&new_dstate, false);
			vgic_delivery_state_set_hw_active(&new_dstate, false);
		}
	} while (!atomic_compare_exchange_strong_explicit(
		dstate, &old_dstate, new_dstate, memory_order_relaxed,
		memory_order_relaxed));

	VGIC_TRACE(DSTATE_CHANGED, vic, vcpu, "deliver {:d}: {:#x} -> {:#x}",
		   virq, vgic_delivery_state_raw(old_dstate),
		   vgic_delivery_state_raw(new_dstate));

	if (vcpu == NULL) {
		// VIRQ is unrouted. Flag it in the shared search bitmap.
		if (pending && enabled) {
			vgic_flag_unrouted(vic, virq);
#if VGIC_HAS_1N
			// If this is a 1-of-N VIRQ, we might need to pick a
			// VCPU to wake (if E1NWF is enabled).
			need_wakeup =
				vgic_delivery_state_get_route_1n(&new_dstate);
#else
			// There is no VCPU to wake.
			need_wakeup = false;
#endif
		} else {
			need_wakeup = false;
		}

		goto out;
	}

	vgic_deliver_list_or_flag_info_t info = vgic_deliver_list_or_flag(
		vic, vcpu, source, old_dstate, new_dstate, lr_r, dstate, virq,
		remote_cpu, lr_priority, is_private, to_self, is_hw, priority,
		pending, enabled, route_valid);

	need_wakeup   = info.need_wakeup;
	need_sync_all = info.need_sync_all;

out:
	return (vgic_deliver_info_t){
		.new_dstate    = new_dstate,
		.old_dstate    = old_dstate,
		.need_wakeup   = need_wakeup,
		.need_sync_all = need_sync_all,
	};
}

static void
vgic_deliver_update_spi_route(vgic_delivery_state_t old_dstate,
			      const vic_t *vic, const thread_t *vcpu,
			      cpu_index_t remote_cpu, virq_source_t *source)
{
#if !VGIC_HAS_1N
	(void)old_dstate;
#endif

	if ((source == NULL) ||
	    (source->trigger != VIRQ_TRIGGER_VGIC_FORWARDED_SPI)) {
		// Not a HW IRQ; don't try to update the route.
	}
#if VGIC_HAS_1N
	else if (vgic_delivery_state_get_route_1n(&old_dstate)) {
		// IRQ doesn't have a fixed route, so there is no need to update
		// it here. Note that we may update it later when it is listed.
	}
#endif
	else if (cpulocal_index_valid(remote_cpu)) {
		assert(vcpu != NULL);
		// HW IRQ was delivered on the wrong CPU, probably because the
		// VCPU was migrated. Update the route. Note that we don't need
		// to disable / enable the IRQ or execute any waits or barriers
		// here because we are tolerant of further misrouting.
		hwirq_t *hwirq = hwirq_from_virq_source(source);
		(void)gicv3_spi_set_route(hwirq->irq, vcpu->vgic_irouter);

		VGIC_TRACE(HWSTATE_CHANGED, vic, vcpu,
			   "lazy reroute {:d}: to cpu {:d}", hwirq->irq,
			   remote_cpu);
	} else {
		// Directly routed to the correct CPU or not routed to any CPU
		// yet; nothing to do.
	}
}

// Try to deliver a VIRQ to a specified target for a specified reason.
//
// The specified VCPU is the current route of the VIRQ if it is shared (in which
// case it may be NULL), or the owner of the VIRQ if it is private.
//
// The pending flags in assert_dstate will be asserted in the delivery state.
// This may be 0 if pending flags have already been set by the caller. This
// value must not have any flags set other than the four pending flags and the
// enabled flag.
//
// If the level_src pending bit or the hw_active bit is being set, the VIRQ
// source must be specified. Otherwise, the source may be NULL, even if a
// registered source exists for the VIRQ.
//
// The is_private flag should be set if the delivered interrupt cannot possibly
// be rerouted. This is used to reduce the set of VCPUs that receive IPIs when a
// currently listed interrupt is redelivered, e.g. on an SGI to a busy VCPU.
//
// If it is not possible to immediately list the VIRQ, the target's
// pending-check flags will be updated so it will find the VIRQ next time it
// goes looking for pending interrupts to assert.
//
// This function returns the previous delivery state.
vgic_delivery_state_t
vgic_deliver(virq_t virq, vic_t *vic, thread_t *vcpu, virq_source_t *source,
	     _Atomic vgic_delivery_state_t *dstate,
	     vgic_delivery_state_t assert_dstate, bool is_private)
{
	bool to_self	   = vcpu == thread_get_self();
	bool need_wakeup   = true;
	bool need_sync_all = false;

	assert((source != NULL) ||
	       !vgic_delivery_state_get_level_src(&assert_dstate));
	assert((source == NULL) ||
	       (vgic_get_irq_type(source->virq) == VGIC_IRQ_TYPE_PPI) ||
	       (vgic_get_irq_type(source->virq) == VGIC_IRQ_TYPE_SPI));

	cpu_index_t remote_cpu = vgic_lr_owner_lock(vcpu);

	vgic_delivery_state_t old_dstate = atomic_load_relaxed(dstate);
	vgic_delivery_state_t new_dstate =
		vgic_delivery_state_union(old_dstate, assert_dstate);

	if (vgic_delivery_state_get_listed(&old_dstate) &&
	    vgic_delivery_state_is_pending(&new_dstate) &&
	    vgic_delivery_state_get_enabled(&new_dstate) && (vcpu != NULL) &&
	    !cpulocal_index_valid(remote_cpu)) {
		// Fast path: try to reset the pending state in the LR. This can
		// fail if the LR is not found, e.g. because the routing has
		// changed. Note that this function updates dstate if it
		// succeeds, so we can skip the updates below.
		//
		// We don't check the route, priority or group enables here
		// because listed IRQs affected by changes in those since they
		// were first listed either don't need an immediate update, or
		// else will be updated by whoever is changing them.
		//
		// We only need to try this once, because the listed bit can't
		// be changed by anyone else while we're holding the LR lock.
		bool_result_t redeliver_wakeup = vgic_redeliver(
			vic, vcpu, source, dstate, &old_dstate, assert_dstate);
		if (redeliver_wakeup.e == OK) {
			need_wakeup = redeliver_wakeup.r;
			goto out;
		}
	}

	// If this is a physical SPI assertion, we may need to update the route
	// of the physical SPI.
	vgic_deliver_update_spi_route(old_dstate, vic, vcpu, remote_cpu,
				      source);

	// Update the dstate and deliver the interrupt
	vgic_deliver_info_t vgic_deliver_info = vgic_deliver_update_state(
		virq, old_dstate, assert_dstate, dstate, vic, vcpu, remote_cpu,
		source, is_private, to_self);

	new_dstate    = vgic_deliver_info.new_dstate;
	old_dstate    = vgic_deliver_info.old_dstate;
	need_wakeup   = vgic_deliver_info.need_wakeup;
	need_sync_all = vgic_deliver_info.need_sync_all;

out:
	vgic_lr_owner_unlock(vcpu);

	if (need_wakeup) {
		if (to_self) {
			vcpu_wakeup_self();
		} else if (vcpu != NULL) {
			scheduler_lock(vcpu);
			vcpu_wakeup(vcpu);
			scheduler_unlock(vcpu);
		} else {
#if VGIC_HAS_1N
			vgic_wakeup_1n(
				vic, virq,
				vgic_get_delivery_state_is_class0(&new_dstate),
				vgic_get_delivery_state_is_class1(&new_dstate));
#else
			// VIRQ is unrouted; there is no VCPU we can wake.
			assert(!need_wakeup);
#endif
		}
	}

	if (need_sync_all) {
		vgic_sync_all(vic, false);
	}

	return old_dstate;
}

void
vgic_sync_all(vic_t *vic, bool wakeup)
{
	rcu_read_start();

	for (index_t i = 0; i < vic->gicr_count; i++) {
		thread_t *vcpu = atomic_load_consume(&vic->gicr_vcpus[i]);
		if (thread_get_self() == vcpu) {
			wakeup = vgic_sync_vcpu(vcpu, true) || wakeup;
			if (wakeup) {
				vcpu_wakeup_self();
			}
		} else if (vcpu != NULL) {
			cpu_index_t lr_owner = vgic_lr_owner_lock(vcpu);
			if (!vcpu->vgic_group0_enabled &&
			    !vcpu->vgic_group1_enabled) {
				// Nothing should be listed on this CPU, so we
				// don't need to sync it.
			} else {
				if (cpulocal_index_valid(lr_owner)) {
					ipi_one(IPI_REASON_VGIC_SYNC, lr_owner);
				} else {
					wakeup = vgic_sync_vcpu(vcpu, false) ||
						 wakeup;
				}
			}
			vgic_lr_owner_unlock(vcpu);
			if (wakeup) {
				scheduler_lock(vcpu);
				vcpu_wakeup(vcpu);
				scheduler_unlock(vcpu);
			}
		} else {
			// No VCPU attached at this index, nothing to do
		}
	}

	rcu_read_finish();
}

static bool
vgic_gicr_update_group_enables(vic_t *vic, thread_t *gicr_vcpu,
			       GICD_CTLR_DS_t gicd_ctlr)
	REQUIRE_LOCK(gicr_vcpu->vgic_lr_owner_lock) REQUIRE_PREEMPT_DISABLED;

void
vgic_update_enables(vic_t *vic, GICD_CTLR_DS_t gicd_ctlr)
{
	preempt_disable();
	rcu_read_start();

	for (index_t i = 0; i < vic->gicr_count; i++) {
		thread_t   *vcpu     = atomic_load_consume(&vic->gicr_vcpus[i]);
		cpu_index_t lr_owner = vgic_lr_owner_lock_nopreempt(vcpu);
		if (thread_get_self() == vcpu) {
			if (vgic_gicr_update_group_enables(vic, vcpu,
							   gicd_ctlr)) {
				vcpu_wakeup_self();
			}
			vgic_lr_owner_unlock_nopreempt(vcpu);
		} else if (vcpu != NULL) {
			bool wakeup = false;
			if (cpulocal_index_valid(lr_owner)) {
				ipi_one(IPI_REASON_VGIC_ENABLE, lr_owner);
			} else {
				wakeup = vgic_gicr_update_group_enables(
					vic, vcpu, gicd_ctlr);
			}
			vgic_lr_owner_unlock_nopreempt(vcpu);
			if (wakeup) {
				scheduler_lock_nopreempt(vcpu);
				vcpu_wakeup(vcpu);
				scheduler_unlock_nopreempt(vcpu);
			}
		} else {
			// No VCPU attached at this index, nothing to do
			vgic_lr_owner_unlock_nopreempt(vcpu);
		}
	}

	rcu_read_finish();
	preempt_enable();
}

error_t
virq_clear(virq_source_t *source)
{
	error_t			       err    = ERROR_VIRQ_NOT_BOUND;
	_Atomic vgic_delivery_state_t *dstate = NULL;

	// The source's VIC and VCPU pointers are RCU-protected.
	rcu_read_start();

	// We must have a VIC to clear from (note that a disconnected source is
	// always considered clear).
	vic_t *vic = atomic_load_acquire(&source->vic);
	if (compiler_unexpected(vic == NULL)) {
		goto out;
	}

	// Try to find the current target VCPU. This may be inaccurate or NULL
	// for a shared IRQ, but must be correct for a private IRQ.
	thread_t *vcpu = vgic_find_target(vic, source);
	if (compiler_unexpected(vcpu == NULL) && source->is_private) {
		// The VIRQ has been concurrently unbound.
		goto out;
	}

	// At this point we can't fail.
	err = OK;

	// Clear the level_src bit in the delivery state.
	vgic_delivery_state_t clear_dstate = vgic_delivery_state_default();
	vgic_delivery_state_set_level_src(&clear_dstate, true);
	vgic_delivery_state_set_hw_active(&clear_dstate, true);

	dstate = vgic_find_dstate(vic, vcpu, source->virq);
	(void)vgic_undeliver(vic, vcpu, dstate, source->virq, clear_dstate,
			     false);

	// We ignore the result of vgic_undeliver() here, which increases the
	// chances that the VM will receive a spurious IRQ, on the basis that
	// it's cheaper to handle a spurious IRQ than to broadcast a sync that
	// may or may not succeed in preventing it. A caller that really cares
	// about this should be using a check-pending event.

out:
	rcu_read_finish();

	return err;
}

bool_result_t
virq_query(virq_source_t *source)
{
	bool_result_t result = bool_result_error(ERROR_VIRQ_NOT_BOUND);

	rcu_read_start();

	if (source == NULL) {
		goto out;
	}

	vic_t *vic = atomic_load_acquire(&source->vic);
	if (compiler_unexpected(vic == NULL)) {
		goto out;
	}

	// If the VIRQ is private, we must find its target VCPU.
	thread_t *vcpu = NULL;
	if (source->is_private) {
		vcpu = vgic_find_target(vic, source);
		if (compiler_unexpected(vcpu == NULL)) {
			goto out;
		}
	}

	_Atomic vgic_delivery_state_t *dstate =
		vgic_find_dstate(vic, vcpu, source->virq);
	assert(dstate != NULL);

	vgic_delivery_state_t cur_dstate = atomic_load_relaxed(dstate);
	result = bool_result_ok(vgic_delivery_state_get_level_src(&cur_dstate));
out:
	rcu_read_finish();

	return result;
}

// Handle an EOI maintenance interrupt.
//
// These are enabled for all level-triggered interrupts with non-hardware
// sources; this includes registered VIRQ sources, ISPENDR writes, and SETSPI
// writes. They are also enabled when an edge triggered interrupt is asserted
// by software and hardware sources simultaneously.
//
// The specified VCPU must be the current thread. The specified LR must be in
// the invalid state in hardware, but have a software-asserted VIRQ associated
// with it.
static void
vgic_handle_eoi_lr(vic_t *vic, thread_t *vcpu, index_t lr)
	REQUIRE_PREEMPT_DISABLED
{
	assert(thread_get_self() == vcpu);
	assert_debug(lr < CPU_GICH_LR_COUNT);

	// The specified LR should have a software delivery listed in it
	vgic_lr_status_t *status = &vcpu->vgic_lrs[lr];
	assert(status->dstate != NULL);
	assert(!ICH_LR_EL2_base_get_HW(&status->lr.base));

	vgic_read_lr_state(lr);
	(void)vgic_sync_lr(vic, vcpu, status, vgic_delivery_state_default(),
			   true);
	vgic_write_lr(lr);
}

typedef struct {
	vgic_delivery_state_t new_dstate;
	bool		      need_deactivate;
	bool		      res;
	uint8_t		      pad[2];
} vgic_deactivate_info_t;

static vgic_deactivate_info_t
vgic_do_deactivate(const vic_t *vic, const thread_t *vcpu, virq_t virq,
		   _Atomic vgic_delivery_state_t *dstate,
		   vgic_delivery_state_t old_dstate, bool set_edge,
		   bool hw_active, virq_source_t *source, bool local_listed)
	REQUIRE_PREEMPT_DISABLED
{
	bool		      res = false;
	vgic_delivery_state_t new_dstate;
	bool		      need_deactivate;

	do {
		new_dstate	= old_dstate;
		need_deactivate = false;

		if (local_listed) {
			// Nobody else should delist the IRQ from under us.
			assert(vgic_delivery_state_get_listed(&old_dstate) ==
			       true);
			vgic_delivery_state_set_listed(&new_dstate, false);
			vgic_delivery_state_set_need_sync(&new_dstate, false);
			vgic_delivery_state_set_hw_detached(&new_dstate, false);
			if (set_edge) {
				vgic_delivery_state_set_edge(&new_dstate, true);
			}
		} else {
			if (vgic_delivery_state_get_listed(&old_dstate)) {
				// Somebody else has listed the interrupt
				// already. It must have been deactivated some
				// other way, e.g. by a previous ICACTIVE write,
				// so we have nothing to do here.
				VGIC_TRACE(
					DSTATE_CHANGED, vic, vcpu,
					"deactivate {:d}: already listed {:#x}",
					virq,
					vgic_delivery_state_raw(old_dstate));
				res = true;
				goto out;
			}
			if (!vgic_delivery_state_get_active(&old_dstate)) {
				// Interrupt is already inactive; we have
				// nothing to do.
				VGIC_TRACE(
					DSTATE_CHANGED, vic, vcpu,
					"deactivate {:d}: already inactive {:#x}",
					virq,
					vgic_delivery_state_raw(old_dstate));
				res = true;
				goto out;
			}
			assert(!set_edge && !hw_active);
			vgic_delivery_state_set_active(&new_dstate, false);
		}

		// If the hw_active bit is set but the edge bit is not, we are
		// deactivating an acknowledged hardware interrupt.
		if (hw_active ||
		    (vgic_delivery_state_get_hw_active(&old_dstate) &&
		     !vgic_delivery_state_get_edge(&old_dstate))) {
			need_deactivate = true;
			vgic_delivery_state_set_hw_active(&new_dstate, false);
		}

		// If level_src is set, check that the source is still pending
		// before we try to deliver it.
		if (vgic_delivery_state_get_level_src(&old_dstate)) {
			if (!vgic_virq_check_pending(
				    source, vgic_delivery_state_get_edge(
						    &new_dstate))) {
				vgic_delivery_state_set_level_src(&new_dstate,
								  false);
			}
		}
	} while (!atomic_compare_exchange_strong_explicit(
		dstate, &old_dstate, new_dstate, memory_order_relaxed,
		memory_order_relaxed));

	VGIC_TRACE(DSTATE_CHANGED, vic, vcpu, "deactivate {:d}: {:#x} -> {:#x}",
		   virq, vgic_delivery_state_raw(old_dstate),
		   vgic_delivery_state_raw(new_dstate));

out:
	return (vgic_deactivate_info_t){
		.new_dstate	 = new_dstate,
		.need_deactivate = need_deactivate,
		.res		 = res,
	};
}

// Handle a software deactivate of a specific VIRQ.
//
// This may be called by the DIR trap handler if the VM's EOImode is 1, by the
// LRENP maintenance interrupt handler if the VM's EOImode is 0, or by the
// ICACTIVER trap handler in either case.
//
// If the interrupt is listed, the specified VCPU must be the current VCPU, and
// the list register must be known to be in active or pending+active state. In
// this case, the set_edge parameter determines whether the edge bit will be
// set, and the set_hw_active parameter determines whether the hw_active bit
// will be set.
//
// The specified old_dstate value must have been load-acquired before checking
// the listed bit to decide whether to call this function.
void
vgic_deactivate(vic_t *vic, thread_t *vcpu, virq_t virq,
		_Atomic vgic_delivery_state_t *dstate,
		vgic_delivery_state_t old_dstate, bool set_edge, bool hw_active)
	REQUIRE_PREEMPT_DISABLED
{
	bool local_listed = vgic_delivery_state_get_listed(&old_dstate);
	assert(!local_listed || (thread_get_self() == vcpu));

	// Find the registered source, if any.
	rcu_read_start();
	virq_source_t *source = vgic_find_source(vic, vcpu, virq);

	// Clear active in the delivery state, and level_src too if necessary.
	vgic_delivery_state_t new_dstate;
	bool		      need_deactivate;
	bool		      res = false;
	{
		vgic_deactivate_info_t vgic_deactivate_info =
			vgic_do_deactivate(vic, vcpu, virq, dstate, old_dstate,
					   set_edge, hw_active, source,
					   local_listed);
		res		= vgic_deactivate_info.res;
		new_dstate	= vgic_deactivate_info.new_dstate;
		need_deactivate = vgic_deactivate_info.need_deactivate;
	}
	if (res) {
		goto out;
	}

	// If the interrupt is hardware-sourced then forward the deactivation to
	// the hardware.
	if (need_deactivate) {
		assert((source != NULL) &&
		       (source->trigger == VIRQ_TRIGGER_VGIC_FORWARDED_SPI));
		hwirq_t *hwirq = hwirq_from_virq_source(source);
		VGIC_TRACE(HWSTATE_CHANGED, vic, vcpu,
			   "deactivate {:d}: deactivate HW IRQ {:d}", virq,
			   hwirq->irq);
		irq_deactivate(hwirq);
	}

	// If the interrupt is still pending, deliver it immediately. Note that
	// this can't be HW=1, even if the interrupt we just deactivated was,
	// because the physical IRQ is inactive (above). It might be a software
	// delivery that occurred while the physical source was active.
	if (vgic_delivery_state_is_pending(&new_dstate) &&
	    vgic_delivery_state_get_enabled(&new_dstate)) {
		thread_t *new_target =
			vgic_get_route_from_state(vic, new_dstate, false);
		if (new_target != NULL) {
			(void)vgic_deliver(virq, vic, new_target, source,
					   dstate,
					   vgic_delivery_state_default(),
					   !vgic_irq_is_spi(virq));
		}
	}

out:
	rcu_read_finish();
	(void)0;
}

static void
vgic_deactivate_unlisted(vic_t *vic, thread_t *vcpu, virq_t virq)
	REQUIRE_PREEMPT_DISABLED
{
	_Atomic vgic_delivery_state_t *dstate =
		vgic_find_dstate(vic, vcpu, virq);
	vgic_delivery_state_t old_dstate = atomic_load_relaxed(dstate);
	if (vgic_delivery_state_get_listed(&old_dstate)) {
		// Somebody else must have deactivated it already, so ignore the
		// deactivate.
		VGIC_TRACE(DSTATE_CHANGED, vic, vcpu,
			   "deactivate {:d}: already re-listed ({:#x})", virq,
			   vgic_delivery_state_raw(old_dstate));
	} else {
		vgic_deactivate(vic, vcpu, virq, dstate, old_dstate, false,
				false);
	}
}

// Handle an unlisted EOI signalled by an LRENP maintenance interrupt.
//
// The specified VCPU must be the current thread.
static void
vgic_handle_unlisted_eoi(vic_t *vic, thread_t *vcpu) REQUIRE_PREEMPT_DISABLED
{
	assert(thread_get_self() == vcpu);

	vcpu->vgic_active_unlisted_count--;
	index_t i    = vcpu->vgic_active_unlisted_count % VGIC_PRIORITIES;
	virq_t	virq = vcpu->vgic_active_unlisted[i];

	// The hardware has already dropped the active priority, based on the
	// assumption that the highest active priority belongs to this IRQ. All
	// we need to do is deactivate.
	vgic_deactivate_unlisted(vic, vcpu, virq);
}

// List the given VIRQ in the given LR if it is enabled, pending, routable to
// the given VCPU, not listed elsewhere, and has priority equal or higher
// (less) than the specified limit.
//
// The VCPU must be the current owner of the LRs on the calling CPU.
//
// The specified LR must be either already empty, or occupied by a VIRQ with
// priority strictly lower (greater) than the specified mask.
//
// This function returns OK if the given VIRQ was listed, ERROR_DENIED if the
// VIRQ cannot be delivered due to the priority limit or the VCPU's group
// disables (so it should remain flagged), and any other error code if the VIRQ
// cannot be delivered due to its state (disabled, active, already listed, etc).
static error_t
vgic_list_if_pending(vic_t *vic, thread_t *vcpu, virq_t virq,
		     uint8_t priority_limit, index_t lr)
	REQUIRE_LOCK(vcpu->vgic_lr_owner_lock) REQUIRE_PREEMPT_DISABLED
{
	error_t err;
	uint8_t priority;

	// Find the delivery state.
	_Atomic vgic_delivery_state_t *dstate =
		vgic_find_dstate(vic, vcpu, virq);

	vgic_delivery_state_t old_dstate = atomic_load_relaxed(dstate);
	vgic_delivery_state_t new_dstate;
	do {
		if (!vgic_delivery_state_get_enabled(&old_dstate) ||
		    !vgic_delivery_state_is_pending(&old_dstate)) {
			err = ERROR_IDLE;
			goto out;
		}

		if (vgic_delivery_state_get_listed(&old_dstate) ||
		    vgic_delivery_state_get_active(&old_dstate)) {
			err = ERROR_BUSY;
			goto out;
		}

		priority = vgic_delivery_state_get_priority(&old_dstate);
		if ((priority > priority_limit) ||
		    (vgic_delivery_state_get_group1(&old_dstate)
			     ? !vcpu->vgic_group1_enabled
			     : !vcpu->vgic_group0_enabled)) {
			err = ERROR_DENIED;
			goto out;
		}

		// Note: this must be checked _after_ the group disables,
		// because it checks the group disables itself and would
		// incorrectly drop the pending state of a VIRQ blocked by them.
		if (!vgic_route_allowed(vic, vcpu, old_dstate)) {
			err = ERROR_IDLE;
			goto out;
		}

		new_dstate = old_dstate;
		vgic_delivery_state_set_listed(&new_dstate, true);
		vgic_delivery_state_set_edge(&new_dstate, false);
		vgic_delivery_state_set_hw_active(&new_dstate, false);
	} while (!atomic_compare_exchange_strong_explicit(
		dstate, &old_dstate, new_dstate, memory_order_relaxed,
		memory_order_relaxed));

	VGIC_TRACE(DSTATE_CHANGED, vic, vcpu,
		   "list_if_pending {:d}: {:#x} -> {:#x}", virq,
		   vgic_delivery_state_raw(old_dstate),
		   vgic_delivery_state_raw(new_dstate));

	bool	       to_self = (vcpu == thread_get_self());
	bool	       is_hw   = vgic_delivery_state_get_hw_active(&old_dstate);
	virq_source_t *source  = vgic_find_source(vic, vcpu, virq);

	vgic_list_irq(new_dstate, lr, is_hw, priority, dstate, virq, vic, vcpu,
		      source, to_self);

	err = OK;
out:
	return err;
}

static bool
vgic_find_pending_at_priority(vic_t *vic, thread_t *vcpu, index_t prio_index,
			      index_t lr, bool *reset_prio)
	REQUIRE_LOCK(vcpu->vgic_lr_owner_lock) REQUIRE_PREEMPT_DISABLED
{
	bool	listed	 = false;
	uint8_t priority = (uint8_t)(prio_index << VGIC_PRIO_SHIFT);

	_Atomic BITMAP_DECLARE_PTR(VGIC_LOW_RANGES, ranges) =
		&vcpu->vgic_search_ranges_low[prio_index];
	BITMAP_ATOMIC_FOREACH_SET_BEGIN(range, *ranges, VGIC_LOW_RANGES)
		if (compiler_unexpected(!bitmap_atomic_test_and_clear(
			    *ranges, range, memory_order_acquire))) {
			continue;
		}

		bool reset_range = false;
		for (index_t i = 0; i < vgic_low_range_size(range); i++) {
			virq_t virq =
				(virq_t)((range * VGIC_LOW_RANGE_SIZE) + i);

			error_t err = vgic_list_if_pending(vic, vcpu, virq,
							   priority, lr);
			if (err == OK) {
				listed = true;
				break;
			} else if (err == ERROR_DENIED) {
				reset_range = true;
				*reset_prio = true;
			} else {
				// Unable to list
			}
		}

		// If we listed a VIRQ in this range, then we (probably)
		// did not check the entire range, so we need to reset
		// the range's search bit in case there are more VIRQs.
		if (listed) {
			bitmap_atomic_set(*ranges, range, memory_order_relaxed);
			break;
		}

		// If we found a VIRQ in this range that was pending,
		// but we were unable to deliver it to this VCPU due to
		// priority or group disables, reset the range bit.
		if (reset_range) {
			bitmap_atomic_set(*ranges, range, memory_order_relaxed);
		}
	BITMAP_ATOMIC_FOREACH_SET_END

	return listed;
}

// Search for a pending VIRQ to list in the given LR; it must have priority
// strictly higher (less) than the specified mask.
//
// This is used to handle NP maintenance interrupts and delivery IPIs. The
// specified VCPU must be the current thread. The specified LR is either
// empty, or contains a VIRQ with priority equal or lower (greater) than
// the specified mask.
//
// This function returns true if a VIRQ was listed, and false otherwise.
static bool
vgic_find_pending_and_list(vic_t *vic, thread_t *vcpu, uint8_t priority_mask,
			   index_t lr) REQUIRE_LOCK(vcpu->vgic_lr_owner_lock)
	REQUIRE_PREEMPT_DISABLED
{
	bool	listed		= false;
	index_t prio_mask_index = (index_t)priority_mask >> VGIC_PRIO_SHIFT;

	_Atomic BITMAP_DECLARE_PTR(VGIC_PRIORITIES, prios) =
		&vcpu->vgic_search_prios;
	BITMAP_ATOMIC_FOREACH_SET_BEGIN(prio_index, *prios, prio_mask_index)
		if (compiler_unexpected(!bitmap_atomic_test_and_clear(
			    *prios, prio_index, memory_order_acquire))) {
			continue;
		}

		bool reset_prio = false;
#if !GICV3_HAS_VLPI && VGIC_HAS_LPI
#error lpi search ranges not implemented
#endif
		listed = vgic_find_pending_at_priority(vic, vcpu, prio_index,
						       lr, &reset_prio);

		// If we listed a VIRQ at this priority, then we (probably) did
		// not check every range, so we need to reset the priority's
		// search bit in case there ore more VIRQs.
		if (listed) {
			bitmap_atomic_set(*prios, prio_index,
					  memory_order_release);
			break;
		}

		// If we found a VIRQ at this priority that was pending, but we
		// were unable to deliver it to this VCPU due to priority or
		// group disables, reset the priority bit.
		if (reset_prio) {
			bitmap_atomic_set(*prios, prio_index,
					  memory_order_release);
		}
	BITMAP_ATOMIC_FOREACH_SET_END

	return listed;
}

static void
vgic_try_to_list_pending(thread_t *vcpu, vic_t *vic) REQUIRE_PREEMPT_DISABLED
{
	asm_context_sync_ordered(&gich_lr_ordering);
	cpu_index_t lr_owner = vgic_lr_owner_lock_nopreempt(vcpu);
	assert(lr_owner == CPU_INDEX_INVALID);
	register_t elrsr =
		register_ICH_ELRSR_EL2_read_ordered(&gich_lr_ordering);
	elrsr &= util_mask(CPU_GICH_LR_COUNT);

	// If no LRs are empty, find the lowest priority active one.
	if (elrsr == 0U) {
		uint8_t	       lr_priority = GIC_PRIORITY_LOWEST;
		index_result_t lr_r =
			vgic_select_lr(vcpu, GIC_PRIORITY_LOWEST, &lr_priority);
		if (lr_r.e == OK) {
			assert(lr_priority == GIC_PRIORITY_LOWEST);
			elrsr = util_bit(lr_r.r);
		}
	}

	// Attempt to list in all empty LRs (or in the active one we
	// selected above), until we run out of pending IRQs.
	while (elrsr != 0U) {
		index_t lr = compiler_ctz(elrsr);
		elrsr &= ~util_bit(lr);

		assert_debug(lr < CPU_GICH_LR_COUNT);

		if (vgic_find_pending_and_list(vic, vcpu, GIC_PRIORITY_LOWEST,
					       lr)) {
			vcpu_wakeup_self();
		} else {
			// Nothing left deliverable; clear NPIE.
			vcpu->vgic_ich_hcr = register_ICH_HCR_EL2_read();
			ICH_HCR_EL2_set_NPIE(&vcpu->vgic_ich_hcr, false);
			register_ICH_HCR_EL2_write(vcpu->vgic_ich_hcr);
			break;
		}
	}
	vgic_lr_owner_unlock_nopreempt(vcpu);
}

bool
vgic_handle_irq_received_maintenance(void)
{
	assert_preempt_disabled();

	thread_t *vcpu = thread_get_self();
	vic_t	 *vic  = vcpu->vgic_vic;

	if (compiler_unexpected((vcpu->kind != THREAD_KIND_VCPU) ||
				(vic == NULL))) {
		// Spurious IRQ; this can happen if a maintenance interrupt
		// is asserted shortly before a context switch, and the GICR
		// hasn't yet that it is no longer asserted by the time we
		// re-enable interrupts.
		//
		// If the context switch in question is to another VCPU, we
		// won't notice that the IRQ is spurious, but that doesn't do
		// any harm.
		goto out;
	}

	ICH_MISR_EL2_t misr = register_ICH_MISR_EL2_read();

	// The underflow interrupt is always disabled; we don't need it because
	// we never re-list delisted active interrupts
	assert(!ICH_MISR_EL2_get_U(&misr));

	if (ICH_MISR_EL2_get_EOI(&misr)) {
		register_t eisr = register_ICH_EISR_EL2_read();
		while (eisr != 0U) {
			index_t lr = compiler_ctz(eisr);
			eisr &= ~util_bit(lr);

			vgic_handle_eoi_lr(vic, vcpu, lr);
		}
	}

	if (ICH_MISR_EL2_get_LRENP(&misr)) {
		vcpu->vgic_ich_hcr = register_ICH_HCR_EL2_read();
		count_t eoicount =
			ICH_HCR_EL2_get_EOIcount(&vcpu->vgic_ich_hcr);

		for (count_t i = 0; i < eoicount; i++) {
			vgic_handle_unlisted_eoi(vic, vcpu);
		}

		ICH_HCR_EL2_set_EOIcount(&vcpu->vgic_ich_hcr, 0U);
		register_ICH_HCR_EL2_write(vcpu->vgic_ich_hcr);
	}

	if (!vgic_fgt_allowed()) {
		// Check for enable bit changes. This will clear out all of the
		// LRs and redo any deliveries, so we can skip the none-pending
		// handling.
		if (ICH_MISR_EL2_get_VGrp0D(&misr) ||
		    ICH_MISR_EL2_get_VGrp1D(&misr) ||
		    ICH_MISR_EL2_get_VGrp0E(&misr) ||
		    ICH_MISR_EL2_get_VGrp1E(&misr)) {
			GICD_CTLR_DS_t gicd_ctlr =
				atomic_load_acquire(&vic->gicd_ctlr);
			cpu_index_t lr_owner =
				vgic_lr_owner_lock_nopreempt(vcpu);
			assert(lr_owner == CPU_INDEX_INVALID);
			VGIC_TRACE(ASYNC_EVENT, vic, vcpu,
				   "group enable maintenance: {:#x}",
				   ICH_MISR_EL2_raw(misr));
			if (vgic_gicr_update_group_enables(vic, vcpu,
							   gicd_ctlr)) {
				vcpu_wakeup_self();
			}
			vgic_lr_owner_unlock_nopreempt(vcpu);
			goto out;
		}
	}

	// Always try to deliver more interrupts if the NP interrupt is enabled,
	// regardless of whether it is actually asserted. Note that NP may have
	// become asserted as a result of EOI or group disable handling above,
	// so we would have to reread MISR to get the right value anyway.
	if (ICH_HCR_EL2_get_NPIE(&vcpu->vgic_ich_hcr)) {
		vgic_try_to_list_pending(vcpu, vic);
	}

out:
	return true;
}

// Synchronise the delivery state of a single occupied LR in the current thread
// with the VIRQ's GICD / GICR configuration.
//
// The given LR must have an assigned VIRQ, and the hardware state of the LR
// must already have been read into status->lr.
//
// This function returns true if the LR needs to be modified.
static bool
vgic_sync_one(vic_t *vic, thread_t *vcpu, index_t lr)
	REQUIRE_LOCK(vcpu->vgic_lr_owner_lock)
		REQUIRE_PREEMPT_DISABLED EXCLUDE_SCHEDULER_LOCK(vcpu)
{
	assert_debug(lr < CPU_GICH_LR_COUNT);
	vgic_lr_status_t *status = &vcpu->vgic_lrs[lr];
	assert(status->dstate != NULL);
	bool need_update = false;

	vgic_delivery_state_t old_dstate = atomic_load_relaxed(status->dstate);
	if (vgic_delivery_state_get_hw_detached(&old_dstate) ||
	    vgic_delivery_state_get_need_sync(&old_dstate)) {
		(void)vgic_sync_lr(vic, vcpu, status,
				   vgic_delivery_state_default(), true);
		need_update = true;
	}

	return need_update;
}

// Check all LRs for the need-sync flag and synchronise if necessary.
//
// This is called when a sync IPI is either received, or short-circuited during
// context switch; it is also called before blocking on a sync flag. In any case
// we need to check each listed VIRQ for the need-sync bit, and when it is
// found, re-check the deliverability of the VIRQ to the specified CPU (enabled,
// routed, etc).
//
// If the hw_access argument is true, the current LR states are read back from
// hardware, and updated in hardware if necessary. Otherwise they are assumed to
// be up to date already.
//
// The specified VCPU must either be the one that owns the LRs on the physical
// CPU (i.e. either current, or the previous thread in context_switch_post),
// or else be LR-locked and not running.
static bool
vgic_sync_vcpu(thread_t *vcpu, bool hw_access)
	REQUIRE_LOCK(vcpu->vgic_lr_owner_lock) REQUIRE_PREEMPT_DISABLED
{
	bool wakeup = false;

	assert(vcpu != NULL);
	assert((thread_get_self() == vcpu) == hw_access);

	vic_t *vic = vcpu->vgic_vic;

	if (compiler_expected(vic != NULL)) {
		for (index_t i = 0; i < CPU_GICH_LR_COUNT; i++) {
			vgic_lr_status_t *status = &vcpu->vgic_lrs[i];
			if (status->dstate == NULL) {
				continue;
			}
			if (hw_access) {
				assert(thread_get_self() == vcpu);
				vgic_read_lr_state(i);
			}
			if (vgic_sync_one(vic, vcpu, i) && hw_access) {
				vgic_write_lr(i);
			}
		}
	}

	return wakeup;
}

void
vgic_handle_thread_save_state(void)
{
	thread_t *vcpu = thread_get_self();
	assert(vcpu != NULL);
	vic_t *vic = vcpu->vgic_vic;

	if (vic != NULL) {
		for (index_t i = 0; i < CPU_GICH_LR_COUNT; i++) {
			vgic_lr_status_t *status = &vcpu->vgic_lrs[i];
			if (status->dstate == NULL) {
				continue;
			}
			vgic_read_lr_state(i);
		}

		gicv3_read_ich_aprs(vcpu->vgic_ap0rs, vcpu->vgic_ap1rs);
		vcpu->vgic_ich_hcr  = register_ICH_HCR_EL2_read();
		vcpu->vgic_ich_vmcr = register_ICH_VMCR_EL2_read();
	}
}

static bool
vgic_do_delivery_check(vic_t *vic, thread_t *vcpu)
	REQUIRE_LOCK(vcpu->vgic_lr_owner_lock) REQUIRE_PREEMPT_DISABLED
{
	bool wakeup = false;

	vgic_sleep_state_t sleep_state = atomic_load_relaxed(&vcpu->vgic_sleep);
	if (sleep_state != VGIC_SLEEP_STATE_AWAKE) {
		// The GICR is asleep. We can't deliver anything.
		ICH_HCR_EL2_set_NPIE(&vcpu->vgic_ich_hcr, false);

#if VGIC_HAS_1N
		if (sleep_state == VGIC_SLEEP_STATE_WAKEUP_1N) {
			// The GICR has been chosen for 1-of-N wakeup.
			wakeup = true;
			goto out;
		}
#endif

		// If anything is flagged for delivery, wake up immediately.
		wakeup = !bitmap_atomic_empty(vcpu->vgic_search_prios,
					      VGIC_PRIORITIES);
		goto out;
	}

	if (!vcpu->vgic_group0_enabled && !vcpu->vgic_group1_enabled) {
		// Both groups are disabled; no VIRQs are deliverable.
		ICH_HCR_EL2_set_NPIE(&vcpu->vgic_ich_hcr, false);
		goto out;
	}

	index_t prio_index_cutoff = VGIC_PRIORITIES;
	while (!bitmap_atomic_empty(vcpu->vgic_search_prios,
				    prio_index_cutoff)) {
		uint8_t lowest_prio    = GIC_PRIORITY_HIGHEST;
		index_t lowest_prio_lr = 0U;

		// Search for any LR we can safely deliver to.
		for (index_t i = 0; i < CPU_GICH_LR_COUNT; i++) {
			vgic_lr_status_t *status = &vcpu->vgic_lrs[i];

			if ((status->dstate == NULL) ||
			    vgic_lr_is_empty(status->lr)) {
				// LR is empty; we can fill it immediately.
				lowest_prio_lr = i;
				lowest_prio    = GIC_PRIORITY_LOWEST;
				break;
			}

			if (ICH_LR_EL2_base_get_State(&status->lr.base) !=
			    ICH_LR_EL2_STATE_INVALID) {
				// LR is valid; we can try to replace the IRQ in
				// it if it has the (possibly equal) lowest
				// priority of all valid LRs.
				uint8_t this_prio =
					ICH_LR_EL2_base_get_Priority(
						&status->lr.base);
				if (this_prio >= lowest_prio) {
					lowest_prio_lr = i;
					lowest_prio    = this_prio;
				}
			}
		}

		if (lowest_prio > GIC_PRIORITY_HIGHEST) {
			if (vgic_find_pending_and_list(vic, vcpu, lowest_prio,
						       lowest_prio_lr)) {
				wakeup = true;
			} else {
				break;
			}

		} else {
			break;
		}

		// We can't deliver IRQs that are equal or lower (numerically
		// greater) priority than the lowest-priority pending LR, so
		// exclude them from the next vgic_search_prios check.
		prio_index_cutoff = (index_t)lowest_prio >> VGIC_PRIO_SHIFT;
	}

	ICH_HCR_EL2_set_NPIE(&vcpu->vgic_ich_hcr,
			     !bitmap_atomic_empty(vcpu->vgic_search_prios,
						  VGIC_PRIORITIES));

out:
	return wakeup;
}

static bool
vgic_retry_unrouted_virq(vic_t *vic, virq_t virq)
{
	assert(vic != NULL);
	// Only SPIs can be unrouted
	assert(vgic_irq_is_spi(virq));

	preempt_disable();

	_Atomic vgic_delivery_state_t *dstate =
		vgic_find_dstate(vic, NULL, virq);
	assert(dstate != NULL);
	vgic_delivery_state_t current_dstate = atomic_load_relaxed(dstate);

	bool unclaimed = false;
	if (vgic_delivery_state_get_enabled(&current_dstate) &&
	    !vgic_delivery_state_get_listed(&current_dstate) &&
	    vgic_delivery_state_is_pending(&current_dstate)) {
		// The IRQ can be delivered, but hasn't been yet. Choose
		// a route for it, checking the current VCPU first for 1-of-N.
		if (!vgic_try_route_and_flag(vic, virq, current_dstate, true)) {
			unclaimed = true;
		}
	}

	preempt_enable();

	return unclaimed;
}

void
vgic_retry_unrouted(vic_t *vic)
{
	spinlock_acquire(&vic->search_lock);

	BITMAP_ATOMIC_FOREACH_SET_BEGIN(range, vic->search_ranges_low,
					VGIC_LOW_RANGES)
		if (compiler_unexpected(!bitmap_atomic_test_and_clear(
			    vic->search_ranges_low, range,
			    memory_order_acquire))) {
			continue;
		}

		VGIC_DEBUG_TRACE(ROUTE, vic, NULL, "unrouted: check range {:d}",
				 range);

		bool unclaimed = false;
		for (index_t i = 0; i < vgic_low_range_size(range); i++) {
			virq_t virq =
				(virq_t)((range * VGIC_LOW_RANGE_SIZE) + i);
			if (vgic_irq_is_spi(virq) &&
			    vgic_retry_unrouted_virq(vic, virq)) {
				unclaimed = true;
			}
		}

		if (unclaimed) {
			// We didn't succeed in routing all of the IRQs in
			// this range, so reset the range's search bit.
			bitmap_atomic_set(vic->search_ranges_low, range,
					  memory_order_acquire);
		}
	BITMAP_ATOMIC_FOREACH_SET_END

	spinlock_release(&vic->search_lock);
}

#if VGIC_HAS_1N
static bool
vgic_check_unrouted_virq(vic_t *vic, thread_t *vcpu, virq_t virq)
{
	assert(vic != NULL);
	// Only SPIs can be unrouted
	assert(vgic_irq_is_spi(virq));

	_Atomic vgic_delivery_state_t *dstate =
		vgic_find_dstate(vic, NULL, virq);
	assert(dstate != NULL);
	vgic_delivery_state_t current_dstate = atomic_load_relaxed(dstate);

	return vgic_delivery_state_get_enabled(&current_dstate) &&
	       !vgic_delivery_state_get_listed(&current_dstate) &&
	       vgic_delivery_state_is_pending(&current_dstate) &&
	       ((platform_irq_cpu_class((cpu_index_t)vcpu->vgic_gicr_index) ==
		 0U)
			? vgic_get_delivery_state_is_class0(&current_dstate)
			: vgic_get_delivery_state_is_class1(&current_dstate));
}

static bool
vgic_check_unrouted(vic_t *vic, thread_t *vcpu)
{
	bool wakeup_found = false;

	BITMAP_ATOMIC_FOREACH_SET_BEGIN(range, vic->search_ranges_low,
					VGIC_LOW_RANGES)
		VGIC_DEBUG_TRACE(ROUTE, vic, NULL, "unrouted: check range {:d}",
				 range);

		for (index_t i = 0; i < vgic_low_range_size(range); i++) {
			virq_t virq =
				(virq_t)((range * VGIC_LOW_RANGE_SIZE) + i);
			if (vgic_irq_is_spi(virq) &&
			    vgic_check_unrouted_virq(vic, vcpu, virq)) {
				wakeup_found = true;
				break;
			}
		}
	BITMAP_ATOMIC_FOREACH_SET_END

	return wakeup_found;
}
#endif

// This function is called when permanently tearing down a VCPU.
//
// It clears out the list registers, disregarding the priority order of active
// LRs (rather than reclaiming the lowest active priority first as usual). It
// also reroutes all pending inactive IRQs that are flagged in the VCPU's search
// bitmaps, including directly routed IRQs.
//
// The specified thread must not be running on any CPU.
void
vgic_undeliver_all(vic_t *vic, thread_t *vcpu)
{
	cpu_index_t lr_owner = vgic_lr_owner_lock(vcpu);
	assert(!cpulocal_index_valid(lr_owner));

	vcpu->vgic_group0_enabled = false;
	vcpu->vgic_group1_enabled = false;

	for (index_t i = 0U; i < CPU_GICH_LR_COUNT; i++) {
		if (vcpu->vgic_lrs[i].dstate != NULL) {
			vgic_reclaim_lr(vic, vcpu, i, true);
		}
	}

	BITMAP_ATOMIC_FOREACH_SET_BEGIN(prio, vcpu->vgic_search_prios,
					VGIC_PRIORITIES)
		BITMAP_ATOMIC_FOREACH_SET_BEGIN(
			range, vcpu->vgic_search_ranges_low[prio],
			VGIC_LOW_RANGES)
			for (index_t i = 0; i < vgic_low_range_size(range);
			     i++) {
				virq_t virq =
					(virq_t)((range * VGIC_LOW_RANGE_SIZE) +
						 i);
				if (!vgic_irq_is_spi(virq)) {
					// The IRQ can't be rerouted.
					continue;
				}

				_Atomic vgic_delivery_state_t *dstate =
					vgic_find_dstate(vic, vcpu, virq);
				assert(dstate != NULL);
				vgic_delivery_state_t current_dstate =
					atomic_load_relaxed(dstate);

				if (vgic_delivery_state_get_enabled(
					    &current_dstate) &&
				    !vgic_delivery_state_get_listed(
					    &current_dstate) &&
				    vgic_delivery_state_is_pending(
					    &current_dstate)) {
					vgic_route_and_flag(vic, virq,
							    current_dstate,
							    false);
				}
			}
		BITMAP_ATOMIC_FOREACH_SET_END
	BITMAP_ATOMIC_FOREACH_SET_END

	vgic_lr_owner_unlock(vcpu);
}

#if VGIC_HAS_1N
static bool
vgic_do_reroute(vic_t *vic, thread_t *vcpu, index_t prio_index)
	REQUIRE_PREEMPT_DISABLED
{
	bool reset_prio = false;

	_Atomic BITMAP_DECLARE_PTR(VGIC_LOW_RANGES, ranges) =
		&vcpu->vgic_search_ranges_low[prio_index];
	BITMAP_ATOMIC_FOREACH_SET_BEGIN(range, *ranges, VGIC_LOW_RANGES)
		if (compiler_unexpected(!bitmap_atomic_test_and_clear(
			    *ranges, range, memory_order_acquire))) {
			continue;
		}
		bool reset_range = false;
		for (index_t i = 0; i < vgic_low_range_size(range); i++) {
			virq_t virq =
				(virq_t)((range * VGIC_LOW_RANGE_SIZE) + i);
			if (!vgic_irq_is_spi(virq)) {
				// IRQ can't be rerouted; reset the
				// pending flag
				reset_range = true;
				continue;
			}

			_Atomic vgic_delivery_state_t *dstate =
				vgic_find_dstate(vic, NULL, virq);
			assert(dstate != NULL);
			vgic_delivery_state_t current_dstate =
				atomic_load_relaxed(dstate);

			if (!vgic_delivery_state_get_enabled(&current_dstate) ||
			    vgic_delivery_state_get_listed(&current_dstate) ||
			    !vgic_delivery_state_is_pending(&current_dstate)) {
				// Not pending
			} else if (vgic_delivery_state_get_route_1n(
					   &current_dstate)) {
				// 1-of-N; reroute it
				VGIC_DEBUG_TRACE(ROUTE, vic, NULL,
						 "reroute-all: {:d}", virq);
				vgic_route_and_flag(vic, virq, current_dstate,
						    false);
			} else {
				// Direct; reset the pending flag
				reset_range = true;
			}
		}
		if (reset_range) {
			bitmap_atomic_set(*ranges, range, memory_order_relaxed);
			reset_prio = true;
		}
	BITMAP_ATOMIC_FOREACH_SET_END

	return reset_prio;
}
#endif

// This function is called after disabling one or both VIRQ groups.
//
// It removes the pending state from all LRs, and reroutes any pending inactive
// VIRQs that were in the LRs. It also reroutes all pending inactive 1-of-N IRQs
// that are flagged in the VCPU's search bitmaps.
//
// This is distinct from vgic_undeliver_all() in three ways: active LRs remain
// active; direct IRQs aren't rerouted; and the search bitmap is updated
// (because not doing so might prevent a subsequent sleep).
//
// If the specified VCPU is not current, its LR lock must be held, and it must
// not be running remotely.
static void
vgic_reroute_all(vic_t *vic, thread_t *vcpu)
	REQUIRE_LOCK(vcpu->vgic_lr_owner_lock) REQUIRE_PREEMPT_DISABLED
{
#if VGIC_HAS_1N
	BITMAP_ATOMIC_FOREACH_SET_BEGIN(prio_index, vcpu->vgic_search_prios,
					VGIC_PRIORITIES)

		if (compiler_unexpected(!bitmap_atomic_test_and_clear(
			    vcpu->vgic_search_prios, prio_index,
			    memory_order_acquire))) {
			continue;
		}

		bool reset_prio = vgic_do_reroute(vic, vcpu, prio_index);
		if (reset_prio) {
			bitmap_atomic_set(vcpu->vgic_search_prios, prio_index,
					  memory_order_relaxed);
		}
	BITMAP_ATOMIC_FOREACH_SET_END
#endif

	bool from_self = (thread_get_self() == vcpu);
	for (index_t i = 0U; i < CPU_GICH_LR_COUNT; i++) {
		if (vcpu->vgic_lrs[i].dstate != NULL) {
			if (from_self) {
				vgic_read_lr_state(i);
			}
			(void)vgic_sync_lr(vic, vcpu, &vcpu->vgic_lrs[i],
					   vgic_delivery_state_default(),
					   false);
			if (from_self) {
				vgic_write_lr(i);
			}
		}
	}
}

// Check for changes to the group enable bits, and update LRs as necessary.
//
// If the specified VCPU is not current, its LR lock must be held, and it must
// not be running remotely. The GICD_CTLR value should be read from the GICD
// before acquiring the LR lock; any subsequent change to the GICD_CTLR by
// another CPU must trigger another call to this function, typically by sending
// an IPI.
static bool
vgic_gicr_update_group_enables(vic_t *vic, thread_t *gicr_vcpu,
			       GICD_CTLR_DS_t gicd_ctlr)
{
	bool hw_access = thread_get_self() == gicr_vcpu;
	bool wakeup    = false;

	assert_preempt_disabled();

	bool group0_was_enabled = gicr_vcpu->vgic_group0_enabled;
	bool group1_was_enabled = gicr_vcpu->vgic_group1_enabled;

	if (hw_access) {
		// Read ICH_VMCR_EL2 to check the current group enables
		gicr_vcpu->vgic_ich_vmcr =
			register_ICH_VMCR_EL2_read_ordered(&asm_ordering);
	}

	bool group0_enable = GICD_CTLR_DS_get_EnableGrp0(&gicd_ctlr) &&
			     ICH_VMCR_EL2_get_VENG0(&gicr_vcpu->vgic_ich_vmcr);
	bool group1_enable = GICD_CTLR_DS_get_EnableGrp1(&gicd_ctlr) &&
			     ICH_VMCR_EL2_get_VENG1(&gicr_vcpu->vgic_ich_vmcr);

	// Update the group enables. Note that we do this before we clear
	// out the LRs, to ensure that any 1-of-N IRQs that are no longer
	// deliverable will be flagged on another CPU, or as unrouted.
	gicr_vcpu->vgic_group0_enabled = group0_enable;
	gicr_vcpu->vgic_group1_enabled = group1_enable;

	// If either group is newly disabled, reroute everything. Only active
	// IRQs will be left in the LRs. Pending 1-of-N IRQs will be flagged on
	// another CPU if possible, or as unrouted otherwise.
	if ((!group0_enable && group0_was_enabled) ||
	    (!group1_enable && group1_was_enabled)) {
		vgic_reroute_all(vic, gicr_vcpu);
	}

	if (hw_access) {
		// Read ICH_HCR_EL2 so we can safely update the trap enables
		// and call vgic_do_delivery_check()
		gicr_vcpu->vgic_ich_hcr = register_ICH_HCR_EL2_read();
	}

#if VGIC_HAS_LPI && GICV3_HAS_VLPI_V4_1
	// The vSGIEOICount flag is set for every VCPU based on the nASSGIreq
	// flag in GICD_CTLR, which the VM can only update while the groups
	// are disabled in GICD_CTLR. Updating it unconditionally here is
	// probably faster than checking whether we need to update it.
	ICH_HCR_EL2_set_vSGIEOICount(&gicr_vcpu->vgic_ich_hcr,
				     vic->vsgis_enabled);
#endif

	// Update the group enable / disable traps. This isn't needed if we have
	// ARMv8.6-FGT, because we can unconditionally trap all ICC_IGRPENn_EL1
	// writes in that case.
	if (!vgic_fgt_allowed()) {
		ICH_HCR_EL2_set_TALL0(&gicr_vcpu->vgic_ich_hcr, !group0_enable);
		ICH_HCR_EL2_set_TALL1(&gicr_vcpu->vgic_ich_hcr, !group1_enable);
		ICH_HCR_EL2_set_VGrp0DIE(&gicr_vcpu->vgic_ich_hcr,
					 group0_enable);
		ICH_HCR_EL2_set_VGrp1DIE(&gicr_vcpu->vgic_ich_hcr,
					 group1_enable);
	}

	// Now search for and list all deliverable VIRQs.
	if (group0_enable || group1_enable) {
#if VGIC_HAS_1N
		// If either group is newly enabled, check for unrouted 1-of-N
		// VIRQs, and flag them on this CPU if possible.
		if ((group0_enable && !group0_was_enabled) ||
		    (group1_enable && !group1_was_enabled)) {
			vgic_retry_unrouted(vic);
		}
#endif

		wakeup = vgic_do_delivery_check(vic, gicr_vcpu);
	}

	if (hw_access) {
		// Update the trap enables (including NPIE which may be set by
		// the call to vgic_do_delivery_check())
		register_ICH_HCR_EL2_write(gicr_vcpu->vgic_ich_hcr);
	}

	return wakeup;
}

static void
vgic_deliver_pending_sgi(vic_t *vic, thread_t *vcpu)
{
	index_t i;

	while (bitmap_atomic_ffs(vcpu->vgic_pending_sgis, GIC_SGI_NUM, &i)) {
		virq_t virq = (virq_t)i;
		bitmap_atomic_clear(vcpu->vgic_pending_sgis, i,
				    memory_order_relaxed);

		_Atomic vgic_delivery_state_t *dstate =
			&vcpu->vgic_private_states[virq];
		vgic_delivery_state_t assert_dstate =
			vgic_delivery_state_default();
		vgic_delivery_state_set_edge(&assert_dstate, true);

		(void)vgic_deliver(virq, vic, vcpu, NULL, dstate, assert_dstate,
				   true);
	}
}

void
vgic_handle_thread_context_switch_post(thread_t *prev)
{
	vic_t *vic = prev->vgic_vic;

	if (vic != NULL) {
		bool wakeup_prev = false;

		cpu_index_t lr_owner = vgic_lr_owner_lock(prev);
		assert(lr_owner == cpulocal_get_index());
		if (ipi_clear(IPI_REASON_VGIC_SYNC)) {
			if (vgic_sync_vcpu(prev, false)) {
				wakeup_prev = true;
			}
		}

		if (ipi_clear(IPI_REASON_VGIC_ENABLE)) {
			if (vgic_gicr_update_group_enables(
				    vic, prev,
				    atomic_load_acquire(&vic->gicd_ctlr))) {
				wakeup_prev = true;
			}
		}
		atomic_store_relaxed(&prev->vgic_lr_owner_lock.owner,
				     CPU_INDEX_INVALID);

		// Any deliver or SGI IPIs are no longer relevant; discard them.
		(void)ipi_clear(IPI_REASON_VGIC_DELIVER);
		(void)ipi_clear(IPI_REASON_VGIC_SGI);

		if (vcpu_expects_wakeup(prev)) {
			// The prev thread could be woken by a pending IRQ;
			// check for any that are waiting to be delivered.
			//
			// Match the seq_cst fences in vgic_flag_unlocked and
			// vgic_icc_generate_sgi. This ensures that those
			// routines either update the pending states before the
			// fence so we will see them below, or else see the
			// invalid owner after the fence and send a wakeup
			// causing prev to be rescheduled.
			atomic_thread_fence(memory_order_seq_cst);

			wakeup_prev = vgic_do_delivery_check(vic, prev) ||
				      wakeup_prev;

			vgic_lr_owner_unlock(prev);

			if (wakeup_prev) {
				scheduler_lock(prev);
				vcpu_wakeup(prev);
				scheduler_unlock(prev);
			}
			vgic_deliver_pending_sgi(vic, prev);
		} else {
			vgic_lr_owner_unlock(prev);
		}
	}
}

void
vgic_handle_thread_load_state(void) LOCK_IMPL
{
	thread_t *vcpu = thread_get_self();
	assert(vcpu != NULL);
	vic_t *vic = vcpu->vgic_vic;

	if (vic != NULL) {
		spinlock_acquire(&vcpu->vgic_lr_owner_lock.lock);
		atomic_store_relaxed(&vcpu->vgic_lr_owner_lock.owner,
				     cpulocal_get_index());

		// Match the seq_cst fences in vgic_flag_unlocked and
		// vgic_icc_generate_sgi. This ensures that those routines
		// either see us as the new owner and send an IPI after
		// the fence, so we will see and handle it after the context
		// switch ends, or else write the pending IRQ state before
		// the fence, so it is seen by our checks below.
		atomic_thread_fence(memory_order_seq_cst);

		for (index_t i = 0; i < CPU_GICH_LR_COUNT; i++) {
			vgic_write_lr(i);
		}

		(void)vgic_do_delivery_check(vic, vcpu);

		gicv3_write_ich_aprs(vcpu->vgic_ap0rs, vcpu->vgic_ap1rs);
		register_ICH_VMCR_EL2_write(vcpu->vgic_ich_vmcr);
		register_ICH_HCR_EL2_write(vcpu->vgic_ich_hcr);

		spinlock_release(&vcpu->vgic_lr_owner_lock.lock);
		vgic_deliver_pending_sgi(vic, vcpu);
	} else {
		register_ICH_HCR_EL2_write(ICH_HCR_EL2_default());
	}
}

void
vgic_gicr_rd_set_sleep(vic_t *vic, thread_t *gicr_vcpu, bool sleep)
{
#if VGIC_HAS_1N
	if (sleep) {
		// Update the sleep state, but only if we were awake; don't wipe
		// out a wakeup if this is a redundant write of the sleep bit.
		vgic_sleep_state_t old_sleep_state = VGIC_SLEEP_STATE_AWAKE;
		if (atomic_compare_exchange_strong_explicit(
			    &gicr_vcpu->vgic_sleep, &old_sleep_state,
			    VGIC_SLEEP_STATE_ASLEEP, memory_order_relaxed,
			    memory_order_relaxed)) {
			// We successfully entered sleep and there was no
			// existing wakeup. We now need to check whether any
			// IRQs had been marked unrouted prior to us entering
			// sleep. We need a seq_cst fence to order the check
			// after entering sleep, matching the seq_cst fence in
			// vgic_wakeup_1n().
			atomic_thread_fence(memory_order_seq_cst);
			if (vgic_check_unrouted(vic, gicr_vcpu)) {
				old_sleep_state = VGIC_SLEEP_STATE_ASLEEP;
				(void)atomic_compare_exchange_strong_explicit(
					&gicr_vcpu->vgic_sleep,
					&old_sleep_state,
					VGIC_SLEEP_STATE_WAKEUP_1N,
					memory_order_relaxed,
					memory_order_relaxed);
			}
		}
	} else {
		// We're waking up; if there's a wakeup it can be
		// discarded.
		atomic_store_relaxed(&gicr_vcpu->vgic_sleep,
				     VGIC_SLEEP_STATE_AWAKE);
	}
#else
	(void)vic;
	atomic_store_relaxed(&gicr_vcpu->vgic_sleep,
			     sleep ? VGIC_SLEEP_STATE_ASLEEP
				   : VGIC_SLEEP_STATE_AWAKE);
#endif
}

bool
vgic_gicr_rd_check_sleep(thread_t *gicr_vcpu)
{
	bool is_asleep;

	if (atomic_load_relaxed(&gicr_vcpu->vgic_sleep) !=
	    VGIC_SLEEP_STATE_AWAKE) {
		if (!vgic_fgt_allowed()) {
			cpu_index_t lr_owner = vgic_lr_owner_lock(gicr_vcpu);
			// We might not have received the maintenance interrupt
			// yet after the VM cleared the group enable bits.
			// Synchronise the group enables before checking them.
			if (lr_owner == CPU_INDEX_INVALID) {
				(void)vgic_gicr_update_group_enables(
					gicr_vcpu->vgic_vic, gicr_vcpu,
					atomic_load_acquire(
						&gicr_vcpu->vgic_vic
							 ->gicd_ctlr));
			}
			vgic_lr_owner_unlock(gicr_vcpu);
		}
		// We can only sleep if the groups are disabled.
		is_asleep = !gicr_vcpu->vgic_group0_enabled &&
			    !gicr_vcpu->vgic_group1_enabled;
	} else {
		is_asleep = false;
#if VGIC_HAS_LPI && GICV3_HAS_VLPI_V4_1
		if (gicv3_vpe_check_wakeup(false)) {
			// The GICR hasn't finished scheduling the vPE yet.
			// Returning true here means that the GICR_WAKER poll
			// on VCPU resume will effectively prevent the VCPU
			// entering its idle loop (and maybe suspending again)
			// until the GICR has had an opportunity to forward any
			// pending SGIs and LPIs.
			is_asleep = true;
		}
#endif
	}

	return is_asleep;
}

bool
vgic_handle_vcpu_pending_wakeup(void)
{
	thread_t *vcpu = thread_get_self();
	assert(vcpu != NULL);

	bool pending =
		!bitmap_atomic_empty(vcpu->vgic_search_prios, VGIC_PRIORITIES);

#if VGIC_HAS_1N
	if (!pending && (atomic_load_relaxed(&vcpu->vgic_sleep) ==
			 VGIC_SLEEP_STATE_WAKEUP_1N)) {
		pending = true;
	}
#endif

	if (!pending &&
	    (vcpu->vgic_group0_enabled || vcpu->vgic_group1_enabled)) {
		// There might be interrupts left in the LRs. This could happen
		// at a preemption point in a long-running service call, or
		// during a suspend call into a retention state.
		for (index_t i = 0U; !pending && (i < CPU_GICH_LR_COUNT); i++) {
			vgic_read_lr_state(i);
			ICH_LR_EL2_State_t state = ICH_LR_EL2_base_get_State(
				&vcpu->vgic_lrs[i].lr.base);
			// Note: not checking for PENDING_ACTIVE here, because
			// that is not deliverable and can't wake the VCPU.
			if (state == ICH_LR_EL2_STATE_PENDING) {
				pending = true;
			}
		}
	}

	return pending;
}

void
vgic_handle_vcpu_stopped(void)
{
	thread_t *vcpu = thread_get_self();

	if (vcpu->vgic_vic != NULL) {
		// Disable interrupt delivery and reroute any pending IRQs. The
		// VCPU really should have done this itself, but PSCI_CPU_OFF
		// is not able to fail if it hasn't, so we just go ahead and do
		// it ourselves.
		if (vcpu->vgic_group0_enabled || vcpu->vgic_group1_enabled) {
			cpu_index_t remote_cpu = vgic_lr_owner_lock(vcpu);
			assert(remote_cpu == CPU_INDEX_INVALID);

			register_ICH_VMCR_EL2_write_ordered(
				ICH_VMCR_EL2_default(), &asm_ordering);

			(void)vgic_gicr_update_group_enables(
				vcpu->vgic_vic, vcpu, GICD_CTLR_DS_default());

			vgic_lr_owner_unlock(vcpu);
		}
	}
}

vcpu_trap_result_t
vgic_handle_vcpu_trap_wfi(void)
{
	thread_t *vcpu = thread_get_self();
	assert(vcpu != NULL);

	if (vcpu->vgic_vic != NULL) {
		(void)vgic_lr_owner_lock(vcpu);

#if VGIC_HAS_1N
		// Eagerly release invalid LRs. This increases the likelihood
		// that a 1-of-N IRQ that is next delivered to some remote CPU
		// can be locally asserted on that remote CPU.
		register_t elrsr =
			register_ICH_ELRSR_EL2_read_ordered(&gich_lr_ordering);
		while (elrsr != 0U) {
			index_t lr = compiler_ctz(elrsr);
			elrsr &= ~util_bit(lr);

			assert_debug(lr < CPU_GICH_LR_COUNT);

			vgic_lr_status_t *status = &vcpu->vgic_lrs[lr];
			if (status->dstate != NULL) {
				vgic_reclaim_lr(vcpu->vgic_vic, vcpu, lr,
						false);
				// No need to rewrite the LR because we
				// know that it is already invalid
			}
		}
#endif

		// It is possible that a maintenance interrupt is currently
		// pending but was not delivered before the WFI trap. If so,
		// handling it might make more IRQs deliverable, in which case
		// the WFI should not be allowed to sleep.
		//
		// The simplest way to deal with this possibility is to run the
		// maintenance handler directly.
		(void)vgic_handle_irq_received_maintenance();

		vgic_lr_owner_unlock(vcpu);
	}

	// Continue to the default handler
	return VCPU_TRAP_RESULT_UNHANDLED;
}

bool
vgic_handle_ipi_received_enable(void)
{
	thread_t *current = thread_get_self();
	assert(current->vgic_vic != NULL);
	(void)vgic_lr_owner_lock_nopreempt(current);
	bool wakeup = vgic_gicr_update_group_enables(
		current->vgic_vic, current,
		atomic_load_acquire(&current->vgic_vic->gicd_ctlr));
	vgic_lr_owner_unlock_nopreempt(current);
	return wakeup;
}

bool
vgic_handle_ipi_received_sync(void)
{
	thread_t *current = thread_get_self();
	(void)vgic_lr_owner_lock_nopreempt(current);
	bool wakeup = vgic_sync_vcpu(current, true);
	vgic_lr_owner_unlock_nopreempt(current);
	return wakeup;
}

bool
vgic_handle_ipi_received_deliver(void)
{
	thread_t *current = thread_get_self();
	assert(current != NULL);
	vic_t *vic = current->vgic_vic;

	if (vic != NULL) {
		(void)vgic_lr_owner_lock_nopreempt(current);
		current->vgic_ich_hcr = register_ICH_HCR_EL2_read();

		for (index_t i = 0; i < CPU_GICH_LR_COUNT; i++) {
			vgic_lr_status_t *status = &current->vgic_lrs[i];
			if (status->dstate == NULL) {
				continue;
			}
			vgic_read_lr_state(i);
		}

		if (vgic_do_delivery_check(vic, current)) {
			vcpu_wakeup_self();
		}

		register_ICH_HCR_EL2_write(current->vgic_ich_hcr);
		vgic_lr_owner_unlock_nopreempt(current);
	}

	return false;
}

bool
vgic_handle_ipi_received_sgi(void)
{
	thread_t *current = thread_get_self();
	assert(current != NULL);
	vic_t *vic = current->vgic_vic;

	VGIC_TRACE(SGI, vic, current, "sgi ipi: pending {:#x}",
		   atomic_load_relaxed(current->vgic_pending_sgis));

	if (vic != NULL) {
		vgic_deliver_pending_sgi(vic, current);
	}

	return false;
}

// GICC
void
vgic_icc_set_group_enable(bool is_group_1, ICC_IGRPEN_EL1_t igrpen)
{
	thread_t *current = thread_get_self();
	assert(current != NULL);
	vic_t *vic = current->vgic_vic;
	assert(vic != NULL);

	cpu_index_t remote_cpu = vgic_lr_owner_lock(current);
	assert(remote_cpu == CPU_INDEX_INVALID);

	current->vgic_ich_vmcr = register_ICH_VMCR_EL2_read();
	bool enabled	       = ICC_IGRPEN_EL1_get_Enable(&igrpen);
	VGIC_TRACE(ICC_WRITE, vic, current, "group {:d} {:s}",
		   (register_t)is_group_1,
		   (uintptr_t)(enabled ? "enabled" : "disabled"));
	if (is_group_1) {
		ICH_VMCR_EL2_set_VENG1(&current->vgic_ich_vmcr, enabled);
	} else {
		ICH_VMCR_EL2_set_VENG0(&current->vgic_ich_vmcr, enabled);
	}
	register_ICH_VMCR_EL2_write_ordered(current->vgic_ich_vmcr,
					    &asm_ordering);

	GICD_CTLR_DS_t gicd_ctlr = atomic_load_acquire(&vic->gicd_ctlr);
	if (vgic_gicr_update_group_enables(vic, current, gicd_ctlr)) {
		vcpu_wakeup_self();
	}

	vgic_lr_owner_unlock(current);
}

void
vgic_icc_irq_deactivate(vic_t *vic, irq_t irq_num)
{
	thread_t		      *vcpu = thread_get_self();
	_Atomic vgic_delivery_state_t *dstate =
		vgic_find_dstate(vic, vcpu, irq_num);
	assert(dstate != NULL);

	// Don't let context switches delist the VIRQ out from under us
	preempt_disable();

	// Call generic deactivation handling if not currently listed
	vgic_delivery_state_t old_dstate = atomic_load_relaxed(dstate);
	if (!vgic_delivery_state_get_listed(&old_dstate)) {
		vgic_deactivate(vic, thread_get_self(), irq_num, dstate,
				old_dstate, false, false);
		goto out;
	}

	// Search the current CPU's list registers for the VIRQ
	for (index_t lr = 0; lr < CPU_GICH_LR_COUNT; lr++) {
		vgic_lr_status_t *status = &vcpu->vgic_lrs[lr];
		if (status->dstate != dstate) {
			continue;
		}

		vgic_read_lr_state(lr);
		ICH_LR_EL2_State_t state =
			ICH_LR_EL2_base_get_State(&status->lr.base);

		if ((state == ICH_LR_EL2_STATE_PENDING) ||
		    (state == ICH_LR_EL2_STATE_INVALID)) {
			// Interrupt is not active; nothing to do.
			goto out;
		}

		// Determine whether the edge bit should be reset when
		// delisting.
		bool set_edge = state == ICH_LR_EL2_STATE_PENDING_ACTIVE;

		// Determine whether the hw_active bit should be reset when
		// delisting (or alternatively, the physical IRQ should be
		// manually deactivated).
		bool hw_active = ICH_LR_EL2_base_get_HW(&status->lr.base);

		// Kick the interrupt out of the LR. We could potentially keep
		// it listed if it is still pending, but that complicates the
		// code too much and we don't care about EOImode=1 VMs anyway.
		status->lr.base = ICH_LR_EL2_base_default();
		status->dstate	= NULL;
		vgic_write_lr(lr);

#if VGIC_HAS_1N
		if (vgic_delivery_state_get_route_1n(&old_dstate)) {
			virq_source_t *source =
				vgic_find_source(vic, vcpu, irq_num);
			vgic_spi_reset_route_1n(source, old_dstate);
		}
#endif

		vgic_deactivate(vic, thread_get_self(), irq_num, dstate,
				old_dstate, set_edge, hw_active);

		goto out;
	}

	// If we didn't find the LR, it's listed on another CPU.
	//
	// DIR is supposed to work across CPUs so we should flag the IRQ and
	// send an IPI to deactivate it. Possibly an extra dstate bit would
	// work for this. However, few VMs will use EOImode=1 so we don't care
	// very much just yet. For now, warn and do nothing.
	//
	// FIXME:
#if !defined(NDEBUG)
	static _Thread_local bool warned_about_ignored_dir = false;
	if (!warned_about_ignored_dir) {
		TRACE_AND_LOG(VGIC, WARN,
			      "vcpu {:#x}: trapped ICC_DIR_EL1 write "
			      "was cross-CPU; vIRQ {:d} may be stuck active",
			      (uintptr_t)(thread_t *)thread_get_self(),
			      irq_num);
		warned_about_ignored_dir = true;
	}
#endif

out:
	preempt_enable();
}

static void
vgic_send_sgi(vic_t *vic, thread_t *vcpu, virq_t virq, bool is_group_1)
	REQUIRE_RCU_READ
{
	_Atomic vgic_delivery_state_t *dstate =
		&vcpu->vgic_private_states[virq];
	vgic_delivery_state_t old_dstate = atomic_load_relaxed(dstate);

	if (!is_group_1 && vgic_delivery_state_get_group1(&old_dstate)) {
		// SGI0R & ASGI1R do not generate group 1 SGIs
		goto out;
	}

#if GICV3_HAS_VLPI_V4_1 && VGIC_HAS_LPI
	// Raise SGI using direct injection through the ITS if possible.
	//
	// We can only use direct injection if:
	// - The SGI is not listed in an LR (which has unpredictable behaviour
	//   when combined with direct injection of the same SGI)
	// - The VM has permitted vSGI delivery with no active state, by setting
	//   GICD_CTLR.nASSGIreq (cached in vic->vsgis_enabled)
	// - The VCPU has enabled vLPIs, and the ITS commands to sync the SGI
	//   configuration into the LPI tables have completed
	if (!vgic_delivery_state_get_listed(&old_dstate) &&
	    vic->vsgis_enabled && (vgic_vsgi_assert(vcpu, virq) == OK)) {
		goto out;
	}
#endif

	if ((vcpu != thread_get_self()) &&
	    vgic_delivery_state_get_enabled(&old_dstate)) {
		VGIC_TRACE(SGI, vic, vcpu, "sgi fast: {:d}", virq);

		// Mark the SGI as pending delivery, and wake
		// the target VCPU for delivery.
		bitmap_atomic_set(vcpu->vgic_pending_sgis, virq,
				  memory_order_relaxed);

		// Match the seq_cst fences when the owner is changed
		// during the context switch.
		atomic_thread_fence(memory_order_seq_cst);

		cpu_index_t lr_owner =
			atomic_load_relaxed(&vcpu->vgic_lr_owner_lock.owner);

		if (cpulocal_index_valid(lr_owner)) {
			ipi_one(IPI_REASON_VGIC_SGI, lr_owner);
		} else {
			scheduler_lock(vcpu);
			vcpu_wakeup(vcpu);
			scheduler_unlock(vcpu);
		}
	} else {
		// Deliver the interrupt to the target
		vgic_delivery_state_t assert_dstate =
			vgic_delivery_state_default();
		vgic_delivery_state_set_edge(&assert_dstate, true);

		(void)vgic_deliver(virq, vic, vcpu, NULL, dstate, assert_dstate,
				   true);
	}

out:
	return;
}

void
vgic_icc_generate_sgi(vic_t *vic, ICC_SGIR_EL1_t sgir, bool is_group_1)
{
	register_t target_list	 = ICC_SGIR_EL1_get_TargetList(&sgir);
	index_t	   target_offset = 16U * (index_t)ICC_SGIR_EL1_get_RS(&sgir);
	virq_t	   virq		 = ICC_SGIR_EL1_get_INTID(&sgir);

	assert(virq < GIC_SGI_NUM);

	if (compiler_unexpected(ICC_SGIR_EL1_get_IRM(&sgir))) {
		thread_t *current = thread_get_self();
		for (index_t i = 0U; i < vic->gicr_count; i++) {
			rcu_read_start();
			thread_t *vcpu =
				atomic_load_consume(&vic->gicr_vcpus[i]);
			if ((vcpu != NULL) && (vcpu != current)) {
				vgic_send_sgi(vic, vcpu, virq, is_group_1);
			}
			rcu_read_finish();
		}
	} else {
		while (target_list != 0U) {
			index_t target_bit = compiler_ctz(target_list);
			target_list &= ~util_bit(target_bit);

			index_result_t cpu_r = vgic_get_index_for_mpidr(
				vic, (uint8_t)(target_bit + target_offset),
				ICC_SGIR_EL1_get_Aff1(&sgir),
				ICC_SGIR_EL1_get_Aff2(&sgir),
				ICC_SGIR_EL1_get_Aff3(&sgir));
			if (cpu_r.e != OK) {
				// ignore invalid target
				continue;
			}
			assert(cpu_r.r < vic->gicr_count);

			rcu_read_start();
			thread_t *vcpu =
				atomic_load_consume(&vic->gicr_vcpus[cpu_r.r]);
			if (vcpu != NULL) {
				vgic_send_sgi(vic, vcpu, virq, is_group_1);
			}
			rcu_read_finish();
		}
	}
}

```

`hyp/vm/vgic/src/distrib.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>
#include <string.h>

#include <hypcall_def.h>
#include <hypconstants.h>
#include <hypcontainers.h>
#include <hypregisters.h>
#include <hyprights.h>

#include <atomic.h>
#include <bitmap.h>
#include <compiler.h>
#include <cpulocal.h>
#include <cspace.h>
#include <cspace_lookup.h>
#include <irq.h>
#include <log.h>
#include <object.h>
#include <panic.h>
#include <partition.h>
#include <partition_alloc.h>
#include <pgtable.h>
#include <platform_cpu.h>
#include <platform_irq.h>
#include <preempt.h>
#include <rcu.h>
#include <scheduler.h>
#include <spinlock.h>
#include <thread.h>
#include <trace.h>
#include <util.h>
#include <vdevice.h>
#include <vic.h>
#include <virq.h>

#include <events/vic.h>
#include <events/virq.h>

#include <asm/nospec_checks.h>

#if defined(ARCH_ARM_FEAT_FGT) && ARCH_ARM_FEAT_FGT
#include <arm_fgt.h>
#endif

#include "event_handlers.h"
#include "gicv3.h"
#include "internal.h"
#include "useraccess.h"
#include "vgic.h"
#include "vic_base.h"

error_t
vgic_handle_object_create_vic(vic_create_t vic_create)
{
	vic_t *vic = vic_create.vic;
	assert(vic != NULL);
	partition_t *partition = vic->header.partition;
	assert(partition != NULL);

	vic->gicr_count	   = 1U;
	vic->sources_count = 0U;

	spinlock_init(&vic->gicd_lock);
	spinlock_init(&vic->search_lock);

	// Use the DS (disable security) version of GICD_CTLR, because we don't
	// implement security states in the virtual GIC. Note that the DS bit is
	// constant true in this bitfield type.
	GICD_CTLR_DS_t ctlr = GICD_CTLR_DS_default();
	// The virtual GIC has no legacy mode support.
	GICD_CTLR_DS_set_ARE(&ctlr, true);
#if VGIC_HAS_1N
	// We currently don't implement E1NWF=0.
	// FIXME:
	GICD_CTLR_DS_set_E1NWF(&ctlr, true);
#endif
	atomic_init(&vic->gicd_ctlr, ctlr);

	// If not configured otherwise, default to using the same MPIDR mapping
	// as the hardware
	vic->mpidr_mapping = platform_cpu_get_mpidr_mapping();

	return OK;
}

error_t
vic_configure(vic_t *vic, count_t max_vcpus, count_t max_virqs,
	      count_t max_msis, bool allow_fixed_vmaddr)
{
	error_t err = OK;

	vic->allow_fixed_vmaddr = allow_fixed_vmaddr;

	if ((max_vcpus == 0U) || (max_vcpus > PLATFORM_MAX_CORES)) {
		err = ERROR_ARGUMENT_INVALID;
		goto out;
	}
	vic->gicr_count = max_vcpus;

	if (max_virqs > GIC_SPI_NUM) {
		err = ERROR_ARGUMENT_INVALID;
		goto out;
	}
	vic->sources_count = max_virqs;

#if VGIC_HAS_LPI
	if ((max_msis + GIC_LPI_BASE) >= util_bit(VGIC_IDBITS)) {
		err = ERROR_ARGUMENT_INVALID;
		goto out;
	}
	vic->gicd_idbits = compiler_msb(max_msis + GIC_LPI_BASE - 1U) + 1U;
#else
	if (max_msis != 0U) {
		err = ERROR_ARGUMENT_INVALID;
		goto out;
	}
#endif

out:
	return err;
}

bool
vgic_has_lpis(vic_t *vic)
{
#if VGIC_HAS_LPI
	return vic->gicd_idbits >= 14U;
#else
	(void)vic;
	return false;
#endif
}

error_t
vgic_handle_object_activate_vic(vic_t *vic)
{
	partition_t *partition = vic->header.partition;
	assert(partition != NULL);
	error_t		  err = OK;
	void_ptr_result_t alloc_r;

	assert(vic->sources_count <= GIC_SPI_NUM);
	size_t sources_size = sizeof(vic->sources[0U]) * vic->sources_count;

	assert(vic->gicr_count > 0U);
	assert(vic->gicr_count <= PLATFORM_MAX_CORES);
	size_t vcpus_size = sizeof(vic->gicr_vcpus[0U]) * vic->gicr_count;

#if VGIC_HAS_LPI
	if (vgic_has_lpis(vic)) {
		size_t vlpi_propbase_size =
			util_bit(vic->gicd_idbits) - GIC_LPI_BASE;
		size_t vlpi_propbase_align =
			util_bit(GIC_ITS_CMD_VMAPP_VCONF_ADDR_PRESHIFT);
		alloc_r = partition_alloc(vic->header.partition,
					  vlpi_propbase_size,
					  vlpi_propbase_align);
		if (alloc_r.e != OK) {
			err = alloc_r.e;
			goto out;
		}
		// No need for a memset here; the first time a VM enables LPIs
		// we will memcpy the table from VM memory (and zero the rest
		// of the table if necessary) before sending a VMAPP command.
		// The vlpi_config_valid flag indicates that this has been done
		vic->vlpi_config_table = alloc_r.r;
	}
#endif

	if (sources_size != 0U) {
		alloc_r = partition_alloc(partition, sources_size,
					  alignof(vic->sources[0U]));
		if (alloc_r.e != OK) {
			err = alloc_r.e;
			goto out;
		}
		(void)memset_s(alloc_r.r, sources_size, 0, sources_size);
		vic->sources = (virq_source_t *_Atomic *)alloc_r.r;
	}

	alloc_r = partition_alloc(partition, vcpus_size,
				  alignof(vic->gicr_vcpus[0U]));
	if (alloc_r.e != OK) {
		err = alloc_r.e;
		goto out;
	}
	(void)memset_s(alloc_r.r, vcpus_size, 0, vcpus_size);

	vic->gicr_vcpus = (thread_t *_Atomic *)alloc_r.r;

out:
	// We can't free anything here; it will be done in cleanup

	return err;
}

error_t
vgic_handle_addrspace_attach_vdevice(addrspace_t *addrspace,
				     cap_id_t vdevice_object_cap, index_t index,
				     vmaddr_t vbase, size_t size,
				     addrspace_attach_vdevice_flags_t flags)
{
	error_t	  err;
	cspace_t *cspace = cspace_get_self();

	vic_ptr_result_t vic_r = cspace_lookup_vic(
		cspace, vdevice_object_cap, CAP_RIGHTS_VIC_ATTACH_VDEVICE);
	if (compiler_unexpected(vic_r.e) != OK) {
		err = vic_r.e;
		goto out;
	}

	index_result_t index_r =
		nospec_range_check(index, vic_r.r->gicr_count + 1U);
	if (index_r.e != OK) {
		err = ERROR_ARGUMENT_INVALID;
		goto out_ref;
	}

	spinlock_acquire(&vic_r.r->gicd_lock);

	if (index_r.r == 0U) {
		// Attaching the GICD registers.
		if (flags.raw != 0U) {
			err = ERROR_ARGUMENT_INVALID;
			goto out_locked;
		}

		if (vic_r.r->gicd_device.type != VDEVICE_TYPE_NONE) {
			err = ERROR_BUSY;
			goto out_locked;
		}
		vic_r.r->gicd_device.type = VDEVICE_TYPE_VGIC_GICD;

		err = vdevice_attach_vmaddr(&vic_r.r->gicd_device, addrspace,
					    vbase, size);
		if (err != OK) {
			vic_r.r->gicd_device.type = VDEVICE_TYPE_NONE;
		}
	} else {
		// Attaching GICR registers for a specific VCPU.
		if (!vgic_gicr_attach_flags_is_clean(flags.vgic_gicr)) {
			err = ERROR_ARGUMENT_INVALID;
			goto out_locked;
		}

		rcu_read_start();
		thread_t *gicr_vcpu = atomic_load_consume(
			&vic_r.r->gicr_vcpus[index_r.r - 1U]);

		if (gicr_vcpu == NULL) {
			err = ERROR_IDLE;
			goto out_gicr_rcu;
		}

		if (gicr_vcpu->vgic_gicr_device.type != VDEVICE_TYPE_NONE) {
			err = ERROR_BUSY;
			goto out_gicr_rcu;
		}

		if (vgic_gicr_attach_flags_get_last_valid(&flags.vgic_gicr)) {
			gicr_vcpu->vgic_gicr_device_last =
				vgic_gicr_attach_flags_get_last(
					&flags.vgic_gicr);
		} else {
			// Last flag is unspecified; set it by default if this
			// is the highest-numbered GICR, which matches the old
			// behaviour.
			gicr_vcpu->vgic_gicr_device_last =
				(index_r.r == vic_r.r->gicr_count);
		}

		gicr_vcpu->vgic_gicr_device.type = VDEVICE_TYPE_VGIC_GICR;
		err = vdevice_attach_vmaddr(&gicr_vcpu->vgic_gicr_device,
					    addrspace, vbase, size);
		if (err != OK) {
			gicr_vcpu->vgic_gicr_device.type = VDEVICE_TYPE_NONE;
		}

	out_gicr_rcu:
		rcu_read_finish();
	}

out_locked:
	spinlock_release(&vic_r.r->gicd_lock);
out_ref:
	object_put_vic(vic_r.r);
out:
	return err;
}

static bool
vic_do_unbind(virq_source_t *source, bool during_deactivate);

void
vgic_handle_object_deactivate_vic(vic_t *vic)
{
	// We shouldn't be here if there are any GICRs attached
	for (index_t i = 0; i < vic->gicr_count; i++) {
		assert(atomic_load_relaxed(&vic->gicr_vcpus[i]) == NULL);
	}

	rcu_read_start();
	for (index_t i = 0; i < vic->sources_count; i++) {
		virq_source_t *virq_source =
			atomic_load_consume(&vic->sources[i]);

		if (virq_source == NULL) {
			continue;
		}

		if (vic_do_unbind(virq_source, true)) {
			// During deactivate we know that the VCPUs have all
			// exited so there can't be any IRQs left listed (as
			// asserted above). It therefore is not necessary to
			// wait until the end of an RCU grace period to clear
			// vgic_is_bound, as we normally would; we can go ahead
			// and clear it here.
			atomic_store_release(&virq_source->vgic_is_bound,
					     false);
		}
	}
	rcu_read_finish();

	if (vic->gicd_device.type != VDEVICE_TYPE_NONE) {
		vdevice_detach_vmaddr(&vic->gicd_device);
	}
}

void
vgic_handle_object_cleanup_vic(vic_t *vic)
{
	partition_t *partition = vic->header.partition;

	if (vic->gicr_vcpus != NULL) {
		size_t vcpus_size =
			sizeof(vic->gicr_vcpus[0]) * vic->gicr_count;
		(void)partition_free(partition, vic->gicr_vcpus, vcpus_size);
		vic->gicr_vcpus = NULL;
	}

	if (vic->sources != NULL) {
		size_t sources_size =
			sizeof(vic->sources[0]) * vic->sources_count;
		(void)partition_free(partition, vic->sources, sources_size);
		vic->sources = NULL;
	}

#if VGIC_HAS_LPI
	if (vic->vlpi_config_table != NULL) {
		size_t vlpi_propbase_size =
			util_bit(vic->gicd_idbits) - GIC_LPI_BASE;
		(void)partition_free(vic->header.partition,
				     vic->vlpi_config_table,
				     vlpi_propbase_size);
		vic->vlpi_config_table = NULL;
	}
#endif
}

error_t
vic_attach_vcpu(vic_t *vic, thread_t *vcpu, index_t index)
{
	assert(atomic_load_relaxed(&vcpu->header.state) == OBJECT_STATE_INIT);
	assert(atomic_load_relaxed(&vic->header.state) == OBJECT_STATE_ACTIVE);

	error_t err;

	if (vcpu->kind != THREAD_KIND_VCPU) {
		err = ERROR_ARGUMENT_INVALID;
		goto out;
	}

	if (index >= vic->gicr_count) {
		err = ERROR_ARGUMENT_INVALID;
		goto out;
	}

	err = OK;

	if (vcpu->vgic_vic != NULL) {
		object_put_vic(vcpu->vgic_vic);
	}

	vcpu->vgic_vic	      = object_get_vic_additional(vic);
	vcpu->vgic_gicr_index = index;

out:
	return err;
}

error_t
vgic_handle_object_create_thread(thread_create_t thread_create)
{
	thread_t *vcpu = thread_create.thread;
	assert(vcpu != NULL);

	spinlock_init(&vcpu->vgic_lr_owner_lock.lock);
	atomic_store_relaxed(&vcpu->vgic_lr_owner_lock.owner,
			     CPU_INDEX_INVALID);

	if (vcpu->kind == THREAD_KIND_VCPU) {
#if VGIC_HAS_LPI
		GICR_CTLR_t ctlr = GICR_CTLR_default();
		GICR_CTLR_set_IR(&ctlr, true);
		atomic_store_relaxed(&vcpu->vgic_gicr_rd_ctlr, ctlr);
#endif

		// The sleep flag is initially clear. This has no real effect on
		// guests with GICR_WAKER awareness (like Linux), but allows
		// interrupt delivery to work correctly for guests that assume
		// they have a non-secure view of the GIC (like UEFI).
		atomic_init(&vcpu->vgic_sleep, VGIC_SLEEP_STATE_AWAKE);

		vcpu->vgic_ich_hcr = ICH_HCR_EL2_default();

		// Trap changes to the group enable bits.
#if defined(ARCH_ARM_FEAT_FGT) && ARCH_ARM_FEAT_FGT
		if (arm_fgt_is_allowed()) {
			// Use fine-grained traps of the enable registers if
			// they are available, so we don't have to emulate the
			// other registers trapped by TALL[01].
			HFGWTR_EL2_set_ICC_IGRPENn_EL1(
				&vcpu->vcpu_regs_el2.hfgwtr_el2, true);
		} else
#endif
		{
			// Trap all accesses for disabled groups. Note that
			// these traps and the group disable maintenance IRQs
			// are toggled every time we update the group enables.
			//
			// We can't use the group enable maintenance IRQs,
			// because their latency is high enough that a VCPU's
			// idle loop might enable the groups and then disable
			// them again before we know they've been enabled,
			// causing it to get stuck in a loop being woken by IRQs
			// that are never delivered.
			ICH_HCR_EL2_set_TALL0(&vcpu->vgic_ich_hcr, true);
			ICH_HCR_EL2_set_TALL1(&vcpu->vgic_ich_hcr, true);
		}

		// Always set LRENPIE, and keep UIE off. This is because we
		// don't reload active interrupts into the LRs once they've been
		// kicked out; the complexity of doing that outweighs any
		// performance benefit, especially when most VMs are Linux -
		// which uses neither EOImode (in EL1) nor preemption, and
		// therefore will never have multiple active IRQs to trigger
		// this in the first place.
		ICH_HCR_EL2_set_UIE(&vcpu->vgic_ich_hcr, false);
		ICH_HCR_EL2_set_LRENPIE(&vcpu->vgic_ich_hcr, true);
#if VGIC_HAS_LPI && GICV3_HAS_VLPI_V4_1
		// We don't know whether to set vSGIEOICount until the VM
		// enables groups in GICD_CTLR, at which point we must propagate
		// the nASSGIreq bit from the same register to all the vCPUs.
		// That is done in vgic_gicr_update_group_enables().
		ICH_HCR_EL2_set_vSGIEOICount(&vcpu->vgic_ich_hcr, false);
#endif
		// Always trap DIR, so we know which IRQs are being deactivated
		// when the VM uses EOImode=1. We can't rely on LRENPIE/EOIcount
		// in this case (as opposed to EOImode=0, when we can assume the
		// highest priority active interrupts are being deactivated).
		ICH_HCR_EL2_set_TDIR(&vcpu->vgic_ich_hcr, true);
		// Always enable the interface.
		ICH_HCR_EL2_set_En(&vcpu->vgic_ich_hcr, true);

		vcpu->vgic_ich_vmcr = ICH_VMCR_EL2_default();
	}

	return OK;
}

index_result_t
vgic_get_index_for_mpidr(vic_t *vic, uint8_t aff0, uint8_t aff1, uint8_t aff2,
			 uint8_t aff3)
{
	platform_mpidr_mapping_t mapping = vic->mpidr_mapping;
	index_result_t		 ret;

	if (compiler_unexpected(((~mapping.aff_mask[0] & aff0) != 0U) ||
				((~mapping.aff_mask[1] & aff1) != 0U) ||
				((~mapping.aff_mask[2] & aff2) != 0U) ||
				((~mapping.aff_mask[3] & aff3) != 0U))) {
		ret = index_result_error(ERROR_ARGUMENT_INVALID);
		goto out;
	}

	index_t index = 0U;
	index |= ((index_t)aff0 << mapping.aff_shift[0]);
	index |= ((index_t)aff1 << mapping.aff_shift[1]);
	index |= ((index_t)aff2 << mapping.aff_shift[2]);
	index |= ((index_t)aff3 << mapping.aff_shift[3]);

	if (compiler_unexpected(index >= vic->gicr_count)) {
		ret = index_result_error(ERROR_ARGUMENT_INVALID);
		goto out;
	}

	ret = index_result_ok(index);
out:
	return ret;
}

error_t
vgic_handle_object_activate_thread(thread_t *vcpu)
{
	error_t err = OK;
	vic_t  *vic = vcpu->vgic_vic;

	if (vic != NULL) {
		spinlock_acquire(&vic->gicd_lock);

		index_t index = vcpu->vgic_gicr_index;

		if (atomic_load_relaxed(&vic->gicr_vcpus[index]) != NULL) {
			err = ERROR_BUSY;
			goto out_locked;
		}

		// Initialise the local IRQ delivery states, including their
		// route fields which are fixed to this CPU's index to simplify
		// the routing logic elsewhere.
		//
		// The SGIs are always edge-triggered, so set the edge trigger
		// bit in their dstates.
		vgic_delivery_state_t sgi_dstate =
			vgic_delivery_state_default();
		vgic_delivery_state_set_cfg_is_edge(&sgi_dstate, true);
		vgic_delivery_state_set_route(&sgi_dstate, index);
		for (index_t i = 0; i < GIC_SGI_NUM; i++) {
			atomic_init(&vcpu->vgic_private_states[i], sgi_dstate);
		}
		// PPIs are normally level-triggered.
		vgic_delivery_state_t ppi_dstate =
			vgic_delivery_state_default();
		vgic_delivery_state_set_route(&ppi_dstate, index);
		for (index_t i = 0; i < GIC_PPI_NUM; i++) {
			atomic_init(
				&vcpu->vgic_private_states[GIC_PPI_BASE + i],
				ppi_dstate);
		}

		// Determine the physical interrupt route that should be used
		// for interrupts that target this VCPU.
		scheduler_lock_nopreempt(vcpu);
		cpu_index_t affinity = scheduler_get_affinity(vcpu);
		MPIDR_EL1_t mpidr    = platform_cpu_index_to_mpidr(
			   cpulocal_index_valid(affinity) ? affinity : 0U);
		GICD_IROUTER_t phys_route = GICD_IROUTER_default();
		GICD_IROUTER_set_IRM(&phys_route, false);
		GICD_IROUTER_set_Aff0(&phys_route, MPIDR_EL1_get_Aff0(&mpidr));
		GICD_IROUTER_set_Aff1(&phys_route, MPIDR_EL1_get_Aff1(&mpidr));
		GICD_IROUTER_set_Aff2(&phys_route, MPIDR_EL1_get_Aff2(&mpidr));
		GICD_IROUTER_set_Aff3(&phys_route, MPIDR_EL1_get_Aff3(&mpidr));
		vcpu->vgic_irouter = phys_route;

#if VGIC_HAS_LPI && GICV3_HAS_VLPI
#if GICV3_HAS_VLPI_V4_1
		// VSGI setup has not been done yet; set the sequence
		// number to one that will never be complete.
		atomic_init(&vcpu->vgic_vsgi_setup_seq, ~(count_t)0U);
#endif

		if (vgic_has_lpis(vic)) {
			size_t vlpi_pendbase_size =
				BITMAP_NUM_WORDS(util_bit(vic->gicd_idbits)) *
				sizeof(register_t);
			size_t vlpi_pendbase_align =
				util_bit(GIC_ITS_CMD_VMAPP_VPT_ADDR_PRESHIFT);
			void_ptr_result_t alloc_r = partition_alloc(
				vcpu->header.partition, vlpi_pendbase_size,
				vlpi_pendbase_align);
			if (alloc_r.e != OK) {
				err = alloc_r.e;
				goto out_vcpu_locked;
			}

			// Call the ITS driver to allocate a vPE ID and a
			// doorbell LPI for this VCPU. We do this before we
			// save the pending table pointer so the cleanup
			// function can use the pointer to decide whether
			// to call gicv3_its_vpe_cleanup(vcpu).
			err = gicv3_its_vpe_activate(vcpu);
			if (err != OK) {
				(void)partition_free(vcpu->header.partition,
						     alloc_r.r,
						     vlpi_pendbase_size);
				goto out_vcpu_locked;
			}

			// No need to memset here; it will be done (with a
			// possible partial memcpy from the VM) before we issue
			// a VMAPP, when the VM writes 1 to EnableLPIs.
			vcpu->vgic_vlpi_pending_table = alloc_r.r;
		}
#endif

		// Set the GICD's pointer to the VCPU. This is a store release
		// so we can be sure that all of the thread's initialisation is
		// complete before the VGIC tries to use it.
		atomic_store_release(&vic->gicr_vcpus[index], vcpu);

#if VGIC_HAS_LPI && GICV3_HAS_VLPI
	out_vcpu_locked:
#endif
		scheduler_unlock_nopreempt(vcpu);
	out_locked:
		spinlock_release(&vic->gicd_lock);

		if (err == OK) {
			vcpu->vcpu_regs_mpidr_el1 =
				platform_cpu_map_index_to_mpidr(
					&vic->mpidr_mapping, index);

			// Check for IRQs that were routed to this CPU and
			// delivered before it was attached, to make sure they
			// are flagged locally.
			vgic_retry_unrouted(vic);
		}
	}

	return err;
}

void
vgic_handle_scheduler_affinity_changed(thread_t *vcpu, cpu_index_t next_cpu)
{
	MPIDR_EL1_t    mpidr	  = platform_cpu_index_to_mpidr(next_cpu);
	GICD_IROUTER_t phys_route = GICD_IROUTER_default();
	GICD_IROUTER_set_IRM(&phys_route, false);
	GICD_IROUTER_set_Aff0(&phys_route, MPIDR_EL1_get_Aff0(&mpidr));
	GICD_IROUTER_set_Aff1(&phys_route, MPIDR_EL1_get_Aff1(&mpidr));
	GICD_IROUTER_set_Aff2(&phys_route, MPIDR_EL1_get_Aff2(&mpidr));
	GICD_IROUTER_set_Aff3(&phys_route, MPIDR_EL1_get_Aff3(&mpidr));
	vcpu->vgic_irouter = phys_route;
}

void
vgic_handle_object_deactivate_thread(thread_t *thread)
{
	assert(thread_get_self() != thread);
	assert(!cpulocal_index_valid(
		atomic_load_relaxed(&thread->vgic_lr_owner_lock.owner)));

	vic_t *vic = thread->vgic_vic;
	if (vic != NULL) {
		rcu_read_start();
		for (index_t i = 0; i < GIC_PPI_NUM; i++) {
			virq_source_t *virq_source =
				atomic_load_consume(&thread->vgic_sources[i]);

			if (virq_source == NULL) {
				continue;
			}

			vic_unbind(virq_source);
		}
		rcu_read_finish();

		spinlock_acquire(&vic->gicd_lock);

		assert(thread->vgic_gicr_index < vic->gicr_count);
		if (atomic_load_relaxed(
			    &vic->gicr_vcpus[thread->vgic_gicr_index]) ==
		    thread) {
			atomic_store_relaxed(
				&vic->gicr_vcpus[thread->vgic_gicr_index],
				NULL);
		}

#if VGIC_HAS_LPI
		if (vgic_has_lpis(vic) &&
		    (thread->vgic_vlpi_pending_table != NULL)) {
			// Ensure that any outstanding unmap has finished
			GICR_CTLR_t old_ctlr =
				atomic_load_relaxed(&thread->vgic_gicr_rd_ctlr);
			if (GICR_CTLR_get_Enable_LPIs(&old_ctlr)) {
				count_result_t count_r =
					gicv3_its_vpe_unmap(thread);
				assert(count_r.e == OK);
				thread->vgic_vlpi_unmap_seq = count_r.r;
			}
		}
#endif

		if (thread->vgic_gicr_device.type != VDEVICE_TYPE_NONE) {
			vdevice_detach_vmaddr(&thread->vgic_gicr_device);
		}

		spinlock_release(&vic->gicd_lock);
	}
}

void
vgic_unwind_object_activate_thread(thread_t *thread)
{
	vgic_handle_object_deactivate_thread(thread);
}

void
vgic_handle_object_cleanup_thread(thread_t *thread)
{
	partition_t *partition = thread->header.partition;
	assert(partition != NULL);

	vic_t *vic = thread->vgic_vic;
	if (vic != NULL) {
		// Ensure that the VIRQ groups are disabled
		thread->vgic_group0_enabled = false;
		thread->vgic_group1_enabled = false;

		// Clear out all LRs and re-route all pending IRQs
		vgic_undeliver_all(vic, thread);

#if VGIC_HAS_LPI && GICV3_HAS_VLPI
		if (vgic_has_lpis(vic) &&
		    (thread->vgic_vlpi_pending_table != NULL)) {
			// Ensure that any outstanding unmap has finished
			GICR_CTLR_t old_ctlr =
				atomic_load_relaxed(&thread->vgic_gicr_rd_ctlr);
			if (GICR_CTLR_get_Enable_LPIs(&old_ctlr)) {
				(void)gicv3_its_wait(
					0U, thread->vgic_vlpi_unmap_seq);
			}

			// Discard the pending table
			size_t vlpi_pendbase_size =
				BITMAP_NUM_WORDS(util_bit(vic->gicd_idbits)) *
				sizeof(register_t);
			(void)partition_free(thread->header.partition,
					     thread->vgic_vlpi_pending_table,
					     vlpi_pendbase_size);
			thread->vgic_vlpi_pending_table = NULL;

			// Tell the ITS driver to release the allocated vPE ID
			// and doorbell IRQ.
			gicv3_its_vpe_cleanup(thread);
		} else {
			assert(thread->vgic_vlpi_pending_table == NULL);
		}
#endif

#if VGIC_HAS_1N
		// Wake any other threads on the GIC, in case the deferred IRQs
		// can be rerouted.
		vgic_sync_all(vic, true);
#endif

		object_put_vic(vic);
	}
}

static void
vgic_handle_rootvm_create_hwirq(partition_t	 *root_partition,
				cspace_t	 *root_cspace,
				qcbor_enc_ctxt_t *qcbor_enc_ctxt)
{
	index_t i = 0U;
#if GICV3_EXT_IRQS
	index_t last_spi = util_min((count_t)platform_irq_max(),
				    GIC_SPI_EXT_BASE + GIC_SPI_EXT_NUM - 1U);
#else
	index_t last_spi = util_min((count_t)platform_irq_max(),
				    GIC_SPI_BASE + GIC_SPI_NUM - 1U);
#endif

	QCBOREncode_OpenArrayInMap(qcbor_enc_ctxt, "vic_hwirq");
	while (i <= last_spi) {
		hwirq_create_t hwirq_params = {
			.irq = i,
		};

		gicv3_irq_type_t irq_type = gicv3_get_irq_type(i);

		if (irq_type == GICV3_IRQ_TYPE_SPI) {
			hwirq_params.action = HWIRQ_ACTION_VGIC_FORWARD_SPI;
		} else if (irq_type == GICV3_IRQ_TYPE_PPI) {
			hwirq_params.action =
				HWIRQ_ACTION_VIC_BASE_FORWARD_PRIVATE;
#if GICV3_EXT_IRQS
		} else if (irq_type == GICV3_IRQ_TYPE_SPI_EXT) {
			hwirq_params.action = HWIRQ_ACTION_VGIC_FORWARD_SPI;
		} else if (irq_type == GICV3_IRQ_TYPE_PPI_EXT) {
			hwirq_params.action =
				HWIRQ_ACTION_VIC_BASE_FORWARD_PRIVATE;
#endif
		} else {
			QCBOREncode_AddUInt64(qcbor_enc_ctxt,
					      CSPACE_CAP_INVALID);
			goto next_index;
		}

		hwirq_ptr_result_t hwirq_r =
			partition_allocate_hwirq(root_partition, hwirq_params);
		if (hwirq_r.e != OK) {
			panic("Unable to create HW IRQ object");
		}

		error_t err = object_activate_hwirq(hwirq_r.r);
		if (err != OK) {
			if ((err == ERROR_DENIED) ||
			    (err == ERROR_ARGUMENT_INVALID) ||
			    (err == ERROR_BUSY)) {
				QCBOREncode_AddUInt64(qcbor_enc_ctxt,
						      CSPACE_CAP_INVALID);
				object_put_hwirq(hwirq_r.r);
				goto next_index;
			} else {
				panic("Failed to activate HW IRQ object");
			}
		}

		// Create a master cap for the HWIRQ
		object_ptr_t	hwirq_optr = { .hwirq = hwirq_r.r };
		cap_id_result_t cid_r	   = cspace_create_master_cap(
			     root_cspace, hwirq_optr, OBJECT_TYPE_HWIRQ);
		if (cid_r.e != OK) {
			panic("Unable to create cap to HWIRQ");
		}
		QCBOREncode_AddUInt64(qcbor_enc_ctxt, cid_r.r);

	next_index:
		i++;
#if GICV3_EXT_IRQS
		// Skip large range between end of non-extended PPIs and start
		// of extended SPIs to optimize encoding
		if (i == GIC_PPI_EXT_BASE + GIC_PPI_EXT_NUM) {
			i = GIC_SPI_EXT_BASE;
		}
#endif
	}

	// Check if the insertion failed because of buffer overflow, on error
	// the config QCBOR_ENV_CONFIG_SIZE needs to be increased
	if (QCBOREncode_GetErrorState(qcbor_enc_ctxt) != QCBOR_SUCCESS) {
		panic("QCBOR data buffer too small");
	}
	QCBOREncode_CloseArray(qcbor_enc_ctxt);
}

void
vgic_handle_rootvm_init(partition_t *root_partition, thread_t *root_thread,
			cspace_t *root_cspace, hyp_env_data_t *hyp_env,
			qcbor_enc_ctxt_t *qcbor_enc_ctxt)
{
	// Create the VIC object for the root VM
	vic_create_t	 vic_params = { 0 };
	vic_ptr_result_t vic_r =
		partition_allocate_vic(root_partition, vic_params);
	if (vic_r.e != OK) {
		goto vic_fail;
	}
	spinlock_acquire(&vic_r.r->header.lock);
	count_t max_vcpus = 1U;
	count_t max_virqs = 64U;
	count_t max_msis  = 0U;

	assert(qcbor_enc_ctxt != NULL);

	hyp_env->gicd_base   = PLATFORM_GICD_BASE;
	hyp_env->gicr_base   = PLATFORM_GICR_BASE;
	hyp_env->gicr_stride = (size_t)util_bit(GICR_STRIDE_SHIFT);

	QCBOREncode_AddUInt64ToMap(qcbor_enc_ctxt, "gicd_base",
				   PLATFORM_GICD_BASE);
	QCBOREncode_AddUInt64ToMap(qcbor_enc_ctxt, "gicr_stride",
				   (size_t)util_bit(GICR_STRIDE_SHIFT));
	// Array of tuples of base address and number of GICRs for each
	// contiguous GICR range. Currently only one range is supported.
	QCBOREncode_OpenArrayInMap(qcbor_enc_ctxt, "gicr_ranges");
	QCBOREncode_OpenArray(qcbor_enc_ctxt);
	QCBOREncode_AddUInt64(qcbor_enc_ctxt, PLATFORM_GICR_BASE);
	QCBOREncode_AddUInt64(qcbor_enc_ctxt, PLATFORM_GICR_COUNT);
	QCBOREncode_CloseArray(qcbor_enc_ctxt);
	QCBOREncode_CloseArray(qcbor_enc_ctxt);

	if (vic_configure(vic_r.r, max_vcpus, max_virqs, max_msis, false) !=
	    OK) {
		spinlock_release(&vic_r.r->header.lock);
		goto vic_fail;
	}
	spinlock_release(&vic_r.r->header.lock);

	if (object_activate_vic(vic_r.r) != OK) {
		goto vic_fail;
	}

	// Create a master cap for the VIC
	object_ptr_t	vic_optr = { .vic = vic_r.r };
	cap_id_result_t cid_r = cspace_create_master_cap(root_cspace, vic_optr,
							 OBJECT_TYPE_VIC);
	if (cid_r.e != OK) {
		goto vic_fail;
	}
	hyp_env->vic = cid_r.r;
	QCBOREncode_AddUInt64ToMap(qcbor_enc_ctxt, "vic", cid_r.r);

	index_t vic_index = 0U;

	if (vic_attach_vcpu(vic_r.r, root_thread, vic_index) != OK) {
		panic("VIC couldn't attach root VM thread");
	}

	// Create a HWIRQ object for every SPI
	vgic_handle_rootvm_create_hwirq(root_partition, root_cspace,
					qcbor_enc_ctxt);
	hyp_env->gits_base   = 0U;
	hyp_env->gits_stride = 0U;

	return;

vic_fail:
	panic("Unable to create root VM's virtual GIC");
}

void
vgic_handle_rootvm_init_late(thread_t		  *root_thread,
			     const hyp_env_data_t *hyp_env)
{
	assert(root_thread != NULL);
	assert(hyp_env != NULL);

	addrspace_t *root_addrspace = root_thread->addrspace;
	if (root_addrspace == NULL) {
		panic("vgic rootvm_init_late: addrspace not yet created\n");
	}

	vic_t *root_vic = root_thread->vgic_vic;
	spinlock_acquire(&root_vic->gicd_lock);

	root_vic->gicd_device.type = VDEVICE_TYPE_VGIC_GICD;
	if (vdevice_attach_vmaddr(&root_vic->gicd_device, root_addrspace,
				  hyp_env->gicd_base, sizeof(gicd_t)) != OK) {
		panic("vgic rootvm_init_late: unable to map GICD\n");
	}

	rcu_read_start();
	for (index_t i = 0U; i < root_vic->gicr_count; i++) {
		thread_t *gicr_vcpu =
			atomic_load_consume(&root_vic->gicr_vcpus[i]);
		if (gicr_vcpu == NULL) {
			continue;
		}
		gicr_vcpu->vgic_gicr_device.type = VDEVICE_TYPE_VGIC_GICR;
		if (vdevice_attach_vmaddr(
			    &gicr_vcpu->vgic_gicr_device, root_addrspace,
			    hyp_env->gicr_base + (i * hyp_env->gicr_stride),
			    hyp_env->gicr_stride) != OK) {
			panic("vgic rootvm_init_late: unable to map GICR\n");
		}
	}
	rcu_read_finish();
	spinlock_release(&root_vic->gicd_lock);
}

error_t
vgic_handle_object_create_hwirq(hwirq_create_t hwirq_create)
{
	hwirq_t *hwirq = hwirq_create.hwirq;
	assert(hwirq != NULL);

	error_t err = ERROR_ARGUMENT_INVALID;

	if (hwirq_create.action == HWIRQ_ACTION_VGIC_FORWARD_SPI) {
		gicv3_irq_type_t irq_type =
			gicv3_get_irq_type(hwirq_create.irq);
		// The physical IRQ must be an SPI.
		if (irq_type == GICV3_IRQ_TYPE_SPI) {
			err = OK;
#if GICV3_EXT_IRQS
		} else if (irq_type == GICV3_IRQ_TYPE_SPI_EXT) {
			err = OK;
#endif
		}
	} else if (hwirq_create.action ==
		   HWIRQ_ACTION_VIC_BASE_FORWARD_PRIVATE) {
		gicv3_irq_type_t irq_type =
			gicv3_get_irq_type(hwirq_create.irq);
		// The physical IRQ must be an PPI.
		if (irq_type == GICV3_IRQ_TYPE_PPI) {
			err = OK;
#if GICV3_EXT_IRQS
		} else if (irq_type == GICV3_IRQ_TYPE_PPI_EXT) {
			err = OK;
#endif
		}
	} else {
		// Not a forwarded IRQ
		err = OK;
	}

	return err;
}

void
vgic_handle_object_deactivate_hwirq(hwirq_t *hwirq)
{
	if (hwirq->action == HWIRQ_ACTION_VGIC_FORWARD_SPI) {
		vic_unbind(&hwirq->vgic_spi_source);
	}
}

error_t
vgic_bind_hwirq_spi(vic_t *vic, hwirq_t *hwirq, virq_t virq)
{
	error_t err;

	assert(hwirq->action == HWIRQ_ACTION_VGIC_FORWARD_SPI);

	if (vgic_get_irq_type(virq) != VGIC_IRQ_TYPE_SPI) {
		err = ERROR_ARGUMENT_INVALID;
		goto out;
	}

	err = vic_bind_shared(&hwirq->vgic_spi_source, vic, virq,
			      VIRQ_TRIGGER_VGIC_FORWARDED_SPI);
	if (err != OK) {
		goto out;
	}

	// Take the GICD lock to ensure that the vGIC's IRQ config does not
	// change while we are copying it to the hardware GIC
	spinlock_acquire(&vic->gicd_lock);

	_Atomic vgic_delivery_state_t *dstate =
		vgic_find_dstate(vic, NULL, virq);
	assert(dstate != NULL);
	vgic_delivery_state_t current_dstate = atomic_load_relaxed(dstate);

	// Default to an invalid physical route
	GICD_IROUTER_t physical_router = GICD_IROUTER_default();
	GICD_IROUTER_set_IRM(&physical_router, false);
	GICD_IROUTER_set_Aff0(&physical_router, 0xff);
	GICD_IROUTER_set_Aff1(&physical_router, 0xff);
	GICD_IROUTER_set_Aff2(&physical_router, 0xff);
	GICD_IROUTER_set_Aff3(&physical_router, 0xff);

	// Try to set the physical route based on the virtual route
	rcu_read_start();
	thread_t *new_target = vgic_find_target(vic, &hwirq->vgic_spi_source);
	if (new_target != NULL) {
		physical_router = new_target->vgic_irouter;

		VGIC_TRACE(ROUTE, vic, NULL,
			   "bind {:d}: route virt {:d} phys {:#x}", virq,
			   new_target->vgic_gicr_index,
			   GICD_IROUTER_raw(physical_router));
	} else {
#if GICV3_HAS_1N
		// No direct target, so let the physical GIC choose
		GICD_IROUTER_set_IRM(&physical_router, true);
#endif

		VGIC_TRACE(ROUTE, vic, NULL,
			   "bind {:d}: route virt none phys {:#x}", virq,
			   GICD_IROUTER_raw(physical_router));
	}
	rcu_read_finish();

	// Set the chosen physical route
	err = gicv3_spi_set_route(hwirq->irq, physical_router);
	if (err != OK) {
		goto release_lock;
	}

#if GICV3_HAS_GICD_ICLAR
	if (GICD_IROUTER_get_IRM(&physical_router)) {
		// Set the HW IRQ's 1-of-N routing classes.
		err = gicv3_spi_set_classes(
			hwirq->irq,
			!vgic_delivery_state_get_nclass0(&current_dstate),
			vgic_delivery_state_get_class1(&current_dstate));

		if (err != OK) {
			goto release_lock;
		}
	}
#endif

	// Attempt to set the HW IRQ's trigger mode based on the virtual ICFGR;
	// if this fails because the HW trigger mode is fixed, then update the
	// virtual ICFGR insted.
	bool is_edge = vgic_delivery_state_get_cfg_is_edge(&current_dstate);
	irq_trigger_t	     mode     = is_edge ? IRQ_TRIGGER_EDGE_RISING
						: IRQ_TRIGGER_LEVEL_HIGH;
	irq_trigger_result_t new_mode = trigger_virq_set_mode_event(
		VIRQ_TRIGGER_VGIC_FORWARDED_SPI, &hwirq->vgic_spi_source, mode);
	if ((new_mode.e != OK) || (new_mode.r != mode)) {
		vgic_delivery_state_t cfg_is_edge =
			vgic_delivery_state_default();
		vgic_delivery_state_set_cfg_is_edge(&cfg_is_edge, true);
		// Mode change failed; the hardware config must be fixed to the
		// other mode. Flip the software mode.
		if (is_edge) {
			(void)vgic_delivery_state_atomic_intersection(
				dstate, cfg_is_edge, memory_order_relaxed);
		} else {
			(void)vgic_delivery_state_atomic_difference(
				dstate, cfg_is_edge, memory_order_relaxed);
		}
	}

	// Enable the HW IRQ if the virtual enable bit is set (unbound HW IRQs
	// are always disabled).
	if (vgic_delivery_state_get_enabled(&current_dstate)) {
		irq_enable_shared(hwirq);
	}

	hwirq->vgic_enable_hw = true;

release_lock:
	spinlock_release(&vic->gicd_lock);

out:
	return err;
}

error_t
vgic_unbind_hwirq_spi(hwirq_t *hwirq)
{
	error_t err;

	assert(hwirq->action == HWIRQ_ACTION_VGIC_FORWARD_SPI);

	rcu_read_start();
	vic_t *vic = atomic_load_consume(&hwirq->vgic_spi_source.vic);
	if (vic == NULL) {
		rcu_read_finish();
		err = ERROR_VIRQ_NOT_BOUND;
		goto out;
	}

	// Ensure that no other thread can concurrently enable the HW IRQ by
	// enabling the bound VIRQ.
	spinlock_acquire(&vic->gicd_lock);
	hwirq->vgic_enable_hw = false;
	spinlock_release(&vic->gicd_lock);
	rcu_read_finish();

	// Disable the IRQ, and wait for running handlers to complete.
	irq_disable_shared_sync(hwirq);

	// Remove the VIRQ binding, and wait until the source can be reused.
	vic_unbind_sync(&hwirq->vgic_spi_source);

	err = OK;
out:
	return err;
}

bool
vgic_handle_virq_set_enabled_hwirq_spi(virq_source_t *source, bool enabled)
{
	hwirq_t *hwirq = hwirq_from_virq_source(source);
	assert(!source->is_private);
	assert(!platform_irq_is_percpu(hwirq->irq));

	if (enabled) {
		if (compiler_expected(hwirq->vgic_enable_hw)) {
			irq_enable_shared(hwirq);
		}
	} else {
		irq_disable_shared_nosync(hwirq);
	}

	return true;
}

irq_trigger_result_t
vgic_handle_virq_set_mode_hwirq_spi(virq_source_t *source, irq_trigger_t mode)
{
	hwirq_t *hwirq = hwirq_from_virq_source(source);

	assert(!source->is_private);
	assert(!platform_irq_is_percpu(hwirq->irq));

	return gicv3_irq_set_trigger_shared(hwirq->irq, mode);
}

static void
vgic_change_irq_pending(vic_t *vic, thread_t *target, irq_t irq_num,
			bool is_private, virq_source_t *source, bool set,
			bool is_msi)
{
	_Atomic vgic_delivery_state_t *dstate =
		vgic_find_dstate(vic, target, irq_num);
	assert(dstate != NULL);

	preempt_disable();

	// Determine the pending flags to change.
	vgic_delivery_state_t change_dstate = vgic_delivery_state_default();
	vgic_delivery_state_set_edge(&change_dstate, true);
	if (is_msi) {
		vgic_delivery_state_set_level_msg(&change_dstate, true);
	} else {
		vgic_delivery_state_set_level_sw(&change_dstate, true);
	}

	if (set) {
		(void)vgic_deliver(irq_num, vic, target, source, dstate,
				   change_dstate, is_private);
	} else {
		// Forwarded SPIs must be deactivated; otherwise they will
		// become undeliverable until asserted in software. This has no
		// effect on IRQs that are not forwarded SPIs.
		vgic_delivery_state_set_hw_active(&change_dstate, true);

		// Edge-triggered forwarded SPIs need to be cleared in hardware
		// as well, in case they have a pending state the hypervisor
		// hasn't seen yet. This has no effect on level-triggered IRQs.
		bool is_hw =
			(source != NULL) &&
			(source->trigger == VIRQ_TRIGGER_VGIC_FORWARDED_SPI);
		if (is_hw) {
			hwirq_t *hwirq = hwirq_from_virq_source(source);
			gicv3_irq_cancel_nowait(hwirq->irq);
		}

		// Undeliver the IRQ.
		//
		// We don't forcibly reclaim the VIRQ because it might still be
		// pending from a level-triggered hardware source. This means we
		// don't know whether to trigger a sync if the VIRQ is still
		// remotely listed.
		//
		// It is strictly ok not to sync, because the GIC specification
		// implicitly permits this operation to take an arbitrarily long
		// time to be effective (it can't be polled like ICENABLER, and
		// there is no finite-time guarantee of completion like there is
		// for IPRIORITYR etc.). Still, this might cause problems for
		// drivers that assume that ICPENDR works.
		(void)vgic_undeliver(vic, target, dstate, irq_num,
				     change_dstate, false);
	}

	preempt_enable();
}

static void
vgic_change_irq_enable(vic_t *vic, thread_t *target, irq_t irq_num,
		       bool is_private, virq_source_t *source, bool set)
	REQUIRE_PREEMPT_DISABLED
{
	_Atomic vgic_delivery_state_t *dstate =
		vgic_find_dstate(vic, target, irq_num);
	assert(dstate != NULL);

	if ((source != NULL) && !set) {
		(void)trigger_virq_set_enabled_event(source->trigger, source,
						     set);
	}

	vgic_delivery_state_t change_dstate = vgic_delivery_state_default();
	vgic_delivery_state_set_enabled(&change_dstate, true);

	if (set) {
		(void)vgic_deliver(irq_num, vic, target, source, dstate,
				   change_dstate, is_private);

	} else {
		// Undeliver and reclaim the VIRQ.
		if (!vgic_undeliver(vic, target, dstate, irq_num, change_dstate,
				    false)) {
			vgic_sync_all(vic, false);
		}
	}

	if ((source != NULL) && set) {
		(void)trigger_virq_set_enabled_event(source->trigger, source,
						     set);
	}
}

static void
vgic_change_irq_active(vic_t *vic, thread_t *vcpu, irq_t irq_num, bool set)
{
	_Atomic vgic_delivery_state_t *dstate =
		vgic_find_dstate(vic, vcpu, irq_num);
	assert(dstate != NULL);

	// Accurately virtualising ISACTIVER / ICACTIVER, even for reads, is
	// challenging due to the list register model; we would have to be
	// able to simultaneously block all attached VCPUs (including those that
	// are running remotely) and read and write their LRs to do it
	// accurately.
	//
	// This doesn't matter much, though, since they are only really useful
	// for power management (typically at EL3, no not in our VMs) and
	// debugging the GIC driver (which shouldn't be happening in a VM).
	//
	// We take the easy approach here, and simply ignore any writes to
	// currently listed VIRQs.

	// Don't let context switches delist the VIRQ out from under us
	preempt_disable();

	vgic_delivery_state_t old_dstate = atomic_load_relaxed(dstate);
	if (vgic_delivery_state_get_listed(&old_dstate)) {
		// Interrupt is listed; ignore the write.
	} else if (!set) {
		vgic_deactivate(vic, vcpu, irq_num, dstate, old_dstate, false,
				false);
	} else {
		vgic_delivery_state_t new_dstate;
		do {
			if (vgic_delivery_state_get_listed(&old_dstate)) {
				break;
			}
			new_dstate = old_dstate;
			vgic_delivery_state_set_active(&new_dstate, set);
		} while (!atomic_compare_exchange_weak_explicit(
			dstate, &old_dstate, new_dstate, memory_order_relaxed,
			memory_order_relaxed));
	}

	preempt_enable();
}

static void
vgic_sync_group_change(vic_t *vic, virq_t irq_num,
		       _Atomic vgic_delivery_state_t *dstate, bool is_group_1)
{
	assert(dstate != NULL);

	// Atomically update the group bit and obtain the current state.
	vgic_delivery_state_t old_dstate = atomic_load_relaxed(dstate);
	vgic_delivery_state_t new_dstate;
	do {
		new_dstate = old_dstate;
		vgic_delivery_state_set_group1(&new_dstate, is_group_1);
		if (vgic_delivery_state_get_listed(&old_dstate)) {
			// To guarantee that the group change takes effect in
			// finite time, request a sync of the listed VIRQ.
			vgic_delivery_state_set_need_sync(&new_dstate,
							  is_group_1);
		}
	} while (!atomic_compare_exchange_weak_explicit(
		dstate, &old_dstate, new_dstate, memory_order_relaxed,
		memory_order_relaxed));

	if (vgic_delivery_state_get_listed(&old_dstate)) {
		// We requested a sync above; notify the VCPUs.
		vgic_sync_all(vic, false);
	} else {
		// Retry delivery, in case the group change made the IRQ
		// deliverable.
		rcu_read_start();
		thread_t *target =
			vgic_get_route_from_state(vic, new_dstate, false);
		if (target != NULL) {
			virq_source_t *source =
				vgic_find_source(vic, target, irq_num);
			(void)vgic_deliver(irq_num, vic, target, source, dstate,
					   vgic_delivery_state_default(),
					   vgic_irq_is_private(irq_num));
		}
		rcu_read_finish();
	}

	(void)0;
}

static void
vgic_set_irq_priority(vic_t *vic, thread_t *vcpu, irq_t irq_num,
		      uint8_t priority)
{
	_Atomic vgic_delivery_state_t *dstate =
		vgic_find_dstate(vic, vcpu, irq_num);
	assert(dstate != NULL);

	vgic_delivery_state_t old_dstate = atomic_load_relaxed(dstate);
	vgic_delivery_state_t new_dstate;
	do {
		new_dstate = old_dstate;

		vgic_delivery_state_set_priority(&new_dstate, priority);

		// If the priority is being raised (made lesser), then there is
		// a possibility that its target VCPU can't receive it at the
		// old priority due to other active IRQs or a manual priority
		// mask, and is blocked in WFI; in this case we must send a sync
		// if the VIRQ is listed, or retry delivery at the new priority
		// if it is not listed (below).
		if ((priority <
		     vgic_delivery_state_get_priority(&old_dstate)) &&
		    vgic_delivery_state_get_listed(&old_dstate)) {
			vgic_delivery_state_set_need_sync(&new_dstate, true);
		}
	} while (!atomic_compare_exchange_strong_explicit(
		dstate, &old_dstate, new_dstate, memory_order_relaxed,
		memory_order_relaxed));

	if (priority < vgic_delivery_state_get_priority(&old_dstate)) {
		if (vgic_delivery_state_get_listed(&old_dstate)) {
			// To guarantee that the priority change will take
			// effect in finite time, sync all VCPUs that might have
			// it listed.
			vgic_sync_all(vic, false);
		} else if (vgic_delivery_state_get_enabled(&old_dstate) &&
			   vgic_delivery_state_is_pending(&old_dstate)) {
			// Retry delivery, in case it previously did not select
			// a LR only because the priority was too low
			rcu_read_start();
			thread_t *target = vgic_get_route_from_state(
				vic, new_dstate, false);
			if (target != NULL) {
				virq_source_t *source =
					vgic_find_source(vic, target, irq_num);
				(void)vgic_deliver(
					irq_num, vic, target, source, dstate,
					vgic_delivery_state_default(),
					vgic_irq_is_private(irq_num));
			}
			rcu_read_finish();
		} else {
			// Unlisted and not deliverable; nothing to do.
		}
	}
}

void
vgic_gicd_set_control(vic_t *vic, GICD_CTLR_DS_t ctlr)
{
	spinlock_acquire(&vic->gicd_lock);
	GICD_CTLR_DS_t old_ctlr = atomic_load_relaxed(&vic->gicd_ctlr);
	GICD_CTLR_DS_t new_ctlr = old_ctlr;

	GICD_CTLR_DS_copy_EnableGrp0(&new_ctlr, &ctlr);
	GICD_CTLR_DS_copy_EnableGrp1(&new_ctlr, &ctlr);
#if GICV3_HAS_VLPI_V4_1 && VGIC_HAS_LPI
	if (!GICD_CTLR_DS_get_EnableGrp0(&old_ctlr) &&
	    !GICD_CTLR_DS_get_EnableGrp1(&old_ctlr)) {
		GICD_CTLR_DS_copy_nASSGIreq(&new_ctlr, &ctlr);
	}
#endif

	if (!GICD_CTLR_DS_is_equal(new_ctlr, old_ctlr)) {
#if GICV3_HAS_VLPI_V4_1 && VGIC_HAS_LPI
		vic->vsgis_enabled = GICD_CTLR_DS_get_nASSGIreq(&new_ctlr);
#endif
		atomic_store_relaxed(&vic->gicd_ctlr, new_ctlr);
		vgic_update_enables(vic, new_ctlr);
	}

	spinlock_release(&vic->gicd_lock);
}

void
vgic_gicd_set_statusr(vic_t *vic, GICD_STATUSR_t statusr, bool set)
{
	spinlock_acquire(&vic->gicd_lock);
	if (set) {
		vic->gicd_statusr =
			GICD_STATUSR_union(vic->gicd_statusr, statusr);
	} else {
		vic->gicd_statusr =
			GICD_STATUSR_difference(vic->gicd_statusr, statusr);
	}
	spinlock_release(&vic->gicd_lock);
}

void
vgic_gicd_change_irq_pending(vic_t *vic, irq_t irq_num, bool set, bool is_msi)
{
	if (vgic_irq_is_spi(irq_num)) {
		rcu_read_start();
		virq_source_t *source = vgic_find_source(vic, NULL, irq_num);

		// Try to find a thread to deliver to if we're setting the
		// pending bit. This might be NULL if the route is invalid
		// or the VCPU isn't attached.
		thread_t *target =
			set ? vgic_get_route_for_spi(vic, irq_num, false)
			    : NULL;

		vgic_change_irq_pending(vic, target, irq_num, false, source,
					set, is_msi);
		rcu_read_finish();
	} else {
		assert(is_msi);
		// Ignore attempts to message-signal non SPI IRQs
	}
}

void
vgic_gicd_change_irq_enable(vic_t *vic, irq_t irq_num, bool set)
{
	assert(vgic_irq_is_spi(irq_num));

	// Take the GICD lock and locate the source. We must do this
	// with the lock held to ensure that HW IRQs are correctly
	// enabled and disabled.
	spinlock_acquire(&vic->gicd_lock);
	rcu_read_start();
	virq_source_t *source = vgic_find_source(vic, NULL, irq_num);

	// Try to find a thread to deliver to if we're setting the enable bit.
	// This might be NULL if the route is invalid or the VCPU isn't
	// attached.
	thread_t *target = set ? vgic_get_route_for_spi(vic, irq_num, false)
			       : NULL;

	vgic_change_irq_enable(vic, target, irq_num, false, source, set);
	rcu_read_finish();

	spinlock_release(&vic->gicd_lock);
}

void
vgic_gicd_change_irq_active(vic_t *vic, irq_t irq_num, bool set)
{
	if (vgic_irq_is_spi(irq_num)) {
		vgic_change_irq_active(vic, NULL, irq_num, set);
	}
}

void
vgic_gicd_set_irq_group(vic_t *vic, irq_t irq_num, bool is_group_1)
{
	if (vgic_irq_is_spi(irq_num)) {
		_Atomic vgic_delivery_state_t *dstate =
			&vic->spi_states[irq_num - GIC_SPI_BASE];

		vgic_sync_group_change(vic, irq_num, dstate, is_group_1);
	}
}

void
vgic_gicd_set_irq_priority(vic_t *vic, irq_t irq_num, uint8_t priority)
{
	vgic_set_irq_priority(vic, thread_get_self(), irq_num, priority);
}

void
vgic_gicd_set_irq_config(vic_t *vic, irq_t irq_num, bool is_edge)
{
	assert(vgic_irq_is_spi(irq_num));
	assert(vic != NULL);

	// Take the GICD lock to ensure that concurrent writes don't make the
	// HW and dstate views of the config inconsistent
	spinlock_acquire(&vic->gicd_lock);

	bool effective_is_edge = is_edge;

	// If there's a source, update its config. Note that this may fail.
	rcu_read_start();
	virq_source_t *source = vgic_find_source(vic, NULL, irq_num);
	if (source != NULL) {
		irq_trigger_t	     mode = is_edge ? IRQ_TRIGGER_EDGE_RISING
						    : IRQ_TRIGGER_LEVEL_HIGH;
		irq_trigger_result_t new_mode = trigger_virq_set_mode_event(
			source->trigger, source, mode);
		if (new_mode.e != OK) {
			// Unable to set the requested mode; bail out
			rcu_read_finish();
			goto out;
		}
		effective_is_edge = new_mode.r == IRQ_TRIGGER_EDGE_RISING;
	}
	rcu_read_finish();

	// Update the delivery state.
	//
	// There is no need to synchronise: changing this configuration while
	// the interrupt is enabled and pending has an UNKNOWN effect on the
	// interrupt's pending state.
	_Atomic vgic_delivery_state_t *dstate =
		vgic_find_dstate(vic, NULL, irq_num);
	vgic_delivery_state_t change_dstate = vgic_delivery_state_default();
	vgic_delivery_state_set_cfg_is_edge(&change_dstate, true);
	if (effective_is_edge) {
		(void)vgic_delivery_state_atomic_union(dstate, change_dstate,
						       memory_order_relaxed);
	} else {
		// Also clear any leftover software level assertions.
		vgic_delivery_state_set_level_sw(&change_dstate, true);
		vgic_delivery_state_set_level_msg(&change_dstate, true);
		(void)vgic_delivery_state_atomic_difference(
			dstate, change_dstate, memory_order_relaxed);
	}

out:
	spinlock_release(&vic->gicd_lock);
}

static void
vgic_gicd_set_irq_hardware_router(vic_t *vic, irq_t irq_num,
				  vgic_delivery_state_t new_dstate,
				  const thread_t       *new_target,
				  index_t		route_index)
{
	virq_source_t *source = vgic_find_source(vic, NULL, irq_num);
	bool	       is_hw  = (source != NULL) &&
		     (source->trigger == VIRQ_TRIGGER_VGIC_FORWARDED_SPI);
	if (is_hw) {
		// Default to an invalid physical route
		GICD_IROUTER_t physical_router = GICD_IROUTER_default();
		GICD_IROUTER_set_IRM(&physical_router, false);
		GICD_IROUTER_set_Aff0(&physical_router, 0xff);
		GICD_IROUTER_set_Aff1(&physical_router, 0xff);
		GICD_IROUTER_set_Aff2(&physical_router, 0xff);
		GICD_IROUTER_set_Aff3(&physical_router, 0xff);

		// Try to set the physical route based on the virtual target
#if VGIC_HAS_1N && GICV3_HAS_1N
		if (vgic_delivery_state_get_route_1n(&new_dstate)) {
			GICD_IROUTER_set_IRM(&physical_router, true);
		} else
#endif
			if (new_target != NULL) {
			physical_router = new_target->vgic_irouter;
		} else {
			// No valid target
		}

		// Set the chosen physical route
		VGIC_TRACE(ROUTE, vic, NULL, "route {:d}: virt {:d} phys {:#x}",
			   irq_num, route_index,
			   GICD_IROUTER_raw(physical_router));
		irq_t irq = hwirq_from_virq_source(source)->irq;
		(void)gicv3_spi_set_route(irq, physical_router);

#if GICV3_HAS_GICD_ICLAR
		if (GICD_IROUTER_get_IRM(&physical_router)) {
			// Set the HW IRQ's 1-of-N routing classes.
			(void)gicv3_spi_set_classes(
				irq,
				!vgic_delivery_state_get_nclass0(&new_dstate),
				vgic_delivery_state_get_class1(&new_dstate));
		}
#endif
	} else {
		VGIC_TRACE(ROUTE, vic, NULL, "route {:d}: virt {:d} phys N/A",
			   irq_num, route_index);
	}
#if !(VGIC_HAS_1N && GICV3_HAS_1N) && !GICV3_HAS_GICD_ICLAR
	(void)new_dstate;
#endif
}

void
vgic_gicd_set_irq_router(vic_t *vic, irq_t irq_num, uint8_t aff0, uint8_t aff1,
			 uint8_t aff2, uint8_t aff3, bool is_1n)
{
	assert(vgic_irq_is_spi(irq_num));
	_Atomic vgic_delivery_state_t *dstate =
		vgic_find_dstate(vic, NULL, irq_num);
	assert(dstate != NULL);

	// Find the new target index
	index_result_t cpu_r =
		vgic_get_index_for_mpidr(vic, aff0, aff1, aff2, aff3);
	index_t route_index;
	if (cpu_r.e == OK) {
		assert(cpu_r.r < vic->gicr_count);
		route_index = cpu_r.r;
	} else {
		// Use an out-of-range value to indicate an invalid route.
		route_index = PLATFORM_MAX_CORES;
	}

	// Take the GICD lock to ensure that concurrent writes don't make the
	// HW, VIRQ source and GICD register views of the route inconsistent
	spinlock_acquire(&vic->gicd_lock);

	// Update the route in the delivery state
	vgic_delivery_state_t old_dstate = atomic_load_relaxed(dstate);
	vgic_delivery_state_t new_dstate;
	do {
		new_dstate = old_dstate;

		vgic_delivery_state_set_route(&new_dstate, route_index);
#if VGIC_HAS_1N
		vgic_delivery_state_set_route_1n(&new_dstate, is_1n);
#else
		(void)is_1n;
#endif

		// We might need to reroute a listed IRQ, so send a sync.
		if (vgic_delivery_state_get_listed(&old_dstate)) {
			vgic_delivery_state_set_need_sync(&new_dstate, true);
		}
	} while (!atomic_compare_exchange_strong_explicit(
		dstate, &old_dstate, new_dstate, memory_order_relaxed,
		memory_order_relaxed));

	// Find the new target.
	rcu_read_start();
	thread_t *new_target =
		(route_index < vic->gicr_count)
			? atomic_load_consume(&vic->gicr_vcpus[route_index])
			: NULL;

	if (vgic_delivery_state_get_listed(&old_dstate)) {
		// To guarantee that the route change will take effect in finite
		// time, sync all VCPUs that might have it listed.
		vgic_sync_all(vic, false);
	} else if (vgic_delivery_state_get_enabled(&old_dstate) &&
		   vgic_delivery_state_is_pending(&old_dstate)) {
		// Retry delivery, in case it previously did not select a LR
		// only because the priority was too low.
		(void)vgic_deliver(irq_num, vic, new_target, NULL, dstate,
				   vgic_delivery_state_default(),
				   vgic_irq_is_private(irq_num));
	} else {
		// Unlisted and not deliverable; nothing to do.
	}

	// For hardware sourced IRQs, pass the change through to the hardware.
	vgic_gicd_set_irq_hardware_router(vic, irq_num, new_dstate, new_target,
					  route_index);

	spinlock_release(&vic->gicd_lock);
	rcu_read_finish();
}

#if GICV3_HAS_GICD_ICLAR
void
vgic_gicd_set_irq_classes(vic_t *vic, irq_t irq_num, bool class0, bool class1)
{
	assert(vgic_irq_is_spi(irq_num));
	assert(vic != NULL);

	// Take the GICD lock to ensure that concurrent writes don't make the
	// HW and dstate views of the config inconsistent
	spinlock_acquire(&vic->gicd_lock);

	// If there's a source, update its config. Note that this may fail, and
	// it will have no effect if the IRQ is not currently 1-of-N routed.
	rcu_read_start();
	virq_source_t *source = vgic_find_source(vic, NULL, irq_num);
	if ((source != NULL) &&
	    (source->trigger == VIRQ_TRIGGER_VGIC_FORWARDED_SPI)) {
		hwirq_t *hwirq = hwirq_from_virq_source(source);
		error_t err = gicv3_spi_set_classes(hwirq->irq, class0, class1);
		if (err != OK) {
			rcu_read_finish();
			goto out;
		}
	}
	rcu_read_finish();

	// Update the delivery state.
	//
	// There is no need to synchronise: changing this configuration while
	// the interrupt is enabled and pending has an UNKNOWN effect on the
	// interrupt's pending state.
	_Atomic vgic_delivery_state_t *dstate =
		vgic_find_dstate(vic, NULL, irq_num);
	vgic_delivery_state_t old_dstate = atomic_load_relaxed(dstate);
	vgic_delivery_state_t new_dstate;
	do {
		new_dstate = old_dstate;
		vgic_delivery_state_set_nclass0(&new_dstate, !class0);
		vgic_delivery_state_set_class1(&new_dstate, class1);
	} while (!atomic_compare_exchange_weak_explicit(
		dstate, &old_dstate, new_dstate, memory_order_relaxed,
		memory_order_relaxed));

out:
	spinlock_release(&vic->gicd_lock);
}
#endif

// GICR
thread_t *
vgic_get_thread_by_gicr_index(vic_t *vic, index_t gicr_num)
{
	assert(gicr_num < vic->gicr_count);
	return atomic_load_consume(&vic->gicr_vcpus[gicr_num]);
}

#if VGIC_HAS_LPI
// Copy part or all of an LPI config or pending table from VM memory.
static void
vgic_gicr_copy_in(addrspace_t *addrspace, uint8_t *hyp_table,
		  size_t hyp_table_size, vmaddr_t vm_table_ipa, size_t offset,
		  size_t vm_table_size)
{
	error_t err = OK;

	if (util_add_overflows((uintptr_t)hyp_table, offset) ||
	    util_add_overflows(vm_table_ipa, offset)) {
		err = ERROR_ADDR_OVERFLOW;
		goto out;
	}

	if ((offset >= hyp_table_size) || (offset >= vm_table_size)) {
		err = ERROR_ADDR_UNDERFLOW;
		goto out;
	}

	err = useraccess_copy_from_guest_ipa(addrspace, hyp_table + offset,
					     hyp_table_size - offset,
					     vm_table_ipa + offset,
					     vm_table_size - offset, false,
					     false)
		      .e;

out:
	if (err != OK) {
		// Copy failed.
		//
		// Note that GICv4.1 deprecates implementation of SError
		// generation in the GICR & CPU interface (as opposed to the
		// ITS), and recent CPUs don't implement it. So there is no way
		// to report this to the VM. We just log it and continue.
		TRACE_AND_LOG(ERROR, WARN,
			      "vgicr: LPI table copy-in failed: {:d}",
			      (register_t)err);
	}
}

static bool
vgic_gicr_copy_pendbase(vic_t *vic, count_t idbits, thread_t *gicr_vcpu)
{
	assert(vic != NULL);
	assert(gicr_vcpu != NULL);

	GICR_PENDBASER_t pendbaser =
		atomic_load_relaxed(&gicr_vcpu->vgic_gicr_rd_pendbaser);
	bool   ptz = GICR_PENDBASER_get_PTZ(&pendbaser);
	size_t pending_table_size =
		BITMAP_NUM_WORDS(util_bit(vic->gicd_idbits)) *
		sizeof(register_t);
	const size_t pending_table_reserved =
		BITMAP_NUM_WORDS(GIC_LPI_BASE) * sizeof(register_t);

	assert(gicr_vcpu->vgic_vlpi_pending_table != NULL);
	assert(pending_table_size > pending_table_reserved);

	if (ptz) {
		errno_t err_mem = memset_s(gicr_vcpu->vgic_vlpi_pending_table,
					   pending_table_size, 0,
					   pending_table_size);
		if (err_mem != 0) {
			panic("Error in memset_s operation!");
		}
	} else {
		// Zero the reserved part of the pending table
		errno_t err_mem = memset_s(gicr_vcpu->vgic_vlpi_pending_table,
					   pending_table_reserved, 0,
					   pending_table_reserved);
		if (err_mem != 0) {
			panic("Error in memset_s operation!");
		}

		// Look up the physical address of the IPA range specified in
		// the GICR_PENDBASER, and copy it into the pending table. If
		// the lookup fails, or the permissions are wrong, copy zeros.
		vmaddr_t base = GICR_PENDBASER_get_PA(&pendbaser);
		size_t	 vm_table_size =
			BITMAP_NUM_WORDS(util_bit(idbits)) * sizeof(register_t);
		assert(vm_table_size <= pending_table_size);

		vgic_gicr_copy_in(gicr_vcpu->addrspace,
				  gicr_vcpu->vgic_vlpi_pending_table,
				  pending_table_size, base,
				  pending_table_reserved, vm_table_size);

		// Zero the remainder of the pending table
		if (vm_table_size < pending_table_size) {
			err_mem = memset_s(gicr_vcpu->vgic_vlpi_pending_table +
						   vm_table_size,
					   pending_table_size - vm_table_size,
					   0,
					   pending_table_size - vm_table_size);
			if (err_mem != 0) {
				panic("Error in memset_s operation!");
			}
		}
	}
	return ptz;
}

static void
vgic_gicr_copy_propbase_all(vic_t *vic, thread_t *gicr_vcpu,
			    bool zero_remainder)
{
	assert(vic != NULL);

	GICR_PROPBASER_t propbaser =
		atomic_load_relaxed(&vic->gicr_rd_propbaser);
	size_t config_table_size = util_bit(vic->gicd_idbits) - GIC_LPI_BASE;

	count_t	 idbits = util_min(GICR_PROPBASER_get_IDbits(&propbaser) + 1U,
				   vic->gicd_idbits);
	vmaddr_t base	= GICR_PROPBASER_get_PA(&propbaser);
	size_t	 vm_table_size = (util_bit(idbits) >= GIC_LPI_BASE)
					 ? (util_bit(idbits) - GIC_LPI_BASE)
					 : 0U;
	assert(vm_table_size <= config_table_size);

	vgic_gicr_copy_in(gicr_vcpu->addrspace, vic->vlpi_config_table,
			  config_table_size, base, 0U, vm_table_size);

	// Zero the remainder of the pending table
	if (zero_remainder && (vm_table_size < config_table_size)) {
		errno_t err_mem =
			memset_s(vic->vlpi_config_table + vm_table_size,
				 config_table_size - vm_table_size, 0,
				 config_table_size - vm_table_size);
		if (err_mem != 0) {
			panic("Error in memset_s operation!");
		}
	}
}

void
vgic_gicr_copy_propbase_one(vic_t *vic, thread_t *gicr_vcpu, irq_t vlpi)
{
	GICR_PROPBASER_t propbaser =
		atomic_load_relaxed(&vic->gicr_rd_propbaser);
	size_t config_table_size = util_bit(vic->gicd_idbits) - GIC_LPI_BASE;

	count_t idbits = util_min(GICR_PROPBASER_get_IDbits(&propbaser) + 1U,
				  vic->gicd_idbits);
	// Note that we only ever read these mappings (as writing back to them
	// is strictly optional in the spec) so we don't require write access.
	vmaddr_t base = GICR_PROPBASER_get_PA(&propbaser);

	// Ignore requests for out-of-range vLPI numbers
	if ((vlpi >= GIC_LPI_BASE) && (vlpi < util_bit(idbits))) {
		// Copy in a single byte
		vgic_gicr_copy_in(gicr_vcpu->addrspace, vic->vlpi_config_table,
				  config_table_size, base,
				  ((size_t)vlpi - (size_t)GIC_LPI_BASE),
				  ((size_t)vlpi - (size_t)GIC_LPI_BASE + 1U));
	}
}

#if GICV3_HAS_VLPI_V4_1
static void
vgic_update_vsgi(thread_t *gicr_vcpu, irq_t irq_num)
{
	// Note: we don't check whether vSGI delivery is enabled here; that is
	// only done when sending an SGI.
	_Atomic vgic_delivery_state_t *dstate =
		&gicr_vcpu->vgic_private_states[irq_num];
	vgic_delivery_state_t new_dstate = atomic_load_relaxed(dstate);

	// Note: as per the spec, this is a no-op if the vPE is not mapped.
	// The gicv3 driver may ignore the call in that case.
	(void)gicv3_its_vsgi_config(
		gicr_vcpu, irq_num,
		vgic_delivery_state_get_enabled(&new_dstate),
		vgic_delivery_state_get_group1(&new_dstate),
		vgic_delivery_state_get_priority(&new_dstate));
}

static void
vgic_setup_vcpu_vsgis(thread_t *vcpu)
{
	for (virq_t sgi = GIC_SGI_BASE; sgi < GIC_SGI_BASE + GIC_SGI_NUM;
	     sgi++) {
		vgic_update_vsgi(vcpu, sgi);
	}

	count_result_t sync_r = gicv3_its_vsgi_sync(vcpu);
	assert(sync_r.e == OK);
	atomic_store_release(&vcpu->vgic_vsgi_setup_seq, sync_r.r);
}

error_t
vgic_vsgi_assert(thread_t *gicr_vcpu, irq_t irq_num)
{
	error_t err;

	count_t setup_seq =
		atomic_load_acquire(&gicr_vcpu->vgic_vsgi_setup_seq);

	if (setup_seq == ~(count_t)0U) {
		// VSGI setup not queued yet
		err = ERROR_DENIED;
		goto out;
	}

	if (compiler_unexpected(setup_seq != 0U)) {
		bool_result_t complete_r =
			gicv3_its_vsgi_is_complete(setup_seq);
		assert(complete_r.e == OK);
		if (!complete_r.r) {
			// VSGI setup queued but VSYNC not complete yet
			err = ERROR_BUSY;
			goto out;
		}
		atomic_store_release(&gicr_vcpu->vgic_vsgi_setup_seq, 0U);
	}

	VGIC_TRACE(VIRQ_CHANGED, gicr_vcpu->vgic_vic, gicr_vcpu,
		   "sgi {:d}: send vsgi", irq_num);
	err = gicv3_its_vsgi_assert(gicr_vcpu, irq_num);

out:
	return err;
}
#endif

static error_t
vgic_gicr_enable_lpis(vic_t *vic, thread_t *gicr_vcpu)
{
	assert(vic != NULL);
	assert(vgic_has_lpis(vic));
	assert(vic->vlpi_config_table != NULL);
	assert(gicr_vcpu != NULL);
	assert(gicr_vcpu->vgic_vlpi_pending_table != NULL);

	GICR_PROPBASER_t propbaser =
		atomic_load_relaxed(&vic->gicr_rd_propbaser);
	count_t idbits = util_min(GICR_PROPBASER_get_IDbits(&propbaser) + 1U,
				  vic->gicd_idbits);

	// If this is the first VCPU to enable LPIs, we need to copy the
	// LPI configurations from the virtual GICR_PROPBASER. This is not
	// done for subsequent enables; LPI configuration changes must raise
	// explicit invalidates after that point.
	spinlock_acquire(&vic->gicd_lock);
	if (!vic->vlpi_config_valid) {
		vgic_gicr_copy_propbase_all(vic, gicr_vcpu, true);
		vic->vlpi_config_valid = true;
	}
	spinlock_release(&vic->gicd_lock);

	// If the virtual GICR_PENDBASER has the PTZ bit clear when LPIs are
	// enabled, we need to copy the VCPU's VLPI pending states from the
	// virtual GICR_PENDBASER. Otherwise we just zero the VLPI pending
	// states and ignore the GICR_PENDBASER PA entirely.
	//
	// Note that the spec does not require us to ever write back to the
	// pending table.
	bool pending_zeroed = vgic_gicr_copy_pendbase(vic, idbits, gicr_vcpu);

	// Call the ITS driver to map the VCPU into the VPE table.
	paddr_t config_table_phys = partition_virt_to_phys(
		vic->header.partition, (uintptr_t)vic->vlpi_config_table);
	assert(config_table_phys != PADDR_INVALID);
	size_t	config_table_size  = util_bit(vic->gicd_idbits) - GIC_LPI_BASE;
	paddr_t pending_table_phys = partition_virt_to_phys(
		gicr_vcpu->header.partition,
		(uintptr_t)gicr_vcpu->vgic_vlpi_pending_table);
	assert(pending_table_phys != PADDR_INVALID);
	size_t pending_table_size =
		BITMAP_NUM_WORDS(util_bit(vic->gicd_idbits)) *
		sizeof(register_t);
	error_t err = gicv3_its_vpe_map(gicr_vcpu, vic->gicd_idbits,
					config_table_phys, config_table_size,
					pending_table_phys, pending_table_size,
					pending_zeroed);

#if GICV3_HAS_VLPI_V4_1
	if (err == OK) {
		// Tell the ITS about the vPE's vSGI configuration.
		spinlock_acquire(&vic->gicd_lock);
		vgic_setup_vcpu_vsgis(gicr_vcpu);
		spinlock_release(&vic->gicd_lock);
	}
#endif

	if (gicr_vcpu == thread_get_self()) {
		preempt_disable();
		vgic_vpe_schedule_current();
		preempt_enable();
	}

	return err;
}
#endif // VGIC_HAS_LPI

void
vgic_gicr_rd_set_control(vic_t *vic, thread_t *gicr_vcpu, GICR_CTLR_t ctlr)
{
#if VGIC_HAS_LPI
	bool enable_lpis = GICR_CTLR_get_Enable_LPIs(&ctlr) &&
			   vgic_has_lpis(vic);

	if (enable_lpis) {
		GICR_CTLR_t ctlr_enable_lpis = GICR_CTLR_default();
		GICR_CTLR_set_Enable_LPIs(&ctlr_enable_lpis, true);
		GICR_CTLR_t old_ctlr = GICR_CTLR_atomic_union(
			&gicr_vcpu->vgic_gicr_rd_ctlr, ctlr_enable_lpis,
			memory_order_acquire);
		bool old_enable_lpis = GICR_CTLR_get_Enable_LPIs(&old_ctlr);

		if (!old_enable_lpis) {
			error_t err = vgic_gicr_enable_lpis(vic, gicr_vcpu);
			if (err != OK) {
				// LPI enable failed; clear the enable bit.
				TRACE_AND_LOG(ERROR, WARN,
					      "vgicr: LPI enable failed: {:d}",
					      (register_t)err);
				(void)GICR_CTLR_atomic_difference(
					&gicr_vcpu->vgic_gicr_rd_ctlr,
					ctlr_enable_lpis, memory_order_release);
			}
		}
	}
#else
	(void)vic;
	(void)gicr_vcpu;
	(void)ctlr;
#endif
}

GICR_CTLR_t
vgic_gicr_rd_get_control(vic_t *vic, thread_t *gicr_vcpu)
{
	(void)vic;

#if VGIC_HAS_LPI
	GICR_CTLR_t ctlr = atomic_load_relaxed(&gicr_vcpu->vgic_gicr_rd_ctlr);
#if GICV3_HAS_VLPI_V4_1
	bool_result_t disabled_r =
		gicv3_its_vsgi_is_complete(gicr_vcpu->vgic_vsgi_disable_seq);
	if ((disabled_r.e == OK) && !disabled_r.r) {
		GICR_CTLR_set_RWP(&ctlr, true);
	}
#endif
#else
	(void)gicr_vcpu;
	GICR_CTLR_t ctlr = GICR_CTLR_default();
#endif

	return ctlr;
}

void
vgic_gicr_rd_set_statusr(thread_t *gicr_vcpu, GICR_STATUSR_t statusr, bool set)
{
	if (set) {
		(void)GICR_STATUSR_atomic_union(
			&gicr_vcpu->vgic_gicr_rd_statusr, statusr,
			memory_order_relaxed);
	} else {
		(void)GICR_STATUSR_atomic_difference(
			&gicr_vcpu->vgic_gicr_rd_statusr, statusr,
			memory_order_relaxed);
	}
}

#if VGIC_HAS_LPI
void
vgic_gicr_rd_set_propbase(vic_t *vic, GICR_PROPBASER_t propbase)
{
	GICR_PROPBASER_t new_propbase = GICR_PROPBASER_default();

	// We implement the cache and shareability fields as read-only to
	// reflect the fact that the hypervisor always accesses the table
	// through its own shared cacheable mapping.
	GICR_PROPBASER_set_OuterCache(&new_propbase, 0U);
	GICR_PROPBASER_set_InnerCache(&new_propbase, 7U);
	GICR_PROPBASER_set_Shareability(&new_propbase, 1U);

	// Use the physical address and size provided by the VM.
	GICR_PROPBASER_copy_PA(&new_propbase, &propbase);
	GICR_PROPBASER_copy_IDbits(&new_propbase, &propbase);

	// There is no need to synchronise or update anything else here. This
	// value is only used when EnableLPIs changes to 1 or an explicit
	// invalidate is processed.
	atomic_store_relaxed(&vic->gicr_rd_propbaser, new_propbase);
}

void
vgic_gicr_rd_set_pendbase(vic_t *vic, thread_t *gicr_vcpu,
			  GICR_PENDBASER_t pendbase)
{
	(void)vic;

	GICR_PENDBASER_t new_pendbase = GICR_PENDBASER_default();

	// We implement the cache and shareability fields as read-only to
	// reflect the fact that the hypervisor always accesses the table
	// through its own shared cacheable mapping.
	GICR_PENDBASER_set_OuterCache(&new_pendbase, 0U);
	GICR_PENDBASER_set_InnerCache(&new_pendbase, 7U);
	GICR_PENDBASER_set_Shareability(&new_pendbase, 1U);

	// Use the physical address provided by the VM.
	GICR_PENDBASER_set_PA(&new_pendbase, GICR_PENDBASER_get_PA(&pendbase));

	// Copy the PTZ bit. When the VM sets EnableLPIs to 1, this will
	// determine the cache update behaviour and the VMAPP command's PTZ bit.
	// However, the read trap will always zero this.
	GICR_PENDBASER_set_PTZ(&new_pendbase,
			       GICR_PENDBASER_get_PTZ(&pendbase));

	// There is no need to synchronise or update anything else here. This
	// value is only used when EnableLPIs changes to 1 or an explicit
	// invalidate is processed.
	atomic_store_relaxed(&gicr_vcpu->vgic_gicr_rd_pendbaser, new_pendbase);
}

void
vgic_gicr_rd_invlpi(vic_t *vic, thread_t *gicr_vcpu, virq_t vlpi_num)
{
	if (vic->vlpi_config_valid) {
		vgic_gicr_copy_propbase_one(vic, gicr_vcpu, vlpi_num);
		gicv3_vlpi_inv_by_id(gicr_vcpu, vlpi_num);
	}
}

void
vgic_gicr_rd_invall(vic_t *vic, thread_t *gicr_vcpu)
{
	if (vic->vlpi_config_valid) {
		vgic_gicr_copy_propbase_all(vic, gicr_vcpu, false);
		gicv3_vlpi_inv_all(gicr_vcpu);
	}
}

bool
vgic_gicr_get_inv_pending(vic_t *vic, thread_t *gicr_vcpu)
{
	return vic->vlpi_config_valid && gicv3_vlpi_inv_pending(gicr_vcpu);
}
#endif

void
vgic_gicr_sgi_change_sgi_ppi_pending(vic_t *vic, thread_t *gicr_vcpu,
				     irq_t irq_num, bool set)
{
	assert(vgic_irq_is_private(irq_num));

#if GICV3_HAS_VLPI_V4_1 && VGIC_HAS_LPI
	if (!vgic_irq_is_ppi(irq_num) && vic->vsgis_enabled) {
		if (set) {
			if (vgic_vsgi_assert(gicr_vcpu, irq_num) == OK) {
				// Delivered by ITS
				goto out;
			}
			// Need to deliver in software instead; fall through
		} else {
			(void)gicv3_its_vsgi_clear(gicr_vcpu, irq_num);
			// Might be pending in software too; fall through
		}
	}
#endif

	rcu_read_start();
	virq_source_t *source = vgic_find_source(vic, gicr_vcpu, irq_num);
	vgic_change_irq_pending(vic, gicr_vcpu, irq_num, true, source, set,
				false);
	rcu_read_finish();

#if GICV3_HAS_VLPI_V4_1 && VGIC_HAS_LPI
out:
	return;
#endif
}

void
vgic_gicr_sgi_change_sgi_ppi_enable(vic_t *vic, thread_t *gicr_vcpu,
				    irq_t irq_num, bool set)
{
	assert(vgic_irq_is_private(irq_num));

#if GICV3_HAS_VLPI_V4_1 && VGIC_HAS_LPI
	// Take the distributor lock for SGIs to ensure that vSGI config changes
	// by different CPUs don't end up out of order in the ITS.
	spinlock_acquire(&vic->gicd_lock);
#else
	preempt_disable();
#endif

	rcu_read_start();
	virq_source_t *source = vgic_find_source(vic, gicr_vcpu, irq_num);

	assert((source == NULL) ||
	       (source->trigger != VIRQ_TRIGGER_VGIC_FORWARDED_SPI));

	vgic_change_irq_enable(vic, gicr_vcpu, irq_num, true, source, set);

	rcu_read_finish();

#if GICV3_HAS_VLPI_V4_1 && VGIC_HAS_LPI
	if (!vgic_irq_is_ppi(irq_num) && vgic_has_lpis(vic)) {
		vgic_update_vsgi(gicr_vcpu, irq_num);
		if (!set) {
			count_result_t seq_r = gicv3_its_vsgi_sync(gicr_vcpu);
			if (seq_r.e == OK) {
				gicr_vcpu->vgic_vsgi_disable_seq = seq_r.r;
			}
		}
	}
	spinlock_release(&vic->gicd_lock);
#else
	preempt_enable();
#endif
}

void
vgic_gicr_sgi_change_sgi_ppi_active(vic_t *vic, thread_t *gicr_vcpu,
				    irq_t irq_num, bool set)
{
	assert(vgic_irq_is_private(irq_num));

	vgic_change_irq_active(vic, gicr_vcpu, irq_num, set);
}

void
vgic_gicr_sgi_set_sgi_ppi_group(vic_t *vic, thread_t *gicr_vcpu, irq_t irq_num,
				bool is_group_1)
{
	assert(vgic_irq_is_private(irq_num));

#if GICV3_HAS_VLPI_V4_1
	// Take the distributor lock for SGIs to ensure that two config changes
	// by different CPUs don't end up out of order in the ITS.
	spinlock_acquire(&vic->gicd_lock);
#endif

	_Atomic vgic_delivery_state_t *dstate =
		&gicr_vcpu->vgic_private_states[irq_num];

	vgic_sync_group_change(vic, irq_num, dstate, is_group_1);

#if GICV3_HAS_VLPI_V4_1 && VGIC_HAS_LPI
	if (!vgic_irq_is_ppi(irq_num) && vgic_has_lpis(vic)) {
		vgic_update_vsgi(gicr_vcpu, irq_num);
	}
	spinlock_release(&vic->gicd_lock);
#endif
}

void
vgic_gicr_sgi_set_sgi_ppi_priority(vic_t *vic, thread_t *gicr_vcpu,
				   irq_t irq_num, uint8_t priority)
{
	assert(vgic_irq_is_private(irq_num));

	spinlock_acquire(&vic->gicd_lock);

	vgic_set_irq_priority(vic, gicr_vcpu, irq_num, priority);

#if GICV3_HAS_VLPI_V4_1 && VGIC_HAS_LPI
	if (!vgic_irq_is_ppi(irq_num) && vgic_has_lpis(vic)) {
		vgic_update_vsgi(gicr_vcpu, irq_num);
	}
#endif

	spinlock_release(&vic->gicd_lock);
}

void
vgic_gicr_sgi_set_ppi_config(vic_t *vic, thread_t *gicr_vcpu, irq_t irq_num,
			     bool is_edge)
{
	assert(vgic_irq_is_ppi(irq_num));
	assert(vic != NULL);
	assert(gicr_vcpu != NULL);

	// Take the GICD lock to ensure that concurrent writes don't make the
	// dstate and GICR register views of the config inconsistent
	spinlock_acquire(&vic->gicd_lock);

	// Update the delivery state.
	//
	// There is no need to synchronise: changing this configuration while
	// the interrupt is enabled and pending has an UNKNOWN effect on the
	// interrupt's pending state.
	_Atomic vgic_delivery_state_t *dstate =
		vgic_find_dstate(vic, gicr_vcpu, irq_num);
	vgic_delivery_state_t change_dstate = vgic_delivery_state_default();
	vgic_delivery_state_set_cfg_is_edge(&change_dstate, true);
	if (is_edge) {
		(void)vgic_delivery_state_atomic_union(dstate, change_dstate,
						       memory_order_relaxed);
	} else {
		// Also clear any leftover software level assertions.
		vgic_delivery_state_set_level_sw(&change_dstate, true);
		vgic_delivery_state_set_level_msg(&change_dstate, true);
		(void)vgic_delivery_state_atomic_difference(
			dstate, change_dstate, memory_order_relaxed);
	}

	spinlock_release(&vic->gicd_lock);
}

error_t
vic_bind_shared(virq_source_t *source, vic_t *vic, virq_t virq,
		virq_trigger_t trigger)
{
	error_t ret;

	if (atomic_fetch_or_explicit(&source->vgic_is_bound, true,
				     memory_order_acquire)) {
		ret = ERROR_VIRQ_BOUND;
		goto out;
	}
	assert(atomic_load_relaxed(&source->vic) == NULL);

	if (vgic_get_irq_type(virq) != VGIC_IRQ_TYPE_SPI) {
		ret = ERROR_ARGUMENT_INVALID;
		goto out_release;
	}

	if ((virq - GIC_SPI_BASE) >= vic->sources_count) {
		ret = ERROR_ARGUMENT_INVALID;
		goto out_release;
	}

	_Atomic vgic_delivery_state_t *dstate =
		vgic_find_dstate(vic, NULL, virq);

	source->virq		= virq;
	source->trigger		= trigger;
	source->is_private	= false;
	source->vgic_gicr_index = CPU_INDEX_INVALID;

	rcu_read_start();
	virq_source_t *_Atomic *attach_ptr = &vic->sources[virq - GIC_SPI_BASE];
	virq_source_t	       *old_source = atomic_load_acquire(attach_ptr);
	do {
		// If there is already a source bound, we can't bind another.
		if (old_source != NULL) {
			ret = ERROR_BUSY;
			goto out_rcu_release;
		}

		// If the previous source for this VIRQ was a forwarded SPI,
		// we can't bind a new forwarded SPI until the old one has been
		// removed from the LRs and deactivated, to avoid any ambiguity
		// in the meanings of the hw_active and hw_deactivated bits in
		// the delivery state. In that case, ask the caller to try
		// again.
		if (trigger == VIRQ_TRIGGER_VGIC_FORWARDED_SPI) {
			vgic_delivery_state_t current_dstate =
				atomic_load_relaxed(dstate);
			if (vgic_delivery_state_get_hw_detached(
				    &current_dstate)) {
				assert(vgic_delivery_state_get_listed(
					&current_dstate));
				ret = ERROR_RETRY;
				goto out_rcu_release;
			}
		}

		ret = OK;
	} while (!atomic_compare_exchange_strong_explicit(
		attach_ptr, &old_source, source, memory_order_acq_rel,
		memory_order_acquire));

	if (ret == OK) {
		atomic_store_release(&source->vic, vic);
	}
out_rcu_release:
	rcu_read_finish();

out_release:
	if (ret != OK) {
		atomic_store_release(&source->vgic_is_bound, false);
	}

out:
	return ret;
}

static error_t
vic_bind_private(virq_source_t *source, vic_t *vic, thread_t *vcpu, virq_t virq,
		 virq_trigger_t trigger)
{
	error_t ret;

	if (vgic_get_irq_type(virq) != VGIC_IRQ_TYPE_PPI) {
		ret = ERROR_ARGUMENT_INVALID;
		goto out;
	}

	assert(vic != NULL);
	assert(atomic_load_relaxed(&vic->header.state) == OBJECT_STATE_ACTIVE);

	if (atomic_fetch_or_explicit(&source->vgic_is_bound, true,
				     memory_order_acquire)) {
		ret = ERROR_VIRQ_BOUND;
		goto out;
	}
	assert(atomic_load_relaxed(&source->vic) == NULL);

	source->virq		= virq;
	source->trigger		= trigger;
	source->is_private	= true;
	source->vgic_gicr_index = vcpu->vgic_gicr_index;

	spinlock_acquire(&vic->gicd_lock);
	if (atomic_load_relaxed(&vic->gicr_vcpus[vcpu->vgic_gicr_index]) !=
	    vcpu) {
		ret = ERROR_OBJECT_CONFIG;
		goto out_locked;
	}

	virq_source_t *old_source = NULL;
	if (!atomic_compare_exchange_strong_explicit(
		    &vcpu->vgic_sources[virq - GIC_PPI_BASE], &old_source,
		    source, memory_order_release, memory_order_relaxed)) {
		ret = ERROR_BUSY;
	} else {
		atomic_store_release(&source->vic, vic);
		ret = OK;
	}

out_locked:
	spinlock_release(&vic->gicd_lock);

	if (ret != OK) {
		atomic_store_release(&source->vgic_is_bound, false);
	}
out:
	return ret;
}

error_t
vic_bind_private_vcpu(virq_source_t *source, thread_t *vcpu, virq_t virq,
		      virq_trigger_t trigger)
{
	error_t ret;

	assert(source != NULL);
	assert(vcpu != NULL);

	vic_t *vic = vcpu->vgic_vic;
	if (vic == NULL) {
		ret = ERROR_ARGUMENT_INVALID;
	} else {
		ret = vic_bind_private(source, vic, vcpu, virq, trigger);
	}

	return ret;
}

error_t
vic_bind_private_index(virq_source_t *source, vic_t *vic, index_t index,
		       virq_t virq, virq_trigger_t trigger)
{
	error_t ret;

	assert(source != NULL);
	assert(vic != NULL);

	if (index >= vic->gicr_count) {
		ret = ERROR_ARGUMENT_INVALID;
	} else {
		rcu_read_start();

		thread_t *vcpu = atomic_load_consume(&vic->gicr_vcpus[index]);

		if (vcpu == NULL) {
			ret = ERROR_OBJECT_CONFIG;
		} else {
			ret = vic_bind_private(source, vic, vcpu, virq,
					       trigger);
		}

		rcu_read_finish();
	}

	return ret;
}

error_t
vic_bind_private_forward_private(virq_source_t *source, vic_t *vic,
				 thread_t *vcpu, virq_t virq)
{
	error_t ret;

	assert(source != NULL);
	assert(vic != NULL);
	assert(vcpu != NULL);

	if (vgic_get_irq_type(virq) != VGIC_IRQ_TYPE_PPI) {
		ret = ERROR_ARGUMENT_INVALID;
		goto out;
	}

	ret = vic_bind_private_vcpu(source, vcpu, virq,
				    VIRQ_TRIGGER_VIC_BASE_FORWARD_PRIVATE);
out:
	return ret;
}

void
vic_sync_private_forward_private(virq_source_t *source, vic_t *vic,
				 thread_t *vcpu, virq_t virq, irq_t pirq,
				 cpu_index_t pcpu)
{
	// Take the GICD lock to ensure that the vGIC's IRQ config does
	// not change while we are copying it to the hardware GIC
	spinlock_acquire(&vic->gicd_lock);

	_Atomic vgic_delivery_state_t *dstate =
		vgic_find_dstate(vic, vcpu, virq);
	assert(dstate != NULL);
	vgic_delivery_state_t current_dstate = atomic_load_relaxed(dstate);

	bool is_edge = vgic_delivery_state_get_cfg_is_edge(&current_dstate);
	irq_trigger_t mode = is_edge ? IRQ_TRIGGER_EDGE_RISING
				     : IRQ_TRIGGER_LEVEL_HIGH;

	irq_trigger_result_t new_mode = trigger_virq_set_mode_event(
		VIRQ_TRIGGER_VIC_BASE_FORWARD_PRIVATE, source, mode);
	if ((new_mode.e != OK) || (new_mode.r != mode)) {
		vgic_delivery_state_t cfg_is_edge =
			vgic_delivery_state_default();
		vgic_delivery_state_set_cfg_is_edge(&cfg_is_edge, true);
		// Mode change failed; the hardware config must be fixed to the
		// other mode. Flip the software mode.
		if (is_edge) {
			(void)vgic_delivery_state_atomic_intersection(
				dstate, cfg_is_edge, memory_order_relaxed);
		} else {
			(void)vgic_delivery_state_atomic_difference(
				dstate, cfg_is_edge, memory_order_relaxed);
		}
	}

	// Enable the HW IRQ if the virtual enable bit is set (unbound
	// HW IRQs are always disabled).
	if (vgic_delivery_state_get_enabled(&current_dstate)) {
		platform_irq_enable_percpu(pirq, pcpu);
	}

	spinlock_release(&vic->gicd_lock);
}

static bool
vic_do_unbind(virq_source_t *source, bool during_deactivate)
{
	bool complete = false;

	rcu_read_start();

	vic_t *vic = atomic_exchange_explicit(&source->vic, NULL,
					      memory_order_consume);
	if (vic == NULL) {
		// The VIRQ is not bound
		goto out;
	}

	// Try to find the current target VCPU. This may be inaccurate or NULL
	// for a shared IRQ, but must be correct for a private IRQ.
	thread_t *vcpu = vgic_find_target(vic, source);
	if (source->is_private && (vcpu == NULL)) {
		// The VIRQ has been concurrently unbound.
		goto out;
	}

	// Clear the level_src and hw_active bits in the delivery state.
	// The latter bit will implicitly detach and deactivate the physical
	// IRQ, if there is one.
	vgic_delivery_state_t clear_dstate = vgic_delivery_state_default();
	vgic_delivery_state_set_level_src(&clear_dstate, true);
	vgic_delivery_state_set_hw_active(&clear_dstate, true);

	_Atomic vgic_delivery_state_t *dstate =
		vgic_find_dstate(vic, vcpu, source->virq);
	if (!vgic_undeliver(vic, vcpu, dstate, source->virq, clear_dstate,
			    false)) {
		// The VIRQ is still listed somewhere.
		//
		// This should never happen during a deactivate, because there
		// are no VCPUs left to list it in.
		assert(!during_deactivate);

		// For HW sources this can delay both re-registration of the
		// VIRQ and delivery of the HW IRQ (after it is re-registered
		// elsewhere), so start a sync to ensure that delisting happens
		// soon.
		vgic_sync_all(vic, false);
	}

	// Remove the source from the IRQ source array. Note that this must
	// be ordered after the level_src bit is cleared in the undeliver, to
	// ensure that other threads don't see this NULL pointer while the
	// level_src or hw_active bits are still set.
	virq_source_t	       *registered_source = source;
	virq_source_t *_Atomic *registered_source_ptr =
		source->is_private
			? &vcpu->vgic_sources[source->virq - GIC_PPI_BASE]
			: &vic->sources[source->virq - GIC_SPI_BASE];
	if (!atomic_compare_exchange_strong_explicit(
		    registered_source_ptr, &registered_source, NULL,
		    memory_order_release, memory_order_relaxed)) {
		// Somebody else has already released the VIRQ
		goto out;
	}

	complete = true;
out:
	rcu_read_finish();
	return complete;
}

void
vic_unbind(virq_source_t *source)
{
	(void)vic_do_unbind(source, false);
}

void
vic_unbind_sync(virq_source_t *source)
{
	if (vic_do_unbind(source, false)) {
		// Ensure that any remote operations affecting the source object
		// and the unbound VIRQ have completed.
		rcu_sync();

		// Mark the source as no longer bound.
		atomic_store_release(&source->vgic_is_bound, false);
	}
}

static bool_result_t
virq_do_assert(virq_source_t *source, bool edge_only, bool is_hw)
{
	bool_result_t ret;

	// The source's VIC pointer and the target VCPU are RCU-protected.
	rcu_read_start();

	// We must have a VIC to deliver to. Note that we use load-acquire here
	// rather than the usual load-consume, to ensure that we only read the
	// other fields in the source after they have been set.
	vic_t *vic = atomic_load_acquire(&source->vic);
	if (compiler_unexpected(vic == NULL)) {
		ret = bool_result_error(ERROR_VIRQ_NOT_BOUND);
		goto out;
	}

	// Choose a target VCPU to deliver to.
#if VGIC_HAS_1N
	thread_t *vcpu = NULL;

	if (source->is_private) {
		vcpu = vgic_find_target(vic, source);
		if (vcpu == NULL) {
			// The VIRQ has been concurrently unbound.
			ret = bool_result_error(ERROR_VIRQ_NOT_BOUND);
			goto out;
		}
	} else {
		// A shared VIRQ might be 1-of-N, and vgic_find_target() will
		// return NULL in that case, so we can't use it.
		vcpu = vgic_get_route_for_spi(vic, source->virq, is_hw);
	}
#else  // !VGIC_HAS_1N
	thread_t *vcpu = vgic_find_target(vic, source);
	if (source->is_private && (vcpu == NULL)) {
		// The VIRQ has been concurrently unbound.
		ret = bool_result_error(ERROR_VIRQ_NOT_BOUND);
		goto out;
	}
#endif // VGIC_HAS_1N

	// Deliver the interrupt to the target
	_Atomic vgic_delivery_state_t *dstate =
		vgic_find_dstate(vic, vcpu, source->virq);
	vgic_delivery_state_t assert_dstate = vgic_delivery_state_default();
	vgic_delivery_state_set_edge(&assert_dstate, true);
	if (!edge_only) {
		vgic_delivery_state_set_level_src(&assert_dstate, true);
	}
	if (is_hw) {
		vgic_delivery_state_set_hw_active(&assert_dstate, true);
	}

	vgic_delivery_state_t old_dstate =
		vgic_deliver(source->virq, vic, vcpu, source, dstate,
			     assert_dstate, source->is_private);

	ret = bool_result_ok(vgic_delivery_state_get_cfg_is_edge(&old_dstate));
out:
	rcu_read_finish();

	return ret;
}

bool_result_t
virq_assert(virq_source_t *source, bool edge_only)
{
	return virq_do_assert(source, edge_only, false);
}

// Handle a hardware SPI that is forwarded as a VIRQ.
bool
vgic_handle_irq_received_forward_spi(hwirq_t *hwirq)
{
	assert(hwirq != NULL);
	assert(hwirq->vgic_spi_source.trigger ==
	       VIRQ_TRIGGER_VGIC_FORWARDED_SPI);

	bool deactivate = false;

	bool_result_t ret =
		virq_do_assert(&hwirq->vgic_spi_source, false, true);

	if (compiler_unexpected(ret.e != OK)) {
		// Delivery failed, so disable the HW IRQ.
		irq_disable_shared_nosync(hwirq);
		deactivate = true;
	}

	return deactivate;
}

static error_t
vgic_set_mpidr_mapping(vic_t *vic, MPIDR_EL1_t mask, count_t aff0_shift,
		       count_t aff1_shift, count_t aff2_shift,
		       count_t aff3_shift, bool mt)
{
	uint64_t      cpuindex_mask = 0U;
	const count_t shifts[4]	    = { aff0_shift, aff1_shift, aff2_shift,
					aff3_shift };
	const uint8_t masks[4]	    = { MPIDR_EL1_get_Aff0(&mask),
					MPIDR_EL1_get_Aff1(&mask),
					MPIDR_EL1_get_Aff2(&mask),
					MPIDR_EL1_get_Aff3(&mask) };
	error_t	      err;

	for (index_t i = 0U; i < 4U; i++) {
		// Since there are only 32 significant affinity bits, a shift of
		// more than 32 can't be useful, so don't allow it.
		if (shifts[i] >= 32U) {
			err = ERROR_ARGUMENT_INVALID;
			goto out;
		}

		// Collect the output bits, checking that there's no overlap.
		uint64_t field_mask = (uint64_t)masks[i] << shifts[i];
		if ((cpuindex_mask & field_mask) != 0U) {
			err = ERROR_ARGUMENT_INVALID;
			goto out;
		}
		cpuindex_mask |= field_mask;
	}

	// We don't allow sparse mappings, so check that the output bits are
	// contiguous and start from the least significant bit. This is true if
	// the mask is one less than a power of two.
	//
	// Also, the mask has to fit in cpu_index_t, and must not be able to
	// produce CPU_INDEX_INVALID, which currently limits it to 15 bits.
	if (!util_is_p2(cpuindex_mask + 1U) ||
	    (cpuindex_mask >= CPU_INDEX_INVALID)) {
		err = ERROR_ARGUMENT_INVALID;
		goto out;
	}

	// Note: we currently don't check that the mapping can assign unique
	// MPIDR values to all VCPUs. If it doesn't, the VM will probably fail
	// to boot or at least fail to start the VCPUs with duplicated values,
	// but the hypervisor itself will not fail.

	// Construct and set the mapping.
	vic->mpidr_mapping = (platform_mpidr_mapping_t){
		.aff_shift    = { shifts[0], shifts[1], shifts[2], shifts[3] },
		.aff_mask     = { masks[0], masks[1], masks[2], masks[3] },
		.multi_thread = mt,
		.uniprocessor = (cpuindex_mask == 0U),
	};
	err = OK;

out:
	return err;
}

error_t
hypercall_vgic_set_mpidr_mapping(cap_id_t vic_cap, uint64_t mask,
				 count_t aff0_shift, count_t aff1_shift,
				 count_t aff2_shift, count_t aff3_shift,
				 bool mt)
{
	error_t	      err;
	cspace_t     *cspace = cspace_get_self();
	object_type_t type;

	object_ptr_result_t o = cspace_lookup_object_any(
		cspace, vic_cap, CAP_RIGHTS_GENERIC_OBJECT_ACTIVATE, &type);
	if (compiler_unexpected(o.e != OK)) {
		err = o.e;
		goto out_released;
	}
	if (type != OBJECT_TYPE_VIC) {
		err = ERROR_CSPACE_WRONG_OBJECT_TYPE;
		goto out_unlocked;
	}
	vic_t *vic = o.r.vic;

	spinlock_acquire(&vic->header.lock);
	if (atomic_load_relaxed(&vic->header.state) == OBJECT_STATE_INIT) {
		err = vgic_set_mpidr_mapping(vic, MPIDR_EL1_cast(mask),
					     aff0_shift, aff1_shift, aff2_shift,
					     aff3_shift, mt);
	} else {
		err = ERROR_OBJECT_STATE;
	}
	spinlock_release(&vic->header.lock);

out_unlocked:
	object_put(type, o.r);
out_released:

	return err;
}

```

`hyp/vm/vgic/src/sysregs.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>

#include <hypregisters.h>

#include <compiler.h>
#include <panic.h>
#include <preempt.h>
#include <scheduler.h>
#include <thread.h>
#include <util.h>
#include <vcpu.h>

#include <asm/system_registers.h>

#include "event_handlers.h"
#include "gich_lrs.h"
#include "internal.h"

#if defined(ARCH_ARM_FEAT_FGT) && ARCH_ARM_FEAT_FGT
#include <arm_fgt.h>
#endif

static vcpu_trap_result_t
vgic_handle_vcpu_trap_sysreg_write_no_fgt(ESR_EL2_ISS_MSR_MRS_t iss, vic_t *vic,
					  register_t val)
{
	vcpu_trap_result_t ret = VCPU_TRAP_RESULT_EMULATED;

#if defined(ARCH_ARM_FEAT_FGT) && ARCH_ARM_FEAT_FGT
	if (arm_fgt_is_allowed()) {
		// These registers should not be trapped when FGT is available
		ret = VCPU_TRAP_RESULT_UNHANDLED;
		goto out;
	}
#endif

	switch (ESR_EL2_ISS_MSR_MRS_raw(iss)) {
	// Trapped by TALL[01] which are set to trap ICC_IGRPEN[01]_EL1
	case ISS_MRS_MSR_ICC_EOIR0_EL1: {
		// Drop the highest active priority (which we are allowed to
		// assume is the priority of the specified IRQ)
		gicv3_ich_ap0r_clear_highest();
		// Deactivate the interrupt, if EOImode is 0
		ICH_VMCR_EL2_t vmcr = register_ICH_VMCR_EL2_read();
		if (!ICH_VMCR_EL2_get_VEOIM(&vmcr)) {
			vgic_icc_irq_deactivate(
				vic, ICC_EOIR_EL1_get_INTID(
					     &ICC_EOIR_EL1_cast(val)));
		}
		break;
	}

	case ISS_MRS_MSR_ICC_BPR0_EL1: {
		preempt_disable();
		ICH_VMCR_EL2_t vmcr = register_ICH_VMCR_EL2_read();
		ICH_VMCR_EL2_set_VBPR0(&vmcr, (uint8_t)val);
		register_ICH_VMCR_EL2_write(vmcr);
		preempt_enable();
		break;
	}
	case ISS_MRS_MSR_ICC_AP0R0_EL1:
#if CPU_GICH_APR_COUNT >= 1U
		register_ICH_AP0R0_EL2_write((uint32_t)val);
#endif
		break;

	case ISS_MRS_MSR_ICC_AP0R1_EL1:
#if CPU_GICH_APR_COUNT >= 2U
		register_ICH_AP0R1_EL2_write((uint32_t)val);
#endif
		break;

	case ISS_MRS_MSR_ICC_AP0R2_EL1:
#if CPU_GICH_APR_COUNT >= 4U
		register_ICH_AP0R2_EL2_write((uint32_t)val);
#endif
		break;

	case ISS_MRS_MSR_ICC_AP0R3_EL1:
#if CPU_GICH_APR_COUNT >= 4U
		register_ICH_AP0R3_EL2_write((uint32_t)val);
#endif
		break;

	case ISS_MRS_MSR_ICC_EOIR1_EL1: {
		// Drop the highest active priority (which we are allowed to
		// assume is the priority of the specified IRQ)
		gicv3_ich_ap1r_clear_highest();
		// Deactivate the interrupt, if EOImode is 0
		ICH_VMCR_EL2_t vmcr = register_ICH_VMCR_EL2_read();
		if (!ICH_VMCR_EL2_get_VEOIM(&vmcr)) {
			vgic_icc_irq_deactivate(
				vic, ICC_EOIR_EL1_get_INTID(
					     &ICC_EOIR_EL1_cast(val)));
		}
		break;
	}

	case ISS_MRS_MSR_ICC_BPR1_EL1: {
		preempt_disable();
		ICH_VMCR_EL2_t vmcr = register_ICH_VMCR_EL2_read();
		ICH_VMCR_EL2_set_VBPR1(&vmcr, (uint8_t)val);
		register_ICH_VMCR_EL2_write(vmcr);
		preempt_enable();
		break;
	}

	case ISS_MRS_MSR_ICC_AP1R0_EL1:
#if CPU_GICH_APR_COUNT >= 1U
		register_ICH_AP1R0_EL2_write((uint32_t)val);
#endif
		break;

	case ISS_MRS_MSR_ICC_AP1R1_EL1:
#if CPU_GICH_APR_COUNT >= 2U
		register_ICH_AP1R1_EL2_write((uint32_t)val);
#endif
		break;

	case ISS_MRS_MSR_ICC_AP1R2_EL1:
#if CPU_GICH_APR_COUNT >= 4U
		register_ICH_AP1R2_EL2_write((uint32_t)val);
#endif
		break;

	case ISS_MRS_MSR_ICC_AP1R3_EL1:
#if CPU_GICH_APR_COUNT >= 4U
		register_ICH_AP1R3_EL2_write((uint32_t)val);
#endif
		break;
	default:
		ret = VCPU_TRAP_RESULT_UNHANDLED;
		break;
	}

#if defined(ARCH_ARM_FEAT_FGT) && ARCH_ARM_FEAT_FGT
out:
#endif
	return ret;
}

vcpu_trap_result_t
vgic_handle_vcpu_trap_sysreg_write(ESR_EL2_ISS_MSR_MRS_t iss)
{
	vcpu_trap_result_t ret	  = VCPU_TRAP_RESULT_EMULATED;
	thread_t	  *thread = thread_get_self();
	vic_t		  *vic	  = thread->vgic_vic;
	if (vic == NULL) {
		ret = VCPU_TRAP_RESULT_UNHANDLED;
		goto out;
	}

	// Assert this is a write
	assert(!ESR_EL2_ISS_MSR_MRS_get_Direction(&iss));

	// Read the thread's register
	uint8_t	   reg_num = ESR_EL2_ISS_MSR_MRS_get_Rt(&iss);
	register_t val	   = vcpu_gpr_read(thread, reg_num);

	// Remove the fields that are not used in the comparison
	ESR_EL2_ISS_MSR_MRS_set_Rt(&iss, 0);
	ESR_EL2_ISS_MSR_MRS_set_Direction(&iss, false);

	switch (ESR_EL2_ISS_MSR_MRS_raw(iss)) {
	case ISS_MRS_MSR_ICC_DIR_EL1:
		vgic_icc_irq_deactivate(
			vic, ICC_DIR_EL1_get_INTID(&ICC_DIR_EL1_cast(val)));
		break;

	case ISS_MRS_MSR_ICC_ASGI1R_EL1:
		// ICC_ASGI1R_EL1 is treated as an alias of ICC_SGI0R_EL1.
		// This is because virtual accesses are always non-secure, and
		// non-secure writes generate SGIs for group 0 or secure group
		// 1, where the latter is treated as group 0 too because
		// GICD_CTLR.DS=1.
	case ISS_MRS_MSR_ICC_SGI0R_EL1:
		vgic_icc_generate_sgi(vic, ICC_SGIR_EL1_cast(val), false);
		break;

	case ISS_MRS_MSR_ICC_SGI1R_EL1:
		vgic_icc_generate_sgi(vic, ICC_SGIR_EL1_cast(val), true);
		break;

	case ISS_MRS_MSR_ICC_SRE_EL1:
		// WI
		break;

	case ISS_MRS_MSR_ICC_IGRPEN0_EL1:
		vgic_icc_set_group_enable(false, ICC_IGRPEN_EL1_cast(val));
		break;

	case ISS_MRS_MSR_ICC_IGRPEN1_EL1:
		vgic_icc_set_group_enable(true, ICC_IGRPEN_EL1_cast(val));
		break;

	default:
		ret = vgic_handle_vcpu_trap_sysreg_write_no_fgt(iss, vic, val);
		break;
	}

out:
	return ret;
}

static vcpu_trap_result_t
vgic_handle_vcpu_trap_sysreg_read_no_fgt(ESR_EL2_ISS_MSR_MRS_t iss,
					 thread_t *thread, register_t *val)
{
	vcpu_trap_result_t ret = VCPU_TRAP_RESULT_EMULATED;

#if defined(ARCH_ARM_FEAT_FGT) && ARCH_ARM_FEAT_FGT
	if (arm_fgt_is_allowed()) {
		// These registers should not be trapped when FGT is available
		ret = VCPU_TRAP_RESULT_UNHANDLED;
		goto out;
	}
#endif

	switch (ESR_EL2_ISS_MSR_MRS_raw(iss)) {
	// Trapped by TALL[01] which are set to trap ICC_IGRPEN[01]_EL1
	case ISS_MRS_MSR_ICC_IAR0_EL1:
	case ISS_MRS_MSR_ICC_HPPIR0_EL1:
		// We should only get this trap when the group is disabled, so
		// there can't be any deliverable IRQs; return 1023, which is
		// the reserved value meaning no pending interrupt.
		//
		// Note that the reserved IAR0 values that indicate a pending
		// group 1 interrupt (1020 or 1021) can only be returned to EL3
		// reads as of GICv3, so we don't need to check group 1.
		assert(!thread->vgic_group0_enabled);
		*val = 1023U;
		break;

	case ISS_MRS_MSR_ICC_BPR0_EL1: {
		ICH_VMCR_EL2_t vmcr = register_ICH_VMCR_EL2_read();
		*val		    = ICH_VMCR_EL2_get_VBPR0(&vmcr);
		break;
	}

	case ISS_MRS_MSR_ICC_AP0R0_EL1:
#if CPU_GICH_APR_COUNT >= 1U
		*val = register_ICH_AP0R0_EL2_read();
#endif
		break;

	case ISS_MRS_MSR_ICC_AP0R1_EL1:
#if CPU_GICH_APR_COUNT >= 2U
		*val = register_ICH_AP0R1_EL2_read();
#endif
		break;

	case ISS_MRS_MSR_ICC_AP0R2_EL1:
#if CPU_GICH_APR_COUNT >= 4U
		*val = register_ICH_AP0R2_EL2_read();
#endif
		break;

	case ISS_MRS_MSR_ICC_AP0R3_EL1:
#if CPU_GICH_APR_COUNT >= 4U
		*val = register_ICH_AP0R3_EL2_read();
#endif
		break;

	case ISS_MRS_MSR_ICC_IAR1_EL1:
	case ISS_MRS_MSR_ICC_HPPIR1_EL1:
		// We should only get this trap when the group is disabled, so
		// there can't be any deliverable IRQs; return 1023, which is
		// the reserved value meaning no pending interrupt.
		assert(!thread->vgic_group1_enabled);
		*val = 1023U;
		break;

	case ISS_MRS_MSR_ICC_BPR1_EL1: {
		ICH_VMCR_EL2_t vmcr = register_ICH_VMCR_EL2_read();
		*val		    = ICH_VMCR_EL2_get_VBPR1(&vmcr);
		break;
	}

	case ISS_MRS_MSR_ICC_AP1R0_EL1:
#if CPU_GICH_APR_COUNT >= 1U
		*val = register_ICH_AP1R0_EL2_read();
#endif
		break;

	case ISS_MRS_MSR_ICC_AP1R1_EL1:
#if CPU_GICH_APR_COUNT >= 2U
		*val = register_ICH_AP1R1_EL2_read();
#endif
		break;

	case ISS_MRS_MSR_ICC_AP1R2_EL1:
#if CPU_GICH_APR_COUNT >= 4U
		*val = register_ICH_AP1R2_EL2_read();
#endif
		break;

	case ISS_MRS_MSR_ICC_AP1R3_EL1:
#if CPU_GICH_APR_COUNT >= 4U
		*val = register_ICH_AP1R3_EL2_read();
#endif
		break;
	default:
		ret = VCPU_TRAP_RESULT_UNHANDLED;
		break;
	}

#if defined(ARCH_ARM_FEAT_FGT) && ARCH_ARM_FEAT_FGT
out:
#endif
	return ret;
}

vcpu_trap_result_t
vgic_handle_vcpu_trap_sysreg_read(ESR_EL2_ISS_MSR_MRS_t iss)
{
	register_t	   val	  = 0U;
	vcpu_trap_result_t ret	  = VCPU_TRAP_RESULT_EMULATED;
	thread_t	  *thread = thread_get_self();

	// Assert this is a read
	assert(ESR_EL2_ISS_MSR_MRS_get_Direction(&iss));

	uint8_t reg_num = ESR_EL2_ISS_MSR_MRS_get_Rt(&iss);

	// Remove the fields that are not used in the comparison
	ESR_EL2_ISS_MSR_MRS_t temp_iss = iss;
	ESR_EL2_ISS_MSR_MRS_set_Rt(&temp_iss, 0U);
	ESR_EL2_ISS_MSR_MRS_set_Direction(&temp_iss, false);

	switch (ESR_EL2_ISS_MSR_MRS_raw(temp_iss)) {
	case ISS_MRS_MSR_ICC_SRE_EL1: {
		// Return 1 for SRE, DFB and DIB
		ICC_SRE_EL1_t sre;
		ICC_SRE_EL1_init(&sre);
		ICC_SRE_EL1_set_SRE(&sre, true);
		ICC_SRE_EL1_set_DFB(&sre, true);
		ICC_SRE_EL1_set_DIB(&sre, true);
		val = ICC_SRE_EL1_raw(sre);
		break;
	}

	case ISS_MRS_MSR_ICC_IGRPEN0_EL1: {
		ICC_IGRPEN_EL1_t igrpen = ICC_IGRPEN_EL1_default();
		ICC_IGRPEN_EL1_set_Enable(&igrpen, thread->vgic_group0_enabled);
		val = ICC_IGRPEN_EL1_raw(igrpen);
		break;
	}

	case ISS_MRS_MSR_ICC_IGRPEN1_EL1: {
		ICC_IGRPEN_EL1_t igrpen = ICC_IGRPEN_EL1_default();
		ICC_IGRPEN_EL1_set_Enable(&igrpen, thread->vgic_group1_enabled);
		val = ICC_IGRPEN_EL1_raw(igrpen);
		break;
	}

	default:
		ret = vgic_handle_vcpu_trap_sysreg_read_no_fgt(iss, thread,
							       &val);
		break;
	}

	// Update the thread's register
	if (ret == VCPU_TRAP_RESULT_EMULATED) {
		vcpu_gpr_write(thread, reg_num, val);
	}

	return ret;
}

```

`hyp/vm/vgic/src/util.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>

#include <atomic.h>
#include <preempt.h>
#include <spinlock.h>
#include <thread.h>
#include <util.h>

#include "internal.h"

vgic_irq_type_t
vgic_get_irq_type(virq_t irq)
{
	vgic_irq_type_t type;

	if (irq < (virq_t)(GIC_SGI_BASE + GIC_SGI_NUM)) {
		type = VGIC_IRQ_TYPE_SGI;
	} else if ((irq >= (virq_t)GIC_PPI_BASE) &&
		   (irq < (virq_t)(GIC_PPI_BASE + GIC_PPI_NUM))) {
		type = VGIC_IRQ_TYPE_PPI;
	} else if ((irq >= (virq_t)GIC_SPI_BASE) &&
		   (irq < (virq_t)(GIC_SPI_BASE + GIC_SPI_NUM))) {
		type = VGIC_IRQ_TYPE_SPI;
	}
#if VGIC_HAS_EXT_IRQS
	else if ((irq >= (virq_t)GIC_PPI_EXT_BASE) &&
		 (irq < (virq_t)(GIC_PPI_EXT_BASE + GIC_PPI_EXT_NUM))) {
		type = VGIC_IRQ_TYPE_PPI_EXT;
	} else if ((irq >= (virq_t)GIC_SPI_EXT_BASE) &&
		   (irq < (virq_t)(GIC_SPI_EXT_BASE + GIC_SPI_EXT_NUM))) {
		type = VGIC_IRQ_TYPE_SPI_EXT;
	}
#endif
#if VGIC_HAS_LPI
	else if (irq >= (virq_t)GIC_LPI_BASE) {
		type = VGIC_IRQ_TYPE_LPI;
	}
#endif
	else {
		type = VGIC_IRQ_TYPE_RESERVED;
	}

	return type;
}

bool
vgic_irq_is_private(virq_t virq)
{
	bool result;
	switch (vgic_get_irq_type(virq)) {
	case VGIC_IRQ_TYPE_SGI:
	case VGIC_IRQ_TYPE_PPI:
		// If adding any classes here (e.g. PPI_EXT) you _must_ audit
		// all callers of this function and fix up their array indexing
		result = true;
		break;
	case VGIC_IRQ_TYPE_SPI:
	case VGIC_IRQ_TYPE_RESERVED:
#if VGIC_HAS_LPI && GICV3_HAS_VLPI_V4_1
	case VGIC_IRQ_TYPE_LPI:
#endif
	default:
		result = false;
		break;
	}
	return result;
}

bool
vgic_irq_is_spi(virq_t virq)
{
	bool result;
	switch (vgic_get_irq_type(virq)) {
	case VGIC_IRQ_TYPE_SPI:
		// If adding any classes here (e.g. SPI_EXT) you _must_ audit
		// all callers of this function and fix up their array indexing
		result = true;
		break;
	case VGIC_IRQ_TYPE_SGI:
	case VGIC_IRQ_TYPE_PPI:
	case VGIC_IRQ_TYPE_RESERVED:
#if VGIC_HAS_LPI && GICV3_HAS_VLPI_V4_1
	case VGIC_IRQ_TYPE_LPI:
#endif
	default:
		result = false;
		break;
	}
	return result;
}

bool
vgic_irq_is_ppi(virq_t virq)
{
	bool result;
	switch (vgic_get_irq_type(virq)) {
	case VGIC_IRQ_TYPE_PPI:
		// If adding any classes here (e.g. PPI_EXT) you _must_ audit
		// all callers of this function and fix up their array indexing
		result = true;
		break;
	case VGIC_IRQ_TYPE_SGI:
	case VGIC_IRQ_TYPE_SPI:
	case VGIC_IRQ_TYPE_RESERVED:
#if VGIC_HAS_LPI && GICV3_HAS_VLPI_V4_1
	case VGIC_IRQ_TYPE_LPI:
#endif
	default:
		result = false;
		break;
	}
	return result;
}

// Find the target of a given VIRQ source, if it is directly routed or private.
//
// No routing decisions are made by this function; it returns NULL for 1-of-N
// SPIs.
thread_t *
vgic_find_target(vic_t *vic, virq_source_t *source)
{
	thread_t *ret;

	if (source->is_private) {
		if (source->vgic_gicr_index < vic->gicr_count) {
			ret = atomic_load_consume(
				&vic->gicr_vcpus[source->vgic_gicr_index]);
		} else {
			ret = NULL;
		}
	} else {
		_Atomic vgic_delivery_state_t *dstate =
			vgic_find_dstate(vic, NULL, source->virq);
		vgic_delivery_state_t current_dstate =
			atomic_load_relaxed(dstate);

#if VGIC_HAS_1N
		if (vgic_delivery_state_get_route_1n(&current_dstate)) {
			ret = NULL;
			goto out;
		}
#endif

		index_t route_index =
			vgic_delivery_state_get_route(&current_dstate);
		if (route_index < vic->gicr_count) {
			ret = atomic_load_consume(
				&vic->gicr_vcpus[route_index]);
		} else {
			ret = NULL;
		}
	}

#if VGIC_HAS_1N
out:
#endif
	return ret;
}

virq_source_t *
vgic_find_source(vic_t *vic, thread_t *vcpu, virq_t virq)
{
	virq_source_t *source;

	// Load the source object pointer for a VIRQ. This must be a load
	// acquire to ensure that this is accessed prior to reading the virq
	// delivery state's level_src bit, because that bit being set should
	// guarantee that this pointer is non-NULL (see vic_unbind()).

	switch (vgic_get_irq_type(virq)) {
	case VGIC_IRQ_TYPE_SPI:
		assert(vic != NULL);
		if ((virq - GIC_SPI_BASE) < vic->sources_count) {
			source = atomic_load_acquire(
				&vic->sources[virq - GIC_SPI_BASE]);
		} else {
			source = NULL;
		}
		break;
	case VGIC_IRQ_TYPE_PPI:
		assert(vcpu != NULL);
		source = atomic_load_acquire(
			&vcpu->vgic_sources[virq - GIC_PPI_BASE]);
		break;
	case VGIC_IRQ_TYPE_SGI:
	case VGIC_IRQ_TYPE_RESERVED:
#if VGIC_HAS_LPI && GICV3_HAS_VLPI_V4_1
	case VGIC_IRQ_TYPE_LPI:
#endif
	default:
		source = NULL;
		break;
	}
	return source;
}

_Atomic(vgic_delivery_state_t) *
vgic_find_dstate(vic_t *vic, thread_t *vcpu, virq_t virq)
{
	_Atomic vgic_delivery_state_t *dstate;
	switch (vgic_get_irq_type(virq)) {
	case VGIC_IRQ_TYPE_SGI:
	case VGIC_IRQ_TYPE_PPI:
		assert(vcpu != NULL);
		dstate = &vcpu->vgic_private_states[virq];
		break;
	case VGIC_IRQ_TYPE_SPI:
		assert(vic != NULL);
		dstate = &vic->spi_states[virq - GIC_SPI_BASE];
		break;
	case VGIC_IRQ_TYPE_RESERVED:
#if VGIC_HAS_LPI && GICV3_HAS_VLPI_V4_1
	case VGIC_IRQ_TYPE_LPI:
#endif
	default:
		// Invalid IRQ number
		dstate = NULL;
		break;
	}
	return dstate;
}

bool
vgic_delivery_state_is_level_asserted(const vgic_delivery_state_t *x)
{
	return vgic_delivery_state_get_level_sw(x) ||
	       vgic_delivery_state_get_level_msg(x) ||
	       vgic_delivery_state_get_level_src(x);
}

bool
vgic_delivery_state_is_pending(const vgic_delivery_state_t *x)
{
	return vgic_delivery_state_get_cfg_is_edge(x)
		       ? vgic_delivery_state_get_edge(x)
		       : vgic_delivery_state_is_level_asserted(x);
}

cpu_index_t
vgic_lr_owner_lock(thread_t *vcpu)
{
	preempt_disable();
	return vgic_lr_owner_lock_nopreempt(vcpu);
}

cpu_index_t
vgic_lr_owner_lock_nopreempt(thread_t *vcpu) LOCK_IMPL
{
	cpu_index_t remote_cpu;
	if ((vcpu != NULL) && (vcpu != thread_get_self())) {
		spinlock_acquire_nopreempt(&vcpu->vgic_lr_owner_lock.lock);
		remote_cpu =
			atomic_load_relaxed(&vcpu->vgic_lr_owner_lock.owner);
	} else {
		remote_cpu = CPU_INDEX_INVALID;
	}
	return remote_cpu;
}

void
vgic_lr_owner_unlock(thread_t *vcpu)
{
	vgic_lr_owner_unlock_nopreempt(vcpu);
	preempt_enable();
}

void
vgic_lr_owner_unlock_nopreempt(thread_t *vcpu) LOCK_IMPL
{
	if ((vcpu != NULL) && (vcpu != thread_get_self())) {
		spinlock_release_nopreempt(&vcpu->vgic_lr_owner_lock.lock);
	}
}

```

`hyp/vm/vgic/src/vdevice.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>

#include <hypconstants.h>
#include <hypcontainers.h>
#include <hypregisters.h>

#include <atomic.h>
#include <compiler.h>
#include <cpulocal.h>
#include <log.h>
#include <panic.h>
#include <preempt.h>
#include <rcu.h>
#include <scheduler.h>
#include <spinlock.h>
#include <thread.h>
#include <trace.h>
#include <util.h>

#include "event_handlers.h"
#include "gicv3.h"
#include "internal.h"
#include "vgic.h"

// Qualcomm's JEP106 identifier is 0x70, with no continuation bytes. This is
// used in the virtual GICD_IIDR and GICR_IIDR.
#define JEP106_IDENTITY 0x70U
#define JEP106_CONTCODE 0x0U
#define IIDR_IMPLEMENTER                                                       \
	(((uint16_t)JEP106_CONTCODE << 8U) | (uint16_t)JEP106_IDENTITY)
#define IIDR_PRODUCTID (uint8_t)'G' /* For "Gunyah" */
#define IIDR_VARIANT   0U
#define IIDR_REVISION  0U

static void
vgic_update_irqbits_flag(vic_t *vic, const thread_t *vcpu, size_t base_offset,
			 count_t range_base, count_t range_size, uint32_t *bits)
{
	for (index_t i = 0; i < vic->gicr_count; i++) {
		rcu_read_start();
		thread_t *check_vcpu = atomic_load_consume(&vic->gicr_vcpus[i]);
		if (check_vcpu == NULL) {
			goto next_vcpu;
		}

		// If it's the private range, make sure we only look at the
		// targeted VCPU.
		if (vgic_irq_is_private(range_base) && (check_vcpu != vcpu)) {
			goto next_vcpu;
		}

		cpu_index_t remote_cpu = vgic_lr_owner_lock(check_vcpu);

		// If it's remotely running, we can't check its LRs. If any of
		// the range is listed in this VCPU, we're out of luck.
		if (cpulocal_index_valid(remote_cpu)) {
			goto next_vcpu_locked;
		}

		for (count_t lr = 0U; lr < CPU_GICH_LR_COUNT; lr++) {
			vgic_lr_status_t *status = &check_vcpu->vgic_lrs[lr];
			if (status->dstate == NULL) {
				// LR is not in use
				continue;
			}

			virq_t virq =
				ICH_LR_EL2_base_get_vINTID(&status->lr.base);
			if ((virq < range_base) ||
			    (virq >= (range_base + range_size))) {
				// LR's VIRQ is not in this range
				continue;
			}

			uint32_t bit = (uint32_t)util_bit(virq - range_base);
			ICH_LR_EL2_State_t state =
				ICH_LR_EL2_base_get_State(&status->lr.base);

			switch (base_offset) {
			case OFS_GICD_ISPENDR(0U):
			case OFS_GICD_ICPENDR(0U):
				if ((state == ICH_LR_EL2_STATE_PENDING) ||
				    (state ==
				     ICH_LR_EL2_STATE_PENDING_ACTIVE)) {
					(*bits) |= bit;
				} else {
					(*bits) &= ~bit;
				}
				break;
			case OFS_GICD_ISACTIVER(0U):
			case OFS_GICD_ICACTIVER(0U):
				if ((state == ICH_LR_EL2_STATE_ACTIVE) ||
				    (state ==
				     ICH_LR_EL2_STATE_PENDING_ACTIVE)) {
					(*bits) |= bit;
				} else {
					(*bits) &= ~bit;
				}
				break;
			default:
				panic("vgic_read_irqbits: Bad base_offset");
			}
		}

	next_vcpu_locked:
		vgic_lr_owner_unlock(check_vcpu);
	next_vcpu:
		rcu_read_finish();
	}
}

static uint32_t
vgic_read_gicd_irqbits(count_t			      range_size,
		       _Atomic vgic_delivery_state_t *dstates,
		       size_t base_offset, bool *listed)
{
	uint32_t bits = 0U;
	for (count_t i = 0; i < range_size; i++) {
		vgic_delivery_state_t this_dstate =
			atomic_load_relaxed(&dstates[i]);
		bool bit;

		// Note: the GICR base offsets are the same as the GICD offsets,
		// so we don't need to duplicate them here.
		switch (base_offset) {
		case OFS_GICD_IGROUPR(0U):
			bit = vgic_delivery_state_get_group1(&this_dstate);
			break;
		case OFS_GICD_ISENABLER(0U):
		case OFS_GICD_ICENABLER(0U):
			bit = vgic_delivery_state_get_enabled(&this_dstate);
			break;
		case OFS_GICD_ISPENDR(0U):
		case OFS_GICD_ICPENDR(0U):
			bit = vgic_delivery_state_is_pending(&this_dstate);
			if (vgic_delivery_state_get_listed(&this_dstate)) {
				*listed = true;
			}
			break;
		case OFS_GICD_ISACTIVER(0U):
		case OFS_GICD_ICACTIVER(0U):
			bit = vgic_delivery_state_get_active(&this_dstate);
			if (vgic_delivery_state_get_listed(&this_dstate)) {
				*listed = true;
			}
			break;
		default:
			panic("vgic_read_irqbits: Bad base_offset");
		}

		if (bit) {
			bits |= (uint32_t)util_bit(i);
		}
	}

	return bits;
}

static uint32_t
vgic_read_irqbits(vic_t *vic, thread_t *vcpu, size_t base_offset, size_t offset)
{
	assert(vic != NULL);
	assert(vcpu != NULL);
	assert(offset >= base_offset);
	assert(offset <= base_offset + (31 * sizeof(uint32_t)));

	uint32_t bits = 0U;
	count_t	 range_base =
		(count_t)((offset - base_offset) / sizeof(uint32_t)) * 32U;
	count_t range_size =
		util_min(32U, GIC_SPECIAL_INTIDS_BASE - range_base);

	_Atomic vgic_delivery_state_t *dstates =
		vgic_find_dstate(vic, vcpu, range_base);
	if (dstates == NULL) {
		goto out;
	}
	assert(compiler_sizeof_object(dstates) >=
	       range_size * sizeof(*dstates));

	bool listed = false;

	bits = vgic_read_gicd_irqbits(range_size, dstates, base_offset,
				      &listed);

#if GICV3_HAS_VLPI_V4_1 && defined(GICV3_ENABLE_VPE) && GICV3_ENABLE_VPE
	if ((range_base == GIC_SGI_BASE) &&
	    ((base_offset == offsetof(gicd_t, ispendr)) ||
	     (base_offset == offsetof(gicd_t, icpendr)))) {
		// Query the hardware for the vSGI pending state
		uint32_result_t bits_r = gicv3_vpe_vsgi_query(vcpu);
		if (bits_r.e == OK) {
			bits |= bits_r.r;
		}
	}
#endif // GICV3_HAS_VLPI_V4_1 && GICV3_ENABLE_VPE

	if (compiler_expected(!listed)) {
		// We didn't try to read the pending or active state of a VIRQ
		// that is in list register, so the value we've read is
		// accurate.
		goto out;
	}

	// Read back from the current VCPU's physical LRs.
	preempt_disable();
	for (count_t lr = 0U; lr < CPU_GICH_LR_COUNT; lr++) {
		vgic_read_lr_state(lr);
	}
	preempt_enable();

	// Try to update the flags for listed vIRQs, based on the state of
	// every VCPU's list registers.
	vgic_update_irqbits_flag(vic, vcpu, base_offset, range_base, range_size,
				 &bits);
out:
	return bits;
}

static register_t
vgic_read_priority(vic_t *vic, thread_t *vcpu, size_t offset,
		   size_t access_size)
{
	register_t bits = 0U;

	_Atomic vgic_delivery_state_t *dstates =
		vgic_find_dstate(vic, vcpu, (count_t)offset);
	if (dstates == NULL) {
		goto out;
	}
	assert(compiler_sizeof_object(dstates) >=
	       access_size * sizeof(*dstates));

	for (count_t i = 0; i < access_size; i++) {
		vgic_delivery_state_t this_dstate =
			atomic_load_relaxed(&dstates[i]);

		bits |= (register_t)vgic_delivery_state_get_priority(
				&this_dstate)
			<< (i * 8U);
	}

out:
	return bits;
}

static register_t
vgic_read_config(vic_t *vic, thread_t *vcpu, size_t offset)
{
	assert(vic != NULL);
	assert(vcpu != NULL);
	assert(offset <= (63 * sizeof(uint32_t)));

	register_t bits	      = 0U;
	count_t	   range_base = (count_t)(offset / sizeof(uint32_t)) * 16U;
	count_t	   range_size =
		util_min(16U, GIC_SPECIAL_INTIDS_BASE - range_base);

	_Atomic vgic_delivery_state_t *dstates =
		vgic_find_dstate(vic, vcpu, range_base);
	if (dstates == NULL) {
		goto out;
	}
	assert(compiler_sizeof_object(dstates) >=
	       range_size * sizeof(*dstates));

	for (count_t i = 0; i < range_size; i++) {
		vgic_delivery_state_t this_dstate =
			atomic_load_relaxed(&dstates[i]);

		if (vgic_delivery_state_get_cfg_is_edge(&this_dstate)) {
			bits |= util_bit((i * 2U) + 1U);
		}
	}

out:
	return bits;
}

static bool
gicd_vdevice_read(vic_t *vic, size_t offset, register_t *val,
		  size_t access_size)
{
	bool	  ret	 = true;
	thread_t *thread = thread_get_self();

	uint32_t read_val = 0U;

	assert(vic != NULL);

	if ((offset == offsetof(gicd_t, setspi_nsr)) ||
	    (offset == offsetof(gicd_t, clrspi_nsr)) ||
	    (offset == offsetof(gicd_t, setspi_sr)) ||
	    (offset == offsetof(gicd_t, clrspi_sr)) ||
	    (offset == offsetof(gicd_t, sgir))) {
		// WO registers, RAZ
		GICD_STATUSR_t statusr;
		GICD_STATUSR_init(&statusr);
		GICD_STATUSR_set_RWOD(&statusr, true);
		vgic_gicd_set_statusr(vic, statusr, true);
		read_val = 0U;

	} else if (offset == offsetof(gicd_t, ctlr)) {
		read_val =
			GICD_CTLR_DS_raw(atomic_load_relaxed(&vic->gicd_ctlr));

	} else if (offset == offsetof(gicd_t, statusr)) {
		read_val = GICD_STATUSR_raw(vic->gicd_statusr);

	} else if (offset == offsetof(gicd_t, typer)) {
		GICD_TYPER_t typer = GICD_TYPER_default();
		GICD_TYPER_set_ITLinesNumber(
			&typer,
			(count_t)util_balign_up(GIC_SPI_NUM, 32U) / 32U);
		GICD_TYPER_set_MBIS(&typer, true);
#if VGIC_HAS_EXT_IRQS
#error Extended IRQs not yet implemented
#else
		GICD_TYPER_set_ESPI(&typer, false);
#endif

#if VGIC_HAS_LPI
		GICD_TYPER_set_LPIS(&typer, vgic_has_lpis(vic));
		GICD_TYPER_set_IDbits(&typer, vic->gicd_idbits - 1U);
#else
		GICD_TYPER_set_IDbits(&typer, VGIC_IDBITS - 1U);
#endif
		GICD_TYPER_set_A3V(&typer, true);
		GICD_TYPER_set_No1N(&typer, VGIC_HAS_1N == 0);
		read_val = GICD_TYPER_raw(typer);

	} else if (offset == offsetof(gicd_t, iidr)) {
		GICD_IIDR_t iidr = GICD_IIDR_default();
		GICD_IIDR_set_Implementer(&iidr, IIDR_IMPLEMENTER);
		GICD_IIDR_set_ProductID(&iidr, IIDR_PRODUCTID);
		GICD_IIDR_set_Variant(&iidr, IIDR_VARIANT);
		GICD_IIDR_set_Revision(&iidr, IIDR_REVISION);
		read_val = GICD_IIDR_raw(iidr);

	} else if (offset == offsetof(gicd_t, typer2)) {
		GICD_TYPER2_t typer2 = GICD_TYPER2_default();
#if GICV3_HAS_VLPI_V4_1
		GICD_TYPER2_set_nASSGIcap(&typer2, vgic_has_lpis(vic));
#endif
		read_val = GICD_TYPER2_raw(typer2);

	} else if (offset == (size_t)OFS_GICD_PIDR2) {
		read_val = VGIC_PIDR2;

	} else if ((offset >= OFS_GICD_IGROUPR(0U)) &&
		   (offset <= OFS_GICD_IGROUPR(31U))) {
		read_val = vgic_read_irqbits(vic, thread, OFS_GICD_IGROUPR(0),
					     offset);

	} else if ((offset >= OFS_GICD_ISENABLER(0U)) &&
		   (offset <= OFS_GICD_ISENABLER(31U))) {
		read_val = vgic_read_irqbits(vic, thread,
					     OFS_GICD_ISENABLER(0U), offset);

	} else if ((offset >= OFS_GICD_ICENABLER(0U)) &&
		   (offset <= OFS_GICD_ICENABLER(31U))) {
		read_val = vgic_read_irqbits(vic, thread,
					     OFS_GICD_ICENABLER(0U), offset);

	} else if ((offset >= OFS_GICD_ISPENDR(0U)) &&
		   (offset <= OFS_GICD_ISPENDR(31U))) {
		read_val = vgic_read_irqbits(vic, thread, OFS_GICD_ISPENDR(0U),
					     offset);

	} else if ((offset >= OFS_GICD_ICPENDR(0U)) &&
		   (offset <= OFS_GICD_ICPENDR(31U))) {
		read_val = vgic_read_irqbits(vic, thread, OFS_GICD_ICPENDR(0U),
					     offset);

	} else if ((offset >= OFS_GICD_ISACTIVER(0U)) &&
		   (offset <= OFS_GICD_ISACTIVER(31U))) {
		read_val = vgic_read_irqbits(vic, thread,
					     OFS_GICD_ISACTIVER(0U), offset);

	} else if ((offset >= OFS_GICD_ICACTIVER(0U)) &&
		   (offset <= OFS_GICD_ICACTIVER(31U))) {
		read_val = vgic_read_irqbits(vic, thread,
					     OFS_GICD_ICACTIVER(0U), offset);

	} else if (util_offset_in_range(offset, gicd_t, ipriorityr)) {
		read_val = (uint32_t)vgic_read_priority(
			vic, thread, offset - offsetof(gicd_t, ipriorityr),
			access_size);

	} else if (util_offset_in_range(offset, gicd_t, icfgr)) {
		read_val = (uint32_t)vgic_read_config(
			vic, thread, offset - offsetof(gicd_t, icfgr));

	} else if (util_offset_in_range(offset, gicd_t, itargetsr) ||
		   util_offset_in_range(offset, gicd_t, igrpmodr) ||
		   util_offset_in_range(offset, gicd_t, nsacr)) {
		// RAZ ranges
		read_val = 0U;

	} else {
		// Unknown register
		GICD_STATUSR_t statusr;
		GICD_STATUSR_init(&statusr);
		GICD_STATUSR_set_RRD(&statusr, true);
		vgic_gicd_set_statusr(vic, statusr, true);
		read_val = 0U;
	}

	*val = read_val;

	return ret;
}

static bool
gicd_vdevice_write(vic_t *vic, size_t offset, register_t val,
		   size_t access_size)
{
	bool ret = true;

	assert(vic != NULL);
	VGIC_TRACE(GICD_WRITE, vic, NULL, "GICD_WRITE reg = {:x}, val = {:#x}",
		   offset, val);

	if (offset == offsetof(gicd_t, ctlr)) {
		vgic_gicd_set_control(vic, GICD_CTLR_DS_cast((uint32_t)val));

	} else if ((offset == offsetof(gicd_t, typer)) ||
		   (offset == offsetof(gicd_t, iidr)) ||
		   (offset == (size_t)OFS_GICD_PIDR2) ||
		   (offset == offsetof(gicd_t, typer2))) {
		// RO registers
		GICD_STATUSR_t statusr;
		GICD_STATUSR_init(&statusr);
		GICD_STATUSR_set_WROD(&statusr, true);
		vgic_gicd_set_statusr(vic, statusr, true);

	} else if (offset == offsetof(gicd_t, statusr)) {
		GICD_STATUSR_t statusr = GICD_STATUSR_cast((uint32_t)val);
		vgic_gicd_set_statusr(vic, statusr, false);

	} else if ((offset == offsetof(gicd_t, setspi_nsr)) ||
		   (offset == offsetof(gicd_t, clrspi_nsr))) {
		vgic_gicd_change_irq_pending(
			vic,
			GICD_CLRSPI_SETSPI_NSR_SR_get_INTID(
				&GICD_CLRSPI_SETSPI_NSR_SR_cast((uint32_t)val)),
			(offset == offsetof(gicd_t, setspi_nsr)), true);

	} else if ((offset == offsetof(gicd_t, setspi_sr)) ||
		   (offset == offsetof(gicd_t, clrspi_sr))) {
		// WI

	} else if ((offset >= OFS_GICD_IGROUPR(0U)) &&
		   (offset <= OFS_GICD_IGROUPR(31U))) {
		// 32-bit registers, 32-bit access only

		index_t n = (index_t)((offset - OFS_GICD_IGROUPR(0U)) /
				      sizeof(uint32_t));
		for (index_t i = util_max(n * 32U, GIC_SPI_BASE);
		     i < util_min((n + 1U) * 32U, 1020U); i++) {
			vgic_gicd_set_irq_group(
				vic, i, (val & util_bit(i % 32U)) != 0U);
		}

	} else if ((offset >= OFS_GICD_ISENABLER(0U)) &&
		   (offset <= OFS_GICD_ISENABLER(31U))) {
		// 32-bit registers, 32-bit access only

		index_t n = (index_t)((offset - OFS_GICD_ISENABLER(0U)) /
				      sizeof(uint32_t));
		// Ignore writes to the SGI and PPI bits (ISENABLER0)
		if (n != 0U) {
			uint32_t bits = (uint32_t)val;
			if (n == 31U) {
				// Ignore the bits for IRQs 1020-1023
				bits &= ~0xf0000000U;
			}
			while (bits != 0U) {
				index_t i = compiler_ctz(bits);
				bits &= ~((index_t)util_bit(i));

				vgic_gicd_change_irq_enable(vic, (n * 32U) + i,
							    true);
			}
		}

	} else if ((offset >= OFS_GICD_ICENABLER(0U)) &&
		   (offset <= OFS_GICD_ICENABLER(31U))) {
		// 32-bit registers, 32-bit access only

		index_t n = (index_t)((offset - OFS_GICD_ICENABLER(0U)) /
				      sizeof(uint32_t));
		// Ignore writes to the SGI and PPI bits (ICENABLER0)
		if (n != 0U) {
			uint32_t bits = (uint32_t)val;
			if (n == 31U) {
				// Ignore the bits for IRQs 1020-1023
				bits &= ~0xf0000000U;
			}
			while (bits != 0U) {
				index_t i = compiler_ctz(bits);
				bits &= ~((index_t)util_bit(i));

				vgic_gicd_change_irq_enable(vic, (n * 32U) + i,
							    false);
			}
		}

	} else if ((offset >= OFS_GICD_ISPENDR(0U)) &&
		   (offset <= OFS_GICD_ISPENDR(31U))) {
		// 32-bit registers, 32-bit access only

		index_t n = (index_t)((offset - OFS_GICD_ISPENDR(0U)) /
				      sizeof(uint32_t));
		// Ignore writes to the SGI and PPI bits (ISPENDR0)
		if (n != 0U) {
			uint32_t bits = (uint32_t)val;
			if (n == 31U) {
				// Ignore the bits for IRQs 1020-1023
				bits &= ~0xf0000000U;
			}
			while (bits != 0U) {
				index_t i = compiler_ctz(bits);
				bits &= ~((index_t)util_bit(i));

				vgic_gicd_change_irq_pending(vic, (n * 32U) + i,
							     true, false);
			}
		}

	} else if ((offset >= OFS_GICD_ICPENDR(0U)) &&
		   (offset <= OFS_GICD_ICPENDR(31U))) {
		// 32-bit registers, 32-bit access only

		index_t n = (index_t)((offset - OFS_GICD_ICPENDR(0U)) /
				      sizeof(uint32_t));
		// Ignore writes to the SGI and PPI bits (ICPENDR0)
		if (n != 0U) {
			uint32_t bits = (uint32_t)val;
			if (n == 31U) {
				// Ignore the bits for IRQs 1020-1023
				bits &= ~0xf0000000U;
			}
			while (bits != 0U) {
				index_t i = compiler_ctz(bits);
				bits &= ~((index_t)util_bit(i));

				vgic_gicd_change_irq_pending(vic, (n * 32U) + i,
							     false, false);
			}
		}

	} else if ((offset >= OFS_GICD_ISACTIVER(0U)) &&
		   (offset <= OFS_GICD_ISACTIVER(31U))) {
		// 32-bit registers, 32-bit access only

		index_t n = (index_t)((offset - OFS_GICD_ISACTIVER(0U)) /
				      sizeof(uint32_t));
		// Ignore writes to the SGI and PPI bits (ISACTIVER0)
		if (n != 0U) {
			uint32_t bits = (uint32_t)val;
			if (n == 31U) {
				// Ignore the bits for IRQs 1020-1023
				bits &= ~0xf0000000U;
			}
			while (bits != 0U) {
				index_t i = compiler_ctz(bits);
				bits &= ~((index_t)util_bit(i));

				vgic_gicd_change_irq_active(vic, (n * 32U) + i,
							    true);
			}
		}

	} else if ((offset >= OFS_GICD_ICACTIVER(0U)) &&
		   (offset <= OFS_GICD_ICACTIVER(31U))) {
		// 32-bit registers, 32-bit access only

		index_t n = (index_t)((offset - OFS_GICD_ICACTIVER(0U)) /
				      sizeof(uint32_t));
		// Ignore writes to the SGI and PPI bits (ICACTIVER0)
		if (n != 0U) {
			uint32_t bits = (uint32_t)val;
			if (n == 31U) {
				// Ignore the bits for IRQs 1020-1023
				bits &= ~0xf0000000U;
			}
			while (bits != 0U) {
				index_t i = compiler_ctz(bits);
				bits &= ~((index_t)util_bit(i));

				vgic_gicd_change_irq_active(vic, (n * 32U) + i,
							    false);
			}
		}

	} else if ((offset >= OFS_GICD_IPRIORITYR(0U)) &&
		   (offset <= OFS_GICD_IPRIORITYR(1019U))) {
		// 32-bit registers, byte or 32-bit accessible

		index_t n = (index_t)(offset - OFS_GICD_IPRIORITYR(0U));
		// Loop through every byte
		uint32_t shifted_val = (uint32_t)val;
		for (index_t i = util_max(n, GIC_SPI_BASE); i < n + access_size;
		     i++) {
			vgic_gicd_set_irq_priority(vic, i,
						   (uint8_t)shifted_val);
			shifted_val >>= 8U;
		}
	} else if ((offset >= OFS_GICD_ITARGETSR(0U)) &&
		   (offset <= OFS_GICD_ITARGETSR(1019U))) {
		// WI

	} else if ((offset >= OFS_GICD_ICFGR(0U)) &&
		   (offset <= OFS_GICD_ICFGR(63U))) {
		// 32-bit registers, 32-bit access only

		index_t n = (index_t)((offset - OFS_GICD_ICFGR(0U)) /
				      sizeof(uint32_t));
		// Ignore writes to the SGI and PPI bits
		for (index_t i = util_max(n * 16U, GIC_SPI_BASE);
		     i < util_min((n + 1U) * 16U, 1020U); i++) {
			vgic_gicd_set_irq_config(
				vic, i,
				(val & util_bit(((i % 16U) * 2U) + 1U)) != 0U);
		}

	} else if ((offset >= OFS_GICD_IGRPMODR(0U)) &&
		   (offset <= OFS_GICD_IGRPMODR(31U))) {
		// WI

	} else if ((offset >= OFS_GICD_NSACR(0U)) &&
		   (offset <= OFS_GICD_NSACR(63U))) {
		// WI

	} else if (offset == offsetof(gicd_t, sgir)) {
		// WI

	} else if ((offset >= OFS_GICD_CPENDSGIR(0U)) &&
		   (offset <= OFS_GICD_CPENDSGIR(15U))) {
		// WI

	} else if ((offset >= OFS_GICD_SPENDSGIR(0U)) &&
		   (offset <= OFS_GICD_SPENDSGIR(15U))) {
		// WI

	} else if ((offset >= OFS_GICD_IROUTER(0U)) &&
		   (offset <= OFS_GICD_IROUTER(GIC_SPI_NUM - 1))) {
		// 64-bit registers with 64-bit access only

		index_t spi = GIC_SPI_BASE +
			      (index_t)((offset - OFS_GICD_IROUTER(0U)) /
					sizeof(uint64_t));
		GICD_IROUTER_t irouter = GICD_IROUTER_cast(val);
		vgic_gicd_set_irq_router(vic, spi,
					 GICD_IROUTER_get_Aff0(&irouter),
					 GICD_IROUTER_get_Aff1(&irouter),
					 GICD_IROUTER_get_Aff2(&irouter),
					 GICD_IROUTER_get_Aff3(&irouter),
					 GICD_IROUTER_get_IRM(&irouter));

	}
#if GICV3_HAS_GICD_ICLAR
	else if (offset == OFS_GICD_SETCLASSR) {
		GICD_SETCLASSR_t setclassr = GICD_SETCLASSR_cast((uint32_t)val);
		virq_t		 virq	   = GICD_SETCLASSR_get_SPI(&setclassr);
		if (vgic_irq_is_spi(virq)) {
			vgic_gicd_set_irq_classes(
				vic, virq,
				GICD_SETCLASSR_get_Class0(&setclassr),
				GICD_SETCLASSR_get_Class1(&setclassr));
		}
	}
#endif
#if VGIC_HAS_EXT_IRQS
#error extended SPI support not implemented
#endif
#if VGIC_IGNORE_ARRAY_OVERFLOWS
	else if ((offset >= OFS_GICD_IPRIORITYR(1020U)) &&
		 (offset <= OFS_GICD_IPRIORITYR(1023U))) {
		// Ignore priority writes for special IRQs
	} else if ((offset >= OFS_GICD_IROUTER(GIC_SPI_NUM)) &&
		   (offset <= OFS_GICD_IROUTER(1023U))) {
		// Ignore route writes for special IRQs
	}
#endif
	else {
		// Unknown register
		GICD_STATUSR_t statusr;
		GICD_STATUSR_init(&statusr);
		GICD_STATUSR_set_WRD(&statusr, true);
		vgic_gicd_set_statusr(vic, statusr, true);
		ret = false;
	}

	return ret;
}

static bool
gicd_access_allowed(size_t size, size_t offset)
{
	bool ret;

	// First check if the access is size-aligned
	if ((offset & (size - 1U)) != 0UL) {
		ret = false;
	} else if (size == sizeof(uint64_t)) {
		// Doubleword accesses are only allowed for routing registers
		ret = ((offset >= OFS_GICD_IROUTER(0U)) &&
		       (offset <= OFS_GICD_IROUTER(GIC_SPI_NUM - 1U)));
#if VGIC_IGNORE_ARRAY_OVERFLOWS
		// Ignore route accesses for special IRQs
		if ((offset >= OFS_GICD_IROUTER(0U)) &&
		    (offset <= OFS_GICD_IROUTER(1023U))) {
			ret = true;
		}
#endif
	} else if (size == sizeof(uint32_t)) {
		// Word accesses, always allowed
		ret = true;
	} else if (size == sizeof(uint16_t)) {
		// Half-word accesses are only allowed for the SETSPI and CLRSPI
		// registers
		ret = ((offset == offsetof(gicd_t, setspi_nsr)) ||
		       (offset == offsetof(gicd_t, clrspi_nsr)));
	} else if (size == sizeof(uint8_t)) {
		// Byte accesses are only allowed for priority, target and
		// SGI pending registers
		ret = (((offset >= OFS_GICD_IPRIORITYR(0U)) &&
			(offset <= OFS_GICD_IPRIORITYR(1019U))) ||
		       ((offset >= OFS_GICD_ITARGETSR(0U)) &&
			(offset <= OFS_GICD_ITARGETSR(1019U))) ||
		       ((offset >= OFS_GICD_CPENDSGIR(0U)) &&
			(offset <= OFS_GICD_CPENDSGIR(15U))) ||
		       ((offset >= OFS_GICD_SPENDSGIR(0U)) &&
			(offset <= OFS_GICD_SPENDSGIR(15U))));
#if VGIC_IGNORE_ARRAY_OVERFLOWS
		// Ignore priority accesses for special IRQs
		if ((offset >= OFS_GICD_IROUTER(0U)) &&
		    (offset <= OFS_GICD_IROUTER(1023U))) {
			ret = true;
		}
#endif
	} else {
		// Invalid access size
		ret = false;
	}

	return ret;
}

static bool
gicr_vdevice_read(vic_t *vic, thread_t *gicr_vcpu, index_t gicr_num,
		  size_t offset, register_t *val, size_t access_size,
		  bool last_gicr)
{
	bool ret = true;

	(void)vic;

	if ((offset == offsetof(gicr_t, rd.setlpir)) ||
	    (offset == offsetof(gicr_t, rd.clrlpir)) ||
	    (offset == offsetof(gicr_t, rd.invlpir)) ||
	    (offset == offsetof(gicr_t, rd.invallr))) {
		// WO registers, RAZ
		GICR_STATUSR_t statusr;
		GICR_STATUSR_init(&statusr);
		GICR_STATUSR_set_RWOD(&statusr, true);
		vgic_gicr_rd_set_statusr(gicr_vcpu, statusr, true);
		*val = 0U;

	} else if (util_balign_down(offset, sizeof(GICR_TYPER_t)) ==
		   offsetof(gicr_t, rd.typer)) {
		GICR_TYPER_t typer = GICR_TYPER_default();
		GICR_TYPER_set_Aff0(
			&typer,
			MPIDR_EL1_get_Aff0(&gicr_vcpu->vcpu_regs_mpidr_el1));
		GICR_TYPER_set_Aff1(
			&typer,
			MPIDR_EL1_get_Aff1(&gicr_vcpu->vcpu_regs_mpidr_el1));
		GICR_TYPER_set_Aff2(
			&typer,
			MPIDR_EL1_get_Aff2(&gicr_vcpu->vcpu_regs_mpidr_el1));
		GICR_TYPER_set_Aff3(
			&typer,
			MPIDR_EL1_get_Aff3(&gicr_vcpu->vcpu_regs_mpidr_el1));
		GICR_TYPER_set_Last(&typer, last_gicr);

		// The Processor Number is used only to select the target GICR
		// in ITS commands. When ARE is disabled, it also determines the
		// CPU's bit in ITARGETSR, but we don't support that. So it is
		// safe for this to be the logical VCPU index.
		GICR_TYPER_set_Processor_Num(&typer, gicr_num);
#if VGIC_HAS_LPI
		GICR_TYPER_set_PLPIS(&typer, vgic_has_lpis(vic));
#endif
		*val = GICR_TYPER_raw(typer);

		if (offset != offsetof(gicr_t, rd.typer)) {
			// Must be a 32-bit access to the big end
			assert(offset == OFS_GICR_RD_TYPER + sizeof(uint32_t));
			*val >>= 32U;
		}

	} else if (offset == offsetof(gicr_t, rd.iidr)) {
		GICR_IIDR_t iidr = GICR_IIDR_default();
		GICR_IIDR_set_Implementer(&iidr, IIDR_IMPLEMENTER);
		GICR_IIDR_set_ProductID(&iidr, IIDR_PRODUCTID);
		GICR_IIDR_set_Variant(&iidr, IIDR_VARIANT);
		GICR_IIDR_set_Revision(&iidr, IIDR_REVISION);
		*val = GICR_IIDR_raw(iidr);

	} else if (offset == offsetof(gicr_t, PIDR2)) {
		*val = VGIC_PIDR2;

	} else if (offset == offsetof(gicr_t, rd.ctlr)) {
		*val = GICR_CTLR_raw(vgic_gicr_rd_get_control(vic, gicr_vcpu));

	} else if (offset == offsetof(gicr_t, rd.statusr)) {
		*val = GICR_STATUSR_raw(
			atomic_load_relaxed(&gicr_vcpu->vgic_gicr_rd_statusr));

	} else if (offset == offsetof(gicr_t, rd.waker)) {
		GICR_WAKER_t gicr_waker = GICR_WAKER_default();
		GICR_WAKER_set_ProcessorSleep(
			&gicr_waker,
			atomic_load_relaxed(&gicr_vcpu->vgic_sleep) !=
				VGIC_SLEEP_STATE_AWAKE);
		GICR_WAKER_set_ChildrenAsleep(
			&gicr_waker, vgic_gicr_rd_check_sleep(gicr_vcpu));

		*val = GICR_WAKER_raw(gicr_waker);

	} else if (offset == offsetof(gicr_t, rd.propbaser)) {
#if VGIC_HAS_LPI
		*val = GICR_PROPBASER_raw(
			atomic_load_relaxed(&vic->gicr_rd_propbaser));
#else
		*val = 0U;
#endif

	} else if (offset == offsetof(gicr_t, rd.pendbaser)) {
#if VGIC_HAS_LPI
		GICR_PENDBASER_t pendbase =
			atomic_load_relaxed(&gicr_vcpu->vgic_gicr_rd_pendbaser);
		// The PTZ bit is specified as WO/RAZ, but we use it to cache
		// the written value which is used when EnableLPIs is set to 1.
		// Therefore we must clear it here.
		GICR_PENDBASER_set_PTZ(&pendbase, false);
		*val = GICR_PENDBASER_raw(pendbase);
#else
		*val = 0U;
#endif

	} else if (offset == offsetof(gicr_t, rd.syncr)) {
#if VGIC_HAS_LPI
		GICR_SYNCR_t syncr = GICR_SYNCR_default();
		GICR_SYNCR_set_Busy(&syncr,
				    vgic_gicr_get_inv_pending(vic, gicr_vcpu));
		*val = GICR_SYNCR_raw(syncr);
#else
		*val = 0U;
#endif

	} else if ((offset == offsetof(gicr_t, sgi.igroupr0)) ||
		   (offset == offsetof(gicr_t, sgi.isenabler0)) ||
		   (offset == offsetof(gicr_t, sgi.icenabler0)) ||
		   (offset == offsetof(gicr_t, sgi.ispendr0)) ||
		   (offset == offsetof(gicr_t, sgi.icpendr0)) ||
		   (offset == offsetof(gicr_t, sgi.isactiver0)) ||
		   (offset == offsetof(gicr_t, sgi.icactiver0))) {
		*val = (uint32_t)vgic_read_irqbits(
			vic, gicr_vcpu, offset - offsetof(gicr_t, sgi),
			offset - offsetof(gicr_t, sgi));

	} else if ((offset == offsetof(gicr_t, sgi.igrpmodr0)) ||
		   (offset == offsetof(gicr_t, sgi.nsacr))) {
		// RAZ/WI because GICD_CTLR.DS==1
		*val = 0U;

	} else if (util_offset_in_range(offset, gicr_t, sgi.ipriorityr)) {
		*val = vgic_read_priority(
			vic, gicr_vcpu,
			offset - offsetof(gicr_t, sgi.ipriorityr), access_size);

	} else if (util_offset_in_range(offset, gicr_t, sgi.icfgr)) {
		*val = vgic_read_config(vic, gicr_vcpu,
					offset - offsetof(gicr_t, sgi.icfgr));

	} else {
		// Unknown register
		GICR_STATUSR_t statusr;
		GICR_STATUSR_init(&statusr);
		GICR_STATUSR_set_RRD(&statusr, true);
		vgic_gicr_rd_set_statusr(gicr_vcpu, statusr, true);
		*val = 0U;
	}

	return ret;
}

static void
gicr_vdevice_icfgr_write(vic_t *vic, thread_t *gicr_vcpu, register_t val)
{
	// 32-bit register, 32-bit access only
	for (index_t i = 0U; i < GIC_PPI_NUM; i++) {
		vgic_gicr_sgi_set_ppi_config(vic, gicr_vcpu, i + GIC_PPI_BASE,
					     (val & util_bit((i * 2U) + 1U)) !=
						     0U);
	}
}

static void
gicr_vdevice_ipriorityr_write(vic_t *vic, thread_t *gicr_vcpu, size_t offset,
			      register_t val, size_t access_size)
{
	// 32-bit registers, byte or 32-bit accessible
	index_t n = (index_t)(offset - OFS_GICR_SGI_IPRIORITYR(0U));
	// Loop through every byte
	uint32_t shifted_val = (uint32_t)val;
	for (index_t i = 0U; i < access_size; i++) {
		vgic_gicr_sgi_set_sgi_ppi_priority(vic, gicr_vcpu, n + i,
						   (uint8_t)shifted_val);
		shifted_val >>= 8U;
	}
}

static void
gicr_vdevice_activer0_write(vic_t *vic, thread_t *gicr_vcpu, size_t offset,
			    register_t val)
{
	// 32-bit registers, 32-bit access only
	uint32_t bits = (uint32_t)val;
	while (bits != 0U) {
		index_t i = compiler_ctz(bits);
		bits &= ~((index_t)util_bit(i));

		vgic_gicr_sgi_change_sgi_ppi_active(
			vic, gicr_vcpu, i,
			(offset == offsetof(gicr_t, sgi.isactiver0)));
	}
}

static void
gicr_vdevice_pendr0_write(vic_t *vic, thread_t *gicr_vcpu, size_t offset,
			  register_t val)
{
	// 32-bit registers, 32-bit access only
	uint32_t bits = (uint32_t)val;
	while (bits != 0U) {
		index_t i = compiler_ctz(bits);
		bits &= ~((index_t)util_bit(i));

		vgic_gicr_sgi_change_sgi_ppi_pending(
			vic, gicr_vcpu, i,
			(offset == offsetof(gicr_t, sgi.ispendr0)));
	}
}

static void
gicr_vdevice_enabler0_write(vic_t *vic, thread_t *gicr_vcpu, size_t offset,
			    register_t val)
{
	// 32-bit registers, 32-bit access only
	uint32_t bits = (uint32_t)val;
	while (bits != 0U) {
		index_t i = compiler_ctz(bits);
		bits &= ~((index_t)util_bit(i));

		vgic_gicr_sgi_change_sgi_ppi_enable(
			vic, gicr_vcpu, i,
			(offset == offsetof(gicr_t, sgi.isenabler0)));
	}
}

static void
gicr_vdevice_igroupr0_write(vic_t *vic, thread_t *gicr_vcpu, register_t val)
{
	// 32-bit register, 32-bit access only
	for (index_t i = 0U; i < 32U; i++) {
		vgic_gicr_sgi_set_sgi_ppi_group(vic, gicr_vcpu, i,
						(val & util_bit(i)) != 0U);
	}
}

#if VGIC_HAS_LPI
static void
gicr_vdevice_invallr_write(vic_t *vic, thread_t *gicr_vcpu, register_t val)
{
	GICR_INVALLR_t invallr = GICR_INVALLR_cast(val);
	// WI if the virtual bit is set
	if (!GICR_INVALLR_get_V(&invallr)) {
		vgic_gicr_rd_invall(vic, gicr_vcpu);
	}
}

static void
gicr_vdevice_invlpir_write(vic_t *vic, thread_t *gicr_vcpu, register_t val)
{
	GICR_INVLPIR_t invlpir = GICR_INVLPIR_cast(val);
	// WI if the virtual bit is set
	if (!GICR_INVLPIR_get_V(&invlpir)) {
		vgic_gicr_rd_invlpi(vic, gicr_vcpu,
				    GICR_INVLPIR_get_pINTID(&invlpir));
	}
}
#endif

static bool
gicr_vdevice_write(vic_t *vic, thread_t *gicr_vcpu, size_t offset,
		   register_t val, size_t access_size)
{
	bool ret = true;

	VGIC_TRACE(GICR_WRITE, vic, gicr_vcpu,
		   "GICR_WRITE reg = {:x}, val = {:#x}", offset, val);

	if (offset == offsetof(gicr_t, rd.ctlr)) {
		vgic_gicr_rd_set_control(vic, gicr_vcpu,
					 GICR_CTLR_cast((uint32_t)val));

	} else if ((offset == offsetof(gicr_t, rd.iidr)) ||
		   (offset == offsetof(gicr_t, rd.typer)) ||
		   (offset == offsetof(gicr_t, rd.syncr)) ||
		   (offset == offsetof(gicr_t, PIDR2))) {
		// RO registers
		GICR_STATUSR_t statusr;
		GICR_STATUSR_init(&statusr);
		GICR_STATUSR_set_WROD(&statusr, true);
		vgic_gicr_rd_set_statusr(gicr_vcpu, statusr, true);

	} else if (offset == offsetof(gicr_t, rd.statusr)) {
		GICR_STATUSR_t statusr = GICR_STATUSR_cast((uint32_t)val);
		vgic_gicr_rd_set_statusr(gicr_vcpu, statusr, false);

	} else if (offset == offsetof(gicr_t, rd.waker)) {
		vgic_gicr_rd_set_sleep(
			vic, gicr_vcpu,
			GICR_WAKER_get_ProcessorSleep(
				&GICR_WAKER_cast((uint32_t)val)));

	} else if ((offset == offsetof(gicr_t, rd.setlpir)) ||
		   (offset == offsetof(gicr_t, rd.clrlpir))) {
		// Direct LPIs not implemented, WI
		//
		// Implementing these is strictly required by the GICv3 spec
		// when the VCPU has LPI support but no ITS. We define that to
		// be a configuration error in VM provisioning.

#if VGIC_HAS_LPI
	} else if (offset == offsetof(gicr_t, rd.propbaser)) {
		vgic_gicr_rd_set_propbase(vic, GICR_PROPBASER_cast(val));

	} else if (offset == offsetof(gicr_t, rd.pendbaser)) {
		vgic_gicr_rd_set_pendbase(vic, gicr_vcpu,
					  GICR_PENDBASER_cast(val));

	} else if (offset == offsetof(gicr_t, rd.invlpir)) {
		gicr_vdevice_invlpir_write(vic, gicr_vcpu, val);

	} else if (offset == offsetof(gicr_t, rd.invallr)) {
		gicr_vdevice_invallr_write(vic, gicr_vcpu, val);

#endif // VGIC_HAS_LPI

	} else if (offset == offsetof(gicr_t, sgi.igroupr0)) {
		gicr_vdevice_igroupr0_write(vic, gicr_vcpu, val);

	} else if ((offset == offsetof(gicr_t, sgi.isenabler0)) ||
		   (offset == offsetof(gicr_t, sgi.icenabler0))) {
		gicr_vdevice_enabler0_write(vic, gicr_vcpu, offset, val);

	} else if ((offset == offsetof(gicr_t, sgi.ispendr0)) ||
		   (offset == offsetof(gicr_t, sgi.icpendr0))) {
		gicr_vdevice_pendr0_write(vic, gicr_vcpu, offset, val);

	} else if ((offset == offsetof(gicr_t, sgi.isactiver0)) ||
		   (offset == offsetof(gicr_t, sgi.icactiver0))) {
		gicr_vdevice_activer0_write(vic, gicr_vcpu, offset, val);

	} else if ((offset >= OFS_GICR_SGI_IPRIORITYR(0U)) &&
		   (offset <=
		    OFS_GICR_SGI_IPRIORITYR(GIC_PPI_BASE + GIC_PPI_NUM - 1))) {
		gicr_vdevice_ipriorityr_write(vic, gicr_vcpu, offset, val,
					      access_size);

	} else if (offset == OFS_GICR_SGI_ICFGR(0U)) {
		// All interrupts in this register are SGIs, which are always
		// edge-triggered, so it is entirely WI

	} else if (offset == OFS_GICR_SGI_ICFGR(1U)) {
		gicr_vdevice_icfgr_write(vic, gicr_vcpu, val);

	} else if (offset == offsetof(gicr_t, sgi.igrpmodr0)) {
		// WI

	} else if (offset == offsetof(gicr_t, sgi.nsacr)) {
		// WI

	}
#if VGIC_HAS_EXT_IRQS
#error extended PPI support not implemented
#endif
	else {
		// Unknown register
		GICR_STATUSR_t statusr;
		GICR_STATUSR_init(&statusr);
		GICR_STATUSR_set_WRD(&statusr, true);
		vgic_gicr_rd_set_statusr(gicr_vcpu, statusr, true);
		ret = false;
	}

	return ret;
}

static bool
gicr_access_allowed(size_t size, size_t offset)
{
	bool ret;

	// First check if the access is size-aligned
	if ((offset & (size - 1U)) != 0UL) {
		ret = false;
	} else if (size == sizeof(uint64_t)) {
		ret = ((offset == offsetof(gicr_t, rd.invallr)) ||
		       (offset <= offsetof(gicr_t, rd.invlpir)) ||
		       (offset == offsetof(gicr_t, rd.pendbaser)) ||
		       (offset == offsetof(gicr_t, rd.propbaser)) ||
		       (offset == offsetof(gicr_t, rd.setlpir)) ||
		       (offset == offsetof(gicr_t, rd.clrlpir)) ||
		       (offset == offsetof(gicr_t, rd.typer)));
	} else if (size == sizeof(uint32_t)) {
		// Word accesses, always allowed
		ret = true;
	} else if (size == sizeof(uint16_t)) {
		// Half-word accesses are not allowed for GICR registers
		ret = false;
	} else if (size == sizeof(uint8_t)) {
		// Byte accesses are only allowed for priority registers
		ret = (((offset >= OFS_GICR_SGI_IPRIORITYR(0U)) &&
			(offset <= OFS_GICR_SGI_IPRIORITYR(31U))));
	} else {
		// Invalid access size
		ret = false;
	}

	return ret;
}

static vcpu_trap_result_t
vgic_handle_gicd_access(vic_t *vic, size_t offset, size_t access_size,
			register_t *value, bool is_write)
{
	bool access_ok = false;

	if (gicd_access_allowed(access_size, offset)) {
		if (is_write) {
			access_ok = gicd_vdevice_write(vic, offset, *value,
						       access_size);
		} else {
			access_ok = gicd_vdevice_read(vic, offset, value,
						      access_size);
		}
	}
	return access_ok ? VCPU_TRAP_RESULT_EMULATED : VCPU_TRAP_RESULT_FAULT;
}

static vcpu_trap_result_t
vgic_handle_gicr_access(vic_t *vic, thread_t *thread, size_t offset,
			size_t access_size, register_t *value, bool is_write,
			bool last_gicr)
{
	bool access_ok = false;

	if (gicr_access_allowed(access_size, offset)) {
		if (is_write) {
			access_ok = gicr_vdevice_write(vic, thread, offset,
						       *value, access_size);
		} else {
			access_ok = gicr_vdevice_read(vic, thread,
						      thread->vgic_gicr_index,
						      offset, value,
						      access_size, last_gicr);
		}
	}

	return access_ok ? VCPU_TRAP_RESULT_EMULATED : VCPU_TRAP_RESULT_FAULT;
}

vcpu_trap_result_t
vgic_handle_vdevice_access(vdevice_type_t type, vdevice_t *vdevice,
			   size_t offset, size_t access_size, register_t *value,
			   bool is_write)
{
	assert(vdevice != NULL);

	vcpu_trap_result_t ret;

	if (type == VDEVICE_TYPE_VGIC_GICD) {
		vic_t *vic = vic_container_of_gicd_device(vdevice);
		ret = vgic_handle_gicd_access(vic, offset, access_size, value,
					      is_write);
	} else {
		assert(type == VDEVICE_TYPE_VGIC_GICR);
		thread_t *gicr_vcpu =
			thread_container_of_vgic_gicr_device(vdevice);
		vic_t *vic = gicr_vcpu->vgic_vic;
		assert(vic != NULL);
		ret = vgic_handle_gicr_access(vic, gicr_vcpu, offset,
					      access_size, value, is_write,
					      gicr_vcpu->vgic_gicr_device_last);
	}

	return ret;
}

vcpu_trap_result_t
vgic_handle_vdevice_access_fixed_addr(vmaddr_t ipa, size_t access_size,
				      register_t *value, bool is_write)
{
	vcpu_trap_result_t ret;

	thread_t *thread = thread_get_self();
	vic_t	 *vic	 = thread->vgic_vic;

	if ((vic == NULL) || !vic->allow_fixed_vmaddr) {
		ret = VCPU_TRAP_RESULT_UNHANDLED;
	} else if ((ipa >= PLATFORM_GICD_BASE) &&
		   (ipa < PLATFORM_GICD_BASE + 0x10000U)) {
		size_t offset = (size_t)(ipa - PLATFORM_GICD_BASE);
		ret = vgic_handle_gicd_access(vic, offset, access_size, value,
					      is_write);
	} else if ((ipa >= PLATFORM_GICR_BASE) &&
		   (ipa < PLATFORM_GICR_BASE + ((vmaddr_t)PLATFORM_MAX_CORES
						<< GICR_STRIDE_SHIFT))) {
		index_t gicr_num = (index_t)((ipa - PLATFORM_GICR_BASE) >>
					     GICR_STRIDE_SHIFT);
		if ((vic != NULL) && (gicr_num < vic->gicr_count)) {
			rcu_read_start();

			thread_t *gicr_vcpu =
				vgic_get_thread_by_gicr_index(vic, gicr_num);

			if (gicr_vcpu != NULL) {
				bool is_last =
					(gicr_num == (vic->gicr_count - 1U)) ||
					(atomic_load_relaxed(
						 &vic->gicr_vcpus[gicr_num +
								  1U]) == NULL);
				vmaddr_t gicr_base =
					((vmaddr_t)PLATFORM_GICR_BASE +
					 ((vmaddr_t)gicr_num
					  << GICR_STRIDE_SHIFT));
				size_t offset = (size_t)(ipa - gicr_base);
				ret	      = vgic_handle_gicr_access(
					  vic, gicr_vcpu, offset, access_size,
					  value, is_write, is_last);
			} else {
				ret = VCPU_TRAP_RESULT_UNHANDLED;
			}

			rcu_read_finish();
		} else {
			ret = VCPU_TRAP_RESULT_UNHANDLED;
		}
	} else {
		ret = VCPU_TRAP_RESULT_UNHANDLED;
	}

	return ret;
}

```

`hyp/vm/vgic/src/vgic.c`:

```c
// © 2023 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <hyptypes.h>

#include <vgic.h>
#include <vic.h>

vic_t *
vic_get_vic(const thread_t *vcpu)
{
	return vcpu->vgic_vic;
}

const platform_mpidr_mapping_t *
vgic_get_mpidr_mapping(const vic_t *vic)
{
	return &vic->mpidr_mapping;
}

```

`hyp/vm/vgic/src/vpe.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>

#include <atomic.h>
#include <scheduler.h>
#include <vcpu.h>

#include "event_handlers.h"
#include "gicv3.h"
#include "internal.h"

#if VGIC_HAS_LPI && GICV3_HAS_VLPI

error_t
vgic_handle_thread_context_switch_pre(void)
{
	thread_t *current = thread_get_self();

	if (current->vgic_vic != NULL) {
		bool expects_wakeup = vcpu_expects_wakeup(current);
		if (gicv3_vpe_deschedule(expects_wakeup)) {
			scheduler_lock_nopreempt(current);
			vcpu_wakeup(current);
			scheduler_unlock_nopreempt(current);
		}
	}

	return OK;
}

void
vgic_handle_thread_load_state_vpe(void)
{
	thread_t *current = thread_get_self();

	if (current->vgic_vic != NULL) {
		vgic_vpe_schedule_current();
	}
}

// Deschedule the vPE while blocked in EL2 / EL3.
//
// Note that vgic_vpe_schedule_current() is directly registered as both the
// unwinder for this event and the handler for vcpu_block_finish.
bool
vgic_handle_vcpu_block_start(void)
{
	bool wakeup = false;

	if (gicv3_vpe_deschedule(true)) {
		wakeup = true;
		vgic_vpe_schedule_current();
	}

	return wakeup;
}

void
vgic_vpe_schedule_current(void)
{
	thread_t *current = thread_get_self();
	assert(current->kind == THREAD_KIND_VCPU);

	assert(current->vgic_vic != NULL);

	// While it is not especially clear from the spec, it seems that
	// these two enable bits must be set specifically to the GICD_CTLR
	// enable bits, without being masked by the ICV bits.
	//
	// This is because GIC-700 has been observed dropping any
	// vSGI targeted to a disabled group on a scheduled vPE, and
	// might do so for vLPIs too. This is allowed for a group
	// disabled by GICD_CTLR, but not for a group disabled by
	// ICV_IGRPEN*.
	GICD_CTLR_DS_t gicd_ctlr =
		atomic_load_acquire(&current->vgic_vic->gicd_ctlr);
	gicv3_vpe_schedule(GICD_CTLR_DS_get_EnableGrp0(&gicd_ctlr),
			   GICD_CTLR_DS_get_EnableGrp1(&gicd_ctlr));
}

vcpu_trap_result_t
vgic_vpe_handle_vcpu_trap_wfi(void)
{
	// FIXME:
	return gicv3_vpe_check_wakeup(true) ? VCPU_TRAP_RESULT_RETRY
					    : VCPU_TRAP_RESULT_UNHANDLED;
}

bool
vgic_vpe_handle_vcpu_pending_wakeup(void)
{
	return gicv3_vpe_check_wakeup(false);
}

#endif // VGIC_HAS_LPI && GICV3_HAS_VLPI

```

`hyp/vm/vgic/vgic.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

module vgic

subscribe boot_hypervisor_start()
	require_preempt_disabled

subscribe boot_cpu_warm_init()
	require_preempt_disabled

subscribe rootvm_init
	// Run early so other modules can bind VIRQs. Must run after PSCI,
	// which is priority 10.
	priority 1

subscribe rootvm_init_late(root_thread, hyp_env)

subscribe object_create_vic
	priority last

subscribe object_activate_vic

subscribe object_deactivate_vic

subscribe object_cleanup_vic(vic)

subscribe object_create_hwirq

subscribe object_deactivate_hwirq

subscribe object_create_thread

subscribe object_activate_thread
	unwinder vgic_unwind_object_activate_thread(thread)
	// Run early so other modules (timer, etc) can bind to virtual PPIs.
	// Must run after PSCI, which is priority 10.
	priority 1

subscribe object_deactivate_thread

subscribe object_cleanup_thread(thread)

subscribe irq_received[HWIRQ_ACTION_VGIC_FORWARD_SPI]
	handler vgic_handle_irq_received_forward_spi(hwirq)

subscribe irq_received[HWIRQ_ACTION_VGIC_MAINTENANCE]
	handler vgic_handle_irq_received_maintenance()
	require_preempt_disabled

subscribe ipi_received[IPI_REASON_VGIC_ENABLE]
	handler vgic_handle_ipi_received_enable()
	require_preempt_disabled

subscribe ipi_received[IPI_REASON_VGIC_SYNC]
	handler vgic_handle_ipi_received_sync()
	require_preempt_disabled

subscribe ipi_received[IPI_REASON_VGIC_DELIVER]
	handler vgic_handle_ipi_received_deliver()
	require_preempt_disabled

subscribe ipi_received[IPI_REASON_VGIC_SGI]
	handler vgic_handle_ipi_received_sgi()
	require_preempt_disabled

subscribe vic_bind_hwirq[HWIRQ_ACTION_VGIC_FORWARD_SPI]
	handler vgic_bind_hwirq_spi(vic, hwirq, virq)

subscribe vic_unbind_hwirq[HWIRQ_ACTION_VGIC_FORWARD_SPI]
	handler vgic_unbind_hwirq_spi(hwirq)

subscribe virq_set_enabled[VIRQ_TRIGGER_VGIC_FORWARDED_SPI]
	handler vgic_handle_virq_set_enabled_hwirq_spi(source, enabled)

subscribe virq_set_mode[VIRQ_TRIGGER_VGIC_FORWARDED_SPI]
	handler vgic_handle_virq_set_mode_hwirq_spi(source, mode)

subscribe thread_save_state

subscribe thread_context_switch_post(prev)
	// Lowered priority so default priority handlers can restore state
	// that affects virq_check_pending handlers
	priority -100

subscribe thread_load_state()

subscribe scheduler_affinity_changed(thread, next_cpu)

subscribe addrspace_attach_vdevice

subscribe vdevice_access[VDEVICE_TYPE_VGIC_GICD]

subscribe vdevice_access[VDEVICE_TYPE_VGIC_GICR]

subscribe vdevice_access_fixed_addr
	// Raise priority as this is more likely to be performance-critical
	// than other vdevices. This can be removed once we have proper
	// device-kind tracking for vdevices
	priority 1

subscribe vcpu_trap_sysreg_read

subscribe vcpu_trap_sysreg_write
	// Run early, because SGI1R is the most performance-critical sysreg
	// write trap
	priority 100

subscribe vcpu_trap_wfi()
	// Run early, in case we need to trigger a wakeup
	priority 100

subscribe vcpu_pending_wakeup

subscribe vcpu_stopped()

#if VGIC_HAS_LPI && GICV3_HAS_VLPI

// When switching away from a VCPU, deschedule it in the GICR.
subscribe thread_context_switch_pre()
	// Run early, to give vPE descheduling time to complete during the
	// rest of the context switch.
	priority 100
	require_preempt_disabled

// When switching to a VCPU, schedule it in the GICR.
subscribe thread_load_state
	handler vgic_handle_thread_load_state_vpe()
	// Run early, to give vPE scheduling time to complete before we return
	// to userspace.
	priority 100
	require_preempt_disabled

// When blocking for the VCPU, mark the vPE as descheduled. This allows us to
// check for pending vLPIs / vSGIs now, and also to receive a doorbell if one
// arrives later.
subscribe vcpu_block_start()
	unwinder vgic_vpe_schedule_current()
	require_preempt_disabled

subscribe vcpu_block_finish
	handler vgic_vpe_schedule_current()
	require_preempt_disabled

subscribe vcpu_pending_wakeup
	handler vgic_vpe_handle_vcpu_pending_wakeup()
	// Run early, because this is a relatively cheap check and might fail
	// repeatedly.
	priority 100
	require_preempt_disabled

subscribe vcpu_trap_wfi
	handler vgic_vpe_handle_vcpu_trap_wfi()
	// Run early, because this is a relatively cheap check and might fail
	// repeatedly.
	priority 200

#endif // VGIC_HAS_LPI && GICV3_HAS_VLPI

```

`hyp/vm/vgic/vgic.hvc`:

```hvc
// © 2022 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

define vgic_set_mpidr_mapping hypercall {
	call_num 0x67;
	vic		input type cap_id_t;
	mask		input uint64;
	aff0_shift	input type count_t;
	aff1_shift	input type count_t;
	aff2_shift	input type count_t;
	aff3_shift	input type count_t;
	mt		input bool;
	error		output enumeration error;
};

```

`hyp/vm/vgic/vgic.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <types/bitmap.h>

// The hardware has to support at least 5 priority bits, but might not support
// more. We constrain the implementation to 5 anyway.
define VGIC_PRIO_BITS constant type count_t = 5;
define VGIC_PRIORITIES constant type count_t = 1 << VGIC_PRIO_BITS;

// Low bits to shift out of priority values.
define VGIC_PRIO_SHIFT constant type count_t = 8 - VGIC_PRIO_BITS;

// Chunks to split the IRQ search bitmap into for the SPIs and PPIs (<1020)
define VGIC_LOW_RANGES constant type count_t = 64;

// Upper bound for virtual IRQ IDs. Note that this can be further constrained
// by both the hardware (ICH_VTR_EL2.IDbits) and the VM configuration.
#if !defined(VGIC_LPI_BITS)
// May be set by the configuration to any value between 14 and 24, to
// constrain VGIC memory usage. Default to 16.
define VGIC_LPI_BITS constant type count_t = 16;
#endif
define VGIC_IDBITS constant type count_t =
	VGIC_HAS_LPI ? VGIC_LPI_BITS : VGIC_HAS_EXT_IRQS ? 13 : 10;

#if VGIC_HAS_LPI
define VGIC_LPI_NUM constant type count_t = (1 << VGIC_IDBITS) - GIC_LPI_BASE;
#if !GICV3_HAS_VLPI
define VGIC_LPI_RANGES constant type count_t = BITMAP_NUM_WORDS(VGIC_LPI_NUM);
#endif
#endif

// Delivery state for VIRQs.
//
// This tracks why, how and where the VIRQ is currently being asserted.
//
// It is independent of both the HW IRQ activation state and the list register
// status, and is maintained for all VIRQs that can be listed in LRs,
// regardless of whether they have registered sources or whether they are
// currently listed.
//
// The bits have fixed assignments solely to avoid breaking existing debug
// tools.
define vgic_delivery_state bitfield<32>(set_ops) {
	// Currently in a list register.
	//
	// If the VIRQ is a forwarded SPI and is marked as such in the list
	// register, it must have valid transitions in and out of HW listed
	// state within the same non-preemptible critical sections that set
	// and clear this flag.
	//
	// If this flag is set, the active and pending flags in the GICD &
	// GICR may be wrong; if the VCPU is running then the active and
	// pending flags below may also be wrong. All flags will be updated by
	// the VCPU that has listed the VIRQ prior to clearing this flag. It
	// may also update them at other times while the listed flag is set.
	//
	// This flag may only be set or cleared, and a list register may only
	// be manipulated, by the VCPU owning the list register in question,
	// or by another thread holding that VPCU's lr_lock while it is not
	// running.
	0		listed		bool;

	// IRQ is a forwarded SPI that is known to be active in hardware. This
	// is set when an SPI assertion is received (regardless of whether it
	// is listed), or a forwarded SPI is delisted with HW=1 and active or
	// pending state. It is cleared when a forwarded SPI is listed with
	// HW=1, or manually deasserted or deactivated.
	11		hw_active	bool;

	// Add GICR index so we don't need to broadcast sync IPIs
	// FIXME:

	// An interrupt has been rerouted, disabled or reasserted while it is
	// listed on a VCPU that is current on a remote CPU. The remote CPU
	// should check the VIRQ and delist, deroute or reassert it, then
	// clear this bit and call asm_event_wake_updated().
	//
	// Note that an EOI maintenance interrupt is handled the same way
	// other than that the routing check is skipped.
	//
	// This must only be set if the listed bit is set.
	1		need_sync	bool;

	// The source of a forwarded SPI has been removed while the VIRQ is
	// listed. The LR must be read back and converted to a SW VIRQ, and
	// the HW SPI must be deactivated if it is still active.
	//
	// Note that this is only used as a trigger. Clearing it does not
	// imply synchronisation: the IRQ may not have been deactivated yet
	// when it clears.
	//
	// This must only be set if the listed bit is set.
	2		hw_detached	bool;

	// Currently enabled. If this false, the IRQ should not be asserted on
	// any VCPU.
	3		enabled		bool;

	// Also migrate priority and route to this bitfield.
	// FIXME:

	// The interrupt is currently active. This is only valid for an
	// unlisted interrupt; for a listed interrupt the active state is in
	// the LR. If set, it inhibits re-listing of the interrupt until it
	// is cleared, either via LRENP maintenance triggered by an EOIR write
	// on the VCPU that activated it, or by a trapped write to DIR or
	// ICATIVER on any VCPU.
	4		active		bool;

	// The ICFGR bit 1 for the VIRQ.
	//
	// If this is set and the listed bit is clear, the VIRQ's pending
	// state is the edge bit in this bitfield.
	//
	// If this is set and the listed bit is also set, the VIRQ's pending
	// state is the union of the edge bit in this bitfield and the pending
	// bit in the corresponding list register.
	//
	// If this is clear, the VIRQ's pending state is the union of the
	// level_sw, level_msg and level_src bits in this bitfield. If the
	// listed bit is set, and all three level bits are clear, then any
	// pending state in the corresponding list register is spurious and
	// may be discarded at the next opportunity.
	5		cfg_is_edge	bool;

	// IRQ was asserted by a write to GIC[DR]_ISPENDR. Must remain set
	// until its bit is set in GIC[DR]_ICPENDR.
	6		level_sw	bool;

	// IRQ was asserted by a write to GICD_SETSPI while level-triggered.
	// Must remain set until its IRQ number is written to GICD_CLRSPI.
	7		level_msg	bool;

	// IRQ was asserted by a call to virq_assert(edge_only==false). Must
	// remain set until virq_clear() is called or a handler for the
	// virq_check_pending event returns false.
	8		level_src	bool;

	// IRQ was asserted for any reason, and the assertion is known not to
	// have been acknowledged by a VCPU. This includes all of the above
	// sources, in addition to SGIs (excluding GICv4.1 vSGIs), LPIs
	// (excluding GICv4 vLPIs), and calls to virq_assert(edge_only==true).
	//
	// This bit is cleared when the listed bit is set, because the VCPU
	// may acknowledge the interrupt at any time after that. If the IRQ is
	// delisted while pending, this bit should be set before the listed
	// bit is cleared. Note that this means that an assertion after the
	// VIRQ is listed but before it is acknowledged will be spuriously
	// delivered; this is an unavoidable consequence of the list register
	// model and applies to hardware edges too.
	//
	// Apart from determining the pending state when cfg_is_edge is true,
	// this bit is also passed to the virq_check_pending event to indicate
	// that the interrupt has been reasserted since it was delivered to
	// the VM.
	9		edge		bool;

	// IRQ is in Group 1 (rather than Group 0).
	//
	// This bit is copied into the LR when the VIRQ is listed. Changes
	// after listing the VIRQ are not guaranteed to take effect (as
	// permitted by the GICv3 specification).
	10		group1		bool;

	// Virtual IRQ priority.
	//
	// This field is copied into the LR when the VIRQ is listed. Changes
	// after listing the VIRQ are not guaranteed to take effect (as
	// permitted by the GICv3 specification).
	<VGIC_PRIO_BITS>:16
			priority	uint8 lsl(VGIC_PRIO_SHIFT);

	// Virtual IRQ route.
	//
	// This field stores the currently configured route for the VIRQ, in
	// the form of a virtual GICR index. If VGIC_HAS_1N is enabled and the
	// route_1n flag is set, this may be ignored if a better route is
	// available. Note that we reserve enough bits to be able to represent
	// an out-of-range route ID.
	<msb(PLATFORM_MAX_CORES)+1>:24
			route		type index_t;

#if VGIC_HAS_1N
	// Virtual IRQ 1-of-N mode.
	//
	// This flag is true if the hypervisor should automatically route the
	// IRQ to a VCPU. For forwarded SPIs, this may be passed through to
	// the physical GIC, if the requirements for that are satisfied.
	15		route_1n	bool;

#if GICV3_HAS_GICD_ICLAR
	// CPU classes for physical 1-of-N routing
	//
	// These flags are used to control which physical CPUs can receive a
	// 1-of-N routed physical SPI. They apply only to forwarded SPIs, and
	// are only implemented if the underlying GICD has GICD_ICLAR<n>
	// registers, which are implementation-defined by GIC-600 and GIC-700.
	//
	// The class 0 bit is inverted so that the reset value of 0 means "class
	// enabled", like the corresponding hardware bit, but the class 1 bit
	// is not inverted. Also, unlike the hardware bits, the classes are not
	// reset to disabled and read-only when route_1n is clear.
	13		nclass0		bool;
	14		class1		bool;
#endif // GICV3_HAS_GICD_ICLAR
#endif // VGIC_HAS_1N
};

extend virq_trigger enumeration {
	vgic_forwarded_spi;
};

extend virq_source structure module vgic {
	// Flag to protect against concurrent binding of the source.
	is_bound	bool(atomic);

	// Index of the virtual CPU for a private binding. Invalid if shared.
	gicr_index	type index_t;
};

// HW IRQ extensions for forwarding through the virtual GIC.
//
// Note that forwarded HW IRQs can only be SPIs. PPIs must be processed by
// other modules and converted to registered VIRQs. LPIs and SGIs will be
// converted to unregistered VIRQs if handled in software.
extend hwirq object module vgic {
	// The VIRQ source for forwarded SPIs. It must have trigger set to
	// VIRQ_TRIGGER_VGIC_FORWARDED_SPI.
	spi_source	structure virq_source(contained);

	// A flag indicating that the HW IRQ can be safely enabled. If it is
	// false, then an unbind is in progress, and the HW IRQ should be kept
	// disabled. This is protected by the gicd_lock of the bound VIC.
	enable_hw	bool;
};

extend hwirq_action enumeration {
	vgic_forward_spi;
	vgic_maintenance;
};

// Virtual GICD state is in the stand-alone vic object.
extend vic object {
	// Lock protecting group, priority and routing changes for all SPIs,
	// and registration & enablement of hardware SPIs. Must not be
	// acquired while holding a GICR's attochment or LR ownership lock.
	gicd_lock		structure spinlock;

	// Virtual device structure representing the GICD registers. If the
	// type is set to VDEVICE_TYPE_NONE, it has not been mapped. Mapping
	// is protected by the gicd lock.
	gicd_device		structure vdevice(contained);

	// The current values of the virtual GICD_CTLR and GICD_STATUSR.
	// Updates are serialised by gicd_lock, but GICD_CTLR is atomic so we
	// can read it without taking the lock.
	gicd_ctlr		bitfield GICD_CTLR_DS(atomic);
	gicd_statusr		bitfield GICD_STATUSR;

	// The size of the vcpus array.
	gicr_count		type count_t;
	// The array of VCPUs attached to this GICD. Protected by gicd_lock.
	// Weak references; pointers cleared by thread deactivate.
	gicr_vcpus		pointer pointer(atomic) object thread;

	// The array of shared VIRQ sources attached to this GICD, indexed by
	// VIRQ number minus GIC_SPI_BASE. The attachment pointers are
	// protected by RCU and will be cleared if the corresponding VIRQs are
	// deleted or detached.
	sources			pointer pointer(atomic) structure virq_source;
	// The sources array size; shared VIRQs can only have registered sources
	// if they have valid SPI numbers with an offset less than this count.
	// This is fixed after VIC creation.
	sources_count		type count_t;

	// Assertion states of the SPIs (including those with no sources).
	spi_states		array(GIC_SPI_NUM)
		bitfield vgic_delivery_state(atomic);

#if VGIC_HAS_1N
	// Round-robin counter for virtual SPI delivery decisions.
	rr_start_point		type count_t(atomic);
#endif

#if VGIC_HAS_LPI
	// Maximum supported ID bits. Between 14 and VIC_IDBITS if LPIs are
	// enabled for this VIC; otherwise 10.
	gicd_idbits		type count_t;

	// Virtual GICR_PROPBASER. As per the spec we are allowed to share a
	// single copy of this across all virtual GICRs, since the table it
	// points to is also shared.
	gicr_rd_propbaser	bitfield GICR_PROPBASER(atomic);

	// Cache of the VLPI configurations.
	//
	// This is a pointer to an array of 8-bit configuration fields for the
	// virtual LPIs. Unlike the pending bitmap, there is no reserved space
	// at the start corresponding to non-LPI IRQ numbers.
	//
	// Updates to this cache are not protected by any lock, based on the
	// assumption that parallel updates will be copying from the same
	// source anyway.
	vlpi_config_table	pointer uint8;

	// Validity flag for the vlpi config table.
	//
	// This is set the first time a VCPU enables VLPIs, to indicate that
	// the initial copy of the config table from the VM has been done.
	// Subsequent copies are triggered by invalidate commands.
	//
	// The GICD lock must be held while querying this, and the lock must
	// be held across the initial copy-in of the config table.
	vlpi_config_valid	bool;

#if GICV3_HAS_VLPI_V4_1
	// True if we can use the ITS to deliver virtual SGIs. This is only
	// possible on GICv4.1, and only if this VIC has VLPI tables. Also,
	// because an ITS-delivered SGI is really an LPI and therefore has no
	// active state, this could potentially break the guest; so the guest
	// must enable it by setting the virtual GICD_CTLR.nASSGIreq to 1.
	vsgis_enabled		bool;
#endif
#endif

	// True if we should handle fixed address vdevice accesses. This is
	// temporary and will be removed once we have migrated away from fixed
	// addresses.
	allow_fixed_vmaddr	bool;

	// Lock to serialise unrouted-IRQ searches. This only needs to be held
	// while searching, not when flagging an IRQ in the search bitmaps.
	search_lock		structure spinlock;

	// Cache of VIRQ number ranges that may need to be checked when
	// searching for previously unrouted VIRQs to be delivered. This
	// search occurs when a VCPU is first attached to a VGIC. If
	// 1-of-N delivery is enabled, it also occurs when a group is enabled.
	//
	// Bits are set in this array after an attempt is made to deliver a
	// corresponding IRQ, but no route can be found for it. The write must
	// be release-ordered after the delivery state update.
	//
	// To search for a pending IRQ, the VCPU must find a set bit in the
	// array, clear it with an atomic load-acquire and re-check that it
	// was set, check all the IRQs in the corresponding range, and reset
	// the bit if at least one unrouted pending IRQ was found, regardless
	// of whether that IRQ was claimed by the VCPU. The search lock must
	// be held, to prevent the transient clearing of a bit hiding an IRQ
	// from a concurrent search.
	search_ranges_low	BITMAP(VGIC_LOW_RANGES, atomic);

	// Mapping between GICR indices and 4-level affinity values for
	// MPIDR, GICR_TYPER etc. If not configured explicitly, this defaults
	// to being the same as the underlying hardware's mapping.
	mpidr_mapping		structure platform_mpidr_mapping;
};

extend cap_rights_vic bitfield {
	1	attach_vcpu	bool;
	2	attach_vdevice	bool;
};

// State tracking for a list register.
define vgic_lr_status structure {
	// The listed VIRQ's delivery state. The listed bit should be set.
	// This pointer is NULL if the list register is free.
	dstate		pointer bitfield vgic_delivery_state(atomic);

	// The content of the LR as it was last written to or read from the
	// hardware. Note that this may be out of date if this LR belongs to
	// the current VCPU on some physical CPU.
	lr		union ICH_LR_EL2;
};

define vgic_lr_owner_lock structure(lockable) {
	owner		type cpu_index_t(atomic);
	lock		structure spinlock;
};

define vgic_sleep_state enumeration {
	awake = 0;
	asleep;
#if VGIC_HAS_1N
	wakeup_1n;
#endif
};

// Virtual GICR state is embedded in the associated thread context.
extend thread object module vgic {
	// Reference-counted pointer to the vic that owns this GICR. This
	// cannot change once the thread is activated.
	vic		pointer object vic;
	// Index of this GICR in the register map and attachment array. This
	// cannot change once the thread is activated; it is checked for
	// uniqueness (i.e. no conflicts with other threads) at activation
	// time.
	gicr_index	type index_t;

	// Virtual device structure representing the GICR registers. If the
	// type is set to VDEVICE_TYPE_NONE, it has not been mapped. Mapping
	// is protected by the gicd lock.
	gicr_device		structure vdevice(contained);

	// Flag indicating whether this GICR is the last in a contiguous range
	// of mapped vdevices. This must be set correctly, because the Linux
	// GIC driver relies on it to terminate GICR iterations; it does not
	// check the sizes of the redistributor regions.
	//
	// This is only used for accesses through gicr_device; fixed access
	// uses a hard-coded index check.
	gicr_device_last	bool;

	// Physical route register that should be used to target this thread.
	irouter		bitfield GICD_IROUTER;

	// The array of private VIRQ sources attached to this GICR, indexed by
	// VIRQ number minus GIC_PPI_BASE. The attachment pointers are protected
	// by RCU and will be cleared if the corresponding VIRQs are deleted
	// or detached.
	sources			array(GIC_PPI_NUM) pointer(atomic)
			structure virq_source;

	// Assertion states of the SGIs and PPIs. These are only accessed
	// atomically.
	private_states		array(GIC_SGI_NUM + GIC_PPI_NUM)
		bitfield vgic_delivery_state(atomic);

	// The GICR's rd_base register state.
	gicr_rd_statusr		bitfield GICR_STATUSR(atomic);
#if VGIC_HAS_LPI
	gicr_rd_ctlr		bitfield GICR_CTLR(atomic);
	gicr_rd_pendbaser	bitfield GICR_PENDBASER(atomic);
#endif

	// The ICH EL2 register state. Parts of these are exposed to userspace
	// through the ICV EL1 registers. These are accessed only by the owner
	// thread.
	ich_hcr		bitfield ICH_HCR_EL2;
	ich_vmcr	bitfield ICH_VMCR_EL2;

	// Active priority registers.
	//
	// These are used only for context switching. Writing any value other
	// than 0 or the context-saved value has UNPREDICTABLE behaviour. (In
	// fact, the manual says that writing any value other than 0 or the
	// last value read has UNPREDICTABLE behaviour, but that makes safe
	// context switching impossible, so we assume it's a bad copy-paste
	// from the ICC documentation.)
	ap0rs		array(CPU_GICH_APR_COUNT) uint32;
	ap1rs		array(CPU_GICH_APR_COUNT) uint32;

	// Current state of each of the LRs.
	//
	// Updates to these status structures are protected by a combination
	// of lr_lock and lr_owner. Specifically, the thread acquires lr_lock
	// and sets lr_owner to the physical CPU index during context load,
	// then resets lr_owner to CPU_INDEX_INVALID during context save.
	//
	// Between context switches, the thread that owns this GICR may change
	// the LR state without acquiring the lock, but must update it in the
	// physical LRs as well.
	//
	// Any other thread wishing to update these structures must acquire
	// lr_lock _and_ check that lr_owner is invalid before changing the
	// status; simply holding lr_lock is never enough because the physical
	// LRs cannot be remotely updated (and also because the GICR's thread
	// is allowed to update without holding the lock). If lr_owner is
	// valid, the thread must signal the GICR's owner to perform the
	// desired update rather than doing it directly. For deliveries, this
	// is done with the deliver IPI and the search range flags below; for
	// undeliveries it is done with the need-sync flag in the dstate.
	//
	// The lr_owner variable may also be read without holding the lock, to
	// determine where an IPI targeting this vCPU should be sent, if
	// anywhere. Such a read must be ordered after any delivery state
	// changes with a seq_cst fence.
	lrs		array(CPU_GICH_LR_COUNT) structure vgic_lr_status;
	lr_owner_lock	structure vgic_lr_owner_lock;

	// Current group enable states of the virtual GICR. These are not
	// directly visible to the VCPU, but are kept consistent with the
	// virtual GICD_CTLR and ICV_IGRPEN[01]_EL1.
	//
	// As required by the spec, clearing one of these flags will return
	// all 1-of-N IRQs to the distributor for rerouting; i.e. they will be
	// delisted and kept out of the LRs. To simplify the implementation of
	// both these bits and GICR_WAKER (see below), we do that for all
	// IRQs, 1-of-N or otherwise.
	//
	// There may be a lag between ICV_IGRPEN[01]_EL1 being cleared and
	// the maintenance interrupt showing up to tell us to update these
	// flags. That is OK because the 1-of-N rerouting does not have to be
	// instant, and the other effects are implemented in hardware. The
	// only time we must check that these flags are up-to-date is when a VM
	// reads GICR_WAKER.ChildrenAsleep while ProcessorSleep is set.
	//
	// These flags are protected by the LR lock.
	group0_enabled	bool;
	group1_enabled	bool;

	// Current GICR_WAKER.ProcessorSleep state.
	//
	// The effects of setting this flag are:
	// - enabling selection of the VCPU for a 1-of-N wakeup
	// - short-circuiting delivery checks to wake up without listing
	// - polling group enables on GICR_WAKER.ChildrenAsleep reads
	// - not polling VPE scheduling on GICR_WAKER.ChildrenAsleep reads
	//
	// The spec requires that setting this flag disables all IRQ delivery
	// and returns all IRQs to the distributor. However, it also says that
	// setting this flag when either of the group enables is set has
	// UNPREDICTABLE behaviour, and we already return all IRQs to the
	// distributor when groups are disabled (see above).
	//
	// Therefore it is within spec for this flag to have no direct effect
	// on interrupt delivery. The advantage of doing this is that interrupt
	// delivery will work in VMs that assume they don't have control of
	// GICR_WAKER, like UEFI, as long as they don't rely on 1-of-N wakeup.
	//
	// When 1-of-N support is enabled, this flag has a third state that
	// indicates that it is asleep, but has been chosen for 1-of-N wakeup.
	sleep		enumeration vgic_sleep_state(atomic);

	// Cache of VIRQ numbers that may need to be checked when searching
	// for a pending IRQ to deliver.
	//
	// Bits are set in these arrays after an attempt is made to deliver a
	// corresponding IRQ to the VCPU that fails to immediately list it, or
	// when an IRQ is kicked out of the list registers due to low priority.
	//
	// To search for a pending IRQ of a specific priority, the VCPU must
	// find a set bit in that priority's element in the array, clear it
	// with an atomic load-acquire and re-check that it was set, check all
	// the IRQs in the corresponding range, and reset the bit if the
	// search did not fail (regardless of whether the IRQ found was
	// successfully delivered).
	//
	// Reads of this bitmap when searching must be acquire-ordered before
	// reads of the delivery states. Writes of this bitmap when flagging
	// must be release-ordered after writes of the delivery states.
	search_ranges_low	array(VGIC_PRIORITIES)
		BITMAP(VGIC_LOW_RANGES, atomic);

#if VGIC_HAS_LPI && !GICV3_HAS_VLPI
	// Extended search ranges for software-implemented LPIs.
	search_ranges_lpi	array(VGIC_PRIORITIES)
		BITMAP(VGIC_LPI_RANGES, atomic);
#error Software VLPIs are not implemented yet
#elif VGIC_HAS_LPI && GICV3_HAS_VLPI
	// Cache of the VLPI pending state.
	//
	// This is a pointer to a bitmap with 1 << gicd_idbits bits if LPIs
	// are supported by this VIC, or otherwise NULL. It must be
	// 64k-aligned, due to the 16-bit shift of the physical base in the
	// VMAPP command.
	//
	// The contents of this table are undefined when EnableLPIs is false in
	// gicr_rd_ctlr. When that bit is set to true, this table is either
	// entirely zeroed, or else partially copied from VM memory and the
	// remainder zeroed, depending on the PTZ bit of gicr_rd_pendbaser.
	vlpi_pending_table	pointer uint8;

	// Sequence number of the vPE unmap. Valid only after the VCPU is
	// deactivated while its EnableLPIs bit is true.
	vlpi_unmap_seq		type count_t;

#if GICV3_HAS_VLPI_V4_1
	// Sequence number of the VSYNC following the initial VSGI setup.
	// If this is set to ~0, the VSGI setup commands have not all been
	// enqueued in the ITS yet, and software delivery must be used; if it
	// is set to 0, the setup commands are known to have completed, and
	// polling the ITS is not necessary. Otherwise, software delivery must
	// be used until the specified sequence number is complete.
	vsgi_setup_seq		type count_t(atomic);

	// Sequence number of the VSYNC following the most recent VSGI clear
	// enable operation. This is used to set GICR_CTLR.RWP on trapped reads.
	vsgi_disable_seq	type count_t;
#endif
#endif

	// Cache of shifted priorities that may have nonzero bits in their
	// search ranges.
	//
	// To select a priority to search for, the VCPU must find the least
	// significant set bit in the array, clear it, search for an interrupt
	// of the corresponding priority, and reset the bit if one was found.
	//
	// To search for a pending IRQ of unknown priority, the VCPU must find
	// a set bit in the bitmap, clear it, search the corresponding bitmap
	// in the search_ranges array, and then reset the bit unless the search
	// checked and cleared every bit in the search ranges bitmap.
	//
	// Reads of this bitmap when searching must be acquire-ordered before
	// reads of the search ranges bitmap. Writes of this bitmap when
	// flagging must be release-ordered after writes of the search ranges
	// bitmap.
	search_prios		BITMAP(VGIC_PRIORITIES, atomic);

	// Record of active interrupts that were kicked out of the list
	// registers due to higher-priority pending interrupts.
	//
	// If the VM has EOImode set to 0, this functions as a stack, which is
	// always sorted with highest priority at the top; we know this
	// because we always kick out the lowest priority listed active
	// interrupt, and interrupts can't enter the active state if they are
	// lower-priority than anything already on the stack. This allows us
	// to handle EOIcount!=0 interrupts by popping from the stack.
	//
	// If the VM has EOImode is set to 1, this functions as a write-only
	// ring. We do this because VM EOImode==1 is slow anyway (it has to
	// trap every ICV_DIR_EL1 write) and is rarely used (it's only useful
	// if the VM's kernel forwards IRQs to EL0), and changes to EOImode
	// can't be selectively trapped, so writing unconditionally is better
	// than checking EOImode all the time. Note that VGIC_PRIORITIES is a
	// power of two, so indexing modulo VGIC_PRIORITIES is cheap.
	active_unlisted		array(VGIC_PRIORITIES) type virq_t;
	active_unlisted_count	type count_t;

	// Bitmap of SGIs pending delivery.
	pending_sgis		BITMAP(GIC_SGI_NUM, atomic);
};

extend ipi_reason enumeration {
	vgic_enable;
	vgic_sync;
	vgic_deliver;
	vgic_sgi;
};

extend hyp_env_data structure {
	vic		type cap_id_t;
	gicd_base	type paddr_t;
	gicr_base	type paddr_t;
	gicr_stride	size;

	gits_base	type paddr_t;
	gits_stride	size;
};

#if GICV3_HAS_GICD_ICLAR
define GICD_SETCLASSR bitfield<32> {
	12:0		SPI	type virq_t;
	30		Class0	bool;
	31		Class1	bool;
	others		unknown=0;
};

define OFS_GICD_SETCLASSR constant size = 0x28;
#endif // GICV3_HAS_GICD_ICLAR

define VGIC_PIDR2 constant uint32 = 0x30; // GICv3

define vgic_irq_type enumeration {
	sgi;
	ppi;
	spi;
#if VGIC_HAS_EXT_IRQS
	ppi_ext;
	spi_ext;
#endif
#if VGIC_HAS_LPI
	lpi;
#endif
	reserved;
};

extend trace_class enumeration {
	VGIC = 17;
	VGIC_DEBUG = 18;
};

extend trace_id enumeration {
	VGIC_VIRQ_CHANGED = 0x20;
	VGIC_DSTATE_CHANGED = 0x21;
	VGIC_HWSTATE_CHANGED = 0x22;
	VGIC_HWSTATE_UNCHANGED = 0x23;
	VGIC_GICD_WRITE = 0x24;
	VGIC_GICR_WRITE = 0x25;
	VGIC_SGI = 0x26;
	VGIC_ROUTE = 0x28;
	VGIC_ICC_WRITE = 0x29;
	VGIC_ASYNC_EVENT = 0x2a;
};

extend vdevice_type enumeration {
	vgic_gicd;
	vgic_gicr;
};

define vgic_gicr_attach_flags public bitfield<64> {
	0		last_valid	bool;
	1		last		bool;
	others		unknown=0;
};

extend addrspace_attach_vdevice_flags union {
	vgic_gicr	bitfield vgic_gicr_attach_flags;
};

```

`hyp/vm/vic_base/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

base_module hyp/platform/gicv3
interface virq
interface vic
local_include
source hypercalls.c forward_private.c
types vic_base.tc
events vic_base.ev

```

`hyp/vm/vic_base/include/vic_base.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

// Internal vic module functions that are called by vic_base

// Configure a new virtual interrupt controller object.
error_t
vic_configure(vic_t *vic, count_t max_vcpus, count_t max_virqs,
	      count_t max_msis, bool allow_fixed_vmaddr);

// Attach a new VCPU to an active virtual interrupt controller object.
error_t
vic_attach_vcpu(vic_t *vic, thread_t *vcpu, index_t index);

#if VIC_BASE_FORWARD_PRIVATE
// bind PPI
error_t
vic_bind_private_forward_private(virq_source_t *source, vic_t *vic,
				 thread_t *vcpu, virq_t virq);

void
vic_sync_private_forward_private(virq_source_t *source, vic_t *vic,
				 thread_t *vcpu, virq_t virq, irq_t pirq,
				 cpu_index_t pcpu);

#endif // VIC_BASE_FORWARD_PRIVATE

```

`hyp/vm/vic_base/src/forward_private.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#if VIC_BASE_FORWARD_PRIVATE

#include <assert.h>
#include <hyptypes.h>
#include <string.h>

#include <hypcontainers.h>

#include <atomic.h>
#include <compiler.h>
#include <cpulocal.h>
#include <irq.h>
#include <list.h>
#include <log.h>
#include <object.h>
#include <partition.h>
#include <platform_irq.h>
#include <rcu.h>
#include <scheduler.h>
#include <spinlock.h>
#include <thread.h>
#include <trace.h>
#include <util.h>
#include <vic.h>
#include <virq.h>

#include <events/virq.h>

#include "event_handlers.h"
#include "panic.h"
#include "vic_base.h"

static vic_private_irq_info_t *
private_irq_info_from_virq_source(virq_source_t *source)
{
	assert(source != NULL);
	assert(source->trigger == VIRQ_TRIGGER_VIC_BASE_FORWARD_PRIVATE);

	return vic_private_irq_info_container_of_source(source);
}

// Called with the forward-private lock held.
static error_t
vic_bind_private_hwirq_helper(vic_forward_private_t *fp, thread_t *vcpu)
{
	error_t	    err;
	cpu_index_t cpu;

	assert(vcpu->vic_base_forward_private_active);

	if (!vcpu_option_flags_get_pinned(&vcpu->vcpu_options)) {
		err = ERROR_DENIED;
		goto out;
	}

	scheduler_lock(vcpu);
	cpu = vcpu->scheduler_affinity;
	scheduler_unlock(vcpu);

	assert(cpulocal_index_valid(cpu));

	vic_private_irq_info_t *irq_info = &fp->irq_info[cpu];

	err = vic_bind_private_forward_private(&irq_info->source, fp->vic, vcpu,
					       fp->virq);

	if ((err == OK) && vcpu->vic_base_forward_private_in_sync) {
		vic_sync_private_forward_private(&irq_info->source, fp->vic,
						 vcpu, fp->virq, irq_info->irq,
						 cpu);
	}

out:
	return err;
}

// Called with the forward-private lock held.
static void
vic_unbind_private_hwirq_helper(hwirq_t *hwirq)
{
	assert(hwirq->action == HWIRQ_ACTION_VIC_BASE_FORWARD_PRIVATE);

	vic_forward_private_t *fp = atomic_exchange_explicit(
		&hwirq->vic_base_forward_private, NULL, memory_order_consume);
	if (fp != NULL) {
		vic_t *vic = fp->vic;
		assert(vic != NULL);

		spinlock_acquire(&vic->forward_private_lock);

		for (cpu_index_t i = 0U; i < PLATFORM_MAX_CORES; i++) {
			vic_unbind(&fp->irq_info[i].source);
		}

		(void)list_delete_node(&vic->forward_private_list,
				       &fp->list_node);

		spinlock_release(&vic->forward_private_lock);

		rcu_enqueue(&fp->rcu_entry,
			    RCU_UPDATE_CLASS_VIC_BASE_FREE_FORWARD_PRIVATE);
	}
}

// Called with the forward-private lock held.
static void
vic_sync_private_hwirq_helper(vic_forward_private_t *fp, thread_t *vcpu)
{
	assert(vcpu->vic_base_forward_private_active);
	assert(vcpu_option_flags_get_pinned(&vcpu->vcpu_options));

	scheduler_lock(vcpu);
	cpu_index_t cpu = vcpu->scheduler_affinity;
	scheduler_unlock(vcpu);

	assert(cpulocal_index_valid(cpu));

	vic_private_irq_info_t *irq_info = &fp->irq_info[cpu];

	vic_sync_private_forward_private(&irq_info->source, fp->vic, vcpu,
					 fp->virq, irq_info->irq, cpu);
}

// Called with the forward-private lock held.
static void
vic_disable_private_hwirq_helper(vic_forward_private_t *fp, thread_t *vcpu)
{
	assert(vcpu->vic_base_forward_private_active);
	assert(vcpu_option_flags_get_pinned(&vcpu->vcpu_options));

	scheduler_lock(vcpu);
	cpu_index_t cpu = vcpu->scheduler_affinity;
	scheduler_unlock(vcpu);

	assert(cpulocal_index_valid(cpu));

	vic_private_irq_info_t *irq_info = &fp->irq_info[cpu];

	platform_irq_disable_percpu(irq_info->irq, cpu);
}

error_t
vic_bind_hwirq_forward_private(vic_t *vic, hwirq_t *hwirq, virq_t virq)
{
	error_t err = OK;

	assert(hwirq->action == HWIRQ_ACTION_VIC_BASE_FORWARD_PRIVATE);

	partition_t *partition = vic->header.partition;
	assert(partition != NULL);

	size_t		  size	  = sizeof(vic_forward_private_t);
	void_ptr_result_t alloc_r = partition_alloc(
		partition, size, alignof(vic_forward_private_t));
	if (alloc_r.e != OK) {
		err = ERROR_NOMEM;
		goto out;
	}

	vic_forward_private_t *fp = (vic_forward_private_t *)alloc_r.r;
	(void)memset_s(fp, sizeof(*fp), 0, sizeof(*fp));

	fp->vic	 = object_get_vic_additional(vic);
	fp->virq = virq;

	for (cpu_index_t i = 0U; i < PLATFORM_MAX_CORES; i++) {
		fp->irq_info[i].cpu = i;
		fp->irq_info[i].irq = hwirq->irq;
	}

	// We must acquire this lock before setting the fp pointer in the
	// hwirq object. This prevents a race with a concurrent unbind on the
	// same hwirq, which might otherwise be able to clear the fp pointer and
	// run its vic_unbind() calls too early, before the bind calls below,
	// leading to the fp structure being freed while the sources in it are
	// still bound.
	spinlock_acquire(&vic->forward_private_lock);

	vic_forward_private_t *expected = NULL;
	if (!atomic_compare_exchange_strong_explicit(
		    &hwirq->vic_base_forward_private, &expected, fp,
		    memory_order_release, memory_order_relaxed)) {
		spinlock_release(&vic->forward_private_lock);
		(void)partition_free(partition, fp, size);
		err = ERROR_DENIED;
		goto out;
	}

	list_insert_at_tail(&vic->forward_private_list, &fp->list_node);

	// Bind for VCPUs that are attached to the VIC and active.
	for (cpu_index_t i = 0U; i < PLATFORM_MAX_CORES; i++) {
		rcu_read_start();

		thread_t *vcpu = atomic_load_consume(&vic->gicr_vcpus[i]);
		if ((vcpu != NULL) && vcpu->vic_base_forward_private_active) {
			err = vic_bind_private_hwirq_helper(fp, vcpu);
			if (err != OK) {
				rcu_read_finish();
				break;
			}
		}

		rcu_read_finish();
	}

	spinlock_release(&vic->forward_private_lock);

	if (err != OK) {
		vic_unbind_private_hwirq_helper(hwirq);
	}

out:
	return err;
}

error_t
vic_unbind_hwirq_forward_private(hwirq_t *hwirq)
{
	vic_unbind_private_hwirq_helper(hwirq);

	return OK;
}

bool
vic_handle_vcpu_activate_thread_forward_private(thread_t *thread)
{
	bool   ret = true;
	vic_t *vic = vic_get_vic(thread);

	if (vic != NULL) {
		spinlock_acquire(&vic->forward_private_lock);

		thread->vic_base_forward_private_active	 = true;
		thread->vic_base_forward_private_in_sync = false;

		vic_forward_private_t *fp;

		list_foreach_container (fp, &vic->forward_private_list,
					vic_forward_private, list_node) {
			if (vic_bind_private_hwirq_helper(fp, thread) != OK) {
				ret = false;
				break;
			}
		}

		spinlock_release(&vic->forward_private_lock);
	}

	return ret;
}

error_t
vic_handle_object_create_vic_forward_private(vic_create_t vic_create)
{
	vic_t *vic = vic_create.vic;

	spinlock_init(&vic->forward_private_lock);
	list_init(&vic->forward_private_list);

	return OK;
}

void
vic_handle_object_deactivate_hwirq_forward_private(hwirq_t *hwirq)
{
	vic_unbind_private_hwirq_helper(hwirq);
}

bool
vic_handle_irq_received_forward_private(hwirq_t *hwirq)
{
	assert(hwirq != NULL);
	assert(hwirq->action == HWIRQ_ACTION_VIC_BASE_FORWARD_PRIVATE);

	bool_result_t is_edge_r;
	bool	      deactivate;
	_Atomic bool *hw_active;
	cpu_index_t   cpu = cpulocal_get_index();

	rcu_read_start();

	vic_forward_private_t *fp =
		atomic_load_consume(&hwirq->vic_base_forward_private);
	if (fp == NULL) {
		irq_disable_local(hwirq);
		deactivate = true;
		goto out;
	}

	vic_private_irq_info_t *irq_info = &fp->irq_info[cpu];

	hw_active = &irq_info->hw_active;
	atomic_store_relaxed(hw_active, true);

	virq_source_t *source = &irq_info->source;
	is_edge_r	      = virq_assert(source, false);

	if (compiler_unexpected(is_edge_r.e != OK)) {
		// We were unable to deliver the IRQ (because we lost a
		// race with unbind), so disable it.
		irq_disable_local(hwirq);
		deactivate = true;
	} else if (is_edge_r.r) {
		// The IRQ was delivered successfully in edge-triggered
		// mode; we must deactivate it on return (if a VIRQ
		// handler has not already done so), because we have no
		// guarantee that the check-pending handler will be
		// called after deactivate.
		//
		// We are relying here on the physical interrupt also
		// being edge-triggered! If it is level-triggered there
		// will be an interrupt storm. The vic_bind_hwirq and
		// virq_set_mode handlers must ensure that the mode
		// remains consistent between the VIRQ and hardware.
		assert(hw_active != NULL);
		deactivate = atomic_fetch_and_explicit(hw_active, false,
						       memory_order_relaxed);
	} else {
		// The IRQ was delivered successfully in level-triggered
		// mode; it will be deactivated in the check-pending
		// handler.
		deactivate = false;
	}

out:
	rcu_read_finish();

	return deactivate;
}

bool
vic_handle_virq_check_pending_forward_private(virq_source_t *source,
					      bool	     reasserted)
{
	vic_private_irq_info_t *irq_info =
		private_irq_info_from_virq_source(source);

	// FIXME:
	if (!reasserted &&
	    atomic_fetch_and_explicit(&irq_info->hw_active, false,
				      memory_order_relaxed)) {
		if (compiler_expected(cpulocal_get_index() == irq_info->cpu)) {
			platform_irq_deactivate(irq_info->irq);
		} else {
			platform_irq_deactivate_percpu(irq_info->irq,
						       irq_info->cpu);
		}
	}

	return reasserted;
}

bool
vic_handle_virq_set_enabled_forward_private(virq_source_t *source, bool enabled)
{
	vic_private_irq_info_t *irq_info =
		private_irq_info_from_virq_source(source);

	assert(source->is_private);
	assert(platform_irq_is_percpu(irq_info->irq));

	// Note that we don't check the forward-private flag here, because we
	// can't safely take the lock; the vgic module calls this handler with
	// the GICD lock held, and the sync handler above calls a vgic function
	// that acquires the GICD lock with the forward-private lock held.
	// The same applies to the other VIRQ configuration handlers.
	// FIXME:
	if (enabled) {
		platform_irq_enable_percpu(irq_info->irq, irq_info->cpu);
	} else {
		platform_irq_disable_percpu(irq_info->irq, irq_info->cpu);
	}

	return true;
}

irq_trigger_result_t
vic_handle_virq_set_mode_forward_private(virq_source_t *source,
					 irq_trigger_t	mode)
{
	vic_private_irq_info_t *irq_info =
		private_irq_info_from_virq_source(source);

	assert(source->is_private);
	assert(platform_irq_is_percpu(irq_info->irq));

	// FIXME:
	return platform_irq_set_mode_percpu(irq_info->irq, mode, irq_info->cpu);
}

rcu_update_status_t
vic_handle_free_forward_private(rcu_entry_t *entry)
{
	rcu_update_status_t ret = rcu_update_status_default();

	assert(entry != NULL);

	vic_forward_private_t *fp =
		vic_forward_private_container_of_rcu_entry(entry);
	vic_t *vic = fp->vic;

	partition_t *partition = vic->header.partition;
	assert(partition != NULL);
	(void)partition_free(partition, fp, sizeof(vic_forward_private_t));

	object_put_vic(vic);

	return ret;
}

void
vic_base_handle_vcpu_started(bool warm_reset)
{
	thread_t *vcpu = thread_get_self();
	vic_t	 *vic  = vic_get_vic(vcpu);

	if (warm_reset || (vic == NULL) ||
	    !vcpu_option_flags_get_pinned(&vcpu->vcpu_options)) {
		// Nothing to do
		goto out;
	}

	spinlock_acquire(&vic->forward_private_lock);

	assert(!vcpu->vic_base_forward_private_in_sync);

	vic_forward_private_t *fp;
	list_foreach_container (fp, &vic->forward_private_list,
				vic_forward_private, list_node) {
		vic_sync_private_hwirq_helper(fp, vcpu);
	}
	vcpu->vic_base_forward_private_in_sync = true;

	spinlock_release(&vic->forward_private_lock);

out:
	return;
}

void
vic_base_handle_vcpu_stopped(void)
{
	thread_t *vcpu = thread_get_self();
	vic_t	 *vic  = vic_get_vic(vcpu);

	if ((vic == NULL) ||
	    !vcpu_option_flags_get_pinned(&vcpu->vcpu_options)) {
		// Nothing to do
		goto out;
	}

	spinlock_acquire(&vic->forward_private_lock);
	if (vcpu->vic_base_forward_private_in_sync) {
		vic_forward_private_t *fp;
		list_foreach_container (fp, &vic->forward_private_list,
					vic_forward_private, list_node) {
			vic_disable_private_hwirq_helper(fp, vcpu);
		}
		vcpu->vic_base_forward_private_in_sync = false;
	}
	spinlock_release(&vic->forward_private_lock);

out:
	return;
}
#endif

```

`hyp/vm/vic_base/src/hypercalls.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <hyptypes.h>

#include <hypcall_def.h>
#include <hyprights.h>

#include <atomic.h>
#include <compiler.h>
#include <cspace.h>
#include <cspace_lookup.h>
#include <object.h>
#include <partition.h>
#include <spinlock.h>

#include <events/vic.h>

#include "vic_base.h"

error_t
hypercall_hwirq_bind_virq(cap_id_t hwirq_cap, cap_id_t vic_cap, virq_t virq)
{
	error_t	  err;
	cspace_t *cspace = cspace_get_self();

	hwirq_ptr_result_t hwirq_r = cspace_lookup_hwirq(
		cspace, hwirq_cap, CAP_RIGHTS_HWIRQ_BIND_VIC);
	if (compiler_unexpected(hwirq_r.e) != OK) {
		err = hwirq_r.e;
		goto out;
	}

	vic_ptr_result_t vic_r =
		cspace_lookup_vic(cspace, vic_cap, CAP_RIGHTS_VIC_BIND_SOURCE);
	if (compiler_unexpected(vic_r.e) != OK) {
		err = vic_r.e;
		goto out_release_hwirq;
	}

	err = trigger_vic_bind_hwirq_event(hwirq_r.r->action, vic_r.r,
					   hwirq_r.r, virq);

	object_put_vic(vic_r.r);
out_release_hwirq:
	object_put_hwirq(hwirq_r.r);
out:
	return err;
}

error_t
hypercall_hwirq_unbind_virq(cap_id_t hwirq_cap)
{
	error_t	  err;
	cspace_t *cspace = cspace_get_self();

	hwirq_ptr_result_t hwirq_r = cspace_lookup_hwirq(
		cspace, hwirq_cap, CAP_RIGHTS_HWIRQ_BIND_VIC);
	if (compiler_unexpected(hwirq_r.e) != OK) {
		err = hwirq_r.e;
		goto out;
	}

	err = trigger_vic_unbind_hwirq_event(hwirq_r.r->action, hwirq_r.r);

	object_put_hwirq(hwirq_r.r);
out:
	return err;
}

error_t
hypercall_vic_configure(cap_id_t vic_cap, count_t max_vcpus, count_t max_virqs,
			vic_option_flags_t vic_options, count_t max_msis)
{
	error_t	      err;
	cspace_t     *cspace = cspace_get_self();
	object_type_t type;

	object_ptr_result_t o = cspace_lookup_object_any(
		cspace, vic_cap, CAP_RIGHTS_GENERIC_OBJECT_ACTIVATE, &type);
	if (compiler_unexpected(o.e != OK)) {
		err = o.e;
		goto out_released;
	}
	if (type != OBJECT_TYPE_VIC) {
		err = ERROR_CSPACE_WRONG_OBJECT_TYPE;
		goto out_unlocked;
	}
	vic_t *vic = o.r.vic;

	if (vic_option_flags_get_res0_0(&vic_options) != 0U) {
		err = ERROR_ARGUMENT_INVALID;
		goto out_unlocked;
	}

	// For backwards compatibility, treat the max_msis argument as 0 if the
	// caller has not set the flag indicating that it is valid.
	if (!vic_option_flags_get_max_msis_valid(&vic_options)) {
		max_msis = 0U;
	}

	spinlock_acquire(&vic->header.lock);
	if (atomic_load_relaxed(&vic->header.state) == OBJECT_STATE_INIT) {
		err = vic_configure(vic, max_vcpus, max_virqs, max_msis,
				    !vic_option_flags_get_disable_default_addr(
					    &vic_options));
	} else {
		err = ERROR_OBJECT_STATE;
	}
	spinlock_release(&vic->header.lock);

out_unlocked:
	object_put(type, o.r);
out_released:

	return err;
}

error_t
hypercall_vic_attach_vcpu(cap_id_t vic_cap, cap_id_t vcpu_cap, index_t index)
{
	error_t	  err;
	cspace_t *cspace = cspace_get_self();

	vic_ptr_result_t vic_r =
		cspace_lookup_vic(cspace, vic_cap, CAP_RIGHTS_VIC_ATTACH_VCPU);
	if (compiler_unexpected(vic_r.e) != OK) {
		err = vic_r.e;
		goto out;
	}

	object_type_t	    type;
	object_ptr_result_t o = cspace_lookup_object_any(
		cspace, vcpu_cap, CAP_RIGHTS_GENERIC_OBJECT_ACTIVATE, &type);
	if (compiler_unexpected(o.e != OK)) {
		err = o.e;
		goto out_release_vic;
	}
	if (type != OBJECT_TYPE_THREAD) {
		err = ERROR_CSPACE_WRONG_OBJECT_TYPE;
		goto out_release_vcpu;
	}
	thread_t *thread = o.r.thread;

	spinlock_acquire(&thread->header.lock);
	if (atomic_load_relaxed(&thread->header.state) == OBJECT_STATE_INIT) {
		err = vic_attach_vcpu(vic_r.r, thread, index);
	} else {
		err = ERROR_OBJECT_STATE;
	}
	spinlock_release(&thread->header.lock);

out_release_vcpu:
	object_put(type, o.r);
out_release_vic:
	object_put_vic(vic_r.r);
out:
	return err;
}

error_t
hypercall_vic_bind_msi_source(cap_id_t vic_cap, cap_id_t msi_source_cap)
{
	error_t	  err;
	cspace_t *cspace = cspace_get_self();

	vic_ptr_result_t vic_r =
		cspace_lookup_vic(cspace, vic_cap, CAP_RIGHTS_VIC_BIND_SOURCE);
	if (compiler_unexpected(vic_r.e) != OK) {
		err = vic_r.e;
		goto out;
	}

	err = trigger_vic_bind_msi_source_event(vic_r.r, msi_source_cap);

	object_put_vic(vic_r.r);
out:
	return err;
}

```

`hyp/vm/vic_base/vic_base.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

module vic_base

#if VIC_BASE_FORWARD_PRIVATE
subscribe vcpu_activate_thread
	handler vic_handle_vcpu_activate_thread_forward_private(thread)
	// Run after pinned flag is set by scheduler.
	priority -20

subscribe object_create_vic
	handler vic_handle_object_create_vic_forward_private

subscribe object_deactivate_hwirq
	handler vic_handle_object_deactivate_hwirq_forward_private

subscribe vic_bind_hwirq[HWIRQ_ACTION_VIC_BASE_FORWARD_PRIVATE]
	handler vic_bind_hwirq_forward_private(vic, hwirq, virq)

subscribe vic_unbind_hwirq[HWIRQ_ACTION_VIC_BASE_FORWARD_PRIVATE]
	handler vic_unbind_hwirq_forward_private(hwirq)

subscribe irq_received[HWIRQ_ACTION_VIC_BASE_FORWARD_PRIVATE]
	handler vic_handle_irq_received_forward_private(hwirq)
	require_preempt_disabled

subscribe virq_check_pending[VIRQ_TRIGGER_VIC_BASE_FORWARD_PRIVATE]
	handler vic_handle_virq_check_pending_forward_private(source, reasserted)
	require_preempt_disabled

subscribe virq_set_enabled[VIRQ_TRIGGER_VIC_BASE_FORWARD_PRIVATE]
	handler vic_handle_virq_set_enabled_forward_private(source, enabled)
	require_preempt_disabled

subscribe virq_set_mode[VIRQ_TRIGGER_VIC_BASE_FORWARD_PRIVATE]
	handler vic_handle_virq_set_mode_forward_private(source, mode)

subscribe rcu_update[RCU_UPDATE_CLASS_VIC_BASE_FREE_FORWARD_PRIVATE]
	handler vic_handle_free_forward_private(entry)

subscribe vcpu_started(warm_reset)

subscribe vcpu_stopped()
#endif

```

`hyp/vm/vic_base/vic_base.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#if VIC_BASE_FORWARD_PRIVATE
// Forward private (per-CPU) HW interrupts using generic VIRQ APIs.

extend virq_trigger enumeration {
	vic_base_forward_private;
};

extend hwirq_action enumeration {
	vic_base_forward_private;
};

extend hwirq object module vic_base {
	// Pointer to an array of PPI sources, indexed by physical CPU.
	forward_private	pointer(atomic) structure vic_forward_private;
};

extend vic object {
	forward_private_lock structure spinlock;
	forward_private_list structure list;
};

extend thread object module vic_base {
	forward_private_active bool;
	forward_private_in_sync bool;
};

extend rcu_update_class enumeration {
	vic_base_free_forward_private;
};

define vic_private_irq_info structure {
	source		structure virq_source(contained);
	irq		type irq_t;
	cpu		type cpu_index_t;
	hw_active	bool(atomic);
};

define vic_forward_private structure {
	vic		pointer object vic;
	virq		type virq_t;
	rcu_entry	structure rcu_entry(contained);
	list_node	structure list_node(contained);
	irq_info	array(PLATFORM_MAX_CORES)
				structure vic_private_irq_info;
};

#endif

#if defined(VIC_BASE_FORWARD_SHARED) && VIC_BASE_FORWARD_SHARED
// Forward shared HW interrupts using generic VIRQ APIs.

// Not needed for GICv3, so this is not implemented yet.
#error unimplemented
#endif

```

`hyp/vm/virtio_input/build.conf`:

```conf
# © 2023 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

local_include
types virtio_input.tc
events virtio_input.ev
hypercalls virtio_input.hvc
base_module hyp/mem/useraccess
source virtio_input.c hypercalls.c

```

`hyp/vm/virtio_input/include/virtio_input.h`:

```h
// © 2023 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

error_t
set_data_sel_ev_bits(const virtio_mmio_t *virtio_mmio, uint32_t subsel,
		     uint32_t size, vmaddr_t data);
error_t
set_data_sel_abs_info(const virtio_mmio_t *virtio_mmio, uint32_t subsel,
		      uint32_t size, vmaddr_t data);

```

`hyp/vm/virtio_input/src/hypercalls.c`:

```c
// © 2023 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <hyptypes.h>
#include <string.h>

#include <hypcall_def.h>
#include <hyprights.h>

#include <atomic.h>
#include <compiler.h>
#include <cspace.h>
#include <cspace_lookup.h>
#include <object.h>
#include <partition.h>
#include <spinlock.h>
#include <util.h>
#include <virq.h>

#include <asm/nospec_checks.h>

#include "useraccess.h"
#include "virtio_input.h"

error_t
hypercall_virtio_input_configure(cap_id_t virtio_mmio_cap, uint64_t devids,
				 uint32_t prop_bits, uint32_t num_evtypes,
				 uint32_t num_absaxes)
{
	error_t	  ret;
	cspace_t *cspace = cspace_get_self();

	virtio_mmio_ptr_result_t p = cspace_lookup_virtio_mmio(
		cspace, virtio_mmio_cap, CAP_RIGHTS_VIRTIO_MMIO_CONFIG);
	if (compiler_unexpected(p.e != OK)) {
		ret = p.e;
		goto out;
	}
	virtio_mmio_t *virtio_mmio = p.r;
	partition_t   *partition   = virtio_mmio->header.partition;

	// Must be a virtio-input device
	if (virtio_mmio->device_type != VIRTIO_DEVICE_TYPE_INPUT) {
		ret = ERROR_OBJECT_CONFIG;
		goto release_virtio_object;
	}

	// save the devids and propbits
	virtio_mmio->input_data->devids	   = devids;
	virtio_mmio->input_data->prop_bits = prop_bits;

	// Validate the upper bound for evtypes and absaxes
	if ((num_evtypes > VIRTIO_INPUT_MAX_EV_TYPES) ||
	    (num_absaxes > VIRTIO_INPUT_MAX_ABS_AXES)) {
		ret = ERROR_ARGUMENT_INVALID;
		goto release_virtio_object;
	}

	size_t		  alloc_size = 0U;
	void_ptr_result_t alloc_ret;
	// allocate mem for evtypes, if not already allocated and count is > 0
	if ((virtio_mmio->input_data->ev_bits == NULL) && (num_evtypes > 0U)) {
		alloc_size = num_evtypes * sizeof(virtio_input_ev_bits_t);
		alloc_ret  = partition_alloc(partition, alloc_size,
					     alignof(virtio_input_ev_bits_t));
		if (alloc_ret.e != OK) {
			ret = ERROR_NOMEM;
			goto release_virtio_object;
		}
		(void)memset_s(alloc_ret.r, alloc_size, 0, alloc_size);

		virtio_mmio->input_data->ev_bits =
			(virtio_input_ev_bits_t *)alloc_ret.r;
		virtio_mmio->input_data->ev_bits_count = num_evtypes;

		// set entry of each ev as VIRTIO_INPUT_SUBSEL_INVALID
		for (uint32_t entry = 0; entry < num_evtypes; entry++) {
			virtio_mmio->input_data->ev_bits[entry].subsel =
				(uint8_t)VIRTIO_INPUT_SUBSEL_INVALID;
		}
	} else {
		if (num_evtypes > 0U) {
			ret = ERROR_BUSY;
			goto release_virtio_object;
		} else {
			// it means device has no evtypes to register, no
			// worries
		}
	}

	// allocate mem for absaxes, if not already allocated and count is > 0
	if ((virtio_mmio->input_data->absinfo == NULL) && (num_absaxes > 0U)) {
		alloc_size = num_absaxes * sizeof(virtio_input_absinfo_t);
		alloc_ret  = partition_alloc(partition, alloc_size,
					     alignof(virtio_input_absinfo_t));
		if (alloc_ret.e != OK) {
			ret = ERROR_NOMEM;
			goto release_virtio_object;
		}
		(void)memset_s(alloc_ret.r, alloc_size, 0, alloc_size);

		virtio_mmio->input_data->absinfo =
			(virtio_input_absinfo_t *)alloc_ret.r;
		virtio_mmio->input_data->absinfo_count = num_absaxes;

		// set entry of each absinfo as VIRTIO_INPUT_SUBSEL_INVALID
		for (uint32_t entry = 0; entry < num_absaxes; entry++) {
			virtio_mmio->input_data->absinfo[entry].subsel =
				(uint8_t)VIRTIO_INPUT_SUBSEL_INVALID;
		}
	} else if (num_absaxes > 0U) {
		ret = ERROR_BUSY;
		goto release_virtio_object;
	} else {
		// device has no absaxes info to register
	}

	ret = OK;
release_virtio_object:
	object_put_virtio_mmio(virtio_mmio);
out:
	return ret;
}

error_t
hypercall_virtio_input_set_data(cap_id_t virtio_mmio_cap, uint32_t sel,
				uint32_t subsel, uint32_t size, vmaddr_t data)
{
	error_t	  ret;
	cspace_t *cspace = cspace_get_self();

	virtio_mmio_ptr_result_t p = cspace_lookup_virtio_mmio(
		cspace, virtio_mmio_cap, CAP_RIGHTS_VIRTIO_MMIO_CONFIG);
	if (compiler_unexpected(p.e != OK)) {
		ret = p.e;
		goto out;
	}
	virtio_mmio_t *virtio_mmio = p.r;

	// Must be a virtio-input device
	if (virtio_mmio->device_type != VIRTIO_DEVICE_TYPE_INPUT) {
		ret = ERROR_CSPACE_WRONG_OBJECT_TYPE;
		goto release_virtio_object;
	}

	switch ((virtio_input_config_select_t)sel) {
	case VIRTIO_INPUT_CONFIG_SELECT_CFG_ID_NAME: {
		// Only subsel 0 is valid for this sel value
		if (subsel == 0U) {
			// copy data from guest va; size is checked by this API
			ret = useraccess_copy_from_guest_va(
				      virtio_mmio->input_data->name,
				      sizeof(virtio_mmio->input_data->name),
				      data, size)
				      .e;
			if (ret == OK) {
				virtio_mmio->input_data->name_size = size;
			} else {
				virtio_mmio->input_data->name_size = 0U;
			}
		} else {
			ret = ERROR_ARGUMENT_INVALID;
		}
		break;
	}
	case VIRTIO_INPUT_CONFIG_SELECT_CFG_ID_SERIAL: {
		// Only subsel 0 is valid for this sel value
		if (subsel == 0U) {
			// copy data from guest va; size is checked by this API
			ret = useraccess_copy_from_guest_va(
				      virtio_mmio->input_data->serial,
				      sizeof(virtio_mmio->input_data->serial),
				      data, size)
				      .e;
			if (ret == OK) {
				virtio_mmio->input_data->serial_size = size;
			} else {
				virtio_mmio->input_data->serial_size = 0U;
			}
		} else {
			ret = ERROR_ARGUMENT_INVALID;
		}
		break;
	}
	case VIRTIO_INPUT_CONFIG_SELECT_CFG_ID_DEVIDS: {
		// Only subsel 0 is valid for this sel value
		if (subsel == 0U) {
			// copy data from guest va; size is checked by this API
			// TODO: should we memset here?
			ret = useraccess_copy_from_guest_va(
				      &virtio_mmio->input_data->devids,
				      sizeof(virtio_mmio->input_data->devids),
				      data, size)
				      .e;
		} else {
			ret = ERROR_ARGUMENT_INVALID;
		}
		break;
	}
	case VIRTIO_INPUT_CONFIG_SELECT_CFG_PROP_BITS: {
		// Only subsel 0 is valid for this sel value
		if (subsel == 0U) {
			// copy data from guest va; size is checked by this API
			// TODO: should we memset here?
			ret = useraccess_copy_from_guest_va(
				      &virtio_mmio->input_data->prop_bits,
				      sizeof(virtio_mmio->input_data->prop_bits),
				      data, size)
				      .e;
		} else {
			ret = ERROR_ARGUMENT_INVALID;
		}
		break;
	}
	case VIRTIO_INPUT_CONFIG_SELECT_CFG_EV_BITS: {
		// check if mem is allocated for ev_bits
		if (virtio_mmio->input_data->ev_bits != NULL) {
			ret = set_data_sel_ev_bits(
				(const virtio_mmio_t *)virtio_mmio, subsel,
				size, data);
		} else {
			// Not properly configured
			ret = ERROR_ARGUMENT_INVALID;
		}
		break;
	}
	case VIRTIO_INPUT_CONFIG_SELECT_CFG_ABS_INFO: {
		// check if mem is allocated for absinfo
		if (virtio_mmio->input_data->absinfo != NULL) {
			ret = set_data_sel_abs_info(
				(const virtio_mmio_t *)virtio_mmio, subsel,
				size, data);
		} else {
			// Not properly configured
			ret = ERROR_ARGUMENT_INVALID;
		}
		break;
	}
	case VIRTIO_INPUT_CONFIG_SELECT_CFG_UNSET:
	default:
		// invalid select event
		ret = ERROR_ARGUMENT_INVALID;
		break;
	}

release_virtio_object:
	object_put_virtio_mmio(virtio_mmio);
out:
	return ret;
}

```

`hyp/vm/virtio_input/src/virtio_input.c`:

```c
// © 2023 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>
#include <string.h>

#include <hypconstants.h>
#include <hypcontainers.h>

#include <atomic.h>
#include <partition.h>
#include <spinlock.h>
#include <thread.h>
#include <util.h>
#include <virq.h>

#include <events/virtio_mmio.h>

#include <asm/nospec_checks.h>

#include "event_handlers.h"
#include "useraccess.h"
#include "virtio_input.h"

error_t
virtio_input_handle_object_activate(virtio_mmio_t *virtio_mmio)
{
	error_t ret;

	partition_t *partition = virtio_mmio->header.partition;
	// Allocate memory for virtio input data struct if device type
	// virtio-input
	if (virtio_mmio->device_type == VIRTIO_DEVICE_TYPE_INPUT) {
		size_t		  alloc_size = sizeof(virtio_input_data_t);
		void_ptr_result_t alloc_ret  = partition_alloc(
			 partition, alloc_size, alignof(virtio_input_data_t));
		if (alloc_ret.e != OK) {
			ret = ERROR_NOMEM;
			goto out;
		}

		(void)memset_s(alloc_ret.r, alloc_size, 0, alloc_size);

		virtio_mmio->input_data = (virtio_input_data_t *)alloc_ret.r;
	}

	ret = OK;
out:
	return ret;
}

error_t
virtio_input_handle_object_cleanup(virtio_mmio_t *virtio_mmio)
{
	if (virtio_mmio->input_data != NULL) {
		partition_t *partition = virtio_mmio->header.partition;
		size_t	     alloc_size;
		void	    *alloc_base;
		/* first free memory for absinfo and evtypes if any */
		if (virtio_mmio->input_data->absinfo_count != 0U) {
			alloc_size = virtio_mmio->input_data->absinfo_count *
				     sizeof(virtio_input_absinfo_t);
			alloc_base = (void *)virtio_mmio->input_data->absinfo;

			error_t err = partition_free(partition, alloc_base,
						     alloc_size);
			assert(err == OK);

			virtio_mmio->input_data->absinfo       = NULL;
			virtio_mmio->input_data->absinfo_count = 0U;
		}

		if (virtio_mmio->input_data->ev_bits_count != 0U) {
			alloc_size = virtio_mmio->input_data->ev_bits_count *
				     sizeof(virtio_input_ev_bits_t);
			alloc_base = (void *)virtio_mmio->input_data->ev_bits;

			error_t err = partition_free(partition, alloc_base,
						     alloc_size);
			assert(err == OK);

			virtio_mmio->input_data->ev_bits       = NULL;
			virtio_mmio->input_data->ev_bits_count = 0U;
		}

		/* now safely free the virtio_input struct */
		alloc_size = sizeof(virtio_mmio->input_data);
		alloc_base = (void *)&virtio_mmio->input_data;

		error_t err = partition_free(partition, alloc_base, alloc_size);
		assert(err == OK);

		virtio_mmio->input_data = NULL;
	} else {
		// ignore
	}

	return OK;
}

error_t
set_data_sel_abs_info(const virtio_mmio_t *virtio_mmio, uint32_t subsel,
		      uint32_t size, vmaddr_t data)
{
	error_t ret;
	if (subsel < VIRTIO_INPUT_MAX_ABS_AXES) {
		// find free entry
		uint32_t entry;
		for (entry = 0; entry < virtio_mmio->input_data->absinfo_count;
		     entry++) {
			if (virtio_mmio->input_data->absinfo[entry].subsel ==
			    VIRTIO_INPUT_SUBSEL_INVALID) {
				// got the free entry
				break;
			} else {
				continue;
			}
		}

		if (entry == virtio_mmio->input_data->absinfo_count) {
			// no free entry
			ret = ERROR_NORESOURCES;
		} else {
			// copy data from guest va; size is checked by this API
			ret = useraccess_copy_from_guest_va(
				      &(virtio_mmio->input_data->absinfo[entry]
						.data),
				      VIRTIO_INPUT_MAX_ABSINFO_SIZE, data, size)
				      .e;
			if (ret == OK) {
				// successful copy, update subsel
				virtio_mmio->input_data->absinfo[entry].subsel =
					(uint8_t)subsel;
			} else {
				// ignore
			}
		}
	} else {
		ret = ERROR_ARGUMENT_INVALID;
	}
	return ret;
}

error_t
set_data_sel_ev_bits(const virtio_mmio_t *virtio_mmio, uint32_t subsel,
		     uint32_t size, vmaddr_t data)
{
	error_t ret;
	if (subsel < VIRTIO_INPUT_MAX_EV_TYPES) {
		// find free entry
		uint32_t entry;
		for (entry = 0; entry < virtio_mmio->input_data->ev_bits_count;
		     entry++) {
			if (virtio_mmio->input_data->ev_bits[entry].subsel ==
			    VIRTIO_INPUT_SUBSEL_INVALID) {
				// got the free entry
				break;
			} else {
				continue;
			}
		}

		if (entry == virtio_mmio->input_data->ev_bits_count) {
			// no free entry
			ret = ERROR_NORESOURCES;
		} else {
			// copy data from guest va; size is checked by this API
			ret = useraccess_copy_from_guest_va(
				      &(virtio_mmio->input_data->ev_bits[entry]
						.data),
				      VIRTIO_INPUT_MAX_BITMAP_SIZE, data, size)
				      .e;
			if (ret == OK) {
				/*successful copy, update the size info
				 * and subsel*/
				virtio_mmio->input_data->ev_bits[entry].size =
					(uint8_t)size;
				virtio_mmio->input_data->ev_bits[entry].subsel =
					(uint8_t)subsel;
			} else {
				// ignore
			}
		}
	} else {
		ret = ERROR_ARGUMENT_INVALID;
	}
	return ret;
}

static void
sel_cfg_abs_info_write(const virtio_mmio_t *virtio_mmio, uint8_t subsel)
{
	if (subsel < VIRTIO_INPUT_MAX_ABS_AXES) {
		// find the ev entry where this subsel entry is stored
		uint32_t entry;
		for (entry = 0; entry < virtio_mmio->input_data->absinfo_count;
		     entry++) {
			if (virtio_mmio->input_data->absinfo[entry].subsel ==
			    subsel) {
				// found the entry
				break;
			} else {
				continue;
			}
		}
		if (entry == virtio_mmio->input_data->absinfo_count) {
			// entry not found, invalid subsel set size 0
			atomic_store_relaxed(&virtio_mmio->regs->device_config
						      .input_config.size,
					     0U);
		} else {
			// valid subsel
			uint8_t size = (uint8_t)VIRTIO_INPUT_MAX_ABSINFO_SIZE;

			(void)memcpy(
				virtio_mmio->regs->device_config.input_config.u
					.abs,
				virtio_mmio->input_data->absinfo[entry].data,
				size);

			// update the size
			atomic_store_relaxed(&virtio_mmio->regs->device_config
						      .input_config.size,
					     size);
		}
	} else {
		// invalid subsel set size 0
		atomic_store_relaxed(
			&virtio_mmio->regs->device_config.input_config.size,
			0U);
	}
}

static void
sel_cfg_ev_bits_write(const virtio_mmio_t *virtio_mmio, uint8_t subsel)
{
	if (subsel < VIRTIO_INPUT_MAX_EV_TYPES) {
		// find the ev entry where this subsel entry is stored
		uint32_t entry;
		for (entry = 0; entry < virtio_mmio->input_data->ev_bits_count;
		     entry++) {
			if (virtio_mmio->input_data->ev_bits[entry].subsel ==
			    subsel) {
				// found the entry
				break;
			} else {
				continue;
			}
		}
		if (entry == virtio_mmio->input_data->ev_bits_count) {
			// entry not found, invalid subsel set size 0
			atomic_store_relaxed(&virtio_mmio->regs->device_config
						      .input_config.size,
					     0U);
		} else {
			// valid subsel
			uint8_t size =
				virtio_mmio->input_data->ev_bits[entry].size;

			(void)memcpy(
				virtio_mmio->regs->device_config.input_config.u
					.bitmap,
				virtio_mmio->input_data->ev_bits[entry].data,
				size);

			// update the size
			atomic_store_relaxed(&virtio_mmio->regs->device_config
						      .input_config.size,
					     size);
		}
	} else {
		// invalid subsel set size 0
		atomic_store_relaxed(
			&virtio_mmio->regs->device_config.input_config.size,
			0U);
	}
}

static void
virtio_input_config_u_write(const virtio_mmio_t *virtio_mmio, uint8_t sel,
			    uint8_t subsel)
{
	switch ((virtio_input_config_select_t)sel) {
	case VIRTIO_INPUT_CONFIG_SELECT_CFG_ID_NAME: {
		if (subsel != 0U) { // only subsel 0 is valid
			atomic_store_relaxed(&virtio_mmio->regs->device_config
						      .input_config.size,
					     0U);
		} else {
			size_t size = virtio_mmio->input_data->name_size;
			for (index_t i = 0U; i < size; i++) {
				atomic_store_relaxed(
					&virtio_mmio->regs->device_config
						 .input_config.u.string[i],
					virtio_mmio->input_data->name[i]);
			}
			// update the size
			atomic_store_relaxed(&virtio_mmio->regs->device_config
						      .input_config.size,
					     (uint8_t)size);
		}
		break;
	}
	case VIRTIO_INPUT_CONFIG_SELECT_CFG_ID_SERIAL: {
		if (subsel != 0U) { // only subsel 0 is valid
			atomic_store_relaxed(&virtio_mmio->regs->device_config
						      .input_config.size,
					     0U);
		} else {
			size_t size = virtio_mmio->input_data->serial_size;
			for (index_t i = 0U; i < size; i++) {
				atomic_store_relaxed(
					&virtio_mmio->regs->device_config
						 .input_config.u.string[i],
					virtio_mmio->input_data->serial[i]);
			}
			// update the size
			atomic_store_relaxed(&virtio_mmio->regs->device_config
						      .input_config.size,
					     (uint8_t)size);
		}
		break;
	}
	case VIRTIO_INPUT_CONFIG_SELECT_CFG_ID_DEVIDS: {
		if (subsel != 0U) { // only subsel 0 is valid
			atomic_store_relaxed(&virtio_mmio->regs->device_config
						      .input_config.size,
					     0U);
		} else {
			size_t size = sizeof(virtio_mmio->input_data->devids);
			atomic_store_relaxed(&virtio_mmio->regs->device_config
						      .input_config.u.ids,
					     virtio_mmio->input_data->devids);
			// update the size
			atomic_store_relaxed(&virtio_mmio->regs->device_config
						      .input_config.size,
					     (uint8_t)size);
		}
		break;
	}
	case VIRTIO_INPUT_CONFIG_SELECT_CFG_PROP_BITS: {
		if (subsel != 0U) { // only subsel 0 is valid
			atomic_store_relaxed(&virtio_mmio->regs->device_config
						      .input_config.size,
					     0U);
		} else {
			size_t size =
				sizeof(virtio_mmio->input_data->prop_bits);
			uint8_t *prop_bits_addr =
				(uint8_t *)&virtio_mmio->input_data->prop_bits;
			for (index_t i = 0U; i < size; i++) {
				atomic_store_relaxed(
					&virtio_mmio->regs->device_config
						 .input_config.u.bitmap[i],
					*prop_bits_addr);
				prop_bits_addr++;
			}
			// update the size
			atomic_store_relaxed(&virtio_mmio->regs->device_config
						      .input_config.size,
					     (uint8_t)size);
		}
		break;
	}
	case VIRTIO_INPUT_CONFIG_SELECT_CFG_EV_BITS: {
		sel_cfg_ev_bits_write(virtio_mmio, subsel);
		break;
	}
	case VIRTIO_INPUT_CONFIG_SELECT_CFG_ABS_INFO: {
		sel_cfg_abs_info_write(virtio_mmio, subsel);
		break;
	}
	case VIRTIO_INPUT_CONFIG_SELECT_CFG_UNSET:
	default:
		// No data; set size to 0
		atomic_store_relaxed(
			&virtio_mmio->regs->device_config.input_config.size,
			0U);
		break;
	}
}

vcpu_trap_result_t
virtio_input_config_write(const virtio_mmio_t *virtio_mmio, size_t write_offset,
			  register_t reg_val, size_t access_size)
{
	vcpu_trap_result_t ret;
	register_t	   val = reg_val;
	size_t		   offset;
	size_t		   access_size_remaining = access_size;

	if (write_offset >= (size_t)OFS_VIRTIO_MMIO_REGS_DEVICE_CONFIG) {
		ret    = VCPU_TRAP_RESULT_FAULT;
		offset = write_offset -
			 (size_t)OFS_VIRTIO_MMIO_REGS_DEVICE_CONFIG;
		while (access_size_remaining != 0U) {
			switch (offset) {
			case OFS_VIRTIO_INPUT_CONFIG_SELECT: {
				atomic_store_relaxed(
					&virtio_mmio->regs->device_config
						 .input_config.select,
					(uint8_t)val);

				uint8_t subsel = atomic_load_relaxed(
					&virtio_mmio->regs->device_config
						 .input_config.subsel);

				// write the appropriate data in u regs
				virtio_input_config_u_write(
					virtio_mmio, (uint8_t)val, subsel);
				// update remianing size
				access_size_remaining =
					access_size_remaining - 1U;
				offset += 1U; // update offset
				val >>= 8;    // update the value
				ret = VCPU_TRAP_RESULT_EMULATED;
				break;
			}
			case OFS_VIRTIO_INPUT_CONFIG_SUBSEL: {
				atomic_store_relaxed(
					&virtio_mmio->regs->device_config
						 .input_config.subsel,
					(uint8_t)val);

				uint8_t sel = atomic_load_relaxed(
					&virtio_mmio->regs->device_config
						 .input_config.select);

				// write the appropriate data in u regs
				virtio_input_config_u_write(virtio_mmio, sel,
							    (uint8_t)val);
				// update remianing size
				access_size_remaining =
					access_size_remaining - 1U;
				offset += 1U; // update offset
				val >>= 8;    // update the value
				ret = VCPU_TRAP_RESULT_EMULATED;
				break;
			}
			default:
				(void)access_size;
				// we will not handle offset after subsel
				access_size_remaining = 0U;
				ret		      = VCPU_TRAP_RESULT_FAULT;
				break;
			}
		}
	} else {
		ret = VCPU_TRAP_RESULT_FAULT;
	}

	return ret;
}

```

`hyp/vm/virtio_input/virtio_input.ev`:

```ev
// © 2023 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

module virtio_input

subscribe virtio_mmio_valid_device_type[VIRTIO_DEVICE_TYPE_INPUT]
	constant true

subscribe virtio_mmio_device_config_write[VIRTIO_DEVICE_TYPE_INPUT]
	handler virtio_input_config_write(virtio_mmio, offset, value, access_size)

subscribe virtio_mmio_device_config_activate[VIRTIO_DEVICE_TYPE_INPUT]
	handler virtio_input_handle_object_activate(virtio_mmio)

subscribe virtio_mmio_device_config_cleanup[VIRTIO_DEVICE_TYPE_INPUT]
	handler virtio_input_handle_object_cleanup(virtio_mmio)

```

`hyp/vm/virtio_input/virtio_input.hvc`:

```hvc
// © 2023 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

define virtio_input_configure hypercall {
	call_num	0x5e;
	virtio_mmio_cap	input type cap_id_t;
	devids		input uint64;
	prop_bits	input uint32;
	num_evtypes	input uint32;
	num_absaxes	input uint32;
	res0		input uregister;
	error		output enumeration error;
};

define virtio_input_set_data hypercall {
	call_num	0x5f;
	virtio_mmio_cap	input type cap_id_t;
	sel		input uint32;
	subsel		input uint32;
	size		input uint32;
	data		input type vmaddr_t;
	res0		input uregister;
	error		output enumeration error;
};

```

`hyp/vm/virtio_input/virtio_input.tc`:

```tc
// © 2023 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

define VIRTIO_MMIO_INPUT_REG_CONFIG_BYTES constant type count_t = (VIRTIO_MMIO_REG_CONFIG_BYTES - 0x88);
define VIRTIO_INPUT_MAX_ABS_AXES constant type count_t = 64;
define VIRTIO_INPUT_MAX_EV_TYPES constant type count_t = 32;
define VIRTIO_INPUT_MAX_STRING_SIZE constant type count_t = 128;
define VIRTIO_INPUT_MAX_BITMAP_SIZE constant type count_t = 128;
define VIRTIO_INPUT_MAX_ABSINFO_SIZE constant type count_t = 20;
define VIRTIO_INPUT_SUBSEL_INVALID constant type count_t = 255;

extend virtio_config_space union {
	input_config	structure virtio_input_config;
};

extend virtio_device_type enumeration {
	INPUT = 18;
};

extend virtio_mmio object {
	input_data	pointer structure virtio_input_data;
};

define virtio_input_absinfo structure {
	subsel		uint8;
	data		array(VIRTIO_INPUT_MAX_ABSINFO_SIZE) uint8;
};

define virtio_input_ev_bits structure {
	subsel		uint8;
	size		uint8;
	data		array(VIRTIO_INPUT_MAX_BITMAP_SIZE) uint8;
};

define virtio_input_data structure {
	name		array(VIRTIO_INPUT_MAX_STRING_SIZE) uint8;
	name_size	size;
	serial		array(VIRTIO_INPUT_MAX_STRING_SIZE) uint8;
	serial_size	size;
	prop_bits	uint32;
	devids		uint64;
	absinfo		pointer structure virtio_input_absinfo;
	absinfo_count	type count_t;
	ev_bits		pointer structure virtio_input_ev_bits;
	ev_bits_count	type count_t;
};

define virtio_input_config structure {
	select @ 0x00	uint8(atomic);
	subsel @ 0x01	uint8(atomic);
	size @ 0x02	uint8(atomic);
	u @ 0x08	union virtio_input_banked_regs;
	config	@ 0x88	array(VIRTIO_MMIO_INPUT_REG_CONFIG_BYTES) uint8(atomic);
};

define virtio_input_banked_regs union {
	string		array(VIRTIO_INPUT_MAX_STRING_SIZE) uint8(atomic);
	bitmap		array(VIRTIO_INPUT_MAX_BITMAP_SIZE) uint8(atomic);
	abs		array(VIRTIO_INPUT_MAX_ABSINFO_SIZE) uint8(atomic);
	ids		uint64(atomic);
};

define virtio_input_config_select enumeration(explicit) {
	CFG_UNSET	= 0x00;
	CFG_ID_NAME	= 0x01;
	CFG_ID_SERIAL	= 0x02;
	CFG_ID_DEVIDS	= 0x03;
	CFG_PROP_BITS	= 0x10;
	CFG_EV_BITS	= 0x11;
	CFG_ABS_INFO	= 0x12;
};

```

`hyp/vm/virtio_mmio/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

interface virtio_mmio
local_include
types virtio_mmio.tc
events virtio_mmio.ev
source virtio_mmio.c vdevice.c hypercalls.c

```

`hyp/vm/virtio_mmio/include/virtio_mmio.h`:

```h
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

error_t
virtio_mmio_configure(virtio_mmio_t *virtio_mmio, memextent_t *memextent,
		      count_t vqs_num, virtio_option_flags_t flags,
		      virtio_device_type_t device_type);

error_t
virtio_mmio_backend_bind_virq(virtio_mmio_t *virtio_mmio, vic_t *vic,
			      virq_t virq);

void
virtio_mmio_backend_unbind_virq(virtio_mmio_t *virtio_mmio);

error_t
virtio_mmio_frontend_bind_virq(virtio_mmio_t *virtio_mmio, vic_t *vic,
			       virq_t virq);

void
virtio_mmio_frontend_unbind_virq(virtio_mmio_t *virtio_mmio);

```

`hyp/vm/virtio_mmio/src/hypercalls.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <hyptypes.h>
#include <string.h>

#include <hypcall_def.h>
#include <hyprights.h>

#include <atomic.h>
#include <compiler.h>
#include <cspace.h>
#include <cspace_lookup.h>
#include <object.h>
#include <partition.h>
#include <spinlock.h>
#include <util.h>
#include <virq.h>

#include <asm/nospec_checks.h>

#include "virtio_mmio.h"

error_t
hypercall_virtio_mmio_configure(cap_id_t virtio_mmio_cap,
				cap_id_t memextent_cap, count_t vqs_num,
				virtio_option_flags_t flags,
				virtio_device_type_t  device_type)
{
	error_t	      err;
	cspace_t     *cspace = cspace_get_self();
	object_type_t type;

	memextent_ptr_result_t m = cspace_lookup_memextent(
		cspace, memextent_cap, CAP_RIGHTS_MEMEXTENT_ATTACH);
	if (compiler_unexpected(m.e != OK)) {
		err = m.e;
		goto out;
	}

	memextent_t *memextent = m.r;

	object_ptr_result_t o = cspace_lookup_object_any(
		cspace, virtio_mmio_cap, CAP_RIGHTS_GENERIC_OBJECT_ACTIVATE,
		&type);
	if (compiler_unexpected(o.e != OK)) {
		err = o.e;
		goto out_memextent_release;
	}
	if (type != OBJECT_TYPE_VIRTIO_MMIO) {
		err = ERROR_CSPACE_WRONG_OBJECT_TYPE;
		goto out_virtio_mmio_release;
	}

	virtio_mmio_t *virtio_mmio = o.r.virtio_mmio;

	spinlock_acquire(&virtio_mmio->header.lock);

	if (atomic_load_relaxed(&virtio_mmio->header.state) ==
	    OBJECT_STATE_INIT) {
		err = virtio_mmio_configure(virtio_mmio, memextent, vqs_num,
					    flags, device_type);
	} else {
		err = ERROR_OBJECT_STATE;
	}

	spinlock_release(&virtio_mmio->header.lock);
out_virtio_mmio_release:
	object_put(type, o.r);
out_memextent_release:
	object_put_memextent(memextent);
out:
	return err;
}

error_t
hypercall_virtio_mmio_backend_bind_virq(cap_id_t virtio_mmio_cap,
					cap_id_t vic_cap, virq_t virq)
{
	error_t	  err	 = OK;
	cspace_t *cspace = cspace_get_self();

	virtio_mmio_ptr_result_t p = cspace_lookup_virtio_mmio(
		cspace, virtio_mmio_cap,
		CAP_RIGHTS_VIRTIO_MMIO_BIND_BACKEND_VIRQ);
	if (compiler_unexpected(p.e != OK)) {
		err = p.e;
		goto out;
	}
	virtio_mmio_t *virtio_mmio = p.r;

	vic_ptr_result_t v =
		cspace_lookup_vic(cspace, vic_cap, CAP_RIGHTS_VIC_BIND_SOURCE);
	if (compiler_unexpected(v.e != OK)) {
		err = v.e;
		goto out_virtio_mmio_release;
	}
	vic_t *vic = v.r;

	err = virtio_mmio_backend_bind_virq(virtio_mmio, vic, virq);

	object_put_vic(vic);
out_virtio_mmio_release:
	object_put_virtio_mmio(virtio_mmio);
out:
	return err;
}

error_t
hypercall_virtio_mmio_backend_unbind_virq(cap_id_t virtio_mmio_cap)
{
	error_t	  err	 = OK;
	cspace_t *cspace = cspace_get_self();

	virtio_mmio_ptr_result_t p = cspace_lookup_virtio_mmio(
		cspace, virtio_mmio_cap,
		CAP_RIGHTS_VIRTIO_MMIO_BIND_BACKEND_VIRQ);
	if (compiler_unexpected(p.e != OK)) {
		err = p.e;
		goto out;
	}
	virtio_mmio_t *virtio_mmio = p.r;

	virtio_mmio_backend_unbind_virq(virtio_mmio);

	object_put_virtio_mmio(virtio_mmio);
out:
	return err;
}

error_t
hypercall_virtio_mmio_backend_assert_virq(cap_id_t virtio_mmio_cap,
					  uint32_t interrupt_status)
{
	error_t	  err	 = OK;
	cspace_t *cspace = cspace_get_self();

	virtio_mmio_ptr_result_t p = cspace_lookup_virtio_mmio(
		cspace, virtio_mmio_cap, CAP_RIGHTS_VIRTIO_MMIO_ASSERT_VIRQ);
	if (compiler_unexpected(p.e != OK)) {
		err = p.e;
		goto out;
	}
	virtio_mmio_t *virtio_mmio = p.r;

	virtio_mmio_status_reg_t status =
		atomic_load_relaxed(&virtio_mmio->regs->status);

	if (virtio_mmio_status_reg_get_device_needs_reset(&status)) {
		err = ERROR_DENIED;
	} else {
#if defined(PLATFORM_NO_DEVICE_ATTR_ATOMIC_UPDATE) &&                          \
	PLATFORM_NO_DEVICE_ATTR_ATOMIC_UPDATE
		spinlock_acquire(&virtio_mmio->lock);
		uint32_t new_irq_status = atomic_load_relaxed(
			&virtio_mmio->regs->interrupt_status);
		new_irq_status |= interrupt_status;
		atomic_store_relaxed(&virtio_mmio->regs->interrupt_status,
				     new_irq_status);
		spinlock_release(&virtio_mmio->lock);
#else
		(void)atomic_fetch_or_explicit(
			&virtio_mmio->regs->interrupt_status, interrupt_status,
			memory_order_relaxed);
#endif
		atomic_thread_fence(memory_order_release);
		// Assert frontend's IRQ
		(void)virq_assert(&virtio_mmio->backend_source, false);
	}

	object_put_virtio_mmio(virtio_mmio);
out:
	return err;
}

error_t
hypercall_virtio_mmio_frontend_bind_virq(cap_id_t virtio_mmio_cap,
					 cap_id_t vic_cap, virq_t virq)
{
	error_t	  err	 = OK;
	cspace_t *cspace = cspace_get_self();

	virtio_mmio_ptr_result_t p = cspace_lookup_virtio_mmio(
		cspace, virtio_mmio_cap,
		CAP_RIGHTS_VIRTIO_MMIO_BIND_FRONTEND_VIRQ);
	if (compiler_unexpected(p.e != OK)) {
		err = p.e;
		goto out;
	}
	virtio_mmio_t *virtio_mmio = p.r;

	vic_ptr_result_t v =
		cspace_lookup_vic(cspace, vic_cap, CAP_RIGHTS_VIC_BIND_SOURCE);
	if (compiler_unexpected(v.e != OK)) {
		err = v.e;
		goto out_virtio_mmio_release;
	}
	vic_t *vic = v.r;

	err = virtio_mmio_frontend_bind_virq(virtio_mmio, vic, virq);

	object_put_vic(vic);
out_virtio_mmio_release:
	object_put_virtio_mmio(virtio_mmio);
out:
	return err;
}

error_t
hypercall_virtio_mmio_frontend_unbind_virq(cap_id_t virtio_mmio_cap)
{
	error_t	  err	 = OK;
	cspace_t *cspace = cspace_get_self();

	virtio_mmio_ptr_result_t p = cspace_lookup_virtio_mmio(
		cspace, virtio_mmio_cap,
		CAP_RIGHTS_VIRTIO_MMIO_BIND_FRONTEND_VIRQ);
	if (compiler_unexpected(p.e != OK)) {
		err = p.e;
		goto out;
	}
	virtio_mmio_t *virtio_mmio = p.r;

	virtio_mmio_frontend_unbind_virq(virtio_mmio);

	object_put_virtio_mmio(virtio_mmio);
out:
	return err;
}

error_t
hypercall_virtio_mmio_backend_set_dev_features(cap_id_t virtio_mmio_cap,
					       uint32_t sel, uint32_t dev_feat)
{
	error_t	  ret	 = OK;
	cspace_t *cspace = cspace_get_self();

	virtio_mmio_ptr_result_t p = cspace_lookup_virtio_mmio(
		cspace, virtio_mmio_cap, CAP_RIGHTS_VIRTIO_MMIO_CONFIG);
	if (compiler_unexpected(p.e != OK)) {
		ret = p.e;
		goto out;
	}
	virtio_mmio_t *virtio_mmio = p.r;

	index_result_t res = nospec_range_check(sel, VIRTIO_MMIO_DEV_FEAT_NUM);
	if (res.e != OK) {
		ret = res.e;
		goto set_failed;
	}

	// Check features enforced by the hypervisor
	if (res.r == 1U) {
		uint32_t allow =
			(uint32_t)util_bit((VIRTIO_F_VERSION_1 - 32U)) |
			(uint32_t)util_bit((VIRTIO_F_ACCESS_PLATFORM - 32U)) |
			dev_feat;
		uint32_t forbid = ~(uint32_t)util_bit(
					  (VIRTIO_F_NOTIFICATION_DATA - 32U)) &
				  dev_feat;

		if ((allow != dev_feat) || (forbid != dev_feat)) {
			ret = ERROR_DENIED;
			goto set_failed;
		}
	}

	virtio_mmio->banked_dev_feat[res.r] = dev_feat;

set_failed:
	object_put_virtio_mmio(virtio_mmio);
out:
	return ret;
}

error_t
hypercall_virtio_mmio_backend_set_queue_num_max(cap_id_t virtio_mmio_cap,
						uint32_t sel,
						uint32_t queue_num_max)
{
	error_t	  ret	 = OK;
	cspace_t *cspace = cspace_get_self();

	virtio_mmio_ptr_result_t p = cspace_lookup_virtio_mmio(
		cspace, virtio_mmio_cap, CAP_RIGHTS_VIRTIO_MMIO_CONFIG);
	if (compiler_unexpected(p.e != OK)) {
		ret = p.e;
		goto out;
	}
	virtio_mmio_t *virtio_mmio = p.r;

	index_result_t res = nospec_range_check(sel, virtio_mmio->vqs_num);
	if (res.e == OK) {
		virtio_mmio->banked_queue_regs[res.r].num_max = queue_num_max;
	} else {
		ret = res.e;
	}

	object_put_virtio_mmio(virtio_mmio);
out:
	return ret;
}

hypercall_virtio_mmio_backend_get_drv_features_result_t
hypercall_virtio_mmio_backend_get_drv_features(cap_id_t virtio_mmio_cap,
					       uint32_t sel)
{
	hypercall_virtio_mmio_backend_get_drv_features_result_t ret = { 0 };
	cspace_t *cspace = cspace_get_self();

	virtio_mmio_ptr_result_t p = cspace_lookup_virtio_mmio(
		cspace, virtio_mmio_cap, CAP_RIGHTS_VIRTIO_MMIO_CONFIG);
	if (compiler_unexpected(p.e != OK)) {
		ret.error = p.e;
		goto out;
	}
	virtio_mmio_t *virtio_mmio = p.r;

	index_result_t res = nospec_range_check(sel, VIRTIO_MMIO_DRV_FEAT_NUM);
	if (res.e == OK) {
		ret.drv_feat = virtio_mmio->banked_drv_feat[res.r];
		ret.error    = OK;
	} else {
		ret.error = res.e;
	}

	object_put_virtio_mmio(virtio_mmio);
out:
	return ret;
}

hypercall_virtio_mmio_backend_get_queue_info_result_t
hypercall_virtio_mmio_backend_get_queue_info(cap_id_t virtio_mmio_cap,
					     uint32_t sel)
{
	hypercall_virtio_mmio_backend_get_queue_info_result_t ret = { 0 };
	cspace_t *cspace = cspace_get_self();

	virtio_mmio_ptr_result_t p = cspace_lookup_virtio_mmio(
		cspace, virtio_mmio_cap, CAP_RIGHTS_VIRTIO_MMIO_CONFIG);
	if (compiler_unexpected(p.e != OK)) {
		ret.error = p.e;
		goto out;
	}
	virtio_mmio_t *virtio_mmio = p.r;

	index_result_t res = nospec_range_check(sel, virtio_mmio->vqs_num);
	if (res.e != OK) {
		object_put_virtio_mmio(virtio_mmio);
		ret.error = res.e;
		goto out;
	}

	virtio_mmio_banked_queue_registers_t *queue_regs =
		&virtio_mmio->banked_queue_regs[res.r];

	ret.queue_num	= queue_regs->num;
	ret.queue_ready = queue_regs->ready;

	ret.queue_desc = queue_regs->desc_high;
	ret.queue_desc = ret.queue_desc << 32;
	ret.queue_desc |= (register_t)queue_regs->desc_low;

	ret.queue_drv = queue_regs->drv_high;
	ret.queue_drv = ret.queue_drv << 32;
	ret.queue_drv |= (register_t)queue_regs->drv_low;

	ret.queue_dev = queue_regs->dev_high;
	ret.queue_dev = ret.queue_dev << 32;
	ret.queue_dev |= (register_t)queue_regs->dev_low;

	ret.error = OK;

	object_put_virtio_mmio(virtio_mmio);
out:
	return ret;
}

hypercall_virtio_mmio_backend_get_notification_result_t
hypercall_virtio_mmio_backend_get_notification(cap_id_t virtio_mmio_cap)
{
	hypercall_virtio_mmio_backend_get_notification_result_t ret = { 0 };
	cspace_t *cspace = cspace_get_self();

	virtio_mmio_ptr_result_t p = cspace_lookup_virtio_mmio(
		cspace, virtio_mmio_cap, CAP_RIGHTS_VIRTIO_MMIO_CONFIG);
	if (compiler_unexpected(p.e != OK)) {
		ret.error = p.e;
		goto out;
	}
	virtio_mmio_t *virtio_mmio = p.r;

	spinlock_acquire(&virtio_mmio->lock);
	ret.vqs_bitmap = atomic_exchange_explicit(&virtio_mmio->vqs_bitmap, 0U,
						  memory_order_relaxed);
	ret.reason     = atomic_load_relaxed(&virtio_mmio->reason);
	atomic_store_relaxed(&virtio_mmio->reason,
			     virtio_mmio_notify_reason_default());
	spinlock_release(&virtio_mmio->lock);

	ret.error = OK;

	object_put_virtio_mmio(virtio_mmio);
out:
	return ret;
}

error_t
hypercall_virtio_mmio_backend_acknowledge_reset(cap_id_t virtio_mmio_cap)
{
	error_t	  ret	 = OK;
	cspace_t *cspace = cspace_get_self();

	virtio_mmio_ptr_result_t p = cspace_lookup_virtio_mmio(
		cspace, virtio_mmio_cap, CAP_RIGHTS_VIRTIO_MMIO_CONFIG);
	if (compiler_unexpected(p.e != OK)) {
		ret = p.e;
		goto out;
	}
	virtio_mmio_t *virtio_mmio = p.r;

	spinlock_acquire(&virtio_mmio->lock);
	atomic_store_relaxed(&virtio_mmio->regs->status,
			     virtio_mmio_status_reg_default());
	spinlock_release(&virtio_mmio->lock);

	object_put_virtio_mmio(virtio_mmio);
out:
	return ret;
}

error_t
hypercall_virtio_mmio_backend_update_status(cap_id_t virtio_mmio_cap,
					    uint32_t val)
{
	error_t	  ret	 = OK;
	cspace_t *cspace = cspace_get_self();

	virtio_mmio_ptr_result_t p = cspace_lookup_virtio_mmio(
		cspace, virtio_mmio_cap, CAP_RIGHTS_VIRTIO_MMIO_CONFIG);
	if (compiler_unexpected(p.e != OK)) {
		ret = p.e;
		goto out;
	}
	virtio_mmio_t *virtio_mmio = p.r;

	spinlock_acquire(&virtio_mmio->lock);
	uint32_t status = virtio_mmio_status_reg_raw(
		atomic_load_relaxed(&virtio_mmio->regs->status));
	status |= val;
	atomic_store_relaxed(&virtio_mmio->regs->status,
			     virtio_mmio_status_reg_cast(status));
	spinlock_release(&virtio_mmio->lock);

	object_put_virtio_mmio(virtio_mmio);
out:
	return ret;
}

```

`hyp/vm/virtio_mmio/src/vdevice.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>

#include <hypconstants.h>
#include <hypcontainers.h>

#include <atomic.h>
#include <partition.h>
#include <spinlock.h>
#include <thread.h>
#include <util.h>
#include <virq.h>

#include <events/virtio_mmio.h>

#include <asm/nospec_checks.h>

#include "event_handlers.h"
#include "virtio_mmio.h"

static bool
virtio_mmio_access_allowed(size_t size, size_t offset)
{
	bool ret;

	// First check if the access is size-aligned
	if ((offset & (size - 1U)) != 0UL) {
		ret = false;
	} else if (size == sizeof(uint32_t)) {
		// Word accesses, always allowed
		ret = true;
	} else if (size == sizeof(uint8_t)) {
		// Byte accesses only allowed for config
		ret = ((offset >= (size_t)OFS_VIRTIO_MMIO_REGS_DEVICE_CONFIG) &&
		       (offset <=
			((size_t)((size_t)OFS_VIRTIO_MMIO_REGS_DEVICE_CONFIG +
				  (VIRTIO_MMIO_REG_CONFIG_BYTES - 1U)))));
	} else {
		// Invalid access size
		ret = false;
	}

	return ret;
}

vcpu_trap_result_t
virtio_mmio_default_write(const virtio_mmio_t *virtio_mmio, size_t write_offset,
			  size_t access_size, register_t val)
{
	vcpu_trap_result_t ret = VCPU_TRAP_RESULT_FAULT;
	if ((write_offset >= (size_t)OFS_VIRTIO_MMIO_REGS_DEVICE_CONFIG) &&
	    (write_offset <=
	     (size_t)((size_t)OFS_VIRTIO_MMIO_REGS_DEVICE_CONFIG +
		      VIRTIO_MMIO_REG_CONFIG_BYTES - 1U))) {
		index_t n =
			(index_t)(write_offset -
				  (size_t)OFS_VIRTIO_MMIO_REGS_DEVICE_CONFIG);
		// Loop through every byte
		register_t shifted_val = val;
		for (index_t i = 0U; i < access_size; i++) {
			atomic_store_relaxed(
				&virtio_mmio->regs->device_config.raw[n + i],
				(uint8_t)shifted_val);
			shifted_val >>= 8U;
		}
		ret = VCPU_TRAP_RESULT_EMULATED;
	} else {
		ret = VCPU_TRAP_RESULT_FAULT;
	}

	return ret;
}

static bool
virtio_mmio_write_queue_sel(virtio_mmio_t *virtio_mmio, uint32_t val)
{
	bool	       ret = true;
	index_result_t res = nospec_range_check(val, virtio_mmio->vqs_num);
	if (res.e == OK) {
		virtio_mmio->queue_sel = res.r;

		// Update corresponding banked registers with read
		// permission
		spinlock_acquire(&virtio_mmio->lock);
		atomic_store_relaxed(
			&virtio_mmio->regs->queue_num_max,
			virtio_mmio->banked_queue_regs[res.r].num_max);
		atomic_store_relaxed(
			&virtio_mmio->regs->queue_ready,
			virtio_mmio->banked_queue_regs[res.r].ready);
		spinlock_release(&virtio_mmio->lock);
	} else {
		ret = false;
	}

	return ret;
}

static bool
virtio_mmio_write_status_reg(virtio_mmio_t *virtio_mmio, uint32_t val)
{
	bool			    ret = true;
	virtio_mmio_notify_reason_t reason;

	if (val != 0U) {
		bool assert_virq = false;

		spinlock_acquire(&virtio_mmio->lock);

		virtio_mmio_status_reg_t old_val =
			atomic_load_relaxed(&virtio_mmio->regs->status);
		virtio_mmio_status_reg_t new_val =
			virtio_mmio_status_reg_cast(val);
		atomic_store_relaxed(&virtio_mmio->regs->status, new_val);

		reason = atomic_load_relaxed(&virtio_mmio->reason);

		if (!virtio_mmio_status_reg_get_driver_ok(&old_val) &&
		    virtio_mmio_status_reg_get_driver_ok(&new_val)) {
			virtio_mmio_notify_reason_set_driver_ok(&reason, true);
			assert_virq = true;

		} else if (!virtio_mmio_status_reg_get_failed(&old_val) &&
			   virtio_mmio_status_reg_get_failed(&new_val)) {
			virtio_mmio_notify_reason_set_failed(&reason, true);
			assert_virq = true;
		} else {
			// Nothing to do
		}

		atomic_store_relaxed(&virtio_mmio->reason, reason);

		spinlock_release(&virtio_mmio->lock);

		if (assert_virq) {
			atomic_thread_fence(memory_order_release);
			(void)virq_assert(&virtio_mmio->frontend_source, false);
		}
	} else if (virtio_mmio_status_reg_raw(atomic_load_relaxed(
			   &virtio_mmio->regs->status)) == 0U) {
		// We do not request a reset the first time the frontend
		// tries to write a zero to the status register
	} else {
		// Assert backend's IRQ to let the
		// backend know that a device reset has been requested.
		spinlock_acquire(&virtio_mmio->lock);
		virtio_mmio_status_reg_t status =
			atomic_load_relaxed(&virtio_mmio->regs->status);
		virtio_mmio_status_reg_set_device_needs_reset(&status, true);
		atomic_store_relaxed(&virtio_mmio->regs->status, status);

		reason = atomic_load_relaxed(&virtio_mmio->reason);
		virtio_mmio_notify_reason_set_reset_rqst(&reason, true);
		atomic_store_relaxed(&virtio_mmio->reason, reason);
		spinlock_release(&virtio_mmio->lock);

		// Clear all bits QueueReady for all queues in the
		// device.
		for (index_t i = 0; i < virtio_mmio->vqs_num; i++) {
			virtio_mmio->banked_queue_regs[i].ready = 0U;
		}
		atomic_store_relaxed(&virtio_mmio->regs->queue_ready, 0U);

		atomic_thread_fence(memory_order_release);
		ret = virq_assert(&virtio_mmio->frontend_source, false).r;
	}

	return ret;
}

static bool
virtio_mmio_write_dev_feat_sel(const virtio_mmio_t *virtio_mmio, uint32_t val)
{
	bool	       ret = true;
	index_result_t res = nospec_range_check(val, VIRTIO_MMIO_DEV_FEAT_NUM);
	if (res.e == OK) {
		// Update corresponding banked register
		atomic_store_relaxed(&virtio_mmio->regs->dev_feat,
				     virtio_mmio->banked_dev_feat[res.r]);
	} else {
		ret = false;
	}

	return ret;
}

static bool
virtio_mmio_write_drv_feat_sel(virtio_mmio_t *virtio_mmio, uint32_t val)
{
	bool	       ret = true;
	index_result_t res = nospec_range_check(val, VIRTIO_MMIO_DRV_FEAT_NUM);
	if (res.e == OK) {
		virtio_mmio->drv_feat_sel = res.r;
	} else {
		ret = false;
	}

	return ret;
}

static void
virtio_mmio_write_queue_notify(virtio_mmio_t *virtio_mmio, uint32_t val)
{
	virtio_mmio_notify_reason_t reason;

	spinlock_acquire(&virtio_mmio->lock);

	// Update bitmap of virtual queues to be notified
	(void)atomic_fetch_or_explicit(&virtio_mmio->vqs_bitmap, util_bit(val),
				       memory_order_relaxed);

	reason = atomic_load_relaxed(&virtio_mmio->reason);
	virtio_mmio_notify_reason_set_new_buffer(&reason, true);
	atomic_store_relaxed(&virtio_mmio->reason, reason);

	spinlock_release(&virtio_mmio->lock);

	// Assert backend's IRQ to notify the backend that there are new
	// buffers to process
	atomic_thread_fence(memory_order_release);
	(void)virq_assert(&virtio_mmio->frontend_source, false);
}

static void
virtio_mmio_write_interrupt_ack(virtio_mmio_t *virtio_mmio, uint32_t val)
{
#if defined(PLATFORM_NO_DEVICE_ATTR_ATOMIC_UPDATE) &&                          \
	PLATFORM_NO_DEVICE_ATTR_ATOMIC_UPDATE
	spinlock_acquire(&virtio_mmio->lock);
	uint32_t interrupt_status =
		atomic_load_relaxed(&virtio_mmio->regs->interrupt_status);
	interrupt_status &= ~val;
	atomic_store_relaxed(&virtio_mmio->regs->interrupt_status,
			     interrupt_status);
	spinlock_release(&virtio_mmio->lock);
#else
	(void)atomic_fetch_and_explicit(&virtio_mmio->regs->interrupt_status,
					~val, memory_order_relaxed);
#endif
}

static bool
virtio_mmio_vdevice_write(virtio_mmio_t *virtio_mmio, size_t offset,
			  uint32_t val, size_t access_size)
{
	bool ret = true;

	switch (offset) {
	case OFS_VIRTIO_MMIO_REGS_DEV_FEAT_SEL:
		ret = virtio_mmio_write_dev_feat_sel(virtio_mmio, val);
		break;

	case OFS_VIRTIO_MMIO_REGS_DRV_FEAT:
		virtio_mmio->banked_drv_feat[virtio_mmio->drv_feat_sel] = val;
		break;

	case OFS_VIRTIO_MMIO_REGS_DRV_FEAT_SEL:
		ret = virtio_mmio_write_drv_feat_sel(virtio_mmio, val);
		break;

	case OFS_VIRTIO_MMIO_REGS_QUEUE_SEL:
		ret = virtio_mmio_write_queue_sel(virtio_mmio, val);
		break;

	case OFS_VIRTIO_MMIO_REGS_QUEUE_NUM:
		virtio_mmio->banked_queue_regs[virtio_mmio->queue_sel].num =
			val;
		break;

	case OFS_VIRTIO_MMIO_REGS_QUEUE_READY:
		atomic_store_relaxed(&virtio_mmio->regs->queue_ready, val);

		virtio_mmio->banked_queue_regs[virtio_mmio->queue_sel].ready =
			val;
		break;

	case OFS_VIRTIO_MMIO_REGS_QUEUE_NOTIFY:
		virtio_mmio_write_queue_notify(virtio_mmio, val);
		break;

	case OFS_VIRTIO_MMIO_REGS_INTERRUPT_ACK:
		virtio_mmio_write_interrupt_ack(virtio_mmio, val);
		break;

	case OFS_VIRTIO_MMIO_REGS_STATUS:
		// We should not allow the frontend to write 0 to the device
		// status since a 0 status means that the device reset is
		// complete
		ret = virtio_mmio_write_status_reg(virtio_mmio, val);
		break;

	case OFS_VIRTIO_MMIO_REGS_QUEUE_DESC_LOW:
		virtio_mmio->banked_queue_regs[virtio_mmio->queue_sel].desc_low =
			val;
		break;

	case OFS_VIRTIO_MMIO_REGS_QUEUE_DESC_HIGH:
		virtio_mmio->banked_queue_regs[virtio_mmio->queue_sel]
			.desc_high = val;
		break;

	case OFS_VIRTIO_MMIO_REGS_QUEUE_DRV_LOW:
		virtio_mmio->banked_queue_regs[virtio_mmio->queue_sel].drv_low =
			val;
		break;

	case OFS_VIRTIO_MMIO_REGS_QUEUE_DRV_HIGH:
		virtio_mmio->banked_queue_regs[virtio_mmio->queue_sel].drv_high =
			val;
		break;

	case OFS_VIRTIO_MMIO_REGS_QUEUE_DEV_LOW:
		virtio_mmio->banked_queue_regs[virtio_mmio->queue_sel].dev_low =
			val;
		break;

	case OFS_VIRTIO_MMIO_REGS_QUEUE_DEV_HIGH:
		virtio_mmio->banked_queue_regs[virtio_mmio->queue_sel].dev_high =
			val;
		break;

	default:
		ret = (trigger_virtio_mmio_device_config_write_event(
			       virtio_mmio->device_type,
			       (const virtio_mmio_t *)virtio_mmio, offset, val,
			       access_size) == VCPU_TRAP_RESULT_EMULATED);
		break;
	}

	return ret;
}

vcpu_trap_result_t
virtio_mmio_handle_vdevice_access(vdevice_t *vdevice, size_t offset,
				  size_t access_size, register_t *value,
				  bool is_write)
{
	vcpu_trap_result_t ret;

	// Trap only writes from virtio's frontend
	if (!is_write) {
		ret = VCPU_TRAP_RESULT_UNHANDLED;
		goto out;
	}

	assert((vdevice != NULL) &&
	       (vdevice->type == VDEVICE_TYPE_VIRTIO_MMIO));
	virtio_mmio_t *virtio_mmio =
		virtio_mmio_container_of_frontend_device(vdevice);
	if (virtio_mmio == NULL) {
		ret = VCPU_TRAP_RESULT_UNHANDLED;
		goto out;
	}

	if (!virtio_mmio_access_allowed(access_size, offset)) {
		ret = VCPU_TRAP_RESULT_FAULT;
		goto out;
	}

	ret = virtio_mmio_vdevice_write(virtio_mmio, offset, (uint32_t)*value,
					access_size)
		      ? VCPU_TRAP_RESULT_EMULATED
		      : VCPU_TRAP_RESULT_FAULT;

out:
	return ret;
}

```

`hyp/vm/virtio_mmio/src/virtio_mmio.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>
#include <string.h>

#include <hypcontainers.h>

#include <atomic.h>
#include <compiler.h>
#include <hyp_aspace.h>
#include <log.h>
#include <memextent.h>
#include <object.h>
#include <partition.h>
#include <pgtable.h>
#include <spinlock.h>
#include <trace.h>
#include <util.h>
#include <vdevice.h>
#include <vic.h>

#include <events/virtio_mmio.h>

#include <asm/cache.h>
#include <asm/cpu.h>

#include "event_handlers.h"
#include "panic.h"
#include "virtio_mmio.h"

error_t
virtio_mmio_handle_object_create_virtio_mmio(virtio_mmio_create_t create)
{
	virtio_mmio_t *virtio_mmio = create.virtio_mmio;
	spinlock_init(&virtio_mmio->lock);

	return OK;
}

error_t
virtio_mmio_configure(virtio_mmio_t *virtio_mmio, memextent_t *memextent,
		      count_t vqs_num, virtio_option_flags_t flags,
		      virtio_device_type_t device_type)
{
	error_t ret = OK;

	assert(virtio_mmio != NULL);
	assert(memextent != NULL);

	// Memextent should only cover one contiguous virtio config page
	if ((memextent->type != MEMEXTENT_TYPE_BASIC) ||
	    (memextent->size != PGTABLE_VM_PAGE_SIZE) ||
	    (vqs_num > VIRTIO_MMIO_MAX_VQS)) {
		ret = ERROR_ARGUMENT_INVALID;
		goto out;
	}

	if (virtio_option_flags_get_valid_device_type(&flags)) {
		if (trigger_virtio_mmio_valid_device_type_event(device_type)) {
			virtio_mmio->device_type = device_type;
		} else {
			ret = ERROR_ARGUMENT_INVALID;
			goto out;
		}
	} else {
		virtio_mmio->device_type = VIRTIO_DEVICE_TYPE_INVALID;
	}

	if (virtio_mmio->me != NULL) {
		object_put_memextent(virtio_mmio->me);
	}

	virtio_mmio->me	     = object_get_memextent_additional(memextent);
	virtio_mmio->vqs_num = vqs_num;

out:
	return ret;
}

error_t
virtio_mmio_handle_object_activate_virtio_mmio(virtio_mmio_t *virtio_mmio)
{
	error_t ret = OK;

	assert(virtio_mmio != NULL);

	partition_t *partition = virtio_mmio->header.partition;

	if (virtio_mmio->me == NULL) {
		ret = ERROR_OBJECT_CONFIG;
		goto error_no_me;
	}

	virtio_mmio->frontend_device.type = VDEVICE_TYPE_VIRTIO_MMIO;
	ret = vdevice_attach_phys(&virtio_mmio->frontend_device,
				  virtio_mmio->me);
	if (ret != OK) {
		goto error_vdevice;
	}

	// Allocate banked registers based on how many virtual queues will be
	// used
	size_t alloc_size = virtio_mmio->vqs_num *
			    sizeof(virtio_mmio_banked_queue_registers_t);

	void_ptr_result_t alloc_ret =
		partition_alloc(partition, alloc_size, alignof(uint32_t *));
	if (alloc_ret.e != OK) {
		ret = ERROR_NOMEM;
		goto out;
	}
	(void)memset_s(alloc_ret.r, alloc_size, 0, alloc_size);

	virtio_mmio->banked_queue_regs =
		(virtio_mmio_banked_queue_registers_t *)alloc_ret.r;

	ret = trigger_virtio_mmio_device_config_activate_event(
		virtio_mmio->device_type, virtio_mmio);
	if (ret != OK) {
		goto out;
	}

	// Allocate virtio config page
	size_t size = virtio_mmio->me->size;
	if (size < sizeof(*virtio_mmio->regs)) {
		ret = ERROR_ARGUMENT_SIZE;
		goto out;
	}

	virt_range_result_t range = hyp_aspace_allocate(size);
	if (range.e != OK) {
		ret = range.e;
		goto out;
	}

	ret = memextent_attach(partition, virtio_mmio->me, range.r.base,
			       sizeof(*virtio_mmio->regs));
	if (ret != OK) {
		hyp_aspace_deallocate(partition, range.r);
		goto out;
	}

	virtio_mmio->regs = (virtio_mmio_regs_t *)range.r.base;
	virtio_mmio->size = range.r.size;

	// Flush cache before using the uncached mapping
	CACHE_CLEAN_OBJECT(*virtio_mmio->regs);

out:
	if (ret != OK) {
		vdevice_detach_phys(&virtio_mmio->frontend_device,
				    virtio_mmio->me);
	}
error_vdevice:
error_no_me:
	return ret;
}

void
virtio_mmio_handle_object_deactivate_virtio_mmio(virtio_mmio_t *virtio_mmio)
{
	assert(virtio_mmio != NULL);

	vic_unbind(&virtio_mmio->backend_source);
	vic_unbind(&virtio_mmio->frontend_source);

	vdevice_detach_phys(&virtio_mmio->frontend_device, virtio_mmio->me);
}

void
virtio_mmio_handle_object_cleanup_virtio_mmio(virtio_mmio_t *virtio_mmio)
{
	assert(virtio_mmio != NULL);

	partition_t *partition = virtio_mmio->header.partition;

	if (virtio_mmio->regs != NULL) {
		memextent_detach(partition, virtio_mmio->me);

		virt_range_t range = { .base = (uintptr_t)virtio_mmio->regs,
				       .size = virtio_mmio->size };

		hyp_aspace_deallocate(partition, range);

		virtio_mmio->regs = NULL;
		virtio_mmio->size = 0U;
	}

	if (virtio_mmio->banked_queue_regs != NULL) {
		size_t alloc_size =
			virtio_mmio->vqs_num *
			sizeof(virtio_mmio_banked_queue_registers_t);
		void *alloc_base = (void *)virtio_mmio->banked_queue_regs;

		error_t err = partition_free(partition, alloc_base, alloc_size);
		assert(err == OK);

		virtio_mmio->banked_queue_regs = NULL;
		virtio_mmio->vqs_num	       = 0U;
	}

	(void)trigger_virtio_mmio_device_config_cleanup_event(
		virtio_mmio->device_type, virtio_mmio);

	if (virtio_mmio->me != NULL) {
		object_put_memextent(virtio_mmio->me);
		virtio_mmio->me = NULL;
	}
}

void
virtio_mmio_unwind_object_activate_virtio_mmio(virtio_mmio_t *virtio_mmio)
{
	virtio_mmio_handle_object_deactivate_virtio_mmio(virtio_mmio);
	virtio_mmio_handle_object_cleanup_virtio_mmio(virtio_mmio);
}

error_t
virtio_mmio_backend_bind_virq(virtio_mmio_t *virtio_mmio, vic_t *vic,
			      virq_t virq)
{
	error_t ret = OK;

	assert(virtio_mmio != NULL);
	assert(vic != NULL);

	ret = vic_bind_shared(&virtio_mmio->backend_source, vic, virq,
			      VIRQ_TRIGGER_VIRTIO_MMIO_BACKEND);

	return ret;
}

void
virtio_mmio_backend_unbind_virq(virtio_mmio_t *virtio_mmio)
{
	assert(virtio_mmio != NULL);

	vic_unbind_sync(&virtio_mmio->backend_source);
}

error_t
virtio_mmio_frontend_bind_virq(virtio_mmio_t *virtio_mmio, vic_t *vic,
			       virq_t virq)
{
	error_t ret = OK;

	assert(virtio_mmio != NULL);
	assert(vic != NULL);

	ret = vic_bind_shared(&virtio_mmio->frontend_source, vic, virq,
			      VIRQ_TRIGGER_VIRTIO_MMIO_FRONTEND);

	return ret;
}

void
virtio_mmio_frontend_unbind_virq(virtio_mmio_t *virtio_mmio)
{
	assert(virtio_mmio != NULL);

	vic_unbind_sync(&virtio_mmio->frontend_source);
}

bool
virtio_mmio_frontend_handle_virq_check_pending(virq_source_t *source)
{
	assert(source != NULL);

	// Deassert backend's IRQ when get_notification has been called
	virtio_mmio_t *virtio_mmio =
		virtio_mmio_container_of_frontend_source(source);

	virtio_mmio_notify_reason_t reason =
		atomic_load_relaxed(&virtio_mmio->reason);
	return !virtio_mmio_notify_reason_is_equal(
		reason, virtio_mmio_notify_reason_default());
}

bool
virtio_mmio_backend_handle_virq_check_pending(virq_source_t *source)
{
	assert(source != NULL);

	// Deassert frontend's IRQ when interrupt_status is zero, meaning no
	// interrupts are pending to be handled
	virtio_mmio_t *virtio_mmio =
		virtio_mmio_container_of_backend_source(source);

	return (atomic_load_relaxed(&virtio_mmio->regs->interrupt_status) !=
		0U);
}

error_t
virtio_default_handle_object_activate(virtio_mmio_t *virtio_mmio)
{
	(void)virtio_mmio;
	return OK;
}

error_t
virtio_default_handle_object_cleanup(virtio_mmio_t *virtio_mmio)
{
	(void)virtio_mmio;
	return OK;
}

```

`hyp/vm/virtio_mmio/virtio_mmio.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

interface virtio_mmio

selector_event virtio_mmio_valid_device_type
	selector type_:		virtio_device_type_t
	return: bool = false

selector_event virtio_mmio_device_config_write
	selector type_:		virtio_device_type_t
	param virtio_mmio:	const virtio_mmio_t *
	param offset:		size_t
	param value:		register_t
	param access_size:	size_t
	return: vcpu_trap_result_t = VCPU_TRAP_RESULT_UNHANDLED

selector_event virtio_mmio_device_config_activate
	selector type_:		virtio_device_type_t
	param virtio_mmio:	virtio_mmio_t *
	return: error_t = OK

selector_event virtio_mmio_device_config_cleanup
	selector type_:		virtio_device_type_t
	param virtio_mmio:	virtio_mmio_t *
	return: error_t = OK

module virtio_mmio

subscribe object_create_virtio_mmio

subscribe object_activate_virtio_mmio
	unwinder(virtio_mmio)

subscribe object_deactivate_virtio_mmio

subscribe object_cleanup_virtio_mmio(virtio_mmio)

subscribe virq_check_pending[VIRQ_TRIGGER_VIRTIO_MMIO_FRONTEND]
	handler virtio_mmio_frontend_handle_virq_check_pending(source)

subscribe virq_check_pending[VIRQ_TRIGGER_VIRTIO_MMIO_BACKEND]
	handler virtio_mmio_backend_handle_virq_check_pending(source)

subscribe vdevice_access[VDEVICE_TYPE_VIRTIO_MMIO](vdevice, offset, access_size, value, is_write)

subscribe virtio_mmio_device_config_write[VIRTIO_DEVICE_TYPE_INVALID]
	handler virtio_mmio_default_write(virtio_mmio, offset, value, access_size)

subscribe virtio_mmio_device_config_activate[VIRTIO_DEVICE_TYPE_INVALID]
	handler virtio_default_handle_object_activate(virtio_mmio)

subscribe virtio_mmio_device_config_cleanup[VIRTIO_DEVICE_TYPE_INVALID]
	handler virtio_default_handle_object_cleanup(virtio_mmio)

```

`hyp/vm/virtio_mmio/virtio_mmio.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

extend cap_rights_virtio_mmio bitfield {
	0	bind_backend_virq	bool;
	1	bind_frontend_virq	bool;
	2	assert_virq		bool;
	3	config			bool;
};

define VIRTIO_MMIO_MAX_VQS constant type count_t = 16;

// Feature bits
define VIRTIO_F_VERSION_1 constant type index_t = 32;
define VIRTIO_F_ACCESS_PLATFORM constant type index_t = 33;
define VIRTIO_F_NOTIFICATION_DATA constant type index_t = 38;

define VIRTIO_MMIO_REG_CONFIG_BYTES constant type count_t = (PGTABLE_HYP_PAGE_SIZE - 0x100);

define VIRTIO_MMIO_DEV_FEAT_NUM constant type count_t = 2;
define VIRTIO_MMIO_DRV_FEAT_NUM constant type count_t = 2;

// Device Status
define virtio_mmio_status_reg bitfield<32> {
	2	driver_ok		bool;
	6	device_needs_reset	bool;
	7	failed			bool;
	others	unknown=0;
};

extend virtio_mmio object {
	flags				bitfield virtio_option_flags;
	me			pointer object memextent;
	backend_source		structure virq_source(contained);
	frontend_source		structure virq_source(contained);
	frontend_device		structure vdevice(contained);
	device_type			enumeration virtio_device_type;
	regs			pointer structure virtio_mmio_regs;
	size			size;
	vqs_num			type count_t;
	lock			structure spinlock;
	vqs_bitmap		type register_t(atomic);
	drv_feat_sel		uint32;
	queue_sel		uint32;
	reason			bitfield virtio_mmio_notify_reason(atomic);
	banked_dev_feat		array(VIRTIO_MMIO_DEV_FEAT_NUM) uint32;
	banked_drv_feat		array(VIRTIO_MMIO_DRV_FEAT_NUM) uint32;
	banked_queue_regs	pointer structure virtio_mmio_banked_queue_registers;
	pending_rst		bool;
};

define virtio_mmio_banked_queue_registers structure {
	num_max		uint32;
	num		uint32;
	ready		uint32;
	desc_low	uint32;
	desc_high	uint32;
	drv_low		uint32;
	drv_high	uint32;
	dev_low		uint32;
	dev_high	uint32;
};

define virtio_mmio_regs structure(aligned(PGTABLE_HYP_PAGE_SIZE)) {
	magic_value @ 0x000		uint32(atomic);
	version @ 0x004			uint32(atomic);
	dev_id @ 0x008			uint32(atomic);
	vendor_id @ 0x00c		uint32(atomic);
	dev_feat @ 0x010		uint32(atomic);
	dev_feat_sel @ 0x014		uint32(atomic);
	drv_feat @ 0x020		uint32(atomic);
	drv_feat_sel @ 0x024		uint32(atomic);
	queue_sel @ 0x030		uint32(atomic);
	queue_num_max @ 0x034		uint32(atomic);
	queue_num @ 0x038		uint32(atomic);
	queue_ready @ 0x044		uint32(atomic);
	queue_notify @ 0x050		uint32(atomic);
	interrupt_status @ 0x060	uint32(atomic);
	interrupt_ack @ 0x064		uint32(atomic);
	status @ 0x070			bitfield virtio_mmio_status_reg(atomic);
	queue_desc_low @ 0x080		uint32(atomic);
	queue_desc_high @ 0x084		uint32(atomic);
	queue_drv_low @ 0x090		uint32(atomic);
	queue_drv_high @ 0x094		uint32(atomic);
	queue_dev_low @ 0x0a0		uint32(atomic);
	queue_dev_high @ 0x0a4		uint32(atomic);
	config_gen @ 0x0fc		uint32(atomic);
	device_config @ 0x100		union virtio_config_space;
};

extend virq_trigger enumeration {
	virtio_mmio_backend;
	virtio_mmio_frontend;
};

extend vdevice_type enumeration {
	VIRTIO_MMIO;
};

```

`hyp/vm/vpm_base/build.conf`:

```conf
# © 2022 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

interface vpm
types vpm_base.tc
source hypercalls.c

```

`hyp/vm/vpm_base/src/hypercalls.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <hyptypes.h>

#include <hypcall_def.h>
#include <hyprights.h>

#include <atomic.h>
#include <compiler.h>
#include <cspace.h>
#include <cspace_lookup.h>
#include <object.h>
#include <partition.h>
#include <spinlock.h>
#include <vpm.h>

error_t
hypercall_vpm_group_configure(cap_id_t		       vpm_group_cap,
			      vpm_group_option_flags_t flags)
{
	error_t	  err;
	cspace_t *cspace = cspace_get_self();

	if (!vpm_group_option_flags_is_clean(flags)) {
		err = ERROR_UNIMPLEMENTED;
		goto out;
	}

	object_type_t	    type;
	object_ptr_result_t o = cspace_lookup_object_any(
		cspace, vpm_group_cap, CAP_RIGHTS_GENERIC_OBJECT_ACTIVATE,
		&type);
	if (compiler_unexpected(o.e != OK)) {
		err = o.e;
		goto out;
	}
	if (type != OBJECT_TYPE_VPM_GROUP) {
		err = ERROR_CSPACE_WRONG_OBJECT_TYPE;
		goto out_release_vpm_group;
	}
	vpm_group_t *vpm_group = o.r.vpm_group;

	spinlock_acquire(&vpm_group->header.lock);

	if (atomic_load_relaxed(&vpm_group->header.state) ==
	    OBJECT_STATE_INIT) {
		err = vpm_group_configure(vpm_group, flags);
	} else {
		err = ERROR_OBJECT_STATE;
	}

	spinlock_release(&vpm_group->header.lock);

out_release_vpm_group:
	object_put(type, o.r);
out:
	return err;
}

error_t
hypercall_vpm_group_attach_vcpu(cap_id_t vpm_group_cap, cap_id_t vcpu_cap,
				index_t index)
{
	error_t	  err;
	cspace_t *cspace = cspace_get_self();

	vpm_group_ptr_result_t vpm_group_r = cspace_lookup_vpm_group(
		cspace, vpm_group_cap, CAP_RIGHTS_VPM_GROUP_ATTACH_VCPU);
	if (compiler_unexpected(vpm_group_r.e) != OK) {
		err = vpm_group_r.e;
		goto out;
	}

	object_type_t	    type;
	object_ptr_result_t o = cspace_lookup_object_any(
		cspace, vcpu_cap, CAP_RIGHTS_GENERIC_OBJECT_ACTIVATE, &type);
	if (compiler_unexpected(o.e != OK)) {
		err = o.e;
		goto out_release_vic;
	}
	if (type != OBJECT_TYPE_THREAD) {
		err = ERROR_CSPACE_WRONG_OBJECT_TYPE;
		goto out_release_vcpu;
	}
	thread_t *thread = o.r.thread;

	spinlock_acquire(&thread->header.lock);
	if (atomic_load_relaxed(&thread->header.state) == OBJECT_STATE_INIT) {
		err = vpm_attach(vpm_group_r.r, thread, index);
	} else {
		err = ERROR_OBJECT_STATE;
	}
	spinlock_release(&thread->header.lock);

out_release_vcpu:
	object_put(type, o.r);
out_release_vic:
	object_put_vpm_group(vpm_group_r.r);
out:
	return err;
}

error_t
hypercall_vpm_group_bind_virq(cap_id_t vpm_group_cap, cap_id_t vic_cap,
			      virq_t virq)
{
	error_t	  err	 = OK;
	cspace_t *cspace = cspace_get_self();

	vpm_group_ptr_result_t p = cspace_lookup_vpm_group(
		cspace, vpm_group_cap, CAP_RIGHTS_VPM_GROUP_BIND_VIRQ);
	if (compiler_unexpected(p.e != OK)) {
		err = p.e;
		goto out;
	}
	vpm_group_t *vpm_group = p.r;

	vic_ptr_result_t v =
		cspace_lookup_vic(cspace, vic_cap, CAP_RIGHTS_VIC_BIND_SOURCE);
	if (compiler_unexpected(v.e != OK)) {
		err = v.e;
		goto out_vpm_group_release;
	}
	vic_t *vic = v.r;

	err = vpm_bind_virq(vpm_group, vic, virq);

	object_put_vic(vic);
out_vpm_group_release:
	object_put_vpm_group(vpm_group);
out:
	return err;
}

error_t
hypercall_vpm_group_unbind_virq(cap_id_t vpm_group_cap)
{
	error_t	  err	 = OK;
	cspace_t *cspace = cspace_get_self();

	vpm_group_ptr_result_t p = cspace_lookup_vpm_group(
		cspace, vpm_group_cap, CAP_RIGHTS_VPM_GROUP_BIND_VIRQ);
	if (compiler_unexpected(p.e != OK)) {
		err = p.e;
		goto out;
	}
	vpm_group_t *vpm_group = p.r;

	vpm_unbind_virq(vpm_group);

	object_put_vpm_group(vpm_group);
out:
	return err;
}

hypercall_vpm_group_get_state_result_t
hypercall_vpm_group_get_state(cap_id_t vpm_group_cap)
{
	hypercall_vpm_group_get_state_result_t ret    = { 0 };
	cspace_t			      *cspace = cspace_get_self();

	vpm_group_ptr_result_t p = cspace_lookup_vpm_group(
		cspace, vpm_group_cap, CAP_RIGHTS_VPM_GROUP_QUERY);
	if (compiler_unexpected(p.e != OK)) {
		ret.error = p.e;
		goto out;
	}
	vpm_group_t *vpm_group = p.r;

	vpm_state_t state = vpm_get_state(vpm_group);

	ret.error     = OK;
	ret.vpm_state = (uint64_t)state;

	object_put_vpm_group(vpm_group);
out:
	return ret;
}

```

`hyp/vm/vpm_base/vpm_base.tc`:

```tc
// © 2022 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

extend vpm_group object {
	options			bitfield vpm_group_option_flags;
};

define vpm_mode enumeration {
	NONE = 0;
	IDLE;
};

extend thread object {
	vpm_mode		enumeration vpm_mode;
};

```

`hyp/vm/vrtc_pl031/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

interface vrtc
events vrtc_pl031.ev
types vrtc_pl031.tc
source vrtc_pl031.c hypercalls.c

```

`hyp/vm/vrtc_pl031/src/hypercalls.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>

#include <hypcall_def.h>
#include <hyprights.h>

#include <addrspace.h>
#include <atomic.h>
#include <compiler.h>
#include <cspace.h>
#include <cspace_lookup.h>
#include <object.h>
#include <partition.h>
#include <platform_timer.h>
#include <preempt.h>
#include <spinlock.h>
#include <util.h>

error_t
hypercall_vrtc_configure(cap_id_t vrtc_cap, vmaddr_t ipa)
{
	error_t	  err	 = OK;
	cspace_t *cspace = cspace_get_self();

	if (!util_is_baligned(ipa, VRTC_DEV_SIZE) ||
	    util_add_overflows(ipa, VRTC_DEV_SIZE)) {
		err = ERROR_ADDR_INVALID;
		goto out;
	}

	vrtc_ptr_result_t vrtc_r = cspace_lookup_vrtc_any(
		cspace, vrtc_cap, CAP_RIGHTS_VRTC_CONFIGURE);
	if (compiler_unexpected(vrtc_r.e) != OK) {
		err = vrtc_r.e;
		goto out;
	}
	vrtc_t *vrtc = vrtc_r.r;

	spinlock_acquire(&vrtc->header.lock);
	if (atomic_load_relaxed(&vrtc->header.state) == OBJECT_STATE_INIT) {
		vrtc->ipa = ipa;
	} else {
		err = ERROR_OBJECT_STATE;
	}
	spinlock_release(&vrtc->header.lock);

	object_put_vrtc(vrtc);
out:
	return err;
}

error_t
hypercall_vrtc_set_time_base(cap_id_t vrtc_cap, nanoseconds_t time_base,
			     ticks_t sys_timer_ref)
{
	error_t	  err	 = OK;
	cspace_t *cspace = cspace_get_self();

	vrtc_ptr_result_t vrtc_r = cspace_lookup_vrtc(
		cspace, vrtc_cap, CAP_RIGHTS_VRTC_SET_TIME_BASE);
	if (compiler_unexpected(vrtc_r.e) != OK) {
		err = vrtc_r.e;
		goto out;
	}
	vrtc_t *vrtc = vrtc_r.r;

	if (vrtc->time_base != 0U) {
		// The time base has already been set once
		err = ERROR_BUSY;
		goto out_put_vrtc;
	}

	preempt_disable();
	ticks_t now = platform_timer_get_current_ticks();
	if (now < sys_timer_ref) {
		// The snapshot was taken in the future?!
		err = ERROR_ARGUMENT_INVALID;
		goto out_preempt;
	}

	ticks_t time_base_ticks = platform_timer_convert_ns_to_ticks(time_base);

	// Set the time_base to the moment the device was turned on. By using
	// "sys_timer_ref" instead of "now" we account for the time delta
	// between the moment the snapshot was taken and the moment the
	// hypercall is handled in the hypervisor.
	vrtc->time_base = time_base_ticks - sys_timer_ref;
	vrtc->lr	= (rtc_seconds_t)(time_base / TIMER_NANOSECS_IN_SECOND);

out_preempt:
	preempt_enable();
out_put_vrtc:
	object_put_vrtc(vrtc);
out:
	return err;
}

error_t
hypercall_vrtc_attach_addrspace(cap_id_t vrtc_cap, cap_id_t addrspace_cap)
{
	error_t	  err	 = OK;
	cspace_t *cspace = cspace_get_self();

	vrtc_ptr_result_t vrtc_r = cspace_lookup_vrtc(
		cspace, vrtc_cap, CAP_RIGHTS_VRTC_ATTACH_ADDRSPACE);
	if (compiler_unexpected(vrtc_r.e) != OK) {
		err = vrtc_r.e;
		goto out;
	}
	vrtc_t *vrtc = vrtc_r.r;

	addrspace_ptr_result_t addrspace_r = cspace_lookup_addrspace_any(
		cspace, addrspace_cap, CAP_RIGHTS_ADDRSPACE_MAP);
	if (compiler_unexpected(addrspace_r.e != OK)) {
		err = addrspace_r.e;
		goto out_release_vrtc;
	}
	addrspace_t *addrspace = addrspace_r.r;

	spinlock_acquire(&addrspace->header.lock);
	if (atomic_load_relaxed(&addrspace->header.state) !=
	    OBJECT_STATE_ACTIVE) {
		err = ERROR_OBJECT_STATE;
		goto out_release_addrspace;
	}

	err = addrspace_check_range(addrspace, vrtc->ipa, VRTC_DEV_SIZE);
	if (err != OK) {
		goto out_release_addrspace;
	}

	vrtc_t *old_vrtc = addrspace->vrtc;
	if (old_vrtc != NULL) {
		object_put_vrtc(old_vrtc);
	}

	addrspace->vrtc = object_get_vrtc_additional(vrtc);

out_release_addrspace:
	spinlock_release(&addrspace->header.lock);
	object_put_addrspace(addrspace);
out_release_vrtc:
	object_put_vrtc(vrtc);
out:
	return err;
}

```

`hyp/vm/vrtc_pl031/src/vrtc_pl031.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>

#include <atomic.h>
#include <compiler.h>
#include <log.h>
#include <object.h>
#include <panic.h>
#include <platform_timer.h>
#include <preempt.h>
#include <spinlock.h>
#include <thread.h>
#include <util.h>
#include <vic.h>

#include "event_handlers.h"

error_t
vrtc_pl031_handle_object_create_vrtc(vrtc_create_t params)
{
	vrtc_t *vrtc = params.vrtc;
	assert(vrtc != NULL);

	vrtc->ipa	= VMADDR_INVALID;
	vrtc->lr	= 0;
	vrtc->time_base = 0;

	return OK;
}

error_t
vrtc_pl031_handle_object_activate_vrtc(vrtc_t *vrtc)
{
	error_t err = OK;

	assert(vrtc != NULL);

	if (vrtc->ipa == VMADDR_INVALID) {
		// Not configured yet
		err = ERROR_OBJECT_CONFIG;
	}

	return err;
}

void
vrtc_pl031_handle_object_deactivate_addrspace(addrspace_t *addrspace)
{
	assert(addrspace != NULL);

	vrtc_t *vrtc = addrspace->vrtc;

	if (vrtc != NULL) {
		object_put_vrtc(vrtc);
		addrspace->vrtc = NULL;
	}
}

static void
vrtc_pl031_reg_read(vrtc_t *vrtc, size_t offset, register_t *value)
{
	if (offset == offsetof(vrtc_pl031_t, RTCDR)) {
		uint64_t now = platform_timer_get_current_ticks();
		*value = platform_timer_convert_ticks_to_ns(vrtc->time_base +
							    now) /
			 TIMER_NANOSECS_IN_SECOND;
	} else if (offset == offsetof(vrtc_pl031_t, RTCLR)) {
		*value = vrtc->lr;
	} else if (offset == offsetof(vrtc_pl031_t, RTCCR)) {
		// Always enabled
		*value = 1U;
	} else if ((offset >= offsetof(vrtc_pl031_t, RTCPeriphID0)) &&
		   (offset <= offsetof(vrtc_pl031_t, RTCPeriphID3))) {
		// Calculate which byte in the ID register they are after
		uint8_t id = (uint8_t)((offset -
					offsetof(vrtc_pl031_t, RTCPeriphID0)) >>
				       2);
		*value	   = ((register_t)VRTC_PL031_PERIPH_ID >> (id << 3)) &
			 0xffU;
	} else if ((offset >= offsetof(vrtc_pl031_t, RTCPCellID0)) &&
		   (offset <= offsetof(vrtc_pl031_t, RTCPCellID3))) {
		// Calculate which byte in the ID register they are after
		uint8_t id = (uint8_t)((offset -
					offsetof(vrtc_pl031_t, RTCPCellID0)) >>
				       2);
		*value = ((register_t)VRTC_PL031_PCELL_ID >> (id << 3)) & 0xffU;
	} else {
		// All other PL031 registers are treated as RAZ
		*value = 0U;
	}
}

static void
vrtc_pl031_reg_write(vrtc_t *vrtc, size_t offset, register_t *value)
{
	if (offset == offsetof(vrtc_pl031_t, RTCLR)) {
		ticks_t value_ticks = platform_timer_convert_ns_to_ticks(
			*value * TIMER_NANOSECS_IN_SECOND);
		preempt_disable();
		ticks_t now	= platform_timer_get_current_ticks();
		vrtc->time_base = value_ticks - now;
		preempt_enable();
		vrtc->lr = (rtc_seconds_t)(*value);
	}
	// The rest of the registers are WI.
}

vcpu_trap_result_t
vrtc_pl031_handle_vdevice_access_fixed_addr(vmaddr_t ipa, size_t access_size,
					    register_t *value, bool is_write)
{
	vcpu_trap_result_t ret = VCPU_TRAP_RESULT_UNHANDLED;

	thread_t *thread = thread_get_self();

	vrtc_t *vrtc = thread->addrspace->vrtc;
	if ((vrtc == NULL) || (vrtc->ipa == VMADDR_INVALID)) {
		// vRTC not initialised
		goto out;
	}

	if ((ipa < vrtc->ipa) || util_add_overflows(ipa, access_size) ||
	    ((ipa + access_size) > (vrtc->ipa + VRTC_DEV_SIZE))) {
		// Not vRTC
		goto out;
	}

	// Only 32-bit registers of PL031 are emulated
	if (access_size != sizeof(uint32_t) ||
	    !util_is_baligned(ipa, sizeof(uint32_t))) {
		ret = VCPU_TRAP_RESULT_FAULT;
		goto out;
	}

	size_t offset = (size_t)(ipa - vrtc->ipa);

	if (is_write) {
		vrtc_pl031_reg_write(vrtc, offset, value);
	} else {
		vrtc_pl031_reg_read(vrtc, offset, value);
	}

	ret = VCPU_TRAP_RESULT_EMULATED;

out:
	return ret;
}

```

`hyp/vm/vrtc_pl031/vrtc_pl031.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

module vrtc_pl031

subscribe object_create_vrtc(vrtc_create)

subscribe object_activate_vrtc

subscribe object_deactivate_addrspace

subscribe vdevice_access_fixed_addr

```

`hyp/vm/vrtc_pl031/vrtc_pl031.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

define rtc_seconds_t		newtype uint32;

define VRTC_DEV_SIZE constant size = PGTABLE_HYP_PAGE_SIZE;

define VRTC_PL031_PERIPH_ID constant uint32 = 0x00041031;
define VRTC_PL031_PCELL_ID constant uint32 = 0xb105f00d;

extend cap_rights_vrtc bitfield
{
	0	configure		bool;
	1	attach_addrspace	bool;
	2	set_time_base		bool;
};

// One vRTC object per VM. All the vCPUs in the VM get the same vRTC.
extend vrtc object {
	time_base		type ticks_t;
	lr			type rtc_seconds_t;
	ipa			type vmaddr_t;
};

extend addrspace object
{
	vrtc pointer object vrtc;
};

define vrtc_pl031 structure
{
	RTCDR @0x000		uint32;
	RTCMR @0x004		uint32;
	RTCLR @0x008		uint32;
	RTCCR @0x00C		uint32;
	RTCIMSC @0x010		uint32;
	RTCRRS @0x014		uint32;
	RTCMIS @0x018		uint32;
	RTCICR @0x01C		uint32;

	RTCPeriphID0 @0xfe0		uint32;
	RTCPeriphID1 @0xfe4		uint32;
	RTCPeriphID2 @0xfe8		uint32;
	RTCPeriphID3 @0xfec		uint32;
	RTCPCellID0 @0xff0		uint32;
	RTCPCellID1 @0xff4		uint32;
	RTCPCellID2 @0xff8		uint32;
	RTCPCellID3 @0xffc		uint32;
};

```

`hyp/vm/vtrbe/build.conf`:

```conf
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

base_module hyp/platform/trbe
base_module hyp/misc/vet
events vtrbe.ev
source vtrbe.c

```

`hyp/vm/vtrbe/src/vtrbe.c`:

```c
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <assert.h>
#include <hyptypes.h>

#include <hypregisters.h>

#include <compiler.h>
#include <cpulocal.h>
#include <preempt.h>
#include <scheduler.h>
#include <thread.h>
#include <vet.h>

#include <asm/barrier.h>

#include "event_handlers.h"
#include "trbe.h"

void
vtrbe_handle_boot_cpu_cold_init(void)
{
	ID_AA64DFR0_EL1_t id_aa64dfr0 = register_ID_AA64DFR0_EL1_read();
	// NOTE: ID_AA64DFR0.TraceBuffer just indicates if trace buffer is
	// implemented, so here we use equal for assertion.
	assert(ID_AA64DFR0_EL1_get_TraceBuffer(&id_aa64dfr0) == 1U);
}

error_t
vtrbe_handle_object_create_thread(thread_create_t thread_create)
{
	thread_t *thread = thread_create.thread;

	// MDCR_EL2.E2TB == 0b10 to prohibit trace EL2
	MDCR_EL2_set_E2TB(&thread->vcpu_regs_el2.mdcr_el2, 0x2);

	return OK;
}

void
vet_update_trace_buffer_status(thread_t *self)
{
	assert(self != NULL);

#if !DISABLE_TRBE
	// check/set by reading TBRLIMITR.EN == 1
	TRBLIMITR_EL1_t trb_limitr =
		register_TRBLIMITR_EL1_read_ordered(&vet_ordering);
	self->vet_trace_buffer_enabled = TRBLIMITR_EL1_get_E(&trb_limitr);
#endif
}

void
vet_flush_buffer(thread_t *self)
{
	assert(self != NULL);

	if (compiler_unexpected(self->vet_trace_buffer_enabled)) {
		__asm__ volatile("tsb csync" : "+m"(vet_ordering));
	}
}

void
vet_disable_buffer(void)
{
	TRBLIMITR_EL1_t trb_limitr =
		register_TRBLIMITR_EL1_read_ordered(&vet_ordering);
	TRBLIMITR_EL1_set_E(&trb_limitr, false);
	register_TRBLIMITR_EL1_write_ordered(trb_limitr, &vet_ordering);
}

static void
vtrbe_prohibit_registers_access(thread_t *self, bool prohibit)
{
	assert(self != NULL);

	// MDCR_EL2.E2TB == 0b11 to enable access to TRBE
	// MDCR_EL2.E2TB == 0b10 to disable access to TRBE
	uint8_t expect = prohibit ? 0x2U : 0x3U;

	MDCR_EL2_set_E2TB(&self->vcpu_regs_el2.mdcr_el2, expect);
	register_MDCR_EL2_write_ordered(self->vcpu_regs_el2.mdcr_el2,
					&vet_ordering);
}

void
vet_save_buffer_thread_context(thread_t *self)
{
	(void)self;
	vtrbe_prohibit_registers_access(self, true);
}

void
vet_restore_buffer_thread_context(thread_t *self)
{
	vtrbe_prohibit_registers_access(self, false);
}

void
vet_enable_buffer(void)
{
	TRBLIMITR_EL1_t trb_limitr =
		register_TRBLIMITR_EL1_read_ordered(&vet_ordering);
	TRBLIMITR_EL1_set_E(&trb_limitr, true);
	register_TRBLIMITR_EL1_write_ordered(trb_limitr, &vet_ordering);
}

void
vet_save_buffer_power_context(void)
{
	MDCR_EL2_t mdcr_el2;

	// Enable E2TB access
	mdcr_el2 = register_MDCR_EL2_read_ordered(&vet_ordering);
	MDCR_EL2_set_E2TB(&mdcr_el2, 3);
	register_MDCR_EL2_write_ordered(mdcr_el2, &vet_ordering);

	asm_context_sync_ordered(&vet_ordering);

	trbe_save_context_percpu(cpulocal_get_index());

	// Disable E2TB access
	MDCR_EL2_set_E2TB(&mdcr_el2, 2);
	register_MDCR_EL2_write_ordered(mdcr_el2, &vet_ordering);
}

void
vet_restore_buffer_power_context(void)
{
	MDCR_EL2_t mdcr_el2;

	// Enable E2TB access
	mdcr_el2 = register_MDCR_EL2_read_ordered(&vet_ordering);
	MDCR_EL2_set_E2TB(&mdcr_el2, 3);
	register_MDCR_EL2_write_ordered(mdcr_el2, &vet_ordering);

	asm_context_sync_ordered(&vet_ordering);

	trbe_restore_context_percpu(cpulocal_get_index());

	// Disable E2TB access
	MDCR_EL2_set_E2TB(&mdcr_el2, 2);
	register_MDCR_EL2_write_ordered(mdcr_el2, &vet_ordering);
}

vcpu_trap_result_t
vtrbe_handle_vcpu_trap_sysreg(ESR_EL2_ISS_MSR_MRS_t iss)
{
	vcpu_trap_result_t ret;

#if DISABLE_TRBE
	(void)iss;

	ret = VCPU_TRAP_RESULT_UNHANDLED;
#else
	thread_t *current = thread_get_self();

	if (compiler_expected((ESR_EL2_ISS_MSR_MRS_get_Op0(&iss) != 3U) ||
			      (ESR_EL2_ISS_MSR_MRS_get_Op1(&iss) != 0U) ||
			      (ESR_EL2_ISS_MSR_MRS_get_CRn(&iss) != 9U) ||
			      (ESR_EL2_ISS_MSR_MRS_get_CRm(&iss) != 11U))) {
		// Not a TRBE register access.
		ret = VCPU_TRAP_RESULT_UNHANDLED;
	} else if (!vcpu_option_flags_get_trace_allowed(
			   &thread->vcpu_options)) {
		// This VCPU isn't allowed to trace. Fault immediately.
		ret = VCPU_TRAP_RESULT_FAULT;
	} else if (!current->vet_trace_buffer_enabled) {
		// Lazily enable trace buffer register access and restore
		// context.
		current->vet_trace_buffer_enabled = true;

		// only enable the register access
		vtrbe_prohibit_registers_access(false);

		ret = VCPU_TRAP_RESULT_RETRY;
	} else {
		// Probably an attempted OS lock; fall back to default RAZ/WI.
		ret = VCPU_TRAP_RESULT_UNHANDLED;
	}
#endif

	return ret;
}

```

`hyp/vm/vtrbe/vtrbe.ev`:

```ev
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

module vtrbe

subscribe boot_cpu_cold_init()

// Lower priority than vcpu_arch_handle_object_create_thread so it can set up
// MDCR_EL2 properly after it has been initialised to the default value.
subscribe object_create_thread
	priority -10

subscribe vcpu_trap_sysreg_read
	handler vtrbe_handle_vcpu_trap_sysreg

subscribe vcpu_trap_sysreg_write
	handler vtrbe_handle_vcpu_trap_sysreg

```

`repolint.json`:

```json
{
  "$schema": "https://raw.githubusercontent.com/todogroup/repolinter/master/rulesets/schema.json",
  "version": 2,
  "axioms": {
    "linguist": "language",
    "licensee": "license",
    "packagers": "packager"
  },
  "rules": {
    "license-file-exists": {
      "level": "error",
      "rule": {
        "type": "file-existence",
        "options": {
          "globsAny": [
            "LICENSE*"
          ],
          "nocase": true
        }
      }
    },
    "readme-file-exists": {
      "level": "error",
      "rule": {
        "type": "file-existence",
        "options": {
          "globsAny": [
            "README*"
          ],
          "nocase": true
        }
      }
    },
    "contributing-file-exists": {
      "level": "warning",
      "rule": {
        "type": "file-existence",
        "options": {
          "globsAny": [
            "CONTRIBUTING*"
          ],
          "nocase": true
        }
      }
    },
    "code-of-conduct-file-exists": {
      "level": "warning",
      "rule": {
        "type": "file-existence",
        "options": {
          "globsAny": [
            "CODE-OF-CONDUCT*"
          ],
          "nocase": true
        }
      }
    },
    "changelog-file-exists": {
      "level": "warning",
      "rule": {
        "type": "file-existence",
        "options": {
          "globsAny": [
            "CHANGELOG*"
          ],
          "nocase": true
        }
      }
    },
    "readme-references-license": {
      "level": "error",
      "rule": {
        "type": "file-contents",
        "options": {
          "globsAll": [
            "README*"
          ],
          "content": "license",
          "flags": "i"
        }
      }
    },
    "binaries-not-present": {
      "level": "warning",
      "rule": {
        "type": "file-type-exclusion",
        "options": {
          "type": [
            "**/*.exe",
            "**/*.dll",
            "**/*.o",
            "**/*.so",
            "!node_modules/**"
          ]
        }
      }
    },
    "integrates-with-ci": {
      "level": "warning",
      "rule": {
        "type": "file-existence",
        "options": {
          "globsAny": [
            ".gitlab-ci.yml",
            ".travis.yml",
            "appveyor.yml",
            ".appveyor.yml",
            "circle.yml",
            ".circleci/config.yml",
            "Jenkinsfile",
            ".drone.yml",
            ".github/workflows/*",
            "azure-pipelines.yml"
          ]
        }
      }
    },
    "source-license-headers-exist": {
      "level": "error",
      "rule": {
        "type": "file-starts-with",
        "options": {
          "globsAll": [
            "**/*.py",
            "**/*.js",
            "**/*.c",
            "**/*.cc",
            "**/*.cpp",
            "**/*.h",
            "**/*.ts",
            "**/*.sh",
            "**/*.rs"
          ],
          "skip-paths-matching": {
            "patterns": [
              "qcbor/.*\\.[ch]",
              "tools/cpptest/gunyahkw.h",
              "tools/misc/convert-utf-8.sh"
            ]
          },
          "lineCount": 20,
          "patterns": [
            "© 20[2-9][0-9] Qualcomm Innovation Center, Inc",
            "SPDX-License-Identifier"
          ],
          "flags": "i"
        }
      }
    },
    "github-issue-template-exists": {
      "level": "warning",
      "rule": {
        "type": "file-existence",
        "options": {
          "globsAny": [
            "ISSUE_TEMPLATE*",
            ".github/ISSUE_TEMPLATE*"
          ]
        }
      }
    },
    "github-pull-request-template-exists": {
      "level": "warning",
      "rule": {
        "type": "file-existence",
        "options": {
          "globsAny": [
            "PULL_REQUEST_TEMPLATE*",
            ".github/PULL_REQUEST_TEMPLATE*"
          ]
        }
      }
    },
    "javascript-package-metadata-exists": {
      "level": "warning",
      "where": [
        "language=javascript"
      ],
      "rule": {
        "type": "file-existence",
        "options": {
          "globsAny": [
            "package.json"
          ]
        }
      }
    },
    "ruby-package-metadata-exists": {
      "level": "warning",
      "where": [
        "language=ruby"
      ],
      "rule": {
        "type": "file-existence",
        "options": {
          "globsAny": [
            "Gemfile"
          ]
        }
      }
    },
    "java-package-metadata-exists": {
      "level": "warning",
      "where": [
        "language=java"
      ],
      "rule": {
        "type": "file-existence",
        "options": {
          "globsAny": [
            "pom.xml",
            "build.xml",
            "build.gradle"
          ]
        }
      }
    },
    "python-package-metadata-exists": {
      "level": "warning",
      "where": [
        "language=python"
      ],
      "rule": {
        "type": "file-existence",
        "options": {
          "globsAny": [
            "setup.py",
            "requirements.txt"
          ]
        }
      }
    },
    "rust-package-metadata-exists": {
      "level": "error",
      "where": [
         "language=rust"
      ],
      "rule": {
        "type": "file-existence",
        "options": {
          "globsAny": [
            "Cargo.toml",
            "Cargo.lock"
          ]
        }
      }
    },
    "objective-c-package-metadata-exists": {
      "level": "warning",
      "where": [
        "language=objective-c"
      ],
      "rule": {
        "type": "file-existence",
        "options": {
          "globsAny": [
            "Cartfile",
            "Podfile",
            "*.podspec"
          ]
        }
      }
    },
    "swift-package-metadata-exists": {
      "level": "warning",
      "where": [
        "language=swift"
      ],
      "rule": {
        "type": "file-existence",
        "options": {
          "globsAny": [
            "Package.swift"
          ]
        }
      }
    },
    "erlang-package-metadata-exists": {
      "level": "warning",
      "where": [
        "language=erlang"
      ],
      "rule": {
        "type": "file-existence",
        "options": {
          "globsAny": [
            "rebar.config"
          ]
        }
      }
    },
    "elixir-package-metadata-exists": {
      "level": "warning",
      "where": [
        "language=elixir"
      ],
      "rule": {
        "type": "file-existence",
        "options": {
          "globsAny": [
            "mix.exs"
          ]
        }
      }
    },
    "license-detectable-by-licensee": {
      "level": "off",
      "where": [
        "license=*"
      ],
      "rule": {
        "type": "license-detectable-by-licensee",
        "options": {}
      }
    }
  }
}

```

`tools/build/__main__.py`:

```py
# coding: utf-8
#
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

"""
Gunyah build system.

This module is invoked by configure.py with the global variable `graph` set to
an instance of AbstractBuildGraph, which can be used to add rules, targets and
variables to the build graph.
"""

import os
import sys
import logging
import inspect
import re
from collections import namedtuple
from io import open

from ..utils.genfile import GenFile


try:
    FileNotFoundError
except NameError:
    FileNotFoundError = IOError

# Silence flake8 warnings about the externally-defined graph variable
graph = graph  # noqa: F821


logging.basicConfig()
logger = logging.getLogger(__name__)


#
# General setup
#
def relpath(path):
    return os.path.relpath(path, start=graph.root_dir)


class Items(dict):
    def add(self, key, value):
        if key not in self:
            super(Items, self).__setitem__(key, value)
        else:
            raise KeyError("Item duplicate {:s}".format(key))


class module_set(set):
    def __init__(self):
        self.module_re = re.compile(
            r'[A-Za-z][A-Za-z0-9_]*([/][A-Za-z][A-Za-z0-9_]*)*')
        super(module_set, self).__init__()

    def add(self, value):
        if not self.module_re.fullmatch(value):
            print("invalid module name:", value)
            sys.exit(1)
        super(module_set, self).add(value)


build_dir = graph.build_dir

config_base = 'config'
module_base = 'hyp'
arch_base = os.path.join(module_base, 'arch')
interface_base = os.path.join(module_base, 'interfaces')

conf_includes = set()
modules = module_set()
modules.add('arch')

interfaces = set()
objects = set()
external_objects = set()
guestapis = set()
types = set()
hypercalls = set()
registers = list()
test_programs = set()
sa_html = set()
asts = set()
defmap_frags = set()

event_sources = set()
modules_with_events = set()
interfaces_with_events = set()

template_engines = {}
TemplateEngine = namedtuple('TemplateEngine', ['sources', 'config'])

first_class_objects = Items()
first_class_templates = list()

template_engines['first_class_object'] = \
    TemplateEngine(first_class_templates, None)

typed_templates = list()

template_engines['typed'] = \
    TemplateEngine(typed_templates, None)

typed_guestapi_templates = list()

template_engines['typed_guestapi'] = \
    TemplateEngine(typed_guestapi_templates, None)

hypercalls_templates = list()

template_engines['hypercalls'] = \
    TemplateEngine(hypercalls_templates, None)

registers_templates = list()

template_engines['registers'] = \
    TemplateEngine(registers_templates, None)


shvars_re = re.compile(r'\$((\w+)\b|{(\w+)})')


def var_subst(w):
    def shrepl(match):
        name = match.group(2) or match.group(3)
        try:
            return graph.get_env(name)
        except KeyError:
            logger.error("Undefined environment variable: $%s", name)
            sys.exit(1)
    n = 1
    while n:
        w, n = shvars_re.subn(shrepl, w)
    return w


#
# Variant setup
#
def arch_match(arch_name):
    return arch_name in target_arch_names


def process_variant_conf(variant_key, conf, basename):
    graph.add_gen_source(conf)

    platform = variant_key == 'platform'
    featureset = variant_key == 'featureset'
    allow_arch = variant_key and not platform

    if platform:
        if basename in target_arch_names:
            logger.error("existing arch: %s", basename)
        target_arch_names.append(basename)
    with open(conf, 'r', encoding='utf-8') as f:
        for s in f.readlines():
            words = s.split()
            if not words or words[0].startswith('#'):
                # Skip comments or blank lines
                pass
            elif words[0] == 'include':
                include_conf = os.path.join(config_base,
                                            words[1] + '.conf')
                if include_conf not in conf_includes:
                    process_variant_conf(None, include_conf, None)
                    conf_includes.add(include_conf)
            elif featureset and words[0] == 'platforms':
                global featureset_platforms
                featureset_platforms = words[1:]
            elif platform and words[0] == 'base_arch':
                arch_conf = os.path.join(config_base, 'arch',
                                         words[1] + '.conf')
                process_variant_conf(variant_key, arch_conf, words[1])
            elif platform and words[0] == 'alias_arch':
                if words[1] in target_arch_names:
                    logger.error("Alias existing arch: %s",
                                 words[1])
                target_arch_names.append(words[1])
            elif platform and words[0] == 'is_abi':
                global abi_arch
                if abi_arch is not None:
                    logger.warning("Duplicate abi definitions: %s and %s",
                                   abi_arch, basename)
                abi_arch = basename
            elif platform and words[0] == 'defines_link':
                global link_arch
                if link_arch is not None:
                    logger.warning("Duplicate link definitions: %s and %s",
                                   link_arch, basename)
                link_arch = basename
            elif platform and words[0] == 'target_triple':
                global target_triple
                if target_triple is not None:
                    logger.warning(
                        "Duplicate target triple definitions: %s and %s",
                        target_triple, words[1])
                target_triple = words[1]
            elif words[0] == 'module':
                modules.add(words[1])
            elif words[0] == 'flags':
                variant_cflags.extend(map(var_subst, words[1:]))
            elif words[0] == 'ldflags':
                variant_ldflags.extend(map(var_subst, words[1:]))
            elif words[0] == 'configs':
                for c in map(var_subst, words[1:]):
                    add_global_define(c)
            elif allow_arch and words[0] == 'arch_module':
                if arch_match(words[1]):
                    modules.add(words[2])
            elif allow_arch and words[0] == 'arch_flags':
                if arch_match(words[1]):
                    variant_cflags.extend(map(var_subst, words[2:]))
            elif allow_arch and words[0] == 'arch_ldflags':
                if arch_match(words[1]):
                    variant_ldflags.extend(map(var_subst, words[2:]))
            elif allow_arch and words[0] == 'arch_configs':
                if arch_match(words[1]):
                    for c in map(var_subst, words[2:]):
                        add_global_define(c)
            else:
                # TODO: dependencies, configuration variables, etc
                # Restructure this to use a proper parser first
                logger.error('Unknown token "%s" in %s', words[0], conf)
                sys.exit(1)


true_strings = ('true', 't', '1', 'yes', 'y')
false_strings = ('false', 'f', '0', 'no', 'n')
all_arg = graph.get_argument('all', 'false').lower()
if all_arg in true_strings:
    default_all_variants = True
elif all_arg in false_strings:
    default_all_variants = False
else:
    logger.error("Argument all= must have a boolean value, not '%s'", all_arg)
    sys.exit(1)

missing_variant = False
abi_arch = None
link_arch = None
target_triple = None
target_arch_names = []
variant_cflags = []
variant_cppflags = []
variant_defines = []
variant_ldflags = []

featureset_platforms = ['*']

#
# Configs sanity checking
#
configs = {}


def check_global_define(d):
    try:
        define, val = d.split('=')
    except ValueError:
        logger.warning("invalid configuration: %s", d)

    if define in configs:
        if configs[define] == val:
            logger.warning("Duplicate configuration: %s", d)
        else:
            logger.error("Conflicting configuration: %s and %s",
                         '='.join([define, configs[define]]), d)
            sys.exit(-1)
    configs[define] = val


def add_global_define(d):
    check_global_define(d)
    variant_defines.append(d)


for variant_key in ('platform', 'featureset', 'quality'):
    try:
        variant_value = graph.get_env('VARIANT_' + variant_key)
    except KeyError:
        variant_arg = graph.get_argument(
            variant_key, 'all' if default_all_variants else None)

        import glob
        known_variants = frozenset(
            os.path.splitext(os.path.basename(f))[0]
            for f in glob.iglob(os.path.join(
                config_base, variant_key, '*.conf')))
        if not known_variants:
            logger.error('No variants known for key %s', variant_key)
            sys.exit(1)

        if variant_arg is None:
            logger.error('No variant specified for key %s; choices: %s',
                         variant_key, ', '.join(known_variants))
            missing_variant = True
            continue

        if variant_arg == 'all':
            selected_variants = known_variants
        else:
            selected_variants = frozenset(variant_arg.split(','))
            if not (selected_variants <= known_variants):
                logger.error("Unknown variants specified for key %s: %s; "
                             "choices: %s", variant_key,
                             ', '.join(selected_variants - known_variants),
                             ', '.join(known_variants))
                missing_variant = True
                continue

        for val in selected_variants:
            graph.add_variant(os.path.join(build_dir, val))(**{
                'VARIANT_' + variant_key: val
            })

        # Don't build anything until all variants are configured
        sys.exit()

    variant_conf = os.path.join(config_base, variant_key,
                                variant_value + '.conf')
    process_variant_conf(variant_key, variant_conf, variant_value)

if len(featureset_platforms) == 1 and \
        featureset_platforms[0] == '*':
    pass
else:
    if graph.get_env('VARIANT_platform') not in featureset_platforms:
        # Skip plaforms not supported in the featureset
        sys.exit(0)

if missing_variant:
    sys.exit(1)

for a in target_arch_names:
    graph.append_env('CODEGEN_ARCHS', '-a ' + a)


try:
    partial_link_arg = graph.get_env('PARTIAL_LINK')
except KeyError:
    partial_link_arg = graph.get_argument('partial_link', '0')
do_partial_link = partial_link_arg.lower() in true_strings

try:
    sa_enabled_arg = graph.get_env('ENABLE_SA')
except KeyError:
    sa_enabled_arg = graph.get_argument('enable_sa', '0')
do_sa_html = sa_enabled_arg.lower() in true_strings


#
# Match available template generators
#


def template_match(template_engine, d):
    try:
        return template_engines[template_engine]
    except KeyError:
        logger.error('Unknown template system "%s" in %s', template_engine, d)
        sys.exit(1)


#
# Architecture setup
#

# Add the arch-specific include directories for asm/ headers
for arch_name in target_arch_names:
    d = os.path.join(arch_base, arch_name, 'include')
    graph.append_env('CPPFLAGS', '-I ' + relpath(d))

# Add the arch generic include directory for asm-generic/ headers
graph.append_env('CPPFLAGS', '-I ' + os.path.join(
    relpath(arch_base), 'generic', 'include'))


# Set up for a freestanding ARMv8.2 EL2 target
graph.append_env('TARGET_CFLAGS', '-ffreestanding')
graph.append_env('TARGET_CFLAGS', '-ftls-model=local-exec')
graph.append_env('TARGET_CFLAGS', '-fpic')
if not do_partial_link:
    graph.append_env('TARGET_LDFLAGS', '-pie')

# Enable stack protection by default
graph.append_env('TARGET_CFLAGS', '-fstack-protector-strong')


#
# Toolchain setup
#

graph.add_env('ROOT_DIR', os.path.realpath(graph.root_dir))
graph.add_env('BUILD_DIR', os.path.realpath(build_dir))

try:
    llvm_root = graph.get_env('LLVM')
except KeyError:
    try:
        llvm_root = graph.get_env('QCOM_LLVM')
    except KeyError:
        logger.error(
            "Please set $QCOM_LLVM or $LLVM to the root of the prebuilt LLVM")
        sys.exit(1)

# Use a QC prebuilt LLVM
graph.add_env('CLANG', os.path.join(llvm_root, 'bin', 'clang'))
graph.add_env('CLANG_MAP', os.path.join(
    llvm_root, 'bin', 'clang-extdef-mapping'))

graph.add_env('FORMATTER', os.path.join(llvm_root, 'bin', 'clang-format'))

# Use Clang to compile.
graph.add_env('TARGET_TRIPLE', target_triple)
graph.add_env('TARGET_CC', '${CLANG} -target ${TARGET_TRIPLE}')

# Use Clang with LLD to link.
graph.add_env('TARGET_LD', '${TARGET_CC} -fuse-ld=lld')

# Use Clang to preprocess DSL files.
graph.add_env('CPP', '${CLANG}-cpp -target ${TARGET_TRIPLE}')

# Use C18. For the purposes of MISRA, the language is C99 and all differences
# between C99 and C18 are language extensions permitted by a project deviation
# from rule 1.2.
graph.append_env('CFLAGS', '-std=gnu18')
# Turn all warnings on as errors by default
graph.append_env('CFLAGS', '-Weverything')
graph.append_env('CFLAGS', '-Werror')
# Unused macros are expected
graph.append_env('CFLAGS', '-Wno-unused-macros')
# MISRA rule 16.4 requires default: in every switch, even if it is covered
graph.append_env('CFLAGS', '-Wno-covered-switch-default')
# No need for C++ compatibility
graph.append_env('CFLAGS', '-Wno-c++98-compat')
graph.append_env('CFLAGS', '-Wno-c++-compat')
# No need for pre-C99 compatibility; we always use C18
graph.append_env('CFLAGS', '-Wno-declaration-after-statement')
# No need for GCC compatibility
graph.append_env('CFLAGS', '-Wno-gcc-compat')
# Allow GCC's _Alignof(lvalue) as a project deviation from MISRA rule 1.2.
graph.append_env('CFLAGS', '-Wno-gnu-alignof-expression')
# Allow Clang nullability as a project deviation from MISRA rule 1.2.
graph.append_env('CFLAGS', '-Wno-nullability-extension')
# Automatically requiring negative capabilities breaks analysis of reentrant
# locks, like the preemption count.
graph.append_env('CFLAGS', '-Wno-thread-safety-negative')

# We depend on section garbage collection; otherwise there are undefined and
# unused symbols that will be pulled in and cause link failures
graph.append_env('CFLAGS', '-ffunction-sections')
graph.append_env('CFLAGS', '-fdata-sections')
if not do_partial_link:
    graph.append_env('LDFLAGS', '-Wl,--gc-sections')

# Ensure that there are no symbol clashes with externally linked objects.
graph.append_env('CFLAGS', '-fvisibility=hidden')

# Generate DWARF compatible with older T32 releases
graph.append_env('CFLAGS', '-gdwarf-4')

# Catch undefined switches during type system preprocessing
graph.append_env('CPPFLAGS', '-Wundef')
graph.append_env('CPPFLAGS', '-Werror')

# Add the variant-specific flags
if variant_cflags:
    graph.append_env('CFLAGS', ' '.join(variant_cflags))
if variant_cppflags:
    graph.append_env('CPPFLAGS', ' '.join(variant_cppflags))
    graph.append_env('CODEGEN_CONFIGS', ' '.join(variant_cppflags))
if variant_ldflags:
    graph.append_env('TARGET_LDFLAGS', ' '.join(variant_ldflags))

# On scons builds, the abs path may be put into the commandline, strip it out
# of the __FILE__ macro.
root = os.path.abspath(os.curdir) + os.sep
graph.append_env('CFLAGS',
                 '-fmacro-prefix-map={:s}={:s}'.format(root, ''))

graph.append_env('TARGET_CPPFLAGS', '-nostdlibinc')
graph.append_env('TARGET_LDFLAGS', '-nostdlib')
graph.append_env('TARGET_LDFLAGS', '-Wl,-z,max-page-size=0x1000')
graph.append_env('TARGET_LDFLAGS', '-Wl,-z,notext')

# Build rules
compdb_file = os.path.join(build_dir, 'compile_commands.json')
graph.add_compdb(compdb_file, form='clang')
# Compile a target C file.
graph.add_rule('cc',
               '$TARGET_CC $CFLAGS $CPPFLAGS $TARGET_CFLAGS $TARGET_CPPFLAGS '
               '$LOCAL_CFLAGS $LOCAL_CPPFLAGS -MD -MF ${out}.d '
               '-c -o ${out} ${in}',
               depfile='${out}.d', compdbs=[compdb_file])
# Preprocess a DSL file.
graph.add_rule('cpp-dsl', '${CPP} $CPPFLAGS $TARGET_CPPFLAGS $LOCAL_CPPFLAGS '
               '-undef $DSL_DEFINES -x c -P -MD -MF ${out}.d -MT ${out} '
               '${in} > ${out}',
               depfile='${out}.d')
# Link a target binary.
graph.add_rule('ld', '$TARGET_LD $LDFLAGS $TARGET_LDFLAGS $LOCAL_LDFLAGS '
               '${in} -o ${out}')

# CTU rule to generate the .ast files
ctu_dir = os.path.join(build_dir, "ctu")
graph.add_env('CTU_DIR', relpath(ctu_dir))
graph.add_rule('cc-ctu-ast',
               '$TARGET_CC $CFLAGS $CPPFLAGS $TARGET_CFLAGS $TARGET_CPPFLAGS '
               '$LOCAL_CFLAGS $LOCAL_CPPFLAGS '
               '-MD -MF ${out}.d -Wno-unused-command-line-argument '
               '-emit-ast -o${out} ${in}',
               depfile='${out}.d')

graph.add_env('COMPDB_DIR', compdb_file)

# CTU rule to generate the externalDefMap files
graph.add_rule('cc-ctu-map',
               '$CLANG_MAP -p $COMPDB_DIR ${in} | '
               'sed -e "s/\\$$/.ast/g; s| \\+${ROOT_DIR}/| |g" > '
               '${out}')

graph.add_rule('cc-ctu-all', 'cat ${in} > ${out}')

# Run the static analyzer
graph.add_rule('cc-analyze',
               '$TARGET_CC $CFLAGS $CPPFLAGS $TARGET_CFLAGS $TARGET_CPPFLAGS '
               '$LOCAL_CFLAGS $LOCAL_CPPFLAGS '
               '-Wno-unused-command-line-argument '
               '--analyze '
               '-Xanalyzer -analyzer-output=html '
               '-Xanalyzer -analyzer-config '
               '-Xanalyzer experimental-enable-naive-ctu-analysis=true '
               '-Xanalyzer -analyzer-config '
               '-Xanalyzer stable-report-filename=true '
               '-Xanalyzer -analyzer-config '
               '-Xanalyzer unroll-loops=true '
               '-Xanalyzer -analyzer-config '
               '-Xanalyzer ctu-dir=$CTU_DIR '
               '-Xanalyzer -analyzer-disable-checker '
               '-Xanalyzer alpha.core.FixedAddr '
               '-o ${out} '
               '${in}')


#
# Parse the module configurations
#
def process_dir(d, handler):
    conf = os.path.join(d, 'build.conf')
    with open(conf, 'r', encoding='utf-8') as f:
        handler(d, f)
        graph.add_gen_source(conf)


def module_local_headers_gen(d):
    return graph.future_alias(os.path.join(build_dir, d, 'local_headers_gen'))


def parse_module_conf(d, f):
    local_env = {}
    module = os.path.basename(d)
    local_headers_gen = module_local_headers_gen(d)
    local_headers = []
    add_include_dir(get_event_local_inc_dir(module), local_env)
    src_requires = (
        hyptypes_header,
        version_header,
        sym_version_header,
        registers_header,
        typed_headers_gen,
        event_headers_gen,
        hypercalls_headers_gen,
        objects_headers_gen,
        local_headers_gen,
    )
    objs = []
    have_events = False

    for s in f.readlines():
        words = s.split()
        if not words or words[0].startswith('#'):
            # Skip comments or blank lines
            pass
        elif words[0] == 'interface':
            for w in map(var_subst, words[1:]):
                interfaces.add(w)
        elif words[0] == 'types':
            for w in map(var_subst, words[1:]):
                types.add(add_type_dsl(d, w, local_env))
        elif words[0] == 'hypercalls':
            for w in map(var_subst, words[1:]):
                hypercalls.add(add_hypercall_dsl(d, w, local_env))
        elif words[0] == 'events':
            for w in map(var_subst, words[1:]):
                event_sources.add(add_event_dsl(d, w, local_env))
                have_events = True
        elif words[0] == 'registers':
            for w in map(var_subst, words[1:]):
                f = os.path.join(d, w)
                if f in registers:
                    raise KeyError("duplicate {:s}".format(f))
                registers.append(f)
        elif words[0] == 'local_include':
            add_include(d, 'include', local_env)
        elif words[0] == 'source':
            for w in map(var_subst, words[1:]):
                objs.append(add_source(d, w, src_requires, local_env))
        elif words[0] == 'external_object':
            if not do_partial_link:
                for w in map(var_subst, words[1:]):
                    external_objects.add(w)
        elif words[0] == 'flags':
            add_flags(map(var_subst, words[1:]), local_env)
        elif words[0] == 'configs':
            for c in map(var_subst, words[1:]):
                add_global_define(c)
        elif words[0] == 'macros':
            for w in map(var_subst, words[1:]):
                add_macro_include(d, 'include', w)
        elif words[0] == 'arch_types':
            if arch_match(words[1]):
                for w in map(var_subst, words[2:]):
                    types.add(add_type_dsl(
                        os.path.join(d, words[1]), w, local_env))
        elif words[0] == 'arch_hypercalls':
            if arch_match(words[1]):
                for w in map(var_subst, words[2:]):
                    f = os.path.join(words[1], w)
                    hypercalls.add(add_hypercall_dsl(d, f, local_env))
        elif words[0] == 'arch_events':
            if arch_match(words[1]):
                for w in map(var_subst, words[2:]):
                    event_sources.add(add_event_dsl(
                        os.path.join(d, words[1]), w, local_env))
                    have_events = True
        elif words[0] == 'arch_registers':
            if arch_match(words[1]):
                for w in map(var_subst, words[2:]):
                    f = os.path.join(d, words[1], w)
                    if f in registers:
                        raise KeyError("duplicate {:s}".format(f))
                    registers.append(f)
        elif words[0] == 'arch_local_include':
            if arch_match(words[1]):
                add_include(d, os.path.join(words[1], 'include'), local_env)
        elif words[0] == 'arch_source':
            if arch_match(words[1]):
                for w in map(var_subst, words[2:]):
                    objs.append(add_source(os.path.join(d, words[1]),
                                           w, src_requires, local_env))
        elif words[0] == 'arch_external_object':
            if not do_partial_link:
                if arch_match(words[1]):
                    for w in map(var_subst, words[2:]):
                        external_objects.add(w)
        elif words[0] == 'arch_flags':
            if arch_match(words[1]):
                add_flags(map(var_subst, words[2:]), local_env)
        elif words[0] == 'arch_configs':
            if arch_match(words[1]):
                for w in map(var_subst, words[2:]):
                    add_global_define(w)
        elif words[0] == 'arch_macros':
            if arch_match(words[1]):
                for w in map(var_subst, words[2:]):
                    add_macro_include(d, 'include', w)
        elif words[0] == 'first_class_object':
            first_class_objects.add(words[1], words[2:])
        elif words[0] == 'base_module':
            for w in map(var_subst, words[1:]):
                # Require the base module's generated headers
                local_headers.append(module_local_headers_gen(w))
                # FIXME: We can't properly determine whether there are
                # local_includes or not unless we do two-pass parsing of the
                # build configs, so we just add them all.
                logger.disabled = True
                add_include(w, 'include', local_env)
                add_include(os.path.join(build_dir, w), 'include', local_env)
                # FIXME: We assume module has all possible arch include dirs
                for arch_name in target_arch_names:
                    arch_dir = os.path.join(arch_name, 'include')
                    add_include(w, arch_dir, local_env)
                    add_include(os.path.join(build_dir, w),
                                arch_dir, local_env)
                logger.disabled = False
                modules.add(os.path.relpath(w, module_base))
                if w not in module_dirs:
                    module_dirs.append(w)
        elif words[0] == 'template' and words[1] == 'simple':
            for w in map(var_subst, words[2:]):
                add_simple_template(d, w, src_requires, local_env,
                                    local_headers=True, headers=local_headers,
                                    objects=objs)
        elif words[0] == 'template':
            ts = template_match(words[1], d)
            for w in map(var_subst, words[2:]):
                if add_template(ts, d, '', w, src_requires, local_env,
                                module):
                    have_events = True
        elif words[0] == 'arch_template' and words[1] == 'simple':
            if arch_match(words[2]):
                for w in map(var_subst, words[3:]):
                    add_simple_template(d, w, src_requires, local_env,
                                        local_headers=True,
                                        headers=local_headers,
                                        objects=objs, arch=words[2])
        elif words[0] == 'arch_template':
            ts = template_match(words[1], d)
            if arch_match(words[2]):
                for w in map(var_subst, words[3:]):
                    if add_template(ts, d, words[2], w, src_requires,
                                    local_env, module):
                        have_events = True
        elif words[0] == 'assert_config':
            test = ' '.join(words[1:])

            result = eval(test, {}, configs_as_ints)
            if result is True:
                continue
            logger.error('assert_config failed "%s" in module conf for %s',
                         test, d)
            sys.exit(1)
        else:
            # TODO: dependencies, configuration variables, etc
            # Restructure this to use a proper parser first
            logger.error('Unknown token "%s" in module conf for %s',
                         words[0], d)
            sys.exit(1)
    if have_events:
        local_headers.append(get_event_local_inc_file(module))
        modules_with_events.add(module)
        add_event_handlers(module)
    graph.add_alias(local_headers_gen, local_headers)


def parse_interface_conf(d, f):
    local_env = {}
    interface = os.path.basename(d)
    have_events = False
    for s in f.readlines():
        words = s.split()
        if not words or words[0].startswith('#'):
            # Skip comments or blank lines
            pass
        elif words[0] == 'types':
            for w in map(var_subst, words[1:]):
                types.add(add_type_dsl(d, w, local_env))
        elif words[0] == 'hypercalls':
            for w in map(var_subst, words[1:]):
                hypercalls.add(add_hypercall_dsl(d, w, local_env))
        elif words[0] == 'events':
            for w in map(var_subst, words[1:]):
                event_sources.add(add_event_dsl(d, w, local_env))
                have_events = True
        elif words[0] == 'registers':
            for w in map(var_subst, words[1:]):
                f = os.path.join(d, w)
                if f in registers:
                    raise KeyError("duplicate {:s}".format(f))
                registers.append(f)
        elif words[0] == 'macros':
            for w in map(var_subst, words[1:]):
                add_macro_include(d, 'include', w)
        elif words[0] == 'arch_types':
            if arch_match(words[1]):
                for w in map(var_subst, words[2:]):
                    types.add(add_type_dsl(
                        os.path.join(d, words[1]), w, local_env))
        elif words[0] == 'arch_hypercalls':
            if arch_match(words[1]):
                for w in map(var_subst, words[2:]):
                    f = os.path.join(words[1], w)
                    hypercalls.add(add_hypercall_dsl(d, f, local_env))
        elif words[0] == 'arch_events':
            if arch_match(words[1]):
                for w in map(var_subst, words[2:]):
                    event_sources.add(add_event_dsl(
                        os.path.join(d, words[1]), w, local_env))
                    have_events = True
        elif words[0] == 'arch_macros':
            if arch_match(words[1]):
                for w in map(var_subst, words[2:]):
                    add_macro_include(os.path.join(d, words[1], 'include', w))
        elif words[0] == 'first_class_object':
            first_class_objects.add(words[1], words[2:])
        elif words[0] == 'template' and words[1] == 'simple':
            for w in map(var_subst, words[2:]):
                add_simple_template(d, w, src_requires, local_env)
        elif words[0] == 'template':
            ts = template_match(words[1], d)
            for w in map(var_subst, words[2:]):
                if add_template(ts, d, '', w, None, local_env, None):
                    have_events = True
        else:
            # TODO: dependencies, configuration variables, etc
            # Restructure this to use a proper parser first
            logger.error('Unknown token "%s" in interface conf for %s',
                         words[0], d)
            sys.exit(1)
    if have_events:
        interfaces_with_events.add(interface)
        add_event_handlers(interface)


def add_include_dir(d, local_env):
    if not d.startswith(build_dir):
        if not os.path.isdir(d):
            logger.warning("include path: '{:s}' non-existant!".format(d))
    if 'LOCAL_CPPFLAGS' in local_env:
        local_env['LOCAL_CPPFLAGS'] += ' '
    else:
        local_env['LOCAL_CPPFLAGS'] = ''
    local_env['LOCAL_CPPFLAGS'] += '-iquote ' + relpath(d)


def add_include(module_dir, include, local_env):
    add_include_dir(os.path.join(module_dir, include), local_env)


def add_flags(flags, local_env):
    if 'LOCAL_CFLAGS' in local_env:
        local_env['LOCAL_CFLAGS'] += ' '
    else:
        local_env['LOCAL_CFLAGS'] = ''
    local_env['LOCAL_CFLAGS'] += ' '.join(flags)


def add_source_file(src, obj, requires, local_env):
    file_env = local_env.copy()
    if 'LOCAL_CPPFLAGS' not in file_env:
        file_env['LOCAL_CPPFLAGS'] = ''
    else:
        file_env['LOCAL_CPPFLAGS'] += ' '

    graph.add_target([obj], 'cc', [src], requires=requires,
                     **file_env)
    objects.add(obj)

    if do_sa_html and src.endswith(".c"):
        ast = os.path.join(ctu_dir, src + ".ast")
        graph.add_target([ast], 'cc-ctu-ast', [src], requires=requires,
                         **file_env)
        asts.add(ast)

        defmap_frag = os.path.join(ctu_dir, src + ".map")
        graph.add_target([defmap_frag], 'cc-ctu-map', [src], requires=requires,
                         **file_env)
        defmap_frags.add(defmap_frag)

        sa_html_dir = obj + ".html"
        graph.add_target([sa_html_dir], 'cc-analyze', [src], requires=requires,
                         depends=(ast_gen, defmap), **file_env)
        sa_html.add(sa_html_dir)


def add_source(module_dir, src, requires, local_env):
    if not src.endswith(".c") and not src.endswith(".S"):
        logger.error('unknown source file type for: %s', src)
        sys.exit(1)
    out_dir = os.path.join(build_dir, module_dir, 'obj')
    i = os.path.join(module_dir, 'src', src)
    o = os.path.join(out_dir, src + '.o')
    add_source_file(i, o, requires, local_env)
    return o


def add_macro_include(module_dir, include, src):
    graph.append_env('CPPFLAGS', '-imacros {:s}'
                     .format(relpath(os.path.join(module_dir, include, src))))


def add_preproc_dsl(module_dir, src, **local_env):
    out_dir = os.path.join(build_dir, module_dir)
    i = os.path.join(module_dir, src)
    o = os.path.join(out_dir, src + '.pp')
    graph.add_target([o], 'cpp-dsl', [i], **local_env)
    return o


def add_type_dsl(module_dir, src, local_env):
    return add_preproc_dsl(module_dir, src, DSL_DEFINES='-D__TYPED_DSL__',
                           **local_env)


def add_hypercall_dsl(module_dir, src, local_env):
    return add_preproc_dsl(module_dir, src, DSL_DEFINES='-D__HYPERCALLS_DSL__',
                           **local_env)


def add_event_dsl(module_dir, src, local_env):
    return add_preproc_dsl(module_dir, src, requires=(hypconstants_header,),
                           DSL_DEFINES='-D__EVENTS_DSL__', **local_env)


def add_template(ts, d, arch, tmpl_file, requires, local_env, module):
    ext = os.path.splitext(tmpl_file)[1]
    is_event = False
    is_module = module is not None
    if ext == '.h' and is_module:
        mod_gen_dir = os.path.join(objects_build_dir, module)
        add_include(mod_gen_dir, 'include', local_env)
    if ext == '.c' and not is_module:
        logger.error('C template specified for interface %s', d)
        sys.exit(1)
    else:
        ts.sources.append((d, tmpl_file, arch, requires, is_module, local_env))
        if ext == '.ev':
            is_event = True
    return is_event


def add_simple_template(d, t, requires, local_env, local_headers=False,
                        headers=None, objects=None, arch=''):
    i = os.path.join(d, arch, 'templates', t)
    out_name, ext = os.path.splitext(t)
    if ext != '.tmpl':
        logger.warning("Template filename does not end in .tmpl: %s", t)
    out_ext = os.path.splitext(out_name)[1]
    if out_ext == '.h' and headers is not None:
        if local_headers:
            out_dir = os.path.join(build_dir, d, arch, 'include')
            add_include_dir(out_dir, local_env)
        else:
            assert not arch
            out_dir = interface_gen_dir
        o = os.path.join(out_dir, out_name)
        headers.append(o)
    elif out_ext in ('.c', '.S') and objects is not None:
        out_dir = os.path.join(build_dir, d, arch, 'src')
        o = os.path.join(out_dir, out_name)
        oo = o + '.o'
        add_source_file(o, oo, requires, local_env)
        objects.append(oo)
    else:
        logger.error("Unsupported template output: %s", out_name)
        sys.exit(1)
    graph.add_target([o], 'code_gen_asm' if out_ext == '.S' else 'code_gen',
                     [i])


event_handler_modules = set()


def add_event_handlers(module):
    if module in event_handler_modules:
        return
    event_handler_modules.add(module)
    obj = get_event_src_file(module) + '.o'
    event_src_requires = (
        hyptypes_header,
        typed_headers_gen,
        get_event_inc_file(module),
    )
    add_source_file(get_event_src_file(module), obj, event_src_requires,
                    {})


# Header locations
interface_gen_dir = os.path.join(build_dir, 'interface', 'include')
graph.append_env('CPPFLAGS', '-I ' + relpath(interface_gen_dir))
objects_build_dir = os.path.join(build_dir, 'objects')
events_inc_dir = os.path.join(build_dir, 'events', 'include')
objects_headers_gen = graph.future_alias(
    os.path.join(build_dir, 'objects_headers_gen'))

# Support for the event generator
graph.append_env('CPPFLAGS', '-I ' + relpath(events_inc_dir))
event_headers_gen = graph.future_alias(
    os.path.join(build_dir, 'event_headers_gen'))

# Support for the hypercalls generator
hypercalls_headers_gen = graph.future_alias(
    os.path.join(build_dir, 'hypercalls_headers_gen'))


def get_event_local_inc_dir(module):
    return os.path.join(build_dir, 'events', module, 'include')


def get_event_local_inc_file(module):
    return os.path.join(get_event_local_inc_dir(module), 'event_handlers.h')


def get_event_inc_file(module):
    return os.path.join(events_inc_dir, 'events', module + '.h')


def get_event_src_file(module):
    return os.path.join(build_dir, 'events', 'src', module + '.c')


#
# Global generated headers depends
#
build_includes = os.path.join(build_dir, 'include')
hyptypes_header = os.path.join(build_includes, 'hyptypes.h')
hypconstants_header = os.path.join(build_includes, 'hypconstants.h')
registers_header = os.path.join(build_includes, 'hypregisters.h')
version_header = os.path.join(build_includes, 'hypversion.h')
sym_version_header = os.path.join(build_includes, 'hypsymversion.h')
graph.append_env('CPPFLAGS', '-I ' + relpath(build_includes))
typed_headers_gen = graph.future_alias(
    os.path.join(build_dir, 'typed_headers_gen'))

guestapi_interface_types = os.path.join(build_dir, 'guestapi', 'include',
                                        'guest_types.h')
#
# Hypercalls generated files
#
# FIXME: This is not hypervisor source, it should not be built.
# Generation temporarily hard coded here until better handling implemented
hypguest_interface_src = os.path.join(build_dir, 'guestapi', 'src',
                                      'guest_interface.c')
hypguest_interface_header = os.path.join(build_dir, 'guestapi', 'include',
                                         'guest_interface.h')

guestapis.add(hypguest_interface_header)
guestapis.add(hypguest_interface_src)

#
# Set up the simple code generator
#
codegen_script = os.path.join('tools', 'codegen', 'codegen.py')
graph.add_env('CODEGEN', relpath(codegen_script))
graph.add_rule('code_gen', '${CODEGEN} ${CODEGEN_ARCHS} ${CODEGEN_CONFIGS} '
               '-f ${FORMATTER} -o ${out} -d ${out}.d ${in}',
               depfile='${out}.d')
graph.add_rule('code_gen_asm', '${CODEGEN} ${CODEGEN_ARCHS} '
               '${CODEGEN_CONFIGS} -o ${out} -d ${out}.d ${in}',
               depfile='${out}.d')


#
# Set up the Clang static analyser
#
defmap = os.path.join(ctu_dir, "externalDefMap.txt")
ast_gen = graph.future_alias(os.path.join(build_dir, 'ast-gen'))

# Get all configs as Ints or strings
configs_as_ints = dict()


def configs_get_int(c):
    try:
        s = configs[c].strip('uU')
        return int(s, 0)
    except ValueError:
        return configs[c]


for c in configs:
    configs_as_ints[c] = configs_get_int(c)

#
# Collect the lists of objects, modules and interfaces
#
module_dirs = sorted(os.path.join(module_base, m) for m in modules)
for d in module_dirs:
    process_dir(d, parse_module_conf)
for i in sorted(interfaces):
    d = os.path.join(interface_base, i)
    process_dir(d, parse_interface_conf)


#
# Collect all defines and configs
#
def mkdirs(path):
    try:
        os.makedirs(path)
    except OSError as e:
        import errno
        if e.errno == errno.EEXIST:
            pass
        else:
            raise


define_file = os.path.join(build_dir, 'config.h')
mkdirs(os.path.split(define_file)[0])
graph.add_gen_output(define_file)

with GenFile(define_file, 'w') as f:
    if variant_defines:
        for define_arg in variant_defines:
            define, val = define_arg.split('=')
            f.write(u"#define {:s} {:s}\n".format(define, val))
    for i in sorted(interfaces):
        f.write(u"#define INTERFACE_{:s} 1\n".format(i.upper()))
    for i in sorted(modules):
        i = i.replace(os.path.sep, '_')
        f.write(u"#define MODULE_{:s} 1\n".format(i.upper()))

graph.append_env('CPPFLAGS', '-imacros {:s}'.format(relpath(define_file)))
graph.append_env('CODEGEN_CONFIGS',
                 '-imacros {:s}'.format(relpath(define_file)))


#
# Generate types and events for first class objects
#

def add_object_c_template(module, template, requires, object_str, target,
                          local_env):
    out = os.path.join(objects_build_dir, module, target)
    graph.add_target([out], 'object_gen_c', [template], OBJ=object_str,
                     depends=[objects_script])
    add_source_file(out, out + '.o', requires, local_env)


def add_object_h_template(module, template, requires, object_str, target,
                          is_module, local_env):
    if is_module:
        out = os.path.join(objects_build_dir, module, 'include', target)
    else:
        out = os.path.join(objects_incl_dir, target)
    # For now, add all headers here, in future, dependencies for local headers
    # could be more contrained to the module's source files
    objects_headers.append(out)
    graph.add_target([out], 'object_gen_c', [template], OBJ=object_str,
                     depends=[objects_script])


def add_object_event_template(module, template, object_str, target):
    object_ev = os.path.join(objects_build_dir, module, target)
    graph.add_target([object_ev], 'object_gen', [template], OBJ=object_str,
                     depends=[objects_script])
    event_sources.add(object_ev)


def add_object_type_template(module, template, object_str, target):
    object_tc = os.path.join(objects_build_dir, module, target)
    graph.add_target([object_tc], 'object_gen', [template], OBJ=object_str,
                     depends=[objects_script])
    types.add(object_tc)


objects_script = os.path.join('tools', 'objects', 'object_gen.py')
graph.add_env('OBJECTS', relpath(objects_script))
graph.add_rule('object_gen', '${OBJECTS} -t ${in} '
               '${OBJ} -o ${out}')
graph.add_rule('object_gen_c', '${OBJECTS} -t ${in} -f ${FORMATTER} '
               '${OBJ} -o ${out}')


objects_incl_dir = os.path.join(objects_build_dir, 'include')
fc_objects = []
for x in sorted(first_class_objects):
    fc_objects.append(','.join([x] + first_class_objects[x]))
fc_objects = ' '.join(fc_objects)
have_object_incl = False
objects_headers = []

for module_dir, target, arch, src_requires, is_module, local_env in \
        first_class_templates:
    ext = os.path.splitext(target)[1]
    module = os.path.basename(module_dir)
    template = os.path.join(module_dir, arch, 'templates', target + '.tmpl')
    if ext == '.ev':
        add_object_event_template(module, template, fc_objects, target)
    elif ext == '.tc':
        add_object_type_template(module, template, fc_objects, target)
    elif ext == '.c':
        add_object_c_template(module, template, src_requires, fc_objects,
                              target, local_env)
    elif ext == '.h':
        add_object_h_template(module, template, src_requires, fc_objects,
                              target, is_module, local_env)
        if not is_module:
            have_object_incl = True
    else:
        logger.error('Unsupported first_class_object target "%s" in %s',
                     target, module_dir)
        sys.exit(1)

if have_object_incl:
    graph.append_env('CPPFLAGS', '-I ' + relpath(objects_incl_dir))

# An alias target is used to order header generation before source compliation
graph.add_alias(objects_headers_gen, objects_headers)


#
# Setup the types generator
#
types_script = os.path.join('tools', 'typed', 'type_gen.py')
types_pickle = os.path.join(build_dir, 'types.pickle')

graph.add_rule('types_parse', '${TYPED} -a ${ABI} -d ${out}.d '
               '${in} -P ${out}', depfile='${out}.d')
graph.add_target([types_pickle], 'types_parse', sorted(types), ABI=abi_arch)

graph.add_env('TYPED', relpath(types_script))
graph.add_rule('gen_types', '${TYPED} -a ${ABI} -f ${FORMATTER} -d ${out}.d '
               '-p ${in} -o ${out}', depfile='${out}.d')
graph.add_target(hyptypes_header, 'gen_types', types_pickle, ABI=abi_arch)

# gen guest type
graph.add_rule('gen_public_types',
               '${TYPED} --public -a ${ABI} -f ${FORMATTER} -d ${out}.d '
               '-p ${in} -o ${out}', depfile='${out}.d')
graph.add_target(guestapi_interface_types, 'gen_public_types',
                 types_pickle, ABI=abi_arch)

graph.add_rule('gen_types_tmpl', '${TYPED} -a ${ABI} -f ${FORMATTER} '
               '-d ${out}.d -t ${TEMPLATE} -p ${in} -o ${out}',
               depfile='${out}.d')

graph.add_rule('gen_public_types_tmpl', '${TYPED} --public -a ${ABI} '
               '-f ${FORMATTER} -d ${out}.d -t ${TEMPLATE} -p ${in} -o ${out}',
               depfile='${out}.d')

typed_headers = []
for module_dir, target, arch, src_requires, is_module, local_env in \
        typed_templates:
    ext = os.path.splitext(target)[1]
    template = os.path.join(module_dir, arch, 'templates', target + '.tmpl')
    if ext == '.h':
        out = os.path.join(build_dir, 'include', target)
        typed_headers.append(out)

        graph.add_target([out], 'gen_types_tmpl', types_pickle,
                         depends=[template], TEMPLATE=relpath(template),
                         ABI=abi_arch)
    elif ext == '.c':
        out = os.path.join(build_dir, module_dir, target)

        graph.add_target([out], 'gen_types_tmpl', types_pickle,
                         depends=[template], TEMPLATE=relpath(template),
                         ABI=abi_arch)
        add_source_file(out, out + '.o', src_requires, local_env)
    else:
        logger.error('Unsupported typed_template target "%s" in %s',
                     target, module_dir)
        sys.exit(1)

graph.add_alias(typed_headers_gen, typed_headers)

for module_dir, target, arch, src_requires, is_module, local_env in \
        typed_guestapi_templates:
    assert (is_module)
    ext = os.path.splitext(target)[1]
    template = os.path.join(module_dir, arch, 'templates', target + '.tmpl')
    if ext == '.h':
        subdir = 'include'
    elif ext == '.c':
        subdir = 'src'
    else:
        logger.error('Unsupported typed_guestapi target "%s" in %s',
                     target, module_dir)

    out = os.path.join(build_dir, 'guestapi', subdir, 'guest_' + target)
    graph.add_target([out], 'gen_public_types_tmpl', types_pickle,
                     depends=[template], TEMPLATE=relpath(template),
                     ABI=abi_arch)
    guestapis.add(out)

guestapis.add(guestapi_interface_types)

guestapi_gen = os.path.join(build_dir, 'guestapi_gen')
graph.add_alias(guestapi_gen, sorted(guestapis))
graph.add_default_target(guestapi_gen, True)

#
# Setup the hypercalls generator
#
hypercalls_script = os.path.join('tools', 'hypercalls', 'hypercall_gen.py')
graph.add_env('HYPERCALLS', relpath(hypercalls_script))

hypercalls_template_path = os.path.join('tools', 'hypercalls', 'templates')
hypercalls_guest_templates = (('guest_interface.c', hypguest_interface_src),
                              ('guest_interface.h', hypguest_interface_header))

# FIXME:
# FIXME: upgrade Lark and remove LANG env workaround.
graph.add_rule('hypercalls_gen', 'LANG=C.UTF-8'
               ' ${HYPERCALLS} -a ${ABI} -f ${FORMATTER}'
               ' -d ${out}.d -t ${TEMPLATE} -p ${TYPES_PICKLE} ${in}'
               ' -o ${out}', depfile='${out}.d')

hypercalls_headers = []

for module_dir, target, arch, src_requires, is_module, local_env in \
        hypercalls_templates:
    template = os.path.join(module_dir, arch, 'templates', target + '.tmpl')
    out_ext = os.path.splitext(target)[1]
    if out_ext == '.h':
        out = os.path.join(build_dir, 'include', target)
    elif out_ext in ('.c', '.S'):
        out = os.path.join(build_dir, module_dir, 'src', target)
    else:
        logger.error("Unsupported template file: %s", target)
        sys.exit(1)
    graph.add_target([out], 'hypercalls_gen', sorted(hypercalls),
                     TEMPLATE=relpath(template), ABI=abi_arch,
                     TYPES_PICKLE=relpath(types_pickle),
                     depends=[types_pickle, template])
    if out_ext == '.h':
        hypercalls_headers.append(out)
    elif out_ext in ('.c', '.S'):
        oo = out + '.o'
        requires = (
            hyptypes_header,
            hypercalls_headers_gen,
            typed_headers_gen,
            event_headers_gen
        )
        local_env = {}
        add_source_file(out, oo, requires, local_env)
graph.add_alias(hypercalls_headers_gen, hypercalls_headers)

# FIXME: provide a better/standalone way to generate guest headers
for tmpl, out_name in hypercalls_guest_templates:
    template = os.path.join(hypercalls_template_path, tmpl + '.tmpl')
    graph.add_target(out_name, 'hypercalls_gen', sorted(hypercalls),
                     TEMPLATE=relpath(template), ABI=abi_arch,
                     TYPES_PICKLE=relpath(types_pickle),
                     depends=[types_pickle, template])


#
# Setup the events generators
#
def event_template(name):
    return os.path.join('tools', 'events', 'templates', name + '.tmpl')


events_script = os.path.join('tools', 'events', 'event_gen.py')
graph.add_env('EVENTS', relpath(events_script))
event_handlers_tmpl = event_template('handlers.h')
event_triggers_tmpl = event_template('triggers.h')
events_pickle = os.path.join(build_dir, 'events.pickle')
event_src_tmpl = event_template('c')

graph.add_rule('event_parse',
               '${EVENTS} ${INCLUDES} -d ${out}.d ${in} -P ${out}',
               depfile='${out}.d', restat=True)
graph.add_target([events_pickle], 'event_parse', sorted(event_sources))

graph.add_rule('event_gen', '${EVENTS} -t ${TEMPLATE} -m ${MODULE} ${OPTIONS}'
               '${INCLUDES} -d ${out}.d -p ${in} -o ${out}',
               depfile='${out}.d', restat=True)

event_headers = []
for module in sorted(interfaces_with_events | modules_with_events):
    event_out = get_event_inc_file(module)
    event_headers.append(event_out)
    graph.add_target([event_out], 'event_gen', events_pickle,
                     MODULE=module, TEMPLATE=relpath(event_triggers_tmpl),
                     depends=[event_triggers_tmpl])
    event_out = get_event_src_file(module)
    graph.add_target([event_out], 'event_gen', events_pickle,
                     MODULE=module, TEMPLATE=relpath(event_src_tmpl),
                     depends=[event_src_tmpl])
#                     OPTIONS='-f ${FORMATTER}',

# An alias target is used to order header generation before source compliation
graph.add_alias(event_headers_gen, event_headers)

for module in sorted(modules_with_events):
    # Gen handler headers
    event_out = get_event_local_inc_file(module)
    graph.add_target([event_out], 'event_gen', events_pickle,
                     MODULE=module, TEMPLATE=relpath(event_handlers_tmpl),
                     depends=[event_handlers_tmpl])

# Generate the static analysis definition map and ASTs
graph.add_target([defmap], 'cc-ctu-all', sorted(defmap_frags))
graph.add_alias(ast_gen, sorted(asts))

#
# Generate register accessors
#

registers_script = os.path.join('tools', 'registers', 'register_gen.py')
graph.add_env('REGISTERS', relpath(registers_script))
graph.add_rule('registers_gen', '${REGISTERS} -t ${TEMPLATE} -f ${FORMATTER} '
               '-o ${out} ${in}')

registers_pp = list()

# Pre-process the register scripts
for f in registers:
    f_pp = os.path.join(build_dir, f + '.pp')
    graph.add_target([f_pp], 'cpp-dsl', [f])
    registers_pp.append(f_pp)

for module_dir, target, arch, src_requires, is_module, local_env in \
        registers_templates:
    template = os.path.join(module_dir, arch, 'templates', target + '.tmpl')

    header = os.path.join(build_includes, target)
    graph.add_target([header], 'registers_gen', registers_pp,
                     TEMPLATE=relpath(template),
                     depends=[template, registers_script])

#
# Build version setup
#
version_file = os.path.join('hyp', 'core', 'boot', 'include', 'version.h')

if os.path.exists(version_file):
    graph.add_rule('version_copy', 'cp ${in} ${out}')
    graph.add_target([version_header], 'version_copy', [version_file])
else:
    ver_script = os.path.join('tools', 'build', 'gen_ver.py')
    graph.add_rule('version_gen', 'PYTHONPATH=' +
                   relpath(os.path.join('tools', 'utils')) + ' ' +
                   relpath(ver_script) + ' -C ' + relpath('.') +
                   ' -o ${out}', restat=True)
    import subprocess
    gitdir = subprocess.check_output(['git', 'rev-parse', '--git-dir'])
    gitdir = gitdir.decode('utf-8').strip()
    graph.add_target([version_header], 'version_gen',
                     ['{:s}/logs/HEAD'.format(gitdir)], always=True)

#
# Symbols version setup
#
sym_ver_script = os.path.join('tools', 'build', 'gen_sym_ver.py')
graph.add_rule('sym_version_gen', relpath(sym_ver_script) + ' > ${out}')
graph.add_target([sym_version_header], 'sym_version_gen', always=True)

#
# Includes setup
#

# Add module interfaces to the global CPPFLAGS
for interface in sorted(interfaces):
    d = os.path.join(interface_base, interface, 'include')
    graph.append_env('CPPFLAGS', '-I ' + relpath(d))

#
# Top-level targets
#

# Run the static analyser if 'enable_sa' is set in command line
if do_sa_html:
    sa_alias = os.path.join(build_dir, 'sa-html')
    graph.add_alias(sa_alias, sa_html)
    graph.add_default_target(sa_alias)

# Pre-process the linker script
linker_script_in = os.path.join(arch_base, link_arch, 'link.lds')
linker_script = os.path.join(build_dir, 'link.lds.pp')
graph.add_target([linker_script], 'cpp-dsl',
                 [linker_script_in], requires=[hypconstants_header])

# Link the hypervisor ELF file
if do_partial_link:
    hyp_elf = os.path.join(build_dir, 'hyp.o')
    graph.append_env('TARGET_LDFLAGS', '-r -Wl,-x')
    graph.add_default_target(linker_script)
else:
    hyp_elf = os.path.join(build_dir, 'hyp.elf')
    graph.append_env('TARGET_LDFLAGS',
                     '-Wl,-T,{:s}'.format(relpath(linker_script)))
graph.add_target([hyp_elf], 'ld', sorted(objects | external_objects),
                 depends=[linker_script])
graph.add_default_target(hyp_elf)


#
# Python dependencies
#
for m in list(sys.modules.values()) + [relpath]:
    try:
        f = inspect.getsourcefile(m)
    except TypeError:
        continue
    if f is None:
        continue
    f = os.path.relpath(f)
    if f.startswith('../'):
        continue
    graph.add_gen_source(f)

```

`tools/build/gen_sym_ver.py`:

```py
#!/usr/bin/env python3
# coding: utf-8
#
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

from random import SystemRandom
rng = SystemRandom()
print("#define HYP_SYM_VERSION 0x{:x}".format(rng.getrandbits(64)))

```

`tools/build/gen_ver.py`:

```py
#!/usr/bin/env python3
# coding: utf-8
#
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

import argparse
import subprocess
import sys
from datetime import datetime

from genfile import GenFile


def main():
    args = argparse.ArgumentParser()

    args.add_argument("-o", "--output", help="Output file (default: stdout)",
                      default=None)
    args.add_argument("-c", dest='commit', help="Use specified GIT revision",
                      default="HEAD")
    args.add_argument("-C", dest='path',
                      help="Run GIT in the specified directory", default=None)
    args.add_argument("-n", dest='now', action='store_true',
                      help="Use now as timestamp")

    options = args.parse_args()

    cwd = options.path

    ret = subprocess.run(['git', 'diff', options.commit, '--quiet'],
                         cwd=cwd, stdout=subprocess.PIPE)
    dirty = ret.returncode

    ret = subprocess.run(['git', 'rev-parse', '--short', options.commit],
                         cwd=cwd, stdout=subprocess.PIPE)
    if ret.returncode:
        raise Exception('git rev-parse failed\n', ret.stderr)

    rev = ret.stdout.decode("utf-8").strip()

    id = rev + ('-dirty' if dirty else '')
    if options.now or dirty:
        utcnow = datetime.utcnow()
        utcnow = utcnow.replace(microsecond=0)
        time = utcnow.isoformat(sep=' ')
        time = time + ' UTC'
    else:
        ret = subprocess.run(['git', 'show', '-s', '--pretty=%cd',
                              '--date=iso-local', options.commit],
                             cwd=cwd, env={'TZ': 'UTC'},
                             stdout=subprocess.PIPE)
        if ret.returncode:
            raise Exception('git rev-parse failed\n', ret.stderr)
        time = ret.stdout.decode("utf-8").strip()
        time = time.replace('+0000', 'UTC')

    out = '// This file is automatically generated.\n'
    out += '// Do not manually resolve conflicts! Contact ' \
        'hypervisor.team for assistance.\n'
    out += '#define HYP_GIT_VERSION {:s}\n'.format(id)
    out += '#define HYP_BUILD_DATE \"{:s}\"\n'.format(time)

    if options.output:
        with GenFile(options.output, 'w', encoding='utf-8') as f:
            f.write(out)
    else:
        sys.stdout.write(out)


if __name__ == '__main__':
    sys.exit(main())

```

`tools/build/gen_ver.sh`:

```sh
#!/bin/sh
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

status=`git diff HEAD --quiet || echo '-dirty'`

echo '// This file is automatically generated.'
echo '// Do not manually resolve conflicts! Contact hypervisor.team for assistance.'
echo "#define HYP_GIT_VERSION `git rev-parse --short HEAD`$status"

if [ -z "$status" ]
then
	echo "#define HYP_BUILD_DATE \"`TZ=UTC git show -s --pretty="%cd" --date=local HEAD` UTC\""
else
	echo "#define HYP_BUILD_DATE \"`date -R`\""
fi

```

`tools/codegen/codegen.py`:

```py
#!/usr/bin/env python3
#
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

# Simple generic code generator. Assumes that all logic is in the template
# itself and is based only on the architecture names and the preprocessor
# defines, all of which are passed on the command line.
#
# Note that it is generally bad style to have any non-trivial logic in the
# templates. Templates should import Python modules for anything complex.
# The directory containing the template file is automatically added to the
# Python path for this purpose.

from Cheetah.Template import Template

import argparse
import subprocess
import logging
import sys
import inspect
import os
import re

logger = logging.getLogger(__name__)


class DefineAction(argparse.Action):
    def __call__(self, parser, namespace, values, option_string=None):
        defs = getattr(namespace, self.dest, None)
        if defs is None:
            defs = {}
            setattr(namespace, self.dest, defs)
        try:
            name, val = values.split('=')
            try:
                val = int(val.rstrip('uU'), 0)
            except TypeError:
                pass
        except ValueError:
            name = values
            val = True
        defs[name] = val


def main():
    logging.basicConfig(
        level=logging.INFO,
        format="%(message)s",
    )

    args = argparse.ArgumentParser()

    args.add_argument("-o", "--output", help="Output file (default: stdout)",
                      default=sys.stdout,
                      type=argparse.FileType('w', encoding='utf-8'))
    args.add_argument("-D", dest='defines', help="Define config variable",
                      action=DefineAction)
    args.add_argument("-imacros",
                      type=argparse.FileType('r', encoding='utf-8'),
                      help="parse imacros CPP file",
                      default=None)
    args.add_argument("-a", dest='archs', help="Define architecture name",
                      action='append')
    args.add_argument("-f", "--formatter",
                      help="specify clang-format to format the code")
    args.add_argument('-d', "--deps",
                      type=argparse.FileType('w', encoding='utf-8'),
                      help="Write implicit dependencies to Makefile",
                      default=None)
    args.add_argument("template", metavar="TEMPLATE",
                      help="Template file used to generate output",
                      type=argparse.FileType('r', encoding='utf-8'))

    options = args.parse_args()

    defines = {}

    if options.defines:
        defines.update(options.defines)

    if options.imacros:
        d = re.compile(r'#define (?P<def>\w+)(\s+\"?(?P<val>[\w0-9,\ ]+)\"?)?')
        for line in options.imacros.readlines():
            match = d.search(line)
            define = match.group('def')
            val = match.group('val')
            try:
                try:
                    val = int(val.rstrip('uU'), 0)
                except TypeError:
                    pass
                except AttributeError:
                    pass
            except ValueError:
                pass

            if define in defines:
                raise Exception("multiply defined: {}\n", define)

            defines[define] = val

    sys.path.append(os.path.dirname(options.template.name))
    output = str(Template(file=options.template,
                          searchList=(defines,
                                      {'arch_list': options.archs})))

    if options.formatter:
        ret = subprocess.run([options.formatter], input=output.encode("utf-8"),
                             stdout=subprocess.PIPE)
        output = ret.stdout.decode("utf-8")
        if ret.returncode != 0:
            raise Exception("failed to format output:\n", ret.stderr)

    if options.deps is not None:
        deps = set()
        for m in sys.modules.values():
            try:
                f = inspect.getsourcefile(m)
            except TypeError:
                continue
            if f is None:
                continue
            f = os.path.relpath(f)
            if f.startswith('../'):
                continue
            deps.add(f)
        deps.add(options.template.name)
        if options.imacros:
            deps.add(options.imacros.name)

        options.deps.write(options.output.name + ' : ')
        options.deps.write(' '.join(sorted(deps)))
        options.deps.write('\n')
        options.deps.close()

    options.output.write(output)
    options.output.close()


if __name__ == '__main__':
    sys.exit(main())

```

`tools/cpptest/Checkers_Man_All_Req.properties`:

```properties
CERT_C-ENV33-a=true
CERT_C-STR02-a=true
CERT_C-STR02-b=true
CERT_C-STR02-c=true
CERT_C-STR07-a=true
com.parasoft.xtest.checkers.api.common.fullBuild=false
com.parasoft.xtest.checkers.api.common.incrementalBuild=false
com.parasoft.xtest.checkers.api.config.name=MISRA C 2012
com.parasoft.xtest.checkers.api.config.path=Compliance Packs/Automotive Pack
com.parasoft.xtest.checkers.api.config.tool=3
com.parasoft.xtest.checkers.api.config.version=9.5.0
com.parasoft.xtest.checkers.api.config.xtestVersion=1.0.1
com.parasoft.xtest.execution.api.enabled=false
#com.parasoft.xtest.standards.api.cap_errors_per_rule=false
com.parasoft.xtest.standards.api.enabled=true
com.parasoft.xtest.standards.api.max_errors_per_rule=10000
com.parasoft.xtest.testgen.api.enabled=false
#Compliance Standard Misra C 2012
MISRAC2012-DIR_4_10-a=true
MISRAC2012-DIR_4_11-a=true
MISRAC2012-DIR_4_12-a=false
MISRAC2012-DIR_4_13-a=true
MISRAC2012-DIR_4_13-b=true
MISRAC2012-DIR_4_13-c=true
MISRAC2012-DIR_4_13-d=true
MISRAC2012-DIR_4_13-e=true
MISRAC2012-DIR_4_14-a=true
MISRAC2012-DIR_4_14-b=true
MISRAC2012-DIR_4_14-c=true
MISRAC2012-DIR_4_14-d=true
MISRAC2012-DIR_4_14-e=true
MISRAC2012-DIR_4_14-f=true
MISRAC2012-DIR_4_14-g=true
MISRAC2012-DIR_4_14-h=true
MISRAC2012-DIR_4_14-i=true
MISRAC2012-DIR_4_14-j=true
MISRAC2012-DIR_4_14-k=true
MISRAC2012-DIR_4_14-l=true
MISRAC2012-DIR_4_15-a=true
MISRAC2012-DIR_4_1-a=true
MISRAC2012-DIR_4_1-b=true
MISRAC2012-DIR_4_1-c=true
MISRAC2012-DIR_4_1-d=true
MISRAC2012-DIR_4_1-e=true
MISRAC2012-DIR_4_1-f-arbitariesInInternalLinkage=false
MISRAC2012-DIR_4_1-f=true
MISRAC2012-DIR_4_1-f-visibilityThresholdEnabled=true
MISRAC2012-DIR_4_1-f-visibilityThreshold=PRIVATE
MISRAC2012-DIR_4_1-g=true
MISRAC2012-DIR_4_1-h=true
MISRAC2012-DIR_4_1-i=true
MISRAC2012-DIR_4_1-j=true
MISRAC2012-DIR_4_1-k=true
MISRAC2012-DIR_4_1-l=true
MISRAC2012-DIR_4_1-m=true
MISRAC2012-DIR_4_2-a=true
MISRAC2012-DIR_4_3-a=true
MISRAC2012-DIR_4_4-a=true
MISRAC2012-DIR_4_5-a=true
MISRAC2012-DIR_4_6-a=true
MISRAC2012-DIR_4_6-b=true
MISRAC2012-DIR_4_6-c=true
MISRAC2012-DIR_4_7-a=true
MISRAC2012-DIR_4_7-b=true
MISRAC2012-DIR_4_8-a=false
MISRAC2012-DIR_4_9-a=false
MISRAC2012-DIR_5_1-a=true
MISRAC2012-DIR_5_1-b=true
MISRAC2012-DIR_5_1-c=true
MISRAC2012-DIR_5_2-a=true
MISRAC2012-DIR_5_3-a=true
MISRAC2012-RULE_10_1-a=true
MISRAC2012-RULE_10_1-b=true
MISRAC2012-RULE_10_1-c=true
MISRAC2012-RULE_10_1-d=true
MISRAC2012-RULE_10_1-e=true
MISRAC2012-RULE_10_1-f=true
MISRAC2012-RULE_10_1-g=true
MISRAC2012-RULE_10_1-h=true
MISRAC2012-RULE_10_2-a=true
MISRAC2012-RULE_10_3-a=true
MISRAC2012-RULE_10_3-b=true
MISRAC2012-RULE_10_4-a=true
MISRAC2012-RULE_10_4-b=true
MISRAC2012-RULE_10_5-a=true
MISRAC2012-RULE_10_5-b=true
MISRAC2012-RULE_10_5-c=true
MISRAC2012-RULE_10_6-a=true
MISRAC2012-RULE_10_7-a=true
MISRAC2012-RULE_10_7-b=true
MISRAC2012-RULE_10_8-a=true
MISRAC2012-RULE_11_10-a=true
MISRAC2012-RULE_11_1-a=true
MISRAC2012-RULE_11_1-b=true
MISRAC2012-RULE_11_2-a=true
MISRAC2012-RULE_11_3-a=true
MISRAC2012-RULE_11_4-a=true
MISRAC2012-RULE_11_5-a=true
MISRAC2012-RULE_11_6-a=true
MISRAC2012-RULE_11_7-a=true
MISRAC2012-RULE_11_8-a=true
MISRAC2012-RULE_11_9-a=true
MISRAC2012-RULE_11_9-b=true
MISRAC2012-RULE_1_1-a=false
MISRAC2012-RULE_1_1-b=false
MISRAC2012-RULE_1_1-c=false
MISRAC2012-RULE_1_1-d=true
MISRAC2012-RULE_12_1-a=true
MISRAC2012-RULE_12_1-b=false
MISRAC2012-RULE_12_1-c=true
MISRAC2012-RULE_12_2-a=true
MISRAC2012-RULE_12_3-a=false
MISRAC2012-RULE_12_4-a=true
MISRAC2012-RULE_12_4-b=true
MISRAC2012-RULE_12_5-a=true
MISRAC2012-RULE_12_6-a=true
MISRAC2012-RULE_13_1-a=true
MISRAC2012-RULE_13_2-a=true
MISRAC2012-RULE_13_2-b=true
MISRAC2012-RULE_13_2-c=true
MISRAC2012-RULE_13_2-d=true
MISRAC2012-RULE_13_2-e=true
MISRAC2012-RULE_13_2-f=true
MISRAC2012-RULE_13_2-g=true
MISRAC2012-RULE_13_2-h=true
MISRAC2012-RULE_13_3-a=true
MISRAC2012-RULE_13_4-a=true
MISRAC2012-RULE_13_5-a=true
MISRAC2012-RULE_13_6-a=true
MISRAC2012-RULE_13_6-b=true
MISRAC2012-RULE_13_6-c=true
MISRAC2012-RULE_1_3-a=true
MISRAC2012-RULE_1_3-b=true
MISRAC2012-RULE_1_3-c=true
MISRAC2012-RULE_1_3-d=true
MISRAC2012-RULE_1_3-e=true
MISRAC2012-RULE_1_3-f=true
MISRAC2012-RULE_1_3-g=true
MISRAC2012-RULE_1_3-h=true
MISRAC2012-RULE_1_3-i=true
MISRAC2012-RULE_1_3-j=true
MISRAC2012-RULE_1_3-k=true
MISRAC2012-RULE_1_3-l=true
MISRAC2012-RULE_1_3-m=true
MISRAC2012-RULE_1_3-n=true
MISRAC2012-RULE_1_3-o=true
MISRAC2012-RULE_1_5-a=true
MISRAC2012-RULE_1_5-b=true
MISRAC2012-RULE_1_5-c=true
MISRAC2012-RULE_1_5-d=true
MISRAC2012-RULE_1_5-e=true
MISRAC2012-RULE_1_5-f=true
MISRAC2012-RULE_1_5-g=true
MISRAC2012-RULE_14_1-a=true
MISRAC2012-RULE_14_1-b=true
MISRAC2012-RULE_14_2-a=true
MISRAC2012-RULE_14_2-b=true
MISRAC2012-RULE_14_2-c=true
MISRAC2012-RULE_14_2-d=true
MISRAC2012-RULE_14_3-aa=true
MISRAC2012-RULE_14_3-ab=true
MISRAC2012-RULE_14_3-ac=true
MISRAC2012-RULE_14_3-ad=true
MISRAC2012-RULE_14_3-a=true
MISRAC2012-RULE_14_3-b=true
MISRAC2012-RULE_14_3-c=true
MISRAC2012-RULE_14_3-d=true
MISRAC2012-RULE_14_3-e=true
MISRAC2012-RULE_14_3-f=true
MISRAC2012-RULE_14_3-g=true
MISRAC2012-RULE_14_3-h=true
MISRAC2012-RULE_14_3-i=true
MISRAC2012-RULE_14_3-j=true
MISRAC2012-RULE_14_3-k=true
MISRAC2012-RULE_14_3-l=true
MISRAC2012-RULE_14_3-m=true
MISRAC2012-RULE_14_3-n=true
MISRAC2012-RULE_14_3-o=true
MISRAC2012-RULE_14_3-p=true
MISRAC2012-RULE_14_3-q=true
MISRAC2012-RULE_14_3-r=true
MISRAC2012-RULE_14_3-s=true
MISRAC2012-RULE_14_3-t=true
MISRAC2012-RULE_14_3-u=true
MISRAC2012-RULE_14_3-v=true
MISRAC2012-RULE_14_3-w=true
MISRAC2012-RULE_14_3-x=true
MISRAC2012-RULE_14_3-y=true
MISRAC2012-RULE_14_3-z=true
MISRAC2012-RULE_14_4-a=true
MISRAC2012-RULE_15_1-a=false
MISRAC2012-RULE_15_2-a=true
MISRAC2012-RULE_15_3-a=true
MISRAC2012-RULE_15_4-a=false
MISRAC2012-RULE_15_5-a=true
MISRAC2012-RULE_15_6-a=true
MISRAC2012-RULE_15_6-b=true
MISRAC2012-RULE_15_7-a=true
MISRAC2012-RULE_16_1-a=true
MISRAC2012-RULE_16_1-b=true
MISRAC2012-RULE_16_1-c=true
MISRAC2012-RULE_16_1-d=true
MISRAC2012-RULE_16_1-e=true
MISRAC2012-RULE_16_1-f=true
MISRAC2012-RULE_16_1-g=true
MISRAC2012-RULE_16_1-h=true
MISRAC2012-RULE_16_2-a=true
MISRAC2012-RULE_16_3-a=true
MISRAC2012-RULE_16_3-b=true
MISRAC2012-RULE_16_4-a=true
MISRAC2012-RULE_16_4-b=true
MISRAC2012-RULE_16_5-a=true
MISRAC2012-RULE_16_6-a=true
MISRAC2012-RULE_16_7-a=true
MISRAC2012-RULE_16_7-b=true
MISRAC2012-RULE_17_10-a=true
MISRAC2012-RULE_17_11-a=true
MISRAC2012-RULE_17_12-a=true
MISRAC2012-RULE_17_13-a=true
MISRAC2012-RULE_17_1-a=true
MISRAC2012-RULE_17_1-b=true
MISRAC2012-RULE_17_2-a=true
MISRAC2012-RULE_17_3-a=true
MISRAC2012-RULE_17_4-a=true
MISRAC2012-RULE_17_5-a=true
MISRAC2012-RULE_17_6-a=true
MISRAC2012-RULE_17_7-a=true
MISRAC2012-RULE_17_7-b=true
MISRAC2012-RULE_17_8-a=true
MISRAC2012-RULE_17_9-a=true
MISRAC2012-RULE_18_10-a=true
MISRAC2012-RULE_18_1-a=true
MISRAC2012-RULE_18_1-b=true
MISRAC2012-RULE_18_1-c=true
MISRAC2012-RULE_18_2-a=true
MISRAC2012-RULE_18_3-a=true
MISRAC2012-RULE_18_4-a=false
MISRAC2012-RULE_18_5-a=true
MISRAC2012-RULE_18_6-a=true
MISRAC2012-RULE_18_6-b=true
MISRAC2012-RULE_18_6-c=true
MISRAC2012-RULE_18_7-a=true
MISRAC2012-RULE_18_8-a=true
MISRAC2012-RULE_18_9-a=true
MISRAC2012-RULE_19_1-a=true
MISRAC2012-RULE_19_1-b=true
MISRAC2012-RULE_19_1-c=true
MISRAC2012-RULE_19_2-a=false
MISRAC2012-RULE_20_10-a=false
MISRAC2012-RULE_20_11-a=true
MISRAC2012-RULE_20_12-a=true
MISRAC2012-RULE_20_13-a=true
MISRAC2012-RULE_20_14-a=true
MISRAC2012-RULE_20_1-a=true
MISRAC2012-RULE_20_2-a=true
MISRAC2012-RULE_20_2-b=true
MISRAC2012-RULE_20_3-a=true
MISRAC2012-RULE_20_4-a=true
MISRAC2012-RULE_20_4-b=true
MISRAC2012-RULE_20_5-a=true
MISRAC2012-RULE_20_6-a=true
MISRAC2012-RULE_20_7-a=true
MISRAC2012-RULE_20_8-a=true
MISRAC2012-RULE_20_9-a=true
MISRAC2012-RULE_20_9-b=true
MISRAC2012-RULE_21_10-a=true
MISRAC2012-RULE_21_11-a=true
MISRAC2012-RULE_21_12-a=true
MISRAC2012-RULE_21_13-a=true
MISRAC2012-RULE_21_14-a=true
MISRAC2012-RULE_21_15-a=true
MISRAC2012-RULE_21_16-a=true
MISRAC2012-RULE_21_17-a=true
MISRAC2012-RULE_21_17-b=true
MISRAC2012-RULE_21_18-a=true
MISRAC2012-RULE_21_19-a=true
MISRAC2012-RULE_21_19-b=true
MISRAC2012-RULE_21_1-a=true
MISRAC2012-RULE_21_1-b=false
MISRAC2012-RULE_21_1-c=true
MISRAC2012-RULE_21_1-d=true
MISRAC2012-RULE_21_20-a=true
MISRAC2012-RULE_21_21-a=true
MISRAC2012-RULE_21_22-a=true
MISRAC2012-RULE_21_23-a=true
MISRAC2012-RULE_21_24-a=true
MISRAC2012-RULE_21_25-a=true
MISRAC2012-RULE_21_26-a=true
MISRAC2012-RULE_21_2-a=true
MISRAC2012-RULE_21_2-b=false
MISRAC2012-RULE_21_2-c=true
MISRAC2012-RULE_21_3-a=false
MISRAC2012-RULE_21_4-a=true
MISRAC2012-RULE_21_4-b=true
MISRAC2012-RULE_21_5-a=true
MISRAC2012-RULE_21_5-b=true
MISRAC2012-RULE_21_6-a=true
MISRAC2012-RULE_21_7-a=true
MISRAC2012-RULE_21_8-a=true
MISRAC2012-RULE_21_9-a=true
MISRAC2012-RULE_2_1-a=true
MISRAC2012-RULE_2_1-b=true
MISRAC2012-RULE_2_1-c=true
MISRAC2012-RULE_2_1-d=true
MISRAC2012-RULE_2_1-e=true
MISRAC2012-RULE_2_1-f=true
MISRAC2012-RULE_2_1-g=true
MISRAC2012-RULE-22_10-a-reportOnMissingErrnoCheck=false
MISRAC2012-RULE_22_10-a-reportWhenErrnoIsNotZero=false
MISRAC2012-RULE_22_10-a=true
MISRAC2012-RULE_22_11-a=true
MISRAC2012-RULE_22_12-a=true
MISRAC2012-RULE_22_13-a=true
MISRAC2012-RULE_22_14-a=true
MISRAC2012-RULE_22_14-b=true
MISRAC2012-RULE_22_15-a=true
MISRAC2012-RULE_22_16-a=true
MISRAC2012-RULE_22_17-a=true
MISRAC2012-RULE_22_18-a=true
MISRAC2012-RULE_22_19-a=true
MISRAC2012-RULE_22_1-a-fieldsStoreResources=false
MISRAC2012-RULE_22_1-a-nonMemberMethodsStoreResource=false
MISRAC2012-RULE_22_1-a-patternName=^malloc|calloc|realloc|fopen$
MISRAC2012-RULE_22_1-a-patternNameMethodsStore=true
MISRAC2012-RULE_22_1-a-reportUnvalidatedViolations=false
MISRAC2012-RULE_22_1-a-storeByTPMethods=false
MISRAC2012-RULE_22_1-a=true
MISRAC2012-RULE_22_20-a=true
MISRAC2012-RULE_22_2-a=true
MISRAC2012-RULE_22_2-b=true
MISRAC2012-RULE_22_3-a=true
MISRAC2012-RULE_22_4-a=true
MISRAC2012-RULE_22_5-a=true
MISRAC2012-RULE_22_5-b=true
MISRAC2012-RULE_22_6-a=true
MISRAC2012-RULE_22_7-a=true
MISRAC2012-RULE_22_8-a-reportOnMissingErrnoCheck=false
MISRAC2012-RULE_22_8-a-reportOnUnnecessaryErrnoCheck=false
MISRAC2012-RULE_22_8-a=true
MISRAC2012-RULE_22_9-a-reportOnUnnecessaryErrnoCheck=false
MISRAC2012-RULE_22_9-a-reportWhenErrnoIsNotZero=false
MISRAC2012-RULE_22_9-a=true
MISRAC2012-RULE_23_1-a=true
MISRAC2012-RULE_23_1-b=true
MISRAC2012-RULE_23_2-a=true
MISRAC2012-RULE_23_3-a=true
MISRAC2012-RULE_23_4-a=true
MISRAC2012-RULE_23_5-a=true
MISRAC2012-RULE_23_6-a=true
MISRAC2012-RULE_23_6-b=true
MISRAC2012-RULE_23_7-a=true
MISRAC2012-RULE_23_8-a=true
MISRAC2012-RULE_2_2-a=true
MISRAC2012-RULE_2_3-a=false
MISRAC2012-RULE_2_3-b=false
MISRAC2012-RULE_2_4-a=false
MISRAC2012-RULE_2_4-b=false
MISRAC2012-RULE_2_5-a=false
MISRAC2012-RULE_2_6-a=false
MISRAC2012-RULE_2_7-a=false
MISRAC2012-RULE_2_8-a=true
MISRAC2012-RULE_2_8-b=true
MISRAC2012-RULE_2_8-c=true
MISRAC2012-RULE_3_1-a=true
MISRAC2012-RULE_3_1-b=true
MISRAC2012-RULE_3_1-c=true
MISRAC2012-RULE_3_2-a=true
MISRAC2012-RULE_4_1-a=true
MISRAC2012-RULE_4_2-a=true
MISRAC2012-RULE_5_1-a=false
MISRAC2012-RULE_5_2-a=false
MISRAC2012-RULE_5_2-b=true
MISRAC2012-RULE_5_2-c=false
MISRAC2012-RULE_5_2-d=true
MISRAC2012-RULE_5_3-a=true
MISRAC2012-RULE_5_3-b=true
MISRAC2012-RULE_5_4-a=false
MISRAC2012-RULE_5_4-b=true
MISRAC2012-RULE_5_4-c=false
MISRAC2012-RULE_5_4-d=true
MISRAC2012-RULE_5_5-a=false
MISRAC2012-RULE_5_5-b=true
MISRAC2012-RULE_5_6-a=true
MISRAC2012-RULE_5_6-b=true
MISRAC2012-RULE_5_7-a=true
MISRAC2012-RULE_5_7-b=true
MISRAC2012-RULE_5_8-a=true
MISRAC2012-RULE_5_8-b=true
MISRAC2012-RULE_5_9-a=true
MISRAC2012-RULE_5_9-b=true
MISRAC2012-RULE_6_1-a=true
MISRAC2012-RULE_6_2-a=true
MISRAC2012-RULE_6_3-a=true
MISRAC2012-RULE_7_1-a=true
MISRAC2012-RULE_7_2-a=true
MISRAC2012-RULE_7_3-a=true
MISRAC2012-RULE_7_4-a=true
MISRAC2012-RULE_7_5-a=true
MISRAC2012-RULE_7_6-a=true
MISRAC2012-RULE_8_10-a=true
MISRAC2012-RULE_8_11-a=true
MISRAC2012-RULE_8_12-a=true
MISRAC2012-RULE_8_13-a=true
MISRAC2012-RULE_8_13-b=true
MISRAC2012-RULE_8_14-a=true
MISRAC2012-RULE_8_15-a=true
MISRAC2012-RULE_8_15-b=true
MISRAC2012-RULE_8_16-a=true
MISRAC2012-RULE_8_17-a=true
MISRAC2012-RULE_8_1-a=true
MISRAC2012-RULE_8_1-b=true
MISRAC2012-RULE_8_2-a=true
MISRAC2012-RULE_8_2-b=true
MISRAC2012-RULE_8_2-c=true
MISRAC2012-RULE_8_3-a=true
MISRAC2012-RULE_8_3-b=true
MISRAC2012-RULE_8_4-a=true
MISRAC2012-RULE_8_4-b=true
MISRAC2012-RULE_8_5-a=true
MISRAC2012-RULE_8_6-a=false
MISRAC2012-RULE_8_7-a=true
MISRAC2012-RULE_8_8-a=true
MISRAC2012-RULE_8_9-a=true
MISRAC2012-RULE_9_1-a=true
MISRAC2012-RULE_9_2-a=true
MISRAC2012-RULE_9_3-a=true
MISRAC2012-RULE_9_4-a=true
MISRAC2012-RULE_9_5-a=true
MISRAC2012-RULE_9_6-a=true
MISRAC2012-RULE_9_7-a=true

```

`tools/cpptest/Cyclomatic.properties`:

```properties
#For more details on cyclomatic complexity see
#https://emenda.com/wp-content/uploads/2017/07/HIS-sc-metriken.1.3.1_e.pdf

#Comment Density >=20%
#Comment Density "COMF"
METRIC.CLLOCRIM=false
#METRIC.CLLOCRIM.ThresholdEnabled=true
#METRIC.CLLOCRIM.Threshold=g .2


#Cyclomatic Complexity <=10
#Cyclomatic Complexity "v(G)"
#METRIC.CC=true
#METRIC.CC.ThresholdEnabled=true
#METRIC.CC.Threshold=l 21

#number of parameters <=5
#Number of function parameters "PARAM"
METRIC.NOPAR=false
#METRIC.NOPAR.ThresholdEnabled=true
#METRIC.NOPAR.Threshold=l 6


#of instructions <=50
#Number of Instructions per function "STMT"
METRIC.NOLLOCIF=false
#METRIC.NOLLOCIF.ThresholdEnabled=true
#METRIC.NOLLOCIF.Threshold=l 51

#of call levels <=4
#Number of call Levels "LEVEL"
METRIC.NBD=false
#METRIC.NBD.ThresholdEnabled=true
#METRIC.NBD.Threshold=l 5

METRIC.DIF=false
#METRIC.DIF.ThresholdEnabled=true
#METRIC.DIF.Threshold=l 5


#Number of return Points "RETURN"
#Error on != 1
METRIC.NORET=false
#METRIC.NORET.ThresholdEnabled=true
#METRIC.NORET.Threshold=l 2


#All the other metrics that do not have thresholds
METRIC.NOF=false
METRIC.NOC=false
METRIC.NOPLIF=false
METRIC.NOPLIT=false
METRIC.NOPLIM=false
METRIC.NOSLIF=false
METRIC.NOSLIT=false
METRIC.NOSLIM=false
METRIC.NOLLOCIT=false
METRIC.NOLLOCIM=false
METRIC.NOCLIF=false
METRIC.NOCLIT=false
METRIC.NOCLIM=false
METRIC.NOBLIF=false
METRIC.NOBLIT=false
METRIC.NOBLIM=false
METRIC.CLLOCRIT=false
METRIC.CLLOCRIF=false
METRIC.SCC=false
METRIC.MCC=true
METRIC.MCC.ThresholdEnabled=true
METRIC.MCC.Threshold=l 15

METRIC.ECC=false
METRIC.NOT=false
METRIC.NOMIT=false
METRIC.NOPUBMIT=false
METRIC.NOPROTMIT=false
METRIC.NOPRIVMIT=false
METRIC.IDOC=false
METRIC.CBO=false
METRIC.LCOM=false
METRIC.FO=false
METRIC.MI=false
METRIC.RFC=false
METRIC.WMC=false
METRIC.HDIFM=false
METRIC.HEFM=false
METRIC.HICM=false
METRIC.HLENM=false
METRIC.HLEVM=false
METRIC.HNOBM=false
METRIC.HTTPM=false
METRIC.HVOCM=false
METRIC.HVOLM=false


cpptest.analyzer.metrics.enabled=true
# Test configuration metadata
com.parasoft.xtest.checkers.api.config.name=Modified Cyclomatic
com.parasoft.xtest.checkers.api.config.path=Static Analysis
com.parasoft.xtest.checkers.api.config.tool=3
com.parasoft.xtest.checkers.api.config.version=9.5.0
com.parasoft.xtest.checkers.api.config.xtestVersion=1.0.1
com.parasoft.xtest.checkers.api.common.fullBuild=false
com.parasoft.xtest.checkers.api.common.incrementalBuild=false
com.parasoft.xtest.standards.api.enabled=true
com.parasoft.xtest.execution.api.enabled=false
com.parasoft.xtest.testgen.api.enabled=false
com.parasoft.xtest.standards.api.max_errors_per_rule=10000

```

`tools/cpptest/custom.psrc`:

```psrc
codewizard.preprocessorExtraOption -DPARASOFT_CYCLO

```

`tools/cpptest/cyclomatic_xml_to_json.py`:

```py
#!/usr/bin/env python3
# coding: utf-8
#
# © 2023 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

"""
Run as a part of gitlab CI, after Parasoft reports have been generated.

This script converts the Parasoft XML-format report to a Code Climate
compatible json file, that gitlab code quality can interpret.
"""

import xml.etree.ElementTree as ET
import json
import argparse
import sys
import os

argparser = argparse.ArgumentParser(
    description="Convert Parasoft XML to Code Climate JSON")
argparser.add_argument('input', type=argparse.FileType('r'), nargs='?',
                       default=sys.stdin, help="the Parasoft XML input")
argparser.add_argument('--output', '-o', type=argparse.FileType('w'),
                       default=sys.stdout, help="the Code Climate JSON output")
args = argparser.parse_args()

tree = ET.parse(args.input)

parasoft_viols = tree.findall(".//MetViol")

cc_viols = []

severity_map = {
    1: "blocker",
    2: "critical",
    3: "major",
    4: "minor",
    5: "info",
}

# info warning between 15-20
start__threshold = 15
end__threshold = 20


def cc_info_warning(msg, sev):
    warning = int(msg.split(" ")[1])
    if (warning >= start__threshold) and (warning <= end__threshold):
        return 5
    else:
        return sev


cc_viols = [
    ({
        "type": "issue",
        "categories": ["Bug Risk"],
        "severity": (severity_map[cc_info_warning(v.attrib['msg'],
                                                  int(v.attrib['sev']))]),
        "check_name": v.attrib['rule'],
        "description": (v.attrib['msg'] + '. ' +
                        v.attrib['rule.header'] + '. (' +
                        v.attrib['rule'] + ')'),
        "fingerprint": v.attrib['unbViolId'],
        "location": {
            "path": v.attrib['locFile'].split(os.sep, 2)[2],
            "lines": {
                "begin": int(v.attrib['locStartln']),
                "end": int(v.attrib['locEndLn'])
            }
        }
    })
    for v in parasoft_viols]

args.output.write(json.dumps(cc_viols))
args.output.close()

```

`tools/cpptest/gunyahkw.h`:

```h
#ifndef __KW_MODERN_ENGINE__
int
__builtin_ffsll(unsigned long long x);
#kw_override compiler_ffs(x) __builtin_ffsll(x)

/* FPs caused by KW not understanding __builtin_expect(), overrides suggested by
 * TomZ '22 */
#kw_override compiler_expected(x)(x)
#kw_override compiler_unexpected(x)(x)

#kw_override HYP_LOG_FATAL(xx_fmt, ...)(abort())
#endif

```

`tools/cpptest/gunyahkw.kb`:

```kb
partition_alloc - ALLOC hyp:$1, $2 GT(0):$$.r:$$.e NE(0)
partition_alloc - BPS charlength($$.r)=infinity
partition_alloc - BPS bytesize($$.r)=$1
partition_alloc - NPD.SRC env : $$.r : $$.e NE(0)
partition_alloc - UNINIT.HEAP 1 : *($$.r) : 1
partition_alloc - UnsafeAllocSizeAccepter $2
partition_free - FREE hyp $2

```

`tools/cpptest/klocwork_xml_to_json.py`:

```py
#!/usr/bin/env python3
# coding: utf-8
#
# © 2023 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

"""
Run as a part of gitlab CI, after Klocwork reports have been generated.

This script converts the Klocwork JSON-format report to a Code Climate
compatible json file, that gitlab code quality can interpret.
"""

import json
import argparse
import sys
import os
import hashlib

argparser = argparse.ArgumentParser(
    description="Convert Klocwork JSON to Code Climate JSON")
argparser.add_argument('input', type=argparse.FileType('r'), nargs='?',
                       default=sys.stdin, help="the Klocwork JSON input")
argparser.add_argument('--output', '-o', type=argparse.FileType('w'),
                       default=sys.stdout, help="the Code Climate JSON output")
args = argparser.parse_args()

json_input_file = ""

kw_violations = []
issue = {}

severity_map = {
    1: "blocker",
    2: "critical",
    3: "major",
    4: "minor",
    5: "info",
}


def get_severity(v):
    if (v > 5):
        return severity_map[5]
    else:
        return severity_map[v]


with open(args.input.name, 'r') as json_file:
    json_input_file = json.load(json_file)

for v in range(len(json_input_file)):
    # print(json_input_file[v])
    issue["type"] = "issue"
    issue["categories"] = ["Bug Risk"]
    issue["severity"] = get_severity(
        int(json_input_file[v]['severityCode']))
    issue["check_name"] = json_input_file[v]['code']
    issue["description"] = json_input_file[v]['message'] + '. ' + \
        json_input_file[v]['severity'] + \
        '. (' + json_input_file[v]['code'] + ')'
    issue["location"] = {}
    issue["location"]["path"] = os.path.relpath(json_input_file[v]['file'])
    issue["location"]["lines"] = {}
    issue["location"]["lines"]["begin"] = int(json_input_file[v]['line'])
    issue["location"]["lines"]["end"] = int(json_input_file[v]['line'])
    dump_issue = json.dumps(issue)
    issue["fingerprint"] = hashlib.md5(dump_issue.encode('utf-8')).hexdigest()
    # print(issue)
    kw_violations.append(issue)
    issue = {}  # was getting wired result without clearing it
args.output.write(json.dumps(kw_violations))

```

`tools/cpptest/misra_xml_to_json.py`:

```py
#!/usr/bin/env python3
# coding: utf-8
#
# © 2023 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

"""
Run as a part of gitlab CI, after Parasoft reports have been generated.

This script converts the Parasoft XML-format report to a Code Climate
compatible json file, that gitlab code quality can interpret.
"""

import xml.etree.ElementTree as ET
import json
import argparse
import sys
import os
import re

argparser = argparse.ArgumentParser(
    description="Convert Parasoft XML to Code Climate JSON")
argparser.add_argument('input', type=argparse.FileType('r'), nargs='?',
                       default=sys.stdin, help="the Parasoft XML input")
argparser.add_argument('--output', '-o', type=argparse.FileType('w'),
                       default=sys.stdout, help="the Code Climate JSON output")
args = argparser.parse_args()

tree = ET.parse(args.input)

parasoft_viols = tree.findall(".//StdViol") + tree.findall(".//FlowViol")

cc_viols = []

severity_map = {
    1: "blocker",
    2: "critical",
    3: "major",
    4: "minor",
    5: "info",
}

deviation_map = {
    # Deviation because the behaviour proscribed by the rule is exactly the
    # intended behaviour of assert(): it prints the unexpanded expression.
    'MISRAC2012-RULE_20_12-a': [
        (None, re.compile(r"parameter of potential macro 'assert'")),
    ],
    # False positives due to __c11 builtins taking int memory order arguments
    # instead of enum in the Clang implementation.
    'MISRAC2012-RULE_10_3-b': [
        (None, re.compile(r"number '2'.*'essentially Enum'.*"
                          r"'__c11_atomic_load'.*'essentially signed'")),
        (None, re.compile(r"number '3'.*'essentially Enum'.*"
                          r"'__c11_atomic_(store'|exchange'|fetch_).*"
                          r"'essentially signed'")),
        (None, re.compile(r"number '[45]'.*'essentially Enum'.*"
                          r"'__c11_atomic_compare_exchange_(strong|weak)'.*"
                          r"'essentially signed'")),
    ],
    # False positives with unknown cause: the return value of assert_if_const()
    # is always used, to determine whether to call assert_failed()
    'MISRAC2012-RULE_17_7-b': [
        (None, re.compile(r'"assert_if_const"')),
    ],
    'MISRAC2012-RULE_8_7-a': [
        # The could-be-static advisory rule is impractical to enforce for
        # generated accessors, since the type system has no information about
        # which accessors are used.
        (re.compile(r'^build/.*/accessors\.c$'), None),
        # The smccc module specifically has events that are only triggered by
        # handlers for other events.
        (re.compile(r'^build/.*/events/src/smccc\.c$'), None),
        # The object module has type-specific APIs that are only used directly
        # for some specific object types, and otherwise are called only by the
        # type-generic APIs defined in the same file.
        (re.compile(r'^build/.*/objects/.*\.c$'), None),
    ],
    # Invariant expressions are expected and unavoidable in generated event
    # triggers because it is not possible to remove error result types from
    # handlers that never return errors.
    'MISRAC2012-RULE_14_3-ac': [
        (re.compile(r'^build/.*/events/src/.*\.c$'), None),
    ],
    # Could-be-const pointers are expected and unavoidable in generated event
    # triggers because the object may or may not be modified depending on the
    # handlers and the module configuration. The const qualifier is used to
    # specify whether the handlers are allowed to modify the objects, rather
    # than whether they actually do.
    'MISRAC2012-RULE_8_13-a': [
        (re.compile(r'^build/.*/events/src/.*\.c$'), None),
    ],
    # The generated type-generic object functions terminate non-empty default
    # clauses with a _Noreturn function, panic(), to indicate that the object
    # type is invalid. There is an approved deviation for this, and in any
    # case these rules are downgraded to advisory in generated code.
    'MISRAC2012-RULE_16_1-d': [
        (re.compile(r'^build/.*/objects/.*\.c$'), None),
    ],
    'MISRAC2012-RULE_16_3-b': [
        (re.compile(r'^build/.*/objects/.*\.c$'), None),
    ],
    # False positive due to a builtin sizeof variant that does not evaluate its
    # argument, so there is no uninitialised use.
    'MISRAC2012-RULE_9_1-a': [
        (None, re.compile(r'passed to "__builtin_object_size"')),
    ],
    'MISRAC2012-RULE_1_3-b': [
        (None, re.compile(r'passed to "__builtin_object_size"')),
    ],
    # Deviation because casting a pointer to _Atomic to a pointer that can't be
    # dereferenced at all (const void *) is reasonably safe, and is needed for
    # certain builtin functions where the compiler knows the real underlying
    # object type anyway (e.g. __builtin_object_size) or where the object type
    # does not matter (e.g. __builtin_prefetch).
    'MISRAC2012-RULE_11_8-a': [
        (None, re.compile(r"to the 'const void \*' type which removes the "
                          r"'_Atomic' qualifiers")),
    ],
    # Compliance with rule 21.25 would have a significant performance impact.
    # All existing uses have been thoroughly analysed and tested, so we will
    # seek a project-wide deviation for this rule.
    'MISRAC2012-RULE_21_25-a': [
        (None, None),
    ],
}


def matches_deviation(v):
    rule = v.attrib['rule']
    if rule not in deviation_map:
        return False

    msg = v.attrib['msg']
    path = v.attrib['locFile'].split(os.sep, 2)[2]

    def check_constraint(constraint, value):
        if constraint is None:
            return True
        try:
            return constraint.search(value)
        except AttributeError:
            return constraint == value

    for d_path, d_msg in deviation_map[rule]:
        if check_constraint(d_path, path) and check_constraint(d_msg, msg):
            return True

    return False


cc_viols = [
    ({
        "type": "issue",
        "categories": ["Bug Risk"],
        "severity": ('info' if matches_deviation(v)
                     else severity_map[int(v.attrib['sev'])]),
        "check_name": v.attrib['rule'],
        "description": (v.attrib['msg'] + '. ' +
                        v.attrib['rule.header'] + '. (' +
                        v.attrib['rule'] + ')'),
        "fingerprint": v.attrib['unbViolId'],
        "location": {
            "path": v.attrib['locFile'].split(os.sep, 2)[2],
            "lines": {
                "begin": int(v.attrib['locStartln']),
                "end": int(v.attrib['locEndLn'])
            }
        }
    })
    for v in parasoft_viols]

args.output.write(json.dumps(cc_viols))
args.output.close()

```

`tools/debug/tracebuf.py`:

```py
#!/usr/bin/env python3

# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

"""
Convert a trace buffer binary file to text form.
"""

import os
import struct
import argparse
import sys
import tempfile
import subprocess
import itertools
import math


MAP_ID = 0x14
UNMAP_ID = 0x15

TRACE_FORMAT = 1

TRACE_IDS = {
    0: "INFO",
    1: "WARN",
    2: "HYPERCALL",
    3: "DEBUG",
    10: "PANIC",
    11: "ASSERT_FAILED",
    32: "VGIC_VIRQ_CHANGED",
    33: "VGIC_DSTATE_CHANGED",
    34: "VGIC_HWSTATE_CHANGED",
    35: "VGIC_HWSTATE_UNCHANGED",
    36: "VGIC_GICD_WRITE",
    37: "VGIC_GICR_WRITE",
    38: "VGIC_SGI",
    39: "VGIC_ITS_COMMAND",
    40: "VGIC_ROUTE",
    41: "VGIC_ICC_WRITE",
    42: "VGIC_ASYNC_EVENT",
    48: "PSCI_PSTATE_VALIDATION",
    49: "PSCI_VPM_STATE_CHANGED",
    50: "PSCI_VPM_SYSTEM_SUSPEND",
    51: "PSCI_VPM_SYSTEM_RESUME",
    52: "PSCI_VPM_VCPU_SUSPEND",
    53: "PSCI_VPM_VCPU_RESUME",
    54: "PSCI_SYSTEM_SUSPEND",
    55: "PSCI_SYSTEM_RESUME",
    128: "WAIT_QUEUE_RESERVE",
    129: "WAIT_QUEUE_WAKE",
    130: "WAIT_QUEUE_WAKE_ACK",
    131: "WAIT_QUEUE_SLEEP",
    132: "WAIT_QUEUE_RESUME",
    133: "WAIT_QUEUE_FREE",
}


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("input", nargs='?', type=argparse.FileType('rb'),
                        default=sys.stdin.buffer, help="Trace binary file")
    image_args = parser.add_mutually_exclusive_group()
    image_args.add_argument("--elf", '-e', type=argparse.FileType('rb'),
                            help="ELF image")
    image_args.add_argument("--binary", '-b', type=argparse.FileType('rb'),
                            help="Binary hypervisor image")
    timestamp_args = parser.add_mutually_exclusive_group()
    timestamp_args.add_argument("--freq", '-f', type=int, default=19200000,
                                help="Timer frequency in Hz")
    timestamp_args.add_argument("--ticks", '-t', action="store_true",
                                help="Show time in ticks instead of seconds")
    parser.add_argument('-T', "--time-offset", default=0, type=float,
                        help="Offset to subtract from displayed timestamps"
                        " (in the same units as the timestamp)")
    format_args = parser.add_mutually_exclusive_group()
    format_args.add_argument('-s', "--sort", action="store_const",
                             dest='sort', const='s',
                             help="Sort entries by timestamp")
    format_args.add_argument('-r', "--raw", action="store_const",
                             dest='sort', const='u',
                             help="Entries as positioned in trace ring buffer")
    format_args.add_argument('-m', "--merge", action="store_const",
                             dest='sort', const='m',
                             help="Entries merged and sorted by timestamp")
    parser.add_argument("--show-missing", action="store_true",
                        help="Mark invalid or overwritten log entries")
    parser.add_argument('-o', "--output", default=sys.stdout,
                        type=argparse.FileType('w', encoding='utf-8'),
                        help="Output text file")
    parser.set_defaults(sort='s')
    args = parser.parse_args()

    global image
    image = ()
    if args.elf is not None:
        with tempfile.TemporaryDirectory() as tmpdir:
            binfile = os.path.join(tmpdir, 'hyp.bin')
            try:
                objcopy = os.path.join(os.environ['LLVM'], 'bin',
                                       'llvm-objcopy')
            except KeyError:
                try:
                    objcopy = os.path.join(os.environ['QCOM_LLVM'], 'bin',
                                           'llvm-objcopy')
                except KeyError:
                    print("Error environment var LLVM or QCOM_LLVM not set")
                    pass

            subprocess.check_call([objcopy, '-j', '.text',
                                   '-j', '.rodata', '-O', 'binary',
                                   args.elf.name, binfile])
            with open(binfile, 'rb') as binfile:
                image = binfile.read()
    elif args.binary is not None:
        image = args.binary.read()

    entry_iter = read_all_entries(args)
    log = prepare_log(args, entry_iter)
    print_log(args, log)


class Arg(int):

    __cache = {}

    def __new__(cls, value, strict=False):
        if value in Arg.__cache:
            return Arg.__cache[value]
        self = super().__new__(cls, value)
        self.__strict = strict
        Arg.__cache[value] = self
        self.__str = self.__gen_str()
        return self

    def __format__(self, format_spec):
        if format_spec.endswith('s'):
            return str(self).__format__(format_spec)
        return super().__format__(format_spec)

    def __gen_str(self):
        try:
            bs = bytearray()
            assert (self & 0x1fffff) < len(image)
            for i in range((self & 0x1fffff), len(image)):
                b = image[i]
                if b == 0:
                    break
                bs.append(b)
                if len(bs) > 512:
                    break
            return bs.decode('utf-8')
        except Exception:
            if self.__strict:
                raise
            return '<str:{:#x}>'.format(self)

    def __str__(self):
        return self.__str


class LogEntry:
    def __init__(self, ticks, cpu_id, string=''):
        self.ticks = ticks
        self.cpu_id = cpu_id
        self.__str = string

    def __str__(self):
        return self.__str

    def set_string(self, string):
        self.__str = string


class Event(LogEntry):

    __slots__ = ('ticks', 'trace_id', 'trace_ids', 'cpu_id', 'missing_before',
                 'missing_after', '__str')

    def __init__(self, args, info, tag, fmt_ptr, arg0, arg1, arg2, arg3, arg4):
        if info == 0:
            # Empty trace slot
            raise ValueError("empty slot")

        ticks = info & ((1 << 56) - 1)
        cpu_id = info >> 56

        super().__init__(ticks, cpu_id)

        self.trace_id = tag & 0xffff

        if TRACE_FORMAT == 1:
            self.trace_ids = (tag >> 16) & 0xffffffff
            vmid = self.trace_ids & 0xffff
            vcpu = (self.trace_ids >> 16) & 0xffff
            caller_id = '{:#04x}:{:#02d}'.format(vmid, vcpu)
        else:
            self.trace_ids = 0
            caller_id = ''

        if self.trace_id in TRACE_IDS:
            trace_id = TRACE_IDS[self.trace_id]
        else:
            trace_id = '{:#06x}'.format(self.trace_id)

        # Try to obtain a C string at the given offset
        try:
            fmt = str(Arg(fmt_ptr, strict=True))
        except Exception:
            fmt = "? fmt_ptr {:#x}".format(fmt_ptr) + \
                " args {:#x} {:#x} {:#x} {:#x} {:#x}"

        # Try to format the args using the given format string
        try:
            msg = fmt.format(Arg(arg0), Arg(arg1), Arg(arg2), Arg(arg3),
                             Arg(arg4))
        except Exception:
            msg = ("? fmt_str {:s} args {:#x} {:#x} {:#x} {:#x} {:#x}"
                   .format(fmt, arg0, arg1, arg2, arg3, arg4))

        if args.ticks:
            rel_time = int(self.ticks - args.time_offset)
            abs_time = int(self.ticks)
            if args.time_offset:
                ts_str = "[{:12d}/{:12d}]".format(rel_time, abs_time)
            else:
                ts_str = "[{:12d}]".format(abs_time)

        else:
            rel_time = (float(self.ticks) / args.freq) - args.time_offset
            abs_time = float(self.ticks) / args.freq
            if args.time_offset:
                ts_str = "[{:12.6f}/{:12.6f}]".format(rel_time, abs_time)
            else:
                ts_str = "[{:12.6f}]".format(abs_time)

        self.set_string("{:s} <{:d}> {:s} {:s} {:s}\n".format(
            ts_str, self.cpu_id, caller_id, trace_id, msg))

        self.missing_before = False
        self.missing_after = False


def read_entries(args):
    header = args.input.read(64)
    if not header or (len(header) < 64):
        # Reached end of file
        if header:
            print("<skipped trailing bytes>\n")
        raise StopIteration

    magic = struct.unpack('<L', header[:4])[0]
    if magic == 0x46554236:  # 6BUF
        endian = '<'
    elif magic == 0x36425568:  # FUB6
        endian = '>'
    else:
        print("Unexpected magic number {:#x}".format(magic))
        raise StopIteration

    cpu_mask = struct.unpack(endian + 'QQQQ', header[8:40])

    cpu_mask = cpu_mask[0] | (cpu_mask[1] << 64) | (cpu_mask[2] << 128) | \
        (cpu_mask[2] << 192)
    global_buffer = cpu_mask == 0

    cpus = ''
    while cpu_mask != 0:
        msb = cpu_mask.bit_length() - 1
        cpus += '{:d}'.format(msb)
        cpu_mask &= ~(1 << msb)
        if cpu_mask != 0:
            cpus += ','

    if global_buffer:
        print("Processing global buffer...")
    else:
        print("Processing CPU {:s} buffer...".format(cpus))

    entries_max = struct.unpack(endian + 'L', header[4:8])[0]
    head_index = struct.unpack(endian + 'L', header[40:44])[0]

    # Check if this buffer has wrapped around. Since the older traces that
    # don't implement this flag will read it as zero, to stay backwards
    # compatible, we decode a 0 as "wrapped" and 1 as "unwrapped".
    wrapped = True if header[44:45] == b'\x00' else False
    # If wrapped around or old format, read the whole buffer, otherwise only
    # read the valid entries
    entry_count = entries_max if wrapped else head_index

    if entry_count == 0:
        # Empty buffer, skip over the unused bytes
        print("  Empty buffer")
        args.input.seek(entries_max * 64, 1)
        return iter(())
    else:
        print("  Found {:d} entries. Wrapped: {}".format(entry_count, wrapped))

    warn = True
    entries = []
    for i in range(entry_count):
        trace = args.input.read(64)
        if len(trace) < 64:
            print("  Warning, log truncated. Read {:d} of {:d} entries".format(
                i, entry_count))
            break
        try:
            entries.append(Event(args, *struct.unpack(endian + "QQQQQQQQ",
                                                      trace)))
        except ValueError:
            if warn:
                print("  Warning, bad input. Read {:d} of {:d} entries".format(
                    i, entry_count))
                warn = False
            pass

    if args.sort == 'm':
        if global_buffer:
            header_string = "=== GLOBAL TRACES START ===\n"
        else:
            header_string = "=== CPU {:s} TRACES START ===\n".format(cpus)
    else:
        if global_buffer:
            header_string = "=== GLOBAL TRACE ===\n"
        else:
            header_string = "=== CPU {:s} TRACE ===\n".format(cpus)

    if not wrapped or (head_index == entries_max):
        first_index = 0
    else:
        first_index = head_index

    # Add the same timestamp as the first entry
    entry_header = LogEntry(entries[first_index].ticks, 0, header_string)

    if args.sort == 's':
        # Split at the head index
        entry_iter = itertools.chain(
            [entry_header], entries[head_index:], entries[:head_index])
    else:
        entry_iter = itertools.chain([entry_header], entries)

    if not wrapped:
        # Skip over the unused bytes
        if args.input.seekable():
            args.input.seek((entries_max - head_index) * 64, 1)
        else:
            args.input.read((entries_max - head_index) * 64)

    return entry_iter


def read_all_entries(args):
    def entry_iters():
        if args.sort == 'm':
            yield [LogEntry(0, 0, "==== MERGED CPU AND GLOBAL TRACES ====\n")]

        while True:
            try:
                yield read_entries(args)
            except StopIteration:
                break

    return itertools.chain(*entry_iters())


def prepare_log(args, entry_iter):
    if args.show_missing:
        # Simple search for missing entries: look for either an invalid info
        # field, or a timestamp jumping backwards.
        #
        # If the timestamp jumps backwards by less than 10 ticks, we assume
        # that it was an out-of-order trace write due to a race to obtain a
        # slot. This typically suppresses several false positives in any large
        # trace buffer.
        timestamp_slack = 10
        last_timestamp = -math.inf
        missing_entry = False
        log = []
        for entry in entry_iter:
            if entry is None:
                missing_entry = True
                if log:
                    log[-1].missing_after = True
            else:
                if missing_entry:
                    entry.missing_before = True
                    missing_entry = False

                timestamp = entry.ticks
                if timestamp + timestamp_slack < last_timestamp:
                    entry.missing_before = True
                    if log:
                        log[-1].missing_after = True

                last_timestamp = timestamp
                log.append(entry)
    else:
        log = list(entry_iter)

    if args.sort == 'm':
        log = sorted(log, key=lambda e: e.ticks)

    if len(log) == 0:
        sys.exit(1)

    return log


def print_log(args, log):
    prev_entry = None
    for entry in log:
        if args.show_missing and (prev_entry is not None and (
                entry.missing_before or prev_entry.missing_after)):
            args.output.write("<possible missing entries>\n")
        args.output.write(str(entry))
        prev_entry = entry


if __name__ == "__main__":
    main()

```

`tools/elf/package_apps.py`:

```py
#!/usr/bin/env python3

# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

import argparse
import logging
import sys
from elftools.elf.elffile import ELFFile
from elftools.elf.constants import SH_FLAGS, P_FLAGS
from elftools import construct


class NewSegment():
    def __init__(self, base, p_align=16):
        self._data = b''
        hdr = construct.lib.Container()
        hdr.p_type = 'PT_LOAD'
        hdr.p_flags = P_FLAGS.PF_R
        hdr.p_offset = 0
        hdr.p_vaddr = 0
        hdr.p_paddr = 0
        hdr.p_filesz = 0
        hdr.p_memsz = 0
        hdr.p_align = p_align
        self.header = hdr
        # print(self.header)

    def add_data(self, data):
        n = len(data)
        self._data += data

        self.header.p_filesz += n
        self.header.p_memsz += n
        # print(self.header)


class NewELF():
    def __init__(self, base):
        self.structs = base.structs

        self.header = base.header
        # print(self.header)

        self.segments = []
        self.sections = []

        for i in range(0, base.num_segments()):
            seg = base.get_segment(i)
            seg._data = seg.data()
            self.segments.append(seg)
            # print("   ", self.segments[i].header)

        for i in range(0, base.num_sections()):
            sec = base.get_section(i)
            sec._data = sec.data()
            self.sections.append(sec)
            # print("   ", self.sections[i].header)

    def strip(self):
        print("strip() unimplemented")

    def merge_segments(self, elf):
        print("merging...")

        p_last = 0
        # Find the end of the last segment
        for seg in self.segments:
            last = seg.header.p_offset + seg.header.p_filesz
            if last > p_last:
                p_last = last

        p_adj = p_last

        # Append new segments
        for i in range(0, elf.num_segments()):
            seg = elf.get_segment(i)
            seg._data = seg.data()

            p_last = (p_last + (seg.header.p_align - 1)) & \
                     (0xffffffffffffffff ^ (seg.header.p_align - 1))
            # print(hex(p_last))
            seg.header.p_offset = p_last
            # print(seg.header)
            self.segments.append(seg)
            self.header.e_phnum += 1

            p_last = p_last + seg.header.p_filesz

        p_off = p_last - p_adj
        # print(">>", hex(p_adj), hex(p_last), hex(p_off))

        # Adjust file offsets for affected sections
        for sec in self.sections:
            if sec.header.sh_offset >= p_adj:
                # print(sec.header)
                align = sec.header.sh_addralign
                if align > 1:
                    p_off = (p_off + (align - 1)) & \
                            (0xffffffffffffffff ^ (align - 1))
                # print("SA", hex(sec.header.sh_offset), hex(p_off),
                #       hex(sec.header.sh_offset + p_off))
                sec.header.sh_offset += p_off

        if self.header.e_shoff >= p_adj:
            self.header.e_shoff += p_off

    def append_segment(self, newseg):
        print("appending...")

        p_last = 0
        # Find the end of the last segment
        for seg in self.segments:
            last = seg.header.p_offset + seg.header.p_filesz
            if last > p_last:
                p_last = last

        p_adj = p_last

        # Append new segment
        p_last = (p_last + (newseg.header.p_align - 1)) & \
                 (0xffffffffffffffff ^ (newseg.header.p_align - 1))
        # print(hex(p_last))
        newseg.header.p_offset = p_last
        # print(newseg.header)
        self.segments.append(newseg)
        self.header.e_phnum += 1

        p_last = p_last + newseg.header.p_filesz

        p_off = p_last - p_adj
        # print(">>", hex(p_adj), hex(p_last), hex(p_off))

        # Adjust file offsets for affected sections
        for sec in self.sections:
            if sec.header.sh_offset >= p_adj:
                # print(sec.header)
                align = sec.header.sh_addralign
                if align > 1:
                    p_off = (p_off + (align - 1)) & \
                            (0xffffffffffffffff ^ (align - 1))
                # print("SA", hex(sec.header.sh_offset), hex(p_off),
                #       hex(sec.header.sh_offset + p_off))
                sec.header.sh_offset += p_off

        if self.header.e_shoff >= p_adj:
            self.header.e_shoff += p_off

    # Insert a segment into a pre-sorted ELF
    def insert_segment(self, newseg, phys):
        print("inserting...")

        phys_offset = self.segments[0].header.p_paddr - \
            self.segments[0].header.p_vaddr
        newseg.header.p_paddr = phys
        newseg.header.p_vaddr = phys - phys_offset

        p_adj = 0
        idx = 0
        # Find the position to insert segment
        for seg in self.segments:
            if seg.header.p_paddr > newseg.header.p_paddr:
                break
            idx += 1
            # print(seg, hex(seg.header.p_paddr))
            last = seg.header.p_offset + seg.header.p_filesz
            assert (last >= p_adj)
            p_adj = last

        p_prev = p_adj

        # Append new segment
        p_adj = (p_adj + (newseg.header.p_align - 1)) & \
                (0xffffffffffffffff ^ (newseg.header.p_align - 1))
        # print(hex(p_adj))
        newseg.header.p_offset = p_adj
        # print(newseg.header)
        self.segments.insert(idx, newseg)
        self.header.e_phnum += 1

        p_adj = p_adj + newseg.header.p_filesz

        p_off = p_adj - p_prev
        # print(">>", hex(p_adj), hex(p_prev), hex(p_off))

        # Update file offsets of remaining segments
        for seg in self.segments[idx+1:]:
            last = seg.header.p_offset + seg.header.p_filesz
            assert (last >= p_prev)
            p_next = seg.header.p_offset + p_off
            p_adj = (p_next + (seg.header.p_align - 1)) & \
                    (0xffffffffffffffff ^ (seg.header.p_align - 1))
            seg.header.p_offset = p_adj
            p_off += p_adj - p_next

        # Adjust file offsets for affected sections
        for sec in self.sections:
            if sec.header.sh_offset >= p_prev:
                # print(sec.header)
                align = sec.header.sh_addralign
                if align > 1:
                    p_off = (p_off + (align - 1)) & \
                            (0xffffffffffffffff ^ (align - 1))
                # print("SA", hex(sec.header.sh_offset), hex(p_off),
                #       hex(sec.header.sh_offset + p_off))
                sec.header.sh_offset += p_off

        if self.header.e_shoff >= p_prev:
            self.header.e_shoff += p_off

    # Align LOAD segment's p_filesz
    def segment_filesz_align(self, align):
        print("segment align...")

        assert (align & (align - 1)) == 0
        last_end = 0

        # Adjust file offsets for affected sections
        for seg in self.segments:
            # print(seg, seg.header.p_type)
            if seg.header.p_type == 'PT_LOAD':
                assert seg.header.p_offset >= last_end
                if seg.header.p_align < align:
                    print('WARN: segment {:#x} / {:#x} p_align < {:d}'.format(
                          seg.header.p_paddr, seg.header.p_vaddr, align))
                    continue
                p_filesz = seg.header.p_filesz
                seg.header.p_filesz += align - 1
                seg.header.p_filesz &= ~(align - 1)
                if p_filesz < seg.header.p_filesz:
                    assert len(seg._data) == p_filesz
                    pad = bytes([0] * (seg.header.p_filesz-p_filesz))
                    seg._data = seg._data + pad
                if seg.header.p_memsz < seg.header.p_filesz:
                    seg.header.p_memsz = seg.header.p_filesz
                last_end = seg.header.p_offset + seg.header.p_filesz

    # Merge physically adjacent segments
    def merge_physical(self):
        print("merge physical...")

        prev = None

        next_list = []

        # Adjust file offsets for affected sections
        for seg in self.segments:
            # print(seg, seg.header.p_type)
            if seg.header.p_type != 'PT_LOAD':
                next_list.append(seg)
                continue
            if prev:
                prev_end = prev.header.p_paddr + prev.header.p_filesz
                if prev_end == seg.header.p_paddr:
                    assert prev.header.p_filesz == prev.header.p_memsz
                    prev.header.p_filesz += seg.header.p_filesz
                    prev.header.p_memsz += seg.header.p_memsz
                    prev.header.p_flags |= seg.header.p_flags
                    self.header.e_phnum -= 1
                    prev._data = prev._data + seg._data
                    # self.segments.remove(seg)
                    # seg = prev
                else:
                    next_list.append(seg)
                    prev = seg
            else:
                next_list.append(seg)
                prev = seg
        self.segments = next_list

    def write(self, f):

        print("writing...")

        # print("EH", self.header)

        # Write out the ELF header
        f.seek(0)
        self.structs.Elf_Ehdr.build_stream(self.header, f)

        # Write out the ELF program headers
        f.seek(self.header.e_phoff)
        for seg in self.segments:
            # print("PH", seg.header)
            self.structs.Elf_Phdr.build_stream(seg.header, f)

        # Write out the ELF segment data
        for seg in self.segments:
            f.seek(seg.header.p_offset)
            f.write(seg._data)

        # Write out the ELF section headers
        f.seek(self.header.e_shoff)
        for sec in self.sections:
            # print("SH", sec.header)
            self.structs.Elf_Shdr.build_stream(sec.header, f)

        # Write out the ELF non-segment based sections
        for sec in self.sections:
            # Copy extra sections, mostly strings and debug
            if sec.header.sh_flags & SH_FLAGS.SHF_ALLOC == 0:
                # print("SH", sec.header)
                f.seek(sec.header.sh_offset)
                f.write(sec._data)
                continue


def package_files(base, app, runtime, output, p_filesz_align=None,
                  merge_phys=False):

    base_elf = ELFFile(base)
    new = NewELF(base_elf)

    symtab = base_elf.get_section_by_name('.symtab')
    pkg_phys = symtab.get_symbol_by_name('image_pkg_start')
    if pkg_phys:
        print(pkg_phys[0].name, hex(pkg_phys[0].entry.st_value))
        pkg_phys = pkg_phys[0].entry.st_value
    else:
        logging.error("can't find symbol 'image_pkg_start'")
        sys.exit(1)

    # Describe the package header structure
    pkg_hdr = construct.Struct(
        'pkg_hdr',
        construct.ULInt32('ident'),
        construct.ULInt32('items'),
        construct.Array(
            3,
            construct.Struct(
                'list',
                construct.ULInt32('type'),
                construct.ULInt32('offset'))
        ),
    )
    hdr = construct.lib.Container()
    # Initialize package header
    hdr.ident = 0x47504b47  # GPKG
    hdr.items = 0
    items = []
    for i in range(0, 3):
        item = construct.lib.Container()
        item.type = 0
        item.offset = 0
        items.append(item)
    hdr.list = items
    hdr_len = len(pkg_hdr.build(hdr))

    # Add the runtime ELF image
    run_data = runtime.read()
    run_data_len = len(run_data)

    pad = ((run_data_len + 0x1f) & ~0x1f) - run_data_len
    if pad:
        run_data += b'\0' * pad
        run_data_len += pad
    hdr.list[0].type = 0x1  # Runtime
    hdr.list[0].offset = hdr_len
    hdr.items += 1

    # Add the application ELF image
    app_data = app.read()
    app_data_len = len(app_data)

    pad = ((app_data_len + 0x1f) & ~0x1f) - app_data_len
    if pad:
        app_data += b'\0' * pad
        app_data_len += pad

    hdr.list[1].type = 0x2  # Application
    hdr.list[1].offset = hdr_len + run_data_len
    hdr.items += 1

    # note, we align segment to 4K for signing tools
    segment = NewSegment(base_elf, 4096)

    segment.add_data(pkg_hdr.build(hdr))
    segment.add_data(run_data)
    segment.add_data(app_data)

    new.insert_segment(segment, pkg_phys)
    if p_filesz_align:
        new.segment_filesz_align(p_filesz_align)
    if merge_phys:
        new.merge_physical()
    new.write(output)


def main():
    logging.basicConfig(
        level=logging.INFO,
        format="%(message)s",
    )

    args = argparse.ArgumentParser()

    args.add_argument('--segment-size-align',
                      type=int,
                      help="Align p_filesz to page-size")
    args.add_argument('--merge-phys-segments',
                      action='store_true',
                      help="Merge physically adjacent segments")
    args.add_argument('-a', "--app",
                      type=argparse.FileType('rb'),
                      help="Input application ELF",
                      required=True)
    args.add_argument('-r', "--runtime",
                      type=argparse.FileType('rb'),
                      help="Input runtime ELF",
                      required=True)
    args.add_argument('-o', '--output',
                      type=argparse.FileType('wb'),
                      default=sys.stdout,
                      required=True,
                      help="Write output to file")
    args.add_argument('input', metavar='INPUT', nargs=1,
                      type=argparse.FileType('rb'),
                      help="Input hypervisor ELF")
    options = args.parse_args()

    p_filesz_align = options.segment_size_align

    if p_filesz_align is not None:
        if (p_filesz_align == 0) or \
           ((p_filesz_align & (p_filesz_align - 1)) != 0):
            raise ValueError("segment-size-align must be a power of 2!")

    package_files(options.input[0], options.app, options.runtime,
                  options.output, p_filesz_align, options.merge_phys_segments)


if __name__ == '__main__':
    main()

```

`tools/events/event_gen.py`:

```py
#!/usr/bin/env python3
#
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# 2019 Cog Systems Pty Ltd.
#
# SPDX-License-Identifier: BSD-3-Clause

import argparse
import os
import sys
import logging
import subprocess
import inspect
import pickle


if __name__ == '__main__' and __package__ is None:
    sys.path.append(os.path.dirname(os.path.dirname(__file__)))
    from utils import genfile
else:
    from ..utils import genfile


logger = logging.getLogger(__name__)


def main():
    logging.basicConfig(
        level=logging.INFO,
        format="%(message)s",
    )
    __loc__ = os.path.relpath(os.path.realpath(
        os.path.dirname(os.path.join(os.getcwd(), os.path.dirname(__file__)))))

    args = argparse.ArgumentParser()

    mode_args = args.add_mutually_exclusive_group(required=True)
    mode_args.add_argument('-t', '--template',
                           type=argparse.FileType('r', encoding='utf-8'),
                           help="Template file used to generate output")
    mode_args.add_argument('--dump-tree', action='store_true',
                           help="Print the parse tree and exit")
    mode_args.add_argument('-P', '--dump-pickle',
                           type=genfile.GenFileType('wb'),
                           help="Dump the IR to a Python pickle")

    args.add_argument('-m', '--module', default=None,
                      help="Constrain output to a particular module")
    args.add_argument('-I', '--extra-include', action='append', default=[],
                      help="Extra headers to include")
    args.add_argument('-d', "--deps", type=genfile.GenFileType('w'),
                      help="Write implicit dependencies to Makefile",
                      default=None)
    args.add_argument('-o', '--output', type=genfile.GenFileType('w'),
                      default=sys.stdout, help="Write output to file")
    args.add_argument("-f", "--formatter",
                      help="specify clang-format to format the code")
    args.add_argument('-p', '--load-pickle', type=argparse.FileType('rb'),
                      help="Load the IR from a Python pickle")
    args.add_argument('input', metavar='INPUT', nargs='*',
                      type=argparse.FileType('r', encoding='utf-8'),
                      help="Event DSL files to process")
    options = args.parse_args()

    if options.input and options.load_pickle:
        logger.error("Cannot specify both inputs and --load-pickle")
        args.print_usage()
        sys.exit(1)
    elif options.input:
        from lark import Lark, Visitor
        from parser import TransformToIR

        grammar_file = os.path.join(__loc__, 'grammars', 'events_dsl.lark')
        parser = Lark.open(grammar_file, parser='lalr', start='start',
                           propagate_positions=True)

        modules = {}
        events = {}
        transformer = TransformToIR(modules, events)

        for f in options.input:
            tree = parser.parse(f.read())

            class FilenameVisitor(Visitor):
                def __init__(self, filename):
                    self.filename = filename

                def __default__(self, tree):
                    tree.meta.filename = self.filename

            FilenameVisitor(f.name).visit(tree)
            if options.dump_tree:
                print(tree.pretty(), file=options.output)
            transformer.transform(tree)

        if options.dump_tree:
            return 0

        errors = transformer.errors
        for m in modules.values():
            errors += m.resolve(events)

        for m in modules.values():
            errors += m.finalise()

        if errors:
            logger.error("Found %d errors, exiting...", errors)
            sys.exit(1)
    elif options.load_pickle:
        modules = pickle.load(options.load_pickle)
    else:
        logger.error("Must specify inputs or --load-pickle")
        args.print_usage()
        sys.exit(1)

    if options.dump_pickle:
        pickle.dump(modules, options.dump_pickle, protocol=-1)
    else:
        from Cheetah.Template import Template

        try:
            module = modules[options.module]
        except KeyError:
            logger.error("Specified module '%s' is unknown", options.module)
            sys.exit(1)

        ns = [module, {'extra_includes': options.extra_include}]
        template = Template(file=options.template, searchList=ns)

        result = str(template)
        if options.formatter:
            ret = subprocess.run([options.formatter],
                                 input=result.encode("utf-8"),
                                 stdout=subprocess.PIPE)
            result = ret.stdout.decode("utf-8")
            if ret.returncode != 0:
                logger.error("Error formatting output", result)
                sys.exit(1)

        options.output.write(result)

    if options.deps is not None:
        deps = set()
        if options.input:
            deps.add(grammar_file)
        for m in sys.modules.values():
            try:
                f = inspect.getsourcefile(m)
            except TypeError:
                continue
            if f is None:
                continue
            f = os.path.relpath(f)
            if f.startswith('../'):
                continue
            deps.add(f)
        if options.dump_pickle:
            out_name = options.dump_pickle.name
        else:
            out_name = options.output.name
        options.deps.write(out_name + ' : ')
        options.deps.write(' '.join(sorted(deps)))
        options.deps.write('\n')
        options.deps.close()


if __name__ == '__main__':
    main()

```

`tools/events/ir.py`:

```py
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# 2019 Cog Systems Pty Ltd.
#
# SPDX-License-Identifier: BSD-3-Clause

import abc
import logging
import collections


logger = logging.getLogger(__name__)


def _first_of_type(children, cls):
    return next(c for c in children if isinstance(c, cls))


def _first_of_type_opt(children, cls):
    try:
        return _first_of_type(children, cls)
    except StopIteration:
        return None


def _all_of_type(children, cls):
    return tuple(c for c in children if isinstance(c, cls))


class IRObject(object):
    def __getstate__(self):
        state = self.__dict__.copy()
        if 'meta' in state:
            del state['meta']
        return state


class DSLError(Exception):
    pass


class Include(str, IRObject):
    pass


class Symbol(str, IRObject):
    pass


class Type(str, IRObject):
    pass


class ConstExpr(str, IRObject):
    pass


class LockName(str, IRObject):
    pass


class LockAnnotation(IRObject):
    def __init__(self, action, kind, lock):
        self.action = action
        self.kind = kind
        self.lock = lock

    def _check_kind(self, kinds):
        if self.lock in kinds:
            if kinds[self.lock] != self.kind:
                logger.error("%s:%d:%d: "
                             "error: inconsistent lock kinds for %s (%s, %s)",
                             self.meta.filename, self.meta.line,
                             self.meta.column, self.lock, self.kind,
                             kinds[self.lock])
                raise DSLError()
        else:
            kinds[self.lock] = self.kind

    def apply(self, acquires, releases, requires, excludes, kinds):
        self._check_kind(kinds)

        if self.action == 'acquire':
            if self.lock in acquires:
                prev = next(acquires & set([self.lock]))
                logger.error("%s:%d:%d: "
                             "error: %s previously acquired at %s:%d:%d",
                             self.meta.filename, self.meta.line,
                             self.meta.column, self.lock, prev.meta.filename,
                             prev.meta.line, prev.meta.column)
                raise DSLError()
            elif self.lock in releases:
                releases.remove(self.lock)
            else:
                acquires.add(self.lock)
                excludes.add(self.lock)
        elif self.action == 'release':
            if self.lock in releases:
                prev = next(releases & set([self.lock]))
                logger.error("%s:%d:%d: "
                             "error: %s previously released at %s:%d:%d",
                             self.meta.filename, self.meta.line,
                             self.meta.column, self.lock, prev.meta.filename,
                             prev.meta.line, prev.meta.column)
                raise DSLError()
            elif self.lock in acquires:
                acquires.remove(self.lock)
            else:
                releases.add(self.lock)
                requires.add(self.lock)
        elif self.action == 'require':
            if self.lock not in acquires:
                requires.add(self.lock)
        elif self.action == 'exclude':
            if self.lock not in releases:
                excludes.add(self.lock)
        else:
            raise NotImplementedError(self.action)

    def combine(self, actions, kinds):
        self._check_kind(kinds)
        actions[self.action].add(self.lock)

    def unwind(self):
        if self.action == 'acquire':
            ret = LockAnnotation('release', self.kind, self.lock)
            ret.meta = self.meta
            return ret
        elif self.action == 'release':
            ret = LockAnnotation('acquire', self.kind, self.lock)
            ret.meta = self.meta
            return ret
        else:
            return self


class Priority(float, IRObject):
    pass


class Result(IRObject):
    def __init__(self, children, void=False):
        try:
            self.type = _first_of_type(children, Type)
            self.default = _first_of_type(children, ConstExpr)
        except StopIteration:
            if void:
                self.type = Type('void')
                self.default = None
            else:
                raise StopIteration


class ExpectedArgs(list, IRObject):
    pass


class Selectors(list, IRObject):
    pass


class Param(IRObject):
    def __init__(self, children):
        self.name = _first_of_type(children, Symbol)
        self.type = _first_of_type(children, Type)


class SelectorParam(Param):
    pass


class CountParam(Param):
    pass


class AbstractEvent(IRObject, metaclass=abc.ABCMeta):
    def __init__(self, children):
        self.name = _first_of_type(children, Symbol)
        self._param_dict = collections.OrderedDict(
            (c.name, c) for c in children if isinstance(c, Param))

    def set_owner(self, module):
        self.module_name = module.name
        self.module_includes = module.includes

    @abc.abstractmethod
    def subscribe(self, subscription):
        pass

    def finalise(self):
        pass

    @abc.abstractproperty
    def subscribers(self):
        raise NotImplementedError

    @abc.abstractproperty
    def lock_opts(self):
        raise NotImplementedError

    @abc.abstractproperty
    def return_type(self):
        raise NotImplementedError

    @abc.abstractproperty
    def noreturn(self):
        raise NotImplementedError

    def param(self, name):
        return self._param_dict[name]

    @property
    def params(self):
        return tuple(self._param_dict.values())

    @property
    def param_names(self):
        return tuple(p.name for p in self.params)

    @property
    def unused_param_names(self):
        params = set(self.param_names)
        for s in self.subscribers:
            for h in s.all_handlers:
                params -= set(h.args)
                if not params:
                    return set()
        return params


class AbstractSortedEvent(AbstractEvent):
    def __init__(self, children):
        super().__init__(children)
        self._subscribers = []
        self._lock_opts = None

    def subscribe(self, subscription):
        super().subscribe(subscription)
        if subscription.priority is None:
            subscription.priority = 0
        if subscription.selectors is not None:
            logger.error("%s:%d:%d: error: selector %s does not apply to "
                         "non-selector event %s",
                         subscription.selectors[0].meta.filename,
                         subscription.selectors[0].meta.line,
                         subscription.selectors[0].meta.column,
                         subscription.selectors[0], self.name)
            raise DSLError()
        if subscription.constant is not None:
            logger.error("%s:%d:%d: error: constant value %s specified for "
                         "non-selector event %s",
                         subscription.constant.meta.filename,
                         subscription.constant.meta.line,
                         subscription.constant.meta.column,
                         subscription.constant, self.name)
            raise DSLError()
        self._subscribers.append(subscription)

    def finalise(self):
        super().finalise()
        subscribers = sorted(self._subscribers,
                             key=lambda x: (-x.priority, x.handler))
        for left, right in zip(subscribers, subscribers[1:]):
            if left.priority != 0 and left.priority == right.priority:
                logger.error("%s:%d:%d: error: handler %s for event %s has "
                             "the same nonzero priority as handler %s\n"
                             "%s:%d:%d: info: handler %s subscribed here",
                             left.priority.meta.filename,
                             left.priority.meta.line,
                             left.priority.meta.column, left.handler,
                             self.name, right.handler,
                             right.priority.meta.filename,
                             right.priority.meta.line,
                             right.priority.meta.column, right.handler)
                raise DSLError()
        self._subscribers = tuple(subscribers)

        acquires = set()
        releases = set()
        requires = set()
        excludes = set()
        kinds = {}
        for s in self._subscribers:
            for lock_opt in s.lock_opts:
                lock_opt.apply(acquires, releases, requires, excludes, kinds)
        lock_opts = []
        for lock in sorted(acquires):
            lock_opts.append(LockAnnotation('acquire', kinds[lock], lock))
        for lock in sorted(releases):
            lock_opts.append(LockAnnotation('release', kinds[lock], lock))
        for lock in sorted(requires):
            lock_opts.append(LockAnnotation('require', kinds[lock], lock))
        for lock in sorted(excludes):
            lock_opts.append(LockAnnotation('exclude', kinds[lock], lock))
        self._lock_opts = tuple(lock_opts)

        noreturn = (self._subscribers and self._subscribers[-1].handler and
                    self._subscribers[-1].handler.noreturn)
        if noreturn and self.return_type != 'void':
            s = self._subscribers[-1]
            n = s.handler.noreturn
            logger.error("%s:%d:%d: error: last handler %s for event %s must "
                         "return, but is declared as noreturn",
                         n.meta.filename, n.meta.line, n.meta.column,
                         s.handler, self.name)
            raise DSLError()
        for s in self._subscribers[:-1]:
            if s.handler is not None and s.handler.noreturn:
                n = s.handler.noreturn
                logger.error("%s:%d:%d: error: handler %s for event %s does "
                             "not return, but is not the last handler (%s)",
                             n.meta.filename, n.meta.line, n.meta.column,
                             s.handler, self.name,
                             self._subscribers[-1].handler)
                raise DSLError()
        self._noreturn = noreturn

    @property
    def noreturn(self):
        return self._noreturn

    @property
    def subscribers(self):
        return self._subscribers

    @property
    def lock_opts(self):
        return self._lock_opts


class Event(AbstractSortedEvent):
    @property
    def return_type(self):
        return 'void'


class HandledEvent(AbstractSortedEvent):
    def __init__(self, children):
        super().__init__(children)
        self.result = _first_of_type_opt(children, Result)

    @property
    def return_type(self):
        return self.result.type if self.result is not None else 'bool'

    @property
    def default(self):
        return self.result.default if self.result is not None else 'false'


class MultiEvent(AbstractSortedEvent):
    def __init__(self, children):
        super().__init__(children)
        self.count = _first_of_type(children, CountParam)

    @property
    def return_type(self):
        return self.count.type

    @property
    def unused_param_names(self):
        return super().unused_param_names - {self.count.name}


class SetupEvent(AbstractSortedEvent):
    def __init__(self, children):
        super().__init__(children)
        self.result = _first_of_type(children, Result)
        self.success = _first_of_type(children, Success)

        self.result.name = Symbol('result')
        if self.result.name in self._param_dict:
            result = self._param_dict[self.result.name]
            logger.error("%s:%d:%d: error: setup event must not have an "
                         "explicit parameter named '%s'",
                         result.meta.filename, result.meta.line,
                         result.meta.column, self.result.name)
            raise DSLError()
        self._result_param = Param([self.result.name, self.result.type])

    def finalise(self):
        super().finalise()

        if self.subscribers and self.subscribers[-1].unwinder is not None:
            u = self.subscribers[-1].unwinder
            logger.warning("%s:%d:%d: warning: unwinder %s() is unused",
                           u.meta.filename, u.meta.line, u.meta.column, u.name)

    @property
    def return_type(self):
        return self.result.type

    def param(self, name):
        if name == self.result.name:
            return self._result_param
        return super().param(name)


class SelectorEvent(AbstractEvent):
    def __init__(self, children):
        super().__init__(children)
        self.selector = _first_of_type(children, SelectorParam)
        try:
            self.result = _first_of_type(children, Result)
        except StopIteration:
            self.result = Result([Type('bool'), ConstExpr('false')])
        self._subscribers = {}

    @property
    def subscribers(self):
        return self._subscribers.values()

    def subscribe(self, subscription):
        if subscription.priority is not None:
            logger.error("%s:%d:%d: error: priority (%d) cannot be specified "
                         "for subscription to a selector event ('%s')",
                         subscription.priority.meta.filename,
                         subscription.priority.meta.line,
                         subscription.priority.meta.column,
                         subscription.priority, self.name)
            raise DSLError()
        if subscription.selectors is None:
            logger.error("%s:%d:%d: error: no selector specified for "
                         "subscription to selector event '%s'",
                         subscription.event_name.meta.filename,
                         subscription.event_name.meta.line,
                         subscription.event_name.meta.column, self.name)
            raise DSLError()
        for s in self._subscribers:
            for new in subscription.selectors:
                if new in self._subscribers[s].selectors:
                    logger.error("%s:%d:%d: error: duplicate selector '%s' "
                                 "specified for subscription to selector "
                                 "event '%s'",
                                 subscription.event_name.meta.filename,
                                 subscription.event_name.meta.line,
                                 subscription.event_name.meta.column, new,
                                 self.name)
                    raise DSLError()
        key = subscription.selectors[0]
        self._subscribers[key] = subscription

    def finalise(self):
        super().finalise()

        kinds = {}
        actions = {
            'acquire': set(),
            'release': set(),
            'require': set(),
            'exclude': set(),
        }
        for s in self._subscribers.values():
            for lock_opt in s.lock_opts:
                lock_opt.combine(actions, kinds)
        lock_opts = []
        for action in actions.keys():
            for lock in actions[action]:
                lock_opts.append(LockAnnotation(action, kinds[lock], lock))
        self._lock_opts = tuple(lock_opts)

    @property
    def lock_opts(self):
        return self._lock_opts

    @property
    def return_type(self):
        return self.result.type

    @property
    def noreturn(self):
        # Note: this could  be true if the selector is a enum type that is
        # covered by noreturn handlers. We're not likely to ever do that.
        return False

    @property
    def unused_param_names(self):
        return super().unused_param_names - {self.selector.name}


class Optional(IRObject):
    pass


class Public(IRObject):
    pass


class NoReturn(IRObject):
    pass


class Subscription(IRObject):
    def __init__(self, children):
        self.event_name = _first_of_type(children, Symbol)
        self.optional = any(c for c in children if isinstance(c, Optional))
        self.selectors = _first_of_type_opt(children, Selectors)
        self.handler = _first_of_type_opt(children, Handler)
        self.constant = _first_of_type_opt(children, Constant)
        if self.handler is None and self.constant is None:
            self.handler = Handler(_first_of_type_opt(children, ExpectedArgs),
                                   _first_of_type_opt(children, NoReturn))
        self.unwinder = _first_of_type_opt(children, Unwinder)
        self.priority = _first_of_type_opt(children, Priority)
        self.lock_opts = _all_of_type(children, LockAnnotation)

    def set_owner(self, module):
        self.module_name = module.name

    def resolve(self, events):
        try:
            self.event = events[self.event_name]
        except KeyError:
            if not self.optional:
                logger.error(
                    "%s:%d:%d: error: subscribed to unknown event '%s'",
                    self.meta.filename, self.meta.line, self.meta.column,
                    self.event_name)
                raise DSLError()
            self.event = NotImplemented
        else:
            self.event.subscribe(self)

        for h in self.all_handlers:
            h.resolve(self)

    @property
    def all_handlers(self):
        if self.event is not NotImplemented:
            if self.handler is not None:
                yield self.handler
            if self.unwinder is not None:
                yield self.unwinder


class AbstractFunction(IRObject, metaclass=abc.ABCMeta):
    def __init__(self, *children):
        self.name = _first_of_type_opt(children, Symbol)
        self.args = _first_of_type_opt(children, ExpectedArgs)
        self.public = any(c for c in children if isinstance(c, Public))
        self._noreturn = _first_of_type_opt(children, NoReturn)

    def resolve(self, subscription):
        self.subscription = subscription
        self.module_name = subscription.module_name
        self.event = subscription.event

        if self.name is None:
            self.name = self._default_name

        if self.args is None:
            self.args = self._available_params
        else:
            for a in self.args:
                if a not in self._available_params:
                    logger.error(
                        "%s:%d:%d: error: event '%s' has no argument '%s'",
                        a.meta.filename, a.meta.line,
                        a.meta.column, self.event.name, a)
                    raise DSLError()

    @abc.abstractproperty
    def _default_name(self):
        yield NotImplementedError

    @property
    def _available_params(self):
        return self.event.param_names

    @property
    def noreturn(self):
        return self._noreturn

    @property
    def return_type(self):
        return self.event.return_type if not self.noreturn else 'void'

    @property
    def params(self):
        for a in self.args:
            yield self.event.param(a)

    @property
    def lock_opts(self):
        for opt in self.subscription.lock_opts:
            yield opt

    def __lt__(self, other):
        return self.name < other.name

    def __str__(self):
        return self.name

    def __hash__(self):
        """Generate a unique hash for the function."""
        return hash((self.name, self.return_type) +
                    tuple((p.name, p.type) for p in self.params))


class Handler(AbstractFunction):
    @property
    def _default_name(self):
        return "{:s}_handle_{:s}".format(self.module_name, self.event.name)


class Unwinder(AbstractFunction):
    @property
    def _default_name(self):
        return "{:s}_unwind_{:s}".format(self.module_name, self.event.name)

    @property
    def _available_params(self):
        return (self.event.result.name,) + super()._available_params

    @property
    def return_type(self):
        return 'void'

    @property
    def lock_opts(self):
        for opt in self.subscription.lock_opts:
            yield opt.unwind()


class Constant(str, IRObject):
    def __init__(self, children):
        self.value = children[0]


class Success(Constant):
    pass


class Module(IRObject):
    def __init__(self, children):
        self.name = _first_of_type(children, Symbol)
        self.includes = _all_of_type(children, Include)
        self.events = _all_of_type(children, AbstractEvent)
        for e in self.events:
            e.set_owner(self)
        self.subscriptions = _all_of_type(children, Subscription)
        for s in self.subscriptions:
            s.set_owner(self)

    def merge(self, other):
        assert self.name == other.name
        self.includes += other.includes
        self.events += other.events
        self.subscriptions += other.subscriptions
        for s in other.subscriptions:
            s.set_owner(self)

    def resolve(self, events):
        errors = 0
        for s in self.subscriptions:
            try:
                s.resolve(events)
            except DSLError:
                errors += 1
        return errors

    def finalise(self):
        errors = 0
        for e in self.events:
            try:
                e.finalise()
            except DSLError:
                errors += 1
        return errors

    @property
    def handlers(self):
        # Unique event handlers defined by this module.
        #
        # Each of these may be used by multiple subscriptions, either to
        # different events, or to the same selector event with different
        # selections, or even repeatedly for one event.
        seen_handlers = dict()
        for s in self.subscriptions:
            for h in s.all_handlers:
                if h.name in seen_handlers:
                    if seen_handlers[h.name] != hash(h):
                        logger.error("handler decl mismatch: %s",
                                     h.name)
                        raise DSLError()
                    continue
                seen_handlers[h.name] = hash(h)
                yield h

    @property
    def declared_handlers(self):
        # Unique event handlers declared by this module's events.
        seen_handlers = dict()
        for e in self.events:
            for s in e.subscribers:
                for h in s.all_handlers:
                    if h.name in seen_handlers:
                        if seen_handlers[h.name] != hash(h):
                            logger.error("handler decl mismatch: %s",
                                         h.name)
                            raise DSLError()
                        continue
                    if h.public:
                        continue
                    seen_handlers[h.name] = hash(h)
                    yield h

    @property
    def handler_includes(self):
        seen_modules = set()
        seen_includes = set()
        for s in self.subscriptions:
            e = s.event
            if e is NotImplemented:
                continue

            m = e.module_name
            if m in seen_modules:
                continue
            seen_modules.add(m)

            for i in e.module_includes:
                if i in seen_includes:
                    continue
                seen_includes.add(i)
                yield i

    @property
    def simple_events(self):
        return (e for e in self.events if isinstance(e, Event))

    @property
    def handled_events(self):
        return (e for e in self.events if isinstance(e, HandledEvent))

    @property
    def multi_events(self):
        return (e for e in self.events if isinstance(e, MultiEvent))

    @property
    def setup_events(self):
        return (e for e in self.events if isinstance(e, SetupEvent))

    @property
    def selector_events(self):
        return (e for e in self.events if isinstance(e, SelectorEvent))

```

`tools/events/parser.py`:

```py
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# 2019 Cog Systems Pty Ltd.
#
# SPDX-License-Identifier: BSD-3-Clause

from lark import Transformer, v_args
from ir import (
    Include, Symbol, Type, ConstExpr, Priority, Result, ExpectedArgs, Param,
    Selectors, SelectorParam, CountParam, Module, Event, HandledEvent,
    MultiEvent, SetupEvent, SelectorEvent, Subscription, Optional, Public,
    Handler, Constant, Unwinder, Success, LockAnnotation, LockName, NoReturn)

import collections
import logging
import math


logger = logging.getLogger(__name__)


class TransformToIR(Transformer):
    def __init__(self, module_dict, event_dict):
        super().__init__()
        self.module_dict = module_dict
        self.event_dict = event_dict
        self.cur_event_dict = {}
        self.errors = 0

    def _add_without_duplicates(self, type_string, in_dict, new):
        if new.name in in_dict:
            existing = in_dict[new.name]
            new_meta = new.name.meta
            old_meta = existing.name.meta
            logger.error("%s:%d:%d: error: duplicate definition of %s '%s'",
                         new_meta.filename, new_meta.line, new_meta.column,
                         type_string, new.name)
            logger.info("%s:%d:%d: note: previous definition of %s '%s'",
                        old_meta.filename, old_meta.line,
                        old_meta.column, type_string, new.name)
            self.errors += 1
        else:
            in_dict[new.name] = new

    def _general_event(self, event_class, children, meta):
        # Check for duplicated parameters
        params = {}
        for c in children:
            if isinstance(c, Param):
                self._add_without_duplicates('parameter', params, c)

        event = event_class(children)
        event.meta = meta
        self._add_without_duplicates('event', self.cur_event_dict, event)
        return event

    @v_args(meta=True)
    def module(self, children, meta):
        assert not self.cur_event_dict
        m = Module(children)
        m.meta = meta
        if m.name in self.module_dict:
            self.module_dict[m.name].merge(m)
        else:
            self.module_dict[m.name] = m
        return m

    @v_args(meta=True)
    def interface(self, children, meta):
        interface_name = next(c for c in children if isinstance(c, Symbol))
        for name, event in self.cur_event_dict.items():
            if not name.startswith(interface_name + "_"):
                meta = event.name.meta
                logger.error("%s:%d:%d: error: incorrect name: "
                             "'%s' should start with '%s_'",
                             meta.filename, meta.line, meta.column, name,
                             interface_name)
                self.errors += 1
            self.event_dict[name] = event
        self.cur_event_dict = {}
        return self.module(children, meta)

    def include(self, children):
        return Include(''.join(str(c) for c in children))

    @v_args(meta=True)
    def publish_event(self, children, meta):
        return self._general_event(Event, children, meta)

    @v_args(meta=True)
    def publish_handled_event(self, children, meta):
        return self._general_event(HandledEvent, children, meta)

    @v_args(meta=True)
    def publish_multi_event(self, children, meta):
        return self._general_event(MultiEvent, children, meta)

    @v_args(meta=True)
    def publish_setup_event(self, children, meta):
        return self._general_event(SetupEvent, children, meta)

    @v_args(meta=True)
    def publish_selector_event(self, children, meta):
        return self._general_event(SelectorEvent, children, meta)

    @v_args(meta=True)
    def symbol(self, children, meta):
        sym = Symbol(*children)
        sym.meta = meta
        return sym

    @v_args(meta=True)
    def event_param(self, children, meta):
        p = Param(children)
        p.meta = meta
        return p

    @v_args(meta=True)
    def selector_param(self, children, meta):
        p = SelectorParam(children)
        p.meta = meta
        return p

    @v_args(meta=True)
    def count_param(self, children, meta):
        p = CountParam(children)
        p.meta = meta
        return p

    @v_args(meta=True)
    def result(self, children, meta):
        r = Result(children)
        r.meta = meta
        return r

    @v_args(meta=True)
    def void_result(self, children, meta):
        r = Result(children, void=True)
        r.meta = meta
        return r

    @v_args(meta=True)
    def type_decl(self, children, meta):
        t = Type(' '.join(str(c) for c in children))
        t.meta = meta
        return t

    @v_args(meta=True)
    def selector_const(self, children, meta):
        t = Type(' '.join(str(c) for c in children))
        t.meta = meta
        return t

    @v_args(meta=True)
    def subscribe(self, children, meta):
        s = Subscription(children)
        s.meta = meta
        return s

    @v_args(meta=True)
    def selector(self, children, meta):
        s = Selectors(children)
        s.meta = meta
        return s

    subscribe_public = subscribe

    def optional(self, children):
        return Optional()

    def public(self, children):
        return Public()

    @v_args(meta=True)
    def handler(self, children, meta):
        h = Handler(*children)
        h.meta = meta
        return h

    handler_public = handler

    @v_args(meta=True)
    def unwinder(self, children, meta):
        u = Unwinder(*children)
        u.meta = meta
        return u

    unwinder_public = unwinder

    constant = v_args(inline=True)(Constant)

    def expected_args(self, children):
        args = collections.OrderedDict()
        for c in children:
            c.name = c
            self._add_without_duplicates('argument', args, c)
        return ExpectedArgs(args.values())

    @v_args(meta=True)
    def priority(self, children, meta):
        if children[0] in ('first', 'max'):
            c = Priority(math.inf)
        elif children[0] in ('last', 'min'):
            c = Priority(-math.inf)
        elif children[0] == 'default':
            c = Priority(0)
        else:
            c = Priority(children[0])
        c.meta = meta
        return c

    @v_args(meta=True)
    def noreturn(self, children, meta):
        c = NoReturn()
        c.meta = meta
        return c

    @v_args(meta=True)
    def constexpr(self, children, meta):
        c = ConstExpr(' '.join(children))
        c.meta = meta
        return c

    @v_args(meta=True)
    def success(self, children, meta):
        c = Success(' '.join(children))
        c.meta = meta
        return c

    @v_args(meta=True)
    def lock_name(self, children, meta):
        c = LockName(''.join(children))
        c.meta = meta
        return c

    @v_args(meta=True)
    def lock_opt(self, children, meta):
        action, kind = children[0].rsplit()[-1].split('_')
        c = LockAnnotation(action, kind, children[1])
        c.meta = meta
        return c

```

`tools/events/templates/c.tmpl`:

```tmpl
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// 2019 Cog Systems Pty Ltd.
//
// SPDX-License-Identifier: BSD-3-Clause
// This file is automatically generated.
#set empty=True

\#include <hyptypes.h>
\#include <compiler.h>
#for i in $includes
\#include $i
#end for
#if $extra_includes

#end if
#for i in $extra_includes
\#include <$i>
#end for

\#include <events/${name}.h>
#for h in $declared_handlers
#set empty=False

#set noreturn = 'noreturn ' if $h.noreturn else ''
$noreturn$h.return_type
${h.name}(#slurp
#set sep = ''
#set params = list($h.params)
#if $params
#for p in $params
#set space = '' if p.type.endswith('*') else ' '
${sep}${p.type}${space}${p.name}#slurp
#set sep=', '
#end for
#else
void#slurp
#end if
)#slurp
#for l in $h.lock_opts
 ${l.action.upper()}_${l.kind.upper()}(${l.lock})#slurp
#end for
;
#end for
#def prototype(e)
#set noreturn = 'noreturn ' if $e.noreturn else ''
$noreturn$e.return_type
trigger_${e.name}_event(#slurp
#set sep = ''
#if $e.params
#for p in $e.params
#set space = '' if p.type.endswith('*') else ' '
${sep}${p.type}${space}${p.name}#slurp
#set sep=', '
#end for
#else
void#slurp
#end if
)#slurp
#end def
#def call(s, r)
#if r is not None
#set ret_assign = r + ' = '
#else
#set ret_assign = ''
#end if
#if s.handler is not None
#if s.handler.noreturn
${s.handler.name}(${', '.join(s.handler.args)})#slurp
#else
$ret_assign${s.handler.name}(${', '.join(s.handler.args)})#slurp
#end if
#elif s.constant is not None
$ret_assign${s.constant}#slurp
#else
#raise Exception("No handler or constant for subscription " + str($s))
#end if
#end def
#for e in $simple_events

$prototype(e)
{
#for p in sorted(e.unused_param_names)
    (void)$p;
#end for
#for s in e.subscribers
    $call(s, None);
#end for
}
#end for
#for e in $handled_events

$prototype(e)
{
#for p in sorted(e.unused_param_names)
    (void)$p;
#end for
#if e.subscribers
    $e.return_type ret;

#for s in e.subscribers
    $call(s, 'ret');
    if (ret != $e.default) {
        goto out;
    }

#end for
out:
    return ret;
#else
    return $e.default;
#end if
}
#end for
#for e in $multi_events

$prototype(e)
{
#for p in sorted(e.unused_param_names)
    (void)$p;
#end for
#if e.subscribers
    $e.count.type _ret;
#end if
#for s in e.subscribers
    $call(s, '_ret');
    if (_ret >= $e.count.name) {
        return ($e.count.type)0;
    }
    $e.count.name -= _ret;
#end for
    return $e.count.name;
}
#end for
#for e in $setup_events

$prototype(e)
{
#for p in sorted(e.unused_param_names)
    (void)$p;
#end for
    $e.result.type $e.result.name = $e.result.default;

#set indent = ' '*4
#set first = True
#for s in e.subscribers
#if first
#set first = False
#else
${indent}if (compiler_expected($e.result.name == $e.success)) {
#set indent = indent + ' '*4
#end if
${indent}$call(s, $e.result.name);
#end for

#for s in tuple(reversed(e.subscribers))[1:]
#if s.unwinder is not None
${indent}if (compiler_unexpected($e.result.name != $e.success)) {
${indent}    ${s.unwinder.name}(${', '.join(s.unwinder.args)});
${indent}}
#end if
#set indent = indent[:-4]
${indent}}
#end for

    return $e.result.name;
}
#end for
#for e in $selector_events
#set void=$e.return_type=='void'

\#pragma clang diagnostic push
\#pragma clang diagnostic ignored "-Wswitch-enum"

$prototype(e)
{
#for p in sorted(e.unused_param_names)
    (void)$p;
#end for
#if not $void
    $e.return_type ret;
#set ret='ret'
#else
#set ret=None
#end if
#if e.subscribers
    switch ($e.selector.name) {
#for s in e.subscribers
#for selector in $s.selectors
    case $selector:
#end for
        $call(s, ret);
#if s.handler and s.handler.noreturn
        // ${s.handler}() does not return
        compiler_unreachable_break;
#else
        break;
#end if
#end for
    default:
#if $void
	// Nothing to do
#else
        $ret = $e.result.default;
#end if
        break;
    }
#else
    // No subscribers
    (void)$e.selector.name;
#if not $void
    $ret = $e.result.default;
#end if
#end if

#if not $void
    return ret;
#end if
}

\#pragma clang diagnostic pop
#end for
#if empty
void
${name}_events_dummy_function(void);
#end if

```

`tools/events/templates/handlers.h.tmpl`:

```tmpl
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// 2019 Cog Systems Pty Ltd.
//
// SPDX-License-Identifier: BSD-3-Clause
// This file is automatically generated.
#if $includes

#end if
#for i in $handler_includes
\#include $i
#end for
#if $extra_includes

#end if
#for i in $extra_includes
\#include "$i"
#end for
#for h in $handlers
#if $h.public
#continue
#end if

#set noreturn = 'noreturn ' if $h.noreturn else ''
$noreturn$h.return_type
${h.name}(#slurp
#set sep = ''
#for p in $h.params
#set space = '' if p.type.endswith('*') else ' '
${sep}${p.type}${space}${p.name}#slurp
#set sep=', '
#end for
#if not $sep
void#slurp
#end if
)#slurp
#for l in $h.lock_opts
 ${l.action.upper()}_${l.kind.upper()}(${l.lock})#slurp
#end for
;
#end for

```

`tools/events/templates/triggers.h.tmpl`:

```tmpl
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// 2019 Cog Systems Pty Ltd.
//
// SPDX-License-Identifier: BSD-3-Clause
// This file is automatically generated.

#for e in $events

#set noreturn = 'noreturn ' if $e.noreturn else ''
$noreturn$e.return_type
trigger_${e.name}_event(#slurp
#set sep = ''
#if $e.params
#for p in $e.params
#set space = '' if p.type.endswith('*') else ' '
${sep}${p.type}${space}${p.name}#slurp
#set sep=', '
#end for
#else
void#slurp
#end if
)#slurp
#for l in $e.lock_opts
 ${l.action.upper()}_${l.kind.upper()}(${l.lock})#slurp
#end for
;
#end for

```

`tools/grammars/events_dsl.lark`:

```lark
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// 2019 Cog Systems Pty Ltd.
//
// SPDX-License-Identifier: BSD-3-Clause

start: _END_DECL (module | interface)+

interface : _interface_decl include* (_publish | subscribe_public)*
_interface_decl : "interface" symbol _END_DECL

module : _module_decl include* subscribe*
_module_decl : "module" symbol _END_DECL

include : "include" _include_file _END_DECL

_include_file : /</ _filename />/ | /"/ _filename /"/
_filename : /[a-zA-Z0-9\/.]+/

_publish : publish_event
    | publish_handled_event
    | publish_multi_event
    | publish_selector_event
    | publish_setup_event

publish_event : "event" symbol _event_params? _END_DECL

publish_handled_event : "handled_event" symbol _event_params? result? _END_DECL

publish_multi_event : "multi_event" symbol count_param _event_params?  _END_DECL

publish_setup_event : "setup_event" symbol _event_params? result success _END_DECL

publish_selector_event : "selector_event" symbol selector_param _event_params?  void_result? _END_DECL

symbol : IDENTIFIER

_event_params : event_param+
_EVENT_PARAM : _CONT_DECL "param"
event_param : _EVENT_PARAM _param
_SELECTOR_PARAM : _CONT_DECL "selector"
selector_param : _SELECTOR_PARAM _param
_COUNT_PARAM : _CONT_DECL "count"
count_param : _COUNT_PARAM _param
_param : symbol ":" type_decl

_RESULT : _CONT_DECL "return" WS_INLINE? ":"
result : _RESULT type_decl "=" constexpr
void_result : _RESULT type_decl "=" constexpr | _RESULT "void"
_SUCCESS : _CONT_DECL "success" WS_INLINE? ":"
success : _SUCCESS constexpr

type_decl: _type_decl
_type_decl: (QUALIFIER* IDENTIFIER (POINTER QUALIFIER*)*)
	| LPAREN _type_decl RPAREN
	| _type_decl LPAREN POINTER RPAREN LPAREN _arg_type_list? RPAREN
_arg_type_list: _arg_type_list COMMA _type_decl | _type_decl
_type_cast: LPAREN _type_decl RPAREN

subscribe : "subscribe" optional? symbol selector? _subscriber? _subscribe_opts _END_DECL
subscribe_public : "subscribe" optional? symbol selector? _subscriber_public _subscribe_opts _END_DECL

optional : "optional"
selector : "[" _selector_list "]"
// Note, the syntax, "subscribe event_name(args)" without an explicit 'handler'
// is a short-hand for "subscribe event_name handler default_name().. With a
// handler, the braces for arguments should go with the handler_name.
_subscriber : expected_args noreturn? unwinder? | handler unwinder? | unwinder | constant
_subscriber_public : handler_public unwinder_public? | constant
_HANDLER : ":" | _CONT_DECL "handler"
handler : _HANDLER symbol expected_args? public? noreturn?
handler_public : _HANDLER symbol expected_args? public noreturn?
_UNWINDER : _CONT_DECL "unwinder"
unwinder : _UNWINDER symbol? expected_args? public?
unwinder_public : _UNWINDER symbol? expected_args? public
_CONSTANT : _CONT_DECL "constant"
constant : _CONSTANT constexpr
public : _CONT_DECL? "public"
expected_args : "(" _NL_INDENT? _expected_arg_list? ")"
_expected_arg_list : symbol ("," _NL_INDENT? symbol)*
noreturn : _CONT_DECL? "noreturn"
%import common.SIGNED_NUMBER -> NUMBER
_PRIORITY : _CONT_DECL "priority"
priority : _PRIORITY (NUMBER | /min|max|first|last|default/)
_lock_name: IDENTIFIER | LPAREN (IDENTIFIER (ARROW | STOP))* IDENTIFIER RPAREN
lock_name : _lock_name
LOCK_OPT_KIND : _CONT_DECL /(acquire|release|require|exclude)_(lock|read)/
_lock_opt_kind : LOCK_OPT_KIND
lock_opt : _lock_opt_kind lock_name
_subscribe_opts : priority? lock_opt*

IDENTIFIER : /[a-zA-Z_][a-zA-Z0-9_]*/
QUALIFIER.2 : "const" | "volatile" | "_Atomic"
POINTER : "*"
ARROW : "->"
constexpr : _constexpr
_constexpr : IDENTIFIER _args? | SIGNED_INT | SIGNED_FLOAT | UNSIGNED_INT
LPAREN : "("
RPAREN : ")"
COMMA : ","
_args : LPAREN _arg_list? RPAREN
_arg_list : _NL_INDENT? _constexpr [COMMA _arg_list]
selector_const: _type_cast? _constexpr
_selector_list : _NL_INDENT? selector_const ["," _selector_list]
%import common.SIGNED_INT
%import common.SIGNED_FLOAT
%import common.INT
UNSIGNED_INT.2 : INT ("U" | "u")

%import common.WS_INLINE

LINE_COMMENT :  /\/\/[^\r\n]*/ _NL
_NL : /\r?\n/

SEMICOLON : ";"
STOP : "."

_NL_INDENT : _NL+ WS_INLINE
_CONT_DECL : (SEMICOLON WS_INLINE | LINE_COMMENT WS_INLINE | _NL_INDENT)+
_END_DECL : (STOP | LINE_COMMENT | _NL)+

%ignore WS_INLINE
%ignore LINE_COMMENT

```

`tools/grammars/hypercalls_dsl.lark`:

```lark
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

start : _top_level_member*

_top_level_member : _definition

%import .typed_dsl (_customized_type, _identifier, constant_value, LINE_COMMENT, WS)

hypercall_declaration : (_hypercall_property | _hypercall_params) ";"

_hypercall_property : declaration_sensitive | declaration_call_num | declaration_vendor_hyp_call

_hypercall_params : (_identifier | RESERVED) (declaration_input | declaration_output)

RESERVED : "res0" | "res1"

declaration_input : "input" _customized_type
declaration_output : "output" _customized_type
declaration_call_num : "call_num" constant_value
declaration_sensitive : "sensitve_return"
declaration_vendor_hyp_call : "vendor_hyp_call"

_definition : "define" _type_definition
_type_definition : hypercall_definition

hypercall_definition : _identifier "hypercall" "{" hypercall_declaration* "}" ";"

%ignore WS
%ignore LINE_COMMENT

```

`tools/grammars/typed_dsl.lark`:

```lark
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// 2019 Cog Systems Pty Ltd.
//
// SPDX-License-Identifier: BSD-3-Clause

start : _top_level_member*

ZERO : "0"
DIGIT : "0".."9"
DIGIT_NONZERO: "1".."9"
HEX_PREFIX : "0" ("x" | "X")
HEX_DIGIT : "A".."F" | "a".."f"
BIN_PREFIX : "0" ("b" | "B")
BIN_DIGIT : "0"|"1"
UNSIGNED_SUFFIX : "u" | "U"

INTEGER : DIGIT_NONZERO DIGIT* UNSIGNED_SUFFIX?
	| HEX_PREFIX (DIGIT | HEX_DIGIT)+ UNSIGNED_SUFFIX?
	| BIN_PREFIX BIN_DIGIT+ UNSIGNED_SUFFIX?
	| ZERO UNSIGNED_SUFFIX?

constant_value : INTEGER
constant_reference : _identifier

_primary_expr : constant_value
        | constant_reference
        | "(" constant_expression ")"

_unary_expr : _primary_expr
        | unary_operation
        | sizeof_operation
        | alignof_operation
        | maxof_operation
        | minof_operation
        | msb_operation
UNARY_OPERATOR : "~" | "!"
unary_operation : UNARY_OPERATOR _unary_expr | SIGN_OPERATOR _unary_expr
sizeof_operation : "sizeof" "(" _type ")"
alignof_operation : "_Alignof" "(" _type ")"
maxof_operation : "maxof" "(" _type ")"
minof_operation : "minof" "(" _type ")"
msb_operation : "msb" "(" constant_expression ")"

_mult_expr : _unary_expr | mult_operation
MULT_OPERATOR : "*" | "/" | "%"
mult_operation : _mult_expr MULT_OPERATOR _unary_expr

_add_expr : _mult_expr | add_operation
SIGN_OPERATOR : "+" | "-"
add_operation : _add_expr SIGN_OPERATOR _mult_expr

_shift_expr : _add_expr | shift_operation
SHIFT_OPERATOR.2 : "<<" | ">>"
shift_operation : _shift_expr SHIFT_OPERATOR _add_expr

_relational_expr : _shift_expr | relational_operation
RELATIONAL_OPERATOR : "<" | ">" | "<=" | ">="
relational_operation : _relational_expr RELATIONAL_OPERATOR _shift_expr

_equality_expr : _relational_expr | equality_operation
EQUALITY_OPERATOR : "==" | "!="
equality_operation : _equality_expr EQUALITY_OPERATOR _relational_expr

_bitwise_and_expr : _equality_expr | bitwise_and_operation
bitwise_and_operation : _bitwise_and_expr "&" _equality_expr

_bitwise_xor_expr : _bitwise_and_expr | bitwise_xor_operation
bitwise_xor_operation : _bitwise_xor_expr "^" _bitwise_and_expr

_bitwise_or_expr : _bitwise_xor_expr | bitwise_or_operation
bitwise_or_operation : _bitwise_or_expr "|" _bitwise_xor_expr

_logical_and_expr : _bitwise_or_expr | logical_and_operation
logical_and_operation : _logical_and_expr "&&" _bitwise_or_expr

_logical_or_expr : _logical_and_expr | logical_or_operation
logical_or_operation : _logical_or_expr "||" _logical_and_expr

_cond_expr : _logical_or_expr | conditional_operation
conditional_operation : _logical_or_expr "?" constant_expression ":" _cond_expr

constant_expression : _cond_expr
bracketed_constant_expression : constant_value
        | "(" constant_expression ")"

_top_level_member : _extension
            | _definition

_type : _indirect_type | direct_type

_indirect_type : array
            | pointer

array_size : constant_expression

array : "array" "(" array_size ")" _qualifiers? _type

pointer : "pointer" _qualifiers? _type

direct_type : _customized_type _qualifiers?

_qualifiers : "(" qualifier_list? ")"
qualifier_list : _qualifier ("," _qualifier)*

declaration : _identifier declaration_offset? _type ";"
declaration_offset : "@" bracketed_constant_expression

enumeration_noprefix : "noprefix"
enumeration_explicit : "explicit"
enumeration_expr : "=" constant_expression
enumeration_constant : _identifier enumeration_expr? enumeration_noprefix? ";"

bitfield_declaration: (_bitfield_normal | bitfield_delete) ";"
_bitfield_normal: bitfield_specifier _bitfield_type_specifier
_bitfield_type_specifier: bitfield_member | bitfield_unknown | bitfield_const
bitfield_member: _identifier _type _bitfield_modifiers bitfield_default?

_definition : "define" _type_definition
_type_definition : bitfield_definition
                | object_definition
                | structure_definition
                | union_definition
                | alternative_definition
                | constant_definition
                | global_definition
                | enumeration_definition

public : "public"

alternative_definition : _identifier public? "newtype" _type ";"

// Constants aka 'C' defines
constant_definition : _identifier public? "constant" direct_type? "=" constant_expression ";"

// Global variables (never public; declaration only)
global_definition : _identifier "global" direct_type ";"

bitfield_size : constant_expression
_bitfield_size : "<" bitfield_size ">"
bitfield_const_decl : "const"
bitfield_set_ops_decl : "set_ops"
_bitfield_param : bitfield_const_decl | bitfield_set_ops_decl
_bitfield_params : "(" _bitfield_param ("," _bitfield_param)* ")"
bitfield_definition : _identifier public? "bitfield" _bitfield_size? _bitfield_params? "{" bitfield_declaration* "}" ";"

// NOTE: restrict object declaration only in object when handling AST
object_definition : _identifier public? "object" _qualifiers? "{" declaration* "}" ";"

structure_definition : _identifier public? "structure" _qualifiers? "{" declaration* "}" ";"

union_definition : _identifier public? "union" _qualifiers? "{" declaration* "}" ";"

_enumeration_param : enumeration_noprefix | enumeration_explicit
_enumeration_params : "(" _enumeration_param? ("," _enumeration_param)* ")"
enumeration_definition : _identifier public? "enumeration" _enumeration_params? ("{" enumeration_constant* "}")? ";"

_extension : "extend" _type_extension
_type_extension : bitfield_extension
               | structure_extension
               | object_extension
               | union_extension
               | enumeration_extension

module_name : "module" _identifier

bitfield_extension : _identifier "bitfield" module_name? "{" bitfield_declaration+ "}" ";"

object_extension : _identifier "object" module_name? "{" declaration+ "}" ";"

structure_extension : _identifier "structure" module_name? "{" declaration+ "}" ";"

union_extension : _identifier "union" module_name? "{" declaration+ "}" ";"

// TODO: enumeration_extension to support module_name ?
enumeration_extension : _identifier "enumeration" "{" enumeration_constant+ "}" ";"

_customized_type : bitfield_type
                | structure_type
                | object_type
                | union_type
                | enumeration_type
                | alternative_type
		| primitive_type


bitfield_type : "bitfield" _identifier
structure_type : "structure" _identifier
object_noprefix : "noprefix"
object_type : "object" ("(" object_noprefix? ")")? _identifier
union_type : "union" _identifier
enumeration_type : "enumeration" _identifier
_TYPE : "type"
alternative_type : _TYPE _identifier

PRIMITIVE_TYPE.2 : /([su]int(8|16|32|64|ptr)|bool|char|size|[su]register)\b/
primitive_type : PRIMITIVE_TYPE

bitfield_specifier: bitfield_others | bitfield_auto | _bitfield_bit_list
bitfield_bit_range: _bitfield_bit [":" _bitfield_bit]
bitfield_bit_span: bitfield_width ":" _bitfield_bit
_bitfield_bit_range: bitfield_bit_range | bitfield_bit_span
_bitfield_bit_list: _bitfield_bit_range ("," _bitfield_bit_range)*
_bitfield_bit: bracketed_constant_expression
bitfield_width: "<" constant_expression ">"
bitfield_auto: AUTO bitfield_width?
bitfield_others: "others"
bitfield_delete: "delete" _identifier
bitfield_default: "=" constant_expression
_bitfield_modifiers: bitfield_shift?
bitfield_shift: "lsl" "(" constant_expression ")"
UNKNOWN : "unknown"
bitfield_unknown : UNKNOWN bitfield_default?
bitfield_const: bracketed_constant_expression

_qualifier : basic_qualifier | aligned_qualifier | group_qualifier
	  | atomic_qualifier | packed_qualifier | contained_qualifier
	  | writeonly_qualifier | lockable_qualifier | optimize_qualifier
BASIC_QUALIFIER : "const" | "restrict" |  "volatile"
basic_qualifier : BASIC_QUALIFIER
aligned_qualifier : "aligned" "(" constant_expression ")"
group_qualifier : "group" "(" GROUPNAME ("," GROUPNAME )*")"
atomic_qualifier : "atomic"
packed_qualifier : "packed"
contained_qualifier : "contained"
writeonly_qualifier : "writeonly"
lockable_qualifier : "lockable"
optimize_qualifier : "optimize"

BIT : "bit"
BITS : "bits"
ZEROS : "zeros"
ONES : "ones"
SELF : "self"
AUTO : "auto"

!_identifier : IDENTIFIER | _TYPE | PRIMITIVE_TYPE
IDENTIFIER : /\b(?!\d)\w+\b/
GROUPNAME : /~?\b\w+\b/

WS: /[ \t\f\r\n]/+

BLOCK_COMMENT : /\/\*+[^*]*\*+(?:[^\/*][^*]*\*+)*\//
LINE_COMMENT :  /\/\/[^\r\n]*/

%ignore WS
%ignore BLOCK_COMMENT
%ignore LINE_COMMENT

```

`tools/hypercalls/hypercall.py`:

```py
#!/usr/bin/env python3
#
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

import os
from Cheetah.Template import Template
from Cheetah import ImportHooks


def xreg_range(*args):
    return tuple('x{:d}'.format(r) for r in range(*args))


templates_dir = os.path.join('tools', 'hypercalls', 'templates')
abis = {}


class abi():
    def __init__(self, hypcall_base):
        self.hypcall_base = hypcall_base


class abi_aarch64(abi):
    def __init__(self, hypcall_base):
        super().__init__(hypcall_base)

        # The size in bytes of each machine register
        self.register_size = 8

        # Registers used for parameters and results. Note that we don't
        # support indirect results (i.e. structs larger than 16 bytes).
        self.parameter_reg = xreg_range(0, 8)
        self.result_reg = xreg_range(0, 8)

        # Registers clobbered by the hypervisor.
        self.caller_saved_reg = xreg_range(8, 18)

    @classmethod
    def register_name(cls, size, index):
        reg_type = "x" if size == 8 else "w"
        return "{}{}".format(reg_type, index)


# HVC 0 is used for ARM SMCCC (PSCI, etc). Gunyah uses 0x6XXX
abis['aarch64'] = abi_aarch64(0x6000)

# Dictionary with all hypercalls defined
hypcall_dict = dict()
vendor_hypcall_list = []


class Variable:
    def __init__(self, ctype, name, type_definition):
        self.ctype = ctype
        self.name = name
        self.size = type_definition.size
        self.category = type_definition.category
        if type_definition.category == "bitfield":
            self.type_name = type_definition.type_name
        if type_definition.category == "union":
            try:
                raw_type, _ = type_definition.named_member('raw')
                assert raw_type.size == self.size
            except Exception:
                raise Exception(
                    "Public unions must have a correctly sized 'raw' member")
        self.ignore = name.startswith('res0') or name.startswith(
            'res1') or name.endswith('_')
        self.pointer = False
        try:
            from ir import PointerType
            d = type_definition
            if isinstance(d.compound_type, PointerType):
                self.pointer = True
        except AttributeError:
            pass
        if self.ignore:
            if name.startswith('res0'):
                self.default = 0
            elif name.startswith('res1'):
                self.default = 0xffffffffffffffff
            elif name.endswith('_'):
                raise Exception(
                    "Invalid name ending with underscore: {:s}".format(name))
            else:
                raise Exception("Invalid ignored name {:s}".format(name))


class Hypercall:
    def __init__(self, name, num, properties, abi):
        self.name = name
        self.num = num
        self.used_regs = set()
        self.inputs = []
        self.input_count = 0
        self.outputs = []
        self.output_count = 0
        self.clobbers = set()
        self.abi = abis[abi]
        self.properties = properties

        self.hvc_num = "0x{:x}".format(self.abi.hypcall_base + num)

    def check_type(self, var, role):
        if var.size > self.abi.register_size:
            raise Exception('Hypercall {:s}: {:s} {:s} has type {:s}, which '
                            'does not fit in a {:d}-byte machine register '
                            '(size is {:d} bytes)'.format(
                                self.name, role, var.name, var.ctype,
                                self.abi.register_size, var.size))

    def add_input(self, input):
        self.check_type(input, 'input')
        reg = self.abi.parameter_reg[self.input_count]
        self.used_regs.add(reg)
        self.inputs.append((reg, input))
        self.input_count += 1

    def add_output(self, output):
        self.check_type(output, 'output')
        reg = self.abi.result_reg[self.output_count]
        self.used_regs.add(reg)
        self.outputs.append((reg, output))
        self.output_count += 1

    def finalise(self):
        if 'vendor_hyp_call' in self.properties:
            vendor_hypcall_list.append(self)
        else:
            hypcall_dict[self.num] = self

        self.inputs = tuple(self.inputs)
        self.outputs = tuple(self.outputs)

        # Calculate register clobber list for guest interface
        self.clobbers.update((x for x in self.abi.parameter_reg
                              if x not in self.used_regs))
        self.clobbers.update((x for x in self.abi.result_reg
                              if x not in self.used_regs))
        self.clobbers.update(self.abi.caller_saved_reg)


ns = locals()


def apply_template(template_file):
    ImportHooks.install()
    template = Template(file=template_file, searchList=ns)
    result = str(template)
    ImportHooks.uninstall()
    return result

```

`tools/hypercalls/hypercall_gen.py`:

```py
#!/usr/bin/env python3
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

from hypercall import Variable, Hypercall, apply_template
from lark import Lark, Tree

import os
import sys
import argparse
import subprocess
import logging
import pickle
import inspect


# Determine the location of this script.
__loc__ = os.path.realpath(os.path.join(os.getcwd(),
                                        os.path.dirname(__file__)))

# The typed directory is added to the sys path so that when the pickle is
# loaded it can find the corresponding ast nodes.
typed_path = os.path.join(__loc__, '..', 'typed')
sys.path.append(typed_path)


# Silence flake8 warning about CType unused. It is required for pickle.load
from abi import AArch64ABI, CType  # noqa: F401,E402


# The template directory is added to the sys path so templates can be imported
# from it.
template_path = os.path.join(__loc__, 'templates')
sys.path.append(template_path)


logger = logging.getLogger(__name__)

abi_classes = {
    'aarch64': AArch64ABI,
}

primitive_types = dict()
types = dict()

used_ids = set()
used_calls = set()


class HypercallObject:
    def __init__(self, name):
        self.name = name
        self.call_num = None
        self.inputs = []
        self.outputs = []
        self.properties = {}


def get_constant(c):
    type_parent = None
    while isinstance(c, Tree):
        type_parent = c.data
        c = c.children[0]

    assert (type_parent == 'constant_value')
    return c


def get_type(c, ir):
    type_parent = None
    while isinstance(c, Tree):
        type_parent = c.data
        c = c.children[0]

    if "primitive_type" in type_parent:
        try:
            d = primitive_types[c]
        except KeyError:
            logger.error("Type: %s not found", c)
            sys.exit(1)

        return (d.c_type_name, d)
    else:
        try:
            d = types[c]
        except KeyError:
            logger.error("Type: %s not found", c)
            sys.exit(1)

        if not d.type_name.endswith('_t'):
            c = c + '_t'
        return (c, d)

    logger.error("unknown type", c)
    sys.exit(1)


def get_hypercalls(tree, hypercalls, hyp_num, ir):
    for c in tree.children:
        if isinstance(c, Tree):
            if c.data == "hypercall_definition":
                name = c.children[0]
                if name in used_calls:
                    logger.error("Hypercall name: %s already used", name)
                    sys.exit(1)
                used_calls.add(name)
                new_hypercall = HypercallObject(name)
                hypercalls.insert(hyp_num, new_hypercall)
                get_hypercalls(c, hypercalls, hyp_num, ir)
                hyp_num += 1
            elif c.data == "hypercall_declaration":
                if isinstance(c.children[0], Tree):
                    node = c.children[0]
                    if node.data == "declaration_call_num":
                        val = get_constant(node.children[0])
                        if hypercalls[hyp_num].call_num is not None:
                            logger.error("Hypercall: %s multiple call_nums",
                                         hypercalls[hyp_num].name)
                            sys.exit(1)

                        call_num = int(str(val), base=0)
                        if call_num in used_ids:
                            logger.error("Hypercall call_num already used",
                                         hypercalls[hyp_num].name)
                            sys.exit(1)
                        used_ids.add(call_num)

                        hypercalls[hyp_num].call_num = call_num
                    elif node.data == "declaration_sensitive":
                        hypercalls[hyp_num].properties['sensitive'] = True
                    elif node.data == "declaration_vendor_hyp_call":
                        if hypercalls[hyp_num].call_num is not None:
                            logger.error(
                                "Hypercall: %s call_num and "
                                "vendor_hyp_call",
                                hypercalls[hyp_num].name)
                            sys.exit(1)
                        hypercalls[hyp_num].call_num = 0
                        hypercalls[hyp_num].properties['vendor_hyp_call'] = \
                            True
                    else:
                        raise TypeError
                elif isinstance(c.children[1], Tree):
                    identifier = str(c.children[0])
                    node = c.children[1]
                    if node.data == "declaration_input":
                        if len(hypercalls[hyp_num].inputs) >= 8:
                            logger.error("Maximum of 8 inputs per hypercall",
                                         hypercalls[hyp_num].name)
                            sys.exit(1)
                        (t, d) = get_type(node.children[0], ir)
                        hypercalls[hyp_num].inputs.append(
                            Variable(t, identifier, d))
                    elif node.data == "declaration_output":
                        if len(hypercalls[hyp_num].outputs) >= 8:
                            logger.error("Maximum of 8 outputs per hypercall",
                                         hypercalls[hyp_num].name)
                            sys.exit(1)
                        (t, d) = get_type(node.children[0], ir)
                        hypercalls[hyp_num].outputs.append(
                            Variable(t, identifier, d))
                    else:
                        raise TypeError
                else:
                    logger.error("internal error")
                    sys.exit(1)

    return hypercalls, hyp_num


def main():
    logging.basicConfig(
        level=logging.INFO,
        format="%(message)s",
    )

    arg_parser = argparse.ArgumentParser()

    arg_parser.add_argument("-o", "--output",
                            help="Output file (default stdout)",
                            type=argparse.FileType('w', encoding='utf-8'),
                            default=sys.stdout)
    arg_parser.add_argument('-t', '--template',
                            type=argparse.FileType('r', encoding='utf-8'),
                            help="Template file used to generate output")
    arg_parser.add_argument('--traceback', action="store_true",
                            help="Print a full traceback if an error occurs")
    arg_parser.add_argument("-f", "--formatter",
                            help="specify clang-format to format the code")
    arg_parser.add_argument("-d", "--deps", default=None,
                            type=argparse.FileType('w', encoding='utf-8'),
                            help="write implicit dependencies to a Makefile")
    arg_parser.add_argument("input", metavar='INPUT', nargs="*",
                            type=argparse.FileType('r', encoding='utf-8'),
                            help="Input type DSL files to process")
    arg_parser.add_argument('-p', '--load-pickle',
                            type=argparse.FileType('rb'),
                            help="Load the IR from typed Python pickle")
    arg_parser.add_argument("-a", "--abi", help="specify the target machine "
                            "compiler ABI name", choices=abi_classes.keys(),
                            required=True)

    options = arg_parser.parse_args()

    grammar_file = os.path.join(__loc__, '..', 'grammars',
                                'hypercalls_dsl.lark')

    parser = Lark.open(grammar_file, 'start', parser='lalr',
                       lexer='contextual', propagate_positions=True)

    from ir import PrimitiveType
    # Load typed pickle to get the types used for the inputs and output of the
    # hypercall
    ir = pickle.load(options.load_pickle)
    for d in ir.abi_refs:
        if isinstance(d, PrimitiveType):
            if d.indicator not in primitive_types and d.is_public:
                primitive_types[d.indicator] = d
    for d in ir.definitions:
        if d.indicator not in types and d.is_public:
            types[d.indicator] = d

    # Go through all *.hvc files, parse the content, do a top down iteration to
    # get all hypercalls and get the type and size for the inputs and output
    # arguments by searching in the ir of typed.pickle
    hypercalls = []
    hyp_num = 0
    for p in options.input:
        text = p.read()
        parse_tree = parser.parse(text)
        hypercalls, hyp_num = get_hypercalls(
            parse_tree, hypercalls, hyp_num, ir)
    for h in hypercalls:
        hyper = Hypercall(h.name, h.call_num, h.properties, options.abi)
        for i in h.inputs:
            hyper.add_input(i)
        for o in h.outputs:
            hyper.add_output(o)
        hyper.finalise()

    # Apply templates to generate the output code and format it
    result = apply_template(options.template)

    if options.formatter and not options.template.name.endswith('.S.tmpl'):
        ret = subprocess.run([options.formatter],
                             input=result.encode("utf-8"),
                             stdout=subprocess.PIPE)
        result = ret.stdout.decode("utf-8")
        if ret.returncode != 0:
            logger.error("Error formatting output", result)
            sys.exit(1)

    options.output.write(result)
    options.output.close()

    # Write deps last to get template specific imports
    if options.deps is not None:
        deps = set()
        for m in sys.modules.values():
            try:
                f = inspect.getsourcefile(m)
            except TypeError:
                continue
            if f is None:
                continue
            f = os.path.relpath(f)
            if f.startswith('../'):
                continue
            deps.add(f)
        options.deps.write(options.output.name + ' : ')
        options.deps.write(' '.join(sorted(deps)))
        options.deps.write('\n')
        options.deps.close()


if __name__ == '__main__':
    main()

```

`tools/hypercalls/templates/guest_interface.c.tmpl`:

```tmpl
// Automatically generated. Do not modify.
//
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause
#extends hypercall_api
#implements respond
#def prefix: $public_prefix

// Hypervisor Call C Types
\#include <guest_types.h>
// Hypervisor Call definitions
\#include <guest_interface.h>

#for hypcall_num in sorted($hypcall_dict.keys())
    #set $hypcall = $hypcall_dict[$hypcall_num]
    $type_signature($hypcall) {
    #set has_pointer = False
    #set outregs=[]
    #for $out_reg, _ in $hypcall.outputs
        #set outregs+=[$out_reg]
    #end for
    #for $in_reg, $input in $hypcall.inputs
        #if in_reg in outregs
        const
        #end if
#slurp
        register uint${input.size * 8}_t#slurp
        in_${in_reg}_ __asm__("$in_reg") = #slurp
        #if $input.ignore
        ${hex(input.default)}U;
        #else
        (uint${input.size * 8}_t)($register_expr($input));
        #end if
        #if $input.pointer
            #set $has_pointer = True
        #end if
    #end for
    #for $out_reg, $output in $hypcall.outputs
        register uint${output.size * 8}_t#slurp
        out_${out_reg}_ __asm__("$out_reg");
        #if $output.pointer
            #set $has_pointer = True
        #end if
    #end for

    __asm__ volatile( #slurp
    "hvc $hypcall.hvc_num" : #slurp
    #set sep = ''
    #for $out_reg, _ in $hypcall.outputs
        $sep"=r"(out_${out_reg}_) #slurp
        #set sep = ', '
    #end for
    #for $in_reg, _ in $hypcall.inputs
	#if in_reg not in outregs
        ${sep}"+r"(in_${in_reg}_) #slurp
        #set sep = ', '
        #end if
    #end for
        : #slurp
    #set sep = ''
    #for $in_reg, _ in $hypcall.inputs
	#if in_reg in outregs
        ${sep}"r"(in_${in_reg}_) #slurp
        #set sep = ', '
        #end if
    #end for
    : ${', '.join('"{:s}"'.format(c) for c in sorted($hypcall.clobbers, key=lambda x: int(x[1:])))}
#if $has_pointer
    , "memory"
#end if
    );

    #if $hypcall.outputs
    return #slurp
    #if len($hypcall.outputs) < 2
        $register_init($output, 'out_' + $out_reg + '_');
    #else
    ($return_type($hypcall)){
    #for $out_reg, $output in $hypcall.outputs
        .${output.name} = $register_init($output, 'out_' + $out_reg + '_'),
    #end for
    };
    #end if
    #end if
}

#end for

```

`tools/hypercalls/templates/guest_interface.h.tmpl`:

```tmpl
// Automatically generated. Do not modify.
//
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause
#extends hypercall_api
#implements respond
#def prefix: $public_prefix

#for hypcall_num in sorted($hypcall_dict.keys())
    #set $hypcall = $hypcall_dict[$hypcall_num]
$result_struct_definition($hypcall)
$type_signature($hypcall);

#end for

```

`tools/hypercalls/templates/hypercall_api.tmpl`:

```tmpl
## © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
##
## SPDX-License-Identifier: BSD-3-Clause

#def internal_prefix: hypercall_

#def public_prefix: gunyah_hyp_

#def register_expr(variable)
#if variable.category == 'bitfield'
## FIXME: this is an implementation detail of the type system
${variable.name}.bf[0]#slurp
#elif variable.category == 'union'
(uint${8 * variable.size}_t)${variable.name}.raw#slurp
#else
${variable.name}#slurp
#end if
#end def

#def register_init(variable, value)

#if variable.category == 'bitfield'
## FIXME: this is an implementation detail of the type system
${variable.type_name}_cast((uint${8 * variable.size}_t)${value})#slurp
#else
(${variable.ctype})#slurp
$value#slurp
#end if
#end def

#def result_struct(hypercall)
${prefix}${hypercall.name}_result#slurp
#end def

#def result_struct_definition(hypercall)
#unless len(hypercall.outputs) < 2
typedef struct ${result_struct(hypercall)} {
#set pad=0
#for r, o in hypercall.outputs
    #set pad_size=hypercall.abi.register_size - o.size
    ${o.ctype} _Alignas(register_t) ${o.name};
    #if pad_size
    uint8_t _pad${pad}[${pad_size}];	// Pad for struct static zero initialization
    #set pad=pad+1
    #end if
#end for
} ${result_struct(hypercall)}_t;

#end unless
#end def

#def return_type(hypercall)
#if len(hypercall.outputs) > 1
${result_struct(hypercall)}_t#slurp
#else if hypercall.outputs
${hypercall.outputs[0][1].ctype}#slurp
#else
void#slurp
#end if
#end def

#def type_signature(hypercall, suffix='', ignored_inputs=False)
${return_type(hypercall)}
${prefix}${hypercall.name}${suffix}(#slurp
#set sep = ''
#for r, i in hypercall.inputs
#unless i.ignore and not ignored_inputs
$sep${i.ctype} ${i.name}
#set sep = ', '
#end unless
#end for
#unless hypercall.inputs
void#slurp
#end unless
)
#end def

```

`tools/misc/convert-utf-8.sh`:

```sh
#!/bin/bash

set -e

enc=$(file --mime-encoding "$1" | sed -E 's/.*: //g')
if [[ $enc == "binary" ]]; then
	exit 0
fi
if [[ $enc != "utf-8" ]]; then
	iconv -f $enc -t utf-8 "$1" -o "$1.tmp"
	cat "$1.tmp" > "$1"
	rm "$1.tmp"
fi

```

`tools/misc/get_genfiles.py`:

```py
#!/usr/bin/env python3
# coding: utf-8
#
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

"""
Simple script to parse the compile_commands to extract generated source and
header files, to pass to cscope or other source indexing tools.
"""

import json
import sys
import os
import re

build_dir = 'build'
commands_file = 'compile_commands.json'
compile_commands = []

conf_default_weight = 60
build_preferences = {}

files = set()
incdirs = set()

include_regex = re.compile('(-iquote|-I) (\\w+[-\\{:s}\\w]+)'.format(os.sep))
imacros_regex = re.compile('(-imacros) (\\w+[-\\{:s}\\w.]+)'.format(os.sep))

for dir, dir_dirs, dir_files in os.walk('config/featureset'):
    regex = re.compile('^# indexer-weight: (\\d+)')
    for file in dir_files:
        file_base = os.path.splitext(file)[0]
        if file.endswith('.conf'):
            build_preferences[file_base] = conf_default_weight
            infile = os.path.join(dir, file)
            with open(infile, 'r') as f:
                for i, line in enumerate(f):
                    weights = regex.findall(line)
                    if weights:
                        build_preferences[file_base] = int(weights[0])

for dir, dir_dirs, dir_files in os.walk(build_dir):
    if commands_file in dir_files:
        x = os.stat(dir)
        time = max(x.st_atime, x.st_mtime, x.st_ctime)
        compile_commands.append((time, os.path.join(dir, commands_file)))

if not compile_commands:
    print('no build found!', file=sys.stderr)
    exit(1)

newest = 0.0

for time, f in compile_commands:
    x = os.stat(f)
    time = max(x.st_atime, x.st_mtime, x.st_ctime, time)
    for p in build_preferences.keys():
        if p in f:
            # Boost these to preference them
            time += build_preferences[p]

    if time > newest:
        newest = time
        infile = f

if len(compile_commands) > 1:
    print('warning: multiple builds found, using: {:s}'.format(
        infile), file=sys.stderr)

try:
    with open(infile, 'r') as f:
        compile = json.loads(f.read())

        for s in compile:
            if s['file'].startswith(build_dir):
                files.add(s['file'])
            cmd = s['command']
            for t, dir in include_regex.findall(cmd):
                if dir.startswith(build_dir):
                    incdirs.add(dir)
            for t, f in imacros_regex.findall(cmd):
                files.add(f)
except FileNotFoundError:
    exit(1)

for dir in incdirs:
    try:
        for f in os.listdir(dir):
            filename = os.path.join(dir, f)
            if filename.endswith('.h'):
                files.add(filename)
    except OSError:
        pass

for f in files:
    print(f)

```

`tools/misc/setversion.sh`:

```sh
#!/bin/sh
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

outputpath=../../hyp/core/boot/include/version.h
echo "updating Gunyah Hypervisor version #...."
echo "Remember to checkin version.h into P4"
echo chmod a+x ${outputpath}
echo $(source ../build/gen_ver.sh > ${outputpath})
echo "Done! "${outputpath}" is updated!"

```

`tools/misc/update_cscope.sh`:

```sh
#!/bin/sh
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

git ls-files "*.[ch]" > cscope.in
tools/misc/get_genfiles.py >> cscope.in

cscope -bk -icscope.in

```

`tools/objects/object_gen.py`:

```py
#!/usr/bin/env python3
#
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

from Cheetah.Template import Template

import argparse
import subprocess
import sys


class Object:
    __slots__ = 'name', 'config'

    def __init__(self, name):
        items = name.split(',')
        self.name = items[0]
        self.config = items[1:]

    def __str__(self):
        return self.name

    def type_enum(self):
        return "OBJECT_TYPE_{:s}".format(self.name.upper())

    def rcu_destroy_enum(self):
        return "RCU_UPDATE_CLASS_{:s}_DESTROY".format(self.name.upper())


def main():
    args = argparse.ArgumentParser()

    mode_args = args.add_mutually_exclusive_group(required=True)
    mode_args.add_argument('-t', '--template',
                           type=argparse.FileType('r', encoding="utf-8"),
                           help="Template file used to generate output")

    args.add_argument('-o', '--output',
                      type=argparse.FileType('w', encoding="utf-8"),
                      default=sys.stdout, help="Write output to file")
    args.add_argument("-f", "--formatter",
                      help="specify clang-format to format the code")
    args.add_argument('input', metavar='INPUT', nargs='+', action='append',
                      help="List of objects to process")
    options = args.parse_args()

    object_list = [Object(o) for group in options.input for o in group]

    output = "// Automatically generated. Do not modify.\n"
    output += "\n"

    ns = {'object_list': object_list}
    output += str(Template(file=options.template, searchList=ns))

    if options.formatter:
        ret = subprocess.run([options.formatter], input=output.encode("utf-8"),
                             stdout=subprocess.PIPE)
        output = ret.stdout.decode("utf-8")
        if ret.returncode != 0:
            raise Exception("failed to format output:\n ", ret.stderr)

    options.output.write(output)


if __name__ == '__main__':
    main()

```

`tools/registers/register_gen.py`:

```py
#!/usr/bin/env python3
#
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

from Cheetah.Template import Template

import argparse
import itertools
import subprocess
import logging
import sys

logger = logging.getLogger(__name__)

valid_access_strs = \
    set([''.join(x) for x in itertools.chain.from_iterable(
        itertools.combinations('oOrwRW', r) for r in range(1, 6))])


class register:
    def __init__(self, name, type_name, variants=[], access='rw'):
        if access in ['o', 'O']:
            access += 'rw'
        if access not in valid_access_strs:
            logger.error("Invalid access type '%s'", access)
            sys.exit(1)
        self.name = name
        self.type_name = type_name
        self._variants = variants
        self._read = 'r' in access
        self._write = 'w' in access
        self._volatile_read = 'R' in access
        self._barrier_write = 'W' in access
        self._ordered = 'O' in access
        self._non_ordered = 'o' in access or 'O' not in access

    @property
    def variants(self):
        ret = []
        type_name = self.type_name[:-1] if self.type_name.endswith(
            '!') else self.type_name

        for v in self._variants:
            if v.endswith('!'):
                ret.append((v[:-1],
                            type_name if self.type_name.endswith(
                                '!') else v[:-1]))
            else:
                ret.append(('_'.join((self.name, v)),
                            type_name if self.type_name.endswith(
                                '!') else '_'.join((type_name, v))))

        if not ret:
            ret = [(self.name, type_name)]
        return sorted(ret)

    @property
    def is_readable(self):
        return self._read

    @property
    def is_volatile(self):
        return self._volatile_read

    @property
    def is_writable(self):
        return self._write

    @property
    def is_writeable_barrier(self):
        return self._barrier_write

    @property
    def need_ordered(self):
        return self._ordered

    @property
    def need_non_ordered(self):
        return self._non_ordered


def generate_accessors(template, input, ns):
    registers = {}

    for line in input.splitlines():
        if line.startswith('//'):
            continue
        tokens = line.split(maxsplit=1)
        if not tokens:
            continue
        name = tokens[0]
        if name in registers:
            raise Exception("duplicate register:", name)

        if len(tokens) == 1:
            registers[name] = register(name, name)
            continue
        args = tokens[1]

        type_name = name
        if args.startswith('<'):
            type_name, args = args[1:].split('>', maxsplit=1)
            args = args.strip()

        identifiers = []
        if args.startswith('['):
            identifiers, args = args[1:].split(']', maxsplit=1)
            identifiers = identifiers.split()
            args = args.strip()

        if args:
            registers[name] = register(name, type_name, identifiers, args)
        else:
            registers[name] = register(name, type_name, identifiers)

    ns['registers'] = [registers[r] for r in sorted(registers.keys())]

    output = str(Template(file=template, searchList=ns))

    return output


def main():
    logging.basicConfig(
        level=logging.INFO,
        format="%(message)s",
    )

    args = argparse.ArgumentParser()

    mode_args = args.add_mutually_exclusive_group(required=True)
    mode_args.add_argument('-t', '--template',
                           type=argparse.FileType('r', encoding="utf-8"),
                           help="Template file used to generate output")

    args.add_argument('-o', '--output',
                      type=argparse.FileType('w', encoding="utf-8"),
                      default=sys.stdout, help="Write output to file")
    args.add_argument("-f", "--formatter",
                      help="specify clang-format to format the code")
    args.add_argument("input", metavar='INPUT', nargs='*',
                      help="Input type register file to process",
                      type=argparse.FileType('r', encoding="utf-8"))
    options = args.parse_args()

    output = ""

    input = ""
    for f in options.input:
        input += f.read()
        f.close()

    output += generate_accessors(options.template, input, {})

    if options.formatter:
        ret = subprocess.run([options.formatter], input=output.encode("utf-8"),
                             stdout=subprocess.PIPE)
        output = ret.stdout.decode("utf-8")
        if ret.returncode != 0:
            raise Exception("failed to format output:\n ", ret.stderr)

    options.output.write(output)


if __name__ == '__main__':
    main()

```

`tools/requirements.txt`:

```txt
lark_parser==0.8.9
Cheetah3==3.2.6
pyelftools==0.26

```

`tools/typed/abi.py`:

```py
#!/usr/bin/env python3
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

import abc


class CType:
    def __init__(self, name, is_signed, size, align=None, bitsize=None):
        self.name = name
        self.is_signed = is_signed
        self.size = size
        self.align = size if align is None else align
        self.bitsize = bitsize


class ABI(metaclass=abc.ABCMeta):
    """
    Abstract base class for ABI definitions.
    """

    def __init__(self):
        basic_ctypes = (
            CType('bool', False, 1, bitsize=1),
            CType('uint8_t', False, 1),
            CType('uint16_t', False, 2),
            CType('uint32_t', False, 4),
            CType('uint64_t', False, 8),
            CType('uintptr_t', False, self.pointer_size,
                  align=self.pointer_align),
            CType('int8_t', True, 1),
            CType('int16_t', True, 2),
            CType('int32_t', True, 4),
            CType('int64_t', True, 8),
            CType('intptr_t', True, self.pointer_size,
                  align=self.pointer_align),
            CType('char', self.signed_char, 1),
            CType('size_t', False, self.pointer_size,
                  align=self.pointer_align),
            CType('uregister_t', False, self.register_size,
                  align=self.register_align),
        )
        self.c_types = {t.name: t for t in basic_ctypes}

    @staticmethod
    def is_power2(val):
        """Returns true if number is a power of two"""
        return ((val & (val - 1)) == 0) and (val > 0)

    @abc.abstractproperty
    def pointer_size(self):
        """The size of a pointer, in bytes."""
        raise NotImplementedError

    @property
    def pointer_align(self):
        """The alignment of a pointer, in bytes."""
        return self.pointer_size

    @abc.abstractproperty
    def register_size(self):
        """The size of a register, in bytes."""
        raise NotImplementedError

    @property
    def register_align(self):
        """The alignment of a register, in bytes."""
        return self.pointer_size

    @abc.abstractproperty
    def signed_char(self):
        """True if the char type is signed."""
        raise NotImplementedError

    def get_c_type(self, type_name):
        return self.c_types[type_name]

    @abc.abstractmethod
    def get_c_type_name(self, abi_type_name):
        """Return the c name for the abi type name."""
        raise NotImplementedError

    def layout_struct_member(self, current_offset, current_alignment,
                             next_size, next_alignment):
        """
        Return the offset at which a new struct member should be placed.

        In principle this is entirely implementation-defined, but nearly all
        implementations use the same algorithm: add enough padding bytes to
        align the next member, and no more. Any ABI that does something
        different can override this.

        The current_offset argument is the size of the structure in bytes up
        to the end of the last member. This is always nonzero, because
        structures are not allowed to be padded at the start, so this function
        is only called for the second member onwards.

        The current_align argument is the largest alignment that has been seen
        in the members added to the struct so far.

        If next_size is not None, the next_size and next_alignment arguments
        are the size and alignment of the member that is being added to the
        struct. The return value in this case is the offset of the new member.

        If next_size is None, all members have been added and this function
        should determine the amount of padding at the end of the structure.
        The return value in this case is the final size of the structure.

        The return value in any case should be an integer that is not less
        than current_offset.
        """
        if next_size is not None:
            alignment = next_alignment
        else:
            alignment = current_alignment
        assert (self.is_power2(alignment))

        align_mask = alignment - 1
        return (current_offset + align_mask) & ~align_mask

    @abc.abstractmethod
    def get_enum_properties(self, e_min, e_max):
        """
        Returns the size, alignment and signedness of the enum.

        Calculates the underlying type properties that the ABI will use for
        an enum with the given enumerator value range.
        """
        raise NotImplementedError


class AArch64ABI(ABI):
    abi_type_map = {
        'uregister_t': 'uint64_t',
        'sregister_t': 'int64_t',
    }

    @property
    def pointer_size(self):
        """The size of a pointer, in bytes."""
        return 8

    @property
    def register_size(self):
        """The size of a register, in bytes."""
        return 8

    @property
    def signed_char(self):
        """True if the char type is signed."""
        return True

    def get_c_type_name(self, abi_type_name):
        """Return the c name for the abi type name."""
        assert (abi_type_name in self.abi_type_map)
        return self.abi_type_map[abi_type_name]

    def get_enum_properties(self, e_min, e_max):
        """
        Returns the size, alignment and signedness of the enum.

        Calculates the underlying type properties that the ABI will use for
        an enum with the given enumerator value range.
        """

        min_bits = e_min.bit_length()
        max_bits = e_max.bit_length()

        signed = e_min < 0

        if not signed and max_bits <= 32:
            return (4, 4, False)
        elif signed and max_bits <= 31 and min_bits <= 32:
            return (4, 4, True)
        elif not signed and max_bits <= 64:
            return (8, 8, False)
        elif signed and max_bits <= 63 and min_bits <= 64:
            return (8, 8, True)
        else:
            raise NotImplementedError

```

`tools/typed/ast_nodes.py`:

```py
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# 2019 Cog Systems Pty Ltd.
#
# SPDX-License-Identifier: BSD-3-Clause

from lark import Tree, Token
import itertools
from ir import (
    TopLevel, PrimitiveType, BitFieldDeclaration, BitFieldType, StructureType,
    EnumerationType, AlternativeType, BitFieldSpecifier, DirectType,
    PointerType, PrimitiveDeclaration, ArrayType, ConstantDefinition,
    AlternativeDefinition, BitFieldDefinition, StructureDefinition,
    EnumerationDefinition, EnumerationConstant, EnumerationExtension,
    ObjectType, ObjectDeclaration, ObjectDefinition, BitFieldExtension,
    ObjectExtension, Qualifier, AlignedQualifier, GroupQualifier,
    AtomicQualifier, PackedQualifier, ConstantExpression, ConstantReference,
    UnaryOperation, SizeofOperation, AlignofOperation, BinaryOperation,
    ConditionalOperation, UnionType, UnionDefinition, UnionExtension,
    StructureExtension, MinofOperation, MaxofOperation, ContainedQualifier,
    WriteonlyQualifier, PureFunctionCall, LockableQualifier, GlobalDefinition,
    OptimizeQualifier
)
from exceptions import DSLError

"""
The classes in the module represent nodes in the AST.
They are used only to parse the input.

All classes that inherit from CommonTree in this file  will be automatically
imported into the TransformTypes transformer (excluding CommonTree itself).
"""


def toint(text):
    """
    Convert value strings to integers.

    Supports Python styles for decimal, hex and binary (but not octal). Also
    supports (and ignores) a C-style U suffix.
    """
    text = text.rstrip('uU')
    if len(text) > 1 and text[0] == '0' and text[1] not in 'xXbB':
        raise DSLError('Unknown base for value {:s}'.format(text), text)

    return int(text, base=0)


class CommonTree(Tree):
    """
    Common class for all AST nodes
    """

    def __init__(self, program, children, meta, data=None):
        if data is None:
            data = self.__class__.__name__
        super().__init__(data, children, meta)
        self.program = program

    def pass_up(self):
        pass

    @property
    def num_children(self):
        return len(self.children)


class TToken(str):
    __slots__ = ['program', 'line', 'column', 'pos_in_stream']

    def __new__(cls, val, token=None, program=None, line=None, column=None,
                pos_in_stream=None):
        self = super(TToken, cls).__new__(cls, val)
        if token:
            line = token.line
            column = token.column
            pos_in_stream = token.pos_in_stream

        self.program = program
        self.line = line
        self.column = column
        self.pos_in_stream = pos_in_stream
        return self

    def __reduce__(self):
        return (TToken, (str(self), None, self.program, self.line, self.column,
                         self.pos_in_stream))


class Action:
    """
    A class helps rules to register function to provide input for parent node.
    Parameter:
    fn: function to call if parent decide to take this action.
        The signature of this actionis "def f(object)". This action can handle
    the object as it wants.
    name: the rule name who provides this action
    passive: True indicate it needs to be specifically called
    trace: the tree node who provides this action. Just for debug
    """

    def __init__(self, fn, name, passive=False, trace=None):
        self.name = name
        self.trace = trace
        self.fn = fn
        self.passive = passive

    def take(self, obj):
        """
        Take this action, and change the specified obj
        """
        if self.trace:
            print("take ", self)
        return self.fn(obj)

    def __repr__(self):
        more = ""
        if self.trace:
            more = "<" + str(self.trace) + ">"
        return "action %s from %s%s" % (self.fn.__name__, self.name, more)

    def match(self, rule_name):
        return self.name == rule_name


class ActionList:
    """
    The helper to manipulate the actions.
    """

    def __init__(self, actions=[]):
        self.actions = actions.copy()

    def __iter__(self):
        return itertools.chain(self.actions)

    def __iadd__(self, x):
        if isinstance(x, ActionList):
            self.actions += x.actions
            return self
        else:
            raise Exception("only allowed to iadd Action List" + str(type(x)))

    def __repr__(self):
        ret = []
        for a in self.actions:
            ret.append("    " + str(a))
        return "Action list: \n" + '\n'.join(ret)

    def append_actions(self, action_list):
        self.actions += action_list

    def take_all(self, obj, accept_list=[], deny_list=[], remove=True):
        if set(accept_list) & set(deny_list):
            raise Exception("cannot accept/deny same name at the same time: " +
                            ', '.join(set(accept_list) & set(deny_list)))

        for a in list(self.actions):
            if len(accept_list) > 0 and a.name in accept_list:
                a.take(obj)
            elif len(deny_list) > 0 and a.name not in deny_list:
                a.take(obj)
            else:
                continue

            if remove:
                self.actions.remove(a)

    def take(self, obj, name, remove=True, single=True):
        """
        Take actions, can get the return value
        parameters:
        obj: the object who receive the action result
        name: specify the action name to take
        remove: indicate if need to remove the action after take it
        single: indicate if need to take all actions who has the same name.
            If so, just return the last action's return value.

        """
        ret = None
        for a in list(self.actions):
            if a.name == name:
                if remove:
                    self.actions.remove(a)

                if single:
                    return a.take(obj)

                ret = a.take(obj)

        return ret

    def remove_all(self, remove_list):
        self.actions = [a for a in self.actions if a.name not in remove_list]

    def has(self, name):
        for a in self.actions:
            if name == a.name:
                return True
        return False

    def remains(self):
        return len(self.actions) != 0


class CommonListener(CommonTree):
    """
    Common class for all list nodes need to sync.

    Order: Since it's a bottom up node, all children visitor (for read/write)
    is ahead of parent, and left is ahead of right.
    """

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

        self.actions = []
        self.action_list = ActionList()

        # get all actions from children
        for c in self.children:
            if isinstance(c, CommonListener):
                self.action_list += c.pass_up()

        self.init_interfaces()

    def init_interfaces(self):
        """
        In this function, node can:
        * initialize all attributes which will be set by other nodes.
        * set actions for parent nodes
        * choose some actions to take
        """
        pass

    def pass_up(self):
        """
        Setup accepts of this nodes, parents will collect this node's accepts,
        and call them properly.
        By default, it just return remaining accepts
        """
        self.action_list.append_actions(self.actions)
        return self.action_list


class start(CommonListener):
    """
    The root node of the the AST
    """

    def init_interfaces(self):
        self.intermediate_tree = TopLevel()
        al = [
            "bitfield_extension",
            "bitfield_definition",
            "object_definition",
            "object_extension",
            "structure_definition",
            "structure_extension",
            "union_definition",
            "union_extension",
            "enumeration_definition",
            "enumeration_extension",
            "alternative_definition",
            "constant_definition",
            "global_definition",
            "declaration",
            "add_abi_ref",
            "add_constant_ref",
            "add_type_ref",
        ]
        self.action_list.take_all(self.intermediate_tree, al)

        if self.action_list.remains():
            print("Warning: untaken actions remain. ", self.action_list)

    def get_intermediate_tree(self):
        return self.intermediate_tree


"""
Add a pass_up method to tokens so we can treat them the same way as nodes
"""
Token.pass_up = lambda self: self


class constant_value(CommonListener):
    def init_interfaces(self):
        self.value = toint(self.children[0])


class constant_reference(CommonListener):
    def init_interfaces(self):
        self.value = ConstantReference(self.children[0])
        self.actions.append(Action(self.set_symbol_ref, 'add_constant_ref'))

    def set_symbol_ref(self, obj):
        obj.constant_refs.append(self.value)


class constant_expression(CommonListener):
    def init_interfaces(self):
        self.value = ConstantExpression(self.children[0].value)


bracketed_constant_expression = constant_expression


class unary_operation(CommonListener):
    def init_interfaces(self):
        self.value = UnaryOperation(self.children[0], self.children[1].value)


class IConstantFromTypeOperation(CommonListener):
    def init_interfaces(self):
        self.type_ref = None
        al = ["direct_type", "array", "pointer", "alias",
              "primitive_type", "bitfield_type", "structure_type",
              "union_type", "object_type", "enumeration_type",
              "alternative_type"]
        self.action_list.take_all(self, al)

        rl = [
            "pointer_has_pointer",
            "object_type_set_complex",
            "object_type_has_object",
            "object_type_create_declaration",
        ]

        self.action_list.remove_all(rl)
        self.actions.append(Action(self.set_type_ref, 'add_type_ref'))

    def set_type_ref(self, obj):
        if self.type_ref is not None:
            obj.type_refs.append(self.type_ref)


class sizeof_operation(IConstantFromTypeOperation):
    def init_interfaces(self):
        super().init_interfaces()
        self.value = SizeofOperation(self.compound_type)


class alignof_operation(IConstantFromTypeOperation):
    def init_interfaces(self):
        super().init_interfaces()
        self.value = AlignofOperation(self.compound_type)


class minof_operation(IConstantFromTypeOperation):
    def init_interfaces(self):
        super().init_interfaces()
        self.value = MinofOperation(self.compound_type)


class maxof_operation(IConstantFromTypeOperation):
    def init_interfaces(self):
        super().init_interfaces()
        self.value = MaxofOperation(self.compound_type)


class IPureFunction(CommonListener):
    def init_interfaces(self):
        self.action_list.take(self, 'constant_expression')
        self.value = PureFunctionCall(self.children[0].value, self.f)


class msb_operation(IPureFunction):
    def f(self, x):
        return x.bit_length() - 1


class IBinaryOperation(CommonListener):
    def init_interfaces(self):
        self.value = BinaryOperation(self.children[1], self.children[0].value,
                                     self.children[2].value)


class mult_operation(IBinaryOperation):
    pass


class add_operation(IBinaryOperation):
    pass


class shift_operation(IBinaryOperation):
    pass


class relational_operation(IBinaryOperation):
    pass


class equality_operation(IBinaryOperation):
    pass


class IFixedBinaryOperation(CommonListener):
    def init_interfaces(self):
        self.value = BinaryOperation(self.operator, self.children[0].value,
                                     self.children[1].value)


class bitwise_and_operation(IFixedBinaryOperation):
    operator = "&"


class bitwise_xor_operation(IFixedBinaryOperation):
    operator = "^"


class bitwise_or_operation(IFixedBinaryOperation):
    operator = "|"


class logical_and_operation(IFixedBinaryOperation):
    operator = "&&"


class logical_or_operation(IFixedBinaryOperation):
    operator = "||"


class conditional_operation(CommonListener):
    def init_interfaces(self):
        self.value = ConditionalOperation(self.children[0].value,
                                          self.children[1].value,
                                          self.children[2].value)


class IABISpecific:
    """
    Mixin that triggers a call to set_abi().

    This can be used either on a declaration or on a customised type.
    """

    def init_interfaces(self):
        super().init_interfaces()
        self.actions.append(Action(self.set_abi, 'add_abi_ref'))

    def set_abi(self, obj):
        if hasattr(self, 'definition'):
            obj.abi_refs.add(self.definition)
        elif hasattr(self, 'compound_type'):
            obj.abi_refs.add(self.compound_type)
        else:
            raise DSLError("Cannot set ABI", self)


class primitive_type(IABISpecific, CommonListener):
    def init_interfaces(self):
        self.type_name = self.children[0]
        self.compound_type = PrimitiveType(self.type_name)
        self.actions = [Action(self.set_type, "primitive_type", self)]
        super().init_interfaces()

    def set_type(self, declaration):
        declaration.compound_type = self.compound_type


class bitfield_type(CommonListener):
    def init_interfaces(self):
        self.type_name = self.children[0]
        self.compound_type = BitFieldType(self.type_name)
        self.actions = [Action(self.set_type, "bitfield_type")]

    def set_type(self, declaration):
        d = declaration
        d.compound_type = self.compound_type
        d.type_ref = d.compound_type
        d.is_customized_type = True


class structure_type(CommonListener):
    def init_interfaces(self):
        self.type_name = self.children[0]
        self.compound_type = StructureType(self.type_name)
        self.actions = [Action(self.set_type, "structure_type")]

    def set_type(self, declaration):
        d = declaration
        d.compound_type = self.compound_type
        d.type_ref = self.compound_type
        d.is_customized_type = True


class union_type(CommonListener):
    def init_interfaces(self):
        self.type_name = self.children[0]
        self.compound_type = UnionType(self.type_name)
        self.actions = [Action(self.set_type, "union_type")]

        if self.action_list.has("object_type_has_object"):
            raise DSLError("cannot declare an object type member in union",
                           self.declaration.member_name)

    def set_type(self, declaration):
        d = declaration
        d.compound_type = self.compound_type
        d.type_ref = self.compound_type
        d.is_customized_type = True


class enumeration_type(CommonListener):
    def init_interfaces(self):
        self.type_name = self.children[0]
        self.compound_type = EnumerationType(self.type_name)
        self.actions = [Action(self.set_type, "enumeration_type")]

    def set_type(self, declaration):
        d = declaration
        d.compound_type = self.compound_type
        d.type_ref = d.compound_type
        d.is_customized_type = True


class alternative_type(CommonListener):
    def init_interfaces(self):
        self.type_name = self.children[0]
        self.compound_type = AlternativeType(self.type_name)
        self.actions = [Action(self.set_type, "alternative_type")]

    def set_type(self, declaration):
        d = declaration
        d.compound_type = self.compound_type
        d.type_ref = self.compound_type
        d.is_customized_type = True


class direct_type(CommonListener):
    def init_interfaces(self):
        self.compound_type = DirectType()
        self.action_list.take(self.compound_type, "qualifier_list")
        self.actions = [Action(self.set_type, "direct_type")]

    def set_type(self, declaration):
        self.compound_type.set_basic_type(declaration.compound_type)
        declaration.compound_type = self.compound_type


class qualifier_list(CommonListener):
    def init_interfaces(self):
        self.qualifiers = set()
        al = [
            "basic_qualifier",
            "atomic_qualifier",
            "packed_qualifier",
            "aligned_qualifier",
            "group_qualifier",
            "contained_qualifier",
            "writeonly_qualifier",
            "lockable_qualifier",
            "optimize_qualifier",
        ]
        self.action_list.take_all(self, al)
        self.actions = [Action(self.set_qualifiers, "qualifier_list")]

    def set_qualifiers(self, obj):
        obj.qualifiers = self.qualifiers


class basic_qualifier(CommonListener):
    def init_interfaces(self):
        self.qualifier = Qualifier(self.children[0])
        self.actions = [Action(self.add_qualifier, "basic_qualifier")]

    def add_qualifier(self, obj):
        obj.qualifiers.add(self.qualifier)


class atomic_qualifier(CommonListener):
    def init_interfaces(self):
        self.qualifier = AtomicQualifier(self)
        self.actions = [Action(self.add_qualifier, "atomic_qualifier")]

    def add_qualifier(self, obj):
        obj.qualifiers.add(self.qualifier)


class packed_qualifier(CommonListener):
    def init_interfaces(self):
        self.qualifier = PackedQualifier(self)
        self.actions = [Action(self.add_qualifier, "packed_qualifier")]

    def add_qualifier(self, obj):
        obj.qualifiers.add(self.qualifier)


class aligned_qualifier(CommonListener):
    def init_interfaces(self):
        self.action_list.take(self, 'constant_expression')
        self.qualifier = AlignedQualifier(self, self.children[0].value)
        self.actions = [Action(self.add_qualifier, "aligned_qualifier")]

    def add_qualifier(self, obj):
        obj.qualifiers.add(self.qualifier)


class group_qualifier(CommonListener):
    def init_interfaces(self):
        self.qualifier = GroupQualifier(self, self.children)
        self.actions = [Action(self.add_qualifier, "group_qualifier")]

    def add_qualifier(self, obj):
        obj.qualifiers.add(self.qualifier)


class contained_qualifier(CommonListener):
    def init_interfaces(self):
        self.qualifier = ContainedQualifier(self)
        self.actions = [Action(self.add_qualifier, "contained_qualifier")]

    def add_qualifier(self, obj):
        obj.qualifiers.add(self.qualifier)


class writeonly_qualifier(CommonListener):
    def init_interfaces(self):
        self.qualifier = WriteonlyQualifier(self)
        self.actions = [Action(self.add_qualifier, "writeonly_qualifier")]

    def add_qualifier(self, obj):
        obj.qualifiers.add(self.qualifier)


class lockable_qualifier(CommonListener):
    def init_interfaces(self):
        self.qualifier = LockableQualifier(self)
        self.actions = [
            Action(self.add_qualifier, "lockable_qualifier"),
            Action(self.set_name, "describe_lockable_type"),
        ]

    def add_qualifier(self, obj):
        obj.qualifiers.add(self.qualifier)

    def set_name(self, obj):
        self.qualifier.resource_name = ' '.join((obj.type_name, obj.category))


class optimize_qualifier(CommonListener):
    def init_interfaces(self):
        self.qualifier = OptimizeQualifier(self)
        self.actions = [
            Action(self.add_qualifier, "optimize_qualifier"),
            Action(self.set_category, "describe_optimized_type"),
        ]

    def add_qualifier(self, obj):
        obj.qualifiers.add(self.qualifier)

    def set_category(self, obj):
        self.qualifier.category = obj.category


class array_size(CommonListener):
    def init_interfaces(self):
        self.value = self.children[0].value
        self.actions = [
            Action(self.set_size, "array_size"),
        ]

    def set_size(self, obj):
        obj.length = self.value


class array(CommonListener):
    def init_interfaces(self):
        self.compound_type = ArrayType(self)

        al = ["array_size", "qualifier_list", "object_type_set_complex"]
        self.action_list.take_all(self.compound_type, accept_list=al)

        self.actions = [Action(self.set_type, "array")]

    def set_type(self, declaration):
        a = self.compound_type
        a.base_type = declaration.compound_type
        declaration.compound_type = a
        declaration.complex_type = True


class pointer(IABISpecific, CommonListener):
    def init_interfaces(self):
        self.compound_type = PointerType(self)
        al = [
            "qualifier_list",
            "pointer_has_pointer",
            "object_type_set_complex"]
        self.action_list.take_all(self.compound_type, accept_list=al)

        self.actions = [
            Action(self.mark_has_pointer, "pointer_has_pointer"),
            Action(self.set_type, "pointer")
        ]

        # Pointers to objects hide an object
        rl = ["object_type_has_object"]
        self.action_list.remove_all(rl)

        super().init_interfaces()

    def set_type(self, declaration):
        self.compound_type.base_type = declaration.compound_type
        declaration.compound_type = self.compound_type
        declaration.complex_type = True

    def mark_has_pointer(self, pointer):
        pass


class declaration(CommonListener):
    def init_interfaces(self):
        # special case to handle object type declaration
        d = self.action_list.take(self, "object_type_create_declaration")
        if d is None:
            self.declaration = PrimitiveDeclaration()
        else:
            self.declaration = d
            self.action_list.take(self.declaration, 'object_noprefix')

        self.declaration.member_name = self.children[0]
        al = ["direct_type", "array", "pointer", "alias", "primitive_type",
              "bitfield_type", "structure_type", "union_type", "object_type",
              "enumeration_type", "alternative_type", "declaration_offset"]
        self.action_list.take_all(self.declaration, al)

        rl = ["pointer_has_pointer", "object_type_set_complex"]
        self.action_list.remove_all(rl)

        self.actions = [Action(self.set_declaration, "declaration")]

    def set_declaration(self, obj):
        obj.declarations.append(self.declaration)
        self.declaration.owner = obj
        if self.declaration.type_ref is not None:
            obj.type_refs.append(self.declaration.type_ref)


class declaration_offset(CommonListener):
    def init_interfaces(self):
        self.actions = [
            Action(self.set_offset, "declaration_offset"),
        ]

    def set_offset(self, e):
        e.offset = self.children[0].value


class enumeration_expr(CommonListener):
    def init_interfaces(self):
        self.actions = [
            Action(self.set_value, "enumeration_expr"),
        ]

    def set_value(self, e):
        e.value = self.children[0].value


class enumeration_noprefix(CommonListener):
    def init_interfaces(self):
        self.actions = [
            Action(self.set_noprefix, "enumeration_noprefix"),
            Action(self.set_noprefix, "enumeration_attribute"),
        ]

    def set_noprefix(self, e):
        e.prefix = ''


class enumeration_explicit(CommonListener):
    def init_interfaces(self):
        self.actions = [
            Action(self.set_explicit, "enumeration_attribute"),
        ]

    def set_explicit(self, e):
        e.set_explicit()


class enumeration_constant(CommonListener):
    def init_interfaces(self):
        name = self.children[0]
        self.constant = EnumerationConstant(name)

        self.action_list.take_all(self.constant,
                                  ["enumeration_noprefix",
                                   "enumeration_expr"])
        self.action_list.remove_all(["enumeration_attribute"])

        self.actions = [
            Action(self.add_constant, "enumeration_constant"),
        ]

    def add_constant(self, d):
        d.add_enumerator(self.constant)


class bitfield_width(CommonListener):
    def init_interfaces(self):
        self.actions = [
            Action(self.get_width, "bitfield_width"),
        ]

    def get_width(self, bit):
        bit.width = self.children[0].value


class bitfield_bit_range(CommonListener):
    def init_interfaces(self):
        self.actions = [
            Action(self.add_range, "bitfield_bit_range"),
        ]
        assert (len(self.children) <= 2)

    def add_range(self, specifier):
        specifier.add_bit_range(self)

    def get_bits(self):
        bit = int(self.children[0].value)
        if len(self.children) == 1:
            width = 1
        else:
            msb = bit
            bit = int(self.children[1].value)
            width = msb - bit + 1
            if width < 1:
                raise DSLError("invalid bitifield specfier", self.children[1])
        return (bit, width)


class bitfield_auto(CommonListener):
    def init_interfaces(self):
        self.width = None
        self.actions = [
            Action(self.set_bitfield_auto, "bitfield_auto"),
        ]
        self.action_list.take(self, "bitfield_width")

    def set_bitfield_auto(self, specifier):
        specifier.set_type_auto(self.width)


class bitfield_bit_span(CommonListener):
    def init_interfaces(self):
        self.width = None
        self.actions = [
            Action(self.add_range, "bitfield_bit_span"),
        ]
        self.action_list.take(self, "bitfield_width")

    def add_range(self, specifier):
        specifier.add_bit_range(self)

    def get_bits(self):
        return (int(self.children[-1].value), int(self.width))


class bitfield_others(CommonListener):
    def init_interfaces(self):
        self.actions = [
            Action(self.set_bitfield_others, "bitfield_others"),
        ]

    def set_bitfield_others(self, specifier):
        specifier.set_type_others()


class bitfield_delete(CommonListener):
    def init_interfaces(self):
        self.actions = [
            Action(self.bitfield_delete, "bitfield_delete"),
        ]

    def bitfield_delete(self, ext):
        ext.add_delete_member(self.children[0])


class bitfield_specifier(CommonListener):
    def init_interfaces(self):
        self.actions = [
            Action(self.set_specifier, "bitfield_specifier"),
        ]
        self.bitfield_specifier = BitFieldSpecifier()

        al = ["bitfield_bit_range", "bitfield_auto", "bitfield_bit_span",
              "bitfield_others"]
        self.action_list.take_all(self.bitfield_specifier, al)

    def set_specifier(self, declaration):
        declaration.bitfield_specifier = self.bitfield_specifier


class bitfield_unknown(CommonListener):
    def init_interfaces(self):
        self.actions = [
            Action(self.set_bitfield_member, "bitfield_member"),
        ]

    def set_bitfield_member(self, declaration):
        declaration.member_name = self.children[0]
        declaration.set_ignored()


class bitfield_member(CommonListener):
    def init_interfaces(self):
        self.name = self.children[0]

        self.actions = [
            Action(self.set_bitfield_member, "bitfield_member"),
        ]

    def set_bitfield_member(self, declaration):
        declaration.member_name = self.name


class bitfield_const(CommonListener):
    def init_interfaces(self):
        self.actions = [
            Action(self.set_bitfield_member, "bitfield_member"),
        ]

    def set_bitfield_member(self, declaration):
        declaration.member_name = "<const>"


class bitfield_shift(CommonListener):
    def init_interfaces(self):
        self.actions = [
            Action(self.set_bitfield_shift, "bitfield_shift"),
        ]

    def set_bitfield_shift(self, declaration):
        return self.children[0].value


class bitfield_default(CommonListener):
    def init_interfaces(self):
        self.action_list.take(self, 'constant_expression')
        self.default = self.children[0]
        self.actions = [
            Action(self.set_default, 'bitfield_default'),
        ]

    def set_default(self, obj):
        obj.default = self.default


class bitfield_declaration(CommonListener):
    def init_interfaces(self):
        if self.action_list.has("bitfield_delete"):
            return

        self.declaration = BitFieldDeclaration()

        shift = self.action_list.take(self.declaration, "bitfield_shift")

        al = ["bitfield_member", "bitfield_specifier", "bitfield_default",
              "direct_type", "primitive_type", "bitfield_type",
              "enumeration_type", "alternative_type", "structure_type",
              "union_type", "pointer"]

        self.action_list.take_all(self.declaration, al)

        if self.action_list.has("object_type_has_object"):
            raise DSLError("cannot declare an object type member in bitfield",
                           self.declaration.member_name)

        if shift:
            if self.action_list.has("pointer_has_pointer"):
                raise DSLError(
                    "cannot specify shift for pointer member in bitfield",
                    self.declaration.member_name)
            self.declaration.bitfield_specifier.set_type_shift(shift)

        rl = ["pointer_has_pointer"]
        self.action_list.remove_all(rl)

        self.actions = [Action(self.set_declaration, "bitfield_declaration")]

    def set_declaration(self, definition):
        definition.declarations.append(self.declaration)
        if self.declaration.type_ref is not None:
            definition.type_refs.append(self.declaration.type_ref)


class public(CommonListener):
    def init_interfaces(self):
        self.actions = [
            Action(self.set_public, self.__class__.__name__)
        ]

    def set_public(self, definition):
        definition.set_public()


class constant_definition(CommonListener):
    def init_interfaces(self):
        self.name = self.children[0]
        self.action_list.take(self, 'constant_expression')

        d = ConstantDefinition(self.name, self.children[-1].value)
        self.definition = d

        al = ["direct_type", "array", "pointer", "alias",
              "primitive_type", "bitfield_type", "structure_type",
              "union_type", "object_type", "enumeration_type",
              "alternative_type", "public"]
        self.action_list.take_all(self.definition, al)

        rl = ["pointer_has_pointer", "object_type_set_complex"]
        self.action_list.remove_all(rl)

        self.actions = [
            Action(self.set_definition, self.__class__.__name__),
            Action(self.set_type_ref, 'add_type_ref'),
        ]

    def set_definition(self, obj):
        obj.definitions.append(self.definition)

    def set_type_ref(self, obj):
        if self.definition.type_ref is not None:
            obj.type_refs.append(self.definition.type_ref)


class global_definition(CommonListener):
    def init_interfaces(self):
        self.name = self.children[0]

        d = GlobalDefinition(self.name)
        self.definition = d

        al = ["direct_type", "array", "pointer", "alias",
              "primitive_type", "bitfield_type", "structure_type",
              "union_type", "object_type", "enumeration_type",
              "alternative_type"]
        self.action_list.take_all(self.definition, al)

        rl = ["pointer_has_pointer", "object_type_set_complex"]
        self.action_list.remove_all(rl)

        self.actions = [
            Action(self.set_definition, self.__class__.__name__),
            Action(self.set_type_ref, 'add_type_ref'),
        ]

    def set_definition(self, obj):
        obj.definitions.append(self.definition)

    def set_type_ref(self, obj):
        if self.definition.type_ref is not None:
            obj.type_refs.append(self.definition.type_ref)


class ITypeDefinition(CommonListener):
    def init_interfaces(self):
        self.name = self.children[0]
        self.definition = None
        self.actions = [Action(self.set_definition, self.__class__.__name__)]

    def set_definition(self, obj):
        obj.definitions.append(self.definition)
        obj.type_refs += self.definition.type_refs


class alternative_definition(ITypeDefinition):
    def init_interfaces(self):
        super().init_interfaces()

        # must have "_t" postfix
        name = self.name
        if name[-2:] != "_t":
            raise DSLError("Invalid type name.\n"
                           "Type name must have _t as postfix ", name)

        d = AlternativeDefinition(name)
        self.definition = d

        al = ["direct_type", "array", "pointer",
              "primitive_type", "bitfield_type", "structure_type",
              "object_type", "union_type", "enumeration_type", "public"]
        self.action_list.take_all(self.definition, al)

        # special case, should have only 1 type ref
        if d.type_ref is not None:
            d.type_refs.append(d.type_ref)

        rl = ["pointer_has_pointer"]
        self.action_list.remove_all(rl)


class bitfield_const_decl(CommonListener):
    def init_interfaces(self):
        super().init_interfaces()
        self.actions = [Action(self.set_const, "bitfield_const_decl")]

    def set_const(self, obj):
        if obj.const:
            # TODO: proper logger warnings
            print("Warning: redundant bitfield const")
        obj.const = True


class bitfield_set_ops_decl(CommonListener):
    def init_interfaces(self):
        super().init_interfaces()
        self.actions = [Action(self.set_has_set_ops, "bitfield_set_ops_decl")]

    def set_has_set_ops(self, obj):
        obj.has_set_ops = True


class bitfield_definition(ITypeDefinition):
    def init_interfaces(self):
        super().init_interfaces()

        if self.action_list.has("bitfield_delete"):
            raise DSLError(
                "delete only allowed in extend", self.name)

        self.size = -1
        self.const = False

        d = BitFieldDefinition(self.name)
        self.definition = d
        self.action_list.take_all(d, [
            'bitfield_declaration', 'bitfield_size', 'bitfield_const_decl',
            'bitfield_set_ops_decl', 'public'])
        d.update_unit_info()


class structure_definition(IABISpecific, ITypeDefinition):
    def init_interfaces(self):
        super().init_interfaces()

        d = StructureDefinition(self.name)
        self.definition = d

        self.action_list.take(d, "declaration", single=False)
        self.action_list.take(d, "qualifier_list")
        self.action_list.take(d, "public")
        self.action_list.take(d, "describe_lockable_type")
        self.action_list.take(d, "describe_optimized_type")


class union_definition(ITypeDefinition):
    def init_interfaces(self):
        super().init_interfaces()

        d = UnionDefinition(self.name)
        self.definition = d

        self.action_list.take(d, "declaration", single=False)
        self.action_list.take(d, "qualifier_list")
        self.action_list.take(d, "public")
        self.action_list.take(d, "describe_lockable_type")


class enumeration_definition(IABISpecific, ITypeDefinition):
    def init_interfaces(self):
        super().init_interfaces()

        d = EnumerationDefinition(self.name)
        self.definition = d
        self.action_list.take(d, "enumeration_attribute")
        self.action_list.take(d, "public")
        # Unused
        self.action_list.remove_all(["enumeration_noprefix"])

        self.action_list.take(d, "enumeration_constant", single=False)


class object_definition(IABISpecific, ITypeDefinition):
    def init_interfaces(self):
        super().init_interfaces()

        d = ObjectDefinition(self.name)
        self.definition = d

        self.action_list.take(d, "declaration", single=False)
        self.action_list.take(d, "qualifier_list")
        self.action_list.take(d, "public")
        self.action_list.take(d, "describe_lockable_type")
        self.action_list.take(d, "describe_optimized_type")

        rl = ["object_type_has_object"]
        self.action_list.remove_all(rl)


class module_name(CommonListener):
    def init_interfaces(self):
        self.module_name = self.children[0]
        self.actions = [Action(self.set_name, "module_name")]

    def set_name(self, extension):
        extension.module_name = self.module_name


class bitfield_size(CommonListener):
    def init_interfaces(self):
        self.actions = [Action(self.set_size, "bitfield_size")]

    def set_size(self, obj):
        obj.length = int(self.children[0].value)


class ITypeExtension(CommonListener):
    def init_interfaces(self):
        # need to check definition if allowed it in the feature
        self.type_name = self.children[0]
        self.extension = None
        self.actions = [Action(self.set_extension, self.__class__.__name__)]

    def set_extension(self, parent):
        parent.type_refs.append(self.extension)
        parent.type_refs += self.extension.type_refs


class bitfield_extension(ITypeExtension):
    def init_interfaces(self):
        super().init_interfaces()

        e = BitFieldExtension(self.type_name)
        self.extension = e

        self.action_list.take(e, "bitfield_delete", single=False)

        al = ["bitfield_declaration", "module_name"]
        self.action_list.take_all(e, al)


class structure_extension(ITypeExtension):
    def init_interfaces(self):
        super().init_interfaces()

        e = StructureExtension(self.type_name)
        self.extension = e

        al = ["declaration", "module_name"]
        self.action_list.take_all(e, al)


class object_extension(ITypeExtension):
    def init_interfaces(self):
        super().init_interfaces()

        e = ObjectExtension(self.type_name)
        self.extension = e

        rl = ["object_type_has_object"]
        self.action_list.remove_all(rl)

        al = ["declaration", "module_name"]
        self.action_list.take_all(e, al)


class union_extension(ITypeExtension):
    def init_interfaces(self):
        super().init_interfaces()

        e = UnionExtension(self.type_name)
        self.extension = e

        if self.action_list.has("object_type_has_object"):
            raise DSLError("cannot declare an object type member in union",
                           self.declaration.member_name)

        al = ["declaration", "module_name"]
        self.action_list.take_all(e, al)


class enumeration_extension(ITypeExtension):
    def init_interfaces(self):
        super().init_interfaces()

        e = EnumerationExtension(self.type_name)
        self.extension = e

        al = ["enumeration_constant", "module_name"]
        self.action_list.take_all(e, al)


class object_type(CommonListener):
    def init_interfaces(self):
        self.type_name = self.children[-1]
        self.compound_type = ObjectType(self.type_name)

        self.actions = [
            Action(self.set_type, "object_type"),
            Action(self.create_declaration, "object_type_create_declaration"),
            Action(self.has_object, "object_type_has_object"),
            Action(self.set_complex, "object_type_set_complex"),
        ]

    def create_declaration(self, obj):
        d = ObjectDeclaration()
        d.type_ref = self.compound_type
        return d

    def set_type(self, declaration):
        declaration.compound_type = self.compound_type
        declaration.type_ref = declaration.compound_type
        declaration.is_customized_type = True

    def has_object(self, obj):
        return True

    def set_complex(self, obj):
        self.compound_type.complex_type = True


class object_noprefix(CommonListener):
    def init_interfaces(self):
        self.actions = [Action(self.set_noprefix, 'object_noprefix')]

    def set_noprefix(self, obj):
        obj.noprefix = True

```

`tools/typed/exceptions.py`:

```py
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# 2019 Cog Systems Pty Ltd.
#
# SPDX-License-Identifier: BSD-3-Clause

import re


class BaseError(Exception):
    def get_context(self, text, pos, span=40):
        start = max(pos - span, 0)
        end = pos + span
        before = text[start:pos].rsplit('\n', 1)[-1]
        after = text[pos:end].split('\n', 1)[0]
        before_spaces = re.sub(r'\S', ' ', before)
        return before + after + '\n' + before_spaces + '^\n'


class DSLError(BaseError):
    def __init__(self, message, token, expecting=None, state=None):
        meta = getattr(token, 'meta', token)
        self.line = getattr(meta, 'line', '?')
        self.column = getattr(meta, 'column', '?')
        self.pos_in_stream = getattr(meta, 'start_pos',
                                     getattr(meta, 'pos_in_stream', None))
        self.expecting = expecting
        self.state = state

        message = "\nError: %s\n" % message
        message += "\nToken %s, at line %s col %s:\n" % (
            str(token), self.line, self.column)
        if isinstance(self.pos_in_stream, int):
            message += '\n' + self.get_context(token.program,
                                               self.pos_in_stream)
        if expecting:
            message += '\nExpecting: %s\n' % expecting

        super(DSLError, self).__init__(message)


class DSLErrorWithRefs(BaseError):
    def __init__(self, message, token, refs, expecting=None, state=None):
        meta = getattr(token, 'meta', token)
        self.line = getattr(meta, 'line', '?')
        self.column = getattr(meta, 'column', '?')
        self.pos_in_stream = getattr(meta, 'start_pos',
                                     getattr(meta, 'pos_in_stream', None))
        self.expecting = expecting
        self.state = state

        message = "\nError: %s\n" % message
        message += "\nAt line %d col %d:\n" % (self.line, self.column)
        message += '\n' + self.get_context(token.program, self.pos_in_stream)

        message += "\nRefs:"
        for r in refs:
            line = getattr(r, 'line', '?')
            column = getattr(r, 'column', '?')
            pos = getattr(r, 'pos_in_stream', None)
            message += "\nAt line %d col %d:\n" % (line, column)
            message += '\n' + self.get_context(token.program, pos)

        if expecting:
            message += '\nExpecting: %s\n' % expecting

        super(DSLErrorWithRefs, self).__init__(message)


class RangeError(DSLError):
    pass

```

`tools/typed/ir.py`:

```py
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# 2019 Cog Systems Pty Ltd.
#
# SPDX-License-Identifier: BSD-3-Clause

import inspect
import math
import itertools
import os
import abc
from Cheetah.Template import Template
from exceptions import DSLError
from collections import namedtuple
from functools import wraps

from lark import Transformer, Tree, Token, Discard

import tempfile
import Cheetah.ImportHooks

td = tempfile.TemporaryDirectory()
Cheetah.ImportHooks.setCacheDir(td.name)

"""
The classes in the module represent the features of the DSL language.  They
form an intermediate representation and are used to generate the output.  An
instance of TopLevel will contain all information necessary to generate the
output.
"""
__loc__ = os.path.realpath(
    os.path.join(
        os.getcwd(),
        os.path.dirname(__file__)))

default_copyright = \
    "© 2021 Qualcomm Innovation Center, Inc. All rights reserved.\n" \
    "SPDX-License-Identifier: BSD-3-Clause"


def property_autoupdate(f):
    @wraps(f)
    def wrapper(self):
        if not self._updated:
            self._autoupdate()
        return f(self)

    return property(wrapper)


class TopLevel:
    def __init__(self):
        self.declarations = []
        self.definitions = []
        # record if need to link type to it's definition
        self.type_refs = []
        self.constant_refs = []
        # record the need to call set_abi() on the type
        self.abi_refs = set()

    def gen_output(self, public_only=False):
        footer = []
        code = []
        extra = []

        sorted_defs = []
        seen = set()

        def visit(d):
            if d in seen:
                return
            seen.add(d)
            for dep in sorted(d.dependencies, key=lambda x: x.indicator):
                visit(dep)
            sorted_defs.append(d)

        # Sort, ensuring that dependencies come before dependent definitions
        for d in sorted(self.definitions, key=lambda x: x.indicator):
            visit(d)
        assert all(d in self.definitions for d in sorted_defs)
        assert all(d in sorted_defs for d in self.definitions)

        for d in sorted_defs:
            if public_only and not d.is_public:
                continue
            f = d.gen_forward_decl()
            code += f

        if self.definitions:
            code += '\n'

        for d in itertools.chain(sorted_defs, self.declarations):
            if public_only and not d.is_public:
                continue
            c, e = d.gen_code()
            code += c
            extra += e

        # FIXME: move to a template file
        output = "// Automatically generated. Do not modify.\n//\n"
        output += '// ' + '\n// '.join(default_copyright.split('\n')) + '\n\n'
        output += "\n"
        output += "#include <stddef.h>\n"
        output += "#include <stdint.h>\n"
        output += "#include <stdbool.h>\n"
        output += "#include <stdalign.h>\n"
        output += "#include <stdatomic.h>\n"
        # output += "#include <assert.h>\n"
        # output += "#include <string.h>\n"
        output += "#include <stdnoreturn.h>\n"
        output += "\n"
        output += ' '.join(code)
        output += "\n"
        if public_only:
            output += "#include <guest_hypresult.h>\n"
        else:
            output += "#include <hypresult.h>\n"
        output += "\n"
        output += ' '.join(extra)
        output += ' '.join(footer)

        return output

    def get_output_templates(self):
        templates = set()

        # Sort, ensuring that dependencies come before dependent definitions
        for d in self.definitions:
            deps = d.get_template_deps()
            for t in deps:
                templates.add(t)

        return list(templates)

    def apply_template(self, template_file, public_only=False):
        ns = [{
            'declarations': tuple(d for d in self.declarations
                                  if d.is_public or not public_only),
            'definitions': tuple(d for d in self.definitions
                                 if d.is_public or not public_only),
            'primitives': tuple(PrimitiveType(t) for t in
                                PrimitiveType.c_type_names),
            'public_only': public_only,
        }]
        template = Template(file=template_file, searchList=ns)
        return str(template)

    def merge(self, t):
        # TODO: need to handle all declaration type reference, especially
        # objection
        self.declarations += t.declarations
        self.definitions += t.definitions
        self.type_refs += t.type_refs
        self.constant_refs += t.constant_refs
        for a in t.abi_refs:
            self.abi_refs.add(a)

    def _handle_refs(self, abi):
        # FIXME: check for duplicates
        defs = {(d.type_name, d.category): d for d in self.definitions}

        # Set the ABI for types whose definition depends on it
        for t in self.abi_refs:
            t.set_abi(abi)

        # resolve type and constant references
        for r in itertools.chain(self.type_refs, self.constant_refs):
            k = (r.indicator, r.category)
            if k in defs:
                r.link(defs[k])
            else:
                raise DSLError("Failed to find corresponding definition for " +
                               r.indicator + ", category(" + r.category + ")",
                               r.indicator)

    def update(self, abi):
        """
        Second pass, handle internal information & setup connections between
        nodes inside.
        """
        # Add unreferenced primitives
        refs = set()
        for t in self.abi_refs:
            if t.indicator not in refs:
                refs.add(t.indicator)
        for t in PrimitiveType.c_type_names:
            if t not in refs:
                x = PrimitiveType(t)
                self.abi_refs.add(x)

        # link customised declarations and definitions
        self._handle_refs(abi)

        # trigger the update of definitions and declarations
        for d in itertools.chain(self.declarations, self.definitions):
            d.update()


class ICustomized:
    BITFIELD = "bitfield"
    OBJECT = "object"
    STRUCTURE = "structure"
    UNION = "union"
    ENUMERATION = "enumeration"
    ALTERNATIVE = "alternative"
    IMPORT = "import"
    CONSTANT = "constant"
    GLOBAL = "global"


class ICustomizedReference(ICustomized, metaclass=abc.ABCMeta):

    @abc.abstractmethod
    def link(self, definition):
        """
        Link the definition of the base type to this customized object.
        """
        raise NotImplementedError


class IConstantExpression(metaclass=abc.ABCMeta):
    """
    Interface for integer constant expressions.
    """

    def __init__(self):
        super().__init__()
        self._cache = None

    def __int__(self):
        if self._cache is None:
            self._cache = self.to_int()
        return self._cache

    def __reduce__(self):
        return (int, (int(self),))

    def __format__(self, format_spec):
        return int(self).__format__(format_spec)

    @abc.abstractmethod
    def to_int(self):
        """
        Convert the expression to a constant value.

        The result of this method is cached after it returns a value other
        than None.
        """
        raise NotImplementedError


class ConstantExpression(IConstantExpression):
    """
    Top-level constant expression.
    """

    def __init__(self, expr):
        super().__init__()
        self._cache = None
        self.expr = expr

    def to_int(self):
        return int(self.expr)


class ConstantReference(IConstantExpression, ICustomizedReference):
    """
    Constant reference.
    """

    def __init__(self, symbol_name):
        super().__init__()
        self.referenced = False
        self.symbol_name = symbol_name

    @property
    def indicator(self):
        return self.symbol_name

    def to_int(self):
        if self.referenced:
            raise DSLError("Definition of constant is self-referential",
                           self.indicator)
        self.referenced = True
        return int(self.expr)

    @property
    def category(self):
        return self.CONSTANT

    def link(self, definition):
        self.expr = definition.value


class UnaryOperation(IConstantExpression):
    """
    Apply a unary operator to a constant expression.
    """
    operator_map = {
        '+': lambda x: x,
        '-': lambda x: -x,
        '~': lambda x: ~x,
        '!': lambda x: int(x == 0),
    }

    def __init__(self, operator, expr):
        super().__init__()
        try:
            self.func = self.operator_map[operator]
        except KeyError:
            raise DSLError("Unhandled unary operator", self.operator)
        self.expr = expr

    def to_int(self):
        return self.func(int(self.expr))


class BinaryOperation(IConstantExpression):
    """
    Apply a binary operator to a constant expression.
    """
    operator_map = {
        '+': lambda x, y: x + y,
        '-': lambda x, y: x - y,
        '*': lambda x, y: x * y,
        '/': lambda x, y: x // y,
        '%': lambda x, y: x % y,
        '<<': lambda x, y: x << y,
        '>>': lambda x, y: x >> y,
        '<': lambda x, y: int(x < y),
        '>': lambda x, y: int(x > y),
        '<=': lambda x, y: int(x <= y),
        '>=': lambda x, y: int(x >= y),
        '==': lambda x, y: int(x == y),
        '!=': lambda x, y: int(x != y),
        '&': lambda x, y: x & y,
        '^': lambda x, y: x ^ y,
        '|': lambda x, y: x | y,
        '&&': lambda x, y: int(x and y),
        '||': lambda x, y: int(x or y),
    }

    def __init__(self, operator, left_expr, right_expr):
        super().__init__()
        try:
            self.func = self.operator_map[operator]
        except KeyError:
            raise DSLError("Unhandled binary operator", self.operator)
        self.left_expr = left_expr
        self.right_expr = right_expr

    def to_int(self):
        return self.func(int(self.left_expr), int(self.right_expr))


class ConditionalOperation(IConstantExpression):
    """
    Apply a conditional (ternary) operator to a constant expression.
    """

    def __init__(self, cond_expr, true_expr, false_expr):
        super().__init__()
        self.cond_expr = cond_expr
        self.true_expr = true_expr
        self.false_expr = false_expr

    def to_int(self):
        cond = int(self.cond_expr) != 0
        return int(self.true_expr) if cond else int(self.false_expr)


class TypePropertyOperation(IConstantExpression):
    """
    A constant expression evaluating to some integer property of a type.
    """

    def __init__(self, compound_type):
        super().__init__()
        self.compound_type = compound_type

    def to_int(self):
        try:
            return getattr(
                self.compound_type.basic_type.definition,
                self._type_property)
        except AttributeError:
            return getattr(self.compound_type, self._type_property)


class SizeofOperation(TypePropertyOperation):
    _type_property = 'size'


class AlignofOperation(TypePropertyOperation):
    _type_property = 'alignment'


class MinofOperation(TypePropertyOperation):
    _type_property = 'minimum_value'


class MaxofOperation(TypePropertyOperation):
    _type_property = 'maximum_value'


class PureFunctionCall(IConstantExpression):
    """
    A constant expression that applies a pure function to another expression.
    """

    def __init__(self, expr, f):
        super().__init__()
        self.expr = expr
        self.f = f

    def to_int(self):
        return self.f(int(self.expr))


class IType(metaclass=abc.ABCMeta):
    """
    Interface for all types.
    """

    def __init__(self):
        super().__init__()
        self.qualifiers = set()

    @abc.abstractproperty
    def indicator(self):
        """
        Return the AST node that names the type, for use in error messages.
        """
        raise NotImplementedError

    @abc.abstractproperty
    def is_public(self):
        """
        True if this type is exposed in the public API.
        """
        raise NotImplementedError

    def set_const(self):
        """
        Add a const qualifier, if none already exists.

        This is used on members of aggregate types, which can inherit the
        qualifier from the aggregate.
        """
        if self.is_writeonly:
            raise DSLError(
                "Can't have a constant type with a writeonly member",
                self.name)
        self.qualifiers.add(Qualifier("const"))

    def gen_forward_decl(self):
        """
        Generates a forward declaration (if needed) for the type.
        """
        return ([])

    def _gen_type(self, unqualified=False):
        """
        Construct a C type name or declaration.

        This returns a tuple of two lists. The first list contains tokens that
        should appear to the left of the identifier if this type name is used
        as a declaration; the second list contains tokens that should appear
        to the right of the identifier.

        For example, to declare a constant pointer to an array of 8 atomic
        pointers to volatile integers:

            volatile int *_Atomic (*const a)[8];

        The returned value will be:

            (['volatile', 'int', '*', '_Atomic', '(*', 'const'],
             [')', '[', '8', ']'])

        If unqualified is true, the top-level type's qualifiers will be
        omitted, which is necessary when it is used as the return value of
        a field accessor function.
        """
        raise NotImplementedError

    def gen_declaration(self, identifier):
        """
        Construct a C declaration of an object of this type.

        This returns a declaration (excluding any final ; or linebreak) for
        an object of this type with the specified identifier.
        """
        l, r = self._gen_type()
        return ' '.join(itertools.chain(l, [identifier], r))

    def gen_type_name(self, unqualified=False):
        """
        Construct a C type name for this type.

        This returns the type name (type-name in the C grammar), which is used
        for casts, sizeof, alignof, the parenthesised form of _Atomic,
        _Generic, and initialisers.
        """
        l, r = self._gen_type(unqualified=unqualified)
        return ' '.join(l) + ' '.join(r)

    @property
    def basic_type(self):
        return self

    @property
    def has_suffix_declarator(self):
        """
        True if a type declarator appears to the right of the identifier.

        Type specifiers and pointer type declarators appear to the left of the
        identifier. Array and function declarators appear to the right of the
        identifier.
        """
        return self.is_array

    @property
    def bitsize(self):
        """
        The size of this type's value in bits.

        Returns None if the true range of the type is not known, which is the
        case for all scalar types other than booleans, enumerations, and
        non-void pointers.

        Implemented only for scalar types (including pointers).
        """
        raise DSLError("Non-scalar type cannot be used in this context",
                       self.indicator)

    @property
    def minimum_value(self):
        """
        The minimum scalar value of this type.

        Implemented only for scalar types.
        """
        bits = self.size * 8 if self.bitsize is None else self.bitsize
        return -1 << (bits - 1) if self.is_signed else 0

    @property
    def maximum_value(self):
        """
        The maximum scalar value of this type.

        Implemented only for scalar types.
        """
        bits = self.size * 8 if self.bitsize is None else self.bitsize
        return (1 << (bits - (1 if self.is_signed else 0))) - 1

    @abc.abstractproperty
    def size(self):
        """
        The size of this type in bytes.
        """
        raise NotImplementedError

    @property
    def alignment(self):
        """
        The alignment of this type in bytes, after qualifiers are applied.
        """
        return max(
            (q.align_bytes for q in self.qualifiers if q.is_aligned),
            default=1 if self.is_packed else self.default_alignment)

    @abc.abstractproperty
    def default_alignment(self):
        """
        The alignment of this type in bytes, if not overridden by a qualifier.
        """
        raise NotImplementedError

    @property
    def is_const(self):
        """
        Return True if current type is a const type.
        """
        return any((q.is_const for q in self.qualifiers))

    @property
    def is_atomic(self):
        """
        Return True if current type is an atomic type.
        """
        return any((q.is_atomic for q in self.qualifiers))

    @property
    def is_writeonly(self):
        """
        Return True if current type is write-only.

        This is only applicable to bitfield members. It suppresses generation
        of a read accessor for the member.
        """
        return any((q.is_writeonly for q in self.qualifiers))

    @property
    def is_packed(self):
        """
        Return True if the current type is packed.

        This only makes sense for aggregate types and members in them.
        """
        return any((q.is_packed for q in self.qualifiers))

    @property
    def is_contained(self):
        """
        Return True if the container_of macro should be generated for the
        current type.
        """
        return any(q.is_contained for q in self.qualifiers)

    @property
    def is_ordered(self):
        """
        Return True if the type should be generated exactly as ordered in its
        definition.
        """
        raise NotImplementedError

    @property
    def is_signed(self):
        """
        For scalar types, returns whether the type is signed.

        This is not implemented for non-scalar types.
        """
        raise DSLError("Non-scalar type cannot be used in this context",
                       self.indicator)

    @property
    def accessor_basename(self):
        """
        The name prefix for this type's accessors, if any.

        For aggregate types which generate accessor functions, i.e.
        structures, objects and bitfields, this is the type name. For all
        other types, it is None.
        """
        return None

    @property
    def is_array(self):
        """
        True if this type is an array type.
        """
        return False

    @property
    def is_pointer(self):
        """
        True if this type is a pointer type.
        """
        return False

    @property
    def pointer(self):
        """
        A pointer type that points to this type.
        """
        return PointerType(base_type=self)

    @abc.abstractproperty
    def dependencies(self):
        """
        Return all definitions that this type relies on.

        Note, pointers and other non definitions are not considered
        dependencies as they can be forward declared.
        """
        raise NotImplementedError


class ICustomizedType(ICustomizedReference, IType):
    """
    Interface for customized type/symbol like structure, bitfield, object,
    constant, etc.
    """

    def __init__(self, type_suffix=''):
        super().__init__()
        self.definition = None
        self.category = "invalid"
        self.type_name = "invalid"
        # indicate this customized type is used as an array or pointer, so the
        # memory stores multiple element or other type of data instead of a
        # single customized type
        self.complex_type = False
        self.type_suffix = type_suffix

    @property
    def indicator(self):
        return self.type_name

# TODO: separate definitions from extensions list
# turn definitions into a dict, and assert there are no duplicates created
#    c_types = {}
#        if type_name in c_types:
#            raise DSLError("duplicate definition of...")
#
#        IType.c_types[type_name] = self

    def link(self, definition):
        """
        Link the definition with declaration
        """
        self.definition = definition

    @property
    def size(self):
        try:
            return self.definition.size
        except AttributeError:
            raise DSLError(self.type_name + " is not defined", self.type_name)

    @property
    def bitsize(self):
        try:
            return self.definition.bitsize
        except AttributeError:
            raise DSLError(self.type_name + " is not defined", self.type_name)

    @property
    def default_alignment(self):
        try:
            return self.definition.alignment
        except AttributeError:
            raise DSLError(self.type_name + " is not defined", self.type_name)

    @property
    def is_signed(self):
        try:
            return self.definition.is_signed
        except AttributeError:
            raise DSLError(self.type_name + " is not defined", self.type_name)

    @property
    def is_pointer(self):
        try:
            return self.definition.is_pointer
        except AttributeError:
            raise DSLError(self.type_name + " is not defined", self.type_name)

    @property
    def is_array(self):
        try:
            return self.definition.is_array
        except AttributeError:
            raise DSLError(self.type_name + " is not defined", self.type_name)

    @property
    def is_public(self):
        try:
            return self.definition.is_public
        except AttributeError:
            raise DSLError(self.type_name + " is not defined", self.type_name)

    def _gen_type(self, **_):
        return ([self.type_name + self.type_suffix], [])

    @property
    def dependencies(self):
        """
        Return all definitions that this type relies on.

        Note, pointers and other non definitions are not considered
        dependencies as they can be forward declared.
        """
        return (self.definition,)


class IGenCode(metaclass=abc.ABCMeta):
    """
    Interface for C code generation.

    The interface will return (code, extra).
    Code should contain all declarations and definitions. The extra contains
    all flattened information such as getters/setters
    """

    @abc.abstractmethod
    def gen_code(self):
        return ([], [])

    def get_template_deps(self):
        return []


class IAggregation:
    """
    Interface for definition/extension who has declarations & definitions
    """

    def __init__(self):
        super().__init__()
        self.declarations = []
        self.definitions = []
        self.type_refs = []

    def set_declarations(self, declarations):
        self.declarations = declarations

    def set_definitions(self, definitions):
        self.definitions = definitions


class IUpdate():
    """
    Interface for all class who needs the second pass scan
    """

    def update(self):
        """
        Update internal data, prepare to generate code
        """
        pass


class IExtension(ICustomizedReference, IAggregation):
    """
    Interface for extension of bit field or object
    """

    def __init__(self):
        super().__init__()
        self.prefix = None


class ICustomizedDefinition(ICustomized, IType, IAggregation, IUpdate):
    """
    Interface for all customized definition
    """

    def __init__(self):
        super().__init__()
        self.type_name = "invalid"
        self.type_link_cnt = 0
        self._public = False

    @property
    def is_public(self):
        return self._public

    def set_public(self):
        self._public = True

    @property
    def indicator(self):
        return self.type_name

    @property
    def is_container(self):
        """
        True if offset macros and container_of functions should be generated.
        """
        return False

    def gen_type_name(self, unqualified=False):
        """
        Construct a C type name for this type.
        """
        return self.type_name + '_t'


class IDeclaration(IUpdate):
    """
    Interface for all declarations, which add members to compound types.
    """

    BITFIELD = "bitfield"
    OBJECT = "object"
    PRIMITIVE = "primitive"

    SEPARATOR = "_"

    def __init__(self):
        super().__init__()
        self.category = self.PRIMITIVE
        self.member_name = None
        # FIXME: compound_type is badly named; it is really the declared type.
        self.compound_type = None
        # complex type indicates that the actually data stored by this
        # declaration is not just one single element of basic type, instead
        # it has multiple elements or different type of data saved in the
        # memory right now, there's only array and pointer make the type
        # complicated
        self.complex_type = False
        self.is_customized_type = False
        self.is_ignore = False

        # keep the type need to find corresponding definition
        self.type_ref = None

        # indicate which definition owns this declaration
        self.owner = None

    def set_ignored(self):
        assert (not self.is_ignore)
        self.is_ignore = True

    def set_const(self):
        self.compound_type.set_const()

    def get_members(self, prefix=None):
        """
        Return the list of members added to the enclosing aggregate.
        """
        return ((self._get_member_name(prefix), self.compound_type, self),)

    def _get_member_name(self, prefix):
        """
        Get the name of this member, given an optional prefix.
        """
        prefix = '' if prefix is None else prefix + self.SEPARATOR
        member_name = prefix + self.member_name
        return member_name


class PrimitiveType(IType):
    PRIMITIVE = "primitive"
    c_type_names = {
        'bool': 'bool',
        'uint8': 'uint8_t',
        'uint16': 'uint16_t',
        'uint32': 'uint32_t',
        'uint64': 'uint64_t',
        'uintptr': 'uintptr_t',
        'sint8': 'int8_t',
        'sint16': 'int16_t',
        'sint32': 'int32_t',
        'sint64': 'int64_t',
        'sintptr': 'intptr_t',
        'char': 'char',
        'size': 'size_t',
    }
    abi_type_names = {
        'uregister': 'uregister_t',
        'sregister': 'sregister_t',
    }

    def __init__(self, type_name):
        super().__init__()
        self.type_name = type_name
        self.category = self.PRIMITIVE
        assert type_name in itertools.chain(self.c_type_names,
                                            self.abi_type_names)
        if type_name in self.c_type_names:
            self.c_type_name = self.c_type_names[type_name]
        else:
            self.c_type_name = self.abi_type_names[type_name]

    @property
    def indicator(self):
        return self.type_name

    @property
    def is_public(self):
        return True

    def set_abi(self, abi):
        if self.type_name in self.abi_type_names:
            self.c_type_name = abi.get_c_type_name(self.c_type_name)
        ctype = abi.get_c_type(self.c_type_name)
        self._is_signed = ctype.is_signed
        self._size = ctype.size
        self._align = ctype.align
        self._bitsize = ctype.bitsize

    def _gen_type(self, **_):
        return ([self.c_type_name], [])

    @property
    def size(self):
        return self._size

    @property
    def is_signed(self):
        return self._is_signed

    @property
    def default_alignment(self):
        return self._align

    @property
    def bitsize(self):
        return self._bitsize

    def __repr__(self):
        return "PrimitiveType<{:s}>".format(self.indicator)

    @property
    def dependencies(self):
        """
        Return all definitions that this type relies on.

        Note, pointers and other non definitions are not considered
        dependencies as they can be forward declared.
        """
        return ()


class BitFieldType(ICustomizedType):
    def __init__(self, type_name):
        super().__init__('_t')
        self.type_name = type_name
        self.category = self.BITFIELD

    @property
    def accessor_basename(self):
        """
        The name prefix for this type's accessors, if any.

        For aggregate types which generate accessor functions, i.e.
        structures, objects and bitfields, this is the type name. For all
        other types, it is None.
        """
        return self.type_name


class ObjectType(ICustomizedType):
    def __init__(self, type_name):
        super().__init__('_t')
        self.type_name = type_name
        self.category = self.OBJECT

    def link(self, definition):
        super().link(definition)
        if not self.complex_type and definition.type_link_cnt == 0:
            definition.need_export = False
        elif self.complex_type:
            definition.need_export = True
        definition.type_link_cnt += 1

    @property
    def accessor_basename(self):
        """
        The name prefix for this type's accessors, if any.

        For aggregate types which generate accessor functions, i.e.
        structures, objects and bitfields, this is the type name. For all
        other types, it is None.
        """
        return self.type_name


class StructureType(ICustomizedType):
    def __init__(self, type_name):
        super().__init__('_t')
        self.type_name = type_name
        self.category = self.STRUCTURE

    @property
    def accessor_basename(self):
        """
        The name prefix for this type's accessors, if any.

        For aggregate types which generate accessor functions, i.e.
        structures, objects and bitfields, this is the type name. For all
        other types, it is None.
        """
        return self.type_name


class UnionType(ICustomizedType):
    def __init__(self, type_name):
        super().__init__('_t')
        self.type_name = type_name
        self.category = self.UNION

    @property
    def accessor_basename(self):
        """
        The name prefix for this type's accessors, if any.

        For aggregate types which generate accessor functions, i.e.
        structures, objects and bitfields, this is the type name. For all
        other types, it is None.
        """
        return self.type_name


class EnumerationType(ICustomizedType):
    def __init__(self, type_name):
        super().__init__('_t')
        self.type_name = type_name
        self.category = self.ENUMERATION

    @property
    def accessor_basename(self):
        """
        The name prefix for this type's accessors, if any.

        For aggregate types which generate accessor functions, i.e.
        structures, objects and bitfields, this is the type name. For all
        other types, it is None.
        """
        return self.type_name


class AlternativeType(ICustomizedType):
    def __init__(self, type_name):
        super().__init__()
        self.type_name = type_name
        self.category = self.ALTERNATIVE


FieldMap = namedtuple('FieldMap', ['field_bit', 'mapped_bit', 'length'])


class BitFieldMemberMapping:
    """
    Data structure that encodes member's bit mapping to the bitfield
    """

    def __init__(self, shift):
        self.field_shift = shift
        self.field_signed = False
        self.field_maps = []

    def set_signed(self, signed=True):
        self.field_signed = signed

    def add_bit_range(self, field_bit, mapped_bit, length):
        assert int(field_bit) >= 0
        fmap = FieldMap(field_bit, mapped_bit, length)
        self.field_maps.append(fmap)

    def update(self, unit_size):
        """compact field_maps"""
        i = 0
        try:
            while True:
                a = self.field_maps[i]
                b = self.field_maps[i + 1]
                assert a.field_bit + a.length == b.field_bit
                if (a.mapped_bit // unit_size == b.mapped_bit // unit_size) \
                        and (a.mapped_bit + a.length == b.mapped_bit):
                    assert a.field_bit >= 0
                    c = FieldMap(a.field_bit, a.mapped_bit,
                                 a.length + b.length)
                    self.field_maps[i] = c
                    del self.field_maps[i + 1]
                else:
                    i += 1
        except IndexError:
            pass

    def __repr__(self):
        ret = "BitFieldMemberMapping<"
        sep = ''
        for x in self.field_maps:
            ret += "{:s}({:d},{:d},{:d})".format(sep, x.field_bit,
                                                 x.mapped_bit, x.length)
            sep = ','
        ret += ">"
        return ret


class BitFieldSpecifier:
    NONE = 0
    RANGE = 1
    OTHERS = 2
    AUTO = 3

    def __init__(self):
        self.specifier_type = self.NONE
        self.bit_length = None
        self.shift = 0
        self.auto_width = None
        self._bit_ranges = []
        self.mapping = None

    def add_bit_range(self, bit_range):
        assert (self.specifier_type in (self.NONE, self.RANGE))
        self.specifier_type = self.RANGE
        self._bit_ranges.append(bit_range)

    def set_type_shift(self, shift):
        assert (self.specifier_type in (self.RANGE, self.AUTO))
        self.shift = shift

    def set_type_auto(self, width=None):
        assert (self.specifier_type is self.NONE)
        self.specifier_type = self.AUTO
        self.auto_width = width

    def set_type_others(self):
        assert (self.specifier_type is self.NONE)
        self.specifier_type = self.OTHERS

    @property
    def bit_ranges(self):
        return tuple(r.get_bits() for r in self._bit_ranges)

    @property
    def range_width(self):
        assert self.specifier_type is self.RANGE
        bits = 0
        for bit, width in self.bit_ranges:
            bits += width
        return bits

    def update_ranges(self, declaration, physical_ranges, unit_size):
        shift = int(self.shift)
        self.mapping = BitFieldMemberMapping(shift)

        # FIXME - reserved members defaults need to be considered
        #       - extended registers need to have reserved ranges / defaults
        #         recalculated
        # if declaration.is_ignore:
        #    print(" - skip", declaration.member_name)
        #    return

        # Split up any ranges that cross unit boundaries
        split_ranges = []
        for bit, width in self.bit_ranges:
            while (bit // unit_size) != (bit + width - 1) // unit_size:
                split_point = ((bit // unit_size) + 1) * unit_size
                split_width = split_point - bit
                split_ranges.append((bit, split_width))
                bit = split_point
                width -= split_width
            split_ranges.append((bit, width))

        if self.specifier_type is self.RANGE:
            field_bit = shift

            for bit, width in reversed(split_ranges):
                if not physical_ranges.insert_range((bit, width), declaration):
                    raise DSLError("bitfield member conflicts with previously "
                                   "specified bits, freelist:\n" +
                                   str(physical_ranges),
                                   declaration.member_name)
                self.mapping.add_bit_range(field_bit, bit, width)
                field_bit += width
            self.mapping.update(unit_size)
            self.bit_length = field_bit

    def set_signed(self, signed):
        self.mapping.set_signed(signed)


class DirectType(IType):
    def __init__(self):
        super().__init__()
        self._basic_type = None

    def _gen_type(self, unqualified=False):
        l, r = self._basic_type._gen_type()
        ql = []
        if not unqualified:
            for q in self.qualifiers:
                if q.is_restrict:
                    raise DSLError("Restrict qualifier is only for pointer",
                                   self._basic_type.indicator)
                ql.extend(q.gen_qualifier())
        return (ql + l, r)

    def set_basic_type(self, type):
        assert self._basic_type is None
        self._basic_type = type

    @property
    def category(self):
        return self._basic_type.category

    @property
    def type_name(self):
        return self._basic_type.type_name

    @property
    def definition(self):
        return self._basic_type.definition

    @property
    def indicator(self):
        return self._basic_type.indicator

    @property
    def basic_type(self):
        return self._basic_type

    @property
    def size(self):
        return self._basic_type.size

    @property
    def bitsize(self):
        return self._basic_type.bitsize

    @property
    def default_alignment(self):
        return self._basic_type.alignment

    @property
    def is_signed(self):
        return self._basic_type.is_signed

    @property
    def is_public(self):
        return self._basic_type.is_public

    @property
    def dependencies(self):
        """
        Return all definitions that this type relies on.

        Note, pointers and other non definitions are not considered
        dependencies as they can be forward declared.
        """
        return self._basic_type.dependencies


class ArrayType(IType):
    def __init__(self, indicator):
        super().__init__()
        self.length = None
        self.base_type = None
        self._indicator = indicator

    @property
    def indicator(self):
        return self._indicator

    def _gen_type(self, **_):
        l, r = self.base_type._gen_type()
        return (l, ["[{:d}]".format(self.length)] + r)

    @property
    def size(self):
        return int(self.length) * self.base_type.size

    @property
    def default_alignment(self):
        return self.base_type.alignment

    @property
    def is_array(self):
        """
        True if this type is an array type.
        """
        return True

    @property
    def is_public(self):
        return self.base_type.is_public

    @property
    def dependencies(self):
        """
        Return all other types that this type definition relies on.

        Note, pointers to types are not considered dependencies as they can be
        forward declared.
        """
        return self.base_type.dependencies


class PointerType(IType):
    def __init__(self, indicator=None, base_type=None):
        super().__init__()
        self.base_type = base_type
        self._indicator = indicator

    @property
    def indicator(self):
        return self._indicator

    def set_abi(self, abi):
        self._size = abi.pointer_size
        self._align = abi.pointer_align

    def _gen_type(self, unqualified=False):
        l, r = self.base_type._gen_type()

        if unqualified:
            ql = []
        else:
            ql = list(itertools.chain(*(
                q.gen_qualifier() for q in self.qualifiers)))

        if self.base_type.has_suffix_declarator:
            # Needs parentheses to bind the * on the left before the base
            # type's declarator on the right.
            return (l + ["(*"] + ql, [")"] + r)
        else:
            return (l + ["*"] + ql, r)

    @property
    def size(self):
        return self._size

    @property
    def bitsize(self):
        return (self.size * 8) - (self.base_type.alignment - 1).bit_length()

    @property
    def is_signed(self):
        return False

    @property
    def default_alignment(self):
        return self._align

    @property
    def is_pointer(self):
        return True

    @property
    def is_public(self):
        return self.base_type.is_public

    @property
    def dependencies(self):
        """
        Return all definitions that this type relies on.

        Note, pointers and other non definitions are not considered
        dependencies as they can be forward declared.
        """
        if self.base_type.is_atomic:
            # Clang requires atomic-qualified types to be complete, even when
            # taking a pointer to one. This is not clearly required by the
            # standard and seems to be a Clang implementation quirk.
            return self.base_type.dependencies
        return ()


class PrimitiveDeclaration(IDeclaration):
    """
    Declaration of a single member in a structure or object.
    """

    def __init__(self):
        super().__init__()
        self.category = self.PRIMITIVE
        self.offset = None


class ObjectDeclaration(PrimitiveDeclaration):
    """
    Declaration of an object-typed member, which will be flattened.
    """

    def __init__(self):
        super().__init__()
        self.category = self.OBJECT
        self.noprefix = False

    def get_members(self, prefix=None):
        """
        Return the list of members added to the enclosing aggregate.
        """
        if self.offset is not None:
            raise DSLError("Object-typed member cannot have a fixed offset",
                           self.offset)
        if self.complex_type:
            return super().get_members(prefix=prefix)
        prefix = None if self.noprefix else self._get_member_name(prefix)
        members = tuple(self.type_ref.definition._members(prefix))
        for n, t, d in members:
            if d.offset is not None:
                raise DSLError(
                    "Flattened object type cannot contain fixed offsets",
                    d.member_name)
        return members


templates = {}


class BitFieldDeclaration(IGenCode, IDeclaration):
    """
    Declaration of a field in a BitFieldDefinition.
    """

    ACCESSOR_TEMPLATE = "templates/bitfield-generic-accessor.tmpl"

    def __init__(self):
        super().__init__()
        self.category = self.BITFIELD
        self.prefix = None
        self.bitfield_specifier = None
        self.bf_type_name = None
        self.unit_type = None
        self.unit_size = -1
        self.ranges = None
        self.default = None
        self._template = None

    def gen_code(self):
        # validate parameters
        # FIXME:
        # if self.bitfield_specifier.sign_map is not None and \
        #   not self.is_signed:
        #    raise DSLError("specified sign map for unsigned type",
        #                   self.member_name)

        body = []
        footer = []

        # generate code to extra

        # FIXME: should be a list of templates (header, c, etc.) ?
        assert self._template is not None

        if 'bitfield' in templates:
            template = templates['bitfield']
        else:
            template = Template.compile(file=open(self._template, 'r',
                                                  encoding='utf-8'))
            templates['bitfield'] = template

        t = template(namespaces=(self))
        footer.append(str(t))

        return (body, footer)

    def get_members(self, prefix=None):
        """
        Return the list of members added to the enclosing aggregate.

        This doesn't make sense for bitfield declarations, which can never
        flatten anything.
        """
        raise NotImplementedError

    def get_template_deps(self):
        return [os.path.join(__loc__, self.ACCESSOR_TEMPLATE)]

    def update(self):
        super().update()

        if not self.is_ignore and self.compound_type is None:
            return

        if self.compound_type.is_array:
            raise DSLError("cannot declare an array in a bitfield",
                           self.member_name)

        b = self.bitfield_specifier

        # Allocate auto bits
        if b.specifier_type is BitFieldSpecifier.AUTO:
            width = b.auto_width
            if width is None:
                if self.compound_type.bitsize is not None:
                    width = self.compound_type.bitsize
                else:
                    width = self.compound_type.size * 8
            else:
                width = int(width)

            r = self.ranges.alloc_range(width, self)
            if r is None:
                raise DSLError(
                    "unable to allocate {:d} bits from {:s}".format(
                        width, repr(self.ranges)),
                    self.member_name)
            assert (r[1] == width)

            b.bit_length = width
            range_shift = b.shift
            unit_size = self.unit_size
            bit = r[0]
            while ((width > 0) and
                   ((bit // unit_size) != (bit + width - 1) // unit_size)):
                split_point = ((bit // unit_size) + 1) * unit_size
                split_width = split_point - bit
                b.mapping.add_bit_range(range_shift, bit, split_width)
                bit = split_point
                width -= split_width
                range_shift += split_width
            b.mapping.add_bit_range(range_shift, bit, width)

        assert (b.bit_length is not None)

        b.set_signed(self.compound_type.is_signed)

        const = self.compound_type.is_const
        writeonly = self.compound_type.is_writeonly
        if const and writeonly:
            raise DSLError("const and writeonly is invalid", self.member_name)

        # pdb.set_trace()
        member_typesize = self.compound_type.size * 8

        if member_typesize < b.bit_length:
            raise DSLError(
                "too many bits {:d}, exceed type size {:d}".format(
                    b.bit_length, member_typesize),
                self.member_name)

        member_bitsize = self.compound_type.bitsize

        if member_bitsize is not None and member_bitsize > b.bit_length:
            raise DSLError(
                "not enough bits {:d}, need at least {:d}".format(
                    b.bit_length, member_bitsize),
                self.member_name)

        self._template = os.path.join(__loc__, self.ACCESSOR_TEMPLATE)

    @property
    def indicator(self):
        return self.compound_type.indicator

    @property
    def field_shift(self):
        return self.bitfield_specifier.mapping.field_shift

    @property
    def field_signed(self):
        return self.bitfield_specifier.mapping.field_signed

    @property
    def field_maps(self):
        return self.bitfield_specifier.mapping.field_maps

    @property
    def field_length(self):
        return self.bitfield_specifier.bit_length

    @property
    def is_const(self):
        return self.compound_type.is_const

    @property
    def is_writeonly(self):
        return self.compound_type.is_writeonly

    @property
    def is_nested_bitfield(self):
        return (not self.compound_type.is_pointer and
                self.compound_type.category == 'bitfield')

    @property
    def field_name(self):
        return self._get_member_name(self.prefix)

    def update_ranges(self, ranges, unit_size):
        if self.compound_type is not None and self.compound_type.is_pointer:
            # Pointers are automatically shifted left as far as possible,
            # because their bitsize reflects the low bits being fixed to zero
            # by the alignment of the target type. They should never have an
            # explicitly specified shift.
            assert self.bitfield_specifier.shift == 0
            b = self.bitfield_specifier
            ptr_size = self.compound_type.size * 8
            ptr_bits = self.compound_type.bitsize

            if b.specifier_type is BitFieldSpecifier.RANGE:
                range_width = b.range_width
                ptr_bits = range_width

            if ptr_bits < self.compound_type.bitsize:
                raise DSLError(
                    "too few bits {:d}, for pointer bitsize {:d}".format(
                        ptr_bits, self.compound_type.bitsize),
                    self.member_name)
            if ptr_bits > ptr_size:
                raise DSLError(
                    "too many bits {:d}, for pointer type size {:d}".format(
                        ptr_bits, ptr_size),
                    self.member_name)
            self.bitfield_specifier.set_type_shift(ptr_size - ptr_bits)

        self.bitfield_specifier.update_ranges(self, ranges, unit_size)


class StructurePadding:
    def __init__(self, length):
        super().__init__()
        self._length = length

    def gen_declaration(self, identifier):
        return 'uint8_t {:s}[{:d}]'.format(identifier, self._length)


class StructureDefinition(IGenCode, ICustomizedDefinition):

    def __init__(self, type_name):
        super().__init__()
        self.type_name = type_name
        self.category = self.STRUCTURE
        self.declarations = []
        self.extensions = []
        self._abi = None
        self._size = None
        self._alignment = None
        self._layout = None
        self._ordered = True

    def set_abi(self, abi):
        """
        Set the ABI object that provides structure layout rules.
        """
        self._abi = abi

    def set_ordered(self, ordered):
        """
        Set the structure layout rules.
        """
        self._ordered = ordered

    def _update_layout(self):
        """
        Determine the layout of the structure.
        """
        for q in self.qualifiers:
            if q.is_optimized:
                self.set_ordered(False)

        if self.is_ordered:
            member_list = iter(self._members())
        else:
            # Sort members by group, module and alignment
            member_list = list(self._members())

            def member_key(member):
                member_type = member[1]
                member_decl = member[2]

                default_group = chr(0xff)
                if hasattr(member_decl, "module_name"):
                    if member_decl.module_name:
                        default_group = '~' + member_decl.module_name

                key = (default_group, -member_type.alignment)
                for q in member_type.qualifiers:
                    if q.is_group:
                        key = tuple(['/'.join(q.group),
                                     -member_type.alignment])
                        break
                return key

            # list of lists, sort by group and alignment
            member_list = sorted(self._members(), key=member_key)

        packed = self.is_packed
        layout = []

        abi = self._abi

        # The layout is (member name, member type, member offset).

        offset = 0
        max_alignment = 1

        members = set()
        for member_name, member_type, member_decl in member_list:
            if member_name in members:
                raise DSLError("structure {}: duplicated members".format(
                               self.type_name), member_decl.member_name)
            members.add(member_name)
            if member_decl.offset is not None:
                member_pos = int(member_decl.offset)
                if member_pos < offset:
                    raise DSLError("structure {}: "
                                   "Fixed offset of member (@{:d}) is "
                                   "before the end of the previous member "
                                   "(@{:d})".format(self.type_name,
                                                    member_pos, offset),
                                   member_decl.member_name)
                elif member_pos > offset:
                    layout.append(('pad_to_{:s}_'.format(member_name),
                                   StructurePadding(member_pos - offset),
                                   member_pos))
                offset = member_pos
            else:
                member_pos = None
            if not packed:
                pos = abi.layout_struct_member(offset, max_alignment,
                                               member_type.size,
                                               member_type.alignment)
                if pos > offset:
                    if member_pos is not None:
                        raise DSLError("structure {}: "
                                       "Padding needed after fixed offset "
                                       "({:d} bytes)".format(self.type_name,
                                                             pos - offset),
                                       member_decl.member_name)
                    layout.append(('pad_to_{:s}_'.format(member_name),
                                   StructurePadding(pos - offset), pos))
                    offset = pos
            layout.append((member_name, member_type, offset))
            offset += member_type.size
            max_alignment = max(max_alignment, member_type.alignment)

        if offset != 0:
            # update max_alignment for end padding
            for q in self.qualifiers:
                if q.is_aligned:
                    max_alignment = max(max_alignment, q.align_bytes)

            if not packed:
                # Pad the structure at the end
                end = abi.layout_struct_member(
                    offset, max_alignment, None, None)
                if end > offset:
                    layout.append(('pad_end_', StructurePadding(end - offset),
                                   offset))
                    offset = end
            self._alignment = max_alignment

        self._size = offset
        self._layout = tuple(layout)

    def gen_forward_decl(self):
        """
        Generates a forward declaration (if needed) for the type.
        """
        code = []
        code.append("typedef")

        for q in self.qualifiers:
            if q.is_aligned or q.is_packed:
                pass
            elif q.is_atomic or q.is_const:
                code.extend(q.gen_qualifier())
        code.append("struct " + self.type_name + '_s' + ' ' +
                    self.type_name + '_t'";\n")

        return (code)

    def gen_code(self):
        if self._layout is None:
            self._update_layout()

        code = []
        extra = []

        if self._size != 0:
            code.append("struct ")
            for q in self.qualifiers:
                if q.is_aligned or q.is_atomic or q.is_const or q.is_optimized:
                    pass
                elif q.is_packed or q.is_lockable:
                    code.extend(q.gen_qualifier())
                else:
                    raise DSLError("Invalid qualifier for structure", q.name)

            code.append(" " + self.type_name + '_s' " {\n")

            for q in self.qualifiers:
                if q.is_aligned:
                    code.extend(q.gen_qualifier())

            for member_name, member_type, member_offset in self._layout:
                code.append(member_type.gen_declaration(member_name) + ';\n')

            code.append("} ")

            code.append(';\n\n')

        return (code, extra)

    @property
    def size(self):
        if self._size is None:
            self._update_layout()
        return self._size

    @property
    def layout_with_padding(self):
        if self._layout is None:
            self._update_layout()
        return self._layout

    @property
    def layout(self):
        return ((n, t, p)
                for n, t, p in self.layout_with_padding
                if not isinstance(t, StructurePadding))

    @property
    def is_container(self):
        return True

    @property
    def is_ordered(self):
        return self._ordered

    @property
    def default_alignment(self):
        if self._alignment is None:
            self._update_layout()
        return self._alignment

    def _members(self, prefix=None):
        for d in self.declarations:
            yield from d.get_members(prefix=prefix)
        for e in self.extensions:
            yield from e._members(prefix=prefix)

    def update(self):
        """
        Update internal data, prepare to generate code
        """
        used_names = set()
        for member_name, member_type, _ in self._members():
            if member_name in used_names:
                raise DSLError("'structure {:s}': each member needs to have a"
                               " unique name".format(self.type_name),
                               member_type.type_name)
            used_names.add(member_name)

    @property
    def dependencies(self):
        """
        Return all definitions that this type relies on.

        Note, pointers and other non definitions are not considered
        dependencies as they can be forward declared.
        """
        return itertools.chain(*(t.dependencies
                                 for _, t, _ in self._members()))


class ObjectDefinition(StructureDefinition):
    def __init__(self, type_name):
        super().__init__(type_name)
        self.category = self.OBJECT
        # Object definitions are only generated for objects that are never
        # embedded in any other object.
        # FIXME: this should be explicit in the language
        self.need_export = True
        self.set_ordered(False)

    def gen_code(self):
        if self.need_export:
            return super().gen_code()
        else:
            return ([], [])

    def gen_forward_decl(self):
        if self.need_export:
            return super().gen_forward_decl()
        else:
            return ([])

    @property
    def is_container(self):
        return self.need_export

    def gen_type_name(self, unqualified=False):
        """
        Construct a C type name for this type.
        """
        if self.need_export:
            return super().gen_type_name()
        else:
            return None


class UnionDefinition(IGenCode, ICustomizedDefinition):
    def __init__(self, type_name):
        super().__init__()
        self.type_name = type_name
        self.category = self.UNION
        self.declarations = []
        self.extensions = []
        self._size = None
        self._alignment = None

    def gen_forward_decl(self):
        """
        Generates a forward declaration (if needed) for the type.
        """
        code = []
        code.append("typedef")

        for q in self.qualifiers:
            if q.is_aligned or q.is_lockable:
                pass
            elif q.is_atomic or q.is_const:
                code.extend(q.gen_qualifier())
            else:
                raise DSLError("Invalid qualifier for union", q.name)
        code.append("union " + self.type_name + '_u' +
                    ' ' + self.type_name + '_t;\n')

        return code

    def gen_code(self):
        code = []

        code.append('union ')
        for q in self.qualifiers:
            if q.is_aligned or q.is_atomic or q.is_const:
                pass
            elif q.is_lockable:
                code.extend(q.gen_qualifier())
            else:
                raise DSLError("Invalid qualifier for union", q.name)

        code.append(" " + self.type_name + '_u' + " {\n")

        align_qualifiers = tuple(q for q in self.qualifiers if q.is_aligned)

        for member_name, member_type, member_decl in self._members():
            if member_decl.offset is not None and int(member_decl.offset) != 0:
                raise DSLError("Union member must have zero offset",
                               member_decl.member_name)
            for q in align_qualifiers:
                code.extend(q.gen_qualifier())
            code.append(member_type.gen_declaration(member_name) + ';\n')

        code.append("} ")

        code.append(';\n\n')

        return (code, [])

    @property
    def size(self):
        if self._size is None:
            self._size = max(t.size for _, t, _ in self._members())
        return self._size

    @property
    def default_alignment(self):
        if self._alignment is None:
            q_align = max(
                (q.align_bytes for q in self.qualifiers if q.is_aligned),
                default=1)
            m_align = max(t.alignment for _, t, _ in self._members())
            self._alignment = max(q_align, m_align)
        return self._alignment

    def _members(self, prefix=None):
        for d in self.declarations:
            members = d.get_members(prefix=prefix)
            if len(members) > 1:
                raise DSLError("Unions must not contain flattened objects",
                               d.member_name)
            yield from members
        for e in self.extensions:
            yield from e._members(prefix=prefix)

    def named_member(self, name):
        for member_name, member_type, member_decl in self._members():
            if member_name == name:
                return member_type, member_decl
        raise DSLError("'union {:s}': no member named '{:s}'"
                       .format(self.type_name, name))

    def update(self):
        """
        Update internal data, prepare to generate code
        """
        used_names = set()
        for member_name, member_type, _ in self._members():
            if member_name in used_names:
                raise DSLError("'union {:s}': each member needs to have a"
                               " unique name".format(self.type_name),
                               member_type.type_name)
            used_names.add(member_name)

    @property
    def dependencies(self):
        """
        Return all definitions that this type relies on.

        Note, pointers and other non definitions are not considered
        dependencies as they can be forward declared.
        """
        return itertools.chain(*(t.dependencies
                                 for _, t, _ in self._members()))


class EnumerationConstant(object):
    __slots__ = ['name', 'value', 'prefix']

    def __init__(self, name, value=None):
        self.name = name
        self.value = value
        self.prefix = None


enum_const = namedtuple('enum_const', ['name', 'value', 'prefix'])


class EnumerationDefinition(IGenCode, ICustomizedDefinition):
    def __init__(self, type_name):
        super().__init__()
        self.type_name = type_name
        self.category = self.ENUMERATION
        self._enumerators = []
        self.prefix = None
        self.capitalized = True
        self._explicit = False
        self._signed = None
        self._size = None
        self._bitsize = None
        self._alignment = None
        self._min_value = None
        self._max_value = None
        self._updated = False

    def __getstate__(self):
        """
        Temporary workaround to ensure types are updated before pickling. Auto
        update should be removed entirely when issue is resolved.
        """
        if not self._updated:
            self._autoupdate()
        return self.__dict__

    def set_abi(self, abi):
        """
        Set the ABI object that provides structure layout rules.
        """
        self._abi = abi

    def set_explicit(self):
        """
        Set the enumeration as being explicit, no value allocation.
        """
        self._explicit = True

    def add_enumerator(self, e):
        """
        Add an enumerator to the type.
        """
        # If this is the first enumerator, its default value is zero
        if e.value is None and len(self._enumerators) == 0:
            if self._explicit:
                raise DSLError("auto allocated enumerator in explicit "
                               "enumeration", e.name)
            e.value = 0

        self._enumerators.append(e)

    def _autoupdate(self):
        """
        Update internal data, prepare to generate code
        """
        def _check_enumerator(e):
            if e.name in used_names:
                raise DSLError("'enumeration {:s}': each enumerator needs to"
                               " have a unique name".format(self.type_name),
                               e.name)
            if e.value in used_values:
                raise DSLError("'enumeration {:s}': each enumerator needs to"
                               " have a unique value".format(self.type_name),
                               e.name)
            used_names.add(e.name)
            used_values.add(e.value)

        # Ensure constant values are resolved and not duplicates
        used_names = set()
        used_values = set()
        for e in self._enumerators:
            if e.value is not None:
                e.value = int(e.value)
                _check_enumerator(e)

        # Auto-allocation of remaining values
        last_val = None
        for e in self._enumerators:
            if e.value is None and self._explicit:
                raise DSLError("auto allocated enumerator in explicit "
                               "enumeration", e.name)
            if e.value is None:
                assert last_val is not None
                e.value = last_val + 1
                _check_enumerator(e)
            last_val = e.value

        if not self._enumerators:
            raise DSLError('Empty enumeration', self.type_name)
        e_min = min(used_values)
        e_max = max(used_values)
        self._size, self._alignment, self._signed = \
            self._abi.get_enum_properties(e_min, e_max)
        self._bitsize = max(e_min.bit_length(), e_max.bit_length())
        self._min_value = e_min
        self._max_value = e_max

        if self.prefix is None:
            self.prefix = self.type_name + '_'
            if self.capitalized:
                self.prefix = self.prefix.upper()

        # Todo: support suppressing prefix for some enumerators
        for e in self._enumerators:
            if e.prefix is None:
                e.prefix = self.prefix

        # Finalize
        enumerators = [enum_const(e.name, e.value, e.prefix) for e in
                       self._enumerators]
        self._enumerators = enumerators
        self._updated = True

    @property_autoupdate
    def enumerators(self):
        return self._enumerators

    @property_autoupdate
    def size(self):
        return self._size

    @property_autoupdate
    def bitsize(self):
        return self._bitsize

    @property_autoupdate
    def minimum_value(self):
        return self._min_value

    @property_autoupdate
    def maximum_value(self):
        return self._max_value

    @property_autoupdate
    def default_alignment(self):
        return self._alignment

    @property_autoupdate
    def is_signed(self):
        return self._signed

    def get_enum_name(self, e):
        if self.capitalized:
            return e.name.upper()
        else:
            return e.name

    def gen_code(self):
        if not self._updated:
            self._autoupdate()

        code = []
        extra = []

        # generate code now
        code = ['typedef', 'enum', self.type_name + '_e', '{\n']

        sorted_enumerators = sorted(self._enumerators, key=lambda x: x.value)

        code.append(',\n'.join('  ' + e.prefix + self.get_enum_name(e) +
                               ' = ' + str(e.value)
                               for e in sorted_enumerators))

        code.append('\n}')

        code.append(self.type_name + '_t')

        code.append(';\n\n')

        for e in self._enumerators:
            if e.value == self.minimum_value:
                e_min = e.prefix + self.get_enum_name(e)
            if e.value == self.maximum_value:
                e_max = e.prefix + self.get_enum_name(e)

        code.append('#define {:s}__MAX {:s}\n'.format(
            self.type_name.upper(), e_max))
        code.append('#define {:s}__MIN {:s}\n'.format(
            self.type_name.upper(), e_min))
        code.append('\n')

        return (code, extra)

    @property
    def dependencies(self):
        """
        Return all definitions that this type relies on.

        Note, pointers and other non definitions are not considered
        dependencies as they can be forward declared.
        """
        return ()


class IDeclarationDefinition(IGenCode, ICustomizedDefinition, IDeclaration):
    def __init__(self, type_name):
        super().__init__()
        self.type_name = type_name

    @property
    def size(self):
        return self.compound_type.size

    @property
    def bitsize(self):
        return self.compound_type.bitsize

    @property
    def default_alignment(self):
        return self.compound_type.alignment

    @property
    def is_signed(self):
        return self.compound_type.is_signed

    @property
    def dependencies(self):
        """
        Return all definitions that this type relies on.

        Note, pointers and other non definitions are not considered
        dependencies as they can be forward declared.
        """
        if self.compound_type is None:
            return ()
        return self.compound_type.dependencies


class AlternativeDefinition(IDeclarationDefinition):
    def __init__(self, type_name):
        super().__init__(type_name)
        self.category = self.ALTERNATIVE

    def gen_code(self):
        code = []
        extra = []

        # generate code now
        decl = self.compound_type.gen_declaration(self.type_name)
        code = ["typedef ", decl, ';\n']

        return (code, extra)

    def gen_type_name(self, unqualified=False):
        """
        Construct a C type name for this type.
        """
        return self.type_name


class ConstantDefinition(IDeclarationDefinition):
    def __init__(self, type_name, value):
        super().__init__(type_name)
        self.value = value
        self.category = self.CONSTANT

    def gen_code(self):
        code = []
        extra = []

        # generate code now
        val = int(self.value)
        if self.compound_type is not None:
            t = self.compound_type.gen_type_name(unqualified=True)
            cast = '({:s})'.format(t)
            suffix = '' if self.compound_type.is_signed else 'U'
            if val < 0 and not self.compound_type.is_signed:
                val &= ((1 << (self.compound_type.size * 8)) - 1)
            val = '{:d}{:s} // {:#x}'.format(val, suffix, val)
        else:
            val = '{:d}'.format(val)
            cast = ''
        code.append("#define {:s} {:s}{:s}\n".format(
            self.type_name, cast, val))

        return (code, extra)


class GlobalDefinition(IDeclarationDefinition):
    def __init__(self, type_name):
        super().__init__(type_name)
        self.category = self.GLOBAL

    def gen_code(self):
        code = []
        extra = []

        # generate code now
        t = self.compound_type.gen_type_name(unqualified=True)
        code.append("extern {:s} {:s};\n\n".format(t, self.type_name))

        return (code, extra)


class BitFieldDefinition(IGenCode, ICustomizedDefinition):
    TYPE_TEMPLATE = "templates/bitfield-type.tmpl"

    def __init__(self, type_name):
        super().__init__()
        self.type_name = type_name
        self.length = -1
        self.unit_type = "uint64_t"
        self.unit_size = -1
        self.declarations = []
        self.extensions = []
        self.category = self.BITFIELD
        self._ranges = None
        self.const = False
        self._signed = False
        self._has_set_ops = None
        self._template = None

    def update_unit_info(self):
        if self.length <= 8:
            self.unit_size = 8
            self.unit_type = "uint8_t"
        elif self.length <= 16:
            self.unit_size = 16
            self.unit_type = "uint16_t"
        elif self.length <= 32:
            self.unit_size = 32
            self.unit_type = "uint32_t"
        elif self.length <= 64:
            self.unit_size = 64
            self.unit_type = "uint64_t"
        else:
            self.unit_size = 64
            self.unit_type = "uint64_t"

    @property
    def size(self):
        return math.ceil(self.length / self.unit_size) * self.unit_size // 8

    @property
    def ranges(self):
        if self._ranges is None:
            self._update_layout()
        return self._ranges

    @property
    def bitsize(self):
        return max(r[1] + 1 for r in self.ranges.alloc_list)

    @property
    def default_alignment(self):
        return self.unit_size // 8

    @property
    def is_signed(self):
        return self._signed

    @property
    def _all_declarations(self):
        for d in self.declarations:
            yield d
        for e in self.extensions:
            for d in e.declarations:
                yield d

    @property
    def fields(self):
        items = []
        for d in self._all_declarations:
            if d.compound_type is not None:
                items.append(d)
        items = sorted(items, key=lambda x: x.field_maps[0].mapped_bit)
        return tuple(items)

    @property
    def all_fields_boolean(self):
        return all(m.compound_type.bitsize == 1 for m in self.fields)

    @property
    def has_set_ops(self):
        if self._has_set_ops is None:
            return self.all_fields_boolean
        return self._has_set_ops

    @has_set_ops.setter
    def has_set_ops(self, value):
        self._has_set_ops = bool(value)

    def _gen_definition_code(self):
        """
        Return type definition code if this type needs it.
        """
        if self.ranges.range_auto:
            raise Exception("unhandled auto ranges")
            self.length = self.ranges.free_list[0][1] - 1
            self.update_unit_info()

        # generate type definition by template
        ns = {
            "type_name": self.type_name,
            "unit_type": self.unit_type,
            "unit_size": self.unit_size,
            "unit_cnt": self.unit_count,
            "declarations": self._all_declarations,
            "init_values": self.init_values,
            "compare_masks": self.compare_masks,
            "boolean_masks": self.boolean_masks,
            "all_fields_boolean": self.all_fields_boolean,
            "has_set_ops": self.has_set_ops,
        }

        if 'bitfield-type' in templates:
            template = templates['bitfield-type']
        else:
            template = Template.compile(file=open(self._template, 'r',
                                                  encoding='utf-8'))
            templates['bitfield-type'] = template

        t = template(namespaces=(ns))
        return str(t)

    @property
    def unit_count(self):
        return math.ceil(self.length / self.unit_size)

    @property
    def init_values(self):
        init_value = 0
        for d in self._all_declarations:
            if d.default is None:
                continue
            val = int(d.default.value)
            if d.field_length is None and val != 0:
                raise DSLError(
                    "bitfield others must not have a nonzero default",
                    d.member_name)
            if val == 0:
                continue
            if val.bit_length() > d.field_length:
                raise DSLError("Bitfield default value does not fit in field",
                               d.member_name)
            for field_map in d.field_maps:
                field_mask = (1 << field_map.length) - 1
                # First mask out any reserved bits that have been replaced by a
                # field in an extension.
                init_value &= ~(field_mask << field_map.mapped_bit)
                init_value |= (((val >> field_map.field_bit) & field_mask) <<
                               field_map.mapped_bit)
        unit_mask = (1 << self.unit_size) - 1
        return tuple((init_value >> (i * self.unit_size)) & unit_mask
                     for i in range(self.unit_count))

    @property
    def compare_masks(self):
        """
        Tuple of per-unit masks of non-writeonly fields.

        This is used for generating comparison operations.
        """
        compare_mask = 0
        for d in self.fields:
            if d.is_writeonly:
                continue
            for field_map in d.field_maps:
                field_mask = (1 << field_map.length) - 1
                compare_mask |= field_mask << field_map.mapped_bit
        unit_mask = (1 << self.unit_size) - 1
        return tuple((compare_mask >> (i * self.unit_size)) & unit_mask
                     for i in range(self.unit_count))

    @property
    def boolean_masks(self):
        """
        Tuple of per-unit masks of boolean typed fields.

        This is used for generating bitwise set operations that exclude non-
        boolean fields, if there are any. This allows the binary set
        operations to be constructed such that any non-boolean fields from the
        left hand argument are preserved in the result.
        """
        boolean_mask = 0
        for d in self.fields:
            if d.compound_type.bitsize != 1:
                continue
            for field_map in d.field_maps:
                field_mask = (1 << field_map.length) - 1
                boolean_mask |= field_mask << field_map.mapped_bit
        unit_mask = (1 << self.unit_size) - 1
        return tuple((boolean_mask >> (i * self.unit_size)) & unit_mask
                     for i in range(self.unit_count))

    def gen_code(self):
        code = []
        extra = []

        assert self._template is not None

        code.append(self._gen_definition_code())

        # generate getters and setters for all declarations
        for d in self._all_declarations:
            if d.compound_type is None:
                continue
            if d.bitfield_specifier is None:
                raise DSLError("each declaration needs to specify logical" +
                               " physical bit map", d.member_name)
            else:
                c, e = d.gen_code()
                code += c
                extra += e

        return (code, extra)

    def get_template_deps(self):
        templates = []
        for d in self._all_declarations:
            if d.compound_type is None:
                continue
            else:
                templates += d.get_template_deps()
        return templates + [os.path.join(__loc__, self.TYPE_TEMPLATE)]

    def update(self):
        super().update()
        if self._ranges is None:
            self._update_layout()

        self._template = os.path.join(__loc__, self.TYPE_TEMPLATE)

    def _update_layout(self):
        """
        Determine the layout of the bitfield.
        """

        for e in self.extensions:
            for name in e.delete_items:
                found = False
                for i, d in enumerate(self.declarations):
                    if str(d.member_name) == name:
                        del (self.declarations[i])
                        found = True
                        break
                if not found:
                    raise DSLError("can't delete unknown member", name)

        self._ranges = BitFieldRangeCollector(self.length)

        for d in self._all_declarations:
            d.update_ranges(self._ranges, self.unit_size)

        used_names = set()
        for d in self._all_declarations:
            if d.is_ignore:
                continue

            if d.member_name in used_names:
                raise DSLError("'bitfield {:s}': each member needs to"
                               " have a unique name".format(self.type_name),
                               d.member_name)
            used_names.add(d.member_name)

            # if bitfield is constant, update all members
            if self.const:
                d.set_const()

            # share definition information to declarations
            d.bf_type_name = self.type_name
            d.unit_type = self.unit_type
            d.unit_size = self.unit_size
            d.ranges = self._ranges

            d.update()

    @property
    def dependencies(self):
        """
        Return all definitions that this type relies on.

        Note, pointers and other non definitions are not considered
        dependencies as they can be forward declared.
        """
        for m in self.fields:
            yield from m.compound_type.dependencies


class StructureExtension(IExtension):
    def __init__(self, type_name):
        super().__init__()
        self.type_name = type_name
        self.indicator = type_name
        self.module_name = None
        self.category = self.STRUCTURE

    def link(self, definition):
        self.prefix = self.module_name
        definition.extensions.append(self)

        for d in self.declarations:
            d.module_name = self.prefix

    def _members(self, prefix=None):
        if prefix is not None and self.prefix is not None:
            p = prefix + "_" + self.prefix
        elif prefix is not None:
            p = prefix
        else:
            p = self.prefix

        return itertools.chain(*(d.get_members(prefix=p)
                                 for d in self.declarations))


class ObjectExtension(StructureExtension):
    def __init__(self, type_name):
        super().__init__(type_name)
        self.category = self.OBJECT


class UnionExtension(IExtension):
    def __init__(self, type_name):
        super().__init__()
        self.type_name = type_name
        self.indicator = type_name
        self.module_name = None
        self.definition = NotImplemented
        self.category = self.UNION

    def link(self, definition):
        self.prefix = self.module_name
        definition.extensions.append(self)

    def _members(self, prefix=None):
        if prefix is not None and self.prefix is not None:
            p = prefix + "_" + self.prefix
        elif prefix is not None:
            p = prefix
        else:
            p = self.prefix

        for d in itertools.chain(self.declarations):
            members = d.get_members(prefix=p)
            if len(members) > 1:
                raise DSLError("Unions must not contain flattened objects",
                               d.member_name)
            yield from members


class EnumerationExtension(IExtension):
    def __init__(self, type_name):
        super().__init__()
        self.type_name = type_name
        self.indicator = type_name
        self.module_name = None
        self.category = self.ENUMERATION
        self._enumerators = []

    def add_enumerator(self, e):
        """
        Add an enumerator to the extension.
        """
        self._enumerators.append(e)

    def link(self, definition):
        for e in self._enumerators:
            definition.add_enumerator(e)


class BitFieldExtension(IExtension, IGenCode, IUpdate):
    def __init__(self, type_name):
        super().__init__()
        self.type_name = type_name
        self.indicator = type_name
        self.module_name = None
        self.category = self.BITFIELD
        self.delete_items = set()

    def link(self, definition):
        # FIXME check if declarations in extension overlap the original bit
        # fields

        # there's only one level for bitfield extension
        # change the name of declaration as ModuleName__member_name
        if self.module_name is not None:
            self.prefix = self.module_name

        # update definition's declarations list
        definition.extensions.append(self)

    def update(self):
        super().update()

        # Set the module prefix on all our declarations
        if self.prefix is not None:
            for d in self.declarations:
                d.prefix = self.prefix

    def add_delete_member(self, name):
        if name in self.delete_items:
            raise DSLError("delete item: {:s} duplicated".format(name), name)
        self.delete_items.add(name)

    def gen_code(self):
        code = []
        extra = []

        for d in self.declarations:
            c, e = d.gen_code()
            code += c
            extra += e

        return (code, extra)


class Qualifier:
    def __init__(self, name):
        self.name = name

    @property
    def is_const(self):
        return self.name == 'const'

    @property
    def is_writeonly(self):
        return False

    @property
    def is_restrict(self):
        return self.name == 'restrict'

    @property
    def is_atomic(self):
        return False

    @property
    def is_aligned(self):
        return False

    @property
    def is_packed(self):
        return False

    @property
    def is_contained(self):
        return False

    @property
    def is_lockable(self):
        return False

    @property
    def is_optimized(self):
        return False

    @property
    def is_group(self):
        return False

    def gen_qualifier(self):
        return [self.name]

    def __eq__(self, other):
        return (str(self.name) == str(other.name))

    def __hash__(self):
        return hash(str(self.name))


class AlignedQualifier(Qualifier):
    def __init__(self, name, align_bytes):
        super().__init__(name)
        self._align_bytes_expr = align_bytes
        self._align_bytes = None

    @property
    def align_bytes(self):
        if self._align_bytes is None:
            align_bytes = int(self._align_bytes_expr)
            if align_bytes <= 0:
                raise DSLError("Alignment {:d} is not positive"
                               .format(align_bytes), self.name)
            if align_bytes != 1 << (align_bytes - 1).bit_length():
                raise DSLError("Alignment {:d} is not a power of two"
                               .format(align_bytes), self.name)
            self._align_bytes = align_bytes
        return self._align_bytes

    def gen_qualifier(self):
        return ["alignas({:d})".format(self.align_bytes)]

    @property
    def is_aligned(self):
        return self.align_bytes

    def __eq__(self, other):
        return (id(self) == id(other))

    def __hash__(self):
        return hash(id(self))


class GroupQualifier(Qualifier):
    def __init__(self, name, group):
        super().__init__(name)
        self.group = group

    def gen_qualifier(self):
        return [""]

    @property
    def is_group(self):
        return True


class AtomicQualifier(Qualifier):
    def gen_qualifier(self):
        return ["_Atomic"]

    @property
    def is_atomic(self):
        return True


class PackedQualifier(Qualifier):
    def gen_qualifier(self):
        return ["__attribute__((packed))"]

    @property
    def is_packed(self):
        return True


class ContainedQualifier(Qualifier):
    def gen_qualifier(self):
        return [""]

    @property
    def is_contained(self):
        return True


class WriteonlyQualifier(Qualifier):
    def gen_qualifier(self):
        return [""]

    @property
    def is_writeonly(self):
        return True


class LockableQualifier(Qualifier):
    def __init__(self, name):
        super().__init__(name)
        self.resource_name = None

    def gen_qualifier(self):
        if self.resource_name is None:
            raise DSLError(
                "Only structure, object and union definitions may be lockable",
                self.name)
        return ['__attribute__((capability("{:s}")))'
                .format(self.resource_name)]

    @property
    def is_lockable(self):
        return True


class OptimizeQualifier(Qualifier):
    def __init__(self, name):
        super().__init__(name)
        self.category = None

    def gen_qualifier(self):
        if self.category is None:
            raise DSLError(
                "Only structure and object definitions may be optimized",
                self.name)
        return [""]

    @property
    def is_optimized(self):
        return True


class BitFieldRangeCollector:
    """
    BitFieldRangeCollector manages the range defined by length [0, length).
    It addresses the range manage issue from Bit Field declaration.
    """

    def __init__(self, length):
        if length == -1:
            self.range_auto = True
        else:
            self.range_auto = False
        self.free_list = [(0, length - 1 if length != -1 else 0)]
        self.alloc_list = []
        self.reserved_list = []
        self.origLength = length

    def insert_range(self, bit_range, declaration):
        """
        Check if the [start, end] is inside existing range
        definition.
        If it's inside the range, remove specified range, and return True.
        Else return False.
        """
        start = bit_range[0]
        end = bit_range[0] + bit_range[1] - 1
        if start < 0 or end < 0 or start > end:
            return False

        # NOTE: it's OK only if not continue loop after find the target
        for i, (s, e) in enumerate(self.free_list):
            if s <= start and e >= end:
                if declaration.is_ignore:
                    self.reserved_list.append((start, end, declaration))
                    # FIXME: check for reserved overlaps
                    return True

                del self.free_list[i]

                if s < start:
                    self.free_list.insert(i, (s, start - 1))
                    i += 1

                if e > end:
                    self.free_list.insert(i, (end + 1, e))

                self.alloc_list.append((start, end, declaration))

                return True

        return False

    def alloc_range(self, length, declaration):
        """
        Assumption, it's only used by auto logical physical mapping. This use
        case only happens for software bit field definition. It seldom to
        define scattered bit field members. Contiguously available space can
        be find easily.
        Also the typical bit field length is less than 64 bits (maybe 128
        bits), no need to handle alignment right now.

        Return the lowest fragment which satisfy the length requirement.
        Also mark it as used
        """
# FIXME: - option to keep fragment in one word (for bitfields) ?
        if self.range_auto:
            self.free_list[0] = (self.free_list[0][0],
                                 self.free_list[0][1] + length - 1)
        ret = None
        for (s, e) in self.free_list:
            sz = e - s + 1
            if sz >= length:
                ret = (s, length)
                self.insert_range(ret, declaration)
                break

        return ret

    def is_empty(self):
        return len(self.free_list) == 0

    def __repr__(self):
        msg = []
        for s, e in self.free_list:
            msg.append("[" + str(e) + ":" + str(s) + "]")
        return 'Range len(%d), ranges available: %s' % \
            (self.origLength, ','.join(msg))


class TransformTypes(Transformer):
    """
    Bottom up traversal helper. It overrides Transformer to do the
    traversal. Use CommonTree as
    the default tree node.
    """

    def __init__(self, program):
        super().__init__()
        self.program = program

    def __default__(self, children, meta, data):
        "Default operation on tree (for override)"
        import ast_nodes
        return ast_nodes.CommonListener(self.program, children, meta, data)

    def node_handler(self, name):
        import ast_nodes
        x = getattr(ast_nodes, name)
        if not inspect.isclass(x):
            raise AttributeError
        if not issubclass(x, ast_nodes.CommonTree):
            raise AttributeError
        if type(x) in [ast_nodes.CommonTree]:
            raise AttributeError
        return x

    def _call_userfunc(self, tree, new_children=None):
        # Assumes tree is already transformed
        children = new_children if new_children is not None else tree.children
        try:
            f = self.node_handler(tree.data)
        except AttributeError:
            ret = self.__default__(children, tree.meta, tree.data)
        else:
            ret = f(self.program, children, tree.meta)

        return ret

    def _transform_tree(self, tree):

        children = list(self._transform_children(tree.children))

        ret = self._call_userfunc(tree, children)

        return ret

    def _transform_children(self, children):
        import ast_nodes
        for c in children:
            try:
                if isinstance(c, Tree):
                    yield self._transform_tree(c)
                elif isinstance(c, Token):
                    yield ast_nodes.TToken(str(c), c, self.program)
                else:
                    yield c
            except Discard:
                pass

```

`tools/typed/templates/bitfield-generic-accessor.tmpl`:

```tmpl
#*
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# 2019 Cog Systems Pty Ltd.
#
# SPDX-License-Identifier: BSD-3-Clause
*#

#set field_type = $compound_type.gen_type_name(unqualified=True)
#if not $is_const
void ${bf_type_name}_set_${field_name}(${bf_type_name}_t *bit_field, ${compound_type.gen_declaration('val')});
#end if

#if not $is_writeonly
${field_type} ${bf_type_name}_get_${field_name}(const ${bf_type_name}_t *bit_field);
#end if

#if not $is_writeonly and not $is_const
void ${bf_type_name}_copy_${field_name}(
        ${bf_type_name}_t *bit_field_dst,
        const ${bf_type_name}_t *bit_field_src);
#end if

```

`tools/typed/templates/bitfield-type.tmpl`:

```tmpl
#*
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# 2019 Cog Systems Pty Ltd.
#
# SPDX-License-Identifier: BSD-3-Clause
*#

#set unit_postfix=''
#if $unit_cnt > 1
#set unit_postfix=' x{:d}'.format($unit_cnt)
#end if
// Bitfield: $type_name <${unit_type}${unit_postfix}>
typedef struct ${type_name}_b {
#set decs = []
#for $d in $declarations:
#if $d.field_maps
#set decs+=[$d]
#end if
#end for
#for $d in sorted($decs, key=lambda x: x.field_maps[0]):
#if $d.is_ignore
#continue
#end if
#set sep=''
#set maps=''
#for $map in sorted($d.field_maps, key=lambda x: -x.field_bit)
#set maps+=$sep
#if $map.length == 1
#set maps+='{:d}'.format($map.mapped_bit)
#else
#set maps+='{:d}:{:d}'.format($map.mapped_bit+$map.length-1,$map.mapped_bit)
#end if
#set sep=','
#end for
	// ${'{:9s}'.format($maps)} $d.compound_type.gen_declaration($d.field_name)
#end for
	${unit_type} bf[${unit_cnt}];
} ${type_name}_t;

## These accessors are macros rather than inline functions so they can be used
## in contexts that require constant expressions, e.g. in case labels.
#define ${type_name}_default() \
	(${type_name}_t){ .bf = { #slurp
${', '.join(hex(v) + 'U' for v in $init_values)} } }

#define ${type_name}_cast(#slurp
#for i in range($unit_cnt)
#if i
, #slurp
#end if
val_$i#slurp
#end for
) (${type_name}_t){ .bf = { #slurp
#for i in range($unit_cnt)
#if i
, #slurp
#end if
val_$i#slurp
#end for
} }

#if $unit_cnt == 1
${unit_type}
${type_name}_raw(${type_name}_t bit_field);

_Atomic ${unit_type} *
${type_name}_atomic_ptr_raw(_Atomic ${type_name}_t *ptr);

#end if
void
${type_name}_init(${type_name}_t *bit_field);

// Set all unknown/unnamed fields to their expected default values.
// Note, this does NOT clean const named fields to default values.
${type_name}_t
${type_name}_clean(${type_name}_t bit_field);

bool
${type_name}_is_equal(${type_name}_t b1, ${type_name}_t b2);

bool
${type_name}_is_empty(${type_name}_t bit_field);

// Check all unknown/unnamed fields have expected default values.
// Note, this does NOT check whether const named fields have their default
// values.
bool
${type_name}_is_clean(${type_name}_t bit_field);

#if $has_set_ops
// Union of boolean fields of two ${type_name}_t values
#if not $all_fields_boolean
//
// Note: non-boolean fields are preserved from the left-hand argument, and
// discarded from the right-hand argument.
#end if
${type_name}_t
${type_name}_union(${type_name}_t b1, ${type_name}_t b2);

// Intersection of boolean fields of two ${type_name}_t values
#if not $all_fields_boolean
//
// Note: non-boolean fields are preserved from the left-hand argument, and
// discarded from the right-hand argument.
#end if
${type_name}_t
${type_name}_intersection(${type_name}_t b1, ${type_name}_t b2);

// Invert all boolean fields in a ${type_name}_t value
#if not $all_fields_boolean
//
// Note: non-boolean fields are preserved.
#end if
${type_name}_t
${type_name}_inverse(${type_name}_t b);

// Set difference of boolean fields of two ${type_name}_t values
#if not $all_fields_boolean
//
// Note: non-boolean fields are preserved from the left-hand argument, and
// discarded from the right-hand argument.
#end if
${type_name}_t
${type_name}_difference(${type_name}_t b1, ${type_name}_t b2);
#if $unit_cnt == 1
#set mask = hex($boolean_masks[0])

// Atomically replace a ${type_name}_t value with the union of its boolean
// fields with a given ${type_name}_t value, and return the previous value.
#if not $all_fields_boolean
//
// Note: non-boolean fields are not modified.
#end if
${type_name}_t
${type_name}_atomic_union(_Atomic ${type_name}_t *b1, ${type_name}_t b2, memory_order order);

// Atomically replace a ${type_name}_t value with the intersection of its
// boolean fields with a given ${type_name}_t value, and return the previous
// value.
#if not $all_fields_boolean
//
// Note: non-boolean fields are not modified.
#end if
${type_name}_t
${type_name}_atomic_intersection(_Atomic ${type_name}_t *b1, ${type_name}_t b2, memory_order order);

// Atomically replace a ${type_name}_t value with the set difference of its
// boolean fields and a given ${type_name}_t value, and return the previous
// value.
#if not $all_fields_boolean
//
// Note: non-boolean fields are not modified.
#end if
${type_name}_t
${type_name}_atomic_difference(_Atomic ${type_name}_t *b1, ${type_name}_t b2, memory_order order);
#end if
#end if

```

`tools/typed/test/sample_inputs/basic_types.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// 2019 Cog Systems Pty Ltd.
//
// SPDX-License-Identifier: BSD-3-Clause

// define customise type
define byte_t newtype uint8;
define half_t newtype uint16;
define word_t newtype uint32;
define dword_t newtype uint32;

// FIXME: array of alternative types currently broken
// TODO - test additional combinations
//define dwordarray_t newtype array(4) type dword_t;

// size_t

// abi defined type
//define vaddr_t newtype uintptr;

define intarray_t newtype pointer array(8) sint8;

define ea enumeration {
	ZERO = 0;
	ONE = 1;
	TWO = 2;
};

define eb enumeration {
	ZERO noprefix;
	ONE;
	TWO;
};

define ec enumeration(noprefix) {
	_EC_ZER0 = 0;
	_EC_NEG = -1;
	_EC_POS = 1;
};

// TODO - use constants in enumerations
//define const1 constant 5;

define error enumeration {
	OK = 0;
	TEST = 1;
	// CONST1 = const1;	// FIXME: no yet supported
	bad_arg = 2;
	negative = -3;
	misc1; // = -2 (implicit)
	misc2; // = -1 (implicit)
	positive = 10;
	//TEST2 = 1;	// duplicate error
};

define enum2 enumeration(noprefix) {
	TEST_VAL1 = 1;
	TEST_VAL2 = 2;
};

extend error enumeration {
	ext1; // values continue from last value of base, or last auto value of previous extension
	ext2;
};

extend error enumeration {
	//second_ext1 = 11; // this should error
	second_ext2 = -10;
};

```

`tools/typed/test/sample_inputs/bitfield.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// 2019 Cog Systems Pty Ltd.
//
// SPDX-License-Identifier: BSD-3-Clause

//define const3 constant 8;
//define const4 constant 8;
//define const5 constant 10;

//define X bitfield<64> {
//	5:0		C uint16 = const3;
//	const3		D bool;
//	const5:const4	E uint16;
//};

define enum2 enumeration(noprefix) {
	TEST_VAL1 = 1;
	TEST_VAL2 = 2;
};

// Bitfield must have complete coverage
define X bitfield<64> {
	7:0		A uint16;		// A[7..0] <= X[7..0] (.. is range short form)
	15:8		B sint16 = 5;		// B[15..7] <= X[15], B[6..0] <= X[14..0] - initialized to 5 (no argument in init function)
	23,19:16	C sint16;		// C[15..4] <= X[23], C[3..0] <= X[19..16]
	26,25,24	D uint8;		// D[2..0] <= X[26..24] (Example long form)
	27,31:29	E uint8;		// E[3] <= X[27], E[2..0] <= X[31..29]
	28,22:20	0x3;			// const bits 0b0011, may be run-time checked in any operation
	35:32		0b1010;			// const bits 0b1010, may be run-time checked
	39:36		unknown=0;		// don't care bits, initialize to 0

	43:40		F uint32 lsl(12);	// F[19..12] <= X[47..40], F[11..0] <= 0

	48		G bool(writeonly);
	49		H uint8(const);		// const (readonly)
	50		I sint8;		// 0 or -1

	63:51		unknown;		// don't care bits, initialized to unknown
	// others	// not allowed unless there are 'auto' bit assignments

	// Not defined yet - field type non-contiguous (e.g. embedded bitfield)
	// 44<4>,<00>,41,40	Y uint16;
};

define Y bitfield<64,const> {
	7:0		A uint16;		// A[7..0] <= X[7..0] (.. is range short form)
	//15:8		B uint16(writeonly);		// A[7..0] <= X[7..0] (.. is range short form)
	23:16		C uint16(const);		// A[7..0] <= X[7..0] (.. is range short form)
	63:24		D uint64;
};

define Z bitfield<64> {
	7:0		A uint16;		// A[7..0] <= X[7..0] (.. is range short form)
	//15:8		B uint16(writeonly);		// A[7..0] <= X[7..0] (.. is range short form)
	23:16		C uint16(const);		// A[7..0] <= X[7..0] (.. is range short form)
	63:24		D uint64;
};

define defaults bitfield<64> {
	7:0		A uint16 = 0xde;
	15:8		B uint16(writeonly) = 0xad;
	23:16		C uint16(const) = 0xbe;
	63:24		unknown = 0x5555555555;
};

extend defaults bitfield {
	delete		A;
	7		F uint16 = 0x1;
	47:40		D uint16 = 0x3;
};

define struct2 structure {
	A	enumeration enum2;
	B	bitfield Z;
};

define Ext0 bitfield<64> {
	7:0		A0 enumeration enum2;
	15:8		B0 sint16;
	23:16		C0 uint16(const);
	63:24		unknown=0;
};

extend Ext0 bitfield {
	35:32		D0 uint16;
};

define Ext1 bitfield<64> {
	7:0		A1 uint16;
	auto<5>		B1 uint8;
	auto		C1 enumeration enum2;
	others		unknown=0;
};

extend Ext1 bitfield {
	delete		A1;
	15:8		D1 uint16;
	auto		E1 uint8;
};


define Ext2 bitfield<64> {
	3:0		A1 uint16;
	auto<5>		B1 uint8;
	auto		C1 uint32;
	others		unknown=0;
};

extend Ext2 bitfield module ExtModule {
	delete		A1;
	7:0		A1 uint16;
	15:8		D1 uint16;
	auto		E1 uint8;
};

```

`tools/typed/test/sample_inputs/bitmap.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

#include <stdalign.h>

#define BITMAP_WORD_BITS (sizeof(uregister) * 8)
#define BITMAP_WORDS(x) (((x) + BITMAP_WORD_BITS - 1)/ BITMAP_WORD_BITS)
#define BITMAP_DECLARE(bits, name) name array(BITMAP_WORDS(bits)) uregister

define test_struct structure {
	BITMAP_DECLARE(512, test_bitmap_512);
	BITMAP_DECLARE(768, test_bitmap_768);
	BITMAP_DECLARE(1, test_bitmap_1);
};

define sizeof_test_struct constant sizeof structure test_struct;
define alignof_test_struct constant alignof(structure test_struct);

define condition_test constant (sizeof_test_struct > (64 % 12)) ?
		42 + 1 : 48 + 52;

```

`tools/typed/test/sample_inputs/conditional.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// 2019 Cog Systems Pty Ltd.
//
// SPDX-License-Identifier: BSD-3-Clause

#if defined(CHOOSE3)
define Choosen structure {
	x uint32;
	xp pointer uint64;
	abc array(8) sint8;
};
#else
define Default structure {
	x uint32;
};
#endif

```

`tools/typed/test/sample_inputs/constexpr.tc`:

```tc
define PAGE_SIZE constant size = 1 << 12;

define sizeof_test structure {
	test uint32;
};

define UNARY_MINUS_TEST constant sint32 = -PAGE_SIZE;
define SIZEOF_MINUS_TEST constant size = PAGE_SIZE - sizeof(structure sizeof_test);
define SIZEOF_MINUSES_TEST constant size = PAGE_SIZE - - - sizeof(structure sizeof_test);

define LOGICAL_AND_TEST constant bool = (1 == 1) && ((2 | 1) == 3);
define LOGICAL_OR_TEST constant bool = (1 == 0) || ((2 & 1) == 3);

```

`tools/typed/test/sample_inputs/object_define.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// 2019 Cog Systems Pty Ltd.
//
// SPDX-License-Identifier: BSD-3-Clause

define ARRAY_SIZE constant = 10;
define CSZ constant = 2;

define TestBitField bitfield<83> {
	31:0	test uint32;
	42:32	test1 uint16;
	50:43	test5 uint8;
};

define DemoPacked structure {
	int_var uint8;
	// this is a bit field
	bit_field_var bitfield TestBitField;
	tt uint32;
};

define NestUI object {
	t uint32;
	uy uint32;
	// illegal_offset @ 0x800 uint32;
};

define UI object {
	o sint32;
	p sint32;
	j array(ARRAY_SIZE) uint32;
	JJ array(ARRAY_SIZE) object NestUI;
	bit_field_in_obj bitfield TestBitField;
	N object NestUI;
	OO object() NestUI;
	N_noprefix object(noprefix) NestUI;
	ds structure DemoPacked;
};

extend UI object module Machine {
	e char;
	f char;
};

define Back object {
	k sint8;
	p sint8;
	TRI object Tripple;
};

define ExecContext object {
	a sint32;
	b sint32;
	O object UI;
	tt pointer object Back;
	yy array(8) object Back;
	jj array(ARRAY_SIZE) object Back;
	permitted_offset @ 0x800 uint32;
};

define Standalone object {
	t uint8;
	u uint8;
};


define Tripple object {
	tj sint8;
	tp uint8;
};

/* optional feature
define EmbeddedExecContext object {
	a sint8;
	b sint8;

	define EmbeddedUI object {
		o sint8;
		p sint8;
	};

	define EmbeddedBack object {
		k sint8;
		p sint8;
	};
};
*/

```

`tools/typed/test/sample_inputs/object_extend.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// 2019 Cog Systems Pty Ltd.
//
// SPDX-License-Identifier: BSD-3-Clause

extend ExecContext object module Machine {
	e char;
	f char;
};
/*
extend EmbeddedExecContext object module Machine {
	ee char;
	ff char;
};
*/

```

`tools/typed/test/sample_inputs/struct.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// 2019 Cog Systems Pty Ltd.
//
// SPDX-License-Identifier: BSD-3-Clause

define CSZ constant = 2;

define StructBitField bitfield<83> {
	31:0	test uint32;
	42:32	test1 uint16;
	58:51	test5 uint8;
};

define Demo2Packed structure {
	int_var uint32;
};

define DemoPacked structure {
	int_var uint8;
	// this is a bit field
	bit_field_var bitfield StructBitField;
	nested_struct structure Demo2Packed;
};

define DemoFixed structure {
	// before_start_var @ -0x1 uint8;
	start_var @ 0x0 uint8;
	cacheline_var @ 0x40 uint8;
	// overlapping_var @ 0x40 uint8;
	// backwards_var @ 0x3f uint8;
	// misaligned_var @ 0x41 uint64;
	next_cacheline_var @ 0x80 uint64;
};

```

`tools/typed/test/sample_inputs/union.tc`:

```tc
// © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
//
// SPDX-License-Identifier: BSD-3-Clause

define UCONST constant 2;

define UBitField bitfield<83> {
	31:0	test uint32;
	42:32	test1 uint16;
	58:51	test5 uint8;
};

define Demo2U union {
	int_var uint32;
	bit_var bitfield UBitField;
};

define DemoUStruct structure {
	int_var uint8;
	// this is a bit field
	bit_field_var bitfield UBitField;
	embedded_union union Demo2U;
};

```

`tools/typed/type_gen.py`:

```py
#!/usr/bin/env python3
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# 2019 Cog Systems Pty Ltd.
#
# SPDX-License-Identifier: BSD-3-Clause

from lark import Lark, ParseError
from exceptions import RangeError, DSLError
from ir import TransformTypes
from abi import AArch64ABI

import argparse
import sys
import os
import subprocess
import inspect
import logging
import pickle

logger = logging.getLogger(__name__)


abi_classes = {
    'aarch64': AArch64ABI,
}


def parse_dsl(parser, inputs, abi):
    trees = []
    for p in inputs:
        text = p.read()
        try:
            parse_tree = parser.parse(text)
            cur_tree = TransformTypes(text).transform(parse_tree)
            trees.append(cur_tree.get_intermediate_tree())
        except ParseError as e:
            raise Exception("Parse error in {:s}: {:s}".format(p.name,
                                                               str(e)))

    final_tree = trees.pop(0)
    for t in trees:
        final_tree.merge(t)

    final_tree.update(abi_classes[abi]())

    return final_tree


def apply_template(tree, template, public_only=False):
    if template is None:
        code = tree.gen_output(public_only=public_only)
    else:
        code = tree.apply_template(template, public_only=public_only)

    return code


def main():
    logging.basicConfig(
        level=logging.INFO,
        format="%(message)s",
    )
    __loc__ = os.path.realpath(
        os.path.dirname(os.path.join(os.getcwd(), os.path.dirname(__file__))))

    arg_parser = argparse.ArgumentParser()

    mode_args = arg_parser.add_mutually_exclusive_group(required=True)
    mode_args.add_argument('-P', '--dump-pickle', type=argparse.FileType('wb'),
                           help="Dump the IR to a Python pickle")
    mode_args.add_argument("-o", "--output",
                           help="Output file (default stdout)",
                           type=argparse.FileType('w', encoding='utf-8'),
                           default=sys.stdout)

    arg_parser.add_argument('-t', '--template',
                            type=argparse.FileType('r', encoding='utf-8'),
                            help="Template file used to generate output")
    arg_parser.add_argument('--public', action='store_true',
                            help="Include only public API types")
    arg_parser.add_argument('--traceback', action="store_true",
                            help="Print a full traceback if an error occurs")
    arg_parser.add_argument("-a", "--abi", help="specify the target machine "
                            "compiler ABI name", choices=abi_classes.keys(),
                            required=True)
    arg_parser.add_argument("-f", "--formatter",
                            help="specify clang-format to format the code")
    arg_parser.add_argument("-d", "--deps", default=None,
                            type=argparse.FileType('w', encoding='utf-8'),
                            help="write implicit dependencies to a Makefile")
    arg_parser.add_argument("input", metavar='INPUT', nargs="*",
                            type=argparse.FileType('r', encoding='utf-8'),
                            help="Input type DSL files to process")
    arg_parser.add_argument('-p', '--load-pickle',
                            type=argparse.FileType('rb'),
                            help="Load the IR from a Python pickle")

    options = arg_parser.parse_args()

    # Calling sanity checks
    if options.input and options.load_pickle:
        logger.error("Cannot specify both inputs and --load-pickle")
        arg_parser.print_usage()
        sys.exit(1)

    grammar_file = os.path.join(__loc__, 'grammars', 'typed_dsl.lark')

    parser = Lark.open(grammar_file, 'start', parser='lalr',
                       lexer='contextual', propagate_positions=True)

    if options.input:
        try:
            ir = parse_dsl(parser, options.input, options.abi)
        except (DSLError, RangeError) as e:
            if options.traceback:
                import traceback
                traceback.print_exc(file=sys.stderr)
            else:
                logger.error("Parse error", e)
            sys.exit(1)

        if options.dump_pickle:
            pickle.dump(ir, options.dump_pickle, protocol=4)
    elif options.load_pickle:
        ir = pickle.load(options.load_pickle)
    else:
        logger.error("Must specify inputs or --load-pickle")
        arg_parser.print_usage()
        sys.exit(1)

    if not options.dump_pickle:
        result = apply_template(ir, options.template,
                                public_only=options.public)

        if options.formatter:
            ret = subprocess.run([options.formatter],
                                 input=result.encode("utf-8"),
                                 stdout=subprocess.PIPE)
            result = ret.stdout.decode("utf-8")
            if ret.returncode != 0:
                logger.error("Error formatting output", result)
                sys.exit(1)

        options.output.write(result)
        options.output.close()

    if options.deps is not None:
        deps = set()
        deps.add(os.path.relpath(grammar_file))
        if options.template is not None:
            deps.add(options.template.name)
        for m in sys.modules.values():
            try:
                f = inspect.getsourcefile(m)
            except TypeError:
                continue
            if f is None:
                continue
            f = os.path.relpath(f)
            if f.startswith('../'):
                continue
            deps.add(f)
        if options.template is None:
            templates = ir.get_output_templates()
            for t in templates:
                deps.add(os.path.relpath(t))
        if options.dump_pickle:
            out_name = options.dump_pickle.name
        else:
            out_name = options.output.name
        options.deps.write(out_name + ' : ')
        options.deps.write(' '.join(sorted(deps)))
        options.deps.write('\n')
        options.deps.close()


if __name__ == '__main__':
    main()

```

`tools/utils/genfile.py`:

```py
# coding: utf-8
#
# © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

"""
"""

import sys
import io
import argparse


class _ReplaceFileMixin(object):
    def __init__(self, name, mode, encoding=None):
        super().__init__()
        self._name = name
        self._mode = mode
        self._encoding = encoding

    @property
    def name(self):
        return self._name

    def close(self):
        tmp = io.open(self._name, self._mode.replace('w', 'r'),
                      encoding=self._encoding)
        old = tmp.read()
        tmp.close()
        self.seek(0)
        new = self.read()
        if old != new:
            replace = io.open(self._name, self._mode, encoding=self._encoding)
            replace.write(new)
            replace.close()

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.close()


class _ReplaceBinaryFile(_ReplaceFileMixin, io.BytesIO):
    pass


class _ReplaceTextFile(_ReplaceFileMixin, io.StringIO):
    pass


class _GenFileFactory(object):
    def __init__(self, mode, encoding=None):
        self._mode = mode
        if mode not in ('w', 'wt', 'wb'):
            raise ValueError("mode {:s} not supported".format(mode))
        if encoding is None and 'b' not in mode:
            # Default to UTF-8 for text files
            encoding = 'utf-8'
        self._encoding = encoding

    def __call__(self, p):
        if sys.hexversion < 0x03030000:
            # Exclusive file creation ('x' mode) isn't available before Python
            # 3.3, so fall back to just replacing the file.
            return io.open(p, self._mode, encoding=self._encoding)

        try:
            return io.open(p, self._mode.replace('w', 'x'),
                           encoding=self._encoding)
        except FileExistsError:
            if 'b' in self._mode:
                return _ReplaceBinaryFile(p, self._mode)
            else:
                return _ReplaceTextFile(p, self._mode, encoding=self._encoding)


class GenFileType(_GenFileFactory, argparse.FileType):
    def __call__(self, p):
        if p == '-':
            assert 'w' in self._mode
            return sys.stdout
        try:
            return super().__call__(p)
        except OSError as e:
            raise argparse.ArgumentTypeError(
                "can't open {:s}: {:s}".format(p, str(e)))


def GenFile(name, mode, encoding=None):
    return _GenFileFactory(mode, encoding=encoding)(name)

```

`tools/vim/examples/cscope_gunyah.vim`:

```vim
" © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
"
" SPDX-License-Identifier: BSD-3-Clause
"
" This simple script, placed in VIMDIR/plugin/ will setup cscope for gunyah.

function! CScopeSetup()
   if filereadable("tools/misc/update_cscope.sh")
       let output=system('./tools/misc/update_cscope.sh')
       cscope reset
   endif
endfunction

call CScopeSetup()

```

`tools/vim/ftdetect/gunyah.vim`:

```vim
" TODO: more accurate file contents based detection
au BufRead,BufNewFile *.S		set filetype=arm64asm
au BufRead,BufNewFile *.inc		set filetype=arm64asm
au BufRead,BufNewFile *.ev		set filetype=events
au BufRead,BufNewFile *.tc		set filetype=types

```

`tools/vim/syntax/events.vim`:

```vim
" Vim syntax file
" Language:	Gunyah Event Files
" Maintainer:	Carl van Schaik <quic_cvanscha@quicinc.com>
" Extensions:   *.ev
"
" © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
"
" SPDX-License-Identifier: BSD-3-Clause

" quit when a syntax file was already loaded
"if exists("b:current_syntax")
"  finish
"endif

syn match evComment '\/\/.*$'

syn match moduleName contained /[_A-Za-z0-9]\+/
syn keyword module module interface contained nextgroup=moduleName skipwhite
syn match moduleDef '^\(module\|interface\)\>.*$' contains=module,evComment

syn match handlerName contained /[_A-Za-z0-9]\+/
syn keyword evHandler handler contained skipwhite nextgroup=handlerName
syn keyword evUnwinder unwinder contained skipwhite nextgroup=handlerName

syn match priorityNum contained /[0-9]\+\>/
syn keyword priorityKeyword contained first last
syn match priorityVal /[_A-Za-z0-9]\+/ contained contains=priorityNum,priorityKeyword
syn keyword priority priority contained skipwhite nextgroup=priorityVal

syn match lockAttrib /(acquire|release|require|exclude)_(lock|read|count)/ contained skipwhite

syn match evName contained /[_A-Za-z0-9]\+/
syn keyword evDirective subscribe event selector_event handled_event contained skipwhite nextgroup=evName

syn match evSubscriber '^subscribe\s.*$' skipnl nextgroup=evSubscriberBody contains=evComment,evDirective
syn match evSubscriberBody '^\(\s\+\).*$' contained skipnl nextgroup=evSubscriberBody contains=evComment,evHandler,evUnwinder,priority

" simple for now
"syn match evSep contained /:/ skipwhite nextgroup=evParam
syn match evParam contained /[_A-Za-z0-9]\+/ nextgroup=evSep
syn keyword evEventDef selector param return contained skipwhite nextgroup=evParam,evSep

syn match evEvent '^\(\(selector\|handled\)_\)\?event\s.*$' skipnl nextgroup=evEventBody contains=evComment,evDirective
syn match evEventBody '^\(\s\+\).*$' contained skipnl nextgroup=evEventBody contains=evComment,evEventDef

" TODO
" handler function params

hi def link evComment		Comment
hi def link evName 		Label
hi def link evDirective		Type
hi def link handlerName		Identifier
hi def link evParam		Identifier
hi def link evHandler		PreProc
hi def link evUnwinder		PreProc
hi def link evEventDef		PreProc
hi def link module		PreProc
hi def link priorityNum		Constant
hi def link priorityKeyword	Special
hi def link priority		PreProc
hi def link lockAttrib		PreProc
hi def link moduleName		Label

"hi def link evEventBody		TODO

let b:current_syntax = "events"

```

`tools/vim/syntax/types.vim`:

```vim
" Vim syntax file
" Language:	Gunyah Type Files
" Maintainer:	Carl van Schaik <quic_cvanscha@quicinc.com>
" Extensions:   *.tc
"
" © 2021 Qualcomm Innovation Center, Inc. All rights reserved.
"
" SPDX-License-Identifier: BSD-3-Clause

" quit when a syntax file was already loaded
"if exists("b:current_syntax")
"  finish
"endif

syn match tcComment '\/\/.*$' contains=@Spell

syn match Constant contained '\<\(0x[0-9a-fA-f]\+\|0b[01]\+\|[0-9]\+\)\>'
syn match Bits contained '<[0-9]\+>'
syn match TypeName contained /[_A-Za-z0-9]\+/

syn keyword tcBaseType contained sint8 sint16 sint32 sint64 bool uint8 uint16 uint32 uint64 char sintptr uintptr sregister uregister size
syn keyword tcQual contained pointer array atomic aligned lockable
syn keyword tcBitmap contained unknown

syn match tcContained contained "\%#=1\<contained\>"

syn match tcConstantEqual /=/ contained skipwhite nextgroup=Constant
syn keyword tcConstant constant contained skipwhite nextgroup=tcConstantEqual
syn keyword Aggregates type object public structure enumeration bitfield contained
syn keyword newtype newtype
syn region tcNewType start="newtype" end=";" contained contains=tcBaseType,tcQual,Constant,Aggregates,newtype
syn keyword tcDefine define contained skipwhite nextgroup=TypeName
syn keyword tcExtend extend contained skipwhite nextgroup=TypeName

"syn match assignment '=' skipwhite contained nextgroup=Constant
"hi def link assignment 	TODO

syn region tcDef start="\(define\|extend\)" end=";" keepend contains=tcDefine,tcExtend,Bits,Aggregates,tcConstant,tcNewType,tcQual,tcBlock,tcComment

"hi def link tcDef 		Error

syn region tcBlock start="{" end="}" contained extend contains=Constant,tcComment,tcBaseType,Aggregates,tcQual,tcBitmap,cppDirective,cppSingle,tcContained
"hi def link tcBlock 		TODO

syntax region cppDirective	display start="^\s*\zs\(%:\|#\)\s*\(define\|if\|ifdef\|elif\|error\)\>\s\+" skip="\\$" end="$" keepend contains=Constant
syntax match cppSingle	display /^\s*\zs\(%:\|#\)\s*\(endif\|else\)\>/

hi def link tcComment		Comment
hi def link tcDefine 		Identifier
hi def link tcExtend 		tcDefine
hi def link cppDirective	PreProc
hi def link cppSingle 		cppDirective
hi def link Aggregates 		Keyword
hi def link newtype		Keyword
hi def link tcQual 		Keyword
hi def link tcBitmap		Special
hi def link tcConstant 		Keyword
hi def link tcContained		Keyword
"hi def link TypeName		Define
hi def link tcBaseType		Type
hi def link Bits		Operator

let b:current_syntax = "types"

```