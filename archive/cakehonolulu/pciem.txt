Project Path: arc_cakehonolulu_pciem_ii04iht7

Source Tree:

```txt
arc_cakehonolulu_pciem_ii04iht7
├── LICENSE
├── Makefile
├── README.md
├── examples
│   ├── nvme
│   │   ├── Makefile
│   │   └── nvme_card.c
│   └── protopciem
│       ├── Makefile
│       ├── driver
│       │   ├── Makefile
│       │   └── protopciem_driver.c
│       ├── protopciem_device.h
│       ├── qemu
│       │   ├── protopciem_backend.c
│       │   ├── protopciem_backend.h
│       │   └── protopciem_cmds.h
│       └── userspace
│           ├── Makefile
│           └── protopciem_card.c
├── include
│   ├── pciem_capabilities.h
│   ├── pciem_dma.h
│   ├── pciem_framework.h
│   ├── pciem_p2p.h
│   ├── pciem_userspace.h
│   └── trace
│       └── smptrace.h
├── kernel
│   ├── Makefile
│   ├── framework
│   │   ├── pciem_capabilities.c
│   │   ├── pciem_dma.c
│   │   ├── pciem_framework.c
│   │   ├── pciem_p2p.c
│   │   └── pciem_userspace.c
│   └── trace
│       ├── arch
│       │   └── x86
│       │       ├── include
│       │       │   └── asm
│       │       │       ├── inat.h
│       │       │       ├── insn-eval.h
│       │       │       └── insn.h
│       │       └── lib
│       │           ├── inat-tables.c
│       │           ├── inat.c
│       │           ├── insn-eval.c
│       │           └── insn.c
│       └── smptrace_main.c
├── resources
│   └── icon.png
└── test_system.sh

```

`LICENSE`:

```
                    GNU GENERAL PUBLIC LICENSE
                       Version 2, June 1991

 Copyright (C) 1989, 1991 Free Software Foundation, Inc.,
 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
 Everyone is permitted to copy and distribute verbatim copies
 of this license document, but changing it is not allowed.

                            Preamble

  The licenses for most software are designed to take away your
freedom to share and change it.  By contrast, the GNU General Public
License is intended to guarantee your freedom to share and change free
software--to make sure the software is free for all its users.  This
General Public License applies to most of the Free Software
Foundation's software and to any other program whose authors commit to
using it.  (Some other Free Software Foundation software is covered by
the GNU Lesser General Public License instead.)  You can apply it to
your programs, too.

  When we speak of free software, we are referring to freedom, not
price.  Our General Public Licenses are designed to make sure that you
have the freedom to distribute copies of free software (and charge for
this service if you wish), that you receive source code or can get it
if you want it, that you can change the software or use pieces of it
in new free programs; and that you know you can do these things.

  To protect your rights, we need to make restrictions that forbid
anyone to deny you these rights or to ask you to surrender the rights.
These restrictions translate to certain responsibilities for you if you
distribute copies of the software, or if you modify it.

  For example, if you distribute copies of such a program, whether
gratis or for a fee, you must give the recipients all the rights that
you have.  You must make sure that they, too, receive or can get the
source code.  And you must show them these terms so they know their
rights.

  We protect your rights with two steps: (1) copyright the software, and
(2) offer you this license which gives you legal permission to copy,
distribute and/or modify the software.

  Also, for each author's protection and ours, we want to make certain
that everyone understands that there is no warranty for this free
software.  If the software is modified by someone else and passed on, we
want its recipients to know that what they have is not the original, so
that any problems introduced by others will not reflect on the original
authors' reputations.

  Finally, any free program is threatened constantly by software
patents.  We wish to avoid the danger that redistributors of a free
program will individually obtain patent licenses, in effect making the
program proprietary.  To prevent this, we have made it clear that any
patent must be licensed for everyone's free use or not licensed at all.

  The precise terms and conditions for copying, distribution and
modification follow.

                    GNU GENERAL PUBLIC LICENSE
   TERMS AND CONDITIONS FOR COPYING, DISTRIBUTION AND MODIFICATION

  0. This License applies to any program or other work which contains
a notice placed by the copyright holder saying it may be distributed
under the terms of this General Public License.  The "Program", below,
refers to any such program or work, and a "work based on the Program"
means either the Program or any derivative work under copyright law:
that is to say, a work containing the Program or a portion of it,
either verbatim or with modifications and/or translated into another
language.  (Hereinafter, translation is included without limitation in
the term "modification".)  Each licensee is addressed as "you".

Activities other than copying, distribution and modification are not
covered by this License; they are outside its scope.  The act of
running the Program is not restricted, and the output from the Program
is covered only if its contents constitute a work based on the
Program (independent of having been made by running the Program).
Whether that is true depends on what the Program does.

  1. You may copy and distribute verbatim copies of the Program's
source code as you receive it, in any medium, provided that you
conspicuously and appropriately publish on each copy an appropriate
copyright notice and disclaimer of warranty; keep intact all the
notices that refer to this License and to the absence of any warranty;
and give any other recipients of the Program a copy of this License
along with the Program.

You may charge a fee for the physical act of transferring a copy, and
you may at your option offer warranty protection in exchange for a fee.

  2. You may modify your copy or copies of the Program or any portion
of it, thus forming a work based on the Program, and copy and
distribute such modifications or work under the terms of Section 1
above, provided that you also meet all of these conditions:

    a) You must cause the modified files to carry prominent notices
    stating that you changed the files and the date of any change.

    b) You must cause any work that you distribute or publish, that in
    whole or in part contains or is derived from the Program or any
    part thereof, to be licensed as a whole at no charge to all third
    parties under the terms of this License.

    c) If the modified program normally reads commands interactively
    when run, you must cause it, when started running for such
    interactive use in the most ordinary way, to print or display an
    announcement including an appropriate copyright notice and a
    notice that there is no warranty (or else, saying that you provide
    a warranty) and that users may redistribute the program under
    these conditions, and telling the user how to view a copy of this
    License.  (Exception: if the Program itself is interactive but
    does not normally print such an announcement, your work based on
    the Program is not required to print an announcement.)

These requirements apply to the modified work as a whole.  If
identifiable sections of that work are not derived from the Program,
and can be reasonably considered independent and separate works in
themselves, then this License, and its terms, do not apply to those
sections when you distribute them as separate works.  But when you
distribute the same sections as part of a whole which is a work based
on the Program, the distribution of the whole must be on the terms of
this License, whose permissions for other licensees extend to the
entire whole, and thus to each and every part regardless of who wrote it.

Thus, it is not the intent of this section to claim rights or contest
your rights to work written entirely by you; rather, the intent is to
exercise the right to control the distribution of derivative or
collective works based on the Program.

In addition, mere aggregation of another work not based on the Program
with the Program (or with a work based on the Program) on a volume of
a storage or distribution medium does not bring the other work under
the scope of this License.

  3. You may copy and distribute the Program (or a work based on it,
under Section 2) in object code or executable form under the terms of
Sections 1 and 2 above provided that you also do one of the following:

    a) Accompany it with the complete corresponding machine-readable
    source code, which must be distributed under the terms of Sections
    1 and 2 above on a medium customarily used for software interchange; or,

    b) Accompany it with a written offer, valid for at least three
    years, to give any third party, for a charge no more than your
    cost of physically performing source distribution, a complete
    machine-readable copy of the corresponding source code, to be
    distributed under the terms of Sections 1 and 2 above on a medium
    customarily used for software interchange; or,

    c) Accompany it with the information you received as to the offer
    to distribute corresponding source code.  (This alternative is
    allowed only for noncommercial distribution and only if you
    received the program in object code or executable form with such
    an offer, in accord with Subsection b above.)

The source code for a work means the preferred form of the work for
making modifications to it.  For an executable work, complete source
code means all the source code for all modules it contains, plus any
associated interface definition files, plus the scripts used to
control compilation and installation of the executable.  However, as a
special exception, the source code distributed need not include
anything that is normally distributed (in either source or binary
form) with the major components (compiler, kernel, and so on) of the
operating system on which the executable runs, unless that component
itself accompanies the executable.

If distribution of executable or object code is made by offering
access to copy from a designated place, then offering equivalent
access to copy the source code from the same place counts as
distribution of the source code, even though third parties are not
compelled to copy the source along with the object code.

  4. You may not copy, modify, sublicense, or distribute the Program
except as expressly provided under this License.  Any attempt
otherwise to copy, modify, sublicense or distribute the Program is
void, and will automatically terminate your rights under this License.
However, parties who have received copies, or rights, from you under
this License will not have their licenses terminated so long as such
parties remain in full compliance.

  5. You are not required to accept this License, since you have not
signed it.  However, nothing else grants you permission to modify or
distribute the Program or its derivative works.  These actions are
prohibited by law if you do not accept this License.  Therefore, by
modifying or distributing the Program (or any work based on the
Program), you indicate your acceptance of this License to do so, and
all its terms and conditions for copying, distributing or modifying
the Program or works based on it.

  6. Each time you redistribute the Program (or any work based on the
Program), the recipient automatically receives a license from the
original licensor to copy, distribute or modify the Program subject to
these terms and conditions.  You may not impose any further
restrictions on the recipients' exercise of the rights granted herein.
You are not responsible for enforcing compliance by third parties to
this License.

  7. If, as a consequence of a court judgment or allegation of patent
infringement or for any other reason (not limited to patent issues),
conditions are imposed on you (whether by court order, agreement or
otherwise) that contradict the conditions of this License, they do not
excuse you from the conditions of this License.  If you cannot
distribute so as to satisfy simultaneously your obligations under this
License and any other pertinent obligations, then as a consequence you
may not distribute the Program at all.  For example, if a patent
license would not permit royalty-free redistribution of the Program by
all those who receive copies directly or indirectly through you, then
the only way you could satisfy both it and this License would be to
refrain entirely from distribution of the Program.

If any portion of this section is held invalid or unenforceable under
any particular circumstance, the balance of the section is intended to
apply and the section as a whole is intended to apply in other
circumstances.

It is not the purpose of this section to induce you to infringe any
patents or other property right claims or to contest validity of any
such claims; this section has the sole purpose of protecting the
integrity of the free software distribution system, which is
implemented by public license practices.  Many people have made
generous contributions to the wide range of software distributed
through that system in reliance on consistent application of that
system; it is up to the author/donor to decide if he or she is willing
to distribute software through any other system and a licensee cannot
impose that choice.

This section is intended to make thoroughly clear what is believed to
be a consequence of the rest of this License.

  8. If the distribution and/or use of the Program is restricted in
certain countries either by patents or by copyrighted interfaces, the
original copyright holder who places the Program under this License
may add an explicit geographical distribution limitation excluding
those countries, so that distribution is permitted only in or among
countries not thus excluded.  In such case, this License incorporates
the limitation as if written in the body of this License.

  9. The Free Software Foundation may publish revised and/or new versions
of the General Public License from time to time.  Such new versions will
be similar in spirit to the present version, but may differ in detail to
address new problems or concerns.

Each version is given a distinguishing version number.  If the Program
specifies a version number of this License which applies to it and "any
later version", you have the option of following the terms and conditions
either of that version or of any later version published by the Free
Software Foundation.  If the Program does not specify a version number of
this License, you may choose any version ever published by the Free Software
Foundation.

  10. If you wish to incorporate parts of the Program into other free
programs whose distribution conditions are different, write to the author
to ask for permission.  For software which is copyrighted by the Free
Software Foundation, write to the Free Software Foundation; we sometimes
make exceptions for this.  Our decision will be guided by the two goals
of preserving the free status of all derivatives of our free software and
of promoting the sharing and reuse of software generally.

                            NO WARRANTY

  11. BECAUSE THE PROGRAM IS LICENSED FREE OF CHARGE, THERE IS NO WARRANTY
FOR THE PROGRAM, TO THE EXTENT PERMITTED BY APPLICABLE LAW.  EXCEPT WHEN
OTHERWISE STATED IN WRITING THE COPYRIGHT HOLDERS AND/OR OTHER PARTIES
PROVIDE THE PROGRAM "AS IS" WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESSED
OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF
MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE.  THE ENTIRE RISK AS
TO THE QUALITY AND PERFORMANCE OF THE PROGRAM IS WITH YOU.  SHOULD THE
PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF ALL NECESSARY SERVICING,
REPAIR OR CORRECTION.

  12. IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING
WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MAY MODIFY AND/OR
REDISTRIBUTE THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES,
INCLUDING ANY GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING
OUT OF THE USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED
TO LOSS OF DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY
YOU OR THIRD PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER
PROGRAMS), EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE
POSSIBILITY OF SUCH DAMAGES.

                     END OF TERMS AND CONDITIONS

            How to Apply These Terms to Your New Programs

  If you develop a new program, and you want it to be of the greatest
possible use to the public, the best way to achieve this is to make it
free software which everyone can redistribute and change under these terms.

  To do so, attach the following notices to the program.  It is safest
to attach them to the start of each source file to most effectively
convey the exclusion of warranty; and each file should have at least
the "copyright" line and a pointer to where the full notice is found.

    <one line to give the program's name and a brief idea of what it does.>
    Copyright (C) <year>  <name of author>

    This program is free software; you can redistribute it and/or modify
    it under the terms of the GNU General Public License as published by
    the Free Software Foundation; either version 2 of the License, or
    (at your option) any later version.

    This program is distributed in the hope that it will be useful,
    but WITHOUT ANY WARRANTY; without even the implied warranty of
    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
    GNU General Public License for more details.

    You should have received a copy of the GNU General Public License along
    with this program; if not, write to the Free Software Foundation, Inc.,
    51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.

Also add information on how to contact you by electronic and paper mail.

If the program is interactive, make it output a short notice like this
when it starts in an interactive mode:

    Gnomovision version 69, Copyright (C) year name of author
    Gnomovision comes with ABSOLUTELY NO WARRANTY; for details type `show w'.
    This is free software, and you are welcome to redistribute it
    under certain conditions; type `show c' for details.

The hypothetical commands `show w' and `show c' should show the appropriate
parts of the General Public License.  Of course, the commands you use may
be called something other than `show w' and `show c'; they could even be
mouse-clicks or menu items--whatever suits your program.

You should also get your employer (if you work as a programmer) or your
school, if any, to sign a "copyright disclaimer" for the program, if
necessary.  Here is a sample; alter the names:

  Yoyodyne, Inc., hereby disclaims all copyright interest in the program
  `Gnomovision' (which makes passes at compilers) written by James Hacker.

  <signature of Ty Coon>, 1 April 1989
  Ty Coon, President of Vice

This General Public License does not permit incorporating your program into
proprietary programs.  If your program is a subroutine library, you may
consider it more useful to permit linking proprietary applications with the
library.  If this is what you want to do, use the GNU Lesser General
Public License instead of this License.

```

`Makefile`:

```
KDIR ?= /lib/modules/$(shell uname -r)/build
GCC ?= gcc
PWD := $(shell pwd)

THIS_MAKEFILE     := $(lastword $(MAKEFILE_LIST))
TOP_DIR           := $(abspath $(dir $(THIS_MAKEFILE)))
EXAMPLES_DIR      := $(TOP_DIR)/examples
EXAMPLE_MAKEFILES := $(wildcard $(EXAMPLES_DIR)/*/Makefile)
EXAMPLES          := $(notdir $(patsubst %/Makefile,%,$(EXAMPLE_MAKEFILES)))

all: modules examples

modules:
	$(MAKE) -C $(KDIR) M=$(PWD)/kernel modules

examples: $(EXAMPLES)

$(EXAMPLES):
	$(MAKE) -C $(EXAMPLES_DIR)/$@

clean:
	$(MAKE) -C $(KDIR) M=$(PWD)/kernel clean
	for e in $(EXAMPLES); do \
		$(MAKE) -C $(EXAMPLES_DIR)/$$e clean; \
	done

.PHONY: all modules examples clean $(EXAMPLES)

```

`README.md`:

```md
<div align="center">
  <img src="resources/icon.png">
</div>

<div align="center">
  A Linux kernel framework enabling synthetic userspace PCIe device emulation.
</div>

<div align="center">
  https://cakehonolulu.github.io/introducing-pciem/

  https://cakehonolulu.github.io/docs/pciem/
</div>

[![CI](https://github.com/cakehonolulu/pciem/actions/workflows/ci.yml/badge.svg?branch=main)](https://github.com/cakehonolulu/pciem/actions/workflows/ci.yml)

## What is PCIem?

PCIem is a framework that creates virtual PCIe devices in the Linux kernel by leveraging a few novel techniques to populate synthetic cards as legitimate PCI devices to the host OS.

To brief what PCIem is: a framework for (Albeit not limited to) developing and testing PCIe device drivers without requiring actual hardware on the host.

## Comparison with libfvio-user

`PCIem` and `libfvio-user` are two different solutions for different needs, there may be confusion when comparing both so herein the differences (See [figure 1](#figure1) for more).

The main one is that `libvfio-user` usually relies on a client (That implements the `vfio-user` protocol), usually QEMU (Through KVM, using VM exits) to expose the emulated PCIe device to the guest. You write your `vfio` server (With a callback mechanism usually) which then interacts with the client.

What `PCIem` does instead is, expose the device *directly* on the host; no KVM, no guests, no virtual machines, nothing. The device appears on the host's PCIe bus as if it was physically connected.

<div align="center" id="figure1">

| Feature | PCIem | libfvio-user |
| :--- | :--- | :--- |
| **Connection** | Device file (`/dev/pciem`) | UNIX Sockets (`vfio-user` protocol) |
| **Target driver runs on** | Host | Guest OS |
| **Emulated device runs on** | Userspace | Userspace |
| **Device accesses** | Direct (Within host) | Virtualized (Guest to Host) |

_Figure 1: Comparison between frameworks_

</div>

## Architecture

```mermaid
graph LR
    subgraph Kernel ["Host Linux Kernel"]
        direction TB

        RealDriver["Real PCIe Driver"]

        subgraph Framework ["PCIem Framework"]
            direction TB
            Config["PCI Config Space"]
            BARs["BARs"]
            IRQ["Interrupts"]
            DMA["DMA / IOMMU"]
        end

    end

    Interface(("/dev/pciem"))

    subgraph User ["Linux Userspace"]
        direction TB
        Shim["Device Emulation"]
    end

    Framework <==> Interface
    Interface <==> Shim
```

## Current Features

- **BAR creation**: Programmatically create and monitor BAR regions.
- **BAR MMIO tracing**: MMIO read/write detection and notification to avoid polling for accesses.
- **Interrupts support**: Legacy/MSI/MSI-X interrupt injection.
- **PCI capability framework**: PCI capabilities system (Linked-list underneath).
- **DMA system**: IOMMU-aware DMA accesses from/to userspace device.
  - **P2P support (Preliminar!)**: Peer-to-peer DMA between devices with whitelist-based access control.
- **Userspace**: Implement your PCIe devices _entirely_ in userspace.

# Examples

## NVME Controller with 1GB disk

The following example basically shows an NVME controller with 1GB of storage attached to. User can freely format, mount, create and remove files from the memory.

https://github.com/user-attachments/assets/6cc9ef8b-72f9-4a0b-b54e-6fbdde7c7589

## ProtoPCIem card

The card is programmed entirely in QEMU (State machine for the card, basically), which does all the userspace initialization and command handling from the real driver running in the host.

Can run software-rendered DOOM (Submits finished frames with DMA to the card which QEMU displays) and also simple OpenGL 1.X games (On the screenshots, tyr-glquake and xash3d; thanks to a custom OpenGL state machine implemented entirely in QEMU that software-renders the command lists and updates the internal state accordingly).

<details>
  <summary>Screenshots</summary>
<p align="center">
  <img width="1903" height="1029" alt="imagen" src="https://github.com/user-attachments/assets/16f64475-ee51-4f79-ae17-b06363f0b12a" />
</p>

<p align="center">
  <img width="1757" height="893" alt="imagen" src="https://github.com/user-attachments/assets/4ad00e14-83e5-4e1f-b374-fbaa92def4e3" />
</p>

<p align="center">
  <img width="1227" height="846" alt="imagen" src="https://github.com/user-attachments/assets/d21a7d84-f857-4790-bdc6-7bf2714e9eda" />
</p>

</details>

## License

* PCIem kernel components: GPLv2
* Examples: dual MIT/GPLv2.

## References

- Blog post: https://cakehonolulu.github.io/introducing-pciem/
- Documentation: https://cakehonolulu.github.io/docs/pciem/
- Hackernews post: https://news.ycombinator.com/item?id=46689065
- PCI Express specification: https://pcisig.com/specifications

```

`examples/nvme/Makefile`:

```
THIS_MAKEFILE := $(lastword $(MAKEFILE_LIST))
CUR_DIR    := $(abspath $(dir $(THIS_MAKEFILE)))

CFLAGS := -Wall -Wextra -O2 -I$(CUR_DIR)/../../include -ggdb

nvme_card: nvme_card.c
	$(CC) $(CFLAGS) -o $@ $^

clean:
	rm -f nvme_card

```

`examples/nvme/nvme_card.c`:

```c
// SPDX-License-Identifier: GPL-2.0 OR MIT
/*
 *  Copyright (C) 2026  Joel Bueno <buenocalvachejoel@gmail.com>
 *  Copyright (C) 2026  Carlos López <carlos.lopezr4096@gmail.com>
 */
#include <assert.h>
#include <err.h>
#include <errno.h>
#include <fcntl.h>
#include <poll.h>
#include <stdbool.h>
#include <stdint.h>
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <sys/eventfd.h>
#include <sys/ioctl.h>
#include <sys/mman.h>
#include <sys/poll.h>
#include <time.h>
#include <unistd.h>

#include "pciem_userspace.h"

#define NVME_REG_CAP                0x00
#define NVME_REG_VS                 0x08
#define NVME_REG_INTMS              0x0c
#define NVME_REG_INTMC              0x10
#define NVME_REG_CC                 0x14
#define NVME_REG_CSTS               0x1c
#define NVME_REG_AQA                0x24
#define NVME_REG_ASQ_LO             0x28
#define NVME_REG_ASQ_HI             0x2c
#define NVME_REG_ACQ_LO             0x30
#define NVME_REG_ACQ_HI             0x34
#define NVME_REG_SQ0TDBL            0x1000
#define NVME_REG_CQ0HDBL            0x1004

#define NVME_CC_ENABLE              (1 << 0)
#define NVME_CC_CSS_NVM             (0 << 4)
#define NVME_CC_IOSQES              (6 << 16)
#define NVME_CC_IOCQES              (4 << 20)

#define NVME_CSTS_RDY               (1 << 0)
#define NVME_CSTS_CFS               (1 << 1)

#define NVME_ADMIN_DELETE_SQ        0x00
#define NVME_ADMIN_CREATE_SQ        0x01
#define NVME_ADMIN_DELETE_CQ        0x04
#define NVME_ADMIN_CREATE_CQ        0x05
#define NVME_ADMIN_IDENTIFY         0x06
#define NVME_ADMIN_SET_FEATURES     0x09
#define NVME_ADMIN_GET_FEATURES     0x0a

#define NVME_CMD_FLUSH              0x00
#define NVME_CMD_WRITE              0x01
#define NVME_CMD_READ               0x02

#define NVME_SCT_GENERIC            0x0
#define NVME_SCT_COMMAND_SPEC       0x1
#define NVME_SCT_MEDIA_ERROR        0x2
#define NVME_SCT_PATH_ERROR         0x3

#define NVME_SC_SUCCESS                  0x00
#define NVME_SC_INVALID_FIELD            0x02
#define NVME_SC_INTERNAL                 0x06
#define NVME_SC_INVALID_NS               0x0b
#define NVME_SC_OPERATION_DENIED         0x15

#define NVME_SC_INVALID_QUEUE            0x00
#define NVME_SC_INVALID_QUEUE_SIZE       0x01
#define NVME_SC_INVALID_INTERRUPT_VECTOR 0x02
#define NVME_SC_INVALID_QUEUE_DELETE     0x03
#define NVME_SC_INVALID_QUEUE_CREATE     0x04

#define NVME_STATUS(sct, sc)       (((sct) << 8 | (sc)) << 1)
#define NVME_SUCCESS                NVME_STATUS(NVME_SCT_GENERIC, NVME_SC_SUCCESS)
#define NVME_INVALID_FIELD          NVME_STATUS(NVME_SCT_GENERIC, NVME_SC_INVALID_FIELD)
#define NVME_INTERNAL               NVME_STATUS(NVME_SCT_GENERIC, NVME_SC_INTERNAL)
#define NVME_INVALID_NS             NVME_STATUS(NVME_SCT_GENERIC, NVME_SC_INVALID_NS)
#define NVME_OPERATION_DENIED       NVME_STATUS(NVME_SCT_GENERIC, NVME_SC_OPERATION_DENIED)

#define NVME_INVALID_QUEUE          NVME_STATUS(NVME_SCT_COMMAND_SPEC, NVME_SC_INVALID_QUEUE)
#define NVME_INVALID_QUEUE_CREATE   NVME_STATUS(NVME_SCT_COMMAND_SPEC, NVME_SC_INVALID_QUEUE_CREATE)
#define NVME_INVALID_QUEUE_DELETE   NVME_STATUS(NVME_SCT_COMMAND_SPEC, NVME_SC_INVALID_QUEUE_DELETE)

#define NVME_SECTOR_SIZE            4096
#define GB                          (1024UL * 1024UL * 1024UL)
#define NVME_DISK_SIZE_GB           1
#define NVME_DISK_SIZE              NVME_DISK_SIZE_GB * GB
#define NVME_TOTAL_SECTORS          ((NVME_DISK_SIZE_GB * 1024UL * 1024 * 1024) / NVME_SECTOR_SIZE)
#define MAX_QUEUES                  16

#define NVME_SHN_NONE     0
#define NVME_SHN_NORMAL   1
#define NVME_SHN_ABRUPT   2

#define NVME_SHST_NORMAL   0
#define NVME_SHST_PROGRESS 1
#define NVME_SHST_COMPLETE 2

enum nvme_feature_id {
    NVME_FEAT_ARBITRATION           = 0x01,
    NVME_FEAT_POWER_MGMT            = 0x02,
    NVME_FEAT_LBA_RANGE             = 0x03,
    NVME_FEAT_TEMP_THRESHOLD        = 0x04,
    NVME_FEAT_ERROR_RECOVERY        = 0x05,
    NVME_FEAT_VOLATILE_WC           = 0x06,
    NVME_FEAT_NUM_QUEUES            = 0x07,
    NVME_FEAT_IRQ_COALESCE          = 0x08,
    NVME_FEAT_IRQ_CONFIG            = 0x09,
    NVME_FEAT_WRITE_ATOMIC          = 0x0A,
    NVME_FEAT_ASYNC_EVENT           = 0x0B,
    NVME_FEAT_AUTO_PST              = 0x0C,
    NVME_FEAT_HOST_MEM_BUF          = 0x0D,
    NVME_FEAT_TIMESTAMP             = 0x0E,
    NVME_FEAT_KATO                  = 0x0F,
    NVME_FEAT_HCTM                  = 0x10,
    NVME_FEAT_NOPSC                 = 0x11,
    NVME_FEAT_RRL                   = 0x12,
    NVME_FEAT_PLM_CONFIG            = 0x13,
    NVME_FEAT_PLM_WINDOW            = 0x14,
    NVME_FEAT_HOST_BEHAVIOR         = 0x16,
};

struct nvme_cc {
    uint32_t en     : 1;  /* bit 0 */
    uint32_t rsvd1  : 13;
    uint8_t  shn    : 2;  /* bits 15:14 */
    uint32_t rsvd2  : 16;
} __attribute__((packed));

struct nvme_csts {
    uint32_t rdy    : 1;  /* bit 0 */
    uint32_t rsvd1  : 1;
    uint32_t shst   : 2;  /* bits 3:2 */
    uint32_t rsvd2  : 28;
} __attribute__((packed));

struct nvme_version {
    uint32_t major : 8;   /* bits 7:0 */
    uint32_t minor : 8;   /* bits 15:8 */
    uint32_t rsvd  : 16;  /* bits 31:16 */
} __attribute__((packed));

struct nvme_cap {
    uint64_t mqes    : 16; /* Max Queue Entries Supported (0-based) */
    uint64_t to      : 8;  /* Timeout in 500ms units */
    uint64_t dstrd   : 4;  /* Doorbell stride */
    uint64_t nssrs   : 1;  /* NVM Subsystem Reset Supported */
    uint64_t css     : 8;  /* Command Sets Supported */
    uint64_t rsvd1   : 3;
    uint64_t mpsmin  : 4;  /* Minimum page size */
    uint64_t mpsmax  : 4;  /* Maximum page size */
    uint64_t rsvd2   : 16;
} __attribute__((packed));

struct nvme_lbaf {
	uint16_t ms;
	uint8_t ds;
	uint8_t rp;
};

struct nvme_id_ns {
	uint64_t nsze;
	uint64_t ncap;
	uint64_t nuse;
	uint8_t nsfeat;
	uint8_t nlbaf;
	uint8_t flbas;
	uint8_t mc;
	uint8_t dpc;
	uint8_t dps;
	uint8_t nmic;
	uint8_t rescap;
	uint8_t fpi;
	uint8_t dlfeat;
	uint16_t nawun;
	uint16_t nawupf;
	uint16_t nacwu;
	uint16_t nabsn;
	uint16_t nabo;
	uint16_t nabspf;
	uint16_t noiob;
	uint8_t nvmcap[16];
	uint16_t npwg;
	uint16_t npwa;
	uint16_t npdg;
	uint16_t npda;
	uint16_t nows;
	uint8_t rsvd74[18];
	uint32_t anagrpid;
	uint8_t rsvd96[3];
	uint8_t nsattr;
	uint16_t nvmsetid;
	uint16_t endgid;
	uint8_t nguid[16];
	uint8_t eui64[8];
	struct nvme_lbaf lbaf[64];
	uint8_t vs[3712];
};

_Static_assert(sizeof(struct nvme_id_ns) == 4096, "nvme_id_ns");

struct nvme_id_power_state {
	uint16_t max_power;
	uint8_t rsvd2;
	uint8_t flags;
	uint32_t entry_lat;
	uint32_t exit_lat;
	uint8_t read_tput;
	uint8_t read_lat;
	uint8_t write_tput;
	uint8_t write_lat;
	uint16_t idle_power;
	uint8_t idle_scale;
	uint8_t rsvd19;
	uint16_t active_power;
	uint8_t active_work_scale;
	uint8_t rsvd23[9];
};

struct nvme_ctrl_id {
	uint16_t vid;
	uint16_t ssvid;
	char sn[20];
	char mn[40];
	char fr[8];
	uint8_t rab;
	uint8_t ieee[3];
	uint8_t cmic;
	uint8_t mdts;
	uint16_t cntlid;
	uint32_t ver;
	uint32_t rtd3r;
	uint32_t rtd3e;
	uint32_t oaes;
	uint32_t ctratt;
	uint8_t rsvd100[11];
	uint8_t cntrltype;
	uint8_t fguid[16];
	uint16_t crdt1;
	uint16_t crdt2;
	uint16_t crdt3;
	uint8_t rsvd134[122];
	uint16_t oacs;
	uint8_t acl;
	uint8_t aerl;
	uint8_t frmw;
	uint8_t lpa;
	uint8_t elpe;
	uint8_t npss;
	uint8_t avscc;
	uint8_t apsta;
	uint16_t wctemp;
	uint16_t cctemp;
	uint16_t mtfa;
	uint32_t hmpre;
	uint32_t hmmin;
	uint8_t tnvmcap[16];
	uint8_t unvmcap[16];
	uint32_t rpmbs;
	uint16_t edstt;
	uint8_t dsto;
	uint8_t fwug;
	uint16_t kas;
	uint16_t hctma;
	uint16_t mntmt;
	uint16_t mxtmt;
	uint32_t sanicap;
	uint32_t hmminds;
	uint16_t hmmaxd;
	uint16_t nvmsetidmax;
	uint16_t endgidmax;
	uint8_t anatt;
	uint8_t anacap;
	uint32_t anagrpmax;
	uint32_t nanagrpid;
	uint8_t rsvd352[160];
	uint8_t sqes;
	uint8_t cqes;
	uint16_t maxcmd;
	uint32_t nn;
	uint16_t oncs;
	uint16_t fuses;
	uint8_t fna;
	uint8_t vwc;
	uint16_t awun;
	uint16_t awupf;
	uint8_t nvscc;
	uint8_t nwpc;
	uint16_t acwu;
	uint8_t rsvd534[2];
	uint32_t sgls;
	uint32_t mnan;
	uint8_t rsvd544[224];
	char subnqn[256];
	uint8_t rsvd1024[768];
	uint32_t ioccsz;
	uint32_t iorcsz;
	uint16_t icdoff;
	uint8_t ctrattr;
	uint8_t msdbd;
	uint8_t rsvd1804[2];
	uint8_t dctype;
	uint8_t rsvd1807[241];
	struct nvme_id_power_state psd[32];
	uint8_t vs[1024];
};

_Static_assert(sizeof(struct nvme_ctrl_id) == 4096, "nvme_ctrl_id");

union cdw10_t {
    uint32_t raw;

    struct {
        uint8_t cns;
        uint8_t rsvd;
        uint16_t nsid;
    } identify;

    struct {
        uint16_t qid;
        uint16_t qsize;
    } create_io_cq;

    struct {
        uint16_t qid;
        uint16_t qsize;
    } create_io_sq;

    struct {
        uint16_t qid;
        uint16_t rsvd;
    } delete_io_cq;

    struct {
        uint16_t qid;
        uint16_t rsvd;
    } delete_io_sq;

    struct {
        uint8_t fid;
        uint8_t sel : 3;
        uint8_t rsvd : 5;
        uint16_t rsvd2;
    } features;
};

union cdw11_t {
    uint32_t raw;

    struct {
        uint16_t vector;
        uint8_t ien : 1;
        uint8_t reserved : 7;
        uint8_t phys_cont : 1;
        uint8_t rsvd : 7;
    } create_io_cq;

    struct {
        uint16_t prp1;
        uint16_t rsvd;
    } create_io_sq;

    struct {
        uint16_t nsqa;
        uint16_t ncqa;
    } feat_num_queues;
};

union cdw12_t {
    uint32_t raw;

    struct {
        uint16_t nlb;
        uint8_t  rsvd;
        uint8_t  flags;
    } rw;
};

struct nvme_command {
    uint8_t  opcode;
    uint8_t  flags;
    uint16_t command_id;
    uint32_t nsid;
    uint64_t rsvd2;
    uint64_t metadata;
    uint64_t prp1;
    uint64_t prp2;
    union cdw10_t cdw10;
    union cdw11_t cdw11;
    union cdw12_t cdw12;
    uint32_t cdw13;
    uint32_t cdw14;
    uint32_t cdw15;
} __attribute__((packed));

struct nvme_completion {
    uint32_t result;
    uint32_t rsvd;
    uint16_t sq_head;
    uint16_t sq_id;
    uint16_t command_id;
    uint16_t status;
} __attribute__((packed));

struct nvme_queue {
    bool created;
    uint16_t sq_size;
    uint16_t cq_size;
    uint64_t sq_addr;
    uint64_t cq_addr;
    uint16_t sq_tail;
    uint16_t sq_head;
    uint16_t cq_tail;
    uint16_t cq_head;
    uint8_t cq_phase;
    uint16_t vector;
    uint16_t id;
};

#define NVME_VS(major, minor, tertiary) \
	(((major) << 16) | ((minor) << 8) | (tertiary))

struct nvme_device {
    int fd;
    int instance_fd;
    void *bar0;
    size_t bar0_size;

    struct pciem_shared_ring *ring;
    int event_fd;

    bool running;
    bool enabled;
    uint32_t csts;

    struct nvme_queue queues[MAX_QUEUES];
    struct nvme_ctrl_id ctrl_id;

    uint8_t *disk_data;
};

#define logx(fmt, ...)                                                \
    do {                                                              \
        struct timespec __ts;                                         \
        clock_gettime(CLOCK_BOOTTIME, &__ts);                          \
        warnx("[%5ld.%06ld] " fmt,                           \
              __ts.tv_sec, __ts.tv_nsec / 1000,                     \
              ##__VA_ARGS__);                                        \
    } while (0)

#define log(fmt, ...)                                                \
    do {                                                              \
        struct timespec __ts;                                         \
        clock_gettime(CLOCK_BOOTTIME, &__ts);                          \
        warn("[%5ld.%06ld] " fmt,                           \
              __ts.tv_sec, __ts.tv_nsec / 1000,                     \
              ##__VA_ARGS__);                                        \
    } while (0)

static uint64_t size2mask(uint64_t size)
{
    return size >= 8
        ? 0xffffffffffffffff
        : (1ULL << (size << 3)) - 1;
}

static const char *reg2str(uint64_t off)
{
    switch (off) {
        case NVME_REG_CAP: return "REG_CAP";
        case NVME_REG_VS: return "REG_VS";
        case NVME_REG_CC: return "REG_CC";
        case NVME_REG_CSTS: return "REG_CSTS";
        case NVME_REG_AQA: return "REG_AQA";
        case NVME_REG_ASQ_LO: return "REG_ASQ_LO";
        case NVME_REG_ASQ_HI: return "REG_ASQ_HI";
        case NVME_REG_ACQ_LO: return "REG_ACQ_LO";
        case NVME_REG_ACQ_HI: return "REG_ACQ_HI";
        default: return "??";
    }
}

static uint64_t nvme_read_reg(struct nvme_device *dev, uint32_t offset, uint32_t size)
{
    volatile uint8_t *base = (volatile uint8_t *)dev->bar0;

    switch (size) {
    case 4:
        return *(volatile uint32_t *)(base + offset);
    case 8:
        return *(volatile uint64_t *)(base + offset);
    default:
        return 0;
    }
}

static void __nvme_write_reg(struct nvme_device *dev, uint32_t offset,
                             uint64_t value, uint32_t size, bool log)
{
    volatile uint8_t *addr = (volatile uint8_t *)dev->bar0 + offset;

    if (log)
        logx("> DEV: write @ 0x%x:%x (%s) = 0x%lx",
                offset, size, reg2str(offset), value);

    switch (size) {
    case 2:
        *(volatile uint16_t*)(addr) = (uint16_t)value;
        break;
    case 4:
        *(volatile uint32_t *)(addr) = (uint32_t)value;
        break;
    case 8:
        *(volatile uint64_t *)(addr) = value;
        break;
    }
    asm volatile("": : :"memory");
}

static void nvme_write_reg(struct nvme_device *dev, uint32_t offset, uint64_t value, uint32_t size)
{
     __nvme_write_reg(dev, offset, value, size, true);
}

static const char* adminopcode2str(uint8_t op)
{
    switch (op) {
        case NVME_ADMIN_DELETE_SQ: return "NVME_ADMIN_DELETE_SQ";
        case NVME_ADMIN_CREATE_SQ: return "NVME_ADMIN_CREATE_SQ";
        case NVME_ADMIN_DELETE_CQ: return "NVME_ADMIN_DELETE_CQ";
        case NVME_ADMIN_CREATE_CQ: return "NVME_ADMIN_CREATE_CQ";
        case NVME_ADMIN_IDENTIFY: return "NVME_ADMIN_IDENTIFY";
        case NVME_ADMIN_SET_FEATURES: return "NVME_ADMIN_SET_FEATURES";
        case NVME_ADMIN_GET_FEATURES: return "NVME_ADMIN_GET_FEATURES";
        default: return "??";
    }
}

static const char *ioopcode2str(uint8_t op)
{
    switch (op) {
        case NVME_CMD_FLUSH: return "NVME_CMD_FLUSH";
        case NVME_CMD_WRITE: return "NVME_CMD_WRITE";
        case NVME_CMD_READ: return "NVME_CMD_READ";
        default: return "??";
    }
}

static const char *shn2str(uint8_t shn)
{
    switch (shn) {
        case NVME_SHN_NONE: return "NVME_SHN_NONE";
        case NVME_SHN_NORMAL: return "NVME_SHN_NORMAL";
        case NVME_SHN_ABRUPT: return "NVME_SHN_ABRUPT";
        default: return "??";
    }
}

static int nvme_dma_rw(struct nvme_device *dev, uint64_t prp1, uint64_t prp2,
                       uint8_t *ram_ptr, uint32_t length, bool write_to_host,
                       const char *aux)
{
    uint32_t page_size = 4096;
    int ret;
    struct pciem_dma_indirect indirect = {
        .prp1 = prp1,
        .prp2 = prp2,
        .user_addr = (uint64_t)ram_ptr,
        .length = length,
        .page_size = page_size,
        .pasid = 0,
        .flags = write_to_host ? PCIEM_DMA_FLAG_WRITE : PCIEM_DMA_FLAG_READ
    };

    ret = ioctl(dev->fd, PCIEM_IOCTL_DMA_INDIRECT, &indirect);
    if (ret)
        log("x failed DMA %s (%s)", write_to_host ? "write" : "read",
            aux ? aux : "N/A");

    return ret;
}

static struct nvme_queue *nvme_get_queue(struct nvme_device *dev, uint16_t idx)
{
    if (idx >= MAX_QUEUES) {
        logx("- invalid queue index: %u", idx);
        return NULL;
    }

    return &dev->queues[idx];
}

static void set_lo32(uint64_t *dst, uint32_t val)
{
    *dst= (*dst & 0xFFFFFFFF00000000ULL) | (uint64_t)val;
}

static void set_hi32(uint64_t *dst, uint32_t val)
{
    *dst = (*dst & 0x00000000FFFFFFFFULL) | ((uint64_t)val << 32);
}

static int nvme_dma_write(struct nvme_device *dev, struct pciem_dma_op *op,
                           const char *aux)
{
    int ret;

    op->flags = PCIEM_DMA_FLAG_WRITE;
    logx("* DMA: DEV:0x%012lx -> DRV:0x%012lx (size = 0x%04x) (%s)",
         op->user_addr, op->guest_iova, op->length, aux ? aux : "N/A");
    ret = ioctl(dev->fd, PCIEM_IOCTL_DMA, op);
    if (ret)
        log("x failed DMA write (%s)", aux ? aux : "N/A");
    return ret;
}

static int nvme_dma_read(struct nvme_device *dev, struct pciem_dma_op *op,
                          const char *aux)
{
    int ret;

    op->flags = PCIEM_DMA_FLAG_READ;
    logx("* DMA: DEV:0x%012lx <- DRV:0x%012lx (size = 0x%04x) (%s)",
         op->user_addr, op->guest_iova, op->length, aux ? aux : "N/A");
    ret = ioctl(dev->fd, PCIEM_IOCTL_DMA, op);
    if (ret)
        log("x failed DMA read (%s)", aux ? aux : "N/A");
    return ret;
}

static void nvme_reset_queue(struct nvme_queue *q)
{
    uint16_t id = q->id;

    memset(q, 0, sizeof(*q));
    q->id = id;
    q->cq_phase = 1;
}

static void nvme_reset_queues(struct nvme_device *dev)
{
    for (int i = 0; i < MAX_QUEUES; ++i) {
        dev->queues[i].id = i;
        nvme_reset_queue(&dev->queues[i]);
    }
}

static uint32_t nvme_identify_cns0(struct nvme_device *dev,
                                   struct nvme_command *cmd)
{
    struct nvme_id_ns ns = {
        .nsze = NVME_TOTAL_SECTORS,
        .ncap = NVME_TOTAL_SECTORS,
        .nuse = NVME_TOTAL_SECTORS,
        .nlbaf = 1,
        .lbaf = { { .ds = 12 } },
    };
    struct pciem_dma_op op = {
        .guest_iova = cmd->prp1,
        .user_addr = (uint64_t)&ns,
        .length = sizeof(ns),
    };
    int ret;

    /* We support only one namespace */
    if (cmd->nsid != 1)
        return NVME_INVALID_NS;

    ret = nvme_dma_write(dev, &op, "identify cns=0 nsid=1");
    if (ret)
        return NVME_INTERNAL;

    return NVME_SUCCESS;
}

static uint32_t nvme_identify_cns1(struct nvme_device *dev,
                                   struct nvme_command *cmd)
{
    struct pciem_dma_op op = {
        .guest_iova = cmd->prp1,
        .user_addr = (uint64_t)&dev->ctrl_id,
        .length = sizeof(dev->ctrl_id),
    };
    int ret;

    if (cmd->nsid != 0)
        return NVME_INVALID_NS;

    ret = nvme_dma_write(dev, &op, "identify cns=1 nsid=0 (ctrl)");
    if (ret)
        return NVME_INTERNAL;

    return NVME_SUCCESS;
}

static uint32_t nvme_identify_cns2(struct nvme_device *dev,
                                   struct nvme_command *cmd)
{
    uint32_t *nslist;
    struct pciem_dma_op op = {
        .guest_iova = cmd->prp1,
        .length = 0x1000,
    };
    int ret;

    if (cmd->nsid != 0)
        return NVME_INVALID_NS;

    nslist = calloc(1, 0x1000);
    if (!nslist)
        return NVME_INTERNAL;

    nslist[0] = 1;
    op.user_addr = (uint64_t)nslist;

    ret = nvme_dma_write(dev, &op, "identify cns=2 nsid=0 (ns list)");
    free(nslist);

    return ret ? NVME_INTERNAL : NVME_SUCCESS;
}

static uint32_t nvme_identify_cns3(struct nvme_device *dev,
                                   struct nvme_command *cmd)
{
    uint8_t *dslist;
    struct pciem_dma_op op = {
        .guest_iova = cmd->prp1,
        .length = 0x1000,
    };
    int ret;

    if (cmd->nsid > 1)
        return NVME_INVALID_NS;

    dslist = calloc(1, 0x1000);
    if (!dslist)
        return NVME_INTERNAL;

    op.user_addr = (uint64_t)dslist;

    ret = nvme_dma_write(dev, &op, "identify cns=3 nsid=X (ds list)");
    free(dslist);

    return ret ? NVME_INTERNAL : NVME_SUCCESS;
}

/*
 * CNS  NSID  Meaning
 * ---  ----  ------------------------------------------------------------
 * 0    0     Invalid
 * 0    1     Identify Namespace 1 (nvme_id_ns)
 * 0    2–4   Identify Namespace N (only valid if namespace exists)
 *
 * 1    0     Identify Controller (nvme_id_ctrl)
 * 1    1–4   Invalid. NSID must be 0 for CNS=1
 *
 * 2    0     Identify Active Namespace ID List (uint32_t array, terminated by 0)
 * 2    1–4   Invalid. NSID must be 0
 *
 * 3    0     Identify Namespace Descriptor List for all namespaces (NVMe ≥1.3)
 * 3    1     Identify Namespace Descriptor List for Namespace 1
 * 3    2–4   Identify Namespace Descriptor List for Namespace N (if it exists)
 *
 * 4    0     Identify Namespace Attribute (NVMe ≥2.0)
 * 4    1–4   Identify Namespace Attribute for Namespace N (if supported)
 */
 static uint32_t nvme_identify(struct nvme_device *dev,
                               struct nvme_command *cmd)
{
    uint8_t cns = cmd->cdw10.identify.cns;

    logx("  identify cns=%u nsid=%u", cns, cmd->nsid);

    switch (cns) {
        case 0: return nvme_identify_cns0(dev, cmd);
        case 1: return nvme_identify_cns1(dev, cmd);
        case 2: return nvme_identify_cns2(dev, cmd);
        case 3: return nvme_identify_cns3(dev, cmd);
    }

    return NVME_INVALID_FIELD;
}

static uint32_t nvme_create_cq(struct nvme_device *dev,
                               struct nvme_command *cmd)
{
    uint16_t qidx = cmd->cdw10.create_io_cq.qid;
    uint16_t qsize = cmd->cdw10.create_io_cq.qsize;
    uint16_t vector = cmd->cdw11.create_io_cq.vector;
    struct nvme_queue *q = nvme_get_queue(dev, qidx);

    if (!q)
        return NVME_INVALID_QUEUE_CREATE;

    q->cq_addr = cmd->prp1;
    q->cq_size = qsize + 1;
    q->vector = vector;
    q->created = true;
    logx("+ created CQ queue %u 0x%lx:0x%x (vector=%d)",
         qidx, q->cq_addr, q->cq_size, q->vector);

    return NVME_SUCCESS;
}

static uint32_t nvme_create_sq(struct nvme_device *dev,
                               struct nvme_command *cmd)
{
    uint16_t qidx = cmd->cdw10.create_io_sq.qid;
    uint16_t qsize = cmd->cdw10.create_io_sq.qsize;
    struct nvme_queue *q = nvme_get_queue(dev, qidx);

    if (!q)
        return NVME_INVALID_QUEUE_CREATE;

    q->sq_addr = cmd->prp1;
    q->sq_size = qsize + 1;
    q->created = true;
    logx("+ created SQ queue %u 0x%lx:0x%x", qidx, q->sq_addr, q->sq_size);

    return NVME_SUCCESS;
}

static uint32_t nvme_delete_queue(struct nvme_device *dev,
                                  struct nvme_command *cmd)
{
    uint16_t qidx = cmd->cdw10.delete_io_cq.qid;

    if (qidx >= MAX_QUEUES) {
        logx("Attempted to delete invalid queue %u\n", qidx);
        return NVME_INVALID_QUEUE_DELETE;
    }

    nvme_reset_queue(&dev->queues[qidx]);
    logx("+ deleted queue %u", qidx);
    return NVME_SUCCESS;
}

static uint32_t nvme_set_features(struct nvme_device *dev,
                                  const struct nvme_command *cmd,
                                  uint32_t *result)
{
    uint8_t fid = cmd->cdw10.features.fid;

    (void)dev;

    switch (fid) {
        case NVME_FEAT_NUM_QUEUES: {
            uint16_t nsq = cmd->cdw11.feat_num_queues.nsqa;
            uint16_t ncq = cmd->cdw11.feat_num_queues.ncqa;
            logx("  FEAT: nsq=%u/%u ncq=%u/%u",
                 nsq, MAX_QUEUES, ncq, MAX_QUEUES);
            nsq = nsq < MAX_QUEUES ? nsq : MAX_QUEUES;
            ncq = ncq < MAX_QUEUES ? ncq : MAX_QUEUES;
            *result = (ncq << 16) | nsq;
            return NVME_SUCCESS;
        }
        case NVME_FEAT_ARBITRATION:
        case NVME_FEAT_POWER_MGMT:
        case NVME_FEAT_ERROR_RECOVERY:
        case NVME_FEAT_VOLATILE_WC:
        case NVME_FEAT_IRQ_COALESCE:
        case NVME_FEAT_IRQ_CONFIG:
        case NVME_FEAT_ASYNC_EVENT:
        case NVME_FEAT_TEMP_THRESHOLD:
        case NVME_FEAT_WRITE_ATOMIC:
        case NVME_FEAT_NOPSC:
        case NVME_FEAT_RRL:
        case NVME_FEAT_PLM_CONFIG:
        case NVME_FEAT_PLM_WINDOW:
            logx("  FEAT: allow 0x%x", fid);
            return NVME_SUCCESS;
        case NVME_FEAT_LBA_RANGE:        // Requires tracking LBA ranges
        case NVME_FEAT_AUTO_PST:         // Autonomous power state transitions
        case NVME_FEAT_HOST_MEM_BUF:     // Host memory buffer
        case NVME_FEAT_TIMESTAMP:        // Timestamp feature
        case NVME_FEAT_KATO:             // Keep alive timeout
        case NVME_FEAT_HCTM:             // Host controlled thermal management
        case NVME_FEAT_HOST_BEHAVIOR:    // Host behavior support
            logx("- FEAT: disallow 0x%x", fid);
            return NVME_INVALID_FIELD;
        default:
            logx("- FEAT: unknown feature 0x%x", fid);
            return NVME_INVALID_FIELD;
    }
}

static uint32_t nvme_execute_admin_command(struct nvme_device *dev,
                                           struct nvme_command *cmd,
                                           uint32_t *result)
{
    logx("  CMD: admin::%s nsid=%u", adminopcode2str(cmd->opcode), cmd->nsid);

    switch (cmd->opcode) {
        case NVME_ADMIN_IDENTIFY:
            return nvme_identify(dev, cmd);
        case NVME_ADMIN_CREATE_CQ:
            return nvme_create_cq(dev, cmd);
        case NVME_ADMIN_CREATE_SQ:
            return nvme_create_sq(dev, cmd);
        case NVME_ADMIN_DELETE_CQ:
        case NVME_ADMIN_DELETE_SQ:
            return nvme_delete_queue(dev, cmd);
        case NVME_ADMIN_SET_FEATURES:
            return nvme_set_features(dev, cmd, result);
        default: {
            logx("? unhandled admin command: 0x%x", cmd->opcode);
            return NVME_INVALID_FIELD;
        }
    }
}

static uint32_t nvme_io_write(struct nvme_device *dev, struct nvme_command *cmd)
{
    uint64_t slba = cmd->cdw10.raw | ((uint64_t)cmd->cdw11.raw << 32);
    uint64_t offset = slba << 12;
    uint32_t nlb = cmd->cdw12.rw.nlb + 1;
    uint64_t len = (uint64_t)nlb << 12;
    int ret;

    if (offset >= NVME_DISK_SIZE) {
        logx("Attempted to write offset %lx/%lx", offset, NVME_DISK_SIZE);
        return NVME_INVALID_FIELD;
    }

    ret = nvme_dma_rw(dev, cmd->prp1, cmd->prp2, dev->disk_data + offset,
                      len, false, "host write");
    return ret ? NVME_INTERNAL : NVME_SUCCESS;
}

static uint32_t nvme_io_read(struct nvme_device *dev, struct nvme_command *cmd)
{
    uint64_t slba = cmd->cdw10.raw | ((uint64_t)cmd->cdw11.raw << 32);
    uint64_t offset = slba << 12;
    uint32_t nlb = cmd->cdw12.rw.nlb + 1;
    uint64_t len = (uint64_t)nlb << 12;
    int ret;

    if (offset >= NVME_DISK_SIZE) {
        logx("Attempted to read offset %lx/%lx", offset, NVME_DISK_SIZE);
        return NVME_INVALID_FIELD;
    }

    ret = nvme_dma_rw(dev, cmd->prp1, cmd->prp2, dev->disk_data + offset,
                      len, true, "host read");
    return ret ? NVME_INTERNAL : NVME_SUCCESS;
}

static uint32_t nvme_execute_io_command(struct nvme_device *dev,
                                        struct nvme_command *cmd)
{
    logx("  CMD: io::%s nsid=%u", ioopcode2str(cmd->opcode), cmd->nsid);
    switch (cmd->opcode) {
        case NVME_CMD_FLUSH:
            return NVME_SUCCESS;
        case NVME_CMD_READ:
            return nvme_io_read(dev, cmd);
        case NVME_CMD_WRITE:
            return nvme_io_write(dev, cmd);
        default: {
            logx("? unhandled I/O command: 0x%x", cmd->opcode);
            return NVME_INVALID_FIELD;
        }
    }
}

static void nvme_execute_command(struct nvme_device *dev,
                                 struct nvme_command *cmd,
                                 struct nvme_queue *q)
{
    struct nvme_completion cpl = {0};
    struct pciem_dma_op op = {0};
    uint32_t status = NVME_OPERATION_DENIED;
    struct pciem_irq_inject irq;

    if (q->id == 0) {
        uint32_t result = 0;
        status = nvme_execute_admin_command(dev, cmd, &result);
        cpl.result = result;
    } else {
        status = nvme_execute_io_command(dev, cmd);
    }
    logx("       ret = %d", status);

    cpl.command_id = cmd->command_id;
    cpl.sq_head = q->sq_head;
    cpl.sq_id = q->id;
    cpl.status = status | (q->cq_phase & 1);

    op.guest_iova = q->cq_addr + q->cq_tail * sizeof(cpl);
    op.user_addr  = (uint64_t)&cpl;
    op.length = sizeof(cpl);
    nvme_dma_write(dev, &op, "completion");

    q->cq_tail++;
    if (q->cq_tail == q->cq_size) {
        q->cq_tail = 0;
        q->cq_phase ^= 1;
    }

    logx(". IRQ: injecting vector=%u on qid=%u", q->vector, q->id);
    // FIXME: this does not work, it's probably an issue in the kernel
    // irq.vector = q->vector;
    irq.vector = 0;
    if (ioctl(dev->fd, PCIEM_IOCTL_INJECT_IRQ, &irq))
        log("x failed IRQ=%d injection", q->vector);
}

static int nvme_read_sq_entry(struct nvme_device *dev, struct nvme_queue *q,
                              struct nvme_command *cmd)
{
    struct pciem_dma_op op = {
        .guest_iova = q->sq_addr + (q->sq_head * sizeof(*cmd)),
        .user_addr = (uint64_t)cmd,
        .length = sizeof(*cmd),
    };

    return nvme_dma_read(dev, &op, "read SQ entry");
}

static void nvme_handle_doorbell_write(struct nvme_device *dev,
                                       struct pciem_event *ev)
{
    uint16_t qidx = (ev->offset - NVME_REG_SQ0TDBL) / 8;
    bool is_cq = ((ev->offset - NVME_REG_SQ0TDBL) % 8) >= 4;
    struct nvme_queue *q;
    struct nvme_command cmd;

    if (is_cq)
        logx("  driver ACK on queue %u", qidx);
    else
        logx("  driver submission on queue %u", qidx);

    q = nvme_get_queue(dev, qidx);
    if (!q)
        return;

    if (is_cq) {
        q->cq_head = ev->data & 0xffff;
        nvme_write_reg(dev, NVME_REG_CQ0HDBL + qidx * 8, ev->data, 4);
        return;
    }

    q->sq_tail = ev->data & 0xffff;

     while (q->sq_head != q->sq_tail) {
        if (!nvme_read_sq_entry(dev, q, &cmd)) {
            q->sq_head = (q->sq_head + 1) % q->sq_size;
            nvme_execute_command(dev, &cmd, q);
        }
    }

    nvme_write_reg(dev, NVME_REG_SQ0TDBL + qidx * 8, ev->data, 4);
}

static void nvme_enable(struct nvme_device *dev)
{
    struct nvme_queue *q = &dev->queues[0];
    uint32_t aqa;

    /* On reset, the configuration is logically purged from device state,
     * but remains valid in memory. The driver expects us to reload these
     * values */
    aqa  = nvme_read_reg(dev, NVME_REG_AQA, 4);
    q->sq_size = (aqa & 0xffff) + 1;
    q->cq_size = ((aqa >> 16) & 0xffff) + 1;
    set_lo32(&q->sq_addr, nvme_read_reg(dev, NVME_REG_ASQ_LO, 4));
    set_hi32(&q->sq_addr, nvme_read_reg(dev, NVME_REG_ASQ_HI, 4));
    set_lo32(&q->cq_addr, nvme_read_reg(dev, NVME_REG_ACQ_LO, 4));
    set_hi32(&q->cq_addr, nvme_read_reg(dev, NVME_REG_ACQ_HI, 4));
    dev->enabled = true;
}

static void nvme_handle_cc_write(struct nvme_device *dev,
                                 struct pciem_event *ev)
{
    union cc {
        uint32_t raw;
        struct nvme_cc reg;
    } cc = { .raw = ev->data };
    union {
        uint32_t raw;
        struct nvme_csts reg;
    } csts = { .raw = dev->csts };

    logx("       CC = 0x%x (en=%d shn=%s)",
         cc.raw, cc.reg.en, shn2str(cc.reg.shn));

    /* Shutdown */
    if (cc.reg.shn != NVME_SHN_NONE) {
        csts.reg.shst = NVME_SHST_PROGRESS;
        csts.reg.rdy = 0;
        dev->csts = csts.raw;
        nvme_write_reg(dev, NVME_REG_CSTS, dev->csts, 4);

        dev->enabled = false;
        nvme_reset_queues(dev);

        csts.reg.shst = NVME_SHST_COMPLETE;
        csts.reg.rdy = 0;
        dev->csts = csts.raw;
        nvme_write_reg(dev, NVME_REG_CSTS, dev->csts, 4);

        dev->running = false;
        logx("  STATUS: device shut down by driver");
        return;
    }

    if (cc.reg.en) {
        nvme_enable(dev);
        csts.reg.rdy = 1;
        csts.reg.shst = NVME_SHST_NORMAL;
        logx("  STATUS: device initialized by driver");
    } else {
        dev->enabled = false;
        csts.raw = 0;
        nvme_reset_queues(dev);
        logx("  STATUS: device disabled by driver");
    }

    dev->csts = csts.raw;
    nvme_write_reg(dev, NVME_REG_CSTS, dev->csts, 4);
}

static void nvme_handle_write(struct nvme_device *dev, struct pciem_event *ev)
{
    if (ev->bar != 0)
        return;

    logx("------");
    logx("< DRV: write @ 0x%lx:%x (%s) = 0x%lx",
          ev->offset, ev->size, reg2str(ev->offset), ev->data);

    struct nvme_queue *q = &dev->queues[0];

    switch (ev->offset) {
        case NVME_REG_CC: {
            nvme_handle_cc_write(dev, ev);
            return;
        }
        case NVME_REG_AQA: {
            if (dev->enabled)
                return;
            q->sq_size = (ev->data & 0xffff) + 1;
            q->cq_size = ((ev->data >> 16) & 0xffff) + 1;
            nvme_write_reg(dev, NVME_REG_AQA, ev->data, 4);
            logx("  admin: sq_size = 0x%x", q->sq_size);
            logx("  admin: cq_size = 0x%x", q->cq_size);
            return;
        }
        case NVME_REG_ASQ_LO: {
            if (dev->enabled)
                return;
            set_lo32(&q->sq_addr, ev->data);
            nvme_write_reg(dev, NVME_REG_ASQ_LO, ev->data, 4);
            logx("  admin: sq_addr = 0x%lx", q->sq_addr);
            return;
        }
        case NVME_REG_ASQ_HI:
            if (dev->enabled)
                return;
            set_hi32(&q->sq_addr, ev->data);
            nvme_write_reg(dev, NVME_REG_ASQ_HI, ev->data, 4);
            logx("  admin: sq_addr = 0x%lx", q->sq_addr);
            return;
        case NVME_REG_ACQ_LO: {
            if (dev->enabled)
                return;
            set_lo32(&q->cq_addr, ev->data);
            nvme_write_reg(dev, NVME_REG_ACQ_LO, ev->data, 4);
            logx("  admin: cq_addr = %lx", q->cq_addr);
            return;
        }
        case NVME_REG_ACQ_HI: {
            if (dev->enabled)
                return;
            set_hi32(&q->cq_addr, ev->data);
            nvme_write_reg(dev, NVME_REG_ACQ_HI, ev->data, 4);
            logx("  admin: cq_addr = %lx", q->cq_addr);
            return;
        }
    }

    if (ev->offset < NVME_REG_SQ0TDBL) {
        logx("- ignoring write @ 0x%lx (unknown register)", ev->offset);
        return;
    }

    if (!dev->enabled) {
        logx("- ignoring write @ 0x%lx (device disabled)", ev->offset);
        return;
    }

    nvme_handle_doorbell_write(dev, ev);
}

static void dev_event_loop(void *arg)
{
    struct nvme_device *dev = arg;
    struct pciem_shared_ring *ring = dev->ring;
    struct pciem_event *ev;
    struct pollfd pollfd;
    int ret, head, tail;

    pollfd.fd = dev->event_fd;
    pollfd.events = POLLIN;

    dev->running = true;

    while (dev->running) {
        int tmp;

        ret = poll(&pollfd, 1, -1);
        if (ret  < 0)
            err(EXIT_FAILURE, "poll");
        if (ret == 0)
            continue;

        read(dev->event_fd, &tmp, sizeof(tmp));

        while (1) {
            head = atomic_load(&ring->head);
            tail = atomic_load(&ring->tail);
            if (head == tail)
                break;

            ev = &ring->events[head];
            ev->data &= size2mask(ev->size);

            if (ev->type & PCIEM_EVENT_MMIO_WRITE)
                nvme_handle_write(dev, ev);

            atomic_store(&ring->head, (head + 1) % PCIEM_RING_SIZE);
        }
    }

    logx("exiting");
}

static void dev_reset(struct nvme_device *dev)
{
    memset(dev, 0, sizeof(*dev));
    dev->fd = -1;
    dev->event_fd = -1;
    dev->instance_fd = -1;
}

static void dev_destroy(struct nvme_device *dev)
{
    if (dev->instance_fd >= 0)
        close(dev->instance_fd);
    if (dev->event_fd)
        close(dev->event_fd >= 0);
    if (dev->ring && dev->ring != MAP_FAILED)
        munmap(dev->ring, sizeof(struct pciem_shared_ring));
    if (dev->bar0 && dev->bar0 != MAP_FAILED)
        munmap(dev->bar0, dev->bar0_size);
    if (dev->fd >= 0)
        close(dev->fd);
    dev_reset(dev);
}

static int dev_register(struct nvme_device *dev)
{
    int ret;
    struct pciem_create_device create = { .flags = 0 };
    struct pciem_config_space cfg = {
        .vendor_id = 0x1234,
        .device_id = 0x5678,
        .subsys_vendor_id = 0x1234,
        .subsys_device_id = 0x0001,
        .revision = 0x01,
        .class_code = {0x02, 0x08, 0x01},
        .header_type = 0x00
    };
    struct pciem_bar_config bars[2] = {
        {
            .bar_index = 0,
            .size = 8192,
            .flags = 0
        },
        {
            .bar_index = 2,
            .size = 8192,
            .flags = 0
        }
    };
    struct pciem_cap_config cap_msix = {
        .cap_type = PCIEM_CAP_MSIX,
        .msix = {
            .bar_index = 2,
            .table_offset = 0,
            .pba_offset = 4096,
            .table_size = 16
        }
    };
    struct pciem_eventfd_config efd_cfg = {0};
    struct pciem_trace_bar trace_cfg = {
        .bar_index = 0,
        .flags = PCIEM_TRACE_WRITES,
    };

    dev_reset(dev);

    dev->ctrl_id = (struct nvme_ctrl_id) {
        .vid = 0x1234,
        .ssvid = 0x5678,
        .sn = "PCIEM-NVME-001     ",
        .mn = "PCIem Virtual NVMe SSD                 ",
        .fr = "1.0    ",
        .nn = 1,
        .sqes = 0x66,
        .cqes = 0x44,
        .ver = NVME_VS(1, 0, 0),
    };

    ret = 1;
    dev->fd = open("/dev/pciem", O_RDWR);
    if (dev->fd < 0)
        goto fail;

    ret = ioctl(dev->fd, PCIEM_IOCTL_CREATE_DEVICE, &create);
    if (ret)
        goto fail;

    ret = ioctl(dev->fd, PCIEM_IOCTL_SET_CONFIG, &cfg);
    if (ret)
        goto fail;

    for (size_t i = 0; i < (sizeof(bars) / sizeof(bars[0])); ++i) {
        ret = ioctl(dev->fd, PCIEM_IOCTL_ADD_BAR, &bars[i]);
        if (ret)
            goto fail;
    }

    ret = ioctl(dev->fd, PCIEM_IOCTL_ADD_CAPABILITY, &cap_msix);
    if (ret)
        goto fail;

    ret = ioctl(dev->fd, PCIEM_IOCTL_TRACE_BAR, &trace_cfg);
    if (ret)
        goto fail;

    dev->event_fd = eventfd(0, EFD_CLOEXEC | EFD_NONBLOCK);
    if (dev->event_fd < 0)
        goto fail;

    efd_cfg.eventfd = dev->event_fd;
    ret = ioctl(dev->fd, PCIEM_IOCTL_SET_EVENTFD, &efd_cfg);
    if (ret)
        goto fail;

    logx("Eventfd configured successfully");

    ret = 1;
    dev->instance_fd = ioctl(dev->fd, PCIEM_IOCTL_REGISTER);
    if (dev->instance_fd < 0)
        goto fail;

    logx("Device registered successfully");
    return 0;

fail:
    warn("Failed to initialize device");
    dev_destroy(dev);
    return ret;
}

static void dev_prepare_ro_regs(struct nvme_device *dev)
{
    union {
        struct nvme_cap cap;
        uint64_t raw;
    } c = {
        .cap = {
            .mqes = 64 - 1,
            .to = 10,
            .dstrd = 1,
            .nssrs = 0,
            .css = 1,
            .mpsmin = 0,
            .mpsmax = 0,
        }
    };

    nvme_write_reg(dev, NVME_REG_VS, NVME_VS(1, 0, 0), 4);
    nvme_write_reg(dev, NVME_REG_CAP, c.raw, 8);
}

static int dev_alloc_resources(struct nvme_device *dev)
{
    int ret = 1;

    dev->bar0_size = 8192;
    dev->bar0 = mmap(NULL, dev->bar0_size, PROT_READ | PROT_WRITE, MAP_SHARED,
                     dev->instance_fd, 0);
    if (dev->bar0 == MAP_FAILED)
        goto fail;

    dev_prepare_ro_regs(dev);

    dev->ring = mmap(NULL, sizeof(struct pciem_shared_ring),
                     PROT_READ | PROT_WRITE, MAP_SHARED, dev->fd, 0);
    if (dev->ring == MAP_FAILED)
        goto fail;
    logx("Shared ring buffer mapped at %p", dev->ring);

    dev->disk_data = calloc(1, NVME_DISK_SIZE);
    if (!dev->disk_data)
        goto fail;

    logx("Device resources allocated successfully");
    return 0;

fail:
    warn("Failed to allocate device resources");
    dev_destroy(dev);
    return ret;
}

int main(void)
{
    struct nvme_device dev = {0};

    if (dev_register(&dev))
        err(EXIT_FAILURE,  "dev_register");

    if (dev_alloc_resources(&dev))
        err(EXIT_FAILURE, "dev_alloc_resources");

    dev.disk_data = calloc(1, NVME_DISK_SIZE);
    if (!dev.disk_data) {
        log("x failed to allocate disk storage (0x%lx bytes)", NVME_DISK_SIZE);
        dev_destroy(&dev);
        exit(EXIT_FAILURE);
    }

    if (ioctl(dev.fd, PCIEM_IOCTL_START, 0))
        err(EXIT_FAILURE, "failed to start device");

    dev_event_loop(&dev);
    dev_destroy(&dev);

    return 0;
}

```

`examples/protopciem/Makefile`:

```
THIS_MAKEFILE := $(lastword $(MAKEFILE_LIST))
THIS_DIR      := $(abspath $(dir $(THIS_MAKEFILE)))

DRIVER_DIR    := $(THIS_DIR)/driver
USERSPACE_DIR := $(THIS_DIR)/userspace

.PHONY: all driver userspace clean

all: driver userspace

driver:
	$(MAKE) -C $(DRIVER_DIR)

userspace:
	$(MAKE) -C $(USERSPACE_DIR)

clean:
	$(MAKE) -C $(DRIVER_DIR) clean
	$(MAKE) -C $(USERSPACE_DIR) clean

```

`examples/protopciem/driver/Makefile`:

```
THIS_MAKEFILE := $(lastword $(MAKEFILE_LIST))
DRIVER_DIR    := $(abspath $(dir $(THIS_MAKEFILE)))

ifneq ($(KERNELRELEASE),)

obj-m := protopciem_driver.o

else

KDIR ?= /lib/modules/$(shell uname -r)/build

all:
	$(MAKE) -C $(KDIR) M=$(DRIVER_DIR) modules

clean:
	$(MAKE) -C $(KDIR) M=$(DRIVER_DIR) clean

.PHONY: all clean

endif

```

`examples/protopciem/driver/protopciem_driver.c`:

```c
// SPDX-License-Identifier: GPL-2.0 OR MIT
/*
 * Copyright (C) 2025-2026  Joel Bueno <buenocalvachejoel@gmail.com>
 */
#define pr_fmt(fmt) KBUILD_MODNAME ": " fmt
#include <linux/completion.h>
#include <linux/delay.h>
#include <linux/dma-mapping.h>
#include <linux/fs.h>
#include <linux/interrupt.h>
#include <linux/jiffies.h>
#include <linux/kernel.h>
#include <linux/miscdevice.h>
#include <linux/mm.h>
#include <linux/module.h>
#include <linux/pci.h>
#include <linux/slab.h>
#include <linux/uaccess.h>
#include <linux/workqueue.h>

MODULE_LICENSE("Dual MIT/GPL");
MODULE_AUTHOR("cakehonolulu (cakehonolulu@protonmail.com)");
MODULE_DESCRIPTION("ProtoPCIem Driver");

#define PCIEM_PCI_VENDOR_ID 0x1F0C
#define PCIEM_PCI_DEVICE_ID 0x0001

#define REG_CONTROL 0x00
#define REG_STATUS 0x04
#define REG_CMD 0x08
#define REG_DATA 0x0C
#define REG_RESULT_LO 0x10
#define REG_RESULT_HI 0x14
#define REG_DMA_SRC_LO 0x20
#define REG_DMA_SRC_HI 0x24
#define REG_DMA_DST_LO 0x28
#define REG_DMA_DST_HI 0x2C
#define REG_DMA_LEN 0x30
#define REG_DMA_FLAGS 0x34

#define CTRL_ENABLE BIT(0)
#define CTRL_RESET BIT(1)

#define STATUS_BUSY BIT(0)
#define STATUS_DONE BIT(1)
#define STATUS_ERROR BIT(2)

#define CMD_ADD 0x01
#define CMD_MULTIPLY 0x02
#define CMD_XOR 0x03
#define CMD_PROCESS_BUFFER 0x04
#define CMD_EXECUTE_CMDBUF 0x05
#define CMD_DMA_FRAME 0x06
#define CMD_DMA_P2P_READ  0x07
#define CMD_DMA_P2P_WRITE 0x08

#define FB_WIDTH 640
#define FB_HEIGHT 480
#define FB_BPP 3
#define FB_PITCH (FB_WIDTH * FB_BPP)
#define FB_SIZE (FB_PITCH * FB_HEIGHT)
#define MMAP_SIZE (FB_SIZE * 2)

#define PROTOPCIEM_IOC_MAGIC 'a'
#define PROTOPCIEM_IOCTL_SUBMIT_FRAME _IOW(PROTOPCIEM_IOC_MAGIC, 1, __u32)

struct pci_device
{
    struct pci_dev *pdev;
    void __iomem *bar0;
    int irq;
    bool msi_enabled;
    struct workqueue_struct *wq;
    struct work_struct work;
    struct completion op_completion;
    spinlock_t lock;
    bool operation_pending;
    u32 last_command;
    u64 last_result;
    u64 operations_completed;
    u64 interrupts_received;
    u64 errors;
    struct miscdevice protopciem_miscdev;
    void *cmdbuf_virt;
    dma_addr_t cmdbuf_phys;
    size_t cmdbuf_size;
    void *framebuf_virt;
    dma_addr_t framebuf_phys;
    size_t framebuf_size;
};

static inline u32 pci_read_reg(struct pci_device *adev, u32 offset)
{
    return ioread32(adev->bar0 + offset);
}

static inline void pci_write_reg(struct pci_device *adev, u32 offset, u32 value)
{
    iowrite32(value, adev->bar0 + offset);
}

static inline void pci_write_reg64(struct pci_device *adev, u32 offset_lo, u64 value)
{
    pci_write_reg(adev, offset_lo, (u32)(value & 0xFFFFFFFF));
    pci_write_reg(adev, offset_lo + 4, (u32)(value >> 32));
}

static inline u64 pci_read_result(struct pci_device *adev)
{
    u32 lo = pci_read_reg(adev, REG_RESULT_LO);
    u32 hi = pci_read_reg(adev, REG_RESULT_HI);
    return ((u64)hi << 32) | lo;
}

static int pci_reset(struct pci_device *adev)
{
    u32 status;
    int timeout = 1000;
    pr_info("Resetting PCI card\n");
    pci_write_reg(adev, REG_CONTROL, CTRL_RESET);
    msleep(10);
    pci_write_reg(adev, REG_CONTROL, 0);
    while (timeout-- > 0)
    {
        status = pci_read_reg(adev, REG_STATUS);
        if (!(status & STATUS_BUSY))
            break;
        udelay(10);
    }
    if (timeout <= 0)
    {
        pr_err("Reset timeout\n");
        return -ETIMEDOUT;
    }
    pr_info("Reset complete\n");
    return 0;
}

static int pci_execute_command(struct pci_device *adev, u32 cmd, u32 data, u64 *result)
{
    unsigned long flags;
    u32 status;
    int ret = 0;
    long timeout_jiffies;
    u64 local_result;

    spin_lock_irqsave(&adev->lock, flags);
    if (adev->operation_pending)
    {
        spin_unlock_irqrestore(&adev->lock, flags);
        return -EBUSY;
    }
    adev->operation_pending = true;
    adev->last_command = cmd;
    if (adev->irq)
    {
        reinit_completion(&adev->op_completion);
    }
    spin_unlock_irqrestore(&adev->lock, flags);
    pr_info("Executing command 0x%02x (IRQ: %s)\n", cmd, adev->irq ? "yes" : "no");
    pci_write_reg(adev, REG_STATUS, 0);
    if (cmd != CMD_EXECUTE_CMDBUF && cmd != CMD_DMA_FRAME)
    {
        pci_write_reg(adev, REG_DATA, data);
    }
    pci_write_reg(adev, REG_CONTROL, CTRL_ENABLE);
    pci_write_reg(adev, REG_CMD, cmd);
    if (adev->irq)
    {
        timeout_jiffies = msecs_to_jiffies(2000);
        if (!wait_for_completion_timeout(&adev->op_completion, timeout_jiffies))
        {
            pr_err("Operation timeout (IRQ mode)\n");
            ret = -ETIMEDOUT;
            spin_lock_irqsave(&adev->lock, flags);
            adev->operation_pending = false;
            spin_unlock_irqrestore(&adev->lock, flags);
            goto out;
        }
        else
        {
            pr_info("Command complete (IRQ mode)\n");
        }
        spin_lock_irqsave(&adev->lock, flags);
        local_result = adev->last_result;
        spin_unlock_irqrestore(&adev->lock, flags);

        status = pci_read_reg(adev, REG_STATUS);
        if (status & STATUS_ERROR)
        {
            pr_err("Operation failed (IRQ mode), reported by device\n");
            ret = -EIO;
        }
    }
    else
    {
        int timeout_poll = 1000;
        pr_info("No IRQ, using polling mode\n");
        while (timeout_poll-- > 0)
        {
            status = pci_read_reg(adev, REG_STATUS);
            if (status & STATUS_DONE)
            {
                adev->last_result = pci_read_result(adev);
                if (status & STATUS_ERROR)
                {
                    pr_warn("Error in operation (Poll mode)\n");
                    adev->errors++;
                    ret = -EIO;
                }
                adev->operations_completed++;
                break;
            }
            udelay(100);
        }
        if (timeout_poll <= 0)
        {
            pr_err("Operation timeout (Poll mode)\n");
            ret = -ETIMEDOUT;
        }
        spin_lock_irqsave(&adev->lock, flags);
        adev->operation_pending = false;
        spin_unlock_irqrestore(&adev->lock, flags);
    }
    pci_write_reg(adev, REG_CONTROL, CTRL_ENABLE);
out:
    if (ret == 0)
    {
        *result = local_result;
    }
    return ret;
}

static irqreturn_t pci_irq_handler(int irq, void *data)
{
    struct pci_device *adev = data;
    u32 status = pci_read_reg(adev, REG_STATUS);
    if (!(status & (STATUS_DONE | STATUS_ERROR)))
    {
        return IRQ_NONE;
    }
    adev->interrupts_received++;
    pci_write_reg(adev, REG_STATUS, 0);
    queue_work(adev->wq, &adev->work);
    return IRQ_HANDLED;
}

static void pci_work_handler(struct work_struct *work)
{
    struct pci_device *adev = container_of(work, struct pci_device, work);
    unsigned long flags;
    u32 status;
    spin_lock_irqsave(&adev->lock, flags);
    if (adev->operation_pending)
    {
        status = pci_read_reg(adev, REG_STATUS);
        adev->last_result = pci_read_result(adev);
        pr_info("Work: result=0x%016llx\n", adev->last_result);
        if (status & STATUS_ERROR)
        {
            pr_warn("Work: Operation completed with error\n");
            adev->errors++;
        }
        pci_write_reg(adev, REG_CMD, 0);
        adev->operations_completed++;
        adev->operation_pending = false;
        complete(&adev->op_completion);
    }
    spin_unlock_irqrestore(&adev->lock, flags);
}

static ssize_t protopciem_write(struct file *file, const char __user *buf, size_t count, loff_t *ppos)
{
    struct pci_device *adev = container_of(file->private_data, struct pci_device, protopciem_miscdev);
    u64 result;
    int ret;
    if (count == 0)
        return 0;
    if (count > adev->cmdbuf_size)
    {
        pr_warn("Command buffer too large: %zu > %zu\n", count, adev->cmdbuf_size);
        return -EINVAL;
    }
    if (copy_from_user(adev->cmdbuf_virt, buf, count))
        return -EFAULT;
    pr_info("Submitting command buffer: host_phys=0x%llx, len=%zu\n", (u64)adev->cmdbuf_phys, count);
    pci_write_reg64(adev, REG_DMA_SRC_LO, adev->cmdbuf_phys);
    pci_write_reg64(adev, REG_DMA_DST_LO, 0);
    pci_write_reg(adev, REG_DMA_LEN, (u32)count);
    ret = pci_execute_command(adev, CMD_EXECUTE_CMDBUF, 0, &result);
    if (ret)
    {
        pr_err("CMD_EXECUTE_CMDBUF command failed: %d\n", ret);
        return ret;
    }
    pr_info("Command buffer processed, result=0x%llx\n", result);
    return count;
}

static long protopciem_ioctl(struct file *file, unsigned int cmd, unsigned long arg)
{
    struct pci_device *adev = container_of(file->private_data, struct pci_device, protopciem_miscdev);
    u64 result;
    int ret;
    u32 buffer_id;
    dma_addr_t frame_phys_addr;

    switch (cmd)
    {
    case PROTOPCIEM_IOCTL_SUBMIT_FRAME: {
        if (copy_from_user(&buffer_id, (void __user *)arg, sizeof(buffer_id)))
            return -EFAULT;

        if (buffer_id > 1)
        {
            pr_warn("DMA_FRAME: Invalid buffer_id %u\n", buffer_id);
            return -EINVAL;
        }

        frame_phys_addr = adev->framebuf_phys + (buffer_id * FB_SIZE);

        pr_info("DMA_FRAME: Submitting frame %u: host_phys=0x%llx, len=%u\n", buffer_id, (u64)frame_phys_addr,
                (u32)FB_SIZE);

        pci_write_reg64(adev, REG_DMA_SRC_LO, frame_phys_addr);
        pci_write_reg64(adev, REG_DMA_DST_LO, 0);
        pci_write_reg(adev, REG_DMA_LEN, (u32)FB_SIZE);

        ret = pci_execute_command(adev, CMD_DMA_FRAME, 0, &result);
        if (ret)
        {
            pr_err("CMD_DMA_FRAME command failed: %d\n", ret);
            return ret;
        }

        pr_info("Frame DMA processed, result=0x%llx\n", result);
        break;
    }
    default:
        return -ENOTTY;
    }
    return 0;
}

static int protopciem_mmap(struct file *file, struct vm_area_struct *vma)
{
    struct pci_device *adev = container_of(file->private_data, struct pci_device, protopciem_miscdev);
    unsigned long size = vma->vm_end - vma->vm_start;
    unsigned long offset = vma->vm_pgoff << PAGE_SHIFT;

    if (offset != 0 || size > adev->framebuf_size)
    {
        pr_warn("mmap invalid, offset %lu or size %lu > %zu\n", offset, size, adev->framebuf_size);
        return -EINVAL;
    }

    if (size != MMAP_SIZE)
    {
        pr_warn("mmap: Warning: App mapping %lu bytes, but driver allocated %zu "
                "(for double buffering)\n",
                size, adev->framebuf_size);
    }

    return dma_mmap_coherent(&adev->pdev->dev, vma, adev->framebuf_virt, adev->framebuf_phys, size);
}

static int protopciem_open(struct inode *inode, struct file *file)
{
    pr_info("Device opened\n");
    return 0;
}

static int protopciem_release(struct inode *inode, struct file *file)
{
    pr_info("Device released\n");
    return 0;
}

static const struct file_operations protopciem_fops = {
    .owner = THIS_MODULE,
    .open = protopciem_open,
    .release = protopciem_release,
    .write = protopciem_write,
    .unlocked_ioctl = protopciem_ioctl,
    .compat_ioctl = protopciem_ioctl,
    .mmap = protopciem_mmap,
};

static ssize_t stats_show(struct device *dev, struct device_attribute *attr, char *buf)
{
    struct pci_device *adev = dev_get_drvdata(dev);
    return scnprintf(buf, PAGE_SIZE, "Operations: %llu\nInterrupts: %llu\nErrors: %llu\n", adev->operations_completed,
                     adev->interrupts_received, adev->errors);
}

static DEVICE_ATTR_RO(stats);
static struct attribute *pci_attrs[] = {
    &dev_attr_stats.attr,
    NULL,
};
static const struct attribute_group pci_attr_group = {
    .attrs = pci_attrs,
};
static const struct attribute_group *pci_groups[] = {
    &pci_attr_group,
    NULL,
};

static int pci_probe(struct pci_dev *pdev, const struct pci_device_id *id)
{
    struct pci_device *adev;
    int ret = 0;
    pr_info("==== Probing PCI Card ====\n");
    pr_info("Vendor: 0x%04x Device: 0x%04x\n", pdev->vendor, pdev->device);
    adev = kzalloc(sizeof(*adev), GFP_KERNEL);
    if (!adev)
        return -ENOMEM;
    adev->pdev = pdev;
    spin_lock_init(&adev->lock);
    init_completion(&adev->op_completion);
    pci_set_drvdata(pdev, adev);
    ret = pci_enable_device(pdev);
    if (ret)
        goto err_free;
    ret = pci_request_regions(pdev, "protopciem_pci");
    if (ret)
        goto err_disable;
    adev->bar0 = pci_iomap(pdev, 0, 0);
    if (!adev->bar0)
    {
        ret = -ENOMEM;
        goto err_regions;
    }
    pr_info("BAR0 mapped at %px (size: %llu bytes)\n", adev->bar0, (u64)pci_resource_len(pdev, 0));

    ret = pci_enable_msi(pdev);
    if (ret == 0)
    {
        adev->msi_enabled = true;
        adev->irq = pdev->irq;
        pr_info("MSI enabled, IRQ: %d\n", adev->irq);
        ret = request_irq(adev->irq, pci_irq_handler, 0, "protopciem_pci", adev);
        if (ret)
        {
            pci_disable_msi(pdev);
            adev->msi_enabled = false;
            adev->irq = 0;
        }
    }
    if (!adev->msi_enabled && pdev->irq > 0)
    {
        adev->irq = pdev->irq;
        ret = request_irq(adev->irq, pci_irq_handler, IRQF_SHARED, "protopciem_pci", adev);
        if (ret)
            adev->irq = 0;
        else
            pr_info("Legacy interrupt enabled, IRQ: %d\n", adev->irq);
    }
    if (adev->irq == 0)
        pr_warn("No interrupt available. Driver will use polling.");

    pci_set_master(pdev);
    adev->wq = create_singlethread_workqueue("pci_wq");
    if (!adev->wq)
    {
        ret = -ENOMEM;
        goto err_irq;
    }
    INIT_WORK(&adev->work, pci_work_handler);
    pci_reset(adev);

    adev->cmdbuf_size = 4 * 1024 * 1024;
    adev->cmdbuf_virt = dma_alloc_coherent(&pdev->dev, adev->cmdbuf_size, &adev->cmdbuf_phys, GFP_KERNEL);
    if (!adev->cmdbuf_virt)
    {
        dev_err(&pdev->dev, "Failed to allocate DMA command buffer\n");
        ret = -ENOMEM;
        goto err_wq;
    }
    dev_info(&pdev->dev, "DMA command buffer: virt=%px phys=%pad size=%zu\n", adev->cmdbuf_virt, &adev->cmdbuf_phys,
             adev->cmdbuf_size);

    adev->framebuf_size = MMAP_SIZE;
    adev->framebuf_virt = dma_alloc_coherent(&pdev->dev, adev->framebuf_size, &adev->framebuf_phys, GFP_KERNEL);
    if (!adev->framebuf_virt)
    {
        dev_err(&pdev->dev, "Failed to allocate DMA frame buffer\n");
        ret = -ENOMEM;
        goto err_dma_cmdbuf;
    }
    dev_info(&pdev->dev, "DMA frame buffer (double): virt=%px phys=%pad size=%zu\n", adev->framebuf_virt,
             &adev->framebuf_phys, adev->framebuf_size);

    adev->protopciem_miscdev.minor = MISC_DYNAMIC_MINOR;
    adev->protopciem_miscdev.name = "protopciem";
    adev->protopciem_miscdev.fops = &protopciem_fops;
    adev->protopciem_miscdev.parent = &pdev->dev;
    ret = misc_register(&adev->protopciem_miscdev);
    if (ret)
    {
        pr_err("Failed to register misc device: %d\n", ret);
        goto err_dma_framebuf;
    }

    pci_write_reg(adev, REG_CONTROL, CTRL_ENABLE);
    pr_info("Device enabled, ready for userspace on /dev/%s\n", adev->protopciem_miscdev.name);
    pr_info("==== Probe Complete ====\n");
    return 0;

err_dma_framebuf:
    dma_free_coherent(&pdev->dev, adev->framebuf_size, adev->framebuf_virt, adev->framebuf_phys);
err_dma_cmdbuf:
    dma_free_coherent(&pdev->dev, adev->cmdbuf_size, adev->cmdbuf_virt, adev->cmdbuf_phys);
err_wq:
    destroy_workqueue(adev->wq);
err_irq:
    if (adev->irq)
        free_irq(adev->irq, adev);
    if (adev->msi_enabled)
        pci_disable_msi(pdev);
    if (adev->bar0)
        pci_iounmap(pdev, adev->bar0);
err_regions:
    pci_release_regions(pdev);
err_disable:
    pci_disable_device(pdev);
err_free:
    kfree(adev);
    return ret;
}

static void pci_remove(struct pci_dev *pdev)
{
    struct pci_device *adev = pci_get_drvdata(pdev);
    pr_info("Removing PCI device\n");
    if (!adev)
        return;
    misc_deregister(&adev->protopciem_miscdev);
    if (adev->cmdbuf_virt)
    {
        dma_free_coherent(&pdev->dev, adev->cmdbuf_size, adev->cmdbuf_virt, adev->cmdbuf_phys);
    }
    if (adev->framebuf_virt)
    {
        dma_free_coherent(&pdev->dev, adev->framebuf_size, adev->framebuf_virt, adev->framebuf_phys);
    }
    pci_write_reg(adev, REG_CONTROL, 0);
    if (adev->wq)
    {
        cancel_work_sync(&adev->work);
        destroy_workqueue(adev->wq);
    }
    if (adev->irq)
        free_irq(adev->irq, adev);
    if (adev->msi_enabled)
        pci_disable_msi(pdev);
    if (adev->bar0)
        pci_iounmap(pdev, adev->bar0);
    pci_release_regions(pdev);
    pci_disable_device(pdev);
    kfree(adev);
    pr_info("Device removed\n");
}

static const struct pci_device_id pci_ids[] = {{PCI_DEVICE(PCIEM_PCI_VENDOR_ID, PCIEM_PCI_DEVICE_ID)},
                                               {
                                                   0,
                                               }};

MODULE_DEVICE_TABLE(pci, pci_ids);

static struct pci_driver pci_driver = {
    .name = "protopciem_pci",
    .id_table = pci_ids,
    .probe = pci_probe,
    .remove = pci_remove,
    .dev_groups = pci_groups,
};

module_pci_driver(pci_driver);

```

`examples/protopciem/protopciem_device.h`:

```h
#ifndef PROTOPCIEM_DEVICE_H
#define PROTOPCIEM_DEVICE_H

#define PCIEM_PCI_VENDOR_ID 0x1F0C
#define PCIEM_PCI_DEVICE_ID 0x0001

#define PCIEM_BAR0_SIZE (64 * 1024)
#define PCIEM_BAR2_SIZE (1024 * 1024)

#define REG_CONTROL 0x00
#define REG_STATUS 0x04
#define REG_CMD 0x08
#define REG_DATA 0x0C
#define REG_RESULT_LO 0x10
#define REG_RESULT_HI 0x14
#define REG_DMA_SRC_LO 0x20
#define REG_DMA_SRC_HI 0x24
#define REG_DMA_DST_LO 0x28
#define REG_DMA_DST_HI 0x2C
#define REG_DMA_LEN 0x30
#define REG_DMA_FLAGS 0x34

#define CTRL_RESET BIT(1)
#define STATUS_BUSY BIT(0)
#define STATUS_DONE BIT(1)
#define STATUS_ERROR BIT(2)

#define CMD_ADD 0x01
#define CMD_MULTIPLY 0x02
#define CMD_XOR 0x03
#define CMD_PROCESS_BUFFER 0x04
#define CMD_EXECUTE_CMDBUF 0x05
#define CMD_DMA_FRAME 0x06
#define CMD_DMA_P2P_READ  0x07
#define CMD_DMA_P2P_WRITE 0x08

#endif /* PROTOPCIEM_DEVICE_H */
```

`examples/protopciem/qemu/protopciem_backend.c`:

```c
#include "qemu/osdep.h"
#include "hw/misc/protopciem_backend.h"
#include "hw/irq.h"
#include "hw/qdev-properties-system.h"
#include "hw/qdev-properties.h"
#include "hw/sysbus.h"
#include "qemu/log.h"
#include "qemu/main-loop.h"
#include "qemu/module.h"
#include "qemu/timer.h"
#include "ui/console.h"
#include "ui/pixel_ops.h"
#include <sys/socket.h>
#include <sys/un.h>

#define QEMU_SOCKET_PATH "/tmp/pciem_qemu.sock"

#define FATAL_ERROR(...)                                                       \
    do {                                                                       \
        printf("ProtoPCIem FATAL: " __VA_ARGS__);                             \
        printf("\n");                                                          \
        exit(1);                                                               \
    } while (0)

#define MSG_REGISTER_WRITE 1
#define MSG_REGISTER_READ  2
#define MSG_RAISE_IRQ      3
#define MSG_DMA_READ       4
#define MSG_DMA_WRITE      5

struct qemu_msg {
    uint32_t type;
    uint32_t offset;
    uint64_t value;
    uint64_t addr;
    uint32_t len;
} __attribute__((packed));

struct qemu_resp {
    uint32_t status;
    uint64_t value;
} __attribute__((packed));

static void gpu_draw_pixel(ProtoPCIemState *s, int x, int y, uint8_t r, uint8_t g, uint8_t b)
{
    if (x < 0 || x >= FB_WIDTH || y < 0 || y >= FB_HEIGHT)
    {
        return;
    }
    int idx = (y * FB_WIDTH + x) * 3;
    s->framebuffer[idx + 0] = r;
    s->framebuffer[idx + 1] = g;
    s->framebuffer[idx + 2] = b;
}

static void gpu_draw_line(ProtoPCIemState *s, int x0, int y0, int x1, int y1, uint8_t r, uint8_t g, uint8_t b)
{
    int dx = abs(x1 - x0), sx = x0 < x1 ? 1 : -1;
    int dy = -abs(y1 - y0), sy = y0 < y1 ? 1 : -1;
    int err = dx + dy, e2;
    for (;;)
    {
        gpu_draw_pixel(s, x0, y0, r, g, b);
        if (x0 == x1 && y0 == y1)
            break;
        e2 = 2 * err;
        if (e2 >= dy)
        {
            err += dy;
            x0 += sx;
        }
        if (e2 <= dx)
        {
            err += dx;
            y0 += sy;
        }
    }
}

static void gpu_clear(ProtoPCIemState *s, uint8_t r, uint8_t g, uint8_t b)
{
    if (r == g && g == b)
    {
        memset(s->framebuffer, r, FB_SIZE);
    }
    else
    {
        for (int i = 0; i < FB_WIDTH * FB_HEIGHT; i++)
        {
            s->framebuffer[i * 3 + 0] = r;
            s->framebuffer[i * 3 + 1] = g;
            s->framebuffer[i * 3 + 2] = b;
        }
    }
}

static void gpu_blit_rect(ProtoPCIemState *s, uint16_t x, uint16_t y, uint16_t width, uint16_t height,
                          const uint8_t *data)
{
    for (int j = 0; j < height; j++)
    {
        for (int i = 0; i < width; i++)
        {
            int src_idx = (j * width + i) * 3;
            gpu_draw_pixel(s, x + i, y + j, data[src_idx + 0], data[src_idx + 1], data[src_idx + 2]);
        }
    }
}

static void backend_update_display(void *opaque)
{
    ProtoPCIemState *s = opaque;
    DisplaySurface *surface = qemu_console_surface(s->con);
    if (!surface)
        return;

    uint8_t *d = surface_data(surface);
    int stride = surface_stride(surface);
    uint8_t *src = s->framebuffer;

    for (int y = 0; y < FB_HEIGHT; y++)
    {
        uint32_t *dst_row = (uint32_t *)(d + y * stride);
        uint8_t *src_row = src + y * FB_WIDTH * 3;
        for (int x = 0; x < FB_WIDTH; x++)
        {
            uint8_t r = src_row[x * 3 + 0];
            uint8_t g = src_row[x * 3 + 1];
            uint8_t b = src_row[x * 3 + 2];
            dst_row[x] = rgb_to_pixel32(r, g, b);
        }
    }
    dpy_gfx_update(s->con, 0, 0, FB_WIDTH, FB_HEIGHT);
}

static void backend_execute_command_buffer(ProtoPCIemState *s)
{
    uint8_t *p = s->cmd_buffer;
    uint8_t *end = p + s->dma_len;

    while (p < end && (p + sizeof(struct cmd_header)) <= end)
    {
        if ((uintptr_t)p % _Alignof(struct cmd_header) != 0)
        {
            FATAL_ERROR("Misaligned command");
        }

        struct cmd_header *hdr = (struct cmd_header *)p;

        if (hdr->length == 0 || (p + hdr->length) > end)
        {
            FATAL_ERROR("Corrupt command buffer");
        }

        switch (hdr->opcode)
        {
        case CMD_OP_NOP:
            break;
        case CMD_OP_CLEAR: {
            struct cmd_clear *cmd = (struct cmd_clear *)p;
            gpu_clear(s, cmd->r, cmd->g, cmd->b);
            break;
        }
        case CMD_OP_DRAW_LINE: {
            struct cmd_draw_line *cmd = (struct cmd_draw_line *)p;
            gpu_draw_line(s, cmd->x0, cmd->y0, cmd->x1, cmd->y1, cmd->r, cmd->g, cmd->b);
            break;
        }
        case CMD_OP_BLIT_RECT: {
            struct cmd_blit_rect *cmd = (struct cmd_blit_rect *)p;
            const uint8_t *data = (const uint8_t *)(cmd + 1);
            gpu_blit_rect(s, cmd->x, cmd->y, cmd->width, cmd->height, data);
            break;
        }
        default:
            printf("Unknown opcode 0x%x\n", hdr->opcode);
            exit(1);
            break;
        }

        p += hdr->length;
    }
}

static int dma_read_from_guest(ProtoPCIemState *s, uint64_t guest_addr,
                                void *dst, uint32_t len)
{
    struct qemu_msg msg;
    struct qemu_resp resp;

    msg.type = MSG_DMA_READ;
    msg.addr = guest_addr;
    msg.len = len;

    if (write(s->socket_fd, &msg, sizeof(msg)) != sizeof(msg)) {
        perror("[QEMU] Failed to send DMA read request");
        return -1;
    }

    if (read(s->socket_fd, &resp, sizeof(resp)) != sizeof(resp)) {
        perror("[QEMU] Failed to receive DMA response");
        return -1;
    }

    if (resp.status != 0) {
        fprintf(stderr, "[QEMU] DMA read failed with status %d\n", resp.status);
        return -1;
    }

    ssize_t total = 0;
    while (total < len) {
        ssize_t n = read(s->socket_fd, (uint8_t *)dst + total, len - total);
        if (n <= 0) {
            perror("[QEMU] Failed to read DMA data");
            return -1;
        }
        total += n;
    }

    return 0;
}

static void backend_process_complete(void *opaque)
{
    ProtoPCIemState *s = opaque;

    switch (s->cmd)
    {
    case CMD_DMA_FRAME:
    case CMD_EXECUTE_CMDBUF: {
        uint64_t src_addr = ((uint64_t)s->dma_src_hi << 32) | s->dma_src_lo;
        uint32_t len = s->dma_len;

        if (s->cmd == CMD_EXECUTE_CMDBUF)
        {
            if (len > s->cmd_buffer_size)
                len = s->cmd_buffer_size;

            if (dma_read_from_guest(s, src_addr, s->cmd_buffer, len) == 0)
            {
                backend_execute_command_buffer(s);
            }
        }
        else if (s->cmd == CMD_DMA_FRAME)
        {
            if (len != FB_SIZE)
            {
                FATAL_ERROR("DMA Frame size mismatch");
            }

            if (dma_read_from_guest(s, src_addr, s->framebuffer, len) == 0)
            {
                backend_update_display(s);
            }
        }

        s->status |= STATUS_DONE;
        s->status &= ~STATUS_BUSY;

        struct qemu_msg msg;
        msg.type = MSG_RAISE_IRQ;
        msg.offset = s->status;
        uint64_t result = ((uint64_t)s->result_hi << 32) | s->result_lo;
        msg.value = result;

        if (write(s->socket_fd, &msg, sizeof(msg)) != sizeof(msg))
        {
            perror("[QEMU] Failed to send IRQ notification");
        }
        return;
    }
    default:
        FATAL_ERROR("Unknown command");
    }
}

static void backend_handle_socket_event(void *opaque)
{
    ProtoPCIemState *s = PROTOPCIEM_BACKEND(opaque);
    struct qemu_msg msg;
    struct qemu_resp resp;

    ssize_t n = read(s->socket_fd, &msg, sizeof(msg));
    if (n != sizeof(msg))
    {
        if (n <= 0)
        {
            printf("[QEMU] Connection closed\n");
            qemu_set_fd_handler(s->socket_fd, NULL, NULL, NULL);
            close(s->socket_fd);
            s->socket_fd = -1;
        }
        return;
    }

    resp.status = 0;
    resp.value = 0;

    switch (msg.type) {
    case MSG_REGISTER_READ: {
        switch (msg.offset) {
        case REG_CONTROL:  resp.value = s->control; break;
        case REG_STATUS:   resp.value = s->status; break;
        case REG_CMD:      resp.value = s->cmd; break;
        case REG_DATA:     resp.value = s->data; break;
        case REG_RESULT_LO: resp.value = s->result_lo; break;
        case REG_RESULT_HI: resp.value = s->result_hi; break;
        case REG_DMA_SRC_LO: resp.value = s->dma_src_lo; break;
        case REG_DMA_SRC_HI: resp.value = s->dma_src_hi; break;
        case REG_DMA_DST_LO: resp.value = s->dma_dst_lo; break;
        case REG_DMA_DST_HI: resp.value = s->dma_dst_hi; break;
        case REG_DMA_LEN:  resp.value = s->dma_len; break;
        default: resp.status = -1; break;
        }
        if (write(s->socket_fd, &resp, sizeof(resp)) != sizeof(resp)) {
            perror("[QEMU] Failed to send register read response");
            return;
        }

        break;
    }

    case MSG_REGISTER_WRITE: {
        switch (msg.offset) {
        case REG_CONTROL:
            s->control = msg.value;
            if (msg.value & 2) {
                s->status = 0;
                s->cmd = 0;
                s->data = 0;
                gpu_clear(s, 0, 0, 0);
                backend_update_display(s);
            }
            break;
        case REG_STATUS:   s->status = msg.value; break;
        case REG_DATA:     s->data = msg.value; break;
        case REG_RESULT_LO: s->result_lo = msg.value; break;
        case REG_RESULT_HI: s->result_hi = msg.value; break;
        case REG_DMA_SRC_LO: s->dma_src_lo = msg.value; break;
        case REG_DMA_SRC_HI: s->dma_src_hi = msg.value; break;
        case REG_DMA_DST_LO: s->dma_dst_lo = msg.value; break;
        case REG_DMA_DST_HI: s->dma_dst_hi = msg.value; break;
        case REG_DMA_LEN:  s->dma_len = msg.value; break;
        case REG_CMD:
            s->cmd = msg.value;
            s->status &= ~STATUS_DONE;
            s->status |= STATUS_BUSY;
            backend_process_complete(s);
            break;
        default:
            resp.status = -1;
            break;
        }
        if (write(s->socket_fd, &resp, sizeof(resp)) != sizeof(resp)) {
            perror("[QEMU] Failed to send register write response");
            return;
        }
        break;
    }

    default:
        printf("[QEMU] Unknown message type: %d\n", msg.type);
        resp.status = -1;
        if (write(s->socket_fd, &resp, sizeof(resp)) != sizeof(resp)) {
            perror("[QEMU] Failed to send error response");
            return;
        }
        break;
    }
}

static uint64_t backend_read(void *opaque, hwaddr offset, unsigned size)
{
    return 0;
}
static void backend_write(void *opaque, hwaddr offset, uint64_t value, unsigned size)
{
}
static const MemoryRegionOps backend_ops = {
    .read = backend_read,
    .write = backend_write,
    .endianness = DEVICE_LITTLE_ENDIAN,
    .valid =
        {
            .min_access_size = 4,
            .max_access_size = 8,
        },
};

static void backend_invalidate_display(void *opaque)
{
    backend_update_display(opaque);
}
static const GraphicHwOps backend_gfx_ops = {
    .invalidate = backend_invalidate_display,
    .gfx_update = backend_update_display,
};

static void protopciem_backend_realize(DeviceState *dev, Error **errp)
{
    ProtoPCIemState *s = PROTOPCIEM_BACKEND(dev);
    SysBusDevice *sbd = SYS_BUS_DEVICE(dev);
    struct sockaddr_un addr;

    memory_region_init_io(&s->iomem, OBJECT(s), &backend_ops, s,
                         "protopciem-backend", 0x1000);
    sysbus_init_mmio(sbd, &s->iomem);
    sysbus_init_irq(sbd, &s->irq);

    printf("[QEMU] Connecting to userspace emulator at %s\n", QEMU_SOCKET_PATH);

    s->socket_fd = socket(AF_UNIX, SOCK_STREAM, 0);
    if (s->socket_fd < 0) {
        perror("[QEMU] Failed to create socket");
        return;
    }

    memset(&addr, 0, sizeof(addr));
    addr.sun_family = AF_UNIX;
    strncpy(addr.sun_path, QEMU_SOCKET_PATH, sizeof(addr.sun_path) - 1);

    int retries = 5;
    while (retries-- > 0) {
        if (connect(s->socket_fd, (struct sockaddr *)&addr, sizeof(addr)) == 0) {
            printf("[QEMU] Connected to userspace emulator!\n");
            break;
        }
        printf("[QEMU] Connection failed, retrying... (%d left)\n", retries);
        sleep(1);
    }

    if (retries < 0) {
        perror("[QEMU] Failed to connect to userspace emulator");
        close(s->socket_fd);
        return;
    }

    qemu_set_fd_handler(s->socket_fd, backend_handle_socket_event, NULL, s);

    s->cmd_buffer_size = CMD_BUFFER_SIZE;
    s->cmd_buffer = g_malloc0(s->cmd_buffer_size);
    s->framebuffer = g_malloc0(FB_SIZE);

    s->con = graphic_console_init(dev, 0, &backend_gfx_ops, s);
    qemu_console_resize(s->con, FB_WIDTH, FB_HEIGHT);

    printf("[QEMU] Backend initialized\n");
}

static void protopciem_backend_class_init(ObjectClass *klass, const void *data)
{
    DeviceClass *dc = DEVICE_CLASS(klass);
    dc->realize = protopciem_backend_realize;
    dc->desc = "ProtoPCIem Accelerator Backend";
}

static const TypeInfo protopciem_backend_info = {
    .name = TYPE_PROTOPCIEM_BACKEND,
    .parent = TYPE_SYS_BUS_DEVICE,
    .instance_size = sizeof(ProtoPCIemState),
    .class_init = protopciem_backend_class_init,
};

static void protopciem_backend_register_types(void)
{
    type_register_static(&protopciem_backend_info);
}

type_init(protopciem_backend_register_types)
```

`examples/protopciem/qemu/protopciem_backend.h`:

```h
#ifndef HW_MISC_PROTOPCIEM_BACKEND_H
#define HW_MISC_PROTOPCIEM_BACKEND_H

#include "hw/sysbus.h"
#include "protopciem_cmds.h"
#include "qemu/bitops.h"
#include "qom/object.h"
#include "ui/console.h"
#include <sys/ioctl.h>

#include "pciem_ioctl.h"
#include "protopciem_device.h"

#define TYPE_PROTOPCIEM_BACKEND "protopciem-backend"
OBJECT_DECLARE_SIMPLE_TYPE(ProtoPCIemState, PROTOPCIEM_BACKEND)

#define FB_WIDTH 640
#define FB_HEIGHT 480
#define FB_SIZE (FB_WIDTH * FB_HEIGHT * 3)

#define CMD_BUFFER_SIZE (4 * 1024 * 1024)

typedef struct ProtoPCIemState
{
    SysBusDevice parent_obj;

    MemoryRegion iomem;

    QemuConsole *con;
    uint8_t *framebuffer;

    uint32_t control;
    uint32_t status;
    uint32_t cmd;
    uint32_t data;
    uint32_t result_lo;
    uint32_t result_hi;
    uint32_t dma_src_lo;
    uint32_t dma_src_hi;
    uint32_t dma_dst_lo;
    uint32_t dma_dst_hi;
    uint32_t dma_len;

    uint8_t *cmd_buffer;
    size_t cmd_buffer_size;

    qemu_irq irq;

    int shim_fd;
    int socket_fd;
} ProtoPCIemState;

#endif
```

`examples/protopciem/qemu/protopciem_cmds.h`:

```h
#ifndef PROTOPCIEM_CMDS_H
#define PROTOPCIEM_CMDS_H

#include <stdint.h>

#define CMD_OP_NOP 0x00
#define CMD_OP_CLEAR 0x01
#define CMD_OP_DRAW_LINE 0x02
#define CMD_OP_BLIT_RECT 0x03

struct cmd_header
{
    uint16_t opcode;
    uint16_t reserved;
    uint32_t length;
} __attribute__((packed));

struct cmd_clear
{
    struct cmd_header hdr;
    uint8_t r;
    uint8_t g;
    uint8_t b;
    uint8_t a;
} __attribute__((packed));

struct cmd_draw_line
{
    struct cmd_header hdr;
    uint16_t x0;
    uint16_t y0;
    uint16_t x1;
    uint16_t y1;
    uint8_t r;
    uint8_t g;
    uint8_t b;
    uint8_t a;
} __attribute__((packed));

struct cmd_blit_rect
{
    struct cmd_header hdr;
    uint16_t x;
    uint16_t y;
    uint16_t width;
    uint16_t height;
} __attribute__((packed));

#endif /* PROTOPCIEM_CMDS_H */
```

`examples/protopciem/userspace/Makefile`:

```
THIS_MAKEFILE := $(lastword $(MAKEFILE_LIST))
CUR_DIR    := $(abspath $(dir $(THIS_MAKEFILE)))

CFLAGS := -Wall -Wextra -O2 -pthread -I$(CUR_DIR)/../../../include

protopciem_card: protopciem_card.c
	$(CC) $(CFLAGS) -o $@ $^

clean:
	rm -f protopciem_card

```

`examples/protopciem/userspace/protopciem_card.c`:

```c
// SPDX-License-Identifier: GPL-2.0 OR MIT
/*
 * Copyright (C) 2025-2026  Joel Bueno <buenocalvachejoel@gmail.com>
 */
#include <err.h>
#include <errno.h>
#include <fcntl.h>
#include <linux/pci_regs.h>
#include <poll.h>
#include <pthread.h>
#include <signal.h>
#include <stdatomic.h>
#include <stdint.h>
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <sys/eventfd.h>
#include <sys/ioctl.h>
#include <sys/mman.h>
#include <sys/socket.h>
#include <sys/un.h>
#include <unistd.h>

#include "pciem_userspace.h"
#include "../protopciem_device.h"

#define QEMU_SOCKET_PATH "/tmp/pciem_qemu.sock"

#ifndef BIT
#define BIT(nr) (1UL << (nr))
#endif

#define MSG_REGISTER_WRITE 1
#define MSG_REGISTER_READ 2
#define MSG_RAISE_IRQ 3
#define MSG_DMA_READ 4
#define MSG_DMA_WRITE 5

struct qemu_msg
{
    uint32_t type;
    uint32_t offset;
    uint64_t value;
    uint64_t addr;
    uint32_t len;
} __attribute__((packed));

struct qemu_resp
{
    uint32_t status;
    uint64_t value;
} __attribute__((packed));

struct device_state
{
    volatile uint32_t *bar0;
    volatile uint32_t *bar2;
    size_t bar0_size;
    size_t bar2_size;
    int pciem_fd;
    int instance_fd;
    int qemu_sock;
    int event_fd;
    int irq_fd;
    atomic_t running;
    int qemu_connected;
    pthread_t qemu_thread;
    uint8_t *dma_bounce_buf;

    pthread_mutex_t sock_lock;
    pthread_cond_t ack_cond;
    volatile int waiting_for_ack;
    struct qemu_resp last_resp;

    struct pciem_shared_ring *event_ring;
};

static struct device_state dev_state;

static int dev_running(struct device_state *st)
{
    return atomic_load(&st->running);
}

static void dev_stop(struct device_state *st)
{
    atomic_store(&st->running, 0);
}

static void signal_handler(int signum)
{
    printf("\n[\x1b[31m*\x1b[0m] %d received, trying to exit...\n", signum);
    dev_stop(&dev_state);
}

static int create_qemu_socket(void)
{
    int sock;
    struct sockaddr_un addr;

    unlink(QEMU_SOCKET_PATH);

    sock = socket(AF_UNIX, SOCK_STREAM, 0);
    if (sock < 0)
    {
        perror("Failed to create socket");
        return -1;
    }

    memset(&addr, 0, sizeof(addr));
    addr.sun_family = AF_UNIX;
    strncpy(addr.sun_path, QEMU_SOCKET_PATH, sizeof(addr.sun_path) - 1);

    if (bind(sock, (struct sockaddr *)&addr, sizeof(addr)) < 0)
    {
        perror("Failed to bind socket");
        close(sock);
        return -1;
    }

    if (listen(sock, 1) < 0)
    {
        perror("Failed to listen on socket");
        close(sock);
        return -1;
    }

    printf("[\x1b[32m*\x1b[0m] Socket at: %s\n", QEMU_SOCKET_PATH);
    printf("[\x1b[33m*\x1b[0m] Waiting for QEMU to connect...\n");

    return sock;
}

static int wait_for_qemu_connection(int listen_sock)
{
    int client_sock;
    struct sockaddr_un client_addr;
    socklen_t client_len = sizeof(client_addr);

    client_sock = accept(listen_sock, (struct sockaddr *)&client_addr, &client_len);
    if (client_sock < 0)
    {
        perror("Failed to accept connection");
        return -1;
    }

    printf("[\x1b[32m*\x1b[0m] QEMU connected!\n");
    return client_sock;
}

static int writen(int fd, const void *buf, size_t n)
{
    size_t written = 0;
    while (written < n)
    {
        ssize_t r = write(fd, (const char *)buf + written, n - written);
        if (r <= 0)
        {
            if (r < 0 && errno == EINTR) continue;
            return -1;
        }
        written += r;
    }
    return 0;
}

static int send_register_write_sync(uint32_t offset, uint32_t value)
{
    struct qemu_msg msg;
    struct timespec timeout;
    int ret;

    msg.type = MSG_REGISTER_WRITE;
    msg.offset = offset;
    msg.value = value;

    pthread_mutex_lock(&dev_state.sock_lock);

    if (write(dev_state.qemu_sock, &msg, sizeof(msg)) != sizeof(msg))
    {
        perror("Socket write failed");
        pthread_mutex_unlock(&dev_state.sock_lock);
        return -1;
    }

    clock_gettime(CLOCK_REALTIME, &timeout);
    timeout.tv_sec += 10;
    
    dev_state.waiting_for_ack = 1;
    while (dev_state.waiting_for_ack)
    {
        ret = pthread_cond_timedwait(&dev_state.ack_cond, &dev_state.sock_lock, &timeout);
        if (ret == ETIMEDOUT) {
            printf("[!] QEMU timeout on reg 0x%x, disconnecting\n", offset);
            dev_state.waiting_for_ack = 0;
            dev_state.qemu_connected = 0;
            pthread_mutex_unlock(&dev_state.sock_lock);
            return -1;
        }
    }
    
    pthread_mutex_unlock(&dev_state.sock_lock);
    return 0;
}

static int forward_command_to_qemu(void)
{
    if (send_register_write_sync(REG_CONTROL, dev_state.bar0[REG_CONTROL / 4]) < 0) return -1;
    if (send_register_write_sync(REG_DATA, dev_state.bar0[REG_DATA / 4]) < 0) return -1;
    if (send_register_write_sync(REG_DMA_SRC_LO, dev_state.bar0[REG_DMA_SRC_LO / 4]) < 0) return -1;
    if (send_register_write_sync(REG_DMA_SRC_HI, dev_state.bar0[REG_DMA_SRC_HI / 4]) < 0) return -1;
    if (send_register_write_sync(REG_DMA_DST_LO, dev_state.bar0[REG_DMA_DST_LO / 4]) < 0) return -1;
    if (send_register_write_sync(REG_DMA_DST_HI, dev_state.bar0[REG_DMA_DST_HI / 4]) < 0) return -1;
    if (send_register_write_sync(REG_DMA_LEN, dev_state.bar0[REG_DMA_LEN / 4]) < 0) return -1;
    if (send_register_write_sync(REG_CMD, dev_state.bar0[REG_CMD / 4]) < 0) return -1;
    return 0;
}

static void inject_irq(uint32_t vector)
{
    if (dev_state.irq_fd >= 0)
    {
        uint64_t val = 1;
        ssize_t ret = write(dev_state.irq_fd, &val, sizeof(val));
        if (ret != sizeof(val))
        {
            struct pciem_irq_inject irq = {.vector = vector};
            ioctl(dev_state.pciem_fd, PCIEM_IOCTL_INJECT_IRQ, &irq);
        }
    }
    else
    {
        struct pciem_irq_inject irq = {.vector = vector};
        ioctl(dev_state.pciem_fd, PCIEM_IOCTL_INJECT_IRQ, &irq);
    }
}

static void *qemu_handler_thread(void *arg)
{
    struct qemu_msg msg;
    struct qemu_resp resp;
    uint32_t header;
    (void) arg;

    while (dev_running(&dev_state) && dev_state.qemu_connected)
    {
        ssize_t n = read(dev_state.qemu_sock, &header, sizeof(header));
        if (n != sizeof(header))
        {
            if (n == 0)
                printf("[\x1b[31m!\x1b[0m] QEMU connection closed!\n");
            else
                perror("Socket read failed");
            dev_state.qemu_connected = 0;
            break;
        }

        if (header == MSG_DMA_READ || header == MSG_RAISE_IRQ)
        {
            msg.type = header;
            n = read(dev_state.qemu_sock, ((char *)&msg) + 4, sizeof(msg) - 4);
            if (n != sizeof(msg) - 4)
                break;

            if (msg.type == MSG_DMA_READ)
            {
                struct pciem_dma_op dma_op = {.guest_iova = msg.addr,
                                              .user_addr = (uint64_t)dev_state.dma_bounce_buf,
                                              .length = msg.len,
                                              .flags = PCIEM_DMA_FLAG_READ,
                                              .pasid = 0};

                if (ioctl(dev_state.pciem_fd, PCIEM_IOCTL_DMA, &dma_op) < 0)
                {
                    perror("[X] DMA read failed");
                    resp.status = -1;
                }
                else
                {
                    resp.status = 0;
                }

                pthread_mutex_lock(&dev_state.sock_lock);
                if (write(dev_state.qemu_sock, &resp, sizeof(resp)) != sizeof(resp))
                {
                    perror("Socket write failed");
                    pthread_mutex_unlock(&dev_state.sock_lock);
                    break;
                }

                if (resp.status == 0)
                {
                    if (writen(dev_state.qemu_sock, dev_state.dma_bounce_buf, msg.len) < 0) {
                        perror("Failed to write DMA data to QEMU");
                        pthread_mutex_unlock(&dev_state.sock_lock);
                        break;
                    }
                }
                pthread_mutex_unlock(&dev_state.sock_lock);
            }
            else if (msg.type == MSG_RAISE_IRQ)
            {
                uint32_t status = msg.offset;
                uint64_t result = msg.value;

                dev_state.bar0[REG_STATUS / 4] = status;
                dev_state.bar0[REG_RESULT_LO / 4] = (uint32_t)(result & 0xFFFFFFFF);
                dev_state.bar0[REG_RESULT_HI / 4] = (uint32_t)(result >> 32);
                dev_state.bar0[REG_CMD / 4] = 0;

                inject_irq(0);
            }
        }
        else
        {
            resp.status = header;
            n = read(dev_state.qemu_sock, ((char *)&resp) + 4, sizeof(resp) - 4);
            if (n != sizeof(resp) - 4)
                break;

            pthread_mutex_lock(&dev_state.sock_lock);
            if (dev_state.waiting_for_ack)
            {
                dev_state.last_resp = resp;
                dev_state.waiting_for_ack = 0;
                pthread_cond_signal(&dev_state.ack_cond);
            }
            pthread_mutex_unlock(&dev_state.sock_lock);
        }
    }

    printf("[\x1b[31m!\x1b[0m] QEMU forwarding stopped\n");
    return NULL;
}

static void process_command_local(void)
{
    uint32_t cmd = dev_state.bar0[REG_CMD / 4];
    uint32_t data = dev_state.bar0[REG_DATA / 4];
    uint64_t result = 0;
    uint32_t status = STATUS_DONE;

    switch (cmd)
    {
    case CMD_ADD:
        result = (uint64_t)data + data;
        break;
    case CMD_MULTIPLY:
        result = (uint64_t)data * data;
        break;
    case CMD_XOR:
        result = data ^ 0xFFFFFFFF;
        break;
    default:
        status |= STATUS_ERROR;
        break;
    }

    dev_state.bar0[REG_RESULT_LO / 4] = (uint32_t)(result & 0xFFFFFFFF);
    dev_state.bar0[REG_RESULT_HI / 4] = (uint32_t)(result >> 32);
    dev_state.bar0[REG_STATUS / 4] = status;
    dev_state.bar0[REG_CMD / 4] = 0;

    inject_irq(0);
}

static int setup_eventfd(void)
{
    struct pciem_eventfd_config efd_cfg;
    
    dev_state.event_fd = eventfd(0, EFD_CLOEXEC | EFD_NONBLOCK);
    if (dev_state.event_fd < 0)
    {
        perror("Failed to create eventfd");
        return -1;
    }

    efd_cfg.eventfd = dev_state.event_fd;
    efd_cfg.reserved = 0;

    if (ioctl(dev_state.pciem_fd, PCIEM_IOCTL_SET_EVENTFD, &efd_cfg) < 0)
    {
        perror("Failed to set eventfd");
        close(dev_state.event_fd);
        dev_state.event_fd = -1;
        return -1;
    }

    printf("[\x1b[32m*\x1b[0m] Eventfd configured: fd=%d\n", dev_state.event_fd);
    return 0;
}

static int setup_irq_fd(void)
{
    struct pciem_irqfd_config irq_cfg;

    dev_state.irq_fd = eventfd(0, EFD_CLOEXEC | EFD_NONBLOCK);
    if (dev_state.irq_fd < 0)
    {
        perror("Failed to create IRQ eventfd");
        return -1;
    }

    irq_cfg.eventfd = dev_state.irq_fd;
    irq_cfg.vector = 0;
    irq_cfg.flags = 0;
    irq_cfg.reserved = 0;

    if (ioctl(dev_state.pciem_fd, PCIEM_IOCTL_SET_IRQFD, &irq_cfg) < 0)
    {
        perror("Failed to set IRQ eventfd");
        close(dev_state.irq_fd);
        dev_state.irq_fd = -1;
        return -1;
    }

    printf("[\x1b[32m*\x1b[0m] IRQ eventfd configured: fd=%d\n", dev_state.irq_fd);
    return 0;
}

static void handle_bar0_write(struct device_state *st, struct pciem_event *event)
{
    volatile uint32_t *bar0 = st->bar0;

    switch (event->offset)
    {
    case REG_CMD: {
        uint32_t cmd = bar0[REG_CMD / 4];

        if (!cmd)
            return;

        bar0[REG_STATUS / 4] = STATUS_BUSY;
        if (st->qemu_connected &&
            (cmd == CMD_EXECUTE_CMDBUF || cmd == CMD_DMA_FRAME))
        {
            if (forward_command_to_qemu() < 0)
                printf("[!] Failed to forward command to QEMU!\n");
        } else {
            process_command_local();
        }
        break;
    }
    default:
        return;
    }
}

static void handle_event(struct device_state *st, struct pciem_event *event)
{
    if (event->type == PCIEM_EVENT_MMIO_WRITE && event->bar == 0)
        handle_bar0_write(st, event);
}

static int register_device(struct device_state *st)
{
    struct pciem_create_device create = {0};
    struct pciem_bar_config bar0 = {
        .bar_index = 0,
        .size = PCIEM_BAR0_SIZE,
        .flags = PCI_BASE_ADDRESS_SPACE_MEMORY | PCI_BASE_ADDRESS_MEM_TYPE_64,
    };
    struct pciem_bar_config bar2 = {
        .bar_index = 2,
        .size = PCIEM_BAR2_SIZE,
        .flags = PCI_BASE_ADDRESS_SPACE_MEMORY | PCI_BASE_ADDRESS_MEM_TYPE_64 |
                 PCI_BASE_ADDRESS_MEM_PREFETCH,
    };
    struct pciem_config_space cfg = {
        .vendor_id = PCIEM_PCI_VENDOR_ID,
        .device_id = PCIEM_PCI_DEVICE_ID,
        .class_code = {0x00, 0x00, 0x0b}
    };
    struct pciem_cap_config cap = {
        .cap_type = PCIEM_CAP_MSI,
        .msi = {
            .has_64bit = 1,
            .has_masking = 1,
        },
    };
    struct pciem_trace_bar trace = {
        .bar_index = 0,
        .flags = PCIEM_TRACE_WRITES,
    };

    st->pciem_fd = open("/dev/pciem", O_RDWR);
    if (st->pciem_fd < 0) {
        warn("open(/dev/pciem)");
        return -1;
    }

    ioctl(st->pciem_fd, PCIEM_IOCTL_CREATE_DEVICE, &create);
    ioctl(st->pciem_fd, PCIEM_IOCTL_ADD_BAR, &bar0);
    ioctl(st->pciem_fd, PCIEM_IOCTL_ADD_BAR, &bar2);
    ioctl(st->pciem_fd, PCIEM_IOCTL_TRACE_BAR, &trace);
    ioctl(st->pciem_fd, PCIEM_IOCTL_ADD_CAPABILITY, &cap);
    ioctl(st->pciem_fd, PCIEM_IOCTL_SET_CONFIG, &cfg);

    st->instance_fd = ioctl(st->pciem_fd, PCIEM_IOCTL_REGISTER, 0);
    if (st->instance_fd < 0) {
        warn("PCIEM_IOCTL_REGISTER");
        return -1;
    }

    return 0;
}

static int map_device(struct device_state *st)
{
    st->bar0_size = PCIEM_BAR0_SIZE;
    st->bar0 = mmap(NULL, dev_state.bar0_size, PROT_READ | PROT_WRITE,
                    MAP_SHARED, st->instance_fd, 0 * 4096);
    if (st->bar0 == MAP_FAILED) {
        warn("mmap BAR0 failed");
        return -1;
    }

    st->bar2_size = PCIEM_BAR2_SIZE;
    st->bar2 = mmap(NULL, dev_state.bar2_size, PROT_READ | PROT_WRITE,
                    MAP_SHARED, st->instance_fd, 2 * 4096);
    if (st->bar2 == MAP_FAILED) {
        warn("mmap BAR2 failed");
        return -1;
    }

    printf("[\x1b[32m*\x1b[0m] BARs mapped successfully via Instance FD\n");

    st->event_ring = mmap(NULL, sizeof(struct pciem_shared_ring),
                          PROT_READ | PROT_WRITE, MAP_SHARED,
                          st->pciem_fd, 0);
    if (st->event_ring == MAP_FAILED) {
        warn("mmap shared event ring failed");
        return -1;
    }

    return 0;
}

static void init_device(struct device_state *st)
{
    st->pciem_fd = -1;

    pthread_mutex_init(&st->sock_lock, NULL);
    pthread_cond_init(&st->ack_cond, NULL);

    st->instance_fd = -1;
    atomic_store(&st->running, 0);
    st->qemu_connected = 0;

    st->event_ring = MAP_FAILED;
    st->bar0 = MAP_FAILED;
    st->bar2 = MAP_FAILED;

    st->qemu_sock = -1;
    st->dma_bounce_buf = NULL;
    st->event_fd = -1;
    st->irq_fd = -1;
}

static void destroy_device(struct device_state *st)
{
    if (st->event_fd >= 0)
        close(st->event_fd);

    if (st->irq_fd >= 0)
    {
        close(st->irq_fd);
        st->irq_fd = -1;
    }

    if (st->dma_bounce_buf)
        free(st->dma_bounce_buf);

    if (st->qemu_sock >= 0)
        close(st->qemu_sock);

    if (st->event_ring != MAP_FAILED)
        munmap(st->event_ring, sizeof(struct pciem_shared_ring));
    if (st->bar2 != MAP_FAILED)
        munmap((void *)st->bar2, st->bar2_size);
    if (st->bar0 != MAP_FAILED)
        munmap((void *)st->bar0, st->bar0_size);

    if (st->instance_fd >= 0)
        close(st->instance_fd);

    pthread_mutex_destroy(&st->sock_lock);
    pthread_cond_destroy(&st->ack_cond);

    if (st->pciem_fd >= 0)
        close(st->pciem_fd);
}

int main(void)
{
    int listen_sock = -1;
    struct sigaction sa;

    if (geteuid() != 0)
    {
        fprintf(stderr, "ERROR: Must run as root\n");
        return 1;
    }

    init_device(&dev_state);
    atomic_store(&dev_state.running, 1);
    if (register_device(&dev_state) < 0)
        goto cleanup;

    printf("[\x1b[32m*\x1b[0m] Device registered, got instance FD: %d\n",
       dev_state.instance_fd);

    if (map_device(&dev_state) < 0)
        goto cleanup;

    memset(&sa, 0, sizeof(sa));
    sa.sa_handler = signal_handler;
    sigaction(SIGINT, &sa, NULL);
    sigaction(SIGTERM, &sa, NULL);

    listen_sock = create_qemu_socket();
    if (listen_sock < 0)
        goto cleanup;

    fd_set readfds;
    struct timeval timeout = {10, 0};
    FD_ZERO(&readfds);
    FD_SET(listen_sock, &readfds);

    if (select(listen_sock + 1, &readfds, NULL, NULL, &timeout) > 0)
    {
        dev_state.qemu_sock = wait_for_qemu_connection(listen_sock);
        if (dev_state.qemu_sock >= 0)
        {
            dev_state.qemu_connected = 1;
            dev_state.dma_bounce_buf = malloc(4 * 1024 * 1024);
            pthread_create(&dev_state.qemu_thread, NULL, qemu_handler_thread, NULL);
            printf("[\x1b[32m*\x1b[0m] QEMU forwarding mode\n");
        }
    }
    else
    {
        printf("[X] QEMU socket not found, running internal emulation...\n");
    }

    if (setup_eventfd() < 0)
    {
        printf("[!] Failed to setup eventfd, falling back to busy polling\n");
    }

    if (setup_irq_fd() < 0)
    {
        printf("[!] Failed to setup IRQ eventfd, falling back to ioctl\n");
    }

    printf("[\x1b[32m*\x1b[0m] Starting event consumer...\n");
    while (dev_running(&dev_state))
    {
        if (dev_state.event_fd >= 0)
        {
            fd_set rfds;
            struct timeval tv;
            int ret;

            FD_ZERO(&rfds);
            FD_SET(dev_state.event_fd, &rfds);

            tv.tv_sec = 1;
            tv.tv_usec = 0;

            ret = select(dev_state.event_fd + 1, &rfds, NULL, NULL, &tv);
            if (ret < 0)
            {
                if (errno == EINTR)
                    continue;
                perror("select() failed");
                break;
            }
            else if (ret > 0)
            {
                uint64_t efd_count;
                if (read(dev_state.event_fd, &efd_count, sizeof(efd_count)) < 0)
                {
                    if (errno != EAGAIN)
                        perror("eventfd read failed");
                }
            }
        }

        int head = atomic_load(&dev_state.event_ring->head);
        int tail = atomic_load(&dev_state.event_ring->tail);
        
        if (head == tail) {
            // TODO: Maybe yield?
            continue;
        }

        struct pciem_event *event = &dev_state.event_ring->events[head];

        atomic_thread_fence(memory_order_acquire);
        handle_event(&dev_state, event);
        atomic_store(&dev_state.event_ring->head, (head + 1) % PCIEM_RING_SIZE);
    }

cleanup:
    printf("\n[\x1b[31m*\x1b[0m] Exit\n");

    if (dev_state.qemu_connected)
    {
        dev_stop(&dev_state);
        pthread_join(dev_state.qemu_thread, NULL);
        if (dev_state.dma_bounce_buf)
        {
            free(dev_state.dma_bounce_buf);
            dev_state.dma_bounce_buf = NULL;
        }
    }

    if (listen_sock >= 0)
        close(listen_sock);

    destroy_device(&dev_state);

    unlink(QEMU_SOCKET_PATH);
    return 0;
}

```

`include/pciem_capabilities.h`:

```h
/* SPDX-License-Identifier: GPL-2.0-only */
/*
 *  Copyright (C) 2025-2026  Joel Bueno
 *  Copyright (C) 2025-2026  Carlos López
 */

#ifndef PCIEM_CAPABILITIES_H
#define PCIEM_CAPABILITIES_H

#include <linux/pci_regs.h>
#include <linux/types.h>

#define MAX_PCI_CAPS 16

enum pciem_cap_type
{
    PCIEM_CAP_MSI,
    PCIEM_CAP_MSIX,
    PCIEM_CAP_PM,
    PCIEM_CAP_PCIE,
    PCIEM_CAP_VSEC,
    PCIEM_CAP_PASID,
};

struct pciem_cap_msi_config
{
    bool has_64bit;
    bool has_per_vector_masking;
    u8 num_vectors_log2;
};

struct pciem_cap_msix_config
{
    u8 bar_index;
    u32 table_offset;
    u32 pba_offset;
    u16 table_size;
};

struct pciem_cap_pm_config
{
    bool d1_support;
    bool d2_support;
    bool pme_support;
    u8 version;
};

struct pciem_cap_pcie_config
{
    u8 device_type;
    u8 link_width;
    u8 link_speed;
};

struct pciem_cap_vsec_config
{
    u16 vendor_id;
    u16 vsec_id;
    u8 vsec_rev;
    u16 vsec_length;
    u8 *data;
};

struct pciem_cap_pasid_config
{
    u8 max_pasid_width;
    bool execute_permission;
    bool privileged_mode;
};

struct pciem_cap_entry
{
    enum pciem_cap_type type;
    u8 offset;
    u8 size;
    union {
        struct pciem_cap_msi_config msi;
        struct pciem_cap_msix_config msix;
        struct pciem_cap_pm_config pm;
        struct pciem_cap_pcie_config pcie;
        struct pciem_cap_vsec_config vsec;
        struct pciem_cap_pasid_config pasid;
    } config;

    union {
        struct pciem_msi_state
        {
            u16 control;
            u32 address_lo;
            u32 address_hi;
            u16 data;
            u32 mask_bits;
        } msi_state;

        struct pciem_msix_state
        {
            u16 control;
        } msix_state;

        struct pciem_pm_state
        {
            u16 control;
            u16 status;
        } pm_state;

        struct pciem_pasid_state
        {
            u16 control;
            u32 pasid;
        } pasid_state;
    } state;
};

struct pciem_cap_manager
{
    struct pciem_cap_entry caps[MAX_PCI_CAPS];
    int num_caps;
    u8 next_offset;
};

struct pciem_root_complex;

int pciem_add_cap_msi(struct pciem_root_complex *v, struct pciem_cap_msi_config *cfg);
int pciem_add_cap_msix(struct pciem_root_complex *v, struct pciem_cap_msix_config *cfg);
int pciem_add_cap_pm(struct pciem_root_complex *v, struct pciem_cap_pm_config *cfg);
int pciem_add_cap_pcie(struct pciem_root_complex *v, struct pciem_cap_pcie_config *cfg);
int pciem_add_cap_vsec(struct pciem_root_complex *v, struct pciem_cap_vsec_config *cfg);
int pciem_add_cap_pasid(struct pciem_root_complex *v, struct pciem_cap_pasid_config *cfg);

void pciem_init_cap_manager(struct pciem_root_complex *v);
void pciem_build_config_space(struct pciem_root_complex *v);
void pciem_cleanup_cap_manager(struct pciem_root_complex *v);

bool pciem_handle_cap_read(struct pciem_root_complex *v, int where, int size, u32 *value);
bool pciem_handle_cap_write(struct pciem_root_complex *v, int where, int size, u32 value);

#endif /* PCIEM_CAPABILITIES_H */

```

`include/pciem_dma.h`:

```h
/* SPDX-License-Identifier: GPL-2.0-only */
/*
 *  Copyright (C) 2025-2026  Joel Bueno
 *  Copyright (C) 2025-2026  Carlos López
 */

#ifndef PCIEM_DMA_H
#define PCIEM_DMA_H

#include <linux/types.h>

struct pciem_root_complex;

int pciem_dma_read_from_guest(struct pciem_root_complex *v, u64 guest_iova, void *dst, size_t len, u32 pasid);
int pciem_dma_write_to_guest(struct pciem_root_complex *v, u64 guest_iova, const void *src, size_t len, u32 pasid);

u64 pciem_dma_atomic_fetch_add(struct pciem_root_complex *v, u64 guest_iova, u64 val, u32 pasid);
u64 pciem_dma_atomic_fetch_sub(struct pciem_root_complex *v, u64 guest_iova, u64 val, u32 pasid);
u64 pciem_dma_atomic_swap(struct pciem_root_complex *v, u64 guest_iova, u64 val, u32 pasid);
u64 pciem_dma_atomic_cas(struct pciem_root_complex *v, u64 guest_iova, u64 expected, u64 new_val, u32 pasid);
u64 pciem_dma_atomic_fetch_and(struct pciem_root_complex *v, u64 guest_iova, u64 val, u32 pasid);
u64 pciem_dma_atomic_fetch_or(struct pciem_root_complex *v, u64 guest_iova, u64 val, u32 pasid);
u64 pciem_dma_atomic_fetch_xor(struct pciem_root_complex *v, u64 guest_iova, u64 val, u32 pasid);

#define pciem_dma_read(v, iova, dst, len) pciem_dma_read_from_guest(v, iova, dst, len, 0)
#define pciem_dma_write(v, iova, src, len) pciem_dma_write_to_guest(v, iova, src, len, 0)

struct pciem_dma_req
{
    u64 guest_iova;
    u64 host_buf_addr;
    u32 length;
    u32 pasid;
    u8 op_type;
    u8 atomic_op;
    u16 reserved;
    u64 atomic_operand;
    u64 atomic_compare;
} __attribute__((packed));

#define PCIEM_ATOMIC_FETCH_ADD 1
#define PCIEM_ATOMIC_FETCH_SUB 2
#define PCIEM_ATOMIC_SWAP 3
#define PCIEM_ATOMIC_CAS 4
#define PCIEM_ATOMIC_FETCH_AND 5
#define PCIEM_ATOMIC_FETCH_OR 6
#define PCIEM_ATOMIC_FETCH_XOR 7

#endif /* PCIEM_DMA_H */
```

`include/pciem_framework.h`:

```h
/* SPDX-License-Identifier: GPL-2.0-only */
/*
 *  Copyright (C) 2025-2026  Joel Bueno
 *  Copyright (C) 2025-2026  Carlos López
 */

#ifndef PCIEM_FRAMEWORK_H
#define PCIEM_FRAMEWORK_H

#include <linux/completion.h>
#include <linux/fs.h>
#include <linux/irq_work.h>
#include <linux/miscdevice.h>
#include <linux/mm.h>
#include <linux/msi.h>
#include <linux/mutex.h>
#include <linux/pci.h>
#include <linux/platform_device.h>
#include <linux/poll.h>
#include <linux/wait.h>
#include <linux/list.h>
#include <linux/spinlock.h>
#include <linux/workqueue.h>

#ifdef CONFIG_X86
#include <asm/pci.h>
#endif

struct pciem_host_bridge_priv {
    struct pciem_root_complex *v;

#ifdef CONFIG_X86
    struct pci_sysdata sd;
#endif
};

#include "pciem_p2p.h"

struct pciem_mempool {
    phys_addr_t         base;
    resource_size_t     total_size;
    resource_size_t     next_offset;
    spinlock_t          lock;
    struct resource     *res;
    bool                initialized;
};

extern struct pciem_mempool pciem_pool;

phys_addr_t pciem_pool_alloc(resource_size_t size);

enum pciem_bus_mode {
    PCIEM_BUS_MODE_VIRTUAL_ROOT = 0,
    PCIEM_BUS_MODE_ATTACH_TO_HOST = 1,
};

struct pciem_bar_info
{
    resource_size_t size;
    u32 flags;
    bool intercept_page_faults;

    u32 base_addr_val;

    struct resource *res;

    struct resource *allocated_res;
    struct page *pages;
    phys_addr_t phys_addr;
    unsigned int order;
    bool mem_owned_by_framework;

    resource_size_t carved_start;
    resource_size_t carved_end;

    struct list_head vma_list;
    spinlock_t vma_lock;
};

struct pciem_hijack_state {
    struct pci_bus *target_bus;
    int hijacked_slot;
    struct pci_ops *original_ops;
    struct pci_ops proxy_ops;
};

struct pciem_root_complex
{
    struct list_head list_node;

    unsigned int msi_irq;
    struct irq_work msi_irq_work;
    unsigned int pending_msi_irq;
    struct pci_dev *pciem_pdev;
    struct pci_bus *root_bus;
    u8 cfg[256];

    enum pciem_bus_mode bus_mode;

    union {
        struct {
            struct pci_host_bridge *bridge;
            int assigned_domain;
            int assigned_busnr;
        } virtual_root;

        struct pciem_hijack_state hijack;
    } mode_state;

    struct pciem_bar_info bars[PCI_STD_NUM_BARS];
    rwlock_t bars_lock;

    struct platform_device *shared_bridge_pdev;

    struct pciem_cap_manager *cap_mgr;
    rwlock_t cap_lock;

    resource_size_t total_carved_start;
    resource_size_t total_carved_end;
    resource_size_t next_carve_offset;

    struct pciem_p2p_manager *p2p_mgr;

    struct work_struct activation_work;
    bool activated;

    bool detaching;
};

void pciem_trigger_msi(struct pciem_root_complex *v, int vector);
int pciem_complete_init(struct pciem_root_complex *v);
int pciem_start_device(struct pciem_root_complex *v);
int pciem_register_bar(struct pciem_root_complex *v, uint32_t bar_num, resource_size_t size, u32 flags);
struct pciem_root_complex *pciem_alloc_root_complex(void);
void pciem_free_root_complex(struct pciem_root_complex *v);
int pciem_init_bar_tracking(void);
void pciem_cleanup_bar_tracking(void);
void pciem_disable_bar_tracking(void);
void __iomem *pciem_get_driver_bar_vaddr(struct pci_dev *pdev, u32 bar);

#endif /* PCIEM_FRAMEWORK_H */

```

`include/pciem_p2p.h`:

```h
/* SPDX-License-Identifier: GPL-2.0-only */
/*
 *  Copyright (C) 2025-2026  Joel Bueno
 *  Copyright (C) 2025-2026  Carlos López
 */

#ifndef PCIEM_P2P_H
#define PCIEM_P2P_H

#include <linux/types.h>
#include <linux/list.h>
#include <linux/mutex.h>

struct pciem_root_complex;

struct pciem_p2p_region {
    struct list_head list;
    phys_addr_t phys_start;
    resource_size_t size;
    void __iomem *kaddr;
    char name[32];
};

struct pciem_p2p_manager {
    struct list_head regions;
    struct mutex lock;
    size_t max_transfer_size;
    bool enabled;
};

#define PCIEM_P2P_MAX_TRANSFER (16 * 1024 * 1024)

int pciem_p2p_init(struct pciem_root_complex *v, const char *regions_str);
void pciem_p2p_cleanup(struct pciem_root_complex *v);
int pciem_p2p_register_region(struct pciem_root_complex *v,
                               phys_addr_t phys,
                               resource_size_t size,
                               const char *name);
int pciem_p2p_unregister_region(struct pciem_root_complex *v,
                                 phys_addr_t phys);
int pciem_p2p_read(struct pciem_root_complex *v,
                   phys_addr_t phys_addr,
                   void *dst,
                   size_t len);
int pciem_p2p_write(struct pciem_root_complex *v,
                    phys_addr_t phys_addr,
                    const void *src,
                    size_t len);
int pciem_p2p_validate_access(struct pciem_root_complex *v,
                               phys_addr_t phys_addr,
                               size_t len);
struct pciem_p2p_region *pciem_p2p_get_region(struct pciem_root_complex *v,
                                               phys_addr_t phys_addr);

#endif /* PCIEM_P2P_H */
```

`include/pciem_userspace.h`:

```h
/* SPDX-License-Identifier: GPL-2.0-only */
/*
 *  Copyright (C) 2025-2026  Joel Bueno
 *  Copyright (C) 2025-2026  Carlos López
 */

#ifndef PCIEM_USERSPACE_H
#define PCIEM_USERSPACE_H

#ifdef __KERNEL__
#include <linux/atomic.h>
#include <linux/spinlock.h>
#include <linux/types.h>
#include <linux/wait.h>
#include <linux/workqueue.h>
#include <linux/poll.h>
#include <linux/pci_regs.h>
#include "trace/smptrace.h"
#else
#include <stdatomic.h>
#include <stdint.h>
typedef atomic_int atomic_t;
#endif

#define PCIEM_CREATE_FLAG_BUS_MODE_MASK     0x00000003
#define PCIEM_CREATE_FLAG_BUS_MODE_VIRTUAL  0x00000000
#define PCIEM_CREATE_FLAG_BUS_MODE_ATTACH   0x00000001

struct pciem_create_device
{
    uint32_t flags;
};

struct pciem_bar_config
{
    uint32_t bar_index;
    uint32_t flags;
    uint64_t size;
    uint32_t reserved;
};

struct pciem_cap_msi_userspace
{
    uint8_t num_vectors_log2;
    uint8_t has_64bit;
    uint8_t has_masking;
    uint8_t reserved;
};

struct pciem_cap_msix_userspace
{
    uint8_t bar_index;
    uint8_t reserved[3];
    uint32_t table_offset;
    uint32_t pba_offset;
    uint16_t table_size;
    uint16_t reserved2;
};

struct pciem_cap_config
{
    uint32_t cap_type;
    union {
        struct pciem_cap_msi_userspace msi;
        struct pciem_cap_msix_userspace msix;
    };
};

struct pciem_config_space
{
    uint16_t vendor_id;
    uint16_t device_id;
    uint16_t subsys_vendor_id;
    uint16_t subsys_device_id;
    uint8_t revision;
    uint8_t class_code[3];
    uint8_t header_type;
    uint8_t reserved[7];
};

struct pciem_event
{
    uint64_t seq;
    uint32_t type;
    uint32_t bar;
    uint64_t offset;
    uint32_t size;
    uint32_t reserved;
    uint64_t data;
    uint64_t timestamp;
};

#define PCIEM_EVENT_MMIO_READ 1
#define PCIEM_EVENT_MMIO_WRITE 2
#define PCIEM_EVENT_CONFIG_READ 3
#define PCIEM_EVENT_CONFIG_WRITE 4
#define PCIEM_EVENT_MSI_ACK 5
#define PCIEM_EVENT_RESET 6

struct pciem_response
{
    uint64_t seq;
    uint64_t data;
    int32_t status;
    uint32_t reserved;
};

struct pciem_irq_inject
{
    uint32_t vector;
    uint32_t reserved;
};

struct pciem_dma_op
{
    uint64_t guest_iova;
    uint64_t user_addr;
    uint32_t length;
    uint32_t pasid;
    uint32_t flags;
    uint32_t reserved;
};

#define PCIEM_DMA_FLAG_READ 0x1
#define PCIEM_DMA_FLAG_WRITE 0x2

struct pciem_dma_atomic
{
    uint64_t guest_iova;
    uint64_t operand;
    uint64_t compare;
    uint32_t op_type;
    uint32_t pasid;
    uint64_t result;
};

#define PCIEM_ATOMIC_FETCH_ADD 1
#define PCIEM_ATOMIC_FETCH_SUB 2
#define PCIEM_ATOMIC_SWAP 3
#define PCIEM_ATOMIC_CAS 4
#define PCIEM_ATOMIC_FETCH_AND 5
#define PCIEM_ATOMIC_FETCH_OR 6
#define PCIEM_ATOMIC_FETCH_XOR 7

struct pciem_p2p_op_user
{
    uint64_t target_phys_addr;
    uint64_t user_addr;
    uint32_t length;
    uint32_t flags;
};

struct pciem_bar_info_query
{
    uint32_t bar_index;
    uint64_t phys_addr;
    uint64_t size;
    uint32_t flags;
};

#define PCIEM_WP_FLAG_BAR_KPROBES  (1 << 0)
#define PCIEM_WP_FLAG_BAR_MANUAL   (1 << 1)

struct pciem_eventfd_config
{
    int32_t eventfd;
    uint32_t reserved;
};

struct pciem_irqfd_config
{
    int32_t eventfd;
    uint32_t vector;
    uint32_t flags;
    uint32_t reserved;
};

#define PCIEM_IRQFD_FLAG_LEVEL    (1 << 0)
#define PCIEM_IRQFD_FLAG_DEASSERT (1 << 1)

struct pciem_dma_indirect
{
    uint64_t prp1;
    uint64_t prp2;
    uint64_t user_addr;
    uint32_t length;
    uint32_t page_size;
    uint32_t pasid;
    uint32_t flags;
    uint32_t reserved;
};

/* Notify userspace on BAR reads */
#define PCIEM_TRACE_READS         (1 << 0)

/* Notify userspace on BAR writes */
#define PCIEM_TRACE_WRITES        (1 << 1)
/*
 * Normally, when PCIem detects a write to a BAR, it emulates that
 * write on its shadow mapping of the BAR, allowing future reads to
 * observe that write. If this flag is set, writes will still be
 * notified (if requested), but PCIem will not write to the BAR.
 * Userspace must update the BAR through its own mapping if it wants
 * the device driver to see updates to the BAR.
 */
#define PCIEM_TRACE_STOP_WRITES   (1 << 2)

/* For PCIEM_IOCTL_TRACE_BAR */
struct pciem_trace_bar
{
    uint32_t bar_index;
    uint32_t flags;
};

#define PCIEM_IOCTL_MAGIC 0xAF

#define PCIEM_IOCTL_CREATE_DEVICE _IOWR(PCIEM_IOCTL_MAGIC, 10, struct pciem_create_device)
#define PCIEM_IOCTL_ADD_BAR _IOW(PCIEM_IOCTL_MAGIC, 11, struct pciem_bar_config)
#define PCIEM_IOCTL_ADD_CAPABILITY _IOW(PCIEM_IOCTL_MAGIC, 12, struct pciem_cap_config)
#define PCIEM_IOCTL_SET_CONFIG _IOW(PCIEM_IOCTL_MAGIC, 13, struct pciem_config_space)
#define PCIEM_IOCTL_REGISTER _IO(PCIEM_IOCTL_MAGIC, 14)
#define PCIEM_IOCTL_INJECT_IRQ _IOW(PCIEM_IOCTL_MAGIC, 15, struct pciem_irq_inject)
#define PCIEM_IOCTL_DMA _IOWR(PCIEM_IOCTL_MAGIC, 16, struct pciem_dma_op)
#define PCIEM_IOCTL_DMA_ATOMIC _IOWR(PCIEM_IOCTL_MAGIC, 17, struct pciem_dma_atomic)
#define PCIEM_IOCTL_P2P _IOWR(PCIEM_IOCTL_MAGIC, 18, struct pciem_p2p_op_user)
#define PCIEM_IOCTL_GET_BAR_INFO _IOWR(PCIEM_IOCTL_MAGIC, 19, struct pciem_bar_info_query)
#define PCIEM_IOCTL_SET_EVENTFD _IOW(PCIEM_IOCTL_MAGIC, 21, struct pciem_eventfd_config)
#define PCIEM_IOCTL_SET_IRQFD _IOW(PCIEM_IOCTL_MAGIC, 22, struct pciem_irqfd_config)
#define PCIEM_IOCTL_DMA_INDIRECT _IOWR(PCIEM_IOCTL_MAGIC, 24, struct pciem_dma_indirect)
#define PCIEM_IOCTL_TRACE_BAR _IOWR(PCIEM_IOCTL_MAGIC, 25, struct pciem_trace_bar)
#define PCIEM_IOCTL_START _IO(PCIEM_IOCTL_MAGIC, 26)

#define PCIEM_RING_SIZE 256
#define PCIEM_MAX_IRQFDS 32

struct pciem_shared_ring
{
    atomic_t head;
    char _pad1[60];
    atomic_t tail;
    char _pad2[60];
    struct pciem_event events[PCIEM_RING_SIZE];
};

#ifdef __KERNEL__

struct pciem_irqfd
{
    struct list_head list;
    struct eventfd_ctx *trigger;
    wait_queue_entry_t wait;
    struct work_struct inject_work;
    struct pciem_userspace_state *us;
    uint32_t vector;
    uint32_t flags;
};

#define PCIEM_UNREGISTERED 0
#define PCIEM_REGISTERING  1
#define PCIEM_REGISTERED   2

struct pciem_irqfds {
    spinlock_t lock;
    struct list_head items;
};

struct pciem_tracer {
    struct pciem_userspace_state *us;
    struct smptrace_ctx ctx;
};

struct pciem_userspace_state
{
    struct pciem_root_complex *rc;

    struct hlist_head pending_requests[256];
    spinlock_t pending_lock;
    uint64_t next_seq;

    atomic_t registered;
    atomic_t event_pending;

    struct pciem_shared_ring *shared_ring;
    spinlock_t shared_ring_lock;

    struct eventfd_ctx *eventfd;
    spinlock_t eventfd_lock;

    struct pciem_irqfds irqfds;

    /* BAR read/write trackers */
    struct pciem_tracer tracers[PCI_STD_NUM_BARS];
};

struct pciem_pending_request
{
    struct hlist_node node;
    uint64_t seq;
    struct completion done;
    uint64_t response_data;
    int response_status;
};

int pciem_userspace_init(void);
void pciem_userspace_cleanup(void);
struct pciem_userspace_state *pciem_userspace_create(void);
void pciem_userspace_destroy(struct pciem_userspace_state *us);
int pciem_userspace_register_device(struct pciem_userspace_state *us);
void pciem_userspace_queue_event(struct pciem_userspace_state *us, struct pciem_event *event);
int pciem_userspace_wait_response(struct pciem_userspace_state *us, uint64_t seq, uint64_t *data_out,
                                  unsigned long timeout_ms);

extern const struct file_operations pciem_device_fops;

#else
#define PCIEM_CAP_MSI 0
#define PCIEM_CAP_MSIX 1
#define PCIEM_CAP_PM 2
#define PCIEM_CAP_PCIE 3
#define PCIEM_CAP_VSEC 4
#define PCIEM_CAP_PASID 5
#endif

#endif /* PCIEM_USERSPACE_H */

```

`include/trace/smptrace.h`:

```h
/* SPDX-License-Identifier: GPL-2.0 */
/*
 *  Copyright (C) 2026  Carlos López <carlos.lopezr4096@gmail.com>
 *  Copyright (C) 2026  Joel Bueno <buenocalvachehjoel@gmail.com>
 */
#ifndef _PCIEM_SMPTRACE_H
#define _PCIEM_SMPTRACE_H
#include <asm/pgtable.h>
#include <linux/kprobes.h>
#include <linux/compiler.h>

union smptrace_data {
	u8 raw[8];
	u8 byte;
	u16 word;
	u32 dword;
	u64 qword;
};

struct smptrace_io {
	u64 offset;
	union smptrace_data data;
	u32 size;
};

struct smptrace_ctx;

typedef void (*smptrace_handler_t)(struct smptrace_ctx *ctx, struct smptrace_io *);

/* User defined hooks */
struct smptrace_notifier {
	smptrace_handler_t read;
	smptrace_handler_t write;
};

/* An un-poisoned PTE */
struct smptrace_pte {
	struct list_head list;
	unsigned long va;
	pteval_t pte;
	unsigned int level;
};

struct smptrace_ctx {
	/* User hooks */
	struct smptrace_notifier notif;
	/* User-defined data for this tracer */
	uint64_t opaque;
	/* PA to be tracked */
	resource_size_t pa;
	/* Size of PA to be tracked */
	unsigned long len;
	/* Whether to emulate writes into the BAR */
	bool stop_writes;

	/*** Do not touch below here ***/

	/* Un-poisoned PTEs */
	struct list_head ptes;

	/* Tracing hooks */
	struct kretprobe ioremap_krp;
	struct kprobe iounmap_kp;
	struct kprobe badarea_kp;

	/* Address and size of the VA the tracee mapped */
	atomic_long_t traced_va;
	unsigned long traced_len;

	/* Address of the shadow memory we maintain. Size is ctx->len */
	void __iomem *shadow_va;

	/* Whether this CPU is handling #PF or not */
	bool __percpu *in_pf;
};


int smptrace_init(struct smptrace_ctx *ctx);
void smptrace_destroy(struct smptrace_ctx *ctx);

#endif

```

`kernel/Makefile`:

```
KDIR ?= /lib/modules/$(shell uname -r)/build

ccflags-y := -I$(src)/../include -I$(src)/framework

pciem-objs := framework/pciem_framework.o \
              framework/pciem_capabilities.o \
              framework/pciem_p2p.o \
              framework/pciem_dma.o \
			  framework/pciem_userspace.o

pciem-objs += trace/smptrace_main.o \
              trace/arch/x86/lib/insn.o \
              trace/arch/x86/lib/insn-eval.o \
              trace/arch/x86/lib/inat.o
ccflags-y += -I$(src)/trace/arch/x86/include/asm

obj-m += pciem.o

ccflags-y += -Wall

all:
	$(MAKE) -C $(KDIR) M=$(PWD) modules

clean:
	$(MAKE) -C $(KDIR) M=$(PWD) clean

.PHONY: all clean

```

`kernel/framework/pciem_capabilities.c`:

```c
// SPDX-License-Identifier: GPL-2.0-only
/*
 * Copyright (C) 2025-2026 Joel Bueno
 *   Author(s): Joel Bueno <buenocalvachehjoel@gmail.com>
 *              Carlos López <carlos.lopezr4096@gmail.com>
 */

#define pr_fmt(fmt) KBUILD_MODNAME ": " fmt

#include "pciem_capabilities.h"
#include "pciem_framework.h"
#include <linux/pci_regs.h>
#include <linux/slab.h>
#include <linux/version.h>
#if LINUX_VERSION_CODE < KERNEL_VERSION(6, 12, 0)
#include <asm/unaligned.h>
#else
#include <linux/unaligned.h>
#endif

static u8 msi_cap_size(struct pciem_cap_msi_config *cfg)
{
    u8 size = 10;

    if (cfg->has_64bit)
    {
        size += 4;
    }

    size += 2;

    if (cfg->has_per_vector_masking)
    {
        size += 8;
    }

    return size;
}

void pciem_init_cap_manager(struct pciem_root_complex *v)
{
    guard(write_lock)(&v->cap_lock);

    if (v->cap_mgr)
        return;

    v->cap_mgr = kzalloc(sizeof(*v->cap_mgr), GFP_KERNEL);
    if (!v->cap_mgr)
    {
        pr_err("Failed to allocate capability manager\n");
        return;
    }
    v->cap_mgr->num_caps = 0;
    v->cap_mgr->next_offset = 0x40;
}

void pciem_cleanup_cap_manager(struct pciem_root_complex *v)
{
    struct pciem_cap_manager *mgr;
    int i;

    guard(write_lock)(&v->cap_lock);

    mgr = v->cap_mgr;
    if (!mgr)
        return;

    for (i = 0; i < v->cap_mgr->num_caps; i++)
    {
        if (mgr->caps[i].type == PCIEM_CAP_VSEC && mgr->caps[i].config.vsec.data)
        {
            kfree(mgr->caps[i].config.vsec.data);
        }
    }

    kfree(mgr);
    v->cap_mgr = NULL;
}

int pciem_add_cap_msi(struct pciem_root_complex *v, struct pciem_cap_msi_config *cfg)
{
    struct pciem_cap_manager *mgr;
    struct pciem_cap_entry *cap;

    guard(write_lock)(&v->cap_lock);

    mgr = v->cap_mgr;
    if (!mgr || mgr->num_caps >= MAX_PCI_CAPS)
        return -ENOMEM;

    cap = &mgr->caps[mgr->num_caps];
    cap->type = PCIEM_CAP_MSI;
    cap->offset = mgr->next_offset;
    cap->size = msi_cap_size(cfg);
    cap->config.msi = *cfg;

    memset(&cap->state.msi_state, 0, sizeof(cap->state.msi_state));
    cap->state.msi_state.control = 0;

    mgr->next_offset += cap->size;
    mgr->num_caps++;

    pr_info("Added MSI capability at offset 0x%02x (size %u)\n", cap->offset, cap->size);

    return 0;
}
EXPORT_SYMBOL(pciem_add_cap_msi);

int pciem_add_cap_msix(struct pciem_root_complex *v, struct pciem_cap_msix_config *cfg)
{
    struct pciem_cap_manager *mgr;
    struct pciem_cap_entry *cap;

    guard(write_lock)(&v->cap_lock);

    mgr = v->cap_mgr;
    if (!mgr || mgr->num_caps >= MAX_PCI_CAPS)
        return -ENOMEM;

    cap = &mgr->caps[mgr->num_caps];
    cap->type = PCIEM_CAP_MSIX;
    cap->offset = mgr->next_offset;
    cap->size = 12;
    cap->config.msix = *cfg;

    cap->state.msix_state.control = 0;

    mgr->next_offset += cap->size;
    mgr->num_caps++;

    pr_info("Added MSI-X capability at offset 0x%02x\n", cap->offset);

    return 0;
}

int pciem_add_cap_pm(struct pciem_root_complex *v, struct pciem_cap_pm_config *cfg)
{
    struct pciem_cap_manager *mgr;
    struct pciem_cap_entry *cap;

    guard(write_lock)(&v->cap_lock);

    mgr = v->cap_mgr;
    if (!mgr || mgr->num_caps >= MAX_PCI_CAPS)
        return -ENOMEM;

    cap = &mgr->caps[mgr->num_caps];
    cap->type = PCIEM_CAP_PM;
    cap->offset = mgr->next_offset;
    cap->size = 8;
    cap->config.pm = *cfg;

    cap->state.pm_state.control = 0;
    cap->state.pm_state.status = 0;

    mgr->next_offset += cap->size;
    mgr->num_caps++;

    pr_info("Added Power Management capability at offset 0x%02x\n", cap->offset);

    return 0;
}

int pciem_add_cap_pcie(struct pciem_root_complex *v, struct pciem_cap_pcie_config *cfg)
{
    struct pciem_cap_manager *mgr;
    struct pciem_cap_entry *cap;

    guard(write_lock)(&v->cap_lock);

    mgr = v->cap_mgr;
    if (!mgr || mgr->num_caps >= MAX_PCI_CAPS)
        return -ENOMEM;

    cap = &mgr->caps[mgr->num_caps];
    cap->type = PCIEM_CAP_PCIE;
    cap->offset = mgr->next_offset;
    cap->size = 60;
    cap->config.pcie = *cfg;

    mgr->next_offset += cap->size;
    mgr->num_caps++;

    pr_info("Added PCIe capability at offset 0x%02x\n", cap->offset);

    return 0;
}

int pciem_add_cap_vsec(struct pciem_root_complex *v, struct pciem_cap_vsec_config *cfg)
{
    struct pciem_cap_manager *mgr;
    struct pciem_cap_entry *cap;
    u8 *data_copy;

    guard(write_lock)(&v->cap_lock);

    mgr = v->cap_mgr;
    if (!mgr || mgr->num_caps >= MAX_PCI_CAPS)
        return -ENOMEM;

    data_copy = kmalloc(cfg->vsec_length, GFP_KERNEL);
    if (!data_copy)
        return -ENOMEM;

    memcpy(data_copy, cfg->data, cfg->vsec_length);

    cap = &mgr->caps[mgr->num_caps];
    cap->type = PCIEM_CAP_VSEC;
    cap->offset = mgr->next_offset;
    cap->size = 8 + cfg->vsec_length;
    cap->config.vsec = *cfg;
    cap->config.vsec.data = data_copy;

    mgr->next_offset += cap->size;
    mgr->num_caps++;

    pr_info("Added VSEC capability at offset 0x%02x (vendor 0x%04x)\n", cap->offset, cfg->vendor_id);

    return 0;
}

int pciem_add_cap_pasid(struct pciem_root_complex *v, struct pciem_cap_pasid_config *cfg)
{
    struct pciem_cap_manager *mgr;
    struct pciem_cap_entry *cap;

    guard(write_lock)(&v->cap_lock);

    mgr = v->cap_mgr;
    if (!mgr || mgr->num_caps >= MAX_PCI_CAPS)
        return -ENOMEM;

    cap = &mgr->caps[mgr->num_caps];
    cap->type = PCIEM_CAP_PASID;
    cap->offset = mgr->next_offset;
    cap->size = 8;
    cap->config.pasid = *cfg;

    cap->state.pasid_state.control = 0;
    cap->state.pasid_state.pasid = 0;

    mgr->next_offset += cap->size;
    mgr->num_caps++;

    pr_info("Added PASID capability at offset 0x%02x\n", cap->offset);

    return 0;
}

void pciem_build_config_space(struct pciem_root_complex *v)
{
    int i;
    struct pciem_cap_manager *mgr = v->cap_mgr;

    if (!mgr || mgr->num_caps == 0)
    {
        v->cfg[PCI_CAPABILITY_LIST] = 0;
        v->cfg[PCI_STATUS] &= ~(PCI_STATUS_CAP_LIST >> 8);
        return;
    }

    v->cfg[PCI_CAPABILITY_LIST] = mgr->caps[0].offset;
    v->cfg[PCI_STATUS] |= (PCI_STATUS_CAP_LIST >> 8);

    for (i = 0; i < mgr->num_caps; i++)
    {
        struct pciem_cap_entry *cap = &mgr->caps[i];
        u8 *cfg = &v->cfg[cap->offset];
        u8 next_ptr = (i + 1 < mgr->num_caps) ? mgr->caps[i + 1].offset : 0;

        switch (cap->type)
        {
        case PCIEM_CAP_MSI: {
            struct pciem_cap_msi_config *msi = &cap->config.msi;
            u16 control = 0;
            u8 pos = 0;

            cfg[pos++] = PCI_CAP_ID_MSI;
            cfg[pos++] = next_ptr;

            if (msi->has_64bit)
            {
                control |= PCI_MSI_FLAGS_64BIT;
            }

            if (msi->has_per_vector_masking)
            {
                control |= PCI_MSI_FLAGS_MASKBIT;
            }

            control |= (msi->num_vectors_log2 << 1);
            put_unaligned_le16(control, &cfg[pos]);
            pos += 2;

            put_unaligned_le32(0, &cfg[pos]);
            pos += 4;

            if (msi->has_64bit)
            {
                put_unaligned_le32(0, &cfg[pos]);
                pos += 4;
            }

            put_unaligned_le16(0, &cfg[pos]);
            pos += 2;

            if (msi->has_per_vector_masking)
            {
                put_unaligned_le32(0, &cfg[pos]);
                pos += 4;
                put_unaligned_le32(0, &cfg[pos]);
            }
            break;
        }

        case PCIEM_CAP_MSIX: {
            struct pciem_cap_msix_config *msix = &cap->config.msix;
            u8 pos = 0;

            cfg[pos++] = PCI_CAP_ID_MSIX;
            cfg[pos++] = next_ptr;

            put_unaligned_le16((msix->table_size - 1) & 0x7FF, &cfg[pos]);
            pos += 2;

            put_unaligned_le32((msix->table_offset & ~0x7) | (msix->bar_index & 0x7), &cfg[pos]);
            pos += 4;

            put_unaligned_le32((msix->pba_offset & ~0x7) | (msix->bar_index & 0x7), &cfg[pos]);
            break;
        }

        case PCIEM_CAP_PM: {
            struct pciem_cap_pm_config *pm = &cap->config.pm;
            u16 pmc = 0;
            u8 pos = 0;

            cfg[pos++] = PCI_CAP_ID_PM;
            cfg[pos++] = next_ptr;

            pmc |= (pm->version & 0x3);
            if (pm->d1_support)
            {
                pmc |= PCI_PM_CAP_D1;
            }
            if (pm->d2_support)
            {
                pmc |= PCI_PM_CAP_D2;
            }
            if (pm->pme_support)
            {
                pmc |= PCI_PM_CAP_PME_D0 | PCI_PM_CAP_PME_D3hot | PCI_PM_CAP_PME_D3cold;
            }
            put_unaligned_le16(pmc, &cfg[pos]);
            pos += 2;

            put_unaligned_le16(0, &cfg[pos]);
            pos += 2;

            cfg[pos++] = 0;
            cfg[pos++] = 0;
            break;
        }

        case PCIEM_CAP_PCIE: {
            struct pciem_cap_pcie_config *pcie = &cap->config.pcie;
            u8 pos = 0;

            cfg[pos++] = PCI_CAP_ID_EXP;
            cfg[pos++] = next_ptr;

            put_unaligned_le16((pcie->device_type << 4) | 2, &cfg[pos]);
            pos += 2;

            put_unaligned_le32(0x00008000, &cfg[pos]);
            pos += 4;

            put_unaligned_le32(0, &cfg[pos]);
            pos += 4;

            put_unaligned_le32((pcie->link_speed & 0xF) | ((pcie->link_width & 0x3F) << 4), &cfg[pos]);
            pos += 4;

            put_unaligned_le32(((pcie->link_speed & 0xF) | ((pcie->link_width & 0x3F) << 4)) << 16, &cfg[pos]);
            pos += 4;

            memset(&cfg[pos], 0, 60 - pos);
            break;
        }

        case PCIEM_CAP_VSEC: {
            struct pciem_cap_vsec_config *vsec = &cap->config.vsec;
            u8 pos = 0;

            cfg[pos++] = PCI_CAP_ID_VNDR;
            cfg[pos++] = next_ptr;

            cfg[pos++] = (8 + vsec->vsec_length) & 0xFF;

            cfg[pos++] = 0;

            put_unaligned_le16(vsec->vendor_id, &cfg[pos]);
            pos += 2;

            cfg[pos++] = vsec->vsec_id & 0xFF;
            cfg[pos++] = ((vsec->vsec_id >> 8) & 0xF) | ((vsec->vsec_rev & 0xF) << 4);

            memcpy(&cfg[pos], vsec->data, vsec->vsec_length);
            break;
        }

        case PCIEM_CAP_PASID: {
            struct pciem_cap_pasid_config *pasid = &cap->config.pasid;
            u16 caps = 0;
            u8 pos = 0;

            cfg[pos++] = 0x1B;
            cfg[pos++] = next_ptr;

            if (pasid->execute_permission)
            {
                caps |= 0x02;
            }
            if (pasid->privileged_mode)
            {
                caps |= 0x04;
            }
            caps |= ((pasid->max_pasid_width - 1) << 8);
            put_unaligned_le16(caps, &cfg[pos]);
            pos += 2;

            put_unaligned_le16(0, &cfg[pos]);
            pos += 2;

            put_unaligned_le16(0, &cfg[pos]);
            break;
        }
        }
    }
}

static bool handle_msi_read(struct pciem_cap_entry *cap, u32 offset, u32 size, u32 *value)
{
    struct pciem_msi_state *st = &cap->state.msi_state;

    if (offset == PCI_MSI_FLAGS && size == 2)
    {
        *value = st->control;
        return true;
    }
    if (offset == PCI_MSI_ADDRESS_LO)
    {
        *value = st->address_lo;
        return true;
    }

    if (cap->config.msi.has_64bit)
    {
        if (offset == PCI_MSI_ADDRESS_HI)
        {
            *value = st->address_hi;
            return true;
        }
        else if (offset == PCI_MSI_DATA_64)
        {
            *value = st->data;
            return true;
        }
    }
    else
    {
        if (offset == PCI_MSI_DATA_32)
        {
            *value = st->data;
            return true;
        }
    }

    return false;
}

static bool handle_msix_read(struct pciem_cap_entry *cap, u32 offset, u32 size, u32 *value)
{
    struct pciem_msix_state *st = &cap->state.msix_state;

    if (offset == PCI_MSIX_FLAGS && size == 2)
    {
        *value = st->control;
        return true;
    }
    return false;
}

static bool handle_pm_read(struct pciem_cap_entry *cap, u32 offset, u32 size, u32 *value)
{
    struct pciem_pm_state *st = &cap->state.pm_state;

    if (offset == PCI_PM_CTRL && size == 2)
    {
        *value = st->control;
        return true;
    }
    return false;
}

static bool handle_pasid_read(struct pciem_cap_entry *cap, u32 offset, u32 size, u32 *value)
{
    struct pciem_pasid_state *st = &cap->state.pasid_state;

    if (offset == PCI_PASID_CTRL && size == 2)
    {
        *value = st->control;
        return true;
    }

    return false;
}

bool pciem_handle_cap_read(struct pciem_root_complex *v, int where, int size, u32 *value)
{
    struct pciem_cap_manager *mgr = v->cap_mgr;
    int i;

    guard(read_lock)(&v->cap_lock);

    if (!mgr)
        return false;

    for (i = 0; i < mgr->num_caps; i++)
    {
        struct pciem_cap_entry *cap = &mgr->caps[i];

        if (where >= cap->offset && where < (cap->offset + cap->size))
        {
            int cap_offset = where - cap->offset;

            switch (cap->type)
            {
            case PCIEM_CAP_MSI:
                return handle_msi_read(cap, cap_offset, size, value);
            case PCIEM_CAP_MSIX:
                return handle_msix_read(cap, cap_offset, size, value);
            case PCIEM_CAP_PM:
                return handle_pm_read(cap, cap_offset, size, value);
            case PCIEM_CAP_PASID:
                return handle_pasid_read(cap, cap_offset, size, value);
            default:
                break;
            }

            return false;
        }
    }

    return false;
}

static bool handle_msi_write(struct pciem_cap_entry *cap, u32 offset, u32 size, u32 value)
{
    struct pciem_msi_state *st = &cap->state.msi_state;

    if (offset == PCI_MSI_FLAGS && size == 2) {
        st->control = value & 0xffff;
        pr_info("MSI Control written: 0x%04x (Enable: %d)\n", value, !!(value & PCI_MSI_FLAGS_ENABLE));
        return true;
    }
    if (offset == PCI_MSI_ADDRESS_LO && size == 4)
    {
        st->address_lo = value;
        pr_info("MSI Address Lo written: 0x%08x\n", value);
        return true;
    }
    if (cap->config.msi.has_64bit)
    {
        if (offset == PCI_MSI_ADDRESS_HI && size == 4)
        {
            st->address_hi = value;
            pr_info("MSI Address Hi written: 0x%08x\n", value);
            return true;
        }
        else if (offset == PCI_MSI_DATA_64 && size == 2)
        {
            st->data = value & 0xFFFF;
            pr_info("MSI Data written: 0x%04x\n", value);
            return true;
        }
        else if (offset == PCI_MSI_MASK_64 && size == 4)
        {
            st->mask_bits = value;
            pr_info("MSI Mask bits written: 0x%08x\n", value);
            return true;
        }
    }
    else
    {
        if (offset == PCI_MSI_DATA_32 && size == 2)
        {
            st->data = value & 0xFFFF;
            pr_info("MSI Data written: 0x%04x\n", value);
            return true;
        }
        else if (offset == PCI_MSI_MASK_32 && size == 4)
        {
            st->mask_bits = value;
            pr_info("MSI Mask bits written: 0x%08x\n", value);
            return true;
        }
    }
    return false;
}

static bool handle_msix_write(struct pciem_cap_entry *cap, u32 offset, u32 size, u32 value)
{
    struct pciem_msix_state *st = &cap->state.msix_state;

    if (offset == PCI_MSIX_FLAGS && size == 2)
    {
        st->control = value & 0xC7FF;
        pr_info("MSI-X Control written: 0x%04x (Enable: %d)\n", value, !!(value & PCI_MSIX_FLAGS_ENABLE));
        return true;
    }

    return false;
}

static bool handle_pm_write(struct pciem_cap_entry *cap, u32 offset, u32 size, u32 value)
{
    struct pciem_pm_state *st = &cap->state.pm_state;

    if (offset == PCI_PM_CTRL && size == 2)
    {
        st->control = value & (PCI_PM_CTRL_STATE_MASK | PCI_PM_CTRL_PME_ENABLE | PCI_PM_CTRL_PME_STATUS);
        pr_info("PM Control written: 0x%04x (Power State: D%d)\n", value, value & 0x3);
        return true;
    }

    return false;
}

static bool handle_pasid_write(struct pciem_cap_entry *cap, u32 offset, u32 size, u32 value)
{
    struct pciem_pasid_state *st = &cap->state.pasid_state;

    if (offset == PCI_PASID_CTRL && size == 2)
    {
        st->control = value & (PCI_PASID_CTRL_ENABLE | PCI_PASID_CTRL_EXEC | PCI_PASID_CTRL_PRIV);
        if (value & PCI_PASID_CTRL_ENABLE)
        {
            pr_info("PASID Enabled\n");
        }
        return true;
    }

    return false;
}

bool pciem_handle_cap_write(struct pciem_root_complex *v, int where, int size, u32 value)
{
    struct pciem_cap_manager *mgr = v->cap_mgr;
    int i;

    /* Take a read lock since we are not updating anything in the cap. manager itself,
     * only the actual capabilities. */
    guard(read_lock)(&v->cap_lock);

    if (!mgr)
        return false;

    for (i = 0; i < mgr->num_caps; i++)
    {
        struct pciem_cap_entry *cap = &mgr->caps[i];

        if (where >= cap->offset && where < (cap->offset + cap->size))
        {
            u32 cap_offset = where - cap->offset;

            switch (cap->type)
            {
            case PCIEM_CAP_MSI:
                return handle_msi_write(cap, cap_offset, size, value);
            case PCIEM_CAP_MSIX:
                return handle_msix_write(cap, cap_offset, size, value);
            case PCIEM_CAP_PM:
                return handle_pm_write(cap, cap_offset, size, value);
            case PCIEM_CAP_PASID:
                return handle_pasid_write(cap, cap_offset, size, value);
            default:
                break;
            }

            return true;
        }
    }

    return false;
}

```

`kernel/framework/pciem_dma.c`:

```c
// SPDX-License-Identifier: GPL-2.0-only
/*
 * Copyright (C) 2025-2026 Joel Bueno
 *   Author(s): Joel Bueno <buenocalvachehjoel@gmail.com>
 *              Carlos López <carlos.lopezr4096@gmail.com>
 */

#define pr_fmt(fmt) KBUILD_MODNAME ": " fmt

#include "pciem_dma.h"
#include "pciem_framework.h"
#include <asm/cacheflush.h>
#include <linux/atomic.h>
#include <linux/iommu.h>
#include <linux/mm.h>
#include <linux/slab.h>

static int translate_iova(struct pciem_root_complex *v, dma_addr_t guest_iova,
                          size_t len, phys_addr_t **phys_pages_out,
                          unsigned int *num_pages)
{
    struct iommu_domain *domain = iommu_get_domain_for_dev(&v->pciem_pdev->dev);
    dma_addr_t iova, iova_start, iova_end;
    size_t max_pages, page_count = 0;
    phys_addr_t *pages = NULL;
    int ret;

    iova_start = PAGE_ALIGN_DOWN(guest_iova);
    iova_end = PAGE_ALIGN(guest_iova + len);
    max_pages = (iova_end - iova_start) >> PAGE_SHIFT;

    pr_info("DMA: translate: 0x%llx (0x%llx - 0x%llx) (%lu pages)",
            guest_iova, iova_start, iova_end, max_pages);

    pages = kmalloc_array(max_pages, sizeof(phys_addr_t), GFP_KERNEL);
    if (!pages) {
        ret = -ENOMEM;
        goto fail;
    }

    for (iova = iova_start; iova < iova_end; iova += PAGE_SIZE) {
        phys_addr_t hpa;

        if (domain) {
            hpa = iommu_iova_to_phys(domain, iova);
            if (!hpa) {
                ret = -EFAULT;
                goto fail;
            }
        } else {
            hpa = iova;
        }

        pages[page_count++] = hpa;
    }

    *num_pages = page_count;
    *phys_pages_out = pages;

    return 0;

fail:
    pr_err("failed to translate IOVA=%llx (%d)", guest_iova, ret);
    if (pages)
        kfree(pages);
    return ret;
}

int pciem_dma_read_from_guest(struct pciem_root_complex *v, u64 guest_iova,
                              void *dst, size_t len, u32 pasid)
{
    phys_addr_t *pages = NULL;
    unsigned int i, num_pages;
    size_t dst_offset = 0;
    int ret;

    if (!v || !dst || !len)
        return -EINVAL;

    ret = translate_iova(v, guest_iova, len, &pages, &num_pages);
    if (ret)
        return ret;

    pr_info("DMA: read:  src=0x%llx dst=0x%lx len=0x%lx (%u pages) PASID %u\n",
            guest_iova, (size_t)dst, len, num_pages, pasid);

    for (i = 0; i < num_pages; ++i) {
        void *src;
        size_t src_offset = (i == 0) ? offset_in_page(guest_iova) : 0;
        size_t chunk_len = min_t(size_t, PAGE_SIZE - src_offset, len - dst_offset);

        src = memremap(pages[i], PAGE_SIZE, MEMREMAP_WB);
        if (!src) {
            kfree(pages);
            return -ENOMEM;
        }

        pr_info("DMA: read%u: src=0x%lx dst=0x%lx len=0x%lx (pa=%llx)",
                i, (size_t)src + src_offset, (size_t)dst + dst_offset,
                chunk_len, pages[i] + src_offset);

        memcpy(dst + dst_offset, src + src_offset, chunk_len);
        memunmap(src);

        dst_offset += chunk_len;
    }

    kfree(pages);
    return 0;
}
EXPORT_SYMBOL(pciem_dma_read_from_guest);

int pciem_dma_write_to_guest(struct pciem_root_complex *v, u64 guest_iova,
                             const void *src, size_t len, u32 pasid)
{
    phys_addr_t *pages;
    unsigned int i, num_pages;
    size_t src_offset = 0;
    int ret;

    if (!v || !src || !len)
        return -EINVAL;

    ret = translate_iova(v, guest_iova, len, &pages, &num_pages);
    if (ret)
        return ret;

    pr_info("DMA: write:  src=0x%lx dst=0x%llx len=0x%lx (%u pages) PASID %u\n",
            (size_t)src, guest_iova, len, num_pages, pasid);

    for (i = 0; i < num_pages; ++i) {
        void *dst;
        unsigned int dst_offset = (i == 0) ? offset_in_page(guest_iova) : 0;
        size_t chunk_len = min_t(size_t, PAGE_SIZE - dst_offset, len - src_offset);

        dst = memremap(pages[i], PAGE_SIZE, MEMREMAP_WB);
        if (!dst) {
            kfree(pages);
            return -ENOMEM;
        }

        pr_info("DMA: write%u: src=0x%lx dst=0x%lx len=0x%lx (pa=%llx)",
                i, (size_t)src + src_offset, (size_t)dst + dst_offset,
                chunk_len, pages[i] + dst_offset);

        memcpy(dst + dst_offset, src + src_offset, chunk_len);
        memunmap(dst);
        src_offset += chunk_len;
    }

    kfree(pages);
    return 0;
}
EXPORT_SYMBOL(pciem_dma_write_to_guest);

static u64 do_atomic_op(struct pciem_root_complex *v, u64 guest_iova, u8 op_type, u64 operand, u64 compare, u32 pasid)
{
    phys_addr_t phys_addr;
    void *kva;
    u64 old_val = 0;
    phys_addr_t *phys_pages = NULL;
    int num_pages;
    atomic64_t *atomic_ptr;
    int ret;

    if (guest_iova & 0x7)
    {
        pr_err("Atomic operation on unaligned address 0x%llx\n", guest_iova);
        return 0;
    }

    ret = translate_iova(v, guest_iova, 8, &phys_pages, &num_pages);
    if (ret < 0)
    {
        pr_err("Failed to translate IOVA for atomic op\n");
        return 0;
    }

    phys_addr = phys_pages[0];
    kfree(phys_pages);

    kva = memremap(phys_addr, 8, MEMREMAP_WB);
    if (!kva)
    {
        pr_err("Failed to map page for atomic op\n");
        return 0;
    }

    if (!IS_ALIGNED((unsigned long)kva, 8))
    {
        pr_err("Mapped address not 8-byte aligned: %px\n", kva);
        memunmap(kva);
        return 0;
    }

    atomic_ptr = (atomic64_t *)kva;

    switch (op_type)
    {
    case PCIEM_ATOMIC_FETCH_ADD:
        old_val = atomic64_fetch_add(operand, atomic_ptr);
        pr_info("Atomic FETCH_ADD: IOVA 0x%llx, old=0x%llx, add=0x%llx, PASID %u\n", guest_iova, old_val, operand,
                pasid);
        break;

    case PCIEM_ATOMIC_FETCH_SUB:
        old_val = atomic64_fetch_sub(operand, atomic_ptr);
        pr_info("Atomic FETCH_SUB: IOVA 0x%llx, old=0x%llx, sub=0x%llx, PASID %u\n", guest_iova, old_val, operand,
                pasid);
        break;

    case PCIEM_ATOMIC_SWAP:
        old_val = atomic64_xchg(atomic_ptr, operand);
        pr_info("Atomic SWAP: IOVA 0x%llx, old=0x%llx, new=0x%llx, PASID %u\n", guest_iova, old_val, operand, pasid);
        break;

    case PCIEM_ATOMIC_CAS:
        old_val = atomic64_cmpxchg(atomic_ptr, compare, operand);
        pr_info("Atomic CAS: IOVA 0x%llx, old=0x%llx, expected=0x%llx, new=0x%llx, PASID %u\n", guest_iova, old_val,
                compare, operand, pasid);
        break;

    case PCIEM_ATOMIC_FETCH_AND:
        old_val = atomic64_fetch_and(operand, atomic_ptr);
        pr_info("Atomic FETCH_AND: IOVA 0x%llx, old=0x%llx, mask=0x%llx, PASID %u\n", guest_iova, old_val, operand,
                pasid);
        break;

    case PCIEM_ATOMIC_FETCH_OR:
        old_val = atomic64_fetch_or(operand, atomic_ptr);
        pr_info("Atomic FETCH_OR: IOVA 0x%llx, old=0x%llx, bits=0x%llx, PASID %u\n", guest_iova, old_val, operand,
                pasid);
        break;

    case PCIEM_ATOMIC_FETCH_XOR:
        old_val = atomic64_fetch_xor(operand, atomic_ptr);
        pr_info("Atomic FETCH_XOR: IOVA 0x%llx, old=0x%llx, bits=0x%llx, PASID %u\n", guest_iova, old_val, operand,
                pasid);
        break;

    default:
        pr_err("Unknown atomic operation type %u\n", op_type);
        break;
    }

    memunmap(kva);

    return old_val;
}

u64 pciem_dma_atomic_fetch_add(struct pciem_root_complex *v, u64 guest_iova, u64 val, u32 pasid)
{
    return do_atomic_op(v, guest_iova, PCIEM_ATOMIC_FETCH_ADD, val, 0, pasid);
}
EXPORT_SYMBOL(pciem_dma_atomic_fetch_add);

u64 pciem_dma_atomic_fetch_sub(struct pciem_root_complex *v, u64 guest_iova, u64 val, u32 pasid)
{
    return do_atomic_op(v, guest_iova, PCIEM_ATOMIC_FETCH_SUB, val, 0, pasid);
}
EXPORT_SYMBOL(pciem_dma_atomic_fetch_sub);

u64 pciem_dma_atomic_swap(struct pciem_root_complex *v, u64 guest_iova, u64 val, u32 pasid)
{
    return do_atomic_op(v, guest_iova, PCIEM_ATOMIC_SWAP, val, 0, pasid);
}
EXPORT_SYMBOL(pciem_dma_atomic_swap);

u64 pciem_dma_atomic_cas(struct pciem_root_complex *v, u64 guest_iova, u64 expected, u64 new_val, u32 pasid)
{
    return do_atomic_op(v, guest_iova, PCIEM_ATOMIC_CAS, new_val, expected, pasid);
}
EXPORT_SYMBOL(pciem_dma_atomic_cas);

u64 pciem_dma_atomic_fetch_and(struct pciem_root_complex *v, u64 guest_iova, u64 val, u32 pasid)
{
    return do_atomic_op(v, guest_iova, PCIEM_ATOMIC_FETCH_AND, val, 0, pasid);
}
EXPORT_SYMBOL(pciem_dma_atomic_fetch_and);

u64 pciem_dma_atomic_fetch_or(struct pciem_root_complex *v, u64 guest_iova, u64 val, u32 pasid)
{
    return do_atomic_op(v, guest_iova, PCIEM_ATOMIC_FETCH_OR, val, 0, pasid);
}
EXPORT_SYMBOL(pciem_dma_atomic_fetch_or);

u64 pciem_dma_atomic_fetch_xor(struct pciem_root_complex *v, u64 guest_iova, u64 val, u32 pasid)
{
    return do_atomic_op(v, guest_iova, PCIEM_ATOMIC_FETCH_XOR, val, 0, pasid);
}
EXPORT_SYMBOL(pciem_dma_atomic_fetch_xor);

```

`kernel/framework/pciem_framework.c`:

```c
// SPDX-License-Identifier: GPL-2.0-only
/*
 * Copyright (C) 2025-2026 Joel Bueno
 *   Author(s): Joel Bueno <buenocalvachehjoel@gmail.com>
 *              Carlos López <carlos.lopezr4096@gmail.com>
 */

#define pr_fmt(fmt) KBUILD_MODNAME ": " fmt

#include <linux/pci.h>
#include <linux/pci_ids.h>

#include <asm/cacheflush.h>
#include <asm/io.h>
#include <asm/tlbflush.h>
#include <linux/atomic.h>
#include <linux/cleanup.h>
#include <linux/delay.h>
#include <linux/init.h>
#include <linux/interrupt.h>
#include <linux/io.h>
#include <linux/iommu.h>
#include <linux/ioport.h>
#include <linux/kernel.h>
#include <linux/kthread.h>
#include <linux/list.h>
#include <linux/module.h>
#include <linux/pci-acpi.h>
#include <linux/pci_regs.h>
#include <linux/resource.h>
#include <linux/sched.h>
#include <linux/sizes.h>
#include <linux/slab.h>
#include <linux/spinlock.h>
#include <linux/uaccess.h>
#include <linux/vmalloc.h>
#include <linux/idr.h>
#include <linux/workqueue.h>

#include "pciem_capabilities.h"
#include "pciem_dma.h"
#include "pciem_framework.h"
#include "pciem_p2p.h"
#include "pciem_userspace.h"

static char *pciem_phys_region = "";
module_param(pciem_phys_region, charp, 0444);
MODULE_PARM_DESC(pciem_phys_region,
    "Physical memory pool for BAR allocations: 0xADDR:0xSIZE");

static char *p2p_regions = "";
module_param(p2p_regions, charp, 0444);
MODULE_PARM_DESC(p2p_regions,
    "P2P whitelist: 0xADDR:0xSIZE,0xADDR:0xSIZE");

static struct miscdevice pciem_dev;
static const struct file_operations pciem_fops;
static struct pci_ops vph_pci_ops;

static void pciem_fixup_bridge_domain(struct pci_host_bridge *bridge, 
                                      struct pciem_host_bridge_priv *priv, 
                                      int domain)
{
    bridge->domain_nr = domain;

#ifdef CONFIG_X86
    priv->sd.domain = domain;
    priv->sd.node = NUMA_NO_NODE;
    bridge->sysdata = &priv->sd;
#else
    bridge->sysdata = priv;
#endif
}

struct pciem_mempool pciem_pool;
EXPORT_SYMBOL(pciem_pool);

phys_addr_t pciem_pool_alloc(resource_size_t size)
{
    phys_addr_t addr;
    resource_size_t aligned_offset;
    unsigned long flags;

    if (!pciem_pool.initialized) {
        pr_err("pool: No physical memory pool configured.\n");
        pr_err("pool: Pass pciem_phys_region=0xADDR:0xSIZE at insmod.\n");
        return 0;
    }

    if (!size || (size & (size - 1))) {
        pr_err("pool: Allocation size 0x%llx is not a power of 2\n", (u64)size);
        return 0;
    }

    spin_lock_irqsave(&pciem_pool.lock, flags);

    aligned_offset = ALIGN(pciem_pool.next_offset, size);

    if (aligned_offset + size > pciem_pool.total_size) {
        spin_unlock_irqrestore(&pciem_pool.lock, flags);
        pr_err("pool: Out of pool memory.\n");
        return 0;
    }

    addr = pciem_pool.base + aligned_offset;
    pciem_pool.next_offset = aligned_offset + size;

    spin_unlock_irqrestore(&pciem_pool.lock, flags);

    pr_info("pool: Allocated 0x%llx bytes at phys 0x%llx (pool offset 0x%llx)\n",
            (u64)size, (u64)addr, (u64)aligned_offset);
    return addr;
}
EXPORT_SYMBOL(pciem_pool_alloc);

static int pciem_pool_init(void)
{
    phys_addr_t base;
    resource_size_t size;
    struct resource *res;

    memset(&pciem_pool, 0, sizeof(pciem_pool));
    spin_lock_init(&pciem_pool.lock);

    if (!pciem_phys_region || !*pciem_phys_region) {
        pr_info("pool: No pciem_phys_region specified\n");
        return 0;
    }

    if (sscanf(pciem_phys_region, "0x%llx:0x%llx",
               (unsigned long long *)&base,
               (unsigned long long *)&size) != 2 &&
        sscanf(pciem_phys_region, "%llx:%llx",
               (unsigned long long *)&base,
               (unsigned long long *)&size) != 2) {
        pr_err("pool: Cannot parse pciem_phys_region=\"%s\"\n", pciem_phys_region);
        return -EINVAL;
    }

    if (!size || (size & (size - 1))) {
        pr_err("pool: Region size 0x%llx must be a power of 2\n", (u64)size);
        return -EINVAL;
    }

    res = kzalloc(sizeof(*res), GFP_KERNEL);
    if (!res)
        return -ENOMEM;

    res->name = "PCIem BAR pool";
    res->start = base;
    res->end = base + size - 1;
    res->flags = IORESOURCE_MEM;

    if (insert_resource(&iomem_resource, res)) {
        pr_err("pool: Failed to claim [0x%llx-0x%llx] in iomem\n",
               (u64)base, (u64)(base + size - 1));
        kfree(res);
        return -EBUSY;
    }

    pciem_pool.base = base;
    pciem_pool.total_size = size;
    pciem_pool.next_offset = 0;
    pciem_pool.res = res;
    pciem_pool.initialized = true;

    pr_info("pool: BAR pool ready [0x%llx – 0x%llx]\n",
            (u64)base, (u64)(base + size - 1));
    return 0;
}

static void pciem_pool_exit(void)
{
    if (!pciem_pool.initialized)
        return;

    if (pciem_pool.res) {
        release_resource(pciem_pool.res);
        kfree(pciem_pool.res);
        pciem_pool.res = NULL;
    }

    pciem_pool.initialized = false;
    pr_info("pool: BAR pool released\n");
}

int pciem_register_bar(struct pciem_root_complex *v, u32 bar_num, resource_size_t size, u32 flags)
{
    phys_addr_t phys;

    if (bar_num >= PCI_STD_NUM_BARS)
        return -EINVAL;

    guard(write_lock)(&v->bars_lock);

    if (size == 0)
    {
        v->bars[bar_num].size = 0;
        v->bars[bar_num].flags = 0;
        return 0;
    }

    if (size & (size - 1))
    {
        pr_err("pciem: BAR %u size 0x%llx is not a power of 2\n", bar_num, (u64)size);
        return -EINVAL;
    }

    phys = pciem_pool_alloc(size);
    if (!phys)
        return -ENOMEM;

    v->bars[bar_num].size = size;
    v->bars[bar_num].flags = flags;
    v->bars[bar_num].base_addr_val = 0;
    v->bars[bar_num].carved_start = phys;
    v->bars[bar_num].carved_end = phys + size - 1;

    pr_info("pciem: Registered BAR %u: size 0x%llx, flags 0x%x\n", bar_num, (u64)size, flags);

    return 0;
}
EXPORT_SYMBOL(pciem_register_bar);

void pciem_trigger_msi(struct pciem_root_complex *v, int vector)
{
    struct pci_dev *dev = v->pciem_pdev;
    int irq;

    if (!dev || (!dev->msi_enabled && !dev->msix_enabled))
    {
        pr_warn("Cannot trigger MSI/MSI-X: device not ready or interrupts not enabled (msi=%d, msix=%d)\n",
                dev ? dev->msi_enabled : 0, dev ? dev->msix_enabled : 0);
        return;
    }

    if (dev->msix_enabled) {
        irq = pci_irq_vector(dev, vector);
        if (irq < 0) {
            pr_warn("Cannot get IRQ for MSI-X vector %d: %d\n", vector, irq);
            return;
        }
        pr_info("Triggering MSI-X vector %d (IRQ %u) via irq_work\n", vector, irq);
    }
    else {
        irq = dev->irq;
        if (irq == 0) {
            pr_warn("Cannot trigger MSI: dev->irq is 0\n");
            return;
        }
        pr_info("Triggering MSI (IRQ %u) via irq_work\n", irq);
    }

    v->pending_msi_irq = irq;
    irq_work_queue(&v->msi_irq_work);
}
EXPORT_SYMBOL(pciem_trigger_msi);

static void pciem_msi_irq_work_func(struct irq_work *work)
{
    struct pciem_root_complex *v = container_of(work, struct pciem_root_complex, msi_irq_work);
    unsigned int irq = v->pending_msi_irq;
    if (irq)
    {
        generic_handle_irq(irq);
    }
}

static void pciem_bus_init_resources(struct pciem_root_complex *v)
{
    u32 i;
    struct pciem_bar_info *bar;
    struct pci_dev *dev __free(pci_dev_put) = pci_get_slot(v->root_bus, 0);

    if (!dev)
        return;

    for (i = 0; i < PCI_STD_NUM_BARS; i++)
    {
        bar = &v->bars[i];
        if (bar->size > 0 && bar->allocated_res)
        {
            dev->resource[i].start = bar->phys_addr;
            dev->resource[i].end = bar->phys_addr + bar->size - 1;
            dev->resource[i].flags |= IORESOURCE_BUSY | IORESOURCE_PCI_FIXED;
            bar->res = &dev->resource[i];

            bar->base_addr_val = (u32)(bar->phys_addr & 0xFFFFFFFF);
            if ((bar->flags & PCI_BASE_ADDRESS_MEM_TYPE_64) &&
                (i + 1 < PCI_STD_NUM_BARS)) {
                v->bars[i + 1].base_addr_val = (u32)(bar->phys_addr >> 32);
            }
        }
    }

    pci_bus_assign_resources(v->root_bus);
    pr_info("init: found pci_dev vendor=%04x device=%04x", dev->vendor, dev->device);
}

static int pciem_reserve_bar_res(struct pciem_bar_info *bar, u32 i, struct list_head *resources)
{
    struct resource_entry *entry;

    if (!bar->allocated_res)
        return 0;

    entry = resource_list_create_entry(bar->allocated_res, i);
    if (!entry)
        return -ENOMEM;

    resource_list_add_tail(entry, resources);
    pr_info("init: Added BAR%u to resource list", i);
    return 0;
}

static int pciem_reserve_bars_res(struct pciem_root_complex *v, struct list_head *resources)
{
    int i, rc;
    struct pciem_bar_info *bar, *prev = NULL;

    for (i = 0; i < PCI_STD_NUM_BARS; i++)
    {
        bar = &v->bars[i];
        if (i > 0)
            prev = &v->bars[i - 1];

        if (!bar->size)
            continue;

        if (i & 1 && prev && prev->flags & PCI_BASE_ADDRESS_MEM_TYPE_64)
            continue;

        rc = pciem_reserve_bar_res(bar, i, resources);
        if (rc)
            return rc;
    }

    return 0;
}

static void pciem_cleanup_bar(struct pciem_bar_info *bar)
{
    if (bar->allocated_res && bar->mem_owned_by_framework)
    {
        if (bar->allocated_res->parent) {
            release_resource(bar->allocated_res);
        }
        kfree(bar->allocated_res->name);
        kfree(bar->allocated_res);
        bar->allocated_res = NULL;
    }
    if (bar->pages) {
        __free_pages(bar->pages, bar->order);
        bar->pages = NULL;
    }
}

static void pciem_cleanup_bars(struct pciem_root_complex *v)
{
    for (int i = 0; i < PCI_STD_NUM_BARS; i++)
        pciem_cleanup_bar(&v->bars[i]);
}

static u32 pciem_read_bar_address(const struct pciem_root_complex *v, u32 idx)
{
    u32 val;
    const struct pciem_bar_info *bar = &v->bars[idx];
    const struct pciem_bar_info *prev = idx > 0 && (idx % 2) == 1
        ? &v->bars[idx - 1]
        : NULL;

    if (bar->size != 0)
    {
        u32 probe_val = (u32)(~(bar->size - 1));

        if (bar->base_addr_val == probe_val)
            val = probe_val | (bar->flags & ~PCI_BASE_ADDRESS_MEM_MASK);
        else
            val = bar->base_addr_val | (bar->flags & ~PCI_BASE_ADDRESS_MEM_MASK);

        return val;
    }

    if (prev && (prev->flags & PCI_BASE_ADDRESS_MEM_TYPE_64))
    {
        u32 probe_val_high = 0xffffffff;

        if (prev->size >= (1ULL << 32))
            probe_val_high = (u32)(~(prev->size - 1) >> 32);

        if (bar->base_addr_val == probe_val_high)
            val = probe_val_high;
        else
            val = bar->base_addr_val;

        return val;
    }

    return 0;
}

static int pciem_conf_read_impl(struct pciem_root_complex *v, int where, int size, u32 *value)
{
    u32 val = ~0U;

    if (!v)
    {
        *value = ~0U;
        return PCIBIOS_DEVICE_NOT_FOUND;
    }
    if (unlikely(v->detaching)) {
        *value = ~0U;
        return PCIBIOS_DEVICE_NOT_FOUND;
    }
    if (where < 0 || (where + size) > (int)sizeof(v->cfg))
    {
        *value = ~0U;
        return PCIBIOS_DEVICE_NOT_FOUND;
    }
    if (pciem_handle_cap_read(v, where, size, &val))
    {
        *value = val;
        return PCIBIOS_SUCCESSFUL;
    }

    if (where >= PCI_BASE_ADDRESS_0 &&
        where < PCI_BASE_ADDRESS_0 + (4 * PCI_STD_NUM_BARS) &&
        (where % 4 == 0) &&
        size == 4)
    {
        int idx = (where - PCI_BASE_ADDRESS_0) / 4;
        *value = pciem_read_bar_address(v, idx);
        return PCIBIOS_SUCCESSFUL;
    }

    if (where == PCI_ROM_ADDRESS && size == 4)
    {
        val = 0;
    }
    else
    {
        switch (size)
        {
        case 1:
            val = v->cfg[where];
            break;
        case 2:
            val = *(u16 *)&v->cfg[where];
            break;
        case 4:
            val = *(u32 *)&v->cfg[where];
            break;
        default:
            val = ~0U;
        }
    }
    *value = val;
    return PCIBIOS_SUCCESSFUL;
}

static int pciem_write_bar_address(struct pciem_root_complex *v, u32 idx, u32 value)
{
    struct pciem_bar_info *bar = &v->bars[idx];
    struct pciem_bar_info *prev = idx > 0 && (idx % 2) == 1
        ? &v->bars[idx - 1]
        : NULL;

    if (bar->size != 0)
    {
        u32 mask = (u32)(~(bar->size - 1));
        if (bar->flags & PCI_BASE_ADDRESS_SPACE_IO)
            mask &= ~PCI_BASE_ADDRESS_IO_MASK;
        else
            mask &= ~PCI_BASE_ADDRESS_MEM_MASK;

        bar->base_addr_val = value & mask;
        return PCIBIOS_SUCCESSFUL;
    }

    if (prev && (prev->flags & PCI_BASE_ADDRESS_MEM_TYPE_64))
    {
        u32 mask_high = 0xffffffff;

        if (prev->size < (1ULL << 32))
            mask_high = 0;
        else
            mask_high = (u32)(~(prev->size - 1) >> 32);

        bar->base_addr_val = value & mask_high;
        return PCIBIOS_SUCCESSFUL;
    }

    return PCIBIOS_FUNC_NOT_SUPPORTED;
}

static int pciem_conf_write_impl(struct pciem_root_complex *v, int where, int size, u32 value)
{
    if (!v)
    {
        return PCIBIOS_DEVICE_NOT_FOUND;
    }
    if (unlikely(v->detaching)) {
        return PCIBIOS_SUCCESSFUL;
    }
    if (where < 0 || (where + size) > (int)sizeof(v->cfg))
    {
        return PCIBIOS_DEVICE_NOT_FOUND;
    }

    if (pciem_handle_cap_write(v, where, size, value))
    {
        return PCIBIOS_SUCCESSFUL;
    }

    if (where >= PCI_BASE_ADDRESS_0 &&
        where < PCI_BASE_ADDRESS_0 + (4 * PCI_STD_NUM_BARS) &&
        (where % 4 == 0) &&
        size == 4)
    {
        int idx = (where - PCI_BASE_ADDRESS_0) / 4;
        return pciem_write_bar_address(v, idx, value);
    }

    if (where == PCI_ROM_ADDRESS)
        return PCIBIOS_SUCCESSFUL;

    switch (size)
    {
    case 1:
        v->cfg[where] = (u8)value;
        break;
    case 2:
        *(u16 *)&v->cfg[where] = (u16)value;
        break;
    case 4:
        *(u32 *)&v->cfg[where] = (u32)value;
        break;
    default:
        return PCIBIOS_FUNC_NOT_SUPPORTED;
    }
    return PCIBIOS_SUCCESSFUL;
}

static int vph_read_config(struct pci_bus *bus, unsigned int devfn, int where, int size, u32 *value)
{
    struct pciem_root_complex *v;

#ifdef CONFIG_X86
    struct pci_sysdata *sd = bus->sysdata;
    struct pciem_host_bridge_priv *priv = container_of(sd, struct pciem_host_bridge_priv, sd);
    v = priv->v;
#else
    struct pciem_host_bridge_priv *priv = bus->sysdata;
    v = priv->v;
#endif

    if (devfn != 0) {
        *value = ~0U;
        return PCIBIOS_DEVICE_NOT_FOUND;
    }

    return pciem_conf_read_impl(v, where, size, value);
}

static int vph_write_config(struct pci_bus *bus, unsigned int devfn, int where, int size, u32 value)
{
    struct pciem_root_complex *v;

#ifdef CONFIG_X86
    struct pci_sysdata *sd = bus->sysdata;
    struct pciem_host_bridge_priv *priv = container_of(sd, struct pciem_host_bridge_priv, sd);
    v = priv->v;
#else
    struct pciem_host_bridge_priv *priv = bus->sysdata;
    v = priv->v;
#endif

    if (devfn != 0) return PCIBIOS_DEVICE_NOT_FOUND;
    return pciem_conf_write_impl(v, where, size, value);
}

static struct pci_ops vph_pci_ops = {
    .read = vph_read_config,
    .write = vph_write_config,
};

static int proxy_read_config(struct pci_bus *bus, unsigned int devfn, int where, int size, u32 *value)
{
    if (!bus || !bus->ops)
        return PCIBIOS_DEVICE_NOT_FOUND;

    struct pciem_root_complex *v = container_of(bus->ops, struct pciem_root_complex,
                                                 mode_state.hijack.proxy_ops);

    if (unlikely(bus->ops != &v->mode_state.hijack.proxy_ops)) {
        pr_warn_once("proxy_read_config called with unexpected ops pointer\n");
        return v->mode_state.hijack.original_ops->read(bus, devfn, where, size, value);
    }

    if (bus->number == v->mode_state.hijack.target_bus->number &&
        PCI_SLOT(devfn) == v->mode_state.hijack.hijacked_slot) {

        /* FIXME: Function 0 for now */
        if (PCI_FUNC(devfn) > 0) {
            *value = ~0U;
            return PCIBIOS_DEVICE_NOT_FOUND;
        }
        return pciem_conf_read_impl(v, where, size, value);
    }

    return v->mode_state.hijack.original_ops->read(bus, devfn, where, size, value);
}

static int proxy_write_config(struct pci_bus *bus, unsigned int devfn, int where, int size, u32 value)
{
    if (!bus || !bus->ops)
        return PCIBIOS_DEVICE_NOT_FOUND;

    struct pciem_root_complex *v = container_of(bus->ops, struct pciem_root_complex,
                                                 mode_state.hijack.proxy_ops);

    if (unlikely(bus->ops != &v->mode_state.hijack.proxy_ops)) {
        pr_warn_once("proxy_write_config called with unexpected ops pointer\n");
        return v->mode_state.hijack.original_ops->write(bus, devfn, where, size, value);
    }

    if (bus->number == v->mode_state.hijack.target_bus->number &&
        PCI_SLOT(devfn) == v->mode_state.hijack.hijacked_slot) {

        if (PCI_FUNC(devfn) > 0) return PCIBIOS_DEVICE_NOT_FOUND;
        return pciem_conf_write_impl(v, where, size, value);
    }

    return v->mode_state.hijack.original_ops->write(bus, devfn, where, size, value);
}

static struct pci_bus *pciem_find_suitable_root_bus(void)
{
    struct pci_bus *bus = NULL;
    struct pci_dev *pdev = NULL;

    bus = pci_find_bus(0, 0);
    if (bus) return bus;

    while ((pdev = pci_get_device(PCI_ANY_ID, PCI_ANY_ID, pdev)) != NULL) {
        if (pdev->bus && !pdev->bus->parent) {
            bus = pdev->bus;
            pci_dev_put(pdev);
            return bus;
        }
    }
    return NULL;
}

static int pciem_find_free_slot(struct pci_bus *bus)
{
    int slot;
    u32 vendor, class;

    if (!bus) return -ENODEV;

    for (slot = 1; slot < 32; slot++) {
        if (pci_bus_read_config_dword(bus, PCI_DEVFN(slot, 0), PCI_VENDOR_ID, &vendor))
            continue;

        if (vendor == 0xffffffff || vendor == 0x00000000) {
            if (pci_bus_read_config_dword(bus, PCI_DEVFN(slot, 0), PCI_CLASS_REVISION, &class) == 0) {
                class >>= 8;

                if ((class >> 8) == PCI_BASE_CLASS_BRIDGE)
                    continue;
            }

            pci_bus_read_config_dword(bus, PCI_DEVFN(slot, 1), PCI_VENDOR_ID, &vendor);

            if (vendor == 0xffffffff || vendor == 0x00000000)
                return slot;
        }
    }
    return -ENOSPC;
}

static void pciem_activation_work_func(struct work_struct *work)
{
    struct pciem_root_complex *v = 
        container_of(work, struct pciem_root_complex, activation_work);

    pr_info("activate: adding device to subsystem now\n");

    if (v->bus_mode == PCIEM_BUS_MODE_VIRTUAL_ROOT) {
        if (v->root_bus)
            pci_bus_add_devices(v->root_bus);
    } else if (v->bus_mode == PCIEM_BUS_MODE_ATTACH_TO_HOST) {
        if (v->pciem_pdev)
            pci_bus_add_device(v->pciem_pdev);
    }
    
    v->activated = true;
}

static int pciem_init_virtual_root_mode(struct pciem_root_complex *v, struct list_head *resources)
{
    int rc, busnr = 1, domain = 0;
    struct pci_host_bridge *bridge;
    struct pciem_host_bridge_priv *priv;

    while (pci_find_bus(domain, busnr)) {
        busnr++;
        if (busnr > 255) {
            pr_err("init: No free bus number available\n");
            return -EBUSY;
        }
    }

    bridge = pci_alloc_host_bridge(sizeof(*priv));
    if (!bridge)
        return -ENOMEM;

    priv = pci_host_bridge_priv(bridge);
    priv->v = v;

    pciem_fixup_bridge_domain(bridge, priv, domain);

    bridge->dev.parent = &v->shared_bridge_pdev->dev;
    bridge->busnr = busnr;
    bridge->ops = &vph_pci_ops;
    list_splice_init(resources, &bridge->windows);

    rc = pci_scan_root_bus_bridge(bridge);
    if (rc < 0) {
        pr_err("init: pci_scan_root_bus_bridge failed: %d\n", rc);
        pci_free_host_bridge(bridge);
        return -ENODEV;
    }

    v->root_bus = bridge->bus;
    if (!v->root_bus) {
        pr_err("init: Failed to create root bus\n");
        return -ENODEV;
    }

    pciem_bus_init_resources(v);
    pci_bus_assign_resources(v->root_bus);

    v->pciem_pdev = pci_get_domain_bus_and_slot(domain, v->root_bus->number, PCI_DEVFN(0, 0));
    if (!v->pciem_pdev) {
        pr_err("init: Failed to find emulated device\n");
        return -ENODEV;
    }

    v->mode_state.virtual_root.bridge = bridge;
    v->mode_state.virtual_root.assigned_domain = domain;
    v->mode_state.virtual_root.assigned_busnr = busnr;

    pr_info("init: Virtual root mode - domain %d, bus %d\n", domain, busnr);
    return 0;
}

static int pciem_init_attach_to_host_mode(struct pciem_root_complex *v)
{
    struct pci_bus *target_bus;
    struct pci_dev *dev;
    int slot, i;

    target_bus = pciem_find_suitable_root_bus();
    if (!target_bus) {
        pr_err("init: No suitable root bus found (paravirt environment?)\n");
        pr_err("init: Try using PCIEM_CREATE_FLAG_BUS_MODE_VIRTUAL instead\n");
        return -ENODEV;
    }

    pr_info("init: Targeting bus %04x:%02x for device injection\n",
            pci_domain_nr(target_bus), target_bus->number);

    slot = pciem_find_free_slot(target_bus);
    if (slot < 0) {
        pr_err("init: No free slots on target bus\n");
        return -ENOSPC;
    }

    pr_info("init: Injecting device at slot %02x on bus %04x:%02x\n",
            slot, pci_domain_nr(target_bus), target_bus->number);

    v->mode_state.hijack.target_bus = target_bus;
    v->mode_state.hijack.hijacked_slot = slot;
    v->mode_state.hijack.original_ops = target_bus->ops;

    v->mode_state.hijack.proxy_ops = *target_bus->ops;
    v->mode_state.hijack.proxy_ops.read = proxy_read_config;
    v->mode_state.hijack.proxy_ops.write = proxy_write_config;

    for (i = 0; i < PCI_STD_NUM_BARS; i++) {
        struct pciem_bar_info *bar = &v->bars[i];
        if (bar->size == 0) continue;
        if (i > 0 && (i % 2 == 1) && (v->bars[i - 1].flags & PCI_BASE_ADDRESS_MEM_TYPE_64))
            continue;

        bar->base_addr_val = (u32)(bar->phys_addr & 0xFFFFFFFF);
        if (bar->flags & PCI_BASE_ADDRESS_MEM_TYPE_64 && i+1 < PCI_STD_NUM_BARS) {
            v->bars[i+1].base_addr_val = (u32)(bar->phys_addr >> 32);
        }
    }

    /* FIXME: How usual would be for config space changes after system is booted? */
    pci_lock_rescan_remove();
    WRITE_ONCE(target_bus->ops, &v->mode_state.hijack.proxy_ops);
    /* FIXME: Are memory barriers needed here? */
    smp_mb();
    dev = pci_scan_single_device(target_bus, PCI_DEVFN(slot, 0));
    pci_unlock_rescan_remove();

    if (!dev) {
        pr_err("init: Scan failed to create device\n");
        WRITE_ONCE(target_bus->ops, v->mode_state.hijack.original_ops);
        return -ENODEV;
    }

    pr_info("init: Setting up device resources\n");
    for (i = 0; i < PCI_STD_NUM_BARS; i++) {
        struct pciem_bar_info *bar = &v->bars[i];

        if (bar->size == 0)
            continue;

        if (i > 0 && (i % 2 == 1) &&
            (v->bars[i - 1].flags & PCI_BASE_ADDRESS_MEM_TYPE_64))
            continue;

        if (bar->allocated_res) {
            dev->resource[i] = *bar->allocated_res;
            dev->resource[i].name = pci_name(dev);
            dev->resource[i].flags |= IORESOURCE_PCI_FIXED | IORESOURCE_BUSY;
            bar->res = &dev->resource[i];

            pr_info("init: BAR%d assigned: %pR\n", i, &dev->resource[i]);
        }
    }


    v->pciem_pdev = dev;
    v->root_bus = target_bus;

    pr_info("init: Attach-to-host mode active: %s\n", pci_name(dev));
    return 0;
}

int pciem_complete_init(struct pciem_root_complex *v)
{
    int rc = 0;
    struct resource *mem_res = NULL;
    LIST_HEAD(resources);
    u32 i;

    struct platform_device_info pdevinfo = {
        .name = "pciem",
        .id = PLATFORM_DEVID_AUTO,
        .res = NULL,
        .num_res = 0,
    };
    
    v->shared_bridge_pdev = platform_device_register_full(&pdevinfo);

    if (IS_ERR(v->shared_bridge_pdev))
    {
        rc = PTR_ERR(v->shared_bridge_pdev);
        goto fail_pdev_null;
    }

    rc = pciem_p2p_init(v, p2p_regions);
    if (rc) {
        pr_warn("pciem: P2P init failed: %d (non-fatal)\n", rc);
    }

    for (i = 0; i < PCI_STD_NUM_BARS; i++)
    {
        struct pciem_bar_info *bar = &v->bars[i];
        struct pciem_bar_info *prev = i > 0 && (i % 2) == 1
            ? &v->bars[i - 1]
            : NULL;

        if (bar->size == 0)
            continue;

        if (prev && (prev->flags & PCI_BASE_ADDRESS_MEM_TYPE_64))
            continue;

        bar->order = get_order(bar->size);
        pr_info("init: preparing BAR%u physical memory (%llu KB, order %u)", i, (u64)bar->size / 1024, bar->order);

        if (!bar->carved_start) {
            pr_err("init: BAR%u has no physical address assigned.\n", i);
            rc = -ENOMEM;
            goto fail_bars;
        }

        mem_res = kzalloc(sizeof(*mem_res), GFP_KERNEL);
        if (!mem_res) {
            rc = -ENOMEM;
            goto fail_bars;
        }

        mem_res->name = kasprintf(GFP_KERNEL, "PCI BAR%u", i);
        if (!mem_res->name) {
            kfree(mem_res);
            rc = -ENOMEM;
            goto fail_bars;
        }

        mem_res->start = bar->carved_start;
        mem_res->end = bar->carved_end;
        mem_res->flags = IORESOURCE_MEM;

        if (request_resource(pciem_pool.res, mem_res)) {
            kfree(mem_res->name);
            kfree(mem_res);
            rc = -EBUSY;
            goto fail_bars;
        }

        bar->allocated_res = mem_res;
        bar->mem_owned_by_framework = true;
        bar->phys_addr = bar->carved_start;
        bar->pages = NULL;
        bar->base_addr_val = (u32)(bar->phys_addr & 0xFFFFFFFF);
        if ((bar->flags & PCI_BASE_ADDRESS_MEM_TYPE_64) &&
            (i + 1 < PCI_STD_NUM_BARS)) {
            v->bars[i + 1].base_addr_val = (u32)(bar->phys_addr >> 32);
        }
    }

    rc = pciem_reserve_bars_res(v, &resources);
    if (rc)
        goto fail_res_list;

    switch (v->bus_mode) {
    case PCIEM_BUS_MODE_VIRTUAL_ROOT:
        pr_info("init: Initializing in VIRTUAL_ROOT mode\n");
        rc = pciem_init_virtual_root_mode(v, &resources);
        break;

    case PCIEM_BUS_MODE_ATTACH_TO_HOST:
        pr_info("init: Initializing in ATTACH_TO_HOST mode\n");
        rc = pciem_init_attach_to_host_mode(v);
        resource_list_free(&resources);
        break;

    default:
        pr_err("init: Unknown bus mode %d\n", v->bus_mode);
        rc = -EINVAL;
        goto fail_res_list;
    }

    if (rc)
        goto fail_device;

    pr_info("init: Device ready (mode: %s)\n",
            v->bus_mode == PCIEM_BUS_MODE_VIRTUAL_ROOT ? "virtual-root" : "attach-to-host");

    return 0;

fail_device:
    if (v->pciem_pdev) {
        if (v->bus_mode == PCIEM_BUS_MODE_ATTACH_TO_HOST) {
            pci_stop_and_remove_bus_device(v->pciem_pdev);
        } else {
            pci_dev_put(v->pciem_pdev);
        }
        v->pciem_pdev = NULL;
    }
    if (v->bus_mode == PCIEM_BUS_MODE_VIRTUAL_ROOT && v->root_bus) {
        pci_remove_root_bus(v->root_bus);
        v->root_bus = NULL;
    } else if (v->bus_mode == PCIEM_BUS_MODE_ATTACH_TO_HOST) {
        if (v->mode_state.hijack.target_bus && v->mode_state.hijack.original_ops) {
            pci_lock_rescan_remove();
            WRITE_ONCE(v->mode_state.hijack.target_bus->ops,
                      v->mode_state.hijack.original_ops);
            smp_mb();
            pci_unlock_rescan_remove();
        }
    }
fail_res_list:
    resource_list_free(&resources);
fail_bars:
    pciem_cleanup_bars(v);
    platform_device_unregister(v->shared_bridge_pdev);
fail_pdev_null:
    v->shared_bridge_pdev = NULL;
    return rc;
}
EXPORT_SYMBOL(pciem_complete_init);

int pciem_start_device(struct pciem_root_complex *v)
{
    if (v->activated)
        return -EALREADY;

    schedule_work(&v->activation_work);
    return 0;
}
EXPORT_SYMBOL(pciem_start_device);

static void pciem_teardown_device(struct pciem_root_complex *v)
{
    pr_info("exit: tearing down pciem device\n");

    cancel_work_sync(&v->activation_work);

    irq_work_sync(&v->msi_irq_work);

    if (v->pciem_pdev) {
        struct device_driver *drv = v->pciem_pdev->dev.driver;

        if (drv) {
            pr_info("exit: Device %s bound to driver '%s' - initiating removal\n",
                    pci_name(v->pciem_pdev), drv->name);
            v->detaching = true;
        }

        pci_stop_and_remove_bus_device(v->pciem_pdev);
        v->pciem_pdev = NULL;
    }

    if (v->root_bus)
    {
        if (v->bus_mode == PCIEM_BUS_MODE_VIRTUAL_ROOT) {
            pci_remove_root_bus(v->root_bus);
        } else if (v->bus_mode == PCIEM_BUS_MODE_ATTACH_TO_HOST) {
            if (v->mode_state.hijack.original_ops) {
                pci_lock_rescan_remove();
                WRITE_ONCE(v->mode_state.hijack.target_bus->ops,
                        v->mode_state.hijack.original_ops);
                smp_mb();
                pci_unlock_rescan_remove();
                synchronize_rcu();
                pr_info("exit: Restored original bus ops\n");
            }
        }
        v->root_bus = NULL;
    }

    pciem_cleanup_bars(v);

    if (v->shared_bridge_pdev)
    {
        platform_device_unregister(v->shared_bridge_pdev);
        v->shared_bridge_pdev = NULL;
    }

    pciem_cleanup_cap_manager(v);
    pciem_p2p_cleanup(v);
}

static int __init pciem_init(void)
{
    int ret;
    pr_info("init: pciem framework loading\n");

    ret = pciem_pool_init();
    if (ret) return ret;

    ret = pciem_userspace_init();
    if (ret) {
        pr_err("init: Failed to initialize userspace support: %d\n", ret);
        goto fail_userspace;
    }

    pciem_dev.minor = MISC_DYNAMIC_MINOR;
    pciem_dev.name = "pciem";
    pciem_dev.fops = &pciem_fops;
    pciem_dev.mode = 0666;

    ret = misc_register(&pciem_dev);
    if (ret) {
        pr_err("init: Failed to register main device: %d\n", ret);
        goto fail_misc;
    }

    pr_info("init: Created /dev/pciem for userspace device creation\n");
    pr_info("init: pciem framework loaded\n");
    return 0;

fail_misc:
    pciem_userspace_cleanup();
fail_userspace:
    pciem_pool_exit();
    return ret;
}

static void __exit pciem_exit(void)
{
    pr_info("exit: unloading pciem framework\n");

    misc_deregister(&pciem_dev);
    pciem_userspace_cleanup();
    pciem_pool_exit();
    pr_info("exit: Unregistered /dev/pciem\n");
    pr_info("exit: pciem framework done");
}

struct pciem_root_complex *pciem_alloc_root_complex(void)
{
    struct pciem_root_complex *v;

    v = kzalloc(sizeof(*v), GFP_KERNEL);
    if (!v)
        return ERR_PTR(-ENOMEM);

    rwlock_init(&v->bars_lock);
    rwlock_init(&v->cap_lock);

    /* Essential initialization that must happen */
    init_irq_work(&v->msi_irq_work, pciem_msi_irq_work_func);
    INIT_WORK(&v->activation_work, pciem_activation_work_func);
    v->pending_msi_irq = 0;
    memset(v->bars, 0, sizeof(v->bars));

    pr_info("Allocated pciem root complex\n");
    return v;
}
EXPORT_SYMBOL(pciem_alloc_root_complex);

void pciem_free_root_complex(struct pciem_root_complex *v)
{
    if (!v)
        return;

    pr_info("Freeing pciem root complex\n");
    pciem_teardown_device(v);
    kfree(v);
}
EXPORT_SYMBOL(pciem_free_root_complex);

static int pciem_open(struct inode *inode, struct file *file)
{
    struct pciem_userspace_state *us;

    us = pciem_userspace_create();
    if (IS_ERR(us))
        return PTR_ERR(us);

    file->private_data = us;
    return 0;
}

static long pciem_ioctl(struct file *file, unsigned int cmd, unsigned long arg)
{
    return pciem_device_fops.unlocked_ioctl(file, cmd, arg);
}

static int pciem_release(struct inode *inode, struct file *file)
{
    if (pciem_device_fops.release)
        return pciem_device_fops.release(inode, file);
    return 0;
}

static ssize_t pciem_read(struct file *file, char __user *buf, size_t count, loff_t *ppos)
{
    if (pciem_device_fops.read)
        return pciem_device_fops.read(file, buf, count, ppos);
    return -EINVAL;
}

static ssize_t pciem_write(struct file *file, const char __user *buf, size_t count, loff_t *ppos)
{
    if (pciem_device_fops.write)
        return pciem_device_fops.write(file, buf, count, ppos);
    return -EINVAL;
}

static __poll_t pciem_poll(struct file *file, struct poll_table_struct *wait)
{
    if (pciem_device_fops.poll)
        return pciem_device_fops.poll(file, wait);
    return 0;
}

static int pciem_mmap(struct file *file, struct vm_area_struct *vma)
{
    if (pciem_device_fops.mmap)
        return pciem_device_fops.mmap(file, vma);
    return -EINVAL;
}

static const struct file_operations pciem_fops = {
    .owner = THIS_MODULE,
    .open = pciem_open,
    .release = pciem_release,
    .read = pciem_read,
    .write = pciem_write,
    .poll = pciem_poll,
    .unlocked_ioctl = pciem_ioctl,
    .compat_ioctl = pciem_ioctl,
    .mmap = pciem_mmap,
};

module_init(pciem_init);
module_exit(pciem_exit);

MODULE_LICENSE("GPL v2");
MODULE_AUTHOR("cakehonolulu (cakehonolulu@protonmail.com)");
MODULE_DESCRIPTION("Synthetic PCIe device framework");

```

`kernel/framework/pciem_p2p.c`:

```c
// SPDX-License-Identifier: GPL-2.0-only
/*
 * Copyright (C) 2025-2026 Joel Bueno
 *   Author(s): Joel Bueno <buenocalvachehjoel@gmail.com>
 *              Carlos López <carlos.lopezr4096@gmail.com>
 */

#define pr_fmt(fmt) "pciem_p2p: " fmt

#include "pciem_p2p.h"
#include "pciem_framework.h"
#include <linux/io.h>
#include <linux/slab.h>
#include <linux/string.h>

static int parse_p2p_regions(struct pciem_p2p_manager *mgr,
                              const char *regions_str)
{
    char *str __free(kfree) = NULL, *token, *cur;
    phys_addr_t phys;
    resource_size_t size;
    int count = 0;

    if (!regions_str || strlen(regions_str) == 0) {
        return 0;
    }

    str = kstrdup(regions_str, GFP_KERNEL);
    if (!str) {
        return -ENOMEM;
    }

    cur = str;
    while ((token = strsep(&cur, ",")) != NULL) {
        struct pciem_p2p_region *region;

        if (sscanf(token, "0x%llx:0x%llx", &phys, &size) != 2 &&
            sscanf(token, "%llx:%llx", &phys, &size) != 2) {
            pr_warn("Invalid P2P region format: '%s'\n", token);
            continue;
        }

        if (size == 0 || size > (1ULL << 40)) {
            pr_warn("Invalid P2P region size: 0x%llx\n", size);
            continue;
        }

        phys_addr_t region_end = phys + size;
        struct pciem_p2p_region *existing;
        bool overlap = false;

        list_for_each_entry(existing, &mgr->regions, list) {
            phys_addr_t existing_end = existing->phys_start + existing->size;

            if ((phys < existing_end && region_end > existing->phys_start)) {
                pr_err("P2P region 0x%llx-0x%llx overlaps with existing 0x%llx-0x%llx\n",
                       phys, region_end, existing->phys_start, existing_end);
                overlap = true;
                break;
            }
        }

        if (overlap) {
            continue;
        }

        region = kzalloc(sizeof(*region), GFP_KERNEL);
        if (!region) {
            pr_err("Failed to allocate P2P region struct\n");
            continue;
        }

        region->phys_start = phys;
        region->size = size;

        region->kaddr = ioremap_wc(phys, size);
        if (!region->kaddr) {
            pr_err("Failed to ioremap P2P region 0x%llx (size 0x%llx)\n",
                   phys, size);
            kfree(region);
            continue;
        }

        snprintf(region->name, sizeof(region->name), "p2p_%d", count);

        list_add_tail(&region->list, &mgr->regions);
        count++;

        pr_info("Registered P2P region: 0x%llx-0x%llx (size %llu KB)\n",
                phys, phys + size, size / 1024);
    }

    if (count > 0) {
        mgr->enabled = true;
        pr_info("P2P enabled with %d regions\n", count);
    }

    return 0;
}

int pciem_p2p_init(struct pciem_root_complex *v, const char *regions_str)
{
    struct pciem_p2p_manager *mgr;
    int ret;

    if (!v) {
        return -EINVAL;
    }

    mgr = kzalloc(sizeof(*mgr), GFP_KERNEL);
    if (!mgr) {
        return -ENOMEM;
    }

    INIT_LIST_HEAD(&mgr->regions);
    mutex_init(&mgr->lock);
    mgr->max_transfer_size = PCIEM_P2P_MAX_TRANSFER;
    mgr->enabled = false;

    ret = parse_p2p_regions(mgr, regions_str);
    if (ret < 0) {
        pr_err("Failed to parse P2P regions: %d\n", ret);
        mutex_destroy(&mgr->lock);
        kfree(mgr);
        return ret;
    }

    v->p2p_mgr = mgr;
    return 0;
}
EXPORT_SYMBOL(pciem_p2p_init);

void pciem_p2p_cleanup(struct pciem_root_complex *v)
{
    struct pciem_p2p_manager *mgr;
    struct pciem_p2p_region *region, *tmp;

    if (!v || !v->p2p_mgr) {
        return;
    }

    mgr = v->p2p_mgr;

    mutex_lock(&mgr->lock);

    list_for_each_entry_safe(region, tmp, &mgr->regions, list) {
        pr_info("Unregistering P2P region: 0x%llx (size 0x%llx)\n",
                region->phys_start, region->size);

        if (region->kaddr) {
            iounmap(region->kaddr);
        }

        list_del(&region->list);
        kfree(region);
    }

    mutex_unlock(&mgr->lock);

    kfree(mgr);
    v->p2p_mgr = NULL;
}
EXPORT_SYMBOL(pciem_p2p_cleanup);

int pciem_p2p_register_region(struct pciem_root_complex *v,
                               phys_addr_t phys,
                               resource_size_t size,
                               const char *name)
{
    struct pciem_p2p_manager *mgr;
    struct pciem_p2p_region *region;

    if (!v || !v->p2p_mgr) {
        return -EINVAL;
    }

    if (size == 0 || size > PCIEM_P2P_MAX_TRANSFER) {
        return -EINVAL;
    }

    mgr = v->p2p_mgr;

    region = kzalloc(sizeof(*region), GFP_KERNEL);
    if (!region) {
        return -ENOMEM;
    }

    region->phys_start = phys;
    region->size = size;
    region->kaddr = ioremap_wc(phys, size);

    if (!region->kaddr) {
        kfree(region);
        return -ENOMEM;
    }

    if (name) {
        strncpy(region->name, name, sizeof(region->name) - 1);
    } else {
        snprintf(region->name, sizeof(region->name), "dynamic_0x%llx", phys);
    }

    guard(mutex)(&mgr->lock);
    list_add_tail(&region->list, &mgr->regions);
    mgr->enabled = true;

    pr_info("Dynamically registered P2P region: %s at 0x%llx (size 0x%llx)\n",
            region->name, phys, size);

    return 0;
}
EXPORT_SYMBOL(pciem_p2p_register_region);

int pciem_p2p_unregister_region(struct pciem_root_complex *v,
                                 phys_addr_t phys)
{
    struct pciem_p2p_manager *mgr;
    struct pciem_p2p_region *region, *tmp;

    if (!v || !v->p2p_mgr) {
        return -EINVAL;
    }

    mgr = v->p2p_mgr;

    guard(mutex)(&mgr->lock);

    list_for_each_entry_safe(region, tmp, &mgr->regions, list) {
        if (region->phys_start == phys) {
            pr_info("Unregistering P2P region: %s\n", region->name);

            if (region->kaddr) {
                iounmap(region->kaddr);
            }

            list_del(&region->list);
            kfree(region);
            return 0;
        }
    }

    return -ENOENT;
}
EXPORT_SYMBOL(pciem_p2p_unregister_region);

struct pciem_p2p_region *pciem_p2p_get_region(struct pciem_root_complex *v,
                                               phys_addr_t phys_addr)
{
    struct pciem_p2p_manager *mgr;
    struct pciem_p2p_region *region;

    if (!v || !v->p2p_mgr) {
        return NULL;
    }

    mgr = v->p2p_mgr;

    list_for_each_entry(region, &mgr->regions, list) {
        phys_addr_t region_end = region->phys_start + region->size;

        if (phys_addr >= region->phys_start && phys_addr < region_end) {
            return region;
        }
    }

    return NULL;
}
EXPORT_SYMBOL(pciem_p2p_get_region);

int pciem_p2p_validate_access(struct pciem_root_complex *v,
                               phys_addr_t phys_addr,
                               size_t len)
{
    struct pciem_p2p_manager *mgr;
    struct pciem_p2p_region *region;
    phys_addr_t access_end;
    int ret = -EACCES;

    if (!v || !v->p2p_mgr) {
        return -EINVAL;
    }

    mgr = v->p2p_mgr;

    if (!mgr->enabled) {
        return -EACCES;
    }

    if (len == 0 || len > mgr->max_transfer_size) {
        return -EINVAL;
    }

    access_end = phys_addr + len;

    if (access_end < phys_addr) {
        return -EINVAL;
    }

    guard(mutex)(&mgr->lock);

    list_for_each_entry(region, &mgr->regions, list) {
        phys_addr_t region_end = region->phys_start + region->size;

        if (phys_addr >= region->phys_start && access_end <= region_end) {
            ret = 0;
            break;
        }
    }

    if (ret != 0) {
        pr_warn_ratelimited("P2P access denied: 0x%llx+0x%zx not whitelisted\n",
                           phys_addr, len);
    }

    return ret;
}
EXPORT_SYMBOL(pciem_p2p_validate_access);

int pciem_p2p_read(struct pciem_root_complex *v,
                   phys_addr_t phys_addr,
                   void *dst,
                   size_t len)
{
    struct pciem_p2p_manager *mgr;
    struct pciem_p2p_region *region;
    size_t offset;
    int ret;

    if (!v || !v->p2p_mgr || !dst) {
        return -EINVAL;
    }

    ret = pciem_p2p_validate_access(v, phys_addr, len);
    if (ret < 0) {
        return ret;
    }

    mgr = v->p2p_mgr;

    guard(mutex)(&mgr->lock);

    region = pciem_p2p_get_region(v, phys_addr);
    if (!region)
        return -EFAULT;

    offset = phys_addr - region->phys_start;

    memcpy_fromio(dst, region->kaddr + offset, len);

    pr_debug("P2P read: 0x%llx+0x%zx from region '%s'\n",
             phys_addr, len, region->name);

    return 0;
}
EXPORT_SYMBOL(pciem_p2p_read);

int pciem_p2p_write(struct pciem_root_complex *v,
                    phys_addr_t phys_addr,
                    const void *src,
                    size_t len)
{
    struct pciem_p2p_manager *mgr;
    struct pciem_p2p_region *region;
    size_t offset;
    int ret;

    if (!v || !v->p2p_mgr || !src) {
        return -EINVAL;
    }

    ret = pciem_p2p_validate_access(v, phys_addr, len);
    if (ret < 0) {
        return ret;
    }

    mgr = v->p2p_mgr;

    guard(mutex)(&mgr->lock);

    region = pciem_p2p_get_region(v, phys_addr);
    if (!region)
        return -EFAULT;

    offset = phys_addr - region->phys_start;

    memcpy_toio(region->kaddr + offset, src, len);

    pr_debug("P2P write: 0x%llx+0x%zx to region '%s'\n",
             phys_addr, len, region->name);

    return 0;
}
EXPORT_SYMBOL(pciem_p2p_write);

```

`kernel/framework/pciem_userspace.c`:

```c
// SPDX-License-Identifier: GPL-2.0-only
/*
 * Copyright (C) 2025-2026 Joel Bueno
 *   Author(s): Joel Bueno <buenocalvachehjoel@gmail.com>
 *              Carlos López <carlos.lopezr4096@gmail.com>
 */

#define pr_fmt(fmt) "pciem_userspace: " fmt

#include <linux/anon_inodes.h>
#include <linux/capability.h>
#include <linux/eventfd.h>
#include <linux/file.h>
#include <linux/fs.h>
#include <linux/hw_breakpoint.h>
#include <linux/mm.h>
#include <linux/module.h>
#include <linux/pci_regs.h>
#include <linux/perf_event.h>
#include <linux/poll.h>
#include <linux/slab.h>
#include <linux/uaccess.h>
#include <linux/version.h>
#include <linux/kthread.h>
#include <linux/delay.h>

#include "pciem_capabilities.h"
#include "pciem_dma.h"
#include "pciem_framework.h"
#include "pciem_p2p.h"
#include "pciem_userspace.h"

#if LINUX_VERSION_CODE < KERNEL_VERSION(6,12,0)

#define IS_ERR_PCPU(ptr) (IS_ERR((const void *)(__force const unsigned long)(ptr)))
#define PTR_ERR_PCPU(ptr) (PTR_ERR((const void *)(__force const unsigned long)(ptr)))

#define EMPTY_FD (struct fd){0}

static struct file *fd_file(struct fd fd)
{
    return fd.file;
}

static bool fd_empty(struct fd fd)
{
    return unlikely(!fd.file);
}

#endif

static int pciem_device_release(struct inode *inode, struct file *file);
static ssize_t pciem_device_write(struct file *file, const char __user *buf, size_t count, loff_t *ppos);
static long pciem_device_ioctl(struct file *file, unsigned int cmd, unsigned long arg);
static int pciem_device_mmap(struct file *file, struct vm_area_struct *vma);

const struct file_operations pciem_device_fops = {
    .owner = THIS_MODULE,
    .release = pciem_device_release,
    .write = pciem_device_write,
    .unlocked_ioctl = pciem_device_ioctl,
    .compat_ioctl = pciem_device_ioctl,
    .mmap = pciem_device_mmap,
};
EXPORT_SYMBOL(pciem_device_fops);

static int pciem_instance_mmap(struct file *file, struct vm_area_struct *vma)
{
    struct pciem_userspace_state *us = file->private_data;
    struct pciem_bar_info *bar;
    unsigned long size = vma->vm_end - vma->vm_start;

    unsigned long bar_index = vma->vm_pgoff;

    if (!us || !us->rc)
        return -ENODEV;

    if (bar_index >= PCI_STD_NUM_BARS)
    {
        pr_err("pciem_instance: Invalid BAR index %lu via mmap offset\n", bar_index);
        return -EINVAL;
    }

    guard(read_lock)(&us->rc->bars_lock);
    bar = &us->rc->bars[bar_index];

    if (bar->size == 0 || bar->phys_addr == 0)
    {
        pr_err("pciem_instance: BAR%lu is not active or has no physical address\n", bar_index);
        return -EINVAL;
    }

    vma->vm_page_prot = pgprot_noncached(vma->vm_page_prot);

    if (remap_pfn_range(vma, vma->vm_start,
                        bar->phys_addr >> PAGE_SHIFT,
                        size, vma->vm_page_prot))
    {
        return -EAGAIN;
    }

    pr_debug("pciem_instance: Mapped BAR%lu (phys: 0x%llx) to userspace via instance FD\n",
            bar_index, (u64)bar->phys_addr);

    return 0;
}

static const struct file_operations pciem_instance_fops = {
    .owner = THIS_MODULE,
    .mmap = pciem_instance_mmap,
};

static void free_pending_request(struct pciem_userspace_state *us, struct pciem_pending_request *req)
{
    unsigned long flags;

    spin_lock_irqsave(&us->pending_lock, flags);
    hlist_del(&req->node);
    spin_unlock_irqrestore(&us->pending_lock, flags);

    kfree(req);
}

static struct pciem_pending_request *find_pending_request(struct pciem_userspace_state *us, uint64_t seq)
{
    struct pciem_pending_request *req;
    int hash = (int)(seq % ARRAY_SIZE(us->pending_requests));

    hlist_for_each_entry(req, &us->pending_requests[hash], node)
    {
        if (req->seq == seq)
            return req;
    }

    return NULL;
}

static void pciem_irqfd_shutdown(struct pciem_irqfd *irqfd)
{
    u64 cnt;

    list_del_init(&irqfd->list);

    eventfd_ctx_remove_wait_queue(irqfd->trigger, &irqfd->wait, &cnt);

    flush_work(&irqfd->inject_work);

    eventfd_ctx_put(irqfd->trigger);
    kfree(irqfd);
}

static void pciem_irqfds_init(struct pciem_irqfds *irqfds)
{
    spin_lock_init(&irqfds->lock);
    INIT_LIST_HEAD(&irqfds->items);
}

static void pciem_irqfds_shutdown(struct pciem_irqfds *irqfds)
{
    struct pciem_irqfd *irqfd, *tmp;

    guard(spinlock_irqsave)(&irqfds->lock);

    list_for_each_entry_safe(irqfd, tmp, &irqfds->items, list) {
        pciem_irqfd_shutdown(irqfd);
    }
}

static void pciem_tracing_destroy(struct pciem_userspace_state *us)
{
    unsigned int i;

    for (i = 0; i < PCI_STD_NUM_BARS; ++i) {
        if (us->tracers[i].us) {
            smptrace_destroy(&us->tracers[i].ctx);
            us->tracers[i].us = NULL;
        }
    }
}

static void pciem_tracing_init(struct pciem_userspace_state *us)
{
    memset(&us->tracers, 0, ARRAY_SIZE(us->tracers));
}

static int pciem_shared_ring_alloc(struct pciem_userspace_state *us)
{
    struct page *page;
    int order = get_order(sizeof(struct pciem_shared_ring));

    page = alloc_pages(GFP_KERNEL_ACCOUNT | __GFP_ZERO | __GFP_COMP, order);
    if (!page)
        return -ENOMEM;

    us->shared_ring = page_address(page);
    atomic_set(&us->shared_ring->head, 0);
    atomic_set(&us->shared_ring->tail, 0);
    spin_lock_init(&us->shared_ring_lock);

    return 0;
}

struct pciem_userspace_state *pciem_userspace_create(void)
{
    struct pciem_userspace_state *us;
    int i, ret;

    us = kzalloc(sizeof(*us), GFP_KERNEL);
    if (!us)
        return ERR_PTR(-ENOMEM);

    pciem_tracing_init(us);

    ret = pciem_shared_ring_alloc(us);
    if (ret) {
        kfree(us);
        return ERR_PTR(ret);
    }

    for (i = 0; i < ARRAY_SIZE(us->pending_requests); i++)
        INIT_HLIST_HEAD(&us->pending_requests[i]);
    spin_lock_init(&us->pending_lock);
    us->next_seq = 1;

    atomic_set(&us->registered, PCIEM_UNREGISTERED);
    atomic_set(&us->event_pending, 0);

    us->eventfd = NULL;
    spin_lock_init(&us->eventfd_lock);

    pciem_irqfds_init(&us->irqfds);

    return us;
}

void pciem_userspace_destroy(struct pciem_userspace_state *us)
{
    struct pciem_pending_request *req;
    struct hlist_node *tmp;
    int i;

    if (!us)
        return;

    pciem_tracing_destroy(us);
    pciem_irqfds_shutdown(&us->irqfds);

    for (i = 0; i < ARRAY_SIZE(us->pending_requests); i++)
    {
        hlist_for_each_entry_safe(req, tmp, &us->pending_requests[i], node)
        {
            req->response_status = -ENODEV;
            complete(&req->done);
            hlist_del(&req->node);
            kfree(req);
        }
    }

    __free_pages(virt_to_page(us->shared_ring), get_order(sizeof(struct pciem_shared_ring)));

    if (us->eventfd)
        eventfd_ctx_put(us->eventfd);

    kfree(us);
}

static bool pciem_shared_ring_push(struct pciem_userspace_state *us,
                                   struct pciem_event *event)
{
    int tail, next_tail, head;

    guard(spinlock_irqsave)(&us->shared_ring_lock);

    tail = atomic_read(&us->shared_ring->tail);
    next_tail = (tail + 1) % PCIEM_RING_SIZE;
    head = atomic_read(&us->shared_ring->head);

    if (next_tail == head)
        return false;

    memcpy(&us->shared_ring->events[tail], event, sizeof(*event));
    atomic_set_release(&us->shared_ring->tail, next_tail);

    return true;
}

static void pciem_eventfd_signal(struct pciem_userspace_state *us)
{
#if LINUX_VERSION_CODE <= KERNEL_VERSION(6,7,0)
    eventfd_signal(us->eventfd, 1);
#else
    eventfd_signal(us->eventfd);
#endif
}

void pciem_userspace_queue_event(struct pciem_userspace_state *us, struct pciem_event *event)
{
    unsigned long flags;

    if (!us || !event)
        return;

    event->timestamp = ktime_get_ns();

    if (!pciem_shared_ring_push(us, event))
        pr_warn_ratelimited("Shared ring buffer full, dropping event for userspace (seq=%llu)\n",
                            event->seq);

    spin_lock_irqsave(&us->eventfd_lock, flags);
    if (us->eventfd)
        pciem_eventfd_signal(us);
    else
        atomic_set(&us->event_pending, 1);
    spin_unlock_irqrestore(&us->eventfd_lock, flags);
}

int pciem_userspace_wait_response(struct pciem_userspace_state *us, uint64_t seq, uint64_t *data_out,
                                  unsigned long timeout_ms)
{
    struct pciem_pending_request *req;
    unsigned long timeout_jiffies;
    int ret;

    req = find_pending_request(us, seq);
    if (!req)
        return -EINVAL;

    timeout_jiffies = msecs_to_jiffies(timeout_ms);
    ret = wait_for_completion_timeout(&req->done, timeout_jiffies);

    if (ret == 0)
    {
        pr_warn("Request seq=%llu timed out\n", seq);
        free_pending_request(us, req);
        return -ETIMEDOUT;
    }

    if (data_out)
        *data_out = req->response_data;

    ret = req->response_status;
    free_pending_request(us, req);

    return ret;
}

static int pciem_check_unregistered(struct pciem_userspace_state *us)
{
    int registered;

    if (!us->rc)
        return -EINVAL;

    registered = atomic_read_acquire(&us->registered);
    if (registered == PCIEM_REGISTERING)
        return -EBUSY;
    if (registered == PCIEM_REGISTERED)
        return -EINVAL;

    return 0;
}

static int pciem_check_registered(struct pciem_userspace_state *us)
{
    int registered;

    if (!us->rc)
        return -EINVAL;

    registered = atomic_read_acquire(&us->registered);
    if (registered == PCIEM_REGISTERING)
        return -EBUSY;
    if (registered == PCIEM_UNREGISTERED)
        return -EINVAL;

    return 0;
}

static int pciem_start_registration(struct pciem_userspace_state *us)
{
    int val = PCIEM_UNREGISTERED;

    if (!us->rc)
        return -EINVAL;

    if (atomic_try_cmpxchg_release(&us->registered, &val, PCIEM_REGISTERING))
        return 0;

    if (val == PCIEM_REGISTERING)
        return -EBUSY;

    /* If the cmpxchg() failed the state can only be REGISTERING
     * (checked above) or REGISTERED (this case), so return EINVAL */
    return -EINVAL;
}

static void pciem_cancel_registration(struct pciem_userspace_state *us)
{
    atomic_set_release(&us->registered, PCIEM_UNREGISTERED);
}

static void pciem_complete_registration(struct pciem_userspace_state *us)
{
    atomic_set_release(&us->registered, PCIEM_REGISTERED);
}

static int pciem_device_release(struct inode *inode, struct file *file)
{
    struct pciem_userspace_state *us = file->private_data;

    pr_info("Userspace device fd closed\n");

    if (us)
    {
        if (!pciem_check_registered(us))
        {
            pr_info("Cleaning up registered device instance\n");
            pciem_free_root_complex(us->rc);
            us->rc = NULL;
        }

        pciem_userspace_destroy(us);
    }

    return 0;
}

static ssize_t pciem_device_write(struct file *file, const char __user *buf, size_t count, loff_t *ppos)
{
    struct pciem_userspace_state *us = file->private_data;
    struct pciem_response response;
    struct pciem_pending_request *req;
    unsigned long flags;

    if (count < sizeof(response))
        return -EINVAL;

    if (copy_from_user(&response, buf, sizeof(response)))
        return -EFAULT;

    spin_lock_irqsave(&us->pending_lock, flags);
    req = find_pending_request(us, response.seq);
    if (req)
    {
        req->response_data = response.data;
        req->response_status = response.status;
        complete(&req->done);
    }
    spin_unlock_irqrestore(&us->pending_lock, flags);

    if (!req)
        return -EINVAL;

    return sizeof(response);
}

static int pciem_device_mmap(struct file *file, struct vm_area_struct *vma)
{
    struct pciem_userspace_state *us = file->private_data;
    unsigned long pfn;
    int ret;

    pfn = page_to_pfn(virt_to_page(us->shared_ring));
    ret = remap_pfn_range(vma, vma->vm_start, pfn, vma->vm_end - vma->vm_start, vma->vm_page_prot);

    if (ret == 0)
        pr_info("Shared ring mmap successful\n");

    return ret;
}

static long pciem_ioctl_create_device(struct pciem_userspace_state *us, struct pciem_create_device __user *arg)
{
    struct pciem_create_device cfg;
    enum pciem_bus_mode mode;

    if (us->rc)
        return -EBUSY;

    if (copy_from_user(&cfg, arg, sizeof(cfg)))
        return -EFAULT;

    us->rc = pciem_alloc_root_complex();
    if (IS_ERR(us->rc))
    {
        int ret = PTR_ERR(us->rc);
        us->rc = NULL;
        return ret;
    }

    switch (cfg.flags & PCIEM_CREATE_FLAG_BUS_MODE_MASK) {
    case PCIEM_CREATE_FLAG_BUS_MODE_VIRTUAL:
        mode = PCIEM_BUS_MODE_VIRTUAL_ROOT;
        pr_info("Userspace requested VIRTUAL_ROOT mode\n");
        break;
    case PCIEM_CREATE_FLAG_BUS_MODE_ATTACH:
        mode = PCIEM_BUS_MODE_ATTACH_TO_HOST;
        pr_info("Userspace requested ATTACH_TO_HOST mode\n");
        break;
    default:
        mode = PCIEM_BUS_MODE_VIRTUAL_ROOT;
        pr_info("Using default VIRTUAL_ROOT mode\n");
        break;
    }

    us->rc->bus_mode = mode;

    pciem_init_cap_manager(us->rc);

    pr_info("Created userspace device instance\n");

    return 0;
}

static long pciem_ioctl_add_bar(struct pciem_userspace_state *us, struct pciem_bar_config __user *arg)
{
    struct pciem_bar_config cfg;
    int ret;

    ret = pciem_check_unregistered(us);
    if (ret)
        return ret;

    if (copy_from_user(&cfg, arg, sizeof(cfg)))
        return -EFAULT;

    if (cfg.bar_index >= PCI_STD_NUM_BARS)
        return -EINVAL;

    if (cfg.size && (cfg.size & (cfg.size - 1)))
    {
        pr_err("BAR%u size 0x%llx is not a power of 2\n", cfg.bar_index, cfg.size);
        return -EINVAL;
    }

    ret = pciem_register_bar(us->rc, cfg.bar_index, cfg.size, cfg.flags);

    if (ret == 0)
    {
        pr_info("Registered BAR%u: size=0x%llx, flags=0x%x\n", cfg.bar_index, cfg.size, cfg.flags);
    }

    return ret;
}

static long pciem_ioctl_add_capability(struct pciem_userspace_state *us, struct pciem_cap_config __user *arg)
{
    struct pciem_cap_config cfg;
    int ret;

    ret = pciem_check_unregistered(us);
    if (ret)
        return ret;

    if (copy_from_user(&cfg, arg, sizeof(cfg)))
        return -EFAULT;

    switch (cfg.cap_type)
    {
    case PCIEM_CAP_MSI: {
        struct pciem_cap_msi_userspace *msi_cfg;
        struct pciem_cap_msi_config msi;

        msi_cfg = &cfg.msi;
        msi.num_vectors_log2 = msi_cfg->num_vectors_log2;
        msi.has_64bit = msi_cfg->has_64bit;
        msi.has_per_vector_masking = msi_cfg->has_masking;

        ret = pciem_add_cap_msi(us->rc, &msi);
        break;
    }

    case PCIEM_CAP_MSIX: {
        struct pciem_cap_msix_userspace *msix_cfg;
        struct pciem_cap_msix_config msix;

        msix_cfg = &cfg.msix;
        msix.bar_index = msix_cfg->bar_index;
        msix.table_offset = msix_cfg->table_offset;
        msix.pba_offset = msix_cfg->pba_offset;
        msix.table_size = msix_cfg->table_size;

        ret = pciem_add_cap_msix(us->rc, &msix);
        break;
    }

    default:
        pr_warn("Unsupported capability type: %d\n", cfg.cap_type);
        ret = -ENOTSUPP;
    }

    return ret;
}

static long pciem_ioctl_set_config(struct pciem_userspace_state *us, struct pciem_config_space __user *arg)
{
    struct pciem_config_space cfg;
    u8 *config;
    int ret;

    ret = pciem_check_unregistered(us);
    if (ret)
        return ret;

    if (copy_from_user(&cfg, arg, sizeof(cfg)))
        return -EFAULT;

    config = us->rc->cfg;

    *(u16 *)(config + PCI_VENDOR_ID) = cfg.vendor_id;
    *(u16 *)(config + PCI_DEVICE_ID) = cfg.device_id;
    *(u16 *)(config + PCI_SUBSYSTEM_VENDOR_ID) = cfg.subsys_vendor_id;
    *(u16 *)(config + PCI_SUBSYSTEM_ID) = cfg.subsys_device_id;
    *(u8 *)(config + PCI_REVISION_ID) = cfg.revision;
    *(u8 *)(config + PCI_CLASS_PROG) = cfg.class_code[0];
    *(u8 *)(config + PCI_CLASS_DEVICE) = cfg.class_code[1];
    *(u8 *)(config + PCI_CLASS_DEVICE + 1) = cfg.class_code[2];
    *(u8 *)(config + PCI_HEADER_TYPE) = cfg.header_type;
    *(u16 *)(config + PCI_COMMAND) = PCI_COMMAND_MEMORY;
    *(u16 *)(config + PCI_STATUS) = PCI_STATUS_CAP_LIST;

    pr_info("Config space set: vendor=0x%04x, device=0x%04x, class=0x%02x%02x%02x\n", cfg.vendor_id, cfg.device_id,
            cfg.class_code[2], cfg.class_code[1], cfg.class_code[0]);

    return 0;
}

static long pciem_ioctl_register(struct pciem_userspace_state *us)
{
    int ret;
    int fd;

    ret = pciem_start_registration(us);
    if (ret)
        return ret;

    pr_info("Registering userspace-defined device on PCI bus\n");

    pciem_build_config_space(us->rc);

    ret = pciem_complete_init(us->rc);
    if (ret)
    {
        pciem_cancel_registration(us);
        pr_err("Failed to complete device initialization: %d\n", ret);
        return ret;
    }

    pciem_complete_registration(us);

    fd = anon_inode_getfd("pciem_instance", &pciem_instance_fops, us, O_RDWR | O_CLOEXEC);
    if (fd < 0) {
        pr_err("Failed to create instance fd\n");
        return fd;
    }

    pr_info("Userspace device registered successfully, returning FD %d\n",
            fd);

    return fd;
}

static long pciem_ioctl_start(struct pciem_userspace_state *us)
{
    int ret;

    ret = pciem_check_registered(us);
    if (ret)
        return ret;

    return pciem_start_device(us->rc);
}

static long pciem_ioctl_inject_irq(struct pciem_userspace_state *us, struct pciem_irq_inject __user *arg)
{
    struct pciem_irq_inject inject;
    int ret;

    ret = pciem_check_registered(us);
    if (ret)
        return ret;

    if (copy_from_user(&inject, arg, sizeof(inject)))
        return -EFAULT;

    pr_debug("Injecting MSI vector %d\n", inject.vector);

    pciem_trigger_msi(us->rc, inject.vector);

    return 0;
}

static long pciem_ioctl_dma(struct pciem_userspace_state *us, struct pciem_dma_op __user *arg)
{
    struct pciem_dma_op op;
    void *kernel_buf;
    int ret;

    ret = pciem_check_registered(us);
    if (ret)
        return ret;

    if (copy_from_user(&op, arg, sizeof(op)))
        return -EFAULT;

    if (op.length == 0)
        return -EINVAL;

    kernel_buf = kmalloc(op.length, GFP_KERNEL);
    if (!kernel_buf)
        return -ENOMEM;

    if (op.flags & PCIEM_DMA_FLAG_WRITE)
    {
        if (copy_from_user(kernel_buf, (void __user *)op.user_addr, op.length))
        {
            kfree(kernel_buf);
            return -EFAULT;
        }

        ret = pciem_dma_write_to_guest(us->rc, op.guest_iova, kernel_buf, op.length, op.pasid);
    }
    else
    {
        ret = pciem_dma_read_from_guest(us->rc, op.guest_iova, kernel_buf, op.length, op.pasid);

        if (ret == 0 && copy_to_user((void __user *)op.user_addr, kernel_buf, op.length))
            ret = -EFAULT;
    }

    kfree(kernel_buf);
    return ret;
}

static long pciem_ioctl_dma_atomic(struct pciem_userspace_state *us, struct pciem_dma_atomic __user *arg)
{
    struct pciem_dma_atomic atomic;
    u64 result;
    int ret;

    ret = pciem_check_registered(us);
    if (ret)
        return ret;

    if (copy_from_user(&atomic, arg, sizeof(atomic)))
        return -EFAULT;

    switch (atomic.op_type)
    {
    case PCIEM_ATOMIC_FETCH_ADD:
        result = pciem_dma_atomic_fetch_add(us->rc, atomic.guest_iova, atomic.operand, atomic.pasid);
        break;
    case PCIEM_ATOMIC_FETCH_SUB:
        result = pciem_dma_atomic_fetch_sub(us->rc, atomic.guest_iova, atomic.operand, atomic.pasid);
        break;
    case PCIEM_ATOMIC_SWAP:
        result = pciem_dma_atomic_swap(us->rc, atomic.guest_iova, atomic.operand, atomic.pasid);
        break;
    case PCIEM_ATOMIC_CAS:
        result = pciem_dma_atomic_cas(us->rc, atomic.guest_iova, atomic.compare, atomic.operand, atomic.pasid);
        break;
    case PCIEM_ATOMIC_FETCH_AND:
        result = pciem_dma_atomic_fetch_and(us->rc, atomic.guest_iova, atomic.operand, atomic.pasid);
        break;
    case PCIEM_ATOMIC_FETCH_OR:
        result = pciem_dma_atomic_fetch_or(us->rc, atomic.guest_iova, atomic.operand, atomic.pasid);
        break;
    case PCIEM_ATOMIC_FETCH_XOR:
        result = pciem_dma_atomic_fetch_xor(us->rc, atomic.guest_iova, atomic.operand, atomic.pasid);
        break;
    default:
        return -EINVAL;
    }

    atomic.result = result;

    if (copy_to_user(arg, &atomic, sizeof(atomic)))
        return -EFAULT;

    return ret;
}

static long pciem_ioctl_p2p(struct pciem_userspace_state *us, struct pciem_p2p_op_user __user *arg)
{
    struct pciem_p2p_op_user op;
    void *kernel_buf;
    int ret;

    ret = pciem_check_registered(us);
    if (ret)
        return ret;

    if (copy_from_user(&op, arg, sizeof(op)))
        return -EFAULT;

    if (op.length == 0)
        return -EINVAL;

    kernel_buf = kmalloc(op.length, GFP_KERNEL);
    if (!kernel_buf)
        return -ENOMEM;

    if (op.flags & PCIEM_DMA_FLAG_WRITE)
    {
        if (copy_from_user(kernel_buf, (void __user *)op.user_addr, op.length))
        {
            kfree(kernel_buf);
            return -EFAULT;
        }

        ret = pciem_p2p_write(us->rc, op.target_phys_addr, kernel_buf, op.length);
    }
    else
    {
        ret = pciem_p2p_read(us->rc, op.target_phys_addr, kernel_buf, op.length);

        if (ret == 0 && copy_to_user((void __user *)op.user_addr, kernel_buf, op.length))
            ret = -EFAULT;
    }

    kfree(kernel_buf);
    return ret;
}

static long pciem_ioctl_get_bar_info(struct pciem_userspace_state *us, struct pciem_bar_info_query __user *arg)
{
    struct pciem_bar_info_query query;
    struct pciem_bar_info *bar;
    int ret;

    ret = pciem_check_registered(us);
    if (ret)
        return ret;

    if (copy_from_user(&query, arg, sizeof(query)))
        return -EFAULT;

    if (query.bar_index >= PCI_STD_NUM_BARS)
        return -EINVAL;

    guard(read_lock)(&us->rc->bars_lock);
    bar = &us->rc->bars[query.bar_index];

    if (bar->size == 0)
        return -ENOENT;

    query.phys_addr = bar->phys_addr;
    query.size = bar->size;
    query.flags = bar->flags;

    if (copy_to_user(arg, &query, sizeof(query)))
        return -EFAULT;

    pr_debug("BAR%u info: phys=0x%llx size=0x%llx flags=0x%x\n", query.bar_index, query.phys_addr, query.size,
             query.flags);

    return 0;
}

static long pciem_ioctl_set_eventfd(struct pciem_userspace_state *us, struct pciem_eventfd_config __user *arg)
{
    struct pciem_eventfd_config cfg;
    struct eventfd_ctx *eventfd = NULL;
    struct eventfd_ctx *old_eventfd = NULL;
    unsigned long flags;
    int fd;

    if (copy_from_user(&cfg, arg, sizeof(cfg)))
        return -EFAULT;

    fd = cfg.eventfd;

    if (fd >= 0)
    {
        eventfd = eventfd_ctx_fdget(fd);
        if (IS_ERR(eventfd))
        {
            pr_err("Failed to get eventfd context for fd %d: %ld\n", fd, PTR_ERR(eventfd));
            return PTR_ERR(eventfd);
        }
        pr_info("Registered eventfd %d for ring buffer notifications\n", fd);
    }

    spin_lock_irqsave(&us->eventfd_lock, flags);
    old_eventfd = us->eventfd;
    us->eventfd = eventfd;
    spin_unlock_irqrestore(&us->eventfd_lock, flags);

    /* If there was no previous eventfd, there may be pending events
     * from before userspace registered this eventfd */
    if (!old_eventfd) {
        if (atomic_xchg(&us->event_pending, 0))
            pciem_eventfd_signal(us);
        return 0;
    }

    /* Free the previous eventfd */
    eventfd_ctx_put(old_eventfd);
    pr_info("Unregistered previous eventfd\n");

    return 0;
}

static void pciem_irqfd_work(struct work_struct *work)
{
    struct pciem_irqfd *irqfd = container_of(work, struct pciem_irqfd, inject_work);
    struct pciem_userspace_state *us = irqfd->us;

    if (us && us->rc) {
        pciem_trigger_msi(us->rc, irqfd->vector);
    }
}

static int pciem_irqfd_wakeup(wait_queue_entry_t *wait, unsigned mode, int sync, void *key)
{
    struct pciem_irqfd *irqfd = container_of(wait, struct pciem_irqfd, wait);
    struct pciem_irqfds *irqfds = &irqfd->us->irqfds;
    __poll_t flags = key_to_poll(key);
    u64 count;

    if (flags & EPOLLIN) {
        eventfd_ctx_do_read(irqfd->trigger, &count);
        schedule_work(&irqfd->inject_work);
    }

    if (flags & EPOLLHUP) {
        guard(spinlock_irqsave)(&irqfds->lock);
        if (!list_empty(&irqfd->list)) {
            pr_info("Unregistering IRQ eventfd for vector %u\n", irqfd->vector);
            pciem_irqfd_shutdown(irqfd);
        }
    }

    return 0;
}

struct pciem_poll_helper {
    struct poll_table_struct pt;
    struct pciem_irqfd *irqfd;
};

static void pciem_irqfd_ptable_queue_proc(struct file *file, wait_queue_head_t *wqh, poll_table *pt)
{
    struct pciem_poll_helper *helper = container_of(pt, struct pciem_poll_helper, pt);
    struct pciem_irqfd *irqfd = helper->irqfd;
    struct pciem_irqfds *irqfds = &irqfd->us->irqfds;

    guard(spinlock_irqsave)(&irqfds->lock);

    add_wait_queue(wqh, &irqfd->wait);
    list_add_tail(&irqfd->list, &irqfds->items);
}

static long pciem_ioctl_set_irqfd(struct pciem_userspace_state *us,
                                        struct pciem_irqfd_config __user *arg)
{
    struct pciem_irqfd_config cfg;
    struct eventfd_ctx *eventfd = NULL;
    struct pciem_irqfd *irqfd = NULL;
    struct fd f = EMPTY_FD;
    struct pciem_poll_helper pt_helper;
    __poll_t events;
    int ret;

    ret = pciem_check_registered(us);
    if (ret)
        return ret;

    if (copy_from_user(&cfg, arg, sizeof(cfg)))
        return -EFAULT;

    irqfd = kzalloc(sizeof(*irqfd), GFP_KERNEL_ACCOUNT);
    if (!irqfd)
        return -ENOMEM;

    eventfd = eventfd_ctx_fdget(cfg.eventfd);
    if (IS_ERR(eventfd)) {
        ret = PTR_ERR(eventfd);
        goto fail;
    }

    f = fdget(cfg.eventfd);
    if (fd_empty(f)) {
        ret = -EBADF;
        goto fail;
    }

    irqfd->trigger = eventfd;
    irqfd->vector = cfg.vector;
    irqfd->flags = cfg.flags;
    irqfd->us = us;
    INIT_LIST_HEAD(&irqfd->list);
    INIT_WORK(&irqfd->inject_work, pciem_irqfd_work);
    init_waitqueue_func_entry(&irqfd->wait, pciem_irqfd_wakeup);

    init_poll_funcptr(&pt_helper.pt, pciem_irqfd_ptable_queue_proc);
    pt_helper.irqfd = irqfd;

    events = vfs_poll(fd_file(f), &pt_helper.pt);
    if (events & EPOLLIN)
        schedule_work(&irqfd->inject_work);

    fdput(f);

    pr_info("Registered IRQ eventfd %d for vector %u (Direct Wakeup)\n", cfg.eventfd, cfg.vector);

    return 0;

fail:
    if (!fd_empty(f))
        fdput(f);
    if (eventfd && !IS_ERR(eventfd))
        eventfd_ctx_put(eventfd);
    if (irqfd)
        kfree(irqfd);
    return ret;
}

static long pciem_ioctl_dma_indirect(struct pciem_userspace_state *us, struct pciem_dma_indirect __user *arg)
{
    struct pciem_dma_indirect req;
    void *data_buf = NULL;
    uint64_t *list_buf = NULL;
    uint64_t cur_prp_list;
    uint32_t page_size;
    uint32_t offset;
    uint32_t chunk;
    uint64_t user_ptr;
    uint32_t remaining;
    int list_idx = 0;
    int ret;

    ret = pciem_check_registered(us);
    if (ret)
        return ret;

    if (copy_from_user(&req, arg, sizeof(req)))
        return -EFAULT;

    if (req.length == 0)
        return 0;

    page_size = req.page_size;

    if (page_size < 4096 || page_size > 65536 || (page_size & (page_size - 1)))
        return -EINVAL;

    remaining = req.length;
    user_ptr = req.user_addr;

    list_buf = kmalloc(page_size, GFP_KERNEL);
    data_buf = kmalloc(page_size, GFP_KERNEL);

    if (!list_buf || !data_buf) {
        ret = -ENOMEM;
        goto out;
    }

    offset = req.prp1 & (page_size - 1);
    chunk = page_size - offset;
    if (chunk > remaining) chunk = remaining;

    if (req.flags & PCIEM_DMA_FLAG_WRITE) {
        if (copy_from_user(data_buf, (void __user *)user_ptr, chunk)) {
            ret = -EFAULT;
            goto out;
        }
        ret = pciem_dma_write_to_guest(us->rc, req.prp1, data_buf, chunk, req.pasid);
    } else {
        ret = pciem_dma_read_from_guest(us->rc, req.prp1, data_buf, chunk, req.pasid);
        if (ret == 0) {
            if (copy_to_user((void __user *)user_ptr, data_buf, chunk))
                ret = -EFAULT;
        }
    }

    if (ret) goto out;

    remaining -= chunk;
    user_ptr += chunk;

    if (remaining == 0) goto out;

    if (remaining <= page_size) {
        if (req.flags & PCIEM_DMA_FLAG_WRITE) {
            if (copy_from_user(data_buf, (void __user *)user_ptr, remaining)) {
                ret = -EFAULT;
                goto out;
            }
            ret = pciem_dma_write_to_guest(us->rc, req.prp2, data_buf, remaining, req.pasid);
        } else {
            ret = pciem_dma_read_from_guest(us->rc, req.prp2, data_buf, remaining, req.pasid);
            if (ret == 0 && copy_to_user((void __user *)user_ptr, data_buf, remaining)) {
                ret = -EFAULT;
            }
        }
        goto out;
    }

    cur_prp_list = req.prp2;
    list_idx = 0;

    uint32_t list_offset = cur_prp_list & (page_size - 1);
    uint32_t list_bytes = page_size - list_offset;
    
    ret = pciem_dma_read_from_guest(us->rc, cur_prp_list, list_buf, list_bytes, req.pasid);
    if (ret) goto out;

    uint64_t *prps = (uint64_t *)list_buf;
    uint32_t max_entries = list_bytes / 8;

    while (remaining > 0) {
        if (list_idx == max_entries - 1 && remaining > page_size) {
            cur_prp_list = prps[list_idx];
            
            list_offset = cur_prp_list & (page_size - 1);
            list_bytes = page_size - list_offset;
            max_entries = list_bytes / 8;

            ret = pciem_dma_read_from_guest(us->rc, cur_prp_list, list_buf, list_bytes, req.pasid);
            if (ret) goto out;
            
            prps = (uint64_t *)list_buf;
            list_idx = 0;
            continue;
        }

        uint64_t data_phys = prps[list_idx++];
        chunk = (remaining < page_size) ? remaining : page_size;

        if (req.flags & PCIEM_DMA_FLAG_WRITE) {
            if (copy_from_user(data_buf, (void __user *)user_ptr, chunk)) {
                ret = -EFAULT;
                goto out;
            }
            ret = pciem_dma_write_to_guest(us->rc, data_phys, data_buf, chunk, req.pasid);
        } else {
            ret = pciem_dma_read_from_guest(us->rc, data_phys, data_buf, chunk, req.pasid);
            if (ret == 0 && copy_to_user((void __user *)user_ptr, data_buf, chunk)) {
                ret = -EFAULT;
            }
        }

        if (ret) goto out;

        remaining -= chunk;
        user_ptr += chunk;
    }

out:
    kfree(list_buf);
    kfree(data_buf);
    return ret;
}

static void pciem_notif_trace(struct smptrace_ctx *ctx, struct smptrace_io *io,
                              uint32_t ev_type)
{
    struct pciem_tracer *tracer = container_of(ctx, struct pciem_tracer, ctx);
    struct pciem_event ev = {0};

    ev.bar = ctx->opaque;
    ev.offset = io->offset;
    ev.size = io->size;
    ev.type = ev_type;
    switch (io->size) {
    case 1:
        ev.data = io->data.byte;
        break;
    case 2:
        ev.data = io->data.word;
        break;
    case 4:
        ev.data = io->data.dword;
        break;
    case 8:
        ev.data = io->data.qword;
        break;
    default:
        BUG();
    }
    pciem_userspace_queue_event(tracer->us, &ev);
}

static void pciem_notif_write(struct smptrace_ctx *ctx, struct smptrace_io *io)
{
    pciem_notif_trace(ctx, io, PCIEM_EVENT_MMIO_WRITE);
}

static void pciem_notif_read(struct smptrace_ctx *ctx, struct smptrace_io *io)
{
    pciem_notif_trace(ctx, io, PCIEM_EVENT_MMIO_READ);
}

static int pciem_ioctl_trace_bar(struct pciem_userspace_state *us,
                                 struct pciem_trace_bar __user *arg)
{
    struct pciem_trace_bar req;
    struct pciem_bar_info *bar;
    struct pciem_tracer *tracer;
    int ret;

    if (copy_from_user(&req, arg, sizeof(req)))
        return -EFAULT;

    if (req.bar_index >= PCI_STD_NUM_BARS)
        return -EINVAL;

    guard(write_lock)(&us->rc->bars_lock);

    bar = &us->rc->bars[req.bar_index];
    tracer = &us->tracers[req.bar_index];
    if (tracer->us)
        return -EINVAL;

    if (!bar->carved_start || !bar->size) {
        pr_warn("cannot trace BAR%u, not registered", req.bar_index);
        return -ENXIO;
    }

    memset(tracer, 0, sizeof(*tracer));
    tracer->ctx.opaque = req.bar_index;
    tracer->ctx.pa = bar->carved_start;
    tracer->ctx.len = bar->size;
    if (req.flags & PCIEM_TRACE_WRITES)
        tracer->ctx.notif.write = pciem_notif_write;
    if (req.flags & PCIEM_TRACE_READS)
        tracer->ctx.notif.read = pciem_notif_read;
    tracer->ctx.stop_writes = req.flags & PCIEM_TRACE_STOP_WRITES;
    ret = smptrace_init(&tracer->ctx);
    if (ret)
        return ret;
    tracer->us = us;

    pr_info("Beginning tracing on BAR%u (PA = 0x%llx)",
            req.bar_index, bar->carved_start);

    return 0;
}

static long pciem_device_ioctl(struct file *file, unsigned int cmd, unsigned long arg)
{
    struct pciem_userspace_state *us = file->private_data;

    switch (cmd)
    {
    case PCIEM_IOCTL_CREATE_DEVICE:
        return pciem_ioctl_create_device(us, (struct pciem_create_device __user *)arg);

    case PCIEM_IOCTL_ADD_BAR:
        return pciem_ioctl_add_bar(us, (struct pciem_bar_config __user *)arg);

    case PCIEM_IOCTL_ADD_CAPABILITY:
        return pciem_ioctl_add_capability(us, (struct pciem_cap_config __user *)arg);

    case PCIEM_IOCTL_SET_CONFIG:
        return pciem_ioctl_set_config(us, (struct pciem_config_space __user *)arg);

    case PCIEM_IOCTL_REGISTER:
        return pciem_ioctl_register(us);

    case PCIEM_IOCTL_START:
        return pciem_ioctl_start(us);

    case PCIEM_IOCTL_INJECT_IRQ:
        return pciem_ioctl_inject_irq(us, (struct pciem_irq_inject __user *)arg);

    case PCIEM_IOCTL_DMA:
        return pciem_ioctl_dma(us, (struct pciem_dma_op __user *)arg);

    case PCIEM_IOCTL_DMA_ATOMIC:
        return pciem_ioctl_dma_atomic(us, (struct pciem_dma_atomic __user *)arg);

    case PCIEM_IOCTL_P2P:
        return pciem_ioctl_p2p(us, (struct pciem_p2p_op_user __user *)arg);

    case PCIEM_IOCTL_GET_BAR_INFO:
        return pciem_ioctl_get_bar_info(us, (struct pciem_bar_info_query __user *)arg);

    case PCIEM_IOCTL_SET_EVENTFD:
        return pciem_ioctl_set_eventfd(us, (struct pciem_eventfd_config __user *)arg);

    case PCIEM_IOCTL_SET_IRQFD:
        return pciem_ioctl_set_irqfd(us, (struct pciem_irqfd_config __user *)arg);

    case PCIEM_IOCTL_DMA_INDIRECT:
        return pciem_ioctl_dma_indirect(us, (struct pciem_dma_indirect __user *)arg);

    case PCIEM_IOCTL_TRACE_BAR:
        return pciem_ioctl_trace_bar(us, (struct pciem_trace_bar __user*)arg);

    default:
        return -ENOTTY;
    }
}

int pciem_userspace_init(void)
{
    pr_info("Userspace device support initialized\n");
    return 0;
}

void pciem_userspace_cleanup(void)
{
    pr_info("Userspace device support cleanup\n");
}

EXPORT_SYMBOL(pciem_userspace_create);
EXPORT_SYMBOL(pciem_userspace_destroy);
EXPORT_SYMBOL(pciem_userspace_queue_event);
EXPORT_SYMBOL(pciem_userspace_wait_response);

```

`kernel/trace/arch/x86/include/asm/inat.h`:

```h
/* SPDX-License-Identifier: GPL-2.0-or-later */
#ifndef _ASM_X86_INAT_H
#define _ASM_X86_INAT_H
/*
 * x86 instruction attributes
 *
 * Written by Masami Hiramatsu <mhiramat@redhat.com>
 */
#include <asm/inat_types.h> /* __ignore_sync_check__ */

/*
 * Internal bits. Don't use bitmasks directly, because these bits are
 * unstable. You should use checking functions.
 */

#define INAT_OPCODE_TABLE_SIZE 256
#define INAT_GROUP_TABLE_SIZE 8

/* Legacy last prefixes */
#define INAT_PFX_OPNDSZ	1	/* 0x66 */ /* LPFX1 */
#define INAT_PFX_REPE	2	/* 0xF3 */ /* LPFX2 */
#define INAT_PFX_REPNE	3	/* 0xF2 */ /* LPFX3 */
/* Other Legacy prefixes */
#define INAT_PFX_LOCK	4	/* 0xF0 */
#define INAT_PFX_CS	5	/* 0x2E */
#define INAT_PFX_DS	6	/* 0x3E */
#define INAT_PFX_ES	7	/* 0x26 */
#define INAT_PFX_FS	8	/* 0x64 */
#define INAT_PFX_GS	9	/* 0x65 */
#define INAT_PFX_SS	10	/* 0x36 */
#define INAT_PFX_ADDRSZ	11	/* 0x67 */
/* x86-64 REX prefix */
#define INAT_PFX_REX	12	/* 0x4X */
/* AVX VEX prefixes */
#define INAT_PFX_VEX2	13	/* 2-bytes VEX prefix */
#define INAT_PFX_VEX3	14	/* 3-bytes VEX prefix */
#define INAT_PFX_EVEX	15	/* EVEX prefix */
/* x86-64 REX2 prefix */
#define INAT_PFX_REX2	16	/* 0xD5 */
/* AMD XOP prefix */
#define INAT_PFX_XOP	17	/* 0x8F */

#define INAT_LSTPFX_MAX	3
#define INAT_LGCPFX_MAX	11

/* Immediate size */
#define INAT_IMM_BYTE		1
#define INAT_IMM_WORD		2
#define INAT_IMM_DWORD		3
#define INAT_IMM_QWORD		4
#define INAT_IMM_PTR		5
#define INAT_IMM_VWORD32	6
#define INAT_IMM_VWORD		7

/* Legacy prefix */
#define INAT_PFX_OFFS	0
#define INAT_PFX_BITS	5
#define INAT_PFX_MAX    ((1 << INAT_PFX_BITS) - 1)
#define INAT_PFX_MASK	(INAT_PFX_MAX << INAT_PFX_OFFS)
/* Escape opcodes */
#define INAT_ESC_OFFS	(INAT_PFX_OFFS + INAT_PFX_BITS)
#define INAT_ESC_BITS	2
#define INAT_ESC_MAX	((1 << INAT_ESC_BITS) - 1)
#define INAT_ESC_MASK	(INAT_ESC_MAX << INAT_ESC_OFFS)
/* Group opcodes (1-16) */
#define INAT_GRP_OFFS	(INAT_ESC_OFFS + INAT_ESC_BITS)
#define INAT_GRP_BITS	5
#define INAT_GRP_MAX	((1 << INAT_GRP_BITS) - 1)
#define INAT_GRP_MASK	(INAT_GRP_MAX << INAT_GRP_OFFS)
/* Immediates */
#define INAT_IMM_OFFS	(INAT_GRP_OFFS + INAT_GRP_BITS)
#define INAT_IMM_BITS	3
#define INAT_IMM_MASK	(((1 << INAT_IMM_BITS) - 1) << INAT_IMM_OFFS)
/* Flags */
#define INAT_FLAG_OFFS	(INAT_IMM_OFFS + INAT_IMM_BITS)
#define INAT_MODRM	(1 << (INAT_FLAG_OFFS))
#define INAT_FORCE64	(1 << (INAT_FLAG_OFFS + 1))
#define INAT_SCNDIMM	(1 << (INAT_FLAG_OFFS + 2))
#define INAT_MOFFSET	(1 << (INAT_FLAG_OFFS + 3))
#define INAT_VARIANT	(1 << (INAT_FLAG_OFFS + 4))
#define INAT_VEXOK	(1 << (INAT_FLAG_OFFS + 5))
#define INAT_XOPOK	INAT_VEXOK
#define INAT_VEXONLY	(1 << (INAT_FLAG_OFFS + 6))
#define INAT_EVEXONLY	(1 << (INAT_FLAG_OFFS + 7))
#define INAT_NO_REX2	(1 << (INAT_FLAG_OFFS + 8))
#define INAT_REX2_VARIANT	(1 << (INAT_FLAG_OFFS + 9))
#define INAT_EVEX_SCALABLE	(1 << (INAT_FLAG_OFFS + 10))
#define INAT_INV64	(1 << (INAT_FLAG_OFFS + 11))
/* Attribute making macros for attribute tables */
#define INAT_MAKE_PREFIX(pfx)	(pfx << INAT_PFX_OFFS)
#define INAT_MAKE_ESCAPE(esc)	(esc << INAT_ESC_OFFS)
#define INAT_MAKE_GROUP(grp)	((grp << INAT_GRP_OFFS) | INAT_MODRM)
#define INAT_MAKE_IMM(imm)	(imm << INAT_IMM_OFFS)

/* Identifiers for segment registers */
#define INAT_SEG_REG_IGNORE	0
#define INAT_SEG_REG_DEFAULT	1
#define INAT_SEG_REG_CS		2
#define INAT_SEG_REG_SS		3
#define INAT_SEG_REG_DS		4
#define INAT_SEG_REG_ES		5
#define INAT_SEG_REG_FS		6
#define INAT_SEG_REG_GS		7

/* Attribute search APIs */
extern insn_attr_t inat_get_opcode_attribute(insn_byte_t opcode);
extern int inat_get_last_prefix_id(insn_byte_t last_pfx);
extern insn_attr_t inat_get_escape_attribute(insn_byte_t opcode,
					     int lpfx_id,
					     insn_attr_t esc_attr);
extern insn_attr_t inat_get_group_attribute(insn_byte_t modrm,
					    int lpfx_id,
					    insn_attr_t esc_attr);
extern insn_attr_t inat_get_avx_attribute(insn_byte_t opcode,
					  insn_byte_t vex_m,
					  insn_byte_t vex_pp);
extern insn_attr_t inat_get_xop_attribute(insn_byte_t opcode,
					  insn_byte_t map_select);

/* Attribute checking functions */
static inline int inat_is_legacy_prefix(insn_attr_t attr)
{
	attr &= INAT_PFX_MASK;
	return attr && attr <= INAT_LGCPFX_MAX;
}

static inline int inat_is_address_size_prefix(insn_attr_t attr)
{
	return (attr & INAT_PFX_MASK) == INAT_PFX_ADDRSZ;
}

static inline int inat_is_operand_size_prefix(insn_attr_t attr)
{
	return (attr & INAT_PFX_MASK) == INAT_PFX_OPNDSZ;
}

static inline int inat_is_rex_prefix(insn_attr_t attr)
{
	return (attr & INAT_PFX_MASK) == INAT_PFX_REX;
}

static inline int inat_is_rex2_prefix(insn_attr_t attr)
{
	return (attr & INAT_PFX_MASK) == INAT_PFX_REX2;
}

static inline int inat_last_prefix_id(insn_attr_t attr)
{
	if ((attr & INAT_PFX_MASK) > INAT_LSTPFX_MAX)
		return 0;
	else
		return attr & INAT_PFX_MASK;
}

static inline int inat_is_vex_prefix(insn_attr_t attr)
{
	attr &= INAT_PFX_MASK;
	return attr == INAT_PFX_VEX2 || attr == INAT_PFX_VEX3 ||
	       attr == INAT_PFX_EVEX;
}

static inline int inat_is_evex_prefix(insn_attr_t attr)
{
	return (attr & INAT_PFX_MASK) == INAT_PFX_EVEX;
}

static inline int inat_is_vex3_prefix(insn_attr_t attr)
{
	return (attr & INAT_PFX_MASK) == INAT_PFX_VEX3;
}

static inline int inat_is_xop_prefix(insn_attr_t attr)
{
	return (attr & INAT_PFX_MASK) == INAT_PFX_XOP;
}

static inline int inat_is_escape(insn_attr_t attr)
{
	return attr & INAT_ESC_MASK;
}

static inline int inat_escape_id(insn_attr_t attr)
{
	return (attr & INAT_ESC_MASK) >> INAT_ESC_OFFS;
}

static inline int inat_is_group(insn_attr_t attr)
{
	return attr & INAT_GRP_MASK;
}

static inline int inat_group_id(insn_attr_t attr)
{
	return (attr & INAT_GRP_MASK) >> INAT_GRP_OFFS;
}

static inline int inat_group_common_attribute(insn_attr_t attr)
{
	return attr & ~INAT_GRP_MASK;
}

static inline int inat_has_immediate(insn_attr_t attr)
{
	return attr & INAT_IMM_MASK;
}

static inline int inat_immediate_size(insn_attr_t attr)
{
	return (attr & INAT_IMM_MASK) >> INAT_IMM_OFFS;
}

static inline int inat_has_modrm(insn_attr_t attr)
{
	return attr & INAT_MODRM;
}

static inline int inat_is_force64(insn_attr_t attr)
{
	return attr & INAT_FORCE64;
}

static inline int inat_has_second_immediate(insn_attr_t attr)
{
	return attr & INAT_SCNDIMM;
}

static inline int inat_has_moffset(insn_attr_t attr)
{
	return attr & INAT_MOFFSET;
}

static inline int inat_has_variant(insn_attr_t attr)
{
	return attr & INAT_VARIANT;
}

static inline int inat_accept_vex(insn_attr_t attr)
{
	return attr & INAT_VEXOK;
}

static inline int inat_accept_xop(insn_attr_t attr)
{
	return attr & INAT_XOPOK;
}

static inline int inat_must_vex(insn_attr_t attr)
{
	return attr & (INAT_VEXONLY | INAT_EVEXONLY);
}

static inline int inat_must_evex(insn_attr_t attr)
{
	return attr & INAT_EVEXONLY;
}

static inline int inat_evex_scalable(insn_attr_t attr)
{
	return attr & INAT_EVEX_SCALABLE;
}

static inline int inat_is_invalid64(insn_attr_t attr)
{
	return attr & INAT_INV64;
}
#endif

```

`kernel/trace/arch/x86/include/asm/insn-eval.h`:

```h
#ifndef _ASM_X86_INSN_EVAL_H
#define _ASM_X86_INSN_EVAL_H
/*
 * A collection of utility functions for x86 instruction analysis to be
 * used in a kernel context. Useful when, for instance, making sense
 * of the registers indicated by operands.
 */

#include <linux/compiler.h>
#include <linux/bug.h>
#include <linux/err.h>
#include <asm/ptrace.h>

#define INSN_CODE_SEG_ADDR_SZ(params) ((params >> 4) & 0xf)
#define INSN_CODE_SEG_OPND_SZ(params) (params & 0xf)
#define INSN_CODE_SEG_PARAMS(oper_sz, addr_sz) (oper_sz | (addr_sz << 4))

int pt_regs_offset(struct pt_regs *regs, int regno);

bool insn_has_rep_prefix(struct insn *insn);
void __user *insn_get_addr_ref(struct insn *insn, struct pt_regs *regs);
int insn_get_modrm_rm_off(struct insn *insn, struct pt_regs *regs);
int insn_get_modrm_reg_off(struct insn *insn, struct pt_regs *regs);
unsigned long *insn_get_modrm_reg_ptr(struct insn *insn, struct pt_regs *regs);
unsigned long insn_get_seg_base(struct pt_regs *regs, int seg_reg_idx);
int insn_get_code_seg_params(struct pt_regs *regs);
int insn_get_effective_ip(struct pt_regs *regs, unsigned long *ip);
int insn_fetch_from_user(struct pt_regs *regs,
			 unsigned char buf[MAX_INSN_SIZE]);
int insn_fetch_from_user_inatomic(struct pt_regs *regs,
				  unsigned char buf[MAX_INSN_SIZE]);
bool insn_decode_from_regs(struct insn *insn, struct pt_regs *regs,
			   unsigned char buf[MAX_INSN_SIZE], int buf_size);

enum insn_mmio_type {
	INSN_MMIO_DECODE_FAILED,
	INSN_MMIO_WRITE,
	INSN_MMIO_WRITE_IMM,
	INSN_MMIO_READ,
	INSN_MMIO_READ_ZERO_EXTEND,
	INSN_MMIO_READ_SIGN_EXTEND,
	INSN_MMIO_MOVS,
};

enum insn_mmio_type insn_decode_mmio(struct insn *insn, int *bytes);

bool insn_is_nop(struct insn *insn);

#endif /* _ASM_X86_INSN_EVAL_H */

```

`kernel/trace/arch/x86/include/asm/insn.h`:

```h
/* SPDX-License-Identifier: GPL-2.0-or-later */
#ifndef _ASM_X86_INSN_H
#define _ASM_X86_INSN_H
/*
 * x86 instruction analysis
 *
 * Copyright (C) IBM Corporation, 2009
 */

#include <asm/byteorder.h>
/* insn_attr_t is defined in inat.h */
#include "inat.h" /* __ignore_sync_check__ */

#if defined(__BYTE_ORDER) ? __BYTE_ORDER == __LITTLE_ENDIAN : defined(__LITTLE_ENDIAN)

struct insn_field {
	union {
		insn_value_t value;
		insn_byte_t bytes[4];
	};
	/* !0 if we've run insn_get_xxx() for this field */
	unsigned char got;
	unsigned char nbytes;
};

static inline void insn_field_set(struct insn_field *p, insn_value_t v,
				  unsigned char n)
{
	p->value = v;
	p->nbytes = n;
}

static inline void insn_set_byte(struct insn_field *p, unsigned char n,
				 insn_byte_t v)
{
	p->bytes[n] = v;
}

#else

struct insn_field {
	insn_value_t value;
	union {
		insn_value_t little;
		insn_byte_t bytes[4];
	};
	/* !0 if we've run insn_get_xxx() for this field */
	unsigned char got;
	unsigned char nbytes;
};

static inline void insn_field_set(struct insn_field *p, insn_value_t v,
				  unsigned char n)
{
	p->value = v;
	p->little = __cpu_to_le32(v);
	p->nbytes = n;
}

static inline void insn_set_byte(struct insn_field *p, unsigned char n,
				 insn_byte_t v)
{
	p->bytes[n] = v;
	p->value = __le32_to_cpu(p->little);
}
#endif

struct insn {
	struct insn_field prefixes;	/*
					 * Prefixes
					 * prefixes.bytes[3]: last prefix
					 */
	struct insn_field rex_prefix;	/* REX prefix */
	union {
		struct insn_field vex_prefix;	/* VEX prefix */
		struct insn_field xop_prefix;	/* XOP prefix */
	};
	struct insn_field opcode;	/*
					 * opcode.bytes[0]: opcode1
					 * opcode.bytes[1]: opcode2
					 * opcode.bytes[2]: opcode3
					 */
	struct insn_field modrm;
	struct insn_field sib;
	struct insn_field displacement;
	union {
		struct insn_field immediate;
		struct insn_field moffset1;	/* for 64bit MOV */
		struct insn_field immediate1;	/* for 64bit imm or off16/32 */
	};
	union {
		struct insn_field moffset2;	/* for 64bit MOV */
		struct insn_field immediate2;	/* for 64bit imm or seg16 */
	};

	int	emulate_prefix_size;
	insn_attr_t attr;
	unsigned char opnd_bytes;
	unsigned char addr_bytes;
	unsigned char length;
	unsigned char x86_64;

	const insn_byte_t *kaddr;	/* kernel address of insn to analyze */
	const insn_byte_t *end_kaddr;	/* kernel address of last insn in buffer */
	const insn_byte_t *next_byte;
};

#define MAX_INSN_SIZE	15

#define X86_MODRM_MOD(modrm) (((modrm) & 0xc0) >> 6)
#define X86_MODRM_REG(modrm) (((modrm) & 0x38) >> 3)
#define X86_MODRM_RM(modrm) ((modrm) & 0x07)

#define X86_SIB_SCALE(sib) (((sib) & 0xc0) >> 6)
#define X86_SIB_INDEX(sib) (((sib) & 0x38) >> 3)
#define X86_SIB_BASE(sib) ((sib) & 0x07)

#define X86_REX2_M(rex) ((rex) & 0x80)	/* REX2 M0 */
#define X86_REX2_R(rex) ((rex) & 0x40)	/* REX2 R4 */
#define X86_REX2_X(rex) ((rex) & 0x20)	/* REX2 X4 */
#define X86_REX2_B(rex) ((rex) & 0x10)	/* REX2 B4 */

#define X86_REX_W(rex) ((rex) & 8)	/* REX or REX2 W */
#define X86_REX_R(rex) ((rex) & 4)	/* REX or REX2 R3 */
#define X86_REX_X(rex) ((rex) & 2)	/* REX or REX2 X3 */
#define X86_REX_B(rex) ((rex) & 1)	/* REX or REX2 B3 */

/* VEX bit flags  */
#define X86_VEX_W(vex)	((vex) & 0x80)	/* VEX3 Byte2 */
#define X86_VEX_R(vex)	((vex) & 0x80)	/* VEX2/3 Byte1 */
#define X86_VEX_X(vex)	((vex) & 0x40)	/* VEX3 Byte1 */
#define X86_VEX_B(vex)	((vex) & 0x20)	/* VEX3 Byte1 */
#define X86_VEX_L(vex)	((vex) & 0x04)	/* VEX3 Byte2, VEX2 Byte1 */
/* VEX bit fields */
#define X86_EVEX_M(vex)	((vex) & 0x07)		/* EVEX Byte1 */
#define X86_VEX3_M(vex)	((vex) & 0x1f)		/* VEX3 Byte1 */
#define X86_VEX2_M	1			/* VEX2.M always 1 */
#define X86_VEX_V(vex)	(((vex) & 0x78) >> 3)	/* VEX3 Byte2, VEX2 Byte1 */
#define X86_VEX_P(vex)	((vex) & 0x03)		/* VEX3 Byte2, VEX2 Byte1 */
#define X86_VEX_M_MAX	0x1f			/* VEX3.M Maximum value */
/* XOP bit fields */
#define X86_XOP_R(xop)	((xop) & 0x80)	/* XOP Byte2 */
#define X86_XOP_X(xop)	((xop) & 0x40)	/* XOP Byte2 */
#define X86_XOP_B(xop)	((xop) & 0x20)	/* XOP Byte2 */
#define X86_XOP_M(xop)	((xop) & 0x1f)	/* XOP Byte2 */
#define X86_XOP_W(xop)	((xop) & 0x80)	/* XOP Byte3 */
#define X86_XOP_V(xop)	((xop) & 0x78)	/* XOP Byte3 */
#define X86_XOP_L(xop)	((xop) & 0x04)	/* XOP Byte3 */
#define X86_XOP_P(xop)	((xop) & 0x03)	/* XOP Byte3 */
#define X86_XOP_M_MIN	0x08	/* Min of XOP.M */
#define X86_XOP_M_MAX	0x1f	/* Max of XOP.M */

extern void insn_init(struct insn *insn, const void *kaddr, int buf_len, int x86_64);
extern int insn_get_prefixes(struct insn *insn);
extern int insn_get_opcode(struct insn *insn);
extern int insn_get_modrm(struct insn *insn);
extern int insn_get_sib(struct insn *insn);
extern int insn_get_displacement(struct insn *insn);
extern int insn_get_immediate(struct insn *insn);
extern int insn_get_length(struct insn *insn);

enum insn_mode {
	INSN_MODE_32,
	INSN_MODE_64,
	/* Mode is determined by the current kernel build. */
	INSN_MODE_KERN,
	INSN_NUM_MODES,
};

extern int insn_decode(struct insn *insn, const void *kaddr, int buf_len, enum insn_mode m);

#define insn_decode_kernel(_insn, _ptr) insn_decode((_insn), (_ptr), MAX_INSN_SIZE, INSN_MODE_KERN)

/* Attribute will be determined after getting ModRM (for opcode groups) */
static inline void insn_get_attribute(struct insn *insn)
{
	insn_get_modrm(insn);
}

/* Instruction uses RIP-relative addressing */
extern int insn_rip_relative(struct insn *insn);

static inline int insn_is_rex2(struct insn *insn)
{
	if (!insn->prefixes.got)
		insn_get_prefixes(insn);
	return insn->rex_prefix.nbytes == 2;
}

static inline insn_byte_t insn_rex2_m_bit(struct insn *insn)
{
	return X86_REX2_M(insn->rex_prefix.bytes[1]);
}

static inline int insn_is_avx_or_xop(struct insn *insn)
{
	if (!insn->prefixes.got)
		insn_get_prefixes(insn);
	return (insn->vex_prefix.value != 0);
}

static inline int insn_is_evex(struct insn *insn)
{
	if (!insn->prefixes.got)
		insn_get_prefixes(insn);
	return (insn->vex_prefix.nbytes == 4);
}

/* If we already know this is AVX/XOP encoded */
static inline int avx_insn_is_xop(struct insn *insn)
{
	insn_attr_t attr = inat_get_opcode_attribute(insn->vex_prefix.bytes[0]);

	return inat_is_xop_prefix(attr);
}

static inline int insn_is_xop(struct insn *insn)
{
	if (!insn_is_avx_or_xop(insn))
		return 0;

	return avx_insn_is_xop(insn);
}

static inline int insn_has_emulate_prefix(struct insn *insn)
{
	return !!insn->emulate_prefix_size;
}

static inline insn_byte_t insn_vex_m_bits(struct insn *insn)
{
	if (insn->vex_prefix.nbytes == 2)	/* 2 bytes VEX */
		return X86_VEX2_M;
	else if (insn->vex_prefix.nbytes == 3)	/* 3 bytes VEX */
		return X86_VEX3_M(insn->vex_prefix.bytes[1]);
	else					/* EVEX */
		return X86_EVEX_M(insn->vex_prefix.bytes[1]);
}

static inline insn_byte_t insn_vex_p_bits(struct insn *insn)
{
	if (insn->vex_prefix.nbytes == 2)	/* 2 bytes VEX */
		return X86_VEX_P(insn->vex_prefix.bytes[1]);
	else
		return X86_VEX_P(insn->vex_prefix.bytes[2]);
}

static inline insn_byte_t insn_vex_w_bit(struct insn *insn)
{
	if (insn->vex_prefix.nbytes < 3)
		return 0;
	return X86_VEX_W(insn->vex_prefix.bytes[2]);
}

static inline insn_byte_t insn_xop_map_bits(struct insn *insn)
{
	if (insn->xop_prefix.nbytes < 3)	/* XOP is 3 bytes */
		return 0;
	return X86_XOP_M(insn->xop_prefix.bytes[1]);
}

static inline insn_byte_t insn_xop_p_bits(struct insn *insn)
{
	return X86_XOP_P(insn->vex_prefix.bytes[2]);
}

/* Get the last prefix id from last prefix or VEX prefix */
static inline int insn_last_prefix_id(struct insn *insn)
{
	if (insn_is_avx_or_xop(insn)) {
		if (avx_insn_is_xop(insn))
			return insn_xop_p_bits(insn);
		return insn_vex_p_bits(insn);	/* VEX_p is a SIMD prefix id */
	}

	if (insn->prefixes.bytes[3])
		return inat_get_last_prefix_id(insn->prefixes.bytes[3]);

	return 0;
}

/* Offset of each field from kaddr */
static inline int insn_offset_rex_prefix(struct insn *insn)
{
	return insn->prefixes.nbytes;
}
static inline int insn_offset_vex_prefix(struct insn *insn)
{
	return insn_offset_rex_prefix(insn) + insn->rex_prefix.nbytes;
}
static inline int insn_offset_opcode(struct insn *insn)
{
	return insn_offset_vex_prefix(insn) + insn->vex_prefix.nbytes;
}
static inline int insn_offset_modrm(struct insn *insn)
{
	return insn_offset_opcode(insn) + insn->opcode.nbytes;
}
static inline int insn_offset_sib(struct insn *insn)
{
	return insn_offset_modrm(insn) + insn->modrm.nbytes;
}
static inline int insn_offset_displacement(struct insn *insn)
{
	return insn_offset_sib(insn) + insn->sib.nbytes;
}
static inline int insn_offset_immediate(struct insn *insn)
{
	return insn_offset_displacement(insn) + insn->displacement.nbytes;
}

/**
 * for_each_insn_prefix() -- Iterate prefixes in the instruction
 * @insn: Pointer to struct insn.
 * @prefix: Prefix byte.
 *
 * Iterate prefix bytes of given @insn. Each prefix byte is stored in @prefix
 * and the index is stored in @idx (note that this @idx is just for a cursor,
 * do not change it.)
 * Since prefixes.nbytes can be bigger than 4 if some prefixes
 * are repeated, it cannot be used for looping over the prefixes.
 */
#define for_each_insn_prefix(insn, prefix)	\
	for (int idx = 0; idx < ARRAY_SIZE(insn->prefixes.bytes) && (prefix = insn->prefixes.bytes[idx]) != 0; idx++)

#define POP_SS_OPCODE 0x1f
#define MOV_SREG_OPCODE 0x8e

/*
 * Intel SDM Vol.3A 6.8.3 states;
 * "Any single-step trap that would be delivered following the MOV to SS
 * instruction or POP to SS instruction (because EFLAGS.TF is 1) is
 * suppressed."
 * This function returns true if @insn is MOV SS or POP SS. On these
 * instructions, single stepping is suppressed.
 */
static inline int insn_masking_exception(struct insn *insn)
{
	return insn->opcode.bytes[0] == POP_SS_OPCODE ||
		(insn->opcode.bytes[0] == MOV_SREG_OPCODE &&
		 X86_MODRM_REG(insn->modrm.bytes[0]) == 2);
}

#endif /* _ASM_X86_INSN_H */

```

`kernel/trace/arch/x86/lib/inat-tables.c`:

```c
/* x86 opcode map generated from x86-opcode-map.txt */
/* Do not change this code. */

/* Table: one byte opcode */
const insn_attr_t inat_primary_table[INAT_OPCODE_TABLE_SIZE] = {
	[0x00] = INAT_MODRM,
	[0x01] = INAT_MODRM,
	[0x02] = INAT_MODRM,
	[0x03] = INAT_MODRM,
	[0x04] = INAT_MAKE_IMM(INAT_IMM_BYTE),
	[0x05] = INAT_MAKE_IMM(INAT_IMM_VWORD32),
	[0x06] = INAT_INV64,
	[0x07] = INAT_INV64,
	[0x08] = INAT_MODRM,
	[0x09] = INAT_MODRM,
	[0x0a] = INAT_MODRM,
	[0x0b] = INAT_MODRM,
	[0x0c] = INAT_MAKE_IMM(INAT_IMM_BYTE),
	[0x0d] = INAT_MAKE_IMM(INAT_IMM_VWORD32),
	[0x0e] = INAT_INV64,
	[0x0f] = INAT_MAKE_ESCAPE(1),
	[0x10] = INAT_MODRM,
	[0x11] = INAT_MODRM,
	[0x12] = INAT_MODRM,
	[0x13] = INAT_MODRM,
	[0x14] = INAT_MAKE_IMM(INAT_IMM_BYTE),
	[0x15] = INAT_MAKE_IMM(INAT_IMM_VWORD32),
	[0x16] = INAT_INV64,
	[0x17] = INAT_INV64,
	[0x18] = INAT_MODRM,
	[0x19] = INAT_MODRM,
	[0x1a] = INAT_MODRM,
	[0x1b] = INAT_MODRM,
	[0x1c] = INAT_MAKE_IMM(INAT_IMM_BYTE),
	[0x1d] = INAT_MAKE_IMM(INAT_IMM_VWORD32),
	[0x1e] = INAT_INV64,
	[0x1f] = INAT_INV64,
	[0x20] = INAT_MODRM,
	[0x21] = INAT_MODRM,
	[0x22] = INAT_MODRM,
	[0x23] = INAT_MODRM,
	[0x24] = INAT_MAKE_IMM(INAT_IMM_BYTE),
	[0x25] = INAT_MAKE_IMM(INAT_IMM_VWORD32),
	[0x26] = INAT_MAKE_PREFIX(INAT_PFX_ES),
	[0x27] = INAT_INV64,
	[0x28] = INAT_MODRM,
	[0x29] = INAT_MODRM,
	[0x2a] = INAT_MODRM,
	[0x2b] = INAT_MODRM,
	[0x2c] = INAT_MAKE_IMM(INAT_IMM_BYTE),
	[0x2d] = INAT_MAKE_IMM(INAT_IMM_VWORD32),
	[0x2e] = INAT_MAKE_PREFIX(INAT_PFX_CS),
	[0x2f] = INAT_INV64,
	[0x30] = INAT_MODRM,
	[0x31] = INAT_MODRM,
	[0x32] = INAT_MODRM,
	[0x33] = INAT_MODRM,
	[0x34] = INAT_MAKE_IMM(INAT_IMM_BYTE),
	[0x35] = INAT_MAKE_IMM(INAT_IMM_VWORD32),
	[0x36] = INAT_MAKE_PREFIX(INAT_PFX_SS),
	[0x37] = INAT_INV64,
	[0x38] = INAT_MODRM,
	[0x39] = INAT_MODRM,
	[0x3a] = INAT_MODRM,
	[0x3b] = INAT_MODRM,
	[0x3c] = INAT_MAKE_IMM(INAT_IMM_BYTE),
	[0x3d] = INAT_MAKE_IMM(INAT_IMM_VWORD32),
	[0x3e] = INAT_MAKE_PREFIX(INAT_PFX_DS),
	[0x3f] = INAT_INV64,
	[0x40] = INAT_MAKE_PREFIX(INAT_PFX_REX),
	[0x41] = INAT_MAKE_PREFIX(INAT_PFX_REX),
	[0x42] = INAT_MAKE_PREFIX(INAT_PFX_REX),
	[0x43] = INAT_MAKE_PREFIX(INAT_PFX_REX),
	[0x44] = INAT_MAKE_PREFIX(INAT_PFX_REX),
	[0x45] = INAT_MAKE_PREFIX(INAT_PFX_REX),
	[0x46] = INAT_MAKE_PREFIX(INAT_PFX_REX),
	[0x47] = INAT_MAKE_PREFIX(INAT_PFX_REX),
	[0x48] = INAT_MAKE_PREFIX(INAT_PFX_REX),
	[0x49] = INAT_MAKE_PREFIX(INAT_PFX_REX),
	[0x4a] = INAT_MAKE_PREFIX(INAT_PFX_REX),
	[0x4b] = INAT_MAKE_PREFIX(INAT_PFX_REX),
	[0x4c] = INAT_MAKE_PREFIX(INAT_PFX_REX),
	[0x4d] = INAT_MAKE_PREFIX(INAT_PFX_REX),
	[0x4e] = INAT_MAKE_PREFIX(INAT_PFX_REX),
	[0x4f] = INAT_MAKE_PREFIX(INAT_PFX_REX),
	[0x50] = INAT_FORCE64,
	[0x51] = INAT_FORCE64,
	[0x52] = INAT_FORCE64,
	[0x53] = INAT_FORCE64,
	[0x54] = INAT_FORCE64,
	[0x55] = INAT_FORCE64,
	[0x56] = INAT_FORCE64,
	[0x57] = INAT_FORCE64,
	[0x58] = INAT_FORCE64,
	[0x59] = INAT_FORCE64,
	[0x5a] = INAT_FORCE64,
	[0x5b] = INAT_FORCE64,
	[0x5c] = INAT_FORCE64,
	[0x5d] = INAT_FORCE64,
	[0x5e] = INAT_FORCE64,
	[0x5f] = INAT_FORCE64,
	[0x60] = INAT_INV64,
	[0x61] = INAT_INV64,
	[0x62] = INAT_MODRM | INAT_MAKE_PREFIX(INAT_PFX_EVEX),
	[0x63] = INAT_MODRM | INAT_MODRM,
	[0x64] = INAT_MAKE_PREFIX(INAT_PFX_FS),
	[0x65] = INAT_MAKE_PREFIX(INAT_PFX_GS),
	[0x66] = INAT_MAKE_PREFIX(INAT_PFX_OPNDSZ),
	[0x67] = INAT_MAKE_PREFIX(INAT_PFX_ADDRSZ),
	[0x68] = INAT_MAKE_IMM(INAT_IMM_VWORD32),
	[0x69] = INAT_MAKE_IMM(INAT_IMM_VWORD32) | INAT_MODRM,
	[0x6a] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_FORCE64,
	[0x6b] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM,
	[0x70] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_NO_REX2,
	[0x71] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_NO_REX2,
	[0x72] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_NO_REX2,
	[0x73] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_NO_REX2,
	[0x74] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_NO_REX2,
	[0x75] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_NO_REX2,
	[0x76] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_NO_REX2,
	[0x77] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_NO_REX2,
	[0x78] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_NO_REX2,
	[0x79] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_NO_REX2,
	[0x7a] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_NO_REX2,
	[0x7b] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_NO_REX2,
	[0x7c] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_NO_REX2,
	[0x7d] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_NO_REX2,
	[0x7e] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_NO_REX2,
	[0x7f] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_NO_REX2,
	[0x80] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_MAKE_GROUP(1),
	[0x81] = INAT_MAKE_IMM(INAT_IMM_VWORD32) | INAT_MODRM | INAT_MAKE_GROUP(1),
	[0x82] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_MAKE_GROUP(1) | INAT_INV64,
	[0x83] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_MAKE_GROUP(1),
	[0x84] = INAT_MODRM,
	[0x85] = INAT_MODRM,
	[0x86] = INAT_MODRM,
	[0x87] = INAT_MODRM,
	[0x88] = INAT_MODRM,
	[0x89] = INAT_MODRM,
	[0x8a] = INAT_MODRM,
	[0x8b] = INAT_MODRM,
	[0x8c] = INAT_MODRM,
	[0x8d] = INAT_MODRM,
	[0x8e] = INAT_MODRM,
	[0x8f] = INAT_MAKE_GROUP(2) | INAT_MODRM | INAT_FORCE64 | INAT_MAKE_PREFIX(INAT_PFX_XOP),
	[0x9a] = INAT_MAKE_IMM(INAT_IMM_PTR) | INAT_INV64,
	[0x9c] = INAT_FORCE64,
	[0x9d] = INAT_FORCE64,
	[0xa0] = INAT_MOFFSET | INAT_NO_REX2,
	[0xa1] = INAT_MOFFSET | INAT_NO_REX2,
	[0xa2] = INAT_MOFFSET | INAT_NO_REX2,
	[0xa3] = INAT_MOFFSET | INAT_NO_REX2,
	[0xa4] = INAT_NO_REX2,
	[0xa5] = INAT_NO_REX2,
	[0xa6] = INAT_NO_REX2,
	[0xa7] = INAT_NO_REX2,
	[0xa8] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_NO_REX2,
	[0xa9] = INAT_MAKE_IMM(INAT_IMM_VWORD32) | INAT_NO_REX2,
	[0xaa] = INAT_NO_REX2,
	[0xab] = INAT_NO_REX2,
	[0xac] = INAT_NO_REX2,
	[0xad] = INAT_NO_REX2,
	[0xae] = INAT_NO_REX2,
	[0xaf] = INAT_NO_REX2,
	[0xb0] = INAT_MAKE_IMM(INAT_IMM_BYTE),
	[0xb1] = INAT_MAKE_IMM(INAT_IMM_BYTE),
	[0xb2] = INAT_MAKE_IMM(INAT_IMM_BYTE),
	[0xb3] = INAT_MAKE_IMM(INAT_IMM_BYTE),
	[0xb4] = INAT_MAKE_IMM(INAT_IMM_BYTE),
	[0xb5] = INAT_MAKE_IMM(INAT_IMM_BYTE),
	[0xb6] = INAT_MAKE_IMM(INAT_IMM_BYTE),
	[0xb7] = INAT_MAKE_IMM(INAT_IMM_BYTE),
	[0xb8] = INAT_MAKE_IMM(INAT_IMM_VWORD),
	[0xb9] = INAT_MAKE_IMM(INAT_IMM_VWORD),
	[0xba] = INAT_MAKE_IMM(INAT_IMM_VWORD),
	[0xbb] = INAT_MAKE_IMM(INAT_IMM_VWORD),
	[0xbc] = INAT_MAKE_IMM(INAT_IMM_VWORD),
	[0xbd] = INAT_MAKE_IMM(INAT_IMM_VWORD),
	[0xbe] = INAT_MAKE_IMM(INAT_IMM_VWORD),
	[0xbf] = INAT_MAKE_IMM(INAT_IMM_VWORD),
	[0xc0] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_MAKE_GROUP(3),
	[0xc1] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_MAKE_GROUP(3),
	[0xc2] = INAT_MAKE_IMM(INAT_IMM_WORD) | INAT_FORCE64,
	[0xc4] = INAT_MODRM | INAT_MAKE_PREFIX(INAT_PFX_VEX3),
	[0xc5] = INAT_MODRM | INAT_MAKE_PREFIX(INAT_PFX_VEX2),
	[0xc6] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_MAKE_GROUP(4),
	[0xc7] = INAT_MAKE_IMM(INAT_IMM_VWORD32) | INAT_MODRM | INAT_MAKE_GROUP(5),
	[0xc8] = INAT_MAKE_IMM(INAT_IMM_WORD) | INAT_SCNDIMM,
	[0xc9] = INAT_FORCE64,
	[0xca] = INAT_MAKE_IMM(INAT_IMM_WORD),
	[0xcd] = INAT_MAKE_IMM(INAT_IMM_BYTE),
	[0xce] = INAT_INV64,
	[0xd0] = INAT_MODRM | INAT_MAKE_GROUP(3),
	[0xd1] = INAT_MODRM | INAT_MAKE_GROUP(3),
	[0xd2] = INAT_MODRM | INAT_MAKE_GROUP(3),
	[0xd3] = INAT_MODRM | INAT_MAKE_GROUP(3),
	[0xd4] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_INV64,
	[0xd5] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MAKE_PREFIX(INAT_PFX_REX2),
	[0xd8] = INAT_MODRM,
	[0xd9] = INAT_MODRM,
	[0xda] = INAT_MODRM,
	[0xdb] = INAT_MODRM,
	[0xdc] = INAT_MODRM,
	[0xdd] = INAT_MODRM,
	[0xde] = INAT_MODRM,
	[0xdf] = INAT_MODRM,
	[0xe0] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_FORCE64 | INAT_NO_REX2,
	[0xe1] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_FORCE64 | INAT_NO_REX2,
	[0xe2] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_FORCE64 | INAT_NO_REX2,
	[0xe3] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_FORCE64 | INAT_NO_REX2,
	[0xe4] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_NO_REX2,
	[0xe5] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_NO_REX2,
	[0xe6] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_NO_REX2,
	[0xe7] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_NO_REX2,
	[0xe8] = INAT_MAKE_IMM(INAT_IMM_VWORD32) | INAT_FORCE64 | INAT_NO_REX2,
	[0xe9] = INAT_MAKE_IMM(INAT_IMM_VWORD32) | INAT_FORCE64 | INAT_NO_REX2,
	[0xea] = INAT_MAKE_IMM(INAT_IMM_PTR) | INAT_INV64 | INAT_NO_REX2,
	[0xeb] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_FORCE64 | INAT_NO_REX2,
	[0xec] = INAT_NO_REX2,
	[0xed] = INAT_NO_REX2,
	[0xee] = INAT_NO_REX2,
	[0xef] = INAT_NO_REX2,
	[0xf0] = INAT_MAKE_PREFIX(INAT_PFX_LOCK),
	[0xf2] = INAT_MAKE_PREFIX(INAT_PFX_REPNE) | INAT_MAKE_PREFIX(INAT_PFX_REPNE),
	[0xf3] = INAT_MAKE_PREFIX(INAT_PFX_REPE) | INAT_MAKE_PREFIX(INAT_PFX_REPE),
	[0xf6] = INAT_MODRM | INAT_MAKE_GROUP(6),
	[0xf7] = INAT_MODRM | INAT_MAKE_GROUP(7),
	[0xfe] = INAT_MAKE_GROUP(8),
	[0xff] = INAT_MAKE_GROUP(9),
};

/* Table: 2-byte opcode (0x0f) */
const insn_attr_t inat_escape_table_1[INAT_OPCODE_TABLE_SIZE] = {
	[0x00] = INAT_MAKE_GROUP(10),
	[0x01] = INAT_MAKE_GROUP(11),
	[0x02] = INAT_MODRM,
	[0x03] = INAT_MODRM,
	[0x0d] = INAT_MAKE_GROUP(12),
	[0x0f] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM,
	[0x10] = INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
	[0x11] = INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
	[0x12] = INAT_MODRM | INAT_VEXOK | INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
	[0x13] = INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
	[0x14] = INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
	[0x15] = INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
	[0x16] = INAT_MODRM | INAT_VEXOK | INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
	[0x17] = INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
	[0x18] = INAT_MAKE_GROUP(13),
	[0x1a] = INAT_MODRM | INAT_VARIANT,
	[0x1b] = INAT_MODRM | INAT_VARIANT,
	[0x1c] = INAT_MAKE_GROUP(14),
	[0x1e] = INAT_MAKE_GROUP(15),
	[0x1f] = INAT_MODRM,
	[0x20] = INAT_MODRM,
	[0x21] = INAT_MODRM,
	[0x22] = INAT_MODRM,
	[0x23] = INAT_MODRM,
	[0x28] = INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
	[0x29] = INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
	[0x2a] = INAT_MODRM | INAT_VARIANT,
	[0x2b] = INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
	[0x2c] = INAT_MODRM | INAT_VARIANT,
	[0x2d] = INAT_MODRM | INAT_VARIANT,
	[0x2e] = INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
	[0x2f] = INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
	[0x30] = INAT_NO_REX2,
	[0x31] = INAT_NO_REX2,
	[0x32] = INAT_NO_REX2,
	[0x33] = INAT_NO_REX2,
	[0x34] = INAT_NO_REX2,
	[0x35] = INAT_NO_REX2,
	[0x37] = INAT_NO_REX2,
	[0x38] = INAT_MAKE_ESCAPE(2),
	[0x3a] = INAT_MAKE_ESCAPE(3),
	[0x40] = INAT_MODRM,
	[0x41] = INAT_MODRM | INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
	[0x42] = INAT_MODRM | INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
	[0x43] = INAT_MODRM,
	[0x44] = INAT_MODRM | INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
	[0x45] = INAT_MODRM | INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
	[0x46] = INAT_MODRM | INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
	[0x47] = INAT_MODRM | INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
	[0x48] = INAT_MODRM,
	[0x49] = INAT_MODRM,
	[0x4a] = INAT_MODRM | INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
	[0x4b] = INAT_MODRM | INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
	[0x4c] = INAT_MODRM,
	[0x4d] = INAT_MODRM,
	[0x4e] = INAT_MODRM,
	[0x4f] = INAT_MODRM,
	[0x50] = INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
	[0x51] = INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
	[0x52] = INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
	[0x53] = INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
	[0x54] = INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
	[0x55] = INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
	[0x56] = INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
	[0x57] = INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
	[0x58] = INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
	[0x59] = INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
	[0x5a] = INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
	[0x5b] = INAT_MODRM | INAT_VEXOK | INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
	[0x5c] = INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
	[0x5d] = INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
	[0x5e] = INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
	[0x5f] = INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
	[0x60] = INAT_MODRM | INAT_VARIANT,
	[0x61] = INAT_MODRM | INAT_VARIANT,
	[0x62] = INAT_MODRM | INAT_VARIANT,
	[0x63] = INAT_MODRM | INAT_VARIANT,
	[0x64] = INAT_MODRM | INAT_VARIANT,
	[0x65] = INAT_MODRM | INAT_VARIANT,
	[0x66] = INAT_MODRM | INAT_VARIANT,
	[0x67] = INAT_MODRM | INAT_VARIANT,
	[0x68] = INAT_MODRM | INAT_VARIANT,
	[0x69] = INAT_MODRM | INAT_VARIANT,
	[0x6a] = INAT_MODRM | INAT_VARIANT,
	[0x6b] = INAT_MODRM | INAT_VARIANT,
	[0x6c] = INAT_VARIANT,
	[0x6d] = INAT_VARIANT,
	[0x6e] = INAT_MODRM | INAT_VARIANT,
	[0x6f] = INAT_MODRM | INAT_VARIANT,
	[0x70] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VARIANT,
	[0x71] = INAT_MAKE_GROUP(16),
	[0x72] = INAT_MAKE_GROUP(17),
	[0x73] = INAT_MAKE_GROUP(18),
	[0x74] = INAT_MODRM | INAT_VARIANT,
	[0x75] = INAT_MODRM | INAT_VARIANT,
	[0x76] = INAT_MODRM | INAT_VARIANT,
	[0x77] = INAT_VEXOK | INAT_VEXOK,
	[0x78] = INAT_MODRM | INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
	[0x79] = INAT_MODRM | INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
	[0x7a] = INAT_VARIANT,
	[0x7b] = INAT_VARIANT,
	[0x7c] = INAT_VARIANT,
	[0x7d] = INAT_VARIANT,
	[0x7e] = INAT_MODRM | INAT_VARIANT,
	[0x7f] = INAT_MODRM | INAT_VARIANT,
	[0x80] = INAT_MAKE_IMM(INAT_IMM_VWORD32) | INAT_FORCE64 | INAT_NO_REX2,
	[0x81] = INAT_MAKE_IMM(INAT_IMM_VWORD32) | INAT_FORCE64 | INAT_NO_REX2,
	[0x82] = INAT_MAKE_IMM(INAT_IMM_VWORD32) | INAT_FORCE64 | INAT_NO_REX2,
	[0x83] = INAT_MAKE_IMM(INAT_IMM_VWORD32) | INAT_FORCE64 | INAT_NO_REX2,
	[0x84] = INAT_MAKE_IMM(INAT_IMM_VWORD32) | INAT_FORCE64 | INAT_NO_REX2,
	[0x85] = INAT_MAKE_IMM(INAT_IMM_VWORD32) | INAT_FORCE64 | INAT_NO_REX2,
	[0x86] = INAT_MAKE_IMM(INAT_IMM_VWORD32) | INAT_FORCE64 | INAT_NO_REX2,
	[0x87] = INAT_MAKE_IMM(INAT_IMM_VWORD32) | INAT_FORCE64 | INAT_NO_REX2,
	[0x88] = INAT_MAKE_IMM(INAT_IMM_VWORD32) | INAT_FORCE64 | INAT_NO_REX2,
	[0x89] = INAT_MAKE_IMM(INAT_IMM_VWORD32) | INAT_FORCE64 | INAT_NO_REX2,
	[0x8a] = INAT_MAKE_IMM(INAT_IMM_VWORD32) | INAT_FORCE64 | INAT_NO_REX2,
	[0x8b] = INAT_MAKE_IMM(INAT_IMM_VWORD32) | INAT_FORCE64 | INAT_NO_REX2,
	[0x8c] = INAT_MAKE_IMM(INAT_IMM_VWORD32) | INAT_FORCE64 | INAT_NO_REX2,
	[0x8d] = INAT_MAKE_IMM(INAT_IMM_VWORD32) | INAT_FORCE64 | INAT_NO_REX2,
	[0x8e] = INAT_MAKE_IMM(INAT_IMM_VWORD32) | INAT_FORCE64 | INAT_NO_REX2,
	[0x8f] = INAT_MAKE_IMM(INAT_IMM_VWORD32) | INAT_FORCE64 | INAT_NO_REX2,
	[0x90] = INAT_MODRM | INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
	[0x91] = INAT_MODRM | INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
	[0x92] = INAT_MODRM | INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
	[0x93] = INAT_MODRM | INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
	[0x94] = INAT_MODRM,
	[0x95] = INAT_MODRM,
	[0x96] = INAT_MODRM,
	[0x97] = INAT_MODRM,
	[0x98] = INAT_MODRM | INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
	[0x99] = INAT_MODRM | INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
	[0x9a] = INAT_MODRM,
	[0x9b] = INAT_MODRM,
	[0x9c] = INAT_MODRM,
	[0x9d] = INAT_MODRM,
	[0x9e] = INAT_MODRM,
	[0x9f] = INAT_MODRM,
	[0xa0] = INAT_FORCE64,
	[0xa1] = INAT_FORCE64,
	[0xa3] = INAT_MODRM,
	[0xa4] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM,
	[0xa5] = INAT_MODRM,
	[0xa6] = INAT_MAKE_GROUP(19),
	[0xa7] = INAT_MAKE_GROUP(20),
	[0xa8] = INAT_FORCE64,
	[0xa9] = INAT_FORCE64,
	[0xab] = INAT_MODRM,
	[0xac] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM,
	[0xad] = INAT_MODRM,
	[0xae] = INAT_MAKE_GROUP(21),
	[0xaf] = INAT_MODRM,
	[0xb0] = INAT_MODRM,
	[0xb1] = INAT_MODRM,
	[0xb2] = INAT_MODRM,
	[0xb3] = INAT_MODRM,
	[0xb4] = INAT_MODRM,
	[0xb5] = INAT_MODRM,
	[0xb6] = INAT_MODRM,
	[0xb7] = INAT_MODRM,
	[0xb8] = INAT_VARIANT,
	[0xb9] = INAT_MAKE_GROUP(22),
	[0xba] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_MAKE_GROUP(23),
	[0xbb] = INAT_MODRM,
	[0xbc] = INAT_MODRM | INAT_VARIANT,
	[0xbd] = INAT_MODRM | INAT_VARIANT,
	[0xbe] = INAT_MODRM,
	[0xbf] = INAT_MODRM,
	[0xc0] = INAT_MODRM,
	[0xc1] = INAT_MODRM,
	[0xc2] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
	[0xc3] = INAT_MODRM,
	[0xc4] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VARIANT,
	[0xc5] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VARIANT,
	[0xc6] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
	[0xc7] = INAT_MAKE_GROUP(24),
	[0xd0] = INAT_VARIANT,
	[0xd1] = INAT_MODRM | INAT_VARIANT,
	[0xd2] = INAT_MODRM | INAT_VARIANT,
	[0xd3] = INAT_MODRM | INAT_VARIANT,
	[0xd4] = INAT_MODRM | INAT_VARIANT,
	[0xd5] = INAT_MODRM | INAT_VARIANT,
	[0xd6] = INAT_VARIANT,
	[0xd7] = INAT_MODRM | INAT_VARIANT,
	[0xd8] = INAT_MODRM | INAT_VARIANT,
	[0xd9] = INAT_MODRM | INAT_VARIANT,
	[0xda] = INAT_MODRM | INAT_VARIANT,
	[0xdb] = INAT_MODRM | INAT_VARIANT,
	[0xdc] = INAT_MODRM | INAT_VARIANT,
	[0xdd] = INAT_MODRM | INAT_VARIANT,
	[0xde] = INAT_MODRM | INAT_VARIANT,
	[0xdf] = INAT_MODRM | INAT_VARIANT,
	[0xe0] = INAT_MODRM | INAT_VARIANT,
	[0xe1] = INAT_MODRM | INAT_VARIANT,
	[0xe2] = INAT_MODRM | INAT_VARIANT,
	[0xe3] = INAT_MODRM | INAT_VARIANT,
	[0xe4] = INAT_MODRM | INAT_VARIANT,
	[0xe5] = INAT_MODRM | INAT_VARIANT,
	[0xe6] = INAT_VARIANT,
	[0xe7] = INAT_MODRM | INAT_VARIANT,
	[0xe8] = INAT_MODRM | INAT_VARIANT,
	[0xe9] = INAT_MODRM | INAT_VARIANT,
	[0xea] = INAT_MODRM | INAT_VARIANT,
	[0xeb] = INAT_MODRM | INAT_VARIANT,
	[0xec] = INAT_MODRM | INAT_VARIANT,
	[0xed] = INAT_MODRM | INAT_VARIANT,
	[0xee] = INAT_MODRM | INAT_VARIANT,
	[0xef] = INAT_MODRM | INAT_VARIANT,
	[0xf0] = INAT_VARIANT,
	[0xf1] = INAT_MODRM | INAT_VARIANT,
	[0xf2] = INAT_MODRM | INAT_VARIANT,
	[0xf3] = INAT_MODRM | INAT_VARIANT,
	[0xf4] = INAT_MODRM | INAT_VARIANT,
	[0xf5] = INAT_MODRM | INAT_VARIANT,
	[0xf6] = INAT_MODRM | INAT_VARIANT,
	[0xf7] = INAT_MODRM | INAT_VARIANT,
	[0xf8] = INAT_MODRM | INAT_VARIANT,
	[0xf9] = INAT_MODRM | INAT_VARIANT,
	[0xfa] = INAT_MODRM | INAT_VARIANT,
	[0xfb] = INAT_MODRM | INAT_VARIANT,
	[0xfc] = INAT_MODRM | INAT_VARIANT,
	[0xfd] = INAT_MODRM | INAT_VARIANT,
	[0xfe] = INAT_MODRM | INAT_VARIANT,
};
const insn_attr_t inat_escape_table_1_1[INAT_OPCODE_TABLE_SIZE] = {
	[0x10] = INAT_MODRM | INAT_VEXOK,
	[0x11] = INAT_MODRM | INAT_VEXOK,
	[0x12] = INAT_MODRM | INAT_VEXOK,
	[0x13] = INAT_MODRM | INAT_VEXOK,
	[0x14] = INAT_MODRM | INAT_VEXOK,
	[0x15] = INAT_MODRM | INAT_VEXOK,
	[0x16] = INAT_MODRM | INAT_VEXOK,
	[0x17] = INAT_MODRM | INAT_VEXOK,
	[0x1a] = INAT_MODRM,
	[0x1b] = INAT_MODRM,
	[0x28] = INAT_MODRM | INAT_VEXOK,
	[0x29] = INAT_MODRM | INAT_VEXOK,
	[0x2a] = INAT_MODRM,
	[0x2b] = INAT_MODRM | INAT_VEXOK,
	[0x2c] = INAT_MODRM,
	[0x2d] = INAT_MODRM,
	[0x2e] = INAT_MODRM | INAT_VEXOK,
	[0x2f] = INAT_MODRM | INAT_VEXOK,
	[0x41] = INAT_MODRM | INAT_VEXOK,
	[0x42] = INAT_MODRM | INAT_VEXOK,
	[0x44] = INAT_MODRM | INAT_VEXOK,
	[0x45] = INAT_MODRM | INAT_VEXOK,
	[0x46] = INAT_MODRM | INAT_VEXOK,
	[0x47] = INAT_MODRM | INAT_VEXOK,
	[0x4a] = INAT_MODRM | INAT_VEXOK,
	[0x4b] = INAT_MODRM | INAT_VEXOK,
	[0x50] = INAT_MODRM | INAT_VEXOK,
	[0x51] = INAT_MODRM | INAT_VEXOK,
	[0x54] = INAT_MODRM | INAT_VEXOK,
	[0x55] = INAT_MODRM | INAT_VEXOK,
	[0x56] = INAT_MODRM | INAT_VEXOK,
	[0x57] = INAT_MODRM | INAT_VEXOK,
	[0x58] = INAT_MODRM | INAT_VEXOK,
	[0x59] = INAT_MODRM | INAT_VEXOK,
	[0x5a] = INAT_MODRM | INAT_VEXOK,
	[0x5b] = INAT_MODRM | INAT_VEXOK,
	[0x5c] = INAT_MODRM | INAT_VEXOK,
	[0x5d] = INAT_MODRM | INAT_VEXOK,
	[0x5e] = INAT_MODRM | INAT_VEXOK,
	[0x5f] = INAT_MODRM | INAT_VEXOK,
	[0x60] = INAT_MODRM | INAT_VEXOK,
	[0x61] = INAT_MODRM | INAT_VEXOK,
	[0x62] = INAT_MODRM | INAT_VEXOK,
	[0x63] = INAT_MODRM | INAT_VEXOK,
	[0x64] = INAT_MODRM | INAT_VEXOK,
	[0x65] = INAT_MODRM | INAT_VEXOK,
	[0x66] = INAT_MODRM | INAT_VEXOK,
	[0x67] = INAT_MODRM | INAT_VEXOK,
	[0x68] = INAT_MODRM | INAT_VEXOK,
	[0x69] = INAT_MODRM | INAT_VEXOK,
	[0x6a] = INAT_MODRM | INAT_VEXOK,
	[0x6b] = INAT_MODRM | INAT_VEXOK,
	[0x6c] = INAT_MODRM | INAT_VEXOK,
	[0x6d] = INAT_MODRM | INAT_VEXOK,
	[0x6e] = INAT_MODRM | INAT_VEXOK,
	[0x6f] = INAT_MODRM | INAT_VEXOK | INAT_MODRM | INAT_VEXOK,
	[0x70] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
	[0x74] = INAT_MODRM | INAT_VEXOK,
	[0x75] = INAT_MODRM | INAT_VEXOK,
	[0x76] = INAT_MODRM | INAT_VEXOK,
	[0x78] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x79] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x7a] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x7b] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x7c] = INAT_MODRM | INAT_VEXOK,
	[0x7d] = INAT_MODRM | INAT_VEXOK,
	[0x7e] = INAT_MODRM | INAT_VEXOK,
	[0x7f] = INAT_MODRM | INAT_VEXOK | INAT_MODRM | INAT_VEXOK,
	[0x90] = INAT_MODRM | INAT_VEXOK,
	[0x91] = INAT_MODRM | INAT_VEXOK,
	[0x92] = INAT_MODRM | INAT_VEXOK,
	[0x93] = INAT_MODRM | INAT_VEXOK,
	[0x98] = INAT_MODRM | INAT_VEXOK,
	[0x99] = INAT_MODRM | INAT_VEXOK,
	[0xbc] = INAT_MODRM,
	[0xbd] = INAT_MODRM,
	[0xc2] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
	[0xc4] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
	[0xc5] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
	[0xc6] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
	[0xd0] = INAT_MODRM | INAT_VEXOK,
	[0xd1] = INAT_MODRM | INAT_VEXOK,
	[0xd2] = INAT_MODRM | INAT_VEXOK,
	[0xd3] = INAT_MODRM | INAT_VEXOK,
	[0xd4] = INAT_MODRM | INAT_VEXOK,
	[0xd5] = INAT_MODRM | INAT_VEXOK,
	[0xd6] = INAT_MODRM | INAT_VEXOK,
	[0xd7] = INAT_MODRM | INAT_VEXOK,
	[0xd8] = INAT_MODRM | INAT_VEXOK,
	[0xd9] = INAT_MODRM | INAT_VEXOK,
	[0xda] = INAT_MODRM | INAT_VEXOK,
	[0xdb] = INAT_MODRM | INAT_VEXOK | INAT_MODRM | INAT_VEXOK,
	[0xdc] = INAT_MODRM | INAT_VEXOK,
	[0xdd] = INAT_MODRM | INAT_VEXOK,
	[0xde] = INAT_MODRM | INAT_VEXOK,
	[0xdf] = INAT_MODRM | INAT_VEXOK | INAT_MODRM | INAT_VEXOK,
	[0xe0] = INAT_MODRM | INAT_VEXOK,
	[0xe1] = INAT_MODRM | INAT_VEXOK,
	[0xe2] = INAT_MODRM | INAT_VEXOK,
	[0xe3] = INAT_MODRM | INAT_VEXOK,
	[0xe4] = INAT_MODRM | INAT_VEXOK,
	[0xe5] = INAT_MODRM | INAT_VEXOK,
	[0xe6] = INAT_MODRM | INAT_VEXOK,
	[0xe7] = INAT_MODRM | INAT_VEXOK,
	[0xe8] = INAT_MODRM | INAT_VEXOK,
	[0xe9] = INAT_MODRM | INAT_VEXOK,
	[0xea] = INAT_MODRM | INAT_VEXOK,
	[0xeb] = INAT_MODRM | INAT_VEXOK | INAT_MODRM | INAT_VEXOK,
	[0xec] = INAT_MODRM | INAT_VEXOK,
	[0xed] = INAT_MODRM | INAT_VEXOK,
	[0xee] = INAT_MODRM | INAT_VEXOK,
	[0xef] = INAT_MODRM | INAT_VEXOK | INAT_MODRM | INAT_VEXOK,
	[0xf1] = INAT_MODRM | INAT_VEXOK,
	[0xf2] = INAT_MODRM | INAT_VEXOK,
	[0xf3] = INAT_MODRM | INAT_VEXOK,
	[0xf4] = INAT_MODRM | INAT_VEXOK,
	[0xf5] = INAT_MODRM | INAT_VEXOK,
	[0xf6] = INAT_MODRM | INAT_VEXOK,
	[0xf7] = INAT_MODRM | INAT_VEXOK,
	[0xf8] = INAT_MODRM | INAT_VEXOK,
	[0xf9] = INAT_MODRM | INAT_VEXOK,
	[0xfa] = INAT_MODRM | INAT_VEXOK,
	[0xfb] = INAT_MODRM | INAT_VEXOK,
	[0xfc] = INAT_MODRM | INAT_VEXOK,
	[0xfd] = INAT_MODRM | INAT_VEXOK,
	[0xfe] = INAT_MODRM | INAT_VEXOK,
};
const insn_attr_t inat_escape_table_1_2[INAT_OPCODE_TABLE_SIZE] = {
	[0x10] = INAT_MODRM | INAT_VEXOK,
	[0x11] = INAT_MODRM | INAT_VEXOK,
	[0x12] = INAT_MODRM | INAT_VEXOK,
	[0x16] = INAT_MODRM | INAT_VEXOK,
	[0x1a] = INAT_MODRM,
	[0x1b] = INAT_MODRM,
	[0x2a] = INAT_MODRM | INAT_VEXOK,
	[0x2c] = INAT_MODRM | INAT_VEXOK,
	[0x2d] = INAT_MODRM | INAT_VEXOK,
	[0x51] = INAT_MODRM | INAT_VEXOK,
	[0x52] = INAT_MODRM | INAT_VEXOK,
	[0x53] = INAT_MODRM | INAT_VEXOK,
	[0x58] = INAT_MODRM | INAT_VEXOK,
	[0x59] = INAT_MODRM | INAT_VEXOK,
	[0x5a] = INAT_MODRM | INAT_VEXOK,
	[0x5b] = INAT_MODRM | INAT_VEXOK,
	[0x5c] = INAT_MODRM | INAT_VEXOK,
	[0x5d] = INAT_MODRM | INAT_VEXOK,
	[0x5e] = INAT_MODRM | INAT_VEXOK,
	[0x5f] = INAT_MODRM | INAT_VEXOK,
	[0x6f] = INAT_MODRM | INAT_VEXOK | INAT_MODRM | INAT_VEXOK,
	[0x70] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
	[0x78] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x79] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x7a] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x7b] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x7e] = INAT_MODRM | INAT_VEXOK,
	[0x7f] = INAT_MODRM | INAT_VEXOK | INAT_MODRM | INAT_VEXOK,
	[0xb8] = INAT_MODRM,
	[0xbc] = INAT_MODRM,
	[0xbd] = INAT_MODRM,
	[0xc2] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
	[0xd6] = INAT_MODRM,
	[0xe6] = INAT_MODRM | INAT_VEXOK | INAT_MODRM | INAT_VEXOK,
};
const insn_attr_t inat_escape_table_1_3[INAT_OPCODE_TABLE_SIZE] = {
	[0x10] = INAT_MODRM | INAT_VEXOK,
	[0x11] = INAT_MODRM | INAT_VEXOK,
	[0x12] = INAT_MODRM | INAT_VEXOK,
	[0x1a] = INAT_MODRM,
	[0x1b] = INAT_MODRM,
	[0x2a] = INAT_MODRM | INAT_VEXOK,
	[0x2c] = INAT_MODRM | INAT_VEXOK,
	[0x2d] = INAT_MODRM | INAT_VEXOK,
	[0x51] = INAT_MODRM | INAT_VEXOK,
	[0x58] = INAT_MODRM | INAT_VEXOK,
	[0x59] = INAT_MODRM | INAT_VEXOK,
	[0x5a] = INAT_MODRM | INAT_VEXOK,
	[0x5c] = INAT_MODRM | INAT_VEXOK,
	[0x5d] = INAT_MODRM | INAT_VEXOK,
	[0x5e] = INAT_MODRM | INAT_VEXOK,
	[0x5f] = INAT_MODRM | INAT_VEXOK,
	[0x6f] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x70] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
	[0x78] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x79] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x7a] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x7b] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x7c] = INAT_MODRM | INAT_VEXOK,
	[0x7d] = INAT_MODRM | INAT_VEXOK,
	[0x7f] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x92] = INAT_MODRM | INAT_VEXOK,
	[0x93] = INAT_MODRM | INAT_VEXOK,
	[0xbc] = INAT_MODRM,
	[0xbd] = INAT_MODRM,
	[0xc2] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
	[0xd0] = INAT_MODRM | INAT_VEXOK,
	[0xd6] = INAT_MODRM,
	[0xe6] = INAT_MODRM | INAT_VEXOK,
	[0xf0] = INAT_MODRM | INAT_VEXOK,
};

/* Table: 3-byte opcode 1 (0x0f 0x38) */
const insn_attr_t inat_escape_table_2[INAT_OPCODE_TABLE_SIZE] = {
	[0x00] = INAT_MODRM | INAT_VARIANT,
	[0x01] = INAT_MODRM | INAT_VARIANT,
	[0x02] = INAT_MODRM | INAT_VARIANT,
	[0x03] = INAT_MODRM | INAT_VARIANT,
	[0x04] = INAT_MODRM | INAT_VARIANT,
	[0x05] = INAT_MODRM | INAT_VARIANT,
	[0x06] = INAT_MODRM | INAT_VARIANT,
	[0x07] = INAT_MODRM | INAT_VARIANT,
	[0x08] = INAT_MODRM | INAT_VARIANT,
	[0x09] = INAT_MODRM | INAT_VARIANT,
	[0x0a] = INAT_MODRM | INAT_VARIANT,
	[0x0b] = INAT_MODRM | INAT_VARIANT,
	[0x0c] = INAT_VARIANT,
	[0x0d] = INAT_VARIANT,
	[0x0e] = INAT_VARIANT,
	[0x0f] = INAT_VARIANT,
	[0x10] = INAT_VARIANT,
	[0x11] = INAT_VARIANT,
	[0x12] = INAT_VARIANT,
	[0x13] = INAT_VARIANT,
	[0x14] = INAT_VARIANT,
	[0x15] = INAT_VARIANT,
	[0x16] = INAT_VARIANT,
	[0x17] = INAT_VARIANT,
	[0x18] = INAT_VARIANT,
	[0x19] = INAT_VARIANT,
	[0x1a] = INAT_VARIANT,
	[0x1b] = INAT_VARIANT,
	[0x1c] = INAT_MODRM | INAT_VARIANT,
	[0x1d] = INAT_MODRM | INAT_VARIANT,
	[0x1e] = INAT_MODRM | INAT_VARIANT,
	[0x1f] = INAT_VARIANT,
	[0x20] = INAT_VARIANT,
	[0x21] = INAT_VARIANT,
	[0x22] = INAT_VARIANT,
	[0x23] = INAT_VARIANT,
	[0x24] = INAT_VARIANT,
	[0x25] = INAT_VARIANT,
	[0x26] = INAT_VARIANT,
	[0x27] = INAT_VARIANT,
	[0x28] = INAT_VARIANT,
	[0x29] = INAT_VARIANT,
	[0x2a] = INAT_VARIANT,
	[0x2b] = INAT_VARIANT,
	[0x2c] = INAT_VARIANT,
	[0x2d] = INAT_VARIANT,
	[0x2e] = INAT_VARIANT,
	[0x2f] = INAT_VARIANT,
	[0x30] = INAT_VARIANT,
	[0x31] = INAT_VARIANT,
	[0x32] = INAT_VARIANT,
	[0x33] = INAT_VARIANT,
	[0x34] = INAT_VARIANT,
	[0x35] = INAT_VARIANT,
	[0x36] = INAT_VARIANT,
	[0x37] = INAT_VARIANT,
	[0x38] = INAT_VARIANT,
	[0x39] = INAT_VARIANT,
	[0x3a] = INAT_VARIANT,
	[0x3b] = INAT_VARIANT,
	[0x3c] = INAT_VARIANT,
	[0x3d] = INAT_VARIANT,
	[0x3e] = INAT_VARIANT,
	[0x3f] = INAT_VARIANT,
	[0x40] = INAT_VARIANT,
	[0x41] = INAT_VARIANT,
	[0x42] = INAT_VARIANT,
	[0x43] = INAT_VARIANT,
	[0x44] = INAT_VARIANT,
	[0x45] = INAT_VARIANT,
	[0x46] = INAT_VARIANT,
	[0x47] = INAT_VARIANT,
	[0x49] = INAT_VEXOK | INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
	[0x4b] = INAT_VARIANT,
	[0x4c] = INAT_VARIANT,
	[0x4d] = INAT_VARIANT,
	[0x4e] = INAT_VARIANT,
	[0x4f] = INAT_VARIANT,
	[0x50] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY | INAT_VARIANT,
	[0x51] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY | INAT_VARIANT,
	[0x52] = INAT_VARIANT,
	[0x53] = INAT_VARIANT,
	[0x54] = INAT_VARIANT,
	[0x55] = INAT_VARIANT,
	[0x58] = INAT_VARIANT,
	[0x59] = INAT_VARIANT,
	[0x5a] = INAT_VARIANT,
	[0x5b] = INAT_VARIANT,
	[0x5c] = INAT_VARIANT,
	[0x5e] = INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
	[0x62] = INAT_VARIANT,
	[0x63] = INAT_VARIANT,
	[0x64] = INAT_VARIANT,
	[0x65] = INAT_VARIANT,
	[0x66] = INAT_VARIANT,
	[0x68] = INAT_VARIANT,
	[0x6c] = INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
	[0x70] = INAT_VARIANT,
	[0x71] = INAT_VARIANT,
	[0x72] = INAT_VARIANT,
	[0x73] = INAT_VARIANT,
	[0x75] = INAT_VARIANT,
	[0x76] = INAT_VARIANT,
	[0x77] = INAT_VARIANT,
	[0x78] = INAT_VARIANT,
	[0x79] = INAT_VARIANT,
	[0x7a] = INAT_VARIANT,
	[0x7b] = INAT_VARIANT,
	[0x7c] = INAT_VARIANT,
	[0x7d] = INAT_VARIANT,
	[0x7e] = INAT_VARIANT,
	[0x7f] = INAT_VARIANT,
	[0x80] = INAT_VARIANT,
	[0x81] = INAT_VARIANT,
	[0x82] = INAT_VARIANT,
	[0x83] = INAT_VARIANT,
	[0x88] = INAT_VARIANT,
	[0x89] = INAT_VARIANT,
	[0x8a] = INAT_VARIANT,
	[0x8b] = INAT_VARIANT,
	[0x8c] = INAT_VARIANT,
	[0x8d] = INAT_VARIANT,
	[0x8e] = INAT_VARIANT,
	[0x8f] = INAT_VARIANT,
	[0x90] = INAT_VARIANT,
	[0x91] = INAT_VARIANT,
	[0x92] = INAT_VARIANT,
	[0x93] = INAT_VARIANT,
	[0x96] = INAT_VARIANT,
	[0x97] = INAT_VARIANT,
	[0x98] = INAT_VARIANT,
	[0x99] = INAT_VARIANT,
	[0x9a] = INAT_VARIANT,
	[0x9b] = INAT_VARIANT,
	[0x9c] = INAT_VARIANT,
	[0x9d] = INAT_VARIANT,
	[0x9e] = INAT_VARIANT,
	[0x9f] = INAT_VARIANT,
	[0xa0] = INAT_VARIANT,
	[0xa1] = INAT_VARIANT,
	[0xa2] = INAT_VARIANT,
	[0xa3] = INAT_VARIANT,
	[0xa6] = INAT_VARIANT,
	[0xa7] = INAT_VARIANT,
	[0xa8] = INAT_VARIANT,
	[0xa9] = INAT_VARIANT,
	[0xaa] = INAT_VARIANT,
	[0xab] = INAT_VARIANT,
	[0xac] = INAT_VARIANT,
	[0xad] = INAT_VARIANT,
	[0xae] = INAT_VARIANT,
	[0xaf] = INAT_VARIANT,
	[0xb0] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY | INAT_VARIANT,
	[0xb1] = INAT_VARIANT,
	[0xb4] = INAT_VARIANT,
	[0xb5] = INAT_VARIANT,
	[0xb6] = INAT_VARIANT,
	[0xb7] = INAT_VARIANT,
	[0xb8] = INAT_VARIANT,
	[0xb9] = INAT_VARIANT,
	[0xba] = INAT_VARIANT,
	[0xbb] = INAT_VARIANT,
	[0xbc] = INAT_VARIANT,
	[0xbd] = INAT_VARIANT,
	[0xbe] = INAT_VARIANT,
	[0xbf] = INAT_VARIANT,
	[0xc4] = INAT_VARIANT,
	[0xc6] = INAT_MAKE_GROUP(25),
	[0xc7] = INAT_MAKE_GROUP(26),
	[0xc8] = INAT_MODRM | INAT_VARIANT,
	[0xc9] = INAT_MODRM,
	[0xca] = INAT_MODRM | INAT_VARIANT,
	[0xcb] = INAT_MODRM | INAT_VARIANT,
	[0xcc] = INAT_MODRM | INAT_VARIANT,
	[0xcd] = INAT_MODRM | INAT_VARIANT,
	[0xcf] = INAT_VARIANT,
	[0xd2] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY | INAT_VARIANT,
	[0xd3] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY | INAT_VARIANT,
	[0xd8] = INAT_VARIANT,
	[0xda] = INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
	[0xdb] = INAT_VARIANT,
	[0xdc] = INAT_VARIANT,
	[0xdd] = INAT_VARIANT,
	[0xde] = INAT_VARIANT,
	[0xdf] = INAT_VARIANT,
	[0xe0] = INAT_VARIANT,
	[0xe1] = INAT_VARIANT,
	[0xe2] = INAT_VARIANT,
	[0xe3] = INAT_VARIANT,
	[0xe4] = INAT_VARIANT,
	[0xe5] = INAT_VARIANT,
	[0xe6] = INAT_VARIANT,
	[0xe7] = INAT_VARIANT,
	[0xe8] = INAT_VARIANT,
	[0xe9] = INAT_VARIANT,
	[0xea] = INAT_VARIANT,
	[0xeb] = INAT_VARIANT,
	[0xec] = INAT_VARIANT,
	[0xed] = INAT_VARIANT,
	[0xee] = INAT_VARIANT,
	[0xef] = INAT_VARIANT,
	[0xf0] = INAT_MODRM | INAT_MODRM | INAT_VARIANT,
	[0xf1] = INAT_MODRM | INAT_MODRM | INAT_VARIANT,
	[0xf2] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
	[0xf3] = INAT_MAKE_GROUP(27),
	[0xf5] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY | INAT_VARIANT,
	[0xf6] = INAT_MODRM | INAT_VARIANT,
	[0xf7] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY | INAT_VARIANT,
	[0xf8] = INAT_VARIANT,
	[0xf9] = INAT_MODRM,
	[0xfa] = INAT_VARIANT,
	[0xfb] = INAT_VARIANT,
	[0xfc] = INAT_MODRM | INAT_VARIANT,
};
const insn_attr_t inat_escape_table_2_1[INAT_OPCODE_TABLE_SIZE] = {
	[0x00] = INAT_MODRM | INAT_VEXOK,
	[0x01] = INAT_MODRM | INAT_VEXOK,
	[0x02] = INAT_MODRM | INAT_VEXOK,
	[0x03] = INAT_MODRM | INAT_VEXOK,
	[0x04] = INAT_MODRM | INAT_VEXOK,
	[0x05] = INAT_MODRM | INAT_VEXOK,
	[0x06] = INAT_MODRM | INAT_VEXOK,
	[0x07] = INAT_MODRM | INAT_VEXOK,
	[0x08] = INAT_MODRM | INAT_VEXOK,
	[0x09] = INAT_MODRM | INAT_VEXOK,
	[0x0a] = INAT_MODRM | INAT_VEXOK,
	[0x0b] = INAT_MODRM | INAT_VEXOK,
	[0x0c] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
	[0x0d] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
	[0x0e] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
	[0x0f] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
	[0x10] = INAT_MODRM | INAT_MODRM | INAT_VEXOK,
	[0x11] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x12] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x13] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
	[0x14] = INAT_MODRM | INAT_MODRM | INAT_VEXOK,
	[0x15] = INAT_MODRM | INAT_MODRM | INAT_VEXOK,
	[0x16] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY | INAT_MODRM | INAT_VEXOK,
	[0x17] = INAT_MODRM | INAT_VEXOK,
	[0x18] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
	[0x19] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY | INAT_MODRM | INAT_VEXOK,
	[0x1a] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY | INAT_MODRM | INAT_VEXOK,
	[0x1b] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x1c] = INAT_MODRM | INAT_VEXOK,
	[0x1d] = INAT_MODRM | INAT_VEXOK,
	[0x1e] = INAT_MODRM | INAT_VEXOK,
	[0x1f] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x20] = INAT_MODRM | INAT_VEXOK,
	[0x21] = INAT_MODRM | INAT_VEXOK,
	[0x22] = INAT_MODRM | INAT_VEXOK,
	[0x23] = INAT_MODRM | INAT_VEXOK,
	[0x24] = INAT_MODRM | INAT_VEXOK,
	[0x25] = INAT_MODRM | INAT_VEXOK,
	[0x26] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x27] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x28] = INAT_MODRM | INAT_VEXOK,
	[0x29] = INAT_MODRM | INAT_VEXOK,
	[0x2a] = INAT_MODRM | INAT_VEXOK,
	[0x2b] = INAT_MODRM | INAT_VEXOK,
	[0x2c] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY | INAT_MODRM | INAT_VEXOK,
	[0x2d] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY | INAT_MODRM | INAT_VEXOK,
	[0x2e] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
	[0x2f] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
	[0x30] = INAT_MODRM | INAT_VEXOK,
	[0x31] = INAT_MODRM | INAT_VEXOK,
	[0x32] = INAT_MODRM | INAT_VEXOK,
	[0x33] = INAT_MODRM | INAT_VEXOK,
	[0x34] = INAT_MODRM | INAT_VEXOK,
	[0x35] = INAT_MODRM | INAT_VEXOK,
	[0x36] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY | INAT_MODRM | INAT_VEXOK,
	[0x37] = INAT_MODRM | INAT_VEXOK,
	[0x38] = INAT_MODRM | INAT_VEXOK,
	[0x39] = INAT_MODRM | INAT_VEXOK | INAT_MODRM | INAT_VEXOK,
	[0x3a] = INAT_MODRM | INAT_VEXOK,
	[0x3b] = INAT_MODRM | INAT_VEXOK | INAT_MODRM | INAT_VEXOK,
	[0x3c] = INAT_MODRM | INAT_VEXOK,
	[0x3d] = INAT_MODRM | INAT_VEXOK | INAT_MODRM | INAT_VEXOK,
	[0x3e] = INAT_MODRM | INAT_VEXOK,
	[0x3f] = INAT_MODRM | INAT_VEXOK | INAT_MODRM | INAT_VEXOK,
	[0x40] = INAT_MODRM | INAT_VEXOK | INAT_MODRM | INAT_VEXOK,
	[0x41] = INAT_MODRM | INAT_VEXOK,
	[0x42] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x43] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x44] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x45] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
	[0x46] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY | INAT_MODRM | INAT_VEXOK,
	[0x47] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
	[0x49] = INAT_MODRM | INAT_VEXOK,
	[0x4b] = INAT_MODRM | INAT_VEXOK,
	[0x4c] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x4d] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x4e] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x4f] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x50] = INAT_MODRM | INAT_VEXOK,
	[0x51] = INAT_MODRM | INAT_VEXOK,
	[0x52] = INAT_MODRM | INAT_VEXOK,
	[0x53] = INAT_MODRM | INAT_VEXOK,
	[0x54] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x55] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x58] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
	[0x59] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY | INAT_MODRM | INAT_VEXOK,
	[0x5a] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY | INAT_MODRM | INAT_VEXOK,
	[0x5b] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x5e] = INAT_MODRM | INAT_VEXOK,
	[0x62] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x63] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x64] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x65] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x66] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x6c] = INAT_MODRM | INAT_VEXOK,
	[0x70] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x71] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x72] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x73] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x75] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x76] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x77] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x78] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
	[0x79] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
	[0x7a] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x7b] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x7c] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x7d] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x7e] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x7f] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x80] = INAT_MODRM,
	[0x81] = INAT_MODRM,
	[0x82] = INAT_MODRM,
	[0x83] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x88] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x89] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x8a] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x8b] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x8c] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
	[0x8d] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x8e] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
	[0x8f] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x90] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY | INAT_MODRM | INAT_VEXOK,
	[0x91] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY | INAT_MODRM | INAT_VEXOK,
	[0x92] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
	[0x93] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
	[0x96] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
	[0x97] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
	[0x98] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
	[0x99] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
	[0x9a] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
	[0x9b] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
	[0x9c] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
	[0x9d] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
	[0x9e] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
	[0x9f] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
	[0xa0] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0xa1] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0xa2] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0xa3] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0xa6] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
	[0xa7] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
	[0xa8] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
	[0xa9] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
	[0xaa] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
	[0xab] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
	[0xac] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
	[0xad] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
	[0xae] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
	[0xaf] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
	[0xb0] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
	[0xb1] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
	[0xb4] = INAT_MODRM | INAT_VEXOK,
	[0xb5] = INAT_MODRM | INAT_VEXOK,
	[0xb6] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
	[0xb7] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
	[0xb8] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
	[0xb9] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
	[0xba] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
	[0xbb] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
	[0xbc] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
	[0xbd] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
	[0xbe] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
	[0xbf] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
	[0xc4] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0xc8] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0xca] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0xcb] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0xcc] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0xcd] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0xcf] = INAT_MODRM | INAT_VEXOK,
	[0xd2] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
	[0xd3] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
	[0xda] = INAT_MODRM | INAT_VEXOK,
	[0xdb] = INAT_MODRM | INAT_VEXOK,
	[0xdc] = INAT_MODRM | INAT_VEXOK,
	[0xdd] = INAT_MODRM | INAT_VEXOK,
	[0xde] = INAT_MODRM | INAT_VEXOK,
	[0xdf] = INAT_MODRM | INAT_VEXOK,
	[0xe0] = INAT_MODRM | INAT_VEXOK,
	[0xe1] = INAT_MODRM | INAT_VEXOK,
	[0xe2] = INAT_MODRM | INAT_VEXOK,
	[0xe3] = INAT_MODRM | INAT_VEXOK,
	[0xe4] = INAT_MODRM | INAT_VEXOK,
	[0xe5] = INAT_MODRM | INAT_VEXOK,
	[0xe6] = INAT_MODRM | INAT_VEXOK,
	[0xe7] = INAT_MODRM | INAT_VEXOK,
	[0xe8] = INAT_MODRM | INAT_VEXOK,
	[0xe9] = INAT_MODRM | INAT_VEXOK,
	[0xea] = INAT_MODRM | INAT_VEXOK,
	[0xeb] = INAT_MODRM | INAT_VEXOK,
	[0xec] = INAT_MODRM | INAT_VEXOK,
	[0xed] = INAT_MODRM | INAT_VEXOK,
	[0xee] = INAT_MODRM | INAT_VEXOK,
	[0xef] = INAT_MODRM | INAT_VEXOK,
	[0xf0] = INAT_MODRM,
	[0xf1] = INAT_MODRM,
	[0xf5] = INAT_MODRM,
	[0xf6] = INAT_MODRM,
	[0xf7] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
	[0xf8] = INAT_MODRM,
	[0xfc] = INAT_MODRM,
};
const insn_attr_t inat_escape_table_2_2[INAT_OPCODE_TABLE_SIZE] = {
	[0x10] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x11] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x12] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x13] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x14] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x15] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x20] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x21] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x22] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x23] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x24] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x25] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x26] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x27] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x28] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x29] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x2a] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x30] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x31] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x32] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x33] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x34] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x35] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x38] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x39] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x3a] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x4b] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
	[0x50] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
	[0x51] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
	[0x52] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x5c] = INAT_MODRM | INAT_VEXOK,
	[0x5e] = INAT_MODRM | INAT_VEXOK,
	[0x72] = INAT_MODRM | INAT_VEXOK,
	[0xb0] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
	[0xb1] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
	[0xd2] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
	[0xd3] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
	[0xd8] = INAT_MODRM | INAT_MODRM | INAT_MODRM | INAT_MODRM,
	[0xda] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
	[0xdc] = INAT_MODRM | INAT_MODRM,
	[0xdd] = INAT_MODRM,
	[0xde] = INAT_MODRM,
	[0xdf] = INAT_MODRM,
	[0xf5] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
	[0xf6] = INAT_MODRM,
	[0xf7] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
	[0xf8] = INAT_MODRM | INAT_MODRM,
	[0xfa] = INAT_MODRM,
	[0xfb] = INAT_MODRM,
	[0xfc] = INAT_MODRM,
};
const insn_attr_t inat_escape_table_2_3[INAT_OPCODE_TABLE_SIZE] = {
	[0x49] = INAT_MODRM | INAT_VEXOK,
	[0x4b] = INAT_MODRM | INAT_VEXOK,
	[0x50] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
	[0x51] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
	[0x52] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x53] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x5c] = INAT_MODRM | INAT_VEXOK,
	[0x5e] = INAT_MODRM | INAT_VEXOK,
	[0x68] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x72] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x9a] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x9b] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0xaa] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0xab] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0xb0] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
	[0xcb] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
	[0xcc] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
	[0xcd] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
	[0xda] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
	[0xf0] = INAT_MODRM | INAT_MODRM,
	[0xf1] = INAT_MODRM | INAT_MODRM,
	[0xf5] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
	[0xf6] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
	[0xf7] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
	[0xf8] = INAT_MODRM | INAT_MODRM,
	[0xfc] = INAT_MODRM,
};

/* Table: 3-byte opcode 2 (0x0f 0x3a) */
const insn_attr_t inat_escape_table_3[INAT_OPCODE_TABLE_SIZE] = {
	[0x00] = INAT_VARIANT,
	[0x01] = INAT_VARIANT,
	[0x02] = INAT_VARIANT,
	[0x03] = INAT_VARIANT,
	[0x04] = INAT_VARIANT,
	[0x05] = INAT_VARIANT,
	[0x06] = INAT_VARIANT,
	[0x08] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
	[0x09] = INAT_VARIANT,
	[0x0a] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
	[0x0b] = INAT_VARIANT,
	[0x0c] = INAT_VARIANT,
	[0x0d] = INAT_VARIANT,
	[0x0e] = INAT_VARIANT,
	[0x0f] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VARIANT,
	[0x14] = INAT_VARIANT,
	[0x15] = INAT_VARIANT,
	[0x16] = INAT_VARIANT,
	[0x17] = INAT_VARIANT,
	[0x18] = INAT_VARIANT,
	[0x19] = INAT_VARIANT,
	[0x1a] = INAT_VARIANT,
	[0x1b] = INAT_VARIANT,
	[0x1d] = INAT_VARIANT,
	[0x1e] = INAT_VARIANT,
	[0x1f] = INAT_VARIANT,
	[0x20] = INAT_VARIANT,
	[0x21] = INAT_VARIANT,
	[0x22] = INAT_VARIANT,
	[0x23] = INAT_VARIANT,
	[0x25] = INAT_VARIANT,
	[0x26] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_VARIANT,
	[0x27] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_VARIANT,
	[0x30] = INAT_VARIANT,
	[0x31] = INAT_VARIANT,
	[0x32] = INAT_VARIANT,
	[0x33] = INAT_VARIANT,
	[0x38] = INAT_VARIANT,
	[0x39] = INAT_VARIANT,
	[0x3a] = INAT_VARIANT,
	[0x3b] = INAT_VARIANT,
	[0x3e] = INAT_VARIANT,
	[0x3f] = INAT_VARIANT,
	[0x40] = INAT_VARIANT,
	[0x41] = INAT_VARIANT,
	[0x42] = INAT_VARIANT,
	[0x43] = INAT_VARIANT,
	[0x44] = INAT_VARIANT,
	[0x46] = INAT_VARIANT,
	[0x4a] = INAT_VARIANT,
	[0x4b] = INAT_VARIANT,
	[0x4c] = INAT_VARIANT,
	[0x50] = INAT_VARIANT,
	[0x51] = INAT_VARIANT,
	[0x54] = INAT_VARIANT,
	[0x55] = INAT_VARIANT,
	[0x56] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_VARIANT,
	[0x57] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_VARIANT,
	[0x60] = INAT_VARIANT,
	[0x61] = INAT_VARIANT,
	[0x62] = INAT_VARIANT,
	[0x63] = INAT_VARIANT,
	[0x66] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_VARIANT,
	[0x67] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_VARIANT,
	[0x70] = INAT_VARIANT,
	[0x71] = INAT_VARIANT,
	[0x72] = INAT_VARIANT,
	[0x73] = INAT_VARIANT,
	[0xc2] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_VARIANT,
	[0xcc] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM,
	[0xce] = INAT_VARIANT,
	[0xcf] = INAT_VARIANT,
	[0xde] = INAT_VARIANT,
	[0xdf] = INAT_VARIANT,
	[0xf0] = INAT_VARIANT,
};
const insn_attr_t inat_escape_table_3_1[INAT_OPCODE_TABLE_SIZE] = {
	[0x00] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
	[0x01] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
	[0x02] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
	[0x03] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x04] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
	[0x05] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
	[0x06] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
	[0x08] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
	[0x09] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
	[0x0a] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
	[0x0b] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
	[0x0c] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
	[0x0d] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
	[0x0e] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
	[0x0f] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
	[0x14] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
	[0x15] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
	[0x16] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
	[0x17] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
	[0x18] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_VEXONLY | INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
	[0x19] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_VEXONLY | INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
	[0x1a] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x1b] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x1d] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
	[0x1e] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x1f] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x20] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
	[0x21] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
	[0x22] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
	[0x23] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x25] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x26] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x27] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x30] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
	[0x31] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
	[0x32] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
	[0x33] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
	[0x38] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_VEXONLY | INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
	[0x39] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_VEXONLY | INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
	[0x3a] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x3b] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x3e] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x3f] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x40] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
	[0x41] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
	[0x42] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
	[0x43] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x44] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
	[0x46] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
	[0x4a] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
	[0x4b] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
	[0x4c] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
	[0x50] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x51] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x54] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x55] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x56] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x57] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x60] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
	[0x61] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
	[0x62] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
	[0x63] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
	[0x66] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x67] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x70] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x71] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x72] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x73] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0xce] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
	[0xcf] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
	[0xde] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
	[0xdf] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
};
const insn_attr_t inat_escape_table_3_2[INAT_OPCODE_TABLE_SIZE] = {
	[0xc2] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0xf0] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM,
};
const insn_attr_t inat_escape_table_3_3[INAT_OPCODE_TABLE_SIZE] = {
	[0xf0] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
};

/* Table: EVEX map 4 */
const insn_attr_t inat_avx_table_4[INAT_OPCODE_TABLE_SIZE] = {
	[0x00] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x01] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE | INAT_VARIANT,
	[0x02] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x03] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE | INAT_VARIANT,
	[0x08] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x09] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE | INAT_VARIANT,
	[0x0a] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x0b] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE | INAT_VARIANT,
	[0x10] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x11] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE | INAT_VARIANT,
	[0x12] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x13] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE | INAT_VARIANT,
	[0x18] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x19] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE | INAT_VARIANT,
	[0x1a] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x1b] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE | INAT_VARIANT,
	[0x20] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x21] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE | INAT_VARIANT,
	[0x22] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x23] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE | INAT_VARIANT,
	[0x24] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE | INAT_VARIANT,
	[0x28] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x29] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE | INAT_VARIANT,
	[0x2a] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x2b] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE | INAT_VARIANT,
	[0x2c] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE | INAT_VARIANT,
	[0x30] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x31] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE | INAT_VARIANT,
	[0x32] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x33] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE | INAT_VARIANT,
	[0x38] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x39] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE | INAT_VARIANT,
	[0x3a] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x3b] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE | INAT_VARIANT,
	[0x40] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE | INAT_VARIANT,
	[0x41] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE | INAT_VARIANT,
	[0x42] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE | INAT_VARIANT,
	[0x43] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE | INAT_VARIANT,
	[0x44] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE | INAT_VARIANT,
	[0x45] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE | INAT_VARIANT,
	[0x46] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE | INAT_VARIANT,
	[0x47] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE | INAT_VARIANT,
	[0x48] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE | INAT_VARIANT,
	[0x49] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE | INAT_VARIANT,
	[0x4a] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE | INAT_VARIANT,
	[0x4b] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE | INAT_VARIANT,
	[0x4c] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE | INAT_VARIANT,
	[0x4d] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE | INAT_VARIANT,
	[0x4e] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE | INAT_VARIANT,
	[0x4f] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE | INAT_VARIANT,
	[0x60] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE | INAT_VARIANT,
	[0x61] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE | INAT_VARIANT,
	[0x65] = INAT_VARIANT,
	[0x66] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_VARIANT,
	[0x69] = INAT_MAKE_IMM(INAT_IMM_VWORD32) | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE | INAT_VARIANT,
	[0x6b] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE | INAT_VARIANT,
	[0x80] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_MAKE_GROUP(1) | INAT_VEXOK | INAT_EVEXONLY,
	[0x81] = INAT_MAKE_IMM(INAT_IMM_VWORD32) | INAT_MODRM | INAT_MAKE_GROUP(1) | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE,
	[0x83] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_MAKE_GROUP(1) | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE,
	[0x84] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x85] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE | INAT_VARIANT,
	[0x88] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE | INAT_VARIANT,
	[0x8f] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0xa5] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE | INAT_VARIANT,
	[0xad] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE | INAT_VARIANT,
	[0xaf] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE | INAT_VARIANT,
	[0xc0] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_MAKE_GROUP(3) | INAT_VEXOK | INAT_EVEXONLY,
	[0xc1] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_MAKE_GROUP(3) | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE,
	[0xd0] = INAT_MODRM | INAT_MAKE_GROUP(3) | INAT_VEXOK | INAT_EVEXONLY,
	[0xd1] = INAT_MODRM | INAT_MAKE_GROUP(3) | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE,
	[0xd2] = INAT_MODRM | INAT_MAKE_GROUP(3) | INAT_VEXOK | INAT_EVEXONLY,
	[0xd3] = INAT_MODRM | INAT_MAKE_GROUP(3) | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE,
	[0xf0] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE | INAT_VARIANT,
	[0xf1] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE | INAT_VARIANT,
	[0xf2] = INAT_VARIANT,
	[0xf4] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE | INAT_VARIANT,
	[0xf5] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE | INAT_VARIANT,
	[0xf6] = INAT_MODRM | INAT_MAKE_GROUP(6) | INAT_VEXOK | INAT_EVEXONLY,
	[0xf7] = INAT_MODRM | INAT_MAKE_GROUP(7) | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE,
	[0xf8] = INAT_VARIANT,
	[0xf9] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0xfe] = INAT_MAKE_GROUP(8) | INAT_VEXOK | INAT_EVEXONLY,
	[0xff] = INAT_MAKE_GROUP(9) | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
};
const insn_attr_t inat_avx_table_4_1[INAT_OPCODE_TABLE_SIZE] = {
	[0x01] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE,
	[0x03] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE,
	[0x09] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE,
	[0x0b] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE,
	[0x11] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE,
	[0x13] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE,
	[0x19] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE,
	[0x1b] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE,
	[0x21] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE,
	[0x23] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE,
	[0x24] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE,
	[0x29] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE,
	[0x2b] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE,
	[0x2c] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE,
	[0x31] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE,
	[0x33] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE,
	[0x39] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE,
	[0x3b] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE,
	[0x40] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE,
	[0x41] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE,
	[0x42] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE,
	[0x43] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE,
	[0x44] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE,
	[0x45] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE,
	[0x46] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE,
	[0x47] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE,
	[0x48] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE,
	[0x49] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE,
	[0x4a] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE,
	[0x4b] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE,
	[0x4c] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE,
	[0x4d] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE,
	[0x4e] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE,
	[0x4f] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE,
	[0x60] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE,
	[0x61] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE,
	[0x65] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x66] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x69] = INAT_MAKE_IMM(INAT_IMM_VWORD32) | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE,
	[0x6b] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE,
	[0x85] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE,
	[0x88] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE,
	[0xa5] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE,
	[0xad] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE,
	[0xaf] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE,
	[0xf1] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE,
	[0xf4] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE,
	[0xf5] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_EVEX_SCALABLE,
	[0xf8] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
};
const insn_attr_t inat_avx_table_4_2[INAT_OPCODE_TABLE_SIZE] = {
	[0x66] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0xf0] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0xf1] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0xf2] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0xf8] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
};
const insn_attr_t inat_avx_table_4_3[INAT_OPCODE_TABLE_SIZE] = {
	[0x40] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x41] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x42] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x43] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x44] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x45] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x46] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x47] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x48] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x49] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x4a] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x4b] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x4c] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x4d] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x4e] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x4f] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0xf8] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
};

/* Table: EVEX map 5 */
const insn_attr_t inat_avx_table_5[INAT_OPCODE_TABLE_SIZE] = {
	[0x10] = INAT_VARIANT,
	[0x11] = INAT_VARIANT,
	[0x1d] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_VARIANT,
	[0x2a] = INAT_VARIANT,
	[0x2c] = INAT_VARIANT,
	[0x2d] = INAT_VARIANT,
	[0x2e] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x2f] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x51] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_VARIANT,
	[0x58] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_VARIANT,
	[0x59] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_VARIANT,
	[0x5a] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_VARIANT,
	[0x5b] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_VARIANT,
	[0x5c] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_VARIANT,
	[0x5d] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_VARIANT,
	[0x5e] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_VARIANT,
	[0x5f] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_VARIANT,
	[0x6e] = INAT_VARIANT,
	[0x78] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_VARIANT,
	[0x79] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_VARIANT,
	[0x7a] = INAT_VARIANT,
	[0x7b] = INAT_VARIANT,
	[0x7c] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_VARIANT,
	[0x7d] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_VARIANT,
	[0x7e] = INAT_VARIANT,
};
const insn_attr_t inat_avx_table_5_1[INAT_OPCODE_TABLE_SIZE] = {
	[0x1d] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x5a] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x5b] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x6e] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x78] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x79] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x7a] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x7b] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x7c] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x7d] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x7e] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
};
const insn_attr_t inat_avx_table_5_2[INAT_OPCODE_TABLE_SIZE] = {
	[0x10] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x11] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x2a] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x2c] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x2d] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x51] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x58] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x59] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x5a] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x5b] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x5c] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x5d] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x5e] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x5f] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x78] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x79] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x7b] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x7d] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
};
const insn_attr_t inat_avx_table_5_3[INAT_OPCODE_TABLE_SIZE] = {
	[0x5a] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x7a] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x7d] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
};

/* Table: EVEX map 6 */
const insn_attr_t inat_avx_table_6[INAT_OPCODE_TABLE_SIZE] = {
	[0x13] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY | INAT_VARIANT,
	[0x2c] = INAT_VARIANT,
	[0x2d] = INAT_VARIANT,
	[0x42] = INAT_VARIANT,
	[0x43] = INAT_VARIANT,
	[0x4c] = INAT_VARIANT,
	[0x4d] = INAT_VARIANT,
	[0x4e] = INAT_VARIANT,
	[0x4f] = INAT_VARIANT,
	[0x56] = INAT_VARIANT,
	[0x57] = INAT_VARIANT,
	[0x96] = INAT_VARIANT,
	[0x97] = INAT_VARIANT,
	[0x98] = INAT_VARIANT,
	[0x99] = INAT_VARIANT,
	[0x9a] = INAT_VARIANT,
	[0x9b] = INAT_VARIANT,
	[0x9c] = INAT_VARIANT,
	[0x9d] = INAT_VARIANT,
	[0x9e] = INAT_VARIANT,
	[0x9f] = INAT_VARIANT,
	[0xa6] = INAT_VARIANT,
	[0xa7] = INAT_VARIANT,
	[0xa8] = INAT_VARIANT,
	[0xa9] = INAT_VARIANT,
	[0xaa] = INAT_VARIANT,
	[0xab] = INAT_VARIANT,
	[0xac] = INAT_VARIANT,
	[0xad] = INAT_VARIANT,
	[0xae] = INAT_VARIANT,
	[0xaf] = INAT_VARIANT,
	[0xb6] = INAT_VARIANT,
	[0xb7] = INAT_VARIANT,
	[0xb8] = INAT_VARIANT,
	[0xb9] = INAT_VARIANT,
	[0xba] = INAT_VARIANT,
	[0xbb] = INAT_VARIANT,
	[0xbc] = INAT_VARIANT,
	[0xbd] = INAT_VARIANT,
	[0xbe] = INAT_VARIANT,
	[0xbf] = INAT_VARIANT,
	[0xd6] = INAT_VARIANT,
	[0xd7] = INAT_VARIANT,
};
const insn_attr_t inat_avx_table_6_1[INAT_OPCODE_TABLE_SIZE] = {
	[0x13] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x2c] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x2d] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x42] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x43] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x4c] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x4d] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x4e] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x4f] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x96] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x97] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x98] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x99] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x9a] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x9b] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x9c] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x9d] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x9e] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x9f] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0xa6] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0xa7] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0xa8] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0xa9] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0xaa] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0xab] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0xac] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0xad] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0xae] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0xaf] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0xb6] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0xb7] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0xb8] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0xb9] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0xba] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0xbb] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0xbc] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0xbd] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0xbe] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0xbf] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
};
const insn_attr_t inat_avx_table_6_2[INAT_OPCODE_TABLE_SIZE] = {
	[0x56] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x57] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0xd6] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0xd7] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
};
const insn_attr_t inat_avx_table_6_3[INAT_OPCODE_TABLE_SIZE] = {
	[0x56] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x57] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0xd6] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0xd7] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
};

/* Table: VEX map 7 */
const insn_attr_t inat_avx_table_7[INAT_OPCODE_TABLE_SIZE] = {
	[0xf8] = INAT_VARIANT,
};
const insn_attr_t inat_avx_table_7_2[INAT_OPCODE_TABLE_SIZE] = {
	[0xf8] = INAT_MAKE_IMM(INAT_IMM_DWORD) | INAT_MODRM | INAT_VEXOK,
};
const insn_attr_t inat_avx_table_7_3[INAT_OPCODE_TABLE_SIZE] = {
	[0xf8] = INAT_MAKE_IMM(INAT_IMM_DWORD) | INAT_MODRM | INAT_VEXOK,
};

/* Table: XOP map 8h */
const insn_attr_t inat_xop_table_0[INAT_OPCODE_TABLE_SIZE] = {
	[0x85] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_XOPOK,
	[0x86] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_XOPOK,
	[0x87] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_XOPOK,
	[0x8e] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_XOPOK,
	[0x8f] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_XOPOK,
	[0x95] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_XOPOK,
	[0x96] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_XOPOK,
	[0x97] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_XOPOK,
	[0x9e] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_XOPOK,
	[0x9f] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_XOPOK,
	[0xa2] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_XOPOK | INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_XOPOK,
	[0xa3] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_XOPOK | INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_XOPOK,
	[0xa6] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_XOPOK,
	[0xb6] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_XOPOK,
	[0xc0] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_XOPOK,
	[0xc1] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_XOPOK,
	[0xc2] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_XOPOK,
	[0xc3] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_XOPOK,
	[0xcc] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_XOPOK,
	[0xcd] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_XOPOK,
	[0xce] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_XOPOK,
	[0xcf] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_XOPOK,
	[0xec] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_XOPOK,
	[0xed] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_XOPOK,
	[0xee] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_XOPOK,
	[0xef] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_XOPOK,
};

/* Table: XOP map 9h */
const insn_attr_t inat_xop_table_1[INAT_OPCODE_TABLE_SIZE] = {
	[0x01] = INAT_MAKE_GROUP(28) | INAT_XOPOK,
	[0x02] = INAT_MAKE_GROUP(29) | INAT_XOPOK,
	[0x12] = INAT_MAKE_GROUP(30) | INAT_XOPOK,
	[0x80] = INAT_MODRM | INAT_XOPOK,
	[0x81] = INAT_MODRM | INAT_XOPOK,
	[0x82] = INAT_MODRM | INAT_XOPOK,
	[0x83] = INAT_MODRM | INAT_XOPOK,
	[0x90] = INAT_MODRM | INAT_XOPOK | INAT_MODRM | INAT_XOPOK,
	[0x91] = INAT_MODRM | INAT_XOPOK | INAT_MODRM | INAT_XOPOK,
	[0x92] = INAT_MODRM | INAT_XOPOK | INAT_MODRM | INAT_XOPOK,
	[0x93] = INAT_MODRM | INAT_XOPOK | INAT_MODRM | INAT_XOPOK,
	[0x94] = INAT_MODRM | INAT_XOPOK | INAT_MODRM | INAT_XOPOK,
	[0x95] = INAT_MODRM | INAT_XOPOK | INAT_MODRM | INAT_XOPOK,
	[0x96] = INAT_MODRM | INAT_XOPOK | INAT_MODRM | INAT_XOPOK,
	[0x97] = INAT_MODRM | INAT_XOPOK | INAT_MODRM | INAT_XOPOK,
	[0x98] = INAT_MODRM | INAT_XOPOK | INAT_MODRM | INAT_XOPOK,
	[0x99] = INAT_MODRM | INAT_XOPOK | INAT_MODRM | INAT_XOPOK,
	[0x9a] = INAT_MODRM | INAT_XOPOK | INAT_MODRM | INAT_XOPOK,
	[0x9b] = INAT_MODRM | INAT_XOPOK | INAT_MODRM | INAT_XOPOK,
	[0xc1] = INAT_MODRM | INAT_XOPOK,
	[0xc2] = INAT_MODRM | INAT_XOPOK,
	[0xc3] = INAT_MODRM | INAT_XOPOK,
	[0xc6] = INAT_MODRM | INAT_XOPOK,
	[0xc7] = INAT_MODRM | INAT_XOPOK,
	[0xcb] = INAT_MODRM | INAT_XOPOK,
	[0xd1] = INAT_MODRM | INAT_XOPOK,
	[0xd2] = INAT_MODRM | INAT_XOPOK,
	[0xd3] = INAT_MODRM | INAT_XOPOK,
	[0xd6] = INAT_MODRM | INAT_XOPOK,
	[0xd7] = INAT_MODRM | INAT_XOPOK,
	[0xdb] = INAT_MODRM | INAT_XOPOK,
	[0xe1] = INAT_MODRM | INAT_XOPOK,
	[0xe2] = INAT_MODRM | INAT_XOPOK,
	[0xe3] = INAT_MODRM | INAT_XOPOK,
};

/* Table: XOP map Ah */
const insn_attr_t inat_xop_table_2[INAT_OPCODE_TABLE_SIZE] = {
	[0x10] = INAT_MAKE_IMM(INAT_IMM_DWORD) | INAT_MODRM | INAT_XOPOK,
	[0x12] = INAT_MAKE_GROUP(31) | INAT_XOPOK,
};

/* GrpTable: Grp1 */

/* GrpTable: Grp1A */

/* GrpTable: Grp2 */

/* GrpTable: Grp3_1 */
const insn_attr_t inat_group_table_6[INAT_GROUP_TABLE_SIZE] = {
	[0x0] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM,
	[0x1] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM,
	[0x2] = INAT_MODRM,
	[0x3] = INAT_MODRM,
	[0x4] = INAT_MODRM,
	[0x5] = INAT_MODRM,
	[0x6] = INAT_MODRM,
	[0x7] = INAT_MODRM,
};

/* GrpTable: Grp3_2 */
const insn_attr_t inat_group_table_7[INAT_GROUP_TABLE_SIZE] = {
	[0x0] = INAT_MAKE_IMM(INAT_IMM_VWORD32) | INAT_MODRM,
	[0x1] = INAT_MAKE_IMM(INAT_IMM_VWORD32) | INAT_MODRM,
	[0x2] = INAT_MODRM,
	[0x3] = INAT_MODRM,
	[0x4] = INAT_MODRM,
	[0x5] = INAT_MODRM,
	[0x6] = INAT_MODRM,
	[0x7] = INAT_MODRM,
};

/* GrpTable: Grp4 */
const insn_attr_t inat_group_table_8[INAT_GROUP_TABLE_SIZE] = {
	[0x0] = INAT_MODRM,
	[0x1] = INAT_MODRM,
};

/* GrpTable: Grp5 */
const insn_attr_t inat_group_table_9[INAT_GROUP_TABLE_SIZE] = {
	[0x0] = INAT_MODRM,
	[0x1] = INAT_MODRM,
	[0x2] = INAT_MODRM | INAT_FORCE64,
	[0x3] = INAT_MODRM,
	[0x4] = INAT_MODRM | INAT_FORCE64,
	[0x5] = INAT_MODRM,
	[0x6] = INAT_MODRM | INAT_FORCE64,
};

/* GrpTable: Grp6 */
const insn_attr_t inat_group_table_10[INAT_GROUP_TABLE_SIZE] = {
	[0x0] = INAT_MODRM,
	[0x1] = INAT_MODRM,
	[0x2] = INAT_MODRM,
	[0x3] = INAT_MODRM,
	[0x4] = INAT_MODRM,
	[0x5] = INAT_MODRM,
	[0x6] = INAT_VARIANT,
};
const insn_attr_t inat_group_table_10_3[INAT_GROUP_TABLE_SIZE] = {
	[0x6] = INAT_MODRM,
};

/* GrpTable: Grp7 */
const insn_attr_t inat_group_table_11[INAT_GROUP_TABLE_SIZE] = {
	[0x0] = INAT_MODRM,
	[0x1] = INAT_MODRM,
	[0x2] = INAT_MODRM,
	[0x3] = INAT_MODRM,
	[0x4] = INAT_MODRM,
	[0x5] = INAT_VARIANT,
	[0x6] = INAT_MODRM,
	[0x7] = INAT_MODRM,
};
const insn_attr_t inat_group_table_11_2[INAT_GROUP_TABLE_SIZE] = {
	[0x5] = INAT_MODRM,
};

/* GrpTable: Grp8 */

/* GrpTable: Grp9 */
const insn_attr_t inat_group_table_24[INAT_GROUP_TABLE_SIZE] = {
	[0x1] = INAT_MODRM,
	[0x6] = INAT_MODRM | INAT_MODRM | INAT_VARIANT,
	[0x7] = INAT_MODRM | INAT_MODRM | INAT_VARIANT,
};
const insn_attr_t inat_group_table_24_1[INAT_GROUP_TABLE_SIZE] = {
	[0x6] = INAT_MODRM,
};
const insn_attr_t inat_group_table_24_2[INAT_GROUP_TABLE_SIZE] = {
	[0x6] = INAT_MODRM | INAT_MODRM,
	[0x7] = INAT_MODRM,
};

/* GrpTable: Grp10 */

/* GrpTable: Grp11A */
const insn_attr_t inat_group_table_4[INAT_GROUP_TABLE_SIZE] = {
	[0x0] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM,
	[0x7] = INAT_MAKE_IMM(INAT_IMM_BYTE),
};

/* GrpTable: Grp11B */
const insn_attr_t inat_group_table_5[INAT_GROUP_TABLE_SIZE] = {
	[0x0] = INAT_MAKE_IMM(INAT_IMM_VWORD32) | INAT_MODRM,
	[0x7] = INAT_MAKE_IMM(INAT_IMM_VWORD32),
};

/* GrpTable: Grp12 */
const insn_attr_t inat_group_table_16[INAT_GROUP_TABLE_SIZE] = {
	[0x2] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VARIANT,
	[0x4] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VARIANT,
	[0x6] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VARIANT,
};
const insn_attr_t inat_group_table_16_1[INAT_GROUP_TABLE_SIZE] = {
	[0x2] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
	[0x4] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
	[0x6] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
};

/* GrpTable: Grp13 */
const insn_attr_t inat_group_table_17[INAT_GROUP_TABLE_SIZE] = {
	[0x0] = INAT_VARIANT,
	[0x1] = INAT_VARIANT,
	[0x2] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VARIANT,
	[0x4] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VARIANT,
	[0x6] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VARIANT,
};
const insn_attr_t inat_group_table_17_1[INAT_GROUP_TABLE_SIZE] = {
	[0x0] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x1] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x2] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
	[0x4] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK | INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
	[0x6] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
};

/* GrpTable: Grp14 */
const insn_attr_t inat_group_table_18[INAT_GROUP_TABLE_SIZE] = {
	[0x2] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VARIANT,
	[0x3] = INAT_VARIANT,
	[0x6] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VARIANT,
	[0x7] = INAT_VARIANT,
};
const insn_attr_t inat_group_table_18_1[INAT_GROUP_TABLE_SIZE] = {
	[0x2] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
	[0x3] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
	[0x6] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
	[0x7] = INAT_MAKE_IMM(INAT_IMM_BYTE) | INAT_MODRM | INAT_VEXOK,
};

/* GrpTable: Grp15 */
const insn_attr_t inat_group_table_21[INAT_GROUP_TABLE_SIZE] = {
	[0x0] = INAT_VARIANT,
	[0x1] = INAT_VARIANT,
	[0x2] = INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
	[0x3] = INAT_MODRM | INAT_VEXOK | INAT_VARIANT,
	[0x4] = INAT_VARIANT,
	[0x5] = INAT_VARIANT,
	[0x6] = INAT_VARIANT,
};
const insn_attr_t inat_group_table_21_1[INAT_GROUP_TABLE_SIZE] = {
	[0x6] = INAT_MODRM,
};
const insn_attr_t inat_group_table_21_2[INAT_GROUP_TABLE_SIZE] = {
	[0x0] = INAT_MODRM,
	[0x1] = INAT_MODRM,
	[0x2] = INAT_MODRM,
	[0x3] = INAT_MODRM,
	[0x4] = INAT_MODRM,
	[0x5] = INAT_MODRM,
	[0x6] = INAT_MODRM | INAT_MODRM,
};
const insn_attr_t inat_group_table_21_3[INAT_GROUP_TABLE_SIZE] = {
	[0x6] = INAT_MODRM,
};

/* GrpTable: Grp16 */
const insn_attr_t inat_group_table_13[INAT_GROUP_TABLE_SIZE] = {
	[0x0] = INAT_MODRM,
	[0x1] = INAT_MODRM,
	[0x2] = INAT_MODRM,
	[0x3] = INAT_MODRM,
};

/* GrpTable: Grp17 */
const insn_attr_t inat_group_table_27[INAT_GROUP_TABLE_SIZE] = {
	[0x1] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
	[0x2] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
	[0x3] = INAT_MODRM | INAT_VEXOK | INAT_VEXONLY,
};

/* GrpTable: Grp18 */
const insn_attr_t inat_group_table_25[INAT_GROUP_TABLE_SIZE] = {
	[0x1] = INAT_VARIANT,
	[0x2] = INAT_VARIANT,
	[0x5] = INAT_VARIANT,
	[0x6] = INAT_VARIANT,
};
const insn_attr_t inat_group_table_25_1[INAT_GROUP_TABLE_SIZE] = {
	[0x1] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x2] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x5] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x6] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
};

/* GrpTable: Grp19 */
const insn_attr_t inat_group_table_26[INAT_GROUP_TABLE_SIZE] = {
	[0x1] = INAT_VARIANT,
	[0x2] = INAT_VARIANT,
	[0x5] = INAT_VARIANT,
	[0x6] = INAT_VARIANT,
};
const insn_attr_t inat_group_table_26_1[INAT_GROUP_TABLE_SIZE] = {
	[0x1] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x2] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x5] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
	[0x6] = INAT_MODRM | INAT_VEXOK | INAT_EVEXONLY,
};

/* GrpTable: Grp20 */
const insn_attr_t inat_group_table_14[INAT_GROUP_TABLE_SIZE] = {
	[0x0] = INAT_MODRM,
};

/* GrpTable: Grp21 */
const insn_attr_t inat_group_table_15[INAT_GROUP_TABLE_SIZE] = {
	[0x1] = INAT_VARIANT,
};
const insn_attr_t inat_group_table_15_2[INAT_GROUP_TABLE_SIZE] = {
	[0x1] = INAT_MODRM,
};

/* GrpTable: GrpP */

/* GrpTable: GrpPDLK */

/* GrpTable: GrpRNG */

/* GrpTable: GrpXOP1 */
const insn_attr_t inat_group_table_28[INAT_GROUP_TABLE_SIZE] = {
	[0x1] = INAT_MODRM | INAT_XOPOK,
	[0x2] = INAT_MODRM | INAT_XOPOK,
	[0x3] = INAT_MODRM | INAT_XOPOK,
	[0x4] = INAT_MODRM | INAT_XOPOK,
	[0x5] = INAT_MODRM | INAT_XOPOK,
	[0x6] = INAT_MODRM | INAT_XOPOK,
	[0x7] = INAT_MODRM | INAT_XOPOK,
};

/* GrpTable: GrpXOP2 */
const insn_attr_t inat_group_table_29[INAT_GROUP_TABLE_SIZE] = {
	[0x1] = INAT_MODRM | INAT_XOPOK,
	[0x6] = INAT_MODRM | INAT_XOPOK,
};

/* GrpTable: GrpXOP3 */
const insn_attr_t inat_group_table_30[INAT_GROUP_TABLE_SIZE] = {
	[0x0] = INAT_MODRM | INAT_XOPOK,
	[0x1] = INAT_MODRM | INAT_XOPOK,
};

/* GrpTable: GrpXOP4 */
const insn_attr_t inat_group_table_31[INAT_GROUP_TABLE_SIZE] = {
	[0x0] = INAT_MAKE_IMM(INAT_IMM_DWORD) | INAT_MODRM | INAT_XOPOK,
	[0x1] = INAT_MAKE_IMM(INAT_IMM_DWORD) | INAT_MODRM | INAT_XOPOK,
};

#ifndef __BOOT_COMPRESSED

/* Escape opcode map array */
const insn_attr_t * const inat_escape_tables[INAT_ESC_MAX + 1][INAT_LSTPFX_MAX + 1] = {
	[1][0] = inat_escape_table_1,
	[1][1] = inat_escape_table_1_1,
	[1][2] = inat_escape_table_1_2,
	[1][3] = inat_escape_table_1_3,
	[2][0] = inat_escape_table_2,
	[2][1] = inat_escape_table_2_1,
	[2][2] = inat_escape_table_2_2,
	[2][3] = inat_escape_table_2_3,
	[3][0] = inat_escape_table_3,
	[3][1] = inat_escape_table_3_1,
	[3][2] = inat_escape_table_3_2,
	[3][3] = inat_escape_table_3_3,
};

/* Group opcode map array */
const insn_attr_t * const inat_group_tables[INAT_GRP_MAX + 1][INAT_LSTPFX_MAX + 1] = {
	[4][0] = inat_group_table_4,
	[5][0] = inat_group_table_5,
	[6][0] = inat_group_table_6,
	[7][0] = inat_group_table_7,
	[8][0] = inat_group_table_8,
	[9][0] = inat_group_table_9,
	[10][0] = inat_group_table_10,
	[10][3] = inat_group_table_10_3,
	[11][0] = inat_group_table_11,
	[11][2] = inat_group_table_11_2,
	[13][0] = inat_group_table_13,
	[14][0] = inat_group_table_14,
	[15][0] = inat_group_table_15,
	[15][2] = inat_group_table_15_2,
	[16][0] = inat_group_table_16,
	[16][1] = inat_group_table_16_1,
	[17][0] = inat_group_table_17,
	[17][1] = inat_group_table_17_1,
	[18][0] = inat_group_table_18,
	[18][1] = inat_group_table_18_1,
	[21][0] = inat_group_table_21,
	[21][1] = inat_group_table_21_1,
	[21][2] = inat_group_table_21_2,
	[21][3] = inat_group_table_21_3,
	[24][0] = inat_group_table_24,
	[24][1] = inat_group_table_24_1,
	[24][2] = inat_group_table_24_2,
	[25][0] = inat_group_table_25,
	[25][1] = inat_group_table_25_1,
	[26][0] = inat_group_table_26,
	[26][1] = inat_group_table_26_1,
	[27][0] = inat_group_table_27,
	[28][0] = inat_group_table_28,
	[29][0] = inat_group_table_29,
	[30][0] = inat_group_table_30,
	[31][0] = inat_group_table_31,
};

/* AVX opcode map array */
const insn_attr_t * const inat_avx_tables[X86_VEX_M_MAX + 1][INAT_LSTPFX_MAX + 1] = {
	[1][0] = inat_escape_table_1,
	[1][1] = inat_escape_table_1_1,
	[1][2] = inat_escape_table_1_2,
	[1][3] = inat_escape_table_1_3,
	[2][0] = inat_escape_table_2,
	[2][1] = inat_escape_table_2_1,
	[2][2] = inat_escape_table_2_2,
	[2][3] = inat_escape_table_2_3,
	[3][0] = inat_escape_table_3,
	[3][1] = inat_escape_table_3_1,
	[3][2] = inat_escape_table_3_2,
	[3][3] = inat_escape_table_3_3,
	[4][0] = inat_avx_table_4,
	[4][1] = inat_avx_table_4_1,
	[4][2] = inat_avx_table_4_2,
	[4][3] = inat_avx_table_4_3,
	[5][0] = inat_avx_table_5,
	[5][1] = inat_avx_table_5_1,
	[5][2] = inat_avx_table_5_2,
	[5][3] = inat_avx_table_5_3,
	[6][0] = inat_avx_table_6,
	[6][1] = inat_avx_table_6_1,
	[6][2] = inat_avx_table_6_2,
	[6][3] = inat_avx_table_6_3,
	[7][0] = inat_avx_table_7,
	[7][2] = inat_avx_table_7_2,
	[7][3] = inat_avx_table_7_3,
};

/* XOP opcode map array */
const insn_attr_t * const inat_xop_tables[X86_XOP_M_MAX - X86_XOP_M_MIN + 1] = {
	[0] = inat_xop_table_0,
	[1] = inat_xop_table_1,
	[2] = inat_xop_table_2,
};
#else /* !__BOOT_COMPRESSED */

/* Escape opcode map array */
static const insn_attr_t *inat_escape_tables[INAT_ESC_MAX + 1][INAT_LSTPFX_MAX + 1];

/* Group opcode map array */
static const insn_attr_t *inat_group_tables[INAT_GRP_MAX + 1][INAT_LSTPFX_MAX + 1];

/* AVX opcode map array */
static const insn_attr_t *inat_avx_tables[X86_VEX_M_MAX + 1][INAT_LSTPFX_MAX + 1];

/* XOP opcode map array */
static const insn_attr_t *inat_xop_tables[X86_XOP_M_MAX - X86_XOP_M_MIN + 1];

static void inat_init_tables(void)
{
	/* Print Escape opcode map array */
	inat_escape_tables[1][0] = inat_escape_table_1;
	inat_escape_tables[1][1] = inat_escape_table_1_1;
	inat_escape_tables[1][2] = inat_escape_table_1_2;
	inat_escape_tables[1][3] = inat_escape_table_1_3;
	inat_escape_tables[2][0] = inat_escape_table_2;
	inat_escape_tables[2][1] = inat_escape_table_2_1;
	inat_escape_tables[2][2] = inat_escape_table_2_2;
	inat_escape_tables[2][3] = inat_escape_table_2_3;
	inat_escape_tables[3][0] = inat_escape_table_3;
	inat_escape_tables[3][1] = inat_escape_table_3_1;
	inat_escape_tables[3][2] = inat_escape_table_3_2;
	inat_escape_tables[3][3] = inat_escape_table_3_3;

	/* Print Group opcode map array */
	inat_group_tables[4][0] = inat_group_table_4;
	inat_group_tables[5][0] = inat_group_table_5;
	inat_group_tables[6][0] = inat_group_table_6;
	inat_group_tables[7][0] = inat_group_table_7;
	inat_group_tables[8][0] = inat_group_table_8;
	inat_group_tables[9][0] = inat_group_table_9;
	inat_group_tables[10][0] = inat_group_table_10;
	inat_group_tables[10][3] = inat_group_table_10_3;
	inat_group_tables[11][0] = inat_group_table_11;
	inat_group_tables[11][2] = inat_group_table_11_2;
	inat_group_tables[13][0] = inat_group_table_13;
	inat_group_tables[14][0] = inat_group_table_14;
	inat_group_tables[15][0] = inat_group_table_15;
	inat_group_tables[15][2] = inat_group_table_15_2;
	inat_group_tables[16][0] = inat_group_table_16;
	inat_group_tables[16][1] = inat_group_table_16_1;
	inat_group_tables[17][0] = inat_group_table_17;
	inat_group_tables[17][1] = inat_group_table_17_1;
	inat_group_tables[18][0] = inat_group_table_18;
	inat_group_tables[18][1] = inat_group_table_18_1;
	inat_group_tables[21][0] = inat_group_table_21;
	inat_group_tables[21][1] = inat_group_table_21_1;
	inat_group_tables[21][2] = inat_group_table_21_2;
	inat_group_tables[21][3] = inat_group_table_21_3;
	inat_group_tables[24][0] = inat_group_table_24;
	inat_group_tables[24][1] = inat_group_table_24_1;
	inat_group_tables[24][2] = inat_group_table_24_2;
	inat_group_tables[25][0] = inat_group_table_25;
	inat_group_tables[25][1] = inat_group_table_25_1;
	inat_group_tables[26][0] = inat_group_table_26;
	inat_group_tables[26][1] = inat_group_table_26_1;
	inat_group_tables[27][0] = inat_group_table_27;
	inat_group_tables[28][0] = inat_group_table_28;
	inat_group_tables[29][0] = inat_group_table_29;
	inat_group_tables[30][0] = inat_group_table_30;
	inat_group_tables[31][0] = inat_group_table_31;

	/* Print AVX opcode map array */
	inat_avx_tables[1][0] = inat_escape_table_1;
	inat_avx_tables[1][1] = inat_escape_table_1_1;
	inat_avx_tables[1][2] = inat_escape_table_1_2;
	inat_avx_tables[1][3] = inat_escape_table_1_3;
	inat_avx_tables[2][0] = inat_escape_table_2;
	inat_avx_tables[2][1] = inat_escape_table_2_1;
	inat_avx_tables[2][2] = inat_escape_table_2_2;
	inat_avx_tables[2][3] = inat_escape_table_2_3;
	inat_avx_tables[3][0] = inat_escape_table_3;
	inat_avx_tables[3][1] = inat_escape_table_3_1;
	inat_avx_tables[3][2] = inat_escape_table_3_2;
	inat_avx_tables[3][3] = inat_escape_table_3_3;
	inat_avx_tables[4][0] = inat_avx_table_4;
	inat_avx_tables[4][1] = inat_avx_table_4_1;
	inat_avx_tables[4][2] = inat_avx_table_4_2;
	inat_avx_tables[4][3] = inat_avx_table_4_3;
	inat_avx_tables[5][0] = inat_avx_table_5;
	inat_avx_tables[5][1] = inat_avx_table_5_1;
	inat_avx_tables[5][2] = inat_avx_table_5_2;
	inat_avx_tables[5][3] = inat_avx_table_5_3;
	inat_avx_tables[6][0] = inat_avx_table_6;
	inat_avx_tables[6][1] = inat_avx_table_6_1;
	inat_avx_tables[6][2] = inat_avx_table_6_2;
	inat_avx_tables[6][3] = inat_avx_table_6_3;
	inat_avx_tables[7][0] = inat_avx_table_7;
	inat_avx_tables[7][2] = inat_avx_table_7_2;
	inat_avx_tables[7][3] = inat_avx_table_7_3;

	/* Print XOP opcode map array */
	inat_xop_tables[0] = inat_xop_table_0;
	inat_xop_tables[1] = inat_xop_table_1;
	inat_xop_tables[2] = inat_xop_table_2;
}
#endif

```

`kernel/trace/arch/x86/lib/inat.c`:

```c
// SPDX-License-Identifier: GPL-2.0-or-later
/*
 * x86 instruction attribute tables
 *
 * Written by Masami Hiramatsu <mhiramat@redhat.com>
 */
#include "insn.h" /* __ignore_sync_check__ */

/* Attribute tables are generated from opcode map */
#include "inat-tables.c"

/* Attribute search APIs */
insn_attr_t inat_get_opcode_attribute(insn_byte_t opcode)
{
	return inat_primary_table[opcode];
}

int inat_get_last_prefix_id(insn_byte_t last_pfx)
{
	insn_attr_t lpfx_attr;

	lpfx_attr = inat_get_opcode_attribute(last_pfx);
	return inat_last_prefix_id(lpfx_attr);
}

insn_attr_t inat_get_escape_attribute(insn_byte_t opcode, int lpfx_id,
				      insn_attr_t esc_attr)
{
	const insn_attr_t *table;
	int n;

	n = inat_escape_id(esc_attr);

	table = inat_escape_tables[n][0];
	if (!table)
		return 0;
	if (inat_has_variant(table[opcode]) && lpfx_id) {
		table = inat_escape_tables[n][lpfx_id];
		if (!table)
			return 0;
	}
	return table[opcode];
}

insn_attr_t inat_get_group_attribute(insn_byte_t modrm, int lpfx_id,
				     insn_attr_t grp_attr)
{
	const insn_attr_t *table;
	int n;

	n = inat_group_id(grp_attr);

	table = inat_group_tables[n][0];
	if (!table)
		return inat_group_common_attribute(grp_attr);
	if (inat_has_variant(table[X86_MODRM_REG(modrm)]) && lpfx_id) {
		table = inat_group_tables[n][lpfx_id];
		if (!table)
			return inat_group_common_attribute(grp_attr);
	}
	return table[X86_MODRM_REG(modrm)] |
	       inat_group_common_attribute(grp_attr);
}

insn_attr_t inat_get_avx_attribute(insn_byte_t opcode, insn_byte_t vex_m,
				   insn_byte_t vex_p)
{
	const insn_attr_t *table;
	if (vex_m > X86_VEX_M_MAX || vex_p > INAT_LSTPFX_MAX)
		return 0;
	/* At first, this checks the master table */
	table = inat_avx_tables[vex_m][0];
	if (!table)
		return 0;
	if (!inat_is_group(table[opcode]) && vex_p) {
		/* If this is not a group, get attribute directly */
		table = inat_avx_tables[vex_m][vex_p];
		if (!table)
			return 0;
	}
	return table[opcode];
}

insn_attr_t inat_get_xop_attribute(insn_byte_t opcode, insn_byte_t map_select)
{
	const insn_attr_t *table;

	if (map_select < X86_XOP_M_MIN || map_select > X86_XOP_M_MAX)
		return 0;
	map_select -= X86_XOP_M_MIN;
	/* At first, this checks the master table */
	table = inat_xop_tables[map_select];
	if (!table)
		return 0;
	return table[opcode];
}

```

`kernel/trace/arch/x86/lib/insn-eval.c`:

```c
/*
 * Utility functions for x86 operand and address decoding
 *
 * Copyright (C) Intel Corporation 2017
 */
#include <linux/kernel.h>
#include <linux/string.h>
#include <linux/ratelimit.h>
#include <linux/version.h>
#include <linux/mmu_context.h>
#include <asm/desc_defs.h>
#include <asm/desc.h>
#include "inat.h"
#include "insn.h"
#include "insn-eval.h"
#include <asm/ldt.h>
#include <asm/msr.h>
#include <asm/vm86.h>

#undef pr_fmt
#define pr_fmt(fmt) "insn: " fmt

enum reg_type {
	REG_TYPE_RM = 0,
	REG_TYPE_REG,
	REG_TYPE_INDEX,
	REG_TYPE_BASE,
};

#if LINUX_VERSION_CODE < KERNEL_VERSION(6, 16, 0)
#define rdmsrq(__a, __b) rdmsrl(__a, __b)
#endif

#ifdef MODULE
#define nmi_uaccess_okay() true
#endif

/**
 * is_string_insn() - Determine if instruction is a string instruction
 * @insn:	Instruction containing the opcode to inspect
 *
 * Returns:
 *
 * true if the instruction, determined by the opcode, is any of the
 * string instructions as defined in the Intel Software Development manual.
 * False otherwise.
 */
static bool is_string_insn(struct insn *insn)
{
	/* All string instructions have a 1-byte opcode. */
	if (insn->opcode.nbytes != 1)
		return false;

	switch (insn->opcode.bytes[0]) {
	case 0x6c ... 0x6f:	/* INS, OUTS */
	case 0xa4 ... 0xa7:	/* MOVS, CMPS */
	case 0xaa ... 0xaf:	/* STOS, LODS, SCAS */
		return true;
	default:
		return false;
	}
}

/**
 * insn_has_rep_prefix() - Determine if instruction has a REP prefix
 * @insn:	Instruction containing the prefix to inspect
 *
 * Returns:
 *
 * true if the instruction has a REP prefix, false if not.
 */
bool insn_has_rep_prefix(struct insn *insn)
{
	insn_byte_t p;

	insn_get_prefixes(insn);

	for_each_insn_prefix(insn, p) {
		if (p == 0xf2 || p == 0xf3)
			return true;
	}

	return false;
}

/**
 * get_seg_reg_override_idx() - obtain segment register override index
 * @insn:	Valid instruction with segment override prefixes
 *
 * Inspect the instruction prefixes in @insn and find segment overrides, if any.
 *
 * Returns:
 *
 * A constant identifying the segment register to use, among CS, SS, DS,
 * ES, FS, or GS. INAT_SEG_REG_DEFAULT is returned if no segment override
 * prefixes were found.
 *
 * -EINVAL in case of error.
 */
static int get_seg_reg_override_idx(struct insn *insn)
{
	int idx = INAT_SEG_REG_DEFAULT;
	int num_overrides = 0;
	insn_byte_t p;

	insn_get_prefixes(insn);

	/* Look for any segment override prefixes. */
	for_each_insn_prefix(insn, p) {
		insn_attr_t attr;

		attr = inat_get_opcode_attribute(p);
		switch (attr) {
		case INAT_MAKE_PREFIX(INAT_PFX_CS):
			idx = INAT_SEG_REG_CS;
			num_overrides++;
			break;
		case INAT_MAKE_PREFIX(INAT_PFX_SS):
			idx = INAT_SEG_REG_SS;
			num_overrides++;
			break;
		case INAT_MAKE_PREFIX(INAT_PFX_DS):
			idx = INAT_SEG_REG_DS;
			num_overrides++;
			break;
		case INAT_MAKE_PREFIX(INAT_PFX_ES):
			idx = INAT_SEG_REG_ES;
			num_overrides++;
			break;
		case INAT_MAKE_PREFIX(INAT_PFX_FS):
			idx = INAT_SEG_REG_FS;
			num_overrides++;
			break;
		case INAT_MAKE_PREFIX(INAT_PFX_GS):
			idx = INAT_SEG_REG_GS;
			num_overrides++;
			break;
		/* No default action needed. */
		}
	}

	/* More than one segment override prefix leads to undefined behavior. */
	if (num_overrides > 1)
		return -EINVAL;

	return idx;
}

/**
 * check_seg_overrides() - check if segment override prefixes are allowed
 * @insn:	Valid instruction with segment override prefixes
 * @regoff:	Operand offset, in pt_regs, for which the check is performed
 *
 * For a particular register used in register-indirect addressing, determine if
 * segment override prefixes can be used. Specifically, no overrides are allowed
 * for rDI if used with a string instruction.
 *
 * Returns:
 *
 * True if segment override prefixes can be used with the register indicated
 * in @regoff. False if otherwise.
 */
static bool check_seg_overrides(struct insn *insn, int regoff)
{
	if (regoff == offsetof(struct pt_regs, di) && is_string_insn(insn))
		return false;

	return true;
}

/**
 * resolve_default_seg() - resolve default segment register index for an operand
 * @insn:	Instruction with opcode and address size. Must be valid.
 * @regs:	Register values as seen when entering kernel mode
 * @off:	Operand offset, in pt_regs, for which resolution is needed
 *
 * Resolve the default segment register index associated with the instruction
 * operand register indicated by @off. Such index is resolved based on defaults
 * described in the Intel Software Development Manual.
 *
 * Returns:
 *
 * If in protected mode, a constant identifying the segment register to use,
 * among CS, SS, ES or DS. If in long mode, INAT_SEG_REG_IGNORE.
 *
 * -EINVAL in case of error.
 */
static int resolve_default_seg(struct insn *insn, struct pt_regs *regs, int off)
{
	if (any_64bit_mode(regs))
		return INAT_SEG_REG_IGNORE;
	/*
	 * Resolve the default segment register as described in Section 3.7.4
	 * of the Intel Software Development Manual Vol. 1:
	 *
	 *  + DS for all references involving r[ABCD]X, and rSI.
	 *  + If used in a string instruction, ES for rDI. Otherwise, DS.
	 *  + AX, CX and DX are not valid register operands in 16-bit address
	 *    encodings but are valid for 32-bit and 64-bit encodings.
	 *  + -EDOM is reserved to identify for cases in which no register
	 *    is used (i.e., displacement-only addressing). Use DS.
	 *  + SS for rSP or rBP.
	 *  + CS for rIP.
	 */

	switch (off) {
	case offsetof(struct pt_regs, ax):
	case offsetof(struct pt_regs, cx):
	case offsetof(struct pt_regs, dx):
		/* Need insn to verify address size. */
		if (insn->addr_bytes == 2)
			return -EINVAL;

		fallthrough;

	case -EDOM:
	case offsetof(struct pt_regs, bx):
	case offsetof(struct pt_regs, si):
		return INAT_SEG_REG_DS;

	case offsetof(struct pt_regs, di):
		if (is_string_insn(insn))
			return INAT_SEG_REG_ES;
		return INAT_SEG_REG_DS;

	case offsetof(struct pt_regs, bp):
	case offsetof(struct pt_regs, sp):
		return INAT_SEG_REG_SS;

	case offsetof(struct pt_regs, ip):
		return INAT_SEG_REG_CS;

	default:
		return -EINVAL;
	}
}

/**
 * resolve_seg_reg() - obtain segment register index
 * @insn:	Instruction with operands
 * @regs:	Register values as seen when entering kernel mode
 * @regoff:	Operand offset, in pt_regs, used to determine segment register
 *
 * Determine the segment register associated with the operands and, if
 * applicable, prefixes and the instruction pointed by @insn.
 *
 * The segment register associated to an operand used in register-indirect
 * addressing depends on:
 *
 * a) Whether running in long mode (in such a case segments are ignored, except
 * if FS or GS are used).
 *
 * b) Whether segment override prefixes can be used. Certain instructions and
 *    registers do not allow override prefixes.
 *
 * c) Whether segment overrides prefixes are found in the instruction prefixes.
 *
 * d) If there are not segment override prefixes or they cannot be used, the
 *    default segment register associated with the operand register is used.
 *
 * The function checks first if segment override prefixes can be used with the
 * operand indicated by @regoff. If allowed, obtain such overridden segment
 * register index. Lastly, if not prefixes were found or cannot be used, resolve
 * the segment register index to use based on the defaults described in the
 * Intel documentation. In long mode, all segment register indexes will be
 * ignored, except if overrides were found for FS or GS. All these operations
 * are done using helper functions.
 *
 * The operand register, @regoff, is represented as the offset from the base of
 * pt_regs.
 *
 * As stated, the main use of this function is to determine the segment register
 * index based on the instruction, its operands and prefixes. Hence, @insn
 * must be valid. However, if @regoff indicates rIP, we don't need to inspect
 * @insn at all as in this case CS is used in all cases. This case is checked
 * before proceeding further.
 *
 * Please note that this function does not return the value in the segment
 * register (i.e., the segment selector) but our defined index. The segment
 * selector needs to be obtained using get_segment_selector() and passing the
 * segment register index resolved by this function.
 *
 * Returns:
 *
 * An index identifying the segment register to use, among CS, SS, DS,
 * ES, FS, or GS. INAT_SEG_REG_IGNORE is returned if running in long mode.
 *
 * -EINVAL in case of error.
 */
static int resolve_seg_reg(struct insn *insn, struct pt_regs *regs, int regoff)
{
	int idx;

	/*
	 * In the unlikely event of having to resolve the segment register
	 * index for rIP, do it first. Segment override prefixes should not
	 * be used. Hence, it is not necessary to inspect the instruction,
	 * which may be invalid at this point.
	 */
	if (regoff == offsetof(struct pt_regs, ip)) {
		if (any_64bit_mode(regs))
			return INAT_SEG_REG_IGNORE;
		else
			return INAT_SEG_REG_CS;
	}

	if (!insn)
		return -EINVAL;

	if (!check_seg_overrides(insn, regoff))
		return resolve_default_seg(insn, regs, regoff);

	idx = get_seg_reg_override_idx(insn);
	if (idx < 0)
		return idx;

	if (idx == INAT_SEG_REG_DEFAULT)
		return resolve_default_seg(insn, regs, regoff);

	/*
	 * In long mode, segment override prefixes are ignored, except for
	 * overrides for FS and GS.
	 */
	if (any_64bit_mode(regs)) {
		if (idx != INAT_SEG_REG_FS &&
		    idx != INAT_SEG_REG_GS)
			idx = INAT_SEG_REG_IGNORE;
	}

	return idx;
}

/**
 * get_segment_selector() - obtain segment selector
 * @regs:		Register values as seen when entering kernel mode
 * @seg_reg_idx:	Segment register index to use
 *
 * Obtain the segment selector from any of the CS, SS, DS, ES, FS, GS segment
 * registers. In CONFIG_X86_32, the segment is obtained from either pt_regs or
 * kernel_vm86_regs as applicable. In CONFIG_X86_64, CS and SS are obtained
 * from pt_regs. DS, ES, FS and GS are obtained by reading the actual CPU
 * registers. This done for only for completeness as in CONFIG_X86_64 segment
 * registers are ignored.
 *
 * Returns:
 *
 * Value of the segment selector, including null when running in
 * long mode.
 *
 * -EINVAL on error.
 */
static short get_segment_selector(struct pt_regs *regs, int seg_reg_idx)
{
	unsigned short sel;

#ifdef CONFIG_X86_64
	switch (seg_reg_idx) {
	case INAT_SEG_REG_IGNORE:
		return 0;
	case INAT_SEG_REG_CS:
		return (unsigned short)(regs->cs & 0xffff);
	case INAT_SEG_REG_SS:
		return (unsigned short)(regs->ss & 0xffff);
	case INAT_SEG_REG_DS:
		savesegment(ds, sel);
		return sel;
	case INAT_SEG_REG_ES:
		savesegment(es, sel);
		return sel;
	case INAT_SEG_REG_FS:
		savesegment(fs, sel);
		return sel;
	case INAT_SEG_REG_GS:
		savesegment(gs, sel);
		return sel;
	default:
		return -EINVAL;
	}
#else /* CONFIG_X86_32 */
	struct kernel_vm86_regs *vm86regs = (struct kernel_vm86_regs *)regs;

	if (v8086_mode(regs)) {
		switch (seg_reg_idx) {
		case INAT_SEG_REG_CS:
			return (unsigned short)(regs->cs & 0xffff);
		case INAT_SEG_REG_SS:
			return (unsigned short)(regs->ss & 0xffff);
		case INAT_SEG_REG_DS:
			return vm86regs->ds;
		case INAT_SEG_REG_ES:
			return vm86regs->es;
		case INAT_SEG_REG_FS:
			return vm86regs->fs;
		case INAT_SEG_REG_GS:
			return vm86regs->gs;
		case INAT_SEG_REG_IGNORE:
		default:
			return -EINVAL;
		}
	}

	switch (seg_reg_idx) {
	case INAT_SEG_REG_CS:
		return (unsigned short)(regs->cs & 0xffff);
	case INAT_SEG_REG_SS:
		return (unsigned short)(regs->ss & 0xffff);
	case INAT_SEG_REG_DS:
		return (unsigned short)(regs->ds & 0xffff);
	case INAT_SEG_REG_ES:
		return (unsigned short)(regs->es & 0xffff);
	case INAT_SEG_REG_FS:
		return (unsigned short)(regs->fs & 0xffff);
	case INAT_SEG_REG_GS:
		savesegment(gs, sel);
		return sel;
	case INAT_SEG_REG_IGNORE:
	default:
		return -EINVAL;
	}
#endif /* CONFIG_X86_64 */
}

static const int pt_regoff[] = {
	offsetof(struct pt_regs, ax),
	offsetof(struct pt_regs, cx),
	offsetof(struct pt_regs, dx),
	offsetof(struct pt_regs, bx),
	offsetof(struct pt_regs, sp),
	offsetof(struct pt_regs, bp),
	offsetof(struct pt_regs, si),
	offsetof(struct pt_regs, di),
#ifdef CONFIG_X86_64
	offsetof(struct pt_regs, r8),
	offsetof(struct pt_regs, r9),
	offsetof(struct pt_regs, r10),
	offsetof(struct pt_regs, r11),
	offsetof(struct pt_regs, r12),
	offsetof(struct pt_regs, r13),
	offsetof(struct pt_regs, r14),
	offsetof(struct pt_regs, r15),
#else
	offsetof(struct pt_regs, ds),
	offsetof(struct pt_regs, es),
	offsetof(struct pt_regs, fs),
	offsetof(struct pt_regs, gs),
#endif
};

int pt_regs_offset(struct pt_regs *regs, int regno)
{
	if ((unsigned)regno < ARRAY_SIZE(pt_regoff))
		return pt_regoff[regno];
	return -EDOM;
}

static int get_regno(struct insn *insn, enum reg_type type)
{
	int nr_registers = ARRAY_SIZE(pt_regoff);
	int regno = 0;

	/*
	 * Don't possibly decode a 32-bit instructions as
	 * reading a 64-bit-only register.
	 */
	if (IS_ENABLED(CONFIG_X86_64) && !insn->x86_64)
		nr_registers -= 8;

	switch (type) {
	case REG_TYPE_RM:
		regno = X86_MODRM_RM(insn->modrm.value);

		/*
		 * ModRM.mod == 0 and ModRM.rm == 5 means a 32-bit displacement
		 * follows the ModRM byte.
		 */
		if (!X86_MODRM_MOD(insn->modrm.value) && regno == 5)
			return -EDOM;

		if (X86_REX_B(insn->rex_prefix.value))
			regno += 8;
		break;

	case REG_TYPE_REG:
		regno = X86_MODRM_REG(insn->modrm.value);

		if (X86_REX_R(insn->rex_prefix.value))
			regno += 8;
		break;

	case REG_TYPE_INDEX:
		regno = X86_SIB_INDEX(insn->sib.value);
		if (X86_REX_X(insn->rex_prefix.value))
			regno += 8;

		/*
		 * If ModRM.mod != 3 and SIB.index = 4 the scale*index
		 * portion of the address computation is null. This is
		 * true only if REX.X is 0. In such a case, the SIB index
		 * is used in the address computation.
		 */
		if (X86_MODRM_MOD(insn->modrm.value) != 3 && regno == 4)
			return -EDOM;
		break;

	case REG_TYPE_BASE:
		regno = X86_SIB_BASE(insn->sib.value);
		/*
		 * If ModRM.mod is 0 and SIB.base == 5, the base of the
		 * register-indirect addressing is 0. In this case, a
		 * 32-bit displacement follows the SIB byte.
		 */
		if (!X86_MODRM_MOD(insn->modrm.value) && regno == 5)
			return -EDOM;

		if (X86_REX_B(insn->rex_prefix.value))
			regno += 8;
		break;

	default:
		pr_err_ratelimited("invalid register type: %d\n", type);
		return -EINVAL;
	}

	if (regno >= nr_registers) {
		WARN_ONCE(1, "decoded an instruction with an invalid register");
		return -EINVAL;
	}
	return regno;
}

static int get_reg_offset(struct insn *insn, struct pt_regs *regs,
			  enum reg_type type)
{
	int regno = get_regno(insn, type);

	if (regno < 0)
		return regno;

	return pt_regs_offset(regs, regno);
}

/**
 * get_reg_offset_16() - Obtain offset of register indicated by instruction
 * @insn:	Instruction containing ModRM byte
 * @regs:	Register values as seen when entering kernel mode
 * @offs1:	Offset of the first operand register
 * @offs2:	Offset of the second operand register, if applicable
 *
 * Obtain the offset, in pt_regs, of the registers indicated by the ModRM byte
 * in @insn. This function is to be used with 16-bit address encodings. The
 * @offs1 and @offs2 will be written with the offset of the two registers
 * indicated by the instruction. In cases where any of the registers is not
 * referenced by the instruction, the value will be set to -EDOM.
 *
 * Returns:
 *
 * 0 on success, -EINVAL on error.
 */
static int get_reg_offset_16(struct insn *insn, struct pt_regs *regs,
			     int *offs1, int *offs2)
{
	/*
	 * 16-bit addressing can use one or two registers. Specifics of
	 * encodings are given in Table 2-1. "16-Bit Addressing Forms with the
	 * ModR/M Byte" of the Intel Software Development Manual.
	 */
	static const int regoff1[] = {
		offsetof(struct pt_regs, bx),
		offsetof(struct pt_regs, bx),
		offsetof(struct pt_regs, bp),
		offsetof(struct pt_regs, bp),
		offsetof(struct pt_regs, si),
		offsetof(struct pt_regs, di),
		offsetof(struct pt_regs, bp),
		offsetof(struct pt_regs, bx),
	};

	static const int regoff2[] = {
		offsetof(struct pt_regs, si),
		offsetof(struct pt_regs, di),
		offsetof(struct pt_regs, si),
		offsetof(struct pt_regs, di),
		-EDOM,
		-EDOM,
		-EDOM,
		-EDOM,
	};

	if (!offs1 || !offs2)
		return -EINVAL;

	/* Operand is a register, use the generic function. */
	if (X86_MODRM_MOD(insn->modrm.value) == 3) {
		*offs1 = insn_get_modrm_rm_off(insn, regs);
		*offs2 = -EDOM;
		return 0;
	}

	*offs1 = regoff1[X86_MODRM_RM(insn->modrm.value)];
	*offs2 = regoff2[X86_MODRM_RM(insn->modrm.value)];

	/*
	 * If ModRM.mod is 0 and ModRM.rm is 110b, then we use displacement-
	 * only addressing. This means that no registers are involved in
	 * computing the effective address. Thus, ensure that the first
	 * register offset is invalid. The second register offset is already
	 * invalid under the aforementioned conditions.
	 */
	if ((X86_MODRM_MOD(insn->modrm.value) == 0) &&
	    (X86_MODRM_RM(insn->modrm.value) == 6))
		*offs1 = -EDOM;

	return 0;
}

/**
 * get_desc() - Obtain contents of a segment descriptor
 * @out:	Segment descriptor contents on success
 * @sel:	Segment selector
 *
 * Given a segment selector, obtain a pointer to the segment descriptor.
 * Both global and local descriptor tables are supported.
 *
 * Returns:
 *
 * True on success, false on failure.
 *
 * NULL on error.
 */
static bool get_desc(struct desc_struct *out, unsigned short sel)
{
	struct desc_ptr gdt_desc = {0, 0};
	unsigned long desc_base;

#ifdef CONFIG_MODIFY_LDT_SYSCALL
	if ((sel & SEGMENT_TI_MASK) == SEGMENT_LDT) {
		bool success = false;
		struct ldt_struct *ldt;

		/* Bits [15:3] contain the index of the desired entry. */
		sel >>= 3;

		/*
		 * If we're not in a valid context with a real (not just lazy)
		 * user mm, then don't even try.
		 */
		if (!nmi_uaccess_okay())
			return false;

		mutex_lock(&current->mm->context.lock);
		ldt = current->mm->context.ldt;
		if (ldt && sel < ldt->nr_entries) {
			*out = ldt->entries[sel];
			success = true;
		}

		mutex_unlock(&current->mm->context.lock);

		return success;
	}
#endif
	native_store_gdt(&gdt_desc);

	/*
	 * Segment descriptors have a size of 8 bytes. Thus, the index is
	 * multiplied by 8 to obtain the memory offset of the desired descriptor
	 * from the base of the GDT. As bits [15:3] of the segment selector
	 * contain the index, it can be regarded as multiplied by 8 already.
	 * All that remains is to clear bits [2:0].
	 */
	desc_base = sel & ~(SEGMENT_RPL_MASK | SEGMENT_TI_MASK);

	if (desc_base > gdt_desc.size)
		return false;

	*out = *(struct desc_struct *)(gdt_desc.address + desc_base);
	return true;
}

/**
 * insn_get_seg_base() - Obtain base address of segment descriptor.
 * @regs:		Register values as seen when entering kernel mode
 * @seg_reg_idx:	Index of the segment register pointing to seg descriptor
 *
 * Obtain the base address of the segment as indicated by the segment descriptor
 * pointed by the segment selector. The segment selector is obtained from the
 * input segment register index @seg_reg_idx.
 *
 * Returns:
 *
 * In protected mode, base address of the segment. Zero in long mode,
 * except when FS or GS are used. In virtual-8086 mode, the segment
 * selector shifted 4 bits to the right.
 *
 * -1L in case of error.
 */
unsigned long insn_get_seg_base(struct pt_regs *regs, int seg_reg_idx)
{
	struct desc_struct desc;
	short sel;

	sel = get_segment_selector(regs, seg_reg_idx);
	if (sel < 0)
		return -1L;

	if (v8086_mode(regs))
		/*
		 * Base is simply the segment selector shifted 4
		 * bits to the right.
		 */
		return (unsigned long)(sel << 4);

	if (any_64bit_mode(regs)) {
		/*
		 * Only FS or GS will have a base address, the rest of
		 * the segments' bases are forced to 0.
		 */
		unsigned long base;

		if (seg_reg_idx == INAT_SEG_REG_FS) {
			rdmsrq(MSR_FS_BASE, base);
		} else if (seg_reg_idx == INAT_SEG_REG_GS) {
			/*
			 * swapgs was called at the kernel entry point. Thus,
			 * MSR_KERNEL_GS_BASE will have the user-space GS base.
			 */
			if (user_mode(regs))
				rdmsrq(MSR_FS_BASE, base);
			else
				rdmsrq(MSR_GS_BASE, base);
		} else {
			base = 0;
		}
		return base;
	}

	/* In protected mode the segment selector cannot be null. */
	if (!sel)
		return -1L;

	if (!get_desc(&desc, sel))
		return -1L;

	return get_desc_base(&desc);
}

/**
 * get_seg_limit() - Obtain the limit of a segment descriptor
 * @regs:		Register values as seen when entering kernel mode
 * @seg_reg_idx:	Index of the segment register pointing to seg descriptor
 *
 * Obtain the limit of the segment as indicated by the segment descriptor
 * pointed by the segment selector. The segment selector is obtained from the
 * input segment register index @seg_reg_idx.
 *
 * Returns:
 *
 * In protected mode, the limit of the segment descriptor in bytes.
 * In long mode and virtual-8086 mode, segment limits are not enforced. Thus,
 * limit is returned as -1L to imply a limit-less segment.
 *
 * Zero is returned on error.
 */
static unsigned long get_seg_limit(struct pt_regs *regs, int seg_reg_idx)
{
	struct desc_struct desc;
	unsigned long limit;
	short sel;

	sel = get_segment_selector(regs, seg_reg_idx);
	if (sel < 0)
		return 0;

	if (any_64bit_mode(regs) || v8086_mode(regs))
		return -1L;

	if (!sel)
		return 0;

	if (!get_desc(&desc, sel))
		return 0;

	/*
	 * If the granularity bit is set, the limit is given in multiples
	 * of 4096. This also means that the 12 least significant bits are
	 * not tested when checking the segment limits. In practice,
	 * this means that the segment ends in (limit << 12) + 0xfff.
	 */
	limit = get_desc_limit(&desc);
	if (desc.g)
		limit = (limit << 12) + 0xfff;

	return limit;
}

/**
 * insn_get_code_seg_params() - Obtain code segment parameters
 * @regs:	Structure with register values as seen when entering kernel mode
 *
 * Obtain address and operand sizes of the code segment. It is obtained from the
 * selector contained in the CS register in regs. In protected mode, the default
 * address is determined by inspecting the L and D bits of the segment
 * descriptor. In virtual-8086 mode, the default is always two bytes for both
 * address and operand sizes.
 *
 * Returns:
 *
 * An int containing ORed-in default parameters on success.
 *
 * -EINVAL on error.
 */
int insn_get_code_seg_params(struct pt_regs *regs)
{
	struct desc_struct desc;
	short sel;

	if (v8086_mode(regs))
		/* Address and operand size are both 16-bit. */
		return INSN_CODE_SEG_PARAMS(2, 2);

	sel = get_segment_selector(regs, INAT_SEG_REG_CS);
	if (sel < 0)
		return sel;

	if (!get_desc(&desc, sel))
		return -EINVAL;

	/*
	 * The most significant byte of the Type field of the segment descriptor
	 * determines whether a segment contains data or code. If this is a data
	 * segment, return error.
	 */
	if (!(desc.type & BIT(3)))
		return -EINVAL;

	switch ((desc.l << 1) | desc.d) {
	case 0: /*
		 * Legacy mode. CS.L=0, CS.D=0. Address and operand size are
		 * both 16-bit.
		 */
		return INSN_CODE_SEG_PARAMS(2, 2);
	case 1: /*
		 * Legacy mode. CS.L=0, CS.D=1. Address and operand size are
		 * both 32-bit.
		 */
		return INSN_CODE_SEG_PARAMS(4, 4);
	case 2: /*
		 * IA-32e 64-bit mode. CS.L=1, CS.D=0. Address size is 64-bit;
		 * operand size is 32-bit.
		 */
		return INSN_CODE_SEG_PARAMS(4, 8);
	case 3: /* Invalid setting. CS.L=1, CS.D=1 */
		fallthrough;
	default:
		return -EINVAL;
	}
}

/**
 * insn_get_modrm_rm_off() - Obtain register in r/m part of the ModRM byte
 * @insn:	Instruction containing the ModRM byte
 * @regs:	Register values as seen when entering kernel mode
 *
 * Returns:
 *
 * The register indicated by the r/m part of the ModRM byte. The
 * register is obtained as an offset from the base of pt_regs. In specific
 * cases, the returned value can be -EDOM to indicate that the particular value
 * of ModRM does not refer to a register and shall be ignored.
 */
int insn_get_modrm_rm_off(struct insn *insn, struct pt_regs *regs)
{
	return get_reg_offset(insn, regs, REG_TYPE_RM);
}

/**
 * insn_get_modrm_reg_off() - Obtain register in reg part of the ModRM byte
 * @insn:	Instruction containing the ModRM byte
 * @regs:	Register values as seen when entering kernel mode
 *
 * Returns:
 *
 * The register indicated by the reg part of the ModRM byte. The
 * register is obtained as an offset from the base of pt_regs.
 */
int insn_get_modrm_reg_off(struct insn *insn, struct pt_regs *regs)
{
	return get_reg_offset(insn, regs, REG_TYPE_REG);
}

/**
 * insn_get_modrm_reg_ptr() - Obtain register pointer based on ModRM byte
 * @insn:	Instruction containing the ModRM byte
 * @regs:	Register values as seen when entering kernel mode
 *
 * Returns:
 *
 * The register indicated by the reg part of the ModRM byte.
 * The register is obtained as a pointer within pt_regs.
 */
unsigned long *insn_get_modrm_reg_ptr(struct insn *insn, struct pt_regs *regs)
{
	int offset;

	offset = insn_get_modrm_reg_off(insn, regs);
	if (offset < 0)
		return NULL;
	return (void *)regs + offset;
}

/**
 * get_seg_base_limit() - obtain base address and limit of a segment
 * @insn:	Instruction. Must be valid.
 * @regs:	Register values as seen when entering kernel mode
 * @regoff:	Operand offset, in pt_regs, used to resolve segment descriptor
 * @base:	Obtained segment base
 * @limit:	Obtained segment limit
 *
 * Obtain the base address and limit of the segment associated with the operand
 * @regoff and, if any or allowed, override prefixes in @insn. This function is
 * different from insn_get_seg_base() as the latter does not resolve the segment
 * associated with the instruction operand. If a limit is not needed (e.g.,
 * when running in long mode), @limit can be NULL.
 *
 * Returns:
 *
 * 0 on success. @base and @limit will contain the base address and of the
 * resolved segment, respectively.
 *
 * -EINVAL on error.
 */
static int get_seg_base_limit(struct insn *insn, struct pt_regs *regs,
			      int regoff, unsigned long *base,
			      unsigned long *limit)
{
	int seg_reg_idx;

	if (!base)
		return -EINVAL;

	seg_reg_idx = resolve_seg_reg(insn, regs, regoff);
	if (seg_reg_idx < 0)
		return seg_reg_idx;

	*base = insn_get_seg_base(regs, seg_reg_idx);
	if (*base == -1L)
		return -EINVAL;

	if (!limit)
		return 0;

	*limit = get_seg_limit(regs, seg_reg_idx);
	if (!(*limit))
		return -EINVAL;

	return 0;
}

/**
 * get_eff_addr_reg() - Obtain effective address from register operand
 * @insn:	Instruction. Must be valid.
 * @regs:	Register values as seen when entering kernel mode
 * @regoff:	Obtained operand offset, in pt_regs, with the effective address
 * @eff_addr:	Obtained effective address
 *
 * Obtain the effective address stored in the register operand as indicated by
 * the ModRM byte. This function is to be used only with register addressing
 * (i.e.,  ModRM.mod is 3). The effective address is saved in @eff_addr. The
 * register operand, as an offset from the base of pt_regs, is saved in @regoff;
 * such offset can then be used to resolve the segment associated with the
 * operand. This function can be used with any of the supported address sizes
 * in x86.
 *
 * Returns:
 *
 * 0 on success. @eff_addr will have the effective address stored in the
 * operand indicated by ModRM. @regoff will have such operand as an offset from
 * the base of pt_regs.
 *
 * -EINVAL on error.
 */
static int get_eff_addr_reg(struct insn *insn, struct pt_regs *regs,
			    int *regoff, long *eff_addr)
{
	int ret;

	ret = insn_get_modrm(insn);
	if (ret)
		return ret;

	if (X86_MODRM_MOD(insn->modrm.value) != 3)
		return -EINVAL;

	*regoff = get_reg_offset(insn, regs, REG_TYPE_RM);
	if (*regoff < 0)
		return -EINVAL;

	/* Ignore bytes that are outside the address size. */
	if (insn->addr_bytes == 2)
		*eff_addr = regs_get_register(regs, *regoff) & 0xffff;
	else if (insn->addr_bytes == 4)
		*eff_addr = regs_get_register(regs, *regoff) & 0xffffffff;
	else /* 64-bit address */
		*eff_addr = regs_get_register(regs, *regoff);

	return 0;
}

/**
 * get_eff_addr_modrm() - Obtain referenced effective address via ModRM
 * @insn:	Instruction. Must be valid.
 * @regs:	Register values as seen when entering kernel mode
 * @regoff:	Obtained operand offset, in pt_regs, associated with segment
 * @eff_addr:	Obtained effective address
 *
 * Obtain the effective address referenced by the ModRM byte of @insn. After
 * identifying the registers involved in the register-indirect memory reference,
 * its value is obtained from the operands in @regs. The computed address is
 * stored @eff_addr. Also, the register operand that indicates the associated
 * segment is stored in @regoff, this parameter can later be used to determine
 * such segment.
 *
 * Returns:
 *
 * 0 on success. @eff_addr will have the referenced effective address. @regoff
 * will have a register, as an offset from the base of pt_regs, that can be used
 * to resolve the associated segment.
 *
 * -EINVAL on error.
 */
static int get_eff_addr_modrm(struct insn *insn, struct pt_regs *regs,
			      int *regoff, long *eff_addr)
{
	long tmp;
	int ret;

	if (insn->addr_bytes != 8 && insn->addr_bytes != 4)
		return -EINVAL;

	ret = insn_get_modrm(insn);
	if (ret)
		return ret;

	if (X86_MODRM_MOD(insn->modrm.value) > 2)
		return -EINVAL;

	*regoff = get_reg_offset(insn, regs, REG_TYPE_RM);

	/*
	 * -EDOM means that we must ignore the address_offset. In such a case,
	 * in 64-bit mode the effective address relative to the rIP of the
	 * following instruction.
	 */
	if (*regoff == -EDOM) {
		if (any_64bit_mode(regs))
			tmp = regs->ip + insn->length;
		else
			tmp = 0;
	} else if (*regoff < 0) {
		return -EINVAL;
	} else {
		tmp = regs_get_register(regs, *regoff);
	}

	if (insn->addr_bytes == 4) {
		int addr32 = (int)(tmp & 0xffffffff) + insn->displacement.value;

		*eff_addr = addr32 & 0xffffffff;
	} else {
		*eff_addr = tmp + insn->displacement.value;
	}

	return 0;
}

/**
 * get_eff_addr_modrm_16() - Obtain referenced effective address via ModRM
 * @insn:	Instruction. Must be valid.
 * @regs:	Register values as seen when entering kernel mode
 * @regoff:	Obtained operand offset, in pt_regs, associated with segment
 * @eff_addr:	Obtained effective address
 *
 * Obtain the 16-bit effective address referenced by the ModRM byte of @insn.
 * After identifying the registers involved in the register-indirect memory
 * reference, its value is obtained from the operands in @regs. The computed
 * address is stored @eff_addr. Also, the register operand that indicates
 * the associated segment is stored in @regoff, this parameter can later be used
 * to determine such segment.
 *
 * Returns:
 *
 * 0 on success. @eff_addr will have the referenced effective address. @regoff
 * will have a register, as an offset from the base of pt_regs, that can be used
 * to resolve the associated segment.
 *
 * -EINVAL on error.
 */
static int get_eff_addr_modrm_16(struct insn *insn, struct pt_regs *regs,
				 int *regoff, short *eff_addr)
{
	int addr_offset1, addr_offset2, ret;
	short addr1 = 0, addr2 = 0, displacement;

	if (insn->addr_bytes != 2)
		return -EINVAL;

	insn_get_modrm(insn);

	if (!insn->modrm.nbytes)
		return -EINVAL;

	if (X86_MODRM_MOD(insn->modrm.value) > 2)
		return -EINVAL;

	ret = get_reg_offset_16(insn, regs, &addr_offset1, &addr_offset2);
	if (ret < 0)
		return -EINVAL;

	/*
	 * Don't fail on invalid offset values. They might be invalid because
	 * they cannot be used for this particular value of ModRM. Instead, use
	 * them in the computation only if they contain a valid value.
	 */
	if (addr_offset1 != -EDOM)
		addr1 = regs_get_register(regs, addr_offset1) & 0xffff;

	if (addr_offset2 != -EDOM)
		addr2 = regs_get_register(regs, addr_offset2) & 0xffff;

	displacement = insn->displacement.value & 0xffff;
	*eff_addr = addr1 + addr2 + displacement;

	/*
	 * The first operand register could indicate to use of either SS or DS
	 * registers to obtain the segment selector.  The second operand
	 * register can only indicate the use of DS. Thus, the first operand
	 * will be used to obtain the segment selector.
	 */
	*regoff = addr_offset1;

	return 0;
}

/**
 * get_eff_addr_sib() - Obtain referenced effective address via SIB
 * @insn:	Instruction. Must be valid.
 * @regs:	Register values as seen when entering kernel mode
 * @base_offset: Obtained operand offset, in pt_regs, associated with segment
 * @eff_addr:	Obtained effective address
 *
 * Obtain the effective address referenced by the SIB byte of @insn. After
 * identifying the registers involved in the indexed, register-indirect memory
 * reference, its value is obtained from the operands in @regs. The computed
 * address is stored @eff_addr. Also, the register operand that indicates the
 * associated segment is stored in @base_offset; this parameter can later be
 * used to determine such segment.
 *
 * Returns:
 *
 * 0 on success. @eff_addr will have the referenced effective address.
 * @base_offset will have a register, as an offset from the base of pt_regs,
 * that can be used to resolve the associated segment.
 *
 * Negative value on error.
 */
static int get_eff_addr_sib(struct insn *insn, struct pt_regs *regs,
			    int *base_offset, long *eff_addr)
{
	long base, indx;
	int indx_offset;
	int ret;

	if (insn->addr_bytes != 8 && insn->addr_bytes != 4)
		return -EINVAL;

	ret = insn_get_modrm(insn);
	if (ret)
		return ret;

	if (!insn->modrm.nbytes)
		return -EINVAL;

	if (X86_MODRM_MOD(insn->modrm.value) > 2)
		return -EINVAL;

	ret = insn_get_sib(insn);
	if (ret)
		return ret;

	if (!insn->sib.nbytes)
		return -EINVAL;

	*base_offset = get_reg_offset(insn, regs, REG_TYPE_BASE);
	indx_offset = get_reg_offset(insn, regs, REG_TYPE_INDEX);

	/*
	 * Negative values in the base and index offset means an error when
	 * decoding the SIB byte. Except -EDOM, which means that the registers
	 * should not be used in the address computation.
	 */
	if (*base_offset == -EDOM)
		base = 0;
	else if (*base_offset < 0)
		return -EINVAL;
	else
		base = regs_get_register(regs, *base_offset);

	if (indx_offset == -EDOM)
		indx = 0;
	else if (indx_offset < 0)
		return -EINVAL;
	else
		indx = regs_get_register(regs, indx_offset);

	if (insn->addr_bytes == 4) {
		int addr32, base32, idx32;

		base32 = base & 0xffffffff;
		idx32 = indx & 0xffffffff;

		addr32 = base32 + idx32 * (1 << X86_SIB_SCALE(insn->sib.value));
		addr32 += insn->displacement.value;

		*eff_addr = addr32 & 0xffffffff;
	} else {
		*eff_addr = base + indx * (1 << X86_SIB_SCALE(insn->sib.value));
		*eff_addr += insn->displacement.value;
	}

	return 0;
}

/**
 * get_addr_ref_16() - Obtain the 16-bit address referred by instruction
 * @insn:	Instruction containing ModRM byte and displacement
 * @regs:	Register values as seen when entering kernel mode
 *
 * This function is to be used with 16-bit address encodings. Obtain the memory
 * address referred by the instruction's ModRM and displacement bytes. Also, the
 * segment used as base is determined by either any segment override prefixes in
 * @insn or the default segment of the registers involved in the address
 * computation. In protected mode, segment limits are enforced.
 *
 * Returns:
 *
 * Linear address referenced by the instruction operands on success.
 *
 * -1L on error.
 */
static void __user *get_addr_ref_16(struct insn *insn, struct pt_regs *regs)
{
	unsigned long linear_addr = -1L, seg_base, seg_limit;
	int ret, regoff;
	short eff_addr;
	long tmp;

	if (insn_get_displacement(insn))
		goto out;

	if (insn->addr_bytes != 2)
		goto out;

	if (X86_MODRM_MOD(insn->modrm.value) == 3) {
		ret = get_eff_addr_reg(insn, regs, &regoff, &tmp);
		if (ret)
			goto out;

		eff_addr = tmp;
	} else {
		ret = get_eff_addr_modrm_16(insn, regs, &regoff, &eff_addr);
		if (ret)
			goto out;
	}

	ret = get_seg_base_limit(insn, regs, regoff, &seg_base, &seg_limit);
	if (ret)
		goto out;

	/*
	 * Before computing the linear address, make sure the effective address
	 * is within the limits of the segment. In virtual-8086 mode, segment
	 * limits are not enforced. In such a case, the segment limit is -1L to
	 * reflect this fact.
	 */
	if ((unsigned long)(eff_addr & 0xffff) > seg_limit)
		goto out;

	linear_addr = (unsigned long)(eff_addr & 0xffff) + seg_base;

	/* Limit linear address to 20 bits */
	if (v8086_mode(regs))
		linear_addr &= 0xfffff;

out:
	return (void __user *)linear_addr;
}

/**
 * get_addr_ref_32() - Obtain a 32-bit linear address
 * @insn:	Instruction with ModRM, SIB bytes and displacement
 * @regs:	Register values as seen when entering kernel mode
 *
 * This function is to be used with 32-bit address encodings to obtain the
 * linear memory address referred by the instruction's ModRM, SIB,
 * displacement bytes and segment base address, as applicable. If in protected
 * mode, segment limits are enforced.
 *
 * Returns:
 *
 * Linear address referenced by instruction and registers on success.
 *
 * -1L on error.
 */
static void __user *get_addr_ref_32(struct insn *insn, struct pt_regs *regs)
{
	unsigned long linear_addr = -1L, seg_base, seg_limit;
	int eff_addr, regoff;
	long tmp;
	int ret;

	if (insn->addr_bytes != 4)
		goto out;

	if (X86_MODRM_MOD(insn->modrm.value) == 3) {
		ret = get_eff_addr_reg(insn, regs, &regoff, &tmp);
		if (ret)
			goto out;

		eff_addr = tmp;

	} else {
		if (insn->sib.nbytes) {
			ret = get_eff_addr_sib(insn, regs, &regoff, &tmp);
			if (ret)
				goto out;

			eff_addr = tmp;
		} else {
			ret = get_eff_addr_modrm(insn, regs, &regoff, &tmp);
			if (ret)
				goto out;

			eff_addr = tmp;
		}
	}

	ret = get_seg_base_limit(insn, regs, regoff, &seg_base, &seg_limit);
	if (ret)
		goto out;

	/*
	 * In protected mode, before computing the linear address, make sure
	 * the effective address is within the limits of the segment.
	 * 32-bit addresses can be used in long and virtual-8086 modes if an
	 * address override prefix is used. In such cases, segment limits are
	 * not enforced. When in virtual-8086 mode, the segment limit is -1L
	 * to reflect this situation.
	 *
	 * After computed, the effective address is treated as an unsigned
	 * quantity.
	 */
	if (!any_64bit_mode(regs) && ((unsigned int)eff_addr > seg_limit))
		goto out;

	/*
	 * Even though 32-bit address encodings are allowed in virtual-8086
	 * mode, the address range is still limited to [0x-0xffff].
	 */
	if (v8086_mode(regs) && (eff_addr & ~0xffff))
		goto out;

	/*
	 * Data type long could be 64 bits in size. Ensure that our 32-bit
	 * effective address is not sign-extended when computing the linear
	 * address.
	 */
	linear_addr = (unsigned long)(eff_addr & 0xffffffff) + seg_base;

	/* Limit linear address to 20 bits */
	if (v8086_mode(regs))
		linear_addr &= 0xfffff;

out:
	return (void __user *)linear_addr;
}

/**
 * get_addr_ref_64() - Obtain a 64-bit linear address
 * @insn:	Instruction struct with ModRM and SIB bytes and displacement
 * @regs:	Structure with register values as seen when entering kernel mode
 *
 * This function is to be used with 64-bit address encodings to obtain the
 * linear memory address referred by the instruction's ModRM, SIB,
 * displacement bytes and segment base address, as applicable.
 *
 * Returns:
 *
 * Linear address referenced by instruction and registers on success.
 *
 * -1L on error.
 */
#ifndef CONFIG_X86_64
static void __user *get_addr_ref_64(struct insn *insn, struct pt_regs *regs)
{
	return (void __user *)-1L;
}
#else
static void __user *get_addr_ref_64(struct insn *insn, struct pt_regs *regs)
{
	unsigned long linear_addr = -1L, seg_base;
	int regoff, ret;
	long eff_addr;

	if (insn->addr_bytes != 8)
		goto out;

	if (X86_MODRM_MOD(insn->modrm.value) == 3) {
		ret = get_eff_addr_reg(insn, regs, &regoff, &eff_addr);
		if (ret)
			goto out;

	} else {
		if (insn->sib.nbytes) {
			ret = get_eff_addr_sib(insn, regs, &regoff, &eff_addr);
			if (ret)
				goto out;
		} else {
			ret = get_eff_addr_modrm(insn, regs, &regoff, &eff_addr);
			if (ret)
				goto out;
		}

	}

	ret = get_seg_base_limit(insn, regs, regoff, &seg_base, NULL);
	if (ret)
		goto out;

	linear_addr = (unsigned long)eff_addr + seg_base;

out:
	return (void __user *)linear_addr;
}
#endif /* CONFIG_X86_64 */

/**
 * insn_get_addr_ref() - Obtain the linear address referred by instruction
 * @insn:	Instruction structure containing ModRM byte and displacement
 * @regs:	Structure with register values as seen when entering kernel mode
 *
 * Obtain the linear address referred by the instruction's ModRM, SIB and
 * displacement bytes, and segment base, as applicable. In protected mode,
 * segment limits are enforced.
 *
 * Returns:
 *
 * Linear address referenced by instruction and registers on success.
 *
 * -1L on error.
 */
void __user *insn_get_addr_ref(struct insn *insn, struct pt_regs *regs)
{
	if (!insn || !regs)
		return (void __user *)-1L;

	if (insn_get_opcode(insn))
		return (void __user *)-1L;

	switch (insn->addr_bytes) {
	case 2:
		return get_addr_ref_16(insn, regs);
	case 4:
		return get_addr_ref_32(insn, regs);
	case 8:
		return get_addr_ref_64(insn, regs);
	default:
		return (void __user *)-1L;
	}
}

int insn_get_effective_ip(struct pt_regs *regs, unsigned long *ip)
{
	unsigned long seg_base = 0;

	/*
	 * If not in user-space long mode, a custom code segment could be in
	 * use. This is true in protected mode (if the process defined a local
	 * descriptor table), or virtual-8086 mode. In most of the cases
	 * seg_base will be zero as in USER_CS.
	 */
	if (!user_64bit_mode(regs)) {
		seg_base = insn_get_seg_base(regs, INAT_SEG_REG_CS);
		if (seg_base == -1L)
			return -EINVAL;
	}

	*ip = seg_base + regs->ip;

	return 0;
}

/**
 * insn_fetch_from_user() - Copy instruction bytes from user-space memory
 * @regs:	Structure with register values as seen when entering kernel mode
 * @buf:	Array to store the fetched instruction
 *
 * Gets the linear address of the instruction and copies the instruction bytes
 * to the buf.
 *
 * Returns:
 *
 * - number of instruction bytes copied.
 * - 0 if nothing was copied.
 * - -EINVAL if the linear address of the instruction could not be calculated
 */
int insn_fetch_from_user(struct pt_regs *regs, unsigned char buf[MAX_INSN_SIZE])
{
	unsigned long ip;
	int not_copied;

	if (insn_get_effective_ip(regs, &ip))
		return -EINVAL;

	not_copied = copy_from_user(buf, (void __user *)ip, MAX_INSN_SIZE);

	return MAX_INSN_SIZE - not_copied;
}

/**
 * insn_fetch_from_user_inatomic() - Copy instruction bytes from user-space memory
 *                                   while in atomic code
 * @regs:	Structure with register values as seen when entering kernel mode
 * @buf:	Array to store the fetched instruction
 *
 * Gets the linear address of the instruction and copies the instruction bytes
 * to the buf. This function must be used in atomic context.
 *
 * Returns:
 *
 *  - number of instruction bytes copied.
 *  - 0 if nothing was copied.
 *  - -EINVAL if the linear address of the instruction could not be calculated.
 */
int insn_fetch_from_user_inatomic(struct pt_regs *regs, unsigned char buf[MAX_INSN_SIZE])
{
	unsigned long ip;
	int not_copied;

	if (insn_get_effective_ip(regs, &ip))
		return -EINVAL;

	not_copied = __copy_from_user_inatomic(buf, (void __user *)ip, MAX_INSN_SIZE);

	return MAX_INSN_SIZE - not_copied;
}

/**
 * insn_decode_from_regs() - Decode an instruction
 * @insn:	Structure to store decoded instruction
 * @regs:	Structure with register values as seen when entering kernel mode
 * @buf:	Buffer containing the instruction bytes
 * @buf_size:   Number of instruction bytes available in buf
 *
 * Decodes the instruction provided in buf and stores the decoding results in
 * insn. Also determines the correct address and operand sizes.
 *
 * Returns:
 *
 * True if instruction was decoded, False otherwise.
 */
bool insn_decode_from_regs(struct insn *insn, struct pt_regs *regs,
			   unsigned char buf[MAX_INSN_SIZE], int buf_size)
{
	int seg_defs;

	insn_init(insn, buf, buf_size, user_64bit_mode(regs));

	/*
	 * Override the default operand and address sizes with what is specified
	 * in the code segment descriptor. The instruction decoder only sets
	 * the address size it to either 4 or 8 address bytes and does nothing
	 * for the operand bytes. This OK for most of the cases, but we could
	 * have special cases where, for instance, a 16-bit code segment
	 * descriptor is used.
	 * If there is an address override prefix, the instruction decoder
	 * correctly updates these values, even for 16-bit defaults.
	 */
	seg_defs = insn_get_code_seg_params(regs);
	if (seg_defs == -EINVAL)
		return false;

	insn->addr_bytes = INSN_CODE_SEG_ADDR_SZ(seg_defs);
	insn->opnd_bytes = INSN_CODE_SEG_OPND_SZ(seg_defs);

	if (insn_get_length(insn))
		return false;

	if (buf_size < insn->length)
		return false;

	return true;
}

/**
 * insn_decode_mmio() - Decode a MMIO instruction
 * @insn:	Structure to store decoded instruction
 * @bytes:	Returns size of memory operand
 *
 * Decodes instruction that used for Memory-mapped I/O.
 *
 * Returns:
 *
 * Type of the instruction. Size of the memory operand is stored in
 * @bytes. If decode failed, INSN_MMIO_DECODE_FAILED returned.
 */
enum insn_mmio_type insn_decode_mmio(struct insn *insn, int *bytes)
{
	enum insn_mmio_type type = INSN_MMIO_DECODE_FAILED;

	*bytes = 0;

	if (insn_get_opcode(insn))
		return INSN_MMIO_DECODE_FAILED;

	switch (insn->opcode.bytes[0]) {
	case 0x88: /* MOV m8,r8 */
		*bytes = 1;
		fallthrough;
	case 0x89: /* MOV m16/m32/m64, r16/m32/m64 */
		if (!*bytes)
			*bytes = insn->opnd_bytes;
		type = INSN_MMIO_WRITE;
		break;

	case 0xc6: /* MOV m8, imm8 */
		*bytes = 1;
		fallthrough;
	case 0xc7: /* MOV m16/m32/m64, imm16/imm32/imm64 */
		if (!*bytes)
			*bytes = insn->opnd_bytes;
		type = INSN_MMIO_WRITE_IMM;
		break;

	case 0x8a: /* MOV r8, m8 */
		*bytes = 1;
		fallthrough;
	case 0x8b: /* MOV r16/r32/r64, m16/m32/m64 */
		if (!*bytes)
			*bytes = insn->opnd_bytes;
		type = INSN_MMIO_READ;
		break;

	case 0xa4: /* MOVS m8, m8 */
		*bytes = 1;
		fallthrough;
	case 0xa5: /* MOVS m16/m32/m64, m16/m32/m64 */
		if (!*bytes)
			*bytes = insn->opnd_bytes;
		type = INSN_MMIO_MOVS;
		break;

	case 0x0f: /* Two-byte instruction */
		switch (insn->opcode.bytes[1]) {
		case 0xb6: /* MOVZX r16/r32/r64, m8 */
			*bytes = 1;
			fallthrough;
		case 0xb7: /* MOVZX r32/r64, m16 */
			if (!*bytes)
				*bytes = 2;
			type = INSN_MMIO_READ_ZERO_EXTEND;
			break;

		case 0xbe: /* MOVSX r16/r32/r64, m8 */
			*bytes = 1;
			fallthrough;
		case 0xbf: /* MOVSX r32/r64, m16 */
			if (!*bytes)
				*bytes = 2;
			type = INSN_MMIO_READ_SIGN_EXTEND;
			break;
		}
		break;
	}

	return type;
}

/*
 * Recognise typical NOP patterns for both 32bit and 64bit.
 *
 * Notably:
 *  - NOP, but not: REP NOP aka PAUSE
 *  - NOPL
 *  - MOV %reg, %reg
 *  - LEA 0(%reg),%reg
 *  - JMP +0
 *
 * Must not have false-positives; instructions identified as a NOP might be
 * emulated as a NOP (uprobe) or Run Length Encoded in a larger NOP
 * (alternatives).
 *
 * False-negatives are fine; need not be exhaustive.
 */
bool insn_is_nop(struct insn *insn)
{
	u8 b3 = 0, x3 = 0, r3 = 0;
	u8 b4 = 0, x4 = 0, r4 = 0, m = 0;
	u8 modrm, modrm_mod, modrm_reg, modrm_rm;
	u8 sib = 0, sib_scale, sib_index, sib_base;
	u8 nrex, rex;
	u8 p, rep = 0;

	if ((nrex = insn->rex_prefix.nbytes)) {
		rex = insn->rex_prefix.bytes[nrex-1];

		r3 = !!X86_REX_R(rex);
		x3 = !!X86_REX_X(rex);
		b3 = !!X86_REX_B(rex);
		if (nrex > 1) {
			r4 = !!X86_REX2_R(rex);
			x4 = !!X86_REX2_X(rex);
			b4 = !!X86_REX2_B(rex);
			m  = !!X86_REX2_M(rex);
		}

	} else if (insn->vex_prefix.nbytes) {
		/*
		 * Ignore VEX encoded NOPs
		 */
		return false;
	}

	if (insn->modrm.nbytes) {
		modrm = insn->modrm.bytes[0];
		modrm_mod = X86_MODRM_MOD(modrm);
		modrm_reg = X86_MODRM_REG(modrm) + 8*r3 + 16*r4;
		modrm_rm  = X86_MODRM_RM(modrm)  + 8*b3 + 16*b4;
		modrm = 1;
	}

	if (insn->sib.nbytes) {
		sib = insn->sib.bytes[0];
		sib_scale = X86_SIB_SCALE(sib);
		sib_index = X86_SIB_INDEX(sib) + 8*x3 + 16*x4;
		sib_base  = X86_SIB_BASE(sib)  + 8*b3 + 16*b4;
		sib = 1;

		modrm_rm = sib_base;
	}

	for_each_insn_prefix(insn, p) {
		if (p == 0xf3) /* REPE */
			rep = 1;
	}

	/*
	 * Opcode map munging:
	 *
	 * REX2: 0 - single byte opcode
	 *       1 - 0f second byte opcode
	 */
	switch (m) {
	case 0: break;
	case 1: insn->opcode.value <<= 8;
		insn->opcode.value |= 0x0f;
		break;
	default:
		return false;
	}

	switch (insn->opcode.bytes[0]) {
	case 0x0f: /* 2nd byte */
		break;

	case 0x89: /* MOV */
		if (modrm_mod != 3) /* register-direct */
			return false;

		/* native size */
		if (insn->opnd_bytes != 4 * (1 + insn->x86_64))
			return false;

		return modrm_reg == modrm_rm; /* MOV %reg, %reg */

	case 0x8d: /* LEA */
		if (modrm_mod == 0 || modrm_mod == 3) /* register-indirect with disp */
			return false;

		/* native size */
		if (insn->opnd_bytes != 4 * (1 + insn->x86_64))
			return false;

		if (insn->displacement.value != 0)
			return false;

		if (sib && (sib_scale != 0 || sib_index != 4)) /* (%reg, %eiz, 1) */
			return false;

		for_each_insn_prefix(insn, p) {
			if (p != 0x3e) /* DS */
				return false;
		}

		return modrm_reg == modrm_rm; /* LEA 0(%reg), %reg */

	case 0x90: /* NOP */
		if (b3 || b4) /* XCHG %r{8,16,24},%rax */
			return false;

		if (rep) /* REP NOP := PAUSE */
			return false;

		return true;

	case 0xe9: /* JMP.d32 */
	case 0xeb: /* JMP.d8 */
		return insn->immediate.value == 0; /* JMP +0 */

	default:
		return false;
	}

	switch (insn->opcode.bytes[1]) {
	case 0x1f:
		return modrm_reg == 0; /* 0f 1f /0 -- NOPL */

	default:
		return false;
	}
}

```

`kernel/trace/arch/x86/lib/insn.c`:

```c
// SPDX-License-Identifier: GPL-2.0-or-later
/*
 * x86 instruction analysis
 *
 * Copyright (C) IBM Corporation, 2002, 2004, 2009
 */

#include <linux/kernel.h>
#ifdef __KERNEL__
#include <linux/string.h>
#else
#include <string.h>
#endif
#include "inat.h" /*__ignore_sync_check__ */
#include "insn.h" /* __ignore_sync_check__ */
#include <linux/version.h>
#if LINUX_VERSION_CODE < KERNEL_VERSION(6, 12, 0)
#include <asm/unaligned.h>
#else
#include <linux/unaligned.h> /* __ignore_sync_check__ */
#endif

#include <linux/errno.h>
#include <linux/kconfig.h>

#include <asm/emulate_prefix.h> /* __ignore_sync_check__ */

#define leXX_to_cpu(t, r)						\
({									\
	__typeof__(t) v;						\
	switch (sizeof(t)) {						\
	case 4: v = le32_to_cpu(r); break;				\
	case 2: v = le16_to_cpu(r); break;				\
	case 1:	v = r; break;						\
	default:							\
		BUILD_BUG(); break;					\
	}								\
	v;								\
})

/* Verify next sizeof(t) bytes can be on the same instruction */
#define validate_next(t, insn, n)	\
	((insn)->next_byte + sizeof(t) + n <= (insn)->end_kaddr)

#define __get_next(t, insn)	\
	({ t r = get_unaligned((t *)(insn)->next_byte); (insn)->next_byte += sizeof(t); leXX_to_cpu(t, r); })

#define __peek_nbyte_next(t, insn, n)	\
	({ t r = get_unaligned((t *)(insn)->next_byte + n); leXX_to_cpu(t, r); })

#define get_next(t, insn)	\
	({ if (unlikely(!validate_next(t, insn, 0))) goto err_out; __get_next(t, insn); })

#define peek_nbyte_next(t, insn, n)	\
	({ if (unlikely(!validate_next(t, insn, n))) goto err_out; __peek_nbyte_next(t, insn, n); })

#define peek_next(t, insn)	peek_nbyte_next(t, insn, 0)

/**
 * insn_init() - initialize struct insn
 * @insn:	&struct insn to be initialized
 * @kaddr:	address (in kernel memory) of instruction (or copy thereof)
 * @buf_len:	length of the insn buffer at @kaddr
 * @x86_64:	!0 for 64-bit kernel or 64-bit app
 */
void insn_init(struct insn *insn, const void *kaddr, int buf_len, int x86_64)
{
	/*
	 * Instructions longer than MAX_INSN_SIZE (15 bytes) are invalid
	 * even if the input buffer is long enough to hold them.
	 */
	if (buf_len > MAX_INSN_SIZE)
		buf_len = MAX_INSN_SIZE;

	memset(insn, 0, sizeof(*insn));
	insn->kaddr = kaddr;
	insn->end_kaddr = kaddr + buf_len;
	insn->next_byte = kaddr;
	insn->x86_64 = x86_64;
	insn->opnd_bytes = 4;
	if (x86_64)
		insn->addr_bytes = 8;
	else
		insn->addr_bytes = 4;
}

static const insn_byte_t xen_prefix[] = { __XEN_EMULATE_PREFIX };
static const insn_byte_t kvm_prefix[] = { __KVM_EMULATE_PREFIX };

static int __insn_get_emulate_prefix(struct insn *insn,
				     const insn_byte_t *prefix, size_t len)
{
	size_t i;

	for (i = 0; i < len; i++) {
		if (peek_nbyte_next(insn_byte_t, insn, i) != prefix[i])
			goto err_out;
	}

	insn->emulate_prefix_size = len;
	insn->next_byte += len;

	return 1;

err_out:
	return 0;
}

static void insn_get_emulate_prefix(struct insn *insn)
{
	if (__insn_get_emulate_prefix(insn, xen_prefix, sizeof(xen_prefix)))
		return;

	__insn_get_emulate_prefix(insn, kvm_prefix, sizeof(kvm_prefix));
}

/**
 * insn_get_prefixes - scan x86 instruction prefix bytes
 * @insn:	&struct insn containing instruction
 *
 * Populates the @insn->prefixes bitmap, and updates @insn->next_byte
 * to point to the (first) opcode.  No effect if @insn->prefixes.got
 * is already set.
 *
 * * Returns:
 * 0:  on success
 * < 0: on error
 */
int insn_get_prefixes(struct insn *insn)
{
	struct insn_field *prefixes = &insn->prefixes;
	insn_attr_t attr;
	insn_byte_t b, lb;
	int i, nb;

	if (prefixes->got)
		return 0;

	insn_get_emulate_prefix(insn);

	nb = 0;
	lb = 0;
	b = peek_next(insn_byte_t, insn);
	attr = inat_get_opcode_attribute(b);
	while (inat_is_legacy_prefix(attr)) {
		/* Skip if same prefix */
		for (i = 0; i < nb; i++)
			if (prefixes->bytes[i] == b)
				goto found;
		if (nb == 4)
			/* Invalid instruction */
			break;
		prefixes->bytes[nb++] = b;
		if (inat_is_address_size_prefix(attr)) {
			/* address size switches 2/4 or 4/8 */
			if (insn->x86_64)
				insn->addr_bytes ^= 12;
			else
				insn->addr_bytes ^= 6;
		} else if (inat_is_operand_size_prefix(attr)) {
			/* oprand size switches 2/4 */
			insn->opnd_bytes ^= 6;
		}
found:
		prefixes->nbytes++;
		insn->next_byte++;
		lb = b;
		b = peek_next(insn_byte_t, insn);
		attr = inat_get_opcode_attribute(b);
	}
	/* Set the last prefix */
	if (lb && lb != insn->prefixes.bytes[3]) {
		if (unlikely(insn->prefixes.bytes[3])) {
			/* Swap the last prefix */
			b = insn->prefixes.bytes[3];
			for (i = 0; i < nb; i++)
				if (prefixes->bytes[i] == lb)
					insn_set_byte(prefixes, i, b);
		}
		insn_set_byte(&insn->prefixes, 3, lb);
	}

	/* Decode REX prefix */
	if (insn->x86_64) {
		b = peek_next(insn_byte_t, insn);
		attr = inat_get_opcode_attribute(b);
		if (inat_is_rex_prefix(attr)) {
			insn_field_set(&insn->rex_prefix, b, 1);
			insn->next_byte++;
			if (X86_REX_W(b))
				/* REX.W overrides opnd_size */
				insn->opnd_bytes = 8;
		} else if (inat_is_rex2_prefix(attr)) {
			insn_set_byte(&insn->rex_prefix, 0, b);
			b = peek_nbyte_next(insn_byte_t, insn, 1);
			insn_set_byte(&insn->rex_prefix, 1, b);
			insn->rex_prefix.nbytes = 2;
			insn->next_byte += 2;
			if (X86_REX_W(b))
				/* REX.W overrides opnd_size */
				insn->opnd_bytes = 8;
			insn->rex_prefix.got = 1;
			goto vex_end;
		}
	}
	insn->rex_prefix.got = 1;

	/* Decode VEX/XOP prefix */
	b = peek_next(insn_byte_t, insn);
	if (inat_is_vex_prefix(attr) || inat_is_xop_prefix(attr)) {
		insn_byte_t b2 = peek_nbyte_next(insn_byte_t, insn, 1);

		if (inat_is_xop_prefix(attr) && X86_MODRM_REG(b2) == 0) {
			/* Grp1A.0 is always POP Ev */
			goto vex_end;
		} else if (!insn->x86_64) {
			/*
			 * In 32-bits mode, if the [7:6] bits (mod bits of
			 * ModRM) on the second byte are not 11b, it is
			 * LDS or LES or BOUND.
			 */
			if (X86_MODRM_MOD(b2) != 3)
				goto vex_end;
		}
		insn_set_byte(&insn->vex_prefix, 0, b);
		insn_set_byte(&insn->vex_prefix, 1, b2);
		if (inat_is_evex_prefix(attr)) {
			b2 = peek_nbyte_next(insn_byte_t, insn, 2);
			insn_set_byte(&insn->vex_prefix, 2, b2);
			b2 = peek_nbyte_next(insn_byte_t, insn, 3);
			insn_set_byte(&insn->vex_prefix, 3, b2);
			insn->vex_prefix.nbytes = 4;
			insn->next_byte += 4;
			if (insn->x86_64 && X86_VEX_W(b2))
				/* VEX.W overrides opnd_size */
				insn->opnd_bytes = 8;
		} else if (inat_is_vex3_prefix(attr) || inat_is_xop_prefix(attr)) {
			b2 = peek_nbyte_next(insn_byte_t, insn, 2);
			insn_set_byte(&insn->vex_prefix, 2, b2);
			insn->vex_prefix.nbytes = 3;
			insn->next_byte += 3;
			if (insn->x86_64 && X86_VEX_W(b2))
				/* VEX.W/XOP.W overrides opnd_size */
				insn->opnd_bytes = 8;
		} else {
			/*
			 * For VEX2, fake VEX3-like byte#2.
			 * Makes it easier to decode vex.W, vex.vvvv,
			 * vex.L and vex.pp. Masking with 0x7f sets vex.W == 0.
			 */
			insn_set_byte(&insn->vex_prefix, 2, b2 & 0x7f);
			insn->vex_prefix.nbytes = 2;
			insn->next_byte += 2;
		}
	}
vex_end:
	insn->vex_prefix.got = 1;

	prefixes->got = 1;

	return 0;

err_out:
	return -ENODATA;
}

/**
 * insn_get_opcode - collect opcode(s)
 * @insn:	&struct insn containing instruction
 *
 * Populates @insn->opcode, updates @insn->next_byte to point past the
 * opcode byte(s), and set @insn->attr (except for groups).
 * If necessary, first collects any preceding (prefix) bytes.
 * Sets @insn->opcode.value = opcode1.  No effect if @insn->opcode.got
 * is already 1.
 *
 * Returns:
 * 0:  on success
 * < 0: on error
 */
int insn_get_opcode(struct insn *insn)
{
	struct insn_field *opcode = &insn->opcode;
	int pfx_id, ret;
	insn_byte_t op;

	if (opcode->got)
		return 0;

	ret = insn_get_prefixes(insn);
	if (ret)
		return ret;

	/* Get first opcode */
	op = get_next(insn_byte_t, insn);
	insn_set_byte(opcode, 0, op);
	opcode->nbytes = 1;

	/* Check if there is VEX/XOP prefix or not */
	if (insn_is_avx_or_xop(insn)) {
		insn_byte_t m, p;

		/* XOP prefix has different encoding */
		if (unlikely(avx_insn_is_xop(insn))) {
			m = insn_xop_map_bits(insn);
			insn->attr = inat_get_xop_attribute(op, m);
			if (!inat_accept_xop(insn->attr)) {
				insn->attr = 0;
				return -EINVAL;
			}
			/* XOP has only 1 byte for opcode */
			goto end;
		}

		m = insn_vex_m_bits(insn);
		p = insn_vex_p_bits(insn);
		insn->attr = inat_get_avx_attribute(op, m, p);
		/* SCALABLE EVEX uses p bits to encode operand size */
		if (inat_evex_scalable(insn->attr) && !insn_vex_w_bit(insn) &&
		    p == INAT_PFX_OPNDSZ)
			insn->opnd_bytes = 2;
		if ((inat_must_evex(insn->attr) && !insn_is_evex(insn)) ||
		    (!inat_accept_vex(insn->attr) &&
		     !inat_is_group(insn->attr))) {
			/* This instruction is bad */
			insn->attr = 0;
			return -EINVAL;
		}
		/* VEX has only 1 byte for opcode */
		goto end;
	}

	/* Check if there is REX2 prefix or not */
	if (insn_is_rex2(insn)) {
		if (insn_rex2_m_bit(insn)) {
			/* map 1 is escape 0x0f */
			insn_attr_t esc_attr = inat_get_opcode_attribute(0x0f);

			pfx_id = insn_last_prefix_id(insn);
			insn->attr = inat_get_escape_attribute(op, pfx_id, esc_attr);
		} else {
			insn->attr = inat_get_opcode_attribute(op);
		}
		goto end;
	}

	insn->attr = inat_get_opcode_attribute(op);
	if (insn->x86_64 && inat_is_invalid64(insn->attr)) {
		/* This instruction is invalid, like UD2. Stop decoding. */
		insn->attr &= INAT_INV64;
	}

	while (inat_is_escape(insn->attr)) {
		/* Get escaped opcode */
		op = get_next(insn_byte_t, insn);
		opcode->bytes[opcode->nbytes++] = op;
		pfx_id = insn_last_prefix_id(insn);
		insn->attr = inat_get_escape_attribute(op, pfx_id, insn->attr);
	}

	if (inat_must_vex(insn->attr)) {
		/* This instruction is bad */
		insn->attr = 0;
		return -EINVAL;
	}

end:
	opcode->got = 1;
	return 0;

err_out:
	return -ENODATA;
}

/**
 * insn_get_modrm - collect ModRM byte, if any
 * @insn:	&struct insn containing instruction
 *
 * Populates @insn->modrm and updates @insn->next_byte to point past the
 * ModRM byte, if any.  If necessary, first collects the preceding bytes
 * (prefixes and opcode(s)).  No effect if @insn->modrm.got is already 1.
 *
 * Returns:
 * 0:  on success
 * < 0: on error
 */
int insn_get_modrm(struct insn *insn)
{
	struct insn_field *modrm = &insn->modrm;
	insn_byte_t pfx_id, mod;
	int ret;

	if (modrm->got)
		return 0;

	ret = insn_get_opcode(insn);
	if (ret)
		return ret;

	if (inat_has_modrm(insn->attr)) {
		mod = get_next(insn_byte_t, insn);
		insn_field_set(modrm, mod, 1);
		if (inat_is_group(insn->attr)) {
			pfx_id = insn_last_prefix_id(insn);
			insn->attr = inat_get_group_attribute(mod, pfx_id,
							      insn->attr);
			if (insn_is_avx_or_xop(insn) && !inat_accept_vex(insn->attr) &&
			    !inat_accept_xop(insn->attr)) {
				/* Bad insn */
				insn->attr = 0;
				return -EINVAL;
			}
		}
	}

	if (insn->x86_64 && inat_is_force64(insn->attr))
		insn->opnd_bytes = 8;

	modrm->got = 1;
	return 0;

err_out:
	return -ENODATA;
}


/**
 * insn_rip_relative() - Does instruction use RIP-relative addressing mode?
 * @insn:	&struct insn containing instruction
 *
 * If necessary, first collects the instruction up to and including the
 * ModRM byte.  No effect if @insn->x86_64 is 0.
 */
int insn_rip_relative(struct insn *insn)
{
	struct insn_field *modrm = &insn->modrm;
	int ret;

	if (!insn->x86_64)
		return 0;

	ret = insn_get_modrm(insn);
	if (ret)
		return 0;
	/*
	 * For rip-relative instructions, the mod field (top 2 bits)
	 * is zero and the r/m field (bottom 3 bits) is 0x5.
	 */
	return (modrm->nbytes && (modrm->bytes[0] & 0xc7) == 0x5);
}

/**
 * insn_get_sib() - Get the SIB byte of instruction
 * @insn:	&struct insn containing instruction
 *
 * If necessary, first collects the instruction up to and including the
 * ModRM byte.
 *
 * Returns:
 * 0: if decoding succeeded
 * < 0: otherwise.
 */
int insn_get_sib(struct insn *insn)
{
	insn_byte_t modrm;
	int ret;

	if (insn->sib.got)
		return 0;

	ret = insn_get_modrm(insn);
	if (ret)
		return ret;

	if (insn->modrm.nbytes) {
		modrm = insn->modrm.bytes[0];
		if (insn->addr_bytes != 2 &&
		    X86_MODRM_MOD(modrm) != 3 && X86_MODRM_RM(modrm) == 4) {
			insn_field_set(&insn->sib,
				       get_next(insn_byte_t, insn), 1);
		}
	}
	insn->sib.got = 1;

	return 0;

err_out:
	return -ENODATA;
}


/**
 * insn_get_displacement() - Get the displacement of instruction
 * @insn:	&struct insn containing instruction
 *
 * If necessary, first collects the instruction up to and including the
 * SIB byte.
 * Displacement value is sign-expanded.
 *
 * * Returns:
 * 0: if decoding succeeded
 * < 0: otherwise.
 */
int insn_get_displacement(struct insn *insn)
{
	insn_byte_t mod, rm, base;
	int ret;

	if (insn->displacement.got)
		return 0;

	ret = insn_get_sib(insn);
	if (ret)
		return ret;

	if (insn->modrm.nbytes) {
		/*
		 * Interpreting the modrm byte:
		 * mod = 00 - no displacement fields (exceptions below)
		 * mod = 01 - 1-byte displacement field
		 * mod = 10 - displacement field is 4 bytes, or 2 bytes if
		 * 	address size = 2 (0x67 prefix in 32-bit mode)
		 * mod = 11 - no memory operand
		 *
		 * If address size = 2...
		 * mod = 00, r/m = 110 - displacement field is 2 bytes
		 *
		 * If address size != 2...
		 * mod != 11, r/m = 100 - SIB byte exists
		 * mod = 00, SIB base = 101 - displacement field is 4 bytes
		 * mod = 00, r/m = 101 - rip-relative addressing, displacement
		 * 	field is 4 bytes
		 */
		mod = X86_MODRM_MOD(insn->modrm.value);
		rm = X86_MODRM_RM(insn->modrm.value);
		base = X86_SIB_BASE(insn->sib.value);
		if (mod == 3)
			goto out;
		if (mod == 1) {
			insn_field_set(&insn->displacement,
				       get_next(signed char, insn), 1);
		} else if (insn->addr_bytes == 2) {
			if ((mod == 0 && rm == 6) || mod == 2) {
				insn_field_set(&insn->displacement,
					       get_next(short, insn), 2);
			}
		} else {
			if ((mod == 0 && rm == 5) || mod == 2 ||
			    (mod == 0 && base == 5)) {
				insn_field_set(&insn->displacement,
					       get_next(int, insn), 4);
			}
		}
	}
out:
	insn->displacement.got = 1;
	return 0;

err_out:
	return -ENODATA;
}

/* Decode moffset16/32/64. Return 0 if failed */
static int __get_moffset(struct insn *insn)
{
	switch (insn->addr_bytes) {
	case 2:
		insn_field_set(&insn->moffset1, get_next(short, insn), 2);
		break;
	case 4:
		insn_field_set(&insn->moffset1, get_next(int, insn), 4);
		break;
	case 8:
		insn_field_set(&insn->moffset1, get_next(int, insn), 4);
		insn_field_set(&insn->moffset2, get_next(int, insn), 4);
		break;
	default:	/* opnd_bytes must be modified manually */
		goto err_out;
	}
	insn->moffset1.got = insn->moffset2.got = 1;

	return 1;

err_out:
	return 0;
}

/* Decode imm v32(Iz). Return 0 if failed */
static int __get_immv32(struct insn *insn)
{
	switch (insn->opnd_bytes) {
	case 2:
		insn_field_set(&insn->immediate, get_next(short, insn), 2);
		break;
	case 4:
	case 8:
		insn_field_set(&insn->immediate, get_next(int, insn), 4);
		break;
	default:	/* opnd_bytes must be modified manually */
		goto err_out;
	}

	return 1;

err_out:
	return 0;
}

/* Decode imm v64(Iv/Ov), Return 0 if failed */
static int __get_immv(struct insn *insn)
{
	switch (insn->opnd_bytes) {
	case 2:
		insn_field_set(&insn->immediate1, get_next(short, insn), 2);
		break;
	case 4:
		insn_field_set(&insn->immediate1, get_next(int, insn), 4);
		insn->immediate1.nbytes = 4;
		break;
	case 8:
		insn_field_set(&insn->immediate1, get_next(int, insn), 4);
		insn_field_set(&insn->immediate2, get_next(int, insn), 4);
		break;
	default:	/* opnd_bytes must be modified manually */
		goto err_out;
	}
	insn->immediate1.got = insn->immediate2.got = 1;

	return 1;
err_out:
	return 0;
}

/* Decode ptr16:16/32(Ap) */
static int __get_immptr(struct insn *insn)
{
	switch (insn->opnd_bytes) {
	case 2:
		insn_field_set(&insn->immediate1, get_next(short, insn), 2);
		break;
	case 4:
		insn_field_set(&insn->immediate1, get_next(int, insn), 4);
		break;
	case 8:
		/* ptr16:64 is not exist (no segment) */
		return 0;
	default:	/* opnd_bytes must be modified manually */
		goto err_out;
	}
	insn_field_set(&insn->immediate2, get_next(unsigned short, insn), 2);
	insn->immediate1.got = insn->immediate2.got = 1;

	return 1;
err_out:
	return 0;
}

/**
 * insn_get_immediate() - Get the immediate in an instruction
 * @insn:	&struct insn containing instruction
 *
 * If necessary, first collects the instruction up to and including the
 * displacement bytes.
 * Basically, most of immediates are sign-expanded. Unsigned-value can be
 * computed by bit masking with ((1 << (nbytes * 8)) - 1)
 *
 * Returns:
 * 0:  on success
 * < 0: on error
 */
int insn_get_immediate(struct insn *insn)
{
	int ret;

	if (insn->immediate.got)
		return 0;

	ret = insn_get_displacement(insn);
	if (ret)
		return ret;

	if (inat_has_moffset(insn->attr)) {
		if (!__get_moffset(insn))
			goto err_out;
		goto done;
	}

	if (!inat_has_immediate(insn->attr))
		goto done;

	switch (inat_immediate_size(insn->attr)) {
	case INAT_IMM_BYTE:
		insn_field_set(&insn->immediate, get_next(signed char, insn), 1);
		break;
	case INAT_IMM_WORD:
		insn_field_set(&insn->immediate, get_next(short, insn), 2);
		break;
	case INAT_IMM_DWORD:
		insn_field_set(&insn->immediate, get_next(int, insn), 4);
		break;
	case INAT_IMM_QWORD:
		insn_field_set(&insn->immediate1, get_next(int, insn), 4);
		insn_field_set(&insn->immediate2, get_next(int, insn), 4);
		break;
	case INAT_IMM_PTR:
		if (!__get_immptr(insn))
			goto err_out;
		break;
	case INAT_IMM_VWORD32:
		if (!__get_immv32(insn))
			goto err_out;
		break;
	case INAT_IMM_VWORD:
		if (!__get_immv(insn))
			goto err_out;
		break;
	default:
		/* Here, insn must have an immediate, but failed */
		goto err_out;
	}
	if (inat_has_second_immediate(insn->attr)) {
		insn_field_set(&insn->immediate2, get_next(signed char, insn), 1);
	}
done:
	insn->immediate.got = 1;
	return 0;

err_out:
	return -ENODATA;
}

/**
 * insn_get_length() - Get the length of instruction
 * @insn:	&struct insn containing instruction
 *
 * If necessary, first collects the instruction up to and including the
 * immediates bytes.
 *
 * Returns:
 *  - 0 on success
 *  - < 0 on error
*/
int insn_get_length(struct insn *insn)
{
	int ret;

	if (insn->length)
		return 0;

	ret = insn_get_immediate(insn);
	if (ret)
		return ret;

	insn->length = (unsigned char)((unsigned long)insn->next_byte
				     - (unsigned long)insn->kaddr);

	return 0;
}

/* Ensure this instruction is decoded completely */
static inline int insn_complete(struct insn *insn)
{
	return insn->opcode.got && insn->modrm.got && insn->sib.got &&
		insn->displacement.got && insn->immediate.got;
}

/**
 * insn_decode() - Decode an x86 instruction
 * @insn:	&struct insn to be initialized
 * @kaddr:	address (in kernel memory) of instruction (or copy thereof)
 * @buf_len:	length of the insn buffer at @kaddr
 * @m:		insn mode, see enum insn_mode
 *
 * Returns:
 * 0: if decoding succeeded
 * < 0: otherwise.
 */
int insn_decode(struct insn *insn, const void *kaddr, int buf_len, enum insn_mode m)
{
	int ret;

/* #define INSN_MODE_KERN	-1 __ignore_sync_check__ mode is only valid in the kernel */

	if (m == INSN_MODE_KERN)
		insn_init(insn, kaddr, buf_len, IS_ENABLED(CONFIG_X86_64));
	else
		insn_init(insn, kaddr, buf_len, m == INSN_MODE_64);

	ret = insn_get_length(insn);
	if (ret)
		return ret;

	if (insn_complete(insn))
		return 0;

	return -EINVAL;
}

```

`kernel/trace/smptrace_main.c`:

```c
// SPDX-License-Identifier: GPL-2.0
/*
 *  Copyright (C) 2026  Carlos López <carlos.lopezr4096@gmail.com>
 *  Copyright (C) 2026  Joel Bueno <buenocalvachehjoel@gmail.com>
 *
 * smptrace: SMP MMIO read/write tracing
 *
 * This module implements something similar to mmiotrace in the Linux kernel,
 * (see Documentation/trace/mmiotrace.rst), with some differences. The main
 * change is that smptrace works with multiple CPUs concurrently, and has first
 * class software APIs for other in-kernel users (instead of just exposing a
 * debugfs interface).
 *
 * The reason why this implementation works in SMP configurations is that
 * single-stepping is not used, avoiding the potential races with that approach.
 * Instead, we hook #PF to emulate faulting MMIO instructions, allowing a
 * strictly per-CPU approach, with little inter-CPU synchronization.
 *
 * mmiotrace also has first-class support in the kernel, while smptrace uses
 * kprobes to hook the relevant pieces. While this works, it may be more
 * brittle to future changes in the  kernel. Do not use this in a production
 * system.
 *
 * smptrace uses as little kernel APIs as possible, since it is built out of
 * tree. This also implies vendoring some kernel code, namely the x86
 * isntruction decoder, which is not exported to kernel modules.
 */
#define pr_fmt(fmt) "%s:smptrace: " fmt, KBUILD_MODNAME
#include <linux/module.h>
#include <linux/kernel.h>
#include <linux/init.h>
#include <linux/fs.h>
#include <linux/miscdevice.h>
#include <linux/kprobes.h>
#include <linux/uaccess.h>
#include <linux/slab.h>
#include <linux/smp.h>
#include <linux/mm.h>
#include <linux/notifier.h>
#include <linux/kallsyms.h>
#include <linux/sched.h>
#include <linux/ptrace.h>
#include <linux/version.h>
#include <asm/debugreg.h>
#include <asm/io.h>
#include <asm/tlbflush.h>
#include <asm/traps.h>

#include "insn.h"
#include "insn-eval.h"
#include "trace/smptrace.h"

#if LINUX_VERSION_CODE < KERNEL_VERSION(6, 12, 0)
static inline pud_t pud_mkinvalid(pud_t pud)
{
       return pfn_pud(pud_pfn(pud),
                      __pgprot(pud_flags(pud) & ~(_PAGE_PRESENT|_PAGE_PROTNONE)));
}
#endif

static void ____write_cr4(unsigned long val)
{
	asm volatile("mov %0,%%cr4": "+r" (val) : : "memory");
}

static void __flush_tlb(void)
{
	unsigned long cr4 = __read_cr4();
	____write_cr4(cr4 ^ X86_CR4_PGE);
	____write_cr4(cr4);
}

static uint64_t level2size(unsigned int level)
{
	switch (level) {
		case PG_LEVEL_4K: return PAGE_SIZE;
		case PG_LEVEL_2M: return HPAGE_PMD_SIZE;
		case PG_LEVEL_1G: return HPAGE_PUD_SIZE;
		default: BUG();
	}
}

static int poison_pte(struct smptrace_ctx *ctx, unsigned long va,
                      unsigned int len)
{
	pte_t *ptep, pte;
	pmd_t *pmdp, pmd;
	pud_t *pudp, pud;
	int64_t remain = len;
	unsigned int level;
	struct smptrace_pte *orig, *tmp;
	int ret;

	while (remain > 0) {
		ptep = lookup_address(va, &level);
		if (!ptep) {
			ret = -ENOENT;
			goto fail;
		}

		orig = kzalloc(sizeof(*orig), GFP_ATOMIC);
		if (!orig) {
			ret = -ENOMEM;
			goto fail;
		}

		INIT_LIST_HEAD(&orig->list);
		orig->va = va;
		orig->level = level;

		/* Swap out PTE */
		switch (orig->level) {
		case PG_LEVEL_4K:
			pte = native_local_ptep_get_and_clear(ptep);
			orig->pte = pte_val(pte);
			break;
		case PG_LEVEL_2M:
			pmdp = (pmd_t *)ptep;
			pmd = pmdp_get(pmdp);
			orig->pte = pmd_val(pmd);
			set_pmd(pmdp, pmd_mkinvalid(pmd));
			break;
		case PG_LEVEL_1G:
			pudp = (pud_t *)ptep;
			pud = pudp_get(pudp);
			orig->pte = pud_val(pud);
			set_pud(pudp, pud_mkinvalid(pud));
			break;
		default:
			pr_err("unexpected page level 0x%x for VA 0x%llx\n",
			       orig->level, (u64)va);
			return -EINVAL;
		}

		pr_info("poisoned PTE for VA=%lx", va);

		remain -= level2size(orig->level);
		va += level2size(orig->level);
		list_add_tail(&orig->list, &ctx->ptes);
	}

	__flush_tlb();
	return 0;

fail:
	/* Free items, but do not bother to unpoison the PTEs. Let our
	 * caller detect the error, and simply return NULL to the caller
	 * of ioremap() */
	list_for_each_entry_safe(orig, tmp, &ctx->ptes, list) {
		list_del(&orig->list);
		kfree(orig);
	}
	__flush_tlb();
	return ret;
}

static struct smptrace_pte *__find_pte(struct smptrace_ctx *ctx, unsigned long va)
{
	struct smptrace_pte *tmp;

	list_for_each_entry(tmp, &ctx->ptes, list) {
		if (tmp->va == va)
			return tmp;
	}
	return NULL;
}

static void restore_pte(struct smptrace_ctx *ctx, unsigned long va,
                        unsigned int len)
{
	unsigned int level;
	pte_t *ptep;
	pmd_t *pmdp;
	pud_t *pudp;
	int64_t remain = len;
	struct smptrace_pte *orig;

	while (remain > 0) {
		ptep = lookup_address(va, &level);
		if (!ptep)
			return;

		orig = __find_pte(ctx, va);
		if (!orig) {
			pr_err("could not find PTE for va=0x%lx", va);
			return;
		}
		if (orig->level != level)
			pr_warn("PTE level mismatch (prev=%u found=%u)", orig->level, level);

		switch(level) {
		case PG_LEVEL_4K:
			set_pte_atomic(ptep, __pte(orig->pte));
			break;
		case PG_LEVEL_2M:
			pmdp = (pmd_t *)ptep;
			set_pmd(pmdp, __pmd(orig->pte));
			break;
		case PG_LEVEL_1G:
			pudp = (pud_t *)ptep;
			set_pud(pudp, __pud(orig->pte));
			break;
		default:
			pr_err("unexpected page level %u for VA 0x%lx\n", level, va);
			return;
		}

		pr_info("restored PTE for VA=%lx", va);

		remain -= level2size(orig->level);
		va += level2size(orig->level);

		list_del(&orig->list);
		kfree(orig);
	}

	__flush_tlb();
}

struct ioremap_args {
	resource_size_t pa;
	unsigned long len;
};

static int __enter_ioremap(struct kretprobe_instance *ri, struct pt_regs *regs)
{
	struct ioremap_args *args = (struct ioremap_args *)ri->data;

	args->pa = regs_get_kernel_argument(regs, 0);
	args->len = regs_get_kernel_argument(regs, 1);
	return 0;
}

static int __exit_ioremap(struct kretprobe_instance *ri, struct pt_regs *regs)
{
	struct kretprobe *rp = get_kretprobe(ri);
	struct smptrace_ctx *ctx = container_of(rp, struct smptrace_ctx, ioremap_krp);
	struct ioremap_args *args = (struct ioremap_args *)ri->data;
	unsigned long va = regs_return_value(regs);

	if (args->pa < ctx->pa || args->pa >= ctx->pa + ctx->len)
		return 0;

	if (atomic_long_cmpxchg(&ctx->traced_va, 0, va)) {
		pr_warn_ratelimited("duplicate ioremap(0x%llx), skipping", args->pa);
		return 0;
	}

	pr_info("poisoning VA=0x%lx:%lx (PA = 0x%llx:%lx)",
	        va, args->len, ctx->pa, ctx->len);

	/*
	 * Before the PTE is poisoned, traced_len will only be read by
	 * __enter_badarea(), which will not intercept anything until this
	 * kretprobe is done (since nobody will fault on ctx->traced_va, as it is
	 * not visible yet), so we can set it safely now.
	 */
	WRITE_ONCE(ctx->traced_len, args->len);

	/*
	 * Now poison the PTE(s). If we fail, return NULL to the caller of ioremap().
	 * To avoid leaking memory, iounmap() the address. Do the iounmap() *after*
	 * setting traced_va to 0, so that our own iounmap() kprobe does not
	 * interfere.
	 */
	if (poison_pte(ctx, va, args->len)) {
		regs_set_return_value(regs, 0);
		atomic_long_set_release(&ctx->traced_va, 0);
		iounmap((void __iomem *)va);

		pr_warn("failed to poison VA=0x%lx:%lx (PA = 0x%llx:%lx)",
		        va, args->len, ctx->pa, ctx->len);
	}

	return 0;
}

static int __enter_iounmap(struct kprobe *kp, struct pt_regs *regs)
{
	struct smptrace_ctx *ctx = container_of(kp, struct smptrace_ctx, iounmap_kp);
	unsigned long va = regs_get_kernel_argument(regs, 0);
	unsigned long to_clear = va;

	/* If cmpxchg fails it means we are not tracking this VA, so ignore */
	if (!atomic_long_try_cmpxchg(&ctx->traced_va, &va, 0))
		return 0;

	pr_info("restoring VA=0x%lx (PA = 0x%llx)", to_clear, ctx->pa);
	restore_pte(ctx, to_clear, ctx->traced_len);
	WRITE_ONCE(ctx->traced_len, 0);
	return 0;
}

static void __fill_io_notif(struct smptrace_io *io, const u8 *data, u32 size,
                            u64 off)
{
	io->offset = off;
	io->size = size;
	switch(size) {
	case 1:
		io->data.byte = *(u8 *)data;
		break;
	case 2:
		io->data.word = *(u16 *)data;
		break;
	case 4:
		io->data.dword = *(u32 *)data;
		break;
	case 8:
		io->data.qword = *(u64 *)data;
		break;
	default:
		BUG();
	}
}

static void emulate_read(struct smptrace_ctx *ctx, u64 addr, u32 size, u8 *dst)
{
	u64 off;
	unsigned long traced_len = READ_ONCE(ctx->traced_len);
	struct smptrace_io io;

	/*
	 * ctx->traced_va is only set to NULL in two situations:
	 *
	 *  - Someone iounmap()s it, at which point that is a bug in the
	 *    driver, as there is a pending access (which we hooked via #PF).
	 *  - Our driver is close()d, which waits until all kprobes (including
	 *    this one) stop running.
	 *
	 * So ctx->traced_va is always safe to read.
	 */
	off  = addr - atomic_long_read(&ctx->traced_va);
	BUG_ON(off >= traced_len || off + size > traced_len);
	memcpy_fromio(dst, ctx->shadow_va + off, size);

	if (ctx->notif.read) {
		__fill_io_notif(&io, dst, size, off);
		ctx->notif.read(ctx, &io);
	}
}

static void emulate_write(struct smptrace_ctx *ctx, u64 addr, u32 size,
                          const u8 *src)
{
	u64 off;
	unsigned long traced_len = READ_ONCE(ctx->traced_len);
	struct smptrace_io io = {0};

	off  = addr - atomic_long_read(&ctx->traced_va);
	BUG_ON(off >= traced_len || off + size > traced_len);
	if (!ctx->stop_writes)
		memcpy_toio(ctx->shadow_va + off, src, size);

	pr_debug("Write @ 0x%llx:%x", off, size);

	if (ctx->notif.write) {
		__fill_io_notif(&io, src, size, off);
		ctx->notif.write(ctx, &io);
	}
}

static int decode_pf_instr(struct pt_regs *regs, struct insn *insn)
{
	u8 buf[MAX_INSN_SIZE];

	if (copy_from_kernel_nofault(buf, (void *)regs->ip, MAX_INSN_SIZE))
		return -EINVAL;

	return insn_decode_kernel(insn, buf);
}

static int emulate_pf_instruction(struct smptrace_ctx *ctx, struct pt_regs *regs)
{
	struct insn insn;
	enum insn_mmio_type mmio;
	long *data;
	u64 addr;
	unsigned int len;
	int ret;
	u8 sign_byte;

	if (user_mode(regs))
		return -EACCES;

	ret = decode_pf_instr(regs, &insn);
	if (ret) {
		pr_warn("failed to decode #PF instr ip=0x%lx", regs->ip);
		return ret;
	}

	mmio = insn_decode_mmio(&insn, &len);
	if (mmio == INSN_MMIO_DECODE_FAILED) {
		pr_warn("failed to decode MMIO instr ip=0x%lx", regs->ip);
		return -EINVAL;
	}

	/* Get a pointer to the data if not writing an immediate, or if
	 * not doing MOVS (which we do not handle yet) */
	if (mmio != INSN_MMIO_WRITE_IMM && mmio != INSN_MMIO_MOVS) {
		data = insn_get_modrm_reg_ptr(&insn, regs);
		if (!data) {
			pr_warn("failed to get modrm reg ptr");
			return -EINVAL;
		}
	}

	/* Get the MMIO source/destination address */
	addr = (u64)insn_get_addr_ref(&insn, regs);

	switch (mmio) {
	case INSN_MMIO_WRITE:
		emulate_write(ctx, addr, len, (u8 *)data);
		break;
	case INSN_MMIO_WRITE_IMM:
		BUG_ON(len > 4);
		emulate_write(ctx, addr, len, (u8 *)insn.immediate1.bytes);
		break;
	case INSN_MMIO_READ:
		/* Zero-extend for 32-bit operation */
		if (len == 4)
			*data = 0;
		emulate_read(ctx, addr, len, (u8 *)data);
		break;
	case INSN_MMIO_READ_ZERO_EXTEND:
		memset(data, 0, insn.opnd_bytes);
		emulate_read(ctx, addr, len, (u8 *)data);
		break;
	case INSN_MMIO_READ_SIGN_EXTEND:
		/* Sign extend based on operand size */
		if (len == 1) {
			u8 val;
			emulate_read(ctx, addr, len, &val);
			sign_byte = (val & 0x80) ? 0xff : 0x00;
		} else {
			u16 val;
			emulate_read(ctx, addr, len, (u8 *)&val);
			sign_byte = (val & 0x8000) ? 0xff : 0x00;
		}
		memset(data, sign_byte, insn.opnd_bytes);
		emulate_read(ctx, addr, len, (u8 *)data);
		break;
	case INSN_MMIO_MOVS:
		pr_warn_ratelimited("unhandled MOVS instruction ip=0x%lx", regs->ip);
		return -ENOTSUPP;
	default:
		pr_warn_ratelimited("unhandled MMIO instruction ip=0x%lx (%d)",
		                    regs->ip, mmio);
		return -ENOTSUPP;
	}

	regs->ip += insn.length;
	return 0;
}

/* Gadget to force a `ret` and skip bad_area_nosemaphore() completely */
static void __used smptrace_ret_gadget(void) {}

static int __enter_badarea(struct kprobe *kp, struct pt_regs *regs)
{
	struct smptrace_ctx *ctx = container_of(kp, struct smptrace_ctx, badarea_kp);
	struct pt_regs *pf_regs = (struct pt_regs *)regs_get_kernel_argument(regs, 0);
	unsigned long pf_va = regs_get_kernel_argument(regs, 2);
	unsigned long traced_va = atomic_long_read(&ctx->traced_va);
	unsigned long traced_len = READ_ONCE(ctx->traced_len);
	int ret;

	if (!traced_va || pf_va < traced_va || pf_va >= traced_va + traced_len)
		return 0;

	if (this_cpu_xchg(*ctx->in_pf, true)) {
		pr_warn("reentrant #PF on 0x%lx, ignoring", traced_va);
		return 0;
	}

	ret = emulate_pf_instruction(ctx, pf_regs);
	this_cpu_write(*ctx->in_pf, false);

	/* Update return address to skip the whole function we hooked */
	if (!ret) {
		instruction_pointer_set(regs, (unsigned long)smptrace_ret_gadget);
		return 1;
	}

	return 0;
}

static int smptrace_activate(struct smptrace_ctx *ctx)
{
	int ret;

	ctx->shadow_va = ioremap(ctx->pa, ctx->len);
	if (!ctx->shadow_va)
		return -ENOMEM;
	/* Force the kernel to set the page as present to avoid a #PF */
	readl(ctx->shadow_va);

	ret = register_kprobe(&ctx->badarea_kp);
	if (ret)
		goto fail_kprobe1;

	ret = register_kprobe(&ctx->iounmap_kp);
	if (ret)
		goto fail_kprobe2;

	ret = register_kretprobe(&ctx->ioremap_krp);
	if (ret)
		goto fail_kprobe3;

	return 0;

fail_kprobe3:
	unregister_kprobe(&ctx->iounmap_kp);
fail_kprobe2:
	unregister_kprobe(&ctx->badarea_kp);
fail_kprobe1:
	iounmap(ctx->shadow_va);
	ctx->shadow_va = NULL;
	ctx->pa = 0;
	return ret;
}

int smptrace_init(struct smptrace_ctx *ctx)
{
	int ret;

	INIT_LIST_HEAD(&ctx->ptes);

	ctx->in_pf = alloc_percpu_gfp(bool, GFP_KERNEL_ACCOUNT);
	if (!ctx->in_pf)
		return -ENOMEM;

	ctx->badarea_kp = (struct kprobe) {
		.pre_handler = __enter_badarea,
		.symbol_name = "bad_area_nosemaphore",
	};

	ctx->iounmap_kp = (struct kprobe) {
		.pre_handler = __enter_iounmap,
		.symbol_name = "iounmap",
	};

	ctx->ioremap_krp = (struct kretprobe) {
		.entry_handler = __enter_ioremap,
		.handler = __exit_ioremap,
		.maxactive = 32,
		.data_size = sizeof(struct ioremap_args),
		.kp = {
			.symbol_name = "ioremap",
		},
	};

	ret = smptrace_activate(ctx);
	if (ret) {
		free_percpu(ctx->in_pf);
		return ret;
	}

	return 0;
}

static void smptrace_deactivate(struct smptrace_ctx *ctx)
{
	unsigned long va;

	/* First, stop hooks on ioremap and iounmap so everyone stops updating
	 * ctx->traced_va */
	unregister_kretprobe(&ctx->ioremap_krp);
	unregister_kprobe(&ctx->iounmap_kp);

	/* Restore the VA, if we had captured any, so we stop getting #PFs */
	va = atomic_long_xchg(&ctx->traced_va, 0);
	if (va) {
		restore_pte(ctx, va, ctx->traced_len);
		WRITE_ONCE(ctx->traced_len, 0);
	}

	/* Stop #PF hook now that we shouldn't be hitting #PF */
	unregister_kprobe(&ctx->badarea_kp);

	iounmap(ctx->shadow_va);
	ctx->shadow_va = 0;
}

void smptrace_destroy(struct smptrace_ctx *ctx)
{
	smptrace_deactivate(ctx);
	free_percpu(ctx->in_pf);
}

```

`test_system.sh`:

```sh
#!/bin/bash
set -e

RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m'

log_info() {
    echo -e "${GREEN}[INFO]${NC} $1"
}

log_warn() {
    echo -e "${YELLOW}[WARN]${NC} $1"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

check_module() {
if lsmod | grep -q "$1"; then
    log_info "Module $1 is loaded"
    return 0
else
    log_warn "Module $1 is not loaded"
    return 1
fi
}

log_info "Building kernel modules..."
make clean
make all

log_info "Unloading all related modules..."
sudo rmmod protopciem_driver 2>/dev/null || true
sudo rmmod pciem 2>/dev/null || true
sleep 1

log_info "Loading pciem"
/usr/src/linux-headers-$(uname -r)/scripts/sign-file sha256 ~/signing_key.priv ~/signing_key.x509 kernel/pciem.ko
sudo insmod kernel/pciem.ko pciem_phys_regions="bar0:0x1bf000000:0x10000,bar2:0x1bf100000:0x100000"
sleep 1

sudo ./examples/protopciem/userspace/protopciem_card &

sleep 1

if ! lspci -d 1f0c:0001 &>/dev/null; then
    log_error "Virtual PCI device not found!"
    exit 1
fi

log_info "Loading ProtoPCIem driver..."
sudo rmmod protopciem_driver 2>/dev/null || true
/usr/src/linux-headers-$(uname -r)/scripts/sign-file sha256 ~/signing_key.priv ~/signing_key.x509 kernel/driver/protopciem_driver.ko
sudo insmod ./examples/protopciem/driver/protopciem_driver.ko
sleep 1
if ! check_module protopciem_driver; then
    log_error "Failed to load ProtoPCIem driver"
    exit 1
fi

log_info "Test complete! Check dmesg for results."

```